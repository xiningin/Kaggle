{"cell_type":{"1741b76b":"code","d1d457cb":"code","968def7d":"code","0b718c04":"code","43332c09":"code","113fac19":"code","440af5c9":"code","589ccb99":"code","560513b5":"code","8584ed54":"code","97cbfc3e":"code","1d22e0b7":"code","501cad15":"code","a6dd72f0":"code","e50d9412":"code","b5f59ab1":"code","a43661a8":"code","cc94a5d2":"code","5b9ce008":"code","0b093919":"code","0931bcc6":"code","c5953e7d":"code","9bb75568":"code","0ef0f612":"code","2e4d015c":"code","c9a1bb87":"code","8e38ced7":"code","6c14a2c7":"code","f605f2d8":"code","3f097b28":"markdown","23089464":"markdown","4a8096e9":"markdown","4de1e036":"markdown","e1993001":"markdown","8505be7a":"markdown","78e5d7fb":"markdown","77535149":"markdown"},"source":{"1741b76b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1d457cb":"# Importing the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","968def7d":"# Importing the data set\ndf = pd.read_csv('\/kaggle\/input\/temperature-forecast-project-using-ml\/temp.csv')","0b718c04":"pd.set_option('display.max_row',25) #Affiche au plus 25 \u00e9l\u00e9ments dans les r\u00e9sultats de pandas\npd.set_option('display.max_column',25) #Affiche au plus 25 \u00e9l\u00e9ments dans les r\u00e9sultats de pandas\ndf.head()","43332c09":"df.dtypes.value_counts() # Compte les nombre de types de variables","113fac19":"df.shape","440af5c9":"plt.figure(figsize=(20,10))\nsns.heatmap(df.isna(),cbar=False)\nplt.show()\nprint((df.isna().sum()\/df.shape[0]*100).sort_values(ascending=False))","589ccb99":"plt.figure(figsize=(18, 6), dpi=80)\nplt.plot(df[\"Next_Tmax\"],label=\"Next_Tmax\")\nplt.plot(df[\"Next_Tmin\"],label=\"Next_Tmin\")\nplt.legend()\nplt.show()","560513b5":"for col in [\"Next_Tmax\",\"Next_Tmin\"]:\n    plt.figure()\n    sns.displot(df[col],kind='kde')\n    plt.show()\nprint(df[\"Next_Tmax\"].mean())\nprint(df[\"Next_Tmax\"].std())\nprint(df[\"Next_Tmin\"].mean())\nprint(df[\"Next_Tmin\"].std())","8584ed54":"plt.figure()\nsns.heatmap(pd.crosstab(df['Next_Tmax'],df['Next_Tmin']))\nplt.show()","97cbfc3e":"sns.heatmap(df.corr())","1d22e0b7":"# Importing the data set\ndf = pd.read_csv('\/kaggle\/input\/temperature-forecast-project-using-ml\/temp.csv')\nSave = df.copy()","501cad15":"def feature_engineering(df):\n    df = df.drop([\"Date\"],axis=1)\n    print(df.dtypes.value_counts()) # Compte les nombre de types de variables\n    return(df)","a6dd72f0":"def imputation(df):\n    #df = df.fillna(-999)\n    df = df.dropna(axis=0)\n    return df","e50d9412":"def encodage(df):\n    return df","b5f59ab1":"def preprocessing(df):\n    df = imputation(df)\n    df = encodage(df)\n    df = feature_engineering(df)\n    \n    X = df.drop(['Next_Tmax','Next_Tmin'],axis=1)\n    y_max = df[\"Next_Tmax\"]\n    y_min = df[\"Next_Tmin\"]\n    \n    print(X.shape)\n    print(y_max.shape)\n    \n    return X,y_max,y_min","a43661a8":"from sklearn.model_selection import train_test_split\ntrainset, testset = train_test_split(df, test_size=0.2, random_state=0)","cc94a5d2":"X_train, y_min_train, y_max_train = preprocessing(trainset)\nX_test, y_min_test, y_max_test = preprocessing(testset)","5b9ce008":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.model_selection import cross_validate","0b093919":"reg_max = make_pipeline(StandardScaler(),\n                    SGDRegressor(loss='squared_loss', penalty='l2', max_iter=1000, tol=1e-3))\nreg_max.fit(X_train, y_max_train)\n\nreg_min = make_pipeline(StandardScaler(),\n                    SGDRegressor(loss='squared_loss', penalty='l2', max_iter=1000, tol=1e-3))\nreg_min.fit(X_train, y_min_train)\n\ncv_results_min = cross_validate(reg_min, X_train, y_min_train, cv=5, scoring=('r2', \"neg_root_mean_squared_error\"), return_train_score=True)\ncv_results_max = cross_validate(reg_max, X_train, y_max_train, cv=5, scoring=('r2', \"neg_root_mean_squared_error\"), return_train_score=True)\n\nprint('Pour le Next_Tmin :')\nprint('Test RMSE :' , -cv_results_min['test_neg_root_mean_squared_error'].mean())\nprint('Test r2 :' , cv_results_min['test_r2'].mean())\nprint(\"Train RMSE :\" , -cv_results_min['train_neg_root_mean_squared_error'].mean())\nprint(\"Train r2 :\" , cv_results_min['train_r2'].mean())\nprint(\"*------------------------------------------*\")\nprint('Pour le Next_Tmax :')\nprint('Test RMSE :' , -cv_results_max['test_neg_root_mean_squared_error'].mean())\nprint('Test r2 :' , cv_results_max['test_r2'].mean())\nprint(\"Train RMSE :\" , -cv_results_max['train_neg_root_mean_squared_error'].mean())\nprint(\"Train r2 :\" , cv_results_max['train_r2'].mean())","0931bcc6":"Next_Tmin_predict = reg_min.predict(X_test)\nNext_Tmax_predict = reg_max.predict(X_test)","c5953e7d":"plt.figure(figsize=(18,6))\nplt.plot(y_min_test.to_numpy(),label=\"Next_Tmin\")\nplt.plot(Next_Tmin_predict,label=\"Next_Tmin_predict\")\nplt.legend()\nplt.show()\n\nplt.figure(figsize=(18,6))\nplt.plot(y_max_test.to_numpy(),label=\"Next_Tmax\")\nplt.plot(Next_Tmax_predict,label=\"Next_Tmax_predict\")\nplt.legend()\nplt.show()","9bb75568":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [70, 130],\n    'max_features': [3, 6],\n    'min_samples_leaf': [2, 3],\n    'min_samples_split': [4, 8],\n    'n_estimators': [1000, 500]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","0ef0f612":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","2e4d015c":"# Fit the grid search to the Max data\n\ngrid_search.fit(X_train, y_max_train)\nprint(grid_search.best_params_)\nbest_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_max_test)","c9a1bb87":"base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_max_train)\nbase_accuracy = evaluate(base_model, X_test, y_max_test)\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) \/ base_accuracy))","8e38ced7":"# Fit the grid search to the Min data\n\ngrid_search.fit(X_train, y_min_train)\nprint(grid_search.best_params_)\nbest_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_min_test)","6c14a2c7":"base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_min_train)\nbase_accuracy = evaluate(base_model, X_test, y_min_test)\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) \/ base_accuracy))","f605f2d8":"Next_Tmax_TreeRegressor = RandomForestRegressor(random_state = 42, \n                                                bootstrap=True, max_depth=110, max_features=4, \n                                                min_samples_leaf=2, min_samples_split=4, n_estimators=900)\nNext_Tmin_TreeRegressor = RandomForestRegressor(random_state = 42, \n                                                bootstrap=True, max_depth=70, max_features=6, \n                                                min_samples_leaf=2, min_samples_split=4, n_estimators=900)\n\n\nprint(\"---Next_Tmax---\")\nNext_Tmax_TreeRegressor.fit(X_train,y_max_train)\nNext_Tmax_Accuracy = evaluate(Next_Tmax_TreeRegressor, X_test, y_max_test)\n\nbase_max_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_max_model.fit(X_train, y_max_train)\nbase_max_accuracy = evaluate(base_max_model, X_test, y_max_test)\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (Next_Tmax_Accuracy - base_max_accuracy) \/ base_max_accuracy))\nprint(\"---------\")\n\n\nprint(\"---Next_Tmin---\")\nNext_Tmin_TreeRegressor.fit(X_train,y_min_train)\nNext_Tmin_Accuracy = evaluate(Next_Tmin_TreeRegressor, X_test, y_min_test)\n\nbase_min_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_min_model.fit(X_train, y_min_train)\nbase_min_accuracy = evaluate(base_min_model, X_test, y_min_test)\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (Next_Tmin_Accuracy - base_min_accuracy) \/ base_min_accuracy))\nprint(\"---------\")","3f097b28":"## Part 1 - SGDRegressor","23089464":"# Exploratory Data Analysis\n\n## Aim :\n* Understand the data (\"A small step forward is better than a big one backwards\")\n* Begin to develop a modelling strategy\n\n## Features\n\n    - station - used weather station number: 1 to 25\n    - Date - Present day: yyyy-mm-dd ('2013-06-30' to '2017-08-30')\n    - Present_Tmax - Maximum air temperature between 0 and 21 h on the present day (\u00c2\u00b0C): 20 to 37.6\n    - Present_Tmin - Minimum air temperature between 0 and 21 h on the present day (\u00c2\u00b0C): 11.3 to 29.9\n    - LDAPS_RHmin - LDAPS model forecast of next-day minimum relative humidity (%): 19.8 to 98.5\n    - LDAPS_RHmax - LDAPS model forecast of next-day maximum relative humidity (%): 58.9 to 100\n    - LDAPSTmaxlapse - LDAPS model forecast of next-day maximum air temperature applied lapse rate (\u00c2\u00b0C): 17.6 to 38.5\n    - LDAPSTminlapse - LDAPS model forecast of next-day minimum air temperature applied lapse rate (\u00c2\u00b0C): 14.3 to 29.6\n    - LDAPS_WS - LDAPS model forecast of next-day average wind speed (m\/s): 2.9 to 21.9\n    - LDAPS_LH - LDAPS model forecast of next-day average latent heat flux (W\/m2): -13.6 to 213.4\n    - LDAPS_CC1 - LDAPS model forecast of next-day 1st 6-hour split average cloud cover (0-5 h) (%): 0 to 0.97\n    - LDAPS_CC2 - LDAPS model forecast of next-day 2nd 6-hour split average cloud cover (6-11 h) (%): 0 to 0.97\n    - LDAPS_CC3 - LDAPS model forecast of next-day 3rd 6-hour split average cloud cover (12-17 h) (%): 0 to 0.98\n    - LDAPS_CC4 - LDAPS model forecast of next-day 4th 6-hour split average cloud cover (18-23 h) (%): 0 to 0.97\n    - LDAPS_PPT1 - LDAPS model forecast of next-day 1st 6-hour split average precipitation (0-5 h) (%): 0 to 23.7\n    - LDAPS_PPT2 - LDAPS model forecast of next-day 2nd 6-hour split average precipitation (6-11 h) (%): 0 to 21.6\n    - LDAPS_PPT3 - LDAPS model forecast of next-day 3rd 6-hour split average precipitation (12-17 h) (%): 0 to 15.8\n    - LDAPS_PPT4 - LDAPS model forecast of next-day 4th 6-hour split average precipitation (18-23 h) (%): 0 to 16.7\n    - lat - Latitude (\u00c2\u00b0): 37.456 to 37.645\n    - lon - Longitude (\u00c2\u00b0): 126.826 to 127.135\n    - DEM - Elevation (m): 12.4 to 212.3\n    - Slope - Slope (\u00c2\u00b0): 0.1 to 5.2\n    - Solar radiation - Daily incoming solar radiation (wh\/m2): 4329.5 to 5992.9\n    - Next_Tmax - The next-day maximum air temperature (\u00c2\u00b0C): 17.4 to 38.9\n    - Next_Tmin - The next-day minimum air temperature (\u00c2\u00b0C): 11.3 to 29.8T  \n\n## Base Chacklist\n#### Shape analysis :\n- **target(s)** : Next_Tmax & Next_Tmin\n- **rows and columns** : 7752 , 25 \n- **features types** : qualitatives : 1 (la date du relev\u00e9) , quantitatives : 24 (tout le reste)\n- **NaN analysis** :\n    - vraiment pas beaucoup de NaN (moiti\u00e9 des variables = 1% de NaN)\n\n#### Features analysis :\n- **Target visualization** :\n    - Next_Tmax = 30.27\u00b0 +\/- 3.12\u00b0\n    - Next_Tmin = 22.930 +\/- 2.49\u00b0","4a8096e9":"# Modeling\n\n## Aim :\n- Standardise features\n- Define a regression model\n- Compute score and RMSE (in \u00b0C)","4de1e036":"# If you like please upvote !\n## Also check my other notebooks :\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83d\udc01Mice Trisomy (100% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-mice-100-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83c\udf97\ufe0fBreast Cancer Detection : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-breast-cancer-detection\n#### \ud83c\udf26\ud83c\udf21 Weather Forecasting \ud83d\udcc8 (98% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/weather-forecasting-98-acc\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Heart Attack \ud83e\ude7a\ud83d\udc93 (90% Acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-heart-attack-90-accuracy-score\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - Mobile price (95.5% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-95-5-acc-mobile-price\n#### \ud83d\udd0eEDA & Modelling\ud83d\udd2e - \ud83e\ude7a\ud83e\udde0 Stroke (74% acc.) : https:\/\/www.kaggle.com\/dorianvoydie\/eda-modelling-stroke-74-acc","e1993001":"## Part 2 - RandomForest","8505be7a":"# Data preprocessing\n\n## Aim :\n- Deal with NaN\n- Encode qualitative features\n- Develop our first modelling strategy","78e5d7fb":"# Conclusion\n\nBy using RandomForest, we can reach accuracies on both targets up to 97% (+\/-0.65\u00b0C), which is truely decent.","77535149":"<h1><center>\ud83c\udf25\u26c5Weather Forecasting\ud83c\udf24\ud83c\udf08<\/center><\/h1>\n<h3><center> \ud83d\udcc8(Prediction at the end)\ud83d\udd2e<\/center><\/h3>\n<center><img src= \"https:\/\/store-images.s-microsoft.com\/image\/apps.16894.c02476d2-2378-4f60-8290-b6d4b3842643.79a2ef6a-4775-4c79-8d93-9caf077660eb.1bbd88a4-0a17-4750-91a0-cd7d98f58e9d\" alt =\"Titanic\" style='width: 600px;'><\/center>\n\n<h3>Context<\/h3>\n<p>\n\nWeather forecasting is the application of science and technology to predict the conditions of the atmosphere for a given location and time. People have attempted to predict the weather informally for millennia and formally since the 19th century. Weather forecasts are made by collecting quantitative data about the current state of the atmosphere, land, and ocean and using meteorology to project how the atmosphere will change at a given place.\n\nOnce calculated manually based mainly upon changes in barometric pressure, current weather conditions, and sky condition or cloud cover, weather forecasting now relies on computer-based models that take many atmospheric factors into account. Human input is still required to pick the best possible forecast model to base the forecast upon, which involves pattern recognition skills, teleconnections, knowledge of model performance, and knowledge of model biases. The inaccuracy of forecasting is due to the chaotic nature of the atmosphere, the massive computational power required to solve the equations that describe the atmosphere, the land, and the ocean, the error involved in measuring the initial conditions, and an incomplete understanding of atmospheric and related processes. Hence, forecasts become less accurate as the difference between current time and the time for which the forecast is being made (the range of the forecast) increases. The use of ensembles and model consensus help narrow the error and provide confidence level in the forecast.\n\nThere is a vast variety of end uses to weather forecasts. Weather warnings are important forecasts because they are used to protect life and property. Forecasts based on temperature and precipitation are important to agriculture, and therefore to traders within commodity markets. Temperature forecasts are used by utility companies to estimate demand over coming days. On an everyday basis, many use weather forecasts to determine what to wear on a given day. Since outdoor activities are severely curtailed by heavy rain, snow and wind chill, forecasts can be used to plan activities around these events, and to plan ahead and survive them.\n\nWeather forecasting is a part of the economy, for example, in 2009, the US spent approximately 5.1 billion $ on weather forecasting, producing benefits estimated at six times as much\n\n<\/p>\n\n"}}