{"cell_type":{"24c92180":"code","2c3fba53":"code","acc6ef39":"code","5cb833da":"code","9cd2c1d9":"code","31a4cdce":"code","390f4161":"code","03e830a5":"code","ca5c99ea":"code","62b9bed1":"code","14fc4738":"code","cec13609":"code","72bc126c":"code","3077de65":"code","3cbc4d09":"code","a030269b":"code","4ed6168e":"code","a8d7ac78":"code","8e1a58d7":"code","ccf0ec8e":"code","30c07293":"code","39be9bb4":"code","157d5a18":"code","7767085b":"code","51225c8a":"code","4390f69a":"code","1dc0add9":"code","1203c92e":"code","6d6353f6":"code","5a324d13":"code","2228a74a":"code","ec8274f7":"code","d400c111":"code","0aabb3b4":"code","8b9895ae":"code","08f03aa2":"code","dde27578":"markdown","b5fbb6d0":"markdown","a992429b":"markdown","8451be74":"markdown","2b622553":"markdown","a28fd56f":"markdown","63aa0a36":"markdown","c4b822b8":"markdown","da431453":"markdown","81959003":"markdown","c104cb3a":"markdown","d6b0645c":"markdown","f0bb2fe3":"markdown","77a1f5f0":"markdown","6032f268":"markdown","f7bc2cb8":"markdown","a13d1e6c":"markdown","b4d5932c":"markdown","7348acc3":"markdown","f48827f1":"markdown","e8be8fea":"markdown","89160999":"markdown","a690afa5":"markdown"},"source":{"24c92180":"import numpy as np \nimport pandas as pd\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n\n\ntraining = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","2c3fba53":"# we are including train_test column so that we can divide the training and test set from the whole_df later\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nwhole_df = pd.concat([training,test])","acc6ef39":"# look at the datatypes and null values\ntraining.info()","5cb833da":"test.info()","9cd2c1d9":"# understand better about the numeric data\n\ntraining.describe()","31a4cdce":"# print which columns are numeric \n\ntraining.select_dtypes(include=['int','float']).columns","390f4161":"training.shape","03e830a5":"test.shape","ca5c99ea":"# lets use pairplot and see how the data is distributed and the blue and orange colour represents the survived data\n\ndf_plot = training[training.describe().columns]\nsns.pairplot(df_plot)","62b9bed1":"sns.barplot(training['Cabin'].value_counts().index,training['Cabin'].value_counts())","14fc4738":"sns.barplot(training['Ticket'].value_counts().index,training['Ticket'].value_counts())","cec13609":"# create all categorical variables to whole_df \n# later we can divide the training and testing set with the help of train_test columns\n\nwhole_df['cabin_multiple'] = whole_df.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nwhole_df['cabin_adv'] = whole_df.Cabin.apply(lambda x: str(x)[0])\nwhole_df['numeric_ticket'] = whole_df.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nwhole_df['ticket_letters'] = whole_df.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nwhole_df['name_title'] = whole_df.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())","72bc126c":"# filling null values with median\nwhole_df.Age = whole_df.Age.fillna(training.Age.median())\nwhole_df.Fare = whole_df.Fare.fillna(training.Fare.median())\n\n# drop null. 2 instances of this in training and 0 in test \nwhole_df.dropna(subset=['Embarked'],inplace = True)\n\n# The reason we are applying log to the columns below because, after we applied log the dustribution was changed near to normal distribution\nwhole_df['norm_sibsp'] = np.log(whole_df.SibSp+1)\n\n# log norm of fare (used)\nwhole_df['norm_fare'] = np.log(whole_df.Fare+1)\n","3077de65":"# converted fare to category for pd.get_dummies()\n\nwhole_df.Pclass = whole_df.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nwhole_dummies = pd.get_dummies(whole_df[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv','cabin_multiple','numeric_ticket','name_title','train_test']])\n","3cbc4d09":"#Split to train test again\n\nX_train = whole_dummies[whole_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = whole_dummies[whole_dummies.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = whole_df[whole_df.train_test==1].Survived","a030269b":"# Scale data \n\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\n\nwhole_dummies_scaled = whole_dummies.copy()\nwhole_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(whole_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nwhole_dummies_scaled\n\nX_train_scale = whole_dummies_scaled[whole_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scale = whole_dummies_scaled[whole_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = whole_df[whole_df.train_test==1].Survived","4ed6168e":"X_train.columns","a8d7ac78":"X_test.columns","8e1a58d7":"y_train.shape","ccf0ec8e":"X_train_scale.shape","30c07293":"X_test_scale.shape","39be9bb4":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC","157d5a18":"gnb = GaussianNB()\nlr = LogisticRegression(max_iter = 2000)\ndt = tree.DecisionTreeClassifier(random_state = 1)\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier(random_state = 1)\nsvc = SVC(probability = True)\nxgb = XGBClassifier(random_state =1)\n\nmodel_list = [gnb,lr,dt,knn,rf,svc,xgb]\nfor model in model_list:\n    print('\\n')\n    print(model)\n    cv = cross_val_score(model,X_train,y_train,cv=10)\n    print(cv)\n    print(cv.mean())","7767085b":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV \n\n# create a function that automatically returns best score and best hyperparameters\ndef performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","51225c8a":"\nlr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scale,y_train)\nperformance(best_clf_lr,'Logistic Regression')","4390f69a":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scale,y_train)\nperformance(best_clf_knn,'KNN')","1dc0add9":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scale,y_train)\nperformance(best_clf_svc,'SVC')","1203c92e":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n\nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scale,y_train)\nperformance(best_clf_rf,'Random Forest')","6d6353f6":"# doing grid search on all these parameters takes lots of time so i did random search\n# based on the result we  will do gridsearch on limited parameters \n\nxgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\n\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scale,y_train)\nperformance(best_clf_xgb_rnd,'XGB')","5a324d13":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [500,550,600,650],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65,0.9],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train_scale,y_train)\nperformance(best_clf_xgb,'XGB')","2228a74a":"# the best parameters we got from grid search is \nbest_clf_xgb.best_estimator_","ec8274f7":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_","d400c111":"# here we are trying all the combinations of models using both hard and soft classifier and see which one is performing better \n\nfrom itertools import combinations\nfrom sklearn.ensemble import VotingClassifier\ncomb_list = []\nhard = []\nsoft = []\nlist_models = [('lr',best_lr),('knn',best_knn),('svc',best_svc),('rf',best_rf),('xgb',best_xgb)]\nfor r in range(3,6):\n    soft_comb = combinations(list_models,r)\n    for i in soft_comb:\n        i = list(i)\n        models_list1 = []\n        for k in i:\n            models_list1.append(k[0])\n        print('\\n')\n        print('Voting classifier for {}'.format(models_list1))\n        comb_list.append(models_list1)\n        vc = VotingClassifier(estimators = i, voting = 'hard') \n#       print('voting_hard :',cross_val_score(vc,X_train,y_train,cv=5))\n        print('voting_hard mean :',cross_val_score(vc,X_train,y_train,cv=5).mean())\n        hard.append(cross_val_score(vc,X_train,y_train,cv=5).mean())\n        vc1 = VotingClassifier(estimators = i, voting = 'soft')\n#       print('voting_soft :',cross_val_score(vc1,X_train,y_train,cv=5))\n        print('voting_soft mean :',cross_val_score(vc1,X_train,y_train,cv=5).mean())\n        soft.append(cross_val_score(vc1,X_train,y_train,cv=5).mean())","0aabb3b4":"comb_list1 = comb_list\nhard1 = hard\nsoft1 = soft ","8b9895ae":"comb_list1 = [str(x) for x in comb_list1]\ncomb_list1 = pd.Series(comb_list1)\ndf = pd.DataFrame([hard,soft],columns=comb_list1)\ndf = df.T\ndf = df.rename(columns = {0:'hard',1:'soft'})\ndf = df.sort_values('soft',ascending = False)\ndf","08f03aa2":"list_models = [('lr',best_lr),('knn',best_knn),('svc',best_svc),('rf',best_rf),('xgb',best_xgb)]\nfor r in range(3,6):\n    soft_comb = combinations(list_models,r)\n    for i in soft_comb:\n        i = list(i)\n        models_list1 = []\n        for k in i:\n            models_list1.append(k[0])\n            \n        # hard voting classifier\n        \n        vc = VotingClassifier(estimators = i, voting = 'hard') \n        vc.fit(X_train_scale, y_train)\n        predictions_hard = vc.predict(X_test_scale).astype(int)\n        final_data_hard = {'PassengerId': test.PassengerId, 'Survived': predictions_hard}\n        submission_hard = pd.DataFrame(data=final_data_hard)\n        filename = ''\n        for k in range(len(i)):\n            filename+=(i[k][0]+'_')\n            \n        submission_hard.to_csv('hard_{}.csv'.format(filename),index=False)\n        \n        # soft voting classifier\n        \n        vc = VotingClassifier(estimators = i, voting = 'soft') \n        vc.fit(X_train_scale, y_train)\n        predictions_soft = vc.predict(X_test_scale).astype(int)\n        final_data_soft = {'PassengerId': test.PassengerId, 'Survived': predictions_soft}\n        submission_soft = pd.DataFrame(data=final_data_soft)\n        filename = ''\n        for k in range(len(i)):\n            filename+=(i[k][0]+'_')\n            \n        submission_soft.to_csv('soft_{}.csv'.format(filename),index=False)","dde27578":"### XGB classifier hyperparameters \n\n* n_estimators = no of trees created in XGB\n\n* colsample_bytree =  percentage of columns you want to select from a tree for helping overfitting and speeding up the process\n\n* max_depth = depth of each tree \n\n* alpha = learning rate ( used when getting the predicted values )\n\n* lambda = regularization parameter \n\n* gamma = it is a user defined penality (it encourages pruning the trees)\n\n* min_child_weight = For regression, that is the minimum number of observations that go to a leaf. For classification, it is the minimum of the hessian","b5fbb6d0":"### Random Forest classifier hyperparameters\n\n* n_esitmators = no of trees created while bagging\n\n* max_features = max number of features considered for splitting the node\n\n* max_depth = max number of levels in the each decision trees\n\n* min_samples_split = minimum no of data points allowed in a leaf node \n\n* boot strap = method for sampling data ( with or without replacement)\n","a992429b":"### Even though we are getting 80% + accuracy ,there will minor overfitting with these models\n### we will write a code which saves all predictions got by the above model\n### later we will randomly select and submit the predictions to kaggle ","8451be74":"### In this notebook we are going to analyze the given titanic dataset, apply feature engineering and use different machine learning models with advanced hyperparameter techniques.\n\n### We will also understand what each hyperparamter in models means.","2b622553":"### **Grid Search hyperparameters**\n\n* verbose = watch the performance of the already-tried combinations of parameters during the execution ( the more number u give the more details u get ) \n\n* n_jobs = no of jobs to run in parallel (-1 means the execution uses all the parallel processors for speeding up the excecution)\n \n* cv = cross validation ( 3 means the data is folded into 3 partitions and in each fold a test and train set it taken, at last it finds the average accuracy of all the fold ) \n\n### Randomized search hyperparameters\n\n* n_iters =  no of iterations for trying the different random combinations of hyperparameters\n \n* verbose = watch the performance of the already-tried combinations of parameters during the execution ( the more number u give the more details u get ) \n\n* n_jobs = no of jobs to run in parallel (-1 means the execution uses all the parallel processor for speeding up the excecution)\n\n* cv = cross validation ( 3 means the data is folded into 3 partitions and in each fold a test and train set it taken, at last it finds the average accuracy of all the fold )","a28fd56f":"## Visualize Insights of the Categorical Data and Overall Distribution of numerical Data","63aa0a36":"## Convert the above results to a dataframe so that we can analyze better","c4b822b8":"### Lets try the all the combinations by using soft voting classsifier and hard voting classifier ","da431453":" As you can see above the columns ticket and cabin needs to be processed properly ","81959003":"> #### The above results are without hyperparameters and in them logistic regression performs the best ","c104cb3a":"### KNeighborsClassifier hyperparameters \n\n#### The main parameters of the class sklearn.neighbors.KNeighborsClassifier are:\n\n* weights = uniform (all weights are equal), distance (the weight is inversely proportional to the distance from the test sample), or any other user-defined function\n\n* algorithm (optional) = brute, ball_tree, KD_tree, or auto. In the first case, the nearest neighbors for each test case are computed by a grid search over the training set. In the second and third cases, the distances between the examples are stored in a tree to accelerate finding nearest neighbors. If you set this parameter to auto, the right way to find the neighbors will be automatically chosen based on the training set.\n\n* leaf_size (optional) = threshold for switching to grid search if the algorithm for finding neighbors is BallTree or KDTree;\n* metric: minkowski, manhattan, euclidean, chebyshev, or other.\n\n* p = Power parameter for the Minkowski metric. When p = 1, this is\n* equivalent to using manhattan_distance (l1), and euclidean_distance\n* (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.\n\n* n_neighbors = no of neighbors to take (default = 5)","d6b0645c":"## Feature Engineering","f0bb2fe3":"## The below are the algorithms we are going to apply : \n\n* Native Bayes theorm\n\n* Logistic Regression \n\n* Decision Tree\n\n* Knearest Neighbors\n\n* Random Forest \n\n* Support Vector Classifier\n\n* XGBoost ","77a1f5f0":"### Now you will have all the submissions No need to try all the submissions because there are many, just choose randomly by analyzing the below table\n   \n### I got 0.79186 with soft voting with the combination of knn,rf,svc. The file name of submission is (soft_knn_svc_rf_.csv)\n\n### This is my first notebook in kaggle , so if there are any possible improvments please comment below\n\n### Will include more visualizations in future ","6032f268":"### SVC hyperparameters\n\n#### kernel in svc is used to tranform the data into higher dimensions and apply svc on the transformed data","f7bc2cb8":"# Titanic: Machine Learning from Disaster","a13d1e6c":"### If you like the kernel, please UpVote It! which motivates me to upload more notebooks\n\n## Let's start playing","b4d5932c":"### As you can see above the data is spreaded in such a way that we cannot use linear models , so we will eliminate using linear regression and try random forest , logistic regression , knn and xgboost","7348acc3":"### Lets use soft voting classifier and hard voting classifier ","f48827f1":"\n## Read below to understand the hyperparameters of gridsearch and randomized search ","e8be8fea":"## If you have any doubts please do comment and if you like please upvote :)","89160999":"### *Top score achieved by this approach is 0.79186 ( Top 8% )*\n![bhh.JPG](attachment:bhh.JPG)","a690afa5":"## Whole Process in detail:\n\n* Importing both train and test data and combing the data into a single dataframe so that when can make change to both the dataframes at once and later we can split them \n\n* visualizing and selecting the columns which need processing \n\n* visualizing to see how the data is distributed and deciding which machine learning algorithms to use\n\n* Converting some columns to categorical and other feature engineering stuff\n\n* scaling data\n\n* Applying different machine learning models without hyperparameters\n\n* Applying different machine learning models with hyperparameters\n\n* Using soft voting classifier,hard voting classifer and predicting the output with all the combinations of different models\n\n* Choosing the best predictions out of all combinations used with soft and hard voting classifier"}}