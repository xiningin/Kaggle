{"cell_type":{"a804d9a3":"code","2deb9b3e":"code","375ff3ea":"code","b74d5eff":"code","1243d47c":"code","f80586ae":"code","d1ccc370":"code","bf51f051":"code","c536a6c4":"code","7fc334ad":"code","aff60923":"code","f8caa5b7":"markdown","56742884":"markdown","6ebf0bfb":"markdown","8e416993":"markdown","07f54583":"markdown","c9032167":"markdown","dd892e80":"markdown","098bb57d":"markdown","ee6d8aa9":"markdown"},"source":{"a804d9a3":"#\u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0438 \u0438\u043c\u043f\u043e\u0440\u0442 \u0432\u0441\u0435\u0445 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432\n!pip install --quiet \/kaggle\/input\/kerasapplications\n!pip install --quiet \/kaggle\/input\/efficientnet-git","2deb9b3e":"import math, os, re, warnings, random, glob\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras import Sequential, Model\nimport efficientnet.tfkeras as efn\n\n#\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0441\u0438\u0434\u0430 \u0438 \u0435\u0433\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435\ndef seed_everything(seed=0):\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n\nseed = 0\nseed_everything(seed)\nwarnings.filterwarnings('ignore')\n","375ff3ea":"#\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 TPU \u0438\u043b\u0438 GPU.\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(f'Running on TPU {tpu.master()}')\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync\nprint(f'REPLICAS: {REPLICAS}')","b74d5eff":"#\u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438\nBATCH_SIZE = 32 * REPLICAS\nHEIGHT = 512\nWIDTH = 512 \nCHANNELS = 3\nN_CLASSES = 5\n#TTA (Test Time Augmentation) - \u0412\u043c\u0435\u0441\u0442\u043e \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e\u0442\u0441\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043a\u043e\u043f\u0438\u0439 \u0441 \u0440\u0430\u0437\u043d\u044b\u043c\u0438 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f\u043c\u0438\n#\u041a\u043b\u0430\u0441\u0441 \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u043e\u043f\u0438\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0438 \u0432\u044b\u0431\u0438\u0440\u0430\u0435\u0442\u0441\u044f \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0447\u0430\u0441\u0442\u044b\u0439 \u043e\u0442\u0432\u0435\u0442\nTTA_STEPS = 8 # Do TTA if > 0","1243d47c":"#\u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u0438\u0437 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430\n\ndef data_augment(image, label):\n    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_1 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_2 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_pixel_3 = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)\n            \n    # Flips\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    if p_spatial > .75:\n        image = tf.image.transpose(image)\n        \n    # Rotates\n    if p_rotate > .75:\n        image = tf.image.rot90(image, k=3) # rotate 270\u00ba\n    elif p_rotate > .5:\n        image = tf.image.rot90(image, k=2) # rotate 180\u00ba\n    elif p_rotate > .25:\n        image = tf.image.rot90(image, k=1) # rotate 90\u00ba\n        \n    # Pixel-level transforms\n    if p_pixel_1 >= .4:\n        image = tf.image.random_saturation(image, lower=.7, upper=1.3)\n    if p_pixel_2 >= .4:\n        image = tf.image.random_contrast(image, lower=.8, upper=1.2)\n    if p_pixel_3 >= .4:\n        image = tf.image.random_brightness(image, max_delta=.1)\n        \n    # Crops\n    if p_crop > .7:\n        if p_crop > .9:\n            image = tf.image.central_crop(image, central_fraction=.7)\n        elif p_crop > .8:\n            image = tf.image.central_crop(image, central_fraction=.8)\n        else:\n            image = tf.image.central_crop(image, central_fraction=.9)\n    elif p_crop > .4:\n        crop_size = tf.random.uniform([], int(HEIGHT*.8), HEIGHT, dtype=tf.int32)\n        image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n        \n#     # Crops\n#     if p_crop > .6:\n#         if p_crop > .9:\n#             image = tf.image.central_crop(image, central_fraction=.5)\n#         elif p_crop > .8:\n#             image = tf.image.central_crop(image, central_fraction=.6)\n#         elif p_crop > .7:\n#             image = tf.image.central_crop(image, central_fraction=.7)\n#         else:\n#             image = tf.image.central_crop(image, central_fraction=.8)\n#     elif p_crop > .3:\n#         crop_size = tf.random.uniform([], int(HEIGHT*.6), HEIGHT, dtype=tf.int32)\n#         image = tf.image.random_crop(image, size=[crop_size, crop_size, CHANNELS])\n            \n\n    return image, label","f80586ae":"# Datasets utility functions\n#\u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438\n\ndef get_name(file_path):\n    parts = tf.strings.split(file_path, os.path.sep)\n    name = parts[-1]\n    return name\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    \n#     image = center_crop(image)\n    return image\n\ndef center_crop(image):\n    image = tf.reshape(image, [600, 800, CHANNELS]) # Original shape\n    \n    h, w = image.shape[0], image.shape[1]\n    if h > w:\n        image = tf.image.crop_to_bounding_box(image, (h - w) \/\/ 2, 0, w, w)\n    else:\n        image = tf.image.crop_to_bounding_box(image, 0, (w - h) \/\/ 2, h, h)\n        \n    image = tf.image.resize(image, [HEIGHT, WIDTH]) # Expected shape\n    return image\n\ndef resize_image(image, label):\n    image = tf.image.resize(image, [HEIGHT, WIDTH])\n    image = tf.reshape(image, [HEIGHT, WIDTH, CHANNELS])\n    return image, label\n\ndef process_path(file_path):\n    name = get_name(file_path)\n    img = tf.io.read_file(file_path)\n    img = decode_image(img)\n    return img, name\n\ndef get_dataset(files_path, shuffled=False, tta=False, extension='jpg'):\n    dataset = tf.data.Dataset.list_files(f'{files_path}*{extension}', shuffle=shuffled)\n    dataset = dataset.map(process_path, num_parallel_calls=AUTO)\n    if tta:\n        dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.map(resize_image, num_parallel_calls=AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","d1ccc370":"#\u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432 \u0438 \u043c\u043e\u0434\u0435\u043b\u0435\u0439\n\ndatabase_base_path = '\/kaggle\/input\/cassava-leaf-disease-classification\/'\nsubmission = pd.read_csv(f'{database_base_path}sample_submission.csv')\ndisplay(submission.head())\n\nTEST_FILENAMES = tf.io.gfile.glob(f'{database_base_path}test_tfrecords\/ld_test*.tfrec')\nNUM_TEST_IMAGES = count_data_items(TEST_FILENAMES)\nprint(f'GCS: test: {NUM_TEST_IMAGES}')","bf51f051":"model_path_list = glob.glob('\/kaggle\/input\/cassava-leaf-disease-training-with-tpu-v2-pods\/*.h5')\nmodel_path_list.sort()\n\nprint('Models to predict:')\nprint(*model_path_list, sep='\\n')","c536a6c4":"#\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\n\ndef model_fn(input_shape, N_CLASSES):\n    inputs = L.Input(shape=input_shape, name='input_image')\n    base_model = efn.EfficientNetB4(input_tensor=inputs, \n                                    include_top=False, \n                                    weights=None, \n                                    pooling='avg')\n\n    x = L.Dropout(.5)(base_model.output)\n    output = L.Dense(N_CLASSES, activation='softmax', name='output')(x)\n    model = Model(inputs=inputs, outputs=output)\n\n    return model\n\nwith strategy.scope():\n    model = model_fn((None, None, CHANNELS), N_CLASSES)\n    \nmodel.summary()\n","7fc334ad":"#\u0410\u043d\u0441\u0430\u043c\u0431\u043b\u044c \u0438\u0437 5 \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439\n\nfiles_path = f'{database_base_path}test_images\/'\ntest_size = len(os.listdir(files_path))\ntest_preds = np.zeros((test_size, N_CLASSES))\n\n\nfor model_path in model_path_list:\n    print(model_path)\n    K.clear_session()\n    model.load_weights(model_path)\n\n    if TTA_STEPS > 0:\n        test_ds = get_dataset(files_path, tta=True).repeat()\n        ct_steps = TTA_STEPS * ((test_size\/BATCH_SIZE) + 1)\n        preds = model.predict(test_ds, steps=ct_steps, verbose=1)[:(test_size * TTA_STEPS)]\n        preds = np.mean(preds.reshape(test_size, TTA_STEPS, N_CLASSES, order='F'), axis=1)\n        test_preds += preds \/ len(model_path_list)\n    else:\n        test_ds = get_dataset(files_path, tta=False)\n        x_test = test_ds.map(lambda image, image_name: image)\n        test_preds += model.predict(x_test) \/ len(model_path_list)\n    \ntest_preds = np.argmax(test_preds, axis=-1)\ntest_names_ds = get_dataset(files_path)\nimage_names = [img_name.numpy().decode('utf-8') for img, img_name in iter(test_names_ds.unbatch())]","aff60923":"submission = pd.DataFrame({'image_id': image_names, 'label': test_preds})\nsubmission.to_csv('submission.csv', index=False)\ndisplay(submission.head())","f8caa5b7":"### Hardware configuration","56742884":"<center><img src=\"https:\/\/raw.githubusercontent.com\/dimitreOliveira\/MachineLearning\/master\/Kaggle\/Cassava%20Leaf%20Disease%20Classification\/banner.png\" width=\"1000\"><\/center>\n<br>\n<center><h1>Cassava Leaf Disease - TPU v2 Pods - Inference<\/h1><\/center>\n<br>\n\n- This is the inference part of the work, the training notebook can be found here [Cassava Leaf Disease - Training with TPU v2 Pods](https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-disease-training-with-tpu-v2-pods)\n- keras-applications GitHub repository can be found [here](https:\/\/www.kaggle.com\/dimitreoliveira\/kerasapplications)\n- efficientnet GitHub repository can be found [here](https:\/\/www.kaggle.com\/dimitreoliveira\/efficientnet-git)\n- Dataset source `center cropped` [512x512](https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-disease-50-tfrecords-center-512x512)\n- Dataset source `external data` `center cropped` [512x512](https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-disease-50-tfrecords-external-512x512)\n- Dataset source [discussion thread](https:\/\/www.kaggle.com\/c\/cassava-leaf-disease-classification\/discussion\/198744)\n- Dataset [creation source](https:\/\/www.kaggle.com\/dimitreoliveira\/cassava-leaf-disease-stratified-tfrecords-256x256)\n- Model experiments [discussion thread](https:\/\/www.kaggle.com\/c\/cassava-leaf-disease-classification\/discussion\/203594)","6ebf0bfb":"# \u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f","8e416993":"## \u0417\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438","07f54583":"# \u041f\u0440\u043e\u0433\u043d\u043e\u0437","c9032167":"## \u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438","dd892e80":"# \u041c\u043e\u0434\u0435\u043b\u044c","098bb57d":"# \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","ee6d8aa9":"# \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438"}}