{"cell_type":{"ec94893a":"code","11b4f937":"code","8bb28aed":"code","8dc66466":"code","57c8ccba":"code","3b0387fc":"code","45e95852":"code","deb779c6":"code","47fa9e75":"code","eba5f9f1":"code","3fce44b1":"code","3160301c":"code","aefac7d8":"code","1fc828a7":"code","d49daf40":"code","a3366d82":"code","37fde273":"code","488f0e76":"code","b3935449":"code","177b14bc":"code","226b699b":"code","4c710558":"code","9fd8ce48":"code","85a08f4e":"code","0a563f08":"code","0ad3222e":"code","0e021d95":"code","45daeae3":"code","7ce372c1":"code","0ce9e148":"code","568e8b64":"code","ee439796":"code","fcc04cfc":"code","401315b4":"code","b04a3dee":"code","07956f97":"code","9d341f84":"code","4a2c30d8":"code","1c775181":"code","705b2981":"code","3daf3cc7":"code","26123f30":"code","b18bc2fe":"markdown","c0774649":"markdown","730964db":"markdown","c9391380":"markdown","a3be1131":"markdown","44358f67":"markdown","3c8d7c68":"markdown","21897c5f":"markdown","f4da0e7a":"markdown","80e24829":"markdown","f77705a7":"markdown","2ba4cc60":"markdown","0f18466c":"markdown","ba3ab667":"markdown","3aa90e5a":"markdown","279e5710":"markdown","7aab89d4":"markdown","1959f86a":"markdown","475b5aa9":"markdown","1233dcd6":"markdown","2a11b047":"markdown","48db3d50":"markdown","0728be51":"markdown","33f4743e":"markdown","1165d10f":"markdown","9a91dd14":"markdown","c4e6378f":"markdown","b78675cd":"markdown","c50d5fe5":"markdown","eb723e0f":"markdown","346fd374":"markdown","99c37d7c":"markdown","3e7f41fe":"markdown","ed99218b":"markdown","ee0b771d":"markdown","bebd68c2":"markdown","a2aad670":"markdown","26466591":"markdown","56119edb":"markdown","8440313d":"markdown","b5b2258e":"markdown","801b317c":"markdown","da82a1b0":"markdown","7e45a3d8":"markdown","85e5f108":"markdown","2855f3d7":"markdown"},"source":{"ec94893a":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport math\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')","11b4f937":"filename = '..\/input\/pune_1965_to_2002.csv'\nSTORAGE_FOLDER = ''","8bb28aed":"rainfall_data_matrix = pd.read_csv(filename)\nrainfall_data_matrix.set_index('Year', inplace=True)\nrainfall_data_matrix = rainfall_data_matrix.transpose()\nrainfall_data_matrix","8dc66466":"dates = pd.date_range(start='1965-01', freq='MS', periods=len(rainfall_data_matrix.columns)*12)\ndates","57c8ccba":"plt.figure(figsize=(13,7))\nplt.plot(rainfall_data_matrix)\nplt.xlabel('Year')\nplt.ylabel('Precipitation(mm)')\nplt.title('Month vs Precipitation across all years')","3b0387fc":"plt.figure(figsize=(10,5))\nplt.boxplot(rainfall_data_matrix)\nplt.xlabel('Month')\nplt.ylabel('Precipitation(mm)')\nplt.title('Month vs Precipitation across all years')","45e95852":"rainfall_data_matrix_np = rainfall_data_matrix.transpose().as_matrix()\n\nshape = rainfall_data_matrix_np.shape\nrainfall_data_matrix_np = rainfall_data_matrix_np.reshape((shape[0] * shape[1], 1))","deb779c6":"rainfall_data = pd.DataFrame({'Precipitation': rainfall_data_matrix_np[:,0]})\nrainfall_data.set_index(dates, inplace=True)\n\ntest_rainfall_data = rainfall_data.ix['1995': '2002']\nrainfall_data = rainfall_data.ix[: '1994']\nrainfall_data = rainfall_data.round(5)\nrainfall_data.head()","47fa9e75":"rainfall_data.shape","eba5f9f1":"test_rainfall_data.shape","3fce44b1":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaler.fit(rainfall_data)","3160301c":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\ndef root_mean_squared_error(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    return rmse","aefac7d8":"def calculate_performance(y_true, y_pred):\n    mse = mean_squared_error(y_true, y_pred)\n    mae = mean_absolute_error(y_true, y_pred)\n    mape = mean_absolute_percentage_error(y_true, y_pred)\n    rmse = root_mean_squared_error(y_true, y_pred)\n    return round(mse, 3), round(mae, 3), round(mape, 3), round(rmse, 3)","1fc828a7":"def plot_keras_model(model, show_shapes=True, show_layer_names=True):\n    return SVG(model_to_dot(model, show_shapes=show_shapes, show_layer_names=show_layer_names).create(prog='dot',format='svg'))","d49daf40":"def get_combinations(parameters):\n    return list(itertools.product(*parameters))","a3366d82":"def create_NN(input_nodes, hidden_nodes, output_nodes):\n    model = Sequential()\n    model.add(Dense(int(hidden_nodes), input_dim=int(input_nodes)))\n    model.add(Dense(int(output_nodes)))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","37fde273":"def train_model(model, X_train, y_train, epochs, batch_size):\n    model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0, shuffle=True)\n    return model","488f0e76":"def reshape_arrays(X_train, y_train):\n    X_train = np.array(X_train)\n    y_train = np.reshape(y_train, (len(y_train), 1))\n    return X_train, y_train","b3935449":"def preprocess_FNN(data, look_back):\n    data = np.array(data)[:, 0]\n    X_train = []\n    y_train = []\n    for i in range(data.shape[0]-look_back):\n        x = data[i:look_back+i][::-1]\n        y = data[look_back+i]\n        X_train.append(list(x))\n        y_train.append(y)\n    input_seq_for_test = data[i+1:look_back+i+1][::-1]\n    return X_train, y_train, input_seq_for_test","177b14bc":"def forecast_FNN(model, input_sequence, future_steps):\n    forecasted_values = []\n    for i in range(future_steps):\n        forecasted_value = model.predict(input_sequence)\n        forecasted_values.append(forecasted_value[0][0])\n        input_sequence[0] = np.append(forecasted_value, input_sequence[0][:-1])\n    return forecasted_values","226b699b":"def FNN(data, look_back, hidden_nodes, output_nodes, epochs, batch_size, future_steps, scaler):\n    data = scaler.transform(data)\n    X_train, y_train, input_seq_for_test_FNN = preprocess_FNN(data, look_back)\n    X_train, y_train = reshape_arrays(X_train, y_train)\n\n    model_FNN = create_NN(input_nodes=look_back, hidden_nodes=hidden_nodes, output_nodes=output_nodes)\n    model_FNN = train_model(model_FNN, X_train, y_train, epochs, batch_size)\n\n    input_seq_for_test_FNN = np.reshape(input_seq_for_test_FNN, (1, len(input_seq_for_test_FNN)))\n    forecasted_values_FNN = forecast_FNN(model_FNN, input_sequence=input_seq_for_test_FNN, future_steps=future_steps)\n    \n    forecasted_values_FNN = list(scaler.inverse_transform([forecasted_values_FNN])[0])\n    \n    return model_FNN, forecasted_values_FNN","4c710558":"def get_accuracies_FNN(rainfall_data, test_rainfall_data, parameters, scaler):\n    combination_of_params = get_combinations(parameters)\n    information_FNN = []\n    iterator = 0\n    print('FNN - Number of combinations: ' + str(len(combination_of_params)))\n    \n    for param in combination_of_params:\n        if (iterator+1) != len(combination_of_params):\n            print(iterator+1, end=' -> ')\n        else:\n            print(iterator+1)\n        iterator = iterator+1\n\n        look_back = param[0]\n        hidden_nodes = param[1]\n        output_nodes = param[2]\n        epochs = param[3]\n        batch_size = param[4]\n        future_steps = param[5]\n\n        model_FNN, forecasted_values_FNN = FNN(rainfall_data, look_back, hidden_nodes, output_nodes, epochs, batch_size, future_steps, scaler)\n        \n        y_true = test_rainfall_data.ix[:future_steps].Precipitation\n        mse, mae, mape, rmse = calculate_performance(y_true, forecasted_values_FNN)\n        \n        info = list(param) + [mse, mae, rmse] + forecasted_values_FNN\n        information_FNN.append(info)\n\n    information_FNN_df = pd.DataFrame(information_FNN)\n    indexes = [str(i) for i in list(range(1, future_steps+1))]\n    information_FNN_df.columns = ['look_back', 'hidden_nodes', 'output_nodes', 'epochs', 'batch_size', 'future_steps', 'MSE', 'MAE', 'RMSE'] + indexes\n    return information_FNN_df","9fd8ce48":"def preprocess_TLNN(data, time_lagged_points):\n    data = np.array(data)[:, 0]\n    X_train = []\n    y_train = []\n    for i in range(max(time_lagged_points), data.shape[0]):\n        x = [data[i-p] for p in time_lagged_points]\n        y = data[i]\n        X_train.append(list(x))\n        y_train.append(y)\n    input_seq_for_test = [data[i+1-p] for p in time_lagged_points]\n    return X_train, y_train, input_seq_for_test","85a08f4e":"def forecast_TLNN(model, time_lagged_points, last_sequence, future_steps):\n    forecasted_values = []\n    max_lag = max(time_lagged_points)\n    for i in range(future_steps):\n        input_sequence = [last_sequence[max_lag - p] for p in time_lagged_points]\n        forecasted_value = model.predict(np.reshape(input_sequence, (1, len(input_sequence))))\n        forecasted_values.append(forecasted_value[0][0])\n        last_sequence = last_sequence[1:] + [forecasted_value[0][0]]\n    return forecasted_values","0a563f08":"def TLNN(data, time_lagged_points, hidden_nodes, output_nodes, epochs, batch_size, future_steps, scaler):\n    data = scaler.transform(data)\n    X_train, y_train, input_seq_for_test_TLNN = preprocess_TLNN(data, time_lagged_points)\n    X_train, y_train = reshape_arrays(X_train, y_train)\n    model_TLNN = create_NN(input_nodes=len(time_lagged_points), hidden_nodes=hidden_nodes, output_nodes=output_nodes)\n    model_TLNN = train_model(model_TLNN, X_train, y_train, epochs, batch_size)\n\n    max_lag = max(time_lagged_points)\n    forecasted_values_TLNN = forecast_TLNN(model_TLNN, time_lagged_points, \n                                      list(data[-max_lag:]), future_steps=future_steps)\n    forecasted_values_TLNN = list(scaler.inverse_transform([forecasted_values_TLNN])[0])\n    \n    return model_TLNN, forecasted_values_TLNN","0ad3222e":"def get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters, scaler):\n    combination_of_params = get_combinations(parameters)\n    information_TLNN = []\n    iterator = 0\n    print('TLNN - Number of combinations: ' + str(len(combination_of_params)))\n    \n    for param in combination_of_params:\n        if (iterator+1) != len(combination_of_params):\n            print(iterator+1, end=' -> ')\n        else:\n            print(iterator+1)\n        iterator = iterator+1\n\n        time_lagged_points = param[0]\n        hidden_nodes = param[1]\n        output_nodes = param[2]\n        epochs = param[3]\n        batch_size = param[4]\n        future_steps = param[5]\n\n        model_TLNN, forecasted_values_TLNN = TLNN(rainfall_data, time_lagged_points, hidden_nodes, output_nodes, epochs, batch_size, future_steps, scaler)\n        \n        y_true = test_rainfall_data.ix[:future_steps].Precipitation\n        mse, mae, mape, rmse = calculate_performance(y_true, forecasted_values_TLNN)\n        \n        info = list(param) + [mse, mae, rmse] + forecasted_values_TLNN\n        information_TLNN.append(info)\n\n    information_TLNN_df = pd.DataFrame(information_TLNN)\n    indexes = [str(i) for i in list(range(1, future_steps+1))]\n    information_TLNN_df.columns = ['look_back_lags', 'hidden_nodes', 'output_nodes', 'epochs', 'batch_size', 'future_steps', 'MSE', 'MAE', 'RMSE'] + indexes\n    return information_TLNN_df","0e021d95":"def preprocess_SANN(data, seasonal_period):\n    data = np.array(data)[:, 0]\n    X_train = []\n    y_train = []\n    for i in range(seasonal_period, data.shape[0]-seasonal_period+1):\n        x = data[i-seasonal_period:i][::-1]\n        y = data[i:i+seasonal_period]\n        X_train.append(list(x))\n        y_train.append(list(y))\n    input_seq_for_test = data[i+1-seasonal_period:i+1][::-1]\n    return X_train, y_train, input_seq_for_test","45daeae3":"def forecast_SANN(model, input_sequence, seasonal_period, future_steps):\n    iterations = future_steps\/seasonal_period\n    forecasted_values = []\n    for i in range(int(iterations) + 1):\n        next_forecasted_values = model.predict(input_sequence)\n        forecasted_values += list(next_forecasted_values[0])\n        input_sequence = next_forecasted_values\n    return forecasted_values[:future_steps]","7ce372c1":"def SANN(data, seasonal_period, hidden_nodes, epochs, batch_size, future_steps, scaler):\n    data = scaler.transform(data)\n    X_train, y_train, input_seq_for_test_SANN = preprocess_SANN(data, seasonal_period)\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n\n    input_seq_for_test_SANN = np.reshape(input_seq_for_test_SANN, (1, len(input_seq_for_test_SANN)))\n    model_SANN = create_NN(input_nodes=seasonal_period, hidden_nodes=hidden_nodes, output_nodes=seasonal_period)\n    model_SANN = train_model(model_SANN, X_train, y_train, epochs, batch_size)\n    \n    forecasted_values_SANN = forecast_SANN(model_SANN, input_seq_for_test_SANN, seasonal_period, future_steps=future_steps)\n    forecasted_values_SANN = list(scaler.inverse_transform([forecasted_values_SANN])[0])\n    return model_SANN, forecasted_values_SANN","0ce9e148":"def get_accuracies_SANN(rainfall_data, test_rainfall_data, parameters, scaler):\n    combination_of_params = get_combinations(parameters)\n    information_SANN = []\n    iterator = 0\n    print('SANN - Number of combinations: ' + str(len(combination_of_params)))\n    \n    for param in combination_of_params:\n        if (iterator+1) != len(combination_of_params):\n            print(iterator+1, end=' -> ')\n        else:\n            print(iterator+1)\n        iterator = iterator+1\n\n        seasonal_period = param[0]\n        hidden_nodes = param[1]\n        epochs = param[2]\n        batch_size = param[3]\n        future_steps = param[4]\n\n        model_SANN, forecasted_values_SANN = SANN(rainfall_data, seasonal_period, hidden_nodes, epochs, batch_size, future_steps, scaler)\n        \n        y_true = test_rainfall_data.ix[:future_steps].Precipitation\n        mse, mae, mape, rmse = calculate_performance(y_true, forecasted_values_SANN)\n        \n        info = list(param) + [mse, mae, rmse] + forecasted_values_SANN\n        information_SANN.append(info)\n\n    information_SANN_df = pd.DataFrame(information_SANN)\n    indexes = [str(i) for i in list(range(1, future_steps+1))]\n    information_SANN_df.columns = ['seasonal_period', 'hidden_nodes', 'epochs', 'batch_size', 'future_steps', 'MSE', 'MAE', 'RMSE'] + indexes\n    return information_SANN_df","568e8b64":"def create_LSTM(input_nodes, hidden_nodes, output_nodes):\n    model = Sequential()\n    model.add(LSTM(hidden_nodes, input_shape=(1, input_nodes)))\n    model.add(Dense(output_nodes))\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model","ee439796":"def preprocess_LSTM(data, look_back):\n    data = np.array(data)[:, 0]\n    X_train = []\n    y_train = []\n    for i in range(data.shape[0]-look_back):\n        x = data[i:look_back+i][::-1]\n        y = data[look_back+i]\n        X_train.append(list(x))\n        y_train.append(y)\n    input_seq_for_test = data[i+1:look_back+i+1][::-1]\n    return X_train, y_train, input_seq_for_test","fcc04cfc":"def forecast_LSTM(model, input_sequence, future_steps):\n    forecasted_values = []\n    for i in range(future_steps):\n        forecasted_value = model.predict(input_sequence)\n        forecasted_values.append(forecasted_value[0][0])\n        input_sequence[0][0] = np.append(forecasted_value, input_sequence[0][0][:-1])\n    return forecasted_values","401315b4":"def Long_Short_Term_Memory(data, look_back, hidden_nodes, output_nodes, epochs, batch_size, future_steps, scaler):\n    data = scaler.transform(data)\n    X_train, y_train, input_seq_for_test_LSTM = preprocess_LSTM(data, look_back)\n    X_train = np.reshape(X_train, (len(X_train), 1, look_back))\n\n    model_LSTM = create_LSTM(input_nodes=look_back, hidden_nodes=hidden_nodes, output_nodes=output_nodes)\n    plot_keras_model(model_LSTM)\n    model_LSTM = train_model(model_LSTM, X_train, y_train, epochs, batch_size)\n\n    input_seq_for_test_LSTM = np.reshape(input_seq_for_test_LSTM, (1, 1, len(input_seq_for_test_LSTM)))\n    forecasted_values_LSTM = forecast_LSTM(model_LSTM, input_sequence=input_seq_for_test_LSTM, future_steps=future_steps)\n    \n    forecasted_values_LSTM = list(scaler.inverse_transform([forecasted_values_LSTM])[0])\n    \n    return model_LSTM, forecasted_values_LSTM","b04a3dee":"def get_accuracies_LSTM(rainfall_data, test_rainfall_data, parameters, scaler):\n    combination_of_params = get_combinations(parameters)\n    information_LSTM = []\n    iterator = 0\n    print('LSTM - Number of combinations: ' + str(len(combination_of_params)))\n    \n    for param in combination_of_params:\n        if (iterator+1) != len(combination_of_params):\n            print(iterator+1, end=' -> ')\n        else:\n            print(iterator+1)\n        iterator = iterator+1\n\n        input_nodes = param[0]\n        hidden_nodes = param[1]\n        output_nodes = param[2]\n        epochs = param[3]\n        batch_size = param[4]\n        future_steps = param[5]\n\n        model_LSTM, forecasted_values_LSTM = Long_Short_Term_Memory(rainfall_data, input_nodes, hidden_nodes, output_nodes, epochs, batch_size, future_steps, scaler)\n        \n        y_true = test_rainfall_data.ix[:future_steps].Precipitation\n        mse, mae, mape, rmse = calculate_performance(y_true, forecasted_values_LSTM)\n        \n        info = list(param) + [mse, mae, rmse] + forecasted_values_LSTM\n        information_LSTM.append(info)\n\n    information_LSTM_df = pd.DataFrame(information_LSTM)\n    indexes = [str(i) for i in list(range(1, future_steps+1))]\n    information_LSTM_df.columns = ['look_back', 'hidden_nodes', 'output_nodes', 'epochs', 'batch_size', 'future_steps', 'MSE', 'MAE', 'RMSE'] + indexes\n    return information_LSTM_df","07956f97":"def analyze_results(data_frame, test_rainfall_data, name, flag=False):\n    optimized_params = data_frame.iloc[data_frame.RMSE.argmin]\n    future_steps = optimized_params.future_steps\n    forecast_values = optimized_params[-1*int(future_steps):]\n    y_true = test_rainfall_data.ix[:int(future_steps)]\n    forecast_values.index = y_true.index\n    \n    print('=== Best parameters of ' + name + ' ===\\n')\n    if (name == 'FNN' or name == 'LSTM'):\n        model = create_NN(optimized_params.look_back, \n                          optimized_params.hidden_nodes, \n                          optimized_params.output_nodes)\n        print('Input nodes(p): ' + str(optimized_params.look_back))\n        print('Hidden nodes: ' + str(optimized_params.hidden_nodes))\n        print('Output nodes: ' + str(optimized_params.output_nodes))\n    elif (name == 'TLNN'):\n        model = create_NN(len(optimized_params.look_back_lags), \n                          optimized_params.hidden_nodes, \n                          optimized_params.output_nodes)\n        s = ''\n        for i in optimized_params.look_back_lags:\n            s = s+' '+str(i)\n        print('Look back lags: ' + s)\n        print('Hidden nodes: ' + str(optimized_params.hidden_nodes))\n        print('Output nodes: ' + str(optimized_params.output_nodes))\n    elif (name == 'SANN'):\n        model = create_NN(optimized_params.seasonal_period, \n                          optimized_params.hidden_nodes, \n                          optimized_params.seasonal_period)\n        print('Input nodes(s): ' + str(optimized_params.seasonal_period))\n        print('Hidden nodes: ' + str(optimized_params.hidden_nodes))\n        print('Output nodes: ' + str(optimized_params.seasonal_period))\n        \n    print('Number of epochs: ' + str(optimized_params.epochs))\n    print('Batch size: ' + str(optimized_params.batch_size))\n    print('Number of future steps forecasted: ' + str(optimized_params.future_steps))\n    print('Mean Squared Error(MSE): ' + str(optimized_params.MSE))\n    print('Mean Absolute Error(MAE): ' + str(optimized_params.MAE))\n    print('Root Mean Squared Error(RMSE): ' + str(optimized_params.RMSE))\n    print('\\n\\n')\n    \n    # Save model\n    from keras.utils import plot_model\n    plot_model(model, to_file = STORAGE_FOLDER + name + '_best_fit_model.png', show_shapes=True, show_layer_names=True)\n    \n    # Save data\n    data_frame.to_csv(STORAGE_FOLDER + name + '_information.csv')\n    optimized_params.to_csv(STORAGE_FOLDER + name + '_optimized_values.csv')\n    \n    plt.figure(figsize=(10,5))\n    plt.plot(y_true, color='green', label='Actual values')\n    plt.plot(forecast_values, color='red', label='Forecasted values')\n    plt.xlabel('Year')\n    plt.ylabel('Monthly mean Precipitation')\n    plt.legend(loc='best')\n    if (flag==False):\n        plt.title(name + ' - Comaprison: Actual vs Forecasted')\n        plt.savefig(STORAGE_FOLDER + name + '_best_forecast'  + '.png')\n    else:\n        plt.title('Best of all: ' + name + ' - Comaprison: Actual vs Forecasted')\n        plt.savefig(STORAGE_FOLDER + 'BEST_FORECAST_' + name + '.png')\n    \n    return optimized_params","9d341f84":"def best_of_all(list_of_methods):\n    RMSE_values = [x.RMSE for x in list_of_methods]\n    index = np.argmin(RMSE_values)\n    if (index==0):\n        name = 'FNN'\n    elif (index == 1):\n        name = 'TLNN'\n    elif (index == 2):\n        name = 'SANN'\n    else:\n        name = 'LSTM'\n    print(RMSE_values)\n    \n    names = ['FNN', 'TLNN', 'SANN', 'LSTM']\n    RMSE_info = pd.Series(RMSE_values, index=names)\n    \n    print('Overall Best method on this data is ' + name)\n    return index, name, RMSE_info","4a2c30d8":"def compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps):\n    \n    information_FNN_df = get_accuracies_FNN(rainfall_data, test_rainfall_data, parameters_FNN, scaler)\n    optimized_params_FNN = analyze_results(information_FNN_df, test_rainfall_data, 'FNN')\n    \n    information_TLNN_df = get_accuracies_TLNN(rainfall_data, test_rainfall_data, parameters_TLNN, scaler)\n    optimized_params_TLNN = analyze_results(information_TLNN_df, test_rainfall_data, 'TLNN')\n    \n    information_SANN_df = get_accuracies_SANN(rainfall_data, test_rainfall_data, parameters_SANN, scaler)\n    optimized_params_SANN = analyze_results(information_SANN_df, test_rainfall_data, 'SANN')\n    \n    information_LSTM_df = get_accuracies_LSTM(rainfall_data, test_rainfall_data, parameters_LSTM, scaler)\n    optimized_params_LSTM = analyze_results(information_LSTM_df, test_rainfall_data, 'LSTM')\n    \n    list_of_methods = [optimized_params_FNN, optimized_params_TLNN, optimized_params_SANN, optimized_params_LSTM]\n    information = [information_FNN_df, information_TLNN_df, information_SANN_df, information_LSTM_df]\n    index, name, RMSE_info = best_of_all(list_of_methods)\n    best_optimized_params = analyze_results(information[index], test_rainfall_data, name, True)\n    return RMSE_info","1c775181":"future_steps = 60","705b2981":"# look_back, hidden_nodes, output_nodes, epochs, batch_size, future_steps\nparameters_FNN = [[1,2,3,6,8,10,12], [3,4,5,6], [1], [500], [20], [future_steps]]\n\n# time_lagged_points, hidden_nodes, output_nodes, epochs, batch_size, future_steps\nparameters_TLNN = [[[1,2,3,11,12], [1,2,3,4,11,12], [1,2,3,11,12,13], [1,2,3,4,5,6,10,11,12]], [3,4,5,6], [1], [300], [20], [future_steps]]\n\n# seasonal_period, hidden_nodes, epochs, batch_size, future_steps\nparameters_SANN = [[12], [3,4,5,6,7,8,9,10], [500], [20], [future_steps]]\n\n# look_back, hidden_nodes, output_nodes, epochs, batch_size, future_steps\nparameters_LSTM = [[1,2,3,4,5,6,7,8,9,10,11,12,13], [3,4,5,6], [1], [300], [20], [future_steps]]\n\nRMSE_info = compare_ANN_methods(rainfall_data, test_rainfall_data, scaler, \n                    parameters_FNN, parameters_TLNN, parameters_SANN, parameters_LSTM, future_steps)","3daf3cc7":"RMSE_info","26123f30":"ax = RMSE_info.plot(kind='bar', figsize=(10,5), rot=0, title='RMSE scores')\n\nfor p in ax.patches:\n    ax.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()\/2., p.get_height()), \n                ha='center', va='center', xytext=(0, 10), \n                textcoords='offset points', fontsize=14, color='black')","b18bc2fe":"![TLNN](https:\/\/i.imgur.com\/YFrPyRi.png)","c0774649":"## Compare the results of above algorithms based on RMSE score.\n- Plots a bar plot comparing the scores of different algorithms.","730964db":"### Seasonal Artificial  Neural Networks","c9391380":"![LSTM](https:\/\/i.imgur.com\/D4s4zfm.jpg)","a3be1131":"### Create a Neural Network","44358f67":"# <center>Time Series Forecasting Using Artificial Neural Networks<\/center>","3c8d7c68":"## Number of future steps to be forecasted","21897c5f":"- If we compare the results, LSTM worked better than all the methods on this data.","f4da0e7a":"## <center> Artificial Neural Networks <\/center>","80e24829":"### Long Short Term Memory","f77705a7":"### SANN - Preprocess data: Convert time series into suitable format","2ba4cc60":"### Time Lagged Neural Networks","0f18466c":"### SANN - Get scores on test data","ba3ab667":"## <center> Time lagged Neural Networks <\/center>\n- In FNN, y_t is a function of previous \"p\" time steps.\n- TLNN is another variation of FNN and it also widely used.\n- In TLNN, the input nodes are the time series values at some particular lags. For example, a typical TLNN for a time series, with seasonal period s = 12 can contain the input nodes as the lagged values at time t \u2212 1 , t \u2212 2 and t \u2212 12 . The value at time t is to be forecasted using the values at lags 1, 2 and 12 as shiwn in the figure.","3aa90e5a":"![Imgur](https:\/\/i.imgur.com\/TOD1J5m.png)","279e5710":"### NOTE:\n- This is an automated code that takes the parameters as input and it tries with all combonations of parameters and selects the best method from all of them based on RMSE scores.\n- It stores MSE, RMSE, parameters for all the combinations along with the forecasted values in a csv file for future reference. Also, the plots that are shown above are also stored. Change the STORAGE_FOLDER accordingly.\n- Change the data and run the code. It's as simple as that.","7aab89d4":"## <center> Seasonal Artificial Neural Networks <\/center>\n- The SANN structure is proposed to improve the forecasting performance of ANNs for seasonal time series data.\n- SANN can learn the seasonal pattern in the series, without removing them, contrary to some other traditional approaches, such as SARIMA.\n- In this model, the seasonal parameter \"s\" is used to determine the number of input and output neurons.\n- The i th and (i+1) th seasonal period observations are respectively used as the values of input and output neurons in this network structure.\n- Thus while forecasting with SANN, the number of input and output neurons should be taken as 12 for monthly and 4 for quarterly time series. The appropriate number of hidden nodes can be determined by performing suitable experiments on the training data.","1959f86a":"### TLNN - Get scores on test data","475b5aa9":"## Analyze the test data and forecasted data\n- Plot the actual and forecasted values.\n- Display optimal parameters.","1233dcd6":"- Feedback is always welcome!!\n- Don't forget to **UPVOTE** if you find this kernel useful.","2a11b047":"- In this kernel, we will apply Neural Networks on Time Series data to forecast.\n- Some of the techniques\/variants of NN that are applied on time series are:\n    - Feed forward Neural Networks(FNN)\n    - Time lagged Neural Networks(TLNN)\n    - Seasonal Artificial Neural Networks(SANN)\n    - Long Short Term Memory(LSTM)\n    \n- Data used for the experiments is \"Pune Precipitation data from 1965 to 2002\".","48db3d50":"### Feed forward Network","0728be51":"## Neural Networks\n- An Artificial Neural Network (ANN) is an information processing paradigm that is inspired by the way biological nervous systems, such as the brain, process information. The key element of this paradigm is the novel structure of the information processing system. It is composed of a large number of highly interconnected processing elements (neurones) working in unison to solve specific problems. ANNs, like people, learn by example. An ANN is configured for a specific application, such as pattern recognition or data classification, through a learning process.\n- For the usage of ANNs, we need not assume anything before applying unlike statistical methods.\n- Artificial Neural Networks are known as Universal approximaters. They can approximate any type of function.\n- Now a days, Neural Networks outperformed all the state of the art methods. CNN, RNN are some of the variants of Neural Networks that showed promising results in the areas of Computer Vision and Natural Language Processing.","33f4743e":"### References:\n- https:\/\/towardsdatascience.com\/recurrent-neural-networks-and-lstm-4b601dd822a5\n- An Introductory Study on Time Series Modeling and Forecasting - https:\/\/arxiv.org\/abs\/1302.6613","1165d10f":"### TLNN - Preprocess data: Convert time series into suitable format","9a91dd14":"## Initialize all the parameters\n- It tries all the combinations of different parameters and selects the optimal parmaters based on the RMSE scores.\n- It also finds the best method for this data.","c4e6378f":"### FNN - Get scores on test data","b78675cd":"- Artificial neural networks (ANNs) approach has been suggested as an alternative technique to time series forecasting and it gained immense popularity in last few years.\n- ANNs try to recognize regularities and patterns in the input data, learn from experience and then provide generalized results based on their known previous knowledge.\n- ANNs have been applied in many different areas, especially for forecasting and classification purposes.\n- ANNs are data-driven and self-adaptive in nature. There is no need to specify a particular model form or to make any a priori assumption about the statistical distribution of the data.\n- ANNs are inherently non-linear, which makes them more practical and accurate in modeling complex data patterns, as opposed to various traditional linear approaches, such as ARIMA methods.\n- Finally, Neural Networks are universal approximaters. ","c50d5fe5":"## Import libraries","eb723e0f":"### FNN - Preprocess data: Convert time series into suitable format","346fd374":"### LSTM - Preprocess data: Convert time series into suitable format","99c37d7c":"## Pick the best method","3e7f41fe":"### LSTM - Create LSTM network","ed99218b":"### SANN - Forecast next 'n' timesteps","ee0b771d":"![SANN](https:\/\/i.imgur.com\/hAw73Hp.png)","bebd68c2":"## <center> Long Short Term Memory(LSTM) <\/center>\n- RNN\u2019s are able to remember important things about the input they received, which enables them to be very precise in predicting what\u2019s coming next. This is the reason why they are used for time series, speech recognition, text prediction,..etc.\n- LSTM is a very powerful model that is used for sequntial data.\n- It has the power to store long term dependencies in it's memory.\n- In a RNN\/LSTM, the information cycles through a loop. When it makes a decision, it takes into consideration the current input and also what it has learned from the inputs it received previously.\n- Here, the previous \"p\" timesteps are used to predict the current timestep as shown in the figure.\n- We can also increase number of hidden units, Increase the number of units at the output.","a2aad670":"## <center> Feed forward ANN <\/center>\n- The most widely used ANNs in forecasting problems are multi-layer perceptrons (MLPs), which use a single hidden layer feed forward network(FNN).\n- The model is characterized by a network of three layers, viz. input, hidden and output layer. There may be more than one hidden layer.","26466591":"### Train model","56119edb":"### FNN - Forecast next 'n' timesteps","8440313d":"### Functions required","b5b2258e":"### TLNN - Forecast next 'n' timesteps","801b317c":"### LSTM - Get scores on test data","da82a1b0":"### LSTM - Forecast next 'n' timesteps","7e45a3d8":"## Plot RMSE scores","85e5f108":"![Neural Network](https:\/\/i.imgur.com\/P2ukWsv.png)","2855f3d7":"- Here, the output(y_t) depends on the previous 'p' time steps.\n- So, the output layer consists of one unit and input layer layer consists of p units.\n- We need to train the model with different number of units in all layers and select the parameters that gives the low RMSE score on test data."}}