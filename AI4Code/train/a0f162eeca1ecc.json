{"cell_type":{"e4df573a":"code","950a41a7":"code","e34dfa1c":"code","fa86f2e2":"code","765d3e18":"code","24fd8228":"code","e71d6e32":"code","068b4f1d":"code","c70d9862":"code","36fc9259":"code","944fea7e":"code","3c944bed":"code","f8c7b8a5":"code","4c7ec3b8":"code","cafc443d":"code","8df4dd1c":"code","8200b78a":"code","16196fc7":"code","672ed445":"code","8636819c":"code","85cfee46":"code","cdb2c228":"code","1f9260b9":"code","045071e4":"code","3d43ffbf":"code","ae9fd7b6":"code","7e61b257":"code","69a9271c":"code","c7e86d4f":"code","9b45c18d":"code","ab2dc9ac":"code","9b54ce53":"code","0df54853":"code","b767023e":"code","a965f357":"code","719434a0":"code","b3d791e6":"code","5a80bede":"code","d00360e7":"code","a3dac1f3":"code","c3ff5355":"code","8836edde":"code","c1b73a62":"code","eff82f26":"code","20d34b13":"code","fca6fa36":"code","212d09de":"code","01fda215":"code","a586fbc2":"code","522e0444":"code","91fe2631":"markdown","4145540b":"markdown","3a0eac12":"markdown","2761e8ee":"markdown","2efd18da":"markdown","bcba2ea8":"markdown"},"source":{"e4df573a":"#this dataset from http:\/\/archive.ics.uci.edu\/ml\/datasets\/Bank+Marketing\nbank_additional = 'bank-additional-full.csv'","950a41a7":"#import from python library\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nimport seaborn as sns","e34dfa1c":"#Create dataframe from the data set\ndf = pd.read_csv ('..\/input\/bank-additional-full.csv', sep=';', decimal ='.', header =0, names = \n                 ['age', 'job', 'marital', 'education', 'default','housing','loan','contact','month',\n                 'day_of_week','duration','campaign','pdays','previous','poutcome','emp.var.rate','cons.price.idx',\n                 'cons.conf.idx','euribor3m','nr.employed','target'])","fa86f2e2":"#look for data types\nprint (df.dtypes)","765d3e18":"#check the import process already correct\nprint (df.head())","24fd8228":"#check how many sample and attribute in the data\nprint (df.shape)","e71d6e32":"#statistic description for numerical variables\nprint (df.describe())","068b4f1d":"#The data is already clean from missing values\nprint (df.isnull().sum())","c70d9862":"#Further check for sanity check, typo errors, redundant white space\n\nprint (df ['age'].value_counts())\nprint (df ['job'].value_counts())\nprint (df ['marital'].value_counts())\nprint (df ['education'].value_counts())\nprint (df ['default'].value_counts())\nprint (df ['housing'].value_counts())\nprint (df ['loan'].value_counts())\nprint (df ['contact'].value_counts())\nprint (df ['month'].value_counts())\nprint (df ['day_of_week'].value_counts())\nprint (df ['duration'].value_counts())\nprint (df ['campaign'].value_counts())\nprint (df ['pdays'].value_counts())\nprint (df ['previous'].value_counts())\nprint (df ['poutcome'].value_counts())\nprint (df ['emp.var.rate'].value_counts())\nprint (df ['cons.price.idx'].value_counts())\nprint (df ['cons.conf.idx'].value_counts())\nprint (df ['euribor3m'].value_counts() )\nprint (df ['nr.employed'] .value_counts())\nprint (df ['target'].value_counts())","36fc9259":"#Checking data distribution for categorical value using pie chart\n# Data to plot\nlabels_house = ['yes', 'no', 'unknown']\nsizes_house = [2175, 1839, 105]\ncolors_house = ['#ff6666', '#ffcc99', '#ffb3e6']\n\nlabels_loan = ['yes', 'no', 'unknown']\nsizes_loan = [665, 3349, 105]\ncolors_loan = ['#c2c2f0','#ffb3e6', '#66b3ff' ]\n\nlabels_contact = ['cellular', 'telephone']\nsizes_contact = [2652, 1467]\ncolors_contact = ['#ff9999','#ffcc99']\n\nlabels_default = ['no','unknown','yes']\nsizes_default = [3523, 454, 142]\ncolors_default = ['#99ff99','#66b3ff','#ff6666' ]\n\n\n# Plot\nplt.rcParams.update({'font.size': 15})\n\nplt.figure(0)\nplt.pie(sizes_house, labels=labels_house, colors=colors_house, autopct='%1.1f%%', startangle=90, pctdistance=0.8)\nplt.title ('Housing Loan')\ncentre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.show()\n\nplt.figure(1)\nplt.pie(sizes_loan,labels=labels_loan, colors=colors_loan, autopct='%1.1f%%',startangle=90,pctdistance=0.8)\nplt.title ('Personal Loan')\ncentre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.show()\n\nplt.figure(2)\nplt.pie(sizes_contact, labels=labels_contact, colors=colors_contact, autopct='%1.1f%%', startangle=90,pctdistance=0.8)\nplt.title ('Contact Method')\ncentre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.show()\n\nplt.figure(3)\nplt.pie(sizes_default, labels=labels_default, colors=colors_default, autopct='%1.1f%%', startangle=90,pctdistance=0.8)\nplt.title ('default')\ncentre_circle = plt.Circle((0,0),0.5,color='black', fc='white',linewidth=0)\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\nplt.show()\n","944fea7e":"# exploring loan ownership vs current campaign result\/outcome\nsns.catplot(x=\"target\", hue=\"loan\", kind=\"count\", data = df)\nplt.title ('result by personal loan status')\nplt.xlabel(\"results(y) of current campaign\")\nplt.show ()","3c944bed":"# exploring loan ownership vs current campaign result\/outcome\nsns.catplot(x=\"target\", hue=\"housing\", kind=\"count\", data = df)\nplt.title ('result by housing loan status')# exploring loan ownership vs current campaign result\/outcome\nplt.show ()\n","f8c7b8a5":"# exploring default status vs current campaign result\/outcome\nsns.catplot(x=\"target\", hue=\"default\", kind=\"count\", data = df)\nplt.title ('result by default status')\nplt.xlabel(\"results(y) of current campaign\")\nplt.show ()","4c7ec3b8":"# clarifying effect of 'yes' value of default status on target as not visible in count plot\nldf = df[(df.default == \"yes\")]\nprint (ldf)","cafc443d":"#result(y) vs previous campaign outcome(poutcome) vs duration \ng = sns.catplot(x=\"duration\", y=\"target\", row = \"poutcome\",\n                kind=\"box\", orient=\"h\", height=2.5, aspect=5,\n                data=df)","8df4dd1c":"# exploring relationship between result and current campaign\nsns.catplot(x=\"target\", kind=\"count\", data = df);\nplt.title ('results (target count) of current campaign')\nplt.xlabel(\"results(target) of current campaign\")\n\nplt.show ()","8200b78a":"# exploring job type vs current campaign result\/outcome\nsns.catplot(x=\"target\", hue=\"job\", kind=\"count\", data = df)\nplt.title ('Result by job type of clients')\nplt.xlabel(\"results(y) of current campaign\")\nplt.show ()","16196fc7":"# exploring job type vs current campaign result\/outcome\nsns.catplot(x=\"target\", hue=\"education\", kind=\"count\", data = df)\nplt.title ('Result by education level of clients')\nplt.xlabel(\"results(y) of current campaign\")\nplt.show ()","672ed445":"# exploring job type vs current campaign result\/outcome\nfig, ax = plt.subplots()\nfig.set_size_inches(25, 8)\nsns.countplot(x = 'age', data = df)\nax.set_xlabel('Age', fontsize=15)\nax.set_ylabel('Count', fontsize=15)\nax.set_title('Age Count Distribution', fontsize=15)\nsns.despine()","8636819c":"#from the heatmap we can see the positive relationship between \"cons.price.idx-emp.var.rate\", \"euribor3m-emp.var.rate\", \"nr.employed-emp.var.rate\", \"euribor3m-cons.price.idx\"\n\nplt.figure(figsize=(11,4))\nsns.heatmap(df[[\"age\", \"duration\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\"euribor3m\", \"nr.employed\"]].corr(),annot=True)\nplt.show()","85cfee46":"#scatter matrix for numeric value, we can see that most of the numerical value is scattered\n\ng = sns.pairplot(df[[\"age\", \"duration\", \"campaign\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\"euribor3m\", \"nr.employed\"]], diag_kind=\"kde\")\nplt.show ()","cdb2c228":"#Creating new dataframe for classification model\ndftree = df[['age', 'job', 'marital', 'education', 'default','housing','loan','contact','month',\n                 'day_of_week','duration','campaign','pdays','previous','poutcome','emp.var.rate','cons.price.idx',\n                 'cons.conf.idx','euribor3m','nr.employed','target']]\n\ndftreetarget = dftree['target']\n","1f9260b9":"print (dftree.head())\nprint (dftree.shape)","045071e4":"#In order to pass the data into k-nearest neighbors we need to encode the categorical values to integers\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\n#encoding\/transforming\ndftree['job'] = le.fit_transform(dftree['job'].astype('str'))\ndftree['marital'] = le.fit_transform(dftree['marital'].astype('str'))\ndftree['education'] = le.fit_transform(dftree['education'].astype('str'))\ndftree['default'] = le.fit_transform(dftree['default'].astype('str'))\ndftree['housing'] = le.fit_transform(dftree['housing'].astype('str'))\ndftree['loan'] = le.fit_transform(dftree['loan'].astype('str'))\ndftree['contact'] = le.fit_transform(dftree['contact'].astype('str'))\ndftree['month'] = le.fit_transform(dftree['month'].astype('str'))\ndftree['day_of_week'] = le.fit_transform(dftree['day_of_week'].astype('str'))\ndftree['poutcome'] = le.fit_transform(dftree['poutcome'].astype('str'))\ndftree['target'] = le.fit_transform(dftreetarget.astype('str'))\n\ndftreetarget = le.fit_transform(dftreetarget.astype('str')) ## separatly creating a target variable for modelling","3d43ffbf":"print (dftree.head())\nprint (dftreetarget)","ae9fd7b6":"#Before running the classification model we need to drop the target value labels\ndftree = dftree.drop('target', axis=1)\nprint (dftree.head())","7e61b257":"#set train and test data 50% train 50% test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dftree.values, dftreetarget, test_size=0.5,random_state=0)\nprint (X_train)\nprint (X_train.shape)\nprint (y_train)\nprint (y_train.shape)\nprint (X_test)\nprint (y_test)\nprint (y_test.shape)","69a9271c":"#Classification using Decision Tree Classifier parameter tuning\n#Using for loop iteration to find best precision for this model\nfrom sklearn.tree import DecisionTreeClassifier\n\nbestprecision = 0\n\nfor a in range (2, 5+1):\n    for b in range (2, 5+1):\n        for c in range (1, 5+1):\n            clf = DecisionTreeClassifier(criterion='gini', max_depth=a, min_samples_split=b, min_samples_leaf=c, max_features=None, max_leaf_nodes=None)\n            fit = clf.fit(X_train, y_train)\n            y_pre = fit.predict(X_test)\n            \n            from sklearn.metrics import confusion_matrix\n            from sklearn.metrics import classification_report\n            cm = confusion_matrix(y_test, y_pre)\n            print (\"max depth = \" + str(a) + \" min samples split = \" + str(b) + \"min samples leaf\" + str(c))\n            print (cm)\n            print (classification_report(y_test,y_pre))\n            \n            precision = round(float(cm.item(3))\/float((cm.item(1)+cm.item(3))),2)\n            \n            if(precision > bestprecision):\n                bestprecision = precision\n                besta = a\n                bestb = b\n                bestc = c\n\nprint (bestprecision, besta, bestb, bestc)","c7e86d4f":"#performance of the best model 50%TRAIN 50%TEST Desc Tree\nfrom sklearn.metrics import accuracy_score\nclf = DecisionTreeClassifier(criterion='gini', max_depth=besta, min_samples_split=bestb, min_samples_leaf=bestc, max_features=None, max_leaf_nodes=None)\nfit = clf.fit(X_train, y_train)\ny_pre = fit.predict(X_test)\ncm = confusion_matrix(y_test, y_pre)\n\nprint (\"Confusion Matrix : \")\n\nprint (cm)\n\nprint (\"Accuracy : \" )\n\nprint (round(accuracy_score(y_test,y_pre),3))\n\nprint (classification_report(y_test,y_pre))","9b45c18d":"#K-Folds Cross Validation using the best k and p for Desc Tree 50%Test\n\n\nprint (\"[Train\/test split] score: {:.5f}\".format(clf.score(X_test, y_test)))\n\nfrom sklearn.model_selection import KFold\nkf = KFold (n_splits=5, random_state=4)\n\nfor train_index, test_index in kf.split (dftree) :\n    print(\"TRAIN :\", train_index, \"TEST: \", test_index)\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    \nfor k, (train_index, test_index) in enumerate(kf.split(dftree)):\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    clf.fit(X_train, y_train)\n    print (\"[fold {0}] score: {1:.5f}\".format(k, clf.score(X_test, y_test)))","ab2dc9ac":"#set train and test data 60% train 40% test\n\n#from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dftree.values, dftreetarget, test_size=0.4,random_state=0)\nprint (X_train)\nprint (X_train.shape)\nprint (y_train)\nprint (y_train.shape)\nprint (X_test)\nprint (y_test)\nprint (y_test.shape)","9b54ce53":"#DescTree40%Test\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfor a in range (2, 5+1):\n    for b in range (2, 5+1):\n        for c in range (1, 5+1):\n            clf = DecisionTreeClassifier(criterion='gini', max_depth=a, min_samples_split=b, min_samples_leaf=c, max_features=None, max_leaf_nodes=None)\n            fit = clf.fit(X_train, y_train)\n            y_pre = fit.predict(X_test)\n            \n            from sklearn.metrics import confusion_matrix\n            from sklearn.metrics import classification_report\n            cm = confusion_matrix(y_test, y_pre)\n            print (\"max depth = \" + str(a) + \" min samples split = \" + str(b) + \"min samples leaf\" + str(c))\n            print (cm)\n            print (classification_report(y_test,y_pre))\n            \n            precision = round(float(cm.item(3))\/float((cm.item(1)+cm.item(3))),2)\n            if(precision > bestprecision):\n                bestprecision = precision\n                besta = a\n                bestb = b\n                bestc = c\n\nprint (bestprecision, besta, bestb, bestc)","0df54853":"#performance of the best model 60%TRAIN 40%TEST Desc Tree\nfrom sklearn.metrics import accuracy_score\nclf = DecisionTreeClassifier(criterion='gini', max_depth=besta, min_samples_split=bestb, min_samples_leaf=bestc, max_features=None, max_leaf_nodes=None)\nfit = clf.fit(X_train, y_train)\ny_pre = fit.predict(X_test)\ncm = confusion_matrix(y_test, y_pre)\n\nprint (\"Confusion Matrix : \")\n\nprint (cm)\n\nprint (\"Accuracy : \" )\n\nprint (round(accuracy_score(y_test,y_pre),3))\n\nprint (classification_report(y_test,y_pre))","b767023e":"#K-Folds Cross Validation using the best k and p for Desc TREE 40%Test\n\nprint (\"[Train\/test split] score: {:.5f}\".format(clf.score(X_test, y_test)))\n\nfrom sklearn.model_selection import KFold\nkf = KFold (n_splits=5, random_state=4)\n\nfor train_index, test_index in kf.split (dftree) :\n    print(\"TRAIN :\", train_index, \"TEST: \", test_index)\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    \nfor k, (train_index, test_index) in enumerate(kf.split(dftree)):\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    clf.fit(X_train, y_train)\n    print (\"[fold {0}] score: {1:.5f}\".format(k, clf.score(X_test, y_test)))","a965f357":"#set train and test data 80% train 20% test\n\n#from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dftree.values, dftreetarget, test_size=0.2,random_state=0)\nprint (X_train)\nprint (X_train.shape)\nprint (y_train)\nprint (y_train.shape)\nprint (X_test)\nprint (y_test)\nprint (y_test.shape)","719434a0":"#DescTree20%Test\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nfor a in range (2, 5+1):\n    for b in range (2, 5+1):\n        for c in range (1, 5+1):\n            clf = DecisionTreeClassifier(criterion='gini', max_depth=a, min_samples_split=b, min_samples_leaf=c, max_features=None, max_leaf_nodes=None)\n            fit = clf.fit(X_train, y_train)\n            y_pre = fit.predict(X_test)\n            \n            cm = confusion_matrix(y_test, y_pre)\n            print (\"max depth = \" + str(a) + \" min samples split = \" + str(b) + \"min samples leaf\" + str(c))\n            print (cm)\n            print (classification_report(y_test,y_pre))\n            \n            precision = round(float(cm.item(3))\/float((cm.item(1)+cm.item(3))),2)\n            if(precision > bestprecision):\n                bestprecision = precision\n                besta = a\n                bestb = b\n                bestc = c\n\nprint (bestprecision, besta, bestb, bestc)","b3d791e6":"#performance of the best model 80%TRAIN 20%TEST Desc Tree\nfrom sklearn.metrics import accuracy_score\nclf = DecisionTreeClassifier(criterion='gini', max_depth=besta, min_samples_split=bestb, min_samples_leaf=bestc, max_features=None, max_leaf_nodes=None)\nfit = clf.fit(X_train, y_train)\ny_pre = fit.predict(X_test)\ncm = confusion_matrix(y_test, y_pre)\n\nprint (\"Confusion Matrix : \")\n\nprint (cm)\n\nprint (\"Accuracy : \" )\n\nprint (round(accuracy_score(y_test,y_pre),3))\n\nprint (classification_report(y_test,y_pre))","5a80bede":"#K-Folds Cross Validation using the best k and p for Desc TREE 20%Test\n\n\nprint (\"[Train\/test split] score: {:.5f}\".format(clf.score(X_test, y_test)))\n\nfrom sklearn.model_selection import KFold\nkf = KFold (n_splits=5, random_state=4)\n\nfor train_index, test_index in kf.split (dftree) :\n    print(\"TRAIN :\", train_index, \"TEST: \", test_index)\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    \nfor k, (train_index, test_index) in enumerate(kf.split(dftree)):\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    clf.fit(X_train, y_train)\n    print (\"[fold {0}] score: {1:.5f}\".format(k, clf.score(X_test, y_test)))","d00360e7":"#set train and test data 50% train 50% test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dftree.values, dftreetarget, test_size=0.5,random_state=0)\nprint (X_train)\nprint (X_train.shape)\nprint (y_train)\nprint (y_train.shape)\nprint (X_test)\nprint (y_test)\nprint (y_test.shape)","a3dac1f3":"#KNearest Neighbour Clasification 50%\n\n#selecting the best attribute k and p using iteration\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nbestprecision = 0\n\nfor k in range (1, 25+1):\n    for i in range (1, 3+1):\n        clf = KNeighborsClassifier(n_neighbors = k, weights='distance', metric='minkowski', p=i)\n        fit = clf.fit(X_train, y_train)\n        y_pre = fit.predict(X_test)\n        from sklearn.metrics import confusion_matrix\n        from sklearn.metrics import classification_report\n        cm = confusion_matrix(y_test, y_pre)\n        print (\"k = \" + str(k) + \" p = \" + str(i))\n        print (cm)\n        print (classification_report(y_test,y_pre))\n        \n        precision = round(float(cm.item(3))\/float((cm.item(1)+cm.item(3))),2)\n        if(precision > bestprecision):\n            bestprecision = precision\n            bestk = k\n            bestp = i\n\nprint (bestprecision, bestk, bestp)","c3ff5355":"#performance of the best model 50%TRAIN 50%TEST k-NN\nfrom sklearn.metrics import accuracy_score\nclf = DecisionTreeClassifier(criterion='gini', max_depth=besta, min_samples_split=bestb, min_samples_leaf=bestc, max_features=None, max_leaf_nodes=None)\nfit = clf.fit(X_train, y_train)\ny_pre = fit.predict(X_test)\ncm = confusion_matrix(y_test, y_pre)\n\nprint (\"Confusion Matrix : \")\n\nprint (cm)\n\nprint (\"Accuracy : \" )\n\nprint (round(accuracy_score(y_test,y_pre),3))\n\nprint (classification_report(y_test,y_pre))","8836edde":"#K-Folds Cross Validation using the best k and p for k nearest neighbors 50%Test\n\n\nprint (\"[Train\/test split] score: {:.5f}\".format(clf.score(X_test, y_test)))\n\nfrom sklearn.model_selection import KFold\nkf = KFold (n_splits=5, random_state=4)\n\nfor train_index, test_index in kf.split (dftree) :\n    print(\"TRAIN :\", train_index, \"TEST: \", test_index)\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    \nfor k, (train_index, test_index) in enumerate(kf.split(dftree)):\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    clf.fit(X_train, y_train)\n    print (\"[fold {0}] score: {1:.5f}\".format(k, clf.score(X_test, y_test)))","c1b73a62":"#set train and test data 60% train 40% test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dftree.values, dftreetarget, test_size=0.5,random_state=0)\nprint (X_train)\nprint (X_train.shape)\nprint (y_train)\nprint (y_train.shape)\nprint (X_test)\nprint (y_test)\nprint (y_test.shape)","eff82f26":"#KNearest Neighbour Clasification\n\n#selecting the best attribute k and p using iteration\n\n#from sklearn.neighbors import KNeighborsClassifier\n\nfor k in range (1, 25+1):\n    for i in range (1, 3+1):\n        clf = KNeighborsClassifier(n_neighbors = k, weights='distance', metric='minkowski', p=i)\n        fit = clf.fit(X_train, y_train)\n        y_pre = fit.predict(X_test)\n        #from sklearn.metrics import confusion_matrix\n        cm = confusion_matrix(y_test, y_pre)\n        print (\"k = \" + str(k) + \" p = \" + str(i))\n        print (cm)\n        print (classification_report(y_test,y_pre))\n        \n        precision = round(float(cm.item(3))\/float((cm.item(1)+cm.item(3))),2)\n        if(precision > bestprecision):\n            bestprecision = precision\n            bestk = k\n            bestp = i\n\nprint (bestprecision, bestk, bestp)\n        ","20d34b13":"#performance of the best model 50%TRAIN 40%TEST k-NN\nfrom sklearn.metrics import accuracy_score\nclf = DecisionTreeClassifier(criterion='gini', max_depth=besta, min_samples_split=bestb, min_samples_leaf=bestc, max_features=None, max_leaf_nodes=None)\nfit = clf.fit(X_train, y_train)\ny_pre = fit.predict(X_test)\ncm = confusion_matrix(y_test, y_pre)\n\nprint (\"Confusion Matrix : \")\n\nprint (cm)\n\nprint (\"Accuracy : \" )\n\nprint (round(accuracy_score(y_test,y_pre),3))\n\nprint (classification_report(y_test,y_pre))","fca6fa36":"#K-Folds Cross Validation using the best k and p for k nearest neighbors\n\n\nprint (\"[Train\/test split] score: {:.5f}\".format(clf.score(X_test, y_test)))\n\n#from sklearn.model_selection import KFold\nkf = KFold (n_splits=5, random_state=4)\n\nfor train_index, test_index in kf.split (dftree) :\n    print(\"TRAIN :\", train_index, \"TEST: \", test_index)\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    \nfor k, (train_index, test_index) in enumerate(kf.split(dftree)):\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    clf.fit(X_train, y_train)\n    print (\"[fold {0}] score: {1:.5f}\".format(k, clf.score(X_test, y_test)))","212d09de":"#set train and test data 80% train 20% test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(dftree.values, dftreetarget, test_size=0.5,random_state=0)\nprint (X_train)\nprint (X_train.shape)\nprint (y_train)\nprint (y_train.shape)\nprint (X_test)\nprint (y_test)\nprint (y_test.shape)","01fda215":"#KNearest Neighbour Clasification\n\n#selecting the best attribute k and p using iteration\n\n#from sklearn.neighbors import KNeighborsClassifier\n\nfor k in range (1, 25+1):\n    for i in range (1, 3+1):\n        clf = KNeighborsClassifier(n_neighbors = k, weights='distance', metric='minkowski', p=i)\n        fit = clf.fit(X_train, y_train)\n        y_pre = fit.predict(X_test)\n\n        cm = confusion_matrix(y_test, y_pre)\n        print (\"k = \" + str(k) + \" p = \" + str(i))\n        print (cm)\n        print (classification_report(y_test,y_pre))\n        \n        precision = round(float(cm.item(3))\/float((cm.item(1)+cm.item(3))),2)\n        if(precision > bestprecision):\n            bestprecision = precision\n            bestk = k\n            bestp = i\n\nprint (bestprecision, bestk, bestp)","a586fbc2":"#performance of the best model 80%TRAIN 20%TEST k-NN\n#from sklearn.metrics import accuracy_score\n\nclf = KNeighborsClassifier(n_neighbors = bestk, weights='distance', metric='minkowski', p=bestp)\nfit = clf.fit(X_train, y_train)\ny_pre = fit.predict(X_test)\ncm = confusion_matrix(y_test, y_pre)\n\nprint (\"Confusion Matrix : \")\n\nprint (cm)\n\nprint (\"Accuracy : \" )\n\nprint (round(accuracy_score(y_test,y_pre),3))\n\nprint (classification_report(y_test,y_pre))","522e0444":"#K-Folds Cross Validation using the best k and p for k nearest neighbors 20%Test\n\n\nprint (\"[Train\/test split] score: {:.5f}\".format(clf.score(X_test, y_test)))\n\n#from sklearn.model_selection import KFold\nkf = KFold (n_splits=5, random_state=4)\n\nfor train_index, test_index in kf.split (dftree) :\n    print(\"TRAIN :\", train_index, \"TEST: \", test_index)\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    \nfor k, (train_index, test_index) in enumerate(kf.split(dftree)):\n    X_train, X_test = dftree.values[train_index], dftree.values[test_index]\n    y_train, y_test = dftreetarget[train_index], dftreetarget[test_index]\n    clf.fit(X_train, y_train)\n    print (\"[fold {0}] score: {1:.5f}\".format(k, clf.score(X_test, y_test)))","91fe2631":"# Reference\n\nAnon., 2015. How to interpret scikit's learn confusion matrix and classification report?. [Online] Available at: https:\/\/stackoverflow.com\/questions\/30746460\/how-to-interpret-scikits-learn-confusion-matrix-and-classification-report\n<br>\nAnon., n.d. Bank Marketing Dataset. [Online] Available at: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Bank+Marketing [Accessed 01 June 2019].\nNavlani, A., 2018. Decision Tree Classification in Python. [Online] Available at: https:\/\/www.datacamp.com\/community\/tutorials\/decision-tree-classification-python\n<br>\nRen, Y., 2019. Classification Part I and II.Practical Data Science Lecture Slides :Semester 1 RMIT University.\n<br>\nShaikh, R., 2018. Cross Validation Explained: Evaluating estimator performance. [Online] Available at: https:\/\/towardsdatascience.com\/cross-validation-explained-evaluating-estimator-performance-e51e5430ff85\n<br>\nS. Moro, P. Cortez and P. Rita., 2014. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems. Elsevier, Volume 62, pp. 22-31.\n<br>\nSrivastava,T., 2018. Introduction to k-Nearest Neighbors: Simplified. [Online] Available at: <https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/introduction-k-neighbours-algorithm-clustering\/>","4145540b":"# Comparison of summarised results of KNN and Decision Tree classifiers:\n\nResults of classification (class 0 = \u2018no \u2019, class 1 = \u2018yes\u2019 after encoding categorical values of target variable):\n\nclf = KNeighborsClassifier(n_neighbors = k, weights='distance', metric='minkowski', p=i)\n\nKnearest Neigbhour parameters and results: \n<table>\n    <thead>\n        <tr>\n            <th>Split<\/th>\n            <th>k<\/th>\n            <th>p<\/th>\n            <th>Accuracy<\/th>\n            <th>Precision_0<\/th>\n            <th>Precision_1<\/th>\n            <th>recall_0<\/th>\n            <th>recall_1<\/th>\n            <th>KF1<\/th>\n            <th>KF2<\/th>\n            <th>KF3<\/th>\n            <th>KF4<\/th>\n            <th>KF5<\/th>\n        <\/tr>\n    <\/thead>\n    <tbody>\n        <tr>\n            <td>50\/50<\/td>\n            <td>19<\/td>\n            <td>1<\/td>\n            <td>0.912<\/td>\n            <td>0.94<\/td>\n            <td>0.64<\/td>\n            <td>0.96<\/td>\n            <td>0.50<\/td>\n            <td>0.966<\/td>\n            <td>0.951<\/td>\n            <td>0.948<\/td>\n            <td>0.888<\/td>\n            <td>0.750<\/td>            \n        <\/tr>\n        <tr>\n            <td>60\/40<\/td>\n            <td>18<\/td>\n            <td>1<\/td>\n            <td>0.914<\/td>\n            <td>0.94<\/td>\n            <td>0.65<\/td>\n            <td>0.97<\/td>\n            <td>0.49<\/td>\n            <td>0.968<\/td>\n            <td>0.951<\/td>\n            <td>0.948<\/td>\n            <td>0.888<\/td>\n            <td>0.752<\/td> \n        <\/tr>\n        <tr>\n            <td>80\/20<\/td>\n            <td>19<\/td>\n            <td>1<\/td>\n            <td>0.917<\/td>\n            <td>0.94<\/td>\n            <td>0.66<\/td>\n            <td>0.97<\/td>\n            <td>0.50<\/td>\n            <td>0.967<\/td>\n            <td>0.952<\/td>\n            <td>0.948<\/td>\n            <td>0.888<\/td>\n            <td>0.753<\/td> \n        <\/tr>\n    <\/tbody>\n    <\/table>\n    \n        \n<br>\nclf = DecisionTreeClassifier(criterion='gini', max_depth=a, min_samples_split=b, min_samples_leaf=c, max_features=None,\nmax_leaf_nodes=None)\n\nDecision Tree parameters and results:\n<table>\n    <thead>\n        <tr>\n            <th>Split<\/th>\n            <th>a<\/th>\n            <th>b<\/th>\n            <th>c<\/th>\n            <th>Accuracy<\/th>\n            <th>Precision_0<\/th>\n            <th>Precision_1<\/th>\n            <th>recall_0<\/th>\n            <th>recall_1<\/th>\n            <th>KF1<\/th>\n            <th>KF2<\/th>\n            <th>KF3<\/th>\n            <th>KF4<\/th>\n            <th>KF5<\/th>\n        <\/tr>\n    <\/thead>\n    <tbody>\n        <tr>\n            <td>50\/50<\/td>\n            <td>4<\/td>\n            <td>2<\/td>\n            <td>1<\/td>\n            <td>0.914<\/td>\n            <td>0.94<\/td>\n            <td>0.66<\/td>\n            <td>0.97<\/td>\n            <td>0.49<\/td>\n            <td>0.965<\/td>\n            <td>0.950<\/td>\n            <td>0.943<\/td>\n            <td>0.884<\/td>\n            <td>0.685<\/td>            \n        <\/tr>\n        <tr>\n            <td>60\/40<\/td>\n            <td>4<\/td>\n            <td>2<\/td>\n            <td>1<\/td>\n            <td>0.916<\/td>\n            <td>0.94<\/td>\n            <td>0.65<\/td>\n            <td>0.97<\/td>\n            <td>0.52<\/td>\n            <td>0.965<\/td>\n            <td>0.950<\/td>\n            <td>0.943<\/td>\n            <td>0.884<\/td>\n            <td>0.684<\/td>     \n        <\/tr>\n        <tr>\n            <td>80\/20<\/td>\n            <td>4<\/td>\n            <td>2<\/td>\n            <td>1<\/td>\n            <td>0.918<\/td>\n            <td>0.95<\/td>\n            <td>0.65<\/td>\n            <td>0.96<\/td>\n            <td>0.56<\/td>\n            <td>0.965<\/td>\n            <td>0.950<\/td>\n            <td>0.943<\/td>\n            <td>0.884<\/td>\n            <td>0.682<\/td>    \n        <\/tr>\n    <\/tbody>\n    <\/table>","3a0eac12":"**Abstract**\n\nClassification methods of K Nearest Neighbour and Decision Tree was performed on a multivariate bank marketing dataset of a Portuguese banking institution with a number of client attributes which determined the outcome of marketing campaign to signup for term deposits based on 50\/50 , 60\/40 and 80\/20 test\/training split of the dataset. \nAfter parameters for each classifier \/ classification method was optimized for each split of dataset, the relevant classifier was then trained on the training set and then used to predict the test set, after which a classification report for each model was generated. Based on the accuracy, precision and recall score of each classification method for the 80\/20 test\/training split, the Decision Tree was found to be a better classifier for the bank marketing data set. It performed better than K Nearest Neighbour classifier in terms of predicting class 1 or \u2018yes\u2019 responses in the target variable \u2018target\u2019 as indicated by the comparison of by confusion matrix and score summaries.","2761e8ee":"**Dataset Information**\nthe dataset is imported from https:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\n\n*Source:*\n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014\n\nThe data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. \n\nThere are four datasets: \n1) bank-additional-full.csv with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014]\n2) bank-additional.csv with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\n3) bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). \n4) bank.csv with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs). \nThe smallest datasets are provided to test more computationally demanding machine learning algorithms (e.g., SVM). \n\nThe classification goal is to predict if the client will subscribe (yes\/no) a term deposit (variable y).\n\n**Attribute information**\n\nbank client data:\n1. age (numeric)\n2. job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3. marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4. education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n5. default: has credit in default? (categorical: 'no','yes','unknown')\n6. housing: has housing loan? (categorical: 'no','yes','unknown')\n7. loan: has personal loan? (categorical: 'no','yes','unknown')\n8. contact: contact communication type (categorical: 'cellular','telephone') \n9. month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n10. day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri') \n11. duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\nother attributes:\n12. campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13. pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14. previous: number of contacts performed before this campaign and for this client (numeric)\n15. poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\nsocial and economic context attributes\n16. emp.var.rate: employment variation rate - quarterly indicator (numeric)\n17. cons.price.idx: consumer price index - monthly indicator (numeric) \n18. cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n19. euribor3m: euribor 3 month rate - daily indicator (numeric)\n20. nr.employed: number of employees - quarterly indicator (numeric)\n\nOutput variable (desired target):\n21. y - has the client subscribed a term deposit? (binary: 'yes','no')","2efd18da":"# Conclusion\n\nBased on the accuracy, precision and recall score of each classification method for the 80\/20 test\/training split the Decision Tree seems to be a better classifier for the bank marketing data set. It performs better than K Nearest Neighbour classifier in term of predicting class 1 or \u2018yes\u2019 responses in the target variable \u2018target\u2019 as indicated by confusion matrix comparison and score summary comparison earlier. The Kfold cross validation evaluation scores for 80\/20 split decision tree are good overall although the similar scores are higher for the KNN evaluation scores.","bcba2ea8":"# Classification Model"}}