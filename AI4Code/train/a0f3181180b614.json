{"cell_type":{"bc1578ef":"code","d126d796":"code","1bb3fa6f":"code","c630b5e4":"code","e7078b1d":"code","b13aec95":"code","0de716b2":"code","4e0e2ea2":"code","4c89a84b":"code","1b97ded8":"code","9f4cf84e":"code","2b292fd1":"code","54d24ae8":"code","ceb7c5bf":"code","7c18fea0":"code","f6f14989":"code","fadbd16d":"code","261b4a3b":"code","37380223":"code","f22ae6d4":"code","5e3a86d7":"code","18dd8544":"code","d511d16c":"code","85b96eb7":"code","61b0575c":"code","ffe6fd67":"code","8f1f889e":"code","b60727c8":"code","e3575a1b":"code","a5af2b99":"code","75d9f556":"code","a083e690":"code","a9beffe3":"code","ec4e4da3":"code","fc97f2e0":"code","eec4781d":"code","ad1e5797":"code","3f22f967":"code","f70b6b8e":"code","7d95e9f2":"code","de345c98":"code","3fd013cb":"code","846251b2":"code","4a4617d2":"code","e478ce2f":"code","54bf2ddd":"code","0e2b19b9":"code","894f6727":"code","391e79cf":"code","e037c5d7":"code","faff8565":"code","689a51b1":"code","2c9b407f":"code","0b9a37ab":"code","5c02e17f":"code","426da582":"code","e94b2ad8":"code","83e4492a":"code","4bc87c6d":"code","c563ebcb":"code","3fc2e01f":"code","e5498221":"code","4432f5c7":"code","e8e91374":"code","a6eef97c":"code","d8e92f5a":"code","4c71a5de":"code","453f92ec":"code","a5660255":"code","c8930650":"markdown","59096a54":"markdown","c5684f5b":"markdown","04c23955":"markdown","51511b7d":"markdown","9e69ebec":"markdown","0fc5a11b":"markdown","5945ce0f":"markdown","ee907200":"markdown","7490b33f":"markdown","63e8b58b":"markdown","5370f752":"markdown","1b5c6769":"markdown","2e767188":"markdown","ea679118":"markdown","9b942ed7":"markdown","3ef0edf7":"markdown","7b73bc9f":"markdown","f3527ad7":"markdown","daf2538d":"markdown","fed75ce5":"markdown","117ed193":"markdown","6dc6968b":"markdown","f7a73da9":"markdown","0a720f62":"markdown","7b5aa3ff":"markdown","170a8d2a":"markdown","c9d14ba7":"markdown","1485b0ee":"markdown","f03860bb":"markdown","b3e0cf38":"markdown","836f5176":"markdown","955f8469":"markdown","f19c0287":"markdown","aa7911b8":"markdown","fd6ca537":"markdown","4c216358":"markdown","7460e53c":"markdown","ccb42d1e":"markdown","b6f3bb71":"markdown","a81be85e":"markdown","493c2bcb":"markdown","622bce1a":"markdown","48d25be1":"markdown","70908389":"markdown","0851a3cc":"markdown","7a3bdff9":"markdown","a32c82a6":"markdown","fbf3cda4":"markdown","c66bcde6":"markdown","7e716e9a":"markdown","833311bb":"markdown","c869056b":"markdown","6dbd2e40":"markdown","fc236f23":"markdown","64b9c665":"markdown","09d47d12":"markdown","cb68f588":"markdown","24fd00df":"markdown","1e92221d":"markdown","bc8a123c":"markdown"},"source":{"bc1578ef":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d126d796":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","1bb3fa6f":"df.describe()","c630b5e4":"print(f\"Rows: {df.shape[0]}\")\nprint(f\"Columns: {df.shape[1]}\")","e7078b1d":"df.columns","b13aec95":"[column for column in df.columns if column[:1] != \"V\"]","0de716b2":"df.isnull().sum()","4e0e2ea2":"num_valid = len(df[ df['Class'] == 0 ])\nnum_fraudulent = len(df[ df['Class'] == 1 ])\n\nprint(\"Absolute Frequencies\")\nprint(f\"Valid Transactions: {num_valid}\")\nprint(f\"Fraudulent Transactions: {num_fraudulent}\\n\")\n\nprint(\"Relative Frequencies\")\nprint(f\"Valid Transactions: {round(num_valid\/len(df) * 100, 2)}%\")\nprint(f\"Fraudulent Transactions: {round(num_fraudulent\/len(df) * 100, 2)}%\")\n\nsns.countplot(df['Class'])\nplt.title('Frequencies of Valid and Non-Fraudulent Transactions')\nplt.xlabel('Class Label')\nplt.ylabel('Number of Transactions')","4c89a84b":"sns.heatmap(df.drop(['Amount', 'Time', 'Class'], axis=1).corr().round(2))","1b97ded8":"sns.heatmap(df.drop(['Time', 'Class'], axis=1).corr().loc[['Amount'],:])","9f4cf84e":"sns.heatmap(df.drop(['Amount', 'Class'], axis=1).corr().loc[['Time'],:])","2b292fd1":"sns.heatmap(df.drop(['Amount', 'Time'], axis=1).corr().loc[['Class'],:])","54d24ae8":"df_fraud = df[ df['Class'] == 1 ]\ndf_valid = df[ df['Class'] == 0 ]","ceb7c5bf":"figure, axes = plt.subplots(2, 1, figsize=(15, 7))\n\nsns.distplot(df_valid['Amount'], ax=axes[0], bins=30)\naxes[0].set_title('Valid Transactions')\n\nsns.distplot(df_fraud['Amount'], ax=axes[1], bins=30)\naxes[1].set_title('Valid Transactions')\n\nplt.tight_layout()","7c18fea0":"figure, axes = plt.subplots(2, 1, figsize=(15, 7))\n\nsns.distplot(df_valid['Amount'].apply(np.sqrt), ax=axes[0], bins=30)\naxes[0].set_title('Valid Transactions')\n\nsns.distplot(df_fraud['Amount'].apply(np.sqrt), ax=axes[1], bins=30)\naxes[1].set_title('Valid Transactions')\n\nplt.tight_layout()","f6f14989":"figure, axes = plt.subplots(2, 1, figsize=(15, 7))\n\nsns.boxplot(df_valid['Amount'], ax=axes[0])\naxes[0].set_title('Valid Transactions')\n\nsns.boxplot(df_fraud['Amount'], ax=axes[1])\naxes[1].set_title('Valid Transactions')\n\nplt.tight_layout()","fadbd16d":"sns.scatterplot(df['Time'], df['Amount'])","261b4a3b":"figure, axes = plt.subplots(28, 2, figsize=(10, 5*28))\n\ndef plot_pca_feature(row, index):\n    pca_id = f\"V{index+1}\"\n    sns.distplot(df[df['Class'] == 0][pca_id], ax=row[0])\n    row[0].set_title(f\"{pca_id} Valid Transaction\")\n\n    sns.distplot(df[df['Class'] == 1][pca_id], ax=row[1]) \n    row[1].set_title(f\"{pca_id} Fraudulent Transaction\")\n    \nfor index, row in enumerate(axes):\n    plot_pca_feature(row, index)\n\nplt.tight_layout()\nplt.show()","37380223":"from sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import train_test_split\n\nX = np.array(df.drop(['Class'], axis=1))\ny = np.array(df['Class'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\ndummy_predictions = dummy_clf.predict(X_test)","f22ae6d4":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(dummy_clf, \n                      X_test, \n                      y_test, \n                      display_labels=df['Class'].unique(),\n                      cmap=plt.cm.Blues)\n\nprint(f\"Dummy Classifier Accuracy: {dummy_clf.score(X_test, y_test)}\")","5e3a86d7":"from sklearn.naive_bayes import GaussianNB\n\nX = np.array(df.drop(['Class'], axis=1))\ny = np.array(df['Class'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nnb_model = GaussianNB()\nnb_model.fit(X_train, y_train)\nnb_predictions = nb_model.predict(X_test)","18dd8544":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(nb_model, \n                      X_test, \n                      y_test, \n                      cmap=plt.cm.Blues,\n                      values_format='d')\n\nprint(f\"Dummy Classifier Accuracy: {nb_model.score(X_test, y_test)}\")","d511d16c":"from sklearn.metrics import recall_score\n\nprint(f\"Dummy Classifier Recall: {recall_score(y_test, dummy_predictions)}\")","85b96eb7":"from sklearn.metrics import precision_score\n\nprint(f\"Dummy Classifier Precision: {precision_score(y_test, dummy_predictions)}\")","61b0575c":"from sklearn.metrics import f1_score\n\nprint(f\"Dummy Classifier F1: {f1_score(y_test, dummy_predictions)}\")","ffe6fd67":"!pip install PTable","8f1f889e":"from sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix,\n    cohen_kappa_score,\n    f1_score,\n    precision_score,\n    recall_score,\n    roc_auc_score,\n    roc_curve,\n    classification_report,\n    balanced_accuracy_score\n)","b60727c8":"from prettytable import PrettyTable\n\nclassification_metrics = {\n    \"Accuracy\": accuracy_score,\n    \"Cohen Kappa\": cohen_kappa_score,\n    \"F1\": f1_score,\n    \"Precision\": precision_score,\n    \"Recall\": recall_score,\n    \"Area Under Precision Recall Curve\": roc_auc_score,\n    \"Balanced Accuracy\": balanced_accuracy_score\n}\n\ndef custom_classification_report(ground_truth, predictions, metrics):\n    table = PrettyTable()\n    table.field_names = [\"Metric\", \"Score\"]\n    for name, metric in metrics.items():\n        table.add_row([name, metric(ground_truth, predictions).round(4)])\n    print(table)","e3575a1b":"custom_classification_report(y_test, nb_predictions, metrics=classification_metrics)","a5af2b99":"!pip install imbalanced-learn","75d9f556":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\n\nX_os_train, y_os_train = SMOTE().fit_resample(X_train, y_train)\nprint(sorted(Counter(y_os_train).items()))","a083e690":"from sklearn.naive_bayes import GaussianNB\n\nnb_os_model = GaussianNB()\nnb_os_model.fit(X_os_train, y_os_train)\nnb_os_predictions = nb_os_model.predict(X_test)\n\ncustom_classification_report(y_test, nb_os_predictions, metrics=classification_metrics)","a9beffe3":"from imblearn.under_sampling import NearMiss\nfrom collections import Counter\n\nX_us_train, y_us_train = NearMiss(version=1).fit_resample(X_train, y_train)\nprint(sorted(Counter(y_us_train).items()))","ec4e4da3":"from sklearn.naive_bayes import GaussianNB\n\nnb_us_model = GaussianNB()\nnb_us_model.fit(X_us_train, y_us_train)\nnb_us_predictions = nb_us_model.predict(X_test)\n\ncustom_classification_report(y_test, nb_us_predictions, metrics=classification_metrics)","fc97f2e0":"print(\"Naive Bayes\")\ncustom_classification_report(y_test, nb_predictions, metrics=classification_metrics)\n\nprint(\"\\nNaive Bayes w\/ Oversampling\")\ncustom_classification_report(y_test, nb_os_predictions, metrics=classification_metrics)\n\nprint(\"\\nNaive Bayes w\/ Undersampling\")\ncustom_classification_report(y_test, nb_us_predictions, metrics=classification_metrics)","eec4781d":"from sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef to_pca(df, n_components):\n    features_df = df.drop(['Amount', 'Time', 'Class'], axis=1)\n    pca = PCA(n_components)\n    pca.fit(features_df)\n    \n    columns = [f\"pca_{i+1}\" for i in range(n_components)]\n    df_pca = pd.DataFrame(pca.transform(features_df), columns=columns, index=df.index)\n    df_pca['Class'] = df['Class']\n    return df_pca\n\ndef plot_pca_graphs(df):\n    df_pca2d = to_pca(df, 2)\n    df_pca3d = to_pca(df, 3)\n    \n    fig = plt.figure(figsize=(15, 7))\n    fig.suptitle('PCA Representations')\n    \n    ax2d = fig.add_subplot(121)\n    sns.scatterplot(\n        x=\"pca_1\",\n        y=\"pca_2\",\n        hue=\"Class\",\n        data=df_pca2d,\n        legend=\"full\",\n        s=30,\n        ax=ax2d\n    )\n    \n    \n    ax3d = fig.add_subplot(122, projection='3d')\n    ax3d.scatter(df_pca3d['pca_1'], df_pca3d['pca_2'], df_pca3d['pca_3'], c=df['Class'], s=30)\n    \n    plt.tight_layout()\n    plt.show()","ad1e5797":"plot_pca_graphs(df)","3f22f967":"from sklearn.manifold import TSNE\n\ndef to_tsne(df, n_components):\n    features_df = df.drop(['Amount', 'Time', 'Class'], axis=1)\n    tsne = TSNE(n_components)\n    tsne.fit(features_df)\n    \n    columns = [f\"tsne_{i+1}\" for i in range(n_components)]\n    df_tsne = pd.DataFrame(tsne.transform(features_df), columns=columns, index=df.index)\n    df_tsne['Class'] = df['Class']\n    return df_tsne\n\ndef plot_tsne_graphs(df):\n    df_tsne2d = to_tsne(df, 2)\n    df_tsne3d = to_tsne(df, 3)\n    \n    fig = plt.figure(figsize=(15, 7))\n    fig.suptitle('t-SNE Representations')\n    \n    ax2d = fig.add_subplot(121)\n    sns.scatterplot(\n        x=\"tsne_1\",\n        y=\"tsne_2\",\n        hue=\"Class\",\n        data=df_tsne2d,\n        legend=\"full\",\n        s=30,\n        ax=ax2d\n    )\n    \n    \n    ax3d = fig.add_subplot(122, projection='3d')\n    ax3d.scatter(df_tsne3d['tsne_1'], df_tsne3d['tsne_2'], df_tsne3d['tsne_3'], c=df_tsne3d['Class'], s=30)\n    \n    plt.tight_layout()\n    plt.show()","f70b6b8e":"def comparative_classification_report(model_name, ground_truth, predictions, oversampled_predictions, undersampled_predictions, metrics):\n    print(model_name)\n    table = PrettyTable()\n    table.field_names = [\"Metric\", \"Orignal Score\", \"Undersampled Score\", \"Oversampled Score\"]\n    for name, metric in metrics.items():\n        original_score = metric(ground_truth, predictions).round(4)\n        undersampled_score = metric(ground_truth, undersampled_predictions).round(4)\n        oversampled_score = metric(ground_truth, oversampled_predictions).round(4)\n        table.add_row([name, original_score, undersampled_score, oversampled_score])\n    print(table)","7d95e9f2":"comparative_classification_report(\"Naive Bayes\", y_test, nb_predictions, nb_os_predictions, nb_us_predictions, metrics=classification_metrics)","de345c98":"from sklearn.tree import DecisionTreeClassifier, plot_tree\n\ndtc_model = DecisionTreeClassifier()\ndtc_model.fit(X_train, y_train)\ndtc_predictions = dtc_model.predict(X_test)","3fd013cb":"dtc_us_model = DecisionTreeClassifier()\ndtc_us_model.fit(X_us_train, y_us_train)\ndtc_us_predictions = dtc_us_model.predict(X_test)","846251b2":"dtc_os_model = DecisionTreeClassifier()\ndtc_os_model.fit(X_os_train, y_os_train)\ndtc_os_predictions = dtc_os_model.predict(X_test)","4a4617d2":"comparative_classification_report(\"Decision Tree Classifier\", y_test, dtc_predictions, dtc_os_predictions, dtc_us_predictions, metrics=classification_metrics)","e478ce2f":"from sklearn.ensemble import RandomForestClassifier\n\nrfc_model = RandomForestClassifier()\nrfc_model.fit(X_train, y_train)\nrfc_predictions = rfc_model.predict(X_test)","54bf2ddd":"rfc_us_model = RandomForestClassifier()\nrfc_us_model.fit(X_us_train, y_us_train)\nrfc_us_predictions = rfc_us_model.predict(X_test)","0e2b19b9":"rfc_os_model = RandomForestClassifier()\nrfc_os_model.fit(X_os_train, y_os_train)\nrfc_os_predictions = rfc_os_model.predict(X_test)","894f6727":"comparative_classification_report(\"Random Forest Classifier\", y_test, rfc_predictions, rfc_os_predictions, rfc_us_predictions, metrics=classification_metrics)","391e79cf":"import tensorflow as tf\nfrom tensorflow import keras","e037c5d7":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndef train_test_validation_split(df):\n#     scaled_df = df.copy()\n#     scaler = StandardScaler()\n#     scaler.fit([scaled_df['Class']])\n#     scaled_df['Class'] = np.squeeze(scaler.transform([scaled_df['Class']]))\n    \n    train_df, test_df = train_test_split(df.drop(['Time'], axis=1), test_size=0.2)\n    train_df, val_df = train_test_split(train_df, test_size=0.2)\n\n    train_features = np.array(train_df.drop(['Class'], axis=1))\n    train_labels = np.array(train_df['Class'])\n\n    val_features = np.array(val_df.drop(['Class'], axis=1))\n    val_labels = np.array(val_df['Class'])\n\n    test_features = np.array(test_df.drop(['Class'], axis=1))\n    test_labels = np.array(test_df['Class'])\n    \n    return (train_features, train_labels), (val_features, val_labels), (test_features, test_labels)","faff8565":"from tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.losses import BinaryCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\ndef create_model(input_shape):\n    model = keras.Sequential([\n        keras.layers.Dense(16, input_shape=input_shape, activation='relu'),\n        keras.layers.Dense(8, activation='relu'),\n        keras.layers.Dropout(0.4),\n        keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    model.compile(\n        optimizer=Adam(lr=0.001),\n        loss=BinaryCrossentropy(),\n        metrics=[Precision(name='precision'), Recall(name='recall'), 'acc']\n    )\n    \n    return model","689a51b1":"def plot_metrics(history):\n    fig = plt.figure(figsize=(10, 7), constrained_layout=False)\n    grid = fig.add_gridspec(2, 2)\n    \n    loss_ax = fig.add_subplot(grid[0, 0])\n    loss_ax.set_ylabel('Loss')\n    loss_ax.set_xlabel('Epoch')\n    loss_ax.semilogy(history.epoch, history.history['loss'], label='Train')\n    loss_ax.semilogy(history.epoch, history.history['val_loss'], label='Validation') \n    \n    accuracy_ax = fig.add_subplot(grid[0, 1])\n    accuracy_ax.set_ylabel('Accuracy')\n    accuracy_ax.set_xlabel('Epoch')\n    accuracy_ax.plot(history.epoch, history.history['acc'], label='Train')\n    accuracy_ax.plot(history.epoch, history.history['val_acc'], label='Validation')\n    \n    precision_ax = fig.add_subplot(grid[1, 1])\n    precision_ax.set_ylabel('Precision')\n    precision_ax.set_xlabel('Epoch')\n    precision_ax.plot(history.epoch, history.history['precision'], label='Train')\n    precision_ax.plot(history.epoch, history.history['val_precision'], label='Validation') \n    \n    recall_ax = fig.add_subplot(grid[1, 0])\n    recall_ax.set_ylabel('Recall')\n    recall_ax.set_xlabel('Epoch')\n    recall_ax.plot(history.epoch, history.history['recall'], label='Train')\n    recall_ax.plot(history.epoch, history.history['val_recall'], label='Validation') \n    \n    plt.tight_layout()\n    plt.legend()\n    plt.show()","2c9b407f":"BATCH_SIZE = 2048\nEPOCHS = 30\n\n(train_features, train_labels), (val_features, val_labels), (test_features, test_labels) = train_test_validation_split(df)","0b9a37ab":"baseline_model = create_model((train_features.shape[-1],))\nbaseline_history = baseline_model.fit(\n    train_features,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(val_features, val_labels)\n)","5c02e17f":"plot_metrics(baseline_history)","426da582":"baseline_model.evaluate(test_features, test_labels)","e94b2ad8":"class_weights = {\n    0: (len(df)\/len(df[ df['Class'] == 0 ])),\n    1: (len(df)\/len(df[ df['Class'] == 1 ]))\n}\n\nweighted_model = create_model((train_features.shape[-1],))\nweighted_history = weighted_model.fit(\n    train_features,\n    train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(val_features, val_labels),\n    class_weight=class_weights\n)","83e4492a":"plot_metrics(weighted_history)","4bc87c6d":"weighted_model.evaluate(test_features, test_labels)","c563ebcb":"def oversampling_train_test_validation_split(df):\n    train_df, test_df = train_test_split(df.drop(['Time'], axis=1), test_size=0.2)\n    \n    features, labels = SMOTE().fit_resample(train_df.drop(['Class'], axis=1), train_df['Class'])\n    converted_train_df = pd.DataFrame(features, columns=list(train_df.columns).remove('Class'))\n    converted_train_df['Class'] = pd.Series(labels)\n    \n    train_df, val_df = train_test_split(converted_train_df, test_size=0.2)\n\n    train_features = np.array(converted_train_df.drop(['Class'], axis=1))\n    train_labels = np.array(converted_train_df['Class'])\n\n    val_features = np.array(val_df.drop(['Class'], axis=1))\n    val_labels = np.array(val_df['Class'])\n\n    test_features = np.array(test_df.drop(['Class'], axis=1))\n    test_labels = np.array(test_df['Class'])\n    \n    return (train_features, train_labels), (val_features, val_labels), (test_features, test_labels)","3fc2e01f":"(os_train_features, os_train_labels), (os_val_features, os_val_labels), (os_test_features, os_test_labels) = oversampling_train_test_validation_split(df)","e5498221":"resampled_model = create_model((train_features.shape[-1],))\nresampled_history = resampled_model.fit(\n    os_train_features,\n    os_train_labels,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_data=(os_val_features, os_val_labels)\n)","4432f5c7":"plot_metrics(resampled_history)","e8e91374":"resampled_model.evaluate(test_features, test_labels)","a6eef97c":"!pip install minisom","d8e92f5a":"from minisom import MiniSom\n\nX = np.array(df.drop(['Amount', 'Class', 'Time'], axis=1))\n\nsom = MiniSom(6, 6, len(X[0]), sigma=0.5, learning_rate=0.5)\nsom.random_weights_init(X)\nsom.train_random(X, 100)","4c71a5de":"quantization_errors = np.linalg.norm(som.quantization(X) - X, axis=1)\nplt.hist(quantization_errors)\nplt.yscale('log', nonposy='clip')","453f92ec":"clustered_df = df.copy()\nclustered_df['Predicted'] = pd.Series([1 if qe > 25 else 0 for qe in quantization_errors])\nclustered_df.head()","a5660255":"correct = 0\nfor predicted, truth in zip(clustered_df['Predicted'], clustered_df['Class']):\n    if predicted == truth:\n        correct += 1\n\nprint(f\"Percent predicted correctly: {round(float(correct)\/len(df) * 100, 3)}%\")","c8930650":"### 4.2 Decision Tree Algorithms\nDecision Tree based algorithms are known to perform well on imbalanced datasets. We can compare the result of our decision tree based algorithms trained on each of the resampled datasets to the results of each of the Naive Bayes models. We'll be testing out two decision tree based algorithms - the Decision Tree Classifier and the Random Forest Classifier","59096a54":"### 4.2.2 Random Forest","c5684f5b":"As before, we have an equal amount of fraudulent and valid transactions, but this time, undersampled so that we fit the minority class's size constraints.","04c23955":"### 4.3.3 Baseline Neural Network\nOur first model will be our baseline model - we'll compare the results of neural networks that we train in the future to the results of this neural network.","51511b7d":"## 4 Classifying Fraudulent Transactions\nThus far, we've created visualizations of the data and it's features, employed different metrics to better assess model performance, and utilized sampling techniques to improve model performance. Our next step is to use the aforementioned techniques to build models with high accuracy and recall without sacrificing precision in the process. In this section, we'll use classification algorithms to detect fraudulent transactions.\n\nOne of our objectives is to evaluate how our model performs on the oversampled and undersampled datasets to seek potential improvements. We can build a new custom classification report that compares our evaluations side by side instead of utilizing three separate tables with similar structures.","9e69ebec":"## 2.3 Sampling Methods\nWe've looked at different metrics that we can use to evaluate models trained on imbalanced datasets, but we haven't yet looked at manipulating the original dataset. There's an old saying that's used pretty commonly in data science - 'garbage in, garbage out'. If we want the best results, we should make sure that the data we feed into the model is as good as possible. \n\nPerhaps the best explanation I've seen comes from the documentation of the `imbalanced-learn` library, which we'll be using to code our sampling techniques.\n\n\"With a greater imbalanced ratio, the decision function favors the class with the larger number of samples, usually referred as the majority class.\" \n\nWe saw this earlier with our Naive Bayes confusion matrix. The model was more likely to misclassify a fraudulent example as valid than it was to misclassify a valid transaction as fraudulent.","0fc5a11b":"Baesd on pure speculation, we can see that the variances of the PCA features of the fraudulent transactions are larger than those of the valid transactions.","5945ce0f":"In addition to the metrics found in a classification report, we've included two additional metrics that we can use to gain additional insight into our model's performance - the cohen's kappa score and the balanced accuracy score. Both of these metrics are useful in scenarios where we have imbalanced datasets. Our primary focus, however, will be on maximizing traditional scores provided in scikit-learn's classification report.\n\nThe following function provides the framework for creating our own classification report.","ee907200":"### 4.3.2 Creating A Model Function\nWe'll be creating multiple models with the same network structure, so we can create a method that can create neural nets without using redundant code. Since our data is relatively structured, we'll use just a single hidden layer, with a dropout layer to prevent overfitting.","7490b33f":"## 2.3.1 Oversampling\nOur first approach is to oversample the minority class. In other words, we want to generate synthetic samples so that our dataset becomes more balanced again. \n\nImbalanced-Learn comes with a few oversampling techniques, chief among them being \n* Random Oversampling\n* Synthetic Minority Oversampling Technique (SMOTE)\n* Adaptive Syntheic (ADASYN)\n\nWe'll be using SMOTE to get our oversampled dataset.","63e8b58b":"Normally, this is an accuracy that we'd be quite happy with, but if we take a look at our confusion matrix we'll quickly see that something's wrong.","5370f752":"It looks like with undersampling, most of our metrics did significantly worse, especially the precision and the accuracy. This was most likely due to the fact that we drastically reduced the size of the dataset to a fraction of a percent of it's former size. \n\nOur oversampling method, however, improved by almost every single metric because of the new balance and the increased dataset size.","1b5c6769":"### 1.4.1 Amount Distributions","2e767188":"We'll also be using a lot of Sci-kit Learn in this section too, so let's build a function that builds three different models trained on the normal, undersampled, and oversampled datasets.","ea679118":"### 2.2.3 Other Measurements to Consider\nWe won't go in-depth with these measurements, but the following are other measurements that can be used for measuring model success in imbalanced scenarios.\n* Cohen Kappa Score - the best description of Cohen's Kappa comes from [this blog post](https:\/\/machinelearningmastery.com\/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset\/). Simply put, it's \"classification accuracy normaized by the imbalance of the classes in the data\".\n* Balanced Accuracy - the average of the proportions correctly predicted each class. [This article](http:\/\/mvpa.blogspot.com\/2015\/12\/balanced-accuracy-what-and-why.html) provides a well written explanation of balanced accuracy.\n* Area Under the Precision Recall Curve - we can visualize the relationship between model precision and recall using a ROC curve, and we can measure model performance by measuring the value of the area under the ROC curve.","9b942ed7":"With the SOM, we can detect potential fraud by analzying the quantiation errors, or the error produced during the reduction","3ef0edf7":"The model's precision, binary accuracy, and loss all look good. However, the model's recall isn't nearly as high as we would like it to be. T Remember that recall is the model's ability to identify a positive value. Our goal with these next two approaches is to improve our model's capability to identify fraudulent transactions.","7b73bc9f":"### 4.3.4. Class Weight Manipulation\nSince we don't have that many fraudulent transactions in our dataset, our model will bias towards classifying transactions as legitimate. To compensate for this difference, we're going to weight the fraudulent transactions more heavily so that the model pays more attention to the fraudulent examples. ","f3527ad7":"Our undersampling model underperformed once again with the decision tree classifier (once again, likely due to a smaller training dataset), with all scores except for recall (which increased) taking a nosedive. Surprisingly, our oversampled model also underperformed as well - all measurements came out with similar or worse results to our original datasets.","daf2538d":"## 3. Dimensionality Reduction","fed75ce5":"To see the outliers, we can use boxplots.","117ed193":"We'll also create a function to plot our metrics during training. For the sake of simplicity, we'll only consider precision, recall, loss, and accuracy. Since we have a relatively large range of potential values for our loss, we'll take the logarithm of the loss over time to reduce our range.","6dc6968b":"Now that we have a balanced dataset, we need to get eliminate the initial class weights because it might cause our dataset to overfit.","f7a73da9":"To summarize in clear terms in [this article](https:\/\/towardsdatascience.com\/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28)\n* High precision + high recall - the class is perfectly handled by the model\n* Low precision + high recall - the class is well detected but the model also includes points of other classes in it\n* High precision + low recall - the model can't detect the class well but is highly trustable when it does\n* Low recall + low precision - the class is poorly handled by the model\n\nAlthough we would prefer high precision and high recall, our goal is to prioritize recall.","0a720f62":"### 2.1 Motivations\n\nWe discovered previously that our data is imbalanced - 99.83% are completely valid transactions, while 0.17% are fraudulent transactions. To demonstrate the potentially disastrous effects this has, let's take an example of a model that opts to classify every single example according to the most frequent label of the dataset. ","7b5aa3ff":"### 1.4.2 Amount Over Time","170a8d2a":"## 5. Clustering Algorithms\nThus far, we've used various classification algorithms and sampling techniques to correctly identify fraudulent transactions. However, what if we didn't have a label. We wouldn't be able to use traditional classification techniques. This is where clustering comes in. There are a wide variety of clustering algorithms, and advantages to each of them. We'll be using a Self Organizing Map (SOM), an unsupervised neural network baesd algorithm that performs rather well on high dimensional data. This algorithm is relatively complex to code from scratch, but there are a few libraries that make experimentation easy.","c9d14ba7":"Something interesting to note - the recall increased, but it came at the cost of precision like we predicted. The low f1 score falls in line with this assumption as well. Our accuracy fell tremendously as well. This makes sense, as we've decreased the number of samples in our dataset to a fraction of a percent of its original size.\n\nLet's compare the results side by side.","1485b0ee":"We'll be using the scores of our naive bayes model as a benchmark for future models.","f03860bb":"### 3.1 Principle Component Analysis (PCA)\nOur original dataset came with clearly named, of which, only three were named purposefully - `Amount`, `Class`, `Time`. However, we also got `V1-V28`, which were obscured using PCA. In this section, we'll be using PCA again to reduce the featureset to consist of only two or three features so that we can plot the two groups.\n\nIf you're interested in learning how PCA works, I'd recommend checking out [this article](https:\/\/builtin.com\/data-science\/step-step-explanation-principal-component-analysis).\n\nWe've defined two helper methods to aid with our PCA analysis\n* `to_pca` - reduces a dataset into a pandas dataframe with `n` PCA features.\n* `plot_pca_graphs` - graphs PCA transformed dataframes onto 2d and 3d plot","b3e0cf38":"### 4.3.1 Creating Our Dataset\nIn this section, we'll be splitting our dataset into training, validation, and testing sets. We'll be using the validation set to evaluate our metrics at each epoch. Note that we won't be using our resampled datasets in this section (yet). First, we'll try to optimize our model for the imbalanced dataset, and the move onto resampling our data. We'll also scale the Amount column to make the gradients easier to compute for our networks.","836f5176":"### 4.2.1 Decision Tree Classifier\nThe goal of decision trees is to create a tree of simple rules that can be used for classification. In practice, they're quite simple to understand, don't require much data preparation (i.e. no normalization needed), and easy to interpret. We'll be taking advantage of the white box based approach of the decision tree classifier by visualizing the rules using Scikit-Learn's `plot_tree` function.","955f8469":"### 3.2 t-SNE\nAnother popular technique for visualizing high dimensional dataset is t-SNE. We'll recreate the methods and plots above, but this time, using t-SNE.","f19c0287":"# Detecting Credit Card Fraud with Neural Nets","aa7911b8":"### 1.4 Fraudulent vs. Valid Transactions","fd6ca537":"### 4.1 Setting Our Baseline\nWe'll be using Naive Bayes as our baseline model. Any models that we build in the future will be compared to results of this model. We've build the model before, so we'll just log the performance below.","4c216358":"## 2.3.2 Undersampling\nOur second method is to undersample the majority class. We have a few techniques that we can use.\n* NearMiss\n* ClusterCentroids\n* Random Undersampling\n* EditedNearestNeighbours\n\nWe'll be using NearMiss for our purposes.","7460e53c":"Data visualization is extremely important in understanding your data. After all, if a picture is worth a thousand words, a graph or plot should be worth even more than that. This becomes problematic, however, when the number of features that we have to work with extends beyond three dimensions. In the Exploratory Data Analysis (EDA) section, we explored individual features (Amount, Time, etc) and the relationship between these features. We have yet to visualize all of the features on a single plot. To do this, we use a technique called dimensionality reduction - which we can use to reduce the number of dimensions while retaining as much information as possible.\n\nAnother key benefit is that computations and algorithms run faster because the dataset because it's a great bit smaller than before. One thing to keep in mind though, is that reducing dimensions does eliminate some of the accuracy.\n\nWe'll be exploring two well known implementations - PCA and t-SNE.","ccb42d1e":"As you can see, we have an equal amount of every class. Let's see how the algorithm stacks up.","b6f3bb71":"### 1.4.3 PCA Feature Distributions","a81be85e":"Some things of interest to note\n* Our Naive Bayes classifier scored significantly lower than our dummy classifier\n* If we look at our confusion matrix, about a third of the fraudulent examples were not labeled as such, while only a fraction of a percent of the valid transactions were labeled as such. \n\nThese are definitely both symptoms of a larger problem. Let's look into different ways of dealing with imbalanced data. ","493c2bcb":"### 2.2.1 Precision and Recall\nOur confusion matrix revealed that our algorithm was much more likely to incorrectly classify a fraudulent transaction than a valid transaction. If our model serves primarily to catch potentially fraudulent transactions, then a better course of actio would be to assess **recall**, or a measure a model's ability to identify positive (in this case, fraudulent transactions) examples.\n\nThe equation for recall is the following, where $TP$ and $FN$ indicate true positives and false negatives.\n$$\nR = \\dfrac{TP}{TP + FN}\n$$\n\nLet's see how our dummy and naive bayes models performed by evaluating their recall. We could use a confusion matrix to generate values for $TP$ and $FN$ and then plug them into the formula, but Scikit-learn already has a recall function built in.","622bce1a":"We can immediately see that there's something wrong here - we never classified any false positives or true positives. I know this exercise seems pointless, considering we're attempting to deconstruct a classifier that just returns the most frequent label, but it provides us with an example as to why traditional accuracy scores can be misleading in situations where we have imbalanced datasets.\n\nLet's try making predictions using an actual classifier.","48d25be1":"### 1.2 Value Counts\n* Our dataset is massively imbalanced. 99.83% of our data are valid transactions. We'll have to compensate for this imbalance using various sampling techniques and utilization of different metrics.\n* We have almost 300,000 examples to work with, which means that models that require large amounts of data can be used here.","70908389":"## 1. Exploratory Data Analysis","0851a3cc":"### 4.3 Neural Networks\nWe'll be using TensorFlow and Keras to create our neural network. There is a [great tutorial](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data) in the TensorFlow documentation concerning classification with imbalanced datasets. This next section is heavily inspired by that tutorial.","7a3bdff9":"As we expected, the f1 score penalized the extreme values of recall and precision of the dummy classifier with a score of 0.","a32c82a6":"One of the most useful applications of machine learning is fraud detection. In this notebook, we'll be exploring the application of machine learning in detecting fraudulent transactions using Neural Networks. \n\nNote: Due to computation restrictions, I wasn't able to ","fbf3cda4":"### 2.2 Using Different Metrics\nThis section was heavily inspired by [this article](https:\/\/towardsdatascience.com\/beyond-accuracy-precision-and-recall-3da06bea9f6c). If you're looking for a great resource for learning about precision, recall, f1 scores, and their applications, give this article a read. \n\nWe've seen that accuracy scores can be extremely misleading. Even though our dummy classifier simply classified every example as valid, we still reached a 99.83% accuracy. \n\nWe've already utilized confusion matrix visualizations to diagnose a few of the problems that our models had. Let's now consider a few quantitative metrics that we can use to test the capabilities of our model.","c66bcde6":"Let's build our classification report. ","7e716e9a":"All of our scores, especially recall and balanced accuracy, seemed to either stay the same or increase. Let's see if the same holds for undersampling.","833311bb":"### 4.3.5 Oversampling\nNext, we'll perform oversampling to balance the dataset. The function below splits the data into training, validation, and testing sets like before, but extrapolates more samples from both the training and validation sets to prevent overfitting and maintain the integrity of the test dataset.","c869056b":"Since our dataset is skewed, then we can take the square root of each series to increase the spread of the dataset. From our dataset, we can see that\n* Fraudulent transactions have an upper bound of just over $2000.\n* The dataset seems to be very heavily skewed to right, which makes sense, since most every-day transactions are of minute value.\n* The dataset seems to have a few outliers.","6dbd2e40":"Our dummy classifier had a 0 recall score. Since every example was classified as valid, there were no true positives. In other words, every time the dummy classifier was given a positive (fraudulent) transaction, it failed to classify it as such.\n\nNow our goal becomes to maximize recall. There's a catch, however. In the context of our problem, our true postives are frauds correctly classified and our false negatives are frauds wrongly labeled as not frauds. If we wanted to max out our recall, then we could eliminate false negatives altogether by classifying all transactions as fraudulent. We would catch all the fraudulent transactions, but we would also catch all of the valid transactions too. This is, of course, not an ideal situation.\n\nEnter precision. This is the model's ability to identify only relevant data points. The equation for precision comes as follows, once again considering $TP$ as true positives and $FP$ as false positives.\n$$\nP = \\dfrac{TP}{TP + FP}\n$$\n\nRemember that in the context of our problem, our true positives are frauds correctly classified, and false positives are valid transactions labeled as fraudulent. Considering that valid transactions comprise, classifying all transactions as fraudulent would increase our false positives to almost 100% of the dataset, which in turn would make precision 0. ","fc236f23":"The purpose of the SOM is to convert high dimensional data into two dimensional data. ","64b9c665":"Let's test out our custom classification report by evaluating our Naive Bayes model.","09d47d12":"### 1.3 Relationship between PCA Features\n* As indicated by the correlation heatmap, there is no correlation between any of the PCA (principal component analysis) features. This is to be expe\n* The Amount, Time, and Class columns seem to have fairly unnoteworthy correlations with most features, save a few exceptions.\n* The Amount column has low correlation with V7 and V21","cb68f588":"## 2 Dealing with Imbalanced Datasets \n","24fd00df":"### 2.2.2 F1 Score\nAlthough our primary goal is to maximize the recall, we also want to make sure that our precision isn't too low either. To find an optimal blend of both precision and recall, we can use an f1 score using the following equation.\n\n$$\nF1 = 2 * \\dfrac{precision * recall}{precision + recall}\n$$\n\nNote that we use this formula instead of a simple average because it penalizes extreme values of either metric. ","1e92221d":"Random forest makes use of decision trees by training multiple decision trees on sub-samples of the datasets and averaging the results to improve predictions and prevent overfitting. Let's see how it performs on our dataset.","bc8a123c":"### 1.1 Analyzing our DataFrame\n* We have 284,807 training examples to work with\n* None of our training samples are null\n* The columns V1-V28 are PCA generated columns, probably to obscure sensitive information\n* The number of PCA columns means that the original dataset contained at least 28 features before obfuscation."}}