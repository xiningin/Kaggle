{"cell_type":{"5378bc24":"code","a11be13c":"code","de617818":"code","3412ca25":"code","f8874095":"code","f2f6d365":"code","2013e6ad":"code","bc8e972c":"code","1bd580f8":"code","755a0f07":"code","7736bca8":"code","ca309df8":"code","ae708ef4":"code","203d3fc6":"code","22a75160":"code","aecea2c6":"code","2b9c99fe":"code","15389094":"code","ed4fbdd3":"code","744b49e9":"code","f5bb1c81":"code","d4b6c77f":"code","6f2f4de2":"markdown","a57086da":"markdown","292a696b":"markdown","a5751db5":"markdown","9c9553f9":"markdown","5b21f725":"markdown","497daaa1":"markdown","6256e907":"markdown","2a0533f3":"markdown","d9e6dbee":"markdown","e9e2b0c8":"markdown","69582543":"markdown","fa01adcd":"markdown"},"source":{"5378bc24":"import statistics\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nimport optuna.integration.lightgbm as lgb\nfrom optuna.integration.lightgbm import LightGBMTunerCV, LightGBMTuner\nimport category_encoders\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nimport warnings\n\nwarnings.filterwarnings('ignore')","a11be13c":"class Config:\n    RANDOM_SEED = 42\n    NUM_FOLDS = 5\n    TARGET_COL_NAME = \"song_popularity\"\n    CATEGORICAL_COLS = [\"key\", \"audio_mode\", \"time_signature\"]\n    EARLY_STOPPING = 500\n    RESULTS_FILE = \"model_execution_results.pkl\"\n    MODEL = \"LGBM\"\n\nDATA_PATH = \"\/kaggle\/input\/song-popularity-prediction\/\"","de617818":"df_train = pd.read_csv(DATA_PATH + \"train.csv\")\ndf_test = pd.read_csv(DATA_PATH + \"test.csv\")","3412ca25":"# split the training dataframe into kfolds for cross validation. We do this before any processing is done\n# on the data. We use stratified kfold if the target distribution is unbalanced\ndef strat_kfold_dataframe(df, target_col_name, num_folds=5):\n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    # randomize of shuffle the rows of dataframe before splitting is done\n    df = df.sample(frac=1, random_state=Config.RANDOM_SEED).reset_index(drop=True)\n    # get the target data\n    y = df[target_col_name].values\n    skf = model_selection.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_SEED)\n    for fold, (train_index, val_index) in enumerate(skf.split(X=df, y=y)):\n        df.loc[val_index, \"kfold\"] = fold    \n    return df     \n\ndf_train = strat_kfold_dataframe(df_train, target_col_name=Config.TARGET_COL_NAME, num_folds=Config.NUM_FOLDS)\ndf_train.head()","f8874095":"cont_cols = ['song_duration_ms', 'acousticness', 'danceability', 'energy', \n            'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'audio_valence']","f2f6d365":"def add_missing_col(df, cols_with_nulls):\n    for col_name in cols_with_nulls:        \n        df[col_name + \"_missing\"] = [int(item) for item in df[col_name].isna().values]\n    return df        \n\ntrain_cols_withnulls = [col for col in df_train.columns if df_train[col].isnull().any()]\ntest_cols_withnulls = [col for col in df_test.columns if df_test[col].isnull().any()]\ndf_train = add_missing_col(df_train, train_cols_withnulls)\ndf_test = add_missing_col(df_test, test_cols_withnulls)","2013e6ad":"def impute_df_col(df, col_name, imputer):\n    imputed_col = imputer.fit_transform(df[col_name].to_numpy().reshape(-1, 1))\n    return pd.Series(imputed_col.reshape(-1))    ","bc8e972c":"def impute_missing_values(df, cols, col_type=\"cont\"):    \n    if col_type == \"cont\":\n        imputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n    elif col_type == \"cat\":\n        imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")            \n    for col in cols:\n        df[col] = impute_df_col(df, col, imputer)\n    return df\n\ndf_train = impute_missing_values(df_train, Config.CATEGORICAL_COLS, col_type=\"cat\")\ndf_train = impute_missing_values(df_train, cont_cols, col_type=\"cont\")\ndf_test = impute_missing_values(df_test, Config.CATEGORICAL_COLS, col_type=\"cat\")\ndf_test = impute_missing_values(df_test, cont_cols, col_type=\"cont\")","1bd580f8":"df_train = pd.get_dummies(df_train, prefix=Config.CATEGORICAL_COLS, columns=Config.CATEGORICAL_COLS)\ndf_test = pd.get_dummies(df_test, prefix=Config.CATEGORICAL_COLS, columns=Config.CATEGORICAL_COLS)","755a0f07":"cols_to_leave = [\"id\", \"kfold\", \"song_popularity_proba\", Config.TARGET_COL_NAME]\ncol_names = [item for item in df_train.columns.values.tolist() if item not in cols_to_leave]        ","7736bca8":"def get_fold_data(fold, df):\n    df_train = df[df.kfold != fold]\n    df_val = df[df.kfold == fold]    \n    X_train = df_train[col_names]\n    y_train = df_train[Config.TARGET_COL_NAME]\n    X_val = df_val[col_names]\n    y_val = df_val[Config.TARGET_COL_NAME]\n    return X_train, y_train, X_val, y_val ","ca309df8":"def run_training(train_df, train_y, val_df, val_y, params=None, callbacks=None):\n    train_data = lgbm.Dataset(\n            data=train_df[col_names], label=train_y, feature_name=col_names\n        )\n    val_data = lgbm.Dataset(\n            data=val_df[col_names], label=val_y, feature_name=col_names,\n            reference=train_data\n        )    \n    if callbacks is not None:        \n        model = lgbm.train(\n                    params,\n                    train_set=train_data,                \n                    valid_sets=val_data,\n                    verbose_eval=-1,\n                    callbacks=callbacks\n                )\n    else:\n        model = lgbm.train(\n                    params,\n                    train_set=train_data,                \n                    valid_sets=val_data,\n                    verbose_eval=-1\n                )       \n    val_preds = model.predict(val_df, num_iteration=model.best_iteration)    \n    auc = roc_auc_score(val_y, val_preds)\n    return auc, val_preds, model    ","ae708ef4":"def tune_params(train_df, train_y, params=None):\n    train_data = lgbm.Dataset(\n            data=train_df[col_names], label=train_y, feature_name=col_names#, \n            #categorical_feature=Config.CATEGORICAL_COLS\n        )   \n    lgbmtuner_cv = LightGBMTunerCV(\n        params,\n        train_set=train_data,        \n        stratified=True,\n        shuffle=True,\n        nfold=Config.NUM_FOLDS,\n        verbose_eval=-1\n    ) \n    lgbmtuner_cv.run()                \n    print(\"Best Params: \", lgbmtuner_cv.best_params)    \n    print(\"Best score: \", lgbmtuner_cv.best_score)    \n    return lgbmtuner_cv    ","203d3fc6":"# params = {\n#         \"objective\": \"binary\",\n#         \"metric\": \"auc\",\n#         \"verbosity\": -1,\n#         \"boosting_type\": \"gbdt\",\n#     }\n\n# train_y = df_train[Config.TARGET_COL_NAME]\n# tuned_model = tune_params(df_train, train_y, params)","22a75160":"# tuning with LightGBMTunerCV\n\nmodel_params = {\n    'objective': 'binary', \n    'metric': 'auc', \n    'verbose': -1, \n    'boosting_type': 'gbdt', \n    'feature_pre_filter': False, \n    'lambda_l1': 9.439044618205312, \n    'lambda_l2': 0.615750336486198, \n    'num_leaves': 3, \n    'feature_fraction': 0.62, \n    'bagging_fraction': 0.5286479709465361, \n    'bagging_freq': 1, \n    'min_child_samples': 20,    \n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    'num_iterations': 10000,\n    \"early_stopping_round\": 500\n    }","aecea2c6":"fold_metrics_model = []\ntest_preds = {}\ndf_train[\"song_popularity_proba\"] = 0.0\nfor fold in range(Config.NUM_FOLDS):\n    train_df, train_y, val_df, val_y = get_fold_data(fold, df_train) \n    test_df = df_test[col_names]    \n    fold_auc_score, fold_val_preds, model = run_training(train_df, train_y, val_df, val_y, params=model_params)\n    print(f\"fold {fold } auc score = {fold_auc_score}\")\n    # add the validation probability predictions for the fold to a new column in train data\n    df_train.loc[df_train.kfold == fold, \"song_popularity_proba\"] = fold_val_preds    \n    fold_test_preds = model.predict(test_df, num_iteration=model.best_iteration)\n    pred_col_name = f\"fold_{fold}_test_preds\"\n    test_preds[pred_col_name] = fold_test_preds \n    fold_metrics_model.append((round(fold_auc_score, 6), model))","2b9c99fe":"fold_metrics = [item[0] for item in fold_metrics_model]\nprint(f\"auc scores = {fold_metrics}\")    \ncv_auc_mean = statistics.mean(fold_metrics)\ncv_auc_stdev = statistics.stdev(fold_metrics)\nprint(f\"mean auc across folds = {cv_auc_mean}, auc stdev across folds = {cv_auc_stdev}\")","15389094":"import os\n\nif os.path.exists(Config.RESULTS_FILE):\n    df_execution_results = pd.read_pickle(Config.RESULTS_FILE)\nelse:\n    df_execution_results = pd.DataFrame({\n        \"model\": pd.Series(dtype=\"str\"),\n        \"fold_auc_scores\": pd.Series(dtype=\"object\"),\n        \"mean_auc\":pd.Series(dtype=\"float64\"),\n        \"auc_stdev\":pd.Series(dtype=\"float64\"),\n        \"model_params\": pd.Series(dtype=\"str\"),\n        \"input_features_used\": pd.Series(dtype=\"str\"),\n        \"Imputation\": pd.Series(dtype=\"bool\"),\n        \"Categorical_Encoding\": pd.Series(dtype=\"bool\")\n    })    ","ed4fbdd3":"execution_results = pd.DataFrame({\n    \"model\": [Config.MODEL + \"_RANDOMSEED_\" + str(Config.RANDOM_SEED)],\n    \"fold_auc_scores\": [fold_metrics],\n    \"mean_auc\": [cv_auc_mean],\n    \"auc_stdev\": [cv_auc_stdev],\n    \"model_params\": [model_params],\n    \"input_features_used\": [col_names],\n    \"Imputation\": [False],\n    \"Categorical_Encoding\": [False]\n})\nmodel_execution_results = pd.concat([df_execution_results, execution_results], ignore_index=True)\nmodel_execution_results","744b49e9":"model_execution_results.to_pickle(\"model_execution_results.pkl\")","f5bb1c81":"df_test_preds = pd.DataFrame(test_preds)\ntest_pred_cols = [f\"fold_{fold}_test_preds\" for fold in range(Config.NUM_FOLDS)]\ndf_test_preds[\"mean_test_pred\"] = df_test_preds[test_pred_cols].mean(axis=1)\nprint(f\"Completed prediction for {len(df_test)} test rows\")\ndf_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ndf_submission['song_popularity']= df_test_preds[\"mean_test_pred\"]\ndf_submission.to_csv('submission_lgbm.csv',index=False)\ndf_submission.head()","d4b6c77f":"lgbm_val_preds = df_train[[\"id\", \"song_popularity_proba\", \"song_popularity\"]]\nlgbm_val_preds.to_csv(\"lgbm_val_preds.csv\")\nprint(\"Saved validation predictions for all folds to csv\")","6f2f4de2":"### The cross validation statistics","a57086da":"### Select the features to use in training","292a696b":"### Save OOF preds to csv (in case you want to use these for model blending later)","a5751db5":"### Hyperparameter tuning with optuna","9c9553f9":"### Use \"mean\" imputing strategy for continuous and \"most frequent\" for categorical features","5b21f725":"Uncomment if you want to run tuning","497daaa1":"### Predictions on the test set\n( Using \"average\" ensembling from the models returned by the training fold )","6256e907":"### Categorical column encoding (one hot) using pandas","2a0533f3":"### Recording the results of the model training run","d9e6dbee":"### Creater marker columns for missing values <br>\nHoping it provides more signal to the model","e9e2b0c8":"### Split the train data into k folds","69582543":"### Load the train and test data","fa01adcd":"### The training loop"}}