{"cell_type":{"fe4c193a":"code","bf31a7c4":"code","a0db3833":"code","c57c9109":"code","1ba8e1a5":"code","5316befe":"code","91d56aa2":"code","cee9ca73":"code","3c01ef0f":"code","b46d009e":"code","943a59f2":"code","51ae0db5":"code","4e8d38a5":"code","79019b88":"code","3b62d33a":"code","f391c19f":"code","4a79815e":"markdown","96fbea6f":"markdown","b3fee3d7":"markdown","a8672969":"markdown","e99e9a83":"markdown","bcd589d1":"markdown","0f436e03":"markdown","f8e0f9c0":"markdown","b0064985":"markdown","d1305bcb":"markdown"},"source":{"fe4c193a":"RETRAIN_FIRST_LEVEL_MODELS = False\nRETRAIN_META_MODEL = True","bf31a7c4":"# Numpy and pandas!\nimport numpy as np\nimport pandas as pd\n\n# Input files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Python\nimport pickle\nimport math\nimport re\nfrom datetime import datetime\nfrom itertools import product\n\nimport matplotlib.pyplot as plt\n\n# ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom xgboost import XGBRegressor, plot_importance\n\npd.set_option('display.float_format', '{:.2f}'.format)","a0db3833":"# Import all input CSVs\nitems = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nshops = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\ncats = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv\")","c57c9109":"#time_group = 'date_block_num'\ntrain['dtime'] = pd.to_datetime(train['date'], format='%d.%m.%Y')\n\ntime_group = 'dtime'\n# shops_to_plot = [20, 9]  # Weird shops\nshops_to_plot = []  # All\n\nto_plot = train.copy()\nif shops_to_plot:\n    to_plot = to_plot[to_plot.shop_id.isin(shops_to_plot)]\n\nto_plot = to_plot.groupby([time_group, 'shop_id']).item_cnt_day.sum().reset_index()\nto_plot.set_index(time_group)\nto_plot.sort_index(inplace=True)\n\nfig = plt.figure(figsize = (35, 15))\nax1 = fig.subplots()\nfor s in to_plot.shop_id.unique():\n    shop_plot_X = to_plot[to_plot.shop_id == s][time_group]\n    shop_plot_Y = to_plot[to_plot.shop_id == s]['item_cnt_day']\n    ax1.plot(shop_plot_X, shop_plot_Y, c=f\"C{s}\", label=f\"Shop {s}\")\n    ax1.legend()\n","1ba8e1a5":"known_items = train['item_id'].unique()\nunknown_items = test[~test.item_id.isin(known_items)]['item_id'].unique()\nprint(len(known_items))\nprint(len(unknown_items))","5316befe":"# Outliers\ntrain = train[(train.item_price < 300000) & (train.item_cnt_day < 1000)]\ntrain = train[train.item_price > 0].reset_index(drop = True)\n\n# Duplicated shops\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11\n\nshops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', \"shop_name\"] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops[\"city\"] = shops.shop_name.str.split(\" \").map( lambda x: x[0] )\nshops[\"category\"] = shops.shop_name.str.split(\" \").map( lambda x: x[1] )\nshops.loc[shops.city == \"!\u042f\u043a\u0443\u0442\u0441\u043a\", \"city\"] = \"\u042f\u043a\u0443\u0442\u0441\u043a\"\n\n# Only keep shop category if there are 5 or more shops of that category, the rest are grouped as \"other\".\ncategory = []\nfor cat in shops.category.unique():\n    if len(shops[shops.category == cat]) >= 5:\n        category.append(cat)\nshops.category = shops.category.apply( lambda x: x if (x in category) else \"other\" )\n\nshops[\"shop_category\"] = LabelEncoder().fit_transform( shops.category )\nshops[\"shop_city\"] = LabelEncoder().fit_transform( shops.city )\nshops = shops[[\"shop_id\", \"shop_category\", \"shop_city\"]]\n\ncats[\"type_code\"] = cats.item_category_name.apply( lambda x: x.split(\" \")[0] ).astype(str)\ncats.loc[ (cats.type_code == \"\u0418\u0433\u0440\u043e\u0432\u044b\u0435\")| (cats.type_code == \"\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b\"), \"category\" ] = \"\u0418\u0433\u0440\u044b\"\ncategory = []\nfor cat in cats.type_code.unique():\n    if len(cats[cats.type_code == cat]) >= 5: \n        category.append( cat )\ncats.type_code = cats.type_code.apply(lambda x: x if (x in category) else \"etc\")\ncats.type_code = LabelEncoder().fit_transform(cats.type_code)\ncats[\"split\"] = cats.item_category_name.apply(lambda x: x.split(\"-\"))\ncats[\"subtype\"] = cats.split.apply(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats[\"subtype_code\"] = LabelEncoder().fit_transform( cats[\"subtype\"] )\ncats = cats[[\"item_category_id\", \"subtype_code\", \"type_code\"]]\n\ndef name_correction(x):\n    x = x.lower() # all letters lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x\n\n# split item names by first bracket\nitems[\"name1\"], items[\"name2\"] = items.item_name.str.split(\"[\", 1).str\nitems[\"name1\"], items[\"name3\"] = items.item_name.str.split(\"(\", 1).str\n\n# replace special characters and turn to lower case\nitems[\"name2\"] = items.name2.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\nitems[\"name3\"] = items.name3.str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', \" \").str.lower()\n\n# fill nulls with '0'\nitems = items.fillna('0')\nitems[\"item_name\"] = items[\"item_name\"].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems.name2 = items.name2.apply( lambda x: x[:-1] if x !=\"0\" else \"0\")\nitems[\"type\"] = items.name2.apply(lambda x: x[0:8] if x.split(\" \")[0] == \"xbox\" else x.split(\" \")[0] )\nitems.loc[(items.type == \"x360\") | (items.type == \"xbox360\") | (items.type == \"xbox 360\") ,\"type\"] = \"xbox 360\"\nitems.loc[ items.type == \"\", \"type\"] = \"mac\"\nitems.type = items.type.apply( lambda x: x.replace(\" \", \"\") )\nitems.loc[ (items.type == 'pc' )| (items.type == 'p\u0441') | (items.type == \"pc\"), \"type\" ] = \"pc\"\nitems.loc[ items.type == '\u0440s3' , \"type\"] = \"ps3\"\ngroup_sum = items.groupby([\"type\"]).agg({\"item_id\": \"count\"})\ngroup_sum = group_sum.reset_index()\ndrop_cols = []\nfor cat in group_sum.type.unique():\n    if group_sum.loc[(group_sum.type == cat), \"item_id\"].values[0] <40:\n        drop_cols.append(cat)\nitems.name2 = items.name2.apply( lambda x: \"other\" if (x in drop_cols) else x )\nitems = items.drop([\"type\"], axis = 1)\n\nitems.name2 = LabelEncoder().fit_transform(items.name2)\nitems.name3 = LabelEncoder().fit_transform(items.name3)\n\nitems.drop([\"item_name\", \"name1\"],axis=1, inplace=True)","91d56aa2":"# Let's first create a matrix with combinations of date_block_num, shop_id and item_id for the moments that we have data.\n# Observe that if we were to create all possible combinations using:\n#     matrix = pd.DataFrame(np.vstack(\n#         np.array(list(product(train.date_block_num.unique(), train.shop_id.unique(), train.item_id.unique())), dtype = np.int16)), columns=cols)\n# We would end up with a quite sparse 42 260 028 rows matrix, which leads me to overflow problems\n\n# Create matrix with every possible combination with entries for each date_block_num\nmatrix = []\ncols  = [\"date_block_num\", \"shop_id\", \"item_id\"]\nfor i in range(34):\n    sales = train[train.date_block_num == i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype = np.int16))\n\nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\n# 11 128 004 rows matrix created\n\n# Downcast some types to save space\nmatrix[\"date_block_num\"] = matrix[\"date_block_num\"].astype(np.int8)\nmatrix[\"shop_id\"] = matrix[\"shop_id\"].astype(np.int8)\nmatrix[\"item_id\"] = matrix[\"item_id\"].astype(np.int16)\nmatrix.sort_values(cols, inplace = True)\n\n# Create item_cnt_month, our target\ngroup = train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg({\"item_cnt_day\": [\"sum\"]})\ngroup.columns = [\"item_cnt_month\"]\ngroup.reset_index(inplace = True)\nmatrix = pd.merge(matrix, group, on=cols, how=\"left\")\nmatrix[\"item_cnt_month\"] = matrix[\"item_cnt_month\"].fillna(0).astype(np.float32)\n\n# Concat test set at the end\ntest[\"date_block_num\"] = 34\ntest[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\ntest[\"shop_id\"] = test.shop_id.astype(np.int8)\ntest[\"item_id\"] = test.item_id.astype(np.int16)\n\nmatrix = pd.concat([matrix, test.drop([\"ID\"], axis=1)], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace = True)\n\n# Merge other tables into matrix\nmatrix = pd.merge(matrix, shops, on=[\"shop_id\"], how=\"left\")\nmatrix = pd.merge(matrix, items, on=[\"item_id\"], how=\"left\")\nmatrix = pd.merge(matrix, cats, on=[\"item_category_id\"], how=\"left\")\nmatrix[\"shop_city\"] = matrix[\"shop_city\"].astype(np.int8)\nmatrix[\"shop_category\"] = matrix[\"shop_category\"].astype(np.int8)\nmatrix[\"item_category_id\"] = matrix[\"item_category_id\"].astype(np.int8)\nmatrix[\"subtype_code\"] = matrix[\"subtype_code\"].astype(np.int8)\nmatrix[\"name2\"] = matrix[\"name2\"].astype(np.int8)\nmatrix[\"name3\"] = matrix[\"name3\"].astype(np.int16)\nmatrix[\"type_code\"] = matrix[\"type_code\"].astype(np.int8)\n\n# For seasonality\nmatrix[\"month\"] = matrix[\"date_block_num\"] % 12\n\n# Lag features\ndef lag_feature(df, lags, cols):\n    for col in cols:\n        tmp = df[[\"date_block_num\", \"shop_id\",\"item_id\", col]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = [\"date_block_num\", \"shop_id\", \"item_id\", col + \"_lag_\" + str(i)]\n            shifted.date_block_num = shifted.date_block_num + i\n            df = pd.merge(df, shifted, on=['date_block_num', 'shop_id', 'item_id'], how='left')\n    return df\n\nmatrix = lag_feature(matrix, [1, 2, 3], ['item_cnt_month'])\n\n# Trends\nmatrix['item_cnt_month_1dev_1'] = matrix['item_cnt_month_lag_1'] - matrix['item_cnt_month_lag_2']\nmatrix['item_cnt_month_1dev_2'] = matrix['item_cnt_month_lag_2'] - matrix['item_cnt_month_lag_3']\nmatrix['item_cnt_month_2dev'] = matrix['item_cnt_month_1dev_1'] - matrix['item_cnt_month_1dev_2']\n\n# Mean encoding of (item_id, shop_id) tuple\n# Here we use the time series approximation where we only make use of known data at each moment (i.e. data with date_block_num value less than the block we are encoding at each moment)\nprint(\"Creating mean encoded features. This might take a couple of minutes...\")\n\ndef add_mean_encoded_feature(df, cols, name):\n    groups = []\n    for block in df['date_block_num'].unique():\n        groupdf = df[df.date_block_num < block].groupby(cols).item_cnt_month.mean().reset_index().rename(columns={'item_cnt_month': name})\n        if not groupdf.empty:\n            groupdf[\"date_block_num\"] = block\n            groups.append(groupdf)\n    groupdf = pd.concat(groups, ignore_index=True)\n\n    print(f\"Created mean encoded feature {name} for columns: {cols}\")\n    cols.append(\"date_block_num\")\n    return df.merge(groupdf, on=cols, how=\"left\")\n\nmatrix = add_mean_encoded_feature(matrix, [\"item_id\", \"shop_id\"], \"item_shop_menc\")\nmatrix = add_mean_encoded_feature(matrix, [\"shop_id\"], \"shop_menc\")\nmatrix = add_mean_encoded_feature(matrix, [\"item_id\"], \"item_menc\")\n\n# Delete first entries as they have less features\nn_months_to_delete = 3\nmatrix = matrix[matrix[\"date_block_num\"] > n_months_to_delete]","cee9ca73":"matrix.fillna(0, inplace=True)\nmatrix.reset_index(inplace=True)\nprint(np.any(np.isnan(matrix)))\nprint(not np.all(np.isfinite(matrix)))","3c01ef0f":"matrix[['index', 'date_block_num', 'shop_id', 'item_id', 'item_cnt_month',\n       'shop_category', 'shop_city', 'item_category_id', 'name2', 'name3']].describe()","b46d009e":"matrix[['subtype_code', 'type_code', 'month', 'item_cnt_month_lag_1',\n       'item_cnt_month_lag_2', 'item_cnt_month_lag_3', 'item_cnt_month_1dev_1',\n       'item_cnt_month_1dev_2', 'item_cnt_month_2dev', 'item_shop_menc',\n       'shop_menc', 'item_menc']].describe()","943a59f2":"matrix.columns","51ae0db5":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nM = 30\nV = 33\n\nif RETRAIN_FIRST_LEVEL_MODELS:\n    xgbModel = XGBRegressor(\n        verbosity=1,\n        max_depth=10, #10\n        n_estimators=100,\n        min_child_weight=0.5, \n        colsample_bytree=0.8, \n        subsample=0.8, \n        eta=0.1, # 0.1\n        seed=42,\n        reg_lambda=0, # 0\n        gamma=0)  # 0\n\n    linModel = LinearRegression()\n    nnModel = Pipeline([('scaler', StandardScaler()), ('nn', MLPRegressor(\n        hidden_layer_sizes=(100, 50),\n        batch_size=100,\n        max_iter=20,\n        verbose=True\n    ))])\n\n    models = {\n        'linear': [linModel, {}],\n        'xgb': [xgbModel, {\n            'eval_metric': \"rmse\",\n            'verbose': True, \n            'early_stopping_rounds': 10\n        }],\n        'nn': [nnModel, {\n            'drop_columns': ['shop_id', 'item_id', 'shop_category', 'shop_city', 'item_category_id', 'name2', 'name3', 'subtype_code', 'type_code']\n        }],\n    }\n\n    meta_train_data = {}\n    meta_valid_data = {}\n    meta_test_data = {}\n    for block in range(M, 35):\n        for k, [m, params] in models.items():\n            print(f\"Preprocessing for block {block}, {k}\")\n            drop_columns = params.pop('drop_columns', [])\n            drop_columns.append('item_cnt_month')\n            X = matrix[matrix.date_block_num < block].drop(drop_columns, axis=1)\n            Y = matrix[matrix.date_block_num < block]['item_cnt_month'].clip(0, 20)\n            Z = matrix[matrix.date_block_num == block].drop(drop_columns, axis=1)\n            ZY = matrix[matrix.date_block_num == block]['item_cnt_month'].clip(0, 20)\n            print(f\"Fitting block {block}, {k}\")\n            if k == 'xgb':\n                if block < V:\n                    params['eval_set'] = [(X, Y), (Z, ZY)]\n                else:\n                    params['eval_set'] = [(X, Y)]\n            m.fit(X, Y, **params)\n            print(f\"Predicting for block {block}, {k}\")\n            if block < V:\n                meta_train_data.setdefault(k, []).append(m.predict(Z))\n            elif block >= V and block != 34:\n                meta_valid_data.setdefault(k, []).append(m.predict(Z))\n            else:\n                meta_test_data.setdefault(k, []).append(m.predict(Z))","4e8d38a5":"meta_train_Y = matrix[(matrix.date_block_num >= M) & (matrix.date_block_num < V)]['item_cnt_month'].clip(0, 20)\nmeta_valid_Y = matrix[matrix.date_block_num == V]['item_cnt_month'].clip(0, 20)\n\nif RETRAIN_FIRST_LEVEL_MODELS:\n    print(\"Creating datasets\")\n    meta_train = pd.DataFrame()\n    meta_valid = pd.DataFrame()\n    meta_test = pd.DataFrame()\n\n    for name, series in meta_train_data.items():\n        meta_train[name] = np.concatenate(series)\n\n    for name, series in meta_valid_data.items():\n        meta_valid[name] = np.concatenate(series)\n\n    for name, series in meta_test_data.items():\n        meta_test[name] = np.concatenate(series)\n\n    for col in meta_train:\n        rmse = math.sqrt(mean_squared_error(meta_train_Y, meta_train[col]))\n        print(f\"{col} rmse: {rmse}\")\n\n    print(\"Saving meta values\")\n    meta_train.to_csv('meta_train.csv', index=False)\n    meta_valid.to_csv('meta_valid.csv', index=False)\n    meta_test.to_csv('meta_test.csv', index=False)\nelse:\n    print(\"Loading meta values\")\n    meta_train = pd.read_csv(\"\/kaggle\/input\/meta-models\/meta_train.csv\")\n    meta_valid = pd.read_csv(\"\/kaggle\/input\/meta-models\/meta_valid.csv\")\n    meta_test = pd.read_csv(\"\/kaggle\/input\/meta-models\/meta_test.csv\")","79019b88":"if RETRAIN_META_MODEL:\n    if True:\n        meta_model = XGBRegressor(\n            n_estimators=1000,\n            seed=42,\n            reg_lambda=0,\n            reg_alpha=0)\n        meta_model.fit(\n                meta_train, \n                meta_train_Y, \n                eval_metric=\"rmse\", \n                eval_set=[(meta_train, meta_train_Y), (meta_valid, meta_valid_Y)], \n                verbose=True, \n                early_stopping_rounds=50)\n    else:\n        meta_model = LinearRegression()\n        meta_model.fit(meta_train, meta_train_Y)\n\n    valid_predicted = meta_model.predict(meta_valid).clip(0, 20)\n    rmse = math.sqrt(mean_squared_error(meta_valid_Y, valid_predicted))\n    print(f\"Validation error of model: {rmse}\")\n    pickle.dump(meta_model, open(\"meta_model.p\", \"wb\"))\nelse:\n    print(\"Loading meta model\")\n    meta_model = pickle.load(open(\"meta_model.p\", \"rb\"))\n    valid_predicted = meta_model.predict(meta_valid).clip(0, 20)\n    rmse  = math.sqrt(mean_squared_error(meta_valid_Y, valid_predicted))\n    print(f\"Validation error of loaded model: {rmse}\")","3b62d33a":"Y_test = meta_model.predict(meta_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index,\n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('pablots_submission_multiple.csv', index=False)","f391c19f":"# Feature importance\ndef plot_features(booster, figsize):\n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(meta_model, (30, 10))","4a79815e":"# Features","96fbea6f":"# Features analysis","b3fee3d7":"# Preprocessing","a8672969":"# Introduction\n\n### This notebook has all the required code to create the models and make predictions, as well as a the preliminary EDA\n\n* In order to use saved models for predictions leave the code as it is\n* In order to retrain first level models change the value of `RETRAIN_FIRST_LEVEL_MODELS` constant to `True` (This process takes several hours)\n* In order to retrain meta model change the value of `RETRAIN_META_MODEL` to `True`","e99e9a83":"# Set up","bcd589d1":"# Prediction","0f436e03":"# KFold scheme for time series training of multiple models","f8e0f9c0":"#### A couple of observations:\n* The effect of weekends and season on sales is noticeable\n* There are tendencies within same shops. Not all of them are selling all the time.\n* Some shops have very scarse and extreme data (like shop 20 or shop 9)\n\n#### We'll try to use those facts in our favor to create features","b0064985":"# EDA","d1305bcb":"#### Unknown items\nWe'll be having to predict sales on 363 items that never appear on training data. Probably the best we can do on them is to give the shop average sales on an item as prediction."}}