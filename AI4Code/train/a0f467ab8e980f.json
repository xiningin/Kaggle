{"cell_type":{"116b84df":"code","a9c6bf98":"code","09415e2c":"code","782beee2":"code","b422ac8b":"code","bb3bd8e4":"code","79b7b372":"code","6b4564b2":"code","3ab640ae":"code","ca03b517":"code","a9c6c7ea":"code","52adf011":"code","ac6940ab":"markdown","829e356b":"markdown","539ba8e9":"markdown","b17b870a":"markdown","fbb8ed7e":"markdown","16169d78":"markdown","a09739f7":"markdown","6dea039c":"markdown","777a271c":"markdown","ad79b276":"markdown","fc090dea":"markdown","a20e3de4":"markdown","880f9c7c":"markdown","7bf85b8c":"markdown","bdca6c1e":"markdown"},"source":{"116b84df":"import pandas as pd       \nimport matplotlib as mat\nimport matplotlib.pyplot as plt    \nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import metrics\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom imblearn.pipeline import Pipeline\nfrom imblearn import FunctionSampler\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","a9c6bf98":"df = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf","09415e2c":"df.info()","782beee2":"df.isin([0]).sum()","b422ac8b":"#Replacing zeros for missing values in five features\ndf.loc[:,'Glucose':'BMI'] = df.loc[:,'Glucose':'BMI'].replace(0,np.nan)\n\nprint('Values = 0\\n')\nprint(df.isin([0]).sum())\nprint('\\nValues = nan\\n')\nprint(df.isnull().sum())","bb3bd8e4":"Y = df['Outcome']\nX = df.copy().drop('Outcome', axis = 1)\nfeatures = X.columns.tolist()","79b7b372":"plt.figure(figsize=(16,10))\n\nfor i,col in enumerate(features):    \n    plt.subplot(2,4,i + 1)\n    sns.boxplot(y=col, data=df)\n    #plt.ylabel('')\n\nplt.tight_layout()\n\nplt.show()","6b4564b2":"def IQR_Outliers (X, features):\n\n    print('# of features: ', len(features))\n    print('Features: ', features)\n\n    indices = [x for x in X.index]\n    #print(indices)\n    print('Number of samples: ', len(indices))\n    \n    out_indexlist = []\n        \n    for col in features:\n       \n        #Using nanpercentile instead of percentile because of nan values\n        Q1 = np.nanpercentile(X[col], 25.)\n        Q3 = np.nanpercentile(X[col], 75.)\n        \n        cut_off = (Q3 - Q1) * 1.5\n        upper, lower = Q3 + cut_off, Q1 - cut_off\n        print ('\\nFeature: ', col)\n        print ('Upper and Lower limits: ', upper, lower)\n                \n        outliers_index = X[col][(X[col] < lower) | (X[col] > upper)].index.tolist()\n        outliers = X[col][(X[col] < lower) | (X[col] > upper)].values\n        print('Number of outliers: ', len(outliers))\n        print('Outliers Index: ', outliers_index)\n        print('Outliers: ', outliers)\n        \n        out_indexlist.extend(outliers_index)\n        \n    #using set to remove duplicates\n    out_indexlist = list(set(out_indexlist))\n    out_indexlist.sort()\n    print('\\nNumber of rows with outliers: ', len(out_indexlist))\n    print('List of rows with outliers: ', out_indexlist)\n    \n    \nIQR_Outliers(X, features)","3ab640ae":"def CustomSampler_IQR (X, y):\n    \n    features = X.columns\n    df = X.copy()\n    df['Outcome'] = y\n    \n    indices = [x for x in df.index]    \n    out_indexlist = []\n        \n    for col in features:\n       \n        #Using nanpercentile instead of percentile because of nan values\n        Q1 = np.nanpercentile(df[col], 25.)\n        Q3 = np.nanpercentile(df[col], 75.)\n        \n        cut_off = (Q3 - Q1) * 1.5\n        upper, lower = Q3 + cut_off, Q1 - cut_off\n                \n        outliers_index = df[col][(df[col] < lower) | (df[col] > upper)].index.tolist()\n        outliers = df[col][(df[col] < lower) | (df[col] > upper)].values        \n        out_indexlist.extend(outliers_index)\n        \n    #using set to remove duplicates\n    out_indexlist = list(set(out_indexlist))\n    \n    clean_data = np.setdiff1d(indices,out_indexlist)\n\n    return X.loc[clean_data], y.loc[clean_data]","ca03b517":"LR_Pipeline = Pipeline([('Outlier_removal', FunctionSampler(func=CustomSampler_IQR, validate = False))\n                        ,('Imputer', SimpleImputer(strategy = \"median\"))\n                        ,('LR',  LogisticRegression(C = 0.7, random_state = 42, max_iter = 1000))])\n\n\nKNN_Pipeline = Pipeline([('Outlier_removal', FunctionSampler(func=CustomSampler_IQR, validate = False))\n                        ,('Imputer',SimpleImputer(strategy = \"median\"))\n                        ,('KNN', KNeighborsClassifier(n_neighbors=7))])","a9c6c7ea":"rp_st_kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats = 3, random_state = 42)\n\ncv_score = cross_val_score(LR_Pipeline, X, Y, cv=rp_st_kfold, scoring='accuracy')\nprint(\"Logistic Regression - Acc(SD): {0:0.4f} ({1:0.4f})\". format(cv_score.mean(), cv_score.std()))\n\ncv_score = cross_val_score(KNN_Pipeline, X, Y, cv=rp_st_kfold, scoring='accuracy')\nprint(\"K-Nearest Neighbors  - Acc(SD): {0:0.4f} ({1:0.4f})\". format(cv_score.mean(), cv_score.std()))","52adf011":"LR_with_outliers = Pipeline([('Imputer', SimpleImputer(strategy = \"median\"))\n                        ,('LR',  LogisticRegression(C = 0.7, random_state = 42, max_iter = 1000))])\n\nKNN_with_outliers = Pipeline([('Imputer',SimpleImputer(strategy = \"median\"))\n                        ,('KNN', KNeighborsClassifier(n_neighbors=7))])\n\ncv_score = cross_val_score(LR_with_outliers, X, Y, cv=rp_st_kfold, scoring='accuracy')\nprint(\"Logistic Regression - Acc(SD): {0:0.4f} ({1:0.4f})\". format(cv_score.mean(), cv_score.std()))\n\ncv_score = cross_val_score(KNN_with_outliers, X, Y, cv=rp_st_kfold, scoring='accuracy')\nprint(\"K-Nearest Neighbors  - Acc(SD): {0:0.4f} ({1:0.4f})\". format(cv_score.mean(), cv_score.std()))","ac6940ab":"## Building and Testing the Pipeline\n\nAfter creating our function, we can build the pipeline. First, we use the 'FunctionSampler' to call our function as a sampler. After that, we call an imputer to fill the missing values, and then we define which classifier will be used. In this case, we'll start by building two pipelines, one for Logistic Regression and other for K-Nearest Neighbors.","829e356b":"For comparison purposes, we can test the same models without removing any outliers. As we can see, the accuracies of our first set of pipelines were a little higher. It's important to remember that our function marked more than 10% of this dataset as outliers. You can explore the data a little further and try different methods to remove or transform outliers to improve the accuracy even more.","539ba8e9":"Apparently, most features have a considerable share of outliers. We can take a better look at them by writing a function to highlight each outlier. We calculate the upper and lower limits for each feature (based on IQR Method) and, for every sample that falls outside those limits, we display their value and index number.","b17b870a":"We can see that 84 out of 768 samples were marked as outliers, which is a significant portion of our dataset. It is worth to point out that dropping every outlier without an effort to understand them is not exactly a good practice. For instance, looking at the 'Age' feature we see that any value higher than 66 is marked as an outlier. By dropping every one of those samples, we lose information about a significant group of the population that our dataset is meant to represent.\n\nGiven the intent of this notebook, we will make it simple and drop every sample outside the limits. We can adapt the previous function to receive all the (training) data and return a clean dataset.","fbb8ed7e":"Now we can assess the performance of our models using cross-validation. For each iteration, the training folds will be scanned for outliers and cleaned while the validation fold will be entirely used for prediction.","16169d78":"## Importing Libraries\n\nInstead of the sklearn pipeline, we're going to use the imblearn pipeline. The advantage of imblearn is that it allows the use of different samplers to deal with imbalanced data. The imblearn package provides its own set of samplers, but we can also use a custom sampler with imblearn.FunctionSampler. We can take advantage of those features to create our function to remove outliers and call it within the pipeline as a sampler.","a09739f7":"That's it. I hope this notebook can be useful for some people. If you have any suggestions or questions, let me know.","6dea039c":"## Visualizing the outliers\n\nIn this notebook, I'm considering as outliers the values beyond the limits based on the [Interquartile Range (IQR) Method](https:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/). I recommend you to take a look at another methods (uni and multivariate) to detect outliers, including those already implemented in the sklearn package.\n\nAn easy way of displaying the distribution of each feature and check for outliers is the use of boxplots.","777a271c":"As stated in this [paper](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352914816300016) that makes use of the same dataset, the value of zero was recorded in place of missing experimental observations. So for all features, except number of pregnancies, we could assume that '0' represents a missing value.\n\nAlthough it is not the intent of this notebook, it might be useful to point this out for those who will use this dataset.","ad79b276":"---\n\n## Overview\n\nThe goal of this short notebook is to show how to use a custom function for outlier removal within a pipeline. Including this step in the pipeline ensures that samples with outliers are only removed from the training data, leaving the test data as it is.\n\nThe resulting pipeline can be tested with K-fold Cross Validation, where, for each iteration, the training folds will have its outliers removed while the test\/validation fold will be completed used for prediction, giving us a more realistic notion of the model's performance in 'unseen' data.\n    \nTo test this approach, I chose two models, Logistic Regression and K-Nearest Neighbors, both which are sensitive to outliers. I\u2019ve compared them with and without the outlier removal function and these were their accuracies on 10-fold CV (with vs without our function):\n- Logistic Regression: 77,12% vs 76,38%\n- K-Nearest Neighbors: 73,60% vs 72,43%\n \nThe motivation for this notebook came from a question I had ([and solved after this answer](https:\/\/www.kaggle.com\/discussion\/230284#1261498)) about how to remove outliers inside K-fold CV to assess their impact on the model. I hope this notebook can be helpful for other beginners like myself. If you have a different approach, feel free to share. If you find useful, please consider upvoting.","fc090dea":"<br>\n<h1 style = \"font-size:30px; font-weight : bold; color : blue; text-align: center; border-radius: 10px 15px;\"> Custom Outlier Removal within a Pipeline <\/h1>\n<br>","a20e3de4":"## <center> If you find this notebook useful, support with an upvote! <center>","880f9c7c":"## References:\n\nhttps:\/\/towardsdatascience.com\/enrich-your-train-fold-with-a-custom-sampler-inside-an-imblearn-pipeline-68f6dff964bf\n\nhttps:\/\/imbalanced-learn.org\/dev\/references\/generated\/imblearn.FunctionSampler.html\n\n\nhttps:\/\/machinelearningmastery.com\/how-to-use-statistics-to-identify-outliers-in-data\/\n\n\n\n","7bf85b8c":"<img src=\"https:\/\/i.imgur.com\/sSS6VKv.png\" width=\"750\" height=\"300\" align=\"center\"\/>","bdca6c1e":"## Checking the Data"}}