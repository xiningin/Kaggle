{"cell_type":{"183c2ce0":"code","ff82dc2d":"code","51e049bc":"code","b83c56b6":"code","1780949b":"code","4077e6ce":"code","05c1b0f6":"code","7176a8cb":"code","f2dc07c7":"code","fd7c3082":"code","5053aa20":"code","fc194a35":"code","8af3ac05":"code","10e8bb83":"code","78bcb519":"code","4592c8fb":"code","482fddba":"markdown","f6cd950e":"markdown"},"source":{"183c2ce0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ntr=pd.read_csv(\"..\/input\/titanic\/train.csv\",encoding='ISO-8859-1')\ntr\nts=pd.read_csv(\"..\/input\/titanic\/test.csv\",encoding='ISO-8859-1')\nts\n\nts","ff82dc2d":"data=ts.Ticket.value_counts()\ndata\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nstyle.use('ggplot')\nfig,ax=plt.subplots(figsize=(50,20))\n # the width of the bars \n\n\nplt.bar(data.index,data.values)\n\nax.set_xticklabels(data.index,minor=False,size=10,rotation=90)\nplt.title(\"Ticket\",size=20)\nplt.show()","51e049bc":"tr.Age.max()\ntr.query(\"Age=='80.0'\")\n\nplt.hist(tr[\"Age\"])\nplt.show()","b83c56b6":"sur=tr[\"Survived\"].value_counts()\nsur\nplt.bar(sur.index,sur.values)\nplt.show()","1780949b":"tr.Sex.max()\n#fg.query(\"Sex=='male'\")\nsx=tr['Sex'].value_counts()\nsx\nplt.bar(sx.index,sx.values)\nplt.title('Sex')\n\n\nplt.show()\ntr.Ticket.min()\n\ntr.query(\"Ticket=='110152'\")\ntk=tr['Ticket'].value_counts()\ntk\nsns.distplot(tk,rug=True,kde=True)\nplt.show()\n","4077e6ce":"X_train=tr.loc[:,['Name','Sex','Age','SibSp','Parch','Ticket']]\ny_train=tr.iloc[:,1]\nX_test=ts.loc[:,['Name','Sex','Age','SibSp','Parch','Ticket']]\n\nX_train\ny_train\nX_test","05c1b0f6":"X_train","7176a8cb":"from sklearn.preprocessing import LabelEncoder\nla=LabelEncoder()\nX_test.iloc[:,0]=la.fit_transform(X_test.iloc[:,0])\nX_test\nX_test.iloc[:,1]=la.fit_transform(X_test.iloc[:,1])\nX_test\nX_test.iloc[:,2]=la.fit_transform(X_test.iloc[:,2])\nX_test\nX_test.iloc[:,5]=la.fit_transform(X_test.iloc[:,5])\nX_test\n\nX_train.iloc[:,0]=la.fit_transform(X_train.iloc[:,0])\nX_train\nX_train.iloc[:,1]=la.fit_transform(X_train.iloc[:,1])\nX_train\nX_train.iloc[:,2]=la.fit_transform(X_train.iloc[:,2])\nX_train\nX_train.iloc[:,5]=la.fit_transform(X_train.iloc[:,5])\nX_train\n\n\nfrom sklearn.preprocessing import Imputer\n\nmissingValueImputer = Imputer (missing_values = 'NaN', strategy = 'mean', \n                               axis = 0)  \nmissingValueImputer = missingValueImputer.fit (X_test.loc[:,['Age']])\n\nX_test.loc[:,['Age']] = missingValueImputer.transform(X_test.loc[:,['Age']])\nX_test\n\nmissingValueImputer = missingValueImputer.fit (X_train.loc[:,['Age']])\n\nX_train.loc[:,['Age']] = missingValueImputer.transform(X_train.loc[:,['Age']])\nX_train","f2dc07c7":"# using MInMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nmms=MinMaxScaler()\nX_trainNorm=mms.fit_transform(X_train)\nX_testNorm=mms.transform(X_test)\nX_trainNorm\nX_testNorm\n\n\n#using StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nsc=StandardScaler()\nX_trainNorm2=sc.fit_transform(X_train)\nX_trainNorm2\nX_testNorm2=sc.transform(X_test)\nX_testNorm2\n\n# using PCA\nfrom sklearn.decomposition import PCA\npa=PCA(n_components=2)\nX_trainNorm1=pa.fit_transform(X_trainNorm2)\nX_trainNorm1\nX_testNorm1=pa.transform(X_testNorm2)\nX_testNorm1\n\nprint(pa.explained_variance_ratio_)","fd7c3082":"#Using Logistic with Standard Scaler\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nhr=lr.fit(X_trainNorm2,y_train)\npred=lr.predict(X_testNorm2)\n\n\n#using gaussian \nfrom sklearn.naive_bayes import GaussianNB\ngb=GaussianNB()\nhr1=gb.fit(X_trainNorm2,y_train)\npred1=gb.predict(X_testNorm2)\n\n\n\n#using knn\nfrom sklearn.neighbors import KNeighborsClassifier\nkc=KNeighborsClassifier()\nhr2=kc.fit(X_trainNorm2,y_train)\n\npred2=kc.predict(X_testNorm2)\n\n\n#using Svm\nfrom sklearn.svm import SVC\nsc=SVC()\nhr3=sc.fit(X_trainNorm2,y_train)\npred3=sc.predict(X_testNorm2)\n\n\n#using Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=10,random_state=0,max_depth=5,criterion='entropy')\nrf.fit(X_trainNorm2,y_train)\npred4=rf.predict(X_testNorm2)\n\n\n#using DecisionTree\nfrom sklearn.tree import DecisionTreeClassifier\ndc = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 3,random_state = 100)\n\ndc.fit(X_trainNorm2,y_train)\npred5=dc.predict(X_testNorm2)\n","5053aa20":"#using Logistic Regression using PCA\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\nax=lr.fit(X_trainNorm1,y_train)\nax\npred6=lr.predict(X_testNorm1)\npred6\n","fc194a35":"#using GaussianNB\nfrom sklearn.naive_bayes import GaussianNB\ngb=GaussianNB()\nax1=gb.fit(X_trainNorm1,y_train)\nax1\npred7=gb.predict(X_testNorm1)\npred7\n\n#using KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nkc=KNeighborsClassifier()\nax2=kc.fit(X_trainNorm1,y_train)\nax2\npred8=kc.predict(X_testNorm1)\npred8\n\n\n#using SVM\nfrom sklearn.svm import SVC\nsc=SVC()\nax3=sc.fit(X_trainNorm1,y_train)\nax3\npred9=sc.predict(X_testNorm1)\npred9\n\n\n\n#using random forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=10,random_state=0,max_depth=5,criterion='entropy')\nrf.fit(X_trainNorm1,y_train)\npred10=rf.predict(X_testNorm1)\npred10\n\n#using Decison tree\nfrom sklearn.tree import DecisionTreeClassifier\ndc = DecisionTreeClassifier(criterion = \"entropy\", max_depth = 3,random_state = 100)\ndc.fit(X_trainNorm1,y_train)\npred11=dc.predict(X_testNorm1)\npred11","8af3ac05":"#using Logistic Regression using MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\ndt=lr.fit(X_trainNorm,y_train)\ndt\npre=lr.predict(X_testNorm)\npre\n\n\n#using GaussianNB\nfrom sklearn.naive_bayes import GaussianNB\ngb=GaussianNB()\ndt1=gb.fit(X_trainNorm,y_train)\ndt1\npre1=gb.predict(X_testNorm)\npre1\n\n\n\n#using KNN\n\nfrom sklearn.neighbors import KNeighborsClassifier\nkc=KNeighborsClassifier()\ndt2=kc.fit(X_trainNorm,y_train)\ndt2\npre2=kc.predict(X_testNorm)\npre2\n\n#using Svm\n\nfrom sklearn.svm import SVC\nsm=SVC()\ndt3=sm.fit(X_trainNorm,y_train)\ndt3\npre3=sm.predict(X_testNorm)\npre3\n\n\n#using RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=10,random_state=0,max_depth=5,criterion='entropy')\nrf.fit(X_trainNorm,y_train)\npre4=rf.predict(X_testNorm)\npre4\n\n\n#usingDecision tree\n\nfrom sklearn.tree import DecisionTreeClassifier\ndc = DecisionTreeClassifier(criterion = \"gini\", max_depth = 3,random_state = 100)\n\ndc.fit(X_trainNorm,y_train)\npre5=dc.predict(X_testNorm)\n","10e8bb83":"#Using Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\ndata=lr.fit(X_train,y_train)\ndata\npred=lr.predict(X_test)\npred\n\n\n\n\n\n\n\n\n\n#using Gussian naive bayes\nfrom sklearn.naive_bayes import GaussianNB\ngb=GaussianNB()\ndata1=gb.fit(X_train,y_train)\ndata1\npred1=gb.predict(X_test)\npred1\n\n\n\n\n#using KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nkc=KNeighborsClassifier()\ndata2=kc.fit(X_train,y_train)\ndata2\npred2=kc.predict(X_test)\npred2\n\n\n\n#using SVM\nfrom sklearn.svm import SVC\nsc=SVC()\ndata3=sc.fit(X_train,y_train)\ndata3\npred3=sc.predict(X_test)\npred3\n\n\n\n\n#using RAndom_forest\nfrom sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=10,random_state=0,max_depth=5,criterion='entropy')\nrf.fit(X_train,y_train)\npred=rf.predict(X_test)\npred\n\n\n\n#using  Decision_Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier(criterion = \"gini\", max_depth = 3,random_state = 100)\ndecision_tree.fit(X_train, y_train)\n\npredictValues =decision_tree.predict(X_test)\n\npredictValues\n\n","78bcb519":"estimators=rf.estimators_[5]\nlabels=['Pclass','Name','Sex','Age','SibSp','Parch']\nfrom sklearn import tree\nfrom graphviz import Source\nfrom IPython.display import SVG\nfrom IPython.display import display\ngraph = Source(tree.export_graphviz(estimators, out_file=None, feature_names=labels, filled = True))\ndisplay(SVG(graph.pipe(format='svg')))","4592c8fb":"data_feature_names = ['Pclass','Name','Sex','Age','SibSp','Parch']\n\nfrom sklearn.tree import export_graphviz\nfrom sklearn import tree\nfrom graphviz import Source\nfrom IPython.display import SVG\nfrom IPython.display import display\n\ngraph = Source(tree.export_graphviz(decision_tree, out_file=None, feature_names=data_feature_names, filled = True,rounded=True))\n\n\ndisplay(SVG(graph.pipe(format='svg')))","482fddba":"                         Analysis and Visualization","f6cd950e":"#                            Machine Learning project"}}