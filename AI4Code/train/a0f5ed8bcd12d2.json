{"cell_type":{"e9bb37b2":"code","5fdf3c32":"code","843d249d":"code","d4eec783":"code","47e3fa3a":"code","7284850e":"code","a76a2721":"code","40212a5f":"code","e47c23ac":"code","5d27a78a":"code","59444789":"code","bfd4c20d":"code","f3ed9ce8":"code","bfdf9004":"code","770de169":"code","8417a3c8":"code","d46a7910":"code","3e10826c":"code","a4a98127":"code","f42b4807":"code","b8e1c54b":"code","8036a723":"code","1ca76b3f":"code","3ef0b129":"code","60037e91":"code","d9bb3176":"code","47d57c79":"code","876577f5":"code","fb74f380":"markdown","2aba709c":"markdown","24689328":"markdown","540a2ac7":"markdown","aaf55ea2":"markdown"},"source":{"e9bb37b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","5fdf3c32":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport datetime\nimport time \n\n%matplotlib inline\nimport warnings\nwarnings.simplefilter('ignore')","843d249d":"from sklearn.linear_model import LassoCV, Lasso, RidgeCV, Ridge\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.datasets import make_regression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","d4eec783":"df_train = pd.read_csv('..\/input\/predciting-price-transaction\/trainPrice.csv')\nsub_sch = pd.read_csv('..\/input\/subway-schoolcsv\/subway_school.csv')\ndf_train = pd.merge(df_train, sub_sch)\ndf_train1=df_train[df_train['city']==0]\ndf_train2=df_train[df_train['city']==1]","47e3fa3a":"# Convert\ndf_train['elapsed_time'] = df_train['transaction_year_month'].map(lambda x: int(x\/100)) - df_train['year_of_completion']\ndf_train['transaction_year'] = df_train['transaction_year_month'].map(lambda x: str(int(x\/100)))\ndf_train['heat_type'] = df_train['heat_type'].astype('category')\ndf_train['heat_fuel'] = df_train['heat_fuel'].astype('category')\ndf_train['front_door_structure'] = df_train['front_door_structure'].astype('category')\ndf_train['heat_type'] = pd.Categorical(df_train['heat_type']).codes\ndf_train['heat_fuel'] = pd.Categorical(df_train['heat_fuel']).codes\ndf_train['front_door_structure'] = pd.Categorical(df_train['front_door_structure']).codes\n# Drop redundant\ndf_train = df_train.drop(['year_of_completion', 'apartment_id', 'key', 'transaction_year_month', 'transaction_date', 'address_by_law', 'room_id'], axis=1)\n# Imputation\ndf_train = df_train.dropna(subset=['room_count','bathroom_count'] )\ndf_train = df_train.dropna(subset=['tallest_building_in_sites','lowest_building_in_sites'])\ndf_train = df_train.dropna(subset=['heat_type','heat_fuel'])\ndf_train['total_parking_capacity_in_site'].fillna(df_train['total_parking_capacity_in_site'].mean(),inplace=True)\nplt.figure(figsize=(15,18))\nsns.heatmap(df_train.corr(), annot=True)","7284850e":"drop_cols1 = ['apartment_building_count_in_sites','bathroom_count','city','elapsed_time']\nlist(set(df_train.columns) - set(drop_cols1))","a76a2721":"# Reference: Course_STAT8017 Tutorial 2 & 5\nclass Solution(object):\n    def convert_impute(self,df_train):\n        # Convert\n        df_train['elapsed_time'] = df_train['transaction_year_month'].map(lambda x: int(x\/100)) - df_train['year_of_completion']\n        df_train['transaction_year'] = df_train['transaction_year_month'].map(lambda x: str(int(x\/100)))\n        df_train['heat_type'] = df_train['heat_type'].astype('category')\n        df_train['heat_fuel'] = df_train['heat_fuel'].astype('category')\n        df_train['front_door_structure'] = df_train['front_door_structure'].astype('category')\n        df_train['heat_type'] = pd.Categorical(df_train['heat_type']).codes\n        df_train['heat_fuel'] = pd.Categorical(df_train['heat_fuel']).codes\n        df_train['front_door_structure'] = pd.Categorical(df_train['front_door_structure']).codes\n        # Drop redundant\n        df_train = df_train.drop(['year_of_completion', 'apartment_id', 'key', 'city', 'transaction_year_month', 'transaction_date', 'address_by_law', 'room_id'], axis=1)\n        # Imputation\n        df_train = df_train.dropna(subset=['room_count','bathroom_count'] )\n        df_train = df_train.dropna(subset=['tallest_building_in_sites','lowest_building_in_sites'])\n        df_train = df_train.dropna(subset=['heat_type','heat_fuel'])\n        df_train['total_parking_capacity_in_site'].fillna(df_train['total_parking_capacity_in_site'].mean(),inplace=True)\n        return df_train\n\n    def corr(self,df_train):\n        plt.figure(figsize=(15,18))\n        sns.heatmap(data.corr(), annot=True)\n    \n    def drophighcor(self,df_train,variable):\n        df_train = df_train.drop(variable, axis=1)\n        return df_train\n    \n    def pretrainx(self,df_train):\n        train_X = df_train.drop(['transaction_real_price'], axis = 1)\n        train_y = pd.DataFrame(df_train['transaction_real_price'], columns=['transaction_real_price'])\n        scaler_x = MinMaxScaler((-1, 1))\n        X = scaler_x.fit_transform(train_X)\n        y = train_y['transaction_real_price'].map(lambda x: np.log(x))\n        return X\n        # Source: HKU Course_STAT8017 Tutorial 2\n        \n    def pretrainy(self,df_train):\n        train_X = df_train.drop(['transaction_real_price'], axis = 1)\n        train_y = pd.DataFrame(df_train['transaction_real_price'], columns=['transaction_real_price'])\n        scaler_x = MinMaxScaler((-1, 1))\n        X = scaler_x.fit_transform(train_X)\n        y = train_y['transaction_real_price'].map(lambda x: np.log(x))\n        return y\n        # Source: HKU Course_STAT8017 Tutorial 2\n    \n    def plotalpha_l(self,X,y):\n        # Create an array of alphas and lists to store scores\n        alpha_space = np.logspace(-4, 0, 50)\n        lasso_scores = []\n        lasso_scores_std = []\n        lasso = Lasso()\n        # Compute scores over range of alphas\n        for alpha in alpha_space:\n            lasso.alpha = alpha\n            lasso_cv_scores = cross_val_score(lasso, X, y, cv=5)\n            lasso_scores.append(np.mean(lasso_cv_scores))\n            lasso_scores_std.append(np.std(lasso_cv_scores))\n        def display_plot(cv_scores, cv_scores_std):\n            fig = plt.figure()\n            ax = fig.add_subplot(1,1,1)\n            ax.plot(alpha_space, cv_scores)\n            std_error = cv_scores_std \/ np.sqrt(10)\n            ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n            ax.set_ylabel('CV Score +\/- Std Error')\n            ax.set_xlabel('Alpha of lasso')\n            ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n            ax.set_xlim([alpha_space[0], alpha_space[-1]])\n            ax.set_xscale('log')\n            plt.show()\n        display_plot(lasso_scores, lasso_scores_std)\n        \n    def fitlasso(self,X,y,df_train,alpha,threshold):\n        #X, y = make_regression()\n        train_X = df_train.drop(['transaction_real_price'], axis = 1)\n        lasso = Lasso(alpha)\n        lasso = lasso.fit(X,y)\n        pred_train_lasso = lasso.predict(X)\n        mse_train_lasso = mean_squared_error(y, pred_train_lasso)\n        ######################## Lasso #########################\n        drop_cols1 = list(np.where(abs(lasso.coef_) <= threshold))\n        drop_cols1 = df_train.columns[[drop_cols1]]\n        print('Parameters of lasso:', lasso.coef_)\n        print('Coefficient of determination R^2 of the prediction: ',lasso.score(X, y))\n        print('Variables can be removed:', drop_cols1[0])\n        print('Variables should be reserved:', list(set(df_train.columns) - set(drop_cols1[0])))\n        plt.figure(figsize=(12,8))\n        plt.plot(lasso.coef_, label='LASSO')\n        plt.xticks(range(len(train_X.columns)),train_X.columns, rotation=80) \n        plt.margins(0.01)\n        plt.show()\n        return drop_cols1[0]\n          \n    def plotalpha_r(self,df_train,y):\n        train_X = df_train.drop(['transaction_real_price'], axis = 1)\n        alpha_space = np.logspace(-4, 0, 50)\n        ridge_scores = []\n        ridge_scores_std = []\n        ridge = Ridge(normalize=True)\n        for alpha in alpha_space:\n            ridge.alpha = alpha\n            ridge_cv_scores = cross_val_score(ridge, train_X, y, cv=5)\n            ridge_scores.append(np.mean(ridge_cv_scores))\n            ridge_scores_std.append(np.std(ridge_cv_scores))\n        def display_plot(cv_scores, cv_scores_std):\n            fig = plt.figure()\n            ax = fig.add_subplot(1,1,1)\n            ax.plot(alpha_space, cv_scores)\n            std_error = cv_scores_std \/ np.sqrt(10)\n            ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n            ax.set_ylabel('CV Score +\/- Std Error')\n            ax.set_xlabel('Alpha of ridge')\n            ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n            ax.set_xlim([alpha_space[0], alpha_space[-1]])\n            ax.set_xscale('log')\n            plt.show()\n        # Display the plot\n        display_plot(ridge_scores, ridge_scores_std)\n        \n    def fitridge(self,X,y,df_train,alpha,threshold):\n        #X, y = make_regression()\n        train_X = df_train.drop(['transaction_real_price'], axis = 1)\n        ridge = Ridge(alpha,normalize=True).fit(train_X, y)\n        ######################## Ridge #########################\n        drop_cols2 = list(np.where(abs(ridge.coef_) <=threshold ))\n        drop_cols2 = df_train.columns[[drop_cols2]]\n        print('Parameters of ridge:', ridge.coef_)\n        print('Coefficient of determination R^2 of the prediction: ',ridge.score(train_X, y))\n        print('From Ridege, variables can be removed:', drop_cols2[0])\n        print('Variables should be reserved:', list(set(df_train.columns) - set(drop_cols2[0])))\n        return drop_cols2[0]\n    \n    def fit_random_forest(self,X,y,df_train):\n        train_X = df_train.drop(['transaction_real_price'], axis = 1)\n        rfr = RandomForestRegressor()\n        rfr.fit(train_X, y)\n        pred_train_rfr = rfr.predict(train_X)\n        Importance = pd.DataFrame({'Features':train_X.columns, 'Importance':rfr.feature_importances_*100})\n        ######################## Random Forest #########################\n        drop_cols3 = list(Importance.sort_values(by='Importance', axis=0, ascending=False).iloc[-5:,0])\n        #drop_cols3 = df_train.columns[[drop_cols3]] \n        Importance = pd.DataFrame({'Importance':rfr.feature_importances_*100}, index=train_X.columns)\n        Importance.sort_values(by='Importance', axis=0, ascending=True).plot(kind='barh', color='b', )\n        plt.xlabel('Variable Importance')\n        plt.gca().legend_ = None\n        print('From Random Forest, variables can be removed:', drop_cols3)\n        print('Variables should be reserved:', list(set(df_train.columns) - set(drop_cols3)))\n        return drop_cols3\n    \n    def finaldrop(self,df_train,drop_cols2):\n        train_X = df_train.drop(['transaction_real_price'], axis = 1)\n        train_X = train_X.drop(drop_cols2, axis=1)\n        return train_X\n    \n    def training(self,train_X,y):\n        scaler_x = MinMaxScaler((-1, 1))\n        X = scaler_x.fit_transform(train_X)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n        ind_var = list(train_X.columns)\n        print('%d Features are selected for training:'%len(ind_var), ind_var)\n        # Lasso\n        lasso = LassoCV(cv=5)\n        lasso.fit(X_train, y_train)\n        pred_train_lasso = lasso.predict(X_train)\n        pred_test_lasso = lasso.predict(X_test)\n        # Ridge\n        ridge = RidgeCV(cv=5)\n        ridge.fit(X_train, y_train)\n        pred_train_ridge = ridge.predict(X_train)\n        pred_test_ridge = ridge.predict(X_test)\n        # Decision tree\n        base = DecisionTreeRegressor()\n        grid = GridSearchCV(base, param_grid={'max_depth': (2, 4, 6, 8, 10), 'min_samples_leaf': [5, 10, 15, 20]}, cv=10)\n        grid.fit(X_train, y_train)                # 5^4 possible tree\n        dtr = grid.best_estimator_\n        pred_train_dtr = dtr.predict(X_train)\n        pred_test_dtr = dtr.predict(X_test)\n        # Random forest\n        rfr = RandomForestRegressor(max_features=10)\n        rfr.fit(X_train, y_train)\n        pred_train_rfr = rfr.predict(X_train)\n        pred_test_rfr = rfr.predict(X_test)\n        plt.figure(figsize=(12,8))\n        plt.plot(rfr.feature_importances_)\n        plt.xticks(range(len(train_X.columns)),train_X.columns, rotation=90) \n        plt.show()\n\n        mse_train_lasso = mean_squared_error(y_train, pred_train_lasso)\n        mse_test_lasso = mean_squared_error(y_test, pred_test_lasso)\n\n        mse_train_ridge = mean_squared_error(y_train, pred_train_ridge)\n        mse_test_ridge = mean_squared_error(y_test, pred_test_ridge)\n\n        mse_train_dtr = mean_squared_error(y_train, pred_train_dtr)\n        mse_test_dtr = mean_squared_error(y_test, pred_test_dtr)\n\n        mse_train_rfr = mean_squared_error(y_train, pred_train_rfr)\n        mse_test_rfr = mean_squared_error(y_test, pred_test_rfr)\n        \n        print(\"LASSO: Training MSE:\", round(mse_train_lasso, 5), \"Testing MSE:\", round(mse_test_lasso, 5))\n        print(\"RIDGE: Training MSE:\", round(mse_train_ridge, 5), \"Testing MSE:\", round(mse_test_ridge, 5))\n        print(\"DTR: Training MSE:\", round(mse_train_dtr, 5), \"Testing MSE:\", round(mse_test_dtr, 5))\n        print(\"RFR: Training MSE:\", round(mse_train_rfr, 5), \"Testing MSE:\", round(mse_test_rfr, 5)) \n        return rfr\n    \n    def testset(self,df_test):\n        df_test['elapsed_time'] = df_test['transaction_year_month'].map(lambda x: int(x\/100)) - df_test['year_of_completion']\n        df_test['transaction_year'] = df_test['transaction_year_month'].map(lambda x: str(int(x\/100)))\n        df_test['heat_type'] = df_test['heat_type'].astype('category')\n        df_test['heat_fuel'] = df_test['heat_fuel'].astype('category')\n        df_test['front_door_structure'] = df_test['front_door_structure'].astype('category')\n        df_test['heat_type'] = pd.Categorical(df_test['heat_type']).codes\n        df_test['heat_fuel'] = pd.Categorical(df_test['heat_fuel']).codes\n        df_test['front_door_structure'] = pd.Categorical(df_test['front_door_structure']).codes\n        df_test['total_parking_capacity_in_site'].fillna(df_test['total_parking_capacity_in_site'].mean(),inplace=True)\n        df_test['room_count'].fillna(df_test['room_count'].mean(),inplace=True)\n        df_test['bathroom_count'].fillna(df_test['bathroom_count'].mean(),inplace=True)\n        df_test['tallest_building_in_sites'].fillna(df_test['tallest_building_in_sites'].mean(),inplace=True)\n        df_test['lowest_building_in_sites'].fillna(df_test['lowest_building_in_sites'].mean(),inplace=True)\n        return df_test\n    \n    def prediction(self,train_X,df_test,rfr):\n        ind_var = list(train_X.columns)\n        df_test['room_count'].fillna(df_test['room_count'].mean(),inplace=True)\n        df_test['bathroom_count'].fillna(df_test['bathroom_count'].mean(),inplace=True)\n        df_test['tallest_building_in_sites'].fillna(df_test['tallest_building_in_sites'].mean(),inplace=True)\n        df_test['lowest_building_in_sites'].fillna(df_test['lowest_building_in_sites'].mean(),inplace=True)\n        X_test = df_test[ind_var]\n        scaler_x = MinMaxScaler((-1, 1))\n        X_test = scaler_x.fit_transform(X_test)\n        pred_test_rfr = rfr.predict(X_test)\n        d = {'key': df_test['key'], 'transaction_real_price': np.exp(pred_test_rfr)}\n        submit_prediction = pd.DataFrame(data=d)\n        submit_prediction.to_csv('Submission_Price.csv', index=False)\n        return submit_prediction","40212a5f":"# Original data convert and impute\ndata=Solution().convert_impute(df_train1)\nSolution().corr(data)\n# drop high correlation variables\ndata1=Solution().drophighcor(data,['total_household_count_in_sites','exclusive_use_area'])\n# Split X&y and scale X\nX=Solution().pretrainx(data1)\ny=Solution().pretrainy(data1)","e47c23ac":"print('------------------------------------ Plot alpha for lasso ------------------------------------')\nSolution().plotalpha_l(X,y)\nprint('-------------------------------------- Lasso Regression --------------------------------------')\nSolution().fitlasso(X,y,data1,0.005,0.00001)","5d27a78a":"print('------------------------------------ Plot alpha for Ridge ------------------------------------')\nSolution().plotalpha_r(data1,y)\nprint('-------------------------------------- Ridge Regression --------------------------------------')\nSolution().fitridge(X,y,data1,0.05,0.001)","59444789":"print('---------------------------------------- Random Forest ---------------------------------------')\ndrop = Solution().fit_random_forest(X,y,data1)","bfd4c20d":"# Drop variables\ndata2=Solution().finaldrop(data1,drop)\ndata2.head()\nprint('------------------------------------ Comparison of Models ------------------------------------')\nregressor1=Solution().training(data2,y)","f3ed9ce8":"print('---------------------------------- Prediction for Test Set -----------------------------------')\ndf_test=pd.read_csv('..\/input\/predciting-price-transaction\/testPrice.csv')\ndf_test = pd.merge(df_test, sub_sch)\ndf_test1=df_test[df_test['city']==0]\ndf_test1=Solution().testset(df_test1)\ndf_test1.head()\nSolution().prediction(data2,df_test1,regressor1).head()","bfdf9004":"# Original data convert and impute\ndata=Solution().convert_impute(df_train2)\nprint('------------------------------------- Correlation Map -------------------------------------')\nSolution().corr(data)\n# drop high correlation variables\ndata1=Solution().drophighcor(data,['total_household_count_in_sites','exclusive_use_area'])\n# Split X&y and scale X\nX=Solution().pretrainx(data1)\ny=Solution().pretrainy(data1)","770de169":"print('------------------------------------ Plot alpha for lasso ------------------------------------')\nSolution().plotalpha_l(X,y)\nprint('-------------------------------------- Lasso Regression --------------------------------------')\nSolution().fitlasso(X,y,data1,0.005,0.00001)","8417a3c8":"print('------------------------------------ Plot alpha for ridge ------------------------------------')\nSolution().plotalpha_r(data1,y)\nprint('-------------------------------------- Ridge Regression --------------------------------------')\nSolution().fitridge(X,y,data1,0.05,0.001)","d46a7910":"print('---------------------------------------- Random Forest ---------------------------------------')\ndrop = Solution().fit_random_forest(X,y,data1)","3e10826c":"data2=Solution().finaldrop(data1,drop)\ndata2.head()\nprint('------------------------------------ Comparison of Models ------------------------------------')\nregressor2=Solution().training(data2,y)","a4a98127":"print('---------------------------------- Prediction for Test Set ------- ---------------------------')\ndf_test=pd.read_csv('..\/input\/predciting-price-transaction\/testPrice.csv')\n#df_test=pd.read_csv('C:\/Users\/jjl\/Desktop\/testPrice.csv')\ndf_test = pd.merge(df_test, sub_sch)\ndf_test2=df_test[df_test['city']==1]\ndf_test2=Solution().testset(df_test2)\ndf_test2.head()\nSolution().prediction(data2,df_test2,regressor2).head()","f42b4807":"from sklearn.cluster import AgglomerativeClustering, KMeans\nimport scipy.cluster.hierarchy as sch\nnp.set_printoptions(suppress=True, precision = 3)","b8e1c54b":"def process_for_trend(df_train):\n    f = lambda x,y : x\/y\n    df_train['avg_price'] = df_train[['transaction_real_price','exclusive_use_area']].apply(lambda x: f(*x), axis=1)\n    df_train['elapsed_time'] = df_train['transaction_year_month'].map(lambda x: int(x\/100)) - df_train['year_of_completion']\n    df_train['transaction_year'] = df_train['transaction_year_month'].map(lambda x: str(int(x\/100)))\n    first_hand = df_train[df_train['elapsed_time'] <= 1]\n    first_hand = first_hand.dropna(subset=['room_count','bathroom_count','heat_type','heat_fuel','heat_type','front_door_structure','total_parking_capacity_in_site'] )\n    first_hand = first_hand.drop_duplicates(subset=['apartment_id','room_id','floor'],keep='first')\n    first_hand['heat_type'] = first_hand['heat_type'].astype('category')\n    first_hand['heat_fuel'] = first_hand['heat_fuel'].astype('category')\n    first_hand['front_door_structure'] = first_hand['front_door_structure'].astype('category')\n    first_hand['heat_type'] = pd.Categorical(first_hand['heat_type']).codes\n    first_hand['heat_fuel'] = pd.Categorical(first_hand['heat_fuel']).codes\n    first_hand['front_door_structure'] = pd.Categorical(first_hand['front_door_structure']).codes\n    return first_hand","8036a723":"def trend(first_hand):\n    df_trend = first_hand[['exclusive_use_area','floor', 'total_parking_capacity_in_site', 'room_count','bathroom_count','sub_count','sch_count', 'tallest_building_in_sites','lowest_building_in_sites','avg_price']].groupby(first_hand['transaction_year'])\n    annual_trend = df_trend.mean()\n    plt.figure(figsize=(12,8))\n    #plot the first 5 variables\n    for col in annual_trend.columns[:5]:\n        #plt.plot(annual_trend[col], label=str(col))\n        plt.plot((annual_trend[col]-annual_trend[col].mean())\/(annual_trend[col].max()-annual_trend[col].min()), label=str(col))\n    plt.legend(loc='upper right')\n    plt.ylabel(\"Scaled Mean\")\n    plt.show()\n    #plot the last 5variables\n    plt.figure(figsize=(10,7))\n    for col in annual_trend.columns[5:10]:\n        plt.plot((annual_trend[col]-annual_trend[col].mean())\/(annual_trend[col].max()-annual_trend[col].min()), label=str(col))\n    plt.legend(loc='upper right')\n    plt.ylabel(\"Scaled Mean\")\n    plt.show()","1ca76b3f":"def cluster(first_hand):\n    X = first_hand[['total_parking_capacity_in_site', 'floor', 'supply_area','sub_count','sch_count', 'tallest_building_in_sites','lowest_building_in_sites']]\n    scaler_x = MinMaxScaler((-1, 1))\n    sX = scaler_x.fit_transform(X)\n    kmean1 = KMeans(n_clusters=4, init='k-means++', random_state=42)\n    kmean1.fit(sX)\n    plt.figure(figsize=(10, 8))\n    plt.plot(kmean1.cluster_centers_[0], label='Cluster 1')\n    plt.plot(kmean1.cluster_centers_[1], label='Cluster 2')\n    plt.plot(kmean1.cluster_centers_[2], label='Cluster 3')\n    plt.plot(kmean1.cluster_centers_[3], label='Cluster 4')\n    plt.legend(loc='upper right')\n    plt.ylabel(\"Mean of Variables\")\n    plt.xticks(range(len(X.columns)),X.columns, rotation=80) \n    plt.show()","3ef0b129":"first_hand1 = process_for_trend(df_train1)\nfirst_hand2 = process_for_trend(df_train2)","60037e91":"print(\"-------------------------------- First-hand House Trend  in Busan ---------------------------------\")\ntrend(first_hand1)","d9bb3176":"print(\"-------------------------------- First-hand House Trend in Busan ---------------------------------\")\ntrend(first_hand2)","47d57c79":"print(\"-------------------------------- Cluster Analysis for First-hand House in Busan ---------------------------------\")\ncluster(first_hand1)","876577f5":"print(\"-------------------------------- Cluster Analysis for First-hand House in Seoul ---------------------------------\")\ncluster(first_hand2)","fb74f380":"Reference: synergy37ai: https:\/\/www.kaggle.com\/econdata\/lightgbm-on-transactionprice-seoul","2aba709c":"## Clustering","24689328":"## For Seoul","540a2ac7":"## For all training data","aaf55ea2":"## For Busan"}}