{"cell_type":{"b9164063":"code","1ad65411":"code","42e4e1c4":"code","b6fd6f01":"code","001f58b9":"code","75d04255":"code","37fc894f":"code","59b3969b":"code","f30177b4":"code","7fbdb07a":"code","ee69a39e":"code","fa79882f":"code","8940da8a":"code","08f5dd3d":"code","441ffd9a":"code","d0ec8c06":"code","94e7e2bb":"code","e79ab4c7":"code","388a0d6b":"code","f263bff4":"code","e6108db1":"code","2dfe08da":"code","4c91f4c4":"code","4c013af3":"code","e49e0b8c":"markdown","232da427":"markdown","5c25a35b":"markdown","43634173":"markdown","642827a1":"markdown","5c05d11f":"markdown"},"source":{"b9164063":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport IPython.display as ipd\nimport librosa\nimport os\nimport librosa.display\n\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn import metrics\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime ","1ad65411":"df = pd.read_csv(\"..\/input\/urbansound8k\/UrbanSound8K.csv\")\ndf.head()","42e4e1c4":"df['class'].value_counts()","b6fd6f01":"filename1 = \"..\/input\/urbansound8k\/fold1\/101415-3-0-2.wav\"\nplt.figure(figsize=(14,5))\ndata,sample_rate=librosa.load(filename1)\nlibrosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename1)","001f58b9":"data","75d04255":"filename2 = \"..\/input\/urbansound8k\/fold5\/100032-3-0-0.wav\"\nplt.figure(figsize=(14,5))\ndata1,sample_rate1=librosa.load(filename2)\nlibrosa.display.waveplot(data1,sr=sample_rate1)\nipd.Audio(filename2)","37fc894f":"def features_extractor(file):\n    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n    \n    return mfccs_scaled_features","59b3969b":"extracted_features=[]\nfor i in range(8732):\n    file_name = '..\/input\/urbansound8k\/fold' + str(df[\"fold\"][i]) + '\/' + df[\"slice_file_name\"][i]\n    final_class_labels=df[\"class\"][i]\n    data=features_extractor(file_name)\n    extracted_features.append([data,final_class_labels])","f30177b4":"extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\nextracted_features_df.head()","7fbdb07a":"X=np.array(extracted_features_df['feature'].tolist())\ny=np.array(extracted_features_df['class'].tolist())","ee69a39e":"X.shape","fa79882f":"y=np.array(pd.get_dummies(y))","8940da8a":"y.shape","08f5dd3d":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=0,stratify=y)","441ffd9a":"X_train.shape","d0ec8c06":"X_test.shape","94e7e2bb":"num_labels=y.shape[1]\nnum_labels","e79ab4c7":"model=Sequential()\n###first layer\nmodel.add(Dense(100,input_shape=(40,)))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n###second layer\nmodel.add(Dense(200))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n###third layer\nmodel.add(Dense(100))\nmodel.add(Activation('tanh'))\nmodel.add(Dropout(0.5))\n\n###final layer\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))","388a0d6b":"model.summary()\n","f263bff4":"model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')","e6108db1":"num_epochs = 100\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath='saved_models\/audio_classification.hdf5', \n                               verbose=1, save_best_only=True)\nstart = datetime.now()\n\nhistory = model.fit(X_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","2dfe08da":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","4c91f4c4":"test_accuracy=model.evaluate(X_test,y_test,verbose=0)\ntest_accuracy","4c013af3":"predictions = model.predict(X_test)\npreds = np.argmax(predictions, axis = 1)\nresult = pd.DataFrame(preds)\nresult.to_csv(\"UrbanSound8kResults.csv\")","e49e0b8c":"# Extract Features\n\nHere we will be using Mel-Frequency Cepstral Coefficients(MFCC) from the audio samples. The MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification.","232da427":"# Now we iterate through every audio file and extract features using Mel-Frequency Cepstral Coefficients\n","5c25a35b":"# Split the dataset into independent and dependent dataset","43634173":"# Label Encoding","642827a1":"# Observation\n\nHere Librosa converts the signal to mono, meaning the channel will alays be 1","5c05d11f":"# Check whether the dataset is imbalanced"}}