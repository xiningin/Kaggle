{"cell_type":{"e4e5f23e":"code","bce6bba5":"code","69beb0c6":"code","600b4db4":"code","0ef2b816":"code","2eb1e364":"code","3d893ec0":"code","7fc085ea":"code","ac666657":"code","4b9f37f6":"code","a7cd3dd8":"code","74389480":"code","327eb3ed":"code","3983f28e":"markdown","6a9beb9d":"markdown","90999e8e":"markdown","e278463a":"markdown","495bcf37":"markdown"},"source":{"e4e5f23e":"# Import the necessary packages\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","bce6bba5":"# Import and read dataset\n\ninput_ = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\ndata = pd.read_csv(input_)\ndf = data.copy()\n\ndata.head(10)","69beb0c6":"data.describe()","600b4db4":"inp_data = data.drop(data[['DEATH_EVENT']], axis=1)\nout_data = data[['DEATH_EVENT']]\n\nscaler = StandardScaler()\ninp_data = scaler.fit_transform(inp_data)\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=42)","0ef2b816":"print(\"X_train Shape : \", X_train.shape)\nprint(\"X_test Shape  : \", X_test.shape)\nprint(\"y_train Shape : \", y_train.shape)\nprint(\"y_test Shape  : \", y_test.shape)","2eb1e364":"test_scores = []\ntrain_scores = []\n\nfor i in range(1,50):\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train, y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))","3d893ec0":"max_train_score = np.max(train_scores)\ntrain_scores_ind = [i for i,v in enumerate(train_scores) if v == max_train_score]\nprint('Max Train Score: {}'.format(max_train_score*100))\nprint(\"k: {}\".format(list(map(lambda x: x+1, train_scores_ind))))","7fc085ea":"max_test_score = np.max(test_scores)\ntest_scores_ind = [i for i,v in enumerate(test_scores) if v == max_test_score]\nprint('Max Test Score: {}'.format(max_test_score*100))\nprint(\"k: {}\".format(list(map(lambda x: x+1, test_scores_ind))))","ac666657":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,50),train_scores,marker='*',label='Train Score')\np = sns.lineplot(range(1,50),test_scores,marker='o',label='Test Score')","4b9f37f6":"knn = KNeighborsClassifier(3)\nknn.fit(X_train,y_train)\nknn.score(X_test,y_test)\n\ny_pred = knn.predict(X_test)\n\ncf_matrix = confusion_matrix(y_pred, y_test)\nsns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","a7cd3dd8":"inp_data = data.iloc[:, [4,7,11]].values\nout_data = data.iloc[:,-1].values\n\nX_train, X_test, y_train, y_test = train_test_split(inp_data, out_data, test_size=0.2, random_state=12345)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)\n\nk_range = list(range(1, 50))\nw = list([\"distance\",\"uniform\"])\nparam_grid = dict(n_neighbors=k_range, weights=w)\n\ngrid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy', n_jobs=-1)\ngrid.fit(X_train, y_train)\ngrid.best_params_","74389480":"knn = KNeighborsClassifier(n_neighbors = 4, weights='distance')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\n\n\nprint('Accuracy Score:               : {:.4f}'.format(accuracy_score(y_test, y_pred)))\nprint('KNN f1-score  : {:.4f}'.format(f1_score(y_pred, y_test)))\nprint('KNN precision : {:.4f}'.format(precision_score(y_pred, y_test)))\nprint('KNN recall    : {:.4f}'.format(recall_score(y_pred, y_test)))\nprint(\"\\n\",classification_report(y_pred, y_test))","327eb3ed":"cf_matrix = confusion_matrix(y_pred, y_test)\nsns.heatmap((cf_matrix \/ np.sum(cf_matrix)*100), annot = True, fmt=\".2f\", cmap=\"Blues\")","3983f28e":"![](https:\/\/ac-cdn.azureedge.net\/infusionnewssiteimages\/agingcare\/21e637ea-aa74-4ae2-b278-181d2cded7a3.jpg)","6a9beb9d":"## Reporting\nI evaluated the results I found with Confusion Matrix, the results are as follows:\n\n### Correctly predicted -> %71.67 (214 of 299 predict are correct)\n- True Negative -> %55.00 -> Those who were predicted not to die and who did not die\n- True Positive -> %16.67 -> Those who were predicted to die and who did die\n\n### Wrong predicted-> %28.33 (85 of 299 predict are wrong)\n- False Positive -> %03.33 -> Those who were predicted to die but who did not die\n- False Negative -> %25.00 -> Those who were predicted to not die but who did die\n\n### Not dead\n- 203 -> Those who haven't died in the real data set\n- 174 -> Predicted for test data set\n\n### The dead\n- 96 -> Those who have died in the real data set\n- 125 -> Predicted for test data set","90999e8e":"---\n\n## Finding the highest score","e278463a":"# Coding Time","495bcf37":"![KNN](https:\/\/i.ibb.co\/FWgzgFL\/knn.png)\n\n- **ML Part 1** - Logistic Regression\n- **ML Part 2 - K-Nearest Neighbors (KNN)**\n- **ML Part 3** - Support Vector Machine (SVM)\n- **ML Part 4** - Artificial Neural Network (NN) \n- **ML Part 5** - Classification and Regression Tree (CART)\n- **ML Part 6** - Random Forests\n- **ML Part 7** - Gradient Boosting Machines (GBM)\n- **ML Part 8** - XGBoost\n- **ML Part 9** - LightGBM\n- **ML Part 10** - CatBoost\n\nK-nearest neighbors is a nonparametric method used for classification and regression. It is one of the easiest machine learning techniques used. It is a lazy learning model with a local approach.\n\n## Basic Theory\nThe basic logic behind KNN is to discover what's around it, assume the test data point is similar to them, and derive the output. In KNN, we seek neighbors and come with foresight.\n\nIn the case of KNN classification, the majority vote is applied to the nearest k data points, while in the KNN regression, the average of the nearest k data points is calculated as output. As a general rule, we choose the odd numbers as k. KNN is a lazy learning model in which calculations occur only at run time. While majority voting is applied over the nearest k data points, in KNN regression, the average of the nearest k data points is calculated as output. As a general rule, we choose the odd numbers as k. KNN is a lazy learning model in which calculations occur only at run time.\n\n![](https:\/\/i.ibb.co\/CMtFYSX\/Ek-A-klama-2020-08-28-000822.jpg)\n\nYellow and purple dots in the above diagram correspond to Class A and Class B in the education data. The red star indicates test data that should be classified. When k = 3, we predict Class B as output, and when K = 6, Class A as output.\n\n## Lost Function\nThere is no training on KNN. During the test, the minimum distance k neighbor will be included in the classification \/ regression.\n\n## Advantages\n- Easy and simple machine learning model.\n- Few hyperparameters to adjust.\n\n## Disadvantages\n- The value of k should be chosen wisely.\n- High calculation cost over the run time if the sample size is large.\n- Appropriate scaling should be provided for fair treatment across features.\n\n## Hyperparameters\n- KNN mainly includes two hyperparameters, K value and distance function.\n- K value:\n    - How many neighbors will join the KNN algorithm. k should be set according to the authentication error.\n- Distance function:\n    - Euclidean distance is the most used similarity function. Manhattan distance, Hamming Distance, Minkowski distance are different alternatives.\n\n## Assumptions\n- There should be a clear requirement about the input area.\n- Moderate sample size as appropriate (due to space and time constraints).\n- Collinearity and outliers should be addressed before training.\n\n## Comparison with Other Models\n- A general difference between KNN and other models is the large real-time computing KNN needs compared to others.\n\n\n### KNN vs Naive Bayes\n- Naive bayes is much faster than KNN due to the real time implementation of KNN.\n- While naive bayes are parametric, KNN is not parametric.\n\n### KNN vs SVM\n- SVM handles outliers better than KNN.\n- SVM performs better than KNN when there are large features and less training data.\n\n### KNN vs Neural Networks (NN)\n- Neural networks require larger training data compared to KNN to achieve sufficient accuracy.\n- NN needs a lot of hyperparameter settings compared to KNN."}}