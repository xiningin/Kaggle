{"cell_type":{"005b30ea":"code","753aac4d":"code","afd02449":"code","c63b7dc4":"code","8b535ca4":"code","9a7021aa":"code","5d08e68d":"code","48ee298d":"code","31320798":"code","2d5d90dc":"code","598c7e0f":"code","0dc0d7c2":"code","91a13fa1":"code","7f98cf30":"code","20c36a2f":"code","1197c810":"markdown","c4db5edd":"markdown","9d74dcc4":"markdown","28205ded":"markdown","c8d96a09":"markdown","d9352ba4":"markdown","51a7ddf7":"markdown","495e4325":"markdown","7e147af9":"markdown","7c555979":"markdown","417f96d4":"markdown","c92c0872":"markdown","8b90088b":"markdown","31262b86":"markdown","a870196c":"markdown","9ed4e96a":"markdown","e1cb4b4c":"markdown","c30c4556":"markdown","97d1bfcb":"markdown","07317d76":"markdown","dcbd6b42":"markdown","f0da85f0":"markdown","2fb94457":"markdown","1e82bfef":"markdown","c8cab5b9":"markdown","4350d705":"markdown","9a89ca09":"markdown","3b4d756b":"markdown","03ae7e5c":"markdown","d653afc7":"markdown","6a4ddc82":"markdown","6984764f":"markdown","81fbc197":"markdown","1af03b2c":"markdown","bd03a8f0":"markdown","da5bc492":"markdown","4113dfd0":"markdown","ec0c8f84":"markdown","058dfa40":"markdown","2aa0a40f":"markdown"},"source":{"005b30ea":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \nimport numpy as np\nimport pandas as pd\nimport torch\ntorch.set_printoptions(edgeitems=2, threshold=50, linewidth=75)","753aac4d":"bikes_numpy = np.loadtxt(\n    \"\/kaggle\/input\/hour-fixed.csv\",\n    dtype=np.float32,\n    delimiter=\",\",\n    skiprows=1,\n    converters={1: lambda x: float(x[8:10])})  # Converts date strings to\n                                               # numbers corresponding to the\n                                               # day of the month in column 1\nbikes = torch.from_numpy(bikes_numpy)\nbikes","afd02449":"pd.read_csv(\"\/kaggle\/input\/hour-fixed.csv\").head()","c63b7dc4":"bikes.shape, bikes.stride()","8b535ca4":"daily_bikes = bikes.view(-1, 24, bikes.shape[1])\ndaily_bikes.shape, daily_bikes.stride()","9a7021aa":"daily_bikes = daily_bikes.transpose(1, 2)\ndaily_bikes.shape, daily_bikes.stride()","5d08e68d":"first_day = bikes[:24].long()\nweather_onehot = torch.zeros(first_day.shape[0], 4)\nfirst_day[:,9]","48ee298d":"weather_onehot.scatter_(\n    dim=1,\n    index=first_day[:,9].unsqueeze(1).long() - 1,   # Decreases the values by 1\n                                                    # because weather situation\n                                                    # ranges from 1 to 4, while\n                                                    # indices are 0-based\n    value=1.0)","31320798":"torch.cat((bikes[:24], weather_onehot), 1)[:1]","2d5d90dc":"daily_weather_onehot = torch.zeros(daily_bikes.shape[0], 4, daily_bikes.shape[2])\ndaily_weather_onehot.shape","598c7e0f":"daily_weather_onehot.scatter_(1, daily_bikes[:,9,:].long().unsqueeze(1) - 1, 1.0)\ndaily_weather_onehot.shape","0dc0d7c2":"daily_bikes = torch.cat((daily_bikes, daily_weather_onehot), dim=1)\ndaily_bikes","91a13fa1":"daily_bikes[:, 9, :] = (daily_bikes[:, 9, :] - 1.0) \/ 3.0","7f98cf30":"temp = daily_bikes[:, 10, :]\ntemp_min = torch.min(temp)\ntemp_max = torch.max(temp)\ndaily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - temp_min) \/ (temp_max - temp_min))","20c36a2f":"temp = daily_bikes[:, 10, :]\ndaily_bikes[:, 10, :] = ((daily_bikes[:, 10, :] - torch.mean(temp)) \/ torch.std(temp))","1197c810":"We see that the rightmost dimension is the number of columns in the original dataset. Then, in the middle dimension, we have time, split into chunks of 24 sequential hours. In other words, we now have N sequences of L hours in a day, for C channels. To get to our desired N \u00d7 C \u00d7 L ordering, we need to transpose the tensor:","c4db5edd":"Let\u2019s go back to our bike-sharing dataset. The first column is the index (the global ordering of the data), the second is the date, and the sixth is the time of day. We have everything we need to create a dataset of daily sequences of ride counts and other exogenous variables. Our dataset is already sorted, but if it were not, we could use torch.sort on it to order it appropriately.","9d74dcc4":"Great: we\u2019ve built another nice dataset, and we\u2019ve seen how to deal with time series data. For this tour d\u2019horizon, it\u2019s important only that we got an idea of how a time series is laid out and how we can wrangle the data in a form that a network will digest.","28205ded":"Now let\u2019s apply some of the techniques we learned earlier to this dataset.","c8d96a09":"We might want to break up the two-year dataset into wider observation periods, like days. This way we\u2019ll have N (for number of samples) collections of C sequences of length L. In other words, our time series dataset would be a tensor of dimension 3 and shape N \u00d7 C \u00d7 L. The C would remain our 17 channels, while L would be 24: 1 per hour of the day. There\u2019s no particular reason why we must use chunks of 24 hours, though the general daily rhythm is likely to give us patterns we can exploit for predictions. We could also use 7 \u00d7 24 = 168 hour blocks to chunk by week instead, if we desired. All of this depends, naturally, on our dataset having the right size\u2014the number of rows must be a multiple of 24 or 168. Also, for this to make sense, we cannot have gaps in the time series.","d9352ba4":"Remember also from the previous chapter that storage is a contiguous, linear container for numbers (floating-point, in this case). Our bikes tensor will have each row stored one after the other in its corresponding storage. This is confirmed by the output from the call to bikes.stride() earlier.","51a7ddf7":"## Ready for training","495e4325":"Our day started with weather \u201c1\u201d and ended with \u201c2,\u201d so that seems right.\nLast, we concatenate our matrix to our original dataset using the cat function.\nLet\u2019s look at the first of our results:","7e147af9":"Then we scatter the one-hot encoding into the tensor in the C dimension. Since this operation is performed in place, only the content of the tensor will change:","7c555979":"In a time series dataset such as this one, rows represent successive time-points: there is a dimension along which they are ordered. Sure, we could treat each row as independent and try to predict the number of circulating bikes based on, say, a particular time of day regardless of what happened earlier. However, the existence of an ordering gives us the opportunity to exploit causal relationships across time. For instance, it allows us to predict bike rides at one time based on the fact that it was raining at an earlier time. For the time being, we\u2019re going to focus on learning how to turn our bike-sharing dataset into something that our neural network will be able to ingest in fixed-size chunks.","417f96d4":"## Shaping the data by time period","c92c0872":"As you learned in the previous chapter, calling view on a tensor returns a new tensor that changes the number of dimensions and the striding information, without changing the storage. This means we can rearrange our tensor at basically zero cost, because no data will be copied. Our call to view requires us to provide the new shape for the returned tensor. We use -1 as a placeholder for \u201chowever many indexes are left, given the other dimensions and the original number of elements.\u201d","8b90088b":"There are multiple possibilities for rescaling variables. We can either map their range to [0.0, 1.0]","31262b86":"Here we prescribed our original bikes dataset and our one-hot-encoded \u201cweather situation\u201d matrix to be concatenated along the column dimension (that is, 1). In other words, the columns of the two datasets are stacked together; or, equivalently, the new one-hot-encoded columns are appended to the original dataset. For cat to succeed, it is required that the tensors have the same size along the other dimensions\u2014the row dimension, in this case. Note that our new last four columns are 1, 0, 0, 0 , exactly as we would expect with a weather value of 1.","a870196c":"The \u201cweather situation\u201d variable is ordinal. It has four levels: 1 for good weather, and 4 for, er, really bad. We could treat this variable as categorical, with levels interpreted as labels, or as a continuous variable. If we decided to go with categorical, we would turn the variable into a one-hot-encoded vector and concatenate the columns with the dataset.","9ed4e96a":"This neural network model will need to see a number of sequences of values for each different quantity, such as ride count, time of day, temperature, and weather conditions: N parallel sequences of size C. C stands for channel, in neural network parlance, and is the same as column for 1D data like we have here. The N dimension represents the time axis, here one entry per hour.","e1cb4b4c":"All we have to do to obtain our daily hours dataset is view the same tensor in batches of 24 hours. Let\u2019s take a look at the shape and strides of our bikes tensor:","c30c4556":"https:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code","97d1bfcb":"* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-dog-detection\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-gan-horse-zebra\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch3-tensors\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-working-with-images\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-3d-images-volumetric-data\n\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch4-tabular-data","07317d76":"What happened here? First, bikes.shape[1] is 17, the number of columns in the bikes tensor. But the real crux of this code is the call to view , which is really important: it changes the way the tensor looks at the same data as contained in storage.","dcbd6b42":"or subtract the mean and divide by the standard deviation:","f0da85f0":"# Working with time series","2fb94457":"We could have done the same with the reshaped daily_bikes tensor. Remember that it is shaped (B, C, L), where L = 24. We first create the zero tensor, with the same B and L, but with the number of additional columns as C:","1e82bfef":"**NOTE** The version of the file we\u2019re using, hour-fixed.csv, has had some processing done to include rows missing from the original dataset. We presume that the missing hours had zero bike active (they were typically in the early morning hours).","c8cab5b9":"![image.png](attachment:image.png)","4350d705":"All data from book **Deep Learning with PyTorch** https:\/\/pytorch.org\/deep-learning-with-pytorch","9a89ca09":"That\u2019s 17,520 hours, 17 columns. Now let\u2019s reshape the data to have 3 axes\u2014day, hour, and then our 17 columns:","3b4d756b":"In the latter case, our variable will have 0 mean and unitary standard deviation. If our variable were drawn from a Gaussian distribution, 68% of the samples would sit in the [-1.0, 1.0] interval.","03ae7e5c":"This could also be a case where it is useful to go beyond the main path. Speculatively, we could also try to reflect l ike categorical, but with order more directly by generalizing one-hot encodings to mapping the i th of our four categories here to a vector that has ones in the positions 0...i and zeros beyond that. Or\u2014similar to the embeddings we discussed in section 4.5.4\u2014we could take partial sums of embeddings, in which case it might make sense to make those positive. As with many things we encounter in practical work, this could be a place where trying what works for others and then experimenting in a systematic fashion is a good idea.","d653afc7":"Then we scatter ones into our matrix according to the corresponding level at each row. Remember the use of unsqueeze to add a singleton dimension as we did in the previous sections:","6a4ddc82":"Other kinds of data look like a time series, in that there is a strict ordering. Top two on the list? Text and audio. We\u2019ll take a look at text next, and the \u201cConclusion\u201d section has links to additional examples for audio.","6984764f":"## Adding a time dimension ","81fbc197":"In the source data, each row is a separate hour of data (figure 4.5 shows a transposed version of this to better fit on the printed page). We want to change the row-per-hour organization so that we have one axis that increases at a rate of one day per index increment, and another axis that represents the hour of the day (independent of the date). The third axis will be our different columns of data (weather, temperature, and so on). Let\u2019s load the data (code\/p1ch4\/4_time_series_bikes.ipynb).","1af03b2c":"Going back to the wine dataset, we could have had a \u201cyear\u201d column that allowed us to look at how wine quality evolved year after year. Unfortunately, we don\u2019t have such data at hand, but we\u2019re working hard on manually collecting the data samples, bottle by bottle. (Stuff for our second edition.) In the meantime, we\u2019ll switch to another interesting dataset: data from a Washington, D.C., bike-sharing system reporting the hourly count of rental bikes in 2011\u20132012 in the Capital Bikeshare system, along with weather and seasonal information (available here: http:\/\/mng.bz\/jgOx). Our goal will be to take a flat, 2D dataset and transform it into a 3D one, as shown in figure 4.5.","bd03a8f0":"In order to make it easier to render our data, we\u2019re going to limit ourselves to the first day for a moment. We initialize a zero-filled matrix with a number of rows equal to the number of hours in the day and number of columns equal to the number of weather levels:","da5bc492":"And we concatenate along the C dimension:","4113dfd0":"As we mentioned in the previous section, rescaling variables to the [0.0, 1.0] interval or the [-1.0, 1.0] interval is something we\u2019ll want to do for all quantitative variables, like temperature (column 10 in our dataset). We\u2019ll see why later; for now, let\u2019s just say that this is beneficial to the training process.","ec0c8f84":"In the previous section, we covered how to represent data organized in a flat table. As we noted, every row in the table was independent from the others; their order did not matter. Or, equivalently, there was no column that encoded information about what rows came earlier and what came later.","058dfa40":"For daily_bikes , the stride is telling us that advancing by 1 along the hour dimension (the second dimension) requires us to advance by 17 places in the storage (or one set of columns); whereas advancing along the day dimension (the first dimension) requires us to advance by a number of elements equal to the length of a row in the storage times 24 (here, 408, which is 17 \u00d7 24).","2aa0a40f":"For every hour, the dataset reports the following variables:\n*  Index of record: instant\n*  Day of month: day\n*  Season: season ( 1 : spring, 2 : summer, 3 : fall, 4 : winter)\n*  Year: yr ( 0 : 2011, 1 : 2012)\n*  Month: mnth ( 1 to 12 )\n*  Hour: hr ( 0 to 23 )\n*  Holiday status: holiday\n*  Day of the week: weekday\n*  Working day status: workingday\n*  Weather situation: weathersit ( 1 : clear, 2 :mist, 3 : light rain\/snow, 4 : heavy rain\/snow)\n*  Temperature in \u00b0C: temp\n*  Perceived temperature in \u00b0C: atemp\n*  Humidity: hum\n*  Wind speed: windspeed\n*  Number of casual users: casual\n*  Number of registered users: registered\n*  Count of rental bikes: cnt"}}