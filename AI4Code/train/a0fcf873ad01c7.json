{"cell_type":{"ec517e60":"code","1ac04cd8":"code","b97f28d9":"code","92b23fda":"code","cba969c1":"code","8c06ff2f":"code","50bd5ce9":"code","567d6726":"code","248f47f0":"code","3aa5c56f":"code","02424902":"code","1df3d522":"code","cf424136":"code","b0bf5527":"code","1f2b96c6":"code","fcb21096":"code","df4f4c73":"code","3d817025":"code","04ce8212":"code","32aed498":"code","3a9fa540":"code","1f8902e5":"code","ee9a8397":"code","e3d06c76":"code","87367c3b":"code","2c093e13":"code","7b52c1e3":"code","c6cfeb87":"code","3bf2d524":"code","4fa6365d":"code","ef764f01":"code","3a6f657d":"code","8a1446e2":"code","97d97353":"code","33c857ec":"code","0757285c":"code","0284e98d":"code","0e3feabd":"code","31e41df6":"code","cc31cdc3":"code","78fe2aa2":"code","db5dfced":"code","9c3ccee9":"code","7691c260":"code","f4d53146":"code","3465b4cb":"code","66146ce0":"code","7467615a":"code","1b70026b":"code","8669e9a7":"code","90f4ef06":"code","b4d9a3e7":"code","0f8228fb":"code","9959a9f5":"code","aedd1800":"code","50b2332f":"code","1b5d2fd2":"code","cd3a0483":"code","673255bb":"code","3f82ab9c":"code","3e8747b6":"code","7250550d":"code","ff8e63e6":"code","d73ea1d6":"code","39da2620":"code","f818d13d":"code","3e4a9fbc":"code","c6c16c6e":"code","13e0817a":"code","8a99eb41":"code","e07abcbf":"code","c921de98":"code","dadf62d2":"code","4e9a82b6":"code","aa3d5270":"markdown","b5a1a3a0":"markdown","b11c4b85":"markdown","5c69e377":"markdown","f4f7dbb2":"markdown","8e6f6267":"markdown","25a854d6":"markdown","1271ad28":"markdown","0e7e37c2":"markdown","5bbcc818":"markdown","3d838b6f":"markdown","b2e10ca1":"markdown","b65d5ab9":"markdown","c13a063c":"markdown","78fd2273":"markdown","a09e5c32":"markdown","2db9db27":"markdown","a1f16011":"markdown","a24b840c":"markdown","e002fdcd":"markdown","beedd821":"markdown","58108249":"markdown","fef6cd59":"markdown","10344bd8":"markdown","90e7aea4":"markdown","9f8e32ea":"markdown","1f2b0803":"markdown"},"source":{"ec517e60":"import numpy as np\nimport pandas as pd\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom math import sqrt\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error\n\n%matplotlib inline","1ac04cd8":"# Import dataset\ntrain_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\ntrain_data.shape, test_data.shape","b97f28d9":"# Preview train dataset\ntrain_data.head()","92b23fda":"# Preview test dataset\ntest_data.head()","cba969c1":"# Remove IDs from train and test set, not useful for model\ntrain_ID = train_data['Id']\ntest_ID = test_data['Id']\ntrain_data.drop(['Id'], axis=1, inplace=True)\ntest_data.drop(['Id'], axis=1, inplace=True)\ntrain_data.shape, test_data.shape","8c06ff2f":"# Analyze SalePrice\ntrain_data['SalePrice'].describe()","50bd5ce9":"# Distribution plot\nf, ax = plt.subplots(figsize=(10,5))\nsns.distplot(train_data['SalePrice'])\nax.set(xlabel=\"SalePrice\")\nax.set(ylabel=\"Frequency\")\nax.set(title=\"SalePrice Distribution\")\nplt.show()","567d6726":"# Skewness and Kurtosis\n# Skewness - measure of the lack of symmetry in the data\n# Kurtosis - shows whether there is many outliers in the data\nprint(\"Skewness: %f\" % train_data['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_data['SalePrice'].kurt())","248f47f0":"# Find the numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric_features = []\nfor i in train_data.columns:\n    if train_data[i].dtype in numeric_dtypes:\n        numeric_features.append(i)","3aa5c56f":"# Visualizing the outliers in numeric features\nplt.subplots(ncols=2, nrows=0, figsize=(12,120))\nplt.subplots_adjust(right=2)\nplt.subplots_adjust(top=2)\nfor i, feature in enumerate(list(train_data[numeric_features]), 1):\n    plt.subplot(len(list(numeric_features)), 3, i)\n    sns.scatterplot(x=feature, y='SalePrice', data=train_data)\n    plt.xlabel('{}'.format(feature), size=15,labelpad=12.5)\n    plt.ylabel('SalePrice', size=15, labelpad=12.5)\n    \nplt.show()","02424902":"# Look at data correlation using heatmap\ncorr = train_data.corr()\nplt.subplots(figsize=(15,15))\nsns.heatmap(corr, fmt='.1f', cmap=\"Blues\", square=True)\nplt.show()","1df3d522":"# Box plot SalePrice\/OverallQual\nfeature = 'OverallQual'\nplt.figure(figsize=(10,5))\nsns.boxplot(train_data[feature], train_data['SalePrice'])\nplt.xlabel(feature)\nplt.ylabel('SalePrice')\nplt.axis(ymin=0, ymax=800000)\nplt.show()","cf424136":"# Box plot SalePrice\/YearBuilt\nfeature = 'YearBuilt'\nplt.figure(figsize=(20,10))\nsns.boxplot(train_data[feature], train_data['SalePrice'])\nplt.xlabel(feature)\nplt.ylabel('SalePrice')\nplt.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);\nplt.show()","b0bf5527":"# Scatter plot SalePrice\/GrLivArea\nfeature = 'GrLivArea'\nplt.figure(figsize=(10,5))\nplt.scatter(train_data[feature], train_data['SalePrice'], c='b', alpha=0.3)\nplt.xlabel(feature)\nplt.ylabel('SalePrice')\nplt.axis(ymin=0, ymax=800000)\nplt.show()","1f2b96c6":"# Scatter plot SalePrice\/GarageArea\nfeature = 'GarageArea'\nplt.figure(figsize=(10,5))\nplt.scatter(train_data[feature], train_data['SalePrice'], c='b', alpha=0.3)\nplt.xlabel(feature)\nplt.ylabel('SalePrice')\nplt.axis(ymin=0, ymax=800000)\nplt.show()","fcb21096":"# Scatter plot SalePrice\/LotArea\nfeature = 'LotArea'\nplt.figure(figsize=(10,5))\nplt.scatter(train_data[feature], train_data['SalePrice'], c='b', alpha=0.3)\nplt.xlabel(feature)\nplt.ylabel('SalePrice')\nplt.axis(ymin=0, ymax=800000)\nplt.show()","df4f4c73":"# Scatter plot SalePrice\/TotalBsmtSF\nfeature = 'TotalBsmtSF'\nplt.figure(figsize=(10,5))\nplt.scatter(train_data[feature], train_data['SalePrice'], c='b', alpha=0.3)\nplt.xlabel(feature)\nplt.ylabel('SalePrice')\nplt.axis(ymin=0, ymax=800000)\nplt.show()","3d817025":"# Box plot SalePrice\/GarageCars\nfeature = 'GarageCars'\nplt.figure(figsize=(10,5))\nsns.boxplot(train_data[feature], train_data['SalePrice'])\nplt.xlabel(feature)\nplt.ylabel('SalePrice')\nplt.axis(ymin=0, ymax=800000);\nplt.show()","04ce8212":"# Remove outliers\ntrain_data.drop(train_data[(train_data['OverallQual']<5) & (train_data['SalePrice']>200000)].index, inplace=True)\ntrain_data.drop(train_data[(train_data['GrLivArea']>4500) & (train_data['SalePrice']<300000)].index, inplace=True)\ntrain_data.reset_index(drop=True, inplace=True)\ntrain_data.shape","32aed498":"# Some non-numeric features stored as numbers, convert to strings\ntrain_data['MSSubClass'] = train_data['MSSubClass'].astype(str)\n#train_data['YrSold'] = train_data['YrSold'].astype(str)\ntrain_data['MoSold'] = train_data['MoSold'].astype(str)\n#train_data['GarageYrBlt'] = train_data['GarageYrBlt'].astype(str)\n\n# How about OverallQual and OverallCond?\n# YrSold and GarageYrBlt should be numeric, not categorial","3a9fa540":"# Split into train and validation datasets\n# Random train and validation dataset\ntrain_data, validation_data = train_test_split(train_data, test_size=0.25, random_state=42, shuffle=True)","1f8902e5":"train_data.describe()","ee9a8397":"validation_data.describe()","e3d06c76":"diffmeanpercent = (train_data.mean() - validation_data.mean())\/ train_data.mean()\ndiffmeanpercent","87367c3b":"# Split features\nY_train = train_data['SalePrice'].reset_index(drop=True)\nX_train = train_data.drop(['SalePrice'], axis=1)\nY_validation = validation_data['SalePrice'].reset_index(drop=True)\nX_validation = validation_data.drop(['SalePrice'], axis=1)\n","2c093e13":"f, ax = plt.subplots(figsize=(10,5))\nsns.distplot(Y_train, fit=norm)\nax.set(xlabel=\"SalePrice\")\nax.set(ylabel=\"Frequency\")\nax.set(title=\"SalePrice Distribution (training set)\")\nplt.show()","7b52c1e3":"f, ax = plt.subplots(figsize=(10,5))\nsns.distplot(Y_validation, fit=norm)\nax.set(xlabel=\"SalePrice\")\nax.set(ylabel=\"Frequency\")\nax.set(title=\"SalePrice Distribution (validation set)\")\nplt.show()","c6cfeb87":"Y_train = np.log1p(Y_train)\nY_validation = np.log1p(Y_validation)","3bf2d524":"f, ax = plt.subplots(figsize=(10,5))\nsns.distplot(Y_train, fit=norm)\nax.set(xlabel=\"SalePrice\")\nax.set(ylabel=\"Frequency\")\nax.set(title=\"SalePrice Distribution (training set)\")\nplt.show()","4fa6365d":"f, ax = plt.subplots(figsize=(10,5))\nsns.distplot(Y_validation, fit=norm)\nax.set(xlabel=\"SalePrice\")\nax.set(ylabel=\"Frequency\")\nax.set(title=\"SalePrice Distribution (validation set)\")\nplt.show()","ef764f01":"Y_train.shape, X_train.shape, Y_validation.shape, X_validation.shape","3a6f657d":"X_test = test_data","8a1446e2":"# Function to calculate the percentage of missing data of each feature\ndef calc_percentage_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    output = pd.DataFrame(sorted(dict_x.items(), key=lambda x: x[1], reverse=True), columns=['Feature', 'Percentage missing data'])\n    return output","97d97353":"# Training set\nX_train_missing = calc_percentage_missing(X_train)\nX_train_missing.head(10)","33c857ec":"# Validation set\nX_validation_missing = calc_percentage_missing(X_validation)\nX_validation_missing.head(10)","0757285c":"# Test set\nX_test_missing = calc_percentage_missing(X_test)\nX_test_missing.head(10)","0284e98d":"# Create function to visualize the amount of missing values\ndef viz_missing_vals(df):\n    df = df[df['Percentage missing data']>0]\n    plt.subplots(figsize=(10,5))\n    plt.bar(df['Feature'], df['Percentage missing data'])\n    plt.xticks(rotation=90);\n    plt.xlabel('Features')\n    plt.ylabel('Percentage of missing values')\n    plt.title('Percentage of missing data by feature')\n    plt.show()","0e3feabd":"# Training data\nviz_missing_vals(X_train_missing)","31e41df6":"# Validation data\nviz_missing_vals(X_validation_missing)","cc31cdc3":"# no of features with missing values in training set\nlen(X_train_missing[X_train_missing['Percentage missing data'] > 0])","78fe2aa2":"# no of features with missing values in validation set\nlen(X_validation_missing[X_validation_missing['Percentage missing data'] > 0])","db5dfced":"# no of features with missing values in test set\nlen(X_test_missing[X_test_missing['Percentage missing data'] > 0])","9c3ccee9":"# Function to handle missing values in each feature in training set\ndef handle_missing_features_train(features):\n    # categorial features\n    features['PoolQC'] = features['PoolQC'].fillna(\"None\")\n    features['MiscFeature'] = features['MiscFeature'].fillna(\"None\")\n    features['Alley'] = features['Alley'].fillna(\"None\")\n    features['Fence'] = features['Fence'].fillna(\"None\")\n    features['FireplaceQu'] = features['FireplaceQu'].fillna(\"None\")\n    features['MasVnrType'] = features['MasVnrType'].fillna(\"None\")\n    \n    # numerical features\n    features['LotFrontage'] = features['LotFrontage'].fillna(features['LotFrontage'].mode()[0])\n    features['MasVnrArea'] = features['MasVnrArea'].fillna(0)\n    features['MSZoning'] = features['MSZoning'].fillna(features['MSZoning'].mode()[0])\n    features['Utilities'] = features['Utilities'].fillna(features['Utilities'].mode()[0])\n    features['Functional'] = features['Functional'].fillna(features['Functional'].mode()[0])\n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['Electrical'] = features['Electrical'].fillna(features['Electrical'].mode()[0])\n    features['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n\n    # garage features\n    for col in ('GarageFinish', 'GarageQual', 'GarageCond', 'GarageType'):\n        features[col]  = features[col].fillna(\"None\")\n    for col in ('GarageYrBlt', 'GarageCars', 'GarageArea'):\n        features[col] = features[col].fillna(0)\n    \n    # basement features\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col]  = features[col].fillna(\"None\")\n    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath'):\n        features[col] = features[col].fillna(0)\n        \n    return features","7691c260":"# Function to handle missing values in each feature in datasets other than training set\n# Uses what was filled in the training set to fill into the other datasets\ndef handle_missing_features_other(dataset, train_set):\n    dataset['PoolQC'] = dataset['PoolQC'].fillna(\"None\")\n    dataset['MiscFeature'] = dataset['MiscFeature'].fillna(\"None\")\n    dataset['Alley'] = dataset['Alley'].fillna(\"None\")\n    dataset['Fence'] = dataset['Fence'].fillna(\"None\")\n    dataset['FireplaceQu'] = dataset['FireplaceQu'].fillna(\"None\")\n    dataset['MasVnrType'] = dataset['MasVnrType'].fillna(\"None\")\n    \n    # numerical features\n    dataset['LotFrontage'] = dataset['LotFrontage'].fillna(train_set['LotFrontage'].mode()[0])\n    dataset['MasVnrArea'] = dataset['MasVnrArea'].fillna(0)\n    dataset['MSZoning'] = dataset['MSZoning'].fillna(train_set['MSZoning'].mode()[0])\n    dataset['Utilities'] = dataset['Utilities'].fillna(train_set['Utilities'].mode()[0])\n    dataset['Functional'] = dataset['Functional'].fillna(train_set['Functional'].mode()[0])\n    dataset['Exterior1st'] = dataset['Exterior1st'].fillna(train_set['Exterior1st'].mode()[0])\n    dataset['Exterior2nd'] = dataset['Exterior2nd'].fillna(train_set['Exterior2nd'].mode()[0])\n    dataset['Electrical'] = dataset['Electrical'].fillna(train_set['Electrical'].mode()[0])\n    dataset['KitchenQual'] = dataset['KitchenQual'].fillna(train_set['KitchenQual'].mode()[0])\n    dataset['SaleType'] = dataset['SaleType'].fillna(train_set['SaleType'].mode()[0])\n\n    \n    # garage features\n    for col in ('GarageFinish', 'GarageQual', 'GarageCond', 'GarageType'):\n        dataset[col]  = dataset[col].fillna(\"None\")\n    for col in ('GarageYrBlt', 'GarageCars', 'GarageArea'):\n        dataset[col] = dataset[col].fillna(0)\n    \n    # basement features\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        dataset[col]  = dataset[col].fillna(\"None\")\n    for col in ('BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath'):\n        dataset[col] = dataset[col].fillna(0)\n        \n    return dataset","f4d53146":"# Training data\nhandle_missing_features_train(X_train)","3465b4cb":"# Validaton data\nhandle_missing_features_other(X_validation, X_train)","66146ce0":"# Test data\nhandle_missing_features_other(X_test, X_train)","7467615a":"# Check that we did not miss any features with missing values\nlen(X_train_missing[X_train_missing['Percentage missing data'] > 0]), len(X_validation_missing[X_validation_missing['Percentage missing data'] > 0]), len(X_test_missing[X_test_missing['Percentage missing data'] > 0])","1b70026b":"# Find the numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric_features = []\nfor i in X_train.columns:\n    if X_train[i].dtype in numeric_dtypes:\n        numeric_features.append(i)","8669e9a7":"# Find skewed numeric features\nfeatures_skew = X_train[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skewed_features = features_skew[features_skew>0.5]\nskew_index = high_skewed_features.index\n\nhigh_skewed_features","90f4ef06":"high_skewed_features_names = list(high_skewed_features.index.values)","b4d9a3e7":"high_skewed_features.count()","0f8228fb":"# Train data\nf, ax = plt.subplots(figsize=(10,5))\nsns.boxplot(data=X_train[high_skewed_features_names], orient='h')\nax.set_xscale(\"log\")\nax.set(ylabel=\"Name of Features\")\nax.set(xlabel=\"Numeric value\")\nax.set(title=\"Distribution of Numeric Features (train_data)\")\nplt.show()","9959a9f5":"# Validation data\nf, ax = plt.subplots(figsize=(10,5))\nsns.boxplot(data=X_validation[high_skewed_features_names], orient='h')\nax.set_xscale(\"log\")\nax.set(ylabel=\"Name of Features\")\nax.set(xlabel=\"Numeric value\")\nax.set(title=\"Distribution of Numeric Features (validation_data)\")\nplt.show()","aedd1800":"# Test data\nf, ax = plt.subplots(figsize=(10,5))\nsns.boxplot(data=X_test[high_skewed_features_names], orient='h')\nax.set_xscale(\"log\")\nax.set(ylabel=\"Name of Features\")\nax.set(xlabel=\"Numeric value\")\nax.set(title=\"Distribution of Numeric Features (test_data)\")\nplt.show()","50b2332f":"plt.subplots(figsize=(12,120))\nfor i, feature in enumerate(list(X_train[high_skewed_features_names]), 1):\n    plt.subplot(len(list(high_skewed_features_names)), 3, i)\n    try:\n        sns.distplot(X_train[feature], bins=50, fit=norm, color='r')\n    except:\n        sns.distplot(X_train[feature], bins=50, fit=norm, color='r', kde_kws={'bw':0.1}) # fix for the problems described above\nplt.show() ","1b5d2fd2":"# Transform skewed features in training set\nfor i in skew_index:\n    #X_train[i] = boxcox1p(X_train[i], boxcox_normmax(X_train[i]+1))\n    X_train[i] = np.log1p(X_train[i])\n    #X_train[i] = np.sqrt(X_train[i])","cd3a0483":"# Calculate skewness after transform\nskewed_features_after = X_train[high_skewed_features_names].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewed_features_after","673255bb":"# Show difference in skewedness after transform\nskew_diff = high_skewed_features - skewed_features_after\nskew_diff","3f82ab9c":"# Validation set\nfor i in skew_index:\n    #X_validation[i] = boxcox1p(X_validation[i], boxcox_normmax(X_validation[i]+1))\n    X_validation[i] = np.log1p(X_validation[i])\n    #X_validation[i] = np.sqrt(X_validation[i])","3e8747b6":"# Test set\nfor i in skew_index:\n    #X_test[i] = boxcox1p(X_test[i], boxcox_normmax(X_test[i]+1))\n    X_test[i] = np.log1p(X_test[i])\n    #X_test[i] = np.sqrt(X_test[i])","7250550d":"# Combine the datasets\nX_all = [X_train, X_validation, X_test]\nX_all = pd.concat(X_all)","ff8e63e6":"# Create custom features\n#X_all['TotalBath'] = X_all.apply(lambda row: (row.FullBath + (0.5 * row.HalfBath) + row.BsmtFullBath + (0.5 * row.BsmtHalfBath)), axis=1)\n#X_all['HouseAge'] = X_all.apply(lambda row: 2010 - row.YearBuilt, axis=1)\n#X_all['GarageAreaPerCar'] = X_all.apply(lambda row: row.GarageArea \/ row.GarageCars if row.GarageCars>=1 else 0, axis=1) # gives inf in predictions\n#X_all['HomeOverallQuality'] = X_all.apply(lambda row: row.OverallQual + row.OverallCond, axis=1) # gives inf in predictions\n\n# Include powers of features which appear to have non linear correlation\n#X_all['OverallQual_sq'] = X_all.apply(lambda row: row.OverallQual **2, axis=1) # gives inf\n#X_all['OverallCond_sq'] = X_all.apply(lambda row: row.OverallCond **2, axis=1) # gives inf","d73ea1d6":"X_all = pd.get_dummies(X_all).reset_index(drop=True)\nX_all.shape","39da2620":"# Remove any duplicated column names\nX_all = X_all.loc[:,~X_all.columns.duplicated()]\nX_all.shape","f818d13d":"# Split into respective sets\nX_train = X_all.iloc[:len(Y_train), :]\nX_validation = X_all.iloc[len(Y_train):(len(Y_train)+len(Y_validation)), :]\nX_test = X_all.iloc[(len(Y_train)+len(Y_validation)):, :]\nX_train.shape, X_validation.shape, X_test.shape","3e4a9fbc":"def use_model(model):\n    model.fit(X_train, Y_train)\n    \n    # Calculate train and validation predictions\n    Y_train_prediction = model.predict(X_train)\n    Y_train_prediction = np.expm1(Y_train_prediction)\n    Y_validation_prediction = model.predict(X_validation)\n    Y_validation_prediction = np.expm1(Y_validation_prediction)\n\n    msle_train = mean_squared_log_error(Y_train, Y_train_prediction)\n    rmsle_train = sqrt(msle_train)\n    print(\"Training set rmsle:\", rmsle_train)\n    msle_validation = mean_squared_log_error(Y_validation, Y_validation_prediction)\n    rmsle_validation = sqrt(msle_validation)\n    print(\"Validations set rmsle:\", rmsle_validation)\n    \n    # Make predictions on competition test set\n    Y_test_prediction = model.predict(X_test)\n    Y_test_prediction = np.expm1(Y_test_prediction)\n    print(\"Y test prediction\")\n    print(Y_test_prediction)\n    \n    return Y_test_prediction","c6c16c6e":"# Build model\nlinear_reg = LinearRegression()\nY_test_prediction = use_model(linear_reg)","13e0817a":"# Lasso regression\nlasso = LassoCV(alphas = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100])\nY_test_prediction = use_model(lasso)\nalpha = lasso.alpha_\nprint('Best alpha is:', alpha)","8a99eb41":"# Use the calculated best alpha and optimize it again\nlasso2 = LassoCV(alphas = [alpha*0.1, alpha*0.2, alpha*0.3, alpha*0.4, alpha*0.5, alpha*0.6, alpha*0.7, alpha*0.8, alpha*0.9, alpha*1, alpha*1.2, alpha*1.4, alpha*1.6, alpha*1.8, alpha*2])\nY_test_prediction = use_model(lasso2)\nalpha2 = lasso2.alpha_\nprint('New best alpha is:', alpha2)","e07abcbf":"coef = pd.Series(lasso.coef_, index = X_train.columns)\n\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","c921de98":"# Ridge regression\nridge = RidgeCV(alphas = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100])\nY_test_prediction = use_model(ridge)\nalpha = ridge.alpha_\nprint('Best alpha is:', alpha)","dadf62d2":"coef = pd.Series(ridge.coef_, index = X_train.columns)\n\nprint(\"Ridge picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","4e9a82b6":"# Make submission csv file\nsubmission = pd.DataFrame({'Id': test_ID, 'SalePrice': Y_test_prediction})\nsubmission.to_csv('my_submission.csv', index=False)\nprint(\"Submission saved\")","aa3d5270":"## Regularized Linear Regression","b5a1a3a0":"# Exploratory Data Analysis","b11c4b85":"# Credits\nAs this is my first Kaggle competition, I have read quite a number of notebooks to help me start my journey.\n\nTo name a few:\n- [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Stacked-Regressions-to-predict-House-Prices): Easy and simple to follow yet has its own complexities with model stacking.\n- [How I made top 0.3% on a Kaggle competition](https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition\/comments): More detailed model stacking which allowed me to discover new models.\n- [A study on Regression applied to the Ames dataset](https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\/notebook): Detailed feature engineering and linear regression analysis but still manages to be easy to follow for beginners.\n\nI hope that my notebooks would also be able to give beginners what these notebooks have provided to me.","5c69e377":"## Create submission CSV file","f4f7dbb2":"Notes on SalePrice:\n- Deviate from normal distribution\n- Have positive skewness","8e6f6267":"Remove outliers in the dataset as noticed from the scatter plots. There might be more outliers that can be removed.","25a854d6":"Use histogram and boxplot to visualize the skewedness of features with a value of skew > 0.5. Many features have a value 0 because of a lack of the particular house feature, e.g. PoolArea has alot of 0 values because not all houses have pools. This causes a problem when plotting the histograms. A fix is to include an additional parameter to the histogram plotting function. More info about the problem [here](http:\/\/https:\/\/github.com\/mwaskom\/seaborn\/issues\/1990).","1271ad28":"## Relationship between features","0e7e37c2":"## Unregularized Linear Regression","5bbcc818":"Plot how SalePrice relates to some of the features.","3d838b6f":"Can observe that SalePrice is skewed to the right. This is a problem as most ML models do not do well with non-normally distributed data. Apply a log(1+x) transform to fix the skew.","b2e10ca1":"## SalePrice","b65d5ab9":"Apply the same transformation function to the validation and test set.","c13a063c":"# Modelling","78fd2273":"We want to fix the skewed features. First, we transform the training data and then use the same parameters to tansform the validation and test data. We use the scipy function boxcox1p which computes the Box-Cox transformation. The gives us a simple transformation that allows us to normalize the data.","a09e5c32":"Visualizing the features of the dataset.","2db9db27":"## Handle missing values\nDo it for the training, validation and test sets separately.","a1f16011":"## Encode categorial features\nEncode categorial features as ML models can only handle numerical features.","a24b840c":"# My first Kaggle competition","e002fdcd":"Look into the distribution of SalePrice","beedd821":"Plot how the features are correlated to each other and SalePrice.","58108249":"## Find skewed features","fef6cd59":"For each dataset, go through each feature and input suitable missing values.","10344bd8":"# Feature Engineering","90e7aea4":"# To Do\n- Consider custom features\n- Feature scaling\n- Explore more deepy Lasso and Ridge\n- Feature scaling for Lasso and Ridge\n- Model stacking\n- Find out why sqrt works rather than boxcox or log for unregularized linear regression\n- Beautify some graphs\n- Add more explanations in some parts, especialy those I found difficult to grasp","9f8e32ea":"## Create custom features","1f2b0803":"## Create training and validation datasets\nSplit the dataset before changing any values to prevent data leakage."}}