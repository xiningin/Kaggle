{"cell_type":{"07aeef91":"code","7b5212e4":"code","d119782a":"code","ca7ad4a7":"code","9d672822":"code","3092f82f":"code","84f3350b":"code","6a1e4046":"code","4ed6215c":"code","e4e5ca4c":"code","34ee9ffb":"code","ef189fd3":"code","f133f10f":"code","f5c47dbf":"code","1488d909":"code","6cb030c5":"code","a9c1c4eb":"code","73ecd85c":"code","c02f28f9":"code","94b178fd":"code","2f31ba86":"code","7c8ddd90":"code","1da5e1ff":"code","438563f9":"code","607e2c74":"markdown","08689a19":"markdown","6e46515e":"markdown","ef9384bc":"markdown","295d75c0":"markdown","c8584ec2":"markdown","012f1b6b":"markdown","4a039efd":"markdown","419c44b8":"markdown","c5ee773a":"markdown","f918627d":"markdown","0eacedd9":"markdown","8103895e":"markdown","98c44b65":"markdown","e7ca1d57":"markdown","089081af":"markdown","e7977e55":"markdown","a57765c5":"markdown","91ddbf44":"markdown","fe0cd5fb":"markdown","7cfa3020":"markdown","a5a3852a":"markdown","b6092e1f":"markdown","d86efab3":"markdown","94b49558":"markdown","daa85657":"markdown","e2efb266":"markdown","4efe9564":"markdown","b2137d66":"markdown","6f2cbe53":"markdown","282c453a":"markdown","29e57433":"markdown","4e2586f9":"markdown","97f08422":"markdown","92a65bed":"markdown","619080a8":"markdown","64d76bdf":"markdown"},"source":{"07aeef91":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport regex as re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport wordcloud\nfrom wordcloud import WordCloud\nimport nltk\nfrom nltk.stem import WordNetLemmatizer \nnltk.download('wordnet')\nimport spacy\nnlp = spacy.load('en_core_web_sm')\nnltk.download('stopwords')","7b5212e4":"import tensorflow as tf\n\n# Get the GPU device name.\ndevice_name = tf.test.gpu_device_name()\n\n# The device name should look like the following:\nif device_name == '\/device:GPU:0':\n    print('Found GPU at: {}'.format(device_name))\nelse:\n    raise SystemError('GPU device not found')","d119782a":"import torch\nimport torch.nn as nn\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","ca7ad4a7":"train_inputs = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/train_inputs.csv').values.astype(np.int_)\ntrain_masks = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/train_masks.csv').values.astype(np.int_)\ntrain_labels = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/train_labels.csv').values.astype(np.int_)\n\nvalidation_inputs = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/validation_inputs.csv').values.astype(np.int_)\nvalidation_masks = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/validation_masks.csv').values.astype(np.int_)\nvalidation_labels = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/validation_labels.csv').values.astype(np.int_)\n\ntest_inputs = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/test_inputs.csv').values.astype(np.int_)\ntest_masks = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/test_masks.csv').values.astype(np.int_)\ntest_labels = pd.read_csv('..\/input\/bert-hate-speech-train-test-validation\/test_labels.csv').values.astype(np.int_)","9d672822":"# Convert all inputs and labels into torch tensors, the required datatype \n# for our model.\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntest_inputs = torch.tensor(test_inputs)\n\ntrain_labels = torch.tensor(train_labels.reshape((-1)).tolist())\nvalidation_labels = torch.tensor(validation_labels.reshape((-1)).tolist())\ntest_labels = torch.tensor(test_labels.reshape((-1)).tolist())\n\n\ntrain_masks = torch.tensor(train_masks)     \nvalidation_masks = torch.tensor(validation_masks)\ntest_masks = torch.tensor(test_masks)","3092f82f":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# The DataLoader needs to know our batch size for training, so we specify it \n# here.\n# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n# 16 or 32.\n\nbatch_size = 32\n\n# Create the DataLoader for our training set.\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set.\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set.\ntest_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = SequentialSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=validation_sampler, batch_size=batch_size)","84f3350b":"from transformers import AutoModel, BertTokenizerFast\nbert = AutoModel.from_pretrained('bert-base-uncased')","6a1e4046":"# freeze all the parameters\nfor param in bert.parameters():\n    param.requires_grad = False","4ed6215c":"class BERT_Arch(nn.Module):\n\n    def __init__(self, bert):\n\n        super(BERT_Arch, self).__init__()\n\n        self.bert = bert \n\n        # dropout layer\n        self.dropout = nn.Dropout(0.1)\n\n        # relu activation function\n        self.relu =  nn.ReLU()\n\n        # dense layer 1\n        self.fc1 = nn.Linear(768,512)\n\n        # dense layer 2 (Output layer)\n        self.fc2 = nn.Linear(512,2)\n\n        #softmax activation function\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    #define the forward pass\n    def forward(self, sent_id, mask):\n\n        #pass the inputs to the model  \n        _, cls_hs = self.bert(sent_id, attention_mask=mask)\n\n        \n#         print(cls_hs.shape)\n        \n        x = self.fc1(cls_hs)\n\n        x = self.relu(x)\n\n        x = self.dropout(x)\n\n        # output layer\n        x = self.fc2(x)\n\n        # apply softmax activation\n        x = self.softmax(x)\n\n        return x","e4e5ca4c":"# pass the pre-trained BERT to our define architecture\nmodel = BERT_Arch(bert)\n\n# push the model to GPU\nmodel = model.to(device)","34ee9ffb":"# optimizer from hugging face transformers\nfrom transformers import AdamW\n\n# define the optimizer\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5)          # learning rate","ef189fd3":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n","f133f10f":"from sklearn.utils.class_weight import compute_class_weight\n\n#compute the class weights\nclass_weights = compute_class_weight('balanced', np.unique(train_labels.cpu().detach().numpy()), train_labels.cpu().detach().numpy().flatten())\n\nprint(\"Class Weights:\",class_weights)","f5c47dbf":"# converting list of class weights to a tensor\nweights= torch.tensor(class_weights,dtype=torch.float)\n\n# push to GPU\nweights = weights.to(device)\n\n# define the loss function\ncross_entropy  = nn.NLLLoss(weight=weights) \n\n# number of training epochs\nepochs = 300","1488d909":"# function to train the model\ndef train():\n  \n    model.train()\n\n    total_loss, total_accuracy = 0, 0\n  \n    # empty list to save model predictions\n    total_preds=[]\n\n    # iterate over batches\n    for step,batch in enumerate(train_dataloader):\n\n        # progress update after every 50 batches.\n        if step % 50 == 0 and not step == 0:\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n\n        # push the batch to gpu\n        batch = [r.to(device) for r in batch]\n\n        sent_id, mask, labels = batch\n\n        # clear previously calculated gradients \n        model.zero_grad()        \n\n        # get model predictions for the current batch\n        preds = model(sent_id, mask)\n\n        # compute the loss between actual and predicted values\n        loss = cross_entropy(preds, labels)\n\n        # add on to the total loss\n        total_loss = total_loss + loss.item()\n\n        # backward pass to calculate the gradients\n        loss.backward()\n\n        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # update parameters\n        optimizer.step()\n\n        # model predictions are stored on GPU. So, push it to CPU\n        preds=preds.detach().cpu().numpy()\n\n        # append the model predictions\n        total_preds.append(preds)\n\n    # compute the training loss of the epoch\n    avg_loss = total_loss \/ len(train_dataloader)\n\n    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n\n    #returns the loss and predictions\n    return avg_loss, total_preds\n","6cb030c5":"# function for evaluating the model\ndef evaluate():\n\n    print(\"\\nEvaluating...\")\n\n    # deactivate dropout layers\n    model.eval()\n\n    total_loss, total_accuracy = 0, 0\n\n    # empty list to save the model predictions\n    total_preds = []\n\n    # iterate over batches\n    for step,batch in enumerate(validation_dataloader):\n\n        # Progress update every 50 batches.\n        if step % 50 == 0 and not step == 0:\n\n\n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(validation_dataloader)))\n\n        # push the batch to gpu\n        batch = [t.to(device) for t in batch]\n\n        sent_id, mask, labels = batch\n\n        # deactivate autograd\n        with torch.no_grad():\n\n            # model predictions\n            preds = model(sent_id, mask)\n\n            # compute the validation loss between actual and predicted values\n            loss = cross_entropy(preds,labels)\n\n            total_loss = total_loss + loss.item()\n\n            preds = preds.detach().cpu().numpy()\n\n            total_preds.append(preds)\n\n    # compute the validation loss of the epoch\n    avg_loss = total_loss \/ len(validation_dataloader) \n\n    # reshape the predictions in form of (number of samples, no. of classes)\n    total_preds  = np.concatenate(total_preds, axis=0)\n    print(total_preds)\n    return avg_loss, total_preds","a9c1c4eb":"from sklearn.metrics import f1_score\n# set initial loss to infinite\nbest_valid_loss = float('inf')\nbest_f1_score = 0.0\n# empty lists to store training and validation loss of each epoch\nval_true = validation_labels.detach().cpu().numpy().reshape((-1,1)) # the actual predictions\n# print(val_true.shape)\ntrain_losses=[]\nvalid_losses=[]\nf1_scores_validation = []\n#for each epoch\nfor epoch in range(epochs):\n    t0 = time.time()\n    print('\\n Epoch {:} \/ {:}'.format(epoch + 1, epochs))\n    \n    #train model\n    train_loss, _ = train()\n    \n    #evaluate model\n    valid_loss, val_preds = evaluate()\n    val_preds =  np.argmax(val_preds,axis = 1).reshape((-1,1))\n    f1_score_val = f1_score(val_true,val_preds)\n    print(f1_score_val)\n    \n    #save the best model\n    if f1_score_val > best_f1_score:\n        best_f1_score = f1_score_val\n        torch.save(model.state_dict(), 'saved_weights.pt')\n\n    # append training and validation loss\n    train_losses.append(train_loss)\n    valid_losses.append(valid_loss)\n    f1_scores_validation.append(f1_score_val)\n    elapsed = format_time(time.time() - t0)\n    print('Elapsed Time', elapsed)   \n    print(f'\\nTraining Loss: {train_loss:.3f}')   \n    print(f'Validation Loss: {valid_loss:.3f}')","73ecd85c":"plt.plot(train_losses)   \nplt.plot(valid_losses)            \nplt.show()           ","c02f28f9":"plt.plot(f1_scores_validation) ","94b178fd":"#load weights of best model\npath = 'saved_weights.pt'\nmodel.load_state_dict(torch.load(path))","2f31ba86":"# get predictions for validation data\nval_preds = np.array([]).reshape((-1,2))\nfor step,batch in enumerate(validation_dataloader):\n\n    # Progress update every 50 batches.\n    if step % 50 == 0 and not step == 0:\n\n\n        # Report progress.\n        print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(validation_dataloader)))\n\n    # push the batch to gpu\n    batch = [t.to(device) for t in batch]\n\n    sent_id, mask, labels = batch\n\n    # deactivate autograd\n    with torch.no_grad():\n\n        # model predictions\n        preds = model(sent_id, mask)\n\n        preds = preds.detach().cpu().numpy()\n        val_preds = np.concatenate((val_preds,preds))","7c8ddd90":"temp = val_preds","1da5e1ff":"from sklearn.metrics import classification_report\nval_true = validation_labels.detach().cpu().numpy().reshape((-1,1))\nval_preds =  np.argmax(val_preds,axis = 1).reshape((-1,1))\nprint(classification_report(val_true, val_preds))","438563f9":"#2000 - 0.5288135593220338\n#3000 - 0.5017301038062284\n#4000 - 0.48686514886164617","607e2c74":"# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n# I believe the 'W' stands for 'Weight Decay fix\"\noptimizer = AdamW(model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\n","08689a19":"from sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score\ncm = confusion_matrix(y_test,y_pred)\nprint(precision_score(y_test,y_pred),recall_score(y_test,y_pred),f1_score(y_test,y_pred))","6e46515e":"# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","ef9384bc":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = .2,random_state =0 )","295d75c0":"## Optimizer & Learning Rate Scheduler\nNow that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n\nFor the purposes of fine-tuning, the authors recommend choosing from the following values:\n- Batch size: 16, 32  (We chose 32 when creating our DataLoaders).\n- Learning rate (Adam): 5e-5, 3e-5, 2e-5  (We'll use 2e-5).\n- Number of epochs: 2, 3, 4  (We'll use 4).\n\nThe epsilon parameter `eps = 1e-8` is \"a very small number to prevent any division by zero in the implementation\" (from [here](https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/)).\n\n","c8584ec2":"# # We will do random search \n# \/*params = {\n#     kernel:['linear', 'poly', 'rbf', 'sigmoid', 'precomputed'],\n# }*\/\n\n\n","012f1b6b":"### Function for calculation of elapsed time","4a039efd":"## END","419c44b8":"# we have got a bow model\n# for starters maybe we can run  a logistic regression on it\nfrom sklearn.linear_model import LogisticRegression\nregressor = LogisticRegression()\nregressor.fit(X_train,y_train)","c5ee773a":"import matplotlib.pyplot as plt\n\n\nimport seaborn as sns\n\n# Use plot styling from seaborn.\nsns.set(style='darkgrid')\n\n# Increase the plot size and font size.\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\n# Plot the learning curve.\n# plt.plot(loss_values, 'b-o')\nplt.plot(eval_f1,'b-o')\n# Label the plot.\nplt.title(\"Training loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\n\nplt.show()","f918627d":"# need to create a bag of words model\n# consider bigrams ? lets go one by one\nbow = vectorizer.transform(df.iloc[:,2])\ny = df.iloc[:,1]","0eacedd9":"<h2> Converting inputs to bert embedding <\/h2>\n","8103895e":"#first we need to make the text document\n\n# we will be creating bag of words model using tfidf\nvectorizer = TfidfVectorizer(\n    tokenizer = tokenize,\n    ngram_range = (1,2),\n    use_idf = True,\n    max_features = 10000\n    # min_df = ...\n    # max_df = ...\n)\nvectorizer.fit(X_train)\n\ntfIdf_train = vectorizer.transform(X_train)\ntfIdf_test = vectorizer.transform(X_test)","98c44b65":"#first we need to make the text document\n\n\n# we will be creating bag of words model using tfidf\nvectorizer = TfidfVectorizer(max_features=2000)\nvectorizer.fit(df.iloc[:,2])\n\n# need to create a bag of words model\n# consider bigrams ? lets go one by one\nbow = vectorizer.transform(df.iloc[:,2])\ny = df.iloc[:,1]\n#splitting the data set \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(bow,y,test_size = .2,random_state =0)\n\n# we have got a bow model\n# for starters maybe we can run  a logistic regression on it\nfrom sklearn.linear_model import LogisticRegression\nregressor = LogisticRegression()\nregressor.fit(X_train,y_train)\n\ny_pred = regressor.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix,precision_score,recall_score,f1_score\ncm = confusion_matrix(y_test,y_pred)\nprint(precision_score(y_test,y_pred),recall_score(y_test,y_pred),f1_score(y_test,y_pred))","e7ca1d57":"# Training Loop","089081af":"print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","e7977e55":"from transformers import BertForSequenceClassification, AdamW, BertConfig\n\n# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels = 2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions = False, # Whether the model returns attentions weights.\n    output_hidden_states = False, # Whether the model returns all hidden-states.\n)\n\n# Tell pytorch to run this model on the GPU.\nmodel.cuda()","a57765c5":"import random\n\n# This training code is based on the `run_glue.py` script here:\n# https:\/\/github.com\/huggingface\/transformers\/blob\/5bfcd0485ece086ebcbed2d008813037968a9e58\/examples\/run_glue.py#L128\n\n\n# Set the seed value all over the place to make this reproducible.\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\n# Store the average loss after each epoch so we can plot them.\nloss_values = []\neval_f1 = []\neval_precision = []\neval_recall = []\n# For each epoch...\nfor epoch_i in range(0, epochs):\n    \n    # ========================================\n    #               Training\n    # ========================================\n    \n    # Perform one full pass over the training set.\n\n    print(\"\")\n    print('======== Epoch {:} \/ {:} ========'.format(epoch_i + 1, epochs))\n    print('Training...')\n\n    # Measure how long the training epoch takes.\n    t0 = time.time()\n\n    # Reset the total loss for this epoch.\n    total_loss = 0\n\n    # Put the model into training mode. Don't be mislead--the call to \n    # `train` just changes the *mode*, it doesn't *perform* the training.\n    # `dropout` and `batchnorm` layers behave differently during training\n    # vs. test (source: https:\/\/stackoverflow.com\/questions\/51433378\/what-does-model-train-do-in-pytorch)\n    model.train()\n\n    # For each batch of training data...\n    for step, batch in enumerate(train_dataloader):\n\n        # Progress update every 40 batches.\n        if step % 40 == 0 and not step == 0:\n            # Calculate elapsed time in minutes.\n            elapsed = format_time(time.time() - t0)\n            \n            # Report progress.\n            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n\n        # Unpack this training batch from our dataloader. \n        #\n        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n        # `to` method.\n        #\n        # `batch` contains three pytorch tensors:\n        #   [0]: input ids \n        #   [1]: attention masks\n        #   [2]: labels \n        b_input_ids = batch[0].to(device)\n        b_input_mask = batch[1].to(device)\n        b_labels = batch[2].to(device)\n\n        # Always clear any previously calculated gradients before performing a\n        # backward pass. PyTorch doesn't do this automatically because \n        # accumulating the gradients is \"convenient while training RNNs\". \n        # (source: https:\/\/stackoverflow.com\/questions\/48001598\/why-do-we-need-to-call-zero-grad-in-pytorch)\n        model.zero_grad()        \n\n        # Perform a forward pass (evaluate the model on this training batch).\n        # This will return the loss (rather than the model output) because we\n        # have provided the `labels`.\n        # The documentation for this `model` function is here: \n        # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n        outputs = model(b_input_ids, \n                    token_type_ids=None, \n                    attention_mask=b_input_mask, \n                    labels=b_labels)\n        \n        # The call to `model` always returns a tuple, so we need to pull the \n        # loss value out of the tuple.\n        loss = outputs[0]\n\n        # Accumulate the training loss over all of the batches so that we can\n        # calculate the average loss at the end. `loss` is a Tensor containing a\n        # single value; the `.item()` function just returns the Python value \n        # from the tensor.\n        total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        # The optimizer dictates the \"update rule\"--how the parameters are\n        # modified based on their gradients, the learning rate, etc.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n\n    # Calculate the average loss over the training data.\n    avg_train_loss = total_loss \/ len(train_dataloader)            \n    \n    # Store the loss value for plotting the learning curve.\n    loss_values.append(avg_train_loss)\n\n    print(\"\")\n    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n        \n    # ========================================\n    #               Validation\n    # ========================================\n    # After the completion of each training epoch, measure our performance on\n    # our validation set.\n\n    print(\"\")\n    print(\"Running Validation...\")\n\n    t0 = time.time()\n\n    # Put the model in evaluation mode--the dropout layers behave differently\n    # during evaluation.\n    model.eval()\n\n    # Tracking variables \n    eval_f1_temp,eval_precision_temp,eval_recall_temp, eval_accuracy = 0, 0, 0, 0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        \n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        \n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        \n        # Telling the model not to compute or store gradients, saving memory and\n        # speeding up validation\n        with torch.no_grad():        \n\n            # Forward pass, calculate logit predictions.\n            # This will return the logits rather than the loss because we have\n            # not provided labels.\n            # token_type_ids is the same as the \"segment ids\", which \n            # differentiates sentence 1 and 2 in 2-sentence tasks.\n            # The documentation for this `model` function is here: \n            # https:\/\/huggingface.co\/transformers\/v2.2.0\/model_doc\/bert.html#transformers.BertForSequenceClassification\n            outputs = model(b_input_ids, \n                            token_type_ids=None, \n                            attention_mask=b_input_mask)\n        \n        # Get the \"logits\" output by the model. The \"logits\" are the output\n        # values prior to applying an activation function like the softmax.\n        logits = outputs[0]\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n        \n        # Calculate the accuracy for this batch of test sentences.\n        metrics = flat_accuracy(logits,label_ids)\n        tmp_eval_accuracy = metrics[0]\n        eval_f1_temp += metrics[1]\n        eval_precision_temp += metrics[2]\n        eval_recall_temp += metrics[3]\n        \n        # Accumulate the total accuracy.\n        eval_accuracy += tmp_eval_accuracy\n        \n        # Track the number of batches\n        nb_eval_steps += 1\n    \n    # Report the final accuracy for this validation run.\n    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy\/nb_eval_steps))\n    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n    eval_f1.append(eval_f1_temp\/nb_eval_steps)\n    eval_precision.append(eval_precision_temp\/nb_eval_steps)\n    eval_recall.append(eval_recall_temp\/nb_eval_steps)\n\nprint(\"\")\nprint(\"Training complete!\")","91ddbf44":"y_pred = regressor.predict(X_test)","fe0cd5fb":"vocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\nidf_vals = vectorizer.idf_\nidf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores","7cfa3020":"Installing the Hugging Face library (transformers)","a5a3852a":"## Loading the data","b6092e1f":"## Train Our Classification Model","d86efab3":"tfIdf_train = tfIdf_train.toarray()\ntfIdf_test = tfIdf_test.toarray()","94b49558":"<h1> Preprocessing W.R.T Bert <\/h1>\n","daa85657":"#We will train an svm classifier\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear')\nclassifier.fit(tfIdf_train,y_train)\n\ny_pred = classifier.predict(tfIdf_test)\n\nfrom sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score\n\ncm = confusion_matrix()\nf1 = f1_score(y_test,y_pred)\nprecision = precision_score(y_test,y_pred)\nrecall = recall_score(y_test,y_pred)\n\nprint(f1,precision,recall)\nprint(cm)","e2efb266":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n","4efe9564":"#splitting the data set \nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(bow,y,test_size = .2,random_state =0)","b2137d66":"print(vectorizer.vocabulary_,vectorizer.get_params,vectorizer)","6f2cbe53":"tfIdf_train.shape","282c453a":"from transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 10\n\n# Total number of training steps is number of batches * number of epochs.\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = 0, # Default value in run_glue.py\n                                            num_training_steps = total_steps)","29e57433":"### Function for calculating accuracy","4e2586f9":"There is a class imbalance in our dataset. The majority of the observations are not spam. So, we will first compute class weights for the labels in the train set and then pass these weights to the loss function so that it takes care of the class imbalance.","97f08422":"In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. ","92a65bed":"import numpy as np\nfrom sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score\n \n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    \n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    print(type(pred_flat),type(labels_flat))\n    f1 = f1_score(labels_flat,pred_flat)\n    precision = precision_score(labels_flat,pred_flat)\n    recall = recall_score(labels_flat,pred_flat)\n\n    print(f1,precision,recall)\n    return [np.sum(pred_flat == labels_flat) \/ len(labels_flat), f1, precision,recall]","619080a8":"Just for curiosity's sake, we can browse all of the model's parameters by name here.\n\nIn the below cell, We have printed out the names and dimensions of the weights for:\n\n1. The embedding layer.\n2. The first of the twelve transformers.\n3. The output layer.\n\n\n","64d76bdf":"Let's take a look at our training loss over all batches:"}}