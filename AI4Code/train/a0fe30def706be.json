{"cell_type":{"738e7dea":"code","96061ef1":"code","0afebf7a":"code","b56c6d1e":"code","8c03f62b":"code","210a1998":"code","0a8ac4fa":"code","93cc5fed":"code","ea4d151f":"code","65da7eac":"code","c083302a":"code","8e1deb63":"code","0eb28a67":"code","34697104":"code","1b4765ed":"code","187c1caf":"code","02ca54b4":"code","5f3a537b":"code","fce2583b":"code","0ed625a3":"code","f291dfe7":"code","d54f864c":"code","efbd708e":"code","56d35193":"code","df883acd":"code","bdce8e3b":"code","4bb9dd9d":"code","bc8342ce":"code","147f52ed":"code","c2280133":"code","933bd25f":"code","b1ea4062":"code","6cf92481":"code","2718b9bf":"code","af88b7ec":"code","8de70de8":"code","9f0f7f3d":"code","86587504":"code","95862838":"code","67fe9406":"code","c87eeb94":"code","84be33bb":"code","773e00db":"code","1ef36add":"code","1543fbb7":"code","40b08df9":"code","5479afd7":"code","5e019d78":"code","020b0255":"code","6f12d167":"code","8eb2fe79":"code","5d65fd19":"markdown","280ad4ba":"markdown","65bfa946":"markdown","b2559f6a":"markdown","8609f70c":"markdown","c9bc2164":"markdown","a37a235c":"markdown","19d8e044":"markdown","fefbc2ec":"markdown","6689298f":"markdown","fcf62239":"markdown","5cf6d212":"markdown"},"source":{"738e7dea":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.decomposition import PCA\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import metrics\n\nfrom sklearn.metrics import explained_variance_score, mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.inspection import permutation_importance\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","96061ef1":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0afebf7a":"naval_df = pd.read_csv(os.path.join(dirname, filename), delim_whitespace=True, header=None)\nnaval_df.head()","b56c6d1e":"naval_df.columns = ['lever_position', 'ship_speed', 'gt_shaft', 'gt_rate', 'gg_rate', 'sp_torque', 'pp_torque', 'hpt_temp', 'gt_c_i_temp', 'gt_c_o_temp', 'hpt_pressure', 'gt_c_i_pressure', 'gt_c_o_pressure', 'gt_exhaust_pressure', 'turbine_inj_control', 'fuel_flow', 'gt_c_decay',  'gt_t_decay']","8c03f62b":"100*naval_df.isna().sum()\/len(naval_df)","210a1998":"naval_df = naval_df.dropna()","0a8ac4fa":"# ??\n#naval_df = naval_df.drop('gt_c_i_temp', axis=1)","93cc5fed":"naval_df.head()","ea4d151f":"naval_df.describe()","65da7eac":"naval_df.info()","c083302a":"naval_df.shape","8e1deb63":"def PrintUniqueLenforallCols(temp_df):\n    for col in temp_df:\n        print(col, len(temp_df[col].unique()))\n        \nPrintUniqueLenforallCols(naval_df)","0eb28a67":"# we can drop gt_c_i_pressure and gt_c_i_temp as they have only 1 unique value, and thus not conributing to our dataset\nnaval_df = naval_df.drop(['gt_c_i_pressure', 'gt_c_i_temp'], axis=1)","34697104":"# let's check lever_position and ship_speed\nnaval_df.lever_position.unique()","1b4765ed":"naval_df.ship_speed.unique()","187c1caf":"naval_df.gt_c_decay.unique()","02ca54b4":"naval_df.gt_t_decay.unique()","5f3a537b":"# let's look at the target variables: \nplt.figure(figsize=(10, 6))\nplt.plot(naval_df.index, naval_df.gt_c_decay,'.-')\nplt.xlabel(\"sampleID\")\nplt.ylabel(\"gt_c_decay\")\nplt.show()","fce2583b":"plt.figure(figsize=(10, 6))\nplt.plot(naval_df.index, naval_df.gt_t_decay,'.-')\nplt.xlabel(\"sampleID\")\nplt.ylabel(\"gt_t_decay\")\nplt.show()","0ed625a3":"# Let's check plot for other features\nplt.figure(figsize=(22, 20))\nicount =1\nfor col in naval_df.columns:\n    plt.subplot(4,4, icount)\n    sns.boxplot(naval_df[col], orient=\"v\")\n    icount = icount+1\nplt.show()","f291dfe7":"plt.figure(figsize=(20,20))\nicount =1\nfor col in naval_df.columns:\n    plt.subplot(4,4, icount)\n    sns.distplot(naval_df[col])\n    icount +=1\nplt.show()","d54f864c":"# let's check the pairplot\nsns.pairplot(naval_df)\nplt.show()","efbd708e":"# let's look at pair plot only for continuous variables\nsns.pairplot(naval_df[naval_df.columns[2:-2]])\nplt.show()","56d35193":"plt.figure(figsize=(15,10))\ncols = naval_df.corr().index\ncorr_mat = np.corrcoef(naval_df[cols].values.T)\nsns.set(font_scale =1)\nhm = sns.heatmap(corr_mat, annot=True, yticklabels = cols.values, xticklabels=cols.values)\nplt.show()","df883acd":"# First let's split data into X and y\n\n# we have two target variables, so we'll have two sets \nnp.random.seed(0)\ndf_train_navel, df_test_navel = train_test_split(naval_df, train_size = 0.7, test_size=0.3, random_state = 100)\n","bdce8e3b":"y_train_c = df_train_navel.pop('gt_c_decay')\ny_train_t = df_train_navel.pop('gt_t_decay')\nX_train = df_train_navel\n\n\ny_test_c = df_test_navel.pop('gt_c_decay')\ny_test_t = df_test_navel.pop('gt_t_decay')\nX_test = df_test_navel\n","4bb9dd9d":"X_train.shape","bc8342ce":"# convert train data\ntr_scaled_features = StandardScaler().fit_transform(X_train.values)\nX_train = pd.DataFrame(tr_scaled_features, index=X_train.index, columns=X_train.columns)\n\n\n# convert test data\ntt_scaled_features = StandardScaler().fit_transform(X_test.values)\nX_test = pd.DataFrame(tt_scaled_features, index=X_test.index, columns=X_test.columns)\n","147f52ed":"pca = PCA(random_state=42)","c2280133":"pca.fit(X_train)","933bd25f":"plt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)","b1ea4062":"var_cumu = np.cumsum(pca.explained_variance_ratio_)\nplt.plot(range(1,len(var_cumu)+1), var_cumu)\nplt.grid()","6cf92481":"print(\"no. of Components  Variance accounted\")\nfor i in range (2, 8):\n    s = (\"      \" + str(i)+ \"             \"+ str(100*var_cumu[i]));\n    print(s)","2718b9bf":"def getPCAMostImportantFeat(model, initial_feature_names):\n    # number of components\n    n_pcs= model.components_.shape[0]\n\n    # get the index of the most important feature on EACH component based on argument value\n    most_important = [np.abs(model.components_[i]).argmax() for i in range(n_pcs)]\n\n    # get the names\n    most_important_names = [initial_feature_names[most_important[i]] for i in range(n_pcs)]\n\n    # Create a dictionary for PCA components and most important feature \n    dic = {'PC{}'.format(i): most_important_names[i] for i in range(n_pcs)}\n\n    # build the dataframe\n    df = pd.DataFrame(dic.items())\n    return df","af88b7ec":"pca_grid_df = getPCAMostImportantFeat(pca,X_train.columns)\npca_grid_df","8de70de8":"pca_4_cpnt = PCA(n_components=4, random_state=42)","9f0f7f3d":"navel_pca_data = pca_4_cpnt.fit_transform(X_train)","86587504":"cmp_lst = []\nfor i in range(1,5):\n    s = 'PC'+ str(i)\n    cmp_lst.append(s)","95862838":"#Create Dataframe\nnavel_pca_X = pd.DataFrame(navel_pca_data, columns=cmp_lst)\nnavel_pca_X","67fe9406":"navel_pca_X.reset_index(drop=True, inplace=True)","c87eeb94":"x_pca_cols = pca_grid_df.iloc[:, 1].tolist()","84be33bb":"# most important feature after running Logistic Regression\n# important_features_lg = pd.Series(lg_coef, index=x_pca_lg_cols)\n# important_features_lg.sort_values()[-10:].plot(kind = 'barh')","773e00db":"cmp_lst","1ef36add":"# Transform test set\nnavel_pca_data_test = pca_4_cpnt.transform(X_test)                               \nnavel_pca_test_X  = pd.DataFrame(navel_pca_data_test, columns=cmp_lst)\n#navel_pca_test_X","1543fbb7":"from sklearn.pipeline import Pipeline\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import BaggingRegressor #Ensemble using averaging method\nfrom xgboost import XGBRegressor #Ensemble using boosting method\nfrom sklearn.ensemble import GradientBoostingRegressor","40b08df9":"# the models that you want to compare\nmodels = {'LinearRegression': LinearRegression(),\n          'RandomForestRegressor': RandomForestRegressor(),\n          'KNeighborsRegressor': KNeighborsRegressor(),\n          'DecisionTreeRegressor':DecisionTreeRegressor(),\n          'BaggingRegressor' : BaggingRegressor(),\n          'XGBRegressor': XGBRegressor()}\n\n\n# the optimisation parameters for each of the above models\nparams = {'LinearRegression': [{'fit_intercept':[True,False],'normalize':[True,False], 'copy_X':[True, False]}],\n          'RandomForestRegressor': [{'n_estimators': [ 50, 60, 80]}],\n          'KNeighborsRegressor': [{'n_neighbors': [2,3,4,5,6]}],\n          'DecisionTreeRegressor': [{'max_depth': [2,4,6,8,10,12]}],        \n          'BaggingRegressor': [{'base_estimator': [None, GradientBoostingRegressor(), KNeighborsRegressor()],\n          'n_estimators': [20,50,100]}],\n          'XGBRegressor': [{'n_estimators': [50,500]}]\n         }\n\n\n#models = {'BaggingRegressor' : BaggingRegressor()}\n#params = {'BaggingRegressor': [{'base_estimator': [None, KNeighborsRegressor()]}]}\n\nx_pca_cols = pca_grid_df.iloc[:, 1].tolist() \nimportant_features_list = []\nplt.figure(figsize=(20, 12))\n\n\ndef runregressors(X_train, Y_train, X_test, Y_test):\n    \"\"\"\n    fits the list of models to the training data, thereby obtaining in each \n    case an evaluation score after GridSearchCV cross-validation\n    \"\"\"\n    i_count = 0\n    fig, ax = plt.subplots(nrows=3, ncols=2, figsize = (20, 15))\n    \n    # Evaluations\n    result_name = []\n    result_summary1 = []\n    result_mae = []\n    result_mse = []\n    result_exp_var = []\n    result_r2_score = []\n    result_ac_score = []\n\n    for name in models.keys():\n        est = models[name]\n        est_params = params[name]\n        gscv = GridSearchCV(estimator=est, param_grid=est_params, cv=5) #, verbose=2\n        gscv.fit(X_train, Y_train)\n        \n        msg1 = str(gscv.best_estimator_)\n        result_summary1.append(msg1)\n        result_name.append(name)\n        \n\n        # Evaluate the model\n        y_pred = gscv.predict(X_test)\n        score = explained_variance_score(Y_test, y_pred)\n        mae = mean_absolute_error(Y_test, y_pred)\n        mse = mean_squared_error(Y_test, y_pred)\n        ascore =gscv.best_estimator_.score( X_test, Y_test)\n        r2 = r2_score(Y_test, y_pred)\n        msg2 = \"%s: %f (%f)\" % (name, score*100, mae*100)\n        #print(msg2)\n        result_mse.append(mse)\n        result_mae.append(mae)\n        result_exp_var.append(score)\n        result_r2_score.append(r2)\n        result_ac_score.append(ascore)\n\n        if name == \"LinearRegression\":\n            important_features = pd.Series(gscv.best_estimator_.coef_ , index=x_pca_cols[:4])\n        elif name == \"KNeighborsRegressor\":\n            # perform permutation importance\n            results = permutation_importance(gscv.best_estimator_, X_train, Y_train, scoring='neg_mean_squared_error')\n            # get importance\n            important_features = pd.Series(results.importances_mean , index=x_pca_cols[:4])\n        elif name == \"BaggingRegressor\":\n            feature_importances = np.mean([tree.feature_importances_ for tree in gscv.best_estimator_], axis=0)\n            important_features = pd.Series(feature_importances , index=x_pca_cols[:4])\n            #threshold = np.mean(feature_importances)\n        else:\n            important_features = pd.Series(gscv.best_estimator_.feature_importances_ , index=x_pca_cols[:4])\n        important_features_list.append(important_features)\n        #important_features.sort_values().plot(kind = 'barh')\n        col = i_count%2\n        row = i_count\/\/2\n        ax[row][col].scatter(Y_test, y_pred)\n        ax[row][col].plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=2)\n        ax[row][col].set_xlabel('Measured')\n        ax[row][col].set_ylabel('Predicted')\n        ax[row][col].set_title(msg2)\n        i_count+=1\n            \n    plt.show()\n\n    \n    result_summary_list = pd.DataFrame({'name': result_name,\n                                        'best_estimator': result_summary1,\n                                        'R2': result_r2_score,\n                                        'MAE': result_mae,\n                                        'MSE': result_mse,\n                                        'explained variance score': result_exp_var,\n                                        'accuracy': result_ac_score})\n    return result_summary_list\n        ","5479afd7":"result_summary_list = runregressors(navel_pca_X, y_train_c, navel_pca_test_X, y_test_c)","5e019d78":"for i in range(0,4):\n    important_features_list[0][i]  = abs(important_features_list[0][i])\n\nfig, ax = plt.subplots(nrows=3, ncols=2, figsize = (20, 15))\ni_count = 0\nnm = result_summary_list.name.to_list()\nfor imp_fea in important_features_list:\n    col = i_count%2\n    row = i_count\/\/2\n    imp_fea.sort_values().plot(kind = 'barh', ax = ax[row][col] )\n    ax[row][col].set_title(nm[i_count])\n    i_count+=1\n            \nplt.show()","020b0255":"\nresult_summary_list","6f12d167":"result_summary_list_t= runregressors(navel_pca_X, y_train_t, navel_pca_test_X, y_test_t)","8eb2fe79":"result_summary_list_t","5d65fd19":"# Predicting Compressor Decay","280ad4ba":"#### from above graph we can se that there seems to be linear pattern between the features, let us check the correlation between parameters ","65bfa946":"# Maintenance of Naval Propulsion Plants Data Set\n#### _Predicting Gas Turbine propulsion plant's decay state coefficient_\n\n\n## Motivation:\nIn this case-study, we generate a predictive model to predict of decay state of rotating equipment using\n\n## Abstract: \n\nDataset source (http:\/\/archive.ics.uci.edu\/ml\/datasets\/condition+based+maintenance+of+naval+propulsion+plants)\nKaggle: https:\/\/www.kaggle.com\/elikplim\/maintenance-of-naval-propulsion-plants-data-set\n\nData have been generated from a sophisticated simulator of a Gas Turbines (GT), mounted on a Frigate characterized by a COmbined Diesel eLectric And Gas (CODLAG) propulsion plant type.\n\n## Problem Statement: \n\nThe experiments have been carried out by means of a numerical simulator of a naval vessel (Frigate) characterized by a Gas Turbine (GT) propulsion plant. The different blocks forming the complete simulator (Propeller, Hull, GT, Gear Box and Controller) have been developed and fine tuned over the year on several similar real propulsion plants. In view of these observations the available data are in agreement with a possible real vessel.\n\nIn this release of the simulator it is also possible to take into account the performance decay over time of the GT components such as GT compressor and turbines.\n\nThe propulsion system behaviour has been described with this parameters:\n- Ship speed (linear function of the lever position lp).\n- Compressor degradation coefficient kMc.\n- Turbine degradation coefficient kMt.\nso that each possible degradation state can be described by a combination of this triple (lp,kMt,kMc).\n\nThe range of decay of compressor and turbine has been sampled with an uniform grid of precision 0.001 so to have a good granularity of representation.\nIn particular for the compressor decay state discretization the kMc coefficient has been investigated in the domain [1; 0.95], and the turbine coefficient in the domain [1; 0.975].\nShip speed has been investigated sampling the range of feasible speed from 3 knots to 27 knots with a granularity of representation equal to tree knots.\nA series of measures (16 features) which indirectly represents of the state of the system subject to performance decay has been acquired and stored in the dataset over the parameter's space.\n\n\n## Background\n\n\n##### reference:\n- https:\/\/www.simonwenkel.com\/2019\/04\/19\/revisitingML-naval-propulsion.html\n- https:\/\/www.researchgate.net\/publication\/245386997_Real-time_simulation_of_a_COGAG_naval_ship_propulsion_system\n- https:\/\/www.linkedin.com\/pulse\/gas-turbine-compressor-decay-state-coefficient-john-kingsley\/?trackingId=5S5swf3uTqCizwyGWxxSIw%3D%3D","b2559f6a":"we don't see any outliner, let's now see normal distribution","8609f70c":"Let's look at the target variables: ","c9bc2164":"# Conclusion\n\n**Predicting Compressor decays**\n*    KNeighborsRegressor with 84% seems to be the best model for prediction.\n\n\n**Predicting turbine decays**\n*    This seems to be a lot more challenging. Despite rather good metrics, we can see different \u201ccategories\u201d quite clearly. Hence, we can conclude that all models generalize rather poorly.","a37a235c":"# Model Building","19d8e044":"Steps:\n\n1. Data cleaning\/Preparation\n2. EDA: Data visuallization and Understanding\n3. PCA: Feature selection\n4. Model building and hypertuning with GridsearchCV:\n    1. LinearRegression\n    2. RandomForestRegressor\n    3. KNeighborsRegressor \n    4. DecisionTreeRegressor\n    5. BaggingRegressor\n    6. XGBRegressor","fefbc2ec":"## EDA","6689298f":"As can be seen, there is strong correlation between the feature set.\nLet's try using RFE or PCA, to reduce the feature set","fcf62239":"# Feature selection: apply PCA on the data","5cf6d212":"# Predicting Turbine Decay"}}