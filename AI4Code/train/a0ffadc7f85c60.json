{"cell_type":{"db0cbcb1":"code","533f47c1":"code","9d086a55":"code","c95958af":"code","fa5f4bd9":"code","90b1de61":"code","5a15e744":"code","b1140077":"code","2762feab":"code","678c90c9":"code","8b992254":"code","ec5226de":"code","cbce696f":"code","62f06a63":"code","d5fbb3d8":"code","48229d6f":"code","8e924585":"code","e58e4493":"code","96401e86":"code","6a09635a":"markdown","631bd5a8":"markdown","899b8c7e":"markdown","10a6d1c8":"markdown","d6acbcf7":"markdown","d388d287":"markdown"},"source":{"db0cbcb1":"!pip install scikit-learn -U","533f47c1":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA, FactorAnalysis\nfrom sklearn.mixture import GaussianMixture, BayesianGaussianMixture\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import TweedieRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9d086a55":"# Loading data \nX_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","c95958af":"# Preparing data as a tabular matrix\ny_train = X_train.target\nX_train = X_train.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","fa5f4bd9":"# Pointing out categorical features\ncategoricals = [item for item in X_train.columns if 'cat' in item]","90b1de61":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(X_train.append(X_test)[categoricals])\nX_train[dummies.columns] = dummies.iloc[:len(X_train), :]\nX_test[dummies.columns] = dummies.iloc[len(X_train): , :]\ndel(dummies)","5a15e744":"# Dealing with categorical data using OrdinalEncoder (only when there are 3 or more levels)\nordinal_encoder = OrdinalEncoder()\nX_train[categoricals[3:]] = ordinal_encoder.fit_transform(X_train[categoricals[3:]]).astype(int)\nX_test[categoricals[3:]] = ordinal_encoder.transform(X_test[categoricals[3:]]).astype(int)\nX_train = X_train.drop(categoricals[:3], axis=\"columns\")\nX_test = X_test.drop(categoricals[:3], axis=\"columns\")","b1140077":"# Feature selection (https:\/\/www.kaggle.com\/lucamassaron\/tutorial-feature-selection-with-boruta-shap)\nimportant_features = ['cat1_A', 'cat1_B', 'cat5', 'cat8', 'cat8_C', 'cat8_E', 'cont0', \n                      'cont1', 'cont10', 'cont11', 'cont12', 'cont13', 'cont2', 'cont3', \n                      'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9']\n\nX_train = X_train[important_features]\nX_test = X_test[important_features]","2762feab":"# Stratifying the data\nkm = KMeans(n_clusters=32, random_state=0)\npca = PCA(n_components=16, random_state=0)\n\npca.fit(X_train)\nkm.fit(pca.transform(X_train))\n\nprint(np.unique(km.labels_, return_counts=True))\n\ny_stratified = km.labels_","678c90c9":"numerics = [f\"cont{i}\" for i in range(14)]","8b992254":"plt.figure(figsize=(15, 10))\nfor k, num in enumerate(numerics):\n    plt.subplot(5, 3, k+1)\n    X_train[num].plot.density()\nplt.show()","ec5226de":"fa = FactorAnalysis(n_components=len(numerics), rotation='varimax', random_state=0)\nfa.fit(X_train[numerics])\n\nfa_feats = [f'fa_{i}'for i in range(len(numerics))][:2]\n\nX_train[fa_feats] = fa.transform(X_train[numerics])[:,:2]\nX_test[fa_feats] = fa.transform(X_test[numerics])[:,:2]","cbce696f":"pca = PCA(n_components=len(numerics), random_state=0)\npca.fit(X_train[numerics])\n\npca_feats = [f'pca_{i}'for i in range(len(numerics))]\n\nX_train[pca_feats] = pca.transform(X_train[numerics])\nX_test[pca_feats] = pca.transform(X_test[numerics])\n\nfa_feats += pca_feats","62f06a63":"gmms = dict()\n\nfor feature in numerics + fa_feats:\n    x = X_train[[feature]].append(X_test[[feature]])[feature].values.reshape(-1, 1)\n    \n    best_score = np.mean(cross_val_score(LinearRegression(), \n                                         X_train[numerics], y_train,\n                                         cv=3,\n                                         scoring='neg_root_mean_squared_error'))\n    \n    print(f\"Original {feature} RMSE score: {best_score:0.5f}\")\n    \n    for k in range(1, 33):\n        gmm_ = GaussianMixture(n_components=k,\n                       max_iter=300,\n                       random_state=0).fit(x)\n\n        clus = pd.get_dummies(gmm_.predict(x)).values * x\n        clus_train = clus[:len(X_train), :]\n        clus_test = clus[len(X_train):, :]\n        score = np.mean(cross_val_score(LinearRegression(), \n                                        np.hstack([X_train[numerics].values, clus_train]), y_train,\n                                        cv=3,\n                                        scoring='neg_root_mean_squared_error'))\n        if score > best_score:\n            best_score = score\n            gmms[feature] = [clus_train, clus_test, k]\n            print(f\"\\t{feature}: {k:2.0f} components -> improved RMSE: {score:0.5f}\")","d5fbb3d8":"# Here we apply the Gaussian Mixture clusters that fitted better our numeric distributions\nfor feature in numerics + fa_feats:\n    if feature in gmms:\n        clus_train, clus_test, k = gmms[feature]\n        print(f\"{feature} fits {k} components with GaussianMixture\")\n        clus_feats = [f'{feature}_gmm_dev_{i}'for i in range(clus_train.shape[1])]\n        X_train[clus_feats] = clus_train\n        X_test[clus_feats] = clus_test","48229d6f":"# Cross-validation prediction\nfolds = 10\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=0)\n\nfold_idxs = list(skf.split(X_train, y_stratified))\n\nscore = list()\n\nfor k, (train_idx, val_idx) in enumerate(fold_idxs):\n    \n    y = y_train[train_idx]\n    yv = y_train[val_idx]\n    X = X_train[important_features].iloc[train_idx, :]\n    Xv = X_train[important_features].iloc[val_idx, :]\n    \n    glm = LinearRegression()\n    glm.fit(X, y)\n    \n    val_preds = glm.predict(Xv)\n    val_rmse = mean_squared_error(y_true=yv, y_pred=val_preds, squared=False)\n    print(f\"Fold {k} RMSE: {val_rmse:0.5f}\")\n    score.append(val_rmse)\n    \nprint(f\"CV RMSE {np.mean(score):0.5f} ({np.std(score):0.5f})\")","8e924585":"# Cross-validation prediction\nfolds = 10\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=0)\n\nfold_idxs = list(skf.split(X_train, y_stratified))\n\npredictions = np.zeros(len(X_test))\nscore = list()\n\nfor k, (train_idx, val_idx) in enumerate(fold_idxs):\n    \n    y = y_train[train_idx]\n    yv = y_train[val_idx]\n    X = X_train.iloc[train_idx, :]\n    Xv = X_train.iloc[val_idx, :]\n    Xt = X_test\n    \n    glm = LinearRegression()\n    glm.fit(X, y)\n    \n    val_preds = glm.predict(Xv)\n    val_rmse = mean_squared_error(y_true=yv, y_pred=val_preds, squared=False)\n    print(f\"Fold {k} RMSE: {val_rmse:0.5f}\")\n    predictions += glm.predict(Xt).ravel()\n    score.append(val_rmse)\n    \npredictions \/= folds\nprint(f\"CV RMSE {np.mean(score):0.5f} ({np.std(score):0.5f})\")","e58e4493":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': predictions})\n\nsubmission.to_csv(\"submission.csv\", index = False)","96401e86":"submission","6a09635a":"At this point, first we test out original data using a linear regression.","631bd5a8":"If dealing with categorical variables is easy, thanks to Boruta-SHAP, it is more difficult to crack the numeric ones. From Boruta-SHAP we know that they are all important but I wonder if it is just ok to leave them as they are.\n\nSince some notebooks and discussion from previous competitions on artificial tabular data mentioned that continuos variables are mixtures of distributions, in this kernel I try to  \n\nMake your stacking richer and more varied!\n\nHappy Kaggling!\n","899b8c7e":"The first step is to combine the variables using a rotated factor analysis and a pca. We can then process by a Gaussian mixture both the original continuous features and the components from the factor analysis","10a6d1c8":"Then we test the same regression using the original data, the pca components and all the gaussian mixtures that we found.","d6acbcf7":"The idea is simple: we test for each continuous feature a Gaussian Mixture clustering by splitting the original variable values into as many variables as the tested clusters. We verify by cross-validation that the solution can bring some improvement to a baseline model and we retain it if it works.","d388d287":"The idea is to start from density plots: for each continuos variable they show peaks that may hint at a mix of distributions."}}