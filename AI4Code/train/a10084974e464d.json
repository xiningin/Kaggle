{"cell_type":{"42634661":"code","9918400f":"code","5e91d164":"code","4a67181d":"code","6c8310c0":"code","0dfbf150":"code","0363525e":"code","a19cdd4b":"code","bd2d1495":"code","e218162f":"code","7a047088":"code","ed2c8253":"code","28ed1a33":"code","f2007848":"code","aaf3c68d":"code","6b51ade0":"code","0aed9c84":"code","e9cd6a9c":"code","70978d70":"code","8ef65773":"code","ea02f4ce":"code","10976059":"code","5548e755":"code","54cf420c":"code","80030fa9":"code","bec739fc":"code","3fff4730":"code","b7152808":"code","5f161940":"code","2ee4b9a4":"code","f9f99077":"code","9a253966":"code","7e3dc643":"code","34d87d8e":"code","bbe4a378":"code","733d721a":"code","0ccf149e":"code","85731cbe":"code","305ca739":"code","7303c6a8":"code","412498c2":"code","f46c8690":"code","cbbbd9f4":"code","7649fdae":"code","74772068":"code","5febca2f":"markdown","d57e7d29":"markdown","7ac72705":"markdown","23a12813":"markdown","a9dac771":"markdown","2d0e244b":"markdown","4a0fcd06":"markdown","c6f69584":"markdown","75974601":"markdown","77897468":"markdown","bba12fc8":"markdown","517ab0e2":"markdown","158f13e4":"markdown","14dbe6c1":"markdown","8b8ff040":"markdown","b8bfffc5":"markdown","134cfd9f":"markdown","5ca2e8b3":"markdown","05c8b609":"markdown","62b4039d":"markdown","33309e26":"markdown","c7a976d2":"markdown","28baf5dd":"markdown","386374f7":"markdown","254ac70a":"markdown","16138f76":"markdown","61649fad":"markdown","f1fd4dc5":"markdown","b8959ba6":"markdown","62bc79a4":"markdown"},"source":{"42634661":"# data exploration \/ preprocessing\nimport pandas as pd\nfrom pandas.api.types import CategoricalDtype\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt, gridspec\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\n# model fitting \/ evaluation\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import (accuracy_score, precision_score,\n                             recall_score, f1_score, \n                             classification_report, plot_confusion_matrix)","9918400f":"# 10.5M rows of raw temperature sensor data\ntemperature_data_orig = pd.read_csv(\"..\/input\/temperature-iot-on-gcp\/temperature_data\/temperature.csv\", parse_dates=[\"timestamp_utc\"], converters = {\"device_id\": str})\n\n# 29 periods when windows open\nwindow_data_orig = pd.read_csv(\"..\/input\/temperature-iot-on-gcp\/temperature_data\/window_opened_closed.csv\")","5e91d164":"assert temperature_data_orig.isna().sum().sum() == 0, \"Some temperature data missing.\"\nassert window_data_orig.isna().sum().sum() == 0, \"Some window data missing.\"\n\nprint(f\"First few lines of temperature data: \\n{temperature_data_orig.head(3)}\\n\")\nprint(f\"First few lines of window data: \\n{window_data_orig.head(3)}\\n\")\n\ntemp_no_dups = (temperature_data_orig.shape == temperature_data_orig.drop_duplicates().shape)\nif not temp_no_dups:\n    print(\"At least one duplicate row in temperature data.\")\n\nwindow_no_dups = (window_data_orig.shape == window_data_orig.drop_duplicates().shape)\nif not window_no_dups:\n    print(\"At least one duplicate row in window data.\")","4a67181d":"window_data = (\n window_data_orig\n .assign(\n     # PST is roughly UTC -8\n     start_dt = lambda x: pd.to_datetime(x.DayPST + \" \" + x.StartTimePST),\n     end_dt = lambda x: pd.to_datetime(x.DayPST + \" \" + x.EndTimePST),\n     start_dt_utc = lambda x: (x.start_dt.dt.tz_localize(\"US\/Pacific\")\n                               .dt.tz_convert(\"UTC\").dt.tz_convert(None)),\n     end_dt_utc = lambda x: (x.end_dt.dt.tz_localize(\"US\/Pacific\")\n                             .dt.tz_convert(\"UTC\").dt.tz_convert(None)),\n     mins_opened = lambda x: (x.end_dt_utc - x.start_dt_utc).dt.total_seconds() \/ 60\n )\n .rename(columns={\"ObjectCode\": \"window_code\", \"ObjectName\": \"window_name\"})\n .drop([\"DayPST\", \"StartTimePST\", \"EndTimePST\", \"start_dt\", \"end_dt\"], axis=1)\n)\n\n# for combining with sensor data later\nwindow_data[\"ts_opened\"] = [\n    pd.date_range(\n        row.start_dt_utc.round(\"10s\"), row.end_dt_utc.round(\"10s\"), freq=\"10s\"\n    ) for row in window_data.itertuples()\n]","6c8310c0":"window_data.head(3)","0dfbf150":"resamp_temp_data = (temperature_data_orig\n                    .assign(dev_id = lambda x: x[\"device_id\"].str[0:3] + \"*\")\n                    .drop([\"timestamp_epoch\", \"temp_f\"], axis=1)\n                    .drop_duplicates()\n                    .set_index(\"timestamp_utc\")\n                    .groupby(\"dev_id\")\n                    .resample(\"10s\").mean()\n                    .reset_index())\n\nresamp_temp_wide_data = (resamp_temp_data\n                         .pivot(index=\"timestamp_utc\", columns=\"dev_id\", \n                                values=\"temp_c\"))\n\ncombined_data = (\n    window_data\n    .explode(\"ts_opened\")\n    [[\"ts_opened\", \"window_code\"]]\n    .merge(resamp_temp_wide_data.reset_index(), how=\"outer\",\n           left_on=\"ts_opened\", right_on=\"timestamp_utc\")\n    .drop(\"ts_opened\", axis=1)\n    .fillna({\"window_code\": 0})\n    # type changes when merge\n    .assign(window_code = lambda x: x[\"window_code\"].astype(int))\n)","0363525e":"print(f\"\"\"Long form of resampled data ({resamp_temp_data.shape[0]} rows):\n{resamp_temp_data.head(3)}\\n\"\"\")\n\nprint(f\"\"\"Wide form of resampled data ({resamp_temp_wide_data.shape[0]} rows):\n{resamp_temp_wide_data.head(3)}\\n\"\"\")\n\nprint(\"Combined data:\")\ncombined_data.head(3)","a19cdd4b":"DEV_IDS = [\"258*\", \"270*\", \"275*\"]  # will be used often","bd2d1495":"figure, axes = plt.subplots(1,2, figsize=(10, 3))\nsns.countplot(x=\"window_code\", data=window_data, ax=axes[0])  # only 29 events\nsns.boxplot(x=\"window_code\", y=\"mins_opened\", data=window_data, ax=axes[1]);  # outlier w1","e218162f":"print(window_data.query(\"mins_opened < 800\")  # exclude outlier\n      .groupby(\"window_code\")\n      .agg({\"mins_opened\": [\"mean\", \"median\", \"min\", \"max\"]}))\n\nsns.displot(\n    window_data.query(\"mins_opened < 800\"),  # exclude outlier\n    x=\"mins_opened\", col=\"window_code\",\n    hue=pd.cut(window_data[\"mins_opened\"], [0, 2, 5, 15, 30, 60, 90, 120]),\n    binwidth=15, multiple=\"stack\", height=2, aspect=1.5\n);","7a047088":"print(f\"\"\"Share of missing data: \n{resamp_temp_wide_data.isna().sum() \/ resamp_temp_wide_data.shape[0]}\n\"\"\")\n\nsns.heatmap(resamp_temp_wide_data.isnull(), cbar=False);","ed2c8253":"for event, rough_start in zip([\"First\", \"Second\"], [\"2020-12-24\", \"2021-01-13\"]):\n    start_na = (resamp_temp_wide_data[resamp_temp_wide_data.isna().sum(axis=1) == 3]\n                .query(f\"timestamp_utc > '{rough_start}'\")\n                .head(1).index[0].strftime(\"%Y-%m-%d %H:%M:%S\"))\n    \n    print(f\"{event} streak of NaNs in all sensors starts: {start_na}\")","28ed1a33":"# prep data\nresamp_temp_period_data = pd.cut(\n    resamp_temp_data[\"timestamp_utc\"], \n    bins=10,\n    labels=[\"bin\" + str(item) for item in range(1, 11)])\nresamp_temp_wide_subset = resamp_temp_wide_data.sample(50_000)\n\n# plots\nfig = plt.figure(figsize=(12, 6))\ngs = gridspec.GridSpec(2, 3, figure=fig)\nax1, ax2, ax3 = (plt.subplot(spec) for spec in [gs[0, 0], gs[0, 1:3], gs[1, :]])\n\nsns.histplot(resamp_temp_wide_subset, bins=30, ax=ax1)\n\nsns.boxplot(\n    x=resamp_temp_period_data, y=\"temp_c\", \n    hue=\"dev_id\", hue_order=DEV_IDS,\n    data=resamp_temp_data, ax=ax2\n).set(xlabel=\"\", ylabel=\"Temperature (\u00b0C)\", ylim=(0, 25))\n\n# time series with annotation for long streak of NaNs in all three sensors\nresamp_temp_wide_subset.plot(ax=ax3)\nplt.axvline(x=pd.to_datetime(\"2020-12-25 19:53:00\"),\n            color=\"red\", linestyle=\"--\", linewidth=0.7)\nplt.axvline(x=pd.to_datetime(\"2021-01-14 08:24:00\"),\n            color=\"red\", linestyle=\"--\", linewidth=0.7);","f2007848":"combined_data[\"window_code\"].value_counts(normalize=True)  # really imbalanced data","aaf3c68d":"(combined_data\n .groupby(\"window_code\")\n [DEV_IDS]\n .agg({dev_id: [\"mean\", \"std\", \"min\", \"max\"] for dev_id in DEV_IDS}))","6b51ade0":"sns_col_pal = sns.color_palette().as_hex()  # to make colours match other plots\n\ndef hide_current_axis(*args, **kwds):\n    plt.gca().set_visible(False)\n\ng = (pd.concat([\n    combined_data[[\"window_code\"] + DEV_IDS].query(\"window_code == 0\").sample(5000),\n    combined_data[[\"window_code\"] + DEV_IDS].query(\"window_code > 0\")\n])\n .pipe(sns.PairGrid, hue=\"window_code\", height=1.7, aspect=1.1,\n       palette={0: sns_col_pal[3], 2: sns_col_pal[0], \n                1: sns_col_pal[1], 3: sns_col_pal[2]})\n .map_diag(sns.histplot, bins=30, \n           element=\"step\", fill=False, cumulative=True,\n           # easier to compare different window_codes than count:\n           stat=\"probability\", common_norm=False)\n .map_upper(hide_current_axis)\n .map_lower(sns.scatterplot, alpha=0.1)\n)\n\ng.fig.legend(\n    handles=g._legend_data.values(), \n    labels=g._legend_data.keys(), \n    loc=\"upper right\", ncol=4\n).set(title=\"window_code\");","0aed9c84":"## prep for plot\ndef add_dt_highlight(sns_plot, y_lims, start_dt, end_dt, ax_pos: int, color=None):\n    color = sns_col_pal[ax_pos] if color is None else color\n    # https:\/\/stackoverflow.com\/a\/52317485 for fill_between with FacetGrid\n    sns_plot.axes[ax_pos].fill_betweenx(y=y_lims, x1=start_dt, x2=end_dt,\n                                        color=color, alpha=0.2)\n        \ndef add_open_periods(sns_plot, y_lims, window_df: pd.DataFrame, \n                     window_code: int, ax_pos: int, color=None):\n    filtered_df = window_df.query(\"window_code == @window_code\")\n    \n    for start, end in zip(filtered_df[\"start_dt_utc\"], filtered_df[\"end_dt_utc\"]):\n        add_dt_highlight(g, y_lims, start, end, ax_pos, color)\n        \nresamp_temp_stats = (resamp_temp_data.groupby(\"dev_id\")\n                     .agg({\"temp_c\": [\"mean\", \"std\"], \n                           \"timestamp_utc\": [\"min\", \"max\"]}))\n\ndev_wind_dict = {\"258*\": 2, \"270*\": 1, \"275*\": 3}\n\n\n## plot\ng = (sns.FacetGrid(resamp_temp_data, col=\"dev_id\", hue=\"dev_id\",\n                   col_wrap=1, height=1.5, aspect=7, dropna=False)\n     .map_dataframe(plt.plot,\"timestamp_utc\",\"temp_c\")\n     .set(ylabel=\"Temperature (\u00b0C)\",\n          xticks=pd.date_range(start=\"2020-12-26\", periods=6, freq=\"W-SAT\")))\n\n# open window\ntemperature_y_lims = plt.gca().axes.get_ylim()\nfor window_num, ax_row in zip([2, 1, 3], [0, 1, 2]):\n    add_open_periods(g, temperature_y_lims, window_data, window_num, ax_pos=ax_row)\n\n# possible missing open window data..?\ng.axes[1].fill_betweenx(\n    y=temperature_y_lims, \n    x1=pd.to_datetime(\"2021-01-06 01:00:00\"), x2=pd.to_datetime(\"2021-01-06 07:00:00\"),\n    color=\"gray\", alpha=0.2\n)\n\n# add band of average temperature\ntemperature_x_lims = plt.gca().axes.get_xlim()\nfor row in range(0, 3):\n    g.axes[row].fill_between(\n        temperature_x_lims,\n        resamp_temp_stats[(\"temp_c\", \"mean\")][row] - \\\n            resamp_temp_stats[(\"temp_c\", \"std\")][row],  \n        resamp_temp_stats[(\"temp_c\", \"mean\")][row] + \\\n            resamp_temp_stats[(\"temp_c\", \"std\")][row], \n        color=sns_col_pal[row], alpha = 0.1)\n\n# update titles\nfor dev_id, ax in g.axes_dict.items():\n    ax.set_title(f\"{dev_id} (likely closest to window {dev_wind_dict[dev_id]})\")","e9cd6a9c":"check_data = window_data.query(\"window_code == 1\").copy()\ncheck_data[\"expanded_ts_opened\"] = [\n    pd.date_range(\n        row.start_dt_utc.round(\"10s\") - pd.Timedelta(5, \"minute\"), \n        row.end_dt_utc.round(\"10s\") + pd.Timedelta(5, \"minute\"), \n        freq=\"10s\"\n    ) for row in check_data.itertuples()\n]\ncheck_data = check_data.reset_index().assign(event_num = lambda x: x[\"index\"] + 1)\n\ng = (check_data\n     .explode(\"expanded_ts_opened\")\n     [[\"expanded_ts_opened\", \"window_code\", \"event_num\"]]\n     .merge(resamp_temp_wide_data.reset_index(), how=\"outer\",\n            left_on=\"expanded_ts_opened\", right_on=\"timestamp_utc\")\n     .pipe((sns.FacetGrid, \"data\"),\n           col=\"event_num\", col_wrap=4, sharex=False,height=3, aspect=1)\n     .map_dataframe(plt.plot, \"timestamp_utc\", \"270*\"))\n\n# add window opening and closing annotation\nfor ax, start, end in zip(g.axes.flat, check_data[\"start_dt_utc\"], check_data[\"end_dt_utc\"]):\n    ax.axvline(x=start, color=\"red\", linestyle=\"--\", linewidth=0.7)\n    ax.axvline(x=end, color=\"red\", linestyle=\"--\", linewidth=0.7)\n\n# update titles\nfor event_num, ax in g.axes_dict.items():\n    mins_open = round(check_data.query('event_num == @event_num')\n                      ['mins_opened'].values[0])\n    ax.set_title(f\"Event {int(event_num)} ({mins_open} minutes)\")\n    \ng.set_xticklabels(rotation=90).fig.tight_layout()","70978d70":"(combined_data\n .query(\"timestamp_utc > '2021-01-06 3:40:00' & timestamp_utc < '2021-01-06 6:20'\")\n [[\"timestamp_utc\", \"270*\"]]\n .plot(\"timestamp_utc\", \"270*\"))\n\n# potential window opening and closing events\nplt.axvline(x=pd.to_datetime(\"2021-01-06 3:57:23\"), color=\"red\", linestyle=\"--\", linewidth=0.7)\nplt.axvline(x=pd.to_datetime(\"2021-01-06 5:46:00\"), color=\"red\", linestyle=\"--\", linewidth=0.7);","8ef65773":"print(\"Total index length:\", len(combined_data.index))\nprint(\"\\nsplitting date time for roughly 80%\/20% split:\")\n(combined_data\n .sort_values(by=\"timestamp_utc\")\n .reset_index(drop=True)\n .iloc[round(len(combined_data.index) * 0.8), 1]\n)","ea02f4ce":"SPLIT_DAY = pd.to_datetime(\"2021-01-30 00:00:00\")  # round up\n\ndef split_data(combined_data_df, train_cols):\n    train = (combined_data_df.query(\"timestamp_utc < @SPLIT_DAY\")\n             .sort_values(\"timestamp_utc\").set_index(\"timestamp_utc\"))\n    test = (combined_data_df.query(\"timestamp_utc >= @SPLIT_DAY\")\n            .sort_values(\"timestamp_utc\").set_index(\"timestamp_utc\"))\n    \n    X_train, y_train = train[train_cols], train[\"window_code\"]\n    X_test, y_test = test[train_cols], test[\"window_code\"]\n    \n    # number of non-0.0 labels important to CV..\n    print(pd.concat(dict(\n        train_count=y_train.value_counts(), \n        train_prop=y_train.value_counts(normalize=True).sort_index(),\n        test_count=y_test.value_counts(),\n        test_prop=y_test.value_counts(normalize=True).sort_index()\n    ), axis = 1).sort_index())\n    \n    imp_freq = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n    X_train = pd.DataFrame(imp_freq.fit_transform(X_train), columns = X_train.columns)\n    X_test = pd.DataFrame(imp_freq.transform(X_test), columns = X_test.columns)\n        \n    return X_train, y_train, X_test, y_test","10976059":"# mismatch in class distrib, especially for w1 (10x) and w2 (5x)\nX_train_orig, y_train_orig, X_test_orig, y_test_orig = split_data(combined_data, DEV_IDS)\n\nassert ~pd.concat([X_train_orig, X_test_orig]).isnull().any().any(), \\\n    \"At least one missing value.\"","5548e755":"update_info = pd.DataFrame({\"start_dt_utc\": pd.to_datetime(\"2021-01-06 3:57:23\"),\n              \"end_dt_utc\": pd.to_datetime(\"2021-01-06 5:46:00\")},\n             index=window_data[window_data.mins_opened > 800].index.astype(int))\n\nedited_window_data = window_data.query(\"end_dt_utc < '2021-02-07 21:09:47'\").copy()\nedited_window_data.update(update_info)\nedited_window_data = (edited_window_data\n                      .assign(mins_opened = lambda x: ((x.end_dt_utc - x.start_dt_utc)\n                                                       .dt.total_seconds() \/ 60)\n                      ))\nedited_window_data[\"ts_opened\"] = [\n    pd.date_range(\n        row.start_dt_utc.round(\"10s\"), row.end_dt_utc.round(\"10s\"), freq=\"10s\"\n    ) for row in edited_window_data.itertuples()\n]\n\nedited_combined_data = (edited_window_data\n                        .explode(\"ts_opened\")\n                        [[\"ts_opened\", \"window_code\"]]\n                        .merge(resamp_temp_wide_data.reset_index(), how=\"outer\",\n                               left_on=\"ts_opened\", right_on=\"timestamp_utc\")\n                        .fillna({\"window_code\": 0})\n                        .assign(window_code = lambda x: x[\"window_code\"].astype(int))\n                        .drop(\"ts_opened\", axis=1))  # all windows closed unless annotated\n\nprint(\"Updated entry in edited data:\")\nedited_window_data.query(\"start_dt_utc == '2021-01-06 03:57:23'\")","54cf420c":"g = (sns.FacetGrid(resamp_temp_data, col=\"dev_id\", hue=\"dev_id\",\n                   col_wrap=1, height=1.5, aspect=7, dropna=False)\n     .map_dataframe(plt.plot,\"timestamp_utc\",\"temp_c\")\n     .set(ylabel=\"Temperature (\u00b0C)\"))\n\n# open windows\ntemperature_y_lims = plt.gca().axes.get_ylim()\nfor window_num, ax_row in zip([2, 1, 3], [0, 1, 2]):\n    add_open_periods(g, temperature_y_lims, edited_window_data, window_num, ax_pos=ax_row)","80030fa9":"edited_combined_data.head(3)","bec739fc":"print(\"Mean temperatures:\\n\", edited_combined_data.groupby(\"window_code\")[DEV_IDS].mean(), \"\\n\")\n\noverlap_dt = len(combined_data[[\"timestamp_utc\", \"window_code\"]]\n                 .groupby(\"timestamp_utc\").count().query(\"window_code > 1\").index)\nprint(f\"Original data: {overlap_dt} time points with overlapping window opening annotations.\")\n\noverlap_dt2 = len(edited_combined_data[[\"timestamp_utc\", \"window_code\"]]\n                  .groupby(\"timestamp_utc\").count().query(\"window_code > 1\").index)\nprint(f\"Edited data: {overlap_dt2} time points with overlapping window opening annotations.\")","3fff4730":"X_train_edited, y_train_edited, X_test_edited, y_test_edited = \\\n    split_data(edited_combined_data, DEV_IDS)\n\nassert all(y_test_orig.reset_index(drop=True) == y_test_edited.reset_index(drop=True)), \\\n    \"something went wrong with editing\"\nassert ~pd.concat([X_train_edited, X_test_edited]).isnull().any().any(), \\\n    \"still at least one missing value\"","b7152808":"# 30m rolling windows, result set to right edge\nroll_av_temp_wide_data = (resamp_temp_wide_data\n                          .rolling(180, min_periods=1)\n                          .median()  \n                          .reset_index()\n                          .rename(columns={dev_id: dev_id[0:3] + \"_ra\" \n                                           for dev_id in DEV_IDS}))\n\nroll_std_temp_wide_data = (resamp_temp_wide_data\n                           .rolling(180, min_periods=1)\n                           .std()\n                           .reset_index()\n                           .rename(columns={dev_id: dev_id[0:3] + \"_rstd\"\n                                            for dev_id in DEV_IDS}))\n\nroll_low_lim_wide_data = (roll_av_temp_wide_data.merge(roll_std_temp_wide_data)\n .assign(\n     lower_258 = lambda x: x[\"258_ra\"] - x[\"258_rstd\"],\n     lower_270 = lambda x: x[\"270_ra\"] - x[\"270_rstd\"],\n     lower_275 = lambda x: x[\"275_ra\"] - x[\"275_rstd\"]\n ).drop([\"258_ra\", \"270_ra\", \"275_ra\", \"258_rstd\", \"270_rstd\", \"275_rstd\"], axis=1))\n\nenhanced_combined_data = (\n    edited_combined_data\n    .merge(roll_low_lim_wide_data, how=\"left\")\n    .assign(\n        time_since_start = lambda x: (x.timestamp_utc - pd.to_datetime(\"2020-12-22\")),\n        days_since_start = lambda x: x.time_since_start.dt.days,\n        lower_lim_258 = lambda x: x[\"258*\"] < x[\"lower_258\"],\n        lower_lim_270 = lambda x: x[\"270*\"] < x[\"lower_270\"],\n        lower_lim_275 = lambda x: x[\"275*\"] < x[\"lower_275\"],\n        diff_5m_258 = lambda x: x[\"258*\"] - x[\"258*\"].shift(periods=30),\n        diff_5m_270 = lambda x: x[\"270*\"] - x[\"270*\"].shift(periods=30),\n        diff_5m_275 = lambda x: x[\"275*\"] - x[\"275*\"].shift(periods=30)\n    )\n    .drop([\"time_since_start\"], axis=1)\n)","5f161940":"enhanced_combined_data.head(3)","2ee4b9a4":"enhanced_train_cols = DEV_IDS + [\"days_since_start\"] + \\\n    [prefix + dev_num \n     for prefix in [\"lower_lim_\", \"diff_5m_\"]\n     for dev_num in [\"258\", \"270\", \"275\"]]\n\nX_train_enhanced, y_train_enhanced, X_test_enhanced, y_test_enhanced = \\\n    split_data(enhanced_combined_data, enhanced_train_cols)\n\nassert all(y_test_orig.reset_index(drop=True) == y_test_enhanced.reset_index(drop=True)), \\\n    \"something went wrong with editing\"\nassert ~pd.concat([X_train_enhanced, X_test_enhanced]).isnull().any().any(), \\\n    \"still at least one missing value\"","f9f99077":"fig = plt.figure(figsize=(6, 6))\ngs = gridspec.GridSpec(3, 2, figure=fig)\nax1, ax2, ax3 = (plt.subplot(spec) for spec in [gs[0, 0], gs[0, 1:], gs[1:, :]])\n\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\n\n(sns.heatmap(X_train_orig.corr(method=\"pearson\"), ax=ax1, annot=True, fmt=\".1f\")\n .set(title=\"Original data set\"))\n(sns.heatmap(X_train_edited.corr(method=\"pearson\"), ax=ax2, annot=True, fmt=\".1f\")\n .set(title=\"Edited data set\"))\nsns.heatmap(enhanced_combined_data[enhanced_train_cols].corr(), ax=ax3, annot=True,\n            fmt=\".1f\").set(title=\"Enhanced data set\");","9a253966":"EVAL_METRICS = [\"precision\", \"recall\", \"f1\"]","7e3dc643":"all_0_y_pred = np.repeat(0.0, y_test_orig.shape)\nall_1_y_pred = np.repeat(1.0, y_test_orig.shape)\n\nmetrics_eg = []\nfor y_pred in [all_0_y_pred, all_1_y_pred]:\n    alt_scores = {metric: \n                  globals()[metric + \"_score\"](y_test_orig, y_pred, \n                                               average=\"macro\", zero_division=0)\n                  for metric in EVAL_METRICS}\n    metrics_eg.append({\"accuracy\": accuracy_score(y_test_orig, y_pred), **alt_scores})\n    \npd.DataFrame(metrics_eg).reset_index().rename(columns={\"index\": \"predicted y\"})","34d87d8e":"# macro averaging (unweighted mean - all classes equal regardless of the support of each class)\n# micro averaging (sum the number of true positives \/ false negatives for each class)\n# weighted averaging (weight the score by the support of the class)\n\nprint(\"averaging method\")\nfor averaging_method in [\"weighted\", \"micro\", \"macro\"]:\n    print(f\"{averaging_method} F1 score:\",\n          round(f1_score(y_test_orig, all_0_y_pred, average=averaging_method), 2))","bbe4a378":"def check_class_prop(combined_data_version: str, cv_splitter):\n    X_train_ps, y_train_ps, X_test, y_test = \\\n        [globals()[f\"{prefix}_{combined_data_version}\"]\n         for prefix in [\"X_train\", \"y_train\", \"X_test\", \"y_test\"]]\n    \n    prop_df_list = []\n    for train_ind, val_ind in cv_splitter.split(X_train_ps, y_train_ps):\n        y_train, y_val = y_train_ps.iloc[train_ind], y_train_ps.iloc[val_ind]\n        fold_df = pd.concat(dict(\n            train_count=y_train.value_counts(), \n            train_prop=y_train.value_counts(normalize=True).sort_index(),\n            val_count=y_val.value_counts(),\n            val_prop=y_val.value_counts(normalize=True).sort_index()\n        ), axis = 1).sort_index()\n        \n        if fold_df.isnull().sum().sum():\n            return \"missing data point from at least one class in at least one fold\"\n        prop_df_list.append(fold_df)\n        \n    final_df = (pd.concat([prop_df_list[0].assign(fold = 1), \n                           prop_df_list[1].assign(fold = 2)])\n                .reset_index()\n                .rename(columns={\"index\": \"class_label\"})\n                .assign(class_label = lambda x: x.class_label.map(int))\n                .style.format(\"{:.3f}\", subset=[\"train_prop\", \"val_prop\"])\n               )\n    return final_df","733d721a":"# check edited data set since there are fewer 1.0 window_code data points\ncheck_class_prop(\"edited\", TimeSeriesSplit())  # n_splits=5 by default","0ccf149e":"tss = TimeSeriesSplit(n_splits=2, test_size=78_000)\n\ncheck_class_prop(\"edited\", tss)","85731cbe":"cat_type = CategoricalDtype(categories=[\"orig\", \"edited\", \"enhanced\"], ordered=True)\n\ndef gen_baseline_eval(classifier_name: str, combined_data_version: str):\n    # build and evaluate model\n    X_train_unsplit, y_train_unsplit, X_test, y_test = \\\n        [globals()[f\"{prefix}_{combined_data_version}\"] for prefix in\n         [\"X_train\", \"y_train\", \"X_test\", \"y_test\"]]\n    pipe = Pipeline(steps=[(\"scaler\", StandardScaler()),\n                           (\"classifier\", globals()[classifier_name])])\n    \n    cv_metrics = dict(precision=[], recall=[], f1=[])\n    for train_index, val_index in tss.split(X_train_unsplit, y_train_unsplit):\n        X_train, X_val = X_train_unsplit.iloc[train_index], X_train_unsplit.iloc[val_index]\n        y_train, y_val = y_train_unsplit.iloc[train_index], y_train_unsplit.iloc[val_index]\n        \n        pipe.fit(X_train, y_train)\n        y_pred = pipe.predict(X_val)\n        \n        for metric_type in EVAL_METRICS:\n            metric_fun = globals()[f\"{metric_type}_score\"]\n            (cv_metrics\n             .get(metric_type)\n             .append(np.array(metric_fun(y_val, y_pred, average=None, zero_division=0))))\n            \n    # macro average for each class and metric combination (more detailed for troubleshooting)\n    metrics_array = [[np.mean(fold) for fold in zip(*cv_metrics.get(metric))]\n                     for metric in EVAL_METRICS]\n    metrics_data = {metric_name: array \n                    for metric_name, array in zip(EVAL_METRICS, metrics_array)}\n    metrics_df = pd.DataFrame(metrics_data)\n    \n    \n    # also macro average for overall summary (easier to compare)\n    summary_metrics = [np.mean(label_metrics) for label_metrics in metrics_array]\n    summary_df = pd.DataFrame([summary_metrics], columns=EVAL_METRICS).rename(index={0: \"avg\"})\n    \n    \n    # make final_dfs easy to combine with each other\n    final_df = (pd.concat([metrics_df, summary_df])\n                .reset_index()\n                .rename(columns={\"index\": \"label\"})\n                .assign(model=classifier_name, data=combined_data_version)\n                .assign(data = lambda x: x[\"data\"].astype(cat_type))\n               )\n    \n    return final_df\n\n\ndef fmt_baseline_eval(pd_df, labs_to_highlight=[\"avg\"]):\n    # default highlight best avg (overall score) only\n    cols_to_highlight = [(metric, column) \n                         for column in labs_to_highlight \n                         for metric in EVAL_METRICS]\n\n    fmted_df = (pd_df\n     .pivot(index=[\"model\", \"data\"], columns=\"label\", values=[\"precision\", \"recall\", \"f1\"])\n     .style.highlight_max(subset=cols_to_highlight).format(\"{:.1%}\")\n    )\n    \n    return fmted_df","305ca739":"# saga is fast with large data sets and compatible with multiclass\nlog_reg = LogisticRegression(solver=\"saga\", max_iter=2000, random_state=0)\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier(random_state=0)","7303c6a8":"baseline_mods_tscv = pd.concat([\n    gen_baseline_eval(model, data_set)\n    for model in [\"log_reg\", \"knn\", \"rf\"]\n    for data_set in [\"orig\", \"edited\", \"enhanced\"]\n])\n\nfmt_baseline_eval(baseline_mods_tscv)","412498c2":"voting_clf = VotingClassifier(\n    estimators=[\n        (\"log_reg\", log_reg),  # highest F1 scores (keep scores from going too low)\n        (\"knn\", knn),  # non-0 metrics for window 3 and second highest F1 scores\n        (\"rf\", rf)  # non-0 metrics for window 3\n    ],\n    weights=[3, 2, 1],\n    voting=\"soft\") # uses probability rather than majority vote\n\npd.concat([\n    gen_baseline_eval(\"voting_clf\", data_set) \n    for data_set in [\"orig\", \"edited\", \"enhanced\"]\n]).pipe(fmt_baseline_eval)","f46c8690":"def test_models(clf_name:str, combined_data_version: str, ax_pos):\n    pipe = Pipeline(steps=[(\"scaler\", StandardScaler()), (\"clf\", globals()[clf_name])])\n    X_train, y_train, X_test, y_test = \\\n        [globals()[f\"{prefix}_{combined_data_version}\"] \n         for prefix in [\"X_train\", \"y_train\", \"X_test\", \"y_test\"]]\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    f1 = round(100 * f1_score(y_test, y_pred, average=\"macro\"))\n    \n    clf_type = \"Ensemble clf\" if clf_name == \"voting_clf\" else \"Best baseline clf\"\n    (plot_confusion_matrix(pipe, X_test, y_test, ax=axs[ax_pos])\n     .ax_.set_title(f\"{clf_type} + {combined_data_version}\\ndata (F1 score {f1}%):\"))\n    \n    return y_pred\n\nfig, axs = plt.subplots(1, 3, figsize=(12, 3))\nplt.subplots_adjust(wspace=0.4)\n\ntest_models(\"log_reg\", \"orig\", 0)\ntest_models(\"log_reg\", \"edited\", 1)\nfinal_y_pred = test_models(\"voting_clf\", \"enhanced\", 2)  # assign results for next step","cbbbd9f4":"print(classification_report(\n    y_true=y_test_enhanced, y_pred=final_y_pred,\n     zero_division=0, labels=[0, 1, 2, 3],\n    target_names=[\"All closed\", \"Window 1 open\", \"Window 2 open\", \"Window 3 open\"]\n))","7649fdae":"def conditions_to_df(conditions, cond_col_name):\n    df = (conditions\n          .to_frame()\n          .rename(columns={\"window_code\": cond_col_name})\n          .assign(\n              cumsum = lambda x: x[cond_col_name].cumsum(),\n              cumsum_lag2 = lambda x: x[\"cumsum\"].shift(2),\n              cumsum_lead1 = lambda x: x[\"cumsum\"].shift(-1),\n              streak_start = lambda x: (x[\"cumsum\"] == x[\"cumsum_lag2\"] + 1) & x[cond_col_name],\n              streak_end = lambda x: (x[\"cumsum\"] == x[\"cumsum_lead1\"]) & x[cond_col_name],\n              true_label = y_test_enhanced\n          ))\n    return df\n\ndef gen_cond_summary(orig_df, event_type):\n    summary_df = pd.DataFrame({\n        \"start_dt_utc\": orig_df.query(\"streak_start\").reset_index()[\"timestamp_utc\"],\n        \"start_tl\": orig_df.query(\"streak_start\").reset_index()[\"true_label\"],\n        \"end_dt_utc\": orig_df.query(\"streak_end\").reset_index()[\"timestamp_utc\"],\n        \"end_tl\": orig_df.query(\"streak_end\").reset_index()[\"true_label\"]\n    }, index=range(orig_df.query(\"streak_start\").shape[0]))\n    \n    assert (summary_df[\"start_tl\"] == summary_df[\"end_tl\"]).all(), \\\n        \"Start and end true labels don't match.\"\n    \n    summary_df = (summary_df\n                  .drop(\"start_tl\", axis=1)\n                  .rename(columns={\"end_tl\": \"window_code\"}))\n    print(f\"{event_type} events:\\n{summary_df}\\n\")\n    \n    return summary_df\n\n\npreds_match = final_y_pred == y_test_enhanced\nwindow_opened = y_test_enhanced != 0\npreds_0 = final_y_pred == 0\n\ntrue_pos = conditions_to_df((preds_match & window_opened), \"is_tp\")\nfalse_pos = conditions_to_df((~preds_match & ~preds_0), \"is_fp\")\nfalse_negs = conditions_to_df((~preds_match & preds_0), \"is_fn\")\n\ntrue_pos_summary = gen_cond_summary(true_pos, \"True positive\")\nfalse_pos_summary = gen_cond_summary(false_pos, \"False positive\")\nfalse_negs_summary = gen_cond_summary(false_negs, \"False negative\")","74772068":"print(\"Vertical bands show false negatives (grey) and true positives (other colours)\")\ng = (sns.FacetGrid(resamp_temp_data.query(\"timestamp_utc >= @SPLIT_DAY\"), \n                   col=\"dev_id\", hue=\"dev_id\",\n                   col_wrap=1, height=1.5, aspect=7, dropna=False)\n     .map_dataframe(plt.plot,\"timestamp_utc\",\"temp_c\")\n     .set(ylabel=\"Temperature (\u00b0C)\"))\n\n# add true positive and false negative annotations\ntemperature_y_lims = plt.gca().axes.get_ylim()\nfor window_num, ax_row in zip([2, 1, 3], [0, 1, 2]):\n    add_open_periods(g, temperature_y_lims, true_pos_summary, window_num, ax_pos=ax_row)\n    add_open_periods(g, temperature_y_lims, false_negs_summary, \n                     window_num, ax_pos=ax_row, color=\"grey\")\n    \n# update titles\nfor dev_id, ax in g.axes_dict.items():\n    ax.set_title(f\"{dev_id} (likely closest to window {dev_wind_dict[dev_id]})\")","5febca2f":"Overlaying the window opening events (shown by vertical shaded regions) on the temperature time series confirms that the sudden temperature drops for devices 258\\* and 270\\* correspond to opening events of the suspected windows. For other window opening events, I don't see obvious correlations with change in temperature. I've also added a horizontal band covering the mean +\/- standard deviation for each time series, hoping that it might be useful for feature engineering, but there seems to be temperatures outside of the band regardless of window states.\n\nCuriously, two window 1 open periods overlap on 2021-01-04 (around 2:30 - 3:00 PM PST as well as around 8 AM - 9:45 PM PST). Again, it seems unreasonable that a window would be left open for 14 hours in winter, *and* that the temperature would be unaffected in all sensors. Additionally, there's one big drop in temperature for sensor 270\\* the next day without a corresponding window opening event (shaded in grey). Since the window opening data was manually entered, these observations suggest there was a possibly a mistake in the data entry, and I should look closer at the window 1 opening events.","d57e7d29":"**Cross validation**\n\n[StratifiedKFold](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html) is normally suitable for cross validation of imbalanced data sets because it guarantees the same proportion of classes in all folds. But it's problematic for our time series data, since the data points are not independent from each other. scikit-learn has [TimeSeriesSplit](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.TimeSeriesSplit.html), a cross-validator specifically for time series data that uses k folds as train set, and k+1th fold as test in a k-fold validation:\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_007.png\" width=\"350\"\/><img src=\"https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cv_indices_010.png\" width=\"350\"\/>\n\nThe downside (especially for this use-case) is for most validation folds, I won't have much training data to use. Since there are only 29 window-opening events, some with missing temperature readings, spread across three windows, it's important to check that the each fold has enough data from all four classes in both training and testing sets:","7ac72705":"Back to [Overview](#overview)\n\n<a id=\"fit-and-evaluate-models\"><\/a>\n### Fit and evaluate models\n\n**Evaluation metrics**\n\nBecause of the class imbalance, the default metric (accuracy) would be high even in a useless model like predicting the majority class (0) or a minority class (e.g. 1) for every data point. Instead, I'll use macro-averaged precision and recall for troubleshooting. Since F1 is the harmonic mean of precision and recall, it's a good summary metric which I'll use for comparing models:","23a12813":"For all three models, using the edited data resulted in a higher average precision, recall and F1 scores than when using the original data set. In all but one case, using the enhanced dataset further improved this. It also resulted in non-zero precision, recall and F1 scores for label 3 data points in two out of three models, which was never seen when using the other two data sets.\n\nNext, I combined the three models in a weighted soft voting classifier. The resulting model gave a higher F1 score, suggesting the weaker models still complement the best-performing model:","a9dac771":"With the data entry error fixed, window 1 opening events are more aligned with 270\\* temperatures (while other entries were unaffected).","2d0e244b":"Back to [Overview](#overview)\n\n<a id=\"explore-data\"><\/a>\n### Explore data\n\nIt looks like the two tables have datetime data in different time zones, so I should fix that and prepare both tables for joining. I'll also rename some columns and shorten device IDs for clarity.\n\nThe temperature data isn't in regular intervals (which is expected with IoT data) and contains duplicates. I'll remove the duplicates then resample the data using the median (downsampling a little in the process) to create two data frames (in long and wide forms):","4a0fcd06":"Using the default five splits, there's minority class data missing in either training or testing set in at least one fold. But I can adjust both the number of splits and the test size in each split, to make sure that it works for both the original and edited data sets:","c6f69584":"There are only 29 window-opening events altogether, one of which lasted almost 14 hours! It's hard to imagine why, especially considering it was winter, and could potentially be a data issue. Let's see what happens when I exclude it:","75974601":"Finally, let's double check there's no collinear features in the data sets:","77897468":"Together, I found the evidence compelling enough to [contact the author](https:\/\/www.kaggle.com\/mattpo\/temperature-iot-on-gcp\/discussion\/242457) to ask for his opinion on my theory. In the mean time, for the rest of this analysis, I will assume this is a manual data entry error that should be corrected before the modelling steps.","bba12fc8":"For a better understanding, I summarised the true and false positive, as well as the false negative predictions:","517ab0e2":"The use of **macro** rather than micro or weighted averaging is also important. This method simply takes the mean of the metric for each class, making the scores for each minority class just as important as the scores for the majority class. For example, for the model always predicting the majority class:","158f13e4":"In the edited data set, the mean 270\\* temperature was lowest when window 1 was opened, similar to the case for 258\\* temperatures and window 2. Also, the multiple window 1 opening annotations was fixed, and share of `window_code` 1 events adjusted to 0.7% (in the training data) from 1.9% in the original data set:","14dbe6c1":"Summary statistics and plots grouped by device ID and window state suggest sensor 258\\* and 270\\* temperatures would be the best predictors for when windows 2 and 1 are open, respectively. For example, based on `window_code` for:\n- temperatures below ~11\u00b0C in 258\\* or below ~13\u00b0C in 270\\* (scatter plot, cumulative histogram)\n- lowest temperature readings and highest temperature variations (std) in 258\\* and 270\\* (summary statistics)\n- lowest mean temperature in 258\\* (summary statistics; curiously, I don't see the same pattern for 270\\*\/window 1)\n\nSince raw temperature readings alone seems insufficient to make the correct predictions, especially at higher temperature ranges, I think temperature difference and speeds of temperature changes might be helpful.\n\nIn contrast to the other two sensors, 275\\* statistics are relatively similar regardless of window state, and temperature patterns are similar for `window_code`s 3 and 0 (window 3 open and no windows open).","8b8ff040":"Back to [Overview](#overview)\n\n<a id=\"combined-data\"><\/a>\n#### **Combined data**\n\nNext, I look at the window and temperature data together and explored relationships between the labels and potential features.\n\nFirst, it's clear this is a data set with very imbalanced classes, with window 1 open events (1.7%) being slightly more frequent than open events from the other two windows (roughly 0.5%):","b8bfffc5":"Back to [Overview](#overview)\n\n<a id=\"manual-data-entry-error\"><\/a>\n#### **Manual data entry error**\n\nLooking at each window 1 opening events:","134cfd9f":"Fitting the best baseline classifier on the original data resulted in a disappointing model that predicted 0 for every data point, and an F1 score of 25% on held-out data. Correcting the single window open entry error increased the F1 score to 46%. Finally, feature engineering and ensembling the models further increased the score to 68%.\n\nFocusing on how to improve on the best performing model, it's clear that inability of the model to predict any positive window 3 labels pulled down the overall F1 scores. In fact, **the macro averaged F1 score for labels 0-2 is 90%**:","5ca2e8b3":"**Original data**","05c8b609":"There were quick drops and rises in 270\\* temperature readings corresponding to most window 1 opening and closing events. The exceptions were event 4 (where data was missing), and events 2, 3 and 6 (which are difficult to explain considering the temperature dropped at least 2\u00b0C in events 8 and 9, which were much shorter events, only 2 minutes). There's also a strange horizontal line in event 3 resulting from the duplicated data.\n\nImportantly, if the 2021-01-04 7:57:23 AM - 9:46:00 PM PST 14-hour window 1 opening event was supposed to be a 2-hour 2021-01-**05** 7:57:23 **PM** - 9:46:00 PM PST (2021-01-06 3:57:23 - 5:46:00 AM UTC) event, it would resolve the overlapping window opening issue and fit with the temperature data very well:","62b4039d":"In this notebook, I use 10.5 million rows of [temperature readings from sensors](https:\/\/www.kaggle.com\/mattpo\/temperature-iot-on-gcp) to detect when which, if any, windows are open. Through exploratory analysis, I detected and [reported](https:\/\/www.kaggle.com\/mattpo\/temperature-iot-on-gcp\/discussion\/242457) what's likely an error in a manually labelled open window event (which led to false minority labels in a very imbalanced data set). Correcting this single entry improved performance in all three baseline models, and increased the F1 score on held out data from 25% to 46%. Feature engineering and ensembling the models further increased the score to 68%. Importantly, for the sensors that were placed near their respective windows, all open window events were detected.\n\n<a id=\"overview\"><\/a>\n### Overview\n\n* [Introduction](#introduction)\n* [Explore data](#explore-data): iteratively explore and process data\n  * [Summary statistics and plots](#summary-statistics-and-plots)\n  * [Combined data](#combined-data): look at temperature and window state data together\n  * [Manual data entry error](#manual-data-entry-error)\n* [Pre-process data](#pre-process-data) (data sets: original, with error corrected, additionally with engineered features)\n* [Fit and evaluate models](#fit-and-evaluate-models)\n  * [Fit models](#fit-models): compare baseline models on three versions of the data; combine with a voting classifier\n  * [Test models](#test-models)\n\n<a id=\"introduction\"><\/a>\n### Introduction\n\nInternet of Things (IoT) is a system of internet-connected devices, used in private and public sectors (e.g. in the context of [manufacturing](https:\/\/rapidminer.com\/blog\/iiot-implementations-in-manufacturing\/), [agriculture](https:\/\/www.businessinsider.com\/smart-farming-iot-agriculture) and [smart cities](https:\/\/www.businessinsider.com\/iot-smart-city-technology)) as well as [in households](https:\/\/ec.europa.eu\/competition-policy\/system\/files\/2021-06\/internet_of_things_preliminary_report.pdf) all over the world. With affordable sensors and components, hobbyist can start their own DIY projects, including an open window detection project described in these [three-part blog posts](https:\/\/blog.doit-intl.com\/production-scale-iot-best-practices-implementation-with-gcp-part-1-3-44e2fa0e6554). The data set is made of two components:\n\n1) Temperature readings from three IoT sensors (258\\* and 270\\* are close to their windows; 275\\* is further away from a third window): `timestamp_utc` (date time), `timestamp_epoch` (integer), `temp_f` (float), `temp_c` (float), `device_id` (string)\n\n2) Open events for the windows (outside of these manually entered events, all windows were closed): `DayPST` (date), `StartTimePST` (time), `EndTimePST` (time), `ObjectCode` (window number, integer), `ObjectName` (window name, string)\n\nReal-world data can be messy, and as [Andrew Ng stated](https:\/\/youtu.be\/06-AZXmwHjo?t=397), sometimes data-centric approaches result in bigger improvements than model-centric approaches. In this analysis, I focus on using data wrangling and visualisation to identify and resolve data issues as well as creating features with predictive power.","33309e26":"On average, windows were opened for 25-40 minutes (median) - still quite a while for winter. Even after excluding the outlier, 4 out of 29 events lasted over 1 hour! On the other end of the scale, 5 events lasted up to 5 minutes, two of which lasted up to 2 minutes.\n\n*temperature data:*","c7a976d2":"**Edited data**: correct window open-state annotation","28baf5dd":"Back to [Overview](#overview)\n\n<a id=\"summary-statistics-and-plots\"><\/a>\n#### **Summary statistics and plots**\n\n*window data:*","386374f7":"Back to [Overview](#overview)\n\n<a id=\"fit-models\"><\/a>\n#### Fit models\n\nFirst, I'll run a few models on all three sets of processed data (original, edited to resolve the manual data entry error, and further enhanced with engineered features) for some baseline metrics:","254ac70a":"Overall, all 5 open window events in the held out data were detected for windows 1 and 2, with at least one streak of open state detected for at least 10 time points in a row. In contrast, out of the 6 false positive events for these windows, 5 occurred in only 1-2 time points in a row. Notably, 5 out of 6 false positive events occurred directly before or after a true positive open window event. Since the task was to detect open windows in order to remind the user to close them, I think the predictions can be used in combination with a rule-based logic to reliably detect open events for these two windows.\n\nUnfortunately, none of the window 3 opening events were detected. Since the sensor was placed much further away from its window compared to other sensors, the signal to noise ratio might be too low. If it's feasible, moving the sensor could be a simple solution. If not, maybe use of other sensors detecting change in humidity, carbon dioxide and\/or outside temperature could help.","16138f76":"Back to [Overview](#overview)\n\n<a id=\"pre-process-data\"><\/a>\n### Pre-process data","61649fad":"Back to [Overview](#overview)\n\n<a id=\"test-models\"><\/a>\n#### Test models","f1fd4dc5":"**Enhanced data**: add engineered features","b8959ba6":"After resampling to 10s intervals, it's clear 12-16% of data for each device is missing. This is also confirmed in the blog post:\n>rows of data where one or more sensors failed to record a value [were excluded]. There were many instances where the power went out \u2014 my entire house for an hour on Christmas Day, Roomba bumping into a sensor power cord, Maple knocking a sensor over... expect the unexpected!\n\nThe data here suggests the sensor data was missing for more than an hour on Christmas day, so perhaps the sensors had to be switched back on after the power outage.","62bc79a4":"The temperature detected by 275\\* seems to be almost always higher than the other two, in line with the fact that it's far from the window. Notably, there are some sudden and brief drops in temperature for 258\\* and 270\\* that probably correspond to window-opening events, but this is not seen in 275\\*.\n\nAll three time series generally share daily seasonality and there are clearly periods of missing data. Interestingly, 270\\* seems to have higher temperature readings than 258\\* at first, but the two start to converge around the start of January."}}