{"cell_type":{"7101abd3":"code","ce96ed00":"code","9d0bbb3a":"code","c2d92f4f":"code","a4141a55":"code","afa6eac4":"code","d4835ede":"code","a16f8b5d":"code","b1773cd3":"code","0f0b3dd3":"code","bf6a697f":"code","b6459c08":"code","cdd2dd24":"code","bd8c80f5":"code","803b9e5e":"code","0e25f082":"code","3b617875":"code","37d0ac89":"code","b69c45b1":"code","11e9373c":"code","b93e3929":"code","2da06938":"code","c49e386e":"code","17e59c7b":"code","78f793f9":"code","e9aca6ea":"code","c46bd12d":"code","b8e45063":"code","4db7ebdd":"code","e6891f64":"code","cd161385":"code","fbc28f53":"code","1cd94aa6":"code","f8001077":"code","596be26a":"code","7cd72d08":"code","645e49fb":"code","68bb0a45":"code","d7e38327":"code","d745d82a":"code","8ee49b35":"code","a6d9c703":"code","71e46436":"code","af3ff7df":"code","6e2cbb91":"code","65d4ca9c":"code","5f2273f5":"code","f497e26a":"code","c8c84995":"code","12bbc0a2":"code","92490064":"markdown","221de3a4":"markdown","ff23e73c":"markdown","040ce9a3":"markdown","ebd8bac2":"markdown","02eaf395":"markdown","053da9d5":"markdown","4df44ea5":"markdown","4863b148":"markdown","dbc513c1":"markdown","af426051":"markdown","e3ce04ab":"markdown","d6042f0b":"markdown","0aeece47":"markdown","f40d342d":"markdown","63d5be8f":"markdown","14857edb":"markdown","c7309f5f":"markdown","8b5c21a9":"markdown","0e261137":"markdown","8674a2bb":"markdown","08cb9dc3":"markdown","20fad254":"markdown","c97170d9":"markdown","de82a0fe":"markdown","c83153fd":"markdown","c5ce4582":"markdown","e8a48600":"markdown","19ae482e":"markdown","ce4df723":"markdown","71b294ce":"markdown","5eeb0394":"markdown","12c7647c":"markdown","d66e1339":"markdown","1fedc371":"markdown","bf132572":"markdown","a9654ca6":"markdown"},"source":{"7101abd3":"import pandas as pd\nimport numpy as np\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numbers\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import neighbors\nimport scipy.stats as stats\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import r2_score\nimport math\nimport random\nimport shutil\nfrom sklearn import tree\n\n\n\n#os.remove(\"\/kaggle\/working\/prediction1.csv\")\n#os.remove(\"\/kaggle\/working\/prediction5.csv\")\n#os.remove(\"\/kaggle\/working\/prediction2.csv\")\n#os.remove(\"\/kaggle\/working\/prediction0.csv\")\n#os.remove(\"\/kaggle\/working\/prediction4.csv\")\n#os.remove(\"\/kaggle\/working\/prediction6.csv\")\n\n\n\n\n\n\n\n","ce96ed00":"\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n","9d0bbb3a":"train_shape=train.shape\ntest_shape=test.shape\n\nprint(\"shape of train df is\",train.shape)\n\nprint(\"shape of test df is\",test.shape)","c2d92f4f":"columns=train.columns\n\nprint (columns)","a4141a55":"head=train.head()\n\nprint(head)","afa6eac4":"\ndef without(big, little):\n    result = []\n    \n    #loop through big list, return stuff that isnt in smaller list\n    \n    for item in big:\n        if item not in little:\n            result.append(item)\n    \n    # need this for dealing with technichal error with results of len 1\n    \n    if len(result)==1:\n\n        return result[0]\n    else:\n        return result\n\n","d4835ede":"\ndef labelled(df,label_name):\n    \n    # get number of rows in df \n    \n    l=len(df)\n    label_col=[]\n\n    # loop over rows append label name to list for each row,\n    \n    for k in range(l):\n        label_col.append(label_name)\n        \n       \n    # list needs to become array before we add it as column\n    \n    label_col=np.asarray(label_col)\n\n    #Add as column to df with column name 'label'\n    \n    df['label']=label_col\n\n    return df\n","a16f8b5d":"def preprocess(train,test):\n\n    #getting testing and training column list\n    all_columns = train.columns\n    predictor_col=test.columns\n    \n    # getting the y or ouptut column using \"without\" function\n    \n    ycol=without(all_columns,predictor_col)\n    \n\n    # seperating train into x and y values using .drop() method\n    X_train=train.drop([ycol],axis=1)\n    y=train[ycol]\n\n    \n    #Adding label column labelling data as test or train\n    \n    X_train_labelled=labelled(X_train,\"train\")\n    X_test_labelled=labelled(test,\"test\")\n\n\n\n    # combining all X data into one df\n    \n    X=pd.concat([X_train_labelled,X_test_labelled])\n    \n    # returning X and y in list, as we want them\n    \n    result=[X,y]\n\n    return result\n\n\n#applying to our data \n\nX,y=preprocess(train,test)\n\n","b1773cd3":"print(\"shape of  X df is\",X.shape,\",expected shape is (\",len(train)+len(test),\",\",len(columns),\")\")\n\nprint(\"shape of y df is\",y.shape,\",expected shape is (\",len(train),\",)\" )","0f0b3dd3":"def na_count(col_data):\n    \n    # Gets list of all na values in  array \n\n    na_list=pd.isna(col_data)\n    \n     # Gets proportion of NA by dividing by total lenght  of array\n    \n    result=sum(na_list)\/len(col_data)\n    \n    #return proportion\n    \n    return result\n    \n    ","bf6a697f":"def na_distribution(df,plot=False,cutoff=.25):\n\n    result=[]\n    nums=[]\n    columns=df.columns\n\n     # lopping through all columns, recoding columns name and proportion of na values\n    \n    for column in columns:\n        col_data=df[column]\n\n        num=na_count(col_data)\n        nums.append(num)\n\n        info=[column,na_count(col_data)]\n        result.append(info)\n        \n    #sorting list \n\n    result.sort()\n    nums.sort()\n\n    \n    # if given plot argument of true, plots na proportion per column, \n    #with line at cutoff value for reference in descending order\n    \n    if plot:\n        plt.plot(nums)\n        plt.axhline(y=cutoff,color='r')\n        plt.show()\n        \n       \n\n        \n    # return list of na proportion per column\n    \n    return result\n\n    \n","b6459c08":"def na_remove(df,cutoff=.25):\n\n    # first lets get proportions for each column using na_distribution\n    \n    bad_columns=[]\n    na_dist=na_distribution(df)\n\n    # loops through columns, getting list of \"bad columns\" whose porportion of na\n    #is above acceptable threshold, default is .25\n    \n    for data in na_dist:\n        if data[1]>cutoff:\n\n            bad_columns.append(data[0])\n        \n    # loops through bad column list, dropping them from our inputed df\n\n    for bad_column in bad_columns:\n\n        df=df.drop([bad_column],axis=1)\n        \n    #returns this edited dataframe\n    \n    return df","cdd2dd24":"#getting initial distribution, plotting na proportion.\n\ninitial_dist=na_distribution(X,plot=True)\n\n#removing columns with proportiong greater than default .25 cutoff\n\nX=na_remove(X)\n\n# plotting again to see change\n\nnew_dist=na_distribution(X,plot=True)\n\n","bd8c80f5":"def is_num(column,cutoff=7):\n\n    alpha=False\n    beta=False\n    \n    l=len(column)\n    num_num=0\n\n    # we are using numbers module to check if stuff in column is number\n    \n    Number = numbers.Number\n\n    # looping over column keeping count of how many entries are numerical\n    \n    for item in column:\n\n        if isinstance(item,Number):\n            num_num+=1\n            \n\n    # get proportion of numerical items, if proportion is greater than .85 we call this a numerical column\n    # reason we arent insisting on 100% is that some values may have been entered as string on accident like one hundred\n    # insted of 100, this gives us flexibility to deal with this.\n    \n    num_prop=num_num\/l\n\n    if num_prop>.85: \n        alpha=True\n        \n    levels=set(column)\n    num_levels=len(levels)\n    \n    if num_levels>cutoff:\n        \n        beta=True\n\n    # return true if proportion of numerical items is greater than .85 and more than 7 levels\n    \n    if alpha and beta:\n        \n        return True\n\n    else:\n        return False","803b9e5e":"\ndef numcat_split(df):\n    \n    #setting up empty data frames to be filled and getting column list\n\n    numerical=pd.DataFrame()\n    categorical=pd.DataFrame()\n    columns=df.columns\n\n    # lopping over column, adding numerical to numerical df, else to categorical\n    \n    for column in columns:\n\n        col_data=df[column]\n\n        if is_num(col_data):\n            numerical[column]=col_data\n\n        else:\n\n            categorical[column]=col_data\n\n    #return list containing 2 new dataframes\n\n    return [numerical,categorical]\n","0e25f082":"X_num,X_cat=numcat_split(X)\n\nprint(\"Shape of our numerical data is\",X_num.shape, \"meaning we have\", len(X_num.columns),\"numerical columns\")\n\nprint(' ')\n\nprint(\"Shape of our categorical data is\",X_cat.shape, \"meaning we have\", len(X_cat.columns),\"categorical columns\")\n      \nprint(' ')\n\nprint('Expected total columns is',len(X.columns),\",actual is\",len(X_num.columns)+len(X_cat.columns))\n","3b617875":"\ndef nan_to_mean(column):\n    \n    # get version of column with no na values and get its mean\n    \n    nona=column[~np.isnan(column)]\n    mean=np.mean(nona)\n\n    # loops through column, replacing nan with mean \n    \n    result=[]\n    for item in column:\n        if np.isnan(item):\n            result.append(mean)\n        else:\n            result.append(item)\n\n    #return new column\n    \n    result=np.asarray(result)\n\n    return result","37d0ac89":"\ndef mean_df(df) :\n\n    # creating blank df to build up with converted columns, and get column list\n    \n    result=pd.DataFrame()\n    columns=df.columns\n\n    #looping over columns converting with nan_to_mean and adding to our result df\n    for column in columns:\n\n        col_data=df[column]\n        mean_data=nan_to_mean(col_data)\n\n        result[column]=mean_data\n        \n    # return df of converted columns\n    \n    return result\n        ","b69c45b1":"def prop_0(column):\n    \n    l=len(column)\n    \n    zero_count=0\n    \n    for item in column:\n        if item==0:\n            \n            zero_count+=1\n    \n    prop= zero_count\/l\n    return prop","11e9373c":"def df_prop_0(df,plot=False):\n    \n    columns=df.columns\n    \n    result=[]\n    props=[]\n    \n    for column in columns:\n        \n        col_data=df[column]\n        \n        prop=prop_0(col_data)\n        report=[column,prop]\n        \n        props.append(prop)\n        result.append(report)\n    \n    if plot:\n        props.sort()\n        plt.plot(props)\n        plt.show()\n    \n    \n    return result\n        \n        \n    ","b93e3929":"# getting initial shape and NA counts\n\nprint(\"initial shape is \",X_num.shape)\n\ninitial_dist=na_distribution(X_num,plot=True,cutoff=.1)\n\n# converting with mean_df\n\nX_num=mean_df(X_num)\n\n# getting new shape and NA counts\n\nprint(\"new shape is \",X_num.shape)\n\nnew_dist=na_distribution(X_num,plot=True,cutoff=.1)\n\n\nzero_props=df_prop_0(X_num)\n","2da06938":"def cat_from_zero(column):\n    \n    cat_column=[]\n    \n    for item in column:\n     \n        if item !=0:\n            cat_column.append(1)\n            \n        else:\n            \n            cat_column.append(item)\n    \n    cat_column=np.asarray(cat_column)\n    return cat_column\n        ","c49e386e":"def zero_to_cat(df,cutoff=.4):\n    \n    props=df_prop_0(df)\n    add_cat=[]\n    \n    \n    for item in props:\n        if item[1]>cutoff:\n            add_cat.append(item[0])\n        \n    for column in add_cat:\n        \n        col_name=column+\"cat\"\n        \n        col_data=df[column]\n        col_data=cat_from_zero(col_data)\n        \n        df[col_name]=col_data\n    \n    return df","17e59c7b":"def clean_cat_col(column):\n\n    clean_col=[]\n\n    #loop throgh column change \"nan\" to \"none\" and leave other values unchanged\n    \n    for item in column:\n\n        if str(item)==\"nan\":\n            clean_col.append(\"none\")\n    \n\n        else:\n            clean_col.append(item)\n\n    clean_col=np.asarray(clean_col)\n\n    return clean_col\n","78f793f9":"def clean_cat_df(df):\n\n    #set up empty output df and get column list\n    \n    clean_df=pd.DataFrame()\n    columns=df.columns\n\n    # loop through columns, converting each with clean_cat_col\n    \n    for column in columns:\n\n        col_data=df[column]\n        clean_df[column]=clean_cat_col(col_data)\n\n    return clean_df","e9aca6ea":"# getting initial shape and NA counts\n\nprint(\"initial shape is \",X_cat.shape)\n\nold_dist=na_distribution(X_cat,plot=True,cutoff=.1)\n\n# converting with clean_cat_df\n\nX_cat=clean_cat_df(X_cat)\n\n# getting new shape and NA counts\n\nprint(\"new shape is \",X_cat.shape)\n\nnew_dist=na_distribution(X_cat,plot=True,cutoff=.1)","c46bd12d":"\ndef column_namer(name,len):\n    \n    col_names=[]\n    \n    #loop through numer of dummies, gets name for each, name is just orignal column name +index \n    for i in range(len):\n\n        col_name=name+str(i)\n        col_names.append(col_name)\n    \n    #return list of column names\n    \n    return col_names","b8e45063":"def get_dummydf(df):\n\n    # first make sure our input df is clean of na's with clean_cat_df, and get column list\n    clean_df = clean_cat_df(df)\n    columns = clean_df.columns\n\n    #create empty ouput df for us to fill with dummy vars\n    \n    dummy_df = pd.DataFrame()\n\n    # loop through columns, getting dummy columns with pd.pd.get_dummies()\n    for column in columns:\n\n        col_data = clean_df[column]\n        dummies = pd.get_dummies(col_data)\n        columns = dummies.columns\n\n        l=len(columns)\n        \n        #for each column get names for dummie columns with column_namer, set 0 as start index\n        \n        names=column_namer(column,l)\n        index=0\n        \n        # give dummies correct names and add them to output df \n        for column in columns:\n\n            data = dummies[column].values\n            name=names[index]\n\n            dummy_df[name] = data\n            index=index+1\n\n    \n    #return output df made up of dummy columns\n    \n    return dummy_df\n\n","4db7ebdd":"def df_proccesed(train,test,na_cut=.25,zcat=True):\n    \n    #getting X and y from train, test using preprocessed\n\n    X, y = preprocess(train, test)\n\n    \n    #Removing na columns and splitting X into numerical and categorical\n    \n    X = na_remove(X,na_cut)\n    \n    X_num,X_cat = numcat_split(X)\n\n    # cleaning numericals with mean_df  getting categorical dumm\n    \n    X_num = mean_df(X_num)\n    \n    if zcat:\n        \n        X_num=zero_to_cat(X_num)\n        \n\n    # getting categorical dummiesd with get_dummy_df\n    \n    X_cat = get_dummydf(X_cat)\n\n    #combining 2 together into one df with all features\n    \n    result = pd.concat([X_num, X_cat], axis=1)\n\n\n    #finding how many train values there are, using label column we created earlier with labelled function\n    \n\n\n    train=sum(result['label1'])\n\n    # splitting X back into train and test rows usin\n    X_train = result[:train]\n    X_test = result[train:]\n\n\n    #returning all data we need to feed model\n    \n    result = [X_train, X_test, y]\n\n    return result","e6891f64":"#getting processed data from train and test using df_processed\n\nX_train, X_test, y= df_proccesed(train,test)\n\n\nprint(X_train.shape)\n\nprint(X_test.shape)\n\nX_train.head()\n","cd161385":"def get_predictions0(data,model=LinearRegression(),ytran=True):\n    \n\n    # data is going to be output to our df_proccesed function, this outputs [X_train, X_test, y]\n    \n    X_train,X_test,y=data\n    \n    if ytran:\n        y=np.log(y)\n\n    #fitting model to our training data\n    model.fit(X_train, y)\n    \n    #getting predictions for our test data \n    predictions = model.predict(X_test)\n    \n    if ytran:\n        predictions=np.exp(predictions)\n    \n    #code below just gets predictions into acceptable format for submission\n    start_index = len(X_train) + 1\n    stop_index=len( X_train)+len(X_test) + 1\n    Predictions = pd.DataFrame(predictions, columns=[\"SalePrice\"], index=range(start_index, stop_index))\n    Predictions.index.name = \"Id\"\n\n    # return predictions in acceptable format\n    \n    return Predictions","fbc28f53":"#getting X_train, X_test, y from train, test\ndata=df_proccesed(train,test)\n\n# getting predictions for our data and the default model, LinearRegression()\n\nprediction0=get_predictions0(data)\n\n# now on your computer save these to csv and submit just using \nprediction0.to_csv(\"prediction0.csv\")","1cd94aa6":"\ndef k_fold_crossval(data,model=LinearRegression(),k=4,printout=True):\n    \n    #set X_train as our X data and y as y, \n    #X_test not used since cross vallidation requires y values for all entries \n    \n    X,y=data[0],data[2]\n    \n    \n    #Get array of scores from k fold cross validation, note this has strange error for k>4\n    # it may be some kind of overflow error, I recommend just using the k=4 default arg\n    \n    scores = cross_val_score(model, X, y, cv=k)\n    \n    mean=scores.mean()\n    std= scores.std()\n    \n    low=mean-(2*std)\n    high=mean+(2*std)\n    \n    result=[low,mean,high]\n    \n    if printout:\n        \n        print(\"score is\",mean,\"+\/-\",(2*std),)\n    \n        print('range is',[low,high])\n    \n    \n    return result\n\n    \n        ","f8001077":"result=k_fold_crossval(data,k=4)","596be26a":"def feature_score(data,feature,model=LinearRegression(),k=4):\n    \n    X=(data[0])[feature]\n    X=X.values.reshape(-1,1)\n    \n    y=data[2]\n    y=y.values.reshape(-1,1)\n    \n    scores = cross_val_score(model, X, y, cv=k)\n    mean=scores.mean()\n    \n    return mean\n    \n    ","7cd72d08":"def rank_features(data,model=LinearRegression(),plot=True,plotrange=15):\n    \n    X,y=data[0],data[2]\n\n    columns=X.columns\n    \n    result=[]\n    scores=[]\n    for feature in columns:\n        \n        score=feature_score(data,feature)\n        \n        scores.append(score)\n        \n        report=[score,feature]\n        result.append(report)\n        result.sort(reverse=True)\n        \n        \n    if plot:\n        \n        scores=np.asarray(scores)\n        indices = np.argsort(scores)[::-1][0:plotrange]\n    \n        \n        plt.figure()\n        plt.title(\"Feature importances\")\n        plt.barh(range(plotrange), scores[indices][::-1],\n        color=\"r\",  align=\"center\")\n        plt.yticks(range(plotrange), columns[indices][::-1])\n    \n        plt.xlim([0, 1])\n        plt.show()\n    \n    \n    return result\n","645e49fb":"ranks=rank_features(data)","68bb0a45":"def drop_neg(data,model=LinearRegression()):\n    \n    X_train,X_test,y=data\n    \n    ranks=rank_features(data,model,plot=False)\n    \n    for item in ranks:\n        if item[0]<0:\n            column=item[1]\n            X_train=X_train.drop([column],axis=1)\n            X_test=X_test.drop([column],axis=1)\n    \n    data=[X_train, X_test, y]\n    \n    return data\n","d7e38327":"print(\"For data:\")\n\nk_fold_crossval(data)\n\nprint(\" \")\n\nprint(\"For data_new:\")\n\ndata_new=drop_neg(data)\n\nk_fold_crossval(data_new)\n\nprediction1=get_predictions0(data_new)\nprediction1.to_csv(\"prediction1.csv\")\n","d745d82a":"def data_copy(data):\n    X_train, X_test, y=data\n    \n    X_train_copy=pd.DataFrame.copy(X_train,deep=True)\n    X_test_copy=pd.DataFrame.copy(X_test,deep=True)\n    \n    return [X_train_copy,X_test_copy,y]","8ee49b35":"def log_feature(data,feature):\n    \n    data_temp=data_copy(data)\n    \n    X_train,X_test,y=data_temp\n    \n    min_train=min(X_train[feature])\n    min_test=min(X_test[feature])\n    \n    Min=min([min_train,min_test])\n    \n    if Min>0:\n        \n        X_train[feature]=np.log(X_train[feature])\n        X_test[feature]=np.log(X_test[feature])\n    \n    else:\n        \n        X_train[feature]=np.log(X_train[feature]+1)\n        X_test[feature]=np.log(X_test[feature]+1)\n        \n    \n    result=[X_train,X_test,y]\n    \n    return result","a6d9c703":"def get_prob_plot(data,feature,transform=False):\n  \n    \n    if transform:\n        data=log_feature(data,feature)\n        X=pd.concat([data[0],data[1]])\n        res = stats.probplot(X[feature], plot=plt)\n        \n    else:\n        X=pd.concat([data[0],data[1]])\n        res = stats.probplot(X[feature], plot=plt)\n        \n    ","71e46436":"plot=stats.probplot(y, plot=plt)","af3ff7df":"def df_proccesed1(train,test,na_cut=.25,ytran=True,drop=True):\n    \n    #getting X and y from train, test using preprocessed\n\n    X, y = preprocess(train, test)\n\n    if ytran:\n        \n        y=np.log(y)\n        \n    \n    #Removing na columns and splitting X into numerical and categorical\n    \n    X = na_remove(X,na_cut)\n    \n    X_num,X_cat = numcat_split(X)\n\n    # cleaning numericals with mean_df  getting categorical dumm\n    X_num = mean_df(X_num)\n\n    # getting categorical dummiesd with get_dummy_df\n    \n    X_cat = get_dummydf(X_cat)\n\n    #combining 2 together into one df with all features\n    \n    result = pd.concat([X_num, X_cat], axis=1)\n\n\n    #finding how many train values there are, using label column we created earlier with labelled function\n    \n    train=sum(result['label1'])\n\n    # splitting X back into train and test rows usin\n    X_train = result[:train]\n    X_test = result[train:]\n\n\n    #returning all data we need to feed model\n    \n    result = [X_train, X_test, y]\n    \n    if drop:\n        \n        \n        result=drop_neg(result)\n        \n\n    return result\n\n\n\n","6e2cbb91":"def get_predictions1(data,model=LinearRegression(),ytran=True):\n    \n    \n    # data is going to be output to our df_proccesed function, this outputs [X_train, X_test, y]\n    \n    X_train,X_test,y=data\n    \n\n    #fitting model to our training data\n    model.fit(X_train, y)\n    \n    #getting predictions for our test data \n    predictions = model.predict(X_test)\n    \n    if ytran:\n        \n        predictions=np.exp(predictions)\n    \n    #code below just gets predictions into acceptable format for submission\n    start_index = len(X_train) + 1\n    stop_index=len( X_train)+len(X_test) + 1\n    Predictions = pd.DataFrame(predictions, columns=[\"SalePrice\"], index=range(start_index, stop_index))\n    Predictions.index.name = \"Id\"\n\n    # return predictions in acceptable format\n    \n    return Predictions","65d4ca9c":"def transform_guesser(data,cut_val=0.005,early_cut=40):\n    \n    X_train, X_test, y = data\n\n    features = X_train.columns\n    to_transform = []\n    \n    #Getting our data and initializing list of columns to apply log transform\n\n    for feature in features[0:early_cut]:\n        \n        \n        data_new = log_feature(data, feature)\n\n        val1 = k_fold_crossval(data, printout=False)\n        \n        val2 = k_fold_crossval(data_new, printout=False)\n        \n        diff=val2[1]-val1[1]\n        \n        # getting cross val score for data with and without a column being transferred for each column\n       \n\n        if (diff>cut_val) :\n            \n            to_transform.append(feature)\n             \n        # if score goes up by more than cutoff value, here .005, we apend said column's name\n        #to list of columns to transform\n             \n             \n             \n    for feature in to_transform:\n        data = log_feature(data, feature)\n    \n    #transforming all columns in the list\n    \n    return data\n\n    # returning data with transformed columns","5f2273f5":"def poly_feature(data,feature1,feature2,centered=False):\n    \n    # this function adds polynomial feature made by multiplying 2 feature columns, feature 1 and feature 2, together\n    data_temp=data[:]\n    X_train, X_test, y = data_temp\n    \n    train_col1=X_train[feature1]\n    train_col2=X_train[feature2]\n    \n    test_col1= X_test[feature1]\n    test_col2=X_test[feature2]\n    \n    train_poly_col=np.multiply(train_col1,train_col2)\n    \n    test_poly_col=np.multiply(test_col1,test_col2)\n    \n    \n    if centered:\n        \n        mean_train=train_poly_col.mean()\n        train_poly_col=train_poly_col-mean_train\n        \n        mean_test=test_poly_col.mean()\n        test_poly_col=test_poly_col-mean_test\n        \n    \n    \n    poly_name=feature1+'x'+feature2\n    \n    \n    X_train[poly_name]=train_poly_col\n    \n    \n    X_test[poly_name]=test_poly_col\n    \n   \n    \n    result=[X_train, X_test, y]\n    \n    \n    return result","f497e26a":"def best_poly_k(data,k=4,features_considered=8,reresult=False,centered=False,printout=True):\n    \n    # appends best k polynomial features to data, as measured by improvement in cross val score\n    \n    ranks=rank_features(data,plot=False)[0:features_considered]\n    \n    poly_added=[]\n    result=[]\n    \n    for i in range(k):\n        \n        best_score=0\n        \n        for rank1 in ranks:\n            feature1=rank1[1]\n            \n            for rank2 in ranks:\n                \n                feature2=rank2[1]\n                \n                features=[feature1,feature2]\n                \n                data_temp=data_copy(data)\n                \n                data_temp=poly_feature(data_temp,feature1,feature2,centered=centered)\n                \n                score=k_fold_crossval(data_temp,printout=False)[1]\n                \n                if (score>best_score) and (features not in poly_added) :\n                    \n                    best_score=score\n                    bestf_1=feature1\n                    bestf_2=feature2\n                    \n                    report=[best_score,[bestf_1,bestf_2]]\n                    \n                    \n        data=poly_feature(data,bestf_1,bestf_2)    \n        \n        if printout:\n            \n            print(\"report:\",report)\n                     \n        \n        poly_added.append([bestf_1,bestf_2])\n        poly_added.append([bestf_2,bestf_1])\n        result.append(report)\n        \n    if reresult:\n        \n        return [data,result]\n    \n    else:\n          return data\n","c8c84995":"\nprint(\"data0\")\ndata0=df_proccesed(train,test)\nval0=k_fold_crossval(data0)\nprint(' ')\n\nprint(\"data1\")\ndata1=df_proccesed1(train,test,drop=False)\nval1=k_fold_crossval(data1)\nprint(' ')\n\nprint(\"data2\")\ndata2=df_proccesed1(train,test,drop=True)\nval2=k_fold_crossval(data2)\nprint(' ')\n\n\nprint(\"data3\")\ndata3=transform_guesser(data2)\nval3=k_fold_crossval(data3)\nprint(' ')\n\nprint(\"data4\")\ndata4=best_poly_k(data3,k=20,features_considered=20)\nval4=k_fold_crossval(data4)\n\nprint(' ')\n\n\nprediction4=get_predictions1(data4)\nprediction4.to_csv('prediction4.csv')\n\n","12bbc0a2":"gmodel=GradientBoostingRegressor\nparams={'n_estimators': 750,'max_depth': 3,'min_samples_split': 5, 'learning_rate': 0.1,'loss': 'ls'}\nmodel=gmodel(**params)\n\n\npredictions5=get_predictions1(data4,model=model)\npredictions5.to_csv('prediction5.csv')\n\nprediction6=(predictions5+prediction4)\/2\nprediction6.to_csv('prediction6.csv')","92490064":"Now we have things in right form, lets start cleaning. First, lets take out the columns with to many NA's. Theres ways of dealing with NA other than removing columns but when proportion gets to high it may be worth just removing. First lets get function to count proportion of na values in column.","221de3a4":"Lets check this for our data","ff23e73c":"Outline:","040ce9a3":"Lets average the predictions of linear regression and gradient boosting regression on this data with polynomial featues added","ebd8bac2":"prediction1 scores 0.16684, little better, lets keep going","02eaf395":"This first naive prediction returned a score of 0.179. Not terrible for first try, but we can do better. First lets get some cross validation capabilities, so we can mock test our model before using the actual testing data. Lets build function that does k-fold cross validation on our model, returns list of R^2 scores, and average","053da9d5":"Now lets apply to our X data frame,see how many of each kind of column we have, and check they add to correct totals.","4df44ea5":"Lets check this all works as intended with our X_num df. mean_df should return df of same shape with na values replaced with mean. We can check this by printing shape, and using na_distribution to check for na","4863b148":"Now lets just get head of our train data using .head() method","dbc513c1":"Lets use these to drop columns with more that .25 na as proportion, and plot the difference\n","af426051":"Now lets bring it all together and return our labelled, properly indexed X and y.","e3ce04ab":"We'll be combining test and train into one X df but we need to keep track of which rows belong to  train and which to test. Rather then just recording the indices, lets add a label column that designates a row as either being test or train. **labelled** function below take df and add \"label\" column, whose value is just a the label_name string, for every row. Here our label_names will be train for the train df and test for test df. We'll just feed the train and test df into labelled to get them their appropriate labels","d6042f0b":"Now we have our data, time for some preliminary exploration. First we get the shape of each dataframe, as well as a printout of the columns, and the \"head\" of our data","0aeece47":"First, lets take the y column out of the train dataframe so that train and test have the same columns when we combine them. Remember here we know the y column has name \"saleprice\" but its usefull to have a function that removes the output column from an arbitrary train df without use having to look up what the output or \"y\" column is called. Its as simple as figuring out which column is missing from the test df and taking it out from train. The **without** function below does exactly that.","f40d342d":"Now lets create function to remove columns if they have to high a proportion of na values.","63d5be8f":"This gets us score of .13144, best I've been able to do. Thanks for reading!","14857edb":"Now lets import our train and test data as pandas dataframes","c7309f5f":"Now lets just get function to do do this to whole numerical df","8b5c21a9":"Now that we have utilities for dealing with numerical columns, lets deal with our categorical columns. First we have to deal with NA\"s. We cant assign to mean since categorical variables dont have mean, so well just assign na's to their own category, the \"none\" category. First, lets get function to convert column na values to \"none\" string","0e261137":"We know test will just have same columns as train, minus the output y column,here called \"SalePrice\" ,so its good enough to get list of train df columns.","8674a2bb":"Now lets get function to apply this to whole df","08cb9dc3":"Lets deal with our numerical features first. Recall, we removed every column with more than .25 na values, but we'll need to remove all na's before feeding into model. For numerical features, we'll be replacing na values with the mean for that column. Lets start by creating a method to replace all the na values in an array with that arrays mean\n","20fad254":"Now lets combine all we've done into one function that takes in train and test and outputs a 2 cleaned  and processed dataframes, one for train and one test,  made up of our cleaned numerical columns and our dummy columns.","c97170d9":"Now lets build function to loop over all columns and build up a numerical and categorical df ","de82a0fe":"Lets see if we can improve this by manipulating our data. First, lets get sense of each features importance, to see what to prioritize.","c83153fd":"Now that we can get a clean categorical df, we'll turn it into form that our models will be able to work with using dummy coding. This assigns a dummy indicator variable to each level of a categorical variable. If the row is in that category, this dummy var takes value one. Otherwise, it takes value 0. First were gonna define a column naming function, this'll automatically rename our dummy variables according to the categorical variables then came from. So for instance if we had a categorical for season of year,called \"season\" we'd get a dummy corresponding to summer fall winter and spring, named season0,season1,season2,season3 respectivly .","c5ce4582":"Now lets get function to plot na's per column, and give us a list of how many na's are in each column","e8a48600":"Now lets make function to convert a clean categorical df to a dummy df ","19ae482e":"Lets check this works with X_cat","ce4df723":"Lets apply this to our data and check it all works.","71b294ce":"Now that we have na's removed, lets seperate our columns into numerical and categorical. We'll be dealing with these different as we do feature selection, so its usefull to seperate them first. First, lets get a function that returns if a column is numerical or not","5eeb0394":"The very thorough guide through my first kaggle comp\nA beginner friendly approach to the cleaning,analysis and modeling process\n                                   \nI've been deliberate about explaining every detail of what I did here, assuming as little backround as I could. Hence the length. I've chosen to write my code in functions. This is both a style preference and I feel it makes it more robust and reasuable. I've included sketches to how all the functions work together to produce the end product. Please refer to those diagrams and  to the hyperlinked outline below for a rough sketch at how to workbook is layed out and what it contains.\n","12c7647c":"Now lets apply all this to our data and see how well we do with just a naive linear regression.","d66e1339":"0.14837 now, getting better, but we can keep going.","1fedc371":"We can see our dataframe is a mix of cateogorical and numerical predictors, and that many columns have many na values.\nWe'll see how to deal with this shortly, but first lets combine train and test into one big X dataframe, and one y dataframe, so we can do the data cleaning simulataneously. Our approach is gonna be to build some functions that take in our train and test data, and return what we want an X and y dataframe.","bf132572":"Now we have data in acceptable clean form, lets try feeding into regression model and seeing how well our predictions do. We build get_predictions function to do all this ","a9654ca6":"Lets get the shape of our new df to make sure all went as planned, using what we know about shampe of dataframes we are combining"}}