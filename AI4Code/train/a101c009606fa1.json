{"cell_type":{"dbc74b26":"code","d824690f":"code","100dff46":"code","5bfb550f":"code","b2316bfe":"code","ab67b1c1":"code","7e375352":"code","3c8c8dc5":"code","3d9e1ded":"code","bbf6aa96":"code","9fb022c2":"code","310b5274":"code","4c983e64":"code","5a907db6":"code","8202175d":"code","239f0492":"code","581815d6":"code","81918665":"code","61b8642a":"code","d9716d5b":"code","7b231390":"code","0709261e":"code","41995116":"code","3d8a1705":"code","e904e38c":"code","bd841c21":"code","ec1fca5a":"code","16b0cf0f":"code","4e8454c2":"code","98f705b6":"code","7a3ebd6e":"code","d2abfc04":"code","a091244b":"code","60765715":"code","627ce49f":"code","1cb683b2":"code","400037cd":"code","65fd6fbb":"code","19a39917":"code","ac7542e5":"code","7f6318ce":"code","8bf22fab":"code","71fb3da6":"code","804d7c25":"code","4fedadef":"code","b2e03e7d":"code","f88a015e":"code","e3e94b54":"code","3b4d3732":"code","8ca2de02":"code","171763fe":"code","9390d508":"code","d8deb040":"code","f2fcb6d5":"code","2a71c62a":"markdown","bcdb0466":"markdown","30fda77c":"markdown","7b75bb37":"markdown","383baa0e":"markdown","a6a1c913":"markdown","d096f3ee":"markdown","1039aba5":"markdown","6912a136":"markdown","82953933":"markdown","7482f380":"markdown","960d4adc":"markdown","a2dea8f9":"markdown","f23fbd44":"markdown","a65a0b1d":"markdown","49a66a83":"markdown","3e8ebcb4":"markdown","f2fd2a88":"markdown","dd712bf0":"markdown","7986d1d6":"markdown","b2e837b2":"markdown","73ba378d":"markdown","899df04c":"markdown","c8595a3f":"markdown","80357ce8":"markdown","6cacc4b3":"markdown","08a27dcb":"markdown","964fd7bc":"markdown","b41a00ca":"markdown"},"source":{"dbc74b26":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn import metrics\nimport sys\nif not sys.warnoptions:\n    import warnings\n    warnings.simplefilter(\"ignore\")\nimport plotly as py\nimport plotly.graph_objs as go\npy.offline.init_notebook_mode(connected = True)\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff","d824690f":"diamonds = pd.read_csv(\"\/kaggle\/input\/diamonds\/diamonds.csv\")\n","100dff46":"diamonds.info()","5bfb550f":"sns.lmplot(y=\"carat\", x=\"price\", hue=\"clarity\", data= diamonds, fit_reg= False)","b2316bfe":"sns.lmplot(y=\"carat\", x=\"price\", hue=\"cut\", data= diamonds, fit_reg= False)","ab67b1c1":"sns.set(style = \"whitegrid\", font_scale = 1.5)\n\nf, axes = plt.subplots(3, figsize = (8,16))\n\nsns.countplot(y = \"clarity\", data = diamonds, ax = axes[0])\n\nsns.countplot(y = \"color\", data = diamonds, ax = axes[1])\n\nsns.countplot(y = \"cut\", data = diamonds, ax = axes[2])\n\nplt.tight_layout()","7e375352":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\ncut = diamonds.iloc[:,1:2]\ncolor = diamonds.iloc[:,2:3]\nclarity = diamonds.iloc[:,3:4]\ncut = ohe.fit_transform(cut).toarray()\ncolor = ohe.fit_transform(color).toarray()\nclarity = ohe.fit_transform(clarity).toarray()\n\ndiamonds.drop(columns = ['cut', 'color', 'clarity'], inplace = True)\n\ncut = pd.DataFrame(cut)\ncolor = pd.DataFrame(color)\nclarity = pd.DataFrame(clarity)\n\ndiamonds = pd.concat([diamonds, cut, color, clarity], axis = 1)\n\nX = diamonds.drop(columns = 'price').values\nY = diamonds.iloc[:,3:4].values\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(X, Y,test_size=0.2, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.fit_transform(x_test)","3c8c8dc5":"import statsmodels.api as sm \n\nX_l = diamonds.iloc[:,[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25]\n].values\nr_ols = sm.OLS(endog = diamonds.iloc[:,-1:], exog =X_l).fit()\n\nprint(r_ols.summary())","3d9e1ded":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(x_train,y_train)\ny_pred_linReg = lin_reg.predict(x_test)\n\nprint(r2_score(y_test, y_pred_linReg))\n\nprint('coef', lin_reg.coef_,'\\n\\n')\n\nprint('intercept', lin_reg.intercept_)","bbf6aa96":"\nfrom sklearn.preprocessing import PolynomialFeatures\npoly_reg = PolynomialFeatures(degree = 1) \nx_poly = poly_reg.fit_transform(x_train) \nx_poly2 = poly_reg.fit_transform(x_test) \nlin_reg = LinearRegression()\nlin_reg.fit(x_poly, y_train)\ny_pred_poly = lin_reg.predict(x_poly2)\n\nprint(r2_score(y_test, y_pred_poly))","9fb022c2":"from sklearn.svm import SVR\nsvr_reg = SVR(kernel = 'linear')\nsvr_reg.fit(X_train, y_train)\ny_pred_svr = svr_reg.predict(X_test)\n\nprint(r2_score(y_test, y_pred_svr))","310b5274":"from sklearn.tree import DecisionTreeRegressor\ndt_reg = DecisionTreeRegressor(random_state = 0)\ndt_reg.fit(X_train,y_train)\ny_pred_dt = dt_reg.predict(X_test)\n\nprint(r2_score(y_test, y_pred_dt))","4c983e64":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(random_state = 0, n_estimators = 100)\nrf_reg.fit(X_train, y_train)\ny_pred_rf = rf_reg.predict(X_test)\n\nprint(r2_score(y_test, y_pred_rf))","5a907db6":"from xgboost import XGBRegressor\nxgb = XGBRegressor(n_estimators = 100)\nxgb.fit(x_train, y_train)\ny_pred_xgb = xgb.predict(x_test)\n\nprint(r2_score(y_test, y_pred_xgb))","8202175d":"iris = sns.load_dataset('iris')","239f0492":"iris.head()","581815d6":"iris.info()","81918665":"for n in range(0,150):\n    if iris['species'][n] == 'setosa':\n        plt.scatter(iris['sepal_length'][n], iris['sepal_width'][n], color = 'red')\n        plt.xlabel('sepal_length')\n        plt.ylabel('sepal_width')\n    elif iris['species'][n] == 'versicolor':\n        plt.scatter(iris['sepal_length'][n], iris['sepal_width'][n], color = 'blue')\n        plt.xlabel('sepal_length')\n        plt.ylabel('sepal_width')\n    elif iris['species'][n] == 'virginica':\n        plt.scatter(iris['sepal_length'][n], iris['sepal_width'][n], color = 'green')\n        plt.xlabel('sepal_length')\n        plt.ylabel('sepal_width')","61b8642a":"sns.lmplot(x = 'sepal_length', y = 'sepal_width', data = iris, hue = 'species', col = 'species')","d9716d5b":"X = iris.iloc[:,:4].values\ny = iris.iloc[:,4:5].values\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)\n\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test = sc.fit_transform(x_test)","7b231390":"def model_evaluate(model, test):\n    y_pred = model.predict(test)\n    print(classification_report(y_test, y_pred))\n    cm = confusion_matrix(y_test, y_pred)\n\n    categories = ['Setosa', 'Versicolor', 'Virginica']\n    \n    sns.heatmap(cm, cmap = 'Blues', fmt = '', annot = True,\n                xticklabels = categories, yticklabels = categories)\n\n    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n    plt.ylabel(\"Actual values\"   , fontdict = {'size':14}, labelpad = 10)\n    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)","0709261e":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","41995116":"from sklearn.svm import SVC\nmodel = SVC(kernel = 'linear') #kernel = poly, rbf, precomputed\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","3d8a1705":"from sklearn.naive_bayes import GaussianNB\nmodel = GaussianNB()\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","e904e38c":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(x_train, y_train)\n\nmodel_evaluate(model, x_test)","bd841c21":"from sklearn.naive_bayes import ComplementNB\nmodel = ComplementNB()\nmodel.fit(x_train, y_train)\n\nmodel_evaluate(model, x_test)","ec1fca5a":"from sklearn.naive_bayes import BernoulliNB\nmodel = BernoulliNB()\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","16b0cf0f":"from sklearn.naive_bayes import CategoricalNB\nmodel = CategoricalNB()\nmodel.fit(x_train, y_train)\n\nmodel_evaluate(model, x_test)","4e8454c2":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier(n_neighbors = 3, metric = 'minkowski')\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","98f705b6":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(random_state = 0)\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","7a3ebd6e":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators = 10, criterion = 'entropy')\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","d2abfc04":"from sklearn.ensemble import AdaBoostClassifier\nmodel = AdaBoostClassifier(n_estimators = 50)\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","a091244b":"from xgboost import XGBClassifier\nmodel = XGBClassifier(n_estimators = 100)\nmodel.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","60765715":"!pip install interpret","627ce49f":"from interpret.glassbox import ExplainableBoostingClassifier\nebm = ExplainableBoostingClassifier()\nebm.fit(X_train, y_train)\n\nmodel_evaluate(model, X_test)","1cb683b2":"mall_customers = pd.read_csv('\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')","400037cd":"mall_customers.head()","65fd6fbb":"mall_customers.info()","19a39917":"mall_customers.describe()","ac7542e5":"lab = mall_customers[\"Gender\"].value_counts().keys().tolist()\nval = mall_customers[\"Gender\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  [ 'royalblue' ,'lime'],\n                             line = dict(color = \"white\",\n                                         width =  1.3)\n                            ),\n               rotation = 20,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Customer attrition in data\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)","7f6318ce":"sns.set(style=\"darkgrid\",font_scale=1.5)\nf, axes = plt.subplots(1,3,figsize=(20,8))\nsns.distplot(mall_customers[\"Age\"], ax = axes[0], color = 'y')     \nsns.distplot(mall_customers[\"Annual Income (k$)\"], ax = axes[1], color = 'g')\nsns.distplot(mall_customers[\"Spending Score (1-100)\"],ax = axes[2], color = 'r')\nplt.tight_layout()","8bf22fab":"dz=ff.create_table(mall_customers.groupby('Gender').mean())\npy.iplot(dz)","71fb3da6":"plt.figure(figsize=(8,4))\nsns.heatmap(mall_customers.corr(),annot=True,cmap=sns.cubehelix_palette(light=1, as_cmap=True),fmt='.2f',linewidths=2)\nplt.show()","804d7c25":"x = mall_customers.iloc[:,2:]\nprint(x.head())\nx = x.values","4fedadef":"kMeans = KMeans(n_clusters = 3, init = 'k-means++')\ny_pred = kMeans.fit_predict(x)\nprint('Pred:\\n', y_pred)\nprint('\\n\\ninertia: ', kMeans.inertia_, '\\n\\nclusters centers:\\n', kMeans.cluster_centers_)","b2e03e7d":"result = []\nfor i in range(1, 12):\n    kMeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 123)\n    kMeans.fit(x)        \n    result.append(kMeans.inertia_)\n\n\nplt.plot(range(1,12), result)\nplt.title('WCSS')\nplt.show()","f88a015e":"kMeans = KMeans(n_clusters = 6, init = 'k-means++') \ny_pred_kMeans = kMeans.fit_predict(x)\nprint('Pred:\\n', y_pred_kMeans)\nprint('\\n\\ninertia: ', kMeans.inertia_, '\\n\\nclusters centers:\\n', kMeans.cluster_centers_)","e3e94b54":"agglomerative = AgglomerativeClustering(n_clusters = 6, affinity = 'euclidean', linkage = 'ward')\ny_pred_agg = agglomerative.fit_predict(x)\nprint('Pred:\\n', y_pred_agg)","3b4d3732":"import scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(x, method = 'ward'))\nplt.show()","8ca2de02":"f, (ax1, ax2) = plt.subplots(1, 2, sharey='col', num = 10, figsize = (15,5))\n\nax1.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = mall_customers , c = y_pred_kMeans,s = 100)\nax1.title.set_text('KMeans')\n\nax2.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = mall_customers , c = y_pred_agg,s = 100)\nax2.title.set_text('Agglomerative')\nf.show()","171763fe":"x = mall_customers.iloc[:,3:].values\n","9390d508":"kMeans = KMeans(n_clusters = 6, init = 'k-means++') \ny_pred_kMeans = kMeans.fit_predict(x)\nprint('Pred:\\n', y_pred_kMeans)\nprint('\\n\\ninertia: ', kMeans.inertia_, '\\n\\nclusters centers:\\n', kMeans.cluster_centers_)\n\nresult = []\nfor i in range(1, 14):\n    kMeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 123)\n    kMeans.fit(x)        \n    result.append(kMeans.inertia_)\n\n\nplt.plot(range(1,14), result)\nplt.title('WCSS')\nplt.show()","d8deb040":"print('K-Means')\nkMeans = KMeans(n_clusters = 5, init = 'k-means++') \ny_pred_kMeans = kMeans.fit_predict(x)\nprint('Pred:\\n', y_pred_kMeans)\nprint('\\n\\ninertia: ', kMeans.inertia_, '\\n\\nclusters centers:\\n', kMeans.cluster_centers_)\n\nprint('\\n\\nAgglomerative')\nagglomerative = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_pred_agg = agglomerative.fit_predict(x)\nprint('Pred:\\n', y_pred_agg)","f2fcb6d5":"f, (ax1, ax2) = plt.subplots(1, 2, sharey='col', num = 10, figsize = (15,5))\n\nax1.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = mall_customers , c = y_pred_kMeans,s = 100)\nax1.title.set_text('K-Means')\nax2.scatter( x = 'Annual Income (k$)' ,y = 'Spending Score (1-100)' , data = mall_customers , c = y_pred_agg,s = 100)\nax2.title.set_text('Agglomerative')\nf.show()\n","2a71c62a":"## KNeighbors Classifier","bcdb0466":"## Other\n\n* **XGBClassifier**","30fda77c":"## Preprocessing","7b75bb37":"# CLASSIFICATION","383baa0e":"## Support Vector Regression","a6a1c913":"# OLS Regression Results","d096f3ee":"## Naive Bayes\n\n* **Gaussian Naive Bayes**","1039aba5":"## Polynomial Regression","6912a136":"## Modelling","82953933":"## Random Forest Classifier","7482f380":"## Linear Regression","960d4adc":"## Support Vector Classifier","a2dea8f9":"# CLUSTERING","f23fbd44":"* **Multinomial Naive Bayes**","a65a0b1d":"## XGBRegressor","49a66a83":"# REGRESSION","3e8ebcb4":"* **Bernoulli Naive Bayes**","f2fd2a88":"**throwing the age column**","dd712bf0":"## Decision Tree","7986d1d6":"## Hierarchical Clustering","b2e837b2":"Kernel trick: 'rbf', 'poly', 'linear', 'sigmoid', 'precomputed'","73ba378d":"## Decision Tree Classifier","899df04c":"## Logistic Regression","c8595a3f":"## AdaBoost","80357ce8":"* **Complement Naive Bayes**","6cacc4b3":"* **Categorical Naive Bayes**","08a27dcb":"**Significance Level: 0.05 we do not screen because there is no column exceeding this value.**","964fd7bc":"* **ExplainableBoostingClassifier**","b41a00ca":"## Random Forest"}}