{"cell_type":{"6257a577":"code","1bca626e":"code","a3fcd638":"code","db6cfc53":"code","c5fdb1ec":"code","fdb83222":"code","a571e6b6":"code","6d8c7414":"code","1b7c1795":"code","6ff26f60":"code","d6d46359":"markdown","2300ef99":"markdown","fd30b9c3":"markdown","37a8626b":"markdown","c831df94":"markdown","7154bbb9":"markdown","4d1b9a81":"markdown","b2b98a40":"markdown","0887fb32":"markdown","18c996ca":"markdown","13debdee":"markdown"},"source":{"6257a577":"import numpy as np\nimport pandas as pd\n\nimport os\nimport sys\n\neffnet_base_path = '..\/input\/efficientnet-pytorch\/EfficientNet-PyTorch\/EfficientNet-PyTorch-master'\n\nsys.path = [effnet_base_path,] + sys.path\n\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nfrom efficientnet_pytorch import model as enet","1bca626e":"class ClassificationDataset:\n    \n    def __init__(self, image_paths, targets): \n        self.image_paths = image_paths\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.image_paths)\n    \n    def __getitem__(self, item):      \n        image = np.load(self.image_paths[item]).astype(float)\n\n        targets = self.targets[item]\n                \n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"targets\": torch.tensor(targets, dtype=torch.long),\n        }","a3fcd638":"df = pd.read_csv('..\/input\/seti-breakthrough-listen\/train_labels.csv')\ndf['img_path'] = df['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/train\/{x[0]}\/{x}.npy')","db6cfc53":"class enetv2(nn.Module):\n    def __init__(self, backbone, out_dim):\n        super(enetv2, self).__init__()\n        self.enet = enet.EfficientNet.from_name(backbone)\n        self.enet.load_state_dict(torch.load(pretrained_models[backbone]))\n        self.myfc = nn.Linear(self.enet._fc.in_features, out_dim)\n        self.enet._fc = nn.Identity()\n        self.conv1 = nn.Conv2d(6, 3, kernel_size=3, stride=1, padding=3, bias=False)\n\n    def extract(self, x):\n        return self.enet(x)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.extract(x)\n        x = self.myfc(x)\n        return x","c5fdb1ec":"pretrained_models = {\n    'efficientnet-b0': '..\/input\/efficientnet-pytorch\/efficientnet-b0-08094119.pth',\n    'efficientnet-b1': '..\/input\/efficientnet-pytorch\/efficientnet-b1-dbc7070a.pth',\n    'efficientnet-b2': '..\/input\/efficientnet-pytorch\/efficientnet-b2-27687264.pth',\n    'efficientnet-b3': '..\/input\/efficientnet-pytorch\/efficientnet-b3-c8376fa2.pth',\n    'efficientnet-b4': '..\/input\/efficientnet-pytorch\/efficientnet-b4-e116e8b3.pth',\n    'efficientnet-b5': '..\/input\/efficientnet-pytorch\/efficientnet-b5-586e6cc6.pth',\n    'efficientnet-b6': '..\/input\/efficientnet-pytorch\/efficientnet-b6-c76e70fd.pth',\n}\n\npretrained_weights = {\n    'efficientnet-b0': '..\/input\/seti-effnet\/efficientnet-b0-4.pt',\n    'efficientnet-b1': '..\/input\/seti-effnet\/efficientnet-b1-4.pt',\n    'efficientnet-b2': '..\/input\/seti-effnet\/efficientnet-b2-4.pt',\n    'efficientnet-b3': '..\/input\/seti-effnet\/efficientnet-b3-4.pt',\n    'efficientnet-b4': '..\/input\/seti-effnet\/efficientnet-b4-4.pt',\n    'efficientnet-b5': '..\/input\/seti-effnet\/efficientnet-b5-4.pt',\n    'efficientnet-b6': '..\/input\/seti-effnet\/efficientnet-b6-4.pt',\n}","fdb83222":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nmodels = []\nfor key in pretrained_models.keys():\n    model = enetv2(key, out_dim=1)\n    model.load_state_dict(torch.load(pretrained_weights[key], map_location=torch.device(device)))\n    models.append(model.to(device))","a571e6b6":"def predict(data_loader, model, device):\n    model.eval()\n    \n    final_targets = []\n    final_outputs = []\n    \n    with torch.no_grad():\n        \n        for data in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            inputs = data[\"image\"]\n            targets = data[\"targets\"]\n            inputs = inputs.to(device, dtype=torch.float)\n            targets = targets.to(device, dtype=torch.float)\n            \n            output = model(inputs)\n            \n            targets = targets.detach().cpu().numpy().tolist()\n            output = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets","6d8c7414":"submission = pd.read_csv('..\/input\/seti-breakthrough-listen\/sample_submission.csv')\nsubmission['img_path'] = submission['id'].apply(lambda x: f'..\/input\/seti-breakthrough-listen\/test\/{x[0]}\/{x}.npy')\n\ntest_dataset = ClassificationDataset(image_paths=submission.img_path.values, targets=submission.target.values)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)","1b7c1795":"sig = torch.nn.Sigmoid()\nouts = []\nfor model in models:\n    predictions, valid_targets = predict(test_loader, model, device=device)\n    predictions = np.array(predictions)[:, 0]\n    out = sig(torch.from_numpy(predictions))\n    out = out.detach().numpy()\n    outs.append(out)","6ff26f60":"# take the mean of all predictions\npred = np.mean(np.array(outs), axis=0)\n\nsubmission.target = pred\nsubmission.drop(['img_path'], axis=1, inplace=True)\nsubmission.to_csv('submission.csv', index=False)","d6d46359":"## Predictions for each model","2300ef99":"<h1 id=\"dataset\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","fd30b9c3":"<h1 id=\"references\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>References\n        <a class=\"anchor-link\" href=\"#references\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","37a8626b":"[Byfone training notebook](https:\/\/www.kaggle.com\/byfone\/efficientnetb1-on-seti-breakthrough-listen)","c831df94":"## Weights","7154bbb9":"<h1 id=\"load\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>Load Models\n        <a class=\"anchor-link\" href=\"#load\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","4d1b9a81":"## Load Models","b2b98a40":"<h1 id=\"model\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>Model\n        <a class=\"anchor-link\" href=\"#model\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","0887fb32":"## Prepare submission dataset","18c996ca":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/23652\/logos\/header.png\" \/>\n<\/div>","13debdee":"<h1 id=\"predict\" style=\"color:#fefd1c; background:#0e1b1f; border:1px dotted #fefd1c;\"> \n    <center>Predictions\n        <a class=\"anchor-link\" href=\"#predict\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}