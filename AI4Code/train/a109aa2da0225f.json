{"cell_type":{"276bd978":"code","cc5502a9":"code","9e38269a":"code","a0d84e55":"code","5c8e7fd4":"code","f77bcaf9":"code","e20fb05d":"code","77316d94":"code","bada0ae7":"code","1b99384c":"code","c7044d2c":"code","5e14ff7d":"code","fa507a6b":"code","104f6dd8":"code","bf40c422":"code","4211c343":"code","ba25512d":"code","73f27032":"code","971663a8":"code","b6410fc2":"code","59b34ec5":"code","dce3a585":"code","9b78c59d":"code","1030c9e6":"code","c08a5ca6":"code","62b4bf1f":"code","5d175904":"code","99c900dd":"code","eea8ff15":"code","2c778426":"code","bec52a44":"markdown","c7e7a77c":"markdown","1b400010":"markdown","eddd3718":"markdown","76c1c707":"markdown","c9cca66e":"markdown","efb2ac6a":"markdown","6206a64a":"markdown","ae0f1660":"markdown","519d8873":"markdown","bf8a021c":"markdown","085d2445":"markdown","d9c85239":"markdown","5f9f2400":"markdown","73dff975":"markdown","e84c2d19":"markdown","d5ec874e":"markdown"},"source":{"276bd978":"import jax.numpy as jnp\nfrom jax import grad\nfrom jax import vmap # for auto-vectorizing functions\nfrom functools import partial # for use with vmap\nfrom jax import jit # for compiling functions for speedup\nfrom jax.experimental import stax # neural network library\nfrom jax.experimental.stax import Conv, Dense, MaxPool, Relu, Flatten, LogSoftmax # neural network layers\nimport matplotlib.pyplot as plt # visualization\nfrom jax import random\nimport numpy as np\nfrom jax.experimental import optimizers\nfrom jax.tree_util import tree_multimap # Element-wise manipulation of collections of numpy arrays","cc5502a9":"'''\nTask Description:\n- Init: num_sample per task\n- Training set initialization\n- Test_set initialization\n- Plot function\n'''\n# Task\nclass Task:\n    def __init__(self, num_sample = 10):\n        self.size = num_sample\n        self.train = None\n    \n    def f(self, x):\n        pass\n        \n    def training_set(self, force_new = False):\n        if self.train is None and force_new == False:\n            self.train = np.random.uniform(-5., 5., self.size)\n            x = self.train\n        elif force_new == False:\n            x = self.train\n        else:\n            x = jnp.random.uniform(-5., 5., self.size)\n        \n        y = self.f(x)\n        return x.reshape((-1, 1)), y.reshape((-1, 1))\n                                             \n    def test_set(self, test_size = 50):\n        x = np.linspace(-5., 5., test_size)\n        y = self.f(x)\n        return x.reshape((-1, 1)), y.reshape((-1, 1))\n    \n    def plot(self, *args, **kwargs):\n        x_test, y_test = self.test_set()\n        plt.plot(x_test.reshape(-1), y_test.reshape(-1), *args, **kwargs) \n\n# Quadratic task\nclass QuadraticTask(Task):\n    def __init__(self, num_sample = 10, a = None, b = None, c = None):\n        super().__init__(num_sample)\n        self.a = self.setup(a)\n        self.b = self.setup(b)\n        self.c = self.setup(c)\n    \n    def f(self, x):\n        return self.a*(x**2) + self.b*x + self.c\n    \n    def setup(self, x, low = 0., hi = 10.):\n        if x is None:\n            return np.random.uniform(low, hi)\n        else:\n            return x                                     \n# Sinewave Task  \nclass SineWaveTask(Task):\n    def __init__(self, num_sample = 20, a = None, b = None):\n        super().__init__(num_sample)\n        self.a = self.setup(a, 1., 5.)\n        self.b = self.setup(b, 0, 2 * np.pi)\n    \n    def f(self, x):\n        return self.a * jnp.sin(x + self.b)\n    \n    def setup(self, x, low = 0., hi = 10.):\n        if x is None:\n            return np.random.uniform(low, hi)\n        else:\n            return x  \n        \n        ","9e38269a":"x, y = QuadraticTask(num_sample = 20).training_set()\nx.shape","a0d84e55":"QuadraticTask().plot()\nSineWaveTask().plot()\nSineWaveTask().plot()","5c8e7fd4":"NUM_TRAIN_TASKS = 20000\nNUM_TEST_TASKS = 100","f77bcaf9":"some_sample = SineWaveTask()\nx, y = some_sample.training_set()\nprint('---------------x for training set:--------------')\nprint(x.shape)\nprint(x)\nprint('------------------------------------------------')\nprint('---------------y for training set:--------------')\nprint(y.shape)\nprint(y)\nprint('------------------------------------------------')","e20fb05d":"NUM_TRAIN_TASKS = 20000\nNUM_TEST_TASKS = 100","77316d94":"# Auto \ntask_type = SineWaveTask\nTRAIN_TASKS = [task_type() for _ in range(NUM_TRAIN_TASKS)]\nTEST_TASKS = [task_type() for _ in range(NUM_TEST_TASKS)]\n#Custom","bada0ae7":"from jax import random\nkey = random.PRNGKey(0)","1b99384c":"# Use stax to set up network initialization and evaluation functions\nnet_init, net_apply = stax.serial(\n    Dense(40), Relu,\n    Dense(40), Relu,\n    Dense(1)\n)\n\nin_shape = (-1, 1,)\nout_shape, net_params = net_init(key, in_shape)\n\n# Define optimizer\nopt_init, opt_update, get_params = optimizers.adam(step_size=1e-3)\nopt_state = opt_init(net_params)","c7044d2c":"ALPHA = 0.01\nBETA = 1e-4","5e14ff7d":"def loss(params, inputs, targets):\n    # Computes average loss for the batch\n    predictions = net_apply(params, inputs)\n    return np.mean((targets - predictions)**2)","fa507a6b":"def inner_update(params, alpha, x1, y1):\n    '''\n    input:\n    - params: model's parameters\n    - inputs\n    - targets: true label\n    output\n    - updated parameters\n    '''\n    grads = grad(loss)(params, x1, y1)\n    grad_update_fn = lambda g, state: (state - alpha * g)\n    return tree_multimap(grad_update_fn, grads, params)\n\ndef maml_loss(params, alpha, x1, y1, x2, y2):\n    '''\n    input:\n    - params: model's parameters\n    - x1, y1: task's train set\n    - x2, y2: task's test set\n    output:\n    - Loss after update parameters 1 time on the test set.\n    '''\n    params_updated = inner_update(params, alpha, x1, y1)\n    return loss(params_updated, x2, y2)\n    ","104f6dd8":"from tqdm import tqdm\n#Define a step (using jit to improve speed)\n@jit\ndef step(i, opt_state, alpha, x1, y1, x2, y2):\n    '''\n    input:\n    - step number, opt_state (contains params)\n    -x1, y1: train, x2, y2: test and get loss\n    output:\n    - new opt_state and loss\n    '''\n    # Get params from opt_state\n    p = get_params(opt_state)\n    # calculate gradient from maml_loss\n    g = grad(maml_loss)(p, alpha, x1, y1, x2, y2)\n    g_alpha = grad(maml_loss, 1)(p, alpha, x1, y1, x2, y2)\n    # pre-model update trial on task.\n    l = maml_loss(p, alpha, x1, y1, x2, y2)\n    alpha = alpha - BETA * g_alpha\n    return opt_update(i, g, opt_state), alpha, l\n\nmaml_losses = []\n\n# Run the originally created model on all the training tasks, train on the support set, \n# test and get loss on original model on the query set.\n\nfor i in tqdm(range(len(TRAIN_TASKS))):\n    # get x_support, y_support, x_query, y_query\n    \n    x_support, y_support = TRAIN_TASKS[i].training_set()\n    x_query, y_query = TRAIN_TASKS[i].test_set()\n    opt_state, ALPHA, l = step(i, opt_state, ALPHA, x_support, y_support, x_query, y_query)\n    maml_losses.append(l)\n#     if (i+1) % 1000 == 0:\n#         print('----------------------Training step {}----------------------'.format(i))\n#         print('Size of x_support: ' + str(x_support.shape) + ' and y_support: ' + str(y_support.shape))\n#         print('Size of x_query: ' + str(x_query.shape) + ' and y_query: ' + str(y_query.shape))\n#         print('LOSS: ' + str(l))\n#         print('------------------------------------------------------------')\n    \nnet_params = get_params(opt_state)","bf40c422":"plt.plot(np.arange(len(TRAIN_TASKS)), np.array(maml_losses))","4211c343":"maml_losses[-10:]","ba25512d":"# we will take TEST_TASKS[0] for trial.\n\nx_support, y_support = TEST_TASKS[23].training_set()\nx_query, y_query = TEST_TASKS[23].test_set()\n\n# y_first_prediction = net_apply(net_params, x_query)\nparams = net_params.copy()\n# Define sample optimizer\nplt.plot(x_query, y_query, label = 'real')\nepochs = 2\nfor i in range(epochs):\n    # normal gradient descent step\n    params = inner_update(params, ALPHA, x_support, y_support)\n    y_predict = None\n    if i == 0 or (i+1)%epochs == 0:\n        y_predict = net_apply(params, x_query)  \n        plt.plot(x_query, y_predict, label = 'prediction after {} steps'.format(i+1))\nprint(loss(params, x_query, y_query))\nplt.legend()","73f27032":"epochs= 4\n\n# Define sample optimizer\nresults = []\ntest_losses = []\n\nfor i in tqdm(range(len(TEST_TASKS))):\n    params = net_params.copy()\n    x_support, y_support = TEST_TASKS[i].training_set()\n    x_query, y_query = TEST_TASKS[i].test_set()\n    for i in range(epochs):\n        params = inner_update(params, ALPHA, x_support, y_support)\n    y_prediction = net_apply(params, x_query)\n    results.append((x_query, y_prediction, y_query))\n    test_loss = loss(params, x_query, y_query)\n    test_losses.append(test_loss)\n\nprint('Loss on the test set is {}'.format(np.mean(test_losses)))\n    ","971663a8":"# plot a test task to view accuracy\n# index of test task\nidx = 60\nx_query, y_predict, y_query = results[idx][0],results[idx][1],results[idx][2]\nplt.plot(x_query, y_predict, label = 'prediction')\nplt.plot(x_query, y_query, label = 'groundtruth')\nplt.legend()","b6410fc2":"np.argmax(test_losses)","59b34ec5":"#Define batch size\nBATCH_SIZE = 4","dce3a585":"@jit\ndef maml_loss_batch(params, alpha, x1_b, y1_b, x2_b, y2_b):\n    '''\n    input:\n    - params\n    - x1_b, y1_b, x2_b, y2_b: batches of sample task \n    output:\n    - combined loss of the batch\n    '''\n    return np.mean(vmap(partial(maml_loss, params, alpha))(x1_b, y1_b, x2_b, y2_b))\n    \n    \n    ","9b78c59d":"@jit\ndef batch_step(i, opt_state, alpha, x1_b, y1_b, x2_b, y2_b):\n    p = get_params(opt_state)\n    g = grad(maml_loss_batch)(p, alpha, x1_b, y1_b, x2_b, y2_b)\n    g_alpha = grad(maml_loss_batch, 1)(p, alpha, x1_b, y1_b, x2_b, y2_b)\n    l = maml_loss_batch(p, alpha, x1_b, y1_b, x2_b, y2_b)\n    alpha = alpha - BETA*g_alpha\n    return opt_update(i, g, opt_state), alpha, l","1030c9e6":"# Use stax to set up network initialization and evaluation functions\nnet_init, net_apply = stax.serial(\n    Dense(40), Relu,\n    Dense(40), Relu,\n    Dense(1)\n)\n\nin_shape = (-1, 1,)\nout_shape, net_params = net_init(key, in_shape)\n\n# Define optimizer\nopt_init, opt_update, get_params = optimizers.adam(step_size=BETA)\nopt_state = opt_init(net_params)","c08a5ca6":"counter = 0\nstart_idx = 0\nend_idx = 0\ntraining_losses = []\nALPHA = 0.01\nwhile True:\n    start_idx = end_idx\n    end_idx = start_idx + BATCH_SIZE\n    tasks = TRAIN_TASKS[start_idx:end_idx]\n    x_support_batch = []\n    y_support_batch = []\n    x_query_batch = []\n    y_query_batch = []\n    for task in tasks:\n        x_support, y_support = task.training_set()\n        x_query, y_query = task.test_set()\n        x_support_batch.append(x_support)\n        y_support_batch.append(y_support)\n        x_query_batch.append(x_query)\n        y_query_batch.append(y_query)\n    x_support_batch = jnp.stack(x_support_batch)\n    y_support_batch = jnp.stack(y_support_batch)\n    x_query_batch = jnp.stack(x_query_batch)\n    y_query_batch = jnp.stack(y_query_batch)\n    \n    opt_state,ALPHA, loss_batch = batch_step(counter, opt_state, ALPHA, x_support_batch, y_support_batch, x_query_batch, y_query_batch)\n    training_losses.append(loss_batch)\n    \n    counter += 1\n    \n    if end_idx >= len(TRAIN_TASKS):\n        break\n    ","62b4bf1f":"net_params = get_params(opt_state)","5d175904":"plt.plot(np.arange(counter), training_losses)","99c900dd":"np.mean(np.array(training_losses))","eea8ff15":"# we will take TEST_TASKS[0] for trial.\n\nx_support, y_support = TEST_TASKS[23].training_set()\nx_query, y_query = TEST_TASKS[23].test_set()\n\n# y_first_prediction = net_apply(net_params, x_query)\nparams = net_params.copy()\n# Define sample optimizer\nplt.plot(x_query, y_query, label = 'real')\nepochs = 10\nfor i in range(epochs):\n    # normal gradient descent step\n    params = inner_update(params, ALPHA, x_support, y_support)\n    y_predict = None\n    if i == 0 or (i+1)%epochs == 0:\n        y_predict = net_apply(params, x_query)  \n        plt.plot(x_query, y_predict, label = 'prediction after {} steps'.format(i+1))\nprint(loss(params, x_query, y_query))\nplt.legend()","2c778426":"epochs= 10\n\n# Define sample optimizer\nresults = []\ntest_losses = []\n\nfor i in tqdm(range(len(TEST_TASKS))):\n    params = net_params.copy()\n    x_support, y_support = TEST_TASKS[i].training_set()\n    x_query, y_query = TEST_TASKS[i].test_set()\n    for i in range(epochs):\n        params = inner_update(params, ALPHA, x_support, y_support)\n    y_prediction = net_apply(params, x_query)\n    results.append((x_query, y_prediction, y_query))\n    test_loss = loss(params, x_query, y_query)\n    test_losses.append(test_loss)\n\nprint('Loss on the test set is {}'.format(np.mean(test_losses)))","bec52a44":"Define Neural Network","c7e7a77c":"Run and test on different tasks","1b400010":"**2. Task initialization**","eddd3718":"unit test","76c1c707":"Important notes:\nIn inner_update() function:\n- For SineWaveTask, alpha = 0.01\n- For quadratic Task, alpha = 0.0001\n\nOtherwise try tuning alpha (that's the key)","c9cca66e":"- Define Neural Network\n- Define Loss (mse)\n- Define MAML loss\n- Run model on training task but only on train set\n- Test model on test tasks => Get loss => Backprop back to the original model\n","efb2ac6a":"Reuse some of the code in the previous section, we will implement MAML with batch.\n\nTo do:\n* Rewrite MAML loss when using batch.\n* Rewrite batch step\n* Training procedure in batch.\n* Testing procedure in batch.\n\nfor batch in batches:\n  grads = grad(maml_loss)(params, x1_b, y1_b, x2_b, y2_b)\n  opt_state = step(i, opt_state, grads)","6206a64a":"Define Loss","ae0f1660":"**1. Library Import**","519d8873":"Test set trial:","bf8a021c":"**5. MAML set up (with Batch)**","085d2445":"Trial on a test task","d9c85239":"**4. Set up MAML (with no batch)**","5f9f2400":"# MAML implementation with JAX","73dff975":"Define MAML Loss","e84c2d19":"**3. Training Task and Testing Task split**","d5ec874e":"Initialize a brand new model"}}