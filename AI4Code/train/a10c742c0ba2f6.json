{"cell_type":{"5be64fad":"code","6f882a12":"code","d393a090":"code","98386b31":"code","73551dd6":"code","133bdde1":"code","fa173e3f":"code","ed902779":"code","ac7809c9":"code","0cb79e90":"code","f46606b5":"code","86c8a609":"code","d02bd5c6":"code","d694292f":"code","16f9bd19":"code","2323ebb6":"code","ed583b5a":"code","4d12c4e4":"code","faea6dda":"code","b249c710":"code","623ddfe3":"code","2cddd8bc":"code","0a6fbb30":"code","564d11bb":"code","61314277":"code","800a7287":"code","2c73896f":"code","4ad6ba57":"code","3692e5ed":"code","87f9060d":"code","67eccf5d":"code","21af9ce8":"code","d6657a00":"code","2e4a455c":"code","150a4e0e":"code","7f9a3715":"code","848b02db":"code","bea5e523":"code","3f078377":"code","2592bf87":"code","efd4f583":"code","b9164641":"code","83cb4055":"code","b0b5cee3":"code","1def38ae":"code","0ed01a4c":"code","567dcf79":"code","bd0f8100":"code","dd6bb661":"code","db130970":"code","7fb6db3d":"code","6255bc62":"code","c465eb8d":"code","8087f45d":"code","e29dfdff":"code","dcf86682":"code","d704e3b8":"code","22b761c2":"code","fe25efe0":"code","9fd9fbed":"code","e9d6ce3f":"code","e601c9d6":"code","39c39424":"code","ee057420":"code","8c871fed":"code","b0c59260":"code","dc242c8f":"code","09c2f559":"code","d0f369a1":"code","00a09ea0":"code","97e61e9b":"code","7f4832d1":"code","f6e02124":"code","2a9956fe":"code","1e233419":"code","c57ca7d5":"code","a8ec44b5":"code","08c7dbac":"code","64d0fb51":"code","e757f21a":"code","a71f1d48":"code","bebff103":"code","3e305889":"code","87bc776a":"code","91463931":"code","be3b226b":"code","ae57f189":"code","038fdd26":"code","379dc3ab":"code","13751e76":"code","3487aa47":"code","51ab9e7f":"code","66dc0acc":"code","3411996e":"code","e1f09b1a":"code","207a5619":"markdown","e72c6c3f":"markdown","db9d4d9a":"markdown","2c1fe8df":"markdown","b5100320":"markdown","25a0ba57":"markdown","14faaf0c":"markdown","3ada520a":"markdown","cf62510a":"markdown","90b78947":"markdown","f0d49cb1":"markdown","927ffc72":"markdown","3d5db7b7":"markdown","944007e7":"markdown","5c589ac4":"markdown","de138df8":"markdown","88a05e21":"markdown","6b85bbee":"markdown","0ebdd42d":"markdown"},"source":{"5be64fad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f882a12":"import os\nimport gc\nimport matplotlib.pyplot as plt\nimport sklearn\nimport scipy.sparse \nimport lightgbm as lgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline \nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nimport joblib\n\n\nimport lightgbm\nfrom xgboost import XGBRegressor\n\npd.set_option('display.max_rows', 600)\npd.set_option('display.max_columns', 50)\nsns.set(rc={'figure.figsize':(20, 10)})","d393a090":"item_cat = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nsales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv', parse_dates=['date'], infer_datetime_format=True, dayfirst=True)\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ntest = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsample = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')","98386b31":"sales.head(5)","73551dd6":"# some additional preprocessing of the data for easier analysis\nunique_dates = pd.DataFrame({'date': sales['date'].drop_duplicates()})\nunique_dates['date_parsed'] = pd.to_datetime(unique_dates.date, format=\"%d.%m.%Y\")\nunique_dates['day'] = unique_dates['date_parsed'].apply(lambda d: d.day)\nunique_dates['month'] = unique_dates['date_parsed'].apply(lambda d: d.month)\nunique_dates['year'] = unique_dates['date_parsed'].apply(lambda d: d.year)\n\ntrain = sales.merge(unique_dates, on='date').sort_values('date_parsed')","133bdde1":"\ndata = train.groupby(['year', 'month']).agg({'item_cnt_day': np.sum}).reset_index().pivot(index='month', columns='year', values='item_cnt_day')\ndata.plot(figsize=(12, 8))","fa173e3f":"data = train.groupby(['year', 'month', 'day']).agg({'item_cnt_day': np.sum}).unstack('year')\ndata.plot(figsize=(12, 8))","ed902779":"sales.head()\n","ac7809c9":"sales.dtypes\n","0cb79e90":"sales.isnull().sum()\n","f46606b5":"sns.boxplot(x=sales['item_cnt_day']);\n","86c8a609":"sales[sales['item_cnt_day'] > 900]\n","d02bd5c6":"sales[sales['item_id'] == 11373].sort_values(by='item_cnt_day').tail(10)\n","d694292f":"sales = sales[sales['item_cnt_day'] <= 1000]\n","16f9bd19":"sns.boxplot(sales['item_price'])\n","2323ebb6":"sales[sales['item_price']>250000]\n","ed583b5a":"sales[sales['item_id'] == 6066]\n","4d12c4e4":"items[items['item_id']==6066]\n","faea6dda":"item_cat[item_cat['item_category_id'] == 75]\n","b249c710":"sales = sales[sales['item_price'] < 250000]\n","623ddfe3":"items['item_name'].shape[0] == items['item_name'].nunique()\n","2cddd8bc":"shops.shape[0] == shops['shop_name'].nunique()\n","0a6fbb30":"shops.head()\n","564d11bb":"clean_shop_names = [shop_name[1:] if shop_name[0]=='!' else shop_name for shop_name in shops['shop_name']]\nshops['shop_name'] = clean_shop_names","61314277":"\ncity_names = [shop_name.split(' ')[0] for shop_name in clean_shop_names]\nshops['city'] = city_names\nshops['city'] = LabelEncoder().fit_transform(shops['city'])","800a7287":"shops.drop(columns='shop_name', inplace=True)\n","2c73896f":"shops.head()\n","4ad6ba57":"item_cat['sub_category'] = [word.split('-')[1].strip() if len(word.split('-')) > 1 else 'None' for word in item_cat['item_category_name']]\nitem_cat['sub_category'] = LabelEncoder().fit_transform(item_cat['sub_category'])","3692e5ed":"item_cat.drop(columns='item_category_name', inplace=True)\n","87f9060d":"item_cat.head()\n","67eccf5d":"items.drop(columns='item_name', inplace=True)\n","21af9ce8":"sales.head()\n","d6657a00":"test.head()\n","2e4a455c":"from itertools import product\n\ngrid = []\n\nfor month in sales['date_block_num'].unique():\n    \n    shop_ids = sales.loc[sales['date_block_num'] == month, 'shop_id'].unique()\n    item_ids = sales.loc[sales['date_block_num'] == month, 'item_id'].unique()\n    grid.append(np.array(list(product(shop_ids, item_ids, [month]))))","150a4e0e":"col_names = ['shop_id', 'item_id', 'date_block_num']\ngrid_df = pd.DataFrame(np.vstack(grid), columns = col_names)","7f9a3715":"sales_gb = sales.groupby(['date_block_num', 'shop_id', 'item_id'])\n","848b02db":"# it drops the date column automatically for sum and mean\nagg_sales = sales_gb.agg(\n    {\n     'item_cnt_day':[np.sum]\n    }\n).fillna(0).clip(0,20)","bea5e523":"agg_sales.columns = ['target']\n","3f078377":"monthly_sales = pd.merge(grid_df, agg_sales, how='left', on=col_names)\n","2592bf87":"monthly_sales['target'] = monthly_sales['target'].fillna(0).clip(0,20)\n","efd4f583":"test_mod = test[['shop_id', 'item_id']].copy()\ntest_mod['date_block_num'] = 34\ntest_mod['target'] = np.nan\n","b9164641":"\ndata = pd.concat([monthly_sales, \n                  test_mod], axis=0)","83cb4055":"\ndata = pd.merge(data, items, \n                         how='left', on=['item_id'])","b0b5cee3":"data = pd.merge(data, items, \n                         how='left', on=['item_id'])","1def38ae":"\ndata = pd.merge(data, shops, how='left',\n                         on=['shop_id'])","0ed01a4c":"data.tail()","567dcf79":"\ndef add_target_encoding(data, join_on, name, y_name='target'):\n    \n    data_agg = data.groupby(join_on).agg({y_name:['mean']})\n    data_agg.columns = [name]\n\n    return pd.merge(data, data_agg, on=join_on)","bd0f8100":"#month\ndata = add_target_encoding(data,['date_block_num'], 'target_month')\n","dd6bb661":"#month-item\ndata = add_target_encoding(data,['date_block_num', 'item_id'], \n                           'target_month_item')","db130970":"#month-shop\ndata = add_target_encoding(data,['date_block_num', 'shop_id'], \n                           'target_month_shop')","7fb6db3d":"data = add_target_encoding(data,['date_block_num', 'shop_id'], \n                           'target_month_shop_category')\n","6255bc62":"small_int_columns = ['city', 'item_category_id', 'category', 'sub_category',\n                     'date_block_num', 'shop_id']","c465eb8d":"target_columns = [col for col in data.columns if col.startswith('target')]\n","8087f45d":"\nfor col in target_columns:\n    data[col] = data[col].astype(np.float32)","e29dfdff":"\ndel sales, test, monthly_sales, test_mod\ngc.collect()","dcf86682":"\ndef lag_features(df, lags, group_cols, shift_col):\n    \"\"\"\n    Arguments:\n        df (pd.DataFrame)\n        lags (list((int)): the number of months to lag by\n        group_cols (list(str)): the list of columns that need to be the merged key\n        shift_col (str): the column name that is to be shifted by\n    \"\"\"\n\n    for lag in lags:\n        new_col = '{0}_lag_{1}'.format(shift_col, lag)\n        df[new_col] = df.groupby(group_cols)[shift_col].shift(lag)\n\n    return df","d704e3b8":"lags = [1, 2, 3, 6, 12]\ngroup_cols = ['shop_id', 'item_id']\norder_col = 'date_block_num'\n\ndata = data.sort_values(by=group_cols+[order_col], ascending=True)\ndata = lag_features(data, lags, group_cols, 'target')","22b761c2":"data = lag_features(data, lags, group_cols, 'target_month_item')\n","fe25efe0":"data = lag_features(data, lags, group_cols, 'target_month_shop')\n","9fd9fbed":"one_lag_columns = [col for col in target_columns if col not in ['target', 'target_month_item', 'target_month_shop']]\n","e9d6ce3f":"\nfor col in one_lag_columns:\n    data = lag_features(data, [1], group_cols, col)","e601c9d6":"data = data[data['date_block_num'] >= 12]","39c39424":"y = data[['target', 'date_block_num']]\n","ee057420":"data.drop(columns=target_columns, inplace=True)\n","8c871fed":"gc.collect()\n","b0c59260":"data = data.fillna(0)\n","dc242c8f":"X_train = data.loc[data['date_block_num'] <= 32].drop(columns=['date_block_num'])\nX_valid = data.loc[data['date_block_num'] == 33].drop(columns=['date_block_num'])\nX_test = data.loc[data['date_block_num'] == 34].drop(columns=['date_block_num'])\n\ny_train = y.loc[y['date_block_num'] <= 32, 'target'].values\ny_valid = y.loc[y['date_block_num'] == 33, 'target'].values","09c2f559":"lr = LinearRegression()\n","d0f369a1":"\n# remember to scale data when adding regularization\nlr.fit(X_train, y_train)","00a09ea0":"y_valid_predict_lr = lr.predict(X_valid).clip(0, 20)\n","97e61e9b":"np.sqrt(np.mean((y_valid_predict_lr - y_valid.clip(0, 20))**2))\n","7f4832d1":"lgb_params = { 'max_depth':5,\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.03, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 40,\n               'bagging_freq':1,\n               'verbose':0 \n              }","f6e02124":"\ndtrain = lightgbm.Dataset(X_train, label=y_train.clip(0,20))\ndvalid = lightgbm.Dataset(X_valid, label=y_valid.clip(0,20))","2a9956fe":"eval_results = {}\n","1e233419":"model = lightgbm.train(lgb_params,\n                       dtrain,\n                       valid_sets=[dtrain,dvalid],\n                       valid_names=['train', 'valid'],\n                       evals_result=eval_results,\n                       num_boost_round=200,\n                       early_stopping_rounds=10,\n                       verbose_eval=True)\n","c57ca7d5":"y_valid_predict_lgbm = model.predict(X_valid).clip(0, 20)\n","a8ec44b5":"np.sqrt(np.mean((y_valid_predict_lgbm - y_valid.clip(0, 20))**2))\n","08c7dbac":"dates_train = data.loc[data['date_block_num'] <= 32, 'date_block_num']\ndates_valid = data.loc[data['date_block_num'] == 33, 'date_block_num']\ndates_test = data.loc[data['date_block_num'] == 34, 'date_block_num']","64d0fb51":"pred_lr = lr.predict(X_valid.fillna(0)).clip(0,20)\n","e757f21a":"pred_lgb = model.predict(X_valid).clip(0, 20)\n","a71f1d48":"X_valid_level2 = np.c_[pred_lr, pred_lgb]\n","bebff103":"dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]\n\ny_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])].clip(0,20)","3e305889":"X_train_level2 = np.zeros([y_train_level2.shape[0], 2])\ny_train_with_index = pd.Series(data=y_train, index=X_train.index)\n\nfor cur_block_num in [27, 28, 29, 30, 31, 32]:\n    \n    print(cur_block_num)\n    \n    train_indeces = dates_train[dates_train < cur_block_num].index\n    valid_indeces = dates_train[dates_train == cur_block_num].index \n    \n    cur_X_train = X_train.loc[train_indeces]\n    cur_y_train = y_train_with_index.loc[train_indeces]\n    \n    lr.fit(cur_X_train.fillna(0), cur_y_train)\n    model = lightgbm.train(lgb_params, lightgbm.Dataset(cur_X_train, \n                                              label=cur_y_train), 100)\n    \n    cur_X_valid = X_train.loc[valid_indeces]\n    \n     \n    X_train_level2[np.where(dates_train_level2==cur_block_num) , 0] = lr.predict(cur_X_valid.fillna(0)).clip(0,20)\n    X_train_level2[np.where(dates_train_level2==cur_block_num) , 1] = model.predict(cur_X_valid).clip(0,20)","87bc776a":"alphas_to_try = np.linspace(0, 1, 1001)","91463931":"rmse_scores = [np.sqrt(mean_squared_error(y_train_level2, alpha*X_train_level2[:, 0]+(1-alpha)*X_train_level2[:, 1])) for alpha in alphas_to_try]","be3b226b":"best_alpha = alphas_to_try[np.argmin(rmse_scores)]\n","ae57f189":"rmse_train_simple_mix = np.max(rmse_scores)\n","038fdd26":"rmse_train_simple_mix\n","379dc3ab":"test = pd.read_csv(os.path.join('..\/input\/competitive-data-science-predict-future-sales\/', 'test.csv'))\ntest.drop(columns='ID', inplace=True)","13751e76":"X_test_final = pd.merge(test, X_test, on=['shop_id', 'item_id'])\n","3487aa47":"y_test_predict_lr = lr.predict(X_test_final.fillna(0)).clip(0, 20)\n","51ab9e7f":"y_test_predict_lgb = model.predict(X_test_final).clip(0, 20)\n","66dc0acc":"y_final_predict = best_alpha*y_test_predict_lr+(1-best_alpha)*y_test_predict_lgb\n","3411996e":"submission = sample.copy()\n","e1f09b1a":"\nsubmission['item_cnt_month'] = y_final_predict\nsubmission.to_csv('submission_smpl.csv',\n                        index = False)","207a5619":"# Linear Model\n","e72c6c3f":"# City and category inf","db9d4d9a":"# **READING DATA****","2c1fe8df":"# Lags","b5100320":"**Removing data from before month 12 (we only have data for all lags starting from month 13):**","25a0ba57":"**Dropping columns with mean-encoding (to avoid data leakage):**","14faaf0c":"# Any Repeated Entries\n","3ada520a":"# **Visualizations**","cf62510a":"# Total sales by years and month","90b78947":"# Target Mean Encoding","f0d49cb1":"# Memory Management","927ffc72":"# LightGBM","3d5db7b7":"# by day\n","944007e7":"# Target (month-shop-item agreggation)","5c589ac4":"# Monthly Data","de138df8":"# Processing strings\n","88a05e21":"# **IMPORT lIBRARIES**","6b85bbee":"# Features with One Lag:","0ebdd42d":"# Merging Train and Test Set"}}