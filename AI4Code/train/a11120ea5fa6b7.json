{"cell_type":{"9c7f3341":"code","df9908f7":"code","6676d7c0":"code","4db387fe":"code","cd9d1c48":"code","8f11dc2a":"code","13f391d9":"code","62ddcb1a":"code","053b4085":"code","05d37a8f":"code","f80bdaa6":"code","f60a3839":"code","20383248":"code","8e15c51b":"code","47c6d217":"code","fc950488":"code","8a2aedf1":"code","9dfe6e45":"code","45d4e6dd":"code","acfadca9":"code","d85f96d3":"code","f37b7d26":"code","8ab89890":"code","add72555":"code","73205a3c":"code","f7ebf00a":"code","56c894d1":"code","4e2cfdb6":"code","c929c920":"code","d11bd82b":"markdown","004bcc96":"markdown","9ee80ee3":"markdown","a5239db3":"markdown"},"source":{"9c7f3341":"%%capture\n! pip install --upgrade jax\n! pip install --upgrade jaxlib\n! pip install git+https:\/\/github.com\/huggingface\/transformers.git\n! pip install git+https:\/\/github.com\/deepmind\/optax.git\n! pip install flax\n! conda install -y -c conda-forge datasets\n! conda install -y importlib-metadata","df9908f7":"%%sh\npip install -q wandb --upgrade","6676d7c0":"import os\nif 'TPU_NAME' in os.environ:\n    import requests\n    if 'TPU_DRIVER_MODE' not in globals():\n        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475\/requestversion\/tpu_driver_nightly'\n        resp = requests.post(url)\n        TPU_DRIVER_MODE = 1\n\n    from jax.config import config\n    config.FLAGS.jax_xla_backend = \"tpu_driver\"\n    config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n    print('Registered TPU:', config.FLAGS.jax_backend_target)\nelse:\n    print('No TPU detected. Can be changed under \"Runtime\/Change runtime type\".')","4db387fe":"import numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\nimport datasets\nfrom datasets import load_dataset, load_metric\n\nimport jax\nimport flax\nimport optax\nimport jaxlib\nimport jax.numpy as jnp\n\nfrom itertools import chain\nfrom typing import Callable\n\nfrom flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\nfrom flax.training import train_state\nfrom flax import traverse_util\n\nfrom transformers import AutoTokenizer, FlaxAutoModelForSequenceClassification, AutoConfig\n\nimport wandb\n\njax.local_devices()","cd9d1c48":"class Config:\n    nb_epochs = 5\n    lr = 2e-5\n    per_device_bs = 32\n    num_labels = 2\n    model_name = 'bert-base-uncased'\n    total_batch_size = per_device_bs * jax.local_device_count()\n    tokenizer = AutoTokenizer.from_pretrained(model_name)","8f11dc2a":"# You can add your W&B API token as Kaggle secret with the name \"WANDB_API_KEY\".\n# To get your W&B API token, visit https:\/\/wandb.ai\/authorize\n\n# W&B Login\n# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# wb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n# wandb.login(key=wb_key)","13f391d9":"CONFIG = dict(\n    lr=2e-5,\n    model_name = 'bert-base-uncased',\n    epochs = 5,\n    split = 0.10,\n    per_device_bs = 32,\n    seed = 42,\n    num_labels = 2,\n    infra = \"Kaggle\",\n    competition = 'none',\n    _wandb_kernel = 'tanaym'\n)\n\nrun = wandb.init(project='jax_flax', \n                 config=CONFIG,\n                 group='bert',\n                 job_type='train',\n                 anonymous='allow'\n                )","62ddcb1a":"def wandb_log(**kwargs):\n    \"\"\"\n    Logs a key-value pair to W&B\n    \"\"\"\n    for k, v in kwargs.items():\n        wandb.log({k: v})","053b4085":"def simple_acc(preds, labels):\n    assert len(preds) == len(labels), \"Predictions and Labels matrices must be of same length\"\n    acc = (preds == labels).sum() \/ len(preds)\n    return acc\n\nclass ACCURACY(datasets.Metric):\n    def _info(self):\n        return datasets.MetricInfo(\n            description=\"Calculates Accuracy metric.\",\n            citation=\"TODO: _CITATION\",\n            inputs_description=\"_KWARGS_DESCRIPTION\",\n            features=datasets.Features({\n                'predictions': datasets.Value('int64'),\n                'references': datasets.Value('int64'),\n            }),\n            codebase_urls=[],\n            reference_urls=[],\n            format='numpy'\n        )\n\n    def _compute(self, predictions, references):\n        return {\"ACCURACY\": simple_acc(predictions, references)}\n    \nmetric = ACCURACY()","05d37a8f":"def split_and_save(file_path: str, split: float = 0.10):\n    file = pd.read_csv(file_path, encoding='latin-1', names=['sentiment', 'id', 'date', 'query', 'username', 'text'])\n    file = file[['sentiment', 'text']]\n    file['sentiment'] = file['sentiment'].map({4: 1, 0: 0})\n    \n    file = file.sample(frac=1).reset_index(drop=True)\n    split_nb = int(len(file) * split)\n    \n    test_set = file[:split_nb].reset_index(drop=True)\n    train_set = file[split_nb:].reset_index(drop=True)\n    \n    train_set.to_csv(\"train_file.csv\", index=None)\n    test_set.to_csv(\"test_file.csv\", index=None)\n    print(\"Done.\")\n\nsplit_and_save(\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\")","f80bdaa6":"# Get the training and testing files loaded in HF dataset format\nraw_train = load_dataset(\"csv\", data_files={'train': ['.\/train_file.csv']})\nraw_test = load_dataset(\"csv\", data_files={'test': ['.\/test_file.csv']})","f60a3839":"def preprocess_function(data):\n    \"\"\"\n    Preprocessing function\n    \"\"\"\n    texts = (data[\"text\"],)\n    processed = Config.tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n    processed[\"labels\"] = data[\"sentiment\"]\n    return processed","20383248":"%%time\ntrain_dataset = raw_train.map(preprocess_function, batched=True, remove_columns=raw_train[\"train\"].column_names)\ntest_dataset = raw_test.map(preprocess_function, batched=True, remove_columns=raw_test['test'].column_names)","8e15c51b":"train = train_dataset['train']\nvalid = test_dataset['test']\nprint(len(train), len(valid))","47c6d217":"config = AutoConfig.from_pretrained(Config.model_name, num_labels=Config.num_labels)\nmodel = FlaxAutoModelForSequenceClassification.from_pretrained(Config.model_name, config=config, seed=42)","fc950488":"num_train_steps = len(train) \/\/ Config.total_batch_size * Config.nb_epochs\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=Config.lr, pct_start=0.1)\nprint(\"The number of train steps (all the epochs) is\", num_train_steps)","8a2aedf1":"optimizer = optax.adamw(learning_rate=Config.lr, b1=0.9, b2=0.999, eps=1e-6, weight_decay=1e-2)","9dfe6e45":"def loss_fn(logits, targets):\n    loss = optax.softmax_cross_entropy(logits, onehot(targets, num_classes=Config.num_labels))\n    return jnp.mean(loss)\ndef eval_fn(logits):\n    return logits.argmax(-1)","45d4e6dd":"class TrainState(train_state.TrainState):\n    eval_function: Callable = flax.struct.field(pytree_node=False)\n    loss_function: Callable = flax.struct.field(pytree_node=False)","acfadca9":"state = TrainState.create(\n    apply_fn = model.__call__,\n    params = model.params,\n    tx = optimizer,\n    eval_function=eval_fn,\n    loss_function=loss_fn,\n)","d85f96d3":"def train_step(state, batch, dropout_rng):\n    targets = batch.pop(\"labels\")\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n    \n    def loss_function(params):\n        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n        loss = state.loss_function(logits, targets)\n        return loss\n    \n    grad_fn = jax.value_and_grad(loss_function)\n    loss, grad = grad_fn(state.params)\n    grad = jax.lax.pmean(grad, \"batch\")\n    new_state = state.apply_gradients(grads=grad)\n    metrics = jax.lax.pmean({'loss': loss, 'learning_rate': learning_rate_function(state.step)}, axis_name='batch')\n    \n    return new_state, metrics, new_dropout_rng","f37b7d26":"parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))","8ab89890":"def eval_step(state, batch):\n    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n    return state.eval_function(logits)","add72555":"parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")","73205a3c":"def sentimentTrainDataLoader(rng, dataset, batch_size):\n    steps_per_epoch = len(dataset) \/\/ batch_size\n    perms = jax.random.permutation(rng, len(dataset))\n    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n    perms = perms.reshape((steps_per_epoch, batch_size))\n\n    for perm in perms:\n        batch = dataset[perm]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch","f7ebf00a":"def sentimentEvalDataLoader(dataset, batch_size):\n    for i in range(len(dataset) \/\/ batch_size):\n        batch = dataset[i * batch_size : (i + 1) * batch_size]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch","56c894d1":"state = flax.jax_utils.replicate(state)","4e2cfdb6":"rng = jax.random.PRNGKey(42)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())","c929c920":"for i, epoch in enumerate(tqdm(range(1, Config.nb_epochs + 1), desc=f\"Epoch...\", position=0, leave=True)):\n    rng, input_rng = jax.random.split(rng)\n\n    # train\n    with tqdm(total=len(train) \/\/ Config.total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n        for batch in sentimentTrainDataLoader(input_rng, train, Config.total_batch_size):\n            state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n            progress_bar_train.update(1)\n\n    # evaluate\n    with tqdm(total=len(valid) \/\/ Config.total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n        for batch in sentimentEvalDataLoader(valid, Config.total_batch_size):\n            labels = batch.pop(\"labels\")\n            predictions = parallel_eval_step(state, batch)\n            metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n            progress_bar_eval.update(1)\n\n    eval_metric = metric.compute()\n\n    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n    eval_score = round(list(eval_metric.values())[0], 3)\n    metric_name = list(eval_metric.keys())[0]\n    \n    wandb_log(\n        train_loss=loss,\n        valid_acc=eval_score\n    )\n    print(f\"{i+1}\/{Config.nb_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")","d11bd82b":"<center><img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\"\/><\/center><br>\n<p style=\"text-align:center\">WandB is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br>We'll be using this to train our K Fold Cross Validation and gain better insights about our training. <br><br><\/p>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","004bcc96":"### [Check out the W&B Run Page here $\\rightarrow$](https:\/\/wandb.ai\/anony-mouse-121867\/jax_flax\/runs\/28s0ucnx)\n\n![img](https:\/\/i.imgur.com\/YnmHupI.gif)","9ee80ee3":"<h1 style='text-align: center'> Sentiment Classification - 1.6M Tweets + HuggingFace BERT + Jax\/Flax TPUs + W&B Tracking <\/h1>\n\n<p style='text-align: center'>\nSentiment Classification on 1.6 Million tweets using Jax\/Flax with TPUs using HuggingFace BERT and W&B Tracking!<br> \nI have used this <a href='https:\/\/colab.research.google.com\/github\/huggingface\/notebooks\/blob\/master\/examples\/text_classification_flax.ipynb'> script<\/a> as a base and then modified it to work with this dataset.<br>\nI have also used Weights and Biases tracking to keep track of the training process and the experiments I will conduct.\n<\/p>","a5239db3":"**You can upvote this kernel, if you found it useful!**"}}