{"cell_type":{"2c77ff55":"code","12355b59":"code","6b376fe6":"code","6022815c":"code","0bea24ac":"code","6b04a7ee":"code","550fa6ed":"code","8333f222":"code","c3e9537c":"code","d2046710":"code","0148c5a8":"code","b9047521":"code","e7d6beb6":"code","4bb32548":"markdown","71be7372":"markdown","20d5ec28":"markdown","46bd42be":"markdown","d33ac979":"markdown","fece5a25":"markdown","75154c72":"markdown","52212a59":"markdown","15003f69":"markdown","ee637cbd":"markdown"},"source":{"2c77ff55":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku \nimport numpy as np\nimport matplotlib.pyplot as plt","12355b59":"tokenizer = Tokenizer()\ndata = open('..\/input\/poem-generation\/poem.txt').read()\n\ncorpus = data.lower().split(\"\\n\")\n\n\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1\n\n# create input sequences using list of tokens\ninput_sequences = []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n\n# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n# create predictors and label\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n\nlabel = ku.to_categorical(label, num_classes=total_words)\n\nprint(max_sequence_len)","6b376fe6":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150, return_sequences = True)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(total_words\/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","6022815c":"print(model.summary())","0bea24ac":"history = model.fit(predictors, label, epochs=300, verbose=1)","6b04a7ee":"print(\"Model Accuracy: \"+str(history.history['accuracy'][len(history.history['accuracy'])-1]))\nprint(\"Model Loss: \"+str(history.history['loss'][len(history.history['loss'])-1]))","550fa6ed":"acc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.title('Training accuracy')\n\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.title('Training loss')\nplt.legend()\n\nplt.show()","8333f222":"seed_text = \"The View has adorable beauty\"\nnext_words = 100\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    prediction=model.predict(token_list) \n    predicted=np.argmax(prediction,axis=1)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","c3e9537c":"seed_text = \"Brave man fought till his last breath and died while serving to nation\"\nnext_words = 100\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    prediction=model.predict(token_list) \n    predicted=np.argmax(prediction,axis=1)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","d2046710":"model.save('literature_generator.h5')","0148c5a8":"from tensorflow.keras.models import load_model","b9047521":"literature_gen = load_model('.\/literature_generator.h5')","e7d6beb6":"seed_text = \"The Moon Shine was beautiful\"\nnext_words = 100\n  \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    prediction=literature_gen.predict(token_list) \n    predicted=np.argmax(prediction,axis=1)\n    output_word = \"\"\n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\nprint(seed_text)","4bb32548":"#### Importing and Pre-processing Data","71be7372":"#### Testing Model ","20d5ec28":"#### Saving Model","46bd42be":"#### Training Model","d33ac979":"#### Visualizing Model Performance","fece5a25":"## Training LSTM Model to genrate Literature.","75154c72":"#### Importing Libraries","52212a59":"#### Buildig and Compiling Model","15003f69":"#### Testing Saved Model","ee637cbd":"#### Loading Saved Model"}}