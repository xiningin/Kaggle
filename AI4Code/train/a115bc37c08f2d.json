{"cell_type":{"51c6ba87":"code","28ade5c3":"code","3df420aa":"code","f3d08ebc":"code","b567d53c":"code","d1120392":"code","1b3c8f58":"code","e9bb35dc":"code","7b9b77b5":"code","413681f5":"code","61b16890":"code","89656e7d":"code","8aa8eb2b":"code","1cd0e2d4":"code","ea0da47b":"code","2a11a080":"code","1e6d28b7":"code","fbde26a0":"code","637bd1f3":"code","c2584d9b":"code","89a3b192":"markdown","bfedb320":"markdown","a8a83f0e":"markdown","7154acd9":"markdown","e6844a55":"markdown","5790d696":"markdown"},"source":{"51c6ba87":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn import metrics\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, log_loss","28ade5c3":"df = pd.read_csv('..\/input\/data-analytics-challenge-prodigy18\/Prodigy18 Train Data.csv')","3df420aa":"df.sample(5)","f3d08ebc":"df.info()","b567d53c":"df.isnull().sum().plot(kind='bar', title='Num Of Missing Value')","d1120392":"for a in df.columns:\n    print(f\"{a} : {df[a].nunique()} Unique Value\")","1b3c8f58":"cat_col = [col for col in df.columns if df[col].nunique()<=10]\nplt.figure(figsize=(15,40))\nfor i,column in enumerate(cat_col):\n    plt.subplot(len(cat_col), 2, i+1)\n    plt.suptitle(\"Plot Value Proportion\", fontsize=20, x=0.5, y=1)\n    plt.pie(x=df[column].value_counts(), labels=df[~(df[column].isna())][column].unique(), autopct='%.0f%%')\n#     sns.countplot(data=df, x=column)\n    plt.title(f\"{column}\")\n    plt.tight_layout()","e9bb35dc":"cont_col = ['age','length_of_service','avg_training_score']\nfig = make_subplots(rows=2, cols=2,\n                    specs=[[{\"secondary_y\": True}, {\"secondary_y\": True}],\n                           [{\"secondary_y\": True}, {\"secondary_y\": True}]])\n\nfor i,col in enumerate(cont_col):\n    i = i+1\n    colz = 2 if i%2==0 else 1\n    rowz = i\/2\n    fig.add_trace(\n    go.Histogram(x=df[col], nbinsx = 20, name=f\"{col}\"),\n    row=int(f\"{int((i\/2)+1) if colz==1 else int((i\/2))}\"), col=colz, secondary_y=False)\nfig.update_layout(\n    autosize=False,\n    width=1000,\n    height=700)\nfig.show()","7b9b77b5":"plt.figure(figsize=(10,5))\ndf['region'].value_counts().plot(kind='bar', title='Region Value')","413681f5":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr().abs(), annot=True)","61b16890":"# Because education don't have high correlation with any columns, i will impute with 'Unknown'\ndf['education'] = df['education'].fillna('Unknown')","89656e7d":"# For previous year rating i will impute with 0.0\ndf['previous_year_rating'] = df['previous_year_rating'].fillna(0.0)","8aa8eb2b":"# Encode string categorical column to number\nstr_cat_col = ['department','region','education','gender','recruitment_channel']\nfor col in str_cat_col:\n    df[col] = LabelEncoder().fit_transform(df[col])","1cd0e2d4":"# Scale age and avg_training_score with MinMaxScaller\nfor col in ['age','avg_training_score']:\n    df[col] = MinMaxScaler().fit_transform(df[[col]])","ea0da47b":"df = df.drop(columns='employee_id')","2a11a080":"df","1e6d28b7":"X = df.drop(columns = 'is_promoted')\nY = df['is_promoted']","fbde26a0":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=48)","637bd1f3":"algorithm = [\n#     LogisticRegression(),\n    KNeighborsClassifier(),\n    RandomForestClassifier(),\n    XGBClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n]","c2584d9b":"log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns = log_cols)\n\nfor cla in algorithm:\n    cla.fit(X_train, Y_train)\n    name = cla.__class__.__name__\n    print(\"=\" * 30)\n    print(name)\n    print('****Results****')\n    \n    train_predictions = cla.predict(X_test)\n    acc = accuracy_score(Y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = cla.predict(X_test)\n    ll = log_loss(Y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    print(\"\\n\")\n    \n    log_entry = pd.DataFrame([[name, acc * 100, ll]], columns = log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\" * 30)","89a3b192":"<h1 style='background:#CCE2CB; border:0; color:black'><center> EDA <\/center><\/h1> ","bfedb320":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Reading Data <\/center><\/h1> ","a8a83f0e":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Importing Libraries <\/center><\/h1> ","7154acd9":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Cleaning & Feature Engineering <\/center><\/h1> ","e6844a55":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Split Data <\/center><\/h1> ","5790d696":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Train <\/center><\/h1> "}}