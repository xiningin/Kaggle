{"cell_type":{"8fae6eed":"code","09151626":"code","23c60f59":"code","4dc6dd00":"code","99fef27e":"code","8c0a0698":"code","ece92b84":"code","7f5b6c2b":"code","65c1ea5d":"code","039a1506":"code","4a8ebfe2":"code","379a3115":"code","53d71f12":"code","e877b5dd":"code","cb3b7074":"code","6a16b38b":"code","6c3ee3c8":"code","5d6dc795":"code","639ee446":"code","95ccd6cd":"code","a7ba6137":"code","022305bb":"code","d6204c9f":"code","2db305df":"markdown","8f906adc":"markdown","5bb64243":"markdown","bd3194d7":"markdown","211f16f7":"markdown","f7602ab4":"markdown","21a983bc":"markdown","29780bb8":"markdown","34fde098":"markdown","d4550116":"markdown","a263a690":"markdown","f53b04cc":"markdown","d78cd76c":"markdown","02529c62":"markdown","ea2375f8":"markdown","f13a318e":"markdown"},"source":{"8fae6eed":"import numpy as np\nimport pandas as pd\npd.set_option(\"display.max_colwidth\", 80)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport holoviews as hv\nfrom holoviews import opts\nhv.extension('bokeh')\nfrom wordcloud import STOPWORDS\nfrom collections import defaultdict\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer","09151626":"train = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/train.csv\")\nprint('Train Set Shape = {}'.format(train.shape))\ntrain.head(3)","23c60f59":"test = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/test.csv\")\nprint('Test Set Shape = {}'.format(test.shape))\ntest.head(3)","4dc6dd00":"submission = pd.read_csv(\"\/kaggle\/input\/quora-insincere-questions-classification\/sample_submission.csv\")\nsubmission.head(3)","99fef27e":"print(f'the number of insincere questions is : {len(train[train[\"target\"]==1])} \/ {len(train)}')\nprint(f'the number of not-insincere questions is : {len(train[train[\"target\"]==0])} \/ {len(train)}')\ntrain_counts = train[\"target\"].value_counts()\nhv.Bars((train_counts.keys(), train_counts.values),\"Target Label\",\"Counts\").opts(width=600,height=400,title=\"Target Counts\",tools=['hover'])","8c0a0698":"print(f'the number of nulls in train set : {train.isnull().any().sum()}')\nprint(f'the number of nulls in test set : {test.isnull().any().sum()}')","ece92b84":"#ngram function\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]","7f5b6c2b":"train_0 = train[train[\"target\"]==0]\ntrain_1 = train[train[\"target\"]==1]","65c1ea5d":"freq_dict_0_uni = defaultdict(int)\nfor sent in train_0[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict_0_uni[word] += 1\n\nfreq_dict_1_uni = defaultdict(int)\nfor sent in train_1[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict_1_uni[word] += 1","039a1506":"data_0_uni = list(sorted(freq_dict_0_uni.items(), key=lambda x: x[1],reverse=True))\nbars_0_uni = hv.Bars(data_0_uni[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"unigram frequency with target=0\", color=\"red\")\ndata_1_uni = list(sorted(freq_dict_1_uni.items(), key=lambda x: x[1],reverse=True))\nbars_1_uni = hv.Bars(data_1_uni[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"unigram frequency with target=1\", color=\"blue\")\n\n(bars_0_uni + bars_1_uni).opts(opts.Bars(tools=['hover']))","4a8ebfe2":"freq_dict_0_bi = defaultdict(int)\nfor sent in train_0[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        freq_dict_0_bi[word] += 1\n\nfreq_dict_1_bi = defaultdict(int)\nfor sent in train_1[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=2):\n        freq_dict_1_bi[word] += 1","379a3115":"data_0_bi = list(sorted(freq_dict_0_bi.items(), key=lambda x: x[1],reverse=True))\nbars_0_bi = hv.Bars(data_0_bi[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"bigram frequency with target=0\", color=\"red\")\ndata_1_bi = list(sorted(freq_dict_1_bi.items(), key=lambda x: x[1],reverse=True))\nbars_1_bi = hv.Bars(data_1_bi[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"bigram frequency with target=1\", color=\"blue\")\n\n(bars_0_bi + bars_1_bi).opts(opts.Bars(tools=['hover']))","53d71f12":"freq_dict_0_tri = defaultdict(int)\nfor sent in train_0[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=3):\n        freq_dict_0_tri[word] += 1\n\nfreq_dict_1_tri = defaultdict(int)\nfor sent in train_1[\"question_text\"]:\n    for word in generate_ngrams(sent,n_gram=3):\n        freq_dict_1_tri[word] += 1","e877b5dd":"data_0_tri = list(sorted(freq_dict_0_tri.items(), key=lambda x: x[1],reverse=True))\nbars_0_tri = hv.Bars(data_0_tri[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"trigram frequency with target=0\", color=\"red\")\ndata_1_tri = list(sorted(freq_dict_1_tri.items(), key=lambda x: x[1],reverse=True))\nbars_1_tri = hv.Bars(data_1_tri[0:50][::-1],\"Word\",\"Count\").opts(invert_axes=True, width=500, height=800, title=\"trigram frequency with target=1\", color=\"blue\")\n\n(bars_0_tri + bars_1_tri).opts(opts.Bars(tools=['hover']))","cb3b7074":"def tfidf_features(text, _max_features=10, _max_ngrams=2):\n    tfidf = TfidfVectorizer(max_features=_max_features, use_idf=True, ngram_range=(1,_max_ngrams))\n    vec = tfidf.fit_transform(text).toarray()\n    tfidf_df = pd.DataFrame(vec, columns=[\"TFIDF_\" + n for n in tfidf.get_feature_names()])\n    return tfidf_df","6a16b38b":"tfidf_train = tfidf_features(train[\"question_text\"])\ntrain = pd.concat([train, tfidf_train], axis=1)\ntfidf_test = tfidf_features(test[\"question_text\"])\ntest = pd.concat([test, tfidf_test], axis=1)","6c3ee3c8":"uni_0_words = [word_freq[0] for word_freq in data_0_uni[0:50]]\nuni_1_words = [word_freq[0] for word_freq in data_1_uni[0:50]]\nbi_0_words = [word_freq[0] for word_freq in data_0_bi[0:50]]\nbi_1_words = [word_freq[0] for word_freq in data_1_bi[0:50]]\ntri_0_words = [word_freq[0] for word_freq in data_0_tri[0:50]]\ntri_1_words = [word_freq[0] for word_freq in data_1_tri[0:50]]","5d6dc795":"for df in [train,test]:\n    uni_0_feature = []\n    uni_1_feature = []\n    bi_0_feature = []\n    bi_1_feature = []\n    tri_0_feature = []\n    tri_1_feature = []\n    for line in df[\"question_text\"]:\n        uni_0_len = len([word for word in uni_0_words if word in line])\n        uni_1_len = len([word for word in uni_1_words if word in line])\n        bi_0_len = len([word for word in bi_0_words if word in line])\n        bi_1_len = len([word for word in bi_1_words if word in line])\n        tri_0_len = len([word for word in tri_0_words if word in line])\n        tri_1_len = len([word for word in tri_1_words if word in line])\n        \n        uni_0_feature.append(uni_0_len)\n        uni_1_feature.append(uni_1_len)\n        bi_0_feature.append(bi_0_len)\n        bi_1_feature.append(bi_1_len)\n        tri_0_feature.append(tri_0_len)\n        tri_1_feature.append(tri_1_len)\n    df[\"uni_0\"] = uni_0_feature\n    df[\"uni_1\"] = uni_1_feature\n    df[\"bi_0\"] = bi_0_feature\n    df[\"bi_1\"] = bi_1_feature\n    df[\"tri_0\"] = tri_0_feature\n    df[\"tri_1\"] = tri_1_feature\n        ","639ee446":"train.head(3)","95ccd6cd":"test.head(3)","a7ba6137":"for df in [train,test]:\n    df[\"sent_len\"] = df[\"question_text\"].apply(lambda x: len(x.split()))\n    df[\"word_mean_len\"] = df[\"question_text\"].apply(lambda x: np.mean([len(i) for i in x.split()]))\n    df[\"punc_num\"] = df[\"question_text\"].apply(lambda x: len([c for c in x.split() if c in string.punctuation]))","022305bb":"train.head(3)","d6204c9f":"test.head(3)","2db305df":"## Meta Feature","8f906adc":"### Bigram","5bb64243":"# Feature Extraction","bd3194d7":"# EDA","211f16f7":"# Submission","f7602ab4":"## N-gram Feature","21a983bc":"### Unigram","29780bb8":"## Null Ratio","34fde098":"## Target Distribution","d4550116":"# Modeling","a263a690":"## N-gram Frequencies","f53b04cc":"## TF-IDF","d78cd76c":"# Table of Contents\n\n- **[Load Library](#Load-Library)**\n- **[Load Data](#Load-Data)**\n- **[EDA](#EDA)**\n- **[Feature Extraction](#Feature-Extraction)**\n- **[Modeling](#Modeling)**\n- **[Submission](#Submission)**","02529c62":"# Load Library","ea2375f8":"### Trigram","f13a318e":"# Load Data"}}