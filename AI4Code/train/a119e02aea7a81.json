{"cell_type":{"61a43171":"code","4f148186":"code","9c1ae4d3":"code","874370cd":"code","2ae6960c":"code","b4fc6efc":"code","fd9fe011":"code","709c49bf":"code","4a8d7d22":"code","d3fddecf":"code","e1e298ca":"code","8c651b10":"code","472c520a":"code","ea8c09e8":"code","b6525d1d":"code","dcad9708":"code","89b460f8":"code","c28a0c8c":"code","348eeaa5":"code","54752806":"code","29cb8a79":"code","5434f9e1":"code","af9c6a2e":"code","98bca450":"code","03f1611e":"code","9111bd01":"code","a85c4c4a":"code","7338ae57":"code","3cc4b726":"code","da7cba7b":"code","6f50fe2e":"code","1dfb3bd6":"code","e72dd9af":"code","bcd638cd":"code","aeb3da66":"code","9360f216":"code","7ffffbba":"code","574d4a1c":"code","b4fde2b4":"code","0bebda03":"code","6960193e":"code","fec9a1b6":"code","be24627e":"code","51e18366":"code","2301d968":"code","5f71055f":"code","9af43715":"code","4c489f5a":"code","f7926e29":"code","ae2c300a":"code","74b22dc3":"code","c3c0097e":"code","e331d958":"code","9e675447":"code","05625d42":"code","8b91866e":"code","a9c05226":"code","d9f0ebe5":"code","c48b14f9":"code","18b7183e":"code","f169221e":"code","0ea6719b":"code","7aa3c0cf":"code","a3387df0":"code","bdfb030b":"code","52ccd016":"code","7e43f829":"markdown","d94060c8":"markdown","9426aab6":"markdown","497ce597":"markdown","19ee600f":"markdown","3c122170":"markdown","460552d7":"markdown","e4fe724c":"markdown","99af002e":"markdown","fed8a7ce":"markdown","37e7b9fa":"markdown","9168749b":"markdown","93b1693c":"markdown","08761081":"markdown","12a981ce":"markdown","8ddd3601":"markdown","1bd701a3":"markdown","189c6f0b":"markdown","85b84bfe":"markdown","ac71ea00":"markdown","385fc5b8":"markdown","707ba4fa":"markdown"},"source":{"61a43171":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4f148186":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain_data.head()","9c1ae4d3":"test_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data.head()","874370cd":"women = train_data.loc[train_data.Sex == 'female']['Survived']\nrate_women = sum(women)\/len(women)\n\nprint('% of women who survived:', rate_women)\nprint('survived rate of women is {}%'.format(rate_women))\n","2ae6960c":"men = train_data[train_data['Sex'] == 'male']['Survived']\nrate_men = sum(men)\/len(men)\n\nprint('% of men who survived:', rate_men)\n","b4fc6efc":"train_data.info()","fd9fe011":"## null check\ntrain_data.isnull().any()","709c49bf":"train_data['Cabin'].unique()","4a8d7d22":"train_data['Embarked'].value_counts()","d3fddecf":"from sklearn.impute import SimpleImputer\n\ntrain_data = train_data.dropna(subset=['Embarked'])\n\ntrain_data['Age'] = train_data['Age'].apply(lambda x: round(x, 0))\ntest_data['Age'] = test_data['Age'].apply(lambda x: round(x, 0))\n\nimp = SimpleImputer(strategy='most_frequent')\ntrain_imp = imp.fit_transform(train_data['Age'].values.reshape(-1, 1))\ntest_imp = imp.transform(test_data['Age'].values.reshape(-1, 1))\n\ntrain_data['Age'] = train_imp\ntest_data['Age'] = test_imp\n\ntrain_data['Cabin'] = train_data['Cabin'].fillna('nan')\ntest_data['Cabin'] = test_data['Cabin'].fillna('nan')\n\ntrain_data['Cabin_cnt'] = train_data['Cabin'].apply(lambda x: len(x.split(' ')) if x != 'nan' else 0)\ntest_data['Cabin_cnt'] = test_data['Cabin'].apply(lambda x: len(x.split(' ')) if x != 'nan' else 0)\n\nprint(train_data.isnull().any())","e1e298ca":"print(train_data['SibSp'].unique())\nprint(test_data['SibSp'].unique())","8c651b10":"import pickle\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n\ny = train_data['Survived'].values\n\nfeatures = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin_cnt', 'Embarked', 'Age']\n\nX = train_data[features]\nX_test = test_data[features]\n\nprint(X.shape)\nprint(X.columns)\nprint(X_test.shape)\nprint(X_test.columns)\n\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nprint(X)\nprint(y)\n","472c520a":"# save the datas\nnp.savez_compressed('np_savez_comp', X=X, y=y)","ea8c09e8":"!ls","b6525d1d":"# load from npz\ndatas = np.load('np_savez_comp.npz', allow_pickle=True)\nprint(datas.files)","dcad9708":"X = datas['X']\ny = datas['y']","89b460f8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nprint(X_train)\nprint(y_train)","c28a0c8c":"score_lists = []","348eeaa5":"from sklearn.svm import SVC\nsvm = SVC(kernel='linear', C=1.0, random_state=0)\nsvm.fit(X_train, y_train)\n\nwith open('Support Vector Machine.pickle', mode='wb') as fp:\n    pickle.dump(svm, fp)\n","54752806":"score = svm.score(X_test, y_test)\nprint('score: {}' .format(score))","29cb8a79":"# \u30ab\u30fc\u30cd\u30eb\u3067\u66f2\u7dda\u3067\u5224\u5b9a\u3057\u305f\u5834\u5408(rbf)\nsvm2 = SVC(kernel='rbf', C=1.0, random_state=0)\nsvm2.fit(X_train, y_train)","5434f9e1":"score = svm2.score(X_test, y_test)\nprint('score: {}' .format(score))","af9c6a2e":"# gamma\u3092\u88dc\u6b63\nsvm3 = SVC(kernel='rbf', C=1.0, gamma=0.10, random_state=0)\nsvm3.fit(X_train, y_train)\nscore = svm3.score(X_test, y_test)\nprint('score: {}' .format(score))","98bca450":"# gamma\u3092\u88dc\u6b63\nsvm3 = SVC(kernel='rbf', C=1.0, gamma=100.0, random_state=0)\nsvm3.fit(X_train, y_train)\nscore = svm3.score(X_test, y_test)\nprint('score: {}' .format(score))","03f1611e":"from sklearn.linear_model import LogisticRegression\nlogistic_regression = LogisticRegression(penalty='l2', C=100, random_state=0, max_iter=1000)\nlogistic_regression.fit(X_train, y_train)\n\nwith open('Logistic Regression.pickle', mode='wb') as fp:\n    pickle.dump(logistic_regression, fp)\n","9111bd01":"score = logistic_regression.score(X_test, y_test)\nprint('score: {}' .format(score))","a85c4c4a":"from sklearn.tree import DecisionTreeClassifier\ndesicion_tree = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\ndesicion_tree.fit(X_train, y_train)\n\nwith open('Decision Tree.pickle', mode='wb') as fp:\n    pickle.dump(desicion_tree, fp)\n","7338ae57":"score = desicion_tree.score(X_test, y_test)\nprint('score: {}' .format(score))","3cc4b726":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\nknn.fit(X_train, y_train)\n\nwith open('Nearest Neighbors.pickle', mode='wb') as fp:\n    pickle.dump(knn, fp)\n","da7cba7b":"score = knn.score(X_test, y_test)\nprint('score: {}' .format(score))","6f50fe2e":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n\nwith open('Naive Bayes.pickle', mode='wb') as fp:\n    pickle.dump(gnb, fp)\n","1dfb3bd6":"score = gnb.score(X_test, y_test)\nprint('score: {}' .format(score))","e72dd9af":"from sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\nrandom_forest.fit(X_train, y_train)\n\nwith open('Random Forest.pickle', mode='wb') as fp:\n    pickle.dump(random_forest, fp)\n","bcd638cd":"score = random_forest.score(X_test, y_test)\nprint('score: {}' .format(score))","aeb3da66":"from sklearn.ensemble import AdaBoostClassifier\n\nada_boost = AdaBoostClassifier(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm='SAMME.R', random_state=None)\nada_boost.fit(X_train, y_train)\n\nwith open('Ada Boost.pickle', mode='wb') as fp:\n    pickle.dump(ada_boost, fp)\n","9360f216":"score = ada_boost.score(X_test, y_test)\nprint('score: {}' .format(score))","7ffffbba":"names = [\"Support Vector Machine\", \"Logistic Regression\", \"Nearest Neighbors\",\n         \"Decision Tree\",\"Random Forest\", \"Naive Bayes\", \"Ada Boost\"]\n\n# \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9806\u306b\u5b9f\u884c\nresult = []\n\nfor name in names:\n    with open(name + '.pickle', 'rb') as fp:\n        clf = pickle.load(fp)\n    \n#     clf.fit(X_train, y_train)\n    score1 = clf.score(X_train, y_train)\n    score2 = clf.score(X_test, y_test)\n    result.append([score1, score2])\n    \n    print(name)\n\ndf_result = pd.DataFrame(result, columns=['train', 'test'], index = names)\ndf_result\n","574d4a1c":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nkfold = KFold(n_splits=5, random_state=0, shuffle=True)\n\n# svm\nresult = cross_val_score(svm, X, y, cv=kfold, scoring='accuracy')\nprint('svm:{}'.format(result.mean()))\n\n# logistic_regression\nresult = cross_val_score(logistic_regression, X, y, cv=kfold, scoring='accuracy')\nprint('logistic regression:{}'.format(result.mean()))\n\n# desicion_tree\nresult = cross_val_score(desicion_tree, X, y, cv=kfold, scoring='accuracy')\nprint('desicion tree:{}'.format(result.mean()))\n\n# knn\nresult = cross_val_score(knn, X, y, cv=kfold, scoring='accuracy')\nprint('knn:{}'.format(result.mean()))\n\n# random_forest\nresult = cross_val_score(random_forest, X, y, cv=kfold, scoring='accuracy')\nprint('random forest:{}'.format(result.mean()))\n","b4fde2b4":"from sklearn.model_selection import GridSearchCV\n\nparams = {'max_depth': list(range(2, 10)), \n          'min_samples_leaf': list(range(1, 12, 2))}\n\ngrid_search = GridSearchCV(random_forest, params, cv=5, return_train_score=True)\n# grid_search = GridSearchCV(random_forest, params, cv=5, return_train_score=True, verbose=3)\ngrid_search.fit(X, y)","0bebda03":"print('best score: {:0.3f}'.format(grid_search.score(X, y)))\nprint('best params: {}'.format(grid_search.best_params_))\nprint('best val score:  {:0.3f}'.format(grid_search.best_score_))","6960193e":"import seaborn as sns\n%matplotlib inline\n\ncv_result = pd.DataFrame(grid_search.cv_results_)\ncv_result = cv_result[['param_max_depth', 'param_min_samples_leaf', 'mean_test_score']]\ncv_result_pivot = cv_result.pivot_table('mean_test_score', 'param_max_depth', 'param_min_samples_leaf')\n\nheat_map = sns.heatmap(cv_result_pivot, cmap='Greys', annot=True);","fec9a1b6":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100)\nxgb.fit(X_train, y_train)","be24627e":"score = xgb.score(X_test, y_test)\nprint('score: {}' .format(score))","51e18366":"from lightgbm import LGBMClassifier\n\nlgb = LGBMClassifier(max_depth=3, learning_rate=0.1, n_estimators=100)\nlgb.fit(X_train, y_train)","2301d968":"score = lgb.score(X_test, y_test)\nprint('score: {}' .format(score))","5f71055f":"params = {'max_depth': list(range(2, 10)),\n          'eta': [0.01, 0.1, 1.0],\n          'gamma': [0, 0.1],\n          'min_child_weight': [1, 2],\n          'nthread': [2, 4],\n          'n_estimators': list(range(50, 200, 50))}\n\nxgb = XGBClassifier()\n\nreg_cv = GridSearchCV(xgb, params, cv=5, return_train_score=True)\n# reg_cv = GridSearchCV(xgb, params, cv=5, return_train_score=True, verbose=3)\nreg_cv.fit(X_train, y_train)\n","9af43715":"with open('XGBoost.pickle', mode='wb') as fp:\n    xgb = XGBClassifier(**reg_cv.best_params_)\n    xgb.fit(X_train, y_train)\n    pickle.dump(xgb, fp)\n","4c489f5a":"print('best score: {:0.3f}'.format(reg_cv.score(X_train, y_train)))\nprint('best params: {}'.format(reg_cv.best_params_))\nprint('best val score:  {:0.3f}'.format(reg_cv.best_score_))","f7926e29":"xgb = reg_cv.best_estimator_\nscore = xgb.score(X_test, y_test)\nprint('score: {}' .format(score))","ae2c300a":"lgb = LGBMClassifier()\n\nreg_cv = GridSearchCV(lgb, params, cv=5, return_train_score=True)\n# reg_cv = GridSearchCV(lgb, params, cv=5, return_train_score=True, verbose=3)\nreg_cv.fit(X_train, y_train)\n","74b22dc3":"with open('LightGBM.pickle', mode='wb') as fp:\n    lgb = LGBMClassifier(**reg_cv.best_params_)\n    lgb.fit(X_train, y_train)\n    pickle.dump(lgb, fp)\n","c3c0097e":"print('best score: {:0.3f}'.format(reg_cv.score(X_train, y_train)))\nprint('best params: {}'.format(reg_cv.best_params_))\nprint('best val score:  {:0.3f}'.format(reg_cv.best_score_))","e331d958":"lgb = reg_cv.best_estimator_\nscore = lgb.score(X_test, y_test)\nprint('score: {}' .format(score))","9e675447":"from sklearn.ensemble import AdaBoostClassifier\n\nnames = [\"Support Vector Machine\", \"Logistic Regression\", \"Nearest Neighbors\",\n         \"Decision Tree\",\"Random Forest\", \"Ada Boost\", \n         \"Naive Bayes\", 'XGBoost', 'LightGBM']\n\n# \u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u9806\u306b\u5b9f\u884c\nresult = []\n\nfor name in names:\n    with open(name + '.pickle', 'rb') as fp:\n        clf = pickle.load(fp)\n\n#     clf.fit(X_train, y_train)\n    score1 = clf.score(X_train, y_train)\n    score2 = clf.score(X_test, y_test)\n    result.append([score1, score2])\n    print(name)\n\ndf_result = pd.DataFrame(result, columns=['train', 'test'], index = names)\ndf_result\n","05625d42":"batch_size = 64\nn_epochs = 100","8b91866e":"X_train.shape","a9c05226":"import tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.Sequential([\n    keras.layers.Dense(512, activation='relu', input_shape=[X_train.shape[1]]),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(512, activation='relu'),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(2, activation='softmax'),\n])","d9f0ebe5":"model.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","c48b14f9":"model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size)","18b7183e":"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n\nprint('\\nTest accuracy:', test_acc)","f169221e":"import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\ntrain_X = torch.tensor(X_train, dtype=torch.float32)\ntrain_y = torch.tensor(y_train)\ntest_X = torch.tensor(X_test, dtype=torch.float32)\ntest_y = torch.tensor(y_test)","0ea6719b":"train = TensorDataset(train_X, train_y)\nprint(train[0])\n\ntrain_loader = DataLoader(train, batch_size=batch_size, shuffle=True)","7aa3c0cf":"class Net(nn.Module):\n    def __init__(self, col_num):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(col_num, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.fc3 = nn.Linear(512, 2)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x)) # ReLU: max(x, 0)\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = self.fc3(x)\n        return x\n    \nnet = Net(X_train.shape[1])","a3387df0":"# \u640d\u5931\u95a2\u6570\ncriterion = nn.CrossEntropyLoss() # \u4ed6\u30af\u30e9\u30b9\u5206\u985e:\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\u8aa4\u5dee\n#criterion = nn.MSELoss() # \u56de\u5e30:\u5e73\u5747\u4e8c\u4e57\u8aa4\u5dee\n#criterion = nn.L1Loss() # \u56de\u5e30:\u5e73\u5747\u7d76\u5bfe\u5024\u8aa4\u5dee\n# criterion = nn.BCELoss() # \u4e8c\u5024\u5206\u985e:\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\n# criterion = nn.BCEWithLogitsLoss() # \u4e8c\u5024\u5206\u985e:\u30ed\u30b8\u30c3\u30c8\u30fb\u30d0\u30a4\u30ca\u30ea\u4ea4\u5dee\u30a8\u30f3\u30c8\u30ed\u30d4\u30fc\n\n# \u6700\u9069\u5316\u95a2\u6570\noptimizer = optim.SGD(net.parameters(), lr=0.01)\n# optimizer = optim.Adam(net.parameters(), lr=0.0001)","bdfb030b":"for epoch in range(n_epochs):\n    total_loss = 0\n    \n    for i, data in enumerate(train_loader):\n        inputs, labels = data\n        \n        optimizer.zero_grad()\n        \n        outputs = net(inputs)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    if (epoch+i)%60 == 0:\n        print(epoch+1, total_loss)\n","52ccd016":"result = torch.max(net(test_X).data, 1)[1]\naccuracy = sum(test_y.data.numpy() == result.numpy()) \/ len(test_y.data.numpy())\naccuracy","7e43f829":"### KFold(k-\u5206\u5272\u4ea4\u5dee\u691c\u8a3c)","d94060c8":"### Tensorflow","9426aab6":"## Preparing Data \/ \u30c7\u30fc\u30bf\u6e96\u5099","497ce597":"## Analyze Data \/ \u30c7\u30fc\u30bf\u5206\u6790","19ee600f":"## Decition Tree \/ \u6c7a\u5b9a\u6728","3c122170":"## PyTorch","460552d7":"## sklearn's classifier models \/ sklearn\u306e\u5206\u985e\u30e2\u30c7\u30eb\n- Support Vector Machine(SVM) \/ \u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30bf\u30fc\u30de\u30b7\u30f3\n- Logistic Regression \/ \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30\n- Decision Tree \/ \u6c7a\u5b9a\u6728\n- K-NearestNeighbor \/ k\u8fd1\u508d\u6cd5\n- Naive Bayes \/ \u30ca\u30a4\u30fc\u30d6\u30d9\u30a4\u30ba\n\n\u203b\u5404\u5206\u985e\u5668\u306e\u7c21\u5358\u306a\u8aac\u660e\nhttps:\/\/www.octoparse.jp\/blog\/10-machine-learning-algorithms-you-should-know\/\n\n\u203b\u3053\u3063\u3061\u306e\u65b9\u304c\u3044\u3044\u304b\u3082\nhttp:\/\/hirai.me\/notes_pyml_03.html#3.5%E3%80%80%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%ABSVM-%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%9F%E9%9D%9E%E7%B7%9A%E5%BD%A2%E5%95%8F%E9%A1%8C%E3%81%AE%E6%B1%82%E8%A7%A3","e4fe724c":"## Logistic Regression \/ \u30ed\u30b8\u30b9\u30c6\u30a3\u30c3\u30af\u56de\u5e30","99af002e":"## AdaBoost \/ \u30a2\u30c0\u30d6\u30fc\u30b9\u30c8","fed8a7ce":"## Naive Bayes \/ \u30ca\u30a4\u30fc\u30d6\u30d9\u30a4\u30ba","37e7b9fa":"## Support Vector Machine \/ \u30b5\u30dd\u30fc\u30c8\u30d9\u30af\u30bf\u30fc\u30de\u30b7\u30f3","9168749b":"### Grid search conbination with cross validation for decision tree","93b1693c":"### XGBoost for sklearn","08761081":"## K-NearestNeighbor \/ k\u8fd1\u508d\u6cd5","12a981ce":"### LightGBM for sklearn","8ddd3601":"### \u307e\u3068\u3081\u3066\u5206\u985e\u5668\u691c\u8a3c","1bd701a3":"## Random Forest(Ensemble) \/ \u30e9\u30f3\u30c0\u30e0\u30d5\u30a9\u30ec\u30b9\u30c8(\u30a2\u30f3\u30b5\u30f3\u30d6\u30eb)","189c6f0b":"## Cross Validation and Grid Search \/ \u30af\u30ed\u30b9\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u3068\u30b0\u30ea\u30c3\u30c9\u30b5\u30fc\u30c1","85b84bfe":"## Nural Network\n- Tensorflow\n- PyTorch","ac71ea00":"## GBDT(Gradient Boosting Decision Tree)\n- XGBoost\n- LightGBM","385fc5b8":"### Grid Serch","707ba4fa":"### \u304a\u307e\u3068\u30812"}}