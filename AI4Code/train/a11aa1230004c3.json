{"cell_type":{"14d19722":"code","cb413b0b":"code","4a2ec30f":"code","4e1df4cc":"code","85af4b98":"code","69f34d94":"code","a81b64cd":"code","b49e0f14":"code","a93833c6":"code","12eb4b95":"code","81ec9789":"code","542f1612":"code","546ef056":"code","ca3e66a2":"code","d89cdd5d":"code","21d204b4":"code","11d01d18":"code","8ac72c74":"code","bf2a554d":"code","825bcfae":"code","d4e4ad9e":"code","59aff14e":"code","84a512bf":"code","2d35661b":"code","3e940f95":"code","c60e8a14":"markdown","2481be3b":"markdown","d87dc7b3":"markdown","6d0fb9ea":"markdown","94ad816d":"markdown","cc131195":"markdown","7c703c82":"markdown","8852658f":"markdown","2c7af5ff":"markdown","a9fb32e8":"markdown","96b75b34":"markdown","05cadda0":"markdown","0aeb0db6":"markdown","0259b0d4":"markdown","9e09ea53":"markdown","291c9e96":"markdown","c1d686b3":"markdown","8490b86d":"markdown","f5a6d9f5":"markdown","d676be8e":"markdown","2e9539d9":"markdown","b0502ef1":"markdown","5ab683d2":"markdown","3eb76636":"markdown","8f0ef756":"markdown","168935d9":"markdown","cff3638c":"markdown","ef10f877":"markdown","de66dd9f":"markdown","6f83111b":"markdown"},"source":{"14d19722":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cb413b0b":"import gensim\nimport matplotlib\nimport pickle\nimport gc","4a2ec30f":"def readarticle(filepath):\n    paperdata = {\"paper_id\" : None, \"title\" : None, \"abstract\" : None}\n    with open(filepath) as file:\n        filedata = json.load(file)\n        paperdata[\"paper_id\"] = filedata[\"paper_id\"]\n        paperdata[\"title\"] = filedata[\"metadata\"][\"title\"]\n                \n        if \"abstract\" in filedata:\n            abstract = []\n            for paragraph in filedata[\"abstract\"]:\n                abstract.append(paragraph[\"text\"])\n            abstract = '\\n'.join(abstract)\n            paperdata[\"abstract\"] = abstract\n        else:\n            paperdata[\"abstract\"] = []\n    return paperdata\n\ndef read_multiple(jsonfiles_pathnames):\n    papers = {\"paper_id\" : [], \"title\" : [], \"abstract\" : []}\n    for filepath in jsonfiles_pathnames:\n        paperdata = readarticle(filepath)\n        if len(paperdata[\"abstract\"]) > 0: \n            papers[\"paper_id\"].append(paperdata[\"paper_id\"])\n            papers[\"title\"].append(paperdata[\"title\"])\n            papers[\"abstract\"].append(paperdata[\"abstract\"])\n            #papers[\"body_text\"].append(paperdata[\"body_text\"])\n            #print(\"not none\")\n        #else:\n            #print(\"none\")\n    print(len(papers[\"paper_id\"]))\n    print(len(papers[\"title\"]))\n    print(len(papers[\"abstract\"]))\n    return papers\n\ndef make_bigram(tokenized_data, min_count = 5, threshold = 100):\n    bigram_phrases = gensim.models.Phrases(tokenized_data, min_count = min_count, threshold = threshold)\n    #after Phrases a Phraser is faster to access\n    bigram = gensim.models.phrases.Phraser(bigram_phrases)\n    gc.collect()\n    return bigram\n\ndef make_trigram(tokenized_data, min_count = 5, threshold = 100):\n    bigram_phrases = gensim.models.Phrases(tokenized_data, min_count = min_count, threshold = threshold)\n    trigram_phrases = gensim.models.Phrases(bigram_phrases[tokenized_data], threshold = 100)\n    #after Phrases a Phraser is faster to access\n    trigram = gensim.models.phrases.Phraser(trigram_phrases)\n    gc.collect()\n    return trigram\n\ndef readjson_retbodytext(jsonfiles_pathnames):\n    print(\"reading json files\")\n    documents = read_multiple(jsonfiles_pathnames)\n    print(\"writing documents dictionary to output for use in another kernel\")\n    with open(\"documents_dict.pkl\", 'wb') as f:\n        pickle.dump(documents, f)\n    print(\"done writing documents dict.  Format is paper_id, title, body_text\")\n    gc.collect()\n    return documents[\"abstract\"]\n    \ndef open_tokenize(jsonfiles_pathnames):\n    \n    body_text = readjson_retbodytext(jsonfiles_pathnames)\n    \n    print(\"removing stopwords, steming, and tokenizing.  This is expensive\")\n    tokenized_documents = gensim.parsing.preprocessing.preprocess_documents(body_text)\n    print(\"done preprocessing documents. now writing to output to be used in another documents\")\n    with open(\"tokenized_documents.pkl\", 'wb') as f:\n        pickle.dump(tokenized_documents, f)\n    print(\"done writing file\")\n    \n    gc.collect()\n    return tokenized_documents","4e1df4cc":"#no_n_below should be uint, ex: no_n_below = 3 or no_n_below = 5\n#no_freq_above should be float [0,1], ex: no_freq_above = 0.5\n#n_feats should be uint, ex: n_feats = 1024 or n_feats = 2048\ndef create_dictionary(tokenized_documents, n_feats, no_n_below = 3, no_freq_above = 0.5):\n    print(\"creating dictionary\")\n    id2word_dict = gensim.corpora.Dictionary(tokenized_documents)\n    print(\"done creating dictionary\")\n    \n    print(\"prior dictionary len %i\" % len(id2word_dict))\n    id2word_dict.filter_extremes(no_below = no_n_below, no_above = no_freq_above, keep_n = n_feats, keep_tokens = None)\n    print(\"current dictionary len %i\" % len(id2word_dict))\n    \n    return id2word_dict\n\ndef corpus_tf(id2word_dict, tokenized_documents):\n    return [id2word_dict.doc2bow(document) for document in tokenized_documents]\n\ndef try_parameters(tokenized_documents, n_feats, n_topics):\n    id2word_dict = create_dictionary(tokenized_documents, n_feats = n_feats)\n    tfcorpus = corpus_tf(id2word_dict, tokenized_documents)\n    print(\"training lda model with %i features and %i topics\" % (n_feats, n_topics))\n    lda_model = gensim.models.ldamodel.LdaModel(corpus = tfcorpus, num_topics = n_topics, id2word = id2word_dict, per_word_topics = False)\n    coherence_model = gensim.models.CoherenceModel(model = lda_model, texts = tokenized_documents, dictionary = id2word_dict, coherence = \"c_v\")\n    coherence_score = coherence_model.get_coherence()\n    print(\"coherence for unknown ngram with %i features and %i topics: %f\" % (n_feats, n_topics, coherence_score))\n    gc.collect()\n    return coherence_score\n\ndef loop_lda(tokenized_documents, \n                     tfcorpus, \n                     id2word_dict,\n                     start, #suggest 2 or something\n                     stop, # suggest 20 or similar\n                     step,\n                     per_word_topics = False): #compute list of topics for each word\n    topic_counts = []\n    coherence_scores = []\n    for n_topics in range (start, stop, step):\n        lda_model = gensim.models.ldamodel.LdaModel(corpus = tfcorpus, num_topics = n_topics, id2word = id2word_dict, per_word_topics = per_word_topics)\n        coherence_model = gensim.models.CoherenceModel(model = lda_model, texts = tokenized_documents, dictionary = id2word_dict, coherence = \"c_v\")\n        coherence_score = coherence_model.get_coherence()\n        coherence_scores.append(coherence_score)\n        topic_counts.append(n_topics)\n        print(\"coherence of %f with %i topics\" % (coherence_score, n_topics))\n              \n    return topic_counts, coherence_scores;\n        \ndef loop_ntopics_lda(tokenized_documents, n_feats, start, stop, step):\n    id2word_dict = create_dictionary(tokenized_documents, n_feats = n_feats)\n    tfcorpus = corpus_tf(id2word_dict, tokenized_documents)\n    topic_counts, coherence_scores = loop_lda(tokenized_documents, tfcorpus, id2word_dict, start, stop, step)\n    gc.collect()\n    return topic_counts, coherence_scores\n\nngram_bounds = (1,2)\nn_feats_bounds = (512,2048)\nn_topics_bounds = (1,20)\n\nbounds = [ngram_bounds, n_feats_bounds, n_topics_bounds]\n\ndef lda_objective(X, tokenized_documents, tokenized_bigram_documents):\n    ngram = int(round(X[0])) #bound should be [1,2]\n    n_feats = int(round(X[1])) #bounds should be [512, 2048]\n    n_topics = int(round(X[2])) #bouns should be [1,20]\n    \n    if ngram == 2:\n        documents = tokenized_bigram_documents\n        type_string = \"tokenized_bigram_documents\"\n    else:\n        documents = tokenized_documents\n        type_string = \"tokenized_documents\"\n\n    print(\"creating dictionary with %s for: %i %i %i\" % (type_string, ngram, n_feats, n_topics))\n    id2word_dict = create_dictionary(documents, n_feats = n_feats)\n\n    print(\"done creating dictionary.  creating corpus for: %i %i %i\" % (ngram, n_feats, n_topics))\n    tfcorpus = corpus_tf(id2word_dict, documents)\n\n    print(\"done creating corpus.  Building model for: %i %i %i\" % (ngram, n_feats, n_topics))\n    lda_model = gensim.models.ldamodel.LdaModel(corpus = tfcorpus, num_topics = n_topics, id2word = id2word_dict, per_word_topics = False)\n\n    print(\"calculating coherence for: %i %i %i\" % (ngram, n_feats, n_topics))\n    coherence_model = gensim.models.CoherenceModel(model = lda_model, texts = documents, dictionary = id2word_dict, coherence = \"c_v\")\n    coherence = coherence_model.get_coherence()\n    #we want to MAX coherence.  but we will be using a \n    value2minimize = 1 - coherence\n    return value2minimize\n","85af4b98":"def topic_distribution(query_string, id2word_dict, lda_model):\n    tokenized_query = gensim.parsing.preprocessing.preprocess_string(query_string)\n\n    print(\"tokens in query: %i\" % (len(tokenized_query)))\n    print(tokenized_query)\n    \n    vectorized_query = id2word_dict.doc2bow(tokenized_query)\n    \n    return lda_model[vectorized_query]    #query topic vector (distribution)\n    \ndef corpus_similarities_print3(query_topicvec, index, documents_dict):\n    similarities = index[query_topicvec]\n    ranked_indices = sorted(enumerate(similarities), key = lambda item: -item[1])\n    #papers = {\"paper_id\" : [], \"title\" : [], \"abstract\" : []}\n    \n    document_pids = documents_dict[\"paper_id\"]\n    document_titles = documents_dict[\"title\"]\n    #document_abstracts = documents_dict[\"abstract\"]\n    \n    print(ranked_indices[0][0])\n    topdex = ranked_indices[0][0]\n    second = ranked_indices[1][0]\n    third = ranked_indices[2][0]\n    \n    print(\"\\nTOP RESULT TITLE: %s\" % (document_titles[topdex]))\n    print(\"TOP RESULT PID: %s\" % (document_pids[topdex]))\n    print(second)\n    print(\"\\nSecond RESULT TITLE: %s\" % (document_titles[second]))\n    print(\"Second RESULT PID: %s\" % (document_pids[second]))\n    print(third)\n    print(\"\\nThird RESULT TITLE: %s\" % (document_titles[third]))\n    print(\"Third RESULT PID: %s\" % (document_pids[third]))\n    gc.collect()","69f34d94":"coherence_results_path = \"\/kaggle\/input\/try-lda-parameters\/coherence_dict.pkl\"\nwith open(coherence_results_path, \"rb\") as f:\n    coherence_results = pickle.load(f)\n\n#including the defintion for this dictionary in the comments for our reference\n\n#coherence_dict = {\"topic_counts\" : topic_counts,\n#                  \"coherence_1gram_512features\" : coherence_1gram_512features,\n#                  \"coherence_1gram_1024features\" : coherence_1gram_1024features,\n#                  \"coherence_2gram_256features\" : coherence_2gram_256features, \n#                  \"coherence_2gram_512features\" : coherence_2gram_512features,\n#                  \"coherence_2gram_1024features\" : coherence_2gram_1024features}\n\nstart = coherence_results[\"topic_counts\"][0]\nlength = len(coherence_results[\"topic_counts\"])\nstop = length + start\n\nprint(start)\nprint(length)\nprint(stop)\n\nx = range(start, stop, 1)\nmatplotlib.pyplot.plot(x, coherence_results[\"coherence_1gram_512features\"], label = \"1gram_512feats\")\nmatplotlib.pyplot.plot(x, coherence_results[\"coherence_1gram_1024features\"], label = \"1gram_1024feats\")\nmatplotlib.pyplot.plot(x, coherence_results[\"coherence_2gram_256features\"], label = \"2gram_256feats\")\nmatplotlib.pyplot.plot(x, coherence_results[\"coherence_2gram_512features\"], label = \"2gram_512feats\")\nmatplotlib.pyplot.plot(x, coherence_results[\"coherence_2gram_1024features\"], label = \"2gram_1024feats\")\n\nmatplotlib.pyplot.xlabel(\"Number of topics\")\nmatplotlib.pyplot.ylabel(\"Coherence score\")\n\nmatplotlib.pyplot.title(\"Coherence values for for gram 1 and 2, and features 256, 512, and 1024\")\nmatplotlib.pyplot.legend()\nmatplotlib.pyplot.show()","a81b64cd":"tokenized_path = \"\/kaggle\/input\/preprocess-cord19\/tokenized_documents.pkl\"\nprint(\"opening %s\" % str(tokenized_path)) \nwith open(tokenized_path, \"rb\") as f:\n    tokenized_documents = pickle.load(f)\nprint(\"done opening tokenized documents.  Optimizing\")\n\nbigram_path = \"\/kaggle\/input\/preprocess-cord19\/bigram_model.pkl\"\nprint(\"opening %s\" % str(bigram_path))\nwith open(bigram_path, \"rb\") as f:\n    bigram_model = pickle.load(f)\nprint(\"creating bigram documents\")\ntokenized_document = [bigram_model[document] for document in tokenized_documents]\nprint(\"done retrieving documents. lets optimize\")\n\nn_feats = 512\nn_topics = 18\nid2word_dict = create_dictionary(tokenized_documents, n_feats = n_feats)\ntfcorpus = corpus_tf(id2word_dict, tokenized_documents)\nprint(\"training lda model with %i topics\" % (n_topics))\nlda_model = gensim.models.ldamodel.LdaModel(corpus = tfcorpus, num_topics = n_topics, id2word = id2word_dict, per_word_topics = False)\ncoherence_model = gensim.models.CoherenceModel(model = lda_model, texts = tokenized_documents, dictionary = id2word_dict, coherence = \"c_v\")\ncoherence_score = coherence_model.get_coherence()\nprint(\"Achieved coherence of: %f\" % (coherence_score))\n","b49e0f14":"#build index for document similarity.  the similarity method used will be cosine similarity\nprint(\"creating index\")\nindex = gensim.similarities.MatrixSimilarity(lda_model[tfcorpus])\nprint(\"done creating index\")\n\ndocuments_path = \"\/kaggle\/input\/preprocess-cord19\/documents_dict.pkl\"\nwith open(documents_path, \"rb\") as f:\n    documents_dict = pickle.load(f)\nprint(\"done opening raw documents\")\n\n","a93833c6":"query = \"What is known about transmission, incubation, and environmental stability?\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","12eb4b95":"query = \"What do we know about natural history, transmission, and diagnostics for the virus? \"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","81ec9789":"query = \"What have we learned about infection prevention and control?\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","542f1612":"query = \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","546ef056":"query = \"Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","ca3e66a2":"query = \"Seasonality of transmission.\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","d89cdd5d":"query = \"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","21d204b4":"query = \"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","11d01d18":"query = \"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","8ac72c74":"query = \"Natural history of the virus and shedding of it from an infected person\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","bf2a554d":"query = \"Implementation of diagnostics and products to improve clinical processes\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","825bcfae":"query = \"Disease models, including animal models for infection, disease and transmission\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","d4e4ad9e":"query = \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","59aff14e":"query = \"Immune response and immunity\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","84a512bf":"query = \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","2d35661b":"query = \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","3e940f95":"query = \"Role of the environment in transmission\"\nquery_topicvec = topic_distribution(query, id2word_dict, lda_model)\ncorpus_similarities_print3(query_topicvec, index, documents_dict)","c60e8a14":"# Additional Imports \nGensim imported for natural language processing tasks; matplotlib imported for plotting; pickle used for opening results saved from previous steps;","2481be3b":"# \"Role of the environment in transmission\"","d87dc7b3":"# \"Immune response and immunity\"","6d0fb9ea":"# \"Seasonality of transmission.\"","94ad816d":"# Specific Questions\nThese are the specific bullet points to be addressed in this task","cc131195":"# \"Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery\"","7c703c82":"# Try LDA Parameters steps\nCode available in the public kernel that produced the \"try-lda-parameters\" input.\nTry ngrams (1 and 2), number of features, and number of topics\n1.  Open tokenized documents from preprocesing steps.  This will serve as our 1gram.\n2.  Set features to 512 and loop through [2,20] LDA topics and collect coherence scores. Set features to 1024 and loop through [2,20] LDA topics and collect coherence scores.\n3.  Open bigram model.  Run tokenized document through the bigram model in order to obtain tokenized bigram documents.\n4.  Set features to 256 and loop through [2,20] LDA topics and collect coherence scores. Set features to 512 and loop through [2,20] LDA topics and collect coherence scores.\n5.  Save results for use in this notebook.\n","8852658f":"# Definitions for query topic vectors and ranking \n**Topic distribution:**\nThis definition takes a provided string, tokenizes it with the provided dictionary (either word or bigram based), and then returns a topic distribution vector using a provided LDA topic model.\n**Print top 3 matches:**\nthis takes a query in the form of a topic distribution and a gensim similarity matrix designed to efficiently calculate cosine similarities and then creates a ranked list of indices for the matches.  The indices are used along with a provided dictionary of the raw data in order to print the title and abstract of the top three matching papers.","2c7af5ff":"# \"Tools and studies to monitor phenotypic change and potential adaptation of the virus\"","a9fb32e8":"# \"What have we learned about infection prevention and control?\"","96b75b34":"# \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\"","05cadda0":"# Create similarity index.\nThis gensim matrix is designed to easily calculate cosine similary with-respect-to entry indices.  It can be sorted to ","0aeb0db6":"# \"Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\"","0259b0d4":"# \"Implementation of diagnostics and products to improve clinical processes\"","9e09ea53":"# \"Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\"","291c9e96":"# \"Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\"","c1d686b3":"# \"Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\"","8490b86d":"# Definitions from preprocessing kernel\n\nIncluding definitions from the preprocessing step here.  They are not used in this kernel.  The output from the preprocessing step is used as input to this notebook.\n\n**Definition for reading the json files:**\nThe purpose of these definitions is to read all json files in a path and return a dictionary containing entries for the paper's id, title, abstract, and body text. These json files are the papers that we have full body text for.  Now that we are up to 50k+ documents we keep running out of memory.  We will limit ourselves to abstracts under the assumption that abstracts should summarize the entire paper including results.\n\n**Bigram and trigram definitions:**\nBelow are definitions for making bigrams (sequences of 2 words) and trigrams (collections of 3 words). We will be skipping trigrams, but the definition is included in case it is desired later.\n\n**Definitions for tokenizing documents:**\nReading in the documents was done inside the same function that tokenizes the documents so that the raw documents can go out of scope. This is done to reduce memory usage. We will only be looking at papers that have a json file. We will be saving the resulting dictionary for use later.","f5a6d9f5":"# Broad questions\nWe will start will the broad scope questions for this task.","d676be8e":"# \"Natural history of the virus and shedding of it from an infected person\"","2e9539d9":"# \"What is known about transmission, incubation, and environmental stability?\"","b0502ef1":"# Introduction","5ab683d2":"# Definitions for \"Try LDA Parameters\" kernel\nIn this portion of our effort to tune some of the LDA parameters, we will try configurations of several hyperparameters.  The parameters we will vary are: ngram (1 or 2), number of features (words or bigrams) in the feature distribution vectors of the topics, and number of topics to factor for when creating the LDA model (topic-feature matrix).\n\n**Create Dictionary:**\nThis is the definition for creating a dictionary for our corpus. Number of features (word stems of ngrams) will be the variable optimized for using this function.\n\n**Create term frequency corpus:**\nDefinition for creating the corpus from the dictionary and documents.\n\n**Try model configuration:**\nDefinition to train a LDA Model and the compute the coherence score.\n\n**Loop through number of topics:**  Given a set ngram (1 or 2) and feature length, loop through number of topics given the start, stop, and step bounds.  For each n_topics build an LDA topic model, calculate coherence, and return coherence list.\n\n**Objective function for differential evolution optimization:**\nOriginally we wanted to use differential evolution to optimize LDA parameters.  However, we were unable to use this technique because each mumber of the population requires compute and memory resources to evaluate the objective function. \n\nDifferential evolution uses a population to try many different parameters, removes the poor performing population members, makes combinations of the more successfull population members, performs some mutations, and then repeats until some stopping condintion (number of evolutions, or no improvement after a given number of steps).  In this way it hopes to achieve good non-linear optimization.  Differential evolution is similar to genetic algorithms, but skips the chromosome string encoding and acts directly on the variables.   We would need to reserve extra compute resources for this technique.  Note that we need to define a vector of tuples for the bounds, and that some parameters need to be converted to to int (uint) because this scipy function works with floats only.","3eb76636":"This is the final portion of an effor to optimize some parameters for a Latent Dirichlet Allocation (LDA) topic model. We break up this effort into multiple steps in order to speed up testing and making changes. This also uses less memory and is less likely to time out.  There are many parameters we could attempt to optimize.  However, due to processing constraints we limit ourselves to the number of topics and the number of features (word or bigrams).\n\nA topic model learns a topic-feature matrix of abstract topics and features (word or ngrams) and a document-topic matrix of documents and topics, from a document-feature matrix of documents and features. From this factorization we achieve statistical feature vectors for each topic and topic vectors for each document in the training corpus. We can then find topic vectors for the questions we would like to ask the corpus of documents. We will use the closest matching documents in the CORD-19 dataset in an attempt to answer the task questions. LDA assumes a Dirichlet prior on topic-feature and document-topic distributions. In other words it assumes each topic is defined by and small collection of words or ngrams and that each documnent consists of a small number of topics.\n\nThe advantage of trying to find some configurations of LDA training parameters with better coherence scores is that the resulting topic model might perform better under this tasks queries than they would from random parameter selection.  However, coherence is not a human measure (involving humans would be expensive and negate the benifit of first step automation) and we only varied a select few parameters (there are more parameter that could impact human perception of the results).\n\nWe will be relying on paper titles and PIDs as answers to the task's queries.  The papers have the advantage of being written by humans (rather than a generated summary) and therefore is assumed to be understandable.  However, it is up to the person asking the question to then go to the referenced papers and determine if their question is answered.","8f0ef756":"# \"Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\"","168935d9":"# \"Disease models, including animal models for infection, disease and transmission\"","cff3638c":"# Open and view results from parameter search\nHere we will view the results from our various combinations of ngram (1 and 2), number of features, and number of topics","ef10f877":"# \"What do we know about natural history, transmission, and diagnostics for the virus? \"","de66dd9f":"# Choose ngram, number of features, and number of topics\nFrom the above results we choose our ngram (words or bigram), number of featurs, and number of topics.  It would be preferable to save the best performing model from the previous testing and load it.\n* We will select the tokenized bigram documents\n* We will select 512 features\n* We will use 18 topics","6f83111b":"# Preprocessing steps\nCode available in the public kernel that produced the preprocess input.\n\n1. **Look at metadata and print information**\n2. **Collect json file paths**\n3. **Remove stopwords, stem, tokenize documents, and save.**  Stopwords are words common to all text, such as \"and\" and \"the,\" and therefore we remove them. Stemming reduces words to their root form. For example, \"infected\" and \"infecting\" will both be reduced to \"infect.\" Tokenization converts the body text of each paper into a vector of whitespace-separated text (words). These tokens will be treated as semantic features, particularly after stemming. When we create the document-feature matrix (term frequency corpus) the exact text of the word (or bigram) will no longer be observed, as these features define the row vectors of that matrix. Here, we save the tokenized documents.\n4. **Create bigram model and save.  In this section we create and save the bigram model.** This just contains the word pair. We will have to run the tokenized documents through the model in order to create a bigram tokenized version of the documents. We will do that in another step therefore we save the model here."}}