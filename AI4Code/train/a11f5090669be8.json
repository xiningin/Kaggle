{"cell_type":{"b3d86b1c":"code","584332a3":"code","9c99d8a8":"code","6f0d0469":"code","dad2b41e":"code","283f1034":"code","841283bf":"code","ba386040":"code","621d205e":"code","ea1fc56d":"code","a0a5c275":"code","ede7ea72":"code","33f885d5":"code","e766f654":"code","02f1ca5c":"code","b1dd0008":"code","4f720d65":"code","5c9deb27":"code","2802f603":"code","fb5e8502":"code","a4f09b07":"code","0f4f6562":"code","a0383552":"code","91c36eaf":"code","4d17e4e7":"code","2b021795":"code","1e136252":"code","e750000e":"code","1c611282":"code","35b194c0":"code","2447eaa5":"code","38bc93e7":"code","00abbe5a":"code","468e8f40":"code","ae1dcd27":"code","33857cf0":"code","5b40c84c":"code","ebb4943b":"code","ae65d4c4":"code","83e48691":"code","4aff1cf1":"code","1c8afafe":"code","2adc9942":"code","8b0dedb9":"code","5535debb":"code","7b48c6bd":"code","be600354":"code","9335ddc1":"code","e8243f50":"code","07565ac4":"code","d3917a02":"code","3021a5d9":"code","70bf0a00":"code","6b7a2a95":"code","4aa249a0":"code","98f116c9":"code","ae821a5d":"code","00087668":"code","52ac6443":"code","86c9db3a":"code","b1cb4dde":"code","45a82697":"code","26644b98":"code","2e8a8a5d":"code","e3156b4d":"code","9b9c2233":"code","e192739c":"code","6282526c":"code","b21a8180":"code","a601f7d2":"code","88157276":"code","9c3970d9":"code","328a60f3":"code","6902b558":"code","09e5b054":"code","a759757d":"code","2ab7a21e":"code","e9c824be":"code","fe3a5b60":"code","28b10b29":"code","d848e4b2":"code","94ecf15b":"markdown","7442ca65":"markdown","cb2a0f77":"markdown","15a2d785":"markdown","7420da2b":"markdown","0780a444":"markdown","bb6fd405":"markdown","e443087e":"markdown","c6c57432":"markdown","4c57b6d3":"markdown","6d7dee93":"markdown","aa726adb":"markdown","f9634760":"markdown","f711d076":"markdown","6de8c7c9":"markdown","07cd4339":"markdown","bcf350cb":"markdown","8262824b":"markdown","88dd881f":"markdown","5bd27fe4":"markdown","e0569eeb":"markdown","7de992f0":"markdown","63f8219b":"markdown","0d5176ef":"markdown","55359e12":"markdown","b2d78472":"markdown","eaec2b63":"markdown","b4dedc19":"markdown","7d0357b7":"markdown","ae64f918":"markdown","f192fb9a":"markdown","e934adee":"markdown","7519643b":"markdown","c2597a6e":"markdown","b2917cde":"markdown","735ddd55":"markdown","d7cf37e2":"markdown","5964baf2":"markdown"},"source":{"b3d86b1c":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n%matplotlib inline\n\n# crossvalidation\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom numpy import std\nfrom numpy import mean\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# hyperparameter Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Classification Report\nfrom sklearn import metrics\n","584332a3":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine_df = [train_df, test_df]","9c99d8a8":"train_df.columns.values","6f0d0469":"train_df.nunique()","dad2b41e":"train_df.info()","283f1034":"# Numerical feilds\ntrain_df.describe()","841283bf":"# Non Numerical Feilds\ntrain_df.describe(include = 'O')","ba386040":"df = train_df.drop(['Survived'], axis =1)\nfor col in df.columns.values:\n    grouped_data = train_df.groupby([col]).agg({'Survived' : 'sum'})\n    grouped_data['%'] = grouped_data.apply(lambda x: 100*x \/ x.sum())\n    print(grouped_data.sort_values(by =['Survived'], ascending = False))\n    print('-'*40)\n   \n    ","621d205e":"Gender_mapping = {'female': 0, 'male':1}\nfor dataset in combine_df:\n    dataset['Sex']= dataset['Sex'].map(Gender_mapping)\n    \ntrain_df.groupby(['Sex']).agg({'Survived':'count'})","ea1fc56d":"Embarked_mapping = {'S': 0, 'C':1, 'Q':2 }\nfor dataset in combine_df:\n    dataset['Embarked']= dataset['Embarked'].map(Embarked_mapping)\n    \ntrain_df.groupby(['Embarked']).agg({'Survived':'count'})","a0a5c275":"for dataset in combine_df:\n    dataset['Title'] = dataset['Name'].str.extract(r' ([A-Za-z]+)\\.[^\\.]*')\n    \ntrain_df.groupby(['Title']).agg({'Survived' : 'count'})","ede7ea72":"for dataset in combine_df:\n    dataset['Title'] = dataset['Title'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Lady','Major','Sir','Rev'],'Rare')\n    dataset['Title'] = dataset['Title'].replace(['Miss','Mlle'],'Ms')\n    dataset['Title'] = dataset['Title'].replace(['Mme'],'Mrs')\n    \ntrain_df.groupby(['Title']).agg({'Survived' : 'count'})","33f885d5":"title_mapping = {'Mr':1, 'Ms':2, 'Mrs':3,'Master':4, 'Rare':5}\nfor dataset in combine_df:\n    dataset['Title']=dataset['Title'].map(title_mapping)\n    \ntrain_df.groupby(['Title']).agg({'Survived' : 'count'})","e766f654":"for dataset in combine_df:\n    dataset.drop(['Cabin','Ticket','Name'], axis = 1,inplace=True)\ntrain_df.drop(['PassengerId'], axis = 1, inplace = True)\ntrain_df.head()","02f1ca5c":"pearsoncorr = train_df.corr()\nfig, ax = plt.subplots(figsize=(14,6)) \nsns.heatmap(pearsoncorr,annot=True,fmt='.1g',vmin = -1, vmax=1, center = 0, cmap = 'coolwarm');","b1dd0008":"train_df.head()","4f720d65":"for df in combine_df:\n    df['Embarked'] = df['Embarked'].fillna(0)","5c9deb27":"train_df.groupby(['Embarked']).agg({'Survived':'count'})","2802f603":"for dataset in combine_df:\n    for i in dataset.Pclass.unique():\n        for j in dataset.SibSp.unique():\n            #boolean mask\n            mask1 = (dataset['Pclass'] == i) & (dataset['SibSp'] == j)\n            mask2 = mask1 & (dataset['Age'].notnull())\n\n            #get median by mask without NaNs\n            \n            if dataset.loc[mask2].Age.count() > 0:\n                med = dataset.loc[mask2, 'Age'].median()\n            else:\n                med = dataset.Age.median()\n\n            #repalce NaNs\n            dataset.loc[mask1, 'Age'] = dataset.loc[mask1, 'Age'].fillna(med)\n\n","fb5e8502":"test_df[test_df.isnull().any(axis = 1)]","a4f09b07":"test_df.groupby(['Fare']).agg({'Sex':'count'})","0f4f6562":"test_df['Fare'] = test_df['Fare'].fillna(0.0)","a0383552":"test_df.groupby(['Title']).agg({'Sex':'count'})","91c36eaf":"test_df['Title'] = test_df['Title'].fillna(1.0)","4d17e4e7":"test_df[test_df.isnull().any(axis = 1)]","2b021795":"train_df[train_df.isnull().any(axis = 1)]","1e136252":"for dataset in combine_df:\n    dataset['Family'] = dataset.SibSp + dataset.Parch + 1","e750000e":"train_df","1c611282":"train_df.groupby(['Family']).agg({'Survived':'mean'}).sort_values(by = 'Survived', ascending = False)","35b194c0":"for dataset in combine_df:\n    m2 = (dataset['Family'] == 1)\n    dataset['IsAlone'] = 0\n    dataset.loc[m2, 'IsAlone'] = 1","2447eaa5":"train_df.groupby(['IsAlone']).agg({'Survived':'mean'}).sort_values(by ='Survived', ascending = False)","38bc93e7":"train_df = train_df.drop(['Family','SibSp','Parch'], axis = 1)\ntest_df = test_df.drop(['Family','SibSp','Parch'], axis = 1)\ncombine_df = [train_df,test_df ]","00abbe5a":"Age_Hist = sns.FacetGrid(train_df, col='Survived')\nAge_Hist.map(plt.hist, 'Age', bins = 20);","468e8f40":"train_df['Age_band'] = pd.cut(train_df.Age, 5)\ntrain_df.groupby(['Age_band']).agg({'Survived':'mean'}).sort_values(by  = 'Age_band', ascending = True)","ae1dcd27":"for dataset in combine_df:\n    dataset.loc[dataset['Age'] < 16, 'Age'] = 0\n    dataset.loc[((dataset['Age'] >= 16) & (dataset['Age'] < 32)), 'Age'] = 1\n    dataset.loc[((dataset['Age'] >= 32) & (dataset['Age'] < 48)), 'Age'] = 2\n    dataset.loc[((dataset['Age'] >= 48) & (dataset['Age'] < 64)), 'Age'] = 3\n    dataset.loc[dataset['Age'] >= 64, 'Age'] = 4\n    \ntrain_df = train_df.drop('Age_band', axis = 1)","33857cf0":"train_df.groupby(['Age']).agg({'Survived':'mean'}).sort_values(by = 'Survived', ascending = False)","5b40c84c":"AgeBand_hist = sns.FacetGrid(train_df, col = 'Survived')\nAgeBand_hist.map(plt.hist, 'Age');","ebb4943b":"train_df['Fare_Band'] = pd.cut(train_df.Fare, 4)\ntrain_df.groupby(['Fare_Band']).agg({'Survived':'mean'}).sort_values(by  = 'Fare_Band', ascending = True)","ae65d4c4":"# Train_df\ntrain_df.loc[(train_df['Fare'] < 128), 'Fare'] = 0\ntrain_df.loc[((train_df['Fare'] >= 128) & (train_df['Fare'] < 256)), 'Fare'] = 1\ntrain_df.loc[((train_df['Fare'] >= 256) & (train_df['Fare'] < 384)), 'Fare'] = 2\ntrain_df.loc[(train_df['Fare'] >= 384) , 'Fare'] = 3\n","83e48691":"\n# Test_df\ntest_df.loc[(test_df['Fare'] < 128), 'Fare'] = 0\ntest_df.loc[((test_df['Fare'] >= 128) & (test_df['Fare'] < 256)), 'Fare'] = 1\ntest_df.loc[((test_df['Fare'] >= 256) & (test_df['Fare'] < 384)), 'Fare'] = 2\ntest_df.loc[(test_df['Fare'] >= 384) , 'Fare'] = 3","4aff1cf1":"train_df.groupby(['Fare']).agg({'Survived':'mean'}).sort_values(by = 'Fare', ascending = True)\n    \n    ","1c8afafe":"train_df = train_df.drop(['Fare_Band'], axis = 1)\ncombine_df = [train_df, test_df]","2adc9942":"train_df.head(2)","8b0dedb9":"for dataset in combine_df:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.groupby(['Age*Class']).agg({'Survived':'mean'}).sort_values(by = 'Survived', ascending = False)","5535debb":"# Creating Crossfold\ncv = KFold(n_splits = 10, random_state = 1, shuffle = True)\n\n# Creating Training and Testing dataset from train_df\nX_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","7b48c6bd":"X_train","be600354":"X_test.info()","9335ddc1":"# Normalizing continuous variables\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range = (0,1))\n\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)","e8243f50":"# Normalizing continuous variables\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range = (0,1))\n\nscaler.fit(X_test)\nX_test = scaler.transform(X_test)","07565ac4":"# Setting training size\ntrain_sizes = [20, 100, 200, 400, 800]\ndef plot_learning_curve(model, train_sizes, X_train, Y_train, cv, title):\n    \n    train_sizes, train_scores, validation_scores = learning_curve(estimator = model,X = X_train, y = Y_train, train_sizes = train_sizes, cv = cv, scoring = 'neg_mean_squared_error')\n    \n    train_scores_mean = - train_scores.mean(axis = 1)\n    validation_scores_mean = - validation_scores.mean(axis = 1)\n    \n    # Ploting\n    plt.style.use('seaborn')\n    plt.plot(train_sizes, train_scores_mean, label = 'Training Error')\n    plt.plot(train_sizes, validation_scores_mean, label = 'Validation Error')\n    plt.ylabel('MSE', fontsize = 14)\n    plt.xlabel('Training set size', fontsize = 14)\n    plt.title(title, fontsize = 18)\n    plt.legend()\n    #plt.ylim(0, 40)\n             \n    \n    ","d3917a02":"# Logistic Regression\n\n# create model\nlogReg = LogisticRegression()\n# evaluate model\nscores = cross_val_score(logReg, X_train, Y_train, scoring = 'accuracy', cv = cv, n_jobs = -1)\n# report performance\nprint('Accuracy of logReg: %.3f (%.3f)' % (scores.mean(), scores.std()))\n\n# learning Curve\nplot_learning_curve(logReg, train_sizes, X_train, Y_train, cv, 'Learning curves for a linear regression model')\n","3021a5d9":"solvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [10000, 100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ngrid_search = GridSearchCV(estimator=logReg, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train, Y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n","70bf0a00":"# Logistic Regression with Hypertuning\n\n# create model\nlogReg = LogisticRegression(C = 100.0, penalty= 'l2', solver = 'newton-cg')\n# evaluate model\nscores = cross_val_score(logReg, X_train, Y_train, scoring = 'accuracy', cv = cv, n_jobs = -1)\n# report performance\nprint('Accuracy of logReg: %.3f (%.3f)' % (scores.mean(), scores.std()))\n\n# Saving Score\nacc_logReg = round(mean(scores) * 100, 2 )\nstd_logReg = round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(logReg, train_sizes, X_train, Y_train, cv, 'Learning curves for a linear regression model')\n\n","6b7a2a95":"# Support Vector Machines\n\n#create Model\nsvc = SVC()\n# Evaluate Model\nscores = cross_val_score(svc, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of svc: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# learning Curve\nplot_learning_curve(svc, train_sizes, X_train, Y_train, cv, 'Learning curves for a svc model')","4aa249a0":"# Defining parameters to be tuned in dictionary\nparam_grid = {'kernel' : ['linear', 'rbf', 'poly'],\n              'gamma' :  [0.1, 1],\n              'C' : [0.1,10]\n                 }\n\n\ngrid = GridSearchCV(SVC(), param_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n\n# Fitting the model\ngrid_result = grid.fit(X_train, Y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n\n","98f116c9":"# Support Vector Machines with Best parameters\n\n#create Model\nsvc = SVC(C = 10, gamma = 1, kernel = 'poly')\n# Evaluate Model\nscores = cross_val_score(svc, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of svc: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# Saving Score\nacc_svc = round(mean(scores) * 100, 2 )\nstd_svc= round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(svc, train_sizes, X_train, Y_train, cv, 'Learning curves for a svc model')\n","ae821a5d":"# KNN\n\n#create Model\nknn = KNeighborsClassifier()\n# Evaluate Model\nscores = cross_val_score(knn, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of knn: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# learning Curve\nplot_learning_curve(knn, train_sizes, X_train, Y_train, cv, 'Learning curves for a KNN model')","00087668":"# KNN Hypertuning\n\n# Defining parameters to be tuned in dictionary\nparam_grid = {'n_neighbors' : list(range(1,20)) ,\n              'p' :  [1, 2]\n                 }\ngrid = GridSearchCV(KNeighborsClassifier(), param_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n\n# Fitting the model\ngrid_result = grid.fit(X_train, Y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","52ac6443":"# KNN After hypertunning\n\n#create Model\nknn = KNeighborsClassifier(n_neighbors = 18, p= 2)\n# Evaluate Model\nscores = cross_val_score(knn, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of knn: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# Saving Score\nacc_knn = round(mean(scores) * 100, 2 )\nstd_knn = round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(knn, train_sizes, X_train, Y_train, cv, 'Learning curves for a KNN model')","86c9db3a":"# Gaussian Naive Bayes\n\n#create Model\ngaussian = GaussianNB()\n# Evaluate Model\nscores = cross_val_score(gaussian, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of gaussian: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# learning Curve\nplot_learning_curve(gaussian, train_sizes, X_train, Y_train, cv, 'Learning curves for a Gaussian model')","b1cb4dde":"# GaussianNB Hypertuning\n\n# Defining parameters to be tuned in dictionary\nparam_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n\ngrid = GridSearchCV(GaussianNB(), param_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n\n# Fitting the model\ngrid_result = grid.fit(X_train, Y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","45a82697":"# Gaussian Naive Bayes After Tuning\n\n#create Model\ngaussian = GaussianNB(var_smoothing = 0.657933224657568)\n# Evaluate Model\nscores = cross_val_score(gaussian, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of gaussian: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# Saving Score\nacc_gaussian = round(mean(scores) * 100, 2 )\nstd_gaussian = round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(gaussian, train_sizes, X_train, Y_train, cv, 'Learning curves for a Gaussian model')","26644b98":"# Perceptron\n\n#create Model\nperceptron = Perceptron()\n# Evaluate Model\nscores = cross_val_score(perceptron, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of perceptron: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# learning Curve\nplot_learning_curve(perceptron, train_sizes, X_train, Y_train, cv, 'Learning curves for a Perceptron model')","2e8a8a5d":"# Perceptron Hypertuning\n\n# Defining parameters to be tuned in dictionary\nparam_grid = {'penalty': ['l2','l1','elasticnet'],\n             'alpha' : np.logspace(0,-9, num=100),\n             'max_iter' : [50, 100, 500, 1000]\n             }\n\ngrid = GridSearchCV(Perceptron(), param_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n\n# Fitting the model\ngrid_result = grid.fit(X_train, Y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","e3156b4d":"# Perceptron After Tuning\n\n#create Model\nperceptron = Perceptron(alpha = 2.310129700083158e-05, penalty = 'l1', max_iter= 50)\n# Evaluate Model\nscores = cross_val_score(perceptron, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of perceptron: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# Saving Score\nacc_perceptron = round(mean(scores) * 100, 2 )\nstd_perceptron= round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(perceptron, train_sizes, X_train, Y_train, cv, 'Learning curves for a Perceptron model')","9b9c2233":"# Linear SVC\n\n#create Model\nlin_svc = LinearSVC()\n# Evaluate Model\nscores = cross_val_score(lin_svc, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of lin_svc: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# learning Curve\nplot_learning_curve(lin_svc, train_sizes, X_train, Y_train, cv, 'Learning curves for a lin_svc model')","e192739c":"# Defining parameters to be tuned in dictionary\nparam_grid = {\n              'C' : np.logspace(3,-5, num = 100),\n                'max_iter' : [1000, 5000]\n                 }\ngrid = GridSearchCV(LinearSVC(), param_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n\n# Fitting the model\ngrid_result = grid.fit(X_train, Y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n","6282526c":"# Linear SVC After tunning\n\n#create Model\nlin_svc = LinearSVC(C=475.0810162102798, max_iter = 5000)\n# Evaluate Model\nscores = cross_val_score(lin_svc, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of lin_svc: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# Saving Score\nacc_lin_svc = round(mean(scores) * 100, 2 )\nstd_lin_svc = round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(lin_svc, train_sizes, X_train, Y_train, cv, 'Learning curves for a lin_svc model')","b21a8180":"# Stochastic Gradient Descent\n\n#create Model\nsgd = SGDClassifier()\n# Evaluate Model\nscores = cross_val_score(sgd, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of sgd: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# learning Curve\nplot_learning_curve(sgd, train_sizes, X_train, Y_train, cv, 'Learning curves for a sgd model')","a601f7d2":"# Defining parameters to be tuned in dictionary\nparam_grid = {'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3], # learning rate\n    'max_iter': [1000], # number of epochs\n    'loss': ['log'], # logistic regression,\n    'penalty': ['l2'],\n    'n_jobs': [-1]\n}\ngrid = GridSearchCV(SGDClassifier(), param_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n\n# Fitting the model\ngrid_result = grid.fit(X_train, Y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n","88157276":"# Stochastic Gradient Descent After tunning\n\n#create Model\nsgd = SGDClassifier(alpha = 0.0001, loss= 'log', max_iter = 1000, n_jobs = -1, penalty = 'l2')\n# Evaluate Model\nscores = cross_val_score(sgd, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of sgd: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# Saving Score\nacc_sgd = round(mean(scores) * 100, 2 )\nstd_sgd = round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(sgd, train_sizes, X_train, Y_train, cv, 'Learning curves for a sgd model')","9c3970d9":"# Decision Tree\n\n#create Model\ndec_tree = DecisionTreeClassifier()\n# Evaluate Model\nscores = cross_val_score(dec_tree, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of dec_tree: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# learning Curve\nplot_learning_curve(dec_tree, train_sizes, X_train, Y_train, cv, 'Learning curves for a Decision tree model')","328a60f3":"# Defining parameters to be tuned in dictionary\nparam_grid = {\"max_depth\": [3, None],\n\"max_features\": [1, 2, 3, 4,5 ,6 ,7 ,8 ,9],\n\"min_samples_leaf\": [1, 2, 3, 4,5 ,6 ,7 ,8 ,9],\n\"criterion\": [\"gini\", \"entropy\"]}\n\ngrid = GridSearchCV(DecisionTreeClassifier(),param_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n# Fitting the model\ngrid_result = grid.fit(X_train, Y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n\n","6902b558":"# Decision Tree after tunning\n\n#create Model\ndec_tree = DecisionTreeClassifier(criterion = 'gini', max_depth = 3, max_features = 6, min_samples_leaf = 8)\n# Evaluate Model\nscores = cross_val_score(dec_tree, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of dec_tree: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# Saving Score\n\nacc_dec_tree = round(mean(scores) * 100, 2 )\nstd_dec_tree = round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(dec_tree, train_sizes, X_train, Y_train, cv, 'Learning curves for a Decision tree model')","09e5b054":"# RandomForestClassifier\n\n#create Model\nrandom_forest = RandomForestClassifier()\n# Evaluate Model\nscores = cross_val_score(random_forest, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of random_forest: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# learning Curve\nplot_learning_curve(random_forest, train_sizes, X_train, Y_train, cv, 'Learning curves for a random_forest model')","a759757d":"# RandomForestClassifier tuning\n\n# Number of trees in random forest\nn_estimators = [200]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [20]\n# Minimum number of samples required to split a node\nmin_samples_split = [2]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [2]\n# Method of selecting samples for training each tree\nbootstrap = [True]\n\n\n# Defining parameters to be tuned in dictionary\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\ngrid = GridSearchCV(RandomForestClassifier(), param_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n\n# Fitting the model\ngrid_result = grid.fit(X_train, Y_train)\n\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n\n\n","2ab7a21e":"# RandomForestClassifier After tuning\n\n#create Model\nrandom_forest = RandomForestClassifier(bootstrap = True, max_depth = 20, max_features = 'auto', min_samples_leaf = 2, n_estimators = 200) \n# Evaluate Model\nscores = cross_val_score(random_forest, X_train,Y_train, scoring ='accuracy', cv = cv, n_jobs = -1)\n# Report Performance\nprint('Accuracy of random_forest: %.3f (%.3f)' % (mean(scores), std(scores)))\n\n# Saving Score\n\nacc_random_forest = round(mean(scores) * 100, 2 )\nstd_random_forest = round(std(scores) * 100, 2)\n\n# learning Curve\nplot_learning_curve(random_forest, train_sizes, X_train, Y_train, cv, 'Learning curves for a random_forest model')","e9c824be":"model_score = pd.DataFrame({\n                            'Model' : ['LogisticRegression', 'LinearSVC', 'SVC', 'KNeighborsClassifier', 'RandomForestClassifier','GaussianNB','Perceptron','SGDClassifier','DecisionTreeClassifier'],\n                            'Score' : [acc_logReg, acc_lin_svc, acc_svc, acc_knn, acc_random_forest, acc_gaussian, acc_perceptron, acc_sgd, acc_dec_tree],\n                            'Standard Dev' : [std_logReg, std_lin_svc, std_svc, std_knn, std_random_forest, std_gaussian, std_perceptron, std_sgd, std_dec_tree]\n})\nmodel_score.sort_values(by = 'Score', ascending = False)","fe3a5b60":"# learning Curve\nplot_learning_curve(svc, train_sizes, X_train, Y_train, cv, 'Learning curves for a svc model')","28b10b29":"# SVC Model\n\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","d848e4b2":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n#submission.to_csv('..\/submission.csv', index=False)","94ecf15b":"##### 1. Family feild combining SblSp and Parch.\nWe can combine SibSp and Parch and add 1 to it for the passenger itself.","7442ca65":"### Lets Check the Survial relation with other feilds\n","cb2a0f77":"#### 2. Completing :- We would want to complete the Age and embarked\n        For completing it, we first need to find the correlation of the feilds with each other.","15a2d785":"### 3. Creating\n#### 3. Creating :- We can create the following-\n                1. Family feild combining SblSp and Parch.\n                2. Age range feild.\n                3. Fare range feild.","7420da2b":"##### 4. We can drop Cabin, Ticket, PassengerId and Name. ","0780a444":"### Normalization of the data","bb6fd405":"##### 2. We can convert Embarked to Numerical value.","e443087e":"#### 4. Classifying :- 1. Women (Sex=female) were more likely to have survived.\n                  2. Children (Age<) were more likely to have survived.\n                  3. The upper-class passengers (Pclass=1) were more likely to have survived.","c6c57432":"#####  3. We can extract Titles from Name.","4c57b6d3":"#### Learning Curve for SVC","6d7dee93":"##### 2. Age range feild.\n\nTo create an Age band, we would first require to analyse the Age col itself.","aa726adb":"### Result display using  SVC","f9634760":"## 3. Wrangle, clean and prepare data.\n\nLets first find out the features in our dataset. i.e. Columns","f711d076":"#### Completing Test_df","6de8c7c9":"## Assumptions and Observations using above data\n1. PassengerId, Name, Fare, Cabin and Ticket has no impact directly on survival.\n2. 1st Class in Pclass survived the most (39%).\n3. Females survived more than males (68%).\n4. Age range between 20 and 30 survived the most.\n5. Passengers with 0 siblings or spouses survived the most(61%).\n6. Passengers with 0 parents or children survived the most (68%).\n7. Passengers with S embarked survived the most(63%).","07cd4339":"# Workflow Stages\n\n1. Understand the problem definition.\n2. Understand and Acquire Training and Test data.\n3. Wrangle, clean and prepare data.\n4. Analyse, Identify the pattern and explore data\n5. Model, Predict and Solve the problem\n6. Visualize, Report the problem and final solution","bcf350cb":"### Compiling and ranking all the model's results after tuning to decide which one to use.","8262824b":"#### Observation -\n1. Low error\n2. Medium variance\n\n#### Hypertuning the SVC parameters","88dd881f":"### Observations from the above data\n1. Non Categorial :- PassengerId, Name, Age, Ticket, Fare, Cabin.\n2. Sure Categorial :- Survived, Pclass, Sex, Embarked.\n3. Discrete Non Categorial :- SibSp, Parch.","5bd27fe4":"Following observations from above\n1. 30 < Age < 35 survived the least.\n2. Only 1 passanger in 80 and survived.\n3. 0-5 aged passengers had high survival rate.\n4. Most passengers ranged in 15 - 40.","e0569eeb":"### Observations from the above finding\n1. int :- PassengerId, Survived, Pclass, SibSp, Parch.\n2. Float :- Age, Fare.\n3. String :- Name, Sex, Ticket, Cabin, Embarked\n\nSome of the feilds are Alphanumeric( mixed type ) in nature :- Ticket, Cabin","7de992f0":"##### 2. Age = As age has maximum correralation with Pclass and SibSp\n\nCompleting a numerical continuous feature\n\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\nA simple way is to generate random numbers between mean and standard deviation.\n\nMore accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\nCombine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","63f8219b":"#### Now let's find out if the data is categorial or numerical.","0d5176ef":"#### Leraning Curve\nLets create a learning curve function that can be called for each model to test if the model has high bias or high variance.\n1. learning_curve() to generate the data needed to plot a learning curve. The function returns a tuple containing three elements: the training set sizes, and the error scores on both the validation sets and the training sets.","55359e12":"#### Observation\n1. High Variance\n2. Normalization can be used.\n3. Tuning parameters like - solver, penalty and C can be checked for","b2d78472":"## 2. Understand and Acquire Training and Test data.\n\n### 2a. Understand the data\n\n1. Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n2. Test.csv will contain the details of a subset of the passengers on board (418 to be exact) and importantly, will NOT reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\n#### Data Dictionary\n\n\n1. survival := Survival(0 = No, 1 = Yes).\n2. pclass := Ticket class(1 = 1st, 2 = 2nd, 3 = 3rd)\n3. sex := Sex\t\n4. Age := Age in years\t\n5. sibsp := No. of siblings \/ spouses aboard the Titanic\t\n6. parch :=\tNo. of parents \/ children aboard the Titanic\t\n7. ticket := Ticket number\t\n8. fare := Passenger fare\t\n9. cabin := Cabin number\t\n10. embarked := Port of Embarkation(C = Cherbourg, Q = Queenstown, S = Southampton)\n\n#### Variable Notes\n\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.\n\n","eaec2b63":"#### Executing the Models","b4dedc19":"We can create another feature IsAlone","7d0357b7":"### Observation from the Description of data\n1. Features with null values :- Cabin > Age > Embarked.\n2. All Names are Unique.\n3. Non categorial Feilds with Duplicate values :- Ticket > Cabin.\n4. Minimum Age is 0.42 years.\n5. Maximum Age is 80 years.\n6. Minimum Fare is 0.\n7. There are passengers with 0 SibSp and Parch.\n8. Maximum Male passengers 577 \/ 891.\n9. Maximum \"S\" embarked 644\/ 891.","ae64f918":"### Assumtions based on data analysis\n1. Correcting :- We can do the following:-\n                    1. We can convert Sex to numerical value.\n                    2. We can drop Cabin, Ticket and PassengerId. \n                    3. We can extract Titles from Name and then drop it.\n                    4. We can convert Embarked to Numerical value.\n2. Completing :- We would want to complete the Age and embarked.\n3. Creating :- We can create the following-\n                1. Family feild combining SblSp and Parch.\n                2. Age range feild.\n                3. Fare range feild.\n4. Classifying :- 1. Women (Sex=female) were more likely to have survived.\n                  2. Children (Age<) were more likely to have survived.\n                  3. The upper-class passengers (Pclass=1) were more likely to have survived.\n                  \n#### 1. Correcting :- We can do the following:-\n                    1. We can convert Sex to numerical value.\n                    2. We can convert Embarked to Numerical value.\n                    3. We can extract Titles from Name.\n                    4. We can drop Cabin, Ticket, PassengerId and Name. \n                    \n                   \n","f192fb9a":"##### 1. Completing Embarked\nThere is no correlation of embarked with any feild other than survived.\nHence we will fill it with median() as whole.","e934adee":"## 1. Understand the problem definition.\n\nThe problem is to find which passengers survived the Titanic Shipwreck.\n\n### The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","7519643b":"### 2b. Acquire Training and Test data\n#### 2b.1. Import all Liberies","c2597a6e":"### Model, Predict and Solve the problem\n\nLets first understand the type of problem -\n1. Supervised Learning\n2. Classification = Survived or not\n3. Train dataset = 891 rows\n\nHence we will try following Models -\n1. LinearSVC \n2. KNeighborsClassifier\n3. SVC\n4. RandomForestClassifier\n5. Logistic Regression\n6. Naive Bayes classifier\n7. Decision Tree\n8. Perceptron\n9. Artificial neural network\n10. RVM or Relevance Vector Machine\n\nWe will be using K-fold instead of train-test split to train the train_df\n","b2917cde":"Lets run Logistic reg again with tuned hyperparameters -","735ddd55":"#### Let's find out the data types of the data","d7cf37e2":"#### 2b.2 Load CSVs","5964baf2":"#### 1. We can convert Sex to numerical value"}}