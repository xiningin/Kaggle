{"cell_type":{"42e9e902":"code","6f949d3a":"code","215de678":"code","75bbf3c2":"code","0a12bcca":"code","9a754be3":"code","33b0f2ea":"code","67158ca4":"code","2b167240":"code","e69fcf34":"code","a5565647":"code","d4147a9a":"code","7676753c":"code","8c049c46":"code","d033822a":"code","98170964":"code","5347b309":"code","fb10126d":"code","aa3bb2bd":"code","b4c5385a":"code","1816892c":"code","f3676ed2":"code","f6b534ef":"code","929ed85e":"code","df8eac53":"code","daa0a4c1":"code","b0923764":"code","41126451":"code","bcbd3acd":"code","0026197b":"code","7e9b85fb":"code","c74cd1a0":"code","addc2f54":"code","5487b330":"code","f8eb75ce":"code","8cc8fdfa":"code","b4cfbb11":"code","96ce9b67":"code","67ae05bf":"code","b5410136":"code","9cfad64f":"code","4485cea5":"code","a62ff1a8":"code","56e83709":"code","7853e3c3":"code","f7b9f625":"code","b3b8fb1e":"code","0f68aec9":"markdown","d0dc3259":"markdown","1a1aa2a4":"markdown","60bc650c":"markdown","20578405":"markdown","61911d2f":"markdown","621180d7":"markdown","eef762d9":"markdown","6eb7fd5c":"markdown","ba0741e7":"markdown","8c09be9c":"markdown","240b1d5f":"markdown","0a2afff1":"markdown"},"source":{"42e9e902":"import tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_hub as hub\nfrom tensorflow.keras.layers import *\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport IPython\n\nfrom PIL import Image\nfrom PIL import ImageColor\nfrom PIL import ImageDraw\nfrom PIL import ImageFont\nfrom PIL import ImageOps\n\n\nimport tempfile\nfrom six.moves.urllib.request import urlopen\nfrom six import BytesIO\n\nimport os\nimport pathlib\n\n# For measuring the inference time.\nimport time\n\n# Check available GPU devices.\nprint(\"The following GPU devices are available: %s\" % tf.test.gpu_device_name())","6f949d3a":"# Check the embedding\nword = 'football'\nprint(embeddings_index[word].shape)","215de678":"data_dir = '..\/input\/flickr-image-dataset\/flickr30k_images'\nimage_dir = f'{data_dir}\/flickr30k_images'\ncsv_file = f'{data_dir}\/results.csv'","75bbf3c2":"df = pd.read_csv(csv_file, delimiter='|')\n\nprint(f'[INFO] The shape of dataframe: {df.shape}')\nprint(f'[INFO] The columns in the dataframe: {df.columns}')\nprint(f'[INFO] Unique image names: {len(pd.unique(df[\"image_name\"]))}')","0a12bcca":"df.columns = ['image_name', 'comment_number', 'comment']\ndel df['comment_number']\n\n# Under scrutiny I had found that 19999 had a messed up entry\ndf['comment'][19999] = ' A dog runs across the grass .'\n\n# Image names now correspond to the absolute position\ndf['image_name'] = image_dir+'\/'+df['image_name']\n\n# <start> comment <end>\ndf['comment'] = \"<start> \"+df['comment']+\" <end>\"","9a754be3":"# Shuffle the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","33b0f2ea":"SIZE = len(df)\n\ntrain_size = int(0.7* SIZE) \nval_size = int(0.1* SIZE)\ntest_size = int(0.2* SIZE)\n\ntrain_size, val_size, test_size","67158ca4":"# Enter different indices.\nindex = 10000\n\nimage_name = df['image_name'][index]\ncomment = df['comment'][index]\n\nprint(comment)\n\nIPython.display.Image(filename=image_name)","2b167240":"# Choose the top 5000 words from the vocabulary\ntop_k = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~')","e69fcf34":"# build the vocabulary\ntokenizer.fit_on_texts(df['comment'])","a5565647":"# This is a sanity check function\ndef check_vocab(word):\n    i = tokenizer.word_index[word]\n    print(f\"The index of the word: {i}\")\n    print(f\"Index {i} is word {tokenizer.index_word[i]}\")\n    \ncheck_vocab(\"<end>\")","d4147a9a":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","7676753c":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip -q glove.6B.zip","8c049c46":"EMBEDDING_DIM = 100\n\npath_to_glove_file = os.path.join(\n    os.path.expanduser(\"~\"), \"\/kaggle\/working\/glove.6B.100d.txt\"\n)\n\nembeddings_index = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\n\nword_index = tokenizer.word_index\nembedding_matrix = np.zeros((len(word_index), EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","d033822a":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(df['comment'])","98170964":"# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ncap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')","5347b309":"cap_vector.shape","fb10126d":"train_cap = cap_vector[:train_size] \nval_cap = cap_vector[train_size:train_size+val_size]\ntest_cap = cap_vector[train_size+val_size:]\n\ntrain_cap.shape, val_cap.shape, test_cap.shape","aa3bb2bd":"cap_train_ds = tf.data.Dataset.from_tensor_slices(train_cap)\ncap_val_ds = tf.data.Dataset.from_tensor_slices(val_cap)\ncap_test_ds = tf.data.Dataset.from_tensor_slices(test_cap)\n\ncap_train_ds, cap_val_ds, cap_test_ds","b4c5385a":"def load_img(image_path):\n    # Decode file as strings\n    img = tf.io.read_file(image_path)\n    # Decode image, (0 to 1)\n    img = tf.image.decode_jpeg(img, channels=3)\n    # Resize the image to specific height and width\n    img = tf.image.resize(img, (256, 256))\n    #img  = tf.image.per_image_standardization(img)\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img","1816892c":"img_name = df['image_name'].values\n\ntrain_img = img_name[:train_size] \nval_img = img_name[train_size:train_size+val_size]\ntest_img = img_name[train_size+val_size:]\n\ntrain_img.shape, val_img.shape, test_img.shape","f3676ed2":"img_train_ds = tf.data.Dataset.from_tensor_slices(train_img).map(load_img)\nimg_val_ds = tf.data.Dataset.from_tensor_slices(val_img).map(load_img)\nimg_test_ds = tf.data.Dataset.from_tensor_slices(test_img).map(load_img)\n\nimg_train_ds, img_val_ds, img_test_ds","f6b534ef":"train_ds = tf.data.Dataset.zip((img_train_ds, cap_train_ds))\nval_ds = tf.data.Dataset.zip((img_val_ds, cap_val_ds))\ntest_ds = tf.data.Dataset.zip((img_test_ds, cap_test_ds))","929ed85e":"# Cache, prefecth and batch the dataset\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 256\n\ntrain_ds = train_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)\ntest_ds = test_ds.shuffle(42).batch(BATCH_SIZE).cache().prefetch(buffer_size=AUTOTUNE)","df8eac53":"def display_image(image):\n  fig = plt.figure(figsize=(20, 15))\n  plt.grid(False)\n  plt.imshow(image)\n\n\ndef download_and_resize_image(url, new_width=256, new_height=256,\n                              display=False):\n  _, filename = tempfile.mkstemp(suffix=\".jpg\")\n  response = urlopen(url)\n  image_data = response.read()\n  image_data = BytesIO(image_data)\n  pil_image = Image.open(image_data)\n  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n  pil_image_rgb = pil_image.convert(\"RGB\")\n  pil_image_rgb.save(filename, format=\"JPEG\", quality=90)\n  print(\"Image downloaded to %s.\" % filename)\n  if display:\n    display_image(pil_image)\n  return filename\n\n\ndef draw_bounding_box_on_image(image,\n                               ymin,\n                               xmin,\n                               ymax,\n                               xmax,\n                               color,\n                               font,\n                               thickness=4,\n                               display_str_list=()):\n  \"\"\"Adds a bounding box to an image.\"\"\"\n  draw = ImageDraw.Draw(image)\n  im_width, im_height = image.size\n  (left, right, top, bottom) = (xmin * im_width, xmax * im_width,\n                                ymin * im_height, ymax * im_height)\n  draw.line([(left, top), (left, bottom), (right, bottom), (right, top),\n             (left, top)],\n            width=thickness,\n            fill=color)\n\n  # If the total height of the display strings added to the top of the bounding\n  # box exceeds the top of the image, stack the strings below the bounding box\n  # instead of above.\n  display_str_heights = [font.getsize(ds)[1] for ds in display_str_list]\n  # Each display_str has a top and bottom margin of 0.05x.\n  total_display_str_height = (1 + 2 * 0.05) * sum(display_str_heights)\n\n  if top > total_display_str_height:\n    text_bottom = top\n  else:\n    text_bottom = top + total_display_str_height\n  # Reverse list and print from bottom to top.\n  for display_str in display_str_list[::-1]:\n    text_width, text_height = font.getsize(display_str)\n    margin = np.ceil(0.05 * text_height)\n    draw.rectangle([(left, text_bottom - text_height - 2 * margin),\n                    (left + text_width, text_bottom)],\n                   fill=color)\n    draw.text((left + margin, text_bottom - text_height - margin),\n              display_str,\n              fill=\"black\",\n              font=font)\n    text_bottom -= text_height - 2 * margin\n\n\ndef draw_boxes(image, boxes, class_names, scores, max_boxes=10, min_score=0.1):\n  \"\"\"Overlay labeled boxes on an image with formatted scores and label names.\"\"\"\n  colors = list(ImageColor.colormap.values())\n  cl_names = []\n  try:\n    font = ImageFont.truetype(\"\/usr\/share\/fonts\/truetype\/liberation\/LiberationSansNarrow-Regular.ttf\",\n                              25)\n  except IOError:\n    print(\"Font not found, using default font.\")\n    font = ImageFont.load_default()\n\n  for i in range(min(boxes.shape[0], max_boxes)):\n    if scores[i] >= min_score:\n      ymin, xmin, ymax, xmax = tuple(boxes[i])\n      display_str = \"{}: {}%\".format(class_names[i].decode(\"ascii\"),\n                                     int(100 * scores[i]))\n      cl_names.append(display_str)  \n      color = colors[hash(class_names[i]) % len(colors)]\n      image_pil = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n      draw_bounding_box_on_image(\n          image_pil,\n          ymin,\n          xmin,\n          ymax,\n          xmax,\n          color,\n          font,\n          display_str_list=[display_str])\n      np.copyto(image, np.array(image_pil))\n  return image, cl_names","daa0a4c1":"module_handle = \"https:\/\/tfhub.dev\/google\/faster_rcnn\/openimages_v4\/inception_resnet_v2\/1\"\n\ndetector = hub.load(module_handle).signatures['default']","b0923764":"def run_detector(detector, img):\n  #img = load_img(path)\n\n  converted_img  = tf.image.convert_image_dtype(img, tf.float32)[tf.newaxis, ...]\n  start_time = time.time()\n  result = detector(converted_img)\n  end_time = time.time()\n\n  result = {key:value.numpy() for key,value in result.items()}\n\n  print(\"Found %d objects.\" % len(result[\"detection_scores\"]))\n  print(\"Inference time: \", end_time-start_time)\n\n  image_with_boxes, class_names = draw_boxes(\n      img.numpy(), result[\"detection_boxes\"],\n      result[\"detection_class_entities\"], result[\"detection_scores\"])\n\n  #display_image(image_with_boxes)\n  return class_names","41126451":"for img, cap in train_ds.take(1):\n    print(img.shape)\n    print(cap.shape)\n    #plt.imshow(img[0])\n    lst_images = run_detector(detector, img)\n    for c in cap[0]:\n        print(tokenizer.index_word[c.numpy()],end=' ')","bcbd3acd":"for i in range(len(lst_images)):\n    lst_images[i] = lst_images[i].split(':')[0].lower()\n    lst_images[i] = lst_images[i].split(' ')[0]\n    \n    \nprint(lst_images)    ","0026197b":"embeddings_index['mule']","7e9b85fb":"run_detector(detector, downloaded_image_path)","c74cd1a0":"# Some global variables\nEMBEDDIN_DIM = 100\nVOCAB_SIZE = 5000\nUNITS_RNN = 256","addc2f54":"# Checking the CNN\nencoder = CNN_Encoder(EMBEDDIN_DIM)\nfor image, caption in train_ds.take(1):\n    print(encoder(image).shape)\n    break","5487b330":"class RNN_Decoder(tf.keras.Model):\n    def __init__(self, embedding_dim, units, vocab_size):\n        super(RNN_Decoder, self).__init__()\n        self.units = units\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        self.embedding = Embedding(len(word_index),\n                                   EMBEDDING_DIM,\n                                   weights=[embedding_matrix],\n                                   input_length=80,\n                                   trainable=False)\n    \n    def build(self, input_shape):\n        self.gru = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.fc1 = Dense(self.units)\n        self.fc2 = Dense(self.vocab_size)\n\n    def call(self, x, initial_zero=False):\n        # x, (batch, 512)\n        # hidden, (batch, 256)\n        if initial_zero:\n            initial_state = decoder.reset_state(batch_size=x.shape[0])\n            output, state = self.gru(inputs=x,\n                                     initial_state=initial_state)\n        else:\n            output, state = self.gru(inputs=x)\n        # output, (batch, 256)\n        x = self.fc1(output)\n        x = self.fc2(x)\n        \n        return x, state\n    \n    def embed(self, x):\n        return self.embedding(x)\n    \n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","f8eb75ce":"# Checking the RNN\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)\nfor image, caption in train_ds.take(1):\n    features = tf.expand_dims(encoder(image),1) # (batch, 1, 128)\n    em_words = decoder.embed(caption)\n    x = tf.concat([features,em_words],axis=1)\n    print(x.shape)\n    predictions, state = decoder(x, True)\n    print(predictions.shape)\n    print(state.shape)","8cc8fdfa":"encoder = CNN_Encoder(EMBEDDIN_DIM)\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)","b4cfbb11":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n","96ce9b67":"@tf.function\ndef train_step(img_tensor, target):\n    # img_tensor (batch, 224,224,3)\n    # target     (batch, 80)\n    loss = 0\n    with tf.GradientTape() as tape:\n        features = tf.expand_dims(encoder(img_tensor),1) # (batch, 1, 128)\n        em_words = decoder.embed(target)\n        x = tf.concat([features,em_words],axis=1)\n        predictions, _ = decoder(x, True)\n\n        loss = loss_function(target[:,1:], predictions[:,1:-1,:])\n\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n    return loss","67ae05bf":"loss_plot = []","b5410136":"EPOCHS = 20\n\nfor epoch in range(EPOCHS):\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_ds.take(20)):\n        loss = train_step(img_tensor, target)\n        total_loss += loss\n        if batch % 10 == 0:\n            print (f'Epoch {epoch} Batch {batch} Loss {loss.numpy():.4f}')\n    # storing the epoch end loss value to plot later\n    loss_plot.append(loss)\n    print(f'Loss: {total_loss\/20:.4f}')","9cfad64f":"plt.plot(loss_plot)","4485cea5":"img, cap = next(iter(test_ds.take(1)))\n\nimg[0].shape, cap[0].shape","a62ff1a8":"img = tf.expand_dims(img[0],0)\ncap = tf.expand_dims(cap[0],0)\n\nimg.shape, cap.shape","56e83709":"feature = tf.expand_dims(encoder(img),1) # (1, 1, 128)\n\nfeature.shape","7853e3c3":"# For the image\nprediction, _ = decoder(feature, True)\nprint(prediction.shape)","f7b9f625":"word = tf.reshape(tokenizer.word_index['<start>'], shape=(1,1))\nem_words = decoder.embed(word)\nprint(em_words.shape)\n\nprediction, _ = decoder(em_words)\nidx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\nword = tokenizer.index_word[idx]\nprint(word)","b3b8fb1e":"count = 0\nwhile word != '<end>':\n    print(word, end=\" \")\n    if count > 20:\n        break\n    word_int = tf.reshape(tokenizer.word_index[word], shape=(1,1))  \n    em_words = decoder.embed(word_int)\n    prediction, _ = decoder(em_words)\n    idx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\n    word = tokenizer.index_word[idx]\n    count += 1\n\nplt.imshow(image[0])\nplt.show()","0f68aec9":"# Text Handling\n- Defined the size of the vocab which is `5000`.\n- Initialized the Tokenizer class.\n    - Standardized (all to lower case)\n    - Filters the punctuations\n    - Splits the text\n    - Creates the vocabulary (`<start>, <end> and <unk>` is defined)","d0dc3259":"Here we are padding the sentences so that each of the sentences are of the same length.","1a1aa2a4":"## Inference","60bc650c":"## Model\n### Show (Encoder)\n- InceptionV3: This will act like the feature extractor\n- Use an FC layer to extract the features of the image\n- The features will be used as the initial hidden state for the RNN\n\n### Tell (Decoder)\n- The initial hidden state is used\n- The text is embedded\n- Usage of an LSTM to produce softmax on the vocab\n- Loss with captions","20578405":"# Data\n- data_dir is the path to the images and the `results.csv`\n- image_dir is the path exculsively to the images\n- csv_file is the path to the `results.csv` file","61911d2f":"Here we fit the `tokenizer` object on the captions. This helps in the updation of the vocab that the `tokenizer` object might have.\n\nIn the first iteration the vocabulary does not start from `0`. Both the dictionaries have 1 as the key or value.","621180d7":"We use `Adam` as the optimizer.\n\nThe loss is `SparseCategoricalCrossentropy`, because here it would be inefficient to use one-hot-encoders are the ground truth. We will also use mask to help mask the `<pad>` so that we do not let the sequence model learn to overfit on the same.","eef762d9":"Here we read the csv file as a dataframe and make some observations from it.\nFor a quick EDA we are going to \n- check the shape of the dataframe\n- check the names of the columns\n- find out the unique image names there are","6eb7fd5c":"Sanity check for the division of datasets","ba0741e7":"# Imports\nThe following packages are imported:\n","8c09be9c":"# Image Handling\n- Load the image\n- decode jpeg\n- resize\n- standardize","240b1d5f":"A quick observation here is to see that the dataframe has `158915` elements but only `31783` image names. This means that there is a duplicacy involved. On further inspection we will see that each image has 5 unique captions attached to it ($31783\\times 5=158915$)\n\nWhile looking into the dataframe I found out that `19999` had some messed up entries. This has led me to manually change the entries in that row.","0a2afff1":"# Joint Dataset\nWe have the individual datasets with us. We need to zip the img and the cap dataset now."}}