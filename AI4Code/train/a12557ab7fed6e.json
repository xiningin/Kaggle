{"cell_type":{"f8d63e02":"code","0fadd7d7":"code","4c028e0f":"code","bf3dae63":"code","24029f37":"code","aa0814f6":"code","c9cce948":"code","ee54574d":"code","6350fd0d":"code","d4725657":"code","7939e48d":"code","09a6ced5":"code","7554a9fb":"code","9e76081f":"code","02ab02f5":"code","1fdd92a2":"code","4276afec":"code","d997223f":"code","c7821a9f":"code","c7c30574":"code","3f938768":"code","761941fa":"code","92601cf7":"code","6d05c03f":"code","e458bbd0":"code","5f7a1bf1":"code","126f34a5":"code","b1da140b":"code","5409f225":"code","4c97a38b":"code","8d8b9c01":"code","c19b4757":"code","be664be8":"code","7f903452":"code","aa71d0cf":"code","0dbb3042":"code","e8fef425":"code","3e94da6a":"code","b1b15563":"code","f508ef54":"code","c8f1d1fd":"code","2a3127e1":"code","0402b957":"code","600b4ca4":"code","cf843e5c":"code","f9bb23d6":"code","5e37c71d":"code","d4ec6dd0":"code","ae78b225":"code","67dc1e22":"code","c98b62fd":"code","31abd2c5":"code","461aa695":"code","02168090":"code","993e5e17":"code","7ac43904":"code","c1f7fbac":"code","56c4debe":"code","06982d53":"code","0c69cdfc":"code","7479b0a3":"code","882328d2":"code","d7146646":"code","09d6a8f5":"code","7098f3ef":"code","edf31593":"code","2c3128b9":"code","c06cc551":"code","930899cc":"code","9f15b709":"code","86286913":"code","8911c420":"code","9c4c3fc5":"code","9012d080":"code","fda486e4":"code","7a41f85b":"code","ffb1d1ff":"code","16c968d0":"markdown","ad5d8a57":"markdown","315f835d":"markdown","d7293f49":"markdown","1bad012e":"markdown","065b14ed":"markdown","e24c96fb":"markdown","aa8f0461":"markdown","2d900fdf":"markdown","c71201fb":"markdown","c242cf94":"markdown","1daf18c1":"markdown","aebdf75d":"markdown","30239da2":"markdown","c739883f":"markdown","58103136":"markdown","b89a22fe":"markdown","11c4bcee":"markdown","6f89d12b":"markdown","8ea69421":"markdown","f81bb311":"markdown","b3172399":"markdown","2c3b8e2e":"markdown","c236fca0":"markdown","f7ee437f":"markdown","a76b5d20":"markdown","68f70100":"markdown","bda735ba":"markdown","9bdca82c":"markdown","551875b9":"markdown","e1bfe70f":"markdown","2e651e98":"markdown","ebfa2c14":"markdown","5d35d6af":"markdown","672422ca":"markdown","4c35be33":"markdown","c94dc719":"markdown","94d59704":"markdown","d3c03f0f":"markdown","66a624d9":"markdown","5e8993c7":"markdown","e4c1cf50":"markdown","ee5f2de1":"markdown","485a9cbb":"markdown","637b2470":"markdown","7fa18a5c":"markdown","aed8a45c":"markdown","6199fb36":"markdown","fc1d06cb":"markdown","58ec38ea":"markdown"},"source":{"f8d63e02":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline","0fadd7d7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf=pd.read_csv('\/kaggle\/input\/fish-market\/Fish.csv')\ndf.head()","4c028e0f":"# df.dtypes\n# Use info() to get a quick description of the data\n#total number of rows, each attribute\u2019s type, and the number of nonnull values\ndf.info()","bf3dae63":"#print(df.isnull().sum())\ndf.shape","24029f37":"# Counts the number of each species\nspecies_counts = df[\"Species\"].value_counts().to_frame()\nspecies_counts.reset_index(inplace=True)\nspecies_counts.columns = [\"Species\",\"Counts\"] \nspecies_counts","aa0814f6":"#Visualize the proportion of each species in the dataset\nplt.bar(species_counts[\"Species\"], species_counts[\"Counts\"]\/sum(species_counts[\"Counts\"]))\nplt.title(\"Species Value Counts\")","c9cce948":"# Get a statistical summary of each column\ndf.describe()","ee54574d":"# Check the correlation (Pearson Correlation) between variables\nplt.figure(figsize=(10,5))\nc= df.corr()\nsns.heatmap(c,cmap=\"BrBG\",annot=True)\nc","6350fd0d":"from scipy import stats","d4725657":"pearson_coef, p_value = stats.pearsonr(df['Length1'], df['Weight'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)","7939e48d":"pearson_coef, p_value = stats.pearsonr(df['Length2'], df['Weight'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)","09a6ced5":"pearson_coef, p_value = stats.pearsonr(df['Length3'], df['Weight'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)","7554a9fb":"pearson_coef, p_value = stats.pearsonr(df['Height'], df['Weight'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)","9e76081f":"pearson_coef, p_value = stats.pearsonr(df['Width'], df['Weight'])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)","02ab02f5":"fig, ([ax1, ax2], [ax3, ax4], [ax5, ax6]) = plt.subplots(3, 2, figsize=(20,20))\nsns.regplot(x=\"Length1\", y=\"Weight\", data=df, ax=ax1)\nsns.regplot(x=\"Length2\", y=\"Weight\", data=df, ax=ax2)\nsns.regplot(x=\"Length3\", y=\"Weight\", data=df, ax=ax3)\nsns.regplot(x=\"Height\", y=\"Weight\", data=df, ax=ax4)\nsns.regplot(x=\"Width\", y=\"Weight\", data=df, ax=ax5)\nax1.tick_params(labelrotation=0, labelsize=20)\nax2.tick_params(labelrotation=0, labelsize=20)\nax3.tick_params(labelrotation=0, labelsize=20)\nax4.tick_params(labelrotation=0, labelsize=20)\nax5.tick_params(labelrotation=0, labelsize=20)\nax6.tick_params(labelrotation=0, labelsize=20)\nax1.set_ylim(0,)\nax2.set_ylim(0,)\nax3.set_ylim(0,)\nax4.set_ylim(0,)\nax5.set_ylim(0,)\nfig.delaxes(ax6)\nfig.tight_layout()","1fdd92a2":"# Count the value of each species\ndf['Species'].value_counts()","4276afec":"# Calculate the mean of each variables based on species\nspecies_group = df.groupby([\"Species\"], as_index=False).mean()\nspecies_group","d997223f":"fig, ([ax1, ax2], [ax3, ax4], [ax5, ax6]) = plt.subplots(3, 2, figsize=(20,20))\nsns.boxplot(x=\"Species\", y=\"Weight\", data=df, ax=ax1)\nsns.boxplot(x=\"Species\", y=\"Length1\", data=df, ax=ax2)\nsns.boxplot(x=\"Species\", y=\"Length2\", data=df, ax=ax3)\nsns.boxplot(x=\"Species\", y=\"Length3\", data=df, ax=ax4)\nsns.boxplot(x=\"Species\", y=\"Height\", data=df, ax=ax5)\nsns.boxplot(x=\"Species\", y=\"Width\", data=df, ax=ax6)\nax1.tick_params(labelrotation=0, labelsize=20)\nax2.tick_params(labelrotation=0, labelsize=20)\nax3.tick_params(labelrotation=0, labelsize=20)\nax4.tick_params(labelrotation=0, labelsize=20)\nax5.tick_params(labelrotation=0, labelsize=20)\nax6.tick_params(labelrotation=0, labelsize=20)\nfig.tight_layout()","c7821a9f":"lhw_df = df[[\"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\"]]","c7c30574":"sns.boxplot(x=df['Weight'])","3f938768":"labels = [\"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\"]\nplt.figure(figsize=(10,8))\nplt.boxplot(lhw_df, vert=False, patch_artist=True, labels = labels) \nplt.xlabel('centimeter (cm)')\n#plt.title('Multiple Box Plot')  ","761941fa":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","92601cf7":"df = df[~((df < (Q1 - 1.5 * IQR)) |(df > (Q3 + 1.5 * IQR))).any(axis=1)]\ndf.shape","6d05c03f":"df.loc[df.Weight < 5]","e458bbd0":"df.drop(40, inplace=True)\ndf.reset_index(drop=True, inplace=True)\ndf.shape","5f7a1bf1":"from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import cross_val_score","126f34a5":"# Will use this later to evaluate the cross-validation\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())","b1da140b":"features1=[\"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\"]\ny=df[\"Weight\"].values","5409f225":"# Normalize the data so all values are within the range of 0 and 1.\nscaler = MinMaxScaler()\nx = scaler.fit_transform(df[features1])\nx[0:5]","4c97a38b":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=0)\nfor train_index, test_index in sss.split(df, df[\"Species\"]):\n     x_train, x_test = x[train_index], x[test_index]\n     y_train, y_test = y[train_index], y[test_index]\nprint ('Train set:', x_train.shape,  y_train.shape)\nprint ('Test set:', x_test.shape,  y_test.shape)","8d8b9c01":"# Check the proportion of Species in the test set\nsss_test_set = df.loc[test_index]\nsss_test_set[\"Species\"].value_counts()\/len(sss_test_set)","c19b4757":"# Check the proportion of Species in the train set\nsss_train_set = df.loc[train_index]\nsss_train_set[\"Species\"].value_counts()\/len(sss_train_set)","be664be8":"# Check the proportion of Species in the whole data set\ndf[\"Species\"].value_counts()\/len(df)","7f903452":"lr=LinearRegression()\nlr.fit(x_train, y_train)\nprint ('Coefficients: ', lr.coef_)\nprint ('Intercept: ',lr.intercept_)","aa71d0cf":"r2scores = cross_val_score(lr, x_train, y_train, cv=4)\ndisplay_scores(r2scores)","0dbb3042":"print(\"Mean Square Error (MSE) of training data: %.2f\" % np.mean((lr.predict(x_train) - y_train) ** 2))\nprint(\"R2-score of training data: %.2f\" % r2_score(y_train, lr.predict(x_train)) )","e8fef425":"y_hat= lr.predict(x_test)\ny_hat","3e94da6a":"# Compare the y_hat with the actual results\nnp.asanyarray(y_test)","b1b15563":"print(\"Mean Square Error (MSE): %.2f\" % np.mean((y_hat - y_test) ** 2))\nprint(\"Root Mean Square Error (RMSE): %.2f\" % np.sqrt(np.mean((y_hat - y_test) ** 2)))\nprint(\"R2-score: %.2f\" % r2_score(y_test, y_hat) )\nprint(\"R2-score: %.2f\" % lr.score(x_test , y_test)) # this score should be the same as above","f508ef54":"# Compute the 95% confidence interval for the genaralization error\nconfidence = 0.95\nsquared_errors = (y_hat - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1, loc=squared_errors.mean(),\n    scale=stats.sem(squared_errors)))","c8f1d1fd":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\nnp.set_printoptions(suppress=True) # Surpress the scientific notation","2a3127e1":"# We use degree 2 since from the previous regression plot, it seems degree 2 will be a good model\npoly = PolynomialFeatures(degree=2)\nx_train_poly=poly.fit_transform(x_train)\nx_train_poly","0402b957":"lr1=LinearRegression()\nlr1.fit(x_train_poly, y_train)\nprint ('Coefficients: ',lr1.coef_)\nprint ('Intercept: ',lr1.intercept_)","600b4ca4":"# Use cross-validation to get an estimate of a model\u2019s generalization\nr2scores_poly = cross_val_score(lr1, x_train_poly, y_train, cv=4)\ndisplay_scores(r2scores_poly)","cf843e5c":"print(\"Mean Square Error (MSE) of training data: %.2f\" % np.mean((lr1.predict(x_train_poly) - y_train) ** 2))\nprint(\"R2-score of training data: %.2f\" % r2_score(y_train, lr1.predict(x_train_poly)) )","f9bb23d6":"x_test_poly=poly.fit_transform(x_test)\ny_hat_poly= lr1.predict(x_test_poly)\ny_hat_poly","5e37c71d":"np.asanyarray(y_test)","d4ec6dd0":"print(\"Residual sum of squares (MSE): %.2f\" % mean_squared_error(y_test, y_hat_poly))\nprint(\"Root Mean Square Error (RMSE): %.2f\" % np.sqrt(np.mean((y_hat_poly - y_test) ** 2)))\nprint(\"R2-score: %.2f\" % r2_score(y_test, y_hat_poly) )","ae78b225":"# Compute the 95% confidence interval for the genaralization error\nconfidence = 0.95\nsquared_errors_poly = (y_hat_poly - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors_poly) - 1, loc=squared_errors_poly.mean(),\n    scale=stats.sem(squared_errors_poly)))","67dc1e22":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV","c98b62fd":"features2=[\"Weight\", \"Length1\", \"Length2\", \"Length3\", \"Height\", \"Width\"]\ny2=df[\"Species\"].values","31abd2c5":"# Rescaling the distribution of values so that the mean of observed values is 0 and \n# the standard deviation is 1\nscaler_S = StandardScaler()\nx2 = scaler_S.fit(df[features2]).transform(df[features2].astype(float))\nx2[0:5]","461aa695":"x2_train, x2_test, y2_train, y2_test = train_test_split( x2, y2, test_size=0.2, random_state=0)\nprint ('Train set:', x2_train.shape,  y2_train.shape)\nprint ('Test set:', x2_test.shape,  y2_test.shape)","02168090":"# Find the best k\nKs = 31\nmean_acc = np.zeros((Ks-1))\nk_scores = []\n\nfor n in range(1,Ks):\n    \n    #Train Model and Predict  \n    neigh = KNeighborsClassifier(n_neighbors = n).fit(x2_train,y2_train)\n    \n    #obtain cross_val_score for KNeighborsClassifier with k neighbours\n    scores = cross_val_score(neigh, x2_train, y2_train, cv=4, scoring='accuracy')\n    \n    #append mean of scores for k neighbors to k_scores list\n    k_scores.append(scores.mean())\n    \n    #yhat_knn=neigh.predict(x2_test)\n    #mean_acc[n-1] = accuracy_score(y2_test, yhat_knn)\n\nprint(k_scores)","993e5e17":"max(k_scores)","7ac43904":"# Visualize the accuracy for k between 1 to 9\nplt.plot(range(1,Ks),k_scores,'g')\nplt.ylabel('Cross-Validation Accuracy')\nplt.xlabel('Number of Neighbors (K)')\nplt.tight_layout()\nplt.show()","c1f7fbac":"k_range = list(range(1, 31))\nprint(k_range)","56c4debe":"param_grid = dict(n_neighbors=k_range)\nprint(param_grid)","06982d53":"# instantiate the grid\ngrid = GridSearchCV(neigh, param_grid, cv=4, scoring='accuracy')\n# fit the grid with data\ngrid.fit(x2_train, y2_train)\n# view the complete results (list of named tuples)\n#grid.cv_results_\ngrid.best_params_","0c69cdfc":"# Use k = 1 for best accuracy\nk = 1 \nneigh = KNeighborsClassifier(n_neighbors = k).fit(x2_train,y2_train)\nneigh","7479b0a3":"# Use cross-validation to get an estimate of a model\u2019s generalization\naccuracy_scores_knn = cross_val_score(neigh, x2_train, y2_train, cv=4)\ndisplay_scores(accuracy_scores_knn)","882328d2":"print(\"Train set Accuracy: \", accuracy_score(y2_train, neigh.predict(x2_train)))","d7146646":"yhat_knn = neigh.predict(x2_test)\nyhat_knn","09d6a8f5":"# Compare the y_hat_knn with the actual results\ny2_test","7098f3ef":"print(\"Test set Accuracy: \", accuracy_score(y2_test, yhat_knn))","edf31593":"from sklearn.tree import DecisionTreeClassifier","2c3128b9":"# Preparing parameter for GridSearchCV\ncriterion = ['gini', 'entropy']\nmax_depth = [2,3,4,5,6,7,8,9,10,11,12]","c06cc551":"# Now we are creating a dictionary to set all the parameters options for different objects.\nparam_grid2 = dict(criterion=criterion, max_depth=max_depth)\n#param_grid2 = {'criterion':['gini','entropy'],'max_depth':[2,3,4,5,6,7,8,9,10,11,12]}","930899cc":"# instantiate the grid\ngrid2 = GridSearchCV(DecisionTreeClassifier(), param_grid2, cv=4,)\n# fit the grid with data\ngrid2.fit(x2_train, y2_train)\n# view the complete results (list of named tuples)\n#grid.cv_results_\ngrid2.best_params_","9f15b709":"# Use the best_params from the grid search\nspeciesTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\nspeciesTree.fit(x2_train,y2_train)","86286913":"# Use cross-validation to get an estimate of a model\u2019s generalization\naccuracy_scores_tree = cross_val_score(speciesTree, x2_train, y2_train, cv=4)\ndisplay_scores(accuracy_scores_tree)","8911c420":"print(\"Train set Accuracy: \", accuracy_score(y2_train, speciesTree.predict(x2_train)))","9c4c3fc5":"predTree = speciesTree.predict(x2_test)\npredTree","9012d080":"# Compare the predTree with the actual results\ny2_test","fda486e4":"print(\"Test set Accuracy: \", accuracy_score(y2_test, predTree))","7a41f85b":"# Visualize the decision tree\n#from six import StringIO\n#import pydotplus\nimport matplotlib.image as mpimg\nfrom sklearn import tree\nfrom sklearn.tree import export_graphviz ","ffb1d1ff":"import graphviz\nfilename = \"speciestree.png\"\ntargetNames = df[\"Species\"].unique().tolist()\nfeatureNames = df.columns[1:7]\ndata = tree.export_graphviz(speciesTree,feature_names=featureNames,\n                            class_names=targetNames,filled=True, rounded=True,  \n                            special_characters=True,\n                            out_file=None)\ngraph = graphviz.Source(data)\ngraph","16c968d0":"Note that the Whitefish has the smallest number of representation in the dataset.","ad5d8a57":"From the correllation data above, it seems that there is a strong correlation between the weight, lengths, height, and width with the smallest correlation value of 0.63. ","315f835d":"We use regression plot to see how linear is the relation between lengths, height, width and weight.","d7293f49":"<h3>Summary<\/h3>\n\n| Algorithm       | Accuracy |\n| ----------------| -------- |\n| KNN             | 0.84     |\n| Decision Tree   | 0.77     | ","1bad012e":"From the species_group data, we can see that each species has a unique characteristic. For example, Pike, which has the highest weight, has mean weight of 42 g, length 1, 2 and 3 of 42 cm, 45 cm, 49 cm respectively, and the height of 7 cm and width of 5 cm. It seems that the weight, lengths, height, and width are a good predictor of the type of species.","065b14ed":"Note that we also can use GridSearchCV to find the best k.","e24c96fb":"<h3>Model 1.1: Linear Regression<\/h3>","aa8f0461":"<h3>Summary<\/h3>\n\n| Algorithm            | RMSE    | $R^2$Score| \n| -------------------- | ------- | --------- | \n| Linear  Regression   | 104.46  | 0.85      | \n| Polynomial Regression| 40.72   | 0.98      | ","2d900fdf":"This dataset is a record of 7 common different fish species in fish market sales. The data is taken from https:\/\/www.kaggle.com\/aungpyaeap\/fish-market. The goal of the analysis is to predict (1) the weight and (2) the type of the fish.","c71201fb":"After checking the proportion of species in the test set and train set, then compare it with the proportion of species in the whole data set. We know that the Stratified Shuffle Split is working correctly.","c242cf94":"<h3>Prepare the Data for Training and Testing<\/h3>","1daf18c1":"Polynomial regression (degree 2) is a better model in comparison to the linear regression since it gives us lower MSE and higher R2-score. Moreover, the polynomial regression doesn't give us negative values prediction while linear regression does have few negative values prediction. ","aebdf75d":"The mean accuracy scores of the cross-validation of the Decision Tree model is 0.74, which is a little lower than that of the KNN model. However, the train set accuracy is 0.86. It shows that the model is not overfit. ","30239da2":"<h3>Removing incorrect data<\/h3> ","c739883f":"Evaluate our system on the Test Set","58103136":"<h4>Conclusion<\/h4>\nK-Nearest Neighbours seems to performs slightly better than the decision tree.  ","b89a22fe":"As we can see the k which provides the best mean accuracy is 1, 2, 7 ","11c4bcee":"Using boxplot, we can check if the weight, lengths, height, and width variables are a good predictor of the type of species.","6f89d12b":"We you used cross-validation to get an estimate of a model\u2019s generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then our model is overfitting. If it performs poorly on both, then it is underfitting","8ea69421":"<h1><center>Fish Market - Predicting the Weight and the Type of Fish<\/center><\/h1>","f81bb311":"<h2>Lenghts, height, and widht vs weight.<\/h2>","b3172399":"The mean accuracy scores of the KNN model is 0.79, which is good. However, the accuracy of the training set is 1.0. This shows that the model is overfit.","2c3b8e2e":"<h2>Lenghts, height, width, and weight vs species.<\/h2>","c236fca0":"<h2>Explore the data<\/h2>","f7ee437f":"On row 40, the roach has weight of 0.0 gram despite having moderate lengths, height, and width. Since the weight is something that we want to predict, we will just drop the whole row. ","a76b5d20":"<h2>1. Training and Testing: Using Lengths, Height, and Width to Predict Weight<\/h2> ","68f70100":"We can see that there are few outliers.","bda735ba":"<h2>2. Training and Testing: Using Weight, Lengths, Height, and Width to Predict the Type of Species <\/h2> ","9bdca82c":"<h3>Model 1.2: Polynomial Regression<\/h3>","551875b9":"From the above result, we can say that the correlation between lenghts, height, and width againts weight is statistically significant since the p-value is < 0.001, and the linear relationship is strong since they are above 0.72.","e1bfe70f":"Evaluate our system on the Test Set","2e651e98":"We see that the distributions of the lengths, hight, and width againts the species are a little overlaped. It shows that they are not a \"perfect\" predictors.","ebfa2c14":"If we compare the y_test and y_hat, the prediction did a pretty good job. However, for y_test < 20, the y_hat has negative value which is not we want.","5d35d6af":"Check the Pearson Correlation and P-value between lenghts, height, width, and weight.","672422ca":"The test accuracy of the Decision Tree is far lower than the accuracy of the test set of the KNN. Let's visualize it.","4c35be33":"Using Cross-Validation of 4 folds, the mean $R^2$ scores looks pretty good since the the mean is close to 0.90. Moreover, the value is very close to the performance of the training set, which is 0.91. This shows that the model is not underfit or overfit.  ","c94dc719":"Double-click <b>here<\/b> for other option.\n<!--\ndot_data = StringIO()\nfilename = \"speciestree.png\"\nfeatureNames = df.columns[1:7]\ntargetNames = df[\"Species\"].unique().tolist()\nout=tree.export_graphviz(speciesTree,feature_names=featureNames, \n                         out_file=dot_data, class_names= targetNames, filled=True, \n                         rounded=True, special_characters=True,rotate=False)  \ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png(filename)\nimg = mpimg.imread(filename)\nplt.figure(figsize=(50, 25))\nplt.imshow(img, interpolation='nearest')\n-->","94d59704":"From our previous <b>Species Value Counts Historgram<\/b>, we see that the proportion of each species is not equally represented in our dataset. Therefore, we use <b>StratifiedShuffleSplit<\/b> to split the train and test set based on the species.","d3c03f0f":"Evaluate our system on the Test Set","66a624d9":"<h3>Prepare the Data for Training and Testing<\/h3>","5e8993c7":"<h3>Removing some outliers.<\/h3>","e4c1cf50":"<h2>Reading the data<\/h2>","ee5f2de1":"<h3>Model 2.1: K-Nearest Neighbours (KNN)<\/h3>","485a9cbb":"Note that the weight is in gram (g), the lengths, height, and width is in centimeter (cm).","637b2470":"The mean $R^2$ scores of cross-validation method of the degree 2 polynomial regression model, which is 0.97, is better than that of the linear regression. Moreover, the value is close to the value of the training set, which is 0.98. This shows that the model is not underfit or overfit.  ","7fa18a5c":"<h2>Detecting Outliers<\/h2>","aed8a45c":"We choose not to use StratifiedShuffleSplit since in this case the test accuracy for KNN and Decision Tree is far worse than just using Random Sampling.","6199fb36":"<h3>Model 2.2: Decission Tree<\/h3>","fc1d06cb":"From the plots above, we can see that as lengths, height, and weight increase, the weight also increases. This indicates a positive direct correlation between these variables. Again, it looks like the  length, height, and widht is a good predictor of weight. However, note that the trends of the data is not really linear. They are a little curvy. So, instead of just using linear regression to fit the data, we can also try to use polynomial or non-linear regression.   ","58ec38ea":"Evaluate our system on the Test Set"}}