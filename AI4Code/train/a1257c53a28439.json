{"cell_type":{"ea1310e6":"code","e45cb11e":"code","595b4981":"code","51bccb53":"code","56306fb9":"code","77bfac50":"code","41a51a12":"code","fa9f69ef":"code","b568c681":"code","76f27485":"code","7c106708":"code","99a1cb6a":"code","8a97db6d":"code","a41b98e5":"code","85b07d40":"code","8d00e678":"code","9c698f83":"code","ff5ffbee":"code","0437e327":"code","091e8975":"code","7d547bfb":"code","310b44f0":"code","b85d26a7":"code","15424ed1":"code","9c2da72e":"code","0a7ede6a":"code","e21be974":"code","f92cd3b2":"code","d1157dfb":"code","967e0e85":"code","c73108ff":"code","e662b47e":"markdown","160b04cd":"markdown","c8402d40":"markdown"},"source":{"ea1310e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e45cb11e":"!unzip \/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip\n!unzip \/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip","595b4981":"data = pd.read_csv('train.tsv',sep = '\\t')\ndata.info()","51bccb53":"data['Sentiment'].value_counts(normalize = True)","56306fb9":"from sklearn.model_selection import train_test_split\ntrain_d, test_d, train_y, test_y = train_test_split(\n    data['Phrase'], data['Sentiment'], test_size=0.25, random_state=5)","77bfac50":"def get_dummies(labels, size = 5):\n    res = []\n    for i in labels:\n        temp = [0] * size\n        temp[i] = 1\n        res.append(temp)\n    return res\n\ntrain_labels, test_labels = get_dummies(train_y), get_dummies(test_y)","41a51a12":"!pip install pytorch_transformers","fa9f69ef":"from pytorch_transformers import BertTokenizer\nmodel_name = 'bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\n\nsample_sentence = train_d[0]\n\nprint(sample_sentence)\n\nprint(tokenizer.tokenize('[CLS]' + sample_sentence + '[SEP]'))\n\nprint(tokenizer.convert_tokens_to_ids(tokenizer.tokenize('[CLS]' + sample_sentence + '[SEP]')))\n\n\n# print(tokenizer.encode_plus(\n#                         sample_sentence,    # Sentence to encode.\n#                         add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n#                         max_length = 100,           # Pad & truncate all sentences.\n#                         pad_to_max_length = True,\n#                         return_attention_mask = True,   # Construct attn. masks.\n# #                         return_tensors = 'pt',     # Return pytorch tensors.\n#                    ))","b568c681":"from pytorch_transformers import BertTokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\ntokenized_text = [tokenizer.tokenize('[CLS]' + i + '[SEP]') for i in train_d]\ninput_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]","76f27485":"len(input_ids)","7c106708":"input_ids[0]","99a1cb6a":"for j in range(len(input_ids)):\n    i = input_ids[j]\n    if len(i) != 128:\n        input_ids[j].extend([0] * (128 - len(i))) #extend sentence to 512, but we dont need dat much","8a97db6d":"from torch.utils.data import DataLoader, TensorDataset\nimport torch\ntrain_set = TensorDataset(torch.LongTensor(input_ids),\n                         torch.FloatTensor(train_labels))\ntrain_loader = DataLoader(dataset = train_set, batch_size = 32, shuffle = True)","a41b98e5":"tokenized_text = [tokenizer.tokenize('[CLS]' + i + '[SEP]') for i in test_d]\ninput_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]\n\nfor j in range(len(input_ids)):\n    i = input_ids[j]\n    if len(i) != 128:\n        input_ids[j].extend([0] * (128 - len(i))) #extend sentence to 512, but we dont need dat much\n\ntest_set = TensorDataset(torch.LongTensor(input_ids),\n                         torch.FloatTensor(test_labels))\ntest_loader = DataLoader(dataset = test_set, batch_size = 32, shuffle = True)","85b07d40":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","8d00e678":"import torch.nn as nn\nimport torch.nn.functional as F\nfrom pytorch_transformers import BertModel\n\nclass fn_cls(nn.Module):\n    def __init__(self):\n        super(fn_cls, self).__init__()\n        self.model = BertModel.from_pretrained(model_name)\n        self.model.to(device)\n        self.dropout = nn.Dropout(0.1)\n        self.l1 = nn.Linear(768, 5) #768 is the bert-base hidden size\n    def forward(self, x, attention_mask = None):\n        outputs = self.model(x, attention_mask = attention_mask)\n        x = outputs[1]\n        x = x.view(-1, 768)\n        x = self.dropout(x)\n        x = self.l1(x)\n        return x        ","9c698f83":"from pytorch_transformers import BertModel\nmodel = BertModel.from_pretrained(model_name)\n# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","ff5ffbee":"from torch import optim\n\ncls = fn_cls()\ncls.to(device)\nlossF = nn.BCELoss()\nsigmoid = nn.Sigmoid()\noptimizer = optim.Adam(cls.parameters(), lr = 1e-5)","0437e327":"def predict(logits):\n    res = torch.argmax(logits, 1)\n    return res","091e8975":"def train_model(data, target):\n    correct = 0\n    cls.train()\n    data = data.to(device)\n    target = target.to(device)\n    mask = []\n    for sample in data:\n        mask.append([1 if i != 0 else 0 for i in sample])\n    mask = torch.Tensor(mask).to(device)\n    \n    output = cls(data, attention_mask = mask)\n    loss = lossF(sigmoid(output).view(-1, 5), target)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    pred = predict(output)\n    correct += (pred == predict(target)).sum().item()\n    \n    return correct,loss\n    ","7d547bfb":"def eval_model():\n    cls.eval()\n    correct = 0\n    total = 0\n    for batch_idx, (data, target) in enumerate(test_loader):\n        data = data.to(device)\n        target = target.to(device)\n        mask = []\n        for sample in data:\n            mask.append([1 if i != 0 else 0 for i in sample])\n        mask = torch.Tensor(mask).to(device)\n\n        output = cls(data, attention_mask=mask)\n        pred = predict(output)\n\n        correct += (pred == predict(target)).sum().item()\n        total += len(data)\n    return correct\/total","310b44f0":"from torch.autograd import Variable\nimport time\n\npre = time.time()\nepoch = 3\n\nfor i in range(epoch):\n    train_corrects = []\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data).to(device), Variable(target).view(-1, 5).to(device)\n        train_correct,loss = train_model(data, target)\n        train_corrects.append(train_correct)\n        if batch_idx % 1000 == 0:\n            test_correct = eval_model()\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\t Loss:{:.4f} \\tTraining_acc: {:.4f} \\tTesting_acc: {:.4f}'.format(\n                i+1, batch_idx, len(train_loader), 100. *\n                batch_idx\/len(train_loader), loss.item(),\n                sum(train_corrects)\/ ((batch_idx + 1) * 32),\n                test_correct))\n\n                   \nprint('time comsumed: ', time.time() - pre)","b85d26a7":"test_df = pd.read_csv('test.tsv', sep = '\\t')\ntest_df.info()","15424ed1":"len(test_df)","9c2da72e":"tokenized_text = [tokenizer.tokenize('[CLS]' + i + '[SEP]') for i in test_df['Phrase']]\ninput_ids = [tokenizer.convert_tokens_to_ids(i) for i in tokenized_text]\n\nfor j in range(len(input_ids)):\n    i = input_ids[j]\n    if len(i) != 128:\n        input_ids[j].extend([0] * (128 - len(i))) #extend sentence to 512, but we dont need dat much\n\noutput_data = TensorDataset(torch.LongTensor(input_ids))\noutput_loader = DataLoader(dataset = output_data, batch_size = 1, shuffle = False)","0a7ede6a":"from tqdm.notebook import tqdm\ncls.eval()\n\nres_pred = []\nfor batch_idx, (data) in enumerate(tqdm(output_loader)):\n    data = data[0].to(device)\n    \n    mask = []\n    for sample in data:\n        mask.append([1 if i != 0 else 0 for i in sample])\n    mask = torch.Tensor(mask).to(device)\n    \n    output = cls(data, attention_mask=mask)\n    pred = predict(output)\n    res_pred.append(pred)\n    ","e21be974":"res_final = [result.item() for result in res_pred]","f92cd3b2":"test_df['Sentiment'] = res_final\nresult_df = test_df[['PhraseId','Sentiment']]","d1157dfb":"input_id","967e0e85":"input_ids[0]","c73108ff":"result_df.to_csv('Submission.csv',index = False)","e662b47e":"in the train data, phrase is the sentence and Sentiment is the label","160b04cd":"BerT part","c8402d40":"build our classifer"}}