{"cell_type":{"330890c9":"code","64a3967c":"code","89beade9":"code","8cf46cb4":"code","542134f5":"code","dda99e27":"code","425469c3":"code","c9c1ac42":"code","9361a0a2":"code","ae17887e":"code","2449bcf8":"code","dd63b6d7":"code","ca0bd31d":"code","28cb7a63":"code","efc0e271":"code","545231ca":"code","e6dd89f2":"code","04d9971c":"code","fde31f77":"code","99cb3f94":"code","8dcc7a67":"code","5a79dcc9":"code","1c51e0ae":"code","5096bb2b":"code","7dbf6e29":"code","303c905b":"code","01c38516":"code","918973f4":"code","a7ad6ef2":"markdown","079c3b20":"markdown","9200315d":"markdown","49fbe9ee":"markdown","a6da39af":"markdown","a33a3104":"markdown","87a4492b":"markdown","3a24c857":"markdown","335aa33f":"markdown","765b3a8f":"markdown","7b467135":"markdown","974a58ed":"markdown","a30b16f9":"markdown","474e76f4":"markdown","e3431666":"markdown","a8e39c7f":"markdown","c1c4c713":"markdown","38990c8c":"markdown","7b23b988":"markdown"},"source":{"330890c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64a3967c":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport pylab as plot\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,KFold,cross_val_score\nfrom matplotlib import style\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier,XGBRFRegressor,XGBRegressor\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import cross_val_score as cvs\n","89beade9":"column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\nhouse_price = pd.read_csv('\/kaggle\/input\/boston-house-prices\/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\nhouse_price.head()","8cf46cb4":"house_price.shape","542134f5":"house_price.describe()","dda99e27":"#Plotting boxplots to see if there are any outliers in our data (considering data betwen 25th and 75th percentile as non outlier)\nfig, ax = plt.subplots(ncols=7, nrows=2, figsize=(15, 5))\nax = ax.flatten()\nindex = 0\nfor i in house_price.columns:\n    sns.boxplot(y=i, data=house_price, ax=ax[index])\n    index +=1\nplt.tight_layout(pad=0.4)\nplt.show()","425469c3":"sns.pairplot(house_price)","c9c1ac42":"#Establishing correlation in data\n\nfrom matplotlib import style\n#creating a correlation matrix\n\nstyle.use(\"classic\")\nsns.heatmap(house_price.corr(),annot=True,cmap='inferno')","9361a0a2":"x=house_price.drop(\"MEDV\",axis=1)\ny=house_price[\"MEDV\"]\nx.shape","ae17887e":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nstdsc = StandardScaler()\nX_train_std = stdsc.fit_transform(x)","2449bcf8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train_std, y, test_size=0.2, random_state=1)","dd63b6d7":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)","ca0bd31d":"from sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot\nforest = RandomForestRegressor(n_estimators=500, random_state = 0)\nforest.fit(X_train,y_train)\nimportances = forest.feature_importances_\nfeat_labels = house_price.columns[1:]\nindices = np.argsort(importances)[::-1]\n\n# plot feature importance\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[indices[f]], importances[indices[f]]))\n","28cb7a63":"#removing variables 'ZN' and 'CHAS' form data\nhouse_price = house_price.drop(['CHAS', 'CHAS', 'TAX', 'INDUS', 'NOX'], axis=1)","efc0e271":"house_price.head()","545231ca":"#fitting our model to train and test\nlm = LinearRegression()\nmodel = lm.fit(X_train,y_train)","e6dd89f2":"pred_y = lm.predict(X_test)\nY_compare_linear= pd.DataFrame({\"Actual\": y_test, \"Predict\": pred_y})\nY_compare_linear.head()","04d9971c":"print(r2_score(y_test,pred_y))\nprint(mean_absolute_error(y_test,pred_y))\nprint(mean_squared_error(y_test,pred_y))","fde31f77":"xgb=XGBRegressor(n_estimators=1000)\nxgb.fit(X_train,y_train)\nkfold=KFold(n_splits=5)\nres=cross_val_score(model,X_train,y_train,cv=kfold)","99cb3f94":"yp=xgb.predict(X_test)\nY_compare_XGBReg= pd.DataFrame({\"Actual\": y_test, \"Predict\": yp})\nY_compare_XGBReg.head()","8dcc7a67":"print(r2_score(y_test,yp))\nprint(mean_absolute_error(y_test,yp))\nprint(mean_squared_error(y_test,yp))\nprint(r2_score(y_test,yp))\nprint(mean_absolute_error(y_test,yp))\nprint(mean_squared_error(y_test,yp))","5a79dcc9":"knn = KNeighborsRegressor(n_neighbors=13)\nknn.fit(X_train,y_train)\nY_pred = knn.predict(X_test)\nY_compare_knn = pd.DataFrame({'Actual': y_test, 'Predicted': Y_pred})\nY_compare_knn.head()","1c51e0ae":"print(r2_score(y_test,Y_pred))\nprint(mean_absolute_error(y_test,Y_pred))\nprint(mean_squared_error(y_test,Y_pred))","5096bb2b":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_train,y_train)\nY_pred = rf.predict(X_test)\nY_compare_randomforrest = pd.DataFrame({'Actual': y_test, 'Predicted': Y_pred})\nY_compare_randomforrest.head() #displaying the comparision btween actual and predicted values of MEDV","7dbf6e29":"print(r2_score(y_test,Y_pred))\nprint(mean_absolute_error(y_test,Y_pred))\nprint(mean_squared_error(y_test,Y_pred))","303c905b":"fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(25, 4))\nax = ax.flatten()\nY_compare_linear.head(10).plot(kind='bar', title='Linear Regression', grid=True, ax=ax[0])\nY_compare_XGBReg.head(10).plot(kind='bar', title='XGBRegressor', grid=True, ax=ax[1])\nY_compare_randomforrest.head(10).plot(kind='bar', title='Random Forrest Regression', grid=True, ax=ax[2])\nY_compare_knn.head(10).plot(kind='bar', title='KNN Regression', grid=True, ax=ax[3])\nplt.show()","01c38516":"print('According to R squared scorring method we got below scores for out machine learning models:')\nmodelNames = ['Linear Regression', 'XGBRegressor', 'Random Forrest', 'K-Nearest Neighbour']\nmodelRegressors = [lm, xgb, rf, knn]\nmodels = pd.DataFrame({'modelNames' : modelNames, 'modelRegressors' : modelRegressors})\ncounter=0\nscore=[]\nfor i in models['modelRegressors']:\n    accuracy = cvs(i, X_train, y_train, scoring='r2', cv=5)\n    print('Accuracy of %s Regression model is %.2f' %(models.iloc[counter,0],accuracy.mean()))\n    score.append(accuracy.mean())\n    counter+=1","918973f4":"pd.DataFrame({'Model Name' : modelNames,'Score' : score}).sort_values(by='Score', ascending=True).plot(x=0, y=1, kind='bar', figsize=(15,5), title='Comparison of R2 scores of differnt models', )\nplt.show()","a7ad6ef2":"we can summarise that Random Forrest (r2 =  0.87) machine learning model gives the best score","079c3b20":"To fit a regression model, the features of interest are the ones with a high correlation with the target variable \u2018MEDV\u2019","9200315d":"Plotting compariasion of actual and predicted values of MEDV that we got using different machine learning models","49fbe9ee":"##  Preprocessing","a6da39af":"### Interpreting Data Description","a33a3104":"## K-Nearest Neighbour","87a4492b":"## Random Forrest Regression","3a24c857":"1. we can infer from the histogram that the \u2018MEDV\u2019 variable seems to be normally distributed but contain several outliers.\n2. We can spot a linear relationship between \u2018RM\u2019 and House prices \u2018MEDV\u2019","335aa33f":"## Train a model","765b3a8f":"### feature scaling","7b467135":"## Checking  outliers in the data","974a58ed":"### Data Exploration","a30b16f9":"## Evaluation ","474e76f4":"## Assessing feature importance with random forests","e3431666":"Scores (R squared) of different machine learning models using K-fold cross validation:","a8e39c7f":"## XGBRegressor","c1c4c713":"Columns CRIM, ZN, RM, DIS, PTRATIO, B, LSTAT and MEDV have outliers.","38990c8c":"1. There is no missing values\n2. Variable 'ZN' is 0 for 25th and 50th percentile that will result in skweed data\n3. Also for variable 'CHAS' it's 0 for 25th, 50th and 75th percentile that will also show us that data is highly skweed.","7b23b988":"## LinearRegression"}}