{"cell_type":{"d3e5ac30":"code","f6967fcd":"code","2cd79b9e":"code","f338f530":"code","cc7b4761":"code","3f06e597":"code","17589be0":"code","b0939ecd":"code","d55aafe0":"code","e7fa45ea":"code","ef6d0587":"code","ffb2905a":"code","8d810ce6":"code","c1166bc5":"code","2cfd00e1":"code","678f8fef":"code","6eff7920":"code","a5ad69d5":"code","a866a968":"code","74a2900b":"code","c1aa5100":"code","a275c0b2":"code","03223e6f":"code","10558e24":"code","42885d9d":"markdown","3c83d2af":"markdown","d5b5cb2f":"markdown","63583117":"markdown","0cd4be67":"markdown","88a7907e":"markdown","bba1edee":"markdown","e7f0fbfb":"markdown","26dd4e57":"markdown","0db31c1a":"markdown","4cac91c6":"markdown","c81c7fe9":"markdown"},"source":{"d3e5ac30":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest  = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nsub   = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\n\nprint ('Train Shape :', train.shape)\nprint ('test Shape :' , test.shape)","f6967fcd":"SEED = 42\nfrom tensorflow.random import set_seed\nfrom numpy.random import seed\nseed(SEED)\nset_seed(SEED)","2cd79b9e":"import xgboost as xgb\nX = train.drop('label', axis = 1).values\nY = train.label.values\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.1, random_state = SEED)\nprint (X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n\ntrain_dm = xgb.DMatrix(data = X_train, label = y_train)\ntest_dm  = xgb.DMatrix(data = X_test,  label = y_test)\n\nparams = {\n    'max_depth'             : 5, \n    'eta'                   : 0.03, \n    'min_child_weight'      : 50, \n    'num_boost_round'       : 250, \n    'objective'             :'multi:softprob', \n    'seed'                  : SEED, \n    'num_class'             : 10,\n    'silent'                : 1,\n    'colsample_bytree'      : 0.5\n}\n%time model  = xgb.train(params, train_dm, num_boost_round = params['num_boost_round'])","f338f530":"from sklearn.metrics import accuracy_score, classification_report\nprint ('TRAIN ACCURACY : ', accuracy_score(y_train, [x.argmax() for x in model.predict(train_dm)]))\nprint ('VAL ACCURACY : '  , accuracy_score(y_test,  [x.argmax() for x in model.predict(test_dm)]))","cc7b4761":"score_dm = xgb.DMatrix(data = test.values)\nsub_xgb = pd.Series([x.argmax() for x in model.predict(score_dm)], index  = np.arange(test.shape[0]) + 1).reset_index()\nsub_xgb.columns = sub.columns\nsub_xgb.to_csv('submission_xgboost.csv', index = False)","3f06e597":"X_train = train.drop('label', axis = 1).values.reshape(train.shape[0], 784).astype('float')\/255\ny_train = train.label.values\nX_test  = test.values.reshape(test.shape[0], 784).astype('float')\/255\nprint (X_train.shape, X_test.shape)","17589be0":"from tensorflow.keras.utils import to_categorical\ny_train = to_categorical(y_train)\nprint (y_train.shape)","b0939ecd":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\ndef baseline_model():\n    model = Sequential([\n        Dense(784, input_dim = (784), activation = 'relu'),\n        Dense(10, activation = 'softmax'),\n    ])\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model","d55aafe0":"model = baseline_model()\nmodel.summary()","e7fa45ea":"%time history = model.fit(X_train, y_train, validation_split = 0.1, epochs=40, batch_size=200, verbose=0)","ef6d0587":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()    ","ffb2905a":"import numpy as np\nsub_mlp = pd.Series([x.argmax() for x in model.predict(X_test)], index  = np.arange(test.shape[0]) + 1).reset_index()\nsub_mlp.columns = sub.columns\nsub_mlp.to_csv('submission_baseline.csv', index = False)","8d810ce6":"X_train = train.drop('label', axis = 1).values.reshape(train.shape[0], 28, 28, 1).astype('float')\/255\ny_train = to_categorical(train.label.values)\nX_test  = test.values.reshape(test.shape[0], 28, 28, 1).astype('float')\/255\nprint (X_train.shape, X_test.shape, y_train.shape)","c1166bc5":"from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n\ndef basic_cnn_model():\n    model = Sequential([\n        Conv2D(32, (5,5), input_shape = (28,28,1), activation = 'relu'),\n        MaxPooling2D(2,2),\n        Dropout(0.2),\n        Flatten(),\n        Dense(128, activation = 'relu'),\n        Dense(10, activation = 'softmax')\n    ])\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model","2cfd00e1":"model = basic_cnn_model()\nmodel.summary()","678f8fef":"%time history = model.fit(X_train, y_train, validation_split = 0.1, epochs=40, batch_size=200, verbose=0)","6eff7920":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","a5ad69d5":"# saving submission file\nsub_cnn = pd.Series([x.argmax() for x in model.predict(X_test)], index  = np.arange(test.shape[0]) + 1).reset_index()\nsub_cnn.columns = sub.columns\nsub_cnn.to_csv('submission_basic_CNN.csv', index = False)","a866a968":"def large_cnn_model():\n    model = Sequential([\n        Conv2D(32, (5,5), input_shape = (28,28,1), activation = 'relu'),\n        MaxPooling2D(2,2),\n        Conv2D(32, (3,3), activation = 'relu'),\n        MaxPooling2D(2,2),\n        Dropout(0.2),\n        Flatten(),\n        Dense(128, activation = 'relu'),\n        Dense(10, activation = 'softmax')\n    ])\n    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n    return model","74a2900b":"model = large_cnn_model()\nmodel.summary()","c1aa5100":"%time history = model.fit(X_train, y_train, validation_split = 0.1, epochs=40, batch_size=200, verbose=0)","a275c0b2":"# saving submission file\nsub_large_cnn = pd.Series([x.argmax() for x in model.predict(X_test)], index  = np.arange(test.shape[0]) + 1).reset_index()\nsub_large_cnn.columns = sub.columns\nsub_large_cnn.to_csv('submission_large_CNN.csv', index = False)","03223e6f":"import matplotlib.pyplot as plt\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LOSS')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()\n\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('ACCURACY')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","10558e24":"ls -l","42885d9d":"## 1. Basline Model - Multilayer Perceptron","3c83d2af":"# MNIST Recognizing Digits\nMNIST is the hello world dataset for Image Recognition or Object Detection. The objective is to detect the handwritten digit numbered 0 to 9. This notebook will build 4 models\n1. XGBoost Model \n2. Multi Layer Perceptron\n3. Basic CNN\n4. Large CNN\nWe will compare these models with accuracy metric. \n\nThe following image captures the performance of these four models as submissions. We can observe that XGBoost model is not far behind but a simple MLP outperforms XGBoost and CNNs improve the performance by a good amount.\n\n\n![Kaggle Performance of Models](https:\/\/lh3.googleusercontent.com\/qb8VQRuRjuzPIUp4764gfRkVaxYzdgQRWSmggiQXmyQX-M4nO7XcgwFo6nZ3FCATtSV5lxe9jrDPDsWrJ08pWgv2dyzA_HhKjTwRRchKj4TacEL8YSvYqw1HFyvVKk9f93ySWFtoqU6WZYh9UD63hxwN-xaiEZigFSzF0yvV93A4L2qeDZ8TIzN0hywpBCHOVrLfGqrpeDupdrwPo4eS96Q86WathAYspMT4TVVVdSp7tmPH00byAp4Olgb03Opry1roEp_3NPwSe6B5EFXfLr0jT3JDYY8kb__uO8kgc7w-8edkGx3jkB_Q-evPgpdCEANW6fJlP98OXt702koT329uGBIpsKUIb42H5ZKkEl7ErYIRwIsFBiCtJzxgKc66J0f6Pc_Xv_PfcghnoNH-okSgmMw4-T7xoCoyGrkmHta6VuHHqWe9_OBm8Jc6t9xss5vpN5ziFPD1eppmmB-B5AIpINmjp7-VgZ4Psjy1Ky2Eko4eLVE8bvwdXm1GTqHXhflOOYRMh4Rfaw3JUUkSHGZPqmdYdc0040HAgz1uvPfrP-rMSq05Xy7hy8RYzRf2iWV3dv1kHM94LLzImBcu3c4YfUfekUaNsU9gu9fK6ElT5qmH_oXcb6Pj8nx_hGSMmgZq3qLYoMLW7gw-kp7Zpz2uEH73O9T_fy9V5CsLOYUmRRxA7T4E1gEaeY09nx4=w1675-h814-no?authuser=0)\n\nThe details for these models are below","d5b5cb2f":"## 0. Pitting against XGBoost","63583117":"## Data Loadin' and Preppin'","0cd4be67":"## 2. Training Simple CNN","88a7907e":"Reshaping dataset to 28 x 28 pixel values and standardizing the output of the images by dividing by 255","bba1edee":"#### Fit Model","e7f0fbfb":"## 3. Training a Larger Convolutional Neural Network ","26dd4e57":"### Note\n* This time the loss and validation accuracy are closer for train and test sets for previous two versions\n* The model is trained with nearly 1\/5th of previous CNN params and 1\/6th of MLP's params\n* The training time even though the parameters were less was higher","0db31c1a":"setting SEED values to make results reproducible","4cac91c6":"### Note\n* The curves for these plots are much more closer for CNN version than the multilayer perceptron version\n* The model trained with less number of parameters than the MLP version\n* The training time was slightly higher than MLP version","c81c7fe9":"One hot encoding of label "}}