{"cell_type":{"28562189":"code","dba836da":"code","12f3b00c":"code","ff54738b":"code","c7d2c79d":"code","5a9d9e48":"code","ad67458a":"code","1bb54578":"code","61c433ef":"code","bf2dd9d9":"code","31ebba37":"code","66a6266b":"code","ce3b4a46":"code","c70620e0":"code","d133f882":"code","251de934":"code","a56bc11f":"code","85ab7dad":"code","5444b462":"code","28ce6575":"code","0e430b83":"code","2b686b70":"code","58d7cc1d":"code","02423cee":"code","3af25697":"code","4d86a677":"code","bc2a1bc5":"code","8f88ec94":"markdown","606147d2":"markdown","6a4de372":"markdown","fb06ecc1":"markdown","00a2f036":"markdown","ee58e874":"markdown","da5294a5":"markdown","363e4717":"markdown","a656bfdc":"markdown","c74c9a96":"markdown","40b54133":"markdown","3978a963":"markdown","6bfd66dc":"markdown","785b51e5":"markdown"},"source":{"28562189":"# linear algebra\nimport numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nfrom sklearn import preprocessing\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","dba836da":"traindf = pd.read_csv('..\/input\/titanic\/train.csv').set_index('PassengerId')\ntestdf = pd.read_csv('..\/input\/titanic\/test.csv').set_index('PassengerId')","12f3b00c":"traindf.info()","ff54738b":"traindf.head()","c7d2c79d":"traindf.isnull().sum().sort_values(ascending=False)","5a9d9e48":"sns.barplot(x='Pclass', y='Survived', data=traindf)","ad67458a":"sns.barplot(x='SibSp', y='Survived', data=traindf)","1bb54578":"sns.barplot(x='Parch', y='Survived', data=traindf)","61c433ef":"sns.barplot(x='Sex', y='Survived', data=traindf)","bf2dd9d9":"sns.barplot(x='Embarked', y='Survived', data=traindf)","31ebba37":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = traindf[traindf['Sex']=='female']\nmen = traindf[traindf['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","66a6266b":"FacetGrid = sns.FacetGrid(traindf, row='Embarked', height=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","ce3b4a46":"data = [traindf, testdf]\n\nfor dataset in data:\n    mean = traindf[\"Age\"].mean()\n    std = traindf[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    print(age_slice)\n    dataset[\"Age\"] = dataset[\"Age\"].astype(int)\ntraindf[\"Age\"].isnull().sum()","c70620e0":"common_value = 'S'\ndata = [traindf, testdf]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(common_value)","d133f882":"le = preprocessing.LabelEncoder()\nle.fit(traindf['Embarked'])\ntraindf['Embarked']=le.transform((traindf['Embarked']))\ntestdf['Embarked']=le.transform((testdf['Embarked']))","251de934":"genders = {\"male\": 0, \"female\": 1}\ndata = [traindf, testdf]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","a56bc11f":"data = [traindf, testdf]\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in data:\n    # extract titles\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    # replace titles with a more common title or as Rare\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    # convert titles into numbers\n    dataset['Title'] = dataset['Title'].map(titles)\n    # filling NaN with 0, to get safe\n    dataset['Title'] = dataset['Title'].fillna(0)","85ab7dad":"sns.barplot(x='Title', y='Survived', data=traindf)","5444b462":"data = [traindf, testdf]\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 10, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 15), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 15) & (dataset['Age'] <= 20), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 28), 'Age'] = 3\n    dataset.loc[(dataset['Age'] > 28) & (dataset['Age'] <= 35), 'Age'] = 4\n    dataset.loc[(dataset['Age'] > 35) & (dataset['Age'] <= 40), 'Age'] = 5\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 66), 'Age'] = 6\n    dataset.loc[ dataset['Age'] > 66, 'Age'] = 6\n\n# let's see how it's distributed \ntraindf['Age'].value_counts()","28ce6575":"traindf = traindf.drop(columns=['Name','Ticket','Cabin','Fare'])\ntestdf= testdf.drop(columns=['Name','Ticket','Cabin','Fare'])","0e430b83":"traindf.isnull().sum().sort_values(ascending=False)","2b686b70":"X_train = traindf.drop(\"Survived\", axis=1)\nY_train = traindf[\"Survived\"]\nX_test  = testdf","58d7cc1d":"svcmodel = SVC(C=0.1, gamma=1, kernel='poly')\nsvcmodel.fit(X_train,Y_train)\nRandommodel = RandomForestClassifier()\nRandommodel.fit(X_train,Y_train)\nY_predsvc = svcmodel.predict(X_test)\nY_predR = Randommodel.predict(X_test)","02423cee":"#Use cross validation for evaluating the model\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nk_fold = KFold(n_splits=10, shuffle=True, random_state=0)","3af25697":"score = cross_val_score(svcmodel,X_train, Y_train, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(\"SVC score:{}\".format(score.mean()))\nscore1 = cross_val_score(Randommodel,X_train, Y_train, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(\"Random forst score {}:\".format(score1.mean()))\n\n\n","4d86a677":"submission1 = pd.DataFrame(columns = ['PassengerId','Survived'])\nsubmission1['PassengerId'] = testdf.index\nsubmission1['Survived'] = Y_predsvc\nsubmission1.to_csv('SVM.csv',index = False)","bc2a1bc5":"submission2 = pd.DataFrame(columns = ['PassengerId','Survived'])\nsubmission2['PassengerId'] = testdf.index\nsubmission2['Survived'] = Y_predR\nsubmission2.to_csv('RF.csv',index = False)","8f88ec94":"Now we are done with the plotting and we have gotten insights from data which we can use when feature engineering and feature selection stages","606147d2":"Now the next step is preprocessing the data like filling nan values,encoding the categorical values ","6a4de372":"First we plot Passenger class with the survival rate and see there is indeed some relation and people with 1st class have high survival and this is explainable for obvious reasons.Similarly we plot each feature with survival rate and try to find features that are relevant. ","fb06ecc1":"Now as we can see the age has some range of values and from the plot we saw that a particular range of age had same results.For exaple children upto age 10 haave high chabces of survival so we made them a single category","00a2f036":"Clearly we see a pattern here so we will use this as feature too","ee58e874":"Now let us check if we still have any missing value","da5294a5":"# Titanic Survival prediction \n","363e4717":"In This tutorial we are going to explore the titanic dataset and then use Support Vector Classifier(SVC) and [Random Forest](https:\/\/www.mygreatlearning.com\/blog\/random-forest-algorithm) to predict if the person survived or not in the test dataset","a656bfdc":"As you may have seen the data for this particular problem has been provided as train set and test set.We have to train the model with train set and then predict the outcomes of test set.After that we submit our results and get a score which ranges from 0-1 and 1 being the highest.\nSo now we import the data and save them in Pandas dataframe as to manipulate the data\/","c74c9a96":"As you can see, we didnt use the name feature above as it didnt make sense to plot each name against its survival rate.But now what we can do is to extract the titles of each person from their names, encode them and see if we can get ameaningful feature","40b54133":"The very first thing we are going to do is explore the dataset and plot the graphs as to know which features are relevant to the survival of a person.For example as you ll see below, we plot the genders of people against their survival rate and it can be concluded that number of males which suffered this tragic death were more as compared to females.This can be explainable as at such times people try to evacuate women and childern first.So yes the gender of a person is a relevant feature.Now let us move ahead and import all the libraries which we may need. ","3978a963":"From here on we train the model, get predictions and then save the predictions as a csv file that we can download from the output section on the right section of notebook and submit the results.","6bfd66dc":"Now after all preprocessing, we can see that some features have many missing values or have no relevance that we can find out so we drop them out.","785b51e5":"As we can see there are some entries missing and we will fill them after analyzing and plotting the data we have. "}}