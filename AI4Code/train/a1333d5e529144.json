{"cell_type":{"25fe4092":"code","aa29e7f5":"code","1b80068d":"code","c49017ff":"code","65155e77":"code","cf99c569":"code","ebd8cb1f":"code","d12c6723":"code","11171352":"code","11327d4f":"code","46344b75":"code","adaad060":"code","7dba909f":"code","06ed1bf1":"code","78480e88":"code","36dec1fd":"code","7f483980":"code","90c12229":"code","8efb3f0e":"code","e6a6a84e":"code","89fb318c":"code","353f477e":"code","161ac3c2":"code","6a099990":"code","358cbbd2":"code","5baa4085":"code","7529981a":"code","eebd8189":"code","d92edbdf":"code","0aaa6ad5":"code","b811bd03":"code","f533a68b":"code","3dc2d35c":"code","b59cfeb2":"code","466b0f22":"markdown","f85aeba1":"markdown","e13597af":"markdown","cc0c08b5":"markdown","d52e29ab":"markdown","a03133b0":"markdown","fd3b7def":"markdown","3f517ad9":"markdown","10a11b4e":"markdown","94163086":"markdown","f8b3fadc":"markdown","fa4510f9":"markdown","1289fcbd":"markdown","ca4cc4ce":"markdown","a38cf7b8":"markdown","d1051d30":"markdown","6c15b0c3":"markdown","7dc8366d":"markdown","08649b7e":"markdown","0521d54b":"markdown","3e6c3bfb":"markdown","8b522c45":"markdown","1d026260":"markdown","e228fa85":"markdown","e9473ca4":"markdown","3dffd27c":"markdown","876409e2":"markdown","a70eb9d8":"markdown","3d2693cc":"markdown","9225e8d6":"markdown","801c8891":"markdown"},"source":{"25fe4092":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","aa29e7f5":"# import the libraries\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\n# hyperparameter tuning\nimport optuna\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","1b80068d":"# read the datas\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\", index_col = 0)\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\", index_col = 0)","c49017ff":"train.head()","65155e77":"test.head()","cf99c569":"# get the number of rows.\ntrain_row,train_col = train.shape\ntest_row, test_col = test.shape\nprint(f\"Number of rows in training dataset------------->{train_row}\\nNumber of columns in training dataset---------->{train_col}\\n\")\nprint(f\"Number of rows in testing dataset-------------->{test_row}\\nNumber of columns in testing dataset----------->{test_col}\")","ebd8cb1f":"print(train.info())\nprint(\"=\"*50)\ntest.info()","d12c6723":"train.describe()","11171352":"train.corr()","11327d4f":"train.corrwith(train['claim'])","46344b75":"test.describe()","adaad060":"plot , ax = plt.subplots(figsize=(10,8))\nsns.heatmap(train.corr())","7dba909f":"plot , ax = plt.subplots(figsize=(10,8))\n\nsns.countplot(train['claim'])","06ed1bf1":"\nhist = train.hist(bins = 25, figsize=(70,45))","78480e88":"features = train.columns.tolist()[0:-1]\ntarget = ['claim']","36dec1fd":"train_row,train_col = train.shape\ntest_row, test_col = test.shape\n#find the missing values w.r.t. column\ntrain_colum_missing = train.isnull().sum()\ntrain_total_missing = sum(train_colum_missing)\n# find the missing values w.r.t. row(number of missing values in the particular row)\ntrain_row_missing = train[features].isnull().sum(axis=1)\n\n# add the missing values to row to the dataframe as a new value\ntrain['no_of_missing_data'] = train_row_missing\n\n\n\n#find the missing values w.r.t. column\ntest_colum_missing = test.isnull().sum()\ntest_total_missing=sum(test_colum_missing)\n# find the missing values w.r.t. row(number of missing values in the particular row)\ntest_row_missing = test[features].isnull().sum(axis=1)\n\n# add the missing values to row to the dataframe as a new value\ntest['no_of_missing_data'] = test_row_missing\n","7f483980":"print(f\"Total number of missing values in training dataset---->{train_total_missing}\")\nprint(f\"Total number of missing values in testing dataset----->{test_total_missing}\")\n# compare this to the whole data\ntrain_no_of_missing_rows = (train['no_of_missing_data'] != 0).sum()\nprint(\"\\n{0:{fill}{align}80}\\n\".format(\"Training Data\" , fill = \"=\", align = \"^\"))\nprint(f\"Total rows -----------------------> {train_row}\\nNumber of rows has missing data---> {train_no_of_missing_rows}\\n{'-'*50}\\nNumber of rows has full data--------> {train_row-train_no_of_missing_rows}\")\n\ntest_no_of_missing_rows = (test['no_of_missing_data'] != 0).sum()\nprint(\"\\n{0:{fill}{align}80}\\n\".format(\"Testing Data\" , fill = \"=\", align = \"^\"))\nprint(f\"Total rows -----------------------> {test_row}\\nNumber of rows has missing data---> {test_no_of_missing_rows}\\n{'-'*50}\\nNumber of rows has full data--------> {test_row-test_no_of_missing_rows}\")","90c12229":"# here i am going to use the media for now.\nimputer = SimpleImputer(missing_values = np.nan, strategy = \"mean\")\ntrain.iloc[:,0:-1]  = pd.DataFrame(imputer.fit_transform(train.iloc[:,0:-1]), columns = train.columns.tolist()[0:-1], index = train.index)\ntest = pd.DataFrame(imputer.transform(test),columns = test.columns, index = test.index)","8efb3f0e":"plot , ax = plt.subplots(figsize=(6,3))\n\nsns.countplot(train['claim'])","e6a6a84e":"# scaler = StandardScaler()\n# train.iloc[:,0:-1] = pd.DataFrame(imputer.fit_transform(train.iloc[:,0:-1]), columns = train.columns.tolist()[0:-1], index = train.index)\n# test = pd.DataFrame(imputer.transform(test), columns = test.columns, index = test.index)","89fb318c":"def feature_adding(data):\n    data[\"min\"] = data.min(axis = 1) # 1 or \u2018columns\u2019 : get mode of each row.\n    data['max'] = data.max(axis = 1)\n    data['std'] = data.std(axis = 1)\n    data['median'] = data.median(axis = 1)\n    data['mean'] = data.mean(axis = 1)\n    #data['mode'] = data.mode(axis = 1)\n    data['mad'] = data.mad(axis = 1) # mean absolute deviation\n    data['skew'] = data.skew(axis=1)\n     # no of missing data already added in the handling missing value section.\n    # scale the data\n    features = data.columns.tolist()\n    scaler = StandardScaler()\n    data[features] = pd.DataFrame(imputer.fit_transform(data[features]), columns = data.columns.tolist(), index = data.index)\n    \n    \n    return data\n    ","353f477e":"train_y = train['claim'].copy()\ntrain_X = feature_adding(train.drop('claim', axis = 1))\ntest = feature_adding(test)","161ac3c2":"del(train)","6a099990":"xgb_params = {\n    'max_depth': 2,\n    'booster': 'gbtree', \n    'n_estimators': 10000,\n    'random_state':42,\n    'tree_method':'gpu_hist',\n    'gpu_id':0,\n    'predictor':\"gpu_predictor\",\n}","358cbbd2":"#Setting the kfold parameters\nn_fold = 15\nkf = KFold(n_splits = n_fold, shuffle = True, random_state = 42)\npred = 0\nresults = np.zeros((train_X.shape[0],))\nmean_acc = 0\n\nxgb_model = XGBClassifier(**xgb_params)\n\n\nfor fold, (train_id, valid_id) in enumerate(kf.split(train_X)):\n    X_train, X_val = train_X.loc[train_id],train_X.loc[valid_id]\n    y_train, y_val = train_y.iloc[train_id], train_y.iloc[valid_id]\n    \n    \n    xgb_model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_val, y_val)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 100)\n    \n    \n    #Out of Fold predictions\n    results=  xgb_model.predict_proba(X_val) \n    \n    pred += xgb_model.predict_proba(test)[:,1] \/ n_fold\n    \n    fold_acc = roc_auc_score(y_val ,results[:,1])\n    \n    print(f\"Fold {fold} | Fold accuracy: {fold_acc}\")\n    \n    mean_acc += fold_acc \/ n_fold\n    \nprint(f\"\\nOverall Accuracy: {mean_acc}\")","5baa4085":"#Setting the kfold parameters\nn_fold = 5\nkf = KFold(n_splits = n_fold, shuffle = True, random_state = 42)\npred = 0\nresults = np.zeros((train_X.shape[0],))\nmean_acc = 0\n\nlog_reg = LogisticRegression(n_jobs = -1,C = 0.01, penalty = 'l2', random_state = 42)\n\n\nfor fold, (train_id, valid_id) in enumerate(kf.split(train_X)):\n    X_train, X_val = train_X.loc[train_id], train_X.loc[valid_id]\n    y_train, y_val = train_y.iloc[train_id], train_y.iloc[valid_id]\n    \n    \n    log_reg.fit(X_train, y_train,\n             )\n    \n    \n    #Out of Fold predictions\n    results=  log_reg.predict_proba(X_val) \n    \n    pred += log_reg.predict_proba(test)[:,1] \/ n_fold\n    \n    fold_acc = roc_auc_score(y_val ,results[:,1])\n    \n    print(f\"Fold {fold} | Fold accuracy: {fold_acc}\")\n    \n    mean_acc += fold_acc \/ n_fold\n    \nprint(f\"\\nOverall Accuracy: {mean_acc}\")","7529981a":"X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, random_state=0, test_size = 0.1)\n\n","eebd8189":"def objective(trial):\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 5000, 50000, 500),\n        'max_depth': trial.suggest_int('max_depth', 2, 8),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 100),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 1.0),\n        'gamma': trial.suggest_loguniform('gamma', 1e-8, 100.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-8, 100.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-8, 100.0),\n        \n        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.5, log=True),\n    }\n    \n    #XGBoost model fitting.\n    xgb = XGBClassifier(**param, \n                         random_state=24,\n                         tree_method='gpu_hist' , \n                         gpu_id = 0, \n                         predictor=\"gpu_predictor\",)\n    \n    \n    xgb.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_val, y_val)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 100)\n    \n    # precdition .\n    xgb_pred = xgb.predict_proba(X_val)\n    \n    acc = roc_auc_score(y_val, xgb_pred[:,1])\n    return acc","d92edbdf":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=40) \n\nprint(\"lenth of the finished trials: \", len(study.trials))\n\n\nprint(\"Best value for the rmse:\", study.best_trial.value)\nprint(\"Best parameters:\", study.best_params)","0aaa6ad5":"best_param = study.best_params\nbest_param\n\n\"\"\"{'n_estimators': 29000,\n 'max_depth': 3,\n 'min_child_weight': 66,\n 'subsample': 0.7842215931394566,\n 'colsample_bytree': 0.5179071234426729,\n 'gamma': 0.4700525265587939,\n 'alpha': 0.000257843578177255,\n 'lambda': 2.4671217994166645,\n 'learning_rate': 0.012418427180920295}\"\"\"","b811bd03":"#Setting the kfold parameters\nn_fold = 5\nkf = KFold(n_splits = n_fold, shuffle = True, random_state = 42)\npred = 0\nresults = np.zeros((train_X.shape[0],))\nmean_acc = 0\n\nxgb_model = XGBClassifier(**best_param,               \n      booster= 'gbtree', \n    random_state = 42,\n    tree_method = 'gpu_hist',\n    gpu_id= 0,\n    predictor=\"gpu_predictor\",)\n\n\nfor fold, (train_id, valid_id) in enumerate(kf.split(train_X)):\n    X_train, X_val = train_X.loc[train_id],train_X.loc[valid_id]\n    y_train, y_val = train_y.iloc[train_id], train_y.iloc[valid_id]\n    \n    \n    xgb_model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_val, y_val)],\n             eval_metric = \"auc\",\n             early_stopping_rounds = 100)\n    \n    \n    #Out of Fold predictions\n    results=  xgb_model.predict_proba(X_val) \n    \n    pred += xgb_model.predict_proba(test)[:,1] \/ n_fold\n    \n    fold_acc = roc_auc_score(y_val ,results[:,1])\n    \n    print(f\"Fold {fold} | Fold accuracy: {fold_acc}\")\n    \n    mean_acc += fold_acc \/ n_fold\n    \nprint(f\"\\nOverall Accuracy: {mean_acc}\")","f533a68b":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","3dc2d35c":"sub['claim'] = pred","b59cfeb2":"sub.to_csv('submission3.csv',index=False)","466b0f22":"<a id=\"section_imbalance\"><\/a>\n## 4.3. Handle imbalance dataset","f85aeba1":"<a id=\"section_intro\"><\/a>\n# 1. Introduction","e13597af":"<a id=\"section_intro\"><\/a>\n# 3. Data Visualization","cc0c08b5":"<a id=\"section_intro\"><\/a>\n## 1.2. Pre-request","d52e29ab":"### Number of groups\nFrom the below plot, we can see there equal number of traget values present in the training data.so, our dataset is a balansed dataset.","a03133b0":"#### after feature engineeering \nscore with default parameter and Standarscalar and median imputer =  0.8152311464421033 <br\/>\nscore with default parameter and Normalization and constant zero imputer =  0.8153624191860824 <br\/>\nscore with default parameter and Standarscalar and mean imputer =  0.8154707510791169 <br\/>\nmean is higher so talking that one.","fd3b7def":"<a id=\"section_missingvalues\"><\/a>\n## 4.1. Handling Missing Values","3f517ad9":"Hi all.....I am a begineer in ML and DL needs lots of fun and resource to learn more in this field. I searched for a community which teach me ML with fun and motivation, I found the answer \"Kaggle\". This is the great community with lots of compatitions, rewards and lots of learning resources which specially made for ML, DL and AI. So, I started learning with Kaggle. I participated in the \"30 day ML\" and found that one was very helpful for me to keep consistent learning. This is my second compatition before going into real world ones:)\n<br\/>\n### Compatition overview:\nThe goal of these competitions is to provide a fun, and approachable for anyone, tabular dataset. These competitions will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition. If you're an established competitions master or grandmaster, these probably won't be much of a challenge for you. We encourage you to avoid saturating the leaderboard.<br\/>\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting whether a claim will be made on an insurance policy. Although the features are anonymized, they have properties relating to real-world features.\n\nGood luck and have fun!\n### Prediction:\nFor this competition, you will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values.\n\nFiles\ntrain.csv - the training data with the target claim column<br>\ntest.csv - the test set; you will be predicting the claim for each row in this file<br>\nsample_submission.csv - a sample submission file in the correct format","10a11b4e":"Adding some feature or reducing some feature by selcting the most important feature(Feature Selection) is called feature engineering.<br\/>\nHere we are going to add some features like min, max, mean, median, mode, standard diviation number of missing values in the row. i got this idea from my random learning from some code books of this compatition.","94163086":"We can see the model having squal number of training and testing dataset, so here we don't need to balance the dataset. all fine....good to go next:)","f8b3fadc":"From the \"train.info()\" we can see all the data here are float values and no categorical values available in training data.\n\nBelow is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum.","fa4510f9":"<a id=\"section_intro\"><\/a>\n## 2.2. Simple Statistics with testing data","1289fcbd":"<a id=\"section_intro\"><\/a>\n## 1.1. Overview","ca4cc4ce":"<a id=\"section_xgboost\"><\/a>\n### 6.1.1. XGBoost","a38cf7b8":"<a id=\"section_wo_hyper\"><\/a>\n## 6.1. Base model without hyperparameter tuning","d1051d30":"<a id=\"section_hyper\"><\/a>\n# 6.2. Model with hyperparameter tuning using optuna","6c15b0c3":"From this we can see , 2\/3 of the dataset containing missing values both in training and testing data. we must need to handle this before going to model traning.<br\/>\nThere are some methods to deal with missing values, we can see some methods in this separate notebook----->https:\/\/www.kaggle.com\/ninjaac\/methods-to-handle-missing-value","7dc8366d":"This dataset has no categorical data, so we don't need to care about this for now. if want we can learn about the categorical values here -->https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables","08649b7e":"<a id=\"section_intro\"><\/a>\n# 2. Understanding Data ","0521d54b":"The 'index_col' will make the 'id' feature as a row index because they are no longer needed for model training and it simply represents the row number:)","3e6c3bfb":"<a id=\"section_sub\"><\/a>\n## 6. Submision","8b522c45":"<a id=\"section_log_reg\"><\/a>\n## 6.1.2. Logistic Regression","1d026260":"- 1. [Introduction](#section_intro) \n    - 1.1. [Overview](#)\n    - 1.2. [Pre-request](#)\n- 2. [Understanding Data](#)\n    - 2.1. [Simple Statistics with training data](#)\n    - 2.2. [Simple Statistics with testing data](#)\n- 3. [Data Visualization](#) \n- 4. [Data Preprocessing](#) \n    - 4.1. [Handling Missing Values](#section_missingvalues) \n    - 4.2. [Handling Categorical data](#section_categoricalData) \n    - 4.3. [Handle imbalance dataset](#section_imbalance) \n    - 4.4. [Feature Reduction (next submission)](#) \n    - 4.5. [Scaling](#section_scaling)\n- 5. [Feature Engineering](#section_feature)\n- 6. [Model](#section_model) \n    - 6.1. [Base model without hyperparameter tuning ](#section_wo_hyper) \n        - 6.1.1. [XGBoost](#section_xgboost) \n        - 6.1.2. [Logistic Regression](#section_log_reg) \n    - 6.2. [Model with hyperparameter tuning using optuna](#section_hyper) \n        - 6.2.1. [XGBoost](#section_xgb_hyper) \n- 7. [Submision](#section_sub)","e228fa85":"<a id=\"section_xgb_hyper\"><\/a>\n## 6.2.1. XGBoost","e9473ca4":"<a id=\"section_intro\"><\/a>\n# 4. Data Preprocessing\n4.1. Handling Missing Values\n4.2. Handling Categorical data\n4.3. Handle imbalance dataset\n4.4. Feature Reduction (next submission)\n4.5. Standardization (or) Normalization","3dffd27c":"<a id=\"section_feature\"><\/a>\n# 5. Feature Engineering","876409e2":"<a id=\"section_intro\"><\/a>\n## 2.1. Simple Statistics with training data","a70eb9d8":"<a id=\"section_categoricalData\"><\/a>\n## 4.2. Handling Categorical data ","3d2693cc":"<a id=\"section_scaling\"><\/a>\n## 4.5. Scaling","9225e8d6":"# just trying path\nscore with default parameter and Standarscalar and median imputer = 0.5075950898769981 (so bad)<br\/>\nscore with default parameter and Standarscalar and mean imputer = 0.5069218256005761 <br\/>\nscore with default parameter and Standarscalar and constant zero imputer = 0.507425029718713 (so bad)<br\/> \n\n#### Not a big difference\nscore with default parameter and Normalization and median imputer = 0.5056612627208464 (so bad)<br\/>\nscore with default parameter and Normalization and mean imputer = 0.5056612627208464 <br\/>\nscore with default parameter and Normalization and constant zero imputer = 0.5055436207845755(so bad)<br\/> \n\n#### after feature engineeering ( i can't find any improvement after feature engneering)\nscore with default parameter and Standarscalar and median imputer = 0.5090028286920467 <br\/>\nscore with default parameter and Normalization and constant zero imputer = 0.5125507184919263","801c8891":"From the \"test.info()\" we can see all the data here are float values and no categorical values available in testing data.\n\nBelow is the basic statistics for each variables which contain information on count, mean, standard deviation, minimum, 1st quartile, median, 3rd quartile and maximum."}}