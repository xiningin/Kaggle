{"cell_type":{"a479612e":"code","9191af16":"code","42b6356b":"code","2cc19233":"code","6930f5bb":"code","3df8fe6a":"code","3e346181":"code","20010662":"code","7c36b982":"code","9a8732d3":"code","aa4abce5":"code","ab712709":"code","61bc72c7":"code","b92086f4":"code","b0ec4175":"code","439b8558":"code","d30e9e8f":"code","ad1b9719":"code","80ee8c46":"code","b31b0f92":"code","4469edc9":"code","2403a48c":"code","3dd0575c":"code","87ea009f":"code","72f6f924":"code","8aa1c096":"code","b6d961d8":"code","63da1fae":"code","e9100e8f":"code","bc6da7de":"code","ae9aa4d8":"code","9cce0d60":"code","e4fc2a15":"code","c0cc9178":"code","9cde3e24":"code","6c68ca8d":"markdown","e91879f0":"markdown","e387bee7":"markdown","0c074edb":"markdown","5a052fba":"markdown","2d0be31f":"markdown","7094f86b":"markdown","ab618a3e":"markdown","5ed230a6":"markdown","4af7b283":"markdown","ad241576":"markdown","5ce1e615":"markdown","8ccd6d3e":"markdown","6ff7cb6f":"markdown","8f0565be":"markdown","0124aa95":"markdown","688fa942":"markdown","5caaa68d":"markdown","b856e0dc":"markdown","be252b71":"markdown","da3afc67":"markdown"},"source":{"a479612e":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport matplotlib.gridspec as gridspec\nimport seaborn as sns \nimport math \nimport re\nfrom IPython.display import display\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy import stats\nfrom scipy.stats import norm,skew\nimport folium \npd.options.display.float_format = '{:.2f}'.format\n\nsns.set_style(\"whitegrid\")\n%matplotlib inline","9191af16":"path=('..\/input\/images\/House.jpg')\ndisplay(Image.open(path))","42b6356b":"path=('..\/input\/images-2\/data.png')\ndisplay(Image.open(path))","2cc19233":"df = pd.read_csv(\"..\/input\/california-housing-prices\/housing.csv\")\ndf.tail(10)","6930f5bb":"df.describe()","3df8fe6a":"fig,(ax1) = plt.subplots(1, figsize=(10,5))\n\nsns.heatmap(df.isnull(), yticklabels = False , cmap = 'plasma', ax = ax1).set_title(\"Missing Values\")\nprint(\"Mssing Values\")","3e346181":"# --->  Finding Missing Values \n\nMissing_values=df[df.isna().any(axis=1)].sort_values(by='total_rooms')['total_rooms'].values\n                                                                                         \n#--> iterating to get the mean Values \n\nTB = [] #< -- Here total Bedroom\nMV = [] #< -- Here Mean values \n\nfor i in Missing_values:\n    values = df[df['total_rooms'] == i]['total_bedrooms'].mean()\n    values= round(values,1)\n    TB.append(i)\n    MV.append(values)\n    \n#--> Creating Dicctionaty to Group the final Values\n\nKey = TB\nVAL = MV\ndic = dict(zip(Key,VAL)) # In this dictionaty we have Nan Values \n\n#--> Eliminating Nan Values from Dicctionaty\nnew_dic = {k : v for k,v in dic.items() if pd.Series(v).notna().all()}\nT_nan_values =len(dic)-len(new_dic)\n\n# Total Nan Values \nprint (\"Total Nan Values in dict =\",T_nan_values)","20010662":"#--> Replacing Values \n\nfor i, j in new_dic.items():\n    df.loc[(df['total_rooms'] == i) & (df['total_bedrooms']!= i), 'total_bedrooms'] = j \n    #find Values in Total roms that = i and total bedrooms == nan and repace them by J.value\n    \ndf[df.isnull().any(axis = 1)] # Excatly the 15 Nan Values ","7c36b982":"value = np.mean(df.total_bedrooms)\ndf.total_bedrooms.fillna(value =value, inplace = True)","9a8732d3":"from sklearn.preprocessing import OneHotEncoder\n\nohc= OneHotEncoder()\nohe=ohc.fit_transform(df.ocean_proximity.values.reshape(-1,1)).toarray()\ndfOneHot = pd.DataFrame(ohe ,columns=[\"Ocean_\"+str(ohc.categories_[0][i])\n                                     for i in range(len(ohc.categories_[0]))])\n\ndata =pd.concat([df,dfOneHot],axis=1)\n\n\ndata.tail(3)","aa4abce5":"#Creating Map \nUSA = folium.Map(location = [37.880,-122.230],tiles='OpenStreetMap',\n                   min_zoom = 6 , max_zoom = 13 , zoom_start = 7)\n\n# Adding Position \nfor (index,row) in data[0:5000].iterrows():\n    folium.Circle(\n        radius = int(row.loc['median_house_value'])\/10000,\n        location = [row.loc['latitude'], row.loc['longitude']],\n        popup = 'House Age ' + str(row.loc['housing_median_age']), color = 'crimson',\n        tooltip =  '<li><bold>Price :' + str(row.loc['median_house_value']) + str('K'),\n        fill = True, fill_color ='#ccfa00').add_to(USA) \n    \ndisplay(USA)","ab712709":"# Correlation \ncorrelation = data.corr()\nf,ax =plt.subplots(figsize =(15,10))\nmask = np.triu(correlation)\nsns.heatmap(correlation, annot=True, mask=mask , ax=ax, \n            linewidths = 4, cmap = 'viridis', square=True).set_title(\"Correlation\")\nbottom,top = ax.get_ylim()\nax.set_ylim (bottom + 0.5 , top - 0.5)\nprint(\"Heatmap - Correlation\")","61bc72c7":"from sklearn.base import BaseEstimator, TransformerMixin\n\nrooms,bedrooms,population,house= 3,4,5,6\n\nclass CombiAttri (BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedroom_per_room = True):\n        self.add_bedroom_per_room = add_bedroom_per_room\n    \n    def fit(self, X, y = None):\n        return self\n         \n    def transform(self, X):\n    #Divide # of rroms \/ # of houses\n        RPH = X[:,rooms] \/ X[: , house] #Rooms per House\n        PPH = X[:,population] \/ X[: , house] #Population per House \n    \n        #Beddrooms per rooms\n        if self.add_bedroom_per_room:\n            ABR = X[: , bedrooms] \/ X [: , rooms]\n            return np.c_[X, RPH ,PPH, ABR]\n        else:\n            return np.c_[X, RPH ,PPH]\n        \n#Running\nothers= CombiAttri()\nextradata = others.transform(data.values)\n\n#Showing\nfdata = pd.DataFrame(\n    extradata,\n    columns = list(data.columns) + [\"rooms_per_household\", \"population_per_household\",\"bedrrom_per_room\"], #adding the New columns\n    index = data.index)\n\nfdata.head()","b92086f4":"fdata.drop('ocean_proximity', axis = 1 , inplace = True)\nfdata = fdata.astype(float)","b0ec4175":"def mul_plot (df, feature):\n    fig=plt.figure(constrained_layout = True , figsize = (12,8))\n    grid= gridspec.GridSpec(ncols = 3 , nrows = 2 , figure=fig)\n\n    ax1= fig.add_subplot(grid[0,1:3])\n    ax1.set_title(\"Histogram\")\n    sns.distplot(df.loc[:,feature], norm_hist = True, ax= ax1)\n\n    ax2= fig.add_subplot(grid[1,1:3])\n    ax2.set_title(\"QQ_plot\")\n    stats.probplot(df.loc[:,feature] , plot=ax2)\n\n    ax3= fig.add_subplot(grid[:2,0])\n    ax3.set_title(\"Box Plot\")\n    sns.boxplot(df.loc[:,feature], orient = \"v\" , ax= ax3)\n    \n    print(\"Skewness: \"+ str(fdata['median_house_value'].skew().round(3))) \n    print(\"Kurtosis: \" + str(fdata['median_house_value'].kurt().round(3)))\n\nmul_plot (fdata,'median_house_value')","439b8558":"\nindexes_2 = fdata.loc[(fdata['median_house_value']>100000) & (fdata['median_income']>=10)].index.to_list()\nindexes_3 = fdata.loc[(fdata['median_house_value']>280000) & (fdata['median_income']<=2)].index.to_list()\nindexes_4 = fdata.loc[(fdata['median_house_value']<280000) & (fdata['median_income']>=8)].index.to_list()\nindexes_4 = fdata.loc[fdata['median_income']>=9].index.to_list()\nindexes_1 = fdata.loc[fdata['median_house_value']>500000].index.to_list()\n\ntotal_drop = indexes_1+indexes_2+indexes_3+indexes_4","d30e9e8f":"fdata.reset_index(drop=True)\nfdata.drop(total_drop, inplace= True)\nfdata.reset_index(drop=True)","ad1b9719":"from sklearn.neighbors import LocalOutlierFactor\n\ndef outliers (x,y, top = 5 , plot = True):\n    lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n    x_ =np.array(x).reshape(-1,1)\n    preds = lof.fit_predict(x_)\n    lof_scr = lof.negative_outlier_factor_\n    out_idx = pd.Series(lof_scr).sort_values()[:top].index\n    \n    if plot:\n        f, ax = plt.subplots(figsize=(9, 6))\n        plt.scatter(x=x, y=y, c=np.exp(lof_scr), cmap='RdBu')\n    return out_idx\n\nouts = outliers(fdata['median_house_value'], fdata['median_income'],top=5)\nprint(\"Outliers detected:\",outs)\nplt.show()","80ee8c46":"''' Normalizing '''\n\n#--- Appliying Log10  = np.log1p()\nfdata['median_house_value'] = np.log1p(fdata['median_house_value'])\n\n#Creating new plot \nmul_plot (fdata,'median_house_value')","b31b0f92":"fdata.reset_index(drop=True)","4469edc9":"path=('..\/input\/images\/machine.gif')\ndisplay(Image.open(path))","2403a48c":"#----> Applying Machine Learning \nfrom sklearn import preprocessing \nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n\nfrom sklearn.linear_model import LinearRegression,SGDRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import StratifiedShuffleSplit,cross_val_score\nfrom sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import accuracy_score","3dd0575c":"# ------> Chossong the best model \ndef Evaluating (model,X,Y, CV,Criteria,sqrt=True ):\n    \n    if sqrt :\n        score = cross_val_score (model,X, Y, cv=CV ,scoring=Criteria)\n        final = -score.mean()\n        final = round(np.sqrt(final),2)\n        MSE.append(final)\n    else:\n        score = cross_val_score (model,X, Y, cv=CV ,scoring=Criteria)\n        final = round(score.mean(),2)\n        R2.append(final)","87ea009f":"#-----> Categorizing \nfdata['income_cat'] = pd.cut(fdata[\"median_income\"],\n                             bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                             labels=[1, 2, 3, 4, 5])\n\n#------> Statify according to income data to have a proporcional distribution \n# ------> Variable \n\ncriteria = fdata['income_cat'] \nX = fdata.drop(\"median_income\", axis = 1)\ny = fdata[\"median_income\"]\n\n\n# -----> Splitting Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify = criteria)\nX_train.drop(\"income_cat\",axis = 1 , inplace = True)\nX_test.drop(\"income_cat\",axis = 1 , inplace = True)\n\n# ------> StandarScaler\nMX = MinMaxScaler()\nX_train = MX.fit_transform(X_train)\nX_test = MX.fit_transform(X_test)\n\nX=X.values\ny=y.values","72f6f924":"# ------> Algorithms before Tunning \nLN = LinearRegression()\nSGDR = SGDRegressor()\nDT = DecisionTreeRegressor()\nRDF = RandomForestRegressor()\nSVR_rbf = SVR()\nXR = XGBRegressor()\n\nMSE = [np.nan]  #Mean Squared Error\nR2= [np.nan]\n\n\n# ------> Running models\nfor i in (LN , SGDR, DT, RDF,SVR_rbf,XR):\n    Evaluating(i,X,y,4,'neg_mean_squared_error')\n\nfor i in (LN , SGDR, DT, RDF,SVR_rbf,XR):\n    Evaluating(i,X,y,4,'r2',sqrt= False)\n\n\nBest_model = pd.DataFrame(data = {'MSE':MSE,'R2':R2},\n             index =['Neural_Network','Linear Regression','Stochastic Gradient Descent',\n                     'DecisionTreeRegressor','RandomForestRegressor',\n                     'Support Vector Machine','XGB Regressor'])\nBest_model.sort_values(by = 'MSE' , ascending=True) ","8aa1c096":"X = fdata.drop(\"median_income\", axis = 1)\nX.drop('income_cat', axis= 1 , inplace = True)\ny = fdata[\"median_income\"]\n\nX= X.values\ny=y.values","b6d961d8":"## -----------------------------> Support Vector Machine\nSVR_rbf = SVR()\nparameters = {'kernel': ('linear', 'rbf','poly'), 'C':[0.00,0.01,0.1,1],'gamma': [0.001,0.01,0.1,1],'epsilon':[0.1,0.2,0.3]}\nSVR_RAND = RandomizedSearchCV(SVR_rbf,parameters,cv=5,n_iter = 50,scoring = 'neg_mean_absolute_error',n_jobs = -1,\n                              verbose = 5,return_train_score=True,random_state=42)\n\n## -----------------------------> Stochastic Gradient Descent\nSGDR = SGDRegressor()\nparams={'alpha':[0.1,0.01,0.001,0.0001,0.00001],'learning_rate':['constant','optimal','invscaling','adaptive'],\n        'max_iter':[100,300,600,1000,1200,1500,2000],'penalty':['l2','l1','elasticnet']}\nSGDR_random_cv = RandomizedSearchCV(estimator = SGDR,param_distributions=params,cv=5,n_iter = 50,\n                              scoring = 'neg_mean_absolute_error',n_jobs = 3,verbose = 5,return_train_score=True,random_state=42)\n\n## -----------------------------> XGB Regressor\n\n# ------> Tunnig XGBRegressor\nXR = XGBRegressor()\n\n# ------> Hyper Parameter Optimization\nn_estimators = [100,500,900,1100,1500]\nmax_depth = [2,3,5,10,15]\nbooster = ['gbtree', 'gblinear']\nlearning_rate = [0.05,0.1,0.15,0.20]\nmin_child_weight = [1,2,3,4]\nbase_score = [0.25,0.5,0.75,1]\n\n# ------> Define the grid of Hyperparameters to search\nhyperparameter_grid = { 'n_estimators': n_estimators,'max_depth': max_depth,'booster': booster,\n                       'learning_rate': learning_rate,'min_child_weight': min_child_weight,\n                       'base_score' : base_score}\n\nXR_random_cv = RandomizedSearchCV(estimator = XR,param_distributions=hyperparameter_grid,cv=5,n_iter = 50,scoring = 'neg_mean_absolute_error',n_jobs = 3,\n                              verbose = 5,return_train_score=True,random_state=42)\n\n## -----------------------------> RandomForestRegressor\nRDF = RandomForestRegressor()\nparameters={'n_estimators': [30,10,30], 'max_features': [2,4,6,8],'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]}\nclf_RDF = GridSearchCV(RDF, parameters , cv = 5 , scoring = 'neg_mean_squared_error', return_train_score = True)\n","63da1fae":"## ----> Best estimatorBest estimator \nBSVR = SVR(C=1, gamma=1, kernel='poly')\nBSGDR= SGDRegressor(alpha=0.1, learning_rate='adaptive')\nBRF = RandomForestRegressor(bootstrap=False, max_features=4, n_estimators=10)\nBXR = XGBRegressor(base_score=1, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.15, max_delta_step=0, max_depth=5,\n             min_child_weight=2, missing=None, monotone_constraints='()',\n             n_estimators=100, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","e9100e8f":"Model = [BSGDR,BRF,BXR]#,BSVR]\nMSE = [np.nan,np.nan]  #Mean Squared Error\nR2= [np.nan,np.nan]  #R2\n\nfor i in (BSGDR,BRF,BXR):\n    Evaluating(i,X,y,4,'neg_mean_squared_error')\n\nfor i in (BSGDR,BRF,BXR):\n    Evaluating(i,X,y,4,'r2',sqrt= False)\n\n\nBest_model_2 = pd.DataFrame(data = {'MSE':MSE,'R2':R2},\n             index =['Neural_Network','Support Vector Machine','Stochastic Gradient Descent','RandomForestRegressor','XGB Regressor'])\nBest_model_2.sort_values(by = 'MSE' , ascending=True)","bc6da7de":"pip install livelossplot","ae9aa4d8":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense , Dropout\n#from tensorflow.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import SGD\nfrom livelossplot import PlotLossesKeras\n\n\n#Modeling\nmodel = Sequential()\nmodel.add(Dense(16, input_dim = (16),  kernel_initializer='normal', activation  = 'relu'))\nmodel.add(Dense(4,  kernel_initializer='normal', activation  = 'relu'))\n\n#compiling\nmodel.add(Dense(1, kernel_initializer='normal', activation = 'sigmoid'))\nmodel.compile(loss = 'mean_squared_error',optimizer = 'adam',metrics = ['mse'])\n\nmodel.fit(X_train,y_train,validation_data =(X_test,y_test),\n          epochs = 100, batch_size= 1000, callbacks=[PlotLossesKeras()],verbose=0)","9cce0d60":"NN_model = model.predict(X_test)\nprint(\"The Mean Square Error using NN is \", round(np.sqrt(mean_squared_error(y_test,NN_model)),4))","e4fc2a15":"from sklearn.model_selection import KFold\nfrom sklearn import metrics\n\nx_main, x_holdout, y_main, y_holdout = train_test_split(X, y, test_size=0.10) \n\n# Cross-validate\nkf = KFold(5)\n\noos_y = []\noos_pred = []\nhist = []\nfold = 0\n\nfor train, test in kf.split(x_main):        \n    fold+=1\n    print(f\"Fold #{fold}\")\n        \n    x_train = x_main[train]\n    y_train = y_main[train]\n    x_test = x_main[test]\n    y_test = y_main[test]\n\n    #Modeling\n    model = Sequential()\n    model.add(Dense(16, input_dim = (16),  kernel_initializer='normal', activation  = 'relu'))\n    model.add(Dense(4,  kernel_initializer='normal', activation  = 'relu'))\n\n    #compiling\n    model.add(Dense(1, kernel_initializer='normal', activation = 'sigmoid'))\n    model.compile(loss = 'mse',\n                optimizer = 'adam',\n                 metrics = ['mse'])\n\n    model.fit(x_train,y_train,validation_data =(x_test,y_test), epochs = 100, batch_size= 1000)\n    history = pd.DataFrame(model.history.history)\n    hist.append(history)\n    pred = model.predict(x_test)\n   \n    oos_y.append(y_test)\n    oos_pred.append(pred) \n\n    # Measure accuracy\n    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n    print(f\"Fold score (RMSE): {score}\")\n    \n# Build the oos prediction list and calculate the error.\noos_y = np.concatenate(oos_y)\noos_pred = np.concatenate(oos_pred)\nscore = np.sqrt(metrics.mean_squared_error(oos_pred,oos_y))\nprint()\nprint(f\"Cross-validated score (RMSE): {score}\")    \n    \n# Write the cross-validated prediction (from the last neural network)\nholdout_pred = model.predict(x_holdout)\n\nscore = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\nprint(f\"Holdout score (RMSE): {score}\")","c0cc9178":"print(f\"Cross-validated score (RMSE): {score}\")\nprint(f\"Holdout score (RMSE): {score}\")","9cde3e24":"fig, axs = plt.subplots(2, 2, figsize= (20,15))\naxs[0, 0].plot(hist[0])\naxs[0, 0].set_title('K_Fold_1')\naxs[0, 1].plot(hist[1], 'tab:orange')\naxs[0, 1].set_title('K_Fold_2')\naxs[1, 0].plot(hist[2], 'tab:green')\naxs[1, 0].set_title('K_Fold_3')\naxs[1, 1].plot(hist[3], 'tab:red')\naxs[1, 1].set_title('K_Fold_4')\n\nfor ax in axs.flat:\n    ax.set(xlabel='EPOCHS', ylabel='MSE')\n\n# Hide x labels and tick labels for top plots and y ticks for right plots.\nfor ax in axs.flat:\n    ax.label_outer()","6c68ca8d":"# 2. Scrubing - Cleaning, Filling and Formating  ","e91879f0":"DROPPING VALUES AND NORMALIZING","e387bee7":"MODELS & SPLITTING","0c074edb":"### 2.2. Formating \n-  Enconding Variable with N categories with their names Using Onehotencoder","5a052fba":"# 3. Explore - Finding significant Pattern and Trends","2d0be31f":" # ** Neural Network** ","7094f86b":"# ** Hyperparameter** +  ** Cross_Validation** ","ab618a3e":"> # **Thanks a lot for passing by..**.  \n- In this notebook I am analyzing **California House Dataset**, I applied some techniques (I'ver learned few weeks ago) such as **GridSearchCv, RandomsearchCv , OutlierDetection** , ***StratifiedShuffleSplit*** and others with a simple set \n- There were only 200 Nan Values but thanks to loops and Dicctionaty interaction I was able to find the average (between specific range) to fill out the missing Values \n- I am evaluating Linear regression, XGBboost Regressor , Decision tree Regressor , Support Vecor Machine Regressor and Random Forest Regressor and I will use Neural Network Soon..!!\n- I am using the OSEMN Methodology \n> - LET'S START","5ed230a6":"### 3.2. Heatmap - Correlation\n- As you might notices Total_rooms ~ Total_bedrooms and Total_population are strongly Correlated , the best apporach would have been Find the strong relation among then and compare them to the Target variable \"Media_house_values\" to reduce dimensionality (variables) but in this case I didnt do it because The set have few columns -variable so it is understandable instead i will create other Combination maybe i can get better resutls, However you can check my notebook \" House price - advance Regression -  Where I have an example","4af7b283":"# 1. Obtaining Data  ","ad241576":"### 2.1. Filling \n- In this section first I found the Missing Values, then I noticed some relation btwn Totalbedrooms ~ TotalRooms - \"Please check the Correlation chart\" , So what I found the Nan Valeus , checked the Totalbedrooms and find the averge according to the same # in Total_bedrooms, Then I Create a Dicctionary and Repalce the Values. The  15 NAN values represent 15 range of non Found values in TotalBedrooms.","5ce1e615":" # *** Neural Network + K Fold ***","8ccd6d3e":"OUTLIERS \n\n- ***General Outliers***\n- **LocalOutlierFactor** which use Knn to find outliers ... here you can have a great explanation https:\/\/www.youtube.com\/watch?v=vnoBkTa7arI&t=14s","6ff7cb6f":"# ***Conclusion***","8f0565be":"### 3.3. Analizing Target Variable \"Media_House_value\"\n- In here We can notice it has tail to the Right and several Values > 500K that Will affect the model because it will have bias, the best option i can think about would be 1. normalize 2. Drop some values ","0124aa95":"### This \"Kernel\" is only for practicing purpose:\n* After Hyperparameter tunning the model seems to be overfitted \n* Xgboost Regressor and NN regressor have the Best Performance\n* K fold + NN codes are from @Jeff Heaton\n* you can find the code on his video : https:\/\/www.youtube.com\/watch?v=maiQf8ray_s\n","688fa942":"TARGET ","5caaa68d":"# Machine Learning Algorithms + Cross_Validation","b856e0dc":"* Now I am interesting in knowing the average of bedrooms \/ Household , the average population \/ Household *** The Following code was subtracted from the book \"On hand in Machine Learning and SkLearn\" ***","be252b71":"### 3.1. Folium - Real Location \n-  Using Folium Library to Locate the Houses , Age and Prices (only the First 5000 Houses) ","da3afc67":"# 4. Model - Machine Learning "}}