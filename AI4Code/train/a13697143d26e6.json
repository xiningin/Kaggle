{"cell_type":{"dc80e325":"code","6a56d116":"code","464ec32d":"code","6bdaeb68":"code","f8d5b182":"code","4c4e8351":"code","96ee425e":"code","7455abbe":"code","323b1ad5":"code","5d5ccc97":"code","20fa13ca":"code","fa08f941":"code","a924fd17":"code","d5395f07":"code","be09446e":"code","3eddfdd5":"code","f7a6ac1b":"code","233efc4c":"code","13eb3cfe":"code","fdae4e66":"code","846e941d":"code","e33fa6f7":"code","709dbcb8":"code","e87dc389":"code","44527955":"code","b5b731b0":"code","393a2f75":"markdown","7f8ed0e1":"markdown","b87a0ad0":"markdown","313e330b":"markdown","d2fe249c":"markdown","ad494053":"markdown","0217bdf6":"markdown","360fb96e":"markdown","d754b20a":"markdown","ad709d8e":"markdown","d3460f13":"markdown","bfc82cdb":"markdown","2a6159e4":"markdown","59fc78c3":"markdown","cdc6eed0":"markdown","eb494cf3":"markdown","4905b67d":"markdown","59f5c0f2":"markdown","4ab2cfef":"markdown","bf05da72":"markdown"},"source":{"dc80e325":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6a56d116":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\n#X = pd.read_csv('..\/input\/train.csv', index_col='Id') \n#X_test = pd.read_csv('..\/input\/test.csv', index_col='Id')\nX = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id') \nX_test = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)\n\n# To keep things simple, we'll drop columns with missing values\ncols_with_missing = [col for col in X.columns if X[col].isnull().any()] \nX.drop(cols_with_missing, axis=1, inplace=True)\nX_test.drop(cols_with_missing, axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.8, test_size=0.2,\n                                                      random_state=0)","464ec32d":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","6bdaeb68":"# Fill in the lines below: drop columns in training and validation data\ndrop_X_train = X_train.select_dtypes(exclude = ['object'])\ndrop_X_valid = X_valid.select_dtypes(exclude = ['object'])","f8d5b182":"model = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel.fit(drop_X_train, y_train)\n#preds = model.predict(X_valid)","4c4e8351":"# Fill in the lines below: drop columns in training and validation data\n#drop_X_train = X_train.select_dtypes(exclude = ['object'])\n#drop_X_valid = X_valid.select_dtypes(exclude = ['object'])\n\ndrop_X_test = X_test.select_dtypes(exclude = ['object'])\n# Check your answers\n#step_1.check()","96ee425e":"#preds = model.predict(drop_X_test)# drop_X_test still have NaN values inspite of preprocessing in X_test.drop(cols_with_missing)\n# I guess NaN values are in 'object' columns and they were not removed by drop function above\n# Try imputing","7455abbe":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='most_frequent')\nimputed_X_test = pd.DataFrame(imputer.fit_transform(drop_X_test))\nimputed_X_test.columns = drop_X_test.columns","323b1ad5":"preds = model.predict(imputed_X_test)","5d5ccc97":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","20fa13ca":"# Categorical columns in the training data\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in object_cols if \n                   set(X_test[col]).issubset(set(X_train[col]))]\n        \n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(object_cols)-set(good_label_cols))\n        \nprint('Categorical columns that will be ordinal encoded:', good_label_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)","fa08f941":"from sklearn.preprocessing import OrdinalEncoder\n\n# Drop categorical columns that will not be encoded\nlabel_X_train = X_train.drop(bad_label_cols, axis=1)\nlabel_X_test = X_test.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nordinal_encoder = OrdinalEncoder()\nlabel_X_train[good_label_cols] = ordinal_encoder.fit_transform(X_train[good_label_cols])\nlabel_X_test[good_label_cols] = ordinal_encoder.fit_transform(X_test[good_label_cols])# Your code here   \n# Check your answer\n#step_2.b.check()","a924fd17":"model = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel.fit(label_X_train, y_train)\n#preds = model.predict(X_valid)","d5395f07":"#ordinal_encoder2 = OrdinalEncoder()\n#label_X_test = ordinal_encoder2.fit_transform(X_test)# Contains NaN again.","be09446e":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='most_frequent')\nimputed_X_test = pd.DataFrame(imputer.fit_transform(label_X_test))\nimputed_X_test.columns = label_X_test.columns","3eddfdd5":"preds = model.predict(imputed_X_test)","f7a6ac1b":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","233efc4c":"# Columns that will be one-hot encoded\nlow_cardinality_cols = [col for col in object_cols if X_test[col].nunique() < 10]#number of unique, before, we used issubset.()\n\n# Columns that will be dropped from the dataset\nhigh_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n\nprint('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\nprint('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)","13eb3cfe":"'''\nfrom sklearn.preprocessing import OneHotEncoder\n\none_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(one_hot_encoder.fit_transform(X_train[low_cardinality_cols]))# Still contains NaN\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_cols_train.head()\n'''\nfrom sklearn.preprocessing import OneHotEncoder\n\none_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(one_hot_encoder.fit_transform(X_train[low_cardinality_cols]))# Still contains NaN\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nprint(OH_X_train.head())","fdae4e66":"model = RandomForestRegressor(n_estimators=100, random_state=0)\nmodel.fit(OH_X_train, y_train)\n#preds = model.predict(X_valid)","846e941d":"print(X_test.head())\nprint(X_train.head())","e33fa6f7":"# Impute First\nfrom sklearn.impute import SimpleImputer\n#imputer = SimpleImputer(strategy='constant')# most_frequent\nimputer = SimpleImputer(strategy='most_frequent')\nimputed_X_test = pd.DataFrame(imputer.fit_transform(X_test))\nimputed_X_test.columns = X_test.columns\n#print(imputed_X_test.head())\nprint(imputed_X_test[low_cardinality_cols].head())\nprint(X_train[low_cardinality_cols].head())","709dbcb8":"from sklearn.preprocessing import OneHotEncoder\n\none_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(one_hot_encoder.fit_transform(X_train[low_cardinality_cols]))\nOH_cols_test = pd.DataFrame(one_hot_encoder.transform(imputed_X_test[low_cardinality_cols]))# Still contains NaN\nprint(OH_cols_test.head())# Less than 122 columns here, which must be wrong!\nprint(OH_cols_train.head())\n# One-hot encoding removed index; put it back\nOH_cols_test.index = imputed_X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_test = imputed_X_test.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\nprint(OH_X_train.head())\nprint(OH_X_test.head())","e87dc389":"OH_cols_test.head()\n#preds = model.predict(OH_X_test)","44527955":"preds = model.predict(OH_X_test)# After applying 'most_frequent' instead of 'constant', problem solved; \n# 'constant' Error: Could not convert string to 'missing value'\n# Maybe 'constant' impute X_test's NaN by 'missing_value' while 'most_frequent' not (actually by string too)","b5b731b0":"output = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': preds})\noutput.to_csv('submission.csv', index=False)","393a2f75":"**Model Part**","7f8ed0e1":"**Submit Part**","b87a0ad0":"**Preprocess should be performed in both X_train and X_test**","313e330b":"**X_train and Model First**","d2fe249c":"**Prediction Part**","ad494053":"# Drop","0217bdf6":"**Investigating cardinality**","360fb96e":"**Imputing Part**","d754b20a":"Try imputing first before prediction","ad709d8e":"**Submission Part**","d3460f13":"**Step3: One Hot Encoding**","bfc82cdb":"**Step2: Ordinal encoding**","2a6159e4":"**Prediction Part**","59fc78c3":"**Model Part**","cdc6eed0":"**Object_columns** still need good\/bad cols here for X_train & X_test","eb494cf3":"**Step1: Drop**","4905b67d":"# Ordinal encoding","59f5c0f2":"**Modeling**","4ab2cfef":"**Preprocess should be performed in both X_train and X_test**","bf05da72":"# Now for One Hot Encoding!"}}