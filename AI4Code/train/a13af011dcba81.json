{"cell_type":{"945f38e5":"code","74be5d91":"code","af9b9824":"code","873c518a":"code","a4065a87":"code","b7506080":"code","0a94e13a":"code","e8903156":"code","016e717b":"code","832e1fab":"code","af753308":"code","d44f9462":"code","7aebe264":"code","9b4939c8":"code","48119903":"code","01cf419a":"code","a09239ee":"code","97a77f92":"code","fef378c8":"code","c8d8ea04":"code","ed293418":"code","ecc4cb7b":"code","3820fdbe":"code","1bea47f4":"code","cda52ac6":"code","5dfc671c":"code","32729506":"code","d3375fa2":"code","2ba0111d":"code","5e8e0482":"code","b53cb780":"code","7637b7d8":"code","0a49f417":"code","60e86efe":"code","56c73275":"code","e36b95c7":"code","200a4f61":"code","057cd9dc":"code","d804b12d":"markdown","bd4506ef":"markdown","a0f533b7":"markdown","e96be59e":"markdown","5dcc2899":"markdown","83452355":"markdown","88152e4a":"markdown","dd162b75":"markdown","56ff7ab7":"markdown","761920bb":"markdown","6ccc5a67":"markdown","3a5f1940":"markdown","03630a35":"markdown","86e83aef":"markdown","09577a66":"markdown","1979eccb":"markdown","89eaabfd":"markdown","d1549ddc":"markdown","86c8ea0f":"markdown","9a855863":"markdown","85c8cf51":"markdown","5752723c":"markdown","fa79e3a1":"markdown","a7e42344":"markdown","021672fc":"markdown","370509f9":"markdown","52c90ca3":"markdown"},"source":{"945f38e5":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing","74be5d91":"data = pd.read_csv('..\/input\/weatherAUS.csv')\nprint('Size of weather data frame is :',data.shape)\ndata.info()\ndata[0:10]","af9b9824":"data.count().sort_values()","873c518a":"data = data.drop(columns=['Evaporation','Sunshine','Cloud3pm','Cloud9am','Date','Location','RISK_MM'],\n                 axis=1)\ndata = data.dropna(how='any')\nprint(data.shape)","a4065a87":"# Replace No and Yes for 0 and 1 in RainToday and RainTomorrow\ndata['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndata['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)\n\n# Categorical variables WindGustDir, WindDir3pm and WindDir9am in dummy variables for each category.\ncategoric_c = ['WindGustDir', 'WindDir3pm', 'WindDir9am']\ndatafinal = pd.get_dummies(data, columns=categoric_c)\nprint(datafinal.shape)\ndatafinal.head()","b7506080":"standa = preprocessing.MinMaxScaler()\nstanda.fit(datafinal)\ndatafinal = pd.DataFrame(standa.transform(datafinal), index=datafinal.index, columns=datafinal.columns)\ndatafinal.head()","0a94e13a":"# Calculate the correlation matrix\ncorr = datafinal.corr()\ncorr1 = pd.DataFrame(abs(corr['RainTomorrow']),columns = ['RainTomorrow'])\nnonvals = corr1.loc[corr1['RainTomorrow'] < 0.005]\nprint('Var correlation < 0.5%',nonvals)\nnonvals = list(nonvals.index.values)\n\n# We extract variables with correlation less than 0.5%\ndatafinal1 = datafinal.drop(columns=nonvals,axis=1)\nprint('Data Final',datafinal1.shape)","e8903156":"from sklearn.model_selection import train_test_split\nY = datafinal1['RainTomorrow']\nX = datafinal1.drop(columns=['RainTomorrow'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)","016e717b":"print('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","832e1fab":"from sklearn.linear_model import LogisticRegression\n\n# We defining the model\nlogreg = LogisticRegression(C=10)\n\n# We train the model\nlogreg.fit(X_train, Y_train)\n\n# We predict target values\nY_predict1 = logreg.predict(X_test)","af753308":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlogreg_cm = confusion_matrix(Y_test, Y_predict1)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(logreg_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Logistic Regression Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","d44f9462":"# Test score\nscore_logreg = logreg.score(X_test, Y_test)\nprint(score_logreg)","7aebe264":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\n# We define the SVM model\nsvmcla = OneVsRestClassifier(BaggingClassifier(SVC(C=10,kernel='rbf',random_state=9, probability=True), \n                                               n_jobs=-1))\n\n# We train model\nsvmcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict2 = svmcla.predict(X_test)","9b4939c8":"# The confusion matrix\nsvmcla_cm = confusion_matrix(Y_test, Y_predict2)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('SVM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","48119903":"# Test score\nscore_svmcla = svmcla.score(X_test, Y_test)\nprint(score_svmcla)","01cf419a":"from sklearn.naive_bayes import GaussianNB\n\n# We define the model\nnbcla = GaussianNB()\n\n# We train model\nnbcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict3 = nbcla.predict(X_test)","a09239ee":"# The confusion matrix\nnbcla_cm = confusion_matrix(Y_test, Y_predict3)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(nbcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Naive Bayes Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","97a77f92":"# Test score\nscore_nbcla = nbcla.score(X_test, Y_test)\nprint(score_nbcla)","fef378c8":"from sklearn.tree import DecisionTreeClassifier\n\n# We define the model\ndtcla = DecisionTreeClassifier(random_state=9)\n\n# We train model\ndtcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict4 = dtcla.predict(X_test)","c8d8ea04":"# The confusion matrix\ndtcla_cm = confusion_matrix(Y_test, Y_predict4)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(dtcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Decision Tree Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","ed293418":"# Test score\nscore_dtcla = dtcla.score(X_test, Y_test)\nprint(score_dtcla)","ecc4cb7b":"from sklearn.ensemble import RandomForestClassifier\n\n# We define the model\nrfcla = RandomForestClassifier(n_estimators=100,random_state=9,n_jobs=-1)\n\n# We train model\nrfcla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict5 = rfcla.predict(X_test)","3820fdbe":"# The confusion matrix\nrfcla_cm = confusion_matrix(Y_test, Y_predict5)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('Random Forest Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","1bea47f4":"# Test score\nscore_rfcla = rfcla.score(X_test, Y_test)\nprint(score_rfcla)","cda52ac6":"from sklearn.neighbors import KNeighborsClassifier\n\n# We define the model\nknncla = KNeighborsClassifier(n_neighbors=15,n_jobs=-1)\n\n# We train model\nknncla.fit(X_train, Y_train)\n\n# We predict target values\nY_predict6 = knncla.predict(X_test)","5dfc671c":"# The confusion matrix\nknncla_cm = confusion_matrix(Y_test, Y_predict6)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(knncla_cm, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('KNN Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","32729506":"# Test score\nscore_knncla= knncla.score(X_test, Y_test)\nprint(score_knncla)","d3375fa2":"Testscores = pd.Series([score_logreg, score_svmcla, score_nbcla, score_dtcla, score_rfcla, score_knncla], \n                        index=['Logistic Regression Score', 'Support Vector Machine Score', 'Naive Bayes Score', 'Decision Tree Score', 'Random Forest Score', 'K-Nearest Neighbour Score']) \nprint(Testscores)","2ba0111d":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Logistic Regression Classification') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\nax3 = fig.add_subplot(3, 3, 3)\nax3.set_title('Naive Bayes Classification')\nax4 = fig.add_subplot(3, 3, 4)\nax4.set_title('Decision Tree Classification')\nax5 = fig.add_subplot(3, 3, 5)\nax5.set_title('Random Forest Classification')\nax6 = fig.add_subplot(3, 3, 6)\nax6.set_title('KNN Classification')\nsns.heatmap(data=logreg_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=nbcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax3)\nsns.heatmap(data=dtcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax4)\nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax5)\nsns.heatmap(data=knncla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax6)\nplt.show()","5e8e0482":"from sklearn.metrics import roc_curve\n\n# Logistic Regression Classification\nY_predict1_proba = logreg.predict_proba(X_test)\nY_predict1_proba = Y_predict1_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict1_proba)\nplt.subplot(331)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Logistic Regression')\nplt.grid(True)\n\n# SVM Classification\nY_predict2_proba = svmcla.predict_proba(X_test)\nY_predict2_proba = Y_predict2_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict2_proba)\nplt.subplot(332)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve SVM')\nplt.grid(True)\n\n# Naive Bayes Classification\nY_predict3_proba = nbcla.predict_proba(X_test)\nY_predict3_proba = Y_predict3_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict3_proba)\nplt.subplot(333)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Naive Bayes')\nplt.grid(True)\n\n# Decision Tree Classification\nY_predict4_proba = dtcla.predict_proba(X_test)\nY_predict4_proba = Y_predict4_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict4_proba)\nplt.subplot(334)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Decision Tree')\nplt.grid(True)\n\n# Random Forest Classification\nY_predict5_proba = rfcla.predict_proba(X_test)\nY_predict5_proba = Y_predict5_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict5_proba)\nplt.subplot(335)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Random Forest')\nplt.grid(True)\n\n# KNN Classification\nY_predict6_proba = knncla.predict_proba(X_test)\nY_predict6_proba = Y_predict6_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y_test, Y_predict6_proba)\nplt.subplot(336)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve KNN')\nplt.grid(True)\nplt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45)\nplt.show()","b53cb780":"Y1 = datafinal['RainTomorrow']\nX1 = datafinal.drop(columns=['RainTomorrow'])\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.feature_selection import SelectFromModel\n\nlsvc = LinearSVC(C=0.02, penalty=\"l1\", dual=False,random_state=9).fit(X1, Y1)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X1)\ncc = list(X1.columns[model.get_support(indices=True)])\nprint(cc)\nprint(len(cc))","7637b7d8":"# Principal component analysis\nfrom sklearn.decomposition import PCA\n\npca = PCA().fit(X1)\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('# of Features')\nplt.ylabel('% Variance Explained')\nplt.title('PCA Analysis')\nplt.grid(True)\nplt.show()","0a49f417":"# Percentage of total variance explained\nvariance = pd.Series(list(np.cumsum(pca.explained_variance_ratio_)), \n                        index= list(range(1, 62))) \nprint(variance[40:61])","60e86efe":"X1 = datafinal[cc] \nfrom sklearn.model_selection import train_test_split\nX1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.2, random_state=9)","56c73275":"# Logistic regression classification\nlogreg.fit(X1_train, Y1_train)\nY1_predict1 = logreg.predict(X1_test)\nlogreg_cm = confusion_matrix(Y1_test, Y1_predict1)\nscore1_logreg = logreg.score(X1_test, Y1_test)\n\n# SVM classification\nsvmcla.fit(X1_train, Y1_train)\nY1_predict2 = svmcla.predict(X1_test)\nsvmcla_cm = confusion_matrix(Y1_test, Y1_predict2)\nscore1_svmcla = svmcla.score(X1_test, Y1_test)\n\n# Naive bayes classification\nnbcla.fit(X1_train, Y1_train)\nY1_predict3 = nbcla.predict(X1_test)\nnbcla_cm = confusion_matrix(Y1_test, Y1_predict3)\nscore1_nbcla = nbcla.score(X1_test, Y1_test)\n\n# Decision tree classification\ndtcla.fit(X1_train, Y1_train)\nY1_predict4 = dtcla.predict(X1_test)\ndtcla_cm = confusion_matrix(Y1_test, Y1_predict4)\nscore1_dtcla = dtcla.score(X1_test, Y1_test)\n\n# Random forest classification\nrfcla.fit(X1_train, Y1_train)\nY1_predict5 = rfcla.predict(X1_test)\nrfcla_cm = confusion_matrix(Y1_test, Y1_predict5)\nscore1_rfcla = rfcla.score(X1_test, Y1_test)\n\n# K-Nearest Neighbor classification\nknncla.fit(X1_train, Y1_train)\nY1_predict6 = knncla.predict(X1_test)\nknncla_cm = confusion_matrix(Y1_test, Y1_predict6)\nscore1_knncla= knncla.score(X1_test, Y1_test)","e36b95c7":"Testscores1 = pd.Series([score1_logreg, score1_svmcla, score1_nbcla, score1_dtcla, \n                         score1_rfcla, score1_knncla], index=['Logistic Regression Score', \n                        'Support Vector Machine Score', 'Naive Bayes Score', 'Decision Tree Score', \n                         'Random Forest Score', 'K-Nearest Neighbour Score']) \nprint(Testscores1)","200a4f61":"fig = plt.figure(figsize=(15,15))\nax1 = fig.add_subplot(3, 3, 1) \nax1.set_title('Logistic Regression Classification') \nax2 = fig.add_subplot(3, 3, 2) \nax2.set_title('SVM Classification')\nax3 = fig.add_subplot(3, 3, 3)\nax3.set_title('Naive Bayes Classification')\nax4 = fig.add_subplot(3, 3, 4)\nax4.set_title('Decision Tree Classification')\nax5 = fig.add_subplot(3, 3, 5)\nax5.set_title('Random Forest Classification')\nax6 = fig.add_subplot(3, 3, 6)\nax6.set_title('KNN Classification')\nsns.heatmap(data=logreg_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax1)\nsns.heatmap(data=svmcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax2)  \nsns.heatmap(data=nbcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax3)\nsns.heatmap(data=dtcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax4)\nsns.heatmap(data=rfcla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax5)\nsns.heatmap(data=knncla_cm, annot=True, linewidth=0.7, linecolor='cyan',cmap=\"YlGnBu\" ,fmt='g', ax=ax6)\nplt.show()","057cd9dc":"# Logistic Regression Classification\nY1_predict1_proba = logreg.predict_proba(X1_test)\nY1_predict1_proba = Y1_predict1_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y1_test, Y1_predict1_proba)\nplt.subplot(331)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Logistic Regression')\nplt.grid(True)\n\n# SVM Classification\nY1_predict2_proba = svmcla.predict_proba(X1_test)\nY1_predict2_proba = Y1_predict2_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y1_test, Y1_predict2_proba)\nplt.subplot(332)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve SVM')\nplt.grid(True)\n\n# Naive Bayes Classification\nY1_predict3_proba = nbcla.predict_proba(X1_test)\nY1_predict3_proba = Y1_predict3_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y1_test, Y1_predict3_proba)\nplt.subplot(333)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Naive Bayes')\nplt.grid(True)\n\n# Decision Tree Classification\nY1_predict4_proba = dtcla.predict_proba(X1_test)\nY1_predict4_proba = Y1_predict4_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y1_test, Y1_predict4_proba)\nplt.subplot(334)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Decision Tree')\nplt.grid(True)\n\n# Random Forest Classification\nY1_predict5_proba = rfcla.predict_proba(X1_test)\nY1_predict5_proba = Y1_predict5_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y1_test, Y1_predict5_proba)\nplt.subplot(335)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve Random Forest')\nplt.grid(True)\n\n# KNN Classification\nY1_predict6_proba = knncla.predict_proba(X1_test)\nY1_predict6_proba = Y1_predict6_proba[:, 1]\nfpr, tpr, thresholds = roc_curve(Y1_test, Y1_predict6_proba)\nplt.subplot(336)\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='ANN')\nplt.xlabel('fpr')\nplt.ylabel('tpr')\nplt.title('ROC Curve KNN')\nplt.grid(True)\nplt.subplots_adjust(top=2, bottom=0.08, left=0.10, right=1.4, hspace=0.45, wspace=0.45)\nplt.show()","d804b12d":"### 1.2. Data for training and testing\n\nNext, we proceed to select a set of training data that will be the input in the learning algorithms, to make sure that after training our classification algorithm it is able to generalize well to new data. For this case we use a test sample size of 20%.","bd4506ef":"## 10. Conclusions\n\nThe results indicate that the random forest technique presents the best results in terms of prediction followed by SVM and logistic regression with test scores close to 85%. The worst results were presented by the naive bayes technique, which may be associated with the assumptions of the model, particularly the assumption of independence. In general, parametric methods (logistic regression and naive bayes) have a higher speed in learning the data, in contrast to non-parametric methods, which require more time to train the parameters. Considering the computation time within the best scores, logistic regression exhibits the best results, followed by random forest, while SVM requires considerable time to perform the calculations. Regarding the reduction of dimensionality, the techniques carried out have worse results, in particular the test scores are reduced by going from 57 to 45 features.","a0f533b7":"Currently the data set has two binary qualitative variables RainToday and RainTomorrow (Yes-No), which we proceed to transform into quantitative variables (1-0). Regarding the categorical variables (WindGustDir, WindDir3pm, WindDir9am) each of them with 16 categories, we convert each category into an additional variable, that is, a dummy variable (1-0).","e96be59e":"## 3. SVM (Support Vector Machine) classification\n\nSVMs (Support Vector Machine) have shown a rapid proliferation during the last years. The learning problem setting for SVMs corresponds to a some unknown and nonlinear dependency (mapping, function) $y = f(x)$ between some high-dimensional input vector $x$ and scalar output $y$. It is noteworthy that there is no information on the joint probability functions, therefore, a free distribution learning must be carried out. The only information available is a training data set $D = {(x_i, y_i) \u2208 X\u00d7Y }, i = 1$, $l$, where $l$ stands for the number of the training data pairs and is therefore equal to the size of the training data set $D$, additionally, $y_i$ is denoted as $d_i$, where $d$ stands for a desired (target) value. Hence, SVMs belong to the supervised learning techniques.\n\nFrom the classification approach, the goal of SVM is to find a hyperplane in an N-dimensional space that clearly classifies the data points. Thus hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n\nAs in the previous case, we started by defining the model. Thus, we choose a penalty parameter equal to 10 because around this the best test score is presented ((C = 0.1 - TS = 0.836), (C = 1.0 - TS =0.848), (C =10.0 - TS = 0.8503), (C = 100.0 - TS = 0.851)). The type of kernel used corresponds to RBF (Radial Basis Function), this kernel nonlinearly maps samples into a higher dimensional space, so it, unlike the linear kernel, can handle the case when the relation between class labels and attributes is nonlinear, additionally, the RBF kernel has fewer numerical difficulties and uses fewer hyperparameters than the polynomial kernel.\n\n","5dcc2899":"In this sense, the variables Sunshine, Evaporation, Cloud3pm, Cloud9am are those that effectively have more missing values, therefore we eliminate such variables from the data set. Additionally, we eliminate variables that are irrelevant in the analysis such as Date, Location and RISK_MM and the rest of the missing values. ","83452355":"### Comparison of classification techniques","88152e4a":"##### The confusion matrix","dd162b75":"## 2. Logistic regression classification\n\nLogistic regression is a technique that can be applied to binary classification problems. This technique uses the logistic function or sigmoid function, which is an S-shaped curve that can assume any real value number and assign it to a value between 0 and 1, but never exactly in those limits. Thus, logistic regression models the probability of the default class (the probability that an input $(X)$ belongs to the default class $(Y=1)$) $(P(X)=P(Y=1|X))$. In order to make the prediction of the probability, the logistic function is used, which allows us to obtain the log-odds or the probit. Thus, the model is a linear combination of the inputs, but that this linear combination relates to the log-odds of the default class.\n\nWe started make an instance of the model setting the default values. We specify the inverse of the regularization strength in 10. Later, we trained the logistic regression model with the training data, and then applied such model to the test data.","56ff7ab7":"Using MinMaxScaler we proceed to standardize the variables, such that,","761920bb":"## 9. Dimensionality reduction \n\nIn the previous analysis, the number of dimensions was reduced for those variables that had a correlation lower than 0.5% with the variable target. For the following analysis algorithms are used for the selection of features seeking to improve the performance of such analysis. Thus, we use linear support machine as an estimator of the weights from which the transformer is built. We define the penalty parameter equal to 0.02 (which controls the dispersion, to lower C, less selected features). ","6ccc5a67":"### Classification\n\nThe classification is performed using the techniques described above, where the only thing that changes is the training and testing data.","3a5f1940":"The results indicate 45 valid features to perform classification analyzes. These results are in line with the analysis of main components, since with around 45 features, 96% of the total variance is explained.","03630a35":"## 8. Comparison of classification techniques","86e83aef":"##### ROC curve","09577a66":"## 7. K-Nearest Neighbor classification\n\nK-Nearest neighbors is a technique that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). This technique is non-parametric since there are no assumptions for the distribution of underlying data and it is lazy since it does not need any training data point model generation. All the training data used in the test phase. This makes the training faster and the test phase slower and more costlier. In this technique, the number of neighbors k is usually an odd number if the number of classes is 2. For finding closest similar points, we find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance.\n\nFor our case, we chose a number of neighbors equal to 15 because it throws the best test score within several selected ((K = 5 - TS = 0.816), (K = 15 - TS = 0.817), (K = 25 - TS = 0.813), (K = 35 - TS = 0.812), (K = 50 - TS = 0.809)).","1979eccb":"### Test score","89eaabfd":"## 1. Data\n\n### 1.1. Data organization\n\nAs a first step we proceed to load and visualize the data in order to prepare the data set.","d1549ddc":"# Rain Classification in Australia\n\nUsing a database that contains the daily meteorological observations for several Australian climate measurement stations, the aim is to predict if it will rain tomorrow through a binary classification model. Thus, the target variable corresponds to RainTomorrow which indicates if it rained tomorrow or not, and there are variables such as temperature, evaporation, wind speed and direction, humidity, pressure, among others, with which it seeks to respond to the objective.\n\nUnder this context, sklearn classification algorithms will be used, namely:\n\n* Logistic Regression Classification (Parametric)\n* Support Vector Machine (SVM) Classification (Non parametric)\n* Naive Bayes Classification (Parametric)\n* Decision Tree Classification (Non parametric)\n* Random Forest Classification (Non parametric)\n* K-Nearest Neighbour (KNN) Classification (Non parametric)\n\nAfter implementing each classification model, a comparison is made with the confusion matrix methods. It is noteworthy that the variable RiskMM (amount of rainfall in millimeters for the next day) was used to create the binary object of analysis. For example, if RISKMM was greater than 0, then the RainTomorrow target variable is equal to Yes. Since it contains information about the future, and since it contains information directly about the target variable, including it would leak the future information to the model. Therefore, it is excluded in the analysis.  ","86c8ea0f":"##### Test Score","9a855863":"## 6. Random forest classification\n\nBased on the previous classification method, random forest is a supervised learning algorithm that creates a forest randomly. This forest, is a set of decision trees, most of the times trained with the bagging method. The essential idea of bagging is to average many noisy but approximately impartial models, and therefore reduce the variation. Each tree is constructed using the following algorithm:\n\n* Let $N$ be the number of test cases, $M$ is the number of variables in the classifier.\n* Let $m$ be the number of input variables to be used to determine the decision in a given node; $m<M$.\n* Choose a training set for this tree and use the rest of the test cases to estimate the error.\n* For each node of the tree, randomly choose $m$ variables on which to base the decision. Calculate the best partition of the training set from the $m$ variables.\n\nFor prediction a new case is pushed down the tree. Then it is assigned the label of the terminal node where it ends. This process is iterated by all the trees in the assembly, and the label that gets the most incidents is reported as the prediction. We define the number of trees in the forest in 100. ","85c8cf51":"We finish debugging the data by eliminating those variables that have a correlation less than 0.5% with the variable of interest.","5752723c":"## 4. Naive bayes classification\n\nThe naive Bayesian classifier is a probabilistic classifier based on Bayes' theorem with strong independence assumptions between the features. Thus, using Bayes theorem $\\left(P(X|Y)=\\frac{P(Y|X)P(X)}{P(Y)}\\right)$, we can find the probability of $X$ happening, given that $Y$ has occurred. Here, $Y$ is the evidence and $X$ is the hypothesis. The assumption made here is that the presence of one particular feature does not affect the other (the predictors\/features are independent). Hence it is called naive. In this case we will assume that we assume the values are sampled from a Gaussian distribution and therefore we consider a Gaussian Naive Bayes. Additionally we assume that all the characteristics are independent of each other, being the strongest assumption.","fa79e3a1":"## 5. Decision tree classification\n\nA decision tree is a flowchart-like tree structure where an internal node represents feature, the branch represents a decision rule, and each leaf node represents the outcome. The decision tree analyzes a set of data to construct a set of rules or questions, which are used to predict a class, i.e., the goal of decision tree is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. In this sense the decision tree selects the best attribute using to divide the records, converting that attribute into a decision node and dividing the data set into smaller subsets, to finally start the construction of the tree repeating this process recursively. As in the previous steps, we define the model that will be trained, and then evaluate its prediction.","a7e42344":"### The confusion matrix","021672fc":"### ROC curve","370509f9":"As shown in the table above, some variables such as Evaporation or Sunshine have missing values, which may affect subsequent estimates. Therefore, we order the variables and make a count of valid values for each column such that,","52c90ca3":"##### Data for training and testing"}}