{"cell_type":{"31f84bb9":"code","b81511bb":"code","6726a1b2":"code","47f7176f":"code","371986d6":"code","ab9065a5":"code","e30489ef":"code","9127504a":"code","70592057":"code","749c0317":"code","598e2ba7":"code","10d59600":"code","f10d2d0e":"code","dc61d544":"code","01acb0fa":"code","ae5a9e16":"code","dafd0666":"code","67d8fced":"code","92a541ae":"code","9e61c972":"code","8e0417c9":"code","bc8c6ccd":"markdown","0a150c0d":"markdown","e73b1820":"markdown","adfd2391":"markdown","8a9ad97d":"markdown","2471a621":"markdown","6a2e8f7c":"markdown","c082dbd9":"markdown","606a7d49":"markdown","17bca29d":"markdown","cd64a6d1":"markdown","c9a51190":"markdown","947483c8":"markdown","5831f81e":"markdown","749f2e46":"markdown","d8b6504b":"markdown","8775e32f":"markdown","3f290928":"markdown","95f83954":"markdown","083cb740":"markdown","ffcbd608":"markdown","e6c24b06":"markdown","d988d0af":"markdown","cc598219":"markdown","2250b6dd":"markdown"},"source":{"31f84bb9":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom scipy.stats import shapiro\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV\nfrom sklearn.metrics import explained_variance_score, r2_score, mean_absolute_error, mean_squared_error\nimport xgboost\nfrom xgboost import XGBRegressor, plot_importance\nfrom collections import Counter\nimport torch\nfrom torch import nn\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/autompg-dataset'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b81511bb":"data = pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv')\ndata \n# number of data points - 398","6726a1b2":"data.columns=['mpg','cylinders','displacement','horsepower','weight','acceleration','model year','origin','car name']\n\ndata.replace({'?':np.nan},inplace=True)\n\ndata['horsepower'] = pd.to_numeric(data['horsepower'])\ndata.dtypes ## horsepower is object, change to numeric","47f7176f":"data.describe()\n","371986d6":"plt.figure(figsize=(15,10))\nsns.heatmap(data.corr(),annot=True,annot_kws={\"size\":12})","ab9065a5":"plt.hist(data['model year'],histtype='bar')","e30489ef":"plt.hist(data['origin'],histtype='bar')","9127504a":"plt.hist(data['mpg'],histtype='bar')","70592057":"data.boxplot(column=['horsepower'])\n","749c0317":"data.boxplot(column=['acceleration'])\n","598e2ba7":"# Outlier detection - courtesy of Yassine's https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 2 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers","10d59600":"drop = detect_outliers(data,0,['mpg','cylinders','displacement','horsepower','weight','acceleration'])\ndata = data.drop(drop, axis = 0).reset_index(drop=True)","f10d2d0e":"data['car make'] = data['car name']\ndata['car make'] = data['car name'].apply(lambda x: x.split()[0]) \ndata.drop(columns=['car name'],inplace=True)\ndata = pd.get_dummies(data,columns=['car make'])\ndata['mpg'] = np.log(1 + 100*data['mpg'])\n\nX = data.drop(columns=['mpg'])\ny = data['mpg']\n\nimp = SimpleImputer(missing_values=np.nan,strategy='median')\nX['horsepower'] = imp.fit(X['horsepower'].values.reshape(-1, 1)).transform(X['horsepower'].values.reshape(-1, 1))\nprint(X.shape)\nprint(y.shape)\nX.head()","dc61d544":"xtrain,xtest,ytrain,ytest = train_test_split(X, y, test_size=0.3, random_state=42)\n","01acb0fa":"xgbr = XGBRegressor()\n\nxgb_params = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'learning_rate': [.03, 0.05, .07], \n              'max_depth': [5, 6],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7]\n              }\n\ngsXGB = GridSearchCV(xgbr, xgb_params, cv = 5, scoring='neg_mean_squared_error', \n                     refit=True, n_jobs = 5, verbose=True)\ngsXGB.fit(xtrain,ytrain)\n\nXGB_best = gsXGB.best_estimator_\n\ngsXGB.best_score_","ae5a9e16":"ypred = XGB_best.predict(xtest)\nexplained_variance_score(ytest,ypred)\nmean_absolute_error(ytest,ypred)\nprint(f\"Mean Squared using XGBoost model -> {mean_squared_error(ytest,ypred,squared=True)}\")","dafd0666":"plot_importance(XGB_best)","67d8fced":"# Define network dimensions\nn_input_dim = xtrain.shape[1]\n# Layer size\nn_hidden = 4 # Number of hidden nodes\nn_output = 1 # Number of output nodes for predicted mpg\n\n# Build mdel\ntorch_model = torch.nn.Sequential(\n    torch.nn.Linear(n_input_dim, n_hidden),\n    torch.nn.ELU(),\n    torch.nn.Linear(n_hidden, n_output)\n)\n    \nprint(torch_model)\n","92a541ae":"loss_func = torch.nn.MSELoss() # Mean Squared Error as Loss metric\nlearning_rate = 0.02 # play with learning rate\noptimizer = torch.optim.Adam(torch_model.parameters(), lr=learning_rate)","9e61c972":"train_error = []\niters = 1000\n\nY_train_t = torch.FloatTensor(ytrain.values).reshape(-1,1) #Converting numpy array to torch tensor\n\nfor i in range(iters):\n    X_train_t = torch.FloatTensor(xtrain.values)  #Converting numpy array to torch tensor\n    y_hat = torch_model(X_train_t)\n    loss = loss_func(y_hat, Y_train_t)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n\n    train_error.append(loss.item())\n    \nfig, ax = plt.subplots(2, 1, figsize=(12,8))\nax[0].plot(train_error)\nax[0].set_ylabel('Loss')\nax[0].set_title('Training Loss')","8e0417c9":"X_test_t = torch.FloatTensor(xtest.values)\nypredict = torch_model(X_test_t)\nprint(f\"Mean Squared Error using PyTorch Basic NN model -> {mean_squared_error(ytest,ypredict.detach().numpy(),squared=True)}\")\n","bc8c6ccd":"# Step 5: Modeling","0a150c0d":"## XGBoost model","e73b1820":"- XGBoost Fine-tuned Model performs better compared to Simple PyTorch NN because of NN requires huge amount of data to create well Generalized Models!\n- We have only 396 data points in our ADS","adfd2391":"## Boxplots","8a9ad97d":"## Do upvote if the notebook was helpful. Thanks!\n","2471a621":"## Udf to remove outliers","6a2e8f7c":"# Step 1: Understand the goal","c082dbd9":"## Notes:\n   - mpg is skewed right, suggesting potential need for log transform","606a7d49":"References:\n - https:\/\/archive.ics.uci.edu\/ml\/datasets\/Auto%2BMPG\n - https:\/\/github.com\/Ranga2904\/MPGPredictor_XG_PyTorch\/blob\/master\/FINAL.ipynb","17bca29d":"# Step 0: Load packages and check files","cd64a6d1":"# Step 4: ADS (Analytical Dataset) Preparation","c9a51190":"# Step 6: Conclusion","947483c8":"## Encoding categorical variables and imputing missing values in horsepower\n","5831f81e":"## **Notes:**\n   - Car weight and displacement have the strongest correlation:the heavier the weight and higher displacement, the lower mpg\n   - Car horsepower and cylider number are also strongly correlated with mpg: more HP and more cylinders, less mpg\n   - Less impactful: car origin, model year, acceleration.\n    ","749f2e46":"### ***Contuining on promise made to myself - publishing atleast 1-2 notebooks every 2 weeks! This is my 4th Notebook!***","d8b6504b":"# Step 2: Data Loading and first look","8775e32f":"## PyTorch Neural Network","3f290928":"## Seaborn heatmap","95f83954":"### **Goal is to predict 'mileage' (mpg column) for cars based on various characteristics:**\n\n    - mpg: continuous (Target Column)\n    - cylinders: multi-valued discrete\n    - displacement: continuous\n    - horsepower: continuous\n    - weight: continuous\n    - acceleration: continuous\n    - model year: multi-valued discrete\n    - origin: multi-valued discrete\n    - car name: string (unique for each instance)","083cb740":"## **Notes:**\n - There are 398 cars in this dataset, with set numbers of cylinders and set model years, set origin. mpg is target variable and is continuous, as are displacement, weight, acceleration\n - Car names are strings\n - Some features potentially encoding","ffcbd608":"# Step 3: Visualization","e6c24b06":"## Creating train and test data","d988d0af":">","cc598219":"# Table of contents\n\n- Step 0: Load packages and check input files\n- Step 1: Understand the goal\n- Step 2: Data Loading and first look\n- Step 3: Visualization\n- Step 4: ADS (Analytical Dataset) Preparation\n- Step 5: Modeling\n- Step 6: Conclusion","2250b6dd":"## Barplots"}}