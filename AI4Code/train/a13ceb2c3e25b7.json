{"cell_type":{"f90d7022":"code","51206e60":"code","274f0117":"code","46aa00a2":"code","10da57c3":"code","46457069":"code","d9eaab16":"code","42cab0a5":"code","6a9db7e0":"code","23c67f71":"code","9a605712":"code","a5b8d6ec":"code","52eba266":"code","7a37bf1d":"code","0081fc20":"code","9e5deafa":"code","626f690f":"code","3db28f69":"code","4fa33ef6":"code","3b899ef6":"code","dd09366a":"code","e85976a1":"code","bd36d8b6":"code","c921c0eb":"code","a5c6a954":"code","473bd43a":"code","56eaa75c":"code","8ad5794e":"code","73cfcdfa":"code","74de1ca1":"code","a0283192":"code","3c3b9d30":"code","8992dcef":"code","99ac54db":"code","7aceca3c":"code","59ab92be":"code","9a3e4633":"code","b1359f21":"code","a85dd1c9":"code","fa984c04":"code","da771a65":"code","d4cb7ef4":"code","05d8949e":"code","bb7989be":"code","b559068e":"code","4629b73e":"code","62bfff97":"code","8ebab3fd":"code","3f1a25ab":"code","73403627":"code","0d6aa157":"code","115a3dfa":"code","39d277e6":"code","5a071ea5":"code","6b1f823b":"code","6b5fce6c":"code","b6cd1ce2":"code","35562a84":"code","9d08d10a":"code","3f347087":"code","5d37b2d7":"code","8bbae95e":"code","df79650c":"code","ce33e67b":"markdown","9bdbeff5":"markdown","e2358663":"markdown","c1b5db87":"markdown","48702f0f":"markdown","a9169962":"markdown","af88fa25":"markdown","a5a4b14f":"markdown","6b8934d1":"markdown","80bb8ae6":"markdown","97812f99":"markdown","04c8b460":"markdown","d78a9c6b":"markdown","98fceffc":"markdown","889bc9cf":"markdown","407be000":"markdown","7d9a7fbe":"markdown","9b1a154c":"markdown","971fc96d":"markdown","ca48298b":"markdown","7c4f152b":"markdown","f56bc155":"markdown","4644a2f5":"markdown","5a30c6af":"markdown","a695bc10":"markdown","cde6dd99":"markdown","5d1c7bc8":"markdown","48fb0646":"markdown","99ff24c5":"markdown","beaf3363":"markdown","d948c2a1":"markdown","8671464d":"markdown","cb57dfb9":"markdown"},"source":{"f90d7022":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","51206e60":"INPUT_TRAIN = \"..\/input\/tabular-playground-series-jan-2022\/train.csv\"\nINPUT_TEST = \"..\/input\/tabular-playground-series-jan-2022\/test.csv\"\nSUBMISSION = \"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\"","274f0117":"def check_and_plot_nan_percentage(df=None, x_offset=0, y_offset=0, print_values=True):\n    \"\"\"\n    Plots the percentage of missing values on each columns of an input dataframe\n\n            Parameters:\n                    df (DataFrame): A pandas Dataframe\n                    x_offset (float): x_offset on each bar value\n                    y_offset (float): y_offset on each bar value\n                    print_values (boolean): Set it to True to display percentage on bars\n\n    \"\"\"\n    if df is None:\n        print(\"Input dataframe is None : exit\")\n        return\n    else:\n        values = []\n        for c in df.columns:\n            values.append(100*df[c].isna().sum() \/ df.shape[0])\n        plt.figure(figsize=(9, 6))\n        plt.title(\"NaN percentage per column\",\n                  fontsize=16,\n                  fontweight='bold',\n                  pad=20\n                  )\n        plt.bar(range(0, len(df.columns)), values, edgecolor='black')\n        plt.xticks(range(0, len(df.columns)), df.columns, rotation=90)\n        xlocs, xlabs = plt.xticks()\n        if print_values:\n            for i, v in enumerate(values):\n                if v > 0:\n                    if i % 2 == 0:\n                        plt.text(xlocs[i] + x_offset, v +\n                                 y_offset, str(round(v, 1)))\n                    else:\n                        plt.text(xlocs[i] + x_offset, v +\n                                 y_offset, str(round(v, 1)))\n        plt.show()","46aa00a2":"def evaluate_smape_model(model=None, X_test=None, y_test=None, biais_values=None):\n    \"\"\"\n    Makes predictions on X_test then, evaluate the metric SMAPE between predictions and y_test\n    and then evaluate SMAPE on predictions + biais values.\n\n            Parameters:\n                    model : A sklearn or tensorflow model\n                    X_test (array): x_offset on each bar value\n                    y_test (array): y_offset on each bar value\n                    biais_values (List): List of biais to add on predictions to evaluate SMAPE\n\n    \"\"\"\n    if model is None:\n        print(\"Input model is None\")\n        return\n    elif X_test is None:\n        print(\"X_test is None\")\n        return\n    elif y_test is None:\n        print(\"y_test is None\")\n        return\n    else:\n        y_pred = np.round(model.predict(X_test)).reshape(-1, 1).astype(int)\n        smp = smape(y_true=y_test, y_pred=y_pred)\n        print(\"SMAPE on test set =\", smp)\n        print(\"#\" * 10)\n\n        if biais_values is not None and len(biais_values) > 0:\n            smps = []\n            for i in biais_values:\n                smp = smape(y_true=y_test, y_pred=y_pred+i)\n                smps.append(smp)\n\n            print(\"Best SMAPE on test =\", min(smps),\n                \" biais =\", biais[smps.index(min(smps))])\n            print(\"#\" * 10)\n\n            plt.plot(biais_values, smps)\n            plt.title(\"Impact of biais on predictions\")\n            plt.ylabel(\"SMAPE Score\")\n            plt.xlabel(\"Biais value\")\n            plt.show()","10da57c3":"def load_train_test_set():\n    \"\"\"\n    Checks if train and test set exist and returns them into a list.\n\n            Return:\n                    X_train (Array): Set use for training in fit method\n                    X_test (Array): Set use to predict\n                    y_test (Array): Set use to compare predictions and true values\n                    y_train (Array): Set use for training in fit method\n\n    \"\"\"\n    if not os.path.exists(\"data\/x_train.npy\"):\n        print(\"X_train has not been created or has been deleted : cannot continue\")\n        return [None, None, None, None]\n    elif not os.path.exists(\"data\/x_test.npy\"):\n        print(\"X_test has not been created or has been deleted : cannot continue\")\n        return [None, None, None, None]\n    elif not os.path.exists(\"data\/y_train.npy\"):\n        print(\"y_train has not been created or has been deleted : cannot continue\")\n        return [None, None, None, None]\n    elif not os.path.exists(\"data\/y_test.npy\"):\n        print(\"y_test has not been created or has been deleted : cannot continue\")\n        return [None, None, None, None]\n    else:\n        X_train = np.load(\"data\/x_train.npy\")\n        X_test = np.load(\"data\/x_test.npy\")\n        y_train = np.load(\"data\/y_train.npy\")\n        y_test = np.load(\"data\/y_test.npy\")\n        return [X_train, X_test, y_train, y_test]","46457069":"df_train = pd.read_csv(INPUT_TRAIN)\ndf_test = pd.read_csv(INPUT_TEST)","d9eaab16":"df_train.info()","42cab0a5":"df_test.info()","6a9db7e0":"print(\"Are test columns in train columns ?\",\n      df_test.columns.isin(df_train.columns).all())","23c67f71":"check_and_plot_nan_percentage(\n    df=df_train, x_offset=0, y_offset=0, print_values=True\n)","9a605712":"check_and_plot_nan_percentage(\n    df=df_test, x_offset=0, y_offset=0, print_values=True\n)","a5b8d6ec":"df_train.head()","52eba266":"df_test.head()","7a37bf1d":"CATEGORICAL_COLUMNS = [\"country\", \"store\", \"product\"]","0081fc20":"for c in CATEGORICAL_COLUMNS:\n    print(\"Train:\", df_train[c].unique())\n    print(\"Test :\", df_test[c].unique())\n    print(\"Are train and test values the same ?\",\n          (df_test[c].unique() == df_train[c].unique()).all())","9e5deafa":"format = '%Y\/%m\/%d'\ndf_train['date'] = pd.to_datetime(df_train['date'], format=format)\ndf_test['date'] = pd.to_datetime(df_test['date'], format=format)","626f690f":"for c in df_train.columns:\n    if c not in CATEGORICAL_COLUMNS:\n        print(df_train[c].describe())\n        print((\"\\n\"))","3db28f69":"df_train['num_sold'].hist()\nplt.show()","4fa33ef6":"df_train = df_train.drop(columns=[\"row_id\"])","3b899ef6":"df_train[\"weekday\"] = df_train[\"date\"].dt.dayofweek\ndf_train[\"month\"] = df_train[\"date\"].dt.month\ndf_train[\"year\"] = df_train[\"date\"].dt.year\ndf_train['is_weekend'] = (df_train['date'].dt.weekday >= 5).astype(int)\n\ndf_test[\"weekday\"] = df_test[\"date\"].dt.dayofweek\ndf_test[\"month\"] = df_test[\"date\"].dt.month\ndf_test[\"year\"] = df_test[\"date\"].dt.year\ndf_test['is_weekend'] = (df_test['date'].dt.weekday >= 5).astype(int)","dd09366a":"df_train = df_train.drop(columns=[\"date\"])\ndf_test = df_test.drop(columns=[\"date\"])","e85976a1":"years = list(df_train[\"year\"].unique())\nmonths = list(df_train[\"month\"].unique())\ndays = list(df_train[\"weekday\"].unique())\ncountries = list(df_train[\"country\"].unique())\nstores = list(df_train[\"store\"].unique())\nproducts = list(df_train[\"product\"].unique())","bd36d8b6":"ax = df_train[\"is_weekend\"].value_counts(normalize=True).plot(kind=\"bar\",\n                                                              title=\"Sales distribution over weekends\",\n                                                              ylabel=\"Sales percentage\"\n                                                              )\nax.set_xticklabels([\"During week\", \"During weekend\"])\nplt.show()","c921c0eb":"total_sales_per_year = []\ntotal_sales_per_month_per_year = []\ntotal_sales_per_day_over_year = []\n\nfor i in range(0, len(years)):\n    total_sales_per_month_per_year.append([])\n    total_sales_per_day_over_year.append([])\n    total_sales_per_year.append(\n        df_train[df_train[\"year\"] == years[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"year\"] == years[i]].copy(deep=True)\n    for month in months:\n        total_sales_per_month_per_year[i].append(\n            sub_df[sub_df[\"month\"] == month][\"num_sold\"].sum())\n    for day in days:\n        total_sales_per_day_over_year[i].append(\n            sub_df[sub_df[\"weekday\"] == day][\"num_sold\"].sum())\n\ntotal_sales_per_year \/= sum(total_sales_per_year)\n\nplt.bar(years, total_sales_per_year)\nplt.xticks(years)\nplt.title(\"Total sales over years\")\nplt.ylabel(\"Percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(years), figsize=[16, 6], sharey=True)\nfor i in range(0, len(years)):\n    axs[i].bar(months, total_sales_per_month_per_year[i])\n    axs[i].set_title(str(\"Year: \" + str(years[i])))\n    axs[i].set_xticks(months)\n    axs[i].set_xlabel(\"Month\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(years), figsize=[16, 6], sharey=True)\nfor i in range(0, len(years)):\n    axs[i].bar(days, total_sales_per_day_over_year[i])\n    axs[i].set_title(str(\"Year: \" + str(years[i])))\n    axs[i].set_xticks(days)\n    axs[i].set_xlabel(\"Weekday\")\nplt.show()","a5c6a954":"total_sales_per_country = []\ntotal_sales_per_country_over_years = []\nfor i in range(0, len(countries)):\n    total_sales_per_country_over_years.append([])\n    total_sales_per_country.append(\n        df_train[df_train[\"country\"] == countries[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"country\"] == countries[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_country_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n\ntotal_sales_per_country \/= sum(total_sales_per_country)\nplt.bar(countries, total_sales_per_country)\nplt.xticks(countries)\nplt.title(\"Total sales over countries\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(countries), figsize=[16, 6], sharey=True)\nfor i in range(0, len(countries)):\n    axs[i].bar(years, total_sales_per_country_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Country: \" + str(countries[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()","473bd43a":"total_sales_per_store = []\ntotal_sales_per_store_over_years = []\ntotal_sales_per_store_per_country = []\ntotal_sales_per_store_per_product = []\n\nfor i in range(0, len(stores)):\n    total_sales_per_store_over_years.append([])\n    total_sales_per_store_per_country.append([])\n    total_sales_per_store_per_product.append([])\n    total_sales_per_store.append(\n        df_train[df_train[\"store\"] == stores[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"store\"] == stores[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_store_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n    for country in countries:\n        total_sales_per_store_per_country[i].append(\n            sub_df[sub_df[\"country\"] == country][\"num_sold\"].sum())\n    for product in products:\n        total_sales_per_store_per_product[i].append(\n            sub_df[sub_df[\"product\"] == product][\"num_sold\"].sum())\n\ntotal_sales_per_store \/= sum(total_sales_per_store)\nplt.bar(stores, total_sales_per_store)\nplt.xticks(stores)\nplt.title(\"Total sales per store over the years\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(years, total_sales_per_store_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(countries, total_sales_per_store_per_country[i])\n    axs[i].set_xticks(countries)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Country\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(stores), figsize=[16, 6], sharey=True)\nfor i in range(0, len(stores)):\n    axs[i].bar(products, total_sales_per_store_per_product[i])\n    axs[i].set_xticks(products)\n    axs[i].set_title(str(\"Store: \" + str(stores[i])))\n    axs[i].set_xlabel(\"Product\")\nplt.show()","56eaa75c":"total_sales_per_product = []\ntotal_sales_per_product_over_years = []\ntotal_sales_per_product_per_country = []\n\nfor i in range(0, len(products)):\n    total_sales_per_product_over_years.append([])\n    total_sales_per_product_per_country.append([])\n    total_sales_per_product.append(\n        df_train[df_train[\"product\"] == products[i]][\"num_sold\"].sum())\n    sub_df = df_train[df_train[\"product\"] == products[i]].copy(deep=True)\n    for year in years:\n        total_sales_per_product_over_years[i].append(\n            sub_df[sub_df[\"year\"] == year][\"num_sold\"].sum())\n    for country in countries:\n        total_sales_per_product_per_country[i].append(\n            sub_df[sub_df[\"country\"] == country][\"num_sold\"].sum())\n\ntotal_sales_per_product \/= sum(total_sales_per_product)\nplt.bar(products, total_sales_per_product)\nplt.xticks(products)\nplt.title(\"Total sales per product over the years\")\nplt.ylabel(\"Sales percentage\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(products), figsize=[16, 6], sharey=True)\nfor i in range(0, len(products)):\n    axs[i].bar(years, total_sales_per_product_over_years[i])\n    axs[i].set_xticks(years)\n    axs[i].set_title(str(\"Products: \" + str(products[i])))\n    axs[i].set_xlabel(\"Year\")\nplt.show()\n\nfig, axs = plt.subplots(1, len(products), figsize=[16, 6], sharey=True)\nfor i in range(0, len(products)):\n    axs[i].bar(countries, total_sales_per_product_per_country[i])\n    axs[i].set_xticks(countries)\n    axs[i].set_title(str(\"Product: \" + str(products[i])))\n    axs[i].set_xlabel(\"Country\")\nplt.show()","8ad5794e":"le = preprocessing.LabelEncoder()\n\nfor c in CATEGORICAL_COLUMNS:\n    df_train[c] = le.fit_transform(df_train[c])\n    df_test[c] = le.transform(df_test[c])","73cfcdfa":"y = np.array(df_train[\"num_sold\"])\ny = y.reshape(-1, 1)","74de1ca1":"X = np.array(df_train.drop(columns=\"num_sold\"))\nX_to_pred = np.array(df_test.drop(columns=[\"row_id\"]))","a0283192":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Save set to use it in different branches\nif not os.path.exists(\"data\/\"):\n    os.mkdir(\"data\/\")\nnp.save(\"data\/x_train.npy\", X_train)\nnp.save(\"data\/x_test.npy\", X_test)\nnp.save(\"data\/y_train.npy\", y_train)\nnp.save(\"data\/y_test.npy\", y_test)","3c3b9d30":"def smape(y_true, y_pred):\n    return (100\/y_true.shape[0]) * np.sum(2 * np.abs(y_pred - y_true) \/ (np.abs(y_true) + np.abs(y_pred)))\n\n# SMAPE must be lowered to increase performances\nsmape_score = make_scorer(score_func=smape, greater_is_better=False)","8992dcef":"X_train, X_test, y_train, y_test = load_train_test_set()","99ac54db":"gboost = GradientBoostingRegressor(random_state=0)\nparameters = {'n_estimators': (50, 75, 100, 150, 200, 250, 300, 500),\n              'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.08, 0.1, 0.15, 0.2),\n              'max_depth': (3, 5, 8), \"max_features\": [\"auto\", \"log2\"]\n              }\nres = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost = res.best_estimator_","7aceca3c":"biais = [i for i in range(-30, 30)]","59ab92be":"evaluate_smape_model(model=opti_gboost, X_test=X_test, y_test=y_test, biais_values=biais)","9a3e4633":"X_train, X_test, y_train, y_test = load_train_test_set()","b1359f21":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y_train > limit\ny_train[outliers] = np.mean(y)\noutliers = y_test > limit\ny_test[outliers] = np.mean(y)","a85dd1c9":"res = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost2 = res.best_estimator_","fa984c04":"evaluate_smape_model(model=opti_gboost2, X_test=X_test, y_test=y_test, biais_values=biais)","da771a65":"X_train, X_test, y_train, y_test = load_train_test_set()","d4cb7ef4":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y_train > limit\ny_train[outliers] = limit\noutliers = y_test > limit\ny_test[outliers] = limit","05d8949e":"res = GridSearchCV(estimator=gboost, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch= \", res.best_params_)\n\nopti_gboost3 = res.best_estimator_","bb7989be":"evaluate_smape_model(model=opti_gboost3, X_test=X_test, y_test=y_test, biais_values=biais)","b559068e":"X_train, X_test, y_train, y_test = load_train_test_set()","4629b73e":"rfr = RandomForestRegressor()\nparameters = {'n_estimators': (100, 200, 300, 500, 1000, 1500, 2000),\n              'max_depth': [None, 3, 5],\n              'bootstrap': [True, False]\n              }\nres = GridSearchCV(estimator=rfr, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch: \", res.best_params_)\n\nopti_rfr = res.best_estimator_","62bfff97":"evaluate_smape_model(model=opti_rfr, X_test=X_test, y_test=y_test, biais_values=biais)","8ebab3fd":"X_train, X_test, y_train, y_test = load_train_test_set()","3f1a25ab":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y_train > limit\ny_train[outliers] = np.mean(y)\noutliers = y_test > limit\ny_test[outliers] = np.mean(y)","73403627":"res = GridSearchCV(estimator=rfr, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch: \", res.best_params_)\n\nopti_rfr2 = res.best_estimator_","0d6aa157":"evaluate_smape_model(model=opti_rfr2, X_test=X_test, y_test=y_test, biais_values=biais)","115a3dfa":"X_train, X_test, y_train, y_test = load_train_test_set()","39d277e6":"y = np.array(df_train[\"num_sold\"])\nlimit = round(np.percentile(y, 96), 2)\noutliers = y_train > limit\ny_train[outliers] = limit\noutliers = y_test > limit\ny_test[outliers] = limit","5a071ea5":"res = GridSearchCV(estimator=rfr, param_grid=parameters,\n                   scoring=smape_score, cv=5).fit(X_train, y_train.ravel())\n\nprint(\"Best params found through the gridsearch: \", res.best_params_)\n\nopti_rfr3 = res.best_estimator_","6b1f823b":"evaluate_smape_model(model=opti_rfr3, X_test=X_test, y_test=y_test, biais_values=biais)","6b5fce6c":"X_train, X_test, y_train, y_test = load_train_test_set()","b6cd1ce2":"best_biais = -2","35562a84":"num_sold = np.round(opti_gboost.predict(X_to_pred)).reshape(-1, 1).astype(int) + best_biais","9d08d10a":"df = pd.read_csv(SUBMISSION)\ndf[\"num_sold\"] = num_sold\ndf.to_csv(\"submission.csv\", index=False)","3f347087":"df_train_original = pd.read_csv(INPUT_TRAIN)","5d37b2d7":"for c in CATEGORICAL_COLUMNS:\n    df_train_original[c] = le.fit_transform(df_train_original[c])\n    df_test[c] = le.inverse_transform(df_test[c])","8bbae95e":"df_test[\"num_sold\"] = num_sold","df79650c":"kagglemart_sales = df_test[df_test[\"store\"] == \"KaggleMart\"][\"num_sold\"].sum()\nkagglerama_sales = df_test[df_test[\"store\"] == \"KaggleRama\"][\"num_sold\"].sum()\nprint(\"Sales predictions for KaggleMart\", kagglemart_sales)\nprint(\"Sales predictions for KaggleRama\", kagglerama_sales)\nprint(\"Checksum valid ?\", num_sold.sum() == kagglemart_sales + kagglerama_sales)","ce33e67b":"* Sales growth from year to year\n* A seasonality is observable over the years: Sales increase in December and January, decrease in February, increase again in March, April and May, then decrease until August and increase again from September\n* Weekend concentrate sales","9bdbeff5":"# 9. Impact of replacing outliers by limit (96th percentile)","e2358663":"# 5. DATA EXPLORATION","c1b5db87":"# 8. Impact of replacing outliers by mean","48702f0f":"Here the gradient boosting model with a biais of -2 and without preprocessing on num_sold is selected despite the SMAPE on test set is not the lower. Indeed, clamping the value of num_sold based on a limit defined from the past could be a good idea only if the sale behaviour will be same as the one in past. But the data exploration shows that there is a stable growth market year from year to year so clamping the values could not represent this growth.","a9169962":"### 2.2 Check missing values on each column","af88fa25":"I will train Gradient boosting + random forest regressor on the train set. The training will be performed on a grid to optimize hyperparameters and validate them through a cross validation on 5-folds. \n\nGridSearchCV will be used to achieve it.\n\nFirst, load train and test set from data folder to make sure to share the same set between all branches \/ try \/ models.","a5a4b14f":"# REFERENCES","6b8934d1":"# 12. Impact of replacing outliers by limit","80bb8ae6":"# 4. FEATURE ENGINEERING","97812f99":"# 11. Impact of replacing outliers by mean","04c8b460":"# 3. CLEANING","d78a9c6b":"<b> Conclusion:<\/b> KaggleRama will have the higher number of sales for 2019.","98fceffc":"# 6.PREPROCESSING","889bc9cf":"# CONSTANTS","407be000":"### 6.4 Create the appropriate score","7d9a7fbe":"# 10. Train the random forest regressor as the gradient boosting","9b1a154c":"* Tensor girl: https:\/\/www.kaggle.com\/usharengaraju\/tensorflow-tf-data-keraspreprocessinglayers-w-b for the cheat code to compute the feature is_weekend in one line of code\n\n* SMAPE formula : https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error","971fc96d":"# 1. DATA LOADING","ca48298b":"I drop date column on each dataset after doing the feature engineering because the intertia will be coupled otherwise.","7c4f152b":"# 2. FIRST DATA LOOK AROUND","f56bc155":"### 6.3 Split train dataframe into train and test set","4644a2f5":"# 7. Training","5a30c6af":"High number of estimators with random forest regressor avoid overfitting but consume more computation time.","a695bc10":"### 6.1 Encode categorical columns","cde6dd99":"# 13. Forecast the number of sales for the coming years (Test dataframe)","5d1c7bc8":"Below I define a list of values that will be added to the output predictions in order to check if the global minimum has been reached.","48fb0646":"### 2.5 Describe numerical columns","99ff24c5":"### 2.4 Convert date to datetime object with the right format","beaf3363":"### 6.2 Construct X and y arrays","d948c2a1":"# FUNCTIONS","8671464d":"# 14. Sales predictions per store","cb57dfb9":"### 2.3 Store categorcial columns and check unique values between train and test set"}}