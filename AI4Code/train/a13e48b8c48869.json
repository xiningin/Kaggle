{"cell_type":{"cdbb86c8":"code","739fd9bd":"code","4a3ac097":"code","afb143dc":"code","f6afef8f":"code","5e2f1d9a":"code","dc3503b3":"code","20a3c49a":"code","8e13f0f9":"code","821275db":"code","3f0d1054":"code","8d39ffbf":"code","7a428041":"code","afd9bdb7":"code","b943ef24":"code","dc23c6e9":"markdown","3986604a":"markdown","2c45cf0a":"markdown","29efa813":"markdown","574710cc":"markdown","a304ad59":"markdown","75918069":"markdown","5e1b425c":"markdown","38866625":"markdown","62d69e09":"markdown","0cac10fe":"markdown","b90f7405":"markdown","01450fa1":"markdown","46d112c5":"markdown","cb8a9ba2":"markdown"},"source":{"cdbb86c8":"import numpy as np\nimport spacy\n\n# Need to load the large model to get the vectors\nnlp = spacy.load('en_core_web_lg')","739fd9bd":"import pandas as pd\nval = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nval","4a3ac097":"# Disabling other pipes because we don't need them and it'll speed up this part a bit\ntext = val.loc[1,\"less_toxic\"]\nwith nlp.disable_pipes():\n    vectors = np.array([token.vector for token in  nlp(text)])","afb143dc":"vectors.shape","f6afef8f":"vectors","5e2f1d9a":"len(nlp(text))","dc3503b3":"# from sklearn.model_selection import train_test_split\n\n# X_train, X_test, y_train, y_test = train_test_split(doc_vectors, spam.label,\n#                                                     test_size=0.1, random_state=1)","20a3c49a":"# from sklearn.svm import LinearSVC\n\n# # Set dual=False to speed up training, and it's not needed\n# svc = LinearSVC(random_state=1, dual=False, max_iter=10000)\n# svc.fit(X_train, y_train)\n# print(f\"Accuracy: {svc.score(X_test, y_test) * 100:.3f}%\", )","8e13f0f9":"def cosine_similarity(a, b):\n    return a.dot(b)\/np.sqrt(a.dot(a) * b.dot(b))","821275db":"a = nlp(\"REPLY NOW FOR FREE TEA\").vector\nb = nlp(\"According to legend, Emperor Shen Nung discovered tea when leaves from a wild tree blew into his pot of boiling water.\").vector\ncosine_similarity(a, b)","3f0d1054":"print(val.loc[0,\"less_toxic\"])","8d39ffbf":"print(val.loc[0,\"more_toxic\"])","7a428041":"a = nlp(val.loc[0,\"less_toxic\"]).vector\nb = nlp(val.loc[0,\"more_toxic\"]).vector\ncosine_similarity(a, b)","afd9bdb7":"a = nlp(val.loc[0,\"less_toxic\"]).vector\nb = nlp(val.loc[0,\"less_toxic\"]).vector\ncosine_similarity(a, b)","b943ef24":"val_sim = [cosine_similarity(nlp(val.loc[i,\"less_toxic\"]).vector, nlp(val.loc[i,\"more_toxic\"]).vector) for i in range(10)]\nprint(val_sim)","dc23c6e9":"pixyz\nlast update 2021 11 17\nYukkurisitettene!","3986604a":"**Marisa:Today, Word vector with Jigsaw data.  \nReimu:Let's go!**  \n<img src=\"https:\/\/2.bp.blogspot.com\/-wg1XBbsYtQ4\/U1T3p1v7o3I\/AAAAAAAAfUg\/A8XE8Wj1ahA\/s400\/figure_happy.png\" width = 100>","2c45cf0a":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https:\/\/www.kaggle.com\/learn\/natural-language-processing\/discussion) to chat with other learners.*","29efa813":"## Classification Models (Skip)\n\nWith the document vectors, you can train scikit-learn models, xgboost models, or any other standard approach to modeling.","574710cc":"## Document Similarity\n\nDocuments with similar content generally have similar vectors. So you can find similar documents by measuring the similarity between the vectors. A common metric for this is the **cosine similarity** which measures the angle between two vectors, $\\mathbf{a}$ and $\\mathbf{b}$.\n\n$$\n\\cos \\theta = \\frac{\\mathbf{a}\\cdot\\mathbf{b}}{\\| \\mathbf{a} \\| \\, \\| \\mathbf{b} \\|}\n$$\n\nThis is the dot product of $\\mathbf{a}$ and $\\mathbf{b}$, divided by the magnitudes of each vector. The cosine similarity can vary between -1 and 1, corresponding complete opposite to perfect similarity, respectively. To calculate it, you can use [the metric from scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.pairwise.cosine_similarity.html) or write your own function.","a304ad59":"These are 300-dimensional vectors, with one vector for each word. However, we only have document-level labels and our models won't be able to use the word-level embeddings. So, you need a vector representation for the entire document. \n\nThere are many ways to combine all the word vectors into a single document vector we can use for model training. A simple and surprisingly effective approach is simply averaging the vectors for each word in the document. Then, you can use these document vectors for modeling.\n\nspaCy calculates the average document vector which you can get with `doc.vector`. Here is an example loading the spam data and converting it to document vectors.","75918069":"**Reimu:If a and b are same word,Which is it 1.0 or -1.0?**","5e1b425c":"**Marisa:This is vector of val[1,\"less_toxic\"]  \nReimu:vector.shape is (50 , 300). What is first number?**\n<img src=\"http:\/\/3.bp.blogspot.com\/-KmQQLtEkmLw\/U1T3r7D0NdI\/AAAAAAAAfVI\/c2d4n2kG00U\/s180-c\/figure_question.png\" width = 100>","38866625":"[![](https:\/\/img.youtube.com\/vi\/FnV0thLS1Fs\/0.jpg)](https:\/\/www.youtube.com\/watch?v=FnV0thLS1Fs)","62d69e09":"**Reimu:Let's do with Jigsaw data.**","0cac10fe":"**Marisa:I'll skip Classification Models.**","b90f7405":"# Word Embeddings\n\nYou know at this point that machine learning on text requires that you first represent the text numerically. So far, you've done this with bag of words representations. But you can usually do better with word embeddings.\n\n**Word embeddings** (also called word vectors) represent each word numerically in such a way that the vector corresponds to how that word is used or what it means. Vector encodings are learned by considering the context in which the words appear. Words that appear in similar contexts will have similar vectors. For example, vectors for \"leopard\", \"lion\", and \"tiger\" will be close together, while they'll be far away from \"planet\" and \"castle\".\n\nEven cooler, relations between words can be examined with mathematical operations. Subtracting the vectors for \"man\" and \"woman\" will return another vector. If you add that to the vector for \"king\" the result is close to the vector for \"queen.\"\n\n![Word vector examples](https:\/\/www.tensorflow.org\/images\/linear-relationships.png)\n\nThese vectors can be used as features for machine learning models. Word vectors will typically improve the performance of your models above bag of words encoding. spaCy provides embeddings learned from a model called Word2Vec. You can access them by loading a large language model like `en_core_web_lg`. Then they will be available on tokens from the `.vector` attribute.","01450fa1":"Here is an example using [support vector machines (SVMs)](https:\/\/scikit-learn.org\/stable\/modules\/svm.html#svm). Scikit-learn provides an SVM classifier `LinearSVC`. This works similar to other scikit-learn models.","46d112c5":"**Marisa:Almost cosine_similarity(a,b) are 0.7~0.9**","cb8a9ba2":"<img src=\"http:\/\/3.bp.blogspot.com\/-KmQQLtEkmLw\/U1T3r7D0NdI\/AAAAAAAAfVI\/c2d4n2kG00U\/s180-c\/figure_question.png\" width = 100>"}}