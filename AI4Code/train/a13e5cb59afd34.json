{"cell_type":{"223eb423":"code","3c340ce7":"code","5e17bc85":"code","5a672312":"code","003ee47d":"code","3bf0e535":"code","92b10050":"code","2e8affd1":"code","ca2b2d88":"code","eb6341df":"code","b2f1c35e":"code","a5c32bb2":"code","4e8c8a12":"code","478c5808":"code","7fd70ecd":"code","f2372dff":"code","6a1ffc20":"code","8151877e":"code","ef4daa8d":"code","fd23bd75":"code","5e89c661":"code","c927dac6":"code","5427c602":"code","181b3c6f":"code","97c2bcb6":"code","90583ad4":"code","52de3444":"code","049ec503":"code","78b62abf":"code","9a0ac87e":"code","49eb1ca1":"code","28368f88":"code","ea71f777":"code","888b21b0":"code","44bbd33e":"code","b7ceb7ee":"code","a2c3e072":"code","e067d90d":"code","51931c44":"code","8275f86a":"code","7d88a657":"code","57213bef":"code","747eb50a":"code","df9dd89f":"code","6bac2a72":"code","28a94eb1":"code","bb24c995":"code","ad8e2b7b":"code","9b1a4482":"code","1a4a05a9":"code","9bc556d6":"code","4a357947":"markdown","cb472142":"markdown","48246564":"markdown","da6079a4":"markdown","1b53617d":"markdown","1b6d700b":"markdown","3b171f11":"markdown","b6ba5e41":"markdown","4a7024a1":"markdown","469f29c5":"markdown","6154ea8b":"markdown","1bdd8c7f":"markdown","601664f3":"markdown","a9164167":"markdown","e05ae514":"markdown","d505e8ff":"markdown"},"source":{"223eb423":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c340ce7":"from itertools import product\nfrom collections import Counter\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nimport category_encoders as ce\nimport warnings\n\npd.set_option('display.max_rows', 400)\npd.set_option('display.max_columns', 160)\npd.set_option('display.max_colwidth', 40)\nwarnings.filterwarnings(\"ignore\")","5e17bc85":"test = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest.head()","5a672312":"categories = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/categories.csv')\npd.DataFrame(categories.category_name.values.reshape(-1, 4))","003ee47d":"#create broader category groupings\ncategories['group_name'] = categories['category_name'].str.extract(r'(^[\\w\\s]*)')\ncategories['group_name'] = categories['group_name'].str.strip()\n#label encode group names\ncategories['group_id']  = le.fit_transform(categories.group_name.values)\ncategories.sample(5)","3bf0e535":"#load items\nitems = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/items.csv')\n\n#clean item_name\nitems['item_name'] = items['item_name'].str.lower()\nitems['item_name'] = items['item_name'].str.replace('.', '')\nfor i in [r'[^\\w\\d\\s\\.]', r'\\bthe\\b', r'\\bin\\b', r'\\bis\\b',r'\\bfor\\b', r'\\bof\\b', r'\\bon\\b', r'\\band\\b',  r'\\bto\\b', r'\\bwith\\b' , r'\\byo\\b']:\n    items['item_name'] = items['item_name'].str.replace(i, ' ')\nitems['item_name'] = items['item_name'].str.replace(r'\\b.\\b', ' ')\n\n#extract first n characters of name\nitems['item_name_no_space'] = items['item_name'].str.replace(' ', '')\nitems['item_name_first4'] = [x[:4] for x in items['item_name_no_space']]\nitems['item_name_first6'] = [x[:6] for x in items['item_name_no_space']]\nitems['item_name_first11'] = [x[:11] for x in items['item_name_no_space']]\ndel items['item_name_no_space']\n                              \n#label encode these columns\nitems.item_name_first4 = le.fit_transform(items.item_name_first4.values)\nitems.item_name_first6 = le.fit_transform(items.item_name_first6.values)\nitems.item_name_first11 = le.fit_transform(items.item_name_first11.values)\n\n#join category_name, group_name and group_id to items\nitems = items.join(categories.set_index('category_id'), on='category_id')\nitems.sample(5)","92b10050":"dupes = items[(items.duplicated(subset=['item_name','category_id'],keep=False))]\ndupes['in_test'] = dupes.item_id.isin(test.item_id.unique())\ndupes = dupes.groupby('item_name').agg({'item_id':['first','last'],'in_test':['first','last']})\n\n#if both item id's are in the test set do nothing\ndupes = dupes[(dupes[('in_test', 'first')]==False) | (dupes[('in_test', 'last')]==False)]\n#if only the first id is in the test set assign this id to both\ntemp = dupes[dupes[('in_test', 'first')]==True]\nkeep_first = dict(zip(temp[('item_id', 'last')], temp[('item_id',  'first')]))\n#if neither id or only the second id is in the test set, assign the second id to both\ntemp = dupes[dupes[('in_test', 'first')]==False]\nkeep_second = dict(zip(temp[('item_id', 'first')], temp[('item_id',  'last')]))\nitem_map = {**keep_first, **keep_second}","2e8affd1":"#loading sales data\nsales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nsales = (sales\n    .query('0 < item_price < 50000 and 0 < item_cnt_day < 1001') #removing outliers\n    .replace({\n        'shop_id':{0:57, 1:58, 11:10}, #replacing obsolete shop id's\n        'item_id':item_map #fixing duplicate item id's  \n    })    \n)\n\n#removing shops which don't appear in the test set\nsales = sales[sales['shop_id'].isin(test.shop_id.unique())]\n\nsales['date'] = pd.to_datetime(sales.date,format='%d.%m.%Y')\nsales['weekday'] = sales.date.dt.dayofweek\n\n#first day the item was sold, day 0 is the first day of the training set period\nsales['first_sale_day'] = sales.date.dt.dayofyear \nsales['first_sale_day'] += 365 * (sales.date.dt.year-2013)\nsales['first_sale_day'] = sales.groupby('item_id')['first_sale_day'].transform('min').astype('int16')\n\n#revenue is needed to accurately calculate prices after grouping\nsales['revenue'] = sales['item_cnt_day']*sales['item_price']","ca2b2d88":"temp = sales.groupby(['shop_id','weekday']).agg({'item_cnt_day':'sum'}).reset_index()\ntemp = pd.merge(temp, sales.groupby(['shop_id']).agg({'item_cnt_day':'sum'}).reset_index(), on='shop_id', how='left')\ntemp.columns = ['shop_id','weekday', 'shop_day_sales', 'shop_total_sales']\ntemp['day_quality'] = temp['shop_day_sales']\/temp['shop_total_sales']\ntemp = temp[['shop_id','weekday','day_quality']]\n\ndates = pd.DataFrame(data={'date':pd.date_range(start='2013-01-01',end='2015-11-30')})\ndates['weekday'] = dates.date.dt.dayofweek\ndates['month'] = dates.date.dt.month\ndates['year'] = dates.date.dt.year - 2013\ndates['date_block_num'] = dates['year']*12 + dates['month'] - 1\ndates['first_day_of_month'] = dates.date.dt.dayofyear\ndates['first_day_of_month'] += 365 * dates['year']\ndates = dates.join(temp.set_index('weekday'), on='weekday')\ndates = dates.groupby(['date_block_num','shop_id','month','year']).agg({'day_quality':'sum','first_day_of_month':'min'}).reset_index()\n\ndates.query('shop_id == 28').head(15)","eb6341df":"sales = (sales.groupby(['date_block_num', 'shop_id', 'item_id'])\n         .agg({'item_cnt_day':'sum', 'revenue':'sum','first_sale_day':'first'}).reset_index()\n         .rename(columns={'item_cnt_day':'item_cnt'})\n        )\nsales.sample(5)","b2f1c35e":"df = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    df.append(np.array(list(product(*[cur_shops, cur_items, [block_num]]))))\n\ndf = pd.DataFrame(np.vstack(df), columns=['shop_id', 'item_id', 'date_block_num'])\ndf.head()","a5c32bb2":"#add the appropriate date_block_num value to the test set\ntest['date_block_num'] = 34\ndel test['ID']","4e8c8a12":"#append test set to training dataframe\ndf = pd.concat([df,test]).fillna(0)\ndf = df.reset_index()\ndel df['index']","478c5808":"#join sales and item inforamtion to the training dataframe\ndf = pd.merge(df, sales, on=['shop_id', 'item_id', 'date_block_num'], how='left').fillna(0)\ndf = pd.merge(df, dates, on=['date_block_num','shop_id'], how='left')\ndf = pd.merge(df, items.drop(columns=['item_name','group_name','category_name']), on='item_id', how='left')","7fd70ecd":"#loading shops.csv\nshops = pd.read_csv('..\/input\/predict-future-sales-eng-translation\/shops.csv')\n\n#clustering shops\nshops_cats = pd.DataFrame(\n    np.array(list(product(*[df['shop_id'].unique(), df['category_id'].unique()]))),\n    columns =['shop_id', 'category_id']\n)\ntemp = df.groupby(['category_id', 'shop_id']).agg({'item_cnt':'sum'}).reset_index()\ntemp2 = temp.groupby('shop_id').agg({'item_cnt':'sum'}).rename(columns={'item_cnt':'shop_total'})\ntemp = temp.join(temp2, on='shop_id')\ntemp['category_proportion'] = temp['item_cnt']\/temp['shop_total']\ntemp = temp[['shop_id', 'category_id', 'category_proportion']]\nshops_cats = pd.merge(shops_cats, temp, on=['shop_id','category_id'], how='left')\nshops_cats = shops_cats.fillna(0)\n\nshops_cats = shops_cats.pivot(index='shop_id', columns=['category_id'])\nkmeans = KMeans(n_clusters=7, random_state=0).fit(shops_cats)\nshops_cats['shop_cluster'] = kmeans.labels_.astype('int8')\n\n#adding these clusters to the shops dataframe\nshops = shops.join(shops_cats['shop_cluster'], on='shop_id')","f2372dff":"#removing unused shop ids\nshops.dropna(inplace=True)\n\n#cleaning the name column\nshops['shop_name'] = shops['shop_name'].str.lower()\nshops['shop_name'] = shops['shop_name'].str.replace(r'[^\\w\\d\\s]', ' ')\n\n#creating a column for the type of shop\nshops['shop_type'] = 'regular'\n\n#there is some overlap in tc and mall, mall is given precedence\nshops.loc[shops['shop_name'].str.contains(r'tc'), 'shop_type'] = 'tc'\nshops.loc[shops['shop_name'].str.contains(r'mall|center|mega'), 'shop_type'] = 'mall'\nshops.loc[shops['shop_id'].isin([9,20]), 'shop_type'] = 'special'\nshops.loc[shops['shop_id'].isin([12,55]), 'shop_type'] = 'online'\n\n#the first word of shop name is largely sufficient as a city feature\nshops['shop_city'] = shops['shop_name'].str.split().str[0]\nshops.loc[shops['shop_id'].isin([12,55]), 'shop_city'] = 'online'\nshops.shop_city = le.fit_transform(shops.shop_city.values)\nshops.shop_type = le.fit_transform(shops.shop_type.values)\nshops.head()","6a1ffc20":"#add shop information to the training dataframe\ndf = pd.merge(df, shops.drop(columns='shop_name'), on='shop_id', how='left')\ndf.head()","8151877e":"df['first_sale_day'] = df.groupby('item_id')['first_sale_day'].transform('max').astype('int16')\ndf.loc[df['first_sale_day']==0, 'first_sale_day'] = 1035\ndf['prev_days_on_sale'] = [max(idx) for idx in zip(df['first_day_of_month']-df['first_sale_day'],[0]*len(df))]\ndel df['first_day_of_month']","ef4daa8d":"#freeing RAM, removing unneeded columns and encoding object columns\ndel sales, categories, shops, shops_cats, temp, temp2, test, dupes, item_map, \ndf.head()","fd23bd75":"df['item_cnt_unclipped'] = df['item_cnt']\ndf['item_cnt'] = df['item_cnt'].clip(0, 20)","5e89c661":"def downcast(df):\n    #reduce size of the dataframe\n    float_cols = [c for c in df if df[c].dtype in [\"float64\"]]\n    int_cols = [c for c in df if df[c].dtype in ['int64']]\n    df[float_cols] = df[float_cols].astype('float32')\n    df[int_cols] = df[int_cols].astype('int16')\n    return df\ndf = downcast(df)","c927dac6":"df['item_age'] = (df['date_block_num'] - df.groupby('item_id')['date_block_num'].transform('min')).astype('int8')\ndf['item_name_first4_age'] = (df['date_block_num'] - df.groupby('item_name_first4')['date_block_num'].transform('min')).astype('int8')\ndf['item_name_first6_age'] = (df['date_block_num'] - df.groupby('item_name_first6')['date_block_num'].transform('min')).astype('int8')\ndf['item_name_first11_age'] = (df['date_block_num'] - df.groupby('item_name_first11')['date_block_num'].transform('min')).astype('int8')\ndf['category_age'] = (df['date_block_num'] - df.groupby('category_id')['date_block_num'].transform('min')).astype('int8')\ndf['group_age'] = (df['date_block_num'] - df.groupby('group_id')['date_block_num'].transform('min')).astype('int8')\ndf['shop_age'] = (df['date_block_num'] - df.groupby('shop_id')['date_block_num'].transform('min')).astype('int8')","5427c602":"def agg_cnt_col(df, merging_cols, new_col,aggregation):\n    temp = df.groupby(merging_cols).agg(aggregation).reset_index()\n    temp.columns = merging_cols + [new_col]\n    df = pd.merge(df, temp, on=merging_cols, how='left')\n    return df\n\n#individual items across all shops\ndf = agg_cnt_col(df, ['date_block_num','item_id'],'item_cnt_all_shops',{'item_cnt':'mean'})\ndf = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'item_cnt_all_shops_median',{'item_cnt':'median'}) \n#all items in category at individual shops\ndf = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'category_cnt',{'item_cnt':'mean'})\ndf = agg_cnt_col(df, ['date_block_num','category_id','shop_id'],'category_cnt_median',{'item_cnt':'median'}) \n#all items in category across all shops\ndf = agg_cnt_col(df, ['date_block_num','category_id'],'category_cnt_all_shops',{'item_cnt':'mean'})\ndf = agg_cnt_col(df, ['date_block_num','category_id'],'category_cnt_all_shops_median',{'item_cnt':'median'})\n#all items in group\ndf = agg_cnt_col(df, ['date_block_num','group_id','shop_id'],'group_cnt',{'item_cnt':'mean'})\n#all items in group across all shops\ndf = agg_cnt_col(df, ['date_block_num','group_id'],'group_cnt_all_shops',{'item_cnt':'mean'})\n#all items at individual shops\ndf = agg_cnt_col(df, ['date_block_num','shop_id'],'shop_cnt',{'item_cnt':'mean'})\n#all items at all shops within the city\ndf = agg_cnt_col(df, ['date_block_num','shop_city'],'city_cnt',{'item_cnt':'mean'})","181b3c6f":"def new_item_sales(df, merging_cols, new_col):\n    temp = (\n        df\n        .query('item_age==0')\n        .groupby(merging_cols)['item_cnt']\n        .mean()\n        .reset_index()\n        .rename(columns={'item_cnt': new_col})\n    )\n    df = pd.merge(df, temp, on=merging_cols, how='left')\n    return df\n\n#mean units sold of new item in category at individual shop\ndf = new_item_sales(df, ['date_block_num','category_id','shop_id'], 'new_items_in_cat')\n#mean units sold of new item in category across all shops\ndf = new_item_sales(df, ['date_block_num','category_id'], 'new_items_in_cat_all_shops')","97c2bcb6":"def agg_price_col(df, merging_cols, new_col):\n    temp = df.groupby(merging_cols).agg({'revenue':'sum','item_cnt_unclipped':'sum'}).reset_index()\n    temp[new_col] = temp['revenue']\/temp['item_cnt_unclipped']\n    temp = temp[merging_cols + [new_col]]\n    df = pd.merge(df, temp, on=merging_cols, how='left')\n    return df\n\n#average item price\ndf = agg_price_col(df,['date_block_num','item_id'],'item_price')\n#average price of items in category\ndf = agg_price_col(df,['date_block_num','category_id'],'category_price')\n#average price of all items\ndf = agg_price_col(df,['date_block_num'],'block_price')","90583ad4":"df = downcast(df)","52de3444":"def lag_feature(df, lag, col, merge_cols):        \n    temp = df[merge_cols + [col]]\n    temp = temp.groupby(merge_cols).agg({f'{col}':'first'}).reset_index()\n    temp.columns = merge_cols + [f'{col}_lag{lag}']\n    temp['date_block_num'] += lag\n    df = pd.merge(df, temp, on=merge_cols, how='left')\n    df[f'{col}_lag{lag}'] = df[f'{col}_lag{lag}'].fillna(0).astype('float32')\n    return df\n","049ec503":"lag12_cols = {\n    'item_cnt':['date_block_num', 'shop_id', 'item_id'], 'item_cnt_all_shops':['date_block_num', 'item_id'],\n    'category_cnt':['date_block_num', 'shop_id', 'category_id'],'category_cnt_all_shops':['date_block_num', 'category_id'],\n    'group_cnt':['date_block_num', 'shop_id', 'group_id'], 'group_cnt_all_shops':['date_block_num', 'group_id'],\n    'shop_cnt':['date_block_num', 'shop_id'], 'city_cnt':['date_block_num', 'shop_city'],\n    'new_items_in_cat':['date_block_num', 'shop_id', 'category_id'],'new_items_in_cat_all_shops':['date_block_num', 'category_id']\n}\nfor col,merge_cols in lag12_cols.items():\n    df[f'{col}_lag1to12'] = 0\n    for i in range(1,13):\n        df = lag_feature(df, i, col, merge_cols)\n        df[f'{col}_lag1to12'] += df[f'{col}_lag{i}']\n        if i > 2:\n            del df[f'{col}_lag{i}']\n    if col == 'item_cnt':\n        del df[f'{col}_lag1']\n        del df[f'{col}_lag2']        \n    else:\n        del df[col]","78b62abf":"lag2_cols = {\n    'item_cnt_unclipped':['date_block_num', 'shop_id', 'item_id'],\n    'item_cnt_all_shops_median':['date_block_num', 'item_id'],\n    'category_cnt_median':['date_block_num', 'shop_id', 'category_id'],\n    'category_cnt_all_shops_median':['date_block_num', 'category_id']\n}\nfor col in lag2_cols:\n    df = lag_feature(df, 1, col, merge_cols)\n    df = lag_feature(df, 2, col, merge_cols)\n    if col!='item_cnt_unclipped':\n        del df[col]","9a0ac87e":"df['item_cnt_diff'] = df['item_cnt_unclipped_lag1']\/df['item_cnt_lag1to12']\ndf['item_cnt_all_shops_diff'] = df['item_cnt_all_shops_lag1']\/df['item_cnt_all_shops_lag1to12']\ndf['category_cnt_diff'] = df['category_cnt_lag1']\/df['category_cnt_lag1to12']\ndf['category_cnt_all_shops_diff'] = df['category_cnt_all_shops_lag1']\/df['category_cnt_all_shops_lag1to12']","49eb1ca1":"df = lag_feature(df, 1, 'category_price',['date_block_num', 'category_id'])\ndf = lag_feature(df, 1, 'block_price',['date_block_num'])\ndel df['category_price'], df['block_price']","28368f88":"df.loc[(df['item_age']>0) & (df['item_cnt_lag1to12'].isna()), 'item_cnt_lag1to12'] = 0\ndf.loc[(df['category_age']>0) & (df['category_cnt_lag1to12'].isna()), 'category_cnt_lag1to12'] = 0\ndf.loc[(df['group_age']>0) & (df['group_cnt_lag1to12'].isna()), 'group_cnt_lag1to12'] = 0","ea71f777":"df['item_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['item_age'],df['shop_age'],[12]*len(df))]\ndf['item_cnt_all_shops_lag1to12'] \/= [min(idx) for idx in zip(df['item_age'],[12]*len(df))]\ndf['category_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['category_age'],df['shop_age'],[12]*len(df))]\ndf['category_cnt_all_shops_lag1to12'] \/= [min(idx) for idx in zip(df['category_age'],[12]*len(df))]\ndf['group_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['group_age'],df['shop_age'],[12]*len(df))]\ndf['group_cnt_all_shops_lag1to12'] \/= [min(idx) for idx in zip(df['group_age'],[12]*len(df))]\ndf['city_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['date_block_num'],[12]*len(df))]\ndf['shop_cnt_lag1to12'] \/= [min(idx) for idx in zip(df['shop_age'],[12]*len(df))]\ndf['new_items_in_cat_lag1to12'] \/= [min(idx) for idx in zip(df['category_age'],df['shop_age'],[12]*len(df))]\ndf['new_items_in_cat_all_shops_lag1to12'] \/= [min(idx) for idx in zip(df['category_age'],[12]*len(df))]","888b21b0":"df = downcast(df)","44bbd33e":"def past_information(df, merging_cols, new_col, aggregation):\n    temp = []\n    for i in range(1,35):\n        block = df.query(f'date_block_num < {i}').groupby(merging_cols).agg(aggregation).reset_index()\n        block.columns = merging_cols + [new_col]\n        block['date_block_num'] = i\n        block = block[block[new_col]>0]\n        temp.append(block)\n    temp = pd.concat(temp)\n    df = pd.merge(df, temp, on=['date_block_num']+merging_cols, how='left')\n    return df\n\n#average item price in latest block item was sold\ndf = past_information(df, ['item_id'],'last_item_price',{'item_price':'last'})\n#total units of item sold at individual shop\ndf = past_information(df, ['shop_id','item_id'],'item_cnt_sum_alltime',{'item_cnt':'sum'})\n#total units of item sold at all shops\ndf = past_information(df, ['item_id'],'item_cnt_sum_alltime_allshops',{'item_cnt':'sum'})\n\n#these columns are no longer needed, and would cause data leakage if retained\ndel df['revenue'], df['item_cnt_unclipped'], df['item_price']","b7ceb7ee":"df['relative_price_item_block_lag1'] = df['last_item_price']\/df['block_price_lag1']","a2c3e072":"df['item_cnt_per_day_alltime'] = (df['item_cnt_sum_alltime']\/df['prev_days_on_sale']).fillna(0)\ndf['item_cnt_per_day_alltime_allshops'] = (df['item_cnt_sum_alltime_allshops']\/df['prev_days_on_sale']).fillna(0)","e067d90d":"import gc\ngc.collect()\ndf = downcast(df)","51931c44":"def matching_name_cat_age(df,n,all_shops):\n    temp_cols = [f'same_name{n}catage_cnt','date_block_num', f'item_name_first{n}','item_age','category_id']\n    if all_shops:\n        temp_cols[0] += '_all_shops'\n    else:\n        temp_cols += ['shop_id']\n    temp = []\n    for i in range(1,35):\n        block = (\n            df\n            .query(f'date_block_num < {i}')\n            .groupby(temp_cols[2:])\n            .agg({'item_cnt':'mean'})\n            .reset_index()\n            .rename(columns={'item_cnt':temp_cols[0]})\n        )\n        block = block[block[temp_cols[0]]>0]\n        block['date_block_num'] = i\n        temp.append(block)\n    temp = pd.concat(temp)\n    df = pd.merge(df, temp, on=temp_cols[1:], how='left')\n    return df\n\nfor n in [4,6,11]:\n    for all_shops in [True,False]:\n        df = matching_name_cat_age(df,n,all_shops)","8275f86a":"#assign appropriate datatypes\ndf = downcast(df)\nint8_cols = [\n    'item_cnt','month','group_id','shop_type',\n    'shop_city','shop_id','date_block_num','category_id',\n    'item_age',\n]\nint16_cols = [\n    'item_id','item_name_first4',\n    'item_name_first6','item_name_first11'\n]\nfor col in int8_cols:\n    df[col] = df[col].astype('int8')\nfor col in int16_cols:\n    df[col] = df[col].astype('int16')","7d88a657":"def nearby_item_data(df,col):\n    if col in ['item_cnt_unclipped_lag1','item_cnt_lag1to12']:\n        cols = ['date_block_num', 'shop_id', 'item_id']\n        temp = df[cols + [col]] \n    else:\n        cols = ['date_block_num', 'item_id']\n        temp = df.groupby(cols).agg({col:'first'}).reset_index()[cols + [col]]   \n    \n    temp.columns = cols + [f'below_{col}']\n    temp['item_id'] += 1\n    df = pd.merge(df, temp, on=cols, how='left')\n    \n    temp.columns = cols + [f'above_{col}']\n    temp['item_id'] -= 2\n    df = pd.merge(df, temp, on=cols, how='left')\n    \n    return df\n\nitem_cols = ['item_cnt_unclipped_lag1','item_cnt_lag1to12',\n             'item_cnt_all_shops_lag1','item_cnt_all_shops_lag1to12']\nfor col in item_cols:\n    df = nearby_item_data(df,col)\n    \ndel temp","57213bef":"results = Counter()\nitems['item_name'].str.split().apply(results.update)\n\nwords = []\ncnts = []\nfor key, value in results.items():\n    words.append(key)\n    cnts.append(value)\n    \ncounts = pd.DataFrame({'word':words,'count':cnts})\ncommon_words = counts.query('count>200').word.to_list()\nfor word in common_words:\n    items[f'{word}_in_name'] = items['item_name'].str.contains(word).astype('int8')\ndrop_cols = [\n    'item_id','category_id','item_name','item_name_first4',\n    'item_name_first6','item_name_first11',\n    'category_name','group_name','group_id'\n]\nitems = items.drop(columns=drop_cols)","747eb50a":"#join these word vectors to the training dataframe\ndf = df.join(items, on='item_id')","df9dd89f":"def binary_encode(df, letters, cols):\n    encoder = ce.BinaryEncoder(cols=[f'item_name_first{letters}'], return_df=True)\n    temp = encoder.fit_transform(df[f'item_name_first{letters}'])\n    df = pd.concat([df,temp], axis=1)\n    del df[f'item_name_first{letters}_0']\n    name_cols = [f'item_name_first{letters}_{x}' for x in range(1,cols)]\n    df[name_cols] = df[name_cols].astype('int8')\n    return df\n\ndf = binary_encode(df, 11, 15)\n    \ndel df['item_name_first4'], df['item_name_first6']","6bac2a72":"#save dataframe for later use\ndf.to_pickle('df_complete.pkl')","28a94eb1":"#Reset the kernel to clear memory.\n%reset -f","bb24c995":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport shap\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\npd.set_option('display.max_rows', 160)\npd.set_option('display.max_columns', 160)\npd.set_option('display.max_colwidth', 30)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ad8e2b7b":"#load the saved training dataframe\ndf = pd.read_pickle('..\/input\/files-top-scoring-notebook-output-exploration\/df_complete.pkl')\n\nX_train = df[~df.date_block_num.isin([0,1,33,34])]\ny_train = X_train['item_cnt']\ndel X_train['item_cnt']\n\nX_val = df[df['date_block_num']==33]\ny_val = X_val['item_cnt']\ndel X_val['item_cnt']\n\nX_test = df[df['date_block_num']==34].drop(columns='item_cnt')\nX_test = X_test.reset_index()\ndel X_test['index']\n\n#free memory\ndel df","9b1a4482":"def build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_val = lgb.Dataset(X_val, y_val)\n    model = lgb.train(params=params, train_set=lgb_train, valid_sets=(lgb_train, lgb_val), verbose_eval=50,\n                     categorical_feature=cat_features)\n    return model","1a4a05a9":"#skip this cell if directly loading saved model \nparams = {\n    'objective': 'rmse',\n    'metric': 'rmse',\n    'num_leaves': 1023,\n    'min_data_in_leaf':10,\n    'feature_fraction':0.7,\n    'learning_rate': 0.01,\n    'num_rounds': 1000,\n    'early_stopping_rounds': 30,\n    'seed': 1\n}\n#designating the categorical features which should be focused on\ncat_features = ['category_id','month','shop_id','shop_city']\n\nlgb_model = build_lgb_model(params, X_train, X_val, y_train, y_val, cat_features)\n\n#save model for later use\nlgb_model.save_model('initial_lgb_model.txt')","9bc556d6":"submission = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nsubmission['item_cnt_month'] = lgb_model.predict(X_test).clip(0,20)\nsubmission[['ID', 'item_cnt_month']].to_csv('initial_lgb_submission.csv', index=False)","4a357947":"Currently, lag1to12 values are the sum of values over the previous 12 months. Differing ages in the dataset mean that some lag1to12 values are calculated over the previous 12 months, but others have had less time to accrue.\n\nWe divide lag1to12 values by the minimum between 12 and previous periods in the dataset. This turns lag1to12 into a monthly average that can be more accurately compared between datapoints.","cb472142":"# Introduction\nThis notebook constructs a prediction model for the Predict Future Sales competition that is the final project for the Coursera course \"How to Win a Data Science Competition\". The task is to predict monthly sales for various items in different retail outlets of the Russian company 1C.","48246564":"These features show the mean sales of items with names that have the same first n characters, in the same category and at the same item age. The hope is that this feature can catch past performance of similar items such as earlier titles in a series, particularly in their debut month.","da6079a4":"**Ages & Aggregating Sales\/Price information**\n\nWe create a feature showing how many days have passed between the first time an item was sold and the beginning of the current month.","1b53617d":"We now create features showing the mean first month sales for items in each category","1b6d700b":"The target variable, 'item_cnt', is the monthly sale count of individual items at individual shops. We now create features showing average monthly sales based on various groupings","3b171f11":"**Adding Shop Information**\n\nWe cluster the shops using K-means clustering, using the proportion of their sales they make in each category as features.\n\nk=7 was selected because:\n\nk=7 resulted in the highest average silhouette score aside from a choice of k=2.\nk=2 would not provide a useful clustering because it creates a feature with value (shop_id==55)*1.\nk=7 is also in an appropriate area when using the elbow method","b6ba5e41":"No columns with float dtype require more than float32 precision and no int dtype columns require values outside the int16 range. The following function will compress the data types of these columns.","4a7024a1":"**Preparing Sales Information**\n\nWe load sales.csv, remove the small proportion of rows without outlying values, use the dictionary we created above to reassign item id's where appropriate, then filter out sales for shops that don't exist in the test set and create features that need to be made before the data is grouped by month.","469f29c5":"We calculate the proportion of weekly sales that occurred on each weekday at each shop. Using this information we can assign a measure of weeks of sales power to each month. February always has 4 exactly weeks worth of days since there are no leap years in our time range and all other months have a value >4 since they have extra days of varying sales power.\n\nMonth, year and first day of the month features are also created.","6154ea8b":"**Encoding Name Information**\n\nWe add boolean features indicating whether an item's name contains any words which frequently appear in the item set.","1bdd8c7f":"These features each have 3 lagged columns returned:\n\n* lag1 shows the value of the prior month\n* lag2 shows the value two months prior\n* lag1to12 is the sum of values across the previous 12 months\n\nOriginal columns will be deleted when they are no longer needed to avoid data leakage.","601664f3":"**Constructing Training Dataframe**\n\nThe test set consists of the cartesian product of 42 shops and 5100 items. To make a training set which approximates the test set we create a training dataframe consisting of the cartesian product (active items) x (active shops) for each month.","a9164167":"**Lagging Values & Features that use Prior Information**\n\nThe following function will be used to create lag features of varying lag periods.","e05ae514":"These features use past information across more than one period of the dataset. They are not suitable to be lagged in the normal manner.","d505e8ff":"We want to clip the target value before aggregating so that mean values are not distorted due to outliers. We retain the unclipped value for use in features that do not aggregate the sales data."}}