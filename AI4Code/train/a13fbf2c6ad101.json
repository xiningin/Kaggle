{"cell_type":{"81e0cad9":"code","d4b20e89":"code","1ef22020":"code","04884572":"code","8d938b1a":"code","84c59001":"code","e2bcac9d":"code","5f201665":"code","ff382549":"code","64306a1b":"code","ca868646":"code","bf8e35eb":"code","9c051b63":"code","abb76fe5":"code","ed12825a":"code","715f0e8a":"code","03388834":"code","3dfde25b":"code","b04b8edc":"code","59f79b50":"code","bedaba95":"code","469e46db":"code","0f826d0d":"code","daac1e55":"code","4ed1c037":"code","96e2d268":"code","1d7f294d":"code","95509a97":"code","f6bc64b0":"code","57268b66":"code","c6f4729a":"code","cc49acaf":"code","cdc92d33":"code","cef28b48":"code","c0e379ee":"code","a8e56d81":"code","57d4dbf0":"code","e84ba005":"code","a61c57ed":"code","095ffb39":"code","7f591111":"code","1884d6d7":"code","7aee6522":"code","ec76002f":"code","9a174041":"code","d7a51062":"code","673bf171":"code","9c4e9fea":"code","ef154018":"code","ca07ab30":"code","93e1f5c2":"code","5e5eeec0":"code","92133109":"markdown","9c2106ad":"markdown","6d345905":"markdown","7ca08008":"markdown","d610eec9":"markdown","693367ac":"markdown","05e93aab":"markdown","e7a61450":"markdown","c9ddd074":"markdown","73a81c2c":"markdown","554e2b61":"markdown"},"source":{"81e0cad9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4b20e89":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport datetime\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport re\nimport json\nfrom tqdm.autonotebook import tqdm\nimport string\nimport collections\nfrom textblob import TextBlob\n\nimport spacy\n\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom keras.preprocessing import sequence, text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import utils\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers import Dense, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.vis_utils import plot_model\n\nimport warnings\nwarnings.filterwarnings('ignore')","1ef22020":"#define stopwords\nfrom nltk.corpus import stopwords\n\nstopwords_list = stopwords.words('english') + list(string.punctuation)\nstopwords_list += [\"''\", '\"\"', '...', '``']","04884572":"def clean_text(txt):\n     return re.sub('[^A-Za-z0-9.]+', ' ', str(txt).lower())","8d938b1a":"#https:\/\/towardsdatascience.com\/text-analysis-feature-engineering-with-nlp-502d6ea9225d\n\ndef text_cleaning(text, flg_stemm = False, flg_lemm = True, lst_stopwords = None):\n    '''\n    Converts all text to lower case, tokenize, remove multiple spaces, stopwords, stemming, lemmatize, \n    then convert all back to string\n    \n    text: string - name of column containing text\n    lst_stopwords: list - list of stopwords to remove\n    flg_stemm: bool - whether stemming is to be applied\n    flg_lemm: bool - whether lemmitisation is to be applied\n    '''\n    \n    #clean (convert to lowercase and remove punctuations and characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    #tokenize (convert from string to list)\n    lst_text = text.split()\n    \n    #remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    stopwords_list]\n                \n    #stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    #lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    #back to string from list\n    text = \" \".join(lst_text)\n    return text","84c59001":"MAX_LENGTH = 64\nOVERLAP = 20\n    \ndef shorten_sentences(sentences):\n    \"\"\"\n    Sentences that have more than MAX_LENGTH words will be split\n    into multiple sentences with overlappings.\n    \"\"\"\n    short_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        if len(words) > MAX_LENGTH:\n            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n        else:\n            short_sentences.append(sentence)\n    return short_sentences","e2bcac9d":"#define paths\nos.listdir('\/kaggle\/input\/coleridgeinitiative-show-us-the-data\/')\ntrain_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","5f201665":"#create a function to get the text from the JSON file and append it to the new column in table\ndef read_json_pub(filename, train_path = train_path, output = 'text'):\n    json_path = os.path.join(train_path, (filename + '.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data","ff382549":"#read \ntrain = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\n\n#review\ntrain.head()","64306a1b":"#https:\/\/www.kaggle.com\/jagdmir\/spacy-ner-model\n\nimport nltk\n\nDATA = []\nlabel_count = 0\n\nfor idx,row in tqdm(train.iterrows()):\n    pub = \"..\/input\/coleridgeinitiative-show-us-the-data\/train\/\" + row.Id + \".json\"            \n    f = open(pub)  \n    data = json.load(f)      \n    \n    balanced = False\n\n    sentences = [clean_text(sentence) for sentence in nltk.sent_tokenize(str(data))]\n    sentences = shorten_sentences(sentences) # make sentences short\n    sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars\n    \n    for sentence in sentences:          \n      \n        a = re.search(row.cleaned_label.lower(), sentence)\n        b = re.search(row.dataset_label.lower(), sentence)\n        c = re.search(row.dataset_title.lower(), sentence)\n        cleaned_label = row.cleaned_label.lower()\n        dataset_label = row.dataset_label.lower()\n        dataset_title = row.dataset_title.lower()\n        \n        if  a != None:\n            DATA.append((sentence, cleaned_label))\n            label_count = label_count + 1\n            balanced = True\n        elif b != None:\n            DATA.append((sentence, dataset_label))\n            label_count = label_count + 1\n            balanced = True\n        elif c != None:\n            DATA.append((sentence, dataset_title))\n            label_count = label_count + 1\n            balanced = True  \n            \n        else:\n            pass\n                \nprint(\"Text with dataset:\", label_count)","ca868646":"#get dataframe\ntrain_df = pd.DataFrame(DATA)\ntrain_df = train_df.rename({0: 'Sentence', 1: 'Label'}, axis = 1)\n\n#review\ntrain_df.tail(10)","bf8e35eb":"print('Sentence:', train_df['Sentence'][555])\nprint('\\n')\nprint('Label:', train_df['Label'][555])","9c051b63":"print('Sentence:', train_df['Sentence'][50000])\nprint('\\n')\nprint('Label:', train_df['Label'][50000])","abb76fe5":"X = train_df['Sentence'].to_numpy()\ny = train_df['Label'].to_numpy()\n\n#split traing data into training a validation sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)","ed12825a":"#check shape\nprint('Train sentences:', X_train.shape, '\\n', \n      'Test sentences:', X_test.shape, '\\n', \n      'Train labels:', y_train.shape, '\\n', \n      'Test labels:', y_test.shape)","715f0e8a":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n#limit on the number of features. We use the top 20K features\ntop_k = 20000\n\n#limit on the length of text sequences. Sequences longer than this will be truncated\nmax_sequence_length = 500\n\n#get max sequence length\nmax_length = len(max(X_train, key = len))\nif max_length > max_sequence_length:\n    max_length = max_sequence_length\n    \nmax_vocab_length = 20000 # max number of words to have in our vocabulary\n\n#method to count the unique words in vocabulary and assign each of those words to indices\ntokenizer = Tokenizer(num_words = top_k)\n\n#create vocabulary with training texts\ntokenizer.fit_on_texts(list(X_train))\n\n#convert text into integer sequences\nX_train_seq = tokenizer.texts_to_sequences(X_train)\nX_test_seq = tokenizer.texts_to_sequences(X_test)\n         \n#fix sequence length to max value. \n#sequences shorter than the length are padded in the beginning and sequences longer are truncated at the beginning\n#this turns our lists of integers into a 2D integer tensor of shape (samples, maxlen)\nX_train_pad  = pad_sequences(X_train_seq, maxlen = max_length)\nX_test_pad = pad_sequences(X_test_seq, maxlen = max_length)","03388834":"#number of unique words in the training data\nsize_of_vocabulary = len(tokenizer.word_index) + 1 #+1 for padding\nprint(size_of_vocabulary)","3dfde25b":"word_index = tokenizer.word_index","b04b8edc":"from sklearn import preprocessing\n\n#use the LabelEncoder to convert text labels to integers, 0, 1, 2, etc.\nencoder = preprocessing.LabelEncoder()\n\n#since we have two different data set (X_train and X_test), \n#we need to fit it on all of our data otherwise there might be some categories in the test set X_test that were not in the train set X_train \n#and we will get errors\nencoder.fit(list(y_train) + list(y_test)) \ny_train = encoder.transform(y_train)\ny_test = encoder.transform(y_test)","59f79b50":"print('X_train shape:', X_train.shape)\nprint('X_test shape:', X_test.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","bedaba95":"encoder.classes_","469e46db":"num_classes = train_df['Label'].nunique() + 1\nnum_classes","0f826d0d":"from keras import utils\n\n#binarize the labels for the neural net\ny_train = utils.to_categorical(y_train, num_classes)\ny_test = utils.to_categorical(y_test, num_classes)","daac1e55":"print('X_train shape:', X_train_pad.shape)\nprint('X_test shape:', X_test_pad.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","4ed1c037":"#load the whole embedding into memory\nembeddings_index = {}\nf = open('..\/input\/glove840b300dtxt\/glove.840B.300d.txt')\n\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    try:\n        coefs = np.asarray(values[1:], dtype = 'float32')\n        embeddings_index[word] = coefs\n    except ValueError: #catch the exception where there are strings in the GloVe text file, can be avoided if use glove.42B.300d.txt\n        pass\n    \nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","96e2d268":"#create an embedding matrix for the words we have in the dataset\nembedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in tqdm(word_index.items()):\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","1d7f294d":"#hyperparameters\nfilters = 4\nkernel_size = 3\ndropout_rate = 0.5\npool_size = 3","95509a97":"#simple bidirectional LSTM with GloVe embeddings and Dense layers\nsepcnn_model = Sequential()\n\n#embedding layer\nsepcnn_model.add(Embedding(size_of_vocabulary, \n                           300,\n                           weights = [embedding_matrix], #load GloVe\n                           input_length = X_train_pad.shape[0],\n                           trainable = False)) #keep frozen\n\nsepcnn_model.add(Dropout(rate = dropout_rate))\nsepcnn_model.add(SeparableConv1D(filters = filters,\n                                  kernel_size = kernel_size,\n                                  activation = 'relu'))\nsepcnn_model.add(SeparableConv1D(filters = filters,\n                                  kernel_size = kernel_size,\n                                  activation = 'relu'))\n    \nsepcnn_model.add(MaxPooling1D(pool_size = pool_size))\nsepcnn_model.add(SeparableConv1D(filters = filters * 2,\n                              kernel_size = kernel_size,\n                              activation = 'relu'))\n\nsepcnn_model.add(SeparableConv1D(filters = filters * 2,\n                              kernel_size = kernel_size,\n                              activation = 'relu'))\n                 \nsepcnn_model.add(GlobalAveragePooling1D())\n\nsepcnn_model.add(Dropout(rate = dropout_rate))\n                          \n#output layer\nsepcnn_model.add(Dense(num_classes, activation = 'softmax')) ","f6bc64b0":"#summary\nsepcnn_model.summary()","57268b66":"#plot\nplot_model(sepcnn_model, to_file = 'sepcnn_model_plot.png', show_shapes = True, show_layer_names = True)","c6f4729a":"#compile\nsepcnn_model.compile(optimizer = keras.optimizers.Adam(0.0001), #low learning rate is good, but the model will take more iterations to converge\n                    loss = 'categorical_crossentropy',\n                    metrics = ['acc'])","cc49acaf":"#from sklearn.utils import class_weight\n\n#correct class imbalance\n#class_weights = list(class_weight.compute_class_weight('balanced',\n#                                                       np.unique(train_df['Label']),\n#                                                       train_df['Label']))\n\n#weights = {}\n#for index, weight in enumerate(class_weights) : weights[index] = weight","cdc92d33":"print('X_train shape:', X_train_pad.shape)\nprint('X_test shape:', X_test_pad.shape)\nprint('y_train shape:', y_train.shape)\nprint('y_test shape:', y_test.shape)","cef28b48":"#add callbacks\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n#define the callbacks\nearly_stopping = [EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 1),\n                 ModelCheckpoint(filepath = 'sepcnn_model.h5', monitor = 'val_loss', save_best_only = True)]","c0e379ee":"start = datetime.datetime.now()\nbatch_size = 256\n\n#fit\nsepcnn_history = sepcnn_model.fit(np.array(X_train_pad), np.array(y_train),\n                                  #class_weight = weights,\n                                  batch_size = batch_size,\n                                  epochs = 20,\n                                  validation_data = (np.array(X_test_pad), np.array(y_test)),\n                                  steps_per_epoch = X_train_pad.shape[0] \/\/ 256,\n                                  validation_steps = X_test_pad.shape[0] \/\/ 256,\n                                  callbacks = early_stopping)","a8e56d81":"end = datetime.datetime.now()\nelapsed = end - start\nprint('Training took a total of {}'.format(elapsed))","57d4dbf0":"#save model\nsepcnn_model.save('sepcnn_model.h5')","e84ba005":"fig , ax = plt.subplots(1,2)\nfig.set_size_inches(20, 8)\n\nsepcnn_train_acc = sepcnn_history.history['acc']\nsepcnn_train_loss = sepcnn_history.history['loss']\nsepcnn_val_acc = sepcnn_history.history['val_acc']\nsepcnn_val_loss = sepcnn_history.history['val_loss']\n\nepochs = range(1, len(sepcnn_train_acc) + 1)\n\nax[0].plot(epochs , sepcnn_train_acc , 'g-o' , label = 'Training Accuracy')\nax[0].plot(epochs , sepcnn_val_acc , 'y-o' , label = 'Validation Accuracy')\nax[0].set_title('SEPCNN Model Train & Validation Accuracy')\nax[0].legend(loc = 'lower right')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Accuracy')\n\nax[1].plot(epochs, sepcnn_train_loss , 'g-o' , label = 'Training Loss')\nax[1].plot(epochs, sepcnn_val_loss , 'y-o' , label = 'Validation Loss')\nax[1].set_title('SEPCNN Model Train & Validation Loss')\nax[1].legend()\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Accuracy')\n\nplt.show()","a61c57ed":"#save\nplt.savefig('sepcnn_acc_loss.png')","095ffb39":"print('Train loss & accuracy:', sepcnn_model.evaluate(X_train_pad, y_train))\nprint('\\n')\nprint('Test loss & accuracy:', sepcnn_model.evaluate(X_test_pad, y_test))","7f591111":"#make prediction\nsepcnn_yhat_test = sepcnn_model.predict(X_test_pad)\n\n#to evaluate accuracy we need a vector of labels\nsepcnn_yhat_test = np.argmax(sepcnn_yhat_test, axis = 1)\nsepcnn_y_test = np.argmax(y_test, axis = 1)\n\n#get classification report\nprint('Model: SEPCNN', '\\n', classification_report(sepcnn_y_test, sepcnn_yhat_test))","1884d6d7":"preds = encoder.inverse_transform([np.argmax(sepcnn_yhat_test)]) \npreds","7aee6522":"#summary table\nsummary_table = pd.DataFrame({'Model': [],\n                              'Accuracy': [],\n                              'Precision': [], 'Recall': [], 'F1': []})","ec76002f":"#update summary table\nsummary_table.loc[0] = ['DL SEPCNN',\n                        round(accuracy_score(sepcnn_y_test, sepcnn_yhat_test), 2),\n                        round(precision_score(sepcnn_y_test, sepcnn_yhat_test, average = 'macro'), 2), \n                        round(recall_score(sepcnn_y_test, sepcnn_yhat_test, average = 'macro'), 2), \n                        round(f1_score(sepcnn_y_test, sepcnn_yhat_test, average = 'macro'), 2)]\nsummary_table.head()","9a174041":"summary_table.to_csv('sepcnn_summary_table.csv')","d7a51062":"#get text\ntqdm.pandas()\ntrain['text'] = train['Id'].progress_apply(read_json_pub)\n\n#clean text\ntrain['text'] = train['text'].progress_apply(clean_text)","673bf171":"from functools import partial\n\n#read data\nsample_submission = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\n\n#apply the function to submission data\ntqdm.pandas()\nsample_submission['text'] = sample_submission['Id'].progress_apply(partial(read_json_pub, train_path = test_path))\n\n#review\nsample_submission.head()","9c4e9fea":"temp_1 = [x.lower() for x in train['dataset_label'].unique()]\ntemp_2 = [x.lower() for x in train['dataset_title'].unique()]\ntemp_3 = [x.lower() for x in train['cleaned_label'].unique()]\n\nexisting_labels = set(temp_1 + temp_2 + temp_3)","ef154018":"literal_matching = True\nsepcnn_prediction = True\n\nid_list = []\nlabels_list = []\n\nfor index, row in tqdm(sample_submission.iterrows()):\n\n    sample_text = row['text']\n\n    row_id = row['Id']\n    \n    #check if the sample text is equal to one of the train samples and if so, use those labels\n    temp_df = train[train['text'] == clean_text(sample_text)]\n    cleaned_labels = temp_df['cleaned_label'].to_list()\n    \n    #literal_matching for known label in sample text\n    if literal_matching:\n        for known_label in existing_labels:\n            if known_label in sample_text.lower():    \n                cleaned_labels.append(clean_text(known_label))\n            \n        print('cleaned label:', set(cleaned_labels))   \n    \n    #lstm_prediction \n    if sepcnn_prediction:\n        \n        #extract sentences\n        sentences = [clean_text(sentence) for sentence in nltk.sent_tokenize(str(sample_text))]\n        sentences = shorten_sentences(sentences) # make sentences short\n        sentences = [sentence for sentence in sentences if len(sentence) > 10] # only accept sentences with length > 10 chars        \n        \n        tokenizer.fit_on_texts(list(sentence))\n        sentence_seq = tokenizer.texts_to_sequences([sentence])\n        sentence_pad  = pad_sequences(sentence_seq, maxlen = max_length)\n            \n        #predict\n        sepcnn_labels = sepcnn_model.predict(sentence_pad)\n    \n        #get label\n        sepcnn_labels = encoder.inverse_transform([np.argmax(sepcnn_labels)])\n            \n        print('sepcnn label:', set(sepcnn_labels))\n        sepcnn_labels = set(sepcnn_labels)\n        \n    cleaned_labels += sepcnn_labels\n    print('updated cleaned label:', set(cleaned_labels))\n        \n    cleaned_labels = set(cleaned_labels)\n    cleaned_labels = [clean_text(x) for x in cleaned_labels]    \n    labels_list.append('|'.join(cleaned_labels))\n    print('label list:', labels_list)   \n    id_list.append(row_id)\n    print('\\n')","ca07ab30":"#get dataframe\nsample_submission['PredictionString'] = labels_list\nsample_submission.drop(columns = 'text', axis = 1, inplace = True)\nsample_submission","93e1f5c2":"print(sample_submission['PredictionString'][0])\nprint('\\n')\nprint(sample_submission['PredictionString'][1])\nprint('\\n')\nprint(sample_submission['PredictionString'][2])\nprint('\\n')\nprint(sample_submission['PredictionString'][3])","5e5eeec0":"#save\nsample_submission.to_csv('submission.csv', index = False)\n\n#check\nsubmission = pd.read_csv('submission.csv')\nsubmission","92133109":"# Create Sentences & Labels","9c2106ad":"# MODELING","6d345905":"### Encode Label","7ca08008":"# PREPROCESSING","d610eec9":"# Libraries","693367ac":"### Binarize Label","05e93aab":"### Load Pretrained Word Vector","e7a61450":"### Train-Test-Split","c9ddd074":"# OBTAIN","73a81c2c":"# PREDICTION","554e2b61":"### Tokenize"}}