{"cell_type":{"24c1a9f2":"code","97871938":"code","0d14a1ed":"code","3cbea8b5":"code","2a9a0fa1":"code","ecfb9508":"code","2d9b5597":"code","f7dea143":"code","cf1957d5":"code","55ccf8b6":"code","2db59b6c":"code","504f4bfc":"code","6e008bbb":"code","c08f6429":"code","f0c329fa":"code","43eb652a":"code","dbf0e556":"code","d8443834":"code","52123a97":"code","796215dd":"code","53730b82":"code","c1e4a940":"code","fe4abdc8":"code","287a3b0f":"code","a6d98875":"code","e13a64fa":"code","6a869a78":"code","dddab75c":"code","ab663b0a":"code","e3d85f67":"code","8b5ccf62":"code","c89634b5":"code","4f1558f6":"markdown","df81c86b":"markdown","dc5f3bc6":"markdown","77bfab98":"markdown","8d685356":"markdown","9612bedc":"markdown","53224024":"markdown","80db00f3":"markdown","3a2d2e42":"markdown","8273ef00":"markdown","d19b5446":"markdown","639276b9":"markdown","ccd6ec1a":"markdown","9fae1764":"markdown","de1f3094":"markdown","01d3f142":"markdown","2094e8f2":"markdown","4bc31122":"markdown","2fea5f24":"markdown","6ef915aa":"markdown","152d02e7":"markdown","b8fbe4a5":"markdown","d0f45f9c":"markdown","c9fa4ae0":"markdown","ad4e6b41":"markdown","8a3f3ef1":"markdown","e4ab6326":"markdown"},"source":{"24c1a9f2":"!python -m pip install \"..\/input\/ipyplot-package\/ipyplot-1.1.0-py3-none-any.whl\" --quiet\n!pip install nicaviz","97871938":"\n\n\nimport os\nimport glob\nimport torch\nimport ipyplot\nimport nicaviz\nimport numpy as np\nimport pandas as pd\nimport random as rn\nfrom PIL import Image\nfrom glob import glob\nimport tensorflow as tf\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\n# Seed for reproducability\nseed = 1234\nrn.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)","0d14a1ed":"# Path variables\nBASE_PATH = \"..\/input\/petfinder-pawpularity-score\/\"\nTRAIN_PATH = BASE_PATH + \"train.csv\"\nTEST_PATH = BASE_PATH + \"test.csv\"\nTRAIN_IMAGES = glob(BASE_PATH + \"train\/*.jpg\")\nTEST_IMAGES = glob(BASE_PATH + \"test\/*.jpg\")\n\n# We are trying to predict this \"Pawpularity\" variable\nTARGET = \"Pawpularity\"","3cbea8b5":"#Load the CSV Files\ntrain_df = pd.read_csv(TRAIN_PATH)\ntest_df = pd.read_csv(TEST_PATH)\n\ntrain_df","2a9a0fa1":"#Add image paths to Dataframes\ntrain_path_creator = lambda x : f'..\/input\/petfinder-pawpularity-score\/train\/{x}.jpg'\ntest_path_creator = lambda x : f'..\/input\/petfinder-pawpularity-score\/test\/{x}.jpg'\n\ntrain_df['img_path'] = train_df['Id'].apply(lambda x: train_path_creator(x))\ntest_df['img_path'] = test_df['Id'].apply(lambda x: test_path_creator(x))\n\n\n\n#Adding Image Height & Width\ndef create_shape_feature(df):\n    width_height_list = []\n    file_size_list = []\n    for path_ in tqdm(df['img_path']):\n        width_height_list.append(Image.open(path_).size)\n        file_size_list.append(os.path.getsize(path_))\n    df['width_height'] = width_height_list\n    df['file_size'] = file_size_list\n    df['width'] = df['width_height'].apply(lambda x: x[0])\n    df['height'] = df['width_height'].apply(lambda x: x[1])\n    return df\n\ntrain_df = create_shape_feature(train_df)\ntest_df = create_shape_feature(test_df)","ecfb9508":"train_df.head()","2d9b5597":"#Dataset Summary\n\nprint(f\"There are {len(TRAIN_IMAGES)} train images provided.\")\nprint(f\"train.csv file has {len(train_df)} rows.\")\n\nprint(f\"There are {len(TEST_IMAGES)} test images provided.\")\nprint(f\"test.csv file has {len(test_df)} rows.\")\n","f7dea143":"# All relevant tabular futures\n\nnon_feature_col = ['Id','width_height','width','height','file_size', 'img_path', 'Pawpularity']\nFEATURES = [col for col in train_df.columns if col not in non_feature_col]\nFEATURES","cf1957d5":"from sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    \"\"\"Numpy RMSE\"\"\"\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndef rmse_pytorch(outputs, labels):\n    \"Pytorch RMSE loss\"\n    return torch.sqrt(torch.mean((outputs - labels)**2))\n\ndef rmse_tf(y_true, y_pred):\n    \"\"\"Tensorflow RMSE loss\"\"\"\n    return tf.sqrt(tf.reduce_mean(tf.squared_difference(y_true, y_pred)))","55ccf8b6":"import seaborn as sns\n\n_, ax = plt.subplots(1,1, figsize=(15, 8))\nsns.histplot(data=train_df, x=\"Pawpularity\", color=\"blue\", kde=True, ax=ax)\nplt.show()","2db59b6c":"# FEATURES\n\n# features = train_df.columns[1:-1].tolist()\nnum_cols = 2\nnum_rows = len(FEATURES) \/\/ num_cols\n\n\nfig, axs = plt.subplots(num_rows,\n                        num_cols,\n                        figsize=(20, 15),\n                        sharex=False,\n                        sharey=True\n                       )\n\nfor i, feature in enumerate(FEATURES):\n    _ = sns.histplot(data=train_df,\n                 x=\"Pawpularity\",\n                 kde=False,\n                 ax=axs[i \/\/ num_cols, i % num_cols],\n                 hue= feature,\n                )\nplt.show()","504f4bfc":"image_paths = []\nlabels = []\ncustom_texts = []\n\nfor col in FEATURES:\n    tmp_df = train_df[train_df[col] == 1]\n    for i in range(4):\n        image_paths.append(tmp_df.iloc[i, :]['img_path'])\n        labels.append(col)\n        target = str(tmp_df.iloc[i, :][TARGET])\n        meta = tmp_df.iloc[i, :][FEATURES + ['width', 'height']].values\n        meta = ''.join([f'{col}:{m}, ' for m, col in zip(meta, FEATURES + ['width', 'height'])])\n        custom_texts.append(f'target: {target}\\n{meta}')\n    tmp_df = train_df[train_df[col] == 0]\n    \n#Visualizing feature-wise image\nipyplot.plot_class_tabs(image_paths, labels, custom_texts=custom_texts, force_b64=True, img_width=450)","6e008bbb":"\ntrain_df.nica.mass_plot(\n    plt_set = FEATURES,\n    columns = 3,\n    plottype = \"countplot\")","c08f6429":"#Plot the correlation Heatmap\n\ndata_corr = train_df[FEATURES + [TARGET]].corr()\nplt.figure(figsize = (15,15))\ndataplot = sns.heatmap(data_corr, annot=True)\n  \nplt.show()","f0c329fa":"train_df.nica.pivot_plots( FEATURES, 'Pawpularity', np.mean, palette=[\"Reds\"])","43eb652a":"import cv2\n\ndef plot_pictures(target_df):\n    plt.figure(figsize=(20, 50))\n    \n    n_rows = min(60, target_df.shape[0])\n    \n    for i in range(n_rows):\n        row = target_df.iloc[i]\n        img_path = f\"..\/input\/petfinder-pawpularity-score\/train\/{row['Id']}.jpg\"\n        Pawpularity = row[\"Pawpularity\"]\n        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        plt.subplot(12, 5, i+1)\n        plt.title(f\"Pawpularity: {Pawpularity}\")\n        plt.imshow(img)\n    plt.tight_layout()\n    plt.show()\n    plt.close()","dbf0e556":"target_df = train_df[train_df[\"Pawpularity\"] <= 3]\nplot_pictures(target_df.head(10))","d8443834":"target_df = train_df[(45 < train_df[\"Pawpularity\"]) & (train_df[\"Pawpularity\"] <= 55)]\nplot_pictures(target_df.head(10))","52123a97":"target_df = train_df[train_df[\"Pawpularity\"] >= 98]\nplot_pictures(target_df.head(10))","796215dd":"#Least Popular image\nleast_pawpular = train_df[train_df[TARGET] == train_df[TARGET].min()].iloc[0]\npath = f\"{BASE_PATH}train\/{least_pawpular['Id']}.jpg\"\nim = plt.imread(path)\nplt.figure(figsize=(15, 6))\nplt.imshow(im)\nplt.title(path.split(\"\/\")[-1])\nplt.xticks([]), plt.yticks([])\nprint(f\"Accompanying features:\")\ntrain_df[train_df['Id']==path.split('\/')[-1].split('.')[0]]","53730b82":"#Random Test Image\npath = np.random.choice(TEST_IMAGES)\nim = plt.imread(path)\nplt.figure(figsize=(15, 6))\nplt.imshow(im)\nplt.title(path.split(\"\/\")[-1])\nplt.xticks([]), plt.yticks([])\nprint(f\"Accompanying features:\")\ntest_df[test_df['Id']==path.split('\/')[-1].split('.')[0]]","c1e4a940":"ipyplot.plot_images(test_df['img_path'].values, force_b64=True, img_width=200)","fe4abdc8":"train_df['mean_pred'] = train_df[TARGET].mean()","287a3b0f":"print(f\"Mean prediction is: {train_df['mean_pred'].iloc[0].round(2)}\")\nprint(f\"Using this naive baseline train RMSE is: {rmse(train_df[TARGET], train_df['mean_pred']).round(2)}\")","a6d98875":"from sklearn.tree import DecisionTreeRegressor, export_text, plot_tree\n\nX_train, X_test, y_train, y_test = train_test_split(train_df[FEATURES], train_df[TARGET], test_size=0.2, random_state=seed)\nreg = DecisionTreeRegressor(random_state=seed, max_depth=3)\nreg.fit(X_train, y_train)","e13a64fa":"print(f\"Train RMSE: {rmse(y_train, reg.predict(X_train)).round(4)}\")\nprint(f\"Test RMSE: {rmse(y_test, reg.predict(X_test)).round(4)}\")","6a869a78":"# Create PNG file\ntext_representation = export_text(reg)\nwith open(\"tree.log\", \"w\") as f:\n    f.write(text_representation)\n\nfig = plt.figure(figsize=(25, 10))\n_ = plot_tree(reg, \n              feature_names=FEATURES,\n              class_names=TARGET,\n              filled=True)","dddab75c":"# Train final model on all training data\nreg.fit(train_df[FEATURES], train_df[TARGET])","ab663b0a":"test_df.head(2)","e3d85f67":"test_df[TARGET] = reg.predict(test_df[FEATURES])\nsub = test_df[['Id', TARGET]]\nsub.to_csv(\"submission.csv\", index=False)","8b5ccf62":"sub.head(2)","c89634b5":"sub[TARGET].plot(kind='hist', bins=15, title='Prediction distribution');","4f1558f6":"# EDA on Tabular data","df81c86b":"## Submission","dc5f3bc6":"However, the features have a low correlation to the target variable. Linear models trained on these features are therefore likely to perform poorly.\n\nOf all the tabular features, **\"Blur\" seems to be the most predictive for the target.**","77bfab98":"Scoring metric is Root Mean Squared Error (RMSE). \n\nFormally defined as:\n\n\n$$\\sqrt{\\Sigma_{i=1}^{n}{\\Big(\\frac{\\hat{y}_i - y_i}{n}\\Big)^2}}$$\n\nwhere $n$ denotes the number of samples, $y_i$ the ground truth value and $\\hat{y}_i$ the prediction value.","8d685356":"# Preparing Train & Test data","9612bedc":"The tree we have trained almost always predicts values close around the mean. There is an exception where Blur=1, Action=1 and Face=0.","53224024":"Our model predicts mostly around the mean of all labels.","80db00f3":"## Average Pawpular : Pawpularity 45 ~ 55","3a2d2e42":"Things we can notice quickly from the distribution of the scores:\n\n1. Majority of the photos have a pawpularity score between 20-50\n2. We have a long tail on the right-hand side with almost 300 images with a perfect score of 100\n3. We surely can't ignore the images with a perfect score during the training phase","8273ef00":"# Metrics and Loss (RMSE)","d19b5446":"![https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score](https:\/\/blog.groomit.me\/wp-content\/uploads\/2018\/02\/petfinder2.jpg)\n\nThis notebook is a quick exploration of the new [Petfinder 2021 competition](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score) and yields some insight into how important the tabular features will be in this competition!\n\nThis notebook contains code from:\n\nhttps:\/\/www.kaggle.com\/currypurin\/petfinder-eda-lgb-meta-features-and-img-size\/notebook\nhttps:\/\/www.kaggle.com\/aakashnain\/which-features-to-use-and-why\nhttps:\/\/www.kaggle.com\/nicapotato\/pawpular-eda","639276b9":"\n\n## Pawpularity Distribution\n\nThe first thing that we will check is the distribution of `pawpularity` score. We will use `sns.histplot(..).` for plotting the distribution. You can read about the API [here](https:\/\/seaborn.pydata.org\/generated\/seaborn.histplot.html)","ccd6ec1a":"# Image Analysis","9fae1764":"## Baseline (Decision Tree Regressor):","de1f3094":"The above plot is pretty interesting. A few things that we can notice from this plot:\n\n1. Although one would expect **Subject Focus** to be a very important feature for making a photo popular, in this case, it hardly contributes to a high score.\n2. **Eyes, Face, and Near (Single Pet)** are the only three features that are dominant for a popularity score of more than 50. These are the only features that contributed most to  a score of 100\n3. A group photo with other pets\/humans doesn't give a good score\n4. Collage, as expected, doesn't improve the score. The distribution of scores with\/without Collage is the same\n5. **Blur(Out of focus or noisy)** tends to decrease the score as expected. Most blurred photos scored between 20-30\n6. Any pet doing an action in a photo doesn't make the pet more attractive, hence the score isn't affected at all\n\n\n## Why are the features important?\n\nEngagement with a photo depends very much on the **aesthetics** of a photo. To give you a simple example, a not-so good looking pet (Sorry, every pet is cute! Here I am just talking in terms of the photo), would look cuter with a focus on the features of the pet rather than a good looking pet doing some weird trick far away from the camera. \n\nAnd aesthetics isn't just that. The term aesthetics itself is very broad as it depends very much on an individual perception of a photo. \"How to capture aesthetics of a photo in an ML model\" is an open research area. So, instead of just looking at raw photos and trying to predict an engagement\/pawpularity score is much harder than predicting the score for the same photo but with additional features that aren't directly captured in a simple model, especially traditional ML models ","01d3f142":"## Visualize decision tree","2094e8f2":"## Features and PawPularity\n\nWe are provided with **twelve** features, as meta-data, that we can use as additional features for training our models. Each of these features is binary, meaning they are either present or absent in the image. These features are:\n\n1. **Focus** - Pet stands out against the uncluttered background, not too close \/ far.\n2. **Eyes** - Both eyes are facing front or near-front, with at least 1 eye\/pupil decently clear.\n3. **Face** - Decently clear face, facing front or near-front.\n4. **Near** - Single pet taking up a significant portion of photo (roughly over 50% of photo width or height).\n5. **Action** - Pet in the middle of an action (e.g., jumping).\n6. **Accessory** - Accompanying physical or digital accessory\/prop (i.e. toy, digital sticker), excluding collar and leash.\n7. **Group** - More than 1 pet in the photo.\n8. **Collage** - Digitally-retouched photo (i.e. with digital photo frame, a combination of multiple photos).\n9. **Human** - Human in the photo.\n10. **Occlusion** - Specific undesirable objects blocking part of the pet (i.e. human, cage, or fence). Note that not all blocking objects are considered occlusion.\n11. **Info** - Custom-added text or labels (i.e. pet name, description).\n12. **Blur** - Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0.\n\nBefore discussing why these features are needed, let's check how the pawpularity score is affected by the presence of a certain feature. We will use the same distribution plot but with `hue` where hue would be a feature from the given features","4bc31122":"## Most Pawpular : Pawpularity > 98","2fea5f24":"Our baseline model will be a decision tree with a depth of 3. In this way we can easily visualize the tree and get insight in the most important feature rules. Unfortunately, the binary features do not seem to yield important rules for predicting Pawpularity. It seems that the image data will yield the most important features to predict Pawpularity in this competition.","6ef915aa":"# PetFinder.my - Pawpularity Contest\n### Predict the popularity of shelter pet photos\n\nIn this competition, you\u2019ll analyze raw images and metadata to predict the \u201cPawpularity\u201d of pet photos. You'll train and test your model on PetFinder.my's thousands of pet profiles. Winning versions will offer accurate recommendations that will improve animal welfare.","152d02e7":"# Feature Analysis","b8fbe4a5":"The test data images in the dataset are randomly generated images.\n\n**The actual test data comprises about 6800 pet photos similar to the training set photos.**","d0f45f9c":"### Least Pawpular example (1)","c9fa4ae0":"## Least Pawpular : Pawpularity < 10","ad4e6b41":"# Naive Baseline (Mean of target): ","8a3f3ef1":"That's it! I hope this notebook helped you to get started for the Petfinder 2021 competition!\n\nIf you have any questions or feedback, feel free to comment below. You can also contact me on Twitter [@carlolepelaars](https:\/\/twitter.com\/carlolepelaars).","e4ab6326":"## Test Image Analysis"}}