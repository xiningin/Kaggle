{"cell_type":{"75f748c1":"code","275e7a89":"code","fab16e88":"code","333e46dd":"code","c6f9513e":"code","fb6ce509":"code","b0883c9f":"markdown","0112c9a6":"markdown","dab611c9":"markdown","a0cc7f22":"markdown","9580b481":"markdown","7e5f0a12":"markdown","a11467a0":"markdown"},"source":{"75f748c1":"from gensim.models import KeyedVectors\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom nltk.tokenize import TweetTokenizer\nfrom joblib import dump, load\nimport glob\nimport json\nimport csv\nimport os\nimport os.path\nimport numpy as np","275e7a89":"# Basic variables and configuration\n\n# The tokens in the training data file can be of two categories, \n# either inside a risk factor-trigger (I) or not (O)\n\nINSIDE_LABEL = \"I\"\nOUTSIDE_LABEL = \"O\"\n\n# When training the model to classify a token, \n# use features in the form of word2vec vectors representing the five preceeding \n# and the five words following the token.\n# The vectors are concatenated into one long vector\n\nWINDOW_SIZE = 5\n\n# Length of the vectors in the pre-trained word2vec model\nVECTOR_LENGTH = 200\n\n# For the current token, a vector consisting of \n# the one-hot encoding of this vector used.\n# Only tokens that occur at least twice in the training data, \n# receives a position in \n# the one-hot encoding vector.\n# Tokens occurring only once are disregarded, and the classifier has to rely the \n# word2vec representation vector for this token.\n# This is to done to limit the features that are learned from, \n# given the small amount of training data available.\n\nFREQUENCY_CUT_OFF = 2\n\n# If a word is not found in the word2vec model, it receives a \n# default vector representation, only consisting of 0:s.\n\ndefault_vector = np.array([0.0] * VECTOR_LENGTH)\n\n# If TEST_RUN is set to True, the code is run with only a small portion of the data. \n# This is only to be used for test and debugging\n\nTEST_RUN = False \n","fab16e88":"## As input parameter the function takes the name of the file\n## with the annotated data\n## The file has the following format, with one token on each line, \n## and the manual annotation separated by tab\n## \n## Risk    B-RISK\n## factors    I\n## for    I\n## more    \n## severe    O\n## disease    \n##\n## The OUTSIDE class can either be represented by O or by no annotation.\n\n## There are three different annotated classes in the file, \n## plus the OUTSIDE_LABEL class.\n## Collapse the three annotated classes into one class, the INSIDE_LABEL class\n\n## Also, make a list of all tokens that have been annotated at least twice\n## as being a inside a risk factor-trigger\n\ndef read_data(file_name):\n    words = []\n    classifications = []\n    feature_dict = {}\n    with open(file_name) as f:\n        for line in f:\n            data = OUTSIDE_LABEL\n            word = \"\"\n            if line.strip() != \"\":\n                sp = line.split(\"\\t\")\n                word = sp[0].strip()\n                if len(sp) > 1 and sp[2].strip() != \"\" \\\n                and sp[2].strip() != OUTSIDE_LABEL:\n                    data = INSIDE_LABEL\n            words.append(word)\n            classifications.append(data)\n            if data == INSIDE_LABEL:\n                if word in feature_dict:\n                    feature_dict[word] = feature_dict[word] + 1\n                else:\n                    feature_dict[word] = 1\n                \n            \n    feature_list = \\\n    [word for word, freq in feature_dict.items() \\\n     if freq >= FREQUENCY_CUT_OFF]\n    return words, classifications, feature_list\n        \n\n# Just converts OUTSIDE_LABEL to 0 and INSIDE_LABEL TO 1\ndef get_training_labels_vec(training_labels):\n    labels = []\n    for label in training_labels:\n        if label == OUTSIDE_LABEL:\n            labels.append(0)\n        else:\n            labels.append(1)\n    return labels\n\n\n\n    \n\n","333e46dd":"# If the word is in the pre-trained word2vec model, return its vector.\n# Otherwise, return the default vector\ndef get_vector_for_word(word, word2vec_model):\n    if word in word2vec_model:\n        vector = word2vec_model[word]\n    else:\n        vector = default_vector\n    return vector\n\n## For a list of tokens, return a matrix with the same number of tokens \n## as words in the input list.\n## That is, the row is the feature vector representing the token. \ndef get_vectors(words, word2vec_model, feature_list):\n    return_vectors = []\n    for i in range(0, len(words)):\n        word = words[i]\n        \n        # The word2vec vector for the current word\n        vector = get_vector_for_word(word, word2vec_model)\n        \n        # The one-hot encoding for the current word (if it is included in the list\n        # of featurs that occur at least twice, otherwise, use vector of 0:s)\n        one_hot_encoding_vector = len(feature_list)*[0.0]\n        if word in feature_list:\n            index_of_word = feature_list.index(word)\n            one_hot_encoding_vector[index_of_word] = 1.0\n        vector = np.concatenate((vector, one_hot_encoding_vector))\n        \n        # For each of the WINDOW_SIZE tokens before the current token and each of\n        # the WINDOW_SIZE tokens after the current token, \n        # concatenate the word2vec vector of the token to the feature vector\n        for j in range(1, WINDOW_SIZE+1):\n            before_index = i - j\n            after_index = i -j\n            # If the token is in the end or beginning of the paragraph, use the \n            # default vector as context vector\n            before_vector = default_vector \n            after_vector = default_vector\n            if before_index >= 0:\n                before_vector = \\\n                get_vector_for_word(words[before_index], word2vec_model)\n            if after_index >= len(words):\n                after_vector = \\\n                get_vector_for_word(words[after_index], word2vec_model)\n            vector = np.concatenate((vector, before_vector))\n            vector = np.concatenate((vector, after_vector))\n        \n        return_vectors.append(vector)\n    np_return_vectors = np.array([np.array(ti) for ti in return_vectors])\n    return np_return_vectors","c6f9513e":"# Load the word2vec model\n# The word2vec model can be found from\n# https:\/\/github.com\/RaRe-Technologies\/gensim-data\/issues\/28\nprint(\"Starting to load the word2vec model\")\n\n\nword2vec_model = {}\nWORD2VECPATH = '..\/input\/word2vecmodels\/pubmed2018_w2v_200D.bin'\nif TEST_RUN: # Only load a subset of the model when in debugging mode.\n    word2vec_model = \\\n    KeyedVectors.load_word2vec_format(WORD2VECPATH, binary=True,\\\n                                          limit=5000)\nelse:\n    word2vec_model = \\\n    KeyedVectors.load_word2vec_format(WORD2VECPATH, binary=True)\n\nclf = None\nfeature_list = None\n\n# The trained models, and feature list, are saved here:\nmodel_file_name = 'risk_mention_model.joblib'\nfeature_list_file_name = 'risk_mention_features.joblib'\n\n# To save time, when you only want to use the models and\n# not retrain them, it is also possible to manually upload the trained \n# models into the input data.\n# The code checks if these models exist among the input data, and if that\n# is the case, they are not retrained. Therefore, if you want to re-train the\n# model, and have uploaded the files in the input data, you need to \n# remove them from there\nsaved_model_path = os.path.join(\"..\/input\/models\", model_file_name)\nfeature_list_path = os.path.join(\"..\/input\/models\", feature_list_file_name)\n\n# If no model has been trained, \n# and manually added to input files, train a new model:\nif not os.path.isfile(saved_model_path) or not os.path.isfile(feature_list_path):\n    \n    ANNOTATED_DATA_PATH = \\\n    \"..\/input\/manually-annotated-risk-factor-expressions\/manually_annotated_data.txt\"\n    training_data, training_labels, feature_list = \\\n    read_data(ANNOTATED_DATA_PATH)\n    \n    print(\"Features used in the one-hot encoding vector \" + str(feature_list))\n    \n    print(\"Starts transforming the text into feature vectors\")\n    y = get_training_labels_vec(training_labels)\n    X = get_vectors(training_data, word2vec_model, feature_list)\n    \n    if TEST_RUN: # For test and debugging, only use a small part of the data\n        y = y[:10000]\n        X = X[:10000]\n        \n    print(\"Starts training the model\")\n    clf = \\\n        LogisticRegressionCV(random_state=0, \\\n                             max_iter=10000, \\\n                             cv=2, \\\n                             n_jobs=8).fit(X, y)\n\n    dump(clf, model_file_name)\n    print(\"Write created model to: \" + model_file_name)\n    \n    dump(feature_list, feature_list_file_name)\n    print(\"Write feature list to: \" + feature_list_file_name)\n\n        ","fb6ce509":"# If a previously trained model is used, load this model\nif clf == None or feature_list == None:\n    print(\"Starts loading a previously trained model\")\n    clf = load(saved_model_path)\n    feature_list = load(feature_list_path)\n    \nCONTENT_DIR = \"\/kaggle\/input\/CORD-19-research-challenge\/\"\n\n# Exclude papers from the data sets, for which text parts have been annotated\nids_to_exclude = set()\nwith open(\"..\/input\/manually-annotated-risk-factor-expressions\/id_of_annotated_files.txt\") as f:\n    for line in f:\n        ids_to_exclude.add(line.strip())\n\n# Exclude text under headings with the name of introduction or background, for instance, \n# as they are likely to refer to old studies, \n# rather than being new ones. \nheadings_to_exclude_set = set()\nwith open(\"..\/input\/headings\/headings_to_exclude.txt\") as f:\n    for line in f:\n        headings_to_exclude_set.add(line.strip())\n\n# To tokenise the texts in the Cord-19 set\ntknzr = TweetTokenizer(preserve_case=True)\n\n# A list that is updated with information as new papers are classified with\n# the model. When all papapers have been processed, the list\n# contains information on the 50 paragraphs that are\n# most likely to contain expressions for risk factors\nprediction_scores = [(0.0, \"EMPTY\", \"EMPTY\", \"EMPTY\")]*50 \n\n# Run on the four directories of Cord-19\nfor dir_name in [\"biorxiv_medrxiv\", \"comm_use_subset\",\\\n                 \"custom_license\", \"noncomm_use_subset\"]:\n    path = os.path.join(CONTENT_DIR, dir_name, dir_name, \"pdf_json\")\n    \n    # All files in the directory\n    files = glob.glob(path + \"\/*.json\")\n    \n    \n    paragraphs_read_so_far = 0\n    \n    if TEST_RUN: # Only use the first 100 files if it is a test run\n        files = files[:100]\n    \n    print(\"Found \" + str(len(files)) + \" files in \" + dir_name)\n    \n    nr_of_papers_on_covid_19 = 0\n    \n    for file in files:\n        with open(file) as f:\n            file_texts = []\n            data = json.load(f)\n            paper_id = data[\"paper_id\"].strip()\n            \n            # Exclude papers that are included in training data\n            if paper_id in ids_to_exclude: \n                continue\n            \n            # Default is that the paper is not about covid-19\n            is_paper_about_covid_19 = False\n                  \n            # Loop through all paragraphs in the article\n            for el in data[\"body_text\"]:\n                text = el[\"text\"]\n                \n                # Check if paper contains covid-19 information\n                # by checking if it contains any of four\n                # covid-19-relevant key word\n                if not is_paper_about_covid_19:\n                    for covid_trigger in \\\n                    [\"covid\", \"coronavirusdisease\",\\\n                     \"sars-cov-2\", \"2019-ncov\"]:\n                        if covid_trigger in text.lower().replace(\" \", \"\"):\n                            is_paper_about_covid_19 = True\n               \n                # Add all paragraphs that not to be excluded\n                if el[\"section\"].lower() not in headings_to_exclude_set:\n                    file_texts.append(text)\n\n            # If the article is about covid-19\n            if is_paper_about_covid_19:\n                nr_of_papers_on_covid_19 = nr_of_papers_on_covid_19 + 1\n                \n                # Add lists of tokens in eah paragraph to paragraph_token_list\n                paragraph_token_list = []\n                for text in file_texts:\n                    tokens = tknzr.tokenize(text)\n                    paragraph_token_list.append(tokens)\n               \n                # For each paragraph, transform the token-list\n                # into features, and run it through the\n                # machine learning model\n                for paragraph in paragraph_token_list:\n                    # Get feature vector for each token in the paragraph\n                    X = get_vectors(paragraph, word2vec_model, feature_list)\n                    \n                    # Get predictions\n                    predictions = clf.predict(X)\n                    # Get probabilites for predicions\n                    prediction_prob = clf.predict_proba(X)\n                    \n                    # If at least one token in a paragraph has been \n                    # classified as a risk factor-trigger\n                    if 1 in predictions: \n                        # Loop through each prediction (for each token)\n                        # in the paragraph\n                        for nr, (predicted_prob, predicted) \\\n                            in enumerate(zip(prediction_prob, predictions)):\n                            \n                            # If the token is classified as a risk factor trigger\n                            if predicted == 1: \n                                \n                                # Determine indexes for neighbouring tokens \n                                # for the current token.\n                                # (To be able to present the neighbours\n                                # to a token that are part of constructing \n                                # the feature vector.)\n                                left_window_index = nr - WINDOW_SIZE + 1\n                                right_window_index = nr + WINDOW_SIZE + 1\n                                if left_window_index < 0:\n                                    left_window_index = 0\n                                if right_window_index >= len(paragraph):\n                                    left_window_index = len(paragraph) - 1\n                                    \n                                # For the list of the 50 tokens \n                                # that are most likely to \n                                # be classified as a \n                                # risk factor-trigger:\n                                # If the probability for \n                                # the current token being\n                                # a risk factor trigger \n                                # is higher than the classification \n                                # with the lowest probability in the list:\n                                # Add the current token to the list \n                                # of the 50 most probable\n                                # risk factor triggers\n                                least_confident_prediction_among_top_N = \\\n                                prediction_scores[-1][0]\n                                prob_for_token_being_a_trigger = \\\n                                np.amax(predicted_prob)\n                                if prob_for_token_being_a_trigger > \\\n                                    least_confident_prediction_among_top_N:\n                                    prediction_scores.append(\\\n                                        (prob_for_token_being_a_trigger, \\\n                                        \" \".join(paragraph), \\\n                                         \" \".join(paragraph[left_window_index:\\\n                                                            right_window_index]), \\\n                                         paper_id))\n                                    \n                                    # Sort the top-50 list, and remove the \n                                    # least probable classification\n                                    prediction_scores.sort(reverse=True)\n                                    prediction_scores = prediction_scores[:-1]\n                                \n                    paragraphs_read_so_far = paragraphs_read_so_far + 1\n                    if paragraphs_read_so_far % 1000 == 0:\n                        print(\"Read and classified \" \\\n                              + str(paragraphs_read_so_far) + \\\n                              \" paragraphs.\")\n                        \n    print(\"Found \" + str(nr_of_papers_on_covid_19) + \\\n          \" papers on covid-19 in \"\\\n          + dir_name)\n\n# Print results to console and to \"top_papers.html\"\nwith open(\"top_papers.html\", \"w\") as f:\n    paper_id_dict = {}\n    \n    for (score, text, window_text, paper_id) in prediction_scores:\n        if paper_id not in paper_id_dict:\n            paper_id_dict[paper_id] = []\n        paper_id_dict[paper_id].append((score, text, window_text))\n        \n    for key, item_list in paper_id_dict.items():\n        for (score, text, window_text) in item_list:\n            print(\"Logistic regression model score: \" \\\n                  + str(round(score, 2)))\n            print(\"Expression that was found: \" \\\n                  + str(window_text))\n            f.write(\"<div>\" + str(round(score, 2)) \\\n                    + \"<\/div>\")\n            f.write(\"<p> \" + text.replace(window_text, \"<i>\"\\\n                    + window_text + \"<\/i>\") + \" <\/p>\")\n            \n        print(\"Paper id: \" + str(key))\n        print()\n            \n        f.write(\"<br>\")","b0883c9f":"## Two help functions for reading from the file with manually annotated data\nThe main output is two equally long lists containing: (i) The annotated tokens, (ii ) with what category the token was annotated.\n\nAnother output is a list of tokens that have been annotated as inside a risk factor at least twice.","0112c9a6":"#Manually annotating 50,000 tokens for risk factor expressions, and using them for training a logistic regression model built on word2vec features\n\nOur approach to the task *\"What do we know about COVID-19 risk factors?\"* was to train a supervised machine learning model to recognise language expressions that are used in the scientific papers in the data set to express risk factors for diseases in general. If we apply this model to papers on the topic of Covid-19, it should hopefully be able to detect paragraphs that talk about risk factors for Covid-19.\n\nFor training our supervised model, we used the machine learning and text mining libraries [scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) and [gensim](https:\/\/radimrehurek.com\/gensim\/). For selecting data to annotate and for carrying out the annotations, we used two different tools developed and maintained within the [Spr\u00e5kbanken Sam](http:\/\/www.sprakbanken.se\/eng) infrastructure. Spr\u00e5kbanken Sam is a part of a Swedish national e-infrastructure that supports NLP-based research.\n\nWe believe that our contribution might be valuable to the Covid-19 text mining community by taking an approach that is different from most other submissions in two respects: i) It focuses on learning the language that is used for expressing that something is a risk factors, which makes it less likely to be biased towards risk factors that are already known, ii) it uses a supervised machine learning approach trained on manually annotated data. We don't claim this method to be better, but rather that by approaching the problem in a manner that differs from other approaches, we are likely to have contributed with a method that is suitable to combine with many of the other submissions to the task. We this potential to combine our approach with other contributions, and thereby improve them, is likely to be our most important contribution to the community.\n\n##Selecting data for manual annotation\n\nIn order to train a supervised machine learning model, annotated data is needed. We, therefore, used the following approach for selecting data from the Cord-19 corpus to manually annotate:\n\n1. We constructed a list of the words (or sometimes bi- and tri-grams) in the *\"What do we know about COVID-19 risk factors?\"* call that we estimated would be likely trigger-words for more general text about risk factors. These included, for instance, \"risk factors\", \"factors\", \"co-infections\", \"co-morbidities\", \"high-risk\", \"pre-existing\", \"susceptibility of\". (But we did not include such words as \"pulmonary disease\", \"pregnant\", or \"smoking\", as they are too specific.)\n2. We, thereafter, expanded our list of trigger words by generating synonyms to them, using the [Gavagai living lexicon](https:\/\/lexicon.gavagai.se\/lookup\/). \n3. We then counted the occurrence of the words in the CORD-19 corpus. Words that occurred very often, e.g. \"factors\", where then specified further with bigrams. For instance, \"factors\" was removed as a trigger word and \"socio-economic factors\",  \"behavioral factors\", \"environmental factors\" and \"genetic factors\" were added.\n4. We then read some of the paragraphs in the CORD-19 corpus that contained these trigger words, in the search for new general trigger words, and found words such as \"more common among\" and \"more likely\", which we also added to our list.\n\nThis four-step approach resulted in a list of [104 words for risk factors](https:\/\/www.kaggle.com\/mariaskeppstedt\/trigger-words).\n\nWe then limited the search in the CORD-19 corpus to articles indexed in PubMed, and retrieved all paragraphs containing any of the 104 risk factor words. This resulted in that 30,000 paragraphs were found. That is, much more text than what would be possible for us to manually annotate. We, therefore, needed to further limit the number of paragraphs to select for manual annotation. \n\nWe needed a method that would filter out texts in which it was likely that these trigger-words were used for talking about risk factors for diseases.  Given our limited amount of time available for manual annotation, it also seems sensible to focus on annotating the kinds of texts that are often re-occurring in the CORD-19 corpus, rather than to try to also capture outliers. Finding re-occurring content, and creating automatic classes in this content, can be done in an unsupervised fashion, for instance by using topic modelling.\n\nWe therefore applied a graphical topic modelling tool that is called Topics2Themes on our corpus of 30,000 CORD-19 paragraphs. Topics2Themes is an [open-source](https:\/\/github.com\/mariask2\/topics2themes\/) tool that is maintained and developed within the [Spr\u00e5kbanken Sam](http:\/\/www.sprakbanken.se\/eng) infrastructure. We instructed the Topics2Themes to try to find 20 topics in the data set, but to run the topic modelling algorithm 50 times and only retain the topics that were stable enough to occur in all re-runs. This resulted in that nine stable topics were found, as shown in the middle panel in the screen shot of the tool below (the topics panel). The left-most panel shows the terms associated with each topic, and the right-most panel shows the texts that are associated with the topics. In the screen shot, the topic \"Results of studies of co-morbidities and other risk factors\" have been selected by the tool user, and the tool thereby shows the terms and texts most closely associated with this topic in the surrounding panels.\n\nFor each of the nine topics, we read the 15 most closely associated texts, and classified them according to if they contained a mention of a risk factor for a disease or not. The labelling functionality of the tool, i.e., the label in the top, right-hand side of each text was used for this classification. A green label (Me) shows that the text has been classified as describing a risk factor for a disease, whereas a yellow label (No) shows that the text has been classified as not describing a risk factor. For five of these nine automatically extracted topics, more than one of the top-ten texts associated with the topics described a risk factor for a disease. These were the five topics: \"Results of studies of co-morbidities and other risk factors\", \"Causes of respiratory tract infections in children, and whether such previous infections influence the development of asthma\", \"Mostly texts related to antibodies and immunity\", \"Risk factors for influenza, symptoms for influenza, factors influencing whether people vaccinate or not\", and \"Typical reports of common co-infections and how usual these are, and sometimes, also studies of whether they effect severity\". For these five topics, more texts associated with them were manually classified (around 70 more texts for each topic).\n\nAmong texts classified as containing a risk factor for a disease, we also included texts which stated that something was NOT a risk factor for a disease. E.g., as in the second part of the sentence \"[...] only age was associated with this outcome,  whereas the presence of comorbidities I was I not.\"\n\nThe classification of texts in Topics2Themes resulted in a total of 419 manually classified texts, of which 150 were manually classified as describing risk factors for diseases.\n\n![topics2themescord-19.jpg](attachment:topics2themescord-19.jpg)\n\n\nThere is, of course, a risk that our approach of selecting data though extending a list of key words, might have resulted in a slightly biased data set. If we have the time for it in the near future, we plan to construct a small evaluation set, with expressions for risk factors. We will then use another approach for harvesting data, in order to determine if the approach used here have been biased. We might, for instance, use keywords in article headings or MeSH codes to retrieve articles relevant for risk factors, and thereafter annotate the entire articles for risk factor-triggers. \n\n## Manual annotation of risk factor triggers\nAs described above, our approach for the task was to learn to detect the language used for expressing risk factors, i.e., expressions that we call \"risk factor triggers\". For the 150 paragraphs which we, with the Topics2Themes tool, had classified as describing risk factors for diseases, we therefore performed an additional round of annotations. These annotations consisted of marking \"risk factor triggers\", i.e. the spans of texts that were used by the article authors to indicate that something is a risk factor for a disease. \n\nFor the annotations, we used a web-based annotation tool for marking spans of text, which also has been developed within the [Spr\u00e5kbanken Sam](http:\/\/www.sprakbanken.se\/eng) infrastructure. In the screen-shot below, four annotations of risk factor triggers are shown. \n\n![annotations.png](attachment:annotations.png)\n\nWe also annotated triggers for the opposite, i.e. language triggers that indicate that something is NOT a risk factor, with another tag:\n\n![noriskannotations.png](attachment:noriskannotations.png)\n\nThis second round of annotation took a little more than three hours to carry out.\n\nIt should be noted that we did not have any medical knowledge that could guide us in performing the annotations. It would, of course, have been preferable with a medically trained annotator. The annotations are, however, provided [here](https:\/\/www.kaggle.com\/mariaskeppstedt\/manually-annotated-risk-factor-expressions#manually_annotated_data.txt), and could be further improved by an annotator with medical training.\n\n## Training a supervised machine learning model\n\nOnce data with risk factor-triggers was annotated, it could be used for training a supervised model to detect the triggers. We collapsed the two annotated categories (\"trigger words for something being a risk factor\" and \"trigger words for something NOT being a risk factor\"), into one category, i.e. the \"risk factor trigger\" category.\n\nAs there were only 150 paragraphs in the training data that contained risk factors, we believe that the best approach to detect the triggers is a type of machine learning model that works well also when the training data is scarce. We chose a *logistic regression* model, as it outputs a confidence measure that is easily interpretable. We applied the model on a token level, i.e. the task of the model was to decide whether a token was part of a risk factor-trigger or not. The logistic regressions model returns a probability score, between 0 and 1, for a token being a risk factor-trigger.\n\nAs features for the model, we used the token that is to be classified. For instance, if the word \"risk\" is often annotated as a part of a risk factor-trigger, we would like the model to learn to classify \"risk\" as a risk factor-trigger. However, also the context of the word is important. If the word \"risk\" appears in the context of \"factor\" it is much more likely to be a risk factor-trigger, than if it occurs in the context of, let's say, \"analysis\". We, therefore, want the model to also take the context into account. We do not want the context to be too precise though. Let's say that the training data contains the sentence \"Diabetes is a risk factor for ..\", then we do not want the model to learn that specifically \"diabetes\" is a likely context, but it should also be able to detect that something similar to \"diabetes\" is a likely context, i.e. \"obesity\" in \"Obesity is a risk factor for ..\". \n\nAs representations for the context words, we therefore used semantic vectors that represent the context. As \"diabetes\" and \"obesity\" are semantically similar, they will be represented by semantic vectors that are similar. We use vectors from a word2vec model pre-trained on 27 million biomedical articles. (From https:\/\/github.com\/RaRe-Technologies\/gensim-data\/issues\/28, see more below.)\nWhen constructing the features, we also use the semantic vector for the token that is to be classified.\n\nWhen applying the trained model for classifying text, we extract the highest confidence-value for each paragraph, i.e. the token that the model classifies as most likely to be a risk factor-trigger. This value is used for signalling the likelihood that a paragraph describes risk factors for diseases. \n\n(An alternative here, would be to use a structured predictor, for instance a conditional random fields model, in which each token training and classification takes into account how the surrounding tokens are classified.  However, our guess here is that it would make little difference, as we use word-vectors for the surrounding tokens as features and only take the confidence value of the most confident token into account. But it is only a guess.)\n\n## Libraries and classes used \nFor training and using a logistic regression model, the [LogisticRegressionCV class from the scikit-learn library](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegressionCV.html) was used.\n\nFor handling the pre-trained word2vec-vectors, [the Gensim library ](https:\/\/radimrehurek.com\/gensim\/auto_examples\/index.html) was used.\n\nThe feature vectors were constructed and concatenated using Numpy, and the TweetTokenizer class was used for tokenising the Cord-19 texts that were to be classified. (There is probably a more suitable tokeniser, but the TweetTokenizer is likely to work reasonably well here.) \n","dab611c9":"## Transforming text data into vectors ","a0cc7f22":"## Use the trained model to find relevant articles in the CORD-19 data set\n\nAll paragraphs in the cord-19-research-challenge that are included in a paper that contains the words \"covid\", \"coronavirus\", disease 2019\", \"sars-cov-2\" or \"2019-nCoV\" is then run through the model. \n\nPapers which have at least one paragraph included in the test data set are excluded. Paragraphs under headings with names such as \"introduction\" and \"background\" are also excluded, as these texts might be more likely to contain information on previous research.\n\nThe 50 paragraphs that get the highest score for risk factor mentions is then returned (together with their score and paper id).","9580b481":"## Basic configuration\n\nThese variables can be changed, if the model is to be run with another configuration, e.g. another window size for what neighbouring tokens to feed as features to the classifier.","7e5f0a12":"## Training the logistic regression model\n\nThe corpus of around 50,000 manually annotated tokens from the CORD-19-research-challenge was used as training data.\n\nThese annotations were transformed into features (using the help-functionality above) and used for training a logistic regression model.\n\nA pre-trained word2vec model is first loaded, to be used as features. This model can be found [here](https:\/\/github.com\/RaRe-Technologies\/gensim-data\/issues\/28).\n","a11467a0":"The 50 papers that are most likely to contain mentions of risk factors, according to the model trained, are listed in the output file \"top_papers.html\". \n\nThe expressions leading to this classification are also printed to the console.\n\n\n\n\n"}}