{"cell_type":{"6411863c":"code","32141e79":"code","facc0a1f":"code","c54032aa":"code","b1fe86be":"code","72cd6282":"code","dc0b63cd":"code","dafbde69":"code","f55f73c8":"code","89f73d07":"code","5196c3c3":"code","51b455f7":"code","4900e5c5":"code","4969dca9":"code","9761de62":"code","0fa9ab6a":"code","f27f492c":"code","7614d68f":"code","47a14d98":"code","7da5c14b":"code","39fafe00":"code","78d3a1f3":"code","6ed96901":"code","67e66f43":"code","2c078ae8":"code","f17534b5":"code","3142a50c":"code","2305cd12":"code","cbd0d080":"code","092ae1fd":"code","101637c7":"code","e9d04dea":"code","a069a7d8":"code","832767a9":"code","f65719d6":"code","cd37d1fd":"code","6f241952":"code","031283de":"code","6fd52eb4":"code","a642b143":"code","0a2173ff":"code","09fe5887":"code","3cbb6c97":"code","1fa93f24":"code","838a73f2":"code","b57b43ba":"code","4d15891c":"code","ddf4d7dc":"code","ac29953e":"markdown","855359c5":"markdown","bec440c4":"markdown","dcb74df3":"markdown","45af36fc":"markdown","d2f59a84":"markdown","e0bc0d79":"markdown","d0b276b8":"markdown","ac1e82f1":"markdown","750f9319":"markdown","6b57ec38":"markdown","1d1dc17d":"markdown","f21ce3cc":"markdown","38c3768c":"markdown","234c50e4":"markdown"},"source":{"6411863c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","32141e79":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor","facc0a1f":"items = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nsales_train = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv')\nitem_cat = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')","c54032aa":"print('ITEMS')\ndisplay(items.head())\n\nprint('ITEM CATEGORIES')\ndisplay(item_cat.head())\n\nprint('SHOPS')\ndisplay(shops.head())\n\nprint('SALES TRAIN')\ndisplay(sales_train.head())\n\nprint('TEST')\ndisplay(test.head())\n\nprint('SAMPLE SUBMISSION')\ndisplay(sample_submission.head())","b1fe86be":"# Exploring Sample train dataset\nsales_train.info()","72cd6282":"# Checking missing values for sales train and test dataset\nprint('Sales train')\ndisplay(sales_train.isnull().sum())\n\nprint('test')\ndisplay(test.isnull().sum())","dc0b63cd":"print('Sales train')\ndisplay(sales_train.describe())\n\nprint('Test')\ndisplay(test.describe())","dafbde69":"# Removing duplicates from Sales train dataset\nsubset = ['date','date_block_num','shop_id','item_id','item_cnt_day']\nsales_train.duplicated(subset=subset).value_counts()","f55f73c8":"sales_train.drop_duplicates(subset=subset,inplace=True)","89f73d07":"#Checking for negative values in item_price and item_cnt_day and dropping them as it contains only 1 negative value.\nprint('Negative values in item_price')\ndisplay(sales_train[sales_train['item_price']<0])\n\nprint('Negative values in item_cnt_day')\ndisplay(sales_train[sales_train['item_cnt_day']<0])\n","5196c3c3":"sales_train = sales_train[sales_train['item_price']>0]\nsales_train = sales_train[sales_train['item_cnt_day']>0]\n","51b455f7":"# Finding Outliers for item price and item_cnt_day\nsns.boxplot(sales_train['item_price'])","4900e5c5":"sns.boxplot(sales_train['item_cnt_day'])","4969dca9":"#Dropping Ouliers\ndef drop_out(df,feature,high_percentile = .99):\n    df_shape = df.shape[0]         #sales train df size before dropping\n    max_val = df[feature].quantile(high_percentile)      #Percentile value\n    print('Dropping Outliers for ... {}'.format(feature))\n    df = df[df[feature] < max_val]\n    print(str(df_shape - df.shape[0]) + ' ' + feature + ' values over ' + str(max_val) + ' have been removed' )\n    return df\n    ","9761de62":"#Dropping outliers for item_price\nsales_train = drop_out(sales_train,'item_price')","0fa9ab6a":"#Dropping outliers for item_cnt_day\nsales_train = drop_out(sales_train,'item_cnt_day')","f27f492c":"# Creating new dataframe with item_price feature group by shop_id and item_id to get price for each item per shop.\n# We can use this dataframe to create item_price feature for the test dataset.","7614d68f":"shop_price_df = sales_train[['shop_id','item_id','item_price']]\nshop_price_df = shop_price_df.groupby(['shop_id','item_id']).apply(lambda df: df['item_price'][-2:].mean())\nshop_price_df = shop_price_df.to_frame(name='item_price')\nshop_price_df","47a14d98":"# Merge this dataframe with test dataframe to create item_price feature in test dataset.\ntest = pd.merge(test,shop_price_df,how='left', left_on=['shop_id','item_id'], right_on=['shop_id','item_id'])\ntest.head()","7da5c14b":"#check null values\ntest['item_price'].isnull().sum()      ","39fafe00":"#Split date feature to month and year from sales_train dataset\nsales_train['month'] = [date.split('.')[1] for date in sales_train['date']]\nsales_train['year'] = [date.split('.')[2] for date in sales_train['date']]\n\nsales_train.drop(['date','date_block_num'],axis=1,inplace=True)\n\n#create month and year features fot test dataset\ntest['month'] = 11\ntest['year'] = 2015","78d3a1f3":"#change item_cnt_day to item_cnt_month \nsales_train_monthly = sales_train.groupby(['year','month','shop_id','item_id'],\n                                          as_index=False)[['item_cnt_day']].sum()\nsales_train_monthly.rename(columns={'item_cnt_day':'item_cnt_month'},inplace=True)\n\nsales_train_monthly = pd.merge(sales_train_monthly,shop_price_df,how='left',\n                               left_on=['shop_id','item_id'],right_on=['shop_id','item_id'])\nsales_train_monthly.head()","6ed96901":"sales_train = sales_train_monthly\nsales_train.head()","67e66f43":"test = test.reindex(columns=['ID','year','month','shop_id','item_id','item_price'])\ntest.head()","2c078ae8":"#Extracting main categories \nitem_cat['main categories'] = [x.split('-')[0] for x in item_cat['item_category_name']]\n\n# Some items dont have sub category for them we will use None as a sub category\nsub_cat=[]\nfor i in range(len(item_cat)):\n    try:\n        sub_cat.append(item_cat['item_category_name'][i].split('-')[1])\n    except IndexError as e:\n        sub_cat.append('None')\n\nitem_cat['sub categories'] = sub_cat\nitem_cat.drop(['item_category_name'],axis=1,inplace=True)\nitem_cat.head()","f17534b5":"items = pd.merge(items,item_cat,how='left')\n\n#drop item_name and item_category_id\nitems.drop(['item_name','item_category_id'],axis=1,inplace=True)\nitems.head()","3142a50c":"# Merge items to test and sales_train dataset\nsales_train = pd.merge(sales_train,items,how='left')\ntest = pd.merge(test,items,how='left')","2305cd12":"from string import punctuation\n\n#replace all the punctuations from shop_name feature\nshops['shop_name_cleaned'] = shops['shop_name'].apply(lambda s:''.join([x for x in s if x not in punctuation]))\n\n#Extract shop city name\nshops['shop_city'] = shops['shop_name_cleaned'].apply(lambda s:s.split()[0])\n\n#Extract shop type\nshops['shop_type'] = shops['shop_name_cleaned'].apply(lambda s:s.split()[1])\n\n#Extract shop name\nshops['shop_name'] = shops['shop_name_cleaned'].apply(lambda s:s.split()[2:])\n\n#Dropping shop_name_cleaned\nshops.drop(['shop_name_cleaned'],axis=1,inplace=True)\n\nshops.head()","cbd0d080":"#Merging sales_train and test dataset with shops dataset\nsales_train = pd.merge(sales_train,shops,how='left')\ntest = pd.merge(test,shops,how='left')","092ae1fd":"print('SALES TRAIN')\ndisplay(sales_train.head())\n\nprint('TEST')\ndisplay(test.head())","101637c7":"#fill missing values with median of each main_category and sub_category\ntest['item_price'] = test.groupby(['main categories',\n                                   'sub categories'])['item_price'].apply(lambda df: df.fillna(df.median()))\ntest['item_price'].isnull().sum()","e9d04dea":"# Fill missing values with median of each sub category\ntest['item_price'] = test.groupby(['sub categories'])['item_price'].apply(lambda df : df.fillna(df.median()))\ntest['item_price'].isnull().sum()","a069a7d8":"test[test['item_price'].isnull()]","832767a9":"#fill missing values with median of main_category and sub_category from sales_train dataset\n\nfiller = sales_train[(sales_train['main categories'] == 'PC') & (sales_train['sub categories'] == '\u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438')]['item_price'].median()\nfiller = 0\ntest['item_price'].fillna(filler, inplace=True)","f65719d6":"test['item_price'].isnull().sum()","cd37d1fd":"# Clipping the item_cnt_month value to [0,20] range\nsales_train['item_cnt_month'] = sales_train['item_cnt_month'].clip(0,20)","6f241952":"# Defining a target array and dropping it from sales_train dataset\ntarget_aaray = sales_train['item_cnt_month']\nsales_train.drop(['item_cnt_month'],axis=1,inplace=True)\n\n#Dropping ID column from test dataset\ntest_id = test['ID']\ntest.drop(['ID'],axis=1,inplace=True)\n","031283de":"#Drop shop_id and item_id from sales_train and test dataset\nsales_train.drop(['shop_id','item_id'],axis=1,inplace=True)\ntest.drop(['shop_id','item_id'],axis=1,inplace=True)","6fd52eb4":"#Reducing memory usage\ndef downcast_dtypes(df):\n    # Selecting columns to downcast\n    flt_cols = [c for c in df if df[c].dtype == 'float64']\n    int_cols = [d for d in df if df[d].dtype == 'int64']\n    \n    #Downcasting\n    df[flt_cols] = df[flt_cols].astype(np.float16)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","a642b143":"downcast_dtypes(sales_train)\ndowncast_dtypes(test)\n\nprint('SALES TRAIN')\ndisplay(sales_train.head())\n\nprint('TEST')\ndisplay(test.head())","0a2173ff":"sales_train.info()","09fe5887":"#Check for null values in test and sales_train dataset\nprint('Missing data in sales train: ',sales_train.isnull().any().sum())\nprint('Missing data in test: ',test.isnull().any().sum())","3cbb6c97":"#Normality test function\ndef norm_test(data,alpha=0.05):\n    from scipy import stats\n    statistic,p_val = stats.normaltest(data)\n    \n    #null hypothesis: array comes from a normal distribution\n    if p_val < alpha:  \n        #The null hypothesis can be rejected\n        is_normal_dist = False\n    else:\n        #The null hypothesis cannot be rejected\n        is_normal_dist = True\n    \n    return is_normal_dist","1fa93f24":"# Check normality of all numerical features and change them to normal distribution\nfor i in sales_train.columns:\n    if sales_train[i].dtype != 'object':\n        if norm_test(sales_train[i]) == False:\n            sales_train[i] = np.log1p(sales_train[i])\n            test[i] = np.log1p(test[i])","838a73f2":"target_aaray = np.log1p(target_aaray)","b57b43ba":"#ENCODING\nord_enc = OrdinalEncoder()\narr = sales_train.to_numpy()\n#x = sales_train.to_numpy(dtype=str)\nprint(arr)\nX = ord_enc.fit_transform(arr)\nY = target_aaray\n\n#X_predict = ord_enc.fit_transform(test.to_numpy())","4d15891c":"x_train,x_test,y_train,y_test = train_test_split(sales_train,target_aaray,test_size=0.1,random_state=0)","ddf4d7dc":"#Create a model\nxg_model = XGBRegressor()\n\n#Fitting model\nxg_model.fit(x_train, y_train, eval_metric=\"rmse\", eval_set=[(x_train, y_train), (x_test, y_test)], verbose=True, early_stopping_rounds = 20)","ac29953e":"# Data Visualization","855359c5":"*No Missing values found*","bec440c4":"* 3. Shops dataset","dcb74df3":"* Item price in test dataset contains null values.fill this by creating more features from item_categories","45af36fc":"# Importing Libraries","d2f59a84":"# Exploring other Dataset\n* 1. Item_categories","e0bc0d79":"# Model creation \n* We will use XGBRegressor model to predict total sales for every product and store","d0b276b8":"# Loading Data","ac1e82f1":"* 2. Items dataset","750f9319":"*Sales train dataset contains negative values in feature item_price and  item_cnt_day.*","6b57ec38":"# Exploring the Dataset","1d1dc17d":"* Fill Missing values in item_price","f21ce3cc":"* Train test split","38c3768c":"# Reading the Data","234c50e4":"* All the null item_price data contains same main categories (PC) and sub catogories (\u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438) value. This main and sub categories are not present in test dataset but are present in sales_train dataset."}}