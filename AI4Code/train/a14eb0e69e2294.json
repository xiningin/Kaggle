{"cell_type":{"40b91c5d":"code","4b65b32d":"code","c261c492":"code","df7a4414":"code","32690fc9":"code","fc5f73a6":"code","4e940c5a":"code","80f37a23":"code","b8f6b39f":"code","d80baeb5":"code","1eeba545":"code","bc700e9f":"code","c421ca2c":"code","09c3004a":"code","8282d05d":"code","33e1b268":"code","0d26de96":"code","bd14d239":"code","0c0fee4b":"code","97d6fcc2":"code","277b7449":"code","03b6c245":"code","3365ddaf":"code","903794fb":"code","0afc1795":"code","480921d9":"code","96fa397c":"code","7ac20d30":"code","4ece8f71":"code","3ab9777a":"code","5b72b9c5":"code","3a94deeb":"code","a7d17ab2":"code","2047a1b8":"code","e13b865f":"code","be800742":"code","c9d24bdd":"code","32a3490c":"code","1b37b643":"code","9b603f7a":"code","48e2f674":"code","35461d6a":"code","550233d6":"code","c1bad521":"code","9c33c9fd":"code","81bd5acd":"code","4343bbd7":"code","57f93bc6":"code","5e0bec32":"code","a1bac999":"code","d6e6bf37":"code","7bfe140f":"markdown","2bc00afb":"markdown","dad490b0":"markdown","64b520bd":"markdown","bf8b187b":"markdown","56bdadc0":"markdown","a7e7702e":"markdown","46426860":"markdown","1c599879":"markdown","0966eb1a":"markdown","9778d947":"markdown","6a8129d3":"markdown","6fcc826d":"markdown","0a704d71":"markdown","7c13b6be":"markdown","20076325":"markdown","ce6743c3":"markdown","69020d32":"markdown","11661b15":"markdown","5a6d13a1":"markdown","ca218d34":"markdown","dd0874c1":"markdown"},"source":{"40b91c5d":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_colwidth', None)","4b65b32d":"df = pd.read_csv(\"\/kaggle\/input\/sentiment-analysis-for-financial-news\/all-data.csv\", encoding='latin-1', header = None)\ndf.columns = [\"Sentiment\", \"Headline\"]\ndf.sample(5)","c261c492":"df.Sentiment.hist()","df7a4414":"sent=pd.DataFrame(df.Sentiment.value_counts()\/len(df)).rename({'Sentiment':'Probability'},axis=1)\nsent.style.format({'Probability':'{:.0%}'})","32690fc9":"d={'neutral':0,'negative':0,'positive':0}\nfreq=pd.DataFrame(columns=['neutral','negative','positive'])\nfor _ in range(200):\n    tmp=df.sample(10)\n    for sent in tmp.Sentiment.unique(): d[sent]+=tmp.Sentiment.value_counts().loc[sent]\n    cumFreq=sum(d.values())\n    freq=freq.append(pd.DataFrame([[d['neutral']\/cumFreq,d['negative']\/cumFreq,d['positive']\/cumFreq]],columns=['neutral','negative','positive']))\nfreq.reset_index(drop=True).plot()","fc5f73a6":"#1\nprobProfit           =df[(df.Headline.str.contains('profit'))].shape[0]\/len(df)\nprobPositive         =df[((df.Sentiment=='positive'))].shape[0]\/len(df)\nprobProfitANDPositive=df[(df.Headline.str.contains('profit'))&(df.Sentiment=='positive')].shape[0]\/len(df)\n\nprint(f'Probability of Profits is {probProfit:.0%}')\nprint(f'Probability of Positive Sentiment is {probPositive:.0%}')\nprint(f'Probability of Profits and Positive Sentiment is {probProfitANDPositive:.0%}')\ndf[(df.Headline.str.contains('profit'))&(df.Sentiment=='positive')].sample(2)","4e940c5a":"#2\nprobLoss           =df[(df.Headline.str.contains('loss'))].shape[0]\/len(df)\nprobNegative       =df[((df.Sentiment=='negative'))].shape[0]\/len(df)\nprobLossANDNegative=df[(df.Headline.str.contains('loss'))&(df.Sentiment=='negative')].shape[0]\/len(df)\n\nprint(f'Probability of Loss is {probProfit:.0%}')\nprint(f'Probability of Negative Sentiment is {probNegative:.0%}')\nprint(f'Probability of Loss and Negative Sentiment is {probLossANDNegative:.0%}')\ndf[(df.Headline.str.contains('loss'))&(df.Sentiment=='negative')].sample(2)","80f37a23":"#first approach - math P(A|B)=P(A,B)\/P(B)\nf'P(Sentence containing \"profit\" | Positive Sentiment) is {probProfitANDPositive \/ probPositive:0.0%}.'","b8f6b39f":"#2nd approach - number of positive sentences as base\/denominator and numerator as the number of sentences that are both positive and has profits\nnumPositive         =df[((df.Sentiment=='positive'))].shape[0]\nnumProfitANDPositive=df[(df.Headline.str.contains('profit'))&(df.Sentiment=='positive')].shape[0]\n\nf'P(Sentence containing \"profit\" | Positive Sentiment) is {numProfitANDPositive \/ numPositive:0.0%}.'","d80baeb5":"#Using a Simple Sentence and its variants as a toy example\ndf.loc[[965]]","1eeba545":"from sklearn.feature_extraction.text import CountVectorizer \n\nvectorizer = CountVectorizer()","bc700e9f":"sentences=['Productional situation has now improved.',\n                            'Productional situation has now deproved.',\n                            'the situation has improved.']\nX=vectorizer.fit_transform(sentences)\nX.toarray()","c421ca2c":"y=np.array(['positive','negative','positive'])","09c3004a":"vocab=vectorizer.get_feature_names()\nvocab","8282d05d":"pd.DataFrame(X.toarray(),columns=vocab,index=sentences)","33e1b268":"tst_term_doc=vectorizer.transform(['the situation deproved,deproved!'])\ntst_term_doc.toarray()","0d26de96":"import numpy as np\nb=np.log((2\/3)\/(1\/3)) #we know that 2 out of 3 documents are positive and 1 out of 3 documents is negative\n\nf'Log of prior probability ratio is {b:.1}'","bd14d239":"posProb=(X[y=='positive'].sum(0)+1)\/(X[y=='positive'].sum(0).sum()+len(vocab))\nnegProb=(X[y=='negative'].sum(0)+1)\/(X[y=='negative'].sum(0).sum()+len(vocab))\n\n#convert matrix to a numpy array\nposProb=np.squeeze(np.asarray(posProb)) \nnegProb=np.squeeze(np.asarray(negProb))","0c0fee4b":"R = np.log(posProb\/negProb)","97d6fcc2":"preds=tst_term_doc@R+b\nprint(preds)\npreds=np.array(['negative' if x <0 else 'positive' for x in preds])","277b7449":"df=df[df.Sentiment!='neutral']","03b6c245":"from sklearn.model_selection import train_test_split\ntrn_x, tst_x, trn_y, tst_y = train_test_split(df.Headline, df.Sentiment, test_size=0.2, random_state=42)","3365ddaf":"p=(trn_y=='positive').sum()\/len(trn_y) #prior for positive\nq=(trn_y=='negative').sum()\/len(trn_y) #prior for negative\n\nf'Prior probabilities for positive and negative classes are {p:.0%} and {q:.0%}'","903794fb":"import numpy as np\nb=np.log(p\/q)\n\nf'Log of prior probability ratio is {b:.1}'","0afc1795":"#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html\nfrom sklearn.feature_extraction.text import CountVectorizer \n\nvectorizer = CountVectorizer()\ntrn_term_doc= vectorizer.fit_transform(trn_x)\ntst_term_doc= vectorizer.transform(tst_x)","480921d9":"vocab=vectorizer.get_feature_names()\nvocab[500:510]","96fa397c":"trn_term_doc","7ac20d30":"len(vectorizer.vocabulary_)","4ece8f71":"#type code here","3ab9777a":"trn_x.sample(1)","5b72b9c5":"type(vectorizer.vocabulary_)","3a94deeb":"vectorizer.vocabulary_['the']","a7d17ab2":"trn_term_doc[1,4401]","2047a1b8":"#type code here","e13b865f":"X=trn_term_doc\ny=np.array(trn_y)\n\nposProb=(X[y=='positive'].sum(axis=0)+1)\/(sum(y=='positive')+len(vocab))\nnegProb=(X[y=='negative'].sum(axis=0)+1)\/(sum(y=='negative')+len(vocab))\n\n#convert matrix to a numpy array\nposProb=np.squeeze(np.asarray(posProb)) \nnegProb=np.squeeze(np.asarray(negProb))","be800742":"#answer","c9d24bdd":"R = np.log(posProb\/negProb)","32a3490c":"vectorizer.vocabulary_['fell']","1b37b643":"R[1788]","9b603f7a":"preds=tst_term_doc @ R+b\npreds=np.array(['negative' if x <0 else 'positive' for x in preds])","48e2f674":"(preds[:4]==tst_y[:4]).mean()","35461d6a":"acc=(preds==tst_y).mean()\nf'Accuracy using NB is {acc:.1%}'","550233d6":"vectorizer = CountVectorizer(binary=True,ngram_range=(1,4)) #using unigrams, bigrams and trigrams and binarize\nX=trn_term_doc_bin_ngram= vectorizer.fit_transform(trn_x)\ntst_term_doc_bin_ngram= vectorizer.transform(tst_x)","c1bad521":"R = np.log((X[y=='positive'].sum(axis=0)+1)\/(X[y=='positive'].sum(0).sum()+len(vocab))\/(X[y=='negative'].sum(axis=0)+1)\/(X[y=='negative'].sum(0).sum()+len(vocab)))\nR = np.squeeze(np.asarray(R))","9c33c9fd":"from sklearn.svm import LinearSVC #for this tutorial, consider this as a blackbox machine learning model","81bd5acd":"x_nb=trn_term_doc_bin_ngram.multiply(R)\nm = LinearSVC().fit(x_nb, y)","4343bbd7":"acc_nbsvm=m.score(tst_term_doc_bin_ngram.multiply(R),tst_y)\nf'Accuracy using NBSVM is {acc_nbsvm:.3%}, {acc_nbsvm-acc:.1%} better than NB alone!'","57f93bc6":"#type code here","5e0bec32":"!pip install -qq nbsvm\n\nfrom nbsvm import NBSVM\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline","a1bac999":"df = pd.read_csv(\"\/kaggle\/input\/sentiment-analysis-for-financial-news\/all-data.csv\", encoding='latin-1', header = None)\ndf.columns = [\"Sentiment\", \"Headline\"]\n\nclf = make_pipeline(CountVectorizer(binary=True), NBSVM())\nscores = cross_val_score(clf, df.Headline, df.Sentiment, cv=10)\nf'Accuracy on all data is slightly higher than LSTM, LPS and HSC at {np.mean(scores):.0%}!'","d6e6bf37":"df=pd.read_csv('..\/input\/sentiment-analysis-for-financial-news\/FinancialPhraseBank\/Sentences_AllAgree.txt', encoding='latin-1', header = None,sep='.@')\ndf.columns = [\"Headline\",\"Sentiment\"]\n\nclf = make_pipeline(CountVectorizer(binary=True), NBSVM())\nscores = cross_val_score(clf, df.Headline, df.Sentiment, cv=10)\n\nf'Accuracy on 100% agreement data is much higher than LSTM, LPS and HSC at {np.mean(scores):.0%}!'","7bfe140f":"# Beating LSTM, LPS and HSC with NBSVM\nWithout any fancy architecture\/pre-processing, it beats LSTMs, a deep learning model!\n- https:\/\/arxiv.org\/pdf\/1908.10063.pdf - refer to Table 2 in finBERT paper for comparison","2bc00afb":"Changing CountVectorizer to be binary and take in bi\/tri grams","dad490b0":"The long run frequencies will converge to the probabilities..","64b520bd":"second way: given the sentence is positive, our \"world\" becomes only positive sentences. so to calculate probability of profits in that world, we can take count of sentences that are both positive and contain profits divided by the count of positive sentences","bf8b187b":"![Bayes](https:\/\/i1.wp.com\/www.rensvandeschoot.com\/wp-content\/uploads\/2017\/09\/bayes-theorem.png?ssl=1)","56bdadc0":"first way: if we just apply the formula","a7e7702e":"## Now let's do the same thing, but for the entire dataset. \nLet's exclude neutral sentences to keep things simple and binary.","46426860":"## Implementing the excel walkthrough in python","1c599879":"What happens if there is a word in `tst_x` that didn't occur in trn_x?","0966eb1a":"Exercise: Try from scratch, creating NB model using bigrams or 4-grams and see if it's better?\n> For `CountVectorizer` parameters and to tweak them, refer to https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html ","9778d947":"## Frequentist Approach To Probability - Probability as the long run frequency of events","6a8129d3":"## A simple walkthrough of Naive Bayes and Count Ratios, refer to Google Sheets link: \nhttps:\/\/docs.google.com\/spreadsheets\/d\/1ha-kv2gV1OwBhVWpdg42Fcu7NMlYdl_rvPAljZqwYeU\/edit?usp=sharing ","6fcc826d":"## Conditional Probability - Probability of event A happening given event B has happened\nWhat is the probability of a sentence containing \"profit\" given sentiment is positive?","0a704d71":"# Probability and Naive Bayes\nIn this tutorial, we cover some basic probability, code up a baseline NLP financial sentiment classifier from scratch, using Bayes Rule, and try to beat some DL models with NB-SVM!\n- Companion Slides: https:\/\/docs.google.com\/presentation\/d\/1S_hFtayisDzBVVwG-CAa4rRVquIq0Ax3BCU5oUbMrzU\/edit?usp=sharing\n- Companion Excel: https:\/\/docs.google.com\/spreadsheets\/d\/1ha-kv2gV1OwBhVWpdg42Fcu7NMlYdl_rvPAljZqwYeU\/edit?usp=sharing","7c13b6be":"Qn: How often does the word \"the\" appear in the phrase below? Confirm that the correct counts are stored in the term-document matrix.\n> - **Tip1:** To find the position of an element(x) in a list: LIST.index(x)\n> - **Tip2:** Convert a matrix to a numpy array for easier indexing: ARRAY = np.squeeze(np.asarray(MATRIX)) ","20076325":"Naive Bayes as input features","ce6743c3":"Qn: How many documents and terms are there?","69020d32":"Qn: Why do we +1 in numerator?","11661b15":"## Joint Probability - the intersection of 2 events\n1. What is the probability of a sentence containing \"profit\" and sentiment is positive?\n1. What is the probability of a sentence containing \"loss\" and sentiment is negative?","5a6d13a1":"## NB-SVM\nCan we do better than a standard Naive Bayes?\nYes, by \n1. Binarizing\n1. Bi-grams (or maybe even tri-grams!)\n1. Use NB features as input to SVM\n\n#### Source: https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf","ca218d34":"![intersection](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/9\/99\/Venn0001.svg\/2560px-Venn0001.svg.png)","dd0874c1":"Question: What is the higher probabiltiy for the word 'fell' and 'rose'?\n> **Tip:** To find the position of an element(x) in a list, use list.index(x)"}}