{"cell_type":{"dd6916ac":"code","b3a36ce1":"code","d70cbd23":"code","68fd037c":"code","1cafbd47":"code","505b1b4f":"code","84e62c23":"code","47119726":"code","e74c2a4f":"code","ed9e0bc6":"code","811e2bbc":"code","3cfaefff":"code","31a012ba":"code","4e99fd97":"markdown","0b0f6cb9":"markdown","e17982a6":"markdown","a94f31e8":"markdown","a5c30970":"markdown","fc946516":"markdown","0bb20c8a":"markdown","a03a34bc":"markdown","c6f6f196":"markdown","51c6a2c7":"markdown","b669045e":"markdown","41e043fe":"markdown","323f66d8":"markdown","0fa25bfe":"markdown"},"source":{"dd6916ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b3a36ce1":"# import the data\npath = r'\/kaggle\/input\/data-on-songs-from-billboard-19992019\/BillboardFromLast20\/billboardHot100_1999-2019.csv'\nbb_data = pd.read_csv(path, index_col=0)\nbb_data.tail()","d70cbd23":"# Filter to only country songs\ncolumns = ['Artists', 'Name', 'Weekly.rank', 'Peak.position', 'Lyrics']\ncountry_songs = bb_data[bb_data.Genre.str.contains('Country', regex=False)][columns].copy()\ncountry_songs","68fd037c":"###\n# Calculating Peak.position\n###\nfrom time import perf_counter\nnew_peak_positions = country_songs.groupby(['Artists','Name']).agg(min_rank=('Weekly.rank', min))\n\n\ndef newPeak(row):\n    ndx = (row['Artists'], row['Name'])\n    new_peak_position = new_peak_positions.loc[ndx, 'min_rank']\n    if np.isnan(row[\"Peak.position\"]):\n        row[\"Peak.position\"] = new_peak_position\n    else:\n        new_min = min(row[\"Peak.position\"], new_peak_position)\n        row[\"Peak.position\"] = new_min\n    return row[\"Peak.position\"]\n\nstart = perf_counter()\ncountry_songs[\"Peak.position\"] = country_songs.apply(newPeak, axis=1)\nstop = perf_counter()\nprint(\"Time to calculate new peak position:\", stop-start) \ncountry_songs.head()","1cafbd47":"# Define the billboard rank for critical banger zone\nhighway_to_the_banger_zone = 5\n\n# Separate by weekly rank\ncountry_bangers = country_songs[bb_data['Weekly.rank'] <= highway_to_the_banger_zone]\ncountry_bops = country_songs[bb_data['Weekly.rank'] > highway_to_the_banger_zone]\n\n# import plotting modules\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# who has spent the most time on the charts?\nbopzone_weeks = country_bops['Artists'].value_counts()[0:20]\nfig, (ax1, ax2) = plt.subplots(2, figsize=(10,12))\nax1.set_title(\"Weeks in the Bop Zone (Top 20 Artists)\")\nsns.barplot(y=bopzone_weeks.index, x=bopzone_weeks.values, ax=ax1)\n\n# who has spent the most time in banger zone?\nbangerzone_weeks = country_bangers['Artists'].value_counts()[0:20]\nax2.set_title(\"Weeks in the Banger Zone (Top 20 Artists)\")\nsns.barplot(y=bangerzone_weeks.index, x=bangerzone_weeks.values, ax=ax2)","505b1b4f":"###\n# Data Prep\n### \n\n# Create a unique list of songs\ncolumns = ['Artists', 'Name', 'Peak.position', 'Lyrics']\ncountry_song_set = country_songs[columns].copy().drop_duplicates()\n\n# Make sure Country Roads never made the charts\ncountry_roads = country_song_set[country_song_set[\"Name\"].str.contains(\"country road\", case=False, regex=False)].loc[:,[\"Artists\", \"Name\"]]\nif country_roads.size == 0:\n    print('\"Take Me Home, Country Roads\" did not make the billboards from 1999-2019')\nelse:\n    print('\"Take Me Home, Country Roads\" did make the billboards from 1999-2019')","84e62c23":"##\n# Natural Language Processing - Introduction of terms\n##\n\n# Import NLP module\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n# set up an example\ntest_string = country_song_set.loc[1, 'Lyrics']\n\n\n# load the English language pipeline and tokenize the text string\nnlp = spacy.load('en')\ndocx = nlp(test_string)\n\n# demo of lemmatizing and stop words\nlst = list()\ndir(lst)\nfor word in docx:\n    lst.append([word.text, word.lemma_, word.is_stop, word.is_punct, word.is_space])\n\n# display the results\nfrom tabulate import tabulate    \nprint(tabulate(lst[0:25], headers=['word', 'lemma', 'isStop', 'isPunct', 'isSpace']))\n","47119726":"###\n# Is Country Roads a banger? - Create the tokenizer\n###\n\n# Create a method to preprocess the lyrics\ndef Tokenize(lyrics):\n    # Create doc\n    doc = nlp(lyrics)\n    \n    #lemmatize and remove stop words\n    tokens = [word.lemma_.lower().strip() for word in doc if word.lemma_ != '-PRON-' and not word.is_stop and not word.is_punct and not word.is_space]\n    return tokens\n    \nprint(Tokenize(test_string))","e74c2a4f":"# Create real data set\nX = country_song_set['Lyrics']\ny = country_song_set['Peak.position'] <= highway_to_the_banger_zone\n\n# Split the data into testing and validation sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n\n# show sample from the train data\nprint('Train Data: IsBanger Counts\\n---------------------------\\n',y_train.value_counts())\nprint('\\nTest Data: IsBanger Counts\\n---------------------------\\n',y_test.value_counts())\npd.concat([X_train, y_train], axis=1)","ed9e0bc6":"start = perf_counter()\n\n\n# Use Bag of Words methodology (try TF-IDF later)\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nbow_vector = CountVectorizer(tokenizer = Tokenize, ngram_range=(1,1))\n\n\n# For binary target, use logistic regression\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter=1000)\n\n\n# Create the bow pipeline and fit the model\nfrom sklearn.pipeline import Pipeline\nbow_pipe = Pipeline([(\"Vectorizer\", bow_vector),\n                 (\"Classifier\", model)])\nbow_pipe.fit(X_train, y_train)\n\n\n# Predict values using the trained model\ny_predicted_bow = bow_pipe.predict(X_test)\n\n\n# Measure performance\nstop = perf_counter()\nprint(\"Time to train and evaluate model (in seconds):\", stop-start) ","811e2bbc":"# Create a confusion matrix\ndef ConfusionMatrix(lstPredictions, lstActual):\n    tp = tn = fp = fn = 0\n    for i in range(0, len(lstPredictions)):\n        if lstPredictions[i] == True:\n            if lstActual[i] == True: tp+=1\n            else: fp+=1\n        else:\n            if lstActual[i] == False: tn+=1\n            else: fn+=1\n    return tp, tn, fp, fn\n\n\ndef EvalMetrics(tp, tn, fp, fn, display_results=True):\n    # Model Accuracy - How often was the model right?\n    acc = (tp + tn) \/ (tp + tn + fp + fn)\n    \n    # Sensitivity\/Recall\/TP Rate - How often do we correctly classify the bangers?\n    sens = tp \/ (tp + fn) if (tp + fn) != 0 else 0\n    \n    # Specificity - How often did we correctly classify the bops?\n    spec = tn \/ (tn + fp) if (tn + fp) != 0 else 0\n    \n    # Precision - How certain are we that something that the model says is a banger is truly a banger?\n    prec = tp \/ (tp + fp) if (tp + fp) != 0 else 0\n    \n    if display_results:\n        print(\"Model Accuracy - How often was the model right? - {0:.0%}\".format(acc))\n        print(\"Sensitivity\/Recall\/TP Rate - How often do we correctly classify the bangers? - {0:.0%}\".format(sens))\n        print(\"Specificity - How often did we correctly classify the bops? - {0:.0%}\".format(spec))\n        print(\"Precision - How certain are we that something that the model says is a banger is truly a banger? - {0:.0%}\".format(prec))\n    \n    return {\"Model Accuracy\": acc, \"Sensitivity\":sens, \"Specificity\":spec, \"Precision\":prec}\n\ntp, tn, fp, fn = ConfusionMatrix(y_predicted_bow, list(y_test))\n\nprint(\"(tp, tn,\\n fp, fn)\")\nprint(('({}, {},\\n {}, {})\\n\\n'.format(tp, tn, fp, fn)))\n\ndEval = EvalMetrics(tp, tn, fp, fn)\n\n\n# When validating binary, use \"metrics\"  from scikit\nfrom sklearn import metrics\nprint(\"\\nsklearn.metrics\\n---------------\")\nprint(\"Accuracy (num correct \/ total):\",metrics.accuracy_score(y_test, y_predicted_bow))\nprint(\"Sensitivity\/Recall (true negs \/ all actual negs):\",metrics.recall_score(y_test, y_predicted_bow))\nprint(\"Precision (true positives \/ all predicted positives):\",metrics.precision_score(y_test, y_predicted_bow))\n\n# 44, 98, 52, 60","3cfaefff":"###\n# Try using TF-IDF\n###\nstart = perf_counter()\n\n# Create TF-IDF vectorizer\ntfidf_vector = TfidfVectorizer(tokenizer = Tokenize, ngram_range=(1,1))\n\n# Create the TF-IDF pipeline and fit the model\ntfidf_pipe = Pipeline([(\"Vectorizer\", tfidf_vector),\n                 (\"Classifier\", model)])\ntfidf_pipe.fit(X_train, y_train)\n\n\n# Predict values using the trained model\ny_predicted_tfidf = tfidf_pipe.predict(X_test)\n\n\n# Measure Performance\nstop = perf_counter()\nprint(\"Time to train and evaluate model (in seconds):\", stop-start) \n\n\ntp, tn, fp, fn = ConfusionMatrix(y_predicted_tfidf, list(y_test))\nprint(\"(tp, tn,\\n fp, fn)\")\nprint(('({}, {},\\n {}, {})\\n\\n'.format(tp, tn, fp, fn)))\n\ndEval = EvalMetrics(tp, tn, fp, fn)","31a012ba":"country_roads_lyrics ='''\n    Almost heaven, West Virginia\n    Blue Ridge Mountains, Shenandoah River\n    Life is old there, older than the trees\n    Younger than the mountains, growing like a breeze\n    Country roads, take me home\n    To the place I belong\n    West Virginia, mountain mama\n    Take me home, country roads\n    All my memories gather 'round her\n    Miner's lady, stranger to blue water\n    Dark and dusty, painted on the sky\n    Misty taste of moonshine, teardrop in my eye\n    Country roads, take me home\n    To the place I belong\n    West Virginia, mountain mama\n    Take me home, country roads\n    I hear her voice, in the morning hour she calls me\n    The radio reminds me of my home far away\n    Driving down the road, I get a feeling\n    That I should have been home yesterday, yesterday\n    Country roads, take me home\n    To the place I belong\n    West Virginia, mountain mama\n    Take me home, country roads\n    Country roads, take me home\n    To the place I belong\n    West Virginia, mountain mama\n    Take me home, country roads\n    Take me home, down country roads\n    Take me home, down country roads'''\n\ncountry_roads = [country_roads_lyrics]\n\n# Train the model\ntfidf_pipe_full = Pipeline([(\"Vectorizer\", tfidf_vector),\n                 (\"Classifier\", model)])\ntfidf_pipe_full.fit(X, y)\n\n# Predict if Country Roads is a banger\ncountry_roads_isBanger = tfidf_pipe_full.predict(country_roads)\n\nif country_roads_isBanger: print(\"Computers don't lie... Country Roads is a banger!\")\nelse: print(\"Computers don't lie... Country Roads IS NOT a banger!\")","4e99fd97":"WELL THERE YA HAVE IT, BOYS!","0b0f6cb9":"First, we're going to import in Billboard's Top 100 from years 1999-2019.","e17982a6":"Now we measure the performance.","a94f31e8":"Here's what we're left with when we apply this process to Old Town Road.","a5c30970":"Now we can train the model. Using the Bag of Words methodoly, we just turn this into a giant table where the count of words are represented as columns.","fc946516":"So the model tried guessing that song was a banger 1 out of the 254 guesses, and it was wrong. Really, this is a problem with training a model on such lopsided datasets. \n\nLet's try a Term Frequency - Inverse Document Frequency (TF-IDF) model. This method gives extra \"weight\" to words that appear more often in bangers.","0bb20c8a":"We're primarily concerned with Country Bangers, so let's filter out all songs that aren't country.","a03a34bc":"It looks like this model just guessed that *all songs aren't bangers*. That doesn't do us any good.\n\nBut on second thought...\n\n...did we just find the prefect model to evaluate if Country Roads is a banger?","c6f6f196":"We can see that some songs have a peak position of NaN (not a number). That simply won't do. Let's calculate the missing values.","51c6a2c7":"The data thus far doesn't really show anything we didn't already know.\n - Taylor Swift... girl can write a song. Look at that banger to bop ratio!\n - FGL (split into two lines) is also a banger writing machine.\n - Nelly and Uncle Kracker are A-list country artists.\n \n\nWe'll turn this into a set of unique values and make sure Country Roads (1971) didn't crack the charts in the last two decades.","b669045e":"Now let's train a model using all the lyrics. Before we feed in all the data, we want to evaluate how effective the model is. We'll train the model with 80% of the data and evaluate it against the remaining 20%. Here's what the data looks like.","41e043fe":"Now lets prep the data for some real galaxy brain stuff. \n\nWe're going to use Natrual Language Processing to see if we can identify a banger by the lyrics. To do so, we'll want to see how often each word appears in each song, but some words are \"different\" but really mean the same thing. Take the Big Bills Certified Banger \"Save A Horse, Ride A Cowby\". He \"rides\" into the city, but he's \"riding\" up in down broadway. Two words that really just mean \"ride\". For this, we turn to \"lemmatizing\".\n\nAlso, some words have more meaning than others. **Beer**, **trucks**, **horses**. Those have meaning. Words like *the, a, and, but* matter less. In NLP, we call these \"stop words\".","323f66d8":"From the dawn of Big Bills, there has been one question that has torn the band in two. The heated debates have turned Montucky Cold Snacks into Montucky Warm Snacks. Warm Peebers have been made into even warmer (yet still refreshing) Peebers. Joe, out of frustration, played the drums louder and lounder, and Mitch, with his sensitive ears, couldn't take it any longer. So Mitch set out to answer the ultimate question - Is \"Take Me Home, Country Roads\" a banger?","0fa25bfe":"Much better.\n\nNow let's start talking about our methods. To define a banger, we're going to go with what The People say. Sure, some of The People are real Soglins and don't have an innate sense for bangers, but that's the data we have and we're going to use it.\n\nWe can define our Critical Banger Zone to separate our bangers from our bops."}}