{"cell_type":{"318ee44c":"code","5491c605":"code","72e5ed91":"code","2678cb32":"code","192088fa":"code","8cc3e4ac":"code","0ae0e01e":"code","5c4a03e8":"code","28a9b317":"code","949be513":"code","cd0d76f1":"code","bf3ccf89":"code","0cdd3f64":"code","fec781b4":"code","73c2f187":"code","442cdcd9":"code","70b993d2":"code","1a6477bd":"code","8c47d063":"code","a62f9399":"code","b4ac18d8":"code","9ad4cfec":"code","9b41caaf":"code","c72a53eb":"code","fe428924":"code","6bec72c2":"code","47b84bcb":"code","ca9dabe4":"code","7ff1f91d":"code","e0192229":"code","6edd2677":"code","8164325c":"code","bcfd8713":"code","a240a697":"code","e0091577":"code","6cd30e21":"markdown","756a2b74":"markdown","d5b15d3e":"markdown","a41f4f10":"markdown","514ad1d5":"markdown","2f44ac66":"markdown","55a3baae":"markdown","7808403b":"markdown"},"source":{"318ee44c":"import pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\n\nimport optuna\nimport shap\nimport pickle\nfrom xgboost import plot_tree\nimport graphviz\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.svm import SVC\n","5491c605":"import warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_rows = 999\npd.options.display.max_columns = 999","72e5ed91":"def get_data():    \n    # local data\n    #df = pd.read_csv('\/Users\/cjpw\/Documents\/jupyter\/heart_disease\/heart.csv')\n    \n    # kaggle data\n    df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\n    \n    return df","2678cb32":"df = get_data()","192088fa":"df.HeartDisease.value_counts(normalize=True)*100\n# we have a pretty balanced dataset. If the model were to choose 1 (yes) for every response it would be right 55% of the time. so this is our really low benchmark","8cc3e4ac":"# lets check the disribution of values for each feature\ndef all_hists():\n    fig, ax = plt.subplots(1, len(df.columns), figsize=(3*len(df.columns), 6))\n    for i, var in enumerate(df):\n        df[var].hist(ax=ax[i])\n        ax[i].set_title(var)\n    plt.show()\n\nall_hists()","0ae0e01e":"# checking the resting BP, we have a zero score\ndf.loc[df['RestingBP'] < 100].sort_values('RestingBP',ascending = True)","5c4a03e8":"df.RestingBP.hist()","28a9b317":"# checking the max heart rate\ndf.loc[df['MaxHR'] < 70].sort_values('MaxHR',ascending = True)\n# looks okay, doest seem to go below 60 which is strange as in the hist it looked weird","949be513":"df['MaxHR'].hist()","cd0d76f1":"# checking age\nprint(\"min age: \", df.Age.min())\nprint(\"max age: \", df.Age.max())\n","bf3ccf89":"# checking the cholestrol\ndf.loc[df['Cholesterol']  == 0].sort_values('Cholesterol',ascending = True).head(10)\n# we have 172 rows with no data, it seems 0's should be NaN","0cdd3f64":"# lets make the null rows nan and then we can drop\n\ndef find_and_drop_nans(df):\n    df['Cholesterol'] = df['Cholesterol'].replace(0, np.nan)\n    df['RestingBP'] = df['RestingBP'].replace(0, np.nan)\n\n    #print(df.Age.count())\n    df.dropna(inplace = True)\n    #print(df.Age.count())\n    # we have ended up with 746 items after dropping NaN's\n    return df","fec781b4":"df = find_and_drop_nans(df)","73c2f187":"# lets call the histogram func again and see what has changed\nall_hists()\n# looks good, we fixed everything!","442cdcd9":"df.dtypes","70b993d2":"# now lets check the values in the text category fields\nfor i in df.select_dtypes(\"object\"):\n    print(f'{i}')\n    print(df[i].value_counts())\n    print(\"\\n\", \"\\n\")\n\n","1a6477bd":"df.head()","8c47d063":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n        print(\"\")\n        print(\"\")\n    else:\n        print('Confusion matrix, without normalization')\n        print(\"\")\n        print(\"\")\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('Actual Value')\n    plt.xlabel('Predicted Value')","a62f9399":"# function to encode\n\ndef encode_values(df):\n    \n    \n\n    # defining the columns we want to encode\n    cols_to_transform = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG','ExerciseAngina','ST_Slope']\n    \n    encoder = OneHotEncoder(handle_unknown='ignore')\n\n    #making an encoding object\n    encoded = encoder.fit_transform(df[cols_to_transform]).toarray()\n\n    # getting the column names of the encoded data\n    column_names = encoder.get_feature_names(cols_to_transform)\n\n    # putting the encoded data into a data frame\n    encoded_df =  pd.DataFrame(encoded, columns = column_names)\n\n    # dropping the original columns from the df\n    df.drop(cols_to_transform, axis = 'columns', inplace = True)\n\n    #merging the encoded data with the other, non encode data. \n    df = pd.merge(df, encoded_df, left_index=True, right_index=True, how = 'inner')\n\n    # just putting the target variable at the end for convenience\n    df = df[['Age', 'RestingBP', 'Cholesterol', 'MaxHR', 'Oldpeak',\n               'Sex_F', 'Sex_M', 'ChestPainType_ASY', 'ChestPainType_ATA',\n               'ChestPainType_NAP', 'ChestPainType_TA', 'FastingBS_0', 'FastingBS_1',\n               'RestingECG_LVH', 'RestingECG_Normal', 'RestingECG_ST',\n               'ExerciseAngina_N', 'ExerciseAngina_Y', 'ST_Slope_Down',\n               'ST_Slope_Flat', 'ST_Slope_Up','HeartDisease']]\n    \n    return df\n","b4ac18d8":"# train test split\ndef my_train_test_func(df):\n    \n    y = df.pop('HeartDisease')\n    X = df.copy()\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33 , random_state=99)\n\n    return X_train, X_test, y_train, y_test","9ad4cfec":"def my_xgb_tree(model,my_num_trees,save_name,long,high):\n    \n    xgb.plot_tree(model, num_trees=my_num_trees)\n    fig = plt.gcf()\n    fig.set_size_inches(long, high)\n    fig.show()\n    fig.savefig(save_name)\n    ","9b41caaf":"# XGBOOOOOOOOST\n\ndef xgbc_run_please(X_train, X_test, y_train, y_test):\n    \n    xgbc = XGBClassifier()\n    print(xgbc)\n    XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n           colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n           max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n           n_estimators=100, n_jobs=1, nthread=None,\n           objective='multi:softprob', random_state=0, reg_alpha=0,\n           reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n           subsample=1, verbosity=1) \n\n    xgbc.fit(X_train, y_train)\n    scores = cross_val_score(xgbc, X_train, y_train, cv=5)\n    print(\"Mean cross-validation score: %.2f\" % scores.mean())\n\n    kfold = KFold(n_splits=10, shuffle=True)\n    kf_cv_scores = cross_val_score(xgbc, X_train, y_train, cv=kfold )\n    print(\"K-fold CV average score: %.2f\" % kf_cv_scores.mean())\n\n    y_pred = xgbc.predict(X_test)\n    \n    cm = confusion_matrix(y_test, y_pred,labels=[0,1])\n    print('ROC AUC score:', roc_auc_score(y_test, y_pred))\n    print(classification_report (y_test, y_pred))\n    \n    plot_confusion_matrix(cm, classes =['no', 'yes'],\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues)\n   # function to generate the tree \n    my_xgb_tree(xgbc,2,'xgbc_tree.png',100,200)\n    \n    ","c72a53eb":"def svm_please(X_train, X_test, y_train, y_test):\n\n    clf = SVC(kernel='linear')\n    clf.fit(X_train,y_train)\n    y_pred = clf.predict(X_test)\n\n    cm = confusion_matrix(y_test, y_pred,labels=[0,1])\n    print('ROC AUC score:', roc_auc_score(y_test, y_pred))\n    print(classification_report (y_test, y_pred))\n    \n    plot_confusion_matrix(cm, classes =['yes', 'no'],\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues)\n    ","fe428924":"# Running SVM Model\n\ndf = get_data()\n#df = find_and_drop_nans(df)\ndf = encode_values(df)\nX_train, X_test, y_train, y_test = my_train_test_func(df)\n# xgbc = xgbc_run_please(X_train, X_test, y_train, y_test)\nsvm = svm_please(X_train, X_test, y_train, y_test)\n\n","6bec72c2":"# Running XGB Model\n\ndf = get_data()\n#df = find_and_drop_nans(df)\ndf = encode_values(df)\nX_train, X_test, y_train, y_test = my_train_test_func(df)\nxgbc = xgbc_run_please(X_train, X_test, y_train, y_test)\n","47b84bcb":"df = get_data()\ndf = find_and_drop_nans(df)\ndf = encode_values(df)\ndf.info()\n\n# need to figure out why we onlyhave 574 rows..","ca9dabe4":"corr_df = df.corr().abs()\ncorr_df['HeartDisease'].sort_values(ascending = False)\n","7ff1f91d":"# looking a heart disease by age\n\nplt.figure(figsize=(18,12))\ndf[df['HeartDisease']==1]['Age'].hist(alpha=0.5,color='blue',\n                                              bins=30,label='heart disease')\ndf[df['HeartDisease']==0]['Age'].hist(alpha=0.5,color='red',\n                                              bins=30,label='no heart disease')\nplt.legend()\nplt.ylabel('count')\nplt.xlabel('Age')\n\nplt.figtext(0.05,-0.05,'''\nObservation:\\n \nAs we would excect, heart disease rates are much higher as people age \n''', family='San', size=15, ha='left')","e0192229":"# plotting age against cholestrol\n\ndf['age_group'] = pd.qcut(df['Age'], q=8)\n\n# import plotly.express as px\n# fig = px.box(df, x=\"age_group\", y=\"Cholesterol\")\n# fig.show()\n\nimport seaborn as sns\nplt.figure(figsize=(10, 10))\nsns.boxplot(x='age_group', y='Cholesterol', data=df)\nsns.set(font_scale=1)\n","6edd2677":"import seaborn as sns\nplt.figure(figsize=(4, 4))\nsns.boxplot(x='HeartDisease', y='RestingBP', data=df)\nsns.set(font_scale=1)\n","8164325c":"import seaborn as sns\nplt.figure(figsize=(4, 4))\nsns.boxplot(x='HeartDisease', y='MaxHR', data=df)\nsns.set(font_scale=1)","bcfd8713":"import seaborn as sns\nplt.figure(figsize=(4, 4))\nsns.boxplot(x='HeartDisease', y='Cholesterol', data=df)\nsns.set(font_scale=1)\n","a240a697":"def make_mi_scores(X, y):\n    X = X.copy()\n    # Exclude continous features\n    X = X.select_dtypes(['int'])\n    mi_scores = mutual_info_classif(X, y, random_state=1)\n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title('Mutual Information Scores')","e0091577":"df = get_data()\n#df = find_and_drop_nans(df)\n# df = encode_values(df)\n\nX = df. drop('HeartDisease', axis = 'columns')\ny = df['HeartDisease']\nmi_scores = make_mi_scores(X, y)\n\n# Show Mutual Information (MI) score plot\nplt.figure(dpi=120, figsize=(4, 4))\nsns.set(font_scale=1)\nplot_mi_scores(mi_scores)","6cd30e21":"Now we can run the above\n","756a2b74":"to do\n\n- try and get the bloody tree to work\n- go through ruth's bank classification model and do some of the same steps \n- check feature importance\n- utility score\n- hyper parameter tuning\n- try and shw","d5b15d3e":"# Now we have the baseline we can do a bit more exploration in to the data","a41f4f10":"COLUMN INFO:\nAge: age of the patient [years]\nSex: sex of the patient [M: Male, F: Female]\nChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\nRestingBP: resting blood pressure [mm Hg]\nCholesterol: serum cholesterol [mm\/dl]\nFastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\nRestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\nMaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\nExerciseAngina: exercise-induced angina [Y: Yes, N: No]\nOldpeak: oldpeak = ST [Numeric value measured in depression]\nST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\nHeartDisease: output class [1: heart disease, 0: Normal]","514ad1d5":"Observations about the above\n- is there gender bias, 3 times as many men as women?\n- issues to investigat\n- super low resting heartbeats","2f44ac66":"lets start by checking the distribution of the target variable","55a3baae":"function for producing MI Scores","7808403b":"do people really have such high cholestrol or is the data mixed up?"}}