{"cell_type":{"950950ab":"code","f2becf7d":"code","2a311b45":"code","1113ad13":"code","e3340355":"code","2efc1497":"code","1342b506":"code","a929bbda":"code","1411a520":"code","17020bb6":"code","3f46cab1":"code","31de1a04":"code","1588f380":"code","342ed19c":"code","35dea530":"code","9a22e92a":"code","713023ec":"code","bd4c8726":"code","720d8cdc":"code","ef3f727f":"code","e61ee2e6":"code","68364a14":"code","baa6bb97":"code","9be4cb67":"code","991eb755":"code","650a3d87":"code","1cebd0cf":"code","d9880447":"code","0e5b32fa":"code","ba1fbe8f":"code","10783188":"code","e753d01e":"code","2c7af142":"code","70d02cef":"code","0d6057f1":"code","5d186f14":"code","a07f1f7b":"code","f75eb7dc":"code","62c48893":"code","9ed1341b":"code","43b71f5f":"code","cd2d06fa":"code","ebdd4fd2":"code","abb2f754":"code","4645ec30":"code","c2ab79f2":"code","cd3381db":"code","9c273404":"code","b16dd6ca":"code","9d265216":"code","d4d44382":"code","d0902bd3":"code","396b8c8e":"code","dfab5da5":"code","bcc4ffb5":"code","de12891f":"code","26f30e0d":"code","ef276614":"code","3c896704":"code","d4b798b8":"code","75bc7e9f":"code","4e26a88a":"code","bce848e9":"code","72b0cfdb":"code","d6caf11e":"code","8f82ee7d":"code","804d8888":"code","7bdbeef6":"code","e55a42b0":"code","46d50131":"code","3920dd96":"code","68d15cce":"code","57d09e04":"code","45ebf877":"code","d6b662f4":"code","9f817931":"code","fed3ab43":"markdown","e0253766":"markdown","26355b46":"markdown","e32f5ad1":"markdown","7da6d33e":"markdown","bf487077":"markdown","8d8bebbd":"markdown","e46d3040":"markdown","59c02b64":"markdown","31e811d3":"markdown","e18a2498":"markdown","aedcc102":"markdown","d446f8c3":"markdown","f41c5e31":"markdown","4a5eaa6f":"markdown","9367e185":"markdown","3ecbc1e4":"markdown","e8e22814":"markdown","477e6349":"markdown","8c0a5e7e":"markdown","e1105d55":"markdown","8715ac26":"markdown","b8ca9fa2":"markdown","feb18e39":"markdown","48136eea":"markdown","c7301ac5":"markdown","b5856497":"markdown","721af5ce":"markdown","9a264c90":"markdown","e3501614":"markdown","c7205bf8":"markdown","795abcc9":"markdown","8c1d447a":"markdown","2ad04193":"markdown"},"source":{"950950ab":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#pd.set_option('display.max_columns', None)","f2becf7d":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","2a311b45":"train.shape, test.shape","1113ad13":"train.head()","e3340355":"test.head()","2efc1497":"train.info()","1342b506":"test.info()","a929bbda":"train.describe()","1411a520":"test.describe()","17020bb6":"train.drop(\"Id\",1,inplace=True)\n\ntest_id = test.Id\ntest.drop(\"Id\",1,inplace=True)","3f46cab1":"plt.figure(figsize=(15,15))\n\ncorrMatrix = train.corr()\nmask = np.triu(corrMatrix)\nsns.heatmap(corrMatrix,\n            annot=True,\n            fmt='.1f',\n            cmap='coolwarm',            \n            mask=mask,\n            linewidths=1,\n            cbar=False)\nplt.show()","31de1a04":"corr_dict=corrMatrix['SalePrice'].sort_values(ascending=False).to_dict()\n\nimportant_columns=[]\nfor key,value in corr_dict.items():\n    if ((value>0.1) & (value<0.8)) | (value<=-0.1):\n        important_columns.append(key)\n\nimportant_columns","1588f380":"# Let's first find out features with null values\nhousing=pd.concat([train,test],axis=0,sort=False)\n\nnull_cols = []\nfor feature in housing.drop(\"SalePrice\",1).columns:\n    if housing[feature].isnull().sum() > 0:\n        print(feature)\n        null_cols.append(feature)","342ed19c":"# lets check features with null values\nobjs = []\nfor col in null_cols:\n    print(\"Inspecting column:\",col)\n    print(\"no. of null values in training dataset:\", train[col].isnull().sum())\n    print(\"no. of null values in test     dataset:\", test[col].isnull().sum())\n    print(\"Different Values and its Frequencies - Training Data:\")\n    print(train[col].value_counts(),\"\\n\")\n    \n    \n    # creat a list of object type variables    \n    if train[col].dtype == \"object\":\n        objs.append(col)","35dea530":"# Feature engineering and filling null values\n#train_test['LotFrontage'] = train_test['LotFrontage'].fillna(train_test.groupby('1stFlrSF')['LotFrontage'].transform('mean'))\n#housing['LotFrontage'].interpolate(method='linear',inplace=True)\n#housing['LotFrontage']=housing['LotFrontage'].astype(int)\n\n# Set Fireplace quality to \"Nothing\" in case there are no fireplaces in the house\nhousing.loc[housing['Fireplaces']==0,'FireplaceQu']='Nothing'\n\nhousing['MasVnrArea'] = housing['MasVnrArea'].fillna(housing.groupby('MasVnrType')['MasVnrArea'].transform('mean'))\nhousing['MasVnrArea'].interpolate(method='linear',inplace=True)\n#housing['MasVnrArea']=housing['MasVnrArea'].astype(int)\n\nhousing[\"Fence\"] = housing[\"Fence\"].fillna(\"None\")\nhousing[\"FireplaceQu\"] = housing[\"FireplaceQu\"].fillna(\"None\")\nhousing[\"Alley\"] = housing[\"Alley\"].fillna(\"None\")\nhousing[\"PoolQC\"] = housing[\"PoolQC\"].fillna(\"None\")\nhousing[\"MiscFeature\"] = housing[\"MiscFeature\"].fillna(\"None\")\n\nhousing.loc[housing['BsmtFinSF1']==0,'BsmtFinType1']='Unf'\nhousing.loc[housing['BsmtFinSF2']==0,'BsmtQual']='TA'\n\nhousing['YrBltRmd']=housing['YearBuilt']+housing['YearRemodAdd']\n\nhousing['Total_Square_Feet'] = (housing['BsmtFinSF1'] + housing['BsmtFinSF2'] + housing['1stFlrSF'] + housing['2ndFlrSF'] + housing['TotalBsmtSF'])\nhousing['Total_Bath'] = (housing['FullBath'] + (0.5 * housing['HalfBath']) + housing['BsmtFullBath'] + (0.5 * housing['BsmtHalfBath']))\nhousing['Total_Porch_Area'] = (housing['OpenPorchSF'] + housing['3SsnPorch'] + housing['EnclosedPorch'] + housing['ScreenPorch'] + housing['WoodDeckSF'])\n\n\nhousing['exists_pool'] = housing['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nhousing['exists_garage'] = housing['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nhousing['exists_fireplace'] = housing['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\nhousing['exists_bsmt'] = housing['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nhousing['old_house'] = housing['YearBuilt'].apply(lambda x: 1 if x <1990 else 0)\n\n# object type feature will be treated by \"forward fill\"\nfor col in objs:\n    housing[str(col)]=housing[str(col)].fillna(method='ffill')","9a22e92a":"# check if any null values are still present\n\nnull_cols = []\nfor feature in housing.drop(\"SalePrice\",1).columns:\n    if housing[feature].isnull().sum() > 0:\n        print(feature)\n        null_cols.append(feature)\n\nprint(\"No. of columns with null values:\",len(null_cols))","713023ec":"# helper function to handle categorical variables\nfrom sklearn.preprocessing import LabelEncoder\n\ncolumns = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', 'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', 'YrSold', 'MoSold', 'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope', 'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond')\n\n\nfor col in columns:\n    lbl_enc = LabelEncoder() \n    lbl_enc.fit(list(housing[col].values)) \n    housing[col] = lbl_enc.transform(list(housing[col].values))","bd4c8726":"housing.shape","720d8cdc":"from scipy.stats import skew\n\nnumeric_features = housing.dtypes[housing.dtypes != \"object\"].index\nskewed_features = housing[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False)\nprint(skewed_features)","ef3f727f":"# display features with high skewness\nhigh_skewness = skewed_features[abs(skewed_features) > 0.9]\nskewed_features = high_skewness.index\n\nprint(high_skewness)\nprint('\\nVariables with high skewness: \\n\\n',skewed_features)","e61ee2e6":"from scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom scipy.stats import boxcox\nfrom scipy.special import inv_boxcox\n\nfor feature in skewed_features:\n    housing[feature] = boxcox1p(housing[feature], boxcox_normmax(housing[feature] + 1))\n","68364a14":"housing.shape","baa6bb97":"# create dummy variables for the remaining categorical variables\nhousing = pd.get_dummies(housing,dtype=\"int8\")\nhousing.shape","9be4cb67":"train=housing[0:1460]\ntest=housing[1460:2919]","991eb755":"train.shape,test.shape","650a3d87":"# numeric variables with null values will be treated here\ntrain.interpolate(method='linear',inplace=True)\ntest.interpolate(method='linear',inplace=True)","1cebd0cf":"# check if any null values are still present\n\nnull_cols = []\nfor feature in housing.drop(\"SalePrice\",1).columns:\n    if train[feature].isnull().sum() > 0:\n        print(feature)\n        null_cols.append(feature)\n    if test[feature].isnull().sum() > 0:\n        print(feature)\n        null_cols.append(feature)\n\nprint(\"No. of columns with null values:\",len(null_cols))","d9880447":"corr_new_train=train.corr()\nplt.figure(figsize=(5,15))\nsns.heatmap(corr_new_train[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).head(30),annot_kws={\"size\": 16},vmin=-1, cmap='PiYG', annot=True)\nsns.set(font_scale=2)","0e5b32fa":"corr_dict2=corr_new_train['SalePrice'].sort_values(ascending=False).to_dict()\ncorr_dict2","ba1fbe8f":"# columns which are highly correlated to saleprice\nbest_columns=[]\nfor key,value in corr_dict2.items():\n    if ((value>=0.3175) & (value<0.9)) | (value<=-0.315):\n        best_columns.append(key)\nbest_columns","10783188":"# boxplots for the best columns\nplt.figure(figsize=(25,10))\nsns.set(font_scale=1.4)\ntrain.boxplot(column=best_columns)\nplt.xticks(weight='bold',rotation=90)\n","e753d01e":"plt.figure(figsize=(10,8))\nsns.set(font_scale=1.2)\nsns.distplot(train['SalePrice'],color='violet')\nplt.xlabel('SalePrice',fontsize=20)\nprint('Skew Dist:',train['SalePrice'].skew())\nprint('Kurtosis Dist:',train['SalePrice'].kurt())","2c7af142":"# lets take log of saleprice and check its skewness\ntrain['SalePrice_Log1p'] = np.log1p(train.SalePrice)","70d02cef":"print(min(train['SalePrice_Log1p']))\nprint(max(train['SalePrice_Log1p']))","0d6057f1":"plt.figure(figsize=(10,8))\nsns.set(font_scale=1.2)\nsns.distplot(train['SalePrice_Log1p'],color='indigo')\nplt.xlabel('SalePrice_Log1p',fontsize=20)\nprint('Skew Dist:',train['SalePrice_Log1p'].skew())\nprint('Kurtosis Dist:',train['SalePrice_Log1p'].kurt())","5d186f14":"#  Apply RobustScalar on training dataset\n\n# Scale features using statistics that are robust to outliers.\n# Robust Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range).\n# The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).\nrbst_scaler=RobustScaler()\ntrain_rbst=rbst_scaler.fit_transform(train)","a07f1f7b":"# apply PCA and plot a graph to show the variance explained by principal components\nplt.style.use('ggplot')\nplt.figure(figsize=(20,12))\npca=PCA(50).fit(train_rbst)\nplt.plot(pca.explained_variance_ratio_.cumsum())\nplt.xticks(np.arange(0, 50, 1))\nplt.xlabel('Number of components',fontweight='bold',size=14)\nplt.ylabel('Explanined variance ratio for number of components',fontweight='bold',size=14)\n\n# We will use 3 PCs to transform the dataset created in step 1\ntrain_pca=PCA(3).fit_transform(train_rbst)","f75eb7dc":"# Apply NearestNeighbors to find out appropriate epsilon value\n\n# We will calculate the distance from each point to its closest neighbour using the NearestNeighbors. \n# The point itself is included in n_neighbors. \nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(train)\n\n# The kneighbors method returns two arrays, one which contains the distance to the closest n_neighbors points and the other which contains the index for each of those points.\ndistances, indices = nbrs.kneighbors(train)\n\n# Next, we sort and plot results.\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.figure(figsize=(15,15))\nplt.plot(distances)\n\n# The optimal value for epsilon will be found at the point of maximum curvature.","62c48893":"# Apply DBSCAN with epsilon value of 1400 to form clusters\ndbscan = DBSCAN(eps=1400, min_samples=20).fit(train_pca)\ncore_samples_mask = np.zeros_like(dbscan.labels_, dtype=bool)\ncore_samples_mask[dbscan.core_sample_indices_] = True\n\nprint(\"Clusters formed:\",set(dbscan.labels_))\n\nlabels=dbscan.labels_\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nprint(\"\\nNumber of clusters except cluster\", \"-1\", \"which represents outliers \")","9ed1341b":"# plot the clusters formed\nunique_labels = set(labels)\nplt.figure(figsize=(12,12))\ncolors = [plt.cm.prism(each)  for each in np.linspace(0, 1, len(unique_labels))]\nfor k, col in zip(unique_labels, colors):\n    if k == -1:\n        # Black used for noise.\n        col = [0, 0, 0, 1]\n\n    class_member_mask = (labels == k)\n    \n    xy = train_pca[class_member_mask & core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=14)\n\n    xy = train_pca[class_member_mask & ~core_samples_mask]\n    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n             markeredgecolor='k', markersize=6)\n\nplt.title('Estimated number of clusters: %d' % n_clusters_)\nplt.show()","43b71f5f":"labels=pd.DataFrame(labels,columns=['Classes'])\nprint(labels[labels['Classes']==-1])","cd2d06fa":"train=pd.concat([train,labels],axis=1)","ebdd4fd2":"train[train.Classes==-1]","abb2f754":"train.drop([197,810,1170,1182,1298,1386,1423],axis=0,inplace=True)","4645ec30":"plt.style.use('dark_background')\nfig, axes = plt.subplots(18, 2,figsize=(20,80))\nfig.subplots_adjust(hspace=0.6)\ncolors=[plt.cm.prism_r(each) for each in np.linspace(0, 1, len(best_columns))]\nfor i,ax,color in zip(best_columns,axes.flatten(),colors):\n    sns.regplot(x=train[i], y=train[\"SalePrice\"], fit_reg=True,marker='o',scatter_kws={'s':50,'alpha':0.8},color=color,ax=ax)\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n    ax.set_yticks(np.arange(0,900001,100000))\n    ax.set_title('SalePrice'+' - '+str(i),color=color,fontweight='bold',size=20)","c2ab79f2":"plt.style.use('ggplot')\nfig, axes = plt.subplots(18, 2,figsize=(20,60))\nfig.subplots_adjust(hspace=0.8)\nsns.set(font_scale=1.2)\ncolors=[plt.cm.prism_r(each) for each in np.linspace(0, 1, len(best_columns))]\nfor i,ax,color in zip(best_columns,axes.flatten(),colors):\n    sns.regplot(x=train[i], y=train[\"SalePrice_Log1p\"], fit_reg=True,marker='o',scatter_kws={'s':50,'alpha':0.7},color=color,ax=ax)\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice_Log1p',fontsize=12)\n    ax.set_title('SalePrice_Log1p'+' - '+str(i),color=color,fontweight='bold',size=20)","cd3381db":"train = train[train.GarageArea * train.GarageCars < 3700]\ntrain = train[(train.FullBath + (train.HalfBath*0.5) + train.BsmtFullBath + (train.BsmtHalfBath*0.5))<5]","9c273404":"train.shape,test.shape","b16dd6ca":"plt.style.use('dark_background')\ncorr1_new_train=train.corr()\nplt.figure(figsize=(5,15))\nsns.heatmap(corr1_new_train[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).head(25),annot_kws={\"size\": 16},vmin=-1, cmap='PiYG', annot=True)\nsns.set(font_scale=2)","9d265216":"test.drop(\"SalePrice\",1,inplace=True)","d4d44382":"train.shape,test.shape","d0902bd3":"X=train.drop(['SalePrice','SalePrice_Log1p','Classes'],axis=1)\ny=train.SalePrice_Log1p","396b8c8e":"def overfit_reducer(df):\n\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > 99.9:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\noverfitted_features = overfit_reducer(X)","dfab5da5":"train.shape,test.shape","bcc4ffb5":"X.drop(overfitted_features,axis=1,inplace=True)\ntest.drop(overfitted_features,axis=1,inplace=True)\nprint('X.shape',X.shape)\nprint('test.shape',test.shape)","de12891f":"from sklearn.preprocessing import StandardScaler,PowerTransformer\n\nstd_scaler=StandardScaler()\nrbst_scaler=RobustScaler()\npower_transformer=PowerTransformer()\n\nX_std=std_scaler.fit_transform(X)\nX_rbst=rbst_scaler.fit_transform(X)\nX_pwr=power_transformer.fit_transform(X)\n\ntest_std=std_scaler.transform(test)\ntest_rbst=rbst_scaler.transform(test)\ntest_pwr=power_transformer.transform(test)","26f30e0d":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X_std,y,test_size=0.02,random_state=52)\nprint('X_train Shape :',X_train.shape)\nprint('X_test Shape :',X_test.shape)\nprint('y_train Shape :',y_train.shape)\nprint('y_test Shape :',y_test.shape)","ef276614":"from sklearn.ensemble import RandomForestRegressor\nrf_base = RandomForestRegressor()\nrf_base.fit(X_train,y_train)","3c896704":"def rmse(predictions, targets):\n    return np.sqrt(((predictions - targets) ** 2).mean())","d4b798b8":"y_pred = rf_base.predict(X_test)\nRMSE_base = rmse(y_pred,y_test)\nRMSE_base","75bc7e9f":"# Look at parameters used by our current forest\nfrom pprint import pprint\n\nprint('Parameters currently in use:\\n')\npprint(rf_base.get_params())","4e26a88a":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","bce848e9":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","72b0cfdb":"rf_random.best_params_","d6caf11e":"# build model with the best parameters from random search \nbest_random = rf_random.best_estimator_\nbest_random.fit(X_train,y_train)\n\ny_pred = best_random.predict(X_test)\nRMSE_random = rmse(y_pred,y_test)\nRMSE_random","8f82ee7d":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [None,10,20],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [1,3,5],\n    'min_samples_split': [2,4,6],\n    'n_estimators': [200,300,400]\n}\n# Create a based model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","804d8888":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","7bdbeef6":"# build model with the best parameters from random search \nbest_grid = grid_search.best_estimator_\nbest_grid.fit(X_train,y_train)\n\ny_pred = best_grid.predict(X_test)\nRMSE_grid = rmse(y_pred,y_test)\nRMSE_grid","e55a42b0":"y_pred_rf = best_grid.predict(test_std)\ny_pred_rf = np.floor(np.expm1(y_pred_rf))","46d50131":"alphas=[1e-9,1e-8,1e-7,1e-6]\n\nkfolds = KFold(n_splits=8, shuffle=True, random_state=42)\n\nlassocv_reg= make_pipeline(LassoCV(alphas=alphas, cv=kfolds))\nlassocv_reg.fit(X_train, y_train)\ny_pred = lassocv_reg.predict(X_test)\n\nRMSE_lasoo = rmse(y_pred,y_test)\nRMSE_lasoo","3920dd96":"y_pred_lasso = lassocv_reg.predict(test_std)\ny_pred_lasso = np.floor(np.expm1(y_pred_lasso))\ny_pred_lasso","68d15cce":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\nridge_reg= make_pipeline(RidgeCV(alphas=alphas, cv=kfolds))\nridge_reg.fit(X_train, y_train)\ny_pred_ridge =ridge_reg.predict(X_test)\n\nRMSE_ridge = rmse(y_pred,y_test)\nRMSE_ridge","57d09e04":"y_pred_ridge = ridge_reg.predict(test_std)\ny_pred_ridge = np.floor(np.expm1(y_pred_ridge))\ny_pred_ridge","45ebf877":"kfolds = KFold(n_splits=8, shuffle=True, random_state=42)\n\nalphas=[0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006]\nl1ratio=[0.87, 0.9,0.92, 0.95,0.97, 0.99, 1]\n\nelastic_reg= make_pipeline(ElasticNetCV(alphas=alphas, cv=kfolds, l1_ratio=l1ratio))\nelastic_reg.fit(X_train, y_train)\ny_pred = elastic_reg.predict(X_test)\n\nelastic_ridge = rmse(y_pred,y_test)\nelastic_ridge","d6b662f4":"y_pred_elastic = elastic_reg.predict(test_std)\ny_pred_elastic = np.floor(np.expm1(y_pred_elastic))\ny_pred_elastic","9f817931":"# lets prepare for the prediction submission\nsub = pd.DataFrame()\nsub['Id'] = test_id\ny_pred=(y_pred_lasso*0.25)+(y_pred_elastic*0.5)+(y_pred_ridge*0.25)\nsub['SalePrice'] = y_pred\nsub.to_csv('submission.csv',index=False)","fed3ab43":"The most important arguments in RandomizedSearchCV are n_iter, which controls the number of different combinations to try, and cv which is the number of folds to use for cross validation (we use 100 and 3 respectively). More iterations will cover a wider search space and more cv folds reduces the chances of overfitting, but raising each will increase the run time. Machine learning is a field of trade-offs, and performance vs time is one of the most fundamental.\n\nWe can view the best parameters from fitting the random search:","e0253766":"### Step # 3 - Find optimum value of epsilon\nRead a great blog [here](https:\/\/towardsdatascience.com\/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc#:~:text=In%20layman's%20terms%2C%20we%20find,and%20select%20that%20as%20epsilon.) to learn how to find out the optimum value of epsilon.\n\nWe find a suitable value for epsilon by calculating the distance to the nearest n points for each point, sorting and plotting the results. Then we look to see where the change is most pronounced (think of the angle between your arm and forearm) and select that as epsilon.","26355b46":"# Acknowledgement\n\nThis notebook is highly inspired from [This](https:\/\/www.kaggle.com\/darkside92\/detailed-examination-for-house-price-top-10) notebook!","e32f5ad1":"### Step # 1 - Apply RobustScalar on training dataset","7da6d33e":"### Check for skewness in numeric features","bf487077":"Meaning of each hyper parameter:\n\n* n_estimators = number of trees in the foreset\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min number of data points placed in a node before the node is split\n* min_samples_leaf = min number of data points allowed in a leaf node\n* bootstrap = method for sampling data points (with or without replacement)","8d8bebbd":"# ElasticNetCV","e46d3040":"From these results, we should be able to narrow the range of values for each hyperparameter.","59c02b64":"# Model Building","31e811d3":"### Random Search Cross Validation \n\nUsing Scikit-Learn\u2019s RandomizedSearchCV method, we can define a grid of hyperparameter ranges, and randomly sample from the grid, performing K-Fold CV with each combination of values.","e18a2498":"### Steps to apply DBSCAN\n\nThe curse of dimensionality has many different facets and one of them is that in high-dimensional domain finding relevant neighbours becomes hard. That is due to qualitative as well as quantitative reasons.\n\nQualitative, it becomes difficult to have a reasonable metric of similarity. As different dimensions are not necessarily comparable, what constitutes an appropriate \"distance\" become ill-defined.\n\nQuantitative, even when a distance is well defined, the expected Euclidean distance between any two neighbouring points gets increasingly large. In a high dimensional problem, the data are sparse and as a result local neighbourhoods contain very few points. That is the reason why high-dimensional density estimation is a notoriously hard problem. As a rule of thump, density estimation tends to become quite difficult above 4-5 dimensions. \n\nReturning to DBSCAN: In DBSCAN, through the concepts of eps and neighbourhood we try to define regions of \"high density\". This task is prone to provide spurious results when the number of dimensions in a sample is high. Even if we assume a relatively well-behaved sample uniformly distributed in [0,1]d, the average distance between points increases by \u221ad. \n\nPCA is going to help us reducing the dimensionality and creating accurate clusters. So, we are going to apply PCA before applying DBSCAN.\n\nBut, PCA is again prone to outliers, so we will apply Robustscaler on training dataset before applying PCA.\n\nIn short, this is what we are going to do:\n\n1. Apply RobustScalar on training dataset, call transformed dataset as \"train_rbst\"\n\n2. Apply PCA on \"train_rbst\", check how these components explain the variance in the dataset and pick the appropriate no. of princial components. dont go beyond 4 PCs. call the transformed dataset(with selected PCs) as \"train_pca\".\n\n3. find out appropriate epsilon value by applying NearestNeighbor on \"train\" dataset\n\n4. Apply DBSCAN on \"train_pca\" using the epsilon value found in step 3.\n\n5. And finally remove outliers (records with classes having a value of -1)\n\nLet's implement it.","aedcc102":"Log of SalePrice seems to be normally distributed","d446f8c3":"On each iteration, the algorithm will choose a difference combination of the features. Altogether, there are 2 * 12 * 2 * 3 * 3 * 10 = 4320 settings! However, the benefit of a random search is that we are not trying every combination, but selecting at random to sample a wide range of values.","f41c5e31":"### Check correlation of SalePrice with other features","4a5eaa6f":"# Ensemble","9367e185":"### Grid Search with Cross Validation\nRandom search allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with GridSearchCV, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define. To use Grid Search, we make another grid based on the best values provided by random search:","3ecbc1e4":"### Evaluate Random Search\nTo determine if random search yielded a better model, we compare the base model with the best random search model.","e8e22814":"There are some outliers, we will need to fix, but befor that we will check if saleprice is also skewed!","477e6349":"# RidgeCV","8c0a5e7e":"Correlation increased marginally after removing outliers","e1105d55":"# Feature Engineering and Null Values Treatment on combined dataset","8715ac26":"### Step # 4 - Apply DBSCAN with epsilon value of 1400 to form clusters","b8ca9fa2":"# Null Values Treatment","feb18e39":"### Convert categorical variables to numeric variables","48136eea":"### Step # 2 - Apply PCA and plot a graph to show the variance explained by principal components","c7301ac5":"# Random Forest - Baseline","b5856497":"To use RandomizedSearchCV, we first need to create a parameter grid to sample from during fitting:","721af5ce":"# LassoCV","9a264c90":"# Heat Map","e3501614":"# Handling Outliers using DBSACN\n\nDensity-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander and Xiaowei Xu in 1996. \n\nIt is a density-based clustering algorithm: given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). \n\nDBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.\n\nIn 2014, the algorithm was awarded the test of time award (an award given to algorithms which have received substantial attention in theory and practice) at the leading data mining conference, KDD.\n\nDBSCAN defines clusters based on following parameters: \n* eps: Two points are considered neighbors if the distance between the two points is below the threshold epsilon.\n* min_samples: The minimum number of neighbors a given point should have in order to be classified as a core point. It\u2019s important to note that the point itself is included in the minimum number of samples.\n* metric: The metric to use when calculating distance between instances in a feature array (i.e. euclidean distance).\n\nDBSCAN requires the user to input the maximum distance apart each point of data can be to be considered part of a cluster and how many data points it takes to form a cluster. To find this maximum distance we will need to do some work, stay tuned!\n\nSince DBSCAN creates clusters based on epsilon and the number of neighbors each point has, it can find clusters of any shape. DBSCAN works best when the clusters are of the same density (distance between points). \n\nWhen clusters of varying density are present, this can make it hard for DBSCAN to identify the clusters.","c7205bf8":"### Step # 5 - Finding & removing the outliers","795abcc9":"We are done with Feature Enggineering, lets break the combined dataset!","8c1d447a":"Let's check the features which are highly correlated with the target variable","2ad04193":"# Hyper Parameter Tuning"}}