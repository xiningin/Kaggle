{"cell_type":{"f4e29803":"code","ffc5227b":"code","d6d47a38":"code","894c6c98":"code","37ea0064":"code","ac2d0671":"code","237983fc":"code","fd0ab8be":"code","458a0534":"code","8d06eb13":"code","35c9a04b":"code","4b2969b7":"code","059048a2":"code","a6c0585a":"code","011c070f":"code","0184bfaa":"code","5e10b421":"markdown","7ad16935":"markdown","4e3265f7":"markdown","a15a068e":"markdown","d2a12058":"markdown"},"source":{"f4e29803":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ffc5227b":"!pip install ..\/input\/sacremoses\/sacremoses-master\/\n!pip install ..\/input\/transformers\/transformers-master\/","d6d47a38":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport gc\nimport time\nimport random\nimport os\nimport torch\nfrom scipy.stats import spearmanr\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom math import floor, ceil\nfrom transformers import AdamW,BertForSequenceClassification, get_cosine_with_hard_restarts_schedule_with_warmup\n","894c6c98":"sample_submission = pd.read_csv(\"..\/input\/google-quest-challenge\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/google-quest-challenge\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/google-quest-challenge\/train.csv\")\n\nMAX_SEQUENCE_LENGTH = 512","37ea0064":"print('train shape =', train.shape)\nprint('test shape =', test.shape)\n\noutput_categories = list(train.columns[11:])\ninput_categories = list(train.columns[[1,2,5]])\nprint('\\noutput categories:\\n\\t', output_categories)\nprint('\\ninput categories:\\n\\t', input_categories)","ac2d0671":"'''\ncredit to @akensert\n'''\n\ndef _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\n## head+tail version\ndef _trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    \n    t_len = len(t)\n    q_len = len(q)\n    a_len = len(a)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len > t_len:\n            t_new_len = t_len\n            a_max_len = a_max_len + floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + ceil((t_max_len - t_len)\/2)\n        else:\n            t_new_len = t_max_len\n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len\n            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n        \n        if t_len > t_new_len:\n            ind1 = floor(t_new_len\/2)\n            ind2 = ceil(t_new_len\/2)\n            t = t[:ind1]+t[-ind2:]\n        else:\n            t = t[:t_new_len]\n\n        if q_len > q_new_len:\n            ind1 = floor(q_new_len\/2)\n            ind2 = ceil(q_new_len\/2)\n            q = q[:ind1]+q[-ind2:]\n        else:\n            q = q[:q_new_len]\n\n        if a_len > a_new_len:\n            ind1 = floor(a_new_len\/2)\n            ind2 = ceil(a_new_len\/2)\n            a = a[:ind1]+a[-ind2:]\n        else:\n            a = a[:a_new_len]\n    \n    return t, q, a\n\ndef _convert_to_bert_inputs(title, question, answer, tokenizer, max_sequence_length):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + answer + [\"[SEP]\"]\n\n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in tqdm(df[columns].iterrows()):\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        t, q, a = _trim_input(t, q, a, max_sequence_length)\n\n        ids, masks, segments = _convert_to_bert_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])","237983fc":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","fd0ab8be":"class TextDataset(torch.utils.data.TensorDataset):\n\n    def __init__(self, x_train, idxs, targets=None):\n        self.input_ids = x_train[0][idxs]\n        self.input_masks = x_train[1][idxs]\n        self.input_segments = x_train[2][idxs]\n        self.targets = targets[idxs] if targets is not None else np.zeros((x_train[0].shape[0], 30))\n\n    def __getitem__(self, idx):\n#         x_train = self.x_train[idx]\n        input_ids =  self.input_ids[idx]\n        input_masks = self.input_masks[idx]\n        input_segments = self.input_segments[idx]\n\n        target = self.targets[idx]\n\n        return input_ids, input_masks, input_segments, target\n\n    def __len__(self):\n        return len(self.input_ids)","458a0534":"from transformers import BertTokenizer,BertConfig","8d06eb13":"# pretrained_weights = '\/kaggle\/input\/bertbaseuncased\/bert-base-uncased-vocab.txt'\ntokenizer = BertTokenizer.from_pretrained('..\/input\/bertbaseuncased\/bert-base-uncased-vocab.txt')","35c9a04b":"x_train = compute_input_arays(train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\ny_train = compute_output_arrays(train, output_categories)\nx_test = compute_input_arays(test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)","4b2969b7":"bert_config = BertConfig.from_pretrained('..\/input\/bertbaseuncased\/bert-base-uncased-config.json') \nbert_config.num_labels = 30","059048a2":"import torch\nfrom torch import nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom transformers import BertPreTrainedModel,BertModel\n\n\nclass CustomizedBert(BertPreTrainedModel):\n    r\"\"\"\n        **labels**: (`optional`) ``torch.LongTensor`` of shape ``(batch_size,)``:\n            Labels for computing the sequence classification\/regression loss.\n            Indices should be in ``[0, ..., config.num_labels - 1]``.\n            If ``config.num_labels == 1`` a regression loss is computed (Mean-Square loss),\n            If ``config.num_labels > 1`` a classification loss is computed (Cross-Entropy).\n\n    Outputs: `Tuple` comprising various elements depending on the configuration (config) and inputs:\n        **loss**: (`optional`, returned when ``labels`` is provided) ``torch.FloatTensor`` of shape ``(1,)``:\n            Classification (or regression if config.num_labels==1) loss.\n        **logits**: ``torch.FloatTensor`` of shape ``(batch_size, config.num_labels)``\n            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n        **hidden_states**: (`optional`, returned when ``config.output_hidden_states=True``)\n            list of ``torch.FloatTensor`` (one for the output of each layer + the output of the embeddings)\n            of shape ``(batch_size, sequence_length, hidden_size)``:\n            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n        **attentions**: (`optional`, returned when ``config.output_attentions=True``)\n            list of ``torch.FloatTensor`` (one for each layer) of shape ``(batch_size, num_heads, sequence_length, sequence_length)``:\n            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n\n    Examples::\n\n        tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n        input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n        labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n        outputs = model(input_ids, labels=labels)\n        loss, logits = outputs[:2]\n\n    \"\"\"\n\n    def __init__(self, config):\n        super(CustomizedBert, self).__init__(config)\n        self.num_labels = config.num_labels\n\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size*2, self.config.num_labels)\n\n        self.init_weights()\n    def forward(\n        self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None,\n        labels=None,\n    ):\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n        )\n\n        ## mean max pooling and concatenate to a vector\n        \n        avg_pool = torch.mean(outputs[0], 1)\n        max_pool, _ = torch.max(outputs[0], 1)\n        pooled_output = torch.cat((max_pool, avg_pool), 1)\n\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        if labels is not None:\n            if self.num_labels == 1:\n                #  We are doing regression\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1), labels.view(-1))\n            else:\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            outputs = (loss,) + outputs\n\n        return outputs  # (loss), logits, (hidden_states), (attentions)","a6c0585a":"import numpy as np\n\nclass callback:\n    def __init__(self):\n        self.score = list()\n        self.model = list()\n        self.data = list()\n    \n    def put(self, model,data, score):\n        self.score.append(score)\n        self.model.append(model)\n        self.data.append(data)\n\n    def get_model(self):\n        ind = np.argmin(self.score)\n        return self.model[ind]\n    def get_data(self):\n        ind = np.argmin(self.score)\n        return self.data[ind]","011c070f":"NFOLDS = 5\nBATCH_SIZE = 4\nEPOCHS = 4\nSEED = 8516\nnum_warmup_steps = 100\nlr = 5e-5\n\n\ngradient_accumulation_steps = 1\nseed_everything(SEED)\n\nmodel_list = list()\n\n\ny_oof = np.zeros((len(train), 30))\ntest_pred = np.zeros((len(test), 30))\n\nkf = KFold(n_splits=NFOLDS, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(TextDataset(x_test, test.index),batch_size=BATCH_SIZE, shuffle=False)\n\n\nfor i, (train_idx, valid_idx) in enumerate(kf.split(x_train[0])):\n    \n    \n    print(f'fold {i+1}')\n    gc.collect()\n    \n    ## loader\n    train_loader = torch.utils.data.DataLoader(TextDataset(x_train, train_idx, y_train),batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = torch.utils.data.DataLoader(TextDataset(x_train, valid_idx, y_train),batch_size=BATCH_SIZE, shuffle=False)\n    \n\n    t_total = len(train_loader)\/\/gradient_accumulation_steps*EPOCHS\n\n\n    net = CustomizedBert.from_pretrained('..\/input\/bertbaseuncased\/', config=bert_config)\n    net.cuda()\n    \n    loss_fn = torch.nn.BCEWithLogitsLoss()\n    optimizer = AdamW(net.parameters(), lr = lr)\n    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=t_total)  # PyTorch scheduler\n\n    cb = callback()\n\n\n    for epoch in range(EPOCHS):  \n\n\n        \n        start_time = time.time()\n        avg_loss = 0.0\n        net.train()\n\n\n        for step, data in enumerate(train_loader):\n\n            # get the inputs\n            input_ids, input_masks, input_segments, labels = data\n\n\n            pred = net(input_ids = input_ids.long().cuda(),\n                             labels = None,\n                             attention_mask = input_masks.cuda(),\n                             token_type_ids = input_segments.long().cuda()\n                            )[0]\n            \n            \n            loss = loss_fn(pred, labels.cuda())\n        \n            avg_loss += loss.item()\n            loss = loss \/ gradient_accumulation_steps\n            loss.backward()\n\n            if (step + 1) % gradient_accumulation_steps == 0:\n\n                # Calling the step function on an Optimizer makes an update to its parameters\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n                \n                \n        avg_val_loss = 0.0\n\n        valid_preds = np.zeros((len(valid_idx), 30))\n        true_label = np.zeros((len(valid_idx), 30))\n        \n        net.eval()\n        for j,data in enumerate(val_loader):\n\n            # get the inputs\n            input_ids, input_masks, input_segments, labels = data\n            pred = net(input_ids = input_ids.long().cuda(),\n                             labels = None,\n                             attention_mask = input_masks.cuda(),\n                             token_type_ids = input_segments.long().cuda()\n                            )[0]\n\n            loss_val = loss_fn(pred, labels.cuda())\n            avg_val_loss += loss_val.item()\n     \n            valid_preds[j * BATCH_SIZE:(j+1) * BATCH_SIZE] = torch.sigmoid(pred).cpu().detach().numpy()\n            true_label[j * BATCH_SIZE:(j+1) * BATCH_SIZE]  = labels\n\n\n        elapsed_time = time.time() - start_time \n\n        score = 0\n        for i in range(30):\n          s = np.nan_to_num(\n                    spearmanr(true_label[:, i], valid_preds[:, i]).correlation \/ 30)\n          score += s\n\n        \n\n        print('Epoch {}\/{} \\t loss={:.4f}\\t val_loss={:.4f}\\t spearmanr={:.4f}\\t time={:.2f}s'.format(epoch+1, EPOCHS, avg_loss\/len(train_loader),avg_val_loss\/len(val_loader),score, elapsed_time))\n\n        cb.put(net,valid_preds,avg_val_loss\/len(val_loader))\n\n\n\n    model_list.append(cb.get_model())\n    y_oof[valid_idx] = cb.get_data()\n\n    \n    result = list()\n    net.eval()\n    with torch.no_grad():\n        for data in test_loader:\n            input_ids, input_masks, input_segments, labels = data\n            y_pred = net(input_ids = input_ids.long().cuda(),\n                                labels = None,\n                                attention_mask = input_masks.cuda(),\n                                token_type_ids = input_segments.long().cuda(),\n                            )[0]\n            result.extend(torch.sigmoid(y_pred).cpu().detach().numpy())\n            \n    test_pred += np.array(result)\/NFOLDS\n\n","0184bfaa":"sample_submission.loc[:, output_categories] = test_pred\nsample_submission.to_csv('submission.csv', index=False)","5e10b421":"## Preprocess\nThe original version come from [Bert-base TF2.0](https:\/\/www.kaggle.com\/akensert\/bert-base-tf2-0-now-huggingface-transformer).\n\nModified `_trim_input` to head + tail parts of texts version.\n","7ad16935":"## Single example about how to train a customized BERT model with PyTorch version","4e3265f7":"## Customized BERT\nUse mean and max pooling of the last hidden state instead of `pooled_output` of BERT to get better performance.\n\nHere is a example about customized BERT modified from [BertForSequenceClassification](https:\/\/huggingface.co\/transformers\/model_doc\/bert.html#transformers.BertForSequenceClassification)","a15a068e":"## Callback class\nSingle object stores BERT model in each epochs.\n\nWhen finish training phase, we use callback object to get the BERT model having minimum validation loss in each folds.","d2a12058":"## Training model"}}