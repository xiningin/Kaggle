{"cell_type":{"367e02a0":"code","254bc793":"code","7f2d77f5":"code","98ed919a":"code","c8456771":"code","029f7cfa":"code","cce11715":"code","415c3478":"code","24c97817":"code","4a9927f3":"code","abab62ef":"code","4ccf6209":"code","503eb674":"code","e63399a0":"code","aa95dc35":"code","78b2c412":"code","726dec21":"code","dc07dbde":"code","942365c1":"code","2e05f1b5":"code","6c94bbaa":"code","a547e12e":"code","1ded4002":"code","5b157b2e":"code","70294c0d":"code","d530f676":"code","526f17ad":"code","38f6b958":"code","54ffbd42":"code","38c44334":"code","309abf42":"code","8a051f8d":"code","6563fc9a":"code","5104dd80":"code","b9372d5a":"code","abcbd12c":"code","3d430f92":"code","ce27d143":"code","08737621":"code","a70cadbc":"code","da7815ae":"code","545dab74":"code","826ecbd9":"code","a3302955":"code","7d9d172f":"code","25bcc580":"code","266c3df4":"code","637958c1":"code","5e59c9dd":"code","edc3d69a":"code","dd0ce9d4":"code","12891423":"code","75848934":"code","26f60e08":"code","2c4d9657":"code","eea1d580":"code","87f7ab0d":"code","ee2df5c1":"code","129b63df":"code","1513d193":"code","cc1f22d5":"code","12c0aa4a":"code","d423a252":"code","de06c158":"code","819fa0ef":"code","6083e920":"code","0893fc02":"code","7c3ca2fe":"code","0a0dcc55":"code","5a204a18":"code","5bc1a7d3":"code","96e36fc5":"code","f40b63f8":"code","7e967444":"code","d41571ac":"code","8572ea98":"code","52850818":"code","03f3a60f":"code","27b24136":"code","8c732ef2":"code","3a3478d1":"code","205e4021":"code","01f2c681":"code","5b6205cb":"code","4c699f06":"code","f1267ca9":"code","ce29b026":"code","62493ffa":"code","80e1c3db":"code","ef655a31":"code","21f8ddbe":"code","b1ba4348":"code","1c9dcc50":"code","04271029":"code","51ca8f54":"code","47275d2f":"code","88354aad":"code","ecf7ff96":"code","fc76130e":"code","811edb93":"code","54c9a864":"code","facb8e91":"code","aa71a1a8":"code","9ac96a6e":"markdown","0c1fd555":"markdown","7a5f9e65":"markdown","8c5dda23":"markdown","6f113233":"markdown","bae4d089":"markdown","2cf4dd73":"markdown","5f81348a":"markdown","697df7d0":"markdown","2e632bf2":"markdown","413b22a1":"markdown","9691220e":"markdown","f7e536d2":"markdown","89751093":"markdown","7ac79a37":"markdown","2d106e06":"markdown","eff2789d":"markdown","a219e4be":"markdown","679151e8":"markdown","b00876de":"markdown","c80ffb79":"markdown","2bf3367a":"markdown","5256e1b4":"markdown","83689694":"markdown","113509b5":"markdown","6bb46e9d":"markdown","acfec72f":"markdown","a8aa0042":"markdown","2f3fe966":"markdown","c455a38d":"markdown","fac88782":"markdown","1cb5ef8c":"markdown","d1a2b303":"markdown","bc4abf9f":"markdown","c1d3e090":"markdown","2e42bffc":"markdown","d5210ed6":"markdown","90d098b1":"markdown","d6e35bcd":"markdown","be896085":"markdown","6fca50db":"markdown","01104d34":"markdown","575dcb2b":"markdown","512bd07e":"markdown","0be3ebab":"markdown","32017679":"markdown"},"source":{"367e02a0":"import numpy as np\nimport torch\nimport tensorflow\nimport pandas as pd\nimport os\nimport json\nimport time\nimport glob\nimport re\nimport sys\nimport collections\nfrom nltk import flatten\nimport dask\nfrom dask import delayed,compute\nimport dask.dataframe as dd\nfrom dask.multiprocessing import get\nfrom tqdm._tqdm_notebook import tqdm_notebook\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\ntqdm_notebook.pandas()","254bc793":"sys.path.insert(0, \"..\/\")\n\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge'\n\ncorona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n                  \"abstract\": [None], \"text_body\": [None]}\ncorona_df = pd.DataFrame.from_dict(corona_features)\n\njson_filenames = glob.glob(f'{root_path}\/**\/*.json', recursive=True)","7f2d77f5":"def return_corona_df(json_filenames, df, source):\n    \n    for file_name in tqdm(json_filenames[20000:50000]):\n\n        row = {}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            doc_id = data['paper_id']\n            row['doc_id'] = doc_id\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n            try:\n                abstract_list = [abst['text'] for abst in data['abstract']]\n                abstract = \"\\n \".join(abstract_list)\n\n                row['abstract'] = abstract\n            except:\n                row['abstract'] = np.nan\n            # And lastly the body of the text. \n            body_list = [bt['text'] for bt in data['body_text']]\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Now just add to the dataframe. \n            \n            if source == 'b':\n                row['source'] = \"BIORXIV\"\n            elif source == \"c\":\n                row['source'] = \"COMMON_USE_SUB\"\n            elif source == \"n\":\n                row['source'] = \"NON_COMMON_USE\"\n            elif source == \"p\":\n                row['source'] = \"PMC_CUSTOM_LICENSE\"\n            \n            df = df.append(row, ignore_index=True)\n            \n            \n    return df\n    \ncorona_df = return_corona_df(json_filenames,corona_df, 'b')","98ed919a":"corona_df.dropna(subset=['text_body'],inplace=True)","c8456771":"corona_df.head()","029f7cfa":"#Install scispcy and spacy and pretrained model enc_core_sci_lg for analysis\n!pip install -U spacy\n!pip install scispacy\n# !pip install https:\/\/med7.s3.eu-west-2.amazonaws.com\/en_core_med7_lg.tar.gz\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","cce11715":"import scispacy\nimport spacy\nimport en_core_sci_lg","415c3478":"## loading en_core_sci_lg model and disabling parser and ner, as these are not going to be used in EDA, disabling these functions from NLP pipeline can sometimes make a big difference and improve loading speed\nnlp = en_core_sci_lg.load(disable=[\"parser\", \"ner\"])\nnlp.max_length = 2000000","24c97817":"# Function for cleaning data by using POS tagging and Lemmatization\ndef clean_text(sentence):\n    if sentence:\n        tokens = [word.lemma_ for word in nlp(str(sentence)) \n                  if not (word.like_num \n                          or word.is_stop\n                          or word.is_punct\n                          or word.is_space\n                          or word.like_url\n                          or word.like_email\n                          or word.is_currency\n                          or word.pos_ =='VBZ' \n                          or word.pos_ =='ADP'\n                          or word.pos_ =='PRON'\n                          or word.pos_ =='AUX'\n\n                         )] \n    else :\n        return np.nan\n    return tokens","4a9927f3":"#cleaning of abstract data\ncorona_df[\"cleaned_abstract_tokens\"] = corona_df['abstract'][22000:25000].progress_apply(lambda x: clean_text(x))\n","abab62ef":"## this function can take take of lot of memory and execution time. uncomment to run and check output of frquency and word cloud\n#cleaning of text body data\n\n#corona_df[\"cleaned_text_body_tokens\"] = corona_df['text_body'].progress_apply(lambda x: clean_text(str(x)))","4ccf6209":"# example for pos tagging\nimport spacy\nfrom spacy import displacy\nnlp = en_core_sci_lg.load()","503eb674":"## Create nlp object of spacy\ndoc = nlp(corona_df[\"title\"].iloc[22000])","e63399a0":"## Render output by using displacy module\ndisplacy.render(doc, style=\"dep\")","aa95dc35":"tokens_df= corona_df[\"cleaned_abstract_tokens\"].dropna() #drop null values\n# Get top 30 tokens based upon frequency in whole corpus\nword_freq_top30 = pd.DataFrame(collections.Counter(flatten(tokens_df.to_list())).most_common(30),columns=['words',\"frequency\"])","78b2c412":"##Plot bar graph of words wrt frequency \nsns.set(style=\"whitegrid\")\nsns.set(rc={'figure.figsize':(20,11)})\nax = sns.barplot(x=word_freq_top30['words'][1:], y=word_freq_top30['frequency'])\n#ax.set_xticklabels(ax.get_xticklabels(),rotation=45)\nplt.xticks(rotation=45)","726dec21":"from wordcloud import WordCloud, STOPWORDS","dc07dbde":"text = ' '.join(flatten(tokens_df.to_list())) ## text string of cleaned tokens \nstopwords = set(STOPWORDS) \n## add additional stopwwords based upon data, this is not final list, more words can added as per requiremet and data availability  \nstopwords.update([\"nan\",\"find\",\"show\",\"conclusion\",\"case\",\"include\",\"human\",\"biorxib\",\"day\",\"total\",\"author\",\"funder\",'virus','protein'])\nwordcloud = WordCloud(stopwords=stopwords, background_color=\"white\",width=1600,height=800).generate(text)\n\n##Plotting\nplt.figure(figsize=(20,10))\nplt.axis(\"off\")\n# \"bilinear\" interpolation is used to make the displayed image appear more smoothly without overlapping of words  \n# can change to some other interpolation technique to see changes like \"hamming\"\nplt.imshow(wordcloud, interpolation=\"bilinear\") \n\nplt.show()","942365c1":"## This function will extract all abbreviations and respective full form used within the text.\n## This function has further scope of improvement\nimport re\ndef extract_abbreviations(text):\n    '''\n    Input: Text string\n    Output: Dictionary of abbreviation and respective full form\n    '''\n    abbr_fullform_dict=dict()\n    abbr = re.findall('\\(([A-Z]{2,})\\)',text)\n    #print(abbr)\n    if len(abbr)<10:\n        for i in abbr:\n            span_abbr = re.search(i,text).span()\n            len_full_form = str(span_abbr[1]-span_abbr[0])\n            try:\n                full_form = re.search('(\\w+\\s)'+'{'+len_full_form+'}'+'\\('+i+'\\)',text).group(0)\n                if full_form[0].lower()!= i.lower()[0]:\n                    full_form = ' '.join(full_form.split()[1:])\n                if full_form.lower()[-1]!= i.lower()[-1]:\n                    full_form = ' '.join(full_form.split()[:-1])\n                full_form = full_form.replace('('+i+')','').strip()\n                abbr_fullform_dict[i] = full_form\n            except AttributeError:\n                print(\"error\")\n                pass\n    \n        return abbr_fullform_dict    ","2e05f1b5":"#corona_df[\"abbr_dict\"]= pd.read_csv('\/kaggle\/input\/abbreviations\/abb.csv',names=['abb_dict'],header=None)","6c94bbaa":"## calling function for extractons of abbreviations and respective full form from text\ncorona_df[\"abbr_dict\"] = corona_df[\"text_body\"].progress_apply(lambda x:extract_abbreviations(x))","a547e12e":"#Function for replacing abbreviations by its full form\nimport ast\nimport numpy as np\ndef remove_abbriviation(text,abbr_dict):\n    try :\n       \n        #abbr_dict = ast.literal_eval(abbr_dict)\n    \n        for key,value in abbr_dict.items():\n                #print(key,value)\n                text = text.replace(key,value)\n    except AttributeError:            \n          pass\n    return text    ","1ded4002":"#Function for removing emailids from text\ndef remove_email(text):\n    if text:\n        text = text.lower()\n        text = re.sub('([a-zA-Z0-9_\\-\\.]+)@([a-zA-Z0-9_\\-\\.]+)\\.([a-zA-Z]{2,5})','*email*',str(text))\n    else:\n        pass\n    return text","5b157b2e":"# Function for removing website links from text\ndef remove_weblink(text):\n    if text:\n        text=text.lower()\n        text = re.sub('(https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\\/\\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})','link',str(text))\n    else:\n        pass\n    return text","70294c0d":"# Function for removing paper refrences from text e.g [19],[1a,2b] or [19,14,15]\ndef remove_refrence(text):\n    if text:\n        text=text.lower()\n        text = re.sub('\\[\\d+(,\\s{0,}\\d+){0,}\\]','',str(text))   \n    else:\n        pass\n    return text","d530f676":"# Function for removing non ASCII charecters like '\u03ea','\u00f3','\u00fc','\u00a9','\u00b5','\u25b2','\u2192'\n# This function check if charecter hex value in range [\\x00,\\x7F] (in decimal [0,127] i.e range of ASCII charecters) and replace if it occurs outside limit\ndef remove_ghost_char(text):\n    if text:\n        text = re.sub(r'[^\\x00-\\x7F]+',' ', str(text))\n    else:\n        pass\n    \n    return text","526f17ad":"# This function remove all bracktes with data\ndef remove_brackets(text):\n    if text:\n        text = re.sub('(\\(.*?\\))|(\\[.*?\\])','',str(text))   \n    else:\n        pass\n    return text                  ","38f6b958":"# Function for removing multiple spaces \ndef remove_extra_spaces(text):\n    if text:\n        text = re.sub(r'( +)',' ', str(text))\n    else: \n        pass\n   \n    return text","54ffbd42":"def preprocess(df,dask_=False ,remove_abbr_=False,remove_email_=True,remove_weblink_=True,\n               remove_refrence_=True,remove_brackets_=True,remove_ghost_char_=True,remove_extra_spaces_=True):\n    start_time = time.time()\n#     #series_text = corona_df['text_body']\n#     series_abbr  = corona_df['abbr_dict']\n#     _series = df[\"text_body\"]\n    \n    if remove_abbr_: \n        print('Replacing abbreviations now!')\n        _series = df.apply(lambda x: remove_abbriviation(x[\"text_body\"],x[\"abbr_dict\"]),axis=1)\n    else:\n        _series = df[\"text_body\"]\n                          \n    if remove_email_: \n        print('Removing Email now!')\n        if dask_:\n            _series = (dd\n                               .from_pandas(_series, npartitions=4)\n                               .apply( lambda x: remove_email(x),meta=('text_body', 'object'))\n                               .compute(scheduler='processes')\n                                )\n        else:\n            _series = _series.apply(lambda x: remove_email(x))\n    if remove_weblink_: \n        print('Removing weblink now!') \n        if dask_:                       \n            _series = (dd\n                               .from_pandas(df[\"text_body\"], npartitions=4)\n                               .apply(lambda x: remove_weblink(x),meta=('text_body', 'object'))\n                               .compute(scheduler='processes')\n                                )\n        else:\n            _series = _series.apply(lambda x:  remove_weblink(x))                \n    if remove_refrence_: \n        print('Removing refrences now!')\n        if dask_:                    \n            _series = (dd\n                               .from_pandas(_series, npartitions=4)\n                               .apply(lambda x: remove_refrence(x),meta=('text_body', 'object'))\n                               .compute(scheduler='processes')\n                                )\n        else:\n             _series = _series.apply(lambda x:  remove_refrence(x))                   \n                          \n    if remove_brackets_: \n        print('Removing brackets now!')\n        if dask_:                        \n            _series = (dd\n                               .from_pandas(_series, npartitions=4)\n                               .apply(lambda x: remove_brackets(x),meta=('text_body', 'object'))\n                               .compute(scheduler='processes')\n                                )\n        else :\n            _series = _series.apply(lambda x:  remove_brackets(x))                 \n    if remove_ghost_char_: \n        print('Removing bad charecters now!')\n        if dask_:                       \n            _series = (dd\n                               .from_pandas(_series, npartitions=4)\n                               .apply(lambda x: remove_ghost_char(x),meta=('text_body', 'object'))\n                               .compute(scheduler='processes')\n                                )\n        else:\n            _series = _series.apply(lambda x: remove_ghost_char(x))                   \n    if remove_extra_spaces_:\n        print('Removing Extra spaces now!')\n        if dask_:                       \n            _series =(dd\n                               .from_pandas(_series, npartitions=4)\n                               .apply(lambda x: remove_extra_spaces(x),meta=('text_body', 'object'))\n                               .compute(scheduler='processes')\n                                )\n        else:\n             _series = _series.apply(lambda x: remove_extra_spaces(x))                 \n    print (\"completed preprocessing text in {:2f} minutes\".format((time.time()-start_time)\/60))\n    \n    return _series","38c44334":"# set \"dask_ = True\", to operate this function using Dask\n# if good computing resources are available(cores>4 and RAM>16GB) then set it 'TRUE', don't try to run dask here, it will take all the RAM and freeze everything\ncorona_df[\"text_body\"] = preprocess(corona_df,dask_= False)","309abf42":"## install required libraries for BERT\n!pip install transformers\n!wget -O scibert_uncased.tar https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-research\/scibert\/huggingface_pytorch\/scibert_scivocab_uncased.tar\n!tar -xvf scibert_uncased.tar","8a051f8d":"## import libraries\nimport torch\nfrom transformers import BertTokenizer, BertModel","6563fc9a":"# Let's load pretrained BERT model\nmodel_version = 'scibert_scivocab_uncased'\ndo_lower_case = True\nmodel = BertModel.from_pretrained(model_version)\ntokenizer = BertTokenizer.from_pretrained(model_version,do_lower_case=do_lower_case)","5104dd80":"## Take 100 sentences to demonstrate BERT embeddings\nsent_series= corona_df[\"text_body\"].progress_apply(lambda x:re.split('\\.',x))\nsent= flatten(sent_series.to_list())[0:100]","b9372d5a":"#tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with\ntokenized = df[\"sent\"].progress_apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))","abcbd12c":"from tqdm import tqdm\nmax_len = 0\nfor i in tqdm(tokenized.values):\n    if len(i) > max_len:\n        max_len = len(i)\n\npadded= np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])","3d430f92":"padded.shape","ce27d143":"attention_mask = np.where(padded != 0, 1, 0)\nattention_mask.shape","08737621":"## Uncomment for BERT encoddings (takes lots of memory, therefore USE is used for completing tasks)\n# input_ids = torch.tensor(padded)  \n# attention_mask = torch.tensor(attention_mask)\n\n# with torch.no_grad():\n#     last_hidden_states = model(input_ids, attention_mask=attention_mask)","a70cadbc":"embeddings = last_hidden_states[0][:,0,:].numpy()","da7815ae":"embeddings.shape","545dab74":"query = 'incubate with the membranes'","826ecbd9":"tok = torch.tensor(tokenizer.encode(query)).unsqueeze(0)","a3302955":"with torch.no_grad():\n    last_hidden_states = model(tok )","7d9d172f":"embeddings_query = last_hidden_states[0][:,0,:].numpy()","25bcc580":"#embeddings_query.shape","266c3df4":"!pip install \"tensorflow_hub>=0.6.0\"\n!pip install \"tensorflow>=2.0.0\"","637958c1":"import tensorflow as tf\nimport tensorflow_hub as hub\n\n## Load USE model\nmodule_url = \"https:\/\/tfhub.dev\/google\/nnlm-en-dim128\/2\"\nembed = hub.KerasLayer(module_url)","5e59c9dd":"corona_df[\"text_body_sent\"]= corona_df[\"text_body\"].progress_apply(lambda x:re.split('\\n',x))","edc3d69a":"import nltk\nsent= nltk.flatten(corona_df[\"text_body_sent\"].to_list())","dd0ce9d4":"df_sent = pd.DataFrame(columns=[\"sent\",\"len\"])\ndf_sent[\"sent\"] = sent\ndf_sent[\"len\"] = [len(str(i).split()) for i in sent]","12891423":"df_sent['len'].describe()","75848934":"## sentences those contains tokens more than 10\nsent = [i for i in sent if len(str(i).split())>10] ","26f60e08":"# create embeddings for sentences\nembeddings = embed(sent)","2c4d9657":"embeddings.shape","eea1d580":"query = [\" transmission of virus in community\"]","87f7ab0d":"## embedding for query\nembeddings_query = embed(query)","ee2df5c1":"embeddings_query.shape","129b63df":"#Calculate cosine similarity of query with all sentences\ndef cosine_similarity_func(embeddings,embeddings_query):\n    '''\n    Input:\n         embeddings: array or tensor of all sentence embeddings (nX128 for n sentences)\n         embeddings_query: array or tensor of query embedding (1X128)\n    Output:\n         cosine_similarity: cosine similarity of query with each sentence (nX1) \n    '''\n    # x.y\n    dot_product = np.sum(np.multiply(np.array(embeddings),np.array(embeddings_query)),axis=1)\n    \n    #||x||.||y||\n    prod_sqrt_magnitude = np.multiply(np.sum(np.array(embeddings)**2,axis=1)**0.5, np.sum(np.array(embeddings_query)**2,axis=1)**0.5)\n    \n    #x.y\/(||x||.||y||)\n    cosine_similarity  = dot_product\/prod_sqrt_magnitude\n    return cosine_similarity","1513d193":"# function for recommend text based upon query\ndef recommended_text(query,embeddings,sent,threshold_min=.95,threshold_max = 1):\n    '''\n    Input:\n         query: list of queries\n         embeddings: embeddings of all sentences\n         sent:list all sentences\n         threshold_min: lower limit of threshold for which sentence is supposed to be similar with query\n         threshold_max: upper limit of threshold for which sentence is supposed to be similar with query\n         \n    Output:\n          recommend_text: list of similar sentences with query\n    '''\n    recommend_text = []\n    embeddings_query = embed(query) #create embedding for query\n    \n    cosine_similarity = cosine_similarity_func(embeddings,embeddings_query) # get cosine similarity with all sentences\n    \n    # standardize cosine similarity output, Range(0,1)\n    standardize_cosine_simi  = (cosine_similarity-min(cosine_similarity))\/(max(cosine_similarity)-min(cosine_similarity))\n    \n    #sort sent based upon cosine similarity score\n    sent_prob = list(map(lambda x, y:(x,y), standardize_cosine_simi, sent)) \n    sent_prob.sort(key=lambda tup: tup[0], reverse=True)\n\n    # select sentences by using upper and lower threshold\n    for i,j in sent_prob:\n        if (i >threshold_min) and (i<=threshold_max):\n            recommend_text.append(j)\n    return recommend_text  ","cc1f22d5":"query = [\"range of incubation period of SARS\"]","12c0aa4a":"result = recommended_text(query,embeddings,sent,threshold_min=.95)","d423a252":"result","de06c158":"query = [\"transmission of virus in community\"]","819fa0ef":"result = recommended_text(query,embeddings,sent,threshold_min = .95)","6083e920":"result[0:30]     #top 30","0893fc02":"# effect of environment factors on virus\n# effect of weather on virus\n# effect of climate on virus\n#effect of environment factors on virus\nquery = [\"effect of environment factors on virus\"] ","7c3ca2fe":"result = recommended_text(query,embeddings,sent,threshold_min = .95)","0a0dcc55":"result[0:50] #top 50 results ","5a204a18":"#intervals of virus\n#seasonal outbreaks\nquery = [\"seasonal outbreaks\" ]","5bc1a7d3":"result = recommended_text(query,embeddings,sent,threshold_min =.93)","96e36fc5":"result","f40b63f8":"query = [\"adhesion to hydrophilic surfaces\"]","7e967444":"result = recommended_text(query,embeddings,sent)","d41571ac":"result","8572ea98":"query = [\"persistence of virus on different inanimate surfaces\"]","52850818":"result =recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.97)","03f3a60f":"result","27b24136":"query = ['implementation of diagnostics and products to improve clinical processes']","8c732ef2":"result = recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.95)","3a3478d1":"result[:30] #top 50 results","205e4021":"query = ['Physical science of the coronavirus']","01f2c681":"result = recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.95)","5b6205cb":"result[:30]  # top 30 results","4c699f06":"query = ['implementation of diagnostics to improve clinical processes']","f1267ca9":"results = recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.95)","ce29b026":"results[0:30] #top 30 results","62493ffa":"query = [\"desease models of transmission, infection and disease\"]\n#query = [\"desease models of transmission\"]","80e1c3db":"result = recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.95)","ef655a31":"result","21f8ddbe":"query = [\"tools and studies to monitor phenotypic change and potential adaptation of the virus\"]","b1ba4348":"result = recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.95)","1c9dcc50":"result[:30] # top 3 results","04271029":"query = [\"immune response and immunity\"]","51ca8f54":"results = recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.90)","47275d2f":"results","88354aad":"query = [\"effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\"]","ecf7ff96":"result = recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.95)","fc76130e":"result[:30] #top 30 results","811edb93":"query = [\"effectiveness of personal protective equipment and its usefulness to reduce risk of transmission in health care and community settings\"]","54c9a864":"result = recommended_text(query,embeddings,sent,threshold_max=1,threshold_min=.95)","facb8e91":"len(result)","aa71a1a8":"result[0:30] #top 30 results","9ac96a6e":"### Frequency graph\nPlot frequency of words in documents.","0c1fd555":"## Dataset Description\nIn response to the COVID-19 pandemic, the White House and a coalition of leading research groups have prepared the COVID-19 Open Research Dataset (CORD-19). CORD-19 is a resource of over 47,000 scholarly articles, including over 36,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses. This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. There is a growing urgency for these approaches because of the rapid acceleration in new coronavirus literature, making it difficult for the medical research community to keep up","7a5f9e65":"![https:\/\/iamcheated.indianmoney.com\/uploads\/NewsImages\/News_IMAGE-20200306120354.jpg](https:\/\/iamcheated.indianmoney.com\/uploads\/NewsImages\/News_IMAGE-20200306120354.jpg)","8c5dda23":"## Import required libraries","6f113233":"### Remove Email id","bae4d089":"### COVID-19 Open Research Dataset Challenge (CORD-19)\nAn AI challenge with AI2, CZI, MSR, Georgetown, NIH & The White House","2cf4dd73":"### Tools and studies to monitor phenotypic change and potential adaptation of the virus","5f81348a":"### cosine similarity function\n$$\\begin{eqnarray}\ncos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||} = \\frac {\\sum\\limits_{i=0}^{n-1} x_i \\cdot y_i}{\\sqrt{\\sum_{i=1}^{n-1} (x_i)^2} * \\sqrt{\\sum_{i=1}^{n-1} (y_i)^2} } \n\\end{eqnarray}$$","697df7d0":"## Incubation period \nRange of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.","2e632bf2":"#### Role of environment in Transmission","413b22a1":"### Wordcloud","9691220e":"### Padding\nAfter tokenization, tokenized is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths)","f7e536d2":"## Word Embeddings using BERT","89751093":"The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs unlike BERT, which can only handles up to 512 characters. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 128 dimensional vector. The universal-sentence-encoder model is trained with a deep averaging network (DAN) encoder.","7ac79a37":"### Implementation of diagnostics and products to improve clinical processes","2d106e06":"## Extract JSON files to CSV","eff2789d":"### Remove weblink","a219e4be":"### Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings","679151e8":"https:\/\/github.com\/jalammar\/jalammar.github.io\/blob\/master\/notebooks\/bert\/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb","b00876de":"### Tokenization","c80ffb79":"### Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings","2bf3367a":"## Embeddings for query","5256e1b4":"## Persistence of virus on surfaces of different materials \n(e,g., copper, stainless steel, plastic).","83689694":"### Spacy processing pipeline\n![https:\/\/spacy.io\/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg](https:\/\/spacy.io\/pipeline-7a14d4edd18f3edfee8f34393bff2992.svg)","113509b5":"### Recommend text based upon cosine similarity ","6bb46e9d":"### Seasonality of transmission","acfec72f":"### Tokenize documents by newline charecter","a8aa0042":"### Implemantation of Dask for fast processing and Better utilization of CPU\nBy using dask processing time can be decreased by ~10 times","2f3fe966":"### Masking\nIf we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:","c455a38d":"## Preprocessing","fac88782":"### Physical science of the coronavirus","1cb5ef8c":"## Call to Action\n\nWe are issuing a call to action to the world's artificial intelligence experts to develop text and data mining tools that can help the medical community develop answers to high priority scientific questions. The CORD-19 dataset represents the most extensive machine-readable coronavirus literature collection available for data mining to date. This allows the worldwide AI research community the opportunity to apply text and data mining approaches to find answers to questions within, and connect insights across, this content in support of the ongoing COVID-19 response efforts worldwide. There is a growing urgency for these approaches because of the rapid increase in coronavirus literature, making it difficult for the medical community to keep up.\n\nA list of our initial key questions can be found under the Tasks [https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks) section of this dataset. These key scientific questions are drawn from the NASEM\u2019s SCIED (National Academies of Sciences, Engineering, and Medicine\u2019s Standing Committee on Emerging Infectious Diseases and 21st Century Health Threats) research topics and the World Health Organization\u2019s R&D Blueprint for COVID-19.\n\nMany of these questions are suitable for text mining, and we encourage researchers to develop text mining tools to provide insights on these questions.\n\nWe are maintaining a summary of the community's contributions. For guidance on how to make your contributions useful, we're maintaining a forum thread with the feedback we're getting from the medical and health policy communities.\n","d1a2b303":"## Task 1 (COVID-19 Open Research Dataset Challenge (CORD-19))\n* **What is known about transmission, incubation, and environmental stability?**\n   \n #### Task Details\n****What is known about transmission, incubation, and environmental stability? What do we know about natural history, transmission, and diagnostics for the virus? What have we learned about infection prevention and control?****\n\nSpecifically, we want to know what the literature reports about:\n\n* Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.\n* Prevalence of asymptomatic shedding and transmission (e.g., particularly children).\n* Seasonality of transmission.\n* Physical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).\n* Persistence and stability on a multitude of substrates and sources (e.g., nasal discharge, sputum, urine, fecal matter, blood).\n* Persistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\n* Natural history of the virus and shedding of it from an infected person\n* Implementation of diagnostics and products to improve clinical processes\n* Disease models, including animal models for infection, disease and transmission\n* Tools and studies to monitor phenotypic change and potential adaptation of the virus\n* Immune response and immunity\n* Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\n* Effectiveness of personal protective equipment (PPE) and its usefulness to reduce risk of transmission in health care and community settings\n* Role of the environment in transmission","bc4abf9f":"*   ****tokenizer****:Segment text into tokens.\n*   ****tagger****: Assign part-of-speech tags.\n*   ****parser****: Assign dependency labels.\n*   ****ner****: Detect and label named entities. \n\nNOTE: you can create own pipeline components for futher processing","c1d3e090":"### In this section we will create embeddings by using pretrainted transformer model BERT\n\nNote: Word embeddings created by BERT takes lots of memory(>16GB), therefore instead of BERT embeddings, USE(Universal sentence encoder) embeddings are used for recommandation of text based upon query.  ","2e42bffc":"### Immune response and immunity","d5210ed6":"### Implementation of diagnostics and products to improve clinical processes","90d098b1":"### Disease models, including animal models for infection, disease and transmission","d6e35bcd":"### Model #1: And Now, Deep Learning!\nNow that we have our model and inputs ready, let's run our model","be896085":"### Transmission","6fca50db":"## EDA","01104d34":"### Create embeddings(Deep-Learning)","575dcb2b":"# Tasks","512bd07e":"## Word embeddings using USE\n[https:\/\/ai.googleblog.com\/2019\/07\/multilingual-universal-sentence-encoder.html](https:\/\/ai.googleblog.com\/2019\/07\/multilingual-universal-sentence-encoder.html)","0be3ebab":"## Physical science of the coronavirus\n(e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, environmental survival to inform decontamination efforts for affected areas and provide information about viral shedding).","32017679":"### Extract all abbreviations and fullform from text"}}