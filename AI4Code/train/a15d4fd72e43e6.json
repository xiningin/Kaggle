{"cell_type":{"318394dd":"code","84274487":"code","9b9dcda3":"code","37bb41de":"code","79089071":"code","cd54e17c":"code","a4ad1c64":"code","38cf53e7":"code","bcf676f8":"code","de073a88":"code","cd513752":"code","c59e46f9":"code","b188c38a":"code","d7eb3fac":"code","1715bd90":"code","d56c27d0":"code","edd2c171":"code","89bae371":"code","e700390d":"code","306cb348":"code","9d15e4b6":"code","f3748b54":"markdown","e51e5e35":"markdown","05cfce8a":"markdown","a17521ae":"markdown","a083e2cc":"markdown","2ebbce5f":"markdown","ae377000":"markdown","3b8a96f3":"markdown","4eb1dbf4":"markdown","760ff263":"markdown","8b0c54a4":"markdown"},"source":{"318394dd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","84274487":"# This decoratore shows some info about function perormance\n# etc time,shpe changes,nan values\n\ndef info(function):\n    import datetime\n    def wrapper(data,*args,**kargs):\n        tic    = datetime.datetime.now()\n        result = function(data,*args,**kargs)\n        toc    = datetime.datetime.now()\n        print(function.__name__,' took ', toc-tic)\n        print('Shape: ',data.shape,' ----> ', result.shape)\n        print('NaN value: ', result.isna().sum()[result.isna().sum() != 0])\n        print('\\n')\n        return result\n    return wrapper","9b9dcda3":"# let`s load datasets as usually\n\ntrain  = pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/train.csv')\ntest   = pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/test.csv')\nstores = pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/store.csv')\nsample = pd.read_csv('\/kaggle\/input\/rossmann-store-sales\/sample_submission.csv')","37bb41de":"# Thank to notebooks we can definve evaluation metric\n\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1.\/(y[ind]**2)\n    return w\n\n\ndef rmspe(yhat, y):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe","79089071":"import seaborn as sns\n\nfig,ax =plt.subplots(1,2,figsize = (20,10))\nins1 = ax[0].inset_axes([0.5,0.5,0.4,0.4])\nins2 = ax[1].inset_axes([0.7,0.7,0.2,0.2])\n\nsns.distplot(train[train.Sales != 0].Sales,ax=ax[0],bins=100) \nsns.distplot(np.log1p(train[train.Sales != 0].Sales),ax=ins1,bins=100,color = 'red')\n\n\nsns.boxplot(train[train.Sales != 0].Sales,ax=ax[1])\nsns.boxplot(np.log1p(train[train.Sales != 0].Sales),ax=ins2)\n\n\n# We see that sales values shpw positive skeew, it can be fixed by applying np.log1p (embedded plot)\n# Also there are some outliers, lets define functions to perform transformation and outliers removal","cd54e17c":"@info\ndef log_transf(df):\n    # log transformation function to remove skeew\n    df.Sales     = np.log1p(df.Sales)\n    df.Customers = np.log1p(df.Customers)\n    return df\n\n@info\ndef remove_outliers(df,column='Sales'):\n    # interquntile approach to remove outliers\n    q1  = df[column].quantile(0.2)\n    q3  = df[column].quantile(0.8)\n    iqr = q3-q1\n    iqr_lower = q1 - 1.5*iqr\n    iqr_upper = q3 + 1.5*iqr\n    \n    df = df.loc[(df[column] > iqr_lower) & (df[column]< iqr_upper),:]\n    return df","a4ad1c64":"@info\ndef timeseries_features(df):\n    # move to datetime format\n    df.Date = pd.to_datetime(df.Date)\n    df = df.sort_values('Date').reset_index(drop = True)\n    \n    # derive regular for ml task time series features\n    df['month']          = df.Date.dt.month\n    df['dayofmonth']     = df.Date.dt.day\n    df['dayofyear']      = df.Date.dt.dayofyear\n    df['year']           = df.Date.dt.year\n    df['is_weekday']     = df.DayOfWeek.apply(lambda x: 0 if x in (6,7) else 1)\n    df['is_month_start'] = df.Date.dt.is_month_start.astype(int)\n    df['is_month_end']   = df.Date.dt.is_month_end.astype(int)\n\n    # also lets take into account holidays\n    from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n\n    holidays = calendar().holidays(start = df.Date.min(), end = df.Date.max())\n    df['is_holiday'] = df.Date.isin(holidays).astype(int)\n    \n\n    return df\n\n@info\ndef clean_main(df):\n    # drop days with 0 sales\n    df = df.loc[df.Sales != 0,:].reset_index(drop = True)\n    df = df.drop(['Open'],axis = 1)\n    \n    # beacus unique values contain mixed dtype array(['a', '0', 'b', 'c', 0], dtype=object)\n    # also could be fixed during pandas importing\n    df.StateHoliday = df.StateHoliday.astype(str)\n    \n    return df\n\n@info\ndef clean_store(df):\n    # lets drop columns with high content of nan values\n    df.CompetitionDistance.fillna(df.CompetitionDistance.mean(),inplace = True)\n    df.drop(['CompetitionOpenSinceMonth','CompetitionOpenSinceYear','Promo2SinceWeek','Promo2SinceYear'],axis = 1,inplace = True)\n    \n    import calendar\n    \n    # We have a list of promo monthes and we can derive usefull feature\n    # presence or absence of promo\n    \n    # first create encoded dictionary Month:Number eg Feb:2\n    month_dict = {v: k for k,v in enumerate(calendar.month_abbr)}\n    del month_dict['']\n    del month_dict['Sep']\n    month_dict['NaN']   = 0 # assign absence of promo 0\n    month_dict['Sept']  = 9 # There is no Sep\n\n    # Secondly, we treat PromoInterval columns, making each row list instead of string  now we have smth like ['Feb','Mar','Sept']\n    # and lets apply dictionary\n    df.PromoInterval = df.PromoInterval.fillna('NaN')\n    df.PromoInterval = df.PromoInterval.str.split(',')\n    # Lastly we are applyin transformation\n    df.PromoInterval = df.PromoInterval.apply(lambda x: [month_dict[value] for value in x if month_dict.get(value)])\n    \n    # Lets create new feature that us equal to number of promo monthes\n    df['promo_len']  = df.PromoInterval.apply(lambda x: len(x))\n    return df\n\n\n# Pipeline for train file\ntrain_prep = (train\n             .copy()\n             .pipe(log_transf)\n             .pipe(remove_outliers)\n             .pipe(timeseries_features)\n             .pipe(clean_main)\n             )\n\n# Pipeline for store file\nstore_prep = (stores\n             .copy()\n             .pipe(clean_store)\n             )\n# Now we merge two\ndata_prep               = pd.merge(train_prep,store_prep,how='left',on='Store')\n\n# Using our transformation in PromoInterval interval, we create binary new feature is_promo or not\ndata_prep['is_promo']   = data_prep.apply(lambda x: 1 if x['month'] in x['PromoInterval'] else 0,axis = 1)\ndata_prep               = data_prep.drop('PromoInterval',axis=1).reset_index(drop=True)","38cf53e7":"# Here I would like to know what is rmspe score with th emost dumb approach\n\n# we devide data on train and test\ntest_bs  = data_prep[data_prep.year == 2015]\ntrain_bs = data_prep[data_prep.year  < 2015]\n\n# I am going to use mean Sales grouped by store-month-day among previous years as predicted values for 2015\npredict_bs = (train_bs\n              .groupby(['Store','month','dayofmonth']).Sales.mean().reset_index().rename({'Sales':'predictions'},axis = 1)\n              .merge(test_bs,how='right',on = ['Store','month','dayofmonth'])\n              .fillna(train_bs.Sales.mean())\n              .sort_values('Date')\n              )\n\n# Display baseline\nprint('Baseline to overcome = {:.2f}'.format(rmspe(np.expm1(predict_bs.Sales),np.expm1(predict_bs.predictions))))\n\n# Let`s see how prediction looks like\nfig,ax = plt.subplots(1,3,figsize = (30,10))\n\nrnd_store = np.random.randint(min(predict_bs.Store),max(predict_bs.Store),3)\n\nfor idx,store in enumerate(rnd_store):\n    \n    ax[idx].plot(predict_bs[predict_bs.Store == store].Date,np.expm1(predict_bs[predict_bs.Store == store].Sales), color = 'blue'    ,label = 'Observed')\n    ax[idx].plot(predict_bs[predict_bs.Store == store].Date,np.expm1(predict_bs[predict_bs.Store == store].predictions),color = 'red',label = 'Predicted')\n\n    ax[idx].legend()\n    ax[idx].set_title('Store '+str(store))\n    \n    \n# It doesn`t look so bad","bcf676f8":"# There are two few reasons to use mean (aka target) encoding\n# We have 1115 stores, definetly there is correlation between store and sales\n# We could perform leave stores as it is ----> not good for known reasons\n# We could perform OneHotEncoding        ----> not goodm becaouse we will have 1115 new columns, mainly sparse\n# We can do mean encoding, eg encode stores as mean\/std\/other of target\n# I am going to use Customers to encode store, because we don`t have customers in test set\n# Obviusly customers can be good feature\n\ndef mean_encoding(df,column,target,func = np.mean):\n    \n    # perform target encoding on column with some function\n    \n    enc_col_name = target+'_enc_'+func.__name__\n    df_temp = (df\n               .groupby(column)[target]\n               .apply(func)\n               .reset_index()\n               .rename({target:enc_col_name},axis=1)\n              )\n    \n    df = df.merge(df_temp,how='left',on = column)\n        \n    \n    return df,df_temp\n\ndata_prep,dict_for_test = mean_encoding(data_prep,'Store','Customers',func = np.mean) ","de073a88":"# also it is good to statistic\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\n\n\n# first lets check our data for stationarity\n\ncounter = 0\nfor store in data_prep.Store.unique():\n    df_store = data_prep.copy().loc[data_prep.Store == store,['Date','Sales']].set_index('Date')\n    # since we removed some dates, lets resample data on a daily basis and fillna with 0\n    df_store = df_store.resample('D').fillna('bfill')\n    adf = adfuller(df_store,regression='c', autolag='AIC')\n    \n    if adf[1] > 0.05:\n        print('Adfuller for store {} : p-value = {:.5f} > 5% -----> NON STATIONARY'.format(store,adf[1]*100))\n        counter+=1\n        # also we can use it as a feature\n        # Doesnt make sense becaause only ~3 of store are not statonary\n        \nprint('\\n {:.2f} % of stores are non stationary '.format(counter\/len(data_prep.Store.unique())*100))\n\n# There is a chance to use traditional time series technique(ARIMA,SARIMAX, smothing) but i would ike to continue with ml","cd513752":"# lets check few random stores \n\nrnd_store = np.random.randint(min(data_prep.Store),max(data_prep.Store),3)\n\nfig,ax = plt.subplots(3,2,figsize = (15,10))\n\nfor idx,store in enumerate(rnd_store):\n    df_store = data_prep.copy().loc[data_prep.Store == store,['Date','Sales']].set_index('Date')\n    df_store = df_store.resample('D').fillna('bfill')\n    plot_acf(df_store,lags = 60,ax = ax[idx,0],label = store) \n    plot_pacf(df_store,lags = 60,ax = ax[idx,1], label = store)\n    ax[idx,0].set_title('Autocorelation for store {}'.format(store))\n    ax[idx,1].set_title('Partial Autocorelation for store {}'.format(store))\n    plt.tight_layout()\n    \n    \n# By running this part few times we can notice that almost for all stores there is hogh corelation with following lags:\n# 1 14,28,42, 49\n# Therefore lets use this values to create new features\n# But we need to preduct 48 days in future, threre fore we cannot use something lower 48","c59e46f9":"def lag_creator(df,lags = [1,14,28,42,49],col = 'Sales',mean = False,std = False,rolling = 90):\n    #  we can create lags if we want\n    for lag in lags:\n        col_tag = 'lag-'+str(lag)\n        df[col_tag] = df.groupby(['Store'])[col].shift(lag).values\n        if mean:\n            col_tag_mean = 'lag_mean-'+str(lag)\n            df[col_tag_mean] = df.groupby(['Store'])[col].shift(lag).rolling(window = rolling).mean().values\n        elif std:\n            col_tag_std= 'lag_std-'+str(lag)\n            df[col_tag_mean] = df.groupby(['Store'])[col].shift(lag).rolling(window = rolling).std().values\n        elif mean==True and std == True:\n            col_tag_mean = 'lag_mean-'+str(lag)\n            df[col_tag_mean] = df.groupby(['Store'])[col].shift(lag).rolling(window = rolling).mean().values\n            col_tag_std= 'lag_std-'+str(lag)\n            df[col_tag_mean] = df.groupby(['Store'])[col].shift(lag).rolling(window = rolling).std().values\n    else: \n        print('No statistics lags')\n            \n    return df.dropna().reset_index(drop = True)\n\n#data_prep = data_prep.pipe(lag_creator)\n# We won`t use lags","b188c38a":"# finally lets check on nan and dublicated values\n \nprint('NaN summary\\n\\n',data_prep.isna().sum()\/len(data_prep)*100,'\\n')\nprint('Number of absoulute    dublicates:',data_prep.duplicated().sum())\nprint('Number of Store - Date dublicates:',data_prep.duplicated(subset = ['Date','Store']).sum())","d7eb3fac":"from pandas.plotting import scatter_matrix\nimport seaborn\n\ncorr = data_prep.corr()\nplt.figure(figsize=(15,15))\nseaborn.heatmap(corr)","1715bd90":"stores = np.random.randint(train.Store.min(),train.Store.max(),2)\nplt.figure(figsize=(15,10))\nfor store in stores:\n    plt.plot(data_prep.loc[(data_prep.Store == store) & (data_prep.year == 2013),'Date'],data_prep.loc[(data_prep.Store == store) & (data_prep.year == 2013),'Sales'],label = store)\n    plt.legend()","d56c27d0":"ohe_col = data_prep.select_dtypes('object').columns.tolist()+['Store','DayOfWeek','month']\nnum_col = data_prep.select_dtypes('float').columns.tolist()","edd2c171":"X = data_prep.drop(['Date','Sales','Customers','Store'],axis = 1)\ny = data_prep.Sales\n\n\nX_train,X_val = X.loc[X.year < 2015,:],X.loc[X.year == 2015,:]\ny_train,y_val = y[:X_train.index[-1]+1], y[X_train.index[-1]+1:]","89bae371":"from sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder,LabelEncoder,OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\n\ntransformer = make_column_transformer(\n    (StandardScaler(),['CompetitionDistance', 'Customers_enc_mean']),\n    (OneHotEncoder(),['StateHoliday', 'StoreType', 'Assortment', 'DayOfWeek', 'month']),\n    remainder = 'passthrough'\n)\n\n\nimport xgboost as xgb\n\nregressor = xgb.XGBRegressor(n_estimators = 200,\n                             max_depth    =  10\n                            )\n\n\npipeline = make_pipeline(transformer,\n                         regressor)\n\n\npipeline.fit(X_train,y_train)\n\n\nprint('TRAIN RMSPE = ',rmspe(np.expm1(pipeline.predict(X_train)),np.expm1(y_train)))\nprint('VAL   RMSPE = ',rmspe(np.expm1(pipeline.predict(X_val)),np.expm1(y_val)))","e700390d":"# We need to apply same transformation on test set as we did we train set\n\ntest_prep = (test\n             .copy()\n             .pipe(timeseries_features)\n             .drop(['Open','Date'],axis=1)\n             )\n\ntest_prep  = pd.merge(test_prep,store_prep,how='left',on='Store')\ntest_prep['is_promo']   = test_prep.apply(lambda x: 1 if x['month'] in x['PromoInterval'] else 0,axis = 1)\ntest_prep  = pd.merge(test_prep,dict_for_test,how='left',on='Store')\ntest_prep = test_prep.drop(['PromoInterval','Store'],axis=1).reset_index(drop=True)\n","306cb348":"test_id = test_prep.Id\ntest_prep.drop('Id',axis=1,inplace = True)\npredict = np.expm1(pipeline.predict(test_prep)) # Remember to make inverse transformation","9d15e4b6":"sub = pd.DataFrame({'Id':test_id,'Sales':predict}).sort_values('Id').reset_index(drop=True)\nsub.to_csv('submission.csv',index=False)","f3748b54":"# Metric","e51e5e35":"# Predicting test set","05cfce8a":"# Mean Encoding","a17521ae":"# Set a baseline","a083e2cc":"# Helper function","2ebbce5f":"# Stationarity","ae377000":"# Lags","3b8a96f3":"# Training","4eb1dbf4":"# Check the data","760ff263":"# Time series feature engineering","8b0c54a4":"# Submission"}}