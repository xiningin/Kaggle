{"cell_type":{"2db00f3e":"code","de647d6c":"code","0420a9b3":"code","11e8deea":"code","2b1ddfe3":"code","d68f2d4b":"code","650f8477":"code","bbcb7eb3":"code","835b4e98":"code","dea0ab60":"code","c46fe336":"code","5cdc53b1":"code","b04f1833":"code","16cbee8f":"code","9dc12856":"code","1c5b6618":"code","c5c13772":"code","86a7a8f7":"code","44cfda8f":"code","d09548c4":"code","2a3476ef":"code","92e8adf7":"code","4113a299":"code","f7f86a64":"code","7af6f673":"code","4dfe0fd1":"code","87f37dda":"code","df79dd76":"code","86e00e03":"code","773104ec":"code","3418f081":"code","60bbbaff":"code","88b85654":"code","b4b1e45b":"code","b0b72f2a":"code","efd7d886":"code","14d5d214":"code","f4327652":"code","bd2fabbc":"code","e9399922":"code","eb026bb1":"code","a8a83002":"code","196ebbfb":"code","d0d06e68":"code","39b3209c":"code","ea06e642":"code","46d96fe1":"code","b6cc38e6":"code","f440e7b9":"code","cbc8b564":"code","db685b6d":"code","8edc520b":"code","bd77f54e":"code","616d5811":"code","b91455bf":"code","2596bbab":"code","47803e7f":"code","1a72fdd6":"code","63975049":"code","a36b1d73":"code","e07e052c":"code","6982560e":"code","b268a002":"code","0a817d10":"code","1f3b7e15":"code","63f879ff":"code","54002028":"code","2dd92522":"code","3b1b43ea":"code","1492b736":"code","5595841c":"code","de15fd00":"code","ae7c5a99":"code","2e8e2b2e":"code","cb8b5e07":"code","89ad39be":"code","3f51d7f5":"code","5c2650ed":"markdown","64540088":"markdown"},"source":{"2db00f3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","de647d6c":"housing = pd.read_csv('\/kaggle\/input\/california-housing-prices\/housing.csv')","0420a9b3":"housing.head()","11e8deea":"housing.info()","2b1ddfe3":"housing['ocean_proximity'].value_counts()","d68f2d4b":"housing.describe()","650f8477":"housing.hist(bins=50, figsize=(20,15))\nplt.show()","bbcb7eb3":"np.random.seed(42)","835b4e98":"import numpy as np\n\n#def split_train_test(data, test_ratio):\n#    shuffled_indices = np.random.permutation(len(data))\n#    test_set_size = int(len(data) * test_ratio)\n#    test_indices = shuffled_indices[:test_set_size]\n#    train_indices = shuffled_indices[test_set_size:]\n#    return data.iloc[train_indices], data.iloc[test_indices]","dea0ab60":"#train_set, test_set = split_train_test(housing, 0.2)","c46fe336":"#len(train_set)","5cdc53b1":"#len(test_set)","b04f1833":"from sklearn.model_selection import train_test_split","16cbee8f":"train_set, test_set = train_test_split(housing, test_size = 0.2, random_state = 42)","9dc12856":"len(train_set)","1c5b6618":"len(test_set)","c5c13772":"housing['income_cat'] = pd.cut(housing['median_income'],\n                              bins = [0.0, 1.5, 3.0, 4.5, 6.0, np.inf],\n                              labels=[1, 2, 3, 4, 5])\nhousing['income_cat'].hist()","86a7a8f7":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\nfor train_index, test_index in split.split(housing, housing['income_cat']):\n    strat_train_set = housing.loc[train_index]\n    strat_test_set = housing.loc[test_index]","44cfda8f":"strat_test_set['income_cat'].value_counts() \/ len(strat_train_set)","d09548c4":"for set_ in (strat_train_set, strat_test_set):\n    set_.drop('income_cat', axis=1, inplace= True)","2a3476ef":"housing = strat_train_set.copy()","92e8adf7":"housing.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)","4113a299":"#Looking for Correlations\ncorr_matrix = housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","f7f86a64":"from pandas.plotting import scatter_matrix\nattributes = ['median_house_value', \"median_income\", \"total_rooms\", \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize = (12,8))","7af6f673":"housing.plot(kind='scatter', x=\"median_income\", y = \"median_house_value\", alpha=0.1)","4dfe0fd1":"housing.describe()","87f37dda":"housing['rooms_per_household']=housing['total_rooms']\/housing['households']\nhousing['bedrooms_per_room']=housing['total_bedrooms']\/housing['total_rooms']\nhousing[\"population_per_household\"]=housing['population']\/housing['households']","df79dd76":"housing.head()","86e00e03":"corr_matrix = housing.corr()\ncorr_matrix['median_house_value'].sort_values(ascending=False)","773104ec":"housing = strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_labels = strat_train_set['median_house_value'].copy()","3418f081":"#Option 1\n#housing.dropna(subset=[\"total_bedrooms\"])","60bbbaff":"#Option 2\n#housing.drop('total_bedrooms, axis=1')","88b85654":"#Option 3\n#median = housing['total_bedrooms'].median()\n#housing['total_bedrooms'].fillna(median,inplace=True)","b4b1e45b":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')","b0b72f2a":"housing_num = housing.drop('ocean_proximity', axis=1)","efd7d886":"imputer.fit(housing_num)","14d5d214":"housing_num.head()","f4327652":"X = imputer.transform(housing_num)","bd2fabbc":"housing_tr = pd.DataFrame(X, columns=housing_num.columns, index = housing_num.index)","e9399922":"housing_cat = housing[['ocean_proximity']]\nhousing_cat.head(10)","eb026bb1":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","a8a83002":"housing_cat_1hot.toarray()","196ebbfb":"cat_encoder.categories_","d0d06e68":"from sklearn.base import BaseEstimator, TransformerMixin\n\n# column index\nrooms_ix, bedrooms_ix, population_ix, households_ix = 3, 4, 5, 6\n\nclass CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n    def __init__(self, add_bedrooms_per_room=True): # no *args or **kargs\n        self.add_bedrooms_per_room = add_bedrooms_per_room\n    def fit(self, X, y=None):\n        return self  # nothing else to do\n    def transform(self, X):\n        rooms_per_household = X[:, rooms_ix] \/ X[:, households_ix]\n        population_per_household = X[:, population_ix] \/ X[:, households_ix]\n        if self.add_bedrooms_per_room:\n            bedrooms_per_room = X[:, bedrooms_ix] \/ X[:, rooms_ix]\n            return np.c_[X, rooms_per_household, population_per_household,\n                         bedrooms_per_room]\n        else:\n            return np.c_[X, rooms_per_household, population_per_household]\n\nattr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\nhousing_extra_attribs = attr_adder.transform(housing.values)","39b3209c":"col_names = \"total_rooms\", \"total_bedrooms\", \"population\", \"households\"\nrooms_ix, bedrooms_ix, population_ix, households_ix = [\n    housing.columns.get_loc(c) for c in col_names] # get the column indices","ea06e642":"housing_extra_attribs = pd.DataFrame(\n    housing_extra_attribs,\n    columns=list(housing.columns)+[\"rooms_per_household\", \"population_per_household\"],\n    index=housing.index)\nhousing_extra_attribs.head()","46d96fe1":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        ('attribs_adder', CombinedAttributesAdder()),\n        ('std_scaler', StandardScaler()),\n    ])\n\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","b6cc38e6":"housing_num_tr","f440e7b9":"from sklearn.compose import ColumnTransformer\n\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs),\n    ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","cbc8b564":"housing_prepared","db685b6d":"from sklearn.linear_model import LinearRegression\n\nlin_reg_model = LinearRegression()\nlin_reg_model.fit(housing_prepared, housing_labels)","8edc520b":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint('Predictions: ', lin_reg_model.predict(some_data_prepared))","bd77f54e":"print('Labels ', list(some_labels))","616d5811":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg_model.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","b91455bf":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared, housing_labels)","2596bbab":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","47803e7f":"#Cross Validation\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(tree_reg, housing_prepared, housing_labels,\n                        scoring=\"neg_mean_squared_error\", cv = 10)\ntree_rmse_scores = np.sqrt(-scores)","1a72fdd6":"def display_scores(scores):\n    print(\"Scores: \", scores)\n    print(\"Mean: \", scores.mean())\n    print(\"Standard Deviation: \", scores.std())","63975049":"display_scores(tree_rmse_scores)","a36b1d73":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_reg.fit(housing_prepared, housing_labels)","e07e052c":"housing_predictions = forest_reg.predict(housing_prepared)\nforest_mse = mean_squared_error(housing_labels, housing_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","6982560e":"from sklearn.model_selection import cross_val_score\n\nforest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,\n                                scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","b268a002":"scores = cross_val_score(lin_reg_model, housing_prepared, housing_labels, scoring=\"neg_mean_squared_error\", cv=10)\npd.Series(np.sqrt(-scores)).describe()","0a817d10":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    # try 12 (3\u00d74) combinations of hyperparameters\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    # then try 6 (2\u00d73) combinations with bootstrap set as False\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error',\n                           return_train_score=True)\ngrid_search.fit(housing_prepared, housing_labels)","1f3b7e15":"grid_search.best_params_","63f879ff":"grid_search.best_estimator_","54002028":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","2dd92522":"pd.DataFrame(grid_search.cv_results_)","3b1b43ea":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_distribs = {\n        'n_estimators': randint(low=1, high=200),\n        'max_features': randint(low=1, high=8),\n    }\n\nforest_reg = RandomForestRegressor(random_state=42)\nrnd_search = RandomizedSearchCV(forest_reg, param_distributions=param_distribs,\n                                n_iter=10, cv=5, scoring='neg_mean_squared_error', random_state=42)\nrnd_search.fit(housing_prepared, housing_labels)","1492b736":"cvres = rnd_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","5595841c":"feature_importances = grid_search.best_estimator_.feature_importances_\nfeature_importances","de15fd00":"extra_attribs = [\"rooms_per_hhold\", \"pop_per_hhold\", \"bedrooms_per_room\"]\n#cat_encoder = cat_pipeline.named_steps[\"cat_encoder\"] # old solution\ncat_encoder = full_pipeline.named_transformers_[\"cat\"]\ncat_one_hot_attribs = list(cat_encoder.categories_[0])\nattributes = num_attribs + extra_attribs + cat_one_hot_attribs\nsorted(zip(feature_importances, attributes), reverse=True)","ae7c5a99":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\", axis=1)\ny_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","2e8e2b2e":"final_rmse","cb8b5e07":"from scipy import stats\n\nconfidence = 0.95\nsquared_errors = (final_predictions - y_test) ** 2\nnp.sqrt(stats.t.interval(confidence, len(squared_errors) - 1,\n                         loc=squared_errors.mean(),\n                         scale=stats.sem(squared_errors)))","89ad39be":"m = len(squared_errors)\nmean = squared_errors.mean()\ntscore = stats.t.ppf((1 + confidence) \/ 2, df=m - 1)\ntmargin = tscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - tmargin), np.sqrt(mean + tmargin)","3f51d7f5":"zscore = stats.norm.ppf((1 + confidence) \/ 2)\nzmargin = zscore * squared_errors.std(ddof=1) \/ np.sqrt(m)\nnp.sqrt(mean - zmargin), np.sqrt(mean + zmargin)","5c2650ed":"Data Cleaning","64540088":"Machine Learning Project CheckList\n\n1. Frame the problem and look at the big picture.\n2. Get the data.\n3. Explore the Data\n4. Prepare the data to better expose the underlying patterns to Machine Learning Algorithms.\n5. Explore many different models and shortlist the best ones.\n6. Fine tune your models and combine them into a gret solution.\n7. Present your solution.\n8. Launch, monitor, and maintain your system."}}