{"cell_type":{"a17631c2":"code","d8178568":"code","c1d08e72":"code","6b8b67b3":"code","9cbcee74":"code","b701aa3a":"code","b6747274":"code","7a1212d0":"code","9351e6d7":"code","313a618b":"code","8f4c7c3c":"code","1c4dee7f":"code","6e7e0177":"code","e8c51b3c":"code","9f433163":"code","5afb22a2":"code","e3b2857e":"code","d67e0798":"code","5737d1a4":"code","4d0ba5a9":"code","0ee9b724":"markdown","65c4c003":"markdown","80f17d24":"markdown","b27655cc":"markdown","1bf8283a":"markdown"},"source":{"a17631c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8178568":"from xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import RobustScaler\nimport warnings\nwarnings.filterwarnings('ignore')","c1d08e72":"import gc","6b8b67b3":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df\n","9cbcee74":"train = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\ntrain = reduce_memory_usage(train)\ntest = reduce_memory_usage(test)","b701aa3a":"x_data = train.drop(['id', 'target'], axis=1)\ny_data = train.target\nx_test = test.drop('id', axis=1)","b6747274":"del train, test\ngc.collect()","7a1212d0":"float_columns = ['f'+str(i) for i in range(242)]\nfloat_columns.remove('f22')\nfloat_columns.remove('f43')\nint_columns = ['f'+str(i) for i in range(242,285)]+['f22','f43']\ncols = float_columns + int_columns\n","9351e6d7":"scaler = RobustScaler()\nx_data[float_columns] = scaler.fit_transform(x_data[float_columns])\nx_test[float_columns] = scaler.transform(x_test[float_columns]) ","313a618b":"print('x_data shape {}'.format(x_data.shape))\nprint('x_test shape {}'.format(x_test.shape))","8f4c7c3c":"from sklearn.model_selection import train_test_split\n\n# x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.3, random_state=50)\n","1c4dee7f":"# from sklearn.svm import SVC\n\n# model = SVC()\n# model.fit(x_train, y_train)\n# roc_auc_score(y_val, model.predict_proba(x_val))\n","6e7e0177":"def objective(trial):\n    gc.collect()\n    x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.1, random_state=50)\n    param_grid = {'objective': 'binary:logistic',\n              'use_label_encoder': False,\n              'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n              'learning_rate': trial.suggest_discrete_uniform('learning_rate',0.01,0.1,0.01),\n              'subsample': trial.suggest_discrete_uniform('subsample', 0.3, 1.0, 0.1),\n              'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1.0, 0.1),\n              'max_depth': trial.suggest_int('max_depth', 2, 20),\n              'booster': 'gbtree',\n              'gamma': trial.suggest_uniform('gamma',1.0,10.0),\n              'reg_alpha': trial.suggest_int('reg_alpha',50,100),\n              'reg_lambda': trial.suggest_int('reg_lambda',50,100),\n              'random_state': 42,\n                 }\n\n#     gamma = trial.suggest_discrete_uniform('gamma_', 0.3, 3, 0.1)\n#     print('gamma', gamma)\n    x_train[float_columns] = np.sin(x_train[float_columns] * np.pi * 2)\n    x_val[float_columns] = np.sin(x_val[float_columns] * np.pi * 2)\n    \n    xgb_model = XGBClassifier(**param_grid, tree_method='gpu_hist', predictor='gpu_predictor',\n                            eval_metric=['logloss'])\n\n    xgb_model.fit(x_train, y_train, verbose=False)\n    y_pred = xgb_model.predict_proba(x_val)[:, 1]\n    \n    return roc_auc_score(y_val, y_pred)","e8c51b3c":"import optuna\nfrom optuna.samplers import TPESampler\n\ntrain_time = 1 * 60 * 60 # h * m * s\nstudy = optuna.create_study(direction='maximize', sampler=TPESampler(), study_name='XGBClassifier')\nstudy.optimize(objective, timeout=train_time)\n\nprint('Number of finished trials: ', len(study.trials))\nprint('Best trial:')\ntrial = study.best_trial\n\nprint('\\tValue: {}'.format(trial.value))\nprint('\\tParams: ')\nfor key, value in trial.params.items():\n    print('\\t\\t{}: {}'.format(key, value))","9f433163":"params = trial.params\n\n# gamma = params.pop('gamma_')\n# gamma = 2.6\nxgb_params = params\n# xgb_params = {}\nxgb_params['tree_method'] = 'gpu_hist'\nxgb_params['predictor'] = 'gpu_predictor'\n\n# xgb_params = {\n#     'objective': 'binary:logistic',\n#     'use_label_encoder': False,\n#     'n_estimators': 4095,\n#     'learning_rate':0.05,\n#     'subsample': 0.6,\n#     'colsample_bytree': 0.2,\n#     'max_depth': 5,\n#     'booster': 'gbtree',\n#     'gamma': 9.227759584552311,\n#     'reg_alpha': 63,\n#     'reg_lambda': 56,\n#     'tree_method': 'gpu_hist',\n#     'predictor': 'gpu_predictor',\n#     'n_jobs': 4\n# }","5afb22a2":"# xgb_params","e3b2857e":"# gamma","d67e0798":"from sklearn.model_selection import KFold\n\nn_split = 10\nkfold = KFold(n_split)\n\nval_pred = np.zeros(y_data.shape)\ny_test = np.zeros((x_test.shape[0],))\nx_data[float_columns] = np.sin(x_data[float_columns] * np.pi * 2)\nx_test[float_columns] = np.sin(x_test[float_columns] * np.pi * 2)\n\nfor i, (train_index, val_index) in enumerate(kfold.split(x_data)):\n    # train model\n    print(\"fold {} training\".format(i))\n    gc.collect()\n    \n    model = XGBClassifier(**xgb_params, eval_metric=['logloss'])\n    model.fit(x_data.iloc[train_index], y_data.iloc[train_index])\n    \n    # predict val and test\n    val_pred[val_index] = model.predict_proba(x_data.iloc[val_index])[:, 1]\n    vla_score = roc_auc_score(y_data.iloc[val_index], val_pred[val_index])\n    print(\"fold {} validation auc score {}\".format(i, vla_score))\n    \n    y_test += model.predict_proba(x_test)[:, 1] \/ n_split\n    \n# evaluate validation score    \nprint(\"val auc score :\", roc_auc_score(y_data, val_pred))","5737d1a4":"sub_mission = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nsub_mission.target = y_test\nsub_mission.to_csv('submission.csv', index=False)","4d0ba5a9":"import seaborn as sns\nsns.histplot(y_test)","0ee9b724":"### KFold","65c4c003":"### optuna","80f17d24":"### submission","b27655cc":"### data preprocessing","1bf8283a":"svm"}}