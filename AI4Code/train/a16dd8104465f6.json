{"cell_type":{"bbb02118":"code","60a3f8f1":"code","a6e2bb2f":"code","48a96609":"code","4fbc5744":"code","82c51bbe":"code","6fc26f30":"code","2bbdf351":"code","c044e2b1":"code","1069c059":"code","9f77ce04":"code","90d7aab6":"code","68337999":"code","6e24227f":"code","91fa0876":"markdown","de820997":"markdown","d95b524a":"markdown","8148b8e0":"markdown","31362613":"markdown","c49f536a":"markdown","53bb62d9":"markdown","a1669d63":"markdown","26f7128b":"markdown","f8969966":"markdown","c61fb799":"markdown","01eaa4af":"markdown","062298b6":"markdown","48c3a1fe":"markdown"},"source":{"bbb02118":"print(\"CREDIT CARD FRAUD DETECTION\")\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n#Loading dataset\ndata = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\nfraud = data[data['Class']==1]\nnormal = data[data['Class']==0]\n\noutlierFraction = len(fraud)\/float(len(normal))\nprint(\"outlier fraction is \",outlierFraction)\nprint('Fraud Cases: {}'.format(len(data[data['Class'] == 1]))) \nprint('Valid Transactions: {}'.format(len(data[data['Class'] == 0]))) \nprint(\"There is only 0.17% fraud transactions out all the transactions. The data is highly Unbalanced.\")","60a3f8f1":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nLABELS = [\"Normal\", \"Fraud\"]\ncount_classes = pd.value_counts(data['Class'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.xticks(range(2), LABELS)\nplt.title('Count of Fraud vs. Normal Transactions')\nplt.ylabel('Count')\nplt.xlabel('Class (0:Normal, 1:Fraud)')","a6e2bb2f":"corrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(15,15))\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),cmap=\"Blues\")","48a96609":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time of transaction vs Amount')\nax1.scatter(fraud.Time, fraud.Amount)\nax1.set_title('Fraud')\nax2.scatter(normal.Time, normal.Amount)\nax2.set_title('Normal')\nplt.xlabel('Time (in Seconds)')\nplt.ylabel('Amount')\nplt.show()","4fbc5744":"from sklearn.preprocessing import StandardScaler\nscaler2 = StandardScaler()\n#scaling the amount column\ndata['Amount']=StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndata.sample(5)\n#dropping old amount and time columns\ndata.drop([\"Time\"], axis = 1, inplace = True)","82c51bbe":"data.sample(5)","6fc26f30":"# feature data (predictors)\ny = data['Class']\nX = data.drop(columns=['Class'])","2bbdf351":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)","c044e2b1":"from imblearn.over_sampling import RandomOverSampler, SMOTE\n\nsmote = SMOTE(sampling_strategy='auto', random_state=42)\nX_smote, y_smote = smote.fit_resample(X_train, y_train)\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train == 0))) \n\nsm = SMOTE(random_state = 2) \nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel()) \n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res == 0)))","1069c059":"from sklearn.metrics import classification_report, accuracy_score  \nfrom sklearn.metrics import precision_score, recall_score \nfrom sklearn.metrics import f1_score, matthews_corrcoef\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import confusion_matrix\n#Build the model\nclf = LogisticRegression()\n# Train the classifier\nclf.fit(X_train_res, y_train_res)\n#test the model\ny_pred = clf.predict(X_test)\n\nprint('the Model used is Logistic Regression')\nlacc= accuracy_score(y_test,y_pred)\nprint('The accuracy is {}'.format(lacc))\nlprec= precision_score(y_test,y_pred)\nprint('The precision is {}'.format(lprec))\nlrec= recall_score(y_test,y_pred)\nprint('The recall is {}'.format(lrec))\nlf1= f1_score(y_test,y_pred)\nprint('The F1-Score is {}'.format(lf1))\nlMCC=matthews_corrcoef(y_test,y_pred)\nprint('The Matthews correlation coefficient is{}'.format(lMCC))\n\n#confusion matrix\nimport matplotlib.pyplot as plt \nLABELS = ['Normal', 'Fraud'] \nconf_matrix = confusion_matrix(y_test, y_pred) \nplt.figure(figsize =(6, 6)) \nsns.set(font_scale=1.5)\nsns.heatmap(conf_matrix, xticklabels = LABELS,  \n            yticklabels = LABELS, annot = True, annot_kws={\"size\": 16}, fmt =\"d\");\nplt.title(\"Confusion matrix\") \nplt.ylabel('True class') \nplt.xlabel('Predicted class') \nplt.show()","9f77ce04":"from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\nclf.fit(X_train_res,y_train_res)\ny_pred = clf.predict(X_test)\n\nprint('the Model used is Decison Tree')\ndacc= accuracy_score(y_test,y_pred)\nprint('The accuracy is {}'.format(dacc))\ndprec= precision_score(y_test,y_pred)\nprint('The precision is {}'.format(dprec))\ndrec= recall_score(y_test,y_pred)\nprint('The recall is {}'.format(drec))\ndf1= f1_score(y_test,y_pred)\nprint('The F1-Score is {}'.format(df1))\ndMCC=matthews_corrcoef(y_test,y_pred)\nprint('The Matthews correlation coefficient is{}'.format(dMCC))\n\n#confusion matrix \nLABELS = ['Normal', 'Fraud'] \nconf_matrix = confusion_matrix(y_test, y_pred) \nplt.figure(figsize =(6, 6)) \nsns.set(font_scale=1.5)\nsns.heatmap(conf_matrix, xticklabels = LABELS,  \n            yticklabels = LABELS, annot = True, annot_kws={\"size\": 16}, fmt =\"d\");\nplt.title(\"Confusion matrix\") \nplt.ylabel('True class') \nplt.xlabel('Predicted class') \nplt.show()","90d7aab6":"from sklearn. ensemble import BaggingClassifier\nbg = BaggingClassifier()\nbg_model = bg.fit(X_train_res,y_train_res)\ny_pred = bg_model.predict(X_test)\n \nprint('the Model used is Bagging Classifier')\nbacc= accuracy_score(y_test,y_pred)\nprint('The accuracy is {}'.format(bacc))\nbprec= precision_score(y_test,y_pred)\nprint('The precision is {}'.format(bprec))\nbrec= recall_score(y_test,y_pred)\nprint('The recall is {}'.format(brec))\nbf1= f1_score(y_test,y_pred)\nprint('The F1-Score is {}'.format(bf1))\nbMCC=matthews_corrcoef(y_test,y_pred)\nprint('The Matthews correlation coefficient is{}'.format(bMCC))\n\n#confusion matrix \nLABELS = ['Normal', 'Fraud'] \nconf_matrix = confusion_matrix(y_test,y_pred) \nplt.figure(figsize =(6, 6)) \nsns.set(font_scale=1.5)\nsns.heatmap(conf_matrix, xticklabels = LABELS,  \n            yticklabels = LABELS, annot = True, annot_kws={\"size\": 16}, fmt =\"d\");\nplt.title(\"Confusion matrix\") \nplt.ylabel('True class') \nplt.xlabel('Predicted class') \nplt.show()","68337999":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train_res,y_train_res)\ny_pred = rfc.predict(X_test)\n\nn_errors = (y_pred != y_test).sum()\nprint('The model used is Random Forest classifier')\nracc= accuracy_score(y_test,y_pred)\nprint('The accuracy is {}'.format(racc))\nrprec= precision_score(y_test,y_pred)\nprint('The precision is {}'.format(rprec))\nrrec= recall_score(y_test,y_pred)\nprint('The recall is {}'.format(rrec))\nrf1= f1_score(y_test,y_pred)\nprint('The F1-Score is {}'.format(rf1))\nrMCC=matthews_corrcoef(y_test,y_pred)\nprint('The Matthews correlation coefficient is{}'.format(rMCC))\n\n#confusion matrix\nLABELS = ['Normal', 'Fraud'] \nconf_matrix = confusion_matrix(y_test, y_pred) \nplt.figure(figsize =(6, 6)) \nsns.set(font_scale=1.5)\nsns.heatmap(conf_matrix, xticklabels = LABELS,  \n            yticklabels = LABELS, annot = True, annot_kws={\"size\": 16}, fmt =\"d\");\nplt.title(\"Confusion matrix\") \nplt.ylabel('True class') \nplt.xlabel('Predicted class') \nplt.show()","6e24227f":"import pandas as pd\nx={'Logistic R':[lacc,lprec,lrec,lf1,lMCC], 'Decison Tree':[dacc,dprec,drec,df1,dMCC], 'Bagging':[bacc,bprec,brec,bf1,bMCC], 'Random Forest':[racc,rprec,rrec,rf1,rMCC]}\ndf= pd.DataFrame(x, columns=['Logistic R','Decison Tree', 'Bagging','Random Forest'])\ndf.index=['Accuracy','Precision','Recall','F1-score','MCC']\ndf","91fa0876":"# **RANDOM FOREST**","de820997":"*from the above bar graph it is clearly evident that our dataset is highly imbalanced *","d95b524a":"# CREDIT CARD FRAUD DETECTION USING VARIOUS MACHINE LEARNING TECHNIQUES:A COMAPARITIVE ANALYSIS\n\nA comapritive study on various Machine learning Classification techniques which can be used to detect the Credit card fraud cases , we have tried to keep our models as simple as possible for better understanding .\n\nDATASET used is from kaggle .\n","8148b8e0":"# Bagging classifier using decision tree as base estimator","31362613":"# SCATTER PLOT\n\nScatter plot between time and amout gives us a crisp understanding that the fraud transaction are not happening at  a particular time frame it is taking place continously through the time period of two days , so we can say that time is not a relevant factor when it comes to predict the models hence we can drop time .","c49f536a":"# CORRELATION MATRIX\nTo check weather there is any correlation and to avoid multicollinearity.","53bb62d9":"# **LOGISTICS REGRESSION**","a1669d63":"# Partition data into train and test sets","26f7128b":"# Ploting data of normal and fraud transaction","f8969966":"# FEATURE SELECTION\nOur target variable is class and we will all the features except TIME to predict our model","c61fb799":"# BALANCING THE DATSET USING SMOTE \n\nAs we know we have a higly Imbalance Dataset and we have to first balance it before applying any classification technique if we will not balance the datset the reults which we will get would be highly biased towards the majority class .\n\nTo balance the dataset we have used OVERSAMPLING SMOTE technique.\nSMOTE is an oversampling technique that generates synthetic samples from the minority class. It is used to obtain a synthetically class-balanced or nearly class-balanced training set, which is then used to train the classifier","01eaa4af":"# DATA STANDARIZATION\n\nAs mentioned in the dataset description V1 to V28 are already and scaled with PCA now we are left with two factors \"AMOUNT\" and \"TIME\" , we have used StandardScaler function to standarize \"AMOUNT\" and as discuused earlier there is no need of \"TIME\" hence we have drooped it.","062298b6":"# Comparison of models\nBy looking at the confusin matrix and mathews correlation cofficient it is quite evident that RANDOM FOREST CLASSIFICATION technique is give us a satisfactory result .","48c3a1fe":"# **DECISON TREE CLASSIFIER**"}}