{"cell_type":{"652c3250":"code","26ad6f15":"code","301874e9":"code","4da4a45b":"code","cc153d8c":"code","0cd64dd5":"code","395bd14a":"code","f62cb06b":"code","8b9ec136":"code","d5e2476c":"code","33b51763":"code","1efbf991":"code","b6a27172":"code","193d29e9":"code","1e8e5972":"code","522969d7":"code","add6e554":"code","c618dbb2":"code","1e9d570b":"code","8a745469":"code","95ff82d7":"code","704cdb29":"code","5f7f5a65":"code","d8b538a5":"code","a6bf7225":"code","3d5568a5":"code","94301f61":"code","fb23bdc7":"code","8ef13fe5":"code","71bfc342":"code","a2aafe04":"code","6e5e1ae4":"code","e3cf7841":"code","1cdfb6b1":"code","a09cfa14":"code","c4225aa9":"code","e9a499f0":"code","2c0f0c3e":"code","dfffa20d":"code","a121e42e":"code","11f566ba":"code","b2955213":"code","5781065c":"code","d5a6dc61":"code","5b858d9b":"code","c5f35b80":"code","82c48c4a":"code","a59de65f":"code","e0f80dc9":"code","ddd6eaca":"code","d08dd9cc":"code","3a6f5a56":"code","e7d06934":"code","bc94f68b":"code","2e3cb000":"code","5e5f7451":"code","5b3ed18c":"code","f2042f75":"code","1019f758":"code","8c7952db":"code","2df87ebf":"code","7be1e312":"code","a1a39ff2":"code","2aeb5157":"code","08d967c0":"code","2ae74388":"code","4a58e777":"code","14960df8":"code","8cc9007c":"code","0d232b72":"code","ed1813f2":"code","c88713d5":"code","6f0f6a87":"code","bb334401":"code","0dd15a7e":"code","1ccdf064":"code","7d326ffd":"code","1a4b1b0b":"code","0aa43f2d":"code","489debc5":"code","736f017f":"code","e1e28c97":"code","464f5fe3":"code","1433bda9":"code","37eb37ff":"code","9f33b8a6":"code","fc4c6283":"code","ec435de4":"code","d0f50ccc":"code","1546c49c":"code","e45a0d3d":"code","357dbed8":"code","214f844e":"code","e34fa249":"code","6348afa7":"code","265637c7":"code","781a593f":"code","5eb34c7f":"code","5c35888f":"markdown","5ccbddec":"markdown","d8eaddee":"markdown","154a4a97":"markdown","1c4f32c9":"markdown","0ea57b00":"markdown","0032c7a3":"markdown","47651a81":"markdown","26e7ba3d":"markdown","bead0951":"markdown","75548927":"markdown","6a612bf3":"markdown","ed67f771":"markdown","eb68c67c":"markdown","5314f9b0":"markdown","68df4a79":"markdown","ed6e5146":"markdown","3cfdac0c":"markdown","746245c2":"markdown","82d97d79":"markdown","9f215f76":"markdown","0b026e1d":"markdown","04689914":"markdown","b4f12aa3":"markdown","59bbbbc6":"markdown","528c0e0c":"markdown","f8d83fcd":"markdown","486d49ef":"markdown","27bb652a":"markdown","282f4ccf":"markdown","78351dcc":"markdown","d61d06c9":"markdown","8453ff31":"markdown","db2443ee":"markdown","b8b1b6d2":"markdown","5843739a":"markdown","0eb47c28":"markdown","6ec827c5":"markdown","10ca0181":"markdown","e3af5936":"markdown","1b695d29":"markdown","29ed42ef":"markdown","16afceba":"markdown","10d2f664":"markdown","9e2fbcc6":"markdown","e4d896fb":"markdown","b0d57c4c":"markdown","641f4cef":"markdown","c415bd34":"markdown","4cff4fb1":"markdown","b8278d50":"markdown","87ca1a8a":"markdown","0b9db08c":"markdown","d9e7e499":"markdown","fecb1dd3":"markdown","1fb005c8":"markdown","571138b1":"markdown","df56e72f":"markdown","999e0d9e":"markdown","6297575a":"markdown","765f513c":"markdown","b55f83de":"markdown","667396fa":"markdown","843d3d71":"markdown","95ce4fc5":"markdown","15092fb0":"markdown","35502df9":"markdown","1813f47c":"markdown","4cccc278":"markdown","5463e1b9":"markdown","a5cd0656":"markdown","41d141a5":"markdown","3f105e08":"markdown","79e581fb":"markdown","7efb56a7":"markdown","1eb2da57":"markdown","12a6cead":"markdown","d5e0542e":"markdown","eab059f4":"markdown","a03f0d13":"markdown","64a1c5de":"markdown","7a4d8604":"markdown","ee7e67b1":"markdown","4b266bbe":"markdown","c29cf864":"markdown","374ded2e":"markdown","28ae0172":"markdown","6f7fe33b":"markdown","0ac93a4e":"markdown","a28a7318":"markdown","da93e223":"markdown","01470592":"markdown","51ee98c3":"markdown","ee7a13a8":"markdown","ed2e0519":"markdown","238b4c6f":"markdown","7f46f999":"markdown","b76261f0":"markdown"},"source":{"652c3250":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","26ad6f15":"import pandas_summary as ps\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer as Imputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso, ElasticNet\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport gc\n\nfrom scipy.stats import norm","301874e9":"sns.set()\nwarnings.simplefilter('ignore')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","4da4a45b":"all_metrics = []","cc153d8c":"folder = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/'\ntrain_df = pd.read_csv(folder+'train.csv')\ntest_df = pd.read_csv(folder+'test.csv')\nsub_df = pd.read_csv(folder+'sample_submission.csv')","0cd64dd5":"print('train: ', train_df.shape)\nprint('test: ', test_df.shape)\nprint('sample_submission: ', sub_df.shape)","395bd14a":"train_df.head()","f62cb06b":"train_df.info()","8b9ec136":"test_df.head()","d5e2476c":"test_df.info()","33b51763":"sub_df.head()","1efbf991":"dfs = ps.DataFrameSummary(train_df)\nprint('categoricals: ', dfs.categoricals.tolist())\nprint('numerics: ', dfs.numerics.tolist())\ndfs.summary()","b6a27172":"dfs = ps.DataFrameSummary(test_df)\nprint('categoricals: ', dfs.categoricals.tolist())\nprint('numerics: ', dfs.numerics.tolist())\ndfs.summary()","193d29e9":"train_df.drop('Id', inplace=True, axis=1)\ntest_df.drop('Id', inplace=True, axis=1)","1e8e5972":"ps.DataFrameSummary(train_df[['SalePrice']]).summary().T","522969d7":"plt.figure(figsize=(12,5))\nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","add6e554":"train_df['SalePrice'] = np.log(train_df['SalePrice'])","c618dbb2":"ps.DataFrameSummary(train_df[['SalePrice']]).summary().T","1e9d570b":"plt.figure(figsize=(12,5))\nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution');","8a745469":"y = train_df['SalePrice']\ntrain_df.drop(['SalePrice'], axis=1, inplace=True)","95ff82d7":"def missing_values_table(df, info=True):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        if info:\n            print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n                \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n                  \" columns that have missing values.\")\n        return mis_val_table_ren_columns","704cdb29":"missing_values_table(train_df).T","5f7f5a65":"missing_values_table(test_df).T","d8b538a5":"miss_df = missing_values_table(train_df)\ndrops = miss_df[miss_df['% of Total Values'] >75].index.tolist()\ntrain_df.drop(drops, inplace=True, axis=1)\ntest_df.drop(drops, inplace=True, axis=1)","a6bf7225":"print('Dropped: ', drops)","3d5568a5":"del miss_df\ngc.collect();","94301f61":"dfs = ps.DataFrameSummary(train_df)\ncat_cols = dfs.categoricals.tolist() + dfs.bools.tolist()\nnum_cols = dfs.numerics.tolist()","fb23bdc7":"train_df[cat_cols] = train_df[cat_cols].fillna('?')\ntest_df[cat_cols] = test_df[cat_cols].fillna('?')","8ef13fe5":"class MeanEncoding(BaseEstimator):\n    \"\"\"   In Mean Encoding we take the number \n    of labels into account along with the target variable \n    to encode the labels into machine comprehensible values    \"\"\"\n    \n    def __init__(self, feature, C=0.1):\n        self.C = C\n        self.feature = feature\n        \n    def fit(self, X_train, y_train):\n        \n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n        \n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        \n        self.encoding = (self.global_mean * self.C + mean * size) \/ (self.C + size)\n    \n    def transform(self, X_test):\n        \n        X_test[self.feature] = X_test[self.feature].map(self.encoding).fillna(self.global_mean).values\n        \n        return X_test\n    \n    def fit_transform(self, X_train, y_train):\n        \n        df = pd.DataFrame({'feature': X_train[self.feature], 'target': y_train}).dropna()\n        \n        self.global_mean = df.target.mean()\n        mean = df.groupby('feature').target.mean()\n        size = df.groupby('feature').target.size()\n        self.encoding = (self.global_mean * self.C + mean * size) \/ (self.C + size)\n        \n        X_train[self.feature] = X_train[self.feature].map(self.encoding).fillna(self.global_mean).values\n        \n        return X_train","71bfc342":"for f in cat_cols:\n    me = MeanEncoding(f, C=0.1)\n    me.fit(train_df, y)\n    train_df = me.transform(train_df)\n    test_df = me.transform(test_df)","a2aafe04":"imputer = Imputer(strategy=\"mean\")\nimputer.fit(train_df[num_cols])\ntrain_df[num_cols] = imputer.transform(train_df[num_cols])\ntest_df[num_cols] = imputer.transform(test_df[num_cols])","6e5e1ae4":"dfs = ps.DataFrameSummary(train_df)\ndfs.summary()","e3cf7841":"dfs = ps.DataFrameSummary(test_df)\ndfs.summary()","1cdfb6b1":"train_df.hist(figsize=(35, 30));","a09cfa14":"train_corr = train_df.corr()\n# plot the heatmap and annotation on it\nfig, ax = plt.subplots(figsize=(18,18))\nsns.heatmap(train_corr, xticklabels=train_corr.columns, yticklabels=train_corr.columns, annot=True, ax=ax);","c4225aa9":"high_corr = pd.DataFrame(train_corr[(train_corr > 0.8) & (train_corr != 1)].fillna(0).sum(axis=0))\nhigh_corr[high_corr[0]>0]","e9a499f0":"drops = ['Exterior2nd', '1stFlrSF', 'GrLivArea', 'Fireplaces', 'GarageCars', 'GarageCond', 'SaleType']\ntrain_df.drop(drops, inplace=True, axis=1)\ntest_df.drop(drops, inplace=True, axis=1)","2c0f0c3e":"# scale data\nscaler = StandardScaler().fit(train_df)\ntrain_df = scaler.transform(train_df)\ntest_df = scaler.transform(test_df)","dfffa20d":"X_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2, random_state=10)","a121e42e":"# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\nX_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \ntheta_best_analytic = np.linalg.inv(X_train_with_c.T.dot(X_train_with_c)).dot(X_train_with_c.T).dot(y_train)\npd.DataFrame(theta_best_analytic).T","11f566ba":"X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test]\ny_pred_simple = X_test_with_c.dot(theta_best_analytic)","b2955213":"all_metrics.append(['analytical_solution', mean_squared_error(y_test, y_pred_simple)])\nmean_squared_error(y_test, y_pred_simple)","5781065c":"lr = LinearRegression()\nlr.fit(X_train, y_train)","d5a6dc61":"all_metrics.append(['sklearn_LR', mean_squared_error(y_test, lr.predict(X_test))])\nmean_squared_error(y_test, lr.predict(X_test))","5b858d9b":"# error expectation\nMU = np.round(np.sum((y_test - lr.predict(X_test)))\/y_test.shape[0], 5)\nMU","c5f35b80":"# error variance\nVAR = ((y_test - lr.predict(X_test)) - np.mean(y_test - lr.predict(X_test))).mean()\nVAR","82c48c4a":"# error variance is not generally constant\nfig, ax = plt.subplots(figsize=(12,5))\nplt.plot(((y_test - lr.predict(X_test)) - np.mean(y_test - lr.predict(X_test))))\nfig.suptitle(\"Pic. Error spread relative to their mean value\", \\\n            fontsize = 14, y = 1.03)\nax.axhline(y=MU, color='grey', ls='--');","a59de65f":"def sample_corr(arr, it=10000):\n    arr = arr.values.reshape(-1,1)\n    val_list = []\n    for iti in range(it):\n        random_index = list(set(np.random.randint(arr.shape[0], size=2)))\n        val_list.append(np.sum(arr[random_index, :]))\n        \n    return np.mean(val_list)","e0f80dc9":"sample_corr((y_test - lr.predict(X_test)))","ddd6eaca":"def gradient_descent(mu, x, y, params, numIterations, lf='MSE', prnt=False):\n    \"\"\"\n\u00a0\u00a0\u00a0\u00a0 The function implements a batch gradient descent algorithm.\n\u00a0\u00a0\u00a0\u00a0 mu - learning rate\n\u00a0\u00a0\u00a0\u00a0 params - number of parameters, including free parameter\n\u00a0\u00a0\u00a0\u00a0 numIterations - number of iterations (int)\n\u00a0\u00a0\u00a0\u00a0 prnt - whether or not to print the calculation, if prnt = True, \n     then at every hundredth step the value of the loss function is displayed\n\u00a0\u00a0\u00a0\u00a0 lf - loss function, default 'MSE', you can select 'MAE'\n    \"\"\"\n    \n    n = x.shape[0] #\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0439 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    theta = np.ones(params).reshape(params,1) # [ 1.  1. 1.] - \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u043f\u0443\u0441\u0442\u044c \u0431\u0443\u0434\u0443\u0442 \u0440\u0430\u0432\u043d\u044b 1\n    x_transpose = x.transpose() # \u0442\u0440\u0430\u043d\u0441\u043f\u043e\u043d\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u043c\u0430\u0442\u0440\u0438\u0446\u0430 x\n    for iter in range( 0, numIterations ):\n        hypothesis = np.dot(x, theta) # \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u0435\n        loss = hypothesis - y.values.reshape(len(y),1) # \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0441\u0442\u0430\u0442\u043a\u0430\n        \n        if lf=='MSE':\n            J = np.sum(loss ** 2) \/ n  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c (\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u044b)\n            if prnt and (iter % 10000)==0:\n                print( \"iter %s | MSE: %.3f\" % (iter, J) )\n        \n        elif lf=='MAE':\n            J = np.sum(abs(loss)) \/ n  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c (\u043c\u043e\u0434\u0443\u043b\u0438)\n            if prnt and (iter % 10000)==0:\n                print( \"iter %s | MAE: %.3f\" % (iter, J) )\n        \n        gradient = np.dot(x_transpose, loss) * 2 \/ n         \n        theta = theta - mu * gradient  # update\n    \n    return (theta)","d08dd9cc":"%%time\n# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\nX_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n# run the gradient descent algorithm\n\ntheta_best_gd = gradient_descent(0.0001, X_train_with_c, y_train, params=X_train_with_c.shape[1], numIterations=100000, lf='MSE', prnt=True)","3a6f5a56":"# received parameters (gradient descent)\npd.DataFrame(np.round(theta_best_gd, 3)).T","e7d06934":"# received parameters (analytical solution)\npd.DataFrame(np.round(theta_best_analytic, 3)).T","bc94f68b":"# take new points from the test sample and apply the coefficients to calculate the function value from them,\n# obtained by our gradient descent algorithm\nX_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \ny_pred_gd = X_test_with_c.dot(theta_best_gd)","2e3cb000":"all_metrics.append(['Gradient_Descent', mean_squared_error(y_test, y_pred_gd)])\nmean_squared_error(y_test, y_pred_gd)","5e5f7451":"def learning_schedule(val, p1=100, p2=50):\n    \"\"\"The function takes parameters as arguments\n    p1 - parameter for the numerator (default is 100),\n    p2 - parameter for the denominator (default is 50),\n    val - value set by user\n    and returns a simple conversion\n    return p1\/(p2 + val)\n    \"\"\"\n    return p1\/(p2 + val)","5b3ed18c":"def st_gradient_descent(x, y, params, num_epochs, num_iter, lf='MSE', prnt=False):\n    \"\"\"\n     The function implements the stochastic gradient descent algorithm.\n\u00a0\u00a0\u00a0\u00a0 mu - learning rate\n\u00a0\u00a0\u00a0\u00a0 params - number of parameters, including free parameter\n\u00a0\u00a0\u00a0\u00a0 num_epochs - number of eras\n\u00a0\u00a0\u00a0\u00a0 num_iter - the number of iterations (int) in the era\n\u00a0\u00a0\u00a0\u00a0 prnt - print or not calculations every 100 iterations\n\u00a0\u00a0\u00a0\u00a0 lf - loss function, default 'MSE', you can select 'MAE'\n\u00a0\u00a0\u00a0\u00a0 Without the learning_schedule () function, it won\u2019t work.\n    \"\"\"\n    \n    n = x.shape[0] #\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0439 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    theta = np.ones(params).reshape(params,1) # [ 1.  1.  1.] - \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u043f\u0443\u0441\u0442\u044c \u0431\u0443\u0434\u0443\u0442 \u0440\u0430\u0432\u043d\u044b 1\n    y = y.values.reshape(-1,1)\n    lr=0.0001\n    for epoch in range(num_epochs):\n        for iteration in range(num_iter):\n            random_index = np.random.randint(n)\n            x_rand = x[random_index, :].reshape(1, params)\n            y_rand = y[random_index, :]\n\n            hypothesis = np.dot(x_rand, theta) # \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u0435\n            loss = hypothesis - y_rand.reshape(-1,1) # \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0441\u0442\u0430\u0442\u043a\u0430\n\n            if lf=='MSE':\n                J = np.sum(loss ** 2) \/ n  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c (\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u044b)\n                if prnt and (iteration % 10000)==0:\n                    print( \"epoch %s | iter %s | MSE: %.3f\" % (epoch, iteration, J) )\n\n            elif lf=='MAE':\n                J = np.sum(abs(loss)) \/ n  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c (\u043c\u043e\u0434\u0443\u043b\u0438)\n                if prnt and (iteration % 10000)==0:\n                    print( \"iter %s | MAE: %.3f\" % (iteration, J) )\n\n            gradient = np.dot(x_rand.transpose(), loss) * 2 \/ n  \n            lr = learning_schedule(val=100, p1=1, p2=10000)\n            \n            theta = theta - lr * gradient  # update\n    \n    return (theta)","f2042f75":"%%time\ntheta_best_sgd = st_gradient_descent(X_train_with_c, y_train, params=X_train_with_c.shape[1], num_epochs=100, num_iter=2000, lf='MSE', prnt=True)","1019f758":"# received parameters (SGD)\npd.DataFrame(np.round(theta_best_sgd, 3)).T","8c7952db":"X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \ny_pred_sgd = X_test_with_c.dot(theta_best_sgd)","2df87ebf":"all_metrics.append(['Stochastic_Gradient_Descent', mean_squared_error(y_test, y_pred_sgd)])\nmean_squared_error(y_test, y_pred_sgd)","7be1e312":"sgd_reg = SGDRegressor(max_iter=100000, penalty=None, eta0=0.1, random_state=42)\nsgd_reg.fit(X_train_with_c, y_train.ravel())","a1a39ff2":"print('Parameters returned by SGDRegressor from the sklearn library:')\ntheta_best_sgd_sklearn = np.array(sgd_reg.coef_)\npd.DataFrame(np.round(theta_best_sgd_sklearn, 3)).T","2aeb5157":"X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \ny_pred_sk_sgd = X_test_with_c.dot(theta_best_sgd_sklearn)","08d967c0":"all_metrics.append(['Sklearn_Stochastic_Gradient_Descent', mean_squared_error(y_test, y_pred_sk_sgd)])\nmean_squared_error(y_test, y_pred_sk_sgd)","2ae74388":"def mb_gradient_descent(x, y, mu, params, num_epochs, num_iter, batch_size=0.2, lf='MSE', prnt=False):\n    \"\"\"\n    The function implements a mini-batch gradient descent algorithm.\n    mu - learning rate\n    params - number of parameters, including free parameter\n    num_iter - the number of iterations (int) in the era\n    batch_size - packet size for one iteration\n    prnt - print or not computation\n    lf - loss function, default 'MSE', you can select 'MAE'\n    \"\"\"\n    \n    n = x.shape[0] #\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0439 \u0432 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\n    theta = np.ones(params).reshape(params,1) # [ 1.  1.  1.] - \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u043f\u0443\u0441\u0442\u044c \u0431\u0443\u0434\u0443\u0442 \u0440\u0430\u0432\u043d\u044b 1\n    y = y.values.reshape(-1,1)\n    \n    for epoch in range(num_epochs):\n        for iteration in range(num_iter):\n            random_index = list(set(np.random.randint(200, size=round(n*batch_size)).tolist()))\n            x_rand = x[random_index, :].reshape(len(random_index), params)\n            y_rand = y[random_index, :]\n\n            hypothesis = np.dot(x_rand, theta) # \u043c\u0430\u0442\u0440\u0438\u0447\u043d\u043e\u0435 \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u0435\n            loss = hypothesis - y_rand # \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u043e\u0441\u0442\u0430\u0442\u043a\u0430\n\n            if lf=='MSE':\n                J = np.sum(loss ** 2) \/ n  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c (\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u044b)\n                if prnt and (iteration % 10000)==0:\n                    print( \"epoch %s | iter %s | MSE: %.3f\" % (epoch, iteration, J) )\n\n            elif lf=='MAE':\n                J = np.sum(abs(loss)) \/ n  # \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043f\u043e\u0442\u0435\u0440\u044c (\u043c\u043e\u0434\u0443\u043b\u0438)\n                if prnt and (iteration % 10000)==0:\n                    print( \"iter %s | MAE: %.3f\" % (iteration, J) )\n\n            gradient = np.dot(x_rand.transpose(), loss) * 2 \/ n  \n            theta = theta - mu * gradient  # update\n    \n    return (theta)","4a58e777":"%%time\ntheta_best_mbgs = mb_gradient_descent(X_train_with_c, y_train, mu=0.001, params=X_train_with_c.shape[1], num_epochs=50, num_iter=2000, batch_size=0.2, lf='MSE', prnt=True)\npd.DataFrame(np.round(theta_best_mbgs, 3)).T","14960df8":"X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \ny_pred_mbgs = X_test_with_c.dot(theta_best_mbgs)","8cc9007c":"all_metrics.append(['Mini_batch_gradient_descent', mean_squared_error(y_test, y_pred_mbgs)])\nmean_squared_error(y_test, y_pred_mbgs)","0d232b72":"# we create data for training the model in the amount of m\nm=100\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) \/ 1.5\n# create data to test the model in the amount of m\nX_new = np.linspace(0, 3, m).reshape(m, 1)\ny_new =  1 + 0.5 * X_new + np.random.randn(m, 1) \/ 1.5\n\n# prepare 6 graphs for visualizing the forecast with a change in alpha\nfig = plt.figure(figsize = (14,6))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n# \u0441\u043f\u0438\u0441\u043e\u043a \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 alpha\nalpha_list = [0, 0.00001, 0.002, 0.5, 0.8, 1]\nfor n,i in zip([ni for ni in range(1,len(alpha_list)+1)], alpha_list):\n    model = Ridge(i) # sklearn ridge regression model with alpha\n    poly_features = PolynomialFeatures(degree=18, include_bias=False) # PolynomialFeatures \u0441 \u0441\u0442\u0435\u043f\u0435\u043d\u044c\u044e 18\n    # expanding data for training with polynomial features\n    \u0425_poly_features = poly_features.fit_transform(X)\n    # we scale advanced data for training\n    X_poly_features_scaled = StandardScaler().fit_transform(\u0425_poly_features)\n    # we scale the target variable for training\n    y_scaled = StandardScaler().fit_transform(y)\n    \n    model.fit(X_poly_features_scaled, y_scaled) # \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c Ridge\n    \n    # doing the procedure with the data to check (X_new)\n    X_new_poly_features = poly_features.fit_transform(X_new) # \u0440\u0430\u0441\u0448\u0438\u0440\u044f\u0435\u043c\n    X_new_poly_features_scaled = StandardScaler().fit_transform(X_new_poly_features) # \u0448\u043a\u0430\u043b\u0438\u0440\u0443\u0435\u043c\n\n    y_pred = model.predict(X_new_poly_features_scaled) # \u0434\u0435\u043b\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n    \n    # plot\n    ax = fig.add_subplot(2,3,n)\n    # all x and y will be drawn in a scaled version\n    ax.plot(StandardScaler().fit_transform(X_new), y_pred, \"r\", label='X_new', linewidth=4)\n    ax.plot(StandardScaler().fit_transform(X_new), StandardScaler().fit_transform(y_new), \"b.\")\n    ax.set_title(\"alpha={}\".format(i), fontsize = 10)\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"y\")\n    ax.set_ylim((-3, 3))\n    fig.suptitle(\"Pic. Forecast change with increasing coefficient limit\", \\\n            fontsize = 14, y = 1.03)\nfig.tight_layout()\nfig.show()","ed1813f2":"# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\nX_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \ntheta_best_analytic = np.linalg.inv(X_train_with_c.T.dot(X_train_with_c)).dot(X_train_with_c.T).dot(y_train)\npd.DataFrame(theta_best_analytic).T","c88713d5":"# set the alpha parameter\nalpha = 0.5\n# create matrix A\nA = np.zeros((theta_best_analytic.shape[0],theta_best_analytic.shape[0]))\nfor s,c in zip([si for si in range(A.shape[0])], [ci for ci in range(A.shape[1])]):\n    A[s,c] = alpha\nA[0,0] = 0\nA","6f0f6a87":"theta_best_ridge = np.linalg.inv(X_train_with_c.T.dot(X_train_with_c) + alpha*A).dot(X_train_with_c.T).dot(y_train)\npd.DataFrame(np.round(theta_best_ridge, 3)).T","bb334401":"X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \npd.DataFrame(X_test_with_c[:5,:])","0dd15a7e":"# make a prediction using our model\ny_pred_ridge = X_test_with_c.dot(theta_best_ridge)\nall_metrics.append(['Ridge', mean_squared_error(y_test, y_pred_ridge)])\nmean_squared_error(y_test, y_pred_ridge)","1ccdf064":"model_ridge = Ridge(alpha=1)\nmodel_ridge.fit(X_train, y_train)","7d326ffd":"all_metrics.append(['sklearn_Ridge', mean_squared_error(y_test, model_ridge.predict(X_test))])\nmean_squared_error(y_test, model_ridge.predict(X_test))","1a4b1b0b":"def gradient_descent(mu, x, y, params, numIterations, lf='MSE', prnt=False):\n    \"\"\"\n    The function implements a batch gradient descent algorithm.\n    mu - learning rate\n    params - number of parameters, including free parameter\n    numIterations - number of iterations (int)\n    prnt - whether or not to print the calculation, if prnt = True, then at every hundredth step the value of the loss function is displayed\n    lf - loss function, default 'MSE', you can select 'MAE'\n    \"\"\"\n    \n    n = x.shape[0] # number of observations in the sample\n    theta = np.ones(params).reshape(params,1) # [ 1.  1. 1.] - \u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u043e\u0432 \u043f\u0443\u0441\u0442\u044c \u0431\u0443\u0434\u0443\u0442 \u0440\u0430\u0432\u043d\u044b 1\n    x_transpose = x.transpose() # transposed matrix x\n    #print('y', y.shape)\n    for iter in range( 0, numIterations ):\n        hypothesis = np.dot(x, theta) # matrix multiplication\n        loss = hypothesis - y.values.reshape(len(y),1) # residue value\n        \n        if lf=='MSE':\n            J = np.sum(loss ** 2) \/ n  # loss function (squares)\n            if prnt and (iter % 10000)==0:\n                print( \"iter %s | MSE: %.3f\" % (iter, J) )\n        \n        elif lf=='MAE':\n            J = np.sum(abs(loss)) \/ n  # loss function (modules)\n            if prnt and (iter % 10000)==0:\n                print( \"iter %s | MAE: %.3f\" % (iter, J) )\n        \n        gradient = np.dot(x_transpose, loss) * 2 \/ n         \n        theta = theta - mu * gradient  # update\n    \n    return (theta)","0aa43f2d":"def gradient_descent_with_subgrad(mu, x, y, alpha, params, numIterations, prnt=False):\n    \"\"\"\n    The function implements a batch gradient descent algorithm using a subgradient with x values equal to 0.\n    mu - learning rate\n    alpha - hyperparameter of the Lasso regression loss function\n    params - number of parameters, including free parameter\n    numIterations - number of iterations (int)\n    prnt - whether or not to print the calculation, \n    if prnt = True, then at every hundredth step the value of the loss function is displayed\n    \"\"\"\n    \n    n = x.shape[0] # number of observations in the sample\n    theta = np.ones(params).reshape(params,1) # [ 1.  1. 1.] - initial values of the coefficients let be equal to 1\n    x_transpose = x.transpose() # transposed matrix x\n    \n    for iter in range( 0, numIterations ):\n        hypothesis = np.dot(x, theta) # matrix multiplication\n        loss = hypothesis - y.values.reshape(-1,1) # residue value\n        \n        \n        J = np.sum(loss ** 2) \/ n  + alpha*np.sum(np.abs(theta[1:,:]))# loss function (squares)\n        if prnt and (iter % 10000)==0:\n            print( \"iter %s | MSE + alpha*sum(abs(theta)): %.3f\" % (iter, J) )\n        \n        if np.sum(theta == 0)>0: # if at least one of theta parameters is zero\n            gradient = np.dot(x_transpose, loss) * 2 \/ n # the gradient is initially calculated as usual\n            # but we add one more term to the gradient to circumvent the non-differentiability of the loss function at zeros\n            term = (gradient > 0)*np.ones(gradient.shape[0]).reshape(gradient.shape[0],1) + \\\n            (gradient < 0)*np.ones(gradient.shape[0]).reshape(gradient.shape[0],1)*(-1)\n            # add this term to the gradient\n            gradient = gradient + alpha*term.reshape(gradient.shape[0], 1)\n        else:\n            gradient = np.dot(x_transpose, loss) * 2 \/ n # in other cases, the gradient is calculated as usual\n            \n        theta = theta - mu * gradient  # update\n    \n    return (theta)","489debc5":"%%time\n# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\nX_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n# run the gradient descent algorithm\ntheta_best_lasso = gradient_descent_with_subgrad(0.0001, X_train_with_c, y_train, 0.5, params=X_train_with_c.shape[1], numIterations=100000, prnt=True)","736f017f":"pd.DataFrame(np.round(theta_best_lasso, 3)).T","e1e28c97":"X_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \npd.DataFrame(X_test_with_c[:5,:])","464f5fe3":"# make a prediction using our model\ny_pred_lasso = X_test_with_c.dot(theta_best_lasso)","1433bda9":"all_metrics.append(['Lasso', mean_squared_error(y_test, y_pred_lasso)])\nmean_squared_error(y_test, y_pred_lasso)","37eb37ff":"lasso_reg = Lasso(alpha=0.5)\nlasso_reg.fit(X_train, y_train)\nall_metrics.append(['sklearn_Lasso', mean_squared_error(y_test, lasso_reg.predict(X_test))])\nmean_squared_error(y_test, lasso_reg.predict(X_test))","9f33b8a6":"def gradient_descent_with_subgrad_for_elastic_net(mu, x, y, alpha, r, params, numIterations, prnt=False):\n    \"\"\"\n    The function implements a batch gradient descent algorithm using a subgradient with x values equal to 0.\n    mu - learning rate\n    alpha - hyperparameter of the Lasso regression loss function\n    params - number of parameters, including free parameter\n    numIterations - number of iterations (int)\n    prnt - whether or not to print the calculation, if prnt = True, \n    then at every hundredth step the value of the loss function is displayed\n    \"\"\"\n    \n    n = x.shape[0] # number of observations in the sample\n    theta = np.ones(params).reshape(params,1) # [ 1.  1. 1.] - initial values of the coefficients let be equal to 1\n    x_transpose = x.transpose() # transposed matrix x\n    \n    for iter in range( 0, numIterations ):\n        hypothesis = np.dot(x, theta) # matrix multiplication\n        loss = hypothesis - y.values.reshape(-1,1) # residue value\n        \n        J = np.sum(loss ** 2) \/ n  + r*alpha*np.sum(np.abs(theta[1:,:])) + (1-r)\/2 * alpha*np.sum(theta[1:,:]**2)\n        if prnt and (iter % 10000)==0:\n            print( \"iter %s | MSE+r*alpha*sum(abs(theta))+(1-r)\/2*alpha(theta^2): %.3f\" % (iter, J) )\n        \n        if np.sum(theta == 0)>0: # if at least one of theta parameters is zero\n            gradient = np.dot(x_transpose, loss) * 2 \/ n # the gradient is initially calculated as usual\n            # but we add one more term to the gradient to circumvent the non-differentiability of the loss function at zeros\n            term = (gradient > 0)*np.ones(gradient.shape[0]).reshape(gradient.shape[0],1) + \\\n            (gradient < 0)*np.ones(gradient.shape[0]).reshape(gradient.shape[0],1)*(-1)\n            # add this term to the gradient\n            gradient = gradient + alpha*term.reshape(gradient.shape[0], 1)\n        else:\n            gradient = np.dot(x_transpose, loss) * 2 \/ n # in other cases, the gradient is calculated as usual\n            \n        theta = theta - mu * gradient  # update\n    \n    return (theta)","fc4c6283":"%%time\n# add a unit column to the left of the observation matrix (theta zero parameter will be multiplied by 1)\nX_train_with_c = np.c_[np.ones((X_train.shape[0], 1)), X_train] \n# run the gradient descent algorithm\ntheta_best_enet = gradient_descent_with_subgrad_for_elastic_net(0.0001, X_train_with_c, y_train, 0.5, 0.5, params=X_train_with_c.shape[1], numIterations=100000, prnt=True)","ec435de4":"pd.DataFrame(np.round(theta_best_enet, 3)).T","d0f50ccc":"# take test data\n# add left column vector with units\nX_test_with_c = np.c_[np.ones((X_test.shape[0], 1)), X_test] \npd.DataFrame(X_test_with_c[:5,:])","1546c49c":"# make a prediction using our model\ny_pred_elnet = X_test_with_c.dot(theta_best_enet)","e45a0d3d":"all_metrics.append(['Elastic_net', mean_squared_error(y_test, y_pred_elnet)])\nmean_squared_error(y_test, y_pred_elnet)","357dbed8":"elastic_net = ElasticNet(alpha=0.5, l1_ratio=0.5, random_state=42)\nelastic_net.fit(X_train, y_train)\nall_metrics.append(['sklearn_Elastic_net', mean_squared_error(y_test, elastic_net.predict(X_test))])\nmean_squared_error(y_test, elastic_net.predict(X_test))","214f844e":"pd.DataFrame(all_metrics, columns=['Algo', 'RMSE']).sort_values('RMSE')","e34fa249":"test_df_with_c = np.c_[np.ones((test_df.shape[0], 1)), test_df] \npd.DataFrame(test_df_with_c[:5,:])","6348afa7":"log_predict = test_df_with_c.dot(theta_best_ridge)","265637c7":"np.exp(log_predict)[:5]","781a593f":"sub_df['SalePrice'] = np.round(np.exp(log_predict), 0)\nsub_df.head()","5eb34c7f":"sub_df.to_csv('submission.csv',index=False)","5c35888f":"__Gradient (matrix of partial derivatives of the loss function for each parameter from $\\theta_0$ to $\\theta_i$) of the loss function $MSE$ will look like this:__","5ccbddec":"# Read data","d8eaddee":"Lasso regression is another type of regularized regression models in which the L1 norm of the weight vector $\\theta$ is used instead of 1\/2 the norm square of the weight vector $\\theta$ (as in the case of ridge regression).\n\nLasso regression loss function:\n\n$$ J(\\theta) = MSE(X,h_\\theta) + \\alpha \\sum_{i=1}^m |\\theta_i|$$\n\nWhere:\n\n$\\alpha$ - normalization parameter, determines how much we regularize the model. With $\\alpha=0$, the lasso regression becomes just a linear regression. When $\\alpha$ is close to 1, the coefficients (weights) of the model tend to zero (the least important signs are reset first).\n\n$\\theta_i$ - model parameters from $\\theta_1$ to $\\theta_m$. Moreover, $\\theta_0$ (parameter with a free term) is not regularized.","154a4a97":"Where:\n\n$\\hat{y}_k$ - calculated value obtained from the $k$ - observation vector using the model;\n\n$\\theta_i$ - value of the $i- parameter of the model;\n\n$x_{i}^{(k)}$ - the value of the $i$- characteristic of the $k$- vector of the $X$ matrix of size $n$*$(m+1)$. ","1c4f32c9":"#### Batch gradient descent","0ea57b00":"\n___Linear regression calculates the weighted sum of input values (feature values) and free term .___\n\n__The formula for predicting linear regression in the analytical form:__","0032c7a3":"### Let's logarithm the value of the house","47651a81":"Now you can use our trained model for forecasts. Let's make a forecast for test.","26e7ba3d":"##### Implementation of design work tasks:\n\nTo put into practice the following regression models:\n\n* Linear regression. \n* Ridge regression. \n* Lasso regression. \n* Elastic network. \n\nTo apply optimization methods:\n\n* Analytical solutions. \n* Gradient descent:\n* batch. \n* stochastic. \n* mini batch. \n\u00a0\u00a0\u00a0\u00a0\n\nCollect and prepare data, solve the problem of comparability (identical information in different sources is indicated by different names, often measurements are made in different units) and incomplete data (data are not available for long periods) in the following ways:\n\n* Processing of data provided by the customer (work with missing values, aggregation of indicators by periods). \n* Enrichment of data with data obtained by the API from open sources. \n* Designing signs that operate on different phases of the price of houses. ","bead0951":"This modification of the algorithm combines the features of a batch and stochastic implementation.\nThe mini batch gradient descent calculates gradients on small randomly sampled data samples (__mini-batch__).\nThe larger the package, the less the wandering of the algorithm with respect to the shortest path from the starting point to the local minimum.\nIn general, the mini-batch gradient descent is selected closer to the minimum than the stochastic version.\nOn the other hand, this algorithm is more difficult to get away from local minima, especially when there are a lot of them.\nGiven a well-chosen training schedule, the mini-batch gradient descent method allows you to reach a minimum faster than the batch version.","75548927":"$$\\hat{y}_k = \\theta_0 + \\theta_1 x_{1}^{(k)} + \\theta_2 x_{2}^{(k)} + ... + \\theta_i x_{i}^{(k)} + ... + \\theta_m x_{m}^{(n)}$$","6a612bf3":"# Stochastic gradient descent House Prices: Advanced Regression Techniques","ed67f771":"___There are 2 types of training for this model:___\n\u00a0\u00a0\u00a0\u00a0\n- the use of a direct equation in an analytical form that directly calculates the model parameters that are most suitable for a particular data set.\n- application of the gradient descent method, which iteratively selects the model parameters, reducing the values of the loss function.","eb68c67c":"Compare to sklearn implementation","5314f9b0":"Now you can use our trained model for forecasts. Take the points from the test set and make a prediction.","68df4a79":"$$MSE(X,h_\\theta)_i' = \\frac{d}{d\\theta_i} MSE(X,h_\\theta) = \n(\\frac{1}{n}\\sum_{k=1}^n (h_\\theta(X_k) - y_k)^2)' =  (\\frac{1}{n}\\sum_{k=1}^n (\\theta^T X_k - y_k)^2)' = \n\\frac{2}{n}\\sum_{k=1}^n (\\theta^T X_k - y_k) \\sum_{k=1}^m x_{i}^{(k)}$$ ","ed6e5146":"Compare with a similar method from the sklearn library:","3cfdac0c":"We got some values of the coefficients $\\theta$, but we understand that they are obtained from data in which there is noise.","746245c2":"__Calculation of the next step of the gradient descent:__","82d97d79":"For the MSE loss function, the partial derivative with respect to $\\theta_i$ will look like this:","9f215f76":"__$R^2$ (Share of predicted fluctuations in the total number of fluctuations in the data):__\n\n$$R^2 (X,h_\\theta) = 1 - \\frac{\\sum_{k=1}^n (h_\\theta(X_k) - y_k)^2}{\\sum_{k=1}^n (h_\\theta(X_k) - \\bar{y})^2} = 1 - \\frac{RSS}{TSS}$$","0b026e1d":"### Ridge-regression ($L_2$ -regularization) House Prices: Advanced Regression Techniques","04689914":"# Conclusions:","b4f12aa3":"RMSE is also very close to sklearn and far from our analytical solution:","59bbbbc6":"### We will use our own ridge regression algorithm.","528c0e0c":"# Get target","f8d83fcd":"# Fit\n\nRegression models are the simplest and at the same time effective machine learning models that give interpretable results.\nHowever, using ready-made implementations from popular libraries, such as Scikit-Learn, with little thought about what is happening under the hood, this is one thing.\nBut to implement all the features of these algorithms with your own code is completely different.\n\nIn this project work, I plan to thoroughly understand the theoretical foundations of regression models (including regularized ones), how to optimize them (analytical solutions, various gradient descent implementations), implement these algorithms myself and compare them with the implementation in Scikit-Learn.\n\nIn the framework of this work, regression models will be considered that are used to predict continuous random variables (logistic regression is not considered).","486d49ef":"The ridge regression loss function may have the form:\n\n$$ J(\\theta) = MSE(X,h_\\theta) + \\frac{1}{2}\\alpha \\sum_{i=1}^m \\theta_i^2$$\n\n$$ J(\\theta) = RMSE(X,h_\\theta) + \\frac{1}{2}\\alpha \\sum_{i=1}^m \\theta_i^2$$\n\n$\\alpha$ - normalization parameter, determines how much we regularize the model. With $\\alpha=0$, ridge regression becomes just a linear regression. With $\\alpha$ close to 1, the best possible coefficients (weights) of the model tend to zero.\n\n$\\theta_i$ - model parameters from $\\theta_1$ to $\\theta_m$.\n\n`Note to the expression`: $\\theta_0$ (parameter with a free term) is not regularized! The sum $\\sum_{i = 1}^m$ starts at 1, not 0.\n\nOf the various regularization options, we will use first the formula.","27bb652a":"__The principle of the gradient descent method is to move along a function from some random point in the direction of the anti-gradient. At the same time, the step size (learning rate) is very important, which, if too large, will not allow us to step to the minimum, and if too small, we will go down to the minimum for a very long time .__","282f4ccf":"## Select columns with high correlation  to drop","78351dcc":"__Root Mean Squared Error - RMSE (square root of root mean square error):__\n\n$$RMSE(X,h_\\theta) = \\sqrt{\\frac{1}{n}\\sum_{k=1}^n (h_\\theta(X_k) - y_k)^2}$$","d61d06c9":"___Everything is clear with the model. How to train it? To teach this means to set her such parameters with which she will most accurately predict the values of the target variable on the training data .___\n\n___What does \"predict as accurately as possible\" mean? What measure can be used?___","8453ff31":"# Gradient Descent House Prices: Advanced Regression Techniques","db2443ee":"One solution to the latter drawback is to gradually decrease the ___learning rate___. This approach is called ___simulated annealing___. A special function responsible for changing the learning speed is called the ___learning schedule___. As with the selection of the learning rate, there is no exact answer on how to set it, so with the rate of decrease in the learning rate you need to be careful not to stop ahead of time or to jump over the global minimum.","b8b1b6d2":"Thank you!","5843739a":"### Categoricals & Numerics columns","0eb47c28":"$$\nMSE(X,h_\\theta) + \\alpha\n\\left(\\begin{array}{cc} \nsign (\\theta_1) \\\\\nsign (\\theta_2) \\\\\n... \\\\\nsign (\\theta_n)\n\\end{array}\\right) \n$$ \n\nWhere:\n\n$sign (\\theta_i) = \\begin{cases} \n\\displaystyle -1,  \\text{\u0435\u0441\u043b\u0438 $\\theta_i$ < 0} \\\\\n\\displaystyle 0,  \\text{\u0435\u0441\u043b\u0438 $\\theta_i$ = 0} \\\\\n\\displaystyle +1, \\text{\u0435\u0441\u043b\u0438 $\\theta_i$ > 0}\n\\end{cases}$","6ec827c5":"This modification of the algorithm allows you to bypass the problem that the batch method suffers, namely, the need to work with the entire set of training data.\nAt each step, the stochastic gradient descent selects one random sample from the training set and calculates the gradient only on the basis of this sample.\nThis greatly increases the speed of work, but instead of translating to the minimum of the loss function, we will observe a wandering indicator of the loss function, which will decrease only on average.\nHaving reached a minimum, the algorithm will continue to \"rush\" in its vicinity. The final values of the parameters $\\theta$ will be good, but not optimal.","10ca0181":"# Linear Model Guide\nIn statistics, linear regression is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.","e3af5936":"A data model is correctly specified if the data has the following properties:\n\n1. All $X_k$ are deterministic and not all are equal to each other (the matrix $X$ is deterministic, it contains real observations - the vectors $X_k$);\n\n2. Model errors are not systematic, that is, the mathematical expectation of model errors is $ M [u] = 0 $, the variance of model errors is constant and equal to $Var[u]=\\sigma^2$;\n\n3. Errors are uncorrelated, that is, $M[u_i, u_j]=0 $ when $i$ is not equal to $j$ (pairwise correlation is zero).\n\n__Then under these conditions the least-squares estimates are optimal in the class of linear unbiased estimates, in other words $\\hat{\\theta}=(X^TX)^{-1}X^Ty$ is the best estimate possible .__","1b695d29":"Error function with the addition of a vector-subgradient (which we use at points with x's equal to 0):","29ed42ef":"__Regularization__ is the imposition of certain restrictions on the model in order to avoid retraining.\n\nThe simplest example of regularization is to use the 2nd degree of the polynomial when generating polynomial features, but the 2nd.\nIn addition, you can artificially limit the size of the coefficients of the $\\theta$ model.","16afceba":"Let's run our algorithm for 50 epochs with 2000 iterations (only 100,000 times) and measure the execution time.","10d2f664":"__The choice of the loss function affects the result of our efforts to select the best parameters (coefficients $\\theta$) .__\n\n__As a rule, the value of the sum of losses (squared or modules, it doesn\u2019t matter) is normalized to the number of observations (divided by the number of observations) to obtain the average value.__","9e2fbcc6":"Let's run our algorithm for 100 epochs with 2000 iterations (only 100,000 times) and measure the execution time.","e4d896fb":"Compare our implementation of the algorithm with the LinearRegression method implemented in sklearn","b0d57c4c":"__If the signs have different scales, then the algorithm may converge slowly. We recommend scaling data before applying gradient descent .__","641f4cef":"We implement our own algorithm:","c415bd34":"# Lasso regression House Prices: Advanced Regression Techniques\n#### ($L_1$ -regularization, least absolute shrinkage and selection operator regression regression)  House Prices: Advanced Regression Techniques","4cff4fb1":"__Mean Absolute Error - MAE (Mean Absolute Error):__\n\n$$ MAE(X,h_\\theta) = \\frac{1}{n}\\sum_{k=1}^n |\\hat{y}_k - y_k|$$","b8278d50":"$$\ngrad_\\theta MSE(X,h_\\theta) = \n\\left(\\begin{array}{cc} \n\\frac{d}{d\\theta_0} MSE(X,h_\\theta) \\\\\n\\frac{d}{d\\theta_1} MSE(X,h_\\theta) \\\\\n... \\\\\n\\frac{d}{d\\theta_i} MSE(X,h_\\theta) \\\\\n... \\\\\n\\frac{d}{d\\theta_m} MSE(X,h_\\theta)\n\\end{array}\\right)\n= \\frac{2}{n} X^T (X  \\theta^T - y)\n$$ ","87ca1a8a":"Regularized regression models include:\n\u00a0\u00a0\u00a0\u00a0\n* Ridge regression ($L_2$ -regulation)\n* Lasso regression ($L_1$ regularization)\n* Elastic network","0b9db08c":"$$\\theta^{next} = \\theta^{current} - \\mu grad_\\theta MSE(X,h_{\\theta^{current}})$$\n\nWhere:\n\n$\\theta^{current}$ - current value of the coefficient vector $\\theta$;\n\n$\\theta^{next}$ - the following value of the coefficient vector $\\theta$;\n\n$MSE(X,h_{\\theta^{current}})$ - value $MSE$ for function $h$ with the current set of coefficients $\\theta$, which the observation matrix is passed as an argument $X$;\n\n$\\mu$ - learning speed.","d9e7e499":"# Check some Null's","fecb1dd3":"`Advantages`: the algorithm is computationally cheaper than the batch version, quickly gives good results, in the case of an irregular loss function (with many local extrema), the algorithm has good chances to jump out of a local minimum and get to a deeper local minimum or even to a global one.\n\n`Disadvantages`: the final answer will not be optimal (the algorithm will not stop even if it gets to a minimum), it will most likely just be\" good \".","1fb005c8":"Now compare with the sklearn implementation:","571138b1":"# Processing categoricals variables","df56e72f":"Before applying ridge regression, it is recommended to scale the data (bring them to the same dimension), the model is sensitive to the scale of features.","999e0d9e":"__Now we will test this algorithm in practice. We have already created the training data. We write a function for gradient descent and then generate new points and make a forecast .__","6297575a":"An elastic net is a combination of ridge and lasso regression.\n\nElastic Net Loss Function:\n\n$$ J(\\theta) = MSE(X,h_\\theta) + r\\alpha \\sum_{i=1}^m |\\theta_i| + \\frac{1-r}{2}\\alpha \\sum_{i=1}^m \\theta_i ^2$$\n\nWhere:\n\n$r$ is a hyperparameter that controls the proportion between L2 and L1 regularization.\n\nFor $r = 0$, an elastic network becomes just a ridge regression, and for $r = 1$ it becomes a lasso regression.","765f513c":"#### Testing the linear regression model for compliance with the criteria of the Gauss-Markov theorem:","b55f83de":"`Advantages`: the algorithm allows you to find parameters not only in the case of linear regression (for which you can find these parameters analytically), but also for many other models, including neural networks. This advantage applies to all the options for implementing the gradient descent method listed in this paper.\n\n`Disadvantages`: you need to carefully select the learning speed, for better convergence, you need to scale the data before applying the algorithm. Both of the latter drawbacks apply to all the options for implementing the gradient descent method listed in this paper.","667396fa":"Let's measure the RMSE for sklearn:","843d3d71":"We write our own implementation of the algorithm.","95ce4fc5":"### Objective for competition: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\n\n* Predict the value of homes.\n* Exploratory data analysis.\n* Feature engineering.\n* Create your own regression models.\n* Compare metrics.\n* Choose a model.\n* To predict.","15092fb0":"# Check data again","35502df9":"## Make mean target encoding for categorical feature\n\nLet us consider the above table (A simple binary classification). \n\n$$ MeanTargetEnc_i = {((GlobalMean * C) + (Mean_i * Size)) \\over (C + Size)} $$\n\nInstead of finding the mean of the targets, we can also focus on median and other statistical correlations\u2026.These are broadly called target encodings","1813f47c":"### Imputer with mean strategy","4cccc278":"The algorithm is called __*batch gradient descent*__, because at each step it works with a complete package of training data. For real data sets, such an algorithm will work rather slowly.","5463e1b9":"# Describe the data","a5cd0656":"# Drop columns with a lot of NaNs (more than 75%)","41d141a5":"`Comment:` as time measurement showed, stochastic gradient descent is faster than burst. Both implementations in the above examples updated $\\theta$ parameters 100,000 times.","3f105e08":"We will write a function that will take pairs from the array with errors many times and at the end will calculate the expectation.","79e581fb":"# Processing numerics variables","7efb56a7":"### We collect the metrics of all the algorithms","1eb2da57":"`Comment:` The sklearn implementation works much more efficiently in terms of computation speed and accuracy (RMSE parameter).","12a6cead":"To implement gradient descent in the case of a function with the number of parameters $\\theta$ $i$>=2, you need to calculate the gradient (the vector of partial derivatives for each parameter $\\theta_i$. The gradient shows how much the function changes (increases or decreases) with a small step over all $\\theta$.","d5e0542e":"# Linear regression models with regularization","eab059f4":"`Advantages`: an important feature of lasso regression is its ability to nullify the coefficients ($\\theta$) for the least important features, i.e. provide a more sparse (with fewer coefficients) model. The value of the coefficient contributes to the loss function rather large (taken modulo); ideally, to minimize such a contribution, it would be nice to reset the coefficients affecting the forecast accuracy very little. In the case of ridge regression, where the contribution of the value of each coefficient to the loss function is the squared coefficient, it is enough to underestimate the coefficient so that it becomes less than 1 (since numbers from 0 to 1 give a smaller number when squared).\n\n`Disadvantages`: the lasso-regression loss function is not differentiable at zero, you will have to apply gradient descent and do the trick using a subgradient.","a03f0d13":"`Advantages`: the algorithm is computationally cheaper than the batch version, quickly gives good results, in the case of an irregular loss function (with many local extrema), the algorithm has good chances to jump out of a local minimum and get to a deeper local minimum or even to a global one.\n\n`Disadvantages`: the final answer will not be optimal (the algorithm will not stop even if it gets to a minimum), it will most likely just be\" good \".","64a1c5de":"# Choose the best algorithm for House Prices: Advanced Regression Techniques","7a4d8604":"### Mini batch gradient descent House Prices: Advanced Regression Techniques","ee7e67b1":"# Linear regression models without regularization\n\n### Linear Regression House Prices: Advanced Regression Techniques","4b266bbe":"# Predict ","c29cf864":"### Ooops :)","374ded2e":"`Note:` native and off-the-shelf implementations work with roughly the same RMSE","28ae0172":"# Import","6f7fe33b":"The regularization of the model is realized by adding to the loss function *a regularization term*:\n\n$$\\alpha \\sum_{i=1}^m \\theta_i^2$$\n\nWhere:\n\n$\\alpha$ - normalization parameter, determines how much we regularize the model. With $\\alpha=0$, ridge regression becomes just a linear regression. For $\\alpha$ close to 1, the model coefficients (weights) tend to zero;\n\n$\\theta_i$ - model parameters from $\\theta_1$ to $\\theta_m$.\n\n\n`Note to expression`: *the regularization term* is used in conjunction with the loss function __only__ when training the model. When checking the model you need to use an irregular measure of errors. This is also true for the other regularized regression models described below.\n\nThe parameter with the free term $\\theta_0$ is not regularized","0ac93a4e":"Having calculated the gradient (the direction of the maximum growth of the function), we must take a \u201cstep\u201d in the opposite direction (along the anti-gradient).\nAt the first step, from the randomly initialized parameters $\\theta$ we subtract the obtained gradient and update the coefficient values in the vector $\\theta$, at the next step we subtract the last calculated gradient from the most recent vector $\\theta$, and so many, many times. In order to somehow normalize the step, we need to introduce\nanother coefficient is $\\mu$, by which we will multiply the gradient. The $\\mu$ parameter is called the __learning rate__.","a28a7318":"Compare the result (y_pred) with real data (y_test).\n\nWe measure the RMSE indicator for our own implementation:","da93e223":"# Elastic Net House Prices: Advanced Regression Techniques","01470592":"__We will analyze the most common criteria.__","51ee98c3":"Therefore, for real-world problems with hundreds and thousands of features, an approach with iterative optimization of parameters is used - Gradient Descent.","ee7a13a8":"As a result, the vector of coefficients (weights) of the model is formed into a vector of the form:\n\n$$\n\\frac{1}{2} \\alpha \n\\left(\\begin{array}{cc} \n(\\theta_1^2 + \\theta_2^2 + ... + \\theta_m^2)^{1\/2} \\\\\n\\theta_1^2 \\\\\n\\theta_2^2 \\\\\n... \\\\\n\\theta_m\n\\end{array}\\right) \n$$ \n\nWhere:\n\n$(\\theta_1^2 + \\theta_2^2 + ... + \\theta_m^2)^{1\/2}$ - L2-norm of the coefficient vector from $\\theta_1$ \u0434\u043e $\\theta_m$.","ed2e0519":"__Mean Squared Error - MSE (RMS Error):__\n\n$$MSE(X,h_\\theta) = \\frac{1}{n}\\sum_{k=1}^n (\\theta^T X_k - y_k)^2 = \\frac{1}{n}\\sum_{k=1}^n (h_\\theta(X_k) - y_k)^2$$","238b4c6f":"## The obtained parameters are very far from those that we obtained analytically.","7f46f999":"### Ridge regularization example House Prices: Advanced Regression Techniques","b76261f0":"___There are many ways to measure the deviation of forecast values from real values. It is necessary to somehow evaluate the differences between the forecast and real values (residuals).\nHere are some of the ways:___\n\u00a0\u00a0\u00a0\u00a0\n__1. Summarize leftovers__\n\n`Advantages`: easy to understand.\n\n`Disadvantages`: terms with opposite signs can cancel each other and we will not receive information about the actual size of the deviation.\n\n$$\\sum_{k=1}^{n} (\\hat{y}_k - y_k)$$\n\n__2. Sum the remainder modules and divide by the number of forecasts__\n\n`Advantages`: works if errors are distributed abnormally, easy to calculate.\n\n`Disadvantages`: does not work, if the errors are distributed normally, the function of the sum of modules is not differentiable at zero.\n\n$$\\sum_{k=1}^n |\\hat{y}_k - y_k|$$\n\n_3. Sum the squares (or other positive even degrees) of the residuals and divide by the number of predictions__\n\n\u02bbAdvantages`: it works regardless of the distribution of errors, the sum of squares function is differentiable.\n\n`Disadvantages`: it is possible to use only positive even degrees, it is more difficult to calculate than the difference (the absolute value of the difference), in the presence of outliers the error value can\u201c explode \u201d, with an error value of 1 it is always 1 (1 to any degree is 1), large degrees are difficult to calculate.\n\n$$\\sum_{k=1}^n (\\hat{y}_k - y_k)^2$$"}}