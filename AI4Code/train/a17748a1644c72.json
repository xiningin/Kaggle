{"cell_type":{"7b11221f":"code","7af5e3cb":"code","29fadb64":"code","f39db862":"code","bdcb0f90":"code","fb5fc67a":"code","d55c2361":"code","13b97f35":"code","c5714a13":"code","74fa02aa":"code","01248971":"code","d1c64fb2":"code","96d0479c":"code","f325439b":"code","47e3cc5b":"code","309b106a":"code","f1047548":"code","495f4946":"code","437a6f33":"code","226d0cc8":"code","92b4bde3":"code","506b4398":"code","223e6af4":"code","20ca4454":"code","7f5c079d":"code","9c99e0bd":"code","33ffc134":"code","18df73f9":"code","c077f9bc":"code","a412c862":"code","7649e841":"code","58002e3f":"code","9953d663":"code","e716793c":"code","e18b737a":"code","38613b43":"code","725595cc":"code","0b73d88b":"code","37c513ca":"code","6e6f5005":"code","ae48229b":"code","8c507ef4":"code","000e1305":"code","e8ef082e":"code","9304df3e":"code","a7d45c3f":"code","cbb294fb":"code","4815d93f":"code","ee916414":"code","18e997f4":"code","9858a9f3":"code","af273919":"code","3d71cd10":"code","67a083ff":"code","2f1653fb":"code","a437ea4d":"code","13541761":"code","27d835ff":"code","07379e76":"code","58e2e512":"markdown","266bc5a2":"markdown","57218fea":"markdown","bedc9ee9":"markdown","88192bba":"markdown","ee6b8694":"markdown","776749d4":"markdown","ace28a04":"markdown","81982228":"markdown","ca5d1ca1":"markdown","b5df74f4":"markdown","7f6bd483":"markdown","e6e64d5d":"markdown","cb69852b":"markdown","6030adb2":"markdown","8a8ccb50":"markdown","361f79e5":"markdown","2261a589":"markdown","1cf4c046":"markdown","d5d0fd32":"markdown","c53484e5":"markdown","b73062eb":"markdown","db32e85a":"markdown","964b6ed7":"markdown","ef53ad2f":"markdown","7b9b8f0d":"markdown","928729d1":"markdown","94644777":"markdown","d6609e27":"markdown","78f94210":"markdown","643ed948":"markdown","c366c25e":"markdown","f2a3f7e4":"markdown","610be6e7":"markdown","75aa79f9":"markdown","bb786f54":"markdown","85306781":"markdown","277563ca":"markdown","7d520724":"markdown","0ec4f0e5":"markdown","363e19f8":"markdown","e413f7bf":"markdown","284ae966":"markdown","99cf15a4":"markdown","3e3d37af":"markdown","a7858a33":"markdown","8f6913d6":"markdown","7cba83ba":"markdown"},"source":{"7b11221f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7af5e3cb":"import pandas as pd #data processing, CSV File(I\/O)\nimport numpy as np #linear algebra\n\n#Visualization Libraries\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignore warnings\n\n\nfrom sklearn.metrics import classification_report #Build a text report showing the classification metrics.\nfrom sklearn.metrics import accuracy_score #Accuracy classification score.\nfrom sklearn.metrics import confusion_matrix #Compute confusion matrix to evaluate the accuracy of a classification.","29fadb64":"#Read CSV file into data for applying Regression Algorithms\ndf = pd.read_csv('\/kaggle\/input\/housesalesprediction\/kc_house_data.csv')","f39db862":"# to see features and target variable\ndf.head()","bdcb0f90":"r,c = df.shape\nprint(\"Data has \",r,\" Rows\")\nprint(\"Data has \",c,\" Columns\")","fb5fc67a":"# Well know question is is there any NaN value and length of this data so lets look at info\ndf.info()","d55c2361":"# for statistical analysis\ndf.describe()","13b97f35":"plt.subplots(figsize=(17,14))\nsns.heatmap(df.corr(),annot=True,linewidths=0.5,linecolor=\"Black\",fmt=\"1.1f\")\nplt.title(\"Attributes Correlation\",fontsize=30)\nplt.show()","c5714a13":"#Plotly\nfrom plotly.offline import init_notebook_mode,iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go","74fa02aa":"hist1 = [go.Histogram(x=df.grade,marker=dict(color='rgb(102, 0, 102)'))]\n\nhistlayout1 = go.Layout(title=\"Grade Counts of Houses\",xaxis=dict(title=\"Grades\"),yaxis=dict(title=\"Counts\"))\n\nhistfig1 = go.Figure(data=hist1,layout=histlayout1)\n\niplot(histfig1)","01248971":"hist2 = [go.Histogram(x=df.yr_built,xbins=dict(start=np.min(df.yr_built),size=1,end=np.max(df.yr_built)),marker=dict(color='rgb(0,102,0)'))]\n\nhistlayout2 = go.Layout(title=\"Built Year Counts of Houses\",xaxis=dict(title=\"Years\"),yaxis=dict(title=\"Built Counts\"))\n\nhistfig2 = go.Figure(data=hist2,layout=histlayout2)\n\niplot(histfig2)","d1c64fb2":"v1 = [go.Box(y=df.price,name=\"Price Distribution of Houses\",marker=dict(color=\"rgba(64,64,64,0.9)\"),hoverinfo=\"name+y\")]\n\nlayout1 = go.Layout(title=\"Price\")\n\nfig1 = go.Figure(data=v1,layout=layout1)\niplot(fig1)","96d0479c":"v21 = [go.Box(y=df.bedrooms,name=\"Bedrooms\",marker=dict(color=\"rgba(51,0,0,0.9)\"),hoverinfo=\"name+y\")]\nv22 = [go.Box(y=df.bathrooms,name=\"Bathrooms\",marker=dict(color=\"rgba(0,102,102,0.9)\"),hoverinfo=\"name+y\")]\nv23 = [go.Box(y=df.floors,name=\"Floors\",marker=dict(color=\"rgba(204,0,102,0.9)\"),hoverinfo=\"name+y\")]\n\nlayout2 = go.Layout(title=\"Bedrooms,Bathrooms and Floors\",yaxis=dict(range=[0,13])) #I hate 33 bedroom\n\nfig2 = go.Figure(data=v21+v22+v23,layout=layout2)\niplot(fig2)","f325439b":"df[\"color\"] = \"\"\ndf.color[df.grade == 1] = \"rgb(255,255,255)\"\ndf.color[df.grade == 2] = \"rgb(220,220,220)\"\ndf.color[df.grade == 3] = \"rgb(242, 177, 172)\"\ndf.color[df.grade == 4] = \"rgb(255,133,27)\"\ndf.color[df.grade == 5] = \"rgb(255,255,204)\"\ndf.color[df.grade == 6] = \"rgb(255,65,54)\"\ndf.color[df.grade == 7] = \"rgb(178,37,188)\"\ndf.color[df.grade == 8] = \"rgb(51,51,0)\"\ndf.color[df.grade == 9] = \"rgb(37,188,127)\"\ndf.color[df.grade == 10] = \"rgb(26,51,176)\"\ndf.color[df.grade == 11] = \"rgb(132,10,10)\"\ndf.color[df.grade == 12] = \"rgb(82,80,80)\"\ndf.color[df.grade == 13] = \"rgb(0,0,0)\"","47e3cc5b":"#slice +7 grade\ndataplus = df[np.logical_and(df.grade >= 7,df.yr_built >= 2000)] \n#list lat and long\nlats = list(dataplus.lat.values)\nlongs = list(dataplus.long.values)","309b106a":"mapbox_access_token = 'pk.eyJ1IjoiZGFya2NvcmUiLCJhIjoiY2pscGFheHA1MXdqdjNwbmR3c290MTZ6dCJ9.K1FMv_q3ZVlKP13RrjFkjg'\n\nmapp = [go.Scattermapbox(lat=lats,lon=longs,mode=\"markers\",marker=dict(size=4.5,color=dataplus[\"color\"]) ,hoverinfo=\"text\",text=\"Grade:\"+dataplus.grade.apply(str)+\" Built Year:\"+dataplus.yr_built.apply(str)+\" Price:\"+dataplus.price.apply(str))]\n\nlayout5 = dict(title=\" Houses with Grade(7+) & Built Year(>2000) distribution on Map\",width=900,height=750,hovermode=\"closest\",mapbox=dict(bearing=0,pitch=0,zoom=9,center=dict(lat=47.5,lon=-122.161),accesstoken=mapbox_access_token))\n\nfig5 = go.Figure(data=mapp,layout=layout5)\n\niplot(fig5)","f1047548":"X = df[['sqft_living15']]\ny = df.price.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3,random_state=1)","495f4946":"#Linear Regression\nfrom sklearn.linear_model import LinearRegression\nmodelLR = LinearRegression()\n\n#Fit\nmodelLR.fit(X_train, y_train)\n\n#Predict\nY_pred = modelLR.predict(X_test)\n\nmodelLR.score(X_test,y_test)","437a6f33":"new_df = df[['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','lat','waterfront','yr_built','bedrooms']] #Selecting Relevant Features\nX = new_df.values #Features\ny = df.price.values #Target\n\nfrom sklearn.model_selection import train_test_split #To split dataset into test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3,random_state=1)","226d0cc8":"#Linear Regression\nfrom sklearn.linear_model import LinearRegression\nmodelLR = LinearRegression()\n\n#Fit\nmodelLR.fit(X_train, y_train)\n#Predict\nY_pred = modelLR.predict(X_test)\n\nmodelLR.score(X_test,y_test)","92b4bde3":"X1 = [1,2,3,5,6,7,8,9,10,12,13,14,15,16,18,19,21,22]  #Time on 24 hour clock\ny1 = [100,90,80,60,60,55,60,65,70,70,75,76,78,79,90,99,99,100] #Speed of Cars \n\nmymodel = np.poly1d(np.polyfit(X1, y1, 3)) #Polynomial model using NumPy\nmyline = np.linspace(1, 22, 100)\n\nplt.scatter(X1, y1)\nplt.plot(myline, mymodel(myline))\nplt.show()","506b4398":"# Predict Speed at 17\nspeed = mymodel(17)\nprint(speed)","223e6af4":"new_df = df[['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','lat','waterfront','yr_built','bedrooms']]\nX = new_df.values\ny = df.price.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3,random_state=1)","20ca4454":"from sklearn.tree import DecisionTreeRegressor\ndtr= DecisionTreeRegressor(random_state=0)\ndtr.fit(X_train,y_train)\n\ny_pred = dtr.predict(X_test)\n\ndtr.score(X_test,y_test)","7f5c079d":"new_df = df[['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','lat','waterfront','yr_built','bedrooms']]\nX = new_df.values\ny = df.price.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3,random_state=1)","9c99e0bd":"from sklearn.svm import SVR\nsvr_linear = SVR(kernel='linear')\nsvr_linear.fit(X_train, y_train)\n\ny_pred = dtr.predict(X_test)\n\nsvr_linear.score(X_test,y_test)","33ffc134":"new_df = df[['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','lat','waterfront','yr_built','bedrooms']]\nX = new_df.values\ny = df.price.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3,random_state=1)","18df73f9":"import xgboost\nxgb = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=1, max_depth=7)\nxgb.fit(X_train,y_train)\npredictions = xgb.predict(X_test)\nxgb.score(X_test,y_test)","c077f9bc":"new_df = df[['sqft_living','grade', 'sqft_above', 'sqft_living15','bathrooms','view','sqft_basement','lat','waterfront','yr_built','bedrooms']]\nX = new_df.values\ny = df.price.values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3,random_state=1)","a412c862":"from sklearn.linear_model import Lasso\nclf = Lasso(alpha = 50)\nclf.fit(X_train, y_train)\nY = clf.predict(X_test)\nclf.score(X_test,y_test)","7649e841":"# Read the Data for Classification Algorithms\ndata = pd.read_csv('..\/input\/iris\/Iris.csv')","58002e3f":"# to see features and target variable\ndata.head()","9953d663":"r,c = data.shape\nprint(\"Data has \",r,\" Rows\")\nprint(\"Data has \",c,\" Columns\")","e716793c":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","e18b737a":"# for statistical analysis\ndata.describe()","38613b43":"sns.FacetGrid(data, hue='Species', height=7)\\\n.map(plt.scatter,'SepalLengthCm','SepalWidthCm')\\\n.add_legend()","725595cc":"plt.figure(figsize=(14,6))\nsns.countplot(x=\"Species\", data=data)\ndata.loc[:,'Species'].value_counts()","0b73d88b":"sns.pairplot(data, hue='Species')","37c513ca":"X = data.iloc[:,:-1].values #Feature Variables\ny = data.iloc[:,-1].values #Target Variable","6e6f5005":"#Splitting Feature and Target Variable\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","ae48229b":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","8c507ef4":"from mlxtend.plotting import plot_decision_regions\nfrom sklearn import datasets #way to access preprovided data in sklearn\n\n# Loading example data\niris = datasets.load_iris()\nX1 = iris.data[:, [0, 2]] #Features\ny1 = iris.target  #Target","000e1305":"# Training a classifier\ntree = DecisionTreeClassifier()\ntree.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=tree, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('Decision Tree Classifier on Iris Dataset')\nplt.show()\n","e8ef082e":"from sklearn.ensemble import RandomForestClassifier\nModel=RandomForestClassifier(max_depth=2)\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","9304df3e":"# Training a classifier\nforest = RandomForestClassifier(max_depth=2)\nforest.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=forest, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('Random Forest Classifier on Iris Dataset')\nplt.show()\n","a7d45c3f":"# LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\nModel = LogisticRegression()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","cbb294fb":"# Training a classifier\nlr = LogisticRegression()\nlr.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=lr, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('Logistic Regression Classifier on Iris Dataset')\nplt.show()\n","4815d93f":"# K-Nearest Neighbours\nfrom sklearn.neighbors import KNeighborsClassifier\n\nModel = KNeighborsClassifier(n_neighbors=8)\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","ee916414":"# Training a classifier\nknn = KNeighborsClassifier(n_neighbors=8)\nknn.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=knn, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('K-Nearest Neighbor Classifier on Iris Dataset')\nplt.show()\n","18e997f4":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nModel = GaussianNB()\nModel.fit(X_train, y_train)\n\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","9858a9f3":"# Training a classifier\nnb = GaussianNB()\nnb.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=nb, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('Naive Bayes Classifier on Iris Dataset')\nplt.show()\n","af273919":"# Support Vector Machine\nfrom sklearn.svm import SVC\n\nModel = SVC()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","3d71cd10":"# Training a classifier\nsvm = SVC()\nsvm.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=svm, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('SVM Classifier on Iris Dataset')\nplt.show()\n","67a083ff":"# Support Vector Machine's \nfrom sklearn.svm import NuSVC\n\nModel = NuSVC()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","2f1653fb":"# Training a classifier\nNuSupport = NuSVC()\nNuSupport.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=NuSupport, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('Nu - Support Vector Classifier on Iris Dataset')\nplt.show()\n","a437ea4d":"# Linear Support Vector Classification\nfrom sklearn.svm import LinearSVC\n\nModel = LinearSVC()\nModel.fit(X_train, y_train)\ny_pred = Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n# Accuracy score\nprint('accuracy is',accuracy_score(y_pred,y_test))","13541761":"# Training a classifier\nLinearSupport = LinearSVC()\nLinearSupport.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=LinearSupport, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('Linear Support Vector Classifier on Iris Dataset')\nplt.show()\n","27d835ff":"from sklearn.ensemble import GradientBoostingClassifier\nModel=GradientBoostingClassifier()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\n\n# Summary of the predictions made by the classifier\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_pred,y_test))\n\n#Accuracy Score\nprint('accuracy is ',accuracy_score(y_pred,y_test))","07379e76":"# Training a classifier\ngboost=GradientBoostingClassifier()\ngboost.fit(X1, y1)\n\n# Plotting decision regions\nplt.figure(figsize=(16,8))\nplot_decision_regions(X1, y1, clf=gboost, legend=2)\n\n# Adding axes annotations\nplt.xlabel('sepal length [cm]')\nplt.ylabel('petal length [cm]')\nplt.title('Random Forest Classifier on Iris Dataset')\nplt.show()\n","58e2e512":"<a id=\"1.7\"><\/a><br>\n# 7. Lasso Regression","266bc5a2":"Polynomial Regression is a form of linear regression in which the relationship between the independent variable x and dependent variable y is modeled as an nth degree polynomial.\n* In many cases, linear model will not work out. For example if we analyzing the production of chemical synthesis in terms of temperature at which the synthesis take place in such cases we use quadratic model.\n                y = a + b1x + b2^2 + e","57218fea":"* A decision tree is a **decision support tool** that uses a tree-like graph with model of decisions and their possible consequences.\n* It is a way to display an algorithm **that only contains conditional control statements**.\n* A decision tree is a **flowchart-like structure** in which :\n  * each **internal node represents a \u201ctest\u201d on an attribute** *(e.g. whether a coin flip comes up heads or tails)*,\n  * each **branch represents the outcome of the test**,\n  * and each **leaf node represents a class label** *(decision taken after computing all attributes)*.\n  \n<img src=\"https:\/\/miro.medium.com\/max\/820\/0*LHzDR-s89Ggfqn7p.png\">","bedc9ee9":"<a id=\"1.4\"><\/a><br>\n# 4. Decision Tree Regression","88192bba":"<a id=\"1.0\"><\/a><br>\n# 0.EDA On King County House Sale Data","ee6b8694":"<a id=\"2.8\"><\/a><br>\n# 8. Linear Support Vector Classification","776749d4":"* The **goal of the SVM algorithm is to create the best line or decision boundary that can segregate n-dimensional space into classes** so that we can easily put the new data point in the correct category in the future.\n* This best decision boundary is called a **hyperplane**.\n* The data points or vectors that are the closest to the hyperplane and which affect the position of the hyperplane are termed as **Support Vector**.\n* SVM algorithm can be used for Face detection, image classification, text categorization, etc.\n\n<img src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/support-vector-machine-algorithm.png\">","ace28a04":"As you can see:\n * > Length: 21613 (range index)\n * > Features are int and float except for Date which is object\n * > Target variable(PRICE) is float.","81982228":"* Logistic Regression is used when the **dependent variable(target) is categorical**.\n* For Example:\n        * To predict whether an email is spam (1) or (0)\n        * Whether the tumor is malignant (1) or not (0)\n* Logistic regression is named for the function used at the core of the method, **the Logistic Function** (also called Sigmoid Function).\n<img src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/logistic-regression-in-machine-learning.png\">","ca5d1ca1":"<a id=\"1.1\"><\/a><br>\n# 1. Simple Linear Regression","b5df74f4":"* Naive Bayes is a **statistical classification technique** based on **Bayes Theorem**.\n* Naive Bayes classifier assumes that the **effect of a particular feature in a class is independent of other features**.\n* For example :- a loan applicant is desirable or not depending on his\/her income, previous loan and transaction history, age, and location.\n  * Even if these features are interdependent, these **features are still considered independently**.\n  * This assumption **simplifies computation**, and that's why it is considered as naive.\n  * This assumption is called **class conditional independence**.\n\n<img src=\"https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1543836883\/image_4_lyi0ob.png\">","7f6bd483":"<a id=\"100\"><\/a><br>\n# Conclusion","e6e64d5d":"<a id=\"2.6\"><\/a><br>\n# 6. SVM","cb69852b":"-------------------------------------------","6030adb2":"* Gradient boosting is one of the most powerful techniques for building **predictive models**.\n* The idea of boosting came out of the idea of whether a weak learner can be modified to become better.\n* A weak hypothesis or weak learner is defined as one whose performance is at least slightly better than random chance.\n* Gradient boosting is a **greedy algorithm** and can overfit a training dataset quickly.\n* GB builds an additive model in a forward stage-wise fashion.\n\n<img src=\"https:\/\/uc-r.github.io\/public\/images\/analytics\/gbm\/boosted-trees-process.png\">","8a8ccb50":"* Similar to SVC with parameter kernel=\u2019linear\u2019, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n* Solution of **multiclass classification problems** with any number of classes.\n* This class supports both **dense and sparse input** and the multiclass support is handled according to a one-vs-the-rest scheme.\n* Ideal for contemporary applications in digital advertisement, e-commerce, web page categorization, text classification, bioinformatics, proteomics, banking services and many other areas.\n\n![image.png](attachment:image.png)","361f79e5":"<img src='https:\/\/www.aurecongroup.com\/-\/media\/images\/aurecon\/content\/expertise\/digital-engineering-advisory\/machine-learning\/aurecon-machine-learning-large-banner.jpg?w=1170&h=417&as=1&crop=1'>","2261a589":"<a id=\"2.3\"><\/a><br>\n# 3. Logistic Regression","1cf4c046":"* The Random Forest Classifier is a set of decision trees from randomly selected subset of training set.\n* It **aggregates the votes from different decision trees** to decide the final class of the test object.\n* It is an ensemble tree-based learning algorithm.\n    * Ensemble algorithms are those which combines more than one algorithms of same or different kind for classifying objects.\n* It is one of the most accurate learning algorithms available. For many data sets, it produces a **highly accurate classifier**.\n* It runs efficiently on large databases.\n* It can handle **thousands of input variables** without variable deletion.\n\n<img src='https:\/\/miro.medium.com\/max\/1200\/1*5dq_1hnqkboZTcKFfwbO9A.png'>","d5d0fd32":"<a id=\"1.5\"><\/a><br>\n# 5. Support Vector Regression","c53484e5":"* XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework.\n* In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks.\n  * However, when it comes to small-to-medium structured\/tabular data, decision tree based algorithms are considered best-in-class right now.\n \n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*QJZ6W-Pck_W7RlIDwUIN9Q.jpeg\">","b73062eb":"<a id=\"1.6\"><\/a><br>\n# 6. XGboost Regression","db32e85a":"<a id=\"2.1\"><\/a><br>\n# 1. Decision Tree","964b6ed7":"# Introduction\n\nSince Second Year in  my Bachelor's Degree, I have been fascinated by the topic of **Machine Learning**. This kernel is prepared to be a container of many broad topics in the field of Machine Learning. My motive is to make this the ultimate reference to Machine Learning for beginners and experienced people alike.","ef53ad2f":"# REGRESSION ALGORITHMS","7b9b8f0d":"#### Scatter Plot: SepalWidth vs PetalWidth of different Species","928729d1":"#### Countplot: of Flower Species","94644777":"As you can see:\n * > Length: 150 (range index)\n * > Features are float except for Id which is integer\n * > Target variables are object that is like string","d6609e27":"<a id=\"2\"><\/a><br>\n# CLASSIFICATION ALGORITHMS","78f94210":"<a id=\"1.3\"><\/a><br>\n# 3. Polynomial Regression","643ed948":"<a id=\"2.7\"><\/a><br>\n# 7. Nu - Support Vector Classification","c366c25e":"<a id=\"2.4\"><\/a><br>\n# 4. K- Nearest Neighbor","f2a3f7e4":"<a id=\"2.9\"><\/a><br>\n# 9. Gradient Boosting Classifier","610be6e7":"* Support Vector regression is a type of Support vector machine that supports linear and non-linear regression.\n* In simple regression we try to minimise the error rate. While in SVR we try to fit the error within a certain threshold.\n* As it seems in the below graph, the mission is to fit as many instances as possible between the lines while limiting the margin violations.\n   * The violation concept in this example represents as \u03b5 (epsilon).\n   \n<img src=\"https:\/\/miro.medium.com\/max\/1260\/1*25Kk53QBOpBie4_qMSTnAA.png\">","75aa79f9":"Simple Linear regression is a basic and commonly used type of predictive analysis.\n* Have only one feature and one target variable.\n* Supervised learning.\n* We have converted 3 Species into Numerical Values. This will enable us to apply Linear Regression to given Data.\n* **y = ax + b** where y = target, x = feature and a = parameter of model\n* In regression problems target value is continuously varying variable such as price of house or sacral_slope.","bb786f54":"<a id=\"000\"><\/a><br>\n\n### CONTENT\n\n*  [REGRESSION ALGORITHMS](#1)\n    1. [Exploratory Data Analysis](#1.0)\n    1. [Simple Linear Regression](#1.1)\n    1. [Multiple Linear Regression](#1.2)\n    1. [Polynomial Regression](#1.3)\n    1. [Decision Tree Regression](#1.4)\n    1. [SVM Regression](#1.5)\n    1. [XGboost Regression](#1.6)\n    1. [Lasso Regression](#1.7)\n<br>\n<br>\n\n*  [CLASSIFICATION ALGORITHMS](#2)\n    1. [Exploratory Data Analysis](#2.0)\n    1. [Decision Tree Classification](#2.1)\n    1. [Random Forest Classification](#2.2)\n    1. [Logistic Regression](#2.3)\n    1. [K-Nearest Neighbor Classification](#2.4)\n    1. [Naive Bayes Classification](#2.5)\n    1. [SVM Classification](#2.6)\n    1. [Nu-Support Vector Classification](#2.7)\n    1. [Linear Support Vector Classification](#2.8)\n    1. [Gradient Boosting Classifier](#2.9)\n* [End Of Notebook](#100)","85306781":"Multiple Linear regression is a basic and commonly used type of predictive analysis.\n* Have **Multiple features** and one target variable.\n* The goal of multiple regression is to **model the linear relationship between your independent variables and your dependent variable**.","277563ca":"#### Pairplot of Various attributes","7d520724":"<a id=\"2.5\"><\/a><br>\n# 5. Naive Bayes","0ec4f0e5":"### If you have any question or suggestion, I will be happy to hear it.\n### Upvote the Kernel if you found it helpful.\n<a id=\"1000\"><\/a>\n# [**Goto Top of the Notebook**](#000)","363e19f8":"<a id=\"2.0\"><\/a><br>\n# 0. EDA on Iris Flower Data","e413f7bf":"<a id=\"1.2\"><\/a><br>\n# 2. Multiple Linear Regression","284ae966":"* Decision Trees are divided into Classification and Regression Trees.\n* Regression trees are needed when the response variable is numeric or continuous. \n* Classification trees, as the name implies are used to separate the dataset into classes belonging to the response variable. \n* This piece explains a Decision Tree Regression Model practice with Python.\n\n<table><tr><td><img src='https:\/\/miro.medium.com\/max\/1400\/1*avwrArcpwud-MBTgf6n-qw.png'><\/td><td><img src='https:\/\/miro.medium.com\/max\/1400\/1*XZ220vTa7rN8ccJZZNe09w.png'><\/td><\/tr><\/table>","99cf15a4":"* It is like SVC but NuSVC accepts slightly different sets of parameters.\n* The parameter which is different from SVC is as follows \u2212\n         nu \u2212 float, optional, default = 0.5\n* It represents an **upper bound** on the fraction of training errors and a **lower bound** of the fraction of support vectors.\n* Its value should be in the **interval of (o,1]**.\n\n![image.png](attachment:image.png)","3e3d37af":"## Some important things :\n* This kernel is a work in progress so every time you see on your home feed and open it, you will surely find fresh content.\n* I am doing this only after completing various courses in this field. I continue to study more advanced concepts to provide more knowledge and content.\n* If there is any suggestion or any specific topic you would like me to cover, kindly mention that in the comments.\n* **If you like my work, be sure to upvote this kernel** so it looks more relevant and meaningful to the community.","a7858a33":"* The KNN algorithm assumes that **similar things exist in close proximity**.\n* In other words, similar things are near to each other.\n        Birds of a feather flock together.\n* KNN captures the **idea of similarity** (sometimes called distance, proximity, or closeness).\n* KNN is used **for recommending products on Amazon, posts on Facebook, movies on Netflix, or videos on YouTube**.\n* KNN can be used for both classification and regression predictive problems.\n  * However, it is more widely used in classification problems in the industry.\n  \n<img src=\"https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/k-nearest-neighbor-algorithm-for-machine-learning2.png\">","8f6913d6":"<a id=\"2.2\"><\/a><br>\n# 2. Random Forest","7cba83ba":"* Lasso regression is a type of linear regression that uses shrinkage.\n* Shrinkage is where data values are shrunk towards a central point, like the mean. \n* The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters).\n\n<img src=\"https:\/\/miro.medium.com\/proxy\/1*QVzTd8Top6ImHR-3U3QdqQ.png\">"}}