{"cell_type":{"29f55623":"code","4e791517":"code","ca6518c4":"code","dd3528a3":"code","387c9046":"code","3c0cc5b5":"markdown","e4995f33":"markdown"},"source":{"29f55623":"%%writefile submission.py\n\nimport random\nimport numpy as np \nimport pandas as pd\nfrom typing import List, Dict, Tuple, Any\nfrom operator import itemgetter\nfrom collections import defaultdict\n\n\ndef batch(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\n\nclass MemoryPatterns:\n    def __init__(self, min_memory=2, max_memory=20, threshold=0.5, warmup=5, verbose=True):\n        self.min_memory = min_memory\n        self.max_memory = max_memory\n        self.threshold  = threshold\n        self.warmup     = warmup\n        self.verbose    = verbose\n        self.history = {\n            \"step\":      [],\n            \"reward\":    [],\n            \"opponent\":  [],\n            \"pattern\":   [],\n            \"action\":    [],\n            # \"rotn_self\": [],\n            # \"rotn_opp\":  [],\n        }\n        pass\n    \n    def __call__(self, obs, conf):\n        return self.agent(obs, conf)\n\n    \n    # obs  {'remainingOverageTime': 60, 'step': 1, 'reward': 0, 'lastOpponentAction': 0}\n    # conf {'episodeSteps': 1000, 'actTimeout': 1, 'runTimeout': 1200, 'signs': 3, 'tieRewardThreshold': 20, 'agentTimeout': 60}\n    def agent(self, obs, conf):\n        # print('obs', obs)\n        # print('conf', conf)\n        self.obs  = obs\n        self.conf = conf\n        self.update_state(obs, conf)\n        if obs.step < self.warmup:\n            expected = self.random_action(obs, conf)\n        else:\n            for keys in [ (\"opponent\", \"action\"), (\"opponent\",) ]:\n                # history  = self.generate_history([\"opponent\", \"action\"])  # \"action\" must be last\n                history  = self.generate_history([\"opponent\"])  \n                memories = self.build_memory(history) \n                patterns = self.find_patterns(history, memories)\n                if len(patterns): break\n            score, expected, pattern = self.find_best_pattern(patterns)\n            self.history['pattern'].append(pattern)    \n            if self.verbose:\n                print('keys    ', keys)\n                print('history ', history)\n                print('memories', memories)\n                print('patterns', patterns)\n                print('score   ', score)\n                print('expected', expected)\n                print('pattern ', pattern)\n\n        action = (expected + 1) % conf.signs\n        self.history['action'].append(action)\n        \n        if self.verbose:\n            print('action', action)\n        return int(action) \n    \n    \n    def random_action(self, obs, conf) -> int:\n        return random.randint(0, conf.signs-1)\n\n    def sequential_action(self, obs, conf) -> int:\n        return (obs.step + 1) % conf.signs\n\n    \n    def update_state(self, obs, conf):\n        self.history['step'].append( obs.step )\n        self.history['reward'].append( obs.reward )\n        if obs.step != 0:\n            self.history['opponent'].append( obs.lastOpponentAction )\n            # rotn_self = (self.history['opponent'][-1] - self.history['opponent'][-2]) % conf.signs \n            # rotn_opp  = (self.history['opponent'][-1] - self.history['action'][-1]))  % conf.signs\n            # self.history['rotn_self'].append( rotn_self )\n            # self.history['rotn_opp'].append( rotn_opp )\n        \n        \n    def generate_history(self, keys: List[str]) -> List[Tuple[int]]:\n        # Reverse order to correctly match up arrays\n        history = list(zip(*[ reversed(self.history[key]) for key in keys ]))\n        history = list(reversed(history))\n        return history\n    \n    \n    def build_memory(self, history: List[Tuple[int]]) -> List[ Dict[Tuple[int], List[int]] ]:\n        output    = [ dict() ] * self.min_memory\n        expecteds = self.generate_history([\"opponent\"])\n        for batch_size in range(self.min_memory, self.max_memory+1):\n            if batch_size >= len(history): break  # ignore batch sizes larger than history\n            output_batch    = defaultdict(lambda: [0,0,0])\n            history_batches  = list(batch(history, batch_size+1))\n            expected_batches = list(batch(expecteds, batch_size+1))\n            for n, (pattern, expected_batch) in enumerate(zip(history_batches, expected_batches)):\n                previous_pattern = tuple(pattern[:-1])\n                expected         = (expected_batch[-1][-1] or 0) % self.conf.signs  # assume \"action\" is always last \n                output_batch[ previous_pattern ][ expected ] += 1\n            output.append( dict(output_batch) )\n        return output\n\n    \n    def find_patterns(self, history: List[Tuple[int]], memories: List[ Dict[Tuple[int], List[int]] ]) -> List[Tuple[float, int, Tuple[int]]]:\n        patterns = []\n        for n in range(1, self.max_memory+1):\n            if n >= len(history): break\n                \n            pattern = tuple(history[-n:])\n            if pattern in memories[n]:\n                score    = np.std(memories[n][pattern])\n                expected = np.argmax(memories[n][pattern])\n                patterns.append( (score, expected, pattern) )\n        patterns = sorted(patterns, key=itemgetter(0), reverse=True)\n        return patterns\n    \n    \n    def find_best_pattern(self, patterns: List[Tuple[float, int, Tuple[int]]] ) -> Tuple[float, int, Tuple[int]]:\n        patterns       = sorted(patterns, key=itemgetter(0), reverse=True)\n        pattern_scores = self.get_pattern_scores()\n        for (score, expected, pattern) in patterns:\n            break\n            # if pattern in pattern_scores:\n            #     if pattern_scores[pattern] > self.threshold:\n            #         break\n            #     else:\n            #         expected += 1\n            #         break\n            # else:\n            #     break\n        else:\n            score    = 0.0\n            expected = self.random_action(self.obs, self.conf)\n            pattern  = tuple()\n        return score, expected, pattern\n    \n    \n    def get_pattern_scores(self):\n        pattern_rewards = defaultdict(list)\n        for reward, pattern in self.generate_history([\"reward\", \"pattern\"]):\n            pattern_rewards[pattern].append( reward )\n        pattern_scores = { pattern: np.mean(rewards) for patten, rewards in pattern_rewards.items() }\n        return pattern_scores\n                    \n            \n            \ninstance = MemoryPatterns()\ndef kaggle_agent(obs, conf):\n    return instance(obs, conf)","4e791517":"def sequential_agent(obs, conf) -> int:\n    return (obs.step + 1) % conf.signs","ca6518c4":"%run submission.py","dd3528a3":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"rps\", debug=True, configuration={\"episodeSteps\": 20})\n# env.run([ \"submission.py\", \"submission.py\"])\n# env.run([ \"submission.py\", \"..\/input\/rock-paper-scissors-anti-rotn\/anti_rotn.py\"])\nenv.run([ \"submission.py\", sequential_agent])\nenv.render(mode=\"ipython\", width=450, height=450)","387c9046":"agents = {\n    \"rock\":          0,\n    \"sequential\":    sequential_agent,\n    \"anti-rotn\":     \"..\/input\/rock-paper-scissors-anti-rotn\/anti_rotn.py\",\n    \"decision-tree\": \"..\/input\/rock-paper-scissors-decision-tree\/submission.py\",\n}\nfor agent_name, agent_script in agents.items():\n    scores = evaluate(\"rps\", [ \"submission.py\", agent_script])[0]\n    print(f'{scores[0]:4.0f} vs {scores[1]:4.0f} | {agent_name}')","3c0cc5b5":"# Further Reading\n\nThis notebook is part of a series exploring Rock Paper Scissors:\n\nPredetermined\n- [PI Bot](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-pi-bot)\n- [Anti-PI Bot](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-anti-pi-bot)\n- [Anti-Anti-PI Bot](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-anti-anti-pi-bot)\n- [De Bruijn Sequence](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-de-bruijn-sequence)\n\nRNG\n- [Random Agent](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-random-agent)\n- [Random Seed Search](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-random-seed-search)\n- [RNG Statistics](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-rng-statistics)\n\nOpponent Response\n- [Anti-Rotn](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-anti-rotn)\n- [Sequential Strategies](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-sequential-strategies)\n\nStatistical \n- [Weighted Random Agent](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-weighted-random-agent)\n- [Statistical Prediction](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-statistical-prediction)\n- [Anti-Rotn Weighted Random](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-anti-rotn-weighted-random)\n\nMemory Patterns\n- [Naive Bayes](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-naive-bayes)\n- [Memory Patterns](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-memory-patterns)\n\nDecision Tree\n- [XGBoost](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-xgboost)\n- [Multi Stage Decision Tree](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-multi-stage-decision-tree)\n- [Decision Tree Ensemble](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-decision-tree-ensemble)\n\nEnsemble\n- [Multi Armed Stats Bandit](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-multi-armed-stats-bandit)\n\nRoShamBo Competition Winners\n- [Iocaine Powder](https:\/\/www.kaggle.com\/jamesmcguigan\/rps-roshambo-comp-iocaine-powder)\n- [Greenberg](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-greenberg)","e4995f33":"# Rock Paper Scissors - Memory Patterns\n\nThis notebook is an independent reimplemention of the high-level ideas in:\n- https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns"}}