{"cell_type":{"9e000daa":"code","b8fcc898":"code","0c7045cb":"code","0eb64d61":"code","2a65007a":"code","48780be6":"code","7b01dce4":"markdown","f588d224":"markdown","9c539e15":"markdown"},"source":{"9e000daa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\nimport seaborn as sns\nimport time\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Any results you write to the current directory are saved as output.","b8fcc898":"def logloss(y,yp):\n    yp = np.clip(yp,1e-5,1-1e-5)\n    return -y*np.log(yp)-(1-y)*np.log(1-yp)\n    \ndef reverse(tr,te):\n    reverse_list = [0,1,2,3,4,5,6,7,8,11,15,16,18,19,\n                22,24,25,26,27,41,29,\n                32,35,37,40,48,49,47,\n                55,51,52,53,60,61,62,103,65,66,67,69,\n                70,71,74,78,79,\n                82,84,89,90,91,94,95,96,97,99,\n                105,106,110,111,112,118,119,125,128,\n                130,133,134,135,137,\n                140,144,145,147,151,155,157,159,\n                161,162,163,164,167,168,\n                170,171,173,175,176,179,\n                180,181,184,185,187,189,\n                190,191,195,196,199]\n    reverse_list = ['var_%d'%i for i in reverse_list]\n    for col in tr.columns:\n        colx = '_'.join(col.split('_')[:2])\n        if colx in reverse_list and 'count' not in col: \n            tr[col] = tr[col]*(-1)\n            te[col] = te[col]*(-1)\n    return tr,te\n\ndef scale(tr,te):\n    for col in tr.columns:\n        if col.startswith('var_') and 'count' not in col:\n            mean,std = tr[col].mean(),tr[col].std()\n            tr[col] = (tr[col]-mean)\/std\n            if col in te.columns:\n                te[col] = (te[col]-mean)\/std\n    return tr,te\n\ndef getp_vec_sum(x,x_sort,y,std,c=0.5):\n    # x is sorted\n    left = x - std\/c\n    right = x + std\/c\n    p_left = np.searchsorted(x_sort,left)\n    p_right = np.searchsorted(x_sort,right)\n    p_right[p_right>=y.shape[0]] = y.shape[0]-1\n    p_left[p_left>=y.shape[0]] = y.shape[0]-1\n    return (y[p_right]-y[p_left])\n\ndef get_prob(tr,col,x_query=None,smooth=3,silent=1):\n    std = tr[col].std()\n    N = tr.shape[0]\n    tr = tr.dropna(subset=[col])\n    if silent==0:\n        print(\"null ratio %.4f\"%(tr.shape[0]\/N))\n    df = tr.groupby(col).agg({'target':['sum','count']})\n    cols = ['sum_y','count_y']\n    df.columns = cols\n    df = df.reset_index()\n    df = df.sort_values(col)\n    y,c = cols\n    \n    df[y] = df[y].cumsum()\n    df[c] = df[c].cumsum()\n    \n    if x_query is None:\n        rmin,rmax,res = -5.0, 5.0, 501\n        x_query = np.linspace(rmin,rmax,res)\n    \n    dg = pd.DataFrame()\n    tm = getp_vec_sum(x_query,df[col].values,df[y].values,std,c=smooth)\n    cm = getp_vec_sum(x_query,df[col].values,df[c].values,std,c=smooth)+1\n    dg['res'] = tm\/cm\n    dg.loc[cm<500,'res'] = 0.1\n    return dg['res'].values\n\ndef get_probs(tr):\n    y = []\n    for i in range(200):\n        name = 'var_%d'%i\n        res = get_prob(tr,name)\n        y.append(res)\n    return np.vstack(y)\n\ndef split(tr):\n    split = pickle.load(open('cache\/kfolds.pkl','rb'))\n    for trx,tex in split:\n        break\n    trx,vax = tr.iloc[trx],tr.iloc[tex]\n    return trx,vax\n\n\ndef plot_pdf(tr,name):\n    name1 = '%s_no_noise'%name\n    name2 = '%s_no_noise2'%name\n    rmin,rmax,res = -5.0, 5.0, 501\n    x_query = np.linspace(rmin,rmax,res)\n    plt.figure(figsize=(10,5))\n    prob1 = get_prob(tr,name,x_query=x_query)\n    prob2 = get_prob(tr,name1,x_query=x_query)\n    prob3 = get_prob(tr,name2,x_query=x_query)\n    plt.grid()\n    plt.plot(x_query,prob1,color='b',label=name)\n    plt.plot(x_query,prob2,color='r',label=name1)\n    plt.plot(x_query,prob3,color='g',label=name2)\n    plt.legend(loc='upper right')\n    plt.title('PDF of '+name)\n    \ndef plot_pdfs(tr):\n    rmin,rmax,res = -5.0, 5.0, 501\n    x_query = np.linspace(rmin,rmax,res)\n    cols = [i for i in tr.columns if i.startswith('var')]\n    for i in range(50):\n        plt.figure(figsize=(18,10))\n        print('plot var %d to var %d'%(i*4,i*4+4))\n        for j in range(4):\n            cx = i*4+j\n            name = 'var_%d'%cx\n            name1 = '%s_no_noise'%name\n            name2 = '%s_no_noise2'%name\n            plt.subplot(2,2,j+1)\n            prob1 = get_prob(tr,name,x_query=x_query)\n            prob2 = get_prob(tr,name1,x_query=x_query)\n            prob3 = get_prob(tr,name2,x_query=x_query)\n            plt.grid()\n            plt.plot(x_query,prob1,color='b',label=name)\n            plt.plot(x_query,prob2,color='r',label=name1)\n            plt.plot(x_query,prob3,color='g',label=name2)\n            plt.legend(loc='upper right')\n            plt.title('PDF of '+name)\n        plt.show()","0c7045cb":"def build_magic():\n    tr_path,te_path = '..\/input\/train.csv','..\/input\/test.csv'\n\n    tr = pd.read_csv(tr_path)#.drop([IDCOL,YCOL],axis=1)\n    te = pd.read_csv(te_path)#.drop([IDCOL],axis=1)\n    cols = [i for i in tr.columns if i.startswith('var_')]\n    N = tr.shape[0]\n    tr['real'] = 1\n    te0 = te.copy()\n    for col in cols:\n        te[col] = te[col].map(te[col].value_counts())\n    a = te[cols].min(axis=1)\n    te0['real'] = (a == 1).astype('int')\n\n    tr = tr.append(te0).reset_index(drop=True)\n    for col in cols:\n        tr[col+'_count'] = tr[col].map(tr.loc[tr.real==1,col].value_counts())\n    for col in cols:\n        tr.loc[tr[col+'_count']>1,col+'_no_noise'] = tr.loc[tr[col+'_count']>1,col]\n        tr.loc[tr[col+'_count']>2,col+'_no_noise2'] = tr.loc[tr[col+'_count']>2,col]\n    return tr","0eb64d61":"%%time\ntr = build_magic()\nte = tr[tr.target.isnull()]\ntr = tr[tr.target.isnull()==0]\nprint(tr.shape)","2a65007a":"%%time\ntr,te = reverse(tr,te)\ntr,te = scale(tr,te)","48780be6":"%%time\nplot_pdfs(tr)","7b01dce4":"In this kernel, I implement vectorized PDF caculation (without for loop) to get their correlation matrix. This is helpful to study feature grouping.\ncredits to @sibmike https:\/\/www.kaggle.com\/sibmike\/are-vars-mixed-up-time-intervals","f588d224":"**load data & group vars**","9c539e15":"**Functions**"}}