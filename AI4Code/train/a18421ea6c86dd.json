{"cell_type":{"0122d426":"code","023c85f1":"code","df253567":"code","bdef5ef7":"code","ac9264e3":"code","5160a9e0":"code","343d9aff":"code","29bc29b1":"code","ed2f0dad":"code","261230cf":"code","178aefe3":"code","2bbe483c":"code","5d98dbda":"code","ea8658c5":"code","3903d13a":"code","5d62886f":"code","076bc140":"code","42f027b8":"code","fc730621":"code","91243ef8":"code","1faeb9be":"code","f2c65915":"code","24430330":"code","b280e7bc":"code","e3b4e9eb":"code","dad42c79":"code","e4505507":"code","670882ce":"code","0eac9547":"code","b9efd323":"code","a0da21d2":"code","05312ff2":"markdown","c3bb4637":"markdown","0c262c5b":"markdown","488f1717":"markdown","21c2a3d3":"markdown","47c7c057":"markdown","15e59eb4":"markdown","558759b8":"markdown","615c1448":"markdown","3bde14c2":"markdown","c0dc356b":"markdown","f899bcbe":"markdown","4604ff85":"markdown","8e67706c":"markdown","e23ba05c":"markdown","79c67d7a":"markdown","b6296445":"markdown","9fc586eb":"markdown","33ae5d35":"markdown","39042e0d":"markdown","b1e3ba02":"markdown","eaee86ff":"markdown","af80c52d":"markdown"},"source":{"0122d426":"# for array operations\nimport numpy as np \n# for pretty text printing\nfrom pprint import pprint\n\n\n# pipeline module from the HF's transformers library\nfrom transformers import pipeline","023c85f1":"# download and cache a suitable pre-trained model\nclassifier = pipeline('sentiment-analysis')","df253567":"classifier('One of the best products I have ever used')","bdef5ef7":"classifier(['They got thrilled with the movie', \n            'My car starts to underperform', \n            'I love to take icecreams along with milkshakes',\n            'Nobody enters my restaurant after that horror incident'])","ac9264e3":"extractor = pipeline('feature-extraction')","5160a9e0":"x = extractor('This is a nice example to follow')\nx = np.array(x)\nx.shape","343d9aff":"classifier = pipeline('zero-shot-classification')","29bc29b1":"classifier(\"I love courses offered by NPTEL\",\n          candidate_labels = ['sports', 'education', 'business'])","ed2f0dad":"classifier(\"Our Stock prices find a sudden fall\",\n          candidate_labels = ['politics', 'music', 'business', 'finance'])","261230cf":"generator = pipeline('text-generation')","178aefe3":"text = generator('When you get stuck with a Python code')\npprint(text)","2bbe483c":"texts = generator('I love to do trekking in mountains and', \n                  model = 'distilgpt2',\n                  num_return_sequences=5, \n                  max_length=50)\npprint(texts)","5d98dbda":"unmask = pipeline('fill-mask')","ea8658c5":"unmask('I really love icecream along with <mask> in rain', top_k = 5)","3903d13a":"unmask('Sir Isaac Newton focused more in <mask> mechanics during his reseach',\n      model='bert-base-cased')","5d62886f":"unmask('Who am I? I think I suffer from <mask> loss')","076bc140":"ner = pipeline('ner', grouped_entities = True)","42f027b8":"ner('I will meet Roshan and Rudhra when I reach the Indian Institute of Technology at Mumbai')","fc730621":"qa = pipeline('question-answering')","91243ef8":"qa(question = 'Where did I stay last night?',\n  context = 'I visited Darjeeling yesterday, \\\n   spent my night at a hotel in Delhi \\\n   and have returned back to Agra')","1faeb9be":"summarizer = pipeline('summarization')","f2c65915":"summarizer('Once, a group of frogs was roaming around the forest in search of water. \\\nSuddenly, two frogs in the group accidentally fell into a deep pit.\\\nThe other frogs worried about their friends in the pit.\\\nSeeing how deep the pit was, they told the two frogs that there was no way \\\nthey could escape the deep pit and that there was no point in trying.\\\nThey continued to constantly discourage them as the two frogs tried to jump out of the pit. \\\nBut keep falling back.\\\nSoon, one of the two frogs started to believe the other frogs \u2014 that they\u2019ll never be \\\nable to escape the pit and eventually died after giving up.\\\nThe other frog keeps trying and eventually jumps so high that he escapes the pit. \\\nThe other frogs were shocked at this and wondered how he did it.\\\nThe difference was that the second frog was deaf and couldn\u2019t hear \\\nthe discouragement of the group. He simply thought they were cheering him on!')","24430330":"summarizer(\"Researchers at MIT have created a robotic system that can do just that. \\\nThe system, RFusion, is a robotic arm with a camera and radio frequency (RF) antenna \\\nattached to its gripper. It fuses signals from the antenna with visual input from \\\nthe camera to locate and retrieve an item, even if the item is buried under a pile \\\nand completely out of view.\\\nThe RFusion prototype the researchers developed relies on RFID tags, \\\nwhich are cheap, battery-less tags that can be stuck to an item and \\\nreflect signals sent by an antenna. Because RF signals can travel through most surfaces \\\n(like the mound of dirty laundry that may be obscuring the keys), \\\nRFusion is able to locate a tagged item within a pile.\\\nUsing machine learning, the robotic arm automatically zeroes-in on the object's \\\nexact location, moves the items on top of it, grasps the object, \\\nand verifies that it picked up the right thing. The camera, antenna, robotic arm, \\\nand AI are fully integrated, so RFusion can work in any environment without \\\nrequiring a special set up.\\\nWhile finding lost keys is helpful, RFusion could have many broader applications \\\nin the future, like sorting through piles to fulfill orders in a warehouse, identifying \\\nand installing components in an auto manufacturing plant, or helping an elderly \\\nindividual perform daily tasks in the home, though the current prototype isn't \\\nquite fast enough yet for these uses.\\\n\\\"This idea of being able to find items in a chaotic world is an open problem \\\nthat we've been working on for a few years. Having robots that are able to search \\\nfor things under a pile is a growing need in industry today. \\\nRight now, you can think of this as a Roomba on steroids, but in the near term, \\\nthis could have a lot of applications in manufacturing and warehouse environments,\\\" \\\nsaid senior author Fadel Adib, associate professor in the Department of \\\nElectrical Engineering and Computer Science and director of the Signal Kinetics group \\\nin the MIT Media Lab.\\\nCo-authors include research assistant Tara Boroushaki, the lead author; \\\nelectrical engineering and computer science graduate student Isaac Perper; \\\nresearch associate Mergen Nachin; and Alberto Rodriguez, the Class of 1957 \\\nAssociate Professor in the Department of Mechanical Engineering. \\\nThe research will be presented at the Association for Computing Machinery Conference \\\non Embedded Networked Senor Systems next month.\")","b280e7bc":"# translate from english to german\nhelsinki_en_de = pipeline('translation', model='Helsinki-NLP\/opus-mt-en-de')","e3b4e9eb":"result = helsinki_en_de('Transformers have started to lead the Natural Language Processing world recently')\nprint(result)\nde_text = result[0]['translation_text']","dad42c79":"# let's try to revert back the translation \nhelsinki_de_en = pipeline('translation', model='Helsinki-NLP\/opus-mt-de-en')","e4505507":"helsinki_de_en(de_text)","670882ce":"# use a different model to translate back from german to english\nfacebook_de_en = pipeline('translation', model='facebook\/wmt19-de-en')","0eac9547":"facebook_de_en(de_text)","b9efd323":"# a sentence based on a male\nresults = unmask('This man works there as a <mask>', model='bert-base-uncased')\nprint([result['token_str'] for result in results])","a0da21d2":"# an equivalent sentence based on a female\nresults = unmask('This woman works there as a <mask>', model='bert-base-uncased')\nprint([result['token_str'] for result in results])","05312ff2":"That's really great! The summarization is perfect and up to the point.","c3bb4637":"Controlled generation is possible by providing the pre-trained model, the number of return sequences and maximum sequence length as arguments.","0c262c5b":"The results for either gender are drastically differing. The terms 'bartender' and 'waiter\/waitress' are found on both results but with varying output probability. \n\nThis one article seems to be short but it has invoked and used around half-a-dozen of State-Of-The-Art Models along with their checkpoints, Tokenizing methods, Pre-processing and Post-processing techniques to perform all above tasks in just two lines of code each!","488f1717":"A pretty short summary!","21c2a3d3":"### Limitations of pipeline\n\nSince NLP models are trained on large corpus, the outputs of models represent their training data. Hence the outputs may be biased based on gender, race, and so on.","47c7c057":"Using a text input to the classifier - ","15e59eb4":"### Named Entity Recognition (NER)\n\nNER is the task of identifying words that are Names of a person [**PER**] or an organization [**ORG**] or a location [**LOC**]. By providing an argument 'grouped_entities=True' will ensure that a name formed by more than one word be grouped together during post-processing.","558759b8":"#### ------------------------------------------------ \n#### *Articles So Far In This Series*\n#### -> [[NLP Tutorial] Finish Tasks in Two Lines of Code](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-finish-tasks-in-two-lines-of-code)\n#### -> [[NLP Tutorial] Unwrapping Transformers Pipeline](https:\/\/www.kaggle.com\/rajkumarl\/nlp-unwrapping-transformers-pipeline)\n#### -> [[NLP Tutorial] Exploring Tokenizers](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-exploring-tokenizers)\n#### -> [[NLP Tutorial] Fine-Tuning in TensorFlow](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-tensorflow) \n#### -> [[NLP Tutorail] Fine-Tuning in Pytorch](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-in-pytorch) \n#### -> [[NLP Tutorail] Fine-Tuning with Trainer API](https:\/\/www.kaggle.com\/rajkumarl\/nlp-tutorial-fine-tuning-with-trainer-api) \n#### ------------------------------------------------ ","615c1448":"# Effortless NLP using HuggingFace's Tranformers Ecosystem","3bde14c2":"### Sentiment Analysis\n\nSentiment analysis is a binary text classification task in which the text is generally classified into one of the labels - positive and negative. The pipeline module downloads a suitable pre-trained transformer model, preprocesses the input text and outputs the sentiment label and its probability score value.","c0dc356b":"### Feature Extraction\n\nThis pipeline extracts the hidden features from the transformer base, that can be used in downstream applications. Other pretrained models may also be used by feeding them as arguments.","f899bcbe":"### Mask Filling\n\nMask filling is an interesting NLP task, where the pre-trained language model replaces the masks with most suitable words\/phrases.","4604ff85":"### Translation\n\nPre-trained models are available in HuggingFace ecosystem using which we can translate a sentence into hundreds of languages in no time!","8e67706c":"![Image](https:\/\/raw.githubusercontent.com\/RajkumarGalaxy\/dataset\/master\/Images\/pipeline_0.jpg)\n> Image By [Karosu](https:\/\/unsplash.com\/@karosu)\n### Complex NLP Task can be performed with two lines of code using Pipeline. \n### A Pipeline? What does it do?","e23ba05c":"**Pipeline** module in the **transformers** library offers a quick lauch of NLP tasks without any preprocessing or model building activities. It helps one finish a complicated NLP task with just two lines of code.\n\nImport the necessary libraries and modules. ","79c67d7a":"### Question Answering\n\nBy providing a question and a relevent context, the pipeline can find out answer to the question.","b6296445":"Using multiple text inputs at a time - ","9fc586eb":"### Text Summarization","33ae5d35":"Translation to English by either models are the same and they read better than my actual English input! I may use this to-and-fro approach to enhance my English writing!","39042e0d":"### Text Generation\n\nGiven a prompt, the text generation pipeline can generate a text sequence on its own.","b1e3ba02":"### Zero-Shot Classification\n\nZero-shot classification is one of the hottest ML idea, in which, a given text prompt is classified into any one of the given labels without any fine-tuning process.","eaee86ff":"### Thank you for your valuable time!","af80c52d":"Key reference: [HuggingFace's NLP Course](https:\/\/huggingface.co\/course)"}}