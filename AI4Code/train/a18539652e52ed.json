{"cell_type":{"5e92f434":"code","80e0fc1d":"code","37d42232":"code","2a0d51e6":"code","7a77c6e7":"code","79aa5195":"code","0b00129d":"code","ccb63094":"code","2e7bd697":"code","abe7ed0d":"code","cfdc9390":"code","b4dbe462":"markdown","c43aa12e":"markdown","27c05d3f":"markdown","61b96351":"markdown","99f328c8":"markdown","8a5f269f":"markdown","10a5da11":"markdown"},"source":{"5e92f434":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80e0fc1d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.set_option('display.max_rows', 300)\npd.set_option('display.max_columns', 300)\n\ntrain = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv')\ntrain.head(3)","37d42232":"train.target.value_counts()","2a0d51e6":"train.dtypes","7a77c6e7":"train_target0 = train[train[\"target\"] == 0]\ntrain_target1 = train[train[\"target\"] == 1]\ntrain_target0 = train_target0.drop(columns = [\"target\", \"id\"], axis = 1)\ntrain_target1 = train_target1.drop(columns = [\"target\", \"id\"], axis = 1)\ntrain_target0.head(3)","79aa5195":"del train\neda_tsa = pd.DataFrame({\"train_target0_mean\" :train_target0.mean(),\n                        \"train_target1_mean\" :train_target1.mean(),\n                        \"train_target0_skew\" :train_target0.skew(),\n                        \"train_target1_skew\" :train_target1.skew(),\n                        \"train_target0_kur\"  :train_target0.kurt(),\n                        \"train_target1_kur\"  :train_target1.kurt(),\n                        \"train_mean\"         :train.iloc[:, 1:-1].mean(),\n                        \"train_skew\"         :train.iloc[:, 1:-1].skew(),\n                        \"train_kur\"          :train.iloc[:, 1:-1].kurt(),                        \n                        })\neda_tsa.head(3)","0b00129d":"fig = plt.figure(figsize = [22,8])\nplt.plot(eda_tsa[\"train_target0_mean\"], alpha = 0.5, label = \"train_target0_mean\")\nplt.plot(eda_tsa[\"train_target1_mean\"], alpha = 0.5, label = \"train_target1_mean\")\nplt.plot(eda_tsa[\"train_mean\"], alpha = 0.2, label = \"train_mean\")\nplt.legend(loc = \"upper right\")\nplt.show()","ccb63094":"fig = plt.figure(figsize = [22,8])\nplt.plot(eda_tsa[\"train_target0_skew\"], alpha = 0.5, label = \"train_target0_skew\")\nplt.plot(eda_tsa[\"train_target1_skew\"], alpha = 0.5, label = \"train_target1_skew\")\nplt.plot(eda_tsa[\"train_skew\"], alpha = 0.2, label = \"train_skew\")\nplt.legend(loc = \"upper right\")\nplt.show()","2e7bd697":"fig = plt.figure(figsize = [22,8])\nplt.plot(eda_tsa[\"train_target0_kur\"], alpha = 0.5, label = \"train_target0_kurt\")\nplt.plot(eda_tsa[\"train_target1_kur\"], alpha = 0.5, label = \"train_target1_kurt\")\nplt.plot(eda_tsa[\"train_kur\"], alpha = 0.2, label = \"train_kurt\")\nplt.legend(loc = \"upper right\")\nplt.show()","abe7ed0d":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\nforest = RandomForestClassifier(criterion = \"gini\",\n                                n_estimators = 100,\n                                random_state = 1,\n                                n_jobs = -1)\n\nX = train.iloc[:,1:-1]\ny = train.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0, stratify = y)\nforest.fit(X_train, y_train)\n\nprint(\"training dataset score : {:.3f} \/ test dataset score:{:.3f}\".format(forest.score(X_train, y_train), forest.score(X_test, y_test)))\n","cfdc9390":"importances = forest.feature_importances_\nindices = np.argsort(importances)[::-1]\n\nfeat_labels = train.columns[1:-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" % ( f+1, 30, feat_labels[indices[f]], importances[indices[f]]))","b4dbe462":"Skewness is almost the same regardless of the target value\nThe representative value is almost the same regardless of the target value.\nThere seems to be no result that can be simply classified. Let's classify by random forest for the time being\n\n","c43aa12e":"The target values are almost evenly divided.","27c05d3f":"Considering from the maximum score of 1, it seems to be a w value and not a bad score.\n(The score when the test data is predicted from the data learned from all the data is 0.80.)\n\nLet's check the importance of features\n","61b96351":"Here is a comparison of the representative values of train data and test data.\nhttps:\/\/www.kaggle.com\/yamahisa\/eda-in-progress","99f328c8":"The average is almost the same regardless of the target value","8a5f269f":"**EDA of the train data**\n\uff08Since the calculation will stop soon due to lack of memory, only the code will be described for a part. I really want to make a graph.I would appreciate it if you could tell me if there is a good response to this.\uff09","10a5da11":"Kurtosis is almost the same regardless of the target value"}}