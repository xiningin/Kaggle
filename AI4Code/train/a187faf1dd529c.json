{"cell_type":{"ccf0a4e5":"code","0748a0c9":"code","90482c63":"code","edd16200":"code","820239a5":"code","a16df54b":"code","b7391749":"code","c3e0b056":"code","a6a910f6":"code","06a3d3f2":"code","0553f718":"code","f0ccfa56":"code","fb47c084":"markdown","53809d7d":"markdown","e8c423fd":"markdown","7e936d17":"markdown","b8ebba90":"markdown","93cccdee":"markdown","ff939ce1":"markdown","a870014d":"markdown","4499a989":"markdown","830115b8":"markdown","fe730375":"markdown","ff52a1df":"markdown","0c23b13f":"markdown","65611299":"markdown","d6a1380c":"markdown"},"source":{"ccf0a4e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0748a0c9":"# To install Cerbo just do\n!pip install cerbo","90482c63":"# To import the data you can simply do\nimport cerbo.preprocessing as cp\nimport cerbo.ML as cml","edd16200":"loc = \"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\"\n\ndata, col_names = cp.load_custom_data(loc, \"Outcome\", num_features=5, id=False)","820239a5":"\"\"\"\nTask = \"C\" stands for classification do \"R\" for Regression\ndata = data (The dictionary containing the data)\nneighbors = 2 (# of neighbors for KNN)\n\"\"\"\n\nknn = cml.KNN(task=\"c\", data=data, neighbors=2) ","a16df54b":"cml.save_model(knn)","b7391749":"xgb = cml.Boosting(task=\"c\", data=data, algo=\"xgb\")","c3e0b056":"cml.save_model(xgb)","a6a910f6":"lr = cml.LogisticReg(data)","06a3d3f2":"cml.save_model(lr)","0553f718":"rf = cml.RandomForest(task=\"c\", data=data)","f0ccfa56":"cml.save_model(rf)","fb47c084":"After running the ``load_custom_data`` method you can see that we first get a scatter_matrix showing the correlation of some set of features to another. You can change the value of ``num_features`` to display the scatter_matrix for more features!\n\n\nNext, it outputs a correlation map showing the correlations between each of the input features. \n\n\nNote: Data is a dictionary containing 2 NumPy Arrays(One For the Inputs, and one for the outputs)","53809d7d":"## Logistic Regression","e8c423fd":"## Boosting(XGB)","7e936d17":"# Code","b8ebba90":"To load in the data you have to use a method in ```cerbo.preprocessing``` called ```load_custom_data```. The parameters you have to provide to ```load_custom_data```, are\n   * Location of CSV File\n   * The Column you have to predict\n   * Num_Features(For scatter_matrix)\n   * id(Put True if there is an ID column in the CSV file)","93cccdee":"## Random Forests","ff939ce1":"## Training Models\n\n\nNow we are at the step where we can train models. For now, I will just show you how to write a KNN, Boosting(XGB), Decision Tree, and Logistic Regression. And it will only take 1 line to train all of these models and get **Training** and **Testing** accuracy","a870014d":"# What we learned?\n\nWe covered a lot about this new ML API called Cerbo. We showed how you can train models in quite literally 4 lines(2 import statements, 2 lines of code!). With all of this information you can use this library to make ML extremely streamlined and make it have a much lower barrier of entry!","4499a989":"## What is Cerbo?\n\nI will be using the high-level machine learning API known as  [Cerbo](https:\/\/github.com\/StartOnAI\/Cerbo)! It can also be found [here](https:\/\/pypi.org\/project\/cerbo\/). \n\nIt is extremely easy to use and you can train state of the art(SOTA) ML Models in 4 lines of code(Continue looking at this notebook to see how)!\n\nCerbo wraps both Scikit-Learn, and Tensorflow(mainly with Keras) in an easy to use way and it provides a ton of useful functions such as,\n  *     Creating your own data(Multidimensional data for regression and clustering)\n  *     Loading in CSV Files(As seen in this dataset)\n  *     Reading from Image Directory's for CNN's!\n    \nIt also has many machine learning and deep learning algorithms such as:\n  *      CNNs\n  *      GANs\n  *      Boosting(XGB, Gradient Boosting, and Ada Boost)\n  *      KNN's\n  *      And More!\n\nCerbo is also an open-source project so if you are interested in contributing just write a pull request and we will look over it!","830115b8":"## Loading Data and Getting Insights","fe730375":"To save the model do\n\n```save_model(\"model_name\")```","ff52a1df":"## Imports","0c23b13f":"## KNN\n\nEach model returns the actual model, along with a list of predictions","65611299":"Today, I will be demonstrating how to use Cerbo on the Diabetes Prediction Dataset!","d6a1380c":"# Overview\n* What is Cerbo?\n* Using Cerbo for Diabetes Prediction\n    * Data Visualization \n    * Modelling\n\n"}}