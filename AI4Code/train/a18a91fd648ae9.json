{"cell_type":{"91c9ed01":"code","acc9c388":"code","f9eda965":"code","8cd1fcc8":"code","645ffa90":"code","d9045bce":"code","3c767210":"code","00c07bdb":"code","a382f9c7":"code","c428c728":"code","bdca4f0d":"code","6d9ebd29":"code","af3f4ac1":"code","7db29575":"code","7b8aebfc":"code","2e89ef23":"code","f607b7ff":"code","9563325c":"code","1b69f00c":"code","80ff8f69":"code","9461bec9":"code","af67551a":"code","ee88a201":"code","5766f739":"code","b98276e8":"code","f12367bb":"code","ef1a4b00":"code","ba86c9ad":"code","e681102f":"code","928c38ab":"code","085ab2a2":"code","b1d40820":"markdown","d36dad57":"markdown","bb43ff5e":"markdown","9523a07d":"markdown","9fd21036":"markdown","1b422a5e":"markdown","d73285bd":"markdown","0992940b":"markdown","2181c706":"markdown","ae4ff9ce":"markdown"},"source":{"91c9ed01":"import math\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\ndf = pd.read_csv(\"..\/input\/iris-flower-dataset\/IRIS.csv\")\n\ndf.info()\ndf.head()","acc9c388":"# get numpy arrays\nlabel = \"species\"\n\ny = df[label].values\nX = df[[col for col in df.columns if col != label]].values","f9eda965":"label_dict = {1: 'Iris-Setosa',\n              2: 'Iris-Versicolor',\n              3: 'Iris-Virgnica'}\n\nfeature_dict = {0: 'sepal length [cm]',\n                1: 'sepal width [cm]',\n                2: 'petal length [cm]',\n                3: 'petal width [cm]'}\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(8, 6))\n    for cnt in range(4):\n        plt.subplot(2, 2, cnt+1)\n        for lab in ('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'):\n            plt.hist(X[y==lab, cnt],\n                     label=lab,\n                     bins=10,\n                     alpha=0.3,)\n        plt.xlabel(feature_dict[cnt])\n    plt.legend(loc='upper right', fancybox=True, fontsize=8)\n\n    plt.tight_layout()\n    plt.savefig('PREDI.png', format='png', dpi=1200)\n    plt.show()","8cd1fcc8":"# step 1: standardize the data\nfrom sklearn.preprocessing import StandardScaler\n\nX_std = StandardScaler().fit_transform(X)","645ffa90":"# step 2: eigendecomposition\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%s' %eig_vecs)\nprint('\\nEigenvalues \\n%s' %eig_vals)\nnp.linalg.norm(eig_vecs[0]) # eigenvectors have unit norm","d9045bce":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","3c767210":"# step 3: determine top k eigenvectors\ntot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n\n    plt.bar(range(4), var_exp, alpha=0.5, align='center',\n            label='individual explained variance')\n    plt.step(range(4), cum_var_exp, where='mid',\n             label='cumulative explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()\nplt.savefig('PREDI2.png', format='png', dpi=1200)\nplt.show()","00c07bdb":"# step 4: make the projection matrix W\nmatrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n                      eig_pairs[1][1].reshape(4,1)))\nprint('Matrix W:\\n', matrix_w)","a382f9c7":"# step 5: project data X onto new feature space\nY = X_std.dot(matrix_w)\n\nwith plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(6, 4))\n    for lab, col in zip(('Iris-setosa', 'Iris-versicolor', 'Iris-virginica'),\n                        ('blue', 'red', 'green')):\n        plt.scatter(Y[y==lab, 0],\n                    Y[y==lab, 1],\n                    label=lab,\n                    c=col)\n    plt.xlabel('Principal Component 1')\n    plt.ylabel('Principal Component 2')\n    plt.legend(loc='lower center')\n    plt.tight_layout()\n    plt.show()","c428c728":"# Using sklearn\nfrom sklearn.decomposition import PCA as sklearnPCA\nsklearn_pca = sklearnPCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X_std)","bdca4f0d":"# first encode the label so it's convenient to work with\nfrom sklearn.preprocessing import LabelEncoder\n\nenc = LabelEncoder()\nlabel_encoder = enc.fit(y)\ny = label_encoder.transform(y) + 1\n\nlabel_dict = {1: 'Setosa', 2: 'Versicolor', 3:'Virginica'}","6d9ebd29":"# EDA like before\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,6))\n\nfor ax,cnt in zip(axes.ravel(), range(4)):  \n\n    # set bin sizes\n    min_b = math.floor(np.min(X[:,cnt]))\n    max_b = math.ceil(np.max(X[:,cnt]))\n    bins = np.linspace(min_b, max_b, 25)\n\n    # plottling the histograms\n    for lab,col in zip(range(1,4), ('blue', 'red', 'green')):\n        ax.hist(X[y==lab, cnt],\n                   color=col,\n                   label='class %s' %label_dict[lab],\n                   bins=bins,\n                   alpha=0.5,)\n    ylims = ax.get_ylim()\n\n    # plot annotation\n    leg = ax.legend(loc='upper right', fancybox=True, fontsize=8)\n    leg.get_frame().set_alpha(0.5)\n    ax.set_ylim([0, max(ylims)+2])\n    ax.set_xlabel(feature_dict[cnt])\n    ax.set_title('Iris histogram #%s' %str(cnt+1))\n\n    # hide axis ticks\n    ax.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n\n    # remove axis spines\n    ax.spines[\"top\"].set_visible(False)  \n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"bottom\"].set_visible(False)\n    ax.spines[\"left\"].set_visible(False)    \n\naxes[0][0].set_ylabel('count')\naxes[1][0].set_ylabel('count')\n\nfig.tight_layout()       \n\nplt.show()","af3f4ac1":"# step 1: mean vectors\nnp.set_printoptions(precision=4)\n\nmean_vectors = []\nfor _cls in np.unique(y):\n    mean_vectors.append(np.mean(X[y==_cls], axis=0))\n    print('Mean Vector class %s: %s\\n' %(label_dict[_cls], mean_vectors[_cls-1]))\n    \n# for use later\nglobal_mean_vector = np.mean(X, axis=0)","7db29575":"# step 2: scatter matrices\nS_W = np.zeros((4,4))\nfor cl, mv in zip(np.unique(y), mean_vectors):\n    class_sc_mat = np.zeros((4,4))                  # scatter matrix for every class\n    for row in X[y == cl]:\n        row, mv = row.reshape(4,1), mv.reshape(4,1) # make column vectors\n        class_sc_mat += (row-mv).dot((row-mv).T)\n    S_W += class_sc_mat                             # sum class scatter matrices\nprint('within-class Scatter Matrix:\\n', S_W)","7b8aebfc":"# equivalent to cell above, less code, harder to read\ndd = np.zeros((4,4))\nfor idx in range(1,4):\n    dd += np.dot((X[y==idx] - mean_vectors[idx-1]).T, (X[y==idx] - mean_vectors[idx-1]))\ndd","2e89ef23":"# between scatter matrix\nS_B = np.zeros((4,4))\nfor i,mean_vec in enumerate(mean_vectors):  \n    n = X[y==i+1,:].shape[0]\n    mean_vec = mean_vec.reshape(4,1) # make column vector\n    global_mean_vector = global_mean_vector.reshape(4,1) # make column vector\n    S_B += n * (mean_vec - global_mean_vector).dot((mean_vec - global_mean_vector).T)\n\nprint('between-class Scatter Matrix:\\n', S_B)","f607b7ff":"# step 3: eigendecomposition of scatter matrices\neig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n\nfor i in range(len(eig_vals)):\n    eigvec_sc = eig_vecs[:,i].reshape(4,1)   \n    print('\\nEigenvector {}: \\n{}'.format(i+1, eigvec_sc.real))\n    print('Eigenvalue {:}: {:.2e}'.format(i+1, eig_vals[i].real))","9563325c":"# step 4: choose k << d eigenvectors for new subspace\n# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n\nprint('Eigenvalues in decreasing order:\\n')\nfor i in eig_pairs:\n    print(i[0])\n    \nprint('\\nVariance explained:\\n')\neigv_sum = sum(eig_vals)\nfor i,j in enumerate(eig_pairs):\n    print('eigenvalue {0:}: {1:.2%}'.format(i+1, (j[0]\/eigv_sum).real))\n","1b69f00c":"# Step 4: cont'd\nW = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\nprint('Matrix W:\\n', W.real)","80ff8f69":"# step 5: transform samples onto the new subspace\nX_lda = X.dot(W)\nassert X_lda.shape == (150,2), \"The matrix is not 150x2 dimensional.\"","9461bec9":"from matplotlib import pyplot as plt\n\ndef plot_step_lda():\n\n    ax = plt.subplot(111)\n    for label,marker,color in zip(\n        range(1,4),('^', 's', 'o'),('blue', 'red', 'green')):\n\n        plt.scatter(x=X_lda[:,0].real[y == label],\n                y=X_lda[:,1].real[y == label],\n                marker=marker,\n                color=color,\n                alpha=0.5,\n                label=label_dict[label]\n                )\n\n    plt.xlabel('LD1')\n    plt.ylabel('LD2')\n\n    leg = plt.legend(loc='upper right', fancybox=True)\n    leg.get_frame().set_alpha(0.5)\n    plt.title('LDA: Iris projection onto the first 2 linear discriminants')\n\n    # hide axis ticks\n    plt.tick_params(axis=\"both\", which=\"both\", bottom=\"off\", top=\"off\",  \n            labelbottom=\"on\", left=\"off\", right=\"off\", labelleft=\"on\")\n\n    # remove axis spines\n    ax.spines[\"top\"].set_visible(False)  \n    ax.spines[\"right\"].set_visible(False)\n    ax.spines[\"bottom\"].set_visible(False)\n    ax.spines[\"left\"].set_visible(False)    \n\n    plt.grid()\n    plt.tight_layout\n    plt.show()\n\nplot_step_lda()","af67551a":"# in Sklearn\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nsklearn_lda = LDA(n_components=2)\nX_lda_sklearn = sklearn_lda.fit_transform(X, y)","ee88a201":"# even class distribition\ndf.species.value_counts(normalize=True)","5766f739":"from sklearn.model_selection import train_test_split\n\nlabel = \"species\"\ny = df[label]\nenc = LabelEncoder()\nlabel_encoder = enc.fit(y)\n# make labels ints\ny = label_encoder.transform(y)\n# to get the labels back: label_encoder.inverse_transform(y)\n\nX = df[[col for col in df.columns if col != label]]\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=73, #stratitfy=y\n)","b98276e8":"# close enough, stratify argument can't work\nnp.bincount(y_train) \/ len(y_train), np.bincount(y_test) \/ len(y_test)","f12367bb":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n# using prior info to choose n_components\npca_pipe = Pipeline([(\"scaler\", StandardScaler()), (\"pca\", sklearnPCA(n_components=2))])\n# don't need to standardize for LDA: https:\/\/stats.stackexchange.com\/a\/110803\nlda_pipe = Pipeline([(\"lda\", LDA(n_components=1))])\n\n# fit on training data to prevent leakage\npca_fit = pca_pipe.fit(X_train)\nlda_fit = lda_pipe.fit(X_train, y_train)","ef1a4b00":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n\ndef evaluate_predictions(true, pred):\n    print(\"F1:\".ljust(10), f1_score(true, pred, average=\"macro\"))\n    print(\"Precision:\".ljust(10), precision_score(true, pred, average=\"macro\"))\n    print(\"Recall\".ljust(10), recall_score(true, pred, average=\"macro\"))\n\ndef train_and_validate(train_x, train_y, estimator):\n    skf = StratifiedKFold(n_splits=3)\n    for train_index, valid_index in skf.split(train_x, train_y):\n        # training set\n        X_t, y_t = train_x[train_index], train_y[train_index]\n        # validation set\n        X_v, y_v = train_x[valid_index], train_y[valid_index]\n        # train model\n        clf = estimator.fit(X_t, y_t)\n        preds_t = clf.predict(X_t)\n        preds_v = clf.predict(X_v)\n        \n        # evaluate\n        print(\"Training set evaluation - \")\n        evaluate_predictions(preds_t, y_t)\n        print(\"\\nValidation set evaluation - \")\n        evaluate_predictions(preds_v, y_v)\n        print(\"\\n\"*2)\n    return estimator.fit(train_x, train_y)","ba86c9ad":"from sklearn.linear_model import LogisticRegression\n\n# pca data\nX_pca = pca_fit.transform(X_train)\n\nlr_pca = train_and_validate(X_pca, y_train, LogisticRegression(random_state=312))","e681102f":"# LDA data\nX_lda = lda_fit.transform(X_train)\n\nlr_lda = train_and_validate(X_lda, y_train, LogisticRegression(random_state=312))","928c38ab":"# Compare test performance\npreds_pca = lr_pca.predict(pca_fit.transform(X_test))\npreds_lda = lr_lda.predict(lda_fit.transform(X_test))\n\nprint(\"PCA::\")\nevaluate_predictions(preds_pca, y_test)\nprint(\"\\nLDA::\")\nevaluate_predictions(preds_lda, y_test)","085ab2a2":"# TODO: plot the LDA LR deciision boundary if I can","b1d40820":"Next, is a bit overkill for such a small dataset, but it is the \"right way\" to do things. I'll do a stratified k-fold split for training and validation sets.\n\n## Compare PCA and LDA","d36dad57":"# LDA vs PCA on Iris\n\nCompare two matrix factorization techniques, **PCA** and **LDA** (Linear Discriminant Analysis), for multi-class classification problem using the classic Iris dataset.\n\nI'm going off of these two awesome blog posts to better understand each step of these techniques:\n- https:\/\/www.apsl.net\/blog\/2017\/07\/18\/using-linear-discriminant-analysis-lda-data-explore-step-step\/\n- https:\/\/www.apsl.net\/blog\/2017\/06\/21\/using-principal-component-analysis-pac-data-explore-step-step\/\n\nContents:\n- [PCA](#PCA)\n- [LDA](#LDA)\n- [Compare PCA and LDA](#Compare-PCA-and-LDA)\n- [Building a classifier](#Building-a-classifier)","bb43ff5e":"## Results\n\nPerhaps I shouldn't be surprised by the LDA's single feature superior performance due to the supervised nature of the dimensionality reduction technique. However, I _am_ surprised mostly because it's a new technique for me that I leared from Andreas Mueller's Scipy 2020 talk about `dabl`, a package that I'm sure would've been useful here.\n\n## Things I've learned:\n- Sklearn actually has a lot of stuff scattered about to learn. It also doesn't work intuitively (to me!) yet\n- Pipelines are nice, they're my friend\n- Sklearn works with numpy arrays mostly, so getting more familiar with Numpy will help given my pandas dependence (I like naming columns!)\n- [Sklearn has tutorials!](https:\/\/scikit-learn.org\/stable\/tutorial\/index.html) I should go thru them to get more familiar with what's available.\n- [Matplotlib also has tutorials](https:\/\/matplotlib.org\/stable\/tutorials\/index.html), definitely need those to learn the API(s) better.","9523a07d":"Quick note on using the covariance vs correlation matrix.\n\nEspecially, in the field of \"Finance\", the correlation matrix typically used instead of the covariance matrix. However, the eigendecomposition of the covariance matrix (if the input data was standardized) yields the same results as a eigendecomposition on the correlation matrix, since **the correlation matrix can be understood as the normalized covariance matrix.**","9fd21036":"Probably only need the first eigenvector, as opposed to PCA above which suggested 2.","1b422a5e":"## LDA\n\n5 steps:\n1. Compute the d-dimensional mean vectors for each class (where d=number of features)\n2. Compute two \"scatter\" matrices: (a) between-class matrix and (b) within-class matrix\n3. Compute the eigendecomposition for both the combined scatter matrices: $Av=\\lambda v$ where $A=S_W^{-1}S_B$, where $W$ and $B$ indicate the within- and between scatter matrices.\n4. Sort the eigenvectors in descending order by their eigenvalues to form a $d \\times k$ matrix $W$ of eigenvectors.\n5. Use the $W$ matrix to transform the sample into the new subspace. using matrix multiplication: $Z=XW$","d73285bd":"## PCA\n\nSteps:\n1. Standarize the data (be careful of data leakage if using PCA components for modeling)\n2. Obtain the eigendecomposition from the covariance or correlation matrix\n3. Sort eigenvalues in descending order and choose the $k$ eigvenvectors that correspond to the $k$ largest eigenvalues\n4. Construct the project matrix $W$ from the selected $k$ eigenvectors\n5. Transform the original dataset $X$ via $W$ to obtain a $k$-dimensional feature subspace","0992940b":"Together, the first two principal components contain 95.8% of the information.","2181c706":"## Building a classifier\n\nRoughly the steps to follow:\n\n1. Check for class imbalance\n2. Settle on performance metric for training: we'll just look at a bunch\n3. Split data into training test split\n4. Train models using (stratified?) k-fold cross-validation and report validation performance\n5. Evaluate final models on test set","ae4ff9ce":"## Load the data"}}