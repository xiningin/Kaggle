{"cell_type":{"08bb230f":"code","1a27d71b":"code","1b89c1fb":"code","3de10adb":"code","b98831be":"code","d730a3fc":"code","a9451805":"code","5d8bde67":"code","eb1ff4ed":"code","a1eec465":"code","ffffb0ab":"code","e83c0e5f":"code","96158f8d":"code","c1be41d7":"code","34b88a3d":"code","20ffbfa3":"code","06fdce3e":"code","5c959471":"code","10efb707":"code","bda8c5a8":"code","cedb6fa7":"code","66ceca98":"code","40aad977":"markdown","9392d417":"markdown","03f5645d":"markdown","cec654b5":"markdown","5ce2fcfc":"markdown","7f2049d4":"markdown"},"source":{"08bb230f":"import os\nimport gc\nimport warnings\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #suppressing GPU warnings\nwarnings.filterwarnings('ignore')\n\nimport optuna.integration.lightgbm as lgbm  #for optuna-integrated version of LGBM\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport numpy as np\nimport pandas as pd\n\nSEED = 2311","1a27d71b":"#using a private dataset where I have saved the data in feather format for fast loading\ndata_dir = '\/kaggle\/input\/tps-oct21-data\/'","1b89c1fb":"%%time\ntrain = pd.read_feather(data_dir + 'tps-oct21-train.feather')\ntest = pd.read_feather(data_dir + 'tps-oct21-test.feather')","3de10adb":"train.head(3)","b98831be":"test.head(3)","d730a3fc":"target = train.pop('target')\n\nsubmission_index = test['id']  #for final csv submission\n\ntrain.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)","a9451805":"features = list(train.columns)\ncat_features = list(train.select_dtypes(include=['int8']))\nfeatures[:5], cat_features[:5]","5d8bde67":"xtrain, xval, ytrain, yval = train_test_split(\n    train, target,\n    test_size=0.2,\n    stratify=target,\n    random_state=SEED\n)\n\ntrain_set = lgbm.Dataset(xtrain, label=ytrain, \n                         feature_name=features, \n                         categorical_feature=cat_features)\n\nval_set = lgbm.Dataset(xval, label=yval, \n                       reference=train_set)","eb1ff4ed":"params = {\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'boosting':'gbdt',\n    'device_type':'cpu',\n    'verbosity': -1,   \n    'seed': SEED\n}","a1eec465":"%%time\nmodel = lgbm.train(\n    params,\n    train_set=train_set,\n    valid_sets=[val_set],\n    early_stopping_rounds=50,\n    verbose_eval=100,\n)","ffffb0ab":"best_params = model.params #saving for later\n\nprint('Best params: ')\nfor key, value in best_params.items():\n        print(f'\\t{key}: {value}')\n        \nprint(f'Best score: {model.best_score}')","e83c0e5f":"val_preds = model.predict(xval, num_iteration=model.best_iteration)\nval_roc = roc_auc_score(yval, val_preds)\n\n#just checking since we are minimizing logloss instead of maximizing AUC\nprint(f'AUC with best params: {val_roc:.5f}')","96158f8d":"del xtrain, xval, ytrain, yval, train_set, val_set\ngc.collect()","c1be41d7":"model = LGBMClassifier(**best_params)","34b88a3d":"oof_preds = pd.DataFrame(index=train.index, columns=['prediction'], dtype=np.float16)\n\ntest_preds = pd.DataFrame(index=submission_index,\n                          columns=['prediction0', 'prediction1', 'prediction2', \n                                   'prediction3', 'prediction4'],\n                          dtype=np.float16)","20ffbfa3":"%%time\nN_SPLITS = 5\n\ncv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\nfor fold, (train_idx, val_idx) in enumerate(cv.split(train, target)):\n    print('---------------Fold: ' + str(fold) + '---------------')\n    \n    xtrain, xval = train.iloc[train_idx], train.iloc[val_idx]\n    ytrain, yval = target[train_idx], target[val_idx]\n    \n    model.fit(\n        xtrain, ytrain,\n        eval_set=[(xval, yval)],\n        eval_metric='auc',\n        early_stopping_rounds=50,\n        verbose=100\n    )\n    \n    oof_preds.loc[val_idx, 'prediction'] = model.predict_proba(xval)[:, 1]\n    test_preds.loc[:, 'prediction' + str(fold)] = model.predict_proba(test)[:, 1]","06fdce3e":"oof_preds.head()","5c959471":"test_preds.head()","10efb707":"cv_score = roc_auc_score(target, oof_preds.prediction)\nprint(f'CV score (AUC): {cv_score:.5f}')","bda8c5a8":"#saving OOF predictions for stacking ensemble\noof_preds.to_csv('oof_2_lgbm.csv', index=False)","cedb6fa7":"final_preds = np.asarray(test_preds.mean(axis=1))\ntype(final_preds), final_preds[:5]","66ceca98":"output = pd.DataFrame({'id': submission_index, 'target': final_preds})\noutput.to_csv('sub_2_lgbm.csv', index=False)\n\n!head sub_2_lgbm.csv","40aad977":"**References:**  \n1. [Optuna LightGBMTuner documentation](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.integration.lightgbm.LightGBMTuner.html)  \n2. [LightGBM Tuner: New Optuna Integration for Hyperparameter Optimization](https:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258)  \n3. [Kaggler\u2019s Guide to LightGBM Hyperparameter Tuning with Optuna in 2021](https:\/\/towardsdatascience.com\/kagglers-guide-to-lightgbm-hyperparameter-tuning-with-optuna-in-2021-ed048d9838b5) \n\nLightGBMTuner tunes the important hyperparameter variables in a step-wise manner. Since the hyperparameters are tuned sequentially, the search space is a sum of their individual search spaces instead of a multiplicative search space created while trying to find the best combination at once. This point is explained with illustrations in Reference 2.\nThe hyperparameters which are tuned are:  \n* regularization factors - lambda l1, lambda l2\n* num_leaves\n* feature_fraction\n* bagging_fraction, bagging_freq\n* min_child_samples, min_data_in_leaf  \n  \nThese hyperparameters are explained well in Reference 3.","9392d417":"**Generating OOF and Test predictions using best params**","03f5645d":"**CV Score using OOF predictions:**","cec654b5":"A lot of output! \n  \nTL;DR - A total of 67 trials, with some number of trials dedicated to tuning of each hyperparameter, e.g. 20 trials for regularization parameters, 10 trials for bagging (i.e., bagging_fraction + bagging_freq) and so on. The score kept improving until the very end since the tuning was done in a logical (step-wise) manner. Trial 66 was the best result.  \nWe can see the best hyperparameters determined by the tuner:","5ce2fcfc":"Time to submit!","7f2049d4":"**Averaging test predictions for final submission:**"}}