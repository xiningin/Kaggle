{"cell_type":{"032d8a55":"code","450ac3e1":"code","b97e22d3":"code","0deb0b7f":"code","22b8b40d":"code","6a492e86":"code","5d89c0ec":"code","5dd58c59":"code","fbc7b0d6":"code","d35df859":"code","298cd79d":"code","13737594":"code","d82b2066":"code","b1c1983e":"code","300dc46c":"code","5c7c470d":"code","4c4a6a17":"code","52737c1b":"code","51fc1c1c":"code","9547dc41":"code","d7cb8709":"code","94db7b47":"code","e4359d59":"code","4e79a89b":"code","b6a7efc1":"code","6656efaa":"code","3639c202":"code","30edc689":"code","6b2c7212":"code","8aa77d90":"code","733d0fdd":"code","269f07c2":"code","9085fee6":"code","50a5f6d8":"code","caf4f39c":"code","d45c5aa8":"code","10770f6a":"code","f6c10bbe":"markdown","f6b1ca1d":"markdown","3982785e":"markdown","f0234fe8":"markdown","ea1e6ab7":"markdown","973cb2a2":"markdown"},"source":{"032d8a55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","450ac3e1":"#Importing Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\n%matplotlib inline","b97e22d3":"#Importing Datasets\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n#Check both the datasets\ntrain_data.head()\n#train_data.shape\n#train_data.describe()\n#train_data.isnull().sum()\n\ntest_data.head()\n#test_data.shape\n#test_data.describe()\n#test_data.isnull().sum()","0deb0b7f":"#Feature : Age\ntrain_data['Age'].mean()\ntest_data['Age'].mean()","22b8b40d":"train_data['Age'].fillna(train_data['Age'].median(), inplace = True)","6a492e86":"test_data['Age'].fillna(test_data['Age'].median(), inplace = True)","5d89c0ec":"#Feature : Embarked\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace = True)\ntest_data['Embarked'].fillna(test_data['Embarked'].mode()[0], inplace = True)","5dd58c59":"#Feature : Fare\ntest_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)","fbc7b0d6":"#Feature : 'Sex','Embarked'\ntest_data['Sex_Male'] = (test_data['Sex'] == \"male\").astype(int)\ntest_data['Embarked_S'] = (test_data['Embarked'] == \"S\").astype(int)","d35df859":"#SibSp(Sibling-Spouse) and Parch (Parent-children) will help us to determine the size of the family\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch']\ntrain_data.head()","298cd79d":"#Creating new column Agegroup from Age \nbins= [0,12,20,30,60,100]\nlabels = ['Children','Teen','Young Adult','Adult','Elder']\ntrain_data['AgeGroup'] = pd.cut(train_data['Age'], bins=bins, labels=labels, right=False)\nprint (train_data)","13737594":"#Drop the useless columns\nuseless_cols = ['PassengerId','Name','Ticket','Cabin','SibSp','Parch','Age']\ntrain_data.drop(useless_cols,axis=1,inplace=True)","d82b2066":"train_data.head()","b1c1983e":"#Drop the useless columns\nuseless_cols = ['Cabin','Fare']\ntest_data.drop(useless_cols,axis=1,inplace=True)","300dc46c":"#No null values in the train_data\ntrain_data.isnull().sum()","5c7c470d":"#No null values in the test_data\ntest_data.isnull().sum()","4c4a6a17":"#Correlating each column in the data\ntrain_data.corr()","52737c1b":"#Survival by Gender\n#Female Survival is more than Male\nsns.set_style('white')\nsns.barplot(train_data['Sex'],train_data['Survived'])\n","51fc1c1c":"#Survival by Pclass\n#Class 1 survival is better than Class 2 and Class 3\nsns.barplot(train_data['Pclass'],train_data['Survived'])","9547dc41":"#Survival by FamilySize\n#The familysize of 3 has better survival than others\nsns.barplot(train_data['FamilySize'],train_data['Survived'])","d7cb8709":"#Survival by Embarked \n#Port of Embarkation: C has better survival\nsns.barplot(train_data['Embarked'],train_data['Survived'])","94db7b47":"#Survival by Agegroup\n#Children Agegroup has better survival rate\nsns.barplot(train_data['AgeGroup'],train_data['Survived'])","e4359d59":"#Visualize relation between each column using heatmap \nsns.heatmap(train_data.corr())","4e79a89b":"#Convert Categorical data to Numeric\ntrain_data['Sex'].replace(['male','female'],[0,1],inplace=True)\ntrain_data['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ntrain_data['AgeGroup'].replace(['Children','Teen','Young Adult','Adult','Elder'],[0,1,2,3,4],inplace=True)","b6a7efc1":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix","6656efaa":"#Split dataset into train and test data\nfrom sklearn.model_selection import train_test_split\n\nX = train_data[train_data.loc[:, train_data.columns != 'Survived'].columns]\ny = train_data['Survived']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=0)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","3639c202":"#Using Logistic Regression\nlogmodel=LogisticRegression()\nlogmodel.fit(X_train,y_train)\npredictions = logmodel.predict(X_test)\nacc_LR = logmodel.score(X_test, y_test)\nprint('Accuracy:',acc_LR)","30edc689":"#Confusion Matrix\ncnf_matrix = metrics.confusion_matrix(y_test, predictions)\nprint(cnf_matrix)\n\n#Visualizing Confusion Matrix using HeatMap\nconfusion_matrix = pd.crosstab(y_test, predictions, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True)","6b2c7212":"#Using Gaussian Naive Bayes\ngnb = GaussianNB()\ngnb.fit(X_train, y_train)\n \n# making predictions on the testing set\ny_pred = gnb.predict(X_test)\n \n# comparing actual response values (y_test) with predicted response values (y_pred)\nfrom sklearn import metrics\nprint(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred))\n\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\nprint(cnf_matrix)\n\n#Visualizing Confusion Matrix using HeatMap\nconfusion_matrix = pd.crosstab(y_test,y_pred, rownames=['Actual'], colnames=['Predicted'])\nsns.heatmap(confusion_matrix, annot=True)\n ","8aa77d90":"#Using SVM\nclf = svm.SVC(kernel='linear') # Linear Kernel\n\n#Train the model using the training sets\nclf.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n\n#Metrics\nacc_svm = print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","733d0fdd":"#Using Decision Tree\nclf = DecisionTreeClassifier()\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\nacc_DT = print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","269f07c2":"#Using KNN\nknn = KNeighborsClassifier(n_neighbors=3)\n\n#Train the model using the training sets\nknn.fit(X_train, y_train)\n\n#Predict the response for test dataset\ny_pred = knn.predict(X_test)\nacc_knn = print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","9085fee6":"#Using Random Forest\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nacc_RF = print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","50a5f6d8":"#Random Forest provides better accuracy than other models.\nmethods = [\"Logistic Regression\", \"KNN\", \"SVM\", \"Naive Bayes\", \"Decision Tree\", \"Random Forest\"]\naccuracy = [0.787, 0.776,0.787,0.81,0.832, 0.849]\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,10))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\",fontsize=14)\nplt.xlabel(\"ML Models\",fontsize=14)\nsns.barplot(x=methods, y=accuracy)\nplt.show()","caf4f39c":"final_knn = KNeighborsClassifier(n_neighbors=3)\nfinal_knn.fit(X, y)","d45c5aa8":"X_test = test_data[['Pclass', 'Sex_Male', 'Age', 'SibSp', 'Parch', 'Embarked_S']].values\ny_pred = final_knn.predict(X_test)\nscore = final_knn.score(X_test,y_pred)\nscore","10770f6a":"result_submit = pd.DataFrame({'PassengerId': test_data.index, 'Survived': y_pred})\ndisplay(result_submit.head())\nresult_submit.to_csv(\"\/kaggle\/working\/submission.csv\", header=True, index=False)","f6c10bbe":"# Visualization of Data","f6b1ca1d":"# Applying Machine Learning Models","3982785e":"# Testing our Final Testdata using KNN","f0234fe8":"# Dealing with the missing values","ea1e6ab7":"![](http:\/\/faithmag.com\/sites\/default\/files\/styles\/article_full\/public\/2018-09\/titanic2.jpg?h=6521bd5e&itok=H8td6QVv)","973cb2a2":"# Comparing Each Model by Accuracy"}}