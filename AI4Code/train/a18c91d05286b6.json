{"cell_type":{"19b504f6":"code","9b42c1cd":"code","2d1b8961":"code","5cac979f":"code","03eab631":"code","5a1394b4":"code","4af5df60":"code","d1c37915":"code","e84c0fbc":"code","46ad82fb":"code","ad9e6720":"code","d1437a01":"code","d8a6210a":"code","ba695ee6":"code","2077bdff":"code","28941c94":"code","22cf3f02":"code","d76ae3af":"code","cc550d1f":"code","8c9f4d82":"code","07a74bee":"code","ec25902d":"code","c42082d4":"code","53da36a9":"code","0f907a50":"code","2ce7698a":"code","5225187a":"code","c0a1e2b5":"code","915e82cb":"code","e3e13c9a":"code","fb83e180":"markdown","cb7b00f5":"markdown","6fa78ebe":"markdown","74b1fceb":"markdown","5f39cea0":"markdown","5152c8ec":"markdown","16188776":"markdown","114685b9":"markdown","48d93293":"markdown","4eb37a2a":"markdown","3bcfb2cb":"markdown","2540e300":"markdown","120f1449":"markdown","1af11b22":"markdown","7facd9b8":"markdown","34757f02":"markdown","677e290c":"markdown","1a3fc73b":"markdown","0de57e28":"markdown","fdb16e3f":"markdown","1b977637":"markdown","1f88c2b7":"markdown","6b61818a":"markdown","9c206ca6":"markdown","6b0fabdb":"markdown","cce6becd":"markdown","e549dc56":"markdown","99958da2":"markdown","845312c3":"markdown","6ac5c459":"markdown"},"source":{"19b504f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9b42c1cd":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.layers import Input, Dense, Dropout\nfrom keras.models import Model\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","2d1b8961":"data = pd.read_csv(\"..\/input\/Fraud_data_amtstd.csv\")","5cac979f":"data.shape","03eab631":"data.head()","5a1394b4":"data.columns","4af5df60":"data.dtypes","d1c37915":"print(pd.value_counts(data['Class']))\n\nprint(pd.value_counts(data['Class'])\/data['Class'].shape[0])","e84c0fbc":"# Drawing a barplot\npd.value_counts(data['Class']).plot(kind = 'bar', rot=0)\n\n# Giving titles and labels to the plot\nplt.title(\"Transaction class distribution\")\nplt.xticks(range(2), [\"Normal\", \"Fraud\"])\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","46ad82fb":"data = data.values","ad9e6720":"data_nf = data[data[:,-1] == 0]\ntest_f  = data[data[:,-1] == 1]\n\ntrain_nf, test_nf = train_test_split(data_nf, test_size=0.2, random_state=123)","d1437a01":"print(data.shape)\nprint(train_nf.shape)\nprint(test_nf.shape)\nprint(test_f.shape)","d8a6210a":"print(np.unique(data[:,-1], return_counts=True))\nprint(np.unique(train_nf[:,-1], return_counts=True))\nprint(np.unique(test_nf[:,-1], return_counts=True))\nprint(np.unique(test_f[:,-1], return_counts=True))","ba695ee6":"X_train_nf = train_nf[:,:-1]\n\nX_test_nf = test_nf[:,:-1]\n\nX_test_f = test_f[:,:-1]","2077bdff":"input_dim = X_train_nf.shape[1]\n#encoding_dim = 15\nencoding_dim = 150","28941c94":"# Input placeholder\ninput_att = Input(shape=(input_dim,))\n\ninput_dropout = Dropout(0.2)(input_att)\n \n# \"encoded\" is the encoded representation of the input\nencoded = Dense(encoding_dim, activation='relu')(input_dropout)\n\n# \"decoded\" is the lossy reconstruction of the input\ndecoded = Dense(input_dim, activation='linear')(encoded)","22cf3f02":"autoencoder = Model(input_att, decoded)","d76ae3af":"autoencoder.compile(loss='mean_squared_error', optimizer='adam')","cc550d1f":"%time autoencoder.fit(X_train_nf, X_train_nf, epochs=50, shuffle=True, validation_split=0.2, verbose=1)","8c9f4d82":"autoencoder.evaluate(X_train_nf, X_train_nf)","07a74bee":"autoencoder.evaluate(X_test_nf, X_test_nf)","ec25902d":"autoencoder.evaluate(X_test_f, X_test_f)","c42082d4":"def mse_for_each_record(act, pred):\n    error = act - pred\n    squared_error = np.square(error)\n    mean_squared_error = np.mean(squared_error, axis=1)\n    return mean_squared_error","53da36a9":"pred_train_nf = autoencoder.predict(X_train_nf)\n\nmse_train_nf = mse_for_each_record(X_train_nf, pred_train_nf)","0f907a50":"pred_test_nf = autoencoder.predict(X_test_nf)\n\nmse_test_nf = mse_for_each_record(X_test_nf, pred_test_nf)","2ce7698a":"pred_test_f = autoencoder.predict(X_test_f)\n\nmse_test_f = mse_for_each_record(X_test_f, pred_test_f)","5225187a":"plt.subplot(1, 3, 1)\nplt.boxplot(mse_train_nf)\n\nplt.subplot(1, 3, 2)\nplt.boxplot(mse_test_nf)\n\nplt.subplot(1, 3, 3)\nplt.boxplot(mse_test_f)","c0a1e2b5":"print(\"-------mse_train_nf-------\")\nprint(pd.Series(mse_train_nf).describe())\nprint(\"\\n-------mse_test_NF-------\")\nprint(pd.Series(mse_test_nf).describe())\nprint(\"\\n-------mse_test_f-------\")\nprint(pd.Series(mse_test_f).describe())","915e82cb":"cut_off = np.round(np.percentile(mse_train_nf,99),2)\n\nprint(\"Cut-off = {}\".format(cut_off))","e3e13c9a":"print(\"Non-fraud train records = {}%\".format(np.round(np.sum(mse_train_nf <= cut_off)\/train_nf.shape[0],2)*100))\nprint(\"Non-fraud test records  = {}%\".format(np.round(np.sum(mse_test_nf <= cut_off)\/test_nf.shape[0],2)*100))\nprint(\"Fraud test records      = {}%\".format(np.round(np.sum(mse_test_f > cut_off)\/test_f.shape[0],2)*100))","fb83e180":"#### Making predictions on the non-fraud train data","cb7b00f5":"### Build Autoencoder","6fa78ebe":"#### Data distribution w.r.t target attributes","74b1fceb":"#### Compile the model","5f39cea0":"#### mse box plots of non-fraud train, non-fraud test and fraud test data","5152c8ec":"#### Look at first 5 records","16188776":"#### Making predictions on the non-fraud test data","114685b9":"#### Summary statistics on mse of non-fraud train, non-fraud test and fraud test data ","48d93293":"#### Extract numpy array from the DataFrame","4eb37a2a":"### Reading the data","3bcfb2cb":"#### Decide cut-off","2540e300":"#### Evaluate the loss on non-fraud test data","120f1449":"#### Look at the distribution w.r.t target attribute","1af11b22":"#### Evaluate the loss on fraud test data","7facd9b8":"### Understand the data","34757f02":"#### Display column names","677e290c":"### Load required libraries","1a3fc73b":"#### Display No. of records and attributes ","0de57e28":"#### Making predictions on the fraud test data","fdb16e3f":"#### Evaluate the loss on non-fraud train data","1b977637":"#### Data type of each attribute","1f88c2b7":"#### Bar plot","6b61818a":"    O = Normal\n\n    1 = Fraud","9c206ca6":" Observation: \n \n        Data has 30 attributes and 1 lakh records","6b0fabdb":"#### Fit the model","cce6becd":"#### % of correctly predicted non-fraud train, non-fraud test and fraud test records","e549dc56":"#### Function to calculate mse for each record","99958da2":"### Train test split\n\n    Splitting the data into train and test, such that train data has only non-fraud records and test data has both. ","845312c3":"### Explore and identify right cut-off ","6ac5c459":"#### Only extract independent features"}}