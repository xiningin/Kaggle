{"cell_type":{"0635b23b":"code","314dfb87":"code","27d74e47":"code","434c6fa3":"code","2a419d70":"code","02d6c930":"code","c5770362":"code","83b79df6":"code","b6e6cf13":"code","35111702":"code","9662d594":"code","4ad7ac40":"code","c29d7d00":"code","33cacc98":"code","aeec67f1":"code","19617a98":"code","eca79dfe":"code","9284f0fc":"code","ef73e9c7":"code","e95639cc":"code","332cf1f4":"code","bb54fc84":"code","9cb888c0":"code","01e9ff8a":"code","e91eb28d":"code","2ca6bf71":"code","083595f8":"code","ea4d7857":"code","6e8334b0":"code","f15bf1d5":"code","42656ed0":"code","1137b9cf":"code","a66ec72e":"code","1e9a0de7":"code","554ab989":"code","3f2f1740":"code","b7845450":"code","2892f5f2":"code","090e35d3":"code","98733819":"code","87f694bb":"markdown","6f896a71":"markdown","5b8c2e16":"markdown","0c507e3d":"markdown","72af357d":"markdown","dcb0fe5f":"markdown","6244cd42":"markdown","b87e08cd":"markdown","1ffe7a42":"markdown","9c79742e":"markdown","00542c6c":"markdown","2578a44f":"markdown","b8c83766":"markdown","4607a327":"markdown","aa131b9a":"markdown","70941411":"markdown","85189eaa":"markdown","a4960829":"markdown","76f4b5c8":"markdown","267670c0":"markdown","1a9b1f9d":"markdown","c5d6dab6":"markdown","0bd761a8":"markdown","a764e2a4":"markdown","d060006c":"markdown","9779500c":"markdown","442dbdbc":"markdown","2cbe328b":"markdown","ef68eb35":"markdown","f616f9c0":"markdown","ec1362ac":"markdown","e57275f3":"markdown","ef25a8d4":"markdown","b1db139e":"markdown","feac14de":"markdown","483db9a5":"markdown","5491cf72":"markdown","f6a4bc66":"markdown","aa5bdb67":"markdown","b886fe09":"markdown","6025c74d":"markdown","03b1e707":"markdown","778866fe":"markdown","175fd7d2":"markdown","ee02ba34":"markdown","0397e249":"markdown","a7bd5ae7":"markdown","c1326dee":"markdown","c07a869a":"markdown","659106cb":"markdown","95b891d8":"markdown","c581d017":"markdown","6225b37e":"markdown","806e5869":"markdown","850f53cd":"markdown","13c51ba6":"markdown"},"source":{"0635b23b":"import pathlib\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport PIL\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom glob import glob\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPooling2D","314dfb87":"## If you are using the data by mounting the google drive, use the following :\nfrom google.colab import drive\ndrive.mount('\/content\/gdrive')\n##Ref:https:\/\/towardsdatascience.com\/downloading-datasets-into-google-drive-via-google-colab-bcb1b30b0166","27d74e47":"# Defining the path for train and test images\ntrain_path=\"gdrive\/My Drive\/CNN Assignment\/Train\/\"\ntest_path=\"gdrive\/My Drive\/CNN Assignment\/Test\/\"\ndata_dir_train = pathlib.Path(train_path)\ndata_dir_test = pathlib.Path(test_path)","434c6fa3":"image_count_train = len(list(data_dir_train.glob('*\/*.jpg')))\nprint(image_count_train)\nimage_count_test = len(list(data_dir_test.glob('*\/*.jpg')))\nprint(image_count_test)","2a419d70":"batch_size = 32\nimg_height = 180\nimg_width = 180","02d6c930":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir_train, labels='inferred', label_mode='categorical',\n    class_names=None, color_mode='rgb', batch_size=32, image_size=(180,\n    180), shuffle=True, seed=123, validation_split=0.2, subset='training',\n    interpolation='bilinear', follow_links=False, smart_resize=False\n)","c5770362":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir_train, labels='inferred', label_mode='categorical',\n    class_names=None, color_mode='rgb', batch_size=32, image_size=(180,\n    180), shuffle=True, seed=123, validation_split=0.2, subset='validation',\n    interpolation='bilinear', follow_links=False, smart_resize=False\n)","83b79df6":"class_names = train_ds.class_names\nprint(class_names)","b6e6cf13":"import matplotlib.pyplot as plt\nnum=0\nfor dirpath, dirnames, filenames in os.walk(str(train_path)):\n    for filename in [f for f in filenames if f.endswith(\".jpg\")][:1]:\n        img = PIL.Image.open(str(dirpath)+\"\/\"+str(filename))\n        plt.subplot(3,3,num+1)\n        plt.title(str(dirpath).split('\/')[-1])\n        plt.axis('off')\n        plt.imshow(img)\n        num=num+1","35111702":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","9662d594":"model=Sequential([\n    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.\/255., offset=0.0,),         \n    \n    Conv2D(32,(3,3),input_shape=(img_height,img_width,3),activation='relu',padding='same'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.1),\n    \n    Conv2D(64,(3,3),activation='relu',padding='same'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.1),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.25),   \n    Dense(9, activation='softmax')\n])\n","4ad7ac40":"### Todo, choose an appropirate optimiser and loss function\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","c29d7d00":"epochs = 20\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","33cacc98":"# View the summary of all layers\nmodel.summary()","aeec67f1":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","19617a98":"model_update=Sequential([\n    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.\/255., offset=0.0,),         \n                             \n    Conv2D(32,(3,3),input_shape=(img_height,img_width,3),activation='relu',padding='same'),\n    Conv2D(32,(3,3),activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.7),\n    \n    Conv2D(64,(3,3),activation='relu',padding='same'),\n    Conv2D(64,(3,3),activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.7),\n    \n    Conv2D(128,(3,3),activation='relu',padding='same'),\n    Conv2D(128,(3,3),activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.7),\n    \n    Flatten(),\n    Dense(100, activation='relu'),\n    Dropout(0.25),   \n    Dense(9, activation='softmax')\n])","eca79dfe":"model_update.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics='accuracy')","9284f0fc":"## Your code goes here, note: train your model for 20 epochs\nepochs = 20\nhistory = model_update.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","ef73e9c7":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","e95639cc":"data_augmentation=tf.keras.Sequential([\n  layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n  layers.experimental.preprocessing.RandomRotation(0.2)\n])","332cf1f4":"image, label = next(iter(train_ds))\nimage=np.array(image,np.int32)  \nplt.figure(figsize=(10, 10))\nfor i in range(9):\n  augmented_image = data_augmentation(image)\n  ax = plt.subplot(3, 3, i + 1)\n  augmented_image1=np.array(augmented_image[0],np.int32)  \n  plt.imshow((augmented_image1))\n  plt.axis(\"off\")","bb54fc84":"model_augmented=Sequential([\n    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.\/255., offset=0.0,),         \n\n    data_augmentation,\n\n    Conv2D(32,(3,3),input_shape=(img_height,img_width,3),activation='relu',padding='same'),\n    Conv2D(32,(3,3),activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.7),\n    \n    Conv2D(64,(3,3),activation='relu',padding='same'),\n    Conv2D(64,(3,3),activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.7),\n    \n    Conv2D(128,(3,3),activation='relu',padding='same'),\n    Conv2D(128,(3,3),activation='relu'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.7),\n    \n    Flatten(),\n    Dense(100, activation='relu'),\n    Dropout(0.25),   \n    Dense(9, activation='softmax')\n])","9cb888c0":"model_augmented.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics='accuracy')","01e9ff8a":"## Your code goes here, note: train your model for 20 epochs\nepochs = 20\nhistory = model_augmented.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","e91eb28d":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","2ca6bf71":"for i in class_names:\n    directory =train_path+i+'\/'\n    class_directory = pathlib.Path(directory)\n    length=len(list(class_directory.glob('*.jpg')))\n    print(f'{i} has {length} samples.')","083595f8":"!pip install Augmentor","ea4d7857":"path_to_training_dataset=train_path\nimport Augmentor\nfor i in class_names:\n    p = Augmentor.Pipeline(path_to_training_dataset + i)\n    p.rotate(probability=0.7, max_left_rotation=10, max_right_rotation=10)\n    p.sample(500) ## We are adding 500 samples per class to make sure that none of the classes are sparse.","6e8334b0":"image_count_train = len(list(data_dir_train.glob('*\/output\/*.jpg')))\nprint(image_count_train)","f15bf1d5":"path_list = [x for x in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]","42656ed0":"lesion_list_new = [os.path.basename(os.path.dirname(os.path.dirname(y))) for y in glob(os.path.join(data_dir_train, '*','output', '*.jpg'))]","1137b9cf":"dataframe_dict_new = dict(zip(path_list, lesion_list_new))","a66ec72e":"for i in class_names:\n    directory =train_path+i+'\/'\n    directory_out =train_path+i+'\/output\/'\n    class_directory = pathlib.Path(directory)\n    class_directory_out = pathlib.Path(directory_out)\n    length=len(list(class_directory.glob('*.jpg')))\n    length_out=len(list(class_directory_out.glob('*.jpg')))\n    length_tot=length+length_out\n    print(f'{i} has {length_tot} samples.')","1e9a0de7":"batch_size = 32\nimg_height = 180\nimg_width = 180","554ab989":"data_dir_train=train_path\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir_train,\n  seed=123, label_mode='categorical',\n  validation_split = 0.2,\n  subset = 'training',\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","3f2f1740":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir_train,\n  seed=123, label_mode='categorical',\n  validation_split = 0.2,\n  subset = 'validation',\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","b7845450":"model_final=Sequential([\n    tf.keras.layers.experimental.preprocessing.Rescaling(scale=1.\/255., offset=0.0,),         \n    \n    Conv2D(32,(3,3),input_shape=(img_height,img_width,3),activation='relu',padding='same'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.1),\n    \n    Conv2D(64,(3,3),activation='relu',padding='same'),\n    MaxPooling2D(pool_size=(2,2)),\n    Dropout(0.1),\n    \n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.25),   \n    Dense(9, activation='softmax')\n])","2892f5f2":"model_final.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics='accuracy')","090e35d3":"epochs = 30\n## Your code goes here, use 50 epochs.\nhistory = model_final.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","98733819":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","87f694bb":"### <font color=blue> 6.2 Visualizing the Augmented Data <a name='no6.2' \/>","6f896a71":"## <font color=blue>1. Importing all the important libraries <a name='no1' \/>","5b8c2e16":"### <font color=blue> 9.1 Installing Augmentor <a name='no9.1' \/>","0c507e3d":"-  The model accuracy for **Train data set** has dropped to nearly **45%**.\n-  The accuracy for the **Validation set** is at **40%**.\n-  This is a much better model compared to the previous model as there seems to be **No Overfit** with the training accuracy **45%** and validation accuracy at **40%**.","72af357d":"## <font color=blue>4. Model 0 <a name='no4' \/>","dcb0fe5f":"### <font color=blue> 5.1 Creating the Model <a name='no5.1' \/>","6244cd42":"### <font color=blue>7.4 Visualizing the results <a name='no7.4' \/>","b87e08cd":"### <font color=blue> 4.5 Observations <a name='no4.5' \/>","1ffe7a42":"### <font color=blue>11.3 Training the Model <a name='no11.3' \/>","9c79742e":"### <font color=blue>5.2 Compiling the model <a name='no5.2' \/>","00542c6c":"## <font color=blue>7. Model 2 <a name='no7' \/>","2578a44f":"### <font color=blue> 4.3 Training the model <a name='no4.3' \/>","b8c83766":"### <font color=blue> 11.4 Visualizing the model results <a name='no11.4' \/>","4607a327":"### <font color=blue>9.3 Observations <a name='no9.3' \/>","aa131b9a":"### <font color=blue>11.1 Creating the Model <a name='no11.1' \/>","70941411":"This dataset has about 2357 images of skin cancer types. The dataset contains 9 sub-directories in each train and test subdirectories. The 9 sub-directories contains the images of 9 skin cancer types respectively.","85189eaa":"### <font color=blue>7.2 Compiling the model <a name='no7.2' \/>","a4960829":"### <font color=blue> 6.1 Specifying the Augmentation <a name='no6.1' \/>","76f4b5c8":"## <font color=blue>9. Using Augmentor for Class Imbalance Treatment <a name='no9' \/>","267670c0":"## <font color=blue> 10. Modelling Augmented Data <a name='no10' \/>","1a9b1f9d":"-  The model accuracy for **Train data set** has gone up to nearly **48%**.\n-  The accuracy for the **Validation set** is also at **48%**.\n-  This is a much better model compared to the previous two models as there seems to be **No Overfit** with the training accuracy **48%** and validation accuracy at **48%**.\n-  `Data Augmentation` has improved the model performance.","c5d6dab6":"### <font color=blue>2.1 Train Data Set Creation <a name='no2.1' \/>","0bd761a8":"### <font color=blue> 5.4 Visualizing the results <a name='no5.4' \/>","a764e2a4":"# <font color=red> Skin Cancer Detection - ADHITHIA R (adhithia@gmail.com)","d060006c":"To use `Augmentor`, the following general procedure is followed:\n\n1. Instantiate a `Pipeline` object pointing to a directory containing your initial image data set.<br>\n2. Define a number of operations to perform on this data set using your `Pipeline` object.<br>\n3. Execute these operations by calling the `Pipeline\u2019s` `sample()` method.\n","9779500c":"The `image_batch` is a tensor of the shape `(32, 180, 180, 3)`. This is a batch of 32 images of shape `180x180x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.","442dbdbc":"### <font color=blue> 4.4 Visualizing training results <a name='no4.4' \/>","2cbe328b":"-  The training accuracy seems to be nearly **~90%**. \n-  The validation accuracy is nearly **~80%**.\n-  Though the model accuracy has improved, the **class rebalance** has helped **treat the overfitting to some extent**.\n-  Much better models could be built or tried out using **more epochs and more layers**.","ef68eb35":"Use 80% of the images for training, and 20% for validation.","f616f9c0":"### <font color=blue>4.2 Compiling the model <a name='no4.2' \/>","ec1362ac":"Augmentor has stored the augmented images in the output sub-directory of each of the sub-directories of skin cancer types.. Lets take a look at total count of augmented images.","e57275f3":"### <font color=blue> 5.3 Training the model <a name='no5.3' \/>","ef25a8d4":"## <font color=green>Bookmarks to Notebook Sections\n<font color=blue>1. Go to <a href=#no1>Importing all the important libraries<\/a><br>\n2. Go to <a href=#no2>Data Preparation<\/a><br>\n&emsp;&emsp;2.1 Go to <a href=#no2.1>Train Data Set Creation<\/a><br>\n&emsp;&emsp;2.2 Go to <a href=#no2.2>Validation Data Set Creation<\/a><br>\n3. Go to <a href=#no3>Visualizing the Data<\/a><br>\n4. Go to <a href=#no4>Model 0<\/a><br>\n&emsp;&emsp;4.1 Go to <a href=#no4.1>Creating the Model<\/a><br>\n&emsp;&emsp;4.2 Go to <a href=#no4.2>Compiling the Model<\/a><br>\n&emsp;&emsp;4.3 Go to <a href=#no4.3>Training the Model<\/a><br>\n&emsp;&emsp;4.4 Go to <a href=#no4.4>Visualizing the Training Results<\/a><br>\n&emsp;&emsp;4.5 Go to <a href=#no4.5>Observations<\/a><br>\n5. Go to <a href=#no5>Model 1<\/a><br>\n&emsp;&emsp;5.1 Go to <a href=#no5.1>Creating the Model<\/a><br>\n&emsp;&emsp;5.2 Go to <a href=#no5.2>Compiling the Model<\/a><br>\n&emsp;&emsp;5.3 Go to <a href=#no5.3>Training the Model<\/a><br>\n&emsp;&emsp;5.4 Go to <a href=#no5.4>Visualizing the Training Results<\/a><br>\n&emsp;&emsp;5.5 Go to <a href=#no5.5>Observations<\/a><br>\n6. Go to <a href=#no6>Data Augmentation<\/a><br>\n&emsp;&emsp;6.1 Go to <a href=#no6.1>Specifying the Augmentation<\/a><br>\n&emsp;&emsp;6.2 Go to <a href=#no6.2>Visualizing the Augmented Data<\/a><br>\n7. Go to <a href=#no7>Model 2<\/a><br>\n&emsp;&emsp;7.1 Go to <a href=#no7.1>Creating the Model<\/a><br>\n&emsp;&emsp;7.2 Go to <a href=#no7.2>Compiling the Model<\/a><br>\n&emsp;&emsp;7.3 Go to <a href=#no7.3>Training the Model<\/a><br>\n&emsp;&emsp;7.4 Go to <a href=#no7.4>Visualizing the Training Results<\/a><br>\n&emsp;&emsp;7.5 Go to <a href=#no7.5>Observations<\/a><br>\n8. Go to <a href=#no8>Checking for Class Imbalance<\/a><br>\n9. Go to <a href=#no9>Using Augmentor for Class Imbalance Treatment<\/a><br>\n&emsp;&emsp;9.1 Go to <a href=#no9.1>Installing Augmentor<\/a><br>\n&emsp;&emsp;9.2 Go to <a href=#no9.2>Using Augmentor<\/a><br>\n&emsp;&emsp;9.3 Go to <a href=#no9.3>Observations<\/a><br>\n10. Go to <a href=#no10>Modelling Data using Augmentor<\/a><br>\n&emsp;&emsp;10.1 Go to <a href=#no10.1>Creating the Train Data Set<\/a><br>\n&emsp;&emsp;10.2 Go to <a href=#no10.2>Creating the Validation Data Set<\/a><br>\n11. Go to <a href=#no11>Model 3<\/a><br>\n&emsp;&emsp;11.1 Go to <a href=#no11.1>Creating the Model<\/a><br>\n&emsp;&emsp;11.2 Go to <a href=#no11.2>Compiling the Model<\/a><br>\n&emsp;&emsp;11.3 Go to <a href=#no11.3>Training the Model<\/a><br>\n&emsp;&emsp;11.4 Go to <a href=#no11.4>Visualizing the Training Results<\/a><br>\n&emsp;&emsp;11.5 Go to <a href=#no11.5>Observations<\/a><br>","b1db139e":"-  The Augmentor has **helped decrease the imbalance** in class images and that can be viewed from above.","feac14de":"### <font color=blue>10.2 Creating the Validation Data Set <a name='no10.2' \/>","483db9a5":"### <font color=blue>11.2 Compiling the Model <a name='no11.2' \/>","5491cf72":"### <font color=blue>10.1 Creating the Train Data Set <a name='no10.1' \/>","f6a4bc66":"**Project Objective:** To build a CNN based model which can accurately detect melanoma. Melanoma is a type of cancer that can be deadly if not detected early. It accounts for 75% of skin cancer deaths. A solution which can evaluate images and alert the dermatologists about the presence of melanoma has the potential to reduce a lot of manual effort needed in diagnosis.","aa5bdb67":"### <font color=blue>4.1 Creating the model <a name='no4.1' \/>","b886fe09":"-  The samples of various classes are not in equal proportion.\n-  There is a significant **Class Imbalance** observed.\n-  The class with the least number of samples is `Seborrheic Keratosis` with **77**.\n-  The class that dominates the data in terms of proportionate number of samples is `Pigmented Benign Keratosis` with sample size of **462**. ","6025c74d":"### <font color=blue>2.2 Validation Data Set Creation <a name='no2.2' \/>","03b1e707":"So, now we have added 500 images to all the classes to maintain some class balance. We can add more images as we want to improve training process.","778866fe":"## <font color=blue>2. Data Preparation <a name='no2' \/>","175fd7d2":"## <font color=blue>11. Model 3 <a name='no11' \/>","ee02ba34":"## <font color=blue>8. Checking for Class Imbalance <a name='no8' \/>","0397e249":"### <font color=blue> 5.5 Observations <a name='no5.5' \/>","a7bd5ae7":"`Dataset.cache()` keeps the images in memory after they're loaded off disk during the first epoch.\n\n`Dataset.prefetch()` overlaps data preprocessing and model execution while training.","c1326dee":"### <font color=blue>11.5 Observations <a name='no11.5' \/>","c07a869a":"### <font color=blue>7.5  Observations <a name='no7.5' \/>","659106cb":"## <font color=blue>3. Visualizing the data <a name='no3' \/>","95b891d8":"### <font color=blue>9.2 Using Augmentor <a name='no9.2' \/>","c581d017":"### <font color=blue> 7.1 Creating the Model <a name='no7.1' \/>","6225b37e":"## <font color=blue> 5. Model 1 <a name='no5' \/>","806e5869":"### <font color=blue>7.3 Training the model <a name='no7.3' \/>","850f53cd":"-  The accuracy of the model for the **Training data set** is at **78%**. But the **Validation accuracy** is not in par with the training accuracy. \n-  It is only at **53%**. The validation loss as observed is very high.\n-  This could also be indicative of some **Overfit** in the model.\n-  We could add some `Dropout` layers and remove the `BatchNormalization` layers. \n-  And by adding a few more layers, we could improve the accuracy by trying to extract more features.","13c51ba6":"## <font color=blue> 6. Data Augmentation <a name='no6' \/>"}}