{"cell_type":{"a09d5fe0":"code","43cc3948":"code","0b67f9bd":"code","38019878":"code","8187f5a1":"code","aa505194":"code","a3a67176":"code","801a0f45":"code","e0ac88ca":"code","3e1a8bec":"code","0998178b":"code","499724c2":"code","95e6425b":"code","f3273240":"code","8bc5fd7a":"code","27d3f65d":"code","c8b5baf0":"code","52d8089d":"code","ce649edc":"code","c29e92bf":"code","a3da9cc3":"code","3e9b540b":"code","323534cb":"code","20531f7e":"code","9759234b":"code","dc45f991":"code","304c07c4":"code","4343afef":"code","752535b7":"code","d5f8e329":"code","fae7d4b2":"code","11691535":"code","7ff5165c":"code","c0667985":"code","e02267b4":"code","cb710208":"code","74cf3728":"code","68ad6e54":"code","671b2bf2":"code","9fa4d04f":"code","c412265f":"code","51087832":"code","953c43e3":"code","e442eab9":"code","9c5ea72a":"code","55a3cfc3":"code","a6aa6f4a":"code","256e3431":"code","797ee8ff":"code","35c8da93":"code","212266ff":"code","2f4ab5e7":"code","6231ddb0":"code","71fa0cce":"code","4f798476":"code","07e3198d":"code","3c097615":"code","60a849cc":"markdown","fee36523":"markdown","25777437":"markdown","753bc6e5":"markdown","8f36efca":"markdown","2fb41f2d":"markdown","31a4fe55":"markdown","34f45dae":"markdown"},"source":{"a09d5fe0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# to visualise al the columns in the dataframe\npd.pandas.set_option('display.max_columns', None)","43cc3948":"df_train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.head()","0b67f9bd":"## Always remember there will be a chance of data leakage so we need to split the data first and then apply feature\n## Engineering\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(df_train,df_train['SalePrice'],test_size=0.1,random_state=0)","38019878":"X_train.shape, X_test.shape","8187f5a1":"null_cat_features=[feature for feature in df_train.columns if df_train[feature].isnull().sum()>0 and df_train[feature].dtypes=='O']","aa505194":"## Let us capture all the nan values\n## First lets handle Categorical features which are missing\ndef missing_cat_values(data):   \n    null_cat_percentage = []\n    for feature in null_cat_features:\n        null_cat_percentage.append(np.round(data[feature].isnull().mean(),4)*100)\n        \n    data_cat_nan = pd.DataFrame({'Categorical_feature':null_cat_features,'% missing values':null_cat_percentage})\n    return data_cat_nan","a3a67176":"missing_cat_values(X_train)","801a0f45":"missing_cat_values(X_test)","e0ac88ca":"## Replace missing value with a new label\ndef replace_cat_feature(data):\n    data_tmp = data.copy()\n    data_tmp[null_cat_features] = data_tmp[null_cat_features].fillna('Missing')\n    return data_tmp\n\nX_train = replace_cat_feature(X_train)\nX_test  = replace_cat_feature(X_test)","3e1a8bec":"X_train[null_cat_features].isnull().sum()","0998178b":"print(X_test[null_cat_features].isnull().sum())","499724c2":"X_train.head()","95e6425b":"X_test.head()","f3273240":"# lets remove Id column as it not useful for our analysis\nX_train.drop('Id',axis=1,inplace=True)\nX_test.drop('Id',axis=1,inplace=True)","8bc5fd7a":"X_train.reset_index(drop=True,inplace=True)\nX_test.reset_index(drop=True,inplace=True)","27d3f65d":"X_train.head()","c8b5baf0":"X_test.head()","52d8089d":"null_num_features=[feature for feature in df_train.columns if df_train[feature].isnull().sum()>0 and df_train[feature].dtypes!='O']\n## Now lets see the numerical variables which have missing values\ndef missing_num_values(data): \n    ## We will print the numerical nan variables and percentage of missing values\n    null_num_percentage = []\n    for feature in null_num_features:\n        null_num_percentage.append(np.round(data[feature].isnull().mean(),4)*100)\n        \n    data_num_nan = pd.DataFrame({'Numerical_feature':null_num_features,'% missing values':null_num_percentage})\n    return data_num_nan","ce649edc":"missing_num_values(X_train)","c29e92bf":"missing_num_values(X_test)","a3da9cc3":"## Replacing the numerical Missing Values\ndef replace_num_nan(data):\n    for feature in null_num_features:\n        ## We will replace by using training data median since there are outliers\n        median_value=X_train[feature].median()\n        \n        ## create a new feature to capture nan values\n        data[feature+'_nan']=np.where(data[feature].isnull(),1,0)\n        data[feature].fillna(median_value,inplace=True)","3e9b540b":"replace_num_nan(X_train)\nreplace_num_nan(X_test)","323534cb":"missing_num_values(X_train)","20531f7e":"missing_num_values(X_test)","9759234b":"year_features = ['YearBuilt','YearRemodAdd','GarageYrBlt']\n\nX_train[year_features].head()","dc45f991":"X_test[year_features].head()","304c07c4":"## Temporal Variables (Date Time Variables)\ndef modify_year_val(data):\n    for feature in year_features:\n        data[feature]=data['YrSold']-data[feature]","4343afef":"modify_year_val(X_train)","752535b7":"modify_year_val(X_test)","d5f8e329":"X_train[year_features].head()","fae7d4b2":"X_test[year_features].head()","11691535":"num_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\nX_train[num_features].head()","7ff5165c":"X_test[num_features].head()","c0667985":"def log_num_transform(data,num_features):\n    for feature in num_features:\n        data[feature]=np.log(data[feature])","e02267b4":"log_num_transform(X_train,num_features)\nlog_num_transform(X_test,num_features)","cb710208":"X_train[num_features].head()","74cf3728":"X_test[num_features].head()","68ad6e54":"categorical_features=[feature for feature in df_train.columns if df_train[feature].dtype=='O']","671b2bf2":"categorical_features","9fa4d04f":"def modify_rare_cat(data,categorical_features):\n    for feature in categorical_features:\n        temp=X_train.groupby(feature)['SalePrice'].count()\/len(data)\n        temp_df=temp[temp>0.01].index\n        data[feature]=np.where(data[feature].isin(temp_df),data[feature],'Rare_var')","c412265f":"modify_rare_cat(X_train,categorical_features)\nmodify_rare_cat(X_test,categorical_features)","51087832":"def highlight_rare(s):    \n    is_rare = s == 'Rare_var'\n    return ['background-color: red' if v else '' for v in is_rare]","953c43e3":"X_train[categorical_features].style.apply(highlight_rare)","e442eab9":"X_test[categorical_features].style.apply(highlight_rare)","9c5ea72a":"def label_enc_cat(data):\n    for feature in categorical_features:\n        labels_ordered=data.groupby([feature])['SalePrice'].mean().sort_values().index\n        labels_ordered={k:i for i,k in enumerate(labels_ordered)}\n        data[feature]=data[feature].map(labels_ordered)","55a3cfc3":"label_enc_cat(X_train)\nlabel_enc_cat(X_test)","a6aa6f4a":"X_train.head(10)","256e3431":"X_test.head(10)","797ee8ff":"scaling_features=[feature for feature in df_train.columns if feature not in ['Id','SalePrice','YrSold'] ]\nlen(scaling_features)","35c8da93":"scaling_features","212266ff":"from sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\nscaler.fit(X_train[scaling_features])","2f4ab5e7":"X_train[scaling_features] = scaler.transform(X_train[scaling_features])","6231ddb0":"X_test[scaling_features] = scaler.transform(X_test[scaling_features])","71fa0cce":"X_train.head()","4f798476":"X_test.head()","07e3198d":"X_train.to_csv('X_train.csv',index=False)","3c097615":"X_test.to_csv('X_test.csv',index=False)","60a849cc":"## Advanced Housing Prices- Feature Engineering","fee36523":"## Numerical Variables\nSince the numerical variables are skewed we will perform log normal distribution","25777437":"We will be performing all the below steps in Feature Engineering\n\n1. Missing values\n2. Temporal variables\n3. Categorical variables: remove rare labels\n4. Standarise the values of the variables to the same range","753bc6e5":"#### Why we have to scale the train data and transform these into test data?\n\nCheck out the following link:\n\nhttps:\/\/sebastianraschka.com\/faq\/docs\/scale-training-test.html","8f36efca":"## Feature Scaling","2fb41f2d":"## Missing Values","31a4fe55":"## Handling Rare Categorical Feature\n\nWe will remove categorical variables that are present less than 1% of the observations","34f45dae":"The main aim of this project is to predict the house price based on various features which we will discuss as we go ahead\n\n#### Dataset to downloaded from the below link\nhttps:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data"}}