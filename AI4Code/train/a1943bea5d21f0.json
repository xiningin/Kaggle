{"cell_type":{"9b821db2":"code","c834c015":"code","57482c1b":"code","49c8494c":"code","e73cb575":"code","4d1e117a":"code","4ccb26cd":"code","839f8da3":"code","fbfffceb":"code","06f0aca9":"code","a17ac6e2":"code","87d58beb":"code","b11aa17d":"code","fe8d7434":"code","d0d2ca94":"code","74d5f678":"code","91c6919d":"code","c6164843":"code","1360e1bc":"code","bfdd8d49":"code","06cedfe7":"markdown","e606c875":"markdown","b2f0f570":"markdown","85c3ba34":"markdown","5e063a25":"markdown","eee81ae7":"markdown","76288847":"markdown","59760d01":"markdown","cd7da8a3":"markdown","ab01e1ac":"markdown","9d3ead11":"markdown","63e27d69":"markdown","ed5351af":"markdown","c21da2e1":"markdown","212af9f3":"markdown","c2869109":"markdown","fdb98486":"markdown","646ca020":"markdown","f390cba3":"markdown","3f7a75e3":"markdown","f550a5ff":"markdown","6d5448aa":"markdown"},"source":{"9b821db2":"import sys\n#zainstalowanie pytrends dla tego notebooka\n!{sys.executable} -m pip install pytrends \n#zainstalowanie yfinance dla tego notebooka\n!{sys.executable} -m pip install yfinance \n\nimport numpy as np\nimport pandas as pd\nfrom pytrends.request import TrendReq\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\nimport seaborn as sns\nfrom pytrends import dailydata\nimport yfinance as fin\nfrom datetime import datetime","c834c015":"pytrends = TrendReq(hl = 'PL')","57482c1b":"pytrends.build_payload(kw_list = [\"nasza klasa\", \"facebook\"], cat=0, timeframe='today 5-y', geo='PL', gprop='')\ndf1 = pytrends.interest_over_time()\ndf1.describe()","49c8494c":"df1.plot()","e73cb575":"#funkcja \u0142\u0105cz\u0105ca zapytania dla wielu hase\u0142\n#najpierw szukamy has\u0142a z najwi\u0119kszym pikiem\n#nast\u0119pnie \u0142\u0105czymy zapytania po 5 (chyba \u017ce zostanie reszta), gdzie wsp\u00f3lnym czynnikiem jest wcze\u015bniej znalezione has\u0142o\ndef better_compare(kw_list, cat, timeframe, geo, gprop):\n    \n    pytrends = TrendReq()\n    n = len(kw_list)   #oblicznie iteratora do p\u0119tli\n    if n > 5:\n        k = 5\n    else:\n        k = n\n\n    r = (n - 1) % 4 + 1  #reszta, kt\u00f3ra b\u0119dzie obs\u0142ugiwana poza g\u0142\u00f3wn\u0105 p\u0119tl\u0105 \n    \n    arr1 = [None] * k #lista u\u017cywana w p\u0119tli\n    arr2 = [None] * r #lista u\u017cywana dla reszty\n    arr1[0] = kw_list[0] #pierwszy element listy przechowuje najpopularniejsze dotychczas has\u0142o\n    \n    max_val = 0\n    \n    #szukanie najpopularniejszego has\u0142a w p\u0119tli  \n    for i in range(1,n - r,4): \n        \n        for j in range(1, k):\n            arr1[j] = kw_list[i + j - 1]\n    \n        pytrends.build_payload(kw_list = arr1, cat=cat, timeframe=timeframe, geo=geo, gprop = gprop)\n        df1 = pytrends.interest_over_time().drop('isPartial', axis=1)\n        arr1[0] = df1.max().idxmax() # nazwa kolumny z najwi\u0119ksz\u0105 warto\u015bci\u0105, czyli 100\n            \n    arr2[0] = arr1[0]\n            \n    #szukanie najpopularniejszego has\u0142a w reszcie \n    for j in range(1, r):\n        arr2[j] = kw_list[n - r + j]\n    \n    pytrends.build_payload(kw_list = arr2, cat=cat, timeframe=timeframe, geo=geo, gprop = gprop)\n    df1 = pytrends.interest_over_time().drop('isPartial', axis=1)\n    arr1[0] = df1.max().idxmax()   \n    arr2[0] = arr1[0]\n    \n    #df2 jest nasz\u0105 tablic\u0105 wynikow\u0105\n    #najpierw wsadzamy do niej nwyniki dla najpopularniejszego has\u0142a, a potem dok\u0142adamy kolejne\n    pytrends.build_payload(kw_list = [arr1[0]], cat=cat, timeframe=timeframe, geo=geo, gprop = gprop)\n    df2 = pytrends.interest_over_time().drop('isPartial', axis=1)\n    \n    #dok\u0142adanie wynik\u00f3w do df2 w p\u0119tli\n    for i in range(0, n - r ,4):\n        for j in range(1, k):\n            if kw_list[i + j - 1] == arr1[0]:\n                continue\n            arr1[j] = kw_list[i + j - 1]\n        \n        pytrends.build_payload(kw_list = arr1, cat=cat, timeframe=timeframe, geo=geo, gprop = gprop)\n        df1 = pytrends.interest_over_time().drop('isPartial', axis=1)\n        df2 = pd.concat([df2, df1.drop(arr1[0], axis=1)], axis=1)  \n        \n    #dok\u0142adanie wyni\u00f3w do df2 dla hase\u0142 z reszty\n    for j in range(1, r):\n        if kw_list[n - r + j] == arr2[0]:\n                continue\n        arr2[j] = kw_list[n - r + j]\n        \n    pytrends.build_payload(kw_list = arr2, cat=cat, timeframe=timeframe, geo=geo, gprop = gprop)\n    df1 = pytrends.interest_over_time().drop('isPartial', axis=1)\n        \n    df2 = pd.concat([df2, df1.drop(arr1[0], axis=1)], axis=1)   \n    \n    return df2","4d1e117a":"df2 = better_compare(kw_list = ['Arkadiusz Milik', 'Piotr Zieli\u0144ski', 'Krzysztof Pi\u0105tek', 'Grzegorz Krychowiak', 'Wojciech Szcz\u0119sny', 'Kamil Grosicki', '\u0141ukasz Piszczek', 'Jakub B\u0142aszczykowski', 'Kamil Glik'], cat=0, timeframe='today 5-y', geo='PL', gprop='')\ndf2.head()","4ccb26cd":"df2.plot(figsize = (30, 5))","839f8da3":"#funkcja \u0142\u0105cz\u0105ca indywidulane zapytania dla hase\u0142 z listy\n\ndef join_ind_payloads(kw_list, cat, timeframe, geo, gprop):\n    \n    pytrends = TrendReq(hl='PL')\n    df = pd.DataFrame(data={}) #tworzymy pust\u0105 ramk\u0119\n    \n    #umieszczamy dane dla ka\u017cdego kolejnego has\u0142a w ramce\n    for kw in kw_list:\n        pytrends.build_payload(kw_list = [kw], cat=cat, timeframe=timeframe, geo=geo, gprop = gprop)\n        df[kw] = pytrends.interest_over_time()[kw]\n\n    return df","fbfffceb":"join_ind_payloads(kw_list = ['biuro rachunkowe', 'prezent', 'si\u0142ownia'], cat=0, timeframe='2016-11-01 2020-02-01', geo='PL', gprop='').plot(figsize = (30, 5))","06f0aca9":"#funkcja \u0142\u0105cz\u0105ca zapytania dla r\u00f3\u017cnych region\u00f3w\n\ndef popularity_change_by_region(kw, cat, timeframe, geo_list, gprop):\n    # kw - has\u0142o, dla kt\u00f3rego wyszukjemy wyniki\n    # geo_list - lista region\u00f3w\n    # '' w geo_list to ca\u0142y \u015bwiat - musi by\u0107 na pocz\u0105tku listy (z niewyja\u015bnionych przyczyn)\n\n    pytrends = TrendReq(hl = 'PL')\n    df = pd.DataFrame(data={})\n    \n    # Umieszczamy zapytania dla kolejnych region\u00f3w w ramce\n    for geo in geo_list:\n        pytrends.build_payload(kw_list = [kw], cat=cat, timeframe=timeframe, geo=geo, gprop = gprop)\n        if geo == '':\n            df['Worldwide'] = pytrends.interest_over_time()[kw]\n        else:\n            df[geo] = pytrends.interest_over_time()[kw]\n\n    return df","a17ac6e2":"popularity_change_by_region(kw = \"fidget spinner\", cat=0, timeframe=\"2017-01-01 2017-10-01\", geo_list=['', 'PL', 'US'], gprop='').plot(figsize = (30, 5))","87d58beb":"popularity_change_by_region(kw = \"Golf 4\", cat=0, timeframe=\"2004-01-01 2020-01-01\", geo_list=['PL', 'DE'], gprop='').plot(figsize = (30, 5))","b11aa17d":"kw_misery = [\"\/m\/02bft\",\"\/m\/02y_3dj\",\"\/m\/0k_9\", \"\/m\/012lyw\"] #kody temat\u00f3w\n\ndf = join_ind_payloads(kw_list = kw_misery, cat=0, timeframe='2019-01-01 2020-01-01', geo='PL', gprop='') # pobranie danych\ndf['Misery'] = df.mean(axis=1) #wska\u017anik nieszcz\u0119\u015bcia jako \u015brednia z poszczeg\u00f3lnych temat\u00f3w\ndf.Misery.plot(title =\"Poziom nieszcz\u0119\u015bcia w Polsce na przestrzeni roku\", legend=False).set_xlabel(\"Data\") # wykres","fe8d7434":"keyword = kw_misery[1] #temat \"Smutek\"\nsadness = pytrends.get_historical_interest([keyword], year_start=2019, month_start=1, day_start=1, hour_start=0, year_end=2020, month_end=1, day_end=1, hour_end=0, cat=0, geo='PL', gprop='', sleep=0).drop('isPartial', axis=1, errors='ignore')","d0d2ca94":"days_of_week = {\n    0: 'Poniedzia\u0142ek',\n    1 : 'Wtorek',\n    2 : '\u015aroda',\n    3 : 'Czwartek',\n    4 : 'Pi\u0105tek',\n    5 : 'Sobota',\n    6 : 'Niedziela'}\n\nsadness.groupby(sadness.index.dayofweek).mean().rename(columns={keyword: 'Smutek'}, index= days_of_week).plot(kind='bar',title =\"Popularno\u015b\u0107 tematu \\\"Smutek\\\" w zale\u017cno\u015bci od dnia tygodnia\", legend=False).set_xlabel(\"Dzie\u0144 tygodnia\")","74d5f678":"sadness.groupby(sadness.index.hour).mean().plot(kind='bar', title =\"Popularno\u015b\u0107 tematu \\\"Smutek\\\" w zale\u017cno\u015bci od godziny\", legend=False).set_xlabel(\"Godzina\")","91c6919d":"# tworzenie ramki danych do mapy ciep\u0142a\ndf7 = pd.DataFrame(index = range(0, 24))\nj = 0\nfor d in list(days_of_week.values()): # dla ka\u017cdego dnia\n    df7[d] = \"\" # dodanie piustej kolumny dla danego dnia\n    for i in df7.index: # dla ka\u017cdej godziny\n        df7.loc[i, d] = sadness[(sadness.index.hour == i) & (sadness.index.dayofweek == j)].mean()[0] # dodaj \u015bredni\u0105 z wynik\u00f3w dla danego dnia tyg. i godziny\n    df7[d] = pd.to_numeric(df7[d])\n    j += 1","c6164843":"# tworzenie mapy ciep\u0142a\nfig, ax = plt.subplots(figsize=(10,10))\nax.set_title(\"Popularno\u015bci tematu \\\"Smutek\\\" dla poszczeg\u00f3lnych godzin dni tygodnia\")\nax.set_ylabel(\"Godzina\")\nsns.heatmap(df7, ax = ax)","1360e1bc":"corona = pd.read_csv('..\/input\/corona.csv') # pobieramy dane, zawieraj\u0105ce wszytskie dane\ncoronaPL = corona[corona.geoId == \"PL\"] # wybieramy tylko dane dla Polski\n\n# dopasowujemy format daty danych koronawriusa, aby zgadza\u0142 si\u0119 z danymi z Google Trends\ncoronaPL = pd.concat([coronaPL.apply(lambda row: dt.datetime(row.year, row.month, row.day), axis=1), coronaPL[\"cases\"]], axis = 1)\ncoronaPL.set_index(0, inplace = True)\ncoronaPL.index.names = [\"date\"]\n\n# pobieramy dane z Google Tredns i \u0142\u0105czymy z danymi koronawirusa\npytrends.build_payload(kw_list = [\"Koronawirus\"], cat=0, timeframe=\"2020-02-01 2020-06-03\", geo='PL', gprop = '')\ndf = pytrends.interest_over_time().drop('isPartial', axis=1)\ndf = df.join(coronaPL).fillna(0)\n\n# normalizacja min-max liczby przypadk\u00f3w\nminc = df['cases'].min()\nmaxc = df['cases'].max()\ndf['cases'] = df['cases'].map(lambda x: 100*(x - minc)\/(maxc- minc))\n\n# tworzenie wykresu\nax = df.plot(figsize = (20, 5), x_compat = True)\nL=plt.legend()\nL.get_texts()[0].set_text('Popularno\u015b\u0107 zapytania \"Koronawirus\"')\nL.get_texts()[1].set_text('Znormalizowana liczba zachorowa\u0144')\nplt.show()","bfdd8d49":"# Kod zosta\u0142 lekko zmieniony wzgl\u0119dem orygina\u0142u (g\u0142\u00f3wnie zmiana podpis\u00f3w osi i formatu daty na polskie)\n\n# PARAMETRY\nticker = 'BTC-USD' # jakiej akcji szukamy\ndate_range = [1,1,2019,1,1,2020] # okres; format: dzie\u0144, miesi\u0105c, rok\n\nfin_indx = 0 # pokazujemy cen\u0119 otwarcia na wykresie\ntrend_indx = 3 #dane z GT s\u0105 wyskalowane na wykresie wzgl\u0119dem ceny\n\n\n# STWORZENIE ZMIENNYCH DLA OKRESU\nstart_t = dt.datetime(date_range[2],date_range[1],date_range[0])\nend_t   = dt.datetime(date_range[5],date_range[4],date_range[3])\n\n# POBRANIE DANYCH \nticker_handle = fin.Ticker(ticker)\ntrends = dailydata.get_daily_data(ticker_handle.info.get('shortName').split(' ')[0],\n                                  date_range[2],date_range[1],\n                                  date_range[5],date_range[4])\n\nfin_data = fin.download(ticker,start=start_t.strftime('%Y-%m-%d'),\n                        end=end_t.strftime('%Y-%m-%d'))\n\n# DOPASOWYWANIE DANYCH\nfin_keys = fin_data.keys() # keys for naming plotted finance data\ntrend_keys = trends.keys() # keys for naming plotted trends data\n\nfin_x   = [ii.timestamp() for ii in fin_data.index] # formatting dates into timestamp for plotting\nfin_y   = (fin_data.values)[:,fin_indx] # trend data to plot\n\ntrend_x = [ii.timestamp() for ii in trends.index] # formatting dates into timestamp for plotting\ntrend_y = (trends.values)[:,trend_indx] # trend data to plot\n\ntrend_start_indx = np.argmin(np.abs(np.subtract(trend_x,fin_x[0])))\ntrend_end_indx   = np.argmin(np.abs(np.subtract(trend_x,fin_x[-1])))\ntrend_y = [trend_y[np.argmin(np.abs(np.subtract(ii,trend_x)))] for ii in fin_x] # align trends + stock $\ntrend_x = [trend_x[np.argmin(np.abs(np.subtract(ii,trend_x)))] for ii in fin_x] # align trends + stock $\n\n\n# OBLICZANIE KORELACJI\ncorr_xy_array = [0.0]\nfor ii in range(1,len(fin_y)):\n    mean_x = np.nanmean(trend_y[0:ii])\n    mean_y = np.nanmean(fin_y[0:ii])\n    sigma_x = np.sqrt(np.nansum(np.power(trend_y[0:ii]-mean_x,2.0)))\n    sigma_y = np.sqrt(np.nansum(np.power(fin_y[0:ii]-mean_y,2.0)))\n    corr_xy = (np.nansum(np.multiply((np.subtract(trend_y[0:ii],mean_x)),\n                                     np.subtract(fin_y[0:ii],mean_y))))\\\n              \/(sigma_x*sigma_y)\n    if np.isnan(corr_xy):\n        corr_xy = 0.0\n    corr_xy_array.append(corr_xy)\n\n# TWORZENIE WYKRES\u00d3W\nplt.style.use('ggplot')\nfig,axs = plt.subplots(2,1,figsize=(14,9),sharex=True)\n\nax = axs[0]\nax.scatter(trend_x,trend_y,color=plt.cm.tab20(0))\nax2 = ax.twinx()\nax2.grid(False)\nax2.scatter(fin_x,fin_y,color=plt.cm.tab20(2))\n\nax.set_ylabel('Trend',color=plt.cm.tab20(0),fontsize=20)\nax2.set_ylabel('Cena otwarcia [$ USD]',color=plt.cm.tab20(2),fontsize=20)\n\nax3 = axs[1]\nscat3 = ax3.scatter(fin_x,corr_xy_array,color=plt.cm.tab20(4))\nax3.set_ylabel('Wsp\u00f3\u0142czynnik korelacji',\n              fontsize=20,color=plt.cm.tab20(4))\nx_ticks = ax3.get_xticks()\nx_str_labels = [(dt.datetime.fromtimestamp(ii)).strftime('%d-%m-%Y') for ii in x_ticks]\nax3.set_xticklabels(x_str_labels) \nax3.set_xlabel('Data [dzie\u0144-miesi\u0105c-rok]',fontsize=20)\nax2.set_xticklabels(x_str_labels)\n\nax.set_title(ticker+' ({}) '.format(ticker_handle.info.get('shortName'))+' od {} do {}'.format(start_t.strftime('%d\/%m\/%Y'),\n                                        end_t.strftime('%d\/%m\/%Y')),\n          fontsize=20)\n\nplt.show()","06cedfe7":"Widzimy, \u017ce has\u0142a zwi\u0105zane ze smutkiem s\u0105 najcz\u0119\u015bciej wyszukiwane w okolicach p\u00f3\u0142nocy i najrzadziej w ci\u0105gu dnia, zw\u0142aszcza rano.\n\nMo\u017cemy po\u0142\u0105czy\u0107 dwa powy\u017csze wykresy na mapie ciep\u0142a, aby uzyska\u0107 jeszcze dok\u0142adniejsz\u0105 analiz\u0119.","e606c875":"Jak wida\u0107 z wykresu, trend w postaci fidget spiner'\u00f3w by\u0142 w USA najsilniejszy pod koniec kwietnia i dopiero wtedy w Polsce zacz\u0119\u0142y one zyskiwa\u0107 popularno\u015b\u0107. Pik ich popularno\u015bci na \u015bwiecie nast\u0105pi\u0142 oko\u0142o 2 tygodnie p\u00f3\u017aniej, w Polsce natomiast miesi\u0105c p\u00f3\u017aniej ni\u017c w USA.\n\nMo\u017cemy r\u00f3wnie\u017c sprawdzi\u0107 jak kszta\u0142tuje si\u0119 popularno\u015b\u0107 poszczeg\u00f3lnych samochod\u00f3w w danych krajach. W Polsce popularne jest sprowadzanie u\u017cywanych samochod\u00f3w z Niemiec. Powinno by\u0107 zatem mo\u017cliwe zobaczenie zale\u017cno\u015bci w popularno\u015bci pewnych starszych modeli aut pomi\u0119dzy dwoma krajami. Pik popularno\u015bci w Polsce powinien by\u0107 przesuni\u0119ty w czasie wzgl\u0119dem Niemiec.\nZobaczmy wykres dla popularnego w Polsce Volkswagena Golf 4.","b2f0f570":"### Badanie popularno\u015bci koronawirusa wzgl\u0119dem liczby zachorowa\u0144\n\u0179r\u00f3d\u0142o danych: https:\/\/data.europa.eu\/euodp\/pl\/data\/dataset\/covid-19-coronavirus-data\n\nKorzystaj\u0105c z danych dotycz\u0105cych koronawirusa, mo\u017cemy zwizualizowa\u0107 jak kszta\u0142towa\u0142a si\u0119 jego popularno\u015b\u0107 w internecie wzgl\u0119dem liczby zaka\u017ce\u0144. Liczba zaka\u017ce\u0144 jest znormalizowana w ten sam spos\u00f3b, co dane popularno\u015bci z Google Trends (0-100 wzgl\u0119dem najwy\u017cszego wyniku), aby wykres by\u0142 bardziej czytelny.","85c3ba34":"Jak wida\u0107, dane dla wyszukiwa\u0144 \"nasza klasa\" przyjmuj\u0105 warto\u015bci 0\/1\/2 dla okresu 5 lat i s\u0105 w\u0142a\u015bciwie bezu\u017cyteczne.\nAby otrzyma\u0107 sensowne dane dla tego has\u0142a, musimy stworzy\u0107 dla niego osobne zapytanie. Wtedy jednak, nie jeste\u015bmy w stanie bezpo\u015brednio por\u00f3wna\u0107 popularno\u015bci dw\u00f3ch hase\u0142. \n\n#### Dok\u0142adno\u015b\u0107 pomiaru\nKolejne ograniczenie dotyczy faktu, \u017ce dok\u0142adno\u015b\u0107 pomiaru jest zale\u017cna od przedzia\u0142u czasu jaki wybierzemy. Nie mo\u017cemy jej r\u0119cznie ustawi\u0107. Przyk\u0142adowo, je\u017celi chcemy otrzyma\u0107 dane z dok\u0142adno\u015bci\u0105 do godzin, maksymalny zakres czasu jaki mo\u017cemy wybra\u0107 to tydzie\u0144. Poniewa\u017c dane s\u0105 normalizowane wzgl\u0119dem najwy\u017cszego wyniku w danym przedziale czasu, nie mo\u017cemy zwyczajnie sklei\u0107 kilku zapyta\u0144 - musimy na nowo normalizowa\u0107 dane. \n\nArtyku\u0142 wyja\u015bniaj\u0105cy w jaki spos\u00f3b \u0142\u0105czy\u0107 zapytania i normalizowa\u0107 dane, aby otrzyma\u0107 dok\u0142adniejsze dane dla wi\u0119kszych przedzia\u0142\u00f3w czasu: https:\/\/medium.com\/@bewerunge.franz\/google-trends-how-to-acquire-daily-data-for-broad-time-frames-b6c6dfe200e6\n\n### Tworzenie w\u0142asnych funkcji na bazie pytrends\n\nU\u017cywaj\u0105c Python'a, mo\u017cemy obej\u015b\u0107 niekt\u00f3re ograniczenia Google Trends.\n\nPoni\u017csza funkcja pozwala przeprowadzi\u0107 zapytanie dla dowolnej liczby hase\u0142. Dzia\u0142anie funkcji na zlepianiu wielu zapyta\u0144 w jedno, gdzie wsp\u00f3lnym czynnikiem jest has\u0142o z najwi\u0119ksz\u0105 maksymaln\u0105 popularno\u015bci\u0105 spo\u015br\u00f3d wszystkich.","5e063a25":"#### Znajdowanie przeciek\u00f3w maturlanych\n\u0179r\u00f3d\u0142o: https:\/\/twitter.com\/emocjewsieci\/status\/1269935336701997056","eee81ae7":"Z wykresu mo\u017cemy wywnioskowa\u0107, \u017ce:\n- Najcz\u0119\u015bciej wyszukujemy prezenty w okresie \u015bwi\u0105t Bo\u017cego Narodzenia, ale tak\u017ce na walentynki, w okresie komunii \u015bwi\u0119tych oraz na dzie\u0144 dziecka.\n- Popularno\u015b\u0107 biur rachunkowy ro\u015bnie wraz ze zbli\u017caniem si\u0119 terminu zeznania podatkowego.\n- Najwi\u0119ksz\u0105 popularno\u015b\u0107 si\u0142ownie maj\u0105 wraz z pocz\u0105tkiem roku, kiedy ludzie robi\u0105 postanowienia noworoczne.\n\nNie b\u0119dziemy si\u0119 jednak skupia\u0107 na podstawowych funkcjonalno\u015bciach serwisu. Korzystaj\u0105c z danych Google Trends mo\u017cemy bowiem przeprowadzi\u0107 bardziej konkretn\u0105 i zaawansowan\u0105 analiz\u0119. Poni\u017cej zostan\u0105 om\u00f3wione niekt\u00f3re sposoby ciekawego wykorzystania danych udost\u0119pnianych przez serwis.\n\n### Badanie trend\u00f3w wzgl\u0119dem regionu\nGoogle Trends pozwala por\u00f3wna\u0107 popularno\u015b\u0107 hase\u0142 w konkretnych regionach, jednak nie pozwala zbada\u0107 jak zmienia\u0142a si\u0119 popularno\u015b\u0107 danego has\u0142a w czasie dla danych region\u00f3w. Poni\u017cej znajduje si\u0119 funckja, kt\u00f3ra nam na to pozwoli.","76288847":"### Wnioski ko\u0144cowe\nDane z Google Trends mo\u017cemy wykorzysta\u0107 na multum sposob\u00f3w. Obserwowanie przemian spo\u0142ecznych, analiza r\u00f3\u017cnic kulturowych czy badanie kszta\u0142towania si\u0119 trend\u00f3w to tylko niekt\u00f3re z wielu zastosowa\u0144. Has\u0142a, kt\u00f3re wyszukujemy, w pewnym stopniu nakre\u015blaj\u0105 obraz ca\u0142ego spo\u0142ecze\u0144stwa - jakie s\u0105 nasze preferencj\u0119, jak si\u0119 zmieniaj\u0105 w czasie, jak si\u0119 kszta\u0142tuj\u0105 wzgl\u0119dem innych kraj\u00f3w. Analiza danych z Google Trends pozwala zauwa\u017cy\u0107 wiele zale\u017cno\u015bci w spo\u0142ecze\u0144stwie, kt\u00f3rych nie spos\u00f3b dostrzec na codzie\u0144.","59760d01":"Przyk\u0142ad dzia\u0142ania funkcji:","cd7da8a3":"Z wykresu mo\u017cemy zauwa\u017cy\u0107, \u017ce w Niemczech od paru lat odchodzi si\u0119 od Golfa 4, za\u015b w Polsce jego popularno\u015b\u0107 stale ros\u0142a a\u017c do roku 2019. Pik w Polsce jest przesuni\u0119ty o oko\u0142o 6 lat.","ab01e1ac":"Funkcja b\u0119dzie dawa\u0142a poprawne wyniki jedynie dla hase\u0142 uniwersalnych w r\u00f3\u017cnych j\u0119zykach (chyba, \u017ce wyszukujemy dla podregion\u00f3w danego kraju) lub dla temat\u00f3w.\n\nPrzy pomocy tej funkcji mo\u017cemy na przyk\u0142ad zauwa\u017cy\u0107 z jakim op\u00f3\u017anieniem kszta\u0142tuj\u0105 si\u0119 trendy z zachodu na \u015bwiecie i w naszym kraju.","9d3ead11":"Na wykresie wida\u0107 piki popularno\u015bci koronawirusa powi\u0105zane z konkretnymi wydarzeniami:\n- 27.02 - Doniesienia o pierwszym przypadku w \u0142odzi: https:\/\/lodz.wyborcza.pl\/lodz\/7,35136,25736561,koronowirus-w-polsce-zarazona-25-latka-w-lodzi.html \n- 04.03 - Pierwszy potwierdzony przypadek zaka\u017cenia koronawirusem (2019-nCoV) w Polsce\n- 12.03 - Pierwszy zgon\n- 31.03 - Wprowadzenie nowych ogranicze\u0144 (m.in. zamkni\u0119cie park\u00f3w): \nhttps:\/\/www.gov.pl\/web\/koronawirus\/kolejne-kroki\n\n\nZ wykresu mo\u017cemy zauwa\u017cy\u0107, \u017ce zainteresowanie Polacy znudzili si\u0119 koronawirusem, pomimo \u017ce liczba nowych przypadk\u00f3w wcale nie spada i zagro\u017cenie wci\u0105\u017c jest realne. Polacy najbardziej byli zainteresowani wirusem na samym pocz\u0105tku, kiedy potwierdzono w Polsce pierwsz\u0105 ofiar\u0119 \u015bmierteln\u0105.\n\n\n### Badanie popularno\u015bci akcji gie\u0142dowych wzgl\u0119dem ich ceny\n\u0179r\u00f3d\u0142o: https:\/\/makersportal.com\/blog\/2020\/1\/19\/google-trends-x-yahoo-finance\n\nDane z Google Trends mo\u017cemy wykorzysta\u0107 tak\u017ce do analizy finansowej. Zamieszczony wy\u017cej artyku\u0142 pokazuje w jaki spos\u00f3b mo\u017cemy zestawi\u0107 popularno\u015b\u0107 danych akcji w wyszukiwarce Google wzgl\u0119dem ich ceny. Autor wykorzystuje w tym celu pytrends oraz API yfinance, kt\u00f3re pobiera dane gie\u0142dowe z Yahoo Stocks.\n\nPoni\u017cej wykorzysta\u0142em kod z artyku\u0142u, aby przedstawi\u0107 zale\u017cno\u015b\u0107 cen akcji Bitcoin'a od jego popularno\u015bci w internecie. Przedstawiony jest r\u00f3wnie\u017c druki wykres obrazuj\u0105cy korelacj\u0119 pomi\u0119dzy tymi dwoma warto\u015bciami w czasie.\n\n","63e27d69":"### Inne zastosowania danych Google Trends\n\n#### \"iPhone is slow\" wzgl\u0119dem nowych IOS\n\u0179r\u00f3d\u0142o: https:\/\/goodbrothers.digital\/ideation\/google-trends-the-story-goldmine-youre-probably-not-using-enough\/","ed5351af":"# Analiza danych z Google Trends\n\nProjekt indywidualny, kt\u00f3ry zrobi\u0142em w ramach studi\u00f3w. Udost\u0119pni\u0142em na swoim profilu rownie\u017c pierwsz\u0105 cz\u0119\u015b\u0107, dotycz\u0105c\u0105 korzystania z pytrends.","c21da2e1":"### Google Globe Trends\nNa bazie projektu: https:\/\/github.com\/chrisrzhou\/google-globe-trends\nmo\u017cemy stworzy\u0107 w\u0142asn\u0105 interaktywn\u0105 wizualizacj\u0119 danych z Google Trends na kuli ziemskiej, na kszta\u0142t tej:\nhttps:\/\/google-globe-trends.netlify.app\/. Miasta na  \u015bwiecie z najwi\u0119ksz\u0105 liczb\u0105 wyszukiwa\u0144 danego has\u0142a s\u0105 zaznaczone \u015bwiec\u0105cym punktem. Popularno\u015b\u0107 jest liczona dla okresu od 2004 do dzi\u015b.\n\nAby skonfigurowa\u0107 w\u0142asn\u0105 wizualizacj\u0119 nale\u017cy pod\u0105\u017ca\u0107 za instrukcjami opisanymi w projekcie.\nW skr\u00f3cie nale\u017cy:\n1. Sklonowa\u0107 oryginalne repozytorium i umie\u015bci\u0107 je na Github'ie\n2. Zmieni\u0107 warto\u015b\u0107 data.keyword w pliku src\/config.js na po\u017c\u0105dane has\u0142o\n3. Po\u0142\u0105czy\u0107 aplikacj\u0119 Netlify z naszym repozytorium (*Deploy to netlify*)\n4. Wdro\u017cy\u0107 nasz\u0105 wizualizacj\u0119 w Netlify\nW wyniku mamy gotow\u0105 aplikacj\u0119 webow\u0105.\n\nPrzyk\u0142adowa wizualizacja, kt\u00f3r\u0105 stworzy\u0142em dla has\u0142a \"Iphone\":\nhttps:\/\/dazzling-hodgkin-f2e15a.netlify.app\/","212af9f3":"### Ograniczenia\n\n#### Normalizacja\nNa pocz\u0105tku warto zazanczy\u0107, \u017ce dane z serwisu s\u0105 normalizowane:\n\n*Liczby reprezentuj\u0105 poszczeg\u00f3lne zainteresowania w wyszukiwaniu wzgl\u0119dem najwy\u017cszego punktu na wykresie. Warto\u015b\u0107 100 oznacza najwy\u017csz\u0105 popularno\u015b\u0107 has\u0142a. Warto\u015b\u0107 50 oznacza, \u017ce popularno\u015b\u0107 has\u0142a by\u0142a dwukrotnie mniejsza. Warto\u015b\u0107 0 wskazuje, \u017ce dla danego has\u0142a nie ma wystarczaj\u0105cych danych.* - Google Trends\n\nTo znacznie ogranicza mo\u017cliwo\u015bci analizy tych danych. Oznacza to, \u017ce popularno\u015b\u0107 wyszukiwania poszczeg\u00f3lnych hase\u0142 mo\u017cemy por\u00f3wnywa\u0107 jedynie w ramach wsp\u00f3lnych zapyta\u0144, kt\u00f3re z kolei ograniczone s\u0105 do pi\u0119ciu hase\u0142.\n\nNormalizacja warto\u015bciami ca\u0142kowitymi z zakresu 0-100 wp\u0142ywa r\u00f3wnie\u017c negatywnie na dok\u0142adno\u015b\u0107 danych i mo\u017cliwo\u015b\u0107 ich analizy. Je\u017celi chcemy por\u00f3wna\u0107 dwa has\u0142a znacznie r\u00f3\u017cni\u0105ce sie pod wzgl\u0119dem popularno\u015bci, wyniki dla mniej popularnego has\u0142a b\u0119d\u0105 w jeszcze mocniej ograniczonym zakresie i nie b\u0119d\u0105 w \u017caden spos\u00f3b u\u017cyteczne.\n\nPrzyk\u0142adowo, powiedzmy \u017ce chcemy por\u00f3wna\u0107 jak zmienia\u0142a si\u0119 popularno\u015b\u0107 serwisu NaszaKlasa wzgl\u0119dem Facebook'a w ostatnich pi\u0119ciu latach.","c2869109":"Z mapy ciep\u0142a mo\u017cemy wyczyta\u0107, \u017ce Polacy najcz\u0119\u015bciej wyszukuj\u0105 has\u0142a zwi\u0105zne ze smutkiem w okolicach p\u00f3\u0142nocy w \u015brodku tygodnia - szczeg\u00f3lnie cz\u0119sto w niedzielny wiecz\u00f3r. Z kolei najmniej zapyta\u0144 jest w pi\u0105tek i sobot\u0119, w ci\u0105gu dnia.","fdb98486":"Z wykresu kolumnowego dla poszczeg\u00f3lnych dni tygodnia wida\u0107, \u017ce has\u0142a zwi\u0105zane ze smutkiem s\u0105 najmniej popularne w weekend. Najwi\u0119ksza popularno\u015b\u0107 przypada na wtorek. \n\nAnaliza dla godzin:","646ca020":"Kolejn\u0105 funkcj\u0105, kt\u00f3r\u0105 mo\u017cemy doda\u0107 jest funkcja zlepiaj\u0105ca dane z indywidualnych zapyta\u0144 dla poszeczeg\u00f3lnych hase\u0142. Oznacza to, \u017ce dane dla ka\u017cdego has\u0142a s\u0105 znormalizowane wzgl\u0119dem najwi\u0119kszego wyniku tego has\u0142a. O ile takie dane nie pozwalaj\u0105 na por\u00f3wnywanie popularno\u015b\u0107i hase\u0142, to mo\u017cemy \u0142atwo por\u00f3wna\u0107 zale\u017cno\u015bci pomi\u0119dzy poszczeg\u00f3lnymi has\u0142ami. Mniej popularne has\u0142a nie s\u0105 bowiem sp\u0142aszone pod wp\u0142ywem tych znacznie bardziej popularnych.","f390cba3":"Z wykresy widzimy, \u017ce wska\u017anik nieszcz\u0119\u015bcia na pocz\u0105tku roku jest wysoko. Wraz ze zbli\u017caniem si\u0119 lata stopniowo maleje. W maju jest nag\u0142y wzrost - prawdopodobnie ze wzgl\u0119du na matury. W lecie wska\u017anik jest najni\u017cszy. Wraz z ko\u0144cem lata mocno idzie w g\u00f3r\u0119 i osi\u0105ga pik w po\u0142owie pa\u017adziernika. Wraz ze zbli\u017caniem si\u0119 \u015bwi\u0105t mocno spada, jednak po \u015bwi\u0119tach znowu ro\u015bnie.\n\nPoni\u017cej przeprowadzono analiz\u0119 dla poszczeg\u00f3lnych dni tygodnia oraz godzin. W tym celu wykorzytsano funkcj\u0119 get_historical_interest(), kt\u00f3ra zwraca wyniki z dok\u0142adno\u015bci\u0105 do godziny.\nZe wzgl\u0119du na bardzo du\u017c\u0105 ilo\u015b\u0107 danych, badamy jednak tylko temat \"Smutek\".\n\n**UWAGA ODNO\u015aNIE PONI\u017bSZYCH WYKRES\u00d3W:** Funkcja get_historical_interest() skleja wyniki z zapyta\u0144 tygodniowych. Popularno\u015b\u0107 dla poszczeg\u00f3lnych godzin jest normalizowana wzgl\u0119dem najwy\u017cszego wyniku z danego tygodnia. Wynik popularno\u015bci na poni\u017cszych wykresach to **\u015brednia z warto\u015bci popularno\u015bci dla danego dnia\/godziny\/obu wzgl\u0119dem najwy\u017cszego wyniku w tygodniu**. ","3f7a75e3":"### Badanie poziomu smutku\n\u0179r\u00f3d\u0142o: https:\/\/www.washingtonpost.com\/news\/wonk\/wp\/2014\/12\/03\/the-google-misery-index-the-times-of-year-were-most-depressed-anxious-and-stressed\/?arc404=true\n\nZak\u0142adaj\u0105c, \u017ce gdy ludzi s\u0105 smutni, to cz\u0119\u015bciej wyszukuj\u0105 w wyszukiwarce has\u0142\u0105 zwi\u0105zane ze smutkiem, mo\u017cemy u\u017cy\u0107 Google Trends aby zbada\u0107 jak zmienia si\u0119 poziom szcz\u0119\u015bcia spo\u0142ecze\u0144stwa na przestrzeni roku.\n\nNa potrzeby poni\u017cszego wykresu stworzyli\u015bmy wska\u017anik nieszcz\u0119\u015bcia, na kt\u00f3ry sk\u0142adaj\u0105 si\u0119 4 tematy: Zaburzenia depresyjne, Smutek, L\u0119k oraz Stres.\n\nWykres smutku w Polsce 2019:\n","f550a5ff":"Dane z Google Trends pozwalaj\u0105, w podstawowym zakresie:\n- zobaczy\u0107 zmiany popularno\u015bci hase\u0142 w czasie\n- por\u00f3wna\u0107 zmiany popularno\u015bci kilku hase\u0142\n- por\u00f3wna\u0107 poziom popularno\u015bci kilku hase\u0142\n- por\u00f3wna\u0107 popularno\u015b\u0107 hase\u0142 w konkretnych regionach\n\nPrzyk\u0142adowo, mo\u017cemy u\u017cy\u0107 tych danych do zbadania popularno\u015bci rzeczy na przestrzeni roku.","6d5448aa":"Widzimy, \u017ce przez wi\u0119kszo\u015b\u0107 roku utrzymuje si\u0119 wyra\u017ana dodatnia korelacja. Przez trzy miesi\u0105ce wsp\u00f3\u0142czynnik korelacji wynosi\u0142 ponad 0.8. Oznacza to, \u017ce cena Bitcoin'a jest mocno powi\u0105zana z jego popularno\u015bci\u0105."}}