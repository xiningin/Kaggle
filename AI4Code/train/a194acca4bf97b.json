{"cell_type":{"bbd87a75":"code","9a531733":"code","e4ed7c29":"code","92bd3401":"code","f43e7fc3":"code","a20f13a5":"code","14d87fd1":"code","d9cc86b4":"code","0ef71b1d":"code","5db0b6ee":"code","da5b49c7":"code","eade6959":"code","df7053c2":"code","50abc921":"code","1a35ed18":"code","0c109a5f":"code","a4d943b3":"code","75223495":"code","ea55fcd8":"code","5335ea5a":"code","7907f478":"code","f93594e5":"code","6fbfe14e":"code","3a900851":"code","5556afab":"code","8ffb9194":"markdown","b93c341b":"markdown","535bd9b7":"markdown","163a59ce":"markdown","94a295a6":"markdown","45d2d69d":"markdown","43f5709e":"markdown"},"source":{"bbd87a75":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport cv2","9a531733":"# read the dataset\ndigits = pd.read_csv(\"..\/input\/train.csv\")\ndigits.info()","e4ed7c29":"# head\ndigits.head()","92bd3401":"four = digits.iloc[3, 1:]\nfour.shape","f43e7fc3":"four = four.values.reshape(28, 28)\nplt.imshow(four, cmap='gray')","a20f13a5":"# Summarise the counts of 'label' to see how many labels of each digit are present\ndigits.label.astype('category').value_counts()","14d87fd1":"# Summarise count in terms of percentage \n100*(round(digits.label.astype('category').value_counts()\/len(digits.index), 4))","d9cc86b4":"# missing values - there are none\ndigits.isnull().sum()","0ef71b1d":"# Creating training and test sets\n# Splitting the data into train and test\nX = digits.iloc[:, 1:]\nY = digits.iloc[:, 0]\n\n# Rescaling the features\nfrom sklearn.preprocessing import scale\nX = scale(X)\n\n# train test split with train_size=15% and test size=85%\n# This is to minimise the processing time and later we will use the actual test data to do prediction\n#Later we will use the \nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.15, random_state=101)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n","5db0b6ee":"from sklearn import svm\nfrom sklearn import metrics\n\n# an initial SVM model with linear kernel   \nsvm_linear = svm.SVC(kernel='linear')\n\n# fit\nsvm_linear.fit(x_train, y_train)","da5b49c7":"# predict\npredictions = svm_linear.predict(x_test)\npredictions[:10]","eade6959":"# measure accuracy\nmetrics.accuracy_score(y_true=y_test, y_pred=predictions)","df7053c2":"# class-wise accuracy\nclass_wise = metrics.classification_report(y_true=y_test, y_pred=predictions)\nprint(class_wise)","50abc921":"# run gc.collect() (garbage collect) to free up memory\n# else, since the dataset is large and SVM is computationally heavy,\n# it'll throw a memory error while training\ngc.collect()","1a35ed18":"# rbf kernel with other hyperparameters kept to default \nsvm_rbf = svm.SVC(kernel='rbf')\nsvm_rbf.fit(x_train, y_train)","0c109a5f":"# predict\npredictions = svm_rbf.predict(x_test)\n\n# accuracy \nprint(metrics.accuracy_score(y_true=y_test, y_pred=predictions))","a4d943b3":"# conduct (grid search) cross-validation to find the optimal values \n# of cost C and the choice of kernel\n\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'C':[1, 10, 100], \n             'gamma': [1e-2, 1e-3, 1e-4]}\n\n# instantiate a model \nsvc_grid_search = svm.SVC(kernel=\"rbf\")\n\n# create a classifier to perform grid search\nclf = GridSearchCV(svc_grid_search, param_grid=parameters, scoring='accuracy')\n\n# fit\nclf.fit(x_train, y_train)","75223495":"# results\ncv_results = pd.DataFrame(clf.cv_results_)\ncv_results","ea55fcd8":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(16,6))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\nplt.show()","5335ea5a":"# optimal hyperparameters\nbest_C = 1\nbest_gamma = 0.001\n\n# model\nsvm_final = svm.SVC(kernel='rbf', C=best_C, gamma=best_gamma)\n\n# fit\nsvm_final.fit(x_train, y_train)","7907f478":"# read the dataset\ndigits_test = pd.read_csv(\"..\/input\/test.csv\")\ndigits_test.info()","f93594e5":"# predict\npredictions = svm_final.predict(digits_test)","6fbfe14e":"predictions.shape","3a900851":"d = pd.DataFrame({'Label': predictions})\nprint(\"\\n\", \"d\", \"\\n\", d.head())","5556afab":"d.to_csv('digi_recon_predictions.csv', sep=\",\")","8ffb9194":"Also, let's look at the average values of each column, since we'll need to do some rescaling in case the ranges vary too much.","b93c341b":"### Non-Linear SVM\n\nLet's now try a non-linear model with the RBF kernel.","535bd9b7":"The accuracy achieved with a non-linear kernel is slightly higher than a linear one. Let's now do a grid search CV to tune the hyperparameters C and gamma.\n\n### Grid Search Cross-Validation","163a59ce":"#Digits - Classification Using SVM\n","94a295a6":"From the plot above, we can observe that (from higher to lower gamma \/ left to right):\n- At very high gamma (0.01), the model is achieving 100% accuracy on the training data, though the test score is quite low (<75%). Thus, the model is overfitting.\n\n- At gamma=0.001, the training and test scores are comparable at around C=1, though the model starts to overfit at higher values of C\n\n- At gamma=0.0001, the model does not overfit till C=10 but starts showing signs at C=100. Also, the training and test scores are slightly lower than at gamma=0.001.\n\nThus, it seems that the best combination is gamma=0.001 and C=1 (the plot in the middle), which gives the highest test accuracy (~92%) while avoiding overfitting.\n\nLet's now build the final model and see the performance on test data.\n\n### Final Model\n\nLet's now build the final model with chosen hyperparameters.","45d2d69d":"## Model Building\n\nLet's now build the model and tune the hyperparameters. Let's start with a **linear model** first.\n\n### Linear SVM\n\nLet's first try building a linear SVM model (i.e. a linear kernel). ","43f5709e":"## Data Preparation for Model Building\n\nLet's now prepare the dataset for building the model. We'll only use a fraction of the data else training will take a long time.\n"}}