{"cell_type":{"9679aaff":"code","51a526b1":"code","3c2e0e72":"code","6e5b511c":"code","550d57b5":"code","7375b2d4":"code","c0c37b57":"code","bd525d2f":"code","4d355517":"code","5df9affd":"code","cf9afefa":"code","ed15743d":"code","7fe2ab66":"code","b6735650":"code","c3566a37":"code","43ba5a7a":"code","b94bad8f":"code","cffb8066":"code","e9e6be98":"code","a22b5461":"code","22db7ee1":"code","5f4e0799":"code","7c81c24a":"code","286a3bf6":"code","f892ecc9":"code","a8d97067":"code","caea4901":"code","4489e456":"code","b90fc687":"code","ae56a244":"markdown","b4f092cb":"markdown","19b9d238":"markdown","097e4c34":"markdown","8a15b781":"markdown"},"source":{"9679aaff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51a526b1":"! ls -lrth \/kaggle\/input\/electric-power-consumption-data-set","3c2e0e72":"import pandas as pd\nimport numpy as np\nfrom math import sqrt\nfrom numpy import concatenate\nfrom matplotlib import pyplot\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, GRU\nimport tensorflow as tf\nfrom datetime import datetime","6e5b511c":"file = \"\/kaggle\/input\/electric-power-consumption-data-set\/household_power_consumption.txt\"\ndataset = read_csv(file,\n                   parse_dates={'dt' : ['Date', 'Time']},\n                   infer_datetime_format=True, \n                   index_col= 0,\n                   na_values=['nan','?'],\n                   sep=';')\ndataset.fillna(0, inplace=True)\nvalues = dataset.values\n# ensure all data is float\nvalues = values.astype('float32')","550d57b5":"dataset.head()","7375b2d4":"dataset.info()","c0c37b57":"# normalizing input features\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(values)\nscaled =pd.DataFrame(scaled)","bd525d2f":"scaled.shape","4d355517":"## Following code can be made concise by using pandas shift() method:\n# d = scaled[:-1]\n# y = scaled.shift(-1)[:-1][1]\n# d['y'] = y.values\n# display(d) # d is same as final_df\n\ndef create_ts_data(dataset, lookback=1, predicted_col=1):\n    temp=dataset.copy()\n    temp[\"id\"]= range(1, len(temp)+1)\n    temp = temp.iloc[:-lookback, :]\n    temp.set_index('id', inplace =True)\n    predicted_value=dataset.copy()\n    predicted_value = predicted_value.iloc[lookback:,predicted_col]\n    predicted_value.columns=[\"Predcited\"]\n    predicted_value= pd.DataFrame(predicted_value)\n    \n    predicted_value[\"id\"]= range(1, len(predicted_value)+1)\n    predicted_value.set_index('id', inplace =True)\n    final_df= pd.concat([temp, predicted_value], axis=1)\n    #final_df.columns = ['var1(t-1)', 'var2(t-1)', 'var3(t-1)', 'var4(t-1)', 'var5(t-1)', 'var6(t-1)', 'var7(t-1)', 'var8(t-1)','var1(t)']\n    #final_df.set_index('Date', inplace=True)\n    return final_df","5df9affd":"reframed_df= create_ts_data(scaled, 1,0)\nreframed_df.fillna(0, inplace=True)\n\nreframed_df.columns = ['var1(t-1)', 'var2(t-1)', 'var3(t-1)', 'var4(t-1)', 'var5(t-1)', 'var6(t-1)', 'var7(t-1)','var1(t)']\ndisplay(reframed_df.head(4))","cf9afefa":"reframed_df.isna().sum()","ed15743d":"from sklearn.model_selection import train_test_split","7fe2ab66":"y=reframed_df['var1(t)']\nX=reframed_df.drop(['var1(t)'], axis=1)","b6735650":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","c3566a37":"print(\"X_train.shape: \", X_train.shape)\nprint(\"y_train.shape: \", y_train.shape)\n\nprint(\"X_test.shape: \", X_test.shape)\nprint(\"y_test.shape: \", y_test.shape)","43ba5a7a":"# reshape input to be 3D [samples, time steps, features]\nX_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\nprint(X_train.shape, y_train.shape, X_test.shape, y_test.shape)","b94bad8f":"X_train.shape[1], X_train.shape[2]","cffb8066":"lstm_model = Sequential()\n\n#(7\/75\/30\/30\/1)\n\nlstm_model.add(LSTM(75, return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2])))\nlstm_model.add(LSTM(units=30, return_sequences=True))\nlstm_model.add(LSTM(units=30))\nlstm_model.add(Dense(units=1))\n\nlstm_model.compile(loss='mae', optimizer='adam')","e9e6be98":"lstm_model.summary()","a22b5461":"# fit network\nhistory_lstm = lstm_model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test),  shuffle=False)","22db7ee1":"model_gru = Sequential()\nmodel_gru.add(GRU(75, return_sequences=True,input_shape=(X_train.shape[1], X_train.shape[2])))\nmodel_gru.add(GRU(units=30, return_sequences=True))\nmodel_gru.add(GRU(units=30))\nmodel_gru.add(Dense(units=1))\n\nmodel_gru.compile(loss='mae', optimizer='adam')","5f4e0799":"model_gru.summary()","7c81c24a":"gru_history = model_gru.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), shuffle=False)","286a3bf6":"# Loss\npyplot.plot(history_lstm.history['loss'], label='LSTM train', color='red')\npyplot.plot(history_lstm.history['val_loss'], label='LSTM test', color= 'green')\npyplot.plot(gru_history.history['loss'], label='GRU train', color='brown')\npyplot.plot(gru_history.history['val_loss'], label='GRU test', color='blue')\npyplot.legend()\npyplot.show()","f892ecc9":"X_test.shape","a8d97067":"yhat_test","caea4901":"# Predictions with GRU model\n\n# make a prediction\nyhat_test = model_gru.predict(X_test)\nX_test = X_test.reshape((X_test.shape[0], 7))\n\n# invert scaling for forecast\nprint(yhat_test.shape, X_test.shape)\ninv_yhat_test = np.concatenate((yhat_test, X_test[:, -6:]), axis=1)\nprint(inv_yhat_test.shape)\ninv_yhat_test = scaler.inverse_transform(inv_yhat_test)\ninv_yhat_test = inv_yhat_test[:,0]\n","4489e456":"# invert scaling for actual\ny_test = y_test.values.reshape((len(y_test), 1))\ninv_y = np.concatenate((y_test, X_test[:, -6:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]\n\n# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_yhat_test))\nprint('Test RMSE: %.3f' % rmse)","b90fc687":"# Compare predictions with actual values\n## time steps, every step is one hour (you can easily convert the time step to the actual time index)\n## for a demonstration purpose, I only compare the predictions in 200 hours. \npyplot.figure(figsize=(12,8))\naa=[x for x in range(200)]\npyplot.plot(aa, inv_y[:200], marker='.', label=\"actual\")\npyplot.plot(aa, inv_yhat_test[:200], 'r', label=\"prediction\")\npyplot.ylabel('Global_active_power', size=15)\npyplot.xlabel('Time step', size=15)\npyplot.legend(fontsize=15)\npyplot.show()","ae56a244":"## Following steps explained in below mentioned article:\n\nhttps:\/\/medium.com\/datadriveninvestor\/multivariate-time-series-using-gated-recurrent-unit-gru-1039099e545a","b4f092cb":"## LSTM model","19b9d238":"## GRU","097e4c34":"## Check results","8a15b781":"## Reshaping the dataset"}}