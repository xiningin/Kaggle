{"cell_type":{"46494a2a":"code","24821b9a":"code","ba9929fd":"code","7f6b0cf5":"code","7048490a":"code","719adcc8":"code","a9b9ebe3":"code","68fa5246":"code","a3cbef78":"code","d06e37c2":"code","1f688f12":"code","d9084ac8":"code","cd1e9162":"code","1265a041":"code","634c3936":"code","e5586072":"code","2b4b6edf":"code","824fb095":"code","76ee2953":"code","7618e69b":"code","a27230d7":"code","73b03d2d":"code","58844564":"code","01ae39f5":"code","0b499e41":"code","6161b613":"code","c3f04973":"code","f2ce15e5":"code","4f95b4e6":"code","dae07898":"code","0deccf77":"code","0f38fbb0":"code","c9ae2028":"code","53ed500f":"code","0399b9f1":"code","f7495d1e":"code","bca301c6":"code","870b3de8":"code","c056fd71":"code","67ce91f1":"code","3bf9b781":"code","64894a84":"code","4af48670":"code","95e3b074":"code","661f0f10":"markdown","a90d3174":"markdown","972f483d":"markdown","abadb4b7":"markdown","73419be0":"markdown","eafb7686":"markdown","9e38785d":"markdown","63bb3534":"markdown","5f6d7aae":"markdown","9b678840":"markdown","5ab6d690":"markdown","d3f97332":"markdown","c951b4b0":"markdown","58b2fab2":"markdown","5a0e8fc8":"markdown","f2b5bde9":"markdown","76759675":"markdown","c25cfab1":"markdown","afd3f196":"markdown","e4c637a0":"markdown","8e107aee":"markdown","93007e36":"markdown","a1c70d4d":"markdown","1a921e1e":"markdown","adfd3352":"markdown","163d662e":"markdown","a5c88090":"markdown","801c3101":"markdown","cdf169b6":"markdown","40dcdf1a":"markdown","32f02852":"markdown","9c46faf8":"markdown"},"source":{"46494a2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24821b9a":"import pandas as pd #we use this to load, read and transform the dataset\nimport numpy as np #we use this for statistical analysis\nimport matplotlib.pyplot as plt #we use this to visualize the dataset\nimport seaborn as sns #we use this to make countplots\nimport sklearn.metrics as sklm #This is to test the models","ba9929fd":"#here we load the train data\ndata = pd.read_csv('\/kaggle\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\n\n#and immediately I would like to see how this dataset looks like\ndata.head()","7f6b0cf5":"#now let's look closer at the dataset we got\ndata.info()","7048490a":"data.shape","719adcc8":"data.describe()","a9b9ebe3":"#Let's see what the object data looks like\ndata.describe(include='O')","68fa5246":"#Let's see what the options are in the text columns (the objects)\nprint('Gender: ' + str(data['Gender'].unique()))\nprint('Married: ' + str(data['Married'].unique()))\nprint('Dependents: '+ str(data['Dependents'].unique()))\nprint('Education: '+ str(data['Education'].unique()))\nprint('Self_Employed: '+ str(data['Self_Employed'].unique()))\nprint('Property_Area: '+ str(data['Property_Area'].unique()))","a3cbef78":"#first let's count the number of loans approved and rejected\nApproved = data[data['Loan_Status'] == 'Y']['Loan_Status'].count()\nRejected = data[data['Loan_Status'] == 'N']['Loan_Status'].count()\n\n#now let's put these results in a dataframe to visualize them\ndf = {\"Count\" : [Approved, Rejected]} #this is for the legend to be clear that it is counts\nStatus = pd.DataFrame(df, index=[\"Approved\", \"Rejected\"])\n\n#let's visualize the bar plot\nax = Status.plot(kind = 'bar', title = 'Status of the loans')\n\n#here I want to add the labels to the bars and to make this more clear I've made them white of color\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() \/ 2, p.get_height() - 30), color = 'white', fontweight = 'bold')","d06e37c2":"#let's see the percentages of the status:\nprint('The percentage of approved loans : %.2f' % (data['Loan_Status'].value_counts()[0] \/ len(data)))\nprint('The percentage of rejected loans : %.2f' % (data['Loan_Status'].value_counts()[1] \/ len(data)))","1f688f12":"#let's look in what columns there are missing values \ndata.isnull().sum().sort_values(ascending = False)","d9084ac8":"#Let's look at the credit history in more detail to see what the best way is to handle these missing values\n#I will use seaborn for the visualization\nsns.countplot(data['Loan_Status'],hue=data['Credit_History'])","cd1e9162":"print(pd.crosstab(data['Credit_History'],data['Loan_Status']))","1265a041":"print('The percentage of credit history yes : %.2f' % (data['Credit_History'].value_counts()[1] \/ len(data)))\nprint('The percentage of credit history no : %.2f' % (data['Credit_History'].value_counts()[0] \/ len(data)))","634c3936":"data['Credit_History'] = data['Credit_History'].fillna(1)\ndata.isnull().sum().sort_values(ascending = False)","e5586072":"#Continue with Self_Employed\nsns.countplot(data['Loan_Status'],hue=data['Self_Employed'])","2b4b6edf":"data['Self_Employed'] = data['Self_Employed'].fillna('No')\ndata.isnull().sum().sort_values(ascending = False)","824fb095":"#Continue with LoanAmount, as this is a numeric, thus continous number, I will use a scatterplot to see if there is a pattern \/ correlation. \nplt.scatter(data['Loan_Status'], data['LoanAmount'])","76ee2953":"#As the patterns look similar for yes and no, I will fill the missing values with the mean of the column\ndata['LoanAmount'] = data['LoanAmount'].fillna( data['LoanAmount'].mean())\ndata.isnull().sum().sort_values(ascending = False)","7618e69b":"#Let's drop the rest of the missing values:\ndata.dropna(inplace = True)\ndata.shape","a27230d7":"#First I will make a boxplot for the ApplicantIncome.\nplt.boxplot(data['ApplicantIncome'])","73b03d2d":"#We see that there are two great outliers here. \n#let's look closer to these two outliers\noutliers = data[data['ApplicantIncome'] > 50000]\noutliers.head()","58844564":"#As you can see that these are just two rows and the status is not for both approved, I will remove these two rows for the model. \ndata = data[data['ApplicantIncome'] < 50000]\n#let's plot the applicant income again in a boxplot\nplt.boxplot(data['ApplicantIncome'])","01ae39f5":"#still a lot of outliers above the 25000. Let's look closer to those again to be sure we need to add them to get a good model performance\noutliers = data[data['ApplicantIncome'] > 25000]\noutliers.head()","0b499e41":"#First make the target column (Loan_Status) numerical\ndata['Loan_Status'] = np.where((data['Loan_Status'] == 'Y'), 1, 0)","6161b613":"#Next we will drop the loan_ID column as this will only confuse the model later on\ndata.drop('Loan_ID', axis=1, inplace=True)\ndata.info()","c3f04973":"#Next, make all other columns numerical as well. \ndata['Married'] = np.where((data['Married'] == 'Yes'), 1, 0)\ndata['Gender'] = np.where((data['Gender'] == 'Female'), 1, 0)\ndata['Education'] = np.where((data['Education'] == 'Graduate'), 1, 0)\ndata['Self_Employed'] = np.where((data['Self_Employed'] == 'Yes'), 1, 0)\ndata['Dependents'] = np.where((data['Dependents'] == '0'), 0, 1) #I saw that there was \n#no big difference between the number of dependents if there are any. \n#So I made no dependents = 0  and yes dependents = 1","f2ce15e5":"#Lastly I want to change the Property_Area column, but I want to keep all three options. Therefore this I will do differently. \n\ndef f(row):\n  if row['Property_Area'] == \"Rural\":\n    val = 1\n  elif row['Property_Area'] == \"Urban\":\n    val = 0\n  else:\n    val = 2\n  return val\n\ndata['Property_Area'] = data.apply(f, axis=1)","4f95b4e6":"data.info()","dae07898":"#First we need to split the dataset in the y-column (the target) and the components (X), the independent columns. \n#This is needed as we need to use the X columns to predict the y in the model. \n\nX = data.iloc[:,0:11]  #independent columns \ny = data.iloc[:,-1]    #target column = Status of the loan\n","0deccf77":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","0f38fbb0":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#apply SelectKBest class to extract top 10 best features\nbestfeatures = SelectKBest(score_func=chi2, k='all')\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Name of the column','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","c9ae2028":"#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n\n#plot heat map\ng=sns.heatmap(data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","53ed500f":"#Load the chosen models here\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n#add the logistic regression for cross check\nfrom sklearn.linear_model import LogisticRegression","0399b9f1":"from sklearn.model_selection import train_test_split\n\n#First try with the 4 most important features\nX_4 = data[['Credit_History', 'CoapplicantIncome', 'Married', 'Property_Area']] #independent columns chosen \ny = data.iloc[:,-1]    #target column = Status of the loan\n\n#I want to withhold 30 % of the trainset to perform the tests\nX_train, X_test, y_train, y_test= train_test_split(X_4,y, test_size=0.3 , random_state = 25)","f7495d1e":"print('Shape of X_train is: ', X_train.shape)\nprint('Shape of X_test is: ', X_test.shape)\nprint('Shape of Y_train is: ', y_train.shape)\nprint('Shape of y_test is: ', y_test.shape)","bca301c6":"#Let's confirm that we use the same number of status approved versus disapproved in the test and train data.\n#As approved is 1, this can be counted easily. \nprint('The % approved status versus not approved in original_data :',data['Loan_Status'].value_counts().values\/ len(data))\nprint('\\nThe % approved status versus not approved in y_train :',y_train.value_counts().values\/ len(y_train))\nprint('\\nThe % approved status versus not approved in in y_test :',y_test.value_counts().values\/ len(y_test))","870b3de8":"#To check the models, I want to build a check matrix within two functions:\ndef score_model(probs, threshold):\n    return np.array([1 if x > threshold else 0 for x in probs[:,1]])\n\ndef print_metrics(labels, probs, threshold):\n    scores = score_model(probs, threshold)\n    metrics = sklm.precision_recall_fscore_support(labels, scores)\n    conf = sklm.confusion_matrix(labels, scores)\n    print('                 Confusion matrix')\n    print('                 Score positive    Score negative')\n    print('Actual positive    %6d' % conf[0,0] + '             %5d' % conf[0,1])\n    print('Actual negative    %6d' % conf[1,0] + '             %5d' % conf[1,1])\n    print('')\n    print('DETAILS ACCURACY, PRECISION AND RECALL')\n    print('Accuracy        %0.2f' % sklm.accuracy_score(labels, scores))\n    print('AUC             %0.2f' % sklm.roc_auc_score(labels, probs[:,1]))\n    print('Macro precision %0.2f' % float((float(metrics[0][0]) + float(metrics[0][1]))\/2.0))\n    print('Macro recall    %0.2f' % float((float(metrics[1][0]) + float(metrics[1][1]))\/2.0))\n    print(' ')\n    print('           Positive      Negative')\n    print('Num case   %6d' % metrics[3][0] + '        %6d' % metrics[3][1])\n    print('Precision  %6.2f' % metrics[0][0] + '        %6.2f' % metrics[0][1])\n    print('Recall     %6.2f' % metrics[1][0] + '        %6.2f' % metrics[1][1])\n    print('F1         %6.2f' % metrics[2][0] + '        %6.2f' % metrics[2][1])","c056fd71":"#Start with the K-Nearest Neighbors\nK_n = KNeighborsClassifier()\nK_n.fit(X_train, y_train)","67ce91f1":"#Now let's see how this model performs\nprob_K = K_n.predict_proba(X_test)\nprint_metrics(y_test, prob_K, 0.3) ","3bf9b781":"#Continue with the decision tree with a max number of layers of 3\nD_tree = DecisionTreeClassifier(max_depth = 3)\nD_tree.fit(X_train, y_train)","64894a84":"#let's see it's performance\nprob_D = D_tree.predict_proba(X_test)\nprint_metrics(y_test, prob_D, 0.3)","4af48670":"# logistic_regression model\nlogistic_mod = LogisticRegression(C = 1.0, class_weight = {0:0.45, 1:0.55}) \nlogistic_mod.fit(X_train, y_train)","95e3b074":"#Check the performance of the logistic regression model\nprobabilities = logistic_mod.predict_proba(X_test)\nprint_metrics(y_test, probabilities, 0.3) ","661f0f10":"## Take a closer look at some of the numeric features\nWe saw earlier on that applicant income had some strange outliers. Let's find out about them","a90d3174":"### Split the dataset in train and test\nBefore we are going to use the models choosen, we will first split the dataset in a train and test set.\nThis because we want to test the performance of the model on the training set and to be able to check it's accuracy. ","972f483d":"# Loan Status in this Dataset\n\n![approved or rejected](https:\/\/db3pap006files.storage.live.com\/y4pVnKKIPUMfGtdOP-mIsJIDFD6QD9mNmC5br03t9oSX6uCFHlSgyrzOKvkBvemfQbgGRltJXJI1DygwGgxBzszvmqoQtfMhbsE_Ajl8VAnNDIy3BIOXRlTJAB3jdnZYTPtQFmMkHmo74vxcBUc_JjX1kW47Rp33UKov0MllAFFuPU-lzJypcr-s05Yv1bCIpcC9bwZsareXmkMCxxmCZBS67Ya2zrP2Ac3z3F0enmC6qo\/stamp-2114884_1920.png?psid=1&width=192&height=65)\n\nAs Loan Status is the column we want to predict, let's explore this column in the training dataset. ","abadb4b7":"This model does not seem to predict well enough for the positives. The true positives are 6 versus 46 false negative. On the other hand the true negatives are 119 over 1 false positive.  ","73419be0":"# Load and explore the dataset\nWe've gotten a train and a test dataset. In this next step I will first load the train data set to see how this looks like","eafb7686":"The dataset consists of 614 rows and 13 columns. ","9e38785d":"# Conclusion:\nWe would need more data to make the models perform better. \n\nFor now, The decision tree has the highest accuracy and precision scores with the 4 most important features. \nTherefore this would be the model to use for the prediction on the status","63bb3534":"The accuracy seems to be higher (true positives better, 18 now), but still room for improvement","5f6d7aae":"![lening](https:\/\/www.mymoneymantra.com\/blog\/wp-content\/uploads\/2018\/06\/Heres-Why-Banks-are-Offering-Personal-Loans-at-Competitive-Interest-Rates.jpg)","9b678840":"It seems that we have some strange outliers for the applicant income. We will look and handle these later on. ","5ab6d690":"I will only look at the top 3 here (as these have the most missing values) and drop the other missing value rows. ","d3f97332":"##### K-Nearest Neighbors","c951b4b0":"# Bank Loan Status Prediction\n","58b2fab2":"This model seems to perform a bit less than the decision tree model. 16 true positives, but 0 false positives!","5a0e8fc8":"Right so now all columns are numeric\n\n# Most important features\nLet's continue by looking at the most important features according to three different tests. \nThan we will use the top ones to train and test our first model. ","f2b5bde9":"### Import the important libraries \/ packages\nThese packages are needed to load and use the dataset","76759675":"This looks about the same, let's continue.","c25cfab1":"#### Logistic regression","afd3f196":"Seems there are more categorical (binary) columns, such as Gender, Married and Self_Employed ","e4c637a0":"# Machine learning Model\nAs this is a binary problem (so yes or no in the status), I choose for binary models:\n- Decision Tree\n- K-nearest Neighbors\n\nBut we can cross check it with a logistic regression model here.\n\nFor the record, I left out Random Forrest, as this is a random decision tree model, so not the same each time you run the model","8e107aee":"These seem to be ok for the model as 75% is approved. So let's keep them for now. \n\n## Make all columns numeric\nWe need to make all column input numeric to use them further on. \nThis is what I will do now. ","93007e36":"#### Decision Tree","a1c70d4d":"# Handling missing values\nLet's continue with handling the missing values in this dataset. \nLet's see where and how many missing values there are in this dataset.  ","1a921e1e":"Seems that if you have a credit history, it is more likely to get the loan approved. \n\nOptions in handling these missing values:\n- Drop all the rows with missing values\n- Handle the missing values with 0 (so no history) as there is nothing clear. \n- Or we use the most frequent number, which is 1 for the credit history. \n\nIn this case, I tend to go for the most frequent number, as this is 86% of the dataset, so most likely to be true.\n","adfd3352":"It seems that we have a lot of text \/ category information (these are of the Dtype 'object') and a few numerical columns (Dtypes 'int64' and 'float64').\n\nThe last column 'Loan_status' is the column we would like to predict.","163d662e":"# Try and check the models ","a5c88090":"As this seems to have no effect on the outcome, I will fill these with the most frequent one (so No) ","801c3101":"# The goal\nIn this notebook we look at the data we got via this [Kaggle competition](https:\/\/www.kaggle.com\/zaurbegiev\/my-dataset). \n\nWe will see if we can predict whether or not the bank will approve the loan based on certain information. \n\nWe will explore the dataset given, check the various features we have and we will make an algorithm that can predict whether or not the loan would be approved.","cdf169b6":"![BAIME banner](https:\/\/user-images.githubusercontent.com\/47600826\/89530907-9b3f6480-d7ef-11ea-9849-27617f6025cf.png)","40dcdf1a":"Looks like a good feature to use, so let's look deeper!","32f02852":"It looks like this is not well balanced in this set.\nBut as this is the only data we have, I will leave this as is for now. ","9c46faf8":"Seems that the models differ in what feature is the most important.\nFor the first test I will keep:\n- Credit history (high in all three tests and the highest in the correlation)\n- Co Applicant Income (high in two tests, negative in the correlation, but this is explainable, as no income for the spous means more risk)\n- Property Area (high in two tests)\n- Married (mentioned in two tests)\n\nAfter a test, these 4 gave better results than using all features. "}}