{"cell_type":{"3fe2eedb":"code","af998dff":"code","1ceceee0":"code","eb378b5a":"code","f5498812":"code","ee94965a":"code","85424c48":"code","9983c2b7":"code","d6246534":"code","c480286d":"code","e2977cf1":"code","36fbeee4":"code","8d47075d":"markdown","9181e6a4":"markdown","a251b620":"markdown","169146ea":"markdown","8ec9c9aa":"markdown","73b3c5d7":"markdown","f59ffc53":"markdown","b5d071d2":"markdown","7ea7ee8a":"markdown","634e4dd3":"markdown","ed24d4bc":"markdown","51d599a6":"markdown","1bece21f":"markdown","06cb66db":"markdown"},"source":{"3fe2eedb":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import PCA as sklearnPCA\n\ndataset = pd.read_csv('..\/input\/combinelast2\/combine_d2.csv')\ndataset\n","af998dff":"x = dataset.iloc[:,0:2].values\ny = dataset.iloc[:,2].values","1ceceee0":"x_std = StandardScaler().fit_transform(x)\n#x_std","eb378b5a":"mean_vec= np.mean(x_std,axis=0)\ncov_mat = (x_std - mean_vec).T.dot((x_std-mean_vec))\/(x_std.shape[0]-1)\nprint('Covariance matrix \\n%s' %cov_mat)","f5498812":"cov_mat =np.cov(x_std.T)\n\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nprint('Eigenvectors \\n%a' %eig_vecs)","ee94965a":"pca= PCA(n_components=2)\nx_std = pca.fit_transform(x_std)\nexplained_variance= pca.explained_variance_ratio_\nexplained_variance","85424c48":"sklearn_pca = sklearnPCA(n_components = 2)\nY_sklearn = sklearn_pca.fit_transform(x_std)\n#Y_sklearn","9983c2b7":"plt.figure()\nplt.scatter(x, Y_sklearn)","d6246534":"z = dataset['Height']\n\nplt.figure()\nplt.scatter(z, y)\n","c480286d":"w = dataset['Weight']\n\nplt.figure()\nplt.scatter(w, y)","e2977cf1":"import numpy as np\nimport pandas as pd\n\ndata = pd.read_csv(\"..\/input\/wrdata\/1.csv\")\ndata\n\ndata2 = data[[\"Rec\", \"Unnamed: 10\"]]\ndata1 = data2[data2[\"Rec\"] > 80]\ndata3 = data1[data1[\"Unnamed: 10\"]> 800]\ndata","36fbeee4":"%matplotlib inline\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\nfrom numpy import random, float\n\nmodel = KMeans(n_clusters=6)\n\nmodel = model.fit(scale(data3))\n\nplt.figure(figsize=(8, 6))\nplt.scatter(data3[\"Unnamed: 10\"], data3[\"Rec\"], c=model.labels_.astype(float))\nplt.title('Clustering: Rec v.s Yards')\nplt.xlabel('Yards')\nplt.ylabel('Rec')\nplt.annotate(\"Can't Guard Mike\", (1725,149))\nplt.annotate(\"CMC\", (1005,116))\nplt.annotate(\"Julio\", (1394,99))\nplt.annotate(\"Godwin\", (1333,86))\nplt.annotate(\"Waller\", (1145,90))\nplt.annotate(\"D. Adams\", (997,83))\nplt.annotate(\"Nuke\", (1165,104))\nplt.show()\n","8d47075d":"In this notebook; I wanted to study how weight and height affect 40 yard dash times for running backs and receivers from the 2019 NFL combine. Height is measured in inches here. ","9181e6a4":"How the data originally looked:","a251b620":"Clustering: tiers of WRs (and some RBs) for Rec v.s Yards ","169146ea":"CMC and Micheal Thomas are in tiers of their own!","8ec9c9aa":"PCA:","73b3c5d7":"Get the eigenvectors & eigenvalues","f59ffc53":"Subtract from the empirical mean; and build the 2x2 covariance matrix","b5d071d2":"K-Means Clustering","7ea7ee8a":"Only keeping receivers with more than 80 receptions and 800 yards receiving","634e4dd3":"Visualizations of the results","ed24d4bc":"First, I'm practicing PCA (principle component analysis) on NFL combine data. The second half I'm practing the K-Means clustering technique on NFL receiver data. ","51d599a6":"Call the PCA package from SKLearn","1bece21f":"The Transformed data (here we classify 40 YD times based on Heights and Weights); PCA is great for clustering.","06cb66db":"Here we calculate the standard deviation of the x values (Height and weight)"}}