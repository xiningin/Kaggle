{"cell_type":{"32e85291":"code","b3ce77b8":"code","125cac4d":"code","f02fc95d":"code","7184445e":"code","165723aa":"code","b984ef57":"code","4d141516":"code","a79b51eb":"code","6acf3d8c":"code","866a621f":"code","3440f6bd":"code","352a95ea":"code","1634b835":"code","d68bbf4e":"code","f6a82500":"code","518ab1d6":"code","e5bf10b1":"code","3b149f4f":"code","d072e999":"code","881c155a":"code","74457108":"markdown","bf6506a7":"markdown"},"source":{"32e85291":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport cv2\nfrom tqdm.auto import tqdm\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3ce77b8":"df = pd.read_csv('..\/input\/global-wheat-detection\/train.csv')\nbboxs = np.stack(df['bbox'].apply(lambda x: np.fromstring(x[1:-1], sep=',')))\nfor i, column in enumerate(['x', 'y', 'w', 'h']):\n    df[column] = bboxs[:,i]\ndf.drop(columns=['bbox'], inplace=True)\ndf = df[['image_id','x', 'y', 'w', 'h']]\nindex = list(set(df.image_id))\n","125cac4d":"!mkdir train\n!mkdir val","f02fc95d":"!git clone https:\/\/github.com\/eriklindernoren\/PyTorch-GAN\/\n%cd PyTorch-GAN\/implementations\/pix2pix","7184445e":"!ls","165723aa":"import os\nimport numpy as np\nimport math\nimport itertools\nimport time\nimport datetime\nimport sys\n\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\n\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nfrom torch.autograd import Variable\n\nfrom models import *\nfrom datasets import *\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch","b984ef57":"class opt:\n    epoch = 0\n    n_epochs = 10 #change to 50 for train\n    dataset_name = 'test1'\n    batch_size = 8\n    lr = 0.0002\n    b1 = 0.5\n    b2 = 0.999\n    decay_epoch = 100\n    n_cpu = 4\n    img_height = 256\n    img_width = 256\n    channels = 3\n    sample_interval = 100\n    checkpoint_interval = 338","4d141516":"os.makedirs(\"images\/%s\" % opt.dataset_name, exist_ok=True)\nos.makedirs(\"saved_models\/%s\" % opt.dataset_name, exist_ok=True)\n","a79b51eb":"cuda = True if torch.cuda.is_available() else False\n\n# Loss functions\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_pixelwise = torch.nn.L1Loss()\n\n# Loss weight of L1 pixel-wise loss between translated image and real image\nlambda_pixel = 100\n\n# Calculate output of image discriminator (PatchGAN)\npatch = (1, opt.img_height \/\/ 2 ** 4, opt.img_width \/\/ 2 ** 4)\n\n# Initialize generator and discriminator\ngenerator = GeneratorUNet()\ndiscriminator = Discriminator()\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    criterion_GAN.cuda()\n    criterion_pixelwise.cuda()\n\nif opt.epoch != 0:\n    # Load pretrained models\n    generator.load_state_dict(torch.load(\"saved_models\/%s\/generator_%d.pth\" % (opt.dataset_name, opt.epoch)))\n    discriminator.load_state_dict(torch.load(\"saved_models\/%s\/discriminator_%d.pth\" % (opt.dataset_name, opt.epoch)))\nelse:\n    # Initialize weights\n    generator.apply(weights_init_normal)\n    discriminator.apply(weights_init_normal)","6acf3d8c":"optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\noptimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))","866a621f":"import glob\nimport random\nimport os\nimport numpy as np\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom matplotlib import pyplot as plt\n\nclass ImageDataset_color(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"\/*.*\"))\n    def __getitem__(self, index):\n\n        img_A = cv2.imread(self.files[index % len(self.files)])\n        img_A = cv2.cvtColor(img_A,cv2.COLOR_BGR2RGB)\n        img_B = cv2.cvtColor(cv2.cvtColor(img_A,cv2.COLOR_RGB2GRAY),cv2.COLOR_GRAY2RGB)\n        img_A = Image.fromarray(np.array(img_A), \"RGB\")\n        img_B = Image.fromarray(np.array(img_B), \"RGB\")\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)\n    \nclass ImageDataset_edge(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"\/*.*\"))\n    def __getitem__(self, index):\n\n        img_A = cv2.imread(self.files[index % len(self.files)])\n        gray = cv2.cvtColor(img_A, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        img_B = cv2.Canny(blurred,50,150)\n        img_B = cv2.cvtColor(img_B,cv2.COLOR_GRAY2RGB)\n        img_A = cv2.cvtColor(img_A,cv2.COLOR_BGR2RGB)\n        img_A = Image.fromarray(np.array(img_A), \"RGB\")\n        img_B = Image.fromarray(np.array(img_B), \"RGB\")\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)\n    \nclass ImageDataset_edge(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"\/*.*\"))\n    def __getitem__(self, index):\n\n        img_A = cv2.imread(self.files[index % len(self.files)])\n        gray = cv2.cvtColor(img_A, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        img_B = cv2.Canny(blurred,50,150)\n        img_B = cv2.cvtColor(img_B,cv2.COLOR_GRAY2RGB)\n        img_A = cv2.cvtColor(img_A,cv2.COLOR_BGR2RGB)\n        img_A = Image.fromarray(np.array(img_A), \"RGB\")\n        img_B = Image.fromarray(np.array(img_B), \"RGB\")\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)","3440f6bd":"!ls kagglet\/","352a95ea":"# Configure dataloaders\ntransforms_ = [\n    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\ndataloader = DataLoader(\n    ImageDataset_color(\"\/kaggle\/input\/global-wheat-detection\/\", transforms_=transforms_),\n    batch_size=opt.batch_size,\n    shuffle=True,\n    num_workers=opt.n_cpu,\n)\n\nval_dataloader = DataLoader(\n    ImageDataset_color(\"\/kaggle\/input\/global-wheat-detection\/\", transforms_=transforms_, mode=\"test\"),\n    batch_size=10,\n    shuffle=True,\n    num_workers=1,\n)","1634b835":"Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor","d68bbf4e":"def sample_images(batches_done):\n    \"\"\"Saves a generated sample from the validation set\"\"\"\n    imgs = next(iter(val_dataloader))\n    real_A = Variable(imgs[\"B\"].type(Tensor))\n    real_B = Variable(imgs[\"A\"].type(Tensor))\n    fake_B = generator(real_A)\n    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2).cpu().numpy().astype(np.float32)\n    img_sample -=img_sample.min()\n    img_sample\/=img_sample.max()\n    img_sample = img_sample.transpose(0,2,3,1)\n    plt.figure(figsize=[10,20])\n    for row in range(3):\n        plt.subplot(1,3,row+1)\n        plt.imshow(img_sample[row])\n    plt.show()\n    ","f6a82500":"prev_time = time.time()\n\nfor epoch in range(opt.epoch, opt.n_epochs):\n    for i, batch in enumerate(dataloader):\n\n        # Model inputs\n        real_A = Variable(batch[\"B\"].type(Tensor))\n        real_B = Variable(batch[\"A\"].type(Tensor))\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n\n        # ------------------\n        #  Train Generators\n        # ------------------\n\n        optimizer_G.zero_grad()\n\n        # GAN loss\n        fake_B = generator(real_A)\n        pred_fake = discriminator(fake_B, real_A)\n        loss_GAN = criterion_GAN(pred_fake, valid)\n        # Pixel-wise loss\n        loss_pixel = criterion_pixelwise(fake_B, real_B)\n\n        # Total loss\n        loss_G = loss_GAN + lambda_pixel * loss_pixel\n\n        loss_G.backward()\n\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Real loss\n        pred_real = discriminator(real_B, real_A)\n        loss_real = criterion_GAN(pred_real, valid)\n\n        # Fake loss\n        pred_fake = discriminator(fake_B.detach(), real_A)\n        loss_fake = criterion_GAN(pred_fake, fake)\n\n        # Total loss\n        loss_D = 0.5 * (loss_real + loss_fake)\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        # --------------\n        #  Log Progress\n        # --------------\n\n        # Determine approximate time left\n        batches_done = epoch * len(dataloader) + i\n        batches_left = opt.n_epochs * len(dataloader) - batches_done\n        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n        prev_time = time.time()\n\n        # Print log\n        sys.stdout.write(\n            \"\\r[Epoch %d\/%d] [Batch %d\/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n            % (\n                epoch,\n                opt.n_epochs,\n                i,\n                len(dataloader),\n                loss_D.item(),\n                loss_G.item(),\n                loss_pixel.item(),\n                loss_GAN.item(),\n                time_left,\n            )\n        )\n\n        # If at sample interval save image\n        if batches_done % opt.sample_interval == 0:\n            sample_images(batches_done)\n\n    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n        # Save model checkpoints\n        torch.save(generator.state_dict(), \"saved_models\/%s\/generator_%d.pth\" % (opt.dataset_name, epoch))\n        torch.save(discriminator.state_dict(), \"saved_models\/%s\/discriminator_%d.pth\" % (opt.dataset_name, epoch))","518ab1d6":"sample_images(batches_done)","e5bf10b1":"# Configure dataloaders\ntransforms_ = [\n    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\ndataloader = DataLoader(\n    ImageDataset_edge(\"\/kaggle\/input\/global-wheat-detection\/\", transforms_=transforms_),\n    batch_size=opt.batch_size,\n    shuffle=True,\n    num_workers=opt.n_cpu,\n)\n\nval_dataloader = DataLoader(\n    ImageDataset_edge(\"\/kaggle\/input\/global-wheat-detection\/\", transforms_=transforms_, mode=\"test\"),\n    batch_size=10,\n    shuffle=True,\n    num_workers=1,\n)","3b149f4f":"prev_time = time.time()\n\nfor epoch in range(opt.epoch, opt.n_epochs):\n    for i, batch in enumerate(dataloader):\n\n        # Model inputs\n        real_A = Variable(batch[\"B\"].type(Tensor))\n        real_B = Variable(batch[\"A\"].type(Tensor))\n\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *patch))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *patch))), requires_grad=False)\n\n        # ------------------\n        #  Train Generators\n        # ------------------\n\n        optimizer_G.zero_grad()\n\n        # GAN loss\n        fake_B = generator(real_A)\n        pred_fake = discriminator(fake_B, real_A)\n        loss_GAN = criterion_GAN(pred_fake, valid)\n        # Pixel-wise loss\n        loss_pixel = criterion_pixelwise(fake_B, real_B)\n\n        # Total loss\n        loss_G = loss_GAN + lambda_pixel * loss_pixel\n\n        loss_G.backward()\n\n        optimizer_G.step()\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        optimizer_D.zero_grad()\n\n        # Real loss\n        pred_real = discriminator(real_B, real_A)\n        loss_real = criterion_GAN(pred_real, valid)\n\n        # Fake loss\n        pred_fake = discriminator(fake_B.detach(), real_A)\n        loss_fake = criterion_GAN(pred_fake, fake)\n\n        # Total loss\n        loss_D = 0.5 * (loss_real + loss_fake)\n\n        loss_D.backward()\n        optimizer_D.step()\n\n        # --------------\n        #  Log Progress\n        # --------------\n\n        # Determine approximate time left\n        batches_done = epoch * len(dataloader) + i\n        batches_left = opt.n_epochs * len(dataloader) - batches_done\n        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n        prev_time = time.time()\n\n        # Print log\n        sys.stdout.write(\n            \"\\r[Epoch %d\/%d] [Batch %d\/%d] [D loss: %f] [G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n            % (\n                epoch,\n                opt.n_epochs,\n                i,\n                len(dataloader),\n                loss_D.item(),\n                loss_G.item(),\n                loss_pixel.item(),\n                loss_GAN.item(),\n                time_left,\n            )\n        )\n\n        # If at sample interval save image\n        if batches_done % opt.sample_interval == 0:\n            sample_images(batches_done)\n\n    if opt.checkpoint_interval != -1 and epoch % opt.checkpoint_interval == 0:\n        # Save model checkpoints\n        torch.save(generator.state_dict(), \"saved_models\/%s\/generator_%d.pth\" % (opt.dataset_name, epoch))\n        torch.save(discriminator.state_dict(), \"saved_models\/%s\/discriminator_%d.pth\" % (opt.dataset_name, epoch))","d072e999":"sample_images(batches_done)","881c155a":"!rm -rf \/kaggle\/working\/*","74457108":"# To be continued ...","bf6506a7":"# Run color train"}}