{"cell_type":{"81197646":"code","59167a12":"code","60198808":"code","e4263363":"code","3c0ae6a1":"code","2bebd0a7":"code","4a8c4711":"code","d1601f8e":"code","eef6a001":"code","00d034d3":"code","f6e6c514":"markdown","410710d6":"markdown","4d2a7945":"markdown","2ab9c4eb":"markdown","8c7a7dff":"markdown","93c5e394":"markdown","958ae92d":"markdown"},"source":{"81197646":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59167a12":"# Packages for reading, visualization and manipulating data\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nimport numpy as np\nfrom scipy import stats\n\n#scaling and spliting\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\n#ML algorythms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n\n#performance metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","60198808":"#read data ,Data examination, clearing NaN values\nfile_path = '\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv'\ndf = pd.read_csv(file_path)\n\ndf.describe().T","e4263363":"z = np.abs(stats.zscore(df))\nthreshold = 3\nnp.where(z>threshold)\ndf = df[(z < threshold).all(axis=1)]\ndf.describe().T","3c0ae6a1":"df.loc[(df.SkinThickness<5)& (df.Outcome==0), 'SkinThickness']=int(df[(df.Outcome==0)]['SkinThickness'].mean())\ndf.loc[(df.SkinThickness<5)& (df.Outcome==1), 'SkinThickness']=int(df[(df.Outcome==1)]['SkinThickness'].mean())\ndf.loc[(df.Insulin==0)& (df.Outcome==0), 'Insulin']=int(df[(df.Outcome==0)]['Insulin'].mean())\ndf.loc[(df.Insulin==0)& (df.Outcome==1), 'Insulin']=int(df[(df.Outcome==1)]['Insulin'].mean())\ndf.describe().T","2bebd0a7":"#Visiuals\nfor i in df.columns:\n    sns.histplot(x=i, hue= 'Outcome', data=df)\n    plt.title(i)\n    plt.show()\n    \nsns.countplot(df['Outcome'])","4a8c4711":"ss = StandardScaler()\ntrain = df.iloc[:,0:8]\ntrain = ss.fit_transform(train)\ntest = df['Outcome']","d1601f8e":"#correlations between variables\ncorr = df.corr()\nsns.heatmap(corr)\nplt.show()","eef6a001":"#split file for testing and training\nx_train, x_test, y_train, y_test = train_test_split(train,test, train_size=0.7,\n                                                    random_state= 42)","00d034d3":"#Machine Learning models and testing\nmodels = {\"LR\": LogisticRegression(),\n          \"NB\": GaussianNB(),\n          \"KNN\" : KNeighborsClassifier(),\n          \"DT\" : DecisionTreeClassifier(),\n          \"SVM rbf\": SVC(),\n          \"SVM linear\" : SVC(kernel='linear'),\n          'LDA' : LinearDiscriminantAnalysis(),\n          'RFC' : RandomForestClassifier(),\n          'BGC' : BaggingClassifier(),\n          'ABC' : AdaBoostClassifier(),\n          'GBC' : GradientBoostingClassifier(),\n          'DTC' : DecisionTreeClassifier(),\n          }\nfor test, clf in models.items():\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    acc = accuracy_score(y_test,y_pred)\n    print( test + ' scores')\n    print(acc)\n    print(classification_report(y_test, y_pred))\n    print(confusion_matrix(y_test, y_pred))\n    print('*' * 100)","f6e6c514":"First of all, import packages to use later.\n","410710d6":"It can be assumed that Random Forest Classifier is a good option to predict Diabetus Mellitus. \n\nPS: Please rekt me for me to learn. Thank you","4d2a7945":"No missing values in the dataset. However, it is not possible that someone has 0 glucose,bloodpressue, skinthickness , insulin and BMI scores. Therefore it is needed to remove outliners (by changing 0's to NaN and refilling with the column value or by removing Z values below 3; I choose removing Z treshhold method)","2ab9c4eb":"From the histplots it can be considered that the data is not distributed normally. For the data not disturbed normally, StandardScaler could be used.","8c7a7dff":"I think we are good to go.","93c5e394":"As a beginner in ML and python, probably this is my 3rd data project. And any feedback would be appretiated. Anyway here's my shot.","958ae92d":"Now all columns except Insulin and SkinThickness have no 0 values. Replaces 0's with means of the columns."}}