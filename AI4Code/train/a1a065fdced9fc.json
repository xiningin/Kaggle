{"cell_type":{"51a9827f":"code","589f8fea":"code","f2caeab0":"code","1cf69a86":"code","0b33e18c":"code","71325a4b":"code","aca95e5f":"code","c8328297":"code","9aef3461":"markdown","fdb36edd":"markdown","82c34b8b":"markdown","88aa4d55":"markdown"},"source":{"51a9827f":"## Example taken from https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_cv_indices.html\n\nfrom sklearn.model_selection import (TimeSeriesSplit, KFold, ShuffleSplit,\n                                     StratifiedKFold, GroupShuffleSplit,\n                                     GroupKFold, StratifiedShuffleSplit)\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nnp.random.seed(1338)\ncmap_data = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\nn_splits = 4\n\ncvs = [KFold, GroupKFold, ShuffleSplit, StratifiedKFold,\n       GroupShuffleSplit, StratifiedShuffleSplit, TimeSeriesSplit]\n\n\nn_points = 100\nX = np.random.randn(100, 10)\n\npercentiles_classes = [.1, .2, .3,.4]\ny = np.hstack([[ii] * int(100 * perc)\n               for ii, perc in enumerate(percentiles_classes)])\n\n# Evenly spaced groups repeated once\ngroups = np.hstack([[ii] * 10 for ii in range(10)])\n\n\ndef visualize_groups(classes, groups, name):\n    # Visualize dataset groups\n    fig, ax = plt.subplots()\n    ax.scatter(range(len(groups)),  [.5] * len(groups), c=groups, marker='_',\n               lw=50, cmap=cmap_data)\n    ax.scatter(range(len(groups)),  [3.5] * len(groups), c=classes, marker='_',\n               lw=50, cmap=cmap_data)\n    ax.set(ylim=[-1, 5], yticks=[.5, 3.5],\n           yticklabels=['Data\\ngroup', 'Data\\nclass'], xlabel=\"Sample index\")\n\ndef plot_cv_indices(cv, X, y, group, ax, n_splits, lw=10):\n    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n\n    # Generate the training\/testing visualizations for each CV split\n    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y, groups=group)):\n        # Fill in indices with the training\/test groups\n        indices = np.array([np.nan] * len(X))\n        indices[tt] = 1\n        indices[tr] = 0\n\n        # Visualize the results\n        ax.scatter(range(len(indices)), [ii + .5] * len(indices),\n                   c=indices, marker='_', lw=lw, cmap=cmap_cv,\n                   vmin=-.2, vmax=1.2)\n\n    # Plot the data classes and groups at the end\n    ax.scatter(range(len(X)), [ii + 1.5] * len(X),\n               c=y, marker='_', lw=lw, cmap=cmap_data)\n\n    ax.scatter(range(len(X)), [ii + 2.5] * len(X),\n               c=group, marker='_', lw=lw, cmap=cmap_data)\n\n    # Formatting\n    yticklabels = list(range(n_splits)) + ['class', 'group']\n    ax.set(yticks=np.arange(n_splits+2) + .5, yticklabels=yticklabels,\n           xlabel='Sample index', ylabel=\"CV iteration\",\n           ylim=[n_splits+2.2, -.2], xlim=[0, 100])\n    ax.set_title('{}'.format(type(cv).__name__), fontsize=15)\n    return ax","589f8fea":"cvs = [KFold, StratifiedKFold, GroupKFold]\n\n\nfor cv in cvs:\n    this_cv = cv(n_splits=n_splits)\n    fig, ax = plt.subplots(figsize=(6, 3))\n    plot_cv_indices(this_cv, X, y, groups, ax, n_splits)\n\n    ax.legend([Patch(color=cmap_cv(.8)), Patch(color=cmap_cv(.02))],\n              ['Testing set', 'Training set'], loc=(1.02, .8))\n    # Make the legend fit\n    plt.tight_layout()\n    fig.subplots_adjust(right=1)\nplt.show()","f2caeab0":"import numpy as np \nimport pandas as pd\nimport os","1cf69a86":"df = pd.read_csv('\/kaggle\/input\/shopee-product-matching\/train.csv')\ndf.head()","0b33e18c":"label_gc = df.label_group.value_counts().to_dict()\ndf['group_counts'] = df.label_group.map(label_gc)\ndf['psudo_label'] = df.group_counts.map({c: idx for idx, c in enumerate(df.group_counts.unique())})\ndf.head()","71325a4b":"from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits\nfrom sklearn.utils.validation import check_random_state, column_or_1d\nfrom sklearn.utils.multiclass import type_of_target\nfrom collections import defaultdict\n\nclass StratifiedGroupKFold(_BaseKFold):\n    \"\"\"Stratified K-Folds iterator variant with non-overlapping groups.\n    This cross-validation object is a variation of StratifiedKFold attempts to\n    return stratified folds with non-overlapping groups. The folds are made by\n    preserving the percentage of samples for each class.\n    The same group will not appear in two different folds (the number of\n    distinct groups has to be at least equal to the number of folds).\n    The difference between GroupKFold and StratifiedGroupKFold is that\n    the former attempts to create balanced folds such that the number of\n    distinct groups is approximately the same in each fold, whereas\n    StratifiedGroupKFold attempts to create folds which preserve the\n    percentage of samples for each class as much as possible given the\n    constraint of non-overlapping groups between splits.\n    Read more in the :ref:`User Guide <cross_validation>`.\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n    shuffle : bool, default=False\n        Whether to shuffle each class's samples before splitting into batches.\n        Note that the samples within each split will not be shuffled.\n        This implementation can only shuffle groups that have approximately the\n        same y distribution, no global shuffle will be performed.\n    random_state : int or RandomState instance, default=None\n        When `shuffle` is True, `random_state` affects the ordering of the\n        indices, which controls the randomness of each fold for each class.\n        Otherwise, leave `random_state` as `None`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.model_selection import StratifiedGroupKFold\n    >>> X = np.ones((17, 2))\n    >>> y = np.array([0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n    >>> groups = np.array([1, 1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 5, 6, 6, 7, 8, 8])\n    >>> cv = StratifiedGroupKFold(n_splits=3)\n    >>> for train_idxs, test_idxs in cv.split(X, y, groups):\n    ...     print(\"TRAIN:\", groups[train_idxs])\n    ...     print(\"      \", y[train_idxs])\n    ...     print(\" TEST:\", groups[test_idxs])\n    ...     print(\"      \", y[test_idxs])\n    TRAIN: [1 1 2 2 4 5 5 5 5 8 8]\n           [0 0 1 1 1 0 0 0 0 0 0]\n     TEST: [3 3 3 6 6 7]\n           [1 1 1 0 0 0]\n    TRAIN: [3 3 3 4 5 5 5 5 6 6 7]\n           [1 1 1 1 0 0 0 0 0 0 0]\n     TEST: [1 1 2 2 8 8]\n           [0 0 1 1 0 0]\n    TRAIN: [1 1 2 2 3 3 3 6 6 7 8 8]\n           [0 0 1 1 1 1 1 0 0 0 0 0]\n     TEST: [4 5 5 5 5]\n           [1 0 0 0 0]\n    Notes\n    -----\n    The implementation is designed to:\n    * Mimic the behavior of StratifiedKFold as much as possible for trivial\n      groups (e.g. when each group contains only one sample).\n    * Be invariant to class label: relabelling ``y = [\"Happy\", \"Sad\"]`` to\n      ``y = [1, 0]`` should not change the indices generated.\n    * Stratify based on samples as much as possible while keeping\n      non-overlapping groups constraint. That means that in some cases when\n      there is a small number of groups containing a large number of samples\n      the stratification will not be possible and the behavior will be close\n      to GroupKFold.\n    See also\n    --------\n    StratifiedKFold: Takes class information into account to build folds which\n        retain class distributions (for binary or multiclass classification\n        tasks).\n    GroupKFold: K-fold iterator variant with non-overlapping groups.\n    \"\"\"\n\n    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n        super().__init__(n_splits=n_splits, shuffle=shuffle,\n                         random_state=random_state)\n\n    def _iter_test_indices(self, X, y, groups):\n        # Implementation is based on this kaggle kernel:\n        # https:\/\/www.kaggle.com\/jakubwasikowski\/stratified-group-k-fold-cross-validation\n        # and is a subject to Apache 2.0 License. You may obtain a copy of the\n        # License at http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n        # Changelist:\n        # - Refactored function to a class following scikit-learn KFold\n        #   interface.\n        # - Added heuristic for assigning group to the least populated fold in\n        #   cases when all other criteria are equal\n        # - Swtch from using python ``Counter`` to ``np.unique`` to get class\n        #   distribution\n        # - Added scikit-learn checks for input: checking that target is binary\n        #   or multiclass, checking passed random state, checking that number\n        #   of splits is less than number of members in each class, checking\n        #   that least populated class has more members than there are splits.\n        rng = check_random_state(self.random_state)\n        y = np.asarray(y)\n        type_of_target_y = type_of_target(y)\n        allowed_target_types = ('binary', 'multiclass')\n        if type_of_target_y not in allowed_target_types:\n            raise ValueError(\n                'Supported target types are: {}. Got {!r} instead.'.format(\n                    allowed_target_types, type_of_target_y))\n\n        y = column_or_1d(y)\n        _, y_inv, y_cnt = np.unique(y, return_inverse=True, return_counts=True)\n        if np.all(self.n_splits > y_cnt):\n            raise ValueError(\"n_splits=%d cannot be greater than the\"\n                             \" number of members in each class.\"\n                             % (self.n_splits))\n        n_smallest_class = np.min(y_cnt)\n        if self.n_splits > n_smallest_class:\n            warnings.warn((\"The least populated class in y has only %d\"\n                           \" members, which is less than n_splits=%d.\"\n                           % (n_smallest_class, self.n_splits)), UserWarning)\n        n_classes = len(y_cnt)\n        \n        \n        _, groups_inv, groups_cnt = np.unique(\n            groups, return_inverse=True, return_counts=True)\n        y_counts_per_group = np.zeros((len(groups_cnt), n_classes))\n        for class_idx, group_idx in zip(y_inv, groups_inv):\n            y_counts_per_group[group_idx, class_idx] += 1\n\n        y_counts_per_fold = np.zeros((self.n_splits, n_classes))\n        groups_per_fold = defaultdict(set)\n\n        if self.shuffle:\n            rng.shuffle(y_counts_per_group)\n\n        # Stable sort to keep shuffled order for groups with the same\n        # class distribution variance\n        sorted_groups_idx = np.argsort(-np.std(y_counts_per_group, axis=1),\n                                       kind='mergesort')\n\n        for group_idx in sorted_groups_idx:\n            group_y_counts = y_counts_per_group[group_idx]\n            best_fold = self._find_best_fold(\n                y_counts_per_fold=y_counts_per_fold, y_cnt=y_cnt,\n                group_y_counts=group_y_counts)\n            y_counts_per_fold[best_fold] += group_y_counts\n            groups_per_fold[best_fold].add(group_idx)\n\n        for i in range(self.n_splits):\n            test_indices = [idx for idx, group_idx in enumerate(groups_inv)\n                            if group_idx in groups_per_fold[i]]\n            yield test_indices\n\n    def _find_best_fold(\n            self, y_counts_per_fold, y_cnt, group_y_counts):\n        best_fold = None\n        min_eval = np.inf\n        min_samples_in_fold = np.inf\n        for i in range(self.n_splits):\n            y_counts_per_fold[i] += group_y_counts\n            # Summarise the distribution over classes in each proposed fold\n            std_per_class = np.std(\n                y_counts_per_fold \/ y_cnt.reshape(1, -1),\n                axis=0)\n            y_counts_per_fold[i] -= group_y_counts\n            fold_eval = np.mean(std_per_class)\n            samples_in_fold = np.sum(y_counts_per_fold[i])\n            is_current_fold_better = (\n                fold_eval < min_eval or\n                np.isclose(fold_eval, min_eval)\n                and samples_in_fold < min_samples_in_fold\n            )\n            if is_current_fold_better:\n                min_eval = fold_eval\n                min_samples_in_fold = samples_in_fold\n                best_fold = i\n        return best_fold\n\n\n","aca95e5f":"from collections import Counter, defaultdict\n\ntrain_x = df\ntrain_y = train_x.psudo_label.values\ngroups = np.array(train_x.label_group.values)\n\ndef get_distribution(y_vals):\n        y_distr = Counter(y_vals)\n        y_vals_sum = sum(y_distr.values())\n        return [f'{y_distr[i] \/ y_vals_sum:.2%}' for i in range(np.max(y_vals) + 1)]","c8328297":"distrs = [get_distribution(train_y)]\nindex = ['training set']\ncv = StratifiedGroupKFold()\n\nfor fold_ind, (dev_ind, val_ind) in enumerate(cv.split(train_x, train_y, groups)):\n    dev_y, val_y = train_y[dev_ind], train_y[val_ind]\n    dev_groups, val_groups = groups[dev_ind], groups[val_ind]\n    \n    assert len(set(dev_groups) & set(val_groups)) == 0\n    distrs.append(get_distribution(dev_y))\n    index.append(f'development set - fold {fold_ind}')\n    distrs.append(get_distribution(val_y))\n    index.append(f'validation set - fold {fold_ind}')\n\ndisplay('Distribution per class:')\npd.DataFrame(distrs, index=index, columns=[f'Label {l}' for l in range(np.max(train_y) + 1)])","9aef3461":"<a  id='stratifiedgroupkfold'><\/a>\n## StratifiedGroupKFold\n\nStatifiedGroupKFold is useful when our underlying data has groups (in this competition **label_group**) and we also want that the training sample must have similar distribution of data (in our case similar distribution of products with high and low number of macthing product). This sort of cv split technique was implemented in a [kernel in previous competition](https:\/\/www.kaggle.com\/jakubwasikowski\/stratified-group-k-fold-cross-validation). Inspired by this sci-kit learn is also introducing this cv split technique in it's [coming release (0.24.2)](https:\/\/github.com\/scikit-learn\/scikit-learn\/pull\/18649). I have used the same code below to generate the splits.","fdb36edd":"<a id='visualise'><\/a>\n## Visualising different split techniques","82c34b8b":"## Contents\n1. [Why use stratified group k fold ?](#why_use)\n2. [Visualising different split techniques](#visualise)\n3. [StratifiedGroupKFold](#stratifiedgroupkfold)\n4. [Splitting data](#splitting)","88aa4d55":"<a id='why_use'><\/a>\n## Why use stratified group K fold ?\n\nA **stratified K fold** split will divide the dataset into test and train and will make sure that the distribution of labels in both test and train are *similar* (not *same* ). For example is the enitre training datasets is comprised of 40 % class 1 , 50 % class 2 and 10 % class 3 examples then the sample taken to train should also have 40 % class 1 , 50 % class 2 and 10 % class 3 examples. Another kind of cross validation split is called the **GroupKFold**. It is used when the data has underlying grouping. For example I have a product that is produced is batches, the products can be grouped by the batch id, I might need to create folds such that all the products of a batch are in one fold and do not overlap with any other fold. The difference between KFold, StratiifiedKFold and GroupKFold is visualised below.\n"}}