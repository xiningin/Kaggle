{"cell_type":{"f46ccb98":"code","bcfb9f25":"code","946246c4":"code","6d8fec9c":"code","8377bd94":"code","b79de2a9":"code","ea1acd90":"code","a1a2c1f0":"code","6a37edc9":"code","9f82cd73":"code","5d8b7e74":"code","c33fa391":"code","7d214d91":"code","0cd24235":"code","c4ef5845":"code","7e0f9f25":"code","59397c61":"code","1b0e2454":"code","b8c2528d":"code","0a9dd186":"code","c21a01e4":"code","3b3c0441":"code","dbaea49e":"code","a3bf6a3f":"code","30343952":"code","6248d2ae":"code","ac75283a":"code","b4c9deee":"code","72ad38d8":"code","703aebba":"code","abaa39cb":"code","03a2ed05":"code","00f400d0":"markdown","e1a86c73":"markdown","de32015d":"markdown","d9a5eab5":"markdown","486edfe2":"markdown","41046eb4":"markdown","4a0c4e49":"markdown","5b59bfe8":"markdown","43806184":"markdown","643f5cc3":"markdown","17e48600":"markdown","c20b28da":"markdown","9ebf5ff9":"markdown","d38fa73a":"markdown","6108932a":"markdown","7aaf299a":"markdown","d5d31ca5":"markdown","3c3b0ad2":"markdown","2c49582e":"markdown","7447573b":"markdown","34e63070":"markdown","c059e60f":"markdown","5c86c58c":"markdown","1cb190ce":"markdown","d3a56919":"markdown","6916186a":"markdown","49831bef":"markdown","c3970c34":"markdown","6110556f":"markdown","fdc222a9":"markdown","dedfb2de":"markdown"},"source":{"f46ccb98":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolours\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","bcfb9f25":"# Read the training data into a DataFrame\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain.head()","946246c4":"train.isna().any()","6d8fec9c":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest.isna().any()","8377bd94":"train.describe()","b79de2a9":"# Produces two stacked histogram plots, side-by-side, separating passengers by\n# survival\nfig = plt.figure(figsize=(15,5))\nax1 = fig.add_subplot(121); ax2 = fig.add_subplot(122)\n\nax1.hist((train.loc[train[\"Survived\"] == 0, \"Age\"].dropna(),train.loc[train[\"Survived\"] == 1, \"Age\"].dropna()),\n         bins = int(train[\"Age\"].max()-train[\"Age\"].min()), histtype = 'barstacked',\n        color=['darkred','green'], label=['Perished','Survived'], edgecolor='black')\n\nax1.set_xlabel('Age'); ax1.set_ylabel('Number of Occurances')\nax1.set_xlim(int(train[\"Age\"].min()), int(train[\"Age\"].max()))\nax1.legend(loc='upper right', frameon=False, fontsize=12)\n\nax2.hist([train.loc[train[\"Survived\"]==0,\"Fare\"].dropna(),train.loc[train[\"Survived\"]==1,\"Fare\"].dropna()], \n         bins = 50, color=['darkred','green'], histtype='barstacked',label=['Perished','Survived'], log=True,\n        edgecolor='black')\n\nax2.set_xlabel('Fare'); ax2.set_ylabel('Number of Occurances')\nax2.set_xlim(0.0,train[\"Fare\"].max()); ax2.set_ylim(1.0,train[\"Fare\"].max())\nax2.legend(loc='upper right', frameon=False, fontsize=12)\n\nplt.show()","ea1acd90":"# Organise data into survived and perished by feature\nsex_survived = [sum(train.loc[(train[\"Sex\"]==\"female\"),\"Survived\"]),\n            sum(train.loc[(train[\"Sex\"]==\"male\"),\"Survived\"])]\nsex_perished = [len(train.loc[train[\"Sex\"]==\"female\"]) - sex_survived[0], \n                              len(train.loc[train[\"Sex\"]==\"male\"]) - sex_survived[1]]\n\nclass_survived = [sum(train.loc[(train[\"Pclass\"]==1),\"Survived\"]),\n                  sum(train.loc[(train[\"Pclass\"]==2),\"Survived\"]),\n                  sum(train.loc[(train[\"Pclass\"]==3),\"Survived\"])]\nclass_perished = [len(train.loc[(train[\"Pclass\"]==1)]) - class_survived[0],\n                  len(train.loc[(train[\"Pclass\"]==2)]) - class_survived[1],\n                  len(train.loc[(train[\"Pclass\"]==3)]) - class_survived[2]]\n\nembarked_survived = [sum(train.loc[(train[\"Embarked\"]==\"S\"),\"Survived\"]),\n                    sum(train.loc[(train[\"Embarked\"]==\"Q\"),\"Survived\"]), \n                   sum(train.loc[(train[\"Embarked\"]==\"C\"),\"Survived\"]) ]\nembarked_perished = [len(train.loc[train[\"Embarked\"]==\"S\"]) - embarked_survived[0], \n                     len(train.loc[train[\"Embarked\"]==\"Q\"]) - embarked_survived[1],\n                     len(train.loc[train[\"Embarked\"]==\"C\"]) - embarked_survived[2]]\n\n# Plot three stacked graphs, side-by-side, which indicate survival based on\n# discrete features\nfig = plt.figure(figsize=(16,5))\nax1 = fig.add_subplot(131); ax2 = fig.add_subplot(132); ax3 = fig.add_subplot(133); \n\n# Gender survival\nax1.bar([0,1],sex_perished, color='darkred', label='Perished', edgecolor='black')\nax1.bar([0,1], sex_survived, color='green', label='Survived', bottom = sex_perished, edgecolor='black')\nax1.set_xticks(np.arange(2))\nax1.set_xticklabels((\"female\",\"male\"), fontsize=12)\nax1.set_ylabel(\"Number of Passengers\")\nax1.set_xlabel('Sex of Passenger')\n\n# Class survival\nax2.bar([0,1,2],class_perished, color='darkred', label='Perished', edgecolor='black')\nax2.bar([0,1,2], class_survived, color='green', label='Survived', bottom = class_perished, edgecolor='black')\nax2.set_xticks(np.arange(3))\nax2.set_xticklabels((\"First\",\"Second\", \"Third\"), fontsize=12)\nax2.set_xlabel('Class of Travel')\n\n# Embarked survival\nax3.bar([0,1,2],embarked_perished, color='darkred', label='Perished', edgecolor='black')\nax3.bar([0,1,2], embarked_survived, color='green', label='Survived', bottom = embarked_perished, edgecolor='black')\nax3.set_xticks(np.arange(3))\nax3.set_xticklabels((\"S\",\"Q\", \"C\"), fontsize=12)\nax3.set_xlabel('Place of Embarkment')\n\nplt.show()","a1a2c1f0":"# Separate survived and perished by sex\nfemale_survived = train.loc[(train['Sex']=='female') & (train['Survived']==1)]\nfemale_perished = train.loc[(train['Sex']=='female') & (train['Survived']==0)]\nmale_survived = train.loc[(train['Sex']=='male') & (train['Survived']==1)]\nmale_perished = train.loc[(train['Sex']=='male') & (train['Survived']==0)]\n\n# Bin the ages and fares. The ages are distributed fairly evenly, so we\n# choose even bin sizes. Because the distribution of fares is uneven, we\n# choose bins by quartiles.\ntrain[\"Fare Bin\"], fare_bins = pd.qcut(train[\"Fare\"], q=5, labels = [1,2,3,4,5], retbins = True)\ntrain[\"Age Bin\"], age_bins = pd.cut(train[\"Age\"], bins=8, labels = [1,2,3,4,5,6,7,8], retbins = True)\n\n# Calculate the widths and centres of the bins\nage_widths = age_bins[1:] - age_bins[:-1]\nage_bins = (age_bins[:-1] + age_bins[1:])\/2.\n\nfare_widths = fare_bins[1:] - fare_bins[:-1]\nfare_bins = (fare_bins[:-1] + fare_bins[1:])\/2.\n\n# Organise the histograms\nsurvived_age_bar = train.loc[train[\"Survived\"] == 1, \"Age Bin\"].dropna().value_counts().sort_index()\nperished_age_bar = train.loc[train[\"Survived\"] == 0, \"Age Bin\"].dropna().value_counts().sort_index()\n\nsurvived_fare_bar = train.loc[train[\"Survived\"] == 1, \"Fare Bin\"].dropna().value_counts().sort_index()\nperished_fare_bar = train.loc[train[\"Survived\"] == 0, \"Fare Bin\"].dropna().value_counts().sort_index()\n\n# Now make a histogram scatter plot. This largely follows the matplotlib tutorial.\nleft, width = 0.1, 0.7\nbottom, height = 0.1, 0.7\nspacing = 0.005\n\nfig = plt.figure(figsize=(8,8))\n\n# Set up axes\nax_scatter = plt.axes([left, bottom, width, height])\nax_scatter.tick_params(direction='in', which='both', top=True, right=True, labelsize=12)\nax_xbar = plt.axes([left, bottom + height + spacing, width, 0.2])\nax_xbar.tick_params(direction='in', which='both', labelbottom=False)\nax_ybar = plt.axes([left + width + spacing, bottom, 0.2, height])\nax_ybar.tick_params(direction='in', which='both',labelleft=False)\n\n# Plot the scatter data:\nax_scatter.semilogy(female_survived['Age'], female_survived['Fare'], marker='s',\n            markerfacecolor='None', markeredgecolor='green', ls='None', markersize=7)\nax_scatter.semilogy(female_perished['Age'], female_perished['Fare'], marker='s',\n          markerfacecolor='None', markeredgecolor='darkred', ls='None', markersize=7)\nax_scatter.semilogy(male_survived['Age'], male_survived['Fare'], marker='^',\n          markerfacecolor='None', markeredgecolor='green', ls='None', markersize=7)\nax_scatter.semilogy(male_perished['Age'], male_perished['Fare'], marker='^',\n          markerfacecolor='None', markeredgecolor='darkred', ls='None', markersize=7)\n\n# We plot these points off the scatter axis in order to plot a legend describing the\n# meaning of the marker shapes\nax_scatter.semilogy(-1,0.01, marker='^', markeredgecolor='black', markerfacecolor='None',\n                    ls='None',label='Male')\nax_scatter.semilogy(-1,0.01, marker='s', markeredgecolor='black', markerfacecolor='None',\n                    ls='None', label='Female')\n\n# Set the scatter axis limits\nax_scatter.set_xlim(0.0, 81)\nax_scatter.set_ylim(1.0, 1.5*train[\"Fare\"].max())\nax_scatter.set_xlabel('Age', fontsize=14); ax_scatter.set_ylabel('Fare', fontsize=14)\nax_scatter.legend(loc = 'lower right', fontsize=14, frameon=False)\n\n# Plot text to describe colours\nax_scatter.text(0.8,0.95,'Survived', color='green', transform=ax_scatter.transAxes, fontsize=14)\nax_scatter.text(0.8,0.9,'Perished', color='darkred', transform=ax_scatter.transAxes, fontsize=14)\n\n# Plot the ages histogram\nax_xbar.bar(age_bins,perished_age_bar, color='darkred', label='Perished', \n            width = age_widths, edgecolor='black')\nax_xbar.bar(age_bins,survived_age_bar, color='green', label='Survived',\n            bottom = perished_age_bar, width = age_widths, edgecolor='black')\nax_xbar.set_xlim(ax_scatter.get_xlim()); ax_xbar.axis('off')\n\n# Plot the fares histogram\nax_ybar.barh(fare_bins,perished_fare_bar,color='darkred', label='Perished', \n             edgecolor='black', align='center', height = fare_widths)\nax_ybar.barh(fare_bins,survived_fare_bar, color='green', label='Survived',\n            left = perished_fare_bar, edgecolor='black', align='center',\n              height = fare_widths)\nax_ybar.set_yscale('log'); \nax_ybar.set_ylim(ax_scatter.get_ylim()); ax_ybar.axis('off')\n\nplt.show()","6a37edc9":"# Define new family feature\ntrain[\"Family\"] = train[\"SibSp\"] + train[\"Parch\"]\n\nfig = plt.figure(figsize=(16,4))\nax1 = fig.add_subplot(131); ax2 = fig.add_subplot(132); ax3 = fig.add_subplot(133);\n\n# Parch survival\nax1.hist((train.loc[train[\"Survived\"] == 0, \"Parch\"].dropna(),train.loc[train[\"Survived\"] == 1, \"Parch\"].dropna()),\n         bins = int(train[\"Parch\"].max()-train[\"Parch\"].min()), histtype = 'barstacked',\n        color=['darkred','green'], label=['Perished','Survived'], log=True, edgecolor='black')\nax1.set_xlabel('Number of Travelling Parents\/Children')\nax1.set_ylabel('Number of Occurances')\n\n# SibSp survival\nax2.hist((train.loc[train[\"Survived\"] == 0, \"SibSp\"].dropna(),train.loc[train[\"Survived\"] == 1, \"SibSp\"].dropna()),\n         bins = int(train[\"SibSp\"].max()-train[\"SibSp\"].min()), histtype = 'barstacked',\n        color=['darkred','green'], label=['Perished','Survived'], log=True, edgecolor='black')\nax2.set_xlabel('Number of Travelling Siblings\/Spouses')\n\n# Family survival\nax3.hist((train.loc[train[\"Survived\"] == 0, \"Family\"].dropna(),train.loc[train[\"Survived\"] == 1, \"Family\"].dropna()),\n         bins = int(train[\"Family\"].max()-train[\"Family\"].min()), histtype = 'barstacked',\n        color=['darkred','green'], label=['Perished','Survived'], log=True, edgecolor='black')\nax3.set_xlabel('Number of Travelling Family Members')\n\nplt.show()","9f82cd73":"fig = plt.figure(figsize=(16,4))\nax1 = fig.add_subplot(131); ax2 = fig.add_subplot(132); ax3 = fig.add_subplot(133);\n\n# Family survival full\nax1.hist((train.loc[train[\"Survived\"] == 0, \"Family\"].dropna(),train.loc[train[\"Survived\"] == 1, \"Family\"].dropna()),\n         bins = int(train[\"Family\"].max()-train[\"Family\"].min()), histtype = 'barstacked',\n        color=['darkred','green'], label=['Perished','Survived'], edgecolor='black')\nax1.set_xlabel('Number of Travelling Family Members')\nax1.set_ylabel('Number of Occurances')\n\n# Family survival grouping 1\ntrain[\"Family Bins\"] = train[\"Family\"]\n\nmask = train[\"Family\"] == 0\ntrain.loc[mask, \"Family Bins\"] = 0\nmask = train[\"Family\"] ==1\ntrain.loc[mask, \"Family Bins\"] = 1\nmask = train[\"Family\"] == 2\ntrain.loc[mask,\"Family Bins\"] = 1\nmask = train[\"Family\"] == 3\ntrain.loc[mask,\"Family Bins\"] = 2\nmask = train[\"Family\"] > 3\ntrain.loc[mask, \"Family Bins\"] = 3\n\nfamily_bins_survived = [sum(train.loc[(train[\"Family Bins\"]==0),\"Survived\"]),\n                        sum(train.loc[(train[\"Family Bins\"]==1),\"Survived\"]),\n                        sum(train.loc[(train[\"Family Bins\"]==2),\"Survived\"]),\n                        sum(train.loc[(train[\"Family Bins\"]==3),\"Survived\"])]\n\nfamily_bins_perished = [len(train.loc[(train[\"Family Bins\"]==0)]) - family_bins_survived[0],\n                        len(train.loc[(train[\"Family Bins\"]==1)]) - family_bins_survived[1],\n                        len(train.loc[(train[\"Family Bins\"]==2)]) - family_bins_survived[2],\n                        len(train.loc[(train[\"Family Bins\"]==3)]) - family_bins_survived[3]]\n\nax2.bar([0,1,2,3],family_bins_perished,\n        color='darkred', label='Perished', edgecolor='black')\nax2.bar([0,1,2,3], family_bins_survived, color='green', \n        label='Survived', bottom = family_bins_perished, edgecolor='black')\n\nax2.set_xlabel('Size of Family ')\nax2.set_xticks(np.arange(4))\nax2.set_xticklabels(['Alone','Small','Medium','Large'])\n\n# Family survival group 2\ntrain[\"Family Bins\"] = train[\"Family\"]\n        \nmask = train[\"Family\"] > 4\ntrain.loc[mask, \"Family Bins\"] = 5\n\nfamily_bins_survived = [sum(train.loc[(train[\"Family Bins\"]==0),\"Survived\"]),\n                        sum(train.loc[(train[\"Family Bins\"]==1),\"Survived\"]),\n                        sum(train.loc[(train[\"Family Bins\"]==2),\"Survived\"]),\n                        sum(train.loc[(train[\"Family Bins\"]==3),\"Survived\"]),\n                        sum(train.loc[(train[\"Family Bins\"]==4),\"Survived\"]),\n                        sum(train.loc[(train[\"Family Bins\"]==5),\"Survived\"])]\n\nfamily_bins_perished = [len(train.loc[(train[\"Family Bins\"]==0)]) - family_bins_survived[0],\n                        len(train.loc[(train[\"Family Bins\"]==1)]) - family_bins_survived[1],\n                        len(train.loc[(train[\"Family Bins\"]==2)]) - family_bins_survived[2],\n                        len(train.loc[(train[\"Family Bins\"]==3)]) - family_bins_survived[3],\n                        len(train.loc[(train[\"Family Bins\"]==4)]) - family_bins_survived[4],\n                        len(train.loc[(train[\"Family Bins\"]==5)]) - family_bins_survived[5]]\n\nax3.bar(range(6),family_bins_perished,\n        color='darkred', label='Perished', edgecolor='black')\nax3.bar(range(6), family_bins_survived, color='green', \n        label='Survived', bottom = family_bins_perished, edgecolor='black')\n\nax3.set_xlabel('Size of Family')\nax3.set_xticks(np.arange(6))\nax3.set_xticklabels(['0','1','2','3','4', '>4'])\n\n\nplt.show()","5d8b7e74":"# Collect deck information from Cabin feature. Replace missing values with M.\ndecks = []\n\nfor cabin in train[\"Cabin\"]:\n    if pd.isnull(cabin):\n        decks.append('M')\n    else:\n        decks.append(cabin[0])\n                \ntrain[\"Deck\"] = decks\nprint(\"Full Deck Information:\")\nprint(train[\"Deck\"].value_counts())","c33fa391":"deck_survived = []; deck_perished = []\n\n# Separate passengers by survival\nfor deck in train[\"Deck\"].unique():\n    deck_survived.append(sum(train.loc[(train['Deck']==deck),\"Survived\"]))\n    deck_perished.append(len(train.loc[(train['Deck']==deck),\"Survived\"]) - deck_survived[-1])\n\n# First plot each different category. M stands for missing data.\nfig = plt.figure(figsize=(16,6))\nax1 = fig.add_subplot(111); \nax1.bar(range(len(train[\"Deck\"].unique())),deck_perished,\n        color='darkred', label='Perished', edgecolor='black')\nax1.bar(range(len(train[\"Deck\"].unique())),deck_survived,color='green', \n        label='Survived', bottom = deck_perished, edgecolor='black')\n\nax1.set_xlabel('Deck')\nax1.set_xticks(np.arange(len(train[\"Deck\"].unique())))\nax1.set_xticklabels(train[\"Deck\"].unique())\nax1.set_ylabel('Number of Occurances')\n\nplt.show()","7d214d91":"fig = plt.figure(figsize=(15,5))\nax1 = fig.add_subplot(131); ax2 = fig.add_subplot(132); ax3 = fig.add_subplot(133)\n\n# First group by those who have cabins and those who don't\n\ntrain[\"Has Cabin\"] = train[\"Cabin\"]\nmask = train[\"Deck\"] == 'M'\ntrain.loc[mask,\"Has Cabin\"] = 0\ntrain.loc[np.invert(mask), \"Has Cabin\"] = 1\n\ndeck_survived = []; deck_perished = []\n\nfor deck in range(2):\n    deck_survived.append(sum(train.loc[(train['Has Cabin']==deck),\"Survived\"]))\n    deck_perished.append(len(train.loc[(train['Has Cabin']==deck),\"Survived\"]) - deck_survived[-1])\n\nax1.bar(range(2),deck_perished,\n        color='darkred', label='Perished', edgecolor='black')\nax1.bar(range(2),deck_survived,color='green', \n        label='Survived', bottom = deck_perished, edgecolor='black')\n\nax1.set_xticks(np.arange(2))\nax1.set_xticklabels(['No Cabin', \"Cabin\"])\nax1.set_ylabel('Number of Occurances')\n\n# Next group by first, second and third class decks.\ndeck_mapping = {'A': 0, 'B': 0, 'C': 0, 'D': 1, 'E': 1, 'F': 2, 'G': 2, 'T':0, 'M':3}\ntrain[\"Deck Group 1\"] = train[\"Deck\"]\ntrain = train.replace({\"Deck Group 1\":deck_mapping})\n\ndeck_survived = []; deck_perished = []\n\nfor deck in range(4):\n    deck_survived.append(sum(train.loc[(train['Deck Group 1']==deck),\"Survived\"]))\n    deck_perished.append(len(train.loc[(train['Deck Group 1']==deck),\"Survived\"]) - deck_survived[-1])\n\n    \nax2.bar(range(4),deck_perished,\n        color='darkred', label='Perished', edgecolor='black')\nax2.bar(range(4),deck_survived,color='green', \n        label='Survived', bottom = deck_perished, edgecolor='black')\n\nax2.set_xlabel('Deck Class')\nax2.set_xticks(np.arange(4))\nax2.set_xticklabels(['Upper', 'Middle', 'Lower', 'Missing'])\n\n# Finally replace missing values with expected decks by passengers class\ndeck_mapping = {'A': 0, 'B': 1, 'C': 0, 'D': 1, 'E': 1, 'F': 1, 'G': 0, 'T':0, 'M':2}\ntrain[\"Deck Group 2\"] = train[\"Deck\"]\ntrain = train.replace({\"Deck Group 2\":deck_mapping})\n\ndeck_survived = []; deck_perished = []\n\nfor deck in range(3):\n    deck_survived.append(sum(train.loc[(train['Deck Group 2']==deck),\"Survived\"]))\n    deck_perished.append(len(train.loc[(train['Deck Group 2']==deck),\"Survived\"]) - deck_survived[-1])\n\n    \nax3.bar(range(3),deck_perished,\n        color='darkred', label='Perished', edgecolor='black')\nax3.bar(range(3),deck_survived,color='green', \n        label='Survived', bottom = deck_perished, edgecolor='black')\n\nax3.set_xlabel('Deck Class')\nax3.set_xticks(np.arange(3))\nax3.set_xticklabels(['ACGT', 'BDEF', 'M'])\n\nplt.show()","0cd24235":"names = train[\"Name\"]\ntitles = []\n        \n# Split name and find title\nfor name in names:\n    split_name = name.split(',')[1]\n    titles.append(split_name.split()[0][:-1])\n        \ntrain[\"Title\"] = titles\nunique = train[\"Title\"].unique()\n        \n# These are the most common titles, and therefore the ones we care most about\ntitles_mapping = {'Mr':'Mr', 'Mrs': 'Mrs', 'Miss':'Miss', 'Master': 'Master', \n                          'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss'}\n        \n# If the title is not one of the common ones, register it as rare\nfor title in unique:\n    if not title in titles_mapping:\n        titles_mapping[title] = 'Rare' \n        \n# Replace titles to simplify\ntrain = train.replace({\"Title\":titles_mapping})","c4ef5845":"title_survived = []; title_perished = []\n\n# Divide passengers into survived and perished\nfor title in train[\"Title\"].unique():\n    title_survived.append(sum(train.loc[(train['Title']==title),\"Survived\"]))\n    title_perished.append(len(train.loc[(train['Title']==title),\"Survived\"]) - title_survived[-1])\n\n# Plot survival by title\nfig = plt.figure(figsize=(8,5))\nax = fig.add_subplot(111)\n\nax.bar(range(5),title_perished,\n        color='darkred', label='Perished', edgecolor='black')\nax.bar(range(5),title_survived,color='green', \n        label='Survived', bottom = title_perished, edgecolor='black')\n\nax.set_xticks(np.arange(5))\nax.set_xticklabels(train[\"Title\"].unique())\nax.set_xlabel('Title'); ax.set_ylabel('Number of Occurances')\n\nplt.show()","7e0f9f25":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\n\n# Create a validation set\nsimple_train, simple_val = train_test_split(train, test_size=0.2, random_state=42)\n\n# Set aside labels\nsimple_train_labels = np.array(simple_train[\"Survived\"])\nsimple_val_labels = np.array(simple_val[\"Survived\"])\n\n\n# Sort out missing values\nnum_pipeline = Pipeline([('imputer',SimpleImputer(strategy=\"median\")),])\n\n# Define the full data cleaning pipeline\nfull_pipeline  = ColumnTransformer([(\"numerical\",num_pipeline,[\"Pclass\",\"Age\",\"Fare\"]),\n                                   (\"categorical\",OneHotEncoder(),[\"Sex\"]),])\n\n# Finally, clean the data\nsimple_train = full_pipeline.fit_transform(simple_train)\nsimple_val = full_pipeline.fit_transform(simple_val)\n","59397c61":"from sklearn import tree\n\n# Define simple tree and use training set to fit\nsimple_tree =  tree.DecisionTreeClassifier()\nsimple_tree.fit(simple_train,simple_train_labels)\n\n# Use validation set to get an idea of how good the model is\ntrain_score = simple_tree.score(simple_train, simple_train_labels)\nval_score = simple_tree.score(simple_val,simple_val_labels)\nprint('% correct for training set: ', round(100.*train_score,1))\nprint('% correct for validation set: ', round(100.*val_score,1))","1b0e2454":"# Train a simplified model\nsimple_tree_depth = tree.DecisionTreeClassifier(max_depth=5)\nsimple_tree_depth.fit(simple_train, simple_train_labels)\n\n# Use validation set to get an idea of how good the model is\ntrain_score = simple_tree_depth.score(simple_train, simple_train_labels)\nval_score = simple_tree_depth.score(simple_val,simple_val_labels)\nprint('% correct for training set: ', round(100.*train_score,1))\nprint('% correct for validation set: ', round(100.*val_score,1))","b8c2528d":"from sklearn.model_selection import cross_val_score\n\n# Use cross validation to see how well it does\nscores = cross_val_score(simple_tree_depth, simple_train, simple_train_labels, cv=5)\nprint('Mean Score (% correct): ', round(100.*scores.mean(),1))\nprint('Standard Deviation (% correct): ',round(100.*scores.std(),1))\n","0a9dd186":"from sklearn.ensemble import RandomForestClassifier\n\nrnd_tree = RandomForestClassifier(n_estimators=20, max_depth=5)\nscores = cross_val_score(rnd_tree, simple_train, simple_train_labels, cv=5)\n\nprint('Mean Score (% correct): ', round(100.*scores.mean(),1))\nprint('Standard Deviation (% correct): ',round(100.*scores.std(),1))","c21a01e4":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nscores = cross_val_score(knn, simple_train, simple_train_labels, cv=5)\n\nprint('Mean Score (% correct): ', round(100.*scores.mean(),1))\nprint('Standard Deviation (% correct): ',round(100.*scores.std(),1))","3b3c0441":"from sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\n\n# First we need the predictions from each model.\n# We use the validation set for this.\nsimple_tree_predictions = simple_tree_depth.predict(simple_val)\n\n# These models haven't been fitted yet, so we do that first\nrnd_tree.fit(simple_train, simple_train_labels)\nrnd_tree_predictions = rnd_tree.predict(simple_val)\n\nknn.fit(simple_train, simple_train_labels)\nknn_predictions = knn.predict(simple_val)\n\n# Calculate the confusion matrices \nsimple_tree_confusion = confusion_matrix(simple_val_labels,simple_tree_predictions)\nrnd_tree_confusion = confusion_matrix(simple_val_labels,rnd_tree_predictions)\nknn_confusion = confusion_matrix(simple_val_labels,knn_predictions)\n\n# Plotting function - tad ott but I'm a PhD students so pretty plots are\n# a source of pride for me...\ndef plot_confusion_comparison(predictions, titles):\n    \n    \"\"\"\n    Plots a set of confusion matrices.\n    \"\"\"\n    \n    plt_labels = [\"Perished\", \"Survived\"]\n    props = dict(boxstyle='round', facecolor='aliceblue', alpha=1.0, ec='aliceblue')\n    \n    fig = plt.figure(figsize=(len(predictions)*4,5))\n    plt_dim = int(100 + 10*len(predictions))\n    axes = []\n    \n    for iax in range(len(predictions)):\n        axes.append(fig.add_subplot(plt_dim+1+iax))\n\n    for iax,ax in enumerate(axes):\n\n        ax.matshow(predictions[iax], cmap='Blues')\n        ax.set_title(titles[iax])\n        ax.set_xticks(np.arange(2))\n        ax.set_xticklabels(plt_labels)\n        ax.xaxis.set_ticks_position('bottom');\n\n        if iax <1:\n            ax.set_yticks(np.arange(2))\n            ax.set_yticklabels(plt_labels, fontsize=12)\n        else:\n            ax.set_yticks([])\n\n        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\", fontsize=12)\n        for i in range(2):\n            for j in range(2):\n                ax.text(j, i, round(predictions[iax][j,i],3),\n                           ha=\"center\", va=\"center\", color=\"midnightblue\", bbox=props, fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n# Scale the matrices by the number of samples in each column and plot\npredictions = [simple_tree_confusion, rnd_tree_confusion, knn_confusion]\n\nfor ipred, pred in enumerate(predictions):\n    row_sum = pred.sum(axis=1, keepdims=True)\n    predictions[ipred] = predictions[ipred]\/row_sum\n    \ntitles = ['Decision Tree','Random Forest','K Nearest Neighbours']\n\nplot_confusion_comparison(predictions,titles)","dbaea49e":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\n\n# Now we need the probability predictions from each model. The 'score'\n# is the probability of picking survive\nsimple_tree_predictions = cross_val_predict(simple_tree_depth, simple_val, \n                                            simple_val_labels, cv=3,\n                                           method = \"predict_proba\")\nsimple_tree_scores = simple_tree_predictions[:,1] \n\n# These models haven't been fitted yet, so we do that first\nrnd_tree_predictions = cross_val_predict(rnd_tree, simple_val, \n                                            simple_val_labels, cv=3,\n                                           method = \"predict_proba\")\nrnd_tree_scores = rnd_tree_predictions[:,1]\n\nknn_predictions = cross_val_predict(knn, simple_val, \n                                            simple_val_labels, cv=3,\n                                           method = \"predict_proba\")\nknn_scores = knn_predictions[:,1]\n\n# Then we collect the true and false positive rates\ntree_fpr, tree_tpr, tree_thresholds = roc_curve(simple_val_labels,simple_tree_scores)\nrnd_tree_fpr, rnd_tree_tpr, rnd_thresholds = roc_curve(simple_val_labels,rnd_tree_scores)\nknn_fpr, knn_tpr, knn_thresholds = roc_curve(simple_val_labels,knn_scores)\n\n# Then we plot them, along with the random baseline\ndef plot_roc_comparison(predictions, labels):\n    \n    \"\"\"\n    Plots the ROC curves for a given set of predictions (max 5 reasonably),\n    against the random baseline.\n    \"\"\"\n    \n    c = ['cornflowerblue','mediumvioletred','seagreen','rosybrown', 'darkorange']\n    plt.figure(figsize=(6,6))\n    \n    for ipred, pred in enumerate(predictions):\n        plt.plot(pred[0], pred[1], label=labels[ipred], color=c[ipred%len(c)])\n\n    plt.plot([0,1],[0,1], label='Random Baseline', color='gray', ls='--')\n\n    plt.legend(frameon=False, loc='lower right', fontsize=12)\n    plt.xlabel('False Positive Rate', fontsize=12); plt.ylabel('True Positive Rate', fontsize=12)\n    plt.xlim(0.,1.); plt.ylim(0.,1.)\n    plt.tick_params(direction='in', which='both')\n    plt.show()\n\npredictions = [[tree_fpr, tree_tpr],[rnd_tree_fpr, rnd_tree_tpr],[knn_fpr, knn_tpr]]\nplot_roc_comparison(predictions, titles)\n\nprint(f'Decision Tree AUC Score: ', roc_auc_score(simple_val_labels, simple_tree_scores))\nprint(f'Random Forest AUC Score: ', roc_auc_score(simple_val_labels, rnd_tree_scores))\nprint(f'K Nearest Neighbours AUC Score: ', roc_auc_score(simple_val_labels, knn_scores))","a3bf6a3f":"# Define a new class to prepare the data.\n\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nclass PrepareJourneyData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self,):\n        pass\n    \n    def fit(self,X):\n        return self\n    \n    def transform(self,data):\n        \n        # Combine SibSp and Parch into Family\n        data[\"Family\"] = data[\"SibSp\"] + data[\"Parch\"]\n        data[\"Family Bins\"] = data[\"Family\"]\n        \n        # Bin family data into small, medium and large families\n        mask = data[\"Family\"] < 1\n        data.loc[mask, \"Family Bins\"] = 0\n        mask = data[\"Family\"] == 1\n        data.loc[mask, \"Family Bins\"] = 1\n        mask = data[\"Family\"] == 2\n        data.loc[mask, \"Family Bins\"] = 1\n        mask = data[\"Family\"] == 3\n        data.loc[mask, \"Family Bins\"] = 1\n        mask = data[\"Family\"] > 3\n        data.loc[mask,\"Family Bins\"] = 2\n        \n        # Replace any missing values in Embarked with the most common place of embarkment\n        data[\"Embarked\"] = data[\"Embarked\"].fillna(data[\"Embarked\"].mode(dropna=True)[0])\n        embarked_mapping = {'S':0, 'C':1,'Q':2}\n        data = data.replace({\"Embarked\":embarked_mapping})\n    \n        # Obtain deck information from cabin. Replace missing cabins with M\n        decks = []\n\n        for cabin in data[\"Cabin\"]:\n  \n            if pd.isnull(cabin):\n                decks.append('M')\n            else:\n                decks.append(cabin[0])\n        \n       \n        # Group decks into numerical values\n        data[\"Deck\"] = decks\n        deck_mapping = {'A': 0, 'B':0, 'C':0, 'D': 1,'E':1, 'F':2, 'G':2, 'T':0, 'M':3}\n        data = data.replace({\"Deck\":deck_mapping})\n        data = pd.get_dummies(data, columns = [\"Deck\"])\n        \n        # Replace missing fares with median from class of travel\n        classes = data[\"Pclass\"].unique()\n        for pclass in classes:\n            mask = data[\"Pclass\"] == pclass\n            data.loc[mask, \"Fare\"] = data.loc[mask,\"Fare\"].fillna(data.loc[mask,\"Fare\"].median())\n            data.loc[mask & (data[\"Fare\"]<1),\"Fare\"] = data.loc[mask,\"Fare\"].median()\n        \n        # Finally, bin the fare data by quartile\n        data[\"Fare Bin\"], bins = pd.qcut(data[\"Fare\"], q=8, labels = [0,1,2,3,4,5,6,7], retbins = True)\n        \n        # Drop features no longer needed. Only certain decks had any effect on the model,\n        # so I dropped other deck information\n        data = data.drop([\"SibSp\",\"Parch\",\"Fare\", \"Cabin\", \"Family\", \"Deck_2\"], axis=1)\n        \n        return data\n\n\nclass PreparePersonalData(BaseEstimator, TransformerMixin):\n    \n    def __init__(self,):\n        pass\n    \n    def fit(self, X):\n        return self\n    \n    def transform(self, data):\n        \n        # Convert Sex to numerical value\n        encoder = OrdinalEncoder()\n        data[\"Sex\"] = encoder.fit_transform(data[[\"Sex\"]])\n        \n        names = data[\"Name\"]\n        titles = []\n        \n        # This will split the title from the name. It works in all\n        # but one instance (the countess), however we'll deal with \n        # that in a minute\n        for name in names:\n            split_name = name.split(',')[1]\n            titles.append(split_name.split()[0][:-1])\n        \n        data[\"Title\"] = titles\n        unique = data[\"Title\"].unique()\n        \n        # These are the most common titles, and therefore the ones we care most about\n        titles_mapping = {'Mr':'Mr', 'Mrs': 'Mrs', 'Miss':'Miss', 'Master': 'Master', \n                          'Mlle': 'Miss', 'Mme': 'Mrs', 'Ms': 'Miss'}\n        \n        # If the title is not one of the common ones, register it as rare\n        for title in unique:\n            if not title in titles_mapping:\n                titles_mapping[title] = 'Rare'     \n\n        # Replace titles to simplify\n        data = data.replace({\"Title\":titles_mapping})\n        \n        # Passengers with missing ages can then have it assigned by their title, class and whether\n        # they were travelling with parent\/children\n        unique_titles = data[\"Title\"].unique()\n        unique_classes = data[\"Pclass\"].unique()\n        \n        for title in unique_titles:\n\n            for pclass in unique_classes:\n                \n                length_a = len(data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass) & (data[\"Parch\"]>0)])\n                length_b = len(data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass) & (data[\"Parch\"]==0)])\n                \n                if (length_a > 2 and length_b >2):\n                    mean_age = data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass) & (data[\"Parch\"]>0),\"Age\"].mean()\n                    data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass) & (data[\"Parch\"]>0),\"Age\"] = \\\n                            data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass) & (data[\"Parch\"]>0),\"Age\"].fillna(mean_age)\n\n                    mean_age = data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass) & (data[\"Parch\"]==0),\"Age\"].mean()\n                    data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass) & (data[\"Parch\"]==0),\"Age\"] = \\\n                                data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass) & (data[\"Parch\"]==0),\"Age\"].fillna(mean_age)\n                else:\n                    mean_age = data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass),\"Age\"].mean()\n                    data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass),\"Age\"] = \\\n                            data.loc[(data[\"Title\"]==title) & (data[\"Pclass\"]==pclass),\"Age\"].fillna(mean_age)\n\n\n        # Now bin ages\n        data[\"Age Bin\"] = pd.cut(data[\"Age\"], bins=7, labels = [0,1,2,3,4,5,6])\n\n        # Now encode titles\n        titles_mapping = {'Mr': 1, \"Mrs\": 2, \"Master\": 3, \"Miss\": 4, \"Rare\": 5}\n        data = data.replace({\"Title\":titles_mapping})\n        data = pd.get_dummies(data, columns = [\"Title\"])\n        \n        # Finally drop unused features\n        data = data.drop([\"Age\", \"Name\",\"Parch\",\"Pclass\"], axis = 1)\n\n        return data\n\n# Read in clean training dataset\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n# Define the full data cleaning pipeline\nfull_pipeline  = ColumnTransformer([(\"family\",PrepareJourneyData(),[\"Pclass\",\"SibSp\",\"Parch\", \"Fare\",\"Cabin\",\"Embarked\"]),\n                                   (\"personal\", PreparePersonalData(),[\"Sex\",\"Name\",\"Age\", \"Pclass\", \"Parch\"]),\n                                   ])\n\n\n# Set aside labels\ntrain_labels = np.array(train[\"Survived\"])\n\n# Finally, clean the data\nfull_train = full_pipeline.fit_transform(train)\n","30343952":"rnd_forest = RandomForestClassifier(random_state=42)\nscores = cross_val_score(rnd_forest, full_train, train_labels, cv=5)\n\nprint('Mean Score (% correct): ', round(100.*scores.mean(),1))\nprint('Standard Deviation (% correct): ',round(100.*scores.std(),1))","6248d2ae":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'max_depth': [8,12,None], 'max_features':[8,12,'auto'], \n               'n_estimators':[100,200,400], 'bootstrap': [True,False],\n              'min_samples_split': [2,10,50],'min_samples_leaf': [2,6,12]}]\n\ngrid_search = GridSearchCV(rnd_forest, param_grid, cv=3, scoring=\"accuracy\", return_train_score=True)\ngrid_search.fit(full_train, train_labels)\n\ncurves = grid_search.cv_results_\nprint(f'Highest Score: ', round(100.*max(curves[\"mean_test_score\"]),1), '%')\nprint(f'Corresponding Parameters: ', curves[\"params\"][np.argmax(curves[\"mean_test_score\"])])","ac75283a":"# Model with best combination of parameters\nrnd_forest = RandomForestClassifier(max_depth = 12, max_features=8,  n_estimators = 100, min_samples_leaf = 2, \n                                   min_samples_split=2, bootstrap = True, oob_score = True,random_state=42)\n","b4c9deee":"predictions = cross_val_predict(rnd_forest, full_train, \n                                            train_labels, cv=5,\n                                           method = \"predict_proba\")\nscores = predictions[:,1]\nfpr, tpr, rnd_thresholds = roc_curve(train_labels, scores)\n\nplot_roc_comparison([[fpr,tpr]], ['Random Forest'])","72ad38d8":"roc_auc_score(train_labels, scores)","703aebba":"predictions = cross_val_predict(rnd_forest, full_train, \n                                            train_labels, cv=3)\nrnd_tree_confusion = confusion_matrix(train_labels,predictions)\nrow_sum = rnd_tree_confusion.sum(axis=1, keepdims=True)\npredictions = rnd_tree_confusion\/row_sum\nplot_confusion_comparison([predictions], ['Random Forest'])","abaa39cb":"rnd_forest.fit(full_train, train_labels)\n\nfeatures = [\"Class\", \"Family_Bin\", \"Embarked\",\"ABCT\",\"DE\",\"M\", \"Fare Bin\",\n            \"Sex\",\"Age Bin\",'Mr',\"Mrs\",\"Master\",\"Miss\",\"Rare\"]\n\nfor feature, importance in zip(features, rnd_forest.feature_importances_):\n    print(feature,'\\t', round(100.*importance,2),'%')\n    \nprint(f'OOB Score: {rnd_forest.oob_score_}')","03a2ed05":"# Predict the survival and output to a submission file\ntest_data = full_pipeline.fit_transform(test)\n\npredictions =  rnd_forest.predict(test_data)\noutput = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predictions})\noutput.to_csv('titanic_submission.csv', index=False)\noutput.head()","00f400d0":"Whilst the 'Age' feature seems fairly evenly distributed (mean and median are close), the 'Fare' feature seems quite skewed. These will be easy to interpret if plotted. We can also start to get a feel for survival rates by colouring survived and perished passengers differently.","e1a86c73":"From this, the random forest looks like the best model (we should be a little wary of the AUC score though, due to how jagged the curve is). Before attempting to tune hyperparameters, let's see what the effect of adding features to the training dataset is. \n\n### A More Complex Model\nSo far we've only looked at the class, fare, age and gender of the passengers. First of all, we don't need a column per gender, as the only classifications are male or female, and hence one column of 1s or 0s should do. Secondly, we'll add in the features we looked at earlier, and bin features. I ended up using only certain decks, and found that 'Has Cabin' did more harm than good. All the feature engineering is done in the Pipelines below.","de32015d":"So now we have a model which performs better than the baseline. The next step is to see if there are any other models which perform fairly well. Having made a fairly successful decision tree, let's try a random forest.","d9a5eab5":"#### Training a Simple Model\nBefore picking a model to tune, we should see how models compare. The recommended model for this dataset is a Decision Tree, so let's start with that.","486edfe2":"The data shows that the number of accompanying family members has a correlation with survival. The pattern in each of the individual catgories (SibSp and Parch) is similar, and reflects well when these are combined into one feature, hence use of a single Family feature seems sensible. Family members can be further grouped into bins of family size","41046eb4":"That's a pretty bad score compared to the other models, and only slightly better than the random baseline, so this may not be a very good model. To get a better idea, let's look at the confusion matricies.","4a0c4e49":"### Exploring the Titanic Data\n\nTo start we load the dataset, and see what features it has.","5b59bfe8":"Larger family sizes have far fewer samples, hence binning these into one group could help with training. Choosing the best grouping is done using trial and error - grouping into four sizes gives a fair number of samples in each group. \n\nThe next feature to add is the deck information for the passenger. I've seen others replace any missing decks with M, so we'll start with that.","43806184":"Grouping by whether cabins were available or not gives a large number of samples in each category, and shows a clear trend in survival. Overall, having two features describing whether the passenger had cabin information, and one which desribes their deck, grouped by survival, seems reasonable.\n\nFinally we look at survival by title. ","643f5cc3":"The title of a passenger seems to be a good feature, and also could be useful when calculating missing ages. I've seen a few notebooks which mention that being a 'Miss' and travelling with parents changes the mean age dramatically compared to not travelling with parents. Later, we'll make use of this.","17e48600":"This AUC score is lower than before, but the plot is much smoother. Another measure is the Confusion matrix.","c20b28da":"The final model we'll try is a K nearest neighbour classifier. Looking at the plot of survivors with fare and age, there does appear to be some (perhaps fairly loose) groupings, so this classifier may be reasonable.","9ebf5ff9":"We can now test how good this model is, using the ROC curve and confusion matrix as before.","d38fa73a":"Both fare and age seem to play into the probability of survival. The distribution of ages seems fairly even, with a small spike near 0. This means replacing missing values with either the mean or median should be sensible. One the other hand, fares has a very skewed distribution (also note the log y-axis).\n\nNext we will look at survival by other features. ","6108932a":"That's definitely a huge improvement, and confirms that the model was badly overfitting. It doesn't look like its overfitting too much now, so let's use cross validation to check the validation score more robustly. ","7aaf299a":"Now that we have a pipeline which cleans up and organises the dataset for us, we are now in a position to start tuning the random forest model!\n\n### Tuning the Model\nWe start by seeing how the model performs now that it has more features.","d5d31ca5":"The validation score is pretty bad, expecially when you consider that, if we just predicted that everyone died, we'd be right 62% (this is our baseline). As the training score is super high, but the validation score is low, we can predict that the model is overfitting badly. We can try to sort this by reducing the complexity of the model, by setting a maximum depth to the tree. As there are 5 features (if each of the one-hot encoding features are assumed to be individual), let's set the max_depth at 5 for now.","3c3b0ad2":"The majority of this data is missing, which means this may not be a good feature to use (or perhaps the fact that they couldn't recover the cabin information for some passengers says something about the survival of that group). First, let's take a look at how the deck of travel affects survival.","2c49582e":"The data for each passenger includes the class they were travelling in (Pclass), the name, sex, age, fare, as well as where they got on the ship (Embarked), the cabin they travelled in, how many siblings\/spouses they were travelling with (SibSp) and how many parents\/children they were travelling with (Parch). Survival is the label here: it indicates whether the passenger survived (1) or perished at sea (0). \n\nAt first glance, we can see that the Cabin feature has some missing values. It is probably wise then to see if any other features are missing values.","7447573b":"So the 'Age', 'Cabin' and 'Embarked' features have missing values within the training set. Within the test dataset:","34e63070":"Finally, we make predictions to be submitted.","c059e60f":"From these plots, we conclude:\n* Sex is a good indicator of survival.\n* The age of the passenger is an indicator of survival. Ages are fairly evenly distributed, therefore binning them with equal width bins seems sensible. Missing ages can be filled with either the median or the mean, because these are similar.\n* The fare the passenger paid is also an indicator of survival. Fares are highly skewed, with a few very large fares acting as outliers. Because of this, the mean would not be a representative measure to fill missing values. The wide range also means that binning by equal widths is not sensible, so instead when we bin these, it will make more sense to use quartiles. \n* The cabin feature itself may not be a useful feature, however extracting decks and creating a new feature may be useful.\n\n### Feature Engineering\n\nWe now look to make use of other features. We'll start with the 'SibSp' and 'Parch' features. These can be combined to form a new 'Family Size' feature. First we'll compare survival for each individual feature.\n","5c86c58c":"## The Titanic Dataset\n\nThis notebook details my attempt to get to grips with the titanic dataset. Being new to machine learning, I was trying out some methods and plots I'd seen. Overall, I only managed 77% accuracy, but I learnt a lot about pandas and sklearn. This notebook might also be helpful to others who are new to pandas.","1cb190ce":"In general, having a deck seems to increase a passenger's chance of survival. As the number of samples per deck is low, we'll try a few groupings to see if there are any general pattern. First we'll group by if cabin information was available, second we'll group by how close decks were to the lifeboats, and finally we'll group by chance of survival (BDEF had higher chances of survival).","d3a56919":"This shows an improvement in predictions for survival, which is promising. We can also look at how much each feature mattered.","6916186a":"This is about the same accuracy as before, however, we haven't attempted hyperparameter tuning yet. We can use GridSearch to scan through some different combinations. This may take a while to run, so I have typed out the final model with optimum parameters below.","49831bef":"The K Nearest Neighbours model definitely performs the worst, therefore we'll rule that one out. The random forest seems to perform better than the decision tree, however not by much, so with a bit of tuning the decision tree may be enough. In general, both tree models seem to do a better job at predicting those who die than survive, though this could be because more people died than survived (so there is more data available). My takeaway from the confusion matrix is that the model might need more information on survivors, to better classify them, so adding in some of the details in the dataset we ignored for simplicity may be required.\n\nBecause we may want a simple way to measure the success of each model (for example, a numerical score per model), let's look at the ROC curves (which I recently learned about, so really I just want to play about with my new knowledge).","c3970c34":"The test dataset has missing values for 'Age', 'Cabin' and 'Fare', which differs from the training dataset. This will need to be taken into account when writing data processing pipelines later. This is the last time we look at the test dataset, before the predictions.\n\nWe can get a feel for a good measure to fill in these missing values by looking at the distributions of the features.","6110556f":"Female passengers seem to have a better chance of survival compared to male passengers, which agrees with my (limited) knowledge of the Titanic that women and children were prioritised for life boats. Class of travel also has seems to be related to chance of survival. Third class passengers seem to have been least likely to survive, whilst first class seem to have been the most likely. This could be related to the decks in which their cabins were located (and therefore how easy it was for them to make it to the lifeboats). From a schematic of the titanic, it seems that the deck the passenger was staying in is given by the first letter of the Cabin feature. Extracting these decks may prove helpful later. The place of embarkment is related to survival, which again may be due to how cabins were allocated.\n\nFinally, we'll make an attempt to visualise the influence of gender, age and fare on the likelihood of survival (mostly because I wanted to try to make a histogram-scatter plot). To do this, we'll distinguish gender by the shape of the marker in the scatter plot, and we'll use the red\/green colour scheme to indicate survived and perished. Because of the wide range of ages and fares, it may make sense later to bin these rather than supply them raw, so we'll use the histogram aspect of this plot to try to find a good way to do that.","fdc222a9":"### A Simple Model\n\n#### Making a Simple Dataset\nBecause I am new to this, I decided to start slow and take a look at the abilities of a simple model with only a few features. The features I chose to do this were class of travel, age, fare, and sex. I replaced missing values with the median for simplicity, and defined a training and validation set, so I could get a good measure of how well each model worked.","dedfb2de":"So that was my first attempt at pandas, sklearn, feature engineering and random forest classifiers. I've seen others do much better with similar features and methods, so I'm not too sure where I've lost out on accuracy. I also found that choosing bins, which is mostly done with trial and error, has a pretty large effect. One of the features I struggled with was the deck. In general, when separated, only decks DE had non-zero importance for the classifier, but grouping decks in different ways lead to some very different scores. I'm still not sure if the one I picked at the end was the best. Writing some form of grid search function, but for binning of fares, family size, ages and decks may prove beneficial, but I'll leave that for now."}}