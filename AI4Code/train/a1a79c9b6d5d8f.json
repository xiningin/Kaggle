{"cell_type":{"17c420d1":"code","a63e88ef":"code","76eb24aa":"code","427f036c":"code","9de9203f":"code","0d1bf2fb":"code","c4bff5dd":"code","ab0b9726":"code","255807f5":"code","15da225c":"code","9abd4a5e":"code","6aa4d9e6":"code","7ef92185":"code","2ce5f41f":"code","5612f12f":"markdown","d7dd5e71":"markdown","6b9cf6d7":"markdown","b04f7a2a":"markdown","0116e80d":"markdown","428694c2":"markdown","02318d31":"markdown","caaf36e2":"markdown","a15eae18":"markdown","9bb2ffcd":"markdown","81012cd1":"markdown","26546d14":"markdown","dd912fb0":"markdown","433aa85c":"markdown","b1a5219f":"markdown","8fe038c1":"markdown","2bc295b6":"markdown","8f9019ce":"markdown"},"source":{"17c420d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\nfrom sklearn.model_selection import train_test_split # Training\/Testing set split for performance measures\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree # Rule based classifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix # Performance measures\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a63e88ef":"# importing data\ndataset = pd.read_csv(\"..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv\")\ndataset.info()","76eb24aa":"dataset.head()","427f036c":"dataset = dataset.drop(['gameId', 'redFirstBlood', 'redGoldDiff', 'redExperienceDiff'], axis=1) #Redundant Columns","9de9203f":"dataset.describe()","0d1bf2fb":"palette=sns.color_palette(['r', 'b'])\nax1 = sns.countplot(dataset.blueWins, palette=palette)\nax1.set(xticks=[0, 1], xticklabels=['Red Wins', 'Blue Wins'])\nax1.set_title('Blue vs Red Wins')\nax1.set_xlabel('')","c4bff5dd":"fig, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(dataset.corr(), ax=ax, cmap='seismic_r')","ab0b9726":"def top_n_correlations(dataframe, n):\n    \n    correlations = dataframe.corr().unstack()\n    correlations = correlations['blueWins'].abs().sort_values(kind='quicksort', ascending=False)\n    \n    if not n:\n        return correlations\n    \n    return correlations[0:n]","255807f5":"correlations = top_n_correlations(dataset, 11).drop('blueWins')\ncorrelations","15da225c":"grid = sns.PairGrid(data = dataset, vars=['blueGoldDiff', 'blueExperienceDiff', 'blueGoldPerMin', 'blueTotalGold', 'blueTotalExperience'], hue='blueWins', palette=palette, hue_kws={\"marker\": [\"D\", \"o\"], \"alpha\": [0.3, 0.3]})\ngrid.map_diag(plt.hist)\ngrid.map_offdiag(plt.scatter)\ngrid.add_legend()\nplt.show()","9abd4a5e":"X_train, X_test, y_train, y_test = train_test_split(dataset.drop(['blueWins'], axis=1), dataset['blueWins'], test_size=0.33, random_state=0)\nprint('Training examples: ', X_train.shape[0])\nprint('Testing examples: ',X_test.shape[0])","6aa4d9e6":"def test_classifier(classifier, X_train, X_test, y_train, y_test):\n    \n    classifier.fit(X_train, y_train)\n    y_pred = classifier.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    cm = confusion_matrix(y_test, y_pred)\n    \n    return f'{round(accuracy,2)*100}%', cm","7ef92185":"classifier = DecisionTreeClassifier(max_depth=3)\naccuracy, cm = test_classifier(classifier, X_train, X_test, y_train, y_test)\nprint(\"Decision Tree's Accuracy: \", accuracy)\nplt.figure(figsize=(8,8))\nax = sns.heatmap(cm, annot=True, cmap='seismic_r', fmt='g')\nax.set_title(\"Model's Confusion Matrix\")\nax.set_ylabel('Actual')\nax.set_xlabel('Predicted')","2ce5f41f":"fig, ax = plt.subplots(figsize=(20,10))\nplot_tree(classifier, feature_names=dataset.columns[1:], class_names=['redWins', 'blueWins'], precision=1, filled=True, ax=ax)\nplt.show()","5612f12f":"# More Preprocessing\nFor the modeling process, we need to measure the performance of our model by testing it on data it has never seen before, and for that, we're going to split the data into a training set and a test set","d7dd5e71":"This description shows us that the features are on different scales, and if we're going to use an algorithm that needs feature scaling, then feature scaling for this dataset is necessary.","6b9cf6d7":"It turns out that the number of True Positives (Actual 1, Predicted 1) and the number of True Negatives (Actual 0, Predicted 0) are close to each other, also the number of False Positives and False Negatives) which is another evidence on the dataset's balance, and a good accuracy metric to rely on when the dataset is unbalanced.","b04f7a2a":"It turns out that the features are well correlated (not linearly) with each other, also, in the case of red team winning, the stats of the blue team are way behind, and smaller values of features as blueGoldDiff, blueExperienceDiff, etc are more frequent, also we've noticed that blueGoldPerMin and blueTotalGold have almost linear correlation,, which is quite sensible.","0116e80d":"# Dataset Description\nIn this section we're going to view a description of the dataset and some statistics on the continuous variables, including minimum and maximum values, mean, std, this description is very helpful if we're going to work on an algorithm that needs feature scaling, and helps us determine if the dataset needs to be scaled or not ","428694c2":"It turns out that gold difference and experience difference are the most correlated features with the probability of whether the blue team wins or not, which means that they contribute the most in the winning process","02318d31":"# What we've learned from this dataset\n\n* Gold Difference and Experience Difference play the most important roles in deciding what team will win a game.\n* The KDA (Kills\/Deaths\/Assists) are not the only determinants as obtaining gold and experience can be gained by killing minions and neutral monsters.\n* It's not very important to get the Kill if you can get an Assist, it all contributes to the whole Gold & Experience Difference for the team.\n* The early game phase can predict the outcome of the whole game by 72% accuracy, so it's very important.\n* Although they give important advantages, it's not that important to go for an elite monster (Dragon, Herald) if there's a risk of dying to get it.","caaf36e2":"# Data Analysis and Preprocessing\nFirst we're going to import the dataset and view its information to better understand the features and their datatypes, and also check for null values","a15eae18":"# Dataset Correlations\nCorrelation means association - more precisely it is a measure of the extent to which two variables are related. ... Therefore, when one variable increases as the other variable increases, or one variable decreases while the other decreases.\nIn this section, we're going to measure correlations between features of the dataset themselves, and between features of the dataset and the output, to see how much, for example: increasing blueKills contributes to the whole winning process.","9bb2ffcd":"# Removing Redundant Data\nThere are some features that we're not going to use, and some other features that are considered redundant, such as:\n* gameId: A unique key to identify each game, but unnecessary in analysis and modeling\n* redFirstBlood: The opposite of blueFirstBlood, so if blueFirstBlood is 0, we already know that the red team has drawn first blood!\n* redGoldDiff & redExperienceDiff: The negative of blueGoldDiff & blueExperienceDiff, so if blue gold\/experience difference is negative, we already know that the blue team is behind on gold\/experience.","81012cd1":"# Importing Libraries\nIn this section we import different libraries for Analysis, Visualization, Modeling and Evaluation","26546d14":"# Decision Tree Visualization\nSometimes it's very useful to visualize the decision tree that was built, to drive more insight of the rules that were divided from the dataset","dd912fb0":"It turns out there are a lot of features for each game, indicating the stats of the Red and the Blue teams, and a column that indicates if the Blue team wins (1) or not (0)","433aa85c":"# League of Legends Ranked Game Analysis\nLeague of Legends (LoL) is a multiplayer online battle arena video game developed and published by Riot Games for Microsoft Windows and macOS, in which players assume the role of a \"champion\" with unique abilities, varying around their class, and battle against a team of other player- or computer-controlled champions..\n\n![League of Legends](https:\/\/www.gamersdecide.com\/sites\/default\/files\/styles\/news_images\/public\/content-images\/news\/2016\/10\/15\/league-legends-top-ten-wallpapers\/hc6k-custom.jpg)\n\nIn the main game mode, Summoner's Rift, the goal is to destroy the opposing team's Nexus, a structure that lies at the heart of their base, protected by defensive structures. \n\n**Summoner's Rift** is the flagship game mode of League of Legends. When the mainstream press covers the game, it is Summoner's Rift that they refer to. On this map, two teams of five players compete to destroy an enemy structure called a Nexus, which is guarded by the enemy team and a number of defensive structures called turrets, or towers. Each team aims to defend its own structures and destroy the other team's structures. There are six turrets on each lane, three per team, and destroying one generates gold for the team that does so. Two additional turrets protect each team's nexus.\n\n![Summoner's Rift](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/dc\/Map_of_MOBA.svg\/240px-Map_of_MOBA.svg.png)\n\nEach nexus is located in each enemy base on opposite sides of the map, in the lower-left and upper-right hand corners. Minions are generated in waves from each team's nexus, which advance toward the enemy base along three paths: top, middle, and bottom lanes. Players compete to advance these waves of minions into the enemy base, which allows them to destroy enemy structures and ultimately win the match. Summoner's Rift matches typically last between 30\u201340 minutes if played until nexus destruction, but an early surrender functionality is available in the game.\n\n**Essential Game Elements**\n\n* Champions: are the player-controlled character in League of Legends icon League of Legends. Each champion possesses unique abilities and attributes.\n* Minions: are units that comprise the main force sent by the Nexus. They spawn periodically from their nexus and advance along a lane towards the enemy nexus, automatically engaging any enemy unit or structure they encounter. They are controlled by artificial intelligence, and only use basic attacks.\n* Monsters: are neutral units in League of Legends. Unlike minions, monsters do not fight for either team, but when killed, can give a team an advantage over the other.\n* A ward: is a deployable unit that removes the fog of war over the surrounding area.\n* Turrets: also called towers, are heavy fortifications that attack enemy units on sight. Turrets are a core component of League of Legends. They deal damage to enemies and provide vision to their team, allowing them to better control the battlefield.\n* Gold: is the in-game currency of League of Legends. It is used to buy items in the shop that provide champions with bonus stats and abilities, which in turn is one of the main ways for champions to increase their power over the course of a game.\n* Champion experience (XP): is a game mechanic that allows champions to level up after reaching certain amounts of experience. Leveling up allows them access to new abilities or higher ranks of existing abilities.\n\n# This Dataset\n\nThis dataset contains the first 10min. stats of approx. 10k ranked games (SOLO QUEUE) from a high ELO (DIAMOND I to MASTER). Players have roughly the same level, our goal is to do some analysis on the importance of each element in winning a game, and also develop a model that predicts which team will win.","b1a5219f":"# Decision Tree Analysis\n\nvia: https:\/\/www.hackerearth.com\/practice\/machine-learning\/machine-learning-algorithms\/ml-decision-tree\/tutorial\/\n\nDecision Tree Analysis is a general, predictive modelling tool that has applications spanning a number of different areas. In general, decision trees are constructed via an algorithmic approach that identifies ways to split a data set based on different conditions. It is one of the most widely used and practical methods for supervised learning. Decision Trees are a non-parametric supervised learning method used for both classification and regression tasks. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n\n![](https:\/\/s3-ap-southeast-1.amazonaws.com\/he-public-data\/Fig%201-18e1a01b.png)\n\nThe decision rules are generally in form of if-then-else statements. The deeper the tree, the more complex the rules and fitter the model, but it can also, if it goes very deep, overfit the data.\n\nThe basic algorithm used in decision trees is known as the ID3 (by Quinlan) algorithm. The ID3 algorithm builds decision trees using a top-down, greedy approach. Briefly, the steps to the algorithm are:\n- Select the best attribute \u2192 A \n- Assign A as the decision attribute (test case) for the NODE. \n- For each value of A, create a new descendant of the NODE.\n- Sort the training examples to the appropriate descendant node leaf.\n- If examples are perfectly classified, then STOP else iterate over the new leaf nodes.\n\nThe importance of features is calculated using Gini Impurity or Entropy:\n\n**The Entropy** or information gain is calculated by this formula:\n![](https:\/\/miro.medium.com\/max\/442\/1*efLrD1ECWl-utII0KYb7tQ.jpeg)\n\n**The Gini Impurity** is calculated by this formula:\n![](https:\/\/miro.medium.com\/max\/442\/1*vRlwRFknvfgWLBed1vsGoQ.jpeg)\nWhere p is the probability of the class in a given feature\n\n**For this problem** We're going to use the gini score as it is calculated with less computational cost.","8fe038c1":"# Dataset Balance\nA very important task in a classification problem, is to check if the dataset is balanced or not, fortunately it turns out the dataset is perfectly balanced","2bc295b6":"# Model Performance\nWe've chosen a decision tree classifier with max depth of 3, because more depth will lead to overfitting and definitely worse performance on the test set\nThe model has performed 72% accuracy on the test set, we've also plotted a confusion matrix that shows both the number of correctly classified and the missclassified examples in terms of (True Positives, True Negatives, False Positives and False Negatives). ","8f9019ce":"# More on Dataset Correlations\nIn this section we're going to plot the most correlated features with each other"}}