{"cell_type":{"7df0224c":"code","837d830f":"code","b081c141":"code","5bbaa3f7":"code","1e5ebf9f":"code","4f7a710e":"code","e93bdf09":"code","8faece4a":"code","93cd3c41":"code","4241d1b8":"code","c4ce1ad0":"markdown","267ccd1d":"markdown","134d7c78":"markdown","a590b257":"markdown","63b1a024":"markdown"},"source":{"7df0224c":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport itertools\nfrom arch import arch_model\nfrom scipy.stats import shapiro\nfrom scipy.stats import probplot\nfrom statsmodels.stats.diagnostic import het_arch\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.stats.diagnostic import acorr_ljungbox\n\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight') \nplt.rcParams['xtick.labelsize'] = 10\nplt.rcParams['ytick.labelsize'] = 10\n%matplotlib inline","837d830f":"path = '..\/input\/individual_stocks_5yr\/individual_stocks_5yr'\ncsvs = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.csv')]\n\ndf = pd.DataFrame()\nfor file in random.sample(range(1, len(csvs)), 8):\n    stock_df = pd.read_csv(csvs[file])\n    stock_df.index = pd.DatetimeIndex(stock_df.date)\n    name = stock_df['Name'].iloc[0]\n    df[name] = stock_df['close']\n\ndf.plot(figsize=(10, 5), title='Closing Price for 8 Random Stocks')\ndf.head()","b081c141":"stock = 'TDG'\ndf = pd.read_csv(f'..\/input\/individual_stocks_5yr\/individual_stocks_5yr\/{stock}_data.csv')\ndf.index = pd.DatetimeIndex(df.date)\ndf = df.drop(columns=['open', 'high', 'low', 'volume', 'date', 'Name'])\ndf['pct_change'] = 100*df['close'].pct_change()\ndf.dropna(inplace=True)\ndf['close'].plot(figsize=(10, 5), title=f'{stock} Closing Price 2013-2018')\nplt.show()\ndf['pct_change'].plot(figsize=(10, 5), title=f'{stock} Percent Change in Closing Price')\nplt.show()\nacf = plot_acf(df['pct_change'], lags=30)\npacf = plot_pacf(df['pct_change'], lags=30)\nacf.suptitle(f'{stock} Percent Change Autocorrelation and Partial Autocorrelation', fontsize=20)\nacf.set_figheight(5)\nacf.set_figwidth(15)\npacf.set_figheight(5)\npacf.set_figwidth(15)\nplt.show()","5bbaa3f7":"ljung_res = acorr_ljungbox(df['pct_change'], lags= 40, boxpierce=True)\nprint(f'Ljung-Box p-values: {ljung_res[1]}')\nprint(f'\\nBox-Pierce p-values: {ljung_res[3]}')","1e5ebf9f":"def ts_plot(residuals, stan_residuals, lags=50):\n    residuals.plot(title='GARCH Residuals', figsize=(15, 10))\n    plt.show()\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n    ax[0].set_title('GARCH Standardized Residuals KDE')\n    ax[1].set_title('GARCH Standardized Resduals Probability Plot')    \n    residuals.plot(kind='kde', ax=ax[0])\n    probplot(stan_residuals, dist='norm', plot=ax[1])\n    plt.show()\n    acf = plot_acf(stan_residuals, lags=lags)\n    pacf = plot_pacf(stan_residuals, lags=lags)\n    acf.suptitle('GARCH Model Standardized Residual Autocorrelation', fontsize=20)\n    acf.set_figheight(5)\n    acf.set_figwidth(15)\n    pacf.set_figheight(5)\n    pacf.set_figwidth(15)\n    plt.show()","4f7a710e":"garch = arch_model(df['pct_change'], vol='GARCH', p=1, q=1, dist='normal')\nfgarch = garch.fit(disp='off') \nresid = fgarch.resid\nst_resid = np.divide(resid, fgarch.conditional_volatility)\nts_plot(resid, st_resid)\nfgarch.summary()","e93bdf09":"arch_test = het_arch(resid, maxlag=50)\nshapiro_test = shapiro(st_resid)\n\nprint(f'Lagrange mulitplier p-value: {arch_test[1]}')\nprint(f'F test p-value: {arch_test[3]}')\nprint(f'Shapiro-Wilks p-value: {shapiro_test[1]}')","8faece4a":"def gridsearch(data, p_rng, q_rng):\n    top_score, top_results = float('inf'), None\n    top_models = []\n    for p in p_rng:\n        for q in q_rng:\n            try:\n                model = arch_model(data, vol='GARCH', p=p, q=q, dist='normal')\n                model_fit = model.fit(disp='off')\n                resid = model_fit.resid\n                st_resid = np.divide(resid, model_fit.conditional_volatility)\n                results = evaluate_model(resid, st_resid)\n                results['AIC'] = model_fit.aic\n                results['params']['p'] = p\n                results['params']['q'] = q\n                if results['AIC'] < top_score: \n                    top_score = results['AIC']\n                    top_results = results\n                elif results['LM_pvalue'][1] is False:\n                    top_models.append(results)\n            except:\n                continue\n    top_models.append(top_results)\n    return top_models\n                \ndef evaluate_model(residuals, st_residuals, lags=50):\n    results = {\n        'LM_pvalue': None,\n        'F_pvalue': None,\n        'SW_pvalue': None,\n        'AIC': None,\n        'params': {'p': None, 'q': None}\n    }\n    arch_test = het_arch(residuals, maxlag=lags)\n    shap_test = shapiro(st_residuals)\n    # We want falsey values for each of these hypothesis tests\n    results['LM_pvalue'] = [arch_test[1], arch_test[1] < .05]\n    results['F_pvalue'] = [arch_test[3], arch_test[3] < .05]\n    results['SW_pvalue'] = [shap_test[1], shap_test[1] < .05]\n    return results","93cd3c41":"p_rng = range(0,30)\nq_rng = range(0,40)\ndf['dif_pct_change'] = df['pct_change'].diff()\ntop_models = gridsearch(df['dif_pct_change'], p_rng, q_rng)\nprint(top_models)","4241d1b8":"garch = arch_model(df['pct_change'], vol='GARCH', p=17, q=25, dist='normal')\nfgarch = garch.fit(disp='off') \nresid = fgarch.resid\nst_resid = np.divide(resid, fgarch.conditional_volatility)\nts_plot(resid, st_resid)\narch_test = het_arch(resid, maxlag=50)\nshapiro_test = shapiro(st_resid)\nprint(f'Lagrange mulitplier p-value: {arch_test[1]}')\nprint(f'F test p-value: {arch_test[3]}')\nprint(f'Shapiro-Wilks p-value: {shapiro_test[1]}')\nfgarch.summary()","c4ce1ad0":"<a id='garch'><\/a>\n## GARCH Results\n---\nHere are the results of our GARCH(1,1) model: \n\nOmega is the baseline variance for the model, so the square root of omega is the standard deviation of returns, 60%. This means that with our mean around 0 we can expect our returns to be 0 with a 60% standard deviation, which is very volatile. Autocorrelation in a model can dampen the volatility and cluster low volatility days but this model is capable of producing very large surges. \n\nNext, our alpha and beta coefficients. Alpha measures the extent to which a volatility shock today feeds through into next period\u2019s volatility. In our model 9% of the pervious periods volatility will be passed to the next day. Beta is our persistence parameter, if beta is greater than 1 it leads to a postive feedback loop for small shocks that can create runaway volatility. The sum of alpha and beta measures the rate at which our volatility decays and if alpha plus beta equals 1 then our model has persistent volatility and we might want to look at other models like IGARCH (Integrated GARCH).  \n\nAccording the the text, *Market Risk Analysis* the usual range for alpha in a stable market is $0.05<\\alpha<0.1$ and the parameter beta is $0.85<\\beta<0.98$. These ranges won't totally apply to forecasting a single stock but it gives us an idea of what kinds of values we should be seeing. \n\nNext, we have our t-statistics and p-values. T is our estimate divided by the standard error and is used to calculate our p-value. \nThe usual null hypothesis is that our coefficient has no effect but we can reject the null if our p value is less than an alpha (0.05). When we begin having models with more parameters it can be helpful to pay attention to the p-values. \n\nAbove we have run some test for goodness of fit that we'll introduce below. All of the tests indicate we should reject the null so we'll continue to search for better model parameters. We can see there is clearly autocorrelation out to at least the 25th lag, and the standardized residuals do not look like white noise. We'll use a gridsearch with farily large range of p and q to find a model that best fits the volatility. \n\n<a id='fit'><\/a>\n## Model Fit\nTo measure how well our ARCH\/GARCH model fit our data we want to check for: \n1. Autocorrelation in the standardized residuals, we can again use the Ljung-Box test.\n2. ARCH effects(conditional heteroskedasticity) in the residuals, we will use Engle's ARCH test on the residuals.\n3. Normal distribution in the standardized residuals, we can use the Shapiro-Wilk test, Q-Q plot, and if larger n- the Jarque-Bera test to see if our data approaches normality. .  \n\n### Engle's ARCH Test\n* $H_o$: The squared residuals are a sequence of white noise- the residuals are homoscedastic.\n* $H_a$: The squared residuals could not be fitted with a linear regression model and exhibit heteroskedasticity. \n\nIn python we will be using the ARCH Engle's test which is a combination of Ljung-Box and a Lagrange Multiplier test to determine if our ARCH model has captured the conditional heteroskedasticity of our time series. We will pass it the array of residuals and a max lag to use.\n\n### Shapiro-Wilks Test\nThe Shapiro-wilks test evaluates a data sample and quantifies how likely it is that the data was drawn from a Gaussian distribution. \n* $H_o$: The data is normally distributed\n* $H_a$: The data is not normally distributed\n\n### Jarque-Bera Test\nThe Jarque-Bera test is a type of lagrange multiplier test for normality. It usually used for large data sets because other normality tests are not reliable when n is large, Shapiro-Wilk isn't reliable with n more than 2,000. Jarque-Bera specifically mathces skewness and kurtosis to a normal distribution.  \n* $H_o$: The data is normally distributed\n* $H_a$: The data does not come from a normal distribution","267ccd1d":"<a id='conclusion'><\/a>\n## Conclusion\n---\nWith parameters GARCH(17, 25) we have a model that passed our Engel ARCH test and whose standardized residuals appear approximately normal. Looking at the residuals and our correlograms this model does not have residuals that appear to be white noise and both the PACF and ACF seem to have a significant coefficient at lag ~22, so we didn't satisfactorily model the conditional variance. Capturing the volatility of a single stock can be very difficult. \n\nThe benefit of this kernel is that it is easy to rerun with a different stock. The first cell shows 8 random stocks and some will certainly be easier to model than others. 'TDG' seemed like a very volitle stock with a few very large dips that clearly persist in the residuals of our final model. If you use this kernel to model another stock it would probably be better to choose one with less volatility and trend. ","134d7c78":"---\n# Volatility Clustering in the S&P500 and How to Model with GARCH \n**[Nicholas Holloway](https:\/\/github.com\/nholloway)**\n\n---\n### Mission\nStocks and financial time series have familiar jagged edges that crest and fall suddenly, this is in part the result of *volatility clustering* and part of what makes financial timer series difficult to forecast. In this kernel we will cover **heteroskedasticity**, **volatility clustering**, and how to apply the **GARCH** model to forecast the variance of our time series. \n\n### Table of Contents\n1. [Heteroskedasticity](#hetero)\n2. [Volatility Clustering](#volatile)\n3. [ARCH](#arch)\n4. [GARCH](#garch)\n5. [GARCH Results](#results)\n6. [Model Fit](#fit)\n7. [Conclusion](#conclusion)\n\n### Time Series Kernels: 4 of 4 \n* [Stationarity, Smoothing, and Seasonality](https:\/\/www.kaggle.com\/nholloway\/stationarity-smoothing-and-seasonality)\n* [Deconstructing ARIMA](https:\/\/www.kaggle.com\/nholloway\/deconstructing-arima)\n* [Seasonality and SARIMAX](https:\/\/www.kaggle.com\/nholloway\/seasonality-and-sarimax)\n* [Volatility Clustering and GARCH](https:\/\/www.kaggle.com\/nholloway\/volatility-clustering-and-garch)","a590b257":"### Ljung-Box Test\nLjung-Box is a test for autocorrelation that we can use in tandem with our ACF and PACF plots. The Ljung-Box test takes our data, optionally either lag values to test, or the largest lag value to consider, and whether to compute the Box-Pierce statistic. Ljung-Box and Box-Pierce are two similar test statisitcs, $Q$, that are compared against a chi-squared distribution to determine if the series is white noise. We might use the Ljung-Box test on the residuals of our model to look for autocorrelation, ideally our residuals would be white noise. \n\n* $H_o$: The data are independently distributed, no autocorrelation.\n* $H_a$: The data are not independently distributed; they exhibit serial correlation.\n\nThe Ljung-Box with the Box-Pierce option will return, for each lag, the Ljung-Box test statistic, Ljung-Box p-values, Box-Pierce test statistic, and Box-Pierce p-values. \n\nIf $p<\\alpha$ (0.05) we reject the null hypothesis. ","63b1a024":"<a id='hetero'><\/a>\n## Heteroskedasticity\nWe can think about an idealized time series as a set of observations chosen independently and from identical distributions. Understanding this series is as easy as estimating the parameters of the distribution. This is the idea behind stationarity. Once a time series is stationary, we can learn the parameters of the distirbution and forecast, but rarely is it that easy. This kernel is about time series that exhibit heteroskedasticity, specifically conditional heteroskedasticity and the models we use to forecast them.\n\nIf we return to  the idealized time series, heteroskedasticity is when the variance of the underlying distribution used to construct our time series changes as a function of time. A common appearance of heterskedasticity is a time series whose variance increases with time, growing outward in a conal pattern. Small amounts of heteroskedasticity can sometimes be masked by a Box-Cox transformation, which makes the data distribution more normal. In our forecasts of the S&P500, not only are our changes in variance too great to mask but the variance at step $x_t$ is conditional on the variance of past time steps, $x_{t-1}$. This is called conditional heteroskedasticity or *volatility clustering*.  \n\n<a id='volatile'><\/a>\n## Volatility Clustering\nVolatility clustering is the behavior behind the familiar jagged peaks of financial time series. When we look at the plot above we can see there are periods of high and low volatility. Periods where a positive feedback loop forms and changes in variance give rise to greater changes in variance punctuated by periods of relative calm. The ARCH and GARCH models, which we'll introduce shortly, are designed to model the variance for volatile time series. Note, these models do not forecast estimated values. The ARCH and GARCH models specifically capture the expected variance of the residuals from another model. To forecast financial markets we would use ARCH  and GARCH after applying another model like ARIMA first.\n\n<a id='arch'><\/a>\n## Autoregressive Conditional Heteroskedasticity (ARCH)\nIf you're familiar with autoregressive (AR) models, ARCH is easy to understand. In an AR model the current value is dependent on $p$ previous timesteps where $p$ lagged values were highly correlated with the current timestep. With ARCH, the variance of the current time step is dependent on $p$ lagged squared standard error terms, with coefficients $\\alpha$. The standard error is the difference between the observed and predicted value from another model. \n\n$$\\sigma^2_t=\\alpha_0+\\alpha_1u^2_{t-1}+\\alpha_2u^2_{t-2}+...+\\alpha_pu^2_{t-p}$$\n\nIf the volatility tends to cluster then recent large squared errors are likely to lead to large variance, and more large squared errors. Like fitting an AR model we can look at PACF and ACF plots to see where our series is autocorrelated and use that to choose a value for our parameter, $p$. If we have an ARCH(1) or a first order ARCH model we assume the series is stationary except for the change in variance. We often use ARCH models in conjunction with another AR or ARMA model where we would pass the ARCH model the squared residuals from the ARMA model. We can also pass ARCH a mean process, as in the case of this kernel where we will pass the percent changes in stock closing value- which is a series that will hover around 0. \n\n<a id='garch'><\/a>\n## Generalized Autoregressive Conditional Heteroskedasticity (GARCH)\nGARCH extends ARCH by allowing the variance to depend on its own lags and the lags of the squared residuals. GARCH can capture greater changes like increasing and decreasing volatility. \n\n$$\\sigma^2_t=\\alpha_0+\\alpha_1u^2_{t-1}+\\alpha_2u^2_{t-2}+...+\\alpha_pu^2_{t-p}+\\beta_1\\sigma^2_{t-1}+...+\\beta_p\\sigma^2_{t-q}$$\n\nWe have two parameters for GARCH(q, p):\n* $q$: Number of lag variances\n* $p$: Number of lag residual errors \n\nThe parameters we're using here are consistent with the ARCH python library but more often the swapped notation is used. \n\nGARCH often fits data better than ARCH when there's heteroskedasticity and volatility clustering. We will perform an analysis with both GARCH and ARCH and explain the results but there are lots of variations on the GARCH model that introduce new parameters for the specific behaviors of volatility. "}}