{"cell_type":{"7065b84d":"code","c944503d":"code","b7c8bced":"code","a513cb28":"code","c178c313":"code","79ae7f41":"code","452f81e8":"code","10bba3e9":"code","7dbbf23e":"code","658cd034":"code","7bef2d97":"code","9c2f39cd":"code","ed4d63ea":"code","9d40d27a":"code","2cb0dfa0":"code","03cc893c":"code","3622b117":"code","7fe85a50":"code","34d77f44":"code","80e9e0aa":"code","a876a23c":"code","9436c853":"code","0ab1d3e6":"code","30d88029":"code","a57edd05":"code","1b390d3e":"code","a1c73c18":"code","c3c011cd":"code","a4b21296":"code","8dbcbcec":"code","2b5aaec6":"code","abaee44b":"markdown","a72d938d":"markdown","11ae558a":"markdown","f5514855":"markdown","b94cee1c":"markdown","7b51bb57":"markdown","7c998761":"markdown","e1b05075":"markdown","e82ee468":"markdown","d9ce8e58":"markdown","f36a461f":"markdown","f50e5b45":"markdown","78e82214":"markdown","b1a777eb":"markdown","4d2f7178":"markdown","bdba9b52":"markdown","3d7d6113":"markdown","6d44b49a":"markdown","cce1b694":"markdown","9a064d0a":"markdown","ff1f4c5d":"markdown","e1cbae01":"markdown","929ad5e6":"markdown"},"source":{"7065b84d":"import time\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\ncolor = sns.color_palette()\nimport os\n       \nimport plotly.express as px \nimport plotly.graph_objects as go\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","c944503d":"path = '\/kaggle\/input'\n\ntrain = pd.read_csv(f'{path}\/riiid-test-answer-prediction\/train.csv', low_memory=False, nrows=5 * (10**5), \n                       dtype={'row_id': 'int64',\n                              'timestamp': 'int64',\n                              'user_id': 'int32',\n                              'content_id': 'int16',\n                              'content_type_id': 'int8',\n                              'task_container_id': 'int16',\n                              'user_answer': 'int8',\n                              'answered_correctly': 'int8',\n                              'prior_question_elapsed_time': 'float32', \n                              'prior_question_had_explanation': 'boolean',\n                             }\n                      )\n\ntest = pd.read_csv(f'{path}\/riiid-test-answer-prediction\/example_test.csv')\nsubmit = pd.read_csv(f'{path}\/riiid-test-answer-prediction\/example_sample_submission.csv')\nquestions = pd.read_csv(f'{path}\/riiid-test-answer-prediction\/questions.csv')\nlectures = pd.read_csv(f'{path}\/riiid-test-answer-prediction\/lectures.csv')\nprint('Train shapes: ', train.shape)\nprint('Test shapes: ', test.shape)","b7c8bced":"train.head()","a513cb28":"fig,ax = plt.subplots(figsize=(12,8))\nplt.hist(train['timestamp'], bins=40);\nplt.xlabel('timestamp',fontsize=20)\nplt.ylabel('count',fontsize=20)\nax.tick_params(labelsize=20)\nplt.title('count of timestamp',fontsize=25)\nplt.grid()\nplt.ioff()","c178c313":"ds = train['user_id'].value_counts().reset_index()\nds.columns = ['user_id', 'count']\nds['user_id'] = ds['user_id'].astype(str) + '-'\nds = ds.sort_values(['count'])\ntop_40 = ds.tail(40)\n\n\nfig,ax = plt.subplots(figsize=(12,8))\nsns.barplot(top_40['count'],top_40['user_id'])","79ae7f41":"train.content_type_id.value_counts()","452f81e8":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['content_type_id'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Percentage content_type_id Distribution')\nax[0].set_ylabel('Count')\nsns.countplot('content_type_id',data=train,ax=ax[1],order=train['content_type_id'].value_counts().index)\nax[1].set_title('Count of content_type_id')\nplt.show()","10bba3e9":"train.task_container_id.value_counts()","7dbbf23e":"train.user_answer.value_counts()","658cd034":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['user_answer'].value_counts().plot.pie(explode=[0,0.1,0.1,0.1,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Percentage user_answer Distribution')\nax[0].set_ylabel('Count')\nsns.countplot('user_answer',data=train,ax=ax[1],order=train['user_answer'].value_counts().index)\nax[1].set_title('Count of user_answer')\nplt.show()","7bef2d97":"f,ax=plt.subplots(1,2,figsize=(18,8))\ntrain['answered_correctly'].value_counts().plot.pie(explode=[0,0.1,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Percentage Severity Distribution')\nax[0].set_ylabel('Count')\nsns.countplot('answered_correctly',data=train,ax=ax[1],order=train['answered_correctly'].value_counts().index)\nax[1].set_title('Count of answered_correctly')\nplt.show()","9c2f39cd":"train['prior_question_had_explanation'].value_counts()","ed4d63ea":"train = train.loc[train['answered_correctly'] != -1].reset_index(drop=True)\ntrain['prior_question_had_explanation'] = train['prior_question_had_explanation'].fillna(value=False).astype(bool)\n\nuser_answers_ = train.groupby('user_id').agg({ 'answered_correctly': ['mean', 'count']}).copy()\nuser_answers_.columns = ['mean_user_accuracy', 'questions_answered']\n\ncontent_answers_ = train.groupby('content_id').agg({'answered_correctly': ['mean', 'count']}).copy()\ncontent_answers_.columns = ['mean_acc', 'questions_asked']\n\ntrain = train.merge(user_answers_, how='left', on = 'user_id')\ntrain = train.merge(content_answers_, how='left', on = 'content_id')","9d40d27a":"train = pd.merge(train,questions[['question_id','bundle_id','part']], left_on='user_id', right_on='question_id')\ntrain.head()","2cb0dfa0":"grouped_df = train.groupby([\"questions_answered\"])[\"mean_user_accuracy\"].aggregate(\"count\").reset_index()\n\nplt.figure(figsize=(12,8))\nsns.pointplot(grouped_df['questions_answered'].values, grouped_df['mean_user_accuracy'].values, alpha=0.8, color=color[2])\nplt.ylabel('questions_answered', fontsize=12)\nplt.xlabel('mean_user_accuracy', fontsize=12)\nplt.title(\"mean_user_accuracy wise questions_answered\", fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()","03cc893c":"sns.jointplot(x=train.mean_acc.values,y=train.questions_asked.values,height=10)\nplt.ylabel('mean_acc', fontsize=12)\nplt.xlabel('questions_asked', fontsize=12)\nplt.show()","3622b117":"columns = ['timestamp', 'user_id', 'content_id', 'content_type_id','answered_correctly',\n       'task_container_id', 'prior_question_elapsed_time',\n       'prior_question_had_explanation', 'part', 'mean_user_accuracy', 'questions_answered','mean_acc', 'questions_asked']","7fe85a50":"df = train[columns].copy()\ndf.info()","34d77f44":"labels = []\nvalues = []\nfor col in columns:\n    labels.append(col)\n    values.append(np.corrcoef(df[col].values, df.answered_correctly.values)[0,1])\ncorr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\ncorr_df = corr_df.sort_values(by='corr_values')\n\nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,40))\nrects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\nax.set_yticks(ind)\nax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation coefficient of the variables\")\nplt.show()","80e9e0aa":"columns_f = ['timestamp', 'user_id', 'content_id', 'content_type_id',\n       'task_container_id', 'prior_question_elapsed_time',\n       'prior_question_had_explanation', 'part', 'mean_user_accuracy', 'questions_answered','mean_acc', 'questions_asked']","a876a23c":"train.fillna(value = -1, inplace = True)","9436c853":"train.isnull().sum()","0ab1d3e6":"train_y = train['answered_correctly'].values\nnum_df = train[columns_f]\nfeat_name = num_df.columns.values\n\nfrom sklearn import ensemble \nmodel = ensemble.RandomForestClassifier(n_estimators=25,max_depth=30, n_jobs=-1, random_state=0) \nmodel.fit(num_df,train_y)\n\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_],axis=0)\nindi = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indi)), importances[indi], color=color[4], yerr=std[indi], align=\"center\")\nplt.xticks(range(len(indi)), feat_name[indi], rotation='vertical')\nplt.xlim([-1, len(indi)])\nplt.show()","30d88029":"import xgboost as xgb \n\nxgb_prames = {\n    'eta': 0.05,\n    'max_depth': 8,\n    'subsample': 0.7,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:linear',\n    'silent': 1,\n    'seed' : 0\n}\n\ndtrain = xgb.DMatrix(num_df, train_y, feature_names=num_df.columns.values)\n\nmodel = xgb.train(dict(xgb_prames, silent=0), dtrain, num_boost_round=50)\n\n\nfig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\nplt.show()","a57edd05":"y = train['answered_correctly']\nX = train.drop(['answered_correctly', 'user_answer'], axis=1)","1b390d3e":"def fast_auc(y_true, y_prob):\n    \"\"\"\n    fast roc_auc computation: https:\/\/www.kaggle.com\/c\/microsoft-malware-prediction\/discussion\/76013\n    \"\"\"\n    y_true = np.asarray(y_true)\n    y_true = y_true[np.argsort(y_prob)]\n    nfalse = 0\n    auc = 0\n    n = len(y_true)\n    for i in range(n):\n        y_i = y_true[i]\n        nfalse += (1 - y_i)\n        auc += y_i * nfalse\n    auc \/= (nfalse * (n - nfalse))\n    return auc\n\n\ndef eval_auc(y_true, y_pred):\n    \"\"\"\n    Fast auc eval function for lgb.\n    \"\"\"\n    return 'auc', fast_auc(y_true, y_pred), True","a1c73c18":"import lightgbm as lgb\n\nscores = []\nfeature_importance = pd.DataFrame()\nmodels = []\n\nparams = {'num_leaves': 32,\n          'max_bin': 300,\n          'objective': 'binary',\n          'max_depth': 13,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"metric\": 'auc',\n         }\n\nfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n    print(f'Fold {fold_n} started at {time.ctime()}')\n    X_train, X_valid = X[columns_f].iloc[train_index], X[columns_f].iloc[valid_index]\n    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n    model = lgb.LGBMClassifier(**params, n_estimators=700, n_jobs = 1)\n    model.fit(X_train, y_train, \n            eval_set=[(X_train, y_train), (X_valid, y_valid)], eval_metric=eval_auc,\n            verbose=1000, early_stopping_rounds=10)\n    score = max(model.evals_result_['valid_1']['auc'])\n    \n    models.append(model)\n    scores.append(score)\n\n    fold_importance = pd.DataFrame()\n    fold_importance[\"feature\"] = columns_f\n    fold_importance[\"importance\"] = model.feature_importances_\n    fold_importance[\"fold\"] = fold_n + 1\n    feature_importance = pd.concat([feature_importance, fold_importance], axis=0)","c3c011cd":"feature_importance[\"importance\"] \/= 5\ncols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\nplt.figure(figsize=(16, 12));\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False));\nplt.title('LGB Features (avg over folds)');","a4b21296":"import riiideducation\nenv = riiideducation.make_env()","8dbcbcec":"iter_test = env.iter_test()","2b5aaec6":"for (test_df, sample_prediction_df) in iter_test:\n    y_preds = []\n    test_df = test_df.merge(user_answers_, how = 'left', on = 'user_id')\n    test_df = test_df.merge(content_answers_, how = 'left', on = 'content_id')\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value = False).astype(bool)\n    test_df = test_df.loc[test_df['content_type_id'] == 0].reset_index(drop=True)\n    test_df = pd.merge(test_df, questions[['question_id', 'bundle_id', 'part']], left_on='content_id', right_on='question_id', how='left')\n    test_df.fillna(value = -1, inplace = True)\n\n    for model in models:\n        y_pred = model.predict_proba(test_df[columns_f], num_iteration=model.best_iteration_)[:, 1]\n        y_preds.append(y_pred)\n\n    y_preds = sum(y_preds) \/ len(y_preds)\n    test_df['answered_correctly'] = y_preds\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","abaee44b":"# About competition\n\nThe challenge will rely on the world\u2019s largest education dataset, EdNet, which consists of more than 130 million interactions coming from over 780,000 students. EdNet will be offered to top researchers and scientists. \n\nIn this competition, we will just try to create an algorithm for \"Knowledge Tracing,\" the modeling of student knowledge over time. our goal is to accurately predict how students will perform on future interactions. If successful, it\u2019s possible that any student with an Internet connection can enjoy the benefits of a personalized learning experience, regardless of where they live. \n\nOur challenge in this competition is to predict whether students are able to answer their next questions correctly.\n\nThis competition is similar to Two Sigma competition, so we got test data using special API.\n\n<font size=3 color=\"red\">Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>\n\n![](https:\/\/ml8ygptwlcsq.i.optimole.com\/fMKjlhs.f8AX~1c8f3\/w:1200\/h:678\/q:auto\/https:\/\/www.unite.ai\/wp-content\/uploads\/2020\/10\/Screen-Shot-2020-10-06-at-7.38.57-PM.jpg)\n\n","a72d938d":"### lets try to add some Features","11ae558a":"# Xgboost ","f5514855":"Top 40 users by number of actions","b94cee1c":"# Lightgbm","7b51bb57":"## content_type_id \n\n0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture\n","7c998761":"##  task_container_id\n\nId code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id. Monotonically increasing for each user.","e1b05075":"To bo .... \n\nhave to do alots actually \n\n1. tunning modelling\n2. good feature engineering and selection \n3. pca (may be)\n\nwell update soon \n\n<font size=3 color=\"red\">Please upvote this kernel if you like it. It motivates me to produce more quality content :)<\/font>","e82ee468":"as there are more than 100m rows in \"train.csv\", we can't read all the data in kaggle notebooks so , lets just take some ","d9ce8e58":"# import all you need","f36a461f":"## lets try to Exploring the features in train.csv","f50e5b45":"# RandomForestClassifier","78e82214":"lets go by flow...","b1a777eb":"## user_answer\n\nthe user's answer to the question, if any. Read -1 as null, for lectures.","4d2f7178":"# making prediction \n\nyou can get from https:\/\/www.kaggle.com\/sishihara\/riiid-lgbm-5cv-benchmark","bdba9b52":"## answered_correctly( pere ): \n\n(int8) if the user responded correctly. Read -1 as null, for lectures.\n\n","3d7d6113":"### helper functions \n\n![](http:\/\/)took from : https:\/\/www.kaggle.com\/artgor\/riiid-eda-feature-engineering-and-models\/comments","6d44b49a":"### we can actually play with models but for now i will try some and plot the feature_importances_ for the features with target","cce1b694":"## prior_question_had_explanation: \n\n(bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.","9a064d0a":"## well now, lets try to check for corrcoef for all the features with the target","ff1f4c5d":"### timestamp - \n\n> it is imprtant to remember that this is the time between this user interaction and the first event from that user. So starting time could be different for each user","e1cbae01":" **answered_correctly** is our target! **-1** is a special value, we'll talk about it later","929ad5e6":"## modelling "}}