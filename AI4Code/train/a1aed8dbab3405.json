{"cell_type":{"28ebba89":"code","e55d1fa9":"code","31229e1d":"code","7a7542fe":"code","478a9753":"code","159f12c6":"code","5d908f84":"code","97f555be":"code","6312ca70":"code","4a443420":"code","ef046012":"code","6345cc13":"code","07921910":"code","dc349b9a":"code","b79cabff":"code","34e7a9b3":"markdown","cfc9a7fd":"markdown","0bf07590":"markdown","c131388a":"markdown","b29784c7":"markdown","39e86c9e":"markdown","f17c7252":"markdown"},"source":{"28ebba89":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer\n\nimport xgboost as xgb\n\nimport lightgbm as lgbm\nfrom sklearn.utils import shuffle, resample\nfrom sklearn.preprocessing import StandardScaler\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \nimport lightgbm as lgb\nfrom sklearn.model_selection import GridSearchCV\n# Similarly LGBMRegressor can also be imported for a regression model.\nfrom lightgbm import LGBMClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import fbeta_score\nfrom skopt.space import Real, Integer\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize\nfrom skopt.plots import plot_convergence","e55d1fa9":"train = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/test.csv\")","31229e1d":"train.head(2)","7a7542fe":"print('Column\/Columns that are not in test data: ', end = '')\nfor i in train.columns:\n    if i not in test.columns:\n        print(i)","478a9753":"sns.countplot(train['target']);","159f12c6":"train = train.select_dtypes(include=['int','float'])\ntest = test.select_dtypes(include=['int','float'])\nX = train.drop(columns=[\"target\"])\ny=train[\"target\"]","5d908f84":"%%time\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, lambda_l1,lambda_l2,early_stopping_round):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['lambda_l1'] = int(round(lambda_l1))\n        params['lambda_l2'] = int(round(lambda_l2))\n        params['early_stopping_round'] = int(round(early_stopping_round))\n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =250,num_boost_round=1500, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.04, 0.65),\n                                            'num_leaves': (8, 240),\n                                            'feature_fraction': (0.21, 0.9),\n                                            'bagging_fraction': (0.2, 0.9),\n                                            'max_depth': (4, 25),\n                                            'max_bin':(5,65),\n                                            'early_stopping_round' : (10,300),\n                                            'lambda_l1': (0, 10),\n                                            'lambda_l2':  (0, 10)}, random_state=200)                                   \n                                      \n\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=10, n_folds=3, random_seed=6)","97f555be":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='binary'\nopt_params[1]['metric']='auc'\n#opt_params[1]['is_unbalance']=True\n#opt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params","6312ca70":"# separate classes into different datasets\nnormal_class = train.query('target == 0')\nfraudulent_class = train.query('target == 1')\n\n# randomize the datasets\nnormal_class = normal_class.sample(frac=1,random_state=1210)\nfraudulent_class = fraudulent_class.sample(frac=1,random_state=1210)\nresampled = normal_class.sample(n=int(len(fraudulent_class)*4.4), random_state=1210)\ntrain = pd.concat([fraudulent_class,resampled])","4a443420":"train = train.select_dtypes(include=['int','float'])\ntest = test.select_dtypes(include=['int','float'])","ef046012":"train.info()","6345cc13":"X = train.drop(columns=[\"target\"])\ny=train[\"target\"]","07921910":"%%time\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, bagging_fraction, max_depth, max_bin, lambda_l1,lambda_l2,early_stopping_round):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['lambda_l1'] = int(round(lambda_l1))\n        params['lambda_l2'] = int(round(lambda_l2))\n        params['early_stopping_round'] = int(round(early_stopping_round))\n        \n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =250,num_boost_round=1500, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n     \n    lgbBO = BayesianOptimization(lgb_eval, {'learning_rate': (0.04, 0.65),\n                                            'num_leaves': (8, 240),\n                                            'feature_fraction': (0.21, 0.9),\n                                            'bagging_fraction': (0.2, 0.9),\n                                            'max_depth': (4, 25),\n                                            'max_bin':(5,65),\n                                            'early_stopping_round' : (10,300),\n                                            'lambda_l1': (0, 10),\n                                            'lambda_l2':  (0, 10)}, random_state=200)                                   \n                                      \n\n\n    \n    #n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    #init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    \n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    model_auc=[]\n    for model in range(len( lgbBO.res)):\n        model_auc.append(lgbBO.res[model]['target'])\n    \n    # return best parameters\n    return lgbBO.res[pd.Series(model_auc).idxmax()]['target'],lgbBO.res[pd.Series(model_auc).idxmax()]['params']\n\nopt_params = bayes_parameter_opt_lgb(X, y, init_round=5, opt_round=10, n_folds=3, random_seed=6)","dc349b9a":"opt_params[1][\"num_leaves\"] = int(round(opt_params[1][\"num_leaves\"]))\nopt_params[1]['max_depth'] = int(round(opt_params[1]['max_depth']))\nopt_params[1]['max_bin'] = int(round(opt_params[1]['max_bin']))\nopt_params[1]['objective']='binary'\nopt_params[1]['metric']='auc'\n#opt_params[1]['is_unbalance']=True\n#opt_params[1]['boost_from_average']=False\nopt_params=opt_params[1]\nopt_params","b79cabff":"d = {\n    'Datasets': ['Undersampled_Data', 'Base_Data'],\n    'bagging_fraction': [0.9, 0.9],\n    'early_stopping_round': [72,76],\n    'feature_fraction': [0.21,0.21],\n         'lambda_l1': [0,0],\n         'lambda_l2': [10,10],\n         'learning_rate': [0.04,0.04],\n         'max_bin': [34,35],\n         'max_depth': [25,25],\n             'num_leaves': [83,107],\n         'objective': ['binary','binary'],\n         'metric': ['auc', 'auc']\n }\ndf = pd.DataFrame(data=d)\ndf","34e7a9b3":"# Undersampling","cfc9a7fd":"# Undersampled data best params","0bf07590":"# Bayesian Opt with Undersampling","c131388a":"# Bayesian Optimization","b29784c7":"# Undersampled and base data params","39e86c9e":"# Best Params","f17c7252":"# SETUP"}}