{"cell_type":{"303f6a89":"code","e32734a2":"code","f4813e43":"code","15fff193":"code","9ce34d86":"code","b379aa6f":"code","3543f151":"code","ddedc5bb":"code","e22a65a7":"code","71007b83":"code","3b572db6":"code","e931962d":"code","1796a207":"code","9ca3741a":"code","74c61d4c":"code","7fe64c29":"code","cf586d9c":"code","368c05da":"code","4b632f43":"code","d19788b6":"code","5b219262":"code","7dbedd79":"code","71641b4a":"code","3a54240e":"code","2d455a82":"code","e994635b":"code","27c618b4":"code","22104a23":"code","d7862b4e":"code","a53beb88":"code","a22c245d":"code","bc839491":"code","1d27a28f":"code","65a13872":"code","88a28ffb":"code","01818344":"code","b961a1a0":"code","6f6e94fb":"code","326f33f7":"code","b4a30fcc":"code","77c8ea96":"code","8110466b":"code","3167d985":"code","890dec45":"code","52b85fb9":"code","f971ea70":"code","76667a78":"code","00574027":"code","2a6835ed":"code","0e314903":"code","0beb8c39":"code","9c38f6ab":"code","50dfb433":"code","fc7dbcc3":"code","3d9aa20d":"code","c5a797f9":"code","d8f0f407":"code","8dbeb734":"code","ecdcbeda":"code","b618c296":"code","8d5fe3d8":"code","bc84c585":"code","bf737a5a":"code","b27e3fc3":"code","8946e9de":"code","dd12fb5b":"code","e559cb95":"code","3b175d36":"code","753d2feb":"code","6fce051f":"code","3c2d37b0":"code","0e85ab9f":"code","a07a8495":"code","87586b94":"code","9e59f63c":"code","e666e6b4":"code","7988e13f":"code","ff93782b":"code","644c4eb8":"code","b191bf14":"code","fce7e04e":"code","923f4cb0":"code","c0e49a34":"code","263eab08":"code","93203bf9":"code","b0d87207":"code","5332d1e3":"code","1c5d4a8a":"code","753a6d4b":"code","9cda269a":"code","f8e5bf94":"code","e53ce8d5":"code","92426514":"code","a129dffe":"code","91cb2f0a":"code","6bbd76b3":"code","d8d70534":"code","39432480":"code","a4ac3d43":"code","5a68c70d":"code","86a3dc03":"code","49e7549d":"code","7486e385":"code","b690d0c7":"code","d06a8a55":"code","269edec0":"code","5040deea":"code","ccd98a10":"code","56230c1a":"code","ac3e3612":"code","145b42fc":"code","7911e283":"code","3c67ae98":"code","a00b132c":"code","479b2ce0":"code","5bc61066":"code","4f269b87":"code","bac64a01":"code","7a61984c":"code","531ded2d":"code","663e0495":"code","daeb20cb":"code","20bc948f":"code","8fe83bf7":"code","e123d120":"code","b2ad07ba":"code","81c70a9c":"code","cbc7df08":"code","d20aa0b0":"code","cbffa166":"code","55d408bc":"code","225267c7":"code","6da0559d":"code","e7426bd2":"code","33821c93":"code","2f3121e7":"code","a59102c5":"code","e7663f53":"code","4a17d3ae":"code","a61d12c3":"code","347ebbda":"code","85644b22":"code","9636412a":"code","6ca8ed22":"markdown","85c27a2d":"markdown","b04be5b2":"markdown","4d0b8c87":"markdown","449664a5":"markdown","195a0df8":"markdown","2078faab":"markdown","276ad401":"markdown","080ccc74":"markdown","7d2a47f2":"markdown","9db938a0":"markdown","dbc0ddd9":"markdown","add88c68":"markdown","4a21045f":"markdown","230ce708":"markdown","212c217a":"markdown","b2724448":"markdown","b30f8039":"markdown","671c4a84":"markdown","528fca8d":"markdown","1d1f23de":"markdown","73467315":"markdown","b5ae8adc":"markdown","22252bb2":"markdown","b2c85043":"markdown","c97fd929":"markdown","b9e2f33c":"markdown","c4361ded":"markdown","f2a97dfd":"markdown","d382edaf":"markdown","329b56f1":"markdown","be66cf01":"markdown","80f7bafe":"markdown","628c7a1e":"markdown","4b4afb3f":"markdown","95373759":"markdown","225681db":"markdown","3f2027e5":"markdown","ae74659e":"markdown","630da20f":"markdown","5b1a8466":"markdown","533e0c47":"markdown","44f53519":"markdown","01bf5cc1":"markdown","cf3740a2":"markdown","d6382608":"markdown","7f9908df":"markdown"},"source":{"303f6a89":"# Installing kaggle library\n!pip install -q kaggle","e32734a2":"import os\nos.chdir(\"\/kaggle\/input\/predicting-pulsar-starintermediate\")\n!ls","f4813e43":"!pip install --upgrade seaborn","15fff193":"import pandas as pd # library for data manipulation\nimport numpy as np # library for array management\nimport seaborn as sns # wrapper library for visualization\nimport matplotlib.pyplot as plt # library for data visualization\n%matplotlib inline\n\ntrain = pd.read_csv('pulsar_data_train.csv') # read the data into a dataframe\ntrain.head(10) # check the top 10 rows of the dataframe\n\nnp.random.seed(123)","9ce34d86":"train.tail(10) # check the bottom 10 rows of the data frame","b379aa6f":"  # Describe method applied across columns to give the following \n  # (1) Numerical Summary Statistics for Numerical Variables : Central Tendency and Spread\n  # (2) Count Summary Statistics for Categorical Variables : Count , Most Common category , Frequency of Most Common Category\n  train.describe(include='all') ","3543f151":"# Assessing the data types for each variable \/ column\ntrain.dtypes","ddedc5bb":"# Visualizing the extent of missing values in each variable\nsns.heatmap(train.isnull(), yticklabels=False, cbar=False, cmap='viridis')","e22a65a7":"# Numerical Profiling of Missing enrtries in each variable\nfor feature in train.columns:\n    print('Missing values in feature ' + str(feature) + ' : ' + str(len(train[train[feature].isnull() == True])))","71007b83":"# Another method for Numerical profiling of Missing entries in each variable\ntrain.isnull().sum()","3b572db6":"# Visualizing the distribution of Target Class\nplt.figure(figsize = (10, 8))\ntotal = float(len(train))\nax = sns.countplot(x = 'target_class', data = train)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,height + 3,'{:1.2f}'.format(height\/total),ha=\"center\") ","e931962d":"print('No. of instances pulsar stars are detected in dataset is ' + str(len(train[train['target_class'] == 1])))\nprint('No. of instances pulsar stars are not detected in dataset is ' + str(len(train[train['target_class'] == 0])))","1796a207":"# Since all dtypes are float or int, no categorical features\ncontinous_features = list(set(train.columns) - set(['target_class']))\ncontinous_features.sort()\ncontinous_features","9ca3741a":"fig, axes = plt.subplots(nrows=len(continous_features),ncols=2,  figsize=(15, 40))\nfor i in range(len(continous_features)):\n    feature = continous_features[i]\n    plt.figure(figsize = (5, 5))\n    data=train.copy()\n    sns.histplot(x=data[feature].dropna(), ax=axes[i][0])\n    sns.boxplot(x=data[feature].dropna(), ax=axes[i][1])","74c61d4c":"# from matplotlib.ticker import FormatStrFormatter\n\ntrain_dummy = train.copy()\nfor feature in continous_features:\n  fig, axs = plt.subplots(figsize=(22, 9))\n  sns.histplot(train_dummy[train_dummy['target_class']==0][feature].dropna(), color='red')\n  sns.histplot(train_dummy[train_dummy['target_class']==1][feature].dropna(), color='blue')\n\n  plt.legend([0, 1], loc='upper right', prop={'size': 15})\n  plt.show()","7fe64c29":" # from matplotlib.ticker import FormatStrFormatter\nfor i in range(len(continous_features)):\n  feature = continous_features[i]\n  plt.figure(figsize = (10, 5))\n  sns.boxplot(x = 'target_class', y = continous_features[i], data = train)\n  plt.grid()\n  plt.show()","cf586d9c":"plt.figure(figsize = (10, 10))\ncorr_mat = train.corr()\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True)","368c05da":"# Numerical Profiling of Missing enrtries in each variable\nfor feature in train.columns:\n    print('Missing values in feature ' + str(feature) + ' : ' + str(len(train[train[feature].isnull() == True])))","4b632f43":"train.isnull().sum()","d19788b6":"train.isnull().sum()\/len(train) * 100","5b219262":"!pip install scikit-learn","7dbedd79":"null_data = train[train.isnull().any(axis=1)]","71641b4a":"null_data.head()","3a54240e":"null_data.tail()","2d455a82":"null_data.isnull().sum()\/len(train) * 100","e994635b":"len(null_data)","27c618b4":"train_temp = train[train[' Excess kurtosis of the integrated profile'].isnull() & \n      train[' Standard deviation of the DM-SNR curve'].isnull()]\ntrain_temp.head()","22104a23":"print(len(train_temp))","d7862b4e":"train_temp = train[train[' Excess kurtosis of the integrated profile'].isnull() & \n                   train[' Skewness of the DM-SNR curve'].isnull()]\ntrain_temp.head()","a53beb88":"print(len(train_temp))","a22c245d":"train_temp = train[train[' Skewness of the DM-SNR curve'].isnull() & \n      train[' Standard deviation of the DM-SNR curve'].isnull()]\ntrain_temp.head()","bc839491":"len(train_temp)","1d27a28f":"train_temp = train[train[' Excess kurtosis of the integrated profile'].isnull() & \n      train[' Standard deviation of the DM-SNR curve'].isnull() & \n      train[' Skewness of the DM-SNR curve'].isnull()]\ntrain_temp.head()","65a13872":"len(train_temp)","88a28ffb":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.linear_model import BayesianRidge, LinearRegression, SGDRegressor, ARDRegression","01818344":"null_data","b961a1a0":"# ExtraTreesRegressor,BayesianRidge, LinearRegression, SGDRegressor, ARDRegression\nimputer = IterativeImputer(BayesianRidge(), sample_posterior=True, max_iter=100, verbose=1)\nimpute_data = pd.DataFrame(imputer.fit_transform(train), columns=train.columns.values.tolist())","6f6e94fb":"train[[' Excess kurtosis of the integrated profile',\n                        ' Standard deviation of the DM-SNR curve', \n                        ' Skewness of the DM-SNR curve']].describe(include='all') ","326f33f7":"impute_data[[' Excess kurtosis of the integrated profile',\n                        ' Standard deviation of the DM-SNR curve', \n                        ' Skewness of the DM-SNR curve']].describe(include='all') ","b4a30fcc":"impute_data.iloc[list(null_data.index)]","77c8ea96":"impute_data.isnull().sum()","8110466b":"Q1 = train.quantile(0.25)\nQ3 = train.quantile(0.75)\nIQR = Q3 - Q1\nlower_range= Q1-(1.5 * IQR)\nupper_range= Q3+(1.5 * IQR)\nprint('Number of Outliers (Percentage):')\n((train < (lower_range)) | (train > (upper_range))).sum()","3167d985":"Q1 = impute_data.quantile(0.25)\nQ3 = impute_data.quantile(0.75)\nIQR = Q3 - Q1\nlower_range= Q1-(1.5 * IQR)\nupper_range= Q3+(1.5 * IQR)\nprint('Number of Outliers (Percentage):')\n((impute_data < (lower_range)) | (impute_data > (upper_range))).sum()\/len(impute_data) * 100","890dec45":"train_impute_out = impute_data.copy()","52b85fb9":"for cols in train_impute_out.columns[:-1]:\n  train_impute_out[cols] = np.where(train_impute_out[cols]>upper_range[cols],\n                                    upper_range[cols],train_impute_out[cols])\n  train_impute_out[cols] = np.where(train_impute_out[cols]<lower_range[cols],\n                                    lower_range[cols],train_impute_out[cols])","f971ea70":"train.describe(include='all')","76667a78":"cols = list(train.columns)\ncols.reverse()\ncols","00574027":"plt.figure(figsize=(10,8))\nplt.title('Base data')\ntrain.boxplot(vert=0, column=cols)\nplt.xlim(-200, 1300)","2a6835ed":"train_impute_out.describe(include='all')","0e314903":"plt.figure(figsize=(10,8))\nplt.title('Final data')\ntrain_impute_out.boxplot(vert=0, column=cols)\nplt.xlim(-200, 1300)","0beb8c39":" # Correlation plot after outlier treatment\nplt.figure(figsize = (10, 10))\ncorr_mat = train_impute_out.corr()\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True)","9c38f6ab":"train_final = train_impute_out.drop([' Excess kurtosis of the integrated profile', \n                                     ' Skewness of the DM-SNR curve', \n                                     ' Standard deviation of the DM-SNR curve'], axis=1)","50dfb433":"train_final","fc7dbcc3":" # Correlation plot after outlier treatment\nplt.figure(figsize = (10, 10))\ncorr_mat = train_final.corr()\nsns.heatmap(corr_mat, xticklabels = corr_mat.columns, yticklabels = corr_mat.columns, annot=True)","3d9aa20d":"from sklearn.model_selection import train_test_split\n\ny = train_final['target_class']\nx = train_final.copy().drop(['target_class'], axis = 1)","c5a797f9":"x","d8f0f407":"y","8dbeb734":"x_train, x_test1, y_train, y_test1 = train_test_split(x, y, test_size=0.40, random_state=42)\nx_val, x_test, y_val, y_test = train_test_split(x_test1, y_test1, test_size=0.50, random_state=42)","ecdcbeda":"print(x_train.shape)\nprint(x_val.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_val.shape)\nprint(y_test.shape)\n","b618c296":"x.describe(include='all')","8d5fe3d8":"x_train.describe(include='all')","bc84c585":"x_val.describe(include='all')","bf737a5a":"x_test.describe(include='all')","b27e3fc3":"y.describe(include='all')","8946e9de":"y_train.describe(include='all')","dd12fb5b":"y_val.describe(include='all')","e559cb95":"y_test.describe(include='all')","3b175d36":"from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.tree import ExtraTreeClassifier, DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.semi_supervised import LabelPropagation\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier, NearestCentroid\nfrom sklearn.ensemble import GradientBoostingClassifier, ExtraTreesClassifier, RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, IsolationForest, StackingClassifier, VotingClassifier\n\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n!pip install xgboost\nfrom xgboost import XGBClassifier\n","753d2feb":"model = LogisticRegression()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","6fce051f":"param_grid = [    \n    {'penalty' : ['l1'],\n     'tol' : np.logspace(-8, 0, 10),\n     'C' : np.logspace(-4, 4, 10),\n     'fit_intercept' : ['True', 'False'],\n     'class_weight' : [None, 'balanced'],\n     'solver' : ['liblinear', 'saga'],\n     'max_iter' : np.logspace(1, 4, 4)\n    },\n    {'penalty' : ['elasticnet'],\n     'tol' : np.logspace(-8, 0, 10),\n     'C' : np.logspace(-4, 4, 10),\n     'fit_intercept' : ['True', 'False'],\n     'class_weight' : [None, 'balanced'],\n     'solver' : ['saga'],\n     'max_iter' : np.logspace(1, 4, 4)\n    },\n    {'penalty' : ['l2', 'none'],\n     'tol' : np.logspace(-8, 0, 10),\n     'C' : np.logspace(-4, 4, 10),\n     'fit_intercept' : ['True', 'False'],\n     'class_weight' : [None, 'balanced'],\n     'solver' : ['newton-cg', 'lbfgs', 'sag', 'saga'],\n     'max_iter' : np.logspace(1, 4, 4)\n    }\n]","3c2d37b0":"clf_grid = GridSearchCV(model, param_grid= param_grid, cv = 5, verbose=True, n_jobs=-1)","0e85ab9f":"%%time\n# best_clf_grid = clf_grid.fit(x_train, y_train)","a07a8495":"# best_clf_grid.best_estimator_","87586b94":"# est = best_clf_grid.best_estimator_\nest = LogisticRegression(C=0.3593813663804626, class_weight=None, dual=False,\n                   fit_intercept='True', intercept_scaling=1, l1_ratio=None,\n                   max_iter=10.0, multi_class='auto', n_jobs=None, penalty='l1',\n                   random_state=None, solver='liblinear',\n                   tol=5.994842503189409e-07, verbose=0, warm_start=False)","9e59f63c":"est.fit(x_train, y_train)\ny_pred = est.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","e666e6b4":"model = RidgeClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","7988e13f":"param_grid = [    \n    {'alpha' : np.logspace(-4, 4, 10),\n     'fit_intercept' : ['True', 'False'],\n     'normalize' : ['True', 'False'],\n     'max_iter' : list(np.logspace(1,4,4)) + ['None'],\n     'tol' : np.logspace(-8, 0, 10),\n     'class_weight' : [None, 'balanced'],\n     'solver' : ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n    }\n]","ff93782b":"clf_grid = GridSearchCV(model, param_grid= param_grid, cv = 5, verbose=True, n_jobs=-1)","644c4eb8":"%%time\n# best_clf_grid = clf_grid.fit(x_train, y_train)","b191bf14":"# best_clf_grid.best_estimator_","fce7e04e":"# est = best_clf_grid.best_estimator_\nest = RidgeClassifier(alpha=0.005994842503189409, class_weight=None, copy_X=True,\n                fit_intercept='False', max_iter=100.0, normalize='True',\n                random_state=None, solver='sag', tol=1.0)","923f4cb0":"est.fit(x_train, y_train)\ny_pred = est.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","c0e49a34":"model = ExtraTreeClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","263eab08":"param_grid = [    \n    {'criterion' : ['gini', 'entropy'],\n     'splitter' : ['random', 'best'],\n     'max_depth' : list(np.logspace(0,4,5)) + ['None'],\n     'min_samples_split' : np.linspace(2,100,5).astype(int),\n     'min_samples_leaf' : np.linspace(1,100,5).astype(int),\n     'max_features' : ['auto', 'sqrt', 'log2'],\n     'min_impurity_decrease' : np.linspace(0,0.9,5),\n     'class_weight' : [None, 'balanced'],\n     'ccp_alpha' : np.linspace(0,0.9,5)\n    }\n]","93203bf9":"clf_grid = GridSearchCV(model, param_grid= param_grid, cv = 5, verbose=True, n_jobs=-1)","b0d87207":"%%time\n# best_clf_grid = clf_grid.fit(x_train, y_train)","5332d1e3":"# best_clf_grid.best_estimator_","1c5d4a8a":"# est = best_clf_grid.best_estimator_\nest = ExtraTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n                    max_depth=10000.0, max_features='sqrt', max_leaf_nodes=None,\n                    min_impurity_decrease=0.0, min_impurity_split=None,\n                    min_samples_leaf=1, min_samples_split=51,\n                    min_weight_fraction_leaf=0.0, random_state=None,\n                    splitter='random')","753a6d4b":"est.fit(x_train, y_train)\ny_pred = est.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","9cda269a":"model = DecisionTreeClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","f8e5bf94":"param_grid = [    \n    {'criterion' : ['gini', 'entropy'],\n     'splitter' : ['random', 'best'],\n     'max_depth' : list(np.logspace(0,4,5)) + ['None'],\n     'min_samples_split' : np.linspace(2,100,5).astype(int),\n     'min_samples_leaf' : np.linspace(1,100,5).astype(int),\n     'max_features' : ['auto', 'sqrt', 'log2'],\n     'min_impurity_decrease' : np.linspace(0,0.9,5),\n     'class_weight' : [None, 'balanced'],\n     'ccp_alpha' : np.linspace(0,0.9,5)\n    }\n]","e53ce8d5":"clf_grid = GridSearchCV(model, param_grid= param_grid, cv = 5, verbose=True, n_jobs=-1)","92426514":"%%time\n# best_clf_grid = clf_grid.fit(x_train, y_train)","a129dffe":"# best_clf_grid.best_estimator_","91cb2f0a":"# est = best_clf_grid.best_estimator_\nest = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=10.0, max_features='sqrt', max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=26,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='random')","6bbd76b3":"est.fit(x_train, y_train)\ny_pred = est.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","d8d70534":"model = RandomForestClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","39432480":"param_grid = [    \n    {'n_estimators' : np.logspace(2,4,3).astype(int),\n     'criterion' : ['gini', 'entropy'],\n     'max_depth' : list(np.logspace(0,4,5)) + ['None'],\n     'min_samples_split' : np.linspace(2,100,5).astype(int),\n     'max_features' : ['auto', 'sqrt', 'log2'],\n     'class_weight' : [None, 'balanced'],\n     'bootstrap' : ['True','False'],\n     'oob_score' : ['True','False']\n    }\n]","a4ac3d43":"# clf_grid = RandomizedSearchCV(model, param_distributions= param_grid, cv = 5, verbose=True, n_jobs=-1, n_iter=100)","5a68c70d":"%%time\n# best_clf_grid = clf_grid.fit(x_train, y_train)","86a3dc03":"# best_clf_grid.best_estimator_","49e7549d":"# est = best_clf_grid.best_estimator_\nest = RandomForestClassifier(bootstrap='False', max_depth=10.0, max_features='sqrt',\n                       min_samples_split=51, n_estimators=1000,\n                       oob_score='False')","7486e385":"est.fit(x_train, y_train)\ny_pred = est.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","b690d0c7":"model = MLPClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","d06a8a55":"param_grid = [    \n    {'hidden_layer_sizes' : [(100),(100,100),(100,100,100)],\n     'activation' : ['identity', 'logistic', 'tanh', 'relu'],\n     'alpha' : np.logspace(-4, 0, 5),\n     'learning_rate' : ['constant', 'invscaling', 'adaptive'],\n     'max_iter' : [100,200,500]\n    }\n]","269edec0":"clf_grid = GridSearchCV(model, param_grid= param_grid, cv = 5, verbose=True, n_jobs=-1)","5040deea":"%%time\n# best_clf_grid = clf_grid.fit(x_train, y_train)","ccd98a10":"# best_clf_grid.best_estimator_","56230c1a":"# est = best_clf_grid.best_estimator_\nest = MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n              hidden_layer_sizes=100, learning_rate='invscaling',\n              learning_rate_init=0.001, max_fun=15000, max_iter=100,\n              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n              tol=0.0001, validation_fraction=0.1, verbose=False,\n              warm_start=False)","ac3e3612":"est.fit(x_train, y_train)\ny_pred = est.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","145b42fc":"model = AdaBoostClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","7911e283":"est = LogisticRegression()\nmodel = AdaBoostClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","3c67ae98":"est = ExtraTreeClassifier()\nmodel = AdaBoostClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","a00b132c":"est = DecisionTreeClassifier()\nmodel = AdaBoostClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","479b2ce0":"est = RandomForestClassifier()\nmodel = AdaBoostClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","5bc61066":"est = ExtraTreesClassifier()\nmodel = AdaBoostClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","4f269b87":"model = BaggingClassifier()\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","bac64a01":"est = LogisticRegression()\nmodel = BaggingClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","7a61984c":"est = ExtraTreeClassifier()\nmodel = BaggingClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","531ded2d":"est = DecisionTreeClassifier()\nmodel = BaggingClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","663e0495":"est = RandomForestClassifier()\nmodel = BaggingClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","daeb20cb":"est = ExtraTreesClassifier()\nmodel = BaggingClassifier(base_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","20bc948f":"est = LogisticRegression()\nestimators = [('lr',LogisticRegression()),\n              ('rc',RidgeClassifier()),\n              ('et',ExtraTreeClassifier()),\n              ('dt',DecisionTreeClassifier()),\n              ('rf',RandomForestClassifier()),\n              ('ml',MLPClassifier()),\n              ('ab',AdaBoostClassifier()),\n              ('bc',BaggingClassifier()),\n              ('xg',XGBClassifier())]\nmodel = StackingClassifier(estimators=estimators, final_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","8fe83bf7":"est = ExtraTreeClassifier()\nestimators = [('lr',LogisticRegression()),\n              ('rc',RidgeClassifier()),\n              ('et',ExtraTreeClassifier()),\n              ('dt',DecisionTreeClassifier()),\n              ('rf',RandomForestClassifier()),\n              ('ml',MLPClassifier()),\n              ('ab',AdaBoostClassifier()),\n              ('bc',BaggingClassifier()),\n              ('xg',XGBClassifier())]\nmodel = StackingClassifier(estimators=estimators, final_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","e123d120":"est = DecisionTreeClassifier()\nestimators = [('lr',LogisticRegression()),\n              ('rc',RidgeClassifier()),\n              ('et',ExtraTreeClassifier()),\n              ('dt',DecisionTreeClassifier()),\n              ('rf',RandomForestClassifier()),\n              ('ml',MLPClassifier()),\n              ('ab',AdaBoostClassifier()),\n              ('bc',BaggingClassifier()),\n              ('xg',XGBClassifier())]\nmodel = StackingClassifier(estimators=estimators, final_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","b2ad07ba":"est = RandomForestClassifier()\nestimators = [('lr',LogisticRegression()),\n              ('rc',RidgeClassifier()),\n              ('et',ExtraTreeClassifier()),\n              ('dt',DecisionTreeClassifier()),\n              ('rf',RandomForestClassifier()),\n              ('ml',MLPClassifier()),\n              ('ab',AdaBoostClassifier()),\n              ('bc',BaggingClassifier()),\n              ('xg',XGBClassifier())]\nmodel = StackingClassifier(estimators=estimators, final_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","81c70a9c":"est = ExtraTreesClassifier()\nestimators = [('lr',LogisticRegression()),\n              ('rc',RidgeClassifier()),\n              ('et',ExtraTreeClassifier()),\n              ('dt',DecisionTreeClassifier()),\n              ('rf',RandomForestClassifier()),\n              ('ml',MLPClassifier()),\n              ('ab',AdaBoostClassifier()),\n              ('bc',BaggingClassifier()),\n              ('xg',XGBClassifier())]\nmodel = StackingClassifier(estimators=estimators, final_estimator=est)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","cbc7df08":"estimators = [('lr',LogisticRegression()),\n              ('rc',RidgeClassifier()),\n              ('et',ExtraTreeClassifier()),\n              ('dt',DecisionTreeClassifier()),\n              ('rf',RandomForestClassifier()),\n              ('ml',MLPClassifier()),\n              ('ab',AdaBoostClassifier()),\n              ('bc',BaggingClassifier()),\n              ('xg',XGBClassifier())]\nmodel = VotingClassifier(estimators=estimators,voting='hard')\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","d20aa0b0":"estimators = [('lr',LogisticRegression()),\n              ('et',ExtraTreeClassifier()),\n              ('dt',DecisionTreeClassifier()),\n              ('rf',RandomForestClassifier()),\n              ('ml',MLPClassifier()),\n              ('ab',AdaBoostClassifier()),\n              ('bc',BaggingClassifier()),\n              ('xg',XGBClassifier())]\nmodel = VotingClassifier(estimators=estimators,voting='soft')\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","cbffa166":"model = XGBClassifier(nthread=-1)\nmodel.fit(x_train, y_train)\ny_pred = model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","55d408bc":"param_grid = [    \n    {'booster' : ['gbtree','dart'],\n     \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n     \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n     \"min_child_weight\" : [ 1, 3, 5, 7 ],\n     \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n     \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    }\n]","225267c7":"clf_grid = GridSearchCV(model, param_grid= param_grid, cv = 5, verbose=True, n_jobs=-1)","6da0559d":"%%time\n# best_clf_grid = clf_grid.fit(x_train, y_train)","e7426bd2":"# best_clf_grid.best_estimator_","33821c93":"# est = best_clf_grid.best_estimator_\nest = XGBClassifier(base_score=0.5, booster='dart', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0.0,\n              learning_rate=0.25, max_delta_step=0, max_depth=12,\n              min_child_weight=7, missing=None, n_estimators=100, n_jobs=1,\n              nthread=-1, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n              silent=None, subsample=1, verbosity=1)","2f3121e7":"est.fit(x_train, y_train)\ny_pred = est.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","a59102c5":"est = LogisticRegression()\nestimators = [('lr',LogisticRegression()),\n              ('rc',RidgeClassifier()),\n              ('et',ExtraTreeClassifier()),\n              ('dt',DecisionTreeClassifier()),\n              ('rf',RandomForestClassifier()),\n              ('ml',MLPClassifier()),\n              ('ab',AdaBoostClassifier()),\n              ('bc',BaggingClassifier()),\n              ('xg',XGBClassifier())]\nfinal_model = StackingClassifier(estimators=estimators, final_estimator=est)\nfinal_model.fit(x_train, y_train)\ny_pred = final_model.predict(x_val)\nprint(accuracy_score(y_val, y_pred))\nprint(confusion_matrix(y_val, y_pred))\nprint(classification_report(y_val, y_pred))","e7663f53":"test = pd.read_csv('pulsar_data_test.csv')","4a17d3ae":"test_copy = pd.DataFrame(imputer.transform(test), columns=train.columns.values.tolist())","a61d12c3":"for cols in test_copy.columns[:-1]:\n  test_copy[cols] = np.where(test_copy[cols]>upper_range[cols],\n                                    upper_range[cols],test_copy[cols])\n  test_copy[cols] = np.where(test_copy[cols]<lower_range[cols],\n                                    lower_range[cols],test_copy[cols])","347ebbda":"X_test = test_copy.drop([' Excess kurtosis of the integrated profile', \n                                     ' Skewness of the DM-SNR curve', \n                                     ' Standard deviation of the DM-SNR curve'], axis=1)\nX_test = X_test.copy().drop(['target_class'], axis = 1)","85644b22":"y_test_pred = final_model.predict(X_test)","9636412a":"sns.histplot(x=y_test_pred)","6ca8ed22":"Results (using macro avg F1-score and confusion matrix):\n\n\n\n1.   Logistic Regression = 0.92 (49\/13)\n2.   Ridge Classifier = 0.92 (55\/12)\n3.   ExtraTree Classifier = 0.92 (52\/12)\n4.   Decision Tree = 0.92 (49\/17)\n5.   Random Forests = 0.92 (Random Search) (55\/9)\n6.   MLP Classifier = 0.92 (47\/15)\n7. AdaBoost Classifier = 0.92 (53\/8)\n8. Bagging Classifier = 0.92 (48\/14)\n9. Stacking Classifier = 0.93 (47\/13)\n10. Voting Classifier = 0.92 (49\/11)\n11. XGBoost Classifier = 0.92 (51\/11)\n\nStacking classifier seems to work best. Now will get best performing  model\n","85c27a2d":"### Correlations between 2 dependent variables\n\n**Highly positively correlated:**\n1. Skewness of the integrated profile and Excess kurtosis of the DM-SNR curve\n2. Skewness of the DM-SNR curve and Excess kurtosis of the DM-SNR curve\n3. Mean the DM-SNR curve and Standard Deviation of the DM-SNR curve\n\n**Highly negatively correlated:**\n1. Mean of the integrated profile and Excess kurtosis of the integrated profile\n2. Mean of the integrated profile and Skewness of the integrated profile\n3. Excess kurtosis the DM-SNR curve and Standard Deviation of the DM-SNR curve\n\n### Correlations between independent and dependent variable\n\n**Highly positively correlated:**\n1. Excess kurtosis of the integrated profile\n2. Skewness of the integrated profile \n\n**Highly negatively correlated:**\n1. Mean of the integrated profile","b04be5b2":"Comparing the mean and std of the train and imputed train datasets, we see they are almost the same. Moreover, our iterative imputer looks at all available features to predict the missing values","4d0b8c87":"### Integrated Profile\n**Mean** Non pulsars have a higher mean, as well as more outliers.\n**Std. Dev** Non pulsars have a higher deviation as well, with a longer outlier tail\n**Excess Kurtosis** Non pulsars have a lower value, as smaller distribution\n**Skewness** Non pulsars have a smaller, less deviated value (almost 0)\n\n### DM-SNR Curve\n**Mean** Non pulsars have a much lower mean, as well as fewer outler.\n**Std. Dev** Non pulsars have a lower deviation, with a longer outlier tail\n**Excess Kurtosis** Non pulsars have a higher value, and similar distributions\n**Skewness** Non pulsars a larger spread of skewness, as well as a larger value","449664a5":"## Bagging Classifier","195a0df8":"### MLP Classifier","2078faab":"# Feature selection\n\nSince there are only 8 features, dimensionality reduction is not nescessary ","276ad401":"## NULL Detection","080ccc74":"### Outlier detection","7d2a47f2":"### Integrated Profile\nThis curve seems to be a normal distribution. which is symmetric about the mean and regular thickness of tails\n\n#### 1. Mean\nSince mean is close to 50% mark and 25% and 75% are within 1 std. dev. from the mean. Large difference between min and max, hence larger std. dev. Hence the mean of the integrated profile seems volatile with a large standard deviation. \n\n#### 2. Std. Dev\nSince mean is close to 50% mark and 25% and 75% are within less than 1 std. dev. from the mean, this data is normally distributed. The max and min values are 3-4 standard deviations from the mean.\n\n#### 3. Excess Kurtosis (Missing Values)\nMajority of this data (atleast 75%) is less than mean. Hence a large head portion in this distribution. Hence the distribution of the left of mean is more tightly spread than the right. This means the integrated profile's tails are generally the same size as normal distributions.\n\n#### 4. Skewness\nMajority of this data (definitely more 75%) is less than mean. Hence a large head portion in this distribution. Hence the distribution of the left of mean is more tightly spread than the right. Hence the integrated profile must not be very skewed.\n\n### DM-SNR Curve\n\nThis curve is expected to be more highly spread than a normal distribution, and skewed towards the right of the mean. \n\n#### 1. Mean\nStandard deviation of the mean is very high, with more than 75% values being less than mean. Hence mean of most curves are small\n\n#### 2. Std. Dev (Missing Values)\nVery skewed towards the higher side. Can expect most values to have standard deviation lower than 28. However, comparing this with the distribution of DM-SNR curve, which is mostly less than 5. This is a very highly spread DM-SNR curve.\n\n#### 3. Excess Kurtosis\nThis is normally distributed since mean = 50% value and every 25% is approx one std. dev. However, these values are high, hence the DM-SNR curve has fatter tails than normal distribution. \n\n#### 4. Skewness (Missing Values)\nVery high values, and high standard deviation. Hence the DM-SNR curve is very skewed (expected)","9db938a0":"Since there are not many outliers (less than 10 percent for most columns) we can either remove them or cap them.\nHowever, removing them is not advised, so we will cap them using IQR","dbc0ddd9":"### Integrated Profile\n\nWe can expect a mean around 50 with a high standard deviation almost comparable to the mean. Not very skewed (slightly towards the right) and slightly fat tails.\n\n**Mean** Very few outliers, amost a normal distribution except for a large left tail\n\n**Std. Dev** Almost no outliers, a normal distribution with a tail to right of mean\n\n**Excess Kurtosis** Highly crowded, few outliers in the form of a long tail to the right\n\n**Skewness** Highly crowded, few outliers in the form of a long tail to the right\n\n\n### DM-SNR Curve\n\nWe can expect a mean around 50 with a high standard deviation almost comparable to the mean. Not very skewed (slightly towards the right) and slightly fat tails.\n\n**Mean** Highly crowded, skewed towards the right, with a long tail.\n\n**Std. Dev** Right skewed, with a fat right tail.\n\n**Excess Kurtosis** Left skewed normal distribution. Minimal tails.\n\n**Skewness** Highly right skewed with a long right tail","add88c68":"### PREDICTING A PULSAR STAR\n\nHTRU2 is a data set which describes a sample of pulsar candidates collected during the High Time Resolution Universe Survey .\n\nPulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the inter-stellar medium, and states of matter .\n\nAs pulsars rotate, their emission beam sweeps across the sky, and when this crosses our line of sight, produces a detectable pattern of broadband radio emission. As pulsars\nrotate rapidly, this pattern repeats periodically. Thus pulsar search involves looking for periodic radio signals with large radio telescopes.\n\nEach pulsar produces a slightly different emission pattern, which varies slightly with each rotation . Thus a potential signal detection known as a 'candidate', is averaged over many rotations of the pulsar, as determined by the length of an observation. In the absence of additional info, each candidate could potentially describe a real pulsar. However in practice almost all detections are caused by radio frequency interference (RFI) and noise, making legitimate signals hard to find.\n\nMachine learning tools are now being used to automatically label pulsar candidates to facilitate rapid analysis. Classification systems in particular are being widely adopted,\nwhich treat the candidate data sets as binary classification problems. Here the legitimate pulsar examples are a minority positive class, and spurious examples the majority negative class.\n\nThe data set shared here contains 16,259 spurious examples caused by RFI\/noise, and 1,639 real pulsar examples. These examples have all been checked by human annotators.\n\nEach row lists the variables first, and the class label is the final entry. The class labels used are 0 (negative) and 1 (positive).","4a21045f":"### Logistic Regression","230ce708":"## Stacking Classifier","212c217a":"## Correlation Heatmap","b2724448":"### Ridge Classifier","b30f8039":"# Results","671c4a84":"# Data loading and viewing","528fca8d":"# Model Training","1d1f23de":"## Voting Classifier","73467315":"## AdaBoost Classifier","b5ae8adc":"3 columns have null values (Roughly 5% or 10%)","22252bb2":"# Outlier Treatment","b2c85043":"## XGBoost Classifier","c97fd929":"### Random Forest Classifier","b9e2f33c":"We can see that multiple NaN values exist in 'Standard deviation of the DM-SNR curve' (missing values)","c4361ded":"We see that Excess kurtosis and Skewnesscurve are highly correlated (95 percent for DM-SNR and 76 percent for IP). Also their correlation with other features is very similar (10% difference at max) so we can drop any one of these. Also, the Mean and Standard Deviation of the DM-SNR curve is highly correlated. So we can drop any one of these as well. \n\nHowever, Excess kurtosis of the integrated profile,Skewness of the DM-SNR curve and Standard Deviation of the DM-SNR curve had missing values. \n\nAlso, Skewness of the integrated profile had more outliers than Excess kurtosis of the integrated profile and Skewness of the DM-SNR curve had more missing values than Excess kurtosis of the DM-SNR curve\n\nTherefore Skewness and Standard Deviation of the DM-SNR curve is dropped. Also Excess kurtosis of the integrated profile had almost 13% missing values (compared to 2% difference in outliers) So Excess kurtosis of the integrated profile is dropped.\n\n","f2a97dfd":"### Target class based distributions","d382edaf":"The splits are similarly distributed. They can be used to train the model","329b56f1":"### Histogram Plots","be66cf01":"All continuous values","80f7bafe":"# Feature analysis","628c7a1e":"#### *Downloading* data and setup\n","4b4afb3f":"### Decision Tree Classifier","95373759":"# Predicting a Pulsar Star | Kaggle\n\nNilaksh Agarwal","225681db":"### Attribute Information:\n\nEach candidate is described by 8 continuous variables, and a single class variable. The first four are simple statistics obtained from the integrated pulse profile (folded profile). This is an array of continuous variables that describe a longitude-resolved version of the signal that has been averaged in both time and frequency . The remaining four variables are similarly obtained from the DM-SNR curve . These are summarised below:\n\nDM-SNR - Dispersion Measure of the Signal to Noise Ratio\n\nIntegrated profile - Folding signals w.r.t rotational period\n\n1. Mean of the integrated profile.\n2. Standard deviation of the integrated profile.\n3. Excess kurtosis of the integrated profile.\n4. Skewness of the integrated profile.\n5. Mean of the DM-SNR curve.\n6. Standard deviation of the DM-SNR curve.\n7. Excess kurtosis of the DM-SNR curve.\n8. Skewness of the DM-SNR curve.\n9. Class\n\nHTRU 2 Summary\n* 17,898 total examples.\n* 1,639 positive examples.\n* 16,259 negative examples.","3f2027e5":"Splitting into Train and Test","ae74659e":"So, we use bayesian ridge estimator to find the missing values from all other available attributes","630da20f":"# Missing Data Calculation","5b1a8466":"# Predictions","533e0c47":"So, we can see\nCol1: Excess kurtosis of the integrated profile\n\nCol2: Standard deviation of the DM-SNR curve\n\nCol3: Skewness of the DM-SNR curve\n\nAmong the 3255 rows with null vaues,\n1. 140 rows have Col1 and Col2 as NaN\n2. 91 rows have Col1 and Col3 as NaN\n3. 57 rows have Col2 and Col3 as NaN\n4. 5 rows have Col1, Col2 and Col3 as NaN\n\nThis means 278 rows have more than 1 value NaN","44f53519":"Null in 2 columns","01bf5cc1":"### ExtraTree Classifier","cf3740a2":"Class imbalance (10:1 almost)","d6382608":"We see that most features come from independent distributions, however have significant overlap.","7f9908df":"Now we impute the missing values using an iterative imputer (using all other features to find the missing values)"}}