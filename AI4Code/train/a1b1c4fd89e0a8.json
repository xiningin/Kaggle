{"cell_type":{"57838970":"code","4d0a2a49":"code","881a866e":"code","e6012e88":"code","5e51e7ee":"code","ff59120c":"code","fa6ae343":"code","84a54b10":"code","0d3af451":"code","7e4d39b7":"code","326d7523":"code","36829df4":"code","15b8bf09":"code","a9e40940":"code","4543d929":"code","4d37ff84":"code","957e0f79":"code","8602c169":"markdown","10cd435d":"markdown","c74038cd":"markdown","6364a570":"markdown","ca0f7998":"markdown"},"source":{"57838970":"!pip install pycaret[full]","4d0a2a49":"!pip install shap","881a866e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n#from sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_auc_score\nimport gc\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom pycaret.classification import *\nimport shap","e6012e88":"df_train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')","5e51e7ee":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ntrain = reduce_mem_usage(df_train.drop(['id'], axis=1))\ntest = reduce_mem_usage(df_test.drop(['id'], axis=1))\n\ndel df_train\ndel df_test\ngc.collect()","ff59120c":"train","fa6ae343":"test","84a54b10":"train.info()","0d3af451":"test.info()","7e4d39b7":"def do_pycaret(target, train, test):\n    clf = setup(data=train, target=target, silent= True, session_id=42)\n    #add_metric('logloss', 'LogLoss', log_loss, greater_is_better=False, target='pred_proba')\n    add_metric('roc_auc', 'roc_auc', roc_auc_score, greater_is_better=False, target='pred_proba')\n    lightgbm = create_model(\"lightgbm\", fold=5)\n    tuned = tune_model(lightgbm, fold=5)\n    predh = predict_model(tuned)\n    final = finalize_model(tuned)\n    prep_pipe = get_config('prep_pipe')\n    prep_pipe.steps.append(['trained_model', final])\n    pred = prep_pipe.predict_proba(test)\n    return(final, pred)","326d7523":"final, pred = do_pycaret('claim', train, test)","36829df4":"evaluate_model(final)","15b8bf09":"pred","a9e40940":"plot_model(final, plot=\"auc\")","4543d929":"plot_model(final, plot=\"feature\")","4d37ff84":"interpret_model(final)","957e0f79":"submission = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")\nsubmission.claim  = pred[:, 1]\nsubmission.to_csv('submission.csv',index=False)\nsubmission","8602c169":"# Tabular Playground Series - Sep 2021\n\n## Setup\n___","10cd435d":"## Submission","c74038cd":"## Model Analysis","6364a570":"## Predict with PyCaret (LightGBM)\n___","ca0f7998":"## Overview\n___\nThere are missing values, but since PyCaret assigns the average value by default We will leave it as it is."}}