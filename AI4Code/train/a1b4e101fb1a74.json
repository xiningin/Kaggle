{"cell_type":{"016cdbc6":"code","4230eee9":"markdown","0518ba8e":"markdown","6cb76f58":"markdown","61504451":"markdown","328b3f54":"markdown","0bb4d0fe":"markdown","3c909d0c":"markdown","b7e099f9":"markdown","5e85f900":"markdown","5f877d9c":"markdown","544c2ef1":"markdown","74830756":"markdown","eefb415c":"markdown","d922d117":"markdown","a869946c":"markdown"},"source":{"016cdbc6":"import requests\nhost = \"34.223.223.77\"\nurl = \"http:\/\/cord19.covid19insights.org:4004\/search\"\nrequest_headers = {\"content-type\": \"application\/json\"}","4230eee9":"# Code for Search api \n\n Code for this search api at http:\/\/cord19.covid19insights.org:4004\/search  is checked in **public github repo with Apache v2 license: https:\/\/github.com\/covid19-cord19\/cord19** and also imported here inside :\n \n>  All code - \"\/kaggle\/input\/code-repo-for-all-code\/\"\n\n>  Flask Server code - \"\/kaggle\/input\/code-repo-for-all-code\/app.py\"\n\n## Flask server implementation which takes user questions and tasks:\n      - cleans and pre-process for stop words \n      \n      - lookups from precomputed idf from \"\/kaggle\/input\/code-repo-for-all-code\/dictionary\/dictIDF.txt\"\n          - code for computing idf is at \"\/kaggle\/input\/code-repo-for-all-code\/document_search\/document_search_engine.py\"\n          \n      - sends the query to solr server - which can be running \n          - assumption here is that all the 130k research artciles are pre-ingested to Solr\n          -  Code file for solr_ingestion : \/kaggle\/input\/code-repo-for-all-code\/solr_intercepts\/solr_ingestor.py\n              -  It reads all the articles .\/data and send it to solr which needs a core creation before ingestion, script file for core -ingestion is at : \/kaggle\/input\/code-repo-for-all-code\/solr_intercepts\/core_creation.sh\n              \n      - Gets the results from Solr\n      \n      - Computes sentence vector embedding\n          - from tensorflow hub sentence vector embedding model\n          \n      - Runs cosine distance computation to identify similarity score\n      \n      - Returns results in most relevant order\n      \n## Visualization - \nThe calls to apis were working before the submission but for some reason stopped working now. Since we could not visualize all this experience within the notebook here - we have integrated this code on 1 machine on AWS server: 34.223.223.77 and also created a url for this host cord19.covid19insights.org - the whole experience can be visualized by browsing through this website at : http:\/\/cord19.covid19insights.org\n \n\n \n","0518ba8e":"```python\nrequest_data = {\n    \"task\": \"What do we know about vaccines and therapeutics? Effectiveness of drugs being developed and tried to treat COVID-19 patients \",\n    \"subtask\": \"Effectiveness of drugs being developed and tried to treat COVID-19 patients. Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication.\"\n}\n# post data to url\nresponse = requests.post(url, json=request_data, headers=request_headers) \n#store response\ndata = response.json()[0]\n\n\nprint(data['title'], data['body'], data['abstract'], data['url'])\n\n```","6cb76f58":"#### Could not run as this was taking lots of time on Kaggle but works as is.\n```python\n\nimport plotly.graph_objs as go\nimport plotly.offline as plt\nhelo = [go.Scatter3d(x = tsne_df[\"Dim1\"],\n                    y = tsne_df[\"Dim2\"],\n                    z = tsne_df[\"Dim3\"],\n                    mode = 'markers+text',\n                    textposition='middle center',\n                    hoverinfo = 'text',\n                    text = tsne_df[\"label\"],\n                    marker=dict(size=15,color='turquoise',opacity=0.8))]\n \nlayout = go.Layout(title='Sentence Similarity')\nfig = go.Figure(data=helo, layout=layout)\ndisplay(plt.iplot(fig))\n# 10th is the Task Query\n```","61504451":"3-D t-SNE Plot for documents matching with question on basis of sentence similarity![image.png](attachment:image.png)","328b3f54":"Web result ( top 2) from website: \n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/Task_sub_task3.png\" width=\"1200px\"\/>","0bb4d0fe":"We were able to see the results before but now these apis are erroring because of connection error, but here is web result ( top 2) from website http:\/\/cord19.covid19insights.org )\n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/Task_sub_task5.png\" width=\"1200px\"\/>","3c909d0c":"# CORD-19 solution\n\nOverall experience can be visualized using website we created for CORD-19 challenge at http:\/\/cord19.covid19insights.org\/\n\nWe also imported all the code from github to kaggle as \/kaggle\/input\/code-repo-for-all-code\/\n\n# Approach\n\nWe attempted to solve the CORD-19 research challenge by applying two main techniques - \n\n 1. Reducing the **search space !**\n 2. Surface documents based on **similarity rankings**\n 3. Use Sentence vector embeddings from tensorflow hub\n 4. Complete code at Public Github repository with Apache License v2: https:\/\/github.com\/covid19-cord19\/cord19\n 5. Step by step process to run the code can be found at : https:\/\/github.com\/covid19-cord19\/cord19\/blob\/master\/solr_intercepts\/README.md\n\n## Reducing search space\n\nOur solution uses a search engine to crawl and ingest all the 138,000 scholarly articles including 68,000 research articles and associated relevant metadata attributes with the document - title, abstract and body. We selected Solr with Lucene indexes as the search engine to help us reduce 138k search space to top 100 documents using tf-idf and idf boosting technique.\n\n### Solr as Search Engine - \n\nSolr provides many advanced searching capabilities. Here\u2019s how Solr performs the following operations in a sequence to search for a document:\n\n 1. **Indexing** : As the Cord19 files are already in JSON format, we can upload them directly to Solr by calling the index request handler (or simply index handler). \n\n 2. **Querying** : We can search for various terms such as keywords, images or geolocation data, for instance. When you send a query, Solr processes it with a query request handles (or simply query handler) that works similarly to the index handler, only that is used to return documents from the Solr index instead of uploading them.  \n\n\n```python\ndef send_for_solr_indexing(doc, env):\n    \"\"\"\n     Sends the text document for indexing\n     params:\n     doc: text document to be indexed\n     env: dev or prod\n    \"\"\"\n    if(env):\n        solr = pysolr.Solr(config[\"solr_url\"+\"_\"+env], timeout=10)\n    else:\n        solr = pysolr.Solr(config[\"solr_url\"], timeout=10)\n   \n    solr.add(doc)\n    solr.optimize()  \n\n```\n\n**Code snippet**\nTo fine tune Solr search to return right subset of documents, we need to identify the right query terms to boost.  This is done by using term\u2019s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents.\nDuring the document ingestion process we generate the idf dictionary for the entire document corpus that serves as a look up to identify boost parameter values.\n\n\n```python\ndef process_request(self, term):\n    \"\"\"\n     Process request for re-computed inverse document frequency so we can boost query terms.\n     \n     params:\n     term: new terms to be added\n    \"\"\"\n    self.dictionary = self.load_dictionary(\".\/dictionary\", \"dictIDF.txt\")\n    term_weights = {}\n    for term in terms:\n        try:\n            if term in self.dictionary:\n                term_weights[term]  = self.dictionary[term]\n            else:\n                term_weights[term]  = 5.0\n        except:\n            pass\n    solr_query = ' '.join(['{0}:{3}^{4} {1}:{3}^{4} {2}:{3}^{4}'.format(\"body\", \"abstract\", \"title\", term, term_weights[term]) for term in terms])\n    print(solr_query)\n    return solr_query\n```\n\n 3. **Ranking the Results** : As it matches indexed documents to a query, Solr ranks the results by their relevance score \u2013 the most relevant hits appear at the top of the matched documents\n \n \nOverall approach in architecture diagram\n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/covid19_2.png\" width=\"1500px\"\/>\n\n\n<br>\n<br>\n<br>\n<br>\n\n## Sentence vector embeddings from tensorflow hub\n\nSemantic similarity is a measure of the degree to which two pieces of text carry the same meaning. This is broadly useful in obtaining good coverage over the numerous ways that a thought can be expressed using language without needing to manually enumerate them.\n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/sentence_embedding.png\" width=\"800px\"\/>\n\nWe use Tensorflow hub module for universal-sentence-encoder, The Universal Sentence Encoder encodes text into high-dimensional vectors that can be used for text classification, semantic similarity, clustering and other natural language tasks.The model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. It is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. The input is variable length English text and the output is a 512 dimensional vector.\n\nOne of the example use case:\n\n```python\nimport tensorflow_hub as hub\n\nembed = hub.load(\"https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/3\")\nembeddings = embed([\n    \"The quick brown fox jumps over the lazy dog.\",\n    \"I am a sentence for which I would like to get its embedding\"])[\"outputs\"]\n\nprint(embeddings)\n\n# The following are example embedding output of 512 dimensions per sentence\n# Embedding for: The quick brown fox jumps over the lazy dog.\n# [-0.03133016 -0.06338634 -0.01607501, ...]\n# Embedding for: I am a sentence for which I would like to get its embedding.\n# [0.05080863 -0.0165243   0.01573782, ...]\n\n```\nTypical model pipeline for sentence-encoder in google based flow looks like: \n\nSentence Semantic Similarity solution\n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/Tf-hub_sentence_semantic_similarity.png\" width=\"800px\"\/>\n\n## Document similarity \n\nDocument similarity (or distance between documents) is a one of the central themes in Information Retrieval. How humans usually define how similar are documents? Usually documents treated as similar if they are semantically close and describe similar concepts. \nClassical approach from computational linguistics is to measure similarity based on the content overlap between documents. For this we will represent documents as sentence embeddings (vectors) , so each document will be a sparse vector. And define measure of overlap as angle between vectors\n\nSimilarity (doc1, doc2) = Cos(\u03b8) = (doc1 . doc2) \/ (||doc1|| . ||doc2||)\n\n\tWhere doc1 and doc2 are task and document embedding vectors\n\nThe resulting similarity ranges from \u22121 meaning exactly opposite, to 1 meaning exactly the same, with 0 indicating orthogonality or decorrelation, while in-between values indicate intermediate similarity or dissimilarity.\n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/cosine_similarity.png\" width=\"800px\"\/>\n\nSubset of documents (result from Solr search) is used to similarity check and rank them in descending order of similarity score\n\n```python\n\n    for task in range(0, len(taskEmbeddings)):\n         j = 1\n         for doc in range(0, len(docsEmbeddings)):\n             cos_sim = cosine_similarity(taskEmbeddings[task].reshape(1, -1), docsEmbeddings[doc].reshape(1, -1))\n             score.append(round(cos_sim[0][0], 3))\n             print('Cosine similarity: Task {0} Document {1} = {2}'.format(i, j, round(cos_sim[0][0], 3)))\n             j = j + 1\n         i = i + 1\n    documentDF['score'] = score\n    documentDF.sort_values(by=['score'], ascending=False, inplace=True)\n\n```\n\n### How to make it work on your local\n\nALL THE INSTRUCTIONS ARE IN README.md file at : \n  http:\/\/github.com\/covid19-cord19\/cord19\/tree\/master\/solr_intercepts\n  \n  ","b7e099f9":"```python\n\nrequest_data = {\n    \"task\": \"What do we know about vaccines and therapeutics? Effectiveness of drugs being developed and tried to treat COVID-19 patients \",\n    \"subtask\": \"Alternative models to aid decision makers in determining how to prioritize and distribute scarce, newly proven therapeutics as production ramps up. This could include identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need.\"\n}\n# post data to url\nresponse = requests.post(url, json=request_data, headers=request_headers) \n#store response\ndata = response.json()[0]\n\n\nprint(data['title'], data['body'], data['abstract'], data['url'])\n\n```","5e85f900":"#### Could not run as this was taking lots of time on Kaggle but works as is.\n```python\ndf1= pd.DataFrame(taskEmbeddings)\ndf2= pd.DataFrame(docsEmbeddings)\nfinal_df = pd.concat([df2, df1], axis=0)\ntotal_docs = messages_docs+messages_task\n \nfrom sklearn.manifold import TSNE\nmodel = TSNE(n_components=3, verbose=1,random_state=0, metric=\"cosine\",perplexity = 50,n_iter = 10000)\ntsne_data_fit = model.fit_transform(final_df.to_numpy())\ntsne_data = np.vstack((tsne_data_fit.T,total_docs)).T\n \ntsne_df = pd.DataFrame(data=tsne_data,columns={\"Dim1\",\"Dim2\",\"Dim3\",\"label\"})\ntsne_df.columns = ['Dim1','Dim2','Dim3','sentence']\ntsne_df[\"label\"] = tsne_df.index\ntsne_df.iloc[tsne_df.shape[0]-1, 4] = \"T\"\n```","5f877d9c":"```python\nrequest_data = {\n    \"task\": \"What do we know about vaccines and therapeutics? Effectiveness of drugs being developed and tried to treat COVID-19 patients \",\n    \"subtask\": \"Methods evaluating potential complication of Antibody-Dependent Enhancement (ADE) in vaccine recipients.\"\n}\n# post data to url\nresponse = requests.post(url, json=request_data, headers=request_headers) \n#store response\ndata = response.json()[0]\n\n\nprint(data['title'], data['body'], data['abstract'], data['url'])\n```","544c2ef1":"We were able to see the results before but now these apis are erroring because of connection error, but here is web result ( top 2) from website http:\/\/cord19.covid19insights.org )\n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/Task_sub_task_4.png\" width=\"1200px\"\/>","74830756":"```python\n\nrequest_data = {\n    \"task\": \"What do we know about vaccines and therapeutics? Effectiveness of drugs being developed and tried to treat COVID-19 patients \",\n    \"subtask\": \"Capabilities to discover a therapeutic (not vaccine) for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents.\"\n}\n# post data to url\nresponse = requests.post(url, json=request_data, headers=request_headers) \n#store response\ndata = response.json()[0]\n\n\nprint(data['title'], data['body'], data['abstract'], data['url'])\n\n```","eefb415c":"Web result ( top 2) from website: \n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/Task_sub_task2.png\" width=\"1200px\"\/>","d922d117":"We were able to see the results before but now these apis are erroring because of connection error, but here is web result ( top 2) from website http:\/\/cord19.covid19insights.org )\n\nWeb result ( top 1 ) from website: \n\n<img src=\"https:\/\/github.com\/covid19-cord19\/cord19\/raw\/master\/images\/Task_sub_task1.png\" width=\"1200px\"\/>","a869946c":"```python\n\nrequest_data = {\n    \"task\": \"What do we know about vaccines and therapeutics? Effectiveness of drugs being developed and tried to treat COVID-19 patients \",\n    \"subtask\": \"Exploration of use of best animal models and their predictive value for a human vaccine.\"\n}\n# post data to url\nresponse = requests.post(url, json=request_data, headers=request_headers) \n#store response\ndata = response.json()[0]\n\n\nprint(data['title'], data['body'], data['abstract'], data['url'])\n\n```"}}