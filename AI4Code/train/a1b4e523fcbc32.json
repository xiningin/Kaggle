{"cell_type":{"adc6fb3e":"code","c8b75af1":"code","f40897b2":"code","c1711507":"code","1516f256":"code","332505c3":"code","1c2fcd5e":"code","aaab1f7b":"code","f68df791":"code","3323bae5":"code","6a063417":"code","1b3a5281":"code","400accca":"code","3c88490c":"code","ff325fc8":"code","15840dbd":"code","511047f7":"code","dac0d6c0":"code","47d28366":"code","6d172482":"code","36654204":"code","7a8130f3":"code","31e799fd":"code","cf8ca65f":"code","7643de18":"code","030d6ab2":"code","1f9a0459":"code","b95b135b":"code","76230994":"code","6db2f6fa":"code","4aba7b12":"code","15a81190":"code","43833d29":"code","22386917":"code","da0c4e3b":"code","cd3c2887":"code","33f5b47c":"code","3367007e":"code","007ff7b0":"code","bffed09a":"code","5bc05000":"code","ea57fba4":"code","b6f65108":"code","443cd9f0":"code","be70c4af":"code","dd88343e":"code","d8762808":"code","ff24f618":"code","839c0166":"code","80e9097d":"code","acdcf95d":"code","2a3bb38e":"code","8d5c53c1":"code","9c93c098":"code","16bd2200":"code","f70308a2":"code","098ae54e":"code","a2c81d8f":"code","15a47336":"code","98863554":"code","98c4c335":"code","62480a6a":"code","1fa80376":"code","5f11d043":"code","5c91aaee":"code","0d0e20dc":"code","3a7073ef":"code","84746488":"code","d4bb1213":"code","199e9fc1":"code","a2eaf6c9":"code","32153012":"code","e0ba2252":"code","01a2f830":"markdown","e04cfbd0":"markdown","5ca276f1":"markdown","9bd06840":"markdown","20591bfa":"markdown","c6229075":"markdown","8d48ce71":"markdown","ab10b6a4":"markdown","7fd3f666":"markdown","a94f3078":"markdown","49aa75b1":"markdown","beab519b":"markdown","d1c5b3fe":"markdown","cd2526d7":"markdown","4edba6b4":"markdown"},"source":{"adc6fb3e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8b75af1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nimport json\n#import tweepy\nimport os\nimport re\nimport requests","f40897b2":"#I will gather data from a variety of sources and in a variety of formats:\n\n#from :\n\n#1- Download this file manually: \n#This file (twitter_archive_enhanced.csv)\n#The WeRateDogs Twitter archive,\n\n#2- Downloaded programmatically using the requests library and the following \n#URL: https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2017\/August\/599fd2ad_image-predictions\/image-predictions.tsv  \n#This file (image_predictions.tsv)\n#The tweet image predictions, i.e., what breed of dog (or other object, animal, etc.)\n# is present in each tweet according to a neural network is hosted on Udacity's servers\n\n#3- Query the Twitter API :\n#for each tweet's JSON data using Python's Tweepy library and store each tweet's entire set of JSON data\n#in a file called tweet_json.txt file.\n#This file (tweet_json.txt)\n#Each tweet's JSON data should be written to its own line.\n#Then \n#Reading this file line by line into a pandas DataFrame with  tweet ID, retweet count, and favorite count. ","c1711507":"df_twt = pd.read_csv('..\/input\/weratedogs\/twitter-archive-enhanced.csv')","1516f256":"df_twt.sample(3)","332505c3":"df_img = pd.read_csv('..\/input\/image-predictions\/image_predictions.tsv', sep = '\\t')","1c2fcd5e":"df_img.sample(1)","aaab1f7b":"# Reading the json file\nid_twt = []\nnum_fav = []\nnum_ret = []\nwith open('..\/input\/tweet-json\/tweet_json.txt', mode = 'r') as f:\n     for line in f.readlines():\n            tweet_data = json.loads(line)\n            id_twt.append(tweet_data['id'])\n            num_fav.append(tweet_data['favorite_count'])\n            num_ret.append(tweet_data['retweet_count'])\n            \ndf_json = pd.DataFrame({'tweet_id':id_twt, 'favorite_count':num_fav, 'retweet_count':num_ret})","f68df791":"df_json.sample(1)","3323bae5":"df_json.info()","6a063417":"df_twt.tail(1)","1b3a5281":"df_img.tail(1)","400accca":"df_json.tail(1)","3c88490c":"df_twt.shape","ff325fc8":"df_img.shape","15840dbd":"df_json.shape","511047f7":"#Assess its quality and tidiness\n#visually and programmatically\n#I will document at least eight (8) quality issues and two (2) tidiness issues\n# such as requested.\n#I created a Python function that includes the required functions for assess\n#from what I learned in this course for easy and speed in performance","dac0d6c0":"def assess_data(topic, source_df, df):\n    print('Use pandas to explore and assess {} datasets  {}  in terms of quality \\\n    and tidness \\n by display data details:'.format(topic, source_df))\n    print('----------------------------------------------------------------')\n    print('Number of columns and rows :')\n    print(df.shape)\n    print('---------------------------')\n    print('The first 5 lines of the data frame :')\n    print(df.head())\n    print('----------------------------------------------------------------')\n    print('The last 5 lines of the data frame :')\n    print(df.tail())\n    print('----------------------------------------------------------------')\n    print('The sample 7 lines of the data frame :')\n    print(df.sample(7))\n    print('----------------------------------------------------------------')\n    print('Duplicate rows in the {} {} dataset'.format(topic, source_df))\n    print(df.duplicated().sum())\n    print('----------------------------------------------------------------')\n    print('Datatype of column in the {} {} dataset'.format(topic, source_df))\n    print(df.info())\n    print('----------------------------------------------------------------')\n    print('Features with missing values in the {} {} dataset'.format(topic, source_df))\n    print(df.isnull().sum())\n    print('----------------------------------------------------------------')\n    print('Number of unique values for quality in the {} {} dataset'.format(topic, source_df))\n    print(df.nunique())\n    print('----------------------------------------------------------------')\n    print('Number of rows with missing values in the {} {} dataset'.format(topic, source_df))\n    print(df.isnull().any(axis=1).sum())\n    print('----------------------------------------------------------------')","47d28366":"#assess\n## run : def\nif __name__ == '__main__':\n    t = 'We_Rate_Dogs'\n    s1 = 'from Twitter archive file'\n    s2 = 'from Tweet Image Prediction'\n    s3 = 'from Twitter API and JSON'\n\n    print('--------------------------------------------------------------')\n    print('1 - Inspect to Assess dataframe of Twitter archive file = df_twt :')\n    assess_data(t, s1, df_twt)\n    print('--------------------------------------------------------------')\n    print('2- Inspect to Assess dataframe of Tweet Image Prediction = df_img :')\n    assess_data(t, s2, df_img)\n    print('--------------------------------------------------------------')\n    print('3- Inspect to Assess dataframe of Twitter API and JSON = df_json :')\n    assess_data(t, s3, df_json)\n    print('--------------------------------------------------------------') ","6d172482":"#Tidiness issue 2 : Messy\u00a0data, also known as\u00a0untidy\u00a0data. Untidy data has\u00a0structural issues.\n#1- all data frames must be integrated into one frame with a common factor, tweet_id, to be easy to clean\n#2- each dog must have one stage, so we will create one column for the stage and ignore the null values\n\n##Quality Issues 8 : Dirty\u00a0data, also known as\u00a0low quality\u00a0data. Low quality data has\u00a0content issues.\n#1- I will drop the columns for the 4 stages as they become unnecessary or duplicate\n#2-  I will drop the columns :  'retweeted_status_id', 'retweeted_status_user_id' and 'retweeted_status_timestamp'\n    #I will keep only those rows in  archieve table that are original tweets and not retweets\n#3-  I will convert tweet_id to str\n#4- I will convert timestamp to datetime to be suitable\n#5- I will remove columns that are not needed for analysis\n#6- I will drop duplicate jpg_url  2075 - 2009 = 66\n  #To remove duplicates on specific column(s), use subset, to remove duplicates and keep last occurences, use keep\n#7- I will put a space in place of the underscore for both p1,p2 and p3\n#8- I will convert rating_numerator, rating_denominator to float to be suitable","36654204":"#clean\n#I will clean each of the issues documented while assessing.\n\n#The result will be a high quality and tidy master pandas DataFrame.\n\n# Copies of original dataframes to clean. \n\ntwt_clean = df_twt.copy()\njson_clean = df_json.copy()\nimg_clean = df_img.copy()","7a8130f3":"#Tidiness issue 2","31e799fd":"# Tidiness(1) : Merge 3 dataframes in one dataframe :\n\n#Define:\n#First, all data frames must be integrated into one frame with a common factor, tweet_id, to be easy to clean\n\n#Code:\ntwt_clean = pd.merge(twt_clean, json_clean, on = ['tweet_id'], how = 'inner')\ntwt_clean = pd.merge(twt_clean, img_clean, on ='tweet_id', how = 'inner')\n\n#Test:\ntwt_clean.info()","cf8ca65f":"# Tidiness(2) : Merge ['doggo', 'floofer', 'pupper', 'puppo'] in one Column :\n\n#Define:\n#Second, each dog must have one stage, so we will create one column for the stage and ignore the null values \n\n\n#Code:\ntwt_clean['stage'] = twt_clean[['doggo', 'floofer', 'pupper', 'puppo']].max(axis=1)\n\n#Test:\ntwt_clean.stage.value_counts().sum()","7643de18":"twt_clean.info()","030d6ab2":"#become 15 object","1f9a0459":"#Quality Issues 8 :","b95b135b":"# Quality(1) \n\n#Define:\n# I will drop the columns for the 4 stages as they become unnecessary or duplicate \n\n#Code:\ntwt_clean.drop(['doggo', 'floofer', 'pupper', 'puppo'], axis=1, inplace=True)\n\n#Test:\ntwt_clean.info()","76230994":"# Quality(2) \n\n#Define:\n# I will drop the columns :  'retweeted_status_id', 'retweeted_status_user_id' and 'retweeted_status_timestamp'\n#I will keep only those rows in  archieve table that are original tweets and not retweets\n\n#Code:\ntwt_clean = twt_clean.drop(['retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], axis=1)\n\n#Test:\ntwt_clean.info()","6db2f6fa":"# Quality(3) \n\n#Define:\n# I will convert tweet_id to str\n\n#Code:\ntwt_clean['tweet_id'] = twt_clean['tweet_id'].astype(str)\n\n#Test:\ntwt_clean.info()","4aba7b12":"# Quality(4) \n\n#Define:\n# I will convert timestamp to datetime to be suitable\n\n#Code:\ntwt_clean.timestamp = pd.to_datetime(twt_clean.timestamp)\n\n#Test:\ntwt_clean.info()","15a81190":"# Quality(5) \n\n#Define:\n# I will remove columns that are not needed for analysis\n\n#Code:\ntwt_clean.drop(['in_reply_to_status_id', 'in_reply_to_user_id'], axis=1, inplace= True)\n\n#Test:\ntwt_clean.columns","43833d29":"# Quality(6) \n\n#Define:\n# I will drop duplicate jpg_url  2075 - 2009 = 66\n#To remove duplicates on specific column(s), use subset, to remove duplicates and keep last occurences, use keep\n\n#Code:\ntwt_clean = twt_clean.drop_duplicates(subset=['jpg_url'], keep='last')\n\n#Test:\nsum(twt_clean['jpg_url'].duplicated())","22386917":"# Quality(7) \n\n#Define:\n#I will put a space in place of the underscore for both p1,p2 and p3\n\n#Code:\ntwt_clean['p1'] = twt_clean['p1'].str.replace('_', ' ')\ntwt_clean['p2'] = twt_clean['p2'].str.replace('_', ' ')\ntwt_clean['p3'] = twt_clean['p3'].str.replace('_', ' ')\n\n#Test:\ntwt_clean.head()","da0c4e3b":"# Quality(8) \n\n#Define:\n# I will convert rating_numerator, rating_denominator to float to be suitable\n\n#Code:\ntwt_clean['rating_numerator'] = twt_clean['rating_numerator'].astype(float)\ntwt_clean['rating_denominator'] = twt_clean['rating_denominator'].astype(float)\n\n#Test:\ntwt_clean.info()","cd3c2887":"twt_master = twt_clean.copy()\n# I will save twt_master dataframe to a CSV file\ntwt_master.to_csv('twitter_archive_master.csv')","33f5b47c":"#statistics \ntwt_master.describe()","3367007e":"twt_master.p1.value_counts()","007ff7b0":"twt_master.p2.value_counts()","bffed09a":"twt_master.p3.value_counts()","5bc05000":"#This indicates that the most prediction is to golden retriever and Labrador retriever","ea57fba4":"twt_master['rating_denominator'].value_counts()","b6f65108":"twt_master['rating_denominator'].value_counts().plot(kind='bar');","443cd9f0":"#This indicates that the most rating_denominator is 10 to fact dog","be70c4af":"#to display favorite_count vs stage\nplt.scatter(twt_master['favorite_count'], twt_master['stage'])\nplt.xlabel('favorite_count')\nplt.ylabel('stage')\nplt.title('stages and favorites by Scatter plot')","dd88343e":"#This indicates that the most favorites stage is pupper","d8762808":"#to display favorite_count vs retweet_count\nplt.scatter(twt_master['favorite_count'], twt_master['retweet_count'])\nplt.xlabel('favorite_count')\nplt.ylabel('retweet_count')\nplt.title('Retweets and favorites by Scatter plot')","ff24f618":"#to display p_conf is how confident the algorithm is in its prediction\n#for p1,p2 and p3","839c0166":"plt.hist(x = twt_master.p1_conf, bins = 80)\nplt.show()","80e9097d":"plt.hist(x = twt_master.p2_conf, bins = 80)\nplt.show()","acdcf95d":"plt.hist(x = twt_master.p3_conf, bins = 80)\nplt.show()","2a3bb38e":"# plot stage of dog\ntwt_master['stage'].hist();","8d5c53c1":"#This indicates that the most fact stages is pupper ","9c93c098":"twt_master['stage'].describe()","16bd2200":"import seaborn as sns\nsns.set_style('darkgrid')","f70308a2":"df = pd.read_csv('twitter_archive_master.csv')","098ae54e":"df.head()","a2c81d8f":"df.info()","15a47336":"# Report (1)\n# 2008 rows\n# 23 columns\n# there values by null but it no display because none","98863554":"df['rating_denominator'].value_counts().plot(kind='bar');","98c4c335":"#This indicates that the most rating_denominator is 10 to fact dog","62480a6a":"#to display favorite_count vs stage\nplt.scatter(df['favorite_count'], df['stage'])\nplt.xlabel('favorite_count')\nplt.ylabel('stage')\nplt.title('stages and favorites by Scatter plot')","1fa80376":"#This indicates that the most favorites stage is pupper","5f11d043":"#to display favorite_count vs retweet_count\nplt.scatter(df['favorite_count'], df['retweet_count'])\nplt.xlabel('favorite_count')\nplt.ylabel('retweet_count')\nplt.title('Retweets and favorites by Scatter plot')","5c91aaee":"#to display p_conf is how confident the algorithm is in its prediction\n#for p1,p2 and p3","0d0e20dc":"plt.hist(x = df.p1_conf, bins = 80)\nplt.show()","3a7073ef":"plt.hist(x = df.p2_conf, bins = 80)\nplt.show()","84746488":"plt.hist(x = df.p3_conf, bins = 80)\nplt.show()","d4bb1213":"# plot stage of dog\ndf['stage'].hist();","199e9fc1":"#This indicates that the most fact stages is pupper ","a2eaf6c9":"df['img_num'].value_counts().plot(kind='bar');","32153012":"df['img_num'].value_counts().plot(kind='pie');","e0ba2252":"df['stage'].value_counts().plot(kind='pie');","01a2f830":"# # job great with udacity  by Hedra Atia","e04cfbd0":"# 3- Additional Data via the Twitter API: This file ( tweet_json.txt)\u00b6\n# Accessing Project Data Without a Twitter Account\n# I can't set up a Twitter developer account so i instead follow the directions below by udacity to access the data necessary for the project.","5ca276f1":"# Gathering data","9bd06840":"# **#NOW I have 3 dataframes\n# #df_twt = 0 - 2355 = 2356 rows\n# #df_img = 0 - 2074 = 2075 rows\n# #df_json = 0 - 2353 = 2354 rows\n\n# #for sure**","20591bfa":" # 1- Download this file manually: This file (twitter_archive_enhanced.csv)","c6229075":"# 2- Downloaded manually: This file (image_predictions.tsv)","8d48ce71":"# Document at least eight (8) quality issues and two (2) tidiness issues","ab10b6a4":"# #This indicates also that the most fact stages is pupper ","7fd3f666":"# Storing data","a94f3078":"# Cleaning data","49aa75b1":"# Project WeRateDogs Twitter Data: Wrangle and Analyze Data","beab519b":"# importing libraries","d1c5b3fe":"# Analyzing and Visualizing","cd2526d7":"# #visualizations  with alot of details and Transparency and clarity \n# #by using seaborn","4edba6b4":"# Assessing data"}}