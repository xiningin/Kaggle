{"cell_type":{"9738bc03":"code","87fac57e":"code","0e546d1e":"code","095a4554":"code","f1e931ca":"code","41f1913a":"code","bdb2e3fc":"code","6c06dca0":"code","6b88325f":"code","74875999":"code","701317f0":"code","1033fef3":"code","770a4986":"code","f8d7ad8f":"code","46a9ccfd":"code","aa0c1a2b":"code","44edbf2d":"code","d7db78ab":"code","fc781f75":"code","41592356":"code","fb64e3d3":"code","5766eb51":"code","90d39186":"markdown","da5d8c50":"markdown","e28d38dc":"markdown","0c09a840":"markdown","53e00840":"markdown","d14e59dd":"markdown","2fa148c1":"markdown","82f4e744":"markdown","cc635642":"markdown","046e5766":"markdown","c94b0873":"markdown","75ec68ff":"markdown","14fb381c":"markdown","11934489":"markdown","741b80d4":"markdown"},"source":{"9738bc03":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.spatial.distance import cdist\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","87fac57e":"data = pd.read_csv(\"\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")","0e546d1e":"data.head()","095a4554":"data.sample(5)","f1e931ca":"data.info()","41f1913a":"data.isnull().sum()","bdb2e3fc":"data.describe()","6c06dca0":"lis=[]\nfor i in data['Gender'].values:\n    if i == 'Male':\n        lis.append(1)\n    else:\n        lis.append(0)\ndata['Gender'] = lis\ndata","6b88325f":"fig,axes = plt.subplots(1,3,figsize=(18,5))\nsns.distplot(data['Spending Score (1-100)'],ax=axes[0])\nsns.distplot(data[\"Annual Income (k$)\"],ax=axes[1])\nsns.distplot(data[\"Age\"],ax=axes[2])\nplt.show()","74875999":"col_names = ['Annual Income (k$)', 'Age', 'Spending Score (1-100)']\nfeatures = data[col_names]\nscaler = StandardScaler().fit(features.values)\nfeatures = scaler.transform(features.values)\nscaled_features = pd.DataFrame(features, columns = col_names)\ndf = scaled_features","701317f0":"scaled_features['Gender'] = data['Gender']\nscaled_features.head()","1033fef3":"fig,axes = plt.subplots(1,3,figsize=(18,5))\nsns.distplot(scaled_features['Spending Score (1-100)'],ax=axes[0])\nsns.distplot(scaled_features[\"Annual Income (k$)\"],ax=axes[1])\nsns.distplot(scaled_features[\"Age\"],ax=axes[2])\nplt.show()","770a4986":"distortions = []\ninertias = []\nmapping1 = {}\nmapping2 = {}\nK = range(1, 10)\n\nfor k in K:\n    # Building and fitting the model\n    kmeanModel = KMeans(n_clusters=k).fit(df)\n    kmeanModel.fit(df)\n\n    distortions.append(sum(np.min(cdist(df, kmeanModel.cluster_centers_,'euclidean'), axis=1)) \/ df.shape[0])\n    inertias.append(kmeanModel.inertia_)\n\n    mapping1[k] = sum(np.min(cdist(df, kmeanModel.cluster_centers_,'euclidean'), axis=1)) \/ df.shape[0]\n    mapping2[k] = kmeanModel.inertia_\n","f8d7ad8f":"#For Distortion\nfor key, val in mapping1.items():\n    print(f'{key} : {val}')","46a9ccfd":"#For Inertia\nfor key, val in mapping2.items():\n    print(f'{key} : {val}')","aa0c1a2b":"plt.plot(K, distortions, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method using Distortion')\nplt.show()\nplt.plot(K, inertias, 'bx-')\nplt.xlabel('Values of K')\nplt.ylabel('Inertia')\nplt.title('The Elbow Method using Distortion')\nplt.show()","44edbf2d":"plt.figure(figsize=(14,6))\nplt.title('Dataset before clustering')\nplt.scatter(df[\"Annual Income (k$)\"], df[\"Spending Score (1-100)\"])\nplt.show()","d7db78ab":"kmeans = KMeans(n_clusters=4, random_state=0).fit(df)\ndf['cluster'] = kmeans.labels_","fc781f75":"plt.figure(figsize=(14,6))\nplt.title('Dataset with 4 clusters')\nsns.scatterplot(x=df[\"Annual Income (k$)\"][df['cluster']== 0],y=df[\"Spending Score (1-100)\"][df['cluster'] == 0],palette=sns.color_palette(\"hls\",10),data=df,\n               legend=\"full\")\nsns.scatterplot(x=df[\"Annual Income (k$)\"][df['cluster']== 1],y=df[\"Spending Score (1-100)\"][df['cluster'] == 1],palette=sns.color_palette(\"hls\",10),data=df,\n               legend=\"full\")\nsns.scatterplot(x=df[\"Annual Income (k$)\"][df['cluster'] == 2],y=df[\"Spending Score (1-100)\"][df['cluster']== 2],palette=sns.color_palette(\"hls\",10),data=df,\n               legend=\"full\")\nsns.scatterplot(x=df[\"Annual Income (k$)\"][df['cluster'] == 3],y=df[\"Spending Score (1-100)\"][df['cluster'] == 3],palette=sns.color_palette(\"hls\",10),data=df,\n               legend=\"full\")\n\nplt.show()","41592356":"fig,axes = plt.subplots(1,3,figsize=(18,5))\nsns.swarmplot(df.cluster,df['Spending Score (1-100)'],ax=axes[0])\nsns.swarmplot(df.cluster,df[\"Annual Income (k$)\"],ax=axes[1])\nsns.swarmplot(df.cluster,df[\"Age\"],ax=axes[2])\nplt.show()","fb64e3d3":"data['cluster'] =  kmeans.labels_\navg_df = data.groupby(['cluster'], as_index=False).mean()\navg_df","5766eb51":"fig,axes = plt.subplots(1,3,figsize=(18,5))\n\nsns.barplot(x='cluster',y='Age',data=avg_df,ax=axes[0])\nsns.barplot(x='cluster',y='Annual Income (k$)',data=avg_df,ax=axes[1])\nsns.barplot(x='cluster',y='Spending Score (1-100)',data=avg_df,ax=axes[2])\n\nplt.show()","90d39186":"# Importing Libraries","da5d8c50":"# Analysis From the Visualizations\n\n#### Cluster 0:\n##### This cluster comprises of age of around 40 with high average annual income and low spending.\n\n#### Cluster 1:\n##### This cluster comprises of age of around 50 with low to mid average income and average spending capacity. \n\n#### Cluster 2:\n##### This cluster comprises of age of around 25 with low average income but high spending score.\n\n#### Cluster 3:\n##### This cluster comprises of age of around  30 with high average income and also high spending score. ","e28d38dc":"\n<h5>Standardization is the method of transforming a variable with a mean of zero and a standard deviation of 1.<\/h5>\n<h4>So let's visualize it.\n<\/h4>","0c09a840":"# ANALYSIS","53e00840":"### Encoding the categorical columns to numerical column values \n\n#### Male =1 Female =0","d14e59dd":"<h3>Here we can see that after the point 4, the distortion\/inertia start decreasing in a linear fashion. So we will take 4 as the optimal value of k.<\/h3>","2fa148c1":"### Let's have a look on the null values and statistical analysis","82f4e744":"## Context\n#### This data set is created only for the learning purpose of the customer segmentation concepts , also known as market basket analysis . I will demonstrate this by using unsupervised ML technique (KMeans Clustering Algorithm) in the simplest form.\n\n## Content\n#### You are owing a supermarket mall and through membership cards , you have some basic data about your customers like Customer ID, age, gender, annual income and spending score.\n### Spending Score is something you assign to the customer based on your defined parameters like customer behavior and purchasing data.\n\n## Problem Statement\n#### You own the mall and want to understand the customers like who can be easily converge [Target Customers] so that the sense can be given to marketing team and plan the strategy accordingly.\n\n[You Can Find The Dataset here](https:\/\/www.kaggle.com\/vjchoudhary7\/customer-segmentation-tutorial-in-python)","cc635642":"# Clustering ","046e5766":"# Mall Customer Segmentation Data","c94b0873":"# If you found this kernel useful, an upvote and your feedback would be appreciated.","75ec68ff":"# Elbow Method for optimal value of k in KMeans\n<h4>\nA fundamental step for any unsupervised algorithm is to determine the optimal number of clusters into which the data may be clustered. The Elbow Method is one of the most popular methods to determine this optimal value of k.<\/h4>","14fb381c":"# Data Insights ","11934489":"<h5>Data standardization is the method of ensuring that your data set could be compared to different data sets. It\u2019s a key part of the research and analysis, and it\u2019s one thing that everybody who makes use of data for comparison should take into account before they even collect, clean, or analyze their first data point.<\/h5>","741b80d4":"# Standardizing variables\n\n<h5>The objective of standardizing variables is to make sure all variables contribute evenly to a scale when items are added together which makes it easier to interpret results of regression or other analysis.<\/h5>\n\n<h5>Standardization is the method of placing different variables on an identical scale. This helps you to compare values between different types of variables.\nData gives more meaning when you compare it to something.<\/h5>\n"}}