{"cell_type":{"ac30a5e6":"code","cef2c74e":"code","7eee91cf":"code","754eb9c6":"code","0bf47df0":"code","5082a1cc":"code","8462f31c":"code","d3a9a516":"code","07902b24":"code","f25ee87c":"code","33c4548b":"code","67cfc486":"code","e94b10f1":"code","c4782a9a":"code","e24bae92":"code","c2b2cca3":"code","e8112648":"code","4f5a2cd7":"code","1ab67cf9":"code","e84e356d":"code","0c6ebf65":"code","ab44040f":"code","b60acac2":"code","bab8d43e":"code","90230867":"code","8eed9012":"code","a7917614":"code","b5cbcd82":"code","c1c7ac2d":"code","09607d8e":"code","72bf0efb":"code","e173dfcb":"code","39de3ab7":"code","3678b696":"code","9cf5b6e2":"markdown","c9354310":"markdown","e83b95a8":"markdown","de30ce1a":"markdown","8d7534fd":"markdown","11375803":"markdown","56cf4797":"markdown","95c0fb8f":"markdown","282ba0c1":"markdown","095a60ed":"markdown","5d5e1186":"markdown","5bc82516":"markdown","f4f35b9e":"markdown","5f09e8ca":"markdown","8fda24de":"markdown"},"source":{"ac30a5e6":"%reset -f","cef2c74e":"# !pip uninstall -y scikit-learn\n# !pip install scikit-learn==0.24.1","7eee91cf":"%%capture\n\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport joblib\n\nimport tensorflow as tf\nmae = tf.keras.losses.MeanAbsoluteError()    ## The loss for this task\n_ = mae([1], [2])                            ## Trick to initialize tensorflow here !\n\nimport os","754eb9c6":"INPUT_DIR = '\/kaggle\/input\/morebikes2021'\n\nSTATION_ID = 252                  ## Change here to train models for other stations\nFEATURE_SET = 'short_full_temp'   ## Change the features here (use 'all' to use all possible features)\n\nSHUFFLE_SAMPLES = True           ## Change here to shuffle the dataset before spliting\nTRAIN_SIZE = 0.80                ## Change the train proportion here\n\nos.listdir(INPUT_DIR)","0bf47df0":"def get_train_path(station_id):\n    return INPUT_DIR+'\/Train\/Train\/station_'+str(station_id)+'_deploy.csv'\n\ndf = pd.read_csv(get_train_path(STATION_ID))\n\ndf.head()","5082a1cc":"def plot_corr_map(station_id, ax, labels=True):\n    df = pd.read_csv(get_train_path(station_id))\n    df = df.drop(['station', 'weekday'], axis=1)\n\n    s = df.corrwith(df['bikes'])\n    if not labels: s = np.array(s)\n    df = pd.DataFrame({'bikes':s})\n\n    ax.set_title('Station '+str(station_id))\n    sns.heatmap(df, annot=True, ax=ax)\n    \nfig, ax = plt.subplots(1, 3, figsize=(15,10)) \n\n## Investigate correlation between features and target variable for specific stations\nplot_corr_map(201, ax=ax[0])\nplot_corr_map(202, ax=ax[1], labels=False)\nplot_corr_map(STATION_ID, ax=ax[2], labels=False)","8462f31c":"def preprocess(df):\n    df = df.drop(['weekday', 'latitude', 'longitude','year', 'month'], axis=1)\n#     df = df.drop(['numDocks'], axis=1)\n    df = df.drop(['station'], axis=1)\n    df['timestamp'] = (df['timestamp'] - 1412114400) \/ 3600    ## 1412114400 is the closest to the Epoch in the dataset\n    return df\n\ndef get_peters_features():\n    \"\"\"\n    Features suggested by the competition\n    \"\"\"\n    short = ['bikes_3h_ago', 'short_profile_3h_diff_bikes', 'short_profile_bikes']\n    short_temp = short + ['temperature.C']\n    full = ['bikes_3h_ago', 'full_profile_3h_diff_bikes', 'full_profile_bikes']\n    full_temp = full + ['temperature.C']\n    short_full = ['bikes_3h_ago', 'short_profile_3h_diff_bikes', 'short_profile_bikes', 'full_profile_3h_diff_bikes', 'full_profile_bikes']\n    short_full_temp = short_full + ['temperature.C']\n    \n    if FEATURE_SET=='short':\n        return short\n    elif FEATURE_SET=='short_temp':\n        return short_temp\n    elif FEATURE_SET=='full':\n        return full\n    elif FEATURE_SET=='full_temp':\n        return full_temp\n    elif FEATURE_SET=='short_full':\n        return short_full\n    elif FEATURE_SET=='short_full_temp':\n        return short_full_temp\n    else:\n        return 'all'\n\ndef select_peters_features(df):\n    features_to_use = get_peters_features()\n    if features_to_use == 'all':\n        return df.dropna()\n    else:\n        if 'bikes' in list(df.columns):\n            features_to_use += ['bikes']\n        return df[features_to_use].dropna()","d3a9a516":"def make_learning_data(station_id):\n    df = pd.read_csv(get_train_path(station_id))\n    \n    df = preprocess(df)\n    if SHUFFLE_SAMPLES:\n        df = df.sample(frac=1, random_state=12)\n\n    df = select_peters_features(df)\n\n    df_y = df['bikes']\n    df_X = df.drop(['bikes'], axis=1)\n\n    return np.array(df_X), np.array(df_y)\n\nX, y = make_learning_data(STATION_ID)","07902b24":"\"\"\" Split the data between training and validation \"\"\"\ndef split_dataset(X, y, train_size):\n    dataset_size = len(X)\n    assert dataset_size == len(y)\n    assert dataset_size >= train_size\n\n    X_train, y_train = X[:train_size], y[:train_size]\n    X_val, y_val = X[train_size:], y[train_size:]\n\n    return (X_train, y_train), (X_val, y_val)\n\n(X_train, y_train), (X_val, y_val) = split_dataset(X, y, int(TRAIN_SIZE*len(X)))\n\nprint(\"Train shapes:\", X_train.shape, y_train.shape)\nprint(\"Val shapes:\", X_val.shape, y_val.shape)","f25ee87c":"\"\"\" All models that will be trained and their scores \"\"\"\n\npotential_models = []","33c4548b":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.model_selection import cross_val_score\n\ndegrees = [1, 2, 3]\n\nplt.figure(figsize=(18, 5))\nfor i in range(len(degrees)):\n    ax = plt.subplot(1, len(degrees), i + 1)\n\n    scaled_features = StandardScaler()\n#     scaled_features = MinMaxScaler()\n    polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)\n    linear_regression = LinearRegression()\n#     linear_regression = PoissonRegressor()\n#     linear_regression = Lasso()\n    linear_regression = Ridge()\n    pipeline = Pipeline(\n        [\n            (\"scaled_features\", scaled_features),\n            (\"polynomial_features\", polynomial_features),\n            (\"linear_regression\", linear_regression),\n        ]\n    )\n    pipeline.fit(X_train, y_train)\n\n    y_pred = np.rint(pipeline.predict(X_val))\n    score = mae(y_val, y_pred).numpy()\n    \n    potential_models.append(('linear_reg_'+str(i+1), pipeline, score))\n\n    plt.plot(y_val, \"o\", label=\"True values\")\n    plt.plot(y_pred, \".\", label=\"Predictions\")\n    ax.set_xlabel(\"instances\")\n    plt.ylabel(\"# bikes\")\n    plt.legend(loc=\"best\")\n    plt.title(\"Degree: {}\\nMAE = {:.2f}\".format(degrees[i], score))","67cfc486":"### See what the weights look like\npotential_models[0][1].named_steps['linear_regression'].coef_","e94b10f1":"from sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors=2)\n\nKNN.fit(X_train, y_train)","c4782a9a":"def plot_results(y_pred, title):\n    fig, ax = plt.subplots(1, 1, figsize=(18, 5))\n\n    plt.plot(y_val, \"o\", label=\"True values\")\n    ax.plot(y_pred, \".\", label=\"Predictions\")\n    ax.set_xlabel(\"instances\")\n    ax.set_ylabel(\"# bikes\")\n    ax.legend(loc=\"best\")\n    ax.set_title(\n        title+\" \\nMAE = {:.2f}\".format(\n            mae(y_val, y_pred).numpy()\n        )\n    )\n\ny_pred = KNN.predict(X_val)\n\npotential_models.append(('knn', KNN, mae(y_val, y_pred).numpy()))\n\nplot_results(y_pred, title='kNN')","e24bae92":"# from sklearn.linear_model import LogisticRegressionCV\n\n## LogReg = LogisticRegressionCV(multi_class='multinomial', solver='lbfgs', max_iter=400)\n# LogReg = Pipeline([(\"minmax_scaler\", MinMaxScaler()), \n#                    (\"logistic_reg\", LogisticRegressionCV(multi_class='ovr', solver='lbfgs', max_iter=200))])\n\n# LogReg.fit(X_train, y_train)","c2b2cca3":"# y_pred = LogReg.predict(X_val)\n# potential_models.append(('logistic_reg', LogReg, mae(y_val, y_pred).numpy()))\n# plot_results(y_pred, title='Logistic regression')","e8112648":"from sklearn.tree import DecisionTreeClassifier\n\n# DcTree = DecisionTreeClassifier(random_state=12)\nDcTree = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"decision_tree\", DecisionTreeClassifier(random_state=12))])\n\nDcTree.fit(X_train, y_train)","4f5a2cd7":"y_pred = DcTree.predict(X_val)\npotential_models.append(('decision_tree', DcTree, mae(y_val, y_pred).numpy()))\nplot_results(y_pred, title='Decision Tree')","1ab67cf9":"from sklearn.ensemble import RandomForestClassifier\n\n# RdFor = RandomForestClassifier(n_estimators=300, max_leaf_nodes=None, n_jobs=-1, random_state=12)\nRdFor = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"random_forest\", RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=12))])\n\nRdFor.fit(X_train, y_train)","e84e356d":"y_pred = RdFor.predict(X_val)\npotential_models.append(('random_forest', RdFor, mae(y_val, y_pred).numpy()))\nplot_results(y_pred, title='Random Forest')","0c6ebf65":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\n# SVM1 = OneVsRestClassifier(LinearSVC(random_state=12))\nSVM1 = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"svm_ovr\", OneVsRestClassifier(LinearSVC(random_state=12)))])\n\nSVM1.fit(X_train, y_train)","ab44040f":"y_pred = SVM1.predict(X_val)\npotential_models.append(('svm_ovr', SVM1, mae(y_val, y_pred).numpy()))\nplot_results(y_pred, title='SVM - OvR')","b60acac2":"from sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.svm import SVC\n\n# SVM2 = OneVsOneClassifier(SVC(random_state=12))\nSVM2 = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"svm_ovo\", OneVsOneClassifier(SVC(random_state=12)))])\n\nSVM2.fit(X_train, y_train)","bab8d43e":"y_pred = SVM2.predict(X_val)\npotential_models.append(('svm_ovo', SVM2, mae(y_val, y_pred).numpy()))\nplot_results(y_pred, title='SVM - OvO')","90230867":"plt.hist(y_train)\nplt.title('Number of bikes at station '+str(STATION_ID));","8eed9012":"from sklearn.linear_model import PoissonRegressor\n\n# Poisreg = PoissonRegressor()\nPoisreg = Pipeline([(\"minmax_scaler\", MinMaxScaler()), (\"pois_reg\", PoissonRegressor())])\n\nPoisreg.fit(X_train, y_train)","a7917614":"y_pred = Poisreg.predict(X_val)\npotential_models.append(('poisson_reg', Poisreg, mae(y_val, y_pred).numpy()))\nplot_results(y_pred, title='Poisson regression')","b5cbcd82":"\"\"\" Define the model  \"\"\"\nKerasNet = tf.keras.Sequential([\n\n        tf.keras.layers.Flatten(),\n\n        tf.keras.layers.Dense(4096, activation='relu'),\n#         tf.keras.layers.Dropout(0.25),\n#         tf.keras.layers.Flatten(),\n\n#         tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n        tf.keras.layers.Dense(1024, activation='relu'),\n#         tf.keras.layers.Dropout(0.25),\n\n        tf.keras.layers.Dense(28, activation='softmax')        \n    \n#         tf.keras.layers.Dense(28, activation='relu'),\n#         tf.keras.layers.Dense(1)                    \n                    \n      ])\n\n## Model architecture test with a batch size of 2\nKerasNet(X_train)\n\nKerasNet.summary()","c1c7ac2d":"\"\"\" Compile the model  \"\"\"\nKerasNet.compile(\n#     loss=mae,    \n    loss=tf.losses.sparse_categorical_crossentropy,\n    optimizer=tf.optimizers.Adam(1e-4),\n    metrics=[tf.metrics.sparse_categorical_accuracy])","09607d8e":"\"\"\" Train the model (probably overfits)  \"\"\"\n\nhistory = KerasNet.fit(\n      X_train,\n      y_train,\n      batch_size=60,\n      epochs=100,\n      verbose=0,\n#       steps_per_epoch=100,\n      validation_data=(X_val, y_val),\n)","72bf0efb":"\"\"\" Visualize crossentropy loss and accuracy \"\"\"\n\nfig, (ax2, ax1) = plt.subplots(1, 2, figsize=(18, 5))\n\n# summarize history for accuracy\nax1.plot(history.history['sparse_categorical_accuracy'])\nax1.plot(history.history['val_sparse_categorical_accuracy'])\nax1.set_title('KerasNet accuracy')\nax1.set_ylabel('accuracy')\nax1.set_xlabel('epochs')\nax1.legend(['train', 'val'], loc='upper left')\n\n# summarize history for loss\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nax2.set_title('KerasNet loss')\nax2.set_ylabel('loss')\nax2.set_xlabel('epochs')\nax2.legend(['train', 'val'], loc='upper right');","e173dfcb":"\"\"\" Make a few predictions \"\"\"\n# Evaluate the model on the test data \n# results = KerasNet.evaluate(X_val[:,:,np.newaxis], y_val, batch_size=12)\n# print(\"Results (loss and accuracy) on Station \"+str(201)+\" test dataset:\", results)\n\n# Generate predictions on new data\ny_hat_pred_proba = KerasNet.predict(X_val)\ny_pred = tf.cast(tf.argmax(y_hat_pred_proba, axis=1),tf.int32)\n\n# Predictions in regression scenario\n# y_pred = KerasNet.predict(X_val)\n\n## Save the model\npotential_models.append(('neural_net', KerasNet, mae(y_val, y_pred).numpy()))\n\nplot_results(y_pred, title='Neural Network')","39de3ab7":"# potential_models\n\nlabels = []\ny_plot = []\nfor mod in potential_models:\n    labels.append(mod[0])\n    y_plot.append(mod[2])\n    \nx_plot = np.arange(len(y_plot))\nfig, ax = plt.subplots(1, 1, figsize=(15,5)) \nplt.bar(x_plot, height=y_plot, color=['black', 'red', 'green', 'blue', 'cyan'])\nplt.xticks(ticks=x_plot, labels=labels)\nplt.ylim((0, 5))\nplt.ylabel(\"MAE score\")\nplt.title(\"Comparison of different learning algorithms on station \"+str(STATION_ID))\nplt.legend();","3678b696":"potential_models = sorted(potential_models, key=lambda el:el[2])\n\n## Save the best model\n# if potential_models[0][0] != 'neural_net':\n#     save_name = str(STATION_ID)+'_'+potential_models[0][0]+'.pkl'\n#     joblib.dump(potential_models[0][1], save_name)\n# else:\n#     save_name = str(STATION_ID)+'_'+potential_models[0][0]+'.h5'\n#     potential_models[0][1].save(save_name)\n\n## The best model is ...\npotential_models[0]","9cf5b6e2":"## Logistic regression\n\n*Logistic regression with multiple classes is not properly implemented in our version of scikit-learn (version 0.23), so we left it out, even though it could have brought interesting insights into the data.*","c9354310":"After imputing missing values, we've split the loaded data for training and validation while avoiding shuffling. We will train our models on 80% of the data (approximately 25 days) and evaluate the models on the remaining 6 last days of the month. The idea is that the model that performs best on this slightly drifted data is most likely to perform best on the more drifted testing data (for the next 3 months). Here, the 80\/20% split is for demonstration purpose; in production setting, we went for a 95\/5% division to achieve better performance.","e83b95a8":"## k-Nearest Neighbors\n\nOur task can also be viewed as a multiclass classification problem, with exactly `numDocks+1` classes. The first algorithm we considered for this approach is the low-bias and high-variance k-Nearest Neighbors algorithm with `k=2`. Compared to other models, we found that kNN produces better results when the input data is not normalized.","de30ce1a":"# Sort the models and pick the best\n\nHere, we compare the models and plot the result in a bar chart. As we stated before, only the best model will be selected to make predictions for the corresponding station in the next notebooks.","8d7534fd":"Analysing the correlation between predictor variables and our target variable for three seperate stations indicate that some 'weather' and 'timepoint' features will be important for training. On the other hand, features relating to the station and its location will be dropped (`station`, `latitude` and`longitude`). The same goes for non-numeric features like `weekday` or constant features like `year` or `month`. In order to use all these interresting features for training, the `FEATURE_SET` variable above can be set `all`.","11375803":"## Random Forest\n\nIn an attempt to reduce the variance and improve results obtianed from decision trees, we tested random forests.","56cf4797":"# Objective\nThe goal of this notebook is to analyse the `motorbikes2021` dataset and identify the best features and best per-station machine learning algorithms to use in the next phases of the competition. Our task is to predict the number of bikes at a station in 3h time.","95c0fb8f":"## Poisson regression\n\nAn interresting remark is that the number of bikes at a station (sometimes) appears to follow a Poisson distribution, with the station more often empty than full. This justifies envisioning the Poisson regression as a model for individual stations.","282ba0c1":"## SVM - One vs. One","095a60ed":"# Per-station trainings\n\nWe identified several machine learning algorithms that can be interesting for our task. Each algorithm will be trained, tuned and stored for later comparison at the end of the notebook. Our unique metric for evaluation is the MAE, which tells us by how many (on average) our predictions differ from the actual number of bikes at the stations. \n\nGiven the wide range of values our features can take, most algorithms will require a form of scaling before fitting, hence the widespread usage of the `Pipleline` class in what follows.","5d5e1186":"## Neural Network\n\nThe final model we tested is a 2-layered fully connected neural network. Even with untuned hyper-parameters and blatant overfitting, we found that it outperformed other models for most stations. However, in the spirit of fairness, we will **not** consider neural networks for the subsequent phases of this work.","5bc82516":"## Linear Regression\n\nGiven the nature of the variables, linear regression is the most intuitive choice here. We opted for Ridge regularization to avoid massive weights, while keeping as many features as possible (not setting weights to 0). We also considered using polynomial and interaction features (up to degree 3) which, depending on the initial set of features we chose, improved the results.","f4f35b9e":"# Load data for a few stations and analyse\n\n**_Which features are the most important for learning the number of bikes at a station?_** The answer will focus on a particular station (identified by its id `STATION_ID`). Now some features were engineered and provided to us in the dataset. These are described by `short`, `short_temp`, `full`, `full_temp`, `short_full`, and `short_full_temp`. Change the variable `FEATURE_SET` to either of these values to get different results.","5f09e8ca":"## Decision Tree\n\nAnother high-variance model we tested is decision trees. We placed no limits on the number of leafs nor the minimum features required to split a node.  ","8fda24de":"## SVM - One vs. Rest\n\nAnother algorithm we investigated for classification is Support Vector Machine. Since this is an inherently binary classification algorithm, we tested two techniques for multiclass classification: \n- the efficient [One vs. Rest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multiclass.OneVsRestClassifier.html),\n- the more costly [One vs. One](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multiclass.OneVsOneClassifier.html)."}}