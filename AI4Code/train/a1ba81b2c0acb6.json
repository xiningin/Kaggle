{"cell_type":{"93d7430a":"code","fd4350c3":"code","43e90a12":"code","314408fd":"code","bfac7453":"code","ec29892a":"code","221bb6e8":"code","aff993a6":"code","f47857cc":"code","165ea697":"code","a19d5c64":"code","c5d043c9":"code","843368d1":"code","2cfca94f":"code","632cffe8":"code","9f28765a":"code","e9878f6d":"code","8563a710":"code","00b7cd35":"code","b0fcdf9f":"code","63debe8e":"code","6ee5a47b":"code","ef0574e9":"code","6ec57518":"code","63f63804":"code","c08004b7":"code","46c391ff":"code","cfe2b40c":"code","fdd8196c":"code","67d4d8d7":"code","48fb5028":"code","72256867":"code","feac55c2":"markdown","457cccc3":"markdown","8b3e4b4d":"markdown","7c610eb2":"markdown","cef5078c":"markdown","30788b64":"markdown","6ba0968a":"markdown","fd657d7f":"markdown","fa732b15":"markdown","68ffeb42":"markdown","c4983106":"markdown","72beaf81":"markdown","41cfbe99":"markdown","475839e4":"markdown","2b836ed6":"markdown","b49ecd04":"markdown","960da247":"markdown","2eca8d00":"markdown","336faeb9":"markdown","0d0cc154":"markdown","e569863b":"markdown","6d062145":"markdown","b37a3583":"markdown","f58d1008":"markdown","9a83b922":"markdown","655ac7f0":"markdown","f4862f43":"markdown","a580b336":"markdown","ade6d90c":"markdown","ce0ef525":"markdown"},"source":{"93d7430a":"# Link to dataset\n# https:\/\/www.kaggle.com\/arjunbhasin2013\/ccdata?select=CC+GENERAL.csv","fd4350c3":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport math\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport statsmodels.api as sm\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import r2_score, accuracy_score, classification_report\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","43e90a12":"# # Read and preview data\ntry:\n    df = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\nexcept:\n    df = pd.read_csv('CC GENERAL.csv')\nprint(df.shape)\ndf.head()","314408fd":"# Description of columns\n\n# CUST_ID : Identification of Credit Card holder (Categorical)\n# BALANCE : Balance amount left in their account to make purchases\n# BALANCE_FREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n# PURCHASES : Amount of purchases made from account\n# ONEOFF_PURCHASES : Maximum purchase amount done in one-go\n# INSTALLMENTS_PURCHASES : Amount of purchase done in installment\n# CASH_ADVANCE : Cash in advance given by the user\n# PURCHASES_FREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n# ONEOFF_PURCHASES_FREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n# PURCHASES_INSTALLMENTS_FREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n# CASH_ADVANCE_FREQUENCY : How frequently the cash in advance being paid\n# CASH_ADVANCE_TRX : Number of Transactions made with \"Cash in Advanced\"\n# PURCHASES_TRX : Number of purchase transactions made\n# CREDIT_LIMIT : Limit of Credit Card for user\n# PAYMENTS : Amount of Payment done by user\n# MINIMUM_PAYMENTS : Minimum amount of payments made by user\n# PRC_FULL_PAYMENT : Percent of full payment paid by user\n# TENURE : Tenure of credit card service for user","bfac7453":"# Create a list of all the features\nfeatures = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']","ec29892a":"# Check for columns with missing values and impute them with the median.\ndef impute_nan(df):\n    \"\"\"Check for columns with missing values and impute them with the median.\"\"\"\n    nan_cols = df.columns[df.isnull().any()].tolist()\n    nan_length = len(nan_cols)\n    if nan_length == 0:\n        return df\n    else:\n        print('Imputed features:', nan_cols)\n        for x in nan_cols:\n            df[x].fillna(df[x].median(), inplace=True)\n        return df\n\ndf = impute_nan(df)","221bb6e8":"# Check for duplicates\ndef duplicates(x):\n    \"\"\"Check for duplicate rows. \n    Either dedupe or return original dataset if there are no duplicate rows.\"\"\"\n    y = x.drop_duplicates()\n    duplicate_rows = x.shape[0] - y.shape[0]\n    print('Duplicate rows:', duplicate_rows)\n    if duplicate_rows == 0:\n        return x\n    else:\n        return y\n\ndf = duplicates(df)","aff993a6":"custs = df.CUST_ID.nunique()\nrows = df.shape[0]\n\nif custs == rows:\n    print('CUST_ID is unique')\nelse:\n    print('Need to create a unique identifier')","f47857cc":"def outliers(df, features):\n    \"\"\"Count the number of outliers for each feature using the IQR\"\"\"\n    num_outliers = []\n    pct_outliers = []\n    total_rows = []\n    interquartile_range = []\n    for i in features:\n        Q1 = df[i].quantile(.25)\n        Q3 = df[i].quantile(.65)\n        IQR = Q3 - Q1\n        outliers = len(df[(df[i] < (Q1-1.5*IQR)) | (df[i] > (Q3+1.5*IQR))])\n        rows = len(df[i])\n        pct = outliers\/rows\n        interquartile_range.append(IQR)\n        num_outliers.append(outliers)\n        pct_outliers.append(pct)\n        total_rows.append(rows)\n        \n    count_outliers = pd.DataFrame({'Feature': features\n                               , 'Num_Outliers': num_outliers\n                                , 'Percent_Outliers': pct_outliers\n                                , 'IQR': interquartile_range\n                               , 'Total_Rows': total_rows}).sort_values('Percent_Outliers', ascending=False)\n    return count_outliers\n\ncount_outliers = outliers(df=df, features=features)\ncount_outliers","165ea697":"# Visualize quartiles and outliers with box plots\nfor i in features:\n    sns.boxplot(df[i])\n    plt.show()","a19d5c64":"# Scale the features\nX = df[features]\nX_scaled = pd.DataFrame(RobustScaler().fit_transform(X), columns=X.columns, index=X.index)\ndf_scaled = pd.concat([df['CUST_ID'], X_scaled], axis=1)","c5d043c9":"df_scaled[features].info()","843368d1":"(df_scaled[features].describe().transpose()\n     [['mean', '50%', 'min', 'max']]\n     .rename(columns={'50%': 'median'})\n     .style.background_gradient(cmap = 'RdYlGn'))","2cfca94f":"# Bootstrap sampling\nsamples = 1000\nest_popu_means = {}\nboot = []\nfor a in features:\n    for b in range(samples):\n        c = df[a].sample(frac = 0.33, random_state = 1).mean()\n        boot.append(c)\n    p_mean = sum(boot)\/len(boot)\n    est_popu_means[a] = p_mean\n    boot.clear()\n\n# Mean of each feature in the dataset\nsampling_mean = []\nfor i in features:\n    x = df[i].mean()\n    sampling_mean.append(x)\n\ndf_means = pd.DataFrame({'Feature': list(est_popu_means.keys())\n                       , 'Bootstrap_Mean': list(est_popu_means.values())\n                       , 'Mean_in_dataset': sampling_mean})\n\ndf_means","632cffe8":"for i in features:\n    boot_mean = df_means.loc[df_means['Feature'] == i, 'Bootstrap_Mean'].iloc[0]\n    print(i)\n    print('Bootstrap mean:', boot_mean)\n    sns.histplot(df[i])\n    plt.axvline(boot_mean, color = 'red')\n    plt.show()","9f28765a":"spearman_corr = round(df_scaled[features].corr(method = 'spearman'), 2)\nspearman_corr.style.background_gradient(cmap = 'RdYlGn')","e9878f6d":"response_variable = []\nr2score = []\nresiduals = pd.DataFrame()\n\nfor i in features:\n    X = df_scaled[features]\n    X = X.drop(i, axis=1)\n    y = df_scaled[i]\n    model = sm.OLS(y, X).fit()\n    y_pred = model.predict(X)\n    score = r2_score(y, y_pred)\n    \n    residuals[i] = y - y_pred\n    \n    response_variable.append(i)\n    r2score.append(score)\n\nregression_results = pd.DataFrame({'Response_variable': response_variable\n                                   , 'R2_Score': r2score})\n\nregression_results.sort_values('R2_Score', ascending=False, inplace=True)\nplt.figure(figsize=(5, 10))\nsns.barplot(y = 'Response_variable',\n            x = 'R2_Score',\n            orient = 'h',\n            data=regression_results)\nplt.title('R2 Score For Each Feature Using The Other Features As Predictors', fontsize=18)\nplt.xlabel('R2 Score')\nplt.ylabel('Feature')\nplt.show()","8563a710":"# Plot the distribution of the residuals\n# (Multiple linear regression assumes that the residuals are normally distributed)\nfor i in residuals.columns:\n    skew = round(residuals[i].skew(), 1)\n    sns.histplot(residuals[i])\n    plt.title('Distribution of Residuals: ' + str(i) + ' --- Skew ' + str(skew), fontsize=14)\n    plt.show()","00b7cd35":"# Use the elbow method to choose the optimal number of clusters \nX = df_scaled[features]\nkmeans = KMeans(random_state=1)\nkmeans_vis = KElbowVisualizer(kmeans, k=(1,15), metric='distortion', timings=False).fit(X)\nprint('Optimal number of clusters:', kmeans_vis.elbow_value_)","b0fcdf9f":"# Cluster the customers using the optimal number of clusters\ndf_scaled['CLUSTERS'] = KMeans(n_clusters=kmeans_vis.elbow_value_, random_state=1).fit_predict(X)\n\n# Count the number of customers in each cluster\ncount_clusters = df_scaled.groupby('CLUSTERS').agg({'CUST_ID': 'nunique'})\ncustomers = df_scaled['CUST_ID'].nunique()\ncount_clusters['Percent_of_Customers'] = count_clusters['CUST_ID']\/customers\ncount_clusters","63debe8e":"# Plot the average feature value for each cluster\ndescribe_clusters = df_scaled.groupby('CLUSTERS').mean()\n\nfor i in features:\n    sns.barplot(x = describe_clusters.index\n               , y = i\n               , data = describe_clusters)\n    plt.title('Mean ' + str(i) + ' by Cluster')\n    plt.show()","6ee5a47b":"# Average feature value for each cluster\ndescribe_clusters.transpose().style.background_gradient(cmap = 'RdYlGn')","ef0574e9":"# SMOTE\nX = df_scaled[features]\ny = df_scaled['CLUSTERS']\n\ncounter = Counter(y)\nprint('Before SMOTE:')\nprint(dict(sorted(counter.items())))\nprint('='*50)\n\noversample = SMOTE(random_state = 1)\nX_SMOTE, y_SMOTE = oversample.fit_resample(X, y)\n\ncounter = Counter(y_SMOTE)\nprint('After SMOTE:')\nprint(dict(sorted(counter.items())))\n\ndf_scaled_SMOTE = pd.concat([y_SMOTE, X_SMOTE], axis=1)","6ec57518":"# Separate the feature and target variables\nX = df_scaled_SMOTE[features]\ny = df_scaled_SMOTE['CLUSTERS']\n\n# Create train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=1)\n\n# Fit the decision tree with the training data\ntree_model = DecisionTreeClassifier(random_state=1).fit(X_train, y_train)\n\n# Predict test values\ny_pred = tree_model.predict(X_test)\n\n# Evaluate the model with accuracy on the test dataset\ntree_accuracy = accuracy_score(y_test, y_pred)\n\n# Classification report\nprint('Accuracy:', tree_accuracy)\nprint('Decision Tree Classifier')\nprint(classification_report(y_test, y_pred))","63f63804":"# The multinomial logistic regression model assumes that the predicters are not strongly correlated with one another.\n# Use the variance inflation factor (VIF) to check for multicollinearity\nX = df_scaled_SMOTE[features]\n\nvif = pd.DataFrame()\nvif[\"feature\"] = X.columns\nvif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n\n# Create list of features with a VIF >= 10\nhigh_vif = list(vif[vif[\"VIF\"] >= 10][\"feature\"])\n\n# Remove features with a high VIF\nremove_high_vif = [x for x in features if x not in high_vif]\n\nvif.sort_values(\"VIF\", ascending=False)","c08004b7":"# Multinomial logistic regression\n\n# Separate the feature and target variables\nX = df_scaled_SMOTE[remove_high_vif]\nX = sm.add_constant(X)\ny = df_scaled_SMOTE['CLUSTERS']\n\n# Create train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=1)\n\n# Fit the logistic model with the training data\nlogit_model = LogisticRegression(random_state=1, solver='liblinear').fit(X_train, y_train)\n\n# Predict test values\ny_pred = logit_model.predict(X_test)\n\n# Evaluate the model with accuracy on the test dataset\nMNLogit_accuracy = accuracy_score(y_test, y_pred)\n\n# Classification report\nprint('Accuracy:', MNLogit_accuracy)\nprint('Multinomial Logistic Regression')\nprint(classification_report(y_test, y_pred))","46c391ff":"# Max depth of the decision tree classifier\nX = df_scaled_SMOTE[features]\ny = df_scaled_SMOTE['CLUSTERS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=1)\n\nacc = []\ndepth = []\nfor i in range(1, 16):\n    tree_model = DecisionTreeClassifier(random_state=1, max_depth=i).fit(X_train, y_train)\n    y_pred = tree_model.predict(X_test)\n    tree_accuracy = accuracy_score(y_test, y_pred)\n    acc.append(tree_accuracy)\n    depth.append(i)\n\ntree_depth = pd.DataFrame({'Max_Depth': depth\n                          , 'Accuracy': acc})\nsns.lineplot(x = 'Max_Depth'\n            , y = 'Accuracy'\n            , data = tree_depth)\nplt.title('Max Depth of Decision Tree and its Accuracy', fontsize=14)","cfe2b40c":"# The accuracy stops improving when the tree depth is greater than 10\n# Fit decision tree with the optimal max depth\noptimal_max_depth = 10\n\nX = df_scaled_SMOTE[features]\ny = df_scaled_SMOTE['CLUSTERS']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.33, random_state=1)\ntree_model = DecisionTreeClassifier(random_state=1, max_depth=optimal_max_depth).fit(X_train, y_train)\ny_pred = tree_model.predict(X_test)\ntree_accuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', tree_accuracy)","fdd8196c":"tree_importance = pd.DataFrame({'Feature': features\n                               , 'Importance': tree_model.feature_importances_})\n\ntree_importance.sort_values('Importance', ascending=False, inplace=True)\n\nplt.figure(figsize=(5, 7))\nsns.barplot(y = 'Feature',\n            x = 'Importance',\n            orient = 'h',\n            data=tree_importance)\nplt.title('Feature Importance in the Decision Tree Classifier', fontsize=14)\nplt.show()","67d4d8d7":"# PURCHASES is the most important feature, plot its distribution for each cluster\nunique = sorted(df_scaled_SMOTE.CLUSTERS.unique())\npalette = dict(zip(unique, sns.color_palette(n_colors=len(unique))))\n\nsns.histplot(x = 'PURCHASES'\n            , hue = 'CLUSTERS'\n             , element = 'poly'\n             , palette = palette\n            , data = df_scaled_SMOTE)\nplt.title('Distribution of PURCHASES by CLUSTER', fontsize=14)","48fb5028":"targets = sorted(df_scaled_SMOTE['CLUSTERS'].unique())\ntargets = ['Cluster_' + str(x) for x in targets]\n\nplt.figure(figsize=(175, 25))\nplot_tree(tree_model\n          , feature_names = features\n          , class_names = targets\n          , filled = True\n          , fontsize = 10)\n\nplt.savefig('customer_segmentation_decision_tree.jpg')\nplt.show()","72256867":"def summarize_clusters(cluster):\n    \"\"\"Returns description of cluster\"\"\"\n    counter = Counter(cluster)\n    counter = dict(sorted(counter.items()))\n    summary = dict()\n    for i in counter.keys():\n        if counter[i] >= 5400:\n            summary[i] = ['Average credit card customer (each of their features are within one standard deviation from the mean)', 'Not the highest or lowest for any feature']\n        elif counter[i] >= 1400:\n            summary[i] = ['Lowest number of transactions made with cash in advanced'\n                                , 'Lowest amount of cash in advance transactions'\n                                , 'Lowest number of purchases'\n                                , 'Lowest amount of purchases'\n                                , 'Lowest amount purchases done in installment'\n                                , 'Lowest balance frequency'\n                                , 'Lowest balance amount left in their account to make purchases'\n                                , 'Lowest tenure'\n                                , 'Lowest credit card limit'\n                                , 'Lowest amount of minimum payments']\n        elif counter[i] >= 1190:\n            summary[i] = ['Highest number of transactions made with cash in advanced'\n                                , 'Highest amount of cash in advance transactions'\n                                , 'Highest frequency of cash in advance transactions'\n                                , 'High amount of minimum payments'\n                                , 'Low tenure'\n                                , 'Low balance frequency'\n                                , 'Low purchase freqency']\n        elif counter[i] >= 700:\n            summary[i] = ['High number of purchases'\n                                , 'High amount of purchases'\n                                , 'High amount of purchases done in installment'\n                                , 'High amount of one-off purchases'\n                                , 'High frequency of one-off purchases'\n                                , 'High amount of payments'\n                                , 'High amount of full payments']\n        elif counter[i] >= 30:\n            summary[i] = ['Highest balance frequency'\n                                , 'Highest tenure'\n                                , 'Highest amount of minimum payments'\n                                , 'Lowest amount of full payments'\n                                , 'Lowest amount of one-off purchases'\n                                , 'Lowest frequency of one-off purchases']\n        else:\n            summary[i] = ['Highest number of purchases'\n                                , 'Highest amount of purchases'\n                                , 'Highest frequency of purchases'\n                                , 'Highest amount of one-off purchases'\n                                , 'Highest frequency of one-off purchases'\n                                , 'Highest amount of purchases done in installment'\n                                , 'Highest amount of payments'\n                                , 'Highest amount of full payments'\n                                , 'Highest credit limit'\n                                , 'Highest balance amount left in their account to make purchases'\n                                , 'Lowest frequency of cash in advance transactions']\n    for i in summary.keys():\n        print('Cluster:', i)\n        print(summary[i])\n        print('-'*100)\n\n\nclusters = df_scaled['CLUSTERS']\nsummarize_clusters(clusters)","feac55c2":"## Data types <a class=\"anchor\" id=\"dtypes\"><\/a>","457cccc3":"# The decision tree is more accurate than the logistic model.","8b3e4b4d":"Use box plots to visualize each feature's quartiles and outliers.","7c610eb2":"## Distributions <a class=\"anchor\" id=\"distributions\"><\/a>\nUse a histogram to visualize the distribution of each feature. Include a vertical line on the histogram to indicate its estimate population mean from the bootstrap sampling.","cef5078c":"### Decision tree classifier <a class=\"anchor\" id=\"decision_tree\"><\/a>\nFit a decision tree classifier with training data. Evaluate the tree with a holdout dataset, and print the accuracy along with a few other classification metrics.","30788b64":"## Bootstrapping <a class=\"anchor\" id=\"bootstrapping\"><\/a>\nBootstrap sampling is a technique used to estimate the population mean of a variable. It does this by repeatedly sampling the data with replacement, and recording the mean for each sample. The estimate population mean is the mean of all the sample means.","6ba0968a":"## Multiple linear regression <a class=\"anchor\" id=\"multiple_linear_regression\"><\/a>\nFit a linear regression model for each feature using the other features as preditors. Plot the R2 score for each model to learn which features have strong and weak relationships with the other features.","fd657d7f":"## Visualize decision tree <a class=\"anchor\" id=\"visualize\"><\/a>","fa732b15":"# Getting started <a class=\"anchor\" id=\"getting_started\"><\/a>","68ffeb42":"## Model selection <a class=\"anchor\" id=\"model_selection\"><\/a>\nUse a decision tree classifier and multinomial logistic regression model to learn the rules that distinguish the customer segments. Evaluate the models with a holdout dataset, and choose the most accurate model.","c4983106":"## SMOTE <a class=\"anchor\" id=\"SMOTE\"><\/a>\nClass imbalance can create issues for classification models. Since the clusters are not equally sized (class imbalance), use the synthetic minority oversampling technique (SMOTE) to oversample the smaller clusters.","72beaf81":"### Multinomial logistic regression <a class=\"anchor\" id=\"logistic_regression\"><\/a>\nCheck for multicollinearity. Fit a multinomial logistic regression model with training data. Evaluate the model with a holdout dataset, and print the accuracy along with a few other classification metrics.\n\nLogistic regression assumes the model features are not strongly correlated with each other (multicollinearity). Use the variance inflation factor (VIF) to check for multicollinearity. (VIF is 1 \/ (1 - R2) using multiple regression to model each feature using the other features as predictors.)","41cfbe99":"# Preprocessing <a class=\"anchor\" id=\"preprocessing\"><\/a>","475839e4":"### Optimize decision tree hyperparamters\nOptimize the maximum tree depth hyperparamter using the max_depth function parameter. The optimal max_depth is the point at which the model accuracy stops improving.","2b836ed6":"## Check for outliers <a class=\"anchor\" id=\"outliers\"><\/a>\nCount the number of outliers for each feature using its quartiles and interquartile range (IQR). A value is considered an outlier if it is less than (first quartile - 1.5 * IQR) or greater than (third quartile + 1.5 * IQR).","b49ecd04":"## Spearman correlation <a class=\"anchor\" id=\"correlation\"><\/a>\nUse the spearman correlation since the features are not normally distributed. The spearman correlation uses ranks to measure association between variables, and does not require data to be normally distributed.","960da247":"## Check for duplicate rows <a class=\"anchor\" id=\"duplicates\"><\/a>\nWrite a function that checks for duplicate rows. Either dedupe or return the original dataset, depending on whether there are duplicates.","2eca8d00":"# Modeling <a class=\"anchor\" id=\"modeling\"><\/a>","336faeb9":"## K-means clustering <a class=\"anchor\" id=\"cluster\"><\/a>\nThe clusters will be the customer segments.","0d0cc154":"### Cluster customers <a class=\"anchor\" id=\"cluster_customers\"><\/a>\nCluster the customers using the optimal number of clusters.","e569863b":"# Exploratory analysis <a class=\"anchor\" id=\"EDA\"><\/a>","6d062145":"## Check for unique customer identifier <a class=\"anchor\" id=\"unique\"><\/a>\nWe'll need a unique identifier for each customer to do the segmentation.","b37a3583":"# Table of Contents\n\n* [Getting started](#getting_started)\n* [Data preprocessing](#preprocessing)\n    * [Check for missing values](#imputation)\n    * [Check for duplicates](#duplicates)\n    * [Check for unique customer identifier](#unique)\n    * [Check for outliers](#outliers)\n    * [Robust scaler](#standardize)\n* [Exploratory data analysis](#EDA)\n    * [Data types](#dtypes)\n    * [Summary statistics](#summary_stats)\n    * [Bootstrapping](#bootstrapping)\n    * [Distributions](#distributions)\n    * [Spearman correlation](#correlation)\n    * [Multiple linear regression](#multiple_linear_regression)\n* [Modeling](#modeling)\n    * [K-means clustering](#cluster)\n        * [Find optimal number of clusters](#elbow)\n        * [Cluster customers](#cluster_customers)\n    * [SMOTE](#SMOTE)\n    * [Model selection](#model_selection)\n        * [Decision tree classifier](#decision_tree)\n        * [Multinomial logistic regression](#logistic_regression)\n    * [Feature importance](#feature_importance)\n    * [Visualize decision tree](#visualize)\n* [Summarize insights from analysis](#summary)","f58d1008":"## Summary statistics <a class=\"anchor\" id=\"summary_stats\"><\/a>\nMean, median, min and max","9a83b922":"## Robust scaler <a class=\"anchor\" id=\"standardize\"><\/a>\nClustering, classification and regression models (used later in the notebook) perform best when the data are in the same unit of measurement. Use a scaler to that end, and since most of the features contain a non-trivial amount of outliers, use the robust scaler instead of the standard scaler. The robust scaler uses the median and IQR, which are better estimates of central tendency in the precense of outliers.","655ac7f0":"# Summarize insights from analysis <a class=\"anchor\" id=\"summary\"><\/a>\nI put the insights in a dictionary because the number given to identify each cluster (ie. Cluster #1, Cluster #2, etc.) were switched around when running the notebook.","f4862f43":"## Check for missing values <a class=\"anchor\" id=\"imputation\"><\/a>\nWrite a function that checks for missing values. Use the median to impute the missing values.","a580b336":"## Feature importance <a class=\"anchor\" id=\"feature_importance\"><\/a>\nThe feature importance is measured by how well that feature separates the target classes (impurity-based).","ade6d90c":"### Find optimal number of clusters <a class=\"anchor\" id=\"elbow\"><\/a>\nUse the elbow method to choose the optimal number of clusters. The KElbowVisualizer libary loops through a range of number of clusters and records the distortion (the sum of the squared distances between each observation vector and its dominating centroid). Then it plots the distortion against number of clusters, and identifies the optimal number of clusters using the point on the chart that looks like an elbow.","ce0ef525":"The aim of this analysis is to identify segments of credit card customers that have shared characteristics. I started the project by preprocessing the data, and then moved on to exploring it. After that I used k-means clustering to identify the customer segments, and then used classification models to learn the rules that distinguish the clusters. I finished the analysis by summarizing the insights about the customer segments.\n\nI'm interested to hear how others would tackle the customer segmentation. How would you approach it? What could improve the analysis? Let me know in the comments.\n\n-Mason"}}