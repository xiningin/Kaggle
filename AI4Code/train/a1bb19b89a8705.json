{"cell_type":{"ba9c1eda":"code","4e289d19":"code","e424c3d1":"code","90c9c8e0":"code","76c44626":"code","d29e88e8":"code","6931f64e":"code","e1ac3832":"code","699af71e":"code","3f89f083":"code","3f954a69":"code","080163a6":"code","9fea5a54":"code","78872b24":"code","211edf87":"code","59bf6fb3":"markdown"},"source":{"ba9c1eda":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import plot_model\nfrom warnings import filterwarnings\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport lightgbm as lgbm\n\nfilterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'","4e289d19":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","e424c3d1":"from scipy import optimize\nfrom scipy import special\n\nclass FocalLoss:\n    \"\"\"\n    source: https:\/\/maxhalford.github.io\/blog\/lightgbm-focal-loss\/\n    \"\"\"\n\n    def __init__(self, gamma, alpha=None):\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def at(self, y):\n        if self.alpha is None:\n            return np.ones_like(y)\n        return np.where(y, self.alpha, 1 - self.alpha)\n\n    def pt(self, y, p):\n        p = np.clip(p, 1e-15, 1 - 1e-15)\n        return np.where(y, p, 1 - p)\n\n    def __call__(self, y_true, y_pred):\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        return -at * (1 - pt) ** self.gamma * np.log(pt)\n\n    def grad(self, y_true, y_pred):\n        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        g = self.gamma\n        return at * y * (1 - pt) ** g * (g * pt * np.log(pt) + pt - 1)\n\n    def hess(self, y_true, y_pred):\n        y = 2 * y_true - 1  # {0, 1} -> {-1, 1}\n        at = self.at(y_true)\n        pt = self.pt(y_true, y_pred)\n        g = self.gamma\n\n        u = at * y * (1 - pt) ** g\n        du = -at * y * g * (1 - pt) ** (g - 1)\n        v = g * pt * np.log(pt) + pt - 1\n        dv = g * np.log(pt) + g + 1\n\n        return (du * v + u * dv) * y * (pt * (1 - pt))\n\n    def init_score(self, y_true):\n        res = optimize.minimize_scalar(\n            lambda p: self(y_true, p).sum(),\n            bounds=(0, 1),\n            method='bounded'\n        )\n        p = res.x\n        log_odds = np.log(p \/ (1 - p))\n        return log_odds\n\n    def lgb_obj(self, preds, train_data):\n        y = train_data.get_label()\n        p = special.expit(preds)\n        return self.grad(y, p), self.hess(y, p)\n\n    def lgb_eval(self, preds, train_data):\n        y = train_data.get_label()\n        p = special.expit(preds)\n        is_higher_better = False\n        return 'focal_loss', self(y, p).mean(), is_higher_better","90c9c8e0":"from joblib import Parallel, delayed\nfrom sklearn.multiclass import _ConstantPredictor\nfrom sklearn.preprocessing import LabelBinarizer\nfrom scipy import special\n\n\nclass OneVsRestLightGBMWithCustomizedLoss:\n    \"\"\"\n    source: https:\/\/towardsdatascience.com\/multi-class-classification-using-focal-loss-and-lightgbm-a6a6dec28872\n    \"\"\"\n\n    def __init__(self, loss, n_jobs=3):\n        self.loss = loss\n        self.n_jobs = n_jobs\n\n    def fit(self, X, y, **fit_params):\n\n        self.label_binarizer_ = LabelBinarizer(sparse_output=True)\n        Y = self.label_binarizer_.fit_transform(y)\n        Y = Y.tocsc()\n        self.classes_ = self.label_binarizer_.classes_\n        columns = (col.toarray().ravel() for col in Y.T)\n        if 'eval_set' in fit_params:\n            # use eval_set for early stopping\n            X_val, y_val = fit_params['eval_set'][0]\n            Y_val = self.label_binarizer_.transform(y_val)\n            Y_val = Y_val.tocsc()\n            columns_val = (col.toarray().ravel() for col in Y_val.T)\n            self.results_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit_binary)\n                                                         (X, column, X_val, column_val, **fit_params) for\n                                                         i, (column, column_val) in\n                                                         enumerate(zip(columns, columns_val)))\n        else:\n            # eval set not available\n            self.results_ = Parallel(n_jobs=self.n_jobs)(delayed(self._fit_binary)\n                                                         (X, column, None, None, **fit_params) for i, column\n                                                         in enumerate(columns))\n\n        return self\n\n    def _fit_binary(self, X, y, X_val, y_val, **fit_params):\n        unique_y = np.unique(y)\n        init_score_value = self.loss.init_score(y)\n        if len(unique_y) == 1:\n            estimator = _ConstantPredictor().fit(X, unique_y)\n        else:\n            fit = lgbm.Dataset(X, y, init_score=np.full_like(y, init_score_value, dtype=float))\n            filtering = ['eval_set', 'early_stopping_rounds', 'verbose_eval', 'num_boost_round']\n            local_fit_params = {item:value for item, value in fit_params.items() if item!='eval_set'}\n            \n            if 'num_boost_round' in fit_params:\n                num_boost_round = fit_params['num_boost_round']\n            else:\n                num_boost_round = 100\n                \n            if 'early_stopping_rounds' in fit_params:\n                early_stopping_rounds = fit_params['early_stopping_rounds']\n            else:\n                early_stopping_rounds = 10\n                \n            if 'verbose_eval'  in fit_params:\n                verbose_eval = fit_params['verbose_eval']\n            else:\n                verbose_eval = 10\n                    \n            if 'eval_set' in fit_params:\n                val = lgbm.Dataset(X_val, y_val, init_score=np.full_like(y_val, init_score_value, dtype=float),\n                                  reference=fit)\n        \n                estimator = lgbm.train(params=local_fit_params,\n                                       train_set=fit,\n                                       valid_sets=(fit, val),\n                                       valid_names=('fit', 'val'),\n                                       fobj=self.loss.lgb_obj,\n                                       feval=self.loss.lgb_eval,\n                                       num_boost_round=num_boost_round,\n                                       early_stopping_rounds=early_stopping_rounds,\n                                       verbose_eval=verbose_eval)\n            else:\n                                   \n                estimator = lgbm.train(params=local_fit_params,\n                                       train_set=fit,\n                                       fobj=self.loss.lgb_obj,\n                                       feval=self.loss.lgb_eval,\n                                       num_boost_round=num_boost_round,\n                                       early_stopping_rounds=early_stopping_rounds,\n                                       verbose_eval=verbose_eval)\n\n        return estimator, init_score_value\n\n    def predict(self, X):\n\n        n_samples = X.shape[0]\n        maxima = np.empty(n_samples, dtype=float)\n        maxima.fill(-np.inf)\n        argmaxima = np.zeros(n_samples, dtype=int)\n\n        for i, (e, init_score) in enumerate(self.results_):\n            margins = e.predict(X, raw_score=True)\n            prob = special.expit(margins + init_score)\n            np.maximum(maxima, prob, out=maxima)\n            argmaxima[maxima == prob] = i\n\n        return argmaxima\n\n    def predict_proba(self, X):\n        y = np.zeros((X.shape[0], len(self.results_)))\n        for i, (e, init_score) in enumerate(self.results_):\n            margins = e.predict(X, raw_score=True)\n            y[:, i] = special.expit(margins + init_score)\n        y \/= np.sum(y, axis=1)[:, np.newaxis]\n        return y","76c44626":"train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","d29e88e8":"print(\"The target class distribution:\")\nprint((train.groupby('Cover_Type').Id.nunique() \/ len(train)).apply(lambda p: f\"{p:.3%}\"))","6931f64e":"# Droping Cover_Type 5 label, since there is only one instance of it\ntrain = train[train.Cover_Type != 5]","e1ac3832":"# remove unuseful features\ntrain = train.drop([ 'Soil_Type7', 'Soil_Type15'], axis=1)\ntest = test.drop(['Soil_Type7', 'Soil_Type15'], axis=1)\n\n# extra feature engineering\ndef r(x):\n    if x+180>360:\n        return x-180\n    else:\n        return x+180\n\ndef fe(df):\n    df['EHiElv'] = df['Horizontal_Distance_To_Roadways'] * df['Elevation']\n    df['EViElv'] = df['Vertical_Distance_To_Hydrology'] * df['Elevation']\n    df['Aspect2'] = df.Aspect.map(r)\n    ### source: https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n    df[\"Aspect\"][df[\"Aspect\"] < 0] += 360\n    df[\"Aspect\"][df[\"Aspect\"] > 359] -= 360\n    df.loc[df[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n    df.loc[df[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n    df.loc[df[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n    df.loc[df[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n    df.loc[df[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n    df.loc[df[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n    ########\n    df['Highwater'] = (df.Vertical_Distance_To_Hydrology < 0).astype(int)\n    df['EVDtH'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['EHDtH'] = df.Elevation - df.Horizontal_Distance_To_Hydrology * 0.2\n    df['Euclidean_Distance_to_Hydrolody'] = (df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)**0.5\n    df['Manhattan_Distance_to_Hydrolody'] = df['Horizontal_Distance_To_Hydrology'] + df['Vertical_Distance_To_Hydrology']\n    df['Hydro_Fire_1'] = df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Fire_Points']\n    df['Hydro_Fire_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Fire_Points'])\n    df['Hydro_Road_1'] = abs(df['Horizontal_Distance_To_Hydrology'] + df['Horizontal_Distance_To_Roadways'])\n    df['Hydro_Road_2'] = abs(df['Horizontal_Distance_To_Hydrology'] - df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_1'] = abs(df['Horizontal_Distance_To_Fire_Points'] + df['Horizontal_Distance_To_Roadways'])\n    df['Fire_Road_2'] = abs(df['Horizontal_Distance_To_Fire_Points'] - df['Horizontal_Distance_To_Roadways'])\n    df['Hillshade_3pm_is_zero'] = (df.Hillshade_3pm == 0).astype(int)\n    return df\n\ntrain = fe(train)\ntest = fe(test)\n\n# Summed features pointed out by @craigmthomas (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823)\nsoil_features = [x for x in train.columns if x.startswith(\"Soil_Type\")]\nwilderness_features = [x for x in train.columns if x.startswith(\"Wilderness_Area\")]\n\ntrain[\"soil_type_count\"] = train[soil_features].sum(axis=1)\ntest[\"soil_type_count\"] = test[soil_features].sum(axis=1)\n\ntrain[\"wilderness_area_count\"] = train[wilderness_features].sum(axis=1)\ntest[\"wilderness_area_count\"] = test[wilderness_features].sum(axis=1)","699af71e":"y = train.Cover_Type.values - 1\nX = reduce_mem_usage(train.drop(\"Cover_Type\", axis=1)).set_index(\"Id\")\nXt = reduce_mem_usage(test).set_index(\"Id\")","3f89f083":"import gc\ndel([train, test])\n_ = [gc.collect() for i in range(5)]","3f954a69":"le = LabelEncoder()\ntarget = le.fit_transform(y)\n\n_, classes_num = np.unique(target, return_counts=True)","080163a6":"N_FOLDS = 5\n\n### cross-validation \ncv = KFold(n_splits=N_FOLDS, shuffle=True, random_state=1)\n\npredictions = np.zeros((len(Xt), len(le.classes_)))\noof = np.zeros((len(X), len(le.classes_)))\nscores = list()\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X, y)):\n    X_train, y_train = X.iloc[idx_train, :], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid, :], target[idx_valid]\n    \n    fit_params = {'eval_set': [(X_valid, y_valid)],\n                  'num_boost_round': 1500,\n                  'early_stopping_rounds': 30,\n                  'verbose_eval': 100\n                 }\n    \n    loss = FocalLoss(alpha=0.75, gamma=2.0)\n    model = OneVsRestLightGBMWithCustomizedLoss(loss=loss)\n\n    print('**'*20)\n    print(f\"Fold {fold+1} || Training\")\n    print('**'*20)\n\n    model.fit(X_train, y_train, **fit_params)\n\n    predictions += model.predict_proba(Xt) \/ N_FOLDS\n    oof[idx_valid] = model.predict_proba(X_valid)\n        \n    scores.append(accuracy_score(y_true=y_valid, y_pred=np.argmax(oof[idx_valid], axis=1)))\n    print(f\"cv accuracy fold {fold+1}: {scores[-1]:0.5f}\")","9fea5a54":"print(f\"Average cv accuracy: {np.mean(scores):0.5f} (std={np.std(scores):0.5f})\")","78872b24":"submission.Cover_Type = le.inverse_transform(np.argmax(predictions, axis=1)) + 1\nsubmission.to_csv(\"submission.csv\", index=False)","211edf87":"oof = pd.DataFrame(oof, columns=[f\"prob_{i}\" for i in le.classes_])\noof.insert(loc=0, column='Id', value=range(len(X)))\noof.to_csv(\"oof.csv\", index=False)","59bf6fb3":"In this notebook, I demonstrate how to use the multiclass focal loss that should help you score better with such imbalanced classes. The focal loss function is from https:\/\/github.com\/artemmavrin\/focal-loss\/blob\/master\/docs\/source\/index.rst\n\nThe focal loss is a loss that has been devised for object detection problems where the background is more prominent than the objects to be detected. \n\n![](https:\/\/github.com\/Atomwh\/FocalLoss_Keras\/raw\/master\/images\/fig1-focal%20loss%20results.png)\n\nAs you increase the gamma value, you put more emphasis on hard to classify examples. There is clearly a trade-off for this (high gamma values can be detrimental), but overall if you set the right value it should perform much better than using other tricks for imbalanced data.\n\nIn order to implement the multiclass focal loss, I referred to this article: \n\n\n\nThis notebook owes quite a lot of ideas from \"TPSDEC21-01-Keras Quickstart\" (https:\/\/www.kaggle.com\/ambrosm\/tpsdec21-01-keras-quickstart) by @ambrosm please consider upvoting also his work.\n\nIt also implements the feature engineering suggested by @aguschin (see my post https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291839 for all the references)."}}