{"cell_type":{"e0ef5de2":"code","0f5393c1":"code","d14681bf":"code","1ac776d9":"code","fad0d301":"code","f7b81c65":"code","a735975e":"code","92398059":"code","a043ebff":"code","8b919c9b":"code","a6d74633":"code","5bb84942":"code","8484d8c7":"code","aedb6012":"code","754ccbf6":"code","46d9bb37":"code","69567ffd":"code","32864ae1":"code","9626bda9":"code","26edd6e0":"code","520cd4c0":"code","fe2b4895":"code","22b6f6cd":"code","983f7390":"code","eb4685eb":"code","5200561c":"markdown","17b0c0b9":"markdown","89f067bd":"markdown","b4c17652":"markdown","463269ad":"markdown","8f42c556":"markdown","c30445be":"markdown","051ee369":"markdown","5d79e778":"markdown","7e0b420d":"markdown","1cfecc01":"markdown","ac8fdd60":"markdown","d76e07a5":"markdown","4adf8c21":"markdown","09ede7d0":"markdown","a780af48":"markdown","2059902b":"markdown","22db1578":"markdown","b0c446c6":"markdown","5746bb42":"markdown"},"source":{"e0ef5de2":"try:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    secret_value_0 = user_secrets.get_secret(\"api_key\")\n    wandb.login(key=secret_value_0)\n    anony=None\nexcept:\n    anony = \"must\"\n    print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https:\/\/wandb.ai\/authorize')\n    \nCONFIG = dict(competition = 'WiDSDatathon2022',_wandb_kernel = 'tensorgirl')","0f5393c1":"import os\nimport gc\nimport copy\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold ,RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 18})\nplt.style.use('ggplot')\nimport seaborn as sns\nfrom scipy import stats\n\nimport shap\n\nfrom sklearn.preprocessing import StandardScaler\n\nimport optuna.integration.lightgbm as lgbm\nimport optuna\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport wandb","d14681bf":"train = pd.read_csv(\"..\/input\/widsdatathon2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/widsdatathon2022\/test.csv\")\nprint(\"Number of train samples are\",train.shape)\nprint(\"Number of test samples are\",test.shape)\ncategorical_features = ['State_Factor', 'building_class', 'facility_type']\nnumerical_features=train.select_dtypes('number').columns","1ac776d9":"train.head()","fad0d301":"plt.figure(figsize = (25,11))\nsns.heatmap(train.isna().values, cmap = ['#ffd514','#ff355d'], xticklabels=train.columns)\nplt.title(\"Missing values in training Data\", size=20);","f7b81c65":"#code copied from https:\/\/www.kaggle.com\/shrutisaxena\/wids2022-starter-code\nmissing_columns = [col for col in train.columns if train[col].isnull().any()]\nmissingvalues_count =train.isna().sum()\nmissingValues_df = pd.DataFrame(missingvalues_count.rename('Null Values Count')).loc[missingvalues_count.ne(0)]\nmissingValues_df .style.background_gradient(cmap=\"Pastel1\")","a735975e":"# basic stats of features\ntrain.describe().style.background_gradient(cmap=\"Pastel1\")","92398059":"plt.figure(figsize=(15, 7))\nplt.subplot(121)\nsns.kdeplot(train.site_eui , color = \"#ffd514\")\nplt.subplot(122)\nsns.boxplot(train.site_eui , color = \"#ff355d\")\n","a043ebff":"res = stats.probplot(train['site_eui'], plot=plt)","8b919c9b":"def kdeplot_features(df_train,df_test, feature, title):\n    '''Takes a column from the dataframe and plots the distribution (after count).'''\n    \n    values_train = df_train[feature].to_numpy()\n    values_test = df_test[feature].to_numpy()  \n     \n    plt.figure(figsize = (18, 3))\n    \n    sns.kdeplot(values_train, color = '#ffd514')\n    sns.kdeplot(values_test, color = '#ff355d')\n    \n    plt.title(title, fontsize=15)\n    plt.legend()\n    plt.show();\n    \n    del values_train , values_test\n    gc.collect()\n    \ndef countplot_features(df_train, feature, title):\n    '''Takes a column from the dataframe and plots the distribution (after count).'''\n    \n           \n    plt.figure(figsize = (10, 5))\n    \n    sns.countplot(df_train[feature], color = '#ff355d')\n        \n    plt.title(title, fontsize=15)    \n    plt.show();\n    \n        \ndef create_wandb_hist(x_data=None, x_name=None, title=None, log=None):\n    '''Create and save histogram in W&B Environment.\n    x_data: Pandas Series containing x values\n    x_name: strings containing axis name\n    title: title of the graph\n    log: string containing name of log'''\n    \n    data = [[x] for x in x_data]\n    table = wandb.Table(data=data, columns=[x_name])\n    wandb.log({log : wandb.plot.histogram(table, x_name, title=title)})\n","a6d74633":"# plot distributions of features\nfor feature in numerical_features:\n    if feature != \"site_eui\":\n        kdeplot_features(train,test, feature=feature, title = feature + \" distribution\")","5bb84942":"# plot distributions of categorical features\nfor feature in categorical_features:\n    fig = countplot_features(train, feature=feature, title = \"Frequency of \"+ feature)","8484d8c7":"# Log Plots to W&B environment\ntitle = \"Distribution of Numerical features\"\nrun = wandb.init(project='WiDSDatathon2022', name=title,anonymous=anony,config=CONFIG)\nfor feature in numerical_features:\n    if feature != \"site_eui\":\n        title = \"Distribution of Numerical \"+feature    \n        create_wandb_hist(x_data=train[feature],x_name=feature , title=title,log=\"hist\")    \nwandb.finish()\n\ntitle = \"Countplot Distribution\"\nrun = wandb.init(project='WiDSDatathon2022', name=title,anonymous=anony,config=CONFIG)    \nfor feature in categorical_features:\n    #fig = countplot_features(train, feature=feature, title = feature + \" countplot distribution\")\n    wandb.log({feature + \" countplot distribution\": fig})\nwandb.finish()","aedb6012":"target = train[\"site_eui\"]\ntrain = train.drop([\"site_eui\",\"id\"],axis =1)\ntest = test.drop([\"id\"],axis =1)","754ccbf6":"#code copied from https:\/\/www.kaggle.com\/shrutisaxena\/wids2022-starter-code\n\n# year_built: replace with current year.\ntrain['year_built'] =train['year_built'].replace(np.nan, 2022)\n#replacing rest of the values with mean\ntrain['energy_star_rating']=train['energy_star_rating'].replace(np.nan,train['energy_star_rating'].mean())\ntrain['direction_max_wind_speed']= train['direction_max_wind_speed'].replace(np.nan,train['direction_max_wind_speed'].mean())\ntrain['direction_peak_wind_speed']= train['direction_peak_wind_speed'].replace(np.nan,train['direction_peak_wind_speed'].mean())\ntrain['max_wind_speed']=train['max_wind_speed'].replace(np.nan,train['max_wind_speed'].mean())\ntrain['days_with_fog']=train['days_with_fog'].replace(np.nan,train['days_with_fog'].mean())\n\n##for testdata\n\n# year_built: replace with current year.\ntest['year_built'] =test['year_built'].replace(np.nan, 2022)\n#replacing rest of the values with mean\ntest['energy_star_rating']=test['energy_star_rating'].replace(np.nan,test['energy_star_rating'].mean())\ntest['direction_max_wind_speed']= test['direction_max_wind_speed'].replace(np.nan,test['direction_max_wind_speed'].mean())\ntest['direction_peak_wind_speed']= test['direction_peak_wind_speed'].replace(np.nan,test['direction_peak_wind_speed'].mean())\ntest['max_wind_speed']=test['max_wind_speed'].replace(np.nan,test['max_wind_speed'].mean())\ntest['days_with_fog']=test['days_with_fog'].replace(np.nan,test['days_with_fog'].mean())","46d9bb37":"le = LabelEncoder()\n\ntrain['State_Factor']= le.fit_transform(train['State_Factor']).astype(\"uint8\")\ntest['State_Factor']= le.fit_transform(test['State_Factor']).astype(\"uint8\")\n\ntrain['building_class']= le.fit_transform(train['building_class']).astype(\"uint8\")\ntest['building_class']= le.fit_transform(test['building_class']).astype(\"uint8\")\n\ntrain['facility_type']= le.fit_transform(train['facility_type']).astype(\"uint8\")\ntest['facility_type']= le.fit_transform(test['facility_type']).astype(\"uint8\")","69567ffd":"# Save train data to W&B Artifacts\ntrain.to_csv(\"train_features.csv\", index = False)\nrun = wandb.init(project='WiDSDatathon2022', name='training_data', anonymous=anony,config=CONFIG) \nartifact = wandb.Artifact(name='training_data',type='dataset')\nartifact.add_file(\".\/train_features.csv\")\n\nwandb.log_artifact(artifact)\nwandb.finish()","32864ae1":"trainnames = copy.deepcopy(train)\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n","9626bda9":"rkf = RepeatedKFold(n_splits=3, n_repeats=3, random_state=42)\n\nparams = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_error\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",                \n        \"seed\": 42\n    }\n\nX = copy.deepcopy(train)  \ny = copy.deepcopy(target)\n\nstudy_tuner = optuna.create_study(direction='minimize')\ndtrain = lgbm.Dataset(X, label=y)\n\n# Suppress information only outputs - otherwise optuna is \n# quite verbose, which can be nice, but takes up a lot of space\noptuna.logging.set_verbosity(optuna.logging.WARNING) \n\n# Run optuna LightGBMTunerCV tuning of LightGBM with cross-validation\ntuner = lgbm.LightGBMTunerCV(params, \n                            dtrain, \n                            study=study_tuner,\n                            verbose_eval=False,                            \n                            early_stopping_rounds=250,\n                            time_budget=19800, # Time budget of 5 hours, we will not really need it\n                            seed = 42,\n                            folds=rkf,\n                            num_boost_round=10000,\n                            callbacks=[lgbm.reset_parameter(learning_rate = [0.005]*200 + [0.001]*9800) ] #[0.1]*5 + [0.05]*15 + [0.01]*45 + \n                           )\n\ntuner.run()","26edd6e0":"print(tuner.best_params)\n# Classification error\nprint(tuner.best_score)\n# Or expressed as accuracy\nprint(1.0-tuner.best_score)","520cd4c0":"num_folds = 5\nkf = KFold(n_splits = num_folds, random_state = 42)\nerror = 0\nmodels = []\nfor i, (train_index, val_index) in enumerate(kf.split(train)):\n    if i + 1 < num_folds:\n        continue\n    print(train_index.max(), val_index.min())\n    train_X = train[train_index]\n    val_X = train[val_index]\n    train_y = target[train_index]\n    val_y = target[val_index]\n    lgb_train = lgb.Dataset(train_X, train_y > 0)\n    lgb_eval = lgb.Dataset(val_X, val_y > 0)\n    params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n            'learning_rate': 0.1,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 1.0,\n            'bagging_freq' : 0\n             \n            }\n    gbm_class = lgb.train(params,\n                lgb_train,\n                num_boost_round=2000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=20,\n               verbose_eval = 20)\n    \n    lgb_train = lgb.Dataset(train_X[train_y > 0], train_y[train_y > 0])\n    lgb_eval = lgb.Dataset(val_X[val_y > 0] , val_y[val_y > 0])\n    params = {\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': {'rmse'},\n            'learning_rate': 0.1,\n            'feature_fraction': 0.8,\n            'bagging_fraction': 1.0,\n            'bagging_freq' : 0\n                         }\n    gbm_regress = lgb.train(params,\n                lgb_train,\n                num_boost_round=2000,\n                valid_sets=(lgb_train, lgb_eval),\n               early_stopping_rounds=20,\n               verbose_eval = 20)\n#     models.append(gbm)\n\n    y_pred = (gbm_class.predict(val_X, num_iteration=gbm_class.best_iteration) > .5) *\\\n    (gbm_regress.predict(val_X, num_iteration=gbm_regress.best_iteration))\n    error += np.sqrt(mean_squared_error(y_pred, (val_y)))\/num_folds\n    print(np.sqrt(mean_squared_error(y_pred, (val_y))))\n    break\nprint(error)\n","fe2b4895":"feature_imp = pd.DataFrame(sorted(zip(gbm_regress.feature_importance(), trainnames.columns),reverse = True), columns=['Value','Feature'])\nfeature_imp = feature_imp[feature_imp.Value != 0]\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Feature Importance')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","22b6f6cd":"shap_values = shap.TreeExplainer(gbm_regress).shap_values(trainnames)\nshap.summary_plot(shap_values, trainnames)","983f7390":"res = gbm_regress.predict(test)","eb4685eb":"\nsub = pd.read_csv(\"..\/input\/widsdatathon2022\/sample_solution.csv\")\nsub[\"site_eui\"] = res\nsub.to_csv(\"submission.csv\", index = False)","5200561c":" # <h1 style='background:#F7B2B0; border:0; color:black'><center>WiDS Datathon 2022 - Energy Consumption Prediction<\/center><\/h1> ","17b0c0b9":"# **<span style=\"color:#F7B2B0;\">Missing Values Imputation<\/span>**","89f067bd":"# **<span style=\"color:#F7B2B0;\">Making Submission<\/span>**","b4c17652":"Climate change is a globally relevant, urgent, and multi-faceted issue heavily impacted by energy policy and infrastructure. Addressing climate change involves mitigation (i.e. mitigating greenhouse gas emissions) and adaptation (i.e. preparing for unavoidable consequences). Mitigation of GHG emissions requires changes to electricity systems, transportation, buildings, industry, and land use.\n\nAccording to a report issued by the International Energy Agency (IEA), the lifecycle of buildings from construction to demolition were responsible for 37% of global energy-related and process-related CO2 emissions in 2020. Yet it is possible to drastically reduce the energy consumption of buildings by a combination of easy-to-implement fixes and state-of-the-art strategies. For example, retrofitted buildings can reduce heating and cooling energy requirements by 50-90 percent. Many of these energy efficiency measures also result in overall cost savings and yield other benefits, such as cleaner air for occupants. This potential can be achieved while maintaining the services that buildings provide.\n\n# **<span style=\"color:#F7B2B0;\">Goal<\/span>**\n \nThe goal of this competition is to predict the energy consumption using building characteristics and climate and weather variables .\n\n# **<span style=\"color:#F7B2B0;\">Data<\/span>**\n\nThe WiDS Datathon 2022 focuses on a prediction task involving roughly 100k observations of building energy usage records collected over 7 years and a number of states within the United States. The dataset consists of building characteristics (e.g. floor area, facility type etc), weather data for the location of the building (e.g. annual average temperature, annual total precipitation etc) as well as the energy usage for the building and the given year, measured as Site Energy Usage Intensity (Site EUI). Each row in the data corresponds to the a single building observed in a given year. Your task is to predict the Site EUI for each row, given the characteristics of the building and the weather data for the location of the building.\n\n\n**Files**\n> - ``` train.csv``` - the training dataset where the observed values of the Site EUI for each row is provided\n> - ```test.csv``` - the test dataset where we withhold the observed values of the Site EUI for each row. \n> - ```sample_submission.csv``` - a sample submission file in the correct format\n\n**Columns**\n\n> - ```id:``` building id\n\n> - ```Year_Factor:``` anonymized year in which the weather and energy usage factors were observed\n\n> - ```State_Factor:``` anonymized state in which the building is located\n\n> - ```building_class:``` building classification\n\n> - ```facility_type:``` building usage type\n\n> - ```floor_area:``` floor area (in square feet) of the building\n\n> - ```year_built:``` year in which the building was constructed\n\n> - ```energy_star_rating:``` the energy star rating of the building\n\n> - ```ELEVATION:``` elevation of the building location\n\n> - ```january_min_temp:``` minimum temperature in January (in Fahrenheit) at the location of the building\n\n> - ```january_avg_temp:``` average temperature in January (in Fahrenheit) at the location of the building\n\n> - ```january_max_temp:``` maximum temperature in January (in Fahrenheit) at the location of the building\n\n> - ```cooling_degree_days:``` cooling degree day for a given day is the number of degrees where the daily average temperature exceeds 65 degrees Fahrenheit. Each month is summed to produce an annual total at the location of the building.\n\n> - ```heating_degree_days:``` heating degree day for a given day is the number of degrees where the daily average temperature falls under 65 degrees Fahrenheit. Each month is summed to produce an annual total at the location of the building.\n\n> - ```precipitation_inches:``` annual precipitation in inches at the location of the building\n\n> - ```snowfall_inches:``` annual snowfall in inches at the location of the building\n\n> - ```snowdepth_inches:``` annual snow depth in inches at the location of the building\n\n> - ```avg_temp:``` average temperature over a year at the location of the building\n\n> - ```days_below_30F:``` total number of days below 30 degrees Fahrenheit at the location of the building\n\n> - ```days_below_20F:``` total number of days below 20 degrees Fahrenheit at the location of the building\n\n> - ```days_below_10F:``` total number of days below 10 degrees Fahrenheit at the location of the building\n\n> - ```days_below_0F:``` total number of days below 0 degrees Fahrenheit at the location of the building\n\n> - ```days_above_80F:``` total number of days above 80 degrees Fahrenheit at the location of the building\n\n> - ```days_above_90F:``` total number of days above 90 degrees Fahrenheit at the location of the building\n\n> - ```days_above_100F:``` total number of days above 100 degrees Fahrenheit at the location of the building\n\n> - ```days_above_110F:``` total number of days above 110 degrees Fahrenheit at the location of the building\n\n> - ```direction_max_wind_speed:``` wind direction for maximum wind speed at the location of the building. Given in 360-degree compass point directions (e.g. 360 = north, 180 = south, etc.).\n\n> - ```direction_peak_wind_speed:``` wind direction for peak wind gust speed at the location of the building. Given in 360-degree compass point directions (e.g. 360 = north, 180 = south, etc.).\n\n> - ```max_wind_speed:``` maximum wind speed at the location of the building\n\n> - ```days_with_fog:``` number of days with fog at the location of the building\n\n\n# **<span style=\"color:#F7B2B0;\">Evaluation Metric<\/span>**\n\nThe evaluation metric for this competition is Root Mean Squared Error (RMSE). The RMSE is commonly used measure of the differences between predicted values provided by a model and the actual observed values. \n\n","463269ad":"# **<span style=\"color:#F7B2B0;\">Label Encoding<\/span>**","8f42c556":"<img src=\"https:\/\/camo.githubusercontent.com\/dd842f7b0be57140e68b2ab9cb007992acd131c48284eaf6b1aca758bfea358b\/68747470733a2f2f692e696d6775722e636f6d2f52557469567a482e706e67\">\n\n> I will be integrating W&B for visualizations and logging artifacts!\n> \n> [WiDS Datathon 2022 Energy Consumption Prediction Project on W&B Dashboard](https:\/\/wandb.ai\/usharengaraju\/WiDSDatathon2022)\n> \n> - To get the API key, create an account in the [website](https:\/\/wandb.ai\/site) .\n> - Use secrets to use API Keys more securely \n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\"> Weights & Biases (W&B) is a set of machine learning tools that helps you build better models faster. <strong>Kaggle competitions require fast-paced model development and evaluation<\/strong>. There are a lot of components: exploring the training data, training different models, combining trained models in different combinations (ensembling), and so on.<\/span>\n\n> <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">\u23f3 Lots of components = Lots of places to go wrong = Lots of time spent debugging<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">W&B can be useful for Kaggle competition with it's lightweight and interoperable tools:<\/span>\n\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Quickly track experiments,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Version and iterate on datasets, <br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Evaluate model performance,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Reproduce models,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Visualize results and spot regressions,<br><\/span>\n* <span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">Share findings with colleagues.<\/span>\n\n<span style=\"color: #000508; font-family: Segoe UI; font-size: 1.2em; font-weight: 300;\">To learn more about Weights and Biases check out this <strong><a href=\"https:\/\/www.kaggle.com\/ayuraj\/experiment-tracking-with-weights-and-biases\">kernel<\/a><\/strong>.<\/span>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","c30445be":"# **<span style=\"color:#F7B2B0;\">Prediction on Test Data<\/span>**","051ee369":"# **<span style=\"color:#F7B2B0;\">Feature Scaling<\/span>**","5d79e778":"![](https:\/\/drive.google.com\/uc?id=1hRtZtLamcxuoSADFbuXsGZ-rCLnlffJ-)","7e0b420d":"# **<span style=\"color:#F7B2B0;\">Target variable distribution<\/span>**","1cfecc01":"# Work in progress \ud83d\udea7","ac8fdd60":"# **<span style=\"color:#F7B2B0;\">Feature Importance<\/span>**","d76e07a5":"# **<span style=\"color:#F7B2B0;\">W & B Artifacts<\/span>**\n\nAn artifact as a versioned folder of data.Entire datasets can be directly stored as artifacts .\n\nW&B Artifacts are used for dataset versioning, model versioning . They are also used for tracking dependencies and results across machine learning pipelines.Artifact references can be used to point to data in other systems like S3, GCP, or your own system.\n\nYou can learn more about W&B artifacts [here](https:\/\/docs.wandb.ai\/guides\/artifacts)\n\n![](https:\/\/drive.google.com\/uc?id=1JYSaIMXuEVBheP15xxuaex-32yzxgglV)","4adf8c21":"# **<span style=\"color:#F7B2B0;\">SHAP VALUES<\/span>**\n\n**SHAP (SHapley Additive exPlanations**) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions","09ede7d0":"# **<span style=\"color:#F7B2B0;\">Distribution of Categorical Variables<\/span>**","a780af48":"# **<span style=\"color:#F7B2B0;\">Distribution of Numeric Variables<\/span>**","2059902b":"# **<span style=\"color:#F7B2B0;\">Hyperparameter Tuning Using Optuna<\/span>**\n\n[Source](https:\/\/github.com\/optuna\/optuna)\n\nOptuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API. The code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.\n\n**Key Features**\n\nOptuna has modern functionalities as follows:\n\n\ud83d\udccc **Lightweight, versatile, and platform agnostic architecture**\n\nHandle a wide variety of tasks with a simple installation that has few requirements.\n\n\ud83d\udccc **Pythonic search spaces**\n\nDefine search spaces using familiar Python syntax including conditionals and loops.\n\n\ud83d\udccc **Efficient optimization algorithms**\n\nAdopt state-of-the-art algorithms for sampling hyperparameters and efficiently pruning unpromising trials.\n\n\ud83d\udccc **Easy parallelization**\n\nScale studies to tens or hundreds or workers with little or no changes to the code.\n\n\ud83d\udccc **Quick visualization**\n\nInspect optimization histories from a variety of plotting functions.","22db1578":"# **<span style=\"color:#F7B2B0;\">LightGBM<\/span>**\n\n[Source](https:\/\/github.com\/microsoft\/LightGBM)\n\n**LightGBM** is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n\ud83d\udccc **Faster training speed and higher efficiency.**\n\n\ud83d\udccc **Lower memory usage.**\n\n\ud83d\udccc **Better accuracy.**\n\n\ud83d\udccc **Support of parallel, distributed, and GPU learning.**\n\n\ud83d\udccc **Capable of handling large-scale data.**\n\n\n**LightGBM** is being widely-used in many winning solutions of machine learning competitions.\n\nComparison experiments on public datasets show that LightGBM can outperform existing boosting frameworks on both efficiency and accuracy, with significantly lower memory consumption. \n\n","b0c446c6":"## References :\n\nhttps:\/\/www.kaggle.com\/shrutisaxena\/wids2022-starter-code","5746bb42":"# **<span style=\"color:#F7B2B0;\">Missing Values<\/span>**"}}