{"cell_type":{"ff097382":"code","6e61d9de":"code","e1ffce05":"code","f82469b4":"code","fc54b25e":"code","acc94ba0":"code","f005e297":"code","49cd102c":"code","065c3553":"code","db922881":"code","8188d36f":"markdown"},"source":{"ff097382":"#Import necessay libraries\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport matplotlib\nmatplotlib.rcParams[\"figure.figsize\"] = (20,10)\nimport seaborn as sns\n\n#Preprocessing\nfrom sklearn import model_selection,metrics\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split,KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler,OrdinalEncoder,LabelEncoder\n#Model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error\nfrom catboost import CatBoostRegressor","6e61d9de":"#import the data and shape\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\")\nsample = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\")\nprint(train.shape,test.shape,sample.shape)\ntrain.describe().transpose()","e1ffce05":"train.sample(2)","f82469b4":"train.country.value_counts()","fc54b25e":"train.store.value_counts()","acc94ba0":"#insert the kfold columns\ntrain['kfold'] = -1\n#distributing the data\nkfold = KFold(n_splits = 5,shuffle=True,random_state = 42)\nfor fold, (tr_i,va_i) in enumerate(kfold.split(X=train)):\n    train.loc[va_i,'kfold'] = fold\n    \nprint(train.kfold.value_counts())\ntrain.to_csv(\"folds_5.csv\",index=False)\nprint(\"successfully folds\")","f005e297":"train.columns,train.dtypes\n","49cd102c":"df = pd.read_csv(\".\/folds_5.csv\")\n\n#features taken to train\nusefulfeatures = [f for f in df.columns if f not in(\"row_id\",\"date\",\"kfold\",\"num_sold\")]\nobject_cols = [col for col in df.columns if col in (\"product\",\"store\",\"country\")]\ndf_test= test[useful_features]","065c3553":"final_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.num_sold\n    yvalid = xvalid.num_sold\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ohe = preprocessing.OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    xtrain_ohe = ohe.fit_transform(xtrain[object_cols])\n    xvalid_ohe = ohe.transform(xvalid[object_cols])\n    xtest_ohe = ohe.transform(xtest[object_cols])\n    \n    xtrain_ohe = pd.DataFrame(xtrain_ohe, columns=[f\"ohe_{i}\" for i in range(xtrain_ohe.shape[1])])\n    xvalid_ohe = pd.DataFrame(xvalid_ohe, columns=[f\"ohe_{i}\" for i in range(xvalid_ohe.shape[1])])\n    xtest_ohe = pd.DataFrame(xtest_ohe, columns=[f\"ohe_{i}\" for i in range(xtest_ohe.shape[1])])\n    \n    xtrain = pd.concat([xtrain, xtrain_ohe], axis=1)\n    xvalid = pd.concat([xvalid, xvalid_ohe], axis=1)\n    xtest = pd.concat([xtest, xtest_ohe], axis=1)\n    \n    # this part is missing in the video:\n    xtrain = xtrain.drop(object_cols, axis=1)\n    xvalid = xvalid.drop(object_cols, axis=1)\n    xtest = xtest.drop(object_cols, axis=1)\n    # missing part ends\n     params_lgb = {\n    \"task\": \"train\",\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    'subsample': 0.95312,\n    'learning_rate': 0.001635,\n    \"max_depth\": 3,\n    \"feature_fraction\": 0.2256038826485174,\n    \"bagging_fraction\": 0.7705303688019942,\n    \"min_child_samples\": 290,\n    \"reg_alpha\": 14.68267919457715,\n    \"reg_lambda\": 66.156,\n    \"max_bin\": 772,\n    \"min_data_per_group\": 177,\n    \"bagging_freq\": 1,\n    \"cat_smooth\": 96,\n    \"cat_l2\": 17,\n    \"verbosity\": -1,\n    'random_state':42,\n    'n_estimators':5000,\n    'colsample_bytree':0.1107\n    }\n    \n    lgb_train = lgb.Dataset(xtrain, ytrain)\n    lgb_val = lgb.Dataset(xvalid, yvalid)\n    \n    model = lgb.train(params=params_lgb,\n                      train_set=lgb_train,\n                      valid_sets=lgb_val,\n                      early_stopping_rounds=300,\n                      verbose_eval=False)\n    \n   \n    preds_valid = model.predict(xvalid,num_iteration=model.best_iteration)\n    test_preds = model.predict(xtest,num_iteration=model.best_iteration)\n \n    final_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))","db922881":"#reconfigure of split data\nfinal_predict = np.mean(np.column_stack(final_predictions),axis=1)\nprint(final_predict)\nsample.num_sold = final_predict\nsample.to_csv(\"submission_xgb.csv\",index=False)\nprint(\"Final achieve to send xgboost output data\")","8188d36f":"## **Thankyou_Guys**\n**Version 1 : Apply Catboost techniques**\n**Version 2 : Apply Xgboost techniques**\n## ***And explore More ? ComingSoon!!!!***"}}