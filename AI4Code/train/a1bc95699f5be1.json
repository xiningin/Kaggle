{"cell_type":{"4b82a748":"code","f8ab07ee":"code","bf7449de":"code","01f666d2":"code","680fa50e":"code","8999670d":"code","2877a2d3":"code","f4d59df3":"code","428876e6":"code","9da3b01e":"code","e1c7ac36":"code","6d8b023e":"code","17c8cecf":"code","9dfac180":"code","20f85e5a":"code","3e6eee62":"code","62ccaf45":"code","2c3e3218":"code","fb103f8c":"code","e56f0b0b":"code","c4ee8bc8":"code","28759ba7":"code","1aa5e6bf":"code","ba3037b4":"code","411b7c7b":"code","34dd6d20":"code","b9937b21":"code","dbedf1d1":"code","0d71e3ab":"code","5c511bb2":"code","0b2a35a0":"code","de76b233":"code","d7c763aa":"code","855702d8":"code","c599ce62":"code","4c705803":"code","b82249c6":"code","c6e100f1":"code","a022e8f0":"code","a473977f":"code","a3e2728d":"code","628e4463":"code","1f4d2790":"code","175bf7ba":"code","7e1981b5":"code","07cbfe42":"code","3b719e89":"code","64f1c2a0":"code","823f2758":"code","40e63722":"code","49638615":"code","b2806854":"code","3004dd22":"code","a997438e":"code","0f71fa39":"code","d90418b1":"code","2745800f":"code","9d1b2dcb":"code","6f5c7957":"code","b1ad548f":"code","7229a1ae":"code","14214e45":"code","5859225d":"code","57beff30":"code","86b47468":"code","d064d84e":"code","5b9289e8":"code","a346228e":"code","6d5dce91":"code","cc34f999":"code","a9b159e9":"code","07019af8":"code","d62123f9":"code","40952b27":"code","7b989088":"code","ffb731ae":"code","1fef9512":"code","2dd4ee8f":"code","1589efee":"code","83e3f89d":"code","2460cd90":"code","c3547776":"code","8aa280ca":"code","6b646556":"code","7c928699":"code","d6d50e15":"code","2d1c2b78":"code","a9c73bfe":"code","c946fd23":"code","7490a497":"code","ae607ce3":"code","a725d359":"code","1fc307ed":"code","05dc6a6c":"code","dd139bab":"code","ace3d0e8":"code","638bb530":"code","0830ab26":"code","a068b271":"code","9def0c87":"code","9db35855":"code","f3dcd430":"code","e101e554":"code","0621d9ce":"code","aeb278d1":"code","46f3a4f7":"code","f92ebeeb":"code","561668f3":"code","70dd6cd4":"markdown","f577ca33":"markdown","22ca55bd":"markdown","c010cc1a":"markdown","01ede311":"markdown","ba18c0c2":"markdown","61ba5a63":"markdown","bb797fa2":"markdown","ab5e6865":"markdown","c4a77ab5":"markdown","6c296ba3":"markdown","dde5e218":"markdown","4c0fa0c9":"markdown","db528555":"markdown","8fdfa4d5":"markdown","3aebcab3":"markdown","869d3e07":"markdown","4998c70f":"markdown","9614a0c4":"markdown","72a28949":"markdown","f894cc3c":"markdown","8585850c":"markdown","b73f6402":"markdown","98b5bc2b":"markdown","b8a9a360":"markdown","9764ea53":"markdown","81ff10ba":"markdown","47e8faed":"markdown","b56e6d4b":"markdown","275fb58b":"markdown","12878196":"markdown","157389ad":"markdown","20f2fc7d":"markdown","2fc52b93":"markdown","8d71cc9e":"markdown","88e9488d":"markdown","f046e750":"markdown","e146cfe2":"markdown","26bd7248":"markdown","8e45a62a":"markdown","6075a87d":"markdown","d3c28fc9":"markdown","c893a99e":"markdown","6160dc28":"markdown"},"source":{"4b82a748":"# import the required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n%matplotlib inline\n\n# import the library to ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')","f8ab07ee":"# import the train dataset\ntrain_original = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip')","bf7449de":"# display the data\ntrain_original.head()","01f666d2":"# Check the shape of the train dataset\ntrain_original.shape","680fa50e":"# check the info of the train data\ntrain_original.info()","8999670d":"# First,check the original variance of all the features in the train dataset and store it to new object\ntrain_original_var=pd.DataFrame(train_original.var(axis=0),columns=['Variance'])\ntrain_original_var","2877a2d3":"#Second, define a function to remove the features of train dataset with zero variance \ndef features_zero_var(df):\n    df_original_var=pd.DataFrame(df.var(axis=0),columns=['Variance']) \n    return((df_original_var[df_original_var.Variance==0]))\n# Add the object train_original to the function. i.e., features_zero_var(train_original)\n# Firstly, the variance with respect to each features in the train_original dataset will be stored in the object df_original_var.\n# Index contains the feature names and the corresponding variances will be displayed in the column with name 'Variance'  \n# Lastly, it returns only the features of object 'df_original_var' having variance = 0\n# df_original_var is named because it will be applicable to both train and test data","f4d59df3":"# Call the function to return the train dataset features having zero variance.\nfeatures_zero_var(train_original)","428876e6":"# Remove the features with 0 variance from train dataset and store the data in the new object\ntrain_original_modified= train_original.drop(columns=train_original_var[train_original_var.Variance==0].index)","9da3b01e":"# Display the modified train dataset after removing the features having zero variance\ntrain_original_modified.head()","e1c7ac36":"# Print the original and modified shape of the train dataset\nprint('Shape of original train dataset is:', train_original.shape)\nprint('\\nShape of modified train dataset after removing features having zero variance is:', train_original_modified.shape)","6d8b023e":"# Save the modified train dataset\ntrain_original_modified.to_csv('train_original_modified.csv',index=False)","17c8cecf":"#import the test dataset\ntest_original = pd.read_csv('..\/input\/mercedes-benz-greener-manufacturing\/test.csv.zip')","9dfac180":"# display the data\ntest_original.head()","20f85e5a":"# Check the shape of the data\ntest_original.shape","3e6eee62":"# First,check the original variance of all the features in the test dataset and store it to new object\ntest_original_var=pd.DataFrame(test_original.var(axis=0),columns=['Variance'])\ntest_original_var","62ccaf45":"# Call the function to return the test dataset features having zero variance.\nfeatures_zero_var(test_original)","2c3e3218":"# In test dataset, remove the same features of train dataset having 0 variance.\ntest_original_modified= test_original.drop(columns=['X11', 'X93', 'X107','X233', 'X235', 'X268', 'X289', 'X290', 'X293','X297','X330','X347'])","fb103f8c":"# Display the modified test dataset after removing the features having zero variance\ntest_original_modified.head()","e56f0b0b":"# Print the original and modified shape of the test dataset\nprint('Shape of original test dataset is:', test_original.shape)\nprint('\\nShape of modified test dataset after removing features having zero variance is:', test_original_modified.shape)","c4ee8bc8":"# Save the modified test dataset\ntest_original_modified.to_csv('test_original_modified.csv',index=False)","28759ba7":"# Check the null values in the train dataset\nprint('The sum of null values in the train dataset is:', train_original_modified.isnull().any().sum())","1aa5e6bf":"# Check the unique values in the train dataset\n# unique() function includes the missing value \n# nunique() function excludes the missing value as the default parameter is dropna=True\n# Since there are no missing values in the train and test dataset, we can use nunique()\ntrain_original_modified_UV=pd.DataFrame(train_original_modified.nunique(),columns=['Unique_Values'])\ntrain_original_modified_UV","ba3037b4":"# Print the train dataset features unique values where the values = 2 and values>2.\nprint('Train Features with unique values greater than 2 are as follows:\\n',train_original_modified_UV[train_original_modified_UV.Unique_Values>2].unstack())\nprint('Test Features with unique values equal to 2 are as follows:\\n',train_original_modified_UV[train_original_modified_UV.Unique_Values==2].unstack())","411b7c7b":"# Check the null values in the test dataset\nprint('The sum of null values in the test dataset is:', test_original_modified.isnull().any().sum())","34dd6d20":"# Check the unique values in the test dataset\ntest_original_modified_UV=pd.DataFrame(test_original_modified.nunique(),columns=['Unique_Values'])\ntest_original_modified_UV","b9937b21":"# Print the test dataset features unique values where the values = 2 and values >2.\nprint('Test Features with unique values greater than 2 are as follows:\\n',test_original_modified_UV[test_original_modified_UV.Unique_Values>2].unstack())\nprint('Test Features with unique values equal to 2 are as follows:\\n',test_original_modified_UV[test_original_modified_UV.Unique_Values==2].unstack())","dbedf1d1":"# Before applying label encoder, separate the ID and 'y' features from the train dataset\n##\n##Drop the columns 'ID' and 'y' and store the data into new object train_X_check and verify the shape\ntrain_X_check = train_original_modified.drop(columns = ['ID','y'])\n# train_X_check to verify how the one hot encoding works for the features with multi categorical variables \ntrain_X_check.shape","0d71e3ab":"# perform label encoder\/one hot encoding for the categories features of train_X_check\n##\n# Import the required library\nfrom sklearn.preprocessing import LabelEncoder","5c511bb2":"# Define the function to apply the label encoder for the categories features of train_X_check\ndef label_encoder(df,x):\n    features_cat=df.select_dtypes(include='object').columns # select only the features with datatype Object\n    le=LabelEncoder() # instantiate the label encoder\n    for i in features_cat:\n        x[i]=le.fit_transform(x[i]) # Fit,transform and replace with label encoded data for the existing data in the object datype columns of train_X_check","0b2a35a0":"# Call the function to apply label encoder for the train_X_check data\nlabel_encoder(train_original_modified,train_X_check)","de76b233":"# verify the train_X dataset\ntrain_X_check.head()","d7c763aa":"# Identify the top 10 most frequent categories of features X0,X1,X2,X3,X4,X5,X6,X8\n\n#X0\ntrain_original_modified.X0.value_counts().sort_values(ascending=False).head(10)","855702d8":"# create a list for the top 10 most frequent catergories of feature X0\ntop_10_X0 = [x for x in train_original_modified.X0.value_counts().sort_values(ascending=False).head(10).index]\ntop_10_X0","c599ce62":"# Define a funtion to peform one hot encoding for the top 10 most frequent categories of features\ndef one_hot_top10(df,feature,top10_categories):\n    for category in top10_categories:\n        df[feature+'_'+category]=np.where(train_original_modified[feature]==category,1,0)","4c705803":"# Call the function to perform one hot encoding on feature X0\none_hot_top10(train_original_modified,'X0',top_10_X0)","b82249c6":"# verify the train dataset after applying one hot encoding for the feature X0\ntrain_original_modified.head(3)","c6e100f1":"# verify for one feature if it contains only binary value\ntrain_original_modified.X0_z.unique()","a022e8f0":"# create a list for the top 10 most frequent categories of features X1, X2,X3,X4,X5,X6 and X8\n# Call the function to perform one hot encoding on features X1, X2,X3,X4,X5,X6 and X8 \n# verify the train dataset after applying one hot encoding for the features X1, X2,X3,X4,X5,X6 and X8 \n\n# X1 \ntop_10_X1 = [x for x in train_original_modified.X1.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X1',top_10_X1)\ntrain_original_modified.head(2)","a473977f":"# X2\ntop_10_X2 = [x for x in train_original_modified.X2.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X2',top_10_X2)\ntrain_original_modified.head(2)","a3e2728d":"# X3\ntop_10_X3 = [x for x in train_original_modified.X3.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X3',top_10_X3)\ntrain_original_modified.head(2)","628e4463":"# X4\ntop_10_X4 = [x for x in train_original_modified.X4.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X4',top_10_X4)\ntrain_original_modified.head(2)","1f4d2790":"# X5\ntop_10_X5 = [x for x in train_original_modified.X5.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X5',top_10_X5)\ntrain_original_modified.head(2)","175bf7ba":"# X6\ntop_10_X6 = [x for x in train_original_modified.X6.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X6',top_10_X6)\ntrain_original_modified.head(2)","7e1981b5":"# X8\ntop_10_X8 = [x for x in train_original_modified.X8.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(train_original_modified,'X8',top_10_X8)\ntrain_original_modified.head(2)","07cbfe42":"# Store the data set in train_original_modified to new object train_original_modified_OHE\n# drop the columns which are not required after performing one hot encoding\ntrain_original_modified_OHE = train_original_modified.drop(columns=['X0','X1','X2','X3','X4','X5','X6','X8'])\n# OHE means One hot encoded data","3b719e89":"# Save the modified one hot encoded train dataset\ntrain_original_modified_OHE.to_csv('train_original_modified_OHE.csv',index=False)","64f1c2a0":"# Check the shape of the modified one hot encoded train dataset\ntrain_original_modified_OHE.shape","823f2758":"# create a list for the top 10 most frequent catergories of features X0, X1, X2,X3,X4,X5,X6 and X8\n# Call the function to perform one hot encoding on features X0,X1,X2,X3,X4,X5,X6 and X8 \n# verify the train dataset after applying one hot encoding for the features X0,X1,X2,X3,X4,X5,X6 and X8 \n\n# X0\ntop_10_test_X0 = [x for x in test_original_modified.X0.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X0',top_10_test_X0)\ntest_original_modified.head(2)","40e63722":"# X1\ntop_10_test_X1 = [x for x in test_original_modified.X1.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X1',top_10_test_X1)\ntest_original_modified.head(2)","49638615":"# X2\ntop_10_test_X2 = [x for x in test_original_modified.X2.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X2',top_10_test_X2)\ntest_original_modified.head(2)","b2806854":"# X3\ntop_10_test_X3 = [x for x in test_original_modified.X3.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X3',top_10_test_X3)\ntest_original_modified.head(2)","3004dd22":"# X4\ntop_10_test_X4 = [x for x in test_original_modified.X4.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X4',top_10_test_X4)\ntest_original_modified.head(2)","a997438e":"# X5\ntop_10_test_X5 = [x for x in test_original_modified.X5.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X5',top_10_test_X5)\ntest_original_modified.head(2)","0f71fa39":"# X6\ntop_10_test_X6 = [x for x in test_original_modified.X6.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X6',top_10_test_X6)\ntest_original_modified.head(2)","d90418b1":"# X8\ntop_10_test_X8 = [x for x in test_original_modified.X8.value_counts().sort_values(ascending=False).head(10).index]\none_hot_top10(test_original_modified,'X8',top_10_test_X8)\ntest_original_modified.head(2)","2745800f":"# Store the data set in test_original_modified to new object test_original_modified_OHE\n# drop the columns which are not required after performing one hot encoding\ntest_original_modified_OHE = test_original_modified.drop(columns=['X0','X1','X2','X3','X4','X5','X6','X8'])\n# OHE means One hot encoded data","9d1b2dcb":"# Save the modified one hot encoded train dataset\ntest_original_modified_OHE.to_csv('test_original_modified_OHE.csv',index=False)","6f5c7957":"# Check the shape of the modified one hot encoded train dataset\ntest_original_modified_OHE.shape","b1ad548f":"# Before performing dimensional reduction aka. principal component anlysis (PCA), seperate the following:\n# features 'ID' and 'y' from train_original_modified_OHE dataset and store it in the new object\n# features 'ID' from test_original_modified_OHE.shape and store it in the new object","7229a1ae":"# store the 'ID' values into the new object train_ID and test_ID and verify the shape\n\n# Train dataset\ntrain_ID = train_original_modified_OHE.ID\nprint('The shape of the ID feature in train dataset is:',train_ID.shape)\n\n# Test dataset\ntest_ID = test_original_modified_OHE.ID\nprint('\\nThe shape of the ID feature in test dataset is:',test_ID.shape)","14214e45":"# store the remaining values into the new object train_X and test_X and verify the shape\n\n# Train dataset\ntrain_X = train_original_modified_OHE.drop(columns = ['ID','y'])\nprint('The shape of the final train dataset is:',train_X.shape)\n\n# Test dataset\ntest_X = test_original_modified_OHE.drop(columns = ['ID'])\nprint('\\nThe shape of the final test dataset is:',test_X.shape)","5859225d":"# store the 'y' values into the new object train_y and verify the shape\ntrain_y=train_original_modified_OHE.y\nprint('The shape of the target feature of train dataset is:',train_y.shape)","57beff30":"# save these datasets to csv \ntrain_ID.to_csv('train_ID.csv',index=False)\ntest_ID.to_csv('test_ID.csv',index=False)\ntrain_X.to_csv('train_X.csv',index=False)\ntest_X.to_csv('test_X.csv',index=False)\ntrain_y.to_csv('train_y.csv',index=False)","86b47468":"# import the required PCA library\nfrom sklearn.decomposition import PCA","d064d84e":"# Before performing PCA, the data needs to centered and scaled\n# After centring, the average value for each train and train features will be 0\n# After scaling, the standard deviation for each feature will be 1","5b9289e8":"# create a PCA object (instantiate)\npca=PCA()","a346228e":"# fit and transform the train dataset\n# transform the test dataset\n\n# fit the train dataset\npca_fit_train_X = pca.fit(train_X)\n\n# transform the  train dataset\npca_fit_transform_train_X = pca_fit_train_X.transform(train_X)\n\n# transform the test dataset\npca_transform_test_X = pca.transform(test_X)","6d5dce91":"# visualize the train_X dataset using scree plot to see how many principal components should go into the final plot.\n# Calculate the percentage of variation of each principal components\n# Assuming we have two principal components PC1 and PC2, then  \n# explained_variance_ratio for PC1 = (Variation for PC1\/ (Total variation ie (PC1+PC2)))*100\n##\npca_train_X_variation = np.round(pca_fit_train_X.explained_variance_ratio_.cumsum()*100,decimals=1)\n##\n# cumsum() is used to display PC's variation with respect to cumulative percentage","cc34f999":"# Print the cumulative percentage of expalined variance\npca_train_X_variation","a9b159e9":"# create a PCA object again (instantiate) by considering first 72 principal components\npca_72=PCA(n_components=72)","07019af8":"# fit and transform the train dataset\n# transform the test dataset\n\n# fit the train dataset\npca_fit_train_X_72 = pca_72.fit(train_X)\n\n# transform the train dataset\npca_fit_transform_train_X_72 = pca_fit_train_X_72.transform(train_X)\n\n# transform the test dataset\npca_transform_test_X_72 = pca_72.transform(test_X)","d62123f9":"## check the variation for the first 72 principal components \npca_train_X_variation_72 = np.round(pca_fit_train_X_72.explained_variance_ratio_.cumsum()*100,decimals=1)\npca_train_X_variation_72\n##","40952b27":"# assign labels for each PC's as PC1,2,etc., for visulaization in scree plot \n##\nlabels = ['PC' + str(x) for x in range(1, len(pca_train_X_variation_72)+1)]\n##\n# All the first 72 features of train_X data set will be labelled as PC1, PC2, .....+ PC72\n# Eg: feature X10 will be labelled as PC1, etc., \n# Here X10 will be the first feature since we had one hot encoded and dropped the below listed categorical features\n# X0\tX1\tX2\tX3\tX4\tX5\tX6\tX8\n# After one hot encoding, the newly created columns\/features will get automatically moved to the end.\n# Hence the first PC1 will be the feature X10 and so on.....","7b989088":"# generate the scree plot\n## \nplt.figure(figsize=(25,5))\n##\nplt.bar(x=range(1, len(pca_train_X_variation_72)+1), height=pca_train_X_variation_72,tick_label=labels)\n##\nplt.xticks(rotation=90, color='indigo', size=15)\nplt.yticks(rotation=0, color='indigo', size=15)\n##\n##################\nplt.title('Scree Plot',color='tab:orange', fontsize=25)\n###################\n##\nplt.xlabel('Principal Components', {'color': 'tab:orange', 'fontsize':15})\nplt.ylabel('Cumulative percentage of explained variance ', {'color': 'tab:orange', 'fontsize':15})\n##","ffb731ae":"# Draw the 2D PCA plot by considering only PC1 and PC2\n# PCA plot is to visualize how the data is spread across the origin with new coordinates, based on the loading scores and scaling.","1fef9512":"####\n# Put the new coordinates created by pca_fit_transform_train_X_72 into matrix\n# Rows are the observations (X) and columns are the Principal components (Y)\n####\npca_fit_transform_train_X_72_df = pd.DataFrame(pca_fit_transform_train_X_72,columns=labels )\n#####\n# verify the first 2 rows of data with new coordinates\npca_fit_transform_train_X_72_df.head(2)\n#####","2dd4ee8f":"# Draw the 2D PCA plot for PC1 and PC2\n\n##\n## Removing the cumsum() from the earlier expalained ratio calculation\npca_train_X_variation_72_Nocumsum = np.round(pca_fit_train_X_72.explained_variance_ratio_*100,decimals=1)\n##\nplt.title('PCA Plot',color='tab:orange', fontsize=20)\n##\nplt.scatter(pca_fit_transform_train_X_72_df.PC1, pca_fit_transform_train_X_72_df.PC2)\n##\nplt.xticks(rotation=90, color='indigo', size=15)\nplt.yticks(rotation=0, color='indigo', size=15)\n##\nplt.xlabel('PC1 - {0}%'.format(pca_train_X_variation_72_Nocumsum[0]), {'color': 'tab:orange', 'fontsize':15});\nplt.ylabel('PC2 - {0}%'.format(pca_train_X_variation_72_Nocumsum[1]), {'color': 'tab:orange', 'fontsize':15});\n##\n## The principal components are zero-indexed, So, PC1=[0], PC2=[1]","1589efee":"# print the loading scores\n# Loading scores explains the proportion of each observation with respect to each principal components\n#\n#Lets check only for the PC1\n#\nloading_scores = pd.Series(pca_72.components_[0])\n#\n#\n#sort the loading scores based on absolute value\nsorted_loading_scores=loading_scores.abs().sort_values(ascending=False)\n#\n# display only the top 10 loading scores\nsorted_loading_scores[0:10]","83e3f89d":"# Print the minimum and maximum loading scores of PC1\nprint(sorted_loading_scores.min())\nprint(sorted_loading_scores.max())","2460cd90":"# From PCA, the final train and test datasets are as follows\n\n#train data\npca_fit_transform_train_X_72.shape","c3547776":"#test data\npca_transform_test_X_72.shape","8aa280ca":"#train label\ntrain_y.shape","6b646556":"# train ID\ntrain_ID.shape","7c928699":"test_ID.shape","d6d50e15":"# Before predicting the test values, lets check the target variable train_y for any outliers\n# If present, the value will be replaced with median values\n# Using boxplot to identify the outliers\nplt.boxplot(train_y);","2d1c2b78":"# Print the 50th percentile value which is the median\nprint(train_y.quantile(0.50)) ","a9c73bfe":"# Print the 95th percentile value \nprint(train_y.quantile(0.95)) ","c946fd23":"# Replace the outlier with median values\ntrain_y = np.where(train_y > 120.80600000000001, 99.15, train_y)","7490a497":"# Verify again with box_plot after replacing the outliers with median values\nplt.boxplot(train_y);","ae607ce3":"# Check the shape again   \ntrain_y.shape","a725d359":"# import the required libraries\nimport xgboost as xgb\nfrom sklearn.model_selection import cross_val_score,cross_val_predict\n# Since objective is to predict continuous variable we use XGBregressor\nfrom xgboost import XGBRegressor","1fc307ed":"# Evaluation metrics for regression\n### Mean Absolute Error, Mean Squared Error and R2\n# We will use R2 in this case\n# R2 is also known as Coefficient of Determination\n# It gives the percentage variation in 'y' (test time) explained by 'X'variables\n# or,it gives the percentage of data points that fall within the regression line\n# R2= (1-SSR\/SST) \n# SSR- Sum of square residual; SST- Sum of squares total\n# R2 value should be between 0 to 1\n# -R2 valve indicates the worst model","05dc6a6c":"# print the XGBoost parameters\nprint(XGBRegressor())","dd139bab":"# Instantiate the Regressor\n# specifying random_state ensures same result if we run the model multiple times\n# Objective will be automatically set to ''reg:squarederror'\nxgb_reg = xgb.XGBRegressor() ","ace3d0e8":"#To find best XGBoost Parameters\nparams={ 'learning_rate'   : [0.01,0.05,0.1,1] ,\n         'max_depth'       : [2,3,5,10],\n         'min_child_weight': [ 0, 1, 3],\n         'n_estimators'    : [100,150,200,500],\n         'gamma'           : [1e-2,1e-3,0,0.1,0.01,0.5,1],\n         'colsample_bytree': [0.1,0.5,0.7,1],\n         'subsample'       : [0.2,0.3,0.5,1],\n         'reg_lambda'      : [0,1,10],\n         'reg_alpha'       : [1e-5,1e-3,1e-1,1,1e1] \n        }\n## Explainations of the parameters\n# 'max_depth' - Maximum depth of trees (default = 6, range: [0,\u221e])\n# 'Learning rate'(eta) - scaling the tree by learning rate predicts the output in smaller steps closer to the actual value.\n# 'reg_lambda' - L1 regularization parameter on weights to avoid overfit\n#  'reg_alpha' - L1 regularization parameter on weights to avoid overfit\n# 'gamma' - Minimum loss reduction required to make a further partition on a leaf node of the tree (pruning)\n# 'min_child_weight' - default =1. If the weights of each leaf is less than the min_child weight, then ramove the leaf\n# So weights of the each leaf is > min_child_weight\n# 'colsample_bytree': It is the subsample ratio of columns when constructing each tree.\n# 'Subsample' is the  ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. This will prevent overfitting. \n# Subsampling will occur once in every boosting iteration.\n# 'n_estimators' is the number of trees","638bb530":"# Optimize the Hyperparameter using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","0830ab26":"# Using Random search of parameters with 10 fold cross validation\n# Improve the predictions using cross validation to optimize the parameters\nRandom_Search=RandomizedSearchCV (xgb_reg,params,cv=10, scoring='r2', return_train_score=True, n_jobs=-1,verbose=1) \n# cv=10 - Number of folds in a `(Stratified)KFold`","a068b271":"# Fit the training set to the Randon_Search to obtain the best estimators and parameters.\nRandom_Search.fit(pca_fit_transform_train_X_72,train_y)","9def0c87":"# Print the best estimator\nRandom_Search.best_estimator_","9db35855":"#Print the best parameters\nRandom_Search.best_params_","f3dcd430":"# Instantiate the XGBoost classifier with the best estimators and parameters\nxgb_reg=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.5, gamma=0.01, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=2,\n             min_child_weight=3, missing=None, monotone_constraints='()',\n             n_estimators=500, n_jobs=2, num_parallel_tree=1, random_state=0,\n             reg_alpha=0.1, reg_lambda=10, scale_pos_weight=1, subsample=0.5,\n             tree_method='exact', validate_parameters=1, verbosity=None)","e101e554":"# Check the r2 score of the model using Number of folds in a `(Stratified)KFold` cv=10\nr2_Score = cross_val_score(xgb_reg,pca_fit_transform_train_X_72,train_y,scoring='r2',cv=10)\nr2_Score","0621d9ce":"# Print the mean r2_score\nprint('r2_score of the model with cross validation is:',round(r2_Score.mean(),2))","aeb278d1":"# Fit the training data\nxgb_reg.fit(pca_fit_transform_train_X_72,train_y)","46f3a4f7":"# predict the time taken by car to pass testing using test dataset\nX_test_pred = xgb_reg.predict(pca_transform_test_X_72)\nX_test_pred ","f92ebeeb":"# print the predicted value (time) in the form of table\ndf_test_pred = pd.DataFrame({'ID': test_ID, 'y': X_test_pred})\n# Print the first 10 predicted values\ndf_test_pred.head(10)","561668f3":"# save the predicted time values \ndf_test_pred.to_csv('submission.csv', index=False)","70dd6cd4":"**Comments**: \n> 1. It is observed that all the valus in ID's are unique.In the provided dataaset, the ID represents the unique car configuration. So, this feature must be ignored for testing as it will not make any sense to the prediction.\n> 2. Features X0,X1,X2,X3,X4,X5,X6,X8 are the categorical features which must be converted to numerical values\/one hot encoded values.\n> 3. All the features after X8 are having binary values.","f577ca33":"**Comments**: \n> The modified train dataset contains 366 features, which means the 12 features with zero variance from the original dataset is removed.","22ca55bd":"**Scope of Future Work:**\n> 1.  I have conisdered 72 principal components for the study using principal component analysis(PCA). Features can be further reduced by using different dimensionality reduction techniques [[Reference](https:\/\/thenewstack.io\/3-new-techniques-for-data-dimensionality-reduction-in-machine-learning\/)].\n> 2.  I have used only XGBoost Regressor algorithm for predicting the time, a Mercedes-Benz spends on the test bench. R2 score can be further improved by using other regressor algorithms with hyperparameter tuning and cross validation. \n> 3. As the number of features are more, Deep learning techniques can also be used for the study. ","c010cc1a":"**Comments**: \n> 1. Successfully separated the 'ID' and target feature 'y' from train dataset.\n> 2. Successfully separated the 'ID' feature from test dataset.\n> 3. Also, observed that shape of the train and test data are same.It is therefore good to proceed further.","01ede311":"**Comments**: \n> In case X3, only 7 columns were added as the maximum categorial variable for this feature is only 7.","ba18c0c2":"## Apply the label encoder\/one hot encoding  for the train dataset","61ba5a63":"**Conclusion**:\n>  For a given dataset, XGBoost Regressor algorithm with cross validation results in R2 score of **0.62**.","bb797fa2":"**Comments:** \n> 1. Above PCA plot shows that how the data is spread along X-axis(PC1) and Y-axis (PC2).\n> 2. 11.9% variance of the data is explained by PC1 and 8.2 % of data is explained by PC2\n> 3. Similarly we visulaize how the data is spread among other pricipal components as well","ab5e6865":"**Comments**: \n> In case X4, only 4 columns were added as the maximum categorial variable for this feature is only 4.","c4a77ab5":"**Comments**\n > 1. There are 4209 data points and 378 features in the dataset.\n > 2. It is observed that there are more number of features with binary values. Hence sparsity exists in the train data.\n > 3. Target feature 'y' from the training data is the time that cars spend on the test bench.","6c296ba3":"**Comments**: \n> 1. The modified test dataset contains 365 features. This is because, we had removed the same 12 features having zero variance in the train dataset.\n> 2. The modified test dataset has 365 features, whereas modified train dataset had 366 features. This is because target feature 'y' is missing  in the train dataset","dde5e218":"**Comments** \n> 1. Since r2_score with cross validation is: 0.62 or 62 % which is between 50 to 100%. Hence, its good to proceed with the prediction of time the car takes to pass testing using test data.\n> 2. This means the model explains 62% variability of the target variable (y) around its mean.","4c0fa0c9":"**Comments:** \n> Since all the features are having 0 and 1, there is no need to standardize.","db528555":"**Comments:** \n> label encoder\/one hot encoding is successfully applied to the train dataset","8fdfa4d5":"**References:**\n> 1. Dataset and problem statement: https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing \n> 2. https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing\/discussion\n> 3. https:\/\/medium.com\/swlh\/greener-manufacturing-with-machine-learning-6ec77d0e7a91\n> 4. How to Perform One Hot Encoding for Multi Categorical Variables https:\/\/www.youtube.com\/watch?v=6WDFfaYtN6s","3aebcab3":"##  Check the null and unique values in the train dataset","869d3e07":"**Comments**: \n> 1. It is observed that all the values in ID's are unique. In the provided dataaset, the ID represents the unique car configuration. So, this feature must be ignored for training as it will not make any sense to the prediction. \n> 2. 'y' feature is the target feature. \n> 3. Features X0,X1,X2,X3,X4,X5,X6,X8 are the categorical features which must be converted to numerical values\/one hot encoded values.\n> 4. All the features after X8 are having binary values.","4998c70f":"**Comments:** \n> There are no null values in the train dataset.","9614a0c4":"**Comments**: \n> 1. It is evident from the box plot that outliers are replaced with median values in the target variable train_y\n> 2. Also, there is no change in the shape of the target variable. Hence its good to go with further steps","72a28949":"**Comments**: \n> Above listed features have zero variance.","f894cc3c":"## Predict test_data using XGBoost.","8585850c":"## Similarly remove the zero variance features from the test dataset ","b73f6402":"**Comments**.\n> 1. The train dataset has one feature with float64 datatype which is target (Y) features\n> 2. The train dataset has 369 features with int64 datatype which are features having binary values (0 and 1)\n> 3. The train dataset has 8 features with object datatype which are features having categorical data","98b5bc2b":"**Comments:** \n> The shape of the train dataset is reduced to 428 from 436 since 8 features were dropped after performing one hot encoding","b8a9a360":"### Similarly check the  null and unique values in the  test dataset ","9764ea53":"**Comments**: \n> 1. Above listed features have zero variance.\n> 2. However, since test dataset is not considered for training and only used for testing, we can remove the same features of train dataset in the test dataset as well.\n> 3. This will ensure the same size and shape of the train and test dataset","81ff10ba":"**Comments**: \n> In case X3, only 7 columns were added since there are only 7 unique categories in in this feature.","47e8faed":"**Comments:** \n> The shape of the train dataset is reduced to 429 from 437 since 8 features were dropped after performing one hot encoding","b56e6d4b":"**Comments**:\n> 1. It can be concluded from the above loading scores that, almost all the observations of the train datasets plays a role in separating the Principal components PC1\n> 2. Example: The 175th observation has a 1 unit long vector consisting of the following:\n>    - 0.191403 * PC1 +.......+ Xn * PCn \n>    - 0.191403 is the proportion of 175th observation for PC1 \n>    - This unit vector is called singular vector or eigen vector for PC1\n> 3. similarly the loading scores will be calculated for PC2 as pca_72.components_[1], etc","275fb58b":"**Comments**: Outliers are observed in the target variable train_y","12878196":"**Comments**: \n> 1. By looking into the array of elements, It is observed that among 427 Principal components, ~90% of variation of the data in train_X dataset is explained by only first 72 principal components . \n> 2. lets validate by visualizing scree plot for the first 72 pricipal components ","157389ad":"## Perform dimensionality reduction.","20f2fc7d":"**Comments:**\n> Out of 366 train dataset features, 10 features are having greater than 2 unique values and remaining features are having only 2 unique values (0 and 1). ","2fc52b93":"**Comments:**\n> 1. It is evident from the above table that 10 new features are created with only binary values, for the top 10 most frequent categories of feature X0. \n> \n> 2. Hence the total columns\/features are increased from 366 to 376.\n> \n> 3. Similarly perform one hot encoding for the top 10 frequent categories of remaining categorical features.","8d71cc9e":"**Comments**\n> 1. There are 4209 data points and 377 features in the test dataset.\n> 2. Number of features in the test dataset is 377 because Target feature 'y' is missing in the test dataset compared to train dataset.","88e9488d":"**Comments:** \n> label encoder\/one hot encoding is successfully applied to the test dataset","f046e750":"# Problem Statement: Refer [Mercedes-Benz Greener Manufacturing](https:\/\/www.kaggle.com\/c\/mercedes-benz-greener-manufacturing) \n\n## **Objective:** To Reduce the time a Mercedes-Benz spends on the test bench","e146cfe2":"## Apply the label encoder\/one hot encoding for the test dataset similar to the train dataset","26bd7248":"**Comments:**\n> Above scree plot shows that considering first 72 principal components should be sufficient to represent the train_X dataset","8e45a62a":"**Comments**\n> 1. It is observed from the above table that, label encoder is applied to categorial features of train_X_check data, the encoded labels are not binary (0 and 1) since the features has 4 or more different categories.\n> \n> \n> 2. Example: X5 feature has 29 unique categories. So, when label encoder is applied, the categories will be replaced with 28 unique values starting from 0 to 28.\n> \n> \n> 3. This may impact the accuracy level.\n> \n> \n> 4. Inorder to fix this issue, the following procedure is followed here [[Reference](https:\/\/www.youtube.com\/watch?v=6WDFfaYtN6s)]\n>     - Identify the top 10 most frequent categories from each feature. \n>     \n>     - Perform one hot encoding only for the top 10 most frequent categories.\n>     \n>     - All Top 10 most frequent categories will be considered as '1' and all the remaining categories will be considered as '0' in each feature.\n>     \n>     - By performing above 3 steps ensures only binary values (0 and 1) in all the features","6075a87d":"**Note:**\n\n> 1. In the fit step, loading scores and variation of each principal components are calculated\n> 2. fit method is used only for the train dataset as the test dataset will only learn from the train dataset.\n> 3. In the transform step, cordinates for the PCA plot are generated based on the loading scores and scaled data(we have not scaled the data since all features are having 0 and 1 values).\n> 4. test dataset is only transformed. This is because when we feed the test dataset to the algorithm,it will learn the loading scores and variation of each principal components from the train dataset and predict the outcome.   ","d3c28fc9":"**Comments:**\n> Out of 365 test dataset features, 9 features are having greater than 2 unique values and remaining features are having only 2 unique values (0 and 1). ","c893a99e":"**Comments**: \n> In case X4, only 4 columns were added since there are only 4 unique categories in in this feature.","6160dc28":"**Comments:** \n> There are no null values in the test dataset."}}