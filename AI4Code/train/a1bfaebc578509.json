{"cell_type":{"0352f98e":"code","1d4aa077":"code","b03a06d3":"code","c6b888db":"code","100ab3d3":"code","743c2129":"code","6e1ab0e5":"code","042a59c2":"code","e99a986b":"code","724d45dc":"code","66d8c48c":"code","b864fdbe":"code","c35dd2d0":"code","7c1341cc":"code","00319b2c":"code","bb9e3422":"code","afc2c909":"code","a6d13df5":"code","56b5cc58":"code","02b70141":"code","9fb66bf1":"code","65d21e6a":"code","af66eb2c":"code","03fe5729":"code","fed5e140":"code","f6671012":"markdown","98cb1a34":"markdown","ebd952b8":"markdown","b18110b2":"markdown","0f015ee5":"markdown","2ca744f5":"markdown","ccc69208":"markdown","3c3e7eb5":"markdown","2193630d":"markdown","e178b70f":"markdown","8b677dd2":"markdown","eb74190b":"markdown","7d9b4aae":"markdown","fa5bbad3":"markdown","ab657bbf":"markdown","7b97d9c3":"markdown","2ac9f668":"markdown","aab9b54f":"markdown","124018bd":"markdown","cb93b366":"markdown","fae4e04e":"markdown","e00f1052":"markdown"},"source":{"0352f98e":"import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\nimport keras\nimport pandas as pd\nimport lightgbm as lgb\nimport time\nimport numpy as np\nimport pickle\nimport sklearn\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nfor package in [pd, keras, np, lgb, sklearn]:\n    print(package.__version__)","1d4aa077":"sales_train    = pd.read_csv('sales_train.csv')\nitems           = pd.read_csv('items_english.csv')\nitem_categories = pd.read_csv('categories_english.csv')\nshops           = pd.read_csv('shops_english.csv')\ntest           = pd.read_csv('test.csv')\n\nsales_train.head(10)","b03a06d3":"def get_town(shop_name):\n    pre = shop_name.split()[0]\n    if pre == 'St.':\n        return 'St Petersburg'\n    elif pre == '!':\n        return 'Yakutsk'\n    elif 'Rostov' in pre:\n        return 'Rostov'\n    elif 'Moscow' in pre:\n        return 'Moscow'\n    elif 'Internet' in pre or 'Digital' in pre:\n        return 'Online'\n    else:\n        return pre\n\nshops['town'] = shops['shop_name'].apply(get_town)\nshops.head(60)","c6b888db":"def get_subcategories(category_name):\n    s = category_name.split(' - ')\n    if len(s) == 2:\n        return s[0], s[1]\n    else:\n        return s[0], None\n    \nitem_categories['category_1'], item_categories['category_2'] = zip(*item_categories['category_name'].apply(get_subcategories))\nitem_categories.head(20)","100ab3d3":"print(f'{len(sales_train)} total sales {len(items)} items {len(item_categories)} categories in train')\nprint(f'{len(test)} pairs {test[\"shop_id\"].nunique()} shops {test[\"item_id\"].nunique()} items in test')\nsales_train.isnull().sum()","743c2129":"print(f'{sales_train.duplicated().sum()} duplicates found and removed')\nsales_train.drop_duplicates(inplace=True)","6e1ab0e5":"print(f'{(sales_train[\"item_price\"] < 0).sum()} item_price < 0 removed')\nsales_train = sales_train[sales_train['item_price'] >= 0]","042a59c2":"sales_train[sales_train['shop_id'] == 57]['shop_id'] = 0\nsales_train[sales_train['shop_id'] == 58]['shop_id'] = 1","e99a986b":"sales_train[['date_block_num','item_cnt_day']].groupby('date_block_num').sum().plot()\nplt.show()\nsales_train[['item_price']].boxplot(vert=False)\nplt.show()\nsales_train[['item_cnt_day']].boxplot(vert=False)\nplt.show()","724d45dc":"sales_train.drop(sales_train['item_price'].idxmax(), inplace=True)\nsales_train.drop(sales_train['item_cnt_day'].idxmax(), inplace=True)","66d8c48c":"dates = sales_train[['date','date_block_num']]\ndates['date'] = pd.to_datetime(dates['date'],format='%d.%m.%Y')\ndates['month'] = dates['date'].dt.month\ndates['year'] = dates['date'].dt.year\ndates.drop('date',axis=1,inplace=True)\ndates.drop_duplicates(inplace=True)\n\n#add test block num\ndates = dates.append({'date_block_num' : 34, 'month' : 11, 'year' : 2015},ignore_index=True)\ndates.head(50)\n","b864fdbe":"df = sales_train[['date_block_num','shop_id','item_id','item_cnt_day']]. \\\n    groupby(['date_block_num','shop_id','item_id'], as_index=False).agg({'item_cnt_day' : 'sum'})\ndf['item_cnt_month'] = df['item_cnt_day']\ndf.drop(['item_cnt_day'],inplace=True, axis=1)\ndf.tail(15)","c35dd2d0":"data = pd.DataFrame()\nfor date_block_num in df['date_block_num'].unique():\n    block = df[df['date_block_num'] == date_block_num]\n    new = block.merge(test, on=['shop_id','item_id'],how='outer')\n    new.dropna(subset=['ID'],inplace=True)\n\n    new['date_block_num'].fillna(date_block_num,inplace=True)\n    new['item_cnt_month'].fillna(0 ,inplace=True)\n    \n    data = pd.concat([data, new])\n\n#add test data as date block num 34, but leave item_cnt_month as null (target value)\ndata.tail()","7c1341cc":"test['date_block_num'] = 34\ndata = pd.concat([data, test])\ndata.tail()","00319b2c":"pair = data['shop_id'].astype(str) + '-' + data['item_id'].astype(str)\n\nassert len(pair.value_counts().unique()) == 1\nassert pair.value_counts().unique() == 35","bb9e3422":"item_data = data.merge(items, on=['item_id'])\ncat_data = item_data.merge(item_categories, on=['category_id'])\nfull_data = cat_data.merge(shops, on=['shop_id'])\nfull_data_dates = full_data.merge(dates, on=['date_block_num'])\nfull_data_dates.sort_values('date_block_num',inplace=True)\n","afc2c909":"full_data_dates.drop(columns=['shop_name','category_name','item_name'],inplace=True)\nfull_data_dates.tail()","a6d13df5":"data = full_data_dates\ndata['category_1'] = data['category_1'].astype('category').cat.codes\ndata['category_2'] = data['category_2'].astype('category').cat.codes\ndata['town'] = data['town'].astype('category').cat.codes\ndata['item_cnt_month'] = data['item_cnt_month'].clip(0, 20)\n\nshift_range = [1,2,3,6,12]\nfor shift in shift_range:\n    \n    train_shift = data[['ID','date_block_num','item_cnt_month']]\n    train_shift['date_block_num'] = train_shift['date_block_num'] + shift\n    train_shift[f'item_cnt_month_lag_{shift}'] = train_shift['item_cnt_month']\n    train_shift.drop('item_cnt_month',inplace=True,axis=1)\n    \n    data = data.merge(train_shift, on = ['ID','date_block_num'],how='left')\n    data[f'item_cnt_month_lag_{shift}'] = data[f'item_cnt_month_lag_{shift}'].fillna(0)\n\ndata.head()","56b5cc58":"mae_cols = ['shop_id','item_id','category_id','category_1','category_2','town']\ndates = data['date_block_num']\nvalidation_block = 33\n\nts = time.time()\n#for date_block_num in dates.unique():\n\nres = pd.DataFrame()\n\nfor date_block_num in dates.unique():\n    \n    subset = data.loc[dates == date_block_num]\n    \n    for col in mae_cols:\n\n        df1 = data.loc[(dates < date_block_num) & (dates > date_block_num - 6), ['date_block_num', col, 'item_cnt_month']]\n        df2 = data.loc[(dates < date_block_num) & (dates > date_block_num - 12), ['date_block_num', col, 'item_cnt_month']]\n\n        matrix1 = df1.drop('date_block_num',axis=1)\n        avg1 = matrix1.groupby(col, as_index=False).mean()\n        avg1['date_block_num'] = date_block_num\n        avg1.rename(columns={'item_cnt_month' : f'{col}_cnt_6_month_average'}, inplace=True)\n        \n        matrix2 = df2.drop('date_block_num',axis=1)\n        avg2 = matrix2.groupby(col, as_index=False).mean()\n        avg2['date_block_num'] = date_block_num\n        avg2.rename(columns={'item_cnt_month' : f'{col}_cnt_12_month_average'}, inplace=True)\n\n        subset = subset.merge(avg1, on=[col,'date_block_num'],how='left')\n        subset = subset.merge(avg2, on=[col,'date_block_num'],how='left')\n        \n    res = pd.concat([res, subset])\n\nres.fillna(0, inplace=True)\nprint(f'calcualting mae features took {time.time() - ts} seconds')\n\n#discard old data\nres = res[res.date_block_num > 11]\nres = res.sort_values(by = ['date_block_num','ID'], ascending = [True, True])\nprint(res.columns)\nprint(len(res))\n\n#check ordering is the same for two date blocks as this property will be used later\nassert (res[res.date_block_num == 15]['ID'] == res[res.date_block_num == 12]['ID']).all()\n\nX_train = res[res.date_block_num < validation_block].drop(['item_cnt_month','date_block_num'], axis=1)\ny_train = res[res.date_block_num < validation_block]['item_cnt_month']\nX_valid = res[res.date_block_num == validation_block].drop(['item_cnt_month','date_block_num'], axis=1)\ny_valid = res[res.date_block_num == validation_block]['item_cnt_month']","02b70141":"ts = time.time()\nlgb_params = {\n    'feature_fraction': 0.75,\n    'metric': 'rmse',\n    'bagging_fraction': 0.75, \n    'learning_rate': 0.03, \n    'objective': 'mse',\n    'num_leaves': 1000,\n    'max_depth' : 20,\n    'bagging_freq':1,\n    'verbose':-1,\n}\n\ntrain_data = lgb.Dataset(X_train, label = y_train)\nvalid_data = lgb.Dataset(X_valid, label = y_valid)\n\nlgb_model = lgb.train(\n    lgb_params,\n    train_data,\n    valid_sets=valid_data,\n    num_boost_round=1000,\n    verbose_eval=False\n)\nt = time.time() - ts\n\n#save lgb model\nwith open('lgb_model.pkl','wb') as f:\n    pickle.dump(lgb_model, f)\n\ny_pred_train = lgb_model.predict(X_train)\n\nr2 = r2_score(y_train, y_pred_train)\nmse = mean_squared_error(y_train, y_pred_train)\n\nprint(f'R-squared for training data is {r2}')\nprint(f'MSE for training data is {mse}')\nprint(f'Fitting time is {t} seconds')","9fb66bf1":"ts = time.time()\n\nnn_model = Sequential()\n\nnn_model.add(Dense(100, input_dim=26, activation='relu'))\nnn_model.add(Dense(100, activation='relu'))\nnn_model.add(Dense(1, activation='linear'))\n\nnn_model.compile(loss='mse', optimizer='adam')\nnn_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_valid, y_valid))\n\nt = time.time() - ts\n\n#save lgb model\nwith open('nn_model.pkl','wb') as f:\n    pickle.dump(nn_model, f)\n\ny_pred_train = nn_model.predict(X_train)\n\nr2 = r2_score(y_train, y_pred_train)\nmse = mean_squared_error(y_train, y_pred_train)\n\nprint(f'R-squared for training data is {r2}')\nprint(f'MSE for training data is {mse}')\nprint(f'Fitting time is {t} seconds')","65d21e6a":"y_pred_lgb = lgb_model.predict(X_valid)\nr2 = r2_score(y_valid, y_pred_lgb)\nmse = mean_squared_error(y_valid, y_pred_lgb)\n\nprint(f'Test R-squared for block {validation_block} is {r2} for lgb')\nprint(f'Test MSE for block {validation_block} is {mse} for lgb')\n\ny_pred_nn = nn_model.predict(X_valid)\nr2 = r2_score(y_valid, y_pred_nn)\nmse = mean_squared_error(y_valid, y_pred_nn)\n\nprint(f'Test R-squared for block {validation_block} is {r2} for nn')\nprint(f'Test MSE for block {validation_block} is {mse} for nn')\n","af66eb2c":"meta_features = np.stack([y_pred_nn.squeeze(axis=1), y_pred_lgb], axis=1)\n\nmeta_model = LinearRegression()\nmeta_model.fit(meta_features, y_valid)\n\ny_pred_meta = meta_model.predict(meta_features)\nr2 = r2_score(y_valid, y_pred_meta)\nmse = mean_squared_error(y_valid, y_pred_meta)\n\n#save lgb model\nwith open('meta_model.pkl','wb') as f:\n    pickle.dump(meta_model, f)\n    \nprint(f'R-squared for training is {r2} for meta')\nprint(f'MSE for training is {mse} for meta')\n","03fe5729":"with open('nn_model.pkl','rb') as f:\n    nn_model = pickle.load(f)\nwith open('lgb_model.pkl','rb') as f:\n    lgb_model = pickle.load(f)\nwith open('meta_model.pkl','rb') as f:\n    meta_model = pickle.load(f)","fed5e140":"test_block = 34\n\nX_test = res[res.date_block_num == test_block].drop(['item_cnt_month','date_block_num'], axis=1)\ny_test = res[res.date_block_num == test_block]['item_cnt_month']\n\nmeta_nn = nn_model.predict(X_test).squeeze(axis=1)\nmeta_lgb = lgb_model.predict(X_test)\nmeta_features = np.stack([meta_nn, meta_lgb], axis=1)\n\ny_test = meta_model.predict(meta_features).clip(0, 20)\nX_test['item_cnt_month'] = y_test\n\nsub = X_test[['ID','item_cnt_month']]\n\nsub['ID'] = sub['ID'].astype(int)\nsub = sub.sort_values('ID')\n\nsub.to_csv('submission.csv', index=False)","f6671012":"Month and year are also useful as features","98cb1a34":"Sales over time","ebd952b8":"Fit meta model. Note that in this case, linear regression has no hyper parameters to tune, so can fit on the validation block and then use the resulting model on the test data","b18110b2":"Add mean encoded features for 6 and 12 month intervals for relevant columns. Discard old data (from 2013) but still use it for mean encoding.","0f015ee5":"Clip sales to acceptable range and add 1,2,3,6 and 12 month lag for each pair","2ca744f5":"We now have our training data. The first 34 blocks will be used to fit several time series models with the final one for testing. Merge item and shop names into our data for feature engineering","ccc69208":"Extract features from shops and categories. We see the first part of shops is usually a town. We can use this as a feature. Since there are not many shops, we can define a function to take care of edge cases","3c3e7eb5":"Get unique items, duplicate rows and info on nulls","2193630d":"Now start aggregating the data into the correct form for the task","e178b70f":"Load required data into dataframe","8b677dd2":"item_price should not be negative (returns are signified by item_cnt_day being negative). Check for these rows and remove","eb74190b":"Only concerned with pairs in the test set, so find these for every month. For each date block, join with the test data on shop_id and item_id. If a row has ID null, it doesn't appear in the test data and may therefore be dropped. If item_cnt_month is null, it indicates that the pair had no sales in the date block and this should be filled with 0. If a row has no null values, that indicates the pair appeared in both the train and test data.","7d9b4aae":"Run this cell to load models to avoid training time","fa5bbad3":"Perform similar preprocessing on categories. In most cases, the category is divided into \"{category} - {subcategory}\". Can use this as a feature","ab657bbf":"Train lightGBM","7b97d9c3":"Sales seem to peak in November and December and steadily drop over time. We are almost certainly safe to remove the maximum value in both series. ","2ac9f668":"Neural network","aab9b54f":"No null rows to remove. Check duplicates","124018bd":"Make predictions for submission file","cb93b366":"Substitute shop id's as noted above","fae4e04e":"We have done EDA and feature preprocessing (may do more in notebook depending on model). Save the resulting sheet for use later. We don't need to save shop category and item names as these correlate perfectly with their respective ids and we have already extracted features","e00f1052":"Sanity check. Each pair should appear exactly 35 times (one for each date block)"}}