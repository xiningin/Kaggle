{"cell_type":{"a8057a53":"code","d3c7d050":"code","3a4e4c5f":"code","ca0129a6":"code","9065e1a1":"code","4331419d":"code","4950052b":"code","9018dc01":"code","e307407f":"code","283883c1":"code","6557c57f":"code","9b6b5faa":"code","b0588640":"code","49f89080":"code","cacd902c":"code","6ae1e0f0":"code","5f615935":"code","9d9321c3":"code","de843f29":"code","b88d4ad5":"code","a5aa4fa3":"code","65c896fa":"code","7195d262":"code","0667b191":"code","f43256bd":"code","cb259f60":"code","3dfbf243":"code","10e29f20":"code","65d1ecc0":"code","1736e068":"markdown","afc84d80":"markdown","e9f65247":"markdown","85cf46e6":"markdown","e0e07796":"markdown","6c518bf8":"markdown","7cb0d74e":"markdown","9b68a435":"markdown","c206d7b7":"markdown","b9873dd0":"markdown","cb22f1ab":"markdown","bd1a7a72":"markdown"},"source":{"a8057a53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns # for heatmap plot\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n#import os\n#print(os.listdir(\"..\/input\"))\nCCData = pd.read_csv(\"..\/input\/creditcard.csv\")\nCCData.shape\n# Any results you write to the current directory are saved as output.","d3c7d050":"CCData.describe()","3a4e4c5f":"#Time column is not necessary so removing it form the data\nfrom sklearn.preprocessing import StandardScaler\n#CCData['Amount'].head()\nCCData['NormAmount'] = StandardScaler().fit_transform(CCData['Amount'].values.reshape(-1, 1))\nCCData = CCData.drop(['Time','Amount'],axis=1)\nCCData.head()","ca0129a6":"plt.figure(figsize = (50,50))        # Size of the figure\nsns.heatmap(CCData.corr(),annot = True)","9065e1a1":"# plotting a histogram to identify the frequency of each type in class\ncount_classes = pd.value_counts(CCData['Class'], sort = True).sort_index()\ncount_classes.plot(kind = 'bar')\nplt.title(\"Credit Card - Fraud Class histogram (1 represent Fraud)\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","4331419d":"# Showing ratio\nprint(\"Percentage of normal transactions: \", (len(CCData[CCData.Class == 0])\/len(CCData))*100)\nprint(\"Percentage of fraud transactions: \", (len(CCData[CCData.Class == 1])\/len(CCData))*100)\nprint(\"Total number of transactions in sampled data: \", len(CCData))","4950052b":"from sklearn.model_selection import train_test_split\n\n# Putting feature variable to X\nX = CCData.drop(['Class'],axis=1)\n# Putting response variable to y\ny = CCData['Class']\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7,test_size=0.3,random_state=100)\nprint(\"Number transactions X_train dataset: \", X_train.shape)\nprint(\"Number transactions y_train dataset: \", y_train.shape)\nprint(\"Number transactions X_test dataset: \", X_test.shape)\nprint(\"Number transactions y_test dataset: \", y_test.shape)","9018dc01":"X_train.head()","e307407f":"# if you fail to import imblearn please run following command here -!pip install imblearn\n\nfrom imblearn.over_sampling import SMOTE\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","283883c1":"X_train_res","6557c57f":"X_train_df=pd.DataFrame(X_train_res,columns=X_train.columns)\nX_train_df.head()\n","9b6b5faa":"# plotting a histogram to identify the frequency of each type in class\ncount_classes = pd.value_counts(y_train_res, sort = True).sort_index()\ncount_classes.plot(kind = 'bar')\nplt.title(\"Credit Card - Fraud Class histogram (1 represent Fraud)\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\")","b0588640":"##X_train_res, y_train_res\nX_train_df.shape","49f89080":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg, 10)             # running RFE with 10 variables as output\nrfe = rfe.fit(X_train_df,y_train_res)\nprint(rfe.support_)           # Printing the boolean results\nprint(rfe.ranking_)           # Printing the ranking","cacd902c":"col = X_train_df.columns[rfe.support_]\nprint(col)","6ae1e0f0":"UpdatedTrain_X=X_train_df[col]\nprint(UpdatedTrain_X.shape)\nUpdatedTest_X=X_test[col]\nprint(UpdatedTest_X.shape)","5f615935":"import statsmodels.api as sm\ndf_train_rfe = sm.add_constant(UpdatedTrain_X)\nlog_mod_rfe = sm.GLM(y_train_res,df_train_rfe,family = sm.families.Binomial())\nmod_res_rfe = log_mod_rfe.fit()\nlog_mod_rfe.fit().summary()","9d9321c3":"#Predicting the Test Data\nUpdatedTestCoef_X = sm.add_constant(UpdatedTest_X[col])\npredictions = mod_res_rfe.predict(UpdatedTestCoef_X)","de843f29":"Y_pred= predictions.map(lambda x: 1 if x > 0.5 else 0)\nY_pred.head()","b88d4ad5":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test,Y_pred))","a5aa4fa3":"# Let us calculate \nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, Y_pred), \"\\n\")\nprint(\"accuracy\", metrics.accuracy_score(y_test, Y_pred))\nprint(\"precision\", metrics.precision_score(y_test,Y_pred))\nprint(\"recall\", metrics.recall_score(y_test,Y_pred))\nconfusion=confusion_matrix(y_test,Y_pred)    \nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity\",TP \/ float(TP+FN))\n# positive predictive value \nprint (\"Positive Predection Rate\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"Negative Predection rate\",TN \/ float(TN+ FN))\n# Calculate false postive rate - predicting churn when customer does not have churned\nprint(\"False positive Predection Rate\",FP\/ float(TN+FP))","65c896fa":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None\n\ndraw_roc(y_test, Y_pred)","7195d262":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold\n# Create a based model\nrf = RandomForestClassifier()\n","0667b191":"rf.fit(UpdatedTrain_X, y_train_res)","f43256bd":"y_pred=rf.predict(UpdatedTest_X)\nY_pred=pd.DataFrame(y_pred)\n#Y_pred= Y_pred.map(lambda x: 1 if x > 0.5 else 0)\nY_pred.shape","cb259f60":"# Let's check the report of our default model\nprint(classification_report(y_test,Y_pred))","3dfbf243":"# Let us calculate \nfrom sklearn import metrics\nprint(metrics.confusion_matrix(y_test, Y_pred), \"\\n\")\nprint(\"accuracy\", metrics.accuracy_score(y_test, Y_pred))\nprint(\"precision\", metrics.precision_score(y_test,Y_pred))\nprint(\"recall\", metrics.recall_score(y_test,Y_pred))\nconfusion=confusion_matrix(y_test,Y_pred)    \nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity\",TP \/ float(TP+FN))\n# positive predictive value \nprint (\"Positive Predection Rate\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"Negative Predection rate\",TN \/ float(TN+ FN))\n# Calculate false postive rate - predicting churn when customer does not have churned\nprint(\"False positive Predection Rate\",FP\/ float(TN+FP))","10e29f20":"feature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = UpdatedTrain_X.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)","65d1ecc0":"feature_importances","1736e068":"### Feature Selection","afc84d80":"### From the above graph and percentage it is clear that Non Fraud data points are consierably huge when compared to Fraud datapoints. So we need to redistribute the sample in such 50-50 Fraud & non Fraud dataset.","e9f65247":"As the data has been derived from PCA, Very less Co-relation exists between features","85cf46e6":"## **SMOTE (Synthetic Minority Over-sampling Technique)**\nSMOTE is an over-sampling method. What it does is, it creates synthetic (not duplicate) samples of the minority class. Hence making the minority class equal to the majority class. SMOTE does this by selecting similar records and altering that record one column at a time by a random amount within the difference to the neighbouring records.\n#### if you fail to import imblearn please run following command here - !pip install imblearn","e0e07796":"<h2> Model Building <\/h2>","6c518bf8":"### Converting resulted numpy arrays into dataframes with header info","7cb0d74e":"Obtaining Metrics from the above model","9b68a435":"#### Before Applying SMOTE, breaking the whole dataframe into Test and Train and the apply SMOTE on Train Data","c206d7b7":"<Center><H1>Credit Card Fraud Detection <\/Center><\/H1> ","b9873dd0":"### Data Preparation\nAs the data is generated by PCA not performing any other data Preparation Steps.\n\nThe idea behind StandardScaler is that it will transform your data such that its distribution will have a mean value 0 and standard deviation of 1. Given the distribution of the data, each value in the dataset will have the sample mean value subtracted, and then divided by the standard deviation of the whole dataset.","cb22f1ab":"### As the training set contains 29 Features in it , we will perform Feature Selection using RFE to pick top 10 Features that will assist in bulding the model","bd1a7a72":"## Model -2 : Random Forest"}}