{"cell_type":{"0f7bd762":"code","3701925d":"code","6314f5d0":"code","6204b77e":"code","d6130f60":"code","a8b307d5":"code","ab82f7c4":"code","e130da51":"code","97bb6db8":"code","9415daef":"code","8367c794":"code","97a150e7":"markdown","17328a57":"markdown","dad31847":"markdown","5b3bf201":"markdown","4ec40e45":"markdown","51579f78":"markdown","4c9acc7b":"markdown","90c49e00":"markdown","ce636e09":"markdown","6ab62582":"markdown"},"source":{"0f7bd762":"# =============================================\n# REPRODUCIBLE CODE \n# Note: t-SNE is meant only visualisation 2D\/3D\n# =============================================\nfrom sklearn.manifold import TSNE\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\n\n\ndef get_tsne_df(X, labels, to_dims=2, perplexity_iter_combns=[(30,100)]):\n    \"\"\"\n    - `perplexity_iter_combns` is not `None` to visualize only. Doesn't return df\n        + [(p1, iters1), (p2, iters2), ...]\n    \"\"\"\n    # for visualaisation\n    legend = [str(i) for i in np.unique(labels)]\n    colors = {0 :'red', 1 :'blue', 2 :'green', 3 :'black', 4 :'orange', 5 :'yellow',\n              6 :'pink', 7 :'brown', 8 : 'purple', 9 : 'grey' }\n    num_combns = None\n    rows, cols, = None, None\n    fig, axarr, = None, None\n    if (len(perplexity_iter_combns) > 1):\n        num_combns = len(perplexity_iter_combns)\n        cols = 4\n        rows = math.ceil(num_combns\/cols)\n        # create `axarr` w\/ len=cols*rows\n        fig, axarr = plt.subplots(rows,cols)\n        axarr = axarr.flatten()\n        fig.set_size_inches(5*cols, 5*rows)\n    \n    for idx, (perplexity_val, n_iter_val) in enumerate(perplexity_iter_combns):\n        model = TSNE(\n            n_components  = to_dims, \n            random_state  = 0,\n            perplexity    = perplexity_val,\n            n_iter        = n_iter_val\n        )\n        # configuring the parameteres\n        # the number of components = to_dims\n        # default perplexity = 30\n        # default learning rate = 200\n        # default n_iter = 1000\n\n        tsne_data = model.fit_transform(X)\n\n        # return a dataframe which help us in ploting the result data\n        # (only if single perplexity_iter_combn is given)\n        tsne_data = np.vstack((tsne_data.T, labels)).T\n        tsne_df = pd.DataFrame(data=tsne_data, columns=(\"Dim_1\", \"Dim_2\", \"label\"))\n        if (len(perplexity_iter_combns) == 1):\n            return tsne_df\n        else:\n            # plot sublots for all `perplexity_iter_combns`\n            axarr[idx].scatter(tsne_df['Dim_1'], tsne_df['Dim_2'], c=tsne_df['label'].apply(lambda x: colors[int(x)]), alpha=0.5)\n            axarr[idx].title.set_text(f\"perplexity: {perplexity_val}\\niterations: {n_iter_val}\")\n            axarr[idx].set_xlabel(\"Dim 1\")\n            axarr[idx].set_ylabel(\"Dim 2\")\n            axarr[idx].legend(legend)\n            print(f\"combination {idx+1}of{num_combns} >> perplexity: {perplexity_val} iterations: {n_iter_val} done ...\")\n    plt.show()\n    \n    \n_ = \"\"\"#Plotting \nsns.FacetGrid(df, hue=\"label\", size=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\nplt.show()\n\"\"\"","3701925d":"from sklearn.datasets import load_digits\ndigits = load_digits()\nX_mnist = digits.data\ny_mnist = digits.target","6314f5d0":"df = get_tsne_df(\n    X_mnist, y_mnist, \n    to_dims = 2,\n    perplexity_iter_combns = [(30,1000)]\n)\n\ndf.head()","6204b77e":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.FacetGrid(df, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\n\nplt.title(f\"MNIST Dataset\\nm={X_mnist.shape[1]} n={X_mnist.shape[0]}\")\nplt.show()","d6130f60":"# plot sublots (simply give needed combinations)\n# or use zip()\nperplexity_iter_combns = [\n    # (perplexity_i, iter_i)\n    (1,251),\n    (3,251),\n    (6,251),\n    (10,251),\n    (20,300),\n    (30,500),\n    (100,1000),\n    (len(X_mnist)-1, 1000)\n]\n\ndf = get_tsne_df(\n    X_mnist, y_mnist, \n    to_dims = 2,\n    perplexity_iter_combns = perplexity_iter_combns\n)","a8b307d5":"from sklearn.datasets import load_breast_cancer\nX_bc, y_bc = load_breast_cancer(return_X_y=True)","ab82f7c4":"df_bc = get_tsne_df(\n    X_bc, y_bc, \n    to_dims = 2,\n    perplexity_iter_combns = [(30,1000)]\n)","e130da51":"sns.FacetGrid(df_bc, hue=\"label\", height=6).map(plt.scatter, 'Dim_1', 'Dim_2').add_legend()\n\nplt.title(f\"Boston Dataset\\nm={X_bc.shape[1]} n={X_bc.shape[0]}\")\nplt.show()","97bb6db8":"# plot sublots (simply give needed combinations)\n# or use zip()\nperplexity_iter_combns = [\n    # (perplexity_i, iter_i)\n    (1,251),\n    (3,251),\n    (6,251),\n    (10,251),\n    (20,300),\n    (30,500),\n    (100,1000),\n    (len(X_bc)-1, 1000)\n]\n\ndf = get_tsne_df(\n    X_bc, y_bc, \n    to_dims = 2,\n    perplexity_iter_combns = perplexity_iter_combns\n)","9415daef":"PERPLEXITY = 100\niters = np.arange(250, 1500,50)\nperplexities = np.array([PERPLEXITY]*len(iters))\n\ngenerator = zip(perplexities, iters)\n\nperplexity_iter_combns = []\nfor ppxty,it in generator:\n    perplexity_iter_combns.append((ppxty, it))\n    \nperplexity_iter_combns","8367c794":"df = get_tsne_df(\n    X_bc, y_bc, \n    to_dims = 2,\n    perplexity_iter_combns = perplexity_iter_combns\n)","97a150e7":"![image.png](attachment:image.png)","17328a57":"> Perplexity 100 seems to be good. Let us see all its **stable** clusters by checking all iterations","dad31847":"### MNIST Dataset","5b3bf201":"### Boston House Dataset","4ec40e45":"## Hyperparameters\n\n- **PERPLEXITY:** Can be loosly stated as ***number-of-expected-neighbours for a single datapoint*** i.e (expected cluster size)<br>\n- **ITERATIONS:** Increase until shape **stabalizes**\n\n# **Interpretation** \n\n1. **Preserves** a cluster's neighbourhood distances (based on perplexity)\n    - Even random data shows patterns **@lower perplexity values**\n    - Make sure **`2 <= perplexity < num_samples`** (each point will become a cluster \/ whole data will become a cluster)\n2.  **Meaningless** quantities\n    - **Distance b\/w clusters** (Just makes sure clusters are separated iteratively.)\n    - **Cluster size** (because internally [divided by sum](https:\/\/www.kaggle.com\/l0new0lf\/02-08-normalisation-vs-standardisation-vs-probs) - expands dense clusters and and shriks sparse ones)\n4. For *Topology\/exact-pattern* run **multiple times w\/ dfft. values**\n\n**NOTE**\n\n   - Never run t-SNE once\n   - Run w\/ different hyperparameters (trust the shape that occurs most times)\n   - **Accept a perplexity value** only after checking almost all possible **stable** clusters(in iterations) for a given perplexity\n   \nSource: [distill.pub misread tsne](https:\/\/distill.pub\/2016\/misread-tsne\/) (Live examples)\n\n<br>\n<br>\n<br>\n\n## **Crowding Problem** The need for T-Distribution\n\n> [Occurs esp. w\/ Hypercubes] *t-SNE has to definitely make **mistake**!*\n\nFor example, Consider reducing square-shape-arranged datapoints in 2-D when to 1-D. We **have to make mistake** w\/ 1 of 4 points.\n\n**Solution:** <br>\nWe use student's T-Distribution (in the new dimension space) to make sure **we do not make mistake as much as possible**\n> Key is, if **distribution changes slowly**, *we have more \"wiggle room\"* to place points at. [[source]](https:\/\/www.cs.toronto.edu\/~jlucas\/teaching\/csc411\/lectures\/lec13_handout.pdf)\n>\n> Section 3.2 and figure 1 of [original paper(2008)](https:\/\/lvdmaaten.github.io\/publications\/papers\/JMLR_2008.pdf) explains solution","51579f78":"**Observation**\n> *perplexiy `100` is extremely stable and forms strong clusters*","4c9acc7b":"**Look for stable clusters for a given perplexity**","90c49e00":"# **Implementation**","ce636e09":"# **t-SNE**\n## t-Distributed Stochastic Neighbor Embedding\n\n> # *Meant only for Visualisation!!*","6ab62582":"This notebook concetrates more on **interpretation and implementation** of the algorithm. <br>Know mathematical details in this [video](https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM)\n\n> PCA it is a mathematical technique, but t-SNE is a probabilistic one<br>\n> It is an iterative algorithm based on probability. (May produce slightly diferent results every time!)\n\n\n![image.png](attachment:image.png)"}}