{"cell_type":{"14596782":"code","2db59c52":"code","5602937e":"code","99b73346":"code","1e13e028":"code","ea5d7cea":"code","3605b2a2":"code","fe3c840e":"code","9bc506f0":"code","f512ce24":"code","4f5306de":"code","82250415":"code","e1b5f838":"code","98c98604":"code","b4dfeb78":"code","f5c76548":"code","bcfa3a89":"code","4f4143ed":"code","749ec1a3":"code","ada6b42f":"code","1ffecf3c":"code","e84f6803":"code","5b6d0b58":"code","ba2a6ca4":"code","004b0f00":"markdown","74c0ae9b":"markdown","28efc18a":"markdown","8420aecd":"markdown","1dd5fca4":"markdown","ed0544b4":"markdown","4fa2c3eb":"markdown","9131dcf3":"markdown","f248b085":"markdown","2a8554fe":"markdown","051d3bfe":"markdown","71af0fb3":"markdown","7d112e61":"markdown","0288c5dc":"markdown"},"source":{"14596782":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2db59c52":"import networkx as nx\nG=nx.Graph()\nG.add_edge(1,2)\n","5602937e":"G.add_edge(2,3,weight=0.9)\nnx.draw(G)","99b73346":"import networkx as nx\n!python3 -m pip install --upgrade pip \n!pip3 install mlrose\nimport six\nimport sys\nsys.modules['sklearn.externals.six'] = six\nimport mlrose\nimport numpy as np\ndist_list =[(0,1,35),(0,2,30),(0,3,27),(0,4,34),(0,5,43),(0,6,32),(0,7,70),\n            (1,2,23), (1,3,13),(1,4,53),(1,5,32),(1,6,34),(1,7,50),\n            (2,3,42),(2,4,32),(2,5,23),(2,6,23),(2,7,34),\n            (3,4,23),(3,5,32),(3,6,31),(3,7,42),\n            (4,5,63),(4,6,43),(4,7,52),\n            (5,6,54),(5,7,43),\n            (6,7,40)]\n\n# Initialize fitness function object using dist_list\nfitness_dists = mlrose.TravellingSales(distances = dist_list)\nproblem_fit = mlrose.TSPOpt(length = 8, fitness_fn = fitness_dists,\n                            maximize=False)\nbest_state, best_fitness = mlrose.genetic_alg(problem_fit, random_state =15)\n\nprint(best_state)\n \n\nprint(best_fitness)","1e13e028":"import networkx as nx\ndist_list =[(0,1,35),(0,2,30),(0,3,27),(0,4,34),(0,5,43),(0,6,32),(0,7,70),\n            (1,2,23),(1,3,13),(1,4,53),(1,5,32),(1,6,34),(1,7,50),\n            (2,3,42),(2,4,32),(2,5,23),(2,6,23),(2,7,34),\n            (3,4,23),(3,5,32),(3,6,31),(3,7,42),\n            (4,5,63),(4,6,43),(4,7,52),\n            (5,6,54),(5,7,43),\n            (6,7,40)]\nG=nx.Graph()\nG.add_weighted_edges_from(dist_list)\nedges = [(0,6),(6,7), (7,5), (5,2),(2,1), (1,3), (3,4)]\n\npos = nx.spring_layout(G)\nnx.draw(G, with_labels = True, pos=pos)\n\nnx.draw_networkx_edges(G, pos=pos, edgelist=edges, edge_color=\"r\", width=2)","ea5d7cea":"for node in G.nodes():\n    neighbours=[n for n in nx.neighbors(G,node)]\n    n_neighbors=len(neighbours)\n    n_links=0\n    if n_neighbors>1:\n        for node1 in neighbours:\n            for node2 in neighbours:\n                if G.has_edge(node1,node2):\n                    n_links+=1\n        n_links#\/=2 #because n_links is calculated twice\n        branch_coefficient=n_links #\/(0.5*n_neighbors*(n_neighbors-1))\n        print(clustering_coefficient)\n    else:\n        print(0)","3605b2a2":"for node in G.nodes():\n    neighbours=[n for n in nx.neighbors(G,node)]\n    n_neighbors=len(neighbours)\n    n_links=0\n    if n_neighbors>1:\n        for node1 in neighbours:\n            for node2 in neighbours:\n                if G.has_edge(node1,node2):\n                    n_links+=1\n        n_links\/=2 #because n_links is calculated twice\n        clustering_coefficient=n_links\/(0.5*n_neighbors*(n_neighbors-1))\n        print(clustering_coefficient)\n    else:\n        print(0)","fe3c840e":"nx.average_clustering (G)","9bc506f0":"nx.density(G)","f512ce24":"nx.transitivity(G)","4f5306de":"for node in G.nodes():\n    neighbours=[n for n in nx.neighbors(G,node)]\n    n_neighbors=len(neighbours)\n    n_links=0\n    if n_neighbors>1:\n        for node1 in neighbours:\n            for node2 in neighbours:\n                v = G.number_of_edges() \/ float(G.number_of_nodes())\n        print(v)\n    else:\n        print(0)","82250415":"k=5 \n\nshs=nx.shortest_simple_paths(G, 0,7, weight='weight') \n\nsh_paths=[] \n\nfor i, sh in enumerate(shs): \n\n    sh_paths.append(sh) \n\n    print(sh) \n\n    if i == (k-1): \n\n        break \n\n#K-SHORTEST PATH ALGORITHM (YEN) WITH LENGTHS \n\nlengths=[] \n\n  \n\nfor sh in sh_paths: \n\n    l=len(sh) \n\n    lng=0 \n\n    for i in range(l-1): \n\n        try: \n\n            one_lng=nx.get_edge_attributes(G,'weight')[(sh[i], sh[(i+1)])] \n\n        except: \n\n            one_lng=nx.get_edge_attributes(G,'weight')[(sh[(i+1)], sh[i])] \n\n        lng=lng+one_lng \n\n    lengths.append(lng) \n\n     \n\nfor i in range(len(sh_paths)): \n\n    print(sh_paths[i],'->',lengths[i]) ","e1b5f838":"import pandas as pd \n\npaths_raw=nx.all_pairs_dijkstra_path(G) \n\nlngths_raw=nx.all_pairs_dijkstra_path_length(G) \n\npaths=[] \n\nlngths=[] \n\nnodes=[] \n\n  \n\nfor i, pth in enumerate(paths_raw): \n\n    paths.append(pth) \n\n    nodes.append(pth[0])   \n\n     \n\nfor i, lng in enumerate(lngths_raw): \n\n    lngths.append(lng)  \n\n     \n\npaths_nice=pd.DataFrame(index=nodes, columns=nodes) \n\n  \n\nfor i in nodes: \n\n    for j in nodes: \n\n         \n\n        for k in paths: \n\n            if k[0]==i: \n\n                paths_nice.at[i,j]=str(k[1][j]) \n\n                 \n\n        for k in lngths: \n\n            if k[0]==i: \n\n                paths_nice.at[i,j]=paths_nice.at[i,j]+'  '+'distance:'+str(k[1][j])                 \n\n  \n\npaths_nice   ","98c98604":"min_sp_tree=nx.minimum_spanning_tree(G) \n\nedge_labels = dict(((u, v), d['weight']) for u, v, d in min_sp_tree.edges(data=True)) \n\npos=nx.spring_layout(min_sp_tree) \n\nnx.draw(min_sp_tree,pos=pos, with_labels=True) \n\nnx.draw_networkx_edge_labels(min_sp_tree, pos=pos, edge_labels=edge_labels) ","b4dfeb78":"dg=dict(G.degree()) \n\ns=sum(dg.values()) \n\nfor n in nodes: \n\n    dg[n]=round(dg[n]\/s,3) \n\ndg \n\nneighbours=dict() \n\nfor n in nodes: \n\n    nb=[] \n\n    for j in G.neighbors(n): \n\n        nb.append(j) \n\n    neighbours.update({n:nb}) \n\nneighbours \n\nprobs_nice=pd.DataFrame(index=nodes, columns=nodes) \n\nprobs_nice=probs_nice.fillna(0) \n\n  \n\nfor n in nodes: \n\n    probs_nice[n]=probs_nice[n].astype('float64') \n\n     \n\nfor n in nodes: \n\n    nbs=neighbours[n] \n\n    for ind in nbs: \n\n        probs_nice.at[n,ind]=dg[ind] \n\n  \n\nprobs_nice \n\ndict_step=dict() \n\nfor i, n in enumerate(nodes): \n\n    dict_step.update({i:n}) \n\n     \n\ndict_step \n\nimport numpy as np \n\nstart=0\n\npoint=start \n\nnum_steps=5 \n\npos=[i for i in range(8)] \n\nfull_path=[] \n\nfull_path.append(start) \n\nlength=0 \n\n  \n\nfor i in range(0, num_steps): \n\n    probs=list(dict(probs_nice.loc[point,:]).values()) \n\n    s=sum(probs) \n\n    for i in range(len(probs)): \n\n        probs[i]=probs[i]\/s \n\n    choice=np.random.choice(pos, p=probs) \n\n    full_path.append(dict_step[choice]) \n\n    try: \n\n        lng0=nx.get_edge_attributes(G,'weight')[(point, dict_step[choice])] \n\n    except: \n\n        lng0=nx.get_edge_attributes(G,'weight')[(dict_step[choice], point)] \n\n    length=length+lng0 \n\n    point=dict_step[choice] \n\n     \n\nprint('full path -> ',full_path) \n\nprint('length -> ',length)","f5c76548":"dct=nx.harmonic_centrality(G) \n\ndctk=list(dct.keys()) \n\ndctv=list(dct.values()) \n\nsdf=pd.DataFrame(dctk) \n\nsdf[1]=dctv \n\nsdf[1]=sdf[1].round(3) \n\nsdf[0]=sdf[0].astype('int64') \n\nsdf.columns=['town','centrality'] \n\nsdf=sdf.set_index('town') \n\nsdf=sdf.sort_values('centrality', ascending=False) \n\nsdf.head(10) ","bcfa3a89":"dct=nx.betweenness_centrality(G, weight='weight') \n\ndctk=list(dct.keys()) \n\ndctv=list(dct.values()) \n\nsdf=pd.DataFrame(dctk) \n\nsdf[1]=dctv \n\nsdf[1]=sdf[1].round(3) \n\nsdf[0]=sdf[0].astype('int64') \n\nsdf.columns=['town','centrality'] \n\nsdf=sdf.set_index('town') \n\nsdf=sdf.sort_values('centrality', ascending=False) \n\nsdf.head(5) ","4f4143ed":"dct=nx.closeness_centrality(G, wf_improved=False) \n\ndctk=list(dct.keys()) \n\ndctv=list(dct.values()) \n\nsdf=pd.DataFrame(dctk) \n\nsdf[1]=dctv \n\nsdf[1]=sdf[1].round(3) \n\nsdf[0]=sdf[0].astype('int64') \n\nsdf.columns=['town','centrality'] \n\nsdf=sdf.set_index('town') \n\nsdf=sdf.sort_values('centrality', ascending=False) \n\nsdf.tail(10) ","749ec1a3":"dct=nx.pagerank(G) \n\ndctk=list(dct.keys()) \n\ndctv=list(dct.values()) \n\nsdf=pd.DataFrame(dctk) \n\nsdf[1]=dctv \n\nsdf[1]=sdf[1].round(3) \n\nsdf[0]=sdf[0].astype('int64') \n\nsdf.columns=['town','pagerank_importance'] \n\nsdf=sdf.set_index('town') \n\nsdf=sdf.sort_values('pagerank_importance', ascending=False) \n\nsdf.head(5) ","ada6b42f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1ffecf3c":"import sklearn\nfrom tensorflow import keras\nimport nltk\nimport pandas as pd\nimport numpy as np\nimport re\nimport codecs","e84f6803":"df=pd.read_csv(\"\/kaggle\/input\/mall-customers\/Mall_Customers.csv\")","5b6d0b58":"df1=pd.read_csv(\"\/kaggle\/input\/mall-customers\/Mall_Customers.csv\")\n\ndf2=pd.read_csv(\"\/kaggle\/input\/mall-customers\/Mall_Customers.csv\")\n\ndf2['weight']=df2['Annual Income (k$)'] \n\ndf2=df2.loc[0:19,] \n\ndf2.head() \n\nindicator=[] \n\ncards=list(set(df2['CustomerID'].values))\n\ncards_t=df1['CustomerID'].values.tolist() \n\nfor i in range(len(cards_t)): \n\n    a=cards_t[i] in cards \n\n    indicator.append(a) \n\ndf1['indicator']=indicator \n\n  \n\ndf1=df1[df1['indicator']==True] \n\nind=[i for i in range(df1.shape[0])] \n\ndf1.index=ind \n\ndf1=df1.drop(['indicator'], axis=1) \n\ndf1.head() \n\ndf=df2[['CustomerID','CustomerID','weight']] \n\ndf.head()  ","ba2a6ca4":"G=nx.Graph() \n\nG.add_weighted_edges_from(df1.values) \n\ndf=df1.set_index(['CustomerID']) \n\ndict_customer_age=dict(df1['Age']) \n\ndict_gender=dict(df1['Genre']) \n \ndict_total_trans_cnt=dict(df1['Annual Income (k$)'])   \n\nlk=list(dict_customer_age.keys()) \n\nfor l in lk: \n\n    dict_customer_age[l]={'Age':dict_customer_age[l]} \n\n    dict_gender[l]={'Genre':dict_gender[l]} \n\n    dict_total_trans_cnt[l]={'Annual Income (k$)':dict_total_trans_cnt[l]} \n\nnx.set_node_attributes(G, dict_customer_age) \n\nnx.set_node_attributes(G, dict_gender) \n\nnx.set_node_attributes(G, dict_total_trans_cnt) \n\ndict_transaction_amount=dict(df2.set_index(['CustomerID'])['Annual Income (k$)']) \n\nlk=list(dict_transaction_amount.keys()) \n\nfor l in lk: \n\n    dict_transaction_amount[l]={'Annual Income (k$)':dict_transaction_amount[l]} \n\nnx.set_edge_attributes(G, dict_transaction_amount)","004b0f00":"\u0446\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c \" \u0433\u0430\u0440\u043c.\"","74c0ae9b":"\u0446\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c \" \u0431\u043b\u0438\u0437\u043e\u0441\u0442\u044c\"","28efc18a":"\u0446\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c \"\u043f\u043e\u0441\u0440.\"","8420aecd":"APSP","1dd5fca4":"PageRank","ed0544b4":"\u0421\u043e\u0446\u0438\u0430\u043b\u044c\u043d\u044b\u0439 \u0433\u0440\u0430\u0444","4fa2c3eb":"\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0442\u0440\u0430\u043d\u0437\u0438\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u0438","9131dcf3":"\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u0438","f248b085":"\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u043f\u043b\u043e\u0442\u043d\u043e\u0441\u0442\u0438","2a8554fe":"MSTre\n","051d3bfe":"\u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u0432\u0435\u0442\u0432\u043b\u0435\u043d\u0438\u044f","71af0fb3":" \u0421-\u0431\u043b\u0443\u0436\u0434\u0430\u043d\u0438\u0435","7d112e61":"\u041f\u0443\u0442\u044c \u0419\u0435\u043d\u0430","0288c5dc":"\u0421\u0440\u0435\u0434\u043d\u0438\u0439 \u043a\u0440\u0430\u0442\u0447\u0430\u0439\u0448\u0438\u0439 \u043f\u0443\u0442\u044c \u0447\u0435\u0440\u0435\u0437 \u0433\u0430\u0440\u043c\u043e\u043d\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0441\u0440\u0435\u0434\u043d\u0435\u0435"}}