{"cell_type":{"b4d7fda1":"code","a706a903":"code","753866de":"code","34e7d149":"code","23510212":"code","a1fd2167":"code","1569eceb":"code","9cfdef35":"code","075d1aae":"code","b727a5cd":"code","3f4ad88a":"code","f3ba117c":"code","3a5fb5c2":"code","3aad5faf":"code","7c08eb44":"code","b8c93453":"code","324f7a9e":"code","a9b18451":"code","aaf1fab4":"code","c31fb39b":"code","87778a68":"code","ccd9fa8b":"code","5ff1abe6":"code","b657128b":"code","d0b1c52d":"code","5c36c6a2":"code","9ff957f4":"markdown","da871f8a":"markdown","d74e7542":"markdown","dc4504cc":"markdown","467820d6":"markdown","17675d2c":"markdown","367c9b5c":"markdown","0b87890e":"markdown","2172819c":"markdown","efe5ef94":"markdown","14b9173e":"markdown","156964ea":"markdown","6a0b47ee":"markdown","cd377541":"markdown","9860f5bc":"markdown","b0ef3caf":"markdown","057ee8b5":"markdown","c9f29b8b":"markdown","9697682c":"markdown","62944973":"markdown","1e3f9015":"markdown","d5b6ff0a":"markdown","626ef669":"markdown","36c47e64":"markdown","e2d6a8a5":"markdown","518f54c6":"markdown","3026e165":"markdown","1fd0a299":"markdown","139f61ae":"markdown","0b7c650e":"markdown","4cd2ba5b":"markdown","d524dbc2":"markdown","03c99f1f":"markdown","3517f4e3":"markdown","b508b7be":"markdown","b30ac102":"markdown","739e3041":"markdown","f47e0cc0":"markdown","765300f4":"markdown","8a9d3a96":"markdown","45aa05c3":"markdown","3e4345a4":"markdown","275ae0de":"markdown","c8b4cca7":"markdown","eb0c8ee0":"markdown","7e497cd4":"markdown","43d56dd8":"markdown","6fe9c4f5":"markdown","0a1d360c":"markdown","aa685df0":"markdown","81a02259":"markdown","cd16861e":"markdown","75d4121b":"markdown","1a071e96":"markdown","70ae86dd":"markdown","8a936962":"markdown","0bf71453":"markdown","72ce462c":"markdown","9b253f7a":"markdown","038fb40a":"markdown","a36ebde6":"markdown","2578fc48":"markdown","8cbaed94":"markdown","525d84ef":"markdown","38cae8f6":"markdown","09e244e3":"markdown","0ac7fe68":"markdown","42d7a269":"markdown","9d79ad8d":"markdown","85221468":"markdown","c4463096":"markdown","1e5d5d79":"markdown","86ee1c09":"markdown","32dd49c6":"markdown","d506c158":"markdown","a6e73913":"markdown","a7379fae":"markdown","5525e477":"markdown","3b2acd4d":"markdown","afcd84ce":"markdown","39081205":"markdown","8e3d5eb2":"markdown","aaa526f6":"markdown","e6e47418":"markdown","19141d9e":"markdown","687d3920":"markdown","4a901833":"markdown","55f30587":"markdown","140fabb3":"markdown","6670524d":"markdown","a68b6d58":"markdown","2e7784a4":"markdown","04a09716":"markdown","96ab5dd7":"markdown","08904e1e":"markdown","e657b3cf":"markdown","b48e8db4":"markdown","7cc2fc55":"markdown","2bd50acb":"markdown","40b81c15":"markdown"},"source":{"b4d7fda1":"%matplotlib inline\nimport numpy as np\nimport torch\ntorch.set_printoptions(edgeitems=2, linewidth=75)","a706a903":"t_c = [0.5,  14.0, 15.0, 28.0, 11.0,  8.0,  3.0, -4.0,  6.0, 13.0, 21.0]\nt_u = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4]\nt_c = torch.tensor(t_c)\nt_u = torch.tensor(t_u)","753866de":"t_c, t_u","34e7d149":"def model(t_u, w, b):\n    return w * t_u + b","23510212":"def loss_fn(t_p, t_c):\n    squared_diffs = (t_p - t_c)**2\n    return squared_diffs.mean()","a1fd2167":"w = torch.ones(())\nb = torch.zeros(())\n\nt_p = model(t_u, w, b)\nt_p","1569eceb":"loss = loss_fn(t_p, t_c)\nloss","9cfdef35":"x = torch.ones(())\ny = torch.ones(3,1)\nz = torch.ones(1,3)\na = torch.ones(2, 1, 1)\nprint(f\"shapes: x:         {x.shape},        y: {y.shape}\")\nprint(f\"        z:         {z.shape},    a: {a.shape}\")\nprint(f\"        x * y:     {(x * y).shape}\")\nprint(f\"        y * z:     {(y * z).shape}\")\nprint(f\"        y * z * a: {(y * z * a).shape}\")","075d1aae":"delta = 0.1\n\nloss_rate_of_change_w = \\\n    (loss_fn(model(t_u, w + delta, b), t_c) - \n     loss_fn(model(t_u, w - delta, b), t_c)) \/ (2.0 * delta)","b727a5cd":"learning_rate = 1e-2\n\nw = w - learning_rate * loss_rate_of_change_w","3f4ad88a":"loss_rate_of_change_b = \\\n    (loss_fn(model(t_u, w, b + delta), t_c) - \n     loss_fn(model(t_u, w, b - delta), t_c)) \/ (2.0 * delta)\n\nb = b - learning_rate * loss_rate_of_change_b","f3ba117c":"def loss_fn(t_p, t_c):\n    squared_diffs = (t_p - t_c)**2\n    return squared_diffs.mean()","3a5fb5c2":"def dloss_fn(t_p, t_c):\n    dsq_diffs = 2 * (t_p - t_c) \/ t_p.size(0)  # <1> The division is from the derivative of mean.\n    return dsq_diffs","3aad5faf":"def model(t_u, w, b):\n    return w * t_u + b","7c08eb44":"def dmodel_dw(t_u, w, b):\n    return t_u","b8c93453":"def dmodel_db(t_u, w, b):\n    return 1.0","324f7a9e":"def grad_fn(t_u, t_c, t_p, w, b):\n    dloss_dtp = dloss_fn(t_p, t_c)\n    dloss_dw = dloss_dtp * dmodel_dw(t_u, w, b)\n    dloss_db = dloss_dtp * dmodel_db(t_u, w, b)\n    return torch.stack([dloss_dw.sum(), dloss_db.sum()])\n\n# The summation is the reverse of the broadcasting we implicitly do when\n# applying the parameters to an entire vector of inputs in the model.","a9b18451":"def training_loop(n_epochs, learning_rate, params, t_u, t_c):\n    for epoch in range(1, n_epochs + 1):\n        w, b = params\n        t_p = model(t_u, w, b)  # Forward pass\n        loss = loss_fn(t_p, t_c)\n        grad = grad_fn(t_u, t_c, t_p, w, b)  # Backward pass\n        params = params - learning_rate * grad\n        print('Epoch %d, Loss %f' % (epoch, float(loss)))  # This logging line can be very verbose\n        return params","aaf1fab4":"def training_loop(n_epochs, learning_rate, params, t_u, t_c,\n                  print_params=True):\n    for epoch in range(1, n_epochs + 1):\n        w, b = params\n\n        t_p = model(t_u, w, b)  # <1>\n        loss = loss_fn(t_p, t_c)\n        grad = grad_fn(t_u, t_c, t_p, w, b)  # <2>\n\n        params = params - learning_rate * grad\n\n        if epoch in {1, 2, 3, 10, 11, 99, 100, 4000, 5000}:  # <3>\n            print('Epoch %d, Loss %f' % (epoch, float(loss)))\n            if print_params:\n                print('    Params:', params)\n                print('    Grad:  ', grad)\n        if epoch in {4, 12, 101}:\n            print('...')\n\n        if not torch.isfinite(loss).all():\n            break  # <3>\n            \n    return params","c31fb39b":"training_loop(\n    n_epochs = 100,\n    learning_rate = 1e-2,\n    params = torch.tensor([1.0, 0.0]),\n    t_u = t_u,\n    t_c = t_c)","87778a68":"training_loop(\n    n_epochs = 100, \n    learning_rate = 1e-4, \n    params = torch.tensor([1.0, 0.0]), \n    t_u = t_u, \n    t_c = t_c)","ccd9fa8b":"t_un = 0.1 * t_u","5ff1abe6":"training_loop(\n    n_epochs = 100, \n    learning_rate = 1e-2, \n    params = torch.tensor([1.0, 0.0]), \n    t_u = t_un,  # We\u2019ve updated t_u to our new, rescaled t_un.\n    t_c = t_c)","b657128b":"params = training_loop(\n    n_epochs = 5000, \n    learning_rate = 1e-2, \n    params = torch.tensor([1.0, 0.0]), \n    t_u = t_un, \n    t_c = t_c,\n    print_params = False)\n\nparams","d0b1c52d":"%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nt_p = model(t_un, *params)  # Remember that we\u2019re training on the normalized unknown units.\n                            # We also use argument unpacking.\n\nfig = plt.figure(dpi=600)\nplt.xlabel(\"Temperature (\u00b0Fahrenheit)\")\nplt.ylabel(\"Temperature (\u00b0Celsius)\")\nplt.plot(t_u.numpy(), t_p.detach().numpy())  # But we\u2019re plotting the raw unknown values.\nplt.plot(t_u.numpy(), t_c.numpy(), 'o')\nplt.savefig(\"temp_unknown_plot.png\", format=\"png\")","5c36c6a2":"%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nfig = plt.figure(dpi=600)\nplt.xlabel(\"Measurement\")\nplt.ylabel(\"Temperature (\u00b0Celsius)\")\nplt.plot(t_u.numpy(), t_c.numpy(), 'o')\n\nplt.savefig(\"temp_data_plot.png\", format=\"png\")","9ff957f4":"*d loss_fn \/ d w = (d loss_fn \/ d t_p) * (d t_p \/ d w)*","da871f8a":"This sounds complicated (and it can be error-prone if we don\u2019t pay close attention, which is why we have named the tensor dimensions as shown in section 3.4), but usually, we can either write down the tensor dimensions to see what happens or picture what happens by using space dimensions to show the broadcasting, as in the following figure.","d74e7542":"We\u2019ve figured out the model and the loss function\u2014we\u2019ve already got a good part of the high-level picture in figure 5.2 figured out. Now we need to set the learning process in motion and feed it actual data. Also, enough with math notation; let\u2019s switch to PyTorch\u2014after all, we came here for the fun.","dc4504cc":"This is saying that in the neighborhood of the current values of w and b , a unit increase in w leads to some change in the loss. If the change is negative, then we need to increase w to minimize the loss, whereas if the change is positive, we need to decrease w . By how much? Applying a change to w that is proportional to the rate of change of the loss is a good idea, especially when the loss has several parameters: we apply a change to those that exert a significant change on the loss. It is also wise to change the parameters slowly in general, because the rate of change could be dramatically different at a distance from the neighborhood of the current w value. Therefore, we typically should scale the rate of change by a small factor. This scaling factor has many names; the one we use in machine learning is learning_rate :","467820d6":"We\u2019ll start by making a note of temperature data in good old Celsius 5 and measurements from our new thermometer, and figure things out. After a couple of weeks, here\u2019s the data (code\/p1ch5\/1_parameter_estimation.ipynb):","17675d2c":"Both of the example loss functions have a clear minimum at zero and grow monotonically as the predicted value moves further from the true value in either direction. Because the steepness of the growth also monotonically increases away from the minimum, both of them are said to be convex. Since our model is linear, the loss as a function of w and b is also convex. Cases where the loss is a convex function of the model parameters are usually great to deal with because we can find a minimum very efficientlythrough specialized algorithms. However, we will instead use less powerful but more generally applicable methods in this chapter. We do so because for the deep neural networks we are ultimately interested in, the loss is not a convex function of the inputs.\n\nContrast that with the function shown in figure 5.6, which is not convex.","367c9b5c":"We\u2019re now going to take a problem with a noisy dataset, build a model, and implement a learning algorithm for it. When we start, we\u2019ll be doing everything by hand, but by the end of the chapter we\u2019ll be letting PyTorch do all the heavy lifting for us. When we finish the chapter, we will have covered many of the essential concepts that underlie training deep neural networks, even if our motivating example is very simple and our model isn\u2019t actually a neural network (yet!).","0b87890e":"Gradient descent is not that different from the scenario we just described. The idea is to compute the rate of change of the loss with respect to each parameter, and modify each parameter in the direction of decreasing loss. Just like when we were fiddling with the knobs, we can estimate the rate of change by adding a small number to w and b and seeing how much the loss changes in that neighborhood:","2172819c":"## Visualizing (again)","efe5ef94":"### APPLYING THE DERIVATIVES TO THE MODEL","14b9173e":"This task\u2014fitting model outputs to continuous values in terms of the types discussed in chapter 4\u2014is called a regression problem. In chapter 7 and part 2, we will be concerned with classification problems.","156964ea":"OK, now we need to estimate w and b , the parameters in our model, based on the data we have. We must do it so that temperatures we obtain from running the unknown temperatures t_u through the model are close to temperatures we actually measured in Celsius. If that sounds like fitting a line through a set of measurements, well, yes, because that\u2019s exactly what we\u2019re doing. We\u2019ll go through this simple example using PyTorch and realize that training a neural network will essentially involve changing the model for a slightly more elaborate one, with a few (or a metric ton) more parameters.","6a0b47ee":"Figure 5.2 shows the high-level overview of what we\u2019ll implement by the end of the\nchapter. Given input data and the corresponding desired outputs (ground truth), as\nwell as initial values for the weights, the model is fed input data (forward pass), and a\nmeasure of the error is evaluated by comparing the resulting outputs to the ground\ntruth. In order to optimize the parameter of the model\u2014its weights\u2014the change in\nthe error following a unit change in weights (that is, the gradient of the error with\nrespect to the parameters) is computed using the chain rule for the derivative of a\ncomposite function (backward pass). The value of the weights is then updated in the\ndirection that leads to a decrease in the error. The procedure is repeated until the\nerror, evaluated on unseen data, falls below an acceptable level. If what we just said\nsounds obscure, we\u2019ve got a whole chapter to clear things up. By the time we\u2019re done,\nall the pieces will fall into place, and this paragraph will make perfect sense.","cd377541":"This represents the basic parameter-update step for gradient descent. By reiterating these evaluations (and provided we choose a small enough learning rate), we will converge to an optimal value of the parameters for which the loss computed on the given data is minimal. We\u2019ll show the complete iterative process soon, but the way we just computed our rates of change is rather crude and needs an upgrade before we move on. Let\u2019s see why and how.","9860f5bc":"In the absence of further knowledge, we assume the simplest possible model for converting between the two sets of measurements, just like Kepler might have done. The two may be linearly related\u2014that is, multiplying t_u by a factor and adding a constant, we may get the temperature in Celsius (up to an error that we omit):","b0ef3caf":"What if we could make the neighborhood infinitesimally small, as in figure 5.6? That\u2019s exactly what happens when we analytically take the derivative of the loss with respect to a parameter. In a model with two or more parameters like the one we\u2019re dealing with, we compute the individual derivatives of the loss with respect to each parameter and put them in a vector of derivatives: the gradient.","057ee8b5":"We can see that the first-epoch gradient for the weight is about 50 times larger than the gradient for the bias. This means the weight and bias live in differently scaled spaces. If this is the case, a learning rate that\u2019s large enough to meaningfully update one will be so large as to be unstable for the other; and a rate that\u2019s appropriate for the other won\u2019t be large enough to meaningfully change the first. That means we\u2019re not going to be able to update our parameters unless we change something about our formulation of the problem. We could have individual learning rates for each parameter, but for models with many parameters, this would be too much to bother with; it\u2019s babysitting of the kind we don\u2019t like.","c9f29b8b":"A quick plot of our data in figure 5.3 tells us that it\u2019s noisy, but we think there\u2019s a pattern here.","9697682c":"![image.png](attachment:image.png)","62944973":"# Learning is just parameter estimation","1e3f9015":"We can now initialize the parameters, invoke the model,","d5b6ff0a":"It\u2019s worth noting that the square difference also penalizes wildly wrong results more than the absolute difference does. Often, having more slightly wrong results is better than having a few wildly wrong ones, and the squared difference helps prioritize those as desired.","626ef669":"We need to make sure the loss function makes the loss positive both when t_p is greater than and when it is less than the true t_c , since the goal is for t_p to match t_c . We have a few choices, the most straightforward being |t_p \u2013 t_c| and (t_p \u2013 t_c)^2 . Based on the mathematical expression we choose, we can emphasize or discount certain errors. Conceptually, a loss function is a way of prioritizing which errors to fix from our training samples, so that our parameter updates result in adjustments to the outputs for the highly weighted samples instead of changes to some other samples\u2019 output that had a smaller loss.","36c47e64":"![image.png](attachment:image.png)","e2d6a8a5":"### DEFINING THE GRADIENT FUNCTION","518f54c6":"We\u2019ll optimize the loss function with respect to the parameters using the gradient descent algorithm. In this section, we\u2019ll build our intuition for how gradient descent works from first principles, which will help us a lot in the future. As we mentioned, there are ways to solve our example problem more efficiently, but those approaches aren\u2019t applicable to most deep learning tasks. Gradient descent is actually a very simple idea, and it scales up surprisingly well to large neural network models with millions of parameters.","3026e165":"We implemented the model and the loss in this section. We\u2019ve finally reached the meat of the example: how do we estimate w and b such that the loss reaches a minimum? We\u2019ll first work things out by hand and then learn how to use PyTorch\u2019s superpowers to solve the same problem in a more general, off-the-shelf way.","1fd0a299":"## Iterating to fit the model","139f61ae":"Let\u2019s start with a mental image, which we conveniently sketched out in figure 5.5. Suppose we are in front of a machine sporting two knobs, labeled w and b . We are allowed to see the value of the loss on a screen, and we are told to minimize that value. Not knowing the effect of the knobs on the loss, we start fiddling with them and decide for each knob which direction makes the loss decrease. We decide to rotate both knobs in their direction of decreasing loss. Suppose we\u2019re far from the optimal value: we\u2019d likely see the loss decrease quickly and then slow down as it gets closer to the minimum. We notice that at some point, the loss climbs back up again, so we invert the direction of rotation for one or both knobs. We also learn that when the loss changes slowly, it\u2019s a good idea to adjust the knobs more finely, to avoid reaching the point where the loss goes back up. After a while, eventually, we converge to a minimum.","0b7c650e":"![image.png](attachment:image.png)","4cd2ba5b":"Remembering that d x^2 \/ d x = 2 x , we get","d524dbc2":"Nice\u2014the behavior is now stable. But there\u2019s another problem: the updates to parameters are very small, so the loss decreases very slowly and eventually stalls. We could obviate this issue by making learning_rate adaptive: that is, change according to the magnitude of updates. There are optimization schemes that do that, and we\u2019ll see one toward the end of this chapter, in section 5.5.2.","03c99f1f":"Now, let\u2019s invoke our training loop:","3517f4e3":"Let\u2019s run the loop for enough iterations to see the changes in params get small. We\u2019ll change n_epochs to 5,000:","b508b7be":"**NOTE** Spoiler alert: we know a linear model is correct because the problem and data have been fabricated, but please bear with us. It\u2019s a useful motivating example to build our understanding of what PyTorch is doing under the\nhood.","b30ac102":"we get these derivatives:","739e3041":"### OVERTRAINING","f47e0cc0":"Is this a reasonable assumption? Probably; we\u2019ll see how well the final model performs. We chose to name w and b after weight and bias, two very common terms for linear scaling and the additive constant\u2014we\u2019ll bump into those all the time.","765300f4":"and check the value of the loss:","8a9d3a96":"The fancy name for this is hyperparameter tuning. Hyperparameter refers to the fact that we are training the model\u2019s parameters, but the hyperparameters control how this training goes. Typically these are more or less set manually. In particular, they cannot be part of the same optimization.","45aa05c3":"Putting all of this together, the function returning the gradient of the loss with respect\nto w and b is","3e4345a4":"We mentioned broadcasting in chapter 3, and we promised to look at it more carefully\nwhen we need it. In our example, we have two scalars (zero-dimensional tensors) w\nand b, and we multiply them with and add them to vectors (one-dimensional tensors)\nof length b.","275ae0de":"A loss function (or cost function) is a function that computes a single numerical value that the learning process will attempt to minimize. The calculation of loss typically involves taking the difference between the desired outputs for some training samples and the outputs actually produced by the model when fed those samples. In our case, that would be the difference between the predicted temperatures t_p output by our model and the actual measurements: t_p \u2013 t_c .","c8b4cca7":"For the model, recalling that our model is","eb0c8ee0":"Figure 5.9 The plot of our linear-fit model (solid line) versus our input data (circles)","7e497cd4":"# Down along the gradient","43d56dd8":"Here, the t_c values are temperatures in Celsius, and the t_u values are our unknown units. We can expect noise in both measurements, coming from the devices themselves and from our approximate readings. For convenience, we\u2019ve already put the data into tensors; we\u2019ll use it in a minute.","6fe9c4f5":"How can we limit the magnitude of learning_rate * grad ? Well, that looks easy. We could simply choose a smaller learning_rate , and indeed, the learning rate is one of the things we typically change when training does not go as well as we would like. We usually change learning rates by orders of magnitude, so we might try with 1e-3 or 1e-4 , which would decrease the magnitude of the updates by orders of magnitude. Let\u2019s go with 1e-4 and see how it works out:","0a1d360c":"We just got back from a trip to some obscure location, and we brought back a fancy, wall-mounted analog thermometer. It looks great, and it\u2019s a perfect fit for our living room. Its only flaw is that it doesn\u2019t show units. Not to worry, we\u2019ve got a plan: we\u2019ll build a dataset of readings and corresponding temperature values in our favorite units, choose a model, adjust its weights iteratively until a measure of the error is low enough, and finally be able to interpret the new readings in units we understand.","aa685df0":"## Gathering some data","81a02259":"## Decreasing loss","cd16861e":"We are using a Python trick called argument unpacking here: *params means to pass the elements of params as individual arguments. In Python, this is usually done with lists or tuples, but we can also use argument unpacking with PyTorch tensors, which are split along the leading dimension. So here, model(t_un, *params) is equivalent to model(t_un, params[0], params[1]) .","75d4121b":"## A hot problem","1a071e96":"Of course, this would all be theory if we didn\u2019t have some code examples:","70ae86dd":"## From problem back to PyTorch","8a936962":"Let\u2019s revisit something we did right at the start: plotting our data. Seriously, this is the first thing anyone doing data science should do. Always plot the heck out of the data:","0bf71453":"But look: the values for w and b look an awful lot like the numbers we need to use to convert Celsius to Fahrenheit (after accounting for our earlier normalization when we multiplied our inputs by 0.1). The exact values would be w=5.5556 and b=-17.7778 . Our fancy thermometer was showing temperatures in Fahrenheit the whole time. No big discovery, except that our gradient descent optimization process works!","72ce462c":"NOTE The normalization here absolutely helps get the network trained, but you could make an argument that it\u2019s not strictly needed to optimize the parameters for this particular problem. That\u2019s absolutely true! This problem is small enough that there are numerous ways to beat the parameters into submission. However, for larger, more sophisticated problems, normalization is an easy and effective (if not crucial!) tool to use to improve model convergence.","9b253f7a":"The same idea expressed in mathematical notation is shown in figure 5.7. Again, we\u2019re averaging (that is, summing and dividing by a constant) over all the data points to get a single scalar quantity for each partial derivative of the loss.","038fb40a":"t_c = w * t_u + b","a36ebde6":"All data from book **Deep Learning with PyTorch** https:\/\/pytorch.org\/deep-learning-with-pytorch\n\nhttps:\/\/github.com\/deep-learning-with-pytorch\/dlwpt-code","2578fc48":"We can do the same with b :","8cbaed94":"## Getting analytical","525d84ef":"Broadcasting, which is popular in NumPy and adapted by PyTorch, relaxes this assumption for most binary operations. It uses the following rules to match tensor elements:\n* For each index dimension, counted from the back, if one of the operands is size 1 in that dimension, PyTorch will use the single entry along this dimension with each of the entries in the other tensor along this dimension.\n* If both sizes are greater than 1, they must be the same, and natural matching is used.\n* If one of the tensors has more index dimensions than the other, the entirety of the other tensor will be used for each entry along these dimensions.","38cae8f6":"![image.png](attachment:image.png)","09e244e3":"Let\u2019s try following the same process Kepler used. Along the way, we\u2019ll use a tool he never had available: PyTorch!","0ac7fe68":"## Normalizing inputs","42d7a269":"This code produces figure 5.9. Our linear model is a good model for the data, it seems. It also seems our measurements are somewhat erratic. We should either call our optometrist for a new pair of glasses or think about returning our fancy thermometer.","9d79ad8d":"We now have everything in place to optimize our parameters. Starting from a tentative value for a parameter, we can iteratively apply updates to it for a fixed number of iterations, or until w and b stop changing. There are several stopping criteria; for now, we\u2019ll stick to a fixed number of iterations.","85221468":"Recall that our model is a linear function, and our loss is a sum of squares. Let\u2019s figure out the expressions for the derivatives. Recalling the expression for the loss:","c4463096":"In order to compute the derivative of the loss with respect to a parameter, we can apply the chain rule and compute the derivative of the loss with respect to its input (which is the output of the model), times the derivative of the model with respect to the parameter:","1e5d5d79":"There\u2019s a simpler way to keep things in check: changing the inputs so that the gradients aren\u2019t quite so different. We can make sure the range of the input doesn\u2019t get too far from the range of \u2013 1.0 to 1.0, roughly speaking. In our case, we can achieve something close enough to that by simply multiplying t_u by 0.1:","86ee1c09":"We\u2019re expecting t_u , w , and b to be the input tensor, weight parameter, and bias parameter, respectively. In our model, the parameters will be PyTorch scalars (aka zero-dimensional tensors), and the product operation will use broadcasting to yield the returned tensors. Anyway, time to define our loss:","32dd49c6":"## Broadcasting","d506c158":"![image.png](attachment:image.png)","a6e73913":"### THE TRAINING LOOP","a7379fae":"The complete training loop looks like this (code\/p1ch5\/1_parameter_estimation.ipynb):","5525e477":"* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch2-gan-horse-zebra\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch3-tensors\n* https:\/\/www.kaggle.com\/dmisky\/dlwpt-p1ch5-mechanics-of-learning","3b2acd4d":"### COMPUTING THE DERIVATIVES","afcd84ce":"Usually\u2014and in early versions of PyTorch, too\u2014we can only use element-wise binary operations such as addition, subtraction, multiplication, and division for arguments of the same shape. The entries in matching positions in each of the tensors will be used to calculate the corresponding entry in the result tensor.","39081205":"![image.png](attachment:image.png)","8e3d5eb2":"We\u2019ve already created our data tensors, so now let\u2019s write out the model as a Python function:","aaa526f6":"![image.png](attachment:image.png)","e6e47418":"Let\u2019s flesh it out again: we have a model with some unknown parameters, and we need to estimate those parameters so that the error between predicted outputs and measured values is as low as possible. We notice that we still need to exactly define a measure of the error. Such a measure, which we refer to as the loss function, should be high if the error is high and should ideally be as low as possible for a perfect match. Our optimization process should therefore aim at finding w and b so that the loss function is at a minimum.","19141d9e":"Computing the rate of change by using repeated evaluations of the model and loss in order to probe the behavior of the loss function in the neighborhood of w and b doesn\u2019t scale well to models with many parameters. Also, it is not always clear how large the neighborhood should be. We chose delta equal to 0.1 in the previous section, but it all depends on the shape of the loss as a function of w and b . If the loss changes too quickly compared to delta , we won\u2019t have a very good idea of in which direction the loss is decreasing the most.","687d3920":"Good: our loss decreases while we change parameters along the direction of gradient descent. It doesn\u2019t go exactly to zero; this could mean there aren\u2019t enough iterations to converge to zero, or that the data points don\u2019t sit exactly on a line. As we anticipated, our measurements were not perfectly accurate, or there was noise involved in the reading.","4a901833":"# Less loss is what we want","55f30587":"Since we\u2019re at it, let\u2019s introduce another piece of terminology. We call a training iteration during which we update the parameters for all of our training samples an epoch.","140fabb3":"In this section, we\u2019ll learn how we can take data, choose a model, and estimate the parameters of the model so that it will give good predictions on new data. To do so, we\u2019ll leave the intricacies of planetary motion and divert our attention to the second-hardest problem in physics: calibrating instruments.","6670524d":"The actual logging logic used for the output in this text is more complicated (see cell 15 in the same notebook: http:\/\/mng.bz\/pBB8), but the differences are unimportant for understanding the core concepts in this chapter.","a68b6d58":"![image.png](attachment:image.png)","2e7784a4":"Even though we set our learning rate back to 1e-2 , parameters don\u2019t blow up during iterative updates. Let\u2019s take a look at the gradients: they\u2019re of similar magnitude, so using a single learning_rate for both parameters works just fine. We could probably do a better job of normalization than a simple rescaling by a factor of 10, but since doing so is good enough for our needs, we\u2019re going to stick with that for now.","04a09716":"For our two loss functions |t_p \u2013 t_c| and (t_p \u2013 t_c)^2 , as shown in figure 5.4, we notice that the square of the differences behaves more nicely around the minimum: the derivative of the error-squared loss with respect to t_p is zero when t_p equals t_c . The absolute value, on the other hand, has an undefined derivative right where we\u2019d like to converge. This is less of an issue in practice than it looks like, but we\u2019ll stick to the square of differences for the time being.","96ab5dd7":"The weight tells us how much a given input influences the output. The bias is what the output would be if all inputs were zero.","08904e1e":"Here, we denote the normalized version of t_u by appending an n to the variable name. At this point, we can run the training loop on our normalized input:","e657b3cf":"However, there\u2019s another potential troublemaker in the update term: the gradient itself. Let\u2019s go back and look at grad at epoch 1 during optimization.","b48e8db4":"## Visualizing the data","7cc2fc55":"Note that we are building a tensor of differences, taking their square element-wise, and finally producing a scalar loss function by averaging all of the elements in the resulting tensor. It is a mean square loss.","2bd50acb":"Wait, what happened? Our training process literally blew up, leading to losses becoming inf . This is a clear sign that params is receiving updates that are too large, and their values start oscillating back and forth as each update overshoots and the next overcorrects even more. The optimization process is unstable: it diverges instead of converging to a minimum. We want to see smaller and smaller updates to params, not larger, as shown in figure 5.8.","40b81c15":"## Choosing a linear model as a first try"}}