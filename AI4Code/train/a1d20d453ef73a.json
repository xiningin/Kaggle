{"cell_type":{"bf5e8cb9":"code","b53f361c":"code","2df58d55":"code","f8eba3fe":"code","3f2ce4ed":"code","d789e901":"code","3817168c":"code","1a65da0b":"code","e85d7051":"code","47b872e0":"code","0756f287":"code","a55af00d":"code","0f5b4a14":"code","9cff482a":"code","8c2e01b2":"code","0fe8ffdb":"code","67b5dede":"markdown","d01081f8":"markdown","12ebd2ba":"markdown","9dc25925":"markdown","b97267d0":"markdown","ffc234a3":"markdown","87b4b655":"markdown","dbdf1f4c":"markdown","28db2ad8":"markdown","0b204d2b":"markdown","8ef5ce20":"markdown","a1d618aa":"markdown","af2d52ff":"markdown","7f2c561d":"markdown","9ee8abf1":"markdown","e6c9d3e7":"markdown","a4c29e3c":"markdown","f1354852":"markdown","4ccd42b7":"markdown","f0ec6ad3":"markdown","fcf6cb42":"markdown","16cce093":"markdown"},"source":{"bf5e8cb9":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\n# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nimport catboost\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\n# Let's be rebels and ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","b53f361c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2df58d55":"import pandas as pd\ndatafile = pd.read_csv(\"..\/input\/forest-fires-in-india\/datafile.csv\")","f8eba3fe":"datafile.head()","3f2ce4ed":"datafile.isnull().sum()","d789e901":"import missingno as msno\nmsno.matrix(datafile)","3817168c":"datafile.describe()","1a65da0b":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","e85d7051":"datafile.columns","47b872e0":"Line_Charts = datafile.copy()\nLine_Charts = Line_Charts.drop(['States\/UTs'], axis = 1)\n\n\nplt.figure(figsize=(14,6))\n\n# Add title\ntitle_obj=plt.title(\"Indian reinforest data for fire in different places\")\nplt.setp(title_obj, color='g') \n\nsns.lineplot(data = Line_Charts)","0756f287":"plt.figure(figsize=(10,6))\n\n# Add title\ntitle_obj=plt.title(\"Average fire in indian forest from dataset, by year\")\nplt.setp(title_obj, color='g') \n# Bar chart showing average arrival delay for Spirit Airlines flights by month\nsns.barplot(x=datafile.index, y=datafile['States\/UTs'])\n\n# Add label for vertical axis\ntitle_obj1=plt.ylabel(\"Forest areas in indian\")\nplt.setp(title_obj1, color='g') \nprint('To see the forest areas in plot, change the kernel theme to light!')","a55af00d":"from sklearn.model_selection import train_test_split\n\n\n# Read the data\nX = datafile.copy()\nX_test_full = datafile.copy()\ntest = datafile.copy()\n\n# Remove rows with missing target, separate target from predictors\n# X.dropna(axis=0, subset=['States\/UTs'], inplace=True)\ny = X['States\/UTs']              \nX.drop(['States\/UTs'], axis=1)\n#Whether to drop labels from the index (0 or \u2018index\u2019) or columns (1 or \u2018columns\u2019).\n\n# Break off validation set from training data\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\nlow_cardinality_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n                        X_train_full[cname].dtype == \"object\"]\n\n# Low cardinality means that the column contains a lot of \u201crepeats\u201d in its data range.\n# Examples of categorical variables are race, sex, age group, and educational level. \n# While the latter two variables may also be considered in a numerical manner by using exact values for age \n# and highest grade completed\n# nunique() function to find the number of unique values over the column axis. So when it finds over 10 uniqe \n# values and the cname is a \n# dtype 'object' which means Data type objects are useful for creating structured arrays. \n# A structured array is the one which contains different types of data.\n\n### one line meaning of above####\n## for cname in a dataframes column shall return a value to 'low_cardinality_cols' if there are more then 10 uniqe values\n## and the dtype shall be a object which is a structured array that can have different types of data (lik; int, float string ect.)\n# Select numeric columns\nnumeric_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n\n### for cname (every value, one at the time) in dataframe for columns return a value to 'numeric_cols' if the \n### dtype= int64 or float64.\n\n# Keep selected columns only\nmy_cols = low_cardinality_cols + numeric_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n\n## One-hot encode the data (to shorten the code, we use pandas)\nX_train = pd.get_dummies(X_train)\nX_valid = pd.get_dummies(X_valid)\n    #y_train = pd.get_dummies(y_train)\n    #y_valid = pd.get_dummies(y_valid)\nX_test = pd.get_dummies(X_test)\n\nX_train, X_valid = X_train.align(X_valid, join='left', axis=1)\nX_train, X_test = X_train.align(X_test, join='left', axis=1)\n#aligns them so that the two dataframes have the same row and\/or column configuration.","0f5b4a14":"y_dummies = pd.get_dummies(y_train)\ny_dummies.head()","9cff482a":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n#from xgboost import XGBRegressor\n\n\nmodel2 = RandomForestClassifier(n_estimators=150, max_depth=4, random_state=1)\nmodel = GradientBoostingClassifier(random_state=1)\nmodel3 = DecisionTreeClassifier(max_depth=3, random_state=1)\n#model=SGDClassifier(random_state=1)\n#model=ExtraTreesClassifier(random_state=1)\n#model = XGBRegressor()\n# Define the models\nmodel_1 = RandomForestClassifier(n_estimators=50, random_state=0)\nmodel_2 = RandomForestClassifier(n_estimators=100, random_state=0)\nmodel_3 = RandomForestClassifier(n_estimators=200, min_samples_split=20, random_state=0)\nmodel_4 = RandomForestClassifier(n_estimators=300, max_depth=6, random_state=1)\n\n\n\nmodel.fit(X_train, y_train)\ny_predictions = model.predict(X_valid)\n\nprint('model accuracy score',model.score(X_valid, y_predictions))","8c2e01b2":"model2.fit(X_train, y_train)\nprint('model1 accuracy score',model2.score(X_valid,y_predictions))\nmodel.fit(X_train, y_train)\nprint('model1 accuracy score',model.score(X_valid,y_predictions))\nmodel3.fit(X_train, y_train)\nprint('model1 accuracy score',model3.score(X_valid,y_predictions))","0fe8ffdb":"model_1.fit(X_train, y_train)\nprint('model1 accuracy score',model_1.score(X_valid,y_predictions))\nmodel_2.fit(X_train, y_train)\nprint('model2 accuracy score',model_2.score(X_valid,y_predictions))\nmodel_3.fit(X_train, y_train)\nprint('model3 accuracy score',model_3.score(X_valid,y_predictions))\nmodel_4.fit(X_train, y_train)\nprint('model4 accuracy score',model_4.score(X_valid,y_predictions))","67b5dede":"Et beslutningstr\u00e6 er en flowdiagram-lignende trestruktur, hvor en intern knude repr\u00e6senterer funktion (eller attribut), grenen repr\u00e6senterer en beslutningsregel, og hver bladknude repr\u00e6senterer resultatet. Den \u00f8verste knude i et beslutningstr\u00e6 kaldes rodnoden. Den l\u00e6rer at partitionere p\u00e5 grundlag af attributv\u00e6rdien. Det partitionerer tr\u00e6et p\u00e5 rekursivt vis kalder rekursivt partitionering. Denne flowchart-lignende struktur hj\u00e6lper dig i beslutningsprocessen.","d01081f8":"Random Forest er en ensemble-indl\u00e6ringsmetode til klassificering, regression og andre opgaver, der fungerer ved at konstruere et v\u00e6ld af beslutningstr\u00e6er p\u00e5 tr\u00e6ningstidspunktet og udsende den klasse, der er tilstanden for klasserne (Classifier) eller middel forudsigelse (regression) af de enkelte tr\u00e6er. RandomForestClassifier er korrekte for DecisionTree vane med at overfitting til tr\u00e6ningss\u00e6ttet","12ebd2ba":"![image.png](attachment:image.png)","9dc25925":"# One hot encodning","b97267d0":"As you can see above, the line of code is relatively short and has two main components:\n\nsns.lineplot tells the notebook that we want to create a line chart.\nEvery command that you learn about in this course will start with sns, which indicates that the command comes from the seaborn package. For instance, we use sns.lineplot to make line charts. Soon, you'll learn that we use sns.barplot and sns.heatmap to make bar charts and heatmaps, respectively.\ndata=spotify_data selects the data that will be used to create the chart.\nNote that you will always use this same format when you create a line chart, and the only thing that changes with a new dataset is the name of the dataset. So, if you were working with a different dataset named financial_data, for instance, the line of code would appear as follows:\n\nsns.lineplot(data=financial_data)\nSometimes there are additional details we'd like to modify, like the size of the figure and the title of the chart. Each of these options can easily be set with a single line of code.\n\n--------- from Alexis Cook in Data Visualization","ffc234a3":"Gradient boosting is an algoritme that boost the produces a prediction model in the form of an ensemble of weak prediction moldels. It happens by building the model in en a stage-wise fashion and then i gernalizes them by allowing optimization of an arbitrary differentiable loos function ","87b4b655":"# Model ","dbdf1f4c":"## Theory","28db2ad8":"Maximal dybde af et tr\u00e6: Tr\u00e6ets maksimale dybde. Hvis ingen, udvides nodes, indtil alle bladene indeholder mindre end min_samples_split-pr\u00f8ver. Den h\u00f8jere v\u00e6rdi af maksimal dybde for\u00e5rsager overfitting, og en lavere v\u00e6rdi for\u00e5rsager underfitting","0b204d2b":"Denne XGBoost er et bibliotek til udvikling af hurtige og h\u00f8jtydende gradientforst\u00e6rkende tr\u00e6modeller.\nAt XGBoost opn\u00e5r den bedste ydelse p\u00e5 en r\u00e6kke vanskelige maskinl\u00e6ringsopgaver.","8ef5ce20":"# Data cleaning","a1d618aa":" ### RandomForestClassifier","af2d52ff":"## Disclaimer! This kernel is only for educational purposes and made for fun therefor the content of the kernel should not be taken seriously. ","7f2c561d":"### GradientBoostingClassifie","9ee8abf1":"# Data visualisering","e6c9d3e7":"The above figure is all black becouse there are no missing values, if there where any missing values or nan-values there would be white lines in the boxes, under each columns.","a4c29e3c":"![image.png](attachment:image.png)","f1354852":"### XGBRegressor","4ccd42b7":"# Data importering","f0ec6ad3":"\nOne_hot-kodning g\u00f8r det muligt for ud at omdanne nominelle kategoriske data til funktioner med numercale v\u00e6rdier, mens de ikke matematisk indeb\u00e6rer nogen ordinal realisering mellem klasserne.","fcf6cb42":"### DecisionTreeClassifier","16cce093":"# Predictions"}}