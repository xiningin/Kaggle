{"cell_type":{"2c9ff66b":"code","46619805":"code","012bd274":"code","8243b75a":"code","560e1ee4":"code","247516d8":"code","d0e29111":"code","64e00fbe":"code","76447495":"code","6516a6f9":"code","190c302c":"code","1235152b":"code","56cac078":"code","044f8463":"code","6ee5aa20":"code","6e32eb90":"code","9fcc2a7c":"code","9a7b318b":"code","2f000bf8":"code","6adf7859":"markdown","6737d83b":"markdown","af59ba7a":"markdown","9b0f1486":"markdown","bcd1da5a":"markdown","a79d10af":"markdown","5152d289":"markdown","52a3751e":"markdown","d4c8398f":"markdown","297f64c5":"markdown","dfe2dd66":"markdown","f396fb22":"markdown","ef04d554":"markdown","e4bd3b65":"markdown","7738f6e6":"markdown","aab9ce92":"markdown","c1d512aa":"markdown","7a1440ae":"markdown","b3031859":"markdown","3714a8d6":"markdown","466fb8f7":"markdown","e866f11e":"markdown","6c9eb165":"markdown","f2a8c720":"markdown"},"source":{"2c9ff66b":"# import required modules\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier,export_graphviz\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score,classification_report,precision_score,f1_score","46619805":"# import the data\ndata = pd.read_csv(\"..\/input\/ISP_One_Attrition_Data_file.csv\")","012bd274":"# First few rows of the data\ndata.head()","8243b75a":"# Remove duplicates\ndata.drop_duplicates(subset=None, keep='first', inplace=True)\nprint(\"shape of the data after removing the duplicates\", data.shape)","560e1ee4":"# shape of the data\nprint(\"Number of rows :\",data.shape[0])\nprint(\"Number of columns :\",data.shape[1])\nprint(\"===============================================\")\n# check if there are any missing values in each attribute\nprint(data.isnull().any())\nprint(\"===============================================\")\n# check the number of samples and their data type\ndata.info()\n","247516d8":"# Correlation matrix using standard pearson correlation --> Check for linear dependancy\nplt.figure(figsize = (20,10))\nsns.heatmap(data.corr(method = \"pearson\"),annot = True,cmap=\"coolwarm\")","d0e29111":"# Removing the linearly dependant variable(income)\ncols = [\"expenditure\",\"months_on_network\",\"Num_complaints\",\"number_plan_changes\",\"relocated\",\"monthly_bill\",\"technical_issues_per_month\",\"Speed_test_result\"]\n","64e00fbe":"# statistics\nplt.figure(figsize=(16,8))\nsns.heatmap(data.describe(),fmt = \".2f\",cmap = \"coolwarm\",annot = True)","76447495":"# removing negative values in the months_on_network attribute\ndata = data[data.months_on_network>0]\nprint(\"Number of rows and columns after removing negative values :\",data.shape)","6516a6f9":"# outlier analysis\ndef plots(df,attrib):\n    # this function takes two arguments (dataframe and attribute of interest)\n    \n    # define the figure size\n    plt.figure(figsize = (16,4))\n    \n    # histogram\n    plt.subplot(1,2,1)\n    sns.distplot(df[attrib],bins = 50)\n    plt.title(\"Histogram\")\n    \n    # boxplot\n    plt.subplot(1,2,2)\n    sns.boxplot(y = df[attrib])\n    plt.title(\"Boxplot\")\n    plt.show()\nplots(data,\"expenditure\")\nplots(data,\"months_on_network\")\nplots(data,\"Speed_test_result\")","190c302c":"def find_skewed_boundaries(df,attrib,distance):\n    # distance is the attribute required to estimate the amount of data loss during the outlier trimming using IQR proximity measure\n    \n    IQR = df[attrib].quantile(0.75)-df[attrib].quantile(0.25)\n    lower_boundary = df[attrib].quantile(0.25) - (IQR * distance)\n    upper_boundary = df[attrib].quantile(0.75) + (IQR * distance)\n    \n    return upper_boundary,lower_boundary","1235152b":"# Find the limits for expenditure attribute\nexp_upper_lim,exp_lower_lim = find_skewed_boundaries(data,\"expenditure\",1.5)\nprint(\"Upper and lower limits of expenditure attribute :\",exp_upper_lim,exp_lower_lim)\n\n# Find the limits for months_on_network attribute\nmonths_upper,months_lower = find_skewed_boundaries(data,\"months_on_network\",1.5)\nprint(\"Upper and lower limits of months on network attribute :\", months_upper,months_lower)\n\n# Find the limits for speed_test results\nspeed_upper,speed_lower = find_skewed_boundaries(data,\"Speed_test_result\",1.5)\nprint(\"Upper and lower limits of speed_test_result attribute : \",speed_upper,speed_lower)","56cac078":"# Extract the outliers from each attribute\noutliers_expen = np.where(data[\"expenditure\"] > exp_upper_lim,True,\n                         np.where(data[\"expenditure\"] < exp_lower_lim,True, False))\n\noutliers_months = np.where(data[\"months_on_network\"] > months_upper,True,\n                         np.where(data[\"months_on_network\"] < months_lower,True, False))\n\noutliers_speed = np.where(data[\"Speed_test_result\"] > speed_upper,True,\n                         np.where(data[\"Speed_test_result\"] < speed_lower,True, False))\n\n# trim the dataset\ndata_trimmed = data.loc[~(outliers_expen+outliers_months+outliers_speed),]\nprint(\"Data size before and after the outlier removal : \" ,data.shape , data_trimmed.shape)","044f8463":"sns.countplot(data[\"active_cust\"])\ndata[\"active_cust\"].value_counts()","6ee5aa20":"\n# Normalization\n# Independant attributes or attribute vector\ncols = [\"expenditure\",\"months_on_network\",\"Num_complaints\",\"number_plan_changes\",\"relocated\",\"monthly_bill\",\"technical_issues_per_month\",\"Speed_test_result\"]\nX = data_trimmed[cols].values\n# dependant variable or the Class label\ny = data_trimmed[\"active_cust\"].values\n\n\nscale = StandardScaler()\nX = scale.fit_transform(X)\nprint(X[0:2])\n","6e32eb90":"# train and test split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)\n","9fcc2a7c":"model1 = DecisionTreeClassifier(criterion=\"gini\")\n#train the model\nmodel1.fit(X_train,y_train)\n#prediction for test data\ny_Pred = model1.predict(X_test)\nprint(\"y_pred has the predicted values for x_test\")","9a7b318b":"# Model Evaluation using confusion matrix\ncm = confusion_matrix(y_test,y_Pred)\nprint(\"Accuracy of the model : \",accuracy_score(y_test,y_Pred))\n\n# Confusion matrix\nplt.figure(figsize = (6,6))\nplt.subplot(2,1,1)\nplt.title(\"Confusion matrix : DecisionTreeClassifier\")\nsns.heatmap(cm,annot = True,cmap=\"BuPu\",fmt=\".2f\")\n\n# Countplot\nplt.figure(figsize = (6,6))\nplt.subplot(2,1,2)\nplt.title(\"No. of negative and positive classes in test data\")\n# Countplot to check the class imbalance\nsns.countplot(y_test)\n\n# Performance metrics\nsensitivity = (9754\/(9754+1303))\nprint(\"True positive rate\/sensitivity :\",sensitivity)\nspecificity = (6935\/(1345+6935))\nprint(\"True negative rate\/specificity :\",specificity)\nprecision = precision_score(y_test,y_Pred)\nprint(\"precision_score :\", precision)\nF1_score = f1_score(y_test,y_Pred)\nprint(\"f1 score :\",F1_score)\nprint(\"====================================================\")\nprint(classification_report(y_test,y_Pred))\n","2f000bf8":"# Model implementation : RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators = 20, criterion= \"gini\")\n#train\nmodel.fit(X_train,y_train)\ny_Pred = model.predict(X_test)\n\n#Evaluation of the Random forest classifier \ncm = confusion_matrix(y_test,y_Pred)\nprint(\"Accuracy of the model: \",accuracy_score(y_test,y_Pred))\nplt.figure(figsize = (6,4))\nplt.title(\"Confusion matrix : RandomForestClassifier\")\nsns.heatmap(cm,annot = True,cmap=\"Greens\",fmt=\".2f\")\n\n\n# Model evaluation\nprint(classification_report(y_test,y_Pred))\n","6adf7859":"# 1.4\n<font color = \"#DF12E9\" size = 4> <u>Handling noise\/inconsistent data <\/u>:-<\/font>","6737d83b":"# 1.3\n<font color = \"#DF12E9\" size = 4> <u>Central tendency and other stats<\/u>:-<\/font>","af59ba7a":"<strong><font color = \"#9999ff\" font = 3 size = 4>There are more than 10000 instances less in the class label \"0\", may be model perform well for the class label \"1\"(There will be more false positives).<\/font><\/strong>","9b0f1486":"# 2.\n<U><strong><font color = \"Red\" size = 6> Model Implementation<\/font><\/strong><\/U><br>\none click on below:<br>\n2.1. [Decision tree classifier](#2.1)<br>\n2.2. [Random Forest Classifier](#2.2)","bcd1da5a":"<font color = \"#00cccc\" size = 5><u> Observations: <\/u><\/font>\n<strong><font color = \"#9999ff\" size = 3>\n1. All the three attributes visualized above have skewed distribution, therefore the right method to remove the outliers is IQR proximity measure.\n2. Uppert_limit = (75th percentile)+(1.5 * IQR), Where IQR is the inter quartile range.\n3. Lower_limit = (25th percentile)-(1.5 * IQR).<\/font><\/strong>\n\n","a79d10af":"# 1.6\n<font color = \"DF12E9\" size = 4> <u>Check for a class imbalance problem <\/u>:-<\/font>","5152d289":"<font color = \"#00cccc\" size = 5><u> Observations: <\/u><\/font>\n\n<strong><font color = \"#9999ff\" size = 3>\n1. Accuracy has improved when compare to the single estimator decision tree classifier.\n2. There are 20 estimators are used in RandomForestClassifier to predict the class labels.","52a3751e":"# 2.2\n<font color = \"#DF12E9\" size = 4> <u>Random Forest Classifier <\/u>:-<\/font>","d4c8398f":"<strong><font color = \"#1E074C\" size = 6 fave = \"arial\"><u>ISP-ONE BROADBAND SERVICES.<\/u><\/font><\/strong>","297f64c5":"<font color = \"#00cccc\" size = 5><u> Observations: <\/u><\/font>\n<strong><font color = \"#9999ff\" size = 3>\n1. Normalization is required as there is a large difference between each attribute mean value.\n2. Outlier analysis and removal is required as there is a large difference between the max value and mean value of each attribute.\n3. No missing values.\n4. Inconsistent data in the months_on_network column(there are negative values). <\/font><\/strong>\n","dfe2dd66":"<font color = \"#00cccc\" size = 5><u> Observations: <\/u><\/font><br>\n<strong><font color = \"#9999ff\" size = 3>Let $r$  be the correlation coefficient between the selected attributes.<br>\nIf   $r>0$ : Attributes are positively correlated(One can be removed as redundant).<br>\nIf   $r<0$ : There is no linear dependancy.<\/font><\/strong>\n<br>\n<font color = \"#9999ff\" size = 3><strong>1. From the above correlation matrix, <font color = \"#ff6699\" size = 3>\"income\"<\/font> and <font color = \"#ff6699\" size = 3>\"expenditure\"<\/font> are positively correlated i.e <font color = \"#ff6699\">($r_{income,expenditure} = 1$)<\/font>. Hence one of the attribute can be removed as redundant. <br>\n 2. Similarly, the attributes <font color = \"#ff6699\" size = 3>\"relocated\"<\/font> and <font color = \"#ff6699\" size = 3>\"number_plan_changes\"<\/font> are positively correlated <font color = \"#ff6699\">($r_{relocated,numberplanchanges} = 0.6$)<\/font>.<\/strong><\/font>","f396fb22":"# 1.2\n<font color = \"#DF12E9\" size = 4> <u>Finding linear dependancy between the attributes using Pearson Correlation <\/u>:-<\/font>","ef04d554":"$Accuracy = (TP+TN)\\div(TP+TN+FP+FN) = 0.86$<br>\n$Precision = TP\\div(TP+FP) = 0.88$<br>\n$Recall = TP\\div(TP+FN) = 0.87$ <br>\n$F-score = (2 \\times precision \\times recall)\\div (precision+recall) = 0.87$\n\n<font color = \"#00cccc\" size = 5><u> Observations: <\/u><\/font>\n<strong><font color = \"#9999ff\" size = 3>\n1. Class of interest \"0\" has less precision because of the class imbalance problem that i have depicted before in the preprocessing part.\n2. True negative rate is the proportion of the negative tuples that are correctly predicted(83 percent of the negative classes are predicted correctly).\n3. True positive rate is the proportion of the positive tuples that are correctly predicted(88 percent of the positive classes are predicted correcly).\n4. Accuracy of the model is 0.86 for decicision tree classifier.\n\nLet's use the emsemble method(RandomForestClassifier) and check the accuracy.\n\n\n","e4bd3b65":"<strong><font color = \"#9999ff\" size = 3>\nData size is reduced(not much data loss), 1435 rows are identified as outliers in the different attributes.\nThe remaining attribures are not considered for the outlier analysis as they are more categorical.<\/font><\/strong>","7738f6e6":"=============================================================================================================================================================================","aab9ce92":"# 1.\n<U><strong><font color = \"Red\" size = 6> Data Preprocessing<\/font><\/strong><\/U><br>\n\nOne click on below :<br>\n1.1. [Removing duplicates in the data.](#1.1)<br>\n1.2. [Finding linear dependancy.](#1.2)<br>\n1.3. [Central tendency and other stats](#1.3)<br>\n1.4. [Handling noise\/inconsistent data](#1.4)<br>\n1.5. [Outlier Analysis](#1.5)<br>\n1.6. [Check for a class imbalance problem](#1.6)<br>\n1.7. [Normalization\/scaling.](#1.7)\n","c1d512aa":"# 2.1\n<font color = \"#DF12E9\" size = 4> <u>Decision tree classifier <\/u>:-<\/font>","7a1440ae":"# 1.7\n<font color = \"#DF12E9\" size = 4> <u>Normalization\/scaling <\/u>:-<\/font>","b3031859":"# 1.5\n<font color = \"#DF12E9\" size =4> <u>Outlier Analysis <\/u>:-<\/font>","3714a8d6":"# ********END******","466fb8f7":"<strong><font color = \"#9999ff\" size = 3>The ISP-One is internet service provider offering internet services to users all over the country. They are concerned about customer churn and trying to understand which customers are likely to leave, so that they can make focused effort on customer retention.  \nMain goal is to predict who are all leaving the ISP-ONE or shifting to other internet service provider, this is kind of a early prediction to prevent the customer churn.\nThe given data has the information about each customer in ISP-ONE.<br>\n    Attribute details :<br> \n                      1. active_cust         :  1-active, 0-Attrited.\n                      2. income              :  Monthly income of the customer.\n                      3. months_on_network   :  Duration in months.\n                      4. num_complaints      :  Number of complaints.\n                      5. number_plan_changes :  Number of plan changes.\n                      6. relocated           :  0-No, 1-yes\/relocated.\n                      7. monthly_bill        :  monthly bill of each customer.\n                      8. technical_issues    :  issues per month.\n                      9. speed_test_result   :  Internet speed of ISP-ONE.\n    \nactive_cust is the attribute indicating whether the customer left or not, Since there are two class labels(Yes or No) it is a binary classification problem.\nI will be using the classifier to predict the customer churn.\n<br>\n<font color = \"#05EFF2\" size = 5>Contents:<\/font><br>\none click on below:\n1. [Data Preprocessing](#1.)\n2. [Model Implementation](#2.)\n3. [Model Evaluation](#3.)\n   \n","e866f11e":"# 1.1\n<font color = \"#DF12E9\" size = 4> <u>Removing duplicates in the data <\/u>:- <\/font>","6c9eb165":"# 3.\n<U><strong><font color = \"Red\" size = 6> Model Evaluation<\/font><\/strong><\/U>","f2a8c720":"<img src=\"https:\/\/www.tripleplay.in\/images\/broadband.jpg\" width=\"1500\" height = \"500\">\n<font color = \"#059DA5\" >Image source : https:\/\/www.tripleplay.in\/<\/font>"}}