{"cell_type":{"3c913d7c":"code","788837c1":"code","32347cf3":"code","a6dbe081":"code","d11f1aae":"code","dcb1e0fb":"code","29b4e890":"code","a3eeb811":"code","8b641ba4":"code","d344223b":"code","1d3cb361":"code","12d2792c":"code","8f83009f":"code","468a924f":"code","82e7fc87":"code","5c95b7bd":"code","7eb6a77e":"code","67b5472d":"code","77a350d8":"code","21bf0f39":"code","9b1bae9a":"code","028a69ec":"code","a3365449":"code","92606391":"code","892470c7":"code","44f4d180":"code","7f236bad":"code","999ea6cf":"code","b8badff5":"code","759f8151":"markdown","4875c406":"markdown","ce7e7d16":"markdown","40980726":"markdown","f1c362b5":"markdown","14e68aed":"markdown","e73249df":"markdown","ee99dd0a":"markdown","9de6afec":"markdown","f3a2e601":"markdown"},"source":{"3c913d7c":"import os\nimport random\nimport re\nimport string\n\nfrom collections import defaultdict\n\nimport unidecode\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (confusion_matrix, \n                             ConfusionMatrixDisplay, \n                             classification_report)\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (BertModel, \n                          BertTokenizer, \n                          AdamW, \n                          get_linear_schedule_with_warmup)\n\nfrom pprint import pprint\nfrom tqdm.notebook import tqdm","788837c1":"# Reproducibility\nSEED = 0\n\nrandom.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = True\n\n# If available, use GPU \nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    print(f'Available GPU: {torch.cuda.get_device_name()}')    \nelse:\n    device = torch.device('cpu')","32347cf3":"# Load train data\nusecols = ['id', 'review_title', 'review_text', 'rating']\nraw_train_data = pd.read_csv('..\/input\/i2a2-nlp-2021-sentiment-analysis\/train.csv', \n                             index_col='id', \n                             usecols=usecols)","a6dbe081":"# Concatenate text features\nraw_train_data['review'] = raw_train_data['review_title'] + ' ' + raw_train_data['review_text']\n\nwith pd.option_context('display.max_colwidth', None):\n    display(raw_train_data[['review', 'rating']].head())","d11f1aae":"# Compile regular expressions\nremove_url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\nremove_email = re.compile(r'\\S+@\\S+')\nremove_extra_space = re.compile(r'\\s+')\n\ndef preprocess_text(text: str) -> str:\n\n    # Apply regular expressions\n    text = remove_url.sub('', text)\n    text = remove_email.sub('', text)\n    text = remove_extra_space.sub(' ', text)\n    \n    # Instatiate a TweetTokenizer object\n    tokenizer = TweetTokenizer(reduce_len=True, \n                               strip_handles=True)\n                               \n    # Tokenize the text    \n    tokens = tokenizer.tokenize(text)\n\n    return ' '.join(tokens)","dcb1e0fb":"# Apply preprocessing\nraw_train_data['review_clean'] = raw_train_data['review'].apply(preprocess_text)","29b4e890":"with pd.option_context('display.max_colwidth', None):\n    display(raw_train_data.head())","a3eeb811":"# Split data into 80% for training and 20% for validation\ntrain_data, valid_data = train_test_split(raw_train_data[['review_clean', 'rating']],\n                                          test_size=0.2, \n                                          stratify=raw_train_data['rating'], \n                                          random_state=0)\n\n# Sanity chech\ntrain_data.shape, valid_data.shape","8b641ba4":"# Load BERT tokenizer using a pretrained portuguese model\nPRETRAINED_MODEL_NAME = 'neuralmind\/bert-base-portuguese-cased'\ntokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME, do_lower_case=True)","d344223b":"sentence = train_data.iloc[4]['review_clean']\ntokens = tokenizer.tokenize(sentence)\ntokens_to_ids = tokenizer.convert_tokens_to_ids(tokens)\n\nprint(f'Sentence..: {sentence}', \n      f'Tokens....: {tokens}', \n      f'Tokens IDs: {tokens_to_ids}', sep='\\n\\n')","1d3cb361":"class AmericanasDataset(Dataset):\n\n    def __init__(self, review, rating, tokenizer, max_len):\n        self.review = review\n        self.rating = rating\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.review)\n\n    def __getitem__(self, index):\n        \n        input = self.tokenizer.encode_plus(\n            text = self.review[index],\n            add_special_tokens=True,\n            padding='max_length',\n            truncation=True,\n            max_length=self.max_len,\n            return_tensors='pt',\n            return_token_type_ids=False,\n            return_attention_mask=True,\n        )\n        \n        label_encoder = lambda label: int(label) - 1  # encoding labels: 1 -> 0; 2 -> 1 etc.\n                \n        return {\n            'input_ids': input['input_ids'].squeeze(),\n            'attention_mask': input['attention_mask'].squeeze(),\n            'rating': torch.tensor(label_encoder(self.rating[index]), dtype=torch.long)\n        }","12d2792c":"def create_dataloader(review, rating, tokenizer, max_len, batch_size, shuffle):\n    \n    dataset = AmericanasDataset(review, rating, tokenizer, max_len)\n    \n    dataloader = DataLoader(dataset, batch_size, shuffle, num_workers=2, pin_memory=True)\n    \n    return dataloader","8f83009f":"sentence_len = [len(sentence.split()) for sentence in train_data['review_clean']]\nbins = [*set(sentence_len)]\nsns.histplot(sentence_len, bins=bins, kde=True)\nplt.xlim([-10, 200])\nplt.xlabel('Word Count')\nplt.ylabel('Frequency')\nplt.tight_layout();","468a924f":"MAX_LENGTH = 128 \nBATCH_SIZE = 32\n\n# Prepare training dataloader\ntrain_loader = create_dataloader(train_data['review_clean'].values,\n                                 train_data['rating'].values,\n                                 tokenizer,\n                                 MAX_LENGTH,\n                                 BATCH_SIZE,\n                                 shuffle=True)\n\n# Prepare validation dataloader\nvalid_loader = create_dataloader(valid_data['review_clean'].values,\n                                 valid_data['rating'].values,\n                                 tokenizer,\n                                 MAX_LENGTH,\n                                 BATCH_SIZE,\n                                 shuffle=False)","82e7fc87":"class AmericanasClassifier(nn.Module):\n\n    def __init__(self, bert_model, dropout, num_classes):        \n        super().__init__()\n\n        self.bert = bert_model\n        \n        self.dropout = nn.Dropout(dropout)\n\n        self.dense = nn.Linear(self.bert.config.hidden_size, num_classes)\n        \n    def forward(self, input_ids, attention_mask):\n\n        _, output = self.bert(input_ids=input_ids,\n                              attention_mask=attention_mask)\n        \n        output = self.dropout(output)        \n\n        return self.dense(output)","5c95b7bd":"bert_model = BertModel.from_pretrained(PRETRAINED_MODEL_NAME, return_dict=False)\n\nmodel_params = {\n    'bert_model': bert_model,\n    'dropout': 0.5, \n    'num_classes': train_data['rating'].nunique() \n}\n\nmodel = AmericanasClassifier(**model_params)\n\npprint([*model.named_children()][-2:])","7eb6a77e":"# Training function\ndef train(model, loader, optimizer, criterion) -> tuple:\n    \n    epoch_loss, epoch_acc = 0, 0\n    \n    model.train()\n    \n    for batch in tqdm(loader, leave=None):\n        \n        input_ids, attention_mask, rating = (batch[key].to(device) for key in batch)\n\n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward step\n        outputs = model(input_ids=input_ids, \n                       attention_mask=attention_mask)\n        \n        loss = criterion(outputs, rating)\n        acc = accuracy(outputs, rating)\n        \n        # Backward step\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        # Update the learning rate\n        scheduler.step()\n        \n        # Compute metrics\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(loader), epoch_acc \/ len(loader)","67b5472d":"# Evaluating function\n@torch.no_grad()\ndef evaluate(model, loader, criterion):\n    \n    epoch_loss, epoch_acc = 0, 0\n    \n    model.eval()\n    \n    for batch in tqdm(loader, leave=None):\n\n        input_ids, attention_mask, rating = (batch[key].to(device) for key in batch)\n        \n        outputs = model(input_ids, attention_mask)\n        \n        loss = criterion(outputs, rating)\n        acc = accuracy(outputs, rating)\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(loader), epoch_acc \/ len(loader)","77a350d8":"def accuracy(outputs, rating):\n\n    _, preds = torch.max(outputs.data, dim=1)\n    correct = preds == rating\n    acc = sum(correct) \/ len(correct)\n    \n    return acc","21bf0f39":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","9b1bae9a":"EPOCHS = 4\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss().to(device)\n\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n\ntotal_steps = len(train_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,\n    num_training_steps=total_steps\n)","028a69ec":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","a3365449":"%%time\n\nbest_valid_loss = np.inf\n\nhistory = defaultdict(list)\n\nfor epoch in range(EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_loader, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc:.2%}')\n    print(f'\\tValid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc:.2%}\\n')\n    \n    # Save the best model\n    if valid_loss < best_valid_loss:\n        print(f'The validation loss decreaded: {best_valid_loss:.4f} --> {valid_loss:.4f}. Save the model...')\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n\n\n    history['train_loss'].append(train_loss)\n    history['train_accuracy'].append(train_acc)\n    history['valid_loss'].append(valid_loss)\n    history['valid_accuracy'].append(valid_acc)","92606391":"hist = pd.DataFrame(history)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax = ax.flat\n\nfor ii, metric in enumerate(['loss', 'accuracy']):\n    sns.lineplot(x=hist.index, y=f'train_{metric}', data=hist, label=f'Training {metric}', ax=ax[ii])\n    sns.lineplot(x=hist.index, y=f'valid_{metric}', data=hist, label=f'Validation {metric}', ax=ax[ii])\n    ax[ii].legend(frameon=False)\n\nsns.despine()\nplt.tight_layout()","892470c7":"# Load the saved model\nmodel.load_state_dict(torch.load('best_model.pth'))","44f4d180":"@torch.no_grad()\ndef predictions(model, loader):\n\n    model.eval()\n\n    all_preds = torch.tensor([]).to(device)\n\n    for batch in tqdm(loader, leave=None):\n\n        input_ids, attention_mask, rating = (batch[key].to(device) for key in batch)\n        \n        outputs = model(input_ids, attention_mask)\n\n        _, preds = torch.max(outputs.data, dim=1)\n\n        all_preds = torch.cat([all_preds, preds], dim=0)\n\n    return all_preds.cpu()","7f236bad":"preds = predictions(model, valid_loader).to(torch.long)\n\nlabel_encoder = lambda rating: int(rating) - 1\ny_test = np.array([label_encoder(rating) for rating in valid_data['rating']])","999ea6cf":"cm = confusion_matrix(y_test, preds) \ncm_norm = confusion_matrix(y_test, preds, normalize='true') \n\nfig, ax = plt.subplots(1, 2, figsize=(12, 7))\n\nConfusionMatrixDisplay(cm, display_labels=[1, 2, 3, 4, 5]).plot(cmap='Blues',\n                                                                ax=ax[0], \n                                                                colorbar=False,\n                                                                values_format='d')\nax[0].set_title('Confusion Matrix', fontsize=14)\n\nConfusionMatrixDisplay(cm_norm, display_labels=[1, 2, 3, 4, 5]).plot(cmap='Blues',\n                                                                     values_format='.2%',\n                                                                     colorbar=False,\n                                                                     ax=ax[1])\n\nax[1].set_title('Normalized Confusion Matrix', fontsize=14)\n\nplt.plot(cmap='Blues', colorbar=True)\nplt.tight_layout();","b8badff5":"print(classification_report(y_test, preds, target_names=['1', '2', '3', '4', '5']))","759f8151":"## Loading useful libraries and modules","4875c406":"## Training and evaluating the model","ce7e7d16":"## Confusion Matrix","40980726":"## Datasets and dataloaders","f1c362b5":"## Plotting results","14e68aed":"## Preparing text data to feed the model","e73249df":"## Classification report","ee99dd0a":"## Preprocessing raw text data","9de6afec":"## Loading raw training data","f3a2e601":"## Estimating the `max_length` parameter"}}