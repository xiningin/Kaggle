{"cell_type":{"392acbeb":"code","ba8e5caf":"code","10d3fb60":"code","faa98bee":"code","139133e0":"code","5908dc19":"code","c6c2d43e":"code","8d963c96":"code","cf580b51":"code","3d432407":"code","5f8d6d31":"code","0e7e4505":"code","e902d13b":"code","8642d968":"code","f1957375":"code","dc5d829f":"code","41730ed7":"code","1d9be7ac":"code","1742dc78":"code","b7fabb6e":"code","b9bdbd6c":"code","4b79e8ba":"code","0bc4bd1f":"code","834a3c46":"markdown","2b1e2bb2":"markdown","62e571f6":"markdown","8b96ac6f":"markdown","c211c4d1":"markdown","2cd51912":"markdown","211cb5f4":"markdown","6f396e3a":"markdown","d95afdac":"markdown","87caf8de":"markdown","9bad9e3f":"markdown","ce1b70c8":"markdown","a3466332":"markdown","c8dcbce7":"markdown","a19970fa":"markdown","5ece55d2":"markdown","d9e9228d":"markdown","cc279497":"markdown","c9ddc18a":"markdown","d3a66bb4":"markdown","69d4cb52":"markdown","04945ac3":"markdown"},"source":{"392acbeb":"import pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\n\noxford_indicators = [\n    \"School or university closing\",\n    \"Workplace closing\",\n    \"Cancel public events\",\n    \"Close public transport\",\n    \"Public information campaigns\",\n    \"Restrictions on internal movement\",\n    \"International travel restrictions and control\",\n    \"Fiscal measures\",\n    \"Monetary measures\",\n    \"Emergency investment in healthcare\",\n    \"Investment in vaccines\",\n    \"Testing policy\",\n    \"Contact tracing\"\n]\n\n# Read cached results\ncached_plot_df = pd.read_csv(\"..\/input\/cached-outputs\/cached_plot_matches.csv\", na_values='')\n\ntitle = 'Barriers and enablers (RED), and implications (BLUE) of the uptake of public health measures <br>given the variation in policy responses, <i>hover over dots for top matches<\/i>'\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=list(cached_plot_df['count_matches'])[::-1],\n    y=list(oxford_indicators)[::-1],\n    marker=dict(color=\"crimson\", size=12),\n    mode=\"markers\",\n    hovertemplate = '<b>%{x}: %{y} %{text}',\n    text = cached_plot_df['top_matches_uptake'],\n    showlegend = False))\n\nfig.add_trace(go.Scatter(\n    x=list(cached_plot_df['count_matches_impl'])[::-1],\n    y=list(oxford_indicators)[::-1],\n    marker=dict(color=\"blue\", size=12),\n    mode=\"markers\",\n    hovertemplate = '<b>%{x}: %{y} %{text}',\n    text = cached_plot_df['top_matches_impl'],\n    showlegend = False))\n\n\nfig.add_annotation\n\nfig.update_layout(title=title,\n              hoverlabel_align = 'left',\n              hovermode='closest',\n              xaxis_title='Counts of references in the corpus',\n              yaxis_title='Policy Responses')\n\nfig.show()","ba8e5caf":"import covid19_tools as cv19\nimport pandas as pd\nimport re\nfrom IPython.core.display import display, HTML\nimport html\nimport numpy as np\nimport json\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport glob\nimport os\n\nMETADATA_FILE = '..\/input\/CORD-19-research-challenge\/metadata.csv'\n\n# Load metadata\nmeta = cv19.load_metadata(METADATA_FILE)\n\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","10d3fb60":"!dpkg -i ..\/input\/libgrapenlp\/libgrapenlp_2.8.0-0ubuntu1_xenial_amd64.deb\n!dpkg -i ..\/input\/libgrapenlp\/libgrapenlp-dev_2.8.0-0ubuntu1_xenial_amd64.deb\n!pip install pygrapenlp\n\nfrom collections import OrderedDict\nfrom pygrapenlp import u_out_bound_trie_string_to_string\nfrom pygrapenlp.grammar_engine import GrammarEngine","faa98bee":"from ipywidgets import Image\nf = open(\"..\/input\/grammar\/grammar.png\", \"rb\")\nimage = f.read()\nImage(value=image)","139133e0":"def parse_into_sentences(text):\n    regex_list = [re.compile(r\"([ (\\[]%s([.1-9 ]+[A-Z]?[a-z]?)*[ )\\]])\" % name, re.IGNORECASE) for name in [\"fig\", \"figure\", \"table\"]]\n    regex_list.extend([re.compile(r\"%s\" % term, re.IGNORECASE) for term in [\"et al\\.\", \"i\\.e\\.\", \"e\\.g\\.\", \" [1-9]\\.\", \"\\.\\.\\.\", \"\\.(?=\\))\"]])\n    regex_list.append(re.compile(r\" [A-Z]\\.\"))\n\n    preprocessed_text = text\n    #preprocessed_text\n    for r in regex_list:\n        preprocessed_text = r.sub(\"\", preprocessed_text)\n\n    sentences_list = preprocessed_text.split('.')\n    \n    return sentences_list","5908dc19":"def native_results_to_python_dic(sentence, native_results):\n    top_segments = OrderedDict()\n    if not native_results.empty():\n        top_native_result = native_results.get_elem_at(0)\n        top_native_result_segments = top_native_result.ssa\n        for i in range(0, top_native_result_segments.size()):\n            native_segment = top_native_result_segments.get_elem_at(i)\n            native_segment_label = native_segment.name\n            segment_label = u_out_bound_trie_string_to_string(native_segment_label)\n            segment = OrderedDict()\n            segment['value'] = sentence[native_segment.begin:native_segment.end]\n            segment['start'] = native_segment.begin\n            segment['end'] = native_segment.end\n            top_segments[segment_label] = segment\n    return top_segments\n\ndef execute_phm_soc_grammar():\n    base_dir = os.path.join('..', 'input', 'grammar')\n    grammar_pathname = os.path.join(base_dir, 'soc_grammar.fst2')\n    bin_delaf_pathname = os.path.join(base_dir, 'test_delaf.bin')\n    grammar_engine = GrammarEngine(grammar_pathname, bin_delaf_pathname)\n\n    df = pd.DataFrame([], columns=['index', 'list_phm_impl', 'list_phm_uptake', 'sentence'])\n\n    for record_id in range(len(full_text_repr)):\n        record_text = \"\".join([item['text'] for item in full_text_repr[record_id]['body_text']])\n        record_sentences = parse_into_sentences(record_text)\n        processed = 0\n\n        for sentence in record_sentences:\n            context = {}\n            native_results = grammar_engine.tag(sentence, context)\n            matches = native_results_to_python_dic(sentence, native_results)\n\n            if 'list_phm_impl' in matches or 'list_phm_uptake' in matches:\n                df = df.append(\n                {\n                    'index': record_id, \n                    'list_phm_impl': matches['list_phm_impl']['value'] if 'list_phm_impl' in matches else '', \n                    'list_phm_uptake': matches['list_phm_uptake']['value'] if 'list_phm_uptake' in matches else '',\n                    'sentence': sentence\n\n                }, ignore_index=True)\n                processed += 1\n\n        # print(\"record %d, processed %d sentences, found %d matches\" % (record_id, len(record_sentences), processed))\n\n    print(\"Processed %d documents, found %d matches\" % (len(full_text_repr), df.shape[0]))\n    df.to_csv('output.csv')\n    return df","c6c2d43e":"grammar_terms = ['public health', 'intervention', 'policy', 'policies', 'quarantine', 'lockdown', 'contact tracing', 'distancing', 'emergency']\n\nmeta, soc_ethic_counts = cv19.count_and_tag(meta,\n                                               grammar_terms,\n                                               'soc_ethic')\nnumber_of_SOC_articles = len(meta[meta.tag_soc_ethic == True])\nprint('Number of articles is ', number_of_SOC_articles) \n\nprint('Loading raw data ...')\nmetadata_filter = meta[meta.tag_soc_ethic == True] \nfull_text_repr = cv19.load_full_text(metadata_filter,\n                                     '..\/input\/CORD-19-research-challenge')","8d963c96":"def execute_phm_extract_grammar():\n    base_dir = os.path.join('..', 'input', 'grammar')\n    bin_delaf_pathname = os.path.join(base_dir, 'test_delaf.bin')\n    grammar_pathname = os.path.join(base_dir, 'phm_list.fst2')\n    grammar_engine = GrammarEngine(grammar_pathname, bin_delaf_pathname)\n\n    df = pd.DataFrame([], columns=['index', 'list_phm', 'sentence'])\n\n    for record_id in range(len(full_text_repr)):\n        record_text = \"\".join([item['text'] for item in full_text_repr[record_id]['body_text']])\n        record_sentences = parse_into_sentences(record_text)\n        processed = 0\n\n        for sentence in record_sentences:\n            context = {}\n            native_results = grammar_engine.tag(sentence, context)\n            matches = native_results_to_python_dic(sentence, native_results)\n\n            if 'list_phm' in matches:\n                df = df.append(\n                {\n                    'index': record_id, \n                    'list_phm': matches['list_phm']['value'] if 'list_phm' in matches else '', \n                    'sentence': sentence\n\n                }, ignore_index=True)\n                processed += 1\n\n        # print(\"record %d, processed %d sentences, found %d matches\" % (record_id, len(record_sentences), processed))\n\n    print(\"Processed %d documents, found %d matches\" % (len(full_text_repr), df.shape[0]))\n    \n    return df","cf580b51":"df_grammar_output = execute_phm_extract_grammar()\ndf_grammar_output.to_csv('grammar_output_uptake_impl.csv')","3d432407":"import tensorflow as tf\n\n# download the Universal Sentence Encoder module and uncompress it to the destination folder. \n!mkdir \/kaggle\/working\/universal_sentence_encoder\/\n!curl -L 'https:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/5?tf-hub-format=compressed' | tar -zxvC \/kaggle\/working\/universal_sentence_encoder\/","5f8d6d31":"import tensorflow_hub as hub\n\n# load the Universal Sentence Encoder module\nuse_embed = hub.load('\/kaggle\/working\/universal_sentence_encoder\/')","0e7e4505":"# Read cached grammar output and transform to add the paper id \/ SHA value\ndf = pd.read_csv(\"..\/input\/cached-outputs\/grammar_output_uptake_impl.csv\", na_values='')\ndf = df.drop(columns=['Unnamed: 0'])\ndf_full_text_repr = pd.DataFrame(full_text_repr)\npaper_ids = df_full_text_repr['paper_id']\ndf['paper_id'] = df.apply(lambda row: paper_ids[row['index']], axis=1)\n\ndf.head()","e902d13b":"processed = pd.melt(df, id_vars=[\"paper_id\", \"sentence\"], value_vars=[\"list_phm_impl\", \"list_phm_uptake\"], value_name=\"match\").dropna()\nprocessed.set_index('paper_id')\n\nprocessed['cord_uid'] = np.nan\nprocessed['title'] = np.nan\nprocessed['doi'] = np.nan\nprocessed['url'] = np.nan\nprocessed['authors'] = np.nan\nprocessed['authors_short'] = np.nan\nprocessed['journal'] = np.nan\n\nfor i, row in processed.iterrows():\n    meta_raw = metadata_filter.loc[metadata_filter['sha'] == row['paper_id']]\n    #if len(meta_raw) == 1:\n    if 'title' in meta_raw:\n         processed.loc[i, 'cord_uid'] = meta_raw['cord_uid'].values[0]\n         processed.loc[i, 'title'] = meta_raw['title'].values[0]\n         processed.loc[i, 'doi'] = meta_raw['doi'].values[0]\n         processed.loc[i, 'url'] = meta_raw['url'].values[0]\n         processed.loc[i, 'authors'] = meta_raw['authors'].values[0]\n         processed.loc[i, 'authors_short'] = meta_raw['authors_short'].values[0]\n         processed.loc[i, 'journal'] = meta_raw['journal'].values[0]\n\nprocessed = processed.rename(columns={\"sentence\": \"excerpt\", \"variable\": \"type\"})\nprocessed.head()","8642d968":"oxford_indicators = [\n    \"School or university closing\",\n    \"Workplace closing\",\n    \"Cancel public events\",\n    \"Close public transport\",\n    \"Public information campaigns\",\n    \"Restrictions on internal movement\",\n    \"International travel restrictions and control\",\n    \"Fiscal measures\",\n    \"Monetary measures\",\n    \"Emergency investment in healthcare\",\n    \"Investment in vaccines\",\n    \"Testing policy\",\n    \"Contact tracing\"\n]\noxford_indicators_descriptive = [\n    \"school university and educational facilities\",\n    \"workplace work job\",\n    \"events gatherings of people\",\n    \"public transport bus train\",\n    \"public information campaigns to raise awareness\",\n    \"strict lockdown quarantine\",\n    \"international travel restrictions airport\",\n    \"economy job loss low-income policies adopted fiscal tax spending\",\n    \"value of interest rate\",\n    \"investment hospitals masks healthcare public health patient\",\n    \"vaccine development\",\n    \"testing policy\",\n    \"contact tracing surveillance privacy\"\n]\n\noxford_indicators_embeddings = use_embed(oxford_indicators_descriptive)","f1957375":"CORR_THRESHOLD = 0.2\nBATCH_SIZE = 100\nNUM_TOP_MATCHES_TO_RETURN = 3\nMAX_CHARS_TOOLTIP = 50\n\ndef extract_grammar_matches_uptake_for_public_health_measures(df_input, grammar_label, column_to_match):\n    phm_uptake = df_input[df_input['type'] == grammar_label]\n\n    count_matches = np.zeros(len(oxford_indicators))\n    df_matches = []\n\n    for idx in range(int(len(phm_uptake)\/BATCH_SIZE)):\n        if idx*BATCH_SIZE > len(phm_uptake):\n            break\n        idx_start = idx*BATCH_SIZE\n        idx_end = min((idx+1)*BATCH_SIZE, len(phm_uptake))\n\n        phm_uptake_val = phm_uptake[idx_start:idx_end]\n        phm_uptake_val = phm_uptake_val.reindex()\n        phm_uptake_embeddings = use_embed(phm_uptake_val[column_to_match])\n        corr_uptake = np.inner(oxford_indicators_embeddings, phm_uptake_embeddings)\n\n        for i in range(len(phm_uptake_embeddings)):\n            for j in range(len(oxford_indicators_embeddings)):\n                if corr_uptake[j][i] > CORR_THRESHOLD:\n                    count_matches[j] += 1\n                    df_matches.append([j, \n                                       corr_uptake[j][i], \n                                       phm_uptake_val.iloc[i]['paper_id'], \n                                       phm_uptake_val.iloc[i]['excerpt'],\n                                       phm_uptake_val.iloc[i]['cord_uid'],\n                                       phm_uptake_val.iloc[i]['title'],\n                                       phm_uptake_val.iloc[i]['doi'],\n                                       phm_uptake_val.iloc[i]['url'],\n                                       phm_uptake_val.iloc[i]['authors'],\n                                       phm_uptake_val.iloc[i]['authors_short'],\n                                       phm_uptake_val.iloc[i]['journal']]\n                                     )\n\n                    # print(\" (%d, %d) Found macth %f\\n ---\\t%s\\n ---\\t%s\\n ---\\t%s\" % (i, j, corr_uptake[j][i], oxford_indicators[j], phm_uptake_val.iloc[i], phm_uptake.iloc[i]['sentence']))\n        # print(\"processed batch %d - %d\" % (idx_start, idx_end))\n\n        del corr_uptake\n        del phm_uptake_val\n        del phm_uptake_embeddings\n        \n    return count_matches, df_matches ","dc5d829f":"count_matches_uptake, df_matches = extract_grammar_matches_uptake_for_public_health_measures(processed, 'list_phm_uptake', 'match')\ndf_matches_uptake = pd.DataFrame(df_matches)\ndf_matches_uptake.columns = [\"policy_response\", \"match_score\", \"paper_id\", \"excerpt\", \"cord_uid\", \"title\", \"doi\", \"url\", \"authors\", \"authors_short\", \"journal\"]","41730ed7":"indicator_matches = df_matches_uptake\nindicator_matches = df_matches_uptake[df_matches_uptake['policy_response'] == 0]\nindicator_matches.sort_values(by='match_score', ascending=False, inplace=True)\nprint(\"Found %d entries corresponding to the indicator %s\" % (len(indicator_matches), oxford_indicators[0]))\nindicator_matches.head()","1d9be7ac":"for indicator_id in range(len(oxford_indicators)): \n    indicator_matches = df_matches_uptake\n    indicator_matches = indicator_matches[indicator_matches['policy_response'] == indicator_id]\n    indicator_matches.sort_values(by='match_score', ascending=False, inplace=True)\n\n    to_save = indicator_matches.drop(['paper_id'], axis=1)\n    to_save.to_csv('policy_measures_indicator_%d_findings_plus_excerpts.csv' % indicator_id)\n    print(\"Saved %d entries corresponding to the indicator %s\" % (len(to_save), oxford_indicators[indicator_id]))","1742dc78":"\ndef generate_plot_labels(df_matches, df_input):\n    res = []\n    df_input = df_input.set_index('paper_id')\n\n    for ox_ind in range(len(oxford_indicators)):\n        df_ = df_matches[df_matches['policy_response'] == ox_ind]\n        df_.sort_values(by='match_score', ascending=False, inplace=True)\n        combined = ''\n        num_included = 0\n        included = []\n\n        for p in range(len(oxford_indicators)):\n            if num_included == NUM_TOP_MATCHES_TO_RETURN: \n                break\n            record_id = df_['paper_id'].iloc[p]\n            if record_id not in included:\n                included.append(record_id)\n                record_details = \"\"\n                \n                if 'title' in df_input.loc[record_id]:\n                    if type(df_input.loc[record_id]['title']) == str :\n                        record_details = \"- \\\"%s\\\" by %s\" % (df_input.loc[record_id]['title'], df_input.loc[record_id]['authors_short'])\n                    else:\n                        record_details = \"- \\\"%s\\\" by %s\" % (df_input.loc[record_id]['title'].values[0], df_input.loc[record_id]['authors_short'].values[0])\n                    num_included += 1\n                    \n                    if len(record_details) > MAX_CHARS_TOOLTIP:\n                        r = \"\"\n                        out = [(record_details[i:i+MAX_CHARS_TOOLTIP]) for i in range(0, len(record_details), MAX_CHARS_TOOLTIP)] \n                        for o in out:\n                            r = \"%s<br>%s\" % (r, o)\n                        record_details = r[4:]\n                        \n                    combined = \"%s<br>%s\" % (combined, record_details)\n                        \n\n        res.append(combined)\n        \n    return res\n\ntop_matches_uptake = generate_plot_labels(df_matches_uptake, processed)","b7fabb6e":"count_matches_impl, df_matches = extract_grammar_matches_uptake_for_public_health_measures(processed, 'list_phm_impl', 'match')\ndf_matches_impl = pd.DataFrame(df_matches)\ndf_matches_impl.columns = [\"policy_response\", \"match_score\", \"paper_id\", \"excerpt\", \"cord_uid\", \"title\", \"doi\", \"url\", \"authors\", \"authors_short\", \"journal\"]\ntop_matches_impl = generate_plot_labels(df_matches_impl, processed)","b9bdbd6c":"# Save outputs for quick access\n\ncached_plot = pd.DataFrame()\ncached_plot['count_matches'] = count_matches_uptake\ncached_plot['count_matches_impl'] = count_matches_impl\ncached_plot['top_matches_uptake'] = top_matches_uptake\ncached_plot['top_matches_impl'] = top_matches_impl\n\ncached_plot.to_csv('\/kaggle\/working\/cached_plot_matches.csv', index = False)","4b79e8ba":"import plotly.express as px\nimport plotly.graph_objects as go\n\ntitle = 'Barriers, enablers (RED), and implications (BLUE) of the uptake of public health measures <br>given the variation in policy responses, <i>hover over dots for top matches<\/i>'\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(\n    x=list(count_matches_uptake)[::-1],\n    y=list(oxford_indicators)[::-1],\n    marker=dict(color=\"crimson\", size=12),\n    mode=\"markers\",\n    hovertemplate = '<b>%{x}: %{y} %{text}',\n    text = top_matches_uptake,\n    showlegend = False))\n\nfig.add_trace(go.Scatter(\n    x=list(count_matches_impl)[::-1],\n    y=list(oxford_indicators)[::-1],\n    marker=dict(color=\"blue\", size=12),\n    mode=\"markers\",\n    hovertemplate = '<b>%{x}: %{y} %{text}',\n    text = top_matches_impl,\n    showlegend = False))\n\n\nfig.add_annotation\n\nfig.update_layout(title=title,\n              hoverlabel_align = 'left',\n              hovermode='closest',\n              xaxis_title='Counts of references in the corpus',\n              yaxis_title='Policy Responses')\n\nfig.show()","0bc4bd1f":"# Clean up before transitioning into the next section of the notebook\ndel full_text_repr","834a3c46":"Calculate the semantic similarity between each of the policy responses and the matches returned by the language grammar.","2b1e2bb2":"Here is our [UniText](https:\/\/unitexgramlab.org\/) grammar graph. The grammar aims to help us answer the following questions:\n* What are the enablers and barriers for the uptake of public health measures?\n* What is the impact of public health measures for prevention and control? This includes the rapid identification of the secondary impacts of these measures. (e.g. use of surgical masks, modification of health seeking behaviors for SRH, school closures)?","62e571f6":"# Semantic_Similarieties","8b96ac6f":"Here we describe the technical approach that lead to these findings. We first load needed python libraries and the corpus metadata file.","c211c4d1":"Persist results to the output directory","2cd51912":"We further annotate the grammar output file with some of the data attributes in the metadata file","211cb5f4":"We now want to group our findings into categories which relate to social and ethical issues.\n\nWe found that the [Oxford COVID-19 Government Response Tracker (OxCGRT) white paper](https:\/\/www.bsg.ox.ac.uk\/sites\/default\/files\/2020-04\/BSG-WP-2020-031-v4.0_0.pdf) by the Blavatnik School of Government, University of Oxford, is helpful in providing a categorization of public health measures. We have adopted their categorization in order to present our results to policymakers. The OxCGRT project collects information about the variation in government responses to COVID-19. They do that through a metrics framework consisting of 13 indicators (defined below). ","6f396e3a":"Define a helper function to create summaries which we will use in the data visualization below","d95afdac":"(please wait for the graph to load)\n\nWe found that the [Oxford COVID-19 Government Response Tracker (OxCGRT) white paper](https:\/\/www.bsg.ox.ac.uk\/sites\/default\/files\/2020-04\/BSG-WP-2020-031-v4.0_0.pdf) by the Blavatnik School of Government, University of Oxford, provides a very helpful categorization of public health measures. We have adopted their categorization in order to present our results and allow decision-makers to inspect them at a more granular level. The OxCGRT project collects information about the variation in government responses to COVID-19. They do that through a metrics framework consisting of 13 indicators (in the table below). In our proposal, we aim to further identify the social and ethical concerns related to each of these indicators. See the output csv files for full details as well as a short summary of our findings in the table below. \n\n| Indicator | Description (privided by OxCGRT) | Social & Ethical Concerns  | Number of papers<br>discussing the concerns | Example paper excerpt discussing the indicator | Name of the CSV file with<br>all papers details, excerpts,<br>and similarity scores  |\n|---|---|---|---|---|\n| School or university closing | Record closings of schools and universities | Efforts to support sustained education, access, and capacity building in the area of ethics  | 60  | Public health measures can be highly effective, especially when they are part of a structured programme that includes instruction and education and when they are delivered together  | policy_measures_<br>indicator_0_findings<br>_plus_excerpts.csv |\n| Workplace closing | Record closings of workplaces  | Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the livelihood of people  | 69  |  Therefore, to identify factors that may lead to disproportionate vulnerability in the event of a serious outbreak, we used multivariable logistic regression to model the predicted probability that some groups of working adults (delineated by employment characteristics such as inability to work from home, lack of pay when absent from work, and self-employment) may be less able than identifi ed referent groups to comply with pandemic infl uenza mitigation strategies that require voluntary isolation from work  | policy_measures_<br>indicator_1_findings<br>_plus_excerpts.csv |\n| Cancel public events | Record cancelling public events  |  Efforts to embed ethics across all thematic areas, engage with novel ethical issues that arise | 63  |  Examples of social distancing include closing of schools, non-essential workplace closures, and avoiding large gatherings (public transport, concerts, religious gatherings) in which a large number of individuals are in close contact and facilitating the contagion process | policy_measures_<br>indicator_2_findings<br>_plus_excerpts.csv |\n| Close public transport | Record closing of public transport  | Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the livelihood of people  | 26  |    As summarized inthe Appendix, partial lockdown includes \"closed-off management\" on highways, railways and public transport systems; and sets up checkpoints to control the inflow population, and implements surveillance and tighter controls in each neighborhood | policy_measures_<br>indicator_3_findings<br>_plus_excerpts.csv |\n| Public information campaigns |  Record presence of public information campaigns | Efforts to support awareness and sustained education, access, and capacity building in the area of ethics  |  214 |  Especially in major emergencies involving national security and the safety of lives and property, the government should decisively intervene in publicity control and information dissemination | policy_measures_<br>indicator_4_findings<br>_plus_excerpts.csv |\n| Restrictions on internal movement | Record closing of public transport | Efforts to support awareness and sustained education, access, and capacity building in the area of ethics    | 461  |  Very few jurisdictions have articulated explicit procedures and policies to determine whether or not an individual should be subject to quarantine  | policy_measures_<br>indicator_5_findings<br>_plus_excerpts.csv |\n| International travel restrictions and control | Record restrictions on international travel  |  Efforts to identify how the burden of responding to the outbreak and implementing public health measures affects the livelihood of people | 98  | Travel-related policy and public health responses to epidemics take several approaches, and include travel restrictions to and from affected areas, and passenger screening  | policy_measures_<br>indicator_6_findings<br>_plus_excerpts.csv |\n| Fiscal measures |  What economic stimulus policies are adopted? |  Social concerns about job loss and the exacerbation of income inequality |  38 |  The focused measures include single parent allowance, transportation subsidy for low-income families, job training programs for the unemployed, and so on  | policy_measures_<br>indicator_7_findings<br>_plus_excerpts.csv |\n| Monetary measures |  Monetary economic stimulus |  Social concerns about the value of interest rate | 38  |  In the event of a large-scale or acute emergency response, HHS may request supplemental appropriations from the US Congress specifically for that emergency or may consider reprogramming funds  | policy_measures_<br>indicator_8_findings<br>_plus_excerpts.csv |\n| Emergency investment in healthcare | Short-term spending on, e.g, hospitals, masks, etc  | Social and ethical concerns related to the healthcare system | 435  | The inability to obtain the masks specified in the government directives led to considerable staff alarm and raised the question whether staff should be allowed to see patients if they could not be provided the appropriate mask  | policy_measures_<br>indicator_9_findings<br>_plus_excerpts.csv |\n| Investment in vaccines | Announced public spending on vaccine development  | Social and ethical concerns related to vaccines | 309  | About one-third of the patients who did not receive the vaccination reported that there was no need to receive the vaccine, suggesting more intensive intervention is needed to understand the reasons behind such thoughts and to motivate this group to consider the vaccination  | policy_measures_<br>indicator_10_findings<br>_plus_excerpts.csv |\n| Testing policy | Who can get tested?  | Efforts to identify issues in the access to tests  | 138  |  The vulnerability of emergency response resources to natural events and hazardous material releases should be assessed | policy_measures_<br>indicator_11_findings<br>_plus_excerpts.csv |\n| Contact Tracing | Are governments doing contact tracing? |  Efforts to identify issues related to surveillance and privacy related to the use of contact tracing technology | 175  |   The key differences (revealed in the comments) were the extent to which participants were concerned about the lack of a clear legal and social mandate and the potential for negative Panellist's preferences as to how public health authorities should contact someone flagged by an online event-based communicable disease surveillance system public reactions | policy_measures_<br>indicator_12_findings<br>_plus_excerpts.csv |\n\n \n---\n\nOur conversations with experts as well as qualitative research and overview of the CORD-19 dataset highlighted that there's **a need for management of scarce resources relative to outcome predictions**.\n\nThe following ethical issues exist:\n* Ethical concerns related to the health and well-being of patients: \n  * Who gets admitted to the hospital?\n  * Who gets a ventilator?\n  * Who makes the decision whether a Cardiopulmonary Resuscitation (CPR) is performed? \n  CPR uses mouth-to-mouth or machine breathing and chest compressions to restore the work of the heart and lungs when someone's heart or breathing has stopped. \n* Ethical concerns related to the well-being of caregivers:\n  * Should healthcare practitioners be treated in the same way? \n  Nurses may be forced to work without adequate personal protective equipment to keep them safe. Should they be expected to risk their lives?\n  * Who makes the decision on who are the \"critical workers\" that need to be working?","87caf8de":"The grammar takes text input and tags it according to the graph. In our case, the tags which we seek to apply are \"list_phm_uptake\" and \"list_phm_impl\". We could have achieved similar results with a large number of regular expressions, however grammars are a much more flexible approach to complex natural language processing. We experimented with a number of different grammars and achieved best results with the grammar described here. \n* the tag \"list_phm_uptake\" aims to extract a list of factors on which public health measures depend on\n* the tag \"list_phm_impl\" aims to extract a list of implications of different public health measures\n\nIn the next iterations of this project we aim to create more grammars exploring the rest of the task questions.\n\n## Processing\n\nWe first split each document into a list of sentences and then process each sentence through the grammar.\n\nNext, we define a helper function that splits a single document into a list of sentences. In order to use the '.' character as a separator between sentences, we will:\n - replace mentions of 'et al.' with 'et al' \n - replace mentions of '...' with ''\n - 'etc' will most likely appear at the end of the sentence and therefore might not be a problem\n - replace i.e. with ':' \n - Table and figure numbers, for example: (Table 11 .4), ( Fig. 11 .1B) Fig. 11 .5 shows\n - numbered lists '1.' etc.\n - abbreviations 'Scalpels with no. 11 or no. 22 ', 'while some agents such as F. rodentium and C. piliforme'","9bad9e3f":"We then define a function which will pass all documents in the corpus through the grammar and return the tagged matches in a dataframe.","ce1b70c8":"# Visualization\n\nWe summarize the results in the data visualization. The plot includes two traces, one of them relates to the \"list_phm_impl\" tag in the grammar - exploring the implications of the public health measures, and the other to the \"list_phm_uptake\" tag which explores the factors which are barries and\/or enablers for the uptake of public health measures. Through the data exploration step we found that many factors can be seen as a barrier and as an enabler at the same time.\n\nThe plot incorporates only the top number of matches based on the semantic similarities between the raw matches and the corresponding policy measures.","a3466332":"Extract grammar matches about the implications of policy response measures:","c8dcbce7":"Download and load the tensorflow [Universal Sentence Encoder model](https:\/\/tfhub.dev\/google\/universal-sentence-encoder\/4) which encodes text into high-dimensional vectors that we will use for calculating semantic similarity.","a19970fa":"\n# Approach","5ece55d2":"# What has been published about ethical and social science considerations?\n[COVID-19 Open Research Dataset Challenge (CORD-19)](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge)\n\n\n**The end users of this Kaggle task are very likely to be decision-makers and policy-makers who need to better understand and navigate the ethical and social science considerations during the COVID-19 crisis response efforts. **\n\nWhile researching the very challenging and open ended [questions posed by this Kaggle task](https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks?taskId=563), we take an approach inspired by Qualitative Social Science. Therefore we conducted a few unstructured interviews with policy-makers, ethics counselors, and front-line medical practitioners in order to better understand what are their biggest needs and how our work could be most helpful for them. The details that experts shared with us informed the data exploration and modeling presented in this notebook.\n\nThe goal of this notebook is to link public health measures and social and ethical concerns for individual articles in the corpus. In order to extract such information, we will utilize a language grammar. In our case we've defined a grammar that aims to capture how the literature discusses **factors such as the enablers, barriers, and implications of different public health measures**. Our current grammar is in English but our language grammar approach could be evolved into covering other languages as well.\n\nUltimately, we hope that we are able to **highlight how others have been able to address social and ethical considerations during critical pandemic response efforts**. In this way we aim to inform the decision-makers, ethicists who might be part of a hospital ethics committee, or other individuals who need to navigate difficult decisions.\n\n## Contents\n\n1. [Findings](#Findings)\n2. [Approach](#Approach)\n3. **[Utilizing a Grammar Annotation Pattern Engine for Natural Language Processing](#Grammar_Engine)** - Utilize regular expressions and language grammars in order to extract information about the relationship between public health measures and the social and ethical concerns in the corpus.\n4. **[Calculating semantic similarieties using the Universal Sentence Encoder embeddings](#Semantic_Similarieties)** \n5. **[Saving the results into CSV](#Results)** - exporting CSV files with the paper excerpts which relate to the corresponding research questions  \n6. **[Data Visualization](#Visualization)** - Summarizing the results in a data visualization\n7. [Next Steps](#Next_Steps) - What we plan to work on next\n\n\n\n# Findings\n\nWe summarize our results in the data visualization below. It shows how many of the over 40,000 full text research articles in the dataset discuss the social science and ethical considerations related to the public health measures listed below. Hovering over the data shows you the names and authors of the top 3 most related research papers in each category. To inspect the full list of results see the next subsection or download the CSV files from the notebook output directory.","d9e9228d":"We load the documents in the corpus which mention any of the specific terms required by the grammar. In this way we also narrow down the documents we're working on instead of operating on the whole corpus. If we change the grammar we need to consider changing the grammar terms below in order to ensure we're not excluding documents which might contain matches.","cc279497":"# Results\n\nInspect the results for a concrete category of policy response measures","c9ddc18a":"Next we define helper functions which process the grammar (according to the pygrapenlp library):","d3a66bb4":"Finally we execute the grammar and save the results.","69d4cb52":"# Next_Steps\n\n## Further annotating the dataset with information about the social context discussed in each publication\nWhen answering each question in the Kaggle task, we think that it is critical to provide the context of the answer. For example:\u00a0(1) country or continent, (2) time, and (3) severity of the\u00a0pandemic which is being discussed in the paper. The reason for doing this is that if we don't\u00a0do it many of the answers\u00a0to the\u00a0questions might be misleading as they cannot easily generalize over a large number of dynamic factors. The answers of these questions are contextual and depend on many sociocultural factors. For example we need to consider that:\n* Cultural differences between countries around the world will influence what public health measures are implemented.\n* Many of the\u00a0papers in our dataset refer to previous pandemics. It will be important to compare between pandemics as the context has changed immensely.\n* The severity of the\u00a0pandemic will impact the measures being considered during the emergency response efforts.\n\nIn future work, we hope to be able to extract this contextual information and make it easier for decision-makers to understand it.\n\n## Improving the grammar\nThe language grammar NLU approach is very powerful and utilized by many chatbot systems. We hope to continue work on refining the grammar we use in order to capture the complex language structures which are often used in research articles.\n\n## Topic Modeling and building a knowledge graph\nWe aim to also explore a topic modelling approach in order to identify social and ethical concerns across the entire corpus. \nUltimately, we want to build a knowledge graph where concepts of public health measures will be associated with concepts relating to social and ethical concerns. Using a graph based approach could allow for extracting insights based on multiple articles altogether vs extracting grammar matches on individual articles. \n\n**We hope our research findings will meaningfully contribute to the COVID-19 outbreak response efforts by providing decision-makers and policy-makers with examples of how others have navigated the complex social and ethical challenges posed by this Kaggle task.**","04945ac3":"# Grammar_Engine\n## Defining the grammar\n\nWe've built the grammar using [Unitex\/GramLab](https:\/\/unitexgramlab.org\/) - an open source, cross-platform, multilingual, lexicon- and grammar-based corpus processing tool. We then convert to python using the https:\/\/github.com\/GrapeNLP\/grapenlp-core library.\n\nYou can reproduce our results by using the existing grammar file which is located in the \"input\/grammar\" directory or by creating your own grammar in Unitex.\n\nWe first setup the [GrapeNLP](https:\/\/github.com\/GrapeNLP\/pygrapenlp) library which we use in order to process the grammar graph file in python."}}