{"cell_type":{"bdf413d7":"code","39e1b1d1":"code","161530e4":"code","72b1a77a":"code","5719d444":"code","7ce2ad6c":"code","53a43b18":"code","5eb79cef":"code","fe9359be":"code","951c69ef":"code","f81b68ec":"code","a6910936":"code","ae47b4d6":"code","07dbf221":"code","e89033f0":"code","505f1c36":"code","f0874b35":"code","0afba8cc":"code","d6816bea":"code","a17af7e6":"code","61e78cea":"code","7d498d4a":"code","79c86118":"code","11561b46":"code","0a61877c":"code","3a3b10b5":"code","973eca32":"code","68c25510":"code","a485720a":"code","e474ef41":"code","1fbfdd26":"code","e50210de":"code","81f3a28b":"code","375e1ed9":"code","f22820ec":"code","0c5b1725":"code","c4348569":"code","4eb501c4":"code","d7f803f0":"code","424e29ab":"code","e259b260":"code","2505398c":"code","36205df6":"code","e6ce361b":"code","5375c3c5":"code","1cf0b80d":"code","db8909ee":"code","be88dc13":"code","4b15c8d1":"code","b63fac67":"code","e07839c6":"code","73d48d08":"code","d9a57553":"code","25151c0b":"code","0b570ddb":"code","d951a324":"code","d995e353":"code","b4f4ad40":"code","ee08ec10":"code","4f22150e":"code","28fc9389":"code","63831f6b":"code","909520f5":"code","82cc6e8e":"code","9a7b96ce":"code","367ec772":"code","6eadd195":"code","709f736e":"markdown","46100281":"markdown","5fe40132":"markdown","6afcaab9":"markdown","9fe93b8e":"markdown","1c0f1e95":"markdown","4dba4c66":"markdown","c26ce492":"markdown","a78d0b46":"markdown","44c3ded4":"markdown","6984bd81":"markdown","2086ee8a":"markdown","2f88ae4a":"markdown","ccc22db8":"markdown","e9de9b4b":"markdown","885d0a1b":"markdown","e8f21c95":"markdown","d51e09ba":"markdown","91ad6694":"markdown","c5cfca3f":"markdown","7f92b38c":"markdown","fbb6dea1":"markdown","0573ae11":"markdown","6f8a6559":"markdown"},"source":{"bdf413d7":"# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\nfrom sklearn.pipeline import make_pipeline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns #for better and easier plots\n%matplotlib inline\n\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","39e1b1d1":"train = pd.read_csv(\"..\/input\/train.csv\")#loading datasets\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])# logs respond better to skewed data as we have in this one,decided to change the column here at the beggining for clarity\n\nprint(train.shape, test.shape) #printing the shape","161530e4":"train.head() #head(), taking a look at the 5 first entries","72b1a77a":"#train.info()","5719d444":"#let's create a function to check for null values, calculate the percentage relative to the total size\n#only shows null values.\ndef missing_values_calculate(trainset): \n    nulldata = (trainset.isnull().sum() \/ len(trainset)) * 100\n    nulldata = nulldata.drop(nulldata[nulldata == 0].index).sort_values(ascending=False)\n    ratio_missing_data = pd.DataFrame({'Ratio' : nulldata})\n    return ratio_missing_data.head(30)","7ce2ad6c":"missing_values_calculate(train)","53a43b18":"#let's createa function that fill all the NaN values that means None, eg: NaN for pool means NO poll\ndef imputer_specific_categories(dataset, listfeatures):\n    data = dataset.copy()\n    for names in listfeatures:\n        data[names] = data[names].fillna(\"None\")\n    return data","5eb79cef":"features_meaning_none_value= [\"Alley\", \"PoolQC\", \"MiscFeature\",\"Fence\",\"MiscVal\",\"FireplaceQu\",\n                              'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','MSSubClass']\ntrain_drop = imputer_specific_categories(train, features_meaning_none_value)\ntest = imputer_specific_categories(test,features_meaning_none_value)","fe9359be":"missing_values_calculate(train_drop)#way better now, the highest value, now is lot frontage, 18%","951c69ef":"missing_values_calculate(test) #for the given test dataset, this is what we see...","f81b68ec":"train_drop['total_year_build_and_remod']= train_drop['YearBuilt']+train_drop['YearRemodAdd']\n\ntrain_drop['Total_sqr_are'] = (train_drop['BsmtFinSF1'] + train_drop['BsmtFinSF2'] +\n                                 train_drop['1stFlrSF'] + train_drop['2ndFlrSF'])\n\ntrain_drop['Total_Bathrooms'] = (train_drop['FullBath'] + (0.5 * train_drop['HalfBath']) +\n                               train_drop['BsmtFullBath'] + (0.5 * train_drop['BsmtHalfBath']))\n\ntrain_drop['Total_porch_sf'] = (train_drop['OpenPorchSF'] + train_drop['3SsnPorch'] +\n                              train_drop['EnclosedPorch'] + train_drop['ScreenPorch'] +\n                              train_drop['WoodDeckSF'])\n\ntest['total_year_build_and_remod']= test['YearBuilt']+test['YearRemodAdd']\n\ntest['Total_sqr_are'] = (test['BsmtFinSF1'] + test['BsmtFinSF2'] +\n                                 test['1stFlrSF'] + test['2ndFlrSF'])\n\ntest['Total_Bathrooms'] = (test['FullBath'] + (0.5 * test['HalfBath']) +\n                               test['BsmtFullBath'] + (0.5 * test['BsmtHalfBath']))\n\ntest['Total_porch_sf'] = (test['OpenPorchSF'] + test['3SsnPorch'] +\n                              test['EnclosedPorch'] + test['ScreenPorch'] +\n                              test['WoodDeckSF'])","a6910936":"def has_feature_function(data, column_name_features={}):\n    for column,name in column_name_features.items():\n        data[name] = data[column].apply(lambda x: 1 if x > 0 else 0)","ae47b4d6":"columns = {'PoolArea': 'haspool',\n           '2ndFlrSF': 'has2ndfloor',\n           'GarageArea': 'hasgarage',\n           'TotalBsmtSF': 'hasbasement',\n           'Fireplaces': 'hasfireplace',\n          }","07dbf221":"has_feature_function(train_drop, columns) #calling our function in our training set\nhas_feature_function(test, columns) #and calling our function for our testing set","e89033f0":"train_drop.shape #it has increased the number of features significantly","505f1c36":"corr = train_drop.corr() #Let's take a look at the pearson's corr, just to have an overall view of how the attributes influence the price.\n#using this correlation, we can have an idea of the linear correlation, positive and negative.\nax = sns.set(rc={'figure.figsize':(40,25)})\nplt.xticks(fontsize=20)\nplt.yticks(fontsize=20)\nsns.heatmap(corr[:30], annot=True).set_title('Pearsons Correlation Factors Heat Map', color='black', size='20')","f0874b35":"ax = sns.set(rc={'figure.figsize':(10,5)})\ntrain_drop[\"OverallQual\"].value_counts().plot.bar()","0afba8cc":"#let's use the seaborn plot to have a nice plot,showing the bell shape curve of this distribution.\nsns.countplot(train_drop[\"OverallQual\"]) #not surprisingly, average quality is more representative.","d6816bea":"# Plots the disribution of a variable colored by value of the target\n\ndef kde_target(var_name, df, corr_target, shade_or_not = False):\n    sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n    sns.set_style(\"darkgrid\")    \n    # Calculate the correlation coefficient between the new variable and the target\n    corr = df[corr_target].corr(df[var_name])\n    for var in df[var_name].sort_values().unique():            \n        sns.kdeplot(df.ix[df[var_name] == var, corr_target], label = '%s : %d' %(var_name,var),shade=shade_or_not)\n        \n    # label the plot\n    plt.xticks(fontsize=10)\n    plt.yticks(fontsize=10)\n    plt.xlabel('Distribution(%s)'%(corr_target), fontsize=10); plt.ylabel(corr_target,fontsize=10); plt.title('%s Distribution' % var_name,fontsize=10);\n    plt.legend(fontsize=10);\n    \n    # print out the correlation\n    print('The correlation between %s and the %s is %0.4f' % (var_name, corr_target, corr))","a17af7e6":"kde_target(\"OverallQual\", train_drop, \"SalePrice\", True)","61e78cea":"sns.countplot(y=train_drop[\"OverallCond\"]); plt.title('Overall Condition distribution', fontsize=15)\nplt.ylabel('Overall Condition', fontsize=15)\nplt.xlabel('Count', fontsize=15)","7d498d4a":"sns.catplot(x='OverallCond', y='SalePrice', data=train_drop)\nplt.title('Sale Price in relationship to Overall Condition')\nsns.catplot(x='OverallQual', y='SalePrice', data=train_drop)\nplt.title('Sale Price in relationship to Overall Quality')\nplt.tight_layout(h_pad = 2.5)","79c86118":"from_2000 = train_drop[train_drop['YearBuilt'] > 2000]\nax = sns.set(rc={'figure.figsize':(10,5)})\nsns.countplot(from_2000[\"YearBuilt\"])\nplt.subplot(1,1,1)\nsns.catplot(y='SalePrice', x='YearBuilt', data=from_2000); plt.xticks(rotation='vertical')","11561b46":"#let's check the relationship between sale price and living area, as it could be seen in the pearson's correlation\n#there's a positive correlation between sales price and living area.\nfig, ax = plt.subplots()\nax.scatter(x = train_drop['GrLivArea'], y = train_drop['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)","0a61877c":"#as can be seen above, only two ouliers that should be taken care of, lower prices relative to higher living area\n#we have, as well, two outliers properly placed at the top of the chart, let's remove the two bottom outliers\ntrain_drop.drop(train_drop[(train_drop[\"SalePrice\"] < 300000) & (train_drop[\"GrLivArea\"] > 4000)].index, inplace=True)","3a3b10b5":"train_labels = train_drop[\"SalePrice\"].copy() #creating the prediction target.","973eca32":"train_drop.drop([\"SalePrice\",'Utilities', 'Street', 'PoolQC','Id'], axis=1, inplace=True) #this is really important, separate our target Y from our X\ntest.drop(['Utilities', 'Street', 'PoolQC','Id'], axis=1, inplace=True)","68c25510":"num_attribs = train_drop.select_dtypes(exclude=['object']) #selecting all the numerical data to use in our function DataFrameSelector\ncat_attribs = train_drop.select_dtypes(exclude=['int64','float64']) #selecting non numerical data to use in our function DataFrameSelector","a485720a":"#this pipeline is gonna be use for numerical atributes and standard scaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,RobustScaler, MinMaxScaler\n\n\nnum_pipeline = Pipeline([\n        ('imputer', SimpleImputer(strategy=\"median\")),\n        #('std_scaler', StandardScaler()),\n        #('robust_scaler', RobustScaler()),\n        ('minmaxscaler', MinMaxScaler()),\n    ])","e474ef41":"# Inspired from stackoverflow.com\/questions\/25239958\n#this is gonna be used to imput categorical values\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass MostFrequentImputer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        self.most_frequent_ = pd.Series([X[c].value_counts().index[0] for c in X],\n                                        index=X.columns)\n        return self\n    def transform(self, X, y=None):\n        return X.fillna(self.most_frequent_)","1fbfdd26":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder #gonna try this one later\n\ncat_pipeline = Pipeline([\n        (\"imputer\", MostFrequentImputer()),\n        (\"cat_encoder\", OneHotEncoder()),\n    ])","e50210de":"from sklearn.compose import ColumnTransformer\n\n\nfull_pipeline = ColumnTransformer([\n        (\"num\", num_pipeline, list(num_attribs)),\n        (\"cat\", cat_pipeline, list(cat_attribs)),\n    ])","81f3a28b":"train_dummies = pd.get_dummies(train_drop)\ntest_dummies = pd.get_dummies(test)","375e1ed9":"train_dummies, test_dummies = train_dummies.align(test_dummies, join='inner', axis=1)","f22820ec":"print(train_dummies.shape, test_dummies.shape)","0c5b1725":"#let's create this function to make it easier and clean to fit the model and use the cross_val_score and obtain results\nimport time #implementing in this function the time spent on training the model\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, KFold\n\ndef modelfit(alg, dtrain, target, only_predict = False):\n    #Fit the algorithm on the data\n    time_start = time.perf_counter() #start counting the time\n    if not only_predict:\n        alg.fit(dtrain, target)\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain)\n    \n    kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n    \n    cv_score = cross_val_score(alg, dtrain,target, cv=kfolds, scoring='neg_mean_squared_error')\n    cv_score = np.sqrt(-cv_score)\n    \n    time_end = time.perf_counter()\n    \n    total_time = time_end-time_start\n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"RMSE :  {:.4f}\".format(np.sqrt(mean_squared_error(target, dtrain_predictions))))\n    print(\"CV Score : Mean -  %.4f | Std -  %.4f | Min -  %.4f | Max - %.4f\" % (np.mean(cv_score),np.std(cv_score),np.min(cv_score),np.max(cv_score)))\n    print(\"Amount of time spent during training the model and cross validation: %4.3f seconds\" % (total_time))","c4348569":"# Plot feature importance\ndef plot_feature_importance(model, df):\n    feature_importance = model.feature_importances_[:30]\n    # make importances relative to max importance\n    plt.figure(figsize=(20, 20)) #figure size\n    feature_importance = 100.0 * (feature_importance \/ feature_importance.max()) #making it a percentage relative to the max value\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + .5\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, df.columns[sorted_idx], fontsize=15) #used train_drop here to show the name of each feature instead of our train_prepared \n    plt.xlabel('Relative Importance', fontsize=20)\n    plt.ylabel('Features', fontsize=20)\n    plt.title('Variable Importance', fontsize=30)","4eb501c4":"train_prepared = full_pipeline.fit_transform(train_drop)\ntrain_dummies_prepared = num_pipeline.fit_transform(train_dummies)","d7f803f0":"#ok, nice, now that we have our data prepared, let's start testing some models\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nmodelfit(lin_reg, train_prepared, train_labels)","424e29ab":"from sklearn.linear_model import Lasso\n\nmodel_lasso =  Lasso()\nmodelfit(model_lasso, train_dummies_prepared, train_labels)","e259b260":"from sklearn.linear_model import ElasticNet\n\nmodel_elastic = ElasticNet(max_iter=1e7, l1_ratio=0.9)\nmodelfit(model_elastic, train_prepared, train_labels)","2505398c":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor(random_state=42)\nmodelfit(tree_reg, train_prepared, train_labels) ","36205df6":"from sklearn.kernel_ridge import KernelRidge\n\nmodel_kernel = KernelRidge(alpha=0.6, kernel='polynomial', degree=5, coef0=2.5) #the higher the degree, the polynomial degree, the better it fits the data, but it increases variance, and the model does not generalize well\nmodelfit(model_kernel, train_prepared, train_labels)","e6ce361b":"from sklearn.ensemble import RandomForestRegressor\n\nforest_reg = RandomForestRegressor(n_estimators=40, random_state=42, \n                                   min_samples_leaf=3, max_features=0.5, n_jobs=-1, oob_score=True)\nmodelfit(forest_reg, train_dummies_prepared, train_labels)","5375c3c5":"plot_feature_importance(forest_reg, train_dummies)","1cf0b80d":"from sklearn.ensemble import AdaBoostRegressor\n\ntree_ada = DecisionTreeRegressor(random_state = 42,max_depth = 4)\n\nada_reg = AdaBoostRegressor(\n    tree_reg, n_estimators=3000, random_state=42,learning_rate=0.009, loss='square')\nmodelfit(ada_reg, train_dummies_prepared, train_labels)","db8909ee":"plot_feature_importance(ada_reg, train_dummies)","be88dc13":"from sklearn.ensemble import GradientBoostingRegressor\n\nparams = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n          'learning_rate': 0.01, 'loss': 'ls'}\n\nparams_1 = {'n_estimators': 3000, 'learning_rate': 0.05, 'max_depth' : 4,\n            'max_features': 'sqrt', 'min_samples_leaf': 15, 'min_samples_split': 10, \n            'loss': 'huber', 'random_state': 42}\n\ntrain_dummies_prepared = num_pipeline.fit_transform(train_dummies)\n\ngdb_model = GradientBoostingRegressor(**params_1)\nmodelfit(gdb_model, train_dummies_prepared, train_labels)","4b15c8d1":"plot_feature_importance(gdb_model, train_dummies)","b63fac67":"#this function is gonna be used when it has been estimated the best features, eg: using random forest, in our case Gradient Boosting\n## then we would want to especify only those features when we train our model.\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\ndef indices_of_top_k(arr, k):\n    return np.sort(np.argpartition(np.array(arr), -k)[-k:])\n\nclass TopFeatureSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, feature_importances, k):\n        self.feature_importances = feature_importances\n        self.k = k\n    def fit(self, X, y=None):\n        self.feature_indices_ = indices_of_top_k(self.feature_importances, self.k)\n        return self\n    def transform(self, X):\n        return X[:, self.feature_indices_]","e07839c6":"import xgboost as xgb\n\n    \nparams = {'objective': 'reg:linear', \n              'eta': 0.01, \n              'max_depth': 6, \n              'subsample': 0.6, \n              'colsample_bytree': 0.7,  \n              'eval_metric': 'rmse', \n              'seed': 42, \n              'silent': True,\n    }\n\nmodel_xgb = xgb.XGBRegressor(**params,n_estimators=3000,learning_rate=0.05,verbose_eval=True)","73d48d08":"#train_prepared_feature_importances = preparation_and_feature_selection_pipeline.fit_transform(train_drop)\n#in case you wanna test with only some features, the ones you found to be more important, this is why you have this pipeline","d9a57553":"from datetime import datetime\n\n\nfold_val_pred = []\nfold_err = []\nfold_importance = []\nrecord = dict()\nkfold = list(KFold(10,shuffle = True, random_state = 42).split(train_dummies))\n\ndef xgb_model(train, params, train_label, fold, verbose, pipeline):\n\n    \n    for i, (trn, val) in enumerate(fold) :\n        print(i+1, \"fold.    RMSE\")\n    \n        trn_x = train.loc[trn, :]\n        trn_y = train_label[trn]\n        val_x = train.loc[val, :]\n        val_y = train_label[val]\n    \n        fold_val_pred = []\n    \n        start = datetime.now()\n        model = xgb.train(params\n                      , xgb.DMatrix(trn_x, trn_y)\n                      , 3500\n                      , [(xgb.DMatrix(trn_x, trn_y), 'train'), (xgb.DMatrix(val_x, val_y), 'valid')]\n                      , verbose_eval=verbose\n                      , early_stopping_rounds=500\n                      , callbacks = [xgb.callback.record_evaluation(record)])\n        best_idx = np.argmin(np.array(record['valid']['rmse']))\n\n        val_pred = model.predict(xgb.DMatrix(val_x), ntree_limit=model.best_ntree_limit)\n                \n        fold_val_pred.append(val_pred*0.2)\n        fold_err.append(record['valid_0']['rmse'][best_idx])\n        fold_importance.append(model.feature_importance('gain'))\n        \n        print(\"xgb model.\", \"{0:.5f}\".format(result['error']), '(' + str(int((datetime.now()-start).seconds\/60)) + 'm)')","25151c0b":"modelfit(model_xgb, train_dummies, train_labels)","0b570ddb":"plot_feature_importance(model_xgb, train_dummies)","d951a324":"#modelfit(xgb_model, train_prepared_feature_importances, train_labels)","d995e353":"from sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\n\n\npipe = Pipeline([\n    ('pca', PCA(5)),\n    ('xgb', xgb.XGBClassifier())\n])\n\nparam_grid = {\n    'pca__n_components': [3, 5, 7, 10,15],\n    'xgb__n_estimators': [500,1000,2000,3500],\n    'xgb__learning_rate': [0.09,0.1,0.05,0.001]\n}\n#grid_xgb = GridSearchCV(pipe, param_grid, scoring='neg_mean_absolute_error', verbose=10)\n#grid_xgb.fit(train_prepared, train_labels)\n#print(grid_xgb.best_params_)","b4f4ad40":"#pipe.fit(train_prepared, train_labels)","ee08ec10":"#modelfit(pipe, train_prepared, train_labels, True)","4f22150e":"import lightgbm as lgb\n\nlgb_model = lgb.LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.005, \n                                       n_estimators=10000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\n\nmodelfit(lgb_model, train_dummies, train_labels)","28fc9389":"plot_feature_importance(lgb_model, train_dummies)","63831f6b":"### THIS CODE HAS BEEN USED BEFORE I KNEW ABOUT THE STACKING REGRESSOR ###\n#this class is gonna be used to average the results of different models and get the best result out of it\nfrom sklearn.base import RegressorMixin\nfrom sklearn.base import clone\n\n\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","909520f5":"from mlxtend.regressor import StackingCVRegressor\nmodels_total = (model_kernel, model_lasso, model_elastic, gdb_model, lgb_model)\nmodel_ada_gbd = (ada_reg,gdb_model)\nstack_gen = StackingCVRegressor(regressors= models_total,\n                                meta_regressor=model_xgb,\n                                use_features_in_secondary=True)","82cc6e8e":"#running again some models, now using only some features: \nmodelfit(stack_gen, train_dummies_prepared, train_labels)","9a7b96ce":"test_dummies_prepared = num_pipeline.fit_transform(test_dummies)\n#test_prepared = preparation_and_feature_selection_pipeline.fit_transform(test) #if you want to train with only specific features","367ec772":"prediction_some_regressors = stack_gen.predict(test_dummies_prepared)","6eadd195":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['SalePrice'] = np.expm1(prediction_some_regressors)\nsub.to_csv(\"stack_regression.csv\", index=False)","709f736e":"### creating boolean features that tell us whether a given house has some features\n#### gonna create a function to help us doing so and to avoid repeating code","46100281":"* Don't forget to drop your labels that have been previously copied to our train_labels variable.\n##### Plus, I am gonna drop some irrelevant columns that doesn't seem to add value to our models","5fe40132":"**I would expect a better score for the Gradient Boosting, still, better than Ada in this case, let's try gradient boosting on steroids XGBoost**","6afcaab9":"### Averaging Models:","9fe93b8e":"#### Kernel Ridge has given us the best result so far, it's overfitting, tho\n* obviously, using a higher degree polynomial would make this model fit even better, but would cause a high variance(overfitting) ","1c0f1e95":"**And why not, let's try another gradient boosting, this time, the light gradient boosting**","4dba4c66":"Well, gonna try some other parameters later","c26ce492":"Ok, XGBoost is the best one so far, let's stack those models together and see what we got","a78d0b46":"Well, Not bad, let's see what lasso gives us","44c3ded4":"* Quite a large picture, but we can(barely) see the correlations","6984bd81":"* Now, gonna implement the code with the stacking regressor with a meta regressor lgbm","2086ee8a":"Before that, plotting the relative weight of each feature would be informative","2f88ae4a":"**Cool, we've created our pipelines to help us imput and transforme our data, now let's write a function to help us fit our data and cross validate it.","ccc22db8":"Next, gonna use another ensemble algorithm, Gradient Boosting","e9de9b4b":"Now let's use our preprocess pipeline to process the data and scale it, As I have created dummies, I won't be using the full pipeline...only the numerical one, but gonna compare to see which one is better","885d0a1b":"**Decision trees with normal parameters is expected to be overfitting, just running the models to have an overall view of how the data is fitting**","e8f21c95":"**Instead of calling it Ytrain, gonna call it labels here, next line I am gonna create our labels to use in our model**","d51e09ba":"## As far as I know, we don't have a voting classifier for regression, so, in order to stack regressor together, we gotta create our own class and stack them together, this is what I am gonna do next\nActually we have, and I am gonna use it...\nStackingCVRegressor\n","91ad6694":"**That's impressive, isn't it? XBoost has given us the lowest Root Mean Squared Error(RMSE) so far.**","c5cfca3f":"##### The random forest is fitting the data rather well, but it's overusing one feature....","7f92b38c":"***","fbb6dea1":"**Gonna create a pipeline for preparing and selecting features, in case I want to do so.**\n\n*but first we need to create a class to use in our pipeline*\n","0573ae11":"**Ada boosting is fitting the data way better than the random forest, but see how it is overfitting actually, cross validation is not giving a good score...**","6f8a6559":"## Feature engineering\n### Some features can be created to help our model"}}