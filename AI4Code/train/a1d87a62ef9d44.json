{"cell_type":{"6752a40b":"code","9ee4f214":"code","bb390755":"code","2d0234f1":"code","99e21f09":"code","44bc471b":"code","218c0af7":"code","2e1d8fd6":"code","a1180f21":"code","8b7669b2":"code","b3900c5e":"code","5230b133":"code","84cab84f":"code","c424154c":"code","b4e7182c":"code","1aab2cab":"code","6a6ba203":"code","3db488af":"code","ce15b96d":"code","c5d44d9b":"code","fc67eb59":"code","1fae38a7":"code","023b9f56":"code","bdf4166d":"code","e35e4c74":"code","d45217ae":"code","a2d6ce0a":"code","5c2708f0":"code","c13045aa":"code","7f6f0ba6":"code","f3cf7819":"code","bf3e25ce":"code","bfb386aa":"markdown","8970bffa":"markdown","af678af6":"markdown","cdb107a1":"markdown","7bc1c858":"markdown","5e2af4c6":"markdown","093225fa":"markdown"},"source":{"6752a40b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve, auc\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9ee4f214":"data= pd.read_csv(\"..\/input\/GSE58606_data.csv\")","bb390755":"data.head()","2d0234f1":"data=data.dropna(axis=0, how='any')\ndata.shape","99e21f09":"assert data.target.notnull().all()\n#returns nothing it means we don't have any nan values.","44bc471b":"data.groupby(\"target_actual\").count()","218c0af7":"data.groupby(\"target\").count()","2e1d8fd6":"correlations= data.corr()\ncorrelations = correlations[\"target\"].sort_values(ascending=False)","a1180f21":"corr_many= correlations[correlations >0.5]\ncorr_many","8b7669b2":"corr_few= correlations[(correlations >0.10) & (correlations < 0.11)]\ncorr_few","b3900c5e":"features= correlations.index[0:10]\nf,ax= plt.subplots(figsize=(10,10))\nsns.heatmap(data.loc[:,features].corr(), annot=True, linewidths=.5, fmt='.1f', ax=ax)\nplt.show()","5230b133":"#Data for Analysis\nfeature =data[data.columns[0:1926]] #independent columns\ntarget=data.iloc[:,1926] #target column i.e success\n\nmodel = ExtraTreesClassifier()\nmodel.fit(feature,target)\nprint(model.feature_importances_) #use inbuilt class \n\nfeat_importances = pd.Series(model.feature_importances_, index=feature.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.title(\"top 10 most important features in data\")\nplt.show()","84cab84f":"## Normal Breast Tissue\nnormal= data[data.target==0]\nnormal","c424154c":"normal.describe()","b4e7182c":"### the highest correlation\nnormal[\"46361 : hsa-miR-1278\"].hist()","1aab2cab":"## the lowest correlation\nnormal[\"168626 : hsa-miR-4662a-5p\"].hist()","6a6ba203":"## Cancer data\ncancer= data[data.target==1]\ncancer.describe()","3db488af":"cancer[\"46361 : hsa-miR-1278\"].hist()","ce15b96d":"cancer[\"168626 : hsa-miR-4662a-5p\"].hist()","c5d44d9b":"### Normalization\nX =data[data.columns[0:1926]] #independent columns\nY=data.iloc[:,1926] #target column i.e target\nX= (X - np.min(X))\/(np.max(X) - np.min(X))","fc67eb59":"#Train and Test Splitting\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=1)\n\n#Model and Training\nclf = svm.SVC()\ny_pred=clf.fit(X_train,Y_train).predict(X_test)\n\nprint(\"SVM score:\", clf.score(X_test,Y_test))","1fae38a7":"X_test.shape","023b9f56":"#Model Evaluation\nconf_mat = confusion_matrix(Y_test,y_pred)\nacc = accuracy_score(Y_test,y_pred)\nprecision = precision_score(Y_test,y_pred)\nrecall = recall_score(Y_test,y_pred)\nf1= f1_score(Y_test,y_pred)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b',\nlabel='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","bdf4166d":"#Print Results\nprint('Confusion Matrix is :')\nprint(conf_mat)\nprint('\\nAccuracy is :')\nprint(acc)\nprint('\\nPrecision is :')\nprint(precision)\nprint('\\nRecall is: ')\nprint(recall)\nprint('\\nF-score is: ')\nprint(f1)","e35e4c74":"#Train and Test Splitting\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=1)\n\n#Model and Training\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train,Y_train).predict(X_test)\n\n#Model Evaluation\nconf_mat = confusion_matrix(Y_test,y_pred)\nacc = accuracy_score(Y_test,y_pred)\nprecision = precision_score(Y_test,y_pred)\nrecall = recall_score(Y_test,y_pred)\nf1= f1_score(Y_test,y_pred)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b',\nlabel='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","d45217ae":"#Print Results\nprint('Confusion Matrix is :')\nprint(conf_mat)\nprint('\\nAccuracy is :')\nprint(acc)\nprint('\\nPrecision is :')\nprint(precision)\nprint('\\nRecall is: ')\nprint(recall)\nprint('\\nF-score is: ')\nprint(f1)","a2d6ce0a":"print(\"Naive Bayes score:\", gnb.score(X_test,Y_test))","5c2708f0":"#Train and Test Splitting\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=1)\n\n#Model and Training\nknn = KNeighborsClassifier(n_neighbors=5)\ny_pred = knn.fit(X_train, Y_train).predict(X_test)\n\n#Model Evaluation\nconf_mat = confusion_matrix(Y_test,y_pred)\nacc = accuracy_score(Y_test,y_pred)\nprecision = precision_score(Y_test,y_pred)\nrecall = recall_score(Y_test,y_pred)\nf1= f1_score(Y_test,y_pred)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b',\nlabel='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","c13045aa":"#Print Results\nprint('Confusion Matrix is :')\nprint(conf_mat)\nprint('\\nAccuracy is :')\nprint(acc)\nprint('\\nPrecision is :')\nprint(precision)\nprint('\\nRecall is: ')\nprint(recall)\nprint('\\nF-score is: ')\nprint(f1)","7f6f0ba6":"print(\"KNN score:\", knn.score(X_test,Y_test))","f3cf7819":"#Train and Test Splitting\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=1)\n\n#Model and Training\nknn = KNeighborsClassifier(n_neighbors=3)\ny_pred = knn.fit(X_train, Y_train).predict(X_test)\n\n#Model Evaluation\nconf_mat = confusion_matrix(Y_test,y_pred)\nacc = accuracy_score(Y_test,y_pred)\nprecision = precision_score(Y_test,y_pred)\nrecall = recall_score(Y_test,y_pred)\nf1= f1_score(Y_test,y_pred)\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(Y_test, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b',\nlabel='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","bf3e25ce":"#Print Results\nprint('Confusion Matrix is :')\nprint(conf_mat)\nprint('\\nAccuracy is :')\nprint(acc)\nprint('\\nPrecision is :')\nprint(precision)\nprint('\\nRecall is: ')\nprint(recall)\nprint('\\nF-score is: ')\nprint(f1)","bfb386aa":"## Modelling with SVM","8970bffa":"### Feature Selection:","af678af6":"## Modelling with KNN","cdb107a1":"## Submission","7bc1c858":"## Compare to Normal and Cancer Tissues","5e2af4c6":"* Naive Bayes and KNN algorithms have perfect score for this dataset. \ud83c\udf89\nBut if neighbors of k-value are equal to  number which is except 5, The accuracy of the algorithm is dropping. So the best neighbor is equal to 5.\n* SVM is also good. But when we compare with KNN and Naive Bayes, the accuracy is lower as you can see.","093225fa":"## Modelling with Naive Bayes"}}