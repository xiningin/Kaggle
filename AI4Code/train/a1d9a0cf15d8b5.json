{"cell_type":{"8afed4bd":"code","b8f7782d":"code","8302d4c9":"code","0e2842b5":"code","1a0b23df":"code","a8979ac5":"code","2d3b59b5":"code","6bdadb98":"code","0295f08d":"code","791ef881":"code","3c91b34c":"code","812f6db8":"code","0b756ee5":"markdown","c882769e":"markdown","2fedc23d":"markdown"},"source":{"8afed4bd":"import tensorflow.compat.v1 as tf\nimport gym\nimport numpy as np\nimport warnings","b8f7782d":"import matplotlib.pyplot as plt\n%matplotlib inline\nfrom IPython import display","8302d4c9":"tf.set_random_seed(1)\nnp.random.seed(1)\ntf.disable_eager_execution()\nwarnings.filterwarnings('ignore')","0e2842b5":"bathc_size = 32\nlearning_rate = 0.01\nepsilon = 0.9\ngamma = 0.9\ntarget_replace_iter = 100\nmemory_capacity = 2000\nmemory_counter = 0\nlearning_step_counter = 0\nenv = gym.make('CartPole-v0')\nenv = env.unwrapped\nn_actions = env.action_space.n\nn_states = env.observation_space.shape[0]\nmemory = np.zeros((memory_capacity, n_states * 2 + 2))","1a0b23df":"tf_s = tf.placeholder(tf.float32, [None, n_states])\ntf_a = tf.placeholder(tf.int32, [None, ])\ntf_r = tf.placeholder(tf.float32, [None, ])\ntf_s_ = tf.placeholder(tf.float32, [None, n_states])","a8979ac5":"with tf.variable_scope('q'):\n    l_eval = tf.layers.dense(tf_s, 10, tf.nn.relu, kernel_initializer = tf.random_normal_initializer(0, 0.1))\n    q = tf.layers.dense(l_eval, n_actions, trainable=False)\n","2d3b59b5":"with tf.variable_scope('q_next'):\n    l_target = tf.layers.dense(tf_s_, 10, tf.nn.relu, trainable= False)\n    q_next = tf.layers.dense(l_target, n_actions, trainable=False)\n    \n    ","6bdadb98":"q_target = tf_r + gamma  * tf.reduce_max(q_next, axis = 1)\na_indices = tf.stack([tf.range(tf.shape(tf_a)[0], dtype = tf.int32), tf_a], axis = 1)\nq_wrt_a = tf.gather_nd(params = q, indices = a_indices)\n\nloss = tf.reduce_mean(tf.squared_difference(q_target, q_wrt_a))\ntrain_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n\n\n","0295f08d":"def choose_action(s):\n    s = s[np.newaxis, :]\n    if np.random.uniform() < epsilon:\n        actions_value = sess.run(q, feed_dict = {tf_s:s})\n        action = np.argmax(actions_a_indicesvalue)\n    else:\n        action = np.random.randint(0, n_actions)\n  \n    return action\n","791ef881":"def store_transition(s, a, r, s_):\n    global memory_counter\n    transition = np.hstack((s, [a, r], s_))\n    index = memory_counter % memory_capacity\n    memory[index, :] = transition\n    memory_counter+=1\n","3c91b34c":"def learn():\n    # Update the target network\n    global learning_step_counter\n    \n    if learning_step_counter % target_replace_iter == 0:\n        t_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'q_next')\n        e_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope = 'q')\n        sess.run([tf.assign(t, e) for t, e, in zip(t_params, e_params)])\n        \n    learning_step_counter += 1\n    \n    sample_index = np.random.choice(memory_capacity, bathc_size)\n    b_memory = memory[sample_index, :]\n    b_s= b_memory[:, n_states]\n    b_a = b_memory[:, n_states].astype(int)\n    b_r = b_memory[:, n_states +1]\n    b_s_ = b_memory[:, -n_states:]\n    \n    sess.run(train_op, {tf_s:b_s, tf_a:b_a, tf_r:b_r,tf_s_:b_s_})\n    ","812f6db8":"print('Collecting Experience ....')\n\nfor i_episode in range(400):\n    s = env.reset()\n    ep_r = 0\n    \n    while True:\n        env.render()\n        a = choice_action(s)\n        \n        s_, r, done, info = env.step(a)\n        \n        x, x_dot, theta, theta_dot = s_\n        \n        r1 = (env.x_threshold - abs(x)) \/ env.x_threshold - 0.8\n        r2 = (env.theta_threshold_radians - abs(theta)) \/ env.theta_threshold_radians - 0.5\n        r = r1 + r2\n        \n        store_transition(s, a, r, s_)\n        ep_r += r\n        \n        if memory_counter > memory_capacity:\n            learn()\n            if done:\n                print('Ep: ', i_episode, '|Ep_r: ', round(ep_r, 2))\n                \n        if done:\n            break\n            \n        \n    display.clear_output(wait=True)\n    display.display(plt.gcf())","0b756ee5":"# Evaluation of Networks","c882769e":"# Target Network ( Not to train )","2fedc23d":"# Create HyperParameter"}}