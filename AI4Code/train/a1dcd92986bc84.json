{"cell_type":{"11c1e67f":"code","f5eb5cc6":"code","1d518e5b":"code","e83731dd":"code","34b2b1d0":"code","e1e088fc":"code","1ac7b8fb":"code","e44c51d2":"code","ef84bb25":"code","9cc78818":"code","b5abce31":"code","1946f91d":"code","132057e3":"code","750ae236":"code","d0ff000a":"code","8ef9cf5e":"code","2d1ba21e":"code","87ddfcee":"markdown","c5645c1c":"markdown","23ca255b":"markdown","defbaa1a":"markdown","e35d7309":"markdown","d4a5400e":"markdown","965ad849":"markdown","ca2c8a2f":"markdown","253c62e2":"markdown","ca9e89d9":"markdown","73264841":"markdown","9b0f2a51":"markdown","1d8105de":"markdown","9fe39ae2":"markdown","d59f8df9":"markdown","d6b13b1b":"markdown","4c1f41e7":"markdown","ad51a72c":"markdown","6483a017":"markdown","b9f2dd96":"markdown","d4cdfa3f":"markdown"},"source":{"11c1e67f":"!pip install -q -U tensorflow-hub tensorflow-text tensorflow-addons\n!mkdir \/kaggle\/tmp\/\n!cd \/kaggle\/tmp\/","f5eb5cc6":"import os\nimport collections\nimport json\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_hub as hub\nimport tensorflow_text as text\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm\n\n# Suppressing tf.hub warnings\ntf.get_logger().setLevel(\"ERROR\")","1d518e5b":"root_dir = \"\/kaggle\/tmp\/datasets\"\nannotations_dir = os.path.join(root_dir, \"annotations\")\nimages_dir = os.path.join(root_dir, \"train2014\")\ntfrecords_dir = os.path.join(root_dir, \"tfrecords\")\nannotation_file = os.path.join(annotations_dir, \"captions_train2014.json\")\n\n# Download caption annotation files\nif not os.path.exists(annotations_dir):\n    annotation_zip = tf.keras.utils.get_file(\n        \"captions.zip\",\n        cache_dir=os.path.abspath(\"\/kaggle\/tmp\/\"),\n        origin=\"http:\/\/images.cocodataset.org\/annotations\/annotations_trainval2014.zip\",\n        extract=True,\n    )\n    os.remove(annotation_zip)\n\n# Download image files\nif not os.path.exists(images_dir):\n    image_zip = tf.keras.utils.get_file(\n        \"train2014.zip\",\n        cache_dir=os.path.abspath(\"\/kaggle\/tmp\/\"),\n        origin=\"http:\/\/images.cocodataset.org\/zips\/train2014.zip\",\n        extract=True,\n    )\n    os.remove(image_zip)\n\nprint(\"Dataset is downloaded and extracted successfully.\")\n\nwith open(annotation_file, \"r\") as f:\n    annotations = json.load(f)[\"annotations\"]\n\nimage_path_to_caption = collections.defaultdict(list)\nfor element in annotations:\n    caption = f\"{element['caption'].lower().rstrip('.')}\"\n    image_path = images_dir + \"\/COCO_train2014_\" + \"%012d.jpg\" % (element[\"image_id\"])\n    image_path_to_caption[image_path].append(caption)\n\nimage_paths = list(image_path_to_caption.keys())\nprint(f\"Number of images: {len(image_paths)}\")","e83731dd":"train_size = 30000\nvalid_size = 5000\ncaptions_per_image = 2\nimages_per_file = 2000\n\ntrain_image_paths = image_paths[:train_size]\nnum_train_files = int(np.ceil(train_size \/ images_per_file))\ntrain_files_prefix = os.path.join(tfrecords_dir, \"train\")\n\nvalid_image_paths = image_paths[-valid_size:]\nnum_valid_files = int(np.ceil(valid_size \/ images_per_file))\nvalid_files_prefix = os.path.join(tfrecords_dir, \"valid\")\n\ntf.io.gfile.makedirs(tfrecords_dir)\n\n\ndef bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef create_example(image_path, caption):\n    feature = {\n        \"caption\": bytes_feature(caption.encode()),\n        \"raw_image\": bytes_feature(tf.io.read_file(image_path).numpy()),\n    }\n    return tf.train.Example(features=tf.train.Features(feature=feature))\n\n\ndef write_tfrecords(file_name, image_paths):\n    caption_list = []\n    image_path_list = []\n    for image_path in image_paths:\n        captions = image_path_to_caption[image_path][:captions_per_image]\n        caption_list.extend(captions)\n        image_path_list.extend([image_path] * len(captions))\n\n    with tf.io.TFRecordWriter(file_name) as writer:\n        for example_idx in range(len(image_path_list)):\n            example = create_example(\n                image_path_list[example_idx], caption_list[example_idx]\n            )\n            writer.write(example.SerializeToString())\n    return example_idx + 1\n\n\ndef write_data(image_paths, num_files, files_prefix):\n    example_counter = 0\n    for file_idx in tqdm(range(num_files)):\n        file_name = files_prefix + \"-%02d.tfrecord\" % (file_idx)\n        start_idx = images_per_file * file_idx\n        end_idx = start_idx + images_per_file\n        example_counter += write_tfrecords(file_name, image_paths[start_idx:end_idx])\n    return example_counter\n\n\ntrain_example_count = write_data(train_image_paths, num_train_files, train_files_prefix)\nprint(f\"{train_example_count} training examples were written to tfrecord files.\")\n\nvalid_example_count = write_data(valid_image_paths, num_valid_files, valid_files_prefix)\nprint(f\"{valid_example_count} evaluation examples were written to tfrecord files.\")","34b2b1d0":"\nfeature_description = {\n    \"caption\": tf.io.FixedLenFeature([], tf.string),\n    \"raw_image\": tf.io.FixedLenFeature([], tf.string),\n}\n\n\ndef read_example(example):\n    features = tf.io.parse_single_example(example, feature_description)\n    raw_image = features.pop(\"raw_image\")\n    features[\"image\"] = tf.image.resize(\n        tf.image.decode_jpeg(raw_image, channels=3), size=(299, 299)\n    )\n    return features\n\n\ndef get_dataset(file_pattern, batch_size):\n\n    return (\n        tf.data.TFRecordDataset(tf.data.Dataset.list_files(file_pattern))\n        .map(\n            read_example,\n            num_parallel_calls=tf.data.experimental.AUTOTUNE,\n            deterministic=False,\n        )\n        .shuffle(batch_size * 10)\n        .prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n        .batch(batch_size)\n    )\n","e1e088fc":"\ndef project_embeddings(\n    embeddings, num_projection_layers, projection_dims, dropout_rate\n):\n    projected_embeddings = layers.Dense(units=projection_dims)(embeddings)\n    for _ in range(num_projection_layers):\n        x = tf.nn.gelu(projected_embeddings)\n        x = layers.Dense(projection_dims)(x)\n        x = layers.Dropout(dropout_rate)(x)\n        x = layers.Add()([projected_embeddings, x])\n        projected_embeddings = layers.LayerNormalization()(x)\n    return projected_embeddings\n","1ac7b8fb":"\ndef create_vision_encoder(\n    num_projection_layers, projection_dims, dropout_rate, trainable=False\n):\n    # Load the pre-trained Xception model to be used as the base encoder.\n    xception = keras.applications.Xception(\n        include_top=False, weights=\"imagenet\", pooling=\"avg\"\n    )\n    # Set the trainability of the base encoder.\n    for layer in xception.layers:\n        layer.trainable = trainable\n    # Receive the images as inputs.\n    inputs = layers.Input(shape=(299, 299, 3), name=\"image_input\")\n    # Preprocess the input image.\n    xception_input = tf.keras.applications.xception.preprocess_input(inputs)\n    # Generate the embeddings for the images using the xception model.\n    embeddings = xception(xception_input)\n    # Project the embeddings produced by the model.\n    outputs = project_embeddings(\n        embeddings, num_projection_layers, projection_dims, dropout_rate\n    )\n    # Create the vision encoder model.\n    return keras.Model(inputs, outputs, name=\"vision_encoder\")\n","e44c51d2":"\ndef create_text_encoder(\n    num_projection_layers, projection_dims, dropout_rate, trainable=False\n):\n    # Load the BERT preprocessing module.\n    preprocess = hub.KerasLayer(\n        \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/2\",\n        name=\"text_preprocessing\",\n    )\n    # Load the pre-trained BERT model to be used as the base encoder.\n    bert = hub.KerasLayer(\n        \"https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-4_H-512_A-8\/1\",\n        \"bert\",\n    )\n    # Set the trainability of the base encoder.\n    bert.trainable = trainable\n    # Receive the text as inputs.\n    inputs = layers.Input(shape=(), dtype=tf.string, name=\"text_input\")\n    # Preprocess the text.\n    bert_inputs = preprocess(inputs)\n    # Generate embeddings for the preprocessed text using the BERT model.\n    embeddings = bert(bert_inputs)[\"pooled_output\"]\n    # Project the embeddings produced by the model.\n    outputs = project_embeddings(\n        embeddings, num_projection_layers, projection_dims, dropout_rate\n    )\n    # Create the text encoder model.\n    return keras.Model(inputs, outputs, name=\"text_encoder\")\n","ef84bb25":"\nclass DualEncoder(keras.Model):\n    def __init__(self, text_encoder, image_encoder, temperature=1.0, **kwargs):\n        super(DualEncoder, self).__init__(**kwargs)\n        self.text_encoder = text_encoder\n        self.image_encoder = image_encoder\n        self.temperature = temperature\n        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n\n    @property\n    def metrics(self):\n        return [self.loss_tracker]\n\n    def call(self, features, training=False):\n        # Place each encoder on a separate GPU (if available).\n        # TF will fallback on available devices if there are fewer than 2 GPUs.\n        with tf.device(\"\/gpu:0\"):\n            # Get the embeddings for the captions.\n            caption_embeddings = text_encoder(features[\"caption\"], training=training)\n        with tf.device(\"\/gpu:1\"):\n            # Get the embeddings for the images.\n            image_embeddings = vision_encoder(features[\"image\"], training=training)\n        return caption_embeddings, image_embeddings\n\n    def compute_loss(self, caption_embeddings, image_embeddings):\n        # logits[i][j] is the dot_similarity(caption_i, image_j).\n        logits = (\n            tf.matmul(caption_embeddings, image_embeddings, transpose_b=True)\n            \/ self.temperature\n        )\n        # images_similarity[i][j] is the dot_similarity(image_i, image_j).\n        images_similarity = tf.matmul(\n            image_embeddings, image_embeddings, transpose_b=True\n        )\n        # captions_similarity[i][j] is the dot_similarity(caption_i, caption_j).\n        captions_similarity = tf.matmul(\n            caption_embeddings, caption_embeddings, transpose_b=True\n        )\n        # targets[i][j] = avarage dot_similarity(caption_i, caption_j) and dot_similarity(image_i, image_j).\n        targets = keras.activations.softmax(\n            (captions_similarity + images_similarity) \/ (2 * self.temperature)\n        )\n        # Compute the loss for the captions using crossentropy\n        captions_loss = keras.losses.categorical_crossentropy(\n            y_true=targets, y_pred=logits, from_logits=True\n        )\n        # Compute the loss for the images using crossentropy\n        images_loss = keras.losses.categorical_crossentropy(\n            y_true=tf.transpose(targets), y_pred=tf.transpose(logits), from_logits=True\n        )\n        # Return the mean of the loss over the batch.\n        return (captions_loss + images_loss) \/ 2\n\n    def train_step(self, features):\n        with tf.GradientTape() as tape:\n            # Forward pass\n            caption_embeddings, image_embeddings = self(features, training=True)\n            loss = self.compute_loss(caption_embeddings, image_embeddings)\n        # Backward pass\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        # Monitor loss\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n\n    def test_step(self, features):\n        caption_embeddings, image_embeddings = self(features, training=False)\n        loss = self.compute_loss(caption_embeddings, image_embeddings)\n        self.loss_tracker.update_state(loss)\n        return {\"loss\": self.loss_tracker.result()}\n","9cc78818":"num_epochs = 5  # In practice, train for at least 30 epochs\nbatch_size = 256\n\nvision_encoder = create_vision_encoder(\n    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n)\ntext_encoder = create_text_encoder(\n    num_projection_layers=1, projection_dims=256, dropout_rate=0.1\n)\ndual_encoder = DualEncoder(text_encoder, vision_encoder, temperature=0.05)\ndual_encoder.compile(\n    optimizer=tfa.optimizers.AdamW(learning_rate=0.001, weight_decay=0.001)\n)","b5abce31":"print(f\"Number of GPUs: {len(tf.config.list_physical_devices('GPU'))}\")\nprint(f\"Number of examples (caption-image pairs): {train_example_count}\")\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Steps per epoch: {int(np.ceil(train_example_count \/ batch_size))}\")\ntrain_dataset = get_dataset(os.path.join(tfrecords_dir, \"train-*.tfrecord\"), batch_size)\nvalid_dataset = get_dataset(os.path.join(tfrecords_dir, \"valid-*.tfrecord\"), batch_size)\n# Create a learning rate scheduler callback.\nreduce_lr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", factor=0.2, patience=3\n)\n# Create an early stopping callback.\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", patience=5, restore_best_weights=True\n)\nhistory = dual_encoder.fit(\n    train_dataset,\n    epochs=num_epochs,\n    validation_data=valid_dataset,\n    callbacks=[reduce_lr, early_stopping],\n)\nprint(\"Training completed. Saving vision and text encoders...\")\nvision_encoder.save(\"vision_encoder\")\ntext_encoder.save(\"text_encoder\")\nprint(\"Models are saved.\")","1946f91d":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"train\", \"valid\"], loc=\"upper right\")\nplt.show()","132057e3":"print(\"Loading vision and text encoders...\")\nvision_encoder = keras.models.load_model(\"vision_encoder\")\ntext_encoder = keras.models.load_model(\"text_encoder\")\nprint(\"Models are loaded.\")\n\n\ndef read_image(image_path):\n    image_array = tf.image.decode_jpeg(tf.io.read_file(image_path), channels=3)\n    return tf.image.resize(image_array, (299, 299))\n\n\nprint(f\"Generating embeddings for {len(image_paths)} images...\")\nimage_embeddings = vision_encoder.predict(\n    tf.data.Dataset.from_tensor_slices(image_paths).map(read_image).batch(batch_size),\n    verbose=1,\n)\nprint(f\"Image embeddings shape: {image_embeddings.shape}.\")","750ae236":"\ndef find_matches(image_embeddings, queries, k=9, normalize=True):\n    # Get the embedding for the query.\n    query_embedding = text_encoder(tf.convert_to_tensor(queries))\n    # Normalize the query and the image embeddings.\n    if normalize:\n        image_embeddings = tf.math.l2_normalize(image_embeddings, axis=1)\n        query_embedding = tf.math.l2_normalize(query_embedding, axis=1)\n    # Compute the dot product between the query and the image embeddings.\n    dot_similarity = tf.matmul(query_embedding, image_embeddings, transpose_b=True)\n    # Retrieve top k indices.\n    results = tf.math.top_k(dot_similarity, k).indices.numpy()\n    # Return matching image paths.\n    return [[image_paths[idx] for idx in indices] for indices in results]\n","d0ff000a":"query = \"a family standing next to the ocean on a sandy beach with a surf board\"\nmatches = find_matches(image_embeddings, [query], normalize=True)[0]\n\nplt.figure(figsize=(20, 20))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(mpimg.imread(matches[i]))\n    plt.axis(\"off\")\n","8ef9cf5e":"\ndef compute_top_k_accuracy(image_paths, k=100):\n    hits = 0\n    num_batches = int(np.ceil(len(image_paths) \/ batch_size))\n    for idx in tqdm(range(num_batches)):\n        start_idx = idx * batch_size\n        end_idx = start_idx + batch_size\n        current_image_paths = image_paths[start_idx:end_idx]\n        queries = [\n            image_path_to_caption[image_path][0] for image_path in current_image_paths\n        ]\n        result = find_matches(image_embeddings, queries, k)\n        hits += sum(\n            [\n                image_path in matches\n                for (image_path, matches) in list(zip(current_image_paths, result))\n            ]\n        )\n\n    return hits \/ len(image_paths)\n\n\nprint(\"Scoring training data...\")\ntrain_accuracy = compute_top_k_accuracy(train_image_paths)\nprint(f\"Train accuracy: {round(train_accuracy * 100, 3)}%\")\n\nprint(\"Scoring evaluation data...\")\neval_accuracy = compute_top_k_accuracy(image_paths[train_size:])\nprint(f\"Eval accuracy: {round(eval_accuracy * 100, 3)}%\")\n","2d1ba21e":"!rm -rf \"\/kaggle\/tmp\/*\"","87ddfcee":"## Implement the projection head\n\nThe projection head is used to transform the image and the text embeddings to\nthe same embedding space with the same dimensionality.","c5645c1c":"### Process and save the data to TFRecord files\n\nYou can change the `sample_size` parameter to control many image-caption pairs\nwill be used for training the dual encoder model.\nIn this example we set `train_size` to 30,000 images,\nwhich is about 35% of the dataset. We use 2 captions for each\nimage, thus producing 60,000 image-caption pairs. The size of the training set\naffects the quality of the produced encoders, but more examples would lead to\nlonger training time.","23ca255b":"## Final remarks\n\nYou can obtain better results by increasing the size of the training sample,\ntrain for more  epochs, explore other base encoders for images and text,\nset the base encoders to be trainable, and tune the hyperparameters,\nespecially the `temperature` for the softmax in the loss computation.","defbaa1a":"## Search for images using natural language queries\n\nWe can then retrieve images corresponding to natural language queries via\nthe following steps:\n\n1. Generate embeddings for the images by feeding them into the `vision_encoder`.\n2. Feed the natural language query to the `text_encoder` to generate a query embedding.\n3. Compute the similarity between the query embedding and the image embeddings\nin the index to retrieve the indices of the top matches.\n4. Look up the paths of the top matching images to display them.\n\nNote that, after training the `dual encoder`, only the fine-tuned `vision_encoder`\nand `text_encoder` models will be used, while the `dual_encoder` model will be discarded.","e35d7309":"**Credit:** All I did here is I took the tutorial from https:\/\/keras.io\/examples\/nlp\/nl_image_search\/ and then I made a bunch of small modifications in order to get it to run inside of a GPU-enabled Kaggle Notebook.  The natural language image search is a neat concept!  It was fun to explore.","d4a5400e":"Note that training the model with 60,000 image-caption pairs, with a batch size of 256,\ntakes around 12 minutes per epoch using a V100 GPU accelerator. If 2 GPUs are available,\nthe epoch takes around 8 minutes.","965ad849":"Set the `query` variable to the type of images you want to search for.\nTry things like: 'a plate of healthy food',\n'a woman wearing a hat is walking down a sidewalk',\n'a bird sits near to the water', or 'wild animals are standing in a field'.","ca2c8a2f":"# Natural language image search with a Dual Encoder\n\n**Author:** [Khalid Salama](https:\/\/www.linkedin.com\/in\/khalid-salama-24403144\/)<br>\n**Date created:** 2021\/01\/30<br>\n**Last modified:** 2021\/01\/30<br>\n**Description:** Implementation of a dual encoder model for retrieving images that match natural language queries.","253c62e2":"### Generate embeddings for the images\n\nWe load the images and feed them into the `vision_encoder` to generate their embeddings.\nIn large scale systems, this step is performed using a parallel data processing framework,\nsuch as [Apache Spark](https:\/\/spark.apache.org) or [Apache Beam](https:\/\/beam.apache.org).\nGenerating the image embeddings may take several minutes.","ca9e89d9":"## Implement the dual encoder\n\nTo calculate the loss, we compute the pairwise dot-product similarity between\neach `caption_i` and `images_j` in the batch as the predictions.\nThe target similarity between `caption_i`  and `image_j` is computed as\nthe average of the (dot-product similarity between `caption_i` and `caption_j`)\nand (the dot-product similarity between `image_i` and `image_j`).\nThen, we use crossentropy to compute the loss between the targets and the predictions.","73264841":"**Credit:** All I did here is I took the tutorial from https:\/\/keras.io\/examples\/nlp\/nl_image_search\/ and then I made a bunch of small modifications in order to get it to run inside of a GPU-enabled Kaggle Notebook.  The natural language image search is a neat concept!  It was fun to explore.","9b0f2a51":"## Introduction\n\nThe example demonstrates how to build a dual encoder (also known as two-tower) neural network\nmodel to search for images using natural language. The model is inspired by\nthe [CLIP](https:\/\/openai.com\/blog\/clip\/)\napproach, introduced by Alec Radford et al. The idea is to train a vision encoder and a text\nencoder jointly to project the representation of images and their captions into the same embedding\nspace, such that the caption embeddings are located near the embeddings of the images they describe.\n\nThis example requires TensorFlow 2.4 or higher.\nIn addition, [TensorFlow Hub](https:\/\/www.tensorflow.org\/hub)\nand [TensorFlow Text](https:\/\/www.tensorflow.org\/tutorials\/tensorflow_text\/intro)\nare required for the BERT model, and [TensorFlow Addons](https:\/\/www.tensorflow.org\/addons)\nis required for the AdamW optimizer. These libraries can be installed using the\nfollowing command:\n\n```python\npip install -q -U tensorflow-hub tensorflow-text tensorflow-addons\n```","1d8105de":"Plotting the training loss:","9fe39ae2":"## Evaluate the retrieval quality\n\nTo evaluate the dual encoder model, we use the captions as queries.\nWe use the out-of-training-sample images and captions to evaluate the retrieval quality,\nusing top k accuracy. A true prediction is counted if, for a given caption, its associated image\nis retrieved within the top k matches.","d59f8df9":"### Create `tf.data.Dataset` for training and evaluation","d6b13b1b":"## Train the dual encoder model\n\nIn this experiment, we freeze the base encoders for text and images, and make only\nthe projection head trainable.","4c1f41e7":"### Retrieve relevant images\n\nIn this example, we use exact matching by computing the dot product similarity\nbetween the input query embedding and the image embeddings, and retrieve the top k\nmatches. However, *approximate* similarity matching, using frameworks like\n[ScaNN](https:\/\/github.com\/google-research\/google-research\/tree\/master\/scann),\n[Annoy](https:\/\/github.com\/spotify\/annoy), or [Faiss](https:\/\/github.com\/facebookresearch\/faiss)\nis preferred in real-time use cases to scale with a large number of images.","ad51a72c":"## Setup","6483a017":"## Prepare the data\n\nWe will use the [MS-COCO](https:\/\/cocodataset.org\/#home) dataset to train our\ndual encoder model. MS-COCO contains over 82,000 images, each of which has at least\n5 different caption annotations. The dataset is usually used for\n[image captioning](https:\/\/www.tensorflow.org\/tutorials\/text\/image_captioning)\ntasks, but we can repurpose the image-caption pairs to train our dual encoder\nmodel for image search.\n\n###\nDownload and extract the data\n\nFirst, let's download the dataset, which consists of two compressed folders:\none with images, and the other\u2014with associated image captions.\nNote that the compressed images folder is 13GB in size.","b9f2dd96":"## Implement the text encoder\n\nWe use [BERT](https:\/\/tfhub.dev\/tensorflow\/small_bert\/bert_en_uncased_L-12_H-256_A-4\/1)\nfrom [TensorFlow Hub](https:\/\/tfhub.dev) as the text encoder","d4cdfa3f":"## Implement the vision encoder\n\nIn this example, we use [Xception](https:\/\/keras.io\/api\/applications\/xception\/)\nfrom [Keras Applications](https:\/\/keras.io\/api\/applications\/) as the base for the\nvision encoder."}}