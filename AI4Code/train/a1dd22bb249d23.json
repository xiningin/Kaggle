{"cell_type":{"28deeead":"code","78569a8e":"code","67c67773":"code","176f44ff":"code","98b69f9a":"code","d225a772":"code","4242da93":"markdown","49fddcb8":"markdown","9671fa15":"markdown","2a36a899":"markdown","13f7539f":"markdown","a9c934c9":"markdown","fe5c8f3d":"markdown","09aecec6":"markdown"},"source":{"28deeead":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom parameterizedcnn import ParameterizedCNN \n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\n\nimport tensorflow as tf\nimport sys","78569a8e":"mtrain = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_train.csv\")\nmtest = pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_test.csv\")\nmnist_train = np.array(mtrain)\nmnist_test = np.array(mtest)\nmnist_train_label = mnist_train[:,0]\nmnist_test_label = mnist_test[:,0]\nmnist_train = mnist_train[:,1:]\nmnist_test = mnist_test[:,1:]\nmnist_train=mnist_train.reshape(60000,28,28,1)\nmnist_train = mnist_train.astype(\"float32\")\/255.0\nmnist_test=mnist_test.reshape(10000,28,28,1)\nmnist_test = mnist_test.astype(\"float32\")\/255.0\n\n\nplt.imshow(mnist_train[0].reshape(28,28), cmap='gray')\nmnist_train_label = to_categorical(mnist_train_label)\nmnist_test_label = to_categorical(mnist_test_label)","67c67773":"def step_function(X,y):\n    with tf.GradientTape() as tape:\n        pred = model(X)\n        loss = categorical_crossentropy(y, pred)\n        \n        grads=tape.gradient(loss, model.trainable_variables)\n        opt.apply_gradients(zip(grads, model.trainable_variables))","176f44ff":"EPOCHS = 25\nBS = 64\nINIT_LR = 1e-3\n\n# model parameters\ndefault_parameters = {\"filters\":[16,32,64], \"filter_size\":[3,3,3], \"pool_size\":[2,2,2],\"padding\": [\"same\",\"same\",\"same\"], \"drop_out\":[0.3,0.4,0.5],\"dense\":256}\nimg_shape = (28,28,1)\nmnist_classes = 10\n\nmodel = ParameterizedCNN.generate_model(input_shape=img_shape, hyperparameters = default_parameters, classes=mnist_classes)\n\nopt = Adam(lr=INIT_LR, decay=INIT_LR\/EPOCHS)\nmodel.summary()","98b69f9a":"updates = int(len(mnist_train)\/BS)\nfor i in range(0, updates):\n    start = i * BS\n    end = start + BS\n    \n    step_function(mnist_train[start:end], mnist_train_label[start:end])","d225a772":"model.compile(optimizer=opt, loss=categorical_crossentropy,\tmetrics=[\"acc\"])\n\n(loss, acc) = model.evaluate(mnist_test, mnist_test_label)\nprint(\"Model accuracy : {:.4f}\".format(acc))","4242da93":"### STEP FUNCTION\n1. Defined the loss from the prediction\n2. Capture the gradients in the Tape from the predicted-loss\n3. apply (update) the gradients - for the weights\n\nThis is components 2,3 & 4 required for Gradient Tape as mentioned above.","49fddcb8":"> *adapted from PyImagesearch :)*","9671fa15":"### Apply the step function\n","2a36a899":"### Instantiate the Parameterized CNN model\nThis is component# 1 required for Gradient Tape ","13f7539f":"### Parameterized CNN\nThis is a utilty to create a custom dynamic CNN with the network architecture defined using the paramaters.\nThe utility defined [here](https:\/\/www.kaggle.com\/pankaj1234\/parameterizedcnn) with all the required information. ","a9c934c9":"> Use the MNIST dataset for the analysis ","fe5c8f3d":"Hence this apply the custom training loop required to train the model using Tensorflow Gradient Tape API.","09aecec6":"# Tensorflow Gradient Tape - Example\n\nThis is a simple example to demonstrate the gradient tape functionality from Tensorflow API. The gradient tape is a powerfull concept in tensorflow which allow us to wtite the custom training loops while calculating automatic differentiation (computational differentation).\nThe **Automatic (computational) differentiation** is fast and efficient way to compute partial derivatives using chain rules with simple aritmatic operations.\n\n## Gradient Tape\nThe tensorflow Gradient Tape require 4 basic components:\n1. The model\n2. The **loss** function - to compute model loss\n3. The **optimizer** to update the model weights\n4. The **Step** functions to encapsulate the forwar & backward pass of the network\n"}}