{"cell_type":{"b6e2c4ac":"code","e46a422b":"code","816ad3db":"code","bdbc377a":"code","aca36841":"code","befb0a64":"code","58c1d74a":"code","0eb99e21":"code","a9e919ad":"code","82452dd3":"code","fb11cac2":"code","d35e1f7d":"code","31217420":"code","41e62e0c":"code","846e7f8c":"code","3caeed67":"code","28933e66":"code","b0f526d1":"code","d26634cc":"code","ed985647":"code","69d4b908":"code","0231c9fc":"code","0b2c4539":"code","04e8fd57":"code","be2b3b11":"code","f4df7ef1":"code","e4371287":"code","7de3c4be":"code","6ca0c373":"code","5be04ed4":"code","04cb4cee":"code","00c9db36":"code","58e834f7":"code","08873882":"code","c2583f89":"code","b0ff7cc9":"code","90da8065":"code","a6e125e6":"code","a85102a6":"code","20732f59":"markdown","99dc49d2":"markdown","ef50de85":"markdown","ac06aedf":"markdown","102ec56a":"markdown","113b924f":"markdown","c62eee23":"markdown","d526057d":"markdown","ce901570":"markdown","dcd52799":"markdown","df914749":"markdown","2fb3d831":"markdown","19558f71":"markdown","305618be":"markdown","a60adf6c":"markdown","3798ba83":"markdown","76229974":"markdown","95c8a07e":"markdown","98baf722":"markdown","154f42e0":"markdown","c74406f7":"markdown","86f06d8e":"markdown","29e6da8c":"markdown","57d68a00":"markdown","4792f746":"markdown","bdb8729a":"markdown","ea7a463a":"markdown","4ec34cb9":"markdown","1d7691b7":"markdown","c9458c08":"markdown","b6e71ee5":"markdown","371633ce":"markdown","4bafddc0":"markdown","3f9b1acb":"markdown"},"source":{"b6e2c4ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n# plotting\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# text preprocessing\nimport re, string, nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom bs4 import BeautifulSoup \n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","e46a422b":"# Helper functions\n\ndef text_cleaner(text):\n    '''Cleans tweet text for modeling.\n    \n    Changes text to lower case.\n    Removes hyperlinks, @users, html tags, punctuation, words with numbers\n    inside them, and numbers.\n    \n    Args:\n        text (string): the text to be cleaned\n        \n    Returns:\n        text (string): the cleaned text\n    '''\n    # change the text to lower case\n    text = text.lower()\n    # remove hyperlinks from the text\n    text = re.sub(r'https?:\/\/\\S+|www\\.\\S+', '', text)\n    # remove @user tagging\n    text = re.sub(r'@\\S+', '', text)\n    # remove html tags\n    soup = BeautifulSoup(text, 'lxml')\n    text = soup.get_text()\n    # replace URL-encoded spaces\n    text = re.sub('%20', ' ', text)\n    # remove punctuation\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    # remove words with numbers inside them\n    text = re.sub('\\w*\\d\\w*', '', text)\n    # remove remaining numbers\n    text = re.sub('\\d', '', text)\n    # tokenize the text\n    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    text = tokenizer.tokenize(text)\n    # rejoin the text using a single space for the seperator\n    text = ' '.join(text)\n    return text\n\ndef text_normalize(text, stem_it = False, lemmatize_it = False):\n    '''Normalizes tweet text for modeling.\n    \n    Tokenizes text, stems or lemmatizes the text if desired, then rejoins\n    the text back into a single string.\n    \n    Args:\n        text (string): the text to be normalized\n        stem_it (boolean): whether to stem the input text.\n            Default = False\n        lemmatize_it (boolean): whether to lemmatize the input text\n            Default = False\n        \n    Returns:\n        text (string): the normalized text\n    '''\n    tokenizer = nltk.tokenize.WhitespaceTokenizer()\n    stemmer = nltk.PorterStemmer()\n    lemmatizer = nltk.WordNetLemmatizer()\n    # tokenize the text\n    text = tokenizer.tokenize(text)\n    # stem or lemmatize the text, if required.\n    if stem_it:\n        text = [stemmer.stem(word) for word in text]\n    elif lemmatize_it:\n        text = [lemmatizer.lemmatize(word) for word in text]\n    else:\n        # if the text wasn't stemmed\/lemmatized, rejoin and return it\n        return ' '.join(text)\n    # rejoin the text and return it\n    text = ' '.join(text)\n    return text\n\ndef model_scoring(score_array):\n    '''Prints and returns mean and standard deviation of an array of scores\n    \n    Args:\n        score_array (numpy array): an array of scores to be summarized\n    \n    Returns:\n        mean_score (float): the mean of the array of scores\n        stability_score (float): the standard deviation of the array of scores\n    '''\n    # calculate the mean\n    mean_score = np.mean(score_array)\n    # calculate the std dev\n    stability_score = np.std(score_array)\n    # print and return the mean and std dev\n    print('Mean score: ', mean_score, '+\/-', stability_score * 2)\n    return mean_score, stability_score","816ad3db":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}\ndef convert_abbrev(word):\n    return abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word","bdbc377a":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","aca36841":"train.head(5)","befb0a64":"train.info()","58c1d74a":"test.head()","0eb99e21":"test.info()","a9e919ad":"train.loc[train.duplicated(subset = 'text') == True]","82452dd3":"test.loc[test.duplicated(subset = 'text') == True]","fb11cac2":"sns.barplot(train['target'].value_counts().index, train['target'].value_counts());","d35e1f7d":"train.sample(5)","31217420":"test.sample(5)","41e62e0c":"train.drop_duplicates(subset = 'text', inplace = True)","846e7f8c":"train.info()","3caeed67":"cleaned_train = train.copy()","28933e66":"cleaned_train['text'] = cleaned_train['text'].apply(lambda x: convert_abbrev(x))","b0f526d1":"cleaned_train['text'] = cleaned_train['text'].apply(lambda x: text_cleaner(x))","d26634cc":"len(cleaned_train['location'].unique())","ed985647":"len(cleaned_train['keyword'].unique())","69d4b908":"print(cleaned_train['keyword'].unique())","0231c9fc":"cleaned_train['keyword'].isna().sum()","0b2c4539":"cleaned_train['keyword'].fillna('nokeyword', inplace = True)","04e8fd57":"cleaned_train['keyword'] = cleaned_train['keyword'].apply(lambda x: text_cleaner(x))","be2b3b11":"for row in cleaned_train.index:\n    keyword = cleaned_train['keyword'][row]\n    text = cleaned_train['text'][row]\n    if keyword in text:\n        pass\n    elif keyword != 'nokeyword':\n        print(row, 'MISSING KEYWORD:', keyword, 'IN TEXT:', text)","f4df7ef1":"cleaned_train.drop(columns = ['keyword', 'location'], inplace = True)","e4371287":"cleaned_train.info()","7de3c4be":"cleaned_train.loc[(cleaned_train['text'].isna()==True) | (cleaned_train['text'] == '')]","6ca0c373":"cleaned_train.drop(index = 5115, inplace = True)","5be04ed4":"# lemmatized copy\nlemmatized_train = cleaned_train.copy()\nlemmatized_train['text'] = lemmatized_train['text'].apply(lambda x: text_normalize(x, lemmatize_it = True))\nlemmatized_train.sample(5)","04cb4cee":"# copy test data\ncleaned_test = test.copy()\n# drop \"keyword\" and \"location\" columns\ncleaned_test.drop(columns = ['keyword', 'location'], inplace = True)\n# convert abbreviations\ncleaned_test['text'] = cleaned_test['text'].apply(lambda x: convert_abbrev(x))\n# clean the text\ncleaned_test['text'] = cleaned_test['text'].apply(lambda x: text_cleaner(x))\n# lemmatize the text\ncleaned_test['text'] = cleaned_test['text'].apply(lambda x: text_normalize(x, lemmatize_it = True))\ncleaned_test.sample(5)","00c9db36":"# check for blank text\ncleaned_test.loc[(cleaned_test['text'].isna()==True) | (cleaned_test['text'] == '')]","58e834f7":"cleaned_test.info()","08873882":"# modeling imports\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer","c2583f89":"p_classes = dict(train['target'].value_counts(normalize=True))\nnaive_approach = p_classes[0]\nprint('Class probabilities: ', p_classes,\n      '\\nChance tweet is not about a real disaster: ', np.round(naive_approach, decimals = 4))","b0ff7cc9":"count_vectorizer = CountVectorizer(strip_accents = 'unicode',\n                                   ngram_range = (1, 3), # consider unigrams, bigrams, and trigrams\n                                   binary = True)","90da8065":"mnb_train_vector_lemma = count_vectorizer.fit_transform(lemmatized_train['text'])\n\nclf_mnb = MultinomialNB()\n\n# Weighted F1 score\nscores = model_selection.cross_val_score(clf_mnb,\n                                         mnb_train_vector_lemma, cleaned_train[\"target\"],\n                                         cv=5,\n                                         scoring=\"f1_weighted\")\n\nprint('Weighted F1:')\nclf_score = model_scoring(scores)\n\n# Weighted precision score\nscores = model_selection.cross_val_score(clf_mnb,\n                                         mnb_train_vector_lemma, cleaned_train[\"target\"],\n                                         cv=5,\n                                         scoring=\"precision_weighted\")\n\nprint('\\nWeighted precision:')\nclf_score = model_scoring(scores)\n\n# Weighted recall score\nscores = model_selection.cross_val_score(clf_mnb,\n                                         mnb_train_vector_lemma, cleaned_train[\"target\"],\n                                         cv=5,\n                                         scoring=\"recall_weighted\")\n\nprint('\\nWeighted recall:')\nclf_score = model_scoring(scores)","a6e125e6":"count_vectorizer.fit(lemmatized_train['text'])\ntest_vector_lemma = count_vectorizer.transform(cleaned_test['text'])\nclf_mnb.fit(mnb_train_vector_lemma, cleaned_train[\"target\"])\nmnb_preds = clf_mnb.predict(test_vector_lemma)","a85102a6":"model_sub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nmodel_sub['target'] = mnb_preds\nmodel_sub.to_csv('..\/working\/mnb_prediction_submission.csv', index = False)","20732f59":"#### Is the training target balanced?\nTo see, I made a barplot of the value counts of each target class.","99dc49d2":"#### Make predictions","ef50de85":"### Modeling\nI performed many experiments offline before settling on my final model.","ac06aedf":"#### Replace abbreviations\nTo begin, I replaced the abbreviations in the text field of the testing data. My thinking is that replacing the abbreviations in both sets of data allows me to more accurately compare and model their content.","102ec56a":"#### Clean the tweets' \"text\" field.\nThere are a lot of things in the \"text\" field that don't really provide any information for the purpose of predicting the purpose of the tweets.\n\nThis includes hyperlinks, HTML tags, punctuation, numbers, words that have numbers inside them, and \"stop words.\" Stop words are words that are commonly used, but don't really provide meaning by themselves, like \"the,\" \"or,\" and \"that.\"\n\nA quick glance also reveals misspelled words. I will look for a spelling checker and return to this later.\n\nFor this purpose, I wrote a function to clean the text, \"*text_cleaner*,\" defined with the other helper functions near the start of this notebook.","113b924f":"Credit for the abbreviation list below, and the function that follows: _Up-to-date list of Slangs for Text Preprocessing by @nmaguette_ (https:\/\/www.kaggle.com\/nmaguette\/up-to-date-slangs-conversion-for-text-processing\/)\n\nThese abbreviations and the accompanying function are used to translate common slang or texting\/tweeting abbreviations into full words.","c62eee23":"Preliminary spot checking looked like the keywords wouldn't really be useful for modeling.\n\n\nSince neither the \"location\" nor the \"keyword\" fields looked useful, I chose to drop them.","d526057d":"Since it turned out there was only one, and it contained no other information, I dropped that observation.","ce901570":"#### Are there any duplicate tweets in the training data?","dcd52799":"Class 1 accounts less than half of the observations, which will need to be accounted for.\n\nTo get a further feel for the data, I looked at a random sample of both the training and test sets.","df914749":"#### Cleaning is required\nIt's clear to me at this point that a lot of cleaning will be required to prepare this data for modeling.\n\nI'll start by dropping the duplicate tweets in the training data.","2fb3d831":"I created an additional copy of the cleaned training data to hold a lemmatized copy of the text, because all the models I ran offline performed better on lemmatized text.","19558f71":"### Inspect basic information about the data","305618be":"At this point, I thought to have a look at the \"location\" and \"keyword\" fields.","a60adf6c":"There are duplicates in the test data, but I just have to accept that, since removing them will result in an incomplete submission.","3798ba83":"So - my model must demonstrate more than ~58% accuracy to outperform the naive approach.","76229974":"There are 222 unique values in the \"keyword\" field, out of 7447 entries. I decided to look a little closer.","95c8a07e":"### Going Forward\nThis is a work in progress as I learn about NLP from this competition. I plan to try other models, possibly a neural net approach, in addition to adjusting parameters in my current models to try to improve performance.\n\nThank you for reading this far! Feedback is welcome and appreciated, as I am just a beginner in the field and I am sure there are things I have missed or perhaps misunderstood.","98baf722":"It was possible that there were tweets with no text left after cleaning (for instance, if they only had hyperlinks in them), so I checked for that.","154f42e0":"# A Beginner's Look At NLP With Disaster Tweets\nI am a beginner in the field of Data Science and this is some of my first real work on Natural Language Processing. As such, this notebook documents my experience as a newcomer to NLP and Kaggle competitions in general.\n\nI start with the typical list of library imports.","c74406f7":"With 3328 unique values in the \"location\" field, I don't think it's going to be a lot of use to me without a lot of work. I may return to that later, if I think it will add value.","86f06d8e":"Now that I was done processing the training data, I needed to perform the same operations on the test data (except for dropping records).","29e6da8c":"## Read the data from files into DataFrames","57d68a00":"### My Best Model So Far\n\nThe model that performed best so far is a multinomial naive Bayes using lemmatized and CountVectorized data. Stopwords were left inside the text; model scores went down ~2%-4% when stopwords were removed from the training data.","4792f746":"## Define helper functions\nI used these for text cleaning and normalization, as well as to summarize model performance.","bdb8729a":"### How good does my model have to be?\nI had a fairly obvious question here: how good does my model have to be in order to outperform the naive approach? In this case, the naive approach would be to simply declare that none of the tweets in the training set was about a disaster or emergency.","ea7a463a":"#### Creating A Submission","4ec34cb9":"I wrote a quick for-loop to check if the keyword is in the text of a given tweet.","1d7691b7":"I wasn't too sure how much value there was in the \"keyword\" field. I decided to compare the \"keyword\" field to the \"text\" field. My thinking was that if the keywords were already present in the text, there's no need to do anything with the \"keyword\" field.\n\nIn order to make the comparison, I started by cleaning the \"keyword\" field.\n\nFirst, since I knew there were missing values, I filled those in with 'nokeyword,' then I ran the *text_cleaner* function on the \"keyword\" column.","c9458c08":"There are 110 tweets with duplicate text. I will drop the duplicates.\n\n#### Are there any duplicate tweets in the test data?","b6e71ee5":"#### Look at how many rows and columns, as well as basic information about the training data.\n* There are 7613 rows and 5 columns.\n* There are missing values in the 'keyword' and 'location' fields.\n* The 'id' and 'target' fields are integer data; the rest are string data","371633ce":"There are a total of 7503 observations remaining after removing the duplicate tweets.\n\n#### Make a copy of the training data to work on.\nThis isn't strictly necessary. I could have worked directly on the training data instead of a copy. I thought it might be useful to retain the original data for comparison purposes later, so I made a copy to do my processing on.","4bafddc0":"#### Look at basic information for the testing data:\n* There are 3263 rows and 4 columns (no 'target' column).\n* There are missing values in the 'keyword' and 'location' columns.\n* The 'id' column is integer data; the rest are string data.","3f9b1acb":"#### Analysis\nThis model achieved a cross-validated mean weighted F1 score with 95% confidence interval of **0.7071 +\/- 0.0721** on the training data.\n\nTo compare apples to apples, the balanced accuracy score on the training data was **0.7109 +\/- 0.0732**. This is a reasonable increase over the naive approach's accuracy of **0.5738**, although there is definitely still room for improvement in the model.\n\nAdditional metrics looked at were weighted precision and weighted recall. I chose to use weighted metrics for my model evaluation due to the class imbalance in the target variable. The weighted precision with 95% confidence interval was **0.7192 +\/- 0.0745**. The weighted recall with 95% confidence interval was **0.7061 +\/- 0.0733**.\n\nThe weighted precision was slightly higher than the weighted recall. That indicates that the model was a little more likely to be correct when it did predict that a tweet was about a disaster than it was likely to be correct in its predictions regarding disasters over all."}}