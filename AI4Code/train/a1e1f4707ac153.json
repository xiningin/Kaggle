{"cell_type":{"9e059447":"code","12e9bc16":"code","249d70a5":"code","700d9c41":"code","c94d189e":"code","694ffe9a":"code","cdc23764":"code","9292e6ee":"code","e1b0cc9d":"code","cd4d1145":"code","e5961d95":"code","ac3efbd2":"code","70512e4b":"code","510cf30b":"code","c68db692":"code","8a9c673c":"code","a23cf8e2":"code","ba01e440":"code","fe37e3e9":"code","96452510":"code","487c8cbf":"code","d4e75597":"code","3380c5bc":"markdown","67262ece":"markdown","7d1a944d":"markdown","e94545bc":"markdown"},"source":{"9e059447":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import roc_auc_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport shap\nimport math\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom sklearn import preprocessing\nimport xgboost as xgb\nimport gc\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","12e9bc16":"train = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/train.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\ntest = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/test.csv')","249d70a5":"train_time = train['time'].values\ntrain_time_0 = train_time[:50000]\nfor i in range(1,100):\n    train_time_0 = np.hstack([train_time_0, train_time[:50000]])\ntrain['time'] = train_time_0\n\n\ntrain_time_1 = train_time[:50000]\nfor i in range(1,40):\n    train_time_1 = np.hstack([train_time_1, train_time[:50000]])\ntest['time'] = train_time_1","700d9c41":"np.unique(train_time_1).shape","c94d189e":"np.unique(train_time_0).shape","694ffe9a":"n_groups = 100\ntrain[\"group\"] = 0\nfor i in range(n_groups):\n    ids = np.arange(i*50000, (i+1)*50000)\n    train.loc[ids,\"group\"] = i\n    \nn_groups = 40\ntest[\"group\"] = 0\nfor i in range(n_groups):\n    ids = np.arange(i*50000, (i+1)*50000)\n    test.loc[ids,\"group\"] = i\n    \ntrain['signal_scaled'] = 0\ntest['signal_scaled'] = 0\ntrain['signal_2'] = 0\ntest['signal_2'] = 0","cdc23764":"n_groups = 100\nfor i in range(n_groups):\n    sub = train[train.group == i]\n    signals = sub.signal.values\n    imax, imin = math.floor(np.max(signals)), math.ceil(np.min(signals))\n    signals = (signals - np.min(signals))\/(np.max(signals) - np.min(signals))\n    signals = signals*(imax-imin)\n    train.loc[sub.index,\"signal_scaled\"] = list(np.array(signals))\n    train.loc[sub.index,\"signal_2\"] = [0,] +list(np.array(signals[:-1]))","9292e6ee":"n_groups = 40\nfor i in range(n_groups):\n    sub = test[test.group == i]\n    signals = sub.signal.values\n    imax, imin = math.floor(np.max(signals)), math.ceil(np.min(signals))\n    signals = (signals - np.min(signals))\/(np.max(signals) - np.min(signals))\n    signals = signals*(imax-imin)\n    test.loc[sub.index,\"signal_scaled\"] = list(np.array(signals))\n    test.loc[sub.index,\"signal_2\"] = [0,] +list(np.array(signals[:-1]))","e1b0cc9d":"train.head()","cd4d1145":"test.head()","e5961d95":"del train['open_channels']\ndel train['group']\n\ntrain.head()","ac3efbd2":"test.head()","70512e4b":"del test['group']\ntest.head()","510cf30b":"train['target'] = 0\ntest['target'] = 1","c68db692":"train_test = pd.concat([train, test], axis =0)\ntarget = train_test['target'].values\n\n\ndel train, test\ngc.collect()","8a9c673c":"train, test = model_selection.train_test_split(train_test, test_size=0.33, random_state=42, shuffle=True)\ndel train_test\ngc.collect()","a23cf8e2":"train_y = train['target'].values\ntest_y = test['target'].values\ndel train['target'], test['target']\ngc.collect()","ba01e440":"train = lgb.Dataset(train, label=train_y)\ntest = lgb.Dataset(test, label=test_y)\ngc.collect()","fe37e3e9":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 2,\n         'learning_rate': 0.2,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 44,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","96452510":"num_round = 50\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 50)","487c8cbf":"columns = ['time', 'signal', 'signal_scaled', 'signal_2']","d4e75597":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(20))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","3380c5bc":"In this notebook we'll try to use adversarial validation in order to see how similar\/different the train and test sets are. We already know that we are dealing with a synthetic dataset, so **in principle**, there should not be much difference between the train and test sets. Nontheless, would be interesting to find out if that is indeed the case.\n\nWe are also dealign with time-series signal data, so we are not starting with a lot of features. We'll createa and modify a few based on the few high scoring kernels.","67262ece":"The following signal processing parts are taken from the following Khoi Nguyen kernel: https:\/\/www.kaggle.com\/suicaokhoailang\/an-embarrassingly-simple-baseline-0-960-lb","7d1a944d":"That's really interesting. Even with so few features we are gettign AUC of 0.75. That's farily significant for this kind of problem.\n\nLet's take a look at the feature imporances.","e94545bc":"So it would seem that there is a difference in the signal itself between two sets. And it's good to see that time plays no importance, as it shouldn't."}}