{"cell_type":{"f7292456":"code","efd89781":"code","d6c18b5f":"code","c9c02dc4":"code","7b8c9e7d":"code","3f08dbae":"code","eb74f1ec":"code","09833cef":"code","473bfc2a":"code","c248129e":"code","8ee630d9":"code","787f5262":"code","bcdb124b":"code","f993c2d8":"code","67720bff":"code","dda4eef9":"code","eb007787":"code","341790a0":"code","ed9c980d":"code","2eef23e7":"code","cf1a6dc7":"code","821d73d9":"code","3ba7ef86":"code","4550201e":"code","49a640dc":"code","f8c67bcb":"code","9e89f7cd":"code","dedbddee":"code","00f2968d":"code","8894c89d":"code","ef6c488c":"code","34051a85":"code","d854a4a7":"code","82bf568c":"code","687ded5b":"code","403d9c56":"code","38e2326a":"code","b5ee28d6":"code","cc5d85ae":"code","fb11e159":"code","56dbafda":"code","63ae0a32":"code","7d741252":"code","3d675008":"code","f89d513d":"code","e1e8434a":"code","e70be005":"code","3cba30e0":"code","e7fdea79":"code","1dfe98aa":"code","e6529836":"code","023d7f2b":"code","773d5961":"code","eec21f80":"code","0d883331":"code","9c5041d7":"code","18fc4abf":"code","58432d72":"code","66f92a66":"code","2a2e6813":"code","4ed70756":"code","c6fea041":"code","4a29ed1c":"code","1ec1e087":"code","f29feabf":"code","b786a4fe":"code","7468aac0":"code","2a0259ab":"code","dd5669ef":"code","336967f2":"code","bfe22cd4":"code","a6ce8b13":"code","e8575b79":"code","522112f4":"code","2edbf2d8":"code","0231790e":"code","0fa348ac":"code","9fa9c7aa":"code","1749f120":"code","040803f1":"code","c2630f28":"code","4f33f00a":"code","8737777b":"code","925244f1":"code","6727d457":"code","a68f8837":"code","2df136ed":"code","8096c22a":"code","02f94d45":"code","f387280e":"code","fe30d670":"code","1d59d53d":"code","e199fd60":"code","d42b8413":"code","4481c68e":"code","91985b60":"code","14b2c474":"code","6344d758":"code","15bd1f87":"code","951d8c9c":"code","3393d9f5":"code","32849285":"code","1e13ce04":"code","81a852f5":"code","59097b3e":"code","b44a1cea":"code","a20390e1":"code","11b1584c":"code","4d46255c":"code","8fa0618b":"code","113f4cf4":"code","c8780953":"code","e4e4adf1":"code","353dabe8":"code","05e05131":"code","7fd90e86":"code","8327d301":"code","405cee67":"code","a016bb53":"code","10b00925":"code","c440da36":"code","989d7610":"code","dda914e7":"code","477664bf":"code","6c3ab511":"code","23a22890":"code","e6ca1938":"code","d9e121e8":"code","64a65dd3":"code","44a4fb3f":"markdown","9b4cf896":"markdown","4bcabc29":"markdown","0abae6eb":"markdown","10af84e8":"markdown","ecc613d8":"markdown","5caf3c98":"markdown","558a8504":"markdown","fe79a29a":"markdown","42b46d55":"markdown","474dbaeb":"markdown","682ae6c5":"markdown","d4b75a23":"markdown","82ee73eb":"markdown","91507f09":"markdown","bc5c37fa":"markdown","f49e11ae":"markdown","2a2358e2":"markdown","642f2d49":"markdown","a8b9cacc":"markdown","08d2f10a":"markdown","f091894c":"markdown","08445e5e":"markdown","e50af587":"markdown","afa8ccfa":"markdown","c32a55bc":"markdown","aab26bd2":"markdown","67335c76":"markdown","d57afd57":"markdown","ec3aa045":"markdown","bcfe058c":"markdown","fbb87c4b":"markdown","f74f5ca1":"markdown","177daf9b":"markdown","ef020194":"markdown","20423e72":"markdown","e625ad08":"markdown","0bd1e555":"markdown","3a26418d":"markdown","9d6f9fed":"markdown","f531e49f":"markdown","909bd149":"markdown","b21a9656":"markdown","0d551693":"markdown","cebab50f":"markdown","93cd7f49":"markdown","e7e2fc3f":"markdown","aaf948e9":"markdown","ccb50aa9":"markdown","d0892744":"markdown","057d7bbe":"markdown","0c1280ed":"markdown","18df382b":"markdown","79702989":"markdown","c9e43d03":"markdown","87b3801d":"markdown","7996698a":"markdown","b654e1db":"markdown","ea040e9c":"markdown","9e6ae632":"markdown","21e5c3c2":"markdown","7e1056f1":"markdown","a42cba75":"markdown"},"source":{"f7292456":"### installlation required\n#!pip install mlxtend\n\n# libraries\nimport pandas as pd\npd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)\npd.set_option('display.width', 500)\n# \u00e7\u0131kt\u0131n\u0131n tek bir sat\u0131rda olmas\u0131n\u0131 sa\u011flar.\npd.set_option('display.expand_frame_repr', False)\nfrom mlxtend.frequent_patterns import apriori, association_rules\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","efd89781":"df=pd.read_csv('\/kaggle\/input\/online-retail-ii-data-set-from-ml-repository\/Year 2010-2011.csv', encoding = 'unicode_escape')\ndf.head()","d6c18b5f":"def check_df(dataframe):\n    print(\"################ Shape ####################\")\n    print(dataframe.shape)\n    print(\"############### Columns ###################\")\n    print(dataframe.columns)\n    print(\"############### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"############### Head ######################\")\n    print(dataframe.head())\n    print(\"############### Tail ######################\")\n    print(dataframe.tail())\n    print(\"############### Describe ###################\")\n    print(dataframe.describe().T)\n\ncheck_df(df)","c9c02dc4":"def outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.01)\n    quartile3 = dataframe[variable].quantile(0.99)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\ndef retail_data_prep(dataframe):\n    dataframe.dropna(inplace=True)\n    dataframe = dataframe[~dataframe[\"Invoice\"].str.contains(\"C\", na=False)]\n    dataframe = dataframe[dataframe[\"Quantity\"] > 0]\n    dataframe = dataframe[dataframe[\"Price\"] > 0]\n    replace_with_thresholds(dataframe, \"Quantity\")\n    replace_with_thresholds(dataframe, \"Price\")\n    return dataframe","7b8c9e7d":"df = retail_data_prep(df)","3f08dbae":"# EXAMPLE:\n\n# Description   NINE DRAWER OFFICE TIDY   SET 2 TEA TOWELS I LOVE LONDON    SPACEBOY BABY GIFT SET\n# Invoice\n# 536370                              0                                 1                       0\n# 536852                              1                                 0                       1\n# 536974                              0                                 0                       0\n# 537065                              1                                 0                       0\n# 537463                              0                                 0                       1","eb74f1ec":"df_de = df[df['Country'] == \"Germany\"]\ndf_de.head(10)","09833cef":"df_de.groupby(['Invoice', 'Description']).agg({\"Quantity\": \"sum\"}).head(10)","473bfc2a":"df_de.groupby(['Invoice', 'Description']).agg({\"Quantity\": \"sum\"}).unstack().iloc[0:5, 0:5]","c248129e":"df_de.groupby(['Invoice', 'Description']).agg({\"Quantity\": \"sum\"}).unstack().fillna(0).iloc[0:5, 0:5]","8ee630d9":"# applymap will itinirate all cells in the dataframe. apply would only itinirate in row or columns\ndf_de.groupby(['Invoice', 'Description']).agg({\"Quantity\": \"sum\"}).unstack().fillna(0).applymap(\n    lambda x: 1 if x > 0 else 0).iloc[0:5, 0:5]","787f5262":"def create_invoice_product_df(dataframe, id=False):\n    if id:\n        return dataframe.groupby(['Invoice', \"StockCode\"])['Quantity'].sum().unstack().fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)\n    else:\n        return dataframe.groupby(['Invoice', 'Description'])['Quantity'].sum().unstack().fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)","bcdb124b":"de_inv_pro_df = create_invoice_product_df(df_de)\nde_inv_pro_df.head()","f993c2d8":"def create_invoice_product_df(dataframe, id= True):\n    if id:\n        return dataframe.groupby(['Invoice', \"StockCode\"])['Quantity'].sum().unstack().fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)\n    else:\n        return dataframe.groupby(['Invoice', 'Description'])['Quantity'].sum().unstack().fillna(0). \\\n            applymap(lambda x: 1 if x > 0 else 0)","67720bff":"de_inv_pro_df = create_invoice_product_df(df_de)\nde_inv_pro_df.head()\n","dda4eef9":"de_inv_pro_df = create_invoice_product_df(df_de, id=True)","eb007787":"def check_id(dataframe, stock_code):\n    product_name = dataframe[dataframe[\"StockCode\"] == stock_code][[\"Description\"]].values[0].tolist()\n    print(product_name)","341790a0":"check_id(df_de, \"10002\")","ed9c980d":"# minumum support value 0.01, we don't want to get below 0.01 \n# In real life scenarios, this minimum support value is very low.\nfrequent_itemsets = apriori(de_inv_pro_df, min_support=0.01, use_colnames=True)\nfrequent_itemsets.sort_values(\"support\", ascending=False).head(10)\n","2eef23e7":"rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.01)\nrules.sort_values(\"support\", ascending=False).head()\n\n","cf1a6dc7":"rules.sort_values(\"lift\", ascending=False).head(10)","821d73d9":"product_id = \"22492\"\ncheck_id(df, product_id)","3ba7ef86":"sorted_rules = rules.sort_values(\"lift\", ascending=False)\n\nrecommendation_list = []\n\nfor i, product in enumerate(sorted_rules[\"antecedents\"]):\n    for j in list(product):\n        if j == product_id:\n            recommendation_list.append(list(sorted_rules.iloc[i][\"consequents\"])[0])\n\nrecommendation_list[0:3]","4550201e":"check_id(df, \"21915\")","49a640dc":"def arl_recommender(rules_df, product_id, rec_count=1):\n\n    sorted_rules = rules_df.sort_values(\"lift\", ascending=False)\n\n    recommendation_list = []\n\n    for i, product in sorted_rules[\"antecedents\"].items():\n        for j in list(product):\n            if j == product_id:\n                recommendation_list.append(list(sorted_rules.iloc[i][\"consequents\"]))\n\n    recommendation_list = list({item for item_list in recommendation_list for item in item_list})\n\n    return recommendation_list[:rec_count]","f8c67bcb":"check_id(df, \"23049\")","9e89f7cd":"arl_recommender(rules, \"22492\", 1)","dedbddee":"arl_recommender(rules, \"22492\", 2)","00f2968d":"import pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\npd.set_option('display.expand_frame_repr', False)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8894c89d":"import pandas as pd\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 500)\npd.set_option('display.expand_frame_repr', False)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ef6c488c":"df2 = pd.read_csv(\"\/kaggle\/input\/movies-metadata\/movies_metadata.csv\", sep = \";\", encoding = 'unicode_escape', low_memory=False)\ndf2.head()","34051a85":"df2[\"overview\"].head()","d854a4a7":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = ['This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?']","82bf568c":"vectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()\nX.toarray()\n","687ded5b":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(analyzer='word')\nX = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()\n\nX.toarray()","403d9c56":"df2['overview'].head()","38e2326a":"tfidf = TfidfVectorizer(stop_words='english')\ndf2['overview'] = df2['overview'].fillna('')\ntfidf_matrix = tfidf.fit_transform(df2['overview'])\ntfidf_matrix.shape","b5ee28d6":"cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n\ncosine_sim.shape","cc5d85ae":"cosine_sim[1]","fb11e159":"indices = pd.Series(df2.index, index=df2['title'])\n\nindices = indices[~indices.index.duplicated(keep='last')]","56dbafda":"indices.shape","63ae0a32":"indices[:10]","7d741252":"# showing the index number of the films\nindices[\"The American President\"]","3d675008":"movie_index = indices[\"The American President\"]","f89d513d":"cosine_sim[movie_index]","e1e8434a":"similarity_scores = pd.DataFrame(cosine_sim[movie_index], columns=[\"score\"])\nmovie_indices = similarity_scores.sort_values(\"score\", ascending=False)[1:11].index\n\ndf2['title'].iloc[movie_indices]","e70be005":"def content_based_recommender(title, cosine_sim, dataframe):\n    # generates index\n    indices = pd.Series(dataframe.index, index=dataframe['title'])\n    indices = indices[~indices.index.duplicated(keep='last')]\n    # capturing the index of the title\n    movie_index = indices[title]\n    # Calculating similarity scores by title\n    similarity_scores = pd.DataFrame(cosine_sim[movie_index], columns=[\"score\"])\n    # the top 10 movies except itself(the movie we chose)\n    movie_indices = similarity_scores.sort_values(\"score\", ascending=False)[1:11].index\n    return dataframe['title'].iloc[movie_indices]","3cba30e0":"# Example\ncontent_based_recommender(\"The Matrix\", cosine_sim, df2)","e7fdea79":"# Example\ncontent_based_recommender(\"The Godfather\", cosine_sim, df2)","1dfe98aa":"## Cosine Similarity Function, if you want to use it again.\n# def calculate_cosine_sim(dataframe):\n#    tfidf = TfidfVectorizer(stop_words='english')\n#    dataframe['overview'] = dataframe['overview'].fillna('')\n#    tfidf_matrix = tfidf.fit_transform(dataframe['overview'])\n#    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n#    return cosine_sim\n\n# cosine_sim = calculate_cosine_sim(df)\n\n# content_based_recommender('The Dark Knight Rises', cosine_sim, df)","e6529836":"import pandas as pd\npd.set_option('display.max_columns', 20)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\nmovie = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/movie.csv')\nrating = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/rating.csv')\ndf3 = movie.merge(rating, how=\"left\", on=\"movieId\")\ndf3.head()","023d7f2b":"movie = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/movie.csv')\nrating = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/rating.csv')\ndf3 = movie.merge(rating, how=\"left\", on=\"movieId\")\ndf3.head()","773d5961":"df3.shape","eec21f80":"df3.shape","0d883331":"# movie counts\ndf3[\"title\"].nunique()","9c5041d7":"# rating count of each movies\ndf3[\"title\"].value_counts().head()","18fc4abf":"rating_counts = pd.DataFrame(df3[\"title\"].value_counts())","58432d72":"# we don't want all ratings, therefore we add trashold. It will not bring under 10000\nrare_movies = rating_counts[rating_counts[\"title\"] <= 10000].index","66f92a66":"# taking out rare movies from dataframe\ncommon_movies = df3[~df3[\"title\"].isin(rare_movies)]","2a2e6813":"# all ratings\ncommon_movies.shape","4ed70756":"# we have 462 movies now\ncommon_movies[\"title\"].nunique()","c6fea041":"# let's pivot it\nuser_movie_df = common_movies.pivot_table(index=[\"userId\"], columns=[\"title\"], values=\"rating\")","4a29ed1c":"user_movie_df.shape","1ec1e087":"user_movie_df.head(10)","f29feabf":"user_movie_df.columns","b786a4fe":"# columns count and title count are equal\nlen(user_movie_df.columns)","7468aac0":"common_movies[\"title\"].nunique()","2a0259ab":"movie_name = \"Matrix, The (1999)\"","dd5669ef":"movie_name = user_movie_df[movie_name]","336967f2":"user_movie_df.corrwith(movie_name).sort_values(ascending=False).head(10)","bfe22cd4":"#  Another Example\nmovie_name = \"12 Angry Men (1957)\"\nmovie_name = user_movie_df[movie_name]\nuser_movie_df.corrwith(movie_name).sort_values(ascending=False).head(10)","a6ce8b13":"# random selection of movies\nmovie_name = pd.Series(user_movie_df.columns).sample(1).values[0]\nmovie_name = user_movie_df[movie_name]\nuser_movie_df.corrwith(movie_name).sort_values(ascending=False).head(10)","e8575b79":"# script of all codes\ndef create_user_movie_df():\n    movie = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/movie.csv')\n    rating = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/rating.csv')\n    df = movie.merge(rating, how=\"left\", on=\"movieId\")\n    comment_counts = pd.DataFrame(df[\"title\"].value_counts())\n    rare_movies = comment_counts[comment_counts[\"title\"] <= 1000].index\n    common_movies = df[~df[\"title\"].isin(rare_movies)]\n    user_movie_df = common_movies.pivot_table(index=[\"userId\"], columns=[\"title\"], values=\"rating\")\n    return user_movie_df","522112f4":"# user_movie_df = create_user_movie_df()\n\ndef item_based_recommender(movie_name, user_movie_df):\n    movie_name = user_movie_df[movie_name]\n    return user_movie_df.corrwith(movie_name).sort_values(ascending=False).head(10)","2edbf2d8":"item_based_recommender(\"Fight Club (1999)\", user_movie_df)","0231790e":"movie_name = pd.Series(user_movie_df.columns).sample(1).values[0]\n\nitem_based_recommender(movie_name, user_movie_df)","0fa348ac":"# this function helps to find the movie names\ndef check_film(keyword, user_movie_df):\n    return [col for col in user_movie_df.columns if keyword in col]\n\n\ncheck_film(\"Str\", user_movie_df)","9fa9c7aa":"item_based_recommender(\"Forrest Gump (1994)\", user_movie_df)","1749f120":"user_movie_df","040803f1":"# Let's define a random user\nrandom_user = int(pd.Series(user_movie_df.index).sample(1, random_state=35).values)","c2630f28":"# the output random user id\nrandom_user","4f33f00a":"# we selected the random_user's movie here\nrandom_user_df = user_movie_df[user_movie_df.index == random_user]\nrandom_user_df","8737777b":"# Taking out all NaN\nmovies_watched = random_user_df.columns[random_user_df.notna().any()].tolist()\n\n# if you want to see all the movies that watched by random user, execute this\nmovies_watched","925244f1":"# Let's check if random user watch 2001: A Space Odyssey (1968)\nuser_movie_df.loc[user_movie_df.index == random_user, user_movie_df.columns == \"Aladdin (1992)\"]","6727d457":"# how many movies he watched\nlen(movies_watched)","a68f8837":"# James' movies\nmovies_watched_df = user_movie_df[movies_watched]\nmovies_watched_df.head()","2df136ed":"# Number of people who watched at least one movie in common with James. 137658 people watched at least on movie, common movies count 191\nmovies_watched_df.shape","8096c22a":"# user_movie_count indicates how many movies each user watched\n# notnull gives us binary output (1 or 0) if we don't do that, the ratings count, like 3.0 + 4.0 = 7.0\nuser_movie_count = movies_watched_df.T.notnull().sum()\n\n# moving user_movie_count in the columns\nuser_movie_count = user_movie_count.reset_index()\n\nuser_movie_count.columns = [\"userId\", \"movie_count\"]\nuser_movie_count.head()","02f94d45":"# 40 treshold in common movies\nuser_movie_count[user_movie_count[\"movie_count\"] > 40].sort_values(\"movie_count\", ascending=False)","f387280e":"# how many people watch the same movies with James (he watched 50 movies)\n# there is one person watching the same movies\nuser_movie_count[user_movie_count[\"movie_count\"] == 50].count()","fe30d670":"# let's bring users id watching the same movies\nusers_same_movies = user_movie_count[user_movie_count[\"movie_count\"] > 40][\"userId\"]\nusers_same_movies.head()","1d59d53d":"users_same_movies.count()","e199fd60":"final_df = pd.concat([movies_watched_df[movies_watched_df.index.isin(users_same_movies)],\n                      random_user_df[movies_watched]])\n\nfinal_df.head()","d42b8413":"final_df.shape","4481c68e":"# We set all user in the columns, but it doesn't look good,therefore, we will make them tidy\nfinal_df.T.corr()","91985b60":"# making above matrix tidy\ncorr_df = final_df.T.corr().unstack().sort_values().drop_duplicates()\ncorr_df = pd.DataFrame(corr_df, columns=[\"corr\"])\ncorr_df.index.names = ['user_id_1', 'user_id_2']\ncorr_df = corr_df.reset_index()","14b2c474":"corr_df.head()","6344d758":"# Users with 65 percent or more correlation with James\ntop_users = corr_df[(corr_df[\"user_id_1\"] == random_user) & (corr_df[\"corr\"] >= 0.65)][\n    [\"user_id_2\", \"corr\"]].reset_index(drop=True)","15bd1f87":"top_users = top_users.sort_values(by='corr', ascending=False)\ntop_users.rename(columns={\"user_id_2\": \"userId\"}, inplace=True)\ntop_users","951d8c9c":"# lets merge our new table with ratings\nrating = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/rating.csv')\ntop_users_ratings = top_users.merge(rating[[\"userId\", \"movieId\", \"rating\"]], how='inner')","3393d9f5":"# taking out James from the table\ntop_users_ratings = top_users_ratings[top_users_ratings[\"userId\"] != random_user]","32849285":"# We have a problem here. There are two levels in data. One is corr, other one is rating. Someone has high corr but rating is 1.0 with James, another person has low corr, but high rating. Which one should we consider? \ntop_users_ratings.head()","1e13ce04":"# Calculation of weighted_rating\ntop_users_ratings['weighted_rating'] = top_users_ratings['corr'] * top_users_ratings['rating']\ntop_users_ratings.head()","81a852f5":"recommendation_df = top_users_ratings.groupby('movieId').agg({\"weighted_rating\": \"mean\"})\n\nrecommendation_df = recommendation_df.reset_index()","59097b3e":"recommendation_df.head()","b44a1cea":"# there are 8071 movies\nrecommendation_df[[\"movieId\"]].nunique()","a20390e1":"movies_to_be_recommend = recommendation_df[recommendation_df[\"weighted_rating\"] > 3.5].sort_values(\"weighted_rating\", ascending=False)","11b1584c":"movies_to_be_recommend","4d46255c":"# list of movies to recommend james\nmovie = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/movie.csv')\nmovies_to_be_recommend.merge(movie[[\"movieId\", \"title\"]])","8fa0618b":"# def user_based_recommender():\n#     import pickle\n#     import pandas as pd\n#     # user_movie_df = pickle.load(open('user_movie_df.pkl', 'rb'))\n#     random_user = int(pd.Series(user_movie_df.index).sample(1, random_state=45).values)\n#     random_user_df = user_movie_df[user_movie_df.index == random_user]\n#     movies_watched = random_user_df.columns[random_user_df.notna().any()].tolist()\n#     movies_watched_df = user_movie_df[movies_watched]\n#     user_movie_count = movies_watched_df.T.notnull().sum()\n#     user_movie_count = user_movie_count.reset_index()\n#     user_movie_count.columns = [\"userId\", \"movie_count\"]\n#     users_same_movies = user_movie_count[user_movie_count[\"movie_count\"] > 20][\"userId\"]\n#\n#     final_df = pd.concat([movies_watched_df[movies_watched_df.index.isin(users_same_movies)],\n#                           random_user_df[movies_watched]])\n#\n#     corr_df = final_df.T.corr().unstack().sort_values().drop_duplicates()\n#     corr_df = pd.DataFrame(corr_df, columns=[\"corr\"])\n#     corr_df.index.names = ['user_id_1', 'user_id_2']\n#     corr_df = corr_df.reset_index()\n#\n#     top_users = corr_df[(corr_df[\"user_id_1\"] == random_user) & (corr_df[\"corr\"] >= 0.65)][\n#         [\"user_id_2\", \"corr\"]].reset_index(drop=True)\n#\n#     top_users = top_users.sort_values(by='corr', ascending=False)\n#     top_users.rename(columns={\"user_id_2\": \"userId\"}, inplace=True)\n#     rating = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/rating.csv')\n#     top_users_ratings = top_users.merge(rating[[\"userId\", \"movieId\", \"rating\"]], how='inner')\n#     top_users_ratings['weighted_rating'] = top_users_ratings['corr'] * top_users_ratings['rating']\n#     top_users_ratings = top_users_ratings[top_users_ratings[\"userId\"] != random_user]\n#\n#     recommendation_df = top_users_ratings.groupby('movieId').agg({\"weighted_rating\": \"mean\"})\n#     recommendation_df = recommendation_df.reset_index()\n#\n#     movies_to_be_recommend = recommendation_df[recommendation_df[\"weighted_rating\"] > 3.7].sort_values(\"weighted_rating\", ascending=False)\n#     movie = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/movie.csv')\n#     return movies_to_be_recommend.merge(movie[[\"movieId\", \"title\"]])","113f4cf4":"# user_based_recommender()","c8780953":"# pip install surprise\nimport pandas as pd\nfrom surprise import Reader, SVD, Dataset, accuracy\nfrom surprise.model_selection import GridSearchCV, train_test_split, cross_validate\npd.set_option('display.max_columns', None)","e4e4adf1":"movie = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/movie.csv')\nrating = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/rating.csv')\ndf = movie.merge(rating, how=\"left\", on=\"movieId\")\ndf.head()","353dabe8":"# We reduce the dataset to these four movies in terms of both followability and performance..\nmovie_ids = [130219, 356, 4422, 541]\nmovies = [\"The Dark Knight (2011)\",\n          \"Cries and Whispers (Viskningar och rop) (1972)\",\n          \"Forrest Gump (1994)\",\n          \"Blade Runner (1982)\"]","05e05131":"sample_df = df[df.movieId.isin(movie_ids)]\nsample_df.shape\nsample_df.head()","7fd90e86":"# creating the user movie dataframe\nuser_movie_df = sample_df.pivot_table(index=[\"userId\"], columns=[\"title\"], values=\"rating\")\nuser_movie_df.head()","8327d301":"# The surprise library requires between which numbers it will be. We give 1-5 range.\nreader = Reader(rating_scale=(1, 5))\n\n# The data we created in accordance with the data structure of the surprise library\ndata = Dataset.load_from_df(sample_df[['userId', 'movieId', 'rating']], reader)\ntype(data)","405cee67":"trainset, testset = train_test_split(data, test_size=.25)\n\nsvd_model = SVD()\nsvd_model.fit(trainset)","a016bb53":"predictions = svd_model.test(testset)\n\n# predictions","10b00925":"accuracy.rmse(predictions)","c440da36":"cross_validate(svd_model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\nuser_movie_df.head()","989d7610":"# Let's guess blade runner( 541) for userid 1\nsvd_model.predict(uid=1.0, iid=541, verbose=True)","dda914e7":"# Let's guess Whispers (356) for userid 1. We guessed for the movie she didn't watc\nsvd_model.predict(uid=1.0, iid=356, verbose=True)","477664bf":"# here it is our parameters for the model, epoch and learning rate\nparam_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005]}\n\ngs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3, n_jobs=-1, joblib_verbose=True)\n\ngs.fit(data)\n\ngs.best_score['rmse']","6c3ab511":"gs.best_params['rmse']","23a22890":"svd_model = SVD(**gs.best_params['rmse'])\n\ndata = data.build_full_trainset()\nsvd_model.fit(data)","e6ca1938":"user_movie_df.head()","d9e121e8":"# Let's guess blade runner(541) for userid 1\nsvd_model.predict(uid=1.0, iid=541, verbose=True)","64a65dd3":"#  Another example Cries and Whispers (356) for user id1.\nsvd_model.predict(uid=1.0, iid=356, verbose=True)","44a4fb3f":"Let's put all the codes in a single script","9b4cf896":"<a id = \"5\"><\/a><br>\n### 1.1.5 LIBRARIES","4bcabc29":"<a id = \"16\"><\/a><br>\n#### 2.1.3 tf-idf \nThere are normalized numeric representations.\n\n* **STEP 1:** TF(t) = (Frequency of occurrence of a t term in the relevant document) \/ (Total number of terms in the document)(term frequency)\n* **Step 2:** IDF(t) = 1 + log_e(Total number of documents + 1) \/ (number of documents with t term + 1) (inverse document frequency)\n* **Step 3:** TF-IDF = TF(t) * IDF(t)\n* **Step 4:** L2 normalization to TF-IDF values","0abae6eb":"* If there is a product in the columns, we expect 1, if not 0.\n* Firstly, we will assign 0 to NaN values.","10af84e8":"<a id = \"17\"><\/a><br>\n## 2.2 Creating Cosine Similarity Matrix","ecc613d8":"<a id = \"23\"><\/a><br>\n### 3.1.2 Creating the User Movie Df","5caf3c98":"<a id = \"3\"><\/a><br>\n### 1.1.3 Dataset Story\n* The data set named Online Retail II is a UK-based online sale.\n* Store's sales between 01\/12\/2009 - 09\/12\/2011.\n* The product catalog of this company includes souvenirs. promotion can be considered as products.\n* There is also information that most of its customers are wholesalers..","558a8504":"<a id = \"26\"><\/a><br>\n### 3.2.1. Data Preprocessing \nWe have already defined user movie matrix above as named **user_movie_df**. We will use this data, instead of making all the data process again.","fe79a29a":"* We have 20000797 rows, 6 columns, we don't want to use all of them. let's take some sample from this data.\n* You can make the dataset smaller with this below code. It takes sample from orginal data. Frac is in ration of percentage you desire.\n* df = df.sample(frac =.50)","42b46d55":"* **antecedent support:** probability of the first product\n* **consequent support:** probability of the second product and others\n* **support:** probability of two products (or more) appearing together\n* **confidence:** when product x is bought, the probability of purchasing product y\n* **lift:** when x is taken, the probability of getting y increases by this much (lift)","474dbaeb":"<a id = \"25\"><\/a><br>\n## 3.2. User-Based Collaborative Filtering\nSuggestions are made based on user similarities.\n\n\n* Step 1: Preparing the Data Set\n* Step 2: Determining the Movies Watched by the User to Suggest\n* Step 3: Accessing Data and Ids of Other Users Watching the Same Movies\n* Step 4: Identifying Users with the Most Similar Behaviors to the User to Suggest\n* Step 5: Calculating the Weighted Average Recommendation Score","682ae6c5":"* The ouput indicates **Invoice 536527** has these above items. \n","d4b75a23":"<a id = \"32\"><\/a><br>\n### 3.3.1 Data Preprocessing","82ee73eb":"<a id = \"14\"><\/a><br>\n### 2.1.1 Libraries","91507f09":"<a id = \"10\"><\/a><br>\n## 1.3. Association Rules\n* We will subtract the probabilities of all possible products being together.","bc5c37fa":"<a id = \"4\"><\/a><br>\n### 1.1.4 Variables\n* **InvoiceNo**: Invoice number. The unique number of each transaction, namely the invoice. Aborted operation if it starts with C.\n* **StockCode**: Product code. Unique number for each product.\n* **Description**: Product name\n* **Quantity**: Number of products. It expresses how many of the products on the invoices have been sold.\n* **InvoiceDate**: Invoice date and time.\n* **UnitPrice**: Product price (in GBP)\n* **CustomerID**: Unique customer number\n* **Country**: The country where the customer lives.","f49e11ae":"<a id = \"20\"><\/a><br>\n# 3. Collaborative Filtering\n* Item-Based Collaborative Filtering\n* User-Based Collaborative Filtering\n* Model-Based Collaborative Filtering","2a2358e2":"Euclidean Distance \nIt finds the distance between two vectors.","642f2d49":"<a id = \"19\"><\/a><br>\n## 2.4 Functionalize All Code of Content-Based Filtering","a8b9cacc":"If want to see two product suggection","08d2f10a":"\u00dcr\u00fcn i\u00e7eriklerinin benzerlikleri \u00fczerinden tavsiyeler geli\u015ftirilir.","f091894c":"Cosine Similarity\nA metric focused on the similarity of two vectors.","08445e5e":"### Euclidean Distance\n$ d(p, q) = \\sqrt \\sum_{i = 1}^\\infty x_i$","e50af587":"<a id = \"33\"><\/a><br>\n### 3.3.2 Modelling","afa8ccfa":"<a id = \"18\"><\/a><br>\n## 2.3 Making Suggestions Based on Similarities","c32a55bc":"<a id = \"30\"><\/a><br>\n### 3.2.5. Calculating the Weighted Average Recommendation Score\n* We will create a single score by simultaneously considering the impact of the users most similar to James (correlation) and the rating.","aab26bd2":"<a id = \"11\"><\/a><br>\n## 1.4 Making Product Suggestions to Users at the Shopping Cart Stage","67335c76":"There are 8071 movies, we can't recommend all these movies. we need a trashold. We do not want to recommend **weighted_score** below **3.5**","d57afd57":"* Let's turn all these codes into a single function (named create_invoice_product_df).","ec3aa045":"<a id = \"28\"><\/a><br>\n### 3.2.3. Accessing Data and Ids of Other Users Watching the Same Movies","bcfe058c":"<a id = \"9\"><\/a><br>\n## 1.2 Preparing ARL Data Structure (Invoice-Product Matrix)","fbb87c4b":"#### Some Notes\n* For example, if I had 10,000 products, I wouldn't be interested in all of them. In this case it should be done at category level\n* When the person adds a product to the cart, what I will suggest should already be clear.\n* I know what to suggest with product X, but if the person has already bought this product, it is necessary to make a correction accordingly. There must be an intermediate control mechanism. At the database level, the userid should be checked. If the person has not bought that product after checking, it is necessary to recommend that product. This cannot be done. There is a fine line you should consider","f74f5ca1":"<a id = \"15\"><\/a><br>\n#### 2.1.2 CountVectorizer\nIn the Count operation, the number of times each word occurs in each document is counted. For instace; let's look at the example below. We have four sentences. We will convert all words to the matrix. If the word is in it, it will count 1 or more, othewise 0.","177daf9b":"<a id = \"24\"><\/a><br>\n### 3.1.3 Making Item-Based Movie Suggestions","ef020194":"<a id = \"27\"><\/a><br>\n### 3.2.2. Determining The Movies Watched By The User To Make A Suggestion","20423e72":"### Cosine Similarity","e625ad08":"# Introduction\nRecommender systems are a big part of our lives, recommending products and movies that we want to buy or watch. Recommender systems have been around for decades but have recently come into the spotlight. \n\nIn this notebook, We will discuss three types of recommender system: **(1)Association rule learning** (ARL), **(2)content-based** and **(3)collaborative filtering** approaches.  In this notebook, we will explain how to build a recommender system with these three methods.\n\nThe functions of a recommender system are to suggest things to the user based on a variety of criteria. These systems forecast the most probable product that customers would buy, and it is interesting to them. Netflix, Amazon, and other businesses employ recommendation algorithms to assist their clients in locating appropriate items or movies.\n\n<font color = 'blue'>\nContent: \n\n    \n1. [**Association Rule Learning - ARL**](#1)\n    * 1.1 [Data Preprocessing](#2)\n        * 1.1.2 [Business Problem](#3)\n        * 1.1.3 [Dataset Story](#4)\n        * 1.1.4 [Variables](#5)\n        * 1.1.5 [Libraries](#6)\n        * 1.1.6 [Load and Check Data](#7)\n        * 1.1.7 [Outlier Observations](#8)\n    * 1.2 [Preparing ARL Data Structure (Invoice-Product Matrix)](#9)\n    * 1.3 [Association Rules](#10)\n    * 1.4 [Making Product Suggestions to Users at the Shopping Cart Stage](#11)\n    \n1. [**Content-Based Filtering**](#12)\n    * 2.1 [Generating TF-IDF Matrix](#13)\n        * 2.1.1 [Libraries](#14)\n        * 2.1.2 [CountVectorizer](#15)\n        * 2.1.3 [tf-idf](#16)\n    * 2.2 [Creating Cosine Similarity Matrix](#17)\n    * 2.3 [Making Suggestions Based on Similarities](#18)\n    * 2.4 [Functionalize All Code of Content-Based Filtering](#19)\n    \n1. [**Collaborative Filtering**](#20)\n     * 3.1 [Item-Based Collaborative Filtering](#21)\n        * 3.1.1 [Data Preprocessing](#22)\n        * 3.1.2 [Creating the User Movie Df](#23)\n        * 3.1.3 [Making Item-Based Movie Suggestions](#24)\n     * 3.2 [User-Based Collaborative Filtering](#25)\n        * 3.2.1 [Data Preprocessing](#26)\n        * 3.2.2 [Determining The Movies Watched By The User To Make A Suggestion](#27)\n        * 3.2.3 [Accessing Data and Ids of Other Users Watching the Same Movies](#28)\n        * 3.2.4 [Identifying Users with the Most Similar Behaviors to the User to Suggest](#29)\n        * 3.2.5 [Calculating the Weighted Average Recommendation Score](#30)\n     * 3.3 [Model-Based Collaborative Filtering - Matrix Factorization](#31)\n        * 3.3.1 [Data Preprocessing](#32)\n        * 3.3.2 [Modelling](#33)\n        * 3.3.3 [Model Tuning](#34)\n        * 3.3.4 [Final Model and Prediction](#35)\n    \n1. [References](#36)\n    \n    \n\n\n\n","0bd1e555":"<a id = \"21\"><\/a><br>\n## 3.1 Item-Based Collaborative Filtering\n* Suggestions are made on item similarity.\n* For instance; there are movies that show the same liking structure as The Lord of The Rings movie.","3a26418d":"As we can see above, there are lots of people watched at least one movie in common, but we need to put a threshold here. Common movies count 50","9d6f9fed":"<a id = \"22\"><\/a><br>\n### 3.1.2 Data Preprocessing","f531e49f":"<a id = \"35\"><\/a><br>\n### 3.3.4 Final Model and Prediction","909bd149":"<a id = \"1\"><\/a><br>\n# 1. Association Rule Learning  \nAssociation rule learning is a rule-based machine learning approach for finding significant connections between variables in large databases. It is designed to identify strong rules that have been identified in databases using various measures of interest.\n\nOur aim is to suggest products to users in the product purchasing process by applying association analysis to the online retail II dataset.\n\n1. Data Preprocessing\n2. Preparing ARL Data Structure (Invoice-Product Matrix))\n3. Association Rules\n4. Making Product Suggestions to Users at the Shopping Cart Stage","b21a9656":"<a id = \"7\"><\/a><br>\n### 1.1.6 Load and Check Data ","0d551693":"* We want to see the description (products) in the columns.\n* And we want to see whether there are products at the intersections of the matrix or not. We can do this with unstack function, we could have used pivot funct instead of it.","cebab50f":"* We have a problem here. There are two levels in data. One is corr, other one is rating. Someone has high corr but rating is 1.0 with James, another person has low corr, but high rating. Which one should we consider? \n\n* I need to make a weighting based on rating and correlation.","93cd7f49":"User_id1 didn't give any rate for Cries and Whispers, but we predicted that userid1 would give 4.09.\n\nIf we apply this method for all the data, all NaN values are going to be filled by the predicted values","e7e2fc3f":"* Let's define a function for checking StockCode number","aaf948e9":"* Seconly, If there is a product in the columns, we will convert it to 1.","ccb50aa9":"<a id = \"12\"><\/a><br>\n# 2. Content-Based Filtering\n* Represent texts mathematically (vectoring texts)\n* Count Vector (word count)\n* TF-IDF\n\nIn this content-based section, we will go through these steps below.\n1. Creating the TF-IDF Matrix\n2. Creating the Cosine Similarity Matrix\n3. Making Suggestions Based on Similarities\n4. Preparation of the Working Script","d0892744":"* Let's check first stockcode's name **10002** ","057d7bbe":"* We will only work on Germany, let's filter. ","0c1280ed":"<a id = \"2\"><\/a><br>\n## 1.1 Data Preprocessing\n\n<a id = \"3\"><\/a><br>\n### 1.1.2 Business Problem\nTo suggest products to customers who have reached the basket stage.","18df382b":"<a id = \"34\"><\/a><br>\n### 3.3.3 Model Tuning\nThere is the problem of how long I will do the process of changing the values. Should I do this replacement 10 times, like 100 times? So this is a hyperparameter for me that needs to be optimized by the user. How many times will I update the weight? With this question, what will my learning speed be? There is a learning rate that represents the speed of these updates.","79702989":"Cosine similarities of The American President movie and other movies.","c9e43d03":"* There is one more problem this above table. There are many ratings given to a movie. We will use groupby for this problem.","87b3801d":"* It doesn't look good, let's make id = True in **create_invoice_product_df** for better looking matrix","7996698a":"<a id = \"29\"><\/a><br>\n### 3.2.4. Identifying Users with the Most Similar Behaviors to the User to Suggest\n* We will perform 3 steps:\n* 1. We will aggregate data from James and other users.\n* 2. We will create the correlation df.\n* 3. We will find the most similar finders (Top Users)","b654e1db":"<a id = \"36\"><\/a><br>\n# 4. References\n* https:\/\/github.com\/mvahit\n* https:\/\/www.veribilimiokulu.com\/\n* https:\/\/www.linkedin.com\/in\/vahitkeskin\/\n* https:\/\/www.analyticssteps.com\/blogs\/what-are-recommendation-systems-machine-learning\n* https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning","ea040e9c":"Example Product Id: 22492","9e6ae632":"* In this section, we will create a matrix of invoice and products as in the example below. ","21e5c3c2":"<a id = \"13\"><\/a><br>\n## 2.1 Generating TF-IDF Matrix\nRecommendation System Based on Movie Overviews","7e1056f1":"<a id = \"8\"><\/a><br>\n### 1.1.7 Outlier Observations","a42cba75":"<a id = \"31\"><\/a><br>\n## 3.3 Matrix Factorization\n* To fill in the blanks, the weights of the latent features that are assumed to exist for users and movles are found over the existing data and predictions are made for non-existent observations with these weights.\n\n* Matrix factorization assume; There are some latent factors when users like a movie. These hidden factors are also present in movies.\n\n1. Decomposes the user-item matrix into 2 less dimensional matrices.\n2. It assumes that the transition from two matrices to the user-item matrix occurs with latent factors. We will assume the latent variables.\n3. The weights of the latent factors are found on the filled observations.\n4. Empty observations are filled with the weights found.\n\n* The reason why James watches the movie; the genre of the film, the director or actors of the film, the duration of the film, the language in which the film was shot. While you are liking the movie, there are some factors that you are not aware of. These are called latent factors, features in machine learning.\n\n\n**add FORMULA OF MATRIX FAC.**\n\n* It's an optimization, it's actually a gradient descent work.\n\n\nLet's make it more clear.\n* It is assumed that the rating matrix is formed by the product of two factor matrices (dot product).\n* Factor matrices? User latent factors, movie latent factors, are actually two separate matrices.\n* Latent factors? Or latent features? Latent factors or variables.\n* Users and movies are considered to have scores for latent features.\n* These weights (scores) are first found on the existing data and then the empty sections are filled according to these weights.\n\n#### What are these factors in this data?\nComedy, horror, adventure, action, thriller, youth, having a specific actor, director.\n\nADD TABLES | PICTURE\n\n$ r_{11} = p_{11} * q_{11} + p_{12} * q_{21} $\n\n* All p and q are found iteratively over the existing values and then used.\n* Initially, random p and q values and the values in the rating matrix are tried to be estimated.\n* In each iteration, erroneous estimations are arranged and the values in the rating matrix are tried to be approached.\n* For example, if 5 is called 3 in one iteration, the next one is called 4, then 5 is called.\n* Thus, p and q values are filled as a result of a certain iteration.\n* Estimation is made for null observations based on the available p and q.\n\n#### Some Notes\n* Matrix Factorization vs SVD is not the same\n\n* SVD (Singular Value Decomposition) is a size reduction method.\n\n* ALS --> Spark ALS for big data. Only difference ALS make some changes on p and q values."}}