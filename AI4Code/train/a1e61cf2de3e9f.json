{"cell_type":{"aad27b9d":"code","869b272b":"code","0ae59988":"code","bdf0e02e":"code","aeafec22":"code","2d1fa223":"code","afbbe83e":"code","67fbe678":"code","0c30c06f":"code","659b9f30":"code","2fb34f93":"code","8117eee8":"code","f5c3a08d":"code","c0835052":"code","4a565411":"code","170e85db":"code","8b68b5af":"code","9cc9e463":"code","c1901bba":"code","62016af4":"code","24b86c34":"code","beaad183":"code","f0fc65b2":"code","65480a4f":"code","1888be0c":"code","471ea113":"code","863d43ae":"code","8c674168":"code","be7995d8":"code","8df87e3a":"code","0781a8b5":"code","e568d35c":"code","9a198f0d":"code","f9491f0c":"markdown","30d7a503":"markdown","6268a8e7":"markdown","70c2405e":"markdown","82a8756f":"markdown","846538fd":"markdown","a690cfe3":"markdown","a6475cee":"markdown"},"source":{"aad27b9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport gc\nimport os\nimport warnings\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom keras.layers import Input\nfrom keras.models import Model\n\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications import Xception\nfrom keras.applications.resnet50 import ResNet50, preprocess_input\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras import layers, models, optimizers\nfrom keras import regularizers\n#from keras.regularizers import l2\n\nfrom keras import backend as K\nwarnings.filterwarnings(action='ignore')\n%matplotlib inline\n\n\nimport PIL\nfrom PIL import ImageOps, ImageFilter, ImageDraw\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Any results you write to the current directory are saved as output.","869b272b":"DATA_PATH = '..\/input\/'\nos.listdir(DATA_PATH)","0ae59988":"# \uc774\ubbf8\uc9c0 \ud3f4\ub354 \uacbd\ub85c\nTRAIN_IMG_PATH = os.path.join(DATA_PATH, 'train')\nTEST_IMG_PATH = os.path.join(DATA_PATH, 'test')\n\n# CSV \ud30c\uc77c \uacbd\ub85c\ndf_train = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ndf_test = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\ndf_class = pd.read_csv(os.path.join(DATA_PATH, 'class.csv'))\n\nimage_size = 299\nbatch_size = 16","bdf0e02e":"df_train.head()","aeafec22":"df_test.head()","2d1fa223":"df_class.head()","afbbe83e":"df_train.shape, df_test.shape, df_class.shape","67fbe678":"# Train Data\uc758 class \ubd84\ud3ec \ud655\uc778 \nplt.figure(figsize=(12, 6))\nsns.countplot(df_train[\"class\"], order=df_train[\"class\"].value_counts(ascending=True).index)\nplt.title(\"Number of data per each class\")\nplt.show()","0c30c06f":"def crop_boxing_img(img_name, margin=16, size=(image_size, image_size)):\n    if img_name.split('_')[0] == 'train':\n        PATH = TRAIN_IMG_PATH\n        data = df_train\n    else:\n        PATH = TEST_IMG_PATH\n        data = df_test\n\n    img = PIL.Image.open(os.path.join(PATH, img_name))\n    pos = data.loc[data[\"img_file\"] == img_name, ['bbox_x1', 'bbox_y1', 'bbox_x2', 'bbox_y2']].values.reshape(-1)\n\n    width, height = img.size\n    x1 = max(0, pos[0] - margin)\n    y1 = max(0, pos[1] - margin)\n    x2 = min(pos[2] + margin, width)\n    y2 = min(pos[3] + margin, height)\n\n    return img.crop((x1, y1, x2, y2)).resize(size)","659b9f30":"# Train : validation data = 9 : 1\nTRAIN_CROPPED_PATH = \".\/train_crop\"\nVALID_CROPPED_PATH = '\/valid_crop'\nTEST_CROPPED_PATH = \".\/test_crop\"\n\nnb_train_sample = df_train.shape[0] * 0.9\nnb_validation_sample = df_train.shape[0] - nb_train_sample\nnb_test_sample = df_test.shape[0]","2fb34f93":"# Make Directory \nif (os.path.isdir(TRAIN_CROPPED_PATH) == False):\n    os.mkdir(TRAIN_CROPPED_PATH)\n\nif (os.path.isdir(VALID_CROPPED_PATH) == False):\n    os.mkdir(VALID_CROPPED_PATH)\n\nif (os.path.isdir(TEST_CROPPED_PATH) == False):\n    os.mkdir(TEST_CROPPED_PATH)","8117eee8":"# \ud074\ub798\uc2a4\ubcc4 \ub514\ub809\ud1a0\ub9ac\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574\uc11c class \ud615\uc744 \ubb38\uc790\uc5f4\ub85c \ubcc0\uacbd\ndf_train['class'] = df_train['class'].astype('str')","f5c3a08d":"for i, row in df_train.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    \n    if ( i < nb_train_sample):\n        class_path = os.path.join(TRAIN_CROPPED_PATH, df_train['class'][i])\n        if(os.path.isdir(class_path) == False):\n            os.mkdir(class_path)\n\n        cropped.save(os.path.join(class_path, row['img_file']))\n    else:\n        class_path = os.path.join(VALID_CROPPED_PATH, df_train['class'][i])\n        if(os.path.isdir(class_path) == False):\n            os.mkdir(class_path)\n\n        cropped.save(os.path.join(class_path, row['img_file']))\n\nfor i, row in df_test.iterrows():\n    cropped = crop_boxing_img(row['img_file'])\n    cropped.save(os.path.join(TEST_CROPPED_PATH, row['img_file']))","c0835052":"train_datagen = ImageDataGenerator(\n    # \uc124\uc815\uac12\uc744 \ubcc0\uacbd\ud558\uba74\uc11c \uc870\uc815\ud588\uace0 \uc544\ub798 \uac12\uc5d0\uc11c \uac00\uc7a5 \ub192\uc740 \uc810\uc218\uac00 \ub098\uc634.\n    rescale=1.\/255,\n    rotation_range=20.,\n    width_shift_range=0.5,\n    height_shift_range=0.5,\n    shear_range = 0.1,\n    horizontal_flip=True,\n    vertical_flip=False,\n    zoom_range=0.3,\n    fill_mode='reflect'\n    )\n\nvalid_datagen = ImageDataGenerator(rescale=1.\/255)\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\n\n# Make Generator\ntrain_generator = train_datagen.flow_from_directory(\n    TRAIN_CROPPED_PATH,\n    target_size=(image_size, image_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    seed=2019,\n    color_mode='rgb'\n)\n\nvalidation_generator = valid_datagen.flow_from_directory(\n    VALID_CROPPED_PATH,\n    target_size=(image_size,image_size),\n    batch_size=batch_size,\n    class_mode='categorical',\n    seed=2019,\n    color_mode='rgb'\n)\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=df_test,\n    directory=TEST_CROPPED_PATH,\n    x_col='img_file',\n    y_col=None,\n    target_size= (image_size,image_size),\n    color_mode='rgb',\n    class_mode=None,\n    batch_size=batch_size,\n    shuffle=False\n)","4a565411":"model_path = '..\/model\/'","170e85db":"# \ubaa8\ub378\uc5d0 \uc57d\uac04\uc758 fine-tunning\uc744 \ud574\uc11c \uc131\ub2a5\uc744 \ub192\uc784. \ndef get_model(application):\n    \n    input_layer = Input(shape=(image_size,image_size, 3))\n    base_model = application(weights='imagenet', include_top=False)(input_layer)\n\n    \n    x = layers.GlobalAveragePooling2D()(base_model)\n    x = layers.Dense(1024, activation='relu',\n                     bias_initializer='zeros',\n                     kernel_initializer='he_normal',\n                     kernel_regularizer=regularizers.L1L2(l1=0.001, l2=0.001))(x)\n    #model.add(layers.Dense(1024, activation='relu', bias_initializer='zeros', kernel_initializer='he_normal', kernel_regularizer=l2(0.001)))\n    \n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.5)(x)\n\n    output_layer = layers.Dense(196, activation='softmax')(x)    \n    model = Model(input_layer, output_layer)\n    model.summary()\n\n    #optimizer = optimizers.SGD(lr=1e-4, decay=1e-6, momentum=0.9, nesterov=True)\n    #optimizer = optimizers.RMSprop(lr=0.0001)\n    optimizer = optimizers.nadam(lr=0.0001)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n\n    return model\n","8b68b5af":"Xception_best_model = 'best_Xception_model.hdf5'\nResNet50_best_model = 'best_ResNet50_model.hdf5'","9cc9e463":"Xmodel = get_model(Xception)","c1901bba":"Rmodel = get_model(ResNet50)\n","62016af4":"if not os.path.exists(model_path):\n    os.mkdir(model_path)","24b86c34":"patient = 3\ncallbacks1 = [\n    EarlyStopping(monitor='val_loss', patience=patient, mode='min', verbose=1),\n    \n    # val_loss \uc774 \ud5a5\uc0c1\ub418\uc9c0 \uc54a\uc744 \ub54c \ud559\uc2b5\ub960\uc744 \uc791\uac8c \ud55c\ub2e4. (factor = 0.5 : \ucf5c\ubc31\uc774 \ud638\ucd9c\ub418\uba74 \ud559\uc2b5\ub960\uc744 50\ubc30\ub85c \uc904\uc784.)\n    #ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = patient \/ 2, min_lr=0.00001, verbose=1, mode='min'),\n    ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 1, min_lr=0.00001, verbose=1, mode='min'),\n    \n    # \ubaa8\ub378\uc744 \uc800\uc7a5\ud558\uace0 \ubcf5\uc6d0\ud558\uae30 \uc704\ud55c \ucf5c\ubc31 \ud568\uc218. (\ud6c8\ub828\ud558\ub294 \ub3d9\uc548 \uc5b4\ub5a4 \uc9c0\uc810\uc5d0\uc11c \ubaa8\ub378\uc758 \ud604\uc7ac \uac00\uc911\uce58\ub97c \uc800\uc7a5\ud55c\ub2e4.)\n    ModelCheckpoint(filepath=model_path+Xception_best_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min'),\n    ]\n\ncallbacks2 = [\n    EarlyStopping(monitor='val_loss', patience=patient, mode='min', verbose=1),\n    \n    # val_loss \uc774 \ud5a5\uc0c1\ub418\uc9c0 \uc54a\uc744 \ub54c \ud559\uc2b5\ub960\uc744 \uc791\uac8c \ud55c\ub2e4. (factor = 0.5 : \ucf5c\ubc31\uc774 \ud638\ucd9c\ub418\uba74 \ud559\uc2b5\ub960\uc744 50\ubc30\ub85c \uc904\uc784.)\n    #ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = patient \/ 2, min_lr=0.00001, verbose=1, mode='min'),\n    ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 1, min_lr=0.00001, verbose=1, mode='min'),\n    \n    # \ubaa8\ub378\uc744 \uc800\uc7a5\ud558\uace0 \ubcf5\uc6d0\ud558\uae30 \uc704\ud55c \ucf5c\ubc31 \ud568\uc218. (\ud6c8\ub828\ud558\ub294 \ub3d9\uc548 \uc5b4\ub5a4 \uc9c0\uc810\uc5d0\uc11c \ubaa8\ub378\uc758 \ud604\uc7ac \uac00\uc911\uce58\ub97c \uc800\uc7a5\ud55c\ub2e4.)\n    ModelCheckpoint(filepath=model_path+ResNet50_best_model, monitor='val_loss', verbose=1, save_best_only=True, mode='min'),\n    ]","beaad183":"def get_steps(num_samples, batch_size):\n    if (num_samples % batch_size) > 0 :\n        return (num_samples \/\/ batch_size) + 1\n    else :\n        return num_samples \/\/ batch_size","f0fc65b2":"history = Xmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=get_steps(nb_train_sample, batch_size) * 2,\n    epochs=100,\n    validation_data=validation_generator,\n    validation_steps=get_steps(nb_validation_sample, batch_size),\n    verbose=1,\n    callbacks = callbacks1\n)","65480a4f":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training Acc')\nplt.plot(epochs, val_acc, 'b', label='Validation Acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, 'bo', label='Traing loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Trainging and validation loss')\nplt.legend()\nplt.show()","1888be0c":"history = Rmodel.fit_generator(\n    train_generator,\n    steps_per_epoch=get_steps(nb_train_sample, batch_size) * 2,\n    epochs=100,\n    validation_data=validation_generator,\n    validation_steps=get_steps(nb_validation_sample, batch_size),\n    verbose=1,\n    callbacks = callbacks2\n)","471ea113":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training Acc')\nplt.plot(epochs, val_acc, 'b', label='Validation Acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, 'bo', label='Traing loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Trainging and validation loss')\nplt.legend()\nplt.show()","863d43ae":"labels = []\ntta_steps = 10\n\nXmodel.load_weights(model_path+Xception_best_model)\nRmodel.load_weights(model_path+ResNet50_best_model)","8c674168":"# TTA \ucd9c\ucc98 : https:\/\/towardsdatascience.com\/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d\npredictions = []\ntest_generator.reset()\n\nfor i in tqdm(range(tta_steps)):\n    Xception_prediction = Xmodel.predict_generator(\n        generator=test_generator,\n        steps = get_steps(nb_test_sample, batch_size),\n        verbose=1\n    )\n    test_generator.reset()\n    predictions.append(Xception_prediction)\n    \nfinal_prediction = predictions[0] + predictions[1] + predictions[2] + predictions[3] + predictions[4] + predictions[5] + predictions[6] + predictions[7] + predictions[8] + predictions[9]\nfinal_prediction \/= 10.0   \n\nlabels.append(final_prediction)","be7995d8":"predictions = []\ntest_generator.reset()\n\nfor i in tqdm(range(tta_steps)):\n    ResNet50_prediction = Rmodel.predict_generator(\n        generator=test_generator,\n        steps = get_steps(nb_test_sample, batch_size),\n        verbose=1\n    )\n    test_generator.reset()\n    predictions.append(ResNet50_prediction)\n\nfinal_prediction = predictions[0] + predictions[1] + predictions[2] + predictions[3] + predictions[4] + predictions[5] + predictions[6] + predictions[7] + predictions[8] + predictions[9]\nfinal_prediction \/= 10.0 \n    \nlabels.append(final_prediction)","8df87e3a":"final_prediction = labels[0] + labels[1]","0781a8b5":"final_prediction \/= 2.0","e568d35c":"predicted_class_indices=np.argmax(final_prediction, axis=1)\n\n# Generator class dictionary mapping\nlabels = (train_generator.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npredictions = [labels[k] for k in predicted_class_indices]\n\nsubmission = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\nsubmission[\"class\"] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","9a198f0d":"!rm -rf *_crop","f9491f0c":"### Model Training","30d7a503":"### Model \uc0dd\uc131","6268a8e7":"### 41th Solution\n\n \uac1c\uc778\uc801\uc73c\ub85c \ub9ce\uc740 \uac83\uc744 \ubc30\uc6b4 \ub300\ud68c\uc600\uc2b5\ub2c8\ub2e4. \n \ub9c9\ud310\uc5d0 \uc2dc\uac04\uc774 \ub9ce\uc774 \uc5c6\uc5b4 \ub354 \uac1c\uc120\ud558\uc9c0 \ubabb\ud55c \uc810\uc740 \uc544\uc26c\uc6c0\uc73c\ub85c \ub0a8\uc2b5\ub2c8\ub2e4. \n \ub9ce\uc740 \uace0\uc218\ubd84\ub4e4\uc758 \ub3c4\uc6c0\uc73c\ub85c 50\uc704 \uc548\uc5d0 \uc785\uc0c1\ud560 \uc218 \uc788\uc5c8\ub2e4\uace0 \uc0dd\uac01\ud558\uba70, \n \uc544\ub798 \ucc38\uace0\ucee4\ub110 \uc774\uc678\uc5d0\ub3c4 \ub9ce\uc740 \ucee4\ub110\uc744 \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4. \uadf8 \ubd84\ub4e4\uaed8 \uc774\uc790\ub9ac\ub97c \ube4c\ub824 \uac10\uc0ac\uc758 \uc778\uc0ac\ub97c \uc804\ud569\ub2c8\ub2e4. \n \uadf8\ub9ac\uace0 \uc6b4\uc601\uc9c4 \uc5ec\ub7ec\ubd84 \uc218\uace0\ud558\uc168\uc2b5\ub2c8\ub2e4. \n \uc88b\uc740 \ub300\ud68c \uacc4\ucd5c\ud574 \uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4. \n\n##### \uc0ac\uc6a9\ud55c \uae30\ubc95\n > Image cropping & \uc81c\ub110\ub808\uc774\uc158, TTA, \ubaa8\ub378\uc559\uc0c1\ube14 \n   (KFold\ub97c \uc801\uc6a9\ud574 \ubcf4\uc9c0 \ubabb\ud55c \uac74 \uc544\uc26c\uc6c0\uc73c\ub85c \ub0a8\ub124\uc694..)\n\n##### *\ucc38\uace0\ucee4\ub110\nhttps:\/\/www.kaggle.com\/fulrose\/3rd-ml-month-car-model-classification-baseline\nhttps:\/\/www.kaggle.com\/tmheo74\/3rd-ml-month-car-image-cropping-updated-7-10\nhttps:\/\/www.kaggle.com\/easter3163\/3rd-ml-month-keras-efficientnet ","70c2405e":"### Predict","82a8756f":"### Generator\n\n\uc774\ubbf8\uc9c0 \ud559\uc2b5\uac04\uc5d0\ub294 \uac70\uc758 \ud544\uc218\uc791\uc5c5, \ubc30\uce58 \uc0ac\uc774\uc988 \ub2e8\uc704 \ub9cc\ud07c\ub9cc \ud30c\uc77c\uc744 \ubd88\ub7ec\uc640\uc11c \ud559\uc2b5\uc774 \ub05d\ub098\uba74 \ub2e4\uc2dc \ubd88\ub7ec\uc640 \ud559\uc2b5\uc744 \ubc18\ubcf5\ud558\uae30\uc5d0 \uba54\ubaa8\ub9ac\ub97c \uc808\uc57d\ud560 \uc218 \uc788\ub2e4.\n\n\n\n#### keras\uc758 ImageDataGenerator\ub97c \uc774\uc6a9\ud558\uc5ec \ub370\uc774\ud130 \uc99d\uc2dd\uc744 \ud569\ub2c8\ub2e4.\n\n - rotation_range : \ub79c\ub364\ud558\uac8c \uc0ac\uc9c4\uc744 \ud68c\uc804\uc2dc\ud0ac \uac01\ub3c4 \ubc94\uc704\n - width_shift_range : \uc0ac\uc9c4 \uc218\ud3c9\uc774\ub3d9 \ube44\uc728\n - height_shift_range : \uc0ac\uc9c4 \uc218\uc9c1\uc774\ub3d9 \ube44\uc728\n - horizontal_flip : \uc0ac\uc9c4 \uc218\ud3c9\uc73c\ub85c \ub4a4\uc9d1\uae30\n - zoom_range : \ub79c\ub364\ud558\uac8c \uc0ac\uc9c4 \ud655\ub300\n - fill_mode : \uc0ac\uc9c4\uc744 \ud68c\uc804\/\uc774\ub3d9\ud55c\ud6c4\uc5d0 \uc0c8\ub86d\uac8c \uc0dd\uc131\ud574\uc57c\ud560 \ud53d\uc140\uc744 \ucc44\uc6b8 \ubc29\ubc95","846538fd":"### Cropped Image Dataset\n\n\uc81c\uacf5\ub41c \ubc14\uc778\ub529 \ubc15\uc2a4 x1(\uc88c\uc0c1\ub2e8), y1(\uc88c\uc0c1\ub2e8), x2(\uc6b0\ud558\ub2e8), y2(\uc6b0\ud558\ub2e8) \uc815\ubcf4\ub97c \uc774\uc6a9\ud574 \ubc30\uacbd\uc744 \uc798\ub77c\ub0b8\ub2e4.","a690cfe3":"#### \ud6c8\ub828\ub370\uc774\ud130 \uc800\uc7a5\n\n\ud6c8\ub828\ud558\uba74\uc11c \uac80\uc99d\ub370\uc774\ud130\uc5d0 \ub300\ud574\uc11c val_loss\uac00 \uac00\uc7a5 \uc791\uc740 \uacbd\uc6b0\uc5d0 \ub300\ud574\uc11c weight \uc800\uc7a5. \n\uc774 \ud30c\uc77c\uba85\uc744 \uc798 \uad00\ub9ac\ud574 \ub450\uc5c8\ub2e4\uac00 \ub098\uc911\uc5d0 \uc559\uc0c1\ube14\uc5d0 \uc774\uc6a9\ud560 \uc608\uc815\uc784. best_model.hd5\ub85c \uc800\uc7a5\uc744 \ud558\ub2e4\uac00 \ud6c8\ub828\uc774 \ub05d\ub098\uba74 bestmodel(\uc0ac\uc804\ud6c8\ub828\ucee4\ube0c\ub137).hd5\ub85c \uc800\uc7a5.","a6475cee":"### Submission"}}