{"cell_type":{"131693e9":"code","2468d3e6":"code","49d3d727":"code","6e96e582":"code","50961c07":"code","12eff8ac":"code","87cfdfcc":"code","b84001c4":"code","6123dcb3":"code","f1958ad3":"code","f4236ef3":"code","2a3ce8d7":"code","f48cc40e":"code","1e2e3c3a":"code","8e590b36":"code","00ceea5f":"code","80bf521f":"code","98b752c9":"code","ba11ad61":"code","57952b6e":"code","ec2e5d48":"code","3c0bfbce":"code","e532e82a":"code","55d91e64":"code","c6d26c11":"code","58a99be1":"code","b7130534":"code","b0b29c50":"code","a32996c4":"code","247d3a52":"code","b4444def":"code","f59d9217":"code","350004bf":"code","283b216f":"code","308ea1b5":"code","34bdaaaf":"code","9949a678":"code","a98f3820":"code","fdb3f7c2":"code","f2d06830":"code","7c1132b1":"code","82c9eec7":"markdown","29516576":"markdown","cf24bead":"markdown","9380e4cd":"markdown","464749ac":"markdown","1071b289":"markdown","85d06335":"markdown","3ed8c6b9":"markdown","766dacae":"markdown","48a654b5":"markdown","32a93daa":"markdown"},"source":{"131693e9":"### Import required libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\n","2468d3e6":"%%time\n# Read train and test files\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\ngc.collect()","49d3d727":"%%time\n# training set\nprint (\"Training set:\")\nn_data  = len(train_df)\nn_features = train_df.shape[1]\nprint (\"Number of Records: {}\".format(n_data))\nprint (\"Number of Features: {}\".format(n_features))\n\n# testing set\nprint (\"\\nTesting set:\")\nn_data  = len(test_df)\nn_features = test_df.shape[1]\nprint (\"Number of Records: {}\".format(n_data))\nprint (\"Number of Features: {}\".format(n_features))\ngc.collect()","6e96e582":"X_train = train_df.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(train_df[\"target\"].values)\n\nX_test = test_df.drop([\"ID\"], axis=1)","50961c07":"print(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\nif (train_df.columns[train_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","12eff8ac":"zero_count = []\nfor col in X_train.columns[2:]:\n    zero_count.append([i[1] for i in list(X_train[col].value_counts().items()) if i[0] == 0][0])\n    \nprint('{0} features of 4491 have zeroes in 99% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.99])))\nprint('{0} features of 4491 have zeroes in 98% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.98])))\nprint('{0} features of 4491 have zeroes in 97% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.97])))\nprint('{0} features of 4491 have zeroes in 96% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.96])))\nprint('{0} features of 4491 have zeroes in 95% or more samples.'.format(len([i for i in zero_count if i >= 4459 * 0.95])))\n\ncols_to_drop = [col for col in X_train.columns[2:] if [i[1] for i in list(X_train[col].value_counts().items()) if i[0] == 0][0] >= 4459 * 0.98]\n\nX_train.drop(cols_to_drop, axis=1, inplace=True)\nX_test.drop(cols_to_drop, axis=1, inplace=True)\n\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","87cfdfcc":"colsToRemove = []\nfor col in X_train.columns:\n    if X_train[col].std() == 0: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\ntrain_df.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\ntest_df.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","b84001c4":"def drop_sparse(train, test):\n    flist = [x for x in train.columns if not x in ['ID','target']]\n    for f in flist:\n        if len(np.unique(train[f]))<2:\n            train.drop(f, axis=1, inplace=True)\n            test.drop(f, axis=1, inplace=True)\n    return train, test","6123dcb3":"X_train, X_test = drop_sparse(X_train, X_test)\n\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","f1958ad3":"def add_OtherAgg(train, test, features):\n    flist = [x for x in train.columns if not x in ['ID','target','SumZeros','SumValues']]\n    if 'OtherAgg' in features:\n        train['Mean'] = train.mean(axis=1)\n        train['Median'] = train.median(axis=1)\n        train['Mode'] = train.mode(axis=1)\n        train['Max'] = train.max(axis=1)\n        train['Var'] = train.var(axis=1)\n        train['Std'] = train.std(axis=1)\n        \n        test['Mean'] = test.mean(axis=1)\n        test['Median'] = test.median(axis=1)\n        test['Mode'] = test.mode(axis=1)\n        test['Max'] = test.max(axis=1)\n        test['Var'] = test.var(axis=1)\n        test['Std'] = test.std(axis=1)\n    flist = [x for x in train.columns if not x in ['ID','target','SumZeros','SumValues']]\n\n    return train, test","f4236ef3":"def kmeans(X_Tr,Xte):\n    flist = [x for x in X_Tr.columns if not x in ['ID','target']]\n    flist_kmeans = []\n    for ncl in range(2,11):\n        cls = KMeans(n_clusters=ncl)\n        cls.fit_predict(X_train[flist].values)\n        X_Tr['kmeans_cluster_'+str(ncl)] = cls.predict(X_Tr[flist].values)\n        Xte['kmeans_cluster_'+str(ncl)] = cls.predict(Xte[flist].values)\n        flist_kmeans.append('kmeans_cluster_'+str(ncl))\n    print(flist_kmeans)\n    \n    return X_Tr,Xte","2a3ce8d7":"def pca(X_Tr,Xte):\n    flist = [x for x in X_Tr.columns if not x in ['ID','target']]\n    n_components = 20\n    flist_pca = []\n    pca = PCA(n_components=n_components)\n    x_train_projected = pca.fit_transform(normalize(X_Tr[flist], axis=0))\n    x_test_projected = pca.transform(normalize(X_test[flist], axis=0))\n    for npca in range(0, n_components):\n        X_Tr.insert(1, 'PCA_'+str(npca+1), x_train_projected[:, npca])\n        Xte.insert(1, 'PCA_'+str(npca+1), x_test_projected[:, npca])\n        flist_pca.append('PCA_'+str(npca+1))\n    print(flist_pca)","f48cc40e":"print('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))","1e2e3c3a":"from sklearn.decomposition import PCA, TruncatedSVD, FastICA\nfrom sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\nPERC_TRESHOLD = 0.98   ### Percentage of zeros in each feature ###\nN_COMP = 20            ### Number of decomposition components ###\n\nprint(\"\\nStart decomposition process...\")\nprint(\"PCA\")\npca = PCA(n_components=N_COMP, random_state=17)\npca_results_train = pca.fit_transform(X_train)\npca_results_test = pca.transform(X_test)\n\nprint(\"tSVD\")\ntsvd = TruncatedSVD(n_components=N_COMP, random_state=17)\ntsvd_results_train = tsvd.fit_transform(X_train)\ntsvd_results_test = tsvd.transform(X_test)\n\nprint(\"ICA\")\nica = FastICA(n_components=N_COMP, random_state=17)\nica_results_train = ica.fit_transform(X_train)\nica_results_test = ica.transform(X_test)\n\nprint(\"GRP\")\ngrp = GaussianRandomProjection(n_components=N_COMP, eps=0.1, random_state=17)\ngrp_results_train = grp.fit_transform(X_train)\ngrp_results_test = grp.transform(X_test)\n\nprint(\"SRP\")\nsrp = SparseRandomProjection(n_components=N_COMP, dense_output=True, random_state=17)\nsrp_results_train = srp.fit_transform(X_train)\nsrp_results_test = srp.transform(X_test)\n\nprint(\"Append decomposition components to datasets...\")\nfor i in range(1, N_COMP + 1):\n    X_train['pca_' + str(i)] = pca_results_train[:, i - 1]\n    X_test['pca_' + str(i)] = pca_results_test[:, i - 1]\n\n    X_train['ica_' + str(i)] = ica_results_train[:, i - 1]\n    X_test['ica_' + str(i)] = ica_results_test[:, i - 1]\n\n    X_train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n    X_test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n\n    X_train['grp_' + str(i)] = grp_results_train[:, i - 1]\n    X_test['grp_' + str(i)] = grp_results_test[:, i - 1]\n\n    X_train['srp_' + str(i)] = srp_results_train[:, i - 1]\n    X_test['srp_' + str(i)] = srp_results_test[:, i - 1]\nprint('\\nTrain shape: {}\\nTest shape: {}'.format(X_train.shape, X_test.shape))\n","8e590b36":"print(X_train.shape, y_train.shape)\nprint(X_test.shape)","00ceea5f":"# Find feature importance\nclf_gb = GradientBoostingRegressor(random_state = 42)\nclf_gb.fit(X_train, y_train)\nprint(clf_gb)","80bf521f":"# GradientBoostingRegressor feature importance - top 100\nfeat_importances = pd.Series(clf_gb.feature_importances_, index=X_train.columns)\nfeat_importances = feat_importances.nlargest(100)\nplt.figure(figsize=(16,15))\nfeat_importances.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","98b752c9":"# GradientBoostingRegressor feature importance - top 25\nfeat_importances_gb = pd.Series(clf_gb.feature_importances_, index=X_train.columns)\nfeat_importances_gb = feat_importances_gb.nlargest(100)\nplt.figure(figsize=(16,8))\nfeat_importances_gb.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","ba11ad61":"print(pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(100))","57952b6e":"# Find feature importance\nclf_rf = RandomForestRegressor(random_state = 42)\nclf_rf.fit(X_train, y_train)\nprint(clf_rf)","ec2e5d48":"# RandomForestRegressor feature importance - top 25\nfeat_importances_rf = pd.Series(clf_rf.feature_importances_, index=X_train.columns)\nfeat_importances_rf = feat_importances_rf.nlargest(100)\nplt.figure(figsize=(16,8))\nfeat_importances_gb.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","3c0bfbce":"print(pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(10))","e532e82a":"plt.figure()\nfig, ax = plt.subplots(1, 2, figsize=(16,6))\nfeat_importances_gb.plot(kind='barh', ax=ax[0])\nfeat_importances_rf.plot(kind='barh', ax=ax[1])\nax[0].invert_yaxis()\nax[1].invert_yaxis()\nplt.show()","55d91e64":"s1 = pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(10).index\ns2 = pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(10).index\n\ncommon_features = pd.Series(list(set(s1).intersection(set(s2)))).values\n\nprint(common_features)","c6d26c11":"df_plot = X_train[['f190486d6', 'eeb9cd3aa', '58e2e02e6', '58232a6fb', '15ace8c9f', '9fd594eec']]\ndf_plot['target'] = y_train\n\ng = sns.pairplot(df_plot, diag_kind=\"kde\", palette=\"BuGn_r\")\ng.fig.suptitle('Pairplot of Top 6 Important Features',fontsize=26)","58a99be1":"# PLot Correlation HeatMap for top 20 features from GB and RF Models\ns1 = pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(100).index\ns2 = pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(100).index\n\ncommon_features = pd.Series(list(set(s1).union(set(s2)))).values\n\nprint(common_features)\nprint(len(common_features))","b7130534":"df_plot = pd.DataFrame(X_train, columns = common_features)\ncorr = df_plot.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 16))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Correlation HeatMap\", fontsize=15)\nplt.show()","b0b29c50":"X_train = pd.DataFrame(X_train, columns = common_features)\nX_test = pd.DataFrame(X_test, columns= common_features)","a32996c4":"X_train.shape,y_train.shape","247d3a52":"dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","b4444def":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n#     params = {\n#         \"objective\" : \"regression\",\n#         \"metric\" : \"rmse\",\n#         \"num_leaves\" : 40,\n#         \"learning_rate\" : 0.005,\n#         \"bagging_fraction\" : 0.7,\n#         \"feature_fraction\" : 0.6,\n#         \"bagging_frequency\" : 6,\n#         \"bagging_seed\" : 42,\n#         \"verbosity\" : -1,\n#         \"seed\": 42\n#     }\n    params = {\n    'task': 'train',\n    'boosting_type': 'dart',\n    'objective': 'regression',\n    'metric': 'rmse',\n    \"learning_rate\": 0.01,\n    \"num_leaves\": 200,\n    \"feature_fraction\": 0.50,\n    \"bagging_fraction\": 0.50,\n    'bagging_freq': 4,\n    \"max_depth\": -1,\n    \"reg_alpha\": 0.3,\n    \"reg_lambda\": 0.1,\n    #\"min_split_gain\":0.2,\n    \"min_child_weight\":10,\n    'zero_as_missing':True\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgtrain, lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=150, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","f59d9217":"# Training LGB# Traini \npred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"LightGBM Training Completed...\")","350004bf":"print(\"Features Importance...\")\ngain = model.feature_importance('gain')\nfeatureimp = pd.DataFrame({'feature':model.feature_name(), \n                   'split':model.feature_importance('split'), \n                   'gain':100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\nprint(featureimp[:15])","283b216f":"def run_xgb(train_X, train_y, val_X, val_y, test_X):\n#     params = {'objective': 'reg:linear', \n#           'eval_metric': 'rmse',\n#           'eta': 0.001,\n#           'max_depth': 10, \n#           'subsample': 0.6, \n#           'colsample_bytree': 0.6,\n#           'alpha':0.001,\n#           'random_state': 42, \n#           'silent': True}\n    params = {'objective': 'reg:linear', 'eval_metric': 'rmse', 'eta': 0.005, 'max_depth': 10, 'subsample': 0.7, 'colsample_bytree': 0.5, 'alpha':0, 'silent': True, 'random_state':5}\n    \n    tr_data = xgb.DMatrix(train_X, train_y)\n    va_data = xgb.DMatrix(val_X, val_y)\n    \n    watchlist = [(tr_data, 'train'), (va_data, 'valid')]\n    \n    model_xgb = xgb.train(params, tr_data, 2000, watchlist, maximize=False, early_stopping_rounds = 100, verbose_eval=100)\n    \n    dtest = xgb.DMatrix(test_X)\n    xgb_pred_y = np.expm1(model_xgb.predict(dtest, ntree_limit=model_xgb.best_ntree_limit))\n    \n    return xgb_pred_y, model_xgb","308ea1b5":"# Training XGB\npred_test_xgb, model_xgb = run_xgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"XGB Training Completed...\")","34bdaaaf":"from sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor","9949a678":"# cb_model = CatBoostRegressor(iterations=500,\n#                              learning_rate=0.05,\n#                              depth=10,\n#                              eval_metric='RMSE',\n#                              random_seed = 42,\n#                              bagging_temperature = 0.2,\n#                              od_type='Iter',\n#                              metric_period = 50,\n#                              od_wait=20)\ncb_model = CatBoostRegressor(iterations=1000, learning_rate=0.05, depth=10, l2_leaf_reg=20, bootstrap_type='Bernoulli', eval_metric='RMSE', metric_period=50, od_type='Iter', od_wait=45, random_seed=42, allow_writing_files=False)","a98f3820":"cb_model.fit(dev_X, dev_y,\n             eval_set=(val_X, val_y),\n             use_best_model=True,\n             verbose=True)","fdb3f7c2":"pred_test_cat = np.expm1(cb_model.predict(X_test))","f2d06830":"sub = pd.read_csv('..\/input\/sample_submission.csv')\n\nsub_lgb = pd.DataFrame()\nsub_lgb[\"target\"] = pred_test\n\nsub_xgb = pd.DataFrame()\nsub_xgb[\"target\"] = pred_test_xgb\n\nsub_cat = pd.DataFrame()\nsub_cat[\"target\"] = pred_test_cat\n\nsub[\"target\"] = (sub_lgb[\"target\"] + sub_xgb[\"target\"] + sub_cat[\"target\"])\/3","7c1132b1":"print(sub.head())\nsub.to_csv('final.csv', index=False)","82c9eec7":"## Santander Value Prediction Challenge\n\nIn this competition, Santander Group is asking us to help them identify the value of transactions for each potential customer. We are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column. Our task is to predict the value of target column in the test set.\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error. The data set consists of train.csv and test.csv\n\n\n![Santander](https:\/\/camo.githubusercontent.com\/6b4418509c3424e4770808066e762fc4d8780b82\/68747470733a2f2f64796e6c2e6d6b746763646e2e636f6d2f702f4f696f50446b696a555342656858506f356e43435f4345642d30685a6b5a527639342d48486e4a6a2d65412f32333236783833322e6a7067)","29516576":"#### GradientBoostingRegressor vs RandomForestRegressor Top 25 Features","cf24bead":"The above plot looks very cluttered. Instead, we will take a look at the top 25 features.","9380e4cd":"#### Load Data","464749ac":"So, we can see that there are a total of just 6 common features in top 25 features of RandomForestRegressor and GradientBoostingRegressor. So, we should be careful in choosing what feature sto pick for further analysis.","1071b289":"#### Data summary","85d06335":"#### Data Visualization\n\nBelow we will see some visualizations related to the top features.","3ed8c6b9":"* Below we will list the top 100 features with their feature importances.","766dacae":"#### Load Libraries","48a654b5":"#### Feature Importance from RandomForestRegressor","32a93daa":"#### Correlation HeatMap"}}