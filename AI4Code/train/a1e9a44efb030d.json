{"cell_type":{"0ec575c5":"code","9d83ac94":"code","36914847":"code","1685f62a":"code","d75b9f8b":"code","8a82e472":"code","6a172900":"code","8b733031":"code","9a7f8533":"code","0db17832":"code","7ecea70d":"code","a68970c9":"code","b5b904bb":"code","1f64ab90":"code","3a6de13b":"code","c3850764":"code","2cc2fa7d":"code","fc81c89b":"code","3c8af11f":"code","7cb36619":"code","254e9f87":"code","213e522c":"code","18ea68ca":"code","4323f9d5":"code","c51f0913":"code","25835b9b":"code","0cc122b8":"code","d335c6db":"code","ca13bc89":"code","8a6494a0":"code","90fc6d6b":"code","4780e016":"code","af279281":"code","b43ad78f":"code","2df0540b":"code","49c21976":"code","ef63ef01":"code","2739caab":"code","030112ef":"code","d1bcebf9":"code","44bd8c2c":"code","4116e368":"code","0a7e5e59":"code","fe6756a5":"code","bc4ab525":"code","2c5cb5bf":"markdown","56aa6301":"markdown","5cde82c9":"markdown","6844ea5c":"markdown","8abb756f":"markdown","e54ffc08":"markdown","d5728beb":"markdown","f0ef63f2":"markdown","0284cf9c":"markdown","44c15231":"markdown","ccd744b4":"markdown","4897b8f7":"markdown","4c9d5836":"markdown","64a0f03e":"markdown","29c1e100":"markdown","68324d9b":"markdown","1a74f9c4":"markdown","3097b3c7":"markdown","f9d8a9c9":"markdown","84a3cbde":"markdown","9e438085":"markdown","f9f606a9":"markdown","9dd7dd2b":"markdown","68e90303":"markdown","442bc853":"markdown","2708af0c":"markdown","d5bbd40f":"markdown","4e45f8a3":"markdown","80ec08da":"markdown","fc0ba268":"markdown","515d8d7a":"markdown","1bbd0543":"markdown","958ddec0":"markdown","43ec4f07":"markdown","bb5dd744":"markdown","1fa3a406":"markdown","e4867eed":"markdown","82a45b3a":"markdown"},"source":{"0ec575c5":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set(style=\"darkgrid\")\nimport cufflinks as cf\n\nimport plotly.offline as py \nimport plotly.figure_factory as ff \nimport plotly.graph_objs as go \n\nfrom plotly import tools \nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot, plot \n\npy.init_notebook_mode(connected= True)\ncf.go_offline()\n\nimport warnings\nwarnings.filterwarnings('ignore')","9d83ac94":"# Importing the Data set \ndf = pd.read_csv(\"..\/input\/loan.csv\", low_memory=False)","36914847":"df.head(3)","1685f62a":"data = pd.read_excel(\"..\/input\/LCDataDictionary.xlsx\")\ndata","d75b9f8b":"df.info()","8a82e472":"# Ploting the data types in the data set\ndtyp = df.dtypes.value_counts()\n\nplt.figure(figsize= (15,5))\nsns.barplot (x = dtyp.index, y = dtyp.values)","6a172900":"df.describe()","8b733031":"df.drop([\"policy_code\", \"id\", \"member_id\", \"emp_title\",\"url\"]\n        , axis = 1, inplace=True)","9a7f8533":"# Returns the data frame of the repective data types\nnumb_cols = df.select_dtypes(include=(\"int64\",\"float64\"))\n\ncat_cols = df.select_dtypes(include=(\"object\"))","0db17832":"purp = cat_cols[\"purpose\"].value_counts()\nemp_len = df[\"emp_length\"].value_counts()\nstates = df[\"addr_state\"].value_counts()","7ecea70d":"#df.count()[\"loan_status\"]","a68970c9":"def uni_item(col):\n    out = []\n    for item in col:\n        if item not in out:\n            out.append(item)\n            pass\n    return(out)","b5b904bb":"us_states = uni_item(df[\"addr_state\"])","1f64ab90":"df[\"loan_status\"].value_counts()","3a6de13b":"term = uni_item(df[\"term\"])\ngrade = uni_item(df[\"grade\"])\nhome = uni_item(df[\"home_ownership\"])\nveri = uni_item(df[\"verification_status\"])\nloan_stat = uni_item(df[\"loan_status\"])\nsub_grade = uni_item(df[\"sub_grade\"])","c3850764":"cat_cols.columns","2cc2fa7d":"data = []\nfor i in loan_stat:\n    data.append(go.Box(y = df[df[\"loan_status\"]==i][\"int_rate\"], name = i))\n    \nlayout = go.Layout(title = 'Interested Rate based on Loan Status', \n                   xaxis = dict(title = 'Status'), \n                   yaxis = dict(title = 'Interest Rate'))\nfig = dict(data = data, layout = layout)\npy.iplot(fig)\n","fc81c89b":"data1= []\nfor i in grade:\n    data1.append(go.Box(y = df[df[\"grade\"]==i][\"int_rate\"], name = i))\n    \nlayout = go.Layout(title = 'Interested Rate based on Grades', \n                   xaxis = dict(title = 'Grades'), \n                   yaxis = dict(title = 'Interest Rate'))\nfig = dict(data = data1, layout = layout)\npy.iplot(fig)\n","3c8af11f":"data2= []\nfor i in sub_grade:\n    data2.append(go.Box(y = df[df[\"sub_grade\"]==i][\"int_rate\"], name = i))\n    \nlayout = go.Layout(title = 'Interested Rate based on Sub Grades', \n                   xaxis = dict(title = 'Sub Grades'), \n                   yaxis = dict(title = 'Interest Rate'))\nfig = dict(data = data2, layout = layout)\npy.iplot(fig)\n","7cb36619":"df.groupby(\"term\").agg({\"int_rate\":np.mean, \"loan_amnt\": np.mean}, axis = 1).reset_index()","254e9f87":"trace1 = go.Bar(x = emp_len.index, y = emp_len.values, \n                  marker= dict(color = df[\"int_rate\"], colorscale = \"Earth\"))\n\n    \ntrace2 = go.B(x = purp.index, y = purp.values, \n                  marker= dict(color = df[\"int_rate\"], colorscale = \"YlGnBu\"))\n   \nfig = tools.make_subplots(rows =2, cols=1 , subplot_titles=('Employement Length','Reasons'))\n\nfig.append_trace(trace1, 1,1)\nfig.append_trace(trace2, 2,1)\n\nfig[\"layout\"].update(height = 900, width = 1000,  showlegend = False)\n\npy.iplot(fig)","213e522c":"colors = ['lime', 'navy', 'red', 'lightgreen', 'pink', 'lightblue', 'purple']\n\ntrace1 = go.Pie(labels = purp.index, values = purp.values,name= \"Purpose\", \n                    hole= .7, textposition= \"inside\",\n                    \n                    domain= dict(x = [0, .48]), \n                  \n                    marker=dict(colors=colors))\n    \n    \ntrace2 = go.Pie(labels = emp_len.index, values = emp_len.values,name= \"Employement Length\", \n                    hole= .7, textposition= \"inside\",\n                    \n                    domain= dict(x = [.51, 1]), \n                  \n                    marker=dict(colors=colors))\n\n \n    \nlayout = go.Layout(title = \"title\",annotations = [dict(text = \"Distributions\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = 0, y = 1),\n                                           dict(text = \"title2\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .8,y = 1\n                                               ) ] )\ndata = [trace1, trace2 ]                                              \n    \nfig = go.Figure(data = data, layout = layout)\n  \npy.iplot(fig)","18ea68ca":"def func_scatter(col1, col2, title = \"Main title\", xtitle = \"X title\", y = \"Y title\", color = \"red\"):\n    trace = go.Scatter( x= col1, y = col2, mode = \"m\", marker = dict(color = color))\n    layout = go.Layout(title = title)\n\n    data = [trace]\n\n    fig = go.Figure(data = data, layout=layout)\n\n    py.iplot(fig)\n","4323f9d5":"month_group = df.groupby(\"issue_d\").agg({\"int_rate\":np.mean, \"loan_amnt\":np.mean}).reset_index()\n\nmonth_group[\"issue_d\"] = pd.to_datetime(month_group[\"issue_d\"])\n\nmonth_group= month_group.sort_values(by=\"issue_d\")","c51f0913":"trace = go.Scatter(x = month_group[\"issue_d\"],  y = month_group[\"int_rate\"],mode='markers',\n                   marker = dict(color = (\"red\")))\n\nlayout = go.Layout(title = \"Average Interest rate\")\n\ndata = [trace]\n\nfig = go.Figure(data = data, layout=layout)\n\npy.iplot(fig)\n\n'''plt.figure(figsize=(20,10))\n\nsns.pointplot(x = month_group[\"issue_d\"].sort_values(), y = month_group[\"int_rate\"], data=month_group,\n             markers='x', color=\"g\")\n\nplt.xticks(rotation = 90)'''\n","25835b9b":"trace = go.Scatter(x = month_group[\"issue_d\"],  y = month_group[\"loan_amnt\"],mode='markers',\n) \n\nlayout = go.Layout(title = \"Monthly Average Loan Amount Applied\")\n\ndata = [trace]\n\nfig = go.Figure(data = data, layout=layout)\n\npy.iplot(fig)\n\n'''\nplt.figure(figsize=(20,10))\n\nsns.pointplot(x = month_group[\"issue_d\"], y = month_group[\"loan_amnt\"], data=month_group, \n             color=\"r\")\n\nplt.xticks(rotation = 90)'''","0cc122b8":"plt.scatter(x = df[\"annual_inc\"], y=df[\"int_rate\"], color=\"maroon\")","d335c6db":"total = np.product(df.shape) # total will be 65666046\nmiss_values = df.isnull().sum().sum() # Total missing values are 17998490","ca13bc89":"per_total = (miss_values\/total)*100\nprint(f\"The total percentage of missing values in data set is {(per_total)}\")","8a6494a0":"print(df.isnull().any().value_counts(), \"\\n\")\n\nprint(f\"The columns that have missing values are total {df.isnull().any().sum()}\")","90fc6d6b":"total_num = df.isnull().sum().sort_values(ascending=False)\n\nperc = df.isnull().sum()\/df.isnull().count() *100\nperc1 = (round(perc,2).sort_values(ascending=False))\n\n# Creating a data frame:\ndf_miss = pd.concat([total_num, perc1], axis =1 , keys =[\"Total Missing Values\", \"Percentage %\"]).sort_values(by =\"Percentage %\", ascending = False)\n","4780e016":"top_mis = df_miss[df_miss[\"Percentage %\"]>0]\ntop_mis.reset_index(inplace=True)\ntop_mis","af279281":"plt.figure(figsize=(15,5))\nsns.heatmap(df.isnull(), cbar = False, yticklabels=False, cmap=\"magma\" )","b43ad78f":"null_perc = df.isnull().sum()\/df.isnull().count()\n\nplt.figure(figsize=(15,5))\n\nsns.barplot(x = np.arange(len(null_perc)), y =null_perc, palette=\"magma\")\n\nplt.xticks(np.arange(len(null_perc)),null_perc.index,rotation=90)\nplt.ylabel('% missing data')","2df0540b":"# Creating a bar chart: \n\ntrace = go.Bar(y = top_mis[\"index\"], x = top_mis[\"Percentage %\"], \n                marker=dict(color=\"#d6a5ff\",\n            line=dict(color='rgb(107, 107, 107)', width=1)),\n            orientation='h' )\n\nlayout = go.Layout(\n    height=500,\n    autosize=True,\n    title = \"Missing Values\",\n    hovermode='closest',\n    xaxis=dict(title='', ticklen=5, zeroline=False, gridwidth=2, domain=[0.2, 1]),\n    yaxis=dict(title='Percentage %', ticklen=5, gridwidth=10),\n    \n)\n\ndata = [trace]\n\nfig = go.Figure(data = data, layout=layout)\npy.iplot(fig)","49c21976":"df.drop(top_mis[top_mis[\"Percentage %\"]>75][\"index\"], axis = 1, inplace=True)","ef63ef01":"df.drop([\"next_pymnt_d\",\"mths_since_last_delinq\"]\n        , axis = 1, inplace=True)","2739caab":"'''def ind_corr(cols):\n    for col in cols:\n        print(df.corr()[col].sort_values(ascending =False).head(3), \"\\n\")\nind_corr(num_col.columns)'''","030112ef":"#il_util 1.000000 -- all_util 0.569145 \n\n#open_rv_12m 1.000000 -- open_rv_24m 0.767262 -- open_acc_6m 0.626833\n\n#annual_inc_joint 1.000000 -- annual_inc 0.711091\n\n#open_acc_6m 1.000000 -- open_rv_12m 0.626833\n\n#total_pymnt 1.000000 -- total_pymnt_inv 0.997592\n\n#total_rec_prncp 0.970043 #out_prncp 1.000000 -- out_prncp_inv       0.999997\n\n#recoveries 1.000000 -- collection_recovery_fee 0.802420\n\n#revol_util 1.000000 -- all_util           0.657901\n\n#revol_bal 1.000000 -- total_rev_hi_lim    0.821189\n\n#open_acc 1.000000 -- total_acc           0.695075\n\n#annual_inc 1.000000 -- annual_inc_joint    0.711091\n\n#installment 1.000000 -- funded_amnt 0.946005 -- loan_amnt 0.944977 -- funded_amnt_inv 0.943632","d1bcebf9":"# Therefore, In order to avoid multicolinearity. We have to drop furhter columns:total_rev_hi_lim\ndf.drop([\"loan_amnt\",\"funded_amnt_inv\", \"total_acc\", \"total_rev_hi_lim\",\n         \"collection_recovery_fee\", \"out_prncp_inv\",\"total_rec_prncp\", \"total_pymnt_inv\",\n         \"annual_inc\",  ], \n        axis = 1, inplace = True)","44bd8c2c":"n_miss_tot = df.isnull().sum().sort_values(ascending = False)\nn_perc = df.isnull().sum()\/df.isnull().count()*100\nn_perc1 = (round(n_perc, 2).sort_values(ascending = False))\nn_miss_df = pd.concat([n_miss_tot, n_perc1], axis = 1, keys=([\"total\", \"Percentage\"])).sort_values(\n    by =\"Percentage\", ascending = False).head(8)\nn_miss_df","4116e368":"df.drop([\"last_pymnt_d\", \"title\",\"last_credit_pull_d\", \"earliest_cr_line\", \"issue_d\", \"addr_state\",\n        \"zip_code\"], axis =1 , inplace = True)","0a7e5e59":"df[\"tot_cur_bal\"] = df[\"tot_cur_bal\"].fillna(df[\"tot_cur_bal\"].mean())\n\ndf[\"tot_coll_amt\"] = df[\"tot_coll_amt\"].fillna(0)\n\ndf[\"emp_length\"]=df[\"emp_length\"].fillna(0)\n\ndf[\"revol_util\"]= df[\"revol_util\"].fillna(df[\"revol_util\"].mean())\n\ndf[\"collections_12_mths_ex_med\"]= df[\"collections_12_mths_ex_med\"].fillna(df[\"collections_12_mths_ex_med\"].mean())\n\ndf[\"acc_now_delinq\"]=df[\"acc_now_delinq\"].fillna(df[\"acc_now_delinq\"].mode())\n\ndf[\"delinq_2yrs\"]=df[\"delinq_2yrs\"].fillna(0)\n\ndf[\"inq_last_6mths\"]= df[\"inq_last_6mths\"].fillna(0)\n\ndf[\"pub_rec\"]= df[\"pub_rec\"].fillna(0)\n\ndf[\"acc_now_delinq\"]= df[\"acc_now_delinq\"].fillna(0)\n\ndf[\"open_acc\"]= df[\"open_acc\"].fillna(df[\"open_acc\"].mean())","fe6756a5":"tot_cel = df.isnull().sum().sum()\nprint(tot_cel)\nplt.figure(figsize=(15,4))\nsns.heatmap(df.isnull(), cbar = False, yticklabels=False, cmap=\"magma\" )","bc4ab525":"No outliears in the data according to my analysis. ","2c5cb5bf":"**Step 1** Droping all the columns which have missing values more than 80% and the redundant columns","56aa6301":"## 4.2 Outliers:","5cde82c9":"### 2.a Distribution of the Data","6844ea5c":"We will be droping all the columns which have missing values more than 80% in our next step. However, it is always recommended to do some research about the specific features. And, come up with hypothesis that why they were not recorded or they don't exists? \n\nIn addition to that, there are several ways to replace them (imputation) and other techniques to fill up the minor missing values with potential statistics techniques.\n\nI will be going through some basic ones during the next stage.","8abb756f":" **Step 1** \nLet's check what is the overal percentage of missing values in our data set. To do that first we have to  count the total missing values and then divide those by total number of cells. ","e54ffc08":"**Step 4** Filling up missing values","d5728beb":"### 2.c Creating variables based on the data data types:\n","f0ef63f2":"The percent of missing values in our entire data set is **27.401 %**","0284cf9c":"#### Out of curiosity total 112 people have an interest rate of 28.89! ","44c15231":"##### 3 data types in the data. ","ccd744b4":"## 3. Exploratory Data Analysis","4897b8f7":"### Let's take a deep look at all the columns remained with missing values. \n\n    \u2022 tot_cur_bal:\tTotal current balance of all accounts\n    \n    \u2022 tot_coll_amt:\tTotal collection amounts ever owed\n    \n    \u2022 emp_length: \tEmployment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.\n    \n    \u2022 last_pymnt_d:\tLast month payment was received\n    \n    \u2022 revol_util: Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.\n    \n    \u2022 title: The loan title provided by the borrower.\n  \n    \u2022 collections_12_mths_ex_med: Number of collections in 12 months excluding medical collections\n    \n    \u2022 last_credit_pull_d: The most recent month LC pulled credit for this loan\n\n","4c9d5836":"###### Above heatmap shows how many values are missing in every columns. All the light colored columns represents the amount of missing values present in that specific column. ","64a0f03e":"## 1. Importing the Libraries ","29c1e100":"#### Above table shows our which features has maximum missing values in descending order. ","68324d9b":"**40 columns** have missing values ","1a74f9c4":"#### Looks like we are done with the missing values: ","3097b3c7":"## 4.2 Feature Engineering and Encoding","f9d8a9c9":"Every one has their own pipeline\/struction which they follow. The mentioned below one is mine.\n## Structure of the code book: \n#### 1. Importing the potential Libraries \n#### 2. Importing and Understanding the Data Set: \n    a. Distribtuion of the the data types  \n    b. Statistics analysis \n    c. Identifying all the numberical and Categorical columns in the data  \n#### 3. Exploratory Data Analysis\n    a. Correlation matrix for all numberical columns\n    b. Univariate Plots \n    c. Bivariate Plots\n    d. Multivariate Plots\n\n### 4. Data Preprocessing\n    a. Missing Values\n    b. Outliers\n    c. Feature Engineering \n    d. Categorical Encoding \n    e. Features Scaling \n    f. Cross-Validation\n    \n###  5. Models Selection \n\n###  6. Model Evaluation \n    \n    ","84a3cbde":"Dropping columns based on their redundancy.","9e438085":"### In order to avoid multicolinearity. We will deleting columns which are highly correlated to one and another. \n\n**Step 2** Droping columns based on multicolinearity","f9f606a9":"# 4. Data Preprocessing ","9dd7dd2b":"**Step 3** \n\nVisualising the missing values in each column for better understanding:","68e90303":"#### For total currentl balance I will be replacing the missing values with the mean of the column. \n\n##### I am not sure what is the best way to fill the missing values in Total Collection Amount columns but I feel it is related to the recoveries columns. However for our first try I will be filling the value with the 0. As, I believe mojority of the borrowers never owed any amount.  \n\n#####  It seems like all the missing values in column emp_length is because of the fact that the borrower has Employement length less than 0  Therefore, we will filling the missing values with the 0. \n\n#####  Last payment date column is also not important for us to predict the interest rate for a give user .  Therefore, we will be dropping this collumns aswell. \n\n###### All the rest columns have very less missing values. Therfore, any practice of filling the missing values wont make a huge difference\n\n##### revol_util: Filling the values with mean\n\n##### Title column is also an extension of purpose columns. Therefore, we wont be needing it aswell. \n\n##### Total collection amounts ever owed: with mean \n\n##### last_credit_pull_d: Will be droped.  ","442bc853":"## This Kernel is under active upgradation\n\n\n\n\nIn this report I will be predicting the interest rate based on the given data. The data provided is between 2007 to 2016. ","2708af0c":"Above, I made 2 new varaibles consisting categorical features in cat_cols and numerical features in num_cols. This will help me to manipulate the data in a flexible manner. In addition, having an understanding of the numbers of categorical features in data set can  be usefull in Bivariate Plots. ","d5bbd40f":"### So, what we have observed this far? \n#### There are total ***73 features in the data*** and our ***target variable is interest rate column***. The data has 3 data types and have maximum float values. Aroud 35% of the data is missing which I will get in more details during the Data Preprocessing Stage. \n#### Our target variabel (interest rate) has following statistics: \n  1. **Minimum value: 5.320000**\n  2. **Maximum Value: 28.99 (WOW!)**\n  3. **Mean: 13.246740**\n  4. **And Standard Deviation of 4.381867**","4e45f8a3":"we have **887379 number of observations** and **74 number of columns**","80ec08da":"#### Therefore, based on last invetigation we will be dropping the columns aswell: ","fc0ba268":"#### Looks like a lot of features are **highly correlated with each and other**. Therefore let's get in more depth and remove all the features who are hihly correlated one and other in order to **multicolinearity and overfitting our model with redundant features. ","515d8d7a":"### 4.2. Handling Missing Values: \n","1bbd0543":"**Step 2** \nCreating a Data Frame of the columns having missing values with the percentage and total number of misisng values.","958ddec0":"## Please feel free to leave a comment or any feedback you would like to give. \n### Thank you. ","43ec4f07":"#### How many columns have missing values? ","bb5dd744":"### 2. b. Brief Statistical Analysis ","1fa3a406":"## 2. Importing and Understanding the Rectangular Data \n","e4867eed":"# 4.1. Missing Values ","82a45b3a":"Let's checkk if we have succesfully filled all the missing values. "}}