{"cell_type":{"a1a959f8":"code","745d4867":"code","9ce6a584":"code","48d4c596":"code","e5d4594b":"code","67373b50":"code","cf57b6e5":"code","5d648a8e":"code","bc7324ee":"code","202b8ea2":"code","7f3f7f71":"code","40d44105":"code","010f0780":"code","d7be3c51":"code","d3b245e1":"code","a34931cc":"code","fa527eeb":"code","18581673":"code","bef47b94":"code","009f6b77":"code","fc79bb6a":"code","69348fd0":"code","6423966e":"code","3814562e":"code","5d7bba2f":"code","8fb22938":"code","d6357871":"code","ad9e11c9":"code","9ef5509e":"code","7841491a":"code","4ba5334a":"code","eb419d3b":"code","11d38c67":"code","276715ba":"code","c877c6b3":"code","75cba7a4":"code","1094d290":"code","1d25b490":"code","e597ec75":"code","4ca70719":"code","10e24079":"code","e4fe74cc":"code","311863c6":"code","9b905ad8":"code","e67d83b1":"code","7215e2b9":"code","b790cd45":"code","c5fd90cd":"markdown","823220d8":"markdown","44f9745d":"markdown","a2b14af6":"markdown","e844db2a":"markdown","d674fa9f":"markdown","6d3d5e97":"markdown","b9ea979f":"markdown","70bcc4b5":"markdown","95b13800":"markdown","57a5d230":"markdown","f6f5fcc0":"markdown","871cc8c7":"markdown","0a261855":"markdown","565c9dba":"markdown","db960781":"markdown","45c39fba":"markdown","2f4c3b9f":"markdown","97a1f8cc":"markdown","2a7cf06a":"markdown","235b7c49":"markdown","49e4103e":"markdown","485b5bee":"markdown","d18f39be":"markdown","580146cd":"markdown","a98527e8":"markdown","21c7c70d":"markdown","71a65263":"markdown","9857a5fd":"markdown","cbef6591":"markdown","24f7290d":"markdown","4951638c":"markdown","73091cd6":"markdown","9e31fd75":"markdown","129b4fda":"markdown","0f2822f3":"markdown","d0988d8e":"markdown","376039ff":"markdown","22028110":"markdown","14095384":"markdown","ee0d48bc":"markdown","535bca47":"markdown","338a4996":"markdown"},"source":{"a1a959f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import LabelEncoder,Normalizer\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom lightgbm import LGBMClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score,r2_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom itertools import combinations\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nos.chdir(\"\/kaggle\/working\/\")\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","745d4867":"titanic_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")","9ce6a584":"titanic_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","48d4c596":"titanic_train.head()","e5d4594b":"titanic_test.head()","67373b50":"plt.rcParams['figure.figsize'] = (10,3)\nplot = sns.countplot(x = 'Survived', data=titanic_train)\nplt.xlabel('whether passenger survived?')\nplt.ylabel('# of passengers')\nplt.title('Survivors Count')\nfor txt in plot.texts:\n    txt.set_visible(False)","cf57b6e5":"df = titanic_train[['Sex','Survived']]\ngrouped_df = df.groupby('Sex')\nsurvivor_percent = grouped_df.sum().values.T*100\/grouped_df.apply(len).values\nsurvivor_percent = survivor_percent[0]\ngender = list(grouped_df.sum().index)\nplt.rcParams['figure.figsize'] = (10,3)\nplot = plt.barh(gender,survivor_percent,color=['orange','green'])\nplt.title('Gender-wise Survival Percentage')\nplt.xlabel('Survival %')","5d648a8e":"title_df = titanic_train[['Name','Survived']]\ntitle_df['Title'] = title_df['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())\ngrouped_df = title_df.groupby('Title')\nsurvivor_percent = grouped_df.sum().values.T*100\/grouped_df.apply(len).values\nsurvivor_percent = survivor_percent[0]\ntitles = list(grouped_df.sum().index)\nplt.rcParams['figure.figsize'] = (10,3)\nplt.barh(titles,survivor_percent,color=['orange','green','blue','red','pink','lime','yellow','brown','grey'])\nplt.xlabel('Survival %')\nplt.ylabel('Title')\nplt.title('Title of Passengers vs Survival %')","bc7324ee":"df = titanic_train[['Name','Parch','SibSp','Survived']]\nalone_df = df[(df['Parch'] == 0) & (df['SibSp'] == 0)]\nnot_alone_df = df[(df['Parch'] != 0) | (df['SibSp'] != 0)]\nsurvival_percent = []\npassenger_type = ['Without Family Members','With Family Members']\nfor df_type in [alone_df,not_alone_df]:\n    survival_percent.append(df_type['Survived'].sum()*100\/len(df))\nplt.barh(passenger_type,survival_percent,color=['green','orange'])\nplt.xlabel('Survival %')\nplt.title('Survival % comparsion of passengers with and without family members aboard the ship')","202b8ea2":"df = titanic_train[['Fare','Survived','Ticket','Pclass']]\nprint(\"The minimum fare of ticket is: \",min(list(df['Fare'])))\nprint(\"The maximum fare of ticket is:\",max(list(df['Fare'])))","7f3f7f71":"price_interval = [(0,128.25),(128.25,256.5),(256.5,384.75),(384.75,513)]\nticket_type = ['Cheap','Decent','Fairly High','Costly']\nsurvivor_percent = []\nfor interval in price_interval:\n    fare_df = df[(df['Fare']>=interval[0]) & (df['Fare']<interval[1])]\n    percent = fare_df['Survived'].sum()*100\/len(fare_df)\n    survivor_percent.append(percent)\nplt.subplot(1,2,1)\nplt.pie(survivor_percent,autopct='%1.1f%%',explode=(0,0,0,0.1))\nplt.legend([tk_type+': Survivor %: '+str(round(pct,2)) \n           for tk_type,pct in zip(ticket_type,survivor_percent)],bbox_to_anchor=(1,0))\nplt.title('Survivor % based on price of tickets bought by passengers')\nplt.subplot(1,2,2)\nticket_class_df = df[['Pclass','Survived']]\ngrouped_df = ticket_class_df.groupby('Pclass')\nsurvivor_percent = grouped_df.sum().T.values[0]*100\/grouped_df.apply(len).values\nclass_list = list(grouped_df.sum().index)\nplt.barh(class_list,survivor_percent,color=['orange','green','blue'])\nplt.title('Survivor % based on ticket class bought by passengers')\nplt.xlabel('Survivor %')\nplt.ylabel('Ticket Class')","40d44105":"df = titanic_train[['Cabin','Survived']]\ndf['Cabin'] = df.fillna('Unknown')['Cabin']\ngroup_df = df.groupby('Cabin')\ncabin_no = list(group_df.sum().index)\nsurvivor_percent = group_df.sum().values.T[0]*100\/group_df.apply(len).values\ncabin_df = pd.DataFrame({'Cabin no.':cabin_no,'Survivor %':survivor_percent})\nunknown = len(df[df['Cabin']=='Unknown'])*100\/len(df)\nprint(\"Percentage of Unknown Cabins:\",unknown)\nplt.rcParams['figure.figsize'] = (20,3)\nplot = sns.scatterplot(data = cabin_df,x = 'Cabin no.',y = 'Survivor %')\nplt.xticks(rotation=90)\nfor txt in plot.texts:\n    txt.set_visible(False)","010f0780":"df = titanic_train[['Embarked','Survived']]\ngrouped_df = df.groupby('Embarked')\nembarked_port = list(grouped_df.sum().index)\nfullname_dict = {'S':'Southampton','C':'Cherbourg','Q':'Queenstown'}\nsurvivor_percent = grouped_df.sum().values.T[0]*100\/grouped_df.apply(len)\nplt.barh([fullname_dict[port] for port in embarked_port],survivor_percent,\n        color=['orange','green','red'])\nplt.xlabel('Survivor %')\nplt.title('Port of Embarkation vs Survival %')","d7be3c51":"titanic_train['Title'] = titanic_train['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())\ntitanic_test['Title'] = titanic_test['Name'].apply(lambda x:x.split(',')[1].split('.')[0].strip())","d3b245e1":"new_train = titanic_train.drop(['PassengerId','Name'],axis=1)","a34931cc":"print(\"Percentage of null values in different columns:\")\nnew_train.isnull().sum()*100\/len(new_train)","fa527eeb":"new_train = new_train.drop(['Survived','Cabin'],axis=1)","18581673":"new_test = titanic_test.drop(['PassengerId','Name'],axis=1)\nprint(\"Percentage of null values in different columns:\")\nnew_test.isnull().sum()*100\/len(new_test)","bef47b94":"new_test = new_test.drop('Cabin',axis=1)","009f6b77":"new_train['data'] = ['train']*len(new_train)\nnew_test['data'] = ['test']*len(new_test)\ntrain_test_data = pd.concat([new_train,new_test])\ntrain_test_data.index = range(len(train_test_data))\nprint('Percentage of null columns in the combined data:')\ntrain_test_data.isnull().sum()*100\/len(train_test_data)","fc79bb6a":"categorical_features = ['Ticket','Pclass','Sex','Embarked','Title']\nencoded_train_test = train_test_data.copy()\nencoded_train_test.drop('data',inplace=True,axis=1)\n\n#Filling Missing Embarked Column with its Mode, Fare Column with Median\nencoded_train_test['Embarked'] = encoded_train_test.fillna(encoded_train_test['Embarked'].mode()[0])['Embarked']\nencoded_train_test['Fare'] = encoded_train_test.fillna(encoded_train_test['Fare'].median())['Fare']\n#One-hot Encoding\nfor i in categorical_features:\n    encoded_train_test = pd.concat([encoded_train_test, pd.get_dummies(encoded_train_test[i],prefix=i)],axis=1).drop(i,axis=1)\n#Splitting into test data containing null values and train data containing non-null values of Age \ntrain_before_pca = encoded_train_test.copy()\n\n#Outlier Treatment through transformation: Refer the Outlier treatment section \n#below on why I chose these functions\n\ntrain_before_pca['Age'] = np.cbrt(train_before_pca['Age']-30)\ntrain_before_pca['Fare'] = np.cbrt(np.log(train_before_pca['Fare']+5))\ntrain = train_before_pca[train_before_pca['Age'].notna()]\nX_train = train.drop('Age',axis=1)\nY_train = train['Age']\ntest = train_before_pca[train_before_pca['Age'].isna()]\nX_test = test.drop('Age',axis=1)\n\n#Applying PCA to determine no. of features for the given threshold\npca_train = PCA()\nX_train_pca = pca_train.fit_transform(X_train)\nvariance_percent = pca_train.explained_variance_ratio_*100\nthreshold = 80\ntotal_percent = 0\nfeature_count = 0\nfor percent in variance_percent:\n    if(total_percent<threshold):\n        total_percent+=percent\n        feature_count+=1\nprint(\"Required no. of features to achieve \"+str(threshold)+\"% of total variance:\",feature_count)\n\n#For train data having ages values\npca_train = PCA(feature_count)\nX_train_pca = pca_train.fit_transform(X_train)\ncol_list = []\nfor i in range(1,X_train_pca.shape[1]+1):\n    col_list.append('PCA'+str(i))\nX_train_pca = pd.DataFrame(data = X_train_pca,columns = col_list)\n\n#For test data not having ages values\npca_test = PCA(feature_count)\nX_test_pca = pca_train.fit_transform(X_test)\nX_test_pca = pd.DataFrame(data = X_test_pca,columns = col_list)\n\n#Displaying train R2-score\nrf = RandomForestRegressor(random_state = 1)\nrf.fit(X_train_pca,Y_train)\nprint(\"Train R2 score for Random Forest: \", r2_score(Y_train,rf.predict(X_train_pca)))\n\n#Filling missing Age values\npredicted_missing_ages = np.power(rf.predict(X_test_pca),3)+30\nnull_age_indices = list(X_test.index)\nencoded_train_test.loc[null_age_indices,'Age'] = predicted_missing_ages\nencoded_train_test['data'] = list(train_test_data['data'])   ","69348fd0":"plt.subplot(1,2,1)\nsns.boxplot(encoded_train_test['Age'])\nplt.subplot(1,2,2)\nsns.boxplot(encoded_train_test['Fare'])","6423966e":"plt.subplot(1,2,1)\nsns.boxplot(np.cbrt(encoded_train_test['Age']-30))\nplt.subplot(1,2,2)\nsns.boxplot(np.cbrt(np.log(encoded_train_test['Fare']+5)))","3814562e":"encoded_train_test['Age'] = np.cbrt(encoded_train_test['Age']-30)\nencoded_train_test['Fare'] = np.cbrt(np.log(encoded_train_test['Fare']+5))","5d7bba2f":"encoded_train_test.head()","8fb22938":"train_test_before_pca = encoded_train_test.iloc[:,:-1]\npca = PCA()\ntrain_test_after_pca = pca.fit_transform(train_test_before_pca)\nvariance_percent = pca.explained_variance_ratio_*100\nthreshold = 90\ntotal_percent = 0\nfeature_count = 0\nfor percent in variance_percent:\n    if(total_percent<threshold):\n        total_percent+=percent\n        feature_count+=1\nprint(\"Required no. of features to achieve \"+str(threshold)+\"% of total variance:\",feature_count)\npca = PCA(feature_count)\ntrain_test_after_pca = pca.fit_transform(train_test_before_pca)\ncol_list = []\nfor i in range(1,train_test_after_pca.shape[1]+1):\n    col_list.append('PCA'+str(i))\ntrain_test_after_pca = pd.DataFrame(data = train_test_after_pca,columns = col_list)","d6357871":"train_test_after_pca['data'] = list(encoded_train_test['data'])","ad9e11c9":"train_after_pca = train_test_after_pca[train_test_after_pca['data']=='train'].iloc[:,:-1]\ntest_after_pca = train_test_after_pca[train_test_after_pca['data']=='test'].iloc[:,:-1]\ntrain_label = titanic_train['Survived']","9ef5509e":"X_train,X_test,Y_train,Y_test = train_test_split(train_after_pca,train_label,test_size=0.3,random_state=0)","7841491a":"def best_model_param(model,params_dict):\n    best_param_dict = 0\n    best_score = 0\n    param_values = list(params_dict.values())\n    param_keys = list(params_dict.keys())\n    param_values_flatten = []\n    [param_values_flatten.extend(val) for val in param_values]\n    comb_list = list(combinations(param_values_flatten,len(param_keys)))\n    comb_param_values = []\n    for comb in comb_list:\n        temp = []\n        for param in param_values: \n            temp.append(len(set(comb).intersection(set(param))))\n        if temp==[1]*len(param_keys):\n            comb_param_values.append(comb)\n    for comb in comb_param_values:\n        model = model.set_params(**dict(zip(param_keys,comb)))\n        model.fit(X_train,Y_train)\n        score = accuracy_score(Y_test,model.predict(X_test))\n        if(score>best_score):\n            best_score = score\n            best_param_dict = dict(zip(param_keys,comb))\n    return best_param_dict","4ba5334a":"#param_dict = {\"n_estimators\":[50,100,200],\n#              \"max_features\" :list(range(1,len(list(X_train.columns))+1)),\n#              \"random_state\":[10]}\n#rf = RandomForestClassifier()\n#best_param_dict = best_model_param(rf,param_dict)\n#print(best_param_dict)\nrf = RandomForestClassifier(n_estimators = 100, max_features = 10, random_state = 10)","eb419d3b":"#param_dict = {\"C\":np.arange(0.1,1.1,0.1),\"max_iter\":range(100,1100,100)}\n#log_reg = LogisticRegression()\n#best_param_dict = best_model_param(log_reg,param_dict)\n#print(best_param_dict)\nlog_reg = LogisticRegression(C=0.9,max_iter=100)","11d38c67":"#param_dict = {\"learning_rate\":[0.001,0.01,0.1,1],\"n_estimators\":[50,100,200],\"random_state\":[10]}\n#lgbm = LGBMClassifier()\n#best_param_dict = best_model_param(lgbm,param_dict)\n#print(best_param_dict)\nlgbm = LGBMClassifier(learning_rate = 0.01,n_estimators = 200,random_state = 10)","276715ba":"#param_dict = {\"C\":[0.01,0.1,1,10],\"gamma\":[0.01,0.1,1,10]}\n#svc = SVC()\n#best_param_dict = best_model_param(svc,param_dict)\n#print(best_param_dict)\nsvc = SVC(C=1,gamma=1)","c877c6b3":"#param_dict = {\"max_features\" :list(range(1,len(list(X_train.columns))+1)),\n#              \"random_state\":[10]}\n#dtc = DecisionTreeClassifier()\n#best_param_dict = best_model_param(dtc,param_dict)\n#print(best_param_dict)\ndtc = DecisionTreeClassifier(max_features=10,random_state=10) ","75cba7a4":"#param_dict = {\"activation\":[\"logistic\",\"tanh\",\"relu\"],\n#              \"solver\":[\"adam\"],\n#              \"hidden_layer_sizes\":[(100,),(1000,),(10000,)]}\n#mlp = MLPClassifier()\n#best_param_dict = best_model_param(mlp,param_dict)\n#print(best_param_dict)\nmlp = MLPClassifier(activation='logistic',solver='adam',hidden_layer_sizes = (1000,))","1094d290":"rf.fit(X_train,Y_train)\nlog_reg.fit(X_train,Y_train)\nnaive_bayes_model = GaussianNB()\nnaive_bayes_model.fit(X_train,Y_train)\nlgbm.fit(X_train,Y_train)\nsvc.fit(X_train,Y_train)\ndtc.fit(X_train,Y_train)\nmlp.fit(X_train,Y_train)","1d25b490":"Y_train_pred = rf.predict(X_train)\nY_test_pred = rf.predict(X_test)\nprint(\"For Random Forest:\")\nprint(\"Train set accuracy score:\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test set accuracy score:\",accuracy_score(Y_test,Y_test_pred))","e597ec75":"Y_train_pred = lgbm.predict(X_train)\nY_test_pred = lgbm.predict(X_test)\nprint(\"For Light GBM:\")\nprint(\"Train set accuracy score:\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test set accuracy score:\",accuracy_score(Y_test,Y_test_pred))","4ca70719":"Y_train_pred = log_reg.predict(X_train)\nY_test_pred = log_reg.predict(X_test)\nprint(\"For Logistic Regression:\")\nprint(\"Train set accuracy score:\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test set accuracy score:\",accuracy_score(Y_test,Y_test_pred))","10e24079":"Y_train_pred = svc.predict(X_train)\nY_test_pred = svc.predict(X_test)\nprint(\"For Support Vector:\")\nprint(\"Train set accuracy score:\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test set accuracy score:\",accuracy_score(Y_test,Y_test_pred))","e4fe74cc":"Y_train_pred = mlp.predict(X_train)\nY_test_pred = mlp.predict(X_test)\nprint(\"For Multi Layer Perceptron:\")\nprint(\"Train set accuracy score:\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test set accuracy score:\",accuracy_score(Y_test,Y_test_pred))","311863c6":"Y_train_pred = dtc.predict(X_train)\nY_test_pred = dtc.predict(X_test)\nprint(\"For Decision Tree Classifier:\")\nprint(\"Train set accuracy score:\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test set accuracy score:\",accuracy_score(Y_test,Y_test_pred))","9b905ad8":"Y_train_pred = naive_bayes_model.predict(X_train)\nY_test_pred = naive_bayes_model.predict(X_test)\nprint(\"For Naive Bayes:\")\nprint(\"Train set accuracy score:\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test set accuracy score:\",accuracy_score(Y_test,Y_test_pred))","e67d83b1":"vc = VotingClassifier(estimators=[('rf',rf),('log_reg',log_reg),\n                                  ('svc',svc),('lgbm',lgbm),\n                                  ('mlp',mlp)],voting='hard')\nvc.fit(X_train,Y_train)\nY_train_pred = vc.predict(X_train)\nY_test_pred = vc.predict(X_test)\nprint(\"For Voting Classifier:\")\nprint(\"Train set accuracy score:\",accuracy_score(Y_train,Y_train_pred))\nprint(\"Test set accuracy score:\",accuracy_score(Y_test,Y_test_pred))","7215e2b9":"vc.fit(train_after_pca,train_label)","b790cd45":"Y_pred = vc.predict(test_after_pca)\nresult = pd.DataFrame({'PassengerId':list(titanic_test['PassengerId']),'Survived':list(Y_pred)})\nresult.to_csv('\/kaggle\/working\/VotingClassifier.csv',index=False)","c5fd90cd":"## Feature Engineering & Model Training","823220d8":"So we see that Cabin is a good feature to describe the passenger's survival. There are majority of cabins that have either no survivors or all survivors. Few cabins having 50-60% survivors. But the problem comes with number of Unknown Cabins in the dataset is more than 70%. This analysis holds for only 30% of the dataset. So that's why this feature might have to be dropped during model training.","44f9745d":"### For test data","a2b14af6":"### Women and Children please hurry, get to the life boat!","e844db2a":"### Train-Validation Set Split","d674fa9f":"This function selects the best model parameters among the given set of parameters to maximize the validation test accuracy  ","6d3d5e97":"### Before Transformation","b9ea979f":"The graph is more skewed towards death of passengers amounting to 60%, while only about 40% survived the disaster. ","70bcc4b5":"## Importing required Libraries","95b13800":"### Combining train and test data","57a5d230":"### Fitting Models","f6f5fcc0":"### Are you alone, my dear child?\nLet us see whether Passengers traveling alone or those traveling with family members had higher chances of survival.","871cc8c7":"### Printing accuracy of each models on train-validation set","0a261855":"So, passengers with Family Members slightly had more survival % compared to passengers without Family Members on the ship, which makes sense as passengers with Family Members include women, children, elderly who could have been given more priority in case of evacuation compared to healthy male adults. ","565c9dba":"Dropping Cabin column as discussed earlier since it has more 70% null values","db960781":"### Treating Null values\n","45c39fba":"We see that Title of a passenger aboard the ship also played a huge role in their survival of the wreckage. Nobel Titles like Sir, Countess, Lady and other titles like Mme, Mile, Ms ensured definite survival of 100%. While the titles like Jonkheer, Don, Rev are attributed with no survival.","2f4c3b9f":"### Cabin in the Sinking Ship: A survivor's safe haven?","97a1f8cc":"### Selecting best model parameters for each model\nCommented during runtime to save execution time","2a7cf06a":"### Captain! Let's embark on the journey to save as many people as we can \nLet's check whether port of Embarkation has any influence on the survival rate ","235b7c49":"### Will I survive this, mate?","49e4103e":"### Does costly and classy ticket comes with the perk of saving life? \nLet us check fare of tickets, class of tickets vs survival percent. ","485b5bee":"Splitting using data column into original train and test data","d18f39be":"### Train-Validation Set score by the Voting Classifier","580146cd":"So, Decision Tree and Naive Bayes model won't be taken into Voting Classifier because of three reasons:\n* Decision Tree is prone to overfitting, so it might predict incorrect values on unknown test data, which might further worsen the predictions of this Classifier\n* Naive Bayes requires strict non-collinearity among features which is not always true in real-life dataset so it can impact the predictions\n* I have tried adding the above two models to the Classifier, it reduces the accuracy score.","a98527e8":"## Importing and Viewing Datasets","21c7c70d":"We see that people who boarded ship from Cherbourg port have the highest survival percent of more than 55% while the people who boarded ship from Southampton have the lowest survival percent of about 35%.","71a65263":"Adding Data column from encoded_train_test ","9857a5fd":"## Visualizations and EDA","cbef6591":"We will be predicting missing age values using Random Forest Regressor model. But before that, let us fill Embarked Column and Fare Column with Mode and Median respectively because Median is the best option for filling data irrespective of presence of outliers or not.\n\nAfter filling outliers, we have one-hot encoded categorical features: Ticket, Pclass, Sex, Embarked and Title. It leads to more than 400+ columns which necessitates the use of PCA for dimensionality reduction. I have chosen the threshold of 80% variance to be captured by the required number out of 400+ features, which I felt yielded good train accuracy and thus is sufficient enough. ","24f7290d":"### Outlier Treatment","4951638c":"### Adding Title column to train and test dataset","73091cd6":"### As you will, Your Majesty!\nWe observe that many passengers have title of Mr, Mrs, Col., Mistress etc. Let's observe whether having a respectable title entitles them to life-boat, life-vest privileges. ","9e31fd75":"### After transformation","129b4fda":"### Outputting results","0f2822f3":"Let us divide the fare prices into 4 categories: \nCheap (0-128.25), Decent (128.25-256.5), Fairly High (256.5-384.75), Costly (384.75-513).","d0988d8e":"Here we are combining the train test data to fill the missing values in the required columns. I have added a data column which signifies whether the record is from train data or test data, so that it becomes easy to split back into train and test data.","376039ff":"So, we see that Costly tickets and high class tickets does come out with high survival offer! It does makes sense because these tickets bought by people of high-class and title, who were given preference during evacuation of sinking ship. Although, between passengers who bought tickets at decent price have 12% more survival percent than those bought at Fairly High price. On the basis of class, 3rd class tickets have very low survival rate while 1st class tickets have more than 60%  survival rate.   ","22028110":"### Applying PCA\nThe process of applying PCA is same as before followed for filling age values through Random Forest Model prediction. Here, the variance threshold is set to 90%, as it yielded best result for me. ","14095384":"### Percentage of null values in columns","ee0d48bc":"It appears that survival percent among females were more than 50% percent higher than male. Probably because Women and children might have gotten first preference to take up Lifeboats and Lifevests. It will be a useful feature in our model prediction","535bca47":"Dropping Cabin column here also","338a4996":"### For train data"}}