{"cell_type":{"db81f8db":"code","20ac1732":"code","2a2b89ec":"code","50e4df20":"code","5977635a":"code","2ddd40a6":"code","7efdd0cb":"code","e97fce91":"code","81134a4f":"code","cd5cc5ec":"code","ad681ca0":"code","5a264f9e":"code","8032422c":"code","2cc564c0":"code","e2826f2f":"code","51efe35a":"code","a536a12f":"code","95c58585":"code","1976c8f9":"code","240ce70d":"code","158dd0ec":"code","c9be90ca":"code","5a23f1e1":"code","431c77be":"code","1bc42fbe":"code","430dacfa":"code","abf3bbf7":"code","4f524062":"code","3cd1b50d":"code","25782c0b":"code","82ce0107":"code","7c709149":"code","50448a25":"code","b5f1bdb4":"code","ed093ac1":"code","d6e54e8c":"code","8dfae116":"code","02b4aa21":"code","ddcf3692":"code","cfac9ac2":"code","3b1851ec":"code","a2440bf2":"code","b23bcaee":"code","8c7e1a3e":"code","da874664":"code","da168fc5":"code","bee68539":"code","a78873d1":"code","73f600e8":"code","bc1aed93":"code","07cf4912":"code","0adfd340":"code","325844b5":"markdown","f18c2c96":"markdown","f4640af1":"markdown","b1c82a49":"markdown","54dd7239":"markdown","85c11013":"markdown","b83259f9":"markdown","716c2c5e":"markdown","a238442c":"markdown","fed24165":"markdown","626e99da":"markdown","d1664445":"markdown","d5fc0b9c":"markdown","552dc052":"markdown","aca02589":"markdown","19ee7782":"markdown","1657d66a":"markdown","b9d21d3a":"markdown","f956b367":"markdown","a920d4bc":"markdown","d83f4035":"markdown","977588ea":"markdown","a9aa601e":"markdown","16a5c8a3":"markdown","ec66c240":"markdown","5d8fee26":"markdown","2207175a":"markdown","90484226":"markdown","dd43a20c":"markdown","08ad832e":"markdown","1507e377":"markdown","d71a02a8":"markdown","bda15927":"markdown","497810ea":"markdown","9db1f0c2":"markdown","487ae4ca":"markdown","0b6c09bf":"markdown","e038788e":"markdown","a1c1fcc9":"markdown","73e4067e":"markdown","a890b89a":"markdown","c0a97898":"markdown","b8b4191f":"markdown","a7990dc6":"markdown","2f62d978":"markdown","a958cc1f":"markdown","c07ee620":"markdown","3b85cdf6":"markdown","e46c0aa5":"markdown","ac8224aa":"markdown","358fb20f":"markdown"},"source":{"db81f8db":"!pip install scikit-learn --upgrade","20ac1732":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nplt.style.use('ggplot')\n#plt.style.use('default')\n\nimport seaborn as sns\n\nparams = {\n    'text.color': (0.25, 0.25, 0.25),\n    'figure.figsize': [18, 6],\n   }\n\nplt.rcParams.update(params)\n#plt.rcParams.update(plt.rcParamsDefault)\n\nimport pandas as pd\nfrom pandas import json_normalize\n\nimport geopandas as gpd\n\nimport numpy as np\nfrom numpy import percentile\nnp.random.seed(42)\n\nimport copy\nimport time\nfrom tqdm.notebook import tqdm\n\nfrom tabulate import tabulate\n\nfrom scipy import stats\nfrom scipy.stats import skew, kurtosis\n\n# print(plt.style.available)\n# print()\n# print(plt.colormaps())","2a2b89ec":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer, RobustScaler\nfrom sklearn.preprocessing import PolynomialFeatures, PowerTransformer\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, cross_validate\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.linear_model import LinearRegression, HuberRegressor, Lasso, LassoCV\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nimport lightgbm as lgb\n\nfrom sklearn.metrics import mean_squared_error, median_absolute_error, mean_absolute_percentage_error, mean_absolute_error, r2_score","50e4df20":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)","5977635a":"TITLE_SIZE = 24\nTITLE_PADDING = 20","2ddd40a6":"df_train = pd.read_csv(\"..\/input\/cas-data-science-hs-21-c2-ml-lab-1\/houses_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/cas-data-science-hs-21-c2-ml-lab-1\/houses_test.csv\")\n\ndf = pd.concat([df_train, df_test]).reset_index(drop=True)","7efdd0cb":"print(f\"Training data: {df_train.shape[0]:6,.0f} samples and {df_train.shape[1]:3.0f} features, target included\")\nprint(f\"Test data:     {df_test.shape[0]:6,.0f} samples and {df_test.shape[1]:3.0f} features, target not included\")","e97fce91":"display(df.info(memory_usage=\"deep\", verbose=True, show_counts=True))","81134a4f":"display(pd.concat([df.head(3), df.tail(3)]))","cd5cc5ec":"display(df.sample(3, random_state=42).T)","ad681ca0":"print(f\"The count of missing values is {df.drop('price', axis=1).isna().sum().sum()}\")\nprint()\ndisplay(df.drop(\"price\", axis=1).isna().sum())","5a264f9e":"print(f\"There are {df.drop(['price'], axis=1).duplicated().sum()} exact duplicates in the data set.\")","8032422c":"# Sanity check if all IDs are unique \nassert df.id.nunique() == len(df)","2cc564c0":"# Sanity check if all locations are unique\ncols = [\"lat\", \"long\"]\nassert df[cols].duplicated().sum() == 0","e2826f2f":"zip_train = set(df_train.zipcode.values)\nzip_test = set(df_test.zipcode.values)\nmun_train = set(df_train.municipality_name.values)\nmun_test = set(df_test.municipality_name.values)\nprint(f\"We have {len([x for x in zip_test if x not in zip_train])} zip codes in the test data that aren't in the train set.\")\nprint(f\"We have {len([x for x in mun_test if x not in mun_train])} municipalities in the test data that aren't in the train set.\")","51efe35a":"for feature in df.drop(\"id\", axis=1):\n    print(\"-\"*80)\n    print(feature, \"|\", df[feature].dtype)\n    print(df[feature].value_counts(dropna=False).sort_index())\n    print()","a536a12f":"display(df[df.num_rooms==0])\ndisplay(df[df.num_rooms==0].object_type_name.value_counts())","95c58585":"display(df[df.population_in_hectare==0])\ndisplay(df[df.population_in_hectare==0].object_type_name.value_counts())","1976c8f9":"numerical = ['build_year','living_area', 'num_rooms', 'water_percentage_1000',\n           'travel_time_private_transport', 'travel_time_public_transport',\n           'number_of_buildings_in_hectare', 'number_of_apartments_in_hectare',\n           'number_of_workplaces_in_hectare', 'number_of_workplaces_sector_1_in_hectare',\n           'number_of_workplaces_sector_2_in_hectare', 'number_of_workplaces_sector_3_in_hectare', \n           'population_in_hectare', 'price'\n            ]","240ce70d":"figsize = (12,2)\n\nfor num in df[numerical]:\n\n    plt.figure(figsize=figsize)\n    sns.boxplot(data=df, x=num)\n    skew_ = df[num].skew()\n    plt.title(f\"Feature \u00ab{num}\u00bb (skew {skew_:.1f})\", size=TITLE_SIZE, pad=TITLE_PADDING)\n    plt.xlabel(\"\")\n    plt.tight_layout()\n    plt.show()\n    \n    plt.figure(figsize=figsize)\n    sns.stripplot(data=df, x=num, alpha=.5)\n    plt.xlabel(\"\")\n    plt.tight_layout()\n    plt.show()\n    \n    plt.figure(figsize=figsize)\n    sns.histplot(data=df, x=num, kde=True)\n    plt.ylabel(\"\")\n    plt.xlabel(\"\")\n    plt.xlim(0, None)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\n\\n\")","158dd0ec":"for col in df.select_dtypes(\"number\").columns[1:]:\n    plt.figure(figsize=(12,3))\n    df[col].plot(lw=.01, marker='.', markersize=1)\n    plt.title(f\"Feature {col}\", size=12, pad=10)\n    # plt.xticks([])\n    plt.xlabel(\"IDs of property samples\")\n    plt.ylabel(f\"{col}\")\n    # plt.legend([\"female\", \"male\"], markerscale=12, loc=\"lower right\")\n    plt.tight_layout()\n    plt.show()","c9be90ca":"# set threshold to cut off weak correlations\ncorrelation_treshold = 0.3\n\ncorr = df.corr()\ncorr_cutoff = corr[(corr > correlation_treshold) | (corr < -correlation_treshold)]\n\nplt.figure(figsize=(16,16))\nsns.heatmap(corr_cutoff, \n            cmap=\"RdBu_r\", \n            # https:\/\/stackoverflow.com\/questions\/57414771\/how-to-plot-only-the-lower-triangle-of-a-seaborn-heatmap\n            mask=np.triu(corr_cutoff),\n            center=0,\n            square=True, \n            annot=False, \n            cbar_kws={\"shrink\": .7},\n           )\nplt.title(f\"Correlation of features, cutoff {correlation_treshold}\", size=TITLE_SIZE, pad=TITLE_PADDING)\nplt.tight_layout()\nplt.show()","5a23f1e1":"def get_outliers(data, iqr_multiple=3, log_data=False):\n    results = []\n    for col in data[numerical].columns:\n        if log_data==True:\n            q25 = percentile(np.log1p(data[col]), 25)\n            q75 = percentile(np.log1p(data[col]), 75)\n            iqr = q75 - q25\n            upper = q75 + iqr * iqr_multiple\n            to_drop = data[np.log1p(data[col]) > upper].index\n        else:\n            q25 = percentile(data[col], 25)\n            q75 = percentile(data[col], 75)\n            iqr = q75 - q25\n            upper = q75 + iqr * 3\n            to_drop = data[data[col] > upper].index \n        if len(to_drop)>0:\n            results.append((col, len(to_drop)))\n    return pd.DataFrame(results, columns=[\"feature\", \"outlier_count\"]).sort_values(\"outlier_count\", ascending=False).reset_index(drop=True)","431c77be":"display(get_outliers(df_train))\ndisplay(get_outliers(df_train, log_data=True))","1bc42fbe":"crs = {'init':'EPSG:4326'}\n\ngdf = gpd.GeoDataFrame(df, crs=crs, geometry=gpd.points_from_xy(df.long, df.lat))\n# swiss_map = gpd.read_file('_data\/data\/CHE_adm0.shp')\n\nfig, ax = plt.subplots(figsize=(12, 9))\nax.patch.set_facecolor('white')\n\n# swiss_map.plot(ax=ax, color=\"#F0F0F0\")\n\ngdf.plot(ax=ax, alpha=.2, markersize=6, color=\"darkred\")\n\nplt.title(\"Location of properties\", size=TITLE_SIZE, pad=TITLE_PADDING)\nplt.xlabel(\"long\")\nplt.ylabel(\"lat\")\nplt.tight_layout()\nplt.show()","430dacfa":"gdf = gpd.GeoDataFrame(df_train, crs=crs, geometry=gpd.points_from_xy(df_train.long, df_train.lat))\n\nfig, ax = plt.subplots(figsize=(12, 9))\nax.patch.set_facecolor('white')\n\nprice_scaled = np.log1p((df_train.price - df_train.price.min())\/ df_train.price.max())*100\ngdf.plot(ax=ax, alpha=.3, markersize=price_scaled, c=df_train.price, cmap='RdBu_r')\n\nplt.title(\"Location of properties, colored and sized by price\", size=TITLE_SIZE, pad=TITLE_PADDING)\nplt.xlabel(\"long\")\nplt.ylabel(\"lat\")\nplt.tight_layout()\nplt.show()","abf3bbf7":"gdf_train = gpd.GeoDataFrame(df_train, crs=crs, geometry=gpd.points_from_xy(df_train.long, df_train.lat))\ngdf_test = gpd.GeoDataFrame(df_test, crs=crs, geometry=gpd.points_from_xy(df_test.long, df_test.lat))\n\nfig, ax = plt.subplots(figsize=(12, 9), ncols=2, sharey=True)\n\nfor idx in range(0,2):\n    ax[idx].patch.set_facecolor('white')\n    ax[idx].set_xticks([])\n    ax[idx].set_yticks([])\n    ax[idx].set_xlabel(\"\")\n\ngdf_train.plot(ax=ax[0], alpha=.1, markersize=3, color=\"darkred\")\ngdf_test.plot(ax=ax[1], alpha=.5, markersize=3, color=\"darkgreen\")\n\nax[0].set_title(\"Property locations in train set\")\nax[1].set_title(\"Property locations in test set\")\n\nplt.tight_layout()\nplt.show()","4f524062":"top_n = 50\ndf_top = df.municipality_name.value_counts().sort_values(ascending=False)\ndf_top.iloc[-(len(df_top)-top_n)::-1].plot.barh(figsize=(16,12))\nplt.title(f\"Top {top_n} municipalities of properties\", size=TITLE_SIZE, pad=TITLE_PADDING)\nplt.xlabel(\"Counts in data set\")\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,3))\nsns.histplot(data=df_top, kde=True)\nskew_ = df[num].skew()\nplt.title(f\"Municipality count (skew {skew_:.1f})\", size=TITLE_SIZE, pad=TITLE_PADDING)\nplt.xlabel(\"Feature values\")\nplt.tight_layout()\nplt.show()","3cd1b50d":"df_top = df.object_type_name.value_counts().sort_values(ascending=False)\ndf_top.iloc[::-1].plot.barh(figsize=(16,5))\nplt.title(f\"Type of properties\", size=TITLE_SIZE, pad=TITLE_PADDING)\nplt.xlabel(\"Counts in data set\")\nplt.tight_layout()\nplt.show()","25782c0b":"display(df[df.object_type_name==\"Sonstiges\"].head(10))","82ce0107":"df_other = df[df.object_type_name==\"Sonstiges\"]\ndf_main = df.drop(df_other.index)\n\ngdf_main = gpd.GeoDataFrame(df_main, crs=crs, geometry=gpd.points_from_xy(df_main.long, df_main.lat))\ngdf_other = gpd.GeoDataFrame(df_other, crs=crs, geometry=gpd.points_from_xy(df_other.long, df_other.lat))\n\nfig, ax = plt.subplots(figsize=(12, 9), ncols=2, sharey=True)\n\nfor idx in range(0,2):\n    ax[idx].patch.set_facecolor('white')\n    ax[idx].set_xticks([])\n    ax[idx].set_yticks([])\n    ax[idx].set_xlabel(\"\")\n\ngdf_main.plot(ax=ax[0], alpha=.1, markersize=3, color=\"darkred\")\ngdf_other.plot(ax=ax[1], alpha=.7, markersize=5, color=\"darkgreen\")\n\nax[0].set_title(\"Properties labeled in main three categories\")\nax[1].set_title(\"Properties labeled \u00abSonstiges\u00bb\")\n\nplt.tight_layout()\nplt.show()","7c709149":"print(df.price.describe().astype(int))","50448a25":"plt.suptitle(\"Distribution of sales prices\", size=24)\nplt.subplot(2,1,1)\ndf.price.plot(kind=\"hist\", bins=100, rwidth=.9, figsize=(16,5))\nplt.ylabel(\"\")\nplt.ticklabel_format(axis='x', style='plain')\nplt.subplot(2,1,2)\ndf.price.plot(kind=\"box\", vert=False)\nplt.tight_layout()\nplt.ticklabel_format(axis='x', style='plain')\nplt.show()\n\nplt.suptitle(\"Distribution of sales prices, log transformed\", size=24)\nplt.subplot(2,1,1)\nnp.log(df.price).plot(kind=\"hist\", bins=100, rwidth=.9, figsize=(16,5))\nplt.ylabel(\"\")\nplt.subplot(2,1,2)\nnp.log(df.price).plot(kind=\"box\", vert=False)\nplt.tight_layout()\nplt.show()","b5f1bdb4":"corr = df.drop([\"id\", \"lat\", \"long\", \"zipcode\"], axis=1).corr()\n\ncorr[\"price\"].sort_values(ascending=True)[:-1].plot.barh(figsize=(12,6))\nplt.title(f\"Correlation of numerical features to price\", size=TITLE_SIZE, pad=TITLE_PADDING)\nplt.axvline(-correlation_treshold, color=\"grey\")\nplt.axvline(correlation_treshold, color=\"grey\")\nplt.xlabel(f\"Correlation (grey lines indicate +-{correlation_treshold} correlation threshold)\")\nplt.tight_layout()\nplt.show()","ed093ac1":"plt.figure(figsize=(16,5))\nsns.scatterplot(data=df, x=\"living_area\", y=\"price\")\nplt.title(\"Sales price in relation to living area\", size=24, pad=20)\nplt.legend([])\nplt.tight_layout()\nplt.show()\n\nplt.figure(figsize=(16,5))\nsns.scatterplot(data=df, x=\"num_rooms\", y=\"price\")\nplt.title(\"Sales price in relation to count of rooms\", size=24, pad=20)\nplt.legend([])\nplt.tight_layout()\nplt.show()","d6e54e8c":"# reload data to start over from scratch\ndf_train = pd.read_csv(\"..\/input\/cas-data-science-hs-21-c2-ml-lab-1\/houses_train.csv\")\ndf_test = pd.read_csv(\"..\/input\/cas-data-science-hs-21-c2-ml-lab-1\/houses_test.csv\")\ndf = pd.concat([df_train, df_test]).reset_index(drop=True)","8dfae116":"# setting years in the future to current year\nto_fix = df[df.build_year > 2021].index\ndf.loc[to_fix, \"build_year\"] = 2021\n\n# we only have obvious erroneous values in the train set\n# replacing the errors with the mean of the train set (avoiding a data leakage)\nto_fix = df[df.build_year < 1000].index\ndf.loc[to_fix, \"build_year\"] = df.build_year.median()","02b4aa21":"df[\"actual_age\"] = 2021 - df.build_year\n\n# create polynomials for top correlated features\npoly = PolynomialFeatures(degree=2, include_bias=False)\ntop_correlated = [\"num_rooms\", \"living_area\"]\ndf_poly = pd.DataFrame(poly.fit_transform(df[top_correlated].values))\ndf_poly.columns = [\"poly\" + str(x) for x in range(len(df_poly.columns))]\ndf = pd.concat([df, df_poly], axis=1)","ddcf3692":"df_train = df[df.price.notna()].copy()\ndf_test  = df[df.price.isna()].copy().reset_index(drop=True)\n\nX = df_train.drop([\"id\", \"price\"], axis=1).copy()\nX_test = df_test.drop([\"id\"], axis=1).copy()\ny = df_train.price.copy()","cfac9ac2":"X_ = X.select_dtypes(\"number\")\nclf_dummy = DummyRegressor()\nX_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=0)\nclf_dummy.fit(X_train, y_train)\ny_pred = clf_dummy.predict(X_test)\n\nMAPE = mean_absolute_percentage_error(y_test, y_pred)\nMean_AE = mean_absolute_error(y_test, y_pred)\nMedian_AE = median_absolute_error(y_test, y_pred)\nprint(f\"MAPE: {MAPE:.4f} | Mean_AE: {Mean_AE:,.0f} CHF | Median_AE: {Median_AE:,.0f} CHF | R2 score: {clf_dummy.score(X_test, y_test):.3f}\")","3b1851ec":"X_ = X[\"living_area\"].values\nX_ = X_[:, np.newaxis]\nclf_lin = LinearRegression()\nX_train, X_test, y_train, y_test = train_test_split(X_, y, random_state=0)\nclf_lin.fit(X_train, y_train)\ny_pred = clf_lin.predict(X_test)\nMAPE = mean_absolute_percentage_error(y_test, y_pred)\nMean_AE = mean_absolute_error(y_test, y_pred)\nMedian_AE = median_absolute_error(y_test, y_pred)\nprint(f\"MAPE: {MAPE:.4f} | Mean_AE: {Mean_AE:,.0f} CHF | Median_AE: {Median_AE:,.0f} CHF | R2 score: {clf_lin.score(X_test, y_test):.4f}\")","a2440bf2":"numericals = ['lat', 'long', 'build_year', 'living_area', 'num_rooms', 'water_percentage_1000',\n       'travel_time_private_transport', 'travel_time_public_transport',\n       'number_of_buildings_in_hectare', 'number_of_apartments_in_hectare',\n       'number_of_workplaces_in_hectare', 'number_of_workplaces_sector_1_in_hectare',\n       'number_of_workplaces_sector_2_in_hectare', 'number_of_workplaces_sector_3_in_hectare', \n       'population_in_hectare', 'actual_age', 'poly0', 'poly1', 'poly2', 'poly3', 'poly4']\n\ncategoricals = ['zipcode', 'municipality_name', 'object_type_name']","b23bcaee":"classifiers = [\n    DummyRegressor(),\n    LinearRegression(n_jobs=-1),\n    Ridge(),\n    SVC(kernel=\"rbf\"),\n    RandomForestRegressor(n_jobs=-1),\n    KNeighborsRegressor(n_jobs=-1),\n    GradientBoostingRegressor(),\n    lgb.LGBMRegressor(n_jobs=-1),\n]\n\nparams_grid = [{\"preprocessor__preproc_num\": [StandardScaler()], 'clf': classifiers}]","8c7e1a3e":"%%time\npreprocess = ColumnTransformer(\n                    [(\"preproc_num\", make_pipeline(StandardScaler()), numericals),\n                     (\"preproc_cat\", make_pipeline(OneHotEncoder(handle_unknown = \"ignore\")), categoricals),\n                    ])\n\npipe = Pipeline([('preprocessor', preprocess), \n                 ('clf', lgb.LGBMRegressor(n_jobs=-1))])\n\ngrid = GridSearchCV(pipe, params_grid, n_jobs=-1, verbose=1, cv=2, scoring=\"neg_mean_absolute_percentage_error\")\ngrid.fit(X, y)","da874664":"cv_results = pd.DataFrame(grid.cv_results_)\ncols = ['rank_test_score', 'param_clf', 'mean_test_score', 'mean_fit_time']\ndisplay(cv_results[cols].sort_values(\"rank_test_score\", ascending=True).style\n           .highlight_max(color=\"lightgreen\", subset=[\"mean_test_score\"])\n           .background_gradient(cmap='Reds', subset=['mean_fit_time'])\n           .hide_index()\n           .set_precision(3))","da168fc5":"# creating a copy of the data with log transformed numerical features\ndf_log = df.copy()\ncols_to_log = ['living_area', 'num_rooms', 'water_percentage_1000',\n               'travel_time_private_transport', 'travel_time_public_transport',\n               'number_of_buildings_in_hectare', 'number_of_apartments_in_hectare',\n               'number_of_workplaces_in_hectare', 'number_of_workplaces_sector_1_in_hectare',\n               'number_of_workplaces_sector_2_in_hectare', 'number_of_workplaces_sector_3_in_hectare', \n               'population_in_hectare']\n\nfor col in cols_to_log:\n    df_log[col] = np.log1p(df_log[col])\n    \ndf_log.price = np.log1p(df_log.price)","bee68539":"preprocess = ColumnTransformer(\n                    [(\"preproc_num\", make_pipeline(StandardScaler()), numericals),\n                     (\"preproc_cat\", make_pipeline(OneHotEncoder(handle_unknown=\"ignore\")), categoricals),\n                    ])\n\npipe = Pipeline([('preprocessor', preprocess), \n                 ('clf', lgb.LGBMRegressor(n_jobs=-1))])\n\n# train on raw features and log transformed target\nmean_score = np.mean(cross_val_score(pipe, \n                                     X, \n                                     np.log(y), \n                                     scoring=\"neg_mean_absolute_percentage_error\"))\nprint(f\"Log price: {-mean_score:.4f}\")\n\n# train on log transformed features and log transformed target\nmean_score = np.mean(cross_val_score(pipe, \n                                     df_log[df_log.price.notna()].drop(\"id\", axis=1), \n                                     np.log(y), \n                                     scoring=\"neg_mean_absolute_percentage_error\"))\nprint(f\"Log features and price: {-mean_score:.4f}\")","a78873d1":"outliers = []\n\nfor col in [\"living_area\", \"num_rooms\"]:\n    q25 = percentile(np.log1p(df_train[col]), 25)\n    q75 = percentile(np.log1p(df_train[col]), 75)\n    iqr = q75 - q25\n    upper = q75 + iqr * 2\n    to_drop = df_train[np.log1p(df_train[col]) > upper].index.values\n    if len(to_drop)>0:\n        outliers.extend(to_drop)\n        \ndf_train_clean = df_train.drop(outliers)        \nprint(f\"{len(outliers)} outliers removed.\")\n\nmean_score = np.mean(cross_val_score(pipe, \n                                     df_train_clean.drop(\"id\", axis=1), \n                                     np.log(df_train_clean.price), \n                                     scoring=\"neg_mean_absolute_percentage_error\"))\nprint(f\"MAPE (log), outliers removed: {-mean_score:.4f}\")","73f600e8":"search_space = dict(clf__learning_rate=np.logspace(-3, -1, 20),\n                    clf__n_estimators=np.linspace(1000, 3500, 11).astype(int), \n                    clf__max_depth=np.linspace(10, 100, 11).astype(int),\n                    clf__num_leaves=np.linspace(2, 50, 11).astype(int),\n                   )\n\nrnd_search = RandomizedSearchCV(pipe, \n                                search_space, \n                                n_iter=50,\n                                scoring=\"neg_mean_absolute_percentage_error\",\n                                cv=5,\n                                random_state=42, \n                                verbose=1, \n                                n_jobs=-1)\n\n_ = rnd_search.fit(df_train_clean.drop(\"id\", axis=1), \n                   np.log(df_train_clean.price))","bc1aed93":"cv_results = pd.DataFrame(rnd_search.cv_results_)\ncols = ['rank_test_score', 'params', 'mean_test_score']\ncv_results[cols].sort_values(\"rank_test_score\").head(1).params.values","07cf4912":"preprocess = ColumnTransformer(\n                    [(\"preproc_num\", make_pipeline(StandardScaler()), numericals),\n                     (\"preproc_cat\", make_pipeline(OneHotEncoder(handle_unknown=\"ignore\")), categoricals),\n                    ])\n\npipe = Pipeline([('preprocessor', preprocess), \n                 ('clf', lgb.LGBMRegressor(n_jobs=-1, \n                                           n_estimators=2250,\n                                           learning_rate=0.048,\n                                           max_depth=55,\n                                           num_leaves=30,\n                                          ))])","0adfd340":"_ = pipe.fit(df_train_clean.drop(\"id\", axis=1), \n             np.log(df_train_clean.price))\n\ny_pred = pipe.predict(df_test.drop(\"id\", axis=1))\ny_pred = np.exp(y_pred)\n\nsubmission = df_test[[\"id\", \"price\"]].copy()\nsubmission.price = np.round(y_pred).astype(int)\nsubmission.to_csv(\"submission.csv\", index=False)","325844b5":"Looking at the samples where `num_rooms` is 0 I see that many of these are regular properties where there has to be rooms. I assume that value 0 here indicates missing information.","f18c2c96":"**Comparing the locations of the train and the test data I do not notice any obvious differences.**","f4640af1":"## Selecting a classifier","b1c82a49":"There are 47 zip codes and 27 municipalities in the test data that aren't present in the test set. We have to keep that in mind when we encode the categoricals.","54dd7239":"**We don't have missing values in the data.**","85c11013":"# 4) Submission","b83259f9":"### Numerical features\nPlotting the numerical features we can see:\n- **None of the features are normally distributed.**\n- **All features have outliers**. \n- The distribution of **our dependent variable `price` is skewed as well** and has **a substantial amount of outliers.** \n","716c2c5e":"### Geographical coordinates","a238442c":"## Log transforming features and price","fed24165":"### Baseline with Linear Regression and just one numerical feature\nI choose the most correlated continuous feature for the second baseline. This yields a MAPE of 0.44.","626e99da":"## Examining the features","d1664445":"- **LightGBM performs best and also trains very fast.** \n- RandomForest comes close in terms of score but needs ~40x the training time.\n\nI decide to continue with LightGBM.","d5fc0b9c":"By mapping all lat\/long values to a base map of Switzerland we see: \n- The mayority of the properties are located in the north eastern part.\n- There are some noticable hot spots in Wallis (likely Bagnes), the Tessin and Graub\u00fcnden (e.g. Davos). I assume these are mainly holiday properties.","552dc052":"### Value counts\nPrinting value counts for each feature reveals:\n- The properties come from a wide variety of places in Switzerland.\n- Several values for `build_year` are erroneous (e.g. 1, 18, 19, 1019, 2022, 2049, 2106, 2108).\n- There are 90 properties with 0 rooms. These might be missing values (*\u00abnumber of rooms not known to data collector\u00bb*) that already have been filled. We need to check what kind of properties these are.\n- For 713 properties there the registered population in the \u00abhektar\u00bb is 0. Need to check that too.","aca02589":"# 2) Preparing the data\n---","19ee7782":"Coloring and sizing by price does not reveal a clear pattern. \n- The holiday areas stick out though. \n- The many (expensive) properties in Basel are noticable too.","1657d66a":"# 3) Modeling","b9d21d3a":"## Removing outliers","f956b367":"## Loading the data & first observations\n**Printing the basic stats of the dataframes gets us these insights**:\n- The **training data contains 20'313 samples** (aka real estate properties).\n- **The test set contains 2'257 properties for which we have to predict the price.**\n- We have **19 predicting features** and **one dependent variable** which is \u2013 not very surprisingly \u2013 named `price`.\n- Ignoring `id` there are 13 numerical, three categorical and two geolocation features to predict from.\n- At first glance **none of the features seem to have missing values.**\n- With just ~6MB memory consumption is low.","a920d4bc":"We get this additional information about the data set: \n\n- `id` \u2013 id der Liegeschaft\n- `lat` \u2013 Latitude\n- `long` \u2013 Longitude\n- `zipcode` \u2013 Postleitzahl\n- `municipality_name` \u2013 Ortschaft\n- `build_year` \u2013 Baujahr\n- `object_type_name` \u2013 Liegenschafttyp (Wohnung, Sonstiges, Einfamilienhaus, Mehrfamilienhaus)\n- `living_area` \u2013 Wohnfl\u00e4che\n- `num_rooms` \u2013 Anzahl Zimmer\n- `water_percentage_1000` \u2013 Anteil an Gew\u00e4sser in 1 km Umkreis\n- `travel_time_private_transport` \u2013 Reisezeit zum Zentrum mit dem privaten Verkehr\n- `travel_time_public_transport` \u2013 Reisezeit zum Zentrum mit dem \u00f6ffentlichen Verkehr\n- `number_of_buildings_in_hectare` \u2013 Anzahl Geb\u00e4ude innerhalb der Hektare\n- `number_of_apartments_in_hectare` \u2013 Anzahl Wohnungen innerhalb der Hektare\n- `number_of_workplaces_in_hectare` \u2013 Anzahl Arbeitspl\u00e4tze innerhalb der Hektare\n- `number_of_workplaces_sector_1_in_hectare` \u2013 Anzahl Arbeitspl\u00e4tze des Sektor 1 innerhalb der Hektare\n- `number_of_workplaces_sector_2_in_hectare` \u2013 Anzahl Arbeitspl\u00e4tze des Sektor 2 innerhalb der Hektare\n- `number_of_workplaces_sector_3_in_hectare` \u2013 Anzahl Arbeitspl\u00e4tze des Sektor 3 innerhalb der Hektare\n- `population_in_hectare` \u2013 Bev\u00f6lkerungsanzahl innerhalb der Hektare\n- `price` \u2013 Preis der Liegenschaft\n\nQuick reminder to self:\n\n- A \u00abhektar\u00bb equals 10'000 square meters. \n- \u00abSektor 1\u00bb is the economic \u00abPrim\u00e4rsektor\u00bb aka the extraction of raw materials and agriculture. \u00abSektor 2\u00bb is manufacturing, \u00abSektor 3\u00bb is the service industry.\n\n**What we do not know:**\n- We have no information how the data set was collected or put together.\n- We neither know the time frame in which the sales took place. \n- In the Kanton Zurich alone more than [7'000 properties are sold per year](https:\/\/www.zh.ch\/de\/planen-bauen\/raumplanung\/immobilienmarkt.html). From [various sources](https:\/\/www.sred.ch\/produkte-und-preise\/basisprodukt-kunden\/) I gather that there are at least 50k real private real estate sales (\u00abFreihandverk\u00e4ufe\u00bb) per year in Switzerland. So it is possible (but not given in any way) that our data contains sales from a smaller time frame, e.g., from one year. ","d83f4035":"### Outliers","977588ea":"### Correlations between numerical features","a9aa601e":"- Log transforming price yields a meaningful gain on the leaderboard \u2013 from 19.83 to 19.33\n- Log transforming the features too yields a slightly less good result of 19.37","16a5c8a3":"### Categorical features","ec66c240":"**The main take aways from the data exploration are:**\n- We seem to have a reasonable amount of data samples at hand for the task.\n- The data is easy to handle in terms of memory consumption and compute.\n- The exact source, age and collection procedure of the data is unknown.\n- The locations cover a wide range of places, mostly in the german speaking part as well as holiday locations like Bagnes, Tessin and Graub\u00fcnden.\n- The test data seem to be similar to the training data.\n- Some features are linearly correlated to the target. However, there are only two features with a weak to moderate correlation.\n- All numerical features are skewed and contain outliers.\n\n\n**Must do:**\n- Fix errors in `build_year`.\n- Log transform `price`.\n\n**Options to improve modeling:**\n- Remove outliers in `num_rooms` and `living_area`.\n- Interpret values of 0 in `num_rooms` as missing values and impute, e.g. with median.\n- Possibly replace value \u00abSonstige\u00bb with most common type.\n- Engineer new features. \n- Add additional data, e.g. from geographical location. ","5d8fee26":"### Fix errors","2207175a":"**Is there any structure in the data *\u00abalong\u00bb* the samples aka along axis `id`?** Is there **any obvious difference in the data distribution between train and test set?**\n- Though we see again the skewed distributions and outliers there is no obvious structure visible along the axis of IDs. \n- It seems the data is properly shuffled and the test set is a representative sample to predict on.","90484226":"Checking the properties where `population_in_hectare` is 0 I see that again this seems to represent missing values. E.g., if a property is an apartment there have to be other tenants around.","dd43a20c":"- Living area and number of rooms are the only two features with a significant linear correlation to the target variable. \n- Among the other even less correlated features the proximity to water sticks out. Some properties might be close to lakes or have a waterfront to a river and therefore are more expensive. \n- Travel time to the next city center sticks out as negatively correlated. That makes sense \u2013 the shorter the travel time the closer the property is to the center and therefore it is likely to be more expensive.","08ad832e":"#### Municipalities\n- From plotting the top 50 **municipalities** we can observe that **the locations aren't equally distributed**. E.g., Bagnes and Davos rank higher than the size of the municipality would suggest. Again I assume these are mainly holiday properties and for unknown reasons are overweighed in the data.\n- The distribution of the property counts is skewed as well. This is to be expected since the municipalities overall are very different in size, population and housing density.","1507e377":"**We do not have exact duplicates in our data. All IDs and locations are unique too.**","d71a02a8":"We separate the training data again from test data.","bda15927":"## Examining the target variable `price`","497810ea":"- **Price only has a weak to moderate linear correlation to living area and room count.**\n- The geographical features correlate strongly among each other, which is to be expected. \n- Population and housing density correlate with each other. That makes sense too.","9db1f0c2":"## Duplicates","487ae4ca":"- I try 8 different classifiers on the full dataset. \n- For preprocessing I setup a scikit pipeline and use crossvalidation to ensure meaningful scores.\n- I standard scale the numerical data and one hot encode the categorical features. ","0b6c09bf":"A simple outlier analysis by IQR range reveals:\n- Even with a wide IQR cutoff of 3 we can identify several thousands of outliers in the raw data.\n- Log transforming the features substantially reduces the outlier count. \n- In order to not loose to much data **I decide to remove outliers only based on the log transformed data and only for the highly correlated features `num_rooms` and `living_area`.**","e038788e":"# Imports","a1c1fcc9":"### Correlations of features to target variable","73e4067e":"- **Prices range from 205'000 to 3'999'000** (most likely CHF).\n- The **median is 830k**, the **average is ~961k** (affected by the outliers).\n- Prices differ on average by ~534k CHF around the average (standard deviation).\n- We already have seen the very skewed value distribution of sale prices. \n- **Log transforming seems to work good** on the data and likely will help to make more precise predictions.","a890b89a":"## Missing values","c0a97898":"**What are the objects of type \u00abSonstiges\u00bb?** \n- By manually checking objects of this category in Google Maps by their coordinates I find regular (apartment) houses. \n- The label \u00abSonstiges\u00bb therefore likely signifies an unknown type that hasn't been specified but falls in the category of apartment or house.","b8b4191f":"## Optimizing hyperparameters","a7990dc6":"## Conclusions from EDA","2f62d978":"### Baseline with a Dummy Regressor\n- As expected we get an R2 score very close to 0. \n- The mean absolute error for the pure guess of the Dummy Regressor is 392'823 CHF.\n- Our actual evaluation metric MAPE is 0.5.","a958cc1f":"From mapping all properties labeled \u00abSonstiges\u00bb I can not observe any meaningful differences.","c07ee620":"Removing outliers by a cutoff of 1.5 x IQR yields a result of 19.24","3b85cdf6":"# 1) EDA\n---","e46c0aa5":"By **plotting price against to two most correlated features** we observe:\n- A clear linear correlation of price to living area.\n- A somewhat noticeable correlation to room count.\n- Many outliers at the right end of the spectrum.\n- A lot of samples with 0 rooms that spread across a wide range of prices. These are probably unknown values (0==NaN). I will try to impute these with mean\/median.\n- To remove outliers in room counts, a cut off value of 20 rooms seems to be sensible. ","ac8224aa":"#### Object type\n- We mainly have apartments and single family houses in the data set. ","358fb20f":"### Create new features"}}