{"cell_type":{"726871e5":"code","396d6430":"code","4fbec2a4":"code","54a26296":"code","f606a271":"code","438bc30e":"code","5f85cfbb":"code","1bcb8661":"markdown"},"source":{"726871e5":"# Libraries for concurrency and parallelism\n! pip install asks trio","396d6430":"from pathlib import Path\nimport requests\nfrom os import cpu_count\n\nimport datatable as dt\nimport asks\nimport trio","4fbec2a4":"cpu_count()","54a26296":"Path('.\/pics').mkdir(exist_ok=True)\n\nimg_urls = dt.fread(\"..\/input\/wikipedia-image-caption\/test.tsv\", sep='\\t', columns={'image_url'})\nlinks = img_urls.to_list()[0][:1000]","f606a271":"%%time\n# fast way\n\nasync def fetch_pic(s, url):\n    r = await s.get(url)\n    return r.content\n\n\nasync def save_pic(s, url):\n    content = await fetch_pic(s, url)\n    filename = f\"pics\/{url.split('\/')[-1][:100]}\"\n    with open(filename,'wb') as f:\n        f.write(content)\n\n        \nasync def main(links):\n    dname = 'https:\/\/upload.wikimedia.org'\n    s = asks.sessions.Session(dname, connections=cpu_count()*2)\n    async with trio.open_nursery() as n:\n        for url in links:\n            n.start_soon(save_pic, s, url)\n\n            \ntrio.run(main, links)","438bc30e":"%%bash\nls pics | wc -l\nls pics -U | head -6","5f85cfbb":"%%time\n# regular way\n\nfor url in links:\n    r = requests.get(url, stream=True)\n    content=r.content\n    filename = f\"pics\/{url.split('\/')[-1][:100]}\"\n    with open(filename,'wb') as f:\n        f.write(content)\n    ","1bcb8661":"Here's a faster way to download images using the principles of concurrency and parallelism. In other words, asynchronous tasking and multiprocessing. It works best on machines with 8+ cores and when downloading large files like videos.\n\nPlease let me know if you have other good methods for downloading large numbers of files!"}}