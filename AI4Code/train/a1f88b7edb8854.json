{"cell_type":{"0d534a51":"code","27709d43":"code","298ec0ec":"code","7c14f8ae":"code","0836c40f":"code","2de66354":"code","e8adcd14":"code","e435998d":"code","dcd22ad8":"code","3b2043b7":"code","f63773cb":"code","536429f5":"code","100552ed":"code","dea90535":"code","64677c35":"code","9a83fb63":"code","478b4c98":"code","7ed04480":"code","22388980":"code","7a059266":"code","9516f18b":"code","14eb11f9":"code","27063a9c":"code","a28fe841":"code","a0c19d05":"code","7209a672":"code","534220dd":"code","a2e76b5b":"code","59165ad2":"code","0f8cb74d":"code","cc90bc0b":"markdown","e1450747":"markdown","d230e947":"markdown","75989e77":"markdown","4a9f05b8":"markdown","ddc5949f":"markdown","5ee8f6c5":"markdown","1bf46a60":"markdown","8f40d285":"markdown","e5a55a74":"markdown"},"source":{"0d534a51":"import pandas as pd\nimport numpy as np\nimport io\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport mlxtend as ml","27709d43":"df = pd.read_csv('\/kaggle\/input\/transactions-from-a-bakery\/BreadBasket_DMS.csv')","298ec0ec":"# first five row\ndf.head()","7c14f8ae":"# size of datset\ndf.shape","0836c40f":"# summary about dataset\ndf.info()","2de66354":"# statistical summary of numerical variables\ndf.describe()","e8adcd14":"# check for missing values\ndf.isnull().sum() ","e435998d":"# merge date and time column\ndf['Datetime'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\ndf = df[[\"Datetime\", \"Transaction\", \"Item\"]]\n\ndf.head()","dcd22ad8":"df.dtypes","3b2043b7":"# check for unique value in items\ndf['Item'].value_counts().to_dict()","f63773cb":"# Remove none\ndf = df[df['Item'] != 'NONE']","536429f5":"# check NONE value removed or not\ndf[df['Item'] == 'NONE']","100552ed":"# Extract hour of the day and weekday of the week\n# For Datetime: the day of the week are Monday=0, Sunday=6, thereby +1 to become Monday=1, Sunday=7\n\ndf['Hour'] = df['Datetime'].dt.hour\n\ndf[\"Weekday\"] = df[\"Datetime\"].dt.weekday + 1\n\ndf.head()","dea90535":"total_items = len(df)\ntotal_days = len(np.unique(df.Datetime.dt.day))\ntotal_months = len(np.unique(df.Datetime.dt.month))\naverage_items = int(total_items \/ total_days)\nunique_items = df.Item.unique().size\n\nprint(\"Total unique_items: {} sold by the Bakery\".format(unique_items))\nprint('-----------------------------')\nprint(\"Total sales: {} items sold in {} days throughout {} months\".format(total_items, total_days, total_months))\nprint('-----------------------------')\nprint(\"Average_items daily sales: {}\".format(average_items))","64677c35":"# Rank the top 10 best-selling items\ncounts = df.Item.value_counts()\n\npercent = df.Item.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n\ntop_10 = pd.DataFrame({'counts': counts, '%': percent})\n\ntop_10.head(10)","9a83fb63":"# Rank by percentage\nplt.figure(figsize=(8,5))\ndf.Item.value_counts(normalize=True)[:10].plot(kind=\"bar\", title=\"Percentage of Sales by Item\").set(xlabel=\"Item\", ylabel=\"Percentage\")\nplt.show()\n\n# Rank by value\nplt.figure(figsize=(8,5))\ndf.Item.value_counts()[:10].plot(kind=\"bar\", title=\"Total Number of Sales by Item\").set(xlabel=\"Item\", ylabel=\"Total Number\")\nplt.show()","478b4c98":"# set datetime as index \ndf.set_index('Datetime', inplace=True)","7ed04480":"# Number of items sold by day\ndf[\"Item\"].resample(\"D\").count().plot(figsize=(15,5), title=\"Total Number of Items Sold by Date\").set(xlabel=\"Date\", ylabel=\"Total Number of Items Sold\")\nplt.show()","22388980":"# Number of items sold by month\ndf[\"Item\"].resample(\"M\").count().plot(figsize=(15,5), grid=True, title=\"Total Number by Items Sold by Month\").set(xlabel=\"Date\", ylabel=\"Total Number of Items Sold\")\nplt.show()","7a059266":"# Aggregate item sold by hour\ndf_groupby_hour = df.groupby(\"Hour\").agg({\"Item\": lambda item: item.count()\/total_days})\nprint(df_groupby_hour)\n\n# Plot items sold by hour\nplt.figure(figsize=(8,5))\nsns.countplot(x='Hour',data=df)\nplt.title('Items Sales by hour')\nplt.show()","9516f18b":"# sales groupby weekday\ndf_groupby_weekday = df.groupby(\"Weekday\").agg({\"Item\": lambda item: item.count()})\ndf_groupby_weekday.head()","14eb11f9":"# Define dataset to machine learning\ndf_basket = df.groupby([\"Transaction\",\"Item\"]).size().reset_index(name=\"Count\")\n\nmarket_basket = (df_basket.groupby(['Transaction', 'Item'])['Count'].sum().unstack().reset_index().fillna(0).set_index('Transaction'))\nmarket_basket.head()","27063a9c":"# Convert all of our numbers to either a 1 or a 0 (negative numbers are converted to zero, positive numbers are converted to 1)\ndef encode_data(datapoint):\n  if datapoint <= 0:\n    return 0\n  else:\n    return 1","a28fe841":"# Process the transformation into the market_basket dataset\nmarket_basket = market_basket.applymap(encode_data)\n\n# Check the result\nmarket_basket.head()\n\nmarket_basket.isna().sum()","a0c19d05":"# Apriori method request a min_support: Support is defined as the percentage of time that an itemset appears in the dataset.\n# Defined to start seeing data\/results with min_support of 2%\n\nitemsets = apriori(market_basket, min_support= 0.02, use_colnames=True)","7209a672":"# Build your association rules using the mxltend association_rules function.\n# min_threshold can be thought of as the level of confidence percentage that you want to return\n# Defined to use 50% of min_threshold\n\nrules = association_rules(itemsets, metric='lift', min_threshold=0.5)","534220dd":"# Below the list of products sales combinations\n# It can use this information to build a cross-sell recommendation system that promotes these products with each other \n\nrules.sort_values(\"lift\", ascending = False, inplace = True)\nrules.head(10)","a2e76b5b":"support = rules.support.to_numpy()\nconfidence = rules.confidence.to_numpy()\n\nfor i in range (len(support)):\n    support[i] = support[i]\n    confidence[i] = confidence[i]\n\nplt.figure(figsize=(8,6))    \nplt.title('Assonciation Rules')\nplt.xlabel('support')\nplt.ylabel('confidance')\nsns.regplot(x=support, y=confidence, fit_reg=False)\nplt.show()","59165ad2":"# Recommendation of Market Basket\nrec_rules = rules[ (rules['lift'] > 1) & (rules['confidence'] >= 0.5) ]","0f8cb74d":"# Recommendation of Market Basket Dataset\ncols_keep = {'antecedents':'item_1', 'consequents':'item_2', 'support':'support', 'confidence':'confidence', 'lift':'lift'}\ncols_drop = ['antecedent support', 'consequent support', 'leverage', 'conviction']\n\nrecommendation_basket = pd.DataFrame(rec_rules).rename(columns= cols_keep).drop(columns=cols_drop).sort_values(by=['lift'], ascending = False)\n\ndisplay(recommendation_basket)","cc90bc0b":"# Load dataset","e1450747":"# Modeling","d230e947":"# Exploratory data analysis","75989e77":"# What is the Apriori algorithm?\nApriori uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function that exploits the downward closure property of support. We apply an iterative approach or level-wise search where k-frequent itemsets are used to find k+1 itemsets.\n\n## Apriori Property \u2013\nAll subsets of a frequent itemset must be frequent(Apriori propertry).\nIf an itemset is infrequent, all its supersets will be infrequent.\n\n### **The following are the main steps of the algorithm:**\n* Calculate the support of item sets (of size k = 1) in the transactional database (note that support is the frequency of occurrence of an itemset). This is called generating the candidate set.\n\n* Prune the candidate set by eliminating items with a support less than the given threshold.\n\n* Join the frequent itemsets to form sets of size k + 1, and repeat the above sets until no more itemsets can be formed. This will happen when the set(s) formed have a support less than\u200b the given support.\n\n## Bakery Market Basket Analysis \nMarket Basket Analysis is used to increase marketing effectiveness and to improve cross-sell and up-sell opportunities by making the right offer to the right customer. For a retailer, good promotions translate into increased revenue and profits. The objectives of the market basket analysis models are to identify the next product that the customer might be interested to purchase or to browse.\" \n\nRight! Before we implement the algorithm just for the sake of showing off our skills, what is our goal? As discussed previously we are here to determine up-sell opportunities. Let's start with some general questions as a framework: What sort of relationships do we wish to discover? and then, naturally: how would discovering such relationships help the business owner's bottom line? for now, let's keep these in the back of our mind.\n\n\n* Can we get rid of a product 'X' because it is sold infrequently?\nIf the business owner wishes to get rid of a product in order to save any cost and overhead associated with it but unknowingly is getting rid of a product that is part of an item set 'X' and 'Y' where both X and Y are complements, it might not be as straightforward since it may impact other products.","4a9f05b8":"## Building the Apriori model ","ddc5949f":"## If you find this kernel usefull please UPVOTE ","5ee8f6c5":"### Support:\n* refers to the default popularity of an item and can be calculated by finding number of transactions containing a particular item divided by total number of transactions\n\n### Confidence:\n\n* refers to the likelihood that an item B is also bought if item A is bought. It can be calculated by finding the number of transactions where A and B are bought together, divided by total number of transactions where A is bought\n\n### Lift:\n\n* refers to the increase in the ratio of sale of B when A is sold. Lift(A \u2013> B) can be calculated by dividing Confidence(A -> B) divided by Support(B)\n\n### Leverage:\n\n* computes the difference between the observed frequency of A and C appearing together and the frequency that would be expected if A and C were independent\n\n### Conviction:\n\n* A high conviction value means that the consequent is highly depending on the antecedent","1bf46a60":"# Import libraries","8f40d285":"#### There is 786 'NONE' , we need to remove it","e5a55a74":"## If you find this kernel usefull please UPVOTE "}}