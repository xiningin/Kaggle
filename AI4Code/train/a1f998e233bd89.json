{"cell_type":{"6cb21dee":"code","be00427f":"code","18c2b073":"code","3e65c717":"code","dcdaf6f1":"code","6c7678e9":"code","218d9cd9":"code","69dac8b7":"code","7c39c936":"code","66df9412":"code","a38b5391":"code","00066ec5":"code","a3c353fc":"code","e2fda33f":"code","177dbc4b":"code","ef776ab3":"code","f54928b9":"code","1d66fffb":"code","d26a5a0b":"code","cecbb29f":"code","2c837f6e":"markdown","6b9bdc4c":"markdown","5ce8ea89":"markdown","6f3d0344":"markdown","6e9df22e":"markdown","2639c05c":"markdown","00c33f47":"markdown"},"source":{"6cb21dee":"import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv(\"..\/input\/entity-annotated-corpus\/ner_dataset.csv\", encoding=\"latin1\")","be00427f":"data = data.fillna(method=\"ffill\")\ndata.head(10)","18c2b073":"words = list(set(data['Word'].values))\nn_words = len(words)\nn_words\n\ntags = list(set(data['Tag'].values))\nn_tags = len(tags)\nn_tags","3e65c717":"class SentenceGetter(object):\n    def __init__(self , data):\n        self.n_sent =1 \n        self.data = data\n        self.empty = False\n        agg_func = lambda s : [(w,p,t) for w , p , t in zip(s['Word'].values.tolist() ,\n                                                            s['POS'].values.tolist() ,\n                                                            s['Tag'].values.tolist())]\n        \n        self.group_wpt = self.data.groupby('Sentence #').apply(agg_func)\n        self.sentences = [ s for s in self.group_wpt]\n        \n    def get_next(self):\n        \n        try:\n            s = self.group_wpt['Sentence: {}'.format(self.n_sent)]\n            self.n_sent +=1\n            return s\n        except :\n            return None","dcdaf6f1":"sentence_iter = SentenceGetter(data)","6c7678e9":"sentence = sentence_iter.get_next()\nprint(\"Sentence : \" , sentence)","218d9cd9":"sentences = sentence_iter.sentences","69dac8b7":"max_len = 75\nmax_len_char = 10\n\nword2idx = { w:i+2   for i , w in enumerate(words) }\nword2idx[\"PAD\"] = 0\nword2idx[\"UNK\"] = 1\nidx2word = { i:w  for w , i in word2idx.items()}\n\ntag2idx = { t : i+1 for  i , t in enumerate(tags)}\ntag2idx['PAD'] = 0\nidx2tag = { i : t for t , i in tag2idx.items()}","7c39c936":"from keras.preprocessing.sequence import pad_sequences\nX_word = [ [ word2idx[w[0]] for w in s ]  for s in sentences  ]\nX_word = pad_sequences(maxlen=max_len, sequences=X_word, value=word2idx[\"PAD\"], \n                       padding='post', truncating='post')","66df9412":"chars = set([ char_i for w_i in words for char_i in w_i])\nn_chars = len(chars)\nn_chars","a38b5391":"char2idx = { c:i+2 for i , c in enumerate(chars) }\nchar2idx['PAD'] = 0\nchar2idx['UNK'] = 1","00066ec5":"X_char = []\nfor sentence in sentences:\n    sent_seq = []\n    for i in range(max_len):\n        word_seq = []\n        for j in range(max_len_char):\n            try:\n                word_seq.append(char2idx.get(sentence[i][0][j]))\n            except:\n                word_seq.append(char2idx.get(\"PAD\"))\n        sent_seq.append(word_seq)\n    X_char.append(np.array(sent_seq))","a3c353fc":"y = [ [ tag2idx[w[2]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=max_len , sequences=y ,\n                  value=tag2idx[\"PAD\"] , padding='post' , truncating='post')","e2fda33f":"from sklearn.model_selection import train_test_split\nX_word_tr, X_word_te, y_tr, y_te = train_test_split(X_word, y, test_size=0.1, random_state=2018)\nX_char_tr, X_char_te, _, _ = train_test_split(X_char, y, test_size=0.1, random_state=2018)","177dbc4b":"from keras.models import Model , Input\nfrom keras.layers import LSTM , Embedding , Dense , TimeDistributed , Dropout , Conv1D\nfrom keras.layers import Bidirectional , concatenate  , SpatialDropout1D , GlobalMaxPool1D","ef776ab3":"# input and embedding for words\nword_in = Input(shape=(max_len,))\nemb_word = Embedding(input_dim=n_words + 2, output_dim=300,\n                     input_length=max_len, mask_zero=True)(word_in)\n\n# input and embeddings for characters\nchar_in = Input(shape=(max_len, max_len_char,))\nemb_char = TimeDistributed(Embedding(input_dim=n_chars + 2, output_dim=10,\n                           input_length=max_len_char, mask_zero=True))(char_in)\n# character LSTM to get word encodings by characters\nchar_enc = TimeDistributed(LSTM(units=128, return_sequences=False,\n                                recurrent_dropout=0.5))(emb_char)\n\n# main LSTM\nx = concatenate([emb_word, char_enc])\nx = SpatialDropout1D(0.3)(x)\nmain_lstm = Bidirectional(LSTM(units=128, return_sequences=True,\n                               recurrent_dropout=0.6))(x)\nout = TimeDistributed(Dense(n_tags + 1, activation=\"sigmoid\"))(main_lstm)\n\nmodel = Model([word_in, char_in], out)\nmodel.summary()","f54928b9":"import tensorflow as tf\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,\n                                    beta_2=0.999, epsilon=1e-07,\n                                    amsgrad=False, name=\"Adam\")\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer ,\n              metrics=['acc'])","1d66fffb":"history = model.fit([X_word_tr,\n                     np.array(X_char_tr).reshape((len(X_char_tr), max_len, max_len_char))],\n                    np.array(y_tr).reshape(len(y_tr), max_len, 1),\n                    batch_size=32, epochs=10, validation_split=0.1, verbose=1)\n","d26a5a0b":"hist = pd.DataFrame(history.history)\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nplt.figure(figsize=(12,12))\nplt.plot(hist[\"acc\"])\nplt.plot(hist[\"val_acc\"])\nplt.show()\n","cecbb29f":"y_pred = model.predict([X_word_te,\n                        np.array(X_char_te).reshape((len(X_char_te),\n                                                     max_len, max_len_char))])\ni = 1925\np = np.argmax(y_pred[i], axis=-1)\nprint(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\nprint(30 * \"=\")\nfor w, t, pred in zip(X_word_te[i], y_te[i], p):\n    if w != 0:\n        print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))\n","2c837f6e":"*Pad the sentence to a maximum sequence length which can be usable with neural networks*","6b9bdc4c":"# Create the sets of words and tags","5ce8ea89":"# Create the vocabularies of words and tags","6f3d0344":"*Define the character vocabulary for the character embeddings*","6e9df22e":"# Character vocabulary ","2639c05c":"* Define the target tag sequence with same length as the input sequence because for every input word we need to predict a tag","00c33f47":"# POS TAGGING Keras Implementation\n![](https:\/\/miro.medium.com\/max\/1170\/1*CbZE2ZTBlmswW84Knjbqkg.png)"}}