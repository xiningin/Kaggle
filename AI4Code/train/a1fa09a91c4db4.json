{"cell_type":{"55c5b3cf":"code","e0069541":"code","0fd268fa":"code","8d54eb55":"code","2a98c697":"code","6ecac441":"code","5d123c35":"code","bc967ca2":"code","1d6be58c":"code","0462ef32":"code","8291d418":"code","618cb680":"code","1a7ce738":"code","aaf71a19":"code","a4eef8b2":"code","27d347e3":"code","293da149":"code","41d9aa3a":"code","235488cc":"code","64353ff2":"code","48fd94f5":"code","0a84d440":"code","d77bff0b":"code","6afd71c5":"code","9654bee8":"code","9dd0fc8c":"code","ead65114":"code","7d340cff":"code","bf3fb03a":"code","279e05bb":"code","7aec0c23":"code","1e9f5c9d":"code","bd1026a9":"code","56239265":"code","ddb365d4":"code","0380a136":"code","ee6295eb":"code","0be86da6":"code","87ac3f40":"code","6c5b298f":"code","4a37774c":"code","92638a06":"code","ca9da81f":"code","9d370a80":"code","e3200404":"code","4c6525dd":"code","6debdfe0":"code","4276f5bf":"code","2c8fa887":"code","5cea12e6":"code","845ca817":"code","5c3651b4":"code","be420043":"code","ee7e5500":"code","2f15430d":"code","2283e3cd":"code","b7f4b86a":"code","3e3b8d6b":"code","4f5ab4e9":"code","46bf40a0":"code","a037192a":"code","d1a5fd4a":"code","66f29773":"code","fc285856":"code","3659f1d7":"code","96ea25de":"code","6cfd3034":"code","b8e1ec38":"code","1e2afa18":"code","2b1bff2e":"code","df17a3bb":"code","cb5753f9":"code","334ee348":"code","cca8bdab":"code","a54de6fb":"code","dfda6550":"code","881fc58a":"code","b54493de":"code","577bc93f":"code","d81ebfc8":"code","f75025f3":"code","ea444619":"code","d8c7d42f":"code","e34f3afa":"code","5bc7406c":"code","4777675a":"code","f2419228":"code","8f2af99c":"code","7d8de59a":"code","431c774f":"code","1503dd85":"code","3073e1f0":"code","17b0c8c6":"code","1ca82ed2":"code","b9c3de4b":"code","31a0b4e0":"code","642f66a2":"code","44c11754":"code","45081bba":"code","d587dab7":"code","a1f1e28f":"code","b468e65f":"code","fbc77852":"code","57fffd22":"code","7ae90337":"code","44ab6589":"code","3258a12e":"markdown","b2661158":"markdown","33d3e087":"markdown","a94062f6":"markdown","b52019bd":"markdown","c7ecb6b2":"markdown","09be71dc":"markdown","98327f90":"markdown","6fdc3077":"markdown","b33c508d":"markdown","709b1a02":"markdown","ac0e11da":"markdown","85ba584a":"markdown","b01af11e":"markdown","abc4b4e6":"markdown","7bca5753":"markdown","08ef5634":"markdown","9be3af78":"markdown","6ccba0a5":"markdown","72fa972d":"markdown","cfdd63d0":"markdown","e00c46e7":"markdown","bff5c892":"markdown","9a301040":"markdown","d65a3665":"markdown","76130ba3":"markdown","97ffba17":"markdown","bdae440d":"markdown","efe9aa51":"markdown","f8e5779b":"markdown","157ec85b":"markdown","771f4fc4":"markdown","03352ec8":"markdown","2db56bd2":"markdown","441196f6":"markdown","34840908":"markdown","f0a21574":"markdown","19888cf4":"markdown","e9095301":"markdown","91f7e371":"markdown","204d6f3e":"markdown","93750f67":"markdown","3a3f1c52":"markdown","203525a0":"markdown","a0332aab":"markdown","8210a7db":"markdown","ac88d9dc":"markdown","142687f6":"markdown","bb0b71bf":"markdown","ff085905":"markdown","f7d60ef9":"markdown","b17ea076":"markdown","097f6dec":"markdown","9e67d2a2":"markdown","d49cf445":"markdown","fb0dc6d6":"markdown","1eb60759":"markdown","50182f64":"markdown","0ad2ea0f":"markdown","22f602cc":"markdown","45bb9b63":"markdown","67c21f7e":"markdown","e8e9e896":"markdown","8adb2404":"markdown","1f9ad27d":"markdown","0a977645":"markdown"},"source":{"55c5b3cf":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set(style='whitegrid')\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nimport re\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\n\nimport warnings\nwarnings.simplefilter('ignore')\nimport tensorflow as tf","e0069541":"import nltk\nnltk.download('all')","0fd268fa":"df=pd.read_csv('..\/input\/news-popularity-in-social-media-platforms\/News_Final.csv')","8d54eb55":"df.head()","2a98c697":"df.loc[:,['Title','Headline','Topic']]","6ecac441":"df.shape","5d123c35":"df.info()","bc967ca2":"df.Topic.value_counts()","1d6be58c":"df2=df.copy(deep=True)\npie1=pd.DataFrame(df2['Topic'].value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of Topic',y = 'Topic', \n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(12,12))","0462ef32":"df.isnull().sum()","8291d418":"df = df[df['Headline'].notna()]","618cb680":"df.shape","1a7ce738":"sns.set(style='whitegrid')\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(25,8))\n\nsns.distplot(df2[df2['Topic'] == 'economy']['Headline'].str.len(), kde=True, color='r', ax=ax1)\nax1.set_title('Economy')\nsns.distplot(df2[df2['Topic'] == 'obama']['Headline'].str.len(), kde=True, color='b', ax=ax2)\nax2.set_title('Obama')\nsns.distplot(df2[df2['Topic'] == 'microsoft']['Headline'].str.len(), kde=True, color='g', ax=ax3)\nax3.set_title('Microsoft')\nsns.distplot(df2[df2['Topic'] == 'palestine']['Headline'].str.len(), kde=True, color='y', ax=ax4)\nax4.set_title('Palestine')\n\nf.suptitle('Histogram number of characters in topics headlines')","aaf71a19":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(25,8))\n\nsns.distplot(df[df['Topic'] == 'economy']['Headline'].str.split().map(lambda x: len(x)), kde=True, color='r', ax=ax1)\nax1.set_title('Economy')\nsns.distplot(df[df['Topic'] == 'obama']['Headline'].str.split().map(lambda x: len(x)), kde=True, color='b', ax=ax2)\nax2.set_title('Obama')\nsns.distplot(df[df['Topic'] == 'microsoft']['Headline'].str.split().map(lambda x: len(x)), kde=True, color='g', ax=ax3)\nax3.set_title('Microsoft')\nsns.distplot(df[df['Topic'] == 'palestine']['Headline'].str.split().map(lambda x: len(x)), kde=True, color='y', ax=ax4)\nax4.set_title('Palestine')\n\nf.suptitle('Histogram number of words in topics headlines')","a4eef8b2":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","27d347e3":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","293da149":"df['Headline']=df['Headline'].apply(lambda x : remove_URL(x))","41d9aa3a":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","235488cc":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n    \nprint(remove_html(example))","64353ff2":"df['Headline']=df['Headline'].apply(lambda x : remove_html(x))","48fd94f5":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","0a84d440":"df['Headline']=df['Headline'].apply(lambda x: remove_emoji(x))","d77bff0b":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","6afd71c5":"df['Headline']=df['Headline'].apply(lambda x : remove_punct(x))","9654bee8":"df['Headline']=df['Headline'].str.replace('   ', ' ')\ndf['Headline']=df['Headline'].str.replace('     ', ' ')\ndf['Headline']=df['Headline'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ndf['Headline']=df['Headline'].str.replace('  ', ' ')\ndf['Headline']=df['Headline'].str.replace('\u2014', ' ')\ndf['Headline']=df['Headline'].str.replace('\u2013', ' ')","9dd0fc8c":"!pip install transformers","ead65114":"# Load Huggingface transformers\nfrom transformers import TFBertModel,  BertConfig, BertTokenizerFast\n\n# Then what you need from tensorflow.keras\nfrom tensorflow.keras.layers import Input, Dropout, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy\nfrom tensorflow.keras.utils import to_categorical\n\n# And pandas for data import + sklearn because you allways need sklearn\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","7d340cff":"### --------- Import data --------- ###\n\n# Select required columns\ndata = df[['Headline', 'Topic']]\n\n# Set your model output as categorical and save in new label col\ndata['Topic_label'] = pd.Categorical(data['Topic'])\n\n# Transform your output to numeric\ndata['Topic'] = data['Topic_label'].cat.codes\n\n# Split into train and test - stratify over Issue\ndata_train, data_test = train_test_split(data, test_size = 0.1)","bf3fb03a":"### --------- Setup BERT ---------- ###\n\n# Name of the BERT model to use\nmodel_name = 'bert-base-uncased'\n\n# Max length of tokens\nmax_length = 45\n\n# Load transformers config and set output_hidden_states to False\nconfig = BertConfig.from_pretrained(model_name)\nconfig.output_hidden_states = False\n\n# Load BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)\n\n# Load the Transformers BERT model\ntransformer_model = TFBertModel.from_pretrained(model_name, config = config)","279e05bb":"### ------- Build the model ------- ###\n\n# TF Keras documentation: https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/Model\n\n# Load the MainLayer\nbert = transformer_model.layers[0]\n\n# Build your model input\ninput_ids = Input(shape=(max_length,), name='input_ids', dtype='int32')\ninputs = {'input_ids': input_ids}\n\n# Load the Transformers BERT model as a layer in a Keras model\nbert_model = bert(inputs)[1]\ndropout = Dropout(config.hidden_dropout_prob, name='pooled_output')\npooled_output = dropout(bert_model, training=False)\n\n# Then build your model output\ntopics = Dense(units=len(data_train.Topic_label.value_counts()), kernel_initializer=TruncatedNormal(stddev=config.initializer_range), name='topic')(pooled_output)\noutputs = {'topic': topics}\n\n# And combine it all in a model object\nmodel = Model(inputs=inputs, outputs=outputs, name='BERT_MultiLabel_MultiClass')\n\n# Take a look at the model\nmodel.summary()","7aec0c23":"### ------- Train the model ------- ###\n\n# Set an optimizer\noptimizer = Adam(learning_rate=5e-05,epsilon=1e-08,decay=0.01,clipnorm=1.0)\n\n# Set loss and metrics\nloss = {'topic': CategoricalCrossentropy(from_logits = True)}\n\n# Compile the model\nmodel.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n\n# Ready output data for the model\ny_topic = to_categorical(data_train['Topic'])\n\n# Tokenize the input (takes some time)\nx = tokenizer(\n    text=data_train['Headline'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n\n# Fit the model\nhistory = model.fit(\n    x={'input_ids': x['input_ids']},\n    y={'topic': y_topic},\n    validation_split=0.1,\n    batch_size=64,\n    epochs=2,\n    verbose=1)","1e9f5c9d":"def plot_metrics(record):\n  epoch_range = range(1, len(record.history['accuracy'])+1)\n  plt.plot(epoch_range, record.history['accuracy'])\n  plt.plot(epoch_range, record.history['val_accuracy'])\n  plt.title('Classification Accuracy')\n  plt.ylabel('Accuracy')\n  plt.xlabel('Epoch')\n  plt.legend(['Train', 'Val'], loc='lower right')\n  plt.show()\n\n  # Plot training & validation loss values\n  plt.plot(epoch_range, record.history['loss'])\n  plt.plot(epoch_range, record.history['val_loss'])\n  plt.title('Model loss')\n  plt.ylabel('Loss')\n  plt.xlabel('Epoch')\n  plt.legend(['Train', 'Val'], loc='lower right')\n  plt.show()","bd1026a9":"plot_metrics(history)","56239265":"### ----- Evaluate the model ------ ###\n\n# Ready test data\ntest_y_topic = to_categorical(data_test['Topic'])\n\ntest_x = tokenizer(\n    text=data_test['Headline'].to_list(),\n    add_special_tokens=True,\n    max_length=max_length,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = False,\n    verbose = True)\n\n# Run evaluation\nmodel_eval = model.evaluate(\n    x={'input_ids': test_x['input_ids']},\n    y={'product': test_y_topic}\n)","ddb365d4":"label_predicted = model.predict(\n    x={'input_ids': test_x['input_ids']},\n)","0380a136":"label_predicted['topic']","ee6295eb":"test_y_topic","0be86da6":"label_pred_max=[np.argmax(i) for i in label_predicted['topic']]","87ac3f40":"label_actual_max=[np.argmax(i) for i in test_y_topic]","6c5b298f":"from sklearn.metrics import classification_report\n\nreport = classification_report(label_pred_max, label_actual_max)\n\nprint(report)","4a37774c":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(label_pred_max, label_actual_max), display_labels=np.unique(label_actual_max))\ndisp.plot(cmap='Blues') \nplt.grid(False)","92638a06":"df['Headline']=df['Headline'].str.replace('\\d+', '')","ca9da81f":"nltk.download(\"stopwords\")","9d370a80":"from nltk.corpus import stopwords","e3200404":"stop_words = set(stopwords.words(\"english\"))","4c6525dd":"from collections import defaultdict,Counter","6debdfe0":"word_count = Counter(\" \".join(df[df['Topic']=='economy']['Headline']).split()).most_common(100)\nx=[]\ny=[]\nfor word,count in word_count:\n    if (word.casefold() not in stop_words) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y[:10],y=x[:10])\nplt.title('10 most common words in Economy Topic')","4276f5bf":"word_count = Counter(\" \".join(df[df['Topic']=='obama']['Headline']).split()).most_common(100)\nx=[]\ny=[]\nfor word,count in word_count:\n    if (word.casefold() not in stop_words) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y[:10],y=x[:10])\nplt.title('10 most common words in Obama Topic')","2c8fa887":"word_count = Counter(\" \".join(df[df['Topic']=='microsoft']['Headline']).split()).most_common(100)\nx=[]\ny=[]\nfor word,count in word_count:\n    if (word.casefold() not in stop_words) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y[:10],y=x[:10])\nplt.title('10 most common words in Microsoft Topic')","5cea12e6":"word_count = Counter(\" \".join(df[df['Topic']=='palestine']['Headline']).split()).most_common(100)\nx=[]\ny=[]\nfor word,count in word_count:\n    if (word.casefold() not in stop_words) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y[:10],y=x[:10])\nplt.title('10 most common words in Palestine Topic')","845ca817":"# Define ngram generator function\ndef generate_ngrams(text, n_gram):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in stop_words]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","5c3651b4":"N=10","be420043":"# Bigrams\neconomy_bigrams = defaultdict(int)\nobama_bigrams = defaultdict(int)\nmicrosoft_bigrams = defaultdict(int)\npalestine_bigrams = defaultdict(int)\n\nfor instance in df[df['Topic']=='palestine']['Headline']:\n    for word in generate_ngrams(instance, n_gram=2):\n        palestine_bigrams[word] += 1\n\nfor instance in df[df['Topic']=='obama']['Headline']:\n    for word in generate_ngrams(instance, n_gram=2):\n        obama_bigrams[word] += 1 \n\nfor instance in df[df['Topic']=='economy']['Headline']:\n    for word in generate_ngrams(instance, n_gram=2):\n        economy_bigrams[word] += 1     \n\nfor instance in df[df['Topic']=='microsoft']['Headline']:\n    for word in generate_ngrams(instance, n_gram=2):\n        microsoft_bigrams[word] += 1    \n\ndf_palestine_bigrams = pd.DataFrame(sorted(palestine_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_obama_bigrams = pd.DataFrame(sorted(obama_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_economy_bigrams = pd.DataFrame(sorted(economy_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_microsoft_bigrams = pd.DataFrame(sorted(microsoft_bigrams.items(), key=lambda x: x[1])[::-1])","ee7e5500":"fig, axes = plt.subplots(2, 2, figsize=(20, 10), dpi=80)\nplt.tight_layout()\n\nsns.barplot(y=df_palestine_bigrams[0].values[:N], x=df_palestine_bigrams[1].values[:N], ax=axes[0,0], color='r')\naxes[0,0].spines['right'].set_visible(False)\naxes[0,0].tick_params(axis='x', labelsize=13)\naxes[0,0].tick_params(axis='y', labelsize=13)\n\nsns.barplot(y=df_economy_bigrams[0].values[:N], x=df_economy_bigrams[1].values[:N], ax=axes[0,1], color='b')\naxes[0,1].spines['right'].set_visible(False)\naxes[0,1].tick_params(axis='x', labelsize=13)\naxes[0,1].tick_params(axis='y', labelsize=13)\n\nsns.barplot(y=df_obama_bigrams[0].values[:N], x=df_obama_bigrams[1].values[:N], ax=axes[1,0], color='g')\naxes[1,0].spines['right'].set_visible(False)\naxes[1,0].tick_params(axis='x', labelsize=13)\naxes[1,0].tick_params(axis='y', labelsize=13)\n\nsns.barplot(y=df_microsoft_bigrams[0].values[:N], x=df_microsoft_bigrams[1].values[:N], ax=axes[1,1], color='y')\naxes[1,1].spines['right'].set_visible(False)\naxes[1,1].tick_params(axis='x', labelsize=13)\naxes[1,1].tick_params(axis='y', labelsize=13)\n\naxes[0,0].set_title(f'Top {N} most common bigrams in Palestine Topic', fontsize=15)\naxes[0,1].set_title(f'Top {N} most common bigrams in Economy Topic', fontsize=15)\naxes[1,0].set_title(f'Top {N} most common bigrams in Obama Topic', fontsize=15)\naxes[1,1].set_title(f'Top {N} most common bigrams in Microsoft Topic', fontsize=15)\n\nplt.show()\nplt.tight_layout()","2f15430d":"# Trigrams\neconomy_trigrams = defaultdict(int)\nobama_trigrams = defaultdict(int)\nmicrosoft_trigrams = defaultdict(int)\npalestine_trigrams = defaultdict(int)\n\nfor instance in df[df['Topic']=='palestine']['Headline']:\n    for word in generate_ngrams(instance, n_gram=3):\n        palestine_trigrams[word] += 1\n\nfor instance in df[df['Topic']=='obama']['Headline']:\n    for word in generate_ngrams(instance, n_gram=3):\n        obama_trigrams[word] += 1 \n\nfor instance in df[df['Topic']=='economy']['Headline']:\n    for word in generate_ngrams(instance, n_gram=3):\n        economy_trigrams[word] += 1     \n\nfor instance in df[df['Topic']=='microsoft']['Headline']:\n    for word in generate_ngrams(instance, n_gram=3):\n        microsoft_trigrams[word] += 1    \n\ndf_palestine_trigrams = pd.DataFrame(sorted(palestine_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_obama_trigrams = pd.DataFrame(sorted(obama_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_economy_trigrams = pd.DataFrame(sorted(economy_trigrams.items(), key=lambda x: x[1])[::-1])\ndf_microsoft_trigrams = pd.DataFrame(sorted(microsoft_trigrams.items(), key=lambda x: x[1])[::-1])","2283e3cd":"fig, axes = plt.subplots(2, 2, figsize=(20, 10), dpi=80)\nplt.tight_layout()\n\nsns.barplot(y=df_palestine_trigrams[0].values[:N], x=df_palestine_trigrams[1].values[:N], ax=axes[0,0], color='r')\naxes[0,0].spines['right'].set_visible(False)\naxes[0,0].tick_params(axis='x', labelsize=13)\naxes[0,0].tick_params(axis='y', labelsize=13)\n\nsns.barplot(y=df_economy_trigrams[0].values[:N], x=df_economy_trigrams[1].values[:N], ax=axes[0,1], color='b')\naxes[0,1].spines['right'].set_visible(False)\naxes[0,1].tick_params(axis='x', labelsize=13)\naxes[0,1].tick_params(axis='y', labelsize=13)\n\nsns.barplot(y=df_obama_trigrams[0].values[:N], x=df_obama_trigrams[1].values[:N], ax=axes[1,0], color='g')\naxes[1,0].spines['right'].set_visible(False)\naxes[1,0].tick_params(axis='x', labelsize=13)\naxes[1,0].tick_params(axis='y', labelsize=13)\n\nsns.barplot(y=df_microsoft_trigrams[0].values[:N], x=df_microsoft_trigrams[1].values[:N], ax=axes[1,1], color='y')\naxes[1,1].spines['right'].set_visible(False)\naxes[1,1].tick_params(axis='x', labelsize=13)\naxes[1,1].tick_params(axis='y', labelsize=13)\n\naxes[0,0].set_title(f'Top {N} most common trigrams in Palestine Topic', fontsize=15)\naxes[0,1].set_title(f'Top {N} most common trigrams in Economy Topic', fontsize=15)\naxes[1,0].set_title(f'Top {N} most common trigrams in Obama Topic', fontsize=15)\naxes[1,1].set_title(f'Top {N} most common trigrams in Microsoft Topic', fontsize=15)\n\nplt.show()\nplt.tight_layout()","b7f4b86a":"df[['Headline','Topic']]","3e3b8d6b":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","4f5ab4e9":"df['Headline_without_stopwords'] = df['Headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","46bf40a0":"df[['Headline','Headline_without_stopwords']]","a037192a":"training_portion=0.9","d1a5fd4a":"train_size = int(df.shape[0]*training_portion)\n\ntrain_sentences = df['Headline_without_stopwords'][:train_size]\ntrain_labels = df['Topic'][:train_size]\n\nvalidation_sentences = df['Headline_without_stopwords'][train_size:]\nvalidation_labels = df['Topic'][train_size:]\n\nprint(train_size)\nprint(len(train_sentences))\nprint(len(train_labels))\nprint(len(validation_sentences))\nprint(len(validation_labels))","66f29773":"tokenizer0 = Tokenizer()\ntokenizer0.fit_on_texts(train_sentences)\nword_index = tokenizer0.word_index\nlen(word_index)","fc285856":"vocab_size = 30000\noov_tok = '<OOV>'","3659f1d7":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index","96ea25de":"train_sequences = tokenizer.texts_to_sequences(train_sentences)","6cfd3034":"lengths=[]\nfor k in range(len(train_sequences)):\n  lengths.append(len(train_sequences[k]))","b8e1ec38":"pd.DataFrame(lengths, columns=['Lenghts']).describe()","1e2afa18":"plt.hist(lengths, bins=50, alpha=0.5)\nplt.show()","2b1bff2e":"max_length = 45\ntrunc_type = 'post'\npadding_type = 'post'","df17a3bb":"train_padded = pad_sequences(train_sequences,maxlen=max_length,padding=padding_type,truncating=trunc_type)","cb5753f9":"print(len(train_sequences[0]))\nprint(len(train_padded[0]))\n\nprint(len(train_sequences[1]))\nprint(len(train_padded[1]))\n\nprint(len(train_sequences[10]))\nprint(len(train_padded[10]))","334ee348":"validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences,padding=padding_type,maxlen=max_length,truncating=trunc_type)\n\nprint(len(validation_sequences))\nprint(validation_padded.shape)","cca8bdab":"label_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(train_labels)\n\ntraining_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\nvalidation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n\nprint(training_label_seq[0])\nprint(training_label_seq[1])\nprint(training_label_seq[2])\nprint(training_label_seq.shape)\n\nprint(validation_label_seq[0])\nprint(validation_label_seq[1])\nprint(validation_label_seq[2])\nprint(validation_label_seq.shape)","a54de6fb":"np.unique(validation_label_seq)","dfda6550":"from tensorflow.keras.optimizers import Adam\n\nembedding_dim = 32\nmodel = tf.keras.Sequential([\n              tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n              tf.keras.layers.GlobalAveragePooling1D(),\n              tf.keras.layers.Dense(24,activation='relu'),\n              tf.keras.layers.Dense(5,activation='softmax')\n])\nopt=Adam(learning_rate=5e-3)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\nmodel.summary()","881fc58a":"num_epochs = 10\nhistory = model.fit(train_padded,training_label_seq,epochs=num_epochs,\n                    validation_data=(validation_padded,validation_label_seq),\n                    verbose=1)","b54493de":"plot_metrics(history)","577bc93f":"topic_pred=model.predict(validation_padded)\ntopic_pred","d81ebfc8":"class_pred_val = [np.argmax(i) for i in topic_pred]\nprint(class_pred_val[:5])","f75025f3":"validation_label_seq[:5]","ea444619":"validation_sentences[:5]","d8c7d42f":"validation_labels[:5]","e34f3afa":"from sklearn.metrics import classification_report\n\nprint(classification_report(class_pred_val,validation_label_seq))","5bc7406c":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(class_pred_val,validation_label_seq), display_labels=np.unique(validation_label_seq))\ndisp.plot(cmap='Blues') \nplt.grid(False)","4777675a":"label_tokenizer.word_index","f2419228":"tokenizer_glove = Tokenizer()\ntokenizer_glove.fit_on_texts(train_sentences)\nword_index_glove = tokenizer_glove.word_index\nlen(word_index_glove)","8f2af99c":"train_glove_sequences=tokenizer_glove.texts_to_sequences(train_sentences)\ntrain_glove_padded = pad_sequences(train_glove_sequences,maxlen=max_length,\n                                   padding=padding_type,truncating=trunc_type)\n\ntrain_glove_padded.shape","7d8de59a":"val_glove_sequences=tokenizer_glove.texts_to_sequences(validation_sentences)\nval_glove_padded = pad_sequences(val_glove_sequences,maxlen=max_length,\n                                 padding=padding_type,truncating=trunc_type)\n\nval_glove_padded.shape","431c774f":"vocab_size_glove=len(word_index_glove)","1503dd85":"embeddings_index = {};\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;","3073e1f0":"embeddings_matrix = np.zeros((vocab_size_glove+1, 100));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","17b0c8c6":"print(len(embeddings_matrix))","1ca82ed2":"embedding_dim_glove=100\nmodel_glove = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size_glove+1, embedding_dim_glove, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(5,activation='softmax')\n])\nopt=Adam(learning_rate=5e-3)\nmodel_glove.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\nmodel_glove.summary()","b9c3de4b":"num_epochs = 10\nhistory_glove = model_glove.fit(train_glove_padded,training_label_seq,epochs=num_epochs,\n                    validation_data=(val_glove_padded,validation_label_seq),verbose=1)","31a0b4e0":"from tensorflow.keras.optimizers import Adam\n\nmodel_glove2=Sequential()\nmodel_glove2.add(Embedding(vocab_size_glove+1,100,weights=[embeddings_matrix],input_length=max_length,trainable=False))\nmodel_glove2.add(SpatialDropout1D(0.2))\nmodel_glove2.add(LSTM(45, dropout=0.2, recurrent_dropout=0.2))\nmodel_glove2.add(Dense(5, activation='softmax'))\n\nopt=Adam(learning_rate=5e-3)\nmodel_glove2.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\nmodel_glove2.summary()","642f66a2":"num_epochs = 5\nhistory_glove2 = model_glove2.fit(train_glove_padded,training_label_seq,epochs=num_epochs,\n                      validation_data=(val_glove_padded,validation_label_seq),verbose=1,\n                      batch_size=32)","44c11754":"model_glove3=Sequential()\nmodel_glove3.add(Embedding(vocab_size_glove+1,100,weights=[embeddings_matrix],input_length=max_length,trainable=False))\nmodel_glove3.add(LSTM(45, dropout=0.2, recurrent_dropout=0.2))\nmodel_glove3.add(Dense(24,activation='relu'))\nmodel_glove3.add(Dense(5, activation='softmax'))\n\nopt=Adam(learning_rate=5e-3)\nmodel_glove3.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\nmodel_glove3.summary()","45081bba":"num_epochs = 5\nhistory_glove3 = model_glove3.fit(train_glove_padded,training_label_seq,epochs=num_epochs,\n                      validation_data=(val_glove_padded,validation_label_seq),verbose=1,\n                      batch_size=32)","d587dab7":"plot_metrics(history_glove3)","a1f1e28f":"topic_pred_glove=model_glove3.predict(val_glove_padded)\ntopic_pred_glove","b468e65f":"class_pred_val_glove = [np.argmax(i) for i in topic_pred_glove]\nprint(class_pred_val_glove[:5])","fbc77852":"validation_label_seq[:5]","57fffd22":"from sklearn.metrics import classification_report\n\nprint(classification_report(class_pred_val_glove,validation_label_seq))","7ae90337":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(class_pred_val_glove,validation_label_seq), display_labels=np.unique(validation_label_seq))\ndisp.plot(cmap='Blues') \nplt.grid(False)","44ab6589":"label_tokenizer.word_index","3258a12e":"As we know the output corresponds to arrays of 5 values corresponding to probability of instances belonging to each class, from these we have to obtain the maximum of them (argmax) and we will get the class predicted:","b2661158":"# Text classification\n\nThe current project has as main scope the classification of news headlines by their topic using three approaches transformers, bag of words and pre-trained word embedding, the notebook will be splitted into 2 parts the first one comprehends exploratory data analysis, cleaning and development of transformer attention model and the second part corresponds to the other models just mentioned. The dataset contains more than 92 thousand instances corresponding to one of four classes (economy, obama, microsoft, palestine). In order to achieve a high performance in the classification we have to preprocess the instances by applying the wide variety of methods used in Natural Language Preprocessing 'NLP'.","33d3e087":"## Analysis of common words per topic:","a94062f6":"The accuracy in train and validation sets were definitely low and taking into account that a simple scratch model achieved above 96% of accuracy makes us prefer the scratch one for this dataset when comparing similar models, because of this we will change the architecture in order to make this more complex.","b52019bd":"### Bi-grams:","c7ecb6b2":"As we will select the most common words from each topic we have to make sure to avoid selecting the denominated 'stop words' because these will not be relevant in this case, this is why we will import them by downloading from nltk tool and english dictionary:","09be71dc":"## Pre-trained models:\n\nWe will download the Glove pre-trained model for text classification with 100 embedding dimensions, after predicting the classes for the validation set we will compare with those obtained from the previously built scratch model.\n\nAs a first step we have to tokenize again the sentences without limiting the maximum length of them as follows:","98327f90":"Fortunately this third model outperformed the previous one slightly and thus corresponds to the pre-trained model with the best metrics. However there is a couple of disadvantages because it took 41 minutes and 55 seconds to train and in such time the accuracies did not overcome those obtained in the model with own embedding.","6fdc3077":"# Part 2\n\nIn the second part of the project we will do exactly the same task, but using different approaches such as bag of words and pre-trained word embedding, despite these are obsolete methods we can obtain performance that are not too far from what we obtained with BERT, however these does not comprehend the context or meaning of the sentences as attention models which is why are not useful for more complex tasks such as Text generation, Summarization or Chatbots. \n\nWe will start by completing the cleaning of sentences (remove of numbers), then remove the stop words (this was the main reason why I placed this after transformer, because removing stop words would change the meaning of sentences which would make our previous attention model unuseful), later we will perform n-gram analysis (which is absolutely important when analyzing text in NLP) and finally build and train several bag-of-words models so as to find the best one.","b33c508d":"Classification report and confusion matrix:","709b1a02":"## Modeling:\n\nThe following models will be built and compared using their corresponding error measurements:\n\n- Simple text classifier using own embedding.\n- 3 Pre-trained 100 dimensional Glove.\n\nWe will compare both by using their corresponding error metrics and showing their classification reports and confusion matrix:","ac0e11da":"As we can see in above the instances contains 11 features from which Title, Hardline and Topic are the most important in the main scope, let us see in more detail only these columns of the instances contained in the dataset:","85ba584a":"As we know the output corresponds to arrays of 5 values corresponding to probability of instances belonging to each class, from these we have to obtain the maximum of them (argmax) and we will get the class predicted:","b01af11e":"## Modeling\n\nThe model we will build is BERT which stands for Bidirectional Encoder Representations for Transformers, whose key characteristic is that it's non-directional meaning it reads or takes into account all words in the sequence, allowing to understand much better the context of each word based on their surrounding. The tool we are going to use is HuggingFace because it offers a relatively easy way to build the models by just changing the 'head' to the proper for our task.\n\nWe will start by installing the transformers library and importing the functions needed.","abc4b4e6":"The same happends with the actual label which contains 1's representing the class predicted, we have to convert both actual and predicted to the same format to compute the error metrics.","7bca5753":"The next cell considers the model training and we have to set the optimizer, the loss function as categorical crossentropy and accuracy as metric, as final step we take these as features of the model.compile function.","08ef5634":"The model took 1 minute and 20 seconds to train and the accuracy for both training and validation sets look outstanding, but let us see how these behave in the next plots:","9be3af78":"We can see the distribution of the topics in the dataset plotting a Pie Chart, this will give us an early consideration to have if the label is unbalanced:","6ccba0a5":"Now we have to load the tokenizer and the bert-base-uncased model transformer, these have consider the proper BertConfig with output_hidden_states=False.","72fa972d":"As we ran for only 2 epochs the plot doesn't look too appealing, but makes sense.","cfdd63d0":"The loss and accuracy obtained were as expected and similar to validation set metrics, now we will predict the label for test instances as we traditionaly when building models with keras:","e00c46e7":"It would be interesting to see the number of characters contained in the headlines of each topic, even better showing them as histograms so as to see if there is a specific pattern for any topic:","bff5c892":"#### Pre-trained model 3:\n\nWe will go back to the first architecture and change the GlobalAveragePooling1D with LSTM of 45 units followed by 2 Dense layers, let us see the performance:","9a301040":"Now, we will print the first 5 actual classes for the validation set:","d65a3665":"Let us create a new column corresponding to our label as categorical, then we have to split the dataset into training and testing sets, as this contains more than 98000 instances we can choose a relatively low proportion for test because it will contain enough instances to evaluate as out of bag. Therefore 10% will be used for test size, which should correspond to approximately 9800 samples.","76130ba3":"## Cleaning:\n\nWe have to consider that the instances included in the file were obtained by several sources such as: Facebook, LinkedIn and Google Plus, as we know and have seen in some of the headlines are included characters which are not alphanumeric and belong to tags used in social media, these can be useful in specific tasks (mention or hashtag), however in the scope of topic classification such tags are not significatively useful. This is why in the current step we will get rid of them so as to keep only the corpus or core data:","97ffba17":"### Removing HTML tags:","bdae440d":"Let us tokenize all words in our dataset so as to analyze and get an appropriate vocab_size and max_length: ","efe9aa51":"Now that our model has been loaded we can start the processes of building and  tuning according to our dataset and task using the functional API of keras.\n\nAs we see below the input layer must consider the max_length of sequences and then this is fed to the bert model, a dropout layer to reduce overfitting and finally a dense layer with number of neurons equal to number of classes in our label.","f8e5779b":"## Tokenizing:","157ec85b":"Now, as we said we will download the weights of Glove 100 dimensional version  from Stanford:","771f4fc4":"Time now to display the classification report and confusion matrix showing the performance of the model predicting each class.","03352ec8":"Remove stopwords and create new column:","2db56bd2":"The following lines creates a matrix containing the weights and its dimension is the new vocabulary size by 100 embedding dimension, this will be loaded as the weights of the first embedding layer by setting the argument trainable=False.","441196f6":"### Removing multiple spaces:","34840908":"Let us see if there is any null value in the columns, specifically what we are more interested is in Headline and Topic columns, thus we will drop the rows which does not contain useful information in such columns:","f0a21574":"Finally the next cell is to compute the label predicted of the test instances.","19888cf4":"I would like to know any feedback in order to increase the performance of the models or tell me if you found a different one even better!\n\nIf you liked this notebook I would appreciate so much your upvote if you want to see more projects\/tutorials like this one. I encourage you to see my projects portfolio, am sure you will love it.\n\nThank you!","e9095301":"The difference in the misclassification in own embedding and pre-trained models is significant and for this specific dataset we can endorse that the former outperformed the 100-dimensional pre-trained Glove. In order to achieve a better accuracy we could use other versions of the Glove such as 200-D or 300-D including a different combination of layers, regularization and callbacks. However not even the best pre-trained was better than BERT model, this is why such models are obsolete and no matter the type of layers and their combinations in the architecture will make them better than attention models. As we said the difference in error metrics could be slight, but this does not considers the fact that attention models \"understand\" the sentences by looking at all words surrounding each one, creating a context, allowing the model to perform question answering related to the trained text.","91f7e371":"Above we can see this new model contains around 5.7 million parameters to train, this would take a long time, but as we have loaded the weights only 2549 are trainable. In contrast to the scratch model which contains almost 1 million parameters and obviously have to be computed all of them. ","204d6f3e":"### Removing Emojis:","93750f67":"Let us predict the topics for the validation set and print the first 5 so as to compare them and compute the error metrics:","3a3f1c52":"Now, we will print the first 5 actual classes for the validation set:","203525a0":"### Tri-grams:","a0332aab":"Now, we can compute the length of each instance tokenized of the training set and see its distribution in order to choose maximim length which does not drop significant information at the end of the sequence:","8210a7db":"Let us set the such argument to 45, truncation type 'post' and padding type 'post' as follows:","ac88d9dc":"### Removing URLs:","142687f6":"We can see above they have similar characteristics and we can not say too much about a special feature of any of them, would be better to apply this method but for the words.\n\nThis is why, we are goind to compute and display a histogram of the number of words in the headlines of each topic:","bb0b71bf":"### Removing numbers:","ff085905":"### Removing punctuations:","f7d60ef9":"There are 57.514 unique words in the training sentences, we must remember that this number would be higher if we would not have removed the stopwords, we can see each one of the words sorted descendingly by calling the dictionary 'word_index'containing the tokens. As we know when we tokenize  a relatively big dataset we expect that around half of the words hardly ever appear and because we can cut or select only a portion of the tokens so as to be more accurate and speed up the training process. Having said that let us set the vocab size to 30.000.","b17ea076":"##### Prediction for validation set:","097f6dec":"Time now to build the models using the Glove-weights, the main reason why we decided to build 3 of this type of model is to try to find one which stands out significatively because of its architecture, specifically the layers used, later when comparing them we will see just a tiny difference.\n\n#### Pre-trained model 1:\n\nThis model has to be similar to the scratch one in terms of layers so as to compare both and quantify the impact of using Glove weights in the embedding layer:","9e67d2a2":"#### Pre-trained model 2:\n\nThis model will contain layers we have not used yet, SpatialDropout1D which is the equivalent of dropout in dense layers, but applied to 1D-sequences, then a LSTM with 45 units (equal to the sentence length) and finally a Dense softmax layer:","d49cf445":"Classification report and confusion matrix:","fb0dc6d6":"## Exploratory Data Analysis:\nThe first step corresponds to explore the characteristics of the dataset, we will read the file, see the type of each column, drop the empty and null instances, compute the number of characters and words in the corpus of each topic, etc.","1eb60759":"# Part 1","50182f64":"Number of n-grams to generate will be set by variable 'N':","0ad2ea0f":"### Simple text classifier using own embedding:\n\nLet us create a simple, but poweful model including the Embedding layer which comprehends the vocabulary size, input length and embedding dimensions (For our case we will start with 32 and then change to see if we can achieve a better performance), GlobalAveragePooling1D and 2 Dense layers with proper activation functions: ","22f602cc":"Let us display the plots for the metrics of this last model, compute the classification report and show the confusion matrix to see in more detail all misclassifications.","45bb9b63":"The use of LSTM layers in the network makes our training to take much more time, only 5 epochs needed 41 minutes reaching accuracies significatively higher than the previous model.","67c21f7e":"#### Prediction for validation set:","e8e9e896":"Let us predict the topics for the validation set and print the first 5 so as to compare them and compute the error metrics:","8adb2404":"### Train-test split:","1f9ad27d":"'label_predicted' contains a key which is 'topic' same name as our actual label, if we show the array contained it corresponds to a matrix predictions for each instance where the highest in each row is the class predicted, therefore we have to apply argmax, firstly let us see such matrix predicted:","0a977645":"The model took approximately 14 minutes and 44 seconds to train for 2 epochs achieving an outstanding train\/test accuracy of 98%\/98% which makes sense as we expect such performance for these complex models, the unique disadvantage of this is the long time it takes to train, later we will see bag of words model desn't take that much, but in terms of undertanding of sentences we can't compare both. Now let us evaluate the model for the test instances and compute the metrics."}}