{"cell_type":{"04de9154":"code","d074d81b":"code","34c64448":"code","3d28ed1f":"code","a4a14cbb":"code","b2c0d01e":"code","5b105e1f":"code","58589a4e":"code","aa8981a0":"code","db74b1b0":"code","27e7ed51":"code","d3bd6cba":"code","6712c98b":"code","e9bb7d86":"code","c3c27954":"code","db678ff0":"code","7d3b29ab":"code","e6551431":"markdown","ac5cbd98":"markdown","f4b12898":"markdown","1c77b696":"markdown","21bde3e7":"markdown","3768f645":"markdown","c8d65e08":"markdown","b564070c":"markdown","30f9160b":"markdown","0f9d1304":"markdown","fbc1a038":"markdown"},"source":{"04de9154":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow_probability as tfp\n\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Input \nfrom tensorflow.keras.optimizers import Adam\n\ntfd = tfp.distributions\ntfb = tfp.bijectors\n\nnp.random.seed(42)\ntf.random.set_seed(42)","d074d81b":"path = '..\/input\/fish-dataset\/fish.csv'\ndf = pd.read_csv(path)\ndf.head()","34c64448":"features = df.drop('count', axis=1).values.astype(np.float32)\nlabels = df['count'].values.astype(np.float32)\n\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42,shuffle=True)","3d28ed1f":"plt.figure(figsize=(14,5))\n\nvals, counts = np.unique(y_train, return_counts=True)\nplt.subplot(1,2,1)\nplt.stem(vals, counts)\nplt.xlabel('Count: number of fish caught')\nplt.ylabel('Frequency')\nplt.title('Distribution of number of fish caught in training')\n\nplt.subplot(1,2,2)\nplt.stem(vals, counts)\nplt.xlabel('Count: number of fish caught')\nplt.ylabel('Frequency')\nplt.xlim(-1,10)\nplt.title('Zoomed distribution of number of fish caught in training')\nplt.show()","a4a14cbb":"model_lr = Sequential() \nmodel_lr.add(Dense(1,input_dim=X_train.shape[1], activation='linear')) \nmodel_lr.compile(loss='mean_squared_error',optimizer=Adam(learning_rate=0.01))\n\nhist_lr = model_lr.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=5000, verbose=0, batch_size=len(y_train))","b2c0d01e":"plt.figure(figsize=(14,8))\nplt.plot(hist_lr.history['loss'])\nplt.plot(hist_lr.history['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.ylabel('MSE')\nplt.xlabel('Epochs')\nplt.show()","5b105e1f":"y_hat_train = model_lr.predict(X_train)\n\nn = len(X_train)\nsigma_hat_2 = (n-1.)\/(n-2.) * np.var(y_train - y_hat_train.flatten(),ddof=1)\nprint('Estimated variance ', sigma_hat_2)\nprint('Estimated standart deviation ', np.sqrt(sigma_hat_2))\n\ny_hat = model_lr.predict(X_test)\nRMSE_lr = np.sqrt(np.mean((y_test - y_hat.flatten())**2))\nMAE_lr = np.mean(np.abs(y_test - y_hat.flatten())) \n\nNLL_lr =  0.5*np.log(2 * np.pi * sigma_hat_2) + 0.5*np.mean((y_test - y_hat.flatten())**2)\/sigma_hat_2\nprint('NLL on training:', 0.5*np.log(2 * np.pi * sigma_hat_2) + 0.5*np.mean((y_train - y_hat_train.flatten())**2)\/sigma_hat_2)\n\ndf1 = pd.DataFrame(\n          {'RMSE' : RMSE_lr, 'MAE' : MAE_lr, 'NLL (mean)' : NLL_lr}, index=['Linear Regression (MSE Keras)']\n)","58589a4e":"inputs = Input(shape=(X_train.shape[1],))  \nrate = Dense(1, activation=tf.exp)(inputs)\np_y = tfp.layers.DistributionLambda(tfd.Poisson)(rate)\n\nmodel_p = Model(inputs=inputs, outputs=p_y)\n\ndef NLL(y_true, y_hat): #D\n    return -y_hat.log_prob(y_true)\n\nmodel_p.compile(Adam(learning_rate=0.01), loss=NLL)\nmodel_p.summary()","aa8981a0":"hist_p = model_p.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=2000, verbose=0)","db74b1b0":"plt.figure(figsize=(14,8))\nplt.plot(hist_p.history['loss'])\nplt.plot(hist_p.history['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.xlabel('Epochs')\nplt.show()","27e7ed51":"model = Model(inputs=inputs, outputs=p_y) \ny_hat_test = model.predict(X_test).flatten()\n\nrmse=np.sqrt(np.mean((y_test - y_hat_test)**2))\nmae=np.mean(np.abs(y_test - y_hat_test)) \n\nNLL = model_p.evaluate(X_test, y_test)\n\ndf2 = pd.DataFrame(\n         { 'RMSE' : rmse, 'MAE' : mae, 'NLL (mean)' : NLL}, index=['Poisson Regression (TFP)']\n)","d3bd6cba":"def quant_mixture_logistic(out, bits=8, num=3): \n    loc, un_scale, logits = tf.split(out,\n                                     num_or_size_splits=num,\n                                     axis=-1)\n    scale = tf.nn.softplus(un_scale)\n    discretized_logistic_dist = tfd.QuantizedDistribution(\n    distribution=tfd.TransformedDistribution(\n        distribution=tfd.Logistic(loc=loc, scale=scale),\n        bijector=tfb.AffineScalar(shift=-0.5)),\n    low=0.,\n    high=2**bits - 1.)\n    mixture_dist = tfd.MixtureSameFamily(\n        mixture_distribution=tfd.Categorical(logits=logits), #logits will be normalized to one\n        components_distribution=discretized_logistic_dist)\n    return mixture_dist","6712c98b":"inputs = tf.keras.layers.Input(shape=(X_train.shape[1],))  \nout = Dense(9)(inputs)\np_y = tfp.layers.DistributionLambda(quant_mixture_logistic)(out)\n\nmodel = Model(inputs=inputs, outputs=p_y)\n\ndef NLL(y_true, y_hat):\n    return -y_hat.log_prob(tf.reshape(y_true,(-1,)))\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=NLL)\n\nmodel.summary()","e9bb7d86":"hist_mm = model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), epochs=10, verbose=1)","c3c27954":"plt.figure(figsize=(14,8))\nplt.plot(hist_mm.history['loss'])\nplt.plot(hist_mm.history['val_loss'])\nplt.legend(['loss', 'val_loss'])\nplt.xlabel('Epochs')","db678ff0":"NLL_train = model.evaluate(X_train, y_train,verbose=0) \nNLL_test = model.evaluate(X_test, y_test,verbose=0) \n\nprint('NLL on training:', NLL_train)\nprint('NLL on test:', NLL_test)\n\npreds = np.zeros((1000,len(y_test.flatten())))\nfor i in range(0,1000):\n    preds[i,:] = model(X_test).sample().numpy()\ny_hat_test=np.average(preds,axis=0)\n\nmse=np.sqrt(np.mean((y_test - y_hat_test)**2))\nmae=np.mean(np.abs(y_test - y_hat_test))\n\ndf3 = pd.DataFrame(\n         { 'RMSE' : mse, 'MAE' : mae, 'NLL (mean)' : NLL_test}, index=['ZIP (TFP)']\n)","7d3b29ab":"result = pd.concat([df1,df2,df3])\nprint(result)","e6551431":"<h1 id=\"reference\" style=\"color:#b56193; background:#24b7e4;\"> \n    <center>Reference\n        <a class=\"anchor-link\" href=\"#reference\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","ac5cbd98":"## Evaluation","f4b12898":"<h1 id=\"mixture\" style=\"color:#b56193; background:#24b7e4;\"> \n    <center>Regression with a discretized logistic mixture distribution \n        <a class=\"anchor-link\" href=\"#mixture\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","1c77b696":"<h1 id=\"dataset\" style=\"color:#b56193; background:#24b7e4;\"> \n    <center>Dataset\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","21bde3e7":"[Probabilistic Deep Learning](https:\/\/www.manning.com\/books\/probabilistic-deep-learning)","3768f645":"## Evaluation","c8d65e08":"<h1 id=\"poisson\" style=\"color:#b56193; background:#24b7e4;\"> \n    <center>Poisson Regression\n        <a class=\"anchor-link\" href=\"#poisson\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","b564070c":"<h1 id=\"linear\" style=\"color:#b56193; background:#24b7e4;\"> \n    <center>Linear Regression\n        <a class=\"anchor-link\" href=\"#linear\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","30f9160b":"## Evaluate","0f9d1304":"<h1 id=\"result\" style=\"color:#b56193; background:#24b7e4;\"> \n    <center>Result\n        <a class=\"anchor-link\" href=\"#result\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","fbc1a038":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1354114\/2251095\/264cdf126411d75ddd7601a2876049ac\/dataset-cover.jpeg\"\/>\n<\/div>"}}