{"cell_type":{"394ead7b":"code","7f24eae7":"code","9bd806bc":"code","95d6fc3f":"code","d08f8368":"code","3c3b53c4":"code","0fb03552":"code","ea1b9002":"code","dfa34d74":"code","7fa87f43":"code","13978467":"code","fbf75e42":"code","de2afe5a":"code","bd0db652":"code","5bd4f473":"code","f1f6fb73":"code","8e45d1e6":"code","296d8b33":"code","e7042b21":"code","d6b28588":"code","756baa99":"code","eadf49ac":"code","4ee82cc3":"code","46afbcf8":"code","33ebcc13":"code","89e44ffd":"code","1831e259":"code","631a41f9":"code","091194ab":"code","52688e66":"code","46d77533":"code","29f9294e":"code","69a3c9a8":"code","3948c39a":"code","2cf1d537":"markdown","dceeb654":"markdown","62e55103":"markdown","c75a864e":"markdown","0632ddd6":"markdown","82e10b5f":"markdown","1c079ba7":"markdown","91a0ace0":"markdown","b4f413d0":"markdown","e1b54558":"markdown","b5de6da6":"markdown","2fdea6be":"markdown"},"source":{"394ead7b":"CUDA_LAUNCH_BLOCKING=1 ","7f24eae7":"import spacy\nimport os\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport re\nimport torch\nimport json\nimport string\nimport sys\nimport random\nimport time","9bd806bc":"# read meta data\nimport pandas as pd\ndf = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv')\ndf.shape","95d6fc3f":"from tqdm import tqdm\npath = \"..\/input\/CORD-19-research-challenge\/document_parses\"\n# sub-folder\nsubdir = [\"pdf_json\", \"pmc_json\"]\narticle = []\n\nfor d in subdir:\n    for f in tqdm(os.listdir(f\"{path}\/{d}\")):\n        json_file = json.load(open(f\"{path}\/{d}\/{f}\", \"rb\"))\n        title = json_file[\"metadata\"][\"title\"]\n        if d == \"pdf_json\":\n            abstract = \"\\n\\n\".join([t[\"text\"] for t in json_file[\"abstract\"]])\n        else:\n            abstract = \"\\n\"\n        body = \"\\n\\n\".join([t[\"text\"] for t in json_file[\"body_text\"]])\n        paper_id = json_file[\"paper_id\"]\n        article.append([paper_id, title, abstract, body])","d08f8368":"import gc\narticle_df = pd.DataFrame(article, columns = [\"paper_id\", \"title\", \"abstract\", \"body\"])\ndel article\ngc.collect()\narticle_df.head","3c3b53c4":"!pip install spacy_langdetect","0fb03552":"import string\nfrom nltk.stem import WordNetLemmatizer\nfrom spacy_langdetect import LanguageDetector\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport gensim\nfrom langdetect import detect\nfrom nltk.corpus import stopwords\nfrom pprint import pprint\nfrom gensim.models.doc2vec import Doc2Vec\n\nnltk.download('stopwords')\nnltk.download('wordnet')\nnlp = spacy.load('en')\nnlp.add_pipe(LanguageDetector(), name='language_detector', last=True)\n\nclass PreProcess(BaseEstimator, TransformerMixin):\n    def tokenizer(self, input_text):\n        return re.split('\\W+', input_text)\n\n    def remove_urls(self, input_text):\n        return re.sub(r'http.?:\/\/[^\\s]+[\\s]?', '', input_text)\n    \n    def remove_punctuation(self, input_text):\n        trantab = str.maketrans('', '', string.punctuation)\n        return input_text.translate(trantab)\n    \n    def remove_digits(self, input_text):\n        return re.sub('\\d+', '', input_text)\n    \n    def to_lower(self, input_text):\n        return input_text.lower()\n    \n    def remove_stopwords(self, words):\n        stopwords_list = stopwords.words('english')\n        whitelist=[]\n        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n        return clean_words\n    \n    def stemming(self, words):\n        porter = PorterStemmer()\n        stemmed_words = [porter.stem(word) for word in words]\n        return \" \".join(stemmed_words)\n\n    def lemma(self,words):\n        lemmatizer = WordNetLemmatizer()\n        stemmed_words = [lemmatizer.lemmatize(word) for word in words]\n        return stemmed_words \n    def english_only(self, words):\n        english_words = []\n        for word in words:\n            if detect(word) == 'en':\n                english_words.append(word)\n        return english_words\n        ","ea1b9002":"def clean(text):\n    pp = PreProcess() \n    text = str(text)\n    clean = pp.remove_urls(text)\n    clean = pp.remove_punctuation(clean)\n    clean = pp.remove_digits(clean)\n    clean = pp.to_lower(clean)\n    clean = pp.tokenizer(clean)\n    clean = pp.remove_stopwords(clean)\n    clean = pp.lemma(clean)\n    return clean","dfa34d74":"article_df.shape","7fa87f43":"# title\ntitle_tokenized = []\ntitle = article_df['title'].values\nfor i in title:\n    title_tokenized.append(clean(i))\ntitle_tokenized = np.array(title_tokenized)\n# body\nbody_tokenized = []\nbody = article_df['body'].values\nfor i in title:\n    body_tokenized.append(clean(i))\nbody_tokenized = np.array(body_tokenized)\n# abstract\nabstract_tokenized = []\nabstract = article_df['abstract'].values\nfor i in title:\n    abstract_tokenized.append(clean(i))\nabstract_tokenized = np.array(abstract_tokenized)","13978467":"#Clean title, text and abstract \narticle_df['title_tokenized'] = title_tokenized\narticle_df['body_tokenized'] = body_tokenized\narticle_df['abstract_tokenized'] = abstract_tokenized","fbf75e42":"#Combine title, text, and abstract\narticle_df['complete_text_tokenized'] = article_df['title_tokenized'] + article_df['body_tokenized'] + article_df['abstract_tokenized']\nselected_article = article_df[article_df['complete_text_tokenized'].map(len) > 100]\n\n#Describing our final dataframe.\nselected_article.describe","de2afe5a":"selected_article.shape","bd0db652":"def read_corpus(df, column):\n    for i, line in enumerate(df[column]):\n        yield gensim.models.doc2vec.TaggedDocument(line, [i])\n\ntrain_df  = selected_article.sample(frac=1, random_state=42)\n\n#train corpus\ntrain_corpus = (list(read_corpus(train_df, 'complete_text_tokenized'))) ","5bd4f473":"# Doc2VEC : using distributed memory model\nmodel = gensim.models.doc2vec.Doc2Vec(dm=1, vector_size=300, min_count=10, epochs=20, seed=42, workers=10)\nmodel.build_vocab(train_corpus)\nmodel.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)","f1f6fb73":"task5 = \"What do we know about the effectiveness of non-pharmaceutical interventions? What is known about equity and barriers to compliance for non-pharmaceutical interventions? Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases.Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases. Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments.Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches. Methods to control the spread in communities, barriers to compliance and how these vary among different populations.. Models of potential interventions to predict costs and benefits that take account of such factors as race, income, disability, age, geographic location, immigration status, housing status, employment status, and health insurance status. Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs. Research on why people fail to comply with public health advice, even if they want to do so (e.g., social or financial costs may be too high). Research on the economic impact of this or any pandemic. This would include identifying policy and programmatic alternatives that lessen\/mitigate risks to critical government services, food distribution and supplies, access to critical household supplies, and access to health diagnoses, treatment, and needed care, regardless of ability to pay.\"","8e45d1e6":"!pip install rake_nltk","296d8b33":"from rake_nltk import Rake\nr = Rake() \nr.extract_keywords_from_text(task5)\n\ntask = ' '.join(r.get_ranked_phrases())","e7042b21":"task","d6b28588":"def get_doc_vector(doc):\n    tokens = clean(doc) \n    vector = model.infer_vector(tokens)\n    return vector\n\ntask_array = [get_doc_vector(task)]","756baa99":"from sklearn.neighbors import NearestNeighbors\nselected_article['complete_text_vector'] = [vec for vec in model.docvecs.vectors_docs]\ntext_array = selected_article['complete_text_vector'].values.tolist()\n\n#Apply KNN to extract 50 neighbors\nball_tree = NearestNeighbors(algorithm='ball_tree', leaf_size=20).fit(text_array)\ndistances, indices = ball_tree.kneighbors(task_array, n_neighbors=80)\n\ndf_output = pd.DataFrame(columns=['Task','Result_Paper_ID','complete_text_tokenized'])","eadf49ac":"article_df.to_csv(\".\/article_df.csv\", sep=\",\" , encoding='utf-8')\nselected_article.to_csv(\".\/selected_article.csv\", sep=\",\", encoding='utf-8')","4ee82cc3":"del selected_article\ngc.collect()","46afbcf8":"!pwd","33ebcc13":"for i, info in enumerate([task]):\n    df =  article_df.iloc[indices[i]]\n    dist = distances[i]\n    papers_ids = df['paper_id']\n    titles = df['title']\n    complete_texts_tokenized = df['complete_text_tokenized']\n    for l in range(len(dist)):\n        df_output = df_output.append({'Task': i, 'Result_Paper_ID' : papers_ids.iloc[l], 'complete_text_tokenized' : complete_texts_tokenized.iloc[l]}, ignore_index=True)\ndf_output.to_csv('df_output.csv', sep=',', encoding='utf-8')\ndf_output.shape","89e44ffd":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom ast import literal_eval\nstopwords = set(STOPWORDS)\nnew_stopwords = ['copyright', \"manuscript\",\"funders\",'pmc', \"et\", \"europe\",\"al\", 'license', 'display', 'author', 'preprint', 'patient', 'authorfunder','ef','using', 'new', 'set', 'yet', 'fully', 'expected', 'medrxiv', 'available', 'granted','futhermore']\nnew_stopwords_list = stopwords.union(new_stopwords)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='lightblue',\n        stopwords=new_stopwords_list,\n        max_words=500,\n        max_font_size=40, \n        scale=5,\n        random_state=2020\n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15,15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=14)\n        fig.subplots_adjust(top=2)\n  \n    plt.imshow(wordcloud)\n    plt.show()\n\ndf_output = pd.read_csv('df_output.csv')\nnpi = df_output['complete_text_tokenized']","1831e259":"%matplotlib inline \nlem = WordNetLemmatizer()\n\nwords = []\nfor i in npi : \n    keywords= literal_eval(i)\n    for j in keywords:\n        words.append(lem.lemmatize(j))\n\nwords = ' '.join(words)\nshow_wordcloud(words, title = 'Task : What do we know about non-pharmaceutical interventions?')","631a41f9":"del df_output\ngc.collect()","091194ab":"torch.cuda.is_available()","52688e66":"from transformers import BertTokenizer, BertForQuestionAnswering\n\ndevice =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntokenizer = BertTokenizer.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\")\nQA = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\").to(device)","46d77533":"from IPython.display import display, HTML\ndef convert_to_str(token_id):\n    \"\"\"\n    Convert token id to str\n    \"\"\"\n    tokens = tokenizer.convert_ids_to_tokens(token_id)\n    return tokenizer.convert_tokens_to_string(tokens)\n    \n\ndef QA_inference(query, search_on, df):\n    \"\"\"\n    Inference processor for Bert model\n    ========\n    query:       question input [str]\n    search_on:   title, abstract, body\n    df:          data\n    n:           \n    \"\"\"\n    # Init \n    token_id, score, span = [], [], []\n    \n    for i in tqdm(range(len(df))):\n        ids = tokenizer.encode(query, df[search_on][i])\n        token_type_id = [0 if i <= ids.index(102) else 1 for i in range(len(ids))]\n        \n        if len(ids) > 512:\n            if search_on == \"title\" or search_on == \"abstract\":\n                ids, token_type_id = ids[:511] + [102], token_type_id[:512]\n            else:\n                h = (len(ids) - 512)\/\/2 \n                ids, token_type_id = ids[h:h+511] + [102], token_type_id[h:h+512]\n\n\n        # Tensors\n        ids_tensor = torch.tensor([ids]).to(device)\n        token_type_id_tensor = torch.tensor([token_type_id]).to(device)\n        \n        # Inferencing\n        start_scores, end_scores = QA(ids_tensor, token_type_ids=token_type_id_tensor)\n        \n        # releasing gpu memory\n        ids_tensor, token_type_id_tensor, start_scores, end_scores = \\\n            tuple(map(lambda x: x.to('cpu').detach().numpy(), (ids_tensor, token_type_id_tensor, start_scores, end_scores)))\n        \n        span.append([start_scores.argmax(), end_scores.argmax()+1])\n        score.append([start_scores.max(), end_scores.max()])\n        token_id.append(ids)\n    \n    span, score = np.array(span), np.array(score)\n    return span, score, token_id\n","29f9294e":"def display_res(spans, scores, search_on, token_ids, data, top_n=10):\n    min_scores = scores.min(axis=1) \n    sorted_idx = (-min_scores).argsort() # Descending order\n    \n    counter = 0    \n    for idx in sorted_idx:\n        if counter >= top_n:\n            break\n        if spans[idx,0] == 0 or spans[idx,1] == 0 or \\\n            spans[idx,1]<=spans[idx,0]:\n            continue\n        start, end = spans[idx, :]\n\n        text = data[search_on][idx]\n        highlight = convert_to_str(token_ids[idx][start:end])\n        \n        start = text.lower().find(highlight)\n        if start == -1:\n            text = convert_to_str(token_ids[idx]\n                                      [token_ids[idx].index(102)+1:])\n            start = text.find(highlight)\n            end = start + len(highlight)\n            text = text[:-5]\n        else:\n            end = start + len(highlight)\n            highlight = text[start:end]\n        before, after = text[: start], text[end : ]\n    \n        # Putting information in HTML format\n        html_ = f\"<text style=color:red><b>Answer: {highlighted}<\/b><\/text><br><br>\" + \\\n                f\"<b>({count+1}) {df['title'][i]} <\/b><br>\" + \\\n                f\"Score: {score[i].min()} <br>\" + \\\n                \"<p style=line-height:1.5><font size=4>\" + \\\n                before + \\\n                f\"<text style=color:red>{highlighted}<\/text>\" + \\\n                after + \\\n                \"<\/font><\/p>\"\n        \n        display(HTML(html_))\n        \n        counter += 1\n\ndef final(question, search_on, df, top_n=10):\n    \n    spans, scores, token_ids = QA_inference(question, search_on, df)\n    display_res(spans, scores, search_on, token_ids, df, top_n)\n    ","69a3c9a8":"Question = \"What do we know about non-pharmaceutical interventions?\"","3948c39a":"final(Question, \"abstract\", article_df)","2cf1d537":"## Inference","dceeb654":"**Start & End Token Classifiers**\n\n`BERT` needs to highlight a `span` of text containing the answer\u2013this is represented as simply predicting which token marks the start of the answer, and which token marks the end.","62e55103":"Organizing all articles","c75a864e":"Clean and Tokenize text","0632ddd6":"## Result","82e10b5f":"# Q&A Bert Model","1c079ba7":"# COIVD Research Challenge - Non-Pharmaceutical Intervention\n\nCreated by a TransUnion[https:\/\/www.transunion.com\/] data scientist that believes that information can be used to **change our world for the better**. #InformationForGood\n\n# Introduction\n\nI took advantage of NLP and ML tools to develop improved ways of finding relevant research. For the competition, 10 tasks have been proposed. Each task covers some fundamental questions related to COVID-19. In this notebook, my focus is on discovering the task related to **non-pharmaceutical interven**\n\n# Method\n\n**Tools**\n\n    - EDA \n        * Doc2Vec\n        * WordCloud\n        \n    - Model\n        * Bert QA\n\n## 1. Searching the COVID-19 Article Dataset\n\nThe dataset for this competition contains more than 100,000 articles, where they are saved in JSON format. To make it used efficiently, I made a dataframe organizing these articles and contain their `title`, `abstract`, and `body text`.\n\nThe idea of searching related article to non-pharmaceutical intervention is looking for the nearest neighbors in text dimension.  To convert text data into a context can be read by numeric distribution, I used Doc2Vec to represent each article.\n\n## 2. Answering Non-Pharmaceutical Intervention Question From Data\n\nBERT is a contextual word representation model learned from large-scale language model pretraining of a bidirectional Transformer (Vaswani et al. 2017). Recent work has shown major improvements in a wide variety of tasks using BERT or similar Transformer models. The query taken as a input will be a general question for NPI task, and we are looking for the span of answers from most similar articles. \n","91a0ace0":"Train a doc2vec model\n ","b4f413d0":"Before use Doc2vec, use Rake to extract keyword.\n\n`RAKE` short for Rapid Automatic Keyword Extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text.\n\nTurn task detail into a vector","e1b54558":"Check out metadata info","b5de6da6":"Visualize key words in articles related to task5","2fdea6be":"![Example1](https:\/\/www.kaggle.com\/trexwithoutt\/plot-anwser\/1.png)\n    "}}