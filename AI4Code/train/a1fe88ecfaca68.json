{"cell_type":{"5b9b0d41":"code","f0d05d76":"code","e6797177":"code","cd565dd4":"code","0aac4e0d":"code","7ee0682a":"code","8d09ec91":"code","a8bc24ad":"code","dd6c1784":"code","d451dcd6":"code","5586b1da":"code","1310182e":"code","93f00ce1":"code","672cc1db":"code","e4c08ff6":"code","7648fded":"code","54287623":"code","6dd099a4":"code","d5c8da53":"markdown","98af3007":"markdown","ccf6e945":"markdown","1c2789c7":"markdown","2e190099":"markdown","6c601c3f":"markdown"},"source":{"5b9b0d41":"#import libraries \nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\n\n#ignore warning\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\n#use xgb for the second classifier\nimport xgboost as xgb","f0d05d76":"#read dataset\ncbb = pd.read_csv('..\/input\/college-basketball-dataset\/cbb.csv')\ncbb.head(10)","e6797177":"#get all rows without year == 2019\ncbb_ = cbb[~(cbb.YEAR == 2019)]\ncbb_.head(10)\n\n","cd565dd4":"for i,el in enumerate(cbb_):\n    print(el +'   is')\n    print(pd.unique(cbb_[el]))\n    print('-'*30)","0aac4e0d":"\n# generate winrate\ncbb_['WINRATE'] = cbb_['W']\/cbb_['G']\n# generate winrate above bubble\ncbb_['WINRATE_BUBBLE'] = cbb_['WAB']\/cbb_['G']\n#drop W ,G ,team name, Winsabovebubble\ncbb_.drop(columns = ['W','G','TEAM','WAB'],inplace = True)\ncbb_.columns.values","7ee0682a":"#mapping POSTSEASON\npos_mapping = {'Champions':0,'2ND':1,'F4':2,'E8':3,'S16':4,'R32':5,'R64':6,'R68':7}\ncbb_['POSTSEASON'] = cbb_['POSTSEASON'].map(pos_mapping)\ncbb_['POSTSEASON'] = cbb_['POSTSEASON'].fillna(8)\n\n#mapping SEED, punish team without seed\ncbb_['SEED'] = cbb_['SEED'].fillna(30)\n\n#mapping CONF\nconf_mapping = {'WCC':0,'ACC':1 ,'B10':2 ,'SEC':3, 'B12' :4,'Amer':5, 'BE' :6,'MAC':7, 'SC':8, 'MWC':9, 'A10':10, 'P12':11,\n 'OVC': 12,'WAC':13, 'BSth':14, 'BW':15, 'AE': 16,'CAA':17, 'Ivy':18, 'Horz':19,'SB':20, 'CUSA':21, 'Pat':22, 'BSky':23,\n 'MVC': 24,'Slnd':25, 'Sum' :26,'MAAC': 27,'SWAC':28, 'NEC':29 ,'MEAC':30, 'ASun':31}\ncbb_['CONF'] = cbb_['CONF'].map(conf_mapping)\ncbb_['CONF'] = cbb_['CONF'].fillna(100)\n\n#no need to map school name cuz too big and corupt the correlation just drop it\n\n#after preparation finally to split data\ntrain_set = cbb_[~(cbb_.YEAR == 2018)]\ntest_set = cbb_[(cbb_.YEAR == 2018)]\n\n#feature selection in later version\ntrain_set.head(10)","8d09ec91":"\"\"\"\n#just for test visualizing\ntest_cbb = cbb\n\n#plot a multiplot \n\ndf = pd.DataFrame({'g':test_cbb['G'],'P':test_cbb['POSTSEASON']})\nsns.jointplot(x='g',y='P',data = df)\n\"\"\"","a8bc24ad":"#see heatmap the correlation between features\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(24,22))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_set.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","dd6c1784":"#just copied\n# Some useful parameters which will come in handy later on\nntrain = train_set.shape[0]#number of train sets\nntest = test_set.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = KFold(n_splits = NFOLDS, shuffle = False, random_state=None)\n\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)","d451dcd6":"#create Out of Folder function\ndef get_oof(clf, x_train, y_train, x_test):\n    #\u8bad\u7ec3\u96c6\u7684\u6837\u672c\u6570\n    oof_train = np.zeros((ntrain,))\n    #\u6d4b\u8bd5\u96c6\u7684\u6837\u672c\u6570\n    oof_test = np.zeros((ntest,))\n    #(5\uff0c\u6d4b\u8bd5\u96c6\u7684\u6837\u672c\u6570)\n    oof_test_skf = np.empty((NFOLDS, ntest))\n    \n    #\u5bf9\u4e8e\u5927\u8bad\u7ec3\u96c6\u5206\u62105\u5206\uff0c\u7ec6\u5206\u6210\u4e3a4\u4e2a\u5c0f\u8bad\u7ec3\u96c6\u548c1\u4e2a\u5c0f\u6d4b\u8bd5\u96c6\n    for i,(train_index, test_index) in enumerate(kf.split(x_train)):\n        x_tr = x_train[train_index]#\u8bad\u7ec3\u96c6x\n        y_tr = y_train[train_index]#\u8bad\u7ec3\u96c6y\n        x_te = x_train[test_index]#\u6d4b\u8bd5\u96c6x\n\n        clf.train(x_tr, y_tr)#\u8bad\u7ec3 \u5c0f\u8bad\u7ec3\u96c6\n\n        oof_train[test_index] = clf.predict(x_te)#\u5bf9\u5c0f\u6d4b\u8bd5\u96c6 predict\n        oof_test_skf[i, :] = clf.predict(x_test)#\u5bf9\u5927\u6d4b\u8bd5\u96c6 predict\n\n    oof_test[:] = oof_test_skf.mean(axis=0)#kfold \u4e4b\u540e\u5bf9\u4e8e\u5927\u6d4b\u8bd5\u96c6\u7684 predict \u6c42mean\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)#only one column and all values are in one column","5586b1da":"# Put in our parameters for said classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }","1310182e":"# Create 5 objects that represent our 4 models\n#rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)","93f00ce1":"# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\ny_train = train_set['POSTSEASON'].ravel()#\u5c55\u5f00\u62101d\ntrain_set = train_set.drop(['POSTSEASON'], axis=1)\nx_train = train_set.values # Creates an array of the train data\n\ny_test = test_set['POSTSEASON'].ravel()\ntest_set = test_set.drop(['POSTSEASON'], axis=1)\nx_test = test_set.values # Creats an array of the test data\n\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n#rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n\nprint(\"Training is complete\")","672cc1db":"rf_feature = rf.feature_importances(x_train,y_train)\net_feature = et.feature_importances(x_train, y_train)\nada_feature = ada.feature_importances(x_train, y_train)\ngb_feature = gb.feature_importances(x_train,y_train)\n#svc_feature = svc.feature_importances(x_train,y_train)","e4c08ff6":"x_train = np.concatenate(( et_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","7648fded":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(x_train, y_train)\npredictions = gbm.predict(x_test)\nprint(predictions)","54287623":"from sklearn import metrics\nprint(y_test)\nscore = metrics.accuracy_score(y_test, predictions)\nprint(f\"Test score: {score}\")","6dd099a4":"#create output csv\n\"\"\"\n# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n\"\"\"\n","d5c8da53":"# Understand all features\njust copied it from the description\n\n\nTEAM: Name of all schools participated into the contest \u5b66\u6821\u7684\u540d\u5b57\n**looks like irrelevant from the first sight but\n1. maybe the school names also indicate the location and has some correlation with CONF\n**\n\nCONF: The Athletic Conference in which the school participates in (A10 = Atlantic 10, ACC = Atlantic Coast Conference, AE = America East, Amer = American, ASun = ASUN, B10 = Big Ten, B12 = Big 12, BE = Big East, BSky = Big Sky, BSth = Big South, BW = Big West, CAA = Colonial Athletic Association, CUSA = Conference USA, Horz = Horizon League, Ivy = Ivy League, MAAC = Metro Atlantic Athletic Conference, MAC = Mid-American Conference, MEAC = Mid-Eastern Athletic Conference, MVC = Missouri Valley Conference, MWC = Mountain West, NEC = Northeast Conference, OVC = Ohio Valley Conference, P12 = Pac-12, Pat = Patriot League, SB = Sun Belt, SC = Southern Conference, SEC = South Eastern Conference, Slnd = Southland Conference, Sum = Summit League, SWAC = Southwestern Athletic Conference, WAC = Western Athletic Conference, WCC = West Coast Conference) \n**looks like this CONF is geographically related and symbolize which sector does which school plays in**\n\nG: Number of games played\n**the more games played, the more skilled**\n\nW: Number of games won\n**the more games won, the more energetic players are and higher the spirit**\n\nADJOE: Adjusted Offensive Efficiency (An estimate of the offensive efficiency (points scored per 100 possessions) a team would have against the average Division I defense)\n**seems to me its \u8fdb\u653b\u6548\u7387\uff0c\u6bcf100\u6b21\u63a7\u7403\u8fdb\u653b\u5f97\u5206\uff0c\u5bf9\u4e8e\u5e73\u5747defense \u6765\u8bf4\nso the more points scored, the more likely the team to be categorized as a offensive team, which means they play more aggresive\n**\n\nADJDE: Adjusted Defensive Efficiency (An estimate of the defensive efficiency (points allowed per 100 possessions) a team would have against the average Division I offense)\n**seems to me its \u9632\u5b88\u6548\u7387\uff0c\u6bcf100\u6b21(\u5bf9\u65b9)\u63a7\u7403\u6211\u4eec\u88ab\u5f97\u7684\u5206\uff0c\u5bf9\u4e8e\u5e73\u5747offense \u6765\u8bf4\n(here im not sure if points allowed means the points scored by offense or the points we managed to prevent, let me assue its the first situation) so the higher ADJDE, the worse we play defense\nso the more points scored, the more likely the team to be categorized as a defensive team, which means they play more defensive\n**\n\nBARTHAG: Power Rating (Chance of beating an average Division I team)\n**general score of power**\n\nEFG_O: Effective Field Goal Percentage Shot\n**\u6709\u6548\u573a\u5747\u547d\u4e2d\u7387**\n\nEFG_D: Effective Field Goal Percentage Allowed\n**\u5bf9\u65b9\u6709\u6548\u573a\u5747\u547d\u4e2d\u7387**\n\nTOR: Turnover Percentage Allowed (Turnover Rate)\n**\u5931\u8bef\u7387**\n\nTORD: Turnover Percentage Committed (Steal Rate)\n**\u62a2\u65ad\u7387**\n\nORB: Offensive Rebound Percentage\n**\u573a\u5747\u8fdb\u653b\u7bee\u677f**\n\nDRB: Defensive Rebound Percentage\n**\u573a\u5747\u9632\u5b88\u7bee\u677f**\n\nFTR : Free Throw Rate (How often the given team shoots Free Throws)\n**\u7f5a\u7403\u7387**\n\nFTRD: Free Throw Rate Allowed\n**\u5bf9\u65b9\u7f5a\u7403\u7387**\n\n2P_O: Two-Point Shooting Percentage\n**2\u5206\u547d\u4e2d\u7387**\n\n2P_D: Two-Point Shooting Percentage Allowed\n**\u5bf9\u65b92\u5206\u51fa\u624b\u7387**\n\n3P_O: Three-Point Shooting Percentage\n**3\u5206\u547d\u4e2d\u7387**\n\n3P_D: Three-Point Shooting Percentage Allowed\n**\u5bf9\u65b93\u5206\u51fa\u624b\u7387**\n\nADJ_T: Adjusted Tempo (An estimate of the tempo (possessions per 40 minutes) a team would have against the team that wants to play at an average Division I tempo)\n**\u63a7\u7403\u7387**\n\nWAB: Wins Above Bubble (The bubble refers to the cut off between making the NCAA March Madness Tournament and not making it)\n**\u80dc\u5229\u57fa\u4e8e\u73b0\u573a\u71c3\u7206\uff01**\n\nPOSTSEASON: Round where the given team was eliminated or where their season ended (R68 = First Four, R64 = Round of 64, R32 = Round of 32, S16 = Sweet Sixteen, E8 = Elite Eight, F4 = Final Four, 2ND = Runner-up, Champion = Winner of the NCAA March Madness Tournament for that given year)\n**\u603b\u51b3\u8d5b\u7b2c\u51e0\u5f3a**\n\nSEED: Seed in the NCAA March Madness Tournament\n**\u79cd\u5b50\u9009\u624b**\n\nYEAR: Season\n**\u8d5b\u5b63**","98af3007":"Visualization work maybe for the version 0.2 here i will just cleanse, mapping and generate new features\n\ndecision:\n1. mapping all string elements with integer\n2. fill nan with differnt number \n3. delete 'G' and 'W' generate new feature winrate\n\n\np.s. from the visualization\n1. here i fill the nan with big numbers, later should be changed to mean of other values\n2. mapping conf may be first sorted by the win rate\n3. calculate win rate= win times\/game times","ccf6e945":"After overviewing the meaning of each feature, lets take a look into details of the parameters of each feature","1c2789c7":"# training","2e190099":"# Before everything\nthe task 1 is to find the top 10 teams, which i think is a prediction problem, so first i will just do a small test without cross validation, so i will just use cbb15-17 as training set and cbb18 as testing set, as i noticed that cbb19 has 2 more columns which could be 2 more teams.","6c601c3f":"# Review of normal procedure\n1. understand all features and categorize them into numerical values and categoritical values\n2. feature engineering: analyse features by visualizing them and drop the most unrelevant features and generate new representative features based on old features\n3. feature engineering: 7C methods to pre-perpare datasets(cleanse, convert, clear...)\n4. create basemodel to make first-level prediction\n5. ensemble and stack: use the output of first-level prediction to make second-level prediction\n6. score the model "}}