{"cell_type":{"8cf4b748":"code","35079379":"code","412db07c":"code","11f0aa1e":"code","b86e3e12":"code","f53287c6":"code","85a1c517":"code","16bfbacf":"code","6f1ead9b":"code","6ba834eb":"code","694b3da7":"code","a4f6df32":"code","3297b2a9":"code","9760a154":"code","de838a8f":"code","8074fa57":"code","5b1790f1":"code","ae8f9e7f":"code","20f60d0b":"code","0a6cf8d5":"code","b300d7f7":"code","bc7f4002":"code","075b642f":"code","3a5f67c1":"code","8069561b":"code","6fd2d868":"code","4ec8c2e4":"code","80dda5e8":"code","3cc9b648":"code","b321ed2b":"code","9640c78d":"code","bdec3d54":"code","053ad00a":"markdown","a62ee3de":"markdown","fe453f48":"markdown","6fefd30d":"markdown","62fe4396":"markdown","759ae211":"markdown","dddc162e":"markdown","734e03ae":"markdown","273a3f42":"markdown","e84f4e69":"markdown","4cc2319f":"markdown","a30c61a9":"markdown","9bd1a798":"markdown"},"source":{"8cf4b748":"import re    # for regular expressions \nimport nltk  # for text manipulation \nimport string # for text manipulation \nimport warnings \nimport numpy as np \nimport pandas as pd # for data manipulation \nimport matplotlib.pyplot as plt\n\npd.set_option(\"display.max_colwidth\", 200) \nwarnings.filterwarnings(\"ignore\") #ignore warnings\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","35079379":"\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score, classification_report\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn import svm\n\n#from sklearn.feature_extraction.text import CountVectorizer","412db07c":"data = pd.read_csv(\"\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\",encoding='latin-1')\ndata.head()","11f0aa1e":"DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"TweetText\"]\ndata.columns = DATASET_COLUMNS\ndata.head()","b86e3e12":"# Let's keep only target variable and tweets text\ndata.drop(['ids','date','flag','user'],axis = 1,inplace = True)","f53287c6":"data.head()","85a1c517":"positif_data = data[data.target==4].iloc[:25000,:]\nprint(positif_data.shape)\nnegative_data = data[data.target==0].iloc[:1000,:]\nprint(negative_data.shape)","16bfbacf":"tweet_df = pd.concat([positif_data,negative_data],axis = 0)\nprint(tweet_df.shape)\ntweet_df.head()","6f1ead9b":"#Check for missing values\n100*tweet_df.isna().sum()\/len(tweet_df)","6ba834eb":"stopwords=nltk.corpus.stopwords.words('english')","694b3da7":"def remove_stopwords(text):\n    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n    return clean_text","a4f6df32":"from nltk.stem.porter import PorterStemmer\ndef cleanup_tweets(tweet_df):\n    # remove handle\n    tweet_df['clean_tweet'] = tweet_df['TweetText'].str.replace(\"@\", \"\") \n    # remove links\n    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].str.replace(r\"http\\S+\", \"\") \n    # remove punctuations and special characters\n    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].str.replace(\"[^a-zA-Z]\", \" \") \n    # remove stop words\n    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda text : remove_stopwords(text.lower()))\n    # split text and tokenize\n    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda x: x.split())\n    # let's apply stemmer\n    stemmer = PorterStemmer()\n    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])\n    # stitch back words\n    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda x: ' '.join([w for w in x]))\n    # remove small words\n    tweet_df['clean_tweet'] = tweet_df['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","3297b2a9":"cleanup_tweets(tweet_df)","9760a154":"# function to display word clous\nfrom wordcloud import WordCloud,STOPWORDS\ndef createWrdCloudForSentiment(target):\n    temp_df = pd.DataFrame() \n    if target == -1:\n        temp_df = tweet_df\n    else:\n        temp_df = tweet_df[tweet_df.target==target]\n    words = \" \".join(temp_df.clean_tweet)\n    wrdcld = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=1500,\n                      height=1000).generate(words)\n    plt.figure(figsize=(10,10))\n    plt.imshow(wrdcld)\n    plt.axis('off')\n    plt.show","de838a8f":"createWrdCloudForSentiment(-1)","8074fa57":"# tweets for positive sentiment\ncreateWrdCloudForSentiment(4) \n","5b1790f1":"# tweet for depressive sentiments\ncreateWrdCloudForSentiment(0) ","ae8f9e7f":"# Data balance\ndef createPieChartFor(t_df):\n    Lst = 100*t_df.value_counts()\/len(t_df)\n    \n    # set data for pie chart\n    labels = t_df.value_counts().index.values\n    sizes =  Lst \n    \n    # set labels\n    fig1, ax1 = plt.subplots()\n    ax1.pie(sizes, labels=labels, autopct='%1.2f%%', shadow=True, startangle=90)\n    ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n    plt.show()","20f60d0b":"createPieChartFor(tweet_df.target)","0a6cf8d5":"#count_vectorizer = CountVectorizer(stop_words='english') \n#cv = count_vectorizer.fit_transform(data['Clean_TweetText'])\n#cv.shape","b300d7f7":"#X_train,X_test,y_train,y_test = train_test_split(tweet_df , test_size=.2,  random_state=42)\ntrain_df, test_df = train_test_split(tweet_df, test_size=0.3, random_state=42)","bc7f4002":"createPieChartFor(train_df.target)","075b642f":"train_tweets =[]\nfor tweet in train_df.clean_tweet:\n    train_tweets.append(tweet)\n    \ntest_tweets =[]\nfor tweet in test_df.clean_tweet:\n    test_tweets.append(tweet)","3a5f67c1":"train_tweets[:10]","8069561b":"# bag of words model\nvectorizer = TfidfVectorizer()\ntrain_tfidf_model = vectorizer.fit_transform(train_tweets)\ntest_tfidf_model = vectorizer.transform(test_tweets)","6fd2d868":"# let's look at the dataframe\ntrain_tfidf = pd.DataFrame(train_tfidf_model.toarray(), columns=vectorizer.get_feature_names())\ntrain_tfidf.head()","4ec8c2e4":"cls = [LogisticRegression(),\n       MultinomialNB(), \n       DecisionTreeClassifier(),\n       RandomForestClassifier(n_estimators=1000, random_state=42),\n       KNeighborsClassifier(n_neighbors = 5),\n       XGBClassifier(max_depth=6, n_estimators=1000, nthread= 3),\n       svm.SVC()]\n\ncls_name = []","80dda5e8":"lbl_actual = test_df.target\ni = 0\naccuracy = []\nfor cl in cls:\n    model = cl.fit(train_tfidf_model,train_df.target)\n    lbl_pred = model.predict(test_tfidf_model)\n    a = (100*accuracy_score(lbl_pred, lbl_actual))\n    a = round(a,2)\n    accuracy.append(a)\n    cls_name.append(cl.__class__.__name__)\n    print (\"{}  Accuracy Score : {}%\".format(cls_name[i],a))\n    print ( classification_report(lbl_pred, lbl_actual))\n    i +=1","3cc9b648":"plt.figure(figsize=(8,6))\nplt.bar(cls_name, accuracy)\nplt.xticks(rotation=70)\nfor index,data in enumerate(accuracy):\n    plt.text(x=index , y =data+1 , s=f\"{data}%\" , fontdict=dict(fontsize=10))\nplt.tight_layout()\nplt.show()","b321ed2b":"# Save to csv\n\nsvc_model = svm.SVC().fit(train_tfidf_model,train_df.target)\nsvc_lbl_pred = svc_model.predict(test_tfidf_model)","9640c78d":"\nsvc_lbl_pred = pd.DataFrame({'TweetText' : test_df.TweetText,\n                             'target' : rfc_lbl_pred})\nsvc_lbl_pred.head()","bdec3d54":"svc_lbl_pred.to_csv('sentiments.csv', index=False)","053ad00a":"### Make test-train split","a62ee3de":"What are the most common words in the entire dataset?","fe453f48":"# Clean Tweets","6fefd30d":"## TF-IDF","62fe4396":"we see data is quite disproprtinately balance, we can explore data balancing technique latter","759ae211":"What are the most common words in the dataset for Positive and Depressive tweets, respectively?","dddc162e":"# Output","734e03ae":"# EDA","273a3f42":"# Data Cleaning","e84f4e69":"# Conclusion\n\nwe have good accuracy with all the models, (we also need to validate the posibility if the model has been overfitter since our data is imbalanced, we need to explore databalancing techinique)\n \nFor now, we can go with MultinomialNB or SVC which is giving good accuracy of 95.85%","4cc2319f":"# Predicting Depression on Social Media \n## using Sentiment analysis\n\nIn this case study we'll analysis how NLP techinques can be used to predict mental heath (here taking example for depression ) using social media.\n\n\n\nediting in modifyin based on https:\/\/www.kaggle.com\/redaabdou\/depression-on-social-media","a30c61a9":"# Model Training","9bd1a798":"In this section we will visualize the tweets using wordclouds.\nA wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes."}}