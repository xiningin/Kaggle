{"cell_type":{"f3cda151":"code","74edd666":"code","93cf22e6":"code","80019c63":"code","12b149d7":"code","2e1a554b":"code","775c1ace":"code","c9a59a5c":"code","84d0245e":"code","1f8068c2":"code","ea108078":"code","cb0f8020":"markdown","00a5745e":"markdown","10f2adb0":"markdown","ee05ebef":"markdown","e16399ee":"markdown","6c293776":"markdown","8c6e2475":"markdown","0e4748d0":"markdown","047ad064":"markdown","98f14870":"markdown","ac43daae":"markdown"},"source":{"f3cda151":"!pip install scorecardpy","74edd666":"import numpy as np\nimport pandas as pd\nfrom scipy.special import logit\nimport lightgbm as lgb\nimport scorecardpy as sc","93cf22e6":"train = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/test.csv\")\ntrain = train.drop('ID_code', axis = 1)\ntrain.head()","80019c63":"test_id = test.ID_code\ntest = test.drop('ID_code', axis = 1)\ntest.head()","12b149d7":"# Whether there is missing value in the training set\nprint(f\"The number of missing values in the training set is: {np.sum(np.sum(pd.isnull(train)))}\")\n\n# Whether there is missing value in the test set\nprint(f\"The number of missing values in the test set is: {np.sum(np.sum(pd.isnull(test)))}\")","2e1a554b":"correlations = train.drop(\"target\", axis = 1).corr().abs().unstack().sort_values(kind = \"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","775c1ace":"correlations.tail(10)","c9a59a5c":"variables = train.drop(\"target\", axis = 1).columns.values.tolist()\ncorr_pre_res = np.zeros(len(variables))\ni = 0\nfor var in variables:\n    corr_pre_res[i] = np.corrcoef(train[var], train[\"target\"])[0, 1]\n    i += 1","84d0245e":"corr_pre_res = abs(pd.DataFrame(corr_pre_res))\ncorr_pre_res.columns = ['corr_pre_res']\ncorr_pre_res.sort_values(by = 'corr_pre_res')","1f8068c2":"bins = sc.woebin(train, y = 'target', \n                 min_perc_fine_bin = 0.05, # How many bins to cut initially into\n                 min_perc_coarse_bin = 0.05,  # Minimum percentage per final bin\n                 stop_limit = 0.1, # Minimum information value \n                 max_num_bin = 8, # Maximum number of bins\n                 method = 'tree')\n\nsc.woebin_plot(bins)","ea108078":"features = [x for x in train.columns if x.startswith(\"var\")]\n\nhist_df = pd.DataFrame()\nfor var in features:\n    var_stats = train[var].append(test[var]).value_counts()\n    hist_df[var] = pd.Series(test[var]).map(var_stats)\n    hist_df[var] = hist_df[var] > 1\n\nind = hist_df.sum(axis = 1) != 200\nvar_stats = {var: train[var].append(test[ind][var]).value_counts() for var in features}\n\npred = 0\nfor var in features:\n    model = lgb.LGBMClassifier(**{'learning_rate': 0.05, \n                                  'max_bin': 165, \n                                  'max_depth': 5, \n                                  'min_child_samples': 150,\n                                  'min_child_weight': 0.1, \n                                  'min_split_gain': 0.0018, \n                                  'n_estimators': 41,\n                                  'num_leaves': 6, \n                                  'reg_alpha': 2.0, \n                                  'reg_lambda': 2.54, \n                                  'objective': 'binary', \n                                  'n_jobs': -1})\n    model = model.fit(np.hstack([train[var].values.reshape(-1, 1),\n                      train[var].map(var_stats[var]).values.reshape(-1, 1)]), train[\"target\"].values)\n    pred += logit(model.predict_proba(np.hstack([test[var].values.reshape(-1, 1),\n                  test[var].map(var_stats[var]).values.reshape(-1, 1)]))[:, 1])\n    \npd.DataFrame({\"ID_code\": test_id, \"target\": pred}).to_csv(\"submission.csv\", index = False)","cb0f8020":"The correlations between target and variables are all small, so we should not drop some variables according to the correlations.","00a5745e":"It is shown that the correlations between different variables are pretty small. What about the correlations between target and predictors?","10f2adb0":"## Data Exploration","ee05ebef":"### Check if there is any missing value","e16399ee":"#### Since the variables are independent, use each of them combined with the frequency of each of its value as predictor to predict the probability of purchasing with LGBM, and then calculate the logit value of each probability and sum up all the logit values (by 200 predictors) to get the final result.","6c293776":"### Obtain the correlations between different variables\/response","8c6e2475":"### Load the data","0e4748d0":"## Preliminaries","047ad064":"## Feature Exploration (by WOE & IV)","98f14870":"### We can find that the original predictors are not useful enough for prediction (none of the IV is larger than 0.1). Consider constructing some new ones for better prediction.","ac43daae":"## Feature Engineering & Model Building (refer to Dott)\n[922 in 3 minutes](https:\/\/www.kaggle.com\/dott1718\/922-in-3-minutes\/comments)"}}