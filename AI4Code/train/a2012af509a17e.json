{"cell_type":{"50feaa30":"code","c64374b2":"code","164ff8e2":"code","7ebdd354":"code","a627498f":"code","794c0f4f":"code","f8efe316":"code","de649d2a":"code","e6719cf2":"code","a40d77ee":"code","e7b18014":"code","7024589d":"code","8e0e327c":"code","8e7fdd15":"code","f583d74c":"code","22c29fed":"code","bc32d3ae":"code","c20d9679":"code","e3af32b8":"code","7dbee9fa":"code","4c86d926":"code","3ea44ee7":"code","fb036140":"code","4fd7d6ec":"markdown","0216011e":"markdown","6d85e9c3":"markdown","2bd757d0":"markdown","5ca0b55e":"markdown","84c98144":"markdown","f062e5f9":"markdown","847a65cb":"markdown","b4b266b4":"markdown","c4d027c9":"markdown","703b3d3b":"markdown"},"source":{"50feaa30":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt","c64374b2":"# reading dataset\ndf = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv\")\ndf = df.fillna(df.mean())\n\n# This is for Features\nx = df[['condition', 'color_type', 'length(m)', 'height(cm)','X1', 'X2']]\n\n# We have Two Targer Value that is y1 and y2\ny1 = df['breed_category']\ny2 = df['pet_category']\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx['color_type'] = le.fit_transform(x['color_type'])\nx.head()\n\ndf_t = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv\")\ndf_t = df_t.fillna(df.mean())\nxt = df_t[['condition', 'color_type', 'length(m)', 'height(cm)','X1', 'X2']]\nxt['color_type'] = le.transform(xt['color_type'])\nyt1 = []\nyt2 = []\nxt.head()","164ff8e2":"from catboost import CatBoostRegressor\ncbc = CatBoostRegressor()\ncbc.fit(x,y1)\ny_pred1=cbc.predict(xt)\n\ncbc.fit(x,y2)\ny_pred2=cbc.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"CatBoost.csv\", index = False)","7ebdd354":"from sklearn.ensemble import AdaBoostRegressor\nrf_boost=AdaBoostRegressor()\n\nmodel = rf_boost.fit(x, y1)\ny_pred1 = model.predict(xt)\n\nmodel1 = rf_boost.fit(x, y2)\ny_pred2 = model1.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"AdaBoost.csv\", index = False)","a627498f":"from sklearn.ensemble import GradientBoostingRegressor\ngboost=GradientBoostingRegressor()\n\nmodel = gboost.fit(x, y1)\ny_pred1 = model.predict(xt)\n\nmodel2 = gboost.fit(x, y2)\ny_pred2 = model2.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"GradientBoost.csv\", index = False)","794c0f4f":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\n\nmodel = lr.fit(x, y1)\ny_pred = model.predict(xt)\n\nmodel1 = lr.fit(x, y2)\ny_pred1 = model1.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred, y_pred1))\no = np.array(o)\n\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"LinearRegressor.csv\", index = False)","f8efe316":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\n\nmodel = model.fit(x, y1)\ny_pred1 = model.predict(xt)\n\nmodel1 = model.fit(x, y2)\ny_pred2 = model1.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\n\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"XGBoost.csv\", index = False)","de649d2a":"from sklearn.svm import SVC\nsvm = SVC(probability=True)\n\nsvm1 = svm.fit(x, y1)\ny_pred1 = svm1.predict(xt)\n\nsvm2 = svm.fit(x, y2)\ny_pred2 = svm2.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\n\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"SVM.csv\", index = False)","e6719cf2":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\n\nnb1 = nb.fit(x, y1)\ny_pred1 = nb1.predict(xt)\n\nnb2 = nb.fit(x, y2)\ny_pred2 = nb2.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\n\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"NaiveBayes.csv\", index = False)","a40d77ee":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier()\n\nrf1 = rf.fit(x, y1)\ny_pred1 = rf1.predict(xt)\n\nrf2 = rf.fit(x, y2)\ny_pred2 = rf2.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\n\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"RandomForest.csv\", index = False)","e7b18014":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier()\n\nknn1 = knn.fit(x, y1)\ny_pred1 = knn1.predict(xt)\n\nknn2 = knn.fit(x, y2)\ny_pred2 = knn2.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\n\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"KNN.csv\", index = False)","7024589d":"from sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(random_state= 0, max_iter = 10000, learning_rate = 'optimal', penalty = 'l2')\nsgd1 = sgd_reg.fit(x, y1)\ny_pred1 = sgd1.predict(xt) \n\nsgd_reg = SGDRegressor(random_state= 0, max_iter = 25000, learning_rate = 'optimal', penalty = 'l2')\nsgd2 = sgd_reg.fit(x, y2)\ny_pred2 = sgd2.predict(xt) \n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\n\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"SGDRegressor.csv\", index = False)","8e0e327c":"from sklearn.svm import LinearSVR\nfrom sklearn.datasets import make_regression\nlin_SVR = LinearSVR(random_state=0, tol=0.05, C=100, epsilon=0.1)\nSVR1 = lin_SVR.fit(x, y1)\ny_pred1 = SVR1.predict(xt)\n\nSVR2 = lin_SVR.fit(x, y2)\ny_pred2 = SVR2.predict(xt)\n\no = list(zip(df_t[\"pet_id\"], y_pred1, y_pred2))\no = np.array(o)\n\ndf = pd.DataFrame(o, columns = [\"pet_id\", 'breed_category', 'pet_category'])\ndf.to_csv(\"linear_SVR.csv\", index = False)","8e7fdd15":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","f583d74c":"df = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv\")\ndf = df.fillna(df.mean())\n\n# This is for Features\nx = df[['condition', 'color_type', 'length(m)', 'height(cm)','X1', 'X2']]\n\n# We have Two Targer Value that is y1 and y2\ny1 = df['breed_category']\ny2 = df['pet_category']","22c29fed":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx['color_type'] = le.fit_transform(x['color_type'])\nx.head()","bc32d3ae":"df_t = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv\")\ndf_t = df_t.fillna(df.mean())\nxt = df_t[['condition', 'color_type', 'length(m)', 'height(cm)','X1', 'X2']]\nxt['color_type'] = le.transform(xt['color_type'])\nxt.head()","c20d9679":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\n\n\nresults=[]\nnames=[]\nmodels=[]\nlr=LogisticRegression()\nknn=KNeighborsClassifier()\nsvm=SVC(probability=True)\nrf=RandomForestClassifier()\nnb=GaussianNB()\nrf_boost=AdaBoostClassifier()\ngboost=GradientBoostingClassifier()\ncbc=CatBoostClassifier()\n\nmodels.append(('cbc',cbc))\nmodels.append(('lr',lr))\nmodels.append(('knn',knn))\nmodels.append(('svm',svm))\nmodels.append(('rf',rf))\nmodels.append(('nb',nb))\nmodels.append(('rf_boost',rf_boost))\nmodels.append(('gboost',gboost))\n\nfor name,model in models:\n    kfold=KFold(shuffle=True,n_splits=10,random_state=1)\n    cv_results=cross_val_score(model,x,y1,cv=kfold) \n    results.append(cv_results)\n    names.append(name)\n    print(\"%s:%f (%f)\" % (name,np.mean(cv_results),np.var(cv_results,ddof=1)))  ##we have 10 recall scores. we are taking its average.\n    \n#boxplot algorithm comparison\nfig=plt.figure()\nfig.suptitle('algorithm comparison')\nax=fig.add_subplot(111)\nplt.boxplot(results)\nplt.show()","e3af32b8":"y_pre = []\ny_pre1 = []\nfor i in range(len(y_pred)):\n    y_pre.append(y_pred[i][0])\n    y_pre1.append(y_pred1[i][0])","7dbee9fa":"import numpy as np\nimport pandas as pd\nimport lightgbm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer","4c86d926":"df = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/train.csv\")\ndf = df.fillna(df.mean())\n\n# This is for Features\nx = df[['condition', 'color_type', 'length(m)', 'height(cm)','X1', 'X2']]\n\n# We have Two Targer Value that is y1 and y2\ny1 = df['breed_category']\ny2 = df['pet_category']\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nx['color_type'] = le.fit_transform(x['color_type'])\nx.head()\n\n\ndf_t = pd.read_csv(\"\/kaggle\/input\/hackerearth-ml-challenge-pet-adoption\/test.csv\")\ndf_t = df_t.fillna(df.mean())\nxt = df_t[['condition', 'color_type', 'length(m)', 'height(cm)','X1', 'X2']]\nxt['color_type'] = le.transform(xt['color_type'])\nxt.head()","3ea44ee7":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom matplotlib import pyplot\n\nmodel = LGBMClassifier(boosting_type='gbdt', \n                       num_leaves=31, \n                       max_depth=- 1, \n                       learning_rate=0.05, \n                       n_estimators=1000, \n                       subsample_for_bin=15, \n                       min_split_gain=0.0, \n                       min_child_weight=0.001, \n                       min_child_samples=20, \n                       subsample=1.0, \n                       random_state=5)\ncv = RepeatedStratifiedKFold(n_splits=100, n_repeats=3, random_state=1)\nn_scores = cross_val_score(model, x, y1, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\nprint('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))","fb036140":"# fit the model on the whole dataset\nmodel = LGBMClassifier()\nmodel.fit(X, y)","4fd7d6ec":"# Thank You\n### if you like this kerner just hit the like.","0216011e":"## KNN : 79.90% Acc","6d85e9c3":"## CatBoosting : 88.62%","2bd757d0":"## SVM : 71.91 % Acc","5ca0b55e":"## Gradient Boosting : 87.35% Acc","84c98144":"## AdaBoosting : 76.179% Acc","f062e5f9":"## Day3","847a65cb":"## Random Forest : 87.72% Acc","b4b266b4":"## Linear Regression : 55.80% Acc","c4d027c9":"## Naive Bayes: 76.77% Acc","703b3d3b":"## XGBoost : 89.27% Acc"}}