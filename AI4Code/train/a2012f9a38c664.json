{"cell_type":{"127fafb1":"code","ad1275b3":"code","af7aeafa":"code","21c17564":"code","398a07ab":"code","12b5374c":"code","901ede6b":"code","bb5fba0b":"code","48854621":"code","b47f3363":"code","eeff8056":"code","db40dad0":"code","c97e7dd9":"code","b133d72c":"code","18b39d66":"code","1afb636d":"code","3a875dd1":"code","36917424":"code","02c52cb2":"markdown","47740e61":"markdown","897e0149":"markdown","de628d01":"markdown","e0f7a930":"markdown","6079f21f":"markdown","2091e13e":"markdown","943f13c6":"markdown","cdd9ae69":"markdown","546a8dc2":"markdown","222bd574":"markdown","a4e185eb":"markdown","e45f8253":"markdown","5fc26c2a":"markdown"},"source":{"127fafb1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport keras \nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport keras.backend as K\nimport ipywidgets as widgets\nfrom IPython.display import display\n","ad1275b3":"data = pd.read_csv('..\/input\/age-gender-and-ethnicity-face-data-csv\/age_gender.csv')\ndata.head()","af7aeafa":"data = data.pixels.apply(lambda x: np.array(x.split(\" \"),dtype = float))\narr = np.stack(data)\narr = arr \/ 255.0\narr = arr.astype('float32')\narr = arr.reshape(arr.shape[0],48,48,1)\n","21c17564":"plt.figure(figsize = (10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.imshow(arr[i],cmap = 'gray')\n    plt.axis('off')","398a07ab":"batch_size = 64\ndataset = tf.data.Dataset.from_tensor_slices(arr).batch(batch_size).shuffle(132)     \n","12b5374c":"class Sampling(keras.layers.Layer):\n    def call(self, inputs):\n        z_mean , z_log_var = inputs;\n        batch = tf.shape(z_mean)[0]\n        dimension = tf.shape(z_mean)[1]\n        epsilon = K.random_normal(shape = (batch, dimension))\n        return z_mean + tf.exp(z_log_var * 0.5) * epsilon","901ede6b":"latent_dim = 3\nencoder_inputs = layers.Input(shape = (48,48,1))\nx = layers.Conv2D(filters = 32, kernel_size = (2,2), activation = 'relu', padding = 'same')(encoder_inputs)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.MaxPool2D(pool_size = (2,2))(x)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.Conv2D(filters = 128, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.MaxPool2D(pool_size = (2,2))(x)\nx = layers.Conv2D(filters = 128, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.Conv2D(filters = 256, kernel_size = (2,2), activation = 'relu')(x)\nx = layers.MaxPool2D(pool_size = (2,2))(x)\n\nx = layers.Flatten()(x)\nmean = layers.Dense(latent_dim)(x)\nlog_var = layers.Dense(latent_dim)(x)\nz = Sampling()([mean, log_var])\n\nencoder = tf.keras.Model(encoder_inputs, [mean,log_var,z])\n\nencoder.summary()\n","bb5fba0b":"decoder_input = layers.Input(shape = (latent_dim,))\n\nx = layers.Dense(6*6*32, activation = 'relu')(decoder_input)\n\nx = layers.Reshape((6,6,32))(x)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), padding = 'same',activation = 'relu')(x)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), padding = 'same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.UpSampling2D()(x)\nx = layers.Conv2D(filters = 64, kernel_size = (2,2), padding = 'same',activation = 'relu')(x)\nx = layers.Conv2D(filters = 128, kernel_size = (2,2), padding = 'same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.UpSampling2D()(x)\nx = layers.Conv2D(filters = 128, kernel_size = (2,2), padding = 'same',activation = 'relu')(x)\nx = layers.Conv2D(filters = 256, kernel_size = (2,2), padding = 'same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.Conv2D(filters = 256, kernel_size = (2,2), padding = 'same',activation = 'relu')(x)\nx = layers.Conv2D(filters =128, kernel_size = (2,2), padding = 'same')(x)\nx = layers.BatchNormalization()(x)\nx = layers.LeakyReLU()(x)\nx = layers.UpSampling2D()(x)\nx = layers.Conv2D(filters = 1, kernel_size = (2,2), padding = 'same')(x)\n\ndecoder = tf.keras.Model(decoder_input, x)\ndecoder.summary()\n ","48854621":"inp = encoder.input\nout = encoder.output\ndecoder_output = decoder(out[2])\nvae = tf.keras.Model(inp, decoder_output)\nvae.summary()","b47f3363":"def kl_loss(z_log_var,z_mean):\n    kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n    return kl_loss * 0.012\n\n\nloss = tf.keras.losses.BinaryCrossentropy()\ndef reconstruction_loss(data,reconstructed):\n    return loss(data,reconstructed)\noptimizer = tf.keras.optimizers.Adam()    ","eeff8056":"def train_steps(data):\n    with tf.GradientTape() as vae_tape:\n        z_mean,z_log_var,z = encoder(data)\n        reconstructed_image = decoder(z)\n        kl_ = kl_loss(z_log_var,z_mean)\n        reconstruction_ = reconstruction_loss(data, reconstructed_image)\n        total_loss = kl_ + reconstruction_\n\n    gradient2 = vae_tape.gradient(total_loss, vae.trainable_variables)\n    optimizer.apply_gradients(zip(gradient2, vae.trainable_variables))","db40dad0":"noise = tf.keras.backend.random_normal(shape = (9,latent_dim))\ndef visualize(epoch):\n        prediction = decoder(noise)\n        plt.figure(figsize = (5,5))\n        for i in range(9):\n            if(i == 2):\n                plt.title(\"Epoch: {}\".format(epoch))\n            plt.subplot(3,3,i+1)\n            plt.imshow(prediction[i],cmap = 'gray')\n            plt.axis('off')","c97e7dd9":"import time\nimport numpy as np\ndef train(dataset, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        for data in dataset:\n            train_steps(data)\n        print(\"Epoch: {} Time: {}\".format(epoch+1,np.round(time.time()-start),3))\n        if epoch % 3 == 0:\n            visualize(epoch+1)","b133d72c":"train(dataset,15)","18b39d66":"def generate_image(latent1, latent2, latent3):\n    latent_values = np.array([[latent1, latent2, latent3]])\n    reconstruction = np.array(decoder(latent_values))\n    reconstruction = reconstruction.reshape(48,48,1)\n    plt.figure(figsize = (4,4))\n    plt.imshow(reconstruction,cmap = 'gray')\n    plt.axis('off')\n    plt.show()","1afb636d":"import numpy as np\na,b,z = encoder(np.array(arr[:5000]).reshape(np.array(arr[:5000]).shape[0],48,48,1))\nlatent1_min = np.min(z[:,0])-1\nlatent1_max = np.max(z[:,0])+1\n\nlatent2_min = np.min(z[:,1])-1\nlatent2_max = np.max(z[:,1])+1\n\nlatent3_min =np.min(z[:,2])-1\nlatent3_max = np.max(z[:,2])+1\n","3a875dd1":"\nface_image_generator = widgets.interact(\n    generate_image,\n    latent1=(latent1_min, latent1_max),\n    latent2=(latent2_min, latent2_max),\n    latent3=(latent3_min, latent3_max),\n)\n\ndisplay(generate_image)\n##  copy and run this notebook to use this widget","36917424":"# visualize our input image through VAE\ni = np.random.randint(1,2323)\nout = vae.predict(arr[i].reshape(1,48,48,1))\nplt.subplot(1,2,1)\nplt.title(\"Original Image\")\nplt.imshow(arr[i], cmap = 'gray')\nplt.subplot(1,2,2)\nplt.title(\"Reconstructed Image\")\nplt.imshow(out.reshape(48,48,1), cmap = 'gray')","02c52cb2":"## Sampling ","47740e61":"## Load Data","897e0149":"## To use widgets for interactive visualization\nIn order to use this widgets you have to copy and run this model","de628d01":"## Thank You \n## Any Suggestion to improve this notebook is highly appreciated\n\nref: <a href= 'https:\/\/www.kaggle.com\/gcdatkin\/an-introduction-to-variational-autoencoders'> here <\/a>","e0f7a930":"## Defining Decoder Model","6079f21f":"## Defining Batch Size and shuffling data","2091e13e":"## Prepare data","943f13c6":"## VAE Model","cdd9ae69":"## Defining Loss functions","546a8dc2":"## Training","222bd574":"## Import Necessary Libraries","a4e185eb":"## Defining our encoder model","e45f8253":"## Train steps","5fc26c2a":"## Visualize our data\n"}}