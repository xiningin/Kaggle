{"cell_type":{"bb792b24":"code","000d8bc8":"code","96edee02":"code","8be0a859":"code","2256fd94":"code","fca07b45":"code","342f12fb":"code","cf4fbfc1":"code","f8e87b91":"code","58f1c086":"code","bc6b4410":"code","4ec0e4d8":"code","a00dec82":"code","42b2277b":"code","a398fb34":"code","c12e8c4d":"code","ed8be037":"code","32fdaaf6":"code","2e639861":"code","6acc0fc4":"code","6b07ebcf":"code","7bec9ede":"code","47252d90":"code","7d370363":"code","ec6bf3b9":"code","bc5c01ee":"code","0c629699":"code","f94645b0":"code","133cb496":"code","2ac7d323":"code","60759381":"code","e1ab123a":"code","4453d526":"code","12d995d8":"markdown","4c894608":"markdown","1900eb70":"markdown","43ab6d88":"markdown","5fb56183":"markdown","1cce05a6":"markdown","56c7ce56":"markdown","238a1d00":"markdown","01df4fa5":"markdown","d0b9077a":"markdown","8d2c090e":"markdown"},"source":{"bb792b24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","000d8bc8":"data = '\/kaggle\/input\/titanic-dataset-from-kaggle\/train.csv'\n\ndata = pd.read_csv(data) \ndata.head() ","96edee02":"df = data.drop(['Name', 'Ticket','Cabin'],axis=1) \n","8be0a859":"#No. of columns and rows\ndf.shape","2256fd94":"#Reduce null values\nprint(df.isnull().sum()) \ndf.dropna(inplace = True) ","fca07b45":"print(df.columns)","342f12fb":"df.head()\nsex = pd.get_dummies(df.Sex) \n\nsex.drop('female', inplace = True) ","cf4fbfc1":"df['Sex'] = sex\ndf.head()","f8e87b91":"embarked=pd.get_dummies(df.Embarked)\nembarked.head() \n\nembarked.drop('S',axis=1,inplace=True) ","58f1c086":"#Adding embarked dataFrame with df\nembarked.head()\n\ndf = pd.concat([df, embarked],axis=1) ","bc6b4410":"df.head()\ndf.drop('Embarked', axis=1, inplace=True) ","4ec0e4d8":"print(df.head()) \n\nX = df.drop('Survived', axis=1) \ny = df.Survived\n\n","a00dec82":"#feature data.\nX.head() \n\n","42b2277b":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nlg_model = LogisticRegression(max_iter=40) \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1) \n\nlg_model.fit(X_train,y_train) \n\nprediction = lg_model.predict(X_test) ","a398fb34":"#evaluating the accuracy of the model\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, prediction) ","c12e8c4d":"prediction\n","ed8be037":"y_test.head(10) \n","32fdaaf6":"from sklearn.model_selection import cross_val_score\navg_score = cross_val_score(LogisticRegression(solver='liblinear', \n                                   multi_class='auto', \n                                   max_iter=45), X_train, y_train, cv=5) \navg_score.mean() \n","2e639861":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVC\ndef get_score(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train) \n    return model.score(X_test, y_test) \n\n","6acc0fc4":"score_lg = get_score(LogisticRegression(solver='liblinear',max_iter=45, multi_class='auto'), \n    X_train, X_test, y_train, \n    y_test)\nscore_rg = get_score(RandomForestRegressor(), \n            X_train,X_test,y_train,y_test)         \nscore_svm = get_score(SVC(kernel='rbf'), X_train, X_test, \n                      y_train, y_test) \n","6b07ebcf":"print(score_lg) \nprint(score_rg) \nprint(score_svm) \n","7bec9ede":"lg_score = cross_val_score(LogisticRegression( solver = 'liblinear',max_iter=45, multi_class='auto'),X_train, y_train, cv=5) \nRfr_score = cross_val_score(RandomForestRegressor(max_leaf_nodes=100),X_train, y_train, cv=5) \nsvm_score = cross_val_score(SVC(),X_train, y_train, cv=5) ","47252d90":"lg_score\n","7d370363":"#creating the dataFrame for the result of cross_value score\nmy_data = pd.DataFrame({'Lg_score':lg_score,'Rfr_score': Rfr_score,'SVM_score':svm_score}) ","ec6bf3b9":"my_data\n","bc5c01ee":"my_data.to_csv('submission.csv',index=False)","0c629699":"from sklearn.model_selection import cross_val_score\ndef score_value(n_estimators):\n    \n    result = RandomForestRegressor(n_estimators, random_state =0) \n    score = -1* cross_val_score(result, X_train, y_train, cv=5, scoring='neg_mean_absolute_error') \n    return score.mean() ","f94645b0":"print(score_value(50)) \n#using dictionary to fit the value\nresults={}\nfor i in range(50, 401,50):\n    results[i]=score_value(i) \n   # print(results) \n    \nprint(results) ","133cb496":"for key, value in results.items():\n    print(key, value) ","2ac7d323":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor(n_estimators=1000,learning_rate= 0.001) \nmy_model.fit(X_train, y_train) ","60759381":"from sklearn.metrics import mean_absolute_error\n\npreds = my_model.predict(X_test) \nMAE = mean_absolute_error(y_test, preds) \n","e1ab123a":"MAE\n","4453d526":"score = my_model.score(X_test, y_test) \nscore\n","12d995d8":"# Using different methods to increase the accuracy of the model","4c894608":"### In this case getting proper results using GBosst regressor seems to be difficult than approaches used above","1900eb70":"# Check how much the predicted data and actual data are matched","43ab6d88":"# Drop the columns that are unnecessary for Machine learning process","5fb56183":"# Machine Learning process\n\n1. split data into train ","1cce05a6":"# Since Sexal and embarked columns is filled with object data type, so we need to convert it into numerical form","56c7ce56":"# Define function to calculate cross val score and try to evaluate the result using hyper parameter tuning concept\n","238a1d00":"# Here, we are calculating the mean absolute error of the model rather than score","01df4fa5":"# XGboost - Xtreme Gradient Boosting, For optimizing MAE(Mean Absolute Error) at minimum Point\n\n","d0b9077a":"# define the function for avoiding the repitation while coding","8d2c090e":"# Cross Validation score"}}