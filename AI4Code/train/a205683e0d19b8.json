{"cell_type":{"555e5b17":"code","ea998e77":"code","a953f33b":"code","3319cfce":"code","3d0bf6ce":"code","c1f6865b":"code","8726d38f":"code","655ff3a9":"code","93bfae28":"code","c04387d8":"code","205132f5":"code","8d3879c4":"code","2dfbef4f":"code","0bb9b13b":"markdown","de3d02cf":"markdown","70d49962":"markdown","33642b0c":"markdown","13f3fd31":"markdown","e686f08e":"markdown","abe4dd5e":"markdown","f5eac138":"markdown","2d862fde":"markdown","5e9436c8":"markdown","e228c24b":"markdown","5bc7b5c7":"markdown"},"source":{"555e5b17":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\n\nfrom pprint import pprint\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score","ea998e77":"data = pd.read_csv('..\/input\/data.csv')\ndata.head()","a953f33b":"data.isnull().sum()","3319cfce":"df = data.drop(['Unnamed: 32','id'], axis=1)\n\nprint(\"Final Columns in Dataset\")\nprint('='*50)\nprint(df.isnull().sum())\nprint('='*50)","3d0bf6ce":"X = df.iloc[:,1:]\ny = np.where(df['diagnosis']=='M', 1,0).astype(int)\n\nX_train, X_test, y_Train, y_Test = train_test_split(X, y, test_size =0.2, random_state =5)","c1f6865b":"model = RandomForestClassifier()\n\nprint(\"Default Parameters \")\nprint('='*50)\n\npprint(model.get_params())\n\nprint('='*50)","8726d38f":"bootstrap_v = [True, False]\nn_estimators_v = list(range(100,2000,200))\ncriterion = ['gini', 'entropy']\nmin_sample_leaf_v = list(range(1,5,2))\nmax_features_v = ['sqrt', 'log2']\n","655ff3a9":"grid_params  = {\n    'bootstrap' : bootstrap_v,\n    'n_estimators' : n_estimators_v,\n    'criterion' : criterion,\n    'min_samples_leaf' : min_sample_leaf_v,\n    'max_features' : max_features_v\n}\n\nprint(\"Tuning Parameters\")\nprint('='*50)\n\npprint(grid_params)\nprint('='*50)","93bfae28":"grid_search = GridSearchCV(estimator=model, param_grid=grid_params, cv=3, verbose=1)","c04387d8":"grid_search.fit(X_train, y_Train)\n\nprint('Best Parameters for our classsifier')\nprint('='*50)\nprint(grid_search.best_params_)\nprint('='*50)","205132f5":"def evaluate(model, X, y):\n    \n    pprint(model.get_params())\n    print('=='*50)\n    predictions = model.predict(X)\n    report = classification_report(y, predictions)\n    \n    score = accuracy_score(y_true= y, y_pred= predictions)\n    \n    print(report)\n    print('=='*50)\n    print(\"{} {:0.2f}%\".format(\"Accuracy Score :: \", score*100))\n    \n    ","8d3879c4":"evaluate(grid_search.best_estimator_, X_test, y_Test)","2dfbef4f":"model.fit(X_train, y_Train)\nevaluate(model, X_test, y_Test)","0bb9b13b":"### Evaluation of our best Estimator Selected from GridSearchCV","de3d02cf":"## Applying gridSearch on model and fitting it\n\n >We passed our classifier as  estimator.\n\n* estimator = model to apply gridSearch\n* param_grid = the parameter set for tuning the classifier\n* cv = the cross-validation factor.\n* verbose = the intensity of background work that gets printed while fitting","70d49962":"### Classifier RandomForest\n\nI will be using RandomForestClassifier for demo. You can choose any classsifier on which parameter tuning is required.\n\nThe default parameters are also displayed","33642b0c":"### Spliting the data for train and test\n\nWe changed the target column that is diagnosis to binary 0 and 1.\n\n* X -> attributes or features that will help predict out target column diagnosis\n* y -> Target column","13f3fd31":"## Conclusion\n\n>Our **Base Model accuracy was 97.37%** but after *hyperparameter tuning* our accuracy increased to **98.25% on Tuned Model**.\n\nThis is a significant increase in accuracy.\n\n*Score when i ran the kernel, but the accuracy will improve on paramter tuning in most cases*","e686f08e":"### Checking for columns for Nan \n\nWe will check and remove any column that is not required.","abe4dd5e":"### Dropping Columns\n\nColumn : id, Unnamed: 32 will be dropped\n\nid is not required for classification and Unnamed: 32 has Nan","f5eac138":"# **GridSearchCV on RandomForestClassifier**\n\n### In this kernel we will be appplying GridSearch for Hyperparameter Tuning for a classifier\n\n>*I will be using RandomForestClassifier but any Classsifier can be used*","2d862fde":"### Parameters of classifier and their possible values for tuning\n\n*These are based on RandomForestClassifier.*\n>I will be applying GridSearch for tuning bootstrap, n_estimators, criterion, min_samples_leaf, max_features.\n\nYou should use your paramteres as per classifier.","5e9436c8":">Building the set of parameters to pass as variable to gridsearch","e228c24b":"## Function that will evaluate the working of our Classifier on test set\n\n>It prints the parameters of classsifier, Classification report and Accuracy Score","5bc7b5c7":"### Evaluation of our base Model"}}