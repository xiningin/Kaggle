{"cell_type":{"85489865":"code","45358af0":"code","de8eac0d":"code","1d5037da":"code","12635271":"code","e5726e41":"code","794caf8b":"code","51f98765":"code","64653797":"code","fc9ebff1":"code","e956fcfc":"code","12c71424":"code","64c76e0e":"code","434091cd":"code","47e0131b":"code","16ba5235":"code","1a56f3d8":"code","77abac6c":"code","48adb4b6":"code","0afadee6":"code","7e82be3b":"code","fa82d8e2":"code","b1e3b6d2":"code","9689a4b0":"code","ec477374":"code","7226712b":"code","74123703":"code","54d25413":"code","5fff586f":"code","bdd81617":"code","5cd0501c":"code","20c7a80f":"code","e43c767e":"code","842f2742":"code","42995dfb":"code","ad78a216":"code","eb0126bc":"code","824ee1bd":"code","64986a0b":"code","8091dd6f":"code","12a39013":"code","3ad0ae05":"code","c90b0abc":"code","8001913f":"code","21dba943":"code","25fd9d9a":"code","b49bc5bb":"code","e497c42d":"code","31f43c8b":"code","45bf8361":"code","8bcd9ec9":"code","44a8a577":"code","ca13de72":"code","1ec0f0f1":"code","3953cf9a":"code","07a5dce8":"code","24431508":"code","d90b4980":"code","eacd0efc":"code","bdd8aa1f":"code","cfb3390f":"code","4c088157":"code","6405fc6b":"code","bc24f34b":"code","4a930e2e":"code","9b883de5":"code","a224dede":"code","8f699bc5":"code","7663162a":"code","eb3045df":"code","fe451f8a":"code","7091d69a":"code","03ebcbbf":"code","c7fd2131":"code","f737b257":"code","3b074303":"code","5f593d24":"code","27980f28":"code","7e18c150":"code","696500d7":"code","a3f86afa":"code","a93268e0":"code","4837ec0f":"code","f62aeb3a":"code","0f289725":"code","74c42bc2":"code","962a3a63":"code","8c8a35e6":"code","1a410cc6":"code","112ff481":"code","d318889e":"code","8d9862a6":"code","ec0be21e":"code","f29b3b95":"code","b9eecf5d":"code","d10db6d4":"code","e94ed37c":"code","352dc2c0":"code","196d8dd6":"code","f37574dd":"code","77db1a46":"code","1e64da57":"code","8b273621":"code","c57edf77":"code","8006ee5c":"code","f812250a":"code","82acb9a0":"code","b97c6d56":"code","741d95b4":"code","b8ac3dca":"code","2932233f":"markdown","5715fc01":"markdown","014aaaab":"markdown","94c99ea3":"markdown","91893e11":"markdown","932187a1":"markdown"},"source":{"85489865":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45358af0":"# Importing the data mainipulation libraries in python\nimport pandas as pd\nimport numpy as np\n# Importing the data vaiualization libraries in python\nimport matplotlib.pyplot as plt\n# This method helps to place all the visualizations within the Jupyter Notebook\n%matplotlib inline \nimport seaborn as sns\n#!pip install plotly\nimport plotly \nimport plotly.express as px\nimport time\n\nfrom sklearn.model_selection import train_test_split #train and test data split\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Catboost and One Hot Encoding libraries\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\nimport catboost\nfrom catboost import CatBoostClassifier, Pool, cv\nimport time, datetime\n# Used to ignore warnings \nimport warnings\nwarnings.filterwarnings('ignore')","de8eac0d":"# Importing training dataset \ndata = pd.read_csv('..\/input\/titanic\/train.csv')","1d5037da":"# Viewing the training dataset \ndata.head()","12635271":"# Checking Missing data in training dataset \n\ndata.isnull().sum()","e5726e41":"# Viewing number of Rows & Columns in the training Dataset \ndata.shape","794caf8b":"# Describe returns the summary statistics of the features in the dataset \ndata.describe().transpose()","51f98765":"# Gender wise Survival Count \ndata.groupby(['Sex','Survived'])['Survived'].count()","64653797":"# List of numbers of columns in a dataset\ndata.columns ","fc9ebff1":"# Class wsie Value Counts\ndata.Pclass.value_counts()","e956fcfc":"# Total Number of Passengers Survived by Gender\nsurvival_ratio = sns.barplot(x = 'Sex', y = 'Survived', hue = \"Sex\",data = data)\nplt.xlabel('Sex')\nplt.ylabel('Number of Passengers Survived')\nplt.title('Total Number of Passengers Survived by Gender')\nplt.show()","12c71424":"# Total number of passengers survived -\nsns.countplot(data['Survived'])","64c76e0e":"# Pclass - Ordinal variable\n\npd.crosstab(data.Pclass,data.Survived,margins=True)","434091cd":"# Target Variable \ndata['Survived'].value_counts()","47e0131b":"data['Age'].head(20)","16ba5235":"# Data Exploration using AutoML using pandas_profiling library -\n\nimport pandas_profiling","1a56f3d8":"#descriptive statistics\npandas_profiling.ProfileReport(data)","77abac6c":"data['Title']=0\nfor i in data:\n    data['Title']=data.Name.str.extract('([A-Za-z]+)\\.')","48adb4b6":"data['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)","0afadee6":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Title'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","7e82be3b":"data['Family']=0\ndata['Family']=data['Parch']+data['SibSp'] # Family \ndata['Single']=0\ndata.loc[data.Family==0,'Single']=1 # Single","fa82d8e2":"data.columns","b1e3b6d2":"data.head()","9689a4b0":"data.loc[(data.Age.isnull())&(data.Title=='Mr'),'Age']=34\ndata.loc[(data.Age.isnull())&(data.Title=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Title=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Title=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Title=='Other'),'Age']=46","ec477374":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data)\nplt.show()","7226712b":"# Imputation of Missing Data in Training Dataset \n\ndata['Embarked'].fillna('S',inplace=True)","74123703":"# Correlation Matrix - \n\nsns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) \nplt.show()","54d25413":"# Diving Age Feature into various age_group\n\ndata['Age_group']=0\ndata.loc[data['Age']<=16,'Age_group']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_group']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_group']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_group']=3\ndata.loc[data['Age']>64,'Age_group']=4\ndata.head(10)","5fff586f":"data['Age_group'].value_counts()","bdd81617":"data.isnull().sum()","5cd0501c":"# Dropping the features not be included in the current dataset Modeling \n\ndata.drop(['Age','Ticket','Fare','Cabin'],axis=1,inplace=True)","20c7a80f":"data.columns","e43c767e":"data.drop(['Name'],axis=1,inplace=True)","842f2742":"# Set Passenger_Id as index instead of dropping it.\n\ndata.set_index('PassengerId',inplace=True)","42995dfb":"data.head()","ad78a216":"# Feature One Hot Encoding of Train Dataset","eb0126bc":"# One hot encode the categorical columns\ndf_embarked_encoding = pd.get_dummies(data['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_encoding = pd.get_dummies(data['Sex'], \n                                prefix='sex')\n\ndf_plcass_encoding = pd.get_dummies(data['Pclass'], \n                                   prefix='pclass')","824ee1bd":"# Combine the one hot encoded columns with creating new dataset as data_encoding.\ndata_encoding = pd.concat([data, \n                        df_embarked_encoding, \n                        df_sex_encoding, \n                        df_plcass_encoding], axis=1)\n\n# Drop the original categorical columns since we had applied one hot encoding technique.\ndata_encoding = data_encoding.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","64986a0b":"data_encoding.head(5)","8091dd6f":"train_data = data_encoding","12a39013":"train_data.head(5)","3ad0ae05":"# Pop out the target variable Survived from train_data into y dataset. \ny = train_data.pop('Survived')\n\n# Splitting the train_data dataset into train-test split\nX_train, X_test, y_train, y_test = train_test_split(train_data, y, test_size=0.2, random_state=42)","c90b0abc":"X_train.head(5)","8001913f":"y_train.head()","21dba943":"X_train.shape","25fd9d9a":"y_train.shape","b49bc5bb":"X_test.shape","e497c42d":"y_test.shape","31f43c8b":"import lightgbm as lgbm","45bf8361":"# Light GBM on training dataset\ntrain_data = lgbm.Dataset(data=X_train, label=y_train,free_raw_data=False)","8bcd9ec9":"# Create an LGBM dataset from the test\ntest_data = lgbm.Dataset(data=X_test, label=y_test, free_raw_data=False)","44a8a577":"X_train.shape","ca13de72":"lgbm_params = {\n    'boosting': 'dart',          # dart (drop out trees) often performs better\n    'application': 'binary',     #\u00a0Binary classification\n    'learning_rate': 0.05,       # Learning rate, controls size of a gradient descent step\n    'min_data_in_leaf': 20,      # Data set is quite small so reduce this a bit\n    'feature_fraction': 0.7,     # Proportion of features in each boost, controls overfitting\n    'num_leaves': 41,            # Controls size of tree since LGBM uses leaf wise splits\n    'metric': 'binary_logloss',  # Area under ROC curve as the evaulation metric\n    'drop_rate': 0.15\n              }","1ec0f0f1":"start_time = time.time()\n\nevaluation_results = {}\nligbm = lgbm.train(train_set=train_data,\n                 params=lgbm_params,\n                 valid_sets=[train_data, test_data], \n                 valid_names=['Train', 'Test'],\n                 evals_result=evaluation_results,\n                 num_boost_round=500,\n                 early_stopping_rounds=100,\n                 verbose_eval=20,\n                 \n                )\noptimum_boost_rounds = ligbm.best_iteration\nlgm_time = (time.time() - start_time)","3953cf9a":"X_train.shape","07a5dce8":"X_train.columns","24431508":"X_test.shape","d90b4980":"X_test.columns","eacd0efc":"preds = np.round(ligbm.predict(X_test))\nprint('Accuracy score = \\t {}'.format(accuracy_score(y_test, preds)))\nprint('Precision score = \\t {}'.format(precision_score(y_test, preds)))\nprint('Recall score =   \\t {}'.format(recall_score(y_test, preds)))\nprint('F1 score =      \\t {}'.format(f1_score(y_test, preds)))","bdd8aa1f":"#importing all the required ML packages\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure","cfb3390f":"rf=RandomForestClassifier(n_estimators=100)\nrf.fit(X_train,y_train)\nrf_pred=rf.predict(X_test)\nprint('The accuracy of Random Forests is',metrics.accuracy_score(rf_pred,y_test))","4c088157":"preds = np.round(rf.predict(X_test))\nprint('Accuracy score = \\t {}'.format(accuracy_score(y_test, preds)))\nprint('Precision score = \\t {}'.format(precision_score(y_test, preds)))\nprint('Recall score =   \\t {}'.format(recall_score(y_test, preds)))\nprint('F1 score =      \\t {}'.format(f1_score(y_test, preds)))","6405fc6b":"# Hyper Parameter Tunning for training set - Train Dataset - train-test split - Test Part\nn_estimators=range(100,1000,100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X_test,y_test)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","bc24f34b":"preds = np.round(gd.predict(X_test))\nprint('Accuracy score = \\t {}'.format(accuracy_score(y_test, preds)))\nprint('Precision score = \\t {}'.format(precision_score(y_test, preds)))\nprint('Recall score =   \\t {}'.format(recall_score(y_test, preds)))\nprint('F1 score =      \\t {}'.format(f1_score(y_test, preds)))","4a930e2e":"import shap  # package used to calculate Shap values\n\nshap.initjs()\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rf)\n# calculate shap values\nshap_values = explainer.shap_values(X_test)","9b883de5":"shap.summary_plot (shap_values, X_test)","a224dede":"from sklearn.ensemble import GradientBoostingClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV","8f699bc5":"data.head()","7663162a":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)","eb3045df":"preds = np.round(xgb.predict(X_test))\nprint('Accuracy score = \\t {}'.format(accuracy_score(y_test, preds)))\nprint('Precision score = \\t {}'.format(precision_score(y_test, preds)))\nprint('Recall score =   \\t {}'.format(recall_score(y_test, preds)))\nprint('F1 score =      \\t {}'.format(f1_score(y_test, preds)))","fe451f8a":"# Importing testing dataset \ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')","7091d69a":"test_data.head()","03ebcbbf":"data = test_data\ndata.head()","c7fd2131":"data.isnull().sum() #checking the null values for all the columns","f737b257":"data['Title']=0\nfor i in data:\n    data['Title']=data.Name.str.extract('([A-Za-z]+)\\.')","3b074303":"data['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr','Other'],inplace=True)","5f593d24":"## Assigning the NaN Values with the Ceil values of the mean ages\ndata.loc[(data.Age.isnull())&(data.Title=='Mr'),'Age']=33\ndata.loc[(data.Age.isnull())&(data.Title=='Mrs'),'Age']=36\ndata.loc[(data.Age.isnull())&(data.Title=='Master'),'Age']=5\ndata.loc[(data.Age.isnull())&(data.Title=='Miss'),'Age']=22\ndata.loc[(data.Age.isnull())&(data.Title=='Other'),'Age']=46","27980f28":"data.Age.isnull().sum()","7e18c150":"data['Age_group']=0\ndata.loc[data['Age']<=16,'Age_group']=0\ndata.loc[(data['Age']>16)&(data['Age']<=32),'Age_group']=1\ndata.loc[(data['Age']>32)&(data['Age']<=48),'Age_group']=2\ndata.loc[(data['Age']>48)&(data['Age']<=64),'Age_group']=3\ndata.loc[data['Age']>64,'Age_group']=4\ndata.head(5)","696500d7":"data['Family']=0\ndata['Family']=data['Parch']+data['SibSp'] # Family \ndata['Single']=0\ndata.loc[data.Family==0,'Single']=1 # Single","a3f86afa":"data['Sex'].replace(['male','female'],[0,1],inplace=True)\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\ndata['Title'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","a93268e0":"data.drop(['Age','Ticket','Fare','Cabin','Name'],axis=1,inplace=True)","4837ec0f":"data.head()","f62aeb3a":"# One hot encode the categorical columns\ndf_embarked_encoding = pd.get_dummies(data['Embarked'], \n                                     prefix='embarked')\n\ndf_sex_encoding = pd.get_dummies(data['Sex'], \n                                prefix='sex')\n\ndf_plcass_encoding = pd.get_dummies(data['Pclass'], \n                                   prefix='pclass')","0f289725":"# Combine the one hot encoded columns with creating new dataset as data_encoding.\ndata_encoding = pd.concat([data, \n                        df_embarked_encoding, \n                        df_sex_encoding, \n                        df_plcass_encoding], axis=1)\n\n# Drop the original categorical columns since we had applied one hot encoding technique.\ndata_encoding = data_encoding.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","74c42bc2":"data_encoding.head(5)","962a3a63":"test = data_encoding","8c8a35e6":"test.head(5)","1a410cc6":"test.dtypes","112ff481":"predictions = xgb.predict(test)","d318889e":"# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","8d9862a6":"pred_rd = np.round(predictions).astype(int)\npred_rd","ec0be21e":"# Create a submisison dataframe and append the relevant columns\nsubmission_xgboost = pd.DataFrame()\nsubmission_xgboost['PassengerId'] = test['PassengerId']\nsubmission_xgboost['Survived'] = pred_rd # our model predictions on the test dataset\nsubmission_xgboost.head()","f29b3b95":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission_xgboost['Survived'] = submission_xgboost['Survived'].astype(int)\nprint('Converted Survived column to integers.')","b9eecf5d":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmission_xgboost.head()","d10db6d4":"import h2o\nfrom h2o.automl import H2OAutoML","e94ed37c":"h2o.init()","352dc2c0":"%%time\ntrain = h2o.import_file('..\/input\/titanic\/train.csv')\ntest = h2o.import_file('..\/input\/titanic\/test.csv')","196d8dd6":"train.describe()","f37574dd":"x = train.columns\ny = \"Survived\"","77db1a46":"train[y] = train[y].asfactor()\nx.remove(y)","1e64da57":"%%time\n\naml = H2OAutoML(max_models=7, seed=42, max_runtime_secs=7200)\naml.train(x=x, y=y, training_frame=train)","8b273621":"leaderb = aml.leaderboard\nleaderb.head()","c57edf77":"aml.leader","8006ee5c":"pred = aml.predict(test)","f812250a":"pred","82acb9a0":"passengerId = test['PassengerId']","b97c6d56":"pred_df = pred.as_data_frame()\npred_output = pred_df.predict\npassengerId_df = passengerId.as_data_frame()\nresult = pd.concat([passengerId_df,pred_output],axis=1,ignore_index = True)\nresult.columns = ['PassengerId','Survived']\nresult.to_csv('Titanic_H2O_AutoML_Predictions_Output.csv',index=False)","741d95b4":"result","b8ac3dca":"result['Survived'].value_counts()","2932233f":"## Submission File is Ready!! Happy Learning !!","5715fc01":"## Developing Prediction Model using AutoML ##","014aaaab":"## Machine Learning Model Implementation using Tree Based Models ##\n\n### 1. Light GBM Model ###\n### 2. XGBoost Model ###\n### 3. Random Forest Model ###\n### 4. H2O AutoML ### ","94c99ea3":"### XGBoost Model ###","91893e11":"### Random Forest Model ###","932187a1":"### Test Dataset ###"}}