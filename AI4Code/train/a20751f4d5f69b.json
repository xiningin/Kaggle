{"cell_type":{"c7a2eead":"code","6d1b064c":"code","90639ff4":"code","e04342de":"code","8ecaeab7":"code","e2bae781":"code","7809e0bf":"code","c4593c52":"code","834a8904":"code","b1400156":"code","d734a72d":"code","f29a1cba":"code","15dd8f2d":"code","a51fd62a":"code","1708bedc":"code","6578cfcd":"code","6c45ed5c":"code","0696321c":"markdown","d41560b3":"markdown","26d5b331":"markdown","7d357e80":"markdown","e2075568":"markdown","186b0c2b":"markdown","dc6f0dee":"markdown","38e27848":"markdown","b741fa11":"markdown","7576f87c":"markdown","47c53fb9":"markdown","92f5b55b":"markdown","ad596112":"markdown","7c1be5c7":"markdown","a36b4a6f":"markdown","1b4753ce":"markdown","62262cb9":"markdown","e33091a2":"markdown","f9f2a803":"markdown","fd2e28fe":"markdown"},"source":{"c7a2eead":"import pandas as pd\nimport numpy as np\nimport math, re, os, cv2\nimport random\nimport gc\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nprint(\"Tensorflow version \" + tf.__version__)\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\nfrom skimage.util import random_noise\nfrom skimage.transform import rotate, AffineTransform, warp","6d1b064c":"#Path of cloud and Non-cloud datasets\npath = '..\/input\/cloud-anomaly-detection-images\/noncloud\/noncloud'\npath2 = '..\/input\/cloud-anomaly-detection-images\/cloud\/cloud'","90639ff4":"#Loaded and Preprocessed all the non-cloud satellite images for training\nall_images=[]\nimport os\nimg_list = os.listdir(path)\nfor i in tqdm(img_list):\n    img = tf.keras.preprocessing.image.load_img(path+'\/'+str(i), target_size=(384,384,3))\n    img = tf.keras.preprocessing.image.img_to_array(img)\n    img = img\/255.\n    all_images.append(img)\n    \nall_images= np.array(all_images[1:])\nall_images.shape","e04342de":"#Show non-cloud images\nn = 5\nplt.figure(figsize= (20,10))\n\nfor i in range(n):\n    ax = plt.subplot(2, n, i+1)\n    plt.imshow(all_images[i+50])\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    ax = plt.subplot(2, n, i+1+n)\n    plt.imshow(all_images[i+20])\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\nplt.show()","8ecaeab7":"def anticlockwise_rotation(image):\n    angle= random.randint(0,180)\n    return rotate(image, angle)\n\ndef clockwise_rotation(image):\n    angle= random.randint(0,180)\n    return rotate(image, -angle)\n\ndef h_flip(image):\n    return  np.fliplr(image)\n\ndef v_flip(image):\n    return np.flipud(image)\n\ndef add_noise(image):\n    return random_noise(image)\n\ndef blur_image(image):\n    return cv2.GaussianBlur(img, (9,9),0)\n\n#I would not recommend warp_shifting, because it distorts image, but can be used in many use case like \n#classifying blur and non-blur images\ndef warp_shift(image): \n    transform = AffineTransform(translation=(0,40))  #chose x,y values according to your convinience\n    warp_image = warp(image, transform, mode=\"wrap\")\n    return warp_image\n\n\n","e2bae781":"# aug_images=[]\n# for i in tqdm(all_images):\n#     img = add_noise(i)\n#     img = blur_image(img)\n#     img = warp_shift(img)\n#     aug_images.append(img)\n# aug_images = np.array(aug_images)","7809e0bf":"# #Show Augmented non-cloud images\n# n = 5\n# plt.figure(figsize= (20,10))\n\n# for i in range(n):\n#     ax = plt.subplot(2, n, i+1)\n#     plt.imshow(aug_images[i+50])\n#     ax.get_xaxis().set_visible(False)\n#     ax.get_yaxis().set_visible(False)\n\n#     ax = plt.subplot(2, n, i+1+n)\n#     plt.imshow(aug_images[i+20])\n#     ax.get_xaxis().set_visible(False)\n#     ax.get_yaxis().set_visible(False)\n\n# plt.show()","c4593c52":"#Config hyperparameters\nIMAGE_SIZE = [384,384]\nSEED = 42\nn_hidden_1 = 512\nn_hidden_2 = 256\nn_hidden_3 = 64 \nn_hidden_4 = 16\nn_hidden_5 = 8\nconvkernel = (3, 3)  # convolution kernel\npoolkernel = (2, 2)  # pooling kernel","834a8904":"#seeds\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","b1400156":"\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)\n","d734a72d":"def get_model():\n    K.clear_session()\n    with strategy.scope():\n        inp1 = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3))\n\n        x = tf.keras.layers.Conv2D(n_hidden_1, convkernel, activation='relu', padding='same')(inp1)\n        x = tf.keras.layers.Conv2D(n_hidden_1, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.MaxPooling2D(poolkernel, padding='same')(x)#\n\n        x = tf.keras.layers.Conv2D(n_hidden_2, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_2, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.MaxPooling2D(poolkernel, padding='same')(x)#\n\n        x = tf.keras.layers.Conv2D(n_hidden_3, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_3, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_3, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_3, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.MaxPooling2D(poolkernel, padding='same')(x)#\n\n        x = tf.keras.layers.Conv2D(n_hidden_4, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_4, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_4, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_4, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.MaxPooling2D(poolkernel, padding='same')(x)#\n\n        x = tf.keras.layers.Conv2D(n_hidden_5, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_5, convkernel, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2D(n_hidden_5, convkernel, activation='relu', padding='same')(x)\n        encoded = tf.keras.layers.Conv2D(n_hidden_5, convkernel, activation='relu', padding='same')(x)\n\n\n        #decoder\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_5, convkernel, strides=2, activation='relu', padding='same')(encoded)\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_4, convkernel, strides=2, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_3, convkernel, strides=2, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_2, convkernel, strides=2, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_1, convkernel, strides=1, activation='relu', padding='same')(x)\n\n        decoded = tf.keras.layers.Conv2DTranspose(3, convkernel, activation=\"sigmoid\", padding='same')(x)\n\n        model = tf.keras.models.Model(inputs = inp1, outputs = decoded)\n\n        opt = tfa.optimizers.RectifiedAdam(lr=3e-4)\n        model.compile(\n            optimizer = opt,\n            loss = 'mse',\n            metrics = [tf.keras.metrics.RootMeanSquaredError()]\n        )\n\n        return model","f29a1cba":"def get_vgg19():\n    K.clear_session()\n    with strategy.scope():\n        image_input = tf.keras.layers.Input(shape = (*IMAGE_SIZE,3))\n        vg19 = tf.keras.applications.VGG19(input_tensor = image_input, weights = 'imagenet', include_top=False)\n        encoded = vg19.get_layer('block5_pool').output\n        #decode\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_5, convkernel, strides=2, activation='relu', padding='same')(encoded)\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_4, convkernel, strides=2, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_3, convkernel, strides=2, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_2, convkernel, strides=2, activation='relu', padding='same')(x)\n        x = tf.keras.layers.Conv2DTranspose(n_hidden_1, convkernel, strides=2, activation='relu', padding='same')(x)\n        decoded = tf.keras.layers.Conv2DTranspose(3, convkernel, activation=\"sigmoid\", padding='same')(x)\n        model = tf.keras.models.Model(inputs = image_input, outputs = decoded)\n        opt = tfa.optimizers.RectifiedAdam(lr=3e-4)\n        model.compile(\n            optimizer = opt,\n            loss = 'mse',\n            metrics = [tf.keras.metrics.RootMeanSquaredError()]\n        )\n        return model","15dd8f2d":"\nmodel=  get_vgg19() #get_model()\nmodel.summary()","a51fd62a":"#Split the dataset into train and test with a ratio of 80:20.\nX_train, X_test = train_test_split(all_images, test_size=0.2, random_state=SEED)\nprint(X_train.shape, X_test.shape)","1708bedc":"del all_images;  gc.collect()","6578cfcd":"#model training config params\nEPOCHS = 100\nBATCH_SIZE = 16\nNUM_TRAINING_IMAGES = X_train.shape[0]\nsteps = NUM_TRAINING_IMAGES \/\/ BATCH_SIZE\nNUM_VALID_IMAGES = X_test.shape[0]\nval_steps = NUM_VALID_IMAGES \/\/ BATCH_SIZE","6c45ed5c":"#Model training\nsav = tf.keras.callbacks.ModelCheckpoint(\n    \n    'Enc'+'.h5', monitor='val_loss', verbose=1, save_best_only=True,\n    save_weights_only=True, mode='min', save_freq='epoch')\n# lr scheduler\ncb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.4, patience = 2,\n                                                      verbose = 1, mode = 'min',min_delta = 0.0001)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', mode = 'min', patience = 5, \n                                                  verbose = 1, min_delta = 0.0001)\nmodel.fit(X_train, X_train,\n      validation_data=(X_test, X_test),\n      steps_per_epoch=steps,\n      validation_steps=val_steps,\n      epochs = EPOCHS,\n      callbacks= [sav, cb_lr_schedule,early_stopping],\n      verbose = 1)","0696321c":"# **AutoEncoders**\n\nAutoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.","d41560b3":" [Part 2: Satellite Anomaly Detection[Inference] ](https:\/\/www.kaggle.com\/ashoksrinivas\/satellite-anomaly-detection-inference)","26d5b331":"# Image Denoising\n\n![image.png](attachment:image.png)\n\nWe can use AutoEncoders for denoising as well. Here we are passing noisy data to model as an input and training it to remove the noice from the data","7d357e80":"![image.png](attachment:image.png)","e2075568":"# Anomaly Approach\n\n![image.png](attachment:image.png)\n\n* Data description:\n* Cloud Images - 100\n* Non Cloud Images- 1499\n\n* To find out anomaly(Cloud images) we will train our model only with non-anomaly(Non Cloud images) data. So that our model can understand non anomaly data more, than anomaly data. \n\n* When we train our model with non-anomaly data, Reconstruction loss will be less for non-anomaly data compared to anomaly data","186b0c2b":"**Used Pretrained VGG19 for Encoder and Transposed convolution for Decoder**","dc6f0dee":"**Convolution operation on a MxNx3 image matrix with a 3x3x3 Kernel**\n\n![](https:\/\/miro.medium.com\/max\/700\/1*ciDgQEjViWLnCbmX-EeSrA.gif)","38e27848":"# Data Generation\n\nJust like GANs we can use AutoEncoders to generate data. And we can use that data(Noisy) to create robust models.\n\n![image.png](attachment:image.png)\n","b741fa11":"![](https:\/\/miro.medium.com\/max\/3288\/1*uAeANQIOQPqWZnnuH-VEyw.jpeg)","7576f87c":"![image.png](attachment:image.png)","47c53fb9":"# **Transposed Convolution**\n\nTransposed Convolutions are used to upsample the input feature map to a desired output feature map using some learnable parameters.\n\nIt is also referred to as fractionally strided convolution due since stride over the output is equivalent to fractional stride over the input. For instance, a stride of 2 over the output is 1\/2 stride over the input.\n\n\n![image.png](attachment:image.png)","92f5b55b":"# **Implementation of Autoencoder for anomaly detection**","ad596112":"# Understanding AutoEncoders\n\n![image.png](attachment:image.png)\n\n* Encoder: Encoder is just like a normla ANN model. In which the model learns how to reduce the input dimensions and compress the input data into an encoded representation.\n* Bottleneck: Bottleneck is nothing but a hidden layer which contains the compressed representation of the input data.\n* Decoder: In which the model learns how to reconstruct the data from the encoded representation to be as close to the original input as possible.\n* Reconstruction Loss: This is the method that measures measure how well the decoder is performing and how close the output is to the original input.\n","7c1be5c7":"# **Cofiguring TPU for Training**","a36b4a6f":"Convolutional neural network is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects\/objects in the image and be able to differentiate one from the other.","1b4753ce":"*Loading the required Libraries*","62262cb9":"Applications of Transposed Convolution:\n\n* 1)Super- Resolution\n* 2)Semantic Segmentation\n\n![image.png](attachment:image.png)","e33091a2":"# Augment Non-cloud images","f9f2a803":"# **Introduction to convolutional neural network (CNN)**","fd2e28fe":"# Customized autoencoder architecture"}}