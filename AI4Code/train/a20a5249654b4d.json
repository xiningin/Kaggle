{"cell_type":{"94a3ad26":"code","7535817f":"code","7cd599a9":"code","84a59e98":"code","9162b214":"code","8642d713":"code","ea08aff3":"code","da8d9810":"code","c0f89b0f":"code","b730e9a6":"code","36658cf0":"code","03e133c4":"code","0286c9e3":"code","f7439619":"code","92e4f751":"code","85ba8aba":"code","0acb755d":"code","bb3bdfea":"code","89e63f73":"code","b708a7f5":"code","d60d2a0d":"code","90e9d306":"code","b8f16827":"code","cd6f7807":"code","2f654bc4":"code","b4eb1ada":"code","831acf86":"code","0488d2e5":"code","63aea409":"code","1ee33ef1":"code","e59ba5b6":"code","c906bb35":"code","5791068a":"code","7cefac1d":"code","27ea8e45":"code","2bd43b34":"code","5b38d26f":"code","453fa3a0":"code","169d1a21":"code","e98ac0cf":"code","9bb5ab0f":"code","8ed8f4eb":"code","b595e2c3":"code","85183b2f":"code","de295b7e":"code","f70e24dd":"code","e45c0f1c":"code","6310167c":"markdown"},"source":{"94a3ad26":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7535817f":"from matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.simplefilter(\"ignore\")","7cd599a9":"trainLabels_df = pd.read_csv(\"\/kaggle\/input\/data-science-london-scikit-learn\/trainLabels.csv\", header=None)\ntrain_df = pd.read_csv(\"\/kaggle\/input\/data-science-london-scikit-learn\/train.csv\", header=None)\ntest_df = pd.read_csv(\"\/kaggle\/input\/data-science-london-scikit-learn\/test.csv\", header=None)\n","84a59e98":"print(\"trainLabels shape: \", trainLabels_df.shape)\nprint(\"train shape: \", train_df.shape)\nprint(\"test shape: \", test_df.shape)","9162b214":"train_df.head()","8642d713":"trainLabels_df.head()","ea08aff3":"test_df.head()","da8d9810":"train_df.info()","c0f89b0f":"train_df.isnull().sum()","b730e9a6":"train_df.describe()","36658cf0":"trainLabels_df.isnull().sum()","03e133c4":"trainLabels_df.columns = [\"label\"]\nm_train_df = pd.merge(train_df, trainLabels_df, left_index=True, right_index=True)\nm_train_df.head()","0286c9e3":"plt.figure(figsize=(25,25))\nsns.heatmap(m_train_df.corr(), annot=True)","f7439619":"train_df.hist(figsize=(20,20))","92e4f751":"m_train_df","85ba8aba":"int_cols = train_df.columns\nfig, ax= plt.subplots(nrows=10, ncols=4, figsize=(40,20), constrained_layout=True)\nplt.suptitle('Feature distribution by label', size=20, weight='bold')\nax=ax.flatten()\nfor x, i in enumerate(int_cols):\n    sns.boxplot(data=m_train_df, y=i, x='label', ax=ax[x])\n    for s in ['left','right','top','bottom']:\n        ax[x].spines[s].set_visible(False)","0acb755d":"values_freq = m_train_df[\"label\"].value_counts()\nprint(values_freq)\nvalues_freq.plot(kind='pie', autopct='%1.1f%%')\nplt.title('values_freq')\nplt.axis('equal')\nplt.show()","bb3bdfea":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(train_df.values)","89e63f73":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x_train, trainLabels_df.values, test_size=0.1, shuffle=True)","b708a7f5":"x_train.shape, x_test.shape, y_train.shape, y_test.shape","d60d2a0d":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n\ndef print_score(y_pred, y_real):\n    print(\"Accuracy: \", accuracy_score(y_real, y_pred))\n\n    print()\n    print(\"Macro precision_recall_fscore_support (macro) average\")\n    print(precision_recall_fscore_support(y_real, y_pred, average=\"macro\"))\n\n    print()\n    print(\"Macro precision_recall_fscore_support (micro) average\")\n    print(precision_recall_fscore_support(y_real, y_pred, average=\"micro\"))\n\n    print()\n    print(\"Macro precision_recall_fscore_support (weighted) average\")\n    print(precision_recall_fscore_support(y_real, y_pred, average=\"weighted\"))\n    \n    print()\n    print(\"Confusion Matrix\")\n    print(confusion_matrix(y_real, y_pred))","90e9d306":"from sklearn.model_selection import GridSearchCV\n\ndef get_trained_grid(model, grid_params, x_train, y_train ,refit=True, cv=10, verbose=1):\n    grid = GridSearchCV(model, grid_params, refit=refit, cv=cv, verbose=verbose)\n    grid.fit(x_train, y_train)\n    return grid","b8f16827":"def get_grid_best_params(grid):\n    print(grid.best_params_)\n    print(grid.best_estimator_)","cd6f7807":"def print_grid_performance(grid, x_test, y_test):\n    y_pred = grid.predict(x_test)\n    print_score(y_pred, y_test)","2f654bc4":"from sklearn.linear_model import SGDClassifier\n\n%time\ngrid_params = { \"loss\": [\"hinge\", \"log\", \"modified_huber\"],\n               \"penalty\": [\"l1\", \"l2\", \"elasticnet\"]   \n}\n\ngrid = get_trained_grid(SGDClassifier(), grid_params, x_train, y_train)\nget_grid_best_params(grid)","b4eb1ada":"print_grid_performance(grid, x_test, y_test)","831acf86":"from sklearn.neighbors import KNeighborsClassifier\n\n%time\ngrid_params = { \"n_neighbors\": np.arange(1,50)}\n\ngrid = get_trained_grid(KNeighborsClassifier(), grid_params, x_train, y_train)\nget_grid_best_params(grid)","0488d2e5":"print_grid_performance(grid, x_test, y_test)","63aea409":"from sklearn.naive_bayes import GaussianNB\n\n%time\ngrid_params = { \"var_smoothing\": [1e-09] }\n\ngrid = get_trained_grid(GaussianNB(), grid_params, x_train, y_train)\nget_grid_best_params(grid)","1ee33ef1":"print_grid_performance(grid, x_test, y_test)","e59ba5b6":"from sklearn.tree import DecisionTreeClassifier\n\n%time\ngrid_params = {'criterion': [\"gini\", \"entropy\"], \n              'splitter': ['best', 'random'], \n              'max_depth': [3,4,None], \n              'min_samples_split':[2, 4, 6],\n              'min_samples_leaf':[1,2,3]}\n\ngrid = get_trained_grid(DecisionTreeClassifier(), grid_params, x_train, y_train)\nget_grid_best_params(grid)","c906bb35":"print_grid_performance(grid, x_test, y_test)","5791068a":"from sklearn.ensemble import RandomForestClassifier\n\n%time\ngrid_params = {'n_estimators': [10, 20, 50], \n              'max_features': ['auto', 'sqrt', 'log2'], \n              'bootstrap': [True, False], \n              'criterion':['entropy', 'gini']}\n\ngrid = get_trained_grid(RandomForestClassifier(), grid_params, x_train, y_train)\nget_grid_best_params(grid)","7cefac1d":"print_grid_performance(grid, x_test, y_test)","27ea8e45":"from sklearn import svm\n\n%time\ngrid_params = { \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n               \"degree\": [1, 2 ,3, 4, 5, 6] }\n\ngrid = get_trained_grid(svm.SVC(), grid_params, x_train, y_train)\nget_grid_best_params(grid)","2bd43b34":"print_grid_performance(grid, x_test, y_test)","5b38d26f":"from sklearn.naive_bayes import BernoulliNB\n\n%time\ngrid_params = {'alpha': [0.25, 0.5, 1]}\n\ngrid = get_trained_grid(BernoulliNB(), grid_params, x_train, y_train)\nget_grid_best_params(grid)","453fa3a0":"print_grid_performance(grid, x_test, y_test)","169d1a21":"from xgboost import XGBClassifier\n\n%time\ngrid_params = {'learning_rate': [0.01, 0.05, 0.1], \n              'eval_metric': ['error']}\n\ngrid = get_trained_grid(XGBClassifier(), grid_params, x_train, y_train)\nget_grid_best_params(grid)","e98ac0cf":"print_grid_performance(grid, x_test, y_test)","9bb5ab0f":"from sklearn.ensemble import RandomForestClassifier\n\n%time\nclr = RandomForestClassifier(bootstrap=False, criterion=\"gini\", max_features=\"log2\", n_estimators=50)\nclr.fit(x_train, y_train)","8ed8f4eb":"print_grid_performance(clr, x_train, y_train)","b595e2c3":"print_grid_performance(clr, x_test, y_test)","85183b2f":"x_test = test_df.values\nx_test = scaler.transform(x_test)\nx_test[:5]","de295b7e":"submit = pd.DataFrame(clr.predict(x_test))\nsubmission = pd.DataFrame(submit)\nsubmission.columns = ['Solution']\nsubmission['Id'] = np.arange(1,submission.shape[0]+1)\nsubmission = submission[['Id','Solution']]","f70e24dd":"submission","e45c0f1c":"submission.to_csv('submission.csv', index=False)\nprint(submission.shape)","6310167c":"### best model accuracy from Random forest tree 88% to 90%\n#### {'bootstrap': False, 'criterion': 'gini', 'max_features': 'log2', 'n_estimators': 50}"}}