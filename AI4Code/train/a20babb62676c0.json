{"cell_type":{"d55a6cad":"code","40c49a43":"code","29f4dc7d":"code","f311eb7a":"code","ed9d9892":"code","58728ffd":"markdown"},"source":{"d55a6cad":"import os\nimport cv2\nimport pandas as pd\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nRESIZE = 512\nIMG_QUALITY = 95\nDEBUG = False","40c49a43":"import math\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef serialize_example(feature0, feature1, feature2):\n  feature = {\n      'image': _bytes_feature(feature0),\n      'id': _bytes_feature(feature1),\n      'class': _int64_feature(feature2)\n  }\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()","29f4dc7d":"%cd \/kaggle\/working\n\nbase_path = '\/kaggle\/working'\ntrain_videos_path = \"\/kaggle\/input\/nfl-impact-detection\/train\"\n\ndf_labels = pd.read_csv(\"\/kaggle\/input\/nfl-impact-detection\/train_labels.csv\")\ndf_labels['impact'] = np.nan_to_num(df_labels['impact'].values)\ncontent_trainlist = \"\"\ncontent_testlist  = \"\"\n\nfor video_index, video_name in enumerate(os.listdir(train_videos_path)):\n    print(video_index)\n    cap = cv2.VideoCapture(os.path.join(train_videos_path,video_name))\n    if (cap.isOpened() == False):\n      print(\"Unable to read camera feed\")\n    frame_no = 0\n    \n    tf_filename = f'{video_name}.tfrec'\n    with tf.io.TFRecordWriter(tf_filename) as wf:\n        while(True):\n            frame_no += 1\n            ret, frame = cap.read()\n            if not ret:\n                break\n            \n            frame_height, frame_width,_ = frame.shape\n\n            content = \"\"\n            df = df_labels.query(f\"frame=={frame_no}\")\n            df = df[df.video==video_name]\n            \n            img = cv2.resize(frame, (RESIZE, RESIZE))\n            # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  -> Fix:20201121\n            \n            img = cv2.imencode('.jpg', img, (cv2.IMWRITE_JPEG_QUALITY, IMG_QUALITY))[1].tostring()\n            img_name = str.encode(f\"{video_name}_{frame_no}\")\n            target = int(df.impact.max())\n            example = serialize_example(img, img_name, target)\n            wf.write(example)\n    cap.release()\n#     if video_index > 4:\n#         break","f311eb7a":"import numpy as np\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    return image\n\ndef parse_example(example):\n    LABELED_TFREC_FORMAT = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'id': tf.io.FixedLenFeature([], tf.string),\n        'class': tf.io.FixedLenFeature([], tf.int64)\n    }\n    \n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = example['id']\n    target = example['class']\n    return image, label, target\n\ndef display_one(image, title, target, subplot, red=False, titlesize=16):\n    plt.subplot(*subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(f'{title}: {target}')\n    return (subplot[0], subplot[1], subplot[2]+1)\n\ndef display_batch_of_images(databatch):\n    images, labels, targets = databatch\n    images = images.numpy()\n    labels = labels.numpy()\n    targets = targets.numpy()\n    if labels is None:\n        labels = [None for _ in enumerate(images)]\n    if targets is None:\n        targets = [None for _ in enumerate(targets)]\n        \n    # auto-squaring: this will drop data that does not fit into square or square-ish rectangle\n    rows = int(math.sqrt(len(images)))\n    cols = len(images)\/\/rows\n        \n    # size and spacing\n    FIGSIZE = 13.0\n    SPACING = 0.2\n    subplot=(rows, cols, 1)\n    if rows < cols:\n        plt.figure(figsize=(FIGSIZE,FIGSIZE\/cols*rows))\n    else:\n        plt.figure(figsize=(FIGSIZE\/rows*cols,FIGSIZE))\n    \n    # display\n    for i, (image, label, target) in enumerate(zip(images[:rows*cols], labels[:rows*cols], targets[:rows*cols])):\n        title = label\n        title = title.decode('utf-8')\n        correct = True\n        dynamic_titlesize = FIGSIZE*SPACING\/max(rows,cols)*40+3 # magic formula tested to work from 1x1 to 10x10 images\n        subplot = display_one(image, title, target, subplot, not correct, titlesize=dynamic_titlesize)\n    \n    #layout\n    plt.tight_layout()\n    if label is None and predictions is None:\n        plt.subplots_adjust(wspace=0.2, hspace=0.2)\n    else:\n        plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n    plt.show()","ed9d9892":"resize_file = \"58106_002918_Sideline.mp4.tfrec\"\ndataset = tf.data.TFRecordDataset([resize_file]).map(parse_example).batch(25)\ndata = iter(dataset)\ndisplay_batch_of_images(next(data))","58728ffd":"### image checking"}}