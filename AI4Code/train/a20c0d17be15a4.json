{"cell_type":{"c4633a2e":"code","9ca0ca2c":"code","de1b8af9":"code","9bf85a15":"code","4c8cfeb9":"code","ec6270c7":"code","960cc95e":"code","584a2e37":"code","15551ea5":"code","06744428":"code","4a4b27e1":"code","1ad80540":"code","3ae312ea":"code","7f0f0992":"code","00985af6":"code","0570f534":"code","a732c3be":"code","6f89a4e2":"code","374cc64c":"code","fb9097af":"markdown","35da7627":"markdown","9973286d":"markdown","14bd346e":"markdown","f413bfb5":"markdown","a2b38ec5":"markdown","bf01485e":"markdown","68b90d64":"markdown","3bfd0d9c":"markdown"},"source":{"c4633a2e":"# IMPORT LIBRARIES\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import RobustScaler\n\nimport warnings\nwarnings.simplefilter(action = \"ignore\") \n\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","9ca0ca2c":"# READ THE DATA\n\ndiabetes= pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf=diabetes.copy()","de1b8af9":"# AT FIRST GLANCE\n\ndef info_(dataframe):\n    print(\"DF SHAPE:\",dataframe.shape, \"\\n\")\n    print(\"OUTCOME 1\/ALL DF RATIO:\",len(df[df[\"Outcome\"]==1])\/len(df), \"\\n\")\n    print(\"OUTCOME 0\/ALL DF RATIO:\",len(df[df[\"Outcome\"]==0])\/len(df), \"\\n\")\n    print(\"NUMBER OF NULL VALUES:\", df.isnull().any(axis=1).sum(), \"\\n\")\n    print(dataframe.head(3), \"\\n\")\n    object_list=[col for col in dataframe.columns if dataframe[col].dtypes==\"O\"]\n    numeric_list=[col for col in dataframe.columns if dataframe[col].dtypes!=\"O\"]\n    print(\"ALL COLUMNS:\", df.columns, \"\\n\")\n    print(\"OBJECT COLUMNS:\",object_list,\"\\n\")\n    print(\"NUMERIC COLUMNS\",numeric_list,\"\\n\")\n    print(\"CORRELATIONS\",sns.heatmap(df.corr()));\n    print(sns.pairplot(df));\n    return object_list, numeric_list","9bf85a15":"obj_list, num_list=info_(df)","4c8cfeb9":"# 0'lar\u0131n NA de\u011ferler oldu\u011funu yukar\u0131da verilen referanslardan biliyorduk.\n# Bu k\u0131s\u0131mda \u00f6nce Df'in 0'lar\u0131n\u0131n at\u0131lm\u0131\u015f halini bulaca\u011f\u0131m. \n#Sonra NA de\u011ferlerine, bu s\u0131f\u0131rs\u0131z df'e yapt\u0131\u011f\u0131m k\u0131r\u0131l\u0131mla atama yapaca\u011f\u0131m.","ec6270c7":"df1=df.copy()\ndf1=df1.loc[:,\"Pregnancies\": \"Age\"].replace(0, np.nan).dropna()","960cc95e":"for i in df1.columns:\n    if i!=\"Outcome\":\n        print(i.upper(),\"\\n\",\"for diabet:\", df1.loc[df[\"Outcome\"]==1, i].median(),\"\\n\",\"for non-diabet:\" , df1.loc[df[\"Outcome\"]==0, i].median())","584a2e37":"for i in df.columns:\n    for k in range(len(df)):\n        if i!=\"Outcome\":\n            if df.loc[k,i]==0: #e\u011fer de\u011fer 0'a e\u015fitse\n                if df.loc[k,\"Outcome\"]==1:   #bu de\u011ferin outcome'\u0131 1'se\n                     df.loc[k,i]=df1.loc[df[\"Outcome\"]==1, i].median()# na de\u011fer olmayan df'teki outcome'\u0131 1 olanlar\u0131n i medyan\u0131n\u0131 ata.\n                elif df.loc[k,\"Outcome\"]==0:\n                     df.loc[k,i]=df1.loc[df[\"Outcome\"]==0, i].median()# na de\u011fer olmayan df'teki outcome'\u0131 0 olanlar\u0131n i medyan\u0131n\u0131 ata.\n","15551ea5":"y = df[\"Outcome\"]\nX = df.drop([\"Outcome\"], axis = 1)\nlog_model = LogisticRegression().fit(X,y)\ny_pred = log_model.predict(X)\nprint(accuracy_score(y, y_pred))\nprint(classification_report(y, y_pred))","06744428":"y = df[\"Outcome\"]\nX = df.drop([\"Outcome\"], axis = 1)\n\n\n\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('SVM', SVC(gamma='auto')))\nmodels.append(('XGB', GradientBoostingClassifier()))\nmodels.append((\"LightGBM\", LGBMClassifier()))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\n\n\nfor name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 123456)\n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        ","4a4b27e1":"Importance = pd.DataFrame({'Importance':GradientBoostingClassifier().fit(X, y).feature_importances_*100}, \n                          index = X.columns)\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', figsize=(8,6))\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","1ad80540":"df2=df.copy()\ndf2[\"Insulin\/Age\"]=df2[\"Insulin\"]\/df2[\"Age\"]\ndf2[\"BMI\/Age\"]=df2[\"BMI\"]\/df2[\"Age\"]\ndf2[\"Pregnancies\/Age\"]=df2[\"Pregnancies\"]\/df2[\"Age\"]\ndf2[\"Ins*Glu\"]=df2[\"Insulin\"]* df2[\"Glucose\"]\n\ndf2=df2.drop([\"Age\"], axis=1)","3ae312ea":"#\u00c7E\u015e\u0130TL\u0130 FEATURE DENEMELER\u0130\n#df2[\"BMI*Skin\"]=df2[\"BMI\"]* df2[\"SkinThickness\"]\n#df2=df2.drop([DiabetesPedigreeFunction\",\"BloodPressure\",\"SkinThickness\"], axis=1)\n#df2=df2.drop([\"BMI\",\"SkinThickness\"], axis=1)","7f0f0992":"y = df2[\"Outcome\"]\nX = df2.drop([\"Outcome\"], axis = 1)\n\nmodels = []\nmodels.append(('XGB', GradientBoostingClassifier()))\nmodels.append((\"LightGBM\", LGBMClassifier()))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 123456)\n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        ","00985af6":"def lof(dataframe, n,c, numlist, plot_xlim,thr):\n    clf=LocalOutlierFactor(n_neighbors=n, contamination=c)\n    clf.fit_predict(dataframe[numlist])\n    df_scores= clf.negative_outlier_factor_\n    threshold=np.sort(df_scores)[thr]\n    print(np.sort(df_scores)[0:50])\n    pd.DataFrame(np.sort(df_scores)).plot(stacked=True,xlim=[0,plot_xlim], style=\".-\");\n    plt.show()\n    return df_scores,threshold ","0570f534":"cols=[col for col in df2.columns if col!=\"Outcome\"]\nscores, threshold =lof(df2, 20, 0.1, cols, 100,50)","a732c3be":"df3=df2.drop(df2[scores< threshold].index, axis=0).reset_index(drop=True)","6f89a4e2":"y = df3[\"Outcome\"]\nX = df3.drop([\"Outcome\"], axis = 1)\n\nmodels = []\nmodels.append(('XGB', GradientBoostingClassifier()))\nmodels.append((\"LightGBM\", LGBMClassifier()))\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    \n        kfold = KFold(n_splits = 10, random_state = 123456)\n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        ","374cc64c":"# Overfit var m\u0131?\ny = df3[\"Outcome\"]\nX = df3.drop([\"Outcome\"], axis = 1)\nlog_model = LogisticRegression().fit(X,y)\ny_pred = log_model.predict(X)\nprint(accuracy_score(y, y_pred))\nprint(classification_report(y, y_pred))","fb9097af":"# T\u00dcM S\u00dcTUNLARDAK\u0130 0 DE\u011eERLER\u0130NE KIRILIMLA ATAMA","35da7627":"# LOF: LightGBM: 0.901154","9973286d":"# NEW FEATURES : XGB: 0.897180","14bd346e":"# Base Model: Feature Importance ","f413bfb5":"- Bu \u00e7al\u0131\u015fmada yap\u0131lan \u00f6n i\u015fleme ad\u0131mlar\u0131n\u0131n %99'u, tez ve bilimsel ara\u015ft\u0131rmalar\u0131n referans al\u0131nmas\u0131 ile yap\u0131lm\u0131\u015ft\u0131r.\n- \u00d6nce veri seti ve de\u011fi\u015fkenler tan\u0131t\u0131ld\u0131, literat\u00fcrdeki thresholdlar belirtildi.\n- Veri setindeki t\u00fcm 0'lar\u0131n al\u0131namayan de\u011ferleri yani NA'leri temsil etti\u011fi literat\u00fcr ara\u015ft\u0131rmalar\u0131ndan bilinmektedir.\n- \u00d6nce 0'lara NA atand\u0131, sonra bu NA'ler na olmayan df'teki target k\u0131r\u0131l\u0131mlar\u0131na g\u00f6re dolduruldu.\n- Base model kuruldu. (XGB: 0.889)\n- Al\u0131nan ilk feature importance ile 4 yeni de\u011fi\u015fken olu\u015fturuldu, Age de\u011fi\u015fkeni veri setinden at\u0131ld\u0131.Model sonucu: XGB: 0.897180\n- LOF ile ilk 50 de\u011fer drop edildi, Light GBM: 0.901154\n- Verinin son haline lojistic regresyon uygulanarak overfit olup olmad\u0131\u011f\u0131n\u0131n anla\u015f\u0131lmas\u0131 hedeflendmi\u015ftir. \n- Overfit olmad\u0131\u011f\u0131 ve recall de\u011ferinin d\u00fc\u015f\u00fck oldu\u011fu g\u00f6zlemlenmi\u015ftir. Bu durumu d\u00fczelmeye y\u00f6nelik bir eylemde bulunulmam\u0131\u015ft\u0131r.\n- Not: Robust Scaler denenerek sonu\u00e7lar\u0131n daha k\u00f6t\u00fc oldu\u011fu anla\u015f\u0131lm\u0131\u015f olup \u00e7al\u0131\u015fmadan kald\u0131r\u0131lm\u0131\u015ft\u0131r.","a2b38ec5":"# INFORMATION ABOUT THE DATASET\n\n\n    \n# Veri seti \u00fczerinde yap\u0131lan ara\u015ft\u0131rmalara g\u00f6re,\n\n- Veri seti ABD'deki Ulusal Diyabet-Sindirim-B\u00f6brek Hastal\u0131klar\u0131 Enstit\u00fcleri'nde tutulan b\u00fcy\u00fck veri setinin par\u00e7as\u0131d\u0131r.\n- ABD'deki Arizona Eyaleti'nin en b\u00fcy\u00fck 5. \u015fehri olan Phoenix \u015fehrinde ya\u015fayan 21 ya\u015f ve \u00fczerinde olan Pima Indian kad\u0131nlar\u0131\n\u00fczerinde yap\u0131lan diyabet ara\u015ft\u0131rmas\u0131 i\u00e7in kullan\u0131lan verilerdir.(Baz\u0131 kaynaklarda Hintli ya da baz\u0131lar\u0131nda K\u0131z\u0131lderili olarak ge\u00e7iyorlar.)\n- 768 g\u00f6zlem, 8 numerik ba\u011f\u0131ms\u0131z de\u011fi\u015fkenden olu\u015fmaktad\u0131r. Hedef de\u011fi\u015fken \"outcome\" olarak belirtilmi\u015f olup 1 diyabet test sonucunun pozitif olu\u015funu, 0 ise negatif olu\u015funu belirtmektedir. (268:1, 500:0)\n\n# De\u011fi\u015fkenler:\n\n- **Pregnancies**: Hamilelik Say\u0131s\u0131\n- **Glucose**: OGTT: Oral glukoz tolerans testi gebelere yap\u0131l\u0131r. \u0130lk defa gebelikte ba\u015flayan veya te\u015fhis edilen glukoz intolerans\u0131 \u00f6l\u00e7\u00fcm testi.Gestasyonel diyabet tan\u0131s\u0131 konur. \n    - 2 saatlik yap\u0131lan OGTT'yi temsil etmektedir.(tek a\u015famal\u0131)\n    - 140 mg\/dL >= : diyabet riskinin artt\u0131\u011f\u0131 durumdur.\n    -  \n- **BloodPressure**: DBP Diastolik kan bas\u0131nc\u0131n\u0131 mmHg cinsinden temsil etmektedir.\n    \n    - <85 :2 y\u0131lda bir kontrol edilmeli\n    - 85-89 : Senede bir\n    - 90-99 : 2 ay i\u00e7inde \n    - 100-109 : 1 ay i\u00e7inde\n    - 110 > : 1 hafta i\u00e7inde ya da hemen\n\n- **SkinThickness**: TSFT Triceps skinfold thickness (mm).        \n        \n- **Insulin**: Two-hour serum insulin (\u03bcU\/mL) (2HSI). Ins\u00fclin testi(tokluk 2 saat)\n    - 120 alt\u0131 normal- 140-199 gizli \u015feker- 200 ve \u00fczeri diyabet\n\n- **BMI** :Body Mass Index : kilo\/ boy^2\n    - <18.5 zay\u0131f\n    - 18.5-24.9 sa\u011fl\u0131kl\u0131\n    - 25-29.9 kilolu\n    - 30-40 \u015fi\u015fman\n    - 40.1-60 a\u015f\u0131r\u0131 \u015fi\u015fman\n        \n- **DiabetesPedigreeFunction** : Soya\u011fac\u0131  \n- **Age** : Ya\u015f ( diyabet ise ya\u015f median de\u011feri 36, de\u011filse 25)\n- **Outcome**\n\n# D\u0130KKAT\n\n- **Misingler de\u011ferlerin olmay\u0131\u015f\u0131n\u0131 temsil etmektedir.** \n    - (Referans: https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352914816300016#bib65 (3.7. madde))\n    \n# References:\n- https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352914816300016#bib65 (3.7. madde)\n- https:\/\/tr.ulfsciences.com\/native-america-s-alleles-44932\n- https:\/\/www.foodandnutritionjournal.org\/volume7number2\/significance-of-health-related-predictors-of-diabetes-in-pima-indians-women\/\n- https:\/\/www.novonordisk.com.tr\/hastalara-ozel\/diyabet\/diyabet-tanisi-ve-tipleri.html\n- https:\/\/tkd.org.tr\/kilavuz\/k03\/3_18530.htm?wbnum=1103\n- https:\/\/aysetugbasengel.com\/kan-sekeri-degerleri-ne-olmali-aclik-tokluk-kan-sekeri-olcumu\/#Tokluk_Kan_Sekeri_Testi_Nedir,_Nasil_Yapilir\n- http:\/\/www.taylankumeli.com\/hesaplama\/bmi-hesapla\/l","bf01485e":"# DATA PREPROCESSING","68b90d64":"# DATA UNDERSTANDING","3bfd0d9c":"# BASE MODEL : XGB: 0.889371"}}