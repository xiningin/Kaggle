{"cell_type":{"d33ade8b":"code","1deac30b":"code","58abd3c0":"code","9f1416f9":"code","b1b3f00a":"code","2332edca":"code","c16fb35e":"code","f55f7685":"code","399e7b3e":"code","7e20624c":"code","223d2957":"code","a2bb0c4b":"code","5549cf70":"code","b367df3f":"code","8562bc9b":"code","4046d400":"code","9554e365":"code","065ac969":"code","9b3e9c4d":"code","1f4d66c8":"code","2fed2c3c":"code","dc78e396":"code","7e8df3d8":"code","ddadf60d":"code","d16436ee":"code","69d16fd1":"code","8c976ace":"code","4d9b9d13":"code","29cd0209":"code","2a3c0150":"code","18d20d12":"code","d9b8c555":"code","2adb9286":"code","8def4700":"code","df21cb70":"code","e8cc689f":"code","3a5b9380":"code","b2b51ed4":"code","ec7b4740":"code","122b9341":"code","33206561":"code","fb49e6ed":"code","bfb32120":"code","aecfa585":"code","000c7b70":"code","79214caf":"code","e349ddd2":"code","2c478247":"code","b1171a9f":"code","5173078f":"code","c0fa0ebf":"code","eaa5531f":"code","026a8c54":"code","f37c7b92":"code","159302f6":"code","bc6ab166":"code","ccea8c46":"code","2c888287":"code","cdf862d0":"code","32e73da7":"code","18006d5c":"code","79d84ce9":"code","bca4f297":"code","82111732":"code","624b5d80":"code","1bba736f":"code","3b71d883":"code","4b6fdf37":"code","47227819":"code","1199c876":"code","c7038299":"code","27d37039":"markdown","9fd238f3":"markdown","acb5a2d2":"markdown","0d75e5bb":"markdown","a38f7b38":"markdown","9ed5e82e":"markdown","a8bdda3c":"markdown","5c7a75e9":"markdown","561409d5":"markdown","be146d1f":"markdown","3af26635":"markdown","db9df507":"markdown","9cc122bd":"markdown","389af92c":"markdown","11b0d1d1":"markdown","d4ac673d":"markdown","28d0c95e":"markdown","40ec293c":"markdown","ba86bedd":"markdown","a00c4517":"markdown","333f4e5e":"markdown","7d6147b8":"markdown","c4cb5493":"markdown","ae75ec81":"markdown","250b08e1":"markdown","ae48a8b2":"markdown","1e38394d":"markdown","64e2f1d6":"markdown","dd35710d":"markdown","7bc4cb5a":"markdown","3a5b2b98":"markdown","4a01cd10":"markdown","5265aa2d":"markdown","22021a90":"markdown","2fff2268":"markdown","13e0d2b3":"markdown","623bf8c9":"markdown","6563f9f0":"markdown","94083a39":"markdown","686553ca":"markdown","594d4f2d":"markdown","65b43591":"markdown","1ff1d7de":"markdown","6da5b3ec":"markdown","694badc9":"markdown","502ae50e":"markdown","d28b171b":"markdown","f760129d":"markdown","9c1e80d0":"markdown","d47d373a":"markdown","d1d8dc59":"markdown","acf663e6":"markdown","75c42c49":"markdown"},"source":{"d33ade8b":"# Data wrangling\nimport pandas as pd\nimport numpy as np\n\n# Data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistics\nfrom scipy.stats import norm, boxcox_normmax\nfrom scipy.special import boxcox1p\n\n# Modelling\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Miscellaneous\nfrom collections import Counter","1deac30b":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nss = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","58abd3c0":"train.head()","9f1416f9":"# Training set columns\n\ntrain.columns","b1b3f00a":"test.head()","2332edca":"ss.head()","c16fb35e":"# Combine training and test set\n\ncombine = pd.concat([train, test])","f55f7685":"# All dataframe shape\n\nprint(\"Training set shape: \", train.shape)\nprint(\"Test set shape: \", test.shape)\nprint(\"Combined shape: \", combine.shape)\nprint(\"Sample submission shape: \", ss.shape)","399e7b3e":"# Summary statistics for training set\n\ntrain.describe().transpose()","7e20624c":"# Summary statistics for test set\n\ntest.describe().transpose()","223d2957":"combine.dtypes.value_counts()","a2bb0c4b":"# Summary statistics for sale price\n\ntrain['SalePrice'].describe()","5549cf70":"# Sale price distribution\n\nplt.figure(figsize = (12, 5))\nsns.set_style('white')\nsns.distplot(train['SalePrice'], label = 'Skewness: %.2f'%train['SalePrice'].skew())\nplt.legend(loc = 'best')\nplt.title('Sale Price Distribution')","b367df3f":"# Skewness and kurtosis\n\nprint(\"Skewness: %f\"%train['SalePrice'].skew())\nprint(\"Kurtosis: %f\"%train['SalePrice'].kurt())","8562bc9b":"# Correlation between numerical variables\n\ncorr = combine.corr()\nplt.figure(figsize = (12, 8))\nsns.heatmap(corr, cmap = 'coolwarm')","4046d400":"# Features most correlated with sale price\n\ncorr['SalePrice'].sort_values(ascending = False).head(10)","9554e365":"# Sale price against overall quality \n\nsns.set_style('white')\nsns.factorplot(x = 'OverallQual', y = 'SalePrice', data = train, kind = 'bar')\nplt.title('Sale Price Against Overall Quality')","065ac969":"# Sale price against GrLivArea\n\nsns.set_style('darkgrid')\nsns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train)\nplt.title('Sale Price Against GrLivArea')","9b3e9c4d":"# Sale price against garage cars\n\nsns.set_style('white')\nsns.factorplot(x = 'GarageCars', y = 'SalePrice', data = train, kind = 'bar')\nplt.title('Sale Price Against Garage Cars')","1f4d66c8":"# Sale price against TotalBsmtSF\n\nsns.set_style('darkgrid')\nsns.scatterplot(x = 'TotalBsmtSF', y = 'SalePrice', data = train)\nplt.title('Sale Price Against TotalBsmtSF')","2fed2c3c":"# Sale price against FullBath\n\nsns.set_style('white')\nsns.factorplot(x = 'FullBath', y = 'SalePrice', data = train, kind = 'bar')\nplt.title('Sale Price Against Full Bath')","dc78e396":"# Sale price against year built\n\nsns.set_style('white')\nsns.lineplot(x = 'YearBuilt', y = 'SalePrice', data = train)\nplt.title('Sale Price Against Year Built')","7e8df3d8":"sns.set_style('darkgrid')\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols])","ddadf60d":"# Missing data in combined dataframe\nmissing = combine.isnull().sum()\nmissing = missing[missing > 0]\n\n# Percentage missing\npercent_missing = missing \/ len(combine)\n\n# Concat missing and percentage missing\nmissing_df = pd.concat([missing, percent_missing], axis = 1, keys = ['Total', 'Percent'])\n\n# Drop sale price because test set does not have sale price\nmissing_df = missing_df.drop('SalePrice')\n\n# Create dataframe for missing data \nmissing_df.sort_values(by = 'Total', ascending = False, inplace = True)\nmissing_df","d16436ee":"# Columns with missing data\n\nplt.figure(figsize = (15, 5))\nmissing_df['Total'].plot(kind = 'bar')","69d16fd1":"print(\"Number of features with missing data in combined dataframe: \", len(missing_df))","8c976ace":"missing_features = missing_df.index\nmissing_features","4d9b9d13":"# Get missing features that are related to garage\n\ngarage_features = [feature for feature in missing_features if 'Garage' in feature]\ngarage_features","29cd0209":"# Check data types of garage features\n\ncombine[garage_features].dtypes","2a3c0150":"# Fill missing garage features\n\nfor feature in garage_features:\n    if combine[feature].dtype == 'object':\n        combine[feature] = combine[feature].fillna('None')\n    else:\n        combine[feature] = combine[feature].fillna(0)","18d20d12":"# Get missing features that are related to basement\n\nbasement_features = [feature for feature in missing_features if 'Bsmt' in feature]\nbasement_features","d9b8c555":"# Check data types of basement features\n\ncombine[basement_features].dtypes","2adb9286":"# Fill missing basement features\n\nfor feature in basement_features:\n    if combine[feature].dtype == 'object':\n        combine[feature] = combine[feature].fillna('None')\n    else:\n        combine[feature] = combine[feature].fillna(0)","8def4700":"# Get missing features that are related to masonry veneer\n\nmv_features = [feature for feature in missing_features if 'MasVnr' in feature]\nmv_features","df21cb70":"# Check data types of masonry veneer features\n\ncombine[mv_features].dtypes","e8cc689f":"for feature in mv_features:\n    if combine[feature].dtype == 'object':\n        combine[feature] = combine[feature].fillna('None')\n    else:\n        combine[feature] = combine[feature].fillna(0)","3a5b9380":"# Fill features with None\n\nother_features = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\nfor feature in other_features:\n    combine[feature] = combine[feature].fillna('None')","b2b51ed4":"missing = combine.isnull().sum()\nmissing = missing[missing > 0]\nmissing = missing.drop('SalePrice')\nmissing.sort_values(ascending = False, inplace = True)\nmissing","ec7b4740":"# Check data types of the remaining missing features\n\nmissing_features = list(missing.index)\ncombine[missing_features].dtypes","122b9341":"# LotFrontage\ncombine['LotFrontage'] = combine.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# MSZoning\ncombine['MSZoning'] = combine.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n# Remaining features\nmissing_features.remove('LotFrontage')\nmissing_features.remove('MSZoning')\nfor feature in missing_features:\n    combine[feature] = combine[feature].fillna(combine[feature].mode()[0])","33206561":"# Make sure there is no more missing data\n\ncombine.drop('SalePrice', axis = 1).isnull().sum().max()","fb49e6ed":"# Split training and test set from combined dataframe\n\ntrain = combine[:len(train)]\ntest = combine[len(train):]","bfb32120":"# Drop sale price from test set\n\ntest = test.drop('SalePrice', axis = 1)","aecfa585":"print(\"Training set shape: \", train.shape)\nprint(\"Test set shape: \", test.shape)","000c7b70":"# Get numerical features from training set \nnumerical_features = [feature for feature in train if train[feature].dtype != 'object']\n\n# Remove Id and SalePrice\nnumerical_features.remove('Id')\nnumerical_features.remove('SalePrice')","79214caf":"numerical_features","e349ddd2":"# Detect outliers using Tukey method\n\ndef detect_outliers(df, n, features):\n    outlier_indices = [] \n    for col in features: \n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col], 75)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR \n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step)].index\n        outlier_indices.extend(outlier_list_col) \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(key for key, value in outlier_indices.items() if value > n) \n    return multiple_outliers\n\noutliers_to_drop = detect_outliers(train, 6, numerical_features)\nprint(f\"We are dropping {len(outliers_to_drop)} observations.\")","2c478247":"train.iloc[outliers_to_drop]","b1171a9f":"# Drop outliers and reset index\n\nprint(f\"Before: {len(train)} rows\")\ntrain = train.drop(outliers_to_drop).reset_index(drop = True)\nprint(f\"After: {len(train)} rows\")","5173078f":"# Sale price against GrLivArea\n\nsns.set_style('darkgrid')\nsns.scatterplot(x = 'GrLivArea', y = 'SalePrice', data = train)\nplt.title('Sale Price Against GrLivArea')","c0fa0ebf":"# Sale price against TotalBsmtSF\n\nsns.set_style('darkgrid')\nsns.scatterplot(x = 'TotalBsmtSF', y = 'SalePrice', data = train)\nplt.title('Sale Price Against TotalBsmtSF')","eaa5531f":"# Create axes to draw plots\nfig, ax = plt.subplots(1, 2)\n\n# Plot original SalePrice distribution\nsns.distplot(train['SalePrice'], fit = norm, label = 'Skewness: %.2f'%train['SalePrice'].skew(), ax = ax[0])\nax[0].legend(loc = 'best')\nax[0].title.set_text('Sale Price Before Transformation')\n\n# Apply box-cox transformation\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# Plot transformed SalePrice distribution\nsns.distplot(train['SalePrice'], fit = norm, label = 'Skewness: %.2f'%train['SalePrice'].skew(), ax = ax[1])\nax[1].legend(loc = 'best')\nax[1].title.set_text('Sale Price After Transformation')\n\n# Rescaling the subplots\nfig.set_figheight(5)\nfig.set_figwidth(15)","026a8c54":"# Combine training and test set \n\ncombine = pd.concat([train, test])","f37c7b92":"print(\"Training set shape: \", train.shape)\nprint(\"Test set shape: \", test.shape)\nprint(\"Combined shape: \", combine.shape)","159302f6":"skew_features = combine[numerical_features].apply(lambda x: x.skew()).sort_values(ascending = False)\nhigh_skew = skew_features[skew_features > 0.5]\nprint(f\"There are {len(high_skew)} numerical features with skew greater than 0.5. \")\nhigh_skew","bc6ab166":"# Normalise skewed features\n\nfor feature in high_skew.index:\n    combine[feature] = boxcox1p(combine[feature], boxcox_normmax(combine[feature] + 1))","ccea8c46":"# Let's have a look at our existing features\n\ncombine.columns","2c888287":"print(f\"Before: {combine.shape[1]} columns\")\n\ncombine['UnfBsmt'] = (combine['BsmtFinType1'] == 'Unf') * 1\ncombine['HasWoodDeck'] = (combine['WoodDeckSF'] == 0) * 1\ncombine['HasOpenPorch'] = (combine['OpenPorchSF'] == 0) * 1\ncombine['HasEnclosedPorch'] = (combine['EnclosedPorch'] == 0) * 1\ncombine['Has3SsnPorch'] = (combine['3SsnPorch'] == 0) * 1\ncombine['HasScreenPorch'] = (combine['ScreenPorch'] == 0) * 1\ncombine['YearsSinceRemodel'] = combine['YrSold'].astype(int) - combine['YearRemodAdd'].astype(int)\ncombine['TotalHomeQuality'] = combine['OverallQual'] + combine['OverallCond']\ncombine['TotalSF'] = combine['TotalBsmtSF'] + combine['1stFlrSF'] + combine['2ndFlrSF']\ncombine['YearBuiltAndRemodel'] = combine['YearBuilt'] + combine['YearRemodAdd']\ncombine['TotalBathrooms'] = combine['FullBath'] + combine['BsmtFullBath'] + 0.5 * (combine['HalfBath'] + combine['BsmtHalfBath'])\ncombine['TotalPorchSF'] = combine['OpenPorchSF'] + combine['3SsnPorch'] + combine['EnclosedPorch'] + combine['ScreenPorch'] + combine['WoodDeckSF']\ncombine['HasPool'] = combine['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ncombine['Has2ndFloor'] = combine['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ncombine['HasGarage'] = combine['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ncombine['HasBsmt'] = combine['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ncombine['HasFireplace'] = combine['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\nprint(f\"After: {combine.shape[1]} columns\")","cdf862d0":"print(\"Before: \", combine.shape)\ncombine = pd.get_dummies(combine)\nprint(\"After: \", combine.shape)","32e73da7":"train = combine[:len(train)]\ntest = combine[len(train):]","18006d5c":"X_train = train.drop(['Id', 'SalePrice'], axis = 1)\nY_train = train['SalePrice']\nX_test = test.drop(['Id', 'SalePrice'], axis = 1)\ntest_id = test['Id']","79d84ce9":"print(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"X_test shape: \", X_test.shape)","bca4f297":"# Cross validation\n\nkfolds = KFold(n_splits = 10, shuffle = True, random_state = 42)","82111732":"# Evaluation metrics\n\ndef cv_rmse(model, X = X_train, y = Y_train):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring = 'neg_mean_squared_error', cv = kfolds))\n    return rmse","624b5d80":"# Instantiate regressors\n\nridge = make_pipeline(RobustScaler(), RidgeCV())\nlasso = make_pipeline(RobustScaler(), LassoCV())\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV())\nrf = RandomForestRegressor()\nsvr = SVR()\ngbr = GradientBoostingRegressor()\nlightgbm = LGBMRegressor()\nxgboost = XGBRegressor()","1bba736f":"models = [ridge, lasso, elasticnet, rf, svr, gbr, lightgbm, xgboost]\nmean = []\nstd = []\nfor model in models:\n    mean.append(cv_rmse(model).mean())\n    std.append(cv_rmse(model).std())\n\nmodels_df = pd.DataFrame({'Model': ['Ridge', 'Lasso', 'Elastic Net', 'Random Forest', 'SVR', 'Gradient Boosting', 'Light GBM', 'XGBoost'],\n                         'Mean': mean, 'Std': std})\nmodels_df.sort_values(by = 'Mean', inplace = True, ignore_index = True)","3b71d883":"models_df","4b6fdf37":"ridge_model = ridge.fit(X_train, Y_train)\nY_pred = ridge_model.predict(X_test)\nlen(Y_pred)","47227819":"ss.shape","1199c876":"ss.head()","c7038299":"ridge_submission = pd.DataFrame({'Id': test_id, 'SalePrice': np.expm1(Y_pred)})\nridge_submission.head()","27d37039":"Houses with 3 garage cars have the highest sale price. ","9fd238f3":"## 4.1 Impute missing values","acb5a2d2":"I am creating 17 new features as follows:\n\n- UnfBsmt\n- HasWoodDeck\n- HasOpenPorch\n- HasEnclosedPorch\n- Has3SsnPorch\n- HasScreenPorch\n- YearsSinceRemodel\n- TotalHomeQuality\n- TotalSF\n- YearBuiltAndRemodel\n- TotalBathrooms\n- TotalPorchSF\n- HasPool\n- Has2ndFloor\n- HasGarage\n- HasBsmt\n- HasFireplace","0d75e5bb":"We observe that many of the top 10 features that most correlated with sale price have correlation with each other.\n\nTherefore, in the event of a correlated pair, I will choose to examine the feature that has a higher correlation with sale price. In other words:\n\n- GrLivArea\n- GarageCars\n- TotalBsmtSF\n- YearBuilt","a38f7b38":"Outliers that we have detected previously during exploratory data analysis have been removed.","9ed5e82e":"## 3.3.9 Mega scatter plot","a8bdda3c":"## 3.3.6 Numerical variable: TotalBsmtSF","5c7a75e9":"Sale price increases with overall quality but we don't know how overall quality was calculated.","561409d5":"## 4.4 Apply Box-Cox transformation to numerical features with high skewness ","be146d1f":"Here is the game plan for the remaining features:\n\n- Group the houses by Neighborhood and fill the missing values in LotFrontage with the median LotFrontage according to Neighborhood\n- Group the houses by MSSubClass and fill the missing values in MSZoning with the most frequent (mode) MSZoning\n- Since we do not have any information on the remaining categorical features, simply fill with mode of that column","3af26635":"Let's first deal with features that are related to:\n- Garage\n- Basement\n- Masonry veneer\n\nI will make an underlying assumption that the reason behind these missing values is because some houses simply do not have those features.","db9df507":"## 3.3.3 Numerical variable: OverallQual","9cc122bd":"## 4.2.1 Make sure outliers have been removed from GrLivArea and TotalBsmtSF","389af92c":"# 0. Introduction\n\nIn this notebook, I will go through my solution and analysis of the [Kaggle house prices prediction competition](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/overview). The aim of this competition is to analyse 79 different features that describe every aspect of the residential homes in Ames, Iowa and subsequently make predictions on the final sale price of each home.\n\nThe key practice skills in this competition are:\n- Creative feature engineering\n- Advanced regression techniques like random forest and gradient boosting\n\nI have made references to the following notebooks in the making of this notebook:\n- [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by [Pedro Marcelino, PhD](https:\/\/www.kaggle.com\/pmarcelino)\n- [#1 House Prices Solution [top 1%]](https:\/\/www.kaggle.com\/jesucristo\/1-house-prices-solution-top-1) by [Nanashi](https:\/\/www.kaggle.com\/jesucristo)\n- [How I made top 0.3% on a Kaggle competition](https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition#EDA) by [Lavanya Shukla](https:\/\/www.kaggle.com\/lavanyashukla01)\n- [Stacked Regressions: Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by [Serigne](https:\/\/www.kaggle.com\/serigne)","11b0d1d1":"## 4.2 Detect and remove outliers","d4ac673d":"We have 3 numerical garage features and 4 categorical garage features.\n\nI will fill the numerical garage features with 0 and the categorcial garage features with None.","28d0c95e":"Let's check our missing values again.","40ec293c":"## 4.1.4 Other missing values","ba86bedd":"We observe two outliers in the scatter plot with large GrLivArea but low sale price. We need to address this later on by removing them from the training set.","a00c4517":"## 3.3 Feature analysis\n\nFeature analysis, as the name suggests, is analysing features or columns in the dataset and see how they correlate with our response variable that is the final sale price for each home.","333f4e5e":"# 6. Conclusion\n\nMy predictions resulted in a submission score of 0.12550 when submitted to Kaggle.\n\nIf you found any mistakes in the notebook or places where I can potentially improve on, feel free to provide me with some feedback. Happy learning!\n\nMy platforms:\n\n- [Facebook](https:\/\/www.facebook.com\/chongjason914)\n- [Instagram](https:\/\/www.instagram.com\/chongjason914)\n- [Twitter](https:\/\/www.twitter.com\/chongjason914)\n- [LinkedIn](https:\/\/www.linkedin.com\/in\/chongjason914)\n- [YouTube](https:\/\/www.youtube.com\/channel\/UCQXiCnjatxiAKgWjoUlM-Xg?view_as=subscriber)\n- [Medium](https:\/\/www.medium.com\/@chongjason)","7d6147b8":"## 3.3.2 Correlation between numerical variables","c4cb5493":"After checking the data description on the Kaggle competition page, I have decided to fill the missing values in the following columns with None:\n\n- PoolQC\n- MiscFeature\n- Alley\n- Fence\n- FireplaceQu\n\nAgain, the assumption here is that some houses do not have these features.","ae75ec81":"There are 43 text variables and 38 numerical variables in the training set including Id and SalePrice.","250b08e1":"## 5.1 Get the new training and test set","ae48a8b2":"# 4. Data preprocessing\n\nData preprocessing is the process behind getting our datasets ready for model training. In this section, I will be performing the following preprocessing tasks:\n\n- Impute missing values\n- Detect and remove outliers\n- Apply log transformation to sale price\n- Apply Box-Cox transformation to features with high skewness\n- Feature engineering\n- Encode categorical variables","1e38394d":"## 3.3.4 Numerical variable: GrLivArea","64e2f1d6":"## 3.2 Check data types","dd35710d":"## 4.3 Apply log transformation to sale price","7bc4cb5a":"## 4.1.2 Missing values in basement","3a5b2b98":"Since Ridge has the lowest root mean squared error, I will use this model to predict the data in the test set and save a copy of the predictions.","4a01cd10":"# 1. Import libraries\n\nThese are the libraries that I will be using to perform my analysis and modelling. I have grouped them according to their functionalities.","5265aa2d":"Again, fill numerical basement features with 0 and categorical with None.","22021a90":"## 5.3 Model evaluation\n\nI will be using the following regressors for modelling:\n\n- Ridge\n- Lasso\n- Elastic net\n- Random forest\n- Support vector regressor\n- Gradient boosting\n- Light GBM\n- XGBoost","2fff2268":"## 3.3.7 Numerical variable: FullBath","13e0d2b3":"## 5.2 Define cross-validation strategy and evaluation metrics","623bf8c9":"## 4.1.3 Missing values in masonry veneer","6563f9f0":"Just by eyeballing the heatmap, we can observe signs of multicollinearity between some features. \n\nMulticollinearity can be observed in the following features:\n\n- TotalBsmtSF and 1stFlrSF\n- GarageCars and GarageArea\n- GarageYrBlt and YearBuilt\n- TotRmsAbvGrd and GrLivArea","94083a39":"Let's have a look at the datasets.","686553ca":"## 5.4 Make predictions on test data using Ridge","594d4f2d":"## 3.1 Descriptive statistics","65b43591":"Another outlier! The data point on the far right has a large TotalBsmtSF but a low sale price.","1ff1d7de":"## 3.4 Examine missing data","6da5b3ec":"# 2. Import and read data\n\nImport traning set, test set and sample submission.","694badc9":"House prices are trending upwards in general.","502ae50e":"## 3.3.5 Numerical variable: GarageCars","d28b171b":"## 3.3.8 Numerical variable: YearBuilt","f760129d":"## 4.1.1 Missing values in garage","9c1e80d0":"## 4.6 Encode categorical features\n\nAssign numerical values to categorical variables because most models can only handle numerical features.","d47d373a":"# 5. Modelling\n\nIn this section, I will fit 8 different machine learning models to the training set and use the best performing model to make predictions on the test set. The best performing or most accurate model is defined as the model with the lowest root mean squared error during cross-validation.","d1d8dc59":"## 3.3.1 Response (target) variable","acf663e6":"## 4.5 Feature engineering\n\nFeature engineering entails creating new features from existing features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.","75c42c49":"# 3. Exploratory data analysis (EDA)\n\nExploratory data analysis involves analysing datasets in order to summarise their main characteristics. In this section, I will perform the following analysis:\n\n- Descriptive statistics\n- Check data types\n- Feature analysis\n- Examine missing data"}}