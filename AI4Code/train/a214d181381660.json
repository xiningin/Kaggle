{"cell_type":{"b88ede5d":"code","558214b9":"code","e52fcd07":"code","35f7756c":"code","85482300":"code","1edf9fa0":"code","b7a80cf0":"code","9491637d":"code","397e7a36":"code","bf9ceeed":"code","1f5cb318":"code","613869b4":"code","1cac5931":"code","be96005a":"code","f652e68e":"code","1678de8c":"code","e1f7793f":"code","1ccfc6f6":"code","87f7366f":"markdown","54993a34":"markdown","e5f0f241":"markdown","4a81ab6c":"markdown","7222c8c5":"markdown","dde4e14e":"markdown","ff9f8971":"markdown","beec1d21":"markdown","1a1af838":"markdown","5bec9612":"markdown","1f917c27":"markdown","fb24c40b":"markdown"},"source":{"b88ede5d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","558214b9":"data = pd.read_csv('..\/input\/autompg-dataset\/auto-mpg.csv')\ndata.head()","e52fcd07":"#Describe the dataset\ndata.describe(include = 'all').transpose()","35f7756c":"#Car name column can be dropped as this column cannot be fit into model\ndata.drop('car name', axis = 1, inplace = True)\n\n#Origin column can be renamed based on metadata available\ndata['origin'] = data['origin'].replace({1:'america',2:'europe',3:'asia'})\n\n#Convert all categorical variable in Origin into binary column -> Using One Hot Encoding\ndata = pd.get_dummies(data, columns = ['origin'])\n\ndata.head()","85482300":"#Horsepower column should only contain digits but here it contains text too, so first we'll check how many rows have text in them\n\ntemp = pd.DataFrame(data['horsepower'].str.isdigit())\ntemp[temp['horsepower'] == False]","1edf9fa0":"data.iloc[126]","b7a80cf0":"#Replace '?' with NaN\ndata = data.replace('?', np.nan)\ndata[data.isnull().any(axis = 1)]","9491637d":"data.median()","397e7a36":"#instead of dropping the rows, lets replace the missing values with median value.\ndata = data.apply(lambda x: x.fillna(x.median()), axis = 0)\ndata.info()","bf9ceeed":"#horsepower column should have it's datatype changed from object to float\ndata['horsepower'] = data['horsepower'].astype('float64')\ndata.head()","1f5cb318":"sns.pairplot(data, diag_kind = 'kde') #kde helps to plot kernel density estimation instead of default histogram","613869b4":"plt.figure(figsize = (10,8))\nsns.heatmap(data.corr(), annot = True)","1cac5931":"#Prepare X, y with 'mpg' column being the target column\n\nX = data.drop('mpg', axis = 1)\ny = data[['mpg']]","be96005a":"#Let us break the X and y dataframes into training set and test set. For this we will use\n#Sklearn package's data splitting function which is based on random function\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 1)\n\n# Invoke LinearRegression function to find the best fit model on training data\nfrom sklearn.linear_model import LinearRegression\nreg_model = LinearRegression()\nreg_model.fit(X_train,y_train)","f652e68e":"# Let us explore the coefficients for each of the independent attributes\n\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, reg_model.coef_[0][idx]))\n    \n# Model intercept\nintercept = reg_model.intercept_[0]\nprint(\"The intercept for our model is {}\".format(intercept))","1678de8c":"# Model score - R2 or coeff of determinant\n# R^2=1\u2013RSS \/ TSS\n\nreg_model.score(X_test, y_test)","e1f7793f":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn import linear_model\n\npoly = PolynomialFeatures(degree=2, interaction_only=True)\nX_train_p = poly.fit_transform(X_train)\n\nX_test_p = poly.fit_transform(X_test)\n\npoly_clf = linear_model.LinearRegression()\n\npoly_clf.fit(X_train_p, y_train)\n\ny_pred = poly_clf.predict(X_test_p)\n\n#print(y_pred)\n\nprint(poly_clf.score(X_test_p, y_test))\n","1ccfc6f6":"#Shape of orginal X, X_train & X_train_p\n\nprint(X.shape)\nprint(X_train.shape)\nprint(X_train_p.shape)","87f7366f":"Accuracy has improved from 0.84 to 0.86 which is good and it has further scope of improvement that can be achieved by Scaling.","54993a34":"For a Linear Regression, the 1st step is to find Pearson's coefficient 'r', it has value ranging between -1 (Negative correlation) to +1 (Positive correlation) with 0 meaning no linear correlation between X and y. \n\n***r -> Which feature to be used***\n\n![image.png](attachment:image.png)\n\n**Heatmaps** are great for making readily apparent trends particularly when the data is ordered and there is clustering. They clearly indicate 'r' values thus representing the correlation. \n\n*For more info -> [Heatmap](http:\/\/alanpryorjr.com\/visualizations\/seaborn\/heatmap\/heatmap\/)*","e5f0f241":"An R2 of 0.84 indicates it is a good model however in real world, we generally use adjusted R2 = R2 - Fluke correlation for better accuracy.\n\n# Advantages of Linear Regression\n* Simple to implement, easier to interpret output coefficient\n\n# Disadvantages\n* Assumes there's always a linear relationship between X, y\n* Outliers can have huge effects on Regression\n* Assumes all X columns are independent\n* Linear regression model can be pulled to leverage point due to Outliers\n* Looks at relationship between mean of dependent and independent variable\n* For linear classification in linear models linear surfaces are used which may not be optimal solution\n\nFor further improving model accuracy we can convert this Linear equation into a Polynomial equation.","4a81ab6c":"**EDA Inferences:**\n\n* 3 gaussian distributions are clearly visible in this dataset\n* Mpg together with diplacement, horsepower & weight show negative correlation but it is not really linear\n* Mpg shares a positive correlation with acceleration & year which is surprising\n* relation between 'mpg' and 'hp' show hetroscedacity -> doesn't show constant variance this will impact model accuracy\n* Heatmaps help to quickly identify the features that could have an impact on target column","7222c8c5":"On inspecting rows 32, 126, 330 we see '?' has inadvertently crept into horsepower column, we first replace this NaN and later with median values of horsepower column.","dde4e14e":"# Using Polynomial Features","ff9f8971":"**Coefficient of Determinant 'R2' R Squared**\n\nDetermines model fitness, an R2 value tending towards 1 is a better model.\n\n***R2 -> Whether model is good or not***","beec1d21":"# Linear Regression Model Building\n\n* This model generates equation of line\/ plane\/ hyperplane. \n* Linear regression is useful for predicting numerical value or running numbers\n* This model can be used for linear regression and classification \n\n\n\n\n\n\n**Things to do**\n* Prepare X, y with 'mpg' column being the target column\n* Using train test split on X, y columns\n* Invoke LinearRegression function to find the best fit model on training data\n* Explore the coefficients for each of these independent attributes\n","1a1af838":"# Explore Data distribution using Pairplot & Heatmap\n\nPairplots are the best way for one to understand the data well, it gives you a neat understanding of correlation shared by each of these features.\n\n*For more info -> [Pairplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html)*","5bec9612":"We can infer the shape changes in X columns and how using Polynomial features of a degree 2, column nos have increased from 9 to 46. \n\n***Linear Regression is the first step while learning statistics and the fundamentals of Machine Learning. You get your base very strong, that will eventually lead you to understand more complex types of Linear Regression like Gradient Descent & Regularization faster.***","1f917c27":"# Data Preparation\n\n**Things to do:**\n* Describe the dataset\n* Check for null values\n* Check for columns which have messy data\n* Car name column can be dropped as this column cannot be fit into model\n* Origin column can be renamed based on metadata available\n* Convert all categorical variable in Origin into binary column -> Using One Hot Encoding\n* Horsepower has text in some cells -> correct it","fb24c40b":"**AI -> ML -> Shallow Learning -> Supervized Learning -> Linear Regression.**\n\n# Linear Regression \n\nLinear regression is a statistical method to find the relationship between dependent and one or more independent variables. Regression analysis constitutes an important part of a statistics to explore and model relationship between variables.\n\nIt helps to understand which features affect target variable and by how much. This program would understand the relationship shared between features and based on which it would predict a target variable.\n\n**We all know equation of a Line:**\n\n![image.png](attachment:image.png)\n\nLinear regression is built off this line.\n\n**Types of Linear Regression:**\n* Simple Linear Regression\n* Ordinary Least Squares (OLS)\n* Gradient Descent\n* Regularization\n    * Lasso\n    * Ridge\n\n*For more info -> [Wiki](https:\/\/en.wikipedia.org\/wiki\/Linear_regression)*\n\nIn this dataset, we would look at various features of a car and predict miles per gallon for a vehicle with the features shared. \n\n# Import packages and dataset"}}