{"cell_type":{"95bb191f":"code","1e2e47b4":"code","3f14b801":"code","2fc540c6":"code","5523ffe2":"code","f89ef8ba":"code","fa3cfba8":"code","7350073a":"code","87b04758":"code","e58b76be":"code","e5bed454":"code","3fd1d2f8":"code","dabbbd14":"code","2d78406b":"code","0bf04982":"code","c8506b99":"code","dcda9943":"code","19ff5197":"code","853ef756":"code","e2cf86b3":"code","32d706d9":"code","aaa6b5af":"code","be796d79":"code","5aea419e":"code","e12c61c7":"code","f33a767b":"code","0ce9205e":"code","9b49a773":"code","86719018":"code","ba6c72c0":"code","157feebd":"code","01c2cbbb":"code","0cb988b3":"code","a993c177":"code","440f3582":"code","5d81132f":"markdown","33d4d644":"markdown","26b5c8c4":"markdown","86fb3c49":"markdown","4192d670":"markdown"},"source":{"95bb191f":"import pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt","1e2e47b4":"df = pd.concat(map(pd.read_csv, ['\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv', '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv','\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/hyundi.csv', '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv', '\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv','\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv','\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vauxhall.csv','\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/ford.csv','\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv']))\n","3f14b801":"df","2fc540c6":"df.info()","5523ffe2":"sns.heatmap(df.isnull())","f89ef8ba":"df.tax=df.tax.fillna(df[['tax(\u00a3)']].max(1))","fa3cfba8":"sns.heatmap(df.isnull())","7350073a":"df.drop(['tax(\u00a3)'], axis=1,inplace=True)","87b04758":"df.head()","e58b76be":"df.isnull().sum()","e5bed454":"df.shape","3fd1d2f8":"df.info()","dabbbd14":"df.corr()","2d78406b":"sns.heatmap(df.corr(),annot=True)","0bf04982":"fig = plt.figure(figsize=(18,6))\nfig.add_subplot(1,2,1)\nsns.countplot(df['transmission'])\nfig.add_subplot(1,2,2)\nsns.countplot(df['fuelType'])","c8506b99":"sns.catplot(x = 'year', y= 'price', data = df, kind='point', aspect=4);","dcda9943":"sns.relplot(x=\"price\", y=\"transmission\", \n            data=df);","19ff5197":"sns.relplot(x=\"year\", y=\"price\", \n            data=df);","853ef756":"num_cols = df.select_dtypes(exclude=['object'])\n\nfig = plt.figure(figsize=(20,8))\n\nfor col in range(len(num_cols.columns)):\n    fig.add_subplot(2,4,col+1)\n    sns.distplot(num_cols.iloc[:,col], hist=False, rug=True, kde_kws={'bw':0.1}, label='UV')\n    plt.xlabel(num_cols.columns[col])\n\nplt.tight_layout()","e2cf86b3":"fig = plt.figure(figsize=(20,8))\n\nfor col in range(len(num_cols.columns)):\n    fig.add_subplot(2,4,col+1)\n    sns.scatterplot(x=num_cols.iloc[:,col], y=df['price'], label='MV')\n    plt.xlabel(num_cols.columns[col])\n\nplt.tight_layout()","32d706d9":"cars","aaa6b5af":"X = cars.drop(['price'], axis=1)\ny = cars['price']","be796d79":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom math import sqrt\nfrom sklearn.linear_model import LinearRegression, BayesianRidge\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import Ridge,ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import Lasso","5aea419e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25) ","e12c61c7":"log_clf=LinearRegression()\nbye_clf=BayesianRidge()\nrnd_clf = RandomForestRegressor()\nrid_clf = Ridge(alpha=2,max_iter=1000,random_state=1)\nele_clf = ElasticNet()\ngbr_clf=GradientBoostingRegressor()\nlss_clf=Lasso()","f33a767b":"voting_clf = VotingRegressor([('lr', log_clf),('bye', bye_clf), ('rf', rnd_clf), ('rnd', rnd_clf), ('ele', ele_clf), ('gbr', gbr_clf), ('lss', lss_clf)])\nvoting_clf.fit(X_train, y_train)","0ce9205e":"for clf in (log_clf, rnd_clf, rid_clf, bye_clf, voting_clf,ele_clf, gbr_clf,lss_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, 'r2_score', r2_score(y_test, y_pred))","9b49a773":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)","86719018":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.20, random_state=7)","ba6c72c0":"voting_clf2 = VotingRegressor([('lr', log_clf),('bye', bye_clf), ('rf', rnd_clf), ('rnd', rnd_clf), ('ele', ele_clf), ('gbr', gbr_clf),('lss', lss_clf)])\nvoting_clf.fit(X_train, y_train)","157feebd":"for clf in (log_clf, rnd_clf, rid_clf, bye_clf,voting_clf2,ele_clf, gbr_clf,lss_clf):\n    clf.fit(X_train, y_train)\n    y_pred2 = clf.predict(X_test)\n    print(clf.__class__.__name__, 'r2_score', r2_score(y_test, y_pred2))","01c2cbbb":"scaler2 = StandardScaler()\nX_scaled2 = scaler.fit_transform(X)","0cb988b3":"X_train, X_test, y_train, y_test = train_test_split(X_scaled2, y, test_size=0.15, random_state=7)","a993c177":"voting_clf3 = VotingRegressor([('lr', log_clf), ('rf', rnd_clf),('bye',bye_clf), ('rnd', rnd_clf), ('ele', ele_clf), ('gbr', gbr_clf),('lss', lss_clf)])\nvoting_clf.fit(X_train, y_train)","440f3582":"for clf in (log_clf, rnd_clf, rid_clf, voting_clf2,ele_clf, gbr_clf,lss_clf):\n    clf.fit(X_train, y_train)\n    y_pred3 = clf.predict(X_test)\n    print(clf.__class__.__name__, 'r2_score', r2_score(y_test, y_pred3))","5d81132f":"# ***The result so far was without scaliing the data as it generated the best result with the RandomForestRegressor r2_score 0.964***","33d4d644":"We dont need the tax(\u00a3) again so we have to drop it ","26b5c8c4":"##Feature Engineering The categorical variables must be converted into something numerical. either one hot encording from sklearn or using get dummies function from pandas","86fb3c49":"MinMaxScaler reduced the accuracy of RandomForestRegressor which was the best model to be used in this data it reduced the r2_score from  0.964 to 0.954 but it improved the Ridge r2_score to  0.8607171252232906 and also BayesianRidge r2_score 0.8635266838140631","4192d670":"the missing value in the tax column was in the tax(\u00a3) column. I have to drop replace the null values in tax with the values in tax(\u00a3). "}}