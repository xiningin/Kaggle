{"cell_type":{"5d797d48":"code","ec2c0c33":"code","68065732":"code","4f9a4685":"code","98f82b88":"code","986261ef":"code","adb0f9ec":"code","63d1b483":"code","d53c59ad":"code","622d53ea":"code","8ff06ccd":"code","d8616178":"code","ae78558d":"code","7016c57a":"code","e85866da":"code","4a64c63a":"code","a5ddeedc":"code","7c06915f":"code","93b8b591":"code","08a4346c":"code","773673e0":"code","42fa6759":"markdown","1d25c07b":"markdown"},"source":{"5d797d48":"import numpy as np\nimport pandas as pd\n\n#--------------------------------------\n#\n#    Read manually preprocessed data\n#\n#--------------------------------------\n\ndef saveDataFrame(path, df):\n    print(\"Saving CSV:\", path)\n    df.to_csv(path, encoding='utf-8-sig', index=False, sep=\";\")\n\ndef readDataFrame(path):\n    print(\"Reading CSV:\", path)\n    return pd.read_csv(path, encoding='utf-8-sig', sep=\";\")  \n\nraw_data = readDataFrame(r'..\/input\/ekr-docs\/ekr_135_full_manually.csv')\n\n\nprint(raw_data.describe())\nprint('---------------------------------------')\nprint(raw_data['ocr_text'][0][:500])","ec2c0c33":"#--------------------------------------\n#\n#    Convert sequences to NER format: [(tag, token), (tag, token), ...]\n#\n#--------------------------------------\n\nO_tag = 'O'\n\nclass SequenceTagger():\n\n    \"\"\"\n        Lowercarse and remove redundant whitespaces\n    \"\"\"\n    def clean(self, text): \n        return ' '.join(str(text).lower().split())\n    \n    \n    \"\"\"\n        - Convert text and its tags to a list of tag-value pairs:\n          'text ...', {'tag': 'value', ...}   =>   [('O', some), ('xtag', text), ('ytag', value), ...]\n        - Text should contains tag_values exacatly --- TODO: use Levenshtein distance insted of exact match!\n        - Calculate O tag values\n        - Also tokenize text by whitespaces\n    \"\"\"\n    def tag_sequence(self, text, tag_values, unk_value='-'):\n        \n        text = self.clean(text)\n        \n        # calculate positions (charcter indexes) of tags  from tex(tag, start, end)\n        tag_indexes = []\n        for tag, value in tag_values.items():\n            if value != unk_value:\n                value = self.clean(value)\n                start = text.find(value)\n                if start != -1:\n                    tag_indexes.append((tag, start, start+len(value)))\n                else:\n                    print('WARNING: no tag \"'+tag+'\" found with value \"'+value+'\"')\n        \n        # sort tags by appearence in text\n        tag_indexes.sort(key=lambda val: val[1])\n        \n        tag_value_results = []\n        # helper: add tag to results if its value is not empty\n        def append(tag, value):\n            if value:\n                tag_value_results.append((tag, value))\n                \n        # create tag-value pairs by extracting values from text with indexes\n        for i in range(len(tag_indexes)):\n            \n            tag, start, end = tag_indexes[i]\n            \n            # add O text before the first tag\n            if i == 0:\n                append(O_tag, text[0 : start])\n                \n            # add current tag\n            append(tag, text[start : end])\n                \n            # add O text between current and next tag (or after the last tag)\n            if i == len(tag_indexes)-1:\n                next_start = None\n            else:\n                _, next_start, _ = tag_indexes[i+1]\n            append(O_tag, text[end : next_start])\n            \n        return tag_value_results #, tag_value_results # return with whole sequences\n\n\n    def tag(self, df, text_col, cols2tags):\n        def apply_seq_tagging(df_row):\n            tag_values = { tag: df_row[col] for col, tag in cols2tags.items() }\n            return self.tag_sequence(df_row[text_col], tag_values)\n        \n        return df.apply(apply_seq_tagging, axis=1).values\n    \n# todo: j\u00f3pofa, de f\u00f6l\u00f6sleges lemappelni\ncols2tags = {\n    'Aj\u00e1nlatk\u00e9r\u0151 ad\u00f3sz\u00e1ma': 'A_ADO', \n    'Vezet\u0151 aj\u00e1nlattev\u0151 ad\u00f3sz\u00e1ma': 'V_ADO',\n    'Aj\u00e1nlatk\u00e9r\u0151 c\u00edm': 'A_CIM', \n    'Vezet\u0151 aj\u00e1nlattev\u0151 c\u00edm': 'V_CIM', \n    'Aj\u00e1nlatk\u00e9r\u0151 megnevez\u00e9se': 'A_NEV',\n    'Vezet\u0151 aj\u00e1nlattev\u0151 megnevez\u00e9se': 'V_NEV',\n    'Szerz\u0151d\u00f6tt brutt\u00f3 ellen\u00e9rt\u00e9k (\u00f6sszeg\/keret\u00f6sszeg)': 'BR_AR',\n    'Szerz\u0151d\u00f6tt nett\u00f3 ellen\u00e9rt\u00e9k (\u00f6sszeg\/keret\u00f6sszeg)': 'N_AR'\n}\n    \nsequence_tagger = SequenceTagger()\ntagged_docs = sequence_tagger.tag(raw_data, 'ocr_text', cols2tags)\n\n\nprint(tagged_docs[0][0:5]) # first doc first 5 tags","68065732":"import re\n\nclass PreservingTokenizer():\n    \n    \"\"\"\n        Tokenize text values of a doc while preservering position informations\n        - [(tag, value), ...] => [(token, token_index, tag), ...]\n        - May split words to subwords => token_index contains the original word index\n    \"\"\"\n    def tokenize_doc(self, doc):\n        tokenized_doc = []\n        word_counter = 0\n        for tag, value in doc:\n            words = value.split()\n            # making every a-Z and 1-9 standalone\n            subwords = [(subword, word_counter+wi) for wi, word in enumerate(words) \\\n                                                    for subword in re.sub(r'([^\\w])', r' \\1 ', word).split()]\n                \n            if tag == O_tag:\n                for subword, wi in subwords:\n                    tokenized_doc.append((subword, wi, tag))    \n            else:\n                subword, wi = subwords[0]\n                tokenized_doc.append((subword, wi, 'B-'+tag))\n                for subword, wi in subwords[1:]:\n                    tokenized_doc.append((subword, wi, 'I-'+tag))\n                    \n            word_counter += len(words)\n                    \n        return tokenized_doc\n        \n        \n    def tokenize_docs(self, docs):\n        return [self.tokenize_doc(doc) for doc in docs]\n    \n    \"\"\"\n        Detokenize (join) text values of a doc while preservering position informations\n        - [(token, token_index, tag), ...] => [(tag, value), ...]\n        - Concatenate splitted words by token_index\n        - Handle prediction error for in-tag cases, e.g.: B I O I => B I I I\n    \"\"\"\n    def detokenize_doc(self, doc, max_error_distance=2):  # doc: [(subword, wi, tag), ...] # todo: handle removed token\n        tag_values = []\n        last_index = -1\n        for i, item in enumerate(doc):\n            subword, wi, tag = item\n            tag = tag if tag == O_tag else tag[2:]\n            \n            if not tag_values:\n                tag_values.append((tag, subword))\n            else:\n                last_tag, last_value = tag_values[-1]\n                sep = '' if last_index == wi else ' '\n                if last_tag == tag or \\\n                (   # correct tag if the following tag is the same as last\n                    last_tag != O_tag and \\\n                    any([last_tag == next_tag for _, _, next_tag in doc[i:min(i+max_error_distance, len(doc))]])\n                ):\n                    tag_values[-1] = (last_tag, last_value +sep+ subword)\n                else:\n                    tag_values.append((tag, subword))\n                \n            last_index = wi\n            \n        return tag_values\n    \n    def detokenize_docs(self, docs):\n        return [self.detokenize_doc(doc) for doc in docs]\n\n\n\npreserving_tokenizer = PreservingTokenizer()\ntokenized_docs = preserving_tokenizer.tokenize_docs(tagged_docs)\n\n\nprint(tagged_docs[0][0:5]) # first doc first 5 tags\nprint(tokenized_docs[0][0:15]) # first doc first 15 tags\nprint(preserving_tokenizer.detokenize_docs(tokenized_docs)[0][0:5])","4f9a4685":"#--------------------------------------\n#\n#    Normalize data\n#\n#--------------------------------------\n\n# Todo: use unk value? E.g. use only words contained by min 5% of every doc \n\nimport unicodedata\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nnltk.download('stopwords')\n\n\n# Turn a Unicode string to plain ASCII, thanks to # https:\/\/stackoverflow.com\/a\/518232\/2809427\ndef unicodeToAscii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\n\nclass FeaturePreprocessor():\n    def __init__(self, lang = 'hungarian'):\n        self.stopwords = stopwords.words(lang)\n        self.stemmer = SnowballStemmer(lang)\n\n    \"\"\"\n        Lowercarse and stemm\n    \"\"\"    \n    def normalizeWord(self, word):\n        lowered = str(word).casefold()\n        stemmed = self.stemmer.stem(lowered)\n        \n        lowered = unicodeToAscii(lowered)\n        stemmed = unicodeToAscii(stemmed)\n        \n        return lowered, stemmed\n    \n    \"\"\"\n        Create feature dictionary for word at doc[index+shift]\n    \"\"\"\n    def word_features(self, doc, index, shift=0): # todo reduce run time\n        index = index + shift\n        word = doc[index][0]\n        lowered, stemmed = self.normalizeWord(word)\n\n        prefix = f'{shift}:' if shift < 0 else (f'+{shift}:' if shift > 0 else '')\n        features = {\n            # todo: isupper, istitle\n            'bias': 1.0,\n            f'{prefix}stem': stemmed,\n            f'{prefix}[:3]': lowered[:3],\n            f'{prefix}[:2]': lowered[:2],\n            f'{prefix}[-3:]': lowered[-3:],\n            f'{prefix}[-2:]': lowered[-2:],\n            f'{prefix}isdigit': lowered.isdigit(),\n            f'{prefix}isalpha': lowered.isalpha(),\n            f'{prefix}isalnum': lowered.isalnum(),\n            f'{prefix}isstop': lowered in self.stopwords\n        }\n        if index == 0:\n            features[f'{prefix}BOS'] = True\n        elif index == len(doc)-1:\n            features[f'{prefix}EOS'] = True\n            \n        return features\n    \n    \"\"\"\n        Create n-gram like features for word at doc[index]\n        - n = 2 * n_step + 1\n    \"\"\"\n    def n_gram(self, doc, index, n_step):\n        features = {}\n        for shift in range(-n_step, n_step+1):\n            if index+shift >= 0 and index+shift < len(doc):\n                features.update(self.word_features(doc, index, shift))\n\n        return (features, doc[index]) \n    \n    \n    def transform(self, docs, n_step=2):\n        return [[self.n_gram(doc, index, n_step) for index in range(len(doc))] for doc in docs]\n    \n\n\nfeature_preprocessor = FeaturePreprocessor()\nprepared_docs = feature_preprocessor.transform(tokenized_docs)\n\n\nprint(' '.join(np.array(tokenized_docs[0])[:100,0]))\nprint('---------------------------------------')\nprint(np.array(prepared_docs[0])[2]) # first doc third feature\nprint('---------------------------------------')\nprint(' '.join(np.array(tokenized_docs[0])[:5,2])) # first doc first 5 tags","98f82b88":"#--------------------------------------\n#\n#    Split doc into equal-length windows\n#\n#--------------------------------------\n\n\n# TODO: use the longest tag value for calc the window size?\nclass SequenceSplitter():\n    \"\"\"\n        Split length evenly into sublength\n    \"\"\" \n    def split_length_evenly(self, total_len, max_sublength):\n        # calculate split lengths\n        quotient = int(total_len \/ max_sublength) \n        remainder = total_len % max_sublength\n        if remainder == 0:\n            return np.array([max_sublength] * quotient)\n        lens = np.array([max_sublength] * quotient + [remainder])\n        \n        # optimize length to have the smallest diff between min and max\n        min_len, max_len = lens.min(), lens.max()\n        while(max_len - min_len >= 2):\n            lens[lens.argmin()] = min_len + 1\n            lens[lens.argmax()] = max_len - 1\n            min_len, max_len = lens.min(), lens.max()\n       \n        return lens\n    \n    \"\"\"\n        Get starting indexes of sequence windows\n    \"\"\"\n    def window_indexes(self, total_size, window_size, min_overlap_size):\n        total_len = total_size\n        # window = section + overlapping\n        # total_len = section1 + section2 +...+ last_section\n        \n        # last section have full window size since it can not overlap with the following section\n        last_section_len = window_size\n        # other sections may have various sizes, overlapping should \"pad\" them to window size\n        sections_total_len = total_len - last_section_len\n        max_section_len = window_size - min_overlap_size\n        \n        # calculate sections' various lengths\n        section_lens = self.split_length_evenly(sections_total_len, max_section_len)\n        section_lens = np.append(section_lens, last_section_len)\n        \n        # calculate sections' (or windows') starting indexes\n        window_indexes = section_lens.cumsum() - section_lens\n        \n        return window_indexes\n    \n    \"\"\"\n        Split tokens and tags into overlapping windows\n    \"\"\"\n    def n_paragraph(self, doc, window_size, min_overlap_size):\n        \n        total_len = len(doc)\n        window_indexes = self.window_indexes(total_len, window_size, min_overlap_size)\n        \n        return [doc[i : i+window_size] for i in window_indexes]\n\n    \"\"\"\n        Split every doc into overlapping windows with equal lengths\n    \"\"\"\n    def split_docs(self, docs, window_size=150, min_overlap_size=50):\n        splitted_docs = []\n        for doc_i, doc in enumerate(docs):\n            for window in self.n_paragraph(doc, window_size, min_overlap_size):\n                splitted_docs.append((doc_i, window))\n        return splitted_docs\n\n    \"\"\"\n        Concat overlapping windows into original format docs\n        - Handle overlapping tags\n    \"\"\"\n    def concat_doc(self, doc, window_size=150, min_overlap_size=50): # doc: [(subword, wi, tag), ...]\n        section_size = window_size - min_overlap_size\n        concated_doc = []\n        next_token_i = -1\n        for window_i, window in enumerate(doc):\n            for token_i, item in enumerate(window):\n                _, wi, tag = item\n                # if the previous window's overlap already added this token\n                if token_i <= next_token_i:\n                    continue\n                # if in overlap and O tag, go to the next window (except if last window)\n                if token_i > section_size and tag == O_tag and not window_i == len(doc)-1:\n                    break\n                # else add new token item\n                concated_doc.append(item)\n                next_token_index = wi\n                \n        return concated_doc\n    \n    \"\"\"\n        Concat every windows into their original doc\n    \"\"\"\n    def concat_docs(self, docs, window_size=150, min_overlap_size=50):\n        splitted_docs = [[]]\n        last_doc_i = 0\n        for doc_i, window in docs:\n            if doc_i == last_doc_i:\n                splitted_docs[-1].append(window)\n            else:\n                splitted_docs.append([window])\n                last_doc_i = doc_i\n                \n        return [self.concat_doc(doc, window_size, min_overlap_size) for doc in splitted_docs]\n                \n                   \n    \nsequence_splitter = SequenceSplitter()\nwindow_size = 150\noverlap_size = 50\nsplitted_docs = sequence_splitter.split_docs(prepared_docs, window_size, overlap_size)\n\n\nprint(splitted_docs[0][1][1]) # first doc, second window, second token","986261ef":"X = [[item[0] for item in window] for _, window in splitted_docs]\ny = [[item[1][2] for item in window] for _, window in splitted_docs]\n\nprint(np.array(X).shape)\nprint(np.array(y).shape)","adb0f9ec":"!pip install sklearn_crfsuite","63d1b483":"def flatten(l):\n    return [item for sublist in l for item in sublist]","d53c59ad":"#--------------------------------------\n#\n#    Train model\n#\n#--------------------------------------\n\nimport sklearn\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport sklearn_crfsuite\nfrom sklearn_crfsuite import scorers\nfrom sklearn_crfsuite import metrics\n\ncrf = sklearn_crfsuite.CRF(\n    all_possible_transitions=True\n)\n\nparams_space = {\n    'algorithm': ['l2sgd', 'lbfgs'], # lbfgs, l2sgd\n    #'c2': [0.9],\n    #'c1': [0.2, 0.7], # csak lbfgs\n    'max_iterations': [100, 200], #[700, 1000], \n    #'num_memories': [6, 10], # csak lbfgs\n}\n\nf1_scorer = make_scorer(metrics.flat_f1_score, average='weighted', labels=np.unique(flatten(y)))\n\nrs = RandomizedSearchCV(\n    crf, params_space,\n    cv=4,\n    verbose=1,\n    n_jobs=1,\n    n_iter=10,\n    scoring=f1_scorer\n)\n\nrs.fit(X, y)\ncrf = rs.best_estimator_\n\nprint('best params:', rs.best_params_)\nprint('best CV score:', rs.best_score_)\nprint('model size: {:0.2f}M'.format(rs.best_estimator_.size_ \/ 1000000))","622d53ea":"#--------------------------------------\n#\n#    Evaluate and save model\n#\n#--------------------------------------\n\nlabels = list(crf.classes_)\n\ny_pred = crf.predict(X)\n\nprint(classification_report(flatten(y), flatten(y_pred), labels=labels))\n\nlabels.remove('O')\nmetrics.flat_f1_score(y, y_pred, average='weighted', labels=labels)","8ff06ccd":"from joblib import dump, load\ndump(crf, 'crf_model.joblib')","d8616178":"tagged_test = [[(O_tag, text)] for text in raw_data['ocr_text'].values]\n\ntokenized_test = preserving_tokenizer.tokenize_docs(tagged_test)\n\nprint(tokenized_test[0][:15])","ae78558d":"prepared_test = feature_preprocessor.transform(tokenized_test)\n\nprint(prepared_test[0][2])","7016c57a":"splitted_test = sequence_splitter.split_docs(prepared_test)\n\nprint(splitted_test[0][1][1])","e85866da":"X_test = [[item[0] for item in window] for _, window in splitted_test]\n\nprint(np.array(X).shape)","4a64c63a":"loaded_crf = load('crf_model.joblib')","a5ddeedc":"test_pred = loaded_crf.predict(X_test)\n\nprint(test_pred[0])","7c06915f":"def merge_pred_with_docs(pred, docs):\n    return [\n        (\n            docs[i][0], \n            [\n                (\n                    docs[i][1][ii][1][0], \n                    docs[i][1][ii][1][1], \n                    pred[i][ii]\n                ) \n                for ii in range(window_size)\n            ]\n        ) \n        for i in range(len(docs))\n    ]\n\nmerged_pred = merge_pred_with_docs(test_pred, splitted_test)\n\nprint(merged_pred[0])","93b8b591":"concat_pred = sequence_splitter.concat_docs(merged_pred)\n\nprint(concat_pred[0][:15])","08a4346c":"detokenized_pred = preserving_tokenizer.detokenize_docs(concat_pred)\n\nprint(detokenized_pred[0][:5])","773673e0":"def extract_results(pred):\n    results = []\n    for doc in detokenized_pred:\n        tag_values = {}\n        for tag, value in doc:\n            if tag != O_tag and (tag not in tag_values or len(tag_values[tag]) < len(value)):\n                tag_values[tag] = value\n        results.append(tag_values)\n        \n    return results\n\n  \nresults = extract_results(detokenized_pred)\n    \nprint(results[0])","42fa6759":"# Inference","1d25c07b":"# Train"}}