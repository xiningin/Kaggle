{"cell_type":{"5919ab52":"code","560f8642":"code","83f4ea32":"code","fba402d2":"code","6dc4cb5d":"code","5248f6b8":"code","f509002a":"code","59b1fd2c":"code","942a66d2":"code","6c4a2ea7":"code","c95dcaff":"code","18adb34c":"code","8bb64fdd":"code","08cc6503":"code","9d989b9b":"code","26465baa":"code","08a10053":"code","e9376ed2":"code","7f6229ef":"code","83ea1e97":"code","0b518c27":"code","7983fa42":"code","5fe513d7":"code","f7cf9473":"code","fdd33758":"code","88e8d8c8":"code","6adf0bd7":"code","2897bd63":"code","13603752":"code","375606b3":"code","bb26cdfe":"code","f5e71ec1":"code","dbed32ba":"code","f0c8e0f3":"code","fc7c6cd9":"code","f11eb7b7":"code","ca740d43":"code","717f6408":"code","22662ff0":"markdown","55ff70b6":"markdown","eb14133a":"markdown","cc1583fc":"markdown","e9f20027":"markdown","0ac639c3":"markdown","5a7b057f":"markdown","30da577a":"markdown","203b2716":"markdown","951c296b":"markdown","0e858a24":"markdown","b7bef6c3":"markdown","a1a151b7":"markdown","8d7cb036":"markdown","487c318f":"markdown","4218d73d":"markdown","8dd22ae1":"markdown","b9a3792c":"markdown","aa7171fa":"markdown","2f774ee4":"markdown","6085e59b":"markdown","e21451b7":"markdown","ad95a376":"markdown","376b3502":"markdown","2a8cf666":"markdown","9b9bd002":"markdown","172e299e":"markdown","21c33088":"markdown","5de7e105":"markdown","bbc80422":"markdown","223db10a":"markdown","8b055461":"markdown","28cf5872":"markdown","7afc846c":"markdown","3cdee489":"markdown","6dee2c3b":"markdown","3d15475e":"markdown"},"source":{"5919ab52":"import pandas as pd # data processing\nfrom pandas.plotting import scatter_matrix\nimport numpy as np # working with arrays\nimport seaborn as sns #visualization\nimport matplotlib.pyplot as plt # visualization\nimport itertools # advanced tools\n","560f8642":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","83f4ea32":"\ndf = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\n\n#We are going to neglect the time feature which is of no use to build the models\n#The remaining features are the \u2018Amount\u2019 feature that contains the total amount of money being transacted and the \u2018Class\u2019\n#feature that contains whether the transaction is a fraud case or not\n\ndf.drop('Time', axis = 1, inplace = True)\nprint(df.head())\ndf.hist()\nplt.show()","fba402d2":"cases = len(df)\nnonfraud_count = len(df[df.Class == 0])\nfraud_count = len(df[df.Class == 1])\nfraud_percentage = round(fraud_count\/nonfraud_count*100, 2)\ndf.dropna() #dropped NA values if any\n#df.shape","6dc4cb5d":"print('CASE COUNT\\n')\nprint('Total number of cases are {}'.format(cases))\nprint('Number of Non-fraud cases are {}'.format(nonfraud_count))\nprint('Number of Non-fraud cases are {}'.format(fraud_count))\nprint('Percentage of fraud cases is {}'.format(fraud_percentage))","5248f6b8":"#Chcking for null values\ndf.isnull().values.any()","f509002a":"ax = sns.countplot(x=\"Class\", data=df)\nax.set_xticklabels(['Not Fraud','Fraud'])","59b1fd2c":"#Showing the detailed percentages\nnonfraud_cases = df[df.Class == 0]\nfraud_cases = df[df.Class == 1]\n\nprint('CASE AMOUNT STATISTICS')\nprint('\\nNON-FRAUD CASE AMOUNT STATS')\nprint(nonfraud_cases['Amount'].describe())\nprint('\\nFRAUD CASE AMOUNT STATS')\nprint(fraud_cases['Amount'].describe())","942a66d2":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 50\nax1.hist(fraud_cases.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(nonfraud_cases.Amount, bins = bins)\nax2.set_title(\"Normal(Non Fraud)\")\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\nplt.figure(figsize=(20,20))\nplt.show();","6c4a2ea7":"## Correlation\nimport seaborn as sns\n#get correlations of each features in dataset\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n","c95dcaff":"from sklearn.preprocessing import StandardScaler \ndf['Amount'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['Amount'].head(10)","18adb34c":"#Showing the Updated detailed percentages\nnonfraud_cases = df[df.Class == 0]\nfraud_cases = df[df.Class == 1]\n\nprint('CASE AMOUNT STATISTICS')\nprint('\\nNON-FRAUD CASE AMOUNT STATS')\nprint(nonfraud_cases['Amount'].describe())\nprint('\\nFRAUD CASE AMOUNT STATS')\nprint(fraud_cases['Amount'].describe())","8bb64fdd":"#Create independent and Dependent Features\ncolumns = df.columns.tolist()\n# Filter the columns to remove data we do not want \ncolumns = [c for c in columns if c not in [\"Class\"]]\n# Store the variable we are predicting \ntarget = \"Class\"\n# Define a random state \nstate = np.random.RandomState(42)\nX = df[columns]\nY = df[target]\n# Print the shapes of X & Y\nprint(X.shape)\nprint(Y.shape)","08cc6503":"X\nY","9d989b9b":"# Train-Test split\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.5, random_state= 0)\nprint(\"Shape of X_train: \", X_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)","26465baa":"from sklearn.tree import DecisionTreeClassifier # Decision tree algorithm\nfrom sklearn.neighbors import KNeighborsClassifier # KNN algorithm\nfrom sklearn.linear_model import LogisticRegression # Logistic regression algorithm\nfrom sklearn.ensemble import AdaBoostClassifier # Adaboost algorithm\nfrom sklearn.ensemble import RandomForestClassifier # Random forest tree algorithm\nfrom xgboost import XGBClassifier # XGBoost algorithm\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score, recall_score, roc_auc_score ","08a10053":"log_classifier = LogisticRegression()\nlog_classifier.fit(X_train,Y_train)\ny_pred = log_classifier.predict(X_test)\nytrain_pred=log_classifier.predict_proba(X_train)\nytest_pred=log_classifier.predict_proba(X_test)\n","e9376ed2":"#1. Accuracy Score\nprint('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(Y_test, y_pred)))\n#2. F1 Score\nprint('F1 score of the Logistic Regression model is {}'.format(f1_score(Y_test, y_pred)))\n#3. Precison\nprint('Precision of the Logistic Regression model is {}'.format(precision_score(Y_test,y_pred)))\n#4. Recall\nprint('Recall of the Logistic Regression model is {}'.format(recall_score(Y_test,y_pred)))\n#5. Train ROC_AUC\nprint('Logistic Train roc-auc: {}'.format(roc_auc_score(Y_train,ytrain_pred[:,:-1])))\n#6. Test ROC_AUC\nprint('Logistic Test roc-auc: {}'.format(roc_auc_score(Y_test,ytest_pred[:,:-1])))\n","7f6229ef":"clf = DecisionTreeClassifier()\nclf = clf.fit(X_train, Y_train)\ny_pred2 = clf.predict(X_test)\nytrain_pred2=clf.predict_proba(X_train)\nytest_pred2=clf.predict_proba(X_test)","83ea1e97":"#1. Accuracy Score\nprint('Accuracy score of the Decision Tree Classifier model is {}'.format(accuracy_score(Y_test, y_pred2)))\n#2. F1 Score\nprint('F1 score of the Decision Tree Classifier model is {}'.format(f1_score(Y_test, y_pred2)))\n#3. Precison\nprint('Precision of the Decision Tree Classifier model is {}'.format(precision_score(Y_test,y_pred2)))\n#4. Recall\nprint('Recall of the Decision Tree Classifier model is {}'.format(recall_score(Y_test,y_pred2)))\n#5. Train ROC_AUC\nprint('Decision Tree Classifier Train roc-auc: {}'.format(roc_auc_score(Y_train,ytrain_pred2[:,:-1])))\n#6. Test ROC_AUC\nprint('Decision Tree Classifier Test roc-auc: {}'.format(roc_auc_score(Y_test,ytest_pred2[:,:-1])))","0b518c27":"abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\nabc = abc.fit(X_train, Y_train)\ny_pred3 = abc.predict(X_test)\nytrain_pred3=abc.predict_proba(X_train)\nytest_pred3=abc.predict_proba(X_test)","7983fa42":"#1. Accuracy Score\nprint('Accuracy score of the Adaboost Classification model is {}'.format(accuracy_score(Y_test, y_pred3)))\n#2. F1 Score\nprint('F1 score of the Adaboost Classification model is {}'.format(f1_score(Y_test, y_pred3)))\n#3. Precison\nprint('Precision of the Adaboost Classification model is {}'.format(precision_score(Y_test,y_pred3)))\n#4. Recall\nprint('Recall of the Adaboost Classification model is {}'.format(recall_score(Y_test,y_pred3)))\n#5. Train ROC_AUC\nprint('Adaboost Classifier Train roc-auc: {}'.format(roc_auc_score(Y_train,ytrain_pred3[:,:-1])))\n#6. Test ROC_AUC\nprint('Adaboost Classifier Test roc-auc: {}'.format(roc_auc_score(Y_test,ytest_pred3[:,:-1])))\n","5fe513d7":"rf=RandomForestClassifier()\nrf.fit(X_train,Y_train)\ny_pred4 = clf.predict(X_test)\nytrain_pred4=clf.predict_proba(X_train)\nytest_pred4=clf.predict_proba(X_test)","f7cf9473":"#1. Accuracy Score\nprint('Accuracy score of the Random Forest Classifier model is {}'.format(accuracy_score(Y_test, y_pred4)))\n#2. F1 Score\nprint('F1 score of the Random Forest Classifier model is {}'.format(f1_score(Y_test, y_pred4)))\n#3. Precison\nprint('Precision of the Random Forest Classifier model is {}'.format(precision_score(Y_test,y_pred4)))\n#4. Recall\nprint('Recall of the Random Forest Classifier model is {}'.format(recall_score(Y_test,y_pred4)))\n#5. Train ROC_AUC\nprint('Random Forest Classifier Train roc-auc: {}'.format(roc_auc_score(Y_train,ytrain_pred4[:,:-1])))\n#6. Test ROC_AUC\nprint('Random Forest Classifier Test roc-auc: {}'.format(roc_auc_score(Y_test,ytest_pred4[:,:-1])))\n\n","fdd33758":"xgb = XGBClassifier(max_depth = 4)\nxgb.fit(X_train, Y_train)\ny_pred5 = xgb.predict(X_test)\nytrain_pred5=xgb.predict_proba(X_train)\nytest_pred5=xgb.predict_proba(X_test)","88e8d8c8":"#1. Accuracy Score\nprint('Accuracy score of the XG Boost Classifier model is {}'.format(accuracy_score(Y_test, y_pred5)))\n#2. F1 Score\nprint('F1 score of the XG Boost Classifier model is {}'.format(f1_score(Y_test, y_pred5)))\n#3. Precison\nprint('Precision of the XG Boost Classifier model is {}'.format(precision_score(Y_test,y_pred5)))\n#4. Recall\nprint('Recall of the XG Boost Classifier model is {}'.format(recall_score(Y_test,y_pred5)))\n#5. Train ROC_AUC\nprint('XG Boost Classifier Train roc-auc: {}'.format(roc_auc_score(Y_train,ytrain_pred5[:,:-1])))\n#6. Test ROC_AUC\nprint('XG Boost Classifier Test roc-auc: {}'.format(roc_auc_score(Y_test,ytest_pred5[:,:-1])))\n\n","6adf0bd7":"# Separate class\nNon_fraud = df[df['Class'] == 0]\nFraud = df[df['Class'] == 1]  # print the shape of the class\nprint('class 0 = Non_fraud:', Non_fraud.shape)\nprint('class 1 = Fraud:', Fraud.shape)\n","2897bd63":"from imblearn.over_sampling import SMOTE\nfrom collections import Counter\nX_resampled, Y_resampled = SMOTE().fit_resample(X, Y)\nprint(\"Resampled shape of X: \", X_resampled.shape)\nprint(\"Resampled shape of Y: \", Y_resampled.shape)\nvalue_counts = Counter(Y_resampled)\nprint(value_counts)\ntrain_X, test_X, train_Y, test_Y = train_test_split(X_resampled, Y_resampled, test_size= 0.3, random_state= 42)","13603752":"log_classifier = LogisticRegression()\nlog_classifier.fit(train_X,train_Y)\ny_pred = log_classifier.predict(test_X)\nytrain_pred=log_classifier.predict_proba(train_X)\nytest_pred=log_classifier.predict_proba(test_X)","375606b3":"#1. Accuracy Score\nprint('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(test_Y, y_pred)))\n#2. F1 Score\nprint('F1 score of the Logistic Regression model is {}'.format(f1_score(test_Y, y_pred)))\n#3. Precison\nprint('Precision of the Logistic Regression model is {}'.format(precision_score(test_Y,y_pred)))\n#4. Recall\nprint('Recall of the Logistic Regression model is {}'.format(recall_score(test_Y,y_pred)))\n#5. Train ROC_AUC\nprint('Logistic Train roc-auc: {}'.format(roc_auc_score(train_Y,ytrain_pred[:,:-1])))\n#6. Test ROC_AUC\nprint('Logistic Test roc-auc: {}'.format(roc_auc_score(test_Y,ytest_pred[:,:-1])))","bb26cdfe":"clf = DecisionTreeClassifier()\nclf = clf.fit(train_X, train_Y)\ny_pred2 = clf.predict(test_X)\nytrain_pred2=clf.predict_proba(train_X)\nytest_pred2=clf.predict_proba(test_X)","f5e71ec1":"#1. Accuracy Score\nprint('Accuracy score of the Decision Tree Classifier model is {}'.format(accuracy_score(test_Y, y_pred2)))\n#2. F1 Score\nprint('F1 score of the Decision Tree Classifier model is {}'.format(f1_score(test_Y, y_pred2)))\n#3. Precison\nprint('Precision of the Decision Tree Classifier model is {}'.format(precision_score(test_Y,y_pred2)))\n#4. Recall\nprint('Recall of the Decision Tree Classifier model is {}'.format(recall_score(test_Y,y_pred2)))\n#5. Train ROC_AUC\nprint('Decision Tree Classifier Train roc-auc: {}'.format(roc_auc_score(train_Y,ytrain_pred2[:,:-1])))\n#6. Test ROC_AUC\nprint('Decision Tree Classifier Test roc-auc: {}'.format(roc_auc_score(test_Y,ytest_pred2[:,:-1])))","dbed32ba":"abc = AdaBoostClassifier(n_estimators=50,learning_rate=1)\nabc = abc.fit(train_X, train_Y)\ny_pred3 = abc.predict(test_X)\nytrain_pred3=abc.predict_proba(train_X)\nytest_pred3=abc.predict_proba(test_X)","f0c8e0f3":"#1. Accuracy Score\nprint('Accuracy score of the Adaboost Classification model is {}'.format(accuracy_score(test_Y, y_pred3)))\n#2. F1 Score\nprint('F1 score of the Adaboost Classification model is {}'.format(f1_score(test_Y, y_pred3)))\n#3. Precison\nprint('Precision of the Adaboost Classification model is {}'.format(precision_score(test_Y,y_pred3)))\n#4. Recall\nprint('Recall of the Adaboost Classification model is {}'.format(recall_score(test_Y,y_pred3)))\n#5. Train ROC_AUC\nprint('Adaboost Classifier Train roc-auc: {}'.format(roc_auc_score(train_Y,ytrain_pred3[:,:-1])))\n#6. Test ROC_AUC\nprint('Adaboost Classifier Test roc-auc: {}'.format(roc_auc_score(test_Y,ytest_pred3[:,:-1])))\n","fc7c6cd9":"rf=RandomForestClassifier()\nrf.fit(train_X,train_Y)\ny_pred4 = clf.predict(test_X)\nytrain_pred4=clf.predict_proba(train_X)\nytest_pred4=clf.predict_proba(test_X)","f11eb7b7":"#1. Accuracy Score\nprint('Accuracy score of the Random Forest Classifier model is {}'.format(accuracy_score(test_Y, y_pred4)))\n#2. F1 Score\nprint('F1 score of the Random Forest Classifier model is {}'.format(f1_score(test_Y, y_pred4)))\n#3. Precison\nprint('Precision of the Random Forest Classifier model is {}'.format(precision_score(test_Y,y_pred4)))\n#4. Recall\nprint('Recall of the Random Forest Classifier model is {}'.format(recall_score(test_Y,y_pred4)))\n#5. Train ROC_AUC\nprint('Random Forest Train roc-auc: {}'.format(roc_auc_score(train_Y,ytrain_pred4[:,:-1])))\n#6. Test ROC_AUC\nprint('Random Forest Test roc-auc: {}'.format(roc_auc_score(test_Y,ytest_pred4[:,:-1])))\n","ca740d43":"xgb = XGBClassifier(max_depth = 4)\nxgb.fit(train_X, train_Y)\ny_pred5 = xgb.predict(test_X)\nytrain_pred5=xgb.predict_proba(train_X)\nytest_pred5=xgb.predict_proba(test_X)","717f6408":"#1. Accuracy Score\nprint('Accuracy score of the XG Boost Classifier model is {}'.format(accuracy_score(test_Y, y_pred5)))\n#2. F1 Score\nprint('F1 score of the XG Boost Classifier model is {}'.format(f1_score(test_Y, y_pred5)))\n#3. Precison\nprint('Precision of the XG Boost Classifier model is {}'.format(precision_score(test_Y,y_pred5)))\n#4. Recall\nprint('Recall of the XG Boost Classifier model is {}'.format(recall_score(test_Y,y_pred5)))\n#5. Train ROC_AUC\nprint('XG Boost Classifier Train roc-auc: {}'.format(roc_auc_score(train_Y,ytrain_pred5[:,:-1])))\n#6. Test ROC_AUC\nprint('XG Boost Classifier Test roc-auc: {}'.format(roc_auc_score(test_Y,ytest_pred5[:,:-1])))\n","22662ff0":"## Model 3: Adaboost Classification","55ff70b6":"## Model 4: Random Forest Classifier","eb14133a":"## Step2- Reading the dataset","cc1583fc":"We find a balanced distribution of data after SMOTE Tomek operation. \nApplying the ML algorithms once again","e9f20027":"## Handling Imbalanced Data \n","0ac639c3":"## Testing the models on the balanced data ","5a7b057f":"## SMOTE Tomek Technique","30da577a":"## Conclusions ","203b2716":"We saw how the selected ML algorithms perfrom with the unbalanced data. The performance is yet to increase after we perform SMOTETomek to generate new examples that are synthesized from the existing minority class(Fraud Cases)","951c296b":"## Model 5: XG Boost Classifier ","0e858a24":"## Model 1: Logistic Regression","b7bef6c3":"#### Evaluating the model ","a1a151b7":"#### Evaluating the model ","8d7cb036":"## Model 2: Decision Tree Classifier ","487c318f":"#### Evaluating the model","4218d73d":"## Model 4: Random Forest Classifier ","8dd22ae1":"#### Evaluating the model","b9a3792c":"## Model 2: Decision Tree Classifier","aa7171fa":"#### Evaluating the model","2f774ee4":"# Fraud Detection System using ML algorithms","6085e59b":"#### Evaluating the model","e21451b7":"## Model 1: Logistic Regression ","ad95a376":"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","376b3502":"1. The dataset was cleaned and the features were removed using PCA from the beginning because of the confidentiality reasons. This made woking on the model a bit difficult as the features were unknown.\n\n2. The data is very unbalanced and all the models that were run using the dataset showed results which indicated overfitting.\n\n3. I used SMOTE Tomek method to resample the dataset and made it balanced.\n\n4. The models I worked on are Decision Tree, K-Nearest Neighbors (KNN), Logistic Regression, AdaBoost Classifier, Random Forest, and XGBoost\n\n5. The best model was XGBoost classifier showing the best accuracy and F1 score in the balanced dataset.\n\n6. The biggest problem is faced was while implementing SVM in the dataset. SVM took very long time to compute making it an unreasonable alternative for an ML model. I tried using differnt kernels within SVM (Linear and RBF ) but both took very long time to compute the results when the dataset was balanced. ","2a8cf666":"## Step3- Exploratory Data Aanalysis","9b9bd002":"#### Evaluating the model","172e299e":"## Model 3: Adaboost Classification","21c33088":"## Feature Scaling : Normalising the data","5de7e105":"The models we will test here are: Decision Tree, K-Nearest Neighbors (KNN), Logistic Regression, AdaBoost Classifier, Random Forest, and XGBoost\n","bbc80422":"#### Evaluating the model","223db10a":"#### Evaluating the model","8b055461":"## Notes from the dataset ","28cf5872":"Here we see that only 17% of 274807 cases are fraudulent\nSo its clearly a case of unbalanced data\nWe need to handle this unbalanced data to proceed using our ML models","7afc846c":"## Model 5: XG Boost Classifier ","3cdee489":"## Testing the models on the imbalanced data ","6dee2c3b":"## Step1- Importing the libraries","3d15475e":"####  Evaluating the model"}}