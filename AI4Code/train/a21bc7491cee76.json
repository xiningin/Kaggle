{"cell_type":{"5c5d1245":"code","b4f5618b":"code","291518e4":"code","2c0a34e6":"code","cb77f4cb":"code","726009d3":"code","c49bc5e7":"code","366400a6":"code","fe45292c":"code","5ae0c4f0":"code","b48c0676":"code","97897d61":"code","b009b0ab":"code","87ff9782":"code","6fb51d35":"code","3945871e":"code","8615bed6":"code","9fc8acb9":"code","35409467":"code","1624d8df":"code","022fb55d":"code","c3776ea6":"code","87e0c2d6":"code","f38b862a":"code","951176f9":"code","bce4e364":"code","a2f00f15":"code","c00ffa5c":"code","c173e3af":"code","5ac3c295":"code","2bdea4f4":"code","5f56bca3":"code","6acb2747":"code","4dd46c8f":"code","2551d11f":"code","129f4937":"code","bd8ba9c6":"code","107f0b41":"code","54076897":"code","947cf8b9":"code","e7f94670":"code","afcff564":"code","6f27150b":"code","65f75c34":"code","2c2447ba":"code","d0509d7e":"code","10545514":"code","779d1f3a":"code","7a8109fe":"code","fadde83e":"code","4ee0df51":"code","b0f06c7e":"code","d2aeaad1":"code","72a9a208":"code","142fec04":"markdown","7b662269":"markdown","9a21e116":"markdown","077a0824":"markdown","af1c73da":"markdown","72e79983":"markdown","82859de1":"markdown","aaa4daa1":"markdown","b304bbe8":"markdown","2a4ddb0f":"markdown","f3e9a906":"markdown","7f525480":"markdown","0737a99f":"markdown","4cd19357":"markdown","5eb74fc5":"markdown","6b882dbe":"markdown","408a7c7c":"markdown","3d4ef51a":"markdown","8d7007e5":"markdown"},"source":{"5c5d1245":"import os\nimport pathlib\nfrom glob import glob\nimport fnmatch\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport os\nimport PIL","b4f5618b":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping","291518e4":"! cp -R ..\/input\/recipes .\/newrecipes","2c0a34e6":"data_dir_path = \".\/newrecipes\/\"","cb77f4cb":"# import shutil\n# shutil.rmtree(data_dir_path)","726009d3":"num_skipped = 0\n\nfor folder_name in ['briyani', 'burger', 'dosa', 'idly', 'pizza']:\n    folder_path = os.path.join(data_dir_path, folder_name)\n    for fname in os.listdir(folder_path):\n        fpath = os.path.join(folder_path, fname)\n        try:\n            fobj = open(fpath, \"rb\")\n            is_jfif = tf.compat.as_bytes(\"JFIF\") in fobj.peek(10)\n        finally:\n            fobj.close()\n\n        if not is_jfif:\n            num_skipped += 1\n            # Delete corrupted image\n            os.remove(fpath)\n\nprint(\"Deleted %d images\" % num_skipped)","c49bc5e7":"# Defining the path for train and \\test images\ndata_dir_train = pathlib.Path(data_dir_path)","366400a6":"IMAGE_SIZE = 250\nBATCH_SIZE = 32\nCHANNELS = 3","fe45292c":"dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir_path,\n    shuffle=True,\n    seed=123,\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n)","5ae0c4f0":"def get_dataset_partition_tf(ds, train_split=0.70, val_split=0.20, test_split=0.10, shuffle=True, shuffle_size=10000):\n    \n    ds_size = len(ds)\n    \n    if shuffle:\n        ds = ds.shuffle(shuffle_size, seed=12)\n    train_size = int(ds_size * train_split)\n    val_size = int(ds_size * val_split)\n    \n    train_ds = ds.take(train_size)\n    val_ds = ds.skip(train_size).take(val_size)\n    test_ds = ds.skip(train_size).skip(val_size)\n    \n    return train_ds, test_ds, val_ds","b48c0676":"train_ds, test_ds, val_ds = get_dataset_partition_tf(dataset)","97897d61":"print(\"Training size:\", len(train_ds)*BATCH_SIZE)\nprint(\"Validation size:\", len(val_ds)*BATCH_SIZE)\nprint(\"Testing size:\", len(test_ds)*BATCH_SIZE)","b009b0ab":"class_names = dataset.class_names\nclass_names","87ff9782":"temp_class = []\ntemp_dict = {}\nfor images, labels in train_ds:\n    for i, val in enumerate(class_names):\n        if class_names[labels[i]] not in temp_class:\n            temp_class.append(class_names[labels[i]])\n            temp_dict[class_names[labels[i]]] = images[i].numpy().astype(\"uint8\")\n    if len(temp_class) == len(class_names):\n        break","6fb51d35":"plt.figure(figsize=(20, 5))\nfor i in range(len(class_names)):\n    ax = plt.subplot(1, 5, i + 1)\n    plt.imshow(temp_dict[class_names[i]])\n    plt.title(class_names[i])\n    plt.axis(\"off\")","3945871e":"AUTOTUNE = tf.data.experimental.AUTOTUNE\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","8615bed6":"resize_and_rescale = tf.keras.Sequential([\n    layers.experimental.preprocessing.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n    layers.experimental.preprocessing.Rescaling(1.0\/255)\n])","9fc8acb9":"data_augmentation = keras.Sequential([\n    \n    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),    \n    layers.experimental.preprocessing.RandomRotation(0.2),\n    layers.experimental.preprocessing.RandomZoom(0.25),\n\n])","35409467":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(6):\n        augmented_images = data_augmentation(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")","1624d8df":"# Function to visualize training result\ndef visualize_train_result(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    plt.figure(figsize=(16, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(acc, label='Training Accuracy')\n    plt.plot(val_acc, label='Validation Accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Accuracy')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(loss, label='Training Loss')\n    plt.plot(val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n    plt.show()","022fb55d":"input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\nn_classes = len(class_names)\nprint(\"input_shape\", input_shape)","c3776ea6":"def define_model1():\n    model = Sequential([\n        resize_and_rescale,\n        data_augmentation,\n        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n        \n        layers.Conv2D(64, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n\n        layers.Conv2D(128, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n\n        layers.Flatten(),\n\n        layers.Dense(256, activation='relu'),\n        layers.Dense(n_classes, activation='softmax')\n    ])\n    model.build(input_shape)\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  metrics=['accuracy'])\n    return model","87e0c2d6":"model_1 = define_model1()","f38b862a":"# View the summary of all layers\nmodel_1.summary()","951176f9":"earlystop = EarlyStopping(monitor='val_loss', patience=15)\nLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, cooldown=1, verbose=1)","bce4e364":"%%time\nEPOCHS = 40\n\nhistory1 = model_1.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[LR]\n)","a2f00f15":"visualize_train_result(history1)","c00ffa5c":"input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\nprint(\"input_shape\", input_shape)","c173e3af":"def define_model2():\n    model = Sequential([\n        resize_and_rescale,\n        data_augmentation,\n        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n        \n        layers.Conv2D(64, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n\n        layers.Conv2D(128, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.20),\n\n        layers.Flatten(),\n\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.20),\n        layers.Dense(n_classes, activation='softmax')\n    ])\n    model.build(input_shape)\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  metrics=['accuracy'])\n    return model","5ac3c295":"model_2 = define_model2()","2bdea4f4":"# View the summary of all layers\nmodel_2.summary()","5f56bca3":"%%time\nEPOCHS = 40\n\nhistory2 = model_2.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[LR, earlystop]\n)","6acb2747":"visualize_train_result(history2)","4dd46c8f":"input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\nprint(\"input_shape\", input_shape)","2551d11f":"def define_model4():\n    model = Sequential([\n        resize_and_rescale,\n        data_augmentation,\n        layers.Conv2D(16, (3,3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n        \n        layers.Conv2D(32, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n\n        layers.Conv2D(64, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        \n        layers.Conv2D(128, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n        layers.Dropout(0.20),\n\n        layers.Flatten(),\n\n        layers.Dense(256, activation='relu'),\n        layers.Dropout(0.30),\n        layers.Dense(n_classes, activation='softmax')\n    ])\n    model.build(input_shape)\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  metrics=['accuracy'])\n    return model","129f4937":"model_4 = define_model4()","bd8ba9c6":"# View the summary of all layers\nmodel_4.summary()","107f0b41":"%%time\nEPOCHS = 40\n\nhistory4 = model_4.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[LR, earlystop]\n)","54076897":"visualize_train_result(history4)","947cf8b9":"!pip install Augmentor","e7f94670":"import Augmentor","afcff564":"path_to_training_dataset = data_dir_path\nfor i in class_names:\n    p = Augmentor.Pipeline(path_to_training_dataset + i)\n    p.rotate(probability=0.6, max_left_rotation=10, max_right_rotation=10)\n    p.zoom(probability=0.6, min_factor=1.1, max_factor=1.6)\n    p.crop_centre(probability=0.2, percentage_area=0.25, randomise_percentage_area=False)\n    p.sample(400) ## We are adding 400 samples per class to make sure that none of the classes are sparse.","6f27150b":"dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir_path,\n    shuffle=True,\n    seed=123,\n    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n    batch_size=BATCH_SIZE,\n)","65f75c34":"train_ds, test_ds, val_ds = get_dataset_partition_tf(dataset)","2c2447ba":"print(\"Training size:\", len(train_ds)*BATCH_SIZE)\nprint(\"Validation size:\", len(val_ds)*BATCH_SIZE)\nprint(\"Testing size:\", len(test_ds)*BATCH_SIZE)","d0509d7e":"def aug_model1():\n    model = Sequential([\n        resize_and_rescale,\n        layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n        layers.MaxPooling2D((2, 2)),\n        \n        layers.Conv2D(64, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n\n        layers.Conv2D(128, (3,3), activation='relu'),\n        layers.MaxPooling2D((2, 2)),\n\n        layers.Flatten(),\n\n        layers.Dense(256, activation='relu'),\n        layers.Dense(n_classes, activation='softmax')\n    ])\n    model.build(input_shape)\n    model.compile(optimizer='adam',\n                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                  metrics=['accuracy'])\n    return model","10545514":"aug_model_1 = aug_model1()","779d1f3a":"%%time\nEPOCHS = 10\n\naug_history1 = aug_model_1.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks=[LR]\n)","7a8109fe":"visualize_train_result(aug_history1)","fadde83e":"def predict(model, img):\n    img_array = tf.keras.preprocessing.image.img_to_array(img)\n    img_array = tf.expand_dims(img_array, 0)\n    \n    predictions = model.predict(img_array)\n    predicted_class = class_names[np.argmax(predictions[0])]\n    confidence = round(100 * (np.max(predictions[0])), 2)\n    \n    return predicted_class, confidence","4ee0df51":"def predict_test(model):\n    plt.figure(figsize=(15, 15))\n    tak = test_ds.take(1)\n    for images, labels in tak:\n        for i in range(9):\n            ax = plt.subplot(3, 3, i+1)\n\n            plt.imshow(images[i].numpy().astype(\"uint8\"))\n            image_1 = images[i].numpy()\n            predicted_class, confidence = predict(model, images[i].numpy())\n            actual_class = class_names[labels[i]]\n\n            plt.title(f\"Actual: {actual_class}, \\n Predicted: {predicted_class}, \\n Confidence: {confidence}\")        \n            plt.axis(\"off\")","b0f06c7e":"predict_test(aug_model_1)","d2aeaad1":"model_path = '.\/models'\n\nif not os.path.exists(model_path):\n    os.mkdir('.\/models')","72a9a208":"model_name_path = f'{model_path}\/food_classifier_model'\n\naug_model_1.save(model_name_path)\naug_model_1.save(f'{model_name_path}_h5\/my_model.h5')","142fec04":"## Save model","7b662269":"### Create a dataset\n\nDefine some parameters for the loader:","9a21e116":"### Findings:\n- We can see that after using augumented data model is giving preety good accuracy.\n- Model is no more overfitting.\n- Also loss and accuracy is not fluctuating.","077a0824":"## Model 2\n","af1c73da":"### Data augmentation","72e79983":"### Importing all the important libraries","82859de1":"### Findings:\n- After adding new layer we can see that validation accuracy has increased from 80% to 84%.\n- Also model is not overfitting, but model is fluctuating more.\n- Let's use augumentation to increase data and check the result.","aaa4daa1":"### Findings:\n- Training accuracy is 88% and validation accuracy 80%.\n- Let's add new layer to next model and observe the result.","b304bbe8":"### Data normalization","2a4ddb0f":"### Visualize the data\n#### Visualize one instance of all  classes present in the dataset","f3e9a906":"### Findings:\n- Training accuracy is 89% and validation accuracy 81%.\n- Model is slightly overfitting.\n- Let's add dropout to next model and observe the result.","7f525480":"## Let's predict on test data\n","0737a99f":"## Data Augmentation.","4cd19357":"**Keeps the images in memory**","5eb74fc5":"## Model 1","6b882dbe":"## Model 3\n","408a7c7c":"Let's visualize augmented examples by applying data augmentation to the same image several times:","3d4ef51a":"## Problem statement:\n\nTo build a CNN based model which can accurately detect food.\n\n[Dataset is taken from kaggle.](https:\/\/www.kaggle.com\/rajaraman6195\/recipes)\n\n### About this Dataset\nThis is a list of different food listings of images. The dataset includes the set of images for each recipes.\n\nThe dataset contains 5 sub-directories of food images.\n- biryani\n- burger\n- dosa\n- idly\n- pizza","8d7007e5":"**Use augumented data for training**"}}