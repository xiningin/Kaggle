{"cell_type":{"ff74f1ef":"code","bd6c408d":"code","f49c7d80":"code","68c021ce":"code","b10f0575":"code","a133a6a5":"code","906ca2e6":"code","effd4eda":"code","5c729da2":"code","05b010b5":"code","52a3121c":"code","a530523c":"markdown"},"source":{"ff74f1ef":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","bd6c408d":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense, Input, Bidirectional, SpatialDropout1D, Embedding, add, concatenate\nfrom tensorflow.keras.layers import GRU, GlobalAveragePooling1D, LSTM, GlobalMaxPooling1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nimport tensorflow_hub as hub\n\nimport tokenization","f49c7d80":"train = pd.read_csv('..\/input\/preprocesseddata\/train.csv')\ntest = pd.read_csv('..\/input\/preprocesseddata\/test.csv')\nsubmission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","68c021ce":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","b10f0575":"def bert_encoder(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n        \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segments_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segments_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","a133a6a5":"def build_model(bert_layer, max_len=512):\n \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    x = SpatialDropout1D(0.3)(sequence_output)\n    x = Bidirectional(GRU(LSTM_UNITS, return_sequences=True))(x)\n    x = Bidirectional(GRU(LSTM_UNITS, return_sequences=True))(x)\n    hidden = concatenate([GlobalMaxPooling1D()(x),GlobalAveragePooling1D()(x),])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n    result = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=result)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","906ca2e6":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","effd4eda":"train_input = bert_encoder(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encoder(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","5c729da2":"import gc\nNUM_MODELS = 1\n    \nBATCH_SIZE = 16\nLSTM_UNITS = 64\nEPOCHS = 10\nDENSE_HIDDEN_UNITS = 256\ncheckpoint_predictions = []\ncheckpoint_val_pred = []\nweights = []\n\nfor model_idx in range(NUM_MODELS):\n    model = build_model(bert_layer, max_len=160)\n    for global_epoch in range(EPOCHS):\n        model.fit(\n            train_input, train_labels,\n            batch_size=BATCH_SIZE,\n            validation_split=.25,\n            epochs=1,\n            verbose=1,\n            callbacks=[\n                LearningRateScheduler(lambda epoch: 2e-6 * (0.6** global_epoch))\n            ]\n        )\n        checkpoint_predictions.append(model.predict(test_input, batch_size=64).flatten())\n        weights.append(2 ** global_epoch)\n    del model\n    gc.collect()","05b010b5":"test_pred = np.average(checkpoint_predictions, weights=weights, axis=0)","52a3121c":"submission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","a530523c":"## Intro\nThis notebook is a combination of four great notebooks.\n* @xhlulu [Disaster NLP: Keras BERT using TFHub](https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub)\n* @Dieter [BERT-Embeddings + LSTM](https:\/\/www.kaggle.com\/christofhenkel\/bert-embeddings-lstm\/notebook)\n* @Wojtek Rosa [Keras BERT using TFHub (modified train data)](https:\/\/www.kaggle.com\/wrrosa\/keras-bert-using-tfhub-modified-train-data)\n* @Gunes Evitan [NLP with Disaster Tweets - EDA, Cleaning and BERT](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert)\n\nThanks to their great works. I combine the bert_model from @xhlulu, LSTM model from @Dieter and modified data from @Wojtek Rosa and @Gunes Evitan."}}