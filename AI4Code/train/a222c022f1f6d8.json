{"cell_type":{"4673dac2":"code","258e5636":"code","90c2cc40":"code","20de6466":"code","cb70f75c":"code","2be4ac89":"code","3d2cacd8":"code","ece60262":"code","27c36adf":"code","dd19dfcd":"code","3ad21c6a":"code","4efc08ee":"code","77b88da4":"code","59731d82":"code","1e1caf13":"code","6ab7ac44":"code","241f379a":"code","cdac58c1":"code","a3f02e8f":"code","fb507994":"code","0e58e914":"code","66774a76":"code","d272b9dc":"code","02c4d42f":"code","5f6c493a":"code","7d19dc17":"code","70173f94":"code","f3f94ca9":"code","131587c9":"code","361b551f":"code","e948153a":"code","197c60db":"code","de941049":"code","f2a91d4f":"code","6fbcebab":"code","a2fef8f6":"code","196179be":"code","d9a68865":"code","3d5418b7":"code","c9ff26ae":"code","e50b9324":"code","962dc4b3":"code","315616cb":"code","82629346":"code","8ab2319c":"code","b429680d":"code","df5773c0":"code","e419de5e":"code","23e44195":"code","407569dc":"code","33a359fb":"code","cd71c627":"code","f5c795a6":"code","53f6e275":"code","171be045":"code","37c8c858":"code","cbb9c047":"code","ae1af62e":"code","ba175e74":"code","92de4e73":"code","787af669":"code","8f8bdfa3":"code","d8bed68d":"code","20ca83e6":"code","82a36d5d":"code","a547fea8":"code","1527ca82":"code","02b7dcfa":"code","02734161":"code","e36463f1":"code","bb4798f8":"code","8a0843e5":"markdown","7b2a5e8d":"markdown","2c0eb93b":"markdown","59e47871":"markdown","f0e7e0ae":"markdown","66a268ff":"markdown","4fc592a8":"markdown","97fd6298":"markdown","677350c4":"markdown","226ca7e9":"markdown","a316b5a6":"markdown","d0048ae4":"markdown","b6f174d2":"markdown","a08af3b5":"markdown","c96f1160":"markdown","85d8f234":"markdown","1241ab82":"markdown","d92e074f":"markdown","93739b27":"markdown","db248a49":"markdown","44d6f131":"markdown","2fed2fe3":"markdown","b01272ed":"markdown","76d965f8":"markdown","b1293619":"markdown","80746d12":"markdown","88b97ef1":"markdown","38a05997":"markdown","0f673664":"markdown","b31cc132":"markdown","0625e490":"markdown","4475439e":"markdown","f46a5c6e":"markdown","b7aa7713":"markdown","86865d12":"markdown","8c8a55b3":"markdown","f639457d":"markdown","70fd3125":"markdown","d1c98606":"markdown","b95b3b54":"markdown","48d1e528":"markdown","ed36070b":"markdown","5244653e":"markdown","8a5357e3":"markdown","4b5691bd":"markdown","dbdeeee3":"markdown","5741d008":"markdown","4106673a":"markdown","433cad79":"markdown","db4b3047":"markdown","64713c1f":"markdown","2e12cf21":"markdown","4493a29f":"markdown","5511554e":"markdown","d4718576":"markdown","a131bc82":"markdown","164b0177":"markdown","c66e7fa3":"markdown"},"source":{"4673dac2":"#%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import BaggingClassifier,RandomForestClassifier,AdaBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,roc_auc_score,roc_curve,classification_report,f1_score\nfrom lightgbm import LGBMClassifier\nimport itertools\nimport scipy.stats as ss\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n","258e5636":"RS=410 #Random State","90c2cc40":"data=pd.read_csv('\/kaggle\/input\/caravan-insurance-challenge\/caravan-insurance-challenge.csv')\ndata.head()","20de6466":"data.shape","cb70f75c":"data.columns","2be4ac89":"data.ORIGIN.value_counts()","3d2cacd8":"data.info()","ece60262":"data.CARAVAN.value_counts()","27c36adf":"plt.subplots(figsize=(10,8))\nsns.heatmap(data.drop(columns=['ORIGIN']).corr());","dd19dfcd":"fig,axes=plt.subplots(1,2,figsize=(12,8))\nsns.heatmap(data.drop(columns=[\"ORIGIN\"]).iloc[:,:43].corr(),vmin=-1,vmax=1,cmap='coolwarm',ax=axes[0])\nsns.heatmap(data.drop(columns=['ORIGIN']).iloc[:,43:].corr(),vmin=-1,vmax=1,cmap='coolwarm',ax=axes[1])\naxes[0].set_title(\"Upper-left Corrplot\")\naxes[1].set_title(\"Bottom-right Corrplot\")","3ad21c6a":"#Drop percentage representations\ndata_np=data.drop(columns=data.loc[:,(data.columns.str.startswith('p'))]).copy()\ndata_np.to_feather('reduced_cmbd.df')","4efc08ee":"!pip install pyarrow","77b88da4":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False, cf_report=False,\n                          title='Confusion matrix', ax=None, cmap=plt.cm.Blues, cbar=False):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    if cf_report:\n        print(classification_report(y_true,y_pred))\n    \n    fig, ax = (plt.gcf(), ax) if ax is not None else plt.subplots(1,1)\n    \n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.set_title(title)\n    \n    if cbar:\n        fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04) # \"Magic\" numbers (https:\/\/stackoverflow.com\/a\/26720422\/10939610)\n    \n    tick_marks = np.arange(len(classes))\n    ax.set_xticks(tick_marks)\n    ax.set_xticklabels(classes, rotation=45)\n    ax.set_yticks(tick_marks)\n    ax.set_yticklabels(classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        ax.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    fig.tight_layout()\n    ax.set_ylabel('True label')\n    ax.set_xlabel('Predicted label')","59731d82":"def plot_roc(y_true, y_pred, ax=None):\n    \"\"\"Plot ROC curve\"\"\" \n    false_positive_rate, true_positive_rate, threshold = roc_curve(y_true, y_pred)\n    roc_score = roc_auc_score(y_true,y_pred)\n    \n    fig, ax = (plt.gcf(), ax) if ax is not None else plt.subplots(1,1)\n\n    ax.set_title(\"Receiver Operating Characteristic\")\n    ax.plot(false_positive_rate, true_positive_rate)\n    ax.plot([0, 1], ls=\"--\")\n    ax.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n    ax.annotate('ROC: {:.5f}'.format(roc_score), [0.75,0.05])\n    ax.set_ylabel(\"True Positive Rate\")\n    ax.set_xlabel(\"False Positive Rate\")\n    fig.tight_layout()\n    return roc_score","1e1caf13":"def feat_imps(model, X_train, plot=False, n=None):\n    \"\"\" Dataframe containing each feature with its corresponding importance in the given model\n    \n    Args\n    ----\n        model : model, classifier that supports .feature_importances_ (RandomForest, AdaBoost, ect..)\n        X_train : array like, training data object\n        plot : boolean, if True, plots the data in the form of a bargraph\n        n : int, only applicable if plot=True, number of features to plot, (default=15)\n        \n    Returns\n    -------\n        pandas DataFrame : columns = feature name, importance\n    \"\"\"\n    \n    fi_df = pd.DataFrame({'feature':X_train.columns,\n                          'importance':model.feature_importances_}\n                        ).sort_values(by='importance', ascending=False)\n    if plot:\n        fi_df[:(n if n is not None else 15)].plot.bar(x='feature',y='importance')\n    else:\n        return fi_df","6ab7ac44":"def plot_cmroc(y_true, y_pred, classes=[0,1], normalize=True, cf_report=False):\n    \"\"\"Convenience function to plot confusion matrix and ROC curve \"\"\"\n    fig,axes = plt.subplots(1,2, figsize=(9,4))\n    plot_confusion_matrix(y_true, y_pred, classes=classes, normalize=normalize, cf_report=cf_report, ax=axes[0])\n    roc_score = plot_roc(y_true, y_pred, ax=axes[1])\n    fig.tight_layout()\n    plt.show()\n    return roc_score","241f379a":"train_df=data.query(\"ORIGIN=='train'\").iloc[:,1:].copy()\ntest_df=data.query(\"ORIGIN=='test'\").iloc[:,1:].copy()","cdac58c1":"X,y=train_df.drop(columns='CARAVAN'),train_df.CARAVAN\nX_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.20,random_state=RS)","a3f02e8f":"!pip install imblearn","fb507994":"from imblearn.over_sampling import RandomOverSampler,SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.metrics import classification_report_imbalanced\nros=RandomOverSampler(random_state=RS)\nrus=RandomUnderSampler(random_state=RS)\nsmt=SMOTE(random_state=RS,n_jobs=-1)\nX_under,y_under=rus.fit_sample(X_train,y_train)\nX_over,y_over=ros.fit_sample(X_train,y_train)\nX_smote,y_smote=smt.fit_sample(X_train,y_train)\npd.DataFrame([*map(lambda x:ss.describe(x)._asdict(),[y_train,y_under,y_over,y_smote])],index=['Unbalanced','Undersample','Oversample','SMOTE'])","0e58e914":"#Define baseline models\nbc=BaggingClassifier(n_estimators=53,random_state=RS,n_jobs=-1)\nada=AdaBoostClassifier(n_estimators=53,random_state=RS)\nrfc=RandomForestClassifier(n_estimators=53,random_state=RS,n_jobs=-1)\nlgbm=LGBMClassifier(n_estimators=53,random_state=RS,n_jobs=-1)","66774a76":"bc_unbal=plot_cmroc(y_val,bc.fit(X_train,y_train).predict(X_val))","d272b9dc":"ada_unbal=plot_cmroc(y_val,ada.fit(X_train,y_train).predict(X_val))","02c4d42f":"lgbm_unbal=plot_cmroc(y_val,lgbm.fit(X_train,y_train).predict(X_val))","5f6c493a":"rfc_unbal=plot_cmroc(y_val,rfc.fit(X_train,y_train).predict(X_val))","7d19dc17":"models=[bc,ada,rfc,lgbm]\nunbal_scores=[bc_unbal,ada_unbal,rfc_unbal,lgbm_unbal]\nfor model,score in zip(models,unbal_scores):\n    print('{:25s}:{:.5f}'.format(model.__class__.__name__,score))","70173f94":"bc_under=plot_cmroc(y_val,bc.fit(X_under,y_under).predict(X_val))","f3f94ca9":"ada_under=plot_cmroc(y_val,ada.fit(X_under,y_under).predict(X_val))","131587c9":"lgbm_under=plot_cmroc(y_val,lgbm.fit(X_under,y_under).predict(X_val))","361b551f":"rfc_under=plot_cmroc(y_val,rfc.fit(X_under,y_under).predict(X_val))","e948153a":"models=[bc,ada,rfc,lgbm]\nunder_scores=[bc_under,ada_under,rfc_under,lgbm_under]\nfor model,score in zip(models,under_scores):\n    print('{:25s}:{:.5f}'.format(model.__class__.__name__,score))","197c60db":"bc_over=plot_cmroc(y_val,bc.fit(X_over,y_over).predict(X_val))","de941049":"ada_over=plot_cmroc(y_val,ada.fit(X_over,y_over).predict(X_val))","f2a91d4f":"lgbm_over=plot_cmroc(y_val,lgbm.fit(X_over,y_over).predict(X_val))","6fbcebab":"rfc_over=plot_cmroc(y_val,rfc.fit(X_over,y_over).predict(X_val))","a2fef8f6":"models=[bc,ada,rfc,lgbm]\nover_scores=[bc_over,ada_over,rfc_over,lgbm_over]\nfor model,score in zip(models,over_scores):\n    print('{:25s}:{:.5f}'.format(model.__class__.__name__,score))","196179be":"bc_smote=plot_cmroc(y_val,bc.fit(X_smote,y_smote).predict(X_val\n                                                         ))","d9a68865":"ada_smote=plot_cmroc(y_val,ada.fit(X_smote,y_smote).predict(X_val))","3d5418b7":"lgbm_smote=plot_cmroc(y_val,lgbm.fit(X_smote,y_smote).predict(X_val))","c9ff26ae":"rfc_smote=plot_cmroc(y_val,rfc.fit(X_smote,y_smote).predict(X_val))","e50b9324":"models=[bc,ada,rfc,lgbm]\nsmote_scores=[bc_smote,ada_smote,rfc_smote,lgbm_smote]\nfor model,score in zip(models,smote_scores):\n    print('{:25s}:{:.5f}'.format(model.__class__.__name__,score))","962dc4b3":"X_test,y_test=test_df.iloc[:,:-1],test_df.iloc[:,-1]","315616cb":"bc=BaggingClassifier(n_estimators=53,n_jobs=-1)\nada=AdaBoostClassifier(n_estimators=53,random_state=RS)\nrfc=RandomForestClassifier(n_estimators=53,n_jobs=-1,random_state=RS)\nlgbm=LGBMClassifier(n_estimators=53,random_state=RS)\n","82629346":"models=[bc,ada,rfc,lgbm]\nfor model in models:\n    model.fit(X_under,y_under)\n    tpreds=model.predict(X_test)\n    print('{:25s}:{:.5f}'.format(model.__class__.__name__,roc_auc_score(y_test,tpreds)))","8ab2319c":"from sklearn.model_selection import GridSearchCV\nparam_grid={\n    'learning_rate':[0.01,0.05,0.1,1],\n    'n_estimators':[20,40,60,80,100],\n    'num_values':[3,7,17,31],\n    'max_bin':[4,8,16,32,64],\n    'min_child_samples':[3,5,10,20,30],\n}","b429680d":"lgbm_gs=GridSearchCV(LGBMClassifier(),param_grid,n_jobs=-1,scoring='roc_auc',verbose=2,iid=False,cv=5)\nlgbm_gs.fit(X_under,y_under)\nprint('Best parameters:',lgbm_gs.best_params_)","df5773c0":"plot_cmroc(y_val,lgbm_gs.predict(X_val))","e419de5e":"plot_cmroc(y_test,lgbm_gs.predict(X_test))","23e44195":"param_grid_rf={\n    'n_estimators':[40,60,100,128,256],\n    'min_samples_leaf':[3,7,17,31],\n    'max_leaf_nodes':[4,8,16,32,64],\n    'min_samples_split':[3,5,10,20,30],\n}","407569dc":"rfc_gs=GridSearchCV(RandomForestClassifier(),param_grid_rf,n_jobs=-1,scoring='roc_auc',verbose=2,iid=False,cv=5)\nrfc_gs.fit(X_under,y_under)\nprint('Best parameters:',rfc_gs.best_params_)","33a359fb":"plot_cmroc(y_val,rfc_gs.predict(X_val))","cd71c627":"plot_cmroc(y_test,rfc_gs.predict(X_test))","f5c795a6":"lgbm_gs_ub=GridSearchCV(LGBMClassifier(),param_grid,n_jobs=-1,scoring='roc_auc',verbose=1,iid=False,cv=5)\nlgbm_gs_ub.fit(X_train,y_train)\nprint('Best parameters:',lgbm_gs_ub.best_params_)","53f6e275":"plot_cmroc(y_val,lgbm_gs_ub.predict(X_val))","171be045":"plot_cmroc(y_test,lgbm_gs_ub.predict(X_test))","37c8c858":"import warnings\nfrom sklearn.feature_selection import RFE,SelectKBest,chi2\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport statsmodels.api as sm\nimport scipy.stats as ss\nimport joblib\nfrom mlxtend.feature_selection import SequentialFeatureSelector\n\n","cbb9c047":"fi_data=data.drop(columns=['ORIGIN']).copy()\nX,y=fi_data.drop(columns='CARAVAN'),fi_data.CARAVAN","ae1af62e":"#plotting function\ndef scatter_density(data,labels,sca_title='',den_title=\"\",**kwargs):\n    \"\"\"plot a scatter plot and a density plot Args:\n             data:2-d array ,shape (n_samples,2)\n             labels:array-like,class labels to be used for coloring scatterplot\n              sca_title:str,scatter plot title\n              den_title:str,density plot title\n              **kwargs:keyword arguments passed to seaborn.\n              Kdeplot\n              Returns:\n                     ax,matplotlib axis object\"\"\"\n    fig,ax=plt.subplots(1,2,figsize=(10,4),sharey=True,sharex=True)\n    #,gridspec_kw={'width_ratios':[50,50,4]}\n    dataneg=data[labels==0]\n    datapos=data[labels==1]\n    sns.scatterplot(data[:,0],data[:,1],hue=labels,ax=ax[0])\n    #sns.scatterplot(dataneg[:,0],dataneg[:,1],palette='Blues',ax=ax[0],alpha=0.06)\n    #sns.scatterplot(datapos[:,0],datapos[:,1],palette='Oranges',ax=ax[0],alpha=1)\n    sns.kdeplot(datapos[:,0],datapos[:,1],ax=ax[1],cmap='Oranges',**kwargs)\n    sns.kdeplot(dataneg[:,0],dataneg[:,1],ax=ax[1],map='Blues',nlevels=30,**kwargs,shade=True,shade_lowest=False)#,cbar=True,cbar_ax=ax[2])\n    ax[0].set_title(sca_title)\n    ax[1].set_title(den_title)\n    fig.tight_layout()\n    plt.show()\n    return ax","ba175e74":"from sklearn.decomposition import PCA\nXs=pd.DataFrame(StandardScaler().fit_transform(X),columns=X.columns)\npca=PCA(random_state=RS)\nXpca=pca.fit_transform(Xs)","92de4e73":"\npca=PCA(random_state=RS)\n_Xpca_raw=PCA(n_components=2,random_state=RS).fit_transform(X)\nscatter_density(_Xpca_raw,y,'PCA Scatter Unscaled','PCA Density UnScaled');\n","787af669":"from sklearn.decomposition import PCA\nXs=pd.DataFrame(StandardScaler().fit_transform(X),columns=X.columns)\npca=PCA(random_state=RS)\nXpca=pca.fit_transform(Xs)\n\nXpca=pca.fit_transform(Xs)\nscatter_density(Xpca,y,'PCA Scaled:Scatter','PCA Scaled:Density');","8f8bdfa3":"pca.explained_variance_ratio_[:3]","d8bed68d":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.annotate('(64,0.993)',xy=(64,0.993),xytext=(64,0.8),fontsize='medium',arrowprops={'arrowstyle':'->','mutation_scale':15})\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title('Explained variance')\nplt.show()","20ca83e6":"!pip install openTSNE","82a36d5d":"from openTSNE import TSNE\nfrom openTSNE.callbacks import ErrorLogger\ntsne=TSNE(perplexity=75,learning_rate=500,n_iter=1000,metric='euclidean',negative_gradient_method='bh',n_jobs=4,callbacks=ErrorLogger(),random_state=RS)\nXembd=tsne.fit(Xs)","a547fea8":"scatter_density(Xembd,y,'t-SNE scatter','t-SNE density');","1527ca82":"!pip install 'umap-learn==0.3.10'","02b7dcfa":"import umap.umap_ as umap\n\nump=umap.UMAP(n_neighbors=30,min_dist=0.2,random_state=RS,verbose=True)\nXumap=ump.fit_transform(Xs,y)","02734161":"scatter_density(Xumap,y,'UMAP:Scatter','UMAP:Density')","e36463f1":"ump=umap.UMAP(n_neighbors=30,min_dist=0.2,random_state=RS,verbose=False)\nXumap=ump.fit_transform(Xs)","bb4798f8":"ump=umap.UMAP(n_neighbors=30,min_dist=0.2,random_state=RS,verbose=False)\nXumap=ump.fit_transform(Xs)\nscatter_density(Xumap,y,'UMAP:Scatter','UMAP:Density');","8a0843e5":"Boosting(LGBM)","7b2a5e8d":"Random Forest","2c0eb93b":"Imports","59e47871":"The test data will be treated as holdout test set,so we will split train_df into a training validation set.","f0e7e0ae":"Undersampling Evaluation","66a268ff":"Nearly a 18% increase in ROC score was seen across the board using the undersampling method.","4fc592a8":"# t-SNE(T-distributed Stochastic Neighbor Embedding)\n","97fd6298":"# Oversampling","677350c4":"Boosting(AdaBoost)","226ca7e9":"# PCA(principal Component analysis)","a316b5a6":"To address the issue with imbalanced data,we will compare three approaches for each model used:\n1.Random Over Sampling\n2.Random Under Sampling\n3.SMOTE(Synthetic Minority Over-Sampling Technique)\n","d0048ae4":"implement PCA without proper Scaling","b6f174d2":"PCA is effected by differences in magnitude well begin by scaling the data.","a08af3b5":"# Dimensionality Reduction and Features Analysis","c96f1160":"# Modelling on Unbalanced Data: Caravan Insurance","85d8f234":"Boosting(AdaBoost)\n","1241ab82":"Every feature is already encoded as an integer representation,saving us the conversion work.","d92e074f":"Boosting(AdaBoost)","93739b27":"**UMAP(Uniform manifold Approximation and Projection)**\n\nUMAP is relatively recent development in non-linear dimensionality reduction.","db248a49":"# SMOTE","44d6f131":"No NA values,all variables are type of int64.The data is peculiar in that every numeric stands for an attribute of person.Even variables that could be continuous,such as income have been binned.","2fed2fe3":"A correlation plot shows some interesting patterns in the data.There is a clear divide between the two groupings listed in the description file . ","b01272ed":"Unbalanced Data","76d965f8":"About 16% of variance can be explained by these first two abstract components.","b1293619":"So,if this contest happened to evaluated on Area Under ROC,the best model we could have submitted would have been the Random Forest Classifier with a score of 0.66","80746d12":"# SMOTE Evaluation","88b97ef1":"# Exploratory Data Analysis\n","38a05997":"Now we are dealing with the accurate representation of the data,an amorphous point mass.That is why it is so important to check that the assumptions are model are met,otherwise it is all too easy to head down a path leading to dead ends or inavalid conclusions.","0f673664":"Boosting(LGBM)","b31cc132":"Bagging","0625e490":"**MODELS**\n\n\n\n4 Models will be used in total:BaggingClassifier,RandomForestClassifier,AdaBoostClassifier from sklearn and Microsoft's lightgbm","4475439e":"Random Forest","f46a5c6e":"# Random Forest","b7aa7713":"Random Forest","86865d12":"# Undersampling","8c8a55b3":"The density plot shows a clear separation between two groups and even the scatter plot shows some degree of misleading grouping.Properly scaled ,things will look quite a bit different.\n","f639457d":"Data Analysis","70fd3125":"Boosting(LGBM)","d1c98606":"Without doing any sort of resampling,the mean was ~0.058 with heavy skew.Each method of resampling has shifted the mean to 0.5 and eliminated the skewness,each using a different method to achieve this.\n","b95b3b54":"Random Forest","48d1e528":"A bit better of a score likely be achieved through ensembling these models as well,but there are many other tweaks.","ed36070b":"Bagging","5244653e":"In contrast with the unbalanced dataset,with the over sampled data,AdaBoost greatly out performed the other models with this data augmentation method.","8a5357e3":"Bagging\n","4b5691bd":"Boosting(LGBM)","dbdeeee3":"For all of the classifiers,Random under sampling was the most successful method of rebalancing the dataset.With the exception of AdaBoost,the other methods barely outperformed random guessing.\n\nLet's evaluate the best from each group against the holdout test data ","5741d008":"Dimensionality reduction with:\n    \n    1.Principal Component Analysis(PCA)\n    2.t-SNE\n    3.UMAP\nAs well as a few methods to feature selection:\n   \n    1.Stepwise Feature selection\n    2.Recursive Feature elimination\n    3.Feature Importance Analysis\nPerformed on a Logistic Regressor and Random Forest Classifier","4106673a":"Boosting(AdaBoost)","433cad79":"# Oversampling Evaluation","db4b3047":"Bagging","64713c1f":"# Tweaking the best","2e12cf21":"Unbalanced Evaluation","4493a29f":"Although we do begin to see some small clusters taking shape.Depending on parameter choice,t-SNE has been shown to spuriously cluster.Highest density areas overlap between positive and negative saples and there are only a few small pockets where they have successfully separated.","5511554e":"Now,we are dealing with a very imbalanced dataset.","d4718576":"Poor performance across all models when using the unbalanced dataset.AdaBoost was no better than random guessing and the best model,the BaggingClassifier.","a131bc82":"# Dimensionality Reduction","164b0177":"# Grid Search","c66e7fa3":"after zooming in bit,Bottom-right corrplot shows how variables starting with P each have a corresponding variable starting with A this means that having both in our data will likely provide little value."}}