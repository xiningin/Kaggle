{"cell_type":{"671bab53":"code","848d9206":"code","0303d22a":"code","26b0faae":"code","0e875298":"code","88b3ba0e":"code","4426865c":"code","af19896f":"code","31354aa6":"code","850f2368":"code","2d6f0766":"code","c07471fa":"code","b9564dd7":"code","de85f383":"code","f3f4a280":"code","0ce70932":"code","2a6b3a14":"code","b8f8493f":"code","9f569901":"code","18ceaa44":"code","27391c6c":"code","eba9ddc0":"code","95c24182":"code","fa49a2a1":"code","0dde469d":"code","6356ba9f":"code","d2a26aa2":"code","26b16eea":"code","49cb5008":"code","0328c80a":"code","92e28155":"code","27ded2d2":"code","1db48ee5":"code","f88114dc":"code","d010ab64":"code","73c3a065":"code","b3def96f":"code","2b713244":"code","298d62e8":"code","f81baf27":"code","5c562f6b":"code","c242c88b":"code","0d100323":"code","93015d85":"code","643fc113":"code","d6b2b9be":"code","8845407c":"code","bc03e5bc":"code","f6793a21":"code","b97caf45":"code","3af0b9a0":"code","dc9688d9":"code","44ef7ab4":"code","3c10daf5":"code","2e7b34c7":"code","21491702":"code","35fdb003":"code","5d4a3e05":"markdown","c71294b1":"markdown"},"source":{"671bab53":"import os\nimport numpy as np\nimport pandas as pd \nfrom memory_profiler import profile\nfrom typing import List\n\nimport kaggle_uploader","848d9206":"!pip install memory_utils","0303d22a":"import memory_utils\n\nmemory_utils.print_memory()","26b0faae":"from tqdm.auto import tqdm\ntqdm.pandas()","0e875298":"!ls -l \/kaggle\/input\/covid-nlp-preprocess","88b3ba0e":"!ls -l \/kaggle\/","4426865c":"!du .","af19896f":"class COVDoc:\n    def __init__(self):\n        self.doc_id = None\n        self.filepath_proc = None\n        self.filepath_orig = None\n        self.text_proc = None\n        self.text_orig = None\n        self.tokenized_proc = None\n        self.doc_type = None\n    \n    #this function allows me to lazy-load the original text to save memory\n    def load_orig(self):\n            with open(self.filepath_orig) as f:\n                d = json.load(f)\n                body = \"\"\n                for idx, paragraph in enumerate(d[\"body_text\"]):\n                    body += f\" {paragraph}\"\n                self.text_orig = body","31354aa6":"class DocList:\n    doc_list: List[COVDoc] = None\n    doc_mode = True\n    full_text_mode = False\n    #this index will break if multiple iterations at the same time. then need separate object\n    iter_idx = 0\n    \n    def __init__(self):\n        self.doc_list = []\n        \n    def append(self, item:COVDoc):\n        self.doc_list.append(item)\n    \n    def __iter__(self):\n        iter_idx = 0\n        def doc_iterator(docs):\n            for doc in docs:\n                #doc_mode = mode where the whole document object is returned\n                if self.doc_mode:\n                    yield doc\n                    continue\n                #full_text_mode = mode where the full text is returned in one long string\n                if self.full_text_mode:\n                    tmp = \" \".join(doc.text_proc)\n                    yield tmp\n                    del tmp\n                    continue\n                #default mode if not doc_mode or full_text_mode, return whole document as list of words\n                yield doc.text_proc\n            return \n        return doc_iterator(self.doc_list)\n    \n    def __len__(self):\n        return len(self.doc_list)\n    \n\n    ","850f2368":"import glob, os, json\n\nparagraphs = []\n\ndef load_docs(base_path, base_path_orig, doc_type):\n    loaded_docs = DocList()\n    file_paths_proc = glob.glob(base_path)\n    file_names_proc = [os.path.basename(path) for path in file_paths_proc]\n    file_names_orig = [os.path.splitext(filename)[0]+\".json\" for filename in file_names_proc]\n    file_paths_orig = []\n    for filename in file_names_orig:\n        if filename.startswith(\"PMC\"):\n            file_paths_orig.append(os.path.join(base_path_orig, \"pmc_json\", filename))\n        else:\n            file_paths_orig.append(os.path.join(base_path_orig, \"pdf_json\", filename))\n#        file_paths_orig = [os.path.join(base_path_orig, filename) for filename in file_names_orig]\n    for idx, filepath_proc in enumerate(tqdm(file_paths_proc)):\n        doc = COVDoc()\n        doc.doc_type = doc_type\n        loaded_docs.append(doc)\n        doc.filepath_proc = filepath_proc\n        doc.filepath_orig = file_paths_orig[idx]\n        with open(filepath_proc) as f:\n            d = f.read()\n            #print(d)\n            tokenized = d.strip().split(\" \")\n            tokenized[0] = tokenized[0].strip()\n            doc.doc_id = tokenized[0]\n            del tokenized[0]\n            doc.text_proc = tokenized\n    return loaded_docs","2d6f0766":"memory_utils.print_memory()","c07471fa":"!ls -l \/kaggle\/input\/covid-nlp-preprocess\/output\/whole","b9564dd7":"df_metadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\ndf_metadata.head()","de85f383":"med_docs = load_docs(\"\/kaggle\/input\/covid-nlp-preprocess\/output\/whole\/biorxiv_medrxiv\/*.txt\", \"\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\", \"medx\")\nlen(med_docs)","f3f4a280":"memory_utils.print_memory()","0ce70932":"comuse_docs = load_docs(\"\/kaggle\/input\/covid-nlp-preprocess\/output\/whole\/comm_use_subset\/*.txt\", \"\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\", \"comm_user\")\nlen(comuse_docs)","2a6b3a14":"memory_utils.print_memory()","b8f8493f":"noncom_docs = load_docs(\"\/kaggle\/input\/covid-nlp-preprocess\/output\/whole\/noncomm_use_subset\/*.txt\", \"\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\", \"noncomm\")\nlen(noncom_docs)","9f569901":"memory_utils.print_memory()","18ceaa44":"custom_docs = load_docs(\"\/kaggle\/input\/covid-nlp-preprocess\/output\/whole\/custom_license\/*.txt\", \"\/kaggle\/input\/CORD-19-research-challenge\/custom_license\/custom_license\", \"custom\")\nlen(custom_docs)","27391c6c":"memory_utils.print_memory()","eba9ddc0":"all_doc_lists = [med_docs, comuse_docs, noncom_docs, custom_docs]","95c24182":"memory_utils.print_memory()","fa49a2a1":"total_docs = 0\nfor doc_list in all_doc_lists:\n    total_docs += len(doc_list)\ntotal_docs","0dde469d":"import collections\n\ndef count_words():\n    word_count = collections.Counter()\n    with tqdm(total=total_docs) as pbar:\n        for doc_list in all_doc_lists:\n            doc_list.doc_mode = False\n            for doc_text in doc_list:\n                word_count.update(doc_text)\n                pbar.update()\n    return word_count\n\nword_count = count_words()\nlen(word_count)","6356ba9f":"memory_utils.print_memory()","d2a26aa2":"def redo_docs():\n    #all_texts = []\n    with tqdm(total=total_docs) as pbar:\n        for doc_list in all_doc_lists:\n            doc_list.doc_mode = True\n            for doc in doc_list:\n                text = []\n                for token in doc.text_proc:\n                    if word_count[token] < 20:\n                        splits = token.split(\"_\")\n                        if len(splits) > 1:\n                            text.extend(splits)\n                    else:\n                        text.append(token)\n                #text = [token for token in doc.text_proc if word_count[token] > 20]\n                doc.text_proc = text\n                pbar.update()\n            #all_texts.extend(\" \".join(doc.text_proc) for doc in doc_list)\n    #return all_texts\n\n#redo_docs()","26b16eea":"memory_utils.print_memory()","49cb5008":"#redo_docs()\nword_count = count_words()\nlen(word_count)","0328c80a":"memory_utils.print_memory()","92e28155":"all_docs = DocList()\nall_docs.doc_mode = False\nall_docs.full_text_mode = True\nfor doc_list in all_doc_lists:\n    doc_list.doc_mode = True\n    all_docs.doc_list.extend(doc_list)\n","27ded2d2":"all_docs.doc_list[0]","1db48ee5":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#https:\/\/stackoverflow.com\/questions\/34449127\/sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen?noredirect=1&lq=1\nvect = TfidfVectorizer()\ntfidf_matrix = vect.fit_transform(all_docs)\nfeature_names = vect.get_feature_names()\n#TODO: tqdm in custom iterator","f88114dc":"memory_utils.print_memory()","d010ab64":"def weights_for_doc(doc_idx):\n    feature_index = tfidf_matrix[doc_idx , :].nonzero()[1]\n    tfidf_scores = zip(feature_index, [tfidf_matrix[doc_idx, x] for x in feature_index])\n    return tfidf_scores\n","73c3a065":"word_count[\"bob\"]","b3def96f":"threshold_sizes = []\nfor x in tqdm(range(200)):\n    n_of_words = 0\n    for word, count in word_count.items():\n        if count > x:\n            n_of_words += 1\n    threshold_sizes.append(n_of_words)\n    ","2b713244":"threshold_df = pd.DataFrame()\nthreshold_df[\"words\"] = threshold_sizes\nthreshold_df.plot()","298d62e8":"pd.set_option('display.max_rows', 500)\nthreshold_df.head(10)\n","f81baf27":"index_threshold = 100","5c562f6b":"import collections\n\nword_count = collections.Counter()\nall_docs.full_text_mode = False\nall_docs.doc_mode = False\nfor doc_text in tqdm(all_docs):\n    word_count.update(doc_text)\nlen(word_count)\n","c242c88b":"all_docs.full_text_mode = False\nall_docs.doc_mode = True\nall_doc_ids = [doc.doc_id for doc in all_docs]\nall_doc_ids[:10]","0d100323":"memory_utils.print_memory()","93015d85":"from collections import defaultdict\n\ni_index = defaultdict(list)\n\nskipped = 0\nnot_skipped = 0\nfor idx in tqdm(range(len(all_docs))):\n    tfidf_scores = weights_for_doc(idx)\n    #weighted_features = []\n    for w, s in [(feature_names[i], s) for (i, s) in tfidf_scores]:\n        wc = word_count[w]\n        if wc < index_threshold:\n            #reduce size of index or it will not fit in memory\n            skipped += 1\n            continue\n        not_skipped += 1\n        n_s = np.float32(s)\n        n_idx = np.uint16(idx)\n        i_index[w].append((n_idx, n_s))\n#        weighted_features.append((w,s))\n    del tfidf_scores\n#print(tfidf_scores)\nprint(f\"skipped {skipped} features, used {not_skipped} features\")\n#TODO: numpy arrays","643fc113":"memory_utils.print_memory()","d6b2b9be":"from operator import itemgetter\n\ncount = 0\nfor word_list in tqdm(i_index.values()):\n    word_list.sort(key=itemgetter(1), reverse=True)\n    count += 1\n","8845407c":"memory_utils.print_memory()","bc03e5bc":"for word in tqdm(i_index):\n    np_scores = np.array(i_index[word])\n    i_index[word] = np_scores","f6793a21":"memory_utils.print_memory()","b97caf45":"memory_utils.print_memory()","3af0b9a0":"i_index[\"patient\"][:20][0][0]","dc9688d9":"!mkdir upload_dir","44ef7ab4":"import pickle\n\nwith open(\"upload_dir\/tfidf_matrix.pickle\", \"wb\") as f:\n    pickle.dump(tfidf_matrix, f)\n    \nmemory_utils.print_memory()\n\n!ls -l upload_dir","3c10daf5":"with open(\"upload_dir\/feature_names.pickle\", \"wb\") as f:\n    pickle.dump(feature_names, f)#\n\nmemory_utils.print_memory()\n\n!ls -l upload_dir","2e7b34c7":"with open(\"upload_dir\/i_index.pickle\", \"wb\") as f:\n    pickle.dump(i_index, f)\n\nmemory_utils.print_memory()\n\n!ls -l upload_dir","21491702":"with open(\"upload_dir\/doc_ids.pickle\", \"wb\") as f:\n    pickle.dump(all_doc_ids, f)\n\nmemory_utils.print_memory()\n\n!ls -l upload_dir","35fdb003":"import kaggle_uploader\n\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\napi_secret = user_secrets.get_secret(\"kaggle api key\")\n\nkaggle_uploader.resources = []\nkaggle_uploader.init_on_kaggle(\"donkeys\", api_secret)\nkaggle_uploader.base_path = \".\/upload_dir\"\nkaggle_uploader.title = \"COVID TF-IDF\"\nkaggle_uploader.dataset_id = \"covid-tfidf\"\nkaggle_uploader.user_id = \"donkeys\"\nkaggle_uploader.add_resource(\"doc_ids.pickle\", \"pickled doc ids for TF-IDF outputs\")\nkaggle_uploader.add_resource(\"tfidf_matrix.pickle\", \"pickled TF-IDF matrix for covid19 dataset\")\nkaggle_uploader.add_resource(\"feature_names.pickle\", \"pickled TF-IDF features names\")\nkaggle_uploader.add_resource(\"i_index.pickle\", \"pickled inverted TF-IDF index for covid19 dataset\")\nkaggle_uploader.update(\"new version from kernel\")\n#kaggle_uploader.update(\"new version\")\n","5d4a3e05":"# TF-IDF dataset creation for COVID19\n\nThis kernel takes my [pre-processed COVID19 NLP dataset](https:\/\/www.kaggle.com\/donkeys\/covid-nlp-preprocess) and calculates TF-IDF scores for the documents in that set. It also creates an inverted index for the same data. See the [dataset example notebook](https:\/\/www.kaggle.com\/donkeys\/starter-covid-tf-idf-and-inverse-index-2707b22c-a) for more details on what it produces.\n\nThe resulting models are saved as a [new dataset](https:\/\/www.kaggle.com\/donkeys\/covid-tfidf) for future kernels.","c71294b1":"Based on above plots and table, I will pick a number to reduce the size of the inverted index. For the use case in mind, it seems fine for me to go with 100 or even higher for minumum word count. A threshold of 100 should reduce the index size to about 1\/7th of full size. Otherwise this kernel keeps running out of memory and downstream kernels would have big issues as well.."}}