{"cell_type":{"2cea9a24":"code","d5269bc5":"code","1480b5cf":"code","1de0026b":"code","8b59101a":"code","d28590eb":"code","d5d505d8":"code","127e78ed":"code","a16726c9":"code","39262771":"code","c4ddcf02":"code","f4755bf4":"code","c0be35df":"code","9964acd4":"code","9c112731":"code","6af41261":"code","0a6909e9":"code","4145130a":"code","d15e76e4":"code","076ae9fa":"code","b15eabc2":"code","93ece1e2":"code","d3e36d87":"code","6950c0e4":"code","90e8a1a6":"code","d22ca800":"markdown","bd464c52":"markdown","28072442":"markdown","318eb2a6":"markdown","08874d59":"markdown","fa7f9e09":"markdown","818aadd7":"markdown","5203e960":"markdown","a8b2b358":"markdown"},"source":{"2cea9a24":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom nltk.tokenize import TweetTokenizer\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.multiclass import OneVsRestClassifier\npd.set_option('max_colwidth',400)","d5269bc5":"train = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv', sep=\"\\t\")\ntest = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv', sep=\"\\t\")\nsub = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/sampleSubmission.csv', sep=\",\")","1480b5cf":"train.head(10)","1de0026b":"train.loc[train.SentenceId == 2]","8b59101a":"print('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))","d28590eb":"print('Number of phrases in train: {}. Number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}. Number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))","d5d505d8":"print('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","127e78ed":"text = ' '.join(train.loc[train.SentenceId == 4, 'Phrase'].values)\ntext = [i for i in ngrams(text.split(), 3)]","a16726c9":"Counter(text).most_common(20)","39262771":"tokenizer = TweetTokenizer()","c4ddcf02":"vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Phrase'].values) + list(test['Phrase'].values)\nvectorizer.fit(full_text)\ntrain_vectorized = vectorizer.transform(train['Phrase'])\ntest_vectorized = vectorizer.transform(test['Phrase'])","f4755bf4":"y = train['Sentiment']","c0be35df":"logreg = LogisticRegression()\novr = OneVsRestClassifier(logreg)","9964acd4":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","9c112731":"tk = Tokenizer(lower = True, filters='')\ntk.fit_on_texts(full_text)","6af41261":"train_tokenized = tk.texts_to_sequences(train['Phrase'])\ntest_tokenized = tk.texts_to_sequences(test['Phrase'])","0a6909e9":"max_len = 50\nX_train = pad_sequences(train_tokenized, maxlen = max_len)\nX_test = pad_sequences(test_tokenized, maxlen = max_len)","4145130a":"embedding_path = \"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"","d15e76e4":"embed_size = 300\nmax_features = 20000","076ae9fa":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tk.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","b15eabc2":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y.values.reshape(-1, 1))","93ece1e2":"file_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\ndef build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(128,activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(100,activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 15, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    return model","d3e36d87":"model = build_model(lr = 1e-4, lr_d = 0, units = 128, dr = 0.5)","6950c0e4":"pred = model.predict(X_test, batch_size = 1024)","90e8a1a6":"predictions = np.round(np.argmax(pred, axis=1)).astype(int)\n# for blending if necessary.\n#(ovr.predict(test_vectorized) + svc.predict(test_vectorized) + np.round(np.argmax(pred, axis=1)).astype(int)) \/ 3\nsub['Sentiment'] = predictions\nsub.to_csv(\"blend.csv\", index=False)","d22ca800":"Well... I don't think that they are really positive.","bd464c52":"## LSTM-CNN Model\n\nOur CNN-LSTM model consists of an initial LSTM layer which will receive word embeddings for each token in the tweet as inputs. The intuition is that its output tokens will store information not only of the initial token, but also any previous tokens; In other words, the LSTM layer is generating a new encoding for the original input. The output of the LSTM layer is then fed into a convolution layer which we expect will extract local features. Finally the convolution layer\u2019s output will be pooled to a smaller dimension and ultimately outputted as either a positive or negative label.\n\n![image.png](attachment:image.png)","28072442":"# Combined LSTM-CNN Models","318eb2a6":"So, we have only phrases as data. And a phrase can contain a single word. And one punctuation mark can cause phrase to receive a different sentiment. Also assigned sentiments can be strange. This means several things:\n- using stopwords can be a bad idea, especially when phrases contain one single stopword;\n- puntuation could be important, so it should be used;\n- ngrams are necessary to get the most info from data;\n- using features like word count or sentence length won't be useful;","08874d59":"### Thoughts on feature processing and engineering","fa7f9e09":"## Intuition: Why CNNs and LSTMs?\n\nBefore starting, let's give a brief introduction to these networks along with a short analysis of why I thought they would benefit my sentiment analysis task.\n## CNNs\n\nConvolutional Neural Networks (CNNs) are networks initially created for image-related tasks that can learn to capture specific features regardless of locality.\n\nFor a more concrete example of that, imagine we use CNNs to distinguish pictures of Cars vs. pictures of Dogs. Since CNNs learn to capture features regardless of where these might be, the CNN will learn that cars have wheels, and every time it sees a wheel, regardless of where it is on the picture, that feature will activate.\n\nIn our particular case, it could capture a negative phrase such as \"don't like\" regardless of where it happens in the tweet.\n\n*     I don't like watching those types of films\n*     That's the one thing I really don't like.\n*     I saw the movie, and I don't like how it ended.\n\n![image.png](attachment:image.png)\n\n\n## LSTMs\n\nLong-Term Short Term Memory (LSTMs) are a type of network that has a memory that \"remembers\" previous data from the input and makes decisions based on that knowledge. These networks are more directly suited for written data inputs, since each word in a sentence has meaning based on the surrounding words (previous and upcoming words).\n\nIn our particular case, it is possible that an LSTM could allow us to capture changing sentiment in a tweet. For example, a sentence such as: At first I loved it, but then I ended up hating it. has words with conflicting sentiments that would end-up confusing a simple Feed-Forward network. The LSTM, on the other hand, could learn that sentiments expressed towards the end of a sentence mean more than those expressed at the start.\n\n","818aadd7":"## CNN-LSTM Model\n\nThe first model I tried was the CNN-LSTM Model. Our CNN-LSTM model combination consists of an initial convolution\nlayer which will receive word embeddings as input. Its output will then be pooled to a smaller dimension which is then fed into an LSTM layer. The intuition behind this model is that the convolution layer will extract local features and the LSTM layer will then be able to use the ordering of said features to learn about the input\u2019s text ordering. In practice, this model is not as powerful as our other LSTM-CNN model proposed.\n\n![image.png](attachment:image.png)","5203e960":"Let's see for example most common trigrams for positive phrases","a8b2b358":"Source\n> http:\/\/konukoii.com\/blog\/2018\/02\/19\/twitter-sentiment-analysis-using-combined-lstm-cnn-models\/"}}