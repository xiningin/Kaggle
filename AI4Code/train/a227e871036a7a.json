{"cell_type":{"9344ba84":"code","0ae918c1":"code","cdbba5fc":"code","5729fd51":"code","857b56eb":"code","952f2003":"code","7436edba":"code","875f65d0":"code","1c8d2e20":"code","e8c65b7e":"code","3cf96ea5":"code","a3ccc2e7":"code","e7b1393a":"code","2d1e7efb":"markdown","03116670":"markdown","b932b915":"markdown","35c7758c":"markdown","eff634d2":"markdown","e61ef50a":"markdown","590bbd3e":"markdown","39fcc2ad":"markdown","4fffb55d":"markdown","5c8aaf94":"markdown","c828688e":"markdown","b3e490ab":"markdown"},"source":{"9344ba84":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, plot_confusion_matrix, classification_report\nfrom scipy import stats\n\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0ae918c1":"directory = \"\/kaggle\/input\/airline-passenger-satisfaction\/\"\nfeature_tables = ['train.csv', 'test.csv']\n\ndf_train = directory + feature_tables[0]\ndf_test = directory + feature_tables[1]\n\n# Create dataframes\nprint(f'Reading csv from {df_train}...')\ntrain = pd.read_csv(df_train)\nprint('...Complete')\n\nprint(f'Reading csv from {df_train}...')\ntest = pd.read_csv(df_test)\nprint('...Complete')","cdbba5fc":"train.head()","5729fd51":"def transform_gender(x):\n    if x == 'Female':\n        return 1\n    elif x == 'Male':\n        return 0\n    else:\n        return -1\n    \ndef transform_customer_type(x):\n    if x == 'Loyal Customer':\n        return 1\n    elif x == 'disloyal Customer':\n        return 0\n    else:\n        return -1\n    \ndef transform_travel_type(x):\n    if x == 'Business travel':\n        return 1\n    elif x == 'Personal Travel':\n        return 0\n    else:\n        return -1\n    \ndef transform_class(x):\n    if x == 'Business':\n        return 2\n    elif x == 'Eco Plus':\n        return 1\n    elif x == 'Eco':\n        return 0    \n    else:\n        return -1\n    \ndef transform_satisfaction(x):\n    if x == 'satisfied':\n        return 1\n    elif x == 'neutral or dissatisfied':\n        return 0\n    else:\n        return -1\n    \ndef process_data(df):\n    df = df.drop(['Unnamed: 0', 'id'], axis = 1)\n    df['Gender'] = df['Gender'].apply(transform_gender)\n    df['Customer Type'] = df['Customer Type'].apply(transform_customer_type)\n    df['Type of Travel'] = df['Type of Travel'].apply(transform_travel_type)\n    df['Class'] = df['Class'].apply(transform_class)\n    df['satisfaction'] = df['satisfaction'].apply(transform_satisfaction)\n    df['Arrival Delay in Minutes'].fillna(df['Arrival Delay in Minutes'].median(), inplace = True)\n    \n    return df\n\ntrain = process_data(train)\ntest = process_data(test)","857b56eb":"train.head()","952f2003":"#Define our features and target (this is helpful in case you would like to drop any features that harm model performance)\nfeatures = ['Gender', 'Customer Type', 'Age', 'Type of Travel', 'Class',\n       'Flight Distance', 'Inflight wifi service',\n       'Departure\/Arrival time convenient', 'Ease of Online booking',\n       'Gate location', 'Food and drink', 'Online boarding', 'Seat comfort',\n       'Inflight entertainment', 'On-board service', 'Leg room service',\n       'Baggage handling', 'Checkin service', 'Inflight service',\n       'Cleanliness', 'Departure Delay in Minutes', 'Arrival Delay in Minutes']\ntarget = ['satisfaction']\n\n# Split into test and train\nX_train = train[features]\ny_train = train[target].to_numpy()\nX_test = test[features]\ny_test = test[target].to_numpy()\n\n# Normalize Features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","7436edba":"corr = train.corr(method='spearman')\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 18))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, annot = True, mask=mask, cmap=\"YlGnBu\", center=0,\n            square=True, linewidths=.5)","875f65d0":"def run_model(model, X_train, y_train, X_test, y_test, verbose=True):\n    if verbose == False:\n        model.fit(X_train,y_train, verbose=0)\n    else:\n        model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    roc_auc = roc_auc_score(y_test, y_pred)\n    print(\"ROC_AUC = {}\".format(roc_auc))\n    print(classification_report(y_test,y_pred,digits=5))\n    plot_confusion_matrix(model, X_test, y_test,cmap=plt.cm.Blues, normalize = 'all')\n    \n    return model, roc_auc","1c8d2e20":"params_rf = {'max_depth': 25,\n         'min_samples_leaf': 1,\n         'min_samples_split': 2,\n         'n_estimators': 1200,\n         'random_state': 42}\n\nmodel_rf = RandomForestClassifier(**params_rf)\nmodel_rf, roc_auc_rf = run_model(model_rf, X_train, y_train, X_test, y_test)","e8c65b7e":"params_lgb ={'colsample_bytree': 0.85, \n         'max_depth': 15, \n         'min_split_gain': 0.1, \n         'n_estimators': 200, \n         'num_leaves': 50, \n         'reg_alpha': 1.2, \n         'reg_lambda': 1.2, \n         'subsample': 0.95, \n         'subsample_freq': 20}\n\nmodel_lgb = lgb.LGBMClassifier(**params_lgb)\nmodel_lgb, roc_auc_lgb = run_model(model_lgb, X_train, y_train, X_test, y_test)","3cf96ea5":"params_cb ={}\n\nmodel_cb = cb.CatBoostClassifier(**params_cb)\nmodel_cb, roc_auc_cb = run_model(model_cb, X_train, y_train, X_test, y_test, verbose=False)","a3ccc2e7":"params_xgb ={}\n\nmodel_xgb = xgb.XGBClassifier(**params_xgb)\nmodel_xgb, roc_auc_xgb = run_model(model_xgb, X_train, y_train, X_test, y_test)","e7b1393a":"auc_scores = [roc_auc_rf, roc_auc_lgb, roc_auc_cb, roc_auc_xgb]\nmodel_scores = pd.DataFrame(auc_scores, index=['Random Forest','LightGBM','Catboost','XGBoost'], columns=['AUC'])\nmodel_scores.head()","2d1e7efb":"## Can you improve on my best models performance? What factors led instances of mis-classification? Try new things!\n\n## UPVOTE if you found this helpful, insightful, intriquing, interesting, or other adjectives!","03116670":"### LightGBM\n\nNot bad LightGBM! But not quite up at 'Random Forest' levels. Lets keep going!","b932b915":"## Clean and Transform Dataset\nBased on the training data above, there are several things we need to do in order to prepare the data for use in a model. There are several catagorical variables that need to be encoded, including our target variable 'Satisfaction'. There are also a couple of columns that are unnecessary, such as 'Unnamed:0' and 'id'. We can drop these. The functions below will be used to perform the dataset preparation.","35c7758c":"Nice! We now have every feature properly coded, almost time to start creating some models! But first, we will want to normalize our dataset via StandardScaler. Even if the model you are using does not care about a normalized dataset, I still like to do it just out of routine.","eff634d2":"### Random Forest \n\nA great place to start! We are seeing an accuracy of 96.30%. Lets see if the other models can compare.","e61ef50a":"# **Predicting Customer Satisfaction**\n## *Given survey data from an Airline, can we predict the satisfaction of a customer? *\n![image.png](attachment:image.png)\n\nWelcome! This kernels aim is to showcase a few of the basics of data science (data cleaning, encoding, feature engineering, and model training), all while attempting to solve a problem that is common among businesses: *Customer Satisfaction*. \n\nWe will be treating this as a binary classification problem, where we will attemp to create a model that predicts whether a customer was **Satisfied** or **Unsatisfied** with the experience and\/or service which an airline provided.\n\n**If you find this kernel helpful, please UPVOTE**\n\nV10 - Fixed bug where test data was being set to train data\n\nV9 - Initial Public Kernel\n","590bbd3e":"### XGBoost\n\nI expected more from you XGBoost ;)","39fcc2ad":"## Model Evaluation\n\nNow to the models! We will be trying out a few different models to see which one is the best choice for our problem. I have created a small function below which will train, predict, and evaluate all of our models. We will be evaluating performance of our models with the ROC_AUC metric. This metric is good for classification of a dataset which a relatively balance dataset in terms of our target. We will also be looking at the confusion matrix for our model to best understand how our model is mischaracterizing predictions (Are we seeing majority false positives? etc.) \n\nNote that I did some hyperparameter tuning on some of the models, but not all. I will be publishing another kernel showing how to tune hyperparameters, so stay tuned!","4fffb55d":"## Model Comparison and Conclusions\n\nBased on the roc_auc metric, we have a clean winner. Our Random forest classifier outperformed all of our other models. One thing to keep in mind is that both the Catboost and the XGBoost classifiers just used default parameters without any hyperparameter tuning. ","5c8aaf94":"Normalization Complete! Also, just for kicks, lets take a look at a correlation heatmap to see which features correlate well with customer satisfaction.\n* **Best features** - Online Booking, Class, and Type of Travel\n* **Worst features** - Gate location, Gender, and Departure\/Arrival Time Convenient\n\nWe will keep all of our features in for now, but can always return to this step in case we want to drop features.","c828688e":"### Catboost\n\nPretty good for not having tuned hyperparameters here. Expect a performance boost once that is completed.","b3e490ab":"### Notes about the Dataset\nThis dataset has already been split into train and test csv files. 80% of the total dataset is in train.csv and 20% is in test.csv"}}