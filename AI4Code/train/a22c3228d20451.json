{"cell_type":{"f8980ba6":"code","12607c13":"code","0aa2900e":"code","27d6de91":"code","8a27971b":"code","ea3756d4":"code","f506eb47":"code","e4077968":"code","a32dd031":"code","01e2759d":"code","a783a48c":"code","42787694":"code","9d669076":"code","7cb99833":"code","61ba12be":"code","e11aa9ae":"code","bf402df5":"code","578e4a55":"code","2f360d0f":"code","8b9abc85":"code","71be3353":"code","594ee9b5":"code","ce463433":"code","73abfa6b":"code","708b4d4f":"code","f4de0e03":"code","6350ea56":"code","2d63870b":"code","b5145b76":"code","3c76382c":"code","5f985d1b":"code","13427861":"markdown","3d637b51":"markdown","96fd98be":"markdown","b6c46ee3":"markdown","2849e91e":"markdown","9ed9156a":"markdown","32681d8a":"markdown","52762915":"markdown","e1409e5b":"markdown","cf5639f1":"markdown","6b493ccc":"markdown","00564e9c":"markdown","9c9a36b3":"markdown","8617b608":"markdown","d5b9a7ee":"markdown","53caecb2":"markdown","975f17db":"markdown","1235278f":"markdown","67bb6cad":"markdown","49bbeb79":"markdown","685383cb":"markdown","88b957b1":"markdown"},"source":{"f8980ba6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","12607c13":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set() \n%matplotlib inline\n\ndf_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv', index_col=0)\ndf_train.head()","0aa2900e":"df_train.info()","27d6de91":"df_train.isna().sum()","8a27971b":"print(df_train.target.value_counts())\n\nsns.catplot(data=df_train, x='target', kind='count')\nplt.title('Distribution Target class');","ea3756d4":"print(\"Example of a Disaster Tweet (target=1):\")\nprint(df_train.text[df_train.target == 1].iloc[0])\n\nprint(\"\\nExample of a normal Tweet (target=0):\")\nprint(df_train.text[df_train.target == 0].iloc[0])","f506eb47":"print(f\"There are {len(df_train.keyword.unique())} unique keywords.\")","e4077968":"df_keywords = pd.concat([df_train, pd.get_dummies(df_train.target)], axis=1).groupby('keyword').sum().drop('target', axis=1)\ndf_keywords['total'] = df_keywords.apply(sum, axis=1)\ndf_keywords = df_keywords.rename(columns={0:'target_0', 1:'target_1' })\ndf_keywords","a32dd031":"df_keywords.sort_values(by='target_1', ascending=False)[:20]","01e2759d":"df_keywords.sort_values(by='target_0', ascending=False)[:20]","a783a48c":"df_train[['keyword','text', 'target']].groupby(['keyword', 'target']).count()","42787694":"df_keywords[df_keywords.target_0 ==0]","9d669076":"df_keywords[df_keywords.target_1 ==0]","7cb99833":"df_train.location.value_counts()","61ba12be":"df_train.location.value_counts()[:20]","e11aa9ae":"import re\n\ndef remove_state(text):\n    return re.sub(', [A-Z]{2}', '', text)\n\ndf_train['location'] = df_train.location[df_train.location.notna()].apply(lambda x: remove_state(str(x))).apply(lambda x: str(x).lower())","bf402df5":"df_train.location.value_counts()[:20]","578e4a55":"df_train.location[df_train.location.str.contains('york', na=False)].value_counts()","2f360d0f":"df_train['text_len'] = df_train['text'].apply(len)\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split(' ')))\ndf_train.head()","8b9abc85":"fig, axes = plt.subplots(2,3, figsize=(18,14))\n\nsns.kdeplot(data=df_train, x='word_count', hue='target', ax=axes[0, 0])\nsns.histplot(data=df_train, x='word_count', hue='target', bins=20, multiple=\"stack\", ax=axes[0,1])\nsns.boxplot(data=df_train, x='target', y='word_count', ax=axes[0,2])\n\nsns.kdeplot(data=df_train, x='text_len', hue='target', ax=axes[1, 0])\nsns.histplot(data=df_train, x='text_len', hue='target', bins=20, multiple=\"stack\", ax=axes[1,1])\nsns.boxplot(data=df_train, x='target', y='text_len', ax=axes[1,2]);","71be3353":"plt.figure(figsize=(12,10))\nsns.scatterplot(data=df_train, x='text_len', y='word_count');","594ee9b5":"import string\nfrom collections import defaultdict, Counter\n\npunct = string.punctuation\nprint(punct)\n\ntext = ' '.join(df_train.text)\ndict_punct = defaultdict(int)\n\nfor w in text:\n    if w in punct:\n        dict_punct[w] +=1\n\ndf_punct = pd.DataFrame(sorted(dict_punct.items(), key=lambda x: x[1], reverse=True), columns=['punctuation', 'count'])\n\nsns.barplot(data=df_punct, x='punctuation', y='count');","ce463433":"# Separate Dataframe in two \ndf_disaster = df_train[df_train['target'] == 1]\ndf_normal = df_train[df_train['target'] == 0]\n\n# Create two text cariables from each previous dataframes\ntext_disaster = ' '.join(df_disaster.text)\ntext_normal = ' '.join(df_normal.text)\n\ndict_punct_1 = Counter([w for w in text_disaster if w in punct])\ndict_punct_0 = Counter([w for w in text_normal if w in punct])\n\ndf_punct_1 = pd.DataFrame(sorted(dict_punct_1.items(), key=lambda x: x[1], reverse=True), columns=['punctuation', 'count'])\ndf_punct_0 = pd.DataFrame(sorted(dict_punct_0.items(), key=lambda x: x[1], reverse=True), columns=['punctuation', 'count'])\n\nfig, (ax0, ax1) = plt.subplots(1,2,figsize=(16,8))\nsns.barplot(data=df_punct_1, x='punctuation', y='count', ax=ax0)\nax0.title.set_text('Punctuation for Disaster tweets')\nsns.barplot(data=df_punct_0, x='punctuation', y='count', ax=ax1)\nax1.title.set_text('Punctuation for Normal tweets')\nplt.show()","73abfa6b":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\n# We use stop_words='english' to remove empty words\nvectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1,1))\nbag_of_words = vectorizer.fit_transform(df_train.text)\n\ntokenized = vectorizer.vocabulary_","708b4d4f":"sum_words = bag_of_words.sum(axis=0)\nsum_words","f4de0e03":"words_freq = [(word, sum_words[0, idx]) for word, idx in tokenized.items()]\nwords_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\nwords_freq[:10]","6350ea56":"df_common_words = pd.DataFrame(words_freq, columns=['words', 'freq'])\ndf_common_words[:20]","2d63870b":"stop_words.update([\"http\", \"https\", \"\u00fb_\", \"amp\", \"co\", \"rt\", \"pm\"])\n\nvectorizer = CountVectorizer(stop_words=stop_words, ngram_range=(1,1))\nbag_of_words = vectorizer.fit_transform(df_train.text)\ntokenized = vectorizer.vocabulary_\n\nsum_words = bag_of_words.sum(axis=0)\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in tokenized.items()]\nwords_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n\ndf_common_words = pd.DataFrame(words_freq, columns=['words', 'freq'])\ndf_common_words[:10]","b5145b76":"plt.figure(figsize=(12,10))\ng = sns.barplot(data=df_common_words[:20], x='words', y='freq' );\ng.set_xticklabels(g.get_xticklabels(), rotation=40);","3c76382c":"from PIL import Image\nimport numpy as np\nfrom wordcloud import WordCloud\n\ntext = ' '.join(df_train.text)\n\ndef plot_word_cloud(text, masque, background_color = \"white\") :\n    # D\u00e9finir un masque\n    mask_coloring = np.array(Image.open(str(masque)))\n\n    # D\u00e9finir le calque du nuage des mots\n    wc = WordCloud(background_color=background_color, max_words=80, stopwords=stop_words, mask = mask_coloring, max_font_size=100, random_state=42, colormap=\"winter\")\n\n    # G\u00e9n\u00e9rer et afficher le nuage de mots\n    plt.figure(figsize= (20,10))\n    wc.generate(text)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(wc)\n    plt.show()\n\nplot_word_cloud(text, \"..\/input\/twitter\/10wmt-superJumbo-v4.jpg\")","5f985d1b":"# Separate Dataframe in two \ndf_disaster = df_train[df_train['target'] == 1]\ndf_normal = df_train[df_train['target'] == 0]\n\n# Create two text cariables from each previous dataframes\ntext_disaster = ' '.join(df_disaster.text)\ntext_normal = ' '.join(df_normal.text)\n\n# Plot disaster words\nplot_word_cloud(text_disaster, masque='..\/input\/twitter\/10wmt-superJumbo-v4.jpg', background_color='black')\n# Plot normal words\nplot_word_cloud(text_normal, masque='..\/input\/twitter\/10wmt-superJumbo-v4.jpg', background_color='white')","13427861":"**2. location**","3d637b51":"### Let's check more in details the `text` value\n\nWe will create 2 new columns:\n* `text_len` representing the number of characters \n* `word_count` representing the number of worlds","96fd98be":"### Can we complete missing `keywords` ?","b6c46ee3":"## Bag of Words\nLet's see the most used words\n\nWe will use the class `CountVectorizer` from `sklearn.feature_extraction.text`. This class possess a parameter called `ngram_range` which can be used to specify if we want to create a vectorizer composed by n-grams available in the initial corpus.  \nExample:\n- `ngram_range = (1,1)` means we retrieve only words (1-gram)\n- `ngram_range = (1,2)` means we retreive 1-gram and 2-grams\n- `ngram_range = (2,2)` means we retreive only 2-grams","2849e91e":"### 3. Understand the Target variable\n\nThe variable `target` is composed by 0 or 1. \nIf a Tweet is about a real disaster, `target = 1`, else `target = 0`.\n\n**What is a Disaster?**\nAs per the dictionnary definition,a *Disaster* is a serious disruption occurring over a short or long period of time that causes widespread human, material, economic or environmental loss which exceeds the ability of the affected community or society to cope using its own resources.\n\nSo we are expecting to find in the text of the tweet words related to natural catastrophe (fire, flood, tornado...) with significant impact in the  person life.   \n\nLet's check the proportion of our target data.","9ed9156a":"We show the overall punctuation. Now let's split between Disaster and non Disatster tweets.","32681d8a":"### Let's have a closer look to the punctuation\n\nWe can get the list of all punctuation from the package `string`.","52762915":"Once the variable `bag_of_words` is created, we can calculate the number of occurence each word is available in the text.  \n**How to read it?**  \nEasy, each number represent the number of time one word appear. So first word appear 33 times, second word 4 times etc...","e1409e5b":"* There are 3341 differents location (+NaN). Most of them are in the US. Some location represent cities (New York, Los Angeles..) while other represent countries (USA, Canada, Nigeria...)","cf5639f1":"We can easily understand why the first Tweet is taling about a Disaster. We can find strong word like **earthquake**.  \nIn the other end, the other Tweet has nothing specific.","6b493ccc":"Let's check the words from Disasters Tweets and from the normal Tweets.  \nWe will separate the Dataframe in two: \n- One dataframe, only containing Disaster tweets\n- Another one containing other tweets","00564e9c":"We have a good view of words used during Disaster - **Fire, Hiroshima, flood, killed, bomber...**  \nThose words are definitely related to Something bad happening.\n\nWhile we can see nicer words for the \"normal\" tweets - **like, good, people, new**, but there are still words misleading such as **death, fire, weapon**. So we need to be careful.\n\n\n## Conclusion\n\nThis notebook was meant to be a rapid EDA of the Dataset.  \n\nWe first analyse the data in general. We have missing values for variables `location` and `keywords`.  \nThen, we analysed the `target` variable, and we saw that the repartition (0\/1) in the dataset is not totally even. And had a look at one tweet per category.  \nThen we analysed our other variables `keywords` and `location`. There are still possibilities to update and enhance those both features - especially `location` where we saw that we could merge multiple values into one (nyc, new york, new york city...).\n\nWe created 2 quantitative variables `word_count` and `text_len` which represent respectively the number of words in our tweet (`text`) and the length of the tweet (per character). With those 2 quantitative variables we were able to exploit more information with graphs.\n\nWe finally created a Bag of Words, we found the most used words through all tweets. We split this information into two categories - Disaster tweet \/ noraml tweet - and visualise another *Word cloud* where we can see directly the most used words, which can help us to understand why some tweets are more likely to be related to disasters.\n\n### What's next?\n\nI guess, it's only the first EDA. We can start creating a easy and basic model (baseline model).\n\nFrom this model, we can try to get more insight - maybe redo another EDA \/ Feature engineering \/ Data Cleansing -  and try to improve our model step by step.\n\nSo let's do it!\n\n> *List of possible Data Cleansing:*\n> - remove URL, \n> - remove emojis, \n> - remove punctuations,\n> - Clean Location,\n> - Update keywords,\n> - Spelling mistakes\n> - ...","9c9a36b3":"We tried to check the relation between `keywords` and the `target` variable.  \nWe can see that some `keywords` have strong impact on the target value:\n* Keywords *debris*, *derailment* and *wreckage* are only present for `target = 1`\n* While keyword *aftershock* is only used for `target = 0`","8617b608":"# Twitter Disaster Dataset - v1\n\nThis Notebook is part of a set of notebooks to deal with the Twitter Disaster Dataset.\n\n## 1. Exploratory Data Analysis\n\nThe purpose of this part is to explore the data from the dataset:\n1. **Understand the problem**. We'll look at hte problem and each variable.\n2. **Understand the Target**. We'll look at the Target data and relation with other features.\n3. **Basic cleaning**. We'll clean the dataset and handle the missing data, outliers and categorical variables.\n\nThe purpose is to be able to start the Modelling phase with some insight and understanding of the dataset.\nWe will probably do more EDA later on when we will have more insight about the models.","d5b9a7ee":"Columns: \n- keyword and location have missing values\n- No missing values for text and target (expected)","53caecb2":"### Relationship with Other Variables\n\n**1. Keyword**","975f17db":"Our distribution is not even, we have more tweet implying Disaster.\n\nLet's have a look at one tweet per category.","1235278f":"There is room for improvement with `location`.  \nAs per the example above - we could merge most of the values into one and unique value.","67bb6cad":"We can see some indesirable words like *http*, *https*, *\u00fb_*. We will add those words in the `stop_words` variable.","49bbeb79":"### 1. Understand the Problem\n\n**Overview**  \nTwitter has become an important communication channel in times of emergency.  \nThrough their phone, people can announce an emergency they are observing in real-time.  \nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.  \nThe purpose of this project is to build a Model which can predict which Tweets are about real disasters and which one\u2019s aren\u2019t. \n\n\n**What are we predicting?**  \nWe are predicting whether a given tweet is about a real disaster or not. \nIf so, predict a `1`. If not, predict a `0`.\n\n\n**Columns**\n* `id` - a unique identifier for each tweet\n* `text` - the text of the tweet\n* `location` - the location the tweet was sent from (may be blank)\n* `keyword` - a particular keyword from the tweet (may be blank)\n* `target` - this denotes whether a tweet is about a real disaster (1) or not (0) > Target value\n\n**Information**  \nWe have two files:\n* `train.csv`: File with 7613 entries. Will be used to train ou model.\n* `test.csv`: Do not contains `target` column. This is the column we need to predict using the model we would have created previously.\n","685383cb":"The variables `text_len` and `word_count` are interesting metrics as they can help to quantify the available information, and check the quality of it.\n\nLet's plot those new values to see the distirbution of the number of characters and the number of words.","88b957b1":"Tweets related to disasters seem to be longer while shorter tweets are more related to normal tweets.\nAs per the Boxplot, most of Disaster tweets have a lenght between *90* and *138*, while normal tweets have a length between *70* and *130*."}}