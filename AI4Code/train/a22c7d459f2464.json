{"cell_type":{"8971829e":"code","3b07dafc":"code","dd7bf1f9":"code","f5c6059c":"code","d656b24c":"code","dec20499":"code","972c88ee":"code","9395bca7":"code","e6303f86":"code","04eb719e":"code","fff872b2":"code","d543e3c7":"code","646841a1":"code","3b2c2bdf":"code","45f27dfc":"code","271c505b":"code","a3724a26":"code","2968ef04":"code","082fd82e":"code","51af7b20":"code","4adea68d":"code","5678b4fe":"code","de10eda2":"code","e15c20da":"code","ef6abf32":"code","e8d8f4b3":"code","2af67abe":"code","541d8492":"code","2a25ee06":"code","9a4d048e":"markdown","f2386627":"markdown","0c60aa4d":"markdown","fd598622":"markdown","87c242c8":"markdown","12aee835":"markdown","9adfd0a3":"markdown","5076271a":"markdown","4c371398":"markdown","cb76b3fb":"markdown","16bca9ca":"markdown","5e8551b8":"markdown","bfda1e09":"markdown","3f2ea9ee":"markdown","911f4708":"markdown","b618c4bf":"markdown","55a9e1f5":"markdown","6dc4b6f5":"markdown","942164e3":"markdown","d5521ae4":"markdown","103ddc2a":"markdown","051cea8e":"markdown","47cb5d4c":"markdown","b3d73ca7":"markdown","741148b1":"markdown","c97b7218":"markdown","184f64d7":"markdown","508b7c9f":"markdown","da816ee9":"markdown","24275a72":"markdown","69ca9c69":"markdown","a0092640":"markdown","e908331f":"markdown","9792e348":"markdown","e59586d1":"markdown","87e6d92a":"markdown","37f01b40":"markdown"},"source":{"8971829e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom PIL import Image\nsns.set(style=\"ticks\", color_codes=True)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3b07dafc":"diamond=pd.read_csv('diamonds.csv')\ndiamond.head(2)","dd7bf1f9":"Image.open('Anglo-DiamondAnatomy_03.JPG')","f5c6059c":"diamond.drop('Unnamed: 0',axis=1,inplace=True)\n\nprint('Numnber of records in dataset are {} and Features are {}.  '.format(*diamond.shape))\nprint(\"\\nAny missing sample in set:\",diamond.isnull().values.any())","d656b24c":"diamond.describe()","dec20499":"print(\"Categorical column's details :\")\ndiamond.describe(include=['O'])","972c88ee":"plt.figure(figsize=(20,5))\nfor i,feature in enumerate(diamond.select_dtypes(include='O').columns.tolist(),1):\n    plt.subplot(1,3,i)\n    diamond[feature].value_counts(normalize=True).plot.bar()\n    plt.title(feature)\n\nplt.figure(figsize=(20,5))\nfor i,feature in enumerate(diamond.select_dtypes(include='O').columns.tolist(),1):\n    plt.subplot(1,3,i)\n    sns.boxplot(x=feature,y='price',data=diamond,palette='husl')\nplt.show()\n","9395bca7":"cut_clarity=pd.pivot_table(diamond, values='price', columns='cut', index='clarity', aggfunc='median')\ncut_color=pd.pivot_table(diamond, values='price', columns='cut', index='color', aggfunc='median')\nclarity_color=pd.pivot_table(diamond, values='price', columns='clarity', index='color', aggfunc='median')\n\nprint(\"Combination of Color,Cut,Clarity with median price \")\nplt.figure(figsize=(20,5))\nplt.subplot(131)\nsns.heatmap(cut_clarity,cmap='Blues')\n\nplt.subplot(132)\nsns.heatmap(cut_color,cmap='Greys')\n\nplt.subplot(133)\nsns.heatmap(clarity_color,cmap='Blues')\nplt.show()","e6303f86":"plt.figure(figsize=(10,5))\ndiamond.corr()['price'].sort_values()[:-1].plot.barh()\nplt.title('Order of dependence of price on Numerical Features')\nplt.xlabel('Correaltion coefficient with Price')\nplt.ylabel('Feature')\nplt.show()","04eb719e":"list=['carat', 'depth', 'table', 'x', 'y', 'z']\nplt.figure(figsize=(15,10))\nfor i,feature in enumerate(list,1):\n    plt.subplot(2,3,i)\n    sns.scatterplot(diamond[feature],diamond['price'])\n    plt.title(feature)\nplt.show()","fff872b2":"list=['carat', 'depth', 'table', 'x', 'y', 'z']\nplt.figure(figsize=(15,8))\nfor i,feature in enumerate(diamond.select_dtypes(exclude='O').columns.tolist(),1):\n    plt.subplot(3,3,i)\n    plt.hist(diamond[feature],bins=100)\n    plt.title(feature)\nplt.show()","d543e3c7":"sns.heatmap(diamond.corr(),annot=True)\nplt.show()","646841a1":"diamond_d=pd.get_dummies(diamond,columns=diamond.select_dtypes(include='O').columns.tolist(),drop_first=True)\n\nX=diamond_d.drop('price',axis=1)\ny=diamond.price\n\nXc = sm.add_constant(X)\nlin_reg = sm.OLS(y,Xc).fit()\nlin_reg.summary()","3b2c2bdf":"df_results = pd.DataFrame(columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])\n\ndescription = \"Raw OLS\"\nr2=round(lin_reg.rsquared,3)\nadjR2=round(lin_reg.rsquared_adj,3)\nmsem=round(lin_reg.mse_model,2)\nmser=round(lin_reg.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","45f27dfc":"X\ny_log=y.apply(np.log)\n\nXc = sm.add_constant(X)\nlin_reg_log = sm.OLS(y_log,Xc).fit()\nlin_reg_log.summary()","271c505b":"description = \"OLS + Log Price\"\nr2=round(lin_reg_log.rsquared,3)\nadjR2=round(lin_reg_log.rsquared_adj,3)\nmsem=round(lin_reg_log.mse_model,2)\nmser=round(lin_reg_log.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","a3724a26":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(lin_reg.resid, lags=40 , alpha=0.05)\nacf.show()","2968ef04":"from scipy import stats\nprint('JB value before log Transformation of price :',stats.jarque_bera(lin_reg.resid)[0])\nprint('JB value  after log Transformation of price :',stats.jarque_bera(lin_reg_log.resid)[0])","082fd82e":"plt.figure(figsize=(15,4))\nplt.subplot(121)\nplt.title('Before log Transformation of price')\nsns.distplot(lin_reg.resid)\nplt.subplot(122)\nplt.title('After log Transformation of price')\nsns.distplot(lin_reg_log.resid)\nplt.show()","51af7b20":"import statsmodels.api as sm\n#checking 50% of fraction of data, since sample of data can be linear.\nlstat1,pvalue1=sm.stats.diagnostic.linear_rainbow(res=lin_reg, frac=0.5)  \nlstat2,pvalue2=sm.stats.diagnostic.linear_rainbow(res=lin_reg_log, frac=0.5)  \n\nprint('The pvalue before log Transformation of price :',round(pvalue1,3))\nprint('The pvalue after log Transformation of price :',round(pvalue2,3))","4adea68d":"import scipy.stats as stats\nimport pylab\nfrom statsmodels.graphics.gofplots import ProbPlot\n\nprint('Probability Plot:\\n 1)Before\\n 2)After log Transformation of price')\nplt.figure(figsize=(15,5))\nplt.subplot(121)\nst_residual = lin_reg.get_influence().resid_studentized_internal\nstats.probplot(st_residual, dist=\"norm\", plot = pylab)\n\nplt.subplot(122)\nst_residual = lin_reg_log.get_influence().resid_studentized_internal\nstats.probplot(st_residual, dist=\"norm\", plot = pylab)\nplt.show()","5678b4fe":"print('The mean value of residuals should be zero.\\n')\nprint('The mean value of residuals before log Transformation of price :',lin_reg.resid.mean())\nprint('The mean value of residuals after log Transformation of price :',lin_reg_log.resid.mean())","de10eda2":"import statsmodels.stats.api as sms\n\npvalue1 = sms.het_goldfeldquandt(lin_reg.resid, lin_reg.model.exog)[1]\npvalue2 = sms.het_goldfeldquandt(lin_reg_log.resid, lin_reg_log.model.exog)[1]\n\nprint('The pvalue before log Transformation of price :',round(pvalue1,3))\nprint('The pvalue after log Transformation of price :',round(pvalue2,3))","e15c20da":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = [variance_inflation_factor(Xc.values, i) for i in range(Xc.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X.columns).T","ef6abf32":"diamond_d2=diamond_d.drop('x',axis=1)\n\nX2=diamond_d2.drop('price',axis=1)\ny=diamond.price\n\nXc2 = sm.add_constant(X2)\nlin_reg2 = sm.OLS(y,Xc2).fit()\n\ndescription = \"OLS + without X feature \"\nr2=round(lin_reg2.rsquared,3)\nadjR2=round(lin_reg2.rsquared_adj,3)\nmsem=round(lin_reg2.mse_model,2)\nmser=round(lin_reg2.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","e8d8f4b3":"X2\ny_log\n\nXc3 = sm.add_constant(X2)\nlin_reg3 = sm.OLS(y_log,Xc3).fit()\n\ndescription = \"OLS + without X feature + Log Price\"\nr2=round(lin_reg3.rsquared,3)\nadjR2=round(lin_reg3.rsquared_adj,3)\nmsem=round(lin_reg3.mse_model,2)\nmser=round(lin_reg3.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","2af67abe":"from statsmodels.stats.outliers_influence import variance_inflation_factor\npd.set_option('Display.max_columns',None)\nvif = [variance_inflation_factor(Xc3.values, i) for i in range(Xc3.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=X2.columns).T","541d8492":"diamond_d4=diamond_d.drop(['z','cut_Ideal','clarity_SI1'],axis=1)\n\nX4=diamond_d4.drop('price',axis=1)\ny=diamond.price\n\nXc4 = sm.add_constant(X4)\nlin_reg4 = sm.OLS(y,Xc4).fit()\n\ndescription = \"OLS + No multicolinear Feature \"\nr2=round(lin_reg4.rsquared,3)\nadjR2=round(lin_reg4.rsquared_adj,3)\nmsem=round(lin_reg4.mse_model,2)\nmser=round(lin_reg4.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","2a25ee06":"X4\ny_log\n\nXc5 = sm.add_constant(X4)\nlin_reg5 = sm.OLS(y_log,Xc5).fit()\n\ndescription = \"OLS + No multicolinear Feature + Log Price\"\nr2=round(lin_reg5.rsquared,3)\nadjR2=round(lin_reg5.rsquared_adj,3)\nmsem=round(lin_reg5.mse_model,2)\nmser=round(lin_reg5.mse_resid,2)\ndf_results = pd.concat([df_results,pd.DataFrame(np.array([description,r2,adjR2,msem,mser]).reshape(1,-1),\n            columns=['Description','R2','Adjusted_R2','MSE_Model','MSE_Resid'])], axis=0)\n\ndf_results","9a4d048e":"## OLS + without X feature + Log Price","f2386627":"##### Assumption 2- Normality of Residuals","0c60aa4d":"Test done : Jarque Bera test.\n\n* The Jarque\u2013Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution.\n* A large value for the jarque-bera test indicates non normality.\n    ","fd598622":"* The interpretation of VIF is as follows: the square root of a given variable\u2019s VIF shows how much larger the standard error is, compared with what it would be if that predictor were uncorrelated with the other features in the model. If no features are correlated, then all values for VIF will be 1.\n* So, multicollinearity exists here as vif values are high.","87c242c8":" **CUT** \n* Most of the cut present in dataset are of 'Ideal' type with around 40% but median price for them are lowest.\n* 'Fair' cut is least found and hence has high price range comparitively.\n\n**COLOR** \n* G,E,F are easily available colors and economical too.\n* I,J color type will cost you more bucks :)\n\n**CLARITY** \n* SI1 and VS2 are the most often found clarities in diamonds but need to pay more for SI1 clarity than VS2.\n\n**Note :**\n* Price exception are for all cases irrespective of cut,color,clarity.\n","12aee835":"**Do Upvote if you liked my work.**\n\n**Happy learning!!**","9adfd0a3":"Test done : Rainbow test ","5076271a":"* Lot of interdependency can be seen between features, as multiple features have high correlation coefficient.","4c371398":"* Very much close to zero in both case, So linearity is present.","cb76b3fb":"## OLS + No multicolinear Feature + Log Price","16bca9ca":"Lets check VIF again.","5e8551b8":"##### Asssumption 3 - Linearity of residuals\n","bfda1e09":"Still multicolinearity can be seen in some features, so we will be dropping those feature and will check performance of model again.","3f2ea9ee":"\n\nTest done : Durbin- Watson Test.\n\n- It's value ranges from 0-4. \n- If the value of Durbin- Watson is Between 0-2, it's known as Positive Autocorrelation.\n- If the value ranges from 2-4, it is known as Negative autocorrelation.\n- If the value is exactly 2 (generally can take range from 1.5 to 2.5), it means No Autocorrelation .\n- For a good linear model, it should have low or no autocorrelation.\n\nwe can see here the values of dublin watson test (From OLS summary) : \n\n1.190 (1st case),\n\n1.533 (2nd case)\n\nHence no autocorrelation before and after log transformation of price.\n    \n","911f4708":"* Trend of price with carat,x,y,z can be easily seen from scatterplots.\n* While Depth and Table concentrated more in their particular range and have no strong pattern with price.\n* Some outliers are clearly visible in scatterplots and need to be treated.","b618c4bf":"* From above results we can infer that taking log transformation on price feature is giving better results in all cases.\n* R2 and AdjustedR2 values are almost similar.\n* OLS model with Log of price can be considered for further studies as it is giving best result in predicting price of diamonds.\n","55a9e1f5":"##  Dropping feature with highest VIF value","6dc4b6f5":"* Carat and x,y,z feature are deciding factor for price calculation. \n* Table and depth has very little effect in price of diamond.","942164e3":"* Points are symmetrically distributed around a diagonal line in both case(more after log Transformation of price).\n* Hence linearity of residuals can be seen.","d5521ae4":"Here, p value > 0.05 so we failed to reject null hypothesis, it is homoscedasticity distribution.","103ddc2a":"* The critical chi square value at the 5% level of significance is 5.99. \n* In both case the computed value of the JB statistic is greater than 5.99. \n* Thus we reject the null hypothesis and hence the error terms are not normally distributed.","051cea8e":"## OLS + No multicolinear Feature","47cb5d4c":"For Linear Regression, we need to check if the 5 major assumptions hold.\n\n1. No Auto correlation\n2. Normality of error terms\n3. Linearity of variables \n4. No Heteroscedacity\n5. No strong MultiCollinearity\n","b3d73ca7":"* As pvalue > 0.05, We failed to reject H0, Hence residuals are linear.","741148b1":"##### Assumption 1- No autocorrelation","c97b7218":"From the graph below, we can easily see that there is no autocorrelation.","184f64d7":"## 1) Raw OLS model\n","508b7c9f":"Residuals have high kurtosis value, which makes it non linear.","da816ee9":"* As, we can see from the second warning of OLS model itself that Multicollinearity is present in the dataset.\n* It can also be stated by noting the high conditional number.\n","24275a72":"##### Assumption 4 -  Homoscedasticity_test \n\nTest done : Goldfeld Test\n\n* If the variance of the residuals are symmetrically distributed across the regression line , then the data is said to homoscedastic and the residuals should be homoscedastic.\n* This test is based on the hytpothesis testing where null and alternate hypothesis are:\n$$ H_{0} : \\sigma_{u_{i}}~is~constant~across~the~range~of~data $$\n\n$$ H_{a} : \\sigma_{u_{i}}~is~not~constant~across~the~range~of~data $$\n\n\n","69ca9c69":"* It is done to check the linearity of the residuals for a linear regression model.\n* Linearity of residuals is preferred.\n\n $$ H_{0} : Residuals ~are~ linear $$\n $$ H_{1} : Residuals~ are ~not ~linear $$","a0092640":"* Combination of Premium cut,SI1 clarity and H,I,J color makes diamond costly.\n* IF cut generally doesn't attract more money, but its combination with D color were sold foe higher median price.","e908331f":"## 2) OLS model after log transformation of Price\n","9792e348":"# Checking the Asumptions of Linear Regression","e59586d1":"* Carat, price and x covers a wide distribution with significant skewness.\n* Certainly our numerical features are not normally distributed.","87e6d92a":"Did you know?\n\n\n**A person can be turned into a diamond!!**\n\nWant to live on for eternity? With the advancement of lab-grown diamond technology, some companies are now offering to turn the ashes of a deceased love one into a diamond gem that can worn as jewelry by those they left behind. Swiss-based Algordanza has been providing memorial diamonds since 2003, and now operates in 30 different countries.","37f01b40":"##### Assumption 5- NO  MULTI COLLINEARITY"}}