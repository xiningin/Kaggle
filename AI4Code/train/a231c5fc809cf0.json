{"cell_type":{"d9a1ee2c":"code","513af40c":"code","a0c00b45":"code","b32e7f4f":"code","637f1d80":"code","ae0174fe":"code","2864e03d":"code","4368341b":"code","465799dc":"code","b300bdee":"code","0e8f667c":"code","6e02108e":"code","7d6d313e":"code","7c99f70c":"code","9775e1ad":"code","174ffa2d":"code","bf612004":"code","51696247":"code","9ff6e46c":"code","bd482f18":"code","671317c1":"code","e9852c8f":"code","9927758f":"code","1af24d5a":"code","a057a060":"code","7a711383":"code","e736165e":"code","f7c76143":"code","5001fa14":"code","c8c017a0":"code","427a675b":"code","34dd1eea":"code","2bed6681":"code","c543bce5":"code","67f50992":"code","5df4bf0e":"code","1fd47df6":"code","fba50bc9":"code","1b1204d5":"code","28160157":"code","f1664ac8":"code","9672a857":"code","c7e4ee0f":"code","36d97bce":"code","baa8d582":"code","b9f02539":"code","f9cd6570":"code","a40abdd0":"code","e9439f07":"code","cefebc10":"code","417f58c7":"code","04c4e9f3":"code","de6ced58":"code","c0beb7d7":"code","53c27098":"code","44f37ada":"code","f675a355":"code","d5140116":"code","4f371b2b":"code","ddca32c4":"code","48ff5d43":"code","6d621d12":"code","3c0297a2":"code","63293649":"code","04aa478d":"code","4ffbdb0d":"code","7ed028a6":"code","3da03f84":"markdown","e52938eb":"markdown","b2f0e000":"markdown","8c561d21":"markdown","138ab8ca":"markdown","fa2d77e1":"markdown","fa72f5ea":"markdown","dc41334f":"markdown","69f956da":"markdown","aa1ea9a5":"markdown","e06438a2":"markdown","433127cf":"markdown","e83b8b6f":"markdown","47faa83c":"markdown","64639639":"markdown","23638ffa":"markdown","05742614":"markdown","fbf4c6db":"markdown","574dacb5":"markdown","d22c85a2":"markdown","26379cd7":"markdown","58f91b09":"markdown","88b41490":"markdown","f8745412":"markdown","83435fb1":"markdown","439fa0c5":"markdown","41d1cb55":"markdown","e094593d":"markdown","cdac02be":"markdown","5dc538c8":"markdown","f555e9ad":"markdown","3a4f8ed4":"markdown","dbb934ec":"markdown"},"source":{"d9a1ee2c":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n\nimport os\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","513af40c":"rating=pd.read_csv('\/kaggle\/input\/anime-recommendations-database\/rating.csv')\nanime=pd.read_csv('\/kaggle\/input\/anime-recommendations-database\/anime.csv')","a0c00b45":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b32e7f4f":"anime=reduce_mem_usage(anime)\nrating=reduce_mem_usage(rating)","637f1d80":"anime.head()","ae0174fe":"anime.shape","2864e03d":"print(np.median(anime['members']))\nanime=anime[anime['members']>(np.percentile(anime['members'], 50))]\nanime.dropna(axis=0, how='any', subset = ['rating'] ,inplace=True)","4368341b":"rating.head()","465799dc":"rating.nunique()","b300bdee":"rating['rating'] = rating['rating'].replace(-1,np.nan)\nrating[\"user_id\"].unique()\nuser=rating.loc[:,'user_id'].value_counts()\nuser=user.to_frame()\nuser = user.drop(user[user.user_id < 150].index)#drop users who rated less than 150 times to decrease the dataset size\nuser=user.rename(columns={\"user_id\": \"count\"})\nuser['user_id']=user.index","0e8f667c":"user.head()","6e02108e":"rating=pd.merge(user,rating,on='user_id',how='left')\nusers=rating[['user_id','anime_id','rating']]\nusers=users.reset_index()\nanime=anime.reset_index()\ndf = pd.merge(anime,users,on='anime_id',how='inner')\ndf=df.drop(['index_x','index_y'], axis=1)\ndf = df.rename(columns={'rating_x': 'anime_rating','rating_y':'user_rating'})\ndf.head(10)","7d6d313e":"rating_counts=df.loc[:,'anime_id'].value_counts()#\u6bcf\u4e2a\u52a8\u6f2b\u6253\u5206\u4eba\u6570 number of raters each anime\nrating_counts=rating_counts.to_frame()\nrating_counts=rating_counts.rename(columns={'anime_id': 'count_ratings'})\nrating_counts['anime_id']=rating_counts.index\nrating_counts=rating_counts[rating_counts['count_ratings']>300]#\u53ea\u4fdd\u7559300\u4e2a\u4eba\u4ee5\u4e0a\u8bc4\u5206\u7684\u52a8\u6f2bonly keep anime with more than 300 raters.\nrating_counts.head()","7c99f70c":"df1=pd.merge(rating_counts,df,on='anime_id',how='left')\ndf_p = df1.pivot_table(index='user_id', columns='anime_id', values='user_rating')\nprint('Shape User-Movie-Matrix:\\t{}'.format(df_p.shape))\ndf_p.sample(3)","9775e1ad":"df2=df1.dropna(subset=['user_rating','anime_rating','members'])\ndf2=df2.drop_duplicates(subset='name')\ndf2.head()","174ffa2d":"def weighted_rating(x): #x is the dataframe's name\n    m=300\n    C=df2.anime_rating.mean()\n    v = x['count_ratings']\n    R = x['anime_rating']\n    return (v\/(v+m) * R) + (m\/(m+v) * C)","bf612004":"df2['wr'] = df2.apply(weighted_rating, axis=1)\ndf2=df2.sort_values(by='wr',ascending=False)\ndf2.head(10)","51696247":"from sklearn.metrics.pairwise import cosine_similarity\ndf_p=df_p.fillna(0)\nuser_similarity = cosine_similarity(df_p) #ger similarity matrix for users\nuser_similarity.shape","9ff6e46c":"item_similarity = cosine_similarity(df_p.T)#get similarity matrix for animes\nitem_similarity.shape","bd482f18":"item_sim_df = pd.DataFrame(item_similarity, index = df_p.columns, columns = df_p.columns)\nitem_sim_df.head(3) #show similarity matrix for animes","671317c1":"user_sim_df = pd.DataFrame(user_similarity, index = df_p.index, columns = df_p.index)\nuser_sim_df.head(3) #show similarity matrix forusers","e9852c8f":"def similar_users(user):\n    \n    if user not in df_p.index:\n        return('No data available on user {}'.format(user))\n    \n    print('Most Similar Users:\\n')\n    sim_values = user_sim_df.sort_values(by=user, ascending=False).loc[:,user].tolist()[1:6] # sort the similar score and get top5\n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:6]  # get the user_id of those top 5.  \n    zipped = zip(sim_users, sim_values,)\n    for user, sim in zipped:\n        print('User #{0}, Similarity value: {1:.2f}'.format(user, sim)) ","9927758f":"similar_users(3) ","1af24d5a":"similar_users(73)","a057a060":"def similar_animes(anime):\n    \n    if anime not in df_p.columns:\n        return('No anime called {}'.format(anime))\n    \n    print('Most Similar Animes:\\n')\n    sim_values = item_sim_df.sort_values(by=anime, ascending=False).loc[:,anime].tolist()[1:6]\n    sim_animes = item_sim_df.sort_values(by=anime, ascending=False).index[1:6]\n    zipped = zip(sim_animes, sim_values,)\n    for anime, sim in zipped:\n        print('Anime #{0}, Similarity value: {1:.2f}'.format(anime, sim)) ","7a711383":"similar_animes(19)","e736165e":"anime_id_name_match=df1[['anime_id','name']].drop_duplicates()\nanime_id_name_match=anime_id_name_match.sort_values(by='anime_id')\nitem_sim_df_name=item_sim_df.copy()\nitem_sim_df_name.index = anime_id_name_match['name']\nitem_sim_df_name.columns = anime_id_name_match['name']\nitem_sim_df_name.head(3)","f7c76143":"def similar_animes_name(anime_name):\n    count = 1\n    print('Similar shows to {} include:\\n'.format(anime_name))\n    for item in item_sim_df_name.sort_values(by = anime_name, ascending = False).index[1:11]:\n        print('No. {}: {}'.format(count, item))\n        count +=1 ","5001fa14":"import re\n#This function is to find exact anime name by inputing in keywords\ndef find_real_name(x):\n    df1_anime=df1.drop_duplicates(subset='name')\n    find_name=df1_anime[df1_anime['name'].str.contains(x, flags=re.IGNORECASE)] #case non-sensitive\n    return find_name","c8c017a0":"find_real_name('ping')","427a675b":"similar_animes_name('Ping Pong The Animation')","34dd1eea":"def user_like_me(user):\n    # get the user's row\n    s1 = df_p.loc[user,:]\n\n    # get the index of max values in s1, might be more than 1\n    s1_argmax = s1[s1 == s1.max()].index.tolist()\n\n    # randomly choose 1 index\n    #s1_argmax = np.random.choice(s1_argmax) \n    s1_argmax\n    animes=[]\n    for i in s1_argmax:\n        name_list=anime_id_name_match[anime_id_name_match.anime_id==i]['name'].tolist()\n        animes.append(name_list)  \n    print('The user like you the most is also watching:')\n    print(*animes, sep='\\n')","2bed6681":"user_like_me(3324)","c543bce5":"def predicted_rating(anime_name, user):\n    sim_users = user_sim_df.sort_values(by=user, ascending=False).index[1:500] \n    user_values = user_sim_df.sort_values(by=user, ascending=False).loc[:,user].tolist()[1:500]\n    rating_list = []\n    weight_list = []\n    for j, i in enumerate(sim_users):\n        item_sim_df_name_2=df_p.copy()\n        item_sim_df_name_2.columns = anime_id_name_match['name']\n        rating = item_sim_df_name_2.loc[i, anime_name]\n        similarity = user_values[j]\n        if np.isnan(rating):\n            continue\n        elif not np.isnan(rating):\n            rating_list.append(rating*similarity)\n            weight_list.append(similarity)\n    return sum(rating_list)\/sum(weight_list)   ","67f50992":"predicted_rating('Cowboy Bebop', 5)","5df4bf0e":"df['genre_and_type']=df['genre']+','+df['type']\ndf_anime_name_match=df[['anime_id','name','genre_and_type']].drop_duplicates()\ndf_anime_name_match.head()","1fd47df6":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport seaborn as sns\nfrom collections import Counter\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\npd.set_option(\"display.max_colwidth\", 200)\nimport spacy\nimport gensim\nfrom gensim import corpora\n!pip install pyLDAvis\nimport pyLDAvis\nimport pyLDAvis.gensim\n%matplotlib inline\n\nimport itertools\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')","fba50bc9":"# words to be removed from vocabulary\nblockwords = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'am', 'an', 'and', 'any', 'are', \"aren't\", 'as', 'at','since','paid','don','doesn','close',\n 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'doing', \"don't\", 'down', 'during',\n 'each', 'few', 'for', 'from', 'further', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\",\n 'hers', 'herself', 'him', 'himself', 'his', 'how', \"how's\", 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it', \"it's\", 'its', 'itself',\n \"let's\", 'me', 'more', 'most', \"mustn't\", 'my', 'myself', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', 'ours' 'ourselves', 'out', 'over', 'own',\n 'same', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', \"shouldn't\", 'so', 'some', 'such', 'like',\n 'than', 'that',\"that's\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", 'also','can','could','should',\n \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'not','bit','much',\n 'was', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where','within','quite','really','just','together',\n \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\",'will', 'with', \"won't\", 'would', \"wouldn't\", 'hole','furniture','put',\n 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', \"s'yourself'\", 'yourselves', 'drawer','sure',\n 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'hundred', 'thousand', '1st', '2nd', '3rd','nightstand','nightstands','night',\n '4th', '5th', '6th', '7th', '8th', '9th', '10th']","1b1204d5":"df_anime_name_match['genre_and_type']=df_anime_name_match['genre_and_type'].apply(str)","28160157":"stop_words = set(stopwords.words('english'))                      # set of all stop words\nlem=WordNetLemmatizer()\n#p=inflect.engine()\n\ndef process(comment):\n  sent = comment.lower()                                          # lower case all words \n  words = nltk.word_tokenize(sent)\n  words =  [word for word in words if not word in blockwords]     # remove words present in blockwords\n  words = [word for word in words if not word.isdigit()]          # remove digit characters\n  #words = [word for word in words if len(word) > 3]               # remove words with length less than 3\n  #words = [word for word in words if word.isalpha()]              # remove non alphabetic words\n  words = [lem.lemmatize(word) for word in words]                 # lemmatize words to root word\n  sent = ' '.join(words)\n  sent = re.sub(r'\\(', '', sent)\n  sent = re.sub(r'\\)', '', sent)\n  sent = re.sub(r\"'\", '', sent)\n  return sent\n\ndef num_words(sent):                                              # returns number of words in the sentence\n  word_tok=nltk.word_tokenize(sent)\n  return len(word_tok)\n\ndf_anime_name_match['Cleaned_g_t']=df_anime_name_match['genre_and_type'].apply(process)\ndf_anime_name_match['Unclean_len']=df_anime_name_match['genre_and_type'].apply(num_words)                     # word length of uncleaned comments\ndf_anime_name_match['Clean_len']=df_anime_name_match.Cleaned_g_t.apply(num_words)               # word length of cleaned comments\ndf_anime_name_match['percentage reduction']=(df_anime_name_match['Unclean_len']-df_anime_name_match['Clean_len'])\/df_anime_name_match['Unclean_len']*100 # percentage of reduction","f1664ac8":"text= \" \".join(df_anime_name_match['Cleaned_g_t'])\n# Display the generated image:\nwordcloud = WordCloud(max_font_size=35, max_words=40, background_color=\"white\",collocations=False).generate(text)\nplt.figure(figsize=(8,6))\nplt.imshow(wordcloud, interpolation=\"gaussian\")\nplt.title('Top words in anime descriptions',size=19)\nplt.axis(\"off\")\nplt.show()","9672a857":"tf = TfidfVectorizer(stop_words='english')\ntfidf_matrix = tf.fit_transform(df_anime_name_match['Cleaned_g_t'])","c7e4ee0f":"from sklearn.metrics.pairwise import linear_kernel\ncosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\ncosine_sim.shape[0]","36d97bce":"tf_sim=pd.DataFrame(data=cosine_sim) \ntf_sim.index = df_anime_name_match['name']\ntf_sim.columns = df_anime_name_match['name']\ntf_sim.head()","baa8d582":"def similar_animes_content_based(anime):\n    \n    if anime not in tf_sim.columns:\n        return('No anime called {}'.format(anime))\n    \n    print('Most Similar Animes:\\n')\n    sim_values = tf_sim.sort_values(by=anime, ascending=False).loc[:,anime].tolist()[1:11]\n    sim_animes = tf_sim.sort_values(by=anime, ascending=False).index[1:11]\n    zipped = zip(sim_animes, sim_values,)\n    for anime, sim in zipped:\n        print('{0}, {1:.2f}'.format(anime, sim)) ","b9f02539":"find_real_name('doro')","f9cd6570":"similar_animes_content_based('Dororon Enma-kun Meeramera')","a40abdd0":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.decomposition import PCA","e9439f07":"pca = PCA(n_components=3)\npca.fit(df_p)\nprint(pca.explained_variance_ratio_)\nprint(pca.explained_variance_)","cefebc10":"pca_df_p = pca.transform(df_p)\npca_df_p = pd.DataFrame(pca_df_p)\npca_df_p.head(2)","417f58c7":"cluster_3 = pd.DataFrame(pca_df_p[[0,1,2]])","04c4e9f3":"plt.rcParams['figure.figsize'] = (10, 6)\n\n\nfig = plt.figure()\nax = Axes3D(fig)\nax.scatter(cluster_3[0],cluster_3[1],cluster_3[2])\n\nplt.title('Data Distribution PCA in 3D', fontsize=20)\nplt.show()","de6ced58":"from sklearn.cluster import KMeans\n \n'\u5229\u7528SSE\u9009\u62e9k'\nSSE = []  # \u5b58\u653e\u6bcf\u6b21\u7ed3\u679c\u7684\u8bef\u5dee\u5e73\u65b9\u548c \nfor k in range(1,9):\n    estimator = KMeans(n_clusters=k)  # \u6784\u9020\u805a\u7c7b\u5668\n    estimator.fit(pca_df_p[[0,1,2]])\n    SSE.append(estimator.inertia_)\nX = range(1,9)\nplt.xlabel('k')\nplt.ylabel('SSE')\nplt.plot(X,SSE,'o-')\n","c0beb7d7":"from sklearn.metrics import silhouette_score\nScores = []  # \u5b58\u653e\u8f6e\u5ed3\u7cfb\u6570 put silhouette scores here\nfor k in range(2, 14):\n    estimator = KMeans(n_clusters=k)  # \u6784\u9020\u805a\u7c7b\u5668 build k-means model\n    estimator.fit(np.array(pca_df_p[[0,1,2]]))\n    Scores.append(silhouette_score(np.array(pca_df_p[[0,1,2]]), estimator.labels_, metric='euclidean'))\nX = range(2, 14)\nplt.xlabel('k')\nplt.ylabel('Silhouette Coefficient')\nplt.plot(X, Scores, 'o-')\nplt.show()","53c27098":"clusterer = KMeans(n_clusters=2,random_state=30).fit(cluster_3)\ncenters = clusterer.cluster_centers_\nc_preds = clusterer.predict(cluster_3)","44f37ada":"fig = plt.figure()\nax = Axes3D(fig)\nax.scatter(cluster_3[0],cluster_3[1],cluster_3[2], c = c_preds)\nplt.title('Data points in 3D PCA axis', fontsize=20)\nplt.show()","f675a355":"fig = plt.figure(figsize=(10,8))\nplt.scatter(cluster_3[0],cluster_3[1],cluster_3[2],c = c_preds)\nfor ci,c in enumerate(centers):\n    plt.plot(c[1], c[0], c[2],'o', markersize=8, color='red', alpha=1)\n\nplt.xlabel('x_values')\nplt.ylabel('y_values')\n\nplt.title('Data points in 2D PCA axis', fontsize=20)\nplt.show()","d5140116":"df_p_anime=df_p.columns.tolist()\ndf_p_anime = pd.DataFrame (df_p_anime,columns=['anime_id'])\ndf_p_anime=pd.merge(df_p_anime,df1,on='anime_id',how='left')#2826 animes name match\ndf_p_anime=df_p_anime[['anime_id','name','anime_rating']].drop_duplicates()\ndf_p_name=df_p.copy()\ndf_p_name.columns = df_p_anime['name']#2826 animes\ndf_p_name['cluster'] = c_preds\ndf_p_name.head()","4f371b2b":"group1 = df_p_name[df_p_name['cluster']==0]\ngroup2 = df_p_name[df_p_name['cluster']==1]","ddca32c4":"group1_mean=group1.mean().to_frame()\ngroup1_mean.head(10)","48ff5d43":"group2_mean=group2.mean().to_frame()\ngroup2_mean.head(10)","6d621d12":"c=df_p_name.reset_index()\nc=c[['user_id','cluster']]\ndf1_c=pd.merge(df1,c,on='user_id',how='left')\ndf1_c=df1_c.dropna(subset=['cluster'])\ndf1_c['cluster']=df1_c['cluster'].apply(int)\ndf1_c.head()","3c0297a2":"df1_c0 = df1_c[df1_c['cluster']==0]\ndf1_c1 = df1_c[df1_c['cluster']==1]","63293649":"df1_c0['members'].mean()","04aa478d":"df1_c1['members'].mean()","4ffbdb0d":"df1_c0['user_rating'].mean()","7ed028a6":"df1_c1['user_rating'].mean()","3da03f84":"## 3.3 Content Based Using Cosine Similarity\nThe dataset does not have desciption of each anime so I will only use \"genre\" and \"type\" to do this content-based recommendation. We will still use codsine similarity.","e52938eb":"#### User with the highest similarity\n**Use Case 4: I am user 3324. I want to find out the only one user who is the most like me and what is him watching .**","b2f0e000":"<font size=\"3\"><font color='purple'>This is a function I found online to deal with memory usage issues when I used Kaggle kernels. <\/font>","8c561d21":"<font size=\"3\"><font color='purple'>The influence factors of these three groups are not that obvious. The first group seems like stand out while I cannot see a huge difference between the other two.<\/font>","138ab8ca":"<img src=\"https:\/\/www.oreilly.com\/library\/view\/statistics-for-machine\/9781788295758\/assets\/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png\" width=\"400px\">","fa2d77e1":"<font size=\"3\"><font color='purple'>These are the examples of 10 movies with the highest weited average ratings. We would recommend movied to users based on this list. In this case, we only consider the ratings of movies and personal taste is not in consideration so this is a good method but it can also be limited.<\/font>","fa72f5ea":"# Anime Recommender System  \n<font size=\"3\">@Cicily Wu<\/font>","dc41334f":"This data set contains information on user preference data from 73,516 users on 12,294 anime. Each user is able to add anime to their completed list and give it a rating and this data set is a compilation of those ratings.\n\nAnime.csv:\n\nanime_id - myanimelist.net's unique id identifying an anime.  \nname - full name of anime.  \ngenre - comma separated list of genres for this anime.  \ntype - movie, TV, OVA, etc.  \nepisodes - how many episodes in this show. (1 if movie).  \nrating - average rating out of 10 for this anime.  \nmembers - number of community members that are in this anime's \"group\".\n\nRating.csv:\n\nuser_id - non identifiable randomly generated user id.  \nanime_id - the anime that this user has rated.  \nrating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).  ","69f956da":"<font size=\"3\"><font color='purple'>In order to minimize the calculation time, I only chose first 500 users rating to make the prediction. <\/font>","aa1ea9a5":"![](https:\/\/image.slidesharecdn.com\/anime-recommendation-1-180121040742\/95\/anime-recommendation-big-data-certification6-1-638.jpg?cb=1516507702)","e06438a2":"#### Predict ratings\n**Use Case 5: I am user5. I want to find out how will I like the famous anime \"Cowboy Bebop\".**","433127cf":"# 2. Dataset Description","e83b8b6f":"<font size=\"3\"><font color='purple'>Principal component analysis(PCA) is being used. First, I will decrease the dimensions to 3 and see how it goes.<\/font>","47faa83c":"# 1. Problem Statement","64639639":"[<font size=\"3\"><font color='purple'>After sorting by the dataframe, the results will only show the users\/animes with the highest similarity number, which is basically how this system works. I built another one with name which might be easier to check.<\/font><\/font>](http:\/\/)","23638ffa":"# 4. User Group Clustering","05742614":"<font size=\"3\"><font color='purple'>This function is actually very useful when you can only remember the key words of the anime name. You can find the full accurate name with this function and input the correct name to find similar animes recommended to you.<\/font>","fbf4c6db":"<font size=\"3\"><font color='purple'>I'll just devide all users into 2 groups.<\/font>","574dacb5":"#### Similar Animes Based \n**Use Case 2: If I like the anime 19, get me more animes I might be interested in.**","d22c85a2":"<font size=\"3\"><font color='purple'>The users similarity table is shown below. We have 15711 users in total in this recommendor system.<\/font>","26379cd7":"#### Content-based animes \n**Use Case 6: I like the anime with the \"doro\" in the name. Recommend me more animes with the similar genre\/type.**","58f91b09":"<font size=\"3\"><font color='purple'>Below is the similarity based on application of TF-IDF model.<\/font>","88b41490":"<font size=\"3\"><font color='purple'>In the \"elbow\" method, it's apparent that 2 groups is the best.  \nI chose another method called \"silhouette scores\", in which case we only need to find the k with the highest scores in the figure. As the result shows below, 2 is still the best.<\/font>","f8745412":"**Use Case 3: I recalled one intersting anime with \"ping\" in the name, can you find that out and provide me other recommendations?**","83435fb1":"# 3. Build Recommendor System","439fa0c5":"#### Similar Users Based \n**Use Case 1: If I am the user 73, get the number of users who are similar to me.**","41d1cb55":"<font size=\"3\"><font color='purple'>I want to chose the best k-how many groups should we separate these users.  \nThe fist method I chose is \u201celbow\" method. I called it \"elbow\" since the shape of this figure is like a elbow and we need to find the elbow joint, and the corresponding k number is the best. The indicator here is SSE-sum of the squared errors. The SSE will decrease sharpenly as the k number grows. Then at a certain point right after k, it will still decrease but the trend is not that obvious. This k is what we are looking for.<\/font>","e094593d":"## 3.1 Weighted Average rating\nThis is a general method to recommend same high rating movies to users. The idea is to calculate a new score to each movie based on number of raters and each person's rating, which makes more sense than just averaging ratings.","cdac02be":"<font size=\"3\"><font color='purple'>The anime similarity table is shown below. We have 2826 animes in total in this recommendor system.<\/font>","5dc538c8":"Recommender systems aim to predict users\u2019 interests and recommend product items that quite likely are interesting for them. They are among the most powerful machine learning systems that online retailers implement in order to drive sales.\n\nData required for recommender systems stems from explicit user ratings after watching a movie or listening to a song, from implicit search engine queries and purchase histories, or from other knowledge about the users\/items themselves.\n\nIn this project, users data from MyAnimeList is being used to build an anime recommendor system based on user viewing and rating history. Several approaches were implemented.","f555e9ad":"## 3.2 User-rating Based Using Cosine Similarity\nCosine similarity is the measure of similarity between two vectors, by computing the cosine of the angle between two vectors projected into multidimensional space. It can be applied to items available on a dataset to compute similarity to one another via keywords or other metrics.","3a4f8ed4":"<font size=\"3\"><font color='purple'>In order to make the computation faster and easer I want to decrease half of anime dataset. In this case, I chose them based on the number of group members. If an anime has more members which means it's more popular, I'll keep it.<\/font>","dbb934ec":"<font size=\"3\"><font color='purple'>The user-anime matrix is obtained. I will fill NaN value with 0 later.<\/font>"}}