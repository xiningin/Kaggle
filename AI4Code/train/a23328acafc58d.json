{"cell_type":{"d10a1142":"code","f30f4e01":"code","56e8b3a5":"code","39f94b05":"code","bf8830aa":"code","78440d10":"code","ccf68376":"code","786ae44e":"code","4743a02e":"code","b0abdf99":"code","0f52a773":"code","f8003461":"code","60f472a4":"code","73c7211e":"code","7c9f418a":"code","ce10709f":"code","426e94c9":"code","f8b2a024":"code","3780c8b5":"code","07c35ff8":"code","87b935b8":"code","6cda1331":"code","487be4a9":"code","dc8fd51d":"code","9427782b":"code","c1a36a69":"code","01c123a7":"code","69c5588c":"code","5a55a26c":"code","e4a88bfb":"code","a74ba186":"code","a369cd57":"code","480bca71":"code","dd67192e":"code","6a4bab66":"code","bda6de02":"code","61cd3224":"code","cb8091ea":"code","0be1f734":"code","4dedd229":"code","c43db732":"code","b30c8509":"code","1cbf7e09":"code","82afe856":"code","6afd3de8":"markdown","7add5dbf":"markdown","f785ac6f":"markdown","a6c681df":"markdown","d71b82d4":"markdown","dd92c4be":"markdown","17b8522e":"markdown","d7725190":"markdown","8e36f79e":"markdown","0ae44359":"markdown","b0f10751":"markdown","5d791406":"markdown","56abdcde":"markdown","ad99b411":"markdown","55fd9d88":"markdown","8a3bbb7f":"markdown","122504cd":"markdown","4c1a6c33":"markdown","9f3eabb6":"markdown","accfc7c8":"markdown","bcbc27d8":"markdown","8b17ab44":"markdown","b5652f9d":"markdown","19ca2e6c":"markdown","587b4fd7":"markdown","8809e8b8":"markdown","e86f7956":"markdown","52bbbba6":"markdown","2e91fc48":"markdown","6a38fa0e":"markdown","5458e24c":"markdown","c8d93345":"markdown","b1ebd8d1":"markdown","e74fa008":"markdown","25d53e6c":"markdown","5b95abc4":"markdown","fb672f54":"markdown","10d614bc":"markdown","60caf8c6":"markdown","16bb3963":"markdown","6152f7d7":"markdown","0f15c4bd":"markdown","0498e9ae":"markdown","c6b8491b":"markdown","3c1fd4d3":"markdown","c8ee8f5c":"markdown","0ee5b8f5":"markdown","942a1d96":"markdown","352f3ce2":"markdown","40ffa856":"markdown"},"source":{"d10a1142":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt","f30f4e01":"# Load the images and the labels\ntrain_raw = pd.read_csv(\"..\/input\/data-files\/x_train_gr_smpl.csv\")\nlabels = pd.read_csv(\"..\/input\/data-files\/y_train_smpl.csv\")\n\n\n# Display top 2 rows for each dataframe\ndisplay(train_raw.head(2))\ndisplay(labels.head(2))\n\n\n# Train contains the image data and labels in the same table.\ntrain = pd.read_csv(\"..\/input\/data-files\/x_train_gr_smpl.csv\")\ntrain['label'] = labels\n","56e8b3a5":"fig = sns.countplot(x=\"label\", data=train)\nfig.set_title(\"Frequency Table\")","39f94b05":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(train_raw, labels, test_size=0.1, shuffle=True, random_state=3)","bf8830aa":"from sklearn.naive_bayes import MultinomialNB","78440d10":"# Create a function that runs a basic Naive Bayes without any optimization\n# that we can run for different combinations of features and compare scores\n\ndef basic_Naive_Bayes(x_train, y_train, x_test, y_test = []):\n    # initiate the Naive Bayes class\n    clf = MultinomialNB()\n    # Train on the train data .values.ravel() is added to y_train to avoid getting an error message.\n    clf.fit(x_train, y_train.values.ravel())\n\n    # Make predictions on the test data.\n    output = clf.predict(x_test)\n    \n    return output\n\noutput = basic_Naive_Bayes(x_train, y_train, x_test)","ccf68376":"from sklearn.metrics import accuracy_score, f1_score, recall_score, classification_report, confusion_matrix, precision_score\n\n# checking the accuracy\naccuracy = accuracy_score(y_true=y_test, y_pred=output)\nprint(\"The accuracy is {0:.4f}\".format(accuracy))\n\n# checking the f1 score\nf1_s = f1_score(y_true=y_test, y_pred=output, average='weighted')\nprint(\"The f1 score is {0:.4f}\".format(f1_s))\n\n# checking the recall for average= micro\nrecall = recall_score(y_true=y_test, y_pred=output, average='micro')\nprint(\"The avg = micro recall is {0:.16f}\".format(recall))\n\n# checking the recall for average= macro\nrecall = recall_score(y_true=y_test, y_pred=output, average='macro')\nprint(\"The avg = macro recall is {0:.16f}\".format(recall))\n\n# checking the recall for average= weighted\nrecall = recall_score(y_true=y_test, y_pred=output, average='weighted')\nprint(\"The avg = weighted recall is {0:.16f}\".format(recall))\n\n\nprint(classification_report(y_true=y_test, y_pred=output))","786ae44e":"from sklearn.metrics import confusion_matrix\n\n# Create confusion matrix from test data\nconf_matrix_data = confusion_matrix(y_true=y_test, y_pred=output)\n\n# Make confusion matrix with colors\nplt.figure(figsize=(10,5)) # Size of the plot\nax = sns.heatmap(conf_matrix_data, annot=True) # Creating the colors","4743a02e":"def naive_bayes (labels):\n    \n    train_data = train_raw \n    x_train, x_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.1, shuffle=True, random_state=3)\n    output = basic_Naive_Bayes(x_train, y_train, x_test)\n    accuracy = accuracy_score(y_true=y_test, y_pred=output)\n    print(\"* Accuracy: {}\".format(accuracy))","b0abdf99":"def test_naive_bayes (paths):\n    \n    # For every file with labels do:\n    for i, path in enumerate(paths):\n        labels = pd.read_csv(path)\n        print(\"\\nCLASS {}\".format(i))\n        naive_bayes(labels)\n        ","0f52a773":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(train_raw, labels.values.ravel())\n\n# Look at the most important features according to the tree\nimportances  = tree.feature_importances_","f8003461":"importances","60f472a4":"def extract_top_indices(my_array, n_indices):\n    # This gets the indices for the top N highest values in my_array\n    feature_idx = my_array.argsort()[-n_indices:][::-1]\n    # convert the indices to string to match column names in the pandas dataframe\n    str_columns = [str(i) for i in feature_idx]\n    \n    return str_columns","73c7211e":"extract_top_indices(importances, train_raw.shape[0])[-5:]","7c9f418a":"def naive_bayes_on_top_N (correlations, n_indices, labels, binary=True, full_report=False):\n    top_N_indices = extract_top_indices(correlations, n_indices)\n    \n    train_data = train_raw[top_N_indices]\n     \n    x_train, x_test, y_train, y_test = train_test_split(train_data, labels, test_size=0.1, shuffle=True, random_state=3)\n    \n    output = basic_Naive_Bayes(x_train, y_train, x_test)\n    \n    # Binary when we want binary classification (0 or 1)\n    # Micro when we want multiclass classification (0 - 9)\n    if binary:\n        avg = 'binary'\n    else:\n        avg='macro'\n        \n    f1_sc = f1_score(y_true=y_test, y_pred=output, average=avg)  # Ratio between precision and recall\n    accuracy = accuracy_score(y_true=y_test, y_pred=output)\n    \n    # To show the full report for each class set full_report to True\n    if full_report:\n        cr=classification_report(y_true=y_test, y_pred=output)\n    else:\n        cr=\"N\/A\"\n\n    # More metrics can be added ...\n    print(\"Running Naive bayes on top {} classes\".format(n_indices))\n    print(\"* Accuracy: {}\".format(accuracy))\n    print(\"* F1 Score: {}\". format(f1_sc))\n    print(\"classification_report: \\n{}\".format(cr))\n    print(\"_________________________________\")","ce10709f":"def test_importances_scores (path, top_n_array):\n        labels = pd.read_csv(path)\n        \n        for top_n in top_n_array:            \n            naive_bayes_on_top_N(importances, top_n, labels, binary=False, full_report=True)\n\n\nall_class_labels_path = \"..\/input\/data-files\/y_train_smpl.csv\"\ntest_importances_scores(all_class_labels_path, [20, 50, 100])","426e94c9":"# Save the locations of the files in a list to easily loop throgh them\nlist_classes = [\n    \"..\/input\/data-files\/y_train_smpl_0.csv\",\n    \"..\/input\/data-files\/y_train_smpl_1.csv\",\n    \"..\/input\/data-files\/y_train_smpl_2.csv\",\n    \"..\/input\/data-files\/y_train_smpl_3.csv\",\n    \"..\/input\/data-files\/y_train_smpl_4.csv\",\n    \"..\/input\/data-files\/y_train_smpl_5.csv\",\n    \"..\/input\/data-files\/y_train_smpl_6.csv\",\n    \"..\/input\/data-files\/y_train_smpl_7.csv\",\n    \"..\/input\/data-files\/y_train_smpl_8.csv\",\n    \"..\/input\/data-files\/y_train_smpl_9.csv\",\n]","f8b2a024":"def test_importances_scores_all (paths, top_n_array):\n    \n    # For every file with labels do:\n    for i, path in enumerate(paths):\n        labels = pd.read_csv(path)\n        # Run the tree\n        tree = DecisionTreeClassifier(random_state=3)\n        tree.fit(train_raw, labels.values.ravel())\n        importances  = tree.feature_importances_\n        \n        print(\"\\nCLASS {}\".format(i))\n    \n        for top_n in top_n_array:\n            naive_bayes_on_top_N(importances, top_n, labels)\n        \ntest_importances_scores_all(list_classes, [20, 50, 100])","3780c8b5":"import matplotlib.pyplot as plt\n\nacc_list = []\n\nfor i in range (1, 81) :\n    # Top 100-i features\n    top_100_indices_i = extract_top_indices(importances, 100)\n    top_100_features_i = train_raw[top_100_indices_i[0:-i]]\n    \n    x_train_100_i, x_test_100_i, y_train_100_i, y_test_100_i = train_test_split(top_100_features_i, labels, test_size=0.1, shuffle=True, random_state=3)\n    \n\n    output_100_i = basic_Naive_Bayes(x_train_100_i, y_train_100_i, x_test_100_i)\n\n    # print(\"=======================================================================\")\n    # print(\"Classification Report for top 100-i features :\")\n    # print(classification_report(y_true=y_test, y_pred=output_100_i))\n    accuracy_100_i = accuracy_score(y_true=y_test, y_pred=output_100_i)\n    # print(\"The accuracy for top 100 features is {0:.4f}\".format(accuracy_100_i))\n    acc_list.append(accuracy_100_i)\n    # print(len(acc_list))\n\n\n#acc_list.reverse() \nplt.plot(acc_list)\nplt.ylabel('Accuracy based on number of top attributes')\nplt.show()\n    \n","07c35ff8":"import scipy as sc","87b935b8":"def get_correlation_array(path):\n    # Read the file with the classes\n    class_labels = pd.read_csv(path)\n    \n    correlations = []\n    \n    for column in train_raw.columns:\n        corr, _ = sc.stats.pearsonr(train_raw[[column]], class_labels[[\"0\"]])\n        correlations.append(corr[0])\n\n    correlations = np.asarray(correlations)\n    # Get the absolute values (we want the closest values to 1 or -1)\n    correlations = np.absolute(correlations)\n    \n    return correlations\n","6cda1331":"def test_correlation_scores_all(path, top_n_array):\n        correlations = get_correlation_array(path)\n        labels = pd.read_csv(path)\n        \n        for top_n in top_n_array:            \n            naive_bayes_on_top_N(correlations, top_n, labels, binary=False, full_report=True)\n            \nall_class_labels_path = \"..\/input\/data-files\/y_train_smpl.csv\"\ntest_correlation_scores_all(all_class_labels_path, [20, 50, 100])","487be4a9":"def test_correlation_scores (paths, top_n_array):\n    \n    # For every file with labels do:\n    for i, path in enumerate(paths):\n        correlations = get_correlation_array(path)\n        print(\"\\nCLASS {}\".format(i))\n        \n        labels = pd.read_csv(path)\n        for top_n in top_n_array:\n            naive_bayes_on_top_N(correlations, top_n, labels)\n        \n\ntest_correlation_scores (list_classes, [20, 50, 100])","dc8fd51d":"# Comment out whichever one you dont want to use.\n#scores_array = correlations = get_correlation_array('..\/input\/data-files\/y_train_smpl.csv')\nscores_array = importances\nn_features = 50 # Use whichever number we want...\n\n# I'll use the top n according to tree data.\ntop_N_indices = extract_top_indices(scores_array, n_features)","9427782b":"# Assign our features and labels to variables.\nx_data = pd.DataFrame(train_raw[top_N_indices])\ny_data = pd.DataFrame(labels)\n\n# Split the data into train and test\nx_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, shuffle=True, random_state=3)","c1a36a69":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import GridSearchCV\n\ndef fit_model(x_train, y_train, clf, params):    \n    # The classifier\n    estimator = clf()\n    \n    # Convert metric into scorer\n    scorer = make_scorer(accuracy_score)\n    \n    # (estimator, param_grid, scoring, cv) which have values 'regressor', 'params', 'scoring_fnc', and 'cv_sets' respectively.\n    grid = GridSearchCV(estimator=estimator, param_grid=params, scoring=scorer, cv=10)\n\n    # Fit the grid search object to the data to compute the optimal model\n    grid = grid.fit(x_train, y_train.values.ravel())\n    \n    print(\"Best params: {}\".format(grid.best_params_))\n\n    # Return the optimal model after fitting the data\n    return grid.best_estimator_","01c123a7":"def run_metrics (y_true, output):\n    accuracy = accuracy_score(y_true, output)\n    f1_sc = f1_score(y_true, output, average='weighted')\n    \n    print(\"* Accuracy: {}\".format(accuracy))\n    print(\"* F1 Score: {}\". format(f1_sc))","69c5588c":"from sklearn.naive_bayes import MultinomialNB\n\n# Parameters to try\nparams = {'alpha': [0.1, 0.2, 0.4, 0.7, 0.9, 1],\n          'fit_prior': [True, False]}\n\nclf_MNB = fit_model(x_train, y_train, MultinomialNB, params)\noutput = clf_MNB.predict(x_test)\n\nrun_metrics(y_test, output)","5a55a26c":"# Create confusion matrix from test data\nconf_matrix_data = confusion_matrix(y_true=y_test, y_pred=output)\n\n# Make confusion matrix with colors\nplt.figure(figsize=(10,5)) # Size of the plot\nax = sns.heatmap(conf_matrix_data, annot=True) # Creating the colors","e4a88bfb":"from sklearn.naive_bayes import GaussianNB\n\n# Parameters to try\nparams = {'var_smoothing': [1e-9, 1e-9, 1e-7, 1e-6, 1e-5, 1e-4]}\n\nclf_Gauss = fit_model(x_train, y_train, GaussianNB, params)\noutput = clf_Gauss.predict(x_test)\n\nrun_metrics(y_test, output)","a74ba186":"from sklearn.naive_bayes import ComplementNB\n\n# Parameters to try\nparams = {'alpha': [1e-10, 1e-4, 1e-3, 1e-2, 0.1, 0.5, 0.9, 1],\n         'fit_prior': [True, False],\n          'norm': [True, False]\n         }\n\nclf_Compl = fit_model(x_train, y_train, ComplementNB, params)\noutput = clf_Compl.predict(x_test)\n\nrun_metrics(y_test, output)","a369cd57":"from sklearn.naive_bayes import BernoulliNB\n\n# Parameters to try\nparams = {'alpha': [1e-10, 1e-4, 1e-3, 1e-2, 0.1, 0.5, 0.9, 1],\n         'fit_prior': [True, False],\n          'binarize': [0.0, 0.2, 0.4, 0.5, 0.7, 0.8, 1]\n         }\n\nclf_Compl = fit_model(x_train, y_train, BernoulliNB, params)\noutput = clf_Compl.predict(x_test)\n\nrun_metrics(y_test, output)","480bca71":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix","dd67192e":"#Function k_means applies k-means clustering alrorithm on dataset and build confusion matrix of cluster and actual labels\n\ndef k_means(n_clust, data_frame, true_labels):\n    #Initializing parameters \n    k_means = KMeans(n_clusters = n_clust, random_state=123, n_init=10)\n    #Compute k-means clustering\n    k_means.fit(data_frame)\n    c_labels = k_means.labels_\n    c_labels.tolist()\n    data_frame['label'] = c_labels \n    #train['label'] = labels\n    #Compute cluster centers and predict cluster index for each sample\n    y_clust = k_means.predict(data_frame)\n    fig = sns.countplot(x=\"label\", data=data_frame)\n    fig.set_title(\"Frequency Table\")\n    print('Adjusted rand score :',adjusted_rand_score(true_labels, y_clust))\n  ","6a4bab66":"Data = pd.DataFrame(train_raw)\nData['label'] = labels\nLabels = Data['label']\nprint('Kmeans result : All Data')\nk_means(n_clust='10', data_frame=Data, true_labels=Labels)\n#fig = sns.countplot(x=\"label\", data=Data)\n#fig.set_title(\"Frequency Table\")","bda6de02":"k_means(n_clust=10, data_frame=Data, true_labels=Labels)","61cd3224":"def test_k_means(top_n_array):\n        #Top features\n        scores_array = importances\n        labels = pd.read_csv(\"..\/input\/data-files\/y_train_smpl.csv\")\n        for top_n in top_n_array:\n            top_N_indices = extract_top_indices(scores_array, top_n)\n            Data = pd.DataFrame(train_raw[top_N_indices])\n            Data['label'] = labels\n            Labels = Data['label']\n            print('Kmeans result : Features',top_n)\n            k_means(n_clust=10, data_frame=Data, true_labels=Labels)\n        \n\ntest_k_means ([10, 50, 100])","cb8091ea":"from time import time\nimport numpy as np\nfrom scipy import ndimage\nfrom matplotlib import pyplot as plt\nfrom sklearn import manifold\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.metrics import confusion_matrix, adjusted_rand_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.cluster import Birch\nfrom sklearn import metrics\n\n\n","0be1f734":"Data = pd.DataFrame(train_raw)\nData['label'] = labels\nLabels = Data['label']\n\ny_test_1d = []\nfor i in range(0, y_test.shape[0]):\n    y_test_1d.append(y_test[i])\n\ny_train_1d = []\nfor i in range(0, y_train.shape[0]):\n    y_train_1d.append(y_train[i])","4dedd229":"def Birch_clustering(n_clust, data, label, branching_factor, threshold):\n    print('Birch clustering :')\n    brc = Birch(branching_factor=50, n_clusters=10, threshold=0.5,compute_labels=True)\n    pred_labels = brc.fit_predict(data)\n    c_labels = brc.labels_   \n    \n    # Make confusion matrix with colors\n    conf_matrix_data = confusion_matrix(y_true=label, y_pred=pred_labels)    \n    plt.figure(figsize=(20, 10))  # Size of the plot\n    ax = sns.heatmap(conf_matrix_data, annot=True)  # Creating the colors\n    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(label, c_labels))\n    print(\"Completeness: %0.3f\" % metrics.completeness_score(label, c_labels))\n    print(\"V-measure: %0.3f\" % metrics.v_measure_score(label, c_labels))\n    print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(label, c_labels))\n    print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(label, c_labels,average_method='arithmetic'))\n    fig_Birch = sns.countplot(x=\"Birch clustering : label\", data=pred_labels)\n    fig_Birch.set_title(\"Frequency Table\")\n    \ndef Affinity_clustering(pref, data, label):\n    print('Affinity clustering :')\n    # Compute Affinity Propagation\n    af = AffinityPropagation(preference=pref).fit(data)\n    cluster_centers_indices = af.cluster_centers_indices_\n    labels = af.labels_\n    pred_labels = af.fit_predict(data)\n    n_clusters_ = len(cluster_centers_indices)\n    # Make confusion matrix with colors\n    conf_matrix_data = confusion_matrix(y_true=label, y_pred=pred_labels)    \n    plt.figure(figsize=(20, 10))  # Size of the plot\n    ax = sns.heatmap(conf_matrix_data, annot=True)  # Creating the colors\n    print('Estimated number of clusters: %d' % n_clusters_)\n    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(label, labels))\n    print(\"Completeness: %0.3f\" % metrics.completeness_score(label, labels))\n    print(\"V-measure: %0.3f\" % metrics.v_measure_score(label, labels))\n    print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(label, labels))\n    print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(label, labels,average_method='arithmetic'))\n    fig_Aff = sns.countplot(x=\"Affinity Propagation : label\", data=pred_labels)\n    fig_Aff.set_title(\"Frequency Table\")\n    \n    \ndef Hierarchical_clustering(n_clust, data, label, linkage):    \n    print('Hierarchical clustering :')\n    clustering = AgglomerativeClustering(linkage=linkage, n_clusters=n_clust)\n    #clustering.fit(data)\n    pred_labels = clustering.fit_predict(data)\n    c_labels = clustering.labels_ \n    conf_matrix_data = confusion_matrix(y_true=label, y_pred=pred_labels)\n    # Make confusion matrix with colors\n    plt.figure(figsize=(20,10)) # Size of the plot\n    ax = sns.heatmap(conf_matrix_data, annot=True) # Creating the colors\n    print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(label, c_labels))\n    print(\"Completeness: %0.3f\" % metrics.completeness_score(label, c_labels))\n    print(\"V-measure: %0.3f\" % metrics.v_measure_score(label, c_labels))\n    print(\"Adjusted Rand Index: %0.3f\" % metrics.adjusted_rand_score(label, c_labels))\n    print(\"Adjusted Mutual Information: %0.3f\" % metrics.adjusted_mutual_info_score(label, c_labels,average_method='arithmetic'))\n    fig_HC = sns.countplot(x=\"Hierarchical Clustering: label\", data=pred_labels)\n    fig_HC.set_title(\"Frequency Table\")\n\n","c43db732":"Hierarchical_clustering(n_clust = 10,data = Data, label = Labels, linkage = \"ward\")\nHierarchical_clustering(n_clust = 'none',data = Data, label = Labels, linkage = \"ward\")\n#Hierarchical_clustering(n_clust = 10,data = Data, label = Labels, linkage = \"complete\")\n#Hierarchical_clustering(n_clust = 10,data = Data, label = Labels, linkage = \"average\")\nHierarchical_clustering(n_clust = 10,data = Data, label = Labels, linkage = \"single\")\n","b30c8509":"Birch_clustering(n_clust = 1, data = Data, label = Labels, branching_factor =50, threshold =0.5)\nBirch_clustering(n_clust = 10, data = Data, label = Labels, branching_factor =50, threshold =0.5)\nBirch_clustering(n_clust = 'none', data = Data, label = Labels, branching_factor =50, threshold =0.5)\n#Birch_clustering(n_clust = 100, data = Data, label = Labels, branching_factor =50, threshold =0.5)","1cbf7e09":"Affinity_clustering(pref = -50, data = Data, label = Labels)\nAffinity_clustering(pref = 0, data = Data, label = Labels)\nAffinity_clustering(pref = 10, data = Data, label = Labels)\n#Affinity_clustering(pref = 100, data = Data, label = Labels)","82afe856":"Birch_clustering(n_clust = 'none', data = Data, label = Labels, branching_factor =50, threshold =0.5)","6afd3de8":"Class 4,9,7 seems to be hard to predict ","7add5dbf":"# Feature Selection\n\nWe will explore 2 feature selection techniques: one will be using **decision trees** to extract the most important features according to the tree classification; the other one wil be using **correlation** between each feature and the target class. In both cases we will try it for all the classes at once, and for each class individually.","f785ac6f":"**Conclusions Part 3**:\n\nIt looks like our original algorithm (Multinomial NB) is the best at predicting our data. ","a6c681df":"The original score without making any changes to the data using Naive Bayes gives us an accuracy of ~ 0.4629","d71b82d4":"**Conclusion using Correlation:** Correlation improves the score a bit, getting up the accuracy up to ~ 65% for 50 classes. However the the results are still lower than using a decision tree. Correlation shows the relationship between the feature and the class, however correlation doesn't imply causation, which might be why the scores aren't as good as using the trees. ","dd92c4be":"Okay results. This shows that the algorithm used is an okay fit for our data but there is still room for improvement.","17b8522e":"1. Applying K-means as the data comes. \n   \n   The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different      clusters in the predicted and true clusterings.","d7725190":"Here we implement naive bayes without having made any changes in the data. Applying Naive bayes as the data comes.","8e36f79e":"# Part 3\n","0ae44359":"We can see that using correlation, the model has better scores when predicting for the top 50 classes.","b0f10751":"THe results are improved massively when using only a few features.","5d791406":"Really bad results. This shows that the algorithm used might not be a good fit for our data.","56abdcde":"**MultiNomial Naive bayes**","ad99b411":"**Using Grid Search**\n\nIll use [Grid search](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) to find the best set of parameters for each model. Grid search runs the model for different combinations of parameters and returns the best one.","55fd9d88":"## Using correlation","8a3bbb7f":"# Part 4","122504cd":"Conclusion : We obtained really bad results with K-means algorithm. The original labels are hard to recover using K-means. Maybe other implementations will be more appropiate for this data.","4c1a6c33":"**Using correlation for all classes AT ONCE**\n\nWe will use correlation for ``y_train_smpl.csv`` which contains all the classes, and find the best features that best describe all classes.","9f3eabb6":"Here we implement K-means","accfc7c8":"1Extract the top N features from the array with the best scores according to the tree (Copied some code from the link below)\n\n[https:\/\/stackoverflow.com\/questions\/6910641\/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array](https:\/\/stackoverflow.com\/questions\/6910641\/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array)[](http:\/\/)","bcbc27d8":"# PART 1\n\nImplement basic naive bayes and feature extraction.","8b17ab44":"Load the csv files into panda's dataset","b5652f9d":"**Using the decision tree for all labels individually**\n\nWe use y_train_smpl_0.csv to y_train_smpl_9.csv as our labels and calculate the feature importances using a tree for each of them individually.","19ca2e6c":"### Load the csv data","587b4fd7":"Really bad results. This shows that the algorithm used might not be a good fit for our data.","8809e8b8":"**Decision Tree**: First we'll use a decision tree to see what are the most important features used to make splits.\n\nWe'll run the tree with all the data just to see how it separates it into branches. After we will select the top N branches accoding to the tree.","e86f7956":"We will require a 1D version of the y_test and y_train labels for the metric functions","52bbbba6":"### Implementing Naive Bayes algorithm\n\nDocumentation about Naive bayes here: [Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/naive_bayes.html)","2e91fc48":"Classes 0,1,3,4,5 and 8 have the most samples. The rest of the classes have less than half of the samples.","6a38fa0e":"**Complement Naive Bayes**\n\n[Complement Naive Bayes link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.ComplementNB.html#sklearn.naive_bayes.ComplementNB)","5458e24c":"## Using A decision tree","c8d93345":"### The dataset.\n\nWe will use the top N features that we extracted in part 1.","b1ebd8d1":"2. Applying K-means on a reduced Data\n   \n   We will be using **decision trees** to extract the most important features according to the tree classification;","e74fa008":"This graph shows the change in accuracy when we use the top scored n classes. We can see that the model has a better accuracy when it uses around 50 classes, After that starts to drop dramatically.","25d53e6c":"**Using the decision tree for all classes at once**\n\nWe use y_train_smpl.csv as our labels which contain all the classes.","5b95abc4":"In this part we'll try 3 different clustering algorithms : Affinity Propagation, Hierarchical Clustering and Birch.","fb672f54":"### Evaluate the model\n\nUse several metrics to evaluate the model: accuracy, recall, precision ...\n\n[List of metrics we can use](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#classification-metrics)","10d614bc":"We can see that this time most classes get classified correctly.","60caf8c6":"### Using improved implementations of Naive Bayes","16bb3963":"**Using correlation for each class individually**\n\nWe will use correlation for each class file y_train_smpl_0.csv to y_train_smpl_9.csv individually and extract the top features that describe each using correlation.","6152f7d7":"**Bernoulli Naive Bayes**\n\n[Bernoulli Naive bayes link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.BernoulliNB.html#sklearn.naive_bayes.BernoulliNB)","0f15c4bd":"### Randomize data & split it into train and test\n\nShuffle the data. For this we use the train_test_split function which splits the data into train data and test data. There is an option in this function that shuffles the data as well.","0498e9ae":"Confusion Matrix on the test data: Shows how many of the test data points have been correctly classified and which ones haven't. We can see that some classes are better predicted than others. For example label 5 is very easy to predict, since most of them are predicted correctly with very few misclassifications. On the other hand class 7 is very hard to predict. We see that most predictions are wrong, with only a few predicted correctly.","c6b8491b":"#### Visualizing the optimal number of features for all classes.","3c1fd4d3":"# PART 2\n\nBuild two or three Bayes networks of more complex architecture for (a smaller version of) this data set.","c8ee8f5c":"``naive_bayes_on_top_N`` is a function that separates the data into train and test and runs Naive Bayes. Gives out some metrics. The parameters are:\n* Correlations is the array with the scores for each feature. In this the case of the tree is not correlations, it is feature importances.\n* n_indices: the number of indices we want to extract\n* labels: the labels we want to use.\n* binary (boolean): when the labels are binary. Otherwise set to False.\n* full_report: wheather or not you want to see a full report for each classification.","0ee5b8f5":"**Gaussian Naive Bayes**\n\n[Gaussian Naive Bayes Link](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)","942a1d96":"### Implementing K-Means algorithm\n\nDocumentation about K-Means here: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html","352f3ce2":"### Explore how many different classes there are","40ffa856":"**Conclusion on decision tree:** The decision tree used as feature extraction provides very good results, improving the accuracy from ~ 45% to ~ 90%. Decision tree is very useful for feature extraction because if checks how much information is gained when we split the data based in some features. At the end it shows what features had more information gain when making the splits, which shows what features affect the results the most."}}