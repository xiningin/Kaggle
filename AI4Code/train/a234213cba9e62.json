{"cell_type":{"4da495c9":"code","97591fe1":"code","ff4eae05":"code","fee51496":"code","d2b2c07d":"code","bf9688ae":"code","84efd878":"code","c0c31dba":"code","0a3c0913":"code","855a1bb5":"code","310bf552":"code","54b84de0":"markdown","49698512":"markdown"},"source":{"4da495c9":"import math\nimport numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.inspection import plot_partial_dependence\n%matplotlib inline\nsns.set_style('whitegrid')\n\n# Processing\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, LabelEncoder, StandardScaler, RobustScaler\nimport category_encoders as ce # Count, Target, CatBoost\n\n# Models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, BaggingClassifier,\n        ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier)\nfrom sklearn.naive_bayes import BernoulliNB, GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import (LogisticRegression, Perceptron, SGDClassifier,\n        LogisticRegression, PassiveAggressiveClassifier, RidgeClassifier)\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Evaluation\nfrom sklearn.model_selection import cross_validate, cross_val_score\n\ndata_raw = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain = data_raw.copy(deep=True)\ndata_all = [train, data_test]\n\n# pp.ProfileReport(train)","97591fe1":"# Complete\/Delete\nfor dataset in data_all:\n    dataset.drop(['Ticket', 'Cabin'], axis=1, inplace=True)\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace=True)\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n    for c in set(dataset['Pclass']):\n        for s in set(dataset['Sex']):\n            age_median = dataset[(dataset['Pclass'] == c) & (dataset['Sex'] == s)]['Age'].median()\n            dataset.loc[(dataset['Age'].isnull()) & (dataset['Pclass'] == c) & (dataset['Sex'] == s), 'Age'] = age_median\ntrain.drop(['PassengerId'], axis=1, inplace=True)\n\n# Create\ndef get_titles(series):\n    return series.str.extract(' ([a-zA-Z]+)\\.', expand=False)\n\nfor dataset in data_all:\n    dataset['FamilySize'] = dataset['Parch'] + dataset['SibSp'] + 1\n    dataset['IsAlone'] = (dataset['FamilySize'] == 1).astype(int)\n    \n    dataset['Title'] = get_titles(dataset['Name'])\n    title_counts = dataset['Title'].value_counts()\n    dataset['Title'] = dataset['Title'].map(lambda t: t if title_counts[t] >= 10 else 'Rare')\n    dataset.drop('Name', axis=1, inplace=True)\n    \n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n    dataset['AgeBin'] = pd.cut(dataset['Age'], 5)\n\n# Convert\ncategorical_cols = ['Sex', 'Embarked', 'Title', 'AgeBin', 'FareBin']\nnumerical_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize', 'IsAlone']\n\nle = LabelEncoder()\nohe = OneHotEncoder(sparse=False)\nfor dataset in data_all:\n    for col in categorical_cols:\n        dataset[col + '_Code'] = le.fit_transform(dataset[col])\n\n# Scale\n# ss = StandardScaler()\n# trn_ss = pd.DataFrame(ss.fit_transform(train[numerical_cols]), index=train.index, columns=numerical_cols)\n# test_ss = pd.DataFrame(ss.transform(data_test[numerical_cols]), index=data_test.index, columns=numerical_cols)\n# train['Age_Scaled'] = trn_ss['Age']\n# data_test['Age_Scaled'] = test_ss['Age']\n\n# rs = RobustScaler()\n# trn_rs = pd.DataFrame(rs.fit_transform(train[numerical_cols]), index=train.index, columns=numerical_cols)\n# test_rs = pd.DataFrame(rs.transform(data_test[numerical_cols]), index=data_test.index, columns=numerical_cols)\n# train['Fare_Scaled'] = trn_ss['Fare']\n# data_test['Fare_Scaled'] = test_ss['Fare']\n\n# train.describe(include='all')","ff4eae05":"def correlation_heatmap(df):\n    plt.figure(figsize=(20, 10))\n    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n    sns.heatmap(df.corr(), cmap=colormap, annot=True)\n    plt.title('Pearson Correlation of Features', size=15)\n\n# correlation_heatmap(train)","fee51496":"train = train.dropna(axis=0, subset=['Survived'])\ntarg_col = 'Survived'\n# print('int & float cols:', train.columns[(train.dtypes == int) | (train.dtypes == float)].values)\n\nfeature_cols = ['Pclass', 'FamilySize', 'IsAlone', 'Sex_Code', 'Embarked_Code',\n        'Title_Code', 'AgeBin_Code', 'FareBin_Code']\n# print('chosen cols:', feature_cols)\n# train[feature_cols].describe()","d2b2c07d":"# correlation_heatmap(train[feature_cols])","bf9688ae":"classifiers = [\n    KNeighborsClassifier(**{'n_jobs': -1, 'n_neighbors': 7}),\n    SVC(**{'C': 0.390625, 'kernel': 'poly', 'probability': True, 'random_state': 0}),\n    LinearSVC(**{'C': 25.0, 'dual': False, 'loss': 'squared_hinge', 'max_iter': 25000, 'penalty': 'l1', 'random_state': 0}),\n    DecisionTreeClassifier(**{'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 8, 'random_state': 0}),\n    RandomForestClassifier(**{'criterion': 'entropy', 'max_depth': 12, 'min_samples_split': 8, 'n_estimators': 300, 'n_jobs': -1, 'random_state': 0}),\n    AdaBoostClassifier(**{'algorithm': 'SAMME', 'learning_rate': 0.3, 'n_estimators': 300, 'random_state': 0}),\n    BaggingClassifier(**{'max_samples': 0.1, 'n_estimators': 50, 'n_jobs': -1, 'random_state': 0}),\n    ExtraTreesClassifier(**{'criterion': 'gini', 'max_depth': 12, 'min_samples_split': 12, 'n_estimators': 200, 'n_jobs': -1, 'random_state': 0}),\n    GradientBoostingClassifier(**{'learning_rate': 0.3, 'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 10, 'random_state': 0}),\n    BernoulliNB(**{'alpha': 0.1}),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(**{'shrinkage': None, 'solver': 'svd'}),\n    QuadraticDiscriminantAnalysis(),\n    Perceptron(**{'early_stopping': True, 'n_jobs': -1, 'penalty': 'l2', 'random_state': 0}),\n    LogisticRegression(**{'C': 0.09765625, 'max_iter': 20, 'n_jobs': -1, 'penalty': 'l2', 'random_state': 0, 'solver': 'lbfgs'}),\n    PassiveAggressiveClassifier(**{'C': 0.01220703125, 'early_stopping': True, 'loss': 'hinge', 'max_iter': 5, 'n_jobs': -1, 'random_state': 0}),\n    RidgeClassifier(**{'alpha': 0.5, 'normalize': True, 'random_state': 0}),\n    GaussianProcessClassifier(**{'max_iter_predict': 10, 'n_jobs': -1, 'random_state': 0}),\n    XGBClassifier(**{'booster': 'gbtree', 'learning_rate': 0.1, 'max_depth': 10, 'n_jobs': -1, 'random_state': 0, 'reg_alpha': 0.16, 'reg_lambda': 2.56}),\n    LGBMClassifier(**{'boosting_type': 'goss', 'learning_rate': 0.1, 'n_estimators': 1000, 'n_jobs': -1, 'random_state': 0, 'reg_alpha': 0.16, 'reg_lambda': 0}),\n]\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)","84efd878":"clf_scores = pd.DataFrame(columns=['Classifier', 'Test Score', 'Test Score 3*STD'])\nclf_preds = pd.DataFrame(train[targ_col])\nfor i in range(len(classifiers)):\n    clf = classifiers[i]\n    clf_name = clf.__class__.__name__\n#     RFE\n#     if hasattr(clf, 'coef_') or hasattr(clf, 'feature_importances_'):\n#         clf = RFECV(clf, scoring='accuracy', cv=skf)\n#         clf.fit(train[feature_cols], train[targ_col])\n#         rfe_cols = feature_cols.values[clf.support_]\n    clf.fit(train[feature_cols], train[targ_col])\n    \n    cv_results = cross_val_score(clf, train[feature_cols], train[targ_col], cv=skf)\n#     cv_results = cross_val_score(model, train[rfe_cols[clf_name] if hasattr(clf, 'coef_') or hasattr(clf, 'feature_importances_') else feature_cols], train[targ_col], cv=skf)\n    clf_scores.loc[i] = [clf_name, cv_results.mean(), cv_results.std() * 3]\n    clf_preds[clf_name] = clf.predict(train[feature_cols])\nclf_scores","c0c31dba":"correlation_heatmap(clf_preds)","0a3c0913":"vote_eligible = [\n    clf.__class__.__name__ for clf in classifiers\n    if hasattr(clf, 'predict_proba')\n    and not clf.__class__.__name__ in ['AdaBoostClassifier', 'ExtraTreesClassifier', 'SVC', 'LGBMClassifier', 'DecisionTreeClassifier', 'RandomForestClassifier']\n]\nprint(vote_eligible)\ncorrelation_heatmap(clf_preds[vote_eligible])","855a1bb5":"def print_scores_info(model_name, scores):\n    mean = scores.mean() * 100\n    std_3 = scores.std() * 100 * 3\n    print(model_name, 'score mean: ', mean)\n    print(model_name, 'score 3 std range: ', mean - std_3, '\u2014', mean + std_3)","310bf552":"vote_classifiers = [\n    ('knn', KNeighborsClassifier(**{'n_jobs': -1, 'n_neighbors': 7})),\n    ('bag', BaggingClassifier(**{'max_samples': 0.1, 'n_estimators': 50, 'n_jobs': -1, 'random_state': 0})),\n    ('gbc', GradientBoostingClassifier(**{'learning_rate': 0.3, 'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 10, 'random_state': 0})),\n    ('bnb', BernoulliNB(**{'alpha': 0.1})),\n    ('gnb', GaussianNB()),\n    ('lda', LinearDiscriminantAnalysis(**{'shrinkage': None, 'solver': 'svd'})),\n    ('qda', QuadraticDiscriminantAnalysis()),\n    ('log', LogisticRegression(**{'C': 0.09765625, 'max_iter': 20, 'n_jobs': -1, 'penalty': 'l2', 'random_state': 0, 'solver': 'lbfgs'})),\n    ('gpc', GaussianProcessClassifier(**{'max_iter_predict': 10, 'n_jobs': -1, 'random_state': 0})),\n    ('xgb', XGBClassifier(**{'booster': 'gbtree', 'learning_rate': 0.1, 'max_depth': 10, 'n_jobs': -1, 'random_state': 0, 'reg_alpha': 0.16, 'reg_lambda': 2.56})),\n]\n\nvote_hard_clf = VotingClassifier(vote_classifiers, voting='hard')\n\nprint_scores_info('Vote Hard', cross_val_score(vote_hard_clf, train[feature_cols], train[targ_col], cv=skf))\n\nvote_hard_clf.fit(train[feature_cols], train[targ_col])\npreds_test = vote_hard_clf.predict(data_test[feature_cols])\n\noutput = pd.DataFrame({'PassengerId': data_test.PassengerId, 'Survived': preds_test})\noutput.to_csv('submission.csv', index=False)","54b84de0":"# Model","49698512":"followed: https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\/notebook\n\n# TO-DO:\n- improve\n    - plot learning curves?"}}