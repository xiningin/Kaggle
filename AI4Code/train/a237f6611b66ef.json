{"cell_type":{"d1fe967e":"code","b07faa42":"code","5071f4e5":"code","65d3ab77":"code","310f7e91":"code","3a6d7420":"code","bb44e70d":"code","6b9b05c2":"code","0529adf3":"code","47b09542":"code","7692617c":"code","82e0f900":"code","ed111e4e":"code","b71905ba":"code","5af7025c":"code","7f9b1169":"code","cc029d44":"code","3308ae84":"code","ea3e079e":"code","f752289f":"code","7ddd9f94":"code","f7d91530":"code","ce5d0b64":"code","0ad178ed":"code","5a9e821f":"code","8e58e4d9":"code","72396558":"code","7d58fca9":"code","c1cc84ee":"code","275eb1b1":"code","fef0bc33":"code","77062b08":"code","5af5a667":"code","3441d409":"code","d1293b86":"code","b40f626f":"code","65c2c177":"code","19c2fee8":"code","11631d92":"code","6b0ccc34":"code","e915ebd7":"code","b5c3f50f":"code","e3b75431":"code","6e154985":"code","38df2685":"markdown"},"source":{"d1fe967e":"# some notes\n\n# I found this dataset from https:\/\/www.reddit.com\/r\/datasets\/comments\/jd788k\/corn_leaf_infection_dataset_taken_from_field\/\n# As of starting this notebook, this dataset has only been out for 2 days. (19\/10\/2020)\n# This serves as a Logistic Regression Benchmark for future classification projects for this dataset\n\n# If you can use my pre processing pipeline or logic, please cite your source back to this notebook\n# Let's help farmers and their corn crops!","b07faa42":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\n\n# Load images as arrays\nimport PIL\n\n# Filenames via Object Generators\nfrom pathlib import Path\n\n# Python Generators\nimport itertools\nfrom itertools import chain\n\n#Sklearn image preprocessing\nfrom skimage.transform import rescale\n\n# Machine Learning\nfrom sklearn.preprocessing import Normalizer, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\n# Metrics\nfrom sklearn.metrics import plot_confusion_matrix\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n''';\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5071f4e5":"data_raw = pd.read_csv('\/kaggle\/input\/corn-leaf-infection-dataset\/Annotation-export.csv')","65d3ab77":"df = data_raw.copy()","310f7e91":"df.head()","3a6d7420":"df.info()","bb44e70d":"df['label'].value_counts()","6b9b05c2":"'\/kaggle\/input\/corn-leaf-infection-dataset\/Corn Disease detection\/Infected\/'+df.head().loc[0, 'image']","0529adf3":"filenamelist_healthy_generator = Path('\/kaggle\/input\/corn-leaf-infection-dataset\/Corn Disease detection\/Healthy corn\/').glob('*.jpg')","47b09542":"healthy_corn_filenames = [i for i in filenamelist_healthy_generator]","7692617c":"plt.figure(figsize=(20,15))\ngs1 = matplotlib.gridspec.GridSpec(2, 4)\ngs1.update(wspace=0.2, hspace=0.2)\nunique_infected_images = df['image'].unique()\nfor i in range(1,5):\n    plt.subplot(2,4,i)  \n    img = matplotlib.image.imread('\/kaggle\/input\/corn-leaf-infection-dataset\/Corn Disease detection\/Infected\/'+unique_infected_images[i])\n    plt.imshow(img)\n    plt.title('Infected')\n    currentAxes = plt.gca()\n    \n    df_sorted = df[df['image'] == unique_infected_images[i]]\n    \n    for j in [df_sorted.reset_index(drop=True).loc[i, ['xmin', 'ymin', 'xmax', 'ymax']] for i in range(len(df_sorted))]:\n        currentAxes.add_patch(matplotlib.patches.Rectangle(xy=(j['xmin'], j['ymin']), \n                                                           height=abs(j['ymax'] - j['ymin']),\n                                                           width=abs(j['xmax'] - j['xmin']), \n                                                           color='red', \n                                                           linewidth=2, \n                                                           fill=False))\n        plt.scatter(x=[j['xmin'], j['xmax']], y=[j['ymin'], j['ymax']], c='r')\n    #plt.show()\n\nfor i in range(1,5):\n    plt.subplot(2, 4,i+4)\n    img = matplotlib.image.imread(healthy_corn_filenames[i])\n    plt.imshow(img)\n    plt.title('Healthy');\n    \nplt.tight_layout()\nplt.savefig('\/kaggle\/working\/example_corn.png', bbox_inches='tight', pad_inches=0)","82e0f900":"# From here on, we are going to be focusing on the easier of the two tasks of the dataset\n\n# Find a best model to classify Infected vs Healthy leaves\n\n# We are going to use a variety of statistical models to do a bernoulli classification problem: \n    # https:\/\/scikit-learn.org\/0.15\/_images\/plot_classifier_comparison_0011.png\n    \n    # 0: There is not disease on the leaf\n    # 1: There is disease on the leaf\n    \n# What am I going to use?\n    # Sklearn Classification: https:\/\/scikit-learn.org\/0.15\/_images\/plot_classifier_comparison_0011.png\n    # We can consider the BGR image as a (width x height x color) tensor array. \n    # I believe that color is a large indicator of detecting disease so I'm not going to go down to grayscale\n    \n    # To input the image, we need to reduce them down. \n    # Thankfully, because these were all taken with the same photo, they are all the same aspect ratio and I don't need to worry about skewing or shearing them\n        # See here: https:\/\/machinelearningmastery.com\/how-to-manually-scale-image-pixel-data-for-deep-learning\/\n        \n        # Normalize:\n            # I am going to need to rescale these tensors from (0, 255) to (0, 1) to handle preprocessing\n                # Source: experience. I already know I am going to get improved preformance by scaling\n                \n        # Centering:\n            # I want to subtract the mean value to center the data around 0\n            # Source: Experience. I've found that by normalizing and centering, we lose no information of what was in the image but\n                # Instead give our estimator a much easier time in processing through the data by not dealing with extremely large and small numbers\n                \n                # Note, there may be some merit in comparing centering before and after normalization. We'll take that into account here\n                \n                # I am going to do local centering per image, because I think there is value in each matrix being an instance and I am trying to find differences\n                # in each matrix.","ed111e4e":"# Let's test it on one image. We're going to use the first infected image we saw above\n# Load it in with PIL, convert to an dense numpy array, see the first pixel in format RGB","b71905ba":"test_filename = '\/kaggle\/input\/corn-leaf-infection-dataset\/Corn Disease detection\/Infected\/'+unique_infected_images[0]\ntest_image = PIL.Image.open(test_filename)\nimage_data = np.asarray(test_image)\nimage_data[0][0]","5af7025c":"sample_data_raw = image_data[0][0:1]\nsample_data = sample_data_raw","7f9b1169":"# This is a visualization of the standardization methods\nplt.figure(figsize=(25,20))\n# Nothing\nplt.figure(figsize=(15,8))\ncolor=['Reds', 'Blues', 'Greens']\nfor i, j in enumerate(sample_data_raw[0]):\n    plt.subplot(6,3,i+1)\n    plt.imshow(sample_data_raw, cmap=color[i])\n    plt.title(j, color='black', fontsize=15, ha='center')\n\n#Normalize\nsample_data = Normalizer().fit_transform(sample_data_raw)\n#sample_data = StandardScaler().fit_transform(sample_data)\n\nplt.figure(figsize=(15,8))\ncolor=['Reds', 'Blues', 'Greens']\nfor i, j in enumerate(sample_data[0]):\n    plt.subplot(6,3,i+4)\n    plt.imshow(sample_data, cmap=color[i])\n    plt.title(j, color='black', fontsize=15, ha='center')\n\n# Scale\nsample_data = StandardScaler().fit_transform(sample_data_raw.T)\n\nplt.figure(figsize=(15,8))\ncolor=['Reds', 'Blues', 'Greens']\nfor i, j in enumerate(sample_data.T[0]):\n    plt.subplot(6,3,i+7)\n    plt.imshow(sample_data.T, cmap=color[i])\n    plt.title(j, color='black', fontsize=15, ha='center')\n\n#Normalize & Scale, Order matters\n\nsample_data = Normalizer().fit_transform(sample_data_raw)\nsample_data = StandardScaler().fit_transform(sample_data.T)\ncmap=plt.cm.gray\nplt.figure(figsize=(15,8))\ncolor=['Reds', 'Blues', 'Greens']\nfor i, j in enumerate(sample_data.T[0]):\n    plt.subplot(6,3,i+10)\n    plt.imshow(sample_data.T, cmap=color[i])\n    plt.title(j, color='black', fontsize=15, ha='center')\n\n#Scale and Normalize, Order matters\n\nsample_data = StandardScaler().fit_transform(sample_data_raw.T)\nsample_data = Normalizer().fit_transform(sample_data)\n\nplt.figure(figsize=(15,8))\ncolor=['Reds', 'Blues', 'Greens']\nfor i, j in enumerate(sample_data.T[0]):\n    plt.subplot(6,3,i+13)\n    plt.imshow(sample_data.T, cmap=color[i])\n    plt.title(j, color='black', fontsize=15, ha='center')\n\n#Scale and Normalize, Order matters\n\nsample_data = StandardScaler().fit_transform(sample_data_raw.T)\nsample_data = Normalizer().fit_transform(sample_data)\n\nplt.figure(figsize=(15,8))\ncolor=['Reds', 'Blues', 'Greens']\nfor i, j in enumerate(sample_data.T[0]):\n    plt.subplot(6,3,i+16)\n    plt.imshow(sample_data.T, cmap=color[i])\n    plt.title(j, color='black', fontsize=15, ha='center')","cc029d44":"# Okay, so when we are going to do testing on this data, we seriously want smaller pictures","3308ae84":"plt.imshow(image_data)","ea3e079e":"plt.imshow((rescale(image_data[:, :, 1], 0.1, anti_aliasing=True)))","f752289f":"def corn_preprocessing(file_list, n):\n    '''\n    Takes in a list of file names\n    \n    Applys preprocessing to the image files in two sets:\n    \n    Set 0: Nothing, the orignal dataframe is passed through for comparison\n    Set 1: (Normalize, Centering)\n    Set 2: (Centering, Normalize)\n    ''';\n    # Normalizer moves the range of our data from 0 - 1\n    # Standard Scaler removes the mean and scales to unit variance\n    print(file_list[0])\n    \n    # sourced from PIL docs\n    image_data = [np.mean(np.asarray(i.resize((300,300))), axis=2) for i in (PIL.Image.open(i) for i in file_list[:n])] # This is hard coded for preformance, # changed to output grayscale\n    \n    gen_matrix = image_data\n    gen_matix_nn_ss = (StandardScaler().fit_transform(X=Normalizer().fit_transform(i)) for i in image_data)\n    gen_matrix_ss_nn = (Normalizer().fit_transform(X=StandardScaler().fit_transform(i)) for i in image_data)\n    \n    return gen_matrix, gen_matix_nn_ss, gen_matrix_ss_nn","7ddd9f94":"# Get our transformed matricies of the infected images\ninf_normal, inf_norm_ss, inf_ss_norm = corn_preprocessing('\/kaggle\/input\/corn-leaf-infection-dataset\/Corn Disease detection\/Infected\/'+df['image'],100)","f7d91530":"# Let's run a test","ce5d0b64":"# Get our list of filenames of healthy leaves\nfilenamelist_healthy = [i for i in Path('\/kaggle\/input\/corn-leaf-infection-dataset\/Corn Disease detection\/Healthy corn\/').glob('*.jpg')]","0ad178ed":"# Transform the list of healthy filenmes \nhea_normal, hea_norm_ss, hea_ss_norm = corn_preprocessing([str(i) for i in filenamelist_healthy],100)","5a9e821f":"# Okay, now we have a bunch of generators to lazy load our data. as we can't load 13GB * 3 into memory.\n# What we can do is consider a first instance where we train our classifier on some part of the data, let's say a train test split of 20\n\n# Here is a cool trick to combine generators\n# https:\/\/stackoverflow.com\/questions\/3211041\/how-to-join-two-generators-in-python\n\n# We need to add labels to our data. We know that everything for inf is infected. We can set a boolean map of [0,1] which represents the labels for everything in that set","8e58e4d9":"#y = np.append(np.zeros(len(df)),np.ones(len(filenamelist_healthy)))\n\n#This has been hardcoded for only 100 entries\ny = np.append(np.zeros(100),np.ones(100))","72396558":"y.shape","7d58fca9":"# Let's set up a simple classifier class so we can handle multiple estimators and have a consistent train test split across the data when changing estimators","c1cc84ee":"class corn_classifier():\n    '''\n    This classifier class is based off of my previous work, found here:\n    https:\/\/github.com\/vnguye34\/dsfunctions\/blob\/master\/dsfunctions\/pipelines.py\n    '''\n    \n    def __init__(self, X, y):\n        '''\n        Sets up universal train test split for inputted data\n        '''\n        X_train, X_test, y_train, y_test = train_test_split(X, y)\n        \n        # Sourced from Numpy docs for combining an array of arrays\n        self.X_train = np.concatenate([i.reshape(1,-1) for i in X_train] , axis=0)\n        self.X_test = np.concatenate([i.reshape(1,-1) for i in X_test], axis=0)\n        self.y_train = y_train\n        self.y_test = y_test\n        \n    def fit_estimator(self, estimator_object):\n        '''\n        Fits X train, y_train to the object\n        Returns object and the accuracy score for train and test datasets\n        '''\n        fitted_estimator = estimator_object.fit(self.X_train, self.y_train)\n        train_accuracy_score = estimator_object.score(self.X_train, self.y_train)\n        test_accuracy_score = estimator_object.score(self.X_test, self.y_test)\n        \n        return fitted_estimator, train_accuracy_score, test_accuracy_score","275eb1b1":"# Instantiate the class object\ncc = corn_classifier(X =  [i for i in chain(inf_normal, hea_normal)], y = y)","fef0bc33":"plt.imshow(cc.X_train[0].reshape(300,300), cmap=plt.cm.gray)","77062b08":"cc.X_train.shape","5af5a667":"cc.y_train.shape","3441d409":"logr = LogisticRegression()\nfitted_logr, train_acc, test_acc = cc.fit_estimator(estimator_object=logr)","d1293b86":"pd.Series(y).value_counts(normalize=True)","b40f626f":"train_acc, test_acc","65c2c177":"# This seems already too good to be true","19c2fee8":"plot_confusion_matrix(fitted_logr, X = cc.X_test, y_true=cc.y_test)","11631d92":"# I really don't trust how well this model did on 100 samples. See below for a more generalizable model I've attempted","6b0ccc34":"# NOTE: some additional optimization work needs to be done in order to keep progressing\n\n# Build a bacthing dataloader OR use an exising one from Tensorflow or PyTorch","e915ebd7":"# Get our transformed matricies of the infected images\ninf_normal_2, inf_norm_ss_2, inf_ss_norm_2 = corn_preprocessing('\/kaggle\/input\/corn-leaf-infection-dataset\/Corn Disease detection\/Infected\/'+df['image'], 1000)\n\n# Transform the list of healthy filenmes \nhea_normal_2, hea_norm_ss_2, hea_ss_norm_2 = corn_preprocessing([str(i) for i in filenamelist_healthy], 1000)\n\n# set up y\ny_2 = np.append(np.zeros(1000),np.ones(1000))\n\n# Instantiate the class object\ncc_2 = corn_classifier(X =  [i for i in chain(inf_normal_2, hea_normal_2)], y = y_2)\ncc_2_nn_ss = corn_classifier(X =  [i for i in chain(inf_norm_ss_2, hea_norm_ss_2)], y = y_2)\ncc_2_ss_nn = corn_classifier(X =  [i for i in chain(inf_ss_norm_2, hea_ss_norm_2)], y = y_2)\n\n\nlogr_2 = LogisticRegression(max_iter=100_000)\nfitted_logr_2, train_acc_2, test_acc_2 = cc_2.fit_estimator(estimator_object=logr_2)\nfitted_logr_2_nn, train_acc_2_nn, test_acc_2_nn = cc_2.fit_estimator(estimator_object=logr_2)\nfitted_logr_2_ss, train_acc_2_ss, test_acc_2_ss = cc_2.fit_estimator(estimator_object=logr_2)","b5c3f50f":"plot_confusion_matrix(fitted_logr_2, X = cc.X_test, y_true=cc.y_test)","e3b75431":"plot_confusion_matrix(fitted_logr_2_nn, X = cc.X_test, y_true=cc.y_test)","6e154985":"plot_confusion_matrix(fitted_logr_2_ss, X = cc.X_test, y_true=cc.y_test)","38df2685":"# General Assembly - DSIR 824 - Project 4\n# Hackathon Project \ud83d\ude80\n\n### Author: Vivian Nguyen in conjunction with General Assembly DSI-Remote\n\nPick a tabular dataset and make a predictive model with it. \n\n## Timeline\n\n- By 11:30am ET, Slack the instructional team what your 1-line problem statement and the name of your dataset. We will give you a thumbs up or ask questions\/make suggestions.\n- Commit regularly.\n- By 4:30pm ET, make your final push to your GitHub submission repo. \n- At 4:30pm ET, we'll do 5-minute lightning talks where you'll tell the class what you found. No slides necessary - you can walk us through your Jupyter notebook if you prefer. \n\n## Guidelines\n\nWe know this is a short time for a project. We're not expecting anything as polished as project two or three, for example. However, the goal is for you to have something to show for your time. At a minimum, do some EDA and have at least two models fit and scored.\n\nIf you have a big data file, keep it out of GitHub by using your .gitignore file.\n\n## Datasets\n\nYou can pick any dataset you're interested in. \n\nYour dataset and problem do not need to be novel, but please do not pick a dataset that you have used before or that is extremely common in data science examples (such as Titanic, Iris, MNIST, etc.).\n\nThere are many curated collections of interesting datasets online. Here are a few links to get you started:\n\n- [Kaggle](https:\/\/www.kaggle.com\/datasets)\n- [\/r\/datasets](https:\/\/www.reddit.com\/r\/datasets\/)\n- [Data is Plural](https:\/\/docs.google.com\/spreadsheets\/d\/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk\/edit#gid=0)\n\n\nReminder: do not duplicate someone else's work and give credit where credit is due.\n\n## Have fun! \ud83c\udf89"}}