{"cell_type":{"24eac451":"code","a604169b":"code","5f505c6c":"code","c77c0145":"code","0f51b64c":"code","15a84cb5":"code","65969beb":"code","12eaffdb":"code","39011f57":"code","31ee42d6":"code","78df8f21":"code","81ac6b92":"code","8015a848":"code","762e5531":"code","e16202e7":"markdown","7c016fc4":"markdown","9e03fa82":"markdown","bd1282ac":"markdown","5b865feb":"markdown","9dabd5d2":"markdown","0c74da17":"markdown","d41cd395":"markdown"},"source":{"24eac451":"%pip install stylecloud -q","a604169b":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \nfrom keras.utils.vis_utils import plot_model\n\n\n# set seeds for reproducability\nfrom numpy.random import seed\nseed(1)\n\nimport pandas as pd\nimport numpy as np\nimport string, os \n\nimport warnings\nimport stylecloud\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n","5f505c6c":"curr_dir = '..\/input\/..\/input\/covid19-publicationsdatasets-clinical-trials\/'\nct_df = pd.read_csv(curr_dir + 'Clinical_Trails.csv')\nct_df.head()","c77c0145":"all_trails = []\nall_trails.extend(list(ct_df.Abstract.values))\nall_trails = [h for h in all_trails if h != \"Unknown\"]\nlen(all_trails)","0f51b64c":"all_trails","15a84cb5":"def clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n\ncorpus = [clean_text(x) for x in all_trails]\ncorpus[:10]","65969beb":"unique_hashtags=(\" \").join(corpus)\nstylecloud.gen_stylecloud(text = unique_hashtags,\n                          icon_name=\"fas fa-head-side-mask\",\n                          background_color='white',\n                          gradient='horizontal')\n","12eaffdb":"from IPython.display import Image\nImage(filename='stylecloud.png',width = 500, height = 600) ","39011f57":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n    \n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(corpus)\ninp_sequences[:5]","31ee42d6":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","78df8f21":"def create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n    \n    model.add(Embedding(total_words, 10, input_length=input_len))\n\n    \n    model.add(LSTM(100))\n\n    model.add(Dropout(0.1))\n    \n    model.add(Dense(total_words, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    \n    return model\n\nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","81ac6b92":"model.fit(predictors, label, epochs=100, verbose=1)","8015a848":"def generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title()","762e5531":"print (generate_text(\"united states\", 20, model, max_sequence_len))\nprint('*' * 100)\nprint (generate_text(\"clinical trails\", 20, model, max_sequence_len))\nprint('*' * 100)\n\nprint (generate_text(\"donald trump\", 20, model, max_sequence_len))\nprint('*' * 100)\n\nprint (generate_text(\"india and china\", 20, model, max_sequence_len))\nprint('*' * 100)\n\nprint (generate_text(\"virus\", 20, model, max_sequence_len))\nprint('*' * 100)\n\nprint (generate_text(\"covid\", 20, model, max_sequence_len))\nprint('*' * 100)\n\nprint (generate_text(\"treatment\", 20, model, max_sequence_len))\nprint('*' * 100)\n\nprint (generate_text(\"coronavirus\", 20, model, max_sequence_len))\nprint('*' * 100)","e16202e7":"## Removing Punctuations and lower casing ","7c016fc4":"## Pad Sequence\nThe pad_sequences() function in the Keras deep learning library can be used to pad variable length sequences.\n\nThe default padding value is 0.0, which is suitable for most applications, although this can be changed by specifying the preferred value via the \u201cvalue\u201d argument","9e03fa82":"# Abstract Generation of Covid-19 Clinical Trails\n\n<img src = \"https:\/\/media.giphy.com\/media\/Qu1fT51CG14ksIkASL\/giphy.gif\" >\n\n## Dataset Description\nThis file contains all relevant publications, datasets, and clinical trials from Dimensions that are related to COVID-19. The content has been exported from Dimensions using a query in the openly accessible Dimensions application, which you can access at https:\/\/covid-19.dimensions.ai\/.With its range of research outputs including datasets and clinical trials, both of which are just as important as journal articles in the face of a potential pandemic.\n\n","bd1282ac":"## Generating Sequence of N-gram Tokens\nLanguage modelling requires a sequence input data, as given a sequence (of words\/tokens) the aim is the predict next word\/token.This is acheived by **Tokenization** .<br>\n\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types \u2013 word, character, and subword (n-gram characters) tokenization.\n\n","5b865feb":"## END\n#### Cheers !!!!","9dabd5d2":"### Generating the CT's Abstract","0c74da17":"## Generating World Cloud","d41cd395":"## Building LSTM Architecture for Abstract Generation\n<img src  = \"http:\/\/www.shivambansal.com\/blog\/text-lstm\/2.png\">"}}