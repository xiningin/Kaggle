{"cell_type":{"028c3202":"code","6c205b10":"code","9152e85f":"code","086b4698":"code","08c67235":"code","c0611079":"code","40d18e67":"code","33aeac9f":"code","acccf5a2":"code","ba3fd33b":"code","f3138046":"code","665a6aaf":"code","d4697f8d":"code","c71d87ed":"code","9cb550ef":"code","4e487cc2":"code","a1d9b777":"code","567a264d":"code","7b2902e4":"code","835ccf1e":"code","acdb37af":"code","f2c80925":"code","932596ca":"code","e3bcf100":"code","a9dc093a":"code","91844517":"code","aeea658e":"code","055a7ba1":"code","3a278e5d":"code","14cde2c0":"code","526a523f":"code","45be3a1d":"code","e12a87db":"code","b5e14189":"code","b4d1b6b1":"code","3138bd7e":"code","84c17ca3":"code","a17edd6b":"code","c1f560c0":"code","3b5dd727":"code","566025cc":"code","0b6226ff":"code","537948f6":"code","3a0964fa":"code","dc52d249":"code","8df73e43":"code","f0e8280e":"code","9bba0873":"code","428b6e98":"code","18f303b2":"code","b94edc8c":"code","91ea0483":"code","06e41361":"code","f1a64929":"code","073428fc":"code","c8e39c21":"code","dec4ef4c":"code","d16660b8":"code","0d5a0260":"code","77736012":"code","acdf819c":"code","9c9d51b5":"markdown","7572fcea":"markdown","98c715ad":"markdown","dd12e459":"markdown","ebce72d0":"markdown","23732eec":"markdown","43a51da6":"markdown","8e2769aa":"markdown","8451cc68":"markdown","f1c0fa9c":"markdown","762fc6ff":"markdown","39327247":"markdown","ecf24c47":"markdown","2c5f4d98":"markdown","30de4b1d":"markdown","9caf5ab5":"markdown","8b261cf5":"markdown","ee6ce825":"markdown","8d207489":"markdown","8b33bc7c":"markdown","8f5f7d15":"markdown","0e3c26da":"markdown","578ab36d":"markdown","7204a3e9":"markdown","c823b83d":"markdown","4ce2db48":"markdown","0463396a":"markdown","a81ebfb2":"markdown","eb807f58":"markdown","26274220":"markdown","b86dcf88":"markdown","50f107cf":"markdown","047f5c38":"markdown","0bdc8f6c":"markdown","d959b79f":"markdown","21e9045a":"markdown","8c468ac3":"markdown"},"source":{"028c3202":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Model libraries\nimport xgboost as xgb\n\n\n#Other Libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler,MinMaxScaler\n#from feature_engine import categorical_encoders as ce\nfrom sklearn.metrics import f1_score\nimport datetime as dt","6c205b10":"train = pd.read_csv('\/kaggle\/input\/pet-adoption-dataset\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/pet-adoption-dataset\/test.csv')\nprint(train.shape)\ntrain.head()","9152e85f":"train.info()","086b4698":"train.describe()","08c67235":"f, axes = plt.subplots(ncols=2, figsize=(12,4))\nsns.countplot(train['pet_category'],ax=axes[0])\nsns.countplot(train['condition'],ax=axes[1])\nplt.show()","c0611079":"print(train.isnull().sum())\nprint(test.isnull().sum())","40d18e67":"train['condition'].fillna(99,inplace = True)\nprint(train['condition'].value_counts())","33aeac9f":"train.groupby(['condition','pet_category']).size()","acccf5a2":"train.groupby(['condition','breed_category']).size()","ba3fd33b":"#Splitting the column to give provide more features \ntrain['nf1_pet_id'] = train['pet_id'].str[:6]\ntrain['nf2_pet_id'] = train['pet_id'].str[:7]","f3138046":"print(train.nf1_pet_id.nunique())\nprint(train.nf1_pet_id.value_counts())","665a6aaf":"print(train.nf2_pet_id.nunique())\nprint(train.nf2_pet_id.value_counts())","d4697f8d":"train.groupby(['nf1_pet_id','pet_category']).size()","c71d87ed":"train.groupby(['nf2_pet_id','pet_category']).size()","9cb550ef":"# New feature(Waiting time in Months) to find the difference between the issue date and the listing date \n\ntrain['issue_date'] = pd.to_datetime(train['issue_date']).apply(lambda x: x.date()) # removing the time values from the string columns\ntrain['listing_date'] = pd.to_datetime(train['listing_date']).apply(lambda x: x.date()) # removing the time values from the string columns\ntrain['Waiting_time(M)'] = (train['listing_date']- train['issue_date'])\/np.timedelta64(1,'M') # difference of listing_date and issue_date in months\ntrain['Waiting_time(M)'] = np.round(train['Waiting_time(M)'],2) # rounding of the value to 2 decimals\n","4e487cc2":"# Also creating new features from the issue date and listing date by converting their format into months\ntrain['issue_date'] = pd.to_datetime(train['issue_date'])\ntrain['issue_dt_month'] = train['issue_date'].dt.month \ntrain['listing_date'] = pd.to_datetime(train['listing_date'])\ntrain['listing_dt_month'] = train['listing_date'].dt.month","a1d9b777":"train[train['Waiting_time(M)'] < 0]","567a264d":"train = train.drop(train[train['Waiting_time(M)'] < 0].index)\ntrain.shape","7b2902e4":"train[train['length(m)'] == 0]['length(m)'].value_counts()","835ccf1e":"f, axes = plt.subplots(ncols=2, figsize=(12,4))\nsns.distplot(train['length(m)'],ax=axes[0])\nsns.boxplot(train['length(m)'],ax=axes[1])","acdb37af":"# Converting length from 'm' to 'cm'\n\ntrain['length(cm)'] = train['length(m)']*100\n\n# Replacing values less than 10 cm with the mean value\n\nfor value in train['length(cm)']:\n    if (value < 10):\n        train['length(cm)'] = train['length(cm)'].replace(value, np.round(train['length(cm)'].mean()))","f2c80925":"#Creating a new feature from lenght and height as ratio\ntrain['ratio_len_height'] = train['length(cm)']\/train['height(cm)']","932596ca":"train['color_type'].unique()","e3bcf100":"train['color_type1'] = train['color_type']\ntrain['color_type2'] = train['color_type']","a9dc093a":"list = ['Brown','Blue','Black','Cream','Red','Calico','Yellow','Gray','Silver','Chocolate','Orange','Liver']\nfor val in train['color_type']:\n    if (val.split()[0] in list) & (len(val.split())>1):     \n        train['color_type1'] = train['color_type1'].replace(val,val.split()[0])\n        train['color_type2'] = train['color_type2'].replace(val,val.split()[1])","91844517":"test['condition'].fillna(99,inplace = True)","aeea658e":"test['nf1_pet_id'] = test['pet_id'].str[:6]\ntest['nf2_pet_id'] = test['pet_id'].str[:7]","055a7ba1":"test['issue_date'] = pd.to_datetime(test['issue_date']).apply(lambda x: x.date())\ntest['listing_date'] = pd.to_datetime(test['listing_date']).apply(lambda x: x.date())\ntest['Waiting_time(M)'] = (test['listing_date']- test['issue_date'])\/np.timedelta64(1,'M')\ntest['Waiting_time(M)'] = np.round(test['Waiting_time(M)'],2)","3a278e5d":"# Also creating new features from the issue date and listing date by converting their format into months\ntest['issue_date'] = pd.to_datetime(test['issue_date'])\ntest['issue_dt_month'] = test['issue_date'].dt.month\ntest['listing_date'] = pd.to_datetime(test['listing_date'])\ntest['listing_dt_month'] = test['listing_date'].dt.month","14cde2c0":"test = test.drop(test[test['Waiting_time(M)'] < 0].index)","526a523f":"test['length(cm)'] = test['length(m)']*100\n\n# Replacing values less than 10 cm with the mean value\n\nfor value in test['length(cm)']:\n    if (value < 10):\n        test['length(cm)'] = test['length(cm)'].replace(value, np.round(test['length(cm)'].mean()))","45be3a1d":"test['ratio_len_height'] = test['length(cm)']\/test['height(cm)']","e12a87db":"test['color_type1'] = test['color_type']\ntest['color_type2'] = test['color_type']","b5e14189":"list = ['Brown','Blue','Black','Cream','Red','Calico','Yellow','Gray','Silver','Chocolate','Orange','Liver']\nfor val in test['color_type']:\n    if (val.split()[0] in list) & (len(val.split())>1):     \n        test['color_type1'] = test['color_type1'].replace(val,val.split()[0])\n        test['color_type2'] = test['color_type2'].replace(val,val.split()[1])\n        ","b4d1b6b1":"plt.subplots(figsize=(16,8))\nsns.heatmap(train.corr(), annot= True)","3138bd7e":"X1 = train.drop(['breed_category','pet_category'], axis=1)\ny1 = train['pet_category']\nX2 = train.drop(['breed_category'], axis=1)\ny2 = train['breed_category']","84c17ca3":"print(X1.shape)\nprint(y1.shape)\nX1.head()","a17edd6b":"from sklearn.model_selection import train_test_split,cross_val_score\nX_train1, X_vald1, y_train1,y_vald1 = train_test_split(X1,y1,test_size = 0.25, random_state = 42,stratify = y1)\nprint(\"Train size :\" ,X_train1.shape,y_train1.shape)\nprint(\"Validation size :\"  ,X_vald1.shape, y_vald1.shape)","c1f560c0":"X_train1.head()","3b5dd727":"numerical_features1 = ['Waiting_time(M)','ratio_len_height']\nnumerical_features2 = ['X1','X2']\ncategorical_features = ['condition','nf1_pet_id','issue_dt_month','listing_dt_month','color_type1','color_type2']","566025cc":"numeric_transformer1 = Pipeline(steps=[\n    ('scaler', RobustScaler())\n])\n\nnumeric_transformer2 = Pipeline(steps=[\n    ('normalizer',MinMaxScaler())\n])\n\ncategory_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))  \n])\n","0b6226ff":"from sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('drop_columns', 'drop', ['pet_id','issue_date','listing_date','length(m)','color_type','nf2_pet_id','length(cm)','height(cm)',]),\n        ('numeric1', numeric_transformer1,numerical_features1),\n        ('numeric2', numeric_transformer2,numerical_features2),\n        ('category', category_transformer, categorical_features)\n])","537948f6":"preprocessor.fit(X_train1)","3a0964fa":"X_train1 = preprocessor.transform(X_train1)\nX_vald1 = preprocessor.transform(X_vald1)","dc52d249":"import xgboost as xgb\nfrom sklearn.metrics import f1_score\nxgb_model = xgb.XGBClassifier(scale_pos_weight = 0.7,\nmin_child_weight=  3,\nlearning_rate = 0.3,\ngamma = 0.4,\ncolsample_bytree = 0.5, random_state = 42)\nxgb_model.fit(X_train1, y_train1)\ny_pred_xgb = xgb_model.predict(X_vald1)\n\ns1 = f1_score(y_vald1,y_pred_xgb,average = 'weighted')\nprint(s1)","8df73e43":"test1 = test\ntest1 = preprocessor.transform(test1)","f0e8280e":"X1 = preprocessor.transform(X1)","9bba0873":"xgb_model = xgb.XGBClassifier(scale_pos_weight = 0.7,\nmin_child_weight=  3,\nlearning_rate = 0.3,\ngamma = 0.4,\ncolsample_bytree = 0.5)\nxgb_model.fit(X1, y1)\nprediction1 = xgb_model.predict(test1)","428b6e98":"predicted_values1 = pd.DataFrame(prediction1, columns = ['pet_category'])\nprint(predicted_values1)","18f303b2":"from sklearn.model_selection import train_test_split\nX_train2, X_vald2, y_train2,y_vald2 = train_test_split(X2,y2,test_size = 0.25, random_state = 42, stratify = y2)\nprint(\"Train size :\" ,X_train2.shape,y_train2.shape)\nprint(\"Validation size :\"  ,X_vald2.shape, y_vald2.shape)","b94edc8c":"numerical_features3 = ['Waiting_time(M)','length(cm)','height(cm)']\nnumerical_features4= ['X1','X2']\ncategorical_features1 = ['condition','color_type1','color_type2','pet_category','listing_dt_month']\n","91ea0483":"numeric_transformer3 = Pipeline(steps=[\n    ('scaler1', RobustScaler())\n])\n\nnumeric_transformer4 = Pipeline(steps=[\n    ('normalizer1',MinMaxScaler())\n])\n\ncategory_transformer1 = Pipeline(steps=[\n    ('onehot1', OneHotEncoder(handle_unknown='ignore'))  \n])\n\n\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor1 = ColumnTransformer(\n    transformers = [\n        ('drop_columns', 'drop', ['pet_id','issue_date','listing_date','length(m)','color_type','nf1_pet_id','nf2_pet_id','issue_dt_month']),#,'issue_dt_month','nf2_pet_id','nf1_pet_id'\n        ('numeric3', numeric_transformer3,numerical_features3),\n        ('numeric4', numeric_transformer4,numerical_features4),\n        ('category', category_transformer1, categorical_features1)\n\n])","06e41361":"preprocessor1.fit(X_train2)","f1a64929":"X_train2 = preprocessor1.transform(X_train2)\nX_vald2 = preprocessor1.transform(X_vald2)","073428fc":"import xgboost as xgb\nxgb_model2 = xgb.XGBClassifier(scale_pos_weight = 8,\n min_child_weight = 1,\n learning_rate = 0.01,\n gamma= 0.3,\n colsample_bytree = 0.3, \n random_state = 42)\nxgb_model2.fit(X_train2, y_train2)\ny_pred_xgb2= xgb_model2.predict(X_vald2)\n\ns2 = f1_score(y_vald2,y_pred_xgb2,average = 'weighted')\nprint(s2)","c8e39c21":"score = (s1+s2) * 100\/2\nprint('final_score on the using validation data',score)","dec4ef4c":"X2_1 = X2.pop('pet_category')\nX2['pet_category'] = X2_1 # to have the pet_category at the last same as test data \npreprocessor1.fit(X2)\nX2 = preprocessor1.transform(X2)","d16660b8":"test2 = test\ntest2['pet_category'] = predicted_values1['pet_category']","0d5a0260":"test2 = preprocessor1.transform(test2)","77736012":"xgb_model2 = xgb.XGBClassifier(scale_pos_weight = 8,\n min_child_weight = 1,\n learning_rate = 0.01,\n gamma= 0.3,\n colsample_bytree = 0.3)\nxgb_model2.fit(X2, y2)\nprediction2= xgb_model2.predict(test2)","acdf819c":"predicted_values2 = pd.DataFrame(prediction2, columns = ['breed_category'])\nprint(predicted_values2)","9c9d51b5":"### Before we Begin:\nThis notebook is my first attempt to showcase the work done for a hackathon by Hackerearth(Pet Adoption) and on the topic of Multi-ouput multi-classification problem, if you like my work please upvote and also provide your valuable comments on this kernal which will help me in improving further","7572fcea":"Splitting the colour_type variable into two features which gives more information to predict the output","98c715ad":"### Fitting the transform operations on train and transforming on train and validation","dd12e459":"### Multioutput-multiclass classification\n\n- Multioutput-multiclass classification also known as multitask classification \n- It is a classification task where there are multiple target variables and each target variable have either binary or multiclass to predict.\n- All classifiers handling multioutput-multiclass tasks, support the multilabel classification task as a special case.\n\n![](https:\/\/raw.githubusercontent.com\/VijayMukkala\/Mini-Projects\/master\/Pet%20Adoption\/1.png)\n\nThe classifiers that support multiclass-multioutput in sklearn as follows:\n\n![](https:\/\/raw.githubusercontent.com\/VijayMukkala\/Mini-Projects\/master\/Pet%20Adoption\/2.png)\n\nBut at present, no metrics in sklearn.metrics supports the multioutput-multiclass classification task.","ebce72d0":"### Xgboost(Model2)","23732eec":"## Feature Engineering\n### 1. New feature creation from alphanumeric col pet_id","43a51da6":"### Xgboost(Model1)","8e2769aa":"### Feature enginneering and missing value imputation on the *test data* same as train","8451cc68":"### Conclusion\n\nAchieved __91.029 score__ on the __unseen test__ data using the approach of training two models.\n- Building one classification model and predicting the output.\n- Using the predicted output of 1st model as input feature to 2nd model.\n\nThis is my first draft and I will be updating with more additions :):):)","f1c0fa9c":"Looks like these features will add much more value in predicting the target variables","762fc6ff":"### Applying the transformer on the test data and predicting the target variable 'pet_category'","39327247":"### 4. Analyzing the \"Length(m)\" feature","ecf24c47":"From the above graphs we observe that few values are very less and actually zero.\n\nAfter research in the google, the smallest pet is around 10 cm length it might be more than that but as we are not sure of the pet category except dog and cat , for this problem we will conider the length data for the pet more than 10cm is __anomaly__","2c5f4d98":"### Summary:\n\n- Missing value imputation done by replacing the nan with 99\n- created 2 new features from apla numeric column pet_id\n- Created new features from the date columns 'issue_date' & 'listing_date'\n- Checked the anomaly with the date columns and dropped the rows\n- Converted the length column from 'm' to 'cm' and replaced values less than 10cm with the mean value\n- Created new feature from length and height columns\n- Created 2 color_type columns by splitting the values \n\n\n### To do in modelling\n- Cross validation\n- Standardizing the categorical variables\n- Use onehot encoding on the categorical variables\n- Droping the columns\n- Fit the data to a hypertuned model with target value as 'pet_category'\n- Create another hypertuned model with the 'pet_category'values from previous model  and predict 'breed_category' feature","30de4b1d":"The features for the model is selected using feature selection of univariate features by Annova method(__sklearn.feature_selection.f_classif__)","9caf5ab5":"- Replace missing value with 99 and check for the pattern","8b261cf5":"### Creating X & y variables","ee6ce825":"### Building custom Transformer \n\n- scaling on numerical_features1\n- Normalizing on numerical_features2\n- onehotencoding on the categorical features\n- dropping the columns not important for model","8d207489":"### Applying the transformer on the test data and predicting the target variable 'breed_category'","8b33bc7c":"The features for the model is selected using feature selection of univariate features by Annova method(__sklearn.feature_selection.f_classif__)","8f5f7d15":"### Model 2\nCreating another hypertuned model with the 'pet_category' values from previous model and predict'breed_category' feature","0e3c26da":"### 3. Anomaly detection in the date features","578ab36d":"From the distribution of pet_category , we can see that the feature is imbalanced. We can apply undersampling or oversampling techniques or using bagging hyperparameters","7204a3e9":"Looks like all the missing values belong to Breed_category __*2*__ and we hit a jackpot","c823b83d":"### 2.Creating multiple features using date features: issue_date & listing date","4ce2db48":"#### Understanding the distribution of the target labels","0463396a":"### Fitting the transform operations on train and transforming on train and test","a81ebfb2":"### Hyper tuning the parameters for the best result:\n\nBest parameters in this case :\n(scale_pos_weight = 0.7,\nmin_child_weight=  3,\nlearning_rate = 0.3,\ngamma = 0.4,\ncolsample_bytree = 0.5)","eb807f58":"### Column Description in detail\n- 'issue_date' : Date on which the pet was issues to the shelter\n- 'listing_date' : Date when the pet arrived at the shelter\n- 'condition' : Condition of the pet\n- 'color_type' : color of the pet\n- 'length(m)' : Length of the pet(in meter)\n- 'height(cm)' : Height of the pet (in cm)\n- 'breed_category' : Breed Category of the pet\n- 'pet_category' : Category of the pet","26274220":"## Problem Statement:\n\nBuild a Machine Learning model that determines type and breed of the animal based on its physical atrributes and other factors\n\n![](https:\/\/raw.githubusercontent.com\/VijayMukkala\/Mini-Projects\/master\/Pet%20Adoption\/pet-adoption-5441817_640.png)\n\nIn this Kernal ,we will deal with multioutput-multiclass classification task of predicting two labels: 'breed_catagory' and 'pet_catagory'.The evaluation metric is being used is(the average of both f1_scores * 100)\n\nThe best solution I got by implemeting is to train two models instead of using a single model which predicts the multi task calssification.\n- Build one classification model and predict the output.\n- Use the predicted output of 1st model as input feature to 2nd model.","b86dcf88":"## Missing Values imputation\n- Checking for missing values in the data ","50f107cf":"### Model 1 ","047f5c38":"### 5. Creating new features from the \"color_type\" feature\nColor_type feature has few key words like Tabby,brindle,tiger,etc which are categorized to one specific pet.","0bdc8f6c":"### Building custom Transformer for Model2\n\n- scaling on numerical_featuures1\n- Normalizing on numerical_features2\n- onehotencoding on the categorical features\n- dropping the columns not important for model","d959b79f":"As we know that the waiting time cannot be negative , we are gonna drop these rows","21e9045a":"__condition__ column has missing values both in train and test.","8c468ac3":"### Import Packages"}}