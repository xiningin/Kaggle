{"cell_type":{"a0bade2a":"code","8d4d09b3":"code","d19f299b":"code","00914219":"code","1eb8f8c3":"code","ca766ab3":"code","349a9274":"code","aa239cfd":"code","db79cd3e":"code","95944d61":"code","3161276d":"code","51a1fe40":"code","8c6c7dbe":"markdown","34b53c6e":"markdown","63b109c0":"markdown","31078b69":"markdown","80e98fbf":"markdown","a3ecd9a6":"markdown","1aa274e9":"markdown","a8a8408a":"markdown","c8d917f0":"markdown","b3af5334":"markdown","6f06d784":"markdown","4240b0c5":"markdown","471e48da":"markdown","4cc540a3":"markdown","d6ec76a5":"markdown","4a6eb8d7":"markdown","93da5d92":"markdown","5a3e13ad":"markdown","16b245b0":"markdown","ab210277":"markdown","5ad080ec":"markdown","a728d319":"markdown","65c9853c":"markdown","934b4225":"markdown","709fa5ee":"markdown","3ca0e991":"markdown","17e5ce2e":"markdown","b29d4a0d":"markdown","a27bec0e":"markdown"},"source":{"a0bade2a":"# Importing all the required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","8d4d09b3":"# Importing the dataset from your file directory where you have downloaded your csv file\n\ndataset = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndataset.head()","d19f299b":"dataset.shape","00914219":"dataset.isnull().sum()","1eb8f8c3":"dataset.describe()","ca766ab3":"X = dataset.iloc[:, :-1]\nY = dataset.iloc[:, -1]","349a9274":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.3, random_state=123)\nprint (\"Training set size:\", X_train.shape)\nprint (\"Test set size:\", X_test.shape)","aa239cfd":"regressor = LinearRegression()\nregressor.fit(X_train, Y_train)","db79cd3e":"Y_pred = regressor.predict(X_test)","95944d61":"compare = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\ncompare.head(10)","3161276d":"df = compare.head(25)\ndf.plot(kind='bar',figsize=(10,8))","51a1fe40":"print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))  \nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))","8c6c7dbe":"From the above statistics, we can clearly get an overall understanding about each independent variables in the dataset like the count, mean, standard deviation, minimum and maximum values, etc.\n\nIn this dataset, we are going to predict the quality of wine hence the dependent variable is 'quality'. The rest all are independent\/predictor variables, which are the characteristics of the wine, using which we are going to find the quality.\n\nSo, now lets separate out the independent and dependent variables from the dataset.","34b53c6e":"# 4. Hands-on Example:\nIn this section we will see how to actually apply the linear regression using python. We will be doing the multiple linear regression problem here because almost all the real-world problems that you will face will have more than two variables. But do know that the steps we follow for doing multiple linear regression is same as that of simple linear regression.\n\nThe dataset we are going to use here is the red wine quality dataset. This dataset is related to red variants of the Portuguese \u201cVinho Verde\u201d wine. \n\nThis hands-on example is only to show how to apply linear regression model using scikit-learn, so I didn\u2019t go deeper into the Exploratory Data Analysis (EDA) part.\n\nLet\u2019s start our coding:\n","63b109c0":"### *Ordinary Least Squares:*\n\n![image.png](attachment:image.png)\n\n\n \n \n","31078b69":"Linear Regression is one of the most simple machine learning algorithm used extensively in the field of predictive analytics. This method is mostly used for forecasting and finding out cause and effect relationship between variables. It is a linear model i.e., it assumes a linear relationship between the dependent variable (Y) and one or more independent variables (X). In this technique, the dependent variable is continuous, independent variable(s) can be continuous or discrete, and nature of regression line is linear.\n\nThis algorithm falls under supervised learning i.e., the values of the dependent variable in the training dataset is known.\n\nLinear Regression models have many real world applications across all sectors like predicting the sales in Retail, housing price prediction in Real Estate, weather prediction in meteorology, stock price prediction in Finance, predicting disease onset from biological factors in Healthcare, etc. So understanding the intuition behind the model and how to implement it will help us to solve problems in predictive analytics.\n\n\n## **Table of contents:**\n1.\tLinear Regression Assumptions\n2.\tMathematics behind the model  \n   2.1\timple Linear Regression   \n   2.2\tMultiple Linear Regression  \n    \n3.\tEvaluation metrics\n\n4.\tHands-on Example\n","80e98fbf":"The vectorized form looks like,\n![image.png](attachment:image.png)\nThe gradient vector,\u2207_\u03b8 MSE(\u03b8), contains all the partial derivatives of the cost function (one for each model parameter). ","a3ecd9a6":"Like i have already mentioned, training a linear regression model is just a one line of code. \nNow the model is trained and ready to predict the values for the test set.","1aa274e9":"As you can see from the output, there is no missing value in the dataset. If there is any missing value present in the dataset we need to first handle it and then proceed to the next step. Because, the algorithm cannot work with missing values hence either it needs to be removed or imputed with other values.\n\nNow, we will check the descriptive statistics for the dataset to get an understanding about the data:","a8a8408a":"## **2.2 Multiple Linear Regression:**\nWhen there are multiple input variables\/independent variables (X1, X2, X3\u2026.) then it is called multiple linear regression.\nThe multiple linear regression equation looks like this: \n![image.png](attachment:image.png)\n\nIn simple linear regression, we used OLS method for finding the best-fit line. Whereas in this case, we have more than one predictor variable which makes it hard for us to use that simple OLS method.\nBut we can implement a linear regression model for performing Ordinary Least Squares regression using one of the following approaches:\n* Solving the model parameters analytically (Normal Equations method)  \n* Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, etc.)  \n","c8d917f0":"Let's compare the actual and the predicted values for the test set.","b3af5334":"This dataset contains 1599 rows and 12 columns. Each row in this dataset represents a wine and each column represents the feature of that particular wine.\n\nNow, lets check for any missing values in the dataset.","6f06d784":"Thus we have successfully applied Linear Regression model using scikit-learn on the red-wine quality dataset. This example will just give you a gist of how to perform regression analysis. ","4240b0c5":"# **2. Mathematics behind the model:**\nEven though we have python libraries which can do the regression analysis in a single line of code, it is really important to know the mathematics behind the model. Because only when you know how a model works from the scratch, you will be able to tweak different model parameters with respect to your problem statement and the dataset at your hand to get the desired result.\n\nThere are two kinds of variables in a linear regression model:\n* The input or independent or predictor variable(s) is the input for the model and it helps in predicting the output variable. It is represented as X.\n* The output or dependent variable(s) is the output of the model i.e., the variable that we want to predict. It is represented as Y.\n","471e48da":"\n I hope this article will help you understand the linear regression model in detail. There is still some topics such as Polynomial Regression, Regularization technique, optimization algorithms like Stochastic Gradient Descent (SGD), Batch Gradient Descent, Mini-Batch Gradient Descent etc., to be covered with respect to Linear Regression but that\u2019s for the next article. Till then, Happy Learning !!!!\n","4cc540a3":"The update rule to get the updated weights\/parameters is given below,\n![image.png](attachment:image.png)\nWhere \u03b1 is the learning rate hyperparameter.\n\nThis sums up the methods which are there to find the parameters of the Linear Regression model. In the practical application point of view, we don\u2019t need to write the algorithm from scratch every time we need to apply the linear regression model. Python provides a machine learning library called scikit-learn which contains the linear regression algorithm which we can use it by a single line of code as you will see in the next section","d6ec76a5":"As you can see from the output, the training set has 1119 observations and test set has 480 observations.\n\nNow, lets train our model:","4a6eb8d7":"Ordinary Least Squares (OLS) regression is a statistical method of analysis that estimates the relationship between one or more independent variables and a dependent variable. The objective of the least squares method is to find values of \u03b20 and \u03b21 that minimise the sum of the squared difference between Y and Y\u2091. \n\n![image.png](attachment:image.png)","93da5d92":"### *Gradient Descent:*\nGradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.\n\nError\/Cost here represents the sum of squared error between the predicted and the actual value. This error is defined in terms of a function and is called Mean Squared Error (MSE) cost function. So, the objective of this Gradient Descent optimization algorithm is to minimize the MSE cost function.\n\nSuppose let\u2019s assume that we are standing on top of a hill in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest slope. This is exactly what Gradient Descent does: it measures the local gradient of the error function with regards to the parameter vector \u03b8 and it goes in the direction of the descending gradient. Once the gradient is zero, you have reached a minimum.\n\n![image.png](attachment:image.png)\n\nAn important parameter in the Gradient Descent is the size of the steps, determined by the learning rate hyperparameter. If the learning rate is too small, the algorithm will have to go through many iterations to converge, which will take a long time.\n","5a3e13ad":"Now we have our X and Y separately. Again we will split these datasets into training and test set because once our linear regression model is trained we need some data to check how our model is performing.\n\nWe will split the complete dataset into 70% training data and 30% test data.","16b245b0":"The final step is to evaluate the performance of the model. We'll use evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE) and Root Mean Squared Error (RMSE).","ab210277":"### *Normal Equation (closed-form solution)*\nThis approach treats the data as a matrix and uses linear algebra operations to estimate the optimal values for the model parameters. It means that all of the data must be available and you must have enough memory to fit the data and perform matrix operations. So this method should be preferred for smaller datasets.\n\n![image.png](attachment:image.png)\nFor very large datasets, computing the matrix inverse of  X^T.X is costly or in some cases the inverse does not exist (the matrix is non-invertible or singular, e.g., in case of perfect multicollinearity). In such cases, the below explained Gradient Descent approach is preferred.","5ad080ec":"## **2.1 Simple linear regression:**\nWhen there is one input variable\/independent variable (X) then it is called simple linear regression.\n\nThe simple linear regression equation looks like this:\n![image.png](attachment:image.png)\nThe main idea behind this model is to fit a straight line in the data. In order to get the best fit line, we have to find the optimum values for the coefficients\/parameters \u03b20 and \u03b21 in such a way that it minimizes the error between the predicted and the actual value.\n\nSo, how do we find the optimum values for \u03b20 and \u03b21? The simple linear regression can be solved using Ordinary Least Squares (OLS), which is a statistical method, to find the model parameters. \n","a728d319":"![image.png](attachment:image.png)\nOn the other hand, if the learning rate is too high, the algorithm will jump over the minimum, making it diverge. Hence the algorithm won\u2019t reach the minimum.","65c9853c":"The MSE cost function is defined as,\n![image.png](attachment:image.png)\nWhere, m = number of samples in dataset.\n  \n","934b4225":"Let's check the number of rows and columns in the dataset.","709fa5ee":"# **1. Linear Regression Assumptions:**\nRegression is a parametric approach. \u2018Parametric\u2019 means it makes assumptions about the data for the purpose of analysis. If a dataset fails to fulfil its assumptions, then the model will perform poorly on the dataset. For this reason, it is essential to validate these assumptions for successful regression analysis.\n\nThe important assumptions in regression analysis are:\n1.\t**Linearity:** It is assumed that there is a linear relationship between the dependent (Y) and independent (X) variable(s).\n\n2.\t**Autocorrelation:** There should be no correlation between the residual (error) terms. If some correlation is present then it means that the model is unable to identify some relationship in the data.\n\n3.\t**Multicollinearity:**  There should be no correlation between the independent variables. If the independent variables are moderately or highly correlated then it becomes difficult to find out which variable is actually contributing to predict the dependent (response) variable.\n\n4.\t**Homoskedasticity:**  The error terms must have constant variance. The presence of non-constant variance in error terms results in heteroskedasticity. This non-constant variance arises in the presence of outliers. When this phenomenon occurs, the confidence interval for the out of sample prediction tends to be unrealistically wide or narrow.\n\n5.\t**Normality:** The error terms must be normally distributed. Presence on non-normal distribution suggests that there are few unusual data points which must be studied closely to make a better model.\n","3ca0e991":"The partial derivative of the cost function is,\n![image.png](attachment:image.png)\nThe above equation computes the partial derivatives individually for every data point. Instead using the vectorized form, we can compute all of the gradients in one go.","17e5ce2e":"![image.png](attachment:image.png)\nTo implement the Gradient Descent, you need to compute the gradient of the cost function with respect to the parameter vector \u03b8. Here gradient means how much the cost function will change if there is a small change in the parameter vector \u03b8. To get the gradient, we need to take the partial derivative of the cost function.","b29d4a0d":"As you can see from the output, the model predicted decently. Let's plot the actual and predicted values for better understanding. For visualization purpose, we will take only 25 observations from the test set.","a27bec0e":"# **3. Evaluation Metrics:**\nSo far you have learned how linear regression model works and what are the parameters in this model. Once you train your model and got the predict output, you need to check how good is that prediction compared to the actual output. If the predictive power of the model is very poor you need to go back and tune the hyperparameters or use different algorithm so that the model\u2019s error is reduced thereby increasing its predictive power.\n\nThe most commonly used evaluation metrics for linear regression are Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE).\n![image.png](attachment:image.png)\nLet\u2019s apply this linear regression model in a real dataset and see how it works!\n"}}