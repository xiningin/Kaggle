{"cell_type":{"5b27c1b5":"code","fc83ef5c":"code","394702d9":"code","b8bc7de4":"code","844e1097":"code","5feb4c97":"code","5df7be60":"code","bf557dc5":"code","50f9f489":"code","88800b6a":"code","70877052":"code","816c1ddf":"code","0fb5f262":"code","d5b363b5":"code","33bb0d3b":"code","c79dcea4":"code","6dd4a745":"code","5f763608":"code","d9412dfe":"code","9c153daa":"code","15184890":"code","2f4c8cf8":"code","bc5fd068":"code","55639507":"code","ffb2d21b":"code","51907b63":"code","ee019313":"code","6e7cd281":"code","c229415f":"code","dc652b75":"code","fbdbfac7":"code","3a782cd0":"code","35f25d37":"code","c708305a":"code","21f2edf8":"code","a6844019":"code","18b8c889":"code","61dfa119":"code","ffd83495":"code","e9ed8a1c":"code","a45936cb":"code","b0fe4379":"code","73767b2d":"code","fd9a6541":"code","a323bcc6":"code","2e71c1ac":"code","0a82b670":"code","fb71c6f3":"code","9690750f":"code","ccbef2c1":"code","70a878a6":"code","87c06a5a":"code","194f1c0f":"code","ca26a183":"code","534934e8":"code","731d027d":"code","c1ac8689":"markdown","0770b070":"markdown","a3d3ea13":"markdown","3b30aef1":"markdown","74a56dde":"markdown","1854925f":"markdown","d684c477":"markdown","b9378eea":"markdown","db35a34d":"markdown","d8ff7041":"markdown","52b53f2d":"markdown","baf4aabf":"markdown","d822297e":"markdown","d61f7950":"markdown","7ce01e7a":"markdown","a7cd774b":"markdown","b7cc66ad":"markdown","a8cfaf11":"markdown","bd42f53f":"markdown","2d9c6181":"markdown","208e39e3":"markdown","5cfd499d":"markdown","6208bb48":"markdown","31e824fa":"markdown","f9f01781":"markdown","a82ed356":"markdown","ba921702":"markdown","234cf711":"markdown","d16925c0":"markdown","779dae20":"markdown","d872030a":"markdown","5efd388f":"markdown","6adc63ed":"markdown","d03922f3":"markdown","fffc7c5e":"markdown","fa981da2":"markdown","a5d511a0":"markdown"},"source":{"5b27c1b5":"import numpy as np \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid', {\"axes.grid\" : False})\nsns.set_context('notebook')\nnp.random.seed(42)","fc83ef5c":"iris            = pd.read_csv(\".\/data\/iris.csv\")\nlenses          = pd.read_csv(\".\/data\/lenses_final.csv\")\ncar             = pd.read_csv(\".\/data\/cars.csv\")\nhorsecolic      = pd.read_table(\".\/data\/horse-colic.data\")","394702d9":"from sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures","b8bc7de4":"import sklearn\nprint('The scikit-learn version is {}.'.format(sklearn.__version__))","844e1097":"baseDados = baseDados = pd.read_csv('data\/base_regressao_1.csv')\nLR = LinearRegression()\nX = baseDados[['X']]\nY = baseDados.Y\nLR.fit(X,Y)\n\nplt.plot(baseDados.X, LR.predict(baseDados[['X']]), c = 'darkgreen')\nplt.scatter(baseDados.X, baseDados.Y, s = 5, c = 'black')","5feb4c97":"baseDados.head()","5df7be60":"baseDados = baseDados = pd.read_csv('data\/base_regressao_2.csv')\nLR = LinearRegression()\nX = PolynomialFeatures(degree = 2, include_bias = False).fit_transform(baseDados.loc[:, baseDados.columns != 'Y'])\nY = baseDados.Y\nLR.fit(X,Y)\n\nplt.plot(baseDados.X, LR.predict(X), c = 'darkgreen')\nplt.scatter(baseDados.X, baseDados.Y, s = 5, c = 'black')","bf557dc5":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n%matplotlib inline\nfrom sklearn.tree import DecisionTreeClassifier","50f9f489":"pesos = pd.read_csv('weights.csv', sep=';')","88800b6a":"pesos.head()","70877052":"pesos.loc[pesos.sex=='F']['weight'].max()","816c1ddf":"pesos['sex'].value_counts()","0fb5f262":"pesos.hist()","d5b363b5":"pesos.dtypes","33bb0d3b":"pesos.shape","c79dcea4":"pesos.plot()","6dd4a745":"pesos.describe()","5f763608":"pesos.sex.value_counts()","d9412dfe":"plt.scatter(x=pesos.loc[pesos.sex=='M', 'height'], \n            y=pesos.loc[pesos.sex=='M', 'weight'], c='blue')\nplt.scatter(x=pesos.loc[pesos.sex=='F', 'height'], \n            y=pesos.loc[pesos.sex=='F', 'weight'], c='red')","9c153daa":"pesos = pesos[pesos.weight <= 100]\nplt.scatter(x=pesos.loc[pesos.sex=='M', 'height'], \n            y=pesos.loc[pesos.sex=='M', 'weight'], c='blue')\nplt.scatter(x=pesos.loc[pesos.sex=='F', 'height'], \n            y=pesos.loc[pesos.sex=='F', 'weight'], c='red')","15184890":"pesos.head()","2f4c8cf8":"dummies_sex = pd.get_dummies(pesos.sex, drop_first=True)\ndummies_sex.head()","bc5fd068":"pesos = pd.concat([pesos, dummies_sex], axis=1)\npesos.head()","55639507":"X = pesos[['M', 'height']]\ny = pesos.weight","ffb2d21b":"#teste\n#X = pesos[['weight', 'height']]\n#y = pesos.M","51907b63":"X.head()","ee019313":"lin_reg = LinearRegression()\nlin_reg.fit(X, y)","6e7cd281":"#from sklearn.neighbors import KNeighborsClassifier\n#from sklearn.linear_model import LogisticRegression\n#knnaula = KNeighborsClassifier(n_neighbors = 7)\n#knnaula = LogisticRegression()\n#knnaula.fit(X,y)","c229415f":"#y_pred = knnaula.predict(X)","dc652b75":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y, y_pred) # mudar essa linha no ToDo parte B\nconfusion_matrix","fbdbfac7":"from sklearn.metrics import classification_report\nprint(classification_report(y, y_pred)) # mudar essa linha no ToDo parte B","3a782cd0":"lin_reg.coef_","35f25d37":"lin_reg.intercept_","c708305a":"heights = np.linspace(140, 210, 70)\nsex_masc = np.ones(70)\nsex_fem = np.zeros(70)\n\ncoef_sex = float(lin_reg.coef_[0])\ncoef_height = float(lin_reg.coef_[1])\nintercept = float(lin_reg.intercept_)\n\nreta_masc = coef_sex*sex_masc + coef_height*heights + intercept\nreta_fem = coef_sex*sex_fem + coef_height*heights + intercept","21f2edf8":"plt.scatter(x=pesos.loc[pesos.sex=='M', 'height'], \n            y=pesos.loc[pesos.sex=='M', 'weight'], c='blue')\nplt.scatter(x=pesos.loc[pesos.sex=='F', 'height'], \n            y=pesos.loc[pesos.sex=='F', 'weight'], c='red')\nplt.plot(heights, reta_masc, '-', c='blue')\nplt.plot(heights, reta_fem, '-', c='red')\nplt.show()","a6844019":"from sklearn.metrics import mean_squared_error","18b8c889":"y_pred = lin_reg.predict(X)","61dfa119":"mean_squared_error(y, y_pred)","ffd83495":"a = lin_reg.predict([[0, 300]])\na","e9ed8a1c":"b = lin_reg.predict([[1, 150]])\nb","a45936cb":"c = b-a\nc","b0fe4379":"lin_reg.predict([[0, 60]])","73767b2d":"lin_reg.predict([[1, 173]])","fd9a6541":"data = pd.read_csv('boston.csv', sep=';', decimal=',')","a323bcc6":"data.head()","2e71c1ac":"data.describe()","0a82b670":"data.hist(bins=50, figsize=(20,15))","fb71c6f3":"y = #ToDo\nX = #ToDo","9690750f":"lin_reg = #ToDo\n#ToDo","ccbef2c1":"x_new = [[0.02731, 0.0, 7.07, 0, 0.469, 6.421, \n          78.900002, 4.9671, 2, 242, 17.799999, 396.899994, 9.14]]","70a878a6":"lin_reg.predict(x_new)","87c06a5a":"X_new = [[0.02731, 0.0, 7.07, 0, 0.469, 6.421, 78.900002, 4.9671, 2, \n          242, 17.799999, 396.899994, 9.14],\n         [0.67671, 0.8, 5.56, 0, 0.567, 3.132, 60.678976, 2.3465, 3, \n          432,  9.546666, 342.435664, 3.23], \n         [0.05641, 0.0, 1.04, 1, 0.232, 4.322, 67.564646, 1.5678, 5, \n          567, 15.898006, 224.655678, 5.84]]","194f1c0f":"lin_reg.predict(X_new)","ca26a183":"lin_reg.coef_","534934e8":"lin_reg.intercept_","731d027d":"mean_squared_error(lin_reg.predict(X), y)","c1ac8689":"Agora que tratamos os dados, podemos aplicar o m\u00e9todo de regress\u00e3o linear. Para isso especificamos qual ser\u00e1 nossa var\u00e1vel dependente (y) e as vari\u00e1veis independetes (X), ou atributos.","0770b070":"Agora que conhecemos os dados, podemos aplicar o m\u00e9todo de regress\u00e3o linear. Para isso especificamos qual ser\u00e1 nossa var\u00e1vel dependente (y) e as vari\u00e1veis independetes (X), ou atributos.","a3d3ea13":"Prevendo o peso de um homem de 1,73 metros","3b30aef1":"<p style=\"margin-bottom:1cm;\"><\/p>\n<h2 style=\"text-align:center;\">Como encontrar os melhores coeficientes?<\/h2>\n<p style=\"margin-bottom:1cm;\"><\/p>\n\n<b> M\u00e9todo tradicional (m\u00ednimos quadrados): <\/b> Seja $X$ a matriz com vari\u00e1veis preditivas e $y$ a vari\u00e1vel resposta, queremos encontrar $\\beta$ tal que $X\\beta = y$. Se $X$ for invert\u00edvel, ent\u00e3o claramente $\\beta$ = $X^{-1}y$. Quando $X$ n\u00e3o \u00e9 invert\u00edvel, podemos obter a melhor solu\u00e7\u00e3o poss\u00edvel usando a pseudo-inversa de $X$: $\\beta$ = $X^+y$ = $(X^TX)^{-1}X^Ty$.\n\n<b> M\u00e9todo num\u00e9rico (gradiente descendente): <\/b> Podemos definir o erro de uma predi\u00e7\u00e3o $f_{\\beta}(x_i)$ como $L(x_i, y_i) = (f_{\\beta}(x_i) - y_i)^2$.\n\n![](img\/erro_reg.png)\n\nEnt\u00e3o podemos definir o erro total (ou custo) de um modelo como a m\u00e9dia de cada erro individual:\n\n$$ J(f_\\beta, y) = \\frac{1}{N} \\sum_{i = 1}^{N} L(x_i, y_i)$$\n\nO que queremos \u00e9 minimizar $J(f_\\beta, y)$ em termos de $\\beta$. A solu\u00e7\u00e3o $\\beta$ encontrada ser\u00e1 exatamente a mesma dada pelo m\u00e9todo de m\u00ednimos quadrados.","74a56dde":"Agora que temos um modelo treinado, podemos prever o valor de um novo exemplo:","1854925f":"e o valor do bias:","d684c477":"Podemos visualizar os coeficientes da reta","b9378eea":"logo, nossa equa\u00e7\u00e3o da reta \u00e9 <b>peso = sexo_Masculino x 7.9165 + altura x 0.7487 - 64.7764<\/b>. Nesta equa\u00e7\u00e3o j\u00e1 podemos observar que se a pessoa \u00e9 do sexo masculino ela tende a pesar aproximadamente 7.92 quilos a mais","db35a34d":"A fim de obtermos mais detalhes sobre os dados num\u00e9ricos, podemos usar o m\u00e9todo describe()","d8ff7041":"Vamos come\u00e7ar lendo os dados do arquivo .csv","52b53f2d":"<h1 style=\"text-align:center\"> Machine Learning Hands-On <\/h1>","baf4aabf":"Nosso conjunto de dados possui 14 colunas:<br>\n    <b>CRIM<\/b>: taxa de crimes per capta na cidade <br>\n    <b>ZN<\/b>: propor\u00e7\u00e3o de terrenos residenciais zoneada para lotes acima de 25.000<br>\n    <b>INDUS<\/b>: propor\u00e7\u00e3o de zonas de neg\u00f3cios n\u00e3o varejistas na cidade<br>\n    <b>CHAS<\/b>: proximidade ao rio Carles 1 se pr\u00f3ximo, 0 se n\u00e3o<br>\n    <b>NOX<\/b>: concentra\u00e7\u00e3o de oxido nitrico (ppm)<br>\n    <b>RM<\/b>: n\u00famero m\u00e9dio de c\u00f4modos por resid\u00eancia<br>\n    <b>AGE<\/b>: propor\u00e7\u00e3o de unidades ocupadas pelo propriet\u00e1rio constru\u00eddas antes de 1940<br>\n    <b>DIS<\/b>: dist\u00e2ncia ponderada aos 5 centros financeiros de Boston<br>\n    <b>RAD<\/b>: \u00edndice de acessibilidade \u00e0s rodoviais <br>\n    <b>TAX<\/b>: taxa de imposto predial de valor integral por USS 10.000 <br>\n    <b>PT<\/b>: rela\u00e7\u00e3o aluno-professor por cidade<br>\n    <b>B<\/b>: 1000 (Bk \u2212 0,63) 2 onde Bk \u00e9 a propor\u00e7\u00e3o de negros por cidade<br>\n    <b>LSTAT<\/b>: menor status da popula\u00e7\u00e3o <br>\n    <b>MV<\/b>: Valor mediano de casas ocupadas pelo propriet\u00e1rio em USS 1000s","d822297e":"Para valores discretos podemos usar o m\u00e9todo value_counts()","d61f7950":"Anexamos as novas colunas \u00e0 nossa base de dados. Axis = 1 indica que queremos anexar colunas (usar 0 para linhas)","7ce01e7a":"Tamb\u00e9m podemos prever uma lista de valores de uma \u00fanica vez:","a7cd774b":"Nesta base temos 112 pessoas do sexo feminino e 88 do sexo masculino","b7cc66ad":"<h2 style=\"text-align:center; margin:40px\"> Regress\u00e3o Linear <\/h2>\n\n<img src=\"img\/house_prices.png\"\/>\n\nA <b>regress\u00e3o linear<\/b> \u00e9 uma t\u00e9cnica de aprendizado supervisionado que encontra um estimador linear $y \\approx f(x)$ onde $y$ \u00e9 a vari\u00e1vel resposta e $x \\in \\mathbb{R}^m$ ($m$ vari\u00e1veis preditivas).\n\nSe $x \\in \\mathbb{R}$ (uma \u00fanica vari\u00e1vel preditiva), ent\u00e3o $f(x) = ax+b$.\n\nPodemos re-rescrever a fun\u00e7\u00e3o como $ f_{\\beta}(x) = {\\beta_0} + {\\beta_1}x $.\n\nSe $x$ \u00e9 dado por $m$ vari\u00e1veis preditivas, ent\u00e3o $ f_{\\beta}(x) = {\\beta_0} + {\\beta_1}x_1 + ... + {\\beta_n}x_m$.\n\nOs termos $\\beta$ s\u00e3o denominados coeficientes, e s\u00e3o encontrados pelo regressor de forma a melhor se adequar aos dados.","a8cfaf11":"Vamos visualizar esses dados graficamente","bd42f53f":"Aqui encontramos um problema. Exitem alguns pontos muito fora da distribui\u00e7\u00e3o de dados (160, 120), esses pontos s\u00e3o os outliers. Eles podem prejudicar o modelo, porque for\u00e7ar\u00e3o a reta para pr\u00f3ximo deles. Temos que remov\u00ea-los.\n<br>Veja tamb\u00e9m que os pesos das pessoas de sexo masculino est\u00e3o deslocados para a direita, o que indica uma tend\u00eancia de homens serem mais pesados que as mulheres. Nosso modelo nos mostrar\u00e1 isso.","2d9c6181":"Vamos analisar as 5 primeiras linhas dos nossos dados com o m\u00e9todo head() ","208e39e3":"Como visto em aula, para o caso de vari\u00e1veis categ\u00f3ricas devemos utilizar \"dummy variables\". O m\u00e9todo get_dummies do pandas cuida disso. Teremos um valor bin\u00e1rio onde 1 corresponde a masculino e 0 a feminino","5cfd499d":"Tamb\u00e9m \u00e9 poss\u00edvel visualizarmos os coeficientes da reta para cada atributo","6208bb48":"Vamos realizar um o primeiro exerc\u00edcio que consiste em uma regress\u00e3o linear simples, analisando peso, altura e sexo de 200 pessoas","31e824fa":"Vamos plotar nossa fun\u00e7\u00e3o. Observe que como temos uma vari\u00e1vel categ\u00f3rica que possui dois valores, nossa reta se comporta de forma diferente para cada sexo, deslocando 7.9165 pontos para cima se for masculino. Desta forma vamos plotar uma reta para cada sexo.","f9f01781":"<h3> Scikit-Learn <\/h3>\nAgora, vamos rodar os c\u00f3digos abaixo que usam uma implementa\u00e7\u00e3o do algoritmo dispon\u00edvel no pacote <i>sklearn<\/i> (Scikit-Learn).","a82ed356":"<b>Vinicius F. Carid\u00e1<\/b>","ba921702":"Tamb\u00e9m podemos calcular o erro m\u00e9dio quadrado (MSE) do nosso modelo durante o treinamento","234cf711":"Se utilizarmos atributos polinomiais, tamb\u00e9m podemos treinar modelos n\u00e3o lineares usando uma regress\u00e3o linear! Neste exemplo temos um modelo do tipo $ f_{\\beta}(x) = {\\beta_0} + {\\beta_1}x + {\\beta_2}x^2 $","d16925c0":"A fim de obtermos mais detalhes sobre os dados num\u00e9ricos, podemos usar o m\u00e9todo describe()","779dae20":"Prevendo o peso de uma mulher de 1,57 metros","d872030a":"Neste exerc\u00edcio vamos prever o valor mediano de uma casa na redondeza de Boston dados determinados atributos. Para isso utilizamos a base de dados 'boston.csv'","5efd388f":"<h3 class=\"title\"> Links \u00fateis: <\/h3>\n<br>\n<ol>\n  <li class=\"item\"><a href = \"http:\/\/scikit-learn.org\/stable\/\"> Sklearn<\/a>: principal biblioteca de aprendizado de m\u00e1quina para python.<\/li>\n  <li class=\"item\"><a href = \"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/\"> Pandas<\/a>: (quase) tudo o que voc\u00ea precisa para trabalhar rapidamente com tabelas<\/li>\n  <li class=\"item\"><a href = \"https:\/\/docs.scipy.org\/doc\/numpy\/reference\/index.html\"> Numpy<\/a>: fun\u00e7\u00f5es matem\u00e1ticas est\u00e3o aqui<\/li>\n  <li class=\"item\"><a href = \"https:\/\/matplotlib.org\/contents.html\"> Matplotlib<\/a>: te ajuda a plotar!<\/li>\n  <li class=\"item\"><a href = \"https:\/\/seaborn.pydata.org\/api.html\"> Seaborn<\/a>: Deixa a matplotlib bonita (cont\u00e9m plots mais bem estruturados)<\/li>\n<\/ol>","6adc63ed":"Podemos visualizar a distribui\u00e7\u00e3o dos dados por meio de um histograma:","d03922f3":"<h4> Agora vamos analisar um exemplo mais complexo, envolvendo mais vari\u00e1veis <\/h4>","fffc7c5e":"<b>Trabalharemos com as seguintes bases de dados:<\/b>\n- iris.csv\n- lenses_final.csv\n- car.data\n- horse-colic.data\n- PETR4.SA.csv\n- Boston.csv\n- artificial_data\n\n<b>Testem a leitura de um dos arquivos deste curso<\/b>\n- Acessem a documenta\u00e7\u00e3o da fun\u00e7\u00e3o <i>read_csv<\/i> do <i>pandas<\/i>: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.read_csv.html","fa981da2":"<p style=\"margin-bottom:1cm;\"><\/p>\n<h2 style=\"text-align:center;\">Como minimizar?<\/h2>\n<p style=\"margin-bottom:1cm;\"><\/p>\n\nPodemos inicializar os valores de cada $\\beta_i$ aleatoriamente, e ent\u00e3o melhorar nosso chute inicial utilizando as <b>derivadas parciais<\/b> de $J(f_\\beta, y)$:\n\n$$\\beta_{i} := \\beta_{i} - \\alpha \\, \\frac{\\partial J}{\\partial \\beta_{i}}$$\n\nO par\u00e2metro $\\alpha$ \u00e9 chamado de <b>taxa de aprendizado<\/b>, e \u00e9 um par\u00e2metro extremamente importante principalmente no treinamento de <b>redes neurais<\/b>\n\n![](img\/gradiente.gif)","a5d511a0":"Com isso, obtemos o n\u00famero de exemplos, a m\u00e9dia de cada atributo, desvio padr\u00e3o, valor m\u00ednimo, primeiro, segundo e terceiro quartil e valor m\u00e1ximo do atributo, respectivamente."}}