{"cell_type":{"8177abb1":"code","1c5cd520":"code","046fded0":"code","810179cc":"code","cd1d798c":"code","0a7920e5":"code","72c25201":"code","0d8c0489":"code","139006c3":"code","c53b90ea":"code","c1b16d6b":"code","bcf3b637":"code","e3f50a6a":"code","24562a79":"code","3989863f":"code","49a54965":"code","8ffe04e8":"code","44933cb8":"code","ec252488":"code","ff114748":"code","e4d447b1":"code","97cd95fd":"code","9cc05aa9":"code","388fd510":"code","ea5add86":"code","f3932385":"code","34ac5d2f":"code","ddd1b82f":"code","cbaf62d1":"code","47c7bd54":"code","cda4752f":"code","c0c7b4c2":"code","2f4fac23":"code","e7b9a1cb":"code","dfaf5b1f":"code","123c4fea":"code","a654f314":"code","7822f1c2":"code","5155e93f":"code","90ffbe73":"code","cba5e71c":"code","880e6dc5":"code","2adb185a":"code","998e6a46":"code","bcf838d8":"code","934240cb":"code","006d4e4b":"code","476f781c":"code","3546707f":"code","1a9c1ed5":"code","47a57b08":"code","887bca61":"code","bbdbfb0e":"code","2e246fb4":"code","3eb11b79":"code","553ba049":"code","2e1eb3a4":"code","b92c2149":"code","37f50970":"code","33a21094":"code","0ef24c97":"code","2bc75bfe":"code","a6f0e07f":"code","c1490ff7":"code","d1aacfc9":"code","79ff0e2e":"code","c4ec5f74":"code","b25efc71":"code","0d44c722":"code","31efed9b":"code","99abda87":"code","f47a9d73":"code","abd336c9":"code","7ba14d81":"code","a5b706e5":"code","b8991f54":"code","88531efb":"code","f9c00e29":"code","e79e28b3":"code","4bb8e9a5":"code","542fd9e6":"code","6b36cab0":"code","be181381":"code","df8b168c":"code","80d0be4a":"code","a51a8507":"code","1ab6fcc9":"code","13d490dc":"code","eba40cf2":"code","b5d16484":"code","18ea01e6":"code","35c17352":"code","99c0a464":"code","d5c982d7":"code","941ed603":"code","dd4d5e7a":"code","483dc69f":"code","276984f3":"markdown","5e4d927b":"markdown","8c7f045a":"markdown","c2862f97":"markdown","530fd1f3":"markdown","491f64e2":"markdown","f795fb2e":"markdown","422c78c6":"markdown","506e878c":"markdown","8c0c053b":"markdown","775a1114":"markdown","7648b9c8":"markdown","8b816267":"markdown","710a9a6f":"markdown","3c5cfbb3":"markdown","d50d3878":"markdown","040c6345":"markdown","8c455390":"markdown","8646c097":"markdown","bc148d7b":"markdown","f139c4cc":"markdown","5357c612":"markdown","500fae51":"markdown","e18715c3":"markdown","ab5f4196":"markdown","0f68601e":"markdown","216d99fc":"markdown","9a9b48c6":"markdown","620a6aa3":"markdown","a2ecdfd0":"markdown","7ee4411d":"markdown","12784c90":"markdown","dd047076":"markdown","b7ae07e7":"markdown","4a32c865":"markdown","57520604":"markdown","c60771d0":"markdown","8da41f3b":"markdown","eb899a3b":"markdown","de1cc04b":"markdown","45aded22":"markdown","14ebe64c":"markdown","67537be0":"markdown","26a88d67":"markdown","7e467888":"markdown","1964e46b":"markdown","342894f7":"markdown","e29b5065":"markdown","081ed9ee":"markdown","22ed4cc5":"markdown","628dc08c":"markdown","53249f9e":"markdown"},"source":{"8177abb1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math","1c5cd520":"train_dataset = pd.read_csv(r\"..\/input\/titanic\/train.csv\")\ntest_dataset = pd.read_csv(r\"..\/input\/titanic\/test.csv\")","046fded0":"### Viewing sample train_dataset\n\ntrain_dataset.head(10)","810179cc":"### Summary of the train_dataset\n\ntrain_dataset.describe()","cd1d798c":"### Observing whether the dataset has null values or not\n\ntrain_dataset.info()","0a7920e5":"### Understanding the distribution of gender\n\ngender = list(train_dataset['Sex'])\n\nplt.title(\"Understanding the total number of males and females\")\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Value Counts of Male and Female\")\nplt.hist(gender, bins = 5)\nplt.show()","72c25201":"### Plotting an area plot to understand the relation between Gender and Survived\n\ngender_survived = pd.DataFrame(train_dataset.iloc[:, [1, 4]].values)\nmale_dead, female_survived, male_survived, female_dead = list(gender_survived.value_counts())\n\nplt.stackplot(['Male', 'Female'], [male_dead, female_dead], [male_survived, female_survived],  \n              labels = ['Dead', 'Survived'], alpha = 0.7)\nplt.legend()\nplt.xlabel(\"Gender\")\nplt.ylabel(\"Value Counts\")\nplt.title(\"Area chart to show survival rate between Males and Females\")\nplt.show()","0d8c0489":"### Calculating the total bookings done by a passenger\n\ntotal_bookings = list(train_dataset['SibSp'] + train_dataset['Parch'] + 1) ### We add 1 to include the booking he made for his own","139006c3":"### Understanding the ditribution of total bookings done by a passenger\n\nplt.hist(total_bookings, bins = 10)\nplt.xlabel(\"Total bookings done by a passenger\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Understanding the distribution of total bookings done by a passenger\")\nplt.show()","c53b90ea":"### Plotting a scatter plot between total bookings and survived\n\nsurvived = list(train_dataset['Survived'])\n\ncolor = [\"red\" if x == 0 else \"blue\" for x in survived]\nplt.scatter(total_bookings, survived, c = color)\nplt.xlabel(\"Total Number of Bookings\")\nplt.ylabel(\"Survived\")\nplt.title(\"Scatterplot between total number of bookings and survived\")\nplt.show()","c1b16d6b":"### Understanding the distribution of age\n\noriginal_age = list(train_dataset['Age'])\noriginal_fare = list(train_dataset['Fare'])\nnot_null_age = []\ncorresponding_fare = []\n\nfor index in range(len(original_age)):\n    if not math.isnan(original_age[index]) :\n        not_null_age.append(original_age[index])\n        corresponding_fare.append(original_fare[index])","bcf3b637":"### Plotting a histogram to understand the distribution of age\n\nplt.xlabel(\"Distribution of the values age\")\nplt.ylabel(\"Frequency of each age\")\nplt.title(\"Histogram to understand the disribution of age\")\nplt.hist(not_null_age, bins = 50)\nplt.show()","e3f50a6a":"### Plotting a scatterplot between age and fare\n\nplt.xlabel(\"Distribution of the value age in years\")\nplt.ylabel(\"Distribution of the fares\")\nplt.title(\"Scatterplot between age and fare\")\nplt.scatter(not_null_age, corresponding_fare)","24562a79":"### Plotting a box plot for the fare paid by survived passengers and dead passengers\n\nfare_survived = list(train_dataset[train_dataset['Survived'] == 0]['Fare'])\nfare_dead = list(train_dataset[train_dataset['Survived'] == 1]['Fare'])\ndata = [fare_survived, fare_dead]\n\nfig = plt.figure(figsize =(5, 5))\nax = fig.add_axes([0, 0, 1, 1])\nbp = ax.boxplot(data, labels = ['Survived', 'Dead'])\nplt.ylabel(\"Distribution of the fares\")\nplt.title(\"Boxplots for the fare paid by survived passengers and dead passengers\")\nplt.show()","3989863f":"### Understanding the distribution of the variable - Embarked\n\nembarked = list(train_dataset[~train_dataset['Embarked'].isnull()]['Embarked'])\nlen(embarked)","49a54965":"### Plotting a bar graph showing the distribution of Embarked\n\ns_count = embarked.count('S')\nc_count = embarked.count('C')\nq_count = embarked.count('Q')\nplt.xlabel(\"Different Embarked values\")\nplt.ylabel(\"Value counts of each embarked value\")\nplt.title(\"Bar graph showing the distribution of different embarked values\")\nplt.bar(['S', 'C', 'Q'], [s_count, c_count, q_count], label = 'Embarked')\nplt.legend()\nplt.show()","8ffe04e8":"### Plotting an area plot to understand which Embarked has a better survival rate\n\nnot_null_embarked = train_dataset[~train_dataset['Embarked'].isnull()]\nembarked_survived = pd.DataFrame(not_null_embarked.iloc[:, [1, 11]].values)\ns_dead, s_survived, c_survived, c_dead, q_dead, q_survived = list(embarked_survived.value_counts())\n\nplt.stackplot(['S', 'C', 'Q'], [s_dead, c_dead, q_dead], [s_survived, c_survived, q_survived],  \n              labels = ['Dead', 'Survived'], alpha = 0.7)\nplt.legend()\nplt.xlabel(\"Embarked\")\nplt.ylabel(\"Value Counts\")\nplt.title(\"Area chart to show survival rate between various Embarked\")\nplt.show()","44933cb8":"### Plotting an area plot to understand which Pclass has a better survival rate\n\npclass_survived = pd.DataFrame(train_dataset.iloc[:, [1, 2]].values)\ndead_3, survived_1, survived_3, dead_2, survived_2, dead_1 = list(pclass_survived.value_counts())\n\nplt.stackplot(['1', '2', '3'], [dead_1, dead_2, dead_3], [survived_1, survived_2, survived_3],  \n              labels = ['Dead', 'Survived'], alpha = 0.7)\nplt.legend()\nplt.xlabel(\"Passenger Class\")\nplt.ylabel(\"Value Counts\")\nplt.title(\"Area chart to show survival rate between various Pclass\")\nplt.show()","ec252488":"### Fetching the modified ticket values\n'''\nTicket has no null values. However, we will divide tickets into 2 categories.\n1. It has no alphabet in it.\n2. It has an alphabet in it.\n'''\nticket = list(train_dataset['Ticket'])\nticket_class = []\nfor each_ticket in ticket:\n    try:\n        x = int(each_ticket)\n        ticket_class.append(1)\n    except:\n        ticket_class.append(2)","ff114748":"### Plotting an area plot to understand which Ticket has a better survival rate\n\nticket_survived = pd.DataFrame(train_dataset['Survived'])\nticket_survived['Ticket'] = ticket_class\n\nclass1_dead, class1_survived, class2_dead, class2_survived = list(ticket_survived.value_counts())\nplt.stackplot(['1', '2'], [class1_dead, class2_dead], [class1_survived, class2_survived],  \n              labels = ['Dead', 'Survived'], alpha = 0.7)\nplt.legend()\nplt.xlabel(\"Ticket Class\")\nplt.ylabel(\"Value Counts\")\nplt.title(\"Area chart to show survival rate between various Ticket classes\")\nplt.show()","e4d447b1":"### Plotting the correlation between various columns\n\nfiltered_dataset = train_dataset.drop(['PassengerId', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], axis = 1)\nplt.figure(figsize=(6, 6))\nheatmap = sns.heatmap(filtered_dataset.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)","97cd95fd":"### Packages for data preprocessing\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split","9cc05aa9":"### Removing the columns - PassengerId, Name, Cabin in Train and Test sets\n\ntrain_data = train_dataset.drop(['PassengerId', 'Name', 'Cabin'], axis = 1)\ntest_data = test_dataset.drop(['PassengerId', 'Name', 'Cabin'], axis = 1)","388fd510":"train_data.head()","ea5add86":"test_data.head()","f3932385":"test_data.info()","34ac5d2f":"### Modifying the Ticket column in Train and Test sets\n\ndef modifyTicket(data):\n    ticket_class = []\n    for each_ticket in data:\n        try:\n            x = int(each_ticket)\n            ticket_class.append(1)\n        except:\n            ticket_class.append(2)\n    return ticket_class","ddd1b82f":"train_ticket_class = modifyTicket(list(train_data['Ticket']))\nlen(train_ticket_class)","cbaf62d1":"test_ticket_class = modifyTicket(list(test_data['Ticket']))\nlen(test_ticket_class)","47c7bd54":"### Adding the modified column Ticket to Train and Test sets\n\ntrain_data['Ticket'] = train_ticket_class\ntest_data['Ticket'] = test_ticket_class","cda4752f":"train_data.head()","c0c7b4c2":"test_data.head()","2f4fac23":"### Dividing the Train data into dependent and independent variables\n\nX = train_data.iloc[:, 1:].values\nY = train_data.iloc[:, 0].values","e7b9a1cb":"X","dfaf5b1f":"Y","123c4fea":"### Taking care of missing data in Train data \n\n### Replacing missing age with mean\n\nsimpleImputerAge = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nsimpleImputerAge.fit(X[:, 2:3])\nX[:, 2:3] = simpleImputerAge.transform(X[:, 2:3])\n\n### Replacing missing embarked with mode\n\nsimpleImputerEmbarked = SimpleImputer(missing_values = np.nan, strategy = 'most_frequent')\nsimpleImputerEmbarked.fit(X[:, 7:8])\nX[:, 7:8] = simpleImputerEmbarked.transform(X[:, 7:8])","a654f314":"X[0]","7822f1c2":"### Taking care of missing data in Test data\n\n### Replacing missing age with mean\nnew_test = test_data.iloc[:, :].values\n\nsimpleImputerAgeTest = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nsimpleImputerAgeTest.fit(new_test[:, 2:3])\nnew_test[:, 2:3] = simpleImputerAgeTest.transform(new_test[:, 2:3])\n\n### Replacing missing fare with mean\n\nsimpleImputerFareTest = SimpleImputer(missing_values = np.nan, strategy = 'mean')\nsimpleImputerFareTest.fit(new_test[:, 6:7])\nnew_test[:, 6:7] = simpleImputerFareTest.transform(new_test[:, 6:7])","5155e93f":"new_test[0]","90ffbe73":"### Encoding categorical data in Train data\n\ncolumnTransformer = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [1, 7])], \n                                     remainder = 'passthrough')\nX = np.array(columnTransformer.fit_transform(X))\nX[0]","cba5e71c":"### Encoding categorical data in Test data\n\ncolumnTransformerTest = ColumnTransformer(transformers = [('encoder', OneHotEncoder(), [1, 7])], \n                                     remainder = 'passthrough')\nnew_test = np.array(columnTransformerTest.fit_transform(new_test))\nnew_test[0]","880e6dc5":"### Dividing the Train data into training and test sets\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)","2adb185a":"len(X_train)","998e6a46":"len(X_test)","bcf838d8":"len(Y_train)","934240cb":"len(Y_test)","006d4e4b":"### Feature Scaling the Train data\n\nstandardScaler = StandardScaler()\nX_train = standardScaler.fit_transform(X_train)\nX_test = standardScaler.transform(X_test)","476f781c":"X_train[0]","3546707f":"X_test[0]","1a9c1ed5":"### Feature Scaling the Test data\n\nstandardScaler = StandardScaler()\nnew_test = standardScaler.fit_transform(new_test)\nnew_test[0]","47a57b08":"### Importing required libraries\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","887bca61":"### Applying Logistic Regression model \n\nlogistic_classifier = LogisticRegression(random_state = 0)\nlogistic_classifier.fit(X_train, Y_train)","bbdbfb0e":"### Predicting the Test set results\n\nY_pred = logistic_classifier.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","2e246fb4":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","3eb11b79":"### Applying 1NN model\n\nclassifier_1nn = KNeighborsClassifier(n_neighbors = 1, algorithm = 'auto', p = 2, metric = 'minkowski')\nclassifier_1nn.fit(X_train, Y_train)","553ba049":"### Predicting the Test set results\n\nY_pred = classifier_1nn.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","2e1eb3a4":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","b92c2149":"### Applying 3NN model\n\nclassifier_3nn = KNeighborsClassifier(n_neighbors = 3, algorithm = 'auto', p = 2, metric = 'minkowski')\nclassifier_3nn.fit(X_train, Y_train)","37f50970":"### Predicting the Test set results\n\nY_pred = classifier_3nn.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","33a21094":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","0ef24c97":"### Applying 5NN model\n\nclassifier_5nn = KNeighborsClassifier(n_neighbors = 5, algorithm = 'auto', p = 2, metric = 'minkowski')\nclassifier_5nn.fit(X_train, Y_train)","2bc75bfe":"### Predicting the Test set results\n\nY_pred = classifier_5nn.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","a6f0e07f":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","c1490ff7":"### Applying 7NN model\n\nclassifier_7nn = KNeighborsClassifier(n_neighbors = 7, algorithm = 'auto', p = 2, metric = 'minkowski')\nclassifier_7nn.fit(X_train, Y_train)","d1aacfc9":"### Predicting the Test set results\n\nY_pred = classifier_7nn.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","79ff0e2e":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","c4ec5f74":"### Applying 9NN model\n\nclassifier_9nn = KNeighborsClassifier(n_neighbors = 9, algorithm = 'auto', p = 2, metric = 'minkowski')\nclassifier_9nn.fit(X_train, Y_train)","b25efc71":"### Predicting the Test set results\n\nY_pred = classifier_9nn.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","0d44c722":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","31efed9b":"### Applying 11NN model\n\nclassifier_11nn = KNeighborsClassifier(n_neighbors = 11, algorithm = 'auto', p = 2, metric = 'minkowski')\nclassifier_11nn.fit(X_train, Y_train)","99abda87":"### Predicting the Test set results\n\nY_pred = classifier_11nn.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","f47a9d73":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","abd336c9":"### Applying 13NN model\n\nclassifier_13nn = KNeighborsClassifier(n_neighbors = 13, algorithm = 'auto', p = 2, metric = 'minkowski')\nclassifier_13nn.fit(X_train, Y_train)","7ba14d81":"### Predicting the Test set results\n\nY_pred = classifier_13nn.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","a5b706e5":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","b8991f54":"### Applying 15NN model\n\nclassifier_15nn = KNeighborsClassifier(n_neighbors = 15, algorithm = 'auto', p = 2, metric = 'minkowski')\nclassifier_15nn.fit(X_train, Y_train)","88531efb":"### Predicting the Test set results\n\nY_pred = classifier_15nn.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","f9c00e29":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","e79e28b3":"### Applying Naive Bayes Classification model\n\nnaive_bayes_classifier = GaussianNB()\nnaive_bayes_classifier.fit(X_train, Y_train)","4bb8e9a5":"### Predicting the Test set results\n\nY_pred = naive_bayes_classifier.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","542fd9e6":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","6b36cab0":"### Applying Linear SVM Classification model\n\nlinear_svm_classifier = SVC(kernel = 'linear', random_state = 0)\nlinear_svm_classifier.fit(X_train, Y_train)","be181381":"### Predicting the Test set results\n\nY_pred = linear_svm_classifier.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","df8b168c":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","80d0be4a":"### Applying Kernel SVM Classification model\n\nkernel_svm_classifier = SVC(kernel = 'rbf', random_state = 0)\nkernel_svm_classifier.fit(X_train, Y_train)","a51a8507":"### Predicting the Test set results\n\nY_pred = kernel_svm_classifier.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","1ab6fcc9":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","13d490dc":"### Applying Decision Tree Classification model\n\ndecision_tree_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 1)\ndecision_tree_classifier.fit(X_train, Y_train)","eba40cf2":"### Predicting the Test set results\n\nY_pred = decision_tree_classifier.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","b5d16484":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","18ea01e6":"### Applying Random Forest Classification model\n\nrandom_forest_classifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nrandom_forest_classifier.fit(X_train, Y_train)","35c17352":"### Predicting the Test set results\n\nY_pred = random_forest_classifier.predict(X_test)\nprint(np.concatenate((Y_pred.reshape(len(Y_pred), 1), Y_test.reshape(len(Y_test), 1)), 1))","99c0a464":"### Making the confusion matrix\n\ncm = confusion_matrix(Y_test, Y_pred)\nprint(cm)\naccuracy_score(Y_test, Y_pred)","d5c982d7":"### Tabulating the results\n\nfrom tabulate import tabulate\n\ntable = []\ntable.append(['S.No', 'Classification Model', 'Accuracy'])\ntable.append(['1', 'Logistic Regression', '80.44%'])\ntable.append(['2', '1-Nearest Neighbors', '79.33%'])\ntable.append(['3', '3-Nearest Neighbors', '79.88%'])\ntable.append(['4', '5-Nearest Neighbors', '79.88%'])\ntable.append(['5', '7-Nearest Neighbors', '80.44%'])\ntable.append(['6', '9-Nearest Neighbors', '83.24%'])\ntable.append(['7', '11-Nearest Neighbors', '83.79%'])\ntable.append(['8', '13-Nearest Neighbors', '82.13%'])\ntable.append(['9', '15-Nearest Neighbors', '82.12%'])\ntable.append(['10', 'Naive Bayes Classification', '80.45%'])\ntable.append(['11', 'Linear SVM Classification', '78.77%'])\ntable.append(['12', 'Kernel SVM Classification', '80.45%'])\ntable.append(['13', 'Decision Tree Classification', '77.65%'])\ntable.append(['14', 'Random Forest Classification', '83.79%'])\n\nprint(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))","941ed603":"### Predicting the Test data results\n\npredictions_11nn = classifier_11nn.predict(new_test)\nprint(len(predictions_11nn))","dd4d5e7a":"### Creating a result dataframe using PassengerId and generated predictions\n\npassengerId = list(range(892, 1310))\nresult_11nn = pd.DataFrame(passengerId, columns = ['PassengerId'])\nresult_11nn['Survived'] = predictions_11nn\nresult_11nn","483dc69f":"### Creating a CSV file of the predictions\n\ncompression_opts = dict(method = 'zip', archive_name = 'predictions_11nn.csv')  \nresult_11nn.to_csv('predictions_11nn.zip', index = False, compression = compression_opts)","276984f3":"#### Training the 5-Nearest Neighbors model on the Training set","5e4d927b":"#### Exploratory Data Analysis","8c7f045a":"#### Training the Naive Bayes Classification model on the Training set","c2862f97":"#### 8. Which value of the column - Ticket had a better survival rate?","530fd1f3":"#### 9. Correlation between various columns","491f64e2":"#### 3. Understanding the price of ticket with respect to age? (Scatterplot between age and fare)","f795fb2e":"#### Modifying the Ticket column","422c78c6":"#### Training the 13-Nearest Neighbors model on the Training set","506e878c":"#### Data Preprocessing","8c0c053b":"#### Importing the dataset","775a1114":"#### 4. Did ticket fare play a role on a passenger's survival?","7648b9c8":"#### Encoding categorical data","8b816267":"#### Training the 9-Nearest Neighbors model on the Training set","710a9a6f":"#### 1. Did gender play a role on survival?","3c5cfbb3":"#### Training the 1-Nearest Neighbor model on the Training set","d50d3878":"#### Training the Linear Support Vector Machine Classification model on the Training set","040c6345":"#### Handling missing data","8c455390":"On observing the above graph, we can say that the graph is right skewed with outlier at the far end to the right side.","8646c097":"#### Training the 15-Nearest Neighbors model on the Training set","bc148d7b":"#### Training the 3-Nearest Neighbors model on the Training set","f139c4cc":"#### 7. Which value of the column - Pclass had a better survival rate?","5357c612":"#### Training the Random Forest Classification model on the Training set","500fae51":"#### Packages for data preprocessing","e18715c3":"#### 5. Distribution of the variable - Embarked","ab5f4196":"#### Dividing the data into dependent and independent variables","0f68601e":"#### Tabulating the results","216d99fc":"On observing the above graph, we can say that passengers with Ticket class 1 have a good survival rate.","9a9b48c6":"On observing the above table, we can say that the 11-Nearest Neighbors model and the Random Forest Classification model have the best accuracy among all the other classification models. Here, we are going to submit predictions of 11-Nearest Neighbors Classification model to predict the Test data results.","620a6aa3":"On observing the above graph, we can say that most of the passengers embarked in 'S'.","a2ecdfd0":"#### Applying Classification models","7ee4411d":"On observing the above scatterplot, we can say that there is no significant relation between total number of bookings and survived.","12784c90":"#### Training the 11-Nearest Neighbors model on the Training set","dd047076":"#### Importing Required libraries","b7ae07e7":"#### Dividing the dataset into Training and Test sets","4a32c865":"On observing the above boxplots we can say that people who survived paid a fare relatively lesser than those who died. However, we cannot say an exact relation between fare and survived.","57520604":"#### Feature Scaling the data","c60771d0":"On observing the above graph, we can say that the age is distributed normally with a slight right skewed. We can use this data when we replace the na values.","8da41f3b":"On observing the above graph, we can say that the survival rate is better in 'C' and 'Q' than 'S'.","eb899a3b":"#### Predicting the Test data results using 11-Nearest Neighbors Classification Technique","de1cc04b":"On observing the above data, we can see that there are null values in the columns Age, Cabin, and Embarked.","45aded22":"#### Remove unnecessary columns","14ebe64c":"#### Training the Logistic Regression model on the Training set","67537be0":"#### Training the 7-Nearest Neighbors model on the Training set","26a88d67":"#### Training the Kernel Support Vector Machine Classification model on the Training set","7e467888":"Here, we will be answering the following questions:\n1. Did gender play a role on survival?\n2. Did passengers with more bookings (sibsp + parch) survived?\n3. Understanding the price of ticket with respect to age? (Scatterplot between age and fare)\n4. Did ticket fare play a role on a passenger's survival?\n5. Distribution of the variable - Embarked\n6. Which value of the column - Embarked had a better survival rate?\n7. Which value of the column - Pclass had a better survival rate?\n8. Which value of the column - Ticket had a better survival rate?\n9. Correlation between the metrics.","1964e46b":"On observing the above scatterplot, we can say that most of the passengers have a ticket price less than 50.","342894f7":"#### Training the Decision Tree Classification model on the Training set","e29b5065":"#### 6. Which value of the column - Embarked had a better survival rate?","081ed9ee":"## Titanic - Machine Learning From Disaster","22ed4cc5":"On observing the above graph, we can say that the survival rate in Female is higher than that in Male.","628dc08c":"#### Importing the libraries","53249f9e":"#### 2. Did passengers with more bookings (sibsp + parch) survived?"}}