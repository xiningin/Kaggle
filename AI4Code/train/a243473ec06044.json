{"cell_type":{"702720b1":"code","38dee6d9":"code","9ca3a274":"code","0138be0b":"code","8c35b71f":"code","f76e4981":"code","f73c86c7":"code","9b1afead":"code","e27bc2ff":"code","06400280":"code","fee557d8":"code","42d151b4":"code","beee472e":"code","76993867":"code","e620ae6f":"code","90891426":"code","0f021746":"code","c9f384c6":"code","26834267":"code","e1d12c23":"code","fb7a34dc":"code","af196cc3":"markdown","c859d292":"markdown","0e105665":"markdown","281deaeb":"markdown","61c7c609":"markdown","2d7c555a":"markdown","f6c5ad05":"markdown","947314a9":"markdown","266e8bb7":"markdown","31778ef9":"markdown","b33699f6":"markdown","033cd3b5":"markdown","f0726da9":"markdown","e7990825":"markdown","a3634749":"markdown","8ec107aa":"markdown"},"source":{"702720b1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport xgboost as xgb","38dee6d9":"import os\nprint(os.listdir(\"..\/input\"))","9ca3a274":"df_train=pd.read_csv(\"..\/input\/train.csv\")\ndf_test=pd.read_csv('..\/input\/test.csv')","0138be0b":"print('Size of the training set',df_train.shape)\nprint('Size of the test set',df_test.shape)\nmissing =  pd.DataFrame(data = {'Missing in training set': df_train.isnull().sum(), \n                        'Missing in test set': df_test.isnull().sum()})\nmissing","8c35b71f":"s = df_train.shape[0]\ny = df_train['Survived']\ndf_train = df_train.drop(columns = 'Survived')\n\nX = pd.concat([df_train, df_test], ignore_index=True)","f76e4981":"X['Title'] = X['Name'].apply(lambda x: x.partition(',')[-1].split()[0])\n\nX['Title'][(X['Title'] != 'Mr.') & (X['Title'] != 'Mrs.')  &\n           (X['Title'] != 'Master.') & (X['Title'] != 'Miss.') & (X['Sex'] == 'male')] = 'Mr.'\nX['Title'][(X['Title'] != 'Mr.') & (X['Title'] != 'Mrs.')  & \n           (X['Title'] != 'Master.') & (X['Title'] != 'Miss.') & (X['Sex'] == 'female')] = 'Mrs.'","f73c86c7":"ind=X['Cabin'][X['Cabin'].notnull()].index.values\nX['Cabin'][ind] = X['Cabin'][ind].apply(lambda x:  x[0])\n\ngroups = X.groupby('Ticket').count()\ngroups_with_cabin = groups[groups['Cabin'] != 0]\ngroups_with_cabin['Diff'] = groups_with_cabin['Fare'] - groups_with_cabin['Cabin']\n\nind=groups_with_cabin[(groups_with_cabin['Diff']>0)].index\ncabins = X[(X['Ticket'].isin(ind))].sort_values('Ticket')\ncabins = cabins.groupby('Ticket')['Cabin'].transform(lambda x: x.fillna(x.mode()[0]))\nX.set_value(cabins.index,'Cabin', cabins.values)\nX['Cabin'].fillna('N',inplace=True);","9b1afead":"boy = np.round(X['Age'][(X['Name'] == 'Master.')].mean(),decimals = 1)\nX['Age'][(X['Name'] == 'Master.')] = X['Age'][(X['Name'] == 'Master.')].fillna(boy)\n\ngirl = np.round(X['Age'][(X['Parch'] > 0) & (X['Name'] == 'Miss.')].mean(),decimals = 1)\nX['Age'][(X['Parch'] > 0) & (X['Name'] == 'Miss.')] = X['Age'][(X['Parch'] > 0) & (X['Name'] == 'Miss.')].fillna(girl)\n       \nunmarried_female = np.round(X['Age'][(X['Parch'] == 0) & (X['Name'] == 'Miss.')].mean(),decimals = 1)\nX['Age'][(X['Parch'] == 0) & (X['Name'] == 'Miss.')] = X['Age'][(X['Parch'] == 0) & (X['Name'] == 'Miss.')].fillna(unmarried_female)\n        \n#Fill the remaining missing values with the average age of male and female pessangers\nmean_age = X[(X['Name'] != 'Miss.') & (X['Name'] != 'Master.')].groupby('Sex').mean()\nX['Age'][(X['Sex'] == 'female')] = X['Age'][(X['Sex'] == 'female')].fillna(np.round(mean_age['Age']['female'], decimals = 1))\nX['Age'][(X['Sex'] == 'male')] = X['Age'][(X['Sex'] == 'male')].fillna(np.round(mean_age['Age']['male'], decimals = 1))","e27bc2ff":"X['Embarked'].fillna(X['Embarked'].value_counts().idxmax(),inplace=True)","06400280":"X['Fare'] = X.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.mean()))","fee557d8":"X.drop(columns=['PassengerId', 'Name','Cabin', 'Ticket'], inplace=True)","42d151b4":"X.head()","beee472e":"def label_encoding(df):\n    obj = [var for var in df.columns if df[var].dtype=='object']\n    for c in obj:\n        le=LabelEncoder()\n        df[c]=le.fit_transform(df[c])\n    return df","76993867":"X = label_encoding(X)\nX.head(3)","e620ae6f":"X_train = X.iloc[:s, :]\nX_test = X.iloc[s:, :]","90891426":"param = {'max_depth': 4, 'learning_rate': 0.01, 'n_estimators': 500, 'objective': 'binary:logistic',\n        'reg_alpha': 0, 'reg_lambda': 0, 'seed': 1}\n\nxgb_model = xgb.XGBClassifier(**param)\n\n# fit model on all training data\nxgb_model.fit(X_train, y)\nsc = cross_val_score(xgb_model, X_train, y, cv = 10)\nprint(\"Accuracy: %.2f%%\" % (sc.mean() * 100.0))\n\n# Fit model using each importance as a threshold\nthresholds = np.sort(xgb_model.feature_importances_)\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(xgb_model, threshold=thresh, prefit = True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model =xgb.XGBClassifier(**param)\n    sc = cross_val_score(selection_model, select_X_train, y, cv = 10) \n    print(\"Thresh=%.3f, number of features=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], sc.mean()*100.0))","0f021746":"feat_imp = pd.DataFrame(data = {'Feature': X_train.columns, 'Importance': xgb_model.feature_importances_})\nfeat_imp.sort_values(by=['Importance'], ascending = False, inplace = True)\nselected_features = feat_imp['Feature'][:5].values\nprint(selected_features)\nfeat_imp","c9f384c6":"X_train = X_train[selected_features]\nX_test = X_test[selected_features] ","26834267":"xgb_model.fit(X_train, y)\nsurvival = xgb_model.predict(X_test)","e1d12c23":"submission = pd.DataFrame(data = {'PassengerId': df_test['PassengerId'], 'Survived': survival})\nsubmission.head(5)","fb7a34dc":"submission.to_csv('mysubmission.csv', index = False)","af196cc3":"There is one last transformation needed before we can train our model. There are a couple of features that contain string values and usually ML models cannot work with them. We need to transform them in numeric values either by label encoding or one-hot-encoding (OHE). As I am planning to use a tree based model (XGboost) label encoding is very well suited for the job. (I have also tried OHE but it gave me a smaller accuracy).  ","c859d292":"### Feature engineering","0e105665":"I have filled the missing values in Embarked by the most popular value and the missing values in Fare as the average as a function of passenger class. ","281deaeb":"I have used the XGBoost classifier to obtain the feature importance as assigned by this model. Then the features are sorted by importance and the model is trained and cross-validated on smaller and smaller feature number of features. ","61c7c609":"We observe that the maximum accuracy is not obtained using all the features, but using the 5 most important features.  \n\nFirst I stor the feature names and the importance given to them by XGBoost in a dataframe and sort it in a descending order, so the most important feature is on top. Then I select the number of features that give the highest accuracy, 5.","2d7c555a":"Now remove the features that I do not need.","f6c5ad05":"Take only the selected features from the train and test sets and train the model again, then predict the survival in the test set.","947314a9":"As a last step before the modeling we split the dataset into the training and the test set. ","266e8bb7":"This is my solution for this Kaggle competition. To keep the kernel short I have removed the EDA, because that was already presented in other kernels. \n\nI use a simple XGBoost model. I use feature_selection from scikit-learn to improve the accuracy of the model.","31778ef9":"Extract the title Mr., Mrs. etc. from the name. I have kept the titles Mr., Mrs., Master. and Miss. The other titles I have renamed according to the gender either as Mr. or Mrs. (first I have checked that there are no children in these categories). ","b33699f6":"Next I have addressed the missing values in the Cabin feature. First I have extracted the deck from the as being the first character in the Cabin feature. Then I have looked at groups of people that have bought the tickets together and made the assumption that people who travel together most likely stay close to each other on the same deck (I have checked and in the available data this is true except for one case). Unfortunately the wast majority of the Cabin information is missing so I have managed to recover only a handful of values. I have assigned the deck value N to the rest of the missing values. (After some analysis I have decided to completely remove the Cabin feature)","033cd3b5":"# Titanic: Machine Learning from Disaster","f0726da9":"Concatenate the train and test sets and save the target variable in y.","e7990825":"As most adult males did not survive but young boys did, I have calculated the average age of children and adults, by grouping the age according to title, Master. for children, Mr. for adults.  For female passengers it is more complicated as the title 'Miss.' belongs to both girls and unmarried women. Here I made the assumption that the if a Miss. was traveling with her parents it was a child if not it was an adult. This is approximately true, but there were also adults traveling with there parents and children traveling with companions, who were not listed in the ParCh feature. \n\nNext I have filled the remaining missing values with the average age of male and female passengers. ","a3634749":"\n### Missing values","8ec107aa":"### Modeling"}}