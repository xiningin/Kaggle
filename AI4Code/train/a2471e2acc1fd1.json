{"cell_type":{"2187dbab":"code","1b8de099":"code","61f73fde":"code","54d9144c":"code","ffede24f":"code","300dff56":"code","c03f5187":"code","4e74eabb":"code","c8781067":"code","3934235f":"code","cd0ff3a4":"code","2cb7a6ff":"code","6f8f8507":"code","e123b54b":"code","870d91f3":"code","4796ec95":"code","e6cb6e6d":"code","b6aee089":"code","b2b36c7b":"code","9056ea42":"code","23575bcf":"code","d992cc3f":"code","176dae34":"code","bad402c5":"code","90639401":"code","00bf4416":"code","d0dc57f1":"code","d31042c2":"code","6d8cc4db":"code","94603ffd":"code","c493b932":"code","34eddf8a":"code","f8be1119":"code","e31f7e6e":"code","4b1469bc":"code","45871055":"code","fb5d88f4":"code","8834e41b":"code","6cd794b2":"code","f3988741":"code","5520c0da":"code","f4178975":"code","6bd38b1b":"code","be01b176":"code","14c564d0":"code","b12d6670":"code","958ec498":"code","24e1a046":"code","3959eb82":"code","c4a4f0e8":"code","1b1e21cf":"code","36ba3c07":"code","1f0ff50b":"code","803e59aa":"code","2622cacd":"code","a7c41e6f":"code","d47ec4e0":"code","c475a7b5":"code","31946e15":"code","6332d595":"code","f41bda41":"code","8e401d71":"code","2f98cdd1":"code","4d4cb8fe":"code","79268814":"code","16099b60":"code","7196a569":"code","e263ad46":"code","c37a85a6":"code","ec0d75ec":"code","6407efe7":"code","2aaeeb2a":"code","63a6c770":"code","61589236":"code","61310273":"code","142f1605":"code","c0a1fd2a":"code","dca3dfd3":"code","a9b9169a":"code","2e8f0350":"code","fdec2a4d":"code","e6d336f8":"code","8153415b":"code","60131654":"code","6b30579c":"code","26f0cd72":"code","a7ea53e0":"code","acff3907":"code","bd26c294":"code","be041c52":"code","be510b33":"code","3bc50093":"code","567609f9":"code","56670555":"code","effe601d":"code","9d649ef7":"code","6f4ee74e":"code","cb49b18f":"code","7d94b71d":"code","5f1f1043":"code","ad694677":"code","06744415":"code","26bbf7d4":"code","ebf19ef4":"code","b0b5ea55":"code","d4a48486":"code","22fceb72":"code","7609523a":"code","932f8e92":"code","e178b2cb":"code","9e3b6043":"code","dc592ffe":"code","09ead104":"code","ce861c9d":"code","4bad6c98":"code","2bfc8124":"code","6e87e427":"code","226f02ea":"code","ff8a7e5d":"code","490fc9ed":"code","fdb0453f":"code","8d1e8d87":"code","f21d50bb":"code","3c372c3b":"code","4b337c0e":"code","18ee542a":"code","f3de3510":"code","dfa1dfcf":"code","8496b477":"code","404836ae":"code","e4d41d86":"code","b3f18e07":"code","b53c49f7":"code","b26b83bc":"code","757d19bb":"code","8a735bec":"code","9d84bbf9":"code","ead929ae":"code","cca26ec2":"code","1604900e":"code","4f5f4ad7":"code","baf242ad":"code","ec552a73":"code","0c7894db":"code","0c640fe5":"code","56ff95e2":"code","ff35b194":"code","acf3a020":"code","f540f18f":"code","bba5456d":"code","9d6cc12a":"code","bde98d06":"code","9f478497":"code","f4a1a1e7":"code","e1d7422f":"code","06b928e6":"code","d25bdb66":"code","9f2c4d3f":"code","daee8c4c":"code","ad30067a":"code","65f0d777":"markdown","03b7e9bf":"markdown","8d05b8a0":"markdown","a3cd45de":"markdown","440f84d4":"markdown","8f5f4a0c":"markdown","29630d74":"markdown","be77c2b8":"markdown","52d3d740":"markdown","cb4d40f2":"markdown","94bdf1b5":"markdown","28aa94d7":"markdown","9db2fce0":"markdown","11701208":"markdown","9d6d4ac3":"markdown","86f8c5cd":"markdown","b763b182":"markdown","6e85d415":"markdown","22e4d71b":"markdown","43285efe":"markdown","19dfbb57":"markdown","55341910":"markdown","0c81c9b2":"markdown","7c2fc958":"markdown","87c1e439":"markdown","c08abf66":"markdown","c3eacd0d":"markdown","5e212382":"markdown","22ed7f68":"markdown","f798867e":"markdown"},"source":{"2187dbab":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR\nimport xgboost\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","1b8de099":"from warnings import filterwarnings\nfilterwarnings('ignore')","61f73fde":"hitters = pd.read_csv('..\/input\/hitters\/Hitters.csv')\nhitters.head()","54d9144c":"#Exploratory Data Analysis\n#Structural information of the data set\nhitters.info()","ffede24f":"hitters.isnull().sum()","300dff56":"dummies = pd.get_dummies(hitters[['League', 'Division', 'NewLeague']]) \ndummies.head()","c03f5187":"X_ = hitters.drop(['League', 'Division', 'NewLeague'], axis=1).astype('float64') \n\nhitters = pd.concat([X_, dummies[['League_N', 'Division_W', 'NewLeague_N']]], axis=1) \n\nhitters.info()","4e74eabb":"hitters.describe().T","c8781067":"df_1 = hitters.dropna()\ndf_1.head()","3934235f":"df_1.info()","cd0ff3a4":"df = hitters.copy()\ndf['Salary'].fillna(df['Salary'].mean(), inplace = True) \ndf_2 = df.copy()\ndf_2.info()","2cb7a6ff":"hitters.head()","6f8f8507":"null = hitters[hitters['Salary'].isnull()]\n# Selection of observations with missing data\nnull.head()","e123b54b":"df = hitters.dropna() #Delete observations with missing data\nX_train = df.drop('Salary', axis = 1) #Train set definition\nX_train.head()","870d91f3":"y_train = df[['Salary']] #Determination of the dependent variable of the train set\ny_train.head()","4796ec95":"X_test = null.drop('Salary', axis = 1) #Defining observations with missing data in the data set as a test set\nX_test.head()","e6cb6e6d":"gbm_model = GradientBoostingRegressor().fit(X_train, y_train)\ngbm_model_pred_test = gbm_model.predict(X_test)\ngbm_model_pred_test","b6aee089":"X_test['Salary'] = gbm_model_pred_test","b2b36c7b":"df_3 = pd.concat([df, X_test], ignore_index = True)\ndf_3.head()","9056ea42":"df_3.info()","23575bcf":"df_3.describe().T","d992cc3f":"df_3.info()","176dae34":"from sklearn.neighbors import LocalOutlierFactor\nclf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\nclf.fit_predict(df_3)\ndf_scores = clf.negative_outlier_factor_\ndf_scores[0:20]","bad402c5":"np.sort(df_scores)","90639401":"np.sort(df_scores)[16]","00bf4416":"threshold_value = np.sort(df_scores)[16]\nthreshold_value","d0dc57f1":"outlier_df = df_scores > threshold_value","d31042c2":"df_3[df_scores == threshold_value]\n","6d8cc4db":"pressure_value = df_3[df_scores == threshold_value]","94603ffd":"outlier = df_3[~outlier_df] ","c493b932":"outlier.to_records(index=False)","34eddf8a":"res = outlier.to_records(index=False)","f8be1119":"res[:] = pressure_value.to_records(index = False)","e31f7e6e":"outlier = pd.DataFrame(res, index = df_3[~outlier_df].index)\noutlier.describe().T","4b1469bc":"n_outlier = df_3[outlier_df]\nn_outlier.describe().T","45871055":"df_4 = pd.concat([n_outlier, outlier], ignore_index = True)\ndf_4.describe().T","fb5d88f4":"df_3.info()","8834e41b":"df_5 = df_3[df_scores > threshold_value]\ndf_5.info()","6cd794b2":"df.info()","f3988741":"from sklearn.neighbors import LocalOutlierFactor\nclf = LocalOutlierFactor(n_neighbors = 20, contamination = 0.1)\nclf.fit_predict(df)\ndf6_scores = clf.negative_outlier_factor_\ndf6_scores[0:20]","5520c0da":"np.sort(df6_scores)\n","f4178975":"np.sort(df6_scores)[8]","6bd38b1b":"threshold_value6 = np.sort(df6_scores)[8]\nthreshold_value6","be01b176":"outlier_df6 = df6_scores > threshold_value6\noutlier_df6","14c564d0":"df[df6_scores == threshold_value6]","b12d6670":"pressure_value6 = df[df6_scores == threshold_value6]","958ec498":"outlier6 = df[~outlier_df6] \n","24e1a046":"outlier6.to_records(index=False)","3959eb82":"res6 = outlier6.to_records(index=False)\n","c4a4f0e8":"res6[:] = pressure_value6.to_records(index = False)\n","1b1e21cf":"n_outlier6 = df[outlier_df6]\nn_outlier6.describe().T","36ba3c07":"outlier6 = pd.DataFrame(res6, index = df[~outlier_df6].index)\noutlier6.describe().T","1f0ff50b":"df_6 = pd.concat([n_outlier6, outlier6], ignore_index = True)\ndf_6.describe().T","803e59aa":"df_7 = n_outlier6\ndf_7.info()","2622cacd":"df_8 = hitters.copy()\ndf_8.info()","a7c41e6f":"cat_df = df_8.select_dtypes(include=[\"uint8\"])\ncat_df.head()","d47ec4e0":"print(cat_df.League_N.unique())\nprint(cat_df[\"League_N\"].value_counts().count())\nprint(cat_df[\"League_N\"].value_counts())\nprint(df_8[\"League_N\"].value_counts().plot.barh())\ndf_8.groupby('League_N')['Salary'].mean()","c475a7b5":"print(cat_df.Division_W.unique())\nprint(cat_df[\"Division_W\"].value_counts().count())\nprint(cat_df[\"Division_W\"].value_counts())\nprint(df_8[\"Division_W\"].value_counts().plot.barh())\ndf_8.groupby('Division_W')['Salary'].mean()","31946e15":"print(cat_df.NewLeague_N.unique())\nprint(cat_df[\"NewLeague_N\"].value_counts().count())\nprint(cat_df[\"NewLeague_N\"].value_counts())\nprint(df_8[\"NewLeague_N\"].value_counts().plot.barh())\ndf_8.groupby('NewLeague_N')['Salary'].mean()","6332d595":"Experience = []\nfor ex in df_8['Years']:\n    if ex < 5:\n        Experience.append(1)\n    elif (ex >= 5) & (ex < 10):\n        Experience.append(2)\n    elif (ex >= 10) & (ex < 15):\n        Experience.append(3)\n    elif (ex >= 15) & (ex < 20):\n        Experience.append(4)\n    else:\n        Experience.append(5)\ndf_8['Experience'] = Experience","f41bda41":"df_8.groupby(['League_N', 'Division_W', 'NewLeague_N'])['Salary'].mean()","8e401d71":"df_8.groupby(['League_N', 'Division_W', 'NewLeague_N', 'Experience'])['Salary'].mean()","2f98cdd1":"df_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 1), \"Salary\"] = 145.961538\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 2), \"Salary\"] = 774.434536\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 3), \"Salary\"] = 918.073533\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 4), \"Salary\"] = 614.375000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 2), \"Salary\"] = 850.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 3), \"Salary\"] = 833.333333\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 1), \"Salary\"] = 203.821429\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 2), \"Salary\"] = 528.108696\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 3), \"Salary\"] = 786.916700\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 4), \"Salary\"] = 479.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 1), \"Salary\"] = 96.666667\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 0) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 3), \"Salary\"] = 825.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 1), \"Salary\"] = 70.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 2), \"Salary\"] = 525.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 3), \"Salary\"] = 500.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 4), \"Salary\"] = 1050.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 1), \"Salary\"] = 313.753320\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 2), \"Salary\"] = 776.095190\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 3), \"Salary\"] = 949.010143\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 0) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 4), \"Salary\"] = 486.111000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 1), \"Salary\"] = 565.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 2), \"Salary\"] = 405.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 0) & (df_8['Experience'] == 3), \"Salary\"] = 250.000000\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 1), \"Salary\"] = 188.138889\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 2), \"Salary\"] = 538.114053\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 3), \"Salary\"] = 723.452429\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 4), \"Salary\"] = 763.666600\ndf_8.loc[(df_8[\"Salary\"].isnull()) & (df_8[\"League_N\"] == 1) & (df_8['Division_W'] == 1) & (df_8[\"NewLeague_N\"] == 1) & (df_8['Experience'] == 5), \"Salary\"] = 475.000000\n","4d4cb8fe":"df_8.info()","79268814":"df_8['AtBat_rate'] = df_8[\"CAtBat\"] \/ df_8[\"Years\"]\ndf_8['Hits_rate'] = df_8[\"CHits\"] \/ df_8[\"Years\"]\ndf_8['HmRun_rate'] = df_8[\"CHmRun\"] \/ df_8[\"Years\"]\ndf_8['Runs_rate'] = df_8[\"CRuns\"] \/ df_8[\"Years\"]\ndf_8['RBI_rate'] = df_8[\"CRBI\"] \/ df_8[\"Years\"]\ndf_8['Walks_rate'] = df_8[\"CWalks\"] \/ df_8[\"Years\"]\n\ndf_8['1986_AtBat_rate'] = df_8[\"AtBat\"] \/ df_8[\"CAtBat\"]\ndf_8['1986_Hits_rate'] = df_8[\"Hits\"] \/ df_8[\"CHits\"]\ndf_8['1986_HmRun_rate'] = df_8[\"HmRun\"] \/ df_8[\"CHmRun\"]\ndf_8['1986_Runs_rate'] = df_8[\"Runs\"] \/ df_8[\"CRuns\"]\ndf_8['1986_RBI_rate'] = df_8[\"RBI\"] \/ df_8[\"CRBI\"]\ndf_8['1986_Walks_rate'] = df_8[\"Walks\"] \/ df_8[\"CWalks\"]","16099b60":"df_8.info()","7196a569":"df_8 = df_8.dropna()","e263ad46":"df_8.info()","c37a85a6":"def compML(df, y, alg):\n    #train-test distinction\n    y = df[y]\n    X = df.drop('Salary', axis=1)\n    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.20,random_state=46)\n    #modeelling\n    model = alg().fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n    model_name = alg.__name__\n    print(\"  for data set  \", model_name, \" Model Test Error: \",RMSE)","ec0d75ec":"models = [LinearRegression,\n          Ridge,\n          Lasso,\n          ElasticNet,\n          LGBMRegressor, \n          XGBRegressor, \n          GradientBoostingRegressor, \n          RandomForestRegressor, \n          DecisionTreeRegressor,\n          MLPRegressor,\n          KNeighborsRegressor, \n          SVR]","6407efe7":"for i in models:\n    compML(df_1, \"Salary\", i)","2aaeeb2a":"for i in models:\n    compML(df_2, \"Salary\", i)","63a6c770":"for i in models:\n    compML(df_3, \"Salary\", i)","61589236":"for i in models:\n    compML(df_4, \"Salary\", i)","61310273":"for i in models:\n    compML(df_5, \"Salary\", i)","142f1605":"for i in models:\n    compML(df_6, \"Salary\", i)","c0a1fd2a":"for i in models:\n    compML(df_7, \"Salary\", i)","dca3dfd3":"for i in models:\n    compML(df_8, \"Salary\", i)","a9b9169a":"df_4.head()","2e8f0350":"y = df_4['Salary']\nX = df_4.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","fdec2a4d":"knn_model = KNeighborsRegressor().fit(X_train, y_train)","e6d336f8":"knn = KNeighborsRegressor()\nknn_params = {\"n_neighbors\": np.arange(1,30,1)}","8153415b":"knn_cv_model = GridSearchCV(knn, knn_params, cv = 10).fit(X_train, y_train)","60131654":"knn_cv_model.best_params_","6b30579c":"knn_tuned = KNeighborsRegressor(n_neighbors = knn_cv_model.best_params_[\"n_neighbors\"]).fit(X_train, y_train)","26f0cd72":"knn_tuned_y_pred = knn_tuned.predict(X_test)","a7ea53e0":"knn_tuned_RMSE = np.sqrt(mean_squared_error(y_test, knn_tuned_y_pred))\nknn_tuned_RMSE","acff3907":"df_4.head()","bd26c294":"y = df_4['Salary']\nX = df_4.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","be041c52":"svr_model = SVR(\"linear\") ","be510b33":"svr_params = {\"C\": [0.1,0.5,1,3]}","3bc50093":"svr_cv_model = GridSearchCV(svr_model, svr_params, cv = 5, verbose = 2, n_jobs = -1).fit(X_train, y_train)","567609f9":"svr_cv_model.best_params_","56670555":"svr_tuned = SVR(\"linear\", C = 3).fit(X_train, y_train)","effe601d":"svr_model_y_pred = svr_tuned.predict(X_test)","9d649ef7":"svr_model_tuned_RMSE = np.sqrt(mean_squared_error(y_test, svr_model_y_pred))\nsvr_model_tuned_RMSE","6f4ee74e":"df_4.head()","cb49b18f":"y = df_4['Salary']\nX = df_4.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","7d94b71d":"scaler = StandardScaler()","5f1f1043":"scaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)","ad694677":"scaler.fit(X_test)\nX_test_scaled = scaler.transform(X_test)","06744415":"mlp_model = MLPRegressor().fit(X_train_scaled, y_train)","26bbf7d4":"mlp_params = {\"alpha\": [0.1, 0.01, 0.02, 0.001, 0.0001], \n             \"hidden_layer_sizes\": [(10,20), (5,5), (100,100)]}","ebf19ef4":"mlp_cv_model = GridSearchCV(mlp_model, mlp_params, cv = 10, verbose = 2, n_jobs = -1).fit(X_train_scaled, y_train)","b0b5ea55":"mlp_cv_model.best_params_","d4a48486":"mlp_tuned = MLPRegressor(alpha = 0.001, hidden_layer_sizes = (100,100)).fit(X_train_scaled, y_train)","22fceb72":"mlp_y_pred = mlp_tuned.predict(X_test_scaled)","7609523a":"mlp_tuned_RMSE = np.sqrt(mean_squared_error(y_test, mlp_y_pred))\nmlp_tuned_RMSE","932f8e92":"df_4.head()","e178b2cb":"y = df_4['Salary']\nX = df_4.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","9e3b6043":"cart_model = DecisionTreeRegressor()","dc592ffe":"cart_model.fit(X_train, y_train)","09ead104":"cart_params = {\"max_depth\": [2,3,4,5,10,20],\n              \"min_samples_split\": [2,10,5,30,50,10]}","ce861c9d":"cart_cv_model = GridSearchCV(cart_model, cart_params, cv = 10, verbose = 2, n_jobs = -1).fit(X_train, y_train)","4bad6c98":"cart_cv_model.best_params_","2bfc8124":"cart_tuned = DecisionTreeRegressor(max_depth = 4, min_samples_split = 2).fit(X_train, y_train)","6e87e427":"cart_model_y_pred = cart_tuned.predict(X_test)\ncart_tuned_RMSE = np.sqrt(mean_squared_error(y_test, cart_model_y_pred))\ncart_tuned_RMSE","226f02ea":"df_4.head()","ff8a7e5d":"y = df_4['Salary']\nX = df_4.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","490fc9ed":"rf_model = RandomForestRegressor(random_state = 46).fit(X_train, y_train)\nrf_model","fdb0453f":"rf_params = {\"max_depth\": [5,8,10],\n            \"max_features\": [2,5,10],\n            \"n_estimators\": [200, 500, 1000, 2000],\n            \"min_samples_split\": [2,10,80,100]}","8d1e8d87":"rf_cv_model = GridSearchCV(rf_model, rf_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)","f21d50bb":"rf_cv_model.best_params_","3c372c3b":"rf_model = RandomForestRegressor(random_state = 46, \n                                 max_depth = 8,\n                                max_features = 5,\n                                min_samples_split = 2,\n                                 n_estimators = 500)\nrf_tuned = rf_model.fit(X_train, y_train)","4b337c0e":"rf_y_pred = rf_tuned.predict(X_test)\nrf_tuned_RMSE = np.sqrt(mean_squared_error(y_test, rf_y_pred))\nrf_tuned_RMSE","18ee542a":"rf_tuned.feature_importances_*100","f3de3510":"Importance = pd.DataFrame({'Importance':rf_tuned.feature_importances_*100}, \n                          index = X_train.columns)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","dfa1dfcf":"df_4.head()","8496b477":"y = df_4['Salary']\nX = df_4.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","404836ae":"gbm_model = GradientBoostingRegressor().fit(X_train, y_train)\ngbm_model","e4d41d86":"gbm_params = {\"learning_rate\": [0.001,0.1,0.01],\n             \"max_depth\": [3,5,8],\n             \"n_estimators\": [100,200,500],\n             \"subsample\": [1,0.5,0.8],\n             \"loss\": [\"ls\",\"lad\",\"quantile\"]}","b3f18e07":"gbm_cv_model = GridSearchCV(gbm_model, \n                            gbm_params, \n                            cv = 10, \n                            n_jobs=-1, \n                            verbose = 2).fit(X_train, y_train)","b53c49f7":"gbm_cv_model.best_params_","b26b83bc":"gbm_tuned = GradientBoostingRegressor(learning_rate = 0.1,\n                                     loss = \"lad\",\n                                     max_depth = 3,\n                                     n_estimators = 100,\n                                     subsample = 1).fit(X_train, y_train)","757d19bb":"gbm_tuned_y_pred = gbm_tuned.predict(X_test)\ngbm_tuned_RMSE = np.sqrt(mean_squared_error(y_test, gbm_tuned_y_pred))\ngbm_tuned_RMSE","8a735bec":"Importance = pd.DataFrame({'Importance':gbm_tuned.feature_importances_*100}, \n                          index = X_train.columns)\n\n\nImportance.sort_values(by = 'Importance', \n                       axis = 0, \n                       ascending = True).plot(kind = 'barh', \n                                              color = 'r', )\n\nplt.xlabel('Variable Importance')\nplt.gca().legend_ = None","9d84bbf9":"y = df_4['Salary']\nX = df_4.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","ead929ae":"xgb = XGBRegressor()\nxgb","cca26ec2":"xgb_params = {\"learning_rate\": [0.1,0.01,0.5],\n             \"max_depth\": [2,3,4,5,8],\n             \"n_estimators\": [100,200,500,1000],\n             \"colsample_bytree\": [0.4,0.7,1]}","1604900e":"xgb_cv_model  = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X_train, y_train)","4f5f4ad7":"xgb_cv_model.best_params_","baf242ad":"xgb_tuned = XGBRegressor(colsample_bytree = 0.4, \n                         learning_rate = 0.1, \n                         max_depth = 4, \n                         n_estimators = 100).fit(X_train, y_train)","ec552a73":"xgb_tuned_y_pred = xgb_tuned.predict(X_test)\nxgb_tuned_RMSE = np.sqrt(mean_squared_error(y_test, xgb_tuned_y_pred))\nxgb_tuned_RMSE","0c7894db":"df_4.head()","0c640fe5":"y = df_4['Salary']\nX = df_4.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","56ff95e2":"lgb_model = LGBMRegressor()\nlgb_model","ff35b194":"lgbm_params = {\"learning_rate\": [0.01, 0.1, 0.5, 1],\n              \"n_estimators\": [20,40,100,200,500,1000],\n              \"max_depth\": [1,2,3,4,5,6,7,8,9,10]}","acf3a020":"lgbm_cv_model = GridSearchCV(lgb_model, \n                             lgbm_params, \n                             cv = 10, \n                             n_jobs = -1, \n                             verbose =2).fit(X_train, y_train)","f540f18f":"lgbm_cv_model.best_params_","bba5456d":"lgbm_tuned = LGBMRegressor(learning_rate = 0.1, \n                          max_depth = 2, \n                          n_estimators = 200).fit(X_train, y_train)","9d6cc12a":"lgbm_tuned_y_pred = lgbm_tuned.predict(X_test)\nlgbm_tuned_RMSE = np.sqrt(mean_squared_error(y_test, lgbm_tuned_y_pred))\nlgbm_tuned_RMSE","bde98d06":"cat_df = df_4","9f478497":"y = cat_df['Salary']\nX = cat_df.drop('Salary', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)","f4a1a1e7":"catb_model = CatBoostRegressor()","e1d7422f":"catb_params = {\"iterations\": [200,500,100],\n              \"learning_rate\": [0.01,0.1],\n              \"depth\": [3,6,8]}","06b928e6":"catb_cv_model = GridSearchCV(catb_model, \n                           catb_params, \n                           cv = 5, \n                           n_jobs = -1, \n                           verbose = 2).fit(X_train, y_train)","d25bdb66":"catb_cv_model.best_params_","9f2c4d3f":"catb_tuned = CatBoostRegressor(depth = 6, iterations = 500, learning_rate = 0.01).fit(X_train, y_train)","daee8c4c":"catb_tuned_y_pred = catb_tuned.predict(X_test)","ad30067a":"catb_tuned_RMSE = np.sqrt(mean_squared_error(y_test, catb_tuned_y_pred))\ncatb_tuned_RMSE","65f0d777":"## 3. We will create different data sets for different scenarios that we will apply for salary estimation.\n\n### 3.1. We create the data set 'df_1' by simply deleting the missing data without making any changes to the variables:","03b7e9bf":"### 3.5. 'Df_5' is Generated by Filling the Missing Data with Predictions and Deleting the Threshold Data:","8d05b8a0":"# CONCLUSION:\n\n#### In the 'Salary Estimation' study on the 'Hitters' data set, a total of 8 data sets were created:\n\n#### df_1: The observations with missing data in the Hitters data set were created by deleting.\n\n#### df_2: The missing data in the Hitters dataset was created by filling the average of the 'Salary' variable in which they were found.\n\n#### df_3: Missing data in Hitters dataset was estimated and filled with the Gradient Boosting Machine model.\n\n#### df_4: Outliers determined by LocalOutlierFactor in the df_3 dataset were created by suppressing.\n\n#### df_5: The outliers determined by the Local Outlier Factor in the df_3 dataset were deleted.\n\n#### df_6: Observations with missing data in the Hitters data set were created by deleting the outliers.\n\n#### df_7: It was created by deleting missing data and outliers from the Hitters data set.\n\n#### df_8: It was created by adding new variables to the data set. The values in the Years variable were divided, the 'Experience' variable was created, and the annual average of the players' performances and the ratio of their performances in 1986 to all their careers were added as variables.\n\n\n\n## Then a function was written for all 'Regression Models' and estimation was performed on all datasets with individual models.\n\n## Finally, model tuning processes were made with Hyperparameter optimizations and final models were established.Finally, model tuning processes were made with Hyperparameter optimizations and final models were established.","a3cd45de":"## Variable Severity:","440f84d4":"### 3.3.  Missing Data is Filled with Gradient Boosting Regression Estimation Results and data set named 'df_3' is created:","8f5f4a0c":"## 6.7. XGBoost\n\n\n","29630d74":"## Variable Severity","be77c2b8":"It was observed that there were three 'Object' type variables in the data set and there were 59 missing data in the 'Salary' variable.\n\nFirst of all, we get rid of the variables that are seen as 'Object' with the 'get.dummies' operation.","52d3d740":"### 4.2. Adding Variables:\n\nThe dataset contains data from the players in 1986 and throughout their careers and how many years of experience they have had. We add the annual average of these data and the ratio of the data in 1986 to the overall performance.","cb4d40f2":"## 6.9.  CatBoost","94bdf1b5":"### 3.6. 'Df_6' is Created by Deleting Missing Data and Pressure Outlier Data:","28aa94d7":"\n# Salary Prediction on Hitters Data Set:\n\n\n## AIM\nMy aim in this study is to set up machine learning models for the Hitters data set and minimize error scores. The works I have done for this purpose are as follows:\n\n## Hitters: Baseball Data\n\n### Description\nMajor League Baseball Data from the 1986 and 1987 seasons.Major League Baseball Data from the 1986 and 1987 seasons.\n    \n### Format\nA data frame with 322 observations of major league players on the following 20 variables.\n\n### Variables\n\n* AtBat  : Number of times at bat in 1986\n* Hits    : Number of hits in 1986\n* HmRun   : Number of home runs in 1986\n* Runs    : Number of runs in 1986\n* RBI     : Number of runs batted in in 1986\n* Walks   : Number of walks in 1986\n* Years   : Number of years in the major leagues\n* CAtBat  : Number of times at bat during his career\n* CHits   : Number of hits during his career\n* CHmRun  : Number of home runs during his career\n* CRuns   : Number of runs during his career\n* CRBI    : Number of runs batted in during his career\n* CWalks  : Number of walks during his career\n* League  : A factor with levels A and N indicating player's league at the end of 1986\n* Division: A factor with levels E and W indicating player's division at the end of 1986\n* PutOuts : Number of put outs in 1986\n* Assists : Number of assists in 1986\n* Errors  : Number of errors in 1986\n* Salary  : 1987 annual salary on opening day in thousands of dollars\n* NewLeague: A factor with levels A and N indicating player's league at the beginning of 1987.\n\n### Source\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University. This is part of the data that was used in the 1988 ASA Graphics Section Poster Session. The salary data were originally from Sports Illustrated, April 20, 1987. The 1986 and career statistics were obtained from The 1987 Baseball Encyclopedia Update published by Collier Books, Macmillan Publishing Company, New York.\n       \n## 1. Library Import Operations:","9db2fce0":"## 6.2. Support Vector Regression","11701208":"## 6.3. Artificial Neural Networks","9d6d4ac3":"## 2. Reading Data:","86f8c5cd":"### The 'years' variable consists of values between the numbers 1 and 24. We enumerated it with numbers from 1 to 5 in the form of 0-4, 5-9, 10-19, 20-24.","b763b182":"# 6. Hiperparametre Optimizasyonlar\u0131\n\n### The most successful data set in the first estimation made was df_4. Therefore, hyperparameter optimization operations will be done on this data set.\n\n## 6.1. KNN","6e85d415":"## 5. Predict:\n\n### We have 8 data. For each of these, the following models will be completed.\n\n### Models:\n\n          Linear Regression\n          Ridge Regression\n          Lasso Regression\n          ElasticNet Regression\n          LightGBM Regression\n          XGBoost Regression\n          GradientBoosting Regression \n          RandomForest Regression \n          DecisionTree Regression\n          MLP Regression\n          KNeighbors Regression\n          SupportVector Regression\n          \n### First, predictions will be made without optimizing hyperparameter.\n### The datasets will be divided into '80% train set' and '20% test set' and will be set to 'random_state = 46'.","22e4d71b":"### The above results were taken in the estimations made without optimization of hyperparameter.","43285efe":"## 6.5. Random Forests\n\n\n","19dfbb57":"## 6.4. CART (Classification and Regression Tree)\n\n\n","55341910":"### 3.4. 'Df_4' is created by Suppressing Missing Data with Predicted Values and Suppressing Values:","0c81c9b2":"### 3.7.  Missing and Outlier Data Deletion and Creating 'df_7' Data Set:","7c2fc958":"### With Local Outlier Factor, outliers of the variables will be determined.","87c1e439":"##  6.6. Gradient Boosting Machines\n\n\n\n","c08abf66":"## 6.8.  LightGBM","c3eacd0d":"### The variables 'League_N', 'Division_W', 'NewLeague_N', 'Experience' are groupby and the average of the variable 'Salary' is taken and these averages are replaced by missing values in the variable 'Salary'.","5e212382":"### 4.1.  Categorical variables \"League_N, Division_W, NewLeague_N\" ","22ed7f68":"## 4. Feature Engineering:","f798867e":"### 3.2. The data set named 'df_2' is created by assigning the average of the variable 'Salary' where they replace the missing data:"}}