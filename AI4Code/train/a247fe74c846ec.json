{"cell_type":{"10779d37":"code","f41c620a":"code","21055b42":"code","1810a601":"code","fdc1125d":"code","bb0d42d7":"code","2e7b0720":"code","0155c11b":"code","9dc494ad":"code","1001ff01":"code","86f44617":"code","bfb3d6ff":"code","73e60a66":"code","69a06723":"code","86caa584":"code","afe4348c":"code","418eed5e":"code","f02aa36b":"code","5f555297":"code","b105e237":"code","8849788c":"code","479e7b65":"code","ef502807":"code","8af32536":"code","020e189b":"code","d28b7853":"code","91d08ded":"code","a2e1c022":"code","cdbb97d0":"code","38ddac80":"code","accb5481":"code","8e6b4ee8":"code","b08c87df":"code","bf6e253b":"code","932ccce1":"code","14dcd20a":"code","71044ab4":"code","72ad27b4":"code","bc8aad48":"code","349cee10":"code","b3e5227c":"code","bf44eedf":"code","17c1ab7a":"code","2324be8c":"code","58762bcd":"code","1a466670":"code","80d55368":"code","589ceb7e":"code","27a873ea":"code","252fe348":"code","5c9590c6":"code","ddf6a5e5":"code","69fd55a8":"code","b5c93504":"code","c7f015a7":"code","24e624e4":"code","71339fc6":"code","16a4cefa":"code","7e39126b":"code","9fc1e628":"code","8dd247f2":"code","3f2dee50":"code","906b688c":"code","a59c652f":"markdown","b7642fa1":"markdown","4e158d76":"markdown","76369fa5":"markdown","87996034":"markdown","fd877cd0":"markdown","39023ac4":"markdown","58db42b6":"markdown","bcd1f889":"markdown","24e5714b":"markdown","2ce8f4f2":"markdown","dd549aac":"markdown","d850f077":"markdown","f2a5d3a0":"markdown","db9cfa37":"markdown","2ee6b4ca":"markdown","33b3a5c2":"markdown","f3c09ea4":"markdown","4bf7d7d1":"markdown","c4b9742f":"markdown","959c0a09":"markdown"},"source":{"10779d37":"#importing libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_validate\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import precision_score, recall_score,confusion_matrix, plot_precision_recall_curve\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report, precision_recall_curve, average_precision_score\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f41c620a":"#Reading the dataset\ndf = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","21055b42":"#number of features\nprint('Number of features: {}'.format(len(df.columns)-1))","1810a601":"#Looking at amount and Time for Non-fraud transactions\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df.loc[df['Class']==0]['Amount'].values\ntime_val = df.loc[df['Class']==0]['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='b')\nax[0].set_title('Distribution of Transaction Amount - Non-fraud ', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time - Non-fraud ', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\n\n\nplt.show()","fdc1125d":"#Looking at amount and Time for Fraud transactions\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val_f = df.loc[df['Class']==1]['Amount'].values\ntime_val_f = df.loc[df['Class']==1]['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount - Fraud ', fontsize=14)\nax[0].set_xlim([min(amount_val_f), max(amount_val_f)])\n\nsns.distplot(time_val_f, ax=ax[1], color='r')\nax[1].set_title('Distribution of Transaction Time - Fraud ', fontsize=14)\nax[1].set_xlim([min(time_val_f), max(time_val_f)])\n\n\n\nplt.show()","bb0d42d7":"#let's see the amont's mean for each class\nprint(\"Non-Fraud amont's mean: {}\".format(df[df['Class']==0]['Amount'].mean()))\nprint(\"Non-Fraud amont's median: {}\".format(df[df['Class']==0]['Amount'].median()))\nprint('--')\nprint(\"Fraud amont's mean: {}\".format(df[df['Class']==1]['Amount'].mean()))\nprint(\"Fraud amont's median: {}\".format(df[df['Class']==1]['Amount'].median()))","2e7b0720":"#Boxplot of amount column\ndf.boxplot(by='Class', column=['Amount'],grid=True,figsize=(6,10))\nplt.title('Boxplot amount per transaction')\nplt.suptitle('')\nplt.ylabel('Transaction amount')\nplt.xlabel('Fraud (1) or Non-Fraud(0)')\nplt.show()","0155c11b":"#Setting our X and Y\nX = df.drop('Class',axis=1)\ny = df['Class']","9dc494ad":"#splitting - Train and Test partition\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\nprint('N\u00ba Non-Fraud train: {}'.format(y_train.value_counts()[0]))\nprint('N\u00ba Fraud train: {}'.format(y_train.value_counts()[1]))\nprint('Fraud %: {}'.format(y_train.value_counts()[0]\/len(y_train)))\nprint('-----------------')\nprint('N\u00ba Non-Fraud test: {}'.format(y_test.value_counts()[0]))\nprint('N\u00ba Fraud test: {}'.format(y_test.value_counts()[1]))\nprint('Fraud %: {}'.format(y_test.value_counts()[0]\/len(y_test)))\n","1001ff01":"#using a simple Random Forest to find the best's features!\nrf = RandomForestClassifier().fit(X_train,y_train)\nimp = pd.DataFrame({'Features':X_train.columns, 'Importance': rf.feature_importances_})\nprint(imp.sort_values(by=['Importance'],ascending=False))","86f44617":"#barplot of RF feature_importances_\nplt.figure(figsize=(12,5))\nplt.xticks(range(len(imp)), imp['Features'])\nplt.xlabel('Features')\nplt.ylabel('Importance')\nplt.title('Features importance - RF')\nplt.bar(range(len(imp)), imp['Importance']);","bfb3d6ff":"#let's see correlation heatmap\nplt.figure(figsize=[10,10])\nplt.title('Correlation HeatMap')\nsns.heatmap(df.corr());","73e60a66":"#lets see the correlation only for the target column!\ndf.corr()['Class'].sort_values()","69a06723":"#lets use top 8 features and amount!\nfeatures = ['V17','V14','V12','V11','V16','V10','V4','V3', 'Amount']","86caa584":"#Here we create our model - fit and predict also\nmodel = CatBoostClassifier(verbose=False).fit(X_train[features],y_train)\ny_pred_train = model.predict(X_train[features])\ny_pred_test = model.predict(X_test[features])","afe4348c":"#Predict the train set is a good way to measure overfitting!\nprint('Training Set evaluation')\nprint(classification_report(y_train,y_pred_train))\nprint(confusion_matrix(y_train,y_pred_train))","418eed5e":"#Let's look at our test set!\nprint('Test Set evaluation')\nprint(classification_report(y_test,y_pred_test))\nprint(confusion_matrix(y_test,y_pred_test))","f02aa36b":"average_precision = average_precision_score(y_test, y_pred_test)","5f555297":"disp = plot_precision_recall_curve(model, X_test, y_test)\ndisp.ax_.set_title('Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision));","b105e237":"#Here we apply our model into cross validation, with 5 folds, utilizing f1 metrics\nscores = cross_validate(model, X[features], y, cv=5, scoring=('f1'))","8849788c":"scores","479e7b65":"print('F1 Score mean: {}, F1 Score Standard Deviation: {}'.format(round(scores['test_score'].mean(),3),round(scores['test_score'].std(),3)))","ef502807":"X_train, X_test, y_train, y_test = train_test_split(X[features], y, random_state=42, test_size=0.2)","8af32536":"minmax = MinMaxScaler()\nsmote = SMOTE(random_state=42)\nnr = NearMiss()","020e189b":"smote_x, smote_y = smote.fit_resample(pd.DataFrame(minmax.fit_transform(X_train[features]),columns=X_train[features].columns),y_train)\nnr_x, nr_y = nr.fit_resample(pd.DataFrame(minmax.fit_transform(X_train[features]),columns=X_train[features].columns),y_train)","d28b7853":"cols_ = X_train[features].columns\nX_train = pd.DataFrame(minmax.fit_transform(X_train[features]),columns=cols_)","91d08ded":"#Here we create our model - fit and predict also\nmodel_ = CatBoostClassifier(verbose=False)\nmodel_.fit(smote_x[features],smote_y)\ny_pred_train = model_.predict(X_train[features])\ny_pred_test = model_.predict(X_test[features])","a2e1c022":"#Predict the train set is a good way to measure overfitting!\nprint('Training Set evaluation')\nprint(classification_report(y_train,y_pred_train))\nprint(confusion_matrix(y_train,y_pred_train))","cdbb97d0":"#Let's look at our test set!\nprint('Test Set evaluation')\nprint(classification_report(y_test,y_pred_test))\nprint(confusion_matrix(y_test,y_pred_test))","38ddac80":"average_precision_ = average_precision_score(y_test, y_pred_test)","accb5481":"disp = plot_precision_recall_curve(model_, X_test, y_test)\ndisp.ax_.set_title('Precision-Recall curve with SMOTE: '\n                   'AP={0:0.2f}'.format(average_precision_));","8e6b4ee8":"#Here we create our model - fit and predict also\nmodel_nr = CatBoostClassifier(verbose=False)\nmodel_nr.fit(nr_x[features],nr_y)\ny_pred_train = model_nr.predict(X_train[features])\ny_pred_test = model_nr.predict(X_test[features])","b08c87df":"#Predict the train set is a good way to measure overfitting!\nprint('Training Set evaluation')\nprint(classification_report(y_train,y_pred_train))\nprint(confusion_matrix(y_train,y_pred_train))","bf6e253b":"#Let's look at our test set!\nprint('Test Set evaluation')\nprint(classification_report(y_test,y_pred_test))\nprint(confusion_matrix(y_test,y_pred_test))","932ccce1":"average_precision_nr = average_precision_score(y_test, y_pred_test)","14dcd20a":"disp = plot_precision_recall_curve(model_nr, X_test, y_test)\ndisp.ax_.set_title('Precision-Recall curve with NearMiss: '\n                   'AP={0:0.2f}'.format(average_precision_nr));","71044ab4":"import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\nn_inputs = X_train.shape[1]\nn_inputs_nr = nr_x.shape[1]\nn_inputs_smote = smote_x.shape[1]\n\nkeras_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(9, activation='relu'),\n    Dense(9, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\nkeras_model_smote = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs_smote, ), activation='relu'),\n    Dense(9, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\nkeras_model_nr = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs_nr, ), activation='relu'),\n    Dense(9, activation='relu'),\n    Dense(2, activation='softmax')\n])","72ad27b4":"keras_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","bc8aad48":"keras_model.fit(X_train[features], y_train, validation_split=0.2, batch_size=25, epochs=3, shuffle=True, verbose=1)","349cee10":"keras_model_smote.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nkeras_model_nr.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","b3e5227c":"keras_model_smote.fit(smote_x[features],smote_y, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=1)\n","bf44eedf":"keras_model_nr.fit(nr_x[features],nr_y, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=1)","17c1ab7a":"y_pred_test_keras = keras_model.predict_classes(X_test, verbose=1)","2324be8c":"y_pred_test_keras_nr = keras_model_nr.predict_classes(X_test[features], verbose=1)","58762bcd":"y_pred_test_keras_smote = keras_model_smote.predict_classes(X_test[features], verbose=1)","1a466670":"#Let's look at our test set!\nprint('Test Set evaluation without smote or nearmiss')\nprint(classification_report(y_true=y_test,y_pred=y_pred_test_keras))\nprint(confusion_matrix(y_true=y_test,y_pred=y_pred_test))","80d55368":"#Let's look at our test set!\nprint('Test Set evaluation with smote')\nprint(classification_report(y_true=y_test,y_pred=y_pred_test_keras_smote))\nprint(confusion_matrix(y_true=y_test,y_pred=y_pred_test_keras_smote))","589ceb7e":"#Let's look at our test set!\nprint('Test Set evaluation with nearmiss')\nprint(classification_report(y_true=y_test,y_pred=y_pred_test_keras_nr))\nprint(confusion_matrix(y_true=y_test,y_pred=y_pred_test_keras_nr))","27a873ea":"X2 = pd.DataFrame(MinMaxScaler().fit_transform(X.copy()),columns=X.columns)","252fe348":"X_train_keras, X_test_keras, y_train_keras, y_test_keras = train_test_split(X2, y, random_state=42, test_size=0.2)","5c9590c6":"smote_x_keras, smote_y_keras = smote.fit_resample(pd.DataFrame(minmax.fit_transform(X_train_keras),columns=X_train_keras.columns),y_train)\nnr_x_keras, nr_y_keras = nr.fit_resample(pd.DataFrame(minmax.fit_transform(X_train_keras),columns=X_train_keras.columns),y_train)","ddf6a5e5":"n_inputs = X_train_keras.shape[1]\nn_inputs_nr = nr_x_keras.shape[1]\nn_inputs_smote = smote_x_keras.shape[1]\n\nkeras_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(30, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\nkeras_model_smote = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs_smote, ), activation='relu'),\n    Dense(30, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\nkeras_model_nr = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs_nr, ), activation='relu'),\n    Dense(30, activation='relu'),\n    Dense(2, activation='softmax')\n])","69fd55a8":"keras_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","b5c93504":"keras_model.fit(X_train_keras, y_train_keras, validation_split=0.2, batch_size=25, epochs=3, shuffle=True, verbose=1)","c7f015a7":"keras_model_smote.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nkeras_model_nr.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])","24e624e4":"keras_model_smote.fit(smote_x_keras,smote_y_keras, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=0)","71339fc6":"keras_model_nr.fit(nr_x_keras, nr_y_keras, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=0)","16a4cefa":"y_pred_test_keras = keras_model.predict_classes(X_test_keras, verbose=0)","7e39126b":"y_pred_test_keras_nr = keras_model_nr.predict_classes(X_test_keras, verbose=0)","9fc1e628":"y_pred_test_keras_smote = keras_model_smote.predict_classes(X_test_keras, verbose=0)","8dd247f2":"#Let's look at our test set!\nprint('Test Set evaluation without smote or nearmiss')\nprint(classification_report(y_true=y_test_keras,y_pred=y_pred_test_keras))\nprint(confusion_matrix(y_true=y_test_keras,y_pred=y_pred_test_keras))","3f2dee50":"#Let's look at our test set!\nprint('Test Set evaluation with smote')\nprint(classification_report(y_true=y_test_keras,y_pred=y_pred_test_keras_smote))\nprint(confusion_matrix(y_true=y_test_keras,y_pred=y_pred_test_keras_smote))","906b688c":"#Let's look at our test set!\nprint('Test Set evaluation with nearmiss')\nprint(classification_report(y_true=y_test_keras,y_pred=y_pred_test_keras_nr))\nprint(confusion_matrix(y_true=y_test_keras,y_pred=y_pred_test_keras_nr))","a59c652f":"## At the first look, looks like overfitting... again! :(","b7642fa1":"# Creating train and test partition","4e158d76":"# Let's predict and evaluate our model with SMOTE","76369fa5":"# Selecting the features with high correlation, with high feature importance and Amount","87996034":"# Looking at the correlation with the Target!","fd877cd0":"<h1 align=\"center\"> Credit Card Fraud Detection <\/h1>\n\n<h2> Introduction <\/h2>\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n<h2> Inspiration:<\/h2>\n<li>Identify fraudulent credit card transactions.<\/li>\n\n\n<h2> Part 1: <\/h2>\n<ul>\n<li> Undestand the features & correlation with the target. <\/li>\n<li> Select the features based on Random Forest feature importance correlation and data analysis<\/li>\n<li> Utilize CatBoostClassifier <\/li>\n<li> Evaluate our predictions <\/li>\n<\/ul>\n\n<h2> Part 2: <\/h2>\n<ul>\n<li> Re-scaling <\/li>\n<li> Oversample Fraud class with SMOTE <\/li>\n<li> Undersampe Fraud class with NearMiss <\/li>\n<li> Model 1: CatBoostClassifier <\/li><ul>\n<li> Evaluate our predictions <\/li><\/ul>\n<li> Model 2: Keras <\/li><ul>\n<li> Evaluate our predictions <\/li><\/ul>\n<\/ul>\n","39023ac4":"As you can see this simple model could predict also the fraud class without oversample (Fraud-Class) or undersampling (Non-Fraud-Class)\n# Please, if you like upvote!","58db42b6":"# Okay - last try! Keras with all features!","bcd1f889":"# Okay... Let's try Keras with the selected features!","24e5714b":"## As we can see your f1 for fraud class is low :(\n### Let's evaluate our predictions \n- We are going to use precision_recall and precision_recall_plot instead of accuracy ","2ce8f4f2":"## But as we can see your model can predict both classes :)\n### Let's evaluate our predictions \n- We are going to use precision_recall and precision_recall_plot instead of accuracy ","dd549aac":"# Let's look at correlation heatmap!","d850f077":"# Please, if you like upvote!","f2a5d3a0":"## Oh! that's terrible....! Let's try to undersample non fraud class!","db9cfa37":"# As you can see the best models were:\n    - Catboost Classifier + feature selection\n    - Keras without under\/over sample - with all features","2ee6b4ca":"# Let's predict and evaluate our model with NearMiss","33b3a5c2":"As you can see this simple model could predict also the fraud class without oversample (Fraud-Class) or undersampling (Non-Fraud-Class)","f3c09ea4":"# Part 1\n# Understanding our data\n<li>We have 31 columns, 28 are the principal components obtained with PCA!<\/li>\n<li> Non PCA columns are: Amount and Class <\/li>","4bf7d7d1":"# Creating a simple Random Forest Classifier to extract feature_importances","c4b9742f":"## At the first look, looks like overfitting... :(","959c0a09":"# Part 2"}}