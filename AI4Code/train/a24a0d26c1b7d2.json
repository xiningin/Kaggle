{"cell_type":{"6dab1643":"code","9c957db7":"code","9ccc2ac9":"code","0edda2c3":"code","fe75e97d":"code","a179b485":"code","5ee73bc8":"code","33613a7b":"code","9400ae35":"code","7a30c85d":"code","2271ba3c":"code","4a558797":"code","24277e89":"code","b79543c0":"code","68add8ac":"code","fa143e3f":"code","fedf9aec":"markdown","c3efb697":"markdown","a8f9dc10":"markdown","a070f54f":"markdown"},"source":{"6dab1643":"!pip install clean-text","9c957db7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport re\nfrom cleantext import clean\n\nfrom tqdm.auto import tqdm\ntqdm.pandas()\n\npd.options.display.max_colwidth = None","9ccc2ac9":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers as L\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","0edda2c3":"with open('..\/input\/bhagwat-gita-in-english\/gita.txt') as f:\n    data = f.read()\ndf = pd.DataFrame({'text': data.split('\\n')})\nprint(len(df))\ndf.head()  ","fe75e97d":"clean_text = lambda x: clean(x,\n    fix_unicode=True,               # fix various unicode errors\n    to_ascii=True,                  # transliterate to closest ASCII representation\n    lower=True,                     # lowercase text\n    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n    no_urls=True,                  # replace all URLs with a special token\n    no_emails=True,                # replace all email addresses with a special token\n    no_phone_numbers=True,         # replace all phone numbers with a special token\n    no_numbers=True,               # replace all numbers with a special token\n    no_digits=True,                # replace all digits with a special token\n    no_currency_symbols=False,      # replace all currency symbols with a special token\n    no_punct=False,                 # remove punctuations\n    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n    replace_with_url=\"\",\n    replace_with_email=\"\",\n    replace_with_phone_number=\"\",\n    replace_with_number=\"\",\n    replace_with_digit=\"\",\n    replace_with_currency_symbol=\"<CUR>\",\n    lang=\"en\"                       # set to 'de' for German special handling\n)","a179b485":"df['cleaned'] = df['text'].progress_apply(clean_text)\ndf['cleaned'] = df['cleaned'].apply(lambda x: re.sub(r\"\\s\\W*\\s\", \"\", x))\ndf['len'] = df['cleaned'].apply(lambda x: len(x.split()))","5ee73bc8":"fig = plt.figure(figsize=(12, 5))\nfig.suptitle('Length of Sentences', fontsize=12)\ndf['len'].plot.hist(bins=15);","33613a7b":"df = df[df['len'] > 1].drop_duplicates(subset=['cleaned']).reset_index(drop=True)","9400ae35":"y = df.cleaned.values.tolist()\n\ntokenizer = Tokenizer(filters='!\"#$%&()*+,-.\/:;<=>?@^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(y)\nmask_token_id = len(tokenizer.word_index)+1\ntokenizer.word_index.update({'[mask]': mask_token_id})","7a30c85d":"def get_masked_input(sequence):\n    mask = np.random.randint(low=0, high=len(sequence))\n    return [token if i != mask else mask_token_id for i, token in enumerate(sequence)]","2271ba3c":"VOCAB = len(tokenizer.word_index)\nMAX_SEQ_LEN = 15\nEMBEDDING_VECTOR_LENGTH = 32\nN_LSTM_CELLS = 265\nRECURRENT_DROPOUT = 0.5","4a558797":"y = tokenizer.texts_to_sequences(y)\nx = [get_masked_input(seq) for seq in y]\nx = pad_sequences(x, maxlen=MAX_SEQ_LEN, padding='post')\ny = pad_sequences(y, maxlen=MAX_SEQ_LEN, padding='post')\ny = keras.utils.to_categorical(y)\n\nx.shape, y.shape","24277e89":"model = keras.models.Sequential([\n    L.Embedding(input_dim=VOCAB, output_dim=EMBEDDING_VECTOR_LENGTH, input_length=MAX_SEQ_LEN),\n    L.Bidirectional(L.LSTM(N_LSTM_CELLS, return_sequences=True, recurrent_dropout=RECURRENT_DROPOUT)),\n    L.TimeDistributed(L.Dense(VOCAB, activation='softmax'))\n])\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n\nmodel.summary()\nkeras.utils.plot_model(model, show_shapes=True)","b79543c0":"es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, restore_best_weights=True)\nrlp = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1)\nhistory = model.fit(x=x, y=y, validation_split=0.1, callbacks=[es, rlp], epochs=100)","68add8ac":"fig, ax = plt.subplots(2, 1, figsize=(20, 8))\nmetrics = pd.DataFrame(history.history)\nmetrics[['acc', 'val_acc']].plot(ax=ax[0])\nmetrics[['loss', 'val_loss']].plot(ax=ax[1])\nax[0].set_title('Model Accuracy', fontsize=12)\nax[1].set_title('Model Loss', fontsize=12)\nfig.suptitle('Model Metrics', fontsize=18);","fa143e3f":"query = \"to die performing [mask] is no ill;\"\nquery_token_ids = tokenizer.texts_to_sequences([query])\nquery_token_ids = pad_sequences(query_token_ids, maxlen=MAX_SEQ_LEN, padding='post')\n\npred = model(query_token_ids)\npred_seq = np.ravel(pred.numpy().argmax(axis=-1))\npred_text = ' '.join(tokenizer.index_word[token] for token in pred_seq if token != 0)\npred_text","fedf9aec":"# Inference","c3efb697":"# Model","a8f9dc10":"# Data Preparation","a070f54f":"# Training"}}