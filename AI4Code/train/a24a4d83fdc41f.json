{"cell_type":{"758bce4a":"code","f88f3dcf":"code","75b0c510":"code","79edf451":"code","0454faf1":"code","c35f0ece":"code","bf2b541e":"code","91fa5a8d":"code","a6c20b0e":"code","1cace800":"code","d705c0ce":"code","f1916c70":"code","f3b33bd3":"code","8de10238":"code","17d1efa4":"code","ff4fc95d":"code","75a676b4":"code","6c2eb0ae":"code","8433d2fc":"code","57f5a1c1":"code","e1c25f62":"code","3e91e54e":"code","c7f74af6":"code","f9bae035":"code","a857b1f2":"code","e86c6304":"code","ed31b398":"markdown","b933123b":"markdown"},"source":{"758bce4a":"!pip install -q efficientnet >> \/dev\/null","f88f3dcf":"import re\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom kaggle_datasets import KaggleDatasets\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print(\"Device:\", tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"Number of replicas:\", strategy.num_replicas_in_sync)","75b0c510":"# for now using TF 2.2 (compatibility with TPU)\nprint(tf.__version__)","79edf451":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\nGCS_PATH = KaggleDatasets().get_gcs_path('chest-xray-pneumonia')\n\nBATCH_SIZE = 25 * strategy.num_replicas_in_sync\n\nIMAGE_SIZE = [256, 256]","0454faf1":"# ho aumentatop il train set\nPERC_TRAIN = 0.9\n\nfilenames = tf.io.gfile.glob(str(GCS_PATH + \"\/chest_xray\/train\/*\/*\"))\nfilenames.extend(tf.io.gfile.glob(str(GCS_PATH + \"\/chest_xray\/val\/*\/*\")))\n\nrandom.shuffle(filenames)\n\nsplit_ind = int(PERC_TRAIN* len(filenames))\n\ntrain_filenames, val_filenames = filenames[:split_ind], filenames[split_ind:]","c35f0ece":"COUNT_NORMAL = len([filename for filename in train_filenames if \"NORMAL\" in filename])\nprint(\"Normal images count in training set: \" + str(COUNT_NORMAL))\n\nCOUNT_PNEUMONIA = len(\n    [filename for filename in train_filenames if \"PNEUMONIA\" in filename]\n)\nprint(\"Pneumonia images count in training set: \" + str(COUNT_PNEUMONIA))","bf2b541e":"train_list_ds = tf.data.Dataset.from_tensor_slices(train_filenames)\n\nval_list_ds = tf.data.Dataset.from_tensor_slices(val_filenames)\n\nfor f in train_list_ds.take(5):\n    print(f.numpy())","91fa5a8d":"TRAIN_IMG_COUNT = tf.data.experimental.cardinality(train_list_ds).numpy()\nprint(\"Training images count: \" + str(TRAIN_IMG_COUNT))\n\nVAL_IMG_COUNT = tf.data.experimental.cardinality(val_list_ds).numpy()\nprint(\"Validating images count: \" + str(VAL_IMG_COUNT))","a6c20b0e":"CLASS_NAMES = [\n    str(tf.strings.split(item, os.path.sep)[-1].numpy())[2:-1]\n    for item in tf.io.gfile.glob(str(GCS_PATH + \"\/chest_xray\/train\/*\"))\n]\nprint(\"Class names: %s\" % (CLASS_NAMES,))","1cace800":"def get_label(file_path):\n    # convert the path to a list of path components\n    parts = tf.strings.split(file_path, os.path.sep)\n    # The second to last is the class-directory\n    return parts[-2] == \"PNEUMONIA\"\n\n\ndef decode_img(img):\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_jpeg(img, channels=3)\n    # resize the image to the desired size.\n    return tf.image.resize(img, IMAGE_SIZE)\n\n\ndef process_path(file_path):\n    label = get_label(file_path)\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img, label\n\n\ntrain_ds = train_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n\nval_ds = val_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)","d705c0ce":"for image, label in train_ds.take(1):\n    print(\"Image shape: \", image.numpy().shape)\n    print(\"Label: \", label.numpy())","f1916c70":"# ho fatto alcune modifiche rispetto all'esempio su Keras.io per correggere un bug.\n\ntest_filenames = tf.io.gfile.glob(str(GCS_PATH + \"\/chest_xray\/test\/*\/*\"))\n\ntest_list_ds = tf.data.Dataset.from_tensor_slices(test_filenames)\n\nTEST_IMG_COUNT = tf.data.experimental.cardinality(test_list_ds).numpy()\n\nprint(\"Test images count: \" + str(TEST_IMG_COUNT))","f3b33bd3":"test_ds = test_list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.batch(BATCH_SIZE)","8de10238":"def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n\n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n    ds = ds.batch(BATCH_SIZE)\n\n    # `prefetch` lets the dataset fetch batches in the background while the model\n    # is training.\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n\n    return ds","17d1efa4":"train_ds = prepare_for_training(train_ds)\nval_ds = prepare_for_training(val_ds)\n\nimage_batch, label_batch = next(iter(train_ds))","ff4fc95d":"def show_batch(image_batch, label_batch):\n    plt.figure(figsize=(10, 10))\n    for n in range(25):\n        ax = plt.subplot(5, 5, n + 1)\n        plt.imshow(image_batch[n] \/ 255)\n        if label_batch[n]:\n            plt.title(\"PNEUMONIA\")\n        else:\n            plt.title(\"NORMAL\")\n        plt.axis(\"off\")","75a676b4":"show_batch(image_batch.numpy(), label_batch.numpy())","6c2eb0ae":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\n\nimport efficientnet.tfkeras as efn","8433d2fc":"EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, \n        efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7]\n\n# as default it used B0\n\ndef build_model(dim = 256, ef = 0):\n    inp = tf.keras.layers.Input(shape=(*IMAGE_SIZE, 3))\n\n    x = preprocessing.Rescaling(1.0 \/ 255)(inp)\n    \n    base = EFNS[ef](input_shape=(*IMAGE_SIZE, 3), weights='imagenet', include_top = False)\n    \n    x = base(x)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dense(128, activation='relu')(x)\n    x = tf.keras.layers.Dropout(0.3)(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n    \n    model = tf.keras.Model(inputs = inp,outputs = x)\n    \n    return model","57f5a1c1":"# see how many params the model has\n\nmodel_s = build_model(ef = 4)\n\nmodel_s.summary()","e1c25f62":"checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"xray_model.h5\", save_best_only=True)\n\nearly_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n    patience=10, restore_best_weights=True, verbose=1)","3e91e54e":"def get_lr_callback(batch_size=8):\n    lr_start   = 0.000001\n    lr_max     = 0.000005 * 2 * batch_size\/16 # 0.000020 * 8 * batch_size\/16\n    lr_min     = 0.000001\n    lr_ramp_ep = 8\n    lr_sus_ep  = 4\n    lr_decay   = 0.8\n   \n    def lrfn(epoch):\n        if epoch < lr_ramp_ep:\n            lr = (lr_max - lr_start) \/ lr_ramp_ep * epoch + lr_start\n            \n        elif epoch < lr_ramp_ep + lr_sus_ep:\n            lr = lr_max\n            \n        else:\n            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n            \n        return lr\n\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n    \n    return lr_callback","c7f74af6":"with strategy.scope():\n    # proviamo con EF4\n    model = build_model(ef = 4)\n\n    METRICS = [\n        tf.keras.metrics.BinaryAccuracy(),\n        tf.keras.metrics.Precision(name=\"precision\"),\n        tf.keras.metrics.Recall(name=\"recall\"),\n        tf.keras.metrics.AUC(name='auc')\n    ]\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n        loss=\"binary_crossentropy\",\n        metrics=METRICS,\n    )\n\nhistory = model.fit(\n    train_ds,\n    epochs=60,\n    validation_data=val_ds,\n    # class_weight=class_weight,\n    callbacks=[checkpoint_cb, early_stopping_cb, get_lr_callback(BATCH_SIZE)],\n)","f9bae035":"fig, ax = plt.subplots(1, 4, figsize=(20, 5))\nax = ax.ravel()\n\nfor i, met in enumerate([\"auc\", \"recall\", \"binary_accuracy\", \"loss\"]):\n    ax[i].plot(history.history[met])\n    ax[i].plot(history.history[\"val_\" + met])\n    ax[i].set_title(\"Model {}\".format(met))\n    ax[i].set_xlabel(\"epochs\")\n    ax[i].set_ylabel(met)\n    ax[i].legend([\"train\", \"val\"])","a857b1f2":"model.evaluate(test_ds, return_dict=True)","e86c6304":"img = tf.io.read_file(\n    str(GCS_PATH + \"\/chest_xray\/test\/PNEUMONIA\/person100_bacteria_475.jpeg\")\n)\n\nimg = decode_img(img)\/255.\nplt.imshow(img)\n\nimg_array = tf.keras.preprocessing.image.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0)  # Create batch axis\n\nprediction = model.predict(img_array)[0]\nscores = [1 - prediction, prediction]\n\nfor score, name in zip(scores, CLASS_NAMES):\n    print(\"This image is %.2f percent %s\" % ((100 * score), name))","ed31b398":"### This Notebook has been taken from Keras.io. It shows how to detect pneumonia from chest x-ray using a TPU\n\n#### see https:\/\/keras.io\/examples\/vision\/xray_classification_with_tpus\/\n#### Done some changes: changed using Efficient Net for the convolutional base","b933123b":"### build the CNN"}}