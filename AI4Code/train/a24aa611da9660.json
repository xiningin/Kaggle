{"cell_type":{"8cf64e97":"code","aae84361":"code","2d75410b":"code","a476935a":"code","fa6771ec":"code","71b0fb4b":"code","5fc4c5aa":"code","5e97d4bf":"code","464a46c3":"code","0db01af3":"code","d2606593":"code","f893a9b5":"code","ecc1a507":"code","6fb21793":"code","838b3f27":"code","3660f128":"code","bc84b6f1":"code","0d839344":"code","bf1dea70":"code","a0c87c43":"code","6a7836b4":"markdown","2fa49beb":"markdown","5b7437e9":"markdown","166b93d8":"markdown","5a27f69a":"markdown","3fa06675":"markdown","062b1244":"markdown","5d95bae5":"markdown","98dd4fcc":"markdown","392529c0":"markdown","fb778327":"markdown","4e896903":"markdown"},"source":{"8cf64e97":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","aae84361":"import os","2d75410b":"dataset = pd.read_csv('..\/input\/data-preprocessing\/Data.csv')\n# x[lowerbound : upperbound] gives a range btwn lower and upperbound\n# iloc it means the index location so we have to provide the indexes i.e int\n# : colon means range, when used in iloc[:,:] which means that all the elements\n# if there are no numbers before or after colon which by default takes the lowerbound\n# and the upperbound which means all the rows and all the columns.\nX = dataset.iloc[:, :-1].values\n# .values convert dataframe to np array as the time taken for \n# array calculation are less so we convert it to array\n# in the above step we have given iloc[:,:-1] meaning there is no defined upper \n# and lower bound so it takes all the rows, but for the columns it's given\n# :-1 there is no number before colon so it takes the default lowerbound i.e from the starting\n# and after colon the upperbound is defined as -1 meaning the last column or first column from the last\n# as we know in python the right index is excluded so all the columns untill \n# the first col from last will be selected\ny = dataset.iloc[:, -1].values\n# in this code all the rows and only the last column is selected\n# we divided the dataset into X and y bcoz X are independent and y is dependent variable","a476935a":"dataset","fa6771ec":"print(X)\n#all independent variables","71b0fb4b":"print(y)\n#all independent outcomes","5fc4c5aa":"from sklearn.impute import SimpleImputer\n# sklearn also known as sci kit learn is the free machine learning library in python\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html\n# Simple Imputer (class) used to fill up the missing values with a defined \n# strategy(mean, median, most_frequency, constant)\n# creating an object of Simple Imputer with strategy of Mean\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n# fitting the imputer so it calculates the mean of the specified columns\nimputer.fit(X[:, 1:3])\n# transform is when the mean values are placed in the dataset\nX[:, 1:3] = imputer.transform(X[:, 1:3])\n# we want to specify the only columns we need to fill the values","5e97d4bf":"print(X)","464a46c3":"from sklearn.compose import ColumnTransformer\n# Column Transformer is a class and it Applies transformers to columns of an array or pandas DataFrame.\nfrom sklearn.preprocessing import OneHotEncoder\n# OneHotEncoder is a class object used to encode categeorical\n# Encode categorical features as a one-hot numeric array\n# converts categorical to dummy variables so the program can understand\n# create and object and pass the functions\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html?highlight=column%20transformer#sklearn.compose.ColumnTransformer\n# List of (name, transformer, columns)\n# name is encoder, tranformer is a OneHotEncoder function, column is [0] means 1st\n# \u2018first\u2019 : drop the first category in each feature. If only one category is \n# present, the feature will be dropped entirely.\n# remainder = 'passthrough' means all the col which are not specified\n#  will be passed through\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(drop= 'first'), [0])], remainder='passthrough')\n# now we fit here and now the data of col gets converted into dummy variables\nX = np.array(ct.fit_transform(X))","0db01af3":"print(X)","d2606593":"from sklearn.preprocessing import LabelEncoder\n# LabelEncoder is a class used to encode Yes\/No\n# Encode target labels with value between 0 and n_classes-1.\n#This transformer should be used to encode target values, i.e. y, and not the input X.\nle = LabelEncoder() # object\ny = le.fit_transform(y)# fit and here they get converted to 0,1","f893a9b5":"print(y)","ecc1a507":"from sklearn.model_selection import train_test_split\n# for every supervised model we do do the split so that there is no over fitting\n# means the model works well on training data but not testing data\n# to avoid that we use split and we give test_size 0.2 ie 20 % of overall data\n# to reproduce the same result we give some random_state \n# train_test_split takes up two variables\n# X_train, X_test, y_train, y_test it is called unpacking, geting all values at once\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","6fb21793":"print(X_train)","838b3f27":"print(X_test)","3660f128":"print(y_train)","bc84b6f1":"print(y_test)","0d839344":"# there are 2 popular feature scaling techniques they are standardisation and \n# Normalisation, we prefer standardisation as it's output values will be in the\n#range of -3 to +3 and can be applied to any kind of distributed Data.\n# Normalisation works well with normally distributed Data and its range 0 to 1\n# Standardisation works with every data so it's preferred.\n# not all models require scaling the data \n# we don't scale the dummy variables because it has no use in the change of accuracy\nfrom sklearn.preprocessing import StandardScaler # class\nsc = StandardScaler() # object\nX_train[:, 2:] = sc.fit_transform(X_train[:, 2:])# here all the mean and Std(standard deviation) is calculated\nX_test[:, 2:] = sc.transform(X_test[:, 2:]) # the values are standardised in this step.","bf1dea70":"print(X_train)","a0c87c43":"print(X_test)","6a7836b4":"Like this notebook then upvote it.\n\nNeed to improve it then comment below.\n\nEnjoy Machine Learning","2fa49beb":"## Importing the dataset","5b7437e9":"## Taking care of missing data","166b93d8":"**U need to scale both X and y seperately. if the y values are in the range of -3 to +3 or -1 to 1 u need to scale it down as the values are already in the scaled down range. **","5a27f69a":"### Encoding the Dependent Variable","3fa06675":"**I hope this notebook helps u for doing the data preprocessing technique and clear some doubts about doing scaling before or after spliting or need to scale X i.e. independent variable also known as features or y dependent variable also known as Target or lable.**","062b1244":"## Splitting the dataset into the Training set and Test set","5d95bae5":"# Data Preprocessing Tools","98dd4fcc":"## Encoding categorical data","392529c0":"### Encoding the Independent Variable","fb778327":"## Feature Scaling","4e896903":"## Importing the libraries"}}