{"cell_type":{"ad3b38d5":"code","e2ed72d1":"code","9809bff0":"code","3ac3f0f8":"code","0849850c":"code","499fcae1":"code","3818f9e8":"code","22f20c71":"code","625ecfb7":"code","6ad5ac44":"code","d7ef2158":"code","77891e94":"code","002e76b0":"code","e60604ad":"code","d6f34e94":"code","09ae0b1e":"code","fcbf9d2f":"code","16deeb9a":"code","8214e74a":"code","dfea66a7":"code","e34b1169":"code","2f84954b":"code","c29d818c":"code","13b870d8":"code","ff888bb3":"code","2ac8f5d6":"code","21fe2aaf":"code","e032cef2":"markdown","b39d2b73":"markdown","8cf8f64f":"markdown","8e540b0a":"markdown","ab8290e3":"markdown","bbd7bdf5":"markdown","ec7ffdd4":"markdown","931c847d":"markdown","92c8c626":"markdown","ffa29b07":"markdown","4f6c8a05":"markdown","17e551a3":"markdown","dab91a24":"markdown","6131e2ce":"markdown","e0ef6887":"markdown"},"source":{"ad3b38d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","e2ed72d1":"# Import the general libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9809bff0":"# Import the machine learning libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error","3ac3f0f8":"# Import the datasets\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","0849850c":"# Combine the datasets into one dataframe for preprocessing\n# (So they will have the same shape after making dummy variables)\ndf = pd.concat((train, test), ignore_index=True)","499fcae1":"# Define function to impute missing data:\ndef impute_missing_data(df):\n\n    # drop the 'MiscFeature' column inplace\n    df.drop('MiscFeature', axis=1, inplace=True)\n\n    # handle the simple missing values\n    df.loc[df.MasVnrType.isnull(), 'MasVnrType'] = 'None' # no good\n    df.loc[df.MasVnrType == 'None', 'MasVnrArea'] = 0\n    df.loc[df.LotArea.isnull(), 'MasVnrType'] = 0\n    df.loc[df.BsmtQual.isnull(), 'BsmtQual'] = 'NoBsmt'\n    df.loc[df.BsmtCond.isnull(), 'BsmtCond'] = 'NoBsmt'\n    df.loc[df.BsmtExposure.isnull(), 'BsmtExposure'] = 'NoBsmt'\n    df.loc[df.BsmtFinType1.isnull(), 'BsmtFinType1'] = 'NoBsmt'\n    df.loc[df.BsmtFinType2.isnull(), 'BsmtFinType2'] = 'NoBsmt'\n    df.loc[df.BsmtFinType1=='NoBsmt', 'BsmtFinSF1'] = 0\n    df.loc[df.BsmtFinType2=='NoBsmt', 'BsmtFinSF2'] = 0\n    df.loc[df.BsmtFinSF1.isnull(), 'BsmtFinSF1'] = df.BsmtFinSF1.median()\n    df.loc[df.BsmtQual=='NoBsmt', 'BsmtUnfSF'] = 0\n    df.loc[df.BsmtUnfSF.isnull(), 'BsmtUnfSF'] = df.BsmtUnfSF.median()\n    df.loc[df.BsmtQual=='NoBsmt', 'TotalBsmtSF'] = 0\n    df.loc[df.FireplaceQu.isnull(), 'FireplaceQu'] = 'NoFireplace'\n    df.loc[df.GarageType.isnull(), 'GarageType'] = 'NoGarage'\n    df.loc[df.GarageFinish.isnull(), 'GarageFinish'] = 'NoGarage'\n    df.loc[df.GarageQual.isnull(), 'GarageQual'] = 'NoGarage'\n    df.loc[df.GarageCond.isnull(), 'GarageCond'] = 'NoGarage'\n    df.loc[df.BsmtFullBath.isnull(), 'BsmtFullBath'] = 0\n    df.loc[df.BsmtHalfBath.isnull(), 'BsmtHalfBath'] = 0\n    df.loc[df.KitchenQual.isnull(), 'KitchenQual'] = 'TA'\n    df.loc[df.MSZoning.isnull(), 'MSZoning'] = 'RL'\n    df.loc[df.Utilities.isnull(), 'Utilities'] = 'AllPub'\n    df.loc[df.Exterior1st.isnull(), 'Exterior1st'] = 'VinylSd'\n    df.loc[df.Exterior2nd.isnull(), 'Exterior2nd'] = 'VinylSd'\n    df.loc[df.Functional.isnull(), 'Functional'] = 'Typ'\n    df.loc[df.SaleCondition.isnull(), 'SaleCondition'] = 'Normal'\n    df.loc[df.SaleCondition.isnull(), 'SaleType'] = 'WD'\n    df.loc[df['Electrical'].isnull(), 'Electrical'] = 'SBrkr'\n    df.loc[df['SaleType'].isnull(), 'SaleType'] = 'NoSale'\n    df.loc[df['Alley'].isnull(), 'Alley'] = 'NA'\n    df.loc[df['PoolQC'].isnull(), 'PoolQC'] = 'NA'\n    df.loc[df['Fence'].isnull(), 'Fence'] = 'NA'\n\n    # Ken - GarageYrBlt --> Make same as year built for homes with no garage\n    df.loc[df.GarageYrBlt.isnull(), 'GarageYrBlt'] = df['YearBuilt']\n\n    # only one is null and it has type Detchd\n    df.loc[df['GarageArea'].isnull(), 'GarageArea'] = df.loc[df['GarageType']=='Detchd', 'GarageArea'].mean()\n    df.loc[df['GarageCars'].isnull(), 'GarageCars'] = df.loc[df['GarageType']=='Detchd', 'GarageCars'].median()\n\n    # LotFrontage:\n    # Generate a dictionary where the types of Lot Configurations are the keys and the ratio of total LotArea to\n    # LotFrontage are the values\n    frontageratios = dict((df.groupby('LotConfig')['LotArea'].mean() \/ df.groupby('LotConfig')['LotFrontage'].mean()).astype(int))\n\n    # Impute missing LotFrontage values by applying applicable frontageratio to the lot's area\n    df.loc[df.LotFrontage.isnull(), 'LotFrontage'] = (df['LotArea'] \/ df['LotConfig'].map(frontageratios)).astype(int)\n\n    # CentralAir\n    size_mapping = {'Y': 1,'N': 0}\n    df['CentralAir'] = df['CentralAir'].map(size_mapping)\n\n    return df","3818f9e8":"# Define function to change categorical variables to dummy variables\ndef make_dummies(df):\n\n    # Encode the Categorical Data\n    # Encode the Feature Variables (don't need to code the target)\n    catcols = ['MSSubClass','MSZoning','Street','Alley','LotShape',\n               'LandContour','Utilities','LotConfig','LandSlope',\n               'Neighborhood','Condition1','Condition2','BldgType',\n               'HouseStyle','RoofStyle','RoofMatl','Exterior1st',\n               'Exterior2nd','MasVnrType','ExterQual','ExterCond',\n               'Foundation','BsmtQual','BsmtCond','BsmtExposure',\n               'BsmtFinType1','BsmtFinType2','Heating','HeatingQC',\n               'CentralAir','Electrical','KitchenQual','Functional',\n               'FireplaceQu','GarageType','GarageFinish','GarageQual',\n               'GarageCond','PavedDrive','PoolQC','Fence',\n               'SaleType','SaleCondition']\n\n    return pd.get_dummies(df, columns=catcols, drop_first=True)","22f20c71":"# Use custom function (above) to impute the missing values\ndf = impute_missing_data(df)","625ecfb7":"# Use custom function (above) to change categorical features to dummy variables\ndf = make_dummies(df)","6ad5ac44":"# Split back into separate datasets\ntrain = df[:train.shape[0]]\ntest = df[train.shape[0]:]\n\n# Drop the 'SalePrice' column from the test dataset (it was added during combination)\ntest = test.drop('SalePrice', axis=1)","d7ef2158":"X = train.drop('SalePrice', axis=1)\ny = train['SalePrice']","77891e94":"# Logtransform y variable\ny = np.log1p(y)","002e76b0":"# Use train-test-split to create 'local' training and testing datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)","e60604ad":"# Create and fit the regressor object\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)","d6f34e94":"# Predict the test set results\ny_pred = model.predict(X_test)\n\n# Eliminate negative results by setting them to zero\ny_pred[y_pred<0] = 0","09ae0b1e":"# Looking at the scores of the mdoel on the test set\nr2 = model.score(X_test, y_test)\nRMSLE = np.sqrt(mean_squared_log_error(y_test, y_pred))\nRMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n\n# Show results from local train-test split\nprint()\nprint(\"Results from local train-test split:\")\nprint(\"r^2: {}.\".format(r2))\nprint(\"RMSLE: {}.\".format(RMSLE))\nprint(\"RMSE: {}.\".format(RMSE))\nprint()","fcbf9d2f":"# Run the model's prediction method on the test dataset\npredictions = model.predict(test)\npredictions[predictions<0] = 0","16deeb9a":"predictions = np.expm1(predictions)","8214e74a":"predictions.min()","dfea66a7":"predictions.max()","e34b1169":"list(predictions).count('inf')","2f84954b":"list(predictions).count(-1)","c29d818c":"len(predictions)","13b870d8":"submission = test[['Id']].copy()","ff888bb3":"submission['SalePrice'] = predictions","2ac8f5d6":"submission[submission.isin([np.inf, -np.inf, -1]).any(1)]","21fe2aaf":"submission.to_csv('submission_test2.csv', index=False, header=True)","e032cef2":"# Examine the inverse transformed data to see if there are any infinite or negative data:","b39d2b73":"# Create and fit a LinearRegression object:","8cf8f64f":"# Use Model to predict values for Train dataset:","8e540b0a":"# Reverse Log Transform the Predictions:\n\nThis is where I run into trouble on my local machine","ab8290e3":"# Create the X and y variables for the training dataset","bbd7bdf5":"# Use the Model to predict values in the training set:","ec7ffdd4":"# Import Libraries","931c847d":"# Preprocessing","92c8c626":"# Import the Datasets","ffa29b07":"Interesting, this function seems to run with no problem in the kernel.  I'm not sure why it won't run on my machine without overflow issues.  Maybe my CPU or chip is not powerful enough?","4f6c8a05":"# Look at results of Model on training set:","17e551a3":"# Problems with Inverse Logtransforming using np.expm1\n\n## *Update: Figured it out*\n\n*Well, I knew it would be something inane.  I'm currently running the model with all features as a starting point before doing any feature selection.  As a result, I sometimes get negative predictions.  In the train\/test set, I would correct all negative predictions to 0.  When I ran the model on the real training set from kaggle, I was not doing this before inverse transforming the results.  I looked at the results after transforming and didn't see the negative results so I thought they weren't an issue.  I finally remembered this wrinkle and tried the model again correcting the predictions for negative results before running np.expm1() on them and it worked fine.*\n\n*The log transformation improved my simple model TREMENDOUSLY.  I will work on feature selection, but it's amazing how much simple log transform on the dependent variable improved the baseline.*\n\nThis notebook runs a basic multivariate linear regression model using all variables in the dataset to illustrate an issue that I'm having when I logtransform the dependent variable (y), and then try to inverse logtransform my predictions for submission.\n\nThe logtransformation is done using np.log1p:\n> y = np.log1p(y)\n\nI invert the preductions using np.exm1:\n>predictions = np.expm1(predictions)\n\nThe results, at least when I snapshot the kernel here on kaggle or run the script on my local machine, include -1 values and 'inf' values which are invalid for the submission. When I run it locally, I get a message that expm1() function is resulting in overflow problems.  The same happened when I used np.log() and np.exp().\n\nI've tried casting the array as dtype np.float128, but I get errors saying that it isn't a valid datatype\nI've tried using Bigfloat, but am having difficulty installing Bigfloat and its dependencies\n\nI've seen other people using logtransforms on their data and haven't seen this mentioned as a problem","dab91a24":"# Log Transform the dependent variable (y):\n\nThis is the step I'm wondering about.  It seems to vastly improve scores when running on local test sets, but when getting the inverse of this transformation (using np.expm1 below), I run into overflow errors and get negative and 'inf' values in the results, making them invalid.","6131e2ce":"# Split the training dataset into 'local' training and test sets:","e0ef6887":"# Still Problems in the Data\nI thought I had it fixed, but when I run the program and output the data, I can see there are still -1 in 'inf' values which makes the submission invalid.  I can't figure out how to fix this.\n\n## *Update: Figured it out (See update at top of Notebook)*"}}