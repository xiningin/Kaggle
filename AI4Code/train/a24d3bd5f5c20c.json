{"cell_type":{"af906d3e":"code","83e0c024":"code","1b30d0ae":"code","691860a4":"code","5bbf8392":"code","36773984":"code","56a0d41d":"code","a9fe4d2a":"code","86b1b00e":"code","eff93548":"code","0450b04f":"code","29f93f5f":"code","3fcdc940":"code","4f2bf6f3":"code","baf0f99f":"code","7209d266":"code","c3b4f0f0":"code","d34f1e63":"code","faa679f0":"markdown","0e14d577":"markdown","13513fdf":"markdown","5c4f613a":"markdown","ee621b99":"markdown","e59b029d":"markdown","9837d182":"markdown","eac34289":"markdown","c0ca83f1":"markdown","e76af374":"markdown","07926f6a":"markdown","50d67e94":"markdown","faa23715":"markdown"},"source":{"af906d3e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","83e0c024":"# read data using pandas.read_csv\ndata=pd.read_csv(\"..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\")\n# show the first 3 rows of the data\ndata.head(3)","1b30d0ae":"data.isnull().sum().sort_values(ascending=False)\ndata['reviews_per_month'].fillna(0,inplace=True)\ndata['last_review'].fillna(0,inplace=True)\ndata['host_name'].fillna('None',inplace=True)\ndata['name'].fillna('None',inplace=True)","691860a4":"mean = data['price'].describe()['mean']\nstd = data['price'].describe()['std']\n\nmean1 = data['reviews_per_month'].describe()['mean']\nstd1 = data['reviews_per_month'].describe()['std']\n\ndata.drop(index = data[data['price']> (mean+3*std)].index, inplace=True)\ndata.drop(index = data[data['reviews_per_month']> (mean1+3*std1)].index, inplace=True)\ndata = data.reset_index(drop=True)","5bbf8392":"# Extract the 'name' column\ntitle = data['name']\ntitle","36773984":"def word_count(data):\n    temp_list1 = []\n    for i in range(len(data)):\n        temp_list1.append(data[i].lower().split(' '))\n    \n    temp_list2 = []\n    for i in temp_list1:\n        for j in i:\n            temp_list2.append(j)\n\n    word_count_dict = {}\n    for str in temp_list2:\n        if str in word_count_dict.keys():\n            word_count_dict[str] = word_count_dict[str] + 1\n        else:\n            word_count_dict[str] = 1\n    return word_count_dict\n\nword_count_dict = word_count(title)","56a0d41d":"# transform the dict to dataframe, only take words with frequency > 300\nse1 = pd.Series(data=word_count_dict)\na = se1[se1.values>300].sort_values(ascending=False)\n\ndf = pd.DataFrame(data=a,columns=['count'])\ndf = df.reset_index()\ndf.columns=['word','count']\ndf","a9fe4d2a":"# 1.eliminate words with length<=2\nlist_index = []\nfor i in range(len(df)):\n    if len(df['word'][i])<=2:\n        list_index.append(i)\ndf.drop(index=list_index,inplace=True)\ndf = df.reset_index(drop=True)","86b1b00e":"# 2.modify words with suffix such as 'apt.', but this will generate duplicate 'word', I need combine them later.\nword_list = df['word'].tolist()\nanti_replication_list = []\nfor i in word_list:\n    i = i.strip('.')\n    i = i.strip('!')\n    anti_replication_list.append(i)\nword_list = anti_replication_list\n\n# reindex\ndf['word'] = word_list","eff93548":"# 3. combine duplicate words\nword_list = df['word']\nall_dup = word_list.duplicated(keep=False)\nall_dup_index = all_dup[all_dup.values==1].index.tolist()\n\n# find value of duplicates\ndup_word = list(set(df.loc[all_dup_index,'word'].values)) #get the duplicate words\n\n# create a list, containing the sum of the frequency of duplicate words\nfillist = []\nfor i in dup_word:\n    fil = df[df['word']==i]['count'].sum()\n    fillist.append(fil)\n\n# fill the sum to all duplicates, then we can drop_duplicates and only keep 1 left.\nfor i in range(len(dup_word)):\n    df.loc[df['word']==dup_word[i],'count']=fillist[i]\n    \n# drop the duplicates\ndf.drop_duplicates(keep='first',inplace=True)\n\n# reset the index\ndf.reset_index(drop=True,inplace=True)","0450b04f":"df","29f93f5f":"# 1. create a list of words which may influence the price.\nwanted_words_price = [\"central\",   \"midtown\",    \"perfect\",    \"times\",        \"castle\",\n                      \"garden\",    \"park\",       \"heart\",      \"village\",      \"cozy\",\n                      \"private\",   \"apt\",        \"spacious\",   \"sunny\",        \"beautiful\",\n                      \"large\",     \"modern\",     \"luxury\",     \"bright\",       \"charming\",\n                      \"quiet\",     \"cosy\",       \"apt.\",       \"castle\",       \"huge\",\n                      \"loft\",      \"apartment\",  \"manhattan\",  \"entire\",       \"bath\",\n                      \"private\",   \"duplex\",     \"story\",      \"spectacular\",  \"lux\",\n                      \"2br\",       \"2bath\",      \"3br\",        \"artistic\",     \"elegant\",\n                      \"village\",   \"brooklyn\",   \"box\",        \"2ba\",          \"hottest\",\n                      \"deck\",      \"brownstone\", \"amazing\", \"prime\",        \"chelsea\",\n                      \"oasis\",     \"1br\",        \"location\",   \"gym\",          \"stylish\",\n                      \"patio\"]","3fcdc940":"# 2. build the 'name-rating' system\nlist_words_rates = []\nfor i in range(len(data['name'])):\n    rates_word = 0\n    description = data['name'][i].lower().split(' ')\n    for j in description:\n        if j in wanted_words_price:\n            rates_word += 1\n    list_words_rates.append(rates_word)","4f2bf6f3":"# 3. add this factor into the dataset\ndata['name_rating'] = list_words_rates\ndata[['name','name_rating']].head(20)","baf0f99f":"# 4. Some basic analysis based on description\ndata['name_rating'].describe()","7209d266":"# See if there is any relationship between the description and price\nname_vs_price = data.groupby(\"name_rating\")['price'].mean()\nname_vs_price","c3b4f0f0":"# Visualization\nfig, ax = plt.subplots(1,1,figsize=(16,10))\nax.set_title(\"name VS price\")\nsns.barplot(x=name_vs_price.index.tolist(), y=name_vs_price.values.tolist(), palette=\"Blues_d\", ax=ax)","d34f1e63":"corr = data.corr() \nk = 10\ncols = corr.nlargest(k,'price').index \ncm = np.corrcoef(data[cols].values.T) \nsns.set(font_scale=0.8)\nfig = plt.figure(figsize=(10,8))\nheatmap = sns.heatmap(cm,annot=True,yticklabels=cols.values,xticklabels=cols.values,cmap='YlGnBu')\t","faa679f0":"2.2 drop rows based on the 3\u03c3 princle","0e14d577":"### Abstract\n\nMy point of view about the name of houses.\n\nI found that the column 'name' is seldom used in other kernels except for the 'word-cloud'. However, I believe this factor can provide some other information due to the importance of a good name.\n\nAlmost in every field, there is a funnel model. In this sense, content generators always come up with an attractive title to increase the click rate. In this kernel, I want to roughly explore if there is a relationship between the name and price of houses.\n","13513fdf":"we can see that houses with more wanted words always cost more, That's Great!","5c4f613a":"### 3. Generate a dictionary of {word : count}\n","ee621b99":"### 2. Data Preprocessing","e59b029d":"The dataFrame need to be further processed due to 3 main reasons:\n\n1. There are too many useless words, such as \u20185\u2019, as well as prepositions (\u2018in\u2019, \u2019for\u2019,\u2018from\u2019) and some abbreviations such as(\u2018ny\u2019, \u2018bk\u2019).\n2. Some words are written indifferent ways, such as \u2018cosy\u2019 and \u2018cozy\u2019.\n3. Some words contain suffix, like \u2018apt\u2019 and \u2018apt.\u2019","9837d182":"2.1 fill null value","eac34289":"### 1. Acquiring and loading the data\n\nIn this notebook, I mainly use the column 'name' of the data.","c0ca83f1":"### 5. find the relationship between house name and price\nIn this step, I first created a list of target words that are likely to positively influence the price.\n\nAfter that, I built the 'name-rating' system based on a principle: \"More target words, higher prices\".","e76af374":"2.3 extract the 'name' column","07926f6a":"### 4. Process of the dictionary","50d67e94":"We can roughly say that those houses with more wanted words are likely to be more expensive in the airbnb market. One way to evaluate is the preason correlation between factors.","faa23715":"Take words with frequency > 300 and build a dataframe"}}