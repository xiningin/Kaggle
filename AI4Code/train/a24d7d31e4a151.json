{"cell_type":{"4294ba4c":"code","11daa13d":"code","676ad101":"code","3269d723":"code","61f0bc24":"code","ec503b9e":"code","26a18324":"code","227f9883":"code","4e7c16a3":"code","cb11aa01":"code","90840c70":"code","b6003450":"code","374f0568":"code","ce6ed29b":"code","f1b62497":"code","f606810f":"code","6f077aa2":"code","71e2ce5f":"code","385bbbcb":"markdown","af81b696":"markdown","7c85b090":"markdown","f318995a":"markdown","876e4a7e":"markdown","3a97b166":"markdown","58094cc4":"markdown","b052ff6e":"markdown","16f724dc":"markdown","be7dbb3a":"markdown","dbb6f30e":"markdown","7d9fee01":"markdown","0a72ad47":"markdown","d9ab250a":"markdown"},"source":{"4294ba4c":"import numpy as np \nimport pandas as pd \nfrom datetime import datetime\n\nimport os\nfrom os.path import join as pjoin\n\ndata_root = '..\/input\/make-data-ready'\nprint(os.listdir(data_root))\n\npd.set_option('display.max_rows',200)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom pprint import pprint\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","11daa13d":"def load_data(data='train',n=2):\n    df = pd.DataFrame()\n    for i in range(n) :\n        if data=='train':\n            if i > 8 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'train_{i}.pkl'))\n        elif data=='test':\n            if i > 2 :\n                break\n            dfpart = pd.read_pickle(pjoin(data_root,f'test_{i}.pkl'))\n        df = pd.concat([df,dfpart])\n        del dfpart\n    return df\n        ","676ad101":"df_train = load_data(n=9)\ndf_test = load_data('test',n=4)","3269d723":"df = pd.concat([df_train, df_test])","61f0bc24":"col_drop = ['Date_Year', 'Date_Month', 'Date_Week','Date_Hour','device_isMobile','device_deviceCategory',\n       'Date_Day', 'Date_Dayofweek', 'Date_Dayofyear', 'Date_Is_month_end',\n       'Date_Is_month_start', 'Date_Is_quarter_end', 'Date_Is_quarter_start',\n       'Date_Is_year_end', 'Date_Is_year_start','totals_visits',\n           'date','visitId','totals_totalTransactionRevenue','geoNetwork_city','geoNetwork_continent',\n            'geoNetwork_metro','geoNetwork_networkDomain',\n'geoNetwork_region','geoNetwork_subContinent','trafficSource_adContent',\n            'trafficSource_adwordsClickInfo.adNetworkType','trafficSource_adwordsClickInfo.gclId',\n'trafficSource_adwordsClickInfo.slot','trafficSource_campaign',\n            'trafficSource_keyword','trafficSource_referralPath','trafficSource_medium',\n            'customDimensions_value','customDimensions_index','trafficSource_source',\n           'totals_bounces','visitNumber','totals_newVisits']\ndf.drop(col_drop, axis=1, inplace=True)","ec503b9e":"country_drop=df.groupby('geoNetwork_country')['totals_transactions'].sum()[df.groupby('geoNetwork_country')['totals_transactions'].sum().sort_values()<4].index.tolist()\ndf.loc[df[df.geoNetwork_country.isin(country_drop)].index,'geoNetwork_country'] = 'NaN'\n\ndf.loc[df[~df.device_browser.isin(['Edge', 'Internet Explorer', 'Firefox', 'Safari', 'Chrome'])].index,'device_browser'] = 'NaN'\ndf.loc[df[~df.device_operatingSystem.isin(['Android', 'iOS', 'Linux', 'Chrome OS', 'Windows', 'Macintosh'])].index,'device_operatingSystem'] = 'NaN'\n\n","26a18324":"col_lb = ['channelGrouping','device_browser','device_operatingSystem', 'geoNetwork_country',\n          'trafficSource_adwordsClickInfo.isVideoAd','trafficSource_isTrueDirect']\nfor col in col_lb:\n    lb = LabelEncoder()\n    df[col]=lb.fit_transform(df[col])","227f9883":"to_median = ['channelGrouping','device_browser','device_operatingSystem','geoNetwork_country','trafficSource_adwordsClickInfo.isVideoAd','trafficSource_isTrueDirect','trafficSource_adwordsClickInfo.page']\nto_sum =['totals_hits','totals_pageviews','totals_timeOnSite','totals_transactionRevenue', 'totals_transactions']\nto_mean =['totals_hits','totals_pageviews','totals_sessionQualityDim']\nto_std = ['totals_hits','totals_pageviews']\nto_time = 'visitStartTime'","4e7c16a3":"target_period = pd.date_range(start='2016-08-01',end='2018-12-01', freq='2MS')\ntrain_period = target_period.to_series().shift(periods=-210, freq='d',axis= 0)\ntime_to = train_period[train_period.index>np.datetime64('2016-08-01')].astype('int')\/\/10**9\ntime_end = target_period.to_series().shift(periods=-45, freq='d',axis= 0)[4:]","cb11aa01":"    user_x = df.iloc[df_train.shape[0]:,:]\n    test_x = pd.concat([user_x.groupby('fullVisitorId')[to_median].median().add_suffix('_median'),\n    user_x.groupby('fullVisitorId')['visitStartTime'].agg(['first','last']).add_suffix('_time').sub(time_to.values[-1]).abs(),\n    user_x.groupby('fullVisitorId')['visitStartTime'].apply(lambda x: x.max() -x.min()).rename('time_diff'),\n    user_x.groupby('fullVisitorId')[to_sum].sum().add_suffix('_sum'),\n    user_x.groupby('fullVisitorId')[to_mean].mean().add_suffix('_mean'),\n    user_x.groupby('fullVisitorId')[to_std].std(ddof=0).add_suffix('_std')],axis=1).reset_index()\n    \n    test_ID= test_x.fullVisitorId\n    test_x = test_x.drop(['fullVisitorId'], axis=1,errors='ignore')\n    test_x = test_x.astype('int')","90840c70":"    i=4\n    user_x = df[(df.visitStartTime>=(time_to.index.astype('int')\/\/10**9)[i]) & (df.visitStartTime<(time_end.index.astype('int')\/\/10**9)[i])]\n    user_y = df[(df.visitStartTime>=time_to.values[i]) & (df.visitStartTime<time_to.values[i+1])]\n    train_x = pd.concat([user_x.groupby('fullVisitorId')[to_median].median().add_suffix('_median'),\n    user_x.groupby('fullVisitorId')['visitStartTime'].agg(['first','last']).add_suffix('_time').sub(time_to.values[i]).abs(),\n    user_x.groupby('fullVisitorId')['visitStartTime'].apply(lambda x: x.max() -x.min()).rename('time_diff'),\n    user_x.groupby('fullVisitorId')[to_sum].sum().add_suffix('_sum'),\n    user_x.groupby('fullVisitorId')[to_mean].mean().add_suffix('_mean'),\n    user_x.groupby('fullVisitorId')[to_std].std(ddof=0).add_suffix('_std')],axis=1).reset_index()\n    \n    merged=train_x.merge(user_y.groupby('fullVisitorId')['totals_transactionRevenue'].sum().reset_index(),\\\n                              how='left', on='fullVisitorId')\n    val_y = merged.totals_transactionRevenue\n    val_y.fillna(0, inplace=True)\n    val_x = merged.drop(['fullVisitorId','totals_transactionRevenue'], axis=1,errors='ignore')\n    val_x = val_x.astype('int')","b6003450":"import lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.metrics import mean_squared_error","374f0568":"params={'learning_rate': 0.01,\n        'objective':'regression',\n        'metric':'rmse',\n        'num_leaves': 31,\n        'verbose': 1,\n        'bagging_fraction': 0.9,\n        'feature_fraction': 0.9,\n        \"random_state\":42,\n        'max_depth': 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"bagging_frequency\" : 5,\n        'lambda_l2': 0.5,\n        'lambda_l1': 0.5,\n        'min_child_samples': 36\n       }\nxgb_params = {\n        'objective': 'reg:linear',\n        'booster': 'gbtree',\n        'learning_rate': 0.02,\n        'max_depth': 22,\n        'min_child_weight': 57,\n        'gamma' : 1.45,\n        'alpha': 0.0,\n        'lambda': 0.0,\n        'subsample': 0.67,\n        'colsample_bytree': 0.054,\n        'colsample_bylevel': 0.50,\n        'n_jobs': -1,\n        'random_state': 456,\n        'importance_type': 'total_gain'\n    }\n\ncat_param = {\n    'learning_rate' :0.03,\n    'depth' :10,\n    'eval_metric' :'RMSE',\n    'od_type' :'Iter',\n    'metric_period ' : 50,\n    'od_wait' : 20,\n    'seed' : 42\n    \n}","ce6ed29b":"oof_reg_preds = np.zeros(val_x.shape[0])\noof_reg_preds1 = np.zeros(val_x.shape[0])\noof_reg_preds2 = np.zeros(val_x.shape[0])\nmerge_pred = np.zeros(val_x.shape[0])\nmerge_preds = np.zeros(val_x.shape[0])\nsub_preds = np.zeros(test_x.shape[0])\nalist = list(range(time_to.shape[0]-1))\nalist.remove(4)\nfolds = alist\nfolds=range(len(alist)-1)\n\nfor i in alist:\n    print(i)\n    user_x = df[(df.visitStartTime>=(time_to.index.astype('int')\/\/10**9)[i]) & (df.visitStartTime<(time_end.index.astype('int')\/\/10**9)[i])]\n    user_y = df[(df.visitStartTime>=time_to.values[i]) & (df.visitStartTime<time_to.values[i+1])]\n    train_x = pd.concat([user_x.groupby('fullVisitorId')[to_median].median().add_suffix('_median'),\n    user_x.groupby('fullVisitorId')['visitStartTime'].agg(['first','last']).add_suffix('_time').sub(time_to.values[i]).abs(),\n    user_x.groupby('fullVisitorId')['visitStartTime'].apply(lambda x: x.max() -x.min()).rename('time_diff'),\n    user_x.groupby('fullVisitorId')[to_sum].sum().add_suffix('_sum'),\n    user_x.groupby('fullVisitorId')[to_mean].mean().add_suffix('_mean'),\n    user_x.groupby('fullVisitorId')[to_std].std(ddof=0).add_suffix('_std')],axis=1).reset_index()\n    \n    merged=train_x.merge(user_y.groupby('fullVisitorId')['totals_transactionRevenue'].sum().reset_index(),\\\n                              how='left', on='fullVisitorId')\n    train_y = merged.totals_transactionRevenue\n    train_y.fillna(0, inplace=True)\n    train_x = merged.drop(['fullVisitorId','totals_transactionRevenue'], axis=1,errors='ignore')\n    train_x = train_x.astype('int')    \n    \n    reg = lgb.LGBMRegressor(**params,n_estimators=1100)\n    xgb = XGBRegressor(**xgb_params, n_estimators=1000)\n    cat = CatBoostRegressor(iterations=1000,learning_rate=0.03,\n                            depth=10,\n                            eval_metric='RMSE',\n                            random_seed = 42,\n                            bagging_temperature = 0.2,\n                            od_type='Iter',\n                            metric_period = 50,\n                            od_wait=20)\n    print(\"-\"* 20 + \"LightGBM Training\" + \"-\"* 20)\n    reg.fit(train_x, np.log1p(train_y),eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,verbose=100,eval_metric='rmse')\n    print(\"-\"* 20 + \"XGboost Training\" + \"-\"* 20)\n    xgb.fit(train_x, np.log1p(train_y),eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,eval_metric='rmse',verbose=100)\n    print(\"-\"* 20 + \"Catboost Training\" + \"-\"* 20)\n    cat.fit(train_x, np.log1p(train_y), eval_set=[(val_x, np.log1p(val_y))],early_stopping_rounds=50,use_best_model=True,verbose=100)\n    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = train_x.columns\n    imp_df['gain_reg'] = np.zeros(train_x.shape[1])\n    imp_df['gain_xgb'] = np.zeros(train_x.shape[1])\n    imp_df['gain_cat'] = np.zeros(train_x.shape[1])\n    imp_df['gain_reg'] += reg.booster_.feature_importance(importance_type='gain')\/ len(folds)\n    imp_df['gain_xgb'] += xgb.feature_importances_\/ len(folds)\n    imp_df['gain_cat'] += np.array(cat.get_feature_importance())\/ len(folds)\n    \n    # LightGBM\n    oof_reg_preds = reg.predict(val_x, num_iteration=reg.best_iteration_)\n    oof_reg_preds[oof_reg_preds < 0] = 0\n    lgb_preds = reg.predict(test_x, num_iteration=reg.best_iteration_)\n    lgb_preds[lgb_preds < 0] = 0\n    \n    \n    # Xgboost\n    oof_reg_preds1 = xgb.predict(val_x)\n    oof_reg_preds1[oof_reg_preds1 < 0] = 0\n    xgb_preds = xgb.predict(test_x)\n    xgb_preds[xgb_preds < 0] = 0\n    \n    # catboost\n    oof_reg_preds2 = cat.predict(val_x)\n    oof_reg_preds2[oof_reg_preds2 < 0] = 0\n    cat_preds = cat.predict(test_x)\n    cat_preds[cat_preds < 0] = 0\n        \n    #merge all prediction\n    merge_pred = oof_reg_preds * 0.4 + oof_reg_preds1 * 0.3 +oof_reg_preds2 * 0.3\n    merge_preds += (oof_reg_preds \/ len(folds)) * 0.4 + (oof_reg_preds1 \/ len(folds)) * 0.3 + (oof_reg_preds2 \/ len(folds)) * 0.3\n    sub_preds += (lgb_preds \/ len(folds)) * 0.4 + (xgb_preds \/ len(folds)) * 0.3 + (cat_preds \/ len(folds)) * 0.3\n    \n    \nprint(\"LGBM  \", mean_squared_error(np.log1p(val_y), oof_reg_preds) ** .5)\nprint(\"XGBoost  \", mean_squared_error(np.log1p(val_y), oof_reg_preds1) ** .5)\nprint(\"CatBoost  \", mean_squared_error(np.log1p(val_y), oof_reg_preds2) ** .5)\nprint(\"merged  \", mean_squared_error(np.log1p(val_y), merge_pred) ** .5)\nprint(\"stack_merged  \", mean_squared_error(np.log1p(val_y), merge_preds) ** .5)\nprint(\"Zeros  \", mean_squared_error(np.log1p(val_y), np.zeros(val_x.shape[0])) ** .5)","f1b62497":"plt.figure(figsize=(8, 12))\nsns.barplot(x='gain_reg', y='feature', data=imp_df.sort_values('gain_reg', ascending=False))","f606810f":"plt.figure(figsize=(8, 12))\nsns.barplot(x='gain_xgb', y='feature', data=imp_df.sort_values('gain_xgb', ascending=False))","6f077aa2":"plt.figure(figsize=(8, 12))\nsns.barplot(x='gain_cat', y='feature', data=imp_df.sort_values('gain_cat', ascending=False))","71e2ce5f":"sub_df = pd.DataFrame(test_ID)\nsub_df[\"PredictedLogRevenue\"] = sub_preds\nsub_df.to_csv(\"stacked_result.csv\", index=False)","385bbbcb":"## Display feature importances","af81b696":"### Valiation data","7c85b090":"* Data are generated from this script : https:\/\/www.kaggle.com\/qnkhuat\/make-data-ready \n* Stacking part is from this script: https:\/\/www.kaggle.com\/ashishpatel26\/updated-bayesian-lgbm-xgb-cat-fe-kfold-cv","f318995a":"### This kernel is mainly made up of three parts:\n* [**1. Data loading**](#Data loading)\n* [**2. Data preprocessing**](#Data preprocessing)\n* [**3. Model building**](#Model building)\n\n###  Main of the kernel:\n*  Chunk whole of data set  by the period as such a structure: \n* [210 days of training period, 45 days of gap period, 2 months of traget perod].  \n* Aggregating data from the training period, ignoring the  gap period, getting the target from the traget period. \n* The valiation set is set to Dec-Jan which is the same monthly period  as the target peroid of the test set.\n\n### Summary:\n In this competition, the data set is so unbalanced that it's hard to say whether our solution can beat all-zeros. After doing some basic EDA, there are some conclusions are for sure: \n\n1. if a customer will pay,  the  transaction will be highly likely happened at the first month, and no longer than two months after the customer shows up in first time. \n2. the minimum of transaction revenue is no less than 1E+07.\n---\n* For the first one, our test set has a 1.5 months' gap between the traget period  which means our taget is divided into two groups: the first  is the one who has already spent no less than 45 days on thinking whether to pay. The second is the  one who has payed for partial services and is going to pay for additional services. To the first group, the customers are terrific unlikely to pay. To the second one, the customers are likely to pay much the same as they payed before. Under those conditions, my prediction of the number of people to pay is 200 or so.\n* For the second one, as we see, the prediction of our model is full of numbers less than 1E+07. But you'll get a worse score if you set those numbers to zero. Our model keeps betting wisely on minimize RMSE but the result keeps leaving away from the real numbers. \n\n### random thoughts:\n* To set a user-defined objective function, which gives a high penalty once the floor level is breached, will be good for avoiding small values.\n* Time features should be under the first priority.\n* To the second group people, if it's possible to specify them by clustering.\n* if the customers wil return after a full year of services are expired?\n* the data set is lack of some important features such as page views of user's website. To the low volume users, why do they pay the bill for advance services if the free account already meets all the needs?","876e4a7e":"### Features to user level\nThere is also a feature called time_diff, which is directly coded in generating part. And this time- relative feature really works well","3a97b166":"### Label encoding","58094cc4":"## Model building","b052ff6e":"## Save result","16f724dc":"## Data preprocessing","be7dbb3a":"### Time period\n* the training set has a 45 days gap to its target set that is same as the test set \n* the training set has almost the same duration as the test set\n* the valiation set is set to Dec-Jan which is the same monthly period  as the target peroid of the test set","dbb6f30e":"### Drop some features and items","7d9fee01":"# Introduction","0a72ad47":"### Test data","d9ab250a":"## Data loading"}}