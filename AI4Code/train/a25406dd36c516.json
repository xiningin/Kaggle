{"cell_type":{"8e678b48":"code","e20a3c33":"code","1f8e3210":"code","19d3ca9d":"code","f8cfd2a7":"code","7b543aec":"code","3f02b5e4":"code","1c580ecb":"code","d51341aa":"code","5deaedd2":"code","c9009601":"code","ca5bfda3":"code","21b34c50":"code","f9bbe44c":"code","553aa397":"code","be12d75f":"code","c481dda0":"code","dc67f878":"code","852c670f":"code","584a3c49":"code","f13a42af":"code","86f80227":"code","28587ae0":"code","c79acd15":"code","0cdebcf5":"code","30cc148f":"code","cb7493fd":"code","070acfe8":"code","2946d435":"code","45c08d24":"code","8d5003d0":"code","0733194a":"code","e40647b7":"code","47b2f9d9":"code","8c8fe730":"code","08b2f988":"code","9588aafe":"code","d2c425f2":"code","ba7928a2":"code","ff141b0d":"code","1e86e680":"markdown","8e18b38e":"markdown","60c14108":"markdown","57f1843a":"markdown","da2450be":"markdown","6ec9d05a":"markdown","68e16958":"markdown","04c8a2a1":"markdown","925f2cb3":"markdown","862e597f":"markdown","1694ea6f":"markdown","a2dca33b":"markdown","363e82eb":"markdown","9f52dabc":"markdown","d6a83044":"markdown","74dbd96d":"markdown","4862500f":"markdown","104f62e7":"markdown","1f20bbf2":"markdown","d7929726":"markdown","88d51ce4":"markdown","195a1826":"markdown","75d48d5d":"markdown","92946865":"markdown","8ba8be4e":"markdown","1fdd02ee":"markdown","745ee37f":"markdown","f0893c3b":"markdown","215916e9":"markdown","24f7b911":"markdown","29a01bca":"markdown","e72e12f0":"markdown","532ea43f":"markdown"},"source":{"8e678b48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e20a3c33":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Set a few plotting defaults\nplt.style.use('ggplot')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\n\npd.options.display.max_rows = 10000\npd.options.display.max_columns = 10000\npd.options.display.max_colwidth = 1000","1f8e3210":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)","19d3ca9d":"def rmse_exp(y_true, y_pred):\n    return np.sqrt(mean_squared_error(np.expm1(y_true), np.expm1(y_pred)))\n\ndef train_test_split(data, do_ohe=True):\n    df = data.drop(['id','price','data'], axis=1).copy()\n    cat_cols = df.select_dtypes('object').columns\n    for col in cat_cols:\n        if do_ohe:\n            ohe_df = pd.get_dummies(df[[col]], prefix='ohe_'+col)\n            df.drop(col, axis=1, inplace=True)\n            df = pd.concat([df, ohe_df], axis=1)\n        else:\n            le = LabelEncoder()\n            df[col] = le.fit_transform(df[col])\n\n    train_len = data[data['data'] == 'train'].shape[0]\n    X_train = df.iloc[:train_len]\n    X_test = df.iloc[train_len:]\n    y_train = data[data['data'] == 'train']['price']\n    \n    return X_train, X_test, y_train\n\ndef get_oof_lgb(X_train, y_train, X_test, lgb_param, verbose_eval=False, return_cv_score_only=False):\n\n    folds = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n    oof = np.zeros(len(X_train))\n    predictions = np.zeros(len(X_test))\n    feature_importance_df = pd.DataFrame()\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(X_train.values, y_train.values)):\n        if verbose_eval > 0: print(f'Fold : {fold_ + 1}')\n        trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n        val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n\n        num_round = 100000\n        clf = lgb.train(lgb_param, trn_data, num_round, valid_sets=[trn_data, val_data],\n                        verbose_eval=verbose_eval, early_stopping_rounds=200)\n        oof[val_idx] = clf.predict(X_train.iloc[val_idx], num_iteration=clf.best_iteration)\n        predictions += clf.predict(X_test, num_iteration=clf.best_iteration) \/ folds.n_splits\n        \n        cv_fold_score = rmse_exp(y_train.iloc[val_idx], oof[val_idx])\n        \n        if verbose_eval > 0: print(f'Fold {fold_ + 1} \/ CV-Score: {cv_fold_score:.6f}')\n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df['feature'] = X_train.columns.tolist()\n        fold_importance_df['importance'] = clf.feature_importance('gain')\n        fold_importance_df['fold'] = fold_ + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n\n    cv_score = rmse_exp(y_train, oof)\n    print(f'CV-Score: {cv_score:.6f}')\n    if return_cv_score_only: return cv_score\n    else: return oof, predictions, cv_score, feature_importance_df\n    \ndef plot_feature_importance(fi_df, num_feature=20):\n    cols = (fi_df[['feature', 'importance']]\n            .groupby('feature')\n            .mean()\n            .sort_values(by='importance', ascending=False)[:num_feature].index)\n    best_features = fi_df.loc[fi_df.feature.isin(cols)]\n\n    sns.barplot(x='importance', y='feature', data=best_features.sort_values(by='importance', ascending=False))\n    plt.title('Feature Importances (averaged over folds)')\n    plt.tight_layout()\n    plt.show()\n    \ndef plot_numeric_for_regression(df, field, target_field='price'):\n    df = df[df[field].notnull()]\n\n    fig = plt.figure(figsize = (16, 7))\n    ax1 = plt.subplot(121)\n    \n    sns.distplot(df[df['data'] == 'train'][field], label='Train', hist_kws={'alpha': 0.5}, ax=ax1)\n    sns.distplot(df[df['data'] == 'test'][field], label='Test', hist_kws={'alpha': 0.5}, ax=ax1)\n\n    plt.xlabel(field)\n    plt.ylabel('Density')\n    plt.legend()\n    \n    ax2 = plt.subplot(122)\n    \n    df_copy = df[df['data'] == 'train'].copy()\n\n    sns.scatterplot(x=field, y=target_field, data=df_copy, ax=ax2)\n    \n    plt.show()\n    \ndef plot_categorical_for_regression(df, field, target_field='price', show_missing=True, missing_value='NA'):\n    df_copy = df.copy()\n    if show_missing: df_copy[field] = df_copy[field].fillna(missing_value)\n    df_copy = df_copy[df_copy[field].notnull()]\n\n    ax1_param = 121\n    ax2_param = 122\n    fig_size = (16, 7)\n    if df_copy[field].nunique() > 30:\n        ax1_param = 211\n        ax2_param = 212\n        fig_size = (16, 10)\n    \n    fig = plt.figure(figsize = fig_size)\n    ax1 = plt.subplot(ax1_param)\n    \n    sns.countplot(x=field, hue='data', order=np.sort(df_copy[field].unique()), data=df_copy)\n    plt.xticks(rotation=90, fontsize=11)\n    \n    ax2 = plt.subplot(ax2_param)\n    \n    df_copy = df_copy[df_copy['data'] == 'train']\n\n    sns.boxplot(x=field, y=target_field, data=df_copy, order=np.sort(df_copy[field].unique()), ax=ax2)\n    plt.xticks(rotation=90, fontsize=11)\n    \n    plt.show()\n    \ndef load_original_data():\n    train = pd.read_csv('..\/input\/train.csv')\n    test = pd.read_csv('..\/input\/test.csv')\n\n    train_copy = train.copy()\n    train_copy['data'] = 'train'\n    test_copy = test.copy()\n    test_copy['data'] = 'test'\n    test_copy['price'] = np.nan\n\n    # remove outlier\n    train_copy = train_copy[~((train_copy['sqft_living'] > 12000) & (train_copy['price'] < 3000000))].reset_index(drop=True)\n\n    # concat train, test data to preprocess\n    data = pd.concat([train_copy, test_copy], sort=False).reset_index(drop=True)\n    data = data[train_copy.columns]\n\n    data.drop('date', axis=1, inplace=True)\n    data['zipcode'] = data['zipcode'].astype(str)\n\n    # fix skew feature\n    skew_columns = ['price']\n\n    for c in skew_columns:\n        data[c] = np.log1p(data[c])\n        \n    return data","f8cfd2a7":"data = load_original_data()\n\nprint(data.shape)\ndata.head()","7b543aec":"X_train, X_test, y_train = train_test_split(data)\nprint(X_train.shape, X_test.shape)\n\nlgb_param = {\n    'objective': 'regression',\n    'learning_rate': 0.05,\n    'num_leaves': 15,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'feature_fraction': 0.7,\n    'seed': RANDOM_SEED,\n    'metric': ['rmse'],\n}\n\noof, pred, cv_score, fi_df = get_oof_lgb(X_train, y_train, X_test, lgb_param)","3f02b5e4":"plot_feature_importance(fi_df)","1c580ecb":"plot_categorical_for_regression(data, 'zipcode')","d51341aa":"df = X_train\ndf['price'] = y_train\n\nfig = plt.figure(figsize = (16, 12))\n\nax1 = plt.subplot(221)\nsns.scatterplot(x='long', y='lat', hue='ohe_zipcode_98004', size='price', data=df, ax=ax1)\n\nax2 = plt.subplot(222)\nsns.scatterplot(x='long', y='lat', hue='ohe_zipcode_98112', size='price', data=df, ax=ax2)\n\nax3 = plt.subplot(223)\nsns.scatterplot(x='long', y='lat', hue='ohe_zipcode_98023', size='price', data=df, ax=ax3)\n\nax4 = plt.subplot(224)\nsns.scatterplot(x='long', y='lat', hue='ohe_zipcode_98108', size='price', data=df, ax=ax4)\n\nplt.show()","5deaedd2":"data = load_original_data()\n\ndata['zipcode-3'] = 'z_' + data['zipcode'].str[2:3]\ndata['zipcode-4'] = 'z_' + data['zipcode'].str[3:4]\ndata['zipcode-5'] = 'z_' + data['zipcode'].str[4:5]\ndata['zipcode-34'] = 'z_' + data['zipcode'].str[2:4]\ndata['zipcode-45'] = 'z_' + data['zipcode'].str[3:5]\ndata['zipcode-35'] = 'z_' + data['zipcode'].str[2:3] + data['zipcode'].str[4:5]\n\nprint(data.shape)\ndata.head()","c9009601":"data['zipcode'] = 'z_' + data['zipcode']\nsns.scatterplot(x='long', y='lat', hue='zipcode', hue_order=np.sort(data['zipcode'].unique()), data=data);","ca5bfda3":"sns.scatterplot(x='long', y='lat', hue='zipcode-3', hue_order=np.sort(data['zipcode-3'].unique()), data=data);","21b34c50":"sns.scatterplot(x='long', y='lat', hue='zipcode-4', hue_order=np.sort(data['zipcode-4'].unique()), data=data);","f9bbe44c":"sns.scatterplot(x='long', y='lat', hue='zipcode-5', hue_order=np.sort(data['zipcode-5'].unique()), data=data);","553aa397":"sns.scatterplot(x='long', y='lat', hue='zipcode-34', hue_order=np.sort(data['zipcode-34'].unique()), data=data);","be12d75f":"sns.scatterplot(x='long', y='lat', hue='zipcode-45', hue_order=np.sort(data['zipcode-45'].unique()), data=data);","c481dda0":"sns.scatterplot(x='long', y='lat', hue='zipcode-35', hue_order=np.sort(data['zipcode-35'].unique()), data=data);","dc67f878":"X_train, X_test, y_train = train_test_split(data)\nprint(X_train.shape, X_test.shape)\n\nlgb_param = {\n    'objective': 'regression',\n    'learning_rate': 0.05,\n    'num_leaves': 15,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'feature_fraction': 0.7,\n    'seed': RANDOM_SEED,\n    'metric': ['rmse'],\n}\n\noof, pred, cv_score, fi_df = get_oof_lgb(X_train, y_train, X_test, lgb_param)","852c670f":"plot_feature_importance(fi_df)","584a3c49":"plot_categorical_for_regression(data, 'zipcode-35')","f13a42af":"plot_categorical_for_regression(data, 'zipcode-5')","86f80227":"# pca for lat, long\ndata = load_original_data()\n\ncoord = data[['lat','long']]\npca = PCA(n_components=2)\npca.fit(coord)\n\ncoord_pca = pca.transform(coord)\n\ndata['coord_pca1'] = coord_pca[:, 0]\ndata['coord_pca2'] = coord_pca[:, 1]","28587ae0":"sns.scatterplot(x='coord_pca2', y='coord_pca1', hue='price', data=data);","c79acd15":"X_train, X_test, y_train = train_test_split(data)\nprint(X_train.shape, X_test.shape)\n\nlgb_param = {\n    'objective': 'regression',\n    'learning_rate': 0.05,\n    'num_leaves': 15,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'feature_fraction': 0.7,\n    'seed': RANDOM_SEED,\n    'metric': ['rmse'],\n}\n\noof, pred, cv_score, fi_df = get_oof_lgb(X_train, y_train, X_test, lgb_param)","0cdebcf5":"plot_feature_importance(fi_df)","30cc148f":"plot_numeric_for_regression(data, 'coord_pca2')\nplot_numeric_for_regression(data, 'coord_pca1')","cb7493fd":"inertia_arr = []\n\nk_range = range(2, 16)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=RANDOM_SEED).fit(coord)\n \n    # Sum of distances of samples to their closest cluster center\n    interia = kmeans.inertia_\n    print (\"k:\",k, \" cost:\", interia)\n    inertia_arr.append(interia)\n    \ninertia_arr = np.array(inertia_arr)\n\nplt.plot(k_range, inertia_arr)\nplt.vlines(5, ymin=inertia_arr.min()*0.9999, ymax=inertia_arr.max()*1.0003, linestyles='--', colors='b')\nplt.title('Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Inertia');","070acfe8":"data = load_original_data()\n\n# kmeans for lat, long\nkmeans = KMeans(n_clusters=5, random_state=RANDOM_SEED).fit(coord)\ncoord_cluster = kmeans.predict(coord)\ndata['coord_cluster'] = coord_cluster\ndata['coord_cluster'] = data['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))\n\nX_train, X_test, y_train = train_test_split(data)\nprint(X_train.shape, X_test.shape)\n\nlgb_param = {\n    'objective': 'regression',\n    'learning_rate': 0.05,\n    'num_leaves': 15,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'feature_fraction': 0.7,\n    'seed': RANDOM_SEED,\n    'metric': ['rmse'],\n}\n\noof, pred, cv_score, fi_df = get_oof_lgb(X_train, y_train, X_test, lgb_param)","2946d435":"k_range = range(2, 80, 5)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=RANDOM_SEED).fit(coord)\n    coord_cluster = kmeans.predict(coord)\n    data['coord_cluster'] = coord_cluster\n    data['coord_cluster'] = data['coord_cluster'].map(lambda x: str(x).rjust(2, '0'))\n    \n    X_train, X_test, y_train = train_test_split(data)\n\n    lgb_param = {\n        'objective': 'regression',\n        'learning_rate': 0.05,\n        'num_leaves': 15,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 1,\n        'feature_fraction': 0.7,\n        'seed': RANDOM_SEED,\n        'metric': ['rmse'],\n    }\n\n    print('K :', k)\n    get_oof_lgb(X_train, y_train, X_test, lgb_param)\n    print()","45c08d24":"k_range = range(28, 37)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=RANDOM_SEED).fit(coord)\n    coord_cluster = kmeans.predict(coord)\n    data['coord_cluster'] = coord_cluster\n    data['coord_cluster'] = data['coord_cluster'].map(lambda x: str(x).rjust(2, '0'))\n    \n    X_train, X_test, y_train = train_test_split(data)\n\n    lgb_param = {\n        'objective': 'regression',\n        'learning_rate': 0.05,\n        'num_leaves': 15,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 1,\n        'feature_fraction': 0.7,\n        'seed': RANDOM_SEED,\n        'metric': ['rmse'],\n    }\n\n    print('K :', k)\n    get_oof_lgb(X_train, y_train, X_test, lgb_param)\n    print()","8d5003d0":"# kmeans for lat, long\nkmeans = KMeans(n_clusters=32, random_state=RANDOM_SEED).fit(coord)\ncoord_cluster = kmeans.predict(coord)\ndata['coord_cluster'] = coord_cluster\ndata['coord_cluster'] = data['coord_cluster'].map(lambda x: 'c_' + str(x).rjust(2, '0'))","0733194a":"sns.scatterplot(x='long', y='lat', hue='coord_cluster', hue_order=np.sort(data['coord_cluster'].unique()), data=data);","e40647b7":"X_train, X_test, y_train = train_test_split(data)\nprint(X_train.shape, X_test.shape)\n\nlgb_param = {\n    'objective': 'regression',\n    'learning_rate': 0.05,\n    'num_leaves': 15,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'feature_fraction': 0.7,\n    'seed': RANDOM_SEED,\n    'metric': ['rmse'],\n}\n\noof, pred, cv_score, fi_df = get_oof_lgb(X_train, y_train, X_test, lgb_param)","47b2f9d9":"plot_feature_importance(fi_df)","8c8fe730":"df = X_train\ndf['price'] = y_train\nsns.scatterplot(x='long', y='lat', hue='ohe_coord_cluster_c_11', data=df);","08b2f988":"plot_categorical_for_regression(data, 'coord_cluster')","9588aafe":"def haversine_array(lat1, lng1, lat2, lng2): \n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2)) \n    AVG_EARTH_RADIUS = 6371 # in km \n    lat = lat2 - lat1 \n    lng = lng2 - lng1 \n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2 \n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d)) \n    return h","d2c425f2":"print(data['lat'].min(), data['lat'].max(), data['long'].min(), data['long'].max())\n\nhaversine_dist = haversine_array(data['lat'].min(), data['long'].min(), data['lat'].max(), data['long'].max())\nprint(f'max distance: {haversine_dist:.2f}km')","ba7928a2":"neighbor_df = pd.DataFrame()\nlat2 = data['lat'].values\nlong2 = data['long'].values\n\nlat1 = data.loc[0, 'lat'] # id = 0 house lat\nlong1 = data.loc[0, 'long'] # id = 0 house long\ndist_arr = haversine_array(lat1, long1, lat2, long2)\nneighbor_df = pd.DataFrame({\n    'id': np.tile(np.array([data.loc[0, 'id']]), data.shape[0]),\n    'neighbor_id': data['id'],\n    'neighbor_lat': lat2,\n    'neighbor_long': long2,\n    'distance': dist_arr,\n})\n    \nprint(neighbor_df.shape)\nneighbor_df.head()","ff141b0d":"neighbor_df['neighbor_10km'] = neighbor_df['distance'] <= 5\nsns.scatterplot(x='neighbor_long', y='neighbor_lat', hue='neighbor_10km', data=neighbor_df);","1e86e680":"PCA\ub294 \ucc28\uc6d0\ucd95\uc18c\uc5d0 \uc8fc\ub85c \uc0ac\uc6a9\ub418\ub294 \uc54c\uace0\ub9ac\uc998\uc785\ub2c8\ub2e4. \uc704\ub3c4, \uacbd\ub3c4\uc758 \ub370\uc774\ud130\ub9cc \ubcf4\uba74 2\ucc28\uc6d0\uc758 \ub370\uc774\ud130\uc778\ub370, \uc774 \ub370\uc774\ud130\ub97c \ucc28\uc6d0\ucd95\uc18c\ub294 \ud558\uc9c0 \uc54a\uace0 2\ucc28\uc6d0 \uadf8\ub300\ub85c PCA Transformation\uc744 \ud558\uba74 \uc6d0\ubcf8 \ub370\uc774\ud130\ub97c \ubcc0\ud615\ud574\uc11c \uc0c8\ub85c\uc6b4 feature\ub97c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","8e18b38e":"\ub610, \ub2e4\ub978 \ubc29\uc2dd\uc73c\ub85c K \uac12\uc744 \uacb0\uc815\ud574\ubcfc \uc218 \ub3c4 \uc788\ub294\ub370\uc694. K-Means Clustering\uc73c\ub85c \ub9cc\ub4e0 feature\ub97c Regression Model\uc5d0\uc11c \uacb0\uad6d \uc0ac\uc6a9\ud558\uae30 \ub54c\ubb38\uc5d0 K \uac12\ub3c4 \ud558\ub098\uc758 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub85c \ubcf4\uace0 CV Score\uac00 \uac00\uc7a5 \ub0ae\uac8c \ub098\uc624\ub294 K\ub97c \uc120\ud0dd\ud574\uc11c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.","60c14108":"* [Baseline Model](#Baseline-Model)\n* Geo Data Feature Engineering\n  * [Zipcode Feature Engineering](#Zipcode-Feature-Engineering)\n  * [PCA Transformation - Lat, Long](#PCA-Transformation---Lat,-Long)\n  * [K-Means Clustering - Lat, Long](#K-Means-Clustering---Lat,-Long)\n    * [Determine K by Elbow method](#Determine-K-by-Elbow-method)\n    * [Determine K by CV Score](#Determine-K-by-CV-Score)\n  * [Haversine Distance](#Haversine-Distance)\n* [Conclusion](#Conclusion)","57f1843a":"Feature Importance\ub97c \ubcf4\uba74 zipcode\uc758 98004, 98023, 98112, 98108\uc774 \uc911\uc694\ud55c feature\ub85c \ub098\uc624\ub294\ub370\uc694.\n\nzipcode\uc640 price\uc758 boxplot\uc744 \uadf8\ub824\ubcf4\uba74 98004, 98112\ub294 \uc9d1\uac12\uc774 \ube44\uc2fc \uc9c0\uc5ed 98023, 98108\uc740 \uc9d1\uac12\uc774 \ub0ae\uc740 \uc9c0\uc5ed\uc784\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","da2450be":"\ub9c8\uc9c0\ub9c9\uc73c\ub85c Haversine Distance\ub97c \uc0ac\uc6a9\ud55c feature \uc0dd\uc131\uc785\ub2c8\ub2e4. Haversine Distance\ub294 \ub450 \uac1c\uc758 \uc704\ub3c4, \uacbd\ub3c4 \uc88c\ud45c\uc5d0\uc11c \uc9c0\uad6c\uc758 \uace1\ub960\uc744 \uace0\ub824\ud574 \ub450 \uc88c\ud45c \uac04\uc758 \uac70\ub9ac\ub97c \uad6c\ud558\ub294 \ubc29\ubc95\uc785\ub2c8\ub2e4.\n\n\uc544\ub798\uc758 function\uc73c\ub85c Haversine Distance\ub97c \uad6c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","6ec9d05a":"\uc8fc\uc5b4\uc9c4 \ub370\uc774\ud130\uc5d0\uc11c \uc704\ub3c4, \uacbd\ub3c4\uc758 \ucd5c\uc18c\uac12\uacfc, \ucd5c\ub300\uac12 \ub450 \uac1c\uc758 \uc88c\ud45c\uc5d0 \ub300\ud574 \uac70\ub9ac\ub97c \uad6c\ud574\ubcf4\uba74 113.88km\uac00 \ub098\uc624\ub294\ub370, \uac00\uc7a5 \uba3c \uac70\ub9ac\uc758 \uac70\ub9ac\uac00 \uc774 \uc815\ub3c4\ub77c\ub294 \uc598\uae30\uac00 \ub418\uaca0\ub124\uc694.","68e16958":"Cluster\uc758 boxplot\uc744 \ud655\uc778\ud574\ubcf4\uba74 11\ubc88 cluster\uac00 \uc9d1\uac12\uc774 \uac00\uc7a5 \ube44\uc2fc \uc9c0\uc5ed\uc784\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","04c8a2a1":"# Baseline Model","925f2cb3":"# Zipcode Feature Engineering","862e597f":"Zipcode\ub294 \ub2e4\uc12f\uc790\ub9ac\ub85c \ub41c \uc22b\uc790\ud615\uc758 \ub370\uc774\ud130\uc778\ub370, \ub370\uc774\ud130\ub97c \ubcf4\uba74 \uc55e\uc758 \ub450 \uc790\ub9ac\ub294 98\ub85c \ub3d9\uc77c\ud569\ub2c8\ub2e4.\n\n\uadf8\ub798\uc11c \ub4a4\uc758 \uc138 \uc790\ub9ac\uc758 \uc22b\uc790\ub97c \uc5ec\ub7ec\uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \ucabc\uac1c\uc11c \uc0c8\ub85c\uc6b4 feature\ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.  ","1694ea6f":"# [Haversine Distance](https:\/\/en.wikipedia.org\/wiki\/Haversine_formula)","a2dca33b":"\uc704\ub3c4(Lat), \uacbd\ub3c4(Long) \ub370\uc774\ud130\ub97c \uae30\ubc18\uc73c\ub85c\ud55c Feature Engineering\uc740 \uc544\ub798 \uae00\uc744 \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4.\n\n[Good Feature Building Techniques\u200a\u2014\u200aTricks for Kaggle\u200a\u2014\u200aMy Kaggle Code Repository](https:\/\/becominghuman.ai\/good-feature-building-techniques-tricks-for-kaggle-my-kaggle-code-repository-c953b934f1e6)","363e82eb":"# Conclusion\n\n\uc9c0\uae08\uae4c\uc9c0 \uc704\uce58 \ub370\uc774\ud130\ub97c \ud65c\uc6a9\ud55c Feature Engineering\uc744 \uc0b4\ud3b4\ubd24\uc2b5\ub2c8\ub2e4. \ud2b9\ud788, \ub9c8\uc9c0\ub9c9\uc758 Haversine Distance\ub97c \ud65c\uc6a9\ud558\uba74 \uac01 \uc9d1\uc5d0\uc11c \uac00\uae4c\uc6b4 \uc774\uc6c3\uc9d1\uc744 \uacc4\uc0b0\ud560 \uc218 \uc788\uace0, \uc774\ub97c \ubc14\ud0d5\uc73c\ub85c \ub2e4\uc591\ud55c feature\ub97c \ub9cc\ub4e4\uc5b4 \ub0bc \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc800\uac19\uc740 \uacbd\uc6b0 \uc774\ub7ec\ud55c Geo Data Feature Engineering\uacfc Stacking Ensemble\uc744 \ud1b5\ud574 \ud37c\ube14\ub9ad \ub9ac\ub354\ubcf4\ub4dc \uae30\uc900\uc73c\ub85c RMSE \uc2a4\ucf54\uc5b4\uac00 96000\ub300\uae4c\uc9c0 \uc131\ub2a5\uc774 \ub098\uc624\ub294 \ubaa8\ub378\uc744 \ub9cc\ub4e4 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4. \uc81c\uac00 \uacf5\uc720\ud55c \ub0b4\uc6a9\uc774 \ub2e4\ub978 \ubd84\ub4e4\uc5d0\uac8c\ub3c4 \ub3c4\uc6c0\uc774 \ub410\uc73c\uba74 \uc88b\uaca0\ub124\uc694.\n\n\uac10\uc0ac\ud569\ub2c8\ub2e4.","9f52dabc":"\uc544\ub798\ub294 0\ubc88 \uc9d1\uc758 \ubc18\uacbd 5km \uc774\ub0b4\uc758 \uc774\uc6c3\ub4e4\uc744 \uadf8\ub824\ubcf8 \uadf8\ub798\ud504\uc785\ub2c8\ub2e4. ","d6a83044":"\uc544\ub798\ub294 id\uac00 0\uc778 \uc9d1\uacfc \uc804\uccb4 \uc9d1\uacfc\uc758 \uac70\ub9ac\ub97c \uad6c\ud55c \ub370\uc774\ud130\uc785\ub2c8\ub2e4. 0\ubc88 \uc9d1\uacfc \uac00\uae4c\uc6b4 \uc774\uc6c3\uc740 \uc9d1\uac12\uc774 \ube44\uc2b7\ud560 \ud655\ub960\uc774 \ub192\uae30 \ub54c\ubb38\uc5d0 \uac70\ub9ac\ub97c \uae30\ubc18\uc73c\ub85c \uac00\uae4c\uc6b4 \uc774\uc6c3\uc9d1\uc758 \ub370\uc774\ud130\ub97c \ucc3e\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub7f0 \uc2dd\uc73c\ub85c \uc804\uccb4 \uc9d1\ub4e4\uc758 Haversine Distance\ub97c \uad6c\ud558\uba74 \ub098\uc640 \uac00\uae4c\uc6b4 \uc774\uc6c3\uc9d1\ub4e4\uc758 \uc815\ubcf4\ub97c \uc0c8\ub85c\uc6b4 feature\ub85c \ud65c\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","74dbd96d":"# K-Means Clustering - Lat, Long","4862500f":"Feature Importance\ub97c \ubcf4\uba74 zipcode-35\uc758 18, zipcode-5\uc758 8\uc774 \uc911\uc694\ud55c feature\ub85c \ub098\uc624\uace0 boxplot\uc744 \uadf8\ub824\ubcf4\uba74 \uc9d1\uac12\uc774 \uc2fc \uc9c0\uc5ed\uc784\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","104f62e7":"\uae30\ubcf8\uc801\uc778 \ucf54\ub4dc\ub294 \uc544\ub798\uc758 \ucee4\ub110\ub4e4\uc744 \ub9ce\uc774 \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4.\n* https:\/\/www.kaggle.com\/chocozzz\/house-price-prediction-eda-updated-2019-03-12\n* https:\/\/www.kaggle.com\/yeonmin\/default-eda-stacking-introduction","1f20bbf2":"98004, 98112, 98023, 98108 \uc9c0\uc5ed\uc774 \uc5b4\ub290 \uc704\uce58\uc5d0 \uc788\ub294\uc9c0 \uc704\ub3c4, \uacbd\ub3c4 \ub370\uc774\ud130\ub97c \ud1b5\ud574 \ud655\uc778\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","d7929726":"K \uac12\uc774 32\uc77c \ub54c CV Score\uac00 \uac00\uc7a5 \uc88b\uac8c \ub098\uc624\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. Elbow Method\ub85c K \uac12\uc744 \uc120\ud0dd\ud588\uc744 \ub54c\ubcf4\ub2e4 CV Score \uae30\uc900\uc73c\ub85c K\ub97c \uc120\ud0dd\ud560 \ub54c\uac00 \ub354 \ubaa8\ub378 \uc131\ub2a5\uc774 \uc88b\uae30 \ub54c\ubb38\uc5d0 \ucd5c\uc885 K \uac12\uc740 32\ub85c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4.  ","88d51ce4":"\uc0c8\ub86d\uac8c \ub9cc\ub4e0 zipcode feature\ub85c CV Score\uac00 \ub354 \uc88b\uc544\uc9c4 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","195a1826":"Feature Importance\ub97c \ubcf4\uba74 11\ubc88 cluster\uac00 \uc911\uc694\ud55c feature\uc784\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798 \uadf8\ub798\ud504\uc5d0\uc11c 11\ubc88 cluster\uc758 \uc704\uce58\ub97c \ud655\uc778\ud574\ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.","75d48d5d":"\uc0c8\ub86d\uac8c \ub9cc\ub4e0 zipcode feature\ub97c \uc9c0\ub3c4\uc5d0\uc11c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","92946865":"\uc704\uc758 \uadf8\ub798\ud504\ub97c \ubcf4\uba74 5\uac00 \uac00\uc7a5 \uc801\uc808\ud55c K \uac12\uc73c\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4.","8ba8be4e":"# Determine K by Elbow method","1fdd02ee":"LightGBM 5-Fold Out Of Fold Prediction\uc744 Baseline model\ub85c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\n\uae30\ubcf8 \ub370\uc774\ud130\uc758 Baseline model CV \uc2a4\ucf54\uc5b4\ub294 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4. Categorical \ub370\uc774\ud130\ub294 One Hot Encoding\uc73c\ub85c \ucc98\ub9ac\ud558\uaca0\uc2b5\ub2c8\ub2e4.","745ee37f":"K-Means Clustering\uc740 Clustering\uc5d0 \uc0ac\uc6a9\ub418\ub294 \ube44\uc9c0\ub3c4\ud559\uc2b5 \uc54c\uace0\ub9ac\uc998 \uc911\uc758 \ud558\ub098\uc785\ub2c8\ub2e4. \uc704\ub3c4, \uacbd\ub3c4 \ub370\uc774\ud130\ub97c K-Means Clustering \ud558\uba74 \uac00\uae4c\uc6b4 \uc9c0\uc5ed\ub07c\ub9ac Cluster\uac00 \ub9cc\ub4e4\uc5b4\uc9c0\uae30 \ub54c\ubb38\uc5d0 zipcode\uc640 \uc720\uc0ac\ud55c \uac1c\ub150\uc758 \uc0c8\ub85c\uc6b4 feature\ub97c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","f0893c3b":"PCA\ub97c \ud1b5\ud574 \ub9cc\ub4e0 feature\ub85c CV Score\uac00 \ub354 \uc88b\uc544\uc9c4 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uace0, \uc544\ub798 feature importance\uc5d0\uc11c\ub3c4 \uc911\uc694\ud55c feature\ub85c \ub098\uc624\ub294 \uac83\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","215916e9":"\uc774\ub807\uac8c \ub9cc\ub4e4\uc5b4\uc9c4 feature\ub97c 2\ucc28\uc6d0\uc5d0 \uadf8\ub824\ubcf4\uba74 \uc6d0\ubcf8 \ub370\uc774\ud130\uac00 \ubcc0\ud615\ub418\uc11c \uc0c8\ub85c\uc6b4 feature\ub85c \ub9cc\ub4e4\uc5b4\uc9c4 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.","24f7b911":"# PCA Transformation - Lat, Long","29a01bca":"# Geo Data EDA & Feature Engineering\n\n\uc548\ub155\ud558\uc138\uc694. \uce90\uae00 \ucf54\ub9ac\uc544\uc5d0\uc11c \uc8fc\ucd5c\ud55c 2\ud68c \ub300\ud68c\ub97c \uc990\uac81\uac8c \ud558\uace0 \uc788\ub294 \uc0ac\ub78c\uc73c\ub85c\uc11c \uc81c\uac00 \uc0ac\uc6a9\ud558\uace0 \uc788\ub294 Feature Engineering\uc744 \uacf5\uc720\ud558\ub824\uace0 \ud569\ub2c8\ub2e4.\n\n\uc9d1\uac12\uc744 \uacb0\uc815\ud558\ub294\ub370 \uac00\uc7a5 \uc911\uc694\ud55c \uc694\uc18c\ub294 \uc785\uc9c0\ub77c\ub294 \uac83\uc740 \ubaa8\ub450 \uc798 \uc544\uc2e4\uac70\ub77c\uace0 \uc0dd\uac01\ud569\ub2c8\ub2e4. \uadf8\ub798\uc11c \uc800\ub294 \uae30\ubcf8 \ub370\uc774\ud130\uc5d0 \ub300\ud55c Feature Engineering \uc678\uc5d0 \uc704\uce58\uc5d0 \uad00\ub828\ub41c Feature Engineering\uc5d0 \uac00\uc7a5 \uc2dc\uac04\uc744 \uc3df\uace0 \uc788\ub294\ub370\uc694.\n\n\uae30\ubcf8 \ub370\uc774\ud130\uc5d0 \ub300\ud55c EDA \ubc0f Feature Engineering\uc740 \ub2e4\ub978 \ubd84\ub4e4\uc774 \uacf5\uac1c\ud55c \uc88b\uc740 \ucee4\ub110\uc774 \ub9ce\uc774 \uc788\uc73c\ubbc0\ub85c \uc0dd\ub7b5\ud558\uace0, \uc704\uce58\uc5d0 \uad00\ub828\ub41c zipcode, lat, long \ub370\uc774\ud130\uc758 Feature Engineering\uc5d0 \uc9d1\uc911\ud574\uc11c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.\n\n\ubcf4\uc2dc\uace0 \ub3c4\uc6c0\uc774 \ub410\ub2e4\uba74 \uc81c Kernel\uc5d0 Vote \ud574\uc8fc\uc2dc\uba74 \uac10\uc0ac\ud558\uaca0\uc2b5\ub2c8\ub2e4! ","e72e12f0":"K-Means Clustering\uc5d0\uc11c \uc911\uc694\ud55c \uac74 K \uac12\uc744 \uc5b4\ub5bb\uac8c \uacb0\uc815\ud558\ub290\ub0d0 \uc778\ub370\uc694. \uc77c\ubc18\uc801\uc73c\ub85c \ub9ce\uc774 \uc4f0\uc774\ub294 \ubc29\ubc95\uc740 K \uac12\uc744 \ub298\ub824\uac00\uba74\uc11c \uc5ec\ub7ec \uac1c \ub3cc\ub824\ubcf4\uba74, Cluster \uac04\uc758 \uac70\ub9ac\uc758 \ud569\uc744 \ub098\ud0c0\ub0b4\ub294 inertia\uac00 \uae09\uaca9\ud788 \ub5a8\uc5b4\uc9c0\ub294 \uad6c\uac04\uc774 \uc0dd\uae30\ub294\ub370 \uc774 \uc9c0\uc810\uc758 K \uac12\uc744 \ub9ce\uc774 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\ub7f0 \ubc29\uc2dd\uc744 Elbow Method\ub77c\uace0 \ud569\ub2c8\ub2e4. \uba3c\uc800 Elbow Method\ub97c \ud1b5\ud574 K \uac12\uc744 \uacb0\uc815\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","532ea43f":"# Determine K by CV Score"}}