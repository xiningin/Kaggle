{"cell_type":{"fb58678a":"code","3d13cd80":"code","a6d43cee":"code","cd401038":"code","31efc4c4":"code","e748ce5d":"code","eae8a0b9":"code","7bb9f072":"code","9c11fd10":"code","a85bdcfa":"code","b39e95f2":"code","23d0e2df":"code","de28fadc":"code","c73cc59e":"code","80709f4f":"code","1505cd79":"code","2ab7cd59":"code","85d302d3":"code","f312664a":"code","283a8fe3":"code","03494b99":"code","86508867":"code","ae355dcd":"code","b19701d9":"code","e9ce5821":"code","d67b59cb":"code","3bef7f2b":"code","58257832":"code","1775671b":"code","14b87041":"code","7a852847":"code","a529237d":"code","05446beb":"code","011362d1":"code","8b24f2dd":"code","1ec5916b":"markdown","996a5ffc":"markdown","81efc9e2":"markdown","3552f3f5":"markdown","b57e8455":"markdown","9ef5a1ad":"markdown","37438bfb":"markdown","6c8020b6":"markdown","d462a046":"markdown","21a330c2":"markdown","9a0d814e":"markdown","7bb1e92a":"markdown","f04260da":"markdown","58a5efdc":"markdown","84a92de7":"markdown","4acbe7dd":"markdown","2f5787a6":"markdown","9370d9a0":"markdown","f81be26f":"markdown","a1b66f48":"markdown","f1c2c3c4":"markdown","c20956ed":"markdown","ef64e3dd":"markdown","71ff54cd":"markdown","de7c0677":"markdown","40826bea":"markdown","437ebdae":"markdown","b7cd167c":"markdown","f8922fa5":"markdown","1b672e31":"markdown","d96f2884":"markdown","95876d7b":"markdown","3bf452fd":"markdown","54944260":"markdown","a21fd5e0":"markdown","d07b71b1":"markdown","5ec0e1d6":"markdown"},"source":{"fb58678a":"import pandas as pd\nimport numpy as np\n\nSEED = 42 # for reproducability\n\nimport warnings\nwarnings.filterwarnings('ignore') # ignoring any warnings","3d13cd80":"TRAIN_PATH = '..\/input\/home-data-for-ml-course\/train.csv'\n\ntrain = pd.read_csv(TRAIN_PATH)","a6d43cee":"train.shape","cd401038":"train.head()","31efc4c4":"cat_columns = list(train.select_dtypes(include='object').columns)\nprint(len(cat_columns), 'Categorical columns')\nprint(train.shape[1] - len(train.select_dtypes(include='object').columns), 'Numerical columns')","e748ce5d":"train['SalePrice'].describe()","eae8a0b9":"import matplotlib.pyplot as plt\n\n# Histogram\nplt.figure(figsize=(6,6))\nplt.hist(train['SalePrice'], bins=30, color='salmon')\nplt.title('Sale Price Histogram', fontdict={'fontsize': 18}, pad = 20)\nplt.xlabel('Price (US Dollars)')\nplt.show()","7bb9f072":"# Pie Chart\n\nplt.figure(figsize=(5,5))\nlabels = ['< $200,000', '> $200,000']\nsizes = [len(train[train['SalePrice'] < 200000]), \n         len(train[train['SalePrice'] > 200000])]\n\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', explode=[0,0.08], startangle=90)\nplt.title('Price Distribution', fontdict={'fontsize': 16})\nplt.axis('equal')\nplt.show()","9c11fd10":"print('There are', len(train[train['SalePrice'] == 200000]), 'Houses that are exactly $200,000')","a85bdcfa":"corr_matrix = train.corr()\ncorr_matrix['SalePrice'].sort_values(ascending=False).head(15)","b39e95f2":"train = train[train['SalePrice'] != 200000]","23d0e2df":"target = train['SalePrice'].apply(lambda x: 1 if x > 200000 else 0)","de28fadc":"features = train.drop(columns=['SalePrice','Id'])","c73cc59e":"from sklearn.model_selection import train_test_split","80709f4f":"x_train,x_test,y_train,y_test = train_test_split(features,\n                                                 target,\n                                                 test_size=0.2,\n                                                 random_state=SEED)","1505cd79":"x_train[x_train.columns[x_train.isna().any()].tolist()].info()","2ab7cd59":"# Drop columns that don't have much data\n\ncolumns_to_drop = ['MiscFeature', 'PoolQC', 'Fence', 'FireplaceQu', 'Alley', 'LotFrontage']\n\nx_train = x_train.drop(columns=columns_to_drop)\nx_test = x_test.drop(columns=columns_to_drop) # make sure to do same thing to test set as well\n\nfor i in columns_to_drop: # remove any dropped column from the category columns list, since I'm going to use it later\n  if i in cat_columns:\n    cat_columns.remove(i)","85d302d3":"columns_to_impute = ['MasVnrType','MasVnrArea','BsmtQual','BsmtCond','BsmtExposure','Electrical','BsmtFinType1',\n                     'BsmtFinType2','GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond'] ","f312664a":"# Impute Garage Year Built nulls with mean\n\nx_train['GarageYrBlt'] = x_train['GarageYrBlt'].fillna(np.mean(x_train['GarageYrBlt']))\nx_test['GarageYrBlt'] = x_test['GarageYrBlt'].fillna(np.mean(x_train['GarageYrBlt']))\n\n# Impute 'MasVnrArea' nulls with 0.0\n\nx_train['MasVnrArea'] = x_train['MasVnrArea'].fillna(0.0)\nx_test['MasVnrArea'] = x_test['MasVnrArea'].fillna(0.0)","283a8fe3":"# Imputing\n\nfrom sklearn.impute import SimpleImputer\n\nimp = SimpleImputer(strategy='most_frequent').fit(x_train)\n\nx_train_imp = pd.DataFrame(imp.transform(x_train), index = x_train.index, columns = x_train.columns)\nx_test_imp = pd.DataFrame(imp.transform(x_test), index = x_test.index, columns = x_test.columns)","03494b99":"import category_encoders as ce","86508867":"# Encoding categorical values\n\nenc = ce.target_encoder.TargetEncoder(cols=cat_columns).fit(x_train_imp,y_train)\n\nx_train_enc = enc.transform(x_train_imp)\nx_test_enc = enc.transform(x_test_imp)","ae355dcd":"# Scaling the data\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nx_train_scaled = pd.DataFrame(scaler.fit_transform(x_train_enc), index=x_train_enc.index, columns=x_train_enc.columns)\nx_test_scaled = pd.DataFrame(scaler.transform(x_test_enc), index=x_test_enc.index, columns=x_test_enc.columns)","b19701d9":"from sklearn.metrics import roc_auc_score\n\npreds = [] # holds test predictions\nauc_list = [] # holds auc scores\n\n# Function to train model, make test data predictions, and print AUC score\ndef fit_and_score(model):\n  model.fit(x_train_scaled,y_train)\n  temp_preds = np.array(model.predict_proba(x_test_scaled))[:,1]\n  preds.append(temp_preds)\n  print(type(model), 'AUC score: ', roc_auc_score(y_test,temp_preds))\n  auc_list.append(round(roc_auc_score(y_test,temp_preds),5))","e9ce5821":"from xgboost import XGBClassifier\n\nmodel_1 = XGBClassifier(random_state=SEED)\n\nfit_and_score(model_1)","d67b59cb":"from sklearn.linear_model import LogisticRegression\n\nmodel_2 = LogisticRegression(random_state=SEED)\n\nfit_and_score(model_2)","3bef7f2b":"from sklearn.svm import SVC\n\nmodel_3 = SVC(random_state = SEED, probability=True)\n\nfit_and_score(model_3)","58257832":"avg_preds = np.mean(preds,axis=0)\nauc_list.append(round(roc_auc_score(y_test,avg_preds),5))\nprint('Combined AUC score: ', roc_auc_score(y_test,avg_preds))","1775671b":"model_list = ['XGBoost','Logistic Regression','Support Vector Machine','Average Ensemble']\n\n\nmodels_ranked_df = pd.DataFrame(data={'Model': model_list,'AUC Score': auc_list})\nmodels_ranked_df.sort_values(by='AUC Score', ascending = False)","14b87041":"from sklearn import metrics\nimport seaborn as sn","7a852847":"### Plot confusion matrix heat map\n\ndata = metrics.confusion_matrix(y_test, [int(round(i)) for i in avg_preds]) # Need to round each probability to get class\ndf_cm = pd.DataFrame(data, columns = [0,1], index = [0,1])\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\nplt.figure(figsize = (7,5))\nsn.set(font_scale=1.3)\nsn.heatmap(df_cm, cmap = \"Blues\", annot = True, annot_kws = {\"size\": 16})","a529237d":"print(metrics.classification_report(y_test,[int(round(i)) for i in avg_preds]))","05446beb":"# Plot ROC AUC graph\n\nfpr, tpr, threshold = metrics.roc_curve(y_test, avg_preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.figure(figsize=(7,7))\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","011362d1":"features_ranked_df = pd.DataFrame(data={'feature': x_train_scaled.columns, \n                                        'importance': model_1.feature_importances_}\n                                  ).sort_values(by='importance', ascending = False)","8b24f2dd":"features_ranked_df[:10]","1ec5916b":"It looks like there are 1,460 instances with 81 total columns. I'm going to take a look at a few rows to understand the data better.","996a5ffc":"Although we have the same number of misclassifications for each class, our f1 scores vary because of how many instances there were in each class. Our precision and recall are very high for class 0 (under \\$200,000) because there were many more to classify correctly. And this is the reason our f1-score is a bit lower for class 1.","81efc9e2":"Now I'm going to look at the target variable, 'SalePrice'.","3552f3f5":"**Hunter Mitchell**\n\n**October 18th, 2020**","b57e8455":"Now there are no more null values, so we can encode the categorical columns","9ef5a1ad":"It looks like there's a lot of categorical data. Let's see how many categorical columns and numerical columns there are.","37438bfb":"Now I am going to check for null values in the data. If a column has a lot of missing data, I am going to drop the whole column, as it will not carry much information.","6c8020b6":"And for the categorical columns, I am going to fill the null values with the most frequent category","d462a046":"The features are every column other than the Id and Price","21a330c2":"There are a bit more categorical columns than numerical. Before the data can be fed into the model, all the categorical columns will need to be encoded.","9a0d814e":"Now create target column containing a 1 if the price is greater than $200,000, and a 0 if it is less.","7bb1e92a":"# Evaluation","f04260da":"Since this is typically a regression problem, we can get some insight into what features will probably be most useful by looking at how they correlate to sales price.","58a5efdc":"Lastly, I'm going to scale the data using a standard scaler.","84a92de7":"Looks like the ensemble only misclassified 10 instances. It also misclassified the same amount under \\$200,000  as over \\$200,000 which is good. Clearly, class 0 is much darker because the majority of instances are under $200,000","4acbe7dd":"# Prepare Data","2f5787a6":"The models all ended up performing very well. After all, it is not too complicated of a task, especially with how many features there are. The model's performance can be verified further by testing it on more data - there were only 74 houses over $200,000 in the test data. \n\nIt could also be improved in several different ways. First and foremost is training with more data. Another idea would be to train more models. There are several that weren't mentioned that could be added to the ensemble. I also did not attempt to optimize each models' hyperparameters. Implementing grid search or random search would help each model perform better. I also didn't play around with the features much. The models might perform better with some features deleted or created. Lastly, performing other ensemble techniques like bagging and boosting would certainly help classify those last few intances correctly.\n\nOverall, I am happy with the results I achieved and I now have a model which can very accurately classify higher selling houses from lower selling houses. I also now know what contributes most to these classifications.","9370d9a0":"# EDA","f81be26f":"It turns out just over 70% of the houses are less than \\$200,000. \n\nThere could also be houses that are exactly \\$200,000.","a1b66f48":"And plot the distribution","f1c2c3c4":"As for the other columns with missing values, I am going to impute the values depending on the column. I could just delete those rows instead, but that might get rid of important information.","c20956ed":"**Problem Statement:**\n\nThe objective is to construct a model which accurately predicts whether a house will sell at above or below \\$200,000. This is a binary classification problem and will be approached using Machine Learning and Data Science fundamentals.","ef64e3dd":"Before I do anything else, I am going to create a test set to verify the model's performance on data it has never seen before.","71ff54cd":"First, drop instances of exactly $200,000 as discussed already.","de7c0677":"I'm choosing to try three different models which generally perform well with binary classification tasks. These are XGBoost, Logistic Regression, and Support Vector Machine.","40826bea":"The instructions state to only classify houses as above or below $200,000, so I am going to drop these rows when I clean the data.","437ebdae":"The ROC curve is near perfect, with only a tiny bit of area not being captured. \n\nThe XGBoost model also shows how important each feature was in it's decisions, so I'll finish off by looking at that.","b7cd167c":"Comparing AUC Scores by model","f8922fa5":"# Training","1b672e31":"The category encoders library comes with several different encoders. I decided to use Target Encoding, since it generally performs well, and doing something like OneHot Encoding would create a ton of extra columns. ","d96f2884":"I'm also going to average these three models' predictions to see if they can perform better together","95876d7b":"0.99 is a really good Area Under the Curve! This means there are only a few instances that the model misclassifies. Each model by themself performed very well too, with Support Vector Machine slightly doing the best. Now I will graph a confusion matrix to see how many instances the ensemble model misclassified. ","3bf452fd":"Start by importing necessary libraries to explore the data","54944260":"For the numerical features, it looks like 'OverallQual' and 'GrLivArea' will most likely impact the classifications the most, since they correlate highest to sale price","a21fd5e0":"There are two numerical columns and the rest are categorical. For the 'MasVnrArea' column, I am going to fill the null values with 0, since these instances most likely don't have a masonry veneer area at all. For the garage year built column, I am going to fill the null values with the mean.","d07b71b1":"These results are a bit interesting. I expected OverallQual to be number one since it was so highly correlated to SalePrice. Furthermore, the top 3 are all Quality type features, and together hold about 50% of the overall importance. It turns out the categorical columns were very important as well!","5ec0e1d6":"The target variable seems to be normally distributed but perhaps skewed right a bit. It also looks like there are more instances that are under $200,000 than over. I'll plot this as a pie chart as well."}}