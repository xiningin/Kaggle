{"cell_type":{"7d8aa7e6":"code","146b5ab3":"code","c70c29a1":"code","6b526540":"code","19fcc121":"code","f9fe1735":"code","77f0e24a":"code","22281e4a":"code","96371b0e":"code","0415fdbd":"code","ac1f0558":"code","286af75f":"code","0972eeff":"code","1364dd86":"code","52803bf1":"code","2a2349c4":"code","b50e0dfa":"code","b6da544a":"code","f5f66cdb":"markdown","2f7ef378":"markdown","e69598d0":"markdown","8873d3ff":"markdown","9365a69b":"markdown","c4f949d6":"markdown","91d9b499":"markdown","607535eb":"markdown","223c8fe2":"markdown"},"source":{"7d8aa7e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","146b5ab3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt","c70c29a1":"titanic_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_data.head()","6b526540":"titanic_data.isnull().sum()","19fcc121":"titanic_data = titanic_data.drop(columns = \"Cabin\", axis = 1)","f9fe1735":"titanic_data['Age'].fillna(titanic_data['Age'].mean(),inplace=True)","77f0e24a":"print(titanic_data[\"Embarked\"].mode())","22281e4a":"titanic_data[\"Embarked\"].fillna(titanic_data[\"Embarked\"].mode()[0],inplace=True)","96371b0e":"titanic_data.dtypes","0415fdbd":"titanic_data.replace({\"Sex\":{\"male\":0,\"female\":1},\"Embarked\":{\"S\":0,\"C\":1,\"Q\":2}},inplace=True)","ac1f0558":"# titanic_data = titanic_data.drop(columns = ['PassengerId','Name','Ticket','Survived'], axis = 1)","286af75f":"X = titanic_data.drop(columns = ['PassengerId','Name','Ticket','Survived'],axis=1)\nY = titanic_data['Survived']","0972eeff":"X.head()","1364dd86":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 2)","52803bf1":"model = LogisticRegression(solver = \"liblinear\")\nmodel.fit(X_train, Y_train)\n","2a2349c4":"X_train_prediction = model.predict(X_train)\ntraining_data_accuracy = accuracy_score(Y_train,X_train_prediction)\nprint(\"Accuracy score of training data: \", training_data_accuracy)","b50e0dfa":"X_test_prediction = model.predict(X_test)\ntest_data_accuracy = accuracy_score(Y_test, X_test_prediction)\nprint('Accuracy score of test data : ', test_data_accuracy)","b6da544a":"# prediction = model.predict(input_data_reshaped)\n# #print(prediction)\n# if prediction[0]==0:\n#     print(\"Dead\")\n# if prediction[0]==1:\n#     print(\"Alive\")","f5f66cdb":"> Finding the mode value of the \"Embarked\" column as it will have occured the maximum number of times","2f7ef378":"> Replacing the missing values in the \"Age\" column with mean Value","e69598d0":"**\"Sex\", \"Ticket\", \"Embarked\" are object type i.e. String type**\n\n# Transformation into a Categorical Column","8873d3ff":"> Replacing the missing Values in the \"Embarked\" column with mode Value","9365a69b":"# Checking the accuracy","c4f949d6":"# Importing dependencies","91d9b499":"> dropping of insignificant columns","607535eb":"# Logistics Regression","223c8fe2":"# Handling the missing values\n\n> Droping the \"cabin\" column from the dataset as it is not of much importance due to high null value"}}