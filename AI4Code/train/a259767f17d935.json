{"cell_type":{"342e9678":"code","715eaba2":"code","6b4743a6":"code","ca1be7e7":"code","4555d013":"code","cfa88f30":"code","cb0c5112":"code","b7410cad":"code","47261eca":"code","40819f1d":"code","50c80ea3":"code","61c35b93":"code","a530fc4b":"code","3e327dc5":"code","e1619bbb":"code","682f7135":"code","4df9d510":"markdown","8eecc6f5":"markdown","35b7475f":"markdown","a81bfe85":"markdown","d2302bfe":"markdown","aaa2e460":"markdown","8b6702f3":"markdown","28b9d86b":"markdown","5c168ed0":"markdown","a4a5ef09":"markdown","b2cfe777":"markdown","aff7e239":"markdown","1fdd2dd1":"markdown","e4424598":"markdown","d72d97fe":"markdown","cfea6afd":"markdown","fd586d78":"markdown","2cac1183":"markdown","a3835d08":"markdown","b8120c97":"markdown","bf27dea1":"markdown","21985d4a":"markdown","efff3257":"markdown"},"source":{"342e9678":"# Importing Required Libraries\nimport numpy as np\nimport pandas as pd\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics","715eaba2":"data = pd.read_csv(\"..\/input\/train.csv\")\ntrain,test=train_test_split(data,test_size=0.3,random_state=0,stratify=data['Survived'])","6b4743a6":"train.head()","ca1be7e7":"# Dropping Features\ntrain = train.drop(['Name'], axis=1)\ntest = test.drop(['Name'], axis=1)\n\ntrain = train.drop(['Ticket'], axis=1)\ntest = test.drop(['Ticket'], axis=1)\n\ntrain = train.drop(['Cabin'], axis=1)\ntest = test.drop(['Cabin'], axis=1)\n\ntrain = train.drop(['PassengerId'], axis=1)\ntest = test.drop(['PassengerId'], axis=1)\n\n# Convert categorical variables into dummy\/indicator variables\ntrain_processed = pd.get_dummies(train)\ntest_processed = pd.get_dummies(test)\n\n# Filling Null Values\ntrain_processed = train_processed.fillna(train_processed.mean())\ntest_processed = test_processed.fillna(test_processed.mean())\n\n# Create X_train,Y_train,X_test\nX_train = train_processed.drop(['Survived'], axis=1)\nY_train = train_processed['Survived']\n\nX_test  = test_processed.drop(['Survived'], axis=1)\nY_test  = test_processed['Survived']\n\n# Display\nprint(\"Processed DataFrame for Training : Survived is the Target, other columns are features.\")\ndisplay(train_processed.head())","4555d013":"# Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest_preds = random_forest.predict(X_test)\nprint('The accuracy of the Random Forests model is :\\t',metrics.accuracy_score(random_forest_preds,Y_test))","cfa88f30":"import shap ","cb0c5112":"# Create Tree Explainer object that can calculate shap values\nexplainer = shap.TreeExplainer(random_forest)","b7410cad":"#Let's choose some instances from the test dataset to understand to the classifier makes predictions for them.\ntest.loc[[421]]","47261eca":"# Calculate Shap values\nchoosen_instance = X_test.loc[[421]]\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)","40819f1d":"#Let's choose some instances from the test dataset to understand to the classifier makes predictions for them.\ntest.loc[[310]]","50c80ea3":"# Calculate Shap values\nchoosen_instance = X_test.loc[[310]]\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)","61c35b93":"#Let's choose some instances from the test dataset to understand to the classifier makes predictions for them.\ntest.loc[[736]]","a530fc4b":"# Calculate Shap values\nchoosen_instance = X_test.loc[[736]]\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)","3e327dc5":"#Let's choose some instances from the test dataset to understand to the classifier makes predictions for them.\ntest.loc[[788]]","e1619bbb":"# Calculate Shap values\nchoosen_instance = X_test.loc[[788]]\nshap_values = explainer.shap_values(choosen_instance)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], choosen_instance)","682f7135":"shap.summary_plot(shap_values, X_train)","4df9d510":"## Interpreting the Model With Shapely Values\n\n### 1. Import SHAP package","8eecc6f5":"## Model Training\n\nLet's train the RF model and get the accuracy measure.","35b7475f":"### 2. Create the Explainer","a81bfe85":"##### Interpretation :\n\nModel predicted 1 (Fully confident that passenger survives), whereas the base_value is 0.3793. Biggest effect is person being a female; This has increased her chances of survival significantly. Next, passenger class 1 also increases her chances of survival.","d2302bfe":"##### Interpretation :\n\nModel predicted 0.66, whereas the base_value is 0.3793. Although passenger class 3 and being a male passenger has decresed his chances of survival. Biggest effect has come from his age being 1 years old; This has increased his chances of survival significantly.","aaa2e460":"## Dataset\n\nFor the demonstration I have used infamouse [**Titanic dataset**](https:\/\/www.kaggle.com\/c\/titanic) to train the Random Forest Classifier. Objective of this problem to predict the survival for passengers in Titanic.","8b6702f3":"Choosen instance refers to an **Survived female passenger of age 24 travelling in passenger class 1, embarked from C.** Let's see what and how our model predicts her survival.","28b9d86b":"Choosen instance refers to an **Unlucky (not survived) Male passenger of age 21 travelling in passenger class 3, embarked from Q.** Let's see what and how our model predicts his survival.","5c168ed0":"### 3. Use the explainer to explain predictions\n","a4a5ef09":"#### Calculate Shap values example 1 ","b2cfe777":"# Thank you!\n\n### **If you like the notebook and think that it helped you..PLEASE UPVOTE. It will keep me motivated** :) :)","aff7e239":"# Objectives\n### Hello Kaggler!, <span style=\"color:PURPLE\">Objective of this short kernal is to<\/span> <span style=\"color:red\">Interprete a Random Forest Model with Shapely Values method.<\/span>\n\nTo make this very easy to grasp I have used infamouse Titanic data set to train the ML model.\n\n### Additionally, after reading this Kernel I hope that you would \n* Get an understanding How to use SHAP library for calculating Shapley values for a random forest classifier.\n* Get an understanding on how the model makes predictions using shapely values method.\n\n\nThe intent here is not to build the best possible model but rather the focus is on the aspect of interpretability.","1fdd2dd1":"Choosen instance refers to an **Survived male passenger of age 1 travelling in passenger class 3, embarked from S.** Let's see what and how our model predicts his survival.","e4424598":"##### Interpretation :\nWhat you see above is a force plot visualizing shapley values for the features. Feature values in pink cause to increase the prediction. Size of the bar shows the magnitude of the feature's effect. Feature values in blue cause to decrease the prediction. Sum of all feature SHAP values explain why model prediction was different from the baseline.\n\nModel predicted 0.16 (Not survived), whereas the base_value is 0.3793. Biggest effect is person being a male; This has decreased his chances of survival significantly. Next, passenger class 3 also decreases his chances of survival while being 21 and port of embarkation beign S increases his chances of survival.","d72d97fe":"---\n#### Calculate Shap values example 2","cfea6afd":"---\n#### Calculate Shap values example 4","fd586d78":"## Data Preparation\nlet's get the datasets ready to put into training. Since our goal is not to make a better classiefier for the problem, let's train a simple model.\n\nFollowing steps are carried out in the following code block.\n1. Dropping unneeded Features\n    * Lets only use Pclass, Sex, Age, SibSp, Parch and Embarked features.\n2. Convert categorical variables into dummy\/indicator variables.\n    * Pclass, Sex and Embarked features needs to be converted.\n3. Filling Null Values\n4. Create X_train, Y_train, X_test, Y_test datasets","2cac1183":"### Credits\n\n* https:\/\/shap.readthedocs.io\/en\/latest\/\n* https:\/\/christophm.github.io\/interpretable-ml-book\/intro.html\n* https:\/\/www.kaggle.com\/dansbecker\/shap-values#Code-to-Calculate-SHAP-Values","a3835d08":"---\n#### Calculate Shap values example 3","b8120c97":"Choosen instance refers to an **Unlucky(Not Survived) female passenger of age 48 travelling in passenger class 3, embarked from S.** Let's see what and how our model predicts her survival.","bf27dea1":"---","21985d4a":"##### Interpretation :\n\nModel predicted 0.42, whereas the base_value is 0.3793. Biggest effect is person being a female; This has increased her chances of survival significantly. Fare value of 34.38 has also played a part incresing her chances. However, beign a passenger in class 3 and her age (48) has significantly decreased her chances of survival.","efff3257":"# Using SHAP library for calculating Shapley values for a Random Forest Classifier"}}