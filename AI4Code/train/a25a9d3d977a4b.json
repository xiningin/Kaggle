{"cell_type":{"aeb54c60":"code","a339a68a":"code","b7738a48":"code","a8439610":"code","6bc339ff":"code","5294acb2":"code","6c15a159":"code","2a6bd843":"code","3a469a8c":"code","6c74deba":"code","eeaa6012":"code","61ab4722":"code","32c2c113":"code","482d364b":"code","5a03bc60":"markdown","44fba9a4":"markdown","e53bd8e2":"markdown","8466ab86":"markdown","01f64489":"markdown"},"source":{"aeb54c60":"!pip install transformers","a339a68a":"dataset_directory = '..\/input\/jigsaw-toxic-comment-classification-challenge'","b7738a48":"!mkdir data\n!unzip {dataset_directory}\/train.csv.zip -d data\/\n!unzip {dataset_directory}\/test.csv.zip  -d data\/\n!unzip {dataset_directory}\/test_labels.csv.zip  -d data\/\n!unzip {dataset_directory}\/sample_submission.csv.zip  -d data\/","a8439610":"from tqdm.notebook import tqdm\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf","6bc339ff":"train_path = 'data\/train.csv'\ntest_path = 'data\/test.csv'\ntest_labels_path = 'data\/test_labels.csv'\nsubm_path = 'data\/sample_submission.csv'","5294acb2":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n\ndf_train = pd.read_csv(train_path)\ndf_test = pd.read_csv(test_path)\ndf_test_labels = pd.read_csv(test_labels_path)\ndf_test_labels = df_test_labels.set_index('id')\n\ndf_train.head()","6c15a159":"from transformers import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nbert_model_name = 'bert-base-uncased'\n\ntokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\nMAX_LEN = 128\n\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n    tokenized_sentences = []\n\n    for sentence in tqdm(sentences):\n        tokenized_sentence = tokenizer.encode(\n                            sentence,                  # Sentence to encode.\n                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                            max_length = max_seq_len,  # Truncate all sentences.\n                    )\n        \n        tokenized_sentences.append(tokenized_sentence)\n\n    return tokenized_sentences\n\ndef create_attention_masks(tokenized_and_padded_sentences):\n    attention_masks = []\n\n    for sentence in tokenized_and_padded_sentences:\n        att_mask = [int(token_id > 0) for token_id in sentence]\n        attention_masks.append(att_mask)\n\n    return np.asarray(attention_masks)\n\ninput_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\nattention_masks = create_attention_masks(input_ids)","2a6bd843":"from sklearn.model_selection import train_test_split\n\nlabels =  df_train[label_cols].values\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n\ntrain_size = len(train_inputs)\nvalidation_size = len(validation_inputs)","3a469a8c":"BATCH_SIZE = 32\nNR_EPOCHS = 1\n\ndef create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n    if train:\n        dataset = dataset.shuffle(buffer_size=buffer_size)\n    dataset = dataset.repeat(epochs)\n    dataset = dataset.batch(batch_size)\n    if train:\n        dataset = dataset.prefetch(1)\n    \n    return dataset\n\ntrain_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\nvalidation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)","6c74deba":"from transformers import TFBertModel\nfrom tensorflow.keras.layers import Dense, Flatten\n\nclass BertClassifier(tf.keras.Model):    \n    def __init__(self, bert: TFBertModel, num_classes: int):\n        super().__init__()\n        self.bert = bert\n        self.classifier = Dense(num_classes, activation='sigmoid')\n        \n    @tf.function\n    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        outputs = self.bert(input_ids,\n                               attention_mask=attention_mask,\n                               token_type_ids=token_type_ids,\n                               position_ids=position_ids,\n                               head_mask=head_mask)\n        cls_output = outputs[1]\n        cls_output = self.classifier(cls_output)\n                \n        return cls_output\n\nmodel = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))","eeaa6012":"import time\nfrom transformers import create_optimizer\n\nsteps_per_epoch = train_size \/\/ BATCH_SIZE\nvalidation_steps = validation_size \/\/ BATCH_SIZE\n\n# | Loss Function\nloss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\nvalidation_loss = tf.keras.metrics.Mean(name='test_loss')\n\n# | Optimizer (with 1-cycle-policy)\nwarmup_steps = steps_per_epoch \/\/ 3\ntotal_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\noptimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n\n# | Metrics\ntrain_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\nvalidation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n\n@tf.function\ndef train_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    with tf.GradientTape() as tape:\n        predictions = model(token_ids, attention_mask=masks)\n        loss = loss_object(labels, predictions)\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n\n    train_loss(loss)\n\n    for i, auc in enumerate(train_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n        \n@tf.function\ndef validation_step(model, token_ids, masks, labels):\n    labels = tf.dtypes.cast(labels, tf.float32)\n\n    predictions = model(token_ids, attention_mask=masks, training=False)\n    v_loss = loss_object(labels, predictions)\n\n    validation_loss(v_loss)\n    for i, auc in enumerate(validation_auc_metrics):\n        auc.update_state(labels[:,i], predictions[:,i])\n                                              \ndef train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n    for epoch in range(epochs):\n        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n\n        start = time.time()\n\n        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n            train_step(model, token_ids, masks, labels)\n            if i % 1000 == 0:\n                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n                for i, label_name in enumerate(label_cols):\n                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n                    train_auc_metrics[i].reset_states()\n        \n        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n            validation_step(model, token_ids, masks, labels)\n\n        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n\n        for i, label_name in enumerate(label_cols):\n            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n            validation_auc_metrics[i].reset_states()\n\n        print('\\n')\n\n        \ntrain(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)","61ab4722":"test_input_ids = tokenize_sentences(df_test['comment_text'], tokenizer, MAX_LEN)\ntest_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\ntest_attention_masks = create_attention_masks(test_input_ids)","32c2c113":"TEST_BATCH_SIZE = 32\ntest_steps = len(df_test) \/\/ TEST_BATCH_SIZE\n\ntest_dataset = create_dataset((test_input_ids, test_attention_masks), batch_size=TEST_BATCH_SIZE, train=False, epochs=1)\n\ndf_submission = pd.read_csv(subm_path, index_col='id')\n\nfor i, (token_ids, masks) in enumerate(tqdm(test_dataset, total=test_steps)):\n    sample_ids = df_test.iloc[i*TEST_BATCH_SIZE:(i+1)*TEST_BATCH_SIZE]['id']\n    predictions = model(token_ids, attention_mask=masks).numpy()\n\n    df_submission.loc[sample_ids, label_cols] = predictions","482d364b":"df_submission.to_csv('submission.csv')","5a03bc60":"## 1. Data Pipeline\n- Loading the datasets from CSVs\n- Preprocessing (Tokenization, Truncation & Padding)\n- Creating efficient data pipelines using tf.data","44fba9a4":"## 3. Training Loop\n- Use BinaryCrossentropy as loss function (is calculated for each of the output 6 output neurons ...that's like training 6 binary classification tasks at the same time) \n- Use the AdamW optimizer with 1-cycle-policy from the Transformers library\n- AUC evaluation metrics","e53bd8e2":"## 4. Run predictions on test-set & save submission","8466ab86":"# Toxic Comment Classification Challenge\n## BERT - TensorFlow 2 & Hugging Face Transformers Library","01f64489":"## 2. BERT Model\n- Load the pretrained BERT base-model from Transformers library\n- Take the first hidden-state from BERT output (corresponding to CLS token) and feed it into a Dense layer with 6 neurons and sigmoid activation (Classifier). The outputs of this layer can be interpreted as probabilities for each of the 6 classes."}}