{"cell_type":{"2c565493":"code","6af05233":"code","906e3164":"code","d5f5695c":"code","120b546d":"code","5dde510c":"code","22f8b8d6":"code","948b62e2":"code","67722ab8":"code","c8d71647":"code","64f35287":"code","f9710125":"code","0b60fcc6":"code","1335ad8c":"code","d84d213b":"code","3d0e9308":"code","b21a7920":"code","7b70bb61":"code","14183806":"code","64a882b0":"code","eca78e2f":"code","186493c8":"code","375cf64f":"code","1607be2b":"code","07dcd12e":"code","f568805f":"code","49297a17":"code","fc2db656":"code","31667b60":"code","36e73cc7":"code","e72ec765":"code","f2b8a154":"code","ba53f977":"code","07525f8f":"code","71cbdbe8":"code","f7693d88":"code","cd25b6cb":"code","fd3c2aef":"code","9eb85a05":"code","9e05be7a":"code","65e4a246":"code","bad352a4":"code","b38c3370":"code","25ecc374":"code","a660eaf4":"code","6ef40f68":"code","11796433":"code","ba3ed8fb":"code","acda881a":"code","df2753c4":"code","8349d375":"code","3a088074":"code","c3013b93":"code","c0e84734":"code","92ba9bf9":"code","81c7f339":"code","a3e5b5df":"code","864f2bc9":"code","1a57d69b":"code","d231ce35":"code","365182a3":"code","53b93171":"code","665ba282":"code","c96f176a":"code","ca76ad2f":"code","2d49ecd5":"code","210b6a9a":"code","c9dae6ab":"code","1dd6c2b7":"code","31ad06fa":"code","b6c3f2ac":"code","dc21cbf8":"code","3b0bb6dd":"code","e9b17951":"markdown","74092159":"markdown","5656b193":"markdown","cc6daca8":"markdown","ad0c6704":"markdown","afb23da5":"markdown","d949addc":"markdown","89c833ba":"markdown","daf69d35":"markdown","14032462":"markdown","e179c6f9":"markdown","60bb52cb":"markdown","1acc52b5":"markdown","313fad89":"markdown","dbe22d00":"markdown","a7457f9f":"markdown","17be1a87":"markdown","cbcbd873":"markdown","3f954bbd":"markdown","c2b62eaf":"markdown","f1485230":"markdown","15fd8832":"markdown","052bf352":"markdown","ef4e13f3":"markdown","2414aeb3":"markdown","82a5ad05":"markdown","7e2f913a":"markdown","9a237e85":"markdown","6ac60bc1":"markdown","f3e004d0":"markdown","8a515c76":"markdown","e0bd2061":"markdown","9cbd44ac":"markdown","3589e6e5":"markdown","693cc193":"markdown","2bc231fe":"markdown","0ba971b5":"markdown","ff1f1df5":"markdown","298a0383":"markdown","8577b703":"markdown","08b48b7f":"markdown","dbf7f824":"markdown","eef0e01b":"markdown","8f913d2e":"markdown","bb087ea9":"markdown","22338b11":"markdown","5dac15e0":"markdown","43a46a43":"markdown","fda0d1a7":"markdown","5b285bc6":"markdown","cbb5fae0":"markdown","e0405314":"markdown","4bd39d5e":"markdown","c7cb7796":"markdown","f9ff7f1a":"markdown","61024d05":"markdown","d0cc7446":"markdown","a79760cd":"markdown","144a90f1":"markdown","88d8c9c0":"markdown","46b583bc":"markdown","a4f84e7a":"markdown","11969632":"markdown","be5f1692":"markdown","e5600705":"markdown","78792793":"markdown","14a681e6":"markdown","42d7e9aa":"markdown","7235b044":"markdown","852c899f":"markdown","426af9de":"markdown","0325d34e":"markdown","c6d96086":"markdown","7fd850e6":"markdown","f2f8571e":"markdown","3d949a5c":"markdown","19cce33d":"markdown","1ef8d218":"markdown","29b80fc1":"markdown","64c1f4ef":"markdown"},"source":{"2c565493":"import numpy as np\nimport pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport pandas_profiling\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm \nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, precision_score, recall_score\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE","6af05233":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npath = \"\/kaggle\/input\/diabetes\/diabetic_data.csv\"\nbdd = pd.read_csv(path)","906e3164":"pandas_profiling.ProfileReport(bdd)","d5f5695c":"bdd = bdd.sort_values(\"encounter_id\").reset_index(drop=True)\nbdd.head()","120b546d":"bdd[\"uno\"] = 1","5dde510c":"bdd[\"Objective\"] = False\nbdd.loc[bdd.readmitted == \"<30\", \"Objective\"] = True\nbdd = bdd.drop([\"readmitted\"], axis = 1)\n\nsns.countplot(bdd[\"Objective\"])\nplt.show()","22f8b8d6":"bdd[\"alreadyCame\"] = True\nbdd.loc[bdd.duplicated(\"patient_nbr\", keep=\"first\"), \"alreadyCame\"] = False\nbdd.loc[bdd.duplicated(\"patient_nbr\", keep=False),:].sort_values([\"patient_nbr\", \"encounter_id\"]).head(20)","948b62e2":"bdd.loc[bdd.change == \"Ch\", \"change\"] = True\nbdd.loc[bdd.change == \"No\", \"change\"] = False\n\nbdd.loc[bdd.diabetesMed == \"Yes\", \"diabetesMed\"] = True\nbdd.loc[bdd.diabetesMed == \"No\", \"diabetesMed\"] = False\n\nfig, axs = plt.subplots(1, 2, figsize = (10, 5))\nsns.countplot(data = bdd, x =\"change\", hue = \"Objective\", ax = axs[0])\nsns.countplot(data = bdd, x =\"diabetesMed\", hue = \"Objective\", ax = axs[1])\nplt.show()","67722ab8":"bdd[\"A1C\"] = bdd[\"A1Cresult\"]\nbdd.loc[bdd.A1Cresult.isin([\">7\", \">8\"]), \"A1C\"] = \"Abnorm\"\nfig, axs = plt.subplots(1, 2, figsize = (10, 5))\nsns.countplot(data = bdd, x = \"A1Cresult\", hue = \"Objective\", ax = axs[0])\nsns.countplot(data = bdd, x = \"A1C\", hue = \"Objective\", ax = axs[1])\nplt.show()\n\nbdd = pd.get_dummies(data = bdd, columns = [\"A1C\"], prefix = \"A1C\", drop_first=False)\nbdd = bdd.drop([\"A1Cresult\", \"A1C_None\"], axis = 1)","c8d71647":"bdd[\"GluSerum\"] = bdd[\"max_glu_serum\"]\nbdd.loc[bdd.max_glu_serum.isin([\">200\", \">300\"]), \"GluSerum\"] = \"Abnorm\"\nfig, axs = plt.subplots(1, 2, figsize = (10, 5))\nsns.countplot(data = bdd, x = \"max_glu_serum\", hue = \"Objective\", ax = axs[0])\nsns.countplot(data = bdd, x = \"GluSerum\", hue = \"Objective\", ax = axs[1])\nplt.show()\n\nbdd = pd.get_dummies(data = bdd, columns = [\"GluSerum\"], prefix = \"GluSerum\", drop_first=False)\nbdd = bdd.drop([\"max_glu_serum\", \"GluSerum_None\"], axis = 1)","64f35287":"sns.countplot(data = bdd, x = \"gender\", hue = \"Objective\")\nplt.show()","f9710125":"bdd[\"isFemale\"] = False\nbdd.loc[bdd.gender == \"Female\", \"isFemale\"] = True\nbdd = bdd[bdd.gender != \"Unknown\/Invalid\"]","0b60fcc6":"people_multiple_gender = bdd.loc[(bdd.duplicated(\"patient_nbr\", keep=False)), [\"patient_nbr\", \"gender\", \"uno\"]].groupby([\"patient_nbr\", \"gender\"]).count().reset_index()\npeople_multiple_gender = people_multiple_gender[people_multiple_gender.duplicated(\"patient_nbr\", keep=False)]\n\nlist_nb_to_drop = []\n\nfor nb in people_multiple_gender.patient_nbr.unique() :\n    nbmin = 1\n    suppr = False\n    value = \"\"\n    \n    for sex in people_multiple_gender.loc[people_multiple_gender.patient_nbr == nb, \"gender\"] :\n        if people_multiple_gender.loc[(people_multiple_gender.patient_nbr == nb) & (people_multiple_gender.gender == sex), \"uno\"].values[0] == nbmin :\n            suppr = True\n        else :\n            suppr = False\n            nbmin = people_multiple_gender.loc[(people_multiple_gender.patient_nbr == nb) & (people_multiple_gender.gender == sex), \"uno\"]\n            value = sex\n        \n    \n    if suppr :\n        list_nb_to_drop.append(nb)\n    else :\n        bdd.loc[(bdd.patient_nbr == nb), \"gender\"] = value\n\nbdd = bdd[~bdd.patient_nbr.isin(list_nb_to_drop)]\nbdd = bdd.drop(\"gender\", axis = 1)","1335ad8c":"sns.countplot(data = bdd, x = \"race\", hue = \"Objective\")","d84d213b":"people_multiple_race = bdd.loc[(bdd.duplicated(\"patient_nbr\", keep=False)), [\"patient_nbr\", \"race\", \"uno\"]].groupby([\"patient_nbr\", \"race\"]).count().reset_index()\npeople_multiple_race = people_multiple_race[people_multiple_race.duplicated(\"patient_nbr\", keep=False)]\n\nlist_nb_to_drop = []\n\nfor nb in people_multiple_race.patient_nbr.unique() :\n    \n    list_race = list(people_multiple_race.loc[people_multiple_race.patient_nbr == nb, \"race\"].unique())\n    try : \n        list_race.remove(\"?\")\n    except :\n        \"Nothing\"\n    \n    if len(list_race) == 1 :\n        #print(list_race[0])\n        bdd.loc[(bdd.patient_nbr == nb), \"race\"] = list_race[0]\n    else : \n        nbmin = 1\n        suppr = False\n        value = \"\"\n\n        for rac in list_race :\n            if people_multiple_race.loc[(people_multiple_race.patient_nbr == nb) & (people_multiple_race.race == rac), \"uno\"].values[0] == nbmin :\n                suppr = True\n            else :\n                suppr = False\n                nbmin = people_multiple_race.loc[(people_multiple_race.patient_nbr == nb) & (people_multiple_race.race == rac), \"uno\"].values[0]\n                value = rac\n\n    \n    if suppr :\n        list_nb_to_drop.append(nb)\n    else :\n        bdd.loc[(bdd.patient_nbr == nb), \"race\"] = value\n\nbdd = bdd[~bdd.patient_nbr.isin(list_nb_to_drop)]","3d0e9308":"bdd.loc[bdd.race == \"?\", \"race\"] = \"unavailable\"\n\nbdd = pd.get_dummies(data = bdd, columns = [\"race\"], prefix = \"race\", drop_first=False)\nbdd = bdd.drop(\"race_unavailable\", axis = 1)","b21a7920":"def norm_diag(bdd, diag) :\n    if bdd[diag] == \"?\" :\n        return \"Unavailable\"\n    elif bdd[diag][0] == \"E\" :\n        return \"Other\"\n    elif bdd[diag][0] == \"V\" :\n        return \"Other\"\n    else :\n        num = float(bdd[diag])\n        \n        if np.trunc(num) == 250 :\n            return \"Diabetes\"\n        elif num <= 139 :\n            return \"Other\"\n        elif num <= 279 :\n            return \"Neoplasms\"\n        elif num <= 389 :\n            return \"Other\"\n        elif num <= 459 :\n            return \"Circulatory\"\n        elif num <= 519 :\n            return \"Respiratory\"\n        elif num <= 579 :\n            return \"Digestive\"\n        elif num <= 629 :\n            return \"Genitourinary\"\n        elif num <= 679 :\n            return \"Other\"\n        elif num <= 709 :\n            return \"Neoplasms\"\n        elif num <= 739 :\n            return \"Musculoskeletal\"\n        elif num <= 759 :\n            return \"Other\"\n        elif num in [780, 781, 782, 783, 784] : \n            return \"Neoplasms\"\n        elif num == 785 :\n            return \"Circulatory\"\n        elif num == 786 :\n            return \"Respiratory\"\n        elif num == 787 :\n            return \"Digestive\"\n        elif num == 788 :\n            return \"Genitourinary\"\n        elif num == 789 :\n            return \"Digestive\"\n        elif num in np.arange(790, 800) :\n            return \"Neoplasms\"\n        elif num >= 800 :\n            return \"Injury\"\n        else :\n            return num","7b70bb61":"bdd[\"diag_1_norm\"] = bdd.apply(norm_diag, axis=1, diag=\"diag_1\")\nbdd[\"diag_2_norm\"] = bdd.apply(norm_diag, axis=1, diag=\"diag_2\")\nbdd[\"diag_3_norm\"] = bdd.apply(norm_diag, axis=1, diag=\"diag_3\")\n\nlist_diag = ['Circulatory', 'Neoplasms', 'Diabetes', 'Respiratory', 'Other', 'Injury', 'Musculoskeletal', 'Digestive', 'Genitourinary']\n\nfig, axs = plt.subplots(3, 1, figsize = (15, 10))\nsns.countplot(data = bdd, y = \"diag_1_norm\", hue = \"Objective\", ax = axs[0], order = list_diag)\nsns.countplot(data = bdd, y = \"diag_2_norm\", hue = \"Objective\", ax = axs[1], order = list_diag)\nsns.countplot(data = bdd, y = \"diag_3_norm\", hue = \"Objective\", ax = axs[2], order = list_diag)\nplt.show()","14183806":"def diag_atleast (bdd, val) :\n    if (bdd[\"diag_1_norm\"] == val) | (bdd[\"diag_2_norm\"] == val) | (bdd[\"diag_3_norm\"] == val) :\n        return True\n    else :\n        return False\n\nfor val in list_diag :\n    name_var = \"diag_atleast_\"+ val\n    print(name_var)\n    bdd[name_var] = bdd.apply(diag_atleast, axis = 1, val=val)","64a882b0":"fig, axs = plt.subplots(3, 3, figsize = (15, 10))\nsns.countplot(data = bdd, x = \"diag_atleast_Circulatory\", hue = \"Objective\", ax = axs[0,0])\nsns.countplot(data = bdd, x = \"diag_atleast_Neoplasms\", hue = \"Objective\", ax = axs[0,1])\nsns.countplot(data = bdd, x = \"diag_atleast_Diabetes\", hue = \"Objective\", ax = axs[0,2])\nsns.countplot(data = bdd, x = \"diag_atleast_Respiratory\", hue = \"Objective\", ax = axs[1,0])\nsns.countplot(data = bdd, x = \"diag_atleast_Other\", hue = \"Objective\", ax = axs[1,1])\nsns.countplot(data = bdd, x = \"diag_atleast_Injury\", hue = \"Objective\", ax = axs[1,2])\nsns.countplot(data = bdd, x = \"diag_atleast_Musculoskeletal\", hue = \"Objective\", ax = axs[2,0])\nsns.countplot(data = bdd, x = \"diag_atleast_Digestive\", hue = \"Objective\", ax = axs[2,1])\nsns.countplot(data = bdd, x = \"diag_atleast_Genitourinary\", hue = \"Objective\", ax = axs[2,2])\nplt.show()","eca78e2f":"list_diag_inter = list_diag.copy()\n\nfor diag in list_diag :\n    list_diag_inter.remove(diag)\n    \n    for diag2 in list_diag_inter :\n        name = \"diag_\" + diag + \"_&_\" + diag2\n        bdd[name] = (bdd[\"diag_atleast_\" + diag] & bdd[\"diag_atleast_\" + diag2])","186493c8":"bdd = bdd.drop([\"diag_1\", \"diag_2\", \"diag_3\"], axis = 1)","375cf64f":"medoc = ['metformin', 'repaglinide', 'nateglinide',\n       'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',\n       'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',\n       'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',\n       'insulin', 'glyburide-metformin', 'glipizide-metformin',\n       'glimepiride-pioglitazone', 'metformin-rosiglitazone',\n       'metformin-pioglitazone']\n\nmedoc_rare = [\"nateglinide\", \"chlorpropamide\", \"acetohexamide\", \"tolbutamide\",\n             \"acarbose\", \"miglitol\", \"troglitazone\", \"tolazamide\", \"examide\",\n             \"citoglipton\", \"glyburide-metformin\", \"glipizide-metformin\",\n             \"glimepiride-pioglitazone\", \"metformin-rosiglitazone\", \"metformin-pioglitazone\"]\n\nmedoc_usuels = [med for med in medoc if med not in medoc_rare]","1607be2b":"for med in medoc_usuels :\n    name = \"take_\" + med\n    bdd[name] = bdd[med].isin([\"Down\", \"Steady\", \"Up\"])\n    \n    \nfig, axs = plt.subplots(3, 3, figsize = (15, 10))\nsns.countplot(data = bdd, x = \"take_metformin\", hue = \"Objective\", ax = axs[0,0])\nsns.countplot(data = bdd, x = \"take_repaglinide\", hue = \"Objective\", ax = axs[0,1])\nsns.countplot(data = bdd, x = \"take_glimepiride\", hue = \"Objective\", ax = axs[0,2])\nsns.countplot(data = bdd, x = \"take_glipizide\", hue = \"Objective\", ax = axs[1,0])\nsns.countplot(data = bdd, x = \"take_glyburide\", hue = \"Objective\", ax = axs[1,1])\nsns.countplot(data = bdd, x = \"take_pioglitazone\", hue = \"Objective\", ax = axs[1,2])\nsns.countplot(data = bdd, x = \"take_rosiglitazone\", hue = \"Objective\", ax = axs[2,0])\nsns.countplot(data = bdd, x = \"take_insulin\", hue = \"Objective\", ax = axs[2,1])\nplt.show()","07dcd12e":"medoc_inter = medoc_usuels.copy()\n\nfor med in medoc_usuels :\n    \n    medoc_inter.remove(med)\n    \n    for med2 in medoc_inter :\n        name = \"take_\" + med + \"_&_\" + med2\n        bdd[name] = (bdd[\"take_\" + med] & bdd[\"take_\" + med2])","f568805f":"def nbMedocRare (bdd, listMedoc) :\n    nb = 0\n    for med in listMedoc :\n        if bdd[med] != \"No\" :\n            nb += 1\n    return nb\n\nbdd[\"nb_rare_medoc\"] = bdd.apply(nbMedocRare, listMedoc = medoc_rare, axis = 1)","49297a17":"fig, axs = plt.subplots(1, 2, figsize = (10, 5))\nsns.countplot(data = bdd, x = \"admission_type_id\", hue = \"Objective\", ax = axs[0])\naxs[0].set_title(\"admission_type_id before transformation\")\n\nbdd['admission_type_id'] = bdd['admission_type_id'].replace([1, 2, 7], \"emergency\")\nbdd['admission_type_id'] = bdd['admission_type_id'].replace([4, 5, 6, 8], \"unavailable\")\nbdd['admission_type_id'] = bdd['admission_type_id'].replace(3, \"elective\")\n\nsns.countplot(data = bdd, x = \"admission_type_id\", hue = \"Objective\", ax = axs[1])\naxs[1].set_title(\"admission_type_id after transformation\")\n\nbdd = pd.get_dummies(data = bdd, columns = [\"admission_type_id\"], prefix=\"admission_type\", drop_first=False)\nbdd = bdd.drop([\"admission_type_unavailable\"], axis = 1)","fc2db656":"fig, axs = plt.subplots(1, 2, figsize = (10, 5))\nsns.countplot(data = bdd, x = \"admission_source_id\", hue = \"Objective\", ax = axs[0])\naxs[0].set_title(\"admission_source_id before transformation\")\n\nbdd['admission_source_id'] = bdd['admission_source_id'].replace([1, 2, 3], \"referral\")\nbdd['admission_source_id'] = bdd['admission_source_id'].replace([4, 5, 6, 10, 22, 25], \"transfert\")\nbdd['admission_source_id'] = bdd['admission_source_id'].replace([8, 14, 11, 13, 9, 15, 17, 20, 21], \"unavailable\")\nbdd['admission_source_id'] = bdd['admission_source_id'].replace(7, \"emergencyRoom\")\n\nsns.countplot(data = bdd, x = \"admission_source_id\", hue = \"Objective\", ax = axs[1])\naxs[1].set_title(\"admission_source_id after transformation\")\n\nbdd = pd.get_dummies(data = bdd, columns = [\"admission_source_id\"], prefix=\"admission_source\", drop_first=False)\nbdd = bdd.drop([\"admission_source_unavailable\"], axis = 1)","31667b60":"fig, axs = plt.subplots(1, 2, figsize = (10, 5))\nsns.countplot(data = bdd, x = \"discharge_disposition_id\", hue = \"Objective\", ax = axs[0])\naxs[0].set_title(\"discharge_disposition_id before transformation\")\n\nbdd['discharge_disposition_id'] = bdd['discharge_disposition_id'].replace([1, 6, 8, 9, 10], \"home\")\nbdd['discharge_disposition_id'] = bdd['discharge_disposition_id'].replace([2, 3, 4, 5, 14, 22, 23, 24], \"transfert\")\nbdd['discharge_disposition_id'] = bdd['discharge_disposition_id'].replace([18, 25, 26], \"unavailable\")\nbdd['discharge_disposition_id'] = bdd['discharge_disposition_id'].replace([7, 10, 11, 13, 12, 15, 16, 17, 19, 20, 27, 28], \"other\")\n\nsns.countplot(data = bdd, x = \"discharge_disposition_id\", hue = \"Objective\", ax = axs[1])\naxs[1].set_title(\"discharge_disposition_id after transformation\")\n\nbdd = pd.get_dummies(data = bdd, columns = [\"discharge_disposition_id\"], prefix=\"discharge_type\", drop_first=False)\nbdd = bdd.drop([\"discharge_type_unavailable\"], axis = 1)","36e73cc7":"var_quanti = [\"time_in_hospital\", 'num_lab_procedures', 'num_procedures', \n              'num_medications', 'number_outpatient', 'number_emergency', \n              'number_inpatient','number_diagnoses', 'nb_rare_medoc']","e72ec765":"def recup_age (bdd) :\n    return int(bdd.age[-4::].replace('-', '').replace(')', ''))\n\nbdd[\"age_num\"] = bdd.apply(recup_age, axis = 1)\nvar_quanti.append(\"age_num\")\nbdd = bdd.drop(\"age\", axis = 1)","f2b8a154":"def count_num_medoc(bdd) :\n    nb = 0\n    for med in medoc :\n        if bdd[med] != \"No\" :\n            nb += 1\n    return nb\n\ndef count_num_medoc_chgmnt(bdd) :\n    nb = 0\n    for med in medoc :\n        if (bdd[med] != \"No\") & (bdd[med] != \"Steady\") :\n            nb += 1\n    return nb\n\nbdd[\"num_medo_arrived\"] = bdd.apply(count_num_medoc, axis = 1)\nbdd[\"num_medo_chgmnt\"] = bdd.apply(count_num_medoc_chgmnt, axis = 1)\n\nvar_quanti.append(\"num_medo_arrived\")\nvar_quanti.append(\"num_medo_chgmnt\")","ba53f977":"bdd[\"proportion_chgmnt\"] = bdd[\"num_medo_chgmnt\"] \/ bdd[\"num_medo_arrived\"]\nbdd[\"proportion_chgmnt\"] = bdd[\"proportion_chgmnt\"].fillna(0)\nvar_quanti.append(\"proportion_chgmnt\")","07525f8f":"fig, axs = plt.subplots(5, 3, figsize = (20, 20))\n\nsns.countplot(data = bdd, y = \"time_in_hospital\", hue = \"Objective\", ax = axs[0, 0])\naxs[0, 0].set_title('time_in_hospital')\n\nsns.countplot(data = bdd, y = \"num_lab_procedures\", hue = \"Objective\", ax = axs[0, 1])\naxs[0, 1].set_title('num_lab_procedures')\n\nsns.countplot(data = bdd, y = \"num_procedures\", hue = \"Objective\", ax = axs[0, 2])\naxs[0, 2].set_title('num_procedures')\n\n\nsns.countplot(data = bdd, y = \"num_medications\", hue = \"Objective\", ax = axs[1, 0])\naxs[1, 0].set_title('num_medications')\n\nsns.countplot(data = bdd, y = \"number_outpatient\", hue = \"Objective\", ax = axs[1, 1])\naxs[1, 1].set_title('number_outpatient')\n\nsns.countplot(data = bdd, y = \"number_emergency\", hue = \"Objective\", ax = axs[1, 2])\naxs[1, 2].set_title('number_emergency')\n\n\nsns.countplot(data = bdd, y = \"number_inpatient\", hue = \"Objective\", ax = axs[2, 0])\naxs[2, 0].set_title('number_inpatient')\n\nsns.countplot(data = bdd, y = \"number_diagnoses\", hue = \"Objective\", ax = axs[2, 1])\naxs[2, 1].set_title('number_diagnoses')\n\nsns.countplot(data = bdd, y = \"num_medo_arrived\", hue = \"Objective\", ax = axs[2, 2])\naxs[2, 2].set_title('num_medo_arrived')\n\n\nsns.countplot(data = bdd, y = \"num_medo_chgmnt\", hue = \"Objective\", ax = axs[3, 0])\naxs[3, 0].set_title('num_medo_chgmnt')\n\nsns.countplot(data = bdd, y = \"age_num\", hue = \"Objective\", ax = axs[3, 1])\naxs[3, 1].set_title('age_num')\n\nsns.countplot(data = bdd, y = \"proportion_chgmnt\", hue = \"Objective\", ax = axs[3, 2])\naxs[3, 2].set_title('proportion_chgmnt')\n\nsns.countplot(data = bdd, y = \"nb_rare_medoc\", hue = \"Objective\", ax = axs[4, 0])\naxs[4, 0].set_title('nb_rare_medoc')\n\nplt.show()","71cbdbe8":"bdd[var_quanti].skew()","f7693d88":"bdd[\"number_outpatient\"] = np.log1p(bdd[\"number_outpatient\"])\nbdd[\"number_emergency\"] = np.log1p(bdd[\"number_emergency\"])","cd25b6cb":"bdd = bdd.drop([\"weight\", \"medical_specialty\", 'encounter_id', 'patient_nbr', \"payer_code\", 'diag_1_norm', 'diag_2_norm', 'diag_3_norm'] + medoc, axis = 1)\nbdd = bdd.drop(\"uno\", axis = 1)","fd3c2aef":"X = bdd.drop([\"Objective\"], axis=1)\ny = bdd[\"Objective\"]\nprint('Original dataset shape {}'.format(Counter(y)))","9eb85a05":"X.insert(0, \"Intercept\", 1)","9e05be7a":"def reduction_variable_logit(X_train, y_train, showVarToDel=False) :\n    ultime_model = False\n    var_to_del = []\n\n    while (ultime_model == False) :\n        log_reg = sm.Logit(y_train, X_train.drop(var_to_del, axis = 1).astype(float)).fit(maxiter = 100, disp = False)\n\n        max_pvalue = max(log_reg.pvalues)\n\n        if max_pvalue < 0.05 :\n            ultime_model = True\n        else :\n            varToDel = log_reg.pvalues.index[log_reg.pvalues == max(log_reg.pvalues)].values[0]\n            if showVarToDel :\n                print(varToDel + \", p-value = \" + str(max(log_reg.pvalues)))\n            var_to_del.append(varToDel)\n    \n    return log_reg, var_to_del","65e4a246":"plt.figure(figsize = (20, 20))\nsns.heatmap(abs(X.corr()), cmap=\"Greens\")\nplt.show()","bad352a4":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","b38c3370":"log_reg, var_to_del = reduction_variable_logit(X_train, y_train, True)","25ecc374":"log_reg.summary()","a660eaf4":"threshold = 0.5\n\nyhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nprint(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, yhat>threshold)))\nprint(\"Precision is {0:.2f}\".format(precision_score(y_test, yhat>threshold)))\nprint(\"Recall is {0:.2f}\".format(recall_score(y_test, yhat>threshold)))","6ef40f68":"yhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nacc = []\nprec = []\nrec = []\n\nfor ts in np.arange(0.1, 1, 0.01) : \n    acc.append(accuracy_score(y_test, yhat>ts))\n    prec.append(precision_score(y_test, yhat>ts))\n    rec.append(recall_score(y_test, yhat>ts))\n    \nfig = plt.figure(figsize=(15,15))\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = acc, label = \"accuracy\")\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = prec, label = \"precision\")\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = rec, label = \"recall\")\nplt.xlabel(\"Threshold\")\n\nplt.show()","11796433":"undersample = RandomUnderSampler(random_state=42)\nnew_X, new_y = undersample.fit_resample(X, y)\n\nprint('undersampled dataset shape {}'.format(Counter(new_y)))","ba3ed8fb":"X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.2, random_state=42)","acda881a":"log_reg, var_to_del = reduction_variable_logit(X_train, y_train, showVarToDel=True)","df2753c4":"log_reg.summary()","8349d375":"threshold = 0.5\n\nyhat = log_reg.predict(X_test.drop(var_to_del, axis=1).astype(float)) \n\nprint(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, yhat>threshold)))\nprint(\"Precision is {0:.2f}\".format(precision_score(y_test, yhat>threshold)))\nprint(\"Recall is {0:.2f}\".format(recall_score(y_test, yhat>threshold)))","3a088074":"yhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nacc = []\nprec = []\nrec = []\n\nfor ts in np.arange(0.1, 1, 0.01) : \n    acc.append(accuracy_score(y_test, yhat>ts))\n    prec.append(precision_score(y_test, yhat>ts))\n    rec.append(recall_score(y_test, yhat>ts))\n    \nfig = plt.figure(figsize=(15,15))\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = acc, label = \"accuracy\")\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = prec, label = \"precision\")\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = rec, label = \"recall\")\nplt.axvline(0.45, color = \"purple\", label=\"theshold=0.45\")\nplt.xlabel(\"Threshold\")\n\nplt.show()","c3013b93":"threshold = 0.45\n\nyhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nprint(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, yhat>threshold)))\nprint(\"Precision is {0:.2f}\".format(precision_score(y_test, yhat>threshold)))\nprint(\"Recall is {0:.2f}\".format(recall_score(y_test, yhat>threshold)))","c0e84734":"oversample = SMOTE(random_state=42)\nnew_X, new_y = oversample.fit_resample(X, y)\nprint('Oversampled dataset shape {}'.format(Counter(new_y)))","92ba9bf9":"X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.2, random_state=42)","81c7f339":"log_reg, var_to_del = reduction_variable_logit(X_train, y_train, showVarToDel=True)","a3e5b5df":"log_reg.summary()","864f2bc9":"threshold = 0.5\n\nyhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nprint(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, yhat>threshold)))\nprint(\"Precision is {0:.2f}\".format(precision_score(y_test, yhat>threshold)))\nprint(\"Recall is {0:.2f}\".format(recall_score(y_test, yhat>threshold)))","1a57d69b":"yhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nacc = []\nprec = []\nrec = []\n\nfor ts in np.arange(0.1, 1, 0.01) : \n    acc.append(accuracy_score(y_test, yhat>ts))\n    prec.append(precision_score(y_test, yhat>ts))\n    rec.append(recall_score(y_test, yhat>ts))\n    \nfig = plt.figure(figsize=(15,15))\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = acc, label = \"accuracy\")\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = prec, label = \"precision\")\nsns.lineplot(x = np.arange(0.1, 1, 0.01), y = rec, label = \"recall\")\nplt.axvline(0.31, color = \"purple\")\nplt.xlabel(\"Threshold\")\n\nplt.show()","d231ce35":"threshold = 0.31\n\nyhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nprint(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, yhat>threshold)))\nprint(\"Precision is {0:.2f}\".format(precision_score(y_test, yhat>threshold)))\nprint(\"Recall is {0:.2f}\".format(recall_score(y_test, yhat>threshold)))","365182a3":"oversample = SMOTE(random_state = 42)\n\nacc = []\nprec = []\nrec = []\nf_measure = []\nnb_var = []\n\nacc_train = []\nprec_train = []\nrec_train = []\n\nrat = []\n\nfor ratio in tqdm(np.arange(0.13, 1, 0.05)) :\n    try :\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)\n        \n        undersample = RandomUnderSampler(sampling_strategy=ratio, random_state = 42)\n        new_X, new_y = undersample.fit_resample(X_train, y_train)\n\n\n        new_X, new_y = oversample.fit_resample(new_X, new_y)\n\n        log_reg, var_to_del = reduction_variable_logit(new_X, new_y, showVarToDel=False)\n        \n        yhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n        yhat_train = log_reg.predict(new_X.drop(var_to_del, axis = 1).astype(float)) \n\n        threshold = 0.5\n        \n        myPrec = precision_score(y_test, yhat>threshold)\n        myRecall = recall_score(y_test, yhat>threshold)\n        rat.append(ratio)\n        rec.append(myRecall)\n        prec.append(myPrec)\n        acc.append(accuracy_score(y_test, yhat>threshold))\n        f_measure.append(2 * (myPrec * myRecall) \/ (myPrec + myRecall))\n        nb_var.append(len(log_reg.pvalues))\n        \n        rec_train.append(recall_score(new_y, yhat_train>threshold))\n        prec_train.append(precision_score(new_y, yhat_train>threshold))\n        acc_train.append(accuracy_score(new_y, yhat_train>threshold))\n    except :\n        print(\"ERROR : \" + str(ratio))","53b93171":"fig = plt.figure(figsize=(15,15))\nsns.lineplot(x = rat, y = acc, label = \"accuracy\", color = \"blue\")\nsns.lineplot(x = rat, y = prec, label = \"precision\", color=\"orange\")\nsns.lineplot(x = rat, y = rec, label = \"recall\", color=\"green\")\nsns.lineplot(x = rat, y = f_measure, label = \"f_measure\", color=\"red\")\n\nsns.lineplot(x = rat, y = acc_train, label = \"accuracy on train\", style=True, dashes=[(3, 3)], color = \"blue\")\nsns.lineplot(x = rat, y = prec_train, label = \"precision on train\", style=True, dashes=[(3, 3)], color = \"orange\")\nsns.lineplot(x = rat, y = rec_train, label = \"recall on train\", style=True, dashes=[(3, 3)], color = \"green\")\n\nplt.xlabel(\"Ratio Nb_People_Readmitted_Within_30_Days \/ Nb_People before undersampling : 0.13 = no undersampling ; 1=no oversampling\")\n\nplt.show()","665ba282":"fig = plt.figure(figsize=(15,15))\nsns.lineplot(x = rat, y = nb_var)\nplt.xlabel(\"Ratio Nb_People_Readmitted_Within_30_Days \/ Nb_People before undersampling : 0.13 = no undersampling ; 1=no oversampling\")\nplt.ylabel(\"Number of selected variables\")\nplt.show()","c96f176a":"undersample = RandomUnderSampler(random_state=42)\nnew_X, new_y = undersample.fit_resample(X, y)\nprint('Oversampled dataset shape {}'.format(Counter(new_y)))","ca76ad2f":"X_train, X_test, y_train, y_test = train_test_split(new_X, new_y, test_size=0.2, random_state=42)","2d49ecd5":"log_reg, var_to_del = reduction_variable_logit(X_train, y_train, showVarToDel=False)","210b6a9a":"log_reg.summary()","c9dae6ab":"threshold = 0.45\n\nyhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nprint(\"Accuracy is {0:.2f}\".format(accuracy_score(y_test, yhat>threshold)))\nprint(\"Precision is {0:.2f}\".format(precision_score(y_test, yhat>threshold)))\nprint(\"Recall is {0:.2f}\".format(recall_score(y_test, yhat>threshold)))\n\n\nfig, axs = plt.subplots(1, 2, figsize = (20, 10))\n\nsns.heatmap(confusion_matrix(y_test, yhat>threshold), \n            annot=True, \n            cmap=\"Blues\", \n            xticklabels = [\"Predicted False\", \"Predicted True\"], \n            yticklabels = [\"Real False\", \"Real True\"], ax=axs[0])\n\nfpr, tpr, threshold = roc_curve(y_test, yhat)\nroc_auc = auc(fpr, tpr)\naxs[1] = plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\naxs[1] = plt.title('Receiver Operating Characteristic')\naxs[1] = plt.legend(loc = 'lower right')\naxs[1] = plt.plot([0, 1], [0, 1],'r--')\naxs[1] = plt.xlim([0, 1])\naxs[1] = plt.ylim([0, 1])\naxs[1] = plt.ylabel('True Positive Rate')\naxs[1] = plt.xlabel('False Positive Rate')","1dd6c2b7":"threshold_down = 0.45\nthreshold_up = 0.8\n\nyhat = log_reg.predict(X_test.drop(var_to_del, axis = 1).astype(float)) \n\nresponse_df = pd.DataFrame(columns=[\"Real_label\", \"Proba\", \"Predicted_Label\"])\n\nresponse_df[\"Real_label\"] = y_test\nresponse_df[\"Proba\"] = yhat\nresponse_df.loc[response_df[\"Proba\"] > threshold_up, \"Predicted_Label\"] = \"Very_likey\"\nresponse_df.loc[response_df[\"Proba\"] < threshold_up, \"Predicted_Label\"] = \"Likely\"\nresponse_df.loc[response_df[\"Proba\"] < threshold_down, \"Predicted_Label\"] = \"Not_likely\"\n\nresponse_df[\"uno\"] = 1\n\ntable = pd.pivot_table(data=response_df[[\"Predicted_Label\", \"Real_label\", \"uno\"]], index=[\"Predicted_Label\"], columns = \"Real_label\", values=\"uno\", aggfunc=np.sum)\n\ntable.loc[[\"Not_likely\", \"Likely\", \"Very_likey\"], :].fillna(0).transpose()","31ad06fa":"plt.figure(figsize = (20, 10))\n\nsns.heatmap(table.loc[[\"Not_likely\", \"Likely\", \"Very_likey\"], :].fillna(0).apply(lambda x: x \/ float(x.sum()), axis=1).transpose(), \n            cmap=\"Blues\", annot=True, fmt='.2%')\nplt.show()","b6c3f2ac":"log_reg_tot = sm.Logit(new_y, new_X.drop(var_to_del, axis=1).astype(float)).fit()\nlog_reg_tot.summary()","dc21cbf8":"zipped_lists = zip(abs(log_reg_tot.tvalues[1::].values), log_reg_tot.tvalues[1::].index, log_reg_tot.params[1::].values)\nzipped_lists = sorted(zipped_lists, reverse=True)\n\nfig, axs = plt.subplots(1, 2, figsize = (20, 10))\n\nsns.barplot(x = [element for element, _, _ in zipped_lists], \n            y = [element for _, element, _ in zipped_lists], \n            ax=axs[0])\naxs[0].set_xlabel(\"Z-evaluation\")\n\nsns.barplot(x = [element for _, _, element in zipped_lists], \n            y = [element for _, element, _ in zipped_lists], ax=axs[1])\naxs[1].set_xlabel(\"Coefficient\")\n\nplt.show()","3b0bb6dd":"plt.figure(figsize = (20, 20))\nsns.heatmap(abs(X.drop([\"Intercept\"] + var_to_del, axis = 1).corr()), cmap=\"Greens\", annot=True)\nplt.show()","e9b17951":"#### Admissions and discharges","74092159":"## Features removal","5656b193":"For the two tests, A1C and GluSerum, we transform the categorical variables, with 4 possible values, into 2 binary features:\n- test norm: if is =1, the test has been done, and the result is normal.\n- test abnorm: if is =1, the test has been done, and the result is not normal.\n- if both =0, the test has not been done.","cc6daca8":"#### Gender","ad0c6704":"## Chosen logistic regression : on undersampled dataset","afb23da5":"But those three features are not exploitables for now. Thus we tranform those three categorical features into 9 binary features. Indeed, we are going to check for each one if it has been diagnosed during the admission.","d949addc":"First thing: there are many errors due to singular matrix, causing a problem of convergence.","89c833ba":"Hello everybdoy, and welcolme to my notebook.  \nWe are going to analyze the database called \"Diabetes 130 US hospitals for years 1999-2008\". First of all, what is this dataset?  \n  \n\"The data set represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. Information was extracted from the database for encounters that satisfied the following criteria.\n\n1. It is an inpatient encounter (a hospital admission).\n2. It is a diabetic encounter, that is, one during which any kind of diabetes was entered to the system as a diagnosis.\n3. The length of stay was at least 1 day and at most 14 days.\n4. Laboratory tests were performed during the encounter.\n5. Medications were administered during the encounter.  \n  \nThe data contains such attributes as patient number, race, gender, age, admission type, time in hospital, medical specialty of admitting physician, number of lab test performed, HbA1c test result, diagnosis, number of medication, diabetic medications, number of outpatient, inpatient, and emergency visits in the year before the hospitalization, etc.\" (https:\/\/archive.ics.uci.edu\/ml\/datasets\/Diabetes+130-US+hospitals+for+years+1999-2008).  \n  \nI have two objectives with this analysis:\n1. First of all, I would like to set up a model that could predict if a patient will be readmitted within 30 days...\n2. ... and I want this model to explain the drivers that leads to a readmission, or at least some idicators that can be used in order to detect such a readmission.   \n  \nBecause of those two objectives, I am going to create \"just\" a Logistic Regression. The aim of this regression is to assess the probability of an evenement regarding the variables availables:\n$$P(y=1\\mid X) = \\frac{e^{\\beta X}}{1+e^{\\beta X}}$$  \nwhere $y=1$ represents the readmission within 30 days, $X=(x_0, x_1, ..., x_n)$ are the variables, and $\\beta = (\\beta_0, \\beta_1,..., \\beta_n)$ the parameters of the model.","daf69d35":"## Qualitative features","14032462":"### Undersampling regression","e179c6f9":"First of all we sort the database in function of the _encounter_id_ . Indeed, an intuition I have is that low values represent old admissions, and high values represent recent ones. This is an hypothesis I make.","60bb52cb":"### Naive first regression","1acc52b5":"For _admission_type_id_, _admission_source_id_ and _discharge_disposition_id_, we reduce the number of possible values by regrouping them into $n$ wider categories. Then we creates $n-1$ binary features.","313fad89":"Finally, we count the number of \"rare\" drugs the patient is taking.","dbe22d00":"Bayes is still right, (around) 0.5 gives better accuracy, but as it is quite stable, let's move to 0.31 to increase the recall.  \nI still have some doubts, so we are going to try to mix under and over sampling, and track some indicators.","a7457f9f":"Let's see this undersampling:","17be1a87":"### Oversampling & Undersampling regression","cbcbd873":"Indeed we have slightly increased the accuracy... and the recall! This is something very important I think for this project, because we want to catch as many readmitions as possible.  \n  \nNow lets see if the oversampling gives better results:","3f954bbd":"Two features are particulary skewed: *number_emergency* and *number_outpatient*. Thus we are going to make a log transformation:  \n$$variable\\_logtransformed = ln(1+variable)$$  \nWe add 1 in the log, because the variables take the value 0.  \n  \nWe don't transform nb_rare_medoc because it can take only 3 differents values.","c2b62eaf":"# Explanation of the project","f1485230":"# Feature engineering","15fd8832":"Let's split the database into two sub-databases again: train (80% of the data), to create the model, and test (20% remaining) to evaluate it. ","052bf352":"To do so we use the method SMOTE (Synthetic Minority Oversampling TEchnique). The idea is to create new data from existing, through interpolations. Then it's not \"just\" some duplications as with a bootstrap.","ef4e13f3":"Now, let's have a first visualisation of those data:","2414aeb3":"#### First admition or not","82a5ad05":"**0.89 is GREAT, but the 0.01 is AWFULL!**  \n  \nWhy do we have those results? Because we are facing an unbalanced problem: in our database, we have almost 90% of **FALSE** label, and only 10% of **TRUE** label.   \n  \nThis model is smart, he set (almost) all rows to **FALSE**, so he has at least 90% of accuracy.  \nMaybe if we change the threshold we are going to see better results?","7e2f913a":"We check if the _Gender_ remains the same for each admission. If not we set the value at the most common feature.","9a237e85":"#### Race","6ac60bc1":"# Analysis of the model","f3e004d0":"#### Diagnostics","8a515c76":"First of all, let's import some libraries we are going to use within this project, and the database.","e0bd2061":"First, we split our database into two: X with the variables, and y with the responses. We also add a column full of **1** to have an intercept for the regression.","9cbd44ac":"We create a column named _uno_ full of **1**, which will be used in this part.","3589e6e5":"It is not \"good\", but this is more reasonnable. But I have hope, maybe if we don't blindy take the Bayes' threshold we can enhance our results: ","693cc193":"#### Tests A1C and GluSerum","2bc231fe":"Ok that's better. We are on a balanced problem... but we have lost a lot of information. Let's cross our fingers!","0ba971b5":"We do the same check for the feature _Race_.","ff1f1df5":"# Imports","298a0383":"This analysis of the database leads to the following conclusions:\n- A predictive model to detect in advance the readmission within 30 days is possible. Indeed our model, even if it is not very precise, **is still better than nothing**, and gives insights of some causes of readmission, our factors that give the opportunity to detect a readmission.\n- **The status of database and the way I have exploited it is not enough to have a powerful parametric model.** Maybe more precise data or a better feature engineering work could lead to better results.","8577b703":"Then, for each \"common\" drug, we check ih the patient consumes it. As for the diagnosis, we determine which combination of medications the patient is taking.","08b48b7f":"Finally, as our objective is to explain the model, we check which are the most influent features of this model, by looking at the z evaluation. This score is an evaluation of the importance of the variable.  \n$$z\\_evaluation = \\frac{coef}{std\\_err}$$  \nHigh coefficients have a bigger impact on the model, low standard errors reveal the robustness of the coefficient.","dbf7f824":"We are going to play with the sampling_strategy component of the undersampling method: we will make it move from 0.13 (no undersampling) to 1 (full undersampling). \n\nFor each iteration:\n\n1. we separate our DB into train & test part\n2. we undersample then oversample the train part (the idea is to check if this oversampling really makes sense, and could be applied to have a robust model to use in real-life)\n3. we collect accuracy, precision, recall, f measure and the number of variable in the model obtained. We also calculate those three first indicators on the train part.  \n  \n**ARE YOU EXCITED??**  \n  \n(i am)","eef0e01b":"Even with those new labels, there still are many errors.  \nThis model being the best that we have, we will try to understand it.  ","8f913d2e":"We obtain **AMAZING results!** Is this magic? or just an illusion?  \nLet's see what happens if we play with the threshold.","bb087ea9":"We normalize this feature and remove the three patients with \"Unknown\/Invalid\" to have a single binary feature.","22338b11":"Let's have a deeper look at this model, with the confusion matrix and the calculation of the AUC:","5dac15e0":"What we understand from this model:\n- **_number_inpatient_** is clearly the more relevant feature in term of z-evaluation. The higher the number of inpatient visits of the patient in the year preceding the encounter is, the higher the probability of the readmission within 30 days is.\n- _alreadyCame_ is second one, with a negative coefficient. It means that first admission is more likely to lead to a quick readmission.\n- The **discharge type** seems very important. Indeed all 3 resulting features appears in the top 10. In this case, being discharge because of a transfert increase significatively the probability to be readmitted.\n- Other quantitative features seem important with a positive coefficient (meaning a positive correlation with the probability to be readmitted): _number_diagnoses_ , _number_emergency_ , _time_in_hospital_ , _number_medo_arrived_ and _proportion_chgmnt_.\n- About the diagnoses, even if the Circulatory & Diabetes diagnosis seems to increase the probability, I don't think we can infer on this feature. Moreover _diag_atleast_Digestive_ has a p-value > 0.05 when we run it on the full dataset, increasing my doubts about this feature.\n- The same conclusion can be assessed with the drugs: even if taking metformin or glimepiride seems to lower the probability, as the risk increase with the number of medications, I don't know if we can extract much about this feature.\n- The race seems not to be relevant neither. _race_Caucasian_ and _race_AfricanAmerican_ have the same coefficient, and together they represent more than 95% of the studied population. This can been seen in the correlation heatmap below.","43a46a43":"... and the answer is no!  \nBut I won't give up. Lets try two things:  \n- Undersampling: We reduce our dataset to have balanced categories\n- Oversampling: We create artificial data","fda0d1a7":"First of all, we run this model, with selected variables, on the whole undersampled dataset, without the separation train \/ test:","5b285bc6":"Now, we are going to make a first and quick analyze of the existing features in order to understand better current data:","cbb5fae0":"### Response variable","e0405314":"We can see that the overall correlation betweeen the variable is quite low, with some exception, especially with the admission_type and admission_source, and between the number of medications and the proportion of changes in the medications. Even if correlation is something avoided to have the well convergence of the algorithms, we conserve all of the variables. If we observe too many divergences, we will remove some of those variables.","4bd39d5e":"Now that we have this new dataset, let's try to analyze it!  \n  \nAs explain earlier, the objective is not to obtain the best results through a \"black-box\" model. I would like to create a model which determines whether or not a patient is going to be readmitted within 30 days, and what can explain the readmission (or what can help us detecting it).  \n  \nThus we are going to make a first, but relevant we hope, parametric model: **A LOGISTIC REGRESSION** !","c7cb7796":"There are 3 diagnostics, with more than 700 possible values. So we make some order and normalize those diagnostics into 9 wide categories.","f9ff7f1a":"#### Normalization of features _change_ and _diabetesMed_","61024d05":"We transform the qualitative feature _age_ into a quantitative one by taking the upper bound (aribitrary choice)","d0cc7446":"With the hypothesis made on the _encounter\\_id_ feature, we check if the patient has already come in this period.","a79760cd":"### Oversampling regression\n\n","144a90f1":"To create a relevant logistic regression, we must select relevant variables. To do so, we are going to create a model with all variables, and then remove the variable with the highest p-value. By iterations, we are goind to remove all non-relevant features, until all of our variables have a p-value > 0.05. Function below automates this process.","88d8c9c0":"Ok, 19 variables left, not bad. Is it good?","46b583bc":"# Creation of the model","a4f84e7a":"... and the proportion those changements represent.","11969632":"We set at **True** the objective value if the patient is readmitted within 30 days, and **False** otherwise.","be5f1692":"Maybe 1 diagnostic is not enough to detect a future readmission. That is why we check if some pairs of diagnosis have been diagnosed.","e5600705":"**NOOOOOOO!**  \n  \nAbove results shows what I feared: The amazing results we obtained with the oversampling are not robusts. In this case, creating a logistic model with an oversampling method is not relevant, because we are not able to recover the readmissions :(  \n  \nIn fact creating data through SMOTE leads to a complete quasi-separation. It can be observed throught the very low number of variables to be deleted. Thus the resulting dataset is not representatitve of the reality. \nLet's go back to the undersampling method...","78792793":"## Quantitative features","14a681e6":"### Other binary features","42d7e9aa":"## Before the model","7235b044":"36 variables left, the model has converged, LLR p-value equals to zero (meaning that the models at least does something)... It is a good start. Let's evaluate it on our test dataset.  \n  \nWe evaluate the probability, for each people, to come back within 30 days. If the probability exceeds a threshold, we set the response to 1, else we set it to 0.  \n  \n**Let's start with this good old Bayes' threshold : 0.5!**","852c899f":"Oh... so we have 111 variables... and a Pseudo R-square of 61%... this is strange.","426af9de":"We count the number of medications the patient is taking, and the number of changement in his treatment...","0325d34e":"#### Drugs","c6d96086":"With the first analyze of the feautures, we realised that many drugs are very rarely taken by the patients. The others are more commons.","7fd850e6":"**First observation**: There is **not a significative difference** between the people with a readmission within 30 days and the others in term of distribution of those variables.  \n  \n**Second observation**: For the feature _num_lab_procedures_, the data seems to be **truncated**. Is this normal? Is this a problem in the dataset? We dont know, but it will obviously affect the future model. So let's keep it in mind, and we will consider that this observation is normal, that there is no problem in the data.  \n  \n**Third observation**: For a lot of features, the data seems **skewed**. It can also affect the future model by over representing some information. Let's check the skewness through the unbiased skew coefficient.","f2f8571e":"First things to notice:\n1. Many information are embedded within this database. Some categorical features have a lot of differents possible values, so those variables will need to be transformed (expecialy for the diag_1, diag_2 and diag_3 which have more than 700 diffents values).\n2. _weight_ , _payer\\_code_ and _medical\\_specialization_ have a lot of missing values, so we are going to remove those three features.\n3. Some features, especially about drugs, are not well distributed (a lot of _No_ ).  \n4. 101766 different values for _encounter_id_ , and only 71518 for _patient_nbr_ : some patients came several times (which is normal because we are tracking readmission), and we can identify those people.\n  \nThen, we have some cleaning to do before attacking the logistic regression.","3d949a5c":"## Logistic regressions","19cce33d":"We can observe that there is still many False Positive (1000) and False Negative (690). Furthermore an AUC equals to 0.65 is not very good.  \n  \nThus this model is obviously not perfect at all. With an objective to detect readmissions, we can create two thresholds, and separate the two categories into three : **Not likely** to be readmitted, **Likely** to be readmitted, **Very likely** to be readmitted.","1ef8d218":"Looking at the ages validates our hypothesis (or at least it does not refute it). Indeed in the fist encounter of patient **5220**, this patient was **[60-70)**, and then this patient was **[70-80)**.","29b80fc1":"Now we remove all unnecessary data.","64c1f4ef":"We then normalize this categorical feature into 5 binary features. If all are set at 0, it means that this variable is not available."}}