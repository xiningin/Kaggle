{"cell_type":{"a8374400":"code","bb09c307":"code","b8b37b3b":"code","dc27479e":"code","5be90f4e":"code","fc166d69":"code","5c89469d":"code","4255d1a3":"code","78a37409":"code","4303dc2b":"code","1487e759":"code","e2099e17":"code","eb1d8222":"code","eea40c76":"code","9dc92492":"code","58584bee":"markdown","1b7a234c":"markdown","9479285d":"markdown","de3bd635":"markdown","1f9eebfe":"markdown","f08a64e5":"markdown","a03b8624":"markdown","6ee2cd36":"markdown","392f86e3":"markdown","cb35f6f4":"markdown","a3ab6442":"markdown","cec7b2c2":"markdown","c220a4d7":"markdown","92ceba1b":"markdown"},"source":{"a8374400":"from nltk.tokenize import sent_tokenize, word_tokenize  ","bb09c307":"text = \"Hi, My name is Amartya Nambiar. I am a Computer Science Engineer. My favourite color is black\"   # sample text to perform our operations","b8b37b3b":"sent_tokenize(text)    # tokenized on basis of sentence","dc27479e":"words =word_tokenize(text)   #tokenized into words\nprint(len(words))\nprint(words)","5be90f4e":"from nltk.corpus import stopwords        # captured stopwords in the nltk module\nstop = stopwords.words('english')\nprint(len(stop))\nprint(stop[:15])","fc166d69":"clean = [i for i in words if not i in stop]      # removing stopwords from our sample text\nprint(len(clean))\nprint(clean)","5c89469d":"words =word_tokenize(text.lower())           # converting our data into lowercase to get rid of more stopwords. \nclean_lower = [i for i in words if not i in stop]      #this should be done on project-basis\nprint(len(clean_lower))\nprint(clean_lower)","4255d1a3":"import string \npunctuations = list(string.punctuation)        #retrieving the punctuations from string module \nstop += punctuations                            #adding the punctuations to stopwords list\nwords =word_tokenize(text.lower())\nclean_lower = [i for i in words if not i in stop]\nprint(len(clean_lower))\nprint(clean_lower)","78a37409":"from nltk.stem import PorterStemmer    # Importing PorterStemmer \n\nps = PorterStemmer()         # creating an object for PS\nexample = ['helps','helping','helped']   \nstemmed_example = [ps.stem(i) for i in example]\nstemmed_example           # it has been converted to its root meaning 'help'","4303dc2b":"ps.stem('happiness')         # but it isn't always the best choice","1487e759":"from nltk import pos_tag\nfrom nltk.corpus import movie_reviews\ntext = movie_reviews.raw(\"neg\/cv954_19932.txt\")     #retrieving random review","e2099e17":"pos = pos_tag(word_tokenize(text))     #applying pos_tag()\npos[1:5]           # here JJ is adjective, NN is noun, PNN is Pronoun,DT isDdeterminer","eb1d8222":"from nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\nlem.lemmatize('believes')","eea40c76":"lem.lemmatize('happiness')","9dc92492":"lem.lemmatize('happier',pos = 'a')  # we can even provide pos tags - a:adjective, n:noun...","58584bee":"## **Tokenization**","1b7a234c":"## **Parts Of Speech**","9479285d":"## **Stopwords & Flushing them**\n\n","de3bd635":"Now our data is clean from stopwords like 'My','I' and punctuations like '.' and ','","1f9eebfe":"## **Conclusion**\nIn this notebook, I have performed basic NLTK must-know operations for a beginner in NLP.\n\nDefinitions were taken from [tutorialspoint.com](http:\/\/www.tutorialspoint.com)\n\nIf you find this notebook useful, \ud83d\ude4c","f08a64e5":"\nStopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. ","a03b8624":"The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing for English written in the Python programming language. It was developed by Steven Bird and Edward Loper. [Source](https:\/\/en.wikipedia.org\/wiki\/Natural_Language_Toolkit)","6ee2cd36":"To know what is the context of a particular word , for example : Shyam is a Proper Noun,Desk is a Noun and Happy is an adjective. ","392f86e3":"## **Lemmatization**\n\nPorterStemmer class chops off the suffixes from the word but this isn't the best thing to apply to clean our data. \n\nStemming technique only looks at the form of the word whereas Lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word.","cb35f6f4":"In the field of Natural Language Processing, tokenization basically refers to splitting up a larger body of text into smaller lines or words. \n\nThere are mainly two types of tokenization :\n- Sentence Tokenization\n- Word Tokenization","a3ab6442":"## **Stemming**","cec7b2c2":"Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n\nThere are mainly two widely used Stemmer Algorithms:\n- Porter Stemmer  (we'll work on this)\n- Lancaster Stem","c220a4d7":"#### **Introduction to NLTK**","92ceba1b":"We had 20 tokens before, now 16. Which means we have a cleaner text data with no stopwords."}}