{"cell_type":{"bbcf755e":"code","5b047f91":"code","69c92a5e":"code","99de1182":"code","d6ce035b":"code","739cadfb":"code","8c7f1187":"code","a015b394":"code","a669b078":"code","58647251":"code","b8e03ad1":"code","53781cd0":"code","c356e2c5":"code","09f401a0":"code","d39ee2d0":"code","f05d3a16":"code","ad80cb4e":"code","6349c5e7":"code","aafca748":"code","68e6dd7a":"code","d5fdd467":"code","15895566":"code","c8d0da3d":"code","0aa03be9":"code","53b9e71a":"code","d95a240a":"code","7124dc9a":"code","de1d3c1f":"markdown","90b6bd9b":"markdown","32ba773a":"markdown","0cea6f7d":"markdown","c9a14787":"markdown","fd206e59":"markdown","181790f8":"markdown"},"source":{"bbcf755e":"# General Libs\nfrom tensorflow import keras\n#from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n#from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\nfrom tensorflow.keras.applications.densenet import DenseNet201, preprocess_input\n#from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\n%matplotlib inline","5b047f91":"# General Libs\n#from tensorflow import keras\n#from tensorflow.keras.preprocessing.image import ImageDataGenerator\n#from tensorflow.keras.preprocessing import image\n#from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n#from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\n#from tensorflow.keras.applications.densenet import DenseNet201, preprocess_input\n#from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n#from tensorflow.keras.layers import Dense, Flatten\n#from tensorflow.keras.models import Model\n#from tensorflow.keras.optimizers import Adam\n#import numpy as np\n#import random\n#import matplotlib.pyplot as plt\n#%matplotlib inline","69c92a5e":"im_shape = (250,250)\n\n\nTRAINING_DIR = '..\/input\/brain-tumor-classification-mri\/Training'\nTEST_DIR = '..\/input\/brain-tumor-classification-mri\/Testing'\n\nseed = 10\n\nBATCH_SIZE = 16","99de1182":"#Using keras ImageGenerator and flow_from_directoty\n\n# Subdivision in test\/validation\ndata_generator = ImageDataGenerator(rescale=1.\/255, validation_split=0.3)\nval_data_generator = ImageDataGenerator(rescale=1.\/255, validation_split=0.3)","d6ce035b":"# If you want data augmentation, uncomment and run this cell\ndata_generator = ImageDataGenerator(\n        validation_split=0.3,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        #color_mode=grayscale,\n        rescale=1.\/255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\n\nval_data_generator = ImageDataGenerator(rescale=1.\/255, validation_split=0.2)","739cadfb":"# Generator para parte train\ntrain_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=im_shape, shuffle=True, seed=seed,\n                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n# Generator para parte valida\u00e7\u00e3o\nvalidation_generator = val_data_generator.flow_from_directory(TRAINING_DIR, target_size=im_shape, shuffle=False, seed=seed,\n                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n\n# Generator para dataset de teste\ntest_generator = ImageDataGenerator(rescale=1.\/255)\ntest_generator = test_generator.flow_from_directory(TEST_DIR, target_size=im_shape, shuffle=False, seed=seed,\n                                                     class_mode='categorical', batch_size=BATCH_SIZE)\n\nnb_train_samples = train_generator.samples\nnb_validation_samples = validation_generator.samples\nnb_test_samples = test_generator.samples\nclasses = list(train_generator.class_indices.keys())\nprint('Classes: '+str(classes))\nnum_classes  = len(classes)\n","8c7f1187":"# Visualizing some examples\nplt.figure(figsize=(15,15))\nfor i in range(9):\n    #gera subfigures\n    plt.subplot(330 + 1 + i)\n    batch = train_generator.next()[0]*255\n    image = batch[0].astype('uint8')\n    plt.imshow(image)\nplt.show()","a015b394":"model = Sequential()\nmodel.add(Conv2D(200, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(im_shape[0],im_shape[1],3)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(100, kernel_size=(3,3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #adicionado novo pooling\nmodel.add(Conv2D(100, kernel_size=(3,3), activation='relu'))#adicionei mais uma camada de neuronios\nmodel.add(Flatten())\nmodel.add(Dense(300, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.summary()\n\n# Compila o modelo\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=Adam(),\n              metrics=['accuracy'])","a669b078":"epochs = 5\n\n#Callback to save the best model\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,verbose=1)\n]\n\n#Training\nhistory = model.fit(\n        train_generator,\n        steps_per_epoch=nb_train_samples \/\/ BATCH_SIZE,\n        epochs=epochs,\n        callbacks = callbacks_list,\n        validation_data=validation_generator,\n        verbose = 1,\n        validation_steps=nb_validation_samples \/\/ BATCH_SIZE)","58647251":"# Training curves\nimport matplotlib.pyplot as plt\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs_x = range(1, len(loss_values) + 1)\nplt.figure(figsize=(10,10))\nplt.subplot(2,1,1)\nplt.plot(epochs_x, loss_values, 'bo', label='Training loss')\nplt.plot(epochs_x, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation Loss and Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.subplot(2,1,2)\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs_x, acc_values, 'bo', label='Training acc')\nplt.plot(epochs_x, val_acc_values, 'b', label='Validation acc')\n#plt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Acc')\nplt.legend()\nplt.show()","b8e03ad1":"# Load the best saved model\nfrom tensorflow.keras.models import load_model\n\nmodel = load_model('model.h5')","53781cd0":"# Using the validation dataset\nscore = model.evaluate_generator(validation_generator)\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1])","c356e2c5":"# Using the test dataset\nscore = model.evaluate_generator(test_generator)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","09f401a0":"import itertools\n\n#Plot the confusion matrix. Set Normalize = True\/False\ndef plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n","d39ee2d0":"# Some reports\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\n#On test dataset\nY_pred = model.predict_generator(test_generator)\ny_pred = np.argmax(Y_pred, axis=1)\ntarget_names = classes\n\n#Confution Matrix\ncm = confusion_matrix(test_generator.classes, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\n#Classification Report\nprint('Classification Report')\nprint(classification_report(test_generator.classes, y_pred, target_names=target_names))","f05d3a16":"# Alguns par\u00e2metros para leitura do dataset\nim_shape = (299,299)\n\nTRAINING_DIR = '..\/input\/brain-tumor-classification-mri\/Training'\nTEST_DIR = '..\/input\/brain-tumor-classification-mri\/Testing'\n\nseed = 10\n\nBATCH_SIZE = 16","ad80cb4e":"#Using keras ImageGenerator and flow_from_directoty\n\n# Image dataset without augmentation\n#data_generator = ImageDataGenerator(preprocessing_function=preprocess_input, validation_split=0.2)\n# With augmentation\ndata_generator = ImageDataGenerator(\n        validation_split=0.2,\n        rotation_range=20,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        preprocessing_function=preprocess_input,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True,\n        fill_mode='nearest')\nval_data_generator = ImageDataGenerator(preprocessing_function=preprocess_input,validation_split=0.2)","6349c5e7":"\n# Generator para parte train\ntrain_generator = data_generator.flow_from_directory(TRAINING_DIR, target_size=im_shape, shuffle=True, seed=seed,\n                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"training\")\n# Generator para parte valida\u00e7\u00e3o\nvalidation_generator = val_data_generator.flow_from_directory(TRAINING_DIR, target_size=im_shape, shuffle=False, seed=seed,\n                                                     class_mode='categorical', batch_size=BATCH_SIZE, subset=\"validation\")\n\n# Generator para dataset de teste\ntest_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\ntest_generator = test_generator.flow_from_directory(TEST_DIR, target_size=im_shape, shuffle=False, seed=seed,\n                                                     class_mode='categorical', batch_size=BATCH_SIZE)\n\nnb_train_samples = train_generator.samples\nnb_validation_samples = validation_generator.samples\nnb_test_samples = test_generator.samples\nclasses = list(train_generator.class_indices.keys())\nprint('Classes: '+str(classes))\nnum_classes  = len(classes)","aafca748":"# Visualizando alguns exemplos do dataset por meio do Generator criado\nplt.figure(figsize=(15,15))\nfor i in range(9):\n    #gera subfigures\n    plt.subplot(330 + 1 + i)\n    batch = train_generator.next()[0]*255\n    image = batch[0].astype('uint8')\n    plt.imshow(image)\nplt.show()","68e6dd7a":"#base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(im_shape[0], im_shape[1], 3))\n\nbase_model = DenseNet201(weights='imagenet', include_top=False, input_shape=(im_shape[0], im_shape[1], 3))\n\n#base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(im_shape[0], im_shape[1], 3))\n\n#base_model = VGG19(weights='imagenet', include_top=False, input_shape=(im_shape[0], im_shape[1], 3))\n\nx = base_model.output\nx = Flatten()(x)\nx = Dense(300, activation='relu')(x)\npredictions = Dense(num_classes, activation='softmax', kernel_initializer='random_uniform')(x)\n\nmodel = Model(inputs=base_model.input, outputs=predictions)\n\n# Freezing pretrained layers\nfor layer in base_model.layers:\n    layer.trainable=False\n    \noptimizer = Adam()\nmodel.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])","d5fdd467":"epochs = 8\n\n# Saving the best model\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,verbose=1)\n]\n\nhistory = model.fit(\n        train_generator,\n        steps_per_epoch=nb_train_samples \/\/ BATCH_SIZE,\n        epochs=epochs,\n        callbacks = callbacks_list,\n        validation_data=validation_generator,\n        verbose = 1,\n        validation_steps=nb_validation_samples \/\/ BATCH_SIZE)","15895566":"#Vamos ver como foi o treino?\nimport matplotlib.pyplot as plt\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\n\nepochs_x = range(1, len(loss_values) + 1)\nplt.figure(figsize=(10,10))\nplt.subplot(2,1,1)\nplt.plot(epochs_x, loss_values, 'bo', label='Training loss')\nplt.plot(epochs_x, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation Loss and Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n#plt.legend()\nplt.subplot(2,1,2)\nacc_values = history_dict['accuracy']\nval_acc_values = history_dict['val_accuracy']\nplt.plot(epochs_x, acc_values, 'bo', label='Training acc')\nplt.plot(epochs_x, val_acc_values, 'b', label='Validation acc')\n#plt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Acc')\nplt.legend()\nplt.show()","c8d0da3d":"from tensorflow.keras.models import load_model\n# Load the best saved model\nmodel = load_model('model.h5')","0aa03be9":"# Using the validation dataset\nscore = model.evaluate_generator(validation_generator)\n\nprint('Val loss:', score[0])\nprint('Val accuracy:', score[1]) #tensorflow.keras.applications","53b9e71a":"# Using the test dataset\nscore = model.evaluate_generator(test_generator)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","d95a240a":"import itertools\n\n#Plot the confusion matrix. Set Normalize = True\/False\ndef plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","7124dc9a":"# Some reports\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n\n#Confution Matrix and Classification Report\nY_pred = model.predict_generator(test_generator)#, nb_test_samples \/\/ BATCH_SIZE, workers=1)\ny_pred = np.argmax(Y_pred, axis=1)\ntarget_names = classes\n\n#Confution Matrix\ncm = confusion_matrix(test_generator.classes, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\nprint('Classification Report')\nprint(classification_report(test_generator.classes, y_pred, target_names=target_names))","de1d3c1f":"**inception_resnet_v2: val_loss = 0.5493817329406738; val_accuracy = 0.8045375347137451; dense = 100; epoca = 5\n\ninception_resnet_v2: val_loss = 0.5135496258735657; val_accuracy = 0.7958115339279175; dense = 200; epoca = 5**","90b6bd9b":"DenseNet201: val_loss = 0.4068634808063507; val_accuracy = 0.8551483154296875; dense = 200; epoca = 5\n\nDenseNet201: val_loss = 0.4507255554199219; val_accuracy = 0.8481675386428833; dense = 300; epoca = 5\n\nDenseNet201: val_loss = 0.4709782600402832; val_accuracy = 0.8429319262504578; dense = 400; epoca = 8","32ba773a":"VGG19: val_loss = 0.5869849324226379; val_accuracy = 0.8097731471061707; dense = 100; epoca = 5\n\nVGG19: val_loss = 0.5944948792457581; val_accuracy = 0.8132635354995728; dense = 300; epoca = 5\n\nVGG19: val_loss = 0.516057014465332; val_accuracy = 0.8115183115005493; dense = 300; epoca = 8\n\n","0cea6f7d":"# Transfer Learning from a Deep Model","c9a14787":"InceptionV3: val_loss = 0.4922262728214264; val_accuracy = 0.7993019223213196; dense = 100; epoca = 5\n\nInceptionV3: val_loss = 0.44336673617362976; val_accuracy = 0.8342059254646301; dense = 200; epoca = 5\n\nInceptionV3: val_loss = 0.5576071739196777; val_accuracy = 0.7731239199638367; dense = 300; epoca = 8","fd206e59":"# Lendo o dataset","181790f8":"# Imports de Libs"}}