{"cell_type":{"8680c126":"code","f2ec1a93":"code","6fa43cf4":"code","763894e7":"code","f3bff188":"code","be16c745":"code","51eb365e":"code","0e76b12e":"code","edd4bdb0":"code","19096dd8":"code","eac0c987":"code","b98adc97":"code","f58cc61b":"code","d393d3f2":"code","a6a33006":"code","fb04b6e7":"code","54661e4c":"code","15a41525":"code","ef1633aa":"code","05dfbb13":"code","be2cfb92":"code","b0472b22":"code","80f8c3a7":"code","95a573d7":"code","d4db70c7":"code","110f7479":"code","6bbcc3c4":"code","e2f14e56":"markdown","29722021":"markdown","edab4dcf":"markdown","6e5b7e63":"markdown","a5875827":"markdown"},"source":{"8680c126":"#import libraries\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nsns.set(color_codes=True)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#sklearn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics","f2ec1a93":"#lets define col names\ncolnames = ['preg','plas','pres','skin','test','mass','pedi','age','class']\npimadf = pd.read_csv(\"..\/input\/data-science-machine-learning-and-ai-using-python\/pima-indians-diabetes.data\", names=colnames)","6fa43cf4":"pimadf.head()","763894e7":"std = StandardScaler()","f3bff188":"X = pimadf.drop(\"class\", axis=1)\nY = pimadf['class']","be16c745":"X = std.fit_transform(X)","51eb365e":"#Lets split the data\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.30, random_state=7)","0e76b12e":"#lets choose the model\nmodel = GaussianNB()","edd4bdb0":"#Lets train the model\nmodel.fit(X_train, Y_train)\nprint(model)","19096dd8":"#lets make the predictions\npred = model.predict(X_test)","eac0c987":"#lets check the accuracy of the model by printing its score\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\nmodel_score = model.score(X_test, Y_test)\nmodel_score","b98adc97":"metrics.confusion_matrix(pred, Y_test)","f58cc61b":"#lets find the probability\ny_pred_prob = model.predict_proba(X_test)","d393d3f2":"from sklearn.metrics import auc, roc_curve\n\nfpr,tpr,thresholds = roc_curve(Y_test, y_pred_prob[::,1])\nroc_auc = auc(fpr,tpr)\nroc_auc","a6a33006":"#lets plot the roc curve\nplt.plot(fpr,tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0,1],[0,1], color='navy', linestyle='--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Reciever operating characteristic')\nplt.legend(loc='lower right')\nplt.show()","fb04b6e7":"#lets import the libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","54661e4c":"#load the data into data variable\ndata = pd.read_csv('..\/input\/data-science-machine-learning-and-ai-using-python\/Example.csv')\ndata.head()","15a41525":"#plot the data using scatter plot: hint: x=data[Satisfaction], y=data['Loyalty]\nplt.scatter(data['Satisfaction'], data['Loyalty'])\nplt.xlabel('Satisfaction')\nplt.ylabel('Loyalty')\nplt.show()","ef1633aa":"#copy the entire data into X and select the features\nX = data.copy()","05dfbb13":"#Clustering\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(2)\nkmeans.fit(X)","be2cfb92":"#Lets copy the clustering result\nclusters = X.copy()\nclusters['cluster_pred'] = kmeans.fit_predict(X)","b0472b22":"#lets plot the clustered data\nplt.scatter(clusters['Satisfaction'],clusters['Loyalty'], c = clusters['cluster_pred'], cmap='rainbow')\nplt.xlabel('Satisfaction')\nplt.ylabel('Loyalty')\nplt.show()","80f8c3a7":"#lets standardize the variables\nfrom sklearn import preprocessing\n\nx_scaled = preprocessing.scale(X)\nx_scaled","95a573d7":"#Elbow method\nwcss = []\n\nfor i in range(1,30):\n  kmeans = KMeans(i)\n  kmeans.fit(x_scaled)\n  wcss.append(kmeans.inertia_)\n\nwcss","d4db70c7":"#lets visualize the elbow method\nplt.plot(range(1,30), wcss)\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","110f7479":"#now lets take clusters = 4\nkmeans_new = KMeans(4)\nkmeans_new.fit(x_scaled)\n\ncluster_new = X.copy()\ncluster_new['cluster_pred'] = kmeans_new.fit_predict(x_scaled)\ncluster_new","6bbcc3c4":"plt.scatter(cluster_new['Satisfaction'],cluster_new['Loyalty'], c = cluster_new['cluster_pred'], cmap='rainbow')\nplt.xlabel('Satisfaction')\nplt.ylabel('Loyalty')\nplt.show()","e2f14e56":"**KMeans Clustering**\n\nDivision of datapoints into clusters such that each data point is present in only one cluster","29722021":"**Analysis**\n\n1. **Blue Churn:** These are the customers who are less satisfied and less loyal, and therfore can be termed as *Alienated*\n\n2. **Yellow Churn :** These people are less satisfied but are highly loyal\n\n3. **Purple Churn :** These people are with high loyality and high satisfaction and they are termed as *Fans*\n\n4. **Red Churn :** These are the people who are in midst of the things.","edab4dcf":"**Problem Statement**\n\nTo analyze the type of customers in the market based on the features","6e5b7e63":"**Problem Statement**\n\nTo classify patients as diabetic or non-diabetic. The dataset has several different medical predictor features and a target that is **Outcome**. Predictor variables include the number of pregnancies that patient had, their BMI, insulin level, age and so on.....","a5875827":"# **Day 11**\n\n**Naive Bayes Classifier**\n\nIt is a kind of classifier that works on Bayes theorem. Prediction of membership probabilities is made for every class such as the probability of data points is associated a particular class."}}