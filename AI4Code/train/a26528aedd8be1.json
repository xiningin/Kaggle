{"cell_type":{"b37cbb7b":"code","bd955e98":"code","a3e5c3d5":"code","703ede2a":"code","05d8af37":"code","42b037a0":"markdown","bdd7652a":"markdown","572850d8":"markdown","d37a8034":"markdown","abe93bff":"markdown"},"source":{"b37cbb7b":"import numpy as np\nfrom sklearn import preprocessing\n\n# \u52a0\u8f7d\u6570\u636e\nlines=np.loadtxt('..\/input\/USA_Housing.csv', delimiter=',', dtype='str')\nprint(\"\u8f93\u5165\")\nfor i in range(lines.shape[1]-1):\n    print(lines[0, i])\nprint(\"\u6807\u7b7e\")\nprint(lines[0,-1])\n\nx_total = lines[1:, :5].astype('float')\ny_total = lines[1:, 5:].astype('float').flatten()\n\n# \u6570\u636e\u9884\u5904\u7406\u548c\u5207\u5206\nx_total = preprocessing.scale(x_total)\ny_total = preprocessing.scale(y_total)\nx_train = x_total[:4000]\nx_test = x_total[4000:]\ny_train = y_total[:4000]\ny_test = y_total[4000:]\nprint('\u8bad\u7ec3\u96c6\u5927\u5c0f: ', x_train.shape[0])\nprint('\u6d4b\u8bd5\u96c6\u5927\u5c0f: ', x_test.shape[0])\n","bd955e98":"X_train = np.hstack([x_train, np.ones((x_train.shape[0], 1))])\nNE_solution = np.dot(np.dot(np.linalg.inv(np.dot(np.transpose(X_train), X_train)), np.transpose(X_train)), y_train.reshape([-1, 1]))\nprint(NE_solution)\n\nX_test = np.hstack([x_test, np.ones((x_test.shape[0], 1))])\ny_pred_test = np.dot(X_test, NE_solution).flatten()\n\nrmse_loss = np.sqrt(np.square(y_test - y_pred_test).mean())\nprint('rmse_loss:', rmse_loss)","a3e5c3d5":"from sklearn import linear_model\n\nlinreg = linear_model.LinearRegression()\nlinreg.fit(x_train, y_train)\nprint(linreg.coef_)\nprint(linreg.intercept_)\ny_pred_test = linreg.predict(x_test)\n\nrmse_loss = np.sqrt(np.square(y_test - y_pred_test).mean())\nprint('rmse_loss:', rmse_loss)","703ede2a":"def shuffle_aligned_list(data):\n    num = data[0].shape[0]\n    shuffle_index = np.random.permutation(num)\n    return [d[shuffle_index] for d in data]\n\ndef batch_generator(data, batch_size, shuffle=True):\n    batch_count = 0\n    while True:\n        if batch_count * batch_size + batch_size >= data[0].shape[0]:\n            batch_count = 0\n            if shuffle:\n                data = shuffle_aligned_list(data)\n        start = batch_count * batch_size\n        end = start + batch_size\n        batch_count += 1\n        yield [d[start:end] for d in data]","05d8af37":"import matplotlib.pyplot as plt\nnum_steps = 600\nlearning_rate = 0.01\nbatch_size = 40\n\nweight = np.zeros(6)\nnp.random.seed(0)\nbatch_g = batch_generator([x_train, y_train], batch_size, shuffle=True)\nx_test_concat = np.hstack([x_test, np.ones([x_test.shape[0], 1])])\n\nloss_list = []\nfor i in range(num_steps):\n    rmse_loss = np.sqrt(np.square(np.dot(x_test_concat, weight) - y_test).mean())\n    loss_list.append(rmse_loss)\n    \n    x_batch, y_batch = batch_g.__next__()\n    x_batch = np.hstack([x_batch, np.ones([batch_size, 1])])\n    y_pred = np.dot(x_batch, weight)\n    w_gradient = (x_batch * np.tile((y_pred - y_batch).reshape([-1, 1]), 6)).mean(axis=0)\n    weight = weight - learning_rate * w_gradient \n\nprint('weight:', weight)\nprint('rmse_loss:', rmse_loss)\n    \nloss_array = np.array(loss_list)\nplt.plot(np.arange(num_steps), loss_array)\nplt.show()\n","42b037a0":"# \u7ebf\u6027\u56de\u5f52\n\n* \u4efb\u52a1\uff1a \u9884\u6d4b\u623f\u4ef7\n* \u8f93\u5165\uff1a\u4e94\u7ef4\u7279\u5f81\n* \u6807\u7b7e\uff1a\u623f\u4ef7\n","bdd7652a":"### 2. Normal Equation\n$\\mu = (X^T X)^{-1}X^Ty$","572850d8":"### 4. Gradient Descent\n$\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\theta_j} J(\\theta) &= \\frac{\\partial}{\\partial \\theta_j} \\frac{1}{2} (h_\\theta(x) - y)^2\\\\\n &= 2 \\cdot \\frac{1}{2} (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} (h_\\theta(x) - y) \\\\\n &= (h_\\theta(x) - y) \\cdot \\frac{\\partial}{\\partial \\theta_j} (\\sum_{i=0}^n \\theta_i x_i - y) \\\\\n &= (h_\\theta(x) - y) x_j\n\\end{aligned}\n$","d37a8034":"### 3. Sklearn\u5e93","abe93bff":"### 1. \u6570\u636e\u51c6\u5907"}}