{"cell_type":{"143bc097":"code","5236b0b3":"code","f8d9f8c0":"code","342b2908":"code","38cd18aa":"code","405ed9a7":"code","c8bf9d5c":"code","8f867a0e":"code","65a92b1d":"code","03124d60":"code","5c3fcf2b":"code","c6122a17":"code","edabfed1":"code","6ef229d7":"code","2c44f4ad":"code","dafcb75a":"code","9a64abf1":"code","6bae10ff":"code","6f1de228":"code","ce748d3b":"code","b9fdfaf6":"code","3cfec1f5":"code","6199de24":"code","444a4044":"code","c71ae747":"code","317ab35a":"code","9ddfecfb":"code","772c93c0":"code","4aa071ac":"code","448bda9f":"code","c455fe3e":"code","ac2df139":"markdown","7d5c73ee":"markdown","9d2ceac4":"markdown","15cfa1a7":"markdown","d58c5eef":"markdown","47f26c3c":"markdown","5ce3e929":"markdown","856a74c2":"markdown","eacc2032":"markdown","ebc689ed":"markdown","d16f1b30":"markdown","6fcfbfaf":"markdown","0ed30123":"markdown","a201499d":"markdown","07457ab1":"markdown","dcf7f59b":"markdown","6933ffbc":"markdown","9b1f77bf":"markdown","9dd60816":"markdown","58928608":"markdown","d3ec31d7":"markdown","d036d2e1":"markdown","3a1400d0":"markdown","78c4071d":"markdown"},"source":{"143bc097":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5236b0b3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.utils import class_weight\nimport warnings\nwarnings.filterwarnings('ignore')\n","f8d9f8c0":"train_df=pd.read_csv('\/kaggle\/input\/without1-3\/mitbih_train.csv',header=None)\ntest_df=pd.read_csv('\/kaggle\/input\/without01-03\/mitbih_test.csv',header=None)","342b2908":"train_df[187]=train_df[187].astype(int)\nequilibre=train_df[187].value_counts()\nprint(equilibre)\n\n","38cd18aa":"plt.figure(figsize=(20,10))\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(equilibre, labels=['n','v','f'], colors=['red','blue','orange'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","405ed9a7":"from sklearn.utils import resample\n#df_1=train_df[train_df[187]==1]\ndf_2=train_df[train_df[187]==2]\n#df_3=train_df[train_df[187]==3]\ndf_4=train_df[train_df[187]==4]\ndf_0=(train_df[train_df[187]==0]).sample(n=20000,random_state=42)\n\n#df_1_upsample=resample(df_1,replace=True,n_samples=20000,random_state=123)\ndf_2_upsample=resample(df_2,replace=True,n_samples=20000,random_state=124)\n#df_3_upsample=resample(df_3,replace=True,n_samples=20000,random_state=125)\ndf_4_upsample=resample(df_4,replace=True,n_samples=20000,random_state=126)\n\ntrain_df=pd.concat([df_0,df_2_upsample,df_4_upsample])","c8bf9d5c":"equilibre=train_df[187].value_counts()\nprint(equilibre)\n","8f867a0e":"plt.figure(figsize=(20,10))\nmy_circle=plt.Circle( (0,0), 0.7, color='white')\nplt.pie(equilibre, labels=['n','v','f'], colors=['red','blue','orange'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","65a92b1d":"c=train_df.groupby(187,group_keys=False).apply(lambda train_df : train_df.sample(1))","03124d60":"c","5c3fcf2b":" plt.plot(c.iloc[1,:186])","c6122a17":"def plot_hist(class_number,size,min_):\n    img=train_df.loc[train_df[187]==class_number].values\n    img=img[:,min_:size]\n    img_flatten=img.flatten()\n\n    final1=np.arange(min_,size)\n    for i in range (img.shape[0]-1):\n        tempo1=np.arange(min_,size)\n        final1=np.concatenate((final1, tempo1), axis=None)\n    print(len(final1))\n    print(len(img_flatten))\n    plt.hist2d(final1,img_flatten, bins=(80,80),cmap=plt.cm.jet)\n    plt.show()","edabfed1":"plot_hist(0,70,5)","6ef229d7":" plt.plot(c.iloc[1,:186])","2c44f4ad":"plot_hist(0,50,5)","dafcb75a":" plt.plot(c.iloc[2,:186])","9a64abf1":"plot_hist(2,60,30)","6bae10ff":"plt.plot(c.iloc[2,:186])","6f1de228":"plot_hist(2,60,25)","ce748d3b":"plt.plot(c.iloc[0,:186])","b9fdfaf6":"plot_hist(4,50,18)","3cfec1f5":"def add_gaussian_noise(signal):\n    noise=np.random.normal(0,0.05,186)\n    return (signal+noise)\n","6199de24":"tempo=c.iloc[0,:186]\nbruiter=add_gaussian_noise(tempo)\n\nplt.subplot(2,1,1)\nplt.plot(c.iloc[0,:186])\n\nplt.subplot(2,1,2)\nplt.plot(bruiter)\n\nplt.show()\n\n","444a4044":"target_train=train_df[187]\ntarget_test=test_df[187]\ny_train=to_categorical(target_train)\ny_test=to_categorical(target_test)","c71ae747":"X_train=train_df.iloc[:,:186].values\nX_test=test_df.iloc[:,:186].values\n#for i in range(len(X_train)):\n#    X_train[i,:186]= add_gaussian_noise(X_train[i,:186])\nX_train = X_train.reshape(len(X_train), X_train.shape[1],1)\nX_test = X_test.reshape(len(X_test), X_test.shape[1],1)","317ab35a":"def network(X_train,y_train,X_test,y_test):\n    \n    \n\n    im_shape=(X_train.shape[1],1)\n    inputs_cnn=Input(shape=(im_shape), name='inputs_cnn')\n    conv1_1=Convolution1D(64, (6), activation='relu', input_shape=im_shape)(inputs_cnn)\n    conv1_1=BatchNormalization()(conv1_1)\n    pool1=MaxPool1D(pool_size=(3), strides=(2), padding=\"valid\")(conv1_1)\n    conv2_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool1)\n    conv2_1=BatchNormalization()(conv2_1)\n    pool2=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv2_1)\n    conv3_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool2)\n    conv3_1=BatchNormalization()(conv3_1)\n    pool3=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv3_1)\n    \n    conv4_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool3)\n    conv4_1=BatchNormalization()(conv4_1)\n    pool4=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv4_1)\n    conv5_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool4)\n    conv5_1=BatchNormalization()(conv3_1)\n    pool5=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv5_1)\n    \n    conv6_1=Convolution1D(64, (3), activation='relu', input_shape=im_shape)(pool5)\n    conv6_1=BatchNormalization()(conv6_1)\n    pool6=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv6_1)\n    \n    conv7_1=Convolution1D(64, (3), activation='swish', input_shape=im_shape)(pool6)\n    conv7_1=BatchNormalization()(conv7_1)\n    pool7=MaxPool1D(pool_size=(2), strides=(2), padding=\"same\")(conv7_1)\n    flatten=Flatten()(pool7)\n    dense_end1 = Dense(300, activation='tanh')(flatten)\n    dense_end2 = Dense(200, activation='tanh')(dense_end1)\n    main_output = Dense(5, activation='softmax', name='main_output')(dense_end2)\n    \n    \n    model = Model(inputs= inputs_cnn, outputs=main_output)\n    model.compile(optimizer='adam', loss='categorical_crossentropy',metrics = ['accuracy'])\n    \n    \n    callbacks = [EarlyStopping(monitor='val_loss', patience=8),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n\n    history=model.fit(X_train, y_train,epochs=30,callbacks=callbacks, batch_size=32,validation_data=(X_test,y_test))\n    model.load_weights('best_model.h5')\n    return(model,history)","9ddfecfb":"def evaluate_model(history,X_test,y_test,model):\n    scores = model.evaluate((X_test),y_test, verbose=0)\n    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n    \n    print(history)\n    fig1, ax_acc = plt.subplots()\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Model - Accuracy')\n    plt.legend(['Training', 'Validation'], loc='lower right')\n    plt.show()\n    \n    fig2, ax_loss = plt.subplots()\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Model- Loss')\n    plt.legend(['Training', 'Validation'], loc='upper right')\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.show()\n    target_names=['0','2','4']\n    \n    y_true=[]\n    for element in y_test:\n        y_true.append(np.argmax(element))\n    prediction_proba=model.predict(X_test)\n    prediction=np.argmax(prediction_proba,axis=1)\n    cnf_matrix = confusion_matrix(y_true, prediction)\n    ","772c93c0":"\nfrom keras.layers import Dense, Convolution1D, MaxPool1D, Flatten, Dropout\nfrom keras.layers import Input\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nimport keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nmodel,history=network(X_train,y_train,X_test,y_test)\n\n","4aa071ac":"evaluate_model(history,X_test,y_test,model)\ny_pred=model.predict(X_test)\n","448bda9f":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure(figsize=(10, 10))\nplot_confusion_matrix(cnf_matrix, classes=['N', 'V', 'Q'],normalize=True,\n                      title='Confusion matrix, with normalization')\nplt.show()\n\n","c455fe3e":"import pandas as pd\nmitbih_test = pd.read_csv(\"..\/input\/mitbih_test.csv\")","ac2df139":"Here is a normal beat. I don't have something particular to say on that class.  ","7d5c73ee":"I use a fonction ( will depend of the version) where i add a noise to the data to generilize my train.","9d2ceac4":"Here is an exemple of the two classes :\n\n![4-Figure2-1.png](attachment:4-Figure2-1.png)\n\nin the second and third line you have the 2 et 3 class.","15cfa1a7":"In this part i want to study the differente classes. ","d58c5eef":"![image.png](attachment:image.png)\n","47f26c3c":"**Balance of dataset**","5ce3e929":"Fusion beat :\n![3-s2.0-B9780124159365000098-f09-10-9780124159365.jpg](attachment:3-s2.0-B9780124159365000098-f09-10-9780124159365.jpg)\n\nDon't really see the difference with the previous one but i'm not an expert of ECG!","856a74c2":"An electrocardiogram (ECG) is a simple test that can be used to check your heart's rhythm and electrical activity.\n\nSensors attached to the skin are used to detect the electrical signals produced by your heart each time it beats.\n\nThese signals are recorded by a machine and are looked at by a doctor to see if they're unusual.\n\nAn ECG may be requested by a heart specialist (cardiologist) or any doctor who thinks you might have a problem with your heart, including your GP.\nThat's the result of this test we will analyze. ","eacc2032":"I take one sample per class and i store it in a datafrmae in order to have an exmeple. ","ebc689ed":"We underlign that two class(supraventricular and fusion) is weeker than the other. maybe due to less exemple in the starter dataset. I will try to improve in the next version.","d16f1b30":"In this notebook i want to predict different arrhytmia on ECG. We have two different dataset, but i will consider at start only one : mitbih.\nThe MIT-BIH Arrhythmia Database contains 48 half-hour excerpts of two-channel ambulatory ECG recordings, obtained from 47 subjects studied by the BIH Arrhythmia Laboratory between 1975 and 1979. Twenty-three recordings were chosen at random from a set of 4000 24-hour ambulatory ECG recordings collected from a mixed population of inpatients (about 60%) and outpatients (about 40%) at Boston's Beth Israel Hospital; the remaining 25 recordings were selected from the same set to include less common but clinically significant arrhythmias that would not be well-represented in a small random sample.\nsouces : https:\/\/physionet.org\/content\/mitdb\/1.0.0\/\n\n\nArrhythmia Dataset\n\n    Number of Samples: 109446\n    Number of Categories: 5\n    Sampling Frequency: 125Hz\n    Data Source: Physionet's MIT-BIH Arrhythmia Dataset\n    Classes: ['N': 0, 'S': 1, 'V': 2, 'F': 3, 'Q': 4]\n\n\n-N : Non-ecotic beats (normal beat)\n-S : Supraventricular ectopic beats\n-V : Ventricular ectopic beats\n-F : Fusion Beats\n-Q : Unknown Beats","6fcfbfaf":"**What is an ecg? **","0ed30123":"**Classes**","a201499d":"<font color='red'>**If you like my work,please consider giving an upvote !!!**<\/font>\n","07457ab1":"Resample works perfectly we can go on. ","dcf7f59b":"i take the next function from : https:\/\/www.kaggle.com\/coni57\/model-from-arxiv-1805-00794","6933ffbc":"**INTRODUCTION**","9b1f77bf":"**Pretreat**","9dd60816":"Here is a representation for all the class. We take all the signal and map them. Like that we have an estimation what the signal can look like.","58928608":"In this part i will speak o n what i do to transform data. ","d3ec31d7":"We can underligned a huge difference in the balanced of the classes. After some try i have decided to  choose the resample technique more than the class weights for the algorithms. ","d036d2e1":"I will not comment a lot this one because it correspond to other class. ","3a1400d0":"**Load Data**","78c4071d":"**Network**"}}