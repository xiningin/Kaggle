{"cell_type":{"ecd5cb6d":"code","ef16ac4f":"code","a9bc7443":"code","2a36d2d9":"code","e92c285f":"code","9e6d34ed":"code","b9f4ebe3":"code","57001773":"code","1b650ba6":"code","c3a3a421":"code","974df431":"code","76482cae":"code","13b1ae6a":"code","dbd21c11":"code","0be9da84":"markdown","e7455f01":"markdown","5f801038":"markdown","ed3e5f27":"markdown","1b6b66f3":"markdown","dc3ac966":"markdown","300debc5":"markdown","94b012c6":"markdown","36e2a775":"markdown","bec4ecb7":"markdown","f56dcbd3":"markdown","baa70d63":"markdown","d2e94304":"markdown","dc88c975":"markdown","ff4eae65":"markdown","0bcc46be":"markdown","79afe3c1":"markdown","98b8621d":"markdown","5fa1c1a2":"markdown","52416456":"markdown","a7104d34":"markdown","cae61082":"markdown","18ea8af3":"markdown","00cd4a7b":"markdown","a9abe3fd":"markdown"},"source":{"ecd5cb6d":"%%capture\n\n# install necessary libraries from input\n# import progressbar library for offline usage\n!ls ..\/input\/progresbar2local\n!pip install progressbar2 --no-index --find-links=file:\/\/\/kaggle\/input\/progresbar2local\/progressbar2\n\n# import text stat library for additional ml data prep\n!ls ..\/input\/textstat-local\n!pip install textstat --no-index --find-links=file:\/\/\/kaggle\/input\/textstat-local\/textstat ","ef16ac4f":"%%capture\n\n# generate more training samples with text augmentation\nUSE_AUGMENTED_TEXT = False\n# set hyperparameters; only has an impact if fine-tuning RoBERTa\nBATCH_SIZE = 14\nWARM_UP = 411\nLEARNING_RATE = 1.7360873300355438e-05\nEPOCHS = 4\nTOKENIZER_MAX_LENGTH = 280\n# dropout only has an effect if using doc_embedding as a feature_type\nDROPOUT = .3\n# this splits the train data into a train and validation set\nSPLIT_TRAIN_DATA = False\n# set to true to train a new model, else use the fine-tuned version\nTRAIN_FROM_SOURCE = True\n# use saved features from extraction method, else extract anew\nUSE_PROCESSED_FEATURES = False\n# choose feature type between 'logit' and 'doc_embedding'\nFEATURE_TYPE = 'logit'\n# do break testing, cutoff each epoch at 2 iterations to debug things\nDO_BREAK_TESTING = False\n# use textstats as a feature in NN training\n# FIXME: due to a bug, this must be left ON for all iterations but is not actually used in the NN \nUSE_TEXTSTAT = True\n# specifiy loss type in NN, alteratives include \"mse_loss\" or \"smooth_l1_loss\"\nLOSS_FUNCTION_TYPE = 'rmse_loss'\n# specify whether to drop hidden states when running the ML regressor\nDROP_HIDDEN = False\n# import our custom utility scripts\nimport bert_utils, kaggle_config, my_bert_tuner, nn_utils, roberta_tuner\nfrom kaggle_config import (WORKFLOW_ROOT, DATA_PATH, CACHE_PATH, FIG_PATH, \n                           MODEL_PATH, ANALYSIS_PATH, KAGGLE_INPUT)\n\n# import the scripts from offline installer\nimport progressbar, textstat\n\nimport os\nfrom os.path import basename\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nimport seaborn as sns\nimport torch\nimport copy\n\nfrom transformers import (RobertaForSequenceClassification, RobertaTokenizer)\n\n# run the configuration\nINPUTS, DEVICE = kaggle_config.run()\nKAGGLE_TRAIN_PATH = kaggle_config.get_train_path(INPUTS)\nKAGGLE_TEST_PATH = kaggle_config.get_test_path(INPUTS)\n\n# set a seed value for consistent experimentation; optional, else leave as None\nSEED_VAL = 42\n\nif SEED_VAL:\n    import random\n    random.seed(SEED_VAL)\n    np.random.seed(SEED_VAL)\n    torch.manual_seed(SEED_VAL)\n    torch.cuda.manual_seed_all(SEED_VAL)\n    \n# set the path to an offline tokenizer\nPRETTRAINED_ROBERTA_BASE_TOKENIZER_PATH = \"\/kaggle\/input\/tokenizer-roberta\"\n# set the path to an offline model (pre-trained from huggingface)\nPRETTRAINED_ROBERTA_BASE_MODEL_PATH = \"\/kaggle\/input\/pre-trained-roberta-base\"\n# set path to a fine-tuned model (this model got .53 on its own)\nTUNED_ROBERTA_BASE_MODEL_PATH_LOGIT = \"\/kaggle\/input\/my-tuned-roberta-base\"\n# set path to a similar logit model trained on rmse (this gets closer to .51 on its own)\nTUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_RMSE = \"\/kaggle\/input\/crp-tuned-roberta-logit-rmse-2epoch\"\n# from version 54\/55: logit with 4 epochs with batch size 16\nTUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_RMSE_4 = \"\/kaggle\/input\/crp-tuned-roberta-logit-rmse-4epoch\"\n# from version 84\/85: logit with 4 epochs with batch size 16\nTUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_RMSE_TEXTAUG_2 = \"\/kaggle\/input\/crp-tuned-roberta-logit-rmse-2epoch-ta\"\n# from version 99: 3 epochs with text aug\nTUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_RMSE_TEXTAUG_3 = \"\/kaggle\/input\/crp-tuned-roberta-logit-rmse-3epoch-ta\"\n\n# a model trained to overfit on train data with logit as the feature_type, not so good\n# TUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_20EPOCH = \"\/kaggle\/input\/crp-tuned-roberta-logit-overfit-20\"\nTUNED_ROBERTA_BASE_MODEL_PATH_EMBED_9EPOCH = \"\/kaggle\/input\/crp-tuned-roberta-embed-rmse-9\"\n\n# the actual pretrained tokenizer from hugging face for use offline; a base model\nPRETRAINED_ROBERTA_BASE_TOKENIZER = RobertaTokenizer.from_pretrained(PRETTRAINED_ROBERTA_BASE_TOKENIZER_PATH)\n# an actual tuned model based on the logit as a predictor; wrapped inside our model class\nPRETRAINED_ROBERTA_BASE_MODEL_LOGIT_REGRESSSION = roberta_tuner.RobertaLogitRegressor(pre_trained_path=PRETTRAINED_ROBERTA_BASE_MODEL_PATH\n                                                                                     , device=DEVICE)\n\n# an actual tuned model based on training the embeddings to the target value\nPRETRAINED_ROBERTA_BASE_MODEL_EMBED_REGRESSION = roberta_tuner.RobertaEmbeddingRegressor(pre_trained_path=PRETTRAINED_ROBERTA_BASE_MODEL_PATH\n                                                                                        , dropout_rate=DROPOUT\n                                                                                        , device=DEVICE)\n# this model achieves .538 on its own, trained on 1 epoch\nTUNED_ROBERTA_BASE_MODEL_LOGIT_1 = roberta_tuner.RobertaLogitRegressor(pre_trained_path=TUNED_ROBERTA_BASE_MODEL_PATH_LOGIT\n                                                                      , device=DEVICE)\n# this is similar but trained on RSME loss, trained on 2 epoch\nTUNED_ROBERTA_BASE_MODEL_LOGIT_2 = roberta_tuner.RobertaLogitRegressor(pre_trained_path=TUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_RMSE\n                                                                      , device=DEVICE)\n\n# this is the best performing model with .497 after LGBM \nTUNED_ROBERTA_BASE_MODEL_LOGIT_4 = roberta_tuner.RobertaLogitRegressor(pre_trained_path=TUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_RMSE_4\n                                                                      , device=DEVICE)\n# this made with text augmentation\nTUNED_ROBERTA_BASE_MODEL_LOGIT_TA1 = roberta_tuner.RobertaLogitRegressor(pre_trained_path=TUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_RMSE_TEXTAUG_2\n                                                                      , device=DEVICE)\n# this made with text augmentation\nTUNED_ROBERTA_BASE_MODEL_LOGIT_TA2 = roberta_tuner.RobertaLogitRegressor(pre_trained_path=TUNED_ROBERTA_BASE_MODEL_PATH_LOGIT_RMSE_TEXTAUG_3\n                                                                      , device=DEVICE)\n\n# retraind with rmse loss at 9 epochs\nTUNED_ROBERTA_BASE_MODEL_EMBEDDING = roberta_tuner.RobertaLogitRegressor(pre_trained_path=TUNED_ROBERTA_BASE_MODEL_PATH_EMBED_9EPOCH\n                                                                        , device=DEVICE)\n\n# set a dtype for the ID column\nDTYPES = {'id': str}\n\n# from a prior run, load a saved pickle file of hidden states and other features about the train data\nPROCESSED_FROM_LOGIT_1 = os.path.join(KAGGLE_INPUT, 'crp-extracted-train-features', 'train_data_with_extracted_features.bz2')\nPROCESSED_FROM_LOGIT_2 = os.path.join(KAGGLE_INPUT, 'crp-extracted-train-logit-rmse-2epoch', 'train_loader_with_extracted_features.bz2')\n# from version 54\/55: 4 epochs with batch size 16 -> best performer so far\nPROCESSED_FROM_LOGIT_4 = os.path.join(KAGGLE_INPUT, 'crp-extracted-train-logit-rmse-4epoch', 'train_loader_with_extracted_features.bz2')\n# from version 84\/85: text augmentation with 2 epochs\nPROCESSED_FROM_LOGIT_TA1 = os.path.join(KAGGLE_INPUT, 'crp-extracted-train-logit-rmse-2epoch-ta', 'train_loader_with_extracted_features.bz2')\n# from version 99 text aug 3 epochs\nPROCESSED_FROM_LOGIT_TA2 = os.path.join(KAGGLE_INPUT, 'crp-extracted-train-logit-rmse-3epoch-ta', 'train_loader_with_extracted_features.bz2')\nPROCESSED_FROM_MIXED_1 = os.path.join(KAGGLE_INPUT, 'crp-extracted-train-mixed-1', 'train_loader_with_extracted_features.bz2')","a9bc7443":"#### HERE! THIS IS THE BOTTOM LINE!!!! set which model to use and the processed data to use in this script\n# use this as a switch to use in this script\nTUNED_MODEL_TO_USE = TUNED_ROBERTA_BASE_MODEL_LOGIT_4\n# set which processed data should be used in the script, not used if training from source\n# currently skipping this one as it was created with size 512\nPROCESSED_FEATURE_DATA = PROCESSED_FROM_LOGIT_4\n# Optional, set a model specifically to extract word embeddings\n# we tested extracting the embeddings from the model trained specifically for embeddings\nEMBED_MODEL_TO_USE = None","2a36d2d9":"# borrow the concept of image augmentation to create more training samples with text\n\ndef roberta_decoder(df, tokenizer, token_col='input_ids'):\n    print(\"\\n=== Running RoBERTa decoder\\n\")\n    df['excerpt'] = df.apply(lambda x: tokenizer.decode(x[token_col]), axis=1) \n    return df\n\ndef augment_text(df, target_col='target', shuffle=True):\n    # assume that we can create new samples based on standard error\n    if target_col is not None:\n        df['target_high'] = df[target_col] + (df['standard_error'] \/ 2)\n        df['target_low'] = df[target_col] - (df['standard_error'] \/ 2)\n    \n#     # first third with high\n#     aug10 = df.copy(deep=True)\n#     aug10['target'] = aug10['target_high']\n#     aug10['excerpt'] = aug10.apply(lambda x: x['excerpt'].split(), axis=1)\n#     aug10['excerpt'] = aug10.apply(lambda x: x['excerpt'][: len(x['excerpt']) \/\/ 3], axis=1)\n#     aug10['excerpt'] = aug10.apply(lambda x: \" \".join(x['excerpt']), axis=1)\n    \n    # first half with current\n#     aug11 = df.copy(deep=True)\n#     if target_col is not None:\n#         aug11[target_col] = aug11[target_col]\n#     aug11['excerpt'] = aug11.apply(lambda x: x['excerpt'].split(), axis=1)\n#     aug11['excerpt'] = aug11.apply(lambda x: x['excerpt'][: len(x['excerpt']) \/\/ 2], axis=1)\n#     aug11['excerpt'] = aug11.apply(lambda x: \" \".join(x['excerpt']), axis=1)\n    \n#     # last third with high\n#     aug12 = df.copy(deep=True)\n#     aug12['target'] = aug12['target_high']\n#     aug12['excerpt'] = aug12.apply(lambda x: x['excerpt'].split(), axis=1)\n#     aug12['excerpt'] = aug12.apply(lambda x: x['excerpt'][-len(x['excerpt']) \/\/ 3: ], axis=1)\n#     aug12['excerpt'] = aug12.apply(lambda x: \" \".join(x['excerpt']), axis=1)\n    \n#     # last third with low\n#     aug13 = df.copy(deep=True)\n#     aug13['target'] = aug13['target_low']\n#     aug13['excerpt'] = aug13.apply(lambda x: x['excerpt'].split(), axis=1)\n#     aug13['excerpt'] = aug13.apply(lambda x: x['excerpt'][-len(x['excerpt']) \/\/ 3:], axis=1)\n#     aug13['excerpt'] = aug13.apply(lambda x: \" \".join(x['excerpt']), axis=1)\n    \n    # last half with current\n    aug14 = df.copy(deep=True)\n    if target_col is not None:\n        aug14[target_col] = aug14[target_col]\n    aug14['excerpt'] = aug14.apply(lambda x: x['excerpt'].split(), axis=1)\n    aug14['excerpt'] = aug14.apply(lambda x: x['excerpt'][-len(x['excerpt']) \/\/ 2:], axis=1)\n    aug14['excerpt'] = aug14.apply(lambda x: \" \".join(x['excerpt']), axis=1)\n    \n#     # middle third with high\n#     aug20 = df.copy(deep=True)\n#     aug20['target'] = aug20['target_high']\n#     aug20['excerpt'] = aug20.apply(lambda x: x['excerpt'].split(), axis=1)\n#     aug20['excerpt'] = aug20.apply(lambda x: x['excerpt'][len(x['excerpt']) \/\/ 3: len(x['excerpt']) \/\/ 3 + len(x['excerpt']) \/\/ 3], axis=1)\n#     aug20['excerpt'] = aug20.apply(lambda x: \" \".join(x['excerpt']), axis=1)\n    \n#     # middle third with low\n#     aug21 = df.copy(deep=True)\n#     aug21['target'] = aug21['target_low']\n#     aug21['excerpt'] = aug21.apply(lambda x: x['excerpt'].split(), axis=1)\n#     aug21['excerpt'] = aug21.apply(lambda x: x['excerpt'][len(x['excerpt']) \/\/ 3: len(x['excerpt']) \/\/ 3 + len(x['excerpt']) \/\/ 3], axis=1)\n#     aug21['excerpt'] = aug21.apply(lambda x: \" \".join(x['excerpt']), axis=1)\n    \n    # middle third with current\n#     aug22 = df.copy(deep=True)\n#     aug22['target'] = aug22['target']\n#     aug22['excerpt'] = aug22.apply(lambda x: x['excerpt'].split(), axis=1)\n#     aug22['excerpt'] = aug22.apply(lambda x: x['excerpt'][len(x['excerpt']) \/\/ 3: len(x['excerpt']) \/\/ 3 + len(x['excerpt']) \/\/ 3], axis=1)\n#     aug22['excerpt'] = aug22.apply(lambda x: \" \".join(x['excerpt']), axis=1)\n    \n    df = pd.concat([#aug10\n#                     aug11\n#                     , aug12\n#                     , aug13\n                    aug14\n#                     , aug20\n#                     , aug21\n#                     , aug22\n                    , df\n                   ])\n    if shuffle:\n        df = df.sample(frac=1).reset_index(drop=True)\n        \n    if target_col is not None:\n        df.drop(columns=['target_high', 'target_low', 'standard_error'], inplace=True)\n\n    return df\n\nif USE_AUGMENTED_TEXT:\n#     temp = pd.read_csv(KAGGLE_TRAIN_PATH)\n#     KAGGLE_TRAIN_DF = augment_text(temp)\n    KAGGLE_TRAIN_DF = None\n    temp = pd.read_csv(KAGGLE_TEST_PATH)\n    KAGGLE_TEST_DF = augment_text(temp, target_col=None, shuffle=False)\nelse:\n    KAGGLE_TRAIN_DF = None\n    KAGGLE_TEST_DF = None","e92c285f":"# call utility script to return a dictionary of dataloader objects\nKAGGLE_DATA = nn_utils.get_reading_level_data(tokenizer=PRETRAINED_ROBERTA_BASE_TOKENIZER\n                                             , train_path=KAGGLE_TRAIN_PATH if KAGGLE_TRAIN_DF is None else KAGGLE_TRAIN_DF\n                                             , test_path=KAGGLE_TEST_PATH if KAGGLE_TEST_DF is None else KAGGLE_TEST_DF\n                                             , batch_size=BATCH_SIZE\n                                             , shuffle_dataloader=True\n                                             , split_train_data=SPLIT_TRAIN_DATA\n                                             , max_length=TOKENIZER_MAX_LENGTH \n                                              # there is currently a logic bug which requires textstat to be passed\n                                              # to be fixed soon\n                                             , textstat=textstat if USE_TEXTSTAT else None\n                                             , dtypes=DTYPES \n                                             )\n\nfor k, v in KAGGLE_DATA.items():\n    if 'num_labels' not in k:\n        print(f\"Loaded {k} with DataLoader Object of len({len(v)})\") if v is not None else print(\" \")\n    else:\n        print(f\"There are a total of ({v}) labels in the data set.\") if v is not None else print(\" \")","9e6d34ed":"# this will fine-tune a roberta model from pre-trained state or return the fine-tuned state\nif TRAIN_FROM_SOURCE:\n    # train a new model\n    model = PRETRAINED_ROBERTA_BASE_MODEL_LOGIT_REGRESSSION if FEATURE_TYPE == \"logit\" else PRETRAINED_ROBERTA_BASE_MODEL_EMBED_REGRESSION\n    print(f\"\\n=== Learning a new model from source model: {model.config.architectures}.\\n\")\n    tuned_bert_model = my_bert_tuner.run(model=model\n                                         , feature_type=FEATURE_TYPE\n                                         , data=KAGGLE_DATA\n                                         , device=DEVICE\n                                         , do_break_testing=DO_BREAK_TESTING\n                                         , batch_size=BATCH_SIZE\n                                         , learning_rate=LEARNING_RATE\n                                         , dropout_prob=DROPOUT\n                                         , warmup_steps=WARM_UP\n                                         , epochs=EPOCHS\n                                         , progressbar=progressbar\n                                         , loss_function_type=LOSS_FUNCTION_TYPE)\n    print(tuned_bert_model.config)\nelse:\n    # use the fine-tuned model instead\n    print(\"\\n=== Reading a fine-tuned model from disk.\\n\")\n    tuned_bert_model = TUNED_MODEL_TO_USE\n    print(tuned_bert_model.config)","b9f4ebe3":"# a helper function to create a categorical map for a given column\ndef make_text_standard_map(df, source_col='text_standard', cat_col='text_standard_category'):\n    cat_map = df[[source_col, cat_col]].copy(deep=True)\n    cat_map = cat_map.drop_duplicates()\n    cat_map = cat_map.set_index(source_col)\n    cat_map = cat_map.to_dict()[cat_col]\n    return cat_map\n\n\ndef apply_text_standard_map(df, map_dict, source_col='text_standard', cat_col='text_standard_category'):\n    df[cat_col] = df[source_col].map(map_dict)\n    return df\n    \n    \ndef add_textstat_features(df):\n    # adding the text standard seems to boost the accuracy score a bit\n    df['text_standard'] = df['excerpt'].apply(lambda x: textstat.text_standard(x))\n    df['text_standard_category'] = df['text_standard'].astype('category').cat.codes\n\n    ### You can add\/remove any feature below and it will be used in training and test\n    df['coleman_liau_index'] = df['excerpt'].apply(lambda x: textstat.coleman_liau_index(x))\n    df['flesch_reading_ease'] = df['excerpt'].apply(lambda x: textstat.flesch_reading_ease(x))\n    df['smog_index'] = df['excerpt'].apply(lambda x: textstat.smog_index(x))\n    df['gunning_fog'] = df['excerpt'].apply(lambda x: textstat.gunning_fog(x))\n    df['flesch_kincaid_grade'] = df['excerpt'].apply(lambda x: textstat.flesch_kincaid_grade(x))\n    df['automated_readability_index'] = df['excerpt'].apply(lambda x: textstat.automated_readability_index(x))\n    df['dale_chall_readability_score'] = df['excerpt'].apply(lambda x: textstat.dale_chall_readability_score(x))\n    df['linsear_write_formula'] = df['excerpt'].apply(lambda x: textstat.linsear_write_formula(x))\n    ###\n    \n    return df\n\n\ndef process_hidden_states(df, drop_hidden_states=False):\n    # for convenience, moving hidden states to the far right of the df\n    if drop_hidden_states:\n        df.drop(columns=['hidden_states'], inplace=True)\n        return df\n    \n    elif \"hidden_states\" in df.columns:\n        df['hidden_state'] = df['hidden_states']\n        df.drop(columns=['hidden_states'], inplace=True)\n\n        temp = df['hidden_state'].apply(pd.Series)\n        temp = temp.rename(columns = lambda x: 'hidden_state_' + str(x))\n        df = pd.concat([df, temp], axis=1)\n        df.drop(columns=['hidden_state'], inplace=True)\n\n        return df\n    else:\n        print(\"hidden_states not found in dataframe, skipping process_hidden_states\")\n        return df","57001773":"# generating procesed features requires GPU so, we save the results of this step to disk\nif USE_PROCESSED_FEATURES and os.path.isfile(PROCESSED_FEATURE_DATA):\n    processed_train_data = pd.read_pickle(PROCESSED_FEATURE_DATA)\n    print(\"\\n=== Read processed data from disk\\n\")\n    \nelse:\n    # to generate a new processed file, do this\n    processed_train_data = bert_utils.extract_features(model=tuned_bert_model\n                                                      , data=KAGGLE_DATA\n                                                      , device=DEVICE\n                                                      , data_key=\"train_loader\"\n                                                      , progressbar=progressbar\n                                                      , embedding_model=EMBED_MODEL_TO_USE\n                                                      , tokenizer=PRETRAINED_ROBERTA_BASE_TOKENIZER)\n        \n# if not USE_AUGMENTED_TEXT:\n#     commonlit_train_data = pd.read_csv(KAGGLE_TRAIN_PATH)\n#     processed_train_data = pd.merge(left=processed_train_data\n#                                    , right=commonlit_train_data[['id', 'excerpt']]\n#                                    , how='left'\n#                                    , left_on='id'\n#                                    , right_on='id')\n\n# add additional features to the data\nprocessed_train_data = add_textstat_features(processed_train_data)\n\n# reorder and set up data so that we don't have a column of vectors (hidden states)\nprocessed_train_data = process_hidden_states(processed_train_data, drop_hidden_states=DROP_HIDDEN)\n\n# save a mapping of categorical data; apply this to the test set later\ntext_standard_map = make_text_standard_map(processed_train_data)\n\n# print(f\"\\nTrain Data | # Records: {len(processed_train_data)}\\n\")\nprint(\"\\n=== Processed Train Data Sample\\n\")\nprint(processed_train_data.head())","1b650ba6":"print(\"\\n=== Returning model predictions on logit only.\\n\")\nlogit_predictions = bert_utils.predict_from_logit(model=tuned_bert_model\n                                            , data=KAGGLE_DATA\n                                            , device=DEVICE\n                                            , data_key=\"test_loader\"\n                                            , progressbar=progressbar)\n\n# this can be used as a submission on its own which turns out to be pretty good\n# logit_predictions.to_csv(\"submission.csv\", index=False)","c3a3a421":"import sklearn\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm as lgm\n\nfrom sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error\nimport math\n\nx_baseline = processed_train_data['bert_logit']\nx_baseline = x_baseline.to_list()\ny_target = processed_train_data[\"target\"].to_list()\n\nprint(\"=\" * 40)\nprint(\"Scores with Best Performing Logits from RoBERTa Regression\")\nprint(\"R2 Score: \", r2_score(x_baseline, y_target))\nprint(\"RMSE Score: \", math.sqrt(mean_squared_error(y_target, x_baseline)))\nprint(\"This is only an approximation to the public score which tends to be a few points higher than our RMSE calculation.\")","974df431":"# evaluate correlation between features and targets\ncols2evaluate = ['bert_logit'\n               , 'difficult_words_ratio'\n               , 'syllable_ratio'\n               , 'smog_index'\n               , 'text_standard_category'\n               , 'coleman_liau_index'\n               , 'flesch_reading_ease'\n               , 'flesch_kincaid_grade'\n               , 'gunning_fog'\n               , 'automated_readability_index'\n               , 'dale_chall_readability_score'\n               , 'linsear_write_formula' \n               , 'target']\n\n# https:\/\/datascience.stackexchange.com\/questions\/39137\/how-can-i-check-the-correlation-between-features-and-target-variable\ncorr_tests = processed_train_data[cols2evaluate].copy(deep=True)\n    \nprint(corr_tests[corr_tests.columns].corr()['target'][:])","76482cae":"# reference: https:\/\/www.kaggle.com\/lasmith\/house-price-regression-with-lightgbm\n\n# organize data for easy processing in regressors\nlgm_data = processed_train_data.copy(deep=True)\nlgm_data = lgm_data.drop(columns=['excerpt', 'text_standard', 'id'])\n\n# drop the columns that do not seem to add value\n# after adding easy flag \ncols2remove = ['syllable_ratio'\n               , 'coleman_liau_index'\n               , 'flesch_kincaid_grade'\n               , 'gunning_fog'\n               , 'automated_readability_index'\n               , 'linsear_write_formula'\n              ]\n\nlgm_data = lgm_data.drop(columns=cols2remove)\n\nx_features = lgm_data.loc[:, lgm_data.columns != 'target']\ny_target = lgm_data[['target']]\n\n# split the train set into train and validation\nX_train, X_test, y_train, y_test = train_test_split(x_features, y_target, test_size=0.2, random_state=SEED_VAL)\n\nfeature_cols = list(X_train.columns)\nprint(\"The columns used in training:\\n\", feature_cols[:10], \"... \\n\")\n# in the lgm.train paradigm, use the lgm.Dataset constructor\nlgm_train_set = lgm.Dataset(data=X_train, categorical_feature=['text_standard_category'], label=y_train)\nlgm_valid_set = lgm.Dataset(data=X_test, categorical_feature=['text_standard_category'], label=y_test)\n\n# https:\/\/testlightgbm.readthedocs.io\/en\/latest\/Parameters.html\n# https:\/\/towardsdatascience.com\/hyperparameter-tuning-to-reduce-overfitting-lightgbm-5eb81a0b464e\nparams = {\n'boosting_type': 'gbdt',\n'objective': 'regression',\n'metric': 'rmse',\n    # baseline at 100 for min_data_in_leaf\n'min_data_in_leaf': 100,\n    # baseline at .8\n'feature_fraction': .8,\n    # baseline at .8\n'bagging_fraction': 0.8,\n'bagging_freq': 10,\n'max_depth': 16,\n'num_leaves': 32,\n'learning_rate': 0.005,\n    # baseline at max_bin 256\n\"max_bin\": 256,\n\"n_estimators\": 10000,\n}\n\ngbm = lgm.train(params,\n                lgm_train_set,\n                valid_sets=[lgm_train_set, lgm_valid_set],\n                early_stopping_rounds=1000,\n                )","13b1ae6a":"# see the RMSE on all training and target data\nx_features = lgm_data.loc[:, lgm_data.columns != 'target']\ny_target = lgm_data[['target']]\n\n# ensure we are using the same cols \nx_features = x_features[feature_cols]\n\n# run predict on the trained model tree\/weights\ny_pred = gbm.predict(x_features)\ny_target = y_target['target'].to_numpy()\n\nprint(\"=\" * 40)\nprint(\"Scores On Train\/Valid Set with LightGBM after Training\")\nprint(\"R2 Score: \", r2_score(y_target, y_pred))\nprint(\"RMSE Score: \", math.sqrt(mean_squared_error(y_target, y_pred)))","dbd21c11":"# run test data through a similar data pipeline as train data\n# extract features with utility scripts\nprocessed_test_data = bert_utils.extract_features(model=tuned_bert_model\n                                                      , data=KAGGLE_DATA\n                                                      , device=DEVICE\n                                                      , data_key=\"test_loader\"\n                                                      , progressbar=progressbar\n                                                      , embedding_model=EMBED_MODEL_TO_USE\n                                                      , tokenizer=PRETRAINED_ROBERTA_BASE_TOKENIZER\n                                                     )\n\n# add additional features to the data\nprocessed_test_data = add_textstat_features(processed_test_data)\n# drop any columns from the data that we no longer want\nproccessed_test_data = processed_test_data.drop(columns=cols2remove)\n\n# reorder and set up data so that we don't have a column of vectors (hidden states)\nprocessed_test_data = process_hidden_states(processed_test_data, drop_hidden_states=DROP_HIDDEN)\n\n# apply the text standard score mapping dict from earlier\nprocessed_test_data = apply_text_standard_map(processed_test_data, text_standard_map)\n\n\n# get the document ids\ndocument_ids = processed_test_data['id'].to_list()\n# set up the test data\nlgm_test = processed_test_data.drop(columns=['excerpt', 'text_standard', 'id'])\n\n\n# ensure we are using the same colums as used in training\nlgm_test = lgm_test[feature_cols]\npredictions = gbm.predict(lgm_test)\nsubmission = list(zip(document_ids, predictions))\n\nsubmission = pd.DataFrame(submission, columns=['id', 'target'])\nsubmission = submission.groupby('id').mean().reset_index()\nprint(\"=\" * 40)\nprint(\"=\" * 40)\nprint(\"Final ML Submission with LGBMRegressor\\n\")\nprint(submission)\nsubmission.to_csv('submission.csv', index=False)","0be9da84":"### In computer vision, there is a concept known as image augmentation wherein we can take a very few number of images and create many times more samples. The idea is that we can take a picture, then stretch, skew, and rotate the original picture with the knowledge that all the new copies still represent the original picture. For a computer, this allows an algorithm to identify whether a slight different version of the picture is, in fact, the same thing. \n\n* With this project, we only get about 2,800 samples to work with. However, each sample has a target and a standard error. As a result, we know that each target could be plus or minus some amount. In addition, with RoBERTa tokenizer, we are only able to process a limited number of text from each excerpt. \n\n* Can we combine the concept of image augmentation with text? We can try! Below, we are testing various combinations of copies of the train data by making a copy of the first, middle, and last third of each excerpt. Then, we assign a new target to each of the copies in the amount of plus or minus half the standard error. The central idea here is to provide lots more examples from a known target baseline. The hope is to train the network on more samples and produce a better model that can generalize over new data. Unfortunately, so far, this seems to bring down the average score, so we are not currently using this feature in the current high score.","e7455f01":"### Set up all notebook variables and data to control most of the experiment.","5f801038":"### Finally, apply a similar feature extraction method to process the test dataset. These steps should also apply to whatever held out data our model is evaluated against. ","ed3e5f27":"## This is a notebook by The Bernstein Bears, a team set up for the CommonLit competition, for more see the [leaderboard](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/leaderboard).\n\n## Authors: [Justin Chae](https:\/\/www.kaggle.com\/justinchae) and [Simon Benigeri](https:\/\/www.kaggle.com\/simonbenigeri)","1b6b66f3":"### For benchmarking, see how the regressor does on the entire train set","dc3ac966":"### Here, we just return a saved model state from file instead of training it again. If needed, set the variables in the cells above and train from scratch.","300debc5":"### For text processing, some helper functions to extract and apply various features to the data.","94b012c6":"### Here, return a pre-processed file that contains the feature space for the train data.","36e2a775":"### A diagram to show a big picture of how we set up the architecture between RoBERTA and LGBMRegressor. The diagram only contains two of several textstat features but the idea is that we can concat a feature vector for each sample with as many new features as we think are necessary for good prediction. Also, we have changed the linear transformations from a default of 128 to 64.","bec4ecb7":"### In a previous iteration of this notebook, when we trained on the doc_embedding as a feature_type, we concatenated textstat features such as flesh-kincaid score to the embedding space before producing a final linear output for regression. However, we found that this did not yeild ideal results. Furthermore, we wanted a way to train only the embedding weights to the target value and then use the RoBERTa model in inference mode to extract features for a machine learning style regressor","f56dcbd3":"### Run the LGBM Regressor Train Function\n\nWe follow a few tutorials on hyper parameter tuning for the Light GBM regressor. We chose this method over others such as SVM or RF after running some comparative tests in another notebook. Also, LGBMRegressor's ability to handle categorical features was pretty interesting. The bottom line here is that we generally want to build a strong regressor during training without overfitting.\n\n### Some notes on LGBMRegressor Tuning\n\n* Smaller maxbin: reduces train accuracy but has potential to increase generalization\n* Bigger min_data_in_leaf: has potential to reduce overfitting","baa70d63":"### Based on all the models and data options set in the prior cell, set which of the tuned models or processed data files to use in the script. \n\nExample: We used the tuned model to extract logits and embeddings for the train data and the wrote that file to disk. To avoid overusing the GPU, we can just use this pre-processed file to train a regressor which does not necessarily need a GPU. ","d2e94304":"### Some idea of the models we trained for a few or many epochs and with different types of features.\n\nAlthough we tried a variety of approaches, it appears that, if using this type of approach, training a BERT model in regression mode by taking the loss of the output.logit and the target value seems to move the weights into an ideal direction. Said in another way, with this method, we can interpret the output logit, for which there is just one logit per sample (as long as num_labels=1), as the reading difficulty level. The logit as a feature_type worked better as opposed to methods where we concat various text features to the emebdding space or purposefuly take the training loss to near zero (overfit).","dc88c975":"### In the preceeding output of LGBM train, we want to see the RMSE for train and valid to improve much in the same way that we would expect from a learning a model of data in a neural network. Generally, it seems that we start to get nice results in the public score when our RMSE on the given train\/valid data falls below .4. However, based on results from public score submissions, we start to overfit when the RMSE on train drops towards .2, R2 score approaches .98, or when the train RMSE starts to diverg significantly from the validation RMSE. ","ff4eae65":"# Reading Level Difficulty Prediction with RoBERTa and LightGBM.","0bcc46be":"![figure3.png](attachment:f73b2d45-13dd-4060-81dc-531491378abd.png)","79afe3c1":"![Picture1.png](attachment:b8985608-a144-46e6-a6eb-3c9125cccf0c.png)","98b8621d":"## Project Overview and Background\n\n### In this project, we fine-tune a pre-trained [RoBERTa model for sequence classifcation](https:\/\/huggingface.co\/transformers\/model_doc\/roberta.html#robertaforsequenceclassification) in combination with LightGBM to predict reading level difficulty. To fine-tune RoBERTa, we use the RoBERTa regression output, i.e., the logit and train on the loss between the logit and the target value. Using the RoBERTa logit as a sole predictor, we achieve an initial public score of approximately .538. As an alternative to using the logit, we summed the last four hidden states and applied several layers of linear transformation on the document embeddings and use the resulting value to compute the loss. \n\n* Although the document embedding method seems to work quite well for classification tasks, in this case, it appears the logit produces the most useful feature to predict reading difficulty.\n\n* To improve the initial score based on logit alone, we use RoBERTa as a feature extractor by using both the logit and document embeddings from the model. We initially set max sequence length to 280 based on a statistical analysis of document length and set padding to max length. Later, we expand the max length to 512 to see if a regressor can make use of more vectors for each excerpt. \n\n* However, it turns out that longer hidden states in the LGBMRegressor adds too much noise. After some test submissions, the long hidden states (512) reduce the RMSE by about .002 while no hidden states reduce the score by .001. Of the tested options, 280 seems to be optimal choice but other factors like LGBMRegressor tuning might be a determining factor.\n\n### In addition to RoBERTa features (logit and embeddings), there is something to be learned from extracting statistics about the text. As a result, we leverage the [textstat](https:\/\/pypi.org\/project\/textstat\/) library to engineer other features such as flesch-kinkaid or reading grade scores. After some experimentation, we found that evaluated metrics such as number of difficult words per document improve a regression score as opposed to logit alone. Although, it is worth noting that the output logit is a fairly strong predictor on its own. In any case, we also use a categorical reading score, i.e. '11th grade level' in the [LGBMRegressor](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMRegressor.html) to boost the score further. The use of a categorical feature in LGBMRegressor seems to boost the public score to about .505.\n\n* The result of the feature extraction methods allows us to create a feature vector that can be used in either a neural network to fine-tune RoBERTa or as an input to a machine learning regressor such as one provided by scikit-learn or in our case, the LGBMRegressor. We evaluated a variety of combinations of features such as logit with embeddings or logit with syllable count, etc, and found that a hybrid feature vector comprised of categorical reading level score, logit, difficult words ratio, extracted embedding, flesch ease score, and a few others produced optimal results.\n\n* To get an idea of which features would be good predictors, we run a correlation function with [Pandas](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.corr.html) and choose the features that appeared to be strongly correlated, in either direction. One exception is the text standard category feature which comes from the textstat library which does not appear to be correlated but as a categorical feature in LGBMRegressor, it seems to do quite well.\n\n### On of 12 June 2021, with LGBMRegressor and our hybrid feature vector was among the top 49% of scores with a public score of .523. After adjusting the loss function in our RoBERTa tuner (from MSE Loss to RMSE Loss), we were able to produce a logit as a reading difficulty predictor which produced a public score of .505 which was, at the time, among the top 43% of scores. \n\n### Lately, we started training RoBERTa for few more epochs (4 instead of 2) and have increased batch size (from 8 to 16) to smooth out the gradient. As of 18 June 2021, increasing the batch size during RoBERTa fine-tuning gives use a slight boost and produces a public score of .499 which was good enough for the top 44% of entries. After some further tuning of the LGBMRegressor, we gained a fraction of a point to move up to .497 on 19 June 2021.\n\n### Currently, we are experimenting with text augmentation to increase the number of training samples that are available for learning a good model. However, the results are, so far, not producing better accuracy.\n\n\n## Next Steps and Other Things\n\n* The broad-based approach to fine-tune RoBERTa and tack on some encodings about text cohesion works surprisingly well to model the competition's reading difficulty measures. \n* However, this broad approach misses some finer points about unpacking the details that make a text passage difficult or not. For instance, the use of ngrams, topic modeling, and statistical measure such as TF-IDF might encode something that is more useful or perhaps just orthogonal to the RoBERTa approach. \n* Further, we really haven't pre-processed the text (outside of the RoBERTa pre-trained tokenizer), but there is likely a benefit to evaluating the corpus when considering case, punctuation, and stop words.\n* Lastly, we could stand to apply some automated fine tuning for both our Transformer and Regressors and are looking to use something like Optuna to get that work done.\n\n\n## Summary of Learnings\n\n* During fine-tuning of a pre-trained RoBERTa for sequence classification, we are seeing good results with higher batch sizes and fewer epochs, i.e. batch size at between 14-16 and epochs at between 1-4. From what we are understand, both from our experimenets and from open source research, increasing the batch size seems to mitigate the effects of overfitting. \n\n* It is possible to fine-tune RoBERTa within the context of a regression problem. The key is to set num_labels to 1 and use the output.logit as the y_prediction; then take the loss between y_prediction and the target value. See my Kaggle utility script which presents a wrapper around the RoBERTa model from huggingface and sets up the fine-tuning at [https:\/\/www.kaggle.com\/justinchae\/roberta-tuner](https:\/\/www.kaggle.com\/justinchae\/roberta-tuner). In addition, an implementation of the model during training in another utility script that we created for this competition at [https:\/\/www.kaggle.com\/justinchae\/epoch-utils](https:\/\/www.kaggle.com\/justinchae\/epoch-utils).\n\n* We built some additional utility scripts to handle as much outside of the main notebook as possible. In addition to the aforementioned roberta-tuner and epoch-utils, we also use [https:\/\/www.kaggle.com\/justinchae\/my-bert-tuner](https:\/\/www.kaggle.com\/justinchae\/my-bert-tuner) for orchestration and [https:\/\/www.kaggle.com\/justinchae\/kaggle-config](https:\/\/www.kaggle.com\/justinchae\/kaggle-config) to manage global project setup. Lastly, we have a utility script to manage plotting training loss and collate data for our Torch dataloader [https:\/\/www.kaggle.com\/justinchae\/nn-utils](https:\/\/www.kaggle.com\/justinchae\/nn-utils).\n\n* While it is also possible to leverage RoBERTa hidden states as document embeddings, doing so does not seem to produce optimal results, at least the way we approached it. In what is basically a feed forward network, we take the inputs which are vectors having dimensions of batch size, sequence length, and 786 (the hidden size) and take several layers of linear transformation to produce a vector having a single value for each token in the sequence. Then finally, we take a final linear transformation to produce a single value for each excerpt and compute the loss. As compared to the \"logit as a feature\" method, this \"doc_embedding\" method produces a model with about twice the training loss and performs just as poorly on the held out data. \n\n* We like the LGBMRegressor to build a final model of reading difficutly becuase of its ability to handle categorical data and indifference to scaled inputs. When we use the logit as just another feature of the text in combination with features that capture elements of text cohesion and the like, we get an interesting \"hybrid feature vector\" that has, so far at least, produced our high score of .497. \n\n* For LGBMRegressor, we are still learning tons about how to use it. So far though, we like leveraging the  framework of  the LGBM dataset and LGBM train methods to learn a model as opposed to the scikit-learn method of using LGBM fit and predict which LGBM also supports. Why? The train method seems to allow us to set a varity of hyper parameters while training to an optimial number of iterations. As a result, we spend some time tinkering with depth, number of leaves, and bin size while setting early stopping conditions on iterations to prevent overfitting. \n\n* We initially tuned LGBMRegressor in accordance with a Kaggle blog [https:\/\/www.kaggle.com\/lasmith\/house-price-regression-with-lightgbm](https:\/\/www.kaggle.com\/lasmith\/house-price-regression-with-lightgbm). Currently we are experimenting with ways to boost accuracy during training while reducing tendency to overfit since the ultimate goal is to have a model that generalizes well over unseen data. For example, reducing max_bin size while increasing min_data_in_leaf seem to be good levers to reduce overfitting. In additionn, there seems to be a potential benefit of adjusting the feature fraction and bagging fraction from our default setting of .8 for both.\n\n* It has been helpful to capture an internal benchmark of our models during experimentation for things like R2 Score, AUC, or RMSE against the current public score to avoid submitting too many times. For example, we save a notebook and note what our RMSE is, then we submit this same model and see what the public RMSE is. Although not entirely accurate, we can get a sense of what might work and what might not work. \n\n* Despite improving from .585 to .497 with the methods in this notebook, there seems to be a limit to how good we can get. As a result, we probably have to fundamentally shift how we approach this problem.\n\n* We spent an incredible amount of time learning how to navigate some of the nuances of Kaggle, most especially, figuring out how to use our favorite libraries in Kaggle's offline \"no internet\" mode. After some soul searching we found that Kaggle's guidance here [https:\/\/www.kaggle.com\/samuelepino\/pip-installing-packages-with-no-internet](https:\/\/www.kaggle.com\/samuelepino\/pip-installing-packages-with-no-internet) works the best. The gist is to basically download the tar file of a given library, upload it as a dataset and then add it to your list of inputs. Then, as you can see in the top two cells of this notebook, you can just use some command line arguments to run installation scripts. ","5fa1c1a2":"### Reconsider which features to include in regression\n\nAs seen below, the bert_logit, which we trained in the RoBERTa tuner is a very strong predictor while the textstats have varying strength. We think that creating a hybrid feature vector should help improve the model accuracy and generalize over unseen data. The logit, it seems, carries some valuable information about how RoBERTa has encoded the logit-target value loss and also, we get to leverage a watered down version of the embedding space by extracting the hidden states. Further, with the textstats, we encode information about what experts in the domain use to think about reading complexity.","52416456":"## Works Cited\n[1] @misc{liu2019roberta,title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, \n      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis\n      and Luke Zettlemoyer and Veselin Stoyanov}, year={2019}, eprint={1907.11692}, archivePrefix={arXiv}, primaryClass={cs.CL}}\n\n[2] @inproceedings{wolf-etal-2020-transformers, title = \"Transformers: State-of-the-Art Natural Language Processing\", author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\", booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\", month = oct, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https:\/\/www.aclweb.org\/anthology\/2020.emnlp-demos.6\", pages = \"38--45\" }\n\n[3] @incollection{NEURIPS2019_9015, title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library}, author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith}, booktitle = {Advances in Neural Information Processing Systems 32}, editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\\textquotesingle Alch'{e}-Buc and E. Fox and R. Garnett}, pages = {8024--8035}, year = {2019}, publisher = {Curran Associates, Inc.}, url = {http:\/\/papers.neurips.cc\/paper\/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf} }\n\n[4] Chris McCormick and Nick Ryan. (2019, July 22). BERT Fine-Tuning Tutorial with PyTorch. Retrieved from http:\/\/www.mccormickml.com\n\n[5] @article{neeraj2020bertlayers, title = \"Feature-based Approach with BERT\", author = \"Neeraj, Trishala\", journal = \"trishalaneeraj.github.io\", year = \"2020\", url = \"https:\/\/trishalaneeraj.github.io\/2020-04-04\/feature-based-approach-with-bert\"}\n\n[6] @article{scikit-learn,title={Scikit-learn: Machine Learning in {P}ython},author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.\n         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.\n         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and\n         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n journal={Journal of Machine Learning Research}, volume={12},pages={2825--2830},year={2011}\n}\n\n[7] Smith, Laurence (https:\/\/www.kaggle.com\/lasmith), Kaggle Notebook, retreived on 12 July 2021 from: https:\/\/www.kaggle.com\/lasmith\/house-price-regression-with-lightgbm.\n","a7104d34":"### As stated before, returning the logits as a predicted reading difficulty value can produce decent results as a baseline, around .53 on the public board. Here, we just run it to make sure everything the preceded this cell is working as expected.","cae61082":"### Best Scores so far for LGBM based on our measures:\n\n#### For benchmarking, the following scores actually translate to a relatively worse score on the public board at around .51 RMSE. The caution here is that this is mostly reflecting how well you did on the train set with some of the validation set mixed in. As a result, consider this to only be good for a general measure of relative performance during hyperparameter tuning.\n\n* R2 Score:  ~0.9002950668496916\n* RSME Score:  ~0.3263062926902492","18ea8af3":"### Refine the prediction scheme with additional features in a Light GBM Regressor\n\nHere, we just establish a baseline score based soley on the bert_logit, which we said could be used as a reading difficulty prediction. Any more work we do with a regressor should do better than this to be worth doing.","00cd4a7b":"### Install some necessary dependencies for offline mode","a9abe3fd":"![model_losses.png](attachment:db09588c-efed-4219-957b-d6853d13a082.png)"}}