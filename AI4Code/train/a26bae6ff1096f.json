{"cell_type":{"2f13ea3b":"code","cb02f14d":"code","432ef59a":"code","6a1c372a":"code","d8d007a2":"code","3b9f0527":"code","644c8f6c":"code","3b1436fc":"code","c6f099ab":"code","e2c7001d":"code","b7eb1042":"code","0e1bc480":"code","a3206c95":"code","0d65fb2f":"code","755dc8ef":"code","fed94009":"code","823639d0":"code","3c73748a":"code","31b3b21d":"code","ff238aed":"code","e6f3444d":"code","8e1a5dbd":"code","aaa43ef3":"code","8f1de2ae":"code","0d26a862":"code","00e39775":"code","17d04757":"code","92369abd":"code","e3c621a2":"code","fde2682a":"code","fcc23b86":"code","5482d5b3":"code","273d2ca0":"code","1f616c83":"code","1eabfc4d":"code","9650f782":"code","90f47cda":"code","be67930e":"code","16d812d4":"code","18875739":"code","59d84bc9":"code","f4de1c66":"code","ddb48ab2":"code","fef33549":"code","c07a2fc7":"code","5ffaff77":"code","afbcb54a":"code","4cea2a55":"code","de6474cb":"code","ac1a0768":"code","08169470":"code","f83c784b":"code","839a03af":"code","432d6ade":"code","3fee2009":"code","368694ac":"code","b16cc008":"code","f6f6fefd":"code","1f942424":"code","601e4f01":"code","48df3b6d":"code","c8b5c1fd":"code","46358a06":"code","6c6e948b":"code","1812cca7":"code","3f55ed53":"code","fe713db3":"code","d7e820bc":"code","01617183":"code","19036087":"code","8d843bd0":"code","66aeb98d":"code","e5dc3cae":"code","e097a646":"code","c6032755":"code","c78a0db6":"code","e2151fe7":"code","15d2d4ef":"code","3863e60b":"code","47773fdf":"code","1f04faa6":"code","fff13dc9":"code","fd2e26b6":"code","11f41cb4":"code","9c082a48":"code","b97f2384":"code","6e77d900":"code","a4fbcd3d":"code","990b6c7b":"code","3de3600e":"code","dcb48a10":"markdown","d00679c8":"markdown","5c4973ff":"markdown","7616f68d":"markdown","1c32122f":"markdown","dcc2a7c6":"markdown","b2931d9e":"markdown","c00028d3":"markdown","87910d47":"markdown","b1e8e93e":"markdown","3c438bf0":"markdown","1a881b90":"markdown","86b5942e":"markdown","414ec352":"markdown","5b121310":"markdown","98b8a3dd":"markdown","da299e1a":"markdown","5b259a68":"markdown","b86828c5":"markdown","ccfabd14":"markdown"},"source":{"2f13ea3b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","cb02f14d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline   \n# this last command allows us to visualize plots in this notebook environment","432ef59a":"df1=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')   #save a dataframe of the csv files that were given to us \ndf2=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","6a1c372a":"df1['train']=1  # we add a new column, which will later tell us what observations belong to which set(training\/testing)\ndf2['train']=0","d8d007a2":"# df1 will be our training set, df2 the test set\n\ndf=pd.concat([df1,df2],axis=0,sort=False)\n\n# we concatenate the two in order to get a better understanding of the data we are working with","3b9f0527":"df.head()","644c8f6c":"df.info()","3b1436fc":"df.describe().transpose()","c6f099ab":"df.isnull().sum()\n#this way we check how many null values there are on each column","e2c7001d":"sns.heatmap(df.isnull(),cbar=False,yticklabels=False,cmap='viridis')\n\n#in order to better visualize columns with missing values, instead of calculating percentage\n\n# we shouldn't worry about the missing values on 'Survived' column, as they are the ones we are trying to predict","b7eb1042":"# the code for automatic EDA is the following, but in this project we'll focus on creating plots ourselves\n'''\nimport pandas_profiling \ndisplay(pandas_profiling.ProfileReport(df))\n'''","0e1bc480":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,4))\ndf1['Fare'].plot(kind='hist',bins=70)\n#As we can see, this is not a normal distribution, meaning majority of those who embarked purchased cheaper tickets","a3206c95":"plt.figure(figsize=(12,8))\nsns.boxplot(x='Pclass',y='Fare',hue='Survived',data=df1)\n# As we might have guessed, a higher class ticket means a spike in 'Fare'","0d65fb2f":"sns.distplot(df1['Age'].dropna())","755dc8ef":"sns.countplot(x='Survived',data=df1,hue='Sex',palette='RdBu_r')","fed94009":"sns.countplot(x='Pclass',data=df1,hue='Survived')","823639d0":"df1[df1['Fare']>500][['Name','Survived']]\n#Out of pure curiosity, we checked if most expensive tickets ultimately lead to survival","3c73748a":"sns.countplot(x='Embarked',data=df1,hue='Survived')","31b3b21d":"sns.countplot(x='SibSp',data=df1)","ff238aed":"sns.countplot(x='Parch',data=df1)","e6f3444d":"sns.jointplot(x='Age',y='Fare',data=df,kind='hex',cmap='magma')\n# Meaning there were many young adults on board, who opted for the 3rd class tickets","8e1a5dbd":"# A very useful practice is to gain a better understanding of the data through the means of pivot tables\n\npd.pivot_table(df1,index='Survived',columns='Pclass',values='Fare',aggfunc='count')\n\n# Pivot table are a very powerful weapon of a data scientist, but require a little getting used to :))","aaa43ef3":"sns.heatmap(df.corr())\n# A heatmap is often useful to identify the correlations between various columns( how they influence each other )\n# Lighter the color, the stronger the dependance","8f1de2ae":"# Below, we'll separate numerical columns from categorical ones","0d26a862":"numerical_columns=[col for col in df1.columns if df1.dtypes[col] in ['int64','float64'] and col not in['Survived','train']]\nnumerical_columns","00e39775":"categorical_columns=[col for col in df1.columns if df1.dtypes[col] == 'O']\ncategorical_columns","17d04757":"df[numerical_columns].isnull().sum()","92369abd":"sns.boxplot(data=df1,x='Pclass',y='Age')","e3c621a2":"#We'll imput age where is missing, based on mean age for respective Pclass \n\ndef imput_age(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass==1:\n            return 37\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age\n\ndf['Age']=df[['Age','Pclass']].apply(imput_age,axis=1)\n\n","fde2682a":"# Create a new feature, 'child' if age is lower than 18\n\ndef is_child(x):\n    if x>=18:\n        return 0\n    else: \n        return 1\n\n\n\ndf['child']=df['Age'].apply(is_child)","fcc23b86":"df['Fare']=df['Fare'].fillna(df[df['train']==1]['Fare'].mean())   # Doing it this way beacause of the problems \n# encountered using SimpleImputer ","5482d5b3":"# For some reason, using SimpleImputer resulted in an error which stated the kernel \n# could not perform an action on a slice of the dataframe\n\n'''\ndf1=pd.read_csv('train.csv')\ndf2=pd.read_csv('test.csv')\ndf1['train']=1\ndf2['train']=0\n\n\nfrom sklearn.impute import SimpleImputer\nnumerical_transformer=SimpleImputer(strategy='mean')\ndf1[numerical_columns] = numerical_transformer.fit_transform( df1[numerical_columns] )\ndf2[numerical_columns] = numerical_transformer.transform( df2[numerical_columns] )\ndf=pd.concat([df1,df2],axis=0,sort=False)\n'''\n# In this final version it seemed to work, but at the beginning it definitely did not work:\n\n'''\nfrom sklearn.impute import SimpleImputer\nnumerical_transformer=SimpleImputer(strategy='mean')\ndf[df[train]==1][numerical_columns] = numerical_transformer.fit_transform( df[df[train]==1][numerical_columns] )\ndf[df[train]==0][numerical_columns] = numerical_transformer.transform( df[df[train]==1][numerical_columns] )\ndf=pd.concat([df1,df2],axis=0,sort=False)\n'''","273d2ca0":"df[numerical_columns].isnull().sum() # And we are good to go :))","1f616c83":"categorical_columns","1eabfc4d":"df[categorical_columns].isnull().sum()","9650f782":"df['Embarked']=df['Embarked'].fillna('S')\n#Based on the plot created in the EDA section, 'S' is the most frequent value in the 'Embarked' column","90f47cda":"df['rank']=df['Name'].apply(lambda x: x.split(',')[1].split('.')[0])\n\n# We created a new column, 'rank', which contains the way certain people are adressed","be67930e":"df['rank'].value_counts(ascending=False)","16d812d4":"df[df['rank']==' Capt']['Survived']\n\n# Sadly, the Captain went down with the ship","18875739":"def never_let_go(x):\n    if 'Jack' in x:\n        print(x)\n\ndf['Name'].apply(never_let_go)\n\n# Well, it's not the Jack we were searching for, but it was worth trying :))","59d84bc9":"# Let's turn our attention over to the 'Ticket' column\n\n# Through the listed functions, we are able to extract the letters and later the number of the tickets\n\ndef letter_separator(x):\n    mylist=[letter for letter in x if letter.isalpha() ]\n    if len(mylist):\n        return ''.join(mylist)\n    else:\n        return 'None'\n    \ndef digit_separator(x):\n    if not str(x.split()[-1]).isdigit():\n        return 0\n    else:\n        return int(str(x.split()[-1]))    \n    \ndf['ticket_letters']=df['Ticket'].apply(letter_separator)\ndf['ticket_number']=df['Ticket'].apply(digit_separator)   ","f4de1c66":"df.head()","ddb48ab2":"plt.figure(figsize=(20,4))\nsns.countplot(data=df,x='ticket_letters')","fef33549":"df['ticket_letters'].value_counts(ascending=False)[1:]  #avoid including 'None'","c07a2fc7":"# Finally, the 'Cabin' column\n\ndf['Cabin'].unique()","5ffaff77":"df['cabin_letter']=df['Cabin'].apply(lambda x: str(x)[0]) ","afbcb54a":"pd.pivot_table(df,columns='cabin_letter',index='Survived',values='Ticket',aggfunc='count')\n\n# missing cabin rows are marked by the 'n'","4cea2a55":"# New columns in place, we can afford to drop the former ones\n\ndf.drop(['Name','PassengerId'],axis=1,inplace=True)\ndf.drop('Cabin',axis=1,inplace=True)\ndf.drop('Ticket',axis=1,inplace=True)","de6474cb":"# Our numerical and categorical columns have thus changed:\n\nnumerical_columns=[col for col in df.columns if df.dtypes[col] in ['int64','float64'] and col not in['Survived','train']]\nnumerical_columns","ac1a0768":"categorical_columns=[col for col in df.columns if df.dtypes[col] == 'O']\ncategorical_columns","08169470":"import itertools","f83c784b":"copy_df=df.copy()","839a03af":"interactions=pd.DataFrame(index=copy_df.index)","432d6ade":"for col1, col2 in itertools.combinations(categorical_columns,2):\n    new_col_name='_'.join([col1,col2])\n    new_values=copy_df[col1].map(str)+'-'+copy_df[col2].map(str)\n    interactions[new_col_name]=new_values","3fee2009":"interactions\n\n# We had 5 categorical columns, and we asociated every pair possible\n#      ---> Combinations of 5 taken 2 at a time = (5!)\/[(3!)*(2!)] = 10 new columns","368694ac":"# The only reason we created a copy earlier was because I feared I would cause damage to \n# the dataframe by concatenating something the wrong way :))","b16cc008":"df=pd.concat([interactions,df],axis=1,sort=False)","f6f6fefd":"df.head()","1f942424":"# Of course, the categorical columns have changed once again:\n\ncategorical_columns=[col for col in df.columns if df.dtypes[col] == 'O']\ncategorical_columns\n\n# While numerical ones stayed the same","601e4f01":"df.isnull().sum()\n#Since only the values we want to predict are missing, we can go ahead and encode out categorical values","48df3b6d":"# Basically, we encode categorical features differently based on their associated cardinality\n\n# Cardinality means the number of unique values in a column.\n\n# We will use OneHotEncoder for features with low cardinality, because these ones do not add so many new columns to\n# the dataset, which would make it difficult for the computer to process. ","c8b5c1fd":"# We first separate categorical columns in high cardinality ones vs low cardinality:\n\n#high-cardinality categorical columns\nhccc=[col for col in df.columns if df.dtypes[col] == 'O' and df[col].nunique()>10]\nhccc","46358a06":"#low cardinality\nlccc=[col for col in categorical_columns if col not in hccc]\nlccc","6c6e948b":"trainset=df[df['train']==1]\ntestset=df[df['train']==0]","1812cca7":"# I'll show with you my failures in trying to label encode hccc with the basic LabelEncoder\n\n# The problem I had run into was that the testset contained new values on some columns\n\n# For example, on column: 'Sex_rank' we might not have found the value 'female-the Countess' in the trainset, \n# but rather stumble upon it in the testset, in which case the encoder would not know what to do","3f55ed53":"# Failed Attempt nr1:\n'''\nfrom sklearn import preprocessing\nfor feature in hccc:\n  encoded=preprocessing.LabelEncoder().fit(trainset[feature])\n  df[feature+'_labels']=encoded.transform(df[feature])\n#fit only on dataset to avoid leakage\n#trebuie pr fiecare coloana in parte fit, la OneHot e la general ca pune doar 0 sau 1'''","fe713db3":"# Failed Attempt nr2:\n'''from sklearn import preprocessing\nfor feature in hccc:\n  encoder=preprocessing.LabelEncoder()\n  encoder.fit(trainset[feature])\n  encoded_train_feature=encoder.transform(trainset[feature])\n\n  trainset_feature_map=pd.DataFrame({'train_feature':trainset[feature],'encoded_train_feature':encoded_train_feature})'''","d7e820bc":"# Failed Attempt nr3:\n\n'''\nencoder=preprocessing.LabelEncoder()\nencoder.fit(trainset['rank'])\nencoded_train_feature=encoder.transform(trainset['rank'])\n\ntrainset_feature_map=pd.DataFrame({'train_feature':trainset['rank'],'encoded_train_feature':encoded_train_feature})\ntrainset_feature_map=trainset_feature_map.drop_duplicates()\n\nprint(trainset_feature_map)\n\ntestset_feature=pd.DataFrame({'testset_feature':testset['rank']})\ntestset_feature_unique=testset_feature.drop_duplicates()\n\n\nprint(testset_feature_unique)\n\n#pana aici e bine\n\ndef select(x):\n  if len(trainset_feature_map[trainset_feature_map['train_feature']==x]):\n    return trainset_feature_map[trainset_feature_map['train_feature']==x]['encoded_train_feature']\n  else:\n    return len(trainset_feature_map)+1\n\n\ntestset_feature_unique['encoded_test_feature']=testset_feature_unique['testset_feature'].apply(select)\n\nprint(testset_feature_unique)\n\n\n\n# print(trainset_feature_map[trainset_feature_map['train_feature']==' Mr']['encoded_train_feature'])\n  # Be carefull! There is a spacebar right before!\n\n'''","01617183":"# So after serching for 3 days, I managed to find the solution on StackOverflow. Unfortunatelly, \n# I cannot trace it back to give credit to the author\n\n# What the following function does, is basically, it does everything a LabelEncoder does, \n# only it assigns any new value in the testset, the highest label in the training set +1","19036087":"from sklearn.preprocessing import LabelEncoder\nimport numpy as np\n\n\nclass LabelEncoderExt(object):\n    def __init__(self):\n        \"\"\"\n        It differs from LabelEncoder by handling new classes and providing a value for it [Unknown]\n        Unknown will be added in fit and transform will take care of new item. It gives unknown class id\n        \"\"\"\n        self.label_encoder = LabelEncoder()\n        # self.classes_ = self.label_encoder.classes_\n\n    def fit(self, data_list):\n        \"\"\"\n        This will fit the encoder for all the unique values and introduce unknown value\n        :param data_list: A list of string\n        :return: self\n        \"\"\"\n        self.label_encoder = self.label_encoder.fit(list(data_list) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n\n        return self\n\n    def transform(self, data_list):\n        \"\"\"\n        This will transform the data_list to id list where the new values get assigned to Unknown class\n        :param data_list:\n        :return:\n        \"\"\"\n        new_data_list = list(data_list)\n        for unique_item in np.unique(data_list):\n            if unique_item not in self.label_encoder.classes_:\n                new_data_list = ['Unknown' if x==unique_item else x for x in new_data_list]\n\n        return self.label_encoder.transform(new_data_list)","8d843bd0":"for feature in hccc:\n    encoded=LabelEncoderExt().fit(trainset[feature])\n    df[feature+'_labels']=encoded.transform(df[feature])","66aeb98d":"df.drop(hccc,axis=1,inplace=True)","e5dc3cae":"df.head()","e097a646":"from sklearn.preprocessing import OneHotEncoder\nOH_encoder=OneHotEncoder(handle_unknown='ignore',sparse=False)\n#'handle_unknown' argument  helps us avoid errors when test data contains new values, which aren't in the\n# training data\n#'sparse' agument return the output as an array, and not as a huge, hard to use sparse matrix\n\nOH_encoder.fit(trainset[lccc])\n\nOH_encoded=pd.DataFrame(OH_encoder.transform(df[lccc]))\n\nOH_encoded.index=df.index\n","c6032755":"OH_encoded  # same as pd.get_dummies","c78a0db6":"df=pd.concat([df,OH_encoded],axis=1,sort=False)\ndf.drop(lccc,axis=1,inplace=True)","e2151fe7":"df.head()","15d2d4ef":"# Great!\n\n# Let's have a look which features impact the target variable\n# ('Survived') positively and which affect it negatively","3863e60b":"trainset=df[df['train']==1]\ntestset=df[df['train']==0]\n\ntrainset.corr()['Survived'].sort_values(ascending=False)[1:].plot(kind='bar')\n\n# I trust this plot serves the purpose mentioned above. If not, I'd love to see your suggestions in the comments!","47773fdf":"# Using this technique we are able to determine which features are most relevant to predicting survival\n\n# We fill the less useful columns with zeros( causing vaariance to drop to zero), \n# and later select columns with val()!=0","1f04faa6":"from sklearn.feature_selection import SelectKBest, f_classif\nfeature_columns=df.drop('Survived',axis=1).columns\n\n\n# We found that in this case it's better to kepp all 39 feature variables, \n# but I encourage you to play arround with different values for k\n\nselector=SelectKBest(f_classif,k=39)\nX_new=selector.fit_transform(trainset[feature_columns],trainset['Survived'])\n# X_new\n\nselected_features=pd.DataFrame(selector.inverse_transform(X_new),\n                               index=trainset.index,\n                               columns=feature_columns)\nselected_columns = selected_features.columns[selected_features.var() != 0]\n","fff13dc9":"df[selected_columns]","fd2e26b6":"# Naive Bayes 65% accuracy\n\n'''\nX=trainset[selected_columns].values\ny=trainset['Survived'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid=train_test_split(X,y,test_size=0.2,random_state=101)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit_transform(X_train)\nscaler.transform(X_valid)\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier=GaussianNB()\nclassifier.fit(X_train,y_train)\n\npred_naive_bayes=classifier.predict(X_valid)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(pred_naive_bayes,y_valid))\nprint(classification_report(pred_naive_bayes,y_valid))\n'''","11f41cb4":"# Logistic Regression 66% accuracy\n\n'''\nX=trainset[selected_columns].values\ny=trainset['Survived'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid=train_test_split(X,y,test_size=0.2,random_state=101)\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression()\nclassifier.fit(X_train,y_train)\n\npred_logistic_regression=classifier.predict(X_valid)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(pred_logistic_regression,y_valid))\nprint(classification_report(pred_logistic_regression,y_valid))\n'''","9c082a48":"# #KNearestNeighbors 68% accuracy\n'''\nX=trainset[selected_columns].values\ny=trainset['Survived'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid=train_test_split(X,y,test_size=0.2,random_state=101)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit_transform(X_train)\nscaler.transform(X_valid)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=1)\nclassifier.fit(X_train,y_train)\n\npred_KNN=classifier.predict(X_valid)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(pred_KNN,y_valid))\nprint(classification_report(pred_KNN,y_valid))\n'''\n#Choosing the suitable K value ( Elbow Method )\n'''\nerror_rate=[]\nfor i in range(1,40):\n    from sklearn.neighbors import KNeighborsClassifier\n    classifier=KNeighborsClassifier(n_neighbors=i)\n    classifier.fit(X_train,y_train)\n    pred_i=classifier.predict(X_valid)\n    error_rate.append(np.mean(pred_i!=y_valid))   # average error rate\n\nplt.plot(range(1,40),error_rate)\nplt.xlabel('K values')\nplt.ylabel('error rate')\n'''\n# It's called the Elbow Method beacause we select the k value where the steepest drop occured,\n# which causes the plot to look like an elbow\n# In this case, I think the 'elbow' occures at k=21\n\n'''\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=21)\nclassifier.fit(X_train,y_train)\n\npred_KNN=classifier.predict(X_valid)\n\nprint(confusion_matrix(pred_KNN,y_valid))\nprint(classification_report(pred_KNN,y_valid))\n'''","b97f2384":"# Kernel SVM  55% accuracy\n'''\nX=trainset[selected_columns].values\ny=trainset['Survived'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid=train_test_split(X,y,test_size=0.2,random_state=101)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\nscaler.fit_transform(X_train)\nscaler.transform(X_valid)\n\nfrom sklearn.svm import SVC\nclassifier=SVC(kernel='rbf')\nclassifier.fit(X_train,y_train)\n\npred_Kernel_SVM=classifier.predict(X_valid)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(pred_Kernel_SVM,y_valid))\nprint(classification_report(pred_Kernel_SVM,y_valid))\n'''","6e77d900":"# Random Forest 82%\n\nX=trainset[selected_columns].values\ny=trainset['Survived'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid=train_test_split(X,y,test_size=0.2,random_state=101)\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators=400)\nclassifier.fit(X_train,y_train)\n\npred_rfc=classifier.predict(X_valid)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(pred_rfc,y_valid))\nprint(classification_report(pred_rfc,y_valid))\n\n# Cross validation:\n\nfrom sklearn.model_selection import cross_val_score\nscores=(-1)*cross_val_score(classifier,X,y,cv=5,scoring='neg_mean_absolute_error')\nprint(scores.mean())\n\n# GridSearch:\n\n# The way an ML algorithm works is it usually calculates the parameters which would result in the lowest MAE possible\n# However, there are some other parameters(hyperparameters) which we can tune ourselves\n\nfrom sklearn.model_selection import GridSearchCV\n\nclassifier=RandomForestClassifier()\nparam_grid =  {'n_estimators': [800,1000,1200],\n                'bootstrap': [True,False],\n                'max_depth': [5, 10, 15],\n                'min_samples_leaf': [1,2],\n                'min_samples_split': [2,3]}\ngrid=GridSearchCV(classifier,param_grid,n_jobs=-1,verbose=3,scoring='accuracy',cv=10)\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\npred_grid_rfc=grid.predict(X_valid)\nprint(confusion_matrix(pred_grid_rfc,y_valid))\nprint(classification_report(pred_grid_rfc,y_valid))\n","a4fbcd3d":"#XGBoost  82%\n\nX=trainset[selected_columns].values\ny=trainset['Survived'].values\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid=train_test_split(X,y,test_size=0.2,random_state=101)\n\nfrom xgboost import XGBClassifier\nclassifier=XGBClassifier()\n\nclassifier.fit(X_train,y_train,\n        early_stopping_rounds=5,\n        eval_set=[(X_valid,y_valid)],\n        verbose=False)\n\npred_xgb=classifier.predict(X_valid)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nprint(confusion_matrix(pred_xgb,y_valid))\nprint(classification_report(pred_xgb,y_valid))\n\nfrom sklearn.model_selection import cross_val_score\nscores=(-1)*cross_val_score(classifier,X,y,cv=5,scoring='neg_mean_absolute_error')\nprint(scores.mean())\n\nclassifier=XGBClassifier()\nparam_grid={'n_estimators':[100,500,1000],\n            'learning_rate':[0.01,0.05,0.1],\n            'C':[0.25,0.5,0.75,1],\n            'gamma':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}\ngrid=GridSearchCV(classifier,param_grid,n_jobs=-1,verbose=3,scoring='accuracy')\ngrid.fit(X_train,y_train)\nprint(grid.best_params_)\npred_xgb=grid.predict(X_valid)\nprint(confusion_matrix(pred_xgb,y_valid))\nprint(classification_report(pred_xgb,y_valid))","990b6c7b":"X=trainset[selected_columns].values\ny=trainset['Survived'].values\n\nfrom xgboost import XGBClassifier\nxgb=XGBClassifier(n_estimators=100,learning_rate=0.01,C=0.25,gamma=0.3)\n\nxgb.fit(X,y) # we train it on the whole trainset now, we used a valid set earlier to determine accuracy\n\npredictions=xgb.predict(testset[selected_columns].values).astype(int)\n#                         !    !    !\n# It is paramount that you use the .astype(int) method as Kaggle requires \n# the predictions to be integers (0 or 1), and they initially are float64\n#                         !    !    !\nsubmission = pd.DataFrame({\n        \"PassengerId\": df2['PassengerId'],\n        \"Survived\": predictions\n    })\nsubmission.to_csv('my_titanic.csv',index=False)\n\n# we created a dataframe containing the passenger Id's from the dataframe initially given to as, and next to\n# each id we predicted the person's survival","3de3600e":"predictions","dcb48a10":"# Special Thanks\n\nAt some points in the project, when I felt stuck, I sought inspiration from:\n\n* Jose Portilla: [https:\/\/www.udemy.com\/course\/python-for-data-science-and-machine-learning-bootcamp\/](https:\/\/www.udemy.com\/course\/python-for-data-science-and-machine-learning-bootcamp\/)\n\n* Kirill Eremenko & Hadelin de Ponteves: [https:\/\/www.udemy.com\/course\/machinelearning\/](https:\/\/www.udemy.com\/course\/machinelearning\/)\n\n* Ken Jee: [https:\/\/www.kaggle.com\/kenjee\/titanic-project-example](https:\/\/www.kaggle.com\/kenjee\/titanic-project-example) and\n[https:\/\/www.youtube.com\/watch?v=I3FBJdiExcg](https:\/\/www.youtube.com\/watch?v=I3FBJdiExcg)\n\n* Pedro de Matos Gon\u00e7alves: [https:\/\/www.kaggle.com\/pedrodematos\/titanic-a-complete-approach-to-top-6-rank](https:\/\/www.kaggle.com\/pedrodematos\/titanic-a-complete-approach-to-top-6-rank)\n\n* StatQuest with Josh Starmer: [https:\/\/www.youtube.com\/watch?v=fSytzGwwBVw](https:\/\/www.youtube.com\/watch?v=fSytzGwwBVw)","d00679c8":"# General Findings\n\n\nThe project serves as a guide to further establish the steps a data scientist has to follow in order to gain insight\nfrom an unfamilliar dataset.\nThrough the means of EDA, feature generation and later feature selection and hyperparameter optimization, we were able to improve the classifier's accuracy to an impressive 82%.","5c4973ff":"# Feature Engineering ","7616f68d":"# Exploratory Data Analysis\n","1c32122f":"#       Working with numerical columns\n","dcc2a7c6":"# Overview of the project\n\n1. Importing useful libraries\n2. Getting the data\n3. Exploratory Data Analysis\n4. Feature Engineering( Taking care of missing data )\n5. Feature Generation( Using the categorical columns to derive new features )\n6. Encoding the categorical features( Label vs. OneHot )\n7. Selecting the most relevant features( Univariate Feature Selection )\n8. Splitting the data back into train\/test as well as valid sets\n9. Applying different classification algorithms to find the one which yields the highest accuracy\n10. Hyperparameter optimization\n11. Meassuring the error\n12. Creating the 'submission.csv'\n","b2931d9e":"# Feature Generation using 'itertools'","c00028d3":"For a better understanding of this technique, I highly recommend you check out the Kaggle micro-course:\n[https:\/\/www.kaggle.com\/matleonard\/feature-generation](https:\/\/www.kaggle.com\/matleonard\/feature-generation)\n","87910d47":"# Let's OneHot Encode lccc","b1e8e93e":"Useful link: [https:\/\/www.kaggle.com\/matleonard\/feature-selection](https:\/\/www.kaggle.com\/matleonard\/feature-selection)","3c438bf0":"My CPU after applying GridSearch twice:\n\n![bob%20sponge.jpg](attachment:bob%20sponge.jpg)","1a881b90":"# Getting the data","86b5942e":"#       Working with categorical columns\n","414ec352":"# Let's start by importing libraries which we are likely to use in the opening part of this project","5b121310":"# Creating the submission csv","98b8a3dd":"# Label Encoding vs. OneHot Encoding","da299e1a":"# Final words\n\n    Thank you for joinig me on this adventure!\n    \n    Looking forward to hearing from you in the comments!\n    \n                                To your next level,\n                                        Mihalceanu Cristian","5b259a68":"You can find further information on this topic here:\n[https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)\n","b86828c5":"# Code Templates for various classification algorithms","ccfabd14":"# Univariate Feature Selection"}}