{"cell_type":{"e420b419":"code","497b362d":"code","fbffdeb8":"code","0d93e654":"code","b9732d3f":"code","a77aa97e":"code","22887d01":"code","f21ac3a1":"code","d74cd7cd":"code","3340d872":"code","df0c7c86":"code","488ceb08":"code","ba4d1fcf":"code","1298ee7f":"code","92b9fc66":"code","2ee4e587":"code","91580710":"code","b7b4de3e":"code","125ba74d":"code","132c4d7b":"code","d1686828":"code","31f05f5f":"code","41416376":"code","11472200":"code","1a645768":"code","c44df929":"code","94d9252d":"code","c87c4112":"code","a7f679b5":"code","3e8db6ce":"code","aa4c109f":"code","6a83e90c":"code","5401c8b6":"code","f2772c6a":"code","3de8c2c7":"code","d503300a":"code","f01a3aa6":"code","93f88f32":"code","f88f61a9":"code","54634120":"code","36733b37":"code","06503284":"code","a9eb3b7d":"markdown","62705d2f":"markdown","d346d721":"markdown","e6fb3fc3":"markdown","826f681a":"markdown","bd2737a4":"markdown","14d468a0":"markdown","37f95dd4":"markdown"},"source":{"e420b419":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","497b362d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")","fbffdeb8":"df = pd.read_csv(\"..\/input\/bank-marketing-campaign-subscriptions\/Bank_Campaign.csv\", delimiter=\";\", na_values = [\"unknown\"])","0d93e654":"df.head()","b9732d3f":"df.shape","a77aa97e":"round(df.isna().sum() \/ (df.shape[0]), 4).sort_values(ascending=False)","22887d01":"for column in df.columns:\n    if df[column].dtypes == \"object\":\n        df[column] = df[column].fillna(df[column].value_counts(ascending=False).keys()[0])","f21ac3a1":"df.isnull().sum()","d74cd7cd":"df.info()","3340d872":"def subscribed(x):\n    if x == \"no\":\n        return 0\n    else:\n        return 1\n    \ndf[\"subscribed\"] = df[\"subscribed\"].apply(lambda x: subscribed(x))","df0c7c86":"df_objects = []\nfor column in df.columns:\n    if df[column].dtypes == \"object\":\n        df_objects.append(column)\n        \nfor item in df_objects:\n    df_inter = pd.get_dummies(df[item], drop_first=True)\n    df = pd.concat([df, df_inter], axis=1)\n    df.drop(item, axis=1, inplace=True)","488ceb08":"sns.countplot(data=df, x=\"subscribed\")","ba4d1fcf":"X = df.drop(\"subscribed\", axis=1)\ny = df[\"subscribed\"]","1298ee7f":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)","92b9fc66":"print(y.sum())\nprint(y_train.sum())\nprint(y_test.sum())","2ee4e587":"from imblearn.over_sampling import SMOTE","91580710":"oversample = SMOTE()","b7b4de3e":"X_smote, y_smote = oversample.fit_resample(X.values, y.values)","125ba74d":"X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.25, random_state=101)","132c4d7b":"from sklearn.preprocessing import StandardScaler","d1686828":"scaler = StandardScaler()","31f05f5f":"scaler.fit(X_train_smote)","41416376":"X_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX_test_smote = scaler.transform(X_test_smote)\nX_train_smote = scaler.transform(X_train_smote)","11472200":"from sklearn.ensemble import RandomForestClassifier","1a645768":"rfc = RandomForestClassifier()","c44df929":"rfc.fit(X_train_smote, y_train_smote)","94d9252d":"y_rfc = rfc.predict(X_test)","c87c4112":"from sklearn.metrics import classification_report, confusion_matrix, recall_score","a7f679b5":"print(classification_report(y_test, y_rfc))\nprint(confusion_matrix(y_test, y_rfc))","3e8db6ce":"importances = pd.DataFrame({\"Importance\": rfc.feature_importances_ * 100,\n                                   \"Feature\": X.columns})\nimportances = importances.sort_values(by=\"Importance\", ascending=False)\nimportances = importances.set_index(\"Feature\")","aa4c109f":"importances.plot(kind=\"bar\", figsize=(12,6))","6a83e90c":"import tensorflow as tf","5401c8b6":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping","f2772c6a":"X_train_smote.shape","3de8c2c7":"optimizer_a = tf.keras.optimizers.Adam(learning_rate = 0.0001, name=\"Adam\")","d503300a":"model = Sequential()\n\nmodel.add(Dense(units=47, input_dim=47, activation=\"relu\"))\nmodel.add(Dense(units=1, activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optimizer_a)","f01a3aa6":"early_stop = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=25)","93f88f32":"model.fit(x=X_train_smote,\n         y=y_train_smote,\n         validation_data=(X_test_smote, y_test_smote),\n         epochs=600,\n         callbacks=[early_stop])","f88f61a9":"pd.DataFrame(model.history.history).plot()","54634120":"y_net = (model.predict(X_test) > 0.5).astype(\"int32\")","36733b37":"print(classification_report(y_test, y_net))\nprint(confusion_matrix(y_test, y_net))","06503284":"print(\"Random Forest gives us a recall-score of: \", round(recall_score(y_test, y_rfc), 4))\nprint(\"The neuronal network gives us a recall-score of: \", round(recall_score(y_test, y_net), 4))","a9eb3b7d":"# Random Forest","62705d2f":"Die wichtigsten Features sind folgende:","d346d721":"Now we can see that there are now NaN values any more","e6fb3fc3":"Calculating missing values:","826f681a":"# Neuronales Netz","bd2737a4":"Create dummy 0,1-Values for all categorical data:","14d468a0":"By counting the subscribed values, we see, that the data set is very imbalanced. Therefore, I will use the SMOTE technique:","37f95dd4":"For objects, I fill the NaN values with the most common value from this column "}}