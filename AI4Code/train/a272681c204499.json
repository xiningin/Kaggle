{"cell_type":{"2a1884ba":"code","f9335cf6":"code","f25e337c":"code","1d74d75b":"code","f0b773a3":"code","7d1e2306":"code","d9437eb8":"code","993ffcc4":"code","df060506":"code","1ff5cedc":"code","405b0f0a":"code","8c533a45":"code","f39eafdc":"code","0012cec8":"code","c5ab0c1c":"code","cdf66592":"code","17fa5da9":"code","ac0a56e8":"code","2c781169":"code","ed91b90a":"code","fa41eb77":"code","673cd601":"code","4774ac62":"code","d840436c":"code","8a97530b":"code","4ed59c9b":"markdown","ee7da74f":"markdown"},"source":{"2a1884ba":"# Will reduce data load for code test\ntoy = False","f9335cf6":"from kaggle.competitions import twosigmanews\n# You can only call make_env() once, so don't lose it!\nenv = twosigmanews.make_env()\nprint('Done!')","f25e337c":"(market_train_df, news_train_df) = env.get_training_data() ","1d74d75b":"market_train_df.shape, news_train_df.shape","f0b773a3":"# We will reduce the number of samples for memory reasons\nif toy:\n    market_train_df = market_train_df.tail(100_000)\n    news_train_df = news_train_df.tail(300_000)\nelse:\n    market_train_df = market_train_df.tail(3_000_000)\n    news_train_df = news_train_df.tail(6_000_000)","7d1e2306":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom itertools import chain\n\n%matplotlib inline","d9437eb8":"news_cols_agg = {\n    'urgency': ['min', 'count'],\n    'takeSequence': ['max'],\n    'bodySize': ['min', 'max', 'mean', 'std'],\n    'wordCount': ['min', 'max', 'mean', 'std'],\n    'sentenceCount': ['min', 'max', 'mean', 'std'],\n    'companyCount': ['min', 'max', 'mean', 'std'],\n    'marketCommentary': ['min', 'max', 'mean', 'std'],\n    'relevance': ['min', 'max', 'mean', 'std'],\n    'sentimentNegative': ['min', 'max', 'mean', 'std'],\n    'sentimentNeutral': ['min', 'max', 'mean', 'std'],\n    'sentimentPositive': ['min', 'max', 'mean', 'std'],\n    'sentimentWordCount': ['min', 'max', 'mean', 'std'],\n    'noveltyCount12H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount24H': ['min', 'max', 'mean', 'std'],\n    'noveltyCount3D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount5D': ['min', 'max', 'mean', 'std'],\n    'noveltyCount7D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts12H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts24H': ['min', 'max', 'mean', 'std'],\n    'volumeCounts3D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts5D': ['min', 'max', 'mean', 'std'],\n    'volumeCounts7D': ['min', 'max', 'mean', 'std']\n}","993ffcc4":"def join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")    \n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n\n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expandaded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes'] + sorted(news_cols_agg.keys())\n    news_train_df_expanded = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Aggregate numerical news features\n    news_train_df_aggregated = news_train_df_expanded.groupby(['time', 'assetCode']).agg(news_cols_agg)\n    \n    # Free memory\n    del news_train_df_expanded\n\n    # Convert to float32 to save memory\n    news_train_df_aggregated = news_train_df_aggregated.apply(np.float32)\n\n    # Flat columns\n    news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df","df060506":"def get_xy(market_train_df, news_train_df, le=None):\n    x, le = get_x(market_train_df, news_train_df)\n    y = market_train_df['returnsOpenNextMktres10'].clip(-1, 1)\n    return x, y, le\n\n\ndef label_encode(series, min_count):\n    vc = series.value_counts()\n    le = {c:i for i, c in enumerate(vc.index[vc >= min_count])}\n    return le\n\n\ndef get_x(market_train_df, news_train_df, le=None):\n    # Split date into before and after 22h (the time used in train data)\n    # E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n    #      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\n    news_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n    # Round time of market_train_df to 0h of curret day\n    market_train_df['time'] = market_train_df['time'].dt.floor('1D')\n\n    # Join market and news\n    x = join_market_news(market_train_df, news_train_df)\n    \n    # If not label-encoder... encode assetCode\n    if le is None:\n        le_assetCode = label_encode(x['assetCode'], min_count=10)\n        le_assetName = label_encode(x['assetName'], min_count=5)\n    else:\n        # 'unpack' label encoders\n        le_assetCode, le_assetName = le\n        \n    x['assetCode'] = x['assetCode'].map(le_assetCode).fillna(-1).astype(int)\n    x['assetName'] = x['assetName'].map(le_assetName).fillna(-1).astype(int)\n    \n    try:\n        x.drop(columns=['returnsOpenNextMktres10'], inplace=True)\n    except:\n        pass\n    try:\n        x.drop(columns=['universe'], inplace=True)\n    except:\n        pass\n    x['dayofweek'], x['month'] = x.time.dt.dayofweek, x.time.dt.month\n    x.drop(columns='time', inplace=True)\n#    x.fillna(-1000,inplace=True)\n\n    # Fix some mixed-type columns\n    for bogus_col in ['marketCommentary_min', 'marketCommentary_max']:\n        x[bogus_col] = x[bogus_col].astype(float)\n    \n    return x, (le_assetCode, le_assetName)","1ff5cedc":"%%time\n\n# This will take some time...\nX, y, le = get_xy(market_train_df, news_train_df)\n\n# X = X.sample(frac=1, random_state=173)\n# y = y.sample(frac=1, random_state=173)\n\nX.shape, y.shape","405b0f0a":"# Save universe data for latter use\nuniverse = market_train_df['universe']\ntime = market_train_df['time']\n\n# Free memory\ndel market_train_df, news_train_df\nuniverse.value_counts()","8c533a45":"X_ = X\n# Keep only text columns\nX = X_#.iloc[:, X.columns.get_loc('urgency_min'):X.columns.get_loc('dayofweek')]\nX.tail()","f39eafdc":"lgb_params = dict(\n    objective = 'regression_l1',\n    learning_rate = 0.1,\n    num_leaves = 127,\n    max_depth = -1,\n#     min_data_in_leaf = 1000,\n#     min_sum_hessian_in_leaf = 10,\n    bagging_fraction = 0.75,\n    bagging_freq = 2,\n    feature_fraction = 0.5,\n    lambda_l1 = 0.0,\n    lambda_l2 = 1.0,\n    metric = 'None', # This will ignore the loss objetive and use sigma_score instead,\n    seed = 42 # Change for better luck! :)\n)\n\ndef sigma_score(preds, valid_data):\n    df_time = valid_data.params['extra_time']\n    labels = valid_data.get_label()\n    \n#    assert len(labels) == len(df_time)\n\n    x_t = preds * labels #  * df_valid['universe'] -> Here we take out the 'universe' term because we already keep only those equals to 1.\n    \n    # Here we take advantage of the fact that `labels` (used to calculate `x_t`)\n    # is a pd.Series and call `group_by`\n    x_t_sum = x_t.groupby(df_time).sum()\n    score = x_t_sum.mean() \/ x_t_sum.std()\n\n    return 'sigma_score', score, True\n","0012cec8":"def divide_assets(all_assets, parts):\n    return_arr = []\n    uniqueue_assets = all_assets.value_counts()\n    uniqueue_assets = uniqueue_assets.sample(frac=1, random_state=173)\n    for i in range(parts):\n        start_ratio = (i) \/ parts\n        end_ratio = (i+1) \/ parts\n        start_index = int(uniqueue_assets.shape[0] * start_ratio)\n        end_index = int(uniqueue_assets.shape[0] * end_ratio)\n        part_of_assets = uniqueue_assets.iloc[start_index:end_index]\n        return_arr.append(part_of_assets)\n    return return_arr","c5ab0c1c":"def cross_validation(X, Y, parts, scoring):\n    scores = []\n\n    train_cols = X.columns.tolist()\n    train_cols.remove('assetCode')\n    train_cols.remove('assetName')\n    categorical_cols = ['dayofweek', 'month']\n    \n    arr_assets = divide_assets(X['assetCode'], parts)\n    counter = 0\n    for assets in arr_assets:\n        counter += 1\n        print(counter)\n#         start_ratio = (i) \/ parts\n#         end_ratio = (i+1) \/ parts\n#         start_index = int(X.shape[0] * start_ratio)\n#         end_index = int(X.shape[0] * end_ratio)\n#         X_train, y_train = X.iloc[:start_index].append(X.iloc[end_index:]), Y.iloc[:start_index].append(Y.iloc[end_index:]) # append\n#         X_valid, y_valid = X.iloc[start_index:end_index], Y.iloc[start_index:end_index]\n        X_train = X.loc[~X['assetCode'].isin(assets)]\n        y_train = Y[X_train.index]\n#         Y[Y.index.isin(X_train.index)]\n        X_valid = X.loc[X['assetCode'].isin(assets)]\n        y_valid = Y[X_valid.index]\n        \n        u_valid = (universe[X_valid.index] > 0)\n        t_valid = time[X_valid.index]\n        X_valid = X_valid[u_valid]\n        y_valid = y_valid[u_valid]\n        t_valid = t_valid[u_valid]\n        \n        X_train = X_train.drop(['assetCode', 'assetName'], axis=1)\n        X_valid = X_valid.drop(['assetCode', 'assetName'], axis=1)\n        dtrain = lgb.Dataset(X_train, y_train, feature_name=train_cols, categorical_feature=categorical_cols)\n        dvalid = lgb.Dataset(X_valid, y_valid, feature_name=train_cols, categorical_feature=categorical_cols)\n        dvalid.params = {\n            'extra_time': t_valid.factorize()[0]\n        }\n        \n        labels = dvalid.get_label()\n        \n        evals_result = {}\n        \n        model = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=False,\n                          early_stopping_rounds=100, feval=sigma_score, evals_result=evals_result)\n        df_result = pd.DataFrame(evals_result['valid'])\n        max_score = df_result['sigma_score'].max()\n        scores.append(max_score)\n        print(max_score)\n#         df_result = pd.DataFrame(evals_result['valid'])\n        del dtrain, dvalid, evals_result, df_result\n#         lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n#               early_stopping_rounds=100, feval=sigma_score, evals_result=evals_result)","cdf66592":"# cross_validation(X, y, 5, sigma_score)","17fa5da9":"# Stop. Wait a minute ","ac0a56e8":"uniqueue_assets = X['assetCode'].value_counts()\nuniqueue_assets = uniqueue_assets.sample(frac=1, random_state=173)\n\nend_index = int(uniqueue_assets.shape[0] * 0.8)\ntrain_assets = uniqueue_assets.iloc[:end_index]\n\nX_train = X.loc[X['assetCode'].isin(train_assets)]\ny_train = y[X_train.index]\nX_valid = X.loc[~X['assetCode'].isin(train_assets)]\ny_valid = y[X_valid.index]\n\nu_valid = (universe[X_valid.index] > 0)\nt_valid = time[X_valid.index]\nX_valid = X_valid[u_valid]\ny_valid = y_valid[u_valid]\nt_valid = t_valid[u_valid]\n\nX_train = X_train.drop(['assetCode', 'assetName'], axis=1)\nX_valid = X_valid.drop(['assetCode', 'assetName'], axis=1)\n\ntrain_cols = X.columns.tolist()\ntrain_cols.remove('assetCode')\ntrain_cols.remove('assetName')\n        \n# ------------------------------------------------------------\n\ncategorical_cols = ['dayofweek', 'month']\n\n# Note: y data is expected to be a pandas Series, as we will use its group_by function in `sigma_score`\ndtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\ndvalid = lgb.Dataset(X_valid.values, y_valid, feature_name=train_cols, categorical_feature=categorical_cols, free_raw_data=False)\n\n# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\ndvalid.params = {\n    'extra_time': t_valid.factorize()[0]\n}\n\nevals_result = {}\nm = lgb.train(lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), verbose_eval=25,\n              early_stopping_rounds=100, feval=sigma_score, evals_result=evals_result)\n\n# del X_train, y_train, X_valid, y_valid\n\ndf_result = pd.DataFrame(evals_result['valid'])","2c781169":"ax = df_result.plot(figsize=(12, 8))\nax.scatter(df_result['sigma_score'].idxmax(), df_result['sigma_score'].max(), marker='+', color='red')","ed91b90a":"num_boost_round, valid_score = df_result['sigma_score'].idxmax()+1, df_result['sigma_score'].max()\nprint(lgb_params)\nprint(f'Best score was {valid_score:.5f} on round {num_boost_round}')","fa41eb77":"fig, ax = plt.subplots(1, 2, figsize=(14, 14))\nlgb.plot_importance(m, ax=ax[0])\nlgb.plot_importance(m, ax=ax[1], importance_type='gain')\nfig.tight_layout()","673cd601":"# Train full model\n# X = X.drop(['assetCode', 'assetName'], axis=1)\ndtrain_full = lgb.Dataset(X, y, feature_name=train_cols, categorical_feature=categorical_cols)\n\nmodel = lgb.train(lgb_params, dtrain, num_boost_round=num_boost_round)","4774ac62":"def make_predictions(predictions_template_df, market_obs_df, news_obs_df, le):\n    x, _ = get_x(market_obs_df, news_obs_df, le)\n    predictions_template_df.confidenceValue = np.clip(model.predict(x), -1, 1)","d840436c":"days = env.get_prediction_days()\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n    make_predictions(predictions_template_df, market_obs_df, news_obs_df, le)\n    env.predict(predictions_template_df)\nprint('Done!')","8a97530b":"env.write_submission_file()","4ed59c9b":"## A simple model - using the market and news data","ee7da74f":"# Train full model\nNow we train a full model with `num_boost_round` found in validation."}}