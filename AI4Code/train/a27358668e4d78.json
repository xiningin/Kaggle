{"cell_type":{"9b2a0c47":"code","eba62ee0":"code","3754f091":"code","20862210":"code","1ddbafcd":"code","d3327f8e":"code","0925c5af":"code","ce0dc73c":"code","1a92d8d7":"code","b44195da":"code","2a12c0fa":"code","5de2a286":"code","baa4c9cb":"code","0881d67c":"code","54008a26":"code","61b53eb3":"code","5c906b23":"code","b99ad104":"code","5d5ba211":"code","5c2fb592":"code","d9ae8a6a":"code","8adaefc3":"code","930f3927":"code","6fb327d8":"code","c426361e":"code","b97b1c74":"code","6fb2e013":"markdown","395d0bc0":"markdown","5256aab5":"markdown","71b3e7b4":"markdown","ced34048":"markdown","9138e73c":"markdown","1d9d49be":"markdown","31f61ad7":"markdown","a68dc117":"markdown","2e47f666":"markdown","585b875f":"markdown","2c478d05":"markdown","4ea6418e":"markdown","509fcec8":"markdown","53f51524":"markdown","5992f711":"markdown","655d997b":"markdown","a7ab43ce":"markdown","ee4699e9":"markdown"},"source":{"9b2a0c47":"import gc\nimport glob\nimport os\n\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contextlib import contextmanager\nfrom multiprocessing.pool import ThreadPool, Pool\nfrom joblib import Parallel, delayed\nimport time\n\n%matplotlib inline","eba62ee0":"pd.set_option(\"display.max_columns\", 96)\npd.set_option(\"display.max_rows\", 96)\n\nplt.rcParams['figure.figsize'] = (12, 9)","3754f091":"debug = True\nn_debug_samples = 200000","20862210":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')\n\n(market_train_df, news_train_df) = env.get_training_data()\n\ndf_ = market_train_df.copy()\nif debug:\n    df_ = df_.iloc[:n_debug_samples, :]\nprint(df_.shape)","1ddbafcd":"@contextmanager\ndef timer(name):\n    t0 = time.time()\n    yield\n    print('{0} done in {1:.3f} seconds.'.format(name, time.time() - t0))\n    \n\ndef split_df(df, num_splits):\n    \n    df_list = []\n    rows_splits = np.linspace(0, df.shape[0], num_splits).astype(np.int)\n    print('Split into {} parts'.format(num_splits))\n    print('Row splits:\\n{}'.format(rows_splits))\n    \n    for i in range(len(rows_splits) - 1):\n        df_list.append(df.iloc[rows_splits[i]:rows_splits[i+1]])\n        \n    return df_list","d3327f8e":"dfs_split = split_df(df_, 4)\ndfs_ = pd.concat(dfs_split, ignore_index=True, sort=False)","0925c5af":"random_index = np.random.randint(0, df_.shape[0])\n\nprint(random_index)\nprint(df_.iloc[random_index, :])\nprint('\\n', dfs_.iloc[random_index, :])","ce0dc73c":"# Process time column to datetime format\ndef datetime_proc(df):\n    df['time'] = pd.to_datetime(df['time'])\n    return df\n\n\n# Create features containing information about time dimension\ndef create_time_resolutions(df):\n    \n    df = df.copy()\n    df['dt_hour'] = df.time.dt.floor('h')\n    df['dt_day'] = df.time.dt.floor('d')\n    df['dt_weekofyear'] = df.apply(\n        lambda x: '{}_{}'.format(x['time'].weekofyear, x['time'].year), axis=1)\n    \n    return df\n\n\n# Rename columns after grouping for easy merge and access\ndef rename_columns(df):\n    \n    df.columns = pd.Index(['{}{}'.format(\n        c[0], c[1].upper()) for c in df.columns.tolist()])\n    \n    return df\n\n\n# Create grouped features\ndef create_grouped_df(df, group_by, columns_set):\n    \n    df_grouped = df.groupby(group_by)[columns_set].agg(aggs_num).reset_index()\n    df_grouped = rename_columns(df_grouped)\n    \n    return df_grouped\n\n\n# Create grouped features with shift \ndef create_grouped_df_shifted(df, group_by, columns_set, shift_name=['assetName'], shift=1):\n    \n    df_grouped = df.groupby(group_by)[columns_set].agg(aggs_num).groupby(\n        shift_name).shift(shift).reset_index()\n    df_grouped = rename_columns(df_grouped)\n    \n    return df_grouped","1a92d8d7":"# News columns:\nnews_cols_agg_num = ['urgency', 'sentenceCount', 'wordCount',\n                    'firstMentionSentence', 'relevance',\n                   'sentimentClass', 'sentimentNegative',\n                   'sentimentNeutral', 'sentimentPositive',\n                   'sentimentWordCount']\n\nnews_cols_agg_cat = ['sourceId', 'provider', 'headlineTag',\n                    'marketCommentary']\n\n\n# Market columns:\nmarket_cols_agg_num = ['volume', 'close', 'open',\n                      'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',]\n\nmarket_cols_agg_cat = ['assetCode']\n\n\naggs_num = ['mean', 'std']\naggs_cat = ['count']","b44195da":"%%time\n\ndfs_ = df_.assign(time = pd.to_datetime(df_['time']))","2a12c0fa":"with timer('datetime processing:'):\n    df_ = create_time_resolutions(df_)","5de2a286":"with timer('pool datetime processing:'):\n    with Pool(processes=4) as pool:\n        dfs_proc = pool.map(create_time_resolutions, dfs_split)\n        \n\nwith timer('pool datetime processing threads:'):\n    with ThreadPool(processes=4) as pool:\n        dfs_proc = pool.map(create_time_resolutions, dfs_split)","baa4c9cb":"with timer('joblib parallel datetime processing:'):\n    dfs_proc = Parallel(n_jobs=4)(delayed(create_time_resolutions)(i) for i in dfs_split)\n    \n    \nwith timer('joblib parallel datetime processing threads:'):\n    dfs_proc = Parallel(n_jobs=4, prefer='threads')(delayed(create_time_resolutions)(i) for i in dfs_split)","0881d67c":"# Now, let's concat processed DFs into one:\ndf_time = pd.concat(dfs_proc, ignore_index=True, sort=False)","54008a26":"# \ngroupings = [\n    ['dt_day', 'assetName'],\n    ['dt_weekofyear', 'assetName'],\n    ['assetName', 'open'],\n    ['assetName', 'close'],\n]","61b53eb3":"dfs_proc = []\n\nwith timer('grouping features:'):\n    for i in groupings:\n        dfs_proc.append(create_grouped_df(df_time, group_by=i, columns_set=market_cols_agg_num))\n        \n        \ndfs_proc_shift = []\n\nwith timer('grouping features shifted:'):\n    for i in groupings:\n        dfs_proc_shift.append(create_grouped_df_shifted(df_time, group_by=i, columns_set=market_cols_agg_num))","5c906b23":"with timer('pool grouping features parallel:'):\n    with Pool(processes=4) as pool:\n        dfs_proc = pool.starmap(create_grouped_df,\n                                [(df_time, groupings[0], market_cols_agg_num),\n                                (df_time, groupings[1], market_cols_agg_num),\n                                (df_time, groupings[2], market_cols_agg_num),\n                                (df_time, groupings[3], market_cols_agg_num)])\n        \nwith timer('pool grouping features parallel threads:'):\n    with ThreadPool(processes=4) as pool:\n        dfs_proc_shift = pool.starmap(create_grouped_df,\n                                [(df_time, groupings[0], market_cols_agg_num),\n                                (df_time, groupings[1], market_cols_agg_num),\n                                (df_time, groupings[2], market_cols_agg_num),\n                                (df_time, groupings[3], market_cols_agg_num)])\n        \n\nwith timer('pool grouping features shifted parallel:'):\n    with Pool(processes=4) as pool:\n        dfs_proc = pool.starmap(create_grouped_df_shifted,\n                                [(df_time, groupings[0], market_cols_agg_num),\n                                (df_time, groupings[1], market_cols_agg_num),\n                                (df_time, groupings[2], market_cols_agg_num),\n                                (df_time, groupings[3], market_cols_agg_num)])\n        \nwith timer('pool grouping features shifted parallel threads:'):\n    with ThreadPool(processes=4) as pool:\n        dfs_proc_shift = pool.starmap(create_grouped_df_shifted,\n                                [(df_time, groupings[0], market_cols_agg_num),\n                                (df_time, groupings[1], market_cols_agg_num),\n                                (df_time, groupings[2], market_cols_agg_num),\n                                (df_time, groupings[3], market_cols_agg_num)])","b99ad104":"# Without prefer='threads' joblib throws an error.\n\nwith timer('joblib grouping features parallel threads:'):\n    dfs_proc = Parallel(n_jobs=4, prefer='threads')(delayed(create_grouped_df)(\n        df_time, group_by=i, columns_set=market_cols_agg_num) for i in groupings)\n    \n    \nwith timer('joblib grouping features shifted parallel threads:'):\n    dfs_proc_shift = Parallel(n_jobs=4, prefer='threads')(delayed(create_grouped_df_shifted)(\n        df_time, group_by=i, columns_set=market_cols_agg_num) for i in groupings)","5d5ba211":"X = df_time.copy()\n\nfor i in range(len(groupings)):\n    X = X.merge(dfs_proc[i], how='left',\n                on=groupings[i], suffixes=('', '_basic_{}'.format('_'.join(groupings[i]))))\n    X = X.merge(dfs_proc_shift[i], how='left',\n                on=groupings[i], suffixes=('', '_shift1_{}'.format('_'.join(groupings[i]))))\n    \nX","5c2fb592":"# Floor all datetime objects to a specified resolution:\n# 'd' - days\n# 'h' - hours\nX['time_split_resolution'] = X.time.dt.floor('d')\n\n# Select unique values based on market set:\ntrain_times_unique = X.time_split_resolution.unique()\n# Split unique dates into 80\/20% training\/validation split:\ntr_times, valid_times = train_test_split(train_times_unique, test_size=0.2, random_state=1337)\n\n# Create subsets for market and news datasets:\nX_tr = X[X.time_split_resolution.isin(tr_times)]\nX_val = X[X.time_split_resolution.isin(valid_times)]\n\nprint('Dataset shapes: train - {}, valid - {}'.format(X_tr.shape, X_val.shape))","d9ae8a6a":"def get_input(df_, to_drop):\n    X = df_.drop(to_drop, axis=1)\n    y = (df_.loc[:, 'returnsOpenNextMktres10'] >= 0).values.astype(np.uint8)\n    # y = df_.loc[:, 'returnsOpenNextMktres10'].values\n    r = df_.loc[:, 'returnsOpenNextMktres10'].values\n    u = df_.loc[:, 'universe']\n    d = df_.loc[:, 'time'].dt.date\n    return X, y, r, u, d\n\n\nto_drop = ['returnsOpenNextMktres10',\n          'universe',\n          'time',\n          'assetCode',\n          'assetName',\n          'dt_weekofyear',\n          'dt_hour',\n          'dt_day',\n          'time_split_resolution']","8adaefc3":"X_train, y_train, r_train, u_train, d_train = get_input(X_tr, to_drop)\nX_valid, y_valid, r_valid, u_valid, d_valid = get_input(X_val, to_drop)","930f3927":"train_cols = X_train.columns.tolist()\n\ndtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols)\ndvalid = lgb.Dataset(X_valid.values, y_valid,\n                     feature_name=train_cols, reference=dtrain)\n\n\nparams = {'learning_rate': 0.05,\n          'boosting': 'gbdt', \n          'objective': 'binary', \n          'seed': 2018,\n          'nthreads': 1}\n\nwith timer('basic model training:'):\n    lgb_model = lgb.train(params, dtrain, \n                          num_boost_round=1000, \n                          valid_sets=(dvalid,), \n                          valid_names=('valid',), \n                          verbose_eval=25, \n                          early_stopping_rounds=20)","6fb327d8":"params1 = {'learning_rate': 0.05,\n          'boosting': 'gbdt', \n          'objective': 'binary', \n          'num_leaves': 8,\n          'min_data_in_leaf': 4,\n          'max_bin': 255,\n          'bagging_fraction': 0.7,\n          'lambda_l2': 0.01,\n          'max_depth': 12,\n          'seed': 2018,\n          'nthreads': 1}\n\nparams2 = {'learning_rate': 0.005,\n          'boosting': 'gbdt', \n          'objective': 'binary', \n          'num_leaves': 16,\n          'min_data_in_leaf': 2,\n          'max_bin': 255,\n          'bagging_fraction': 0.4,\n          'lambda_l2': 0.001,\n          'max_depth': 16,\n          'seed': 2018,\n          'nthreads': 1}\n\nparams3 = {'learning_rate': 0.1,\n          'boosting': 'gbdt', \n          'objective': 'binary', \n          'num_leaves': 6,\n          'min_data_in_leaf': 4,\n          'max_bin': 128,\n          'bagging_fraction': 0.9,\n          'lambda_l2': 0.03,\n          'max_depth': 7,\n          'seed': 2018,\n          'nthreads': 1}\n\nparams4 = {'learning_rate': 0.001,\n          'boosting': 'gbdt', \n          'objective': 'binary', \n          'num_leaves': 64,\n          'min_data_in_leaf': 16,\n          'max_bin': 255,\n          'bagging_fraction': 0.75,\n          'lambda_l2': 1e-3,\n          'max_depth': 14,\n          'seed': 2018,\n          'nthreads': 1}\n\n\nparams_list = [\n    params1,\n    params2,\n    params3,\n    params4,\n]\n\n\ndef train_lgb(params, dtrain, dvalid):\n    \n    lgb_model = lgb.train(params, dtrain, \n                      num_boost_round=1000, \n                      valid_sets=(dvalid,), \n                      valid_names=('valid',), \n                      verbose_eval=25, \n                      early_stopping_rounds=20)\n    \n    return lgb_model","c426361e":"with timer('joblib lgbm models training:'):\n    lgb_models = Parallel(n_jobs=4, prefer='threads')(delayed(train_lgb)(\n        i, dtrain, dvalid) for i in params_list)","b97b1c74":"for l in lgb_models:\n    y_pred_conf_valid = l.predict(X_valid) * 2 - 1\n    y_pred_conf_valid_binary = (y_pred_conf_valid > 0).astype(np.uint8)\n    print('Valid accuracy: {:.4f}'.format(accuracy_score(y_valid, y_pred_conf_valid_binary)))\n\n    confidence_valid = l.predict(X_valid) * 2 - 1\n    r_valid = r_valid.clip(-1,1)\n    x_t_i = confidence_valid * r_valid * u_valid\n    data = {'day' : d_valid, 'x_t_i' : x_t_i}\n    df = pd.DataFrame(data)\n    x_t = df.groupby('day').sum().values.flatten()\n    mean = np.mean(x_t)\n    std = np.std(x_t)\n    score_valid = mean \/ std\n    print('Valid score: {:.4f}'.format(score_valid))","6fb2e013":"### Features merge:\n\n\nAfter creating features in parallel, let's merge them into main DF easily (thanks to renaming of columns!): ","395d0bc0":"It's time to prepare training and validation sets:","5256aab5":"### Functions for timing and splitting DFs:\n\nFunction for timing taken from [Mercari kernel](https:\/\/www.kaggle.com\/lopuhin\/mercari-golf-0-3875-cv-in-75-loc-1900-s).\n`split_df` function creates number of DFs equal to `num_splits`, each with the same size. This enables easy calling of parallel functions on those mini-DFs.","71b3e7b4":"Below are definitions of a few functions, which will serve as a showcase of parallel processing:","ced34048":"Once again, we begin with setting `time` columns to datetime format.","9138e73c":"Now let's see how creation of datetime features will proceed, both sequentially and in parallel:","1d9d49be":"### Models training:\n\nNow let's train 4 LightGBM models in parallel. This may not be optimal for real use-cases, but let's do it here, just for the sake of experimentation!\nTraining a few models in parallel with different parameters is usually not a good choice, because they will end training in different points in time but due to parallel function call, all the models must finish before you will be able to access them. \nGBM models generally scale well (up to a few threads at least), so it's better to run each model sequentially but with higher number of `nthreads`.\nOne scenario when this may work is when you have access to a machine with a lot of threads and you can train N models, each with 8-12 threads. In such case, it's better to structure the parallel function for training to output model predictions and save the model itself, so you can access them as soon as each model training finishes.","31f61ad7":"### joblib:","a68dc117":"Definitions of aggregates for feature engineering part:","2e47f666":"Seems like for groupings differences in parallel vs sequential processing aren't very significant. This is probably due to grouping operation implementation being efficient on it's own, in such cases the gains will be less visible. It is worth to benchmark the performance of parallel version before wrapping a function into parallel processing methods.\nAn even more interesting fact is that here `threads` backend seems to be significantly faster for `multiprocessing` and it isn't possible to call the `joblib` function without specifying this backend.\nAccording to a stackoverflow [explanation](https:\/\/stackoverflow.com\/questions\/46045956\/whats-the-difference-between-threadpool-vs-pool-in-python-multiprocessing-modul) `ThreadPool` uses threads instead of processes. And according to [joblib docs](https:\/\/joblib.readthedocs.io\/en\/latest\/generated\/joblib.Parallel.html#joblib.Parallel), `threading` backend, which is chosen with `prefer='threads'` causes very low overhead. If grouping is an efficient function, low overhead from the backend itself may be the cause of timing difference.","585b875f":"## Feature engineering:\n\nLet's get to parallel feature engineering. To showcase parallel processing in this case, a few groupings will be created, which will serve as input to `pandas.groupby` function. Those groupings will be processed in parallel.","2c478d05":"### Parallel model training:\n\nWe can parallelize model training too!\nFirst, a list of parameter set will be needed for each model.","4ea6418e":"Parallel processing seems to be quicker than sequential. But an interesting thing can be noticed here, choosing `threads` as multiprocessing backend causes the parallel processing to significantly slow down. With this backend, parallel processing sometimes is even slower than sequential.\nTheoretically, joblib and pool should give the same or very similar results. This is the case with experiments on my home machine but it seems variability in kernels is very high. On one attempt, multiprocessing was much quicker (9s vs 17s), on next one joblib is faster (9s vs 8s).","509fcec8":"![](http:\/\/)## Parallel Data Processing\n\nSpeed of data processing and model training is very important. The faster you are able to iterate over various ideas, the better chance of finding methods that will improve final score. Aspect of speed is especially important in constrained environment, such as kernels. Because this is a kernels competition, those concepts apply here perfectly.\nOne method of improving speed of data processing pipelines is parallel processing. In python, this can be easily done by splitting main input into parts and then applying a chosen function over those parts in parallel. Such functionality is provided for example by `multiprocessing` or `joblib` libraries.\n\nIn this kernel, a few examples of parallel data processing are shown.\nFirst, a function showing how to split data into `n` equal parts is presented. Then, selected functions are applied in parallel on each of the sub-DFs.\n\nThis kernels is partially based on [shifted aggregates kernel](https:\/\/www.kaggle.com\/wrosinski\/lgbm-shifted-aggregates-example).","53f51524":"### multiprocessing:","5992f711":"Some of the models score better, some worse but averaging their predictions may be still beneficial for the ensemble!","655d997b":"Let's split main DF into 4 parts and just for sanity check concatenate them again to check if there are no bugs:","a7ab43ce":"### sequentially:","ee4699e9":"You can call this cell a few times in order to see, whether rows values are equal between original DF and concatenated one after splits:"}}