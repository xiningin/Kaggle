{"cell_type":{"07b52d29":"code","9022db49":"code","3170258d":"code","6e7a6867":"code","b2edce5c":"code","1e0d8057":"code","bbae9a9d":"code","812ba1bd":"code","a07620db":"code","8a235c41":"code","a979e05b":"code","9d944fd4":"code","77eddb95":"code","8ce4110d":"code","6a9ec363":"markdown","5e0b343b":"markdown","21dda424":"markdown","eaf55a99":"markdown","67547162":"markdown","a4501bb2":"markdown","a5e7e6a0":"markdown","c84d1cec":"markdown","be34c063":"markdown"},"source":{"07b52d29":"pip install autokeras","9022db49":"import pandas as pd\nimport autokeras as ak","3170258d":"test = pd.read_csv('..\/input\/titanic\/test.csv', index_col='PassengerId')\ntrain = pd.read_csv('..\/input\/titanic\/train.csv', index_col='PassengerId')","6e7a6867":"train","b2edce5c":"train.select_dtypes(exclude='object').describe()","1e0d8057":"train.select_dtypes(include='object').describe()","bbae9a9d":"train.Embarked.value_counts()","812ba1bd":"train = train.drop(['Cabin', 'Name', 'Ticket'], axis='columns')\ntest = test.drop(['Cabin', 'Name', 'Ticket'], axis='columns')","a07620db":"X = train.drop(['Survived'], axis='columns')\ny = train.Survived","8a235c41":"X.head()","a979e05b":"y.head()","9d944fd4":"nn_clf = ak.StructuredDataClassifier(max_trials=5)\nnn_clf.fit(X, y, epochs=10)","77eddb95":"model_ak = nn_clf.export_model()\nmodel_ak.summary()","8ce4110d":"preds_np = nn_clf.predict(test)\nnn_preds = pd.DataFrame(preds_np, dtype='int64').iloc[:, 0]\nnn_output = pd.DataFrame({'PassengerId': test.index, 'Survived': nn_preds})\nnn_output.to_csv('nn_submission.csv', index=False)\nnn_output","6a9ec363":"The next cell displays the architecture of the best model (wich is ready for future predictions): type and number of hidden layers, with their shape (number of hidden units).","5e0b343b":"# Preparation","21dda424":"The rest of the preparation is somewhat trivial. But more interestingly, it is optionnal! Indeed, the dataset is clean and deep learning algorithms take care of preprocessing steps (including categorical encoding and missing values imputation).\n\nIn the following cells (outputs are hidden) I visualise tabular data, superficially search for outliers in numerical columns, evaluates the amount of missing values and informativeness of cateogical ones (essentialy through unique values count). Finally, I made a few choices:\n- Drop the \"Cabin\" variable because of to much NaN\n- Drop the \"Name\" variable because of almost *n* unique values and the common knowlegde telling us that the title (Mr., Mrs., Dr...) is closely related to the variables \"Sex\" and \"PClass\"\n- Drop the \"Ticket\" variable because of too much unique values and the fact that I don't know how to handle this feature easily and worthily","eaf55a99":"I faced a last problem when writing the submission file (it's never over...): as the model outputs a *numpy* array as predictions, you have to convert it in a *pandas* DataFrame (line 2). But, the dtype being float (0.0 if dead, 1.0 if alive), so Kaggle won't recognize it as a prediction (I had a really bad surprise). We can convert the float to an int thanks to the argument `dtype` (still line 2). At the end of line 2, `.iloc[:, 0]` allows us to convert the DataFrame into a Series (to build the final DataFrame...).\n\nAnd we've made it! Thank you again for reading this notebook :)","67547162":"# Modeling","a4501bb2":"The first issue with AutoKeras is to install it ---this notebook won't be particularly philosophical... This can be done using *pip* (see https:\/\/autokeras.com\/install\/). If you use Anaconda, I hope running the following cell in your local Notebook will be sufficient.","a5e7e6a0":"PS: after trying different values for `max_trials` and `epochs` (I made several versions of this notebook), I realized that there was a huge variability in the test accuracy (on Kaggle competition). So far, my best (and first) sub accuracy was 0.7799 (which ranked great: 3,595 on 50,293). This result decided me to write the notebook. The worst score (I can't tell how bad) occured when I tried to increase `max_trials` to 50 and `epochs` to 200. This is rather disappointing, especially since I don't know if it comes from a pure variability of the modeling procedure due to a small dataset, or to overfitting when setting these arguments to high values. The procedures of auto ML include model selection and tuning, so I'm pretty confident the second hypothesis doesn't hold. Any information about this would be so appreciated!","c84d1cec":"Modeling with AutoKeras is so easy and fast... The first time I ran it, I made my best submisison for the Titanic competition in a few minutes with a very modest computer (Intel i5 processor, NVIDIA GeForce MX 150, 4 GB of RAM). To make this so quick, I passed the arguments `max_trial` to the `StructuredDataClassifier()` and `epochs` to the fitting procedure with very low values (I don't remember exactly which ones). The default settings (if we omit the previous arguments) are 100 trials (I don't know if there is some kind of early stopping) and 1000 epochs (maximum: it stops after 30 epochs without improvement). See more at the official page of the classifier (https:\/\/autokeras.com\/structured_data_classifier\/). I obviously won't try it on my laptop.\n\nThe following cell specifies the model type (we want a classification model, designed for tabular data, with a maximum of 5 trials), and trains it with `.fit()`. There are several ways to provide data to the classifier (see the official tutorial: https:\/\/autokeras.com\/tutorial\/structured_data_classification\/): I chose to split X and y, but you can provide the whole dataset in the first argument and determine the target column in the second one.","be34c063":"Hello, thank you for reading this notebook! My goal here is to share my noob experience about using AutoKeras for the first time, with tips that came out after I encountered some very practical difficulties. I think this kind of easy procedures (automated ones) is very stimulating and makes you want to learn a lot more about the theory. Have fun!"}}