{"cell_type":{"f6da0d05":"code","ee1aaa42":"code","538ac289":"code","839aa0ab":"code","c05f7d49":"code","3327bfef":"code","0ba22fd2":"code","83beb554":"code","146cb030":"code","69601317":"code","4c58cb6c":"code","96b4a417":"code","f00024f9":"code","3ccf6d64":"code","fe074fb2":"code","2aab60a1":"code","8ea5d463":"code","565f895c":"code","62484e9e":"code","2d3d50c3":"code","0bc4490a":"code","98c5715d":"code","db7f9341":"code","f510815e":"code","fef92c7e":"code","38154206":"code","7af9e71b":"code","02f803a9":"code","2cc9a8ae":"code","795db1a4":"code","08b1609c":"code","25648b25":"code","880ce914":"code","33096ec2":"code","5fa202ab":"code","48fec3f2":"code","79764259":"code","42123aed":"code","c616f874":"code","d98502c9":"code","69a56dfb":"code","00a5a855":"code","682a5e7d":"code","56e14fb9":"code","e5f5805e":"code","d8eaf02f":"code","bc0616b6":"code","3e256eba":"code","3240fa60":"code","e0dba6ac":"code","3f77162e":"code","33b55b39":"code","89e98497":"code","8930f131":"code","0cb5cc21":"code","648de8f1":"code","760d91a3":"code","c3697a51":"code","c2fc5f37":"code","5a2bed4e":"code","e77f66ec":"code","3c81eb69":"code","4692c359":"code","d46e1e7f":"code","b87d9902":"code","85d316a2":"code","4f28b863":"code","4563dd71":"code","0d89fdf7":"code","1df7c0b9":"code","e96f9bbf":"markdown","149c7066":"markdown","d45fa16c":"markdown","2623e749":"markdown","0965aa2c":"markdown","c580ba80":"markdown","403f9f40":"markdown","d01bca97":"markdown","050518cf":"markdown","8fbabc23":"markdown","cf1d26b4":"markdown","c7dcead6":"markdown","d76da3cd":"markdown","6063a1af":"markdown","fad22616":"markdown","82ef0416":"markdown","70bbf100":"markdown","5f4f127e":"markdown","638b29d4":"markdown","0baa7bda":"markdown","f4b60903":"markdown","8dbd8866":"markdown","a499d80f":"markdown","468a77e1":"markdown","883807a5":"markdown","436d4bfa":"markdown","bbfad44b":"markdown","3e71221d":"markdown","e25c9f37":"markdown","f0260ff4":"markdown","1739980b":"markdown","55034a51":"markdown","2f170ff2":"markdown","0d38e39b":"markdown","e4d57446":"markdown","0381b389":"markdown","13903cc3":"markdown","446ae520":"markdown","bf729496":"markdown","e7efe074":"markdown","ab1af7f7":"markdown","99e85401":"markdown","6af09737":"markdown","620f9d18":"markdown","af30f6c6":"markdown","6bad4985":"markdown","103911d6":"markdown","5f2739e8":"markdown","c097a640":"markdown","8415b5b1":"markdown","d01dc737":"markdown","5b78b453":"markdown","83b5113d":"markdown"},"source":{"f6da0d05":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport sklearn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom collections import Counter\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier)\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\n\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n\n\nfrom sklearn.metrics import accuracy_score, log_loss\n\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\nfrom mlxtend.classifier import EnsembleVoteClassifier\n\n\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n","ee1aaa42":"pd.set_option('max_columns', 50)\npd.options.display.max_colwidth = 200\n","538ac289":"# Load in the train and test datasets\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Store our passenger ID for easy access\nPassengerId = test['PassengerId'].astype(\"object\")\n\n## Join train and test datasets in order to obtain the same number of features during categorical conversion\ntrain_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\n\n# Preview the data\ntrain.head()","839aa0ab":"train.info()\n\nprint(\"*\"*40)\n\ntest.info()","c05f7d49":"train.describe()","3327bfef":"train.describe(include=['O'])\n","0ba22fd2":"plot1 = sns.barplot(x=\"Pclass\" , y=\"Survived\" , data = train)\nplt.ylim(0,1)\nfor p in plot1.patches:\n    plot1.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2.,0.1 + p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.title(\"Percentage of surviving for the three classes\");","83beb554":"plot2 = sns.barplot(x = \"Sex\" , y = \"Survived\" , data = train)\nplt.ylim(0,1)\nfor p in plot2.patches:\n    plot2.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2., 0.1 + p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.title(\"Percentage of surviving for males and females\");","146cb030":"plot3 = sns.barplot(x = \"SibSp\", y = \"Survived\",data = train , errwidth= 0)\n\nplt.ylim(0,1)\nfor p in plot3.patches:\n    plot3.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2., + p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.title(\"Percentage of surviving by number of siblings \/ spouses aboard the titanic\\n\");","69601317":"plot4 = sns.barplot(x = \"Parch\" , y = \"Survived\" , data = train , errwidth = 0)\n\nplt.ylim(0,0.7)\nplt.yticks([0.1,0.2,0.3,0.4,0.5,0.6])\nfor p in plot4.patches:\n    plot4.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.title(\"Percentage of surviving by number of parents \/ children aboard the titanic\")\n","4c58cb6c":"train[\"Embarked\"].value_counts()","96b4a417":"plot4 = sns.barplot(x = \"Embarked\" , y = \"Survived\" , data = train , errwidth = 0)\n\nplt.ylim(0,0.7)\nplt.yticks([0.1,0.2,0.3,0.4,0.5,0.6])\nfor p in plot4.patches:\n    plot4.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.title(\"Percentage of surviving by number of parents \/ children aboard the titanic\");\n","f00024f9":"sns.distplot(train[\"Fare\"], label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\nplt.legend(loc=\"best\");","3ccf6d64":"# Explore Age vs Survived\ng = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")\n\n","fe074fb2":"# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","2aab60a1":"f,ax=plt.subplots(1,2,figsize=(12,5))\ntrain['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=train,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","8ea5d463":"g = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","565f895c":"g = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train,\n                   size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","62484e9e":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.drop(\"PassengerId\",axis =1, inplace =False).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True);","2d3d50c3":"dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nplot5 = sns.barplot(x = \"FamilySize\" , y = \"Survived\" , data = dataset[:train_len] ,errwidth =0)\n\nplt.ylim(0,1)\nfor p in plot5.patches:\n    plot5.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.title(\"Percentage of surviving by family size\");","0bc4490a":"# Create new features of family size\ndataset['Single'] = dataset['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","98c5715d":"g = sns.catplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","db7f9341":"dataset['Embarked'] = dataset['Embarked'].fillna('S')","f510815e":"# Apply log to Fare to reduce skewness distribution\ndataset[\"LogFare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\ng = sns.distplot(dataset[:train_len][\"LogFare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"LogFare\"].skew()))\ng = g.legend(loc=\"best\")","fef92c7e":"dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n\ndataset['CategoricalFare'] = dataset[\"Fare\"].copy()\n\n# Mapping Fare\ndataset.loc[ dataset['CategoricalFare'] <= 7.91, 'CategoricalFare']                               = 0\ndataset.loc[(dataset['CategoricalFare'] > 7.91) & (dataset['CategoricalFare'] <= 14.454), 'CategoricalFare'] = 1\ndataset.loc[(dataset['CategoricalFare'] > 14.454) & (dataset['CategoricalFare'] <= 31), 'CategoricalFare']   = 2\ndataset.loc[ dataset['CategoricalFare'] > 31, 'CategoricalFare']                                  = 3\ndataset['CategoricalFare'] = dataset['CategoricalFare'].astype(int)\n\nplot7 = sns.barplot(x= \"CategoricalFare\" , y = \"Survived\" , data =dataset[:train_len] ,errwidth =0)\n\nplt.ylim(0,1)\nfor p in plot7.patches:\n    plot7.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\nplt.title(\"Percentage of surviving for each categorical Fare\");","38154206":"# convert Sex into categorical value 0 for male and 1 for female\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})","7af9e71b":"# Explore Age vs Sex, Parch , Pclass and SibSP\ng = sns.catplot(y=\"Age\",x=\"Sex\",data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"Parch\", data=dataset,kind=\"box\")\ng = sns.catplot(y=\"Age\",x=\"SibSp\", data=dataset,kind=\"box\")","02f803a9":"g = sns.heatmap(dataset[:train_len][[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)","2cc9a8ae":"dataset[\"Age1\"] = dataset[\"Age\"].copy()\n\n# Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(dataset[\"Age1\"][dataset[\"Age1\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[:train_len][\"Age1\"].median()\n    age_pred = dataset[\"Age1\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age1'].iloc[i] = age_pred\n    else :\n        dataset['Age1'].iloc[i] = age_med\n\n","795db1a4":"age_avg    = train['Age'].mean()\nage_std    = train['Age'].std()\nage_null_count = dataset['Age'].isnull().sum()\n    \nage_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\ndataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\ndataset['Age'] = dataset['Age'].astype(int)\n","08b1609c":"    \ndataset['CategoricalAge'] = pd.cut(dataset['Age'], 5)\n# Or\n\ndataset['CategoricalAge'] = dataset[\"Age\"].copy()\n\n\n# Mapping Age\ndataset.loc[ dataset['CategoricalAge'] <= 16, 'CategoricalAge']                          = 0\ndataset.loc[(dataset['CategoricalAge'] > 16) & (dataset['CategoricalAge'] <= 32), 'CategoricalAge'] = 1\ndataset.loc[(dataset['CategoricalAge'] > 32) & (dataset['CategoricalAge'] <= 48), 'CategoricalAge'] = 2\ndataset.loc[(dataset['CategoricalAge'] > 48) & (dataset['CategoricalAge'] <= 64), 'CategoricalAge'] = 3\ndataset.loc[ dataset['CategoricalAge'] > 64, 'CategoricalAge']                            = 4","25648b25":"# Get Title from Name\ndataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)","880ce914":"g = sns.countplot(x=\"Title\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45) ","33096ec2":"# Replace Rare Titles \ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset['Title'] = dataset['Title'].replace(['Mlle','Ms','Mme','Miss','Mrs'], 'Mrs-Miss')\n","5fa202ab":"g = sns.countplot(dataset[\"Title\"])\nplt.ylim(0,850)\n\nfor p in g.patches:\n    g.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')","48fec3f2":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","79764259":"# Drop Name variable\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","42123aed":"# Mapping titles\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndataset['CatTitle'] = dataset['Title'].map(title_mapping)\ndataset['CatTitle'] = dataset['CatTitle'].fillna(0).astype(int)","c616f874":"# Replace the Cabin number by the type of cabin 'X' if not\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","d98502c9":"g = sns.countplot(dataset[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","69a56dfb":"g = sns.catplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")\n","00a5a855":"dataset[\"Ticket\"].head()","682a5e7d":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\n","56e14fb9":"dataset.head()","e5f5805e":"dataset = pd.get_dummies(dataset, columns = [\"Title\"],drop_first = True)\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\",drop_first = True)\ndataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\",drop_first = True)\ndataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\",drop_first = True)\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\",drop_first = True)","d8eaf02f":"# Drop useless variables \ndataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","bc0616b6":"dataset.head()","3e256eba":"dataset.columns.values","3240fa60":"DF_train = dataset[:train_len]\nDF_test = dataset[train_len:]\n\nDF_test.drop(labels=[\"Survived\"],axis = 1,inplace=True)\n\nY = DF_train[\"Survived\"].astype(int)\n\n\nDF_train.drop(labels=[\"Survived\"],axis = 1,inplace=True)","e0dba6ac":"Train0 = DF_train.copy()\nTest0  = DF_test.copy()\n\nCol1 = [ 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'FamilySize',\n                   'CatTitle','Single', 'SmallF', 'MedF', 'LargeF', 'Em_Q', 'Em_S', 'Cabin_B', 'Cabin_C', 'Cabin_D',\n                   'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_X', 'T_A4',\n                   'T_A5', 'T_AQ3', 'T_AQ4', 'T_AS', 'T_C', 'T_CA', 'T_CASOTON',\n                   'T_FC', 'T_FCC', 'T_Fa', 'T_LINE', 'T_LP', 'T_PC', 'T_PP', 'T_PPP',\n                   'T_SC', 'T_SCA3', 'T_SCA4', 'T_SCAH', 'T_SCOW', 'T_SCPARIS',\n                   'T_SCParis', 'T_SOC', 'T_SOP', 'T_SOPP', 'T_SOTONO2', 'T_SOTONOQ',\n                   'T_SP', 'T_STONO', 'T_STONO2', 'T_STONOQ', 'T_SWPP', 'T_WC',\n                   'T_WEP', 'T_X', 'Pc_2', 'Pc_3']\n\nTrain1 = DF_train[Col1]\nTest1  = DF_test[Col1]\n\nCol2 = [ 'Sex', 'SibSp', 'Parch', 'FamilySize',\n                   'LogFare',  'Age1', \n                   'Single', 'SmallF', 'MedF', 'LargeF', 'Title_Mr', 'Title_Mrs-Miss',\n                   'Title_Rare', 'Em_Q', 'Em_S', 'Cabin_B', 'Cabin_C', 'Cabin_D',\n                   'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_X', 'T_A4',\n                   'T_A5', 'T_AQ3', 'T_AQ4', 'T_AS', 'T_C', 'T_CA', 'T_CASOTON',\n                   'T_FC', 'T_FCC', 'T_Fa', 'T_LINE', 'T_LP', 'T_PC', 'T_PP', 'T_PPP',\n                   'T_SC', 'T_SCA3', 'T_SCA4', 'T_SCAH', 'T_SCOW', 'T_SCPARIS',\n                   'T_SCParis', 'T_SOC', 'T_SOP', 'T_SOPP', 'T_SOTONO2', 'T_SOTONOQ',\n                   'T_SP', 'T_STONO', 'T_STONO2', 'T_STONOQ', 'T_SWPP', 'T_WC',\n                   'T_WEP', 'T_X', 'Pc_2', 'Pc_3']\n\nTrain2 = DF_train[Col2]\nTest2  = DF_test[Col2]\n\n\nCol3  =  [ 'Sex', 'SibSp', 'Parch', 'FamilySize',\n                    'CategoricalFare', 'CategoricalAge', \n                   'Single', 'SmallF', 'MedF', 'LargeF', 'Title_Mr', 'Title_Mrs-Miss',\n                   'Title_Rare', 'Em_Q', 'Em_S', 'Cabin_B', 'Cabin_C', 'Cabin_D',\n                   'Cabin_E', 'Cabin_F', 'Cabin_G', 'Cabin_T', 'Cabin_X', 'T_A4',\n                   'T_A5', 'T_AQ3', 'T_AQ4', 'T_AS', 'T_C', 'T_CA', 'T_CASOTON',\n                   'T_FC', 'T_FCC', 'T_Fa', 'T_LINE', 'T_LP', 'T_PC', 'T_PP', 'T_PPP',\n                   'T_SC', 'T_SCA3', 'T_SCA4', 'T_SCAH', 'T_SCOW', 'T_SCPARIS',\n                   'T_SCParis', 'T_SOC', 'T_SOP', 'T_SOPP', 'T_SOTONO2', 'T_SOTONOQ',\n                   'T_SP', 'T_STONO', 'T_STONO2', 'T_STONOQ', 'T_SWPP', 'T_WC',\n                   'T_WEP', 'T_X', 'Pc_2', 'Pc_3']\n\nTrain3 = DF_train[Col3]\nTest3  = DF_test[Col3]\n\nTrains = [Train0 , Train1 , Train2, Train3]\n","3f77162e":"Train1.shape , Train2.shape , Train3.shape","33b55b39":"classifiers = [\n    KNeighborsClassifier(),\n    SVC(gamma=0.1,probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    ExtraTreesClassifier(),\n    GaussianNB(),\n    XGBClassifier(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression(),\n    Perceptron(),\n    MLPClassifier(activation=\"logistic\"),\n    LGBMClassifier()]\n\n# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","89e98497":"cv_results = []\nNames=[]\nfor classifier in classifiers :\n    Names.append(classifier.__class__.__name__)\n    cv_results.append(cross_val_score(classifier, Train0, y = Y, scoring = \"accuracy\", cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":Names}).sort_values(by = 'CrossValMeans')\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\n","8930f131":"cv_results = []\nNames=[]\nfor classifier in classifiers :\n    Names.append(classifier.__class__.__name__)\n    cv_results.append(cross_val_score(classifier, Train1, y = Y, scoring = \"accuracy\", cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":Names}).sort_values(by = 'CrossValMeans')\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","0cb5cc21":"cv_results = []\nNames=[]\nfor classifier in classifiers :\n    Names.append(classifier.__class__.__name__)\n    cv_results.append(cross_val_score(classifier, Train2, y = Y, scoring = \"accuracy\", cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":Names}).sort_values(by = 'CrossValMeans')\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","648de8f1":"cv_results = []\nNames=[]\nfor classifier in classifiers :\n    Names.append(classifier.__class__.__name__)\n    cv_results.append(cross_val_score(classifier, Train3, y = Y, scoring = \"accuracy\", cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":Names}).sort_values(by = 'CrossValMeans')\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","760d91a3":"SVC_par = [\n          {\"kernel\":[\"linear\"]},\n          {\"kernel\":[\"rbf\"],\"C\":[1, 10, 50, 100,200,300, 1000],\"gamma\":[0.001,0.01,0.1,1]},\n          {\"kernel\":[\"poly\"],\"degree\":[2,3,4,5]}\n         ]","c3697a51":"SVC_Grid= GridSearchCV(SVC(),SVC_par,scoring=\"accuracy\",cv=10,n_jobs=-1)\nSVC_Grid.fit(Train2,Y)\nSVC_scores = pd.DataFrame(SVC_Grid.cv_results_)\n\nSVC_scores.sort_values(by = \"rank_test_score\")[[\"params\",\"rank_test_score\",\"rank_test_score\",\"mean_test_score\",\"std_test_score\"]].head(10)","c2fc5f37":"GB_par = {  'n_estimators' : [100,200,300],\n            'learning_rate': [0.2, 0.1, 0.05, 0.01],\n            'max_depth': [4, 8],\n            'min_samples_leaf': [50,100,150],\n            'max_features': [0.5,0.3, 0.1],\n            'min_samples_split':[2,5,10]\n              }","5a2bed4e":"GB_Grid= GridSearchCV(GradientBoostingClassifier(),GB_par,scoring=\"accuracy\",cv=10,n_jobs=-1)\nGB_Grid.fit(Train2,Y)\nGB_scores = pd.DataFrame(GB_Grid.cv_results_)\n\nGB_scores.sort_values(by = \"rank_test_score\")[[\"params\",\"rank_test_score\",\"rank_test_score\",\"mean_test_score\",\"std_test_score\"]].head(10)","e77f66ec":"LGBM_par = { 'boosting_type':['gbdt','dart','goss','rf'],\n            'learning_rate':[0.1, 0.2, 0.5, 0.01],\n            'n_estimators':[100, 200, 300],\n            'objective':[\"binary\"]  \n    \n}","3c81eb69":"LGBM_Grid= GridSearchCV(LGBMClassifier(),LGBM_par,scoring=\"accuracy\",cv=10,n_jobs=-1)\nLGBM_Grid.fit(Train2,Y)\nLGBM_scores = pd.DataFrame(LGBM_Grid.cv_results_)\n\nLGBM_scores.sort_values(by = \"rank_test_score\")[[\"params\",\"rank_test_score\",\"rank_test_score\",\"mean_test_score\",\"std_test_score\"]].head(10)","4692c359":"LR_par = {'penalty':['l1', 'l2', 'elasticnet', 'none'],\n          'C':[0.5,1,2,5,10,100],\n          'fit_intercept':[True , False],\n          'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n          'max_iter':[100,200,300]\n \n}","d46e1e7f":"LR_Grid= GridSearchCV(LogisticRegression(),LR_par,scoring=\"accuracy\",cv=10,n_jobs=-1)\nLR_Grid.fit(Train2,Y)\nLR_scores = pd.DataFrame(LR_Grid.cv_results_)\n\nLR_scores.sort_values(by = \"rank_test_score\")[[\"params\",\"rank_test_score\",\"rank_test_score\",\"mean_test_score\",\"std_test_score\"]].head(10)","b87d9902":"LDA_par = {'solver':['svd', 'lsqr']}","85d316a2":"LDA_Grid= GridSearchCV(LinearDiscriminantAnalysis(),LDA_par,scoring=\"accuracy\",cv=10,n_jobs=-1)\nLDA_Grid.fit(Train2,Y)\nLDA_scores = pd.DataFrame(LDA_Grid.cv_results_)\n\nLDA_scores.sort_values(by = \"rank_test_score\")[[\"params\",\"rank_test_score\",\"rank_test_score\",\"mean_test_score\",\"std_test_score\"]].head(10)","4f28b863":"XGB_par = {'n_estimators':[50, 100, 200],\n           'max_depth':[4, 8, 10],\n           'learning_rate':[0.1, 0.2, 0.5, 0.01],\n  \n}","4563dd71":"XGB_Grid= GridSearchCV(XGBClassifier(),XGB_par,scoring=\"accuracy\",cv=10,n_jobs=-1)\nXGB_Grid.fit(Train2,Y)\nXGB_scores = pd.DataFrame(XGB_Grid.cv_results_)\n\nXGB_scores.sort_values(by = \"rank_test_score\")[[\"params\",\"rank_test_score\",\"rank_test_score\",\"mean_test_score\",\"std_test_score\"]].head(10)","0d89fdf7":"XGB = XGB_Grid.best_estimator_\nLDA = LDA_Grid.best_estimator_\nSVC = SVC_Grid.best_estimator_\nLR = LR_Grid.best_estimator_\nLGBM = LGBM_Grid.best_estimator_\nGB = GB_Grid.best_estimator_\n\nvotingC = VotingClassifier(estimators=[('XGB', XGB),\n                                       ('LDA', LDA),\n                                       ('SVC', SVC),\n                                       ('LR', LR),\n                                       ('LGBM', LGBM),\n                                       ('GB', GB)],\n                           voting='hard', n_jobs=-1)\n\n\nvotingC_score = cross_val_score(votingC, Train2, y = Y, scoring = \"accuracy\", cv = 10, n_jobs=-1).mean()\n\nvotingC_score","1df7c0b9":"votingC.fit(Train2, Y)\n\nPred_Survived = pd.Series(votingC.predict(Test2), name=\"Survived\")\n\nresults = pd.concat([PassengerId,Pred_Survived],axis=1)\n\nresults.to_csv(\"Titanic Ensemble Voting.csv\",index=False)","e96f9bbf":"We have fitted all the classifiers with the four cobinaisons of variables, and the first think to consider is that the best classifiers that perform better than others are: Gradient Boosting, SVC, LGBM, Logistic Regression, XGBoost and Linear Discriminant Analysis. For the best combinaison of varibles i choose to keep the second one that contain the LogFare and the Age imputed with the median age of similar rows according to Pclass, Parch and SibSp.","149c7066":"We can also we categorize the variable into 5 equal ranges using the function \"cut\" or manualy.","d45fa16c":"The passenger who has 1 or 2 siblings \/ spouses aboard the titanic is most likely to survive, and the passenger who has 5 or 8 siblings \/ spouses aboard the titanic is most likely to not survice the sink.","2623e749":"1. Types of our varibles:\n\n\nCategorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n\nContinous: Age, Fare. Discrete: SibSp, Parch.\n\n\n2. Null and Missing values:\n\n* Cabin , Age , Embarked features contain a number of null values in that order for the training dataset.\n* Cabin , Age , Fare are incomplete in case of test dataset.","0965aa2c":"### 7. Cabin:\n\nThe Cabin feature column contains 292 values and 1007 missing values.\n\nI supposed that passengers without a cabin have a missing value displayed instead of the cabin number.\nSo we will impute all the missing values with new cabin categorie \"X\". For the rest of cabins, the first letter  indicates the Desk, i choosed to keep this information only, since it indicates the probable location of the passenger in the Titanic.","c580ba80":"#### What is the distribution of numerical feature values across the samples?\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n* Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n* Survived is a categorical feature with 0 or 1 values.\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Most passengers (> 75%) did not travel with parents or children.\n* Nearly 30% of the passengers had siblings and\/or spouse aboard.\n* Fares varied significantly with few passengers (<1%) paying as high as $512.\n* Few elderly passengers (< 1%) within age range 65-80.","403f9f40":"# 1. Import Libraries","d01bca97":"We can notice that's there is a relationship between the class variable and the target variable; as we increase the class, the chance of survived decrease.\n\n### 2. Sex\nIt's a categorical variable with two categories, male and female. According to the below table, it's that the chance of serviving for females (about 75%) is much higher than the chance for surviving for males (about 19%).","050518cf":"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is.","8fbabc23":"### 6. Age\nIt's a continuous variable represent the age of the passenger by years. The variable has a lot of missing values that's will be imputed in data preprocessing phase using the values between \"mean - std\" and \"mean + std\", then we categorize the variable into 5 equal ranges using the function \"cut\".","cf1d26b4":"Age distribution seems to be a tailed distribution, maybe a gaussian distribution.\n\nWe notice that age distributions are not the same in the survived and not survived subpopulations. Indeed, there is a peak corresponding to young passengers, that have survived. We also see that passengers between 60-80 have less survived. \n\nSo, even if \"Age\" is not correlated with \"Survived\", we can see that there is age categories of passengers that of have more or less chance to survive.\n\nIt seems that very young passengers have more chance to survive.","c7dcead6":"### 6. Name \/ Title:\n\nThe Name feature contains information on passenger's title.\n\nSince some passenger with distingused title may be preferred during the evacuation, it is interesting to add them to the model.","d76da3cd":"#### 2. Gradient Boosting:","6063a1af":"### 4. Sex:","fad22616":"## 3.  Hyperparameter tunning for best models:\n#### 1. SVC:","82ef0416":"\"Women and children first\" \n\n","70bbf100":"### 8. Ticket:","5f4f127e":"As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled. \n\nIn this case, it is better to transform it with the log function to reduce this skew (In data preprocessing phase). ","638b29d4":"#### 4. Logistic Regression","0baa7bda":"#### 6. XGBoost:","f4b60903":"# 2. Import Data","8dbd8866":"Because of the low number of passenger that have a cabin, survival probabilities have an important standard deviation and we can't distinguish between survival probability of passengers in the different desks. \n\nBut we can see that passengers with a cabin have generally more chance to survive than passengers without (X).\n\nIt is particularly true for cabin B, C, D, E and F.","a499d80f":"The passenger who has 1 to 3 parents \/ children aboard the titanic is most likely to survive, and the passenger who has 4 or 6 parents \/ children aboard the titanic is most likely to not survice the sink. But in general there is no significant pattern.\n\n### 4. Embarked\nIt's a categorical variable represent Port of Embarkation for each passenger, C = Cherbourg, Q = Queenstown, S = Southampton.","468a77e1":"# 5. Modeling:\n\nSome variables in our new dataset represent the same feature but with different imputation or type. So we will use different combinaison of variables to choose the best ones for deployment.\n\n## Prepare the Data:","883807a5":"The passenger survival is not the same in the 3 classes. First class passengers have more chance to survive than second class and third class passengers.\n\nThis trend is conserved when we look at both male and female passengers.\n\n### 9. Pclass vs Embarked:","436d4bfa":"Skewness is clearly reduced after the log transformation.\n\nWe can also use the function \"qcut\" to get 4 categories for Fare values with the same number of passengers in each categorie, and we put this new values in new variable called \"CategoricalFare\" that's may help us in the modelisation (or we can do the cut manualy better). ","bbfad44b":"It could mean that tickets sharing the same prefixes could be booked for cabins placed together. It could therefore lead to the actual placement of the cabins within the ship.\n\nTickets with same prefixes may have a similar class and survival.\n\nSo i decided to replace the Ticket feature column by the ticket prefixe. Which may be more informative.","3e71221d":"### 7. Survived","e25c9f37":"Indeed, the third class is the most frequent for passenger coming from Southampton (S) and Queenstown (Q), whereas Cherbourg passengers are mostly in first class which have the highest survival rate.\n\n### Correlation:","f0260ff4":"### 1. FamilySize\n\nWith this two variables, we can create another variables that's can help us understand our data and also medeling the data effectively. The first variable to create is the Family size for each passenger, it's a discrete variable and it's simply the number of siblings \/ spouses + the number of parents \/ children + 1 for the passenger himself.\n","1739980b":"#### What is the distribution of categorical features?\n\n* Names are unique across the dataset (count=unique=891)\n* Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n* Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n* Embarked takes three possible values. S port used by most passengers (top=S)\n* Ticket feature has high ratio (22%) of duplicate values (unique=681).","55034a51":"The correlation map confirms the factorplots observations except for Parch. Age is not correlated with Sex, but is negatively correlated with Pclass, Parch and SibSp.\n\nIn the plot of Age in function of Parch, Age is growing with the number of parents \/ children. But the general correlation is negative.\n\nSo, i decided to use SibSP, Parch and Pclass in order to impute the missing ages.\n\nThe strategy is to fill Age with the median age of similar rows according to Pclass, Parch and SibSp.","2f170ff2":"### Dummy variables:","0d38e39b":"### Ensemble modeling:","e4d57446":"### 5. Age:\n\nAge variable has many missing values, so we must impute them because it's an important feature and we can't just drop it.\nWe will use two approches to impute the age values, and we will decide the better one after modeling. \n\nThe first one consist to use the most correlated features to Age variable to predict the missing values. So first of all we will Explore Age vs Sex, Parch , Pclass and SibSP to see if there are significant correlations.\n","0381b389":"# 6. Prediction","13903cc3":"#### 5. Linear Discriminant Analysis:","446ae520":"When we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) to babies and very young childrens.","bf729496":"# 4. Feature Engineering & Data Preprocessing:","e7efe074":"### 3. SibSp and Parch\nSibsp represent the number of siblings \/ spouses aboard the Titanic, and Parch represent the number of parents \/ children aboard the Titanic. the two variables are quantitative discrete with the following distribution:","ab1af7f7":"### 8. Pclass vs Survived by Sex:","99e85401":"Factorplots of family size categories show that Small and Medium families have more chance to survive than single passenger and large families.\n\n### 2. Embarked:\n\nwe replace the null values for the \"Embarked\" with the modal category \"S\" in both train and test data.","6af09737":"It's seems like the passengers whom used the port C for embarkation have the high percentage of surviving.\n\n### 5. Fare\nIt's a continuous variable represent the Passenger fare with no missing values in the train dataset, but with missing values in the test dataset. we have to visualize the distribution of the Fare variable to decide if we will impute the missing values with a \"mean\" or with a \"median\".","620f9d18":"### 1. Pclass\n\nPclass represent the ticket class for the passenger. It's represented as ordinal variable takes values from 1 to 3 with no null values. From the results below we can conclude that the first class (class 1) has the higher percentage among all the three classes for the rate of survived passengers.","af30f6c6":"# Introduction\n\nKnowing from a training set of samples listing passengers who survived or did not survive the Titanic disaster, can our model determine based on a given test dataset not containing the survival information, if these passengers in the test dataset survived or not.\n\n* On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Translated 32% survival rate.\n* One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.\n* Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n\n## Table of contents:\n\n1. Import Libraries\n2. Data import & info\n3. Exploratory Data Analysis\n4. Data preprocessing & Feature Engineering\n5. Modeling\n6. Prediction","6bad4985":"# 3. EDA","103911d6":"There is no significant correlation between any two features in train dataset.","5f2739e8":"The second approche is to impute using the values between \"mean - std\" and \"mean + std\".","c097a640":"### 3. Fare:\n\nWe use the log transformation to get less skewed data.","8415b5b1":"There is 17 titles in the dataset, most of them are very rare and we can group them in 4 categories.","d01dc737":"## 2. Cross Validation:\n","5b78b453":"good! now we have a clean dataset and ready to predict. let's find which classifier works better on this dataset.\n\n## 1. Simple Modeling:\n\nNow we are ready to train a model and predict the required solution. There are a lot of predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a selected few models which we can evaluate. Our problem is a classification problem, we want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria, we can narrow down our choice of models to a few. These include:\n\n* Logistic Regression\n* XGBoost\n* AdaBoost\n* Gradient Boosting\n* Linear Discriminant Analysis\n* Logistic Regression\n* KNN or k-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* Perceptron\n* Artificial neural network\n","83b5113d":"#### 3. LGBM:"}}