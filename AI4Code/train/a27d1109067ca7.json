{"cell_type":{"9f88de88":"code","4b3aa8e7":"code","d980a6e3":"code","bb160fd1":"code","c3b9312f":"code","6d2a99fa":"code","3c20ad1d":"code","2c294d44":"code","293d8aa8":"code","c38bba1c":"code","de2ddfd0":"code","150c6109":"code","2d269245":"code","886fa114":"code","f2f42b40":"code","f75dc3bc":"code","ca661f34":"code","c1f39fb5":"code","1fc4202a":"code","b27970a2":"code","a1aabbda":"code","640a3282":"code","636a4ae7":"code","54bb05c1":"code","8544d15a":"code","b58be809":"code","658f0e28":"code","3b0cd906":"code","40dc6272":"code","64f23772":"code","b44e7ad6":"code","6ec496fc":"code","3d5f7307":"code","ec7678b0":"code","37f33c5d":"markdown"},"source":{"9f88de88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4b3aa8e7":"# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM , Bidirectional\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\nimport gc\nimport json\nfrom keras_preprocessing.text import tokenizer_from_json\nfrom keras.models import model_from_json\n\npd.set_option('max_colwidth', 500)\npd.set_option('max_columns', 500)\npd.set_option('max_rows', 100)\n# Set log\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","d980a6e3":"alexa = pd.read_csv('\/kaggle\/input\/amazon-alexa-reviews\/amazon_alexa.tsv' , delimiter = '\\t' \n                    ,usecols = ['verified_reviews' , 'feedback'] )","bb160fd1":"alexa = alexa.rename(columns={'verified_reviews':'reviews', 'feedback':'sentiment'})\ndisplay(alexa['sentiment'].value_counts()\/alexa.shape[0]*100)\nprint('Shape of Dataset -> ' , alexa.shape)\ndisplay(alexa.sample(6))","c3b9312f":"twitter = pd.read_csv('..\/input\/twitter-sentiment\/Sentiment Analysis Dataset 2.csv', skiprows=[8835,535881] , usecols = ['Sentiment' , 'SentimentText'])\ntwitter = twitter.rename(columns = {'Sentiment': 'sentiment' , 'SentimentText':'reviews'})\ndisplay(twitter['sentiment'].value_counts()\/twitter.shape[0]*100)\nprint('Shape of Dataset -> ' , twitter.shape)\ndisplay(twitter.sample(6))","6d2a99fa":"path = \"\/kaggle\/input\/imdb-movie-reviews-dataset\/aclimdb\/aclImdb\/\"\npositiveFiles = [x for x in os.listdir(path+\"train\/pos\/\") if x.endswith(\".txt\")]\nnegativeFiles = [x for x in os.listdir(path+\"train\/neg\/\") if x.endswith(\".txt\")]\npositiveReviews, negativeReviews = [], []\nfor pfile in positiveFiles:\n    with open(path+\"train\/pos\/\"+pfile, encoding=\"latin1\") as f:\n        positiveReviews.append(f.read())\nfor nfile in negativeFiles:\n    with open(path+\"train\/neg\/\"+nfile, encoding=\"latin1\") as f:\n        negativeReviews.append(f.read())\n\nimdb = pd.concat([\n    pd.DataFrame({\"reviews\":positiveReviews, \"sentiment\":1}),\n    pd.DataFrame({\"reviews\":negativeReviews, \"sentiment\":0}),\n], ignore_index=True).sample(frac=1, random_state=1)\n\n","3c20ad1d":"display(imdb['sentiment'].value_counts()\/imdb.shape[0]*100)\nprint('Shape of Dataset -> ' , imdb.shape)\ndisplay(imdb.sample(6))","2c294d44":"data = pd.concat([alexa, twitter , imdb], axis= 0)\ndel alexa , twitter , imdb\ngc.collect()","293d8aa8":"print(data.shape)\ndisplay(data.sample(5))\ndata['sentiment'].value_counts()\/data.shape[0]*100\n","c38bba1c":"data = data.sample(frac= 0.10 , random_state = 10)","de2ddfd0":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\ndef preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","150c6109":"%%time\ndata.reviews = data.reviews.apply(lambda x: preprocess(x))","2d269245":"data.head(5)","886fa114":"%%time\ndocuments = [_text.split() for _text in data.reviews] ","f2f42b40":"W2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\nw2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n                                            window=W2V_WINDOW, \n                                            min_count=W2V_MIN_COUNT, \n                                            workers=8)\n\n\nw2v_model.build_vocab(documents)\n","f75dc3bc":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","ca661f34":"%%time\nw2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)","c1f39fb5":"w2v_model.most_similar(\"awesome\")","1fc4202a":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(data.reviews)\nvocab_size = len(tokenizer.word_index)+1\nprint('Vocab Size is ',vocab_size)","b27970a2":"SEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024","a1aabbda":"%%time \nx_data = pad_sequences(tokenizer.texts_to_sequences(data.reviews) , maxlen = SEQUENCE_LENGTH)","640a3282":"y_data = data.sentiment\nprint(x_data.shape)\nprint(y_data.shape)\ny_data = y_data.values.reshape(-1,1)","636a4ae7":"w2v_model.wv['sample'].shape","54bb05c1":"embedding_matrix = np.zeros((vocab_size , W2V_SIZE))\nfor word , i in tokenizer.word_index.items():\n    if word in w2v_model.wv:\n        embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","8544d15a":"embedding_layer = Embedding( vocab_size , W2V_SIZE , weights = [embedding_matrix] , input_length = SEQUENCE_LENGTH, trainable = False)\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100 , dropout = 0.2 , recurrent_dropout = 0.2 ))\nmodel.add(Dense(1 , activation = 'sigmoid'))\nmodel.summary()","b58be809":"model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\ncallbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n# ReduceLRonPlateau is to reduce Learning rate when model stopeed improving\n# Early Stopping to stop learning when staturation is reached.","658f0e28":"history = model.fit(x_data , y_data , batch_size = BATCH_SIZE , epochs = EPOCHS , validation_split = 0.1  , verbose = 1 , callbacks = callbacks)","3b0cd906":"def predict(text):\n    start_at = time.time()\n    # Tokenize text\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    # Predict\n    score = model.predict([x_test])[0]\n\n    return {\"score\": float(score),\n       \"elapsed_time\": time.time()-start_at}  ","40dc6272":"print(predict('i am Happy'))\nprint(predict('i not feeling so great .Little Rest can help but you decide what should i do next '))\nprint(predict('i am sitting in library for 6 hours . i learned alot but i am tired'))\nprint(predict('i am tired'))\nprint(predict('good is not good'))\nprint(predict('bad is not good'))\nprint(predict('good is not bad'))\nprint(predict('how i can end up here'))","64f23772":"model.save_weights('model_weights_1.h5')\nwith open('model_architecture_1.json', 'w') as f:\n    f.write(model.to_json())\n    \nmodel.save('entire_model_1.h5')\ntokenizer_json = tokenizer.to_json()\nwith open('tokenizer_1.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))","b44e7ad6":"embedding_layer = Embedding( vocab_size , W2V_SIZE , weights = [embedding_matrix] , input_length = SEQUENCE_LENGTH, trainable = False)\nmodel = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(200 , dropout = 0.2 , recurrent_dropout = 0.2 )))\nmodel.add(Dense(1 , activation = 'sigmoid'))\nmodel.summary()","6ec496fc":"model.compile(loss = 'binary_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'] )\ncallbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]","3d5f7307":"history = model.fit(x_data , y_data , batch_size = BATCH_SIZE , epochs = EPOCHS , validation_split = 0.1  , verbose = 1 , callbacks = callbacks)","ec7678b0":"model.save_weights('model_weights.h5')\nwith open('model_architecture.json', 'w') as f:\n    f.write(model.to_json())\n    \nmodel.save('entire_model.h5')\ntokenizer_json = tokenizer.to_json()\nwith open('tokenizer.json', 'w', encoding='utf-8') as f:\n    f.write(json.dumps(tokenizer_json, ensure_ascii=False))","37f33c5d":"{'score': 0.9392208456993103, 'elapsed_time': 0.45516490936279297}\n{'score': 0.6474549174308777, 'elapsed_time': 0.16463708877563477}\n{'score': 0.195235475897789, 'elapsed_time': 0.17137527465820312}\n{'score': 0.09710333496332169, 'elapsed_time': 0.16474127769470215}\n"}}