{"cell_type":{"4d5ee037":"code","09d7769e":"code","d3a948af":"code","1084deb7":"code","b5e8886a":"code","a6c98b8a":"code","ff3d5a3a":"code","5b2d499f":"code","3fc0a8e6":"code","d1e6c8bf":"code","020340fa":"code","aa5c2a65":"code","ff66ba6e":"code","83e6b4b5":"code","b8894f95":"code","6f45911c":"code","4afb1ece":"code","df483d63":"code","891a2135":"code","c11f1d9e":"code","9a2afeff":"code","3271245b":"code","27274d9b":"code","eed5dc6d":"code","a219264e":"code","8b2833f8":"code","613f69e7":"code","d1dd44d1":"code","afc5c6fe":"code","7a987301":"code","60810111":"code","1dadf942":"code","2417af03":"code","3edc24e9":"code","1a768e43":"code","aebdd2fb":"markdown","5bec9a66":"markdown","83fa7d03":"markdown","31518aac":"markdown","6fe9e09b":"markdown","f233d6c0":"markdown","c02f6ea0":"markdown","4223a675":"markdown","475fb063":"markdown","01df6b64":"markdown","42af43ae":"markdown","e5663ea1":"markdown","865a0a5f":"markdown","4f4d3c68":"markdown","300d8d20":"markdown","f44fa4f2":"markdown","db9edc3d":"markdown","5e4aed61":"markdown","c0b3b7ed":"markdown","88fc324b":"markdown","35faee72":"markdown","84658106":"markdown","1eaaae44":"markdown","6134cb97":"markdown","23087402":"markdown","68867d71":"markdown","89f059fc":"markdown","5fa87f6a":"markdown","2c706b97":"markdown"},"source":{"4d5ee037":"import numpy as np \nimport pandas as pd \n\n# text processing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# XGBoost\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# sklearn \nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import f1_score\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold,RandomizedSearchCV\n\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# File system manangement\nimport os\n\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')","09d7769e":"# List files available\nprint(os.listdir(\"..\/input\/\"))","d3a948af":"#Training data\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nprint('Training data shape: ', train.shape)\ntrain.head()","1084deb7":"# Testing data \ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nprint('Testing data shape: ', test.shape)\ntest.head()","b5e8886a":"#Missing values in training set\ntrain.isnull().sum()","a6c98b8a":"#Missing values in test set\ntest.isnull().sum()","ff3d5a3a":"train['target'].value_counts()","5b2d499f":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='rocket')","3fc0a8e6":"# A disaster tweet\ndisaster_tweets = train[train['target']==1]['text']\ndisaster_tweets.values[1]","d1e6c8bf":"#not a disaster tweet\nnon_disaster_tweets = train[train['target']==0]['text']\nnon_disaster_tweets.values[1]","020340fa":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],\n            orient='h')","aa5c2a65":"train.loc[train['text'].str.contains('disaster', na=False, case=False)].target.value_counts()","ff66ba6e":"# Replacing the ambigious locations name with Standard names\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n            orient='h')","83e6b4b5":"# A quick glance over the existing data\ntrain['text'][:5]","b8894f95":"# Applying a first round of text cleaning techniques\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","6f45911c":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","4afb1ece":"text = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","df483d63":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","891a2135":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","c11f1d9e":"# Stemming and Lemmatization examples\ntext = \"feet cats wolves talked\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","9a2afeff":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","3271245b":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","27274d9b":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","eed5dc6d":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])\n","a219264e":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","8b2833f8":"clf.fit(train_vectors, train[\"target\"])","613f69e7":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","d1dd44d1":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","afc5c6fe":"clf_NB.fit(train_vectors, train[\"target\"])","7a987301":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","60810111":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","1dadf942":"import xgboost as xgb\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","2417af03":"import xgboost as xgb\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores\n\n","3edc24e9":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","1a768e43":"submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)","aebdd2fb":"<div align='center'><font size=\"6\" color=\"#F39C12\">Getting started with Natural Language Processing<\/font><\/div>\n<div align='center'><font size=\"5\" color=\"#F39C12\">A general Introduction<\/font><\/div>\n<hr>\n\n\n<p style='text-align:justify'><b>Key Objectives:<\/b>This notebook comes as a first part to the **Getting started with NLP Notebooks** that I am writing. This notebook explains the concepts of NLP with respect to this current competition. NLP is the field of study that focuses on the interactions between human language and computers. NLP sits at the intersection of computer science, artificial intelligence, and computational linguistics[[source](https:\/\/en.wikipedia.org\/wiki\/Natural_language_processing)]. NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way.<\/p>\n\n<b>Notebooks in this series<\/b>\n\n  <ul>\n      <li><a href=\"https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp\" target=\"_blank\">Part 1: Getting started with NLP : A General Introduction <\/a><\/li>\n      <li><a href=\"https:\/\/www.kaggle.com\/parulpandey\/getting-started-with-nlp-2-countvectorizer\" target=\"_blank\">Part 2: Getting started with NLP: Part 2 - CountVectorizers | TFIDF | Hashing Vectorizer <\/a><\/li><\/ul>\n\n<hr>\n\nAnother notebook based on text processing is : [Basic preprocessing and EDA](https:\/\/www.kaggle.com\/parulpandey\/basic-preprocessing-and-eda\/edit\/run\/31248388) based on the [Tweet Sentiment Extraction](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction)\n\n## Dataset\nThe datasets contains a set of tweets which have been divided into a training and a test set. The training set contains a target column identifying whether the tweet pertains to a real disasteror not.\n\nOur job is to create a ML model to predict whether the test set tweets belong to a disaster or not, in the form of 1 or 0.This is a classic case of a Binary Classification problem. \n\n## Understanding the Evaluation Metric\n\nEvaluation metrics are used to measure the quality of the statistical or machine learning model.There are many different types of evaluation metrics available to test a model. These include classification accuracy, logarithmic loss etc. For this particluar problem, our submissions will be evaluated using **F1** between the predicted and expected answers. \n\nThe **F score**, also called the **F1 score** or **F measure**, is a measure of a test\u2019s accuracy. \n\nThe F score is defined as the weighted harmonic mean of the test\u2019s precision and recall. \n\n![](https:\/\/imgur.com\/nC4QwrO.png)\n\n- Precision, also called the positive predictive value, is the proportion of positive results that truly are positive. \n\n- Recall, also called sensitivity, is the ability of a test to correctly identify positive results to get the true positive rate. \n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/2\/26\/Precisionrecall.svg\/350px-Precisionrecall.svg.png)\n*source:https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall*\n\nThe  F score reaches the best value, meaning perfect precision and recall, at a value of 1. The worst F score, which means lowest precision and lowest recall, would be a value of 0. \n\n\n\n### Why is this Useful?\nThe F score is used to measure a test\u2019s accuracy, and it balances the use of precision and recall to do it. The F score can provide a more realistic measure of a test\u2019s performance by using both precision and recall. The F score is often used in information retrieval for measuring search, document classification, and query classification performance. \n          \n          \nsource: https:\/\/deepai.org\/machine-learning-glossary-and-terms\/f-score          ","5bec9a66":"## Exploring the Target Column\n\n* ** Distribution of the Target Column**\n\nWe have to predict whether a given tweet is about a real disaster or not. - If so, predict a 1. If not, predict a 0.","83fa7d03":"## 3. Stopwords Removal\n\nNow, let's get rid of the stopwords i.e words which occur very frequently but have no possible value like **a, an, the, are **etc.","31518aac":"The columns denote the following: \n\n- The `text` of a tweet\n- A `keyword` from that tweet\n- The `location` the tweet was sent from","6fe9e09b":"## 4. Token normalization\n\nToken normalisation means converting different tokens to their base forms. This can be done either by:\n\n- **Stemming** :  removing and replacing suffixes to get to the root form of the word, which is called the **stem** for instance cats - cat, wolves - wolv \n- **Lemmatization** : Returns the base or dictionary form of a word, which is known as the **lemma** \n\n[*source*](https:\/\/www.coursera.org\/learn\/language-processing\/lecture\/SCd4G\/text-preprocessing)","f233d6c0":"It appears the countvectorizer gives a better performance than TFIDF in this case.","c02f6ea0":"## Naives Bayes Classifier\nWell, this is a decent score. Let's try with another model that is said to work well with text data : Naive Bayes.","4223a675":"# <a name=\"vectorization\"><\/a>  5. Transforming tokens to a vector\nAfter the initial preprocessing phase, we need to transform text into a meaningful vector (or array) of numbers. This can be done by a number of tecniques:\n\n## Bag of Words\n\nThe bag-of-words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n- A vocabulary of known words.\n- A measure of the presence of known words.\n\nWhy is it is called a \u201cbag\u201d of words? That is because any information about the order or structure of words in the document is discarded and the model is only concerned with whether the known words occur in the document, not where they occur in the document.\n\nFor example, \n\n![](https:\/\/imgur.com\/jWqtRP1.png)\n\n*source:[Natural Language Processing course on coursera](https:\/\/www.coursera.org\/learn\/language-processing\/home\/welcome)*\n\nWe can do this using scikit-learn's CountVectorizer, where every row will represent a different tweet and every column will represent a different word.\n\n### Bag of Words - Countvectorizer Features\n\n[Countvectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts. It is important to note here that CountVectorizer comes with a lot of options to automatically do preprocessing, tokenization, and stop word removal.However, i did all the process manually above to just get a better understanding. Let's use a vanilla implementation of the countvectorizer without specifying any parameters.\n","475fb063":"A lot of values are missing in the `location` column in both the training and the testing set.","01df6b64":"Note the test data doesn't have the target column","42af43ae":"# <a name=\"eda\"><\/a> 3. Basic EDA\n\n## Missing values","e5663ea1":"# <a name=\"reading\"><\/a> 2. Reading the datasets","865a0a5f":"Let's see how often the word 'disaster' come in the dataset and whether this help us in determining whether a tweet belongs to a disaster category or not.","4f4d3c68":"It is important to note here that stemming and lemmatization sometimes donot necessarily improve results as at times we donot want to trim words but rather preserve their original form. Hence their usage actually differs from problem to problem. For this problem, I will not use these techniques.","300d8d20":"# Table of Contents\n* [1. Importing the necessary libraries](#imports)\n- [2. Reading the datasets](#reading)\n- [3. Basic EDA](#eda)\n- [4. Text data processing](#processing)\n- [5. Transforming tokens to vectors](#vectorization)\n- [6. Buiding a Text Classification model](#model)","f44fa4f2":"## Exploring the 'keyword' column\nThe keyword column denotes a keyword from the tweet.Let's look at the top 20 keywords in the training data","db9edc3d":"## Making the submission","5e4aed61":"## Exploring the 'location' column\nEven though the column `location` has a number of missing values, let's see the top 20 locations present in the dataset. Since some of the locations are repeated, this will require some bit of cleaning.","c0b3b7ed":"## XGBoost","88fc324b":"### TFIDF Features\n\nA problem with the Bag of Words approach is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much \u201cinformational content\u201d. Also, it will give more weight to longer documents than shorter documents.\n\nOne approach is to rescale the frequency of words by how often they appear in all documents so that the scores for frequent words like \u201cthe\u201d that are also frequent across all documents are penalized. This approach to scoring is called Term Frequency-Inverse Document Frequency, or TF-IDF for short, where:\n\n**Term Frequency: is a scoring of the frequency of the word in the current document.**\n\n```\nTF = (Number of times term t appears in a document)\/(Number of terms in the document)\n```\n\n**Inverse Document Frequency: is a scoring of how rare the word is across documents.**\n\n```\nIDF = 1+log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in.\n```","35faee72":"> # <a name=\"processing\"><\/a> 4. Text Data Preprocessing\n\n## 1. Data Cleaning\n\nBefore we start with any NLP project we need to pre-process the data to get it all in a consistent format.We need to clean, tokenize and convert our data into a matrix. Some of the  basic text pre-processing techniques includes:\n\n* Make text all **lower case** or **uppercase** so that the algorithm does not treat the same words in different cases as different\n* **Removing Noise** i.e everything that isn\u2019t in a standard number or letter i.e Punctuation, Numerical values,  common non-sensical text (\/n)\n* **Tokenization**: Tokenization is just the term used to describe the process of converting the normal text strings into a list of tokens i.e words that we actually want. Sentence tokenizer can be used to find the list of sentences and Word tokenizer can be used to find the list of words in strings.\n* **Stopword Removal**: Sometimes, some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words\n\n### More data cleaning steps after tokenization:\n\n* **Stemming**: Stemming is the process of reducing inflected (or sometimes derived) words to their stem, base or root form\u200a\u2014\u200agenerally a written word form. Example if we were to stem the following words: \u201cStems\u201d, \u201cStemming\u201d, \u201cStemmed\u201d, \u201cand Stemtization\u201d, the result would be a single word \u201cstem\u201d.\n* **Lemmatization**: A slight variant of stemming is lemmatization. The major difference between these is, that, stemming can often create non-existent words, whereas lemmas are actual words. So, your root stem, meaning the word you end up with, is not something you can just look up in a dictionary, but you can look up a lemma. Examples of Lemmatization are that \u201crun\u201d is a base form for words like \u201crunning\u201d or \u201cran\u201d or that the word \u201cbetter\u201d and \u201cgood\u201d are in the same lemma so they are considered the same.\n* Parts of speech tagging\n* Create bi-grams or tri-grams\nAnd more...\n\nHowever, it is not necessary that you would need to use all these steps. The usage depends on your problem at hand. Sometimes removal of stop words helps while at other times, this might not help.Here is a nice table taken from the blog titled : [All you need to know about Text Preprocessing for Machine Learning & NLP](https:\/\/kavita-ganesan.com\/text-preprocessing-tutorial\/#.Xi2BhhczZTY) that summarizes how much preprocessing you should be performing on your text data:\n\n![](https:\/\/kavita-ganesan.com\/wp-content\/uploads\/2019\/02\/Screen-Shot-2019-02-23-at-1.36.52-PM-590x270.png)","84658106":"Just for fun let's create a wordcloud of the clean text to see the most dominating words in the tweets.","1eaaae44":"The training data has 7613 observations and 5 features including the TARGET (the label we want to predict).","6134cb97":"## 2. Tokenization\n\nTokenization is a process that splits an input sequence into so-called tokens where the tokens can be a word, sentence, paragraph etc. Base upon the type of tokens we want, tokenization can be of various types, for instance","23087402":"### Getting it all together- A Text Preprocessing Function\nThis concludes the pre-processing part. It will be prudent to convert all the steps undertaken into a function for better reusability.","68867d71":"* **Exploring the Target Column**\nLet's look at what the disaster and the non disaster tweets look like","89f059fc":"# <a name=\"imports\"><\/a>1. Importing the necessary libraries","5fa87f6a":"well the naive bayes on TFIDF features scores much better than logistic regression model. ","2c706b97":"# <a name=\"model\"><\/a> 6. Building a Text Classification model\nNow the data is ready to be fed into a classification model. Let's create a basic claasification model using commonly used classification algorithms and see how our model performs.\n\n## Logistic Regression Classifier"}}