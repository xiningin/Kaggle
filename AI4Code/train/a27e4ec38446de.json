{"cell_type":{"c086500b":"code","daea5eec":"code","86314c4f":"code","55941034":"code","3b64117d":"code","78c079e3":"code","9afcac25":"code","7217ea3f":"code","39cef101":"code","be0db647":"code","776e0728":"code","ad71c7c2":"code","61e61de6":"markdown","ceb55c43":"markdown","6183eade":"markdown","2edb87a1":"markdown","5f9cb982":"markdown","5f83666f":"markdown","05649303":"markdown","8638a2b7":"markdown"},"source":{"c086500b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","daea5eec":"import zipfile\nimport h5py\nfrom keras.optimizers import *\nimport cv2\nfrom keras.utils import to_categorical\nfrom matplotlib import pyplot as plt\nimport glob, os\nfrom matplotlib import pyplot as plt\nimport h5py\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nimport gc\nfrom keras.applications import *\nfrom keras.layers import *\nfrom keras import backend as K\nfrom keras.models import Model\nimport keras\nimport pandas as pd\nfrom keras.applications.nasnet import NASNetMobile, preprocess_input\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\n\nimport zipfile\nimport h5py\nfrom keras.optimizers import Adam\nimport cv2\nfrom keras.utils import to_categorical\nfrom matplotlib import pyplot as plt\nimport glob, os\nfrom matplotlib import pyplot as plt\nimport h5py\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\nfrom tqdm import tqdm\nimport time\nimport gc\nfrom keras.applications import *\nfrom keras.layers import *\nfrom keras import backend as K\nfrom keras.models import Model","86314c4f":"lbl=[]\nimg=np.zeros((3064,224,224))\nfor i in range(1,3065):\n    try:\n        path='\/kaggle\/input\/brain-tumour\/brainTumorDataPublic_1766\/'\n        with h5py.File(path+str(i)+'.mat') as f:\n          images = f['cjdata']\n          resized = cv2.resize(images['image'][:,:], (224,224), interpolation = cv2.INTER_CUBIC )\n          x=np.asarray(resized)\n          x=(x-np.min(x))\/(np.max(x)-np.min(x))\n          x=x.reshape((1,224,224))\n          img[i-1]=x\n          lbl.append(int(images['label'][0]))\n    except:\n        try:\n          path='\/kaggle\/input\/brain-tumour\/brainTumorDataPublic_22993064\/'\n          with h5py.File(path+str(i)+'.mat') as f:\n              images = f['cjdata']\n              resized = cv2.resize(images['image'][:,:], (224,224), interpolation = cv2.INTER_CUBIC )\n              x=np.asarray(resized)\n              x=(x-np.min(x))\/(np.max(x)-np.min(x))\n              x=x.reshape((1,224,224))\n              img[i-1]=x\n              lbl.append(int(images['label'][0]))\n        except:\n            try:\n              path='\/kaggle\/input\/brain-tumour\/brainTumorDataPublic_15332298\/'\n              with h5py.File(path+str(i)+'.mat') as f:\n                  images = f['cjdata']\n                  resized = cv2.resize(images['image'][:,:], (224,224), interpolation = cv2.INTER_CUBIC )\n                  x=np.asarray(resized)\n                  x=(x-np.min(x))\/(np.max(x)-np.min(x))\n                  x=x.reshape((1,224,224))\n                  img[i-1]=x\n                  lbl.append(int(images['label'][0]))\n            except:\n              path='\/kaggle\/input\/brain-tumour\/brainTumorDataPublic_7671532\/'\n              with h5py.File(path+str(i)+'.mat') as f:\n                  images = f['cjdata']\n                  resized = cv2.resize(images['image'][:,:], (224,224), interpolation = cv2.INTER_CUBIC )\n                  x=np.asarray(resized)\n                  x=(x-np.min(x))\/(np.max(x)-np.min(x))\n                  x=x.reshape((1,224,224))\n                  img[i-1]=x\n                  lbl.append(int(images['label'][0]))\n\npath='\/kaggle\/input\/braintumour\/cvind (2).mat'\n\nwith h5py.File(path) as f:\n      data=f['cvind']\n      idx=data[0]\nimport scipy.io\nobj_arr = {}\nobj_arr['images'] = img\nobj_arr['label'] = lbl\nobj_arr['fold']=idx\nnp.save('check.npy', obj_arr)\n","55941034":"\npath = \"check.npy\" \ndf=np.load(path,allow_pickle=True)\ndf=df.item()\ndf['images']=df['images'].astype(np.float32)","3b64117d":"\n\n#shuffle samples\ndef unison_shuffled_copies(a, b):\n    assert len(a) == len(b)\n    p = np.random.permutation(len(a))\n    return a[p], b[p]\n\n\n\n#change targets\ndef change(img):\n    resized = cv2.resize(img, (299,299), interpolation = cv2.INTER_AREA )\n    return resized\n\n\n\n\n#get train and test splits\ndef get_trn_tst(df,tst_fold):\n  idx=np.asarray(df['fold'])\n  y=np.asarray(df['label'])\n  y-=1\n  img=np.asarray(df['images'])\n  img1=[]\n  for i in range(len(img)):\n        img1.append(change(img[i]))\n  img1=np.asarray(img1)\n  del([img])\n  gc.collect()\n  trn_y=np.asarray(y[(idx!=tst_fold)])\n  trn_img=np.asarray(img1[(idx!=tst_fold)])\n  tst_y=np.asarray(y[(idx==tst_fold)])\n  tst_img=img1[idx==tst_fold]\n  trn_img=np.repeat(trn_img.reshape((trn_img.shape[0],299,299,1)),3,axis=3)\n  tst_img=np.repeat(tst_img.reshape((tst_img.shape[0],299,299,1)),3,axis=3)\n  return (trn_img.copy(),trn_y.copy()),(tst_img.copy(),tst_y.copy())","78c079e3":"\n\nmod=Xception(include_top=True, weights='imagenet')\nmod.summary()","9afcac25":"def load_model(last=True):   \n  K.clear_session() \n  mod=Xception(include_top=True, weights='imagenet')\n  out_1=mod.layers[-2].output\n  out=Dense(3,activation='softmax')(out_1)\n  model=Model(inputs=mod.input,outputs=out)\n\n  if last:\n    for i in range(len(model.layers)):\n        model.layers[i].trainable = False\n  model.layers[-1].trainable=True\n  return model\n\ndef rotate_image(image, angle):\n  image_center = tuple(np.array(image.shape[1::-1]) \/ 2)\n  rot_mat = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n  result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)\n  return result\ndef Hflip( images):\n\t\tseq = iaa.Sequential([iaa.Fliplr(1.0)])\n\t\treturn seq.augment_images(images)\ndef Vflip( images):\n\t\tseq = iaa.Sequential([iaa.Flipud(1.0)])\n\t\treturn seq.augment_images(images)\ndef noise(images):\n    ls=[]\n    for i in images:\n        x = np.random.normal(loc=0, scale=0.05, size=(299,299,3))\n        ls.append(i+x)\n    return ls\ndef rotate(images):\n    ls=[]\n    for angle in range(-15,20,5):\n        for image in images:\n            ls.append(rotate_image(image,angle))\n    return ls\n\nclass DataGenerator(keras.utils.Sequence):\n  def __init__(self, images, labels, batch_size=64, image_dimensions = (96 ,96 ,3), shuffle=False, augment=False):\n    self.labels       = labels              # array of labels\n    self.images = images        # array of image paths\n    self.batch_size   = batch_size          # batch size\n    self.on_epoch_end()\n\n  def __len__(self):\n    return int(np.floor(self.labels.shape[0] \/ self.batch_size))\n\n  def on_epoch_end(self):\n    self.indexes = np.arange(self.labels.shape[0])\n\n  def __getitem__(self, index):\n\t\t# selects indices of data for next batch\n    indexes = self.indexes[index * self.batch_size : (index + 1) * self.batch_size]\n    # select data and load images\n    labels = self.labels.loc[indexes]\n    img = [self.images[k].astype(np.float32) for k in indexes]\n    imgH=Hflip(img)\n    imgV=Vflip(img)\n    imgR=rotate(img)\n    images=[]\n    images.extend(imgH)\n    images.extend(imgV)\n    images.extend(imgR)\n    lbl=labels.copy()\n    labels=pd.DataFrame()\n    labels=pd.concat([labels,lbl],0)\n    labels=pd.concat([labels,lbl],0)\n    labels=pd.concat([labels,lbl],0)\n    labels=pd.concat([labels,lbl],0)\n    labels=pd.concat([labels,lbl],0)\n    labels=pd.concat([labels,lbl],0)\n    labels=pd.concat([labels,lbl],0)\n    labels=pd.concat([labels,lbl],0)\n    labels=pd.concat([labels,lbl],0)\n    del([imgV,imgR,imgH,lbl])\n    gc.collect()\n    #images = np.array([preprocess_input(img) for img in images])\n    return np.asarray(images), labels","7217ea3f":"\nbest_accuracy_last={}\nfinal_accuracy_last={}\nhistory_last={}\nanswers_last={}\npredictions_last={}\npredictions_last_best={}\ntimes_last={}","39cef101":"  def upd(dk,data):\n    if dk==0:\n        dk=data\n    else:\n        for ky in data.keys():\n            dk[ky].extend(data[ky])\n    return dk\n  index=2\n  epoch=25\n  pre_acc=0\n  best=0\n  fold='fold_'+str(index)\n  trn,tst=get_trn_tst(df,index)\n  history_last[fold]=0\n\n\n\n  plt.imshow(trn[0][0])\n  plt.show()\n  plt.imshow(tst[0][0])\n  plt.show()\n\n\n\n  trn_x,trn_y=unison_shuffled_copies(trn[0],trn[1])\n  tst_x,tst_y=unison_shuffled_copies(tst[0],tst[1])\n\n\n\n  model=load_model(last=False)\n\n\n  \n  #compiling the model\n  model.compile(optimizer=Adam(3e-4,decay=1e-3), \n                     loss='categorical_crossentropy', \n                     metrics=['accuracy'])\n  train_data = DataGenerator(trn_x,pd.get_dummies(trn_y), batch_size=4, augment=True)\n  ln=len(trn_y)\n  del([trn_x,trn_y,trn,tst])\n  gc.collect()\n  #fitting the model\n  #timing\n  start=time.time()\n  for i in range(epoch):\n      hist=model.fit_generator(train_data,epochs=1,steps_per_epoch=ln\/\/4)\n#       pre=model.predict(tst_x)\n#       pre=np.argmax(pre,1)\n#       new_acc=accuracy_score(pre,tst_y)\n#       if new_acc>best:\n#             best_accuracy_last[fold]=new_acc\n#             best=new_acc\n#             predictions_last_best[fold]=pre\n      history_last[fold]=upd(history_last[fold],hist.history)\n\n  end=time.time()\n  times_last[fold]=end-start\n\n  #getting the prediction \n  pre=model.predict(tst_x)\n  \n\n\n\n  #select the maximum position\n  pre=np.argmax(pre,1)\n  predictions_last[fold]=pre\n\n  \n  \n  \n  #getting the accuracy\n  new_acc=accuracy_score(pre,tst_y)\n\n  \n\n\n  #storing the predictions\n  final_accuracy_last[fold]=new_acc\n\n\n\n\n\n\n\n\n  #storing the answers\n  answers_last[fold]=tst_y\n    \n    \n    \n    \n  #freeing memory\n  del([tst_x,tst_y])\n  gc.collect()","be0db647":"print(new_acc)","776e0728":"plt.plot(history_last[fold]['loss'])","ad71c7c2":"np.save('best_accuracy_last_fold4_wn.npy',best_accuracy_last)\nnp.save('final_accuracy_last_fold4_wn.npy',final_accuracy_last)\nnp.save('history_last_fold4_wn.npy',history_last)\nnp.save('answers_last_fold4_wn.npy',answers_last)\nnp.save('predictions_last_fold5_wn.npy',predictions_last)\nnp.save('predictions_last_best_fold4_wn.npy',predictions_last_best)\nnp.save('times_last_fold4_wn.npy',times_last)","61e61de6":"Preparing data\n\n","ceb55c43":"Dictionaries to store results\n\n","6183eade":"Make Prediction\n\n","2edb87a1":"Importing libraries\n\n","5f9cb982":"Function to load model\n\n","5f83666f":"Verfying model stricture\n\n","05649303":"Function to shuffle data in fold and load each fold\n\n","8638a2b7":"Loading data\n\n"}}