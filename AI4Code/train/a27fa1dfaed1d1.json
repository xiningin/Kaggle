{"cell_type":{"5ea8b9a3":"code","32ff62fc":"code","64ebc652":"code","b5d07f5d":"code","13672da7":"code","336df756":"code","38c72e4a":"code","2d0e9682":"code","36c6bf07":"code","3f085cbb":"code","a5514d51":"code","8be66dd5":"code","8d2515a3":"code","4566d66e":"code","a04d3493":"code","bb8f20c5":"code","10e84ae4":"code","f143edc6":"code","69ff68f1":"code","d07152d5":"code","95db5c30":"code","b348eed0":"code","6f3c71c2":"code","b3538b41":"code","9265f300":"code","0f317ad9":"code","77dd0cf2":"code","b04b81a8":"code","2a824f1b":"code","a7ad2c84":"code","f27c7b93":"code","cf197945":"code","5c6a9a2a":"code","627a0c51":"code","26229a4b":"code","c6c085d9":"code","c1b402c3":"code","ee9fa0c1":"code","34447127":"code","0881d2b3":"code","9dce951d":"code","695e4164":"code","5e2af9dc":"code","0dd548be":"code","38e37ad7":"code","e3c35100":"markdown","a71134cb":"markdown","0bae6b12":"markdown","c791ffce":"markdown","93ba55b0":"markdown","a6747c54":"markdown","4943aeb2":"markdown","4d1cb84a":"markdown","df0da7cd":"markdown","0a697057":"markdown"},"source":{"5ea8b9a3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly.graph_objs as go\n\n\nimport statsmodels.api as sm\nimport warnings\nfrom scipy import stats\nfrom itertools import product\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score, cross_validate\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nimport warnings\nwarnings.filterwarnings('ignore')","32ff62fc":"train = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/train.csv')\ntest = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/test.csv')","64ebc652":"train.head()","b5d07f5d":"#Loading other data oil,holidays,transaction,stores\noil_df = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/oil.csv')\nholidays_df = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/holidays_events.csv')\nstores_df = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/stores.csv')\ntrans_df = pd.read_csv('..\/input\/store-sales-time-series-forecasting\/transactions.csv',parse_dates=['date'])","13672da7":"#shape of train\/test data\ntrain.shape,test.shape","336df756":"print(\"Column names in the Training Dataset are:\\n\",train.columns)","38c72e4a":"train.info()","2d0e9682":"train.describe().style.set_properties(**{\"background-color\": \"#c69be0\",\"color\": \"black\", \"border-color\": \"black\"})","36c6bf07":"oil_df.head()","3f085cbb":"fig = px.line(oil_df, x='date', y=\"dcoilwtico\")\nfig.update_layout(title = \"Oil by Date\")\nfig.show()","a5514d51":"oil_df[\"date\"] = pd.to_datetime(oil_df.date)\n# Resample\noil_df = oil_df.set_index(\"date\").dcoilwtico.resample(\"D\").sum().reset_index()\n# Interpolate\noil_df[\"dcoilwtico\"] = np.where(oil_df[\"dcoilwtico\"] == 0, np.nan, oil_df[\"dcoilwtico\"])\noil_df[\"dcoilwtico_interpolated\"] =oil_df.dcoilwtico.interpolate()","8be66dd5":"# Plot\np = oil_df.melt(id_vars=['date']+list(oil_df.keys()[5:]), var_name='Legend')\npx.line(p.sort_values([\"Legend\", \"date\"], ascending = [False, True]), x='date', y='value', color='Legend',title = \"Daily Oil Price interpolated\" )","8d2515a3":"print(holidays_df.info())\nholidays_df.head()","4566d66e":"holidays_df['date'] = pd.to_datetime(holidays_df['date'])\nholidays_df['day_holiday'] = holidays_df['date'].dt.day_name()\nholidays_df['month_holiday'] = holidays_df['date'].dt.month\nholidays_df['year_holiday'] = holidays_df['date'].dt.year","a04d3493":"holidays_df['day_holiday'].value_counts().plot.bar(figsize=(15,8),cmap = 'nipy_spectral')\nplt.title('Counts of Holidays',fontsize = 26)","bb8f20c5":"fig = px.pie(holidays_df,names='type', color_discrete_sequence=px.colors.sequential.RdBu,template=\"plotly_dark\")\n\nfig.update_layout(title = \"Holidays Type\")\nfig.show()","10e84ae4":"holidays_df['locale'].value_counts()","f143edc6":"f,ax=plt.subplots(1,2,figsize=(10,5))\nholidays_df['transferred'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True,cmap ='tab20c')\nax[0].set_title('Transferred Holidays Pie chart')\nax[0].set_ylabel('')\nsns.countplot('transferred',data=holidays_df,ax=ax[1],palette='Set2_r')\nax[1].set_title('Transferred Holidays bar chart')\nplt.show()","69ff68f1":"plt.figure(figsize=(8,5))\nsns.countplot(data=holidays_df,x='type',hue='transferred',palette = 'YlOrBr_r')\nplt.legend(['Not Transferred', 'Transferred'])\nplt.xticks(rotation=45)\nplt.title('Transferred Holidays')\nplt.show()","d07152d5":"from wordcloud import WordCloud\nwordcloud = WordCloud(\n                          background_color='black',\n                          max_font_size=50, \n                         ).generate(str(holidays_df['description']))\n\nprint(wordcloud)\nplt.figure(figsize=(15,7))\nplt.title('Word Cloud for Holidays description',fontsize=25)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","95db5c30":"stores_df.head()","b348eed0":"\nfig = px.histogram(stores_df, x=\"city\",color='type',template=\"plotly_dark\")\nfig.update_layout(title = \"Citys by Store Type\")\nfig.show()","6f3c71c2":"trans_df.head()","b3538b41":"trans_df.info()","9265f300":"fig = px.histogram(trans_df, x=\"transactions\",template=\"plotly_dark\")\nfig.update_layout(title = \"Transaction Distribution\")\nfig.show()","0f317ad9":"copy_df= trans_df.copy()\ncopy_df[\"year\"] = copy_df.date.dt.year\ncopy_df[\"dayofweek\"] = copy_df.date.dt.dayofweek+1\ncopy_df = copy_df.groupby([\"year\", \"dayofweek\"]).transactions.mean().reset_index()\npx.line(copy_df, x=\"dayofweek\", y=\"transactions\" , color = \"year\", title = \"Transactions\")","77dd0cf2":"train.head()","b04b81a8":"#Timeline feature\ndef time_feature(df):\n    df['date'] = pd.to_datetime(df['date'])\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['year'] = df['date'].dt.year\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['dayofmonth'] = df['date'].dt.day\n    return df","2a824f1b":"time_feature(train)\ntime_feature(test)","a7ad2c84":"# THANKS TO https:\/\/www.kaggle.com\/shivamb\/store-sales-forecasting-exploration\ndef hbar(col):\n    temp = train.groupby(col).agg({\"sales\" : \"mean\"}).reset_index()\n    temp = temp.sort_values(col, ascending = False)\n    c = {\n        'y' : list(temp['sales']), \n        'x' : list(temp[col]),\n        'title' : \"Average sales by \"+col\n    }\n    trace = go.Bar(y=c['y'], x=c['x'], orientation=\"v\", marker=dict(color=\"#edb705\"))\n    return trace \n\n    layout = go.Layout(title=c['title'], \n                           paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',\n                           xaxis_title=\"\", yaxis_title=\"\", width=650)\n    fig = go.Figure([trace], layout=layout)\n    fig.update_xaxes(tickangle=45, tickfont=dict(color='crimson'))\n    fig.update_yaxes(tickangle=0, tickfont=dict(color='crimson'))\n    fig.show()\n    \ntrace1 = hbar('dayofweek') \ntrace2 = hbar('dayofmonth') \ntrace3 = hbar('dayofyear') \ntrace4 = hbar('month') \ntrace5 = hbar('quarter') \ntrace6 = hbar('year') \n\ntitles = ['Day of Week', 'Day of Month', 'Day of Year', 'Month', 'Quarter', 'Year']\ntitles = ['Avg Sales by ' + _ for _ in titles]\nfig = make_subplots(rows=3, cols=2, subplot_titles = titles)\n\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\nfig.add_trace(trace3, row=2, col=1)\nfig.add_trace(trace4, row=2, col=2)\nfig.add_trace(trace5, row=3, col=1)\nfig.add_trace(trace6, row=3, col=2)\n\nfig.update_layout(height=1200, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', showlegend = False)\nfig.show()","f27c7b93":"#\n# data \ndf_2013 = train[train['year']==2013][['month','sales']]\ndf_2013 = df_2013.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s13'})\ndf_2014 = train[train['year']==2014][['month','sales']]\ndf_2014 = df_2014.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s14'})\ndf_2015 = train[train['year']==2015][['month','sales']]\ndf_2015 = df_2015.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s15'})\ndf_2016 = train[train['year']==2016][['month','sales']]\ndf_2016 = df_2016.groupby('month').agg({\"sales\" : \"mean\"}).reset_index().rename(columns={'sales':'s16'})\ndf_2017 = train[train['year']==2017][['month','sales']]\ndf_2017 = df_2017.groupby('month').agg({\"sales\" : \"mean\"}).reset_index()\ndf_2017_no = pd.DataFrame({'month': [9,10,11,12], 'sales':[0,0,0,0]})\ndf_2017 = df_2017.append(df_2017_no).rename(columns={'sales':'s17'})\ndf_year = df_2013.merge(df_2014,on='month').merge(df_2015,on='month').merge(df_2016,on='month').merge(df_2017,on='month')\n\n# top levels\ntop_labels = ['2013', '2014', '2015', '2016', '2017']\n\ncolors = ['#97c20a', '#a9cf30',\n          '#badb51', '#d0eb7a',\n          '#e3f2b1']\n\n# X axis value \ndf_year = df_year[['s13','s14','s15','s16','s17']].replace(np.nan,0)\nx_data = df_year.values\n\n# y axis value (Month)\ndf_2013['month'] =['1M','2M','3M','4M','5M','6M','7M','8M','9M','10M','11M','12M']\ny_data = df_2013['month'].tolist()\n\nfig = go.Figure()\nfor i in range(0, len(x_data[0])):\n    for xd, yd in zip(x_data, y_data):\n        fig.add_trace(go.Bar(\n            x=[xd[i]], y=[yd],\n            orientation='h',\n            marker=dict(\n                color=colors[i],\n                line=dict(color='rgb(248, 248, 249)', width=1)\n            )\n        ))\n\nfig.update_layout(title='Avg Sales for each Year',\n    xaxis=dict(showgrid=False, \n               zeroline=False, domain=[0.15, 1]),\n    yaxis=dict(showgrid=False, showline=False,\n               showticklabels=False, zeroline=False),\n    barmode='stack', #barnorm='percent',\n    plot_bgcolor='#fff', paper_bgcolor='#fff',\n    margin=dict(l=0, r=50, t=100, b=10),\n    showlegend=False, \n)\n\nannotations = []\nfor yd, xd in zip(y_data, x_data):\n    # labeling the y-axis\n    annotations.append(dict(xref='paper', yref='y',\n                            x=0.14, y=yd,\n                            xanchor='right',\n                            text=str(yd),\n                            font=dict(family='Arial', size=14,\n                                      color='rgb(67, 67, 67)'),\n                            showarrow=False, align='right'))\n    # labeling the first Likert scale (on the top)\n    if yd == y_data[-1]:\n        annotations.append(dict(xref='x', yref='paper',\n                                x=xd[0] \/ 2, y=1.1,\n                                text=top_labels[0],\n                                font=dict(family='Arial', size=14,\n                                          color='rgb(67, 67, 67)'),\n                          showarrow=False))\n    space = xd[0]\n    for i in range(1, len(xd)):\n            # labeling the Likert scale\n            if yd == y_data[-1]:\n                annotations.append(dict(xref='x', yref='paper',\n                                        x=space + (xd[i]\/2), y=1.1,\n                                        text=top_labels[i],\n                                        font=dict(family='Arial', size=14,\n                                                  color='rgb(67, 67, 67)'),\n                                        showarrow=False))\n            space += xd[i]\nfig.update_layout(\n    annotations=annotations)\nfig.show()","cf197945":"agg = train.groupby('date').agg({\"sales\" : \"mean\"}).reset_index()\nfig = px.line(agg, x='date', y=['sales'])\nfig.update_layout(title = \"Average Sales by Date\")\nfig.show()","5c6a9a2a":"train.head()","627a0c51":"#oil_df.columns\n#Index(['date', 'dcoilwtico', 'dcoilwtico_interpolated'], dtype='object')\n#holidays_df.columns\n#Index(['date', 'type', 'locale', 'locale_name', 'description', 'transferred','day_holiday', 'month_holiday', 'year_holiday'],dtype='object')\n#trans_df.columns\n#Index(['date', 'store_nbr', 'transactions'], dtype='object')\n#stores_df.columns\n#Index(['store_nbr', 'city', 'state', 'type', 'cluster'], dtype='object')","26229a4b":"## combine datasets\ntrain1 = train.merge(oil_df, on = 'date', how='left')\ntrain1 = train1.merge(holidays_df, on = 'date', how='left')\ntrain1 = train1.merge(stores_df, on = 'store_nbr', how='left')\ntrain1 = train1.merge(trans_df, on = ['date', 'store_nbr'], how='left')\ntrain1 = train1.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\n\ntest1 = test.merge(oil_df, on = 'date', how='left')\ntest1 = test1.merge(holidays_df, on = 'date', how='left')\ntest1 = test1.merge(stores_df, on = 'store_nbr', how='left')\ntest1 = test1.merge(trans_df, on = ['date', 'store_nbr'], how='left')\ntest1 = test1.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\n\ntrain1.head()","c6c085d9":"# Function to calculate missing values by column# Funct \ndef missing_data(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","c1b402c3":"missing_value = missing_data(train1)\nmissing_value.head(20)","ee9fa0c1":"test_missing = missing_data(test1)\ntest_missing.head()","34447127":"#Data Wranggling\n#train1.isnull().any()","0881d2b3":"agg = train1.groupby([\"year\", \"store_type\"]).agg({\"sales\"  :\"mean\", \"transactions\" : \"mean\"}).reset_index()\nfig = px.box(agg, y=\"sales\", facet_col=\"store_type\", color=\"store_type\",\n             boxmode=\"overlay\", points='all')\nfig.update_layout(title = \"Average Sales Distribution by Store Type\")\nfig.show()","9dce951d":"agg = train1.groupby([\"year\", \"state\"]).agg({\"sales\"  :\"mean\", \"transactions\" : \"mean\"}).reset_index()\nfig = px.box(agg, y=\"sales\", facet_col=\"state\", color=\"state\",\n             boxmode=\"overlay\", points='all')\nfig.update_layout(title = \"Average Sales Distribution by State\")\nfig.show()","695e4164":"agg = train1.groupby([\"year\", \"day_holiday\"]).agg({\"sales\"  :\"mean\", \"transactions\" : \"mean\"}).reset_index()\nfig = px.box(agg, y=\"sales\", facet_col=\"day_holiday\", color=\"day_holiday\",\n             boxmode=\"overlay\", points='all')\nfig.update_layout(title = \"Average Sales Distribution by Holiday day's\")\nfig.show()","5e2af9dc":"agg = train1.groupby([\"year\", \"holiday_type\"]).agg({\"sales\"  :\"mean\", \"transactions\" : \"mean\"}).reset_index()\nfig = px.box(agg, y=\"sales\", facet_col=\"holiday_type\", color=\"holiday_type\",\n             boxmode=\"overlay\", points='all')\nfig.update_layout(title = \"Average Sales Distribution by Holiday Type\")\nfig.show()","0dd548be":"# data\ndf_st_sa = train1.groupby('store_type').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)\ndf_fa_sa = train1.groupby('family').agg({\"sales\" : \"mean\"}).reset_index().sort_values(by='sales', ascending=False)[:10]\ndf_cl_sa = train1.groupby('cluster').agg({\"sales\" : \"mean\"}).reset_index() \n# chart color\ndf_fa_sa['color'] = '#32cf76'\ndf_fa_sa['color'][2:] = '#a5d993'\ndf_cl_sa['color'] = '#c8d984'\n\n# chart\nfig = make_subplots(rows=2, cols=2, \n                    specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n                           [{\"colspan\": 2}, None]],\n                    column_widths=[0.7, 0.3], vertical_spacing=0, horizontal_spacing=0.02,\n                    subplot_titles=(\"Top 10 Highest Product Sales\", \"Highest Sales in Stores\", \"Clusters Vs Sales\"))\n\nfig.add_trace(go.Bar(x=df_fa_sa['sales'], y=df_fa_sa['family'], marker=dict(color= df_fa_sa['color']),\n                     name='Family', orientation='h'), \n                     row=1, col=1)\nfig.add_trace(go.Pie(values=df_st_sa['sales'], labels=df_st_sa['store_type'], name='Store type',\n                     marker=dict(colors=['#68992f','#80b346','#9ac963','#c5e0a6','#e1edd3']), hole=0.7,\n                     hoverinfo='label+percent+value', textinfo='label'), \n                    row=1, col=2)\nfig.add_trace(go.Bar(x=df_cl_sa['cluster'], y=df_cl_sa['sales'], \n                     marker=dict(color= df_cl_sa['color']), name='Cluster'), \n                     row=2, col=1)\n\n# styling\nfig.update_yaxes(showgrid=False, ticksuffix=' ', categoryorder='total ascending', row=1, col=1)\nfig.update_xaxes(visible=False, row=1, col=1)\nfig.update_xaxes(tickmode = 'array', tickvals=df_cl_sa.cluster, ticktext=[i for i in range(1,17)], row=2, col=1)\nfig.update_yaxes(visible=False, row=2, col=1)\nfig.update_layout(height=500, bargap=0.2,\n                  margin=dict(b=0,r=20,l=20), xaxis=dict(tickmode='linear'),\n                  title_text=\"Average Sales Analysis\",\n                  template=\"plotly_white\",\n                  title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n                  font=dict(color='#8a8d93'),\n                  hoverlabel=dict(bgcolor=\"#f2f2f2\", font_size=13, font_family=\"Lato, sans-serif\"),\n                  showlegend=False)\nfig.show()","38e37ad7":"def vbar(col):\n    temp = train1.groupby(col).agg({\"sales\" : \"mean\"}).reset_index()\n    temp = temp.sort_values('sales', ascending = False)\n    c = {\n        'x' : list(temp['sales'])[:15][::-1], \n        'y' : list(temp[col])[:15][::-1],\n        'title' : \"Average sales by \"+col\n    }\n    trace = go.Bar(y=[str(_) + \"    \" for _ in c['y']], x=c['x'], orientation=\"h\", marker=dict(color=\"#c69be0\"))\n    return trace \n\n    layout = go.Layout(title=c['title'], \n                           paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)',\n                           xaxis_title=\"\", yaxis_title=\"\", width=650)\n    fig = go.Figure([trace], layout=layout)\n    fig.update_xaxes(tickangle=45, tickfont=dict(color='crimson'))\n    fig.update_yaxes(tickangle=0, tickfont=dict(color='crimson'))\n    fig.show()\n    \ntrace1 = vbar('family') \ntrace2 = vbar('store_type') \ntrace3 = vbar('state') \ntrace4 = vbar('city')\n\ntitles = ['Store Family', 'Store Type', 'State', 'City']\ntitles = ['Top ' + _ + \" by Average Sales\" for _ in titles]\nfig = make_subplots(rows=2, cols=2, subplot_titles = titles)\n\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=1, col=2)\nfig.add_trace(trace3, row=2, col=1)\nfig.add_trace(trace4, row=2, col=2)\n\nfig.update_layout(height=800, paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)', showlegend = False)\nfig.show()","e3c35100":"# Transactions data ","a71134cb":"<p><font size=\"3\" color=\"#8214c7\" style=\"Comic Sans MS;\">\n    \n**Train Data** The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales. store_nbr identifies the store at which the products are sold. family identifies the type of product sold. sales gives the total sales for a product family at a particular store at a given date. Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips). onpromotion gives the total number of items in a product family that were being promoted at a store at a given date.","0bae6b12":"# **Store Data**","c791ffce":"<p><font size=\"3\" color=\"#8214c7\" style=\"Comic Sans MS;\">\n    \n**Stores data** gives some information about stores such as city, state, type, cluster.\n\n**Transaction data** is highly correlated with train's sales column. You can understand the sales patterns of the stores.\n\n***Holidays and events data is a meta data***. This data is quite valuable to understand past sales, trend and seasonality components. However, it needs to be arranged. You are going to find a comprehensive data manipulation for this data. That part will be one of the most important chapter in this notebook.\n<\/font><\/p>\n","93ba55b0":"# <p><font size=\"10\" color=\"#8214c7\">Loading the data","a6747c54":"# Word Cloud ","4943aeb2":"# <p><font size=\"6\" color=\"#8214c7\">Oil Data\n<p><font size=\"3\" color=\"#8214c7\" style=\"Comic Sans MS;\">\nDaily oil price. Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)","4d1cb84a":"# Holiday's Data\n","df0da7cd":"# <p><font size=\"10\" color=\"#8214c7\">Basic Exploration \n","0a697057":"# Importing the Packages"}}