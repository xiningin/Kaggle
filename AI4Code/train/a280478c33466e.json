{"cell_type":{"68bbcb99":"code","5e9d4adc":"code","bff95ce9":"code","74aba054":"code","c559371d":"code","c2716d65":"code","cecdf1d2":"code","6d14b7ab":"code","0c9098e6":"code","cfed796f":"code","c2f3850e":"markdown","bda7ad2b":"markdown","fc0f722e":"markdown","a6118826":"markdown","b55a8fa1":"markdown","456c957d":"markdown","3a518763":"markdown","629e1c07":"markdown","c8ab38f7":"markdown","aa8cc21e":"markdown","014ff5d0":"markdown","c94629d0":"markdown"},"source":{"68bbcb99":"# Loading the essentials, numpy and pandas\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport scipy.stats as ss\n\n# Next up, os for listing and walking through directories\nimport os\n\n# Plt and seaborn for graphing\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import ScalarMappable\nimport seaborn as sns\n\nprint(os.listdir(\"..\/input\"))","5e9d4adc":"df_blackfriday = pd.read_csv(\"..\/input\/BlackFriday.csv\")\n\ndf_blackfriday.head()","bff95ce9":"nan_to_total_ratio = df_blackfriday.isna().sum() \/ df_blackfriday.shape[0]\nprint(nan_to_total_ratio)","74aba054":"# Save these for later use\nproduct_category_2_series = df_blackfriday['Product_Category_2']\nproduct_category_3_series = df_blackfriday['Product_Category_3']\n\ndf_blackfriday = df_blackfriday.drop(['Product_Category_2', 'Product_Category_3'], axis=1)\n\n","c559371d":"# Store the relevant categories\ncategories = df_blackfriday.columns[2:-1]\n\n# Define function for our check\n# Function used so I can easily exit two nested loops with the 'return' keyword\ndef check_duplicates_and_differences(category, groupby_object):\n    # Iterate over a GroupBy object, where keys are User IDs,\n    # and groups are groups of categorical values sharing a User ID\n    for key,group in groupby_object:\n        value_list = []\n        # Iterate over individual values in group\n        for i, value in enumerate(group):\n            # Branch code here: in the case of Product ID, we check for existence of duplicate values,\n            # else we check for differences between current and previous value\n            if category == 'Product_ID':\n                if value in value_list:\n                    print(\"Found duplicate value: {0} in user_id: {1} of category: {2}\".format(value,group,category))\n                    # Break the loops if any duplicates are found within a category\n                    return\n                value_list.append(value)\n            else:\n                if i>=1 and value != value_list[i-1]:\n                    print(\"Variable {0} of user {1} changes from {2} to {3}\".format(category, key, value_list[i-1], value))\n                    # Break the loops if any changes are found within a category\n                    return\n                value_list.append(value)\n\nfor category in categories:\n    # First, group our category by User ID\n    grouped = df_blackfriday[category].groupby(df_blackfriday['User_ID'])\n\n    check_duplicates_and_differences(category, grouped)","c2716d65":"ncols = 2\n\n# Assuming ncols, calculate number of rows based on number of categories times two\n# since we have 2 graphs per cat, then use these values for our grid size\nnrows = math.ceil(len(categories)*2\/ncols)\ngrid_size = (nrows,ncols)\nprint(grid_size)\n# Multiplier to convert grid size to appropriate size in inches\ninch_multiplier = 4\n\ngrid_inches = tuple(map(lambda x: x * inch_multiplier, grid_size))\n\nplt.figure(figsize=grid_inches)\n\n# 2-step iteration over number of categories times two\nfor index,category in zip(range(1, len(category)*2, 2), categories):\n    plt.subplot(grid_size[0],grid_size[1],index)\n    # Create a common color mapping for both cases so we can more easily compare the differences\n    subcategories = df_blackfriday[category].unique()\n    rgb_values = sns.color_palette(\"Set1\", len(subcategories))\n    color_map = dict(zip(subcategories, rgb_values))\n    \n    # First, plot distribution of users for category\n    if not category == 'Product_Category_1':\n        # Group by user and aggregate values by replacing them with the first value - can do this since\n        # I've shown that the values in these categories don't change per user\n        df_grouped_by_user = df_blackfriday[category].groupby(df_blackfriday['User_ID']).agg('first')\n        # Use normalize parameter to obtain fractions\n        df_grouped_by_user = df_grouped_by_user.sort_values().value_counts(normalize=True)\n        df_grouped_by_user.plot(kind='bar', title=category, color=df_grouped_by_user.index.map(color_map))\n        plt.ylabel('No. of users')\n    else:\n        df_grouped = df_blackfriday[category].sort_values().value_counts(normalize=True)\n        df_grouped.plot(kind='bar', title=category, color=df_grouped.index.map(color_map))\n\n    # Second, plot amount of purchases in dollars per category\n    plt.subplot(grid_size[0],grid_size[1],index+1)\n    \n    df_grouped_by_cat = df_blackfriday['Purchase'].groupby(df_blackfriday[category]).agg('sum')\n    # Divide Series elements by sum of all purchases to get fractions that we can \n    # compare with normalized count distributions\n    df_grouped_by_cat = df_grouped_by_cat.divide(df_blackfriday['Purchase'].sum())\n    df_grouped_by_cat = df_grouped_by_cat.sort_values(ascending=False)\n    df_grouped_by_cat.plot(kind='bar', title=category, color=df_grouped_by_cat.index.map(color_map))\n    plt.ylabel('Purchases')\n        \nplt.subplots_adjust(wspace = 0.2, hspace = 0.5, top=3)","cecdf1d2":"def cramers_corrected_stat(confusion_matrix):\n    \"\"\" calculate Cramers V statistic for categorical-categorical association.\n        uses correction from Bergsma and Wicher, \n        Journal of the Korean Statistical Society 42 (2013): 323-328\n    \"\"\"\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    # Use sum twice because of the way it works for dataframes\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2 - ((k-1)*(r-1))\/(n-1))    \n    rcorr = r - ((r-1)**2)\/(n-1)\n    kcorr = k - ((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr \/ min( (kcorr-1), (rcorr-1)))\n\n","6d14b7ab":"def correlation_ratio(dataframe, nominal_series_name, numerical_series_name):\n    categories_means = []\n    categories_weights = []\n    total_mean = np.average(dataframe[numerical_series_name])\n    total_variance = np.var(dataframe[numerical_series_name])\n    for category in dataframe[nominal_series_name].unique():\n        category_series = dataframe.loc[dataframe[nominal_series_name] == category][numerical_series_name]\n        category_mean = np.average(category_series)\n        categories_means.append(category_mean)\n        categories_weights.append(len(category_series))\n\n    categories_weighted_variance = np.average((categories_means - total_mean)**2, weights=categories_weights)\n    eta = categories_weighted_variance \/ total_variance\n    return eta","0c9098e6":"def create_corr_matrix(dataframe, nominal_columns, numerical_columns):\n    columns = dataframe.columns\n    # Forcing dtype np.float64 seems to be important for sns.heatmap() method to work with the output correlation matrix\n    corr_matrix = pd.DataFrame(index=columns, columns=columns, dtype=np.float64)\n    for i in range(0, len(columns)):\n        for j in range(i, len(columns)):\n            if i == j:\n                corr_matrix.at[columns[j], columns[i]] = 1.00\n            else:\n                if columns[i] in nominal_columns:\n                    if columns[j] in nominal_columns:\n                        # Categorical to categorical correlation\n                        confusion_matrix = pd.crosstab(dataframe[columns[i]], dataframe[columns[j]])\n                        corr_coef = cramers_corrected_stat(confusion_matrix)\n                        corr_matrix.at[columns[j], columns[i]] = corr_coef\n                        corr_matrix.at[columns[i], columns[j]] = corr_coef\n                    else:\n                        # Categorical to continuous correlation\n                        corr_coef = correlation_ratio(dataframe,columns[i], columns[j])\n                        corr_matrix.at[columns[j], columns[i]] = corr_coef\n                        corr_matrix.at[columns[i], columns[j]] = corr_coef\n                else:\n                    if columns[j] in nominal_columns:\n                        # Continuous to categorical correlation\n                        corr_coef = correlation_ratio(dataframe, columns[j], columns[i])\n                        corr_matrix.at[columns[j], columns[i]] = corr_coef\n                        corr_matrix.at[columns[i], columns[j]] = corr_coef\n                    else:\n                        # Continuous to continuous correlation - using Spearman coefficient here\n                        corr_coef, pval = ss.spearmanr(dataframe[columns[j]], dataframe[columns[i]])\n                        corr_matrix.at[columns[j], columns[i]] = corr_coef\n                        corr_matrix.at[columns[i], columns[j]] = corr_coef\n    return corr_matrix   ","cfed796f":"# Drop User_ID, Product_ID and our only continuous variable, Purchase\nnominal_columns = df_blackfriday.columns.drop(['User_ID','Product_ID','Purchase'])\nnumerical_columns = ['Purchase']\ndf_blackfriday_nouser = df_blackfriday.drop(['User_ID', 'Product_ID'], axis=1)\ncorr_matrix = create_corr_matrix(df_blackfriday_nouser, nominal_columns, numerical_columns)\n\nplt.subplots(figsize=(18,8))\n\n# Use vmin = 0 since we only have one numerical column, and the non-continuous association measures used\n# here range from 0 to 1\nsns.heatmap(corr_matrix, vmin=0, square=True)","c2f3850e":"Let's take a look at the distributions of ** purchasers** per various categories rather than the distribution of purchases. In order to get meaningful information on the gender, age etc. distributions we should look at the purchases grouped by user ID since the dataset is actually comprised of individual transactions. Thanks to [this user](https:\/\/www.kaggle.com\/dabate) with [his comment](https:\/\/www.kaggle.com\/shamalip\/black-friday-data-exploration#433093) (first comment in the linked kernel) for pointing it out - I have not initially realized this to be the case and assumed each data row represented a single purchaser.\n\nBefore displaying the distribution graphs, I want to check if the gender, age, occupation, city category, length of stay in a city and marital status for each user have not changed during the collection of this dataset. While the noise in information that is the consequence of these changes for single purchasers is most likely minimal in our case, I wanted to be precise.\n\nAlso, I wanted to check if purchases of the same item (product_id) occured more than once for an individual user.\n\nBelow is a snippet of code used to check all of the above. It's not a very elegant solution so please comment below if you know of a better way to do this:\n","bda7ad2b":"We can see that:\n* Almost 70% of all purchasers are men\n* 35% of the purchasers are aged between 26 and 35 years\n* Occupation no. 4 is most represented with about 12% of all purchasers, followed closely by occupations no. 0 and no. 7 with about 11.5% share\n* Nearly 60% of purchasers are single\n* More than 50% of purchases come from cities in the C category.\n* Most of the purchasers only stayed a year in the current city (about 25%).\n* Most of the products purchased come from subcategory no. 5 with about 28% of the share\n\nComparing the purchasing amount fractions with the count distributions, it seems that the fractions and ordering of subcategories don't match up in the cases of City Category, Gender and Product Category 1. Although most purchasers come from cities in the City Category C, the ones hailing from B cities seem to spend more. In the case of Gender, the distribution tilts even more in favor of men in terms of purchasing power (80% purchases vs. 70% purchasers). Finally, Product Category 1 purchases seem to differ mainly when it comes to product subcategory no. 1 which dominates the purchases with more than a 35% share, while being second to subcategory no. 5 in count distributions with only about 25% share.\n","fc0f722e":"Oops... seems I've included the Product_Category_1 column in the search, which is non-sensical - it is expected that users will purchase products from different subcategories. Nonetheless, the output confirmed that the above code detects changes.\n\nAs far as the other categories go, we can see that the values don't change which means we can safely draw conclusions from our distribution graphs. On the other hand, it seems no identical purchases have been made by any of the users. \n\nI realized also that in the case of Product ID it might've been good enough to just look at the distribution of **purchases** rather than purchasers, but the number of individual buyers of a certain product might be a good feature for a later regression problem of predicting prices.\n\nTime for the distribution graphs. Note that I distinguish between the Product Category variable and the rest - this column doesn't need to be grouped by user since our aim is to find out how much a certain product is purchased.\n\nAdding to this, I thought it would be a good idea to plot another graph for each categorical variable: the amount of purchases in dollars per category.\n","a6118826":"Seems there are missing values mainly in the aforementioned product categories. I am not sure we can safely replace these NA values with zeros assuming that this means that a particular product does not belong to a category, especially considering the fact that these are the only missing values in the dataset and the curator warned that there are missing values. Perhaps I should drop these columns out of the analysis for now.","b55a8fa1":"Already we see some missing values in the 2nd and 3rd product categories - let's take a better look at NaN values.","456c957d":"Time to load the data into a dataframe and look at the variables at my disposal.","3a518763":"Finally, use these functions to construct our correlation heatmap:","629e1c07":"Hello everyone, first kernel here :)\nI will explore the data provided for the black friday sales and attempt to draw some basic conclusions using descriptive statistics.\n\nI begin by loading the necessary libs:","c8ab38f7":"Let's look at the correlations between our variables. In this dataset we have a mix of categorical and continuous variables so we can't simply use pandas .corr() method since it is primed for calculating correlation coefficients for continuous variables with three different approaches: Pearson, Kendall and Spearman. \n\nWith that in mind, I searched for existing solutions in python for calculating the various measurements of association and correlation for categorical -categorical  and continuous - categorical variables. We will be using [Cramer's V](https:\/\/en.wikipedia.org\/wiki\/Cram%C3%A9r%27s_V) measure of association for the categorical - categorical case.\nFirst, props to [this answer at stack exchange](https:\/\/stackoverflow.com\/a\/39266194) for the following function:","aa8cc21e":"The correlation ratio was only calculated for our Purchase variable vs all the other ones since this is the only continuous variable in our analysis. The highest correlation ratio seems to be with the Product Category 1 categorical variable with a correlation ratio value north of 0.6. This is probably because the subcategories of product category 1 represent different types of consumer products, with each product type also having some price variance due to products coming from different manufacturers.\n\nLooking at the Cramer's V measures, the most associated pairs of categorical variables, given [Cohen's (1988) guidelines for behavioral sciences](http:\/\/rcompanion.org\/handbook\/H_10.html) for which there's a medium strength of association (or higher, depending on the number of degrees of freedom) seem to be Occupation - Age, Occupation - Gender and Age - Marital Status. Although we cannot interpret the direction of the association (positive or negative) with Cramer's V measure, the Age and Marital Status association arises probably because of the increased likelihood of being in a marriage the older someone is. The association between Occupation and Gender isn't too surprising either since many occupations tend to be dominated by men or women.  Finally, the association between Occupation and Age is an interesting one - I wasn't sure why this might've occured. One idea is this occurs because of the various 'newer' occupations in the IT field that are dominated by younger people (assuming this is the case).\n\n","014ff5d0":"We use these functions to define a 3rd function to create our correlation matrix (thanks to [dython library](https:\/\/github.com\/shakedzy\/dython) for inspiration):","c94629d0":"I also define a function for the [correlation ratio](https:\/\/en.wikipedia.org\/wiki\/Correlation_ratio) for the case of categorical vs continuous variables;"}}