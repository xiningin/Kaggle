{"cell_type":{"e65eab57":"code","f06e27ee":"code","ece821f5":"code","5855220b":"code","5abcee93":"code","727446fe":"code","e406fcac":"code","016994b1":"code","6448f10b":"code","f9eb3985":"code","861c0c1a":"code","80824bec":"code","2979109a":"code","75a8faf2":"code","19631fa1":"code","93ccb37e":"code","ec8f7a8a":"code","0f62e5da":"code","da7d6448":"code","3e4fbd67":"code","3bed5b91":"code","426502b9":"code","c2e73cda":"code","083b0106":"code","7000815a":"code","f243c483":"code","8763a76b":"code","f6a61852":"code","0fff5774":"code","946947aa":"code","bd6226c9":"code","7d51e521":"code","ec31559c":"code","7b1346d8":"code","219aa682":"code","caa65d00":"code","567e2402":"code","a3072867":"code","5746f557":"code","41f089fd":"code","24c33d32":"code","4ae175c6":"code","0bbaf194":"markdown","a42fdafc":"markdown","a89ac1fb":"markdown","59a7ca7a":"markdown","df95da15":"markdown"},"source":{"e65eab57":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation \nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport regex as re\nimport gc\n# Any results you write to the current directory are saved as output.","f06e27ee":"baseline_tree_score = 0.23092278864723115\nbaseline_neuralnetwork_score = 0.5480561937041435","ece821f5":"train = pd.read_csv('..\/input\/kaggletutorial\/covertype_train.csv')\ntest = pd.read_csv('..\/input\/kaggletutorial\/covertype_test.csv')","5855220b":"train_index = train.shape[0]","5abcee93":"lgbm_param =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.06,\n    \"num_leaves\": 16,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.7,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}","727446fe":"def keras_model(input_dims):\n    model = Sequential()\n    \n    model.add(Dense(input_dims, input_dim=input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims\/\/2))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.2))\n    \n    # output layer (y_pred)\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    \n    # compile this model\n    model.compile(loss='binary_crossentropy', # one may use 'mean_absolute_error' as alternative\n                  optimizer='adam', metrics=['accuracy'])\n    return model\n\ndef keras_history_plot(history):\n    plt.plot(history.history['loss'], 'y', label='train loss')\n    plt.plot(history.history['val_loss'], 'r', label='val loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(loc='upper right')\n    plt.show()","e406fcac":"def baseline_tree_cv(train):\n    train_df = train.copy()\n    y_value = train_df[\"Cover_Type\"]\n    del train_df[\"Cover_Type\"], train_df[\"ID\"]\n    \n    NFOLD = 5\n    folds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\n    total_score = 0\n    best_iteration = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n        train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n        evals_result_dict = {} \n        dtrain = lgbm.Dataset(train_x, label=train_y)\n        dvalid = lgbm.Dataset(valid_x, label=valid_y)\n\n        clf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=3000, valid_sets=[dtrain, dvalid],\n                               early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=500)\n\n        predict = clf.predict(valid_x)\n        cv_score = log_loss(valid_y, predict )\n        total_score += cv_score\n        best_iteration = max(best_iteration, clf.best_iteration)\n        print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n        lgbm.plot_metric(evals_result_dict)\n        plt.show()\n        \n    print(\"Best Iteration\", best_iteration)\n    print(\"Total LogLoss\", total_score \/ NFOLD)\n    print(\"Baseline model Score Diff\", total_score \/ NFOLD - baseline_tree_score)\n    \n    del train_df\n    \n    return best_iteration\n\ndef baseline_keras_cv(train):\n    train_df = train.copy()\n    y_value = train_df['Cover_Type']\n    del train_df['Cover_Type'], train_df['ID']\n    \n    model = keras_model(train_df.shape[1])\n    callbacks = [\n            EarlyStopping(\n                patience=10,\n                verbose=10)\n        ]\n\n    NFOLD = 5\n    folds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\n    total_score = 0\n    best_epoch = 0\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n        train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n        history = model.fit(train_x.values, train_y.values, nb_epoch=30, batch_size = 64, validation_data=(valid_x.values, valid_y.values), \n                            verbose=1, callbacks=callbacks)\n\n        keras_history_plot(history)\n        predict = model.predict(valid_x.values)\n        null_count = np.sum(pd.isnull(predict) )\n        if null_count > 0:\n            print(\"Null Prediction Error: \", null_count)\n            predict[pd.isnull(predict)] = predict[~pd.isnull(predict)].mean()\n\n        cv_score = log_loss(valid_y, predict )\n        total_score += cv_score\n        best_epoch = max(best_epoch, np.max(history.epoch))\n        print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n        \n    print(\"Best Epoch: \", best_epoch)\n    print(\"Total LogLoss\", total_score\/NFOLD)\n    print(\"Baseline model Score Diff\", total_score\/NFOLD - baseline_neuralnetwork_score)","016994b1":"def outlier_binary(frame, col, outlier_range):\n    outlier_feature = col + '_Outlier'\n    frame[outlier_feature] = 0\n    frame.loc[frame[col] > outlier_range, outlier_feature] = 1\n    return frame\n\ndef outlier_divide_ratio(frame, col, outlier_range):\n    outlier_index = frame[col] >= outlier_range\n    outlier_median =  frame.loc[outlier_index, col].median()\n    normal_median = frame.loc[frame[col] < outlier_range, col].median()\n    outlier_ratio = outlier_median \/ normal_median\n    \n    frame.loc[outlier_index, col] = frame.loc[outlier_index, col]\/outlier_ratio\n    return frame\n\ndef frequency_encoding(frame, col):\n    freq_encoding = frame.groupby([col]).size()\/frame.shape[0] \n    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequncy'.format(col)})\n    return frame.merge(freq_encoding, on=col, how='left')\n\ndef binning_category_combine_feature(frame, col1, col2, col1_quantile, col2_quantile):\n    print(col1, ' ', col2, 'Bining Combine')\n    col1_quantile = np.arange(0,1.1,col1_quantile)\n    col2_quantile = np.arange(0,1.1,col2_quantile)\n    \n    col1_label = '{}_quantile_label'.format(col1)\n    frame[col1_label] = pd.qcut(frame[col1], q=col1_quantile, labels = ['{}_quantile_{:.1f}'.format(col1, col) for col in col1_quantile][1:])\n    \n    col2_label = '{}_quantile_label'.format(col2)\n    frame[col2_label] = pd.qcut(frame[col2], q=col2_quantile, labels = ['{}_quantile_{:.1f}'.format(col2, col) for col in col2_quantile][1:])\n    \n    combine_label = 'Binnig_{}_{}_Combine'.format(col1, col2)\n    frame[combine_label] = frame[[col1_label, col2_label]].apply(lambda row: row[col1_label] +'_'+ row[col2_label] ,axis=1)\n    for col in [col1_label, col2_label, combine_label]:\n        frame[col] = frame[col].factorize()[0]\n    \n    # del frame[col1_label], frame[col2_label]\n    gc.collect()\n    return frame, [col1_label, col2_label, combine_label]","6448f10b":"def tree_data_preprocessing(train, test):\n    train_index = train.shape[0]\n    all_data = pd.concat([train, test])\n    del all_data['oil_Type']\n\n    all_column_set = set(all_data.columns)\n    category_feature = []\n    for col in all_data.loc[:, all_data.dtypes=='object'].columns:\n        all_data[col] = all_data[col].factorize()[0]\n        category_feature.append(col)\n\n    numerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n\n    all_data['Elevation'] = np.log1p(all_data['Elevation'])\n\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = frequency_encoding(all_data, 'Soil_Type')\n    all_data = frequency_encoding(all_data, 'Wilderness_Area')\n\n    aspect_train = all_data.loc[all_data['Aspect'].notnull()]\n    aspect_test = all_data.loc[all_data['Aspect'].isnull()]\n    del aspect_train[\"Cover_Type\"], aspect_train['ID']\n    del aspect_test[\"Cover_Type\"], aspect_test['ID']\n\n    numerical_feature_woaspect = numerical_feature[:]\n    numerical_feature_woaspect.remove('Aspect')\n\n    sc = StandardScaler()\n    aspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\n    aspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )\n\n    y_value = aspect_train['Aspect']\n    del aspect_train['Aspect'], aspect_test['Aspect']\n    \n    knn = KNeighborsRegressor(n_neighbors=7)\n    knn.fit(aspect_train,y_value)\n    predict = knn.predict(aspect_test)\n    \n    sns.distplot(predict)\n    sns.distplot(all_data['Aspect'].dropna())\n    plt.title('KNN Aspect Null Imputation')\n    plt.show()\n    \n    all_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n    \n    all_data['Horizontal_Distance_To_Hydrology'] = all_data['Horizontal_Distance_To_Hydrology']\/1000\n    all_data['HF1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Fire_Points'])\n    all_data['HF4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Horizontal_Distance_To_Fire_Points']\n\n    all_data['HR1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['HR4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Horizontal_Distance_To_Roadways']\n\n    all_data['HH1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH3'] = np.log1p(abs(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Vertical_Distance_To_Hydrology']))\n    all_data['HH4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Vertical_Distance_To_Hydrology']\n\n    all_data['FR1'] = all_data['Horizontal_Distance_To_Fire_Points'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR2'] = all_data['Horizontal_Distance_To_Fire_Points'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR3'] = np.log1p(all_data['Horizontal_Distance_To_Fire_Points'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['FR4'] = all_data['Horizontal_Distance_To_Fire_Points'] \/ all_data['Horizontal_Distance_To_Roadways']\n    \n    all_data['Direct_Distance_Hydrology'] = (all_data['Horizontal_Distance_To_Hydrology']**2+all_data['Vertical_Distance_To_Hydrology']**2)**0.5\n    \n    all_data.loc[np.isinf(all_data['HF4']),'HF4'] = 0\n    all_data.loc[np.isinf(all_data['HR4']),'HR4'] = 0\n    all_data.loc[np.isinf(all_data['HH4']),'HH4'] = 0\n    all_data.loc[np.isinf(all_data['FR4']),'FR4'] = 0\n    all_data[['HF4','HH4']] = all_data[['HF4','HH4']].fillna(0)\n    \n    all_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Aspect', 0.1, 0.1) \n    for col in new_col:\n        all_data = frequency_encoding(all_data, col)\n        \n    train_df = all_data.iloc[:train_index]\n    test_df = all_data.iloc[train_index:]\n    \n    soil_mean_encoding = train_df.groupby(['Soil_Type'])['Cover_Type'].agg({'Soil_Type_Mean':'mean', \n                                                                        'Soil_Type_Std':'std', \n                                                                        'Soil_Type_Size':'size', \n                                                                        'Soil_Type_Sum':'sum'}).reset_index()\n    train_df = train_df.merge(soil_mean_encoding, on='Soil_Type', how='left')\n    test_df = test_df.merge(soil_mean_encoding, on='Soil_Type', how='left')\n    \n    wildness_mean_encoding = train_df.groupby(['Wilderness_Area'])['Cover_Type'].agg({'Wilderness_Area_Mean':'mean', \n                                                                              'Wilderness_Area_Std':'std', \n                                                                              'Wilderness_Area_Size':'size', \n                                                                              'Wilderness_Area_Sum':'sum'}).reset_index()\n    train_df = train_df.merge(wildness_mean_encoding, on='Wilderness_Area', how='left')\n    test_df = test_df.merge(wildness_mean_encoding, on='Wilderness_Area', how='left')\n    \n    del all_data, predict, aspect_train, aspect_test\n    gc.collect()\n    \n    return train_df, test_df","f9eb3985":"def nn_data_preprocessing(train, test):\n    train_index = train.shape[0]\n    all_data = pd.concat([train, test])\n    del all_data['oil_Type']\n\n    all_column_set = set(all_data.columns)\n    category_feature = []\n    for col in all_data.loc[:, all_data.dtypes=='object'].columns:\n        all_data[col] = all_data[col].factorize()[0]\n        category_feature.append(col)\n    \n    numerical_feature = list(all_column_set - set(category_feature) - set(['Cover_Type','ID']))\n    \n    all_data['Elevation'] = np.log1p(all_data['Elevation'])\n\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_binary(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Fire_Points', 10000)\n    all_data = outlier_divide_ratio(all_data, 'Horizontal_Distance_To_Roadways', 10000)\n\n    all_data = frequency_encoding(all_data, 'Soil_Type')\n    all_data = frequency_encoding(all_data, 'Wilderness_Area')\n\n    aspect_train = all_data.loc[all_data['Aspect'].notnull()]\n    aspect_test = all_data.loc[all_data['Aspect'].isnull()]\n    del aspect_train[\"Cover_Type\"], aspect_train['ID']\n    del aspect_test[\"Cover_Type\"], aspect_test['ID']\n\n    numerical_feature_woaspect = numerical_feature[:]\n    numerical_feature_woaspect.remove('Aspect')\n\n    sc = StandardScaler()\n    aspect_train[numerical_feature_woaspect] = sc.fit_transform(aspect_train[numerical_feature_woaspect])\n    aspect_test[numerical_feature_woaspect] = sc.transform(aspect_test[numerical_feature_woaspect] )\n\n    y_value = aspect_train['Aspect']\n    del aspect_train['Aspect'], aspect_test['Aspect']\n\n    knn = KNeighborsRegressor(n_neighbors=7)\n    knn.fit(aspect_train,y_value)\n    predict = knn.predict(aspect_test)\n\n    sns.distplot(predict)\n    sns.distplot(all_data['Aspect'].dropna())\n    plt.title('KNN Aspect Null Imputation')\n    plt.show()\n\n    all_data.loc[all_data['Aspect'].isnull(),'Aspect'] = predict\n    \n    all_data['Horizontal_Distance_To_Hydrology'] = all_data['Horizontal_Distance_To_Hydrology']\/1000\n    all_data['HF1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Fire_Points']\n    all_data['HF3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Fire_Points'])\n    all_data['HF4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Horizontal_Distance_To_Fire_Points']\n\n    all_data['HR1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['HR3'] = np.log1p(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['HR4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Horizontal_Distance_To_Roadways']\n\n    all_data['HH1'] = all_data['Horizontal_Distance_To_Hydrology'] + all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH2'] = all_data['Horizontal_Distance_To_Hydrology'] - all_data['Vertical_Distance_To_Hydrology']\n    all_data['HH3'] = np.log1p(abs(all_data['Horizontal_Distance_To_Hydrology'] * all_data['Vertical_Distance_To_Hydrology']))\n    all_data['HH4'] = all_data['Horizontal_Distance_To_Hydrology'] \/ all_data['Vertical_Distance_To_Hydrology']\n\n    all_data['FR1'] = all_data['Horizontal_Distance_To_Fire_Points'] + all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR2'] = all_data['Horizontal_Distance_To_Fire_Points'] - all_data['Horizontal_Distance_To_Roadways']\n    all_data['FR3'] = np.log1p(all_data['Horizontal_Distance_To_Fire_Points'] * all_data['Horizontal_Distance_To_Roadways'])\n    all_data['FR4'] = all_data['Horizontal_Distance_To_Fire_Points'] \/ all_data['Horizontal_Distance_To_Roadways']\n\n    all_data['Direct_Distance_Hydrology'] = (all_data['Horizontal_Distance_To_Hydrology']**2+all_data['Vertical_Distance_To_Hydrology']**2)**0.5\n    \n    all_data.loc[np.isinf(all_data['HF4']),'HF4'] = 0\n    all_data.loc[np.isinf(all_data['HR4']),'HR4'] = 0\n    all_data.loc[np.isinf(all_data['HH4']),'HH4'] = 0\n    all_data.loc[np.isinf(all_data['FR4']),'FR4'] = 0\n    \n    all_data[['HF4','HH4']] = all_data[['HF4','HH4']].fillna(0)\n    \n    all_data, new_col = binning_category_combine_feature(all_data, 'Elevation', 'Aspect', 0.1, 0.1)\n    \n    for col in new_col:\n        all_data = frequency_encoding(all_data, col)\n        \n    all_data.drop(columns=new_col,axis=1,inplace=True)\n    \n    before_one_hot = set(all_data.columns)\n    for col in category_feature:\n        all_data = pd.concat([all_data,pd.get_dummies(all_data[col],prefix=col)],axis=1)\n        \n    one_hot_feature = set(all_data.columns) - before_one_hot\n    \n    train_df = all_data.iloc[:train_index]\n    test_df = all_data.iloc[train_index:]\n    \n    soil_mean_encoding = train_df.groupby(['Soil_Type'])['Cover_Type'].agg({'Soil_Type_Mean':'mean', \n                                                                        'Soil_Type_Std':'std', \n                                                                        'Soil_Type_Size':'size', \n                                                                        'Soil_Type_Sum':'sum'}).reset_index()\n    train_df = train_df.merge(soil_mean_encoding, on='Soil_Type', how='left')\n    test_df = test_df.merge(soil_mean_encoding, on='Soil_Type', how='left')\n    \n    wildness_mean_encoding = train_df.groupby(['Wilderness_Area'])['Cover_Type'].agg({'Wilderness_Area_Mean':'mean', \n                                                                              'Wilderness_Area_Std':'std', \n                                                                              'Wilderness_Area_Size':'size', \n                                                                              'Wilderness_Area_Sum':'sum'}).reset_index()\n    train_df = train_df.merge(wildness_mean_encoding, on='Wilderness_Area', how='left')\n    test_df = test_df.merge(wildness_mean_encoding, on='Wilderness_Area', how='left')\n    \n    train_df.drop(columns=category_feature, axis=1, inplace=True)\n    test_df.drop(columns=category_feature, axis=1, inplace=True)\n    \n    scale_feature = list(set(train_df.columns)-one_hot_feature-set(['Cover_Type','ID']))\n    sc = StandardScaler()\n    train_df[scale_feature] = sc.fit_transform(train_df[scale_feature])\n    test_df[scale_feature] = sc.transform(test_df[scale_feature] )\n    \n    return train_df, test_df","861c0c1a":"org_train_df, org_test_df = tree_data_preprocessing(train, test)","80824bec":"train_df = org_train_df.copy()\ntest_df = org_test_df.copy()","2979109a":"lgbm_param =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 24,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.65,\n    \"subsample\": 0.7,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.2,\n    \"nthread\":8\n}","75a8faf2":"y_value = train_df[\"Cover_Type\"]\ndel train_df[\"Cover_Type\"], train_df[\"ID\"]\ndel test_df[\"Cover_Type\"], test_df[\"ID\"]","19631fa1":"\"\"\" \uc2dc\uac04\uad00\uacc4\uc0c1 CV\ub97c \ub3cc\ub9ac\uc9c0\ub294 \uc54a\uaca0\uc2b5\ub2c8\ub2e4.\nNFOLD = 5\nfolds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\ntotal_score = 0\nbest_iteration = 0\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n    train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n    valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n    evals_result_dict = {} \n    dtrain = lgbm.Dataset(train_x, label=train_y)\n    dvalid = lgbm.Dataset(valid_x, label=valid_y)\n\n    clf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=5000, valid_sets=[dtrain, dvalid],\n                           early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=500)\n\n    predict = clf.predict(valid_x)\n    cv_score = log_loss(valid_y, predict )\n    total_score += cv_score\n    best_iteration = max(best_iteration, clf.best_iteration)\n    print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n    lgbm.plot_metric(evals_result_dict)\n    plt.show()\n\nprint(\"Best Iteration\", best_iteration)\nprint(\"Total LogLoss\", total_score \/ NFOLD)\nprint(\"Baseline model Score Diff\", total_score \/ NFOLD - baseline_tree_score)\n\"\"\"","93ccb37e":"dtrain = lgbm.Dataset(train_df, label=y_value)\nclf = lgbm.train(lgbm_param, train_set=dtrain, num_boost_round=5000)\npredict = clf.predict(test_df)","ec8f7a8a":"submission = pd.read_csv('..\/input\/kaggletutorial\/sample_submission.csv')\nsubmission['Cover_Type'] = predict\nsubmission.to_csv('lgbm_last.csv', index=False)","0f62e5da":"org_train_df, org_test_df = nn_data_preprocessing(train, test)","da7d6448":"train_df = org_train_df.copy()\ntest_df = org_test_df.copy()","3e4fbd67":"def keras_model(input_dims):\n    model = Sequential()\n    \n    model.add(Dense(input_dims, input_dim=input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims\/\/2))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    model.add(Dense(input_dims\/\/5))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.3))\n    \n    # output layer (y_pred)\n    model.add(Dense(1))\n    model.add(Activation('sigmoid'))\n    \n    # compile this model\n    model.compile(loss='binary_crossentropy', \n                  optimizer='adam', metrics=['accuracy'])\n    return model\n\n","3bed5b91":"y_value = train_df['Cover_Type']\ndel train_df['Cover_Type'], train_df['ID']\ndel test_df['Cover_Type'], test_df['ID']\n\nmodel = keras_model(train_df.shape[1])\ncallbacks = [\n        EarlyStopping(\n            patience=10,\n            verbose=10)\n    ]","426502b9":"\"\"\" \uc2dc\uac04\uad00\uacc4\uc0c1 CV\ub97c \ub3cc\ub9ac\uc9c0\ub294 \uc54a\uaca0\uc2b5\ub2c8\ub2e4.\nNFOLD = 5\nfolds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\ntotal_score = 0\nbest_epoch = 0\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y_value)):\n    train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n    valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n    history = model.fit(train_x.values, train_y.values, nb_epoch=30, batch_size = 64, validation_data=(valid_x.values, valid_y.values), \n                        verbose=1, callbacks=callbacks)\n\n    keras_history_plot(history)\n    predict = model.predict(valid_x.values)\n    null_count = np.sum(pd.isnull(predict) )\n    if null_count > 0:\n        print(\"Null Prediction Error: \", null_count)\n        predict[pd.isnull(predict)] = predict[~pd.isnull(predict)].mean()\n\n    cv_score = log_loss(valid_y, predict )\n    total_score += cv_score\n    best_epoch = max(best_epoch, np.max(history.epoch))\n    print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n\nprint(\"Best Epoch: \", best_epoch)\nprint(\"Total LogLoss\", total_score\/NFOLD)\nprint(\"Baseline model Score Diff\", total_score\/NFOLD - baseline_neuralnetwork_score)\n\"\"\"","c2e73cda":"history = model.fit(train_df.values, y_value.values, nb_epoch=30, batch_size = 64, verbose=1)\npredict = model.predict(test_df.values)","083b0106":"submission_nn = pd.read_csv('..\/input\/kaggletutorial\/sample_submission.csv')\nsubmission_nn['Cover_Type'] = predict\nsubmission_nn.to_csv('nn_last.csv', index=False)","7000815a":"def calculate_correlation(base_df, target_df):\n    source = base_df.copy()\n    source = source.merge(target_df,on='ID')\n    corr_df = source.corr()\n    corr = corr_df.ix['Cover_Type_x']['Cover_Type_y']\n    del corr_df, source\n    return corr","f243c483":"source = submission.copy()\nsource = source.merge(submission_nn,on='ID')\nsource","8763a76b":"calculate_correlation(submission, submission_nn)","f6a61852":"class SklearnWrapper(object):\n    def __init__(self, clf, params=None, **kwargs):\n        seed = kwargs.get('seed', 0)\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict_proba(x)[:,1]","0fff5774":"class LgbmWrapper(object):\n    def __init__(self, params=None, **kwargs):\n        seed = kwargs.get('seed', 0)\n        num_rounds = kwargs.get('num_rounds', 1000)\n        early_stopping = kwargs.get('ealry_stopping', 100)\n        eval_function = kwargs.get('eval_function', None)\n        verbose_eval = kwargs.get('verbose_eval', 100)\n\n        self.param = params\n        self.param['seed'] = seed\n        self.num_rounds = num_rounds\n        self.early_stopping = early_stopping\n        self.eval_function = eval_function\n        self.verbose_eval = verbose_eval\n\n    def train(self, x_train, y_train, x_cross=None, y_cross=None):\n        need_cross_validation = True\n        if x_cross is None:\n            need_cross_validation = False\n\n        if isinstance(y_train, pd.DataFrame) is True:\n            y_train = y_train[y_train.columns[0]]\n            if need_cross_validation is True:\n                y_cross = y_cross[y_cross.columns[0]]\n\n        if need_cross_validation is True:\n            dtrain = lgbm.Dataset(x_train, label=y_train, silent=True)\n            dvalid = lgbm.Dataset(x_cross, label=y_cross, silent=True)\n            self.clf = lgbm.train(self.param, train_set=dtrain, num_boost_round=self.num_rounds, valid_sets=dvalid,\n                                  feval=self.eval_function, early_stopping_rounds=self.early_stopping,\n                                  verbose_eval=self.verbose_eval)\n        else:\n            dtrain = lgbm.Dataset(x_train, label=y_train, silent= True)\n            self.clf = lgbm.train(self.param, dtrain, self.num_rounds)\n\n    def predict(self, x):\n        return self.clf.predict(x, num_iteration=self.clf.best_iteration)\n\n    def get_params(self):\n        return self.param","946947aa":"def get_oof(clf, x_train, y_train, x_test, eval_func, **kwargs):\n    nfolds = kwargs.get('NFOLDS', 5)\n    kfold_shuffle = kwargs.get('kfold_shuffle', True)\n    kfold_random_state = kwargs.get('kfold_random_sate', 0)\n\n    ntrain = x_train.shape[0]\n    ntest = x_test.shape[0]\n\n    kf = StratifiedKFold(n_splits= nfolds, shuffle=kfold_shuffle, random_state=kfold_random_state)\n\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((nfolds, ntest))\n\n    cv_sum = 0\n    try:\n        if clf.clf is not None:\n            print(clf.clf)\n    except:\n        print(clf)\n        print(clf.get_params())\n    \n    for i, (train_index, cross_index) in enumerate(kf.split(x_train, y_train)):\n        x_tr, x_cr = None, None\n        y_tr, y_cr = None, None\n        if isinstance(x_tr, pd.DataFrame):\n            x_tr, x_cr = x_train.iloc[train_index], x_train.iloc[cross_index]\n            y_tr, y_cr = y_train.iloc[train_index], y_train.iloc[cross_index]\n        else:\n            x_tr, x_cr = x_train[train_index], x_train[cross_index]\n            y_tr, y_cr = y_train[train_index], y_train[cross_index]\n\n        clf.train(x_tr, y_tr, x_cr, y_cr)\n        oof_train[cross_index] = clf.predict(x_cr)\n        cv_score = eval_func(y_cr, oof_train[cross_index])\n\n        print('Fold %d \/ ' % (i+1), 'CV-Score: %.6f' % cv_score)\n        cv_sum = cv_sum + cv_score\n\n    score = cv_sum \/ nfolds\n    print(\"Average CV-Score: \", score)\n    # Using All Dataset, retrain\n    clf.train(x_train, y_train)\n    oof_test = clf.predict(x_test)\n\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","bd6226c9":"lgbm_param1 =  {\n    'boosting_type': 'dart',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 31,\n    \"max_depth\": 7,\n    \"colsample_bytree\": 0.8,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8,\n    'drop_rate':0.1, \n    'skip_drop':0.5,\n    'max_drop':50, \n    'top_rate':0.1, \n    'other_rate':0.1\n}\n\nlgbm_param2 =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 10,\n    \"max_depth\": 4,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}\n\nlgbm_param3 =  {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    \"learning_rate\": 0.03,\n    \"num_leaves\": 24,\n    \"max_depth\": 6,\n    \"colsample_bytree\": 0.5,\n    \"subsample\": 0.8,\n    \"reg_alpha\": 0.1,\n    \"reg_lambda\": 0.1,\n    \"nthread\":8\n}\n\nrf_params = {\n    'criterion':'gini', 'max_leaf_nodes':24, 'n_estimators':200, 'min_impurity_split':0.0000001,\n    'max_features':0.4, 'max_depth':6, 'min_samples_leaf':20, 'min_samples_split':2,\n    'min_weight_fraction_leaf':0.0, 'bootstrap':True,\n    'random_state':1, 'verbose':False\n    \n}\n\net_parmas = {\n    'criterion':'gini', 'max_leaf_nodes':31, 'n_estimators':200, 'min_impurity_split':0.0000001,\n    'max_features':0.6, 'max_depth':10, 'min_samples_leaf':20, 'min_samples_split':2,\n    'min_weight_fraction_leaf':0.0, 'bootstrap':True,\n    'random_state':1, 'verbose':False \n}","7d51e521":"org_train_df, org_test_df = tree_data_preprocessing(train, test)\ntrain_df = org_train_df.copy()\ntest_df = org_test_df.copy()","ec31559c":"y_value = train_df[\"Cover_Type\"]\ndel train_df[\"Cover_Type\"], train_df[\"ID\"]\ndel test_df[\"Cover_Type\"], test_df[\"ID\"]","7b1346d8":"et_model = SklearnWrapper(clf = ExtraTreesClassifier, params=et_parmas)\nrf_model = SklearnWrapper(clf = RandomForestClassifier, params=rf_params)","219aa682":"et_train, et_test = get_oof(et_model, train_df.values, y_value, test_df.values, log_loss, NFOLDS=3)\nrf_train, rf_test = get_oof(rf_model, train_df.values, y_value, test_df.values, log_loss, NFOLDS=3)","caa65d00":"x_train_second_layer = np.concatenate((rf_train, et_train), axis=1)\nx_test_second_layer = np.concatenate((rf_test, et_test), axis=1)","567e2402":"x_train = pd.DataFrame(x_train_second_layer)\nx_test = pd.DataFrame(x_test_second_layer)","a3072867":"lgbm_meta_params = {\n    'boosting':'gbdt', 'num_leaves':28, 'learning_rate':0.03, 'min_sum_hessian_in_leaf':0.1,\n    'max_depth':7, 'feature_fraction':0.6, 'min_data_in_leaf':30, 'poission_max_delta_step':0.7,\n    'bagging_fraction':0.8, 'min_gain_to_split':0, \n    'objective':'binary', 'seed':1,'metric': 'binary_logloss'\n}\n\nNFOLD = 3\nfolds = StratifiedKFold(n_splits= NFOLD, shuffle=True, random_state=2018)\n\ntotal_score = 0\nbest_iteration = 0\nfor n_fold, (train_idx, valid_idx) in enumerate(folds.split(x_train, y_value)):\n    train_x, train_y = train_df.iloc[train_idx], y_value.iloc[train_idx]\n    valid_x, valid_y = train_df.iloc[valid_idx], y_value.iloc[valid_idx]\n\n    evals_result_dict = {} \n    dtrain = lgbm.Dataset(train_x, label=train_y)\n    dvalid = lgbm.Dataset(valid_x, label=valid_y)\n\n    clf = lgbm.train(lgbm_meta_params, train_set=dtrain, num_boost_round=5000, valid_sets=[dtrain, dvalid],\n                           early_stopping_rounds=200, evals_result=evals_result_dict, verbose_eval=500)\n\n    predict = clf.predict(valid_x)\n    cv_score = log_loss(valid_y, predict )\n    total_score += cv_score\n    best_iteration = max(best_iteration, clf.best_iteration)\n    print('Fold {} LogLoss : {}'.format(n_fold + 1, cv_score ))\n    lgbm.plot_metric(evals_result_dict)\n    plt.show()\n\nprint(\"Best Iteration\", best_iteration)\nprint(\"Total LogLoss\", total_score \/ NFOLD)\nprint(\"Baseline model Score Diff\", total_score \/ NFOLD - baseline_tree_score)","5746f557":"dtrain = lgbm.Dataset(x_train, label=y_value)\nclf = lgbm.train(lgbm_meta_params, train_set=dtrain, num_boost_round=5000)\npredict_stacking = clf.predict(x_test)","41f089fd":"submission_stacking = pd.read_csv('..\/input\/kaggletutorial\/sample_submission.csv')\nsubmission_stacking['Cover_Type'] = predict_stacking\nsubmission_stacking.to_csv('submission_stacking.csv', index=False)","24c33d32":"submission_et = pd.read_csv('..\/input\/kaggletutorial\/sample_submission.csv')\nsubmission_et['Cover_Type'] = et_test\nsubmission_et.to_csv('submission_et.csv', index=False)","4ae175c6":"submission_rf = pd.read_csv('..\/input\/kaggletutorial\/sample_submission.csv')\nsubmission_rf['Cover_Type'] = rf_test\nsubmission_rf.to_csv('submission_rf.csv', index=False)","0bbaf194":"# \uc2dc\uac04\uad00\uacc4\uc0c1 \uc81c\ud55c\ud588\ub358 LightGBM\uc758 HyperParameter\ub97c \uc870\uc808\ud574\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.\n\ub9ce\uc740 Feature\ub97c \ucd94\uac00\ud588\uae30 \ub54c\ubb38\uc5d0 \uc880 \ub354 Iteration\uc744 \uae38\uac8c \ud574\uc90d\ub2c8\ub2e4.","a42fdafc":"# Neural Network Model\ub3c4 \uc81c\ucd9c\ud574\ubd05\ub2c8\ub2e4.","a89ac1fb":"# Weighted Average","59a7ca7a":"# \uc9c0\uae08\uae4c\uc9c0 \uc218\ud589\ud588\ub358 \uc804\ucc98\ub9ac\uacfc\uc815\uc785\ub2c8\ub2e4.","df95da15":"# Stacking"}}