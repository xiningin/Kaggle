{"cell_type":{"44cc665c":"code","5b9af552":"code","9d9d8f0f":"code","37abe1fb":"code","8a264a2e":"code","b5630a2f":"code","d684b47e":"code","2808aa33":"code","804926a4":"code","2396ad08":"code","8772dd2e":"code","43a2e31e":"code","a0f70e9c":"code","3b19632b":"code","122d4b81":"code","2894745b":"code","ae852b4e":"code","fa51db57":"markdown","46d317c0":"markdown","a8160da3":"markdown","cccc3abd":"markdown"},"source":{"44cc665c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cudf\npd.set_option('display.max_columns', 500)\n\n\n# Standard plotly imports\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\nimport cufflinks\nimport cufflinks as cf\nimport plotly.figure_factory as ff\nimport os\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5b9af552":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","9d9d8f0f":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)","37abe1fb":"%%time\ntrain_cudf  = cudf.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\ntrain = train_cudf.to_pandas()\ndel train_cudf\nfeatures = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\nexample_test = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\nsample_prediction_df = pd.read_csv('..\/input\/jane-street-market-prediction\/example_sample_submission.csv')\nprint (\"Data is loaded!\")","8a264a2e":"print('train shape is {}'.format(train.shape))\nprint('features shape is {}'.format(features.shape))\nprint('example_test shape is {}'.format(example_test.shape))\nprint('sample_prediction_df shape is {}'.format(sample_prediction_df.shape))","b5630a2f":"train.head()","d684b47e":"missing_values_count = train.isnull().sum()\nprint (missing_values_count)\ntotal_cells = np.product(train.shape)\ntotal_missing = missing_values_count.sum()\nprint (\"% of missing data = \",(total_missing\/total_cells) * 100)","2808aa33":"train = train[train['weight'] != 0]\n\ntrain['action'] = (train['resp'].values > 0).astype('int')\n\n\nX_train = train.loc[:, train.columns.str.contains('feature')]\ny_train = train.loc[:, 'action']","804926a4":"# Model params and some modifications taked from this kernel\n# https:\/\/www.kaggle.com\/wilddave\/xgb-starter\n#X_train = X_train.fillna(-999)\n\n# Fill null cell with mean of columns\nf_mean = X_train.mean() \nX_train.fillna(f_mean)","2396ad08":"x = train['action'].value_counts().index\ny = train['action'].value_counts().values\n\ntrace2 = go.Bar(\n     x=x ,\n     y=y,\n     marker=dict(\n         color=y,\n         colorscale = 'Viridis',\n         reversescale = True\n     ),\n     name=\"Imbalance\",    \n )\nlayout = dict(\n     title=\"Data imbalance - action\",\n     #width = 900, height = 500,\n     xaxis=go.layout.XAxis(\n     automargin=True),\n     yaxis=dict(\n         showgrid=False,\n         showline=False,\n         showticklabels=True,\n #         domain=[0, 0.85],\n     ), \n)\nfig1 = go.Figure(data=[trace2], layout=layout)\niplot(fig1)","8772dd2e":"del x, y, train, features, example_test, sample_prediction_df","43a2e31e":"#X_train = X_train.fillna(f_mean)","a0f70e9c":"# Fully connect\nimport keras\nfrom keras import backend as K\nfrom numpy import loadtxt\nfrom tensorflow.keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.layers import Input, Concatenate, concatenate, BatchNormalization\nfrom keras.models import Model\nfrom keras.layers import Dense, Conv1D, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.layers import LSTM, Reshape\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom keras.callbacks import TensorBoard\nfrom tensorflow_addons.layers import WeightNormalization\nfrom functools import partial\nimport tensorflow as tf\n\nInputLayer = Input(shape=(X_train.shape[1], 1))\n\n# ConvLayer = Conv1D(filters=20,\n#                            kernel_size=20,\n#                            padding='valid',\n#                            activation='sigmoid',\n#                            strides=1)(InputLayer)\nLSTM_Layer = LSTM(X_train.shape[1])(InputLayer)\n# PoolingLayer = GlobalMaxPooling1D()(LSTM_Layer)\n# PoolingLayer = BatchNormalization()(PoolingLayer)\nPoolingLayer = BatchNormalization()(LSTM_Layer)\nmerge = Dropout(0.4)(PoolingLayer)\n\nmerge = Dense(20, activation='sigmoid')(merge)\n\nOutputLayer = Dense(1,\n                                        activation='sigmoid',\n                                        #kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4),\n                                        #activity_regularizer=regularizers.l2(1e-3)\n                   )(merge)\n\nmodel = Model(inputs=InputLayer, outputs=OutputLayer)\n# merged = WeightNormalization(Dense(1000, activation='relu'))(merged)\n# merged = BatchNormalization()(merged)\nMETRICS = [\n            'accuracy'\n        ]\nmodel.compile(loss='binary_crossentropy',\n              #loss=\"binary_crossentropy\",\n              optimizer='adam',\n              metrics=METRICS)","3b19632b":"X_train_val = X_train.sample(2000,random_state = 1)\ny_train_val = y_train.loc[X_train_val.index]\n\nvalidation = (X_train_val, y_train_val)\n\nX_train_sampling = X_train.drop(X_train_val.index)\ny_train_sampling = y_train.loc[X_train_sampling.index]\n\n\n# for random_state in range(10):\n#     X_train_batch = X_train_sampling.sample(1000,random_state = random_state)\n#     y_train_batch = y_train.loc[X_train_batch.index]\n    \n#     model.fit(X_train_batch, y_train_batch, validation_data = validation, epochs=50)\n\n    \n","122d4b81":"for random_state in range(10):\n    X_train_batch = X_train_sampling.sample(1000,random_state = random_state)\n    y_train_batch = y_train.loc[X_train_batch.index]\n    break\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=11,\n    min_child_weight=9.15,\n    gamma=0.59,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.7,\n    alpha=10.4,\n    nthread=5,\n    missing=-999,\n    random_state=2020,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n\nclf.fit(X_train_batch, y_train_batch)","2894745b":"# %time clf.fit(X_train, y_train)","ae852b4e":"for (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    \n    #y_preds = model.predict(X_test)\n    y_preds =  clf.predict(X_test)\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","fa51db57":"## Create Environment","46d317c0":"# Is the data balanced or not?","a8160da3":"## Training\n##### To activate GPU usage, simply use tree_method='gpu_hist' (took me an hour to figure out, I wish XGBoost documentation was clearer about that).","cccc3abd":"### Missing Values Count"}}