{"cell_type":{"80f4c1af":"code","cb5bcff4":"code","cd9de8fa":"code","5312137c":"code","d00dec8a":"code","776f858a":"code","c5e4e99a":"code","6bd1f406":"code","4c789315":"code","9c69286b":"code","f0f4cae4":"code","faacca6d":"code","9462ff06":"code","7c269cfc":"code","64b3d326":"code","ff987768":"code","53863eae":"code","5db38f52":"code","3b4ed7a8":"code","eddd679e":"code","eb58c6d7":"code","fecddff6":"code","3a9e5696":"code","f80e2c5a":"code","07cb5c91":"code","3db18982":"code","29f6ace4":"code","eb725062":"code","392e3337":"code","b13e2450":"code","4e181084":"code","090e4ba3":"code","c252d941":"code","c0cce569":"code","396a1aa3":"code","8c90a489":"code","b5006475":"code","dae0b6c6":"code","77225a0a":"code","f0f1da01":"code","75b9d8c7":"code","f724a431":"code","becf44a2":"code","315ffa36":"code","2e2e6032":"code","77bb8ed7":"code","9291c406":"code","259f4dfe":"code","cd58e454":"code","47a7f478":"markdown","6077599b":"markdown","8e7dd20b":"markdown","159fd88f":"markdown","df51a2bb":"markdown","357aae78":"markdown","ddf34d5a":"markdown","aa40316a":"markdown","6606af92":"markdown","7ddc5e72":"markdown","44926adf":"markdown","d8e02baf":"markdown","4451b22f":"markdown","5449487b":"markdown","ec3279b8":"markdown","f666e6ab":"markdown","9ac68df8":"markdown","b8500eca":"markdown","2a2fe0ea":"markdown","8f2a6d21":"markdown","3d0e203c":"markdown","4186a11a":"markdown","59f31828":"markdown","62c393ca":"markdown","afed2da4":"markdown","dabe7f66":"markdown","e5d57b4d":"markdown","b799702b":"markdown","0c4e6473":"markdown","c9da7a61":"markdown","29efe585":"markdown","efb62e41":"markdown","07a2a7ef":"markdown","9b885168":"markdown","4cca8c33":"markdown","520ec9ea":"markdown","2458a20d":"markdown","7708ae39":"markdown","bf9302e7":"markdown","2af025d7":"markdown","c6bce8fb":"markdown","3d49b82b":"markdown","8282b782":"markdown","b6471f32":"markdown","e30533be":"markdown","d1ffa49b":"markdown","bd64de53":"markdown","ec5fcbed":"markdown","07f8f375":"markdown","c7f5cb23":"markdown","194d5bb1":"markdown","7a933d33":"markdown","c5f6428e":"markdown","1ab5b924":"markdown","04b765a6":"markdown","e01b337a":"markdown","2c506546":"markdown","70a837f1":"markdown","475eaf55":"markdown","03e898c6":"markdown","f66366ce":"markdown","75d99aee":"markdown","342b4e2f":"markdown","0f478476":"markdown","809bc482":"markdown","039f6e34":"markdown","111e6ff3":"markdown","06ca86e5":"markdown","0f159481":"markdown"},"source":{"80f4c1af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport plotly\nfrom plotly.offline import iplot,init_notebook_mode\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n# Read csv\ndf = pd.read_csv(\"..\/input\/creditcard.csv\")","cb5bcff4":"# Explore the features avaliable in our dataframe\ndf.shape\ndf.info()\ndf.head()\ndf.describe()\nprint(df.Amount.describe())","cd9de8fa":"# Count number of fraud cases and calculate the proportion\nprint(\"Class type and numbers:\\n\",df[\"Class\"].value_counts())\nprint(\"Fraud Proportion\",len(df[df[\"Class\"]==1])\/len(df))","5312137c":"# Plot how fraud and non-fraud cases are scattered \ntrace1 = go.Scatter(x=df[df['Class'] == 0]['V1'],\n                    y=df[df['Class'] == 0]['V2'],\n                    name=\"Normal Cases\",\n                    mode = 'markers',\n                    marker=dict(color=\"blue\"))\n\ntrace2 = go.Scatter(x=df[df['Class'] == 1]['V1'],\n                    y=df[df['Class'] == 1]['V2'],\n                    name=\"Normal Cases\",\n                    mode = 'markers',\n                    marker=dict(color=\"green\"))\n\nlayout = go.Layout(title=\"Scatter plot of V1 vs V2 in fraud and normal cases\",\n                   xaxis=dict(title=\"V1\"),\n                   yaxis=dict(title=\"V2\"))","d00dec8a":"data=[trace1,trace2]\n\nfig = go.Figure(data=data,layout=layout)\n\niplot(fig)","776f858a":"plt.scatter(df.loc[df['Class'] == 0]['V1'], df.loc[df['Class'] == 0]['V2'], label=\"Class #0\", alpha=0.5, linewidth=0.15)\nplt.scatter(df.loc[df['Class'] == 1]['V1'], df.loc[df['Class'] == 1]['V2'], label=\"Class #1\", alpha=0.5, linewidth=0.15,c='r')\nplt.show()","c5e4e99a":"import seaborn as sns\nfig, ax = plt.subplots(1, 2, figsize=(18,4))\n\n# Plot the distribution of \"Time\" and \"Amount\" features \nsns.distplot(df['Time'].values\/(60*60), ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Time', fontsize=14)\nax[0].set_xlim([min(df['Time'].values\/(60*60)), max(df['Time'].values\/(60*60))])\n\nsns.distplot(df['Amount'].values, ax=ax[1], color='g')\nax[1].set_title('Distribution of Transaction Amount', fontsize=14)\nax[1].set_xlim([min(df['Amount'].values), max(df['Amount'].values)])\n\nplt.show()","6bd1f406":"# Seperate total data into non-fraud and fraud cases\ndf_nonfraud = df[df.Class == 0] \ndf_fraud = df[df.Class == 1] \n\n# Summarize of two seperate date\nprint(df_nonfraud.Amount.describe())\nprint('_'*25)\nprint(df_fraud.Amount.describe())\nprint('_'*25)\n\n# Compare statistics and see differences between fraud and normal transactions\nfrom scipy import stats\nF, p = stats.f_oneway(df['Amount'][df['Class'] == 0], df['Amount'][df['Class'] == 1])\nprint(\"F:\", F)\nprint(\"p:\",p)","4c789315":"# Plot of high value transactions($200-$2000)\nbins = np.linspace(200, 2000, 100)\nplt.hist(df_nonfraud.Amount, bins, alpha=1, normed=True, label='Non-Fraud')\nplt.hist(df_fraud.Amount, bins, alpha=1, normed=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Amount by percentage of transactions (transactions \\$200-$2000)\")\nplt.xlabel(\"Transaction amount (USD)\")\nplt.ylabel(\"Percentage of transactions (%)\")\nplt.show()","9c69286b":"# Plot of transactions in 48 hours\nbins = np.linspace(0, 48, 48) #48 hours\nplt.hist((df_nonfraud.Time\/(60*60)), bins, alpha=1, normed=True, label='Non-Fraud')\nplt.hist((df_fraud.Time\/(60*60)), bins, alpha=0.6, normed=True, label='Fraud')\nplt.legend(loc='upper right')\nplt.title(\"Percentage of transactions by hour\")\nplt.xlabel(\"Transaction time from first transaction in the dataset (hours)\")\nplt.ylabel(\"Percentage of transactions (%)\")\nplt.show()","f0f4cae4":"# Plot of transactions in 48 hours\nplt.scatter((df_nonfraud.Time\/(60*60)), df_nonfraud.Amount, alpha=0.6, label='Non-Fraud')\nplt.scatter((df_fraud.Time\/(60*60)), df_fraud.Amount, alpha=0.9, label='Fraud')\nplt.title(\"Amount of transaction by hour\")\nplt.xlabel(\"Transaction time as measured from first transaction in the dataset (hours)\")\nplt.ylabel('Amount (USD)')\nplt.legend(loc='upper right')\nplt.show()","faacca6d":"# Scale \"Time\" and \"Amount\"\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\ndf['scaled_amount'] = RobustScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n\n# 'Time' feature are in second scale and we transform it to hour scale at first.\ndf['Time_new'] = df['Time']\/(60*60)\ndf['scaled_time'] = RobustScaler().fit_transform(df['Time_new'].values.reshape(-1,1))\n# Make a new dataset named \"df_scaled\" dropping out original \"Time\" and \"Amount\"\ndf_scaled = df.drop(['Time','Amount','Time_new'],axis = 1,inplace=False)\ndf_scaled.head()\ndf_scaled.describe()","9462ff06":"# Calculate pearson correlation coefficience in total dataset\ncorr = df_scaled.corr() \n\n# Plot heatmap of correlation\nf, ax = plt.subplots(1, 1, figsize=(10,8))\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})\nax.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=24)","7c269cfc":"# Calculate pearson correlation coefficience in non-fraud dataset\ndf_scaled_nonfraud = df_scaled[df_scaled.Class == 0] \ndf_scaled_fraud = df_scaled[df_scaled.Class == 1] \n\ncorr_nonfraud = df_scaled_nonfraud.corr() \ncorr_fraud = df_scaled_fraud.corr()\n\n# Plot heatmap of correlation\nf1, ax1 = plt.subplots(1, 1, figsize=(10,8))\nsns.heatmap(corr_nonfraud, cmap='coolwarm_r', annot_kws={'size':20})\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=24)\n\n# Plot heatmap of correlation\nf2, ax2 = plt.subplots(1, 1, figsize=(10,8))\nsns.heatmap(corr_fraud, cmap='coolwarm_r', annot_kws={'size':20})\nax2.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=24)","64b3d326":"features = df_scaled.drop(['Class'],axis=1, inplace=False) \nlabels = df_scaled[['Class']]","ff987768":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nimport numpy as np\ndata_all = SelectKBest(f_classif, k='all').fit(features,labels) # https:\/\/github.com\/jiehu567\/CreditCardFraud\nscores = data_all.scores_\n\n# Convert features and scores into a dataframe\n# https:\/\/stackoverflow.com\/questions\/46379095\/convert-two-numpy-array-to-dataframe\nscores = np.array(scores)\nfeature_array = np.array(features.columns)\ndf_score = pd.DataFrame({'feature':feature_array, 'score': list(scores)}, columns=['feature', 'score'])\ndf_score.sort_values(by='score',ascending=False)\n\n# Plot df_score\nplt.figure(figsize=(10,10))\ny_pos = np.arange(len(feature_array))\nbar_pos_rate = 2.5\nplt.barh(y_pos*bar_pos_rate, scores, 2, align='center', alpha=0.7)\nplt.yticks(y_pos*bar_pos_rate, feature_array)\nplt.ylabel('Features',fontsize = 20)\nplt.title('Feature Scores Rank',fontsize = 20)\nplt.xlabel(\"Score\",fontsize = 20)\nplt.show()","53863eae":"# Define the prep_data function to extrac features \ndef prep_data(df):\n    X = df.drop(['Class'],axis=1, inplace=False) #  \n    X = np.array(X).astype(np.float)\n    y = df[['Class']]  \n    y = np.array(y).astype(np.float)\n    return X,y\n\n# Create X and y from the prep_data function \nX, y = prep_data(df_scaled)","5db38f52":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report,roc_auc_score,average_precision_score\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y,random_state=0)\n\n# Define Logistic Regression model\nlogreg = LogisticRegression() \n\n# Fit your pipeline onto your training set \nlogreg.fit(X_train, y_train) \n\n# Obtain predictions by fitting the model onto the test data \ny_predicted = logreg.predict(X_test)\n\n# Calculate Area Under the Receiver Operating Characteristic Curve \nprobs = logreg.predict_proba(X_test)\nroc_auc = roc_auc_score(y_test, probs[:, 1])\nprint('ROC AUC Score:',roc_auc)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\nprint('Average Precision:',average_precision)\n\n# Obtain the results from the classification report and confusion matrix \nprint('Classifcation report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = y_predicted))","3b4ed7a8":"from sklearn.metrics import roc_curve, precision_recall_curve\n\n# Create true and false positive rates\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_predicted)\n\n# Define a roc_curve function\ndef plot_roc_curve(false_positive_rate,true_positive_rate,roc_auc):\n    plt.plot(false_positive_rate, true_positive_rate, linewidth=5, label='AUC = %0.3f'% roc_auc)\n    plt.plot([0,1],[0,1], linewidth=5)\n    plt.xlim([-0.01, 1])\n    plt.ylim([0, 1.01])\n    plt.legend(loc='upper right')\n    plt.title('Receiver operating characteristic curve (ROC)')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()\n\n# Plot the roc curve \nplot_roc_curve(false_positive_rate,true_positive_rate,roc_auc)\n\n# Obtain precision and recall \nprecision, recall, thresholds = precision_recall_curve(y_test, y_predicted)\n\n# Define a precision_recall_curve function\ndef plot_pr_curve(recall, precision, average_precision):\n    plt.step(recall, precision, color='b', alpha=0.2, where='post')\n    plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.ylim([0.0, 1.05])\n    plt.xlim([0.0, 1.0])\n    plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n    plt.show()\n\n# Plot recall precision curve\nplot_pr_curve(recall, precision, average_precision)","eddd679e":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.pipeline import Pipeline # Inorder to avoid testing model on sampled data\nfrom sklearn.metrics import confusion_matrix, classification_report,roc_auc_score,average_precision_score\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y,random_state=0)\n\n# Define which resampling method and which ML model to use in the pipeline\nresampling = BorderlineSMOTE(kind='borderline-2',random_state=0) # instead SMOTE(kind='borderline2') \nlogreg = LogisticRegression() \n\n# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\npipeline = Pipeline([('SMOTE', resampling), ('Logistic Regression', logreg)])\n\n# Fit your pipeline onto your training set \npipeline.fit(X_train, y_train) \n\n# Obtain predictions by fitting the model onto the test data \ny_predicted = pipeline.predict(X_test)\n\n# Calculate Area Under the Receiver Operating Characteristic Curve \nprobs = pipeline.predict_proba(X_test)\nroc_auc = roc_auc_score(y_test, probs[:, 1])\nprint('ROC AUC Score:',roc_auc)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\nprint('Average Precision:',average_precision)\n\n# Obtain the results from the classification report and confusion matrix \nprint('Classifcation report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = y_predicted))","eb58c6d7":"# Create true and false positive rates\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_predicted)\n\n# Plot the roc curve \nplot_roc_curve(false_positive_rate,true_positive_rate,roc_auc)\n\n# Obtain precision and recall \nprecision, recall, thresholds = precision_recall_curve(y_test, y_predicted)\n\n# Plot recall precision curve\nplot_pr_curve(recall, precision, average_precision)","fecddff6":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, average_precision_score\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y,random_state=0)\n\n# Instantiate our model\nknn = KNeighborsClassifier() \n\n# Set the range of hyperparameter(n_neighbors) for knn model\npara_grid = { 'n_neighbors': np.arange(1,10) }\n\n# Fit your pipeline onto your training set \nknn_cv = GridSearchCV(knn,para_grid)\nknn_cv.fit(X_train, y_train) \n\n# Print the best hyperparameters\nprint('Best hyperparameters:', knn_cv.best_params_)","3a9e5696":"# Obtain predictions by fitting the model onto the test data \ny_predicted = knn_cv.predict(X_test)\n\n# Calculate Area Under the Receiver Operating Characteristic Curve \nprobs = knn_cv.predict_proba(X_test)\nroc_auc = roc_auc_score(y_test, probs[:, 1])\nprint('ROC AUC Score:',roc_auc)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\nprint('Average Precision:',average_precision)\n\n# Obtain the results from the classification report and confusion matrix \nprint('Classifcation report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = y_predicted))","f80e2c5a":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.pipeline import Pipeline # Inorder to avoid testing model on sampled data\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y,random_state=0)\n\n# Define which resampling method and which ML model to use in the pipeline\nresampling = BorderlineSMOTE(kind='borderline-2',random_state=0) # instead SMOTE(kind='borderline2') \nknn = KNeighborsClassifier()\n\n# Setup the pipeline\nsteps = [('SMOTE', resampling),('knn', knn)]\npipeline = Pipeline(steps)\n\n# Specify the hyperparameter space\nparameters = {'knn__n_neighbors':np.arange(1,50)}\n\n# Instantiate the GridSearchCV object: cv\ncv = GridSearchCV(pipeline,param_grid=parameters)\n\n# Fit to the training set\ncv.fit(X_train, y_train)\n\n# Predict the labels of the test set: y_pred\ny_predicted = cv.predict(X_test)\n\n# Calculate Area Under the Receiver Operating Characteristic Curve \nprobs = cv.predict_proba(X_test)\nroc_auc = roc_auc_score(y_test, probs[:, 1])\nprint('ROC AUC Score:',roc_auc)\n\n# Obtain the results from the classification report and confusion matrix \nprint('Classifcation report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n', confusion_matrix(y_true = y_test, y_pred = y_predicted))\n\n# Create true and false positive rates abd plot roc curve\nfalse_positive_rate, true_positive_rate, threshold = roc_curve(y_test, y_predicted)\nplot_roc_curve(false_positive_rate,true_positive_rate,roc_auc)\n\n# Obtain precision and recall and plot pr curve\nprecision, recall, thresholds = precision_recall_curve(y_test, y_predicted)\naverage_precision = average_precision_score(y_test, y_predicted)\nplot_pr_curve(recall, precision, average_precision)","07cb5c91":"# Import the decision tree model from sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y, random_state=0)\n\n# Fit a logistic regression model to our data\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\n# Obtain model predictions\ny_predicted = model.predict(X_test)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\n\n# Obtain precision and recall \nprecision, recall, _ = precision_recall_curve(y_test, y_predicted)\n\n# Plot the recall precision tradeoff\nplot_pr_curve(recall, precision, average_precision)\n\n# Print the classifcation report and confusion matrix\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))","3db18982":"# Import the pipeline module we need for this from imblearn\nfrom imblearn.pipeline import Pipeline \nfrom imblearn.over_sampling import BorderlineSMOTE\n\n# Define which resampling method and which ML model to use in the pipeline\nresampling = BorderlineSMOTE(kind='borderline-2',random_state=0) # instead SMOTE(kind='borderline2') \nmodel = DecisionTreeClassifier() \n\n# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\npipeline = Pipeline([('SMOTE', resampling), ('Decision Tree Classifier', model)])\n\n# Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data \npipeline.fit(X_train, y_train) \ny_predicted = pipeline.predict(X_test)\n\n# Obtain the results from the classification report and confusion matrix \nprint('Classifcation report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',  confusion_matrix(y_true = y_test, y_pred = y_predicted))","29f6ace4":"# Import the Random Forest Classifier model from sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n\n# Fit a logistic regression model to our data\nmodel = RandomForestClassifier(random_state=5)\nmodel.fit(X_train, y_train)\n\n# Obtain model predictions\ny_predicted = model.predict(X_test)\n\n# Predict probabilities\nprobs = model.predict_proba(X_test)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\n\n# Obtain precision and recall \nprecision, recall, _ = precision_recall_curve(y_test, y_predicted)\n\n# Plot the recall precision tradeoff\nplot_pr_curve(recall, precision, average_precision)\n\n# Print the classifcation report and confusion matrix\nprint(accuracy_score(y_test, y_predicted))\nprint(\"AUC ROC score: \", roc_auc_score(y_test, probs[:,1]))\n\nprint('Classification report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_true = y_test, y_pred = y_predicted))","eb725062":"# Import the pipeline module we need for this from imblearn\nfrom imblearn.pipeline import Pipeline \nfrom imblearn.over_sampling import BorderlineSMOTE\n\n# Define which resampling method and which ML model to use in the pipeline\n\nresampling = BorderlineSMOTE(kind='borderline-2',random_state=0) # instead SMOTE(kind='borderline2') \nmodel = RandomForestClassifier() \n\n# Define the pipeline, tell it to combine SMOTE with the Logistic Regression model\npipeline = Pipeline([('SMOTE', resampling), ('Random Forest Classifier', model)])\n\n# Fit your pipeline onto your training set and obtain predictions by fitting the model onto the test data \npipeline.fit(X_train, y_train) \ny_predicted = pipeline.predict(X_test)\n\n# Predict probabilities\nprobs = model.predict_proba(X_test)\n\nprint(accuracy_score(y_test, y_predicted))\nprint(\"AUC ROC score: \", roc_auc_score(y_test, probs[:,1]))\n# Obtain the results from the classification report and confusion matrix \n\nprint('Classifcation report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n',  confusion_matrix(y_true = y_test, y_pred = y_predicted))","392e3337":"# Import the Random Forest Classifier model from sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n\n# Define the model with balanced subsample\nmodel = RandomForestClassifier(bootstrap=True,\n                               class_weight={0:1, 1:12}, # 0: non-fraud , 1:fraud\n                               criterion='entropy',\n                               max_depth=10, # Change depth of model\n                               min_samples_leaf=10, # Change the number of samples in leaf nodes\n                               n_estimators=20, # Change the number of trees to use\n                               n_jobs=-1, \n                               random_state=5)\n\n# Fit your training model to your training set\nmodel.fit(X_train, y_train)\n\n# Obtain the predicted values and probabilities from the model \ny_predicted = model.predict(X_test)\n\n# Calculate probs\nprobs = model.predict_proba(X_test)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\n\n# Obtain precision and recall \nprecision, recall, _ = precision_recall_curve(y_test, y_predicted)\n\n# Plot the recall precision tradeoff\nplot_pr_curve(recall, precision, average_precision)\n\n# Print the roc auc score, the classification report and confusion matrix\nprint(\"auc roc score: \", roc_auc_score(y_test, probs[:,1]))\nprint('Classifcation report:\\n', classification_report(y_test, y_predicted))\nprint('Confusion matrix:\\n', confusion_matrix(y_test, y_predicted))","b13e2450":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Define the parameter sets to test\nparam_grid = {\n    'n_estimators': [1, 30], \n    'max_features': ['auto', 'log2'],  \n    'max_depth': [4, 8], \n    'criterion': ['gini', 'entropy']\n}\n\n# Define the model to use\nmodel = RandomForestClassifier(random_state=5)\n\n# Combine the parameter sets with the defined model\nCV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n\n# Fit the model to our training data and obtain best parameters\nCV_model.fit(X_train, y_train)\nCV_model.best_params_","4e181084":"from sklearn.metrics import accuracy_score,roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Build a RandomForestClassifier using the GridSearchCV parameters\nmodel = RandomForestClassifier(bootstrap=True,\n                               class_weight = {0:1,1:12},\n                               criterion = 'entropy',\n                               n_estimators = 30,\n                               max_features = 'auto',\n                               min_samples_leaf = 10,\n                               max_depth = 8,\n                               n_jobs = -1,\n                               random_state = 5)\n\n# Fit the model to your training data and get the predicted results\nmodel.fit(X_train,y_train)\ny_predicted = model.predict(X_test)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\n\n# Obtain precision and recall \nprecision, recall, _ = precision_recall_curve(y_test, y_predicted)\n\n# Plot the recall precision tradeoff\nplot_pr_curve(recall, precision, average_precision)\n\n# Print the roc_auc_score,Classifcation report and Confusin matrix\nprobs = model.predict_proba(X_test)\nprint('roc_auc_score:', roc_auc_score(y_test,probs[:,1]))\nprint('Classification report:\\n',classification_report(y_test,y_predicted))\nprint('Confusion_matrix:\\n',confusion_matrix(y_test,y_predicted))","090e4ba3":"# Import modules \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Create the training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)\n\n# Define the three classifiers to use in the ensemble\nclf1 = LogisticRegression(class_weight={0:1,1:15},random_state=5)\nclf2 = RandomForestClassifier(class_weight={0:1,1:12},\n                              criterion='entropy',\n                              max_depth=10,\n                              max_features='auto',\n                              min_samples_leaf=10, \n                              n_estimators=20,\n                              n_jobs=-1,\n                              random_state=5)\nclf3 = DecisionTreeClassifier(class_weight='balanced',random_state=5)\n\n# Combine the classifiers in the ensemble model\nensemble_model = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('dt', clf3)], voting='hard')\n\n# Fit the model to your training data and get the predicted results\nensemble_model.fit(X_train,y_train)\ny_predicted = ensemble_model.predict(X_test)\n\n# print roc auc score , Classification report and Confusion matrix of the model\nprint('Classifier report:\\n',classification_report(y_test,y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,y_predicted))","c252d941":"# Adjust weights within the Voting Classifier\n\n# Define the ensemble model\nensemble_model = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], \n                                  voting='soft', \n                                  weights=[1, 4, 1], \n                                  flatten_transform=True)\n\n# Fit the model to your training data and get the predicted results\nensemble_model.fit(X_train,y_train)\ny_predicted = ensemble_model.predict(X_test)\n\n# Calculate average precision \naverage_precision = average_precision_score(y_test, y_predicted)\n\n# Obtain precision and recall \nprecision, recall, _ = precision_recall_curve(y_test, y_predicted)\n\n# Plot the recall precision tradeoff\nplot_pr_curve(recall, precision, average_precision)\n\n# print roc auc score , Classification report and Confusion matrix of the model\nprint('Classifier report:\\n',classification_report(y_test,y_predicted))\nprint('Confusion matrix:\\n',confusion_matrix(y_test,y_predicted))\n","c0cce569":"ensemble_model.estimators_","396a1aa3":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\n\n# Split the data into train set and test set\ntrain,test = train_test_split(df,test_size=0.3,random_state=0)\n\n# Get the arrays of features and labels in train dataset\nfeatures_train = train.drop(['Time','Class'],axis=1)\nfeatures_train = features_train.values\nlabels_train = pd.DataFrame(train[['Class']])\nlabels_train = labels_train.values\n\n# Get the arrays of features and labels in test dataset\nfeatures_test = test.drop(['Time','Class'],axis=1)\nfeatures_test = features_test.values\nlabels_test = pd.DataFrame(test[[\"Class\"]])\nlabels_test = labels_test.values\n\n# Normalize the features in both train and test dataset\nfeatures_train = normalize(features_train)\nfeatures_test = normalize(features_test)","8c90a489":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix\n\nmodel = KMeans(n_clusters=2,random_state=0)\nmodel.fit(features_train)\nlabels_train_predicted = model.predict(features_train)\nlabels_test_predicted = model.predict(features_test)\n\n# Decide if model predicted label is aligned with true label \ntrue_negative,false_positive,false_negative,true_positive = confusion_matrix(labels_train,labels_train_predicted).ravel()\nreassignflag = true_negative + true_positive < false_positive + false_negative\nprint(reassignflag)\n\n\nlabels_test_predicted = 1- labels_test_predicted\n","b5006475":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score,f1_score\n# Calculating confusion matrix for kmeans\nprint('Confusion Matrix:\\n',confusion_matrix(labels_test,labels_test_predicted))\n\n# Scoring kmeans\n\nprint('kmeans_precison_score:', precision_score(labels_test,labels_test_predicted))\nprint('kmeans_recall_score:', recall_score(labels_test,labels_test_predicted))\nprint('kmeans_accuracy_score:', accuracy_score(labels_test,labels_test_predicted))\nprint('kmeans_f1_score:',f1_score(labels_test,labels_test_predicted))","dae0b6c6":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\n\n# Split the data into train set and test set\ntrain,test = train_test_split(df,test_size=0.3,random_state=0)\n\n# Get the arrays of features and labels in train dataset\nfeatures_train = train.drop(['Time','Class'],axis=1)\nfeatures_train = features_train.values\nlabels_train = pd.DataFrame(train[['Class']])\nlabels_train = labels_train.values\n\n# Get the arrays of features and labels in test dataset\nfeatures_test = test.drop(['Time','Class'],axis=1)\nfeatures_test = features_test.values\nlabels_test = pd.DataFrame(test[[\"Class\"]])\nlabels_test = labels_test.values\n\n# Normalize the features in both train and test dataset\nfeatures_train = normalize(features_train)\nfeatures_test = normalize(features_test)","77225a0a":"from sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import confusion_matrix\n\nmodel = MiniBatchKMeans(n_clusters=2,random_state=0)\nmodel.fit(features_train)\nlabels_train_predicted = model.predict(features_train)\nlabels_test_predicted = model.predict(features_test)\n\n# Decide if model predicted label is aligned with true label \ntrue_negative,false_positive,false_negative,true_positive = confusion_matrix(labels_train,labels_train_predicted).ravel()\nreassignflag = true_negative + true_positive < false_positive + false_negative\nprint(reassignflag)\n","f0f1da01":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score,f1_score\n# Calculating confusion matrix for kmeans\nprint('Confusion Matrix:\\n',confusion_matrix(labels_test,labels_test_predicted))\n\n# Scoring kmeans\n\nprint('kmeans_precison_score:', precision_score(labels_test,labels_test_predicted))\nprint('kmeans_recall_score:', recall_score(labels_test,labels_test_predicted))\nprint('kmeans_accuracy_score:', accuracy_score(labels_test,labels_test_predicted))\nprint('kmeans_f1_score:',f1_score(labels_test,labels_test_predicted))","75b9d8c7":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Make another copy of df and drop the unimportant \"Time\" feature\ndata = df.drop(['Time'], axis=1) \n\n# Use scikit\u2019s StandardScaler on the \"Amount\" feature\n# The scaler removes the mean and scales the values to unit variance\ndata['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))\n\n# Create the training and testing sets\nX1_train, X1_test = train_test_split(data, test_size=.3, random_state=0)\nX1_train = X1_train[X1_train.Class == 0] # train the model on normal transactions\nX1_train = X1_train.drop(['Class'], axis=1)\n\ny1_test = X1_test['Class']\nX1_test  = X1_test.drop(['Class'], axis=1) #drop the class column\n\n\n#transform to ndarray\nX1_train = X1_train.values\nX1_test = X1_test.values\nX1_train.shape","f724a431":"import tensorflow as tf\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\ninput_dim = X1_train.shape[1] #num of columns, 29\nencoding_dim = 14\nhidden_dim = int(encoding_dim \/ 2)\nlearning_rate = 1e-5\n\ninput_layer = Input(shape=(input_dim, ))\nencoder = Dense(encoding_dim, \n                activation=\"tanh\", \n                activity_regularizer=regularizers.l1(learning_rate))(input_layer)\nencoder = Dense(hidden_dim, activation=\"relu\")(encoder)\ndecoder = Dense(hidden_dim, activation='tanh')(encoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\nautoencoder = Model(inputs=input_layer, outputs=decoder)","becf44a2":"nb_epoch = 100\nbatch_size = 128\nautoencoder.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\n\ncheckpointer = ModelCheckpoint(filepath='autoencoder_fraud.h5',\n                               save_best_only=True,\n                               verbose=0)\n\ntensorboard = TensorBoard(log_dir='.\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\n\nhistory = autoencoder.fit(X1_train, X1_train,\n                          epochs=nb_epoch,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          validation_data=(X1_test, X1_test),\n                          verbose=1,\n                          callbacks=[checkpointer, tensorboard]).history\nload_model('autoencoder_fraud.h5')\n","315ffa36":"plt.plot(history['loss'])\nplt.plot(history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')","2e2e6032":"predictions = autoencoder.predict(X1_test)\nmse = np.mean(np.power(X1_test - predictions, 2), axis=1)\ndf_error = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y1_test})\ndf_error.describe()","77bb8ed7":"# Import modules\nfrom sklearn.metrics import auc, roc_curve,precision_recall_curve\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import recall_score,f1_score,precision_recall_fscore_support\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(df_error.true_class, df_error.reconstruction_error)\nroc_auc = auc(false_positive_rate, true_positive_rate)\n\n# Plot the roc curve \nplot_roc_curve(false_positive_rate,true_positive_rate,roc_auc)","9291c406":"precision, recall, thresholds = precision_recall_curve(df_error.true_class, df_error.reconstruction_error)\n\n# Plot recall precision tradeoff\nplt.plot(recall, precision, linewidth=5, label='Precision-Recall curve')\nplt.title('Recall vs Precision')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nprint(plt.show())\n\n# Plot precision and recall for different thresholds\nplt.plot(thresholds, precision[1:], label=\"Precision\",linewidth=5)\nplt.plot(thresholds, recall[1:], label=\"Recall\",linewidth=5)\nplt.title('Precision and recall for different threshold values')\nplt.xlabel('Threshold')\nplt.ylabel('Precision\/Recall')\nplt.legend()\nprint(plt.show())","259f4dfe":"# Set a threshold\nset_threshold = 5\ngroups = df_error.groupby('true_class')\nfig, ax = plt.subplots()\n\nfor name, group in groups:\n    ax.plot(group.index, \n            group.reconstruction_error, \n            marker='o', \n            ms=3.5, \n            linestyle='',\n            label= \"Fraud\" if name == 1 else \"Nonfraud\")\n    \nax.hlines(set_threshold, \n          ax.get_xlim()[0], \n          ax.get_xlim()[1], \n          colors=\"r\", \n          zorder=100, \n          label='Threshold')\n\nax.legend()\nplt.title(\"Reconstruction error for different classes\")\nplt.ylabel(\"Reconstruction error\")\nplt.xlabel(\"Data point index\")\nplt.show()","cd58e454":"y_pred = [1 if e > set_threshold else 0 for e in df_error.reconstruction_error.values]\nprint('Confusion_matrix:\\n',confusion_matrix(df_error.true_class, y_pred))","47a7f478":"ROC AUC Score: 0.9698899526761653   \nAverage Precision: 0.551956809548183  \nprecision: 0.88   \nrecall: 0.63   \nf1-score: 0.73  \nThe recall does not perform well.","6077599b":"It would be hard to draw a line that cleanly separates fraud and non-fraud transactions.   \nBut it seems that fraudsters are more likely to make small transactions. ","8e7dd20b":"## Distribution of \"Time\" and \"Amount\"","159fd88f":"## Model Evaluation","df51a2bb":"<a id=\"4\"><\/a>\n# 4: K-Nearest Neighbors Classifer","357aae78":"<a id=\"7\"><\/a>\n# Module 7: Random Forest Classifier","ddf34d5a":"## Random Forest Classifier Model adjustments ","aa40316a":"<a id=\"10\"><\/a>\n# 10: MiniBatchKMeans Clustering","6606af92":"## Model results using GridSearchCV","7ddc5e72":"## Model Evaluation","44926adf":"## V1-V28 Feature Selection\nThere are several [feature selection methods](https:\/\/www.datacamp.com\/community\/tutorials\/feature-selection-python)\nHere we'll try Filter methods With ANOVA F-Values. ","d8e02baf":"A very simple Deep Autoencoder in Keras can reconstruct what non fraudulent transactions looks like this. ","4451b22f":"<a id=\"11\"><\/a>\n# 11: Autoencoders \n","5449487b":"## Prepare training data and testing data\n\nFirst, drop the \"Time\" column (not going to use it as it is unimportant) and use the scikit\u2019s StandardScaler on the Amount.  The scaler removes the mean and scales the values to unit variance.  \nAutoencoder is gonna be a bit different from what we are used to. We will create this situation by training our model on the normal transactions, only.   \nReserve 30% of our data for testing","ec3279b8":"## Decision Tree Classifier with SMOTE Data","f666e6ab":"<a id=\"5\"><\/a>\n# Module 5: Support Vector Machine","9ac68df8":"### Summary:\n* We have 284807 entries within 30 features and 1 target (Class).   \n* There are no \"Null\" values, so no need to work on ways to replace missing values.   \n* The mean of all the mounts made is relatively small, approximately USD 88.  \n* Most of the transactions were Non-Fraud (99.83%) of the time, while Fraud transactions occurs (0.17%) of the time in the dataframe.  ","b8500eca":"## Feature Scaling\nAs we know before, features V1-V28 have been transformed by PCA and scaled already.  Whereas feature \"Time\" and \"Amount\" have not. And considering that we will analyze these two features with other V1-V28, they should better be scaled before we train our model using various algorithms.\nHere is [why and how](https:\/\/medium.com\/@ian.dzindo01\/feature-scaling-in-python-a59cc72147c1).  \nWhich scaling mehtod should we use?  \nThe **Standard Scaler** is not recommended as \"Time\" and \"Amount\" features are not normally distributed.   \nThe **Min-Max Scaler** is also not recommende as there are noticeable outliers in feature \"Amount\".  \nThe **Robust Scaler** are robust to outliers: **(xi\u2013Q1(x))\/( Q3(x)\u2013Q1(x))** (Q1 and Q3 represent 25% and 75% quartiles).   \nSo we choose **Robust Scaler** to scale these two features. ","2a2fe0ea":"We can detect 91 out of 147 fraud cases in the test dataset.  \nBut there are 17361 false positive cases which indicated that our KMeans model needs to be improved by selecting good features.  \n","8f2a6d21":"## Reconstruction Error\nAutoencoders are trained to reduce reconstruction error. \n","3d0e203c":"###  Transaction Amount Visualization\nExpect a lot of low-value transactions to be uninteresting (buying cups of coffee, lunches, etc).   \nOnly visualizes the transactions between USD 200 and 2000. ","4186a11a":"## Random Forest Classifier with SMOTE Data Catch Fraud ","59f31828":"The weight option allows you to play with the individual models to get the best final mix for your fraud detection model.  \nBut the model performance does not improve.","62c393ca":"## Prepare unlabeled train and test dataset","afed2da4":"## ROC Curve \nSince we have an imbalanced data set, Receiver Operating Characteristic Curves are not that useful although it's an expected output of most binary classifiers.   \nBecause you can generate a pretty good-looking curve by just simply guessing each one is the non-fraud case. \nBut let\u2019s have a look at ROC curve first.","dabe7f66":"The results of this model just does not perform better.","e5d57b4d":"<a id=\"3\"><\/a>\n# 3: Logistic Regression","b799702b":"### Summary:\n* Time: Most transactions happended in day time.  \n* Amount: Extremely skewed. Mean of transaction amount is 88 USD and the 75% quatile is 77 USD. ","0c4e6473":"The model seems to be performing well enough, although there is significant room for improvement by adding more hidden layers.   \nMore hidden layers would allow this network to encode more complex relationships between the input features. \nThe loss of our current model seems to be converging and more training epochs are not likely going to help.  ","c9da7a61":"The ROC curve plots the true positive rate versus the false positive rate, over different threshold values.  \nWe basically want the blue line to be as close as possible to the upper left corner.   \nAs our dataset is quite imbalanced, ROC doesn\u2019t look very useful for us even though the results look pretty good.","29efe585":"<a id=\"1\"><\/a>\n# 1. Explore Data and Analysis","efb62e41":"The result shows that features V3,V7,V10,V12,V14,V16 and V17 seem to have high scores.  ","07a2a7ef":"Precision =  0.80. The rate of true positive in all positive cases.  \nRecall =   0.76. The rate of true positive in all true cases.  \nF1-score = 0.78 \nFalse positives cases 28.  ","9b885168":"## Plot ROC Curve and Precision-Recall Curve with BorderlienSMOTE Data","4cca8c33":"## Confusion Matrix","520ec9ea":"## Extract features from our scaled dataset \"df_scaled\"","2458a20d":"## Plot ROC Curve and Precision-Recall Curve with Imbalanced Data","7708ae39":"### Summary:\n* The mean transaction amout among fraud cases is 122 USD, higher than 88 USD among normal cases.    \n* And the difference is statistically significant(p<0.05). ","bf9302e7":"###  Transaction Hour\nLet's look at the transaction percentage from day 0 to the next day.  ","2af025d7":" ## Logistic Regression with imbalanced data","c6bce8fb":"## GridSearchCV to find optimal parameters for Random Forest Classifier","3d49b82b":"## Check and visulaize Fraud to Non-fraud Ratio","8282b782":"<a id=\"9\"><\/a>\n# 9: KMeans Clustering","b6471f32":"We can detect 91 out of 147 fraud cases in the test dataset.  \nBut there are 17341 false positive cases which indicated that our MiniBatchKMeans model needs to be improved by selecting good features.  ","e30533be":"## Recall vs. Precision\nConsidering the imbalance of our dataset, we take a look at the Recall vs. Precision trade off.","d1ffa49b":"## Prediction\nIn order to predict whether or not a new transaction is normal or fraudulent, we\u2019ll calculate the reconstruction error from the transaction data itself.   \nIf the error is larger than a predefined threshold, we\u2019ll mark it as a fraud (since our model should have a low error on normal transactions). \n","bd64de53":"## Find the best K ","ec5fcbed":"## K-Nearest Neighbors Classifer with Imbalanced Data","07f8f375":"<a id=\"8\"><\/a>\n# Module 8: Voting Classifier","c7f5cb23":"## Train the Autoencoder Model\nTrain our model for 100 epochs with a batch size of 128 samples and save the best performing model to a file.   \nThe ModelCheckpoint provided by Keras is really handy for such tasks.   \nAdditionally, the training progress will be exported in a format that TensorBoard understands.","194d5bb1":"## Import modules, methods and dataset\n","7a933d33":"<a id=\"2\"><\/a>\n# 2: Resampling for Imbalanced Data \nThere are two types of resampling methods to deal with imbalanced data, one is **under sampling** and another one is **over sampling**. \n* Under sampling: you take ramdom draws from non-fraud observations to match the amount of fraud observations.  But you're randomly throwing away a lot of data and infromation. aka: Random Under Sampling\n* Over sampling: you take ramdom draws from frad cases and copy these observations to increase to amount of fraud samples in your data. But you are traning your model many many duplicates. aka: Random Over Sampling & SMOTE\n* Synthetic Minority Oversampling Technique(SMOTE): Adjust the data imbalance by oversampling the monority observations(fraud cases) using nearest neighbors of fraud cases to create new synthetic fraud cases instead of just coping the monority samples. \n* [BorderlineSMOTE](https:\/\/sci2s.ugr.es\/keel\/keel-dataset\/pdfs\/2005-Han-LNCS.pdf)\n* There is a common mistake when doing resampling, that is testing your model on the oversampled or undersampled dataset. If we want to implement cross validation, remember to split your data into training and testing before oversample or undersample and then just oversample or undersample the training part.   \nAnother way to avoid this is to use **\"Pipeline\"** method. ","c5f6428e":"Accuracy score = \nPrecision =  0.95. The rate of true positive in all positive cases.  \nRecall =  0.73. The rate of true positive in all true cases.  \nF1-score = 0.83\nFalse positives cases = 6, which is much better.   ","1ab5b924":"## Build the Autoencoder Model\nUse 4 fully connected layers with 14, 7, 7 and 29 neurons respectively for this Autoencoder model.     \nThe first two layers are used for encoder, the last two go for the decoder.   \nUse L1 regularization during training","04b765a6":"## Logistic Regression with BorderlineSMOTE Resampling Method","e01b337a":"Precision =  0.95.  \nRecall =  0.71. \nF1-score = 0.81 \nFalse positives cases = 5.  ","2c506546":"ROC AUC Score: 0.9382993521688638  \nAverage Precision: 0.11382394026169773   \nprecision: 0.14  \nrecall: 0.84  \nf1-score: 0.23 \nThe precision does not perform well in Logistic Regression model.","70a837f1":"By combining the classifiers, you can take the best of multiple models.  \nBy combining these together you indeed managed to improve performance.","475eaf55":"Hour \"zero\" corresponds to the hour the first transaction happened and not necessarily 12-1am.   \nGiven the heavy decrease in normal transactions from hours 1 to 8 and again roughly at hours 24 to 32, it seems fraud tends to occur at higher rates during the night.   \nStatistical tests could be used to give evidence for this fact.","03e898c6":"###  Transaction Amount vs. Hour","f66366ce":"### Summary:\n* In the long tail, fraud transaction happened more frequently. \n* It seems \nIt would be hard to differentiate fraud from normal transactions by transaction amount alone.","75d99aee":"## Cut Up the Dataset into Two Datasets and Summarize ","342b4e2f":"# Credit Card Fraud Detection\n\n## Background\n\u201cContinuing the trend of prior years, the cost of fraud continues to rise for global financial institutions...Fraudsters continuously test for the weakest entry point in the financial transaction system and these institutions should apply a multi-layered approach to fraud prevention to combat this growing issue.\u201d\n-LexisNexis Risk Solutions Sr. Director of Fraud and Identity Management Strategy Kimberly Sutherland   \nNilson reports that card fraud (credit, debt, etc) was reportedly [31.26](https:\/\/www.aba.com\/Products\/Endorsed\/Documents\/Rippleshot-State-of-Card-Fraud.pdf) billion dollars in 2018 and expected to increase to  [32.82](https:\/\/www.aba.com\/Products\/Endorsed\/Documents\/Rippleshot-State-of-Card-Fraud.pdf) billion dollars in 2019.   \nFor perspective, in 2018 both PayPal's and Mastercard's revenue were only [15.45](https:\/\/www.paypal.com\/stories\/us\/paypal-reports-fourth-quarter-and-full-year-2018-results) and [14.95](https:\/\/www.macrotrends.net\/stocks\/charts\/MA\/mastercard\/revenue) billion dollars each. \n\n## Detecting Fraud is typically challenging:  \n1. Uncommon: Fraud cases are in a minority, sometimes only 0, sometimes only 0.01% of a company\u2019s transactions are fraudulent. If there are few cases of fraud, then there's little data to learn how to identify them.  This is known as **class imbalance**, and it's one of the main challenges of fraud detection. \n2. Concealed: Fraudsters will also try their best to blend in and conceal their activities. \n3. Change over time: Fraudsters will find new methods to avoid getting caught and change their behaviors over time.\n4. Organized: Fraudsters oftentimes work together and organize their activities in a network, making it harder to detect. \n\n## How does a company deal with fraud?  \n1. Use rules-based systems,based on manually set thresholds and experience to filter out strange cases.\n2. Check the news: the fraud analytics team check the news for suspicious names. \n3. Receive external lists of fraudulent accounts and names: keep track of the external hits lists from the police to reference check against the client base.\n4. Use machine learning algorithms to detect suspicious names and behaviors.\n5. Combine different strategies and various models together to avoid sub-par detection results since organized crime schemes are so sophisticated and quick to adapt. \n\n## Machine Learning in Fraud Detection \n* Traditional rules-based expert systems are not enough to catch fraud. They can do an excellent job of uncovering known patterns; but alone aren\u2019t very effective at uncovering unknown schemes, adapting to new fraud patterns, or handling fraudsters\u2019 increasingly sophisticated techniques. And this is where machine learning becomes necessary for fraud detection.   \n* Many in the financial services industry have updated their fraud detection to include some basic machine learning algorithms including various clustering classifiers, linear approaches, and support vector machines. The most advanced companies in the financial services industry, such as PayPal, have been pioneering more advanced artificial intelligence techniques such as deep neural networks and autoencoders. \n* When building a machine-learning model suite for fraud detection, it is very important not only to identify bad activity(high true positive rate) but also to allow good transactions to go through(low false positive rate).    \n\n## Supervised and Unsupervised Machine Learning\n\n* Supervised Machine Learning: A model that is trained on a set of properly \u201clabeled\u201d transactions. Each transaction is tagged as either fraud or non-fraud.  Supervised machine learning model accuracy is directly correlated with the amount of clean, relevant training data. Common supervised machine learning methods include Linear Regression, Logistic Regression, KNN(K Nearest Neighbors), Decsion Tree, Random Forest and etc. \n* Unsupervised Machine Learning: A model that is trained in cases where tagged transaction data is relatively thin or non-existent. Unsupervised models are designed to discover outliers that represent previously unseen forms of fraud. In the real world of fraud detection, well labeled data is very rare. Therefore supervised machine learning methods alone can not do a good job and unsupervised learning will play an important role in the war. Common unsupervised learning methods in fraud detection are Clustering, Outlier Detection and Dimensionality Reduction. \n\n## Model Evaluation in Credit Card Fraud Detection\n* When working with highly imbalanced data, accuracy is not a reliable performance metric. Because just predicting everything is in the majority class you can even obtain a higher accuracy than  by building a predictive model. So what metrics should we use instead of accuaracy in fraud detection?  \n* Precision: true positives \/ (true positives + false positives)  \n* Recall\/True positive rate : true positives \/ (true positives +false negatives) \n* F1-score: 2 x Precision x Recall \/ (Precision + Recall) = 2 x TP \/ (2 x TP + FP + FN)\n* Confusion Matrix: Shows how many fraud cases you can predict correctly.\n* Classification Report: Tells you about the precision and recall of your model.\n* False positive rate (\u03b1) = type I error = 1 \u2212 specificity = FP \/ (FP + TN)\n* False negative rate (\u03b2) = type II error = 1 \u2212 sensitivity = FN \/ (FN + TP) \n* Receiver Operating Characteristic Curve(ROC): The ROC curve is created by plotting the true positive rate (y=TPR) against the false positive rate (x=FPR) at various threshold settings. It's useful to compare performance of different algorithms for even highly imbalanced data like fraud detection.  \n*  [Area Under Curve (AUC)](https:\/\/www.youtube.com\/watch?v=OAl6eAyP-yo): The ROC Curve and AUC are insensitive to whether your predictive probabilities properly calibrated and they are only sensitive to rank ordering. Choosing a classification threshold is a business decision,which depends on whether you would like rather minimize FPR or maximize TPR.   \n* In fraud detection, a credit card company wants maximaze the True Positive Rate(or Recall)to watch as much fraud as possible(reduce false negatives) as fraudulent transactios can be very costly and a false alarm means someone's transaction is blocked(reduce false positives). The credit card company therefore wants to optimize recall.\n\n## Dataset Introduction \n* The data contains **284,807** European credit card transactions with **492** fraudulent transactions that occurred over two days in September 2013.    \n* Everything except the \"Time\" and \"Amount\" has been reduced by a Principle Component Analysis (PCA) for privacy concerns, which means features V1, V2, ... V28 have been scaled already during PCA.\n* Feature **\"Time\"** represents the seconds elapsed between each transaction and the first transaction in the dataset.   \n* The feature **\"Amount\"** is the transaction amount, which could be used for example-dependant cost-senstive learning.    \n* Feature **\"Class\"** is to label each transaction fraud or not. It takes value 1 in case of fraud and 0 otherwise.    \n<font color='red'>\n* The dataset is highly **imbalanced**, the positive class (frauds) account for **0.172%** of all transactions. Given the class imbalance ratio, Area Under the ROC Curve (AUCROC) is recommend to evaluate the model.  Confusion matrix accuracy is not meaningful for imbalanced classification due to [Accuracy Paradox](https:\/\/en.wikipedia.org\/wiki\/Accuracy_paradox)\n\n\n### Map of Content\n1. [Explore Data and Analysis(EDA) ](#1)\n1. [Resampling for Imbalanced Data](#2)\n1. [Logistic Regression](#3)\n1. [K-Nearest Neighbors Classifer](#4)\n1. [Support Vector Machine](#5)\n1. [Decision Tree Classifer](#6)\n1. [Random Forest Classifer](#7)\n1. [Voting Classifier](#8)\n1. [K-means Clustering](#9)\n1. [BDS](#10)\n1. [Autoencoder Neural Networks](#11)","0f478476":"<a id=\"6\"><\/a>\n# Module 6: Decision Tree Classifier","809bc482":"## Correlation Matrices\nCorrelation matrices are the essence of understanding our data. We want to know if there are features that influence heavily in whether a specific transaction is a fraud. ","039f6e34":"Precision =  0.64. The rate of true positive in all positive cases.  \nRecall =  0.70. The rate of true positive in all true cases.  \nF1-score = 0.66  \nFalse positives cases = 59.  ","111e6ff3":"From the second heatmap of covariance matrix, in fraud cases, some features are strongly related such as V1\u3001V2\u3001V3\u3001V4\u3001V5\u3001V6\u3001V7\u3001V9\u3001V10\u3001V11\u3001V12\u3001V14\u3001V16\u3001V17 with V18, V19","06ca86e5":"## Build the model","0f159481":"The model results don't improve drastically.   \nIf we mostly care about catching fraud, and not so much about the false positives, this does actually not improve our model at all, albeit a simple option to try.  \nBy smartly defining more options in the model, you can obtain better predictions. "}}