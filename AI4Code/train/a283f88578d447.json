{"cell_type":{"29afef3c":"code","2af81e31":"code","af990d55":"code","4051523d":"code","0e2f7033":"code","308d36a7":"code","9787896c":"code","bc48d183":"code","5aeaf666":"code","b69fd7d4":"code","9ad50036":"code","e04e3c89":"code","a56d45b2":"code","f881dd36":"code","15c77050":"code","2f5e2550":"code","615454e0":"code","695f2f51":"code","89f15d19":"code","2b9120b2":"code","ee231d0b":"code","e031fdc5":"code","125ec645":"code","c028d610":"code","ff11e8eb":"code","56e99c9e":"code","7538e793":"code","1e9b9872":"code","0c358f33":"code","fc2b5836":"code","c644999c":"code","65d5a63b":"code","082b037f":"code","904589cb":"code","bfc5b13c":"code","971426cb":"code","5df61355":"code","53023efc":"code","47f3067e":"code","c4ce3543":"code","b9159cd4":"markdown","ee383788":"markdown","8ed548fd":"markdown","8e1b86b4":"markdown","64cbdcd5":"markdown","50864458":"markdown","28dd0afc":"markdown","fd129114":"markdown","42ed28e8":"markdown"},"source":{"29afef3c":"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            import os\nimport string\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\n\nimport torchvision\nfrom torchvision import transforms\nfrom torchvision.transforms import ToTensor, Normalize","2af81e31":"print('Current Directory: ', os.getcwd())\nprint('Directories in Parent Directory: ', os.listdir('..\/'))","af990d55":"input_directory = '..\/input'\nprint('Directories in Input Directory: ', os.listdir(input_directory))","4051523d":"asl_directory = '..\/input\/letters\/random_asl'\ntest_directory = '..\/input\/letters\/random_asl\/asl_alphabet_test\/asl_alphabet_test'\ntrain_directory = '..\/input\/letters\/random_asl\/asl_alphabet_train\/asl_alphabet_train'","0e2f7033":"class ASLDatasetTest(Dataset):\n    char_to_int = {c: ord(c) - ord('A') for c in string.ascii_uppercase}\n    char_to_int['del'] = 26\n    char_to_int['nothing'] = 27\n    char_to_int['space'] = 28\n    int_to_char = {value: key for key, value in char_to_int.items()}\n        \n    def __init__(self, directory: str, transform=None, label_transform=None):\n        super().__init__()        \n        self.directory = directory\n        self.transform = transform\n        self.label_transform = label_transform        \n        self.x = None\n        self.y = None\n        self._load_images()\n    \n    def __getitem__(self, idx):\n        x, y = torchvision.io.read_image(self.x[idx]).type(torch.float32), self.y[idx]\n        if self.transform:\n            x = self.transform(x)\n        if self.label_transform:\n            y = self.label_transform(y)\n        return x, y\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def _load_images(self):\n        self.x = []\n        self.y = []\n        for img in os.listdir(self.directory):\n            class_name = img[:1]\n            if 'nothing' in img:\n                class_name = 'nothing'\n            elif 'space' in img:\n                class_name = 'space'\n            elif 'del' in img:\n                class_name = 'del'\n            self.x.append(os.path.join(self.directory, img))\n            self.y.append(self.char_to_int[class_name])\n        self.y = torch.tensor(self.y, dtype=torch.int64)\n    \n    @staticmethod\n    def get_classname(idx: int) -> str:\n        return ASLDataset.int_to_char[idx]","308d36a7":"class ASLDataset(Dataset):\n    char_to_int = {c: ord(c) - ord('A') for c in string.ascii_uppercase}\n    char_to_int['del'] = 26\n    char_to_int['nothing'] = 27\n    char_to_int['space'] = 28\n    int_to_char = {value: key for key, value in char_to_int.items()}\n        \n    def __init__(self, directory: str, transform=None, label_transform=None):\n        super().__init__()\n        \n        self.directory = directory\n        self.transform = transform\n        self.label_transform = label_transform\n        \n        self.x = None\n        self.y = None\n        self._load_images()\n    \n    def __getitem__(self, idx):\n        x, y = torchvision.io.read_image(self.x[idx]).type(torch.float32), self.y[idx]\n        \n        if self.transform:\n            x = self.transform(x)\n        if self.label_transform:\n            y = self.label_transform(y)\n        \n        return x, y\n    \n    def __len__(self):\n        return len(self.y)\n    \n    def _load_images(self):\n        self.x = []\n        self.y = []\n        \n        for c in os.listdir(self.directory):\n            class_name = c\n            class_dir = os.path.join(self.directory, class_name)\n            for img in os.listdir(class_dir):\n                self.x.append(os.path.join(class_dir, img))\n                self.y.append(self.char_to_int[class_name])\n                \n        self.y = torch.tensor(self.y, dtype=torch.int64)\n    \n    @staticmethod\n    def get_classname(idx: int) -> str:\n        return ASLDataset.int_to_char[idx]","9787896c":"ts = transforms.Compose([\n    transforms.Resize(256),\n    transforms.RandomCrop(224),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n])","bc48d183":"asl_train = ASLDataset('..\/input\/letters\/random_asl\/asl_alphabet_train\/asl_alphabet_train', transform=ts)\nasl_test = ASLDatasetTest('..\/input\/letters\/random_asl\/asl_alphabet_test\/asl_alphabet_test', transform=ts)","5aeaf666":"print(len(asl_train))\nprint(len(asl_test))","b69fd7d4":"train_sampler = SubsetRandomSampler(np.arange(len(asl_train)))\ntest_sampler = SubsetRandomSampler(np.arange(len(asl_test)))","9ad50036":"train_loader = DataLoader(asl_train, 32, sampler=train_sampler)\ntest_loader = DataLoader(asl_test, 32, sampler=test_sampler)","e04e3c89":"import matplotlib.pyplot as plt\nfor img, label in train_loader:\n    print(img.shape, label.shape)\n    plt.imshow(img[0].permute(1, 2, 0))\n    break","a56d45b2":"for x, y in train_loader:\n    print(x.shape)\n    print(y.shape)","f881dd36":"def test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss \/= num_batches\n    correct \/= size\n    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","15c77050":"class AlexNet(nn.Module):\n    def __init__(self, num_classes: int = 1000) -> None:\n        super(AlexNet, self).__init__()\n        \n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n        )\n        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n        \n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(256 * 6 * 6, 4096),\n            nn.ReLU(inplace=True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(inplace=True),\n            nn.Linear(4096, num_classes),\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.features(x)\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x","2f5e2550":"model = AlexNet(29)","615454e0":"criterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=0.0001)","695f2f51":"print(len(train_loader.dataset))\nprint(len(train_loader))","89f15d19":"epochs = 10\n\nfor e in range(epochs):\n    running_loss = 0.0\n    \n    for i, (imgs, labels) in enumerate(train_loader):\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        if i % 7 == 6:\n            print('[Epoch %d, Step %5d] loss: %.3f' %\n                  (e + 1, i + 1, running_loss \/ 7))\n            running_loss = 0.0","2b9120b2":"test(test_loader, model, nn.CrossEntropyLoss())","ee231d0b":"class_correct = list(0. for i in range(29))\nclass_total = list(0. for i in range(29))\n\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(len(labels)):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(29):\n    print('Accuracy of %5s : %2d %%' % (\n        ASLDataset.int_to_char[i], 100 * class_correct[i] \/ class_total[i]))","e031fdc5":"from torchvision import models","125ec645":"model = models.alexnet(pretrained=True)","c028d610":"for param in model.parameters():\n    param.requires_grad = False","ff11e8eb":"print(model)","56e99c9e":"new_clf = nn.Sequential(\n    nn.Dropout(p=0.5, inplace=False),\n    nn.Linear(in_features=9216, out_features=4096, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Dropout(p=0.5, inplace=False),\n    nn.Linear(in_features=4096, out_features=4096, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Linear(in_features=4096, out_features=1000, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Linear(in_features=1000, out_features=29, bias=True),\n)","7538e793":"model.classifier = new_clf","1e9b9872":"print(model)","0c358f33":"criterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=0.0001)","fc2b5836":"epochs = 10\n\nfor e in range(epochs):\n    running_loss = 0.0\n    for i, (imgs, labels) in enumerate(train_loader):\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        if i % 7 == 6:\n            print('[Epoch %d, Step %5d] loss: %.3f' %\n                  (e + 1, i + 1, running_loss \/ 7))\n            running_loss = 0.0","c644999c":"test(test_loader, model, nn.CrossEntropyLoss())","65d5a63b":"class_correct = list(0. for i in range(29))\nclass_total = list(0. for i in range(29))\n\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(len(labels)):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(29):\n    print('Accuracy of %5s : %2d %%' % (\n        ASLDataset.int_to_char[i], 100 * class_correct[i] \/ class_total[i]))","082b037f":"model = models.resnet152(pretrained=True)","904589cb":"# print(model)","bfc5b13c":"new_fc = torch.nn.Sequential(\n    nn.Linear(in_features=2048, out_features=1000, bias=True),\n    nn.ReLU(inplace=True),\n    nn.Linear(in_features=1000, out_features=29, bias=True),\n)","971426cb":"\nmodel.fc = new_fc","5df61355":"criterion = nn.CrossEntropyLoss()\noptimizer = Adam(model.parameters(), lr=0.0001)","53023efc":"epochs = 5\n\nfor e in range(epochs):\n    running_loss = 0.0\n    for i, (imgs, labels) in enumerate(train_loader):\n        outputs = model(imgs)\n        loss = criterion(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        if i % 7 == 6:\n            print('[Epoch %d, Step %5d] loss: %.3f' %\n                  (e + 1, i + 1, running_loss \/ 7))\n            running_loss = 0.0","47f3067e":"test(test_loader, model, nn.CrossEntropyLoss())","c4ce3543":"class_correct = list(0. for i in range(29))\nclass_total = list(0. for i in range(29))\n\nwith torch.no_grad():\n    for data in test_loader:\n        images, labels = data\n        outputs = model(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(len(labels)):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(29):\n    print('Accuracy of %5s : %2d %%' % (\n        ASLDataset.int_to_char[i], 100 * class_correct[i] \/ class_total[i]))","b9159cd4":"# Importing libraries","ee383788":"CNN architecture **AlexNet** is implemented.","8ed548fd":"# ResNet152","8e1b86b4":"## Dataset\nThe data set is a collection of images of alphabets from the American Sign Language, separated in 29 folders which represent the various classes.\n\n## Content\nThe training data set contains 780 images which are 200x200 pixels. There are 29 classes, of which 26 are for the letters A-Z and 3 classes for SPACE, DELETE and NOTHING.\nThese 3 classes are very helpful in real-time applications, and classification.\nThe test data set contains a mere 29 images, to encourage the use of real-world test images.\n\nThe data set is a collection of images of alphabets from the American Sign Language, separated in 29 folders which represent the various classes.","64cbdcd5":"# Report\n\nFrom the above models, we can say that Pretrained ResNet152 gives the most accurate result whereas other models doesn't give that much. For the dataset, I have used custom dataset from American Sign laguage dataset. ","50864458":"# Pre-trained AlexNet and ResNet","28dd0afc":"# AlexNet\n\n<img src='https:\/\/drive.google.com\/uc?export=view&id=1sjEftFGiJ50-m3VevamktVznsx6bY3Yw' width=700>","fd129114":"# Project II\n\n#### Name: Saima Khan\n#### ID: 181-35-2392\n#### Course Title: Machine Learning Driven Data Analysis I \n\n","42ed28e8":"## Creating Dataloader\n\n1.\t A custom dataset is created by extending the Dataset class. It sould, given an index, return the image and its corresponding label as a tuple.\n2.\t Functions and create indices are written for training, validation, and test sets.\n3.\t 3 separate data loaders, each for training, validation, and test data are created. "}}