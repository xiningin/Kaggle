{"cell_type":{"4895b8dc":"code","dcaea134":"code","621e8aa2":"code","4792cf38":"code","00edf62c":"code","92502a7e":"code","287063d7":"code","d7fcf83f":"code","d0f22fc9":"code","4cf78f4b":"code","782c7615":"code","b5660647":"code","19cc150e":"code","6d4454d4":"code","0d674049":"code","7a0a02fd":"code","6562dfde":"code","6b30d23d":"code","5a819031":"code","f25001f1":"code","1099d653":"code","fd2f6021":"code","9eb4ca58":"code","0d6aebd5":"code","9eb5c0f3":"code","11e1b5a1":"code","cbc5d1b2":"code","7cbdd129":"code","500b6e7a":"code","1bd994ae":"code","13121434":"code","796c5647":"code","d82f8d25":"code","c203a38d":"code","ba5d678f":"code","71999646":"code","850837e7":"code","71c1b52d":"code","06ac6e72":"code","ecd08d64":"code","4234ce45":"code","b3a1f365":"code","23a06593":"code","bccd57dd":"code","c1ffd02c":"code","480406b0":"code","e5f90b5d":"code","6f27b228":"code","1ba9a6b8":"code","bd63aeac":"code","1367ee5e":"markdown","fc48e04d":"markdown","4b227582":"markdown","55a2b560":"markdown","e25e9ea2":"markdown","e3658ea7":"markdown","f11a4f4c":"markdown","abb5ab89":"markdown","e7b619b6":"markdown","732a950c":"markdown","ec9d8f93":"markdown","a75be3dd":"markdown","bcd2e51e":"markdown","a544bf0b":"markdown","a01ae8a4":"markdown","53215405":"markdown","34c7e250":"markdown","181b23bd":"markdown","4f2d2005":"markdown","cbbb1b1e":"markdown","5d626b30":"markdown","4efdf89f":"markdown","f2b87ed3":"markdown","31fd5514":"markdown","572c6c26":"markdown","ff1ccf9e":"markdown","b054ac39":"markdown","20afaba5":"markdown","031ed78a":"markdown","67199efb":"markdown","56edbbbd":"markdown","780eb230":"markdown","d30834b1":"markdown","02883972":"markdown","dff1b442":"markdown","9014e9b5":"markdown","93e5f7cf":"markdown","46d56e73":"markdown","b4b800b7":"markdown","3ffe2525":"markdown","96a2a785":"markdown","109ba980":"markdown","dde06761":"markdown","9196fe1e":"markdown","6e3d37f2":"markdown","efa08dab":"markdown","a7ece720":"markdown"},"source":{"4895b8dc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns # Visualization\nimport matplotlib.pyplot as plt # Visualization\nfrom colorama import Fore\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nimport math\n\nimport warnings # Supress warnings \nwarnings.filterwarnings('ignore')\n\nnp.random.seed(7)","dcaea134":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        # print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","621e8aa2":"df = pd.read_csv(\"..\/input\/acea-water-prediction\/Aquifer_Petrignano.csv\")\ndf.head()","4792cf38":"# Remove old rows\ndf = df[df.Rainfall_Bastia_Umbra.notna()].reset_index(drop=True)\n# Remove not usefull columns\ndf = df.drop(['Depth_to_Groundwater_P24', 'Temperature_Petrignano'], axis=1)","00edf62c":"# Simplify column names\ndf.columns = ['date', 'rainfall', 'depth_to_groundwater', 'temperature', 'drainage_volume', 'river_hydrometry']\n\ntargets = ['depth_to_groundwater']\nfeatures = [feature for feature in df.columns if feature not in targets]\ndf.head()","92502a7e":"from datetime import datetime, date \n\ndf['date'] = pd.to_datetime(df['date'], format = '%d\/%m\/%Y')\ndf.head().style.set_properties(subset=['date'], **{'background-color': 'dodgerblue'})","287063d7":"# To compelte the data, as naive method, we will use ffill\nf, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 25))\n\nfor i, column in enumerate(df.drop('date', axis=1).columns):\n    sns.lineplot(x=df['date'], y=df[column].fillna(method='ffill'), ax=ax[i], color='dodgerblue')\n    ax[i].set_title('Feature: {}'.format(column), fontsize=14)\n    ax[i].set_ylabel(ylabel=column, fontsize=14)\n                      \n    ax[i].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])                ","d7fcf83f":"df = df.sort_values(by='date')\n\n# Check time intervals\ndf['delta'] = df['date'] - df['date'].shift(1)\n\ndf[['date', 'delta']].head()","d0f22fc9":"df['delta'].sum(), df['delta'].count()","4cf78f4b":"df = df.drop('delta', axis=1)\ndf.isna().sum()","782c7615":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 15))\n\nold_hydrometry = df['river_hydrometry'].copy()\ndf['river_hydrometry'] = df['river_hydrometry'].replace(0, np.nan)\n\nsns.lineplot(x=df['date'], y=old_hydrometry, ax=ax[0], color='darkorange', label='original')\nsns.lineplot(x=df['date'], y=df['river_hydrometry'].fillna(np.inf), ax=ax[0], color='dodgerblue', label='modified')\nax[0].set_title('Feature: Hydrometry', fontsize=14)\nax[0].set_ylabel(ylabel='Hydrometry', fontsize=14)\nax[0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nold_drainage = df['drainage_volume'].copy()\ndf['drainage_volume'] = df['drainage_volume'].replace(0, np.nan)\n\nsns.lineplot(x=df['date'], y=old_drainage, ax=ax[1], color='darkorange', label='original')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[1], color='dodgerblue', label='modified')\nax[1].set_title('Feature: Drainage', fontsize=14)\nax[1].set_ylabel(ylabel='Drainage', fontsize=14)\nax[1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])","b5660647":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\n\nsns.heatmap(df.T.isna(), cmap='Blues')\nax.set_title('Missing Values', fontsize=16)\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","19cc150e":"f, ax = plt.subplots(nrows=4, ncols=1, figsize=(15, 12))\n\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(0), ax=ax[0], color='darkorange', label = 'modified')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[0], color='dodgerblue', label = 'original')\nax[0].set_title('Fill NaN with 0', fontsize=14)\nax[0].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n\nmean_drainage = df['drainage_volume'].mean()\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(mean_drainage), ax=ax[1], color='darkorange', label = 'modified')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[1], color='dodgerblue', label = 'original')\nax[1].set_title(f'Fill NaN with Mean Value ({mean_drainage:.0f})', fontsize=14)\nax[1].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n\nsns.lineplot(x=df['date'], y=df['drainage_volume'].ffill(), ax=ax[2], color='darkorange', label = 'modified')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[2], color='dodgerblue', label = 'original')\nax[2].set_title(f'FFill', fontsize=14)\nax[2].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n\nsns.lineplot(x=df['date'], y=df['drainage_volume'].interpolate(), ax=ax[3], color='darkorange', label = 'modified')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].fillna(np.inf), ax=ax[3], color='dodgerblue', label = 'original')\nax[3].set_title(f'Interpolate', fontsize=14)\nax[3].set_ylabel(ylabel='Volume C10 Petrignano', fontsize=14)\n\nfor i in range(4):\n    ax[i].set_xlim([date(2019, 5, 1), date(2019, 10, 1)])\n    \nplt.tight_layout()\nplt.show()","6d4454d4":"df['drainage_volume'] = df['drainage_volume'].interpolate()\ndf['river_hydrometry'] = df['river_hydrometry'].interpolate()\ndf['depth_to_groundwater'] = df['depth_to_groundwater'].interpolate()","0d674049":"fig, ax = plt.subplots(ncols=2, nrows=3, sharex=True, figsize=(16,12))\n\nsns.lineplot(df['date'], df['drainage_volume'], color='dodgerblue', ax=ax[0, 0])\nax[0, 0].set_title('Drainage Volume', fontsize=14)\n\nresampled_df = df[['date','drainage_volume']].resample('7D', on='date').sum().reset_index(drop=False)\nsns.lineplot(resampled_df['date'], resampled_df['drainage_volume'], color='dodgerblue', ax=ax[1, 0])\nax[1, 0].set_title('Weekly Drainage Volume', fontsize=14)\n\nresampled_df = df[['date','drainage_volume']].resample('M', on='date').sum().reset_index(drop=False)\nsns.lineplot(resampled_df['date'], resampled_df['drainage_volume'], color='dodgerblue', ax=ax[2, 0])\nax[2, 0].set_title('Monthly Drainage Volume', fontsize=14)\n\nfor i in range(3):\n    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nsns.lineplot(df['date'], df['temperature'], color='dodgerblue', ax=ax[0, 1])\nax[0, 1].set_title('Daily Temperature (Acc.)', fontsize=14)\n\nresampled_df = df[['date','temperature']].resample('7D', on='date').mean().reset_index(drop=False)\nsns.lineplot(resampled_df['date'], resampled_df['temperature'], color='dodgerblue', ax=ax[1, 1])\nax[1, 1].set_title('Weekly Temperature (Acc.)', fontsize=14)\n\nresampled_df = df[['date','temperature']].resample('M', on='date').mean().reset_index(drop=False)\nsns.lineplot(resampled_df['date'], resampled_df['temperature'], color='dodgerblue', ax=ax[2, 1])\nax[2, 1].set_title('Monthly Temperature (Acc.)', fontsize=14)\n\nfor i in range(3):\n    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nplt.show()","7a0a02fd":"# As we can see, downsample to weekly could smooth the data and hgelp with analysis\ndownsample = df[['date',\n                 'depth_to_groundwater', \n                 'temperature',\n                 'drainage_volume', \n                 'river_hydrometry',\n                 'rainfall'\n                ]].resample('7D', on='date').mean().reset_index(drop=False)\n\ndf = downsample.copy()","6562dfde":"# A year has 52 weeks (52 weeks * 7 days per week) aporx.\nrolling_window = 52\nf, ax = plt.subplots(nrows=2, ncols=1, figsize=(15, 12))\n\nsns.lineplot(x=df['date'], y=df['drainage_volume'], ax=ax[0], color='dodgerblue')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].rolling(rolling_window).mean(), ax=ax[0], color='black', label='rolling mean')\nsns.lineplot(x=df['date'], y=df['drainage_volume'].rolling(rolling_window).std(), ax=ax[0], color='orange', label='rolling std')\nax[0].set_title('Depth to Groundwater: Non-stationary \\nnon-constant mean & non-constant variance', fontsize=14)\nax[0].set_ylabel(ylabel='Drainage Volume', fontsize=14)\nax[0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nsns.lineplot(x=df['date'], y=df['temperature'], ax=ax[1], color='dodgerblue')\nsns.lineplot(x=df['date'], y=df['temperature'].rolling(rolling_window).mean(), ax=ax[1], color='black', label='rolling mean')\nsns.lineplot(x=df['date'], y=df['temperature'].rolling(rolling_window).std(), ax=ax[1], color='orange', label='rolling std')\nax[1].set_title('Temperature: Non-stationary \\nvariance is time-dependent (seasonality)', fontsize=14)\nax[1].set_ylabel(ylabel='Temperature', fontsize=14)\nax[1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n\nplt.tight_layout()\nplt.show()","6b30d23d":"# https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.stattools.adfuller.html\nfrom statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(df['depth_to_groundwater'].values)\nresult","5a819031":"# Thanks to https:\/\/www.kaggle.com\/iamleonie for this function!\nf, ax = plt.subplots(nrows=3, ncols=2, figsize=(15, 9))\n\ndef visualize_adfuller_results(series, title, ax):\n    result = adfuller(series)\n    significance_level = 0.05\n    adf_stat = result[0]\n    p_val = result[1]\n    crit_val_1 = result[4]['1%']\n    crit_val_5 = result[4]['5%']\n    crit_val_10 = result[4]['10%']\n\n    if (p_val < significance_level) & ((adf_stat < crit_val_1)):\n        linecolor = 'forestgreen' \n    elif (p_val < significance_level) & (adf_stat < crit_val_5):\n        linecolor = 'orange'\n    elif (p_val < significance_level) & (adf_stat < crit_val_10):\n        linecolor = 'red'\n    else:\n        linecolor = 'purple'\n    sns.lineplot(x=df['date'], y=series, ax=ax, color=linecolor)\n    ax.set_title(f'ADF Statistic {adf_stat:0.3f}, p-value: {p_val:0.3f}\\nCritical Values 1%: {crit_val_1:0.3f}, 5%: {crit_val_5:0.3f}, 10%: {crit_val_10:0.3f}', fontsize=14)\n    ax.set_ylabel(ylabel=title, fontsize=14)\n\nvisualize_adfuller_results(df['rainfall'].values, 'Rainfall', ax[0, 0])\nvisualize_adfuller_results(df['temperature'].values, 'Temperature', ax[1, 0])\nvisualize_adfuller_results(df['river_hydrometry'].values, 'River_Hydrometry', ax[0, 1])\nvisualize_adfuller_results(df['drainage_volume'].values, 'Drainage_Volume', ax[1, 1])\nvisualize_adfuller_results(df['depth_to_groundwater'].values, 'Depth_to_Groundwater', ax[2, 0])\n\nf.delaxes(ax[2, 1])\nplt.tight_layout()\nplt.show()","f25001f1":"# Log Transform of absolute values\n# (Log transoform of negative values will return NaN)\ndf['depth_to_groundwater_log'] = np.log(abs(df['depth_to_groundwater']))\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\nvisualize_adfuller_results(df['depth_to_groundwater_log'], 'Transformed \\n Depth to Groundwater', ax[0])\n\nsns.distplot(df['depth_to_groundwater_log'], ax=ax[1])","1099d653":"# First Order Differencing\nts_diff = np.diff(df['depth_to_groundwater'])\ndf['depth_to_groundwater_diff_1'] = np.append([0], ts_diff)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 6))\nvisualize_adfuller_results(df['depth_to_groundwater_diff_1'], 'Differenced (1. Order) \\n Depth to Groundwater', ax)","fd2f6021":"df['year'] = pd.DatetimeIndex(df['date']).year\ndf['month'] = pd.DatetimeIndex(df['date']).month\ndf['day'] = pd.DatetimeIndex(df['date']).day\ndf['day_of_year'] = pd.DatetimeIndex(df['date']).dayofyear\ndf['week_of_year'] = pd.DatetimeIndex(df['date']).weekofyear\ndf['quarter'] = pd.DatetimeIndex(df['date']).quarter\ndf['season'] = df['month'] % 12 \/\/ 3 + 1\n\ndf[['date', 'year', 'month', 'day', 'day_of_year', 'week_of_year', 'quarter', 'season']].head()","9eb4ca58":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 3))\n\nsns.lineplot(x=df['date'], y=df['month'], color='dodgerblue')\nax.set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\nplt.show()","0d6aebd5":"month_in_year = 12\ndf['month_sin'] = np.sin(2*np.pi*df['month']\/month_in_year)\ndf['month_cos'] = np.cos(2*np.pi*df['month']\/month_in_year)\n\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))\n\nsns.scatterplot(x=df.month_sin, y=df.month_cos, color='dodgerblue')\nplt.show()","9eb5c0f3":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ncore_columns =  [\n    'rainfall', 'temperature', 'drainage_volume', \n    'river_hydrometry', 'depth_to_groundwater'\n]\n\nfor column in core_columns:\n    decomp = seasonal_decompose(df[column], period=52, model='additive', extrapolate_trend='freq')\n    df[f\"{column}_trend\"] = decomp.trend\n    df[f\"{column}_seasonal\"] = decomp.seasonal","11e1b5a1":"fig, ax = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(16,8))\n\nfor i, column in enumerate(['temperature', 'depth_to_groundwater']):\n    \n    res = seasonal_decompose(df[column], freq=52, model='additive', extrapolate_trend='freq')\n\n    ax[0,i].set_title('Decomposition of {}'.format(column), fontsize=16)\n    res.observed.plot(ax=ax[0,i], legend=False, color='dodgerblue')\n    ax[0,i].set_ylabel('Observed', fontsize=14)\n\n    res.trend.plot(ax=ax[1,i], legend=False, color='dodgerblue')\n    ax[1,i].set_ylabel('Trend', fontsize=14)\n\n    res.seasonal.plot(ax=ax[2,i], legend=False, color='dodgerblue')\n    ax[2,i].set_ylabel('Seasonal', fontsize=14)\n    \n    res.resid.plot(ax=ax[3,i], legend=False, color='dodgerblue')\n    ax[3,i].set_ylabel('Residual', fontsize=14)\n\nplt.show()","cbc5d1b2":"weeks_in_month = 4\n\nfor column in core_columns:\n    df[f'{column}_seasonal_shift_b_2m'] = df[f'{column}_seasonal'].shift(-2 * weeks_in_month)\n    df[f'{column}_seasonal_shift_b_1m'] = df[f'{column}_seasonal'].shift(-1 * weeks_in_month)\n    df[f'{column}_seasonal_shift_1m'] = df[f'{column}_seasonal'].shift(1 * weeks_in_month)\n    df[f'{column}_seasonal_shift_2m'] = df[f'{column}_seasonal'].shift(2 * weeks_in_month)\n    df[f'{column}_seasonal_shift_3m'] = df[f'{column}_seasonal'].shift(3 * weeks_in_month)","7cbdd129":"f, ax = plt.subplots(nrows=5, ncols=1, figsize=(15, 12))\nf.suptitle('Seasonal Components of Features', fontsize=16)\n\nfor i, column in enumerate(core_columns):\n    sns.lineplot(x=df['date'], y=df[column + '_seasonal'], ax=ax[i], color='dodgerblue', label='P25')\n    ax[i].set_ylabel(ylabel=column, fontsize=14)\n    ax[i].set_xlim([date(2017, 9, 30), date(2020, 6, 30)])\n    \nplt.tight_layout()\nplt.show()","500b6e7a":"f, ax = plt.subplots(nrows=1, ncols=2, figsize=(16, 8))\n\ncorrmat = df[core_columns].corr()\n\nsns.heatmap(corrmat, annot=True, vmin=-1, vmax=1, cmap='coolwarm_r', ax=ax[0])\nax[0].set_title('Correlation Matrix of Core Features', fontsize=16)\n\nshifted_cols = [\n    'depth_to_groundwater_seasonal',         \n    'temperature_seasonal_shift_b_2m',\n    'drainage_volume_seasonal_shift_2m', \n    'river_hydrometry_seasonal_shift_3m'\n]\ncorrmat = df[shifted_cols].corr()\n\nsns.heatmap(corrmat, annot=True, vmin=-1, vmax=1, cmap='coolwarm_r', ax=ax[1])\nax[1].set_title('Correlation Matrix of Lagged Features', fontsize=16)\n\n\nplt.tight_layout()\nplt.show()","1bd994ae":"from pandas.plotting import autocorrelation_plot\n\nautocorrelation_plot(df['depth_to_groundwater_diff_1'])\nplt.show()","13121434":"from statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nf, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 8))\n\nplot_acf(df['depth_to_groundwater_diff_1'], lags=100, ax=ax[0])\nplot_pacf(df['depth_to_groundwater_diff_1'], lags=100, ax=ax[1])\n\nplt.show()","796c5647":"from sklearn.model_selection import TimeSeriesSplit\n\nN_SPLITS = 3\n\nX = df['date']\ny = df['depth_to_groundwater']\n\nfolds = TimeSeriesSplit(n_splits=N_SPLITS)","d82f8d25":"f, ax = plt.subplots(nrows=N_SPLITS, ncols=2, figsize=(16, 9))\n\nfor i, (train_index, valid_index) in enumerate(folds.split(X)):\n    X_train, X_valid = X[train_index], X[valid_index]\n    y_train, y_valid = y[train_index], y[valid_index]\n\n    sns.lineplot(\n        x=X_train, \n        y=y_train, \n        ax=ax[i,0], \n        color='dodgerblue', \n        label='train'\n    )\n    sns.lineplot(\n        x=X_train[len(X_train) - len(X_valid):(len(X_train) - len(X_valid) + len(X_valid))], \n        y=y_train[len(X_train) - len(X_valid):(len(X_train) - len(X_valid) + len(X_valid))], \n        ax=ax[i,1], \n        color='dodgerblue', \n        label='train'\n    )\n\n    for j in range(2):\n        sns.lineplot(x= X_valid, y= y_valid, ax=ax[i, j], color='darkorange', label='validation')\n    ax[i, 0].set_title(f\"Rolling Window with Adjusting Training Size (Split {i+1})\", fontsize=16)\n    ax[i, 1].set_title(f\"Rolling Window with Constant Training Size (Split {i+1})\", fontsize=16)\n\nfor i in range(N_SPLITS):\n    ax[i, 0].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n    ax[i, 1].set_xlim([date(2009, 1, 1), date(2020, 6, 30)])\n    \nplt.tight_layout()\nplt.show()","c203a38d":"train_size = int(0.85 * len(df))\ntest_size = len(df) - train_size\n\nunivariate_df = df[['date', 'depth_to_groundwater']].copy()\nunivariate_df.columns = ['ds', 'y']\n\ntrain = univariate_df.iloc[:train_size, :]\n\nx_train, y_train = pd.DataFrame(univariate_df.iloc[:train_size, 0]), pd.DataFrame(univariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(univariate_df.iloc[train_size:, 0]), pd.DataFrame(univariate_df.iloc[train_size:, 1])\n\nprint(len(train), len(x_valid))","ba5d678f":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nimport math\n\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nmodel.fit(train)\n\n# x_valid = model.make_future_dataframe(periods=test_size, freq='w')\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred.tail(test_size)['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred.tail(test_size)['yhat']))\n\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))","71999646":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nplt.show()","850837e7":"from statsmodels.tsa.arima_model import ARIMA\n\n# Fit model\nmodel = ARIMA(y_train, order=(1,1,1))\nmodel_fit = model.fit()\n\n# Prediction with ARIMA\ny_pred, se, conf = model_fit.forecast(90)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred)\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred))\n\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))","71c1b52d":"f, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel_fit.plot_predict(1, 599, ax=ax)\nsns.lineplot(x=x_valid.index, y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nax.set_ylim(-35, -18)\nplt.show()","06ac6e72":"f, ax = plt.subplots(1)\nf.set_figheight(4)\nf.set_figwidth(15)\n\nsns.lineplot(x=x_valid.index, y=y_pred, ax=ax, color='blue', label='predicted') #navajowhite\nsns.lineplot(x=x_valid.index, y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nplt.show()","ecd08d64":"!pip install pmdarima","4234ce45":"from statsmodels.tsa.arima_model import ARIMA\nimport pmdarima as pm\n\nmodel = pm.auto_arima(y_train, start_p=1, start_q=1,\n                      test='adf',       # use adftest to find optimal 'd'\n                      max_p=3, max_q=3, # maximum p and q\n                      m=1,              # frequency of series\n                      d=None,           # let model determine 'd'\n                      seasonal=False,   # No Seasonality\n                      start_P=0, \n                      D=0, \n                      trace=True,\n                      error_action='ignore',  \n                      suppress_warnings=True, \n                      stepwise=True)\n\nprint(model.summary())","b3a1f365":"model.plot_diagnostics(figsize=(16,8))\nplt.show()","23a06593":"from sklearn.preprocessing import MinMaxScaler\n\ndata = univariate_df.filter(['y'])\n#Convert the dataframe to a numpy array\ndataset = data.values\n\nscaler = MinMaxScaler(feature_range=(-1, 0))\nscaled_data = scaler.fit_transform(dataset)\n\nscaled_data[:10]","bccd57dd":"# Defines the rolling window\nlook_back = 52\n# Split into train and test sets\ntrain, test = scaled_data[:train_size-look_back,:], scaled_data[train_size-look_back:,:]\n\ndef create_dataset(dataset, look_back=1):\n    X, Y = [], []\n    for i in range(look_back, len(dataset)):\n        a = dataset[i-look_back:i, 0]\n        X.append(a)\n        Y.append(dataset[i, 0])\n    return np.array(X), np.array(Y)\n\nx_train, y_train = create_dataset(train, look_back)\nx_test, y_test = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features]\nx_train = np.reshape(x_train, (x_train.shape[0], 1, x_train.shape[1]))\nx_test = np.reshape(x_test, (x_test.shape[0], 1, x_test.shape[1]))\n\nprint(len(x_train), len(x_test))","c1ffd02c":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\n#Build the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(128, return_sequences=True, input_shape=(x_train.shape[1], x_train.shape[2])))\nmodel.add(LSTM(64, return_sequences=False))\nmodel.add(Dense(25))\nmodel.add(Dense(1))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')\n\n#Train the model\nmodel.fit(x_train, y_train, batch_size=1, epochs=5, validation_data=(x_test, y_test))\n\nmodel.summary()","480406b0":"# Lets predict with the model\ntrain_predict = model.predict(x_train)\ntest_predict = model.predict(x_test)\n\n# invert predictions\ntrain_predict = scaler.inverse_transform(train_predict)\ny_train = scaler.inverse_transform([y_train])\n\ntest_predict = scaler.inverse_transform(test_predict)\ny_test = scaler.inverse_transform([y_test])\n\n# Get the root mean squared error (RMSE) and MAE\nscore_rmse = np.sqrt(mean_squared_error(y_test[0], test_predict[:,0]))\nscore_mae = mean_absolute_error(y_test[0], test_predict[:,0])\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))","e5f90b5d":"x_train_ticks = univariate_df.head(train_size)['ds']\ny_train = univariate_df.head(train_size)['y']\nx_test_ticks = univariate_df.tail(test_size)['ds']\n\n# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nsns.lineplot(x=x_train_ticks, y=y_train, ax=ax, label='Train Set') #navajowhite\nsns.lineplot(x=x_test_ticks, y=test_predict[:,0], ax=ax, color='green', label='Prediction') #navajowhite\nsns.lineplot(x=x_test_ticks, y=y_test[0], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nplt.show()","6f27b228":"feature_columns = [\n    'rainfall',\n    'temperature',\n    'drainage_volume',\n    'river_hydrometry',\n]\ntarget_column = ['depth_to_groundwater']\n\ntrain_size = int(0.85 * len(df))\n\nmultivariate_df = df[['date'] + target_column + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, [0,2,3,4,5]]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, [0,2,3,4,5]]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","1ba9a6b8":"from fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nmodel.add_regressor('rainfall')\nmodel.add_regressor('temperature')\nmodel.add_regressor('drainage_volume')\nmodel.add_regressor('river_hydrometry')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint(Fore.GREEN + 'RMSE: {}'.format(score_rmse))","bd63aeac":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nplt.show()","1367ee5e":"<a id='5.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">5.1 Models for Univariate Time Series<\/p>\n\nFirst of all, we are going to analize univariate TimeSeries forecasting.\n\n**Univariate time series**: Only one variable is varying over time. For example, data collected from a sensor measuring the temperature of a room every second. Therefore, each second, you will only have a one-dimensional value, which is the temperature.","fc48e04d":"The idea with this plot is to understand which train and test set are we using to fit the model in each iteration. ","4b227582":"<a id='5.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">5.2 Models for Multivariate Time Series<\/p>\n\nFinnally, we are going to analize multivariate TimeSeries forecasting.\n\n**Multivariate time series:** Multiple variables are varying over time. For example, a tri-axial accelerometer. There are three accelerations, one for each axis (x,y,z) and they vary simultaneously over time.","55a2b560":"This is already the case in our data: The time interval is one day and the data is already in chronological order. Therefore, we do not have to do this additional data preparation step.","e25e9ea2":"![Time-Series-Analysis.jpg](attachment:Time-Series-Analysis.jpg)\n\nIn this project I want to deep dive into TimeSeries analysis to show how to review the data, how to preview it and how to engineering.\n\nI also want to explore some of the typical TimeSeries topics such as:\n* ACF\/PACF\n* ARIMA\n* Auto-ARIMA\n* Prophet\n* Augmented Dickey-Fuller (ADF)\n","e3658ea7":"The check for stationarity can be done via three different approaches:\n\n1. **visually**: plot time series and check for trends or seasonality\n2. **basic statistics**: split time series and compare the mean and variance of each partition\n3. **statistical test**: Augmented Dickey Fuller test","f11a4f4c":"<a id='3.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">3.2 TimeSeries Decomposition<\/p>\n\nTime series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components.\n\nThese components are defined as follows:\n\n* **Level**: The average value in the series.\n* **Trend**: The increasing or decreasing value in the series.\n* **Seasonality**: The repeating short-term cycle in the series.\n* **Noise**: The random variation in the series.\n\nDecomposition provides a useful abstract model for thinking about time series generally and for better understanding problems during time series analysis and forecasting.\n\nAll series have a level and noise. The trend and seasonality components are optional.\n\nIt is helpful to think of the components as combining either additively or multiplicatively:\n* **Additive**: $y(t) = Level + Trend + Seasonality + Noise$\n* **Multiplicative**: $y(t) = Level * Trend * Seasonality * Noise$\n\nIn this case we are going to use function seasonal_decompose() from the [statsmodels](https:\/\/www.statsmodels.org\/stable\/generated\/statsmodels.tsa.seasonal.seasonal_decompose.html) library.","abb5ab89":"<a id='2'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">2. Data Preprocessing \u2699\ufe0f<\/p>","e7b619b6":"<a id='3'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">3. Feature engineering \ud83d\udd27<\/p>","732a950c":"<a id='1'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">1. Data visualization \ud83d\udcca<\/p>","ec9d8f93":"<a id='2.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">2.3 Stationarity<\/p>\n\nSome time-series models, such as such as ARIMA, assume that the underlying data is stationary. Stationarity describes that the time-series has\n\n* constant mean and mean is not time-dependent\n* constant variance and variance is not time-dependent\n* constant covariance and covariance is not time-dependent\n\n![stationarity.png](attachment:stationarity.png)","a75be3dd":"In this **visual check**, we can see that the **features don't have constant mean and std**, but they are close to it.","bcd2e51e":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">Table of Content<\/p>\n\n* [1. Data visualization \ud83d\udcca](#1)\n* [2. Data Preprocessing \u2699\ufe0f](#2)\n    * [2.1 Handle Missings](#2.1)\n    * [2.2 Smoothing data \/ Resampling](#2.2)\n    * [2.3 Stationarity](#2.3)\n        * [2.3.1 Augmented Dickey-Fuller (ADF)](#2.3.1)\n        * [2.3.2 Transforming](#2.3.2)\n        * [2.3.3 Differencing](#2.3.3)\n* [3. Feature engineering \ud83d\udd27](#3)\n    * [3.1 Encoding Cyclical Features](#3.1)\n    * [3.2 TimeSeries Decomposition](#3.2)\n    * [3.3 Lag](#3.3)\n* [4. Exploratory Data Analysis \ud83d\udcca](#4)\n    * [4.1 Autocorrelation Analysis](#4.1)\n* [5. Modeling](#5)\n    * [5.1 Models for Univariate Time Series](#5.1)\n        * [5.1.1 Univariate Prophet](#5.1.1)\n        * [5.1.2 ARIMA](#5.1.2)\n        * [5.1.3 Auto-ARIMA](#5.1.3)\n        * [5.1.4 LSTM](#5.1.4)\n    * [5.2 Models for Multivariate Time Series](#5.2)\n        * [5.1.1 Multivariate Prophet](#5.2.1)\n* [6. Conclusions](#6)\n* [7. References](#7)","a544bf0b":"<a id='5.1.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center\">5.1.2 ARIMA<\/p>\n\nThe second model that i want to try is ARIMA.\n\nThe Auto-Regressive Integrated Moving Average (ARIMA) model describes the **autocorrelations** in the data. The model assumes that the time-series is **stationary**. It consists of three main parts:\n* <font color='purple'>Auto-Regressive (AR) filter (long term)<\/font>: \n    \n    $\\color{purple}{y_t = c + \\alpha_1 y_{t-1} + \\dots \\alpha_{\\color{purple}p}y_{t-\\color{purple}p} + \\epsilon_t = c + \\sum_{i=1}^p{\\alpha_i}y_{t-i} + \\epsilon_t}$  -> p\n* <font color='orange'> Integration filter (stochastic trend)<\/font>\n    \n    -> d\n* <font color='blue'>Moving Average (MA) filter (short term)<\/font>:\n\n    $\\color{blue}{y_t = c + \\epsilon_t + \\beta_1 \\epsilon_{t-1} + \\dots + \\beta_{q} \\epsilon_{t-q} = c + \\epsilon_t + \\sum_{i=1}^q{\\beta_i}\\epsilon_{t-i}} $  -> q \n\n\n**ARIMA**: $y_t = c + \\color{purple}{\\alpha_1 y_{t-1} + \\dots + \\alpha_{\\color{purple}p}y_{t-\\color{purple}p}} + \\color{blue}{\\epsilon_t + \\beta_1 \\epsilon_{t-1} + \\dots + \\beta_{q} \\epsilon_{t-q}}$\n\n\nARIMA(\n<font color='purple'>p<\/font>,\n<font color='orange'>d<\/font>,\n<font color='blue'>q<\/font>)\n\n* <font color='purple'>p<\/font>: Lag order (reference  PACF in [Autocorrelation Analysis](#4.1-Autocorrelation-Analysis))\n* <font color='orange'>d<\/font>: Degree of differencing. (reference  Differencing in [Stationarity](#2.3-Stationarity))\n* <font color='blue'>q<\/font>: Order of moving average (check out ACF in [Autocorrelation Analysis](#4.1-Autocorrelation-Analysis))\n\n### Steps to analyze ARIMA\n\n* **Step 1 \u2014 Check stationarity**: If a time series has a trend or seasonality component, it must be made stationary before we can use ARIMA to forecast. .\n* **Step 2 \u2014 Difference**: If the time series is not stationary, it needs to be stationarized through differencing. Take the first difference, then check for stationarity. Take as many differences as it takes. Make sure you check seasonal differencing as well.\n* **Step 3 \u2014 Filter out a validation sample**: This will be used to validate how accurate our model is. Use train test validation split to achieve this\n* **Step 4 \u2014 Select AR and MA terms**: Use the ACF and PACF to decide whether to include an AR term(s), MA term(s), or both.\n* **Step 5 \u2014 Build the model**: Build the model and set the number of periods to forecast to N (depends on your needs).\n* **Step 6 \u2014 Validate model**: Compare the predicted values to the actuals in the validation sample.","a01ae8a4":"### Missing Values, how to handle\n\n* **Option 1: Fill NaN with Outlier or Zero**\n\nIn this specific example filling the missing value with an outlier value such as np.inf or 0 seems to be very naive. However, using values like -999, is sometimes a good idea.\n\n* **Option 2: Fill NaN with Mean Value**\n\nFilling NaNs with the mean value is also not sufficient and naive, and doesn't seems to be a good option.\n\n* **Option 3: Fill NaN with Last Value with .ffill()**\n\nFilling NaNs with the last value could be bit better.\n\n* **Option 4: Fill NaN with Linearly Interpolated Value with .interpolate()**\n\nFilling NaNs with the interpolated values is the best option in this small examlple but it requires knowledge of the neighouring value","53215405":"<a id='5.1.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center\">5.1.3 Auto-ARIMA<\/p>","34c7e250":"If the data is not stationary but we want to use a model such as ARIMA (that requires this characteristic), the data has to be transformed.\n\nThe two most common methods to transform series into stationarity ones are:\n\n* **Transformation**: e.g. log or square root to stabilize non-constant variance\n* **Differencing**: subtracts the current value from the previous","181b23bd":"<a id='7'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">7. References \ud83d\udcdd<\/p>","4f2d2005":"<a id='4'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">4. Exploratory Data Analysis \ud83d\udcca<\/p>\n\nNow, we are going to plot the data and try to extract some knowledge.","cbbb1b1e":"<a id='5'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">5. Modeling \ud83e\udde9<\/p>\n\nTime series can be either univariate or multivariate:\n\n* **Univariate** time series only has a single time-dependent variable.\n* **Multivariate** time series have a multiple time-dependent variable.\n\nBut, first of all we are going to see how does cross-validation technic works in TimeSeries Analysis. ","5d626b30":"<a id='2.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">2.2 Smoothing data \/ Resampling<\/p>\n\nResampling can provide additional information on the data. There are two types of resampling:\n\n* **Upsampling** is when the frequency of samples is increased (e.g. days to hours)\n* **Downsampling** is when the frequency of samples is decreased (e.g. days to weeks)\n\nIn this example, we will do some downsampling with the .resample() function (similar to groupby and aggregate as mean).","4efdf89f":"<a id='2.3.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center\">2.3.3 Differencing<\/p>","f2b87ed3":"<a id='2.3.2'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center\">2.3.2 Transforming<\/p>","31fd5514":"As we can see:\n* **depth_to_groundwater**: reaches its maximum around May\/June and its minimum around November\n* **temperature**: reaches its maxmium around August and its minimum around January\n* **drainage_volume**: reaches its minimum around July.\n* **river_hydrometry**: reaches its maximum around February\/March and its minimum around September","572c6c26":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">TimeSeries \ud83d\udcc8 ARIMA, Prophet, ADF, PACF... \ud83d\udcda Beginner to Pro<\/p>","ff1ccf9e":"Lets encode this cyclical feature:\n\n![cyclical-features.gif](attachment:cyclical-features.gif)","b054ac39":"<a id='4.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">4.1 Autocorrelation Analysis<\/p>\n\nACF and PACF plots: After a time series has been stationarized by differencing, the next step in fitting an ARIMA model is to determine whether AR or MA terms are needed to correct any autocorrelation that remains in the differenced series. Of course, with software like Statgraphics, you could just try some different combinations of terms and see what works best. But there is a more systematic way to do this. By looking at the **autocorrelation function (ACF)** and **partial autocorrelation (PACF)** plots of the differenced series, you can tentatively identify the numbers of AR and\/or MA terms that are needed.\n\n* **Autocorrelation Function (ACF)**: P = Periods to lag for eg: (if P= 3 then we will use the three previous periods of our time series in the autoregressive portion of the calculation) P helps adjust the line that is being fitted to forecast the series. P corresponds with MA parameter\n* **Partial Autocorrelation Function (PACF)**: D = In an ARIMA model we transform a time series into stationary one(series without trend or seasonality) using differencing. D refers to the number of differencing transformations required by the time series to get stationary. D corresponds with AR parameter.\n\nAutocorrelation plots help in detecting seasonality.","20afaba5":"<a id='5.1.4'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center\">5.1.4 LSTM<\/p>\n\nWe are going to use a multi-layered LSTM recurrent neural network to predict the last value of a sequence of values.\n\nThe following data pre-processing and feature engineering need to be done before construct the LSTM model.\n* Create the dataset, ensure all data is float.\n* Normalize the features.\n* Split into training and test sets.\n* Convert an array of values into a dataset matrix.\n* Reshape into X=t and Y=t+1.\n* Reshape input to be 3D (num_samples, num_timesteps, num_features).","031ed78a":"<a id='5.1.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center\">5.1.1 Prophet<\/p>\n\nThe first model (which also can handle multivariate problems) we are going to try is Facebook Prophet.\n\nProphet, or \u201cFacebook Prophet,\u201d is an open-source library for univariate (one variable) time series forecasting developed by Facebook.\n\nProphet implements what they refer to as an additive time series forecasting model, and the implementation supports trends, seasonality, and holidays.","67199efb":"## Chronological Order and Equidistant Timestamps\n\nThe data should be in chronological order and the timestamps should be equidistant in time series. The chronological order can be achieved by sorting the dataframe by the timestamps. Equidisant timestamps indicates constant time intervals. To check this, the difference between each timestamp can be taken. If this is not the case, you can decide on a constant time interval and resample the data.\n\nBut, first of all, lets see how to check if our dataset is correctly formated!","56edbbbd":"<a id='2.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">2.1 Handle Missings<\/p>\n\nAs we can see, the dataset has some null values.\n\nFurthermore, plotting the time series reveals that there seem to be **some zero values that seems to be nulls** for `drainage_volume`, and `river_hydrometry`. We will have to clean them by replacing them by nan values and filling them afterwards","780eb230":"<a id='2.3.1'><\/a>\n### Unit Root Test\n\nUnit root is a characteristic of a time series that makes it non-stationary. And ADF test belong to the unit root test. Technically , a unit root is said to exist in a time series of value of alpha =1 in below equation.\n\n$Y_t = \t\\alpha Y_{t-1} + \t\\beta X_{e} + \\epsilon $\n\nwhere Yt is value of the time series at time \u2018t\u2019 and Xe is an exogenous variable .\n\n**The presence of a unit root means the time series is non-stationary.**\n\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center\">2.3.1 Augmented Dickey-Fuller (ADF)<\/p>\n\n\n**Augmented Dickey-Fuller (ADF)** test is a type of statistical test called a unit root test. Unit roots are a cause for non-stationarity.\n\n* **Null Hypothesis (H0)**: Time series has a unit root. (Time series is not stationary).\n\n* **Alternate Hypothesis (H1)**: Time series has no unit root (Time series is stationary).\n\n**If the null hypothesis can be rejected, we can conclude that the time series is stationary.**\n\nThere are two ways to rejects the null hypothesis:\n\nOn the one hand, the null hypothesis can be rejected if the p-value is below a set significance level. The defaults significance level is 5%\n\n* <font color='red'>**p-value > significance level (default: 0.05)**<\/font>: Fail to reject the null hypothesis (H0), the data has a unit root and is <font color='red'>non-stationary<\/font>.\n* <font color='green'>**p-value <= significance level (default: 0.05)**<\/font>: Reject the null hypothesis (H0), the data does not have a unit root and is <font color='green'>stationary<\/font>.\n    \nOn the other hand, the null hypothesis can be rejects if the test statistic is less than the critical value.\n* <font color='red'>**ADF statistic > critical value**<\/font>: Fail to reject the null hypothesis (H0), the data has a unit root and is <font color='red'>non-stationary<\/font>.\n* <font color='green'>**ADF statistic < critical value**<\/font>: Reject the null hypothesis (H0), the data does not have a unit root and is <font color='green'>stationary<\/font>.","d30834b1":"Now, we are going to check for each variable:\n* The p-value is less than 0.05\n* Check the range of the ADF statistic compared with critical_values ","02883972":"So how to interpret the plot diagnostics?\n\n* **Top left**: The residual errors seem to fluctuate around a mean of zero and have a uniform variance between (-4, 4).\n\n* **Top Right**: The density plot suggest normal distribution with mean zero.\n\n* **Bottom left**: The most part of the blue dots are over the red line, so it seems that the distribution in very low skewed (not skewed for me).\n\n* **Bottom Right**: The Correlogram, aka, ACF plot shows the residual errors are not autocorrelated.","dff1b442":"The best results are taken from Univariate LSTM (with rolling window of 1 year) and multi-variate Prophet.","9014e9b5":"As we saw in the previous Steps, AutoARIMA (auto_arima) validates that (1,1,1) is the best configuration for (p,d,q).","93e5f7cf":"<a id='3.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">3.1 Encoding Cyclical Features<\/p>\n\nThe new time features are cyclical. For example,the feature month cycles between 1 and 12 for every year. While the difference between each month increments by 1 during the year, between two years the `month` feature jumps from 12 (December) to 1 (January). This results in a -11 difference, which can confuse a lot of models.","46d56e73":"Since this is a TimeSeries problem, we have to parse 'date' column","b4b800b7":"Differencing can be done in different orders:\n* First order differencing: linear trends with $z_i = y_i - y_{i-1}$\n* Second-order differencing: quadratic trends with $z_i = (y_i - y_{i-1}) - (y_{i-1} - y_{i-2})$\n* and so on...","3ffe2525":"<a id='5.2.1'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:100%; text-align:center\">5.2.1 Multivariate Prophet<\/p>","96a2a785":"Here I am going to reference some useful links that I have used to build this notebook\n* Special reference for the helpful information and plots - https:\/\/www.kaggle.com\/iamleonie\/intro-to-time-series-forecasting\n* ARIMA - https:\/\/towardsdatascience.com\/time-series-forecasting-arima-models-7f221e9eee06\n* Auto-ARIMA - https:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/\n* Keras LSTM - https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/\n* Prophet - https:\/\/towardsdatascience.com\/time-series-prediction-using-prophet-in-python-35d65f626236\n* Special reference - https:\/\/www.kaggle.com\/iamleonie\/intro-to-time-series-forecasting\/notebook#Models\n* Cyclical features - https:\/\/towardsdatascience.com\/cyclical-features-encoding-its-about-time-ce23581845ca\n* ADF - https:\/\/medium.com\/@cmukesh8688\/why-is-augmented-dickey-fuller-test-adf-test-so-important-in-time-series-analysis-6fc97c6be2f0\n* ACF\/PACF - https:\/\/towardsdatascience.com\/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8\n* LSTM - https:\/\/towardsdatascience.com\/time-series-analysis-visualization-forecasting-with-lstm-77a905180eba","109ba980":"As we can see, the best option in this case, **seems to best option!**\n\nSo, lets interplota missing values","dde06761":"<a id='6'><\/a>\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center\">6. Conclusions \ud83d\udc8e<\/p>","9196fe1e":"Features:\n* **Rainfall** indicates the quantity of rain falling (mm)\n* **Temperature** indicates the temperature (\u00b0C)\n* **Volume** indicates the volume of water taken from the drinking water treatment plant (m 3 )\n* **Hydrometry** indicates the groundwater level (m)\n\nTarget:\n* **Depth to Groundwater** indicates the groundwater level (m from the ground floor)","6e3d37f2":"<a id='3.3'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:120%; text-align:center\">3.3 Lag<\/p>\n\n\nWe want to calculate each variable with a `shift()` (lag) to compare the correlationwith the other variables.\n\nYou can see [documentation of shift function](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.shift.html) for more information.","efa08dab":"So now we have to fix missing values. Lets take a look of these cases and explore what can we do with them.","a7ece720":"As we can see, the features are higher correlated in the case of shifted features (lagged ones) than the original ones."}}