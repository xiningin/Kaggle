{"cell_type":{"d458d31b":"code","c7967524":"code","0d49e3a4":"code","75124d31":"code","2cec73a3":"code","e0e34250":"code","d48edfdb":"code","a8a0f15c":"code","83d0d1c5":"code","725e0236":"code","764a350f":"code","a5fc25f9":"code","270d2179":"code","42984bf0":"code","57c0d723":"code","44f2a757":"code","cf1399ff":"code","f1b60be0":"code","bb0b9a44":"code","1e41b790":"code","8467311c":"code","c59c597e":"code","e2a63089":"code","0fde0eed":"code","799d2181":"code","5775a166":"code","15a55812":"code","827a5290":"code","eff57420":"code","25252235":"code","808dc397":"code","0e69817e":"code","6ab04a92":"code","86dca5da":"code","2d34464b":"code","282ffb21":"code","c2bee640":"code","20b12c1c":"code","03bb0438":"code","86b510ae":"code","106b0ca2":"code","7d346ae5":"code","8edaec7a":"code","0eb26a64":"code","48bac19c":"code","864b778e":"code","be449180":"code","e6e60470":"code","d3033479":"code","12038d37":"code","b27d94b9":"code","e03e41a2":"code","1b05c928":"code","6aa4f3ca":"code","505dd192":"code","51f14a7e":"code","52778360":"code","bf2ab8b9":"code","e1c058a1":"code","4886c34a":"code","d02a2a3a":"code","e9051095":"markdown","40424809":"markdown","be372c83":"markdown","405b93d0":"markdown","ce53c84e":"markdown","468de192":"markdown","9354f687":"markdown","54a37a89":"markdown","03cfc493":"markdown","bcbdc324":"markdown","f5af9270":"markdown","60e3aac2":"markdown","dc40198e":"markdown","0064bb71":"markdown","c6a86b86":"markdown","b2d51399":"markdown","ba59a8dc":"markdown","56af6e68":"markdown","373bf045":"markdown","72a6caf6":"markdown","4fdd2341":"markdown","ebafb3b2":"markdown","9e000961":"markdown","5a979c3f":"markdown","8e01fa6c":"markdown","d5df443f":"markdown","5bcd84c3":"markdown","86acd8d3":"markdown","4bb67084":"markdown","e6481936":"markdown","0bc2f245":"markdown","1a094061":"markdown","946c1ba0":"markdown","04fe7a1b":"markdown","1bf62cbd":"markdown","0c844515":"markdown","372357fc":"markdown","2d85d494":"markdown","fcdcaf10":"markdown","7ee9d9c2":"markdown","5d6a16fc":"markdown","b6fd618a":"markdown","2b326db5":"markdown","2c40db19":"markdown","ba7637ee":"markdown","be72649e":"markdown","823182e5":"markdown","3df2de29":"markdown","3c54ae1b":"markdown","52889e60":"markdown","ca57cb47":"markdown","80d6a599":"markdown","71604db4":"markdown"},"source":{"d458d31b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","c7967524":"df_columns  = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"]","0d49e3a4":"df  = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', names=df_columns)","75124d31":"df.head()","2cec73a3":"df = df.drop(['id', 'date', 'flag', 'user'], axis = 1)","e0e34250":"lab_to_string = {0:\"Negative\", 4:\"Positive\"}\ndef label_decoder(label):\n  return lab_to_string[label]\ndf.target = df.target.apply(lambda x: label_decoder(x))","d48edfdb":"df.head()","a8a0f15c":"sns.countplot(x='target', data=df);\nprint (df.target.value_counts())","83d0d1c5":"df.head(20)","725e0236":"df.tail(20)","764a350f":"df = df.sample(frac=1).reset_index(drop=True)","a5fc25f9":"df.head()","270d2179":"import nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re","42984bf0":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\ntext_regex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n# this regex is for mentions and links, regex can be so fancy I got this regex from here\n# https:\/\/gist.github.com\/ryanblock\/3506204","57c0d723":"def clean_text(text, stem=False):\n  text = re.sub(text_regex, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    if token not in stop_words:\n      if stem:\n        tokens.append(stemmer.stem(token))\n      else:\n        tokens.append(token)\n  return \" \".join(tokens)","44f2a757":"df.text = df.text.apply(lambda x: clean_text(x))","cf1399ff":"df.head(10)","f1b60be0":"from wordcloud import WordCloud","bb0b9a44":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.target == 'Positive'].text))\nplt.imshow(wc , interpolation = 'bilinear');","1e41b790":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.target == 'Negative'].text))\nplt.imshow(wc , interpolation = 'bilinear');","8467311c":"from sklearn.model_selection import train_test_split","c59c597e":"X, y = train_test_split(df, test_size=.20, random_state=42)\n# I will make 80% of the data for the training as we still have another tweets ","e2a63089":"X.shape, y.shape","0fde0eed":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer","799d2181":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X.text)\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1","5775a166":"print (vocab_size)","15a55812":"from tensorflow.keras.preprocessing.sequence import pad_sequences","827a5290":"x_train = pad_sequences(tokenizer.texts_to_sequences(X.text),\n                        maxlen =30)\nx_test = pad_sequences(tokenizer.texts_to_sequences(y.text),\n                       maxlen = 30)","eff57420":"from sklearn.preprocessing import LabelEncoder","25252235":"LE = LabelEncoder()\nLE.fit(X.target.to_list())\n\ny_train = LE.transform(X.target.to_list())\ny_test = LE.transform(y.target.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)","808dc397":"y_train.shape, y_test.shape","0e69817e":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove.6B.zip","6ab04a92":"GLOVE_EMB = '\/kaggle\/working\/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 1024\nEPOCHS = 1\nMODEL_PATH = '...\/output\/kaggle\/working\/best_model.hdf5'","86dca5da":"embeddings_index = {}\n\nf = open(GLOVE_EMB, encoding=\"utf8\")\nfor line in f:\n  values = line.split()\n  word = value = values[0]\n  coefs = np.asarray(values[1:], dtype='float32')\n  embeddings_index[word] = coefs\nf.close()","2d34464b":"print(len(embeddings_index))","282ffb21":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector","c2bee640":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=30,\n                                          trainable=False)","20b12c1c":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau","03bb0438":"sequence_input = Input(shape=(30,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","86b510ae":"model.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])","106b0ca2":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","7d346ae5":"score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint('-----')\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])","8edaec7a":"df = pd.read_csv('..\/input\/teams-tweets-for-nlp\/teams.csv')","0eb26a64":"df.head()","48bac19c":"df.tweet = df.tweet.apply(lambda x: clean_text(x))","864b778e":"df.head()","be449180":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Team == 'Manchester'].tweet))\nplt.imshow(wc , interpolation = 'bilinear');","e6e60470":"plt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(df[df.Team == 'Barcelona'].tweet))\nplt.imshow(wc , interpolation = 'bilinear');","d3033479":"x = df.tweet","12038d37":"x.head()","b27d94b9":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(x)\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1","e03e41a2":"print (vocab_size)","1b05c928":"x = pad_sequences(tokenizer.texts_to_sequences(x),\n                        maxlen =30)","6aa4f3ca":"result = model.predict(x)","505dd192":"result","51f14a7e":"df['value'] = result","52778360":"df.head()","bf2ab8b9":"df['value']= df['value'].astype(str)","e1c058a1":"df.loc[df['value'] > '0.6', 'value'] = 'Positive'\ndf.loc[df['value'] < '0.4', 'value'] = 'Negative'\ndf.loc[df['value'].between('0.4', '0.6'), 'value'] = 'Neutral'           ","4886c34a":"df.head(20)","d02a2a3a":"sns.countplot(x='Team', hue='value', data=df)","e9051095":"# First - Building The Model","40424809":"Getting the data is specefied in the other ipynb , please check it I just used diffrent Notebook cuz twiitter API Consume Most of the Memory and this kernal is pretty heavy so I didn't want it to crash\n<br>\n\"I Also needed a clean workspace, things here are little bit messy (:\"","be372c83":"We Reached our Goal !!\n<br>\nAm sure that manchester will skew a little to Positives but due that we have no much tweets it appears like it is equal","405b93d0":"- **Hyperlinks and Mentions**\nTwitter is a social media platform where people can tag and mentions other people's ID and share videos and blogs from internet. So the tweets often contain lots of Hyperlinks and twitter mentions.\n- Mentions >> @mazen, @someone\n- Hyperlinks >>  https:\/\/keras.io, https:\/\/tensorflow.org","ce53c84e":"### Model Prepairing and Encoding","468de192":"### Refrences with Workflow","9354f687":"As was displayed above the dataset is indexed as negative comes first then positives so wee need to randomize it as for the traiin test split part, we don't need the most of the train set to contain negatives and of the test set contains positives, so we will randomize it","54a37a89":"# Sentimental Analysis","03cfc493":"# Sentimental Analysis","bcbdc324":"- Twitter Preprocessing and Text Cleaning with Regex: [Here](https:\/\/www.kdnuggets.com\/2016\/06\/mining-twitter-data-python-part-2.html)\n- NLTK Stop Words: [Here](https:\/\/pythonspot.com\/nltk-stop-words\/)\n- Creating Word Cloud plot: [Here](https:\/\/www.datacamp.com\/community\/tutorials\/wordcloud-python)\n- Sentdex Helped me much with Tokenization and all the playlist with helpful: [Here](https:\/\/www.youtube.com\/watch?v=FLZvOKSCkxY&ab_channel=sentdex)\n- How Word Embeding Works: [Here](https:\/\/www.youtube.com\/watch?v=pO_6Jk0QtKw)\n- Stanford's Embeding library: [Here](https:\/\/rcpedia.stanford.edu\/topicGuides\/textProcessingWord_Embeddings.html)\n- Neural Networks for Text Classification: [Here](https:\/\/www.youtube.com\/watch?v=VxHy1lQRzvs)\n- This Helped me getiing best Hyperparamters, Optimizers, Activation Functions for the Neural Networks: [Here](https:\/\/realpython.com\/python-keras-text-classification\/)\n- Using Tweepy and Twitter API: [Here](https:\/\/www.earthdatascience.org\/courses\/use-data-open-source-python\/intro-to-apis\/twitter-data-in-python\/)\n","f5af9270":"### Now to the Nueral Networks","60e3aac2":"Word cloud for Barcelona","dc40198e":"What is **Word Embeding** ?\n<br>\nA word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems","0064bb71":"**Word Index** contains the index for each word","c6a86b86":"Now Let's change our Sentimental Value < .5 for Negative & > .5 for Positive","b2d51399":"### Evaluating The Model","ba59a8dc":"## Prepairing tweets data","56af6e68":"**Vocab Size** represents the total number of word in the data corpus","373bf045":"- **Stemming\/Lematization** \nStemming and Lemmatization both generate the foundation sort of the inflected words and therefore the only difference is that stem may not be an actual word whereas, lemma is an actual language word. Stemming follows an algorithm with steps to perform on the words which makes it faster.\n- adjustable >> adjust #stemming\n- was >> (to) be # Lematization","72a6caf6":"#### Positives","4fdd2341":"thankfully, the dataset is equally splited each target has 800,000 values","ebafb3b2":"Ok, Why I chose **Manchester united** and **Barcelona** ?\nIf you are following footbal 'or not even following it you must have read it somewhere in social media' Reccently Cristiano Ronaldo joined his old team again which is united and he left juventus and Most people are very happy with that On the other side Messi also reccently left his olf team Barcelona and He was there for over 20 years!! and also most of people are sad or negative about it and the team is performing so badly without him\n<br>\nWow that matches our problem here !, We needed 2 campains to see if tweets are positive or negative","9e000961":"### Encoding\nThe goal here is to classify a binary values of 0 & 1, As Machine Learning don't understand String on our target values, we will encode it","5a979c3f":"I trained this model using Google Colab workspace as it smoothly run on my GPU but here locally there is some problems with CUDNN so I used only 1 epoch so it won't take much time, but generally accuracy is higher when using more epochs","8e01fa6c":"**Text Classification** is a process involved in Sentiment Analysis. It is classification of peoples opinion or expressions into different sentiments. Sentiments include Positive, Neutral, and Negative, Review Ratings and Happy, Sad. Sentiment Analysis can be done on different consumer centered industries to analyse people's opinion on a particular product or subject.","d5df443f":"Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. The process is called Tokenization.","5bcd84c3":"Much better","86acd8d3":"Now let's Claen the Tweets text with the function we build earlier","4bb67084":"The input shape will be 30 as it is the same as MaxLen we defiened\n<br>\nI used relu activation across all the layers except for the Dense Layer as It must be sigmoid for classifying problems as the \ntarget is from 0 to 1\n<br>\nthe droptout layer is to prevent overfitting I tried to make it .7 but .5 gave me better accuracy\n<br>","e6481936":"the data sets has 6 columns but we are only intersted in the text, target columns so we will drop the rest ","0bc2f245":"Maxlen:\n<br>\nIn the dataset, there will n number of training samples. In the context of NLP, these training samples are not guaranteed to be of fixed length. Some may be of length 1024, some may be of length 24 and so on.\nin our model, that is used in the example needs datasamples to be of fixed length. Inorder to make sure that all the samples are of atleast a minimum length, they will be padded. Here post padding is used so I assgned it to be 30 as I already tOkenized words we don't need very high value.","1a094061":"for the target column the 0 refers to negative, 4 refers to positive so I will the change the values to appear as a string","946c1ba0":"**Sentiment Analysis** is a perfect problem in NLP for getting started in it. You can really learn a lot of concepts and techniques to master through doing project. Kaggle is a great place to learn and contribute your own ideas and creations. I learnt lot of things from other, now it's my turn to make document my project.","04fe7a1b":"There is many  GloVe & Word2Vec types we will use one ffrom Stanford Ai\n<br>\nhttps:\/\/nlp.stanford.edu\/projects\/glove\/\n<br>\nAnd I already had the package and I wanted to try it\n<br>\nyou can download it from here \n<br>\nhttp:\/\/nlp.stanford.edu\/data\/glove.6B.zip","1bf62cbd":"**Tokinzer** create tokens for every word in the data corpus and map them to a index using dictionary.","0c844515":"#### Negatives","372357fc":"I think this is beautiful! how python can be so powerful\n<br>\nThe  remaining thing is tokenizing","2d85d494":"### Word Embeding","fcdcaf10":"Word cloud For Manchester","7ee9d9c2":"**Natural Language Processing** or NLP is a branch of Artificial Intelligence which deal with bridging the machines understanding humans in their Natural Language. Natural Language can be in form of text or sound, which are used for humans to communicate each other. NLP can enable humans to communicate to machines in a natural way.","5d6a16fc":"### Preprocessing The Text","b6fd618a":"### Wordclouds For postives and Negatives","2b326db5":"Loading out GloVe and specifying batch, epoches sizes","2c40db19":"- **Stopwords**\nStopwords are commonly used words in English which have no contextual meaning in an sentence. So therefore we remove them before classification.\n'I', 'my', 'myself', 'ourself'","ba7637ee":"Now we will use **Pad Sequence**\n<br>\nwhat does it to ?\nIt is used to ensure that all sequences in a list have the same length, By default this is done by padding 0 in the beginning of each sequence until each sequence has the same length as the longest sequence.","be72649e":"**What we will do ?**\n- We will build a ML model that classify tweets as positive or negative\n- Using Twitter API and tweety to get tweets based on keywords that we will specifiy later\n- We will use The ML model that we built earlier to make a real-time Setimental Analysis on tweets that we got","823182e5":"Now we use Adam Optimizer , I tried Gradient Descent Optimzer but Adam gave me higher accuracy, Again Machine Learning is the art of experimenting !\n<br>\nFor the Learning rate paramaters I did similar project on NLP before I just copied them","3df2de29":"**NOTE:** We could use Word2Vec or tfidf then we classify the data with LogisticRegression or SVM but in our case the data is so big it will extremly hard and slow to Tune the model and we won't gett a good model as you know Machine learning is the art of experimenting\n<br>\nThe Perfect option here is to use Nueral Networks, it doesn't need much tuning and It can runs on my GPU much faster","3c54ae1b":"The text we have is from twitter so it contains @username, hyperlink texts, emoticons and punctuations so we gotta clean it for our model and we will Use NLTK library for it","52889e60":"Now lets tokenize this text","ca57cb47":"Now we Embed our Glove as keras layer that we will use for Nueral Networks","80d6a599":"**Please Consider Upvoting if you liked the kernal**","71604db4":"### Tokenization"}}