{"cell_type":{"6a46d163":"code","5486c04d":"code","017faec8":"code","275be80b":"code","fb8feabb":"code","4e07031b":"code","27bf5d0d":"code","377fbe60":"code","3e29afa2":"code","be5f0ff8":"code","ab920ca4":"code","af3e2b40":"code","32800e0d":"code","fce0ab79":"code","c98b04d3":"code","54781a48":"code","4bb81e08":"code","bb8cba88":"code","6049821c":"code","ebdc9a4a":"code","32b2d2dc":"markdown","6a19f60f":"markdown","a3cb6039":"markdown","ec0ea72d":"markdown","5e4f77e7":"markdown","7f03f0f1":"markdown","e4234596":"markdown","e824ee06":"markdown","b8314156":"markdown","576c9577":"markdown","74d9df57":"markdown","658ccc22":"markdown","387ca7c3":"markdown","126e8188":"markdown","8bb240a0":"markdown","2382681f":"markdown","638cfead":"markdown","3834cd6d":"markdown","8f01da30":"markdown","7eea7ded":"markdown","75bd70e2":"markdown","b022abae":"markdown","cc3c48c2":"markdown"},"source":{"6a46d163":"!pip install -q keras-bert keras-rectified-adam\n!wget -q https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip\n!unzip -o uncased_L-12_H-768_A-12.zip","5486c04d":"SEQ_LEN = 128\nBATCH_SIZE = 1024\nEPOCHS = 15\nLR = 1e-4","017faec8":"import os\n\npretrained_path = 'uncased_L-12_H-768_A-12'\nconfig_path = os.path.join(pretrained_path, 'bert_config.json')\ncheckpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\nvocab_path = os.path.join(pretrained_path, 'vocab.txt')\n\n# TF_KERAS must be added to environment variables in order to use TPU\nos.environ['TF_KERAS'] = '1'","275be80b":"import tensorflow as tf\nfrom keras_bert import get_custom_objects\n\n# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","fb8feabb":"import codecs\nfrom keras_bert import load_trained_model_from_checkpoint\n\ntoken_dict = {}\nwith codecs.open(vocab_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n\nwith tpu_strategy.scope():\n    model = load_trained_model_from_checkpoint(\n        config_path,\n        checkpoint_path,\n        training=True,\n        trainable=True,\n        seq_len=SEQ_LEN,\n    )","4e07031b":"import pandas as pd\n\ntrain_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col='id')\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', index_col='id')","27bf5d0d":"import string, re\nfrom bs4 import BeautifulSoup\n\ndef strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n\n# Removing URL's\ndef remove_url(text):\n    return re.sub(r'http\\S+', '', text)\n\ndef add_space(text):\n    return re.sub('%20', ' ', text)\n\ndef remove_hashtags(text):\n    return re.sub('#', '', text)\n\ndef remove_at_signs(text):\n    return re.sub('@', '', text)\n\n#Removing the stopwords from text\ndef remove_punctuation(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = add_space(text)\n    text = remove_url(text)\n    text = remove_hashtags(text)\n    text = remove_at_signs(text)\n    text = remove_punctuation(text)\n    return text","377fbe60":"def preprocess_df(df):\n    df = df.fillna(\"\")\n    df['text'] = df['keyword'] + \" \" + df['text']\n    del df['keyword']\n    df['location'] = df['location'].astype('category')\n    df['location'] = df['location'].cat.codes\n    df['text'] = df['text'].apply(denoise_text)\n    return df\n\ntrain_df = preprocess_df(train_df)\ntest_df = preprocess_df(test_df)","3e29afa2":"import numpy as np\n\nnp.random.seed(1)\n\nmsk = np.random.rand(len(train_df)) < 0.8\ntrain_df, dev_df = train_df[msk], train_df[~msk]","be5f0ff8":"from tqdm import tqdm\nfrom keras_bert import Tokenizer\nimport numpy as np\n\ntokenizer = Tokenizer(token_dict)\n\ndef tokenize(df): \n    X, y = [], []\n    for i, index in enumerate(tqdm(df.index.values)):\n        ids, segments = tokenizer.encode(df.text.values[i], max_len=SEQ_LEN)\n        X.append(ids)\n        try:\n            label = df.target.values[i]\n            y.append(label)\n        except:\n            y.append(0)\n    items = list(zip(X, y))\n    np.random.shuffle(items)\n    indices, sentiments = zip(*items)\n    indices = np.array(indices)\n    mod = indices.shape[0] % BATCH_SIZE\n    if mod > 0:\n        indices, sentiments = indices[:-mod], sentiments[:-mod]\n    return [indices, np.zeros_like(indices)], np.array(sentiments)\n\n\nX_train, y_train = tokenize(train_df)\nX_dev, y_dev = tokenize(dev_df)","ab920ca4":"X_test = []\nfor i, index in enumerate(tqdm(test_df.index.values)):\n    ids, segments = tokenizer.encode(test_df.text.values[i], max_len=SEQ_LEN)\n    X_test.append(ids)\nX_test = [np.array(X_test), np.zeros_like(X_test)]","af3e2b40":"from tensorflow.python import keras\nfrom keras_radam import RAdam\n\nwith tpu_strategy.scope():\n    inputs = model.inputs[:2]\n    dense = model.get_layer('NSP-Dense').output\n    outputs = keras.layers.Dense(units=2, activation='softmax')(dense)\n    model = keras.models.Model(inputs, outputs)\n    model.compile(\n        RAdam(lr=LR),\n        loss='sparse_categorical_crossentropy',\n        metrics=['sparse_categorical_accuracy'],\n    )","32800e0d":"model.summary()","fce0ab79":"learning_rate_reduction = keras.callbacks.ReduceLROnPlateau(monitor='val_sparse_categorical_accuracy', patience=2, verbose=1,factor=0.5, min_lr=1e-5)","c98b04d3":"hist = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[learning_rate_reduction])","54781a48":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(10, 10))\n\nepochs = np.arange(EPOCHS)\nplt.subplot(2, 2, 1)\nplt.xlabel('epochs')\nplt.ylabel('loss')\nsns.lineplot(epochs, hist.history['loss'])\n\nplt.subplot(2, 2, 2)\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nsns.lineplot(epochs, hist.history['sparse_categorical_accuracy'])\n\nplt.subplot(2, 2, 3)\nplt.xlabel('epochs')\nplt.ylabel('val_loss')\nsns.lineplot(epochs, hist.history['val_loss'])\n\nplt.subplot(2, 2, 4)\nplt.xlabel('epochs')\nplt.ylabel('val_accuracy')\nsns.lineplot(epochs, hist.history['val_sparse_categorical_accuracy'])\n\nplt.show()","4bb81e08":"print(\"Accuracy of the model on Training Data is - {} %\".format(model.evaluate(X_train,y_train)[1]*100))\nprint(\"Accuracy of the model on Dev Data is - {} %\".format(model.evaluate(X_dev,y_dev)[1]*100))","bb8cba88":"classes = model.predict(X_test)[:, 0]","6049821c":"submission = pd.DataFrame(\n    {'id': list(test_df.index.values),\n     'target': list((classes < 0.5).astype(int)),\n    }).set_index('id')","ebdc9a4a":"submission.to_csv('submission.csv')","32b2d2dc":"We have now preprocessed the data, so we are now ready to initialize and train our model.","6a19f60f":"The layers of the model are as follows.","a3cb6039":"We then concatenate the keyword and text columns into one single text column, and remove the location column.","ec0ea72d":"We check the loss and accuracy of the model on the training and dev data for each epoch.","5e4f77e7":"We set up a denoiser function to apply to the text in the dataframe, which removes punctuation, erroneous @ signs, urls etc.","7f03f0f1":"# Prerequisites","e4234596":"**If you enjoyed this notebook, please give it an upvote!**","e824ee06":"In order to set up the BERT model, there are a number of steps we have to take first. First, we install a few convenient packages.","b8314156":"We set up the inputs and outputs of the model, with the pretrained BERT layer followed by a dense layer of 2 outputs (probability of each class).","576c9577":"We add learning rate reduction to prevent overfitting to the training set.","74d9df57":"The accuracy on the training and dev data is similar: approximately 80-85%","658ccc22":"# Generating New Predictions","387ca7c3":"# Model Training ","126e8188":"# Data Preprocessing","8bb240a0":"The loss and accuracy graphs look good with no clear overfitting.","2382681f":"After applying our preprocessing steps to the dataframes, we split the training dataframe into dev and training sub dataframes.","638cfead":"The next preprocessing step is tokenization. This is done through the Tokenizer class from the Keras-Bert package. ","3834cd6d":"We set the global variables to be used throughout.","8f01da30":"We now load and preprocess the data for input into the model.","7eea7ded":"The next three cells set up the TPU environment, and load the pretrained model.","75bd70e2":"Finally, we create the set of predictions on the test data, and create the submission file!","b022abae":"Finally, we fit the model.","cc3c48c2":"# Model Evaluation"}}