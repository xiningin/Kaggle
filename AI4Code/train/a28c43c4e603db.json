{"cell_type":{"0fa9ea74":"code","b92cc899":"code","0b2bdfe5":"code","2b419331":"code","b3ddf5de":"code","330ac233":"code","93207599":"code","e9eb8da2":"code","0ee72892":"code","6db6f03b":"code","71a4fc4e":"code","c8cadf43":"code","f06002b2":"code","919bca38":"code","376409c2":"code","008fc188":"code","109a1bb3":"code","80832d88":"code","5029b7d8":"code","ccfabc72":"code","c62048d4":"code","32ea3b98":"code","9bff3b7e":"code","4103e47e":"code","4351ec16":"code","b6bd6767":"code","2800d757":"code","7034bb01":"code","9bd1d987":"code","a425b959":"code","ebff9f79":"markdown","58df6e7a":"markdown","ba895023":"markdown","dedc9dcb":"markdown","0db20cf4":"markdown","d9b249fd":"markdown","fd2385b2":"markdown","71aef2be":"markdown","98512d6f":"markdown"},"source":{"0fa9ea74":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport transformers\nfrom transformers import RobertaModel, RobertaTokenizer\nimport torch\n\nfrom torch.utils.data import DataLoader, Dataset\nimport re\nimport matplotlib.pyplot as plt\n%matplotlib inline","b92cc899":"train=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin1')\ntest=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding='latin1')","0b2bdfe5":"train.head()","2b419331":"train = train[['OriginalTweet', 'Sentiment']]\ntest = test[['OriginalTweet', 'Sentiment']]","b3ddf5de":"text = ' '.join(train['OriginalTweet'])\ntext[:1500]","330ac233":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: x.lower())\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: x.lower())","93207599":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: re.sub('\\r', '', x))\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: re.sub('\\r', '', x))","e9eb8da2":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: re.sub('\\n', '', x))\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: re.sub('\\n', '', x))","0ee72892":"train['OriginalTweet'] = train['OriginalTweet'].apply(lambda x: re.sub(\"\\'\", \"\", x))\ntest['OriginalTweet'] = test['OriginalTweet'].apply(lambda x: re.sub(\"\\'\", \"\", x))","6db6f03b":"text = ' '.join(train['OriginalTweet'])\ntext[:1500]","71a4fc4e":"text[-1500:]","c8cadf43":"train['Sentiment'].unique()","f06002b2":"mapped = {'Extremely Negative': 0,'Negative': 0,'Neutral': 1,'Positive': 2,'Extremely Positive': 2}","919bca38":"train['Sentiment'] = train['Sentiment'].map(mapped)\ntest['Sentiment'] = test['Sentiment'].map(mapped)","376409c2":"pip install transformers","008fc188":"tokenizer = RobertaTokenizer.from_pretrained('roberta-base')","109a1bb3":"max_len = 200","80832d88":"class SentimentData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.OriginalTweet\n        self.targets = self.data.Sentiment\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","5029b7d8":"print(\"TRAIN Dataset: {}\".format(train.shape))\nprint(\"TEST Dataset: {}\".format(test.shape))\n\ntraining_set = SentimentData(train, tokenizer, max_len)\ntesting_set = SentimentData(test, tokenizer, max_len)","ccfabc72":"training_set = SentimentData(train, tokenizer, max_len= max_len)\ntest_set = SentimentData(test, tokenizer, max_len= max_len)","c62048d4":"training_loader = DataLoader(training_set, batch_size = 16)\ntesting_loader = DataLoader(test_set, batch_size= 16)","32ea3b98":"class RobertaClass(torch.nn.Module):\n    def __init__(self):\n        super(RobertaClass, self).__init__()\n        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(768, 5)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output","9bff3b7e":"from torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","4103e47e":"model = RobertaClass()\nmodel.to(device)","4351ec16":"loss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params= model.parameters(), lr = 2e-5)","b6bd6767":"def calcuate_accuracy(preds, targets):\n    n_correct = (preds==targets).sum().item()\n    return n_correct","2800d757":"def train(epoch):\n    tr_loss = 0\n    n_correct = 0\n    nb_tr_steps = 0\n    nb_tr_examples = 0\n    model.train()\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.long)\n\n        outputs = model(ids, mask, token_type_ids)\n        loss = loss_function(outputs, targets)\n        tr_loss += loss.item()\n        big_val, big_idx = torch.max(outputs.data, dim=1)\n        n_correct += calcuate_accuracy(big_idx, targets)\n\n        nb_tr_steps += 1\n        nb_tr_examples+=targets.size(0)\n        \n        if _%5000==0:\n            loss_step = tr_loss\/nb_tr_steps\n            accu_step = (n_correct*100)\/nb_tr_examples \n            print(f\"Training Loss per 5000 steps: {loss_step}\")\n            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n\n        optimizer.zero_grad()\n        loss.backward()\n        # # When using GPU\n        optimizer.step()\n\n    print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)\/nb_tr_examples}')\n    epoch_loss = tr_loss\/nb_tr_steps\n    epoch_accu = (n_correct*100)\/nb_tr_examples\n    print(f\"Training Loss Epoch: {epoch_loss}\")\n    print(f\"Training Accuracy Epoch: {epoch_accu}\")\n\n    return","7034bb01":"EPOCHS = 3\nfor epoch in range(EPOCHS):\n    train(epoch)","9bd1d987":"def testing(model, testing_loader):\n    model.eval()\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(testing_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask, token_type_ids).squeeze()\n            loss = loss_function(outputs, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.data, dim=1)\n            n_correct += calcuate_accuracy(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n            \n            if _%5000==0:\n                loss_step = tr_loss\/nb_tr_steps\n                accu_step = (n_correct*100)\/nb_tr_examples\n                print(f\"Test Loss per 100 steps: {loss_step}\")\n                print(f\"Test Accuracy per 100 steps: {accu_step}\")\n    epoch_loss = tr_loss\/nb_tr_steps\n    epoch_accu = (n_correct*100)\/nb_tr_examples\n    print(f\"Test Loss Epoch: {epoch_loss}\")\n    print(f\"Test Accuracy Epoch: {epoch_accu}\")\n    \n    return epoch_accu","a425b959":"acc = testing(model, testing_loader)\nprint(\"Accuracy on test data = %0.2f%%\" % acc)","ebff9f79":"****Let's do some cleaning.(Roberta is good enough for this dataset even if we don't clean the dataset. You can try it.)****","58df6e7a":"****Trainng****","ba895023":"****Initializing the tokenizer****","dedc9dcb":"****Upvote if you learn something. Next I'll try to use XLNet for this dataset.****","0db20cf4":"****Evaluation****","d9b249fd":"****Creating the network****","fd2385b2":"****Thanks****","71aef2be":"****Taking a look at data****","98512d6f":"****Let's map the Sentiments into something that machine can understand****"}}