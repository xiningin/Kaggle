{"cell_type":{"f6ed1c94":"code","a926afcd":"code","1bec1a70":"code","598715a7":"code","6689bc00":"code","73ab6bdd":"code","27d80095":"code","3c6a72e1":"code","d8f71988":"code","0510d60e":"code","06cbfd85":"code","c108d156":"code","21d6182b":"code","5ee54627":"code","5e87b61f":"code","ceece293":"code","aeb31376":"code","6b269edf":"code","1ea48ac9":"code","c1bc1c09":"code","5a3b481d":"code","873a1816":"code","d0a26d9a":"code","180b4041":"code","4b5d65c1":"code","308d7d5c":"code","19b5c644":"code","9e7b9515":"code","8472f4fd":"code","741935c3":"code","5a0e8f36":"code","30a8ed4b":"code","34789b40":"code","20a63a0f":"code","4845d79e":"code","522a35eb":"code","e12705b6":"code","4347218f":"code","fe659bfa":"code","58e89ec8":"code","1e054e82":"code","7a2c4fbf":"code","4ee509e6":"code","8e9536db":"code","5f8f1573":"code","8de66610":"code","64edc257":"code","99248538":"code","1eb89eb9":"code","8325bea7":"code","c9cddadb":"code","f0c75802":"code","0b8c7f86":"code","91f328e6":"code","374a9c41":"code","423c48b1":"code","4c3a83c5":"code","8dd509b4":"code","b4d7168b":"code","a38b23db":"code","8e95fad0":"markdown","ea81fdf1":"markdown","22d001fe":"markdown","ca407886":"markdown","df52e0cd":"markdown","f0f2d92b":"markdown","089ee640":"markdown","faef7412":"markdown","6a89396c":"markdown","4ebb7a0b":"markdown","63b0ad03":"markdown","df0f253d":"markdown","d881bd0b":"markdown","ba0fe9cb":"markdown","89047245":"markdown","33019674":"markdown","1a477a53":"markdown","edf0e37f":"markdown","9fcf593b":"markdown","7e1d8ae1":"markdown","5e3a6509":"markdown","e465f0f1":"markdown","3fbef945":"markdown","a008eaaf":"markdown","4c0b192c":"markdown","77173666":"markdown","56149f98":"markdown","ac251127":"markdown","82fde0b3":"markdown","44544292":"markdown","cd3d8157":"markdown","eff3f5ee":"markdown","238664a5":"markdown","e2a4f68d":"markdown","a8ca41ae":"markdown","ad59f102":"markdown"},"source":{"f6ed1c94":"import numpy as np\nimport pandas as pd\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nimport lightgbm as lgbm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\nimport hyperopt\nfrom sklearn.preprocessing import LabelEncoder","a926afcd":"!ls ..\/input","1bec1a70":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_train.head()","598715a7":"df_train.shape","6689bc00":"df_test.shape","73ab6bdd":"df_train[\"train\"] = 1\ndf_test[\"train\"] = 0\ndf_all = pd.concat([df_train, df_test], sort=False)\ndf_all.head()","27d80095":"df_all.tail()","3c6a72e1":"y = df_train[\"Survived\"]\ny.head()","d8f71988":"def parse_cabin_type(x):\n    if pd.isnull(x):\n        return None\n    #print(\"X:\"+x[0])\n    #cabin id consists of letter+numbers. letter is the type\/deck, numbers are cabin number on deck\n    return x[0]","0510d60e":"def parse_cabin_number(x):\n    if pd.isnull(x):\n        return -1\n#        return np.nan\n    cabs = x.split()\n    cab = cabs[0]\n    num = cab[1:]\n    if len(num) < 2:\n        return -1\n        #return np.nan\n    return num","06cbfd85":"def parse_cabin_count(x):\n    if pd.isnull(x):\n        return np.nan\n    #a typical passenger has a single cabin but some had multiple. in that case they are space separated\n    cabs = x.split()\n    return len(cabs)","c108d156":"df_train.dtypes","21d6182b":"cabin_types = df_all[\"Cabin\"].apply(lambda x: parse_cabin_type(x))\ncabin_types = cabin_types.unique()\n#drop the nan value from list of cabin types\ncabin_types = np.delete(cabin_types, np.where(cabin_types == None))\ncabin_types","5ee54627":"df_all[\"cabin_type\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_type(x))\ndf_all[\"cabin_num\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_number(x))\ndf_all[\"cabin_count\"] = df_all[\"Cabin\"].apply(lambda x: parse_cabin_count(x))\ndf_all[\"cabin_num\"] = df_all[\"cabin_num\"].astype(int)\ndf_all.head()","5e87b61f":"embarked_dummies = pd.get_dummies(df_all[\"Embarked\"], prefix=\"embarked_\", dummy_na=True)\n#TODO: see if imputing embardked makes a difference\ndf_all = pd.concat([df_all, embarked_dummies], axis=1)\ndf_all.head()","ceece293":"cabin_type_dummies = pd.get_dummies(df_all[\"cabin_type\"], prefix=\"cabin_type_\", dummy_na=True)\ndf_all = pd.concat([df_all, cabin_type_dummies], axis=1)\ndf_all.head()","aeb31376":"l_enc = LabelEncoder()\ndf_all[\"sex_label\"] = l_enc.fit_transform(df_all[\"Sex\"])\ndf_all.head()","6b269edf":"df_all[\"family_size\"] = df_all[\"SibSp\"] + df_all[\"Parch\"] + 1\ndf_all.head()","1ea48ac9":"df_all.columns","c1bc1c09":"# Cleaning name and extracting Title\nfor name_string in df_all['Name']:\n    df_all['Title'] = df_all['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\ndf_all.head()","5a3b481d":"df_all[\"Title\"].unique()","873a1816":"# Replacing rare titles \nmapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n           \ndf_all.replace({'Title': mapping}, inplace=True)\n#titles = ['Miss', 'Mr', 'Mrs', 'Royal', 'Other', 'Master']","d0a26d9a":"titles = df_all[\"Title\"].unique()\ntitles","180b4041":"title_dummies = pd.get_dummies(df_all[\"Title\"], prefix=\"title\", dummy_na=True)\ndf_all = pd.concat([df_all, title_dummies], axis=1)\ndf_all.head()","4b5d65c1":"df_all['Age'].isnull().sum()","308d7d5c":"df_all[\"Age\"].value_counts().count()","19b5c644":"titles = list(titles)\n# Replacing missing age by median age for title \nfor title in titles:\n    age_to_impute = df_all.groupby('Title')['Age'].median()[titles.index(title)]\n    df_all.loc[(df_all['Age'].isnull()) & (df_all['Title'] == title), 'Age'] = age_to_impute","9e7b9515":"df_all['Age'].isnull().sum()","8472f4fd":"df_all[\"Age\"].value_counts().count()","741935c3":"df_all.groupby('Pclass').agg({'Fare': lambda x: x.isnull().sum()})","5a0e8f36":"df_all[df_all[\"Fare\"].isnull()]","30a8ed4b":"df_all.loc[152]","34789b40":"p3_median_fare = df_all[df_all[\"Pclass\"] == 3][\"Fare\"].median()\np3_median_fare","20a63a0f":"df_all[\"Fare\"].fillna(p3_median_fare, inplace=True)","4845d79e":"df_all.loc[152]","522a35eb":"#name col seems to be in format \"last name, first names\". \n#so split by comma and take first item in resulting list should give last name..\ndf_all['Last_Name'] = df_all['Name'].apply(lambda x: str.split(x, \",\")[0])","e12705b6":"#this would be the default value if no family member is found\nDEFAULT_SURVIVAL_VALUE = 0.5\ndf_all['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\nfor grp, grp_df in df_all[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n\n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df_all.loc[df_all['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin == 0.0):\n                df_all.loc[df_all['PassengerId'] == passID, 'Family_Survival'] = 0\n                \nfor _, grp_df in df_all.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df_all.loc[df_all['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin == 0.0):\n                    df_all.loc[df_all['PassengerId'] == passID, 'Family_Survival'] = 0","4347218f":"df_train = df_all[df_all[\"train\"] == 1]\ndf_test = df_all[df_all[\"train\"] == 0]","fe659bfa":"X_cols = set(df_train.columns)\n\nX_cols -= set(['PassengerId', 'Survived', 'Sex', 'Name', 'Ticket', 'Cabin', \n               'Embarked', 'cabin_type', 'Title', 'train', 'Last_Name'])\nX_cols = list(X_cols)\nX_cols\n","58e89ec8":"df_train[X_cols].head()","1e054e82":"from sklearn.model_selection import cross_val_score, StratifiedKFold\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\n\ndef stratified_test_prediction_avg_vote(clf, X_train, X_test, y, use_eval_set, n_folds, n_classes, \n                                        fit_params, verbosity):\n    folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=69)\n    #N columns, one per target label. each contains probability of that value\n    sub_preds = np.zeros((X_test.shape[0], n_classes))\n    oof_preds = np.zeros((X_train.shape[0]))\n    score = 0\n    acc_score = 0\n    acc_score_total = 0\n    misclassified_indices = []\n    misclassified_expected = []\n    misclassified_actual = []\n    for i, (train_index, test_index) in enumerate(folds.split(X_train, y)):\n        print('-' * 20, i, '-' * 20)\n\n        X_val, y_val = X_train.iloc[test_index], y[test_index]\n        if use_eval_set:\n            clf.fit(X_train.iloc[train_index], y[train_index], eval_set=([(X_val, y_val)]), verbose=verbosity, **fit_params)\n        else:\n            #random forest does not know parameter \"eval_set\" or \"verbose\"\n            clf.fit(X_train.iloc[train_index], y[train_index], **fit_params)\n        #could directly do predict() here instead of predict_proba() but then mismatch comparison would not be possible\n        oof_preds[test_index] = clf.predict_proba(X_train.iloc[test_index])[:,1].flatten()\n        #we predict on whole test set, thus split by n_splits, not n_splits - 1\n        sub_preds += clf.predict_proba(X_test) \/ folds.n_splits\n#        sub_preds += clf.predict(X_test) \/ folds.n_splits\n#        score += clf.score(X_train.iloc[test_index], y[test_index])\n        preds_this_round = oof_preds[test_index] >= 0.5\n        acc_score = accuracy_score(y[test_index], preds_this_round)\n        acc_score_total += acc_score\n        print('accuracy score ', acc_score)\n        if hasattr(clf, 'feature_importances_'):\n            importances = clf.feature_importances_\n            features = X_train.columns\n\n            feat_importances = pd.Series(importances, index=features)\n            feat_importances.nlargest(30).sort_values().plot(kind='barh', color='#86bf91', figsize=(10, 8))\n            plt.show()\n        else:\n            print(\"classifier has no feature importances: skipping feature plot\")\n\n        missed = y[test_index] != preds_this_round\n        misclassified_indices.extend(test_index[missed])\n        m1 = y[test_index][missed]\n        misclassified_expected.append(m1)\n        m2 = oof_preds[test_index][missed].astype(\"int\")\n        misclassified_actual.append(m2)\n\n    print(f\"acc_score: {acc_score}\")\n    sub_sub = sub_preds[:5]\n    print(f\"sub_preds: {sub_sub}\")\n    avg_accuracy = acc_score_total \/ folds.n_splits\n    print('Avg Accuracy', avg_accuracy)\n    result = {\n        \"avg_accuracy\": avg_accuracy,\n        \"misclassified_indices\": misclassified_indices,\n        \"misclassified_samples_expected\": misclassified_expected,\n        \"misclassified_samples_actual\": misclassified_actual,\n        \"oof_predictions\": oof_preds,\n        \"predictions\": sub_preds,\n    }\n    return result\n","7a2c4fbf":"#check if given parameter can be interpreted as a numerical value\ndef is_number(s):\n    if s is None:\n        return False\n    try:\n        float(s)\n        return True\n    except ValueError:\n        return False\n\n#convert given set of paramaters to integer values\n#this at least cuts the excess float decimals if they are there\ndef convert_int_params(names, params):\n    for int_type in names:\n        #sometimes the parameters can be choices between options or numerical values. like \"log2\" vs \"1-10\"\n        raw_val = params[int_type]\n        if is_number(raw_val):\n            params[int_type] = int(raw_val)\n    return params\n\n#convert float parameters to 3 digit precision strings\n#just for simpler diplay and all\ndef convert_float_params(names, params):\n    for float_type in names:\n        raw_val = params[float_type]\n        if is_number(raw_val):\n            params[float_type] = '{:.3f}'.format(raw_val)\n    return params\n","4ee509e6":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgbm\nfrom hyperopt import hp, tpe, Trials\nfrom hyperopt.fmin import fmin\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nimport hyperopt\n\n# how many CV folds to do on the data\nn_folds = 5\n# max number of rows to use for X and y. to reduce time and compare options faster\nmax_n = None\n# max number of trials hyperopt runs\nn_trials = 500\n#verbosity in LGBM is how often progress is printed. with 100=print progress every 100 rounds. 0 is quite?\nverbosity = 0\n#if true, print summary accuracy\/loss after each round\nprint_summary = False\n\nfrom sklearn.metrics import accuracy_score, log_loss\n\nall_accuracies = []\nall_losses = []\nall_params = []\n\n# run n_folds of cross validation on the data\n# averages fold results\ndef fit_cv(X, y, params, fit_params, n_classes):\n    # cut the data if max_n is set\n    if max_n is not None:\n        X = X[:max_n]\n        y = y[:max_n]\n\n    score = 0\n    acc_score = 0\n    folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=69)\n\n    if print_summary:\n        print(f\"Running {n_folds} folds...\")\n    oof_preds = np.zeros((X.shape[0], n_classes))\n    for i, (train_index, test_index) in enumerate(folds.split(X, y)):\n        if verbosity > 0:\n            print('-' * 20, f\"RUNNING FOLD: {i}\/{n_folds}\", '-' * 20)\n\n        clf = lgbm.LGBMClassifier(**params)\n        X_train, y_train = X.iloc[train_index], y[train_index]\n        X_test, y_test = X.iloc[test_index], y[test_index]\n        # verbose = print loss at every \"verbose\" rounds.\n        #if 100 it prints progress 100,200,300,... iterations\n        clf.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=verbosity, **fit_params)\n        oof_preds[test_index] = clf.predict_proba(X.iloc[test_index])\n        #score += clf.score(X.iloc[test_index], y[test_index])\n        acc_score += accuracy_score(y[test_index], oof_preds[test_index][:,1] >= 0.5)\n        # print('score ', clf.score(X.iloc[test_index], y[test_index]))\n        importances = clf.feature_importances_\n        features = X.columns\n    #accuracy is calculated each fold so divide by n_folds.\n    #not n_folds -1 because it is not sum by row but overall sum of accuracy of all test indices\n    total_acc_score = acc_score \/ n_folds\n    all_accuracies.append(total_acc_score)\n    logloss = log_loss(y, oof_preds)\n    all_losses.append(logloss)\n    all_params.append(params)\n    if print_summary:\n        print(f\"total acc: {total_acc_score}, logloss={logloss}\")\n    return total_acc_score, logloss\n\ndef create_fit_params(params):\n    using_dart = params['boosting_type'] == \"dart\"\n    if params[\"objective\"] == \"binary\":\n        # https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst\n        fit_params = {\"eval_metric\": [\"binary_logloss\", \"auc\"]}\n    else:\n        fit_params = {\"eval_metric\": \"multi_logloss\"}\n    if using_dart:\n        n_estimators = 2000\n    else:\n        n_estimators = 15000\n        fit_params[\"early_stopping_rounds\"] = 100\n    params[\"n_estimators\"] = n_estimators\n    return fit_params\n\n\n# this is the objective function the hyperopt aims to minimize\n# i call it objective_sklearn because the lgbm functions called use sklearn API\ndef objective_sklearn(params):\n    int_types = [\"num_leaves\", \"min_child_samples\", \"subsample_for_bin\", \"min_data_in_leaf\"]\n    params = convert_int_params(int_types, params)\n\n    # Extract the boosting type\n    params['boosting_type'] = params['boosting_type']['boosting_type']\n    #    print(\"running with params:\"+str(params))\n\n    fit_params = create_fit_params(params)\n    if params['objective'] == \"binary\":\n        n_classes = 2\n    else:\n        n_classes = params[\"num_class\"]\n\n    score, logloss = fit_cv(X, y, params, fit_params, n_classes)\n    if verbosity == 0:\n        if print_summary:\n            print(\"Score {:.3f}\".format(score))\n    else:\n        print(\"Score {:.3f} params {}\".format(score, params))\n    #using logloss here for the loss but uncommenting line below calculates it from average accuracy\n#    loss = 1 - score\n    loss = logloss\n    result = {\"loss\": loss, \"score\": score, \"params\": params, 'status': hyperopt.STATUS_OK}\n    return result\n\ndef optimize_lgbm(n_classes, max_n_search=None):\n    # https:\/\/github.com\/Microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst\n    # https:\/\/indico.cern.ch\/event\/617754\/contributions\/2590694\/attachments\/1459648\/2254154\/catboost_for_CMS.pdf\n    space = {\n        #this is just piling on most of the possible parameter values for LGBM\n        #some of them apparently don't make sense together, but works for now.. :)\n        'class_weight': hp.choice('class_weight', [None, 'balanced']),\n        'boosting_type': hp.choice('boosting_type',\n                                   [{'boosting_type': 'gbdt',\n#                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n                                     },\n                                    {'boosting_type': 'dart',\n#                                     'subsample': hp.uniform('dart_subsample', 0.5, 1)\n                                     },\n                                    {'boosting_type': 'goss'}]),\n        'num_leaves': hp.quniform('num_leaves', 30, 150, 1),\n        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n        'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n        'feature_fraction': hp.uniform('feature_fraction', 0.5, 1),\n        'bagging_fraction': hp.uniform('bagging_fraction', 0.5, 1), #alias \"subsample\"\n        'min_data_in_leaf': hp.qloguniform('min_data_in_leaf', 0, 6, 1),\n        'lambda_l1': hp.choice('lambda_l1', [0, hp.loguniform('lambda_l1_positive', -16, 2)]),\n        'lambda_l2': hp.choice('lambda_l2', [0, hp.loguniform('lambda_l2_positive', -16, 2)]),\n        'verbose': -1,\n        #the LGBM parameters docs list various aliases, and the LGBM implementation seems to complain about\n        #the following not being used due to other params, so trying to silence the complaints by setting to None\n        'subsample': None, #overridden by bagging_fraction\n        'reg_alpha': None, #overridden by lambda_l1\n        'reg_lambda': None, #overridden by lambda_l2\n        'min_sum_hessian_in_leaf': None, #overrides min_child_weight\n        'min_child_samples': None, #overridden by min_data_in_leaf\n        'colsample_bytree': None, #overridden by feature_fraction\n#        'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n        'min_child_weight': hp.loguniform('min_child_weight', -16, 5), #also aliases to min_sum_hessian\n#        'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n#        'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n#        'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n    }\n    if n_classes > 2:\n        space['objective'] = \"multiclass\"\n        space[\"num_class\"] = n_classes\n    else:\n        space['objective'] = \"binary\"\n        #space[\"num_class\"] = 1\n\n    global max_n\n    max_n = max_n_search\n    trials = Trials()\n    best = fmin(fn=objective_sklearn,\n                space=space,\n                algo=tpe.suggest,\n                max_evals=n_trials,\n                trials=trials,\n               verbose= 1)\n\n    # find the trial with lowest loss value. this is what we consider the best one\n    idx = np.argmin(trials.losses())\n    print(idx)\n\n    print(trials.trials[idx])\n\n    # these should be the training parameters to use to achieve the best score in best trial\n    params = trials.trials[idx][\"result\"][\"params\"]\n    max_n = None\n\n    print(params)\n    return params\n\n# run a search for binary classification\ndef classify_binary(X_cols, df_train, df_test, y_param):\n    global X\n    global y\n    y = y_param\n    nrows = max_n\n\n    X = df_train[X_cols]\n    X_test = df_test[X_cols]\n\n    # use 2 classes as this is a binary classification\n    # the second param is the number of rows to use for training\n    params = optimize_lgbm(2, 5000)\n    print(params)\n\n    clf = lgbm.LGBMClassifier(**params)\n\n    fit_params = create_fit_params(params)\n\n    search_results = stratified_test_prediction_avg_vote(clf, X, X_test, y, use_eval_set=True,\n                                                         n_folds=n_folds, n_classes=2, fit_params=fit_params, verbosity=verbosity)\n    predictions = search_results[\"predictions\"]\n    oof_predictions = search_results[\"oof_predictions\"]\n    avg_accuracy = search_results[\"avg_accuracy\"]\n    misclassified_indices = search_results[\"misclassified_indices\"]\n    misclassified_samples_expected = search_results[\"misclassified_samples_expected\"]\n    misclassified_samples_actual = search_results[\"misclassified_samples_actual\"]\n\n    return predictions, oof_predictions, avg_accuracy, misclassified_indices\n","8e9536db":"predictions, oof_predictions, _, misclassified_indices = classify_binary(X_cols, df_train, df_test, y)\n","5f8f1573":"df_losses = pd.DataFrame()\ndf_losses[\"loss\"] = all_losses\ndf_losses[\"accuracy\"] = all_accuracies\ndf_losses.plot(figsize=(14,8))","8de66610":"df_losses.sort_values(by=\"loss\", ascending=False).head(10)","64edc257":"df_losses.sort_values(by=\"accuracy\", ascending=False).head(10)","99248538":"df_losses.sort_values(by=\"loss\", ascending=True).head(10)","1eb89eb9":"ss = pd.read_csv('..\/input\/gender_submission.csv')\n# predicting only true values, so take column 1 (0 is false column)\nnp_preds = np.array(predictions)[: ,1]\nss[\"Survived\"] = np.where(np_preds > 0.5, 1, 0)\nss.to_csv('lgbm.csv', index=False)\nss.head(10)","8325bea7":"len(misclassified_indices)","c9cddadb":"misclassified_indices[:10]","f0c75802":"oof_predictions[misclassified_indices][:10]","0b8c7f86":"oof_series = pd.Series(oof_predictions[misclassified_indices])\noof_series.index = y[misclassified_indices].index\n#oof_series","91f328e6":"miss_scale_raw = y[misclassified_indices] - oof_predictions[misclassified_indices]\nmiss_scale_abs = abs(miss_scale_raw)\ndf_miss_scale = pd.concat([miss_scale_raw, miss_scale_abs, oof_series, y[misclassified_indices]], axis=1)\ndf_miss_scale.columns = [\"Raw_Diff\", \"Abs_Diff\", \"Prediction\", \"Actual\"]\ndf_miss_scale.head()","374a9c41":"df_top_misses = df_miss_scale.sort_values(by=\"Abs_Diff\", ascending=False)\ndf_top_misses.head()","423c48b1":"top10 = df_top_misses.iloc[0:10].index\ntop10","4c3a83c5":"df_train.iloc[top10]","8dd509b4":"df_bottom_misses = df_miss_scale.sort_values(by=\"Abs_Diff\", ascending=True)\ndf_bottom_misses.head()","b4d7168b":"bottom10 = df_bottom_misses.iloc[0:10].index\nbottom10","a38b23db":"df_train.iloc[bottom10]","8e95fad0":"What about the ones that were misclassified with the smallest margin?","ea81fdf1":"Select the features to use for prediction. Drop the ones not useful (used for processing earlier):","22d001fe":"*Family survival* is another feature I got from the [kernel](https:\/\/www.kaggle.com\/vincentlugat\/200-lines-randomized-search-lgbm-82-3) I linked above. Another good example of a very clever way to look at the data. Group people into families (or any other group travelling together) by matching their last name and fare. Create a feature to indicate if others in this same group survived or not. This turns out to be a very nice predictor feature, as this same model with similar parameters would always score around max 77% accuracy at one point. Adding this upped the accuracy to over 81%. I suppose they are more likely to move and act together.\n\nIn a way, this feels a little like cheating. If I wanted to figure what features on the ship contributed to survival, for cases where the ship has not sunk yet, this kind of information would not be available. Or you would already know the result, since you know who survived or not. But this is Kaggle, and insights such as these are often what really makes a solution rise in the results. And it shows how to think about the data and relations within it in very clever ways. In other cases it could certainly be much more realistic. Cool!\n\nI guess in other cases, other information could also be useful, such as shared ticket number, or [subsequent ticket numbers](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/11127), cabins, etc. Anyway..","ca407886":"Will be predicting survival. So take that as target:","df52e0cd":"What do these look like?","f0f2d92b":"Now would be a good time to stop, but we can actually have some more fun by looking at the worst misclassifications. That would be the ones where the model makes the biggest mistakes in predicting high confidence of survivor when not surviving and the other way around.\n\nFirst a look at the first few misclassifications:","089ee640":"The above should show how not always having higher accuracy means having lower loss.\n\nAnd the final goal of checking the ordered list of smallest losses (so best iterations according to this metric..):","faef7412":"To create the features for cabin type, cabin number, and number of cabins:","6a89396c":"Index 152 repeats twice, from the train set and from the test set. This is visible in *survival*=0 vs *survival*=NaN. And in *train* = 1 vs *train* = 0. Of course, looking back now, just the *survival*=NaN would have been enough to differentiate the two without needing a custom *train* feature.\n\nAnyway, there is only one missing fare so just impute with median of their passenger class:","4ebb7a0b":"A look at the actual records that were most misclassified:","63b0ad03":"This should show the ones that were misclassified with highest (false) confidense:","df0f253d":"I would expect to start with higher loss and lower accuracy, going towards overall higher average accuracy and lower loss as hyperopt would focus the search on parameters. Well, actually looking at the pic above it does not seem to make such as huge difference. Maybe the problem\/data is not complex enough?","d881bd0b":"Title parsing and family survival borrowed from this [kernel](https:\/\/www.kaggle.com\/vincentlugat\/200-lines-randomized-search-lgbm-82-3). Don't know if that is the original, but thanks all Kagglers :)","ba0fe9cb":"Anyway, to see differences in parameters vs results, print the iterations with highest loss (so \"worst\"..):","89047245":"Following are some functions from my [Github](https:\/\/github.com\/mukatee\/ml-experiments\/tree\/master\/utils), where I have collected some of the code I try to make more generally useful for the times when I need to run this type of models. \n\nThe following function does an N-way stratified split of the training data, uses these splits to build a cross-validation set, and performs cross-validation N times with different set combinations.\n\nExample:\nSplit training data to 5 parts. Take 4 parts to train on, 1 part to validate. Change these around so each of the 5 parts is used as validation set once. This gives 5 different models, each trained on 80% of the data. Finally, use each of these 5 models to predict the real test-set. Average the results of all 5 predictions. Other applications would include majority voting etc. Maybe later.","33019674":"# LGBM classifier using HyperOpt tuning\n\nThis is classifier using the LGBM Python sklearn API to predict passenger survival probability. The LGBM hyperparameters are optimized using Hyperopt. The resulting accuracy is around 80%, which seems to be where most models for this dataset are at the best without cheating. Used features are mainly collected from other popular kernels, with a bit of personal tuning.","1a477a53":"As the shape shows, the dataset is small. But it is nice to practice and experiment on.","edf0e37f":"Mr Storey above is from the test set, so *survived* is NaN.","9fcf593b":"I found the following to be an interesting way to impute age: group by title, use median age per title. Quite often getting good (accuracy) results is not about fitting the most complex model and ensemble but figuring out the data. Like this trick. Again, this is from the kernel I linked above, not my own magic:","7e1d8ae1":"All the lowest loss ones have good accuracy as well. Even if a lower loss is not the highest accuracy in all cases.\n\nSo just to make a submission to try out the leaderboard:","5e3a6509":"One-hot encode categorical variables (embarked, cabin type, gender):","e465f0f1":"Combine train and test sets to create new features for both sets at once. Mark them with 1 and 0 to split them back later.","3fbef945":"So that should show some with high loss but also relatively high accuracy. Meaning accuracy on some of those looks good but loss shows them to be more generally not that good actually.\n\nAnd the ones with highest accuracy:","a008eaaf":"Run hyperopt for *n_trials* iterations to try different hyperparameter combinations. Each iteration consists of *n_folds* cross-validation splits. So *n_folds*=5 results in splitting to 5 folds, using each at a turn as the validation set and the ohter 4 for training:","4c0b192c":"Plotting the accuracy vs loss. Generally when loss goes up, accuracy should go down. The plot changes on runs due to not fixing random seed. But almost always there are spots in the plot where loss goes much higher while accuracy stays up. This would be the case where overall accuracy is good but the error in misclassifications is very high. So expect maybe not to generalize so great on unseen instances, rather like overfitting on this data.","77173666":"The list above might change a bit over different runs, so not going to list specific entries there. But as I look at it now, I see a few small children there (age 2-4 or so), some of which had a family member survice, and so on. I guess many of these are actual, reasonable misclassifications, where not every small baby or man in their best of years and health could survive due to many circumstances (e.g., location\/state during accident, personal actions, etc.).\n\nThis could use a bit more exploration to see what percentages of men, babies, etc. survived and if there is some explanatory factor to play still.","56149f98":"Just some helper functions:","ac251127":"The above columns in the data are quite self-explanatory with two exceptions:\n\nsibsp:\tnumber of siblings \/ spouses aboard the Titanic\t\nparch:\tnumber of parents \/ children aboard the Titanic\n","82fde0b3":"Functions to create features from cabin type, cabin number, and number of cabins a passenger has reserved:","44544292":"Find missing fare values (there is just one..):","cd3d8157":"Create new feature from combined family size:","eff3f5ee":"Since the threshold is 0.5, these are very close to the border of that.","238664a5":"Now that all the features are created, lets split the data back to train and test sets:","e2a4f68d":"Raw diff for the negative\/positive difference in prediction vs actual label. Absolute diff to order both types of errors in one set. And the prediction just to illustrate where the diff comes from:","a8ca41ae":"The above shows the parameters found for the best iteration (lowest loss) by hyperopt, as well as a plot of the most important features for each of the 5 folds as given by the LGBM classifier.\n\nOverall, it seems to consistently rank fare, age, family survival, passenger class, and title of Mr as highest features. I guess higher passenger class meant paying a higher fare, and being located at a higher deck. Or perhaps they were given priority to escape? I am sure more internet searches would help but going with this for now..","ad59f102":"To see what types of cabins there are:"}}