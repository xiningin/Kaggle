{"cell_type":{"3eafa013":"code","8122f5a7":"code","0ce294bc":"code","65df29a0":"code","a7096bba":"code","c5ac2d01":"code","65d390a9":"code","f5990b66":"code","ef225eec":"code","aebfeacf":"code","da5a460b":"code","c47ef124":"code","b92f0e78":"code","e9c0e469":"code","addbea1e":"markdown","18ea0b53":"markdown","28ce3128":"markdown","d0c67d75":"markdown","f88b8e13":"markdown","674192bf":"markdown","5e454bff":"markdown","93d9373d":"markdown","21349004":"markdown","4c976497":"markdown","b5fe3b0f":"markdown","2e8f2ded":"markdown","646ff646":"markdown","c8a0273f":"markdown","c13b9953":"markdown","14c17549":"markdown","029ab0cb":"markdown","3f800fc0":"markdown","9ed073ee":"markdown","7efef8e2":"markdown"},"source":{"3eafa013":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport datetime\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport scipy","8122f5a7":"\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","0ce294bc":"sns.pairplot(train_df.iloc[:,197:])","65df29a0":"sns.pairplot(test_df.iloc[:,198:])","a7096bba":"sns.pairplot(train_df.iloc[:,:5])","c5ac2d01":"pd.isna(train_df).sum().sum()","65d390a9":"sns.pairplot(pd.isna(train_df.iloc[:,198:]))","f5990b66":"# Before excluding certain values in handle_outliers function underneath, we are going to compare two methods and different paramaeters\n# All to see which number of outliers seems reasonable, than we are going to exclude entire row that has this outlier\n#It will be only a few since we will opt for the most extreme case, where deviation from the mean is really ridiculous.\n\ndef out_std(s, nstd=3.0, return_thresholds=False):\n\n    data_mean, data_std = s.mean(), s.std()\n    cut_off = data_std * nstd\n    lower, upper = data_mean - cut_off, data_mean + cut_off\n    if return_thresholds:\n        return lower, upper\n    else:\n        return [False if x < lower or x > upper else True for x in s]\n    \n\n    \n    \nstd2 = train_df.iloc[:,198:].apply(out_std, nstd=2.0)\nstd3 = train_df.iloc[:,198:].apply(out_std, nstd=3.0)\nstd4 = train_df.iloc[:,198:].apply(out_std, nstd=4.0)\n\n    \n    \nf, ((ax1, ax2, ax3)) = plt.subplots(ncols=3, nrows=1, figsize=(22, 12));\nax1.set_title('Outliers with 2 standard deviations');\nax2.set_title('Outliers using 3 standard deviations');\nax3.set_title('Outliers using 4 standard deviations');\n\nsns.heatmap(std2, cmap='Blues', ax=ax1);\nsns.heatmap(std3, cmap='Blues', ax=ax2);\nsns.heatmap(std4, cmap='Blues', ax=ax3);\n\n\nplt.show()","ef225eec":"melted = pd.melt(train_df.iloc[:,194:])\nmelted[\"value\"] = pd.to_numeric(melted[\"value\"])\n","aebfeacf":"sns_plot1=sns.boxplot(x=\"variable\", y=\"value\", data=melted)\nsns_plot1.set_xticklabels(sns_plot1.get_xticklabels(), rotation = 90, fontsize = 10)","da5a460b":"corr = train_df.iloc[:,190:].corr()\n\n# plot the heatmap\nsns_plot2=sns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","c47ef124":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,10,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5,10,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n        plt.tick_params(axis='y', which='major', labelsize=6)\n    plt.show();","b92f0e78":"t0 = train_df.loc[train_df['target'] == 0]\nt1 = train_df.loc[train_df['target'] == 1]\nfeatures = train_df.columns.values[2:52]\nplot_feature_distribution(t0, t1, '0', '1', features)","e9c0e469":"features = train_df.columns.values[2:52]\nplot_feature_distribution(train_df, test_df, 'train', 'test', features)","addbea1e":"**Correlation map**- after throwing the outliers and missing values away (since it is neccessary before calculating pearson correlation coefficient)","18ea0b53":"So what are some other insights that can be gathered using EDA about the data? One interesting thing is the distribution (density) plot of different predicators when in contrast to different classes (0 or 1).","28ce3128":"**Another** thing that should be important to us (to ensure could prediction power) is that **test and train sets are the same**, i.e. they come from the same sample and they represent the whole population. Lets plot it for first 50 variables.","d0c67d75":"Dataset will be [Santander Customer Transaction Prediction](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data) where we have 200 columns of anonymised data. ","f88b8e13":"# Take-off notes from the first analysis:\n\n**Ofcourse this is individual but we can see unbalanced classes in the target variable, not much correlation (we will check it more subsequently!). Mostly normaly distribution among predictors, tough to distinguish which values are to be associated with 0 & 1 class etc...**\n\n\nAs already mentioned one ought to \"zoom in\" and take one special predictor and to subsequent analysis. Doing a 3-d plot with some special interest variables etc. So it really depends on the problem and the domain knowledge of the problem.","674192bf":"**Additionally** we can speaak about skewness distriibution (here it is normal), additional exploration with some specific variables\/domain specific knowledge, contrasting different scatter plots with some categorical variables (here we do not have classes other than the dependent variable), in case of text some word clouds, tf-idf distribution etc etc....\nThere are many options but I think these steps are essential no matter what the dataset at hand is. \nAdditional **(part 2) tutorial** can be made concerning purely textual data and good EDA there.","5e454bff":"Since distribution of independent variables is mostly normal, lets see what happens with outliers when measured with (different) points of standard deviation. (one can see it as z-score)","93d9373d":"As we noticed from the first plots (scattered ones) there is not really much correlation between the variables.","21349004":"![](http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1513868561\/output_65_0_knd6e9.png)","4c976497":"# What should good EDA be capable of?\n1. Verify expected relationships actually exist in the data, thus formulating and validating planned techniques of analysis.\n2. To find some unexpected structure in the data that must be taken into account, thereby suggesting some changes in the planned analysis.\n3. Deliver data-driven insights to business stakeholders by confirming they are asking the right questions and not biasing the investigation with their assumptions.\n4. Provide the context around the problem to make sure the potential value of the data scientist\u2019s output can be maximized.","b5fe3b0f":"When **dealing with outliers** one should be careful and look also at the distribution of the variable at hand. For example let us say that we have a uniform distributed variable, does 2 points of std really say anything about a potential outlier? Best thing one could do is assume (or better yet test with kolmogorov smirnov test) a distribution of a variable. Than depending on the result just throw away values that are to be found far away on the distribution graph.","2e8f2ded":"BUT, this is a boiler plat code that can be re-used later on different projects!","646ff646":"Let us also assume that no pre-processing will be done (often times) before EDA. We will use EDA to help us with that too.","c8a0273f":"Main point is gathering and automising as much as possible. So we will plot all of the variables together (modifying the code for different problems), than \"zoom in\" in case of suspicion. Since there are a lot of indicators I only took some of them to speed up the computation. We can see distributions as well as plots in relationship with other variables.","c13b9953":"We should also plot other variables in dependence to the dependent variable--**target**. Thats the first column so lets just take a couple of the first columns. (run-time!!!)","14c17549":"Same for test set.","029ab0cb":"Another way to look at the outliers but also in the same time get some more information about distribution (IQR, median, mean etc...) is with the box-plot. But we need to do it efficiently:\n","3f800fc0":"Dealing with **missing values**:\n\nThis is a bit specific dataset with no missing values:","9ed073ee":"**Interesting observation** is its not always normal distribution, in some cases and classes we can observe almost bimodal distribution. **Implication?** Normality assumption is not met, be careful in model choices etc if we were to use these predicators.","7efef8e2":"Plots on top of plot on top of plots. It seems that most of the EDA these days is just throwing around fancy plots from fancy libraries. There is no real **insight** or **reusability** from those kinds of notebooks, just to fill in the space.\n\n\n# GOAL: This notebook should serve as a reusable template for **INSIGHTFUL** EDA when approaching a DS problem. \n\nOfcourse there is not a universal solution and it always needs to be modified but I feel like that outlining a couple of general ideas and principles will be usefull since they will repeat themselves."}}