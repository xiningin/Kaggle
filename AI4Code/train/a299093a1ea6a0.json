{"cell_type":{"546500a4":"code","00bad6ec":"code","fa4351bb":"code","f3628c6a":"code","a6af6321":"code","3b1b3dc3":"code","ebf2f1ef":"code","a59daeee":"code","aca9877b":"code","327da832":"code","146c1f5e":"code","23e5f4ee":"code","2338e420":"code","b7ab1e21":"code","ae2974f1":"code","fc41e7e9":"code","4a41a4bf":"code","bf8b7339":"code","17da931d":"code","0836a043":"code","bd5aaf23":"code","fb0252a2":"code","f2b0f6a3":"code","9b65fab3":"code","b17b782d":"code","fa574d50":"code","b3a870a5":"code","5c627a65":"code","cf0fc3a3":"code","73fdaf2f":"code","f22ff35e":"code","1900f6cf":"code","94cc62be":"code","1aecb46c":"code","9f32c601":"code","7cb5449c":"code","c9303821":"code","667c227b":"markdown","047529b9":"markdown","b6ebb075":"markdown","250c02b9":"markdown","23cce276":"markdown","68335d64":"markdown","9b275c22":"markdown","05480ac0":"markdown","b7c5982f":"markdown","fc5c6c3a":"markdown","bac11feb":"markdown","80a37e72":"markdown","21661f79":"markdown","24eecefa":"markdown","0724e843":"markdown","27421609":"markdown","fb8340a4":"markdown","ddd4b423":"markdown","f47cf0fd":"markdown","14d49383":"markdown","44576168":"markdown","a0e51231":"markdown","e02ab79f":"markdown","9aa0c80b":"markdown","52a23d48":"markdown","9d378a80":"markdown","affd4d3d":"markdown","43efd87c":"markdown","2534ee7c":"markdown","b599d311":"markdown","e3c93916":"markdown","abec1220":"markdown","b02ccd80":"markdown","05fba8b8":"markdown","2467f03a":"markdown","29e79c6d":"markdown","d20a35a8":"markdown","cabe9046":"markdown","4e6e835a":"markdown","03058f28":"markdown","76df7c6b":"markdown","6c9c94c4":"markdown","69901d61":"markdown","7f1c3658":"markdown","c7a104ba":"markdown","07250476":"markdown","75b32d02":"markdown","2fb497e2":"markdown"},"source":{"546500a4":"## Imports\nimport torch\nimport torchvision ## Contains some utilities for working with the image data\nfrom torchvision.datasets import MNIST\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport torchvision.transforms as transforms\nfrom torch.utils.data import random_split\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F","00bad6ec":"dataset = MNIST(root = 'data\/', download = True)\nprint(len(dataset))","fa4351bb":"image, label = dataset[10]\nplt.imshow(image, cmap = 'gray')\nprint('Label:', label)","f3628c6a":"## MNIST dataset(images and labels)\nmnist_dataset = MNIST(root = 'data\/', train = True, transform = transforms.ToTensor())\nprint(mnist_dataset)","a6af6321":"image_tensor, label = mnist_dataset[0]\nprint(image_tensor.shape, label)","3b1b3dc3":"print(image_tensor[:,10:15,10:15])\nprint(torch.max(image_tensor), torch.min(image_tensor))","ebf2f1ef":"## Plot the image of the tensor\nplt.imshow(image_tensor[0,10:15,10:15],cmap = 'gray')","a59daeee":"train_data, validation_data = random_split(mnist_dataset, [50000, 10000])\n## Print the length of train and validation datasets\nprint(\"length of Train Datasets: \", len(train_data))\nprint(\"length of Validation Datasets: \", len(validation_data))","aca9877b":"batch_size = 128\ntrain_loader = DataLoader(train_data, batch_size, shuffle = True)\nval_loader = DataLoader(validation_data, batch_size, shuffle = False)","327da832":"import torch.nn as nn\n\ninput_size = 28 * 28\nnum_classes = 10\n\n## Logistic regression model\nmodel = nn.Linear(input_size, num_classes)\nprint(model.weight.shape)\nprint(model.weight)\nprint(model.bias.shape)\nprint(model.bias)","146c1f5e":"for images, labels in train_loader:\n    print(labels)\n    print(images.shape)\n    outputs = model(images)\n    break","23e5f4ee":"class MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n        \n    def forward(self, xb):\n        xb = xb.reshape(-1, 784)\n        print(xb)\n        out = self.linear(xb)\n        print(out)\n        return(out)\n\nmodel = MnistModel()\nprint(model.linear.weight.shape, model.linear.bias.shape)\nlist(model.parameters())","2338e420":"for images, labels in train_loader:\n    outputs = model(images)\n    break\n    \nprint('outputs shape: ', outputs.shape)\nprint('Sample outputs: \\n', outputs[:2].data)","b7ab1e21":"## Apply softmax for each output row\nprobs = F.softmax(outputs, dim = 1)\n\n## chaecking at sample probabilities\nprint(\"Sample probabilities:\\n\", probs[:2].data)\n\nprint(\"\\n\")\n## Add up the probabilities of an output row\nprint(\"Sum: \", torch.sum(probs[0]).item())\nmax_probs, preds = torch.max(probs, dim = 1)\nprint(\"\\n\")\nprint(preds)\nprint(\"\\n\")\nprint(max_probs)","ae2974f1":"labels","fc41e7e9":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim = 1)\n    return(torch.tensor(torch.sum(preds == labels).item()\/ len(preds)))\n\nprint(\"Accuracy: \",accuracy(outputs, labels))\nprint(\"\\n\")\nloss_fn = F.cross_entropy\nprint(\"Loss Function: \",loss_fn)\nprint(\"\\n\")\n## Loss for the current batch\nloss = loss_fn(outputs, labels)\nprint(loss)","4a41a4bf":"class MnistModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n    \n    def forward(self, xb):\n        xb = xb.reshape(-1, 784)\n        out = self.linear(xb)\n        return(out)\n    \n    def training_step(self, batch):\n        images, labels = batch\n        out = self(images) ## Generate predictions\n        loss = F.cross_entropy(out, labels) ## Calculate the loss\n        return(loss)\n    \n    def validation_step(self, batch):\n        images, labels = batch\n        out = self(images)\n        loss = F.cross_entropy(out, labels)\n        acc = accuracy(out, labels)\n        return({'val_loss':loss, 'val_acc': acc})\n    \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()\n        return({'val_loss': epoch_loss.item(), 'val_acc' : epoch_acc.item()})\n    \n    def epoch_end(self, epoch,result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n        \n    \nmodel = MnistModel()","bf8b7339":"def evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return(model.validation_epoch_end(outputs))\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func = torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        \n        ## Training Phas\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        \n        ## Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return(history)","17da931d":"result0 = evaluate(model, val_loader)\nresult0","0836a043":"history1 = fit(5, 0.001, model, train_loader, val_loader)","bd5aaf23":"history2 = fit(5, 0.001, model, train_loader, val_loader)","fb0252a2":"history3 = fit(5, 0.001, model, train_loader, val_loader)","f2b0f6a3":"history4 = fit(5, 0.001, model, train_loader, val_loader)","9b65fab3":"## Replace these values with your result\nhistory = [result0] + history1 + history2 + history3 + history4\naccuracies = [result['val_acc'] for result in history]\nplt.plot(accuracies, '-x')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('Accuracy Vs. No. of epochs')","b17b782d":"## Define the test dataset\ntest_dataset = MNIST(root = 'data\/', train = False, transform = transforms.ToTensor())","fa574d50":"img, label = test_dataset[0]\nplt.imshow(img[0], cmap = 'gray')\nprint(\"shape: \", img.shape)\nprint('Label: ', label)","b3a870a5":"print(img.unsqueeze(0).shape)\nprint(img.shape)","5c627a65":"def predict_image(img, model):\n    xb = img.unsqueeze(0)\n    yb = model(xb)\n    _, preds = torch.max(yb, dim = 1)\n    return(preds[0].item())","cf0fc3a3":"img, label = test_dataset[0]\nplt.imshow(img[0], cmap = 'gray')\nprint('Label:', label, ', Predicted :', predict_image(img, model))","73fdaf2f":"img, label = test_dataset[9]\nplt.imshow(img[0], cmap = 'gray')\nprint(\"Label:\", label, ',Predicted:', predict_image(img, model))","f22ff35e":"img, label = test_dataset[25]\nplt.imshow(img[0], cmap = 'gray')\nprint(\"Label:\", label, ',Predicted:', predict_image(img, model))","1900f6cf":"img, label = test_dataset[5000]\nplt.imshow(img[0], cmap = 'gray')\nprint(\"Label:\", label, ',Predicted:', predict_image(img, model))","94cc62be":"test_loader = DataLoader(test_dataset, batch_size = 256)\nresult = evaluate(model, test_loader)\nresult","1aecb46c":"torch.save(model.state_dict(), 'mnist-logistic.pth')","9f32c601":"model.state_dict()","7cb5449c":"model2 = MnistModel()\nmodel2.load_state_dict(torch.load('mnist-logistic.pth'))\nmodel2.state_dict()","c9303821":"test_loader = DataLoader(test_dataset, batch_size = 256)\nresult = evaluate(model2, test_loader)\nresult","667c227b":"## Training the Model","047529b9":"The values range from 0 to 1, with 0 representing black, 1 white and the values between different shades of grey. We can also plot the tensor as an image using lt.imshow","b6ebb075":"**Note** This leads to an error, because our input data does not have the right shape.\nOur images are of the shape 1X28X28, but we need them to be vectors of size 784 i.e we need to flatten them out. We will use the <b>.reshape()<\/b> method of a tensor, which will allow us to efficiently view each image as a flat vector, without really changing the underlying data.\n\nTo include this additional functionality within model, we need to define a custom model, by extending the <b>nn.Module<\/b> class from PyTorch.","250c02b9":"While building a machine learning\/Deep learning models, it is common to split the dataset into 3 parts:\n\n1. <b>Training set<\/b> - The part of the data will be used to train the model,compute the loss and adjust the weights of the model using gradient descent.\n\n\n2. <b>Validation set<\/b> - This part of the dataset will be used to evalute the traing model, adjusting the hyperparameters and pick the best version of the model.\n\n\n3. <b>Test set<\/b> - This part of the dataset is used to final check the model predictions on the new unseen data to evaluate how well the model is performing.","23cce276":"![image.png](attachment:image.png)","68335d64":"## Credits","9b275c22":"PyTorch is an optimized tensor library for deep learning using GPUs and CPUs.","05480ac0":"#### The softmax function is a function that turns a vector of K real values into a vector of K real values that sum to 1. The input values can be positive, negative, zero, or greater than one, but the softmax transforms them into values between 0 and 1, so that they can be interpreted as probabilities. If one of the inputs is small or negative, the softmax turns it into a small probability, and if an input is large, then it turns it into a large probability, but it will always remain between 0 and 1.\n\n#### The softmax function is sometimes called the softargmax function, or multi-class logistic regression. This is because the softmax is a generalization of logistic regression that can be used for multi-class classification, and its formula is very similar to the sigmoid function which is used for logistic regression. The softmax function can be used in a classifier only when the classes are mutually exclusive.\n\n#### Many multi-layer neural networks end in a penultimate layer which outputs real-valued scores that are not conveniently scaled and which may be difficult to work with. Here the softmax is very useful because it converts the scores to a normalized probability distribution, which can be displayed to a user or used as input to other systems. For this reason it is usual to append a softmax function as the final layer of the neural network.","b7c5982f":"## Saving and loading the Model","fc5c6c3a":"![image.png](attachment:image.png)","bac11feb":"We can interpret the above \"True\" distribution to mean that the training instance has 0% probability of being Class A, 100% probability of Class B and 0% probability of being Class C.\n\nNow suppose the machine learning algorithm predicts the following probability distribution:","80a37e72":"![image.png](attachment:image.png)","21661f79":"<b>Cross-entropy<\/b> is commonly used to quantify the difference between two probabilities distribution. Usually the \"True\" distribution(the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution.\n\nFor example, suppose for a specific training instance,the label is B(out of possible labels A,B and C).The one-hot distribution for this training instance is therefore.","24eecefa":"![image.png](attachment:image.png)","0724e843":"## What is Softmax function?","27421609":"# Identifying hand-written digits(MNIST) using PyTorch","fb8340a4":"Here we evaluate our model by finding the percentage of labels that were predicted correctly i.e. the <b>accuracy<\/b> of the predictions.\n\nThe <b>==<\/b> performas an element-wise comparision of two tensors with the same shape, and returns a tensor of the same shape,containing <b>0s<\/b> for unequal elements, and <b>1s<\/b> for equal elements. Passing the result to <b>torch.sum<\/b> returns the number of labels that were predicted correctly. Finally we divide by the total total number of images to get the accuracy.","ddd4b423":"![image.png](attachment:image.png)","f47cf0fd":"Here <b>p(x) = output probability<\/b>, and <b>q(x) = Actual probability<\/b>.\n\nThe sum is over the three classes A,B and C.In this case the loss is<b>0.479.<\/b>","14d49383":"For each of the 100 input images, we get 10 outputs, one for each class. These outputs represent probabilities, but for the that the output row should lie between 0 to 1 and add upto 1.\n\nFor converting the output to probabilities such that it lies between 0 to 1 we use <b>Softmax function<\/b>.","44576168":"These images are small in size, and recognizing the digits can sometimes be hard. <b>PyTorch<\/b> doesn't know how to work with images. We need to convert the images into <b>tensors<\/b>. We can do this by specifying a <b>transform<\/b> while creating our dataset.\n\nPyTorch datasets allow us to specify one or more transformation function which are applied to the images as they are loaded.\n\n<b>torchvision.transforms<\/b> contains many such predefined functions and we will use <b>ToTensor<\/b> transform to convert images into Pytorch tensors.","a0e51231":"### Defining the Logistic Model","e02ab79f":"![image.png](attachment:image.png)","9aa0c80b":"### Loading the MNIST data with transformation applied while loading","52a23d48":"## Testing with individual images","9d378a80":"We will import <b>torchvision<\/b> which contains some utility functions for working with the image data. It also contain helper classes to automatically download and import the famous datasets like MNIST.\n\nMNIST dataset has 60,000 images which can be used to train the model. There is also an additional test set of 10,000 images which can be created by passing <b>train = False<\/b> to the MNIST class.","affd4d3d":"<b>Logistic Regression<\/b> model is identical to a linear regression model i.e, there are weights and bias matrices, and the output is obtained using simple matrix operations(pred = x@ w.t() + b).\n\nWe can use <b>nn.Linear<\/b> to create the model instead of defining and initializing the matrices manually.\n\nSince <b>nn.Linear<\/b> expects the each training example to a vector, each <b>1 X 28 X 28<\/b> image tensor needs to be flattened out into a vector of size <b>784(28 X 28)<\/b>, before being passed into the model.\n\nThe output for each image is vector of size 10, with each element of the vector signifying the probability a particular target <b>label(i.e 0 to 9)<\/b>. The predicted label for an image is simply the one with the highest probability.","43efd87c":"The initial accuracy is around 8% which is one might expect from a randomly initialized model.\n\nWe are ready to train the model. Let's train for 5 epochs","2534ee7c":"![image.png](attachment:image.png)","b599d311":"Inside the __init__  constructor method, we instantiate the weights and biases using <b>nn.Linear<\/b>. Inside the <b>forward method<\/b>, which is invoked when we pass a batch of inputs to the model, we flatten out the input tensor, and then pass it into <b>self.linear<\/b>.\n\n<b>xb.reshape(-1, 28 * 28)<\/b> indicates to PyTorch that we want a view of the <b>xb<\/b> tensor with two dimensions, where the length along the 2nd dimension is <b>28 * 28(i.e 784)<\/b>. One argument to <b>.reshape<\/b> can be set to <b>-1(in this case the first dimension)<\/b>, to let PyTorch figure it out automatically based on the shape of the original tensor.\n\nNote that the model no longer has <b>.weight and .bias <\/b>attributes(as they are now inside the <b>.linear attribute)<\/b>,but it does have a <b>.parameters<\/b> method which returns a list containg the <b>weights and bias<\/b>, and can be used by a <b>PyTorch optimizer<\/b>.","e3c93916":"The image is now convert to a <b>28 X 28 tensor<\/b>.The first dimension is used to keep track of the color channels. Since images in the <b>MNIST dataset are grayscale<\/b>, there's just one channel. Other datasets have images with color, in that case the color channels would be <b>3(Red, Green, Blue).<\/b>","abec1220":"![image.png](attachment:image.png)","b02ccd80":"## Evaluation Metric and Loss Function","05fba8b8":"<font color='black'><h2 align = 'center' style = 'background:LightGray'> Quick Navigation <\/h2><\/font>\n#### [1. Brief about PyTorch](#1)\n#### [2. Working with images in PyToch(using MNIST Dataset)](#2)\n#### [3. Splitting a dataset into training, Validation and test sets](#3)\n#### [4. Creating PyTorch models with custom logic by extending the nn.Module Class](#4)\n#### [5. Interpreting model outputs as probabilities using softmax, and picking predicted labels](#5)\n#### [6. Picking a good evaluation metric(accuracy) and loss function(cross entropy) for Classification problems](#6)\n#### [7.Setting up a training loop that also evaluates the model using Validation set](#7)\n#### [8. Testing the model manually on randomly picked examples](#8)\n#### [9.Saving and loading the model checkpoints to avoid retraining from scratch](#9)\n#### [10. References](#10)","2467f03a":"### Loading the MNIST dataset","29e79c6d":"<h1><font color='red'> If you learn anything new from this notebook, Please upvote....<\/font><\/h1>","d20a35a8":"#### [1. https:\/\/jovian.ai\/aakashns\/03-logistic-regression](#1)\n#### [2. https:\/\/deepai.org\/machine-learning-glossary-and-terms\/softmax-layer](#2)\n#### [3. https:\/\/stackoverflow.com\/questions\/41990250\/what-is-cross-entropy](#3)\n#### [4. https:\/\/en.wikipedia.org\/wiki\/MNIST_database](#4)\n#### [5. https:\/\/github.com\/pytorch\/pytorch](#5)","cabe9046":"## How it is Calculated ?","4e6e835a":"![image.png](attachment:image.png)","03058f28":"Here is the pseudo-code which we will use to trainthe model","76df7c6b":"Here we will use <b>DataLoaders<\/b> to help us load the data in batches. We will use a batch size of 128. We will set <b>shuffle = True<\/b> for the training dataloader, so that the batches generated in each epoch are different, and this randomization helps in generalizing and speed up the process.\n\nSince Validation dataloader is used only for evaluating the model, there is no need to shuffle the images.","6c9c94c4":"Now to check how this probability distribution is close to True distribution.Here we will use the <b>Cross-entropy loss function.<\/b>","69901d61":"## Model","7f1c3658":"## Training and Validation Datasets","c7a104ba":"The <b>.state_dict<\/b> method returns an OrderedDict containing all the weights and bias matrices mapped to the right attributes of the model.","07250476":"We will use the famous <b>MNIST Handwritten Digits Databases<\/b> as our training dataset.It consists of 28px by 28px grayscale images of handwritten disgits(0 - 9), along with labels for each image indicating which digit it represents. MNIST stands for <b>Modified National Institute of Standards and Technology.<\/b>","75b32d02":"## What is Cross-Entropy","2fb497e2":"While accuracy is a great way to evluate the model, it can't be used as a loss function for optimizing our model using gradient descent in this case for the following reasons:\n\n- > It does not take into account the actual probabilities predicted by the model,so it can't provide sufficient feedback for increemental improvements.\n\nDue to this reason accuracy is a great evaluation metric for classification metric ,but not a good loss function.A commonly used loss function for classification problems is the <b>Cross Entropy<\/b>."}}