{"cell_type":{"3e23fdce":"code","d5d745da":"code","ca8d5c67":"code","25024963":"code","f37c781b":"code","0787263f":"code","cc6184b3":"code","789df134":"code","cd25fb17":"code","31c2739d":"code","c00e77e5":"code","4d287ec7":"code","2f4c86c4":"code","517575db":"code","f61fad64":"code","45dddbb9":"code","02f54b76":"code","fc904dd0":"code","0fd83cd2":"code","2ef9a4bf":"code","40e75a63":"markdown"},"source":{"3e23fdce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5d745da":"import torch # \uc778\uacf5\uc9c0\ub2a5 \ud559\uc2b5\uc744 \uc704\ud55c pytorch \ubd88\ub7ec\uc624\uae30\nimport torch.nn as nn # torch\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 nn \ubaa8\ub4c8 \ubd88\ub7ec\uc624\uae30\nimport torch.optim as optim # \ud559\uc2b5\uc2dc \uc0ac\uc6a9\ud560 optimizer\uac00 \ub2f4\uae34 torch.optim \ubd88\ub7ec\uc624\uae30\n\ndevice = torch.device(\"cuda\") #gpu \uc0ac\uc6a9\uc744 \uc704\ud574 cuda \ud560\ub2f9\ntorch.manual_seed(1) # seed 1\ub85c \uace0\uc815 (\uac19\uc740 input, \uac19\uc740 output\uc744 \uc704\ud574)","ca8d5c67":"train = pd.read_csv('..\/input\/2021-ai-quiz1-p5\/train.csv') # train \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30 (label \ud3ec\ud568)\ntest = pd.read_csv('..\/input\/2021-ai-quiz1-p5\/test.csv') #test \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\nsubmit = pd.read_csv('..\/input\/2021-ai-quiz1-p5\/submit_sample.csv') #\uc81c\ucd9c \ud30c\uc77c \ud615\uc2dd \ubd88\ub7ec\uc624\uae30","25024963":"train_x = train.drop(['diagnosis'],axis=1)\ntrain_y = train['diagnosis']","f37c781b":"train_x = torch.FloatTensor(np.array(train_x)).to(device) # \uc800\uc7a5\ud588\ub358 train_x  numpy \ubc30\uc5f4 -> tensor\ub85c \uc804\ud658\ud558\uace0 gpu \uc0ac\uc6a9\uc744 \uc704\ud574 \uc124\uc815\ntrain_y = torch.FloatTensor(np.array(train_y)).to(device) # \uc800\uc7a5\ud588\ub358 train_y  numpy \ubc30\uc5f4 -> tensor\ub85c \uc804\ud658\ud558\uace0 gpu \uc0ac\uc6a9\uc744 \uc704\ud574 \uc124\uc815\ntest = torch.FloatTensor(np.array(test)).to(device) # \uc800\uc7a5\ud588\ub358 test  numpy \ubc30\uc5f4 -> tensor\ub85c \uc804\ud658\ud558\uace0 gpu \uc0ac\uc6a9\uc744 \uc704\ud574 \uc124\uc815","0787263f":"print(train_x.shape) # \uac01 \ub370\uc774\ud130 shape \ud655\uc778\nprint(train_y.shape)","cc6184b3":"# layor \uc815\uc758 , sequential -> \uc21c\uc11c\ub300\ub85c layor \ud1b5\uacfc\nmodel = nn.Sequential(\n    nn.Linear(30,15,bias=True).to(device), # input 8 , output 16  \ud615\ud0dc\ub85c linear layor\n    nn.Linear(15,15,bias=True).to(device), # input 16 , output 16  \ud615\ud0dc\ub85c linear layor\n    nn.ReLU().to(device),\n    nn.Linear(15,1,bias=True).to(device), # input 16 , output 1  \ud615\ud0dc\ub85c linear layor (\uc774\uc9c4 \ubd84\ub958 \ubb38\uc81c\uc774\ubbc0\ub85c)\n)","789df134":"loss = torch.nn.BCEWithLogitsLoss() # logstic\uc774 \ud3ec\ud568\ub41c loss function\noptimizer = optim.SGD(model.parameters(),lr=0.0001) # optimizer\ub85c adam \uc0ac\uc6a9","cd25fb17":"train_y = train_y.reshape(-1,1)","31c2739d":"epochs = 1000 # epoch \uc124\uc815\n\nfor epoch in range(epochs): #\uac01 \uc5d0\ud3ec\ud06c \ub9c8\ub2e4\n    hx = model(train_x)  # \uc815\uc758\ub0b4\ub9b0 model\uc5d0 train_x \ub370\uc774\ud130 \uc785\ub825\uc73c\ub85c \ub123\uc5b4 \uac00\uc124 \uc124\uc815\n    cost = loss(hx,train_y) # \uc608\uce21\uac12\uacfc true\uac12\uc758 \uc624\ucc28 \uacc4\uc0b0 ( BCEWithLogitsLoss\uc0ac\uc6a9)\n    \n    optimizer.zero_grad() # \ud30c\ub77c\ubbf8\ud130 \ucd08\uae30\ud654\n    cost.backward() # \uc624\ucc28 \uc804\ub2ec\n    optimizer.step() # \uc5c5\ub370\uc774\ud2b8\n    \n    if epoch % 10 == 0:\n        print(\"Epoch : {}\/{}  Cost : {}\".format(epoch,epochs,cost.item())) # \uc124\uc815\ub41c \uc5d0\ud3ec\ud06c \ub9c8\ub2e4 \ud559\uc2b5 \uacb0\uacfc(COST) \ucd9c\ub825","c00e77e5":"hx = model(test) # \ud559\uc2b5\uc774 \uc644\ub8cc\ub41c model\uc5d0 test \ub370\uc774\ud130 \uc801\uc6a9\npred = (hx>=torch.FloatTensor([0.5]).to(device))\npred = pred.reshape(-1).tolist()","4d287ec7":"# true -> 1\ub85c false -> 0\uc73c\ub85c \ubcc0\ud658\nfor i in range(len(pred)):\n    if pred[i] == True:\n        pred[i] = 1\n    elif pred[i] == False:\n        pred[i] = 0","2f4c86c4":"submit['diagnosis'] = pred #\uc800\uc7a5\nsubmit.to_csv('submit.csv',index=False) #\ud30c\uc77c \uc0dd\uc131","517575db":"submit","f61fad64":"hx = model(train_x) # \ud559\uc2b5\uc774 \uc644\ub8cc\ub41c model\uc5d0 test \ub370\uc774\ud130 \uc801\uc6a9\npred = (hx>=torch.FloatTensor([0.5]).to(device))","45dddbb9":"# true -> 1\ub85c false -> 0\uc73c\ub85c \ubcc0\ud658\nfor i in range(len(pred)):\n    if pred[i] == True:\n        pred[i] = 1\n    elif pred[i] == False:\n        pred[i] = 0","02f54b76":"train_y = train_y.reshape(-1).tolist()","fc904dd0":"correct = 0\nfor i in range(len(pred)):\n    if train_y[i] == pred[i]:\n        correct = correct+1\nprint(correct)","0fd83cd2":"accuracy = correct \/ len(pred)","2ef9a4bf":"print(\"accuracy : {}\",  (accuracy*100))","40e75a63":"Learning rate : 0.0001 Optimizer : Adam Epoch : 100 Layor : Linear 3\uac1c \uce35 , LeLU 1\uac1c \uce35 \uc815\ud655\ub3c4 : 87.1859296482412\n\nLearning rate : 0.0001 Optimizer : Adam Epoch : 1000 Layor : Linear 3\uac1c \uce35 , LeLU 1\uac1c \uce35 \uc815\ud655\ub3c4 : 90.7035175879397\n\nLearning rate : 0.1 Optimizer : Adam Epoch : 1000 Layor : Linear 3\uac1c \uce35 , LeLU 1\uac1c \uce35 \uc815\ud655\ub3c4 : 63.31658291457286\n\n\uc73c\ub85c\uc368 \uc5d0\ud3ec\ud06c\ub97c 1000\uc73c\ub85c \uc99d\uac00\uc2dc\ud0a4\uba74 \uc815\ud655\ub3c4\uac00 \uc62c\ub77c\uac00\uace0 learning rate\uac00 0.1\uc5d0\uc120 cost\uac00 \ubc1c\uc0b0\ud558\uba70 \uc815\ud655\ub3c4\uac00 \ub9e4\uc6b0 \ub5a8\uc5b4\uc9c0\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\ub2e4 \ub530\ub77c\uc11c epoch 1000\uc5d0 learning rate 0.0001\uc774 \uac00\uc7a5 \uc801\ud569\ud558\ub2e4\uace0 \ud310\ub2e8\ud588\ub2e4"}}