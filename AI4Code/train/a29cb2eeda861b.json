{"cell_type":{"f5e05f2f":"code","c4495522":"code","145daec9":"code","b057df74":"code","df3990a3":"code","af4098ec":"code","22957950":"code","b9db3b84":"code","8dccba2a":"code","05d708bf":"code","76220622":"code","cbacacd2":"code","90c87390":"code","41cc9d11":"code","cfd4b016":"code","d52132b8":"code","e02d9e91":"code","1c7b8eb0":"code","cf79c67d":"markdown","4d191da7":"markdown","a2bd228f":"markdown","18132756":"markdown","0cd80233":"markdown","cec5a98f":"markdown","9217dce1":"markdown","a54466c7":"markdown","88e7cfb6":"markdown","d5d70eec":"markdown","485a55bf":"markdown","4dae50e5":"markdown","8b653c9e":"markdown","60a22ef7":"markdown","5f2e3004":"markdown","25cb9f2f":"markdown","962d4774":"markdown","d2622b2d":"markdown","4ca9999f":"markdown","4b017f75":"markdown"},"source":{"f5e05f2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4495522":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))\nsigmoid(3)","145daec9":"def tanh(x):\n    numerator = 1-np.exp(-2*x)\n    denominator = 1+np.exp(-2*x)\n    return numerator\/denominator\ntanh(3)","b057df74":"def ReLU(x):\n    if x<0:\n        return 0\n    else:\n        return x\n    \nReLU(3)","df3990a3":"def leakyReLU(x,alpha=0.01):\n    if x<0:\n        return (alpha*x)\n    else:\n        return x\nleakyReLU(3,0.01)","af4098ec":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n    ","22957950":"X=np.array([[0,1],[1,0],[1,1],[0,0]])\ny=np.array([[1],[1],[0],[0]])","b9db3b84":"num_input=2\nnum_hidden=5\nnum_output=1","8dccba2a":"Wxh = np.random.randn(num_input,num_hidden)\nbh = np.zeros((1,num_hidden))","05d708bf":"Why=np.random.randn(num_hidden,num_output)\nby=np.zeros((1,num_output))","76220622":"def sigmoid(z):\n    return 1\/(1+np.exp(-z))","cbacacd2":"def sigmoid_derivative(z):\n    \n    return np.exp(-z)\/((1+np.exp(-z))**2)","90c87390":"def forward_prop(X,Wxh,Why):\n    z1 = np.dot(X,Wxh) + bh\n    a1 = sigmoid(z1)\n    z2 = np.dot(a1,Why) + by\n    y_hat = sigmoid(z2)\n    return z1,a1,z2,y_hat","41cc9d11":"def backword_prop(y_hat, z1, a1, z2):\n    delta2 = np.multiply(-(y-y_hat),sigmoid_derivative(z2))\n    dJ_dWhy = np.dot(a1.T, delta2)\n    delta1 = np.dot(delta2,Why.T)*sigmoid_derivative(z1)\n    dJ_dWxh = np.dot(X.T, delta1)\n    return dJ_dWxh, dJ_dWhy\n","cfd4b016":"def cost_function(y,y_hat):\n    J=0.5*sum((y-y_hat)**2)\n    return J","d52132b8":"alpha = 0.01\nnum_iterations = 5000","e02d9e91":"cost =[]\nfor i in range(5000):\n    z1,a1,z2,y_hat = forward_prop(X,Wxh,Why)\n    dJ_dWxh, dJ_dWhy = backword_prop(y_hat, z1, a1, z2)\n#update weights\n    Wxh = Wxh -alpha * dJ_dWxh\n    Why = Why -alpha * dJ_dWhy\n#compute cost\n    c = cost_function(y, y_hat)\n    cost.append(c)\n","1c7b8eb0":"plt.grid()\nplt.plot(range(5000),cost)\nplt.title('Cost Function')\nplt.xlabel('Training Iterations')\nplt.ylabel('Cost')","cf79c67d":"# Define the backward propagation:","4d191da7":"# Define the number of nodes in each layer","a2bd228f":"# Set the learning rate and the number of training iterations:","18132756":"# Now, we initialize the hidden to output layer weights:","0cd80233":"# Now, let's start training the network with the following code:","cec5a98f":"# Define the derivative of the sigmoid function:","9217dce1":"> Forward propagation is cool, isn't it? But how do we know whether the output generated\n> by the neural network is correct? We define a new function called the cost function ( ), also\n> known as the loss function ( ), which tells us how well our neural network is performing.\n> There are many different cost functions. We will use the mean squared error as a cost\n> function, which can be defined as the mean of the squared difference between the actual\n> output and the predicted output:","a54466c7":"**As you can observe in the following plot, the loss decreases over the training iterations:**","88e7cfb6":"This implies\nweights = weights -\u03b1 * gradients.\n\nWhat is ? It is called the learning rate.\n* If the learning rate is small, then we take a small step downward and our gradient descent can be slow.\n* If the learning rate is large, then we take a large step and our gradient descent will be fast, but we might fail to reach the global minimum and become stuck at a local minimum. \n* So,the learning rate should be chosen optimally:","d5d70eec":"# Define the cost function:","485a55bf":"# PREPARE THE DATA IN THE FORM OF XOR","4dae50e5":"# Define the forward propagation:","8b653c9e":"# Plot the cost function:","60a22ef7":"*****Forward Propagation*****","5f2e3004":"# Define the sigmoid activation function","25cb9f2f":"# Initialize the weights and bias randomly. First, we initialize the input to hidden layer weights:","962d4774":"# FISTRLY IMPORT THE LIBRARIES","d2622b2d":"**def forward_prop(X):\n\n    z1=np.dot(X,Wxh)+bh\n    a1=sigmoid(z1)\n    z2=np.dot(a1,Why)+bh\n    y_hat=sigmoid(z2)\n    return y_hat**","4ca9999f":"# gradient descent.","4b017f75":"LETS COMPLETE THE OUR FIRST NEURAL NETWORK STEP -BY -STEP:"}}