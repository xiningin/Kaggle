{"cell_type":{"10383ad4":"code","b7ee3ad1":"code","05fb6f96":"code","bee7b6ba":"code","d5a2aa61":"code","accb06f9":"code","67929d6a":"code","43fe4d34":"code","86483239":"code","ffd9dbf6":"code","98ee5be2":"code","80c3f003":"code","16f2e196":"code","55199ea1":"code","f1b7b76e":"code","00e78b78":"code","7574bcb7":"code","4ec143d6":"markdown","b0cba8f1":"markdown","935cc03b":"markdown","c9329295":"markdown","c30536c1":"markdown","1759f31e":"markdown","32b4f216":"markdown","8c0fd7e3":"markdown","fe8a5769":"markdown","ee16eaa0":"markdown","79b8452e":"markdown","e91e615a":"markdown","435ac4c7":"markdown"},"source":{"10383ad4":"#!\/bin\/bash\nset -e\n \nusage() {\ncat << EOF\nUsage: $0 [OPTIONS] [LABEL]\nPush a newly-built image with the given LABEL to gcr.io and DockerHub.\n \nOptions:\n    -g, --gpu                   Push the image with GPU support.\n    -s, --source-image IMAGE    Tag for the source image. \nEOF\n}\n \nSOURCE_IMAGE_TAG='kaggle\/python-build:latest'\nSOURCE_IMAGE_TAG_OVERRIDE=''\nTARGET_IMAGE='gcr.io\/kaggle-images\/python'\n \nwhile :; do\n    case \"$1\" in \n        -h|--help)\n            usage\n            exit\n            ;;\n        -g|--gpu)\n            SOURCE_IMAGE_TAG='kaggle\/python-gpu-build:latest'\n            TARGET_IMAGE='gcr.io\/kaggle-private-byod\/python'\n            ;;\n        -s|--source-image)\n            if [[ -z $2 ]]; then\n                usage\n                printf 'ERROR: No IMAGE specified after the %s flag.\\n' \"$1\" >&2\n                exit\n            fi\n            SOURCE_IMAGE_TAG_OVERRIDE=$2\n            shift # skip the flag value\n            ;;\n        -?*)\n            usage\n            printf 'ERROR: Unknown option: %s\\n' \"$1\" >&2\n            exit\n            ;;\n        *)            \n            break\n    esac\n \n    shift\ndone\n \nLABEL=${1:-testing}\n \nif [[ -n \"$SOURCE_IMAGE_TAG_OVERRIDE\" ]]; then\n    SOURCE_IMAGE_TAG=\"$SOURCE_IMAGE_TAG_OVERRIDE\"\nfi\n \nreadonly SOURCE_IMAGE_TAG\nreadonly TARGET_IMAGE\nreadonly LABEL\n \nset -x\ndocker tag \"${SOURCE_IMAGE_TAG}\" \"${TARGET_IMAGE}:${LABEL}\"\ngcloud docker -- push \"${TARGET_IMAGE}:${LABEL}\"\n ","b7ee3ad1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","05fb6f96":"p1 = \"\/kaggle\/input\/covid19-global-forecasting-week-4\/\"\np2 = \"\/kaggle\/input\/world-bank-wdi-212-health-systems\/\"\np3 = \"\/kaggle\/input\/covid19inf\/\"\ntrain = pd.read_csv(p1 + \"train.csv\")\ntest =  pd.read_csv(p1 + \"test.csv\")\nsubmission =  pd.read_csv(p1 + \"submission.csv\")\nhealth = pd.read_csv(p2 + \"2.12_Health_systems.csv\")\ncountry = pd.read_csv(p3 + \"covid19countryinfo.csv\")\npollution = pd.read_csv(p3 + \"region_pollution.csv\")","bee7b6ba":"country.drop(columns= country.columns[range(22,54)], inplace=True)\ncountry[\"pop\"] = country[\"pop\"].str.replace(\",\", \"\").astype('float64')\ncountry[\"quarantine\"] = pd.to_datetime(country.quarantine)\ncountry[\"schools\"] = pd.to_datetime(country.schools)\ncountry[\"restrictions\"] = pd.to_datetime(country.restrictions)","d5a2aa61":"train[\"Date\"] = pd.to_datetime(train.Date)\ntrain[\"country_province\"] = train[\"Province_State\"]\ntrain.country_province.fillna(train[\"Country_Region\"], inplace=True)\ntest[\"Date\"] = pd.to_datetime(test.Date)\ntest[\"country_province\"] = test[\"Province_State\"]\ntest.country_province.fillna(test[\"Country_Region\"], inplace=True)\ntrain = train.merge(country, how='left', left_on = [\"country_province\"], right_on = [\"country\"])\ntrain = train.merge(pollution, how='left', left_on = [\"country_province\"], right_on = [\"Region\"])\ntrain = train.merge(health, how='left', left_on = [\"Country_Region\", \"Province_State\"], right_on = [\"Country_Region\", \"Province_State\"])\ntest = test.merge(country, how='left', left_on = [\"country_province\"], right_on = [\"country\"])\ntest = test.merge(pollution, how='left', left_on = [\"country_province\"], right_on = [\"Region\"])\ntest = test.merge(health, how='left', left_on = [\"Country_Region\", \"Province_State\"], right_on = [\"Country_Region\", \"Province_State\"])\ntrain[\"days\"] = (train.Date - train.Date[0]).dt.days\ntest[\"days\"] = (test.Date - train.Date[0]).dt.days\n\n#The columns with region\/state names in the different csvs are not longer needed\ncolumns_to_drop = [\"country\", \"Region\", \"Province_State\", \"Country_Region\", \"World_Bank_Name\"]","accb06f9":"train[\"in_quarantine\"] = 0\ntrain[\"in_schools\"] = 0\ntrain[\"in_restrictions\"] = 0\ntest[\"in_quarantine\"] = 0\ntest[\"in_schools\"] = 0\ntest[\"in_restrictions\"] = 0\nfor cp in train.country_province.unique():\n    quarantine = country.loc[country.country == cp, \"quarantine\"]\n    schools = country.loc[country.country == cp, \"schools\"]\n    restrictions = country.loc[country.country == cp, \"restrictions\"]\n    if (len(quarantine) > 0) and (quarantine.values[0] is not np.nan):\n        date1 = pd.to_datetime(quarantine.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_quarantine\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_quarantine\"] = (test.Date - date1).dt.days\n        \n    if (len(schools) > 0) and (schools.values[0] is not np.nan):\n        date1 = pd.to_datetime(schools.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_schools\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_schools\"] = (test.Date - date1).dt.days\n\n    if (len(restrictions) > 0) and (restrictions.values[0] is not np.nan):\n        date1 = pd.to_datetime(restrictions.values[0])\n        train.loc[(train.country_province == cp) & (train.Date > date1), \"in_restrictions\"] = (train.Date - date1).dt.days\n        test.loc[(test.country_province == cp) & (test.Date > date1), \"in_restrictions\"] = (test.Date - date1).dt.days\n\ncolumns_to_drop += [\"quarantine\", \"schools\", \"restrictions\"]","67929d6a":"from sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder()\ntrain[\"country_province\"] = lb.fit_transform(train.country_province)\ntest[\"country_province\"] = lb.transform(test.country_province)","43fe4d34":"train[\"days_from_first_case\"] = 0\ntest[\"days_from_first_case\"] = 0\ntrain[\"days_from_first_death\"] = 0\ntrain[\"days_from_case_100\"] = 0\ntest[\"days_from_case_100\"] = 0\ntest[\"days_from_first_death\"] = 0\n\ndates = list(train.Date.unique())\nfor province in train.country_province.unique():\n    #print(province)\n    mask1 = train.country_province == province\n    mask2 = train.ConfirmedCases > 1.0\n    mask3 = train.ConfirmedCases > 100.0\n    mask4 = train.Fatalities > 1.0\n    try:\n        idx1 = train.loc[mask1 & mask2 ,[\"ConfirmedCases\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    #print(dateidx1)\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_first_case\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_first_case\"] = (test.Date - dateidx1).dt.days\n    \n    try:\n        idx1 = train.loc[mask1 & mask3 ,[\"ConfirmedCases\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_case_100\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_case_100\"] = (test.Date - dateidx1).dt.days    \n\n        \n    try:\n        idx1 = train.loc[mask1 & mask4 ,[\"Fatalities\"]].idxmin()[0]\n        dateidx1 = train.iloc[idx1][\"Date\"]\n    except:\n        dateidx1 = test.Date.max()\n        pass\n    train.loc[mask1 & (train.Date >= dateidx1), \"days_from_first_death\"] = (train.Date - dateidx1).dt.days\n    test.loc[mask1 & (test.Date >= dateidx1), \"days_from_first_death\"] = (test.Date - dateidx1).dt.days    \n\ntrain.fillna(value = 0, inplace = True)\ntest.fillna(value = 0, inplace = True)  ","86483239":"#Construction of laged variables \nlag_number = 3\nfor lag in range(1, lag_number + 1):\n    var_name = \"cases_lag%d\" % lag\n    train[var_name] = train.ConfirmedCases.shift(periods = lag)\n    train.loc[train.Date <= train.Date[lag - 1] , var_name] = 0\n    var_name = \"fatalities_lag%d\" % lag\n    train[var_name] = train.Fatalities.shift(periods = 1)\n    train.loc[train.Date <= train.Date[lag - 1] , var_name] = 0","ffd9dbf6":"#Days that coincide in train and test\nprint(train.loc[train.Date.isin(test.Date.unique()), \"Date\"].unique())\n# The smallest of those days will be the separation between train and validation\nsep_date = train.loc[train.Date.isin(test.Date.unique()), \"Date\"].unique().min()\n\nresult_columns = [\"ConfirmedCases\", \"Fatalities\"]\nX_train = train.loc[(train.Date<sep_date),].drop(columns = columns_to_drop + [\"Id\", \"Date\"] + result_columns)\ny_train_cases = train.loc[(train.Date<sep_date),].ConfirmedCases\ny_train_fatalities = train.loc[(train.Date<sep_date),].Fatalities\n\nX_val = train.loc[(train.Date>=sep_date),].drop(columns = columns_to_drop + [\"Id\", \"Date\"] + result_columns)\ny_val_cases = train.loc[(train.Date>=sep_date),].ConfirmedCases\ny_val_fatalities = train.loc[(train.Date>=sep_date),].Fatalities\n\nX_test = test.drop(columns = columns_to_drop + [\"ForecastId\", \"Date\"])","98ee5be2":"from sklearn.metrics import mean_squared_error\ndef validate_models(model_cases, model_fatalities):\n    predict_train_cases = model_cases.predict(X_train)\n    predict_val_cases = model_cases.predict(X_val)\n    print(\"RMSE in train detected cases: \", np.sqrt(mean_squared_error(y_train_cases, predict_train_cases)))\n    print(\"RMSE in validation detected cases: \", np.sqrt(mean_squared_error(y_val_cases, predict_val_cases)))\n    predict_train_fatalities = model_fatalities.predict(X_train)\n    predict_val_fatalities = model_fatalities.predict(X_val)\n    print(\"RMSE in train fatalities: \", np.sqrt(mean_squared_error(y_train_fatalities, predict_train_fatalities)))\n    print(\"RMSE in validation fatalities: \", np.sqrt(mean_squared_error(y_val_fatalities, predict_val_fatalities)))","80c3f003":"from sklearn.linear_model import LinearRegression\nlm_cases = LinearRegression()\nlm_cases.fit(X_train, y_train_cases)\n\nlm_fatalities = LinearRegression()\nlm_fatalities.fit(X_train, y_train_fatalities)\n\nvalidate_models(lm_cases, lm_fatalities)","16f2e196":"#RandomForest\nfrom sklearn.ensemble import RandomForestRegressor\nrf_cases = RandomForestRegressor(n_estimators= 400, max_depth=6, random_state=0, verbose=0, n_jobs=-1)\nrf_cases.fit(X_train, y_train_cases)\n\nrf_fatalities = RandomForestRegressor(n_estimators= 400, max_depth=6, random_state=0, verbose=0, n_jobs=-1)\nrf_fatalities.fit(X_train, y_train_fatalities)\n\nvalidate_models(rf_cases, rf_fatalities)","55199ea1":"import lightgbm as lgb\nlgb_params = {\n               'feature_fraction': 0.8,\n               'metric': 'rmse',\n               'nthread':-1, \n               'min_data_in_leaf': 2**4,\n               'bagging_fraction': 0.75, \n               'learning_rate': 0.5, \n               'objective': 'mse', \n               'bagging_seed': 2**5, \n               'num_leaves': 2**6,\n               'bagging_freq':1,\n               'verbose':1 \n              }\nlgbm_cases = lgb.train(lgb_params, \n                       train_set=lgb.Dataset(X_train, label=y_train_cases), \n                       valid_sets=lgb.Dataset(X_val, label=y_val_cases), \n                       num_boost_round=500)\nlgbm_fatalities = lgb.train(lgb_params, \n                            train_set=lgb.Dataset(X_train, label=y_train_fatalities), \n                            valid_sets=lgb.Dataset(X_val, label=y_val_fatalities), \n                            num_boost_round=500)\nvalidate_models(lgbm_cases, lgbm_fatalities)","f1b7b76e":"lgb.plot_importance(lgbm_fatalities)\nplt.rcParams['figure.figsize'] = [10, 10]\nplt.show()","00e78b78":"lags = {}\npredict_test_cases, predict_test_fatalities = [] , []\ntest_min_day = test.days.min()\nfor i in range(1, lag_number + 1):\n    lags[\"caseslag%d\" % i] = 0\n    lags[\"fatalitieslag%d\" % i] = 0\n    \nfor ind in tqdm_notebook(range(len(X_test))):\n    #print(\"case: {} of {}\".format(ind, len(X_test)))\n    #First lag data are obtained either from previous calculations or train data\n    if X_test.iloc[ind].days == test_min_day:\n        #print(test_min_day)\n        for i in range(1, lag_number + 1):\n            mask1 = train.days == (test_min_day - i)\n            mask2 = train.country_province == X_test.iloc[ind].country_province\n            lags[\"caseslag%d\" % i] = train.loc[mask1 & mask2, \"ConfirmedCases\"].values[0]\n            lags[\"fatalitieslag%d\" % i] = train.loc[mask1 & mask2, \"Fatalities\"].values[0]\n    else:\n        lags[\"caseslag1\"] = pred_cases\n        lags[\"fatalitieslag1\"] = pred_fatalities\n        for i in range(2, lag_number + 1):\n            lags[\"caseslag%d\" % i] = lags[\"caseslag%d\" % (i-1)]\n            lags[\"fatalitieslag%d\" % i] = lags[\"fatalitieslag%d\" % (i-1)]\n    x_test = X_test.iloc[ind].copy()\n    x_test =pd.DataFrame(x_test).transpose()\n    for i in range(1, lag_number + 1):\n        x_test[\"cases_lag%d\" % i] = lags[\"caseslag%d\" % i]\n        x_test[\"fatalities_lag%d\" % i] = lags[\"fatalitieslag%d\" % i]\n        \n    pred_cases = rf_cases.predict(x_test)[0]\n    pred_fatalities = rf_fatalities.predict(x_test)[0]\n    predict_test_cases.append(pred_cases)\n    predict_test_fatalities.append(pred_fatalities)\n    ","7574bcb7":"submission.ConfirmedCases = predict_test_cases\nsubmission.Fatalities = predict_test_fatalities\nsubmission.to_csv(\"\/kaggle\/working\/submission.csv\", index = False)","4ec143d6":"submission","b0cba8f1":"Some data munging in the contry dataset","935cc03b":"Label encode the countries and\/or regions","c9329295":"Finally the train and test data can be prepared for the models eliminating the object variables and repited ones. From train data a small sample is taken for validation, the days that coincide in train and test data.","c30536c1":"Data munging and merging","1759f31e":"Definition of lagged variables","32b4f216":"**Data input**","8c0fd7e3":"More data preparation, this time when it is in quarnatine, school stops and restriction. Those data are not complete and this is a pity but they are valuable.","fe8a5769":"Generate three new variables, days from first death, first reported case and 100th reported case. The virus arrives one place by travelling that is not easy to track for the model.","ee16eaa0":"Both model worked well in train set but much worse in validation, seems overfitting. Let's see the features importances","79b8452e":"Finally the calculation for the test set requires a rolling forecast, going one by one to calculate the lags with the previous values","e91e615a":"Now the models, let's start with a Random Forest and after a lightgbm","435ac4c7":"Before starting the models a function is defined to reduce code to check errors"}}