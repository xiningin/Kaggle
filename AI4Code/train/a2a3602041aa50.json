{"cell_type":{"67d2d5a8":"code","b7a683e4":"code","4a56c134":"code","9f5903cb":"code","47f18a3c":"code","3592c8ab":"code","cdb6a63a":"code","872f97c1":"code","e163d3c3":"code","a0449eeb":"code","e18dea4e":"code","8205cd76":"code","3991201a":"code","2cd8ed48":"code","f60b220a":"code","f9c79260":"code","7f246da8":"code","bd2b9797":"code","8de2a1ef":"code","43c643c7":"code","43b7c2f2":"code","ac00fab5":"code","3698fff6":"code","53be1221":"code","fc803164":"code","c22574b6":"code","808afb8b":"code","036b7f96":"code","1842c263":"code","32885e94":"code","01f80670":"code","4c07d1f3":"code","69592093":"code","150648a4":"code","9c504ecd":"code","2c6bedaf":"code","ce742c5d":"code","b7443f03":"code","e75b3629":"code","58201530":"code","07e7e649":"code","620eef33":"code","f53b45c8":"code","3002a88f":"code","1f0e5809":"code","d55d3ac5":"code","96385b18":"code","70c10c66":"code","17be43ff":"code","e407501e":"code","010cb5c4":"code","cb532967":"code","036d3497":"code","c4edb6e2":"code","866a6235":"code","24c1f1fa":"code","811be0af":"code","ab5bc8c0":"code","a4800fd5":"code","3c7ec444":"code","fff1a7a4":"code","e1236b76":"code","9de1eb22":"markdown","ba04b5e9":"markdown","2b032bf2":"markdown","4f27959f":"markdown","992bee33":"markdown","ae11a624":"markdown","1882d4ec":"markdown","b8e52a8e":"markdown","4c236b97":"markdown","f197db1f":"markdown","71232372":"markdown","c577203d":"markdown","79438155":"markdown","aea0a6de":"markdown","6259700a":"markdown","06addbcc":"markdown","5da00bcb":"markdown","f0698d9f":"markdown","fb24a48e":"markdown","1e5b9e97":"markdown","418f3276":"markdown","21720d88":"markdown","c1958ac3":"markdown","7efee0e6":"markdown","c50d4c7c":"markdown","e5e6c5ab":"markdown","30ef9a70":"markdown","e5507f2a":"markdown","c5986296":"markdown","dc5153b2":"markdown","95ad4fde":"markdown","af8bea05":"markdown","ce8e43bc":"markdown","ebc01f93":"markdown","9f532898":"markdown","864e5c91":"markdown","a282955a":"markdown","36e36cf3":"markdown","cd0579df":"markdown","70106926":"markdown","b92f69d0":"markdown","66566d9f":"markdown","3aa6d1b4":"markdown","36eb26fb":"markdown","aced38bc":"markdown","a425820d":"markdown","8281e7d9":"markdown","540b5faa":"markdown","e0a1b317":"markdown"},"source":{"67d2d5a8":"!pip install -U scikit-learn","b7a683e4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport warnings\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings ( \"ignore\" )\n\n# pd.set_option(\"\"display.height\"\", 10)\n# pd.set_option(\"\"display.max_columns\"\", 500)\n#pd.set_option(\"\"display.width\"\", 1000)\npd.set_option(\"display.max_rows\", 100)\n\n# Version check\nprint('Current Version'.center(20,'-'))\nprint(f\"Pandas:{pd.__version__}\")\nprint(f\"Numpy:{np.__version__}\")\nprint(f\"Seaborn:{sns.__version__}\")\nprint(f\"Scikit learn: {sklearn.__version__}\")","4a56c134":"data = pd.read_csv ( \"..\/input\/online-news-popularity\/OnlineNewsPopularity.csv\" )\nprint(\"Dataset Shape: {}\".format(data.shape))\nprint(data.columns.tolist())","9f5903cb":"for i in data.columns:\n    data = data.rename ( columns = { i : i.lstrip ( ) } )\n\ndata.columns[:10]","47f18a3c":"print(data['timedelta'].nunique())\n\ndata['timedelta'].describe()","3592c8ab":"df = data.drop ( [ \"url\" , \"timedelta\" ] , axis = 1 )\n\ndf.describe ( )","cdb6a63a":"df.isnull ( ).sum ( ).sum ( )","872f97c1":"num = df.select_dtypes ( include = \"number\" )","e163d3c3":"counter = 1\nplt.figure(figsize=(15,18))\nfor col in num.columns:\n    if np.abs(df [col].skew ( )) > 1 and df[col].nunique() < 10:\n        plt.subplot(5,3,counter)\n        counter += 1\n        df [col].value_counts().plot.bar()\n        plt.xticks(rotation = 45)\n        plt.title(f'{col}\\n(skewness {round(df [ col ].skew ( ),2)})')\n\nplt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\nplt.show ( )","a0449eeb":"%%time\n# > 10 sec\ncounter = 1\ntruly_conti = []\nplt.figure(figsize=(18,40))\nfor i in num.columns:\n    if np.abs(df [ i ].skew ( )) > 1 and df[i].nunique() > 10:\n        plt.subplot(20,3,counter)\n        counter += 1\n        truly_conti.append(i)\n        sns.distplot ( df [ i ] )\n        plt.title(f'{i} (skewness {round(df [ i ].skew ( ),1)})')\n        plt.xticks(rotation = 45)\nplt.tight_layout()\nplt.show ( )","e18dea4e":"plt.figure(figsize=(18,3))\nsns.boxenplot(data=df.loc[:,truly_conti[0]],orient='h')\nplt.title(truly_conti[0])\nplt.show()","8205cd76":"# plt.rcParams['figure.figsize'] = 18,10\nplt.figure(figsize=(18,6))\nsns.boxenplot(data=df.loc[:,truly_conti[1:4]],orient='h')\nplt.show()","3991201a":"# plt.rcParams['figure.figsize'] = 18,10\nplt.figure(figsize=(18,6))\nsns.boxenplot(data=df.loc[:,truly_conti[4:10]],orient='h')\nplt.show()","2cd8ed48":"plt.figure(figsize=(18,3))\nsns.boxenplot(data=df.loc[:,truly_conti[10]],orient='h')\nplt.title(truly_conti[10])\nplt.show()","f60b220a":"plt.figure(figsize=(18,3))\nsns.boxenplot(data=df.loc[:,truly_conti[11]],orient='h')\nplt.title(truly_conti[11])\nplt.show()","f9c79260":"plt.figure(figsize=(18,10))\nsns.boxenplot(data=df.loc[:,truly_conti[12:19]],orient='h')\nplt.show()","7f246da8":"plt.figure(figsize=(18,10))\nsns.boxenplot(data=df.loc[:,truly_conti[19:30]],orient='h')\nplt.show()","bd2b9797":"plt.figure(figsize=(18,3))\nsns.boxenplot(data=df.loc[:,truly_conti[30]],orient='h')\nplt.title(truly_conti[30])\nplt.show()","8de2a1ef":"df['shares'].describe(percentiles=[.1,.25,.375,.50,.625,.75,.90,.95,1])","43c643c7":"# Will use the following convention\n# 0: Less popular\n# 1: Popular\ndf['Popularity'] = pd.cut(data.shares,(0,1400,843300), labels=[0,1])\ndf['Popularity'] = df['Popularity'].astype(int)\ndf['Popularity'].value_counts(normalize=True)","43b7c2f2":"words = [\n    'n_tokens_title','n_tokens_content','average_token_length','n_non_stop_words','n_unique_tokens','n_non_stop_unique_tokens'\n]","ac00fab5":"plt.figure(figsize=(10,8))\nsns.heatmap(df[words].corr(),cmap='coolwarm',annot=True)\nplt.show()","3698fff6":"section = df[words + ['Popularity']]\nsns.heatmap(pd.DataFrame(section.corr()['Popularity']),cmap='coolwarm',annot=True)\nplt.show()","53be1221":"sns.pairplot(data=df,diag_kind='kde',vars=words,hue='Popularity')\nplt.show()","fc803164":"links = [\n    'num_hrefs','num_self_hrefs','num_imgs','num_videos'\n]","c22574b6":"plt.figure(figsize=(10,8))\nsns.heatmap(df[links].corr(),cmap='Accent',annot=True)\nplt.show()","808afb8b":"section = df[links + ['Popularity']]\nsns.heatmap(pd.DataFrame(section.corr()['Popularity']),cmap='coolwarm',annot=True)\nplt.show()","036b7f96":"plt.figure(figsize=(10,8))\nsns.pairplot(data=df,diag_kind='kde',vars=section)\nplt.show()","1842c263":"time = [\n    'weekday_is_monday','weekday_is_tuesday','weekday_is_wednesday','weekday_is_thursday','weekday_is_friday','weekday_is_saturday','weekday_is_sunday','is_weekend'\n]","32885e94":"merge = df [ [ \"weekday_is_monday\" , \"weekday_is_tuesday\" , \"weekday_is_wednesday\" ,\n               \"weekday_is_thursday\" , \"weekday_is_friday\" , \"weekday_is_saturday\" , \n                \"weekday_is_sunday\" ] ]\narr = [ ]\nfor i in range ( merge.shape [ 0 ] ):\n    for j in range ( merge.shape [ 1 ] ):\n        if j == 0 and merge.iloc [ i , j ] == 1:\n            arr.append ( \"Monday\" )\n        elif j == 1 and merge.iloc [ i , j ] == 1:\n            arr.append ( \"Tuesday\" )\n        elif j == 2 and merge.iloc [ i , j ] == 1:\n            arr.append ( \"Wednesday\" )\n        elif j == 3 and merge.iloc [ i , j ] == 1:\n            arr.append ( \"Thursday\" )\n        elif j == 4 and merge.iloc [ i , j ] == 1:\n            arr.append ( \"Friday\" )\n        elif j == 5 and merge.iloc [ i , j ] == 1:\n            arr.append ( \"Saturday\" )\n        elif j == 6 and merge.iloc [ i , j ] == 1:\n            arr.append ( \"Sunday\" )\n\ndf [ \"Day\" ] = arr","01f80670":"%%time\nplt.figure(1)\ng = sns.PairGrid(data=df, y_vars=\"shares\",\n                 x_vars=time[:-1],\n                 height=5, aspect=.5)\ng.map(sns.pointplot, scale=1.2, errwidth=4).add_legend()\nsns.despine(fig=g.fig, left=True)\nplt.title('Total shares each day')\n\nplt.figure(2)\ng = sns.catplot(x=\"Day\", y=\"shares\",\n                capsize=.2, palette=\"YlGnBu_d\", height=6, aspect=3,\n                kind=\"point\", data=df)\nplt.tight_layout()\nplt.show()","4c07d1f3":"%%time\ng = sns.PairGrid(df, y_vars=\"shares\",\n                 x_vars=time[:-1],\n                 hue=\"Popularity\",\n                 height=5, aspect=.5)\nplt.title('Distribution of shares per day')\ng.map(sns.pointplot, scale=1.3, errwidth=4)\nsns.despine(fig=g.fig, left=True)\n\ng = sns.catplot(x=\"Day\", y=\"shares\", hue=\"Popularity\",\n                capsize=.2, palette=\"YlGnBu_d\", height=6, aspect=2.5,\n                kind=\"point\", data=df)\ng.despine(left=True)\nplt.tight_layout()\nplt.show()","69592093":"df.drop(columns=['Day'],inplace=True)","150648a4":"plt.figure(figsize=(10,8))\nsns.heatmap(df[time].corr(),cmap='coolwarm',annot=True)\nplt.show()","9c504ecd":"section = df[time + ['Popularity']]\nsns.heatmap(pd.DataFrame(section.corr()['Popularity']),cmap='coolwarm',annot=True)\nplt.show()","2c6bedaf":"# import matplotlib.gridspec as gridspec\n\n# plt.close('all')\n# fig = plt.figure()\n\n# gs1 = gridspec.GridSpec(4, 3)\n# ax1 = fig.add_subplot(gs1[0])\n# ax2 = fig.add_subplot(gs1[1])","ce742c5d":"keywords = ['num_keywords','kw_min_min','kw_max_min','kw_avg_min','kw_min_max','kw_max_max','kw_avg_max','kw_min_avg','kw_max_avg','kw_avg_avg']\ntarget = 'Popularity'\n\n# fig,ax = plt.subplots(4,3,figsize=(10,8))\nfor i,col in enumerate(keywords,1):\n    mean_per_cat = pd.pivot_table(df,values=col,index=target,aggfunc=\"mean\")\n    mean_per_cat.plot.barh(alpha=0.4)\n    plt.title(col)\nplt.show ( )","b7443f03":"plt.figure(figsize=(10,8))\nsns.heatmap(df[keywords].corr(),cmap='Accent',annot=True)\nplt.show()","e75b3629":"section = df[keywords + ['Popularity']]\nsns.heatmap(pd.DataFrame(section.corr()['Popularity']),cmap='coolwarm',annot=True)\nplt.show()","58201530":"channels = [\n    'data_channel_is_lifestyle','data_channel_is_entertainment','data_channel_is_bus','data_channel_is_socmed','data_channel_is_tech','data_channel_is_world'\n]\nchannel_names = ['Lifestyle','Entertainment','BUS','SocMed','Tech','World']","07e7e649":"dict_channels = { k:v for k,v in zip(channels,range(1,7))}\ndict_channels","620eef33":"channel_type = df.loc[:,channels].apply(lambda x: np.argmax(x),axis=1)\nchannel_type","f53b45c8":"df['channels'] = channel_type.apply(lambda x: dict_channels[x])","3002a88f":"g = sns.catplot(x=\"channels\", y=\"shares\",\n                capsize=.2, palette=\"YlGnBu_d\", height=6, aspect=3,\n                kind=\"point\", data=df)\nplt.xticks(range(6),channel_names)\nplt.title(\"Total shares per category\")","1f0e5809":"g = sns.catplot(x=\"channels\", y=\"shares\",hue=\"Popularity\",\n                capsize=.2, palette=\"YlGnBu_d\", height=6, aspect=3,\n                kind=\"point\", data=df)\n\nplt.xticks(range(6),channel_names)\nplt.title('Channels based on popularity')\nplt.show()","d55d3ac5":"df.drop(columns=['channels'],inplace=True)","96385b18":"plt.figure(figsize=(10,8))\nsns.heatmap(df[channels].corr(),cmap='Accent',annot=True)\nplt.show()","70c10c66":"section = df[channels + ['Popularity']]\nsns.heatmap(pd.DataFrame(section.corr()['Popularity']),cmap='coolwarm',annot=True)\nplt.show()","17be43ff":"from scipy.stats import zscore\n\ntarget = 'Popularity'\nX = df.drop(columns=[target,'shares'])\ny = df[target]\n\nprint(f\"Original shape of dataframe: {X.shape}\")","e407501e":"outliers = (X.apply(zscore)<3).all(axis=1)\nX[outliers].shape","010cb5c4":"f\"Choosing this approach we will loose {39644 - 24627} datapoints \"","cb532967":"# This will be the data without any outliers\nX[outliers].to_csv('data.csv',index=False)","036d3497":"# Since we have large amount of data planning to split it in 70-30 fashion\n# train will be 70\n# Of the 30, 10 validation and 20 test set\n# By default, the data will be split in stratified fashion\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=.3,random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)","c4edb6e2":"from fancyimpute import KNN    ","866a6235":"outliers = np.abs(X_train.apply(zscore)) > 3\noutliers.sum().sum()","24c1f1fa":"from tqdm import tqdm","811be0af":"for col_name in tqdm(X_train.columns):\n    mean = X_train[col_name].mean()\n    std = X_train[col_name].std()\n    X_train[col_name] = X_train[col_name].apply(lambda x: np.nan if abs((x-mean)\/std)>3 else x)\n#     X_train[col_name].loc[(X_train[col_name] < lower_limit) | (X_train[col_name] > upper_limit)] = np.NaN","ab5bc8c0":"print(X_train.isna().sum().sum())","a4800fd5":"# KNN based imputation \nX_filled_knn = KNN(k=3).fit_transform(X_train)","3c7ec444":"knn_train = pd.DataFrame(X_filled_knn,columns=X_train.columns)\nknn_train['Popularity'] = y_train.values\nknn_train.to_csv('train.csv',index=False)\nprint(knn_train.shape)","fff1a7a4":"knn_train.head()","e1236b76":"test = pd.concat((X_test,y_test),axis=1)\ntest.to_csv('test.csv',index=False)","9de1eb22":"### Words\n1. n_tokens_title: Number of words in the title\n- n_tokens_content: Number of words in the content\n- average_token_length: Average length of the words in the content \n- n_non_stop_words: Rate of non-stop words in the content\n- n_unique_tokens: Rate of unique words in the content\n- n_non_stop_unique_tokens: Rate of unique non-stop words in the content","ba04b5e9":"##### This notebook will be mainly dealing with exploratory data analysis and various outlier treatment methods.","2b032bf2":"##### We can check each row and see if all the features in the row are less than z-score < 3 (those are valid datapoints).","4f27959f":"## Key takeaways","992bee33":"##### Correlation among categorical and continuous is not a right representation but in our case we have all the categories one hot encoded so this can be a fair comparision.","ae11a624":"### Features with high skewness","1882d4ec":"### Null value check","b8e52a8e":"According to the dataset description, time delta is the days between the article publication and the dataset acquisition (non-predictive). The value ranges from 8 to 731, which means a few articles got a lot of time to reach the maximum audience and others didn't. Not sure about the impact, might add an extra feature rate of shares (shares\/time delta) which might be a better indicator. For the time being dropping the feature.","4c236b97":"<center>![Header](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fhomeopathyplus.com%2Fwp-content%2Fuploads%2F2017%2F11%2FGrowth.jpg&f=1&nofb=1)<center>","f197db1f":"![image.png](attachment:image.png)","71232372":"1. LDA_00: Closeness to LDA topic 0\n- LDA_01: Closeness to LDA topic 1\n- LDA_02: Closeness to LDA topic 2\n- LDA_03: Closeness to LDA topic 3\n- LDA_04: Closeness to LDA topic 4\n- title_subjectivity: Title subjectivity\n- title_sentiment_polarity: Title polarity\n- abs_title_subjectivity: Absolute subjectivity level\n- abs_title_sentiment_polarity: Absolute polarity level\n- global_subjectivity: Text subjectivity\n- global_sentiment_polarity: Text sentiment polarity\n- global_rate_positive_words: Rate of positive words in the content\n- global_rate_negative_words: Rate of negative words in the content\n- avg_positive_polarity: Avg. polarity of positive words\n- min_positive_polarity: Min. polarity of positive words\n- max_positive_polarity: Max. polarity of positive words\n- avg_negative_polarity: Avg. polarity of negative words\n- min_negative_polarity: Min. polarity of negative words\n- max_negative_polarity: Max. polarity of negative words\n\n","c577203d":"### 3. Reducing \n\nIn this approach we will reduce the effect of outliers by applying transformations like log and square root.\n","79438155":"\n## <center>Start With Why?<\/center>\n\nWe are living in an era of data explosion if you think that it is too much of an exaggeration then it must be interesting to know that, more than 2 million articles are published everyday on the web (Source: MarketingProfs). So now, there is a high chance that your content, even after being a really good work might get unnoticed due to the huge volume of options. Which makes it an interesting study to understand what kind of articles flourish in this ecosystem.\n\n### Business objective\nThere can be multiple use cases for the given set of data, however, I am considering the below scenario\n> Finding out which articles will be popular and prescribing measures to increase the visibility of the content.\n\n### Optimization baseline\n\nThis project has been inspired from the research work presented in the paper - [A Proactive Intelligent Decision Support System\nfor Predicting the Popularity of Online News](http:\/\/repositorium.sdum.uminho.pt\/bitstream\/1822\/39169\/1\/main.pdf). So the attempt will \nbe to achieve a better accuracy ( 67% ) than the best result in the paper.\n\nThe data was collected on Jan 2015 from mashable.com and following are the features present.","aea0a6de":"##### Most of the link features seems like theya are not linearly correlated with Popularity","6259700a":"Extreme values observed in the above case, direct removal of outliers will not be feasible","06addbcc":"### 4. Marking the outliers","5da00bcb":"Only single outliers in these columns, in such cases it will be safe to REMOVE them.","f0698d9f":"### 5. Keep them\n\nYeah you read it right, there are certain outliers which are not meant to be removed. When the datapoints are not errors and are just extreme cases which are likely to occur then we will be training without any treatment.","fb24a48e":"##### No multicollinearity: There are no features which are correlated with each other.\n\nChecking the importance of features with the target variable.","1e5b9e97":"#### Distribution of truly continuous values","418f3276":"> The below two methods requires the data to be splitted before processing or else our model will be a victim of data leakage.\n\n![split](https:\/\/www.i2tutorials.com\/wp-content\/uploads\/2019\/09\/Training-dataset-Testing-Dataset-Validation-dataset-i2tutorials.png)","21720d88":"<img src=\"https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fmiro.medium.com%2Fmax%2F1400%2F0*2NlsLGlMtgtII_hN.&f=1&nofb=1\" width=500px\/>","c1958ac3":"### Keywords\n1. num_keywords: Number of keywords in the metadata\n- kw_min_min: Worst keyword (min. shares)\n- kw_max_min: Worst keyword (max. shares)\n- kw_avg_min: Worst keyword (avg. shares)\n- kw_min_max: Best keyword (min. shares)\n- kw_max_max: Best keyword (max. shares)\n- kw_avg_max: Best keyword (avg. shares)\n- kw_min_avg: Avg. keyword (min. shares)\n- kw_max_avg: Avg. keyword (max. shares)\n- kw_avg_avg: Avg. keyword (avg. shares)\n- data_channel_is_lifestyle: Is data channel 'Lifestyle'?\n- data_channel_is_entertainment: Is data channel 'Entertainment'?\n- data_channel_is_bus: Is data channel 'Business'?\n- data_channel_is_socmed: Is data channel 'Social Media'?\n- data_channel_is_tech: Is data channel 'Tech'?\n- data_channel_is_world: Is data channel 'World'?\n","7efee0e6":"## Output","c50d4c7c":"### 1. Removing values\nRemoving all the outliers and building model.","e5e6c5ab":"The dataset was collection on Jan, 2015.","30ef9a70":"##### The total number of shares is maximum on Saturday, Sunday and Monday in the order.","e5507f2a":"### Natural Language Processing","c5986296":"## Outlier treatment","dc5153b2":"## 2. Replacing \n\nThis can be applied, when an outlier is like a missing\/incorrect value.\nThen the outliers can be imputed with the following values.\n\n1. Mean\/Median\/Mode\n2. Capping with extreme values\n3. Forward \/ backward fill\n3. KNN based imputation\n4. Using domain knowledege","95ad4fde":"#### All of the above columns are having an extra space in the beginning we will be removing that next.","af8bea05":"## Taking <strike>one<\/strike> two variables at a time\nBivariate analysis","ce8e43bc":"### Links and digital media\n1. num_hrefs: Number of links\n- num_self_hrefs: Number of links to other articles published by Mashable\n- num_imgs: Number of images\n- num_videos: Number of videos\n","ebc01f93":"### Time\n1. weekday_is_monday: Was the article published on a Monday?\n- weekday_is_tuesday: Was the article published on a Tuesday?\n- weekday_is_wednesday: Was the article published on a Wednesday?\n- weekday_is_thursday: Was the article published on a Thursday?\n- weekday_is_friday: Was the article published on a Friday?\n- weekday_is_saturday: Was the article published on a Saturday?\n- weekday_is_sunday: Was the article published on a Sunday?\n- is_weekend: Was the article published on the weekend?","9f532898":"Fortunately or unfortunately we are having massive outliers in our dataset and we will be working through multiple outlier treatment techniques","864e5c91":"##### num_href,num_imgs and num_href,num_self_hrefs are having some correlation but it is quite low so nothing commendable can be mentioned now.","a282955a":"##### However, Monday, Tuesday and Wednesday has seen more content popularity.\n\nWe would no longer need `Day` so dropping the feature.\n","36e36cf3":"##### We can see super collinearity among few features, let's check for the correlation with the target variable","cd0579df":"##### Finally the target variable","70106926":"In most of the cases, outliers are only extreme values and removing them can be seen as the most straight forward method however, it is not always the best approach. \n\nInitially we will be checking our features and be treating them all together.","b92f69d0":"#### Distribution of Features that have low unique values","66566d9f":"We will choose the median value(1400) as the threshold to cut for the following reasons\n- The value looks high enough to be considered popular.\n- It will evenly distribute the datapoints between both the classes.\n- This was the threshold set in the paper and it will help us perform better comparision.","3aa6d1b4":"## Loading the Data","36eb26fb":"##### No multicollinearity: There are no features which are correlated with each other. (Except weekend and saturday\/sunday)\n\nChecking the importance of features with the target variable.","aced38bc":"##### Here we will observe features with similar range together and rest individually for better analysis","a425820d":"##### As per our, business objective it is a classification problem so, we will create a \"label\" feature based on the target column - shares. But where to fix the threshold?\n","8281e7d9":"### Outlier checks","540b5faa":"## Dependecy check","e0a1b317":"We got lucky here \ud83c\udf40"}}