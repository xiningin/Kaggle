{"cell_type":{"8004e052":"code","c0a17562":"code","6713aab3":"code","12b593b4":"code","a2a5f25f":"code","45319392":"code","c91aec4e":"code","ddb8136c":"code","faefdeaf":"code","071590ae":"code","67c1cf97":"code","13c9630c":"code","f0fbe5a1":"code","26a62e08":"code","64877ddf":"code","00a5b94a":"code","2e0aa384":"code","f23c67ce":"code","5f3a2bd7":"code","d0544f4f":"code","46b638d5":"code","4f30678a":"code","80079452":"code","c559140f":"code","e0adf331":"code","dae5b3a4":"code","f9cfbbc0":"code","ac83d6ea":"code","6ae0d0c6":"code","235d9ba7":"markdown","91d05b30":"markdown","bb50b587":"markdown","d6b838dd":"markdown","ecd8aa3f":"markdown","6ed26515":"markdown","3a596465":"markdown","6a13db19":"markdown","b657f8e6":"markdown","2b59a8a4":"markdown","56f5b39d":"markdown","d609ac97":"markdown","ecf07fbf":"markdown","fb1494d0":"markdown","dcf3c49e":"markdown","ba9f414a":"markdown"},"source":{"8004e052":"import numpy as np\nimport pandas as pd\nimport string\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport matplotlib as plt\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image","c0a17562":"message_data = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\",encoding = \"latin\")\nmessage_data.head()","6713aab3":"message_data = message_data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1)","12b593b4":"message_data = message_data.rename(columns = {'v1':'Spam\/Not_Spam','v2':'message'})","a2a5f25f":"message_data.info()","45319392":"message_data.groupby('Spam\/Not_Spam').describe()","c91aec4e":"message_data_copy = message_data['message'].copy()","ddb8136c":"def text_preprocess(text):\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    return \" \".join(text)","faefdeaf":"message_data_copy = message_data_copy.apply(text_preprocess)","071590ae":"message_data_copy","67c1cf97":"vectorizer = TfidfVectorizer(\"english\")","13c9630c":"message_mat = vectorizer.fit_transform(message_data_copy)\nmessage_mat","f0fbe5a1":"message_train, message_test, spam_nospam_train, spam_nospam_test = train_test_split(message_mat, \nmessage_data['Spam\/Not_Spam'], test_size=0.3, random_state=20)","26a62e08":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nSpam_model = LogisticRegression(solver='liblinear', penalty='l1')\nSpam_model.fit(message_train, spam_nospam_train)\npred = Spam_model.predict(message_test)\naccuracy_score(spam_nospam_test,pred)","64877ddf":"def stemmer (text):\n    text = text.split()\n    words = \"\"\n    for i in text:\n            stemmer = SnowballStemmer(\"english\")\n            words += (stemmer.stem(i))+\" \"\n    return words","00a5b94a":"message_data_copy = message_data_copy.apply(stemmer)\nvectorizer = TfidfVectorizer(\"english\")\nmessage_mat = vectorizer.fit_transform(message_data_copy)","2e0aa384":"message_train, message_test, spam_nospam_train, spam_nospam_test = train_test_split(message_mat, \n                                                        message_data['Spam\/Not_Spam'], test_size=0.3, random_state=20)","f23c67ce":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nSpam_model = LogisticRegression(solver='liblinear', penalty='l1')\nSpam_model.fit(message_train, spam_nospam_train)\npred = Spam_model.predict(message_test)\naccuracy_score(spam_nospam_test,pred)","5f3a2bd7":"message_data['length'] = message_data['message'].apply(len)\nmessage_data.head()","d0544f4f":"length = message_data['length'].to_numpy()\nnew_mat = np.hstack((message_mat.todense(),length[:, None]))","46b638d5":"message_train, message_test, spam_nospam_train, spam_nospam_test = train_test_split(new_mat, \n                                                        message_data['Spam\/Not_Spam'], test_size=0.3, random_state=20)","4f30678a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nSpam_model = LogisticRegression(solver='liblinear', penalty='l1')\nSpam_model.fit(message_train, spam_nospam_train)\npred = Spam_model.predict(message_test)\naccuracy_score(spam_nospam_test,pred)","80079452":"import matplotlib.pyplot as plt\nimport csv\n\nx = ['spam','ham']\ny = []\nfile = '\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv'\nwith open(file,'r',encoding = \"ISO-8859-1\") as csvfile:\n    plots = csv.reader(csvfile, delimiter=',')\n    s= 0\n    h = 0\n\n    for row in plots:\n        if row[0] == 'spam':\n            s += 1\n        elif row[0] == 'ham':\n            h += 1\n    y = [s,h]\nplt.bar(x, y, color='g', width=0.72, label=\"Mails\")\nplt.xlabel('Label')\nplt.ylabel('Mails')\nplt.legend()\nplt.show()\n","c559140f":"plt.pie(y,labels =x,autopct='%1.0f%%')\nplt.title('Classified Mails')\nplt.show()","e0adf331":"plt.figure(figsize=(12, 8))\n\nmessage_data[message_data['Spam\/Not_Spam']=='ham'].length.plot(bins=35, kind='hist', color='blue', \n                                       label='Ham messages', alpha=0.6)\nmessage_data[message_data['Spam\/Not_Spam']=='spam'].length.plot(kind='hist', color='red', \n                                       label='Spam messages', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Message Length\")","dae5b3a4":"ham_df = message_data[message_data['Spam\/Not_Spam'] == 'ham']['length'].value_counts().sort_index()\nspam_df = message_data[message_data['Spam\/Not_Spam'] == 'spam']['length'].value_counts().sort_index()\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(\n    x=ham_df.index,\n    y=ham_df.values,\n    name='ham',\n    fill='tozeroy',\n))\nfig.add_trace(go.Scatter(\n    x=spam_df.index,\n    y=spam_df.values,\n    name='spam',\n    fill='tozeroy',\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Ham\/Spam Classified<\/span>'\n)\nfig.update_xaxes(range=[0, 70])\nfig.show()","f9cfbbc0":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ac83d6ea":"c_mask = np.array(Image.open('\/kaggle\/input\/mask-images\/comment_image.png'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=c_mask,\n)\nwc.generate(' '.join(text for text in message_data.loc[message_data['Spam\/Not_Spam'] == 'ham', 'message']))\nplt.figure(figsize=(18,10))\nplt.title('Top words for HAM messages', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","6ae0d0c6":"c_mask = np.array(Image.open('\/kaggle\/input\/mask-images\/comment_image.png'))\n\nwc = WordCloud(\n    background_color='white', \n    max_words=200, \n    mask=c_mask,\n)\nwc.generate(' '.join(text for text in message_data.loc[message_data['Spam\/Not_Spam'] == 'spam', 'message']))\nplt.figure(figsize=(18,10))\nplt.title('Top words for SPAM messages', \n          fontdict={'size': 22,  'verticalalignment': 'bottom'})\nplt.imshow(wc)\nplt.axis(\"off\")\nplt.show()","235d9ba7":"![image.png](attachment:e29da761-6007-426d-9e57-4d3b6b825a33.png)","91d05b30":"# Spam\/Ham Classifer Model \n\nEach and every individual faces the problem of missing crucial e-mails because of the various unwanted emails recieved daily due to signing up with various websites for some or the other purposes.\n\nThis websites then keeping on sending us various updates and any change in their system through emails. So, at times it might happen that we miss our any important mails which need to be addressed as soon as possible.\n\nSo to overcome this in most better way,this model is designed to create filter of ham\/spam mails.","bb50b587":"# 5. Method 3 - Using Message Length\n<a id=\"fifth\"><\/a>","d6b838dd":"# 7. Word Clouds\n<a id=\"seventh\"><\/a>\n\nData visualizations (like charts, graphs, infographics, and more) give businesses a valuable way to communicate important information at a glance, but what if your raw data is text-based? If you want a stunning visualization format to highlight important textual data points, using a word cloud can make dull data sizzle and immediately convey crucial information.\n\nWord clouds (also known as text clouds or tag clouds) work in a simple way: the more a specific word appears in a source of textual data (such as a speech, blog post, or database), the bigger and bolder it appears in the word cloud.\n\nA word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it\u2019s mentioned within a given text and the more important it is.","ecd8aa3f":"# 6. Visualisations\n<a id=\"sixth\"><\/a>","6ed26515":"# 3. Method 1 - Accuracy using Logistic Regression by just removing puntuations\n<a id=\"third\"><\/a>","3a596465":"# 2. Data Cleaning\n<a id=\"second\"><\/a>","6a13db19":"# Algorithm Used :\n\n## Logistic Regression:\n\nLogistic regression is a supervised learning classification algorithm used to predict the probability of a target variable. The nature of target or dependent variable is dichotomous, which means there would be only two possible classes.\n\nIn simple words, the dependent variable is binary in nature having data coded as either 1 (stands for success\/yes) or 0 (stands for failure\/no).\n\nMathematically, a logistic regression model predicts P(Y=1) as a function of X. It is one of the simplest ML algorithms that can be used for various classification problems such as spam detection, Diabetes prediction, cancer detection etc.\n\n![image.png](attachment:7f57420c-1c0d-4c19-9b48-efbcaba2e746.png)","b657f8e6":"## B. Pie chart for Percentage of spam and ham messages","2b59a8a4":"## A. Bar plot for number of spam and ham messages","56f5b39d":"# 1. Importing Libraries and reading the data\n<a id=\"first\"><\/a>","d609ac97":"## Table of contents\n\n1. [Importing Libraries and reading the data](#first)\n2. [Data Cleaning](#second)\n3. [Method 1 - Accuracy using Logistic Regression by just removing puntuations](#third)\n4. [Method 2 - Snowball Stemmer](#fourth)\n5. [Method 3 - Using Message Length](#fifth)\n6. [Visualisations](#sixth)\n7. [Word Cloud](#seventh)","ecf07fbf":"## C. Hist plot for ham and spam messages","fb1494d0":"# 4. Method 2 - Sonwball Stemmer\n<a id=\"fourth\"><\/a>","dcf3c49e":"## Vectorization: \n\nUse of vectorizer to fit the data. Fitting basicall means bringing the data into the format in a way its easy for further predictions","ba9f414a":"## D. Line plot"}}