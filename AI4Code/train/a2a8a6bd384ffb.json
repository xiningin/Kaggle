{"cell_type":{"d5500a42":"code","11722322":"code","03d362fe":"code","fbabd27d":"code","3b741b17":"code","42304ec1":"code","9bf452f1":"code","4c403aaa":"code","8576d79b":"code","0323a607":"code","daf3ff3a":"code","11f4c348":"code","f1be7292":"code","df1a40c9":"code","eddb9097":"code","8542c46f":"code","bc1c799c":"markdown","9eedd253":"markdown","4308568a":"markdown","586336e4":"markdown","1f773203":"markdown","85b38cf9":"markdown"},"source":{"d5500a42":"import numpy as np","11722322":"def linear_neuron(x,weight):\n        prediction = (x*weight)\n        return prediction","03d362fe":"weight = 0.2\nx = [1.0,2.0,3.0] #first feature with three samples\nlinear_neuron(x[0],weight) #sending the first sample as input","fbabd27d":"def with_dot(x,weight):\n    x = np.array(x)\n    weight = np.array(weight)\n    prediction = x.dot(weight)\n    return prediction","3b741b17":"def for_three(x,weight):\n    result = [0.0,0.0,0.0]\n    for i in range(len(x)):\n        result[i] = linear_neuron(x[i],weight)\n    return result","42304ec1":"def w_sum(a,b):\n    assert(len(a) == len(b))\n    output = 0\n    for i in range(len(a)):\n        output += (a[i] * b[i])\n    return output\n\nweights = [0.1, 0.2, 0] \n    \ndef neural_network(input, weights):\n    pred = w_sum(input,weights)\n    return pred\n\n# This dataset is the current\n# status at the beginning of\n# each game for the first 4 games\n# in a season.\n\n# toes = current number of toes\n# wlrec = current games won (percent)\n# nfans = fan count (in millions)\n\ntoes =  [8.5, 9.5, 9.9, 9.0]\nwlrec = [0.65, 0.8, 0.8, 0.9]\nnfans = [1.2, 1.3, 0.5, 1.0]\n\n# Input corresponds to every entry\n# for the first game of the season.\n\ninput = [toes[0],wlrec[0],nfans[0]]\npred = neural_network(input,weights)\n\nprint(pred)","9bf452f1":"#Using Numpy\n\nimport numpy as np\nweights = np.array([0.1, 0.2, 0])\ndef neural_network(input, weights):\n    pred = input.dot(weights)\n    return pred\n    \ntoes =  np.array([8.5, 9.5, 9.9, 9.0])\nwlrec = np.array([0.65, 0.8, 0.8, 0.9])\nnfans = np.array([1.2, 1.3, 0.5, 1.0])\n\n# Input corresponds to every entry\n# for the first game of the season.\n\ninput = np.array([toes[0],wlrec[0],nfans[0]])\npred = neural_network(input,weights)\n\nprint(pred)","4c403aaa":"def ele_mul(number,vector):\n    output = [0,0,0]\n    assert(len(output) == len(vector))\n    for i in range(len(vector)):\n        output[i] = number * vector[i]\n    return output\n\nweights = [0.3, 0.2, 0.9] \n\ndef neural_network(input, weights):\n    pred = ele_mul(input,weights)\n    return pred\n    \nwlrec = [0.65, 0.8, 0.8, 0.9]\ninput = wlrec[0]\npred = neural_network(input,weights)\n\nprint(pred)","8576d79b":"weights = [ [0.1, 0.1, -0.3], \n            [0.1, 0.2, 0.0], \n            [0.0, 1.3, 0.1] ] \n\ndef w_sum(a,b):\n    assert(len(a) == len(b))\n    output = 0\n    for i in range(len(a)):\n        output += (a[i] * b[i])\n    return output\n\ndef vect_mat_mul(vect,matrix):\n    assert(len(vect) == len(matrix))\n    output = [0,0,0]\n    for i in range(len(vect)):\n        output[i] = w_sum(vect,matrix[i])\n    return output\n\ndef neural_network(input, weights):\n    pred = vect_mat_mul(input,weights)\n    return pred\n\nt =  [8.5, 9.5, 9.9, 9.0]\nw = [0.65,0.8, 0.8, 0.9]\nn = [1.2, 1.3, 0.5, 1.0]\n\ninput = [t[0],w[0],n[0]]\npred = neural_network(input,weights)\n\nprint(pred)","0323a607":"from random import choice \nfrom numpy import array, dot, random \nunit_step = lambda x: 0 if x < 0 else 1 \ntraining_data = [(array([0,0,1]), 0), \n                 (array([0,1,1]), 1), \n                 (array([1,0,1]), 1), \n                 (array([1,1,1]), 1), ] \nw = random.rand(3)\nprint(w)\nerrors = [] \n \nn = 50 \nfor i in range(n): \n    x, expected = choice(training_data) \n    result = dot(w, x) \n    error = expected - unit_step(result) \n    #print(error)\n    errors.append(error)\n    w +=  error * x \nfor x, _ in training_data: \n    result = dot(x, w) \n    print(\"{}: {} -> {}\".format(x[:2], result, unit_step(result)))","daf3ff3a":"#How are error and weight varying?","11f4c348":"#what is bias and why do we need it? ","f1be7292":"# try for the data \"and\"\n\"\"\"\nx1 x2 y\n0  0  0\n1  0  0\n0  1  0\n1  1  1\n\"\"\"\n\n#try for the data \"xor\"\n\"\"\"\nx1 x2 y\n0  0  0\n1  0  1\n0  1  1\n1  1  0\n\"\"\"","df1a40c9":"# Adaline Gradient Descent\n\n# The ADAptive LInear NEuron (Adaline) is similar to the Perceptron, \n# except that it defines a cost function based on the soft output and an optimization problem.\n# We can therefore leverage various optimization techniques to train Adaline in a more theoretic grounded manner. \n# Let's implement the Adaline using the batch gradient descent (GD) algorithm:\n\nimport numpy as np\n\nclass AdalineGD(object):\n\n    def __init__(self, eta=0.01, epochs=50):\n        self.eta = eta\n        self.epochs = epochs\n\n    def train(self, X, y):\n\n        self.w_ = np.zeros(1 + X.shape[1])\n        self.cost_ = []\n\n        for i in range(self.epochs):\n            output = self.net_input(X)\n            errors = (y - output)\n            self.w_[1:] += self.eta * X.T.dot(errors)\n            self.w_[0] += self.eta * errors.sum()\n            cost = (errors**2).sum() \/ 2.0\n            self.cost_.append(cost)\n        return self\n\n    def net_input(self, X):\n        return np.dot(X, self.w_[1:]) + self.w_[0]\n\n    def activation(self, X):\n        return self.net_input(X)\n\n    def predict(self, X):\n        return np.where(self.activation(X) >= 0.0, 1, -1)","eddb9097":"X = np.array([[1,0],[1,1],[0,1],[0,0]])\ny = np.array([1,1,1,0])\nada = AdalineGD()\nada.train(X, y)","8542c46f":"ada.predict([0,0])","bc1c799c":"\n# Perceptron\n\n<img src=\"https:\/\/cdn-images-1.medium.com\/max\/1600\/1*n6sJ4yZQzwKL9wnF5wnVNg.png\" width=\"400px\">","9eedd253":"# Problems with Perceptrons \n\nconvergence is one of the biggest problems of the perceptron. Frank Rosenblatt proofed mathematically that the perceptron learning rule converges if the two classes can be separated by linear hyperplane, but problems arise if the classes cannot be separated perfectly by a linear classifier.\n\nsince one or more samples will always be misclassified in every epoch so that the learning rule never stops updating the weights.\n\nAlso, it stops updating the weights as soon as all samples are classified correctly. Which doesn't ensure the it has generalized well enough.","4308568a":"Check out this [link](https:\/\/sebastianraschka.com\/Articles\/2015_singlelayer_neurons.html#gradient-descent) for Gradient Descent ","586336e4":"__What does a neuron do ?__\n\nIt calculates the weighted sum and adds bias to it and decides whether to activate it or not. if we see the equation of a neuron it can be written as \n                                                                                           \n                                             Y = sum(weight * input) + bias\n    \nvalue of y can be ranging from -inf to +inf and with this range we do not know how to decide when to activate the neuron. Activation functions help us to decide whether to activate or not by mapping the value in that range to various ranges and decide accordingly","1f773203":"# Multiple Outputs","85b38cf9":"# Multiple Inputs"}}