{"cell_type":{"865a473a":"code","178e02b5":"code","4630d03f":"code","55927fba":"code","41c4653a":"code","690e6c2b":"code","ee41216b":"code","0bf0765f":"code","83ba580a":"code","cf27b99d":"code","f4de0dcb":"code","4c50a033":"code","539efd63":"code","f838c0bd":"code","f88a9e88":"code","673fbbea":"code","3b4fc580":"code","3062d144":"code","48214285":"code","017244a5":"code","025e80c0":"code","ab833d83":"code","ec332d5e":"code","33dd4b53":"code","59cf6c18":"code","504fa5b2":"code","b9584257":"code","e53b3d5e":"code","1d2e929a":"code","98df0a6a":"code","027880b8":"code","a4685603":"code","562d06f3":"code","dd77e59f":"code","3b83d0ee":"code","08bfad12":"code","a64bf23b":"code","6c83502a":"code","79fdeaa8":"code","4dc069bd":"code","f426781b":"code","4a02999f":"code","94be3294":"code","65bb0cad":"code","be31d950":"code","cdc99ce3":"code","9c47b4c5":"code","fdff30ef":"code","8d14a549":"code","292fa3ae":"code","1bee37a5":"code","03f5e493":"code","8b4fa22b":"code","58e7acab":"code","c40a29e4":"code","c3ea4be5":"code","6045810e":"code","b54dfa8a":"code","356f179f":"code","5a03235f":"code","60f534ed":"code","0904a7af":"code","e492f7f8":"code","790feeff":"code","605c1bbc":"markdown","c104f19a":"markdown","19665c04":"markdown","c4ec1dfa":"markdown","df740e78":"markdown","a44f7631":"markdown","996021c4":"markdown"},"source":{"865a473a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import NMF  \nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import mean_absolute_error,silhouette_score\n\nfrom gensim.parsing.preprocessing import remove_stopwords\nfrom wordcloud import WordCloud\nfrom nltk.sentiment import vader\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.tag import pos_tag\nfrom nltk.chunk import ne_chunk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\n#nltk.download('punkt')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('maxent_ne_chunker')\n#nltk.download('words')\n#nltk.download('stopwords')\n\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\nfrom plotly.offline import iplot\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 2000)\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","178e02b5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4630d03f":"df = pd.read_csv('\/kaggle\/input\/donald-trump-political-rallies-transcripts\/Trump_Rallies_Dataset.csv',parse_dates=['Date'])","55927fba":"df = df.drop('Unnamed: 0',axis=1)\ndf = df.drop(97, axis=0)\ndf = df.reset_index().drop('index',axis=1)","41c4653a":"df['Place'] = df['Place'].apply(lambda x: re.sub(r'^.*(?=Rally)','',x))","690e6c2b":"df['Transcript'] = df['Transcript'].apply(lambda x: re.sub(r'\\[.*?\\]','',x))","ee41216b":"df['Transcript'] = df['Transcript'].apply(lambda x: x.lower())","0bf0765f":"tokenizer = RegexpTokenizer('[a-z][a-z]+[a-z]')","83ba580a":"df['Transcript'] = df['Transcript'].apply(lambda x: tokenizer.tokenize(x))","cf27b99d":"df['Transcript'] = df['Transcript'].apply(lambda x: ' '.join(x))","f4de0dcb":"df['Transcript'] = df['Transcript'].apply(lambda x: x.replace('\\\\',''))","4c50a033":"# def nltk_tag_to_wordnet_tag(nltk_tag):\n#     if nltk_tag.startswith('J'):\n#         return wordnet.ADJ\n#     elif nltk_tag.startswith('V'):\n#         return wordnet.VERB\n#     elif nltk_tag.startswith('N'):\n#         return wordnet.NOUN\n#     elif nltk_tag.startswith('R'):\n#         return wordnet.ADV\n#     else:          \n#         return None","539efd63":"# def lemmatize_sentence(sentence):\n#     #tokenize the sentence and find the POS tag for each token\n#     tokenizer = RegexpTokenizer('[a-z][a-z]+[a-z]')\n#     nltk_tagged = nltk.pos_tag(tokenizer.tokenize(sentence))  \n#     #tuple of (token, wordnet_tag)\n#     wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n#     lemmatized_sentence = []\n#     for word, tag in wordnet_tagged:\n#         if tag is None:\n#             #if there is no available tag, append the token as is\n#             lemmatized_sentence.append(word)\n#         else:        \n#             #else use the tag to lemmatize the token\n#             lemmatized_sentence.append(WordNetLemmatizer().lemmatize(word, tag))\n#     return \" \".join(lemmatized_sentence)","f838c0bd":"#df['Transcript'] = df['Transcript'].apply(lambda x: lemmatize_sentence(x))","f88a9e88":"df['Transcript'] = df['Transcript'].apply(lambda x: remove_stopwords(x))","673fbbea":"df.loc[0,'Transcript']","3b4fc580":"df","3062d144":"wordcloud = WordCloud(max_words=30, background_color='white',colormap='magma',width=800, height=400, random_state=48).generate(df.loc[100,'Transcript'])\nplt.figure(figsize=[10,5])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.savefig('wordcloud1.png',transparent=True, bbox_inches='tight')","48214285":"wordcloud = WordCloud(max_words=30, background_color='white',colormap='magma',width=800, height=400, random_state=71).generate(df.loc[101,'Transcript'])\nplt.figure(figsize=[10,5])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.savefig('wordcloud2.png',transparent=True, bbox_inches='tight')","017244a5":"wordcloud = WordCloud(max_words=30,background_color='white',colormap='seismic',width=800, height=400,random_state=28).generate(df.loc[0,'Transcript'])\nplt.figure(figsize=[10,5])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.savefig('wordcloud3.png',transparent=True, bbox_inches='tight')","025e80c0":"wordcloud = WordCloud(max_words=30, background_color='white',colormap='seismic',width=800, height=400,random_state=48).generate(df.loc[1,'Transcript'])\nplt.figure(figsize=[10,5])\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.savefig('wordcloud4.png',transparent=True, bbox_inches='tight')","ab833d83":"count_vectorizer = CountVectorizer(stop_words = 'english')\ndoc_term = count_vectorizer.fit_transform(list(df['Transcript']))\ncountvec = count_vectorizer.fit(list(df['Transcript']))\ndt_matrix = pd.DataFrame(doc_term.toarray().round(3), index=[i for i in df['Place']], columns=count_vectorizer.get_feature_names()).head(10)\ndt_matrix ","ec332d5e":"nmf_model = NMF(n_components = 3,random_state=42)\nmodel = nmf_model.fit(doc_term)\ndoc_topic = model.transform(doc_term)","33dd4b53":"topic_word = pd.DataFrame(nmf_model.components_.round(3),\n             index = [\"component_1\",\"component_2\",\"component_3\"],\n             columns = count_vectorizer.get_feature_names())\ntopic_word","59cf6c18":"def display_topics(model, feature_names, no_top_words, topic_names=None):\n    for ix, topic in enumerate(model.components_):\n        if not topic_names or not topic_names[ix]:\n            print(\"\\nTopic \", ix)\n        else:\n            print(\"\\nTopic: '\",topic_names[ix],\"'\")\n        print(\", \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))","504fa5b2":"display_topics(nmf_model, count_vectorizer.get_feature_names(), 30)","b9584257":"topic_index = []\nfeature_names = count_vectorizer.get_feature_names()\nfor ix, topic in enumerate(nmf_model.components_):\n    topic_index.append(\" \".join([feature_names[i] for i in topic.argsort()]))","e53b3d5e":"H = pd.DataFrame(doc_topic.round(3),\n             index = [i for i in df['Place']],\n             columns = [\"component_1\",\"component_2\",\"component_3\"])\nH","1d2e929a":"H = H.reset_index()\nH = df.join(H).drop(['Transcript','index'],axis=1)\nH","98df0a6a":"H = H.rename(columns={'component_1':'Achievements','component_2':'Plans and Appealing Support','component_3':'Political Adversaries'})\nH","027880b8":"DF = H.copy()\nDF = DF.drop(['Place','Date'],axis=1)\nDF = DF.iloc[:21]\ncos_similar_matrix = pd.DataFrame(cosine_similarity(DF.values),columns=H['Date'].iloc[:21].astype(str),index=H['Date'].iloc[:21].astype(str))\ncos_similar_matrix","a4685603":"sns.set(style='white',font_scale=1.4)\nfig = plt.figure(figsize=[20,18])\nmask = np.triu(np.ones_like(cos_similar_matrix, dtype=bool))\nsns.heatmap(cos_similar_matrix,cmap='Blues',linewidth=3,linecolor='white',vmax = 1, vmin=0.1,mask=mask, annot=True,fmt='0.2f')\nplt.title('Cosine Similarity Heatmap - Last 20 Rallies', weight='bold',fontsize=25)\nplt.xlabel('')\nplt.ylabel('')\nplt.savefig('heatmap.png',transparent=True, bbox_inches='tight')","562d06f3":"data = H.drop(['Place','Date'],axis=1)\ndata.index = H['Date']\ndata","dd77e59f":"data = data.iloc[::-1]\ndata","3b83d0ee":"train = data[:int(0.90*(len(data)))]\ntrain","08bfad12":"valid = data[int(0.90*(len(data))):int(0.95*(len(data)))]\nvalid","a64bf23b":"test = data[int(0.95*(len(data))):]\ntest","6c83502a":"train_Achievements = train['Achievements']\ntrain_Support = train['Plans and Appealing Support']\ntrain_Adversaries = train['Political Adversaries']\nvalid_Achievements = valid['Achievements']\nvalid_Support = valid['Plans and Appealing Support']\nvalid_Adversaries = valid['Political Adversaries']\ntest_Achievements = test['Achievements']\ntest_Support = test['Plans and Appealing Support']\ntest_Adversaries = test['Political Adversaries']","79fdeaa8":"# mae_vector= []\n\n# for period in np.arange(8,13,1):    \n#     for trend in ['n','c','t','ct']:\n#         for p in [1]:\n#             for d in [0]:\n#                 for q in [0]:\n#                     for P in [1]:\n#                         for D in [1]:\n#                             for Q in [1]:\n#                                 forecast_Adversaries = SARIMAX(train_Adversaries,order=(p,d,q),seasonal_order=(P,D,Q,period),trend=trend).fit().forecast(steps=len(valid_Adversaries))\n#                                 MAE_Adversaries = mean_absolute_error(valid['Political Adversaries'],forecast_Adversaries) \n#                                 mae_vector.append((MAE_Adversaries,[(p,d,q),(P,D,Q,period),trend]))\n#     print(period)\n\n# mae, para = zip(*mae_vector)        \n\n# print(f'Best Parameters is {para[np.argmin(mae)]}')    \n# print(f'Lowest MAE is {min(mae)}') ","4dc069bd":"# mae_vector= []\n\n# for period in np.arange(8,13,1):    \n#     for trend in ['n','c','t','ct']:\n#         for p in [1]:\n#             for d in [0]:\n#                 for q in [0]:\n#                     for P in [1]:\n#                         for D in [1]:\n#                             for Q in [1]:\n#                                 forecast_Support = SARIMAX(train_Support,order=(p,d,q),seasonal_order=(P,D,Q,period),trend=trend).fit().forecast(steps=len(valid_Support))\n#                                 MAE_Support = mean_absolute_error(valid['Plans and Appealing Support'],forecast_Support) \n#                                 mae_vector.append((MAE_Support,[(p,d,q),(P,D,Q,period),trend]))\n#     print(period)\n\n# mae, para = zip(*mae_vector)        \n\n# print(f'Best Parameters is {para[np.argmin(mae)]}')    \n# print(f'Lowest MAE is {min(mae)}') ","f426781b":"# mae_vector= []\n\n# for period in np.arange(8,13,1):    \n#     for trend in ['n','c','t','ct']:\n#         for p in [1]:\n#             for d in [0]:\n#                 for q in [0]:\n#                     for P in [1]:\n#                         for D in [1]:\n#                             for Q in [1]:\n#                                 forecast_Achievements = SARIMAX(train_Achievements,order=(p,d,q),seasonal_order=(P,D,Q,period),trend=trend).fit().forecast(steps=len(valid_Achievements))\n#                                 MAE_Achievements = mean_absolute_error(valid['Achievements'],forecast_Achievements) \n#                                 mae_vector.append((MAE_Achievements,[(p,d,q),(P,D,Q,period),trend]))\n#     print(period)\n\n# mae, para = zip(*mae_vector)        \n\n# print(f'Best Parameters is {para[np.argmin(mae)]}')    \n# print(f'Lowest MAE is {min(mae)}') ","4a02999f":"trainvalid = data[:int(0.95*(len(data)))]\ntrainvalid","94be3294":"trainvalid_Achievements = trainvalid['Achievements']\ntrainvalid_Support = trainvalid['Plans and Appealing Support']\ntrainvalid_Adversaries = trainvalid['Political Adversaries']","65bb0cad":"forecast_Adversaries = SARIMAX(trainvalid_Adversaries,order=(1,0,0),seasonal_order=(1,1,1,10),trend='ct').fit().forecast(steps=len(test_Adversaries))\nMAE_Adversaries = mean_absolute_error(test['Political Adversaries'],forecast_Adversaries)\nprint(f'MAE_Adversaries: {MAE_Adversaries}')\n    \nforecast_Support = SARIMAX(trainvalid_Support,order=(1,0,0),seasonal_order=(1,1,1,8),trend='ct').fit().forecast(steps=len(test_Support))\nMAE_Support = mean_absolute_error(test['Plans and Appealing Support'],forecast_Support) \nprint(f'MAE_Support: {MAE_Support}')\n    \nforecast_Achievements = SARIMAX(trainvalid_Achievements,order=(1,0,0),seasonal_order=(1,1,1,9),trend='n').fit().forecast(steps=len(test_Achievements))\nMAE_Achievements = mean_absolute_error(test['Achievements'],forecast_Achievements)  \nprint(f'MAE_Achievements: {MAE_Achievements}')","be31d950":"predicted = test.copy()\npredicted['Achievements'] = list(forecast_Achievements)\npredicted['Plans and Appealing Support'] = list(forecast_Support)\npredicted['Political Adversaries'] = list(forecast_Adversaries)\npredicted","cdc99ce3":"test","9c47b4c5":"predicted2 = predicted.copy()\npredicted2 = predicted2.reset_index()\npredicted2['Date'] = predicted2['Date'].astype(str)","fdff30ef":"H2 = H.copy()\nH2['Date'] = H2['Date'].astype(str)","8d14a549":"H3 = H2.copy()\nH3 = H3.iloc[::-1]","292fa3ae":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x='Date',y='Political Adversaries',data=H2,style=True,markers=True,ci=None,color='r')\nplt.xticks(list(H2['Date'])[0::20])\n#ymin, ymax = plt.ylim()\n#plt.vlines(list(H2['Date'])[3::10], ymin, ymax, linestyle='dashed')\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Attacking Political Adversaries',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot1_0.png',transparent=True, bbox_inches='tight')","1bee37a5":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x=H2.index[::-1],y=H2['Political Adversaries'],style=True,markers=True,ci=None,color='r')\nlst = ['']*102\nlst[-4::-10] = list(H2['Date'][3::10])\nplt.xticks(ticks=range(1,103),labels=lst)\nymin, ymax = plt.ylim()\nplt.vlines(np.arange(8,103,10), ymin, ymax, linestyle='dashed')\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Attacking Political Adversaries',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot1.png',transparent=True, bbox_inches='tight')","03f5e493":"Range = np.arange(8,103,10)","8b4fa22b":"sns.set(style='white',font_scale=1.2,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[9,6])\n#sns.lineplot(x='Date',y='Political Adversaries',data=H2,style=True,markers=True,ci=None,color='r')\nplt.plot(H3['Date'],H3['Political Adversaries'],marker='.',markersize=10,linewidth=1.5,linestyle='-',color='r',label='Actual')\nplt.plot(predicted2['Date'],predicted2['Political Adversaries'],marker='.',markersize=10,linewidth=2,linestyle='--',color='k',label='Predicted')\nplt.xticks(list(H2['Date'])[3::10])\nymin, ymax = plt.ylim()\nplt.vlines(list(H2['Date'])[3::10], ymin, ymax, linestyle=':')\nplt.xlim(['2019-11-06','2020-10-12'])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=15)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=15)\nplt.title('Trump Attacks Political Adversaries',weight='bold',fontsize=15)\nplt.legend()\nsns.despine()\nplt.savefig('PLTlineplot1.png',transparent=True, bbox_inches='tight')","58e7acab":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x='Date',y='Plans and Appealing Support',data=H2,style=True,markers=True,ci=None,color='g')\nplt.xticks(list(H2['Date'])[0::20])\nplt.ylim([0,10])\n#ymin, ymax = plt.ylim()\n#plt.vlines(list(H2['Date'])[5::8], ymin, ymax, linestyle='dashed')\nplt.yticks([0,2,4,6,8,10])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Talks About Plans and Appeals for Support',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot2_0.png',transparent=True, bbox_inches='tight')","c40a29e4":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x=H2.index[::-1],y=H2['Plans and Appealing Support'],style=True,markers=True,ci=None,color='g')\nlst = ['']*102\nlst[-6::-8] = list(H2['Date'][5::8])\nplt.xticks(ticks=range(1,103),labels=lst)\nplt.ylim([0,10])\nymin, ymax = plt.ylim()\nplt.vlines(np.arange(0,103,8), ymin, ymax, linestyle='dashed')\nplt.yticks([0,2,4,6,8,10])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Talks About Plans and Appeals for Support',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot2.png',transparent=True, bbox_inches='tight')","c3ea4be5":"sns.set(style='white',font_scale=1.2,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[9,6])\n#sns.lineplot(x='Date',y='Political Adversaries',data=H2,style=True,markers=True,ci=None,color='r')\nplt.plot(H3['Date'],H3['Plans and Appealing Support'],marker='.',markersize=10,linewidth=1.5,linestyle='-',color='g',label='Actual')\nplt.plot(predicted2['Date'],predicted2['Plans and Appealing Support'],marker='.',markersize=10,linewidth=2,linestyle='--',color='k',label='Predicted')\nplt.xticks(list(H2['Date'])[5::8])\nplt.ylim([0,10])\nymin, ymax = plt.ylim()\nplt.vlines(list(H2['Date'])[5::8], ymin, ymax, linestyle=':')\nplt.xlim(['2019-11-06','2020-10-12'])\nplt.yticks([0,2,4,6,8,10])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=15)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=15)\nplt.title('Trump Talks About Plans, Appeals for Support',weight='bold',fontsize=15)\nplt.legend()\nsns.despine()\nplt.savefig('PLTlineplot2.png',transparent=True, bbox_inches='tight')","6045810e":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x='Date',y='Achievements',data=H2,style=True,markers=True,ci=None)\nplt.xticks(list(H2['Date'])[0::20])\n#ymin, ymax = plt.ylim()\n#plt.vlines(list(H2['Date'])[8::9], ymin, ymax, linestyle='dashed')\nplt.ylabel('Achievements and Bragging')\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Brags about Achievement and Progress',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot3_0.png',transparent=True, bbox_inches='tight')","b54dfa8a":"sns.set(style='white',font_scale=5,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[80,20])\nax = sns.lineplot(x=H2.index[::-1],y=H2['Achievements'],style=True,markers=True,ci=None)\nlst = ['']*102\nlst[-9::-9] = list(H2['Date'][8::9])\nplt.xticks(ticks=range(1,103),labels=lst)\nymin, ymax = plt.ylim()\nplt.vlines(np.arange(3,102,9), ymin, ymax, linestyle='dashed')\nplt.ylabel('Achievements and Bragging')\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=60)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=60)\nplt.title('Trump Brags about Achievement and Progress',weight='bold',fontsize=80)\nsns.despine()\nplt.savefig('SNSlineplot3.png',transparent=True, bbox_inches='tight')","356f179f":"sns.set(style='white',font_scale=1.2,rc = {'lines.markersize': 15} )\nfig = plt.figure(figsize=[9,6])\n#sns.lineplot(x='Date',y='Political Adversaries',data=H2,style=True,markers=True,ci=None,color='r')\nplt.plot(H3['Date'],H3['Achievements'],marker='.',markersize=10,linewidth=1.5,linestyle='-',color='b',label='Actual')\nplt.plot(predicted2['Date'],predicted2['Achievements'],marker='.',markersize=10,linewidth=2,linestyle='--',color='k',label='Predicted')\nplt.xticks(list(H2['Date'])[8::9])\nymin, ymax = plt.ylim()\nplt.vlines(list(H2['Date'])[8::9], ymin, ymax, linestyle=':',)\nplt.xlim(['2019-11-06','2020-10-12'])\nplt.xlabel('Dates of Rallies',weight='bold',fontsize=15)\nplt.ylabel('Topic Relevance in Rally',weight='bold',fontsize=15)\nplt.title('Trump Brags about Achievement and Progress',weight='bold',fontsize=15)\nplt.legend()\nsns.despine()\nplt.savefig('PLTlineplot3.png',transparent=True, bbox_inches='tight')","5a03235f":"sns.set(style='white',font_scale=1)\ninertia = []\nfor num_clusters in range(1,11):\n    km = KMeans(n_clusters=num_clusters,random_state=71)\n    km.fit(H[['Achievements','Plans and Appealing Support','Political Adversaries']])\n    inertia.append(km.inertia_)\n    \nplt.plot(range(1,11),inertia,marker='x')\nplt.ylabel('Sum Inertia',fontsize=15,weight='bold')\nplt.xlabel('No. of Clusters',fontsize=15,weight='bold')\nsns.despine()\nplt.savefig('Elbow.png',transparent=True, bbox_inches='tight')","60f534ed":"sns.set(style='white',font_scale=1)\nsilhouette = []\nfor num_clusters in range(2,11):\n    km = KMeans(n_clusters=num_clusters,random_state=71)\n    km.fit(H[['Achievements','Plans and Appealing Support','Political Adversaries']])\n    labels = km.labels_\n    silhouette.append(silhouette_score(H[['Achievements','Plans and Appealing Support','Political Adversaries']],labels=labels))\n    \nplt.plot(range(2,11),silhouette,marker='x',color='g')\nplt.ylabel('Silhouette Coefficient',fontsize=15,weight='bold')\nplt.xlabel('No. of Clusters',fontsize=15,weight='bold')\nsns.despine()\nplt.savefig('Silhouette.png',transparent=True, bbox_inches='tight')","0904a7af":"km = KMeans(n_clusters=2,random_state=71)\nkm.fit(H[['Achievements','Plans and Appealing Support','Political Adversaries']])\nlabels = km.labels_\nlabels","e492f7f8":"labels = np.where(labels==0,'Trump Mainly Attacking','Trump Mainly Bragging')\n","790feeff":"data1 = []\nclusters = []\ncolors = ['rgb(228,26,28)','rgb(55,126,184)'] # set our dot colors\n\nfor i in range(len(np.unique(labels))): # allows us to split our data into three distinct groups\n    name = np.unique(labels)[i]\n    color = colors[i]\n    x = H[ labels == name ]['Achievements']\n    y = H[ labels == name  ]['Plans and Appealing Support']\n    z = H[ labels == name  ]['Political Adversaries']\n    \n    trace = dict(  # trace is how we \"trace\" or draw our data on the canvas\n        name = name,\n        x = x, y = y, z = z,\n        type = \"scatter3d\",    \n        mode = 'markers',\n        marker = dict( size=2, color=color, line=dict(width=0) ) )\n    data1.append( trace )\n\nlayout = dict( # we modify our canvas here, including initial layout and styles\n    width=800,\n    height=550,\n    autosize=True,\n    title='Trump Rally Topic Clusters',\n    scene=dict(\n        xaxis=dict(\n            gridcolor='rgb(255, 255, 255)',\n            zerolinecolor='rgb(255, 255, 255)',\n            showbackground=True,\n            backgroundcolor='rgb(230, 230,230)',\n            title='Achievements and Bragging',  # set titles, very important\n            titlefont=dict(\n            family='Courier New',\n            size=9,\n            color='#2f2f2f'),  # we can use hex, rgba, or other color variants\n        ),\n        yaxis=dict(\n            gridcolor='rgb(255, 255, 255)',\n            zerolinecolor='rgb(255, 255, 255)',\n            showbackground=True,\n            backgroundcolor='rgb(230, 230,230)',\n            title='Plans and Appealing Support',  # set titles, very important\n            titlefont=dict(\n            family='Courier New',\n            size=9,\n            color='#4f4f4f'),\n        ),\n        zaxis=dict(\n            gridcolor='rgb(255, 255, 255)',\n            zerolinecolor='rgb(255, 255, 255)',\n            showbackground=True,\n            backgroundcolor='rgb(230, 230,230)',\n            title='Political Adversaries',  # set titles, very important\n            titlefont=dict(\n            family='Courier New',\n            size=9,\n            color='#7f7f7f'),\n        ),\n        aspectratio = dict( x=1, y=1, z=1 ), # we can compress large dimensions this way\n        aspectmode = 'manual'        \n    ),\n)\n\nfig = dict(data=data1, layout=layout) # this finally compiles our figure\n\n# run locally in notebook\niplot(fig)","605c1bbc":"## Exploratory Data Analysis\n- Using WordCloud on first 2 and last 2 rallies","c104f19a":"### Try NMF with Countvectorizer (Random = 42)","19665c04":"## Clustering of Trump Rallies\n- K-means for Clustering\n- Used Elbow method and Silhouette coefficients to determine optimum clusters\n- Used 3D plot in Plotly to visualize clusters","c4ec1dfa":"## Text Pre-processing\n- Regular Expression\n- Tokenization using NLTK Regex Tokenizer\n- Stopwords removal using Gensim","df740e78":"## Topic Modeling and Exploratory Data Analysis with Heatmap\n- Scikit-learn Count Vectorizer\n- Scikit-learn Non-negative Matrix Factorization (NMF)\n- Scikit-learn Cosine Similarity","a44f7631":"## Time Series with SARIMA\n- Create Simple Validation manually\n- Observe seasonality in topics in Trump rallies\n- Time Series is observed to be non-stationary with trend and seasonality => SARIMA\n- Manually optimize the period and trend parameter in SARIMA\n- MAE is used as a metric for prediction","996021c4":"# Strategic Analysis of Trump Rallies with NLP\n\n- Initially obtained dataset of 35 rallies from Kaggle but they were found to be too few for time series.\n- Web scrapped about 100 Trump rallies from https:\/\/factba.se\/ from first rally in 2017 to the first rally in 2020 when Trump recovered from Covid-19.\n- Planned to do topic modelling on all Trump rallies, and develop a time series analysis of the rallies. Where possible, I plan to make time series predictions of his trending topics.\n- Also possible to do clustering of Trump rallies based on topics."}}