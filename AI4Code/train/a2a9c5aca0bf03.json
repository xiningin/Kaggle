{"cell_type":{"c0c5840d":"code","a6f0a1a6":"code","2e394ceb":"code","2807c691":"code","5899458f":"code","33d703e7":"code","f6bb808a":"code","8e61c836":"code","01eeadc6":"code","d921f032":"code","a68880b8":"code","99d25643":"code","7e59e1a6":"code","0771821c":"code","fd9be8dc":"code","21177f63":"code","15c3fccb":"code","8b4ba220":"code","995eb3a2":"code","f54077aa":"code","afc51490":"code","c4d91f3e":"code","91da7222":"code","9103fb5d":"code","c622954e":"code","b970d63c":"code","92d233c2":"code","43e243af":"code","c8ba6178":"code","d5fe9b16":"code","960add7f":"code","ae31d9e5":"code","9281c225":"markdown","d859ad0f":"markdown","7584bc03":"markdown","ff7389a7":"markdown","fa4d5898":"markdown","8ec50bfd":"markdown","f2ec0a57":"markdown","a5c69bcf":"markdown","aa7b8379":"markdown","295566b2":"markdown","ba3de3f1":"markdown","6b428ec6":"markdown","6e4bf691":"markdown","b0fca3fd":"markdown","e3a0462d":"markdown","f3f77b70":"markdown","3d18a5de":"markdown","846c36cb":"markdown","a4c4169c":"markdown","02488420":"markdown","56d40ef8":"markdown","668eeb1d":"markdown","74bf4f03":"markdown","49080545":"markdown","c6609335":"markdown","c8089d41":"markdown","a67cac3d":"markdown","82f65083":"markdown","11c9f8df":"markdown","d85108f0":"markdown","63ac938b":"markdown","e1ba9e54":"markdown","51d827a8":"markdown","5615961d":"markdown","d06fe421":"markdown","a7903bcf":"markdown","b2f1cefb":"markdown","073723f6":"markdown","22f2c8a8":"markdown","7846877b":"markdown","9ac9288c":"markdown","d5783059":"markdown","b685b4d0":"markdown","0880dc84":"markdown","86675fac":"markdown","022671d2":"markdown","8036d6e0":"markdown"},"source":{"c0c5840d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ninsurance_df = pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')","a6f0a1a6":"insurance_df.info()","2e394ceb":"insurance_df.head()","2807c691":"insurance_df.isnull().sum()","5899458f":"plt.figure(figsize=(5,5))\nplt.grid()\nsns.countplot(x='sex', data= insurance_df)\nplt.title(\"Sex\", fontsize=15)\nplt.show()","33d703e7":"plt.figure(figsize=(5,5))\nplt.grid()\nsns.countplot(x='smoker', data= insurance_df)\nplt.title(\"Smoker\", fontsize=15)\nplt.show()","f6bb808a":"plt.figure(figsize=(5,5))\nplt.grid()\nsns.distplot(insurance_df['age'])\nplt.title(\"Age\", fontsize=15)\nplt.show()\n\nprint('The maximum age is {}'.format(insurance_df['age'].max()))\nprint('The minimum age is {}'.format(insurance_df['age'].min()))\nprint('The average age is {}'.format(insurance_df['age'].mean()))\nprint('With an exceptionally high population at age {}'.format(int(insurance_df['age'].mode())))","8e61c836":"plt.figure(figsize=(5,5))\nplt.grid()\nsns.countplot(x='region', data= insurance_df)\nplt.title(\"Region\", fontsize=15)\nplt.show()","01eeadc6":"plt.figure(figsize=(5,5))\nplt.grid()\nsns.countplot(x='children', data= insurance_df)\nplt.title(\"Children\", fontsize=15)\nplt.show()","d921f032":"# displot\nplt.figure(figsize=(5,5))\nplt.grid()\nsns.distplot(insurance_df['bmi'])\nplt.title(\"bmi\", fontsize=15)\nplt.show()","a68880b8":"print(\"the max bmi is: \",insurance_df['bmi'].max())\nprint(\"the min bmi is: \",insurance_df['bmi'].min())\nprint(\"the average bmi is: \",insurance_df['bmi'].mean())","99d25643":"# displot\nplt.figure(figsize=(5,5))\nplt.grid()\nsns.distplot(insurance_df['charges'])\nplt.title(\"charges\", fontsize=15)\nplt.show()","7e59e1a6":"print(\"the max charges is: \",insurance_df['charges'].max())\nprint(\"the min charges is: \",insurance_df['charges'].min())\nprint(\"the average charges is: \",insurance_df['charges'].mean())","0771821c":"tmp_df = insurance_df.copy()\ntmp_df_female = insurance_df[insurance_df['sex']=='female']\ntmp_df_male = insurance_df[insurance_df['sex']=='male']\n# plot graph\nplt.figure(figsize=(8,6))\nax = sns.boxplot(x=\"sex\", y=\"charges\", data=tmp_df)\nplt.title(\"charges against sex\", fontsize=15)\nplt.grid()\nplt.show()","fd9be8dc":"print(\"The median medical costs for male is \", tmp_df_male['charges'].median())\nprint(\"The median medical costs for female is \", tmp_df_female['charges'].median())","21177f63":"tmp_df = insurance_df.copy()\n# plot graph\nplt.figure(figsize=(8,6))\nax = sns.boxplot(x=\"smoker\", y=\"charges\", data=tmp_df)\nplt.title(\"charges against smoker\", fontsize=15)\nplt.grid()\nplt.show()","15c3fccb":"tmp_df = insurance_df.copy()\n# plot graph\nplt.figure(figsize=(8,6))\nax = sns.boxplot(x=\"region\", y=\"charges\", data=tmp_df)\nplt.title(\"charges against region\", fontsize=15)\nplt.grid()\nplt.show()","8b4ba220":"tmp_df = insurance_df.copy()\n# plot graph\nplt.figure(figsize=(8,6))\nax = sns.boxplot(x=\"children\", y=\"charges\", data=tmp_df)\nplt.title(\"charges against children\", fontsize=15)\nplt.grid()\nplt.show()","995eb3a2":"# scatterplot\nplt.figure(figsize=(8,6))\nsns.scatterplot(data=insurance_df, x=\"age\", y=\"charges\")\nplt.title(\"charges against age\", fontsize=15)\n\n# regression line\nx = insurance_df['age']\ny = insurance_df['charges']\nm, b = np.polyfit(x, y, 1)\nplt.plot(x, m*x+b, color='yellow')\nplt.grid()\nplt.show()","f54077aa":"# scatterplot\nplt.figure(figsize=(8,6))\nsns.scatterplot(data=insurance_df, x=\"bmi\", y=\"charges\")\nplt.title(\"charges against bmi\", fontsize=15)\n\n# regression line\nx = insurance_df['bmi']\ny = insurance_df['charges']\nm, b = np.polyfit(x, y, 1)\nplt.plot(x, m*x+b, color='yellow')\n# ideal BMI\nplt.vlines(x = 18.5, ymin = 0, ymax=60000,\n           color = 'purple')\nplt.vlines(x = 24.9, ymin = 0, ymax=60000,\n           color = 'purple')\n \nplt.grid()\nplt.show()","afc51490":"insurance_df_smoker=insurance_df[insurance_df['smoker']=='yes']\ninsurance_df_nonsmoker=insurance_df[insurance_df['smoker']=='no']\n# scatterplot\nplt.figure(figsize=(8,6))\nsns.scatterplot(data=insurance_df_smoker, x=\"age\", y=\"charges\",color='red',label='smoker')\nsns.scatterplot(data=insurance_df_nonsmoker, x=\"age\", y=\"charges\",color='blue',label='non-smoker')\nplt.title(\"charges against age\", fontsize=15)\nplt.legend()\nplt.show()","c4d91f3e":"# scatterplot\nplt.figure(figsize=(8,6))\nsns.scatterplot(data=insurance_df_smoker, x=\"bmi\", y=\"charges\",color='red',label='smoker')\nsns.scatterplot(data=insurance_df_nonsmoker, x=\"bmi\", y=\"charges\",color='blue',label='non-smoker')\nplt.title(\"charges against BMI\", fontsize=15)\nplt.show()","91da7222":"import random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.model_selection import cross_val_score\nfrom pprint import pprint\nimport itertools","9103fb5d":"class Dataset:\n    def __init__(self, dataframe, target_label):\n        self.dataframe = dataframe\n        self.target_label = target_label\n\n    def preparing_columns_and_dataset(self):\n        X = self.dataframe.drop(self.target_label, axis=1)\n        y = self.dataframe[self.target_label]\n        # select columns\n        categorical_cols = [cname for cname in X.columns if\n                            X[cname].dtype in ['object']]\n        categorical_cols.append('children')\n\n        # Select numerical columns\n        numerical_cols = [cname for cname in X.columns if\n                          X[cname].dtype in ['int64', 'float64']]\n        numerical_cols.remove('children')\n\n        return X, y, categorical_cols, numerical_cols","c622954e":"class Model:\n\n    def __init__(self, model_name, model, params={}):\n        self.model_name = model_name\n        self.model = model\n        self.params = params\n\n    def pipeline(self, numerical_cols, categorical_cols):\n        numerical_transformer = StandardScaler()\n        # Preprocessing for categorical data\n        # using one-hot-encoding method to cope with categorical data\n        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n        # Bundle preprocessing for numerical and categorical data\n        preprocessor = ColumnTransformer(\n            transformers=[\n                ('num', numerical_transformer, numerical_cols),\n                ('cat', categorical_transformer, categorical_cols)\n            ])\n        return preprocessor\n\n    def customized_model(self):\n        custom_model = self.model(**self.params, random_state=42)\n        return custom_model\n\n    def model_fitting(self, X, y, numerical_cols, categorical_cols, hyper_tuning, print_option=True):\n        # Bundle preprocessing and modeling code in a pipeline\n        preprocessor = self.pipeline(numerical_cols, categorical_cols)\n        if callable(self.model):\n            model = self.customized_model()\n        else:\n            model = self.model\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n        fit_model = Pipeline(steps=[('preprocessor', preprocessor),\n                                    ('model', model)\n                                    ])\n        # cross validation\n        # ------------------------------------------------------\n        all_accuracies = cross_val_score(estimator=fit_model, X=X, y=y, cv=4, scoring='r2')\n        if print_option is True:\n            print(\"Average cross validation score for model\", self.model_name, \":\")\n            print(round(all_accuracies.mean(), 4))\n        # ------------------------------------------------------\n        if hyper_tuning ==0:\n            fit_model.fit(X_train, np.log(y_train))\n        return fit_model, X_train, X_test, y_train, y_test\n\n    def model_pred(self, model_predict, X_test):\n        preds = model_predict.predict(X_test)\n        return preds\n\n    def accuracy_test(self, y_test, preds, print_option=True):\n        r_squared = r2_score(y_test, np.exp(preds))\n        mae = mean_absolute_error(y_test, np.exp(preds))\n        rmse = sqrt(mean_squared_error(y_test, np.exp(preds)))\n        size_of_error = (mean_absolute_error(y_test, np.exp(preds)) \/ ((insurance_df['charges'].sum()) \/ len(insurance_df)))\n        if print_option is True:\n            print(\"Performance of model\", self.model_name,\":\")\n            print(\"---------------------------------------\")\n            print('R square: ',  round(r_squared, 4))\n            #print('Mean absolute error:', round(mae, 4))\n            #print('size of error: ', round(size_of_error, 4))\n            print('Root Mean Squared error: ', round(rmse, 4))\n            print(\"---------------------------------------\")\n            print(\"\\n\")\n        return r_squared, mae, rmse, size_of_error","b970d63c":"class Tuning:\n\n    def random_search_hyperparameter_tuning(self, X, y, numerical_cols, categorical_cols):\n        iterations = 500\n        max_score = 0\n        max_rmse = 0\n        iteration_appear = 0\n        candidate_params = []\n        candidate_params_score = []\n        for i in range(iterations):\n            print('iteration number', i)\n            # set random seed\n            seed_number = i\n            random.seed(seed_number)\n            np.random.seed(seed_number)\n            params = {'random_state':42}  # initialize parameters\n            loss_list = ['linear', 'square', 'exponential']\n            num_list = [50, 100, 1000, 1500]\n            params['learning_rate'] = round(np.random.uniform(0, 1), 3)\n            params['loss'] = random.choice(loss_list)\n            params['n_estimators'] = random.choice(num_list)\n            # params['max_depth'] = np.random.randint(20, 100)\n            print(params)\n            # model fitting\n            regr = AdaBoostRegressor(**params)\n            model_tuning = Model('adaboost', regr)\n            fit_model, _, X_test, _, y_test = model_tuning.model_fitting(X, y, numerical_cols, categorical_cols, 0)\n            prediction = model_tuning.model_pred(fit_model, X_test)\n            # measuring scores\n            r_squared, mae, rmse, size_of_error = model_tuning.accuracy_test(y_test, prediction)\n\n            if r_squared >= 0.85:\n                candidate_params.append(params)\n                iteration_appear = i\n                candidate_params_score.append((iteration_appear, round(r_squared, 4), round(rmse, 4)))\n\n            if r_squared > max_score:\n                max_score = r_squared\n                max_rmse = rmse\n                pp = params\n                iteration_appear = i\n        print(\"*\" * 50)\n        print('Used params', pp)\n        print('iteration_appear', iteration_appear)\n        print(\"r2 is\", round(max_score,3))\n        print(\"rmse is\", round(max_rmse, 3))\n        return pp, candidate_params, candidate_params_score\n\n    def gridsearch_cv_tuning(self,  X, y, numerical_cols, categorical_cols):\n        max_score = 0\n        max_rmse = 0\n        # create list for gridsearch\n        learning_rate = [0.05, 0.1, 0.15, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.594, 0.6, 0.65, 0.7, 0.8, 0.85, 0.9]\n        n_estimators = [50, 100, 1000, 1500, 2000]\n\n        # create empty list to store the\n        # combinations\n        unique_combinations = []\n        for combination in itertools.product(learning_rate, n_estimators):\n            unique_combinations.append(combination)\n        print('grid search...')\n        i = 0\n        for params_tuple in unique_combinations:\n            print('iteration: ', i)\n            params={'random_state':42}\n            params['learning_rate'] = params_tuple[0]\n            params['n_estimators'] = params_tuple[1]\n            print(params)\n            regr = Model('adaboost', AdaBoostRegressor(**params))\n            fit_model, X_train, X_test, y_train, y_test =regr.model_fitting(X, y, numerical_cols, categorical_cols, 0)#, print_option=False)\n            prediction = regr.model_pred(fit_model, X_test)\n            # measuring scores\n            r_squared, mae, rmse, size_of_error = regr.accuracy_test(y_test, prediction) # print_option=False)\n            if r_squared > max_score:\n                max_score = r_squared\n                max_rmse = rmse\n                pp = params\n            i +=1\n        print('gridsearch complete!')\n        print(\"*\" * 50)\n        print('Used params', pp)\n        print(\"r2 is\", round(max_score, 3))\n        print(\"rmse is\", round(max_rmse, 3))\n        return pp, max_score, max_rmse","92d233c2":"# preparing dataset\ninsurance_dataset = Dataset(insurance_df, 'charges')\nX, y, categorical_cols, numerical_cols = insurance_dataset.preparing_columns_and_dataset()","43e243af":"# linear regression\nlr=LinearRegression()\nlinear_model = Model('linear Regression', lr)\n# model fitting\nfit_model_lr, _, X_test_lr, _, y_test_lr = linear_model.model_fitting(X, y, numerical_cols, categorical_cols, 0)\n# model prediction\nprediction_lr = linear_model.model_pred(fit_model_lr, X_test_lr)\n# accuracy test\n_, _, _ , _= linear_model.accuracy_test(y_test_lr, prediction_lr)","c8ba6178":"# xgboost\nxgboost_model = Model('xgboost', XGBRegressor)\n# model fitting\nfit_model_xgb, _, X_test_xgb, _, y_test_xgb = xgboost_model.model_fitting(X, y, numerical_cols, categorical_cols, 0)\n# model prediction\nprediction_xgb = xgboost_model.model_pred(fit_model_xgb, X_test_xgb)\n# accuracy test\n_, _, _, _ = xgboost_model.accuracy_test(y_test_xgb, prediction_xgb)","d5fe9b16":"# adaboost\n# need to pass params directly when instantiating the model for sklearn\nparams_ada = {'n_estimators':100, 'random_state':42}\nregr = AdaBoostRegressor(**params_ada)\nadaboost_model = Model('adaboost', regr)\n# model fitting\nfit_model_ada, _, X_test_ada, _, y_test_ada = adaboost_model.model_fitting(X, y, numerical_cols, categorical_cols, 0)\n# model prediction\nprediction_ada = adaboost_model.model_pred(fit_model_ada, X_test_ada)\n# accuracy test\n_, _, _, _ = adaboost_model.accuracy_test(y_test_ada, prediction_ada)","960add7f":"# hyperparameter tuning\nada_tuning = Tuning()\n\n# use random search to narrow down the hyperparameter tuning range\n# uncomment the following to do random search\n#pp_ada, candidate_pp, candidate_pp_score = ada_tuning.random_search_hyperparameter_tuning(X, y, numerical_cols, categorical_cols)\n#pprint(candidate_pp)\n#pprint(candidate_pp_score)\n\n# use gridsearch to test all possible combination of the parameters\n# uncomment the following to do grid search\n#pp_ada, max_score, max_rmse = ada_tuning.gridsearch_cv_tuning(X, y, numerical_cols, categorical_cols)\npp_ada = {'random_state': 42, 'learning_rate': 0.594, 'n_estimators': 50}\nprint(\"The hyperparameters to be used: \", pp_ada)\nfinal_regr = AdaBoostRegressor(**pp_ada)\nadaboost_model = Model('adaboost', final_regr)\n# model fitting\nfit_model_regr, _, X_test_regr, _, y_test_regr = adaboost_model.model_fitting(X, y, numerical_cols, categorical_cols, 0)\n# model prediction\nprediction_regr = adaboost_model.model_pred(fit_model_regr, X_test_regr)\n# accuracy test\n_, _, _, _ = adaboost_model.accuracy_test(y_test_regr, prediction_regr)","ae31d9e5":"output = pd.DataFrame({'test charges': y_test_regr,\n                       'predict charges': np.exp(prediction_regr)})\noutput.to_csv('submission.csv', index=False)","9281c225":"All categories have a similar median (at around 10000). However, the minimum medical costs rise as number of dependents covered in a policy increase. It is understandable that more people covered in a policy would result in higher medical costs. The reason why medians do not go up with the increasing number of dependents is possible that children are less likely to have serious illness, so most of the time they just require outpatient services, which usually charge relatively low costs. Most of the medical costs come from the policyholders rather than his children.\n\nIn this sense, number of dependents determines the minimum medical cost of a policyholder. However, other factors about the policyholder like age and smoking status are the determining factors of the final medical cost charged.","d859ad0f":"XGBoost is a popular machine learning algorithm for regression problems. \n![image.png](attachment:8ac54d9f-5cd5-4500-9488-7a4d8f032068.png)\nForest of decision trees are created in the process and the final ouput of the predicted value is calculated by taking all fitted trees into consideration.\n\nFor detailed mathematical interpretation of XGBoost, please refer to the link below:\nhttps:\/\/www.digdeepml.com\/2020\/04\/04\/Math-Behind-GBM-and-XGBoost\/","7584bc03":"charges against children","ff7389a7":"Children","fa4d5898":"### Dataset\nAfter preparing the dataset, models would be instantiated for the model fitting and prediction part.Random seed are applied throughtout the model fitting process for reproducibility. Regressors are used in this experiment as regression is more suitable for the objective of the task (prediction of medical insurance charges).\n\n### Data preprocessing\nA preprocessor would be created using the pipeline method in Model class for data preprocessing. \n\nFor numerical data, standard scaler method from sklearn would be applied to standardize numerical features (age and bmi). The performances of the models is likely to be enhanced with standard normalised features. \n\nAs for categorical data, one-hot-encoding method from sklearn is applied to categorical features (sex, children, smoker and region) as most regressors can only process numerical data instead of string data. One-hot-encoding is used rather than label encoding in order to avoid unnecessary ordering within the column.\n\n### Preparing train test set and cross validation\nTrain test split method from sklearn would be used for creating train sets and test sets from original dataset for model runs. The ratio of train set to test set would be 0.75 to 0.25. Shuffling is present in this experiment.\n\nCross validation would be conducted to evaluate the performance of model on different sets of train-test data. As the ratio of train and test set is set to be 0.75 to 0.25 previously, the entire dataset would be splitted into 4 folds. The scoring metrics used to measure the performance of the model in cross validation section would be the average value of R^2 of different train-test data. A higher value of R^2 indicates a better performance of model in cross valudation section.\n\n### Model fitting\nAfter that, the train set would be fitted to the model using the command .fit() in python. Please be noted that a logarithm operation is preformed to y_train at this stage. The reason for this operation is that charges is likely to follow a lognormal distribution according to the previous displot on charges.\n\n### Evaluation metrics\nPredictions would be made on the test set and scores would be calculated for measuring the performance of the model.\n2 loss functions would be used for this experiment:\n\n1. R^2: ratio of variance explained by the model. Higher value of R^2 indicates a better performance of the model.\n2. Root mean square error(RMSE): Square root of the mean value of sum of square error between actual and predicted value. Smaller value of RMSE indicates a better performance of the model.\n\n*Originally, mean average error(mae) is considered to be one of the loss functions used in this experiment. However, as RMSE gives higher penalities to huge differences when compared to MAE, added that the distribution of charges are right-skewed as mentioned before, RMSE is chosen instead.","8ec50bfd":"XGBoost performs better than linear regression model in terms of cross validation and prediction power.","f2ec0a57":"This observation aligns with my previous insights. Insurance firms are fast to spot out smokers but unable to identify those with high BMI scores. Even if you have a high BMI score, medical costs remain low if you do not smoke. However, as insurance firms paid high attention on smokers, those who smoke and have a high BMI scores are charged with high-than-average medical costs. All the outliers having extreme charges are smokers with high BMI scores.\n\nThe pattern may also suggest that the 3 bands we observed in the \"charges against age\" scatterplot are: 1. non-smokers, 2. smokers with low BMI scores and non-smoking outliers, 3. smokers with high BMI scores.","a5c69bcf":"Region","aa7b8379":"# Further Investigation","295566b2":"# Charges against other variables","ba3de3f1":"The result has improved by a small fraction.\n\nThe parameters to be used in the fitted model of adaboost is:\n{'random_state': 42, 'learning_rate': 0.594, 'n_estimators': 50}","6b428ec6":"Charges against smoker","6e4bf691":"charges against age","b0fca3fd":"The Dataset class is used for separating target columns from the feature columns, and dividing columns into categorical columns and numerical columns according to the nature of the data stored inside of them.","e3a0462d":"# Description of variables","f3f77b70":"It is patently obviously that smokers are being charged with higher medical costs when comparing to non-smokers. Most insurance firms reckons smoking as a high risk activity that may result in policyholders' deteriorating health conditions, and thus higher medical costs are collected to offset the risk.","3d18a5de":"Age","846c36cb":"BMI","a4c4169c":"Sex:","02488420":"### Hyperparameter Tuning\nTwo hyperparameter tuning methods are encapsulated under the Tuning class. Random search method would be performed first in order to narrow down the possible optimal value of each parameters. After that, grid search method would be carried out so as to test all possible combinations of parameters for the fitting model.","56d40ef8":"---------------------------------------------------------------------------------------","668eeb1d":"The median of medical costs for two sex groups are roughly the same. (9367 for male and 9412 for female) Outliers can be seen in both groups. The only difference is that the male group has a slightly higher 3rd quantile than that of the female group.\n\nThis finding suggests that sex may not be the major factor that determines the medical costs for each policyholders. Both male and female policyholders are regarded as having the same amount of risk with all other factors held constant.  ","74bf4f03":"charges against region","49080545":"Although adaboost is slightly performing worse than xgboost in cross validation section (0.7879 vs 0.8096), its performance in model fitting section is better than that of xgboost with R^2 being 0.8454 and RMSE being 4829.7704.\n\nApart from that, adaboost has a lot less hyperparameters to be tuned when compared to that of xgboost. Thus, it helps to save time on hyperparameter tuning section.\n\nTherefore adaboost is chosen to be the model for solving the research question.","c6609335":"4 regions have similar spreads, with southeast region having a higher 3rd quantile.\n\nThis may suggest that the place where policyholders live does not contribute much to the rate making for insurance policies.","c8089d41":"So, we observed that smoking is the major contributing factor of deciding the medical costs, and there are observable patterns in charges against BMI and age. Let's do a further investigation into these factor by splitting the dataset into smoker and non-smoker categories.","a67cac3d":"Smoker","82f65083":"3 candidate models, namely linear regression model, xgboost model and adaboost model, are chosen for this experiment.\n\nWe would choose one of them to be the ultimate model for solving the research question.","11c9f8df":"checking for any missing values","d85108f0":"The pattern shows a strong positive correlation between age and charges (i.e. the older you are, the higher the medical costs is)\nWe could also observe that the data points are clustered into roughly 3 groups, with the lowest band(charges ranging from 0 to 20,000) having the strongest relationship between age and charges. ","63ac938b":"We can clearly see that all the policyholders in the lowest band are non-smokers, and smokers occupy higher bands when compared with non-smokers at all age groups.","e1ba9e54":"Showing first 5 rows of the dataset","51d827a8":"## Adaboost","5615961d":"# Explanatory Data Analysis","d06fe421":"Begin by reading csv file\n","a7903bcf":"Adaboost is another regression algorithm used in this experiment.\n![image.png](attachment:591146e6-2deb-42d8-99b7-ccbb9f25387c.png)\nA forest of stumps with 2 leaves (weak learners) rather than trees with undertermined nodes and depths are created during the model fitting process. In each iteration, the weights of each sample would be adjusted according to the size of prediction error made in the previous iteration. The next weak learner would then put more emphasis into those samples which are wrongly predicted.\n\nFor detailed mathematical interpretation of adaboost, please refer to the link below:\nhttps:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.31.314&rep=rep1&type=pdf","b2f1cefb":"charges","073723f6":"## XGBoost","22f2c8a8":"The pattern shows a mildly positively correlated relationship between charges and bmi.(i.e. the higher the BMI of the policyholder, the higher the charges)\n\nBMI is a ratio of height to weight, which is often used as an indicator of people's health. The ideal BMI score range is 18.5 - 24.9 as indicated by the purple lines in the above scattorplot. In theory, those who have scores within the ideal range should be charged with lower prices as they are considered to be healthier than most.\n\nWe can observe that those within the range are charged with lower prices(with the maxmimum at around 30,000). However, even for those who have BMI scores exceeding the ideal range, there is still a majority of them having lower medical costs at around 10,000. \n\nThis may due to the fact that solid information of clients' BMI scores is not collected during the ratemaking process, but rather information about clients' lifestyle is gathered (e.g. smoking habit). Therefore, some of the policyholders having high BMI score remain unspotted and thus they are charged at low prices.","7846877b":"From R square, we can see that linear regression model got a pretty high score(0.6082) which is not too close to 1, meaning that model is not fitting too well with the data points.\n","9ac9288c":"# Summary of variables:\n1. **sex**: there is an approximately equal number of male and female\n2. **smoker**: most of the people are non-smokers\n3. **age**: age of policyholders is ranging from 18 to 64, with an exceptionally high population at age 18\n4. **region**: The policyholders are roughly evenly distributed among 4 regions, namely southeast, southwest, northeast and northwest, with southeast region having a slightly larger population.\n5. **children**: The dataset are divided into 6 categories regarding number of dependents they covered in their policies,with the min number of children being 0 and the max being 5. Most of the people do not have any dependents. The population tend to decrease in size with increasing number of dependents.\n6. **bmi**: BMI is following a normal distribution(having a bell-shaped population with a mean of 31) Value of BMI score is ranging from 15 to 53.\n7. **charges**: Charges in this dataset are ranging from 1122 to 63770 dollars with the mean of 13270. Charges is right-skewed with a long tail stretching to larger values, which means that most policyholders are charged at low prices while a few outliers charging at extremely high prices.","d5783059":"--------------------------------------------------------------------------------","b685b4d0":"## Linear Regression Model","0880dc84":"Charges against sex","86675fac":"# Tuning adaboost","022671d2":"# Regression Model Run","8036d6e0":"# Introduction\n\nThe task is to accurately predict insurance costs based on the following variables:\n\n* **age:** age of primary beneficiary\n* **sex:** insurance contractor gender, female, male\n* **bmi:** Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n* **children:** Number of children covered by health insurance \/ Number of dependents\n* **smoker:** Smoking\n* **region:** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n* **charges:** Individual medical costs billed by health insurance"}}