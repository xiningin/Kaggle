{"cell_type":{"2d326d13":"code","d8567b5e":"code","1aeaf555":"code","76633ee1":"code","9b9df420":"code","97953433":"code","d9502531":"code","e0bff955":"code","3ff26180":"code","402ebd6a":"code","d7216413":"code","f9c30292":"code","097353b9":"code","d613e15d":"code","29916548":"code","99895716":"code","6595963e":"code","099f4a21":"code","0000039e":"code","3dfafca8":"code","7b2a4cbb":"code","56747925":"code","37f467e7":"code","590f1677":"code","cf51010f":"code","e8506d55":"markdown","c068b0cd":"markdown","6b0cc17b":"markdown","e577d30e":"markdown","6fa3da2f":"markdown","e8856208":"markdown","cad5f6b4":"markdown","a986b890":"markdown","65be9efb":"markdown","2800c194":"markdown","9951b7f9":"markdown","b4abec68":"markdown","5e75491b":"markdown","88e70f87":"markdown","43dda9c2":"markdown","86d2c916":"markdown","944a3b2c":"markdown","a1f2a9b2":"markdown"},"source":{"2d326d13":"!pip install --user pyspark==2.3.3 --upgrade|tail -n 1","d8567b5e":"import pandas as pd\nimport numpy as np\nimport json\nimport os\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1aeaf555":"import os\nprint(os.listdir(\"..\/input\/telco-customer-churn\"))\n","76633ee1":"df = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndf.head()","9b9df420":"df = df.drop('customerID', axis=1)\ndf.head(5)","97953433":"df.info()","d9502531":"# Statistics for the columns (features). Set it to all, since default is to describe just the numeric features.\ndf.describe(include = 'all')","e0bff955":"totalCharges = df.columns.get_loc(\"TotalCharges\")\nnew_col = pd.to_numeric(df.iloc[:, totalCharges], errors='coerce')\ndf.iloc[:, totalCharges] = pd.Series(new_col)","3ff26180":"# Check if we have any NaN values and see which features have missing values that should be addressed\nprint(df.isnull().values.any())\ndf.isnull().sum()","402ebd6a":"# Handle missing values for nan_column (TotalCharges)\nfrom sklearn.impute import SimpleImputer\n\n# Find the column number for TotalCharges (starting at 0).\ntotal_charges_idx = df.columns.get_loc(\"TotalCharges\")\nimputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\") #SimpleImputer(strategy=\"most_frequent\")\n\ndf.iloc[:, total_charges_idx] = imputer.fit_transform(df.iloc[:, total_charges_idx].values.reshape(-1, 1))\ndf.iloc[:, total_charges_idx] = pd.Series(df.iloc[:, total_charges_idx])","d7216413":"# Validate that we have addressed any NaN values\nprint(df.isnull().values.any())\ndf.isnull().sum()","f9c30292":"columns_idx = np.s_[0:] # Slice of first row(header) with all columns.\nfirst_record_idx = np.s_[0] # Index of first record\n\nstring_fields = [type(fld) is str for fld in df.iloc[first_record_idx, columns_idx]] # All string fields\nall_features = [x for x in df.columns if x != 'Churn']\ncategorical_columns = list(np.array(df.columns)[columns_idx][string_fields])\ncategorical_features = [x for x in categorical_columns if x != 'Churn']\ncontinuous_features = [x for x in all_features if x not in categorical_features]\n\n#print('All Features: ', all_features)\n#print('\\nCategorical Features: ', categorical_features)\n#print('\\nContinuous Features: ', continuous_features)\n#print('\\nAll Categorical Columns: ', categorical_columns)","097353b9":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nsns.set_palette(\"hls\", 3)","d613e15d":"print(df.groupby(['Churn']).size())\nchurn_plot = sns.countplot(data=df, x='Churn', order=df.Churn.value_counts().index)\nplt.ylabel('Count')\nfor p in churn_plot.patches:\n    height = p.get_height()\n    churn_plot.text(p.get_x()+p.get_width()\/2., height + 1,'{0:.0%}'.format(height\/float(len(df))),ha=\"center\") \nplt.show()","29916548":"# Categorical feature count plots\nf, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9), (ax10, ax11, ax12), (ax13, ax14, ax15)) = plt.subplots(5, 3, figsize=(20, 20))\nax = [ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10, ax11, ax12, ax13, ax14, ax15 ]\n\nfor i in range(len(categorical_features)):\n    sns.countplot(x = categorical_features[i], hue=\"Churn\", data=df, ax=ax[i])","99895716":"# Continuous feature histograms.\nfig, ax = plt.subplots(2, 2, figsize=(28, 8))\ndf[df.Churn == 'No'][continuous_features].hist(bins=20, color=\"blue\", alpha=0.5, ax=ax)\ndf[df.Churn == 'Yes'][continuous_features].hist(bins=20, color=\"orange\", alpha=0.5, ax=ax)\n\n# Or use displots\n#sns.set_palette(\"hls\", 3)\n#f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(25, 25))\n#ax = [ax1, ax2, ax3, ax4]\n#for i in range(len(continuous_features)):\n#    sns.distplot(df[continuous_features[i]], bins=20, hist=True, ax=ax[i])","6595963e":"# Create Grid for pairwise relationships\ngr = sns.PairGrid(df, height=5, hue=\"Churn\")\ngr = gr.map_diag(plt.hist)\ngr = gr.map_offdiag(plt.scatter)\ngr = gr.add_legend()","099f4a21":"# Plot boxplots of numerical columns. More variation in the boxplot implies higher significance. \nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(25, 25))\nax = [ax1, ax2, ax3, ax4]\n\nfor i in range(len(continuous_features)):\n    sns.boxplot(x = 'Churn', y = continuous_features[i], data=df, ax=ax[i])","0000039e":"df.columns","3dfafca8":"\nfrom sklearn.model_selection import train_test_split\nX = df[['SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n       'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n       'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'Contract', 'PaperlessBilling', 'PaymentMethod',\n       'MonthlyCharges', 'TotalCharges']]\ny = df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(\n                X, y, test_size=0.33, random_state=42)","7b2a4cbb":"one_hot_encoded_training_predictors = pd.get_dummies(X_train)\n","56747925":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=42)\nrfc.fit(one_hot_encoded_training_predictors, y_train)","37f467e7":"model = rfc.fit(one_hot_encoded_training_predictors, y_train)","590f1677":"test_data = pd.get_dummies(X_test)\nmodel.score(test_data, y_test)","cf51010f":"from sklearn.metrics import roc_auc_score\ny_scores = model.predict(test_data)\ny_scores = pd.get_dummies(y_scores)\ny_test = pd.get_dummies(y_test)\nroc_auc_score(y_test, y_scores)","e8506d55":"## 2.0 Load and Clean data\n\nThe data will be loaded as a pandas data frame.\n","c068b0cd":"It can be observed that Tenure ranges from 0 (new customer) to 6 years, Monthly charges range from $18 to $118, etc","6b0cc17b":"### 2.6 Visualize data\n\nData visualization can be used to find patterns, detect outliers, understand distribution and more. Following graphs can be used such as:\n\n- Histograms, boxplots, etc: To find distribution \/ spread of our continuous variables.\n- Bar charts: To show frequency in categorical values.\n","e577d30e":"`TotalCharges` column has missing values. There are various ways to address this issue:\n\n- Drop records with missing values \n- Fill in the missing value with one of the following strategies: Zero, Mean of the values for the column, Random value, etc).","6fa3da2f":"\n\n### 2.4 Any NaN values should be removed to create a more accurate model.","e8856208":"Frequency counts charts can be used to get an understanding of the categorical features relative to `Churn`  \n\n- It can be observed that for the `gender` feature, We have relatively equal rates of churn by `gender`\n- It can be observed that for the `InternetService` feature, We have higher churn for those that have \"Fiber optic\" service versus those with \"DSL\"\n","cad5f6b4":"In this notebook I will build the prediction model using the SparkML library.\n\nThis notebook walks you through these steps:\n\n- Load and Visualize data set.\n- Build a predictive model","a986b890":"The simple random forest classifier gives an accuracy of 79%. The best metric for evaluating a binary classifier is ROC AUC(Area Under the curve)","65be9efb":"\n### 2.5 Categorize Features\n\nSome of the columns \/ features are categorized based on wether they are categorical values or continuous (i.e numerical) values which will be used later to build visualizations.","2800c194":"## 3.0 Create a model\n\nNow machine learning model can be created. You could use the insights \/ intuition gained from the data visualization steps above to what kind of model to create or which features to use. For now, I am creating a simple classification model by removing some of the uninformative features such as gender, StreamingTV and StreamingMovies","9951b7f9":"### 3.1 Split the data into training and test sets","b4abec68":"Histrogram charts can be used to get an understanding of the distribution of our continuous \/ numerical features relative to Churn.\n\n- It can be observed that for the `MonthlyCharges` feature, customers that churn tend to pay higher monthly fees than those that stay.\n- It can be observed that for the `tenure` feature, customers that churn tend to be relatively new customers.","5e75491b":"## 1.0 Install required packages\n\nThere are a couple of Python packages I will use in this notebook. ","88e70f87":"To get a high level view of the distribution of `Churn`. What percentage of customer in our dataset are churning vs not churning. ","43dda9c2":"### 2.1 Drop CustomerID feature (column)\nAs it contains PII","86d2c916":"### 2.3 Check for need to Convert TotalCharges column to numeric if it is detected as object\n\nSince above `df.info` shows the \"TotalCharges\" columnn as an object, it needs to converted to numeric.","944a3b2c":"### 2.2 Examine the data types of the features","a1f2a9b2":"# Predicting Telco Customer Churn using Random Forest Classifier"}}