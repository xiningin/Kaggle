{"cell_type":{"2f2b415c":"code","c88d1326":"code","b97a80f6":"code","de990119":"code","d699b869":"code","fcc348da":"code","52cc9dbe":"code","3b6bcee7":"code","923549cf":"code","a2b36113":"code","5794cac4":"code","5f2b96f3":"code","9ad98996":"code","9b9ce4a8":"code","ed3697c4":"code","9018eb99":"code","0956b9b5":"code","dd3583d6":"code","47db9a45":"code","ced8e28b":"code","ac80e4b4":"code","dac468bf":"code","1820a4de":"code","ca0235f3":"code","ff246053":"code","73e71954":"code","3ad9b8ea":"code","0eeac95d":"code","4334e18a":"code","99f29c07":"code","498427e3":"markdown","6e5e9635":"markdown","14139c5e":"markdown","fd6ecad8":"markdown","b861d107":"markdown","e603a1e1":"markdown","556858d6":"markdown","efbe7583":"markdown","a59c452f":"markdown","fe902b8f":"markdown","a79bec0c":"markdown","7cd5588e":"markdown","7aaa5e7c":"markdown","901a7b95":"markdown"},"source":{"2f2b415c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c88d1326":"# let's start on mercedes car\ncclass = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/cclass.csv')\nfocus = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/focus.csv')\naudi = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv')\ntoyota = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv')\nskoda = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/skoda.csv')\nford = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/ford.csv')\nvauxhall = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vauxhall.csv')\nbmw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')\nvw = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/vw.csv')\nhyundai = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/hyundi.csv')\nmerc = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/merc.csv')\ndata = vw.copy()\ndata.head()","b97a80f6":"data.info()","de990119":"# check price distribution\nsns.histplot(data['price'], bins=30)","d699b869":"def plot_numerical(feature):\n    ax = sns.lmplot(x=feature, y='price', data=data)\n    ax.set_xticklabels(rotation=85)\n    plt.show()\n    \ndef plot_categorical(feature, figsize=None):\n    df = data.groupby([feature])['price'].describe()[['mean', '50%', 'min', 'count']]\n\n    labels = df.index.values\n    x = np.arange(len(labels))\n    width = 0.9\n    fig, ax1 = plt.subplots(figsize=(12, 5))\n\n    # plot bars for min, median and mean house price\n    rects1 = ax1.bar(x-width\/2, df['50%'], width\/3, label='median')\n    rects2 = ax1.bar(x-width\/6, df['mean'], width\/3, label='mean')\n    rects3 = ax1.bar(x+width\/6, df['min'], width\/3, label='min')\n\n    ax1.set_ylabel('price', fontsize=15)\n    ax1.set_title(feature, fontsize=18)\n    ax1.set_xticks(x)\n    ax1.set_xticklabels(labels, rotation=85)\n    ax1.legend()\n\n    # plot counts of data points\n    ax2 = ax1.twinx()\n    ax2.set_ylabel('Counts', fontsize=15)\n    ax2.plot(x-width\/2, df['count'], color='red', linestyle='dashed')\n\n    # annotate counts of data points\n    for i, rect in enumerate(rects2):\n        height = int(round(rect.get_height()))\n        ax1.annotate('{}'.format(int(df['count'].iloc[i])),\n                     xy=(rect.get_x() + rect.get_width()\/2, height),\n                     xytext=(0, 3), textcoords=\"offset points\",\n                     ha='center', va='bottom', color='red')\n    plt.show()","fcc348da":"for feature in ['model', 'transmission', 'fuelType']:\n    plot_categorical(feature)","52cc9dbe":"for feature in ['year', 'mileage', 'tax', 'mpg', 'engineSize']:\n    plot_numerical(feature)","3b6bcee7":"categorical_features = ['model', 'transmission', 'fuelType']\nnumerical_features = ['year', 'mileage', 'tax', 'mpg', 'engineSize']","923549cf":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport joblib","a2b36113":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    le = LabelEncoder()\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    #print(feature)\n    \n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))","5794cac4":"plt.figure(figsize=(8, 5))\nsns.heatmap(round(data[numerical_features].corr(method='spearman'), 2), \n            annot=True, mask=None, cmap='GnBu')\nplt.show()","5f2b96f3":"plt.figure(figsize=(10, 5))\nsns.heatmap(round(df[categorical_features+numerical_features+['price']].corr(method='spearman'), 2), annot=True,\n            mask=None, cmap='GnBu')\nplt.show()\n","9ad98996":"from statsmodels.stats.outliers_influence import variance_inflation_factor","9b9ce4a8":"# Calculating VIF\nvif = pd.DataFrame()\nvif[\"variables\"] = [feature for feature in categorical_features+numerical_features if feature not in ['year']]\nvif[\"VIF\"] = [variance_inflation_factor(df[vif['variables']].values, i) for i in range(len(vif[\"variables\"]))]\nprint(vif)","ed3697c4":"NumericData = data[['mileage']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","9018eb99":"NumericData = data[['year']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","0956b9b5":"NumericData = data[['engineSize']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","dd3583d6":"NumericData = data[['tax', 'mpg']]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.title(\"Boxplots for Numerical variables\")\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt)\nbp = sns.stripplot(x='variable', y='value', data=NumericMelt, jitter=True, edgecolor='gray')\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","47db9a45":"# Percentage of outliers present in each variable\noutlier_percentage = {}\nfor feature in numerical_features:\n    tempData = data.sort_values(by=feature)[feature]\n    Q1, Q3 = tempData.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    Lower_range = Q1 - (1.5 * IQR)\n    Upper_range = Q3 + (1.5 * IQR)\n    outlier_percentage[feature] = round((((tempData<(Q1 - 1.5 * IQR)) | (tempData>(Q3 + 1.5 * IQR))).sum()\/tempData.shape[0])*100,2)\noutlier_percentage","ced8e28b":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    \n    le = LabelEncoder()\n    ohe = OneHotEncoder(sparse=False)\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))\n    # load classes\n    columns = joblib.load(\n        open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'rb'))\n\n    if len(le.classes_)>2:\n        # perform one hot encoding\n        ohe.fit(df[[feature]])\n        # save the encoder\n        joblib.dump(ohe, \n                    open(os.path.join(path, \"TextEncoding\/ohe_{}.sav\".format(feature)), 'wb'))\n\n        # transfrom training data\n        # removing first column of encoded data to elude from dummy variable trap\n        tempData = ohe.transform(df[[feature]])[:, 1:]\n\n        # create Dataframe with columns as classes\n        tempData = pd.DataFrame(tempData, columns=columns)\n    else:\n        tempData = df[[feature]]\n    \n    # create dataframe with all the label encoded categorical features along with hot encoding\n    if i==0:\n        encodedData = pd.DataFrame(data=tempData, columns=tempData.columns.values.tolist())\n    else:\n        encodedData = pd.concat([encodedData, tempData], axis=1)","ac80e4b4":"# merge numerical features and categorical encoded features\ndf = df[numerical_features+['price']]\ndf = pd.concat([df, encodedData], axis=1)\ndf.info()","dac468bf":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics, preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import r2_score","1820a4de":"train_data = df.copy()\nfeature_cols = [feature for feature in train_data.columns if feature not in(['price'])]\nprint('features used: ', feature_cols)\n\n# RESCALING\n#scaler = MinMaxScaler()\n#scaler.fit(train_data[feature_cols])\n#train_data[feature_cols] = scaler.transform(train_data[feature_cols])","ca0235f3":"X = train_data[feature_cols]\ny = train_data['price']\n\nvalidation_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=validation_size, random_state=0)","ff246053":"model = LinearRegression()\nmodel.fit(X_train, y_train)","73e71954":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('r2_score: ', round(r2_score(y_train, y_pred)*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('r2_score: ', round(r2_score(y_test, y_pred)*100, 2))","3ad9b8ea":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred[-150:]))), y=y_pred[-150:],\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test[-150:]))), y=y_test[-150:],\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","0eeac95d":"model = XGBRegressor( \n    n_estimators = 1000,\n    learning_rate=0.09, \n    min_child_weight=5,\n    max_depth = 3,\n    subsample = 0.75,\n    seed=7)\n\n\nmodel = model.fit(\n    X_train, \n    y_train, \n    eval_metric=\"rmse\", \n    #early_stopping_rounds=10,\n    #eval_set=[(X_test, y_test)],\n    verbose=False)","4334e18a":"y_pred = model.predict(X_train)\n\nprint('Train metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_train, y_pred)))\nprint('r2_score: ', round(r2_score(y_train, y_pred)*100, 2))\n\ny_pred = model.predict(X_test)\n\nprint('Validation metrics...')\nprint('RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\nprint('r2_score: ', round(r2_score(y_test, y_pred)*100, 2))","99f29c07":"import plotly.graph_objects as go\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(x=list(range(len(y_pred[-150:]))), y=y_pred[-150:],\n                         mode='lines',\n                         name='Prediction'))\nfig.add_trace(go.Scatter(x=list(range(len(y_test[-150:]))), y=y_test[-150:],\n                         mode='lines',\n                         name='True value'))\n\nfig.show()","498427e3":"# Model 1: Linear Regresssion ","6e5e9635":"# Training Model","14139c5e":"# CORRELATION","fd6ecad8":"* mpg - miles per gallon\n* tax - road tax","b861d107":"# Model 2: XGB","e603a1e1":"**Observations-**\n* year - mileage -ve\n* mpg - tax -ve\n* fuelType - engineSize -ve","556858d6":"# Analyzing features using VIF","efbe7583":"**Observations-**\n* model - California models are the costliest ones while Golf and Tiguan are the most popular ones\n* transmission - Manual has cheapest price\n* fuelType - Petrol models are the cheapest and the most popular ones\n* year - new cars are sold at higher prices\n* mileage - lower the mileage or car travelled, higher the price\n* mpg - lower the mpg, higher the car price (usually heavy or luxury cars have lower mpg)\n* engineSize - bigger the enginer, higher the price\n* tax - generally higher the tax, higher the car price","a59c452f":"# Looking at Outliers","fe902b8f":"### Bivariate Analysis Correlation plot with the Categorical variables","a79bec0c":"# EDA","7cd5588e":"# Handling Categorical Features (Label Encoding & One Hot Encoding)","7aaa5e7c":"### Label encoding categorical features for correlation","901a7b95":"### Bivariate Analysis Correlation plot for numerical features"}}