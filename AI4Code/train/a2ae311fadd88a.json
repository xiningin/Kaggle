{"cell_type":{"cc1334b1":"code","40cae090":"code","a5e1e9eb":"code","a80082e9":"code","9f2fd6eb":"code","634998ec":"code","891db13b":"code","fb21e6d5":"code","a2576d97":"code","f713c89d":"code","c916f601":"code","9176e08a":"code","847ca485":"code","77191c79":"code","b0276469":"code","a8529620":"code","e5e88a4a":"code","94081535":"code","98e032ee":"markdown","6b111cb7":"markdown","ebf92259":"markdown","e235f08b":"markdown","dcd97188":"markdown","d7893775":"markdown"},"source":{"cc1334b1":"# Importing necessary libraries   \nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport warnings\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\n\nimport xgboost as xgb\n\nwarnings.filterwarnings('ignore')\nRANDOM_STATE = 1234","40cae090":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","a5e1e9eb":"train.head()","a80082e9":"# this code adopted from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage. Thanks to the original author.\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","9f2fd6eb":"train =  reduce_mem_usage(train)\ntest =  reduce_mem_usage(test)","634998ec":"df_info = {col:np.std(train[col]) for col in train.columns if col not in ['id','claim']}\ndf_info = pd.DataFrame.from_dict(df_info,columns=['std'],orient='index')\ndf_info['max'] = [np.max(train[col]) for col in list(df_info.index)]\ndf_info['min'] = [np.min(train[col]) for col in list(df_info.index)]\ndf_info['null_count'] =[(train[col]).isna().sum() for col in list(df_info.index)]\n\n\ndf_info.style\\\n    .background_gradient(cmap=\"Blues\", subset=['std'])\\\n    .background_gradient(cmap=\"YlOrRd\", subset=['max'])\\\n    .background_gradient(cmap=\"bone\", subset=['min'])\\\n    .background_gradient(cmap=\"Greens\", subset=['null_count'])","891db13b":"train['f_missing'] = train.isna().sum(axis=1)\ntest['f_missing'] = test.isna().sum(axis=1)","fb21e6d5":"kf = KFold(n_splits=5,shuffle=True,random_state=RANDOM_STATE)\ntrain['kfold'] = None\n\nfor fold,(train_idx,valid_idx) in enumerate(kf.split(X=train)):\n    train.loc[valid_idx,'kfold'] = fold","a2576d97":"train.head()","f713c89d":"features = [col for col in train.columns if col.startswith('f')]","c916f601":"#get params\nparams = xgb.XGBClassifier().get_params()","9176e08a":"params","847ca485":"ratio = train[train['claim']==1].shape[0]\/train[train['claim']==0].shape[0]","77191c79":"# split data into training and validation\nxtrain = train[train['kfold']!=1]\nxvalid = train[train['kfold']==1]\n\nytrain = xtrain['claim']\nyvalid = xvalid['claim']\nxtrain = xtrain[features]\nxvalid = xvalid[features]\n\nxtest = test[features]","b0276469":"params['n_estimators'] = 1000\nparams['importance_type'] = 'weight'\nparams['max_depth'] = 8\nparams['max_delta_step'] = 3\nparams['colsample_bytree'] = 1\nparams['learning_rate'] = 0.05\nparams['booster'] = 'dart'\nparams['verbosity'] = 2\nparams['eval_metric'] = 'auc'\nparams['tree_method'] = 'gpu_hist'\nparams['random_state'] = RANDOM_STATE\nparams['num_parallel_tree'] = 10\nparams['scale_pos_weight'] = ratio\nparams['eta'] = 0.1\nparams['subsample'] = 0.8\nparams['objective'] = 'binary:logistic'\nparams['seed'] = RANDOM_STATE\nparams['nthread'] = -1\nparams['silent'] = False\nparams['predictor'] = 'gpu_predictor'","a8529620":"watchlist = xgb.DMatrix(xvalid, label=yvalid, missing = np.nan)\ndtrain = xgb.DMatrix(xtrain,label=ytrain,missing= np.nan)\nwatchlist = [(watchlist,'validation')]\nplst = list(params.items())\nnum_round = 50\nbst = xgb.train(plst, dtrain, num_round, watchlist)","e5e88a4a":"fig, ax = plt.subplots(figsize=(12,18))\nxgb.plot_importance(bst, max_num_features=80, height=0.8, ax=ax)\nplt.show()","94081535":"dtest = xgb.DMatrix(xtest,missing= np.nan)\nPredictions = bst.predict(dtest)\nsample_submission['claim'] = Predictions\nsample_submission.to_csv('submission.csv',index=False)","98e032ee":"# Plot feature importance ","6b111cb7":"# Model training","ebf92259":"## Starter notebook | TPS September - XGBoost","e235f08b":"# Modeling","dcd97188":"# Final Predictions","d7893775":"## Data reading and basic preprocessing"}}