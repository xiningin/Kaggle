{"cell_type":{"a4135e02":"code","65ae41d7":"code","b53d1a0f":"code","66fd5ab0":"code","c01f4797":"code","6a95680b":"code","32f89af1":"code","410b8947":"code","b0a23d77":"code","918f585c":"code","5cdeab99":"code","db1e9e46":"code","472e53d1":"code","a3753a2c":"code","cdb8a203":"code","24931a9f":"code","23453d24":"code","43ddc261":"code","acfb5bfe":"code","bd53c579":"code","2e09526c":"code","90ffc4c4":"code","19b400c5":"code","4a43f84a":"code","ab55b530":"code","0964cb2a":"code","b68968d6":"code","9b4ef8d3":"code","064b2ea2":"code","a146fedc":"code","ab0172fd":"code","5d377869":"code","cc62eacc":"code","d16c3efd":"code","7a38fb85":"code","049a3eb3":"code","a644d3bc":"code","6a93e1b9":"code","a922e441":"code","79defd85":"code","4e77fcbe":"code","d48dc0c4":"code","f08b30f0":"code","f05dca7b":"code","6c1e562c":"code","afeb3a4b":"code","5ab13421":"code","07accc14":"code","dd3326a9":"code","b6ad9396":"markdown","2abf164e":"markdown","b81b6d9f":"markdown","5d4d073e":"markdown","d3c2ad28":"markdown","23679bdb":"markdown","95ae61df":"markdown","af66bbd4":"markdown","21ebae02":"markdown","dcaa2da9":"markdown","ef0f54c9":"markdown","4404982a":"markdown","5b7d7c98":"markdown","4f39f129":"markdown","dcff273b":"markdown","f634ff4f":"markdown","6d0308b3":"markdown","4a1cfbe2":"markdown","b2747c33":"markdown","97879366":"markdown","493b1202":"markdown","58d696f3":"markdown","69b4b187":"markdown","81964994":"markdown","fc6b0786":"markdown","4775ebd9":"markdown","eb99a88f":"markdown","6e876a3e":"markdown","63110973":"markdown","c81ad732":"markdown","b639e32d":"markdown","62302467":"markdown"},"source":{"a4135e02":"import numpy as np \nimport pandas as pd \n\n# graphics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# modeling\nfrom sklearn.linear_model import LinearRegression, Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.model_selection import GridSearchCV\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","65ae41d7":"fifa19 = pd.read_csv('..\/input\/fifa19\/data.csv')","b53d1a0f":"fifa19.columns","66fd5ab0":"fifa19","c01f4797":"# take useful columns\ncol_name_full = list(fifa19.columns)\ncol_names = col_name_full[54:88] # crossing ~ GKReflexes\ncol_names = ['Value', 'Wage','Name', 'Age', 'Position', 'International Reputation', 'Weak Foot', 'Skill Moves', 'Work Rate', 'Body Type', 'Height', 'Weight', 'Overall', 'Potential'] + col_names\nlen(col_names)\n\nfifa19 = fifa19[col_names]","6a95680b":"# change value according to M\/K\nfifa19['Value'] = fifa19['Value'].apply(lambda x: x.replace('\u20ac',''))\nfifa19['Value'] = (fifa19['Value'].replace(r'[KM]+$', '', regex=True).astype(float) * \n                   fifa19['Value'].str.extract(r'[\\d\\.]+([KM]+)', expand=False).fillna(1).replace(['K','M'], [10**3, 10**6]).astype(int))\nfifa19['Value']","32f89af1":"# change wage according to M\/K\nfifa19['Wage'] = fifa19['Wage'].apply(lambda x: x.replace('\u20ac',''))\nfifa19['Wage'] = (fifa19['Wage'].replace(r'[KM]+$', '', regex=True).astype(float) * \n                   fifa19['Wage'].str.extract(r'[\\d\\.]+([KM]+)', expand=False).fillna(1).replace(['K','M'], [10**3, 10**6]).astype(int))\nfifa19['Wage']","410b8947":"# check missing value\nfifa19.isnull().sum()\n\n# drop missing value\nfifa19.drop(fifa19[fifa19['International Reputation'].isnull()].index, axis=0, inplace=True)\n\n# drop players with Value = 0\nfifa19.drop(fifa19[fifa19['Value'] == 0].index, axis=0, inplace=True)\n\n# no more missing value\nfifa19.isnull().sum().sum()","b0a23d77":"# change Weight from object to float, remove 'lbs'\nfifa19['Weight'] = fifa19['Weight'].astype(str)\nfifa19['Weight'] = fifa19['Weight'].apply(lambda x: x.replace('lbs',''))\nfifa19['Weight'] = fifa19['Weight'].apply(lambda x: float(x))\n","918f585c":"# change Weight from object to float, remove 'lbs'\ndef convert_weight(wt):\n    foot = float(wt.split(\"'\")[0])\n    inch = float(wt.split(\"'\")[1])\n    \n    return (foot*12 + inch)\n\nfifa19['Height'] = fifa19['Height'].astype(str)\nfifa19['Height'] = fifa19['Height'].apply(convert_weight)\n","5cdeab99":"corr_mt = fifa19.corr()\n\nplt.figure(figsize=(18,18))\nsns.heatmap(corr_mt, square=True)\nplt.show()","db1e9e46":"fifa19['Position'].value_counts()","472e53d1":"\nDefense_men = ['RWB', 'LWB', 'RCB', 'LCB', 'RB', 'LB', 'CB']\nMid_fielder = ['CM', 'RM', 'LM', 'CAM', 'CDM', 'LCM', 'RCM', 'LAM', 'RAM', 'RDM', 'LDM']\nFront = ['ST', 'LW', 'RW', 'LS', 'RS', 'CF', 'RF', 'LF']\n\nfifa19['Position'].replace(Defense_men, 'Defender', inplace=True)\nfifa19['Position'].replace(Mid_fielder, 'Midfield', inplace=True)\nfifa19['Position'].replace(Front, 'Attacker', inplace=True)","a3753a2c":"fifa19['Position'].value_counts()","cdb8a203":"plt.figure(figsize=(8,8))\nsns.distplot(fifa19['Value'], color='darkgoldenrod')\nplt.title('Value of players', size=16)\nplt.show()","24931a9f":"fig, ax = plt.subplots(2,2, figsize=(16,16))\n\nsns.distplot(fifa19[fifa19.Position == 'Attacker']['Value'], color='r', ax=ax[0,0])\nax[0,0].set_title('Attacker')\n\nsns.distplot(fifa19[fifa19.Position == 'Midfield']['Value'], color='indigo', ax=ax[0,1])\nax[0,1].set_title('Midfield')\n\nsns.distplot(fifa19[fifa19.Position == 'Defender']['Value'], color='g', ax=ax[1,0])\nax[1,0].set_title('Defender')\n\nsns.distplot(fifa19[fifa19.Position == 'GK']['Value'], color='darkgoldenrod', ax=ax[1,1])\nax[1,1].set_title('GK')\n\nfig.suptitle('Value of Players in Different Position', size=16)\nplt.show()","23453d24":"plt.figure(figsize=(8,8))\nsns.distplot(np.log(fifa19['Value']), color='darkgoldenrod')\nplt.show()","43ddc261":"fig, ax = plt.subplots(2,2, figsize=(16,16))\n\nsns.distplot(np.log(fifa19[fifa19.Position == 'Attacker']['Value']), color='r', ax=ax[0,0])\nax[0,0].set_title('Attacker')\n\nsns.distplot(np.log(fifa19[fifa19.Position == 'Midfield']['Value']), color='indigo', ax=ax[0,1])\nax[0,1].set_title('Midfield')\n\nsns.distplot(np.log(fifa19[fifa19.Position == 'Defender']['Value']), color='g', ax=ax[1,0])\nax[1,0].set_title('Defender')\n\nsns.distplot(np.log(fifa19[fifa19.Position == 'GK']['Value']), color='darkgoldenrod', ax=ax[1,1])\nax[1,1].set_title('GK')\n\nfig.suptitle('Value of Players in Different Position', size=16)\nplt.show()","acfb5bfe":"fig, ax = plt.subplots(figsize=(10,6))\n\nsns.kdeplot(np.log(fifa19[fifa19.Position == 'Attacker']['Value']), shade=True, color=\"r\", label=\"Attacker\", ax=ax)\nsns.kdeplot(np.log(fifa19[fifa19.Position == 'Midfield']['Value']), shade=True, color=\"indigo\", label=\"Midfield\", ax=ax)\nsns.kdeplot(np.log(fifa19[fifa19.Position == 'Defender']['Value']), shade=True, color=\"g\", label=\"Defender\", ax=ax)\nsns.kdeplot(np.log(fifa19[fifa19.Position == 'GK']['Value']), shade=True, color=\"darkgoldenrod\", label=\"GK\", ax=ax)\n\nax.set_xlabel(\"log(Value)\")\nax.set_ylabel(\"Density\")\n\nplt.show()","bd53c579":"fig, ax = plt.subplots(1,2, figsize=(16,8))\n\nsns.distplot(fifa19['Height'], kde=False, color='darkred', ax=ax[0])\nax[0].set_title('Height')\n\nsns.distplot(fifa19['Weight'], kde=False, color='darkorchid',ax=ax[1])\nax[1].set_title('Weight')\n\nplt.show()","2e09526c":"fifa19['Position']","90ffc4c4":"# one hot, drop GK to avoid colinearity\none_hot_position = pd.get_dummies(fifa19['Position'].replace('GK', np.nan), prefix='Position')\nfifa19 = pd.concat([fifa19, one_hot_position], axis=1)\nfifa19.drop('Position', axis=1, inplace=True)","19b400c5":"fifa19['Body Type'].value_counts()","4a43f84a":"# fix body type\nfifa19['Body Type'] = fifa19['Body Type'].replace(['Messi', 'C. Ronaldo', 'Neymar', 'Courtois', 'PLAYER_BODY_TYPE_25', 'Shaqiri', 'Akinfenwa'], 'Normal')\nfifa19['Body Type']","ab55b530":"# one hot body type\none_hot_body_type = pd.get_dummies(fifa19['Body Type'].replace('Normal', np.nan), prefix='Body_Type')\nfifa19 = pd.concat([fifa19, one_hot_body_type], axis=1)\nfifa19.drop('Body Type', axis=1, inplace=True)","0964cb2a":"# split 'Work Rate' into two values: attack \/ defense\nwork_rate_list = fifa19['Work Rate'].apply(lambda x: x.split('\/ '))\n\n# create new columns\nfifa19['Attack_WR'] = work_rate_list.apply(lambda x: x[0])\nfifa19['Defense_WR'] = work_rate_list.apply(lambda x: x[1])\n\n# drop original 'Work Rate'\nfifa19.drop('Work Rate', axis=1, inplace=True)\n\n# reassign values, {'Low':1, 'Medium':2, 'High':3}\nfifa19['Attack_WR'].replace({'Low':1, 'Medium':2, 'High':3}, inplace=True)\nfifa19['Defense_WR'].replace({'Low':1, 'Medium':2, 'High':3}, inplace=True)\n\nfifa19[['Attack_WR', 'Defense_WR']]","b68968d6":"fifa_data = fifa19.copy()\nfifa_data.drop(['Wage', 'Name'], axis=1, inplace=True)\nfifa_data['Value'] = np.log(fifa_data['Value'])","9b4ef8d3":"X = fifa_data.drop('Value', axis=1)\ny = fifa_data['Value']\n\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size = 0.3, random_state = 123)\n\nprint('train x:', train_x.shape)\nprint('train y:', train_y.shape)\nprint('valid x:', valid_x.shape)\nprint('valid y:', valid_y.shape)","064b2ea2":"lr = LinearRegression()\nlr.fit(train_x, train_y)\n\n# print errors\ntrain_y_pred = lr.predict(train_x)\nMAE_train = mean_absolute_error(train_y, train_y_pred)\nprint('MAE_train: ', MAE_train)\n\nvalid_y_pred = lr.predict(valid_x)\nMAE_valid = mean_absolute_error(valid_y, valid_y_pred)\nprint('MAE_valid: ', MAE_valid)\n\n#residual plot\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y_pred-train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y_pred-valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Residual vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Residual')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 14, color='red')\nplt.show()\n\n#real vs predicted\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Real vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Real Value')\nplt.legend(loc = 'upper left')\nplt.show()","a146fedc":"print(\"Start Round 1\")\nridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(train_x, train_y)\nalpha = ridge.alpha_\nprint(\"Best alpha Round 1:\", alpha)\n\nprint(\"Start Round 2\")\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], cv = 10)\nridge.fit(train_x, train_y)\nalpha = ridge.alpha_\nprint(\"Best alpha Round 2:\", alpha)\n\nridge = Ridge(alpha=alpha)\nridge.fit(train_x, train_y)\n\n# print errors\ntrain_y_pred = ridge.predict(train_x)\nMAE_train = mean_absolute_error(train_y, train_y_pred)\nprint('MAE_train: ', MAE_train)\n\nvalid_y_pred = ridge.predict(valid_x)\nMAE_valid = mean_absolute_error(valid_y, valid_y_pred)\nprint('MAE_valid: ', MAE_valid)\n\n#residual plot\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y_pred-train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y_pred-valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Residual vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Residual')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 14, color='red')\nplt.show()\n\n#real vs predicted\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Real vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Real Value')\nplt.legend(loc = 'upper left')\nplt.show()","ab0172fd":"# ridge.coef_\n# np.where(ridge.coef_ > 2.20570045e-01)","5d377869":"print(\"Start Round 1\")\nlasso = LassoCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nlasso.fit(train_x, train_y)\nalpha = lasso.alpha_\nprint(\"Best alpha Round 1:\", alpha)\n\nprint(\"Start Round 2\")\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], cv = 10)\nlasso.fit(train_x, train_y)\nalpha = lasso.alpha_\nprint(\"Best alpha Round 2:\", alpha)\n\nlasso = Lasso(alpha=alpha)\nlasso.fit(train_x, train_y)\n\n# print errors\ntrain_y_pred = lasso.predict(train_x)\nMAE_train = mean_absolute_error(train_y, train_y_pred)\nprint('MAE_train: ', MAE_train)\n\nvalid_y_pred = lasso.predict(valid_x)\nMAE_valid = mean_absolute_error(valid_y, valid_y_pred)\nprint('MAE_valid: ', MAE_valid)\n\n#residual plot\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y_pred-train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y_pred-valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Residual vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Residual')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 14, color='red')\nplt.show()\n\n#real vs predicted\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Real vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Real Value')\nplt.legend(loc = 'upper left')\nplt.show()","cc62eacc":"l1_ratio = [.1, .5, .7, .9, .95, .99, 1]\n\nenet = ElasticNetCV(l1_ratio=l1_ratio)\nenet.fit(train_x, train_y)\nbest_ratio = enet.l1_ratio_\nprint(\"Best alpha Round 1:\", best_ratio)\n\n\nenet = ElasticNet(l1_ratio=best_ratio)\nenet.fit(train_x, train_y)\n\n# print errors\ntrain_y_pred = enet.predict(train_x)\nMAE_train = mean_absolute_error(train_y, train_y_pred)\nprint('MAE_train: ', MAE_train)\n\nvalid_y_pred = enet.predict(valid_x)\nMAE_valid = mean_absolute_error(valid_y, valid_y_pred)\nprint('MAE_valid: ', MAE_valid)\n\n#residual plot\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y_pred-train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y_pred-valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Residual vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Residual')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 14, color='red')\nplt.show()\n\n#real vs predicted\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Real vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Real Value')\nplt.legend(loc = 'upper left')\nplt.show()","d16c3efd":"xgb_model = XGBRegressor(n_estimators = 100 , random_state = 50)\nxgb_model.fit(train_x, train_y)","7a38fb85":"# print errors\ntrain_y_pred = xgb_model.predict(train_x)\nMAE_train = mean_absolute_error(train_y, train_y_pred)\nprint('MAE_train: ', MAE_train)\n\nvalid_y_pred = xgb_model.predict(valid_x)\nMAE_valid = mean_absolute_error(valid_y, valid_y_pred)\nprint('MAE_valid: ', MAE_valid)","049a3eb3":"# tune n_estimator\n\nmodel = XGBRegressor(eval_metric='mae')\n\n# n_estimators : 50 ~ 400\nn_estimators = range(50, 400, 50)\nparam = dict(n_estimators = n_estimators)\ngrid_search = GridSearchCV(model, param, scoring='neg_mean_absolute_error', n_jobs = -1, cv=5)\ngrid_result = grid_search.fit(train_x, train_y)\n\nprint('best cv score:', grid_result.best_score_, grid_result.best_params_)\n\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nfor mean, std, param in zip(means, stds, params):\n    print('%f, (%f),%r' % (mean, std, param))\n\nplt.errorbar(n_estimators, means, yerr=stds)\nplt.title('CV error vs n_estimators')\nplt.show()\n","a644d3bc":"# tune learning rate\n\nmodel = XGBRegressor(n_estimators=200, eval_metric='mae')\n\n# learning rate\nlearning_rate = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3]\nparam = dict(learning_rate = learning_rate)\ngrid_search = GridSearchCV(model, param, scoring='neg_mean_absolute_error', n_jobs = -1, cv=5)\ngrid_result = grid_search.fit(train_x, train_y)\n\nprint('best cv score:', grid_result.best_score_, grid_result.best_params_)\n\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\n\nfor mean, std, param in zip(means, stds, params):\n    print('%f, (%f),%r' % (mean, std, param))\n\nplt.errorbar(learning_rate, means, yerr=stds)\nplt.title('CV error vs learning rate')\nplt.show()","6a93e1b9":"# # tune learning rate\n\n# model = XGBRegressor(eval_metric='mae')\n\n\n# # learning rate and n_estimator\n# learning_rate = [0.05, 0.08, 0.1, 0.12, 0.15]\n# n_estimators = [100, 150, 200, 250, 300]\n# param = dict(learning_rate = learning_rate, n_estimators=n_estimators)\n# grid_search = GridSearchCV(model, param, scoring='neg_mean_absolute_error', n_jobs = -1, cv=5)\n# grid_result = grid_search.fit(train_x, train_y)\n\n# print('best cv score:', grid_result.best_score_, grid_result.best_params_)\n\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n\n# for mean, std, param in zip(means, stds, params):\n#     print('%f, (%f),%r' % (mean, std, param))\n\n# plt.errorbar(learning_rate, means, yerr=stds)\n# plt.title('CV error vs learning rate')\n# plt.show()","a922e441":"# # max_depth\n\n# model = XGBRegressor(n_estimators=300, learning_rate=0.1)\n\n# max_depth = [6, 7, 8, 9, 10]\n# param = dict(max_depth = max_depth)\n# grid_search = GridSearchCV(model, param, scoring='neg_mean_absolute_error', n_jobs = -1, cv=5)\n# grid_result = grid_search.fit(train_x, train_y)\n\n# print('best cv score:', grid_result.best_score_, grid_result.best_params_)\n\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n\n# for mean, std, param in zip(means, stds, params):\n#     print('%f, (%f),%r' % (mean, std, param))","79defd85":"# # subsample\n\n# model = XGBRegressor(n_estimators=300, learning_rate=0.1)\n\n# subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n# param = dict(subsample = subsample)\n# grid_search = GridSearchCV(model, param, scoring='neg_mean_absolute_error', n_jobs = -1, cv=5)\n# grid_result = grid_search.fit(train_x, train_y)\n\n# print('best cv score:', grid_result.best_score_, grid_result.best_params_)\n\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n\n# for mean, std, param in zip(means, stds, params):\n#     print('%f, (%f),%r' % (mean, std, param))","4e77fcbe":"# # colsample_bytree\n\n# model = XGBRegressor(n_estimators=300, learning_rate=0.1)\n\n# colsample_bytree = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n# param = dict(colsample_bytree = colsample_bytree)\n# grid_search = GridSearchCV(model, param, scoring='neg_mean_absolute_error', n_jobs = -1, cv=5)\n# grid_result = grid_search.fit(train_x, train_y)\n\n# print('best cv score:', grid_result.best_score_, grid_result.best_params_)\n\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n\n# for mean, std, param in zip(means, stds, params):\n#     print('%f, (%f),%r' % (mean, std, param))","d48dc0c4":"# # colsample_bylevel\n\n# model = XGBRegressor(n_estimators=300, learning_rate=0.1)\n\n# colsample_bylevel = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n# param = dict(colsample_bylevel = colsample_bylevel)\n# grid_search = GridSearchCV(model, param, scoring='neg_mean_absolute_error', n_jobs = -1, cv=5)\n# grid_result = grid_search.fit(train_x, train_y)\n\n# print('best cv score:', grid_result.best_score_, grid_result.best_params_)\n\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n\n# for mean, std, param in zip(means, stds, params):\n#     print('%f, (%f),%r' % (mean, std, param))","f08b30f0":"#            \nxgb_model = XGBRegressor(n_estimators = 300 , random_state = 50, learning_rate=0.1, max_depth=8)\nxgb_model.fit(train_x, train_y)\n\n# print errors\ntrain_y_pred = xgb_model.predict(train_x)\nMAE_train = mean_absolute_error(train_y, train_y_pred)\nprint('MAE_train: ', MAE_train)\n\nvalid_y_pred = xgb_model.predict(valid_x)\nMAE_valid = mean_absolute_error(valid_y, valid_y_pred)\nprint('MAE_valid: ', MAE_valid)","f05dca7b":"#residual plot\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y_pred-train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y_pred-valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Residual vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Residual')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 14, color='red')\nplt.show()\n\n#real vs predicted\nplt.figure(figsize=(8,6))\nplt.scatter(x=train_y_pred,y=train_y, c = 'green', marker='s', label='Training')\nplt.scatter(x=valid_y_pred,y=valid_y, c = 'yellow', marker='s', label='Validation')\nplt.title('Real vs Predicted')\nplt.xlabel('Prediction')\nplt.ylabel('Real Value')\nplt.legend(loc = 'upper left')\nplt.show()","6c1e562c":"# importance_type = ['weight', 'gain', 'cover', 'total_gain', 'total_cover']\nimp = xgb_model.get_booster().get_score(importance_type='gain')\n\nkeys = imp.keys()\nvalues = imp.values()\n\nimp_df = pd.DataFrame({'Attribute':list(keys), 'Gain':list(values)})\nimp_df.sort_values(by='Gain', ascending=False, inplace=True)\n\nplt.figure(figsize=(24,6))\np = sns.barplot(data=imp_df.iloc[0:10], x='Attribute', y='Gain', palette='Set3')\n\nplt.xticks(rotation=65)\nplt.show()","afeb3a4b":"X = fifa_data.drop(['Value', 'Overall', 'Potential'], axis=1)\ny = fifa_data['Value']\n\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size = 0.3, random_state = 123)\n\nprint('train x:', train_x.shape)\nprint('train y:', train_y.shape)\nprint('valid x:', valid_x.shape)\nprint('valid y:', valid_y.shape)","5ab13421":"#            \nxgb_model = XGBRegressor(n_estimators = 300 , random_state = 50, learning_rate=0.1, max_depth=8)\nxgb_model.fit(train_x, train_y)\n\n# print errors\ntrain_y_pred = xgb_model.predict(train_x)\nMAE_train = mean_absolute_error(train_y, train_y_pred)\nprint('MAE_train: ', MAE_train)\n\nvalid_y_pred = xgb_model.predict(valid_x)\nMAE_valid = mean_absolute_error(valid_y, valid_y_pred)\nprint('MAE_valid: ', MAE_valid)","07accc14":"imp_df","dd3326a9":"# importance_type = ['weight', 'gain', 'cover', 'total_gain', 'total_cover']\nimp = xgb_model.get_booster().get_score(importance_type='gain')\n\nkeys = imp.keys()\nvalues = imp.values()\n\nimp_df = pd.DataFrame({'Attribute':list(keys), 'Gain':list(values)})\nimp_df.sort_values(by='Gain', ascending=False, inplace=True)\n\nplt.figure(figsize=(24,6))\np = sns.barplot(data=imp_df.iloc[0:10], x='Attribute', y='Gain', palette='Set3')\n\nplt.xticks(rotation=65)\nplt.show()","b6ad9396":"## Body Type\n* There are some speical entries, where body-type = player-name\n* After some reasearch, it could be intentional that they have special body-types, e.g. Messi is a lot stronger than players of his height. Ronaldo is more flexible compare to other players that are 6'2\"\n* 'PLAYER_BODY_TYPE_25' is M. Salah from Liverpool\n* For now, assign them body-type of 'normal'","2abf164e":"## One hot transformation on Position\n\n* Drop GK to avoid colinearity","b81b6d9f":"## Distribution on Height and Weight","5d4d073e":"### column subsampling for each split in a tree\n\nbest cv score: -0.04070198596075532 {'colsample_bylevel': 1.0}","d3c2ad28":"best cv score: -0.04070198596075532 {'colsample_bytree': 1.0}\n\nusing full columns","23679bdb":"## Position of players\n\nThere are too many `Position` on the field, but after some reasearch, all `Position` can be grouped into four categories:\n* Attacker\n* Midfield\n* Defender\n* Goal Keeper","95ae61df":"### Ordinary Least Squares","af66bbd4":"### Elastic Net","21ebae02":"# What determines a player's Value?\n\nThis project will try to build a model that predicts a player's `Value`, using his attributes. It will also display feature importance to show which of those attributes play a key factor in his `Value`.\n\nThe dataset is from FIFA 2019. Explanatory variables includes the players' `Name`, `Age`, `International Reputation`, along with the position he plays, various rating on passing, tackling, shot power and stanima etc.\n\n* At first, some of the traditional regression methods are used, the results were accpetable.\n\n* Later on, **XGBRegressor** was used and it out-performed the traditonal method by a large margin.","dcaa2da9":"## After removing \"Overall\" and \"Potential\":\n\nNow the top 5 attributes are\uff1a\n* Reactions\n* Ball Control\n* Composure\n* Tackle\n* Short Passing\n\nAll these are fundamental skills of a soccer player, we see on TV from time to time training footages of soccer players, and quite often they are indeed doing those basic training.<br>\n\nInterestingly, `International Reputation`, `Age`, `Height` and `Weight` did not play a big part, neither do which `Position` of the player on the field.","ef0f54c9":"## Change the following attribute to numerical\n\nRight now these have special symbols attached to them:\n\n* `Value`\n* `Wage`\n* `Height`\n* `Weight`","4404982a":"![image.png](attachment:image.png)","5b7d7c98":"best cv score: -0.04070198596075532 {'learning_rate': 0.1, 'n_estimators': 300}","4f39f129":"# Value is highly skewed, therefore taking log(Value)","dcff273b":"# Model Fitting\n\nFour regression Models are fitted:\n* Ordinary Least Dquares\n* Ridge Regression\n* LASSO\n* Elastic Net\n\nAll the models performed OK. <br>\nThe plot of true_value vs. predictions seemed a decent fit.<br><br>\n\nBut soon we will see **XGBoost** out performed them all.","f634ff4f":"### XGBoost\n\nThe basic model already out-performs all the traditional regression models above. With:\n\n* mean absolute error on training set = 0.0254\n* mean absolute error on testing set = 0.0473","6d0308b3":"### Tune max_depth\nbest cv score: -0.03992976590469692 {'max_depth': 8}","4a1cfbe2":"## Work Rate\n\n`Work Rate` determines how far a player is willing to leave his position to participate the game\n\n* 'High' in defence WR for a defender means he will push far into the front to participate in offense\n* 'Low' in denfence WR for an attacker means he will stay in front even the other team is attacking\n\nOriginal data stores `Work Rate` in the formate of Offense\/Defense in on column\n\n* The code will separate out the WR, assign separate column for Offense and Defense\n* Then it will replace Low, Medium, High with numerical values 1, 2, 3","b2747c33":"## Load Libraries","97879366":"## The Value of players at different position seems not differ by much","493b1202":"### Tuning Stochastic Gradient Boosting\n### row subsampling\n\nbest cv score: -0.04070198596075532 {'subsample': 1.0}\n\nusing all rows","58d696f3":"### LASSO Regression","69b4b187":"### Search for best combination of n_estimator and learning_rate","81964994":"## XGBoost Hyperparameter Tuning","fc6b0786":"### column subsampling before creating a tree","4775ebd9":"## Values look much more centered after taking log","eb99a88f":"### Ridge Regression","6e876a3e":"### Fitting final model\n\n* n_estimataor = 300\n* learning_rate = 0.1\n* max_depth = 8\n\n### The result is better than before tuning\n\n### Before:\nMAE_train:  0.02536156163991279 <br>\nMAE_valid:  0.04734808475285998\n\n### After\nMAE_train:  0.008882759918053653 <br>\nMAE_valid:  0.0384808490986644\n\nThe residual plot and true vs. predicted plot are also much better.","63110973":"## Heatmap\n\n* Value and Wage are too closely correlated, in this project Wage will be excluded when building the model.","c81ad732":"### Tune NO. of estimator\n\nthe graph is pretty flat after n_estimator = 200","b639e32d":"# Feature Importance\n\n### The top 3 attribute that determines a player's value are:\n* Overall\n* Potential\n* Age\n\nBut this isn't quite satisfying. Since `Overall` and `Potential` are vague terms, `Overall` is likely made up of all the attributes that belongs to a player.<br>\n\nTherefore, I will try remove `Overall` and `Potential` from the model, and see whether this will give a more clear indication of important attributes of a player.\n","62302467":"### Tune Learning Rate\n\nbest cv score: -0.041570600716031705 {'learning_rate': 0.1}"}}