{"cell_type":{"4e41fcce":"code","b0dfd3bd":"code","b5fc4afa":"code","c9311e2a":"code","2c9d9695":"code","150f2aae":"code","2aaf7b77":"code","ce374e45":"code","c9c88610":"code","06c81fa6":"code","7197fdb4":"code","ef39cae7":"code","e59f0350":"code","f0e69070":"code","166c153c":"code","7a098f4f":"code","581f6d9f":"code","49515ba7":"code","205f09ce":"code","71865c2d":"code","924eec64":"code","4b714a5c":"code","cc0f2e5a":"code","498bf48e":"code","747cab61":"code","9824144f":"code","269f2031":"code","06ab3836":"code","c3e2ed35":"code","0f1011b9":"markdown","47dd1bb8":"markdown","e3eaab33":"markdown","5e8ec1b1":"markdown","191b26bd":"markdown","f289baac":"markdown","7d921abf":"markdown","cad5c32f":"markdown","ef8f158d":"markdown","d444096c":"markdown","f5ec7a14":"markdown","55ffff85":"markdown","41b0e8b9":"markdown","c8759db9":"markdown","310cfd34":"markdown","ee47a880":"markdown","656a2fc0":"markdown","3c18c97c":"markdown","1cb5eb3e":"markdown","d12ec766":"markdown","e23f0286":"markdown","5b3f3da4":"markdown","a2b62f61":"markdown","770648b2":"markdown","ce14317f":"markdown","ca6802b4":"markdown","a200ae1a":"markdown","6d86d330":"markdown","305e96d5":"markdown","2ffaa732":"markdown","74c37d31":"markdown","c611b3f3":"markdown","74fa561d":"markdown","09bcb5ff":"markdown","794a3285":"markdown"},"source":{"4e41fcce":"import os\nimport glob\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.autograd import Function\n\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport pandas as pd\nimport seaborn as sn\n\nfrom tqdm import tqdm","b0dfd3bd":"data_root_folder = '..\/input\/singlecellsegmentation\/SingleCellSegmentation\/'\nclass BasicDataset(Dataset):\n    # This function takes folder name ('train', 'valid', 'test') as input and creates an instance of BasicDataset according to that fodler.\n    def __init__(self, folder):\n        self.folder = os.path.join(data_root_folder, folder)\n        self.imgs_dir = os.path.join(self.folder, 'image')\n        self.masks_dir = os.path.join(self.folder, 'mask')\n        \n        self.imgs_file = glob.glob(os.path.join(self.imgs_dir, '*.png'))\n        self.masks_file = glob.glob(os.path.join(self.masks_dir, '*.png'))\n        \n        assert len(self.imgs_file) == len(self.masks_file), 'There are some missing images or masks in {0}'.format(folder)\n        \n        self.ids = list([i+1 for i in range(len(self.imgs_file))])\n            \n    # This function returns the lenght of the dataset (AKA number of samples in that set)\n    def __len__(self):\n        return len(self.ids)\n    \n    \n    # This function takes an index (i) which is between 0 to `len(BasicDataset)` (The return of the previous function), then returns RGB image, \n    # mask (Binary), and the index of the file name (Which we will use for visualization). The preprocessing step is also implemented in this function.\n    def __getitem__(self, i):\n        idx = self.ids[i]\n        img = cv2.imread(os.path.join(self.imgs_dir, 'image_{0:04d}.png'.format(idx)), cv2.IMREAD_COLOR)\n        mask = cv2.imread(os.path.join(self.masks_dir, 'mask_{0:04d}.png'.format(idx)), cv2.IMREAD_GRAYSCALE)\n\n        # Convert BGR to RGB\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        #Resize all images from 512 to 256 (H and W)\n        img = cv2.resize(img, (256,256))\n        mask = cv2.resize(mask, (256,256))\n        \n        # Scale between 0 to 1\n        img = np.array(img) \/ 255.0\n        mask = np.array(mask) \/ 255.0\n        \n        # Make sure that the mask are binary (0 or 1)\n        mask[mask <= 0.5] = 0.0\n        mask[mask > 0.5] = 1.0\n        \n        mask = np.expand_dims(mask, axis=0)\n        \n        # HWC to CHW\n        img = np.transpose(img, (2, 0, 1))\n        \n\n        return {\n            'image': torch.from_numpy(img).type(torch.FloatTensor),\n            'mask': torch.from_numpy(mask).type(torch.FloatTensor),\n            'img_id': idx\n        }","b5fc4afa":"# Create train, validation, and test dataset instances\ntrain_dataset = BasicDataset('train')\nvalid_dataset = BasicDataset('valid')\ntest_dataset = BasicDataset('test')\n\n\nplt.figure(figsize=(12,8))\nplt.title('Data split distribution')\nplt.bar(0, len(train_dataset), label='Train')\nplt.bar(1, len(valid_dataset), label='Validation')\nplt.bar(2, len(test_dataset), label='Test')\nplt.ylabel('Number of samples')\nplt.xticks([0,1,2],['Train', 'Validation', 'Test'])\nplt.legend()\nplt.show()","c9311e2a":"sample = np.random.randint(0, len(train_dataset))\ndata = train_dataset.__getitem__(sample)\nx = data['image']\ny = data['mask']\nidx = data['img_id']\n\nprint(f'x shape is {x.shape}')\nprint(f'y shape is {y.shape}')\n\nplt.figure(figsize=(12, 8))\nplt.suptitle(f'Sample {idx:04d}')\nimg = np.transpose(x, (1,2,0))\nmask = y[0]\nplt.subplot(1, 2, 1)\nplt.title('Image')\nplt.imshow(img)\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.title('Mask')\nplt.imshow(mask, cmap='gray')\nplt.axis('off')\nplt.tight_layout()\nplt.show()","2c9d9695":"def dice_coeff_binary(y_pred, y_true):\n    \"\"\"Values must be only zero or one.\"\"\"\n    eps = 0.0001\n    inter = torch.dot(y_pred.view(-1), y_true.view(-1))\n    union = torch.sum(y_pred) + torch.sum(y_true)\n    return ((2 * inter.float() + eps) \/ (union.float() + eps)).cpu().numpy(), union.float().cpu().numpy()\n    \ndef dice_coeff_multiclass(y_pred, y_true, n_classes):\n    \"\"\"Dice coeff for batches for both multi-class and binary segmentation\"\"\"\n    \"\"\"Classes must be zero indexed, i.e 0, 1, 2, ... , n_classes, as 0 index stands for background.\"\"\" \n    \"\"\"The DICE wouldn't count the 0 (background) index.\"\"\"\n    \"\"\"y_pred and y_true must be tensors with the same shape.\"\"\"\n    \n    assert y_pred.shape == y_true.shape, 'y_true {0} and y_pred {1} must have the same shape.'.format(y_true.shape, y_pred.shape)\n    \n    # Make sure that the masks' shapes are (Batch, 1, W, H)\n    while len(y_pred.shape) < 4:\n        y_pred = y_pred.unsqueeze(0)\n        y_true = y_true.unsqueeze(0)\n    \n    s = np.zeros((y_pred.shape[0])) # Make a variable to report the dices per each sample in the bacth.\n    \n    # Loop over samples in the batch\n    for i, c in enumerate(zip(y_pred, y_true)):\n        dice_score = 0\n        if n_classes == 1:  # Binary segmentation (Automatically the background will be disregarded)\n            dice_score, _ = dice_coeff_binary(c[0], c[1])\n        else:               # Multi-class segmentation\n            class_dices = np.zeros((n_classes,))\n            for j in range(n_classes):\n                if j == 0: # Skip 0 index as background\n                    continue\n                \n                # Pick up all the pixels in both Ground Truth and estimated mask that has been assigned as class j. Then make a binary map to be passed to 'dice_coeff_binary' function\n                es = (c[0]==j).float()\n                gt = (c[1]==j).float()\n                \n                c_dice, union = dice_coeff_binary(es, gt)\n                if union == 0: # Skip if there are no pixel assigned to class j neither on estimate nor on true mask.\n                    continue\n                else:\n                    class_dices[j] = c_dice\n            if np.all(class_dices == 0.0): # This happens when there is no classes presented in the sample and everything is predicted as background. Thus, DICE is 1.\n                dice_score = 1.0\n            else:\n                dice_score = class_dices[class_dices != 0].mean()\n            \n        s[i] = dice_score\n        \n    # The out put would be a numpy array with shape (Batch,) that contains Dice score for each sample in the batch.\n    return s","150f2aae":"# def train_net(net, epochs=5, batch_size=1, lr=0.001):\n    \n#     if not os.path.isdir('{0}'.format(net.name)):\n#         os.mkdir('{0}'.format(net.name))\n    \n#     # Check if GPU is available\n#     is_cuda = torch.cuda.is_available()\n    \n#     if is_cuda:\n#         net.cuda()\n    \n#     device = torch.device('cuda' if is_cuda else 'cpu')\n#     train = BasicDataset('train')\n#     val = BasicDataset('valid')\n    \n#     n_train = len(train)\n#     n_val = len(val)\n    \n#     train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n#     val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n    \n#     mask_type = torch.float32 if net.n_classes == 1 else torch.long\n\n#     # Change the optimizer and learning rate scheduler if these don't suits your model.\n#     optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n    \n#     # This function is a general mode for multi-class U-Net; however, we only have one class. Therefore, only BCEWithLogitsLoss will be used.\n#     # If you wish to use a different Loss function change 'criterion' to what desires your model.\n#     # A quick reminder, the input of both Loss functions (CrossEntropyLoss and BCEWithLogitsLoss) doesn't need to be normalized (softmax is internally implemented in loss)\n#     if net.n_classes > 1:\n#         criterion = nn.CrossEntropyLoss()\n#     else:\n#         criterion = nn.BCEWithLogitsLoss()\n    \n    \n#     training_summary = dict()\n#     training_summary['best_epoch'] = -1\n#     training_summary['best_epoch_loss'] = np.finfo(np.float64).max\n#     training_summary['best_epoch_dice'] = 0.0\n    \n#     training_summary['training_loss'] = list()\n#     training_summary['training_dice'] = list()\n#     training_summary['validation_loss'] = list()\n#     training_summary['validation_dice'] = list()        \n    \n#     for epoch in range(epochs):\n        \n#         ################################################################################################################################\n#         ########################################################### Training ###########################################################\n#         ################################################################################################################################\n#         net.train()\n#         epoch_loss = list()\n#         epoch_dice = list()\n#         with tqdm(total=n_train, desc=f'Epoch {epoch + 1}\/{epochs}', unit='img') as pbar:\n#             for batch in train_loader:\n                \n#                 # Load a batch and pass it to the device (CPU or GPU)\n#                 imgs = batch['image']\n#                 true_masks = batch['mask']\n\n#                 imgs = imgs.to(device=device, dtype=torch.float32)\n#                 true_masks = true_masks.to(device=device, dtype=mask_type)\n                \n#                 # Produce the estimated mask using current weights\n#                 masks_pred = net(imgs)\n                \n#                 # Compute the loss\n#                 loss = criterion(masks_pred, true_masks)\n#                 batch_loss = loss.item()\n#                 epoch_loss.append(batch_loss*batch_size) # We need to multiply by batch_size since the batch_loss is the average of the loss over all samples in the batch\n                \n#                 # If it's a ulti-class segmentation, the predicted mask would be made by passing the network output through a softmax (convert it to probabliti between 0 and 1)\n#                 # then take the maximum value index over the channel axis as the final label. \n#                 # In case of binary segmentation, we need to pass values (one channel output of the network) through the Sigmoid function (values between 0 and 1), then threshold it\n#                 # by 0.5 (it can be changed).\n#                 if net.n_classes > 1:\n#                     pred = F.softmax(masks_pred, dim=1)\n#                     pred = torch.argmax(pred, dim=1).unsqueeze(1)\n#                 else:\n#                     pred = torch.sigmoid(masks_pred)\n#                     pred = (pred > 0.5).float()                    # You can change the probablity threshold!\n\n#                 dice_value = dice_coeff_multiclass(pred, true_masks, net.n_classes)\n#                 # Loop over the dice scores of all the samples in the batch\n#                 for dice_score in dice_value:\n#                     epoch_dice.append(dice_score)\n#                 pbar.set_postfix(**{'loss (batch)': batch_loss, \n#                                     'DICE Coef (batch)': dice_value.mean(),\n#                                     'loss (avg)': np.array(epoch_loss).mean(), \n#                                     'DICE Coef (avg)': np.array(epoch_dice).mean()\n#                                    })\n                \n#                 # Reset gradient values\n#                 optimizer.zero_grad()\n                \n#                 # Compute the backward losses\n#                 loss.backward()\n                \n#                 # Clip gradient to avoid sudden changes\n#                 nn.utils.clip_grad_value_(net.parameters(), 0.1)\n                \n#                 # Update weights\n#                 optimizer.step()\n\n#                 pbar.update(imgs.shape[0])\n                \n#         training_summary['training_loss'].append(np.array(epoch_loss).mean())\n#         training_summary['training_dice'].append(np.array(epoch_dice).mean())\n                \n#         ################################################################################################################################\n#         ########################################################## Validation ##########################################################\n#         ################################################################################################################################\n        \n#         net.eval()\n#         valid_epoch_loss = list()\n#         valid_epoch_dice = list()\n        \n#         # This part is almost the same as training with the difference that we will set all layers to evaluation mode (effects some layers such as BN and Dropout) and also\n#         # we don't need to calculate the gradient since we are only evaluating current state of the model. This will speed up the process and cause it to consume less memory.\n#         with tqdm(total=n_val, desc='Validation round', unit='batch') as pbar:\n#             with torch.no_grad():\n#                 for batch in val_loader:\n#                     imgs, true_masks = batch['image'], batch['mask']\n#                     imgs = imgs.to(device=device, dtype=torch.float32)\n#                     true_masks = true_masks.to(device=device, dtype=mask_type)\n\n#                     masks_pred = net(imgs)\n                    \n#                     batch_loss = criterion(masks_pred, true_masks).item()\n#                     valid_epoch_loss.append(batch_loss*batch_size)\n\n#                     if net.n_classes > 1:\n#                         pred = F.softmax(masks_pred, dim=1)\n#                         pred = torch.argmax(pred, dim=1).unsqueeze(1)\n#                     else:\n#                         pred = torch.sigmoid(masks_pred)\n#                         pred = (pred > 0.5).float()                    # You can change the probablity threshold!\n                        \n#                     dice_value = dice_coeff_multiclass(pred, true_masks, net.n_classes)\n#                     for dice_score in dice_value:\n#                         valid_epoch_dice.append(dice_score)\n#                     pbar.set_postfix(**{'loss (batch)': batch_loss, \n#                                         'DICE Coef (batch)': dice_value.mean(),\n#                                         'loss (avg)': np.array(valid_epoch_loss).mean(), \n#                                         'DICE Coef (avg)': np.array(valid_epoch_dice).mean()\n#                                        })\n#                     pbar.update()\n        \n#         training_summary['validation_loss'].append(np.array(valid_epoch_loss).mean())\n#         training_summary['validation_dice'].append(np.array(valid_epoch_dice).mean())\n        \n#         # We choose our best model based on validation dice score\n#         save_as_best_model = False\n#         if np.array(valid_epoch_dice).mean() >= training_summary['best_epoch_dice']:\n#             training_summary['best_epoch'] = epoch\n#             training_summary['best_epoch_loss'] = np.array(valid_epoch_loss).mean()\n#             training_summary['best_epoch_dice'] = np.array(valid_epoch_dice).mean()\n#             save_as_best_model = True\n            \n                \n#         # Now update our learning rate scheduler based on the average dice score over all validation samples\n#         scheduler.step(np.array(valid_epoch_dice).mean())\n\n#         ################################################################################################################################\n#         ###################################################### Saveing Checkpoints #####################################################\n#         ################################################################################################################################\n#         torch.save(net.state_dict(), f'{net.name}\/{net.name}_CP_epoch{epoch + 1}.pth')\n        \n#         # Save this state as the best model, if it is!\n#         if save_as_best_model:\n#             torch.save(net.state_dict(), f'{net.name}\/{net.name}_BestModel.pth')\n        \n#         # Training summary will be saved as a json file for further visualization.\n#         with open(f'{net.name}\/{net.name}_TrainingSummary.json', 'w') as f:\n#             json.dump(training_summary, f)\n","2aaf7b77":"def plot_learning_curve(net):\n    with open(f'{net.name}\/{net.name}_TrainingSummary.json', 'r') as f:\n        summary = json.load(f)\n    \n    is_binary = 'best_epoch_dice' in summary\n    \n    best_epoch = summary['best_epoch']\n    best_epoch_loss = summary['best_epoch_loss']\n    \n    total_epochs = len(summary['training_loss'])\n    \n    fig = plt.figure(figsize=(9, 5))\n    \n    ax1 = fig.add_subplot(111)\n    ax1.set_title(f'Learning curve - {net.name}')\n    \n    ln1 = ax1.plot(range(1,total_epochs + 1), summary['training_loss'], c='tab:blue', label = 'training loss', marker='o', markersize = 5)\n    ln2 = ax1.plot(range(1,total_epochs + 1), summary['validation_loss'], c='tab:orange', label = 'validation loss', marker='o', markersize = 5)\n    ln3 = ax1.axvline(best_epoch + 1, c = 'lime', linewidth=7, alpha = 0.5, label = 'Best epoch')\n#     plt.text()\n    \n    if is_binary:\n        ax2 = ax1.twinx()\n        ln4 = ax2.plot(range(1,total_epochs + 1), summary['training_dice'], c='tab:purple', label = 'training dice', marker='o', markersize = 5)\n        ln5 = ax2.plot(range(1,total_epochs + 1), summary['validation_dice'], c='tab:red', label = 'validation dice', marker='o', markersize = 5)\n        ax2.set_ylabel('DICE score')\n        lns = ln1+ln2+[ln3]+ln4+ln5\n    else:\n        lns = list([ln1, ln2, ln3])\n    \n    lbs = [l.get_label() for l in lns]\n    \n    ax1.legend(lns, lbs)\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    plt.savefig(f'{net.name}\/{net.name}_LearningCurve.png')\n    plt.show()","ce374e45":"# def\n# def test_net(net, epoch=None):\n#     if epoch is None:\n#         net.load_state_dict(torch.load(f'{net.name}\/{net.name}_BestModel.pth'))\n#     else:\n#         net.load_state_dict(torch.load(f'{net.name}\/{net.name}_CP_epoch{epoch}.pth'))\n       \n#     # Create directory to save test masks\n#     if not os.path.isdir(f'{net.name}\/pred_mask'):\n#         os.mkdir(f'{net.name}\/pred_mask')\n        \n#     # Check if GPU is available\n#     is_cuda = torch.cuda.is_available()\n    \n#     if is_cuda:\n#         net.cuda()\n    \n#     device = torch.device('cuda' if is_cuda else 'cpu')\n#     test = BasicDataset('test')\n#     n_test = len(test)\n#     test_loader = DataLoader(test, batch_size=1, shuffle=False, num_workers=8, pin_memory=True)\n    \n#     mask_type = torch.float32 if net.n_classes == 1 else torch.long\n    \n    \n#     # Don't forget to change the criterion, if you are using different loss function during training\n#     #if net.n_classes > 1:\n#         #criterion = nn.CrossEntropyLoss()\n#     #else:\n#     criterion = nn.BCEWithLogitsLoss()\n    \n#     net.eval()\n    \n#     test_loss = list()\n#     test_dice = list()\n    \n#     test_dice_loss_to_save = dict()\n\n#     with tqdm(total=n_test, desc='Testing', unit='img') as pbar:\n#         with torch.no_grad():\n#             for batch in test_loader:\n#                 imgs, true_masks, idx = batch['image'], batch['mask'], batch['img_id'].item() # There is only one id in 'img_id' since the batch size is one.\n#                 imgs = imgs.to(device=device, dtype=torch.float32)\n#                 true_masks = true_masks.to(device=device, dtype=mask_type)\n\n#                 masks_pred = net(imgs)\n                \n#                 test_dice_loss_to_save[f'{idx:04d}'] = dict()\n\n#                 batch_loss = criterion(masks_pred, true_masks).item()\n#                 test_loss.append(batch_loss)\n                \n#                 test_dice_loss_to_save[f'{idx:04d}']['loss'] = batch_loss\n\n#                 if net.n_classes > 1:\n#                     pred = F.softmax(masks_pred, dim=1)\n#                     pred = torch.argmax(pred, dim=1).unsqueeze(1)\n#                 else:\n#                     pred = torch.sigmoid(masks_pred)\n#                     pred = (pred > 0.5).float()                    # You can change the probablity threshold!\n\n#                 dice_value = dice_coeff_multiclass(pred, true_masks, net.n_classes)\n#                 test_dice.append(dice_value) # Since the batch size is 1, you don't need to loop over the samples in the batch\n                \n#                 test_dice_loss_to_save[f'{idx:04d}']['dice'] = dice_value.item()\n                \n#                 pbar.set_postfix(**{'loss (batch)': batch_loss, \n#                                     'DICE Coef (batch)': dice_value.mean(),\n#                                     'loss (avg)': np.array(test_loss).mean(),\n#                                     'DICE Coef (avg)': np.array(test_dice).mean(),\n#                                    })\n#                 # Save predicted mask\n#                 # First we need to remove batch and channel axis (Knowing the batch size is one during test) and then convert it to numpy\n#                 pred_numpy = pred.squeeze(0).squeeze(0).cpu().numpy()\n                \n#                 # After that we need to convert the dynamic range from [0, 1] to [0, 255] to save the predicted mask\n#                 pred_numpy = (pred_numpy * 255).astype(np.uint8)\n                \n#                 # Now we use openCV to save our predicted mask with the name of the original image index\n#                 cv2.imwrite(f'{net.name}\/pred_mask\/pred_mask_{idx:04d}.png', pred_numpy)\n                \n#                 pbar.update()\n    \n#     test_dice = np.array(test_dice)\n#     test_loss = np.array(test_loss)\n    \n    \n#     # Save testset dice score and loss value per each sample in a json file\n#     with open(f'{net.name}\/{net.name}_TestResults.json', 'w') as f:\n#         json.dump(test_dice_loss_to_save, f)\n    \n    \n#     print(f'Average test loss is: {test_loss.mean():.4f}, and test loss std is {test_loss.std():.4f}')\n#     print(f'Average test dice score is: {test_dice.mean():.4f}, and test dice score std is {test_dice.std():.4f}')\n    \n#     print(f'{n_test} predicted masks is saved in \"{net.name}\/pred_mask\" directory.')","c9c88610":"def plot_best_worst_samples(net):\n    \n    with open(f'{net.name}\/{net.name}_TestResults.json', 'r') as f:\n        test_resuls = json.load(f)\n        \n    # Sort based on dice scores\n#     print(test_resuls.items())\n    test_resuls = sorted(test_resuls.items(), key=lambda item: item[1]['dice'])\n    \n    best_samples = test_resuls[-5:]\n    worst_samples = test_resuls[:5]\n    \n    plt_titles = (f'Five best test samples of {net.name}', f'Five worst test samples of {net.name}')\n    \n    idx_list = list([best_samples, worst_samples])\n    \n    for i in range(2):\n        plt.figure(figsize=(12,30))\n        plt.suptitle(plt_titles[i])\n        plt_counter = 0\n        for item in idx_list[i]:\n            idx = int(item[0])\n            dice_score = item[1]['dice']\n            \n            img = cv2.imread(f'..\/input\/singlecellsegmentation\/SingleCellSegmentation\/test\/image\/image_{idx:04d}.png', cv2.IMREAD_COLOR)\n            mask = cv2.imread(f'..\/input\/singlecellsegmentation\/SingleCellSegmentation\/test\/mask\/mask_{idx:04d}.png', cv2.IMREAD_GRAYSCALE)\n            pred_mask = cv2.imread(f'{net.name}\/pred_mask\/pred_mask_{idx:04d}.png', cv2.IMREAD_GRAYSCALE)\n            \n            # Don't forgt to resize the images, like what we did in the Dataset implementation\n            img = cv2.resize(img, (256,256))\n            mask = cv2.resize(mask, (256,256))\n\n            # BGR to RGB\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            plt.subplot(5,3,plt_counter*3 + 1)\n            plt.title(f'Original image - {idx:04d}')\n            plt.imshow(img)\n            plt.axis('off')\n\n            plt.subplot(5,3,plt_counter*3 + 2)\n            plt.title(f'True mask')\n            plt.imshow(mask, cmap='gray')\n            plt.axis('off')\n\n            plt.subplot(5,3,plt_counter*3 + 3)\n            plt.title(f'Predicted mask - Dice score = {dice_score:.4f}')\n            plt.imshow(pred_mask, cmap='gray')\n            plt.axis('off')\n\n            plt_counter += 1\n        plt.tight_layout()\n        plt.savefig(f'{net.name}\/{net.name}_{\"Best\" if i == 0 else \"Worst\"}_Samples.png')\n        plt.show()\n    ","06c81fa6":"def statistical_report(net):\n    total_samples = len(glob.glob(f'{net.name}\/pred_mask\/pred_mask_*.png'))\n\n    Y_true = list()\n    Y_pred = list()\n\n    for i in range(total_samples):\n        true_mask = cv2.imread(f'..\/input\/singlecellsegmentation\/SingleCellSegmentation\/test\/mask\/mask_{i+1:04d}.png', cv2.IMREAD_GRAYSCALE) \/ 255.0\n        pred_mask = cv2.imread(f'{net.name}\/pred_mask\/pred_mask_{i+1:04d}.png', cv2.IMREAD_GRAYSCALE) \/ 255.0\n\n        true_mask = (cv2.resize(true_mask, (256, 256)) > 0.5).astype(np.int)\n        pred_mask = (pred_mask > 0.5).astype(np.int)\n\n        Y_true.append(true_mask.flatten())\n        Y_pred.append(pred_mask.flatten())\n\n    Y_true = np.array(Y_true).flatten()\n    Y_pred = np.array(Y_pred).flatten()\n    \n    print(classification_report(Y_true, Y_pred, target_names=['Background', 'Cell']))\n    \n    cm = confusion_matrix(Y_true, Y_pred, normalize='true')\n    \n    df_cm = pd.DataFrame(cm, index = ['Background', 'Cell'],\n                         columns = ['Background', 'Cell'])\n    plt.figure(figsize = (10,7))\n    plt.title(f'Confusion matrix of {net.name}')\n    sn.heatmap(df_cm, annot=True, fmt='.2%')\n    plt.xlabel('Predicted label')\n    plt.ylabel('True label')\n    plt.savefig(f'{net.name}\/{net.name}_ConfusionMatrix.png')\n    plt.show()\n    \n    ","7197fdb4":"def plot_weights_UNet(net):\n    # Turn off gradients\n    with torch.no_grad():\n        # Search in all layers in the network (However, only the first convolution layer will be shown)\n        for m in net.modules():\n            # Check if that layer (m) is an instance of nn.Conv2d\n            if isinstance(m, nn.Conv2d):\n                # Convert weights to numpy array\n                \n                if m.weight.is_cuda:\n                    filters = m.weight.cpu().numpy()\n                else:\n                    filters = m.weight.numpy()\n                    \n\n                # Get filters weidth and hights\n                filter_h = filters.shape[2]\n                filter_w = filters.shape[3]\n\n                # Compute sqrt of total filters to plot first nxn filters only! (Visualization reasons!)\n                sqrt_n_filters = int(np.sqrt(filters.shape[0]))\n\n\n                plt.figure(figsize=(12, 17))\n                plt.suptitle(f'Weights visualization of {net.name} - First Convolution Layer, first {sqrt_n_filters*sqrt_n_filters} filters of size {filter_w}x{filter_h}')\n\n                # Take minimum and maximum values to display filters' weights in a compareable way\n                vmin = filters.min()\n                vmax = filters.min()\n                for i in range(sqrt_n_filters):\n                    for j in range(sqrt_n_filters):\n\n                        # idx is just a counter\n                        idx = i*sqrt_n_filters+j\n\n                        # Dont forget to convert CHW to HWC\n                        weights = filters[idx].transpose((1,2,0))\n\n                        plt.subplot(sqrt_n_filters, sqrt_n_filters, idx+1)\n                        plt.imshow(weights, vmin=vmin, vmax=vmax)\n                        plt.axis('off')\n                plt.tight_layout()\n                plt.savefig(f'{net.name}\/{net.name}_FirstLayerVisualization.png')\n                plt.show()\n                #leave the function early\n                return","ef39cae7":"# class DoubleConv(nn.Module):\n#     \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n\n#     def __init__(self, in_channels, out_channels, mid_channels=None):\n#         super().__init__()\n#         if not mid_channels:\n#             mid_channels = out_channels\n#         self.double_conv = nn.Sequential(\n#             nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n#             nn.BatchNorm2d(mid_channels),\n#             nn.ReLU(inplace=True),\n#             nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n#             nn.BatchNorm2d(out_channels),\n#             nn.ReLU(inplace=True)\n#         )\n\n#     def forward(self, x):\n#         return self.double_conv(x)\n\n\n# class Down(nn.Module):\n#     \"\"\"Downscaling with maxpool then double conv\"\"\"\n\n#     def __init__(self, in_channels, out_channels):\n#         super().__init__()\n#         self.maxpool_conv = nn.Sequential(\n#             nn.MaxPool2d(2),\n#             DoubleConv(in_channels, out_channels)\n#         )\n\n#     def forward(self, x):\n#         return self.maxpool_conv(x)\n\n\n# class Up(nn.Module):\n#     \"\"\"Upscaling then double conv\"\"\"\n\n#     def __init__(self, in_channels, out_channels, bilinear=True):\n#         super().__init__()\n\n#         # if bilinear, use the normal convolutions to reduce the number of channels\n#         if bilinear:\n#             self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n#             self.conv = DoubleConv(in_channels, out_channels, in_channels \/\/ 2)\n#         else:\n#             self.up = nn.ConvTranspose2d(in_channels , in_channels \/\/ 2, kernel_size=2, stride=2)\n#             self.conv = DoubleConv(in_channels, out_channels)\n\n\n#     def forward(self, x1, x2):\n#         x1 = self.up(x1)\n#         # input is CHW\n#         diffY = x2.size()[2] - x1.size()[2]\n#         diffX = x2.size()[3] - x1.size()[3]\n\n#         x1 = F.pad(x1, [diffX \/\/ 2, diffX - diffX \/\/ 2,\n#                         diffY \/\/ 2, diffY - diffY \/\/ 2])\n#         # if you have padding issues, see\n#         # https:\/\/github.com\/HaiyongJiang\/U-Net-Pytorch-Unstructured-Buggy\/commit\/0e854509c2cea854e247a9c615f175f76fbb2e3a\n#         # https:\/\/github.com\/xiaopeng-liao\/Pytorch-UNet\/commit\/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n#         x = torch.cat([x2, x1], dim=1)\n#         return self.conv(x)\n\n\n# class OutConv(nn.Module):\n#     def __init__(self, in_channels, out_channels):\n#         super(OutConv, self).__init__()\n#         self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n\n#     def forward(self, x):\n#         return self.conv(x)","e59f0350":"# class UNet(nn.Module):\n#     def __init__(self, name, n_channels, n_classes, bilinear=True):\n#         super(UNet, self).__init__()\n#         self.name = name\n#         self.n_channels = n_channels\n#         self.n_classes = n_classes\n#         self.bilinear = bilinear\n\n#         self.inc = DoubleConv(n_channels, 64)\n#         self.down1 = Down(64, 128)\n#         self.down2 = Down(128, 256)\n#         self.down3 = Down(256, 512)\n#         factor = 2 if bilinear else 1\n#         self.down4 = Down(512, 1024 \/\/ factor)\n#         self.up1 = Up(1024, 512 \/\/ factor, bilinear)\n#         self.up2 = Up(512, 256 \/\/ factor, bilinear)\n#         self.up3 = Up(256, 128 \/\/ factor, bilinear)\n#         self.up4 = Up(128, 64, bilinear)\n#         self.outc = OutConv(64, n_classes)\n\n#     def forward(self, x):\n#         x1 = self.inc(x)\n#         x2 = self.down1(x1)\n#         x3 = self.down2(x2)\n#         x4 = self.down3(x3)\n#         x5 = self.down4(x4)\n#         x = self.up1(x5, x4)\n#         x = self.up2(x, x3)\n#         x = self.up3(x, x2)\n#         x = self.up4(x, x1)\n#         logits = self.outc(x)\n#         return logits","f0e69070":"# assert torch.cuda.is_available(), 'Turn on GPU accelerator for a faster training, if you prefer to train on CPU, comment this line.'\n# # Input has three channels and we have one class to be segmented (cell)\n# # In fact, we have two classes, cells and background, but with a probability between 0 to 1 we may seperate those.\n# # For multi-class segmentation, make sure you account for background (0 index) as well. For example if you have two classes cat and dog, you must\n# # account for background too; therefore, n_classes = 3, 0: Background, 1: Cat, 2: Dog.\n# my_UNET = UNet(name='Simple U-Net', n_channels=3,n_classes=1)\n# print(my_UNET)","166c153c":"#train_net(my_UNET, epochs=10, batch_size=1, lr = 0.001)","7a098f4f":"#plot_learning_curve(my_UNET)","581f6d9f":"#test_net(my_UNET)","49515ba7":"#plot_best_worst_samples(my_UNET)","205f09ce":"#statistical_report(my_UNET)","71865c2d":"#plot_weights_UNet(my_UNET)","924eec64":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import init\nimport os\n\ndef init_weights(net, init_type = 'normal', gain = 0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            if init_type == 'normal':\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == 'xavier':\n                init.xavier_normal_(m.weight.data, gain = gain)\n            elif init_type == 'kaiming':\n                init.kaiming_normal_(m.weight.data, a = 0, mode = 'fan_in')\n            elif init_type == 'orthogonal':\n                init.orthogonal_(m.weight.data, gain = gain)\n            else:\n                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print('initialize network with %s' % init_type)\n    net.apply(init_func)\n\n\nclass conv_block(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\n\nclass resconv_block(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv2d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv2d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n        \n        residual =  self.Conv_1x1(x)\n        x = self.conv(x)\n        \n        return residual + x\n\n\nclass up_conv(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv2d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n\t\t    nn.BatchNorm2d(ch_out),\n\t\t\tnn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass Recurrent_block(nn.Module):\n    def __init__(self, ch_out, t = 2):\n        super(Recurrent_block, self).__init__()\n        self.t = t\n        self.ch_out = ch_out\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n\t\t    nn.BatchNorm2d(ch_out),\n\t\t\tnn.ReLU(inplace = True)\n            #nn.LeakyReLu(inplace = True)\n        )\n\n    def forward(self,x):\n        for i in range(self.t):\n            if i == 0:\n                x1 = self.conv(x)\n            x1 = self.conv(x+x1)\n        return x1\n\n\nclass RRCNN_block(nn.Module):\n    def __init__(self, ch_in, ch_out, t = 2):\n        super(RRCNN_block, self).__init__()\n        self.RCNN = nn.Sequential(\n            Recurrent_block(ch_out, t = t),\n            Recurrent_block(ch_out, t = t)\n        )\n        self.Conv_1x1 = nn.Conv2d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n        x = self.Conv_1x1(x)\n        x1 = self.RCNN(x)\n        return x + x1\n\n\nclass single_conv(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(single_conv,self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.BatchNorm2d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\n\nclass Attention_block(nn.Module):\n    def __init__(self, F_g, F_l, F_int):\n        super(Attention_block, self).__init__()\n        self.W_g = nn.Sequential(\n            nn.Conv2d(F_g, F_int, kernel_size = 1, stride = 1, padding = 0, bias = True),\n            nn.BatchNorm2d(F_int)\n            )\n        \n        self.W_x = nn.Sequential(\n            nn.Conv2d(F_l, F_int, kernel_size = 1, stride = 1, padding = 0, bias = True),\n            nn.BatchNorm2d(F_int)\n        )\n\n        self.psi = nn.Sequential(\n            nn.Conv2d(F_int, 1, kernel_size = 1, stride = 1, padding = 0, bias = True),\n            nn.BatchNorm2d(1),\n            nn.Sigmoid()\n        )\n        \n        self.relu = nn.ReLU(inplace = True)\n        \n    def forward(self,g,x):\n        g1 = self.W_g(g)\n        x1 = self.W_x(x)\n        psi = self.relu(g1 + x1)\n        psi = self.psi(psi)\n\n        return x * psi\n\nclass R2U_Net(nn.Module):\n    def __init__(self, img_ch = 3, output_ch = 1, t = 2, first_layer_numKernel = 64):\n        super(R2U_Net, self).__init__()\n\n        self.Maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n\n        self.RRCNN1 = RRCNN_block(ch_in = img_ch, ch_out = first_layer_numKernel, t = t)\n        self.RRCNN2 = RRCNN_block(ch_in = first_layer_numKernel, ch_out = 2 * first_layer_numKernel, t = t)\n        self.RRCNN3 = RRCNN_block(ch_in = 2 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel, t = t)\n        self.RRCNN4 = RRCNN_block(ch_in = 4 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel, t = t)\n        self.RRCNN5 = RRCNN_block(ch_in = 8 * first_layer_numKernel, ch_out = 16 * first_layer_numKernel, t = t)\n\n        self.Up5 = up_conv(ch_in = 16 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel, t = t)\n        self.Up_RCNN5 = RRCNN_block(ch_in = 16 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel, t = t)\n\n        self.Up4 = up_conv(ch_in = 8 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel, t = t)\n        self.Up_RCNN4 = RRCNN_block(ch_in = 8 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel, t = t)\n        \n        self.Up3 = up_conv(ch_in = 4 * first_layer_numKernel, ch_out = 2 * first_layer_numKernel, t = t)\n        self.Up_RCNN3 = RRCNN_block(ch_in = 4 * first_layer_numKernel, ch_out = 2 * first_layer_numKernel, t = t)\n        \n        self.Up2 = up_conv(ch_in = 2 * first_layer_numKernel, ch_out = first_layer_numKernel, t = t)\n        self.Up_RCNN2 = RRCNN_block(ch_in = 2 * first_layer_numKernel, ch_out = first_layer_numKernel, t = t)\n\n        self.Conv_1x1 = nn.Conv2d(first_layer_numKernel, output_ch, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.RRCNN1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.RRCNN2(x2)\n        \n        x3 = self.Maxpool(x2)\n        x3 = self.RRCNN3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.RRCNN4(x4)\n\n        x5 = self.Maxpool(x4)\n        x5 = self.RRCNN5(x5)\n\n        # decoding + concat path\n        d5 = self.Up5(x5)\n        d5 = torch.cat((x4, d5), dim = 1)\n        d5 = self.Up_RRCNN5(d5)\n        \n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3, d4), dim = 1)\n        d4 = self.Up_RRCNN4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2, d3), dim = 1)\n        d3 = self.Up_RRCNN3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1, d2), dim = 1)\n        d2 = self.Up_RRCNN2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        return d1\n\n\nclass AttU_Net(nn.Module):\n    def __init__(self, img_ch = 3, output_ch = 1, first_layer_numKernel = 64):\n        super(AttU_Net,self).__init__()\n        self.Maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n\n        self.Conv1 = conv_block(ch_in = img_ch, ch_out = first_layer_numKernel)\n        self.Conv2 = conv_block(ch_in = first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n        self.Conv3 = conv_block(ch_in = 2 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n        self.Conv4 = conv_block(ch_in = 4 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n        self.Conv5 = conv_block(ch_in = 8 * first_layer_numKernel, ch_out = 16 * first_layer_numKernel)\n\n        self.Up5 = up_conv(ch_in = 16 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n        self.Att5 = Attention_block(F_g = 8 * first_layer_numKernel, F_l = 8 * first_layer_numKernel, F_int = 4 * first_layer_numKernel)\n        self.Up_conv5 = conv_block(ch_in = 16 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n\n        self.Up4 = up_conv(ch_in = 8 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n        self.Att4 = Attention_block(F_g = 4 * first_layer_numKernel, F_l = 4* first_layer_numKernel, F_int = 2 * first_layer_numKernel)\n        self.Up_conv4 = conv_block(ch_in = 8 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n        \n        self.Up3 = up_conv(ch_in = 4 * first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n        self.Att3 = Attention_block(F_g = 2 * first_layer_numKernel, F_l = 2 * first_layer_numKernel, F_int = first_layer_numKernel)\n        self.Up_conv3 = conv_block(ch_in = 4 * first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n        \n        self.Up2 = up_conv(ch_in = 2 * first_layer_numKernel, ch_out = first_layer_numKernel)\n        self.Att2 = Attention_block(F_g = first_layer_numKernel, F_l = first_layer_numKernel, F_int = int(first_layer_numKernel \/ 2))\n        self.Up_conv2 = conv_block(ch_in = 2 * first_layer_numKernel, ch_out = first_layer_numKernel)\n\n        self.Conv_1x1 = nn.Conv2d(first_layer_numKernel, output_ch, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n        \n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x5 = self.Conv5(x5)\n\n        # decoding + concat path\n        d5 = self.Up5(x5)\n        x4 = self.Att5(g = d5, x = x4)\n        d5 = torch.cat((x4, d5), dim = 1)        \n        d5 = self.Up_conv5(d5)\n        \n        d4 = self.Up4(d5)\n        x3 = self.Att4(g = d4, x = x3)\n        d4 = torch.cat((x3, d4), dim = 1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        x2 = self.Att3(g = d3, x = x2)\n        d3 = torch.cat((x2, d3), dim = 1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        x1 = self.Att2(g = d2, x = x1)\n        d2 = torch.cat((x1,d2), dim = 1)\n        d2 = self.Up_conv2(d2)\n        \n        d1 = self.Conv_1x1(d2)\n\n        return d1\n\ndef train_net(net, epochs=5, batch_size=1, lr=0.001):\n    \n    if not os.path.isdir('{0}'.format(net.UnetLayer)):\n        os.mkdir('{0}'.format(net.UnetLayer))\n    \n    # Check if GPU is available\n    is_cuda = torch.cuda.is_available()\n    \n    if is_cuda:\n        net.cuda()\n    \n    device = torch.device('cuda' if is_cuda else 'cpu')\n    train = BasicDataset('train')\n    val = BasicDataset('valid')\n    \n    n_train = len(train)\n    n_val = len(val)\n    \n    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n    \n    mask_type = torch.float32\n\n    # Change the optimizer and learning rate scheduler if these don't suits your model.\n    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2)\n    \n    # This function is a general mode for multi-class U-Net; however, we only have one class. Therefore, only BCEWithLogitsLoss will be used.\n    # If you wish to use a different Loss function change 'criterion' to what desires your model.\n    # A quick reminder, the input of both Loss functions (CrossEntropyLoss and BCEWithLogitsLoss) doesn't need to be normalized (softmax is internally implemented in loss)\n    if net.n_classes > 1:\n        criterion = nn.CrossEntropyLoss()\n    else:\n        criterion = nn.BCEWithLogitsLoss()\n    \n    \n    training_summary = dict()\n    training_summary['best_epoch'] = -1\n    training_summary['best_epoch_loss'] = np.finfo(np.float64).max\n    training_summary['best_epoch_dice'] = 0.0\n    \n    training_summary['training_loss'] = list()\n    training_summary['training_dice'] = list()\n    training_summary['validation_loss'] = list()\n    training_summary['validation_dice'] = list()        \n    \n    for epoch in range(epochs):\n        \n        ################################################################################################################################\n        ########################################################### Training ###########################################################\n        ################################################################################################################################\n        net.train()\n        epoch_loss = list()\n        epoch_dice = list()\n        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}\/{epochs}', unit='img') as pbar:\n            for batch in train_loader:\n                \n                # Load a batch and pass it to the device (CPU or GPU)\n                imgs = batch['image']\n                true_masks = batch['mask']\n\n                imgs = imgs.to(device=device, dtype=torch.float32)\n                true_masks = true_masks.to(device=device, dtype=mask_type)\n                \n                # Produce the estimated mask using current weights\n                masks_pred = net(imgs)\n                \n                # Compute the loss\n                loss = criterion(masks_pred, true_masks)\n                batch_loss = loss.item()\n                epoch_loss.append(batch_loss*batch_size) # We need to multiply by batch_size since the batch_loss is the average of the loss over all samples in the batch\n                \n                # If it's a ulti-class segmentation, the predicted mask would be made by passing the network output through a softmax (convert it to probabliti between 0 and 1)\n                # then take the maximum value index over the channel axis as the final label. \n                # In case of binary segmentation, we need to pass values (one channel output of the network) through the Sigmoid function (values between 0 and 1), then threshold it\n                # by 0.5 (it can be changed).\n                if net.n_classes > 1:\n                    pred = F.softmax(masks_pred, dim=1)\n                    pred = torch.argmax(pred, dim=1).unsqueeze(1)\n                else:\n                    pred = torch.sigmoid(masks_pred)\n                    pred = (pred > 0.5).float()                    # You can change the probablity threshold!\n\n                dice_value = dice_coeff_multiclass(pred, true_masks, net.n_classes)\n                # Loop over the dice scores of all the samples in the batch\n                for dice_score in dice_value:\n                    epoch_dice.append(dice_score)\n                pbar.set_postfix(**{'loss (batch)': batch_loss, \n                                    'DICE Coef (batch)': dice_value.mean(),\n                                    'loss (avg)': np.array(epoch_loss).mean(), \n                                    'DICE Coef (avg)': np.array(epoch_dice).mean()\n                                   })\n                \n                # Reset gradient values\n                optimizer.zero_grad()\n                \n                # Compute the backward losses\n                loss.backward()\n                \n                # Clip gradient to avoid sudden changes\n                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n                \n                # Update weights\n                optimizer.step()\n\n                pbar.update(imgs.shape[0])\n                \n        training_summary['training_loss'].append(np.array(epoch_loss).mean())\n        training_summary['training_dice'].append(np.array(epoch_dice).mean())\n                \n        ################################################################################################################################\n        ########################################################## Validation ##########################################################\n        ################################################################################################################################\n        \n        net.eval()\n        valid_epoch_loss = list()\n        valid_epoch_dice = list()\n        \n        # This part is almost the same as training with the difference that we will set all layers to evaluation mode (effects some layers such as BN and Dropout) and also\n        # we don't need to calculate the gradient since we are only evaluating current state of the model. This will speed up the process and cause it to consume less memory.\n        with tqdm(total=n_val, desc='Validation round', unit='batch') as pbar:\n            with torch.no_grad():\n                for batch in val_loader:\n                    imgs, true_masks = batch['image'], batch['mask']\n                    imgs = imgs.to(device=device, dtype=torch.float32)\n                    true_masks = true_masks.to(device=device, dtype=mask_type)\n\n                    masks_pred = net(imgs)\n                    \n                    batch_loss = criterion(masks_pred, true_masks).item()\n                    valid_epoch_loss.append(batch_loss*batch_size)\n\n                    if net.n_classes > 1:\n                        pred = F.softmax(masks_pred, dim=1)\n                        pred = torch.argmax(pred, dim=1).unsqueeze(1)\n                    else:\n                        pred = torch.sigmoid(masks_pred)\n                        pred = (pred > 0.5).float()                    # You can change the probablity threshold!\n                        \n                    dice_value = dice_coeff_multiclass(pred, true_masks, net.n_classes)\n                    for dice_score in dice_value:\n                        valid_epoch_dice.append(dice_score)\n                    pbar.set_postfix(**{'loss (batch)': batch_loss, \n                                        'DICE Coef (batch)': dice_value.mean(),\n                                        'loss (avg)': np.array(valid_epoch_loss).mean(), \n                                        'DICE Coef (avg)': np.array(valid_epoch_dice).mean()\n                                       })\n                    pbar.update()\n        \n        training_summary['validation_loss'].append(np.array(valid_epoch_loss).mean())\n        training_summary['validation_dice'].append(np.array(valid_epoch_dice).mean())\n        \n        # We choose our best model based on validation dice score\n        save_as_best_model = False\n        if np.array(valid_epoch_dice).mean() >= training_summary['best_epoch_dice']:\n            training_summary['best_epoch'] = epoch\n            training_summary['best_epoch_loss'] = np.array(valid_epoch_loss).mean()\n            training_summary['best_epoch_dice'] = np.array(valid_epoch_dice).mean()\n            save_as_best_model = True\n            \n                \n        # Now update our learning rate scheduler based on the average dice score over all validation samples\n        scheduler.step(np.array(valid_epoch_dice).mean())\n\n        ################################################################################################################################\n        ###################################################### Saveing Checkpoints #####################################################\n        ################################################################################################################################\n        os.mkdir('your directory')\n        torch.save(net.state_dict(), your_directory)\n        \n        # Save this state as the best model, if it is!\n        if save_as_best_model:\n            torch.save(net.state_dict(), f'{net.name}\/{net.name}_BestModel.pth')\n        \n        # Training summary will be saved as a json file for further visualization.\n        with open(f'{net.name}\/{net.name}_TrainingSummary.json', 'w') as f:\n            json.dump(training_summary, f)\n\n\n\n\n","4b714a5c":"class ResAttU_Net(nn.Module):\n    def __init__(self, name, UnetLayer=1, img_ch = 3, output_ch = 1, first_layer_numKernel = 64):\n        super(ResAttU_Net, self).__init__()\n\n        self.name = name \n        self.UnetLayer = UnetLayer\n        self.n_channels = 3\n        self.n_classes = 1\n        self.Maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n\n        self.Conv1 = resconv_block(ch_in = img_ch, ch_out = first_layer_numKernel)\n        self.Conv2 = resconv_block(ch_in = first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n        self.Conv3 = resconv_block(ch_in = 2 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n        self.Conv4 = resconv_block(ch_in = 4 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n        self.Conv5 = resconv_block(ch_in = 8 * first_layer_numKernel, ch_out = 16 * first_layer_numKernel)\n\n        self.Up5 = up_conv(ch_in = 16 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n        self.Att5 = Attention_block(F_g = 8 * first_layer_numKernel, F_l = 8 * first_layer_numKernel, F_int = 4 * first_layer_numKernel)\n        self.Up_conv5 = conv_block(ch_in = 16 * first_layer_numKernel, ch_out = 8 * first_layer_numKernel)\n\n        self.Up4 = up_conv(ch_in = 8 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n        self.Att4 = Attention_block(F_g = 4 * first_layer_numKernel, F_l = 4* first_layer_numKernel, F_int = 2 * first_layer_numKernel)\n        self.Up_conv4 = conv_block(ch_in = 8 * first_layer_numKernel, ch_out = 4 * first_layer_numKernel)\n        \n        self.Up3 = up_conv(ch_in = 4 * first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n        self.Att3 = Attention_block(F_g = 2 * first_layer_numKernel, F_l = 2 * first_layer_numKernel, F_int = first_layer_numKernel)\n        self.Up_conv3 = conv_block(ch_in = 4 * first_layer_numKernel, ch_out = 2 * first_layer_numKernel)\n        \n        self.Up2 = up_conv(ch_in = 2 * first_layer_numKernel, ch_out = first_layer_numKernel)\n        self.Att2 = Attention_block(F_g = first_layer_numKernel, F_l = first_layer_numKernel, F_int = int(first_layer_numKernel \/ 2))\n        self.Up_conv2 = conv_block(ch_in = 2 * first_layer_numKernel, ch_out = first_layer_numKernel)\n\n        self.Conv_1x1 = nn.Conv2d(first_layer_numKernel, output_ch, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self, x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n        \n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n        \n        d3 = self.Up3(x3)\n\n        if self.UnetLayer > 3:    \n            x4 = self.Maxpool(x3)\n            x4 = self.Conv4(x4)\n\n            d4 = self.Up4(x4)\n\n            if self.UnetLayer > 4:\n                x5 = self.Maxpool(x4)\n                x5 = self.Conv5(x5)\n\n                # decoding + concat path\n                d5 = self.Up5(x5)\n                x4 = self.Att5(g = d5, x = x4)\n                d5 = torch.cat((x4, d5), dim = 1)        \n                d5 = self.Up_conv5(d5)\n\n                d4 = self.Up4(d5)\n\n            x3 = self.Att4(g = d4, x = x3)\n            d4 = torch.cat((x3, d4), dim = 1)\n            d4 = self.Up_conv4(d4)\n\n            d3 = self.Up3(d4)\n        x2 = self.Att3(g = d3, x = x2)\n        d3 = torch.cat((x2, d3), dim = 1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        x1 = self.Att2(g = d2, x = x1)\n        d2 = torch.cat((x1, d2), dim = 1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        return d1","cc0f2e5a":"# Make an instance of your model\n\nmy_AttResUNet = ResAttU_Net(name='ResAttU_Net')\nprint(my_AttResUNet)","498bf48e":"# Train your model for as many epochs as you like; be careful that the training should finish within the 9-hour session limitation you have.\n\ntrain_net(my_AttResUNet, epochs=1, batch_size=1, lr = 0.001)","747cab61":"# Plot learning curve\n\nplot_learning_curve(my_AttResUNet)","9824144f":"# Test your model\n\ntest_net(my_AttResUNet)","269f2031":"# Display few samples of the results\n\nplot_best_worst_samples(my_AttResUNet)","06ab3836":"# Print the statistical analysis\n\nstatistical_report(my_AttResUNet)","c3e2ed35":"# Plot your model's weights (Visualization)\n\ndef plot_weights_AttResUnet (net):\n   \n    # Turn off gradients\n    with torch.no_grad():\n        # Search in all layers in the network (However, only the first convolution layer will be shown)\n        for m in net.modules():\n            # Check if that layer (m) is an instance of nn.Conv2d\n            for layer in nn.Modules:\n               \n                if isinstance(m, layer):\n                # Convert weights to numpy array\n               \n                    if m.weight.is_cuda:\n                        filters = m.weight.cpu().numpy()\n                    else:\n                        filters = m.weight.numpy()\n                   \n\n                # Get filters weidth and hights\n                    filter_h = filters.shape[2]\n                    filter_w = filters.shape[3]\n\n                # Compute sqrt of total filters to plot first nxn filters only! (Visualization reasons!)\n                    sqrt_n_filters = int(np.sqrt(filters.shape[0]))\n\n\n                    plt.figure(figsize=(12, 17))\n                    plt.suptitle(f'Weights visualization of {net.name} - First Convolution Layer, first {sqrt_n_filters*sqrt_n_filters} filters of size {filter_w}x{filter_h}')\n\n                # Take minimum and maximum values to display filters' weights in a compareable way\n                    vmin = filters.min()\n                    vmax = filters.min()\n                    for i in range(sqrt_n_filters):\n                        for j in range(sqrt_n_filters):\n\n                        # idx is just a counter\n                            idx = i*sqrt_n_filters+j\n\n                        # Dont forget to convert CHW to HWC\n                            weights = filters[idx].transpose((1,2,0))\n\n                            plt.subplot(sqrt_n_filters, sqrt_n_filters, idx+1)\n                            plt.imshow(weights, vmin=vmin, vmax=vmax)\n                            plt.axis('off')\n                    plt.tight_layout()\n                    plt.savefig(f'{net.name}\/{net.name}_FirstLayerVisualization.png')\n                    plt.show()\n                #leave the function early\n                    return\n\n","0f1011b9":"Please write a short report about your implementation and analyze your result in an [IEEE conference paper template](https:\/\/www.ieee.org\/conferences\/publishing\/templates.html) and then submit the PDF version of it via [CourseWorks](https:\/\/courseworks2.columbia.edu\/) along with your copy of this Notebook in \".ipynb\" format. \\\nThe report must include these sections:\n1. Introduction\n2. Dataset description\n2. Method\n3. Results\n4. Discussion and Conclusion\n\nIf you want to include the figures you have made in this Notebook, you need to save them in the working directory. Then you need to download them (Click on the dots on right side of eachh file and \"Download\"). If you are using the general function, they are saved in the model's directory in \"\/Kaggle\/working\/{model name}\/{model name}_{figure name}.png\" pattern.\n\n### Submission Checklist\n* Make sure you've written your name(s)\n* Make sure you save your Notebook correctly and it contains your notes, codes, and figures.\n\n### <span style='color:red;fontsize:28px'> Deadline <\/span>\n***December 17, 2020, 11:59 pm EST***","47dd1bb8":"Now that the model is appropriately trained (by looking at the training curve), we need to test it on the test set to ensure that our trained model works on unseen data.\nThe `test_net` function will load the weights of the \"Best Model\" that obtains the maximum dice score on the validation set (If you wish to load the weights of a specific epoch, you may set the `epoch` argument when you call this function), then creates an instance of the `BasicDataset` using test set and predicts the segmentation masks on the test images. Additionally, this function will save the predicted masks in \"model_name\/pred_mask\" directory with a file format of \"pred_mask_x.png,\" where the \"x\" is the input index. Finally, this function will print average and standard deviation on loss values and dice scores on the test set. This function will also save dice score and loss value for each individual input in a '.json' format (\"model_name\/model_name_TestResults.json\") for further visualization. \\\n(If you changed the loss function during training, you need also change it in this function to generate comparable results.)","e3eaab33":"## Dataset\nThe dataset consists of three folders named \"train,\" \"valid,\" and \"test\" that is for training, validation, and testing the model. Each folder has two folders, \"image,\" containing RGB fluorescence widefield microscopy images as inputs, and \"mask,\" which contains segmentation masks as targets.\n\nRGB images are named by integer numbers; for trainset **\"image_0001**\" to **\"image_4140**\", for validation set **\"image_0001**\" to **\"image_1380**\" , and for test set **\"image_0001**\" to **\"image_2070**\". Similarly, the segmentation masks are **\"mask_0001**\" to **\"mask_x**\", where x depends on the number of samples in the set.\n\nBoth RGB and segmentation masks are in **\".png\"** format with the size of $512\\times512$ pixels.\n\n<span style='color:red;fontsize:18px'>How to add the Dataset: \\<\/span>\n1. On right-side panel click on \"Add Data\"\n2. From opened window click on Search by URL\n3. Search for \"https:\/\/www.kaggle.com\/soroush361\/singlecellsegmentation\"\n4. Click on \"Add\" button.","5e8ec1b1":"Time to train our U-Net using the `train_net` function for 10 epochs!","191b26bd":"First, we need to import all the packages we will need during the implementations.","f289baac":"# Section 2: Simple U-Net\nThis section will implement a simple U-Net then train and validate it on the given dataset. We will utilize previously defined functions to test our best U-net model on the test set and show its performance by reporting the statistical measures.","7d921abf":"### Checklist:\n* Make sure the session is started.\n* Make sure internet access is granted.\n* Make sure the GPU accelerator is selected.\n* Make sure that you've added the dataset (Data->Add data->Search by URL->{search https:\/\/www.kaggle.com\/soroush361\/singlecellsegmentation }->Add)","cad5c32f":"According to the test set's dice scores, the `plot_best_worst_samples` function is designed to display five of the best and worst segmentation masks. This function will take the model to search for its \"TestResults.json\" file.","ef8f158d":"# Section 4: Report and submission","d444096c":"## Sections:\n1. General functions for making dataset of train, validation, and test sets; train the model, save its weights, plotting the learning curve; and test the model on the test set, and present the statistical results.\n2. An implementation of simple U-Net (https:\/\/arxiv.org\/abs\/1505.04597) and its results as an example.\n3. Your implementation of Residual Attention U-Net (https:\/\/arxiv.org\/abs\/2001.05548) and results.\n4. Report and submission.","f5ec7a14":"The following code blocks are just a draft for you to have some hints about what you need to do. Free free to add, remove, edit, or re-write any part of it.","55ffff85":"The final step would be investigating\/visualizing the weights of our trained \"Simple U-Net\". Here we only visualize the first layer's weights using `plot_weights_UNet` function.","41b0e8b9":"# Section 1: General functions\nIn this section, we implemented few functions to make reading the dataset; training, validating, and saving the model; testing and generating the results; and presenting the statistical analysis easy for you. \\\nPlease read the functions and make sure you understand each line of them. \\\nIf you have any questions, reach to CAs via email (Soroush: sa3617@columbia.edu, Hengda: hh2699@columbia.edu) or ask your questions during lecture\/office hours.\n\n**Acknowledgment:** Most of the following implementations are a modified  version of this repository: https:\/\/github.com\/milesial\/Pytorch-UNet\nPlease cite their work if you would use these functions in the future.","c8759db9":"Till Now, we only defined our architecture; here, we need to make an instance of it and set its initial arguments. Then, we will print architecture.","310cfd34":"Let's test our trained U-Net on the test set using `test_net` function.","ee47a880":"<span style=\"color:red;font-size:22px;\" font>Warnings<\/span>\n1. Please don't share the dataset with anyone else other than students in the class.\n2. Please don't copy other teams' code. Don't forget that your worst try is always better than cheating.\n3. Comment on your codes properly.\n4. Please choose meaningful names for your variables.\n4. Proper citing is a must if you are using other papers or codes publicly available. Consider this both in the implementations and final report.\n5. Take advantage of Markdown blocks to explain what you are trying to do in the next code block.","656a2fc0":"## Grading (of 100 points)\n1. Successful implementation - 35 points\n2. Successful results - 35 points\n3. Report - 30 points\n4. Innovative approaches (i.e., changing loss function, new preprocessing pipeline, edge-enhancment, etc.) - Extra 20 points","3c18c97c":"# Art of Engineering - BME Departmental Project","1cb5eb3e":"Now that the test set predicted mask is saved, we may show top five best and works cases in the test set usine `plot_best_worst_samples` function.","d12ec766":"Here is a function that displays the weights of the first convolution layer of the \"Simple U-Net,\" called `plot_weights_UNet.` Unfortunately, because it is almost impossible to make a function that visualizes the weights of all possible models, this function will only work for our \"Simple U-Net\" example. Use this as an example to understand how you can create your own version of this function as it is requested to visualize your implementation's weights.","e23f0286":"Write yours and your teammates name(s) and UNI(s) like: \n\nCostanza Siniscalchi (cs4014)\nTeodora Sutilovic \nElizabeth Caso (eac2269)\n\nin here.","5b3f3da4":"# Section 3: Residual Attention U-Net (Your part)\nIn this section, you are required to read [\"Segmentation with Residual Attention U-Net and an Edge-Enhancement Approach Preserves Cell Shape Features\"](https:\/\/arxiv.org\/abs\/2001.05548) paper and implement their proposed architecture. Then produce the predicted masks for the test set and analyze the performance of your implementation.\nFeel free to use general functions defined in \"Section 1\" or write your version of the training\/validation\/testing procedure. \\\nUse the \"Section 2: Simple U-Net\" implementation as an example for your implementation. \\\n<span style=\"color:red;font-size:18px;\" font>Important<\/span>: You are absolutely welcomed to use the paper's GitHub repository (https:\/\/github.com\/SAIL-GuoLab\/Cell_Segmentation_and_Tracking) to get help for your implementation. Furthermore, if you wish to use other resources (other than other students' code), please cite their work properly.","a2b62f61":"After training the model, we are usually interested in looking at the learning curve (Similar to what you had seen in the \"First Tutorial\"). The `plot_learning_curve` function will take a previously trained model (`net`) and looks for its summary; then loads it and plot the losses and dices after each epoch for both training and validation sets.","770648b2":"Also, let's check if our `BasicDataset` implementation works by pulling out a random sample of the training set. \\\n(Don't forget that we need to reverse some of the preprocessing steps)","ce14317f":"Before moving on, let's look into the number of samples we have for the train, validate, and test our models.","ca6802b4":"When the training is finished, we may plot the training curve of our simple U-Net using `plot_learning_curve` function.","a200ae1a":"To implement the U-Net, we first define four blocks that we will use multiple times while designing the complete architecture. \\\n`DoubleCov` is block that contains these layers: Conv2d->BatchNormalization->ReLU->Conv2d->BatchNormalization->ReLU \\\n`Down` is a downsampling block which contains Maxpooling and `DoubleConv` after Maxpooling. (Decoding layers) \\\n`Up` is an upsampling block that upsamples the input then pass it through a `DoubleConv.` (Encoding Layers) \\\n`OutConv` is just a $(1\\times1)$ 2D convolution that serves as an output layer in our U-Net model.","6d86d330":"Besides the few samples visualization, which serves as a qualitative assessment, we need to present some quantitative statistical analysis using the `statistical_report` function.\n(This function will take some time to finish its job)","305e96d5":"Now we build our U-Net using the blocks we defined above. \\\n<span style=\"color:red;font-size:18px;\" font>Important<\/span>: In order to use previously defined functions we need to have a `name` and `n_classes` property for our model.","2ffaa732":"Our next function is `train_net`. This function is essentially our training core. This function will take the model (`net`), the number of epochs we would like to train our model (`epochs`), batch size (`batch_size`), and learning rate (`lr`). \\\nThis function will create a directory in \"\/kaggle\/working\" directory with the model name to save model weight at the end of each epoch, as well as a training summary that contains training\/validation losses and dice scores in a '.json' file (\"model_name\/model_name_TrainingSummary.json\"). In addition, all further plots will also be saved in this directory. \\\nIn this function, we automatically create an instance of the training set and validation set using our previously defined `BasicDataset.` Then, it will make data loaders for those datasets with the batch size that has been given. It trains the model for the given number of epochs and updates the weights and learning rate schedular. In this function, the loss function will be selected based on the segmentation type (Multi-class or Binary). However, in both scenarios, it will be Cross Entropy. If you wish to change your loss function or learning rate scheduler, you need to modify this function. \\\nRead this function thoroughly and make sure you understand most of it.","74c37d31":"The final general function is `statistical_report` that takes the model (`net`) and look for all the predicted masks for the test set and compares it with the True masks to print the [\"Sensitivity (Recall),\" \"Precision,\" \"F1-score\",](https:\/\/en.wikipedia.org\/wiki\/Sensitivity_and_specificity) and [average accuracy](https:\/\/en.wikipedia.org\/wiki\/Sensitivity_and_specificity). Furthermore, this function will display the confusion matrix for the test set.","c611b3f3":"## Goals:\n1. Learn how to work with large image datasets.\n2. Learn how to implement a simple U-Net.\n3. Learn how to train and test a U-Net.\n3. Implement a Residual Attention U-Net (https:\/\/arxiv.org\/abs\/2001.05548) to segment single-cells using fluorescence widefield microscopy images.\n4. Present the statistical analysis and model's performance.","74fa561d":"In the lectures, we discussed some segmentation metrics. In this project, we are more interested in [DICE score](https:\/\/en.wikipedia.org\/wiki\/S%C3%B8rensen%E2%80%93Dice_coefficient) metric. Therefore, we have implemented a function that computes multi-class DICE scores in general, which works in our binary segmentation.\n![](https:\/\/miro.medium.com\/max\/858\/1*yUd5ckecHjWZf6hGrdlwzA.png)\nThe DICE score shows how successfully our model could predict correct labels in the right location. Its score varies from $0.0$ for no overlap and $1.0$ for perfect overlap.","09bcb5ff":"![](https:\/\/camo.githubusercontent.com\/41ded1456b9dbe13b8d73d8da539dac95cb8aa721ebe5fb798af732ca9f04c92\/68747470733a2f2f692e696d6775722e636f6d2f6a6544567071462e706e67)","794a3285":"In the \"First Tutorial,\" you had seen that we uploaded the whole MNIST dataset into Python variables, and then we trained our model using those variables. However, the MNIST dataset is relatively small, and it was possible to do so. The \"SingleCellSegmentation\" is larger than the MNIST, and we need to read each image\/batch at a time, and train\/validate\/test our model, and then move to the next image\/batch. \\\nTo do so, we need a `BasicDataset` instance that reads an image and respective segmentation mask. \\\nIn below, we implemented the `BasicDataset` class that does the job. Furthermore, you can implement any kinds of preprocessing you wish to do in this implementation. For example, in this class, we resized the resolutions to $256\\times256$ pixels to speed up the training process and reduce memory consumption. In addition, we scaled the intensities from $[0, 255]$ to $[0, 1]$. \\\nThis document will use **Pytorch**, and Pytroch's inputs shape format is $[Channels, Width, Height]$ rather than $[Width, Height, Channels]$, we changed the image axis as well."}}