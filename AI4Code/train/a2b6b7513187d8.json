{"cell_type":{"4b5c541b":"code","40e4736a":"code","a136485c":"code","147df6f5":"code","b8adcc69":"code","6a70b5cc":"code","125e8b84":"code","38c8a284":"code","5025651b":"code","51b18d3d":"code","289dc91b":"code","1568bf8a":"code","70a9cb68":"code","e9427fb0":"code","aac21f53":"code","87c331cf":"code","cbd160df":"code","d7095d29":"code","e553291e":"code","9b36a1f9":"code","ab76514d":"code","f56387a4":"code","1bd7f312":"markdown","e9904359":"markdown"},"source":{"4b5c541b":"# Load libraries\nimport numpy as np\nimport pandas as pd\nimport gc\nimport datetime\nfrom scipy.stats import mode\n\nfrom sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn import preprocessing\n# Params\nNFOLD = 5\nDATA_PATH = '..\/input\/'","40e4736a":"%%time\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count',\n             'event_code','title' ,'game_time', 'type', 'world','timestamp']\ntrain=pd.read_csv('..\/input\/data-science-bowl-2019\/train.csv',usecols=keep_cols)\ntrain_labels=pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv',\n                         usecols=['installation_id','game_session','accuracy_group'])\ntest=pd.read_csv('..\/input\/data-science-bowl-2019\/test.csv',usecols=keep_cols)\nsubmission=pd.read_csv('..\/input\/data-science-bowl-2019\/sample_submission.csv')","a136485c":"not_req=(set(train.installation_id.unique()) - set(train_labels.installation_id.unique()))","147df6f5":"train_new=~train['installation_id'].isin(not_req)\ntrain.where(train_new,inplace=True)\ntrain.dropna(inplace=True)\ntrain['event_code']=train.event_code.astype(int)","b8adcc69":"def extract_time_features(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df['month'] = df['timestamp'].dt.month\n    df['hour'] = df['timestamp'].dt.hour\n    df['year'] = df['timestamp'].dt.year\n    df['dayofweek'] = df['timestamp'].dt.dayofweek\n    df['weekofyear'] = df['timestamp'].dt.weekofyear\n    return df","6a70b5cc":"time_features=['month','hour','year','dayofweek','weekofyear']\ndef prepare_data(df):\n    df=extract_time_features(df)\n    \n    df=df.drop('timestamp',axis=1)\n    #df['timestamp']=pd.to_datetime(df['timestamp'])\n    #df['hour_of_day']=df['timestamp'].map(lambda x : int(x.hour))\n    \n\n    join_one=pd.get_dummies(df[['event_code','installation_id','game_session']],\n                            columns=['event_code']).groupby(['installation_id','game_session'],\n                                                            as_index=False,sort=False).agg(sum)\n\n    agg={'event_count':sum,'game_time':['sum','mean'],'event_id':'count'}\n\n    join_two=df.drop(time_features,axis=1).groupby(['installation_id','game_session']\n                                                   ,as_index=False,sort=False).agg(agg)\n    \n    join_two.columns= [' '.join(col).strip() for col in join_two.columns.values]\n    \n\n    join_three=df[['installation_id','game_session','type','world','title']].groupby(\n                ['installation_id','game_session'],as_index=False,sort=False).first()\n    \n    join_four=df[time_features+['installation_id','game_session']].groupby(['installation_id',\n                'game_session'],as_index=False,sort=False).agg(mode)[time_features].applymap(lambda x: x.mode[0])\n    \n    join_one=join_one.join(join_four)\n    \n    join_five=(join_one.join(join_two.drop(['installation_id','game_session'],axis=1))). \\\n                        join(join_three.drop(['installation_id','game_session'],axis=1))\n    \n    return join_five\n","125e8b84":"join_train=prepare_data(train)\ncols=list(join_train.columns)[2:-3]\njoin_train[cols]=join_train[cols].astype('int16')","38c8a284":"join_test=prepare_data(test)\ncols=list(join_test.columns)[2:-3]\njoin_test[cols]=join_test[cols].astype('int16')","5025651b":"cols=list(join_test.columns[2:-12])\ncols.append('event_id count')\ncols.append('installation_id')","51b18d3d":"df=join_test[['event_count sum','game_time mean','game_time sum',\n    'installation_id']].groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=join_test[cols].groupby('installation_id',as_index=False,\n                               sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=join_test[['title','type','world','installation_id']].groupby('installation_id',\n         as_index=False,sort=False).last().drop('installation_id',axis=1)\n\ndf_four=join_test[time_features+['installation_id']].groupby('installation_id',as_index=False,sort=False). \\\n        agg(mode)[time_features].applymap(lambda x : x.mode[0])","289dc91b":"final_train=pd.merge(train_labels,join_train,on=['installation_id','game_session'],\n                                         how='left').drop(['game_session'],axis=1)\n\n#final_test=join_test.groupby('installation_id',as_index=False,sort=False).last().drop(['game_session','installation_id'],axis=1)\nfinal_test=(df.join(df_two)).join(df_three.join(df_four)).drop('installation_id',axis=1)","1568bf8a":"df=final_train[['event_count sum','game_time mean','game_time sum','installation_id']]. \\\n    groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=final_train[cols].groupby('installation_id',as_index=False,\n                                 sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=final_train[['accuracy_group','title','type','world','installation_id']]. \\\n        groupby('installation_id',as_index=False,sort=False). \\\n        last().drop('installation_id',axis=1)\n\ndf_four=join_train[time_features+['installation_id']].groupby('installation_id',as_index=False,sort=False). \\\n        agg(mode)[time_features].applymap(lambda x : x.mode[0])\n\n\n\nfinal_train=(df.join(df_two)).join(df_three.join(df_four)).drop('installation_id',axis=1)","70a9cb68":"from sklearn.preprocessing import LabelEncoder\nfinal=pd.concat([final_train,final_test])\nencoding=['type','world','title']\nfor col in encoding:\n    lb=LabelEncoder()\n    lb.fit(final[col])\n    final[col]=lb.transform(final[col])\n    \nfinal_train=final[:len(final_train)]\nfinal_test=final[len(final_train):]","e9427fb0":"final_train.shape,final_test.shape","aac21f53":"set(final_train.columns) - set(final_test.columns)","87c331cf":"predictors = list(final_train.columns)","cbd160df":"#train.drop('accuracy_group', axis=1,inplace=True)\n# Mark train as 1, test as 0\nfinal_train['target'] = 1\nfinal_test['target'] = 0\n\n# Concat dataframes\nn_train = final_train.shape[0]\ndf = pd.concat([final_train, final_test], axis = 0)\ndel final_train, final_test\ngc.collect()","d7095d29":"df.drop('accuracy_group',axis=1,inplace=True)","e553291e":"predictors.remove('accuracy_group')","9b36a1f9":"# Prepare for training\n\n# Shuffle dataset\ndf = df.iloc[np.random.permutation(len(df))]\ndf.reset_index(drop = True, inplace = True)\n\n# Get target column name\ntarget = 'target'\n\n# lgb params\nlgb_params = {\n        'boosting': 'gbdt',\n        'application': 'binary',\n        'metric': 'auc', \n        'learning_rate': 0.1,\n        'num_leaves': 32,\n        'max_depth': 8,\n        'bagging_fraction': 0.7,\n        'bagging_freq': 5,\n        'feature_fraction': 0.7,\n}\n\n# Get folds for k-fold CV\nfolds = KFold(n_splits = NFOLD, shuffle = True, random_state = 0)\nfold = folds.split(df)\n    \neval_score = 0\nn_estimators = 0\neval_preds = np.zeros(df.shape[0])","ab76514d":"# Run LightGBM for each fold\nfor i, (train_index, test_index) in enumerate(fold):\n    print( \"\\n[{}] Fold {} of {}\".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), i+1, NFOLD))\n    train_X, valid_X = df[predictors].values[train_index], df[predictors].values[test_index]\n    train_y, valid_y = df[target].values[train_index], df[target].values[test_index]\n\n    dtrain = lgb.Dataset(train_X, label = train_y,\n                          feature_name = list(predictors)\n                          )\n    dvalid = lgb.Dataset(valid_X, label = valid_y,\n                          feature_name = list(predictors)\n                          )\n        \n    eval_results = {}\n    \n    bst = lgb.train(lgb_params, \n                         dtrain, \n                         valid_sets = [dtrain, dvalid], \n                         valid_names = ['train', 'valid'], \n                         evals_result = eval_results, \n                         num_boost_round = 5000,\n                         early_stopping_rounds = 100,\n                         verbose_eval = 100)\n    \n    print(\"\\nRounds:\", bst.best_iteration)\n    print(\"AUC: \", eval_results['valid']['auc'][bst.best_iteration-1])\n\n    n_estimators += bst.best_iteration\n    eval_score += eval_results['valid']['auc'][bst.best_iteration-1]\n   \n    eval_preds[test_index] += bst.predict(valid_X, num_iteration = bst.best_iteration)\n    \nn_estimators = int(round(n_estimators\/NFOLD,0))\neval_score = round(eval_score\/NFOLD,6)\n\nprint(\"\\nModel Report\")\nprint(\"Rounds: \", n_estimators)\nprint(\"AUC: \", eval_score)    ","f56387a4":"# Feature importance\nlgb.plot_importance(bst, max_num_features = 20)","1bd7f312":"As we can see, the separation is almost perfect - which strongly suggests that the train \/ test rows are very easy to distinguish. **Meaning the distribution of Train and Test is not the same**","e9904359":"For more details :\nhttp:\/\/fastml.com\/adversarial-validation-part-one\/\nhttps:\/\/www.kaggle.com\/konradb\/adversarial-validation-and-other-scary-terms\nhttps:\/\/www.kaggle.com\/ogrellier\/adversarial-validation-and-lb-shakeup\n\nBasic data processing from:\nhttps:\/\/www.kaggle.com\/shahules\/xgboost-feature-selection-dsbowl\n\n**Some code snippet below is from other past competition kernels , but now I don't remember the owner, if you are the one please mention in comment and I will add your credit here :)**"}}