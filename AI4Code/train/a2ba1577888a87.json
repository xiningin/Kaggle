{"cell_type":{"19136f6f":"code","aef00a50":"code","17bee306":"code","0e4cb820":"code","9dc1b8e5":"code","6cd632d1":"code","d2423be5":"code","91600caf":"code","c1e1b674":"code","4242a9c7":"code","7bcd594b":"code","df6a3027":"code","12058054":"code","76364487":"code","bb317aea":"code","f8fbcdba":"code","f1f96452":"code","6b2b5f04":"code","e74a623c":"code","860591c6":"code","e6468af2":"code","3b2abd00":"code","5918c28a":"code","dbe5e189":"code","059437f0":"code","28516388":"code","03c2d995":"code","182fa527":"code","ae1a7ca7":"code","aae5d344":"code","23030b6f":"code","6420002e":"code","5c96425a":"code","43e5d006":"code","30ad7386":"code","3b6edec3":"code","0d9116b1":"code","423a57f9":"code","c8ab7666":"code","c44d5888":"code","f0d745b2":"code","cbb01a92":"code","c96150fa":"code","cfdff38e":"code","3e0ad5a3":"code","f8805499":"code","4a7d5230":"code","7195ec7b":"code","c607aa73":"code","630b5463":"code","9d164313":"code","1ff38a01":"code","50be7574":"code","3217b3bd":"code","9d0459b0":"code","3180862a":"code","e07992ff":"code","199e258a":"code","60c03664":"code","50067877":"code","c05069c1":"code","49c5a5f3":"code","258d47a8":"code","43e34465":"code","db3c514e":"code","089f50a2":"code","c1f2be9a":"code","897cf612":"code","b655f171":"code","dda75207":"code","5aa024be":"code","36d80e98":"code","b391ed19":"code","cdd37a93":"code","76cdabe8":"code","1f87fa5d":"code","e3c7cab2":"code","e8938a50":"code","00211c96":"code","4e322454":"code","dd7f9d2f":"code","490449fa":"code","c98b7123":"code","4811f4e5":"code","9c2da40d":"code","b3b2d8f2":"code","e472b5bb":"code","50c059b3":"code","2ceca153":"markdown","611213ad":"markdown","e1390b67":"markdown","8446a419":"markdown","02a2205a":"markdown","b5450946":"markdown","1bb9c0f1":"markdown","8a5cc31f":"markdown","0345c8f7":"markdown","74af4d59":"markdown","8eb40aa2":"markdown","f3e15c66":"markdown","3017f426":"markdown","87d3b484":"markdown","f6e711cc":"markdown","b932b0c6":"markdown","0b1f6c94":"markdown","cf9033bb":"markdown","9a863069":"markdown","45108b25":"markdown","851861f8":"markdown","fe7af1c2":"markdown","d15bac2e":"markdown","6a83cc49":"markdown","63fb9ac9":"markdown","746ee882":"markdown","39b5406c":"markdown","e09fa848":"markdown","c2833d7b":"markdown","4d42d1eb":"markdown","a063f076":"markdown","08cf9bb1":"markdown","0be5bcee":"markdown","bbbb2171":"markdown","45ff376d":"markdown","ddc3f98f":"markdown","d622a9b5":"markdown","e75a74ee":"markdown","cd369d03":"markdown","74cc0981":"markdown","b0caa88e":"markdown","e8d78875":"markdown","5e57c353":"markdown","03a78889":"markdown","1bf35fdf":"markdown","d8a161b2":"markdown","46af21f6":"markdown"},"source":{"19136f6f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aef00a50":"import pandas as pd\nimport numpy as np\n\n# Data visualisation & images\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Pipeline and machine learning algorithms\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n# Model fine-tuning and evaluation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn import model_selection\n\n%matplotlib inline","17bee306":"#Load the train and test data from the dataset\ndf_train=pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test=pd.read_csv('..\/input\/titanic\/test.csv')","0e4cb820":"# Join all data into one file\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\n# Creating y_train variable; we'll need this when modelling, but not before\ny_train = df_train['Survived'].values\n\n# Saving the passenger ID's ready for our submission file at the very end\npassId = df_test['PassengerId']\n\n# Create a new all-encompassing dataset\ndata = pd.concat((df_train, df_test))\n\n# Printing overall data shape\nprint(\"data size is: {}\".format(data.shape))","9dc1b8e5":"df_train.info()","6cd632d1":"df_train.head()","d2423be5":"# Returning descriptive statistics of the train dataset\ndf_train.describe(include = 'all')","91600caf":"df_test.info()","c1e1b674":"df_test.head()","4242a9c7":"# Initiate correlation matrix\ncorr = df_train.corr()  # Pandas dataframe.corr() is used to find the pairwise correlation of all columns in the dataframe. \n# Set-up mask\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set-up figure\nplt.figure(figsize=(14, 8))\n# Title\nplt.title('Overall Correlation of Titanic Features', fontsize=18)\n# Correlation matrix\nsns.heatmap(corr, mask=mask, annot=False,cmap='RdYlGn', linewidths=0.2, annot_kws={'size':20})\nplt.show()","7bcd594b":"# Feature: Survived\n\n# Plot for survived\nfig = plt.figure(figsize = (10,5))\nsns.countplot(x='Survived', data = df_train)\nprint(df_train['Survived'].value_counts())","df6a3027":"# Feature: Pclass\n# Bar chart of each Pclass type\nfig = plt.figure(figsize = (10,10))\nax1 = plt.subplot(2,1,1)\nax1 = sns.countplot(x = 'Pclass', hue = 'Survived', data = df_train)\nax1.set_title('Ticket Class Survival Rate')\nax1.set_xticklabels(['1 Upper','2 Middle','3 Lower'])\nax1.set_ylim(0,400)\nax1.set_xlabel('Ticket Class')\nax1.set_ylabel('Count')\nax1.legend(['No','Yes'])\n\n# Pointplot Pclass type\nax2 = plt.subplot(2,1,2)\nsns.pointplot(x='Pclass', y='Survived', data=df_train)\nax2.set_xlabel('Ticket Class')\nax2.set_ylabel('Percent Survived')\nax2.set_title('Percentage Survived by Ticket Class')","12058054":"# Feature: Age\n# Bar chart of age mapped against sex. For now, missing values have been dropped and will be dealt with later\nsurvived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = df_train[df_train['Sex']=='female']\nmen = df_train[df_train['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=20, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=20, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=20, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=20, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","76364487":"# Feature: SibSp & ParCh\n# Plotting survival rate vs Siblings or Spouse on board\nfig = plt.figure(figsize = (10,12))\nax1 = plt.subplot(2,1,1)\nax1 = sns.countplot(x = 'SibSp', hue = 'Survived', data = df_train)\nax1.set_title('Survival Rate with Total of Siblings and Spouse on Board')\nax1.set_ylim(0,500)\nax1.set_xlabel('# of Sibling and Spouse')\nax1.set_ylabel('Count')\nax1.legend(['No','Yes'],loc = 1)\n\n# Plotting survival rate vs Parents or Children on board\nax2 = plt.subplot(2,1,2)\nax2 = sns.countplot(x = 'Parch', hue = 'Survived', data = df_train)\nax2.set_title('Survival Rate with Total Parents and Children on Board')\nax2.set_ylim(0,500)\nax2.set_xlabel('# of Parents and Children')\nax2.set_ylabel('Count')\nax2.legend(['No','Yes'],loc = 1)","bb317aea":"# Feature: Fare\n# Bar chart of each Fare type\nfig = plt.figure(figsize = (10,10))\nax1 = sns.countplot(x = 'Pclass', hue = 'Survived', data = df_train)\nax1.set_title('Ticket Class Survival Rate with respect to fare')\nax1.set_xticklabels(['1 Upper','2 Middle','3 Lower'])\nax1.set_xlabel('Ticket Class')\nax1.set_ylabel('Fare')\nax1.legend(['No','Yes'])","f8fbcdba":"# Graph to display fare paid per the three ticket types\nfig = plt.figure(figsize = (10,5))\nsns.swarmplot(x=\"Pclass\", y=\"Fare\", data=df_train, hue='Survived')","f1f96452":"print(\"TRAIN DATA:\")\ndf_train.isnull().sum()","6b2b5f04":"sns.heatmap(df_train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n","e74a623c":"print(\"TEST DATA:\")\ndf_test.isnull().sum()","860591c6":"sns.heatmap(df_test.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n","e6468af2":"# Extract last name\ndata['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n# Fill in missing Fare value by overall Fare mean\ndata['Fare'].fillna(data['Fare'].mean(), inplace=True)\n\n# Setting coin flip (e.g. random chance of surviving)\ndefault_survival_chance = 0.5\ndata['Family_Survival'] = default_survival_chance\n\n# Grouping data by last name and fare - looking for families\nfor grp, grp_df in data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    # If not equal to 1, a family is found \n    # Then work out survival chance depending on whether or not that family member survived\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin == 0.0):\n                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n\n# Print the headline\nprint(\"Number of passengers with family survival information:\", \n      data.loc[data['Family_Survival']!=0.5].shape[0])","3b2abd00":"# If not equal to 1, a group member is found\n# Then work out survival chance depending on whether or not that group member survived\nfor _, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n\n# Print the headline\nprint(\"Number of passenger with family\/group survival information: \" \n      +str(data[data['Family_Survival']!=0.5].shape[0]))","5918c28a":"# Reset index for remaining feature engineering steps\ndata = data.reset_index(drop=True)\ndata = data.drop('Survived', axis=1)\ndata.tail()","dbe5e189":"# Visualising fare data\nplt.hist(data['Fare'], bins=40)\nplt.xlabel('Fare')\nplt.ylabel('Count')\nplt.title('Distribution of fares')\nplt.show()","059437f0":"# Turning fare into 4 bins due to heavy skew in data\ndata['Fare'] = pd.qcut(data['Fare'], 4)\n\n# I will now use Label Encoder to convert the bin ranges into numbers\nlbl = LabelEncoder()\ndata['Fare'] = lbl.fit_transform(data['Fare'])","28516388":"# Visualise new look fare variable\nsns.countplot(data['Fare'])\nplt.xlabel('Fare Bin')\nplt.ylabel('Count')\nplt.title('Fare Bins')","03c2d995":"# Inspecting the first five rows of Name\ndf_train['Name'].head()","182fa527":"# New function to return name title only\ndef get_title(name):\n    if '.' in name:\n        return name.split(',')[1].split('.')[0].strip()\n    else:\n        return 'Unknown'","ae1a7ca7":"# Creating two lists of titles, one for each dataset\ntitles_data = sorted(set([x for x in data['Name'].map(lambda x: get_title(x))]))\n\n# Printing list length and items in each list\nprint(len(titles_data), ':', titles_data)","aae5d344":"# New function to classify each title into 1 of 4 overarching titles\ndef set_title(x):\n    title = x['Title']\n    if title in ['Capt', 'Col', 'Don', 'Jonkheer', 'Major', 'Rev', 'Sir']:\n        return 'Mr'\n    elif title in ['the Countess', 'Mme', 'Lady','Dona']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title","23030b6f":"# Applying the get_title function to create the new 'Title' feature\ndata['Title'] = data['Name'].map(lambda x: get_title(x))\ndata['Title'] = data.apply(set_title, axis=1)","6420002e":"# Printing values of the title column (checking function worked!)\nprint(data['Title'].value_counts())","5c96425a":"# Returning NaN within Age across Train & Test set\nprint('Total missing age data: ', pd.isnull(data['Age']).sum())\n","43e5d006":"# Check which statistic to use in imputation\nprint(data['Age'].describe(exclude='NaN'))","30ad7386":"#Imputing Age within the train & test set with the Median, grouped by Pclass and title\ndata['Age'] = data.groupby('Title')['Age'].apply(lambda x: x.fillna(x.median()))","3b6edec3":"# Visualise new look age variable\nplt.hist(data['Age'], bins=40)\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Distribution of ages')\nplt.show()","0d9116b1":"# Turning data into 4 bins due to heavy skew in data\ndata['Age'] = pd.qcut(data['Age'], 4)\n\n# Transforming bins to numbers\nlbl = LabelEncoder()\ndata['Age'] = lbl.fit_transform(data['Age'])","423a57f9":"# Visualise new look fare variable\nplt.xticks(rotation='90')\nsns.countplot(data['Age'])\nplt.xlabel('Age Bin')\nplt.ylabel('Count')\nplt.title('Age Bins')","c8ab7666":"#transferring the titles over to numbers ready for Machine Learning\ndata['Title'] = data['Title'].replace(['Mr', 'Miss', 'Mrs', 'Master'], [0, 1, 2, 3])","c44d5888":"# Recoding sex to numeric values with use of a dictionary for machine learning model compatibility\ndata['Sex'] = data['Sex'].replace(['male', 'female'], [0, 1])","f0d745b2":"# Inspecting the first five rows of Embarked\ndata['Embarked'].head()","cbb01a92":"data['Embarked'].describe()","c96150fa":"# Filling in missing embarked values with the mode (S)\ndata['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])\n\n# Converting to numeric values\ndata['Embarked'] = data['Embarked'].replace(['S', 'C', 'Q'], [0, 1, 2])","cfdff38e":"# Inspecting head of Cabin column\ndata['Cabin'].head()","3e0ad5a3":"# Labelling all NaN values as 'Unknown'\ndata['Cabin'].fillna('Unknown',inplace=True)","f8805499":"# Extracting the first value in the each row of Cabin\ndata['Cabin'] = data['Cabin'].map(lambda x: x[0])","4a7d5230":"# Return the counts of each unique value in the Cabin column\ndata['Cabin'].value_counts()","7195ec7b":"# New function to classify known cabins as 'Known', otherwise 'Unknown'\ndef unknown_cabin(cabin):\n    if cabin != 'U':\n        return 1\n    else:\n        return 0\n    \n# Applying new function to Cabin feature\ndata['Cabin'] = data['Cabin'].apply(lambda x:unknown_cabin(x))","c607aa73":"# Creating two features of relatives and not alone\ndata['FamilySize'] = data['SibSp'] + data['Parch'] + 1\ndata['IsAlone'] = 1 #initialize to yes\/1 is alone\ndata['IsAlone'].loc[data['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n","630b5463":"# Final look at the data\ndata.head()","9d164313":"# Dropping what we know need for Machine Learning\ndata = data.drop(['Name', 'Parch', 'SibSp', 'Ticket', 'Last_Name', 'PassengerId'], axis = 1)","1ff38a01":"# Return to train\/test sets\ntrain = data[:ntrain]\ntest = data[ntrain:]","50be7574":"# Set up feature and target variables in train set, and remove Passenger ID from test set\nX_test = test\nX_train = train\n\n# Scaling data to support modelling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","3217b3bd":"# Initiate 11 classifier models\nran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier()\ngbc = GradientBoostingClassifier()\nsvc = SVC(probability=True)\next = ExtraTreesClassifier()\nada = AdaBoostClassifier()\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier()\n\n# Prepare lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores = []\n\n# Sequentially fit and cross validate all models\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores.append(acc.mean())","9d0459b0":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Score': scores})\n\nresult_df = results.sort_values(by='Score', ascending=False).reset_index(drop=True)\nresult_df.head(11)","3180862a":"# Plot results\nsns.barplot(x='Score', y = 'Model', data = result_df, color = 'c')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\nplt.xlim(0.80, 0.86)","e07992ff":"# Function for new graph\ndef importance_plotting(data, x, y, palette, title):\n    sns.set(style=\"whitegrid\")\n    ft = sns.PairGrid(data, y_vars=y, x_vars=x, size=5, aspect=1.5)\n    ft.map(sns.stripplot, orient='h', palette=palette, edgecolor=\"black\", size=15)\n    \n    for ax, title in zip(ft.axes.flat, titles):\n    # Set a different title for each axes\n        ax.set(title=title)\n    # Make the grid horizontal instead of vertical\n        ax.xaxis.grid(False)\n        ax.yaxis.grid(True)\n    plt.show()","199e258a":"# Building feature importance into a DataFrame\nfi = {'Features':train.columns.tolist(), 'Importance':xgb.feature_importances_}\nimportance = pd.DataFrame(fi, index=None).sort_values('Importance', ascending=False)","60c03664":"# Creating graph title\ntitles = ['The most important features in predicting survival on the Titanic: XGBoost']\n\n# Plotting graph\nimportance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)","50067877":"# Building feature importance into a DataFrame\nfi = {'Features':train.columns.tolist(), 'Importance':np.transpose(log.coef_[0])}\nimportance = pd.DataFrame(fi, index=None).sort_values('Importance', ascending=False)","c05069c1":"# Creating graph title\ntitles = ['The most important features in predicting survival on the Titanic: Logistic Regression']\n\n# Plotting graph\nimportance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)","49c5a5f3":"# Getting feature importances for the 5 models where we can\ngbc_imp = pd.DataFrame({'Feature':train.columns, 'gbc importance':gbc.feature_importances_})\nxgb_imp = pd.DataFrame({'Feature':train.columns, 'xgb importance':xgb.feature_importances_})\nran_imp = pd.DataFrame({'Feature':train.columns, 'ran importance':ran.feature_importances_})\next_imp = pd.DataFrame({'Feature':train.columns, 'ext importance':ext.feature_importances_})\nada_imp = pd.DataFrame({'Feature':train.columns, 'ada importance':ada.feature_importances_})\n\n# Merging results into a single dataframe\nimportances = gbc_imp.merge(xgb_imp, on='Feature').merge(ran_imp, on='Feature').merge(ext_imp, on='Feature').merge(ada_imp, on='Feature')\n\n# Calculating average importance per feature\nimportances['Average'] = importances.mean(axis=1)\n\n# Ranking top to bottom\nimportances = importances.sort_values(by='Average', ascending=False).reset_index(drop=True)\n\n# Display\nimportances","258d47a8":"# Building feature importance into a DataFrame\nfi = {'Features':importances['Feature'], 'Importance':importances['Average']}\nimportance = pd.DataFrame(fi, index=None).sort_values('Importance', ascending=False)","43e34465":"# Creating graph title\ntitles = ['The most important features in predicting survival on the Titanic: 5 model average']\n\n# Plotting graph\nimportance_plotting(importance, 'Importance', 'Features', 'Reds_r', titles)","db3c514e":"# Drop redundant features\ntrain = train.drop(['Embarked','IsAlone'], axis=1)\ntest = test.drop(['Embarked', 'IsAlone'], axis=1)\n\n# Re-build model variables\nX_train = train\nX_test = test\n\n# Transform\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","089f50a2":"# Initiate models\nran = RandomForestClassifier(random_state=1)\nknn = KNeighborsClassifier()\nlog = LogisticRegression()\nxgb = XGBClassifier(random_state=1)\ngbc = GradientBoostingClassifier(random_state=1)\nsvc = SVC(probability=True)\next = ExtraTreesClassifier(random_state=1)\nada = AdaBoostClassifier(random_state=1)\ngnb = GaussianNB()\ngpc = GaussianProcessClassifier()\nbag = BaggingClassifier(random_state=1)\n\n# Lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores_v2 = []\n\n# Fit & cross validate\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores_v2.append(acc.mean())","c1f2be9a":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Original Score': scores,\n    'Score with feature selection': scores_v2})\n\nresult_df = results.sort_values(by='Score with feature selection', ascending=False).reset_index(drop=True)\nresult_df.head(11)","897cf612":"# Plot results\nsns.barplot(x='Score with feature selection', y = 'Model', data = result_df, color = 'c')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\nplt.xlim(0.80, 0.86)","b655f171":"# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [1, 3, 5, 7]\nmin_samples_split = [2, 4, 6, 8, 10]\nmin_samples_leaf = [2, 4, 6, 8, 10]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = RandomForestClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","dda75207":"\n# Parameter's to search\nn_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = [1, 2, 3, 4, 5, 10, 15, 20, 25, 30]\n\n# Setting up parameter grid\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","5aa024be":"\n# Parameter's to search\npenalty = ['l1', 'l2']\nC = np.logspace(0, 4, 10)\n\n# Setting up parameter grid\nhyperparams = {'penalty': penalty, 'C': C}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = LogisticRegression(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","36d80e98":"# Parameter's to search\nlearning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [10, 25, 50, 75, 100, 250, 500, 750, 1000]\n\n# Setting up parameter grid\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = XGBClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","b391ed19":"max_depth = [3, 4, 5, 6, 7, 8, 9, 10]\nmin_child_weight = [1, 2, 3, 4, 5, 6]\n\nhyperparams = {'max_depth': max_depth, 'min_child_weight': min_child_weight}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.0001, n_estimators=10), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","cdd37a93":"gamma = [i*0.1 for i in range(0,5)]\n\nhyperparams = {'gamma': gamma}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.0001, n_estimators=10, max_depth=3, \n                                          min_child_weight=1), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","76cdabe8":"subsample = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\ncolsample_bytree = [0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1]\n    \nhyperparams = {'subsample': subsample, 'colsample_bytree': colsample_bytree}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.0001, n_estimators=10, max_depth=3, \n                                          min_child_weight=1, gamma=0), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","1f87fa5d":"reg_alpha = [1e-5, 1e-2, 0.1, 1, 100]\n    \nhyperparams = {'reg_alpha': reg_alpha}\n\ngd=GridSearchCV(estimator = XGBClassifier(learning_rate=0.0001, n_estimators=10, max_depth=3, \n                                          min_child_weight=1, gamma=0, subsample=0.6, colsample_bytree=0.9),\n                                         param_grid = hyperparams, verbose=True, cv=5, scoring = \"accuracy\")\n\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","e3c7cab2":"\n# Parameter's to search\nlearning_rate = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.2]\nn_estimators = [100, 250, 500, 750, 1000, 1250, 1500]\n\n# Setting up parameter grid\nhyperparams = {'learning_rate': learning_rate, 'n_estimators': n_estimators}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = GradientBoostingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","e8938a50":"# Parameter's to search\nCs = [0.001, 0.01, 0.1, 1, 5, 10, 15, 20, 50, 100]\ngammas = [0.001, 0.01, 0.1, 1]\n\n# Setting up parameter grid\nhyperparams = {'C': Cs, 'gamma' : gammas}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = SVC(probability=True), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","00211c96":"\n# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100]\nmax_depth = [3, None]\nmax_features = [1, 3, 5, 7]\nmin_samples_split = [2, 4, 6, 8, 10]\nmin_samples_leaf = [2, 4, 6, 8, 10]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_depth': max_depth, 'max_features': max_features,\n               'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = ExtraTreesClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","4e322454":"\n# Parameter's to search\nn_estimators = [10, 25, 50, 75, 100, 125, 150, 200]\nlearning_rate = [0.001, 0.01, 0.1, 0.5, 1, 1.5, 2]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'learning_rate': learning_rate}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = AdaBoostClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","dd7f9d2f":"# Parameter's to search\nn_restarts_optimizer = [0, 1, 2, 3]\nmax_iter_predict = [1, 2, 5, 10, 20, 35, 50, 100]\nwarm_start = [True, False]\n\n# Setting up parameter grid\nhyperparams = {'n_restarts_optimizer': n_restarts_optimizer, 'max_iter_predict': max_iter_predict, 'warm_start': warm_start}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = GaussianProcessClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","490449fa":"# Parameter's to search\nn_estimators = [10, 15, 20, 25, 50, 75, 100, 150]\nmax_samples = [1, 2, 3, 5, 7, 10, 15, 20, 25, 30, 50]\nmax_features = [1, 3, 5, 7]\n\n# Setting up parameter grid\nhyperparams = {'n_estimators': n_estimators, 'max_samples': max_samples, 'max_features': max_features}\n\n# Run GridSearch CV\ngd=GridSearchCV(estimator = BaggingClassifier(), param_grid = hyperparams, \n                verbose=True, cv=5, scoring = \"accuracy\")\n\n# Fitting model and return results\ngd.fit(X_train, y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","c98b7123":"# Initiate tuned models\n\nran = RandomForestClassifier(n_estimators=50,\n                             max_depth=3, \n                             max_features=7,\n                             min_samples_leaf=8, \n                             min_samples_split=6,  \n                             random_state=1)\n\nknn = KNeighborsClassifier(algorithm='auto', \n                           leaf_size=3, \n                           n_neighbors=10, \n                           weights='uniform')\n\nlog = LogisticRegression(C=21.544346900318832,\n                         penalty='l2')\n\nxgb = XGBClassifier(learning_rate=0.0001, \n                    n_estimators=10,\n                    random_state=1)\n\ngbc = GradientBoostingClassifier(learning_rate=0.0005,\n                                 n_estimators=1250,\n                                 random_state=1)\n\nsvc = SVC(C=50, gamma=0.01, probability=True)\n\n\next = ExtraTreesClassifier(max_depth=3, \n                           max_features=7,\n                           min_samples_leaf=8, \n                           min_samples_split=4,\n                           n_estimators=25,\n                           random_state=1)\n\nada = AdaBoostClassifier(learning_rate=0.5, \n                         n_estimators=25,\n                         random_state=1)\n\ngpc = GaussianProcessClassifier(max_iter_predict=1)\n\nbag = BaggingClassifier(max_features=7, max_samples=50, n_estimators=20,random_state=1)\n\n# Lists\nmodels = [ran, knn, log, xgb, gbc, svc, ext, ada, gnb, gpc, bag]         \nscores_v3 = []\n\n# Fit & cross-validate\nfor mod in models:\n    mod.fit(X_train, y_train)\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 10)\n    scores_v3.append(acc.mean())","4811f4e5":"# Creating a table of results, ranked highest to lowest\nresults = pd.DataFrame({\n    'Model': ['Random Forest', 'K Nearest Neighbour', 'Logistic Regression', 'XGBoost', 'Gradient Boosting', 'SVC', 'Extra Trees', 'AdaBoost', 'Gaussian Naive Bayes', 'Gaussian Process', 'Bagging Classifier'],\n    'Original Score': scores,\n    'Score with feature selection': scores_v2,\n    'Score with tuned parameters': scores_v3})\n\nresult_df = results.sort_values(by='Score with tuned parameters', ascending=False).reset_index(drop=True)\nresult_df.head(11)","9c2da40d":"# Plot results\nsns.barplot(x='Score with tuned parameters', y = 'Model', data = result_df, color = 'c')\nplt.title('Machine Learning Algorithm Accuracy Score \\n')\nplt.xlabel('Accuracy Score (%)')\nplt.ylabel('Algorithm')\nplt.xlim(0.82, 0.86)","b3b2d8f2":"#Hard Vote or majority rules w\/Tuned Hyperparameters\ngrid_hard = VotingClassifier(estimators = [('Random Forest', ran), \n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbour', knn),\n                                           ('Bagging Classifier', bag)], voting = 'hard')\n\ngrid_hard_cv = model_selection.cross_validate(grid_hard, X_train, y_train, cv = 10, return_train_score=True)\ngrid_hard.fit(X_train, y_train)\n \n\nprint(\"Hard voting on train set score mean: {:.2f}\". format(grid_hard_cv['train_score'].mean()*100))\nprint(\"Hard voting on test set score mean: {:.2f}\". format(grid_hard_cv['test_score'].mean()*100))\n","e472b5bb":"grid_soft = VotingClassifier(estimators = [('Random Forest', ran), \n                                           ('Logistic Regression', log),\n                                           ('XGBoost', xgb),\n                                           ('Gradient Boosting', gbc),\n                                           ('Extra Trees', ext),\n                                           ('AdaBoost', ada),\n                                           ('Gaussian Process', gpc),\n                                           ('SVC', svc),\n                                           ('K Nearest Neighbour', knn),\n                                           ('Bagging Classifier', bag)], voting = 'soft')\n\ngrid_soft_cv = model_selection.cross_validate(grid_soft, X_train, y_train, cv = 10, return_train_score=True)\ngrid_soft.fit(X_train, y_train)\n\nprint(\"Soft voting on train set score mean: {:.2f}\". format(grid_soft_cv['train_score'].mean()*100)) \nprint(\"Soft voting on test set score mean: {:.2f}\". format(grid_soft_cv['test_score'].mean()*100))\n","50c059b3":"# Final predictions\npredictions = grid_hard.predict(X_test)\n\nsubmission = pd.concat([pd.DataFrame(passId), pd.DataFrame(predictions)], axis = 'columns')\n\nsubmission.columns = [\"PassengerId\", \"Survived\"]\nsubmission.to_csv('titanic_submission.csv', header = True, index = False)","2ceca153":"It looks clear now that Embarked & IsAlone really aren't helping us out, and therefore I am going to get rid of them. ","611213ad":"### **Observations:**\n**In case of Train Data:**\n* 177 values are missing from Age feature.\n* 687 values are missing from Cabin feature.\n* 2 values are missing from Embarked feature.\n\n**In case of test Data:**\n* 86 values are missing from Age feature.\n* 327 values are missing from Cabin feature.\n* 1 value is missing from Cabin fare.","e1390b67":"### **Observation:**\nThese graphs reveal that overall women were much more likely to survive than men, and this is largely regardless of age. For both sexes, it appears that chances of survival are more likely at a younger age, which is what might have been expected. From the age of 20, it was consistently more likely that men would not have survived, up until their age approached 80. For women, apart from a potentially anomalous finding around the 8-9 bracket, they were always more likely to survive.","8446a419":"1.  **Group Information**","02a2205a":"### **Voting Classifier**\nVoting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.","b5450946":"# **Titanic: Machine Learning from Disaster**\n### **Contents of the Notebook:**\n* Loading test and train dataset\n* Exploratory Data Analysis\n* Checking missing\/null value\n* Feature engineering\n* ML model implementation\n* Feature Selection\n* Model re-training\n* Model (hyper-parameter) tuning\n* Model re-training\n* Implementing voting classifier\n* Final model prediction & submission\n\n### **Dataset Description:**\n\n* The training set should be used to build the machine learning models. For the training set, they provided the outcome (also known as the \u201cground truth\u201d) for each passenger. \n* The data has been split into two groups:\n  * training set (train.csv)\n  * test set (test.csv)\n* Kaggle Titanic: Machine Learning from Disaster dataset Link : https:\/\/www.kaggle.com\/c\/titanic\/data\n\nLet's jump in :)","1bb9c0f1":"### **Feature engineering :**\n\n1. Family Information","8a5cc31f":"8. **SibSp & Parch**","0345c8f7":"3. **Logistic Regression**","74af4d59":"7. **Cabin**","8eb40aa2":"As previously seen, there is an overwhelming majority of unknown Cabins in the train dataset. Based on this, the best option here might be to create two groups: known and unknown. This will avoid overfitting on the sparse data by cabin level, and, with the help of a new function, is what will be computed next.","f3e15c66":"### **Feature selection**","3017f426":"4. **Age**","87d3b484":"### **Model (hyper-parameter) tuning**\n1. **Random Forest**","f6e711cc":"### **Observation:**\nFare has been displayed per ticket type, revealing that those within Pclass 3 paid a similar fare to those in Pclass 2, but their chance of survival appears to be a lot lower. Pclass contains the highest fares, along with the highest rate of survivial, where orange points denotes the higher ratio .","b932b0c6":"7. **Extra Trees**","0b1f6c94":"9. **Gaussian Process**","cf9033bb":"# **Exploratory Data Analysis**\n\nBefore we can know how much (or little) feature engineering is needed, we need to have a good sense of what we're working with. The simple explorations discussed above are useful in terms of getting a holistic view of the overall dataset. To understand more about specific features, it is considered best practice to visualise it first. Now. let's walk through some simple visualisations, beginning with a correlation matrix.\n### **Visualising The Train Data**","9a863069":"Here, we got 18 unique title values which is a lot, and I anticipate that for many only a few observations exist, which isn't helpful. I'm going to keep this simple and band titles in one of four categories: Mr, Mrs, Master & Miss. To help me complete this I will define my own handy function - see below:","45108b25":"### **Machine Learning Implementation**\nBefore we can fit models, a few more steps are needed in order to get the data in the correct shape for modelling. This involves re-splitting the train & test datasets, followed by setting up our X_train & X_test variables. Note that we already have our y_train variable from before. We don't have a y_test variable, this would be the survival stat per users in the test set, and this is what we are looking to predict!","851861f8":"### **Thank you...! :)**\n\n### **Please upvote if you like the notebook. This will keep me motivated... :)**","fe7af1c2":"There are two missing values for Embarked - let's replace it with the most frequently occurring value. I'll then convert the letters to numeric values.","d15bac2e":"11. **Gaussian Naive Bayes**\n\nGaussian Naive Bayes doesn't have parameters to tune, so we're stuck with the current score. This algorithm is known to be designed to work best on text data (e.g. once passed into a matrix), so perhaps in comparison to the other algorithms, it's less of a surprise to see it performing less favourably on the Titanic dataset.","6a83cc49":"Upon closer inspection into cabin, we can see that it follows a Letter\/Number format. A bit of extra internet research reveals that the letter actually refers to the floor in the titanic where each passenger resided. This information may be helpful in the prediction, e.g. did those in lower cabins have a smaller\/larger chance of survival? Therefore we will begin by extracting the letter only from the Cabin column, and then labelling all NaN's with an 'Unknown' cabin reference.","63fb9ac9":"### **Observation:**\nNot surprisingly, the structure of these two graphs appear similar, with a similar density of passengers featured within each count, with also a similar ratio of survived vs not survived. This adds further rationale for these two features to be combined, which will be performed at the Data Preprocessing stage.","746ee882":"6. **SVC**","39b5406c":"### **Final model prediction & submission**","e09fa848":"Here,mean and percentile breakdown indicates multiple features converging around the 30 mark, which perhaps isn't surprising. Based on this it may be better to proceed with imputing with the median (middle) value. What i'm now going to do is group the dataset by the four different titles, and then impute the missing age values with the average age of each title, be that Mr, Mrs, Master or Miss. The below code completes this:","c2833d7b":"Now, we want to now see how heavily each feature was leaned in the modelling process. Let's look at what feature XGBoost found most useful when achieved the top score in round one. To help present this data i'm going to construct my own gragh - code below.\n\n","4d42d1eb":"Perhaps not the most insightful view at this stage given that some features are pending engineering, however a visible correlation does exist between Survived and Pclass and Fare. Age, SibSp & Parch would also seem like logical predictors and it would be expected that after these variables have been preprocessed their correlation to Survived will increase.\n\nLet's get an initial sense of these features then, beginning with the target: Survived.","a063f076":"The full names as they are will not be helpful to us, although, there's probably something useful within title e.g. categorising males and females, boys and girls. Therefore, i'm going to extract this data and create a new feature for Title, before binning Name.","08cf9bb1":"\n### **Checking Missing Data**","0be5bcee":"6. **Embarked**","bbbb2171":"### **Import Necessary Library**","45ff376d":"2. **K Neighbors Classifier**","ddc3f98f":"3. **Name**","d622a9b5":"### **Observaion:**\nfrom the above figure we came to know that,the higher class ticket, the more likely one is to have survived. ","e75a74ee":"4. **XGBoost**","cd369d03":"### **Model re-training**","74cc0981":"5. **Gradient Boosting Classifier**","b0caa88e":"### **Model re-training**","e8d78875":"5. **Sex**","5e57c353":"2. **Fare**","03a78889":"8. **Adaboost**","1bf35fdf":"### **Loading Train and Test Dataset**","d8a161b2":"10. **Bagging Classifier**","46af21f6":"### **Feature Description:**\n* The **Survived** variable is our outcome or dependent variable. It is a binary nominal datatype of 1 for survived and 0 for did not survive. \n\n* The **PassengerID** and **Ticket** variables are assumed to be random unique identifiers, that have no impact on the outcome variable. Thus, they will be excluded from analysis.\n* The **Pclass** variable is an ordinal datatype for the ticket class where, 1 = upper class, 2 = middle class, and 3 = lower class.\n* The **Name** variable is a nominal datatype. It could be used in feature engineering to derive the gender from title, family size from surname, and  socio-economic status(SES) from titles like doctor or master. Since these variables already exist, we'll make use of it to see if title, like master, makes a difference.\n* The **Sex** and **Embarked** variables are a nominal datatype. They will be converted to dummy variables for mathematical calculations.\n* The **Age** and **Fare** variable are continuous quantitative datatypes.\n* The **SibSp** represents number of related siblings\/spouse aboard and **Parch** represents number of related parents\/children aboard. Both are discrete quantitative datatypes. This can be used for feature engineering to create a family size and is alone variable.\n* The **Cabin** variable is a nominal datatype that can be used in feature engineering for approximate position on ship when the incident occurred and SES from deck levels."}}