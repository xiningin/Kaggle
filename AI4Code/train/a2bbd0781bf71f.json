{"cell_type":{"59edc566":"code","3e4ae63f":"code","da3afd8e":"code","0743b5b9":"code","1c5905d3":"code","1ffecd93":"code","9cc7907c":"code","607dd6fd":"code","c84ac54c":"code","99b1195d":"code","105540dd":"code","da501610":"code","91323b2b":"code","a2a290aa":"code","c3121c62":"code","81654ecc":"code","38e1bc7f":"code","8fdf5791":"code","468ac053":"code","748ff6b7":"code","4b8c1feb":"code","af917994":"code","5c5c6388":"code","d71db5a5":"code","36d8e332":"code","95f660f5":"code","5f8cceb8":"code","4bc32b2e":"code","b36f477b":"code","8d9761af":"code","e9c33c19":"code","12b974e8":"code","7dedaaca":"code","f503399c":"code","d6f43a8a":"code","79d3de79":"code","af8d9ab1":"code","ede03456":"code","47bf5d7d":"code","d1f3b042":"code","dd4b22f0":"code","49cb3b2d":"code","3a93eee3":"code","53282a9e":"code","dbe1d1b5":"code","324243e2":"code","41756b92":"code","609d45a0":"code","dae773ee":"code","cc6e0dd6":"code","c782e206":"code","67a17b9e":"code","66334628":"code","8ab06c74":"markdown","3d1a77ff":"markdown","cb0d54b8":"markdown","0a147e65":"markdown","fa17d402":"markdown","124c0699":"markdown","510d4ae7":"markdown","bb7ddc84":"markdown","bd1d7729":"markdown","0cb7be87":"markdown","03861ab2":"markdown","7617845f":"markdown","9a9edb66":"markdown","9497d6c6":"markdown","516d61bf":"markdown","f0fa6c00":"markdown","df2d5b94":"markdown","e45cbcba":"markdown","3174bee6":"markdown","fa6f6acc":"markdown","f44c7732":"markdown","6decec14":"markdown","9171d908":"markdown","557433a6":"markdown","16603f9c":"markdown","42f08448":"markdown","993905bf":"markdown","5cc800d7":"markdown","8e7ad799":"markdown","e11cc202":"markdown","18837a25":"markdown","dbdf2238":"markdown","7d745251":"markdown","65204e2b":"markdown","7cd35933":"markdown","57dd2a77":"markdown","c4b423b8":"markdown","4d887d5a":"markdown","e3307948":"markdown","29dd7e3c":"markdown","a01bf01f":"markdown","af3be91a":"markdown"},"source":{"59edc566":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Plots\nimport folium\nfrom folium import features\nfrom folium.plugins import HeatMap\nfrom folium.plugins import MarkerCluster\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# For Prediction\nfrom fastai.tabular import *\nfrom fastai.tabular import add_datepart","3e4ae63f":"Tweets = pd.read_csv(\"..\/input\/uk-house-price\/Twitter_case_study_2_dataset.csv\")\nTweets.drop('index', axis=1, inplace=True)\nTweets.head(3)","da3afd8e":"Tweets.country.value_counts(ascending=True).tail().plot.barh();","0743b5b9":"Tweets.source.value_counts(ascending=True).tail().plot.barh();","1c5905d3":"df_map = folium.Map(location=[54.523293, -1.539852], zoom_start=3)\ndata = [[x[0], x[1], 1] for x in np.array(Tweets[['latitude', 'longitude']])]\nHeatMap(data, radius = 20).add_to(df_map)\ndf_map","1ffecd93":"'''map_wb = folium.Map(location=[54.523293, -1.539852],zoom_start=1)\nmc = MarkerCluster()\nfor ind,row in Tweets.iterrows():\n    mc.add_child(folium.CircleMarker(location=[row['latitude'],row['longitude']],\n                        radius=1,color='#3185cc'))\nmap_wb.add_child(mc)\n'''","9cc7907c":"Tweets= Tweets[Tweets['latitude']>48.8566]\ndf_map = folium.Map(location=[54.523293, -1.539852], zoom_start=6)\ndata = [[x[0], x[1], 1] for x in np.array(Tweets[['latitude', 'longitude']])]\nHeatMap(data, radius = 20).add_to(df_map)\ndf_map","607dd6fd":"Tweets['date'] = pd.to_datetime(Tweets['date'])\nTweets.head()","c84ac54c":"Cities = pd.read_csv(\"..\/input\/uk-house-price\/worldcities.csv\")\nCities= Cities[Cities['country'] == 'United Kingdom']\nCities.head(3)","99b1195d":"Cities.drop('city', axis=1, inplace=True)\nCities.drop('country', axis=1, inplace=True)\nCities.drop('iso2', axis=1, inplace=True)\nCities.drop('iso3', axis=1, inplace=True)\nCities.drop('admin_name', axis=1, inplace=True)\n#Cities.drop('capital', axis=1, inplace=True)\nCities.drop('id', axis=1, inplace=True)\nCities['city_ascii']=Cities['city_ascii'].str.upper()\nCities.head(3)","105540dd":"\n\ndef Count_Tweets(lat_city,long_city,ck):\n    Tweets_tmp = Tweets[(Tweets['latitude']<=lat_city+ck) & (Tweets['latitude']>=lat_city-ck) & (Tweets['longitude']>=long_city-ck) & (Tweets['longitude']<=long_city+ck) ]\n   #print(Tweets_tmp.shape[0])\n    return Tweets_tmp.shape[0]\n\n# Number of Tweets within 1 Degree Lat, Long\nck=1\nCities.loc[:,'Tweets_Count_1']=0\nCities['Tweets_Count_1'] = Cities.apply(lambda row: Count_Tweets(row['lat'], row['lng'], ck), axis=1)\n\n# Number of Tweets within 0.5 Degree Lat, Long\nck=0.5\nCities.loc[:,'Tweets_Count_05']=0\nCities['Tweets_Count_05'] = Cities.apply(lambda row: Count_Tweets(row['lat'], row['lng'], ck), axis=1)\n\n# Number of Tweets within 0.25 Degree Lat, Long\nck=0.25\nCities.loc[:,'Tweets_Count_025']=0\nCities['Tweets_Count_025'] = Cities.apply(lambda row: Count_Tweets(row['lat'], row['lng'], ck), axis=1)\n\n# Number of Tweets within 0.1 Degree Lat, Long\nck=0.01\nCities.loc[:,'Tweets_Count_001']=0\nCities['Tweets_Count_001'] = Cities.apply(lambda row: Count_Tweets(row['lat'], row['lng'], ck), axis=1)\n\n\nCities.head(3)","da501610":"df_map = folium.Map(location=[54.523293, -1.539852], zoom_start=5)\ndata = [[x[0], x[1], x[2]] for x in np.array(Cities[['lat', 'lng','Tweets_Count_05']])]\nHeatMap(data, radius = 20).add_to(df_map)\ndf_map","91323b2b":"Paid_Price = pd.read_csv(\"..\/input\/uk-house-price\/price_paid_records.csv\")\nPaid_Price=Paid_Price.sample(frac = 0.05) \nPaid_Price.head(3)","a2a290aa":"Paid_Price['Date of Transfer'] = pd.to_datetime(Paid_Price['Date of Transfer'])\nadd_datepart(Paid_Price, 'Date of Transfer')\nPaid_Price.dtypes","c3121c62":"len(Paid_Price.drop_duplicates())","81654ecc":"sns.boxplot(y = Paid_Price['Price'])\nplt.title('Price')","38e1bc7f":"print(len(Paid_Price))\nPaid_Price = Paid_Price.loc[(Paid_Price['Price'] < (500000)) & (Paid_Price['Price'] > (10000))]\nprint(len(Paid_Price))\nsns.boxplot(y = Paid_Price['Price'])\nplt.title('Price')","8fdf5791":"#Let us take a quick exploratory look at the distribution of house prices. We see that the majority of house prices across all years is less than \u00a3500,000.\nf, ax = plt.subplots(figsize=(8, 7))\n#Paid_Price=Paid_Price[Paid_Price['Price']<500000]\nPaid_Price['Price'].hist()\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")","468ac053":"data = pd.concat([Paid_Price['Price'], Paid_Price['County']], axis=1)\nf, ax = plt.subplots(figsize=(600, 12))\nfig = sns.boxplot(x=Paid_Price['County'], y=\"Price\", data=data)","748ff6b7":"Paid_Price_series=Paid_Price.groupby(['Date of TransferYear'])['Price'].mean().reset_index()\nPaid_Price_series.set_index('Date of TransferYear', inplace=True)\n#Paid_Price_series=Paid_Price_series[Paid_Price_series['Price']<500000]\nax=Paid_Price_series.plot(figsize= (8,6),title = 'Mean Price Variation over Years')#, xlabel='Years',ylabel = 'Mean Price Variation over Years')\n\n# Set the x-axis label\nax.set_xlabel(\"Year\")\n\n# Set the y-axis label\nax.set_ylabel(\"Mean House Price over Years\")","4b8c1feb":"Paid_Price_series=Paid_Price.groupby(['Date of TransferMonth'])['Price'].mean().reset_index()\nPaid_Price_series.set_index('Date of TransferMonth', inplace=True)\n#Paid_Price_series=Paid_Price_series[Paid_Price_series['Price']<500000]\nax=Paid_Price_series.plot(figsize= (8,6),title = 'Mean Price Variation over Month')#, xlabel='Years',ylabel = 'Mean Price Variation over Years')\n\n# Set the x-axis label\nax.set_xlabel(\"Months\")\n\n# Set the y-axis label\nax.set_ylabel(\"Mean House Price over Months\")","af917994":"#Date of TransferDayofweek             int64\nPaid_Price_series=Paid_Price.groupby(['Date of TransferDayofweek'])['Price'].mean().reset_index()\nPaid_Price_series.set_index('Date of TransferDayofweek', inplace=True)\n#Paid_Price_series=Paid_Price_series[Paid_Price_series['Price']<500000]\nax=Paid_Price_series.plot(figsize= (8,6),title = 'Mean Price Variation over Day of Week')#, xlabel='Years',ylabel = 'Mean Price Variation over Years')\n\n# Set the x-axis label\nax.set_xlabel(\"Day of Week\")\n\n# Set the y-axis label\nax.set_ylabel(\"Mean House Price over Day of Week\")","5c5c6388":"#Date of TransferIs_month_end\n#Date of TransferIs_month_start\n#Date of TransferIs_quarter_end\n#Date of TransferIs_quarter_start\n\n\nPaid_Price_series=Paid_Price.groupby(['Date of TransferIs_month_end'])['Price'].mean().reset_index()\nPaid_Price_series.set_index('Date of TransferIs_month_end', inplace=True)\n#Paid_Price_series=Paid_Price_series[Paid_Price_series['Price']<500000]\nax=Paid_Price_series.plot(figsize= (8,6),title = 'Mean Price Variation Month End')#, xlabel='Years',ylabel = 'Mean Price Variation over Years')\n\n# Set the x-axis label\nax.set_xlabel(\"Month End\")\n\n# Set the y-axis label\nax.set_ylabel(\"Mean House Price over Month End\")","d71db5a5":"#Date of TransferIs_month_end\n#Date of TransferIs_month_start\n#Date of TransferIs_quarter_end\n#Date of TransferIs_quarter_start\n\n\nPaid_Price_series=Paid_Price.groupby(['Date of TransferIs_quarter_end'])['Price'].mean().reset_index()\nPaid_Price_series.set_index('Date of TransferIs_quarter_end', inplace=True)\n#Paid_Price_series=Paid_Price_series[Paid_Price_series['Price']<500000]\nax=Paid_Price_series.plot(figsize= (8,6),title = 'Mean Price Variation over Quater End')#, xlabel='Years',ylabel = 'Mean Price Variation over Years')\n\n# Set the x-axis label\nax.set_xlabel(\"Quater End\")\n\n# Set the y-axis label\nax.set_ylabel(\"Mean House Price over Quater End\")","36d8e332":"print(Paid_Price.shape)\nPaid_Price_Tweet= pd.merge (Paid_Price, Cities , left_on= 'Town\/City' ,  right_on = 'city_ascii', how='left')\nprint(Paid_Price_Tweet.shape)\nPaid_Price_Tweet.head(3)","95f660f5":"Paid_Price_Tweet.drop('Transaction unique identifier', axis=1, inplace=True)\nPaid_Price_Tweet.drop('city_ascii', axis=1, inplace=True)\nPaid_Price_Tweet.drop('Date of TransferElapsed', axis=1, inplace=True)","5f8cceb8":"#print(Paid_Price.shape)\n#Paid_Price= pd.merge (Paid_Price, Cities , left_on= 'Town\/City' ,  right_on = 'city_ascii', how='left')\n#print(Paid_Price.shape)","4bc32b2e":"\n# Find Numerical and Categorical variables\nPaid_Price_Tweet['Price'] = Paid_Price_Tweet['Price'].astype(float)\nPaid_Price_Tweet.dtypes","b36f477b":"#Defining the keyword arguments for fastai's TabularList\n\ntrain_data = Paid_Price_Tweet\ndep_var = 'Price'\n\ncat_names = ['Property Type', 'Old\/New', 'Duration', 'Town\/City',\n            'District', 'County', 'PPDCategory Type',\n            'Record Status - monthly file only',\n            'Date of TransferIs_month_end', 'Date of TransferIs_month_start',\n            'Date of TransferIs_quarter_end', 'Date of TransferIs_quarter_start',\n            'Date of TransferIs_year_end', 'Date of TransferIs_year_start']\n\n\ncont_names = ['Date of TransferYear',\t'Date of TransferMonth',\t\n              'Date of TransferWeek',\t'Date of TransferDay',\t\n              'Date of TransferDayofweek',\t'Date of TransferDayofyear',\n              'lat',\t'lng','population',\t'Tweets_Count_1',\t'Tweets_Count_05',\n              'Tweets_Count_025',\t'Tweets_Count_001'\n              ]\npath =''\n\n\ntrain_data = train_data[cat_names + cont_names + [dep_var]]\n\n#List of Processes\/transforms to be applied to the dataset\nprocs = [FillMissing, Categorify, Normalize]\n\n#Start index for creating a validation set from train_data\nstart_indx = len(train_data) - int(len(train_data) * 0.2)\n\n#End index for creating a validation set from train_data\nend_indx = len(train_data)\n\n\n#TabularList for Validation\ntest = (TabularList.from_df(train_data.iloc[start_indx:end_indx].copy(), path=path, cat_names=cat_names, cont_names=cont_names))\n\n#test = val\n\n\n#TabularList for training\ndata = (TabularList.from_df(train_data, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_rand_pct(valid_pct = 0.1)\n                           #.split_by_idx(list(range(start_indx,end_indx)))\n                           .label_from_df(cols=dep_var, label_cls=FloatList)\n                           .add_test(test)\n                           .databunch())\n","8d9761af":"max_y = np.max(train_data['Price'])*1.2\ny_range = torch.tensor([0, max_y], device=defaults.device)\ny_range","e9c33c19":"learn = tabular_learner(data, layers=[600,300], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=[rmse,r2_score])","12b974e8":"learn.lr_find()\nlearn.recorder.plot()","7dedaaca":"learn.fit_one_cycle(1, 5e-2, wd=0.2)","f503399c":"learn.save(\"Learn_Tabular\")\nlearn.load(\"Learn_Tabular\")\nprint()","d6f43a8a":"## Not Executed due to high execution time - uncommemnt if required\n'''\n\ndep_var = 'Price'\n\ncat_names = ['Property Type', 'Old\/New', 'Duration', 'Town\/City',\n            'District', 'County', 'PPDCategory Type',\n            'Record Status - monthly file only',\n            'Date of TransferIs_month_end', 'Date of TransferIs_month_start',\n            'Date of TransferIs_quarter_end', 'Date of TransferIs_quarter_start',\n            'Date of TransferIs_year_end', 'Date of TransferIs_year_start']\n\n\ncont_names = ['Date of TransferYear',\t'Date of TransferMonth',\t\n              'Date of TransferWeek',\t'Date of TransferDay',\t\n              'Date of TransferDayofweek',\t'Date of TransferDayofyear',\n              'lat',\t'lng','population',\t'Tweets_Count_1',\t'Tweets_Count_05',\n              'Tweets_Count_025',\t'Tweets_Count_001'\n              ]\n              \ndef preprocess(Paid_Price_Short):\n    \n\n\n    Paid_Price_Short['Date of Transfer'] = pd.to_datetime(Paid_Price_Short['Date of Transfer'])\n    add_datepart(Paid_Price_Short, 'Date of Transfer')\n    Paid_Price_Short = Paid_Price_Short.loc[(Paid_Price_Short['Price'] < (500000)) & (Paid_Price_Short['Price'] > (10000))]\n\n\n    Paid_Price_Short_Tweet= pd.merge (Paid_Price_Short, Cities , left_on= 'Town\/City' ,  right_on = 'city_ascii', how='left')\n    Paid_Price_Short_Tweet.drop('Transaction unique identifier', axis=1, inplace=True)\n    Paid_Price_Short_Tweet.drop('city_ascii', axis=1, inplace=True)\n    Paid_Price_Short_Tweet.drop('Date of TransferElapsed', axis=1, inplace=True)\n    Paid_Price_Short_Tweet['Price'] = Paid_Price_Short_Tweet['Price'].astype(float)\n    \n    train_data = Paid_Price_Short_Tweet\n    train_data = train_data[cat_names + cont_names + [dep_var]]\n    data = (TabularList.from_df(train_data, path=path, cat_names=cat_names, cont_names=cont_names, procs=procs)\n                           .split_by_rand_pct(valid_pct = 0.1)\n                           #.split_by_idx(list(range(start_indx,end_indx)))\n                           .label_from_df(cols=dep_var, label_cls=FloatList)\n                           #.add_test(test)\n                           .databunch())\n    \n    \n   \n    #learn = tabular_learner(data, layers=[600,300], ps=[0.001,0.01], emb_drop=0.04, y_range=y_range, metrics=[rmse,r2_score])\n    #if COUNT !=0:\n    learn.data = data\n    learn.load(\"Learn_Tabular\",strict=False,remove_module=True)\n    \n\n    #learn.fit_one_cycle(1, 5e-2, wd=0.2)\n    learn.save(\"Learn_Tabular\")\n    #print(COUNT)\n    #COUNT=COUNT+1\n\n\nreader  = pd.read_csv(\"\/content\/drive\/My Drive\/10FA\/price_paid_records.csv\", chunksize=65536) # chunksize depends with you RAM\n[preprocess(r) for r in reader]\n'''","79d3de79":"perc_na = (Paid_Price_Tweet.isnull().sum()\/len(Paid_Price_Tweet))*100\nratio_na = perc_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Percentage' :ratio_na})\nmissing_data","af8d9ab1":"\n# Apply per-column median of that columns and fill with that value for Numerical Variables\n\nnumeric_cols=['Date of TransferYear',\t'Date of TransferMonth',\t\n              'Date of TransferWeek',\t'Date of TransferDay',\t\n              'Date of TransferDayofweek',\t'Date of TransferDayofyear',\n              'lat',\t'lng','population',\t'Tweets_Count_1',\t'Tweets_Count_05',\n              'Tweets_Count_025',\t'Tweets_Count_001'\n              ]\n\nPaid_Price_Tweet[numeric_cols] = Paid_Price_Tweet[numeric_cols].apply(lambda x: x.fillna(x.median()),axis=0)\n","ede03456":"# Apply per-column Highest Frequency value to address Missing variables\n# Convert Non numerical columns to string\n\nfor c_cols in  ['Property Type', 'Old\/New', 'Duration', 'Town\/City',\n            'District', 'County', 'PPDCategory Type',\n            'Record Status - monthly file only',\n            'Date of TransferIs_month_end', 'Date of TransferIs_month_start',\n            'Date of TransferIs_quarter_end', 'Date of TransferIs_quarter_start',\n            'Date of TransferIs_year_end', 'Date of TransferIs_year_start']:\n\n    \n    #train[c_cols] = train[c_cols].cat.add_categories('Unknown')\n    Paid_Price_Tweet=Paid_Price_Tweet.fillna(Paid_Price_Tweet[c_cols].value_counts().index[0])\n    \n    Paid_Price_Tweet[c_cols]=Paid_Price_Tweet[c_cols].astype(str)","47bf5d7d":"\nPaid_Price_Tweet= Paid_Price_Tweet.drop([\n'population',\n], axis=1)","d1f3b042":"for c_cols in  ['Property Type', 'Old\/New', 'Duration', 'Town\/City',\n            'District', 'County', 'PPDCategory Type',\n            'Record Status - monthly file only',\n            'Date of TransferIs_month_end', 'Date of TransferIs_month_start',\n            'Date of TransferIs_quarter_end', 'Date of TransferIs_quarter_start',\n            'Date of TransferIs_year_end', 'Date of TransferIs_year_start']:\n    print(c_cols, Paid_Price_Tweet[c_cols].nunique() )","dd4b22f0":"\nPaid_Price_Tweet= Paid_Price_Tweet.drop([\n'Town\/City',\n 'District',\n 'County'\n\n], axis=1)","49cb3b2d":"Paid_Price_Tweet['Property Type']=Paid_Price_Tweet['Property Type'].astype('category')\nPaid_Price_Tweet['Old\/New']=Paid_Price_Tweet['Old\/New'].astype('category')\nPaid_Price_Tweet['Duration']=Paid_Price_Tweet['Duration'].astype('category')\n#Paid_Price_Tweet['Town\/City']=Paid_Price_Tweet['Town\/City'].astype('category')\n#Paid_Price_Tweet['District ']=Paid_Price_Tweet['District '].astype('category')\n#Paid_Price_Tweet['County']=Paid_Price_Tweet['County'].astype('category')\nPaid_Price_Tweet['PPDCategory Type']=Paid_Price_Tweet['PPDCategory Type'].astype('category')\nPaid_Price_Tweet['Record Status - monthly file only']=Paid_Price_Tweet['Record Status - monthly file only'].astype('category')\nPaid_Price_Tweet['Date of TransferIs_month_end']=Paid_Price_Tweet['Date of TransferIs_month_end'].astype('category')\nPaid_Price_Tweet['Date of TransferIs_month_start']=Paid_Price_Tweet['Date of TransferIs_month_start'].astype('category')\nPaid_Price_Tweet['Date of TransferIs_quarter_end']=Paid_Price_Tweet['Date of TransferIs_quarter_end'].astype('category')\nPaid_Price_Tweet['Date of TransferIs_quarter_start']=Paid_Price_Tweet['Date of TransferIs_quarter_start'].astype('category')\nPaid_Price_Tweet['Date of TransferIs_year_end']=Paid_Price_Tweet['Date of TransferIs_year_end'].astype('category')\nPaid_Price_Tweet['Date of TransferIs_year_start']=Paid_Price_Tweet['Date of TransferIs_year_start'].astype('category')\n","3a93eee3":"print(Paid_Price_Tweet.shape)\ndf_train=pd.get_dummies(Paid_Price_Tweet)\nprint(Paid_Price_Tweet.shape)","53282a9e":"print(df_train.shape)\n\ntarget1=df_train['Price']\n\ndf_train= df_train.drop(['Price'], axis=1)\ndf_train.shape","dbe1d1b5":"from sklearn.model_selection import train_test_split\n\n# in the Random Forest method, involves training each decision tree on a different data sample\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.metrics import r2_score, mean_squared_error # import metrics from sklearn","324243e2":"X_train, X_test, y_train, y_test =train_test_split(df_train, target1)","41756b92":"df_train.head(2)","609d45a0":"#!pip  install scikit-learn==0.19.1","dae773ee":"\nfrom sklearn.metrics import make_scorer, mean_squared_error\n#  k-fold CV, the training set is split into k smaller sets \n#from sklearn.cross_validation import cross_val_score\n\nscorer = make_scorer(mean_squared_error, False)\n\nclf = RandomForestRegressor(n_estimators=20, n_jobs=-1)\n","cc6e0dd6":"# For the lack of time I have only used 20 Trees\nclf","c782e206":"\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)","67a17b9e":"\nscore_r2 = r2_score(y_test, y_pred)\nscore_mse = mean_squared_error(y_test, y_pred)\n\n\nd = {\n     'RF_Regressor': [score_r2 , score_mse] \n  \n    }\nd_i = ['R2', 'Mean Squared Error']\ndf_results = pd.DataFrame(data=d, index = d_i)\ndf_results\n","66334628":"\nimportances = clf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in clf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\n#for f in range(X_train.shape[1]):\n    #print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n\n# Plot the impurity-based feature importances of the forest\nplt.figure(figsize=(20,10))\nplt.title(\"Feature importances\")\n\nplt.bar(range(0,9), importances[indices][0:9],\n        color=\"r\", yerr=std[indices][0:9], align=\"center\")\n\n#plt.bar(range(X_train.shape[1]), importances[indices],\n#        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(0,9), indices[0:9])\n#plt.xlim([-1, X_train.shape[1]])\nplt.show()\n","8ab06c74":"## Model Data on using FAST.AI","3d1a77ff":"### Steps to improve model Performance\n \n Model\n \n - Deeper feature Engineering ( Create more Categories)\n - Manage better the variables with high cardinality\n - Try out Models\n - Fine Tune the model furthermore by using \/ create  a Validation set\n-  Check accuracy on Antilog of sale price ( I created Log of output)\n-  Investigate more deeper into feature importance\n-  Perform more iterations on smaller datasets\n-  Investigate into cases which have high error rate\n\n\nData:\n\n- Obtain More Data from paid and Opersource Platforms which Can improve model Performance\n- Obtain  Data- Such as population, Poulation Density , House Density, Inflation, Bank Loan rate, London Stock Exchange Index","cb0d54b8":"## Data Set 3 : EDA for Price Data for UK","0a147e65":"### Counting Number of Tweets per city","fa17d402":"## Model Data on using Random Forest Model","124c0699":"Let us find Cardinality of all Categorical Variables","510d4ae7":"### Geographic Density of Tweets","bb7ddc84":"### For more information on FAST.AIB please feel free to refer my blogs\nhttps:\/\/medium.com\/@puntambekar.anand\/language-model-using-transfer-learning-for-cricketers-3b022a5717f6\n\nhttps:\/\/medium.com\/analytics-vidhya\/end-to-end-image-classification-web-app-blog-kaggle-kernel-69703e96dbdb?source=---------5------------------","bd1d7729":"# R2 Score: 0.679840","0cb7be87":"### Processor used : NVidia K80 GPU\n### Python Environment: ","03861ab2":"## Eliminate Outliers from Data","7617845f":"# UK House Price EDA and Modelling\n\n# I have only used 10% of Pricing Data available due to high processing time\n\n## R2Score: 0.679840, with only 1 Epoch using NN\n\n\n## Steps performed\n\n- Perform EDA on 3 Data sets Found\n  - Tweet from UK\n  - Geographic Information, Population\n  - Price Data for UK House Prices\n  \n\n- Perform Mergining of Relavant Columns\n\n- Using a Neural Network Perform Prgression\n  - Handling empty variables\n  - Perform Droupout, batch Norm, Learned Embedding\n  - Obtain Results on validation Data Set\n  - Handling large Data Set - Demostrated (Not performed Due to lack of time)\n   \n","9a9edb66":"### Time Series Visualization of the Number of Tweets","9497d6c6":"## Merge Twitter Data Count with Price Data","516d61bf":"### There exist only one Date of tweets available","f0fa6c00":"Remove outliers\nI then reduced the data by removing extreme house prices. Those are the ones that are:\n\nless than \u00a310 Million and,\ngreater than \u00a310,000","df2d5b94":"## Mean Price Appears to Drop at the End of Month","e45cbcba":"## Comments:\n\n- Data was found on Opensource platforms and Violates no laws on application\n- Large Data Set was handled with Chunck Size\n- Interactive Geographic Charts have been Provided for teh user to See\n","3174bee6":"House Price Distribution\nLet us take a quick exploratory look at the distribution of house prices. \n\nI first created a histogram using plot.ly but due to the sheer size this caused performance issues so instead resorted to using a simple matplolib plot.\n\n","fa6f6acc":"The Following Columns have more than 70% values missig and hence I eliminate the same","f44c7732":"Train the Model","6decec14":"Convert Data type of Categorical variables","9171d908":"## Duplicates\nFirst we check Paid_Price to see if there are any duplicate rows in the data, in this data we indentify none.\n\nHowever, searching for duplicates on all columns except ID shows that approximately 0.05% are in fact duplicates but we can ignore this ","557433a6":"Find Important Variables","16603f9c":"Addressing Missing Values","42f08448":"## Code for managing Large Data Sets","993905bf":"### We Set the Learning Rate to 5e-2\n### We Set the Weight Decay to 0.2","5cc800d7":"## The Number of Tweets appear to have a visual Corelation to Poulation of City","8e7ad799":"## Variation of Price over time","e11cc202":"Lets Drop the high carinal variables which may not add value to Model","18837a25":"## Data Set 1 : EDA for Twitter Data","dbdf2238":"## Data Set 2 : EDA for City , Geography and Populations","7d745251":"## Price Appears to drop at the begining and end of Year","65204e2b":"## Number of Tweets within 0.5 Degree Lat\/Log of City Center","7cd35933":"### Number of Tweets by Country\n#### All Terms Mean the same (In different Languages)","57dd2a77":"## Variation of Price with County","c4b423b8":"## The Price Appears of End of week","4d887d5a":"### Keeping Tweets orignated only in the UK","e3307948":"Encode Categorical Features","29dd7e3c":"### Number of Tweets by Device Type\n","a01bf01f":"## At Quarer End Price Appears to be higher","af3be91a":"### Import Relavant Libraries"}}