{"cell_type":{"6c7282b1":"code","a913436c":"code","09a1ec78":"code","99288b9a":"code","ed407d87":"code","35ccc2b5":"code","80a5bbc6":"code","2e56704c":"code","aa479e8f":"code","27304591":"code","53fab42e":"code","1ae5559c":"code","b343c275":"code","062aa0e7":"code","df481a75":"code","2fbaaefd":"code","ea99c918":"code","048589ac":"code","28843680":"code","236bbc20":"code","64b8793e":"code","6aa645b9":"code","df43e423":"code","9973e9fc":"code","31e4ea84":"code","10ce59ed":"code","14a3d059":"code","a3050e80":"code","87f9e8ae":"code","6f041f80":"markdown","6c772eec":"markdown","882e3b1c":"markdown","d055574b":"markdown","a14ded15":"markdown","5cead21c":"markdown","76b958c4":"markdown","21ce7bbe":"markdown","f157fff4":"markdown","ed833b06":"markdown","1fd68aa5":"markdown","2620aa18":"markdown","ddece89c":"markdown","52ac870a":"markdown","65b165cc":"markdown","0c30eb93":"markdown","0e9ff9c2":"markdown","b437d6b8":"markdown","33567640":"markdown","f5cddab9":"markdown","70de6479":"markdown","4c851598":"markdown","dd5813f4":"markdown"},"source":{"6c7282b1":"\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew\nimport warnings\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","a913436c":"dtrain=pd.read_csv('..\/input\/train.csv')\ndtest=pd.read_csv('..\/input\/test.csv')\nprint(dtrain.shape)\n\nprint(dtest.shape)","09a1ec78":"dtrain.head()","99288b9a":"dtest.head()","ed407d87":"test_ID=dtest['Id']\ndtrain=dtrain.drop(['Id'],axis=1)\ndtest=dtest.drop(['Id'],axis=1)","35ccc2b5":"y_train=dtrain['SalePrice']\ndata=pd.concat([dtrain,dtest],ignore_index=True)\ndata=data.drop(['SalePrice'],axis=1)\ndata.shape","80a5bbc6":"plt.figure(figsize=(7,6))\nsns.distplot(y_train)\nplt.show()","2e56704c":"y_train=np.log1p(y_train)\nsns.distplot(y_train)\nplt.show()","aa479e8f":"total = data.isnull().sum().sort_values(ascending=False)\npercent = ((data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False))*100\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\n","27304591":"data['PoolQC']=data['PoolQC'].fillna(\"None\")\ndata['MiscFeature']=data['MiscFeature'].fillna(\"None\")\ndata['Alley']=data['Alley'].fillna(\"None\")\ndata['Fence']=data['Fence'].fillna(\"None\")\ndata['FireplaceQu']=data['FireplaceQu'].fillna(\"None\")\ndata[\"LotFrontage\"] = data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","53fab42e":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    data[col] = data[col].fillna('None')","1ae5559c":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    data[col] = data[col].fillna(0)","b343c275":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    data[col] = data[col].fillna('None')","062aa0e7":"data[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)","df481a75":"data=data.fillna(data.mean())\ndata.shape","2fbaaefd":"corr=dtrain.corr()['SalePrice'].sort_values()[::-1]\nc=corr.head(15).index\nplt.figure(figsize=(12,8))\nsns.heatmap(dtrain[c].corr(),annot=True)","ea99c918":"data=data.drop(['GarageCars','1stFlrSF','2ndFlrSF','TotRmsAbvGrd','GarageYrBlt'],axis=1)","048589ac":"num_f=data.dtypes[data.dtypes!=object].index\nskew_f=data[num_f].apply(lambda x: skew(x.dropna()))\nskew_f=skew_f[skew_f>0.75] #include only those features that have skewness greater than 75%\nskew_f=skew_f.index\n#Apply log transformation\ndata[skew_f]=np.log1p(data[skew_f])","28843680":"data = pd.get_dummies(data)# for handling categorical data\ndata.shape","236bbc20":"x_train=data[:dtrain.shape[0]]\ntest=data[dtrain.shape[0]:]","64b8793e":"#from sklearn.linear_model import Lasso,Ridge,RidgeCV,LassoCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb","6aa645b9":"dtrain=xgb.DMatrix(x_train,label=y_train)\ndtest=xgb.DMatrix(test)","df43e423":"def_params={'min_child_weight': 1,\n        'max_depth': 6,\n        'subsample': 1.0,\n        'colsample_bytree': 1.0,\n        'reg_lambda': 1,\n        'reg_alpha': 0,\n        'learning-rate':0.3,\n        'silent':1 # not a hyperparameter ,used to silence XGBoost\n        }\ncv_res=xgb.cv(def_params,dtrain,nfold=5)\ncv_res.tail()","9973e9fc":"from skopt import BayesSearchCV \nimport warnings\nwarnings.filterwarnings('ignore', message='The objective has been evaluated at this point before.')\n\nparams={'min_child_weight': (0, 50,),\n        'max_depth': (0, 10),\n        'subsample': (0.5, 1.0),\n        'colsample_bytree': (0.5, 1.0),\n        'reg_lambda':(1e-5,100,'log-uniform'),\n        'reg_alpha':(1e-5,100,'log-uniform'),\n        'learning-rate':(0.01,0.2,'log-uniform')\n        }\n\nbayes=BayesSearchCV(xgb.XGBRegressor(),params,n_iter=10,scoring='neg_mean_squared_error',cv=5,random_state=42)\nres=bayes.fit(x_train,y_train)\nprint(res.best_params_)","31e4ea84":"final_params={'colsample_bytree': 0.50, 'max_depth': 7, 'min_child_weight':13, 'reg_alpha': 0.112, 'reg_lambda': 0.0008, 'subsample': 0.65,'eta':0.11,'silent':1}\ncv_res=xgb.cv(final_params,dtrain,num_boost_round=1000,early_stopping_rounds=100,nfold=5)","10ce59ed":"cv_res.loc[30:,['train-rmse-mean','test-rmse-mean']].plot()","14a3d059":"cv_res.tail()","a3050e80":"model_xgb=xgb.train(final_params,dtrain=dtrain,num_boost_round=151)\nfinal_pred=np.expm1(model_xgb.predict(dtest))","87f9e8ae":"s=pd.DataFrame()\ns['Id']=test_ID\ns['SalePrice']=final_pred\ns.to_csv('my_submission.csv',index=False)","6f041f80":"Filling rest of the columns with their mean.","6c772eec":"# **HYPER-PARAMETER TUNING USING BAYESIAN OPTIMIZATION**\n\n#### In the world of machine learning , hyper-parameter tuning plays a very important role in developing better models. Hyper-parameters are parameters specified by \u201chand\u201d to the algorithm and fixed throughout a training pass. Further, the algorihtm typically does not include any logic to optimize them for us. \n\n#### Manually tuning these parameters is time taking ,costly and inefficient(obvioulsy!).It is out of the question to use this technique.There are methods which can help in optimization,they are: \n \n#### *   Grid Search\n#### *  Random Search\n#### *  Bayesian Optimization \n\n#### In this notebook I am going to show how to do Bayesian Optimization for  XGBoost using Ames housing dataset.But first lets load our data and do necessary feature engineering","882e3b1c":"## Bayesian Optimization\n Instead of going through each and every parameter like grid search or only  random parameters in randomized search.We could use a technique called informed search .It means to build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function or in simpler terms it learns from previous iterations and does not use the bad parameters again and again.It only updates itself when better parameters are found.Bayesian Optimization follows this principle.However there are other methods like hyperopt etc. which you can learn later.\n \n \n A clear and easy understanding is given by [Will Koehrsen](http:\/\/towardsdatascience.com\/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f).\n Lets implement this approach using scikit-optimization library [skopt](https:\/\/scikit-optimize.github.io\/) which provides us with BayesSearchCV","d055574b":"We have to remember that we applied log transformation to out target ,so to convert it back  we use exponetial function expm1.","a14ded15":"Let us have a look at how many features have maximum missing values.","5cead21c":"#### I have directly gone to xgboost because I want to sepcifically show optimization in xgboost,you can refer other kernels which shows how various models perform.","76b958c4":"Lets optimize (tune) our hyper-parameters and perform early-stopping to see how we can improve.","21ce7bbe":"## Finalize the  Model and make predictions","f157fff4":"Now we can see that data is distributed normally.","ed833b06":"## 'SalePrice' analysis","1fd68aa5":"## Skewness","2620aa18":"#### If you find this kernel helpful some upvotes would be very much appreciated \ud83d\ude0a.I would very much like any suggestions for improving my model.","ddece89c":"## Missing Data","52ac870a":" Combine the train and test data and remove the target and store it.","65b165cc":"Here we find any mutlicollinear features and remove them(if any).","0c30eb93":"There is a lot of skewness as we can see above.So lets convert it into a normal\/Gaussian distribution.This can be done using log(1+x) transformation. We may have to transform the numerical features also but lets do that later.","0e9ff9c2":"## Submission","b437d6b8":"Separate data into train and test.We combined them in the first place to do feature engineering on the whole data.","33567640":"## Modelling","f5cddab9":"I have filled missing values using the way [Serigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) did (most of them ,till MasVnrArea only).","70de6479":"## Correlation ","4c851598":"By looking at the heatmap we can infer the following,\n* 'GarageCars' and 'GarageArea' are highly correlated with each other.So we can drop one of them.\n* 'TotalBsmtSF' is highly correlated with '1stFlrSF' and '2ndFlrSF'(extend the heatmap and u will find it),I will keep the totalSF and drop 1st and 2nd floorSF.\n* 'TotRmsAbvGrd' and 'GrLivArea' are highly correlated.Lets keep GrLivArea as it is related to SalePrice.\n* GarageYrBlt and YrBuilt are highly correlated to each other.Lets drop GarageYrBuilt.","dd5813f4":"Lets visualize this."}}