{"cell_type":{"a531d2bd":"code","00df97af":"code","be88fad4":"code","833df7a5":"code","f86d643f":"code","f0a2e76d":"code","6fdc0266":"code","e2599289":"code","0709ebfc":"code","738a501e":"code","089013d6":"code","f1bf9055":"code","9664b248":"markdown","f3946ef9":"markdown","89325c91":"markdown","d745923e":"markdown","b038e492":"markdown"},"source":{"a531d2bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00df97af":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import *\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport warnings\nwarnings.filterwarnings('ignore')","be88fad4":"data = pd.read_csv(\"\/kaggle\/input\/daily-air-quality-dataset-india\/air_quality_index.csv\")","833df7a5":"### Changing datatypes for date\n\ndata[\"date\"] = pd.to_datetime(data[\"DATE\"],dayfirst=True)\n\n### creatng unique dataframe for country\n\nusa_data = data[data[\"COUNTRY\"] == \"US\"]\nindia_data = data[data[\"COUNTRY\"] == \"IN\"]\n\n### creating copy of datasets\n\nindia_copy = india_data.copy()\nusa_copy = usa_data.copy()\n\n### checking unique cities of india\n\nunique_city_india = list(india_copy[\"CITY\"].unique())","f86d643f":"### checking top 10 maximum pollutant cities of India\n\ndef max_pollutant_city(pollutant):\n    polu = india_copy[[pollutant,\"CITY\"]].groupby([\"CITY\"]).mean().sort_values(by = pollutant,ascending = False).reset_index()\n    polu[pollutant] = round(polu[pollutant],2)  \n    return polu[:10]\n\ncity_polluted = max_pollutant_city(\"VALUE\")      \n\n## plotting bar plot for max polluted cities\nplt.figure(figsize = (20,10))\nsns.barplot(x = city_polluted[\"CITY\"], y= city_polluted[\"VALUE\"], data = city_polluted, palette=\"rocket\") \nplt.show()                            ","f0a2e76d":"## taking top 6 most polluted cities of India\n\ncities = [\"Gh\u0101zi\u0101b\u0101d\",\"Delhi\",\"New Delhi\",\"Lucknow\",\"Muzaffarnagar\", \"H\u0101pur\"]\nsns.set_style('whitegrid')\nplt.figure(figsize=(16,18))\ni=0\nfor city in cities:\n    p = india_data[india_data[\"CITY\"]==city]\n    i +=1\n    plt.figure(figsize=(30,10))\n    plt.subplot(3,2,i)\n    sns.lineplot(x = p[\"date\"], y = p[\"VALUE\"], data = p)\n    plt.xlabel(city)\n    plt.show()\n\nplt.show()","6fdc0266":"### creating lineplot for each cities of India and USA yearly basis\n\nindia_copy[\"year\"] = india_copy[\"date\"].dt.year\nunique_year = list(india_copy[\"year\"].unique())\nunique_year.sort()\nsns.set_style(\"whitegrid\")\n\ni=0\nfor city in cities:\n    p = india_copy[india_copy[\"CITY\"]==city]\n    i +=1\n    plt.figure(figsize = (30,10))\n    plt.subplot(3,2,i)\n    for year in unique_year:\n        p_year = p[p[\"year\"]== year]\n        xx = p_year.drop(columns = [\"DATE\", \"date\",\"COUNTRY\"])\n        xx.reset_index()\n        sns.lineplot(x = range(len(xx)), y = xx[\"VALUE\"], data = xx, label = str(p_year[\"year\"].iloc[0]))\n        plt.xlabel(city)\n        plt.legend(loc = 'best')\n    plt.show()\nplt.show()","e2599289":"### creating distplot for each cities of India and USA yearly basis\n\nindia_copy[\"year\"] = india_copy[\"date\"].dt.year\nunique_year = list(india_copy[\"year\"].unique())\nunique_year.sort()\nsns.set_style(\"whitegrid\")\n\n\ni=0\nfor city in cities:\n    p = india_copy[india_copy[\"CITY\"]==city]\n    i +=1\n    plt.figure(figsize = (30,10))\n    plt.subplot(3,2,i)\n    j = 0\n    for year in unique_year:\n        p_year = p[p[\"year\"]== year]\n        sns.distplot(p_year[\"VALUE\"], kde = True, hist = False, label = unique_year[j])\n        j +=1\n        plt.xlabel(city)\n        plt.legend()\n    plt.show()\nplt.show()","0709ebfc":"## creating equal spaced array\nquarter_wise = np.arange(0,13,3)\n\nsns.set_style('whitegrid')\n\nfor year in unique_year:\n    pp = india_copy[india_copy[\"year\"] == year]\n    plt.figure()\n    i = 0\n    for city in cities:\n        p = pp[pp[\"CITY\"]==city]\n        p[\"month\"] = p[\"date\"].dt.month\n    ## segregating data in quaterly basis\n        first_quarter = p[(p[\"month\"] >= quarter_wise[0]+1) & (p[\"month\"] <= quarter_wise[1])]\n        second_quarter = p[(p[\"month\"] > quarter_wise[1]) & (p[\"month\"] <= quarter_wise[2])]\n        third_quarter = p[(p[\"month\"] > quarter_wise[2]) & (p[\"month\"] <= quarter_wise[3])]\n        fourth_quarter = p[(p[\"month\"] > quarter_wise[3]) & (p[\"month\"] <= quarter_wise[4])]\n        i+=1\n        plt.figure(figsize=(30,10))\n        plt.subplot(3,2,i)\n        sns.distplot(first_quarter[\"VALUE\"], kde = True, hist = False, label = \"first_quarter\")\n        sns.distplot(second_quarter[\"VALUE\"], kde = True, hist = False, label = \"second_quarter\")\n        sns.distplot(third_quarter[\"VALUE\"], kde = True, hist = False, label = \"third_quarter\")\n        sns.distplot(fourth_quarter[\"VALUE\"],kde = True, hist = False,  label = \"fourth_quarter\")\n        plt.xlabel(city)\n        plt.legend()\n    plt.suptitle(\"plot of \" + str(pp[\"year\"].values[0]) +\" \" + \" dataset\")\n    plt.show()\n    plt.show()\nplt.show()\nplt.show()","738a501e":"### Making Predictions on the most polluted state of India - Gh\u0101zi\u0101b\u0101d\n\nGh\u0101zi\u0101b\u0101d_city = india_copy[india_copy[\"CITY\"] == \"Gh\u0101zi\u0101b\u0101d\"]\n\ndf = Gh\u0101zi\u0101b\u0101d_city.reset_index()[\"VALUE\"]\n\nGh\u0101zi\u0101b\u0101d_city.drop(columns = [\"DATE\",\"COUNTRY\",\"CITY\",\"year\"], axis = 1, inplace = True)\n\ntraining_set = Gh\u0101zi\u0101b\u0101d_city.iloc[:700, 0:1].values\ntest_set = Gh\u0101zi\u0101b\u0101d_city.iloc[700:, 0:1].values\n\n# Feature Scaling\nsc = MinMaxScaler(feature_range = (0, 1))\ntraining_set_scaled = sc.fit_transform(training_set)\n\n# Creating a data structure with 60 time-steps and 1 output\nX_train = []\ny_train = []\nfor i in range(60, 700):\n    X_train.append(training_set_scaled[i-60:i, 0])\n    y_train.append(training_set_scaled[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n\nprint(X_train.shape)\n\nmodel = Sequential()\n#Adding the first LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 256, return_sequences = True, input_shape = (X_train.shape[1], 1)))\nmodel.add(Dropout(0.2))\n# Adding a second LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 256, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a third LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 256, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a fourth LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 100))\nmodel.add(Dropout(0.2))\n# Adding the output layer\nmodel.add(Dense(units = 1))\n\n# Compiling the RNN\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nresult = model.fit(X_train, y_train, epochs = 100, batch_size = 16)\n\n","089013d6":"### plotting the loss\nplt.figure(figsize = (30,15))\nloss = result.history['loss']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, label='Training loss')\nplt.legend()\nplt.show()\n\n\n# Getting the predicted poluution level \ndataset_train = Gh\u0101zi\u0101b\u0101d_city.iloc[:700, 0:1]\ndataset_test = Gh\u0101zi\u0101b\u0101d_city.iloc[700:, 0:1]\ndataset_total = pd.concat((dataset_train, dataset_test), axis = 0)\ninputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\ninputs = inputs.reshape(-1,1)\ninputs = sc.transform(inputs)\nX_test = []\nfor i in range(60, 249):\n    X_test.append(inputs[i-60:i, 0])\nX_test = np.array(X_test)\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\nprint(X_test.shape)\n\n# prediction \npredicted_pollution = model.predict(X_test)\npredicted_pollution = sc.inverse_transform(predicted_pollution)\n\n## making a copy of the dataset\ndataset_test_copy = dataset_test.copy()\ntest_data = pd.merge(dataset_test_copy,Gh\u0101zi\u0101b\u0101d_city, left_index = True, right_index = True)\n\ntest_data.drop(columns = [\"VALUE_x\"], inplace = True)\n\n## resting index value for the dataset\ndataset_test_copy.reset_index(drop = True, inplace = True)\n\ntest_data.reset_index(drop = True, inplace = True)\n\n## plotting actual vs prediction\nplt.figure(figsize = (30,10))\nplt.plot(test_data.loc[:, 'date'], dataset_test_copy, color = 'black', label = 'Actual Pollution')\nplt.plot(test_data.loc[:, 'date'], predicted_pollution, color = 'green', label = 'Predicted Pollution')\nplt.title('City Pollution Prediction')\nplt.xlabel('Time')\nplt.ylabel('Pollution Level Value')\nplt.legend()\nplt.show()","f1bf9055":"n_steps = 7\nn_features = 1\nx_input = np.array([89,57, 67,85, 162,122,94])\ntemp_input=list(x_input)\nlst_output=[]\ni=0\nwhile(i<10):\n    \n    if(len(temp_input)>7):\n        x_input=np.array(temp_input[1:])\n        print(\"{} day input {}\".format(i,x_input))\n        #print(x_input)\n        x_input = x_input.reshape((1, n_steps, n_features))\n        #print(x_input)\n        yhat = model.predict(x_input, verbose=0)\n        yhat = sc.inverse_transform(yhat)\n        print(\"{} day output {}\".format(i,yhat))\n        temp_input.append(yhat[0][0])\n        temp_input=temp_input[1:]\n        #print(temp_input)\n        lst_output.append(yhat[0][0])\n        i=i+1\n    else:\n        x_input = x_input.reshape((1, n_steps, n_features))\n        yhat = model.predict(x_input, verbose=0)\n        yhat = sc.inverse_transform(yhat)\n        print(yhat[0])\n        temp_input.append(yhat[0][0])\n        lst_output.append(yhat[0][0])\n        i=i+1\n    \n\nprint(lst_output)","9664b248":"By taking the mean of Pollutant value, city Gh\u0101zi\u0101b\u0101d seems to be most polluted city of India.\nLet's do some Visualization plots of top 6 cities of India.","f3946ef9":"From this line plot, we can see a sharp spike just before april of 2019, a sharp rise the pollutant value in these cities. \n\nParticlarly the largest spike is observed in New Delhi and Delhi. \n\nA similar pattern can be observed in all those cities. A gradual increase in poluution from October to January. ","89325c91":"From the quarter plot analysis, we can see that third quarter which comprises of April - June has the lowest pollution in all the cities.\n\nand in the fourth quarter which comprises of October to December, has the highest pollution in all the cities. \n\nSo pollution is increasing in the winter month, we can fairly say that. ","d745923e":"From the above distribution plots, we can easily say that in 2020, the distribution plot is speaking about the lockdown effect in those cities as well, where the graph is differerent than the other years. \n\nNow we will check distribution plot for quarterly basis. The aim is to find out in which quarter the distribution plot is very much differnet than the other months.  \n","b038e492":"From these yearly plots we can defer that, the pollution pattern is almost similar in each year in those cities. \n\nIn 2019, we saw the biggest spike in all those cities. \n\nThe data is incomplete for 2021, and for 2020, the pollution is also less than 2019 level which can be inferred from the graphs.  \n\nLet's check the distibution plot to check the distribution of pollution in these cities. "}}