{"cell_type":{"2952d8c3":"code","09b8278f":"code","c7d88e16":"code","586b00bd":"code","1588bcaa":"code","7ccc58e4":"code","5050efe3":"code","7028ab74":"code","143380ee":"code","429a24c4":"code","7a92a9dd":"code","b6475d1b":"code","4063e133":"code","b1d57531":"code","c2315480":"code","87557a67":"code","a30a7c09":"code","af3626bc":"code","7cecdcfd":"code","1e9f4ac1":"code","a64fd028":"code","4a8cd9e6":"code","6e2804a9":"code","6a8eefe6":"code","7dd97b9d":"code","0fa8cfb7":"code","42090774":"markdown","6ea8b8df":"markdown","25550aa5":"markdown","ce040828":"markdown","541f195d":"markdown","5c632d76":"markdown","49a9ba87":"markdown","44bf7a18":"markdown","f826ed27":"markdown","d7f08196":"markdown","31bd9c1f":"markdown","3eb68e11":"markdown","b2e0be3d":"markdown","2e1165e0":"markdown","ba6ffe33":"markdown","9f087d93":"markdown","fd6b17c6":"markdown","6adab074":"markdown","cc4521af":"markdown","7e7c2a3f":"markdown","1087941b":"markdown","e4a44cd9":"markdown","892e8ca0":"markdown","0d29668e":"markdown","a5d7a2cd":"markdown","c7fd2487":"markdown"},"source":{"2952d8c3":"from IPython.display import Image, display_png\n\n!wget -q \"https:\/\/drive.google.com\/uc?export=download&id=1P2RK7Dbn_ETCW4_1uoQKJ4KgUlKZ9f39\" -O \"slide.png\"\ndisplay_png(Image(\"slide.png\", width = 1200, height = 800))","09b8278f":"import sys\nsys.path.append(\"\u30d1\u30b9\") # ()\u306e\u4e2d\u306binput\u306b\u5b58\u5728\u3059\u308b\u3001timm\u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u6b63\u3057\u3044\u30d1\u30b9\u3092\u4e0e\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n\nimport gc\nimport math\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nimport time\nimport warnings\nfrom collections import Counter, defaultdict\nfrom contextlib import contextmanager\n\nimport albumentations as A\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport scipy.stats as stats\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom albumentations import (\n    Blur, Compose, Cutout, HorizontalFlip, IAAAdditiveGaussianNoise,\n    ImageOnlyTransform, Normalize, OneOf, RandomBrightness, RandomBrightnessContrast,\n    RandomContrast, RandomCrop, RandomResizedCrop, Resize, Rotate, ShiftScaleRotate, Transpose, VerticalFlip,\n    HueSaturationValue, CoarseDropout, CenterCrop\n)\nfrom albumentations.pytorch import ToTensorV2\nfrom functools import partial\nfrom pathlib import Path\nfrom PIL import Image\nfrom scipy.special import softmax\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.nn.parameter import Parameter\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_sequence\nfrom torch.optim import SGD, Adam, AdamW\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, CosineAnnealingWarmRestarts, ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm.auto import tqdm\n\nimport timm\nwarnings.filterwarnings('ignore')","c7d88e16":"class CFG:\n    seed = 0 # seed\u5024\n    num_workers = 0 #\u4e26\u5217\u5b9f\u884c\u3059\u308b\u6570\n    batch_size = 64 #\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\n    epochs = 1 #\u30a8\u30dd\u30c3\u30af\u6570\n    size = 224 # \u30ea\u30b5\u30a4\u30ba\u3057\u305f\u5f8c\u306e\u753b\u50cf\u306e\u30b5\u30a4\u30ba\n    model_lr = 1e-5 # \u5b66\u7fd2\u7387\n    T_max = 10 # \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6570\n    min_lr = 1e-6 # \u5b66\u7fd2\u7387\u306e\u6700\u5c0f\u5024\n    weight_decay = 1e-6 # \u5b66\u7fd2\u6e1b\u8870\u5024\n    max_grad_norm = 1000 # \u52fe\u914d\u306e\u6700\u5927\u30ce\u30eb\u30e0\n    print_freq = 1000 # \u5b66\u7fd2\u7d50\u679c\u3092\u8868\u793a\u3059\u308b\u983b\u5ea6\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # CPU of GPU\n    MODEL_NAME = \"resnet34\" # \u30e2\u30c7\u30eb\u540d\n    Version = \"V1\" # model save\u6642\u306eversion","586b00bd":"def init_logger(log_file='train.log'):\n    \"\"\"Output Log.\"\"\"\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\ndef seed_torch(seed=0):\n    \"\"\"Fixed seed value.\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\nLOGGER = init_logger()\nseed_torch(seed=CFG.seed)","1588bcaa":"def asMinutes(s):\n    \"\"\"Convert Seconds to Minutes.\"\"\"\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    \"\"\"Accessing and Converting Time Data.\"\"\"\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value.\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","7ccc58e4":"class TrainDataset(Dataset):\n    \"\"\"Dataset used for training.\"\"\"\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.labels = df['label'].values\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = self.df.loc[idx, \"pixel0\": \"pixel783\"].values.astype(np.uint8).reshape(28, 28) # csv to image\n        image = # \u3053\u3053\u3067cv2.cvtColor\u3092\u7528\u3044\u3066\u3001GRAY to RGB\u3092\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002(http:\/\/opencv.jp\/opencv-2svn\/c\/miscellaneous_image_transformations.html#cvtcolor)\n\n        if self.transform:\n            augmented = self.transform(image=image) # augmentation\n            image = #\u3053\u3053\u3067\u3001augmented\u304b\u3089\u30c7\u30fc\u30bf\u62e1\u5f35\u5f8c\u306e\u753b\u50cf\u3092\u53d6\u5f97\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (https:\/\/albumentations.ai\/docs\/getting_started\/image_augmentation\/)\n\n        label = self.labels[idx] # \u6b63\u89e3\u30e9\u30d9\u30eb\n        return image, label","5050efe3":"class TestDataset(Dataset):\n    \"\"\"Dataset used for inference.\"\"\"\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        image = self.df.loc[idx, \"pixel0\": \"pixel783\"].values.astype(np.uint8).reshape(28, 28)\n        image = # \u3053\u3053\u3067cv2.cvtColor\u3092\u7528\u3044\u3066\u3001GRAY to RGB\u3092\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002 (http:\/\/opencv.jp\/opencv-2svn\/c\/miscellaneous_image_transformations.html#cvtcolor)\n\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = #\u3053\u3053\u3067\u3001augmented\u304b\u3089\u30c7\u30fc\u30bf\u62e1\u5f35\u5f8c\u306e\u753b\u50cf\u3092\u53d6\u5f97\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (https:\/\/albumentations.ai\/docs\/getting_started\/image_augmentation\/)\n\n        return image","7028ab74":"def get_transforms(*, data, size, norms):\n\n    if data == 'train':\n        return Compose([\n            \"\"\"\n            \u3053\u3053\u3067\u3001\u30ea\u30b5\u30a4\u30ba\u3001\u6b63\u898f\u5316\u3001Tensor\u5316\u3092\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\uff08\u30aa\u30d7\u30b7\u30e7\u30f3: \u30c7\u30fc\u30bf\u62e1\u5f35\u306e\u8ffd\u52a0\u3082\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\uff09\n            \n            \u30ea\u30b5\u30a4\u30ba\uff1ahttps:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/geometric\/resize\/#albumentations.augmentations.geometric.resize.Resize\n            \u6b63\u898f\u5316\uff1ahttps:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.Normalize\n            Tensor\u5316\uff1ahttps:\/\/albumentations.ai\/docs\/api_reference\/pytorch\/transforms\/#albumentations.pytorch.transforms.ToTensorV2\n            \"\"\"\n        ])\n\n    elif data == 'valid':\n        return Compose([\n            \"\"\"\n            \u3053\u3053\u3067\u3001\u30ea\u30b5\u30a4\u30ba\u3001\u6b63\u898f\u5316\u3001Tensor\u5316\u3092\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\uff08\u30aa\u30d7\u30b7\u30e7\u30f3: \u30c7\u30fc\u30bf\u62e1\u5f35\u306e\u8ffd\u52a0\u3082\u884c\u3063\u3066\u304f\u3060\u3055\u3044\u3002\uff09\n            \n            \u30ea\u30b5\u30a4\u30ba\uff1ahttps:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/geometric\/resize\/#albumentations.augmentations.geometric.resize.Resize\n            \u6b63\u898f\u5316\uff1ahttps:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.Normalize\n            Tensor\u5316\uff1ahttps:\/\/albumentations.ai\/docs\/api_reference\/pytorch\/transforms\/#albumentations.pytorch.transforms.ToTensorV2\n            \"\"\"\n        ])","143380ee":"# use timm model no pretrained\nclass BaseModel(nn.Module):\n    def __init__(\n        self, model_name=CFG.MODEL_NAME, n_class=10, pretrained=False\n    ):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained) # model load(pretrained=False: \u4e8b\u524d\u5b66\u7fd2\u306a\u3057)\n        n_features = self.model.fc.in_features\n        self.model.classifier =  # \u3053\u3053\u3067\u3001\u51fa\u529b\u5c64\u306e\u6b21\u5143\u3092\u30af\u30e9\u30b9\u6570\u306b\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Linear.html)\n\n    def forward(self, x):\n        output = self.model(x)\n        return output","429a24c4":"def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    #avg_score = AverageMeter()\n\n    \"\"\"\n    \u3053\u3053\u3067\u3001\u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u30e2\u30fc\u30c9\u306b\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Module.html)\n    \"\"\"\n    \n    start = end = time.time()\n    global_step = 0\n\n    for step, (images, labels) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(CFG.device) # \u753b\u50cf\u3092cpu\u304b\u3089gpu\u30d8\n        labels = labels.to(CFG.device) # \u6b63\u89e3\u30e9\u30d9\u30eb\u3092cpu\u304b\u3089gpu\u30d8\n        batch_size = labels.size(0) \n        y_preds = model(images) # \u4e88\u6e2c\u30e9\u30d9\u30eb\n        loss = criterion(y_preds, labels) # loss\u306e\u8a08\u7b97\n\n        losses.update(loss.item(), batch_size) \n        loss.backward() # \u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u52fe\u914d\u3092\u8a08\u7b97\n        \n        del loss \n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n        optimizer.step() # \u30e2\u30c7\u30eb\u66f4\u65b0\n        optimizer.zero_grad() # \u52fe\u914d\u306e\u521d\u671f\u5316\n        global_step += 1\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n            print('Epoch: [{0}][{1}\/{2}] '\n                  'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                  'Elapsed {remain:s} '\n                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                  .format(\n                   epoch+1, step, len(train_loader), batch_time=batch_time,\n                   data_time=data_time, loss=losses,\n                   remain=timeSince(start, float(step+1)\/len(train_loader)),\n                   grad_norm=grad_norm,\n                   ))\n\n        del y_preds, images, labels, batch_size\n    del batch_time, data_time, scores, model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return losses.avg","7a92a9dd":"def valid_fn(valid_loader, model, criterion):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    \n    \"\"\"\n    \u3053\u3053\u3067\u3001\u30e2\u30c7\u30eb\u3092\u63a8\u8ad6\u30e2\u30fc\u30c9\u306b\u5909\u66f4\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \"\"\"\n    \n    preds = []\n    start = end = time.time()\n    for step, (images, labels) in enumerate(valid_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(CFG.device)\n        labels = labels.to(CFG.device)\n        batch_size = labels.size(0)\n        with torch.no_grad():\n        #with torch.inference_mode():\n            y_preds = model(images)\n            loss = criterion(y_preds, labels)\n            losses.update(loss.item(), batch_size)\n            \n            preds.append(y_preds.softmax(1).to(\"cpu\").numpy().argmax(1))\n            del loss\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n                print('EVAL: [{0}\/{1}] '\n                    'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                    'Elapsed {remain:s} '\n                    'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                    .format(\n                    step, len(valid_loader), batch_time=batch_time,\n                    data_time=data_time, loss=losses,\n                    remain=timeSince(start, float(step+1)\/len(valid_loader)),\n                    ))\n        del y_preds, images, labels, batch_size\n    del batch_time, data_time, scores, model\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    preds = np.concatenate(preds)\n\n    return losses.avg, preds","b6475d1b":"DATA_PATH = \"..\/input\/digit-recognizer\" # \u30c7\u30a3\u30ec\u30af\u30c8\u30ea\u30d1\u30b9\n\ntrain = pd.read_csv(DATA_PATH + \"\/train.csv\") # csv\u306e\u8aad\u307f\u8fbc\u307f\ntrain.head()","4063e133":"import plotly.express as px\n\ntarget = train.label.value_counts()\nclass_num = len(target)\n\nfig = px.pie(target,\n             values='label',\n             names=target.index,\n             hole=.4, \n             width=500, height=500)\nfig.update_traces(textinfo='value+label', pull=0.01)\nfig.show()","b1d57531":"\"\"\"\n\u3053\u3053\u3067\u3001train_test_split\u3092\u7528\u3044\u3066\u3001\u5b66\u7fd2\u7528\u30c7\u30fc\u30bf\u3068\u8a55\u4fa1\u7528\u30c7\u30fc\u30bf\u306b\u5206\u5272\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u3000(https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html)\n\"\"\"","c2315480":"images = train.loc[:, \"pixel0\": \"pixel783\"].values.astype(np.uint8).reshape(-1, 28, 28)\n\"\"\"\n\u753b\u50cf\u306e\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97\u3057\u3066\u3001\u4ee5\u4e0b\u306e\u5f62\u3067\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\nnorms = (\u5e73\u5747\u3001\u6a19\u6e96\u504f\u5dee)\n\"\"\"\n\nprint(\"\u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee: \", norms)","87557a67":"print(\"height: \", images.shape[1])\nprint(\"weight: \", images.shape[2])\n\nfor n in range(3):\n    fig, axes = plt.subplots(1, 10, figsize=(20, 30))\n    [axes[i].imshow(images[idx]) for i, idx in enumerate(range(n*10, (n+1)*10))]\n\n    plt.show()","a30a7c09":"model = BaseModel(CFG.MODEL_NAME, class_num)\nmodel","af3626bc":"# Adam \u306f\u52fe\u914d\u3092\u79fb\u52d5\u5e73\u5747\u3068\u5b66\u7fd2\u7387\u306e\u8abf\u6574\u304b\u3089\u306a\u308a\u307e\u3059 (Momentum\u3068RMSProp)\noptimizer = Adam(model.parameters(), lr=CFG.model_lr, weight_decay=CFG.weight_decay, amsgrad=False)","7cecdcfd":"scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=0.001)\n\nlog_s = []\nfor epoch in range(0, 100):\n    log_s.append(scheduler.get_last_lr()[0])\n    scheduler.step()\n\nplt.figure(figsize=(10, 5))\nplt.plot(np.arange(100), log_s)\nplt.show()","1e9f4ac1":"def train_loop(train, fold, class_num=None, norms=(0.5, 0.5)):\n    \n    train_index = # \u3053\u3053\u3067\u3001\u5b66\u7fd2\u7528\u306eindex\u3092\u53d6\u5f97\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    valid_index = # \u3053\u3053\u3067\u3001\u8a55\u4fa1\u7528\u306eindex\u3092\u53d6\u5f97\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \n    train_dataset = TrainDataset(train.loc[train_index].reset_index(drop=True), \n                                 transform=get_transforms(data='train', size=CFG.size, norms=norms)) # \u5b66\u7fd2\u7528\u306edataset\u3092\u4f5c\u6210\n    \n    valid_dataset = # \u3053\u3053\u3067\u3001\u5b66\u7fd2\u7528\u3092\u53c2\u8003\u306b\u3001\u8a55\u4fa1\u7528\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\n    train_loader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True, \n                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)# \u5b66\u7fd2\u7528\u306edatasets\u306ebatch\u3092\u4f5c\u6210\n    \n    valid_loader = # \u3053\u3053\u3067\u3001\u5b66\u7fd2\u7528\u3092\u53c2\u8003\u306b\u3001\u8a55\u4fa1\u7528\u306e\u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \n    model = BaseModel(CFG.MODEL_NAME, class_num).to(CFG.device)\n    \n    optimizer = # \u3053\u3053\u3067\u3001\u5148\u307b\u3069\u8aac\u660e\u3057\u305foptimizer\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (https:\/\/pytorch.org\/docs\/stable\/generated\/torch.optim.Adam.html#torch.optim.Adam)\n    scheduler = # \u3053\u3053\u3067\u3001\u5148\u307b\u3069\u8aac\u660e\u3057\u305fscheduler\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (https:\/\/pytorch.org\/docs\/stable\/generated\/torch.optim.lr_scheduler.CosineAnnealingLR.html)\n    \n    criterion = # \u3053\u3053\u3067\u3001CrossEntropyLoss\u3092\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html)\n\n    best_score = 0.\n    best_loss = np.inf\n    \n    for epoch in range(CFG.epochs):\n        start_time = time.time()\n        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler) # \u5b66\u7fd2\n        avg_val_loss, predict = valid_fn(valid_loader, model, criterion) # \u8a55\u4fa1\u7528\u306e\u63a8\u8ad6\n        valid_labels = train.loc[valid_index, \"label\"].values # \u4e88\u6e2c\u30e9\u30d9\u30eb\n        \n        scheduler.step() \n        score = #\u3053\u3053\u3067\u3001\u4e88\u6e2c\u5024\u3068\u6b63\u89e3\u30e9\u30d9\u30eb\u306eaccuracy\u3092\u3082\u3068\u3081\u3066\u304f\u3060\u3055\u3044\u3002 (\u30d2\u30f3\u30c8: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html)\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Accuracy: {score}')\n\n        if score > best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save(model.state_dict(), \".\/\" + f'{CFG.MODEL_NAME}_fold{fold}_best_{CFG.Version}.pth') # \u30e2\u30c7\u30eb\u306e\u4fdd\u5b58\n            valid_folds_predict = predict\n            \n        del avg_loss, avg_val_loss, valid_labels, predict, score\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    return valid_folds_predict","a64fd028":"preds = train_loop(train, fold=0, class_num=class_num, norms=norms)","4a8cd9e6":"def inference(test, class_num=class_num, n=0, norms=(0.5, 0.5)):\n    model = BaseModel(CFG.MODEL_NAME, class_num, pretrained=False)\n\n    model.load_state_dict(\"\u91cd\u307f\u306e\u30d1\u30b9\") # ()\u306e\u4e2d\u306b\u5148\u307b\u3069\u4fdd\u5b58\u3057\u305f\u91cd\u307f\u3078\u306e\u6b63\u3057\u3044\u30d1\u30b9\u3092\u4e0e\u3048\u3066\u304f\u3060\u3055\u3044\u3002\n    model.to(CFG.device)\n    \n    test_dataset = # \u3053\u3053\u3067\u3001\u5b66\u7fd2\u7528\u3092\u53c2\u8003\u306b\u3001\u30c6\u30b9\u30c8\u7528\u306e\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    test_loader = # \u3053\u3053\u3067\u3001\u5b66\u7fd2\u7528\u3092\u53c2\u8003\u306b\u3001\u30c6\u30b9\u30c8\u7528\u306e\u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u3092\u4f5c\u6210\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n    \n    all_predicts = []\n    model.eval() # \u63a8\u8ad6\u30e2\u30fc\u30c9\n    with torch.no_grad(): # \u52fe\u914d\u306e\u8a08\u7b97\u3092\u3057\u306a\u3044\n        for i, (images) in tqdm(enumerate(test_loader), total=len(test_loader)):\n            images = images.to(CFG.device)\n            y_preds = model(images)\n            predicts = y_preds.softmax(1).to(\"cpu\").numpy().argmax(1)\n            all_predicts.append(predicts)\n    \n    predicts = np.concatenate(all_predicts)\n        \n    return predicts","6e2804a9":"MODEL_PATH = \".\/\" # \u81ea\u8eab\u306e\u4f5c\u6210\u3057\u305f\u30e2\u30c7\u30eb\u3078\u306ePATH\n\ntest = pd.read_csv(DATA_PATH + \"\/test.csv\")\nsubmit = pd.read_csv(DATA_PATH + \"\/sample_submission.csv\")","6a8eefe6":"predicts = inference(test, class_num=class_num, n=0, norms=norms)","7dd97b9d":"submit[\"Label\"] = predicts[0] # \u63d0\u51fa\u7528csv\u306b\u66f8\u304d\u8fbc\u307f\n\n\"\"\"\n\u3053\u3053\u3067\u3001csv\u3092\u66f8\u304d\u51fa\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.DataFrame.to_csv.html)\n\"\"\"","0fa8cfb7":"submit","42090774":"## \u8a55\u4fa1\u6307\u6a19(Accuracy), Confusion matrix\n#### \u8a73\u7d30\u306b\u3064\u3044\u3066\u306f\u3001\u6765\u9031\u8aac\u660e","6ea8b8df":"# Model","25550aa5":"### \u30b9\u30b1\u30b8\u30e5\u30fc\u30e9","ce040828":"# \u8efd\u3081\u306e\u30e2\u30c7\u30eb\u4f5c\u6210","541f195d":"# install library","5c632d76":"# train & valid function","49a9ba87":"### \u6700\u9069\u5316\u95a2\u6570","44bf7a18":"## \u30cf\u30a4\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u7528\u8a9e\u7c21\u6613\u8aac\u660e\u306a\u3069","f826ed27":"#### \u5e73\u5747\u3068\u6a19\u6e96\u504f\u5dee\u3092\u8a08\u7b97","d7f08196":"# \u5f8c\u51e6\u7406","31bd9c1f":"| <div style=\"width:120px; height:30px\"> <\/div> | <div style=\"width:120px\"> <\/div> | <div style=\"width:120px\">**\u4e88\u6e2c**<\/div> | <div style=\"width:120px\"> <\/div> | \n|  :----: | :---: | :---: | :---: | \n| <div style=\"height:40px\"> <\/div>   |     | **pos**  | **neg** | \n| <div style=\"height:40px\"> **\u5b9f\u969b** <\/div>  | **pos** | *TP*   | *FN*  | \n| <div style=\"height:40px\"> <\/div> | **neg** | *FP*  | *TN*  | ","3eb68e11":"> #### * \u5b66\u7fd2\u7387 : \u5b66\u7fd2\u4e00\u56de\u5f53\u305f\u308a\u306e\u66f4\u65b0\u91cf\u306e\u3088\u3046\u306a\u3082\u306e\u3067\u3059\u3002 \u4eca\u56de\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306f1e-5\u3067\u3059\u3002\n> #### * \u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u30fc : epoch\u6570\u306b\u5fdc\u3058\u3066\u3001\u5b66\u7fd2\u7387\u3092\u5909\u5316\u3055\u305b\u308b\u3082\u306e\u3067\u3059\u3002 \u4eca\u56de\u306e\u30c7\u30d5\u30a9\u30eb\u30c8\u306fCosineAnnealingLR\u3067\u3059\u3002\n> #### * \u640d\u5931\u95a2\u6570: \u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u4e88\u6e2c\u30e9\u30d9\u30eb\u306e\u8aa4\u5dee\u306e\u5927\u304d\u3055\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\u3002 \n> #### * \u6700\u9069\u5316\u95a2\u6570: \u52b9\u7387\u3088\u304f\u640d\u5931\u3092\u6e1b\u3089\u3059\u305f\u3081\u306e\u95a2\u6570\u3002 \n> #### * timm : \u753b\u50cf\u306e\u30b3\u30f3\u30da\u3067pytorch\u3092\u4f7f\u3046\u5834\u5408\u306b\u307b\u307c\u78ba\u5b9f\u306b\u4f7f\u7528\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u306e\u3067\u3001\u898b\u3066\u304a\u304d\u307e\u3057\u3087\u3046\u3002[github](https:\/\/github.com\/rwightman\/pytorch-image-models)","b2e0be3d":"# \u304a\u6642\u9593\u306e\u3042\u308b\u65b9\u7528\u306e\u8ab2\u984c\u3067\u3059(*\u3053\u306e\u307e\u307e\u3067\u306f\u52d5\u304d\u307e\u305b\u3093)\n\n## \u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\u3055\u308c\u3066\u3044\u308b\u90e8\u5206\u3092\u3001\u53c2\u8003\u30ea\u30f3\u30af\u3092\u898b\u306a\u304c\u3089\u5b9f\u88c5\u3057\u3066\u304f\u3060\u3055\u3044\u3002 (\u5206\u304b\u3089\u306a\u3044\u90e8\u5206\u306f\u5168\u4f53\u306b\u8cea\u554f\u3057\u3066\u307f\u307e\u3057\u3087\u3046\uff01)","2e1165e0":"# \u5b66\u7fd2","ba6ffe33":"# \u63a8\u8ad6","9f087d93":"# Data\u306e\u78ba\u8a8d","fd6b17c6":"#### $$ accuracy = \\frac{TP + TN}{TP + TN + FP + FN} $$","6adab074":"# \u524d\u51e6\u7406","cc4521af":"### \u30db\u30fc\u30eb\u30c9\u30a2\u30a6\u30c8","7e7c2a3f":"# Dataset","1087941b":"# \u5b66\u7fd2\u306e\u969b\u5fc5\u8981\u306b\u306a\u308b\u77e5\u8b58","e4a44cd9":"# Utils","892e8ca0":"# \u5168\u4f53\u306e\u6d41\u308c","0d29668e":"#### \u753b\u50cf\u30b5\u30a4\u30ba\u306e\u78ba\u8a8d\u3068\u6570\u4f8b\u3092\u8868\u793a","a5d7a2cd":"# Config\n","c7fd2487":"### \u30e2\u30c7\u30eb\u306e\u8aad\u307f\u8fbc\u307f"}}