{"cell_type":{"31d6c245":"code","6b55dbdc":"code","1ff847f9":"code","d0a5e8c2":"code","81ddfdcd":"code","27b72f4c":"code","2078da17":"code","3a084685":"code","227fd67a":"code","10d31fa9":"code","8a93220e":"code","44e874d5":"code","bfdb4cdb":"code","600a70e3":"code","ba9a7672":"code","4d7ec79d":"code","3d477cfa":"code","8a6c1ffb":"code","862abe7e":"code","633710df":"code","40ac7439":"code","76c42acb":"code","8e0abbe2":"code","bab91b0b":"code","9a12a6cc":"code","5dfb9560":"code","5a075796":"code","214fb962":"code","21c9b861":"markdown","2d868d3c":"markdown","5d43ccd0":"markdown","6bfde61f":"markdown","0675f477":"markdown","dc792895":"markdown","39a085b8":"markdown","d03903ee":"markdown","2fa5e8dd":"markdown","c13e5107":"markdown"},"source":{"31d6c245":"# !pip install --upgrade torch torchvision torchaudio","6b55dbdc":"!pip install efficientnet_pytorch","1ff847f9":"import torch\n\ntorch.__version__","d0a5e8c2":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport re\nimport albumentations as A\nimport torch\nimport torchvision\nimport time\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom PIL import Image\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom matplotlib import pyplot as plt\nfrom tqdm import tqdm\n\nimport matplotlib \nmatplotlib.style.use('ggplot')","81ddfdcd":"DIR_INPUT = '..\/input\/global-wheat-detection'\nDIR_TRAIN = f\"{DIR_INPUT}\/train\"\nDIR_TEST = f\"{DIR_INPUT}\/test\"","27b72f4c":"DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(DEVICE)","2078da17":"# # load the model\n# model = torchvision.models.resnet18(pretrained=True).eval()\n# # hook the feature extractor\n# # https:\/\/github.com\/zhoubolei\/CAM\/blob\/master\/pytorch_CAM.py\n# features_blobs = []\n# def hook_feature(module, input, output):\n#     features_blobs.append(output.data.cpu().numpy())\n    \n# new_model = model._modules.get('layer4').register_forward_hook(hook_feature)\n# print(new_model)","3a084685":"\"\"\"\nmodel.py\n\nWe will create a FasterRCNN object detector with EfficientNet backbone for custom training.\nReference: https:\/\/github.com\/lukemelas\/EfficientNet-PyTorch\n\"\"\"\ndef create_model():\n    from efficientnet_pytorch import EfficientNet\n#     backbone = EfficientNet.from_pretrained('efficientnet-b0')\n#     print(backbone)\n    conv_stem = torch.nn.Sequential(EfficientNet.from_pretrained('efficientnet-b0')._conv_stem)\n    bn = torch.nn.Sequential(EfficientNet.from_pretrained('efficientnet-b0')._bn0)\n    blocks = torch.nn.Sequential(*EfficientNet.from_pretrained('efficientnet-b0')._blocks)\n    conv_head = torch.nn.Sequential(EfficientNet.from_pretrained('efficientnet-b0')._conv_head)\n#     conv_head.out_channels = 1280\n    backbone = torch.nn.Sequential(conv_stem, bn, blocks, conv_head)\n    backbone.out_channels = 1280\n    print(backbone)\n    # FasterRCNN needs to know the number of\n    # output channels in a backbone. For EfficientNetB0, it's 1280\n    # so we need to add it here\n#     backbone.out_channels = 1280\n    print('-'*70)\n#     print(backbone)\n    # let's make the RPN generate 5 x 3 anchors per spatial\n    # location, with 5 different sizes and 3 different aspect\n    # ratios. We have a Tuple[Tuple[int]] because each feature\n    # map could potentially have different sizes and\n    # aspect ratios\n    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                       aspect_ratios=((0.5, 1.0, 2.0),))\n    \n    # let's define what are the feature maps that we will\n    # use to perform the region of interest cropping, as well as\n    # the size of the crop after rescaling.\n    # if your backbone returns a Tensor, featmap_names is expected to\n    # be [0]. More generally, the backbone should return an\n    # OrderedDict[Tensor], and in featmap_names you can choose which\n    # feature maps to use.\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n                                                    output_size=7, \n                                                    sampling_ratio=2)\n    # put everything together\n    model = FasterRCNN(backbone, \n                       num_classes=2, \n                       rpn_anchor_generator=anchor_generator,\n                       box_roi_pool=roi_pooler) \n    return model\n\nmodel = create_model()\nprint('-'*50)\n# print(model)","227fd67a":"train_df = pd.read_csv(f\"{DIR_INPUT}\/train.csv\")\nprint(train_df.shape)\ntrain_df.head()","10d31fa9":"train_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r\n\ntrain_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\ntrain_df.drop(columns=['bbox'], inplace=True)\ntrain_df['x'] = train_df['x'].astype(np.float)\ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)\n\ntrain_df.head()","8a93220e":"unique_image_ids = train_df['image_id'].unique()\nprint(f\"Uninque image IDs: {len(unique_image_ids)}\")\nvalid_ids = unique_image_ids[-665:]\ntrain_ids = unique_image_ids[:-665]\nprint(f\"Unqiue image IDs for training: {len(train_ids)}\")\nprint(f\"Unqiue image IDs for validation: {len(valid_ids)}\")","44e874d5":"valid_df = train_df[train_df['image_id'].isin(valid_ids)]\ntrain_df = train_df[train_df['image_id'].isin(train_ids)]\n\nprint(f\"Total training annotation instances: {len(train_df)}\")\nprint(f\"Total validation annotation instances: {len(valid_df)}\")","bfdb4cdb":"train_df.head(7)","600a70e3":"valid_df.head(7)","ba9a7672":"class WheatDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n        \n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        \n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        records = self.df[self.df['image_id'] == image_id]\n        \n        image = cv2.imread(f\"{self.image_dir}\/{image_id}.jpg\", cv2.IMREAD_COLOR).astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image \/= 255.0\n        \n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        \n        # as there is only one class\n        labels = torch.ones((records.shape[0], ), dtype=torch.int64)\n        \n        # no crowd instances\n        iscrowd = torch.zeros((records.shape[0], ), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.stack(tuple(map(torch.FloatTensor, zip(*sample['bboxes'])))).permute(1, 0)\n            \n            return image, target, image_id\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]","4d7ec79d":"# define the training tranforms\ndef get_train_transform():\n    return A.Compose([\n        A.Flip(0.5),\n        A.RandomRotate90(0.5),\n        A.MotionBlur(p=0.2),\n        A.MedianBlur(blur_limit=3, p=0.1),\n        A.Blur(blur_limit=3, p=0.1),\n        ToTensorV2(p=1.0),\n    ], bbox_params={\n        'format': 'pascal_voc',\n        'label_fields': ['labels']\n    })\n\n# define the validation transforms\ndef get_valid_transform():\n    return A.Compose([\n        ToTensorV2(p=1.0),\n    ], bbox_params={\n        'format': 'pascal_voc', \n        'label_fields': ['labels']\n    })","3d477cfa":"class Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n    \n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n    \n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0","8a6c1ffb":"BATCH_SIZE = 8\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = WheatDataset(train_df, DIR_TRAIN, get_train_transform())\nvalid_dataset = WheatDataset(valid_df, DIR_TRAIN, get_valid_transform())\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","862abe7e":"images, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(DEVICE) for image in images)\ntargets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]","633710df":"for i in range(2):\n    boxes = targets[i]['boxes'].cpu().numpy().astype(np.int32)\n    sample = images[i].permute(1, 2, 0).cpu().numpy()\n    plt.figure(figsize=(12, 9))\n    sample = cv2.cvtColor(sample, cv2.COLOR_RGB2BGR)\n    for box in boxes:\n#         print(box)\n        cv2.rectangle(sample,  # the image is in RGB, convert to BGR for cv2 annotations\n                      (box[0], box[1]),\n                      (box[2], box[3]),\n                      (0, 0, 255), 3)\n    plt.imshow(sample[:, :, ::-1])\n    plt.axis('off')","40ac7439":"model = model.to(DEVICE)","76c42acb":"# print(model)","8e0abbe2":"params = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\nlr_scheduler = None","bab91b0b":"NUM_EPOCHS = 55","9a12a6cc":"train_loss_hist = Averager()\nval_loss_hist = Averager()\ntrain_itr = 1\nval_itr = 1\ntrain_loss_list = []\nval_loss_list = []","5dfb9560":"def train(train_data_loader):\n    global train_itr\n    global train_loss_list\n    for i, data in enumerate(train_data_loader):\n        images, targets, image_ids = data\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        train_loss_list.append(loss_value)\n\n        train_loss_hist.send(loss_value)\n\n        optimizer.zero_grad()\n\n        losses.backward()\n        optimizer.step()\n\n        if train_itr % 50 == 0:\n            print(f\"Training iteration #{train_itr} loss: {loss_value}\")\n\n        train_itr += 1\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n    return train_loss_list","5a075796":"def validate(valid_data_loader):\n    global val_itr\n    global val_loss_list\n    for i, data in enumerate(valid_data_loader):\n        images, targets, image_ids = data\n        \n        images = list(image.to(DEVICE) for image in images)\n        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n        \n        with torch.no_grad():\n            loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n        val_loss_list.append(loss_value)\n\n        val_loss_hist.send(loss_value)\n\n#         if itr % 50 == 0:\n#             print(f\"Validation iteration #{itr} loss: {loss_value}\")\n\n        val_itr += 1\n    \n    # update the learning rate\n#     if lr_scheduler is not None:\n#         lr_scheduler.step()\n    return val_loss_list","214fb962":"for epoch in range(NUM_EPOCHS):\n    start = time.time()\n    train_loss_hist.reset()\n    val_loss_hist.reset()\n    train_loss = train(train_data_loader)\n    val_loss = validate(valid_data_loader)\n    print(f\"Epoch #{epoch} train loss: {train_loss_hist.value}\")   \n    print(f\"Epoch #{epoch} validation loss: {val_loss_hist.value}\")   \n    end = time.time()\n    print(f\"Took {(end - start) \/ 60} minutes for epoch {epoch}\")\n    print('SAVING MODEL...')\n    if epoch % 5 == 0:\n        torch.save(model.state_dict(), f\"fasterrcnn_efficientnetb0_{epoch+1}.pth\")\n    print('SAVING COMPLETE...\\n')\n    \n    \n    plt.plot(val_loss, color='red')\n    plt.plot(train_loss, color='blue', alpha=0.5)\n    plt.xlabel('iterations')\n    plt.ylabel('loss')\n    plt.savefig(f\"loss_{epoch+1}.png\")\n    if epoch % 5 == 0:\n        plt.show()\n    plt.close","21c9b861":"## All Imports","2d868d3c":"## Sample Visualization","5d43ccd0":"## Transforms","6bfde61f":"## Prepare Proper DataFrame","0675f477":"## Utilities and Helper Functions","dc792895":"## Training","39a085b8":"## Introductin\n* Reference kernel => https:\/\/www.kaggle.com\/sovitrath\/pytorch-starter-faster-rcnn-train?scriptVersionId=38399463","d03903ee":"## Prepare Dataset","2fa5e8dd":"## Create Model","c13e5107":"## Constant Paths"}}