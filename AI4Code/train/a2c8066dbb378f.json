{"cell_type":{"17ae4827":"code","1f555e0e":"code","dd0df0af":"code","e13f05b1":"code","e33fc813":"code","5e814a69":"code","66f16446":"code","15c5b086":"code","b32efbf1":"code","ee5ca3af":"code","5f8c504c":"code","b1a46a15":"code","e0947b8c":"code","9401f31b":"code","cbdaa903":"code","caa9d48a":"code","6c27325b":"code","3b1fcad3":"code","859affe0":"code","899ec295":"code","dd4b1de8":"code","b430b55e":"code","575882ec":"code","a0e5e387":"code","6beaf5a0":"code","ceddc4ce":"code","ae84526a":"code","290d50dd":"code","6fcbda7b":"code","ba6a6671":"code","230594f3":"markdown","2dc67ea2":"markdown","1249ba09":"markdown","7ade7881":"markdown","ee33e8ad":"markdown","faca7bcb":"markdown","4801ea8b":"markdown","527e044c":"markdown","b5ef30ee":"markdown","810c5660":"markdown"},"source":{"17ae4827":"import gc\nimport glob\nimport os\nimport time\nimport traceback\nfrom contextlib import contextmanager\nfrom enum import Enum\nfrom typing import Dict, List, Optional, Tuple\n\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nfrom IPython.display import display\n\nfrom joblib import delayed, Parallel\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import minmax_scale\nfrom tqdm import tqdm_notebook as tqdm\n\n\n%matplotlib inline\n\nDATA_DIR = '..\/input'\n\n# data configurations\nUSE_PRECOMPUTE_FEATURES = True  # Load precomputed features for train.csv from private dataset (just for speed up)\n\n# model & ensemble configurations\nPREDICT_CNN = True\nPREDICT_MLP = True\nPREDICT_GBDT = True\nPREDICT_TABNET = False\n\nGBDT_NUM_MODELS = 5 #3\nGBDT_LR = 0.02  # 0.1\n\nNN_VALID_TH = 0.185\nNN_MODEL_TOP_N = 3\nTAB_MODEL_TOP_N = 3\nENSEMBLE_METHOD = 'mean'\nNN_NUM_MODELS = 10\nTABNET_NUM_MODELS = 5\n\n# for saving quota\nIS_1ST_STAGE = False\nSHORTCUT_NN_IN_1ST_STAGE = False  # early-stop training to save GPU quota\nSHORTCUT_GBDT_IN_1ST_STAGE = False\nMEMORY_TEST_MODE = False\n\n# for ablation studies\nCV_SPLIT = 'time'  # 'time': time-series KFold 'group': GroupKFold by stock-id\nUSE_PRICE_NN_FEATURES = True  # Use nearest neighbor features that rely on tick size\nUSE_VOL_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\nUSE_SIZE_NN_FEATURES = True  # Use nearest neighbor features that can be calculated without tick size\nUSE_RANDOM_NN_FEATURES = False  # Use random index to aggregate neighbors\n\nUSE_TIME_ID_NN = True  # Use time-id based neighbors\nUSE_STOCK_ID_NN = True  # Use stock-id based neighbors\n\nENABLE_RANK_NORMALIZATION = True  # Enable rank-normalization\n\n\n@contextmanager\ndef timer(name: str):\n    s = time.time()\n    yield\n    elapsed = time.time() - s\n    print(f'[{name}] {elapsed: .3f}sec')\n    \ndef print_trace(name: str = ''):\n    print(f'ERROR RAISED IN {name or \"anonymous\"}')\n    print(traceback.format_exc())","1f555e0e":"!pip -q install ..\/input\/pytorchtabnet\/pytorch_tabnet-2.0.1-py3-none-any.whl","dd0df0af":"train = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'train.csv'))\nstock_ids = set(train['stock_id'])","e13f05b1":"class DataBlock(Enum):\n    TRAIN = 1\n    TEST = 2\n    BOTH = 3\n\n\ndef load_stock_data(stock_id: int, directory: str) -> pd.DataFrame:\n    return pd.read_parquet(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', directory, f'stock_id={stock_id}'))\n\n\ndef load_data(stock_id: int, stem: str, block: DataBlock) -> pd.DataFrame:\n    if block == DataBlock.TRAIN:\n        return load_stock_data(stock_id, f'{stem}_train.parquet')\n    elif block == DataBlock.TEST:\n        return load_stock_data(stock_id, f'{stem}_test.parquet')\n    else:\n        return pd.concat([\n            load_data(stock_id, stem, DataBlock.TRAIN),\n            load_data(stock_id, stem, DataBlock.TEST)\n        ]).reset_index(drop=True)\n\ndef load_book(stock_id: int, block: DataBlock=DataBlock.TRAIN) -> pd.DataFrame:\n    return load_data(stock_id, 'book', block)\n\n\ndef load_trade(stock_id: int, block=DataBlock.TRAIN) -> pd.DataFrame:\n    return load_data(stock_id, 'trade', block)\n\n\ndef calc_wap1(df: pd.DataFrame) -> pd.Series:\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n\ndef calc_wap2(df: pd.DataFrame) -> pd.Series:\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n\ndef log_return(series: np.ndarray):\n    return np.log(series).diff()\n\n\ndef log_return_df2(series: np.ndarray):\n    return np.log(series).diff(2)\n\n\ndef flatten_name(prefix, src_names):\n    ret = []\n    for c in src_names:\n        if c[0] in ['time_id', 'stock_id']:\n            ret.append(c[0])\n        else:\n            ret.append('.'.join([prefix] + list(c)))\n    return ret\n\n\ndef make_book_feature(stock_id, block = DataBlock.TRAIN):\n    book = load_book(stock_id, block)\n\n    book['wap1'] = calc_wap1(book)\n    book['wap2'] = calc_wap2(book)\n    book['log_return1'] = book.groupby(['time_id'])['wap1'].apply(log_return)\n    book['log_return2'] = book.groupby(['time_id'])['wap2'].apply(log_return)\n    book['log_return_ask1'] = book.groupby(['time_id'])['ask_price1'].apply(log_return)\n    book['log_return_ask2'] = book.groupby(['time_id'])['ask_price2'].apply(log_return)\n    book['log_return_bid1'] = book.groupby(['time_id'])['bid_price1'].apply(log_return)\n    book['log_return_bid2'] = book.groupby(['time_id'])['bid_price2'].apply(log_return)\n\n    book['wap_balance'] = abs(book['wap1'] - book['wap2'])\n    book['price_spread'] = (book['ask_price1'] - book['bid_price1']) \/ ((book['ask_price1'] + book['bid_price1']) \/ 2)\n    book['bid_spread'] = book['bid_price1'] - book['bid_price2']\n    book['ask_spread'] = book['ask_price1'] - book['ask_price2']\n    book['total_volume'] = (book['ask_size1'] + book['ask_size2']) + (book['bid_size1'] + book['bid_size2'])\n    book['volume_imbalance'] = abs((book['ask_size1'] + book['ask_size2']) - (book['bid_size1'] + book['bid_size2']))\n    \n    features = {\n        'seconds_in_bucket': ['count'],\n        'wap1': [np.sum, np.mean, np.std],\n        'wap2': [np.sum, np.mean, np.std],\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return_ask1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return_ask2': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return_bid1': [np.sum, realized_volatility, np.mean, np.std],\n        'log_return_bid2': [np.sum, realized_volatility, np.mean, np.std],\n        'wap_balance': [np.sum, np.mean, np.std],\n        'price_spread':[np.sum, np.mean, np.std],\n        'bid_spread':[np.sum, np.mean, np.std],\n        'ask_spread':[np.sum, np.mean, np.std],\n        'total_volume':[np.sum, np.mean, np.std],\n        'volume_imbalance':[np.sum, np.mean, np.std]\n    }\n    \n    agg = book.groupby('time_id').agg(features).reset_index(drop=False)\n    agg.columns = flatten_name('book', agg.columns)\n    agg['stock_id'] = stock_id\n    \n    for time in [450, 300, 150]:\n        d = book[book['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n        d.columns = flatten_name(f'book_{time}', d.columns)\n        agg = pd.merge(agg, d, on='time_id', how='left')\n    return agg\n\n\ndef make_trade_feature(stock_id, block = DataBlock.TRAIN):\n    trade = load_trade(stock_id, block)\n    trade['log_return'] = trade.groupby('time_id')['price'].apply(log_return)\n\n    features = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':['count'],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n\n    agg = trade.groupby('time_id').agg(features).reset_index()\n    agg.columns = flatten_name('trade', agg.columns)\n    agg['stock_id'] = stock_id\n        \n    for time in [450, 300, 150]:\n        d = trade[trade['seconds_in_bucket'] >= time].groupby('time_id').agg(features).reset_index(drop=False)\n        d.columns = flatten_name(f'trade_{time}', d.columns)\n        agg = pd.merge(agg, d, on='time_id', how='left')\n    return agg\n\n\ndef make_book_feature_v2(stock_id, block = DataBlock.TRAIN):\n    book = load_book(stock_id, block)\n\n    prices = book.set_index('time_id')[['bid_price1', 'ask_price1', 'bid_price2', 'ask_price2']]\n    time_ids = list(set(prices.index))\n\n    ticks = {}\n    for tid in time_ids:\n        try:\n            price_list = prices.loc[tid].values.flatten()\n            price_diff = sorted(np.diff(sorted(set(price_list))))\n            ticks[tid] = price_diff[0]\n        except Exception:\n            print_trace(f'tid={tid}')\n            ticks[tid] = np.nan\n        \n    dst = pd.DataFrame()\n    dst['time_id'] = np.unique(book['time_id'])\n    dst['stock_id'] = stock_id\n    dst['tick_size'] = dst['time_id'].map(ticks)\n\n    return dst\n\n\ndef make_features(base, block):\n    stock_ids = set(base['stock_id'])\n    with timer('books'):\n        books = Parallel(n_jobs=-1)(delayed(make_book_feature)(i, block) for i in stock_ids)\n        book = pd.concat(books)\n\n    with timer('trades'):\n        trades = Parallel(n_jobs=-1)(delayed(make_trade_feature)(i, block) for i in stock_ids)\n        trade = pd.concat(trades)\n\n    with timer('extra features'):\n        df = pd.merge(base, book, on=['stock_id', 'time_id'], how='left')\n        df = pd.merge(df, trade, on=['stock_id', 'time_id'], how='left')\n        #df = make_extra_features(df)\n\n    return df\n\n\ndef make_features_v2(base, block):\n    stock_ids = set(base['stock_id'])\n    with timer('books(v2)'):\n        books = Parallel(n_jobs=-1)(delayed(make_book_feature_v2)(i, block) for i in stock_ids)\n        book_v2 = pd.concat(books)\n\n    d = pd.merge(base, book_v2, on=['stock_id', 'time_id'], how='left')\n    return d","e33fc813":"if USE_PRECOMPUTE_FEATURES:\n    with timer('load feather'):\n        df = pd.read_feather(os.path.join(DATA_DIR, 'optiver-df2', 'features_v2.f'))\nelse:\n    df = make_features(train, DataBlock.TRAIN)\n    # v2\n    df = make_features_v2(df, DataBlock.TRAIN)\n\ndf.to_feather('features_v2.f')  # save cache\n\ntest = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'test.csv'))\nif len(test) == 3:\n    print('is 1st stage')\n    IS_1ST_STAGE = True\n\nif IS_1ST_STAGE and MEMORY_TEST_MODE:\n    print('use copy of training data as test data to immitate 2nd stage RAM usage.')\n    test_df = df.iloc[:170000].copy()\n    test_df['time_id'] += 32767\n    test_df['row_id'] = ''\nelse:\n    test_df = make_features(test, DataBlock.TEST)\n    test_df = make_features_v2(test_df, DataBlock.TEST)\n\nprint(df.shape)\nprint(test_df.shape)\ndf = pd.concat([df, test_df.drop('row_id', axis=1)]).reset_index(drop=True)","5e814a69":"N_NEIGHBORS_MAX = 80\n\nclass Neighbors:\n    def __init__(self, \n                 name: str, \n                 pivot: pd.DataFrame, \n                 p: float, \n                 metric: str = 'minkowski', \n                 metric_params: Optional[Dict] = None, \n                 exclude_self: bool = False):\n        self.name = name\n        self.exclude_self = exclude_self\n        self.p = p\n        self.metric = metric\n        \n        if metric == 'random':\n            n_queries = len(pivot)\n            self.neighbors = np.random.randint(n_queries, size=(n_queries, N_NEIGHBORS_MAX))\n        else:\n            nn = NearestNeighbors(\n                n_neighbors=N_NEIGHBORS_MAX, \n                p=p, \n                metric=metric, \n                metric_params=metric_params\n            )\n            nn.fit(pivot)\n            _, self.neighbors = nn.kneighbors(pivot, return_distance=True)\n\n        self.columns = self.index = self.feature_values = self.feature_col = None\n\n    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n        raise NotImplementedError()\n\n    def make_nn_feature(self, n=5, agg=np.mean) -> pd.DataFrame:\n        assert self.feature_values is not None, \"should call rearrange_feature_values beforehand\"\n\n        start = 1 if self.exclude_self else 0\n\n        pivot_aggs = pd.DataFrame(\n            agg(self.feature_values[start:n,:,:], axis=0), \n            columns=self.columns, \n            index=self.index\n        )\n\n        dst = pivot_aggs.unstack().reset_index()\n        dst.columns = ['stock_id', 'time_id', f'{self.feature_col}_nn{n}_{self.name}_{agg.__name__}']\n        return dst\n\n\nclass TimeIdNeighbors(Neighbors):\n    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n        feature_pivot.head()\n\n        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n\n        for i in range(N_NEIGHBORS_MAX):\n            feature_values[i, :, :] += feature_pivot.values[self.neighbors[:, i], :]\n\n        self.columns = list(feature_pivot.columns)\n        self.index = list(feature_pivot.index)\n        self.feature_values = feature_values\n        self.feature_col = feature_col\n        \n    def __repr__(self) -> str:\n        return f\"time-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n\n\nclass StockIdNeighbors(Neighbors):\n    def rearrange_feature_values(self, df: pd.DataFrame, feature_col: str) -> None:\n        \"\"\"stock-id based nearest neighbor features\"\"\"\n        feature_pivot = df.pivot('time_id', 'stock_id', feature_col)\n        feature_pivot = feature_pivot.fillna(feature_pivot.mean())\n\n        feature_values = np.zeros((N_NEIGHBORS_MAX, *feature_pivot.shape))\n\n        for i in range(N_NEIGHBORS_MAX):\n            feature_values[i, :, :] += feature_pivot.values[:, self.neighbors[:, i]]\n\n        self.columns = list(feature_pivot.columns)\n        self.index = list(feature_pivot.index)\n        self.feature_values = feature_values\n        self.feature_col = feature_col\n        \n    def __repr__(self) -> str:\n        return f\"stock-id NN (name={self.name}, metric={self.metric}, p={self.p})\"\n","66f16446":"# the tau itself is meaningless for GBDT, but useful as input to aggregate in Nearest Neighbor features\ndf['trade.tau'] = np.sqrt(1 \/ df['trade.seconds_in_bucket.count'])\ndf['trade_150.tau'] = np.sqrt(1 \/ df['trade_150.seconds_in_bucket.count'])\ndf['book.tau'] = np.sqrt(1 \/ df['book.seconds_in_bucket.count'])\ndf['real_price'] = 0.01 \/ df['tick_size']","15c5b086":"time_id_neighbors: List[Neighbors] = []\nstock_id_neighbors: List[Neighbors] = []\n\nwith timer('knn fit'):\n    df_pv = df[['stock_id', 'time_id']].copy()\n    df_pv['price'] = 0.01 \/ df['tick_size']\n    df_pv['vol'] = df['book.log_return1.realized_volatility']\n    df_pv['trade.tau'] = df['trade.tau']\n    df_pv['trade.size.sum'] = df['book.total_volume.sum']\n\n    if USE_PRICE_NN_FEATURES:\n        pivot = df_pv.pivot('time_id', 'stock_id', 'price')\n        pivot = pivot.fillna(pivot.mean())\n        pivot = pd.DataFrame(minmax_scale(pivot))\n\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_price_c', \n                pivot, \n                p=2, \n                metric='canberra', \n                exclude_self=True\n            )\n        )\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_price_m', \n                pivot, \n                p=2, \n                metric='mahalanobis',\n                metric_params={'V':np.cov(pivot.values.T)}\n            )\n        )\n        stock_id_neighbors.append(\n            StockIdNeighbors(\n                'stock_price_l1', \n                minmax_scale(pivot.transpose()), \n                p=1, \n                exclude_self=True)\n        )\n\n    if USE_VOL_NN_FEATURES:\n        pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n        pivot = pivot.fillna(pivot.mean())\n        pivot = pd.DataFrame(minmax_scale(pivot))\n\n        time_id_neighbors.append(\n            TimeIdNeighbors('time_vol_l1', pivot, p=1)\n        )\n        stock_id_neighbors.append(\n            StockIdNeighbors(\n                'stock_vol_l1', \n                minmax_scale(pivot.transpose()), \n                p=1, \n                exclude_self=True\n            )\n        )\n\n    if USE_SIZE_NN_FEATURES:\n        pivot = df_pv.pivot('time_id', 'stock_id', 'trade.size.sum')\n        pivot = pivot.fillna(pivot.mean())\n        pivot = pd.DataFrame(minmax_scale(pivot))\n\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_size_m', \n                pivot, \n                p=2, \n                metric='mahalanobis', \n                metric_params={'V':np.cov(pivot.values.T)}\n            )\n        )\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_size_c', \n                pivot, \n                p=2, \n                metric='canberra'\n            )\n        )\n        \n    if USE_RANDOM_NN_FEATURES:\n        pivot = df_pv.pivot('time_id', 'stock_id', 'vol')\n        pivot = pivot.fillna(pivot.mean())\n        pivot = pd.DataFrame(minmax_scale(pivot))\n\n        time_id_neighbors.append(\n            TimeIdNeighbors(\n                'time_random', \n                pivot, \n                p=2, \n                metric='random'\n            )\n        )\n        stock_id_neighbors.append(\n            StockIdNeighbors(\n                'stock_random', \n                pivot.transpose(), \n                p=2,\n                metric='random')\n        )\n\n\nif not USE_TIME_ID_NN:\n    time_id_neighbors = []\n    \nif not USE_STOCK_ID_NN:\n    stock_id_neighbors = []","b32efbf1":"def calculate_rank_correraltion(neighbors, top_n=5):\n    if not neighbors:\n        return\n    neighbor_indices = pd.DataFrame()\n    for n in neighbors:\n        neighbor_indices[n.name] = n.neighbors[:,:top_n].flatten()\n\n    sns.heatmap(neighbor_indices.corr('kendall'), annot=True)","ee5ca3af":"time_ids = np.array(sorted(df['time_id'].unique()))\nfor neighbor in time_id_neighbors:\n    print(neighbor)\n    display(\n        pd.DataFrame(\n            time_ids[neighbor.neighbors[:,:10]], \n            index=pd.Index(time_ids, name='time_id'), \n            columns=[f'top_{i+1}' for i in range(10)]\n        ).iloc[1:6]\n    )","5f8c504c":"stock_ids = np.array(sorted(df['stock_id'].unique()))\nfor neighbor in stock_id_neighbors:\n    print(neighbor)\n    display(\n        pd.DataFrame(\n            stock_ids[neighbor.neighbors[:,:10]], \n            index=pd.Index(stock_ids, name='stock_id'), \n            columns=[f'top_{i+1}' for i in range(10)]\n        ).loc[64]\n    )\n    ","b1a46a15":"calculate_rank_correraltion(time_id_neighbors)","e0947b8c":"calculate_rank_correraltion(stock_id_neighbors)","9401f31b":"# features with large changes over time are converted to relative ranks within time-id\nif ENABLE_RANK_NORMALIZATION:\n    df['trade.order_count.mean'] = df.groupby('time_id')['trade.order_count.mean'].rank()\n    df['book.total_volume.sum']  = df.groupby('time_id')['book.total_volume.sum'].rank()\n    df['book.total_volume.mean'] = df.groupby('time_id')['book.total_volume.mean'].rank()\n    df['book.total_volume.std']  = df.groupby('time_id')['book.total_volume.std'].rank()\n\n    df['trade.tau'] = df.groupby('time_id')['trade.tau'].rank()\n\n    for dt in [150, 300, 450]:\n        df[f'book_{dt}.total_volume.sum']  = df.groupby('time_id')[f'book_{dt}.total_volume.sum'].rank()\n        df[f'book_{dt}.total_volume.mean'] = df.groupby('time_id')[f'book_{dt}.total_volume.mean'].rank()\n        df[f'book_{dt}.total_volume.std']  = df.groupby('time_id')[f'book_{dt}.total_volume.std'].rank()\n        df[f'trade_{dt}.order_count.mean'] = df.groupby('time_id')[f'trade_{dt}.order_count.mean'].rank()","cbdaa903":"def make_nearest_neighbor_feature(df: pd.DataFrame) -> pd.DataFrame:\n    df2 = df.copy()\n    print(df2.shape)\n\n    feature_cols_stock = {\n        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n        'trade.seconds_in_bucket.count': [np.mean],\n        'trade.tau': [np.mean],\n        'trade_150.tau': [np.mean],\n        'book.tau': [np.mean],\n        'trade.size.sum': [np.mean],\n        'book.seconds_in_bucket.count': [np.mean],\n    }\n    \n    feature_cols = {\n        'book.log_return1.realized_volatility': [np.mean, np.min, np.max, np.std],\n        'real_price': [np.max, np.mean, np.min],\n        'trade.seconds_in_bucket.count': [np.mean],\n        'trade.tau': [np.mean],\n        'trade.size.sum': [np.mean],\n        'book.seconds_in_bucket.count': [np.mean],\n        'trade_150.tau_nn20_stock_vol_l1_mean': [np.mean],\n        'trade.size.sum_nn20_stock_vol_l1_mean': [np.mean],\n    }\n\n    time_id_neigbor_sizes = [3, 5, 10, 20, 40]\n    time_id_neigbor_sizes_vol = [2, 3, 5, 10, 20, 40]\n    stock_id_neighbor_sizes = [10, 20, 40]\n\n    ndf: Optional[pd.DataFrame] = None\n\n    def _add_ndf(ndf: Optional[pd.DataFrame], dst: pd.DataFrame) -> pd.DataFrame:\n        if ndf is None:\n            return dst\n        else:\n            ndf[dst.columns[-1]] = dst[dst.columns[-1]].astype(np.float32)\n            return ndf\n\n    # neighbor stock_id\n    for feature_col in feature_cols_stock.keys():\n        try:\n            if feature_col not in df2.columns:\n                print(f\"column {feature_col} is skipped\")\n                continue\n\n            if not stock_id_neighbors:\n                continue\n\n            for nn in stock_id_neighbors:\n                nn.rearrange_feature_values(df2, feature_col)\n\n            for agg in feature_cols_stock[feature_col]:\n                for n in stock_id_neighbor_sizes:\n                    try:\n                        for nn in stock_id_neighbors:\n                            dst = nn.make_nn_feature(n, agg)\n                            ndf = _add_ndf(ndf, dst)\n                    except Exception:\n                        print_trace('stock-id nn')\n                        pass\n        except Exception:\n            print_trace('stock-id nn')\n            pass\n\n    if ndf is not None:\n        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n    ndf = None\n\n    print(df2.shape)\n\n    # neighbor time_id\n    for feature_col in feature_cols.keys():\n        try:\n            if not USE_PRICE_NN_FEATURES and feature_col == 'real_price':\n                continue\n            if feature_col not in df2.columns:\n                print(f\"column {feature_col} is skipped\")\n                continue\n\n            for nn in time_id_neighbors:\n                nn.rearrange_feature_values(df2, feature_col)\n\n            if 'volatility' in feature_col:\n                time_id_ns = time_id_neigbor_sizes_vol\n            else:\n                time_id_ns = time_id_neigbor_sizes\n\n            for agg in feature_cols[feature_col]:\n                for n in time_id_ns:\n                    try:\n                        for nn in time_id_neighbors:\n                            dst = nn.make_nn_feature(n, agg)\n                            ndf = _add_ndf(ndf, dst)\n                    except Exception:\n                        print_trace('time-id nn')\n                        pass\n        except Exception:\n            print_trace('time-id nn')\n\n    if ndf is not None:\n        df2 = pd.merge(df2, ndf, on=['time_id', 'stock_id'], how='left')\n\n    # features further derived from nearest neighbor features\n    try:\n        if USE_PRICE_NN_FEATURES:\n            for sz in time_id_neigbor_sizes:\n                denominator = f\"real_price_nn{sz}_time_price_c\"\n\n                df2[f'real_price_rankmin_{sz}']  = df2['real_price'] \/ df2[f\"{denominator}_amin\"]\n                df2[f'real_price_rankmax_{sz}']  = df2['real_price'] \/ df2[f\"{denominator}_amax\"]\n                df2[f'real_price_rankmean_{sz}'] = df2['real_price'] \/ df2[f\"{denominator}_mean\"]\n\n            for sz in time_id_neigbor_sizes_vol:\n                denominator = f\"book.log_return1.realized_volatility_nn{sz}_time_price_c\"\n\n                df2[f'vol_rankmin_{sz}'] = \\\n                    df2['book.log_return1.realized_volatility'] \/ df2[f\"{denominator}_amin\"]\n                df2[f'vol_rankmax_{sz}'] = \\\n                    df2['book.log_return1.realized_volatility'] \/ df2[f\"{denominator}_amax\"]\n\n        price_cols = [c for c in df2.columns if 'real_price' in c and 'rank' not in c]\n        for c in price_cols:\n            del df2[c]\n\n        if USE_PRICE_NN_FEATURES:\n            for sz in time_id_neigbor_sizes_vol:\n                tgt = f'book.log_return1.realized_volatility_nn{sz}_time_price_m_mean'\n                df2[f'{tgt}_rank'] = df2.groupby('time_id')[tgt].rank()\n    except Exception:\n        print_trace('nn features')\n\n    return df2","caa9d48a":"gc.collect()\n\nwith timer('make nearest neighbor feature'):\n    df2 = make_nearest_neighbor_feature(df)\n\nprint(df2.shape)\ndf2.reset_index(drop=True).to_feather('optiver_df2.f')\n\ngc.collect()","6c27325b":"# skew correction for NN\ncols_to_log = [\n    'trade.size.sum',\n    'trade_150.size.sum',\n    'trade_300.size.sum',\n    'trade_450.size.sum',\n    'volume_imbalance'\n]\nfor c in df2.columns:\n    for check in cols_to_log:\n        try:\n            if check in c:\n                df2[c] = np.log(df2[c]+1)\n                break\n        except Exception:\n            print_trace('log1p')","3b1fcad3":"# Rolling average of RV for similar trading volume\ntry:\n    df2.sort_values(by=['stock_id', 'book.total_volume.sum'], inplace=True)\n    df2.reset_index(drop=True, inplace=True)\n    \n    roll_target = 'book.log_return1.realized_volatility'\n\n    for window_size in [3, 10]:\n        df2[f'realized_volatility_roll{window_size}_by_book.total_volume.mean'] = \\\n            df2.groupby('stock_id')[roll_target].rolling(window_size, center=True, min_periods=1) \\\n                                                .mean() \\\n                                                .reset_index() \\\n                                                .sort_values(by=['level_1'])[roll_target].values\nexcept Exception:\n    print_trace('mean RV')","859affe0":"# stock-id embedding (helps little)\ntry:\n    lda_n = 3\n    lda = LatentDirichletAllocation(n_components=lda_n, random_state=0)\n\n    stock_id_emb = pd.DataFrame(\n        lda.fit_transform(pivot.transpose()), \n        index=df_pv.pivot('time_id', 'stock_id', 'vol').columns\n    )\n\n    for i in range(lda_n):\n        df2[f'stock_id_emb{i}'] = df2['stock_id'].map(stock_id_emb[i])\nexcept Exception:\n    print_trace('LDA')","899ec295":"df_train = df2[~df2.target.isnull()].copy()\ndf_test = df2[df2.target.isnull()].copy()\ndel df2, df_pv\ngc.collect()","dd4b1de8":"%matplotlib inline\n\n@contextmanager\ndef timer(name):\n    s = time.time()\n    yield\n    e = time.time() - s\n    print(f\"[{name}] {e:.3f}sec\")\n    \n\ndef calc_price2(df):\n    tick = sorted(np.diff(sorted(np.unique(df.values.flatten()))))[0]\n    return 0.01 \/ tick\n\n\ndef calc_prices(r):\n    df = pd.read_parquet(r.book_path, columns=['time_id', 'ask_price1', 'ask_price2', 'bid_price1', 'bid_price2'])\n    df = df.set_index('time_id')\n    df = df.groupby(level='time_id').apply(calc_price2).to_frame('price').reset_index()\n    df['stock_id'] = r.stock_id\n    return df\n\n\ndef sort_manifold(df, clf):\n    df_ = df.set_index('time_id')\n    df_ = pd.DataFrame(minmax_scale(df_.fillna(df_.mean())))\n\n    X_compoents = clf.fit_transform(df_)\n\n    dft = df.reindex(np.argsort(X_compoents[:,0])).reset_index(drop=True)\n    return np.argsort(X_compoents[:, 0]), X_compoents\n\n\ndef reconstruct_time_id_order():\n    with timer('load files'):\n        df_files = pd.DataFrame(\n            {'book_path': glob.glob('\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/**\/*.parquet')}) \\\n            .eval('stock_id = book_path.str.extract(\"stock_id=(\\d+)\").astype(\"int\")', engine='python')\n\n    with timer('calc prices'):\n        df_prices = pd.concat(Parallel(n_jobs=4, verbose=51)(delayed(calc_prices)(r) for _, r in df_files.iterrows()))\n        df_prices = df_prices.pivot('time_id', 'stock_id', 'price')\n        df_prices.columns = [f'stock_id={i}' for i in df_prices.columns]\n        df_prices = df_prices.reset_index(drop=False)\n\n    with timer('t-SNE(400) -> 50'):\n        clf = TSNE(n_components=1, perplexity=400, random_state=0, n_iter=2000)\n        order, X_compoents = sort_manifold(df_prices, clf)\n\n        clf = TSNE(n_components=1, perplexity=50, random_state=0, init=X_compoents, n_iter=2000, method='exact')\n        order, X_compoents = sort_manifold(df_prices, clf)\n\n        df_ordered = df_prices.reindex(order).reset_index(drop=True)\n        if df_ordered['stock_id=61'].iloc[0] > df_ordered['stock_id=61'].iloc[-1]:\n            df_ordered = df_ordered.reindex(df_ordered.index[::-1]).reset_index(drop=True)\n\n    # AMZN\n    plt.plot(df_ordered['stock_id=61'])\n    \n    return df_ordered[['time_id']]","b430b55e":"if CV_SPLIT == 'time':\n    with timer('calculate order of time-id'):\n        if USE_PRECOMPUTE_FEATURES:\n            timeid_order = pd.read_csv(os.path.join(DATA_DIR, 'optiver-time-id-ordered', 'time_id_order.csv'))\n        else:\n            timeid_order = reconstruct_time_id_order()\n\n    with timer('make folds'):\n        timeid_order['time_id_order'] = np.arange(len(timeid_order))\n        df_train['time_id_order'] = df_train['time_id'].map(timeid_order.set_index('time_id')['time_id_order'])\n        df_train = df_train.sort_values(['time_id_order', 'stock_id']).reset_index(drop=True)\n\n        folds_border = [3830 - 383*4, 3830 - 383*3, 3830 - 383*2, 3830 - 383*1]\n        time_id_orders = df_train['time_id_order']\n\n        folds = []\n        for i, border in enumerate(folds_border):\n            idx_train = np.where(time_id_orders < border)[0]\n            idx_valid = np.where((border <= time_id_orders) & (time_id_orders < border + 383))[0]\n            folds.append((idx_train, idx_valid))\n\n            print(f\"folds{i}: train={len(idx_train)}, valid={len(idx_valid)}\")\n\n    del df_train['time_id_order']\nelif CV_SPLIT == 'group':\n    gkf = GroupKFold(n_splits=4)\n    folds = []\n\n    for i, (idx_train, idx_valid) in enumerate(gkf.split(df_train, None, groups=df_train['time_id'])):\n        folds.append((idx_train, idx_valid))\nelse:\n    raise ValueError()\n\ndf_train.reset_index(drop=True, inplace=True)\ndf_test.reset_index(drop=True, inplace=True)","575882ec":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\n\n# from: https:\/\/blog.amedama.jp\/entry\/lightgbm-cv-feature-importance\ndef plot_importance(cvbooster, figsize=(10, 10)):\n    raw_importances = cvbooster.feature_importance(importance_type='gain')\n    feature_name = cvbooster.boosters[0].feature_name()\n    importance_df = pd.DataFrame(data=raw_importances,\n                                 columns=feature_name)\n    # order by average importance across folds\n    sorted_indices = importance_df.mean(axis=0).sort_values(ascending=False).index\n    sorted_importance_df = importance_df.loc[:, sorted_indices]\n    # plot top-n\n    PLOT_TOP_N = 50\n    plot_cols = sorted_importance_df.columns[:PLOT_TOP_N]\n    _, ax = plt.subplots(figsize=figsize)\n    ax.grid()\n    ax.set_xscale('log')\n    ax.set_ylabel('Feature')\n    ax.set_xlabel('Importance')\n    sns.boxplot(data=sorted_importance_df[plot_cols],\n                orient='h',\n                ax=ax)\n    plt.show()\n\n\ndef get_X(df_src):\n    cols = [c for c in df_src.columns if c not in ['time_id', 'target', 'tick_size']]\n    return df_src[cols]\n\n\nclass EnsembleModel:\n    def __init__(self, models: List[lgb.Booster], weights: Optional[List[float]] = None):\n        self.models = models\n        self.weights = weights\n\n        features = list(self.models[0].feature_name())\n\n        for m in self.models[1:]:\n            assert features == list(m.feature_name())\n\n    def predict(self, x):\n        predicted = np.zeros((len(x), len(self.models)))\n\n        for i, m in enumerate(self.models):\n            w = self.weights[i] if self.weights is not None else 1\n            predicted[:, i] = w * m.predict(x)\n\n        ttl = np.sum(self.weights) if self.weights is not None else len(self.models)\n        return np.sum(predicted, axis=1) \/ ttl\n\n    def feature_name(self) -> List[str]:\n        return self.models[0].feature_name()","a0e5e387":"lr = GBDT_LR\nif SHORTCUT_GBDT_IN_1ST_STAGE and IS_1ST_STAGE:\n    # to save GPU quota\n    lr = 0.3\n\nparams = {\n    'objective': 'regression',\n    'verbose': 0,\n    'metric': '',\n    'reg_alpha': 5,\n    'reg_lambda': 5,\n    'min_data_in_leaf': 1000,\n    'max_depth': -1,\n    'num_leaves': 128,\n    'colsample_bytree': 0.3,\n    'learning_rate': lr\n}\n\nX = get_X(df_train)\ny = df_train['target']\nX.to_feather('X.f')\ndf_train[['target']].to_feather('y.f')\n\ngc.collect()\n\nprint(X.shape)\n\nif PREDICT_GBDT:\n    ds = lgb.Dataset(X, y, weight=1\/np.power(y, 2))\n\n    with timer('lgb.cv'):\n        ret = lgb.cv(params, ds, num_boost_round=8000, folds=folds, #cv,\n                     feval=feval_RMSPE, stratified=False, \n                     return_cvbooster=True, verbose_eval=20,\n                     early_stopping_rounds=int(40*0.1\/lr))\n\n        print(f\"# overall RMSPE: {ret['RMSPE-mean'][-1]}\")\n\n    best_iteration = len(ret['RMSPE-mean'])\n    for i in range(len(folds)):\n        y_pred = ret['cvbooster'].boosters[i].predict(X.iloc[folds[i][1]], num_iteration=best_iteration)\n        y_true = y.iloc[folds[i][1]]\n        print(f\"# fold{i} RMSPE: {rmspe(y_true, y_pred)}\")\n        \n        if i == len(folds) - 1:\n            np.save('pred_gbdt.npy', y_pred)\n\n    plot_importance(ret['cvbooster'], figsize=(10, 20))\n\n    boosters = []\n    with timer('retraining'):\n        for i in range(GBDT_NUM_MODELS):\n            params['seed'] = i\n            boosters.append(lgb.train(params, ds, num_boost_round=int(1.1*best_iteration)))\n\n    booster = EnsembleModel(boosters)\n    del ret\n    del ds\n\ngc.collect()","6beaf5a0":"import gc\nimport os\nimport random\nfrom typing import List, Tuple, Optional, Union\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\nfrom joblib import Parallel, delayed\nfrom sklearn.decomposition import PCA\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n\n\nnull_check_cols = [\n    'book.log_return1.realized_volatility',\n    'book_150.log_return1.realized_volatility',\n    'book_300.log_return1.realized_volatility',\n    'book_450.log_return1.realized_volatility',\n    'trade.log_return.realized_volatility',\n    'trade_150.log_return.realized_volatility',\n    'trade_300.log_return.realized_volatility',\n    'trade_450.log_return.realized_volatility'\n]\n\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef rmspe_metric(y_true, y_pred):\n    rmspe = np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n    return rmspe\n\n\ndef rmspe_loss(y_true, y_pred):\n    rmspe = torch.sqrt(torch.mean(torch.square((y_true - y_pred) \/ y_true)))\n    return rmspe\n\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        return np.sqrt(np.mean(np.square((y_true - y_score) \/ y_true)))\n\ndef RMSPELoss_Tabnet(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) \/ y_true) ** 2 )).clone()\n\n\nclass AverageMeter:\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\nclass TabularDataset(Dataset):\n    def __init__(self, x_num: np.ndarray, x_cat: np.ndarray, y: Optional[np.ndarray]):\n        super().__init__()\n        self.x_num = x_num\n        self.x_cat = x_cat\n        self.y = y\n\n    def __len__(self):\n        return len(self.x_num)\n\n    def __getitem__(self, idx):\n        if self.y is None:\n            return self.x_num[idx], torch.LongTensor(self.x_cat[idx])\n        else:\n            return self.x_num[idx], torch.LongTensor(self.x_cat[idx]), self.y[idx]\n\n\nclass MLP(nn.Module):\n    def __init__(self,\n                 src_num_dim: int,\n                 n_categories: List[int],\n                 dropout: float = 0.0,\n                 hidden: int = 50,\n                 emb_dim: int = 10,\n                 dropout_cat: float = 0.2,\n                 bn: bool = False):\n        super().__init__()\n\n        self.embs = nn.ModuleList([\n            nn.Embedding(x, emb_dim) for x in n_categories])\n        self.cat_dim = emb_dim * len(n_categories)\n        self.dropout_cat = nn.Dropout(dropout_cat)\n\n        if bn:\n            self.sequence = nn.Sequential(\n                nn.Linear(src_num_dim + self.cat_dim, hidden),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(hidden),\n                nn.ReLU(),\n                nn.Linear(hidden, hidden),\n                nn.Dropout(dropout),\n                nn.BatchNorm1d(hidden),\n                nn.ReLU(),\n                nn.Linear(hidden, 1)\n            )\n        else:\n            self.sequence = nn.Sequential(\n                nn.Linear(src_num_dim + self.cat_dim, hidden),\n                nn.Dropout(dropout),\n                nn.ReLU(),\n                nn.Linear(hidden, hidden),\n                nn.Dropout(dropout),\n                nn.ReLU(),\n                nn.Linear(hidden, 1)\n            )\n\n    def forward(self, x_num, x_cat):\n        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n        x_all = torch.cat([x_num, x_cat_emb], 1)\n        x = self.sequence(x_all)\n        return torch.squeeze(x)\n\n\nclass CNN(nn.Module):\n    def __init__(self,\n                 num_features: int,\n                 hidden_size: int,\n                 n_categories: List[int],\n                 emb_dim: int = 10,\n                 dropout_cat: float = 0.2,\n                 channel_1: int = 256,\n                 channel_2: int = 512,\n                 channel_3: int = 512,\n                 dropout_top: float = 0.1,\n                 dropout_mid: float = 0.3,\n                 dropout_bottom: float = 0.2,\n                 weight_norm: bool = True,\n                 two_stage: bool = True,\n                 celu: bool = True,\n                 kernel1: int = 5,\n                 leaky_relu: bool = False):\n        super().__init__()\n\n        num_targets = 1\n\n        cha_1_reshape = int(hidden_size \/ channel_1)\n        cha_po_1 = int(hidden_size \/ channel_1 \/ 2)\n        cha_po_2 = int(hidden_size \/ channel_1 \/ 2 \/ 2) * channel_3\n\n        self.cat_dim = emb_dim * len(n_categories)\n        self.cha_1 = channel_1\n        self.cha_2 = channel_2\n        self.cha_3 = channel_3\n        self.cha_1_reshape = cha_1_reshape\n        self.cha_po_1 = cha_po_1\n        self.cha_po_2 = cha_po_2\n        self.two_stage = two_stage\n\n        self.expand = nn.Sequential(\n            nn.BatchNorm1d(num_features + self.cat_dim),\n            nn.Dropout(dropout_top),\n            nn.utils.weight_norm(nn.Linear(num_features + self.cat_dim, hidden_size), dim=None),\n            nn.CELU(0.06) if celu else nn.ReLU()\n        )\n\n        def _norm(layer, dim=None):\n            return nn.utils.weight_norm(layer, dim=dim) if weight_norm else layer\n\n        self.conv1 = nn.Sequential(\n            nn.BatchNorm1d(channel_1),\n            nn.Dropout(dropout_top),\n            _norm(nn.Conv1d(channel_1, channel_2, kernel_size=kernel1, stride=1, padding=kernel1 \/\/ 2, bias=False)),\n            nn.ReLU(),\n            nn.AdaptiveAvgPool1d(output_size=cha_po_1),\n            nn.BatchNorm1d(channel_2),\n            nn.Dropout(dropout_top),\n            _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n            nn.ReLU()\n        )\n\n        if self.two_stage:\n            self.conv2 = nn.Sequential(\n                nn.BatchNorm1d(channel_2),\n                nn.Dropout(dropout_mid),\n                _norm(nn.Conv1d(channel_2, channel_2, kernel_size=3, stride=1, padding=1, bias=True)),\n                nn.ReLU(),\n                nn.BatchNorm1d(channel_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Conv1d(channel_2, channel_3, kernel_size=5, stride=1, padding=2, bias=True)),\n                nn.ReLU()\n            )\n\n        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n\n        self.flt = nn.Flatten()\n\n        if leaky_relu:\n            self.dense = nn.Sequential(\n                nn.BatchNorm1d(cha_po_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Linear(cha_po_2, num_targets), dim=0),\n                nn.LeakyReLU()\n            )\n        else:\n            self.dense = nn.Sequential(\n                nn.BatchNorm1d(cha_po_2),\n                nn.Dropout(dropout_bottom),\n                _norm(nn.Linear(cha_po_2, num_targets), dim=0)\n            )\n\n        self.embs = nn.ModuleList([nn.Embedding(x, emb_dim) for x in n_categories])\n        self.cat_dim = emb_dim * len(n_categories)\n        self.dropout_cat = nn.Dropout(dropout_cat)\n\n    def forward(self, x_num, x_cat):\n        embs = [embedding(x_cat[:, i]) for i, embedding in enumerate(self.embs)]\n        x_cat_emb = self.dropout_cat(torch.cat(embs, 1))\n        x = torch.cat([x_num, x_cat_emb], 1)\n\n        x = self.expand(x)\n\n        x = x.reshape(x.shape[0], self.cha_1, self.cha_1_reshape)\n\n        x = self.conv1(x)\n\n        if self.two_stage:\n            x = self.conv2(x) * x\n\n        x = self.max_po_c2(x)\n        x = self.flt(x)\n        x = self.dense(x)\n\n        return torch.squeeze(x)\n\n\ndef preprocess_nn(\n        X: pd.DataFrame,\n        scaler: Optional[StandardScaler] = None,\n        scaler_type: str = 'standard',\n        n_pca: int = -1,\n        na_cols: bool = True):\n    if na_cols:\n        #for c in X.columns:\n        for c in null_check_cols:\n            if c in X.columns:\n                X[f\"{c}_isnull\"] = X[c].isnull().astype(int)\n\n    cat_cols = [c for c in X.columns if c in ['time_id', 'stock_id']]\n    num_cols = [c for c in X.columns if c not in cat_cols]\n\n    X_num = X[num_cols].values.astype(np.float32)\n    X_cat = np.nan_to_num(X[cat_cols].values.astype(np.int32))\n\n    def _pca(X_num_):\n        if n_pca > 0:\n            pca = PCA(n_components=n_pca, random_state=0)\n            return pca.fit_transform(X_num)\n        return X_num\n\n    if scaler is None:\n        scaler = StandardScaler()\n        X_num = scaler.fit_transform(X_num)\n        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n        return _pca(X_num), X_cat, cat_cols, scaler\n    else:\n        X_num = scaler.transform(X_num) #TODO: inf\u3067\u3082\u5927\u4e08\u592b\uff1f\n        X_num = np.nan_to_num(X_num, posinf=0, neginf=0)\n        return _pca(X_num), X_cat, cat_cols\n\n\ndef train_epoch(data_loader: DataLoader,\n                model: nn.Module,\n                optimizer,\n                scheduler,\n                device,\n                clip_grad: float = 1.5):\n    model.train()\n    losses = AverageMeter()\n    step = 0\n\n    for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Training'):\n        batch_size = x_num.size(0)\n        x_num = x_num.to(device, dtype=torch.float)\n        x_cat = x_cat.to(device)\n        y = y.to(device, dtype=torch.float)\n\n        loss = rmspe_loss(y, model(x_num, x_cat))\n        losses.update(loss.detach().cpu().numpy(), batch_size)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad)\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if scheduler is not None:\n            scheduler.step()\n\n        step += 1\n\n    return losses.avg\n\n\ndef evaluate(data_loader: DataLoader, model, device):\n    model.eval()\n\n    losses = AverageMeter()\n\n    final_targets = []\n    final_outputs = []\n\n    with torch.no_grad():\n        for x_num, x_cat, y in tqdm(data_loader, position=0, leave=True, desc='Evaluating'):\n            batch_size = x_num.size(0)\n            x_num = x_num.to(device, dtype=torch.float)\n            x_cat = x_cat.to(device)\n            y = y.to(device, dtype=torch.float)\n\n            with torch.no_grad():\n                output = model(x_num, x_cat)\n\n            loss = rmspe_loss(y, output)\n            # record loss\n            losses.update(loss.detach().cpu().numpy(), batch_size)\n\n            targets = y.detach().cpu().numpy()\n            output = output.detach().cpu().numpy()\n\n            final_targets.append(targets)\n            final_outputs.append(output)\n\n    final_targets = np.concatenate(final_targets)\n    final_outputs = np.concatenate(final_outputs)\n\n    try:\n        metric = rmspe_metric(final_targets, final_outputs)\n    except:\n        metric = None\n\n    return final_outputs, final_targets, losses.avg, metric\n\n\ndef predict_nn(X: pd.DataFrame,\n               model: Union[List[MLP], MLP],\n               scaler: StandardScaler,\n               device,\n               ensemble_method='mean'):\n    if not isinstance(model, list):\n        model = [model]\n\n    for m in model:\n        m.eval()\n    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n    valid_dataset = TabularDataset(X_num, X_cat, None)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset,\n                                               batch_size=512,\n                                               shuffle=False,\n                                               num_workers=4)\n\n    final_outputs = []\n\n    with torch.no_grad():\n        for x_num, x_cat in tqdm(valid_loader, position=0, leave=True, desc='Evaluating'):\n            x_num = x_num.to(device, dtype=torch.float)\n            x_cat = x_cat.to(device)\n\n            outputs = []\n            with torch.no_grad():\n                for m in model:\n                    output = m(x_num, x_cat)\n                    outputs.append(output.detach().cpu().numpy())\n\n            if ensemble_method == 'median':\n                pred = np.nanmedian(np.array(outputs), axis=0)\n            else:\n                pred = np.array(outputs).mean(axis=0)\n            final_outputs.append(pred)\n\n    final_outputs = np.concatenate(final_outputs)\n    return final_outputs\n\n\ndef predict_tabnet(X: pd.DataFrame,\n                   model: Union[List[TabNetRegressor], TabNetRegressor],\n                   scaler: StandardScaler,\n                   ensemble_method='mean'):\n    if not isinstance(model, list):\n        model = [model]\n\n    X_num, X_cat, cat_cols = preprocess_nn(X.copy(), scaler=scaler)\n    X_processed = np.concatenate([X_cat, X_num], axis=1)\n\n    predicted = []\n    for m in model:\n        predicted.append(m.predict(X_processed))\n\n    if ensemble_method == 'median':\n        pred = np.nanmedian(np.array(predicted), axis=0)\n    else:\n        pred = np.array(predicted).mean(axis=0)\n\n    return pred\n\n\ndef train_tabnet(X: pd.DataFrame,\n                 y: pd.DataFrame,\n                 folds: List[Tuple],\n                 batch_size: int = 1024,\n                 lr: float = 1e-3,\n                 model_path: str = 'fold_{}.pth',\n                 scaler_type: str = 'standard',\n                 output_dir: str = 'artifacts',\n                 epochs: int = 250,\n                 seed: int = 42,\n                 n_pca: int = -1,\n                 na_cols: bool = True,\n                 patience: int = 10,\n                 factor: float = 0.5,\n                 gamma: float = 2.0,\n                 lambda_sparse: float = 8.0,\n                 n_steps: int = 2,\n                 scheduler_type: str = 'cosine',\n                 n_a: int = 16):\n    seed_everything(seed)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    y = y.values.astype(np.float32)\n    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n\n    best_losses = []\n    best_predictions = []\n\n    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n        y_tr, y_va = y[train_idx], y[valid_idx]\n        y_tr = y_tr.reshape(-1,1)\n        y_va = y_va.reshape(-1,1)\n        X_tr = np.concatenate([X_tr_cat, X_tr], axis=1)\n        X_va = np.concatenate([X_va_cat, X_va], axis=1)\n\n        cat_idxs = [0]\n        cat_dims = [128]\n\n        if scheduler_type == 'cosine':\n            scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False)\n            scheduler_fn = CosineAnnealingWarmRestarts\n        else:\n            scheduler_params = {'mode': 'min', 'min_lr': 1e-7, 'patience': patience, 'factor': factor, 'verbose': True}\n            scheduler_fn = torch.optim.lr_scheduler.ReduceLROnPlateau\n\n        model = TabNetRegressor(\n            cat_idxs=cat_idxs,\n            cat_dims=cat_dims,\n            cat_emb_dim=1,\n            n_d=n_a,\n            n_a=n_a,\n            n_steps=n_steps,\n            gamma=gamma,\n            n_independent=2,\n            n_shared=2,\n            lambda_sparse=lambda_sparse,\n            optimizer_fn=torch.optim.Adam,\n            optimizer_params={'lr': lr},\n            mask_type=\"entmax\",\n            scheduler_fn=scheduler_fn,\n            scheduler_params=scheduler_params,\n            seed=seed,\n            verbose=10\n            #device_name=device,\n            #clip_value=1.5\n        )\n\n        model.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], max_epochs=epochs, patience=50, batch_size=1024*20,\n                  virtual_batch_size=batch_size, num_workers=4, drop_last=False, eval_metric=[RMSPE], loss_fn=RMSPELoss_Tabnet)\n\n        path = os.path.join(output_dir, model_path.format(cv_idx))\n        model.save_model(path)\n\n        predicted = model.predict(X_va)\n\n        rmspe = rmspe_metric(y_va, predicted)\n        best_losses.append(rmspe)\n        best_predictions.append(predicted)\n\n    return best_losses, best_predictions, scaler, model\n\n\ndef train_nn(X: pd.DataFrame,\n             y: pd.DataFrame,\n             folds: List[Tuple],\n             device,\n             emb_dim: int = 25,\n             batch_size: int = 1024,\n             model_type: str = 'mlp',\n             mlp_dropout: float = 0.0,\n             mlp_hidden: int = 64,\n             mlp_bn: bool = False,\n             cnn_hidden: int = 64,\n             cnn_channel1: int = 32,\n             cnn_channel2: int = 32,\n             cnn_channel3: int = 32,\n             cnn_kernel1: int = 5,\n             cnn_celu: bool = False,\n             cnn_weight_norm: bool = False,\n             dropout_emb: bool = 0.0,\n             lr: float = 1e-3,\n             weight_decay: float = 0.0,\n             model_path: str = 'fold_{}.pth',\n             scaler_type: str = 'standard',\n             output_dir: str = 'artifacts',\n             scheduler_type: str = 'onecycle',\n             optimizer_type: str = 'adam',\n             max_lr: float = 0.01,\n             epochs: int = 30,\n             seed: int = 42,\n             n_pca: int = -1,\n             batch_double_freq: int = 50,\n             cnn_dropout: float = 0.1,\n             na_cols: bool = True,\n             cnn_leaky_relu: bool = False,\n             patience: int = 8,\n             factor: float = 0.5):\n    seed_everything(seed)\n\n    os.makedirs(output_dir, exist_ok=True)\n\n    y = y.values.astype(np.float32)\n    X_num, X_cat, cat_cols, scaler = preprocess_nn(X.copy(), scaler_type=scaler_type, n_pca=n_pca, na_cols=na_cols)\n\n    best_losses = []\n    best_predictions = []\n\n    for cv_idx, (train_idx, valid_idx) in enumerate(folds):\n        X_tr, X_va = X_num[train_idx], X_num[valid_idx]\n        X_tr_cat, X_va_cat = X_cat[train_idx], X_cat[valid_idx]\n        y_tr, y_va = y[train_idx], y[valid_idx]\n\n        cur_batch = batch_size\n        best_loss = 1e10\n        best_prediction = None\n\n        print(f\"fold {cv_idx} train: {X_tr.shape}, valid: {X_va.shape}\")\n\n        train_dataset = TabularDataset(X_tr, X_tr_cat, y_tr)\n        valid_dataset = TabularDataset(X_va, X_va_cat, y_va)\n        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=cur_batch, shuffle=True,\n                                                   num_workers=4)\n        valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=cur_batch, shuffle=False,\n                                                   num_workers=4)\n\n        if model_type == 'mlp':\n            model = MLP(X_tr.shape[1],\n                        n_categories=[128],\n                        dropout=mlp_dropout, hidden=mlp_hidden, emb_dim=emb_dim,\n                        dropout_cat=dropout_emb, bn=mlp_bn)\n        elif model_type == 'cnn':\n            model = CNN(X_tr.shape[1],\n                        hidden_size=cnn_hidden,\n                        n_categories=[128],\n                        emb_dim=emb_dim,\n                        dropout_cat=dropout_emb,\n                        channel_1=cnn_channel1,\n                        channel_2=cnn_channel2,\n                        channel_3=cnn_channel3,\n                        two_stage=False,\n                        kernel1=cnn_kernel1,\n                        celu=cnn_celu,\n                        dropout_top=cnn_dropout,\n                        dropout_mid=cnn_dropout,\n                        dropout_bottom=cnn_dropout,\n                        weight_norm=cnn_weight_norm,\n                        leaky_relu=cnn_leaky_relu)\n        else:\n            raise NotImplementedError()\n        model = model.to(device)\n\n        if optimizer_type == 'adamw':\n            opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n        elif optimizer_type == 'adam':\n            opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n        else:\n            raise NotImplementedError()\n\n        scheduler = epoch_scheduler = None\n        if scheduler_type == 'onecycle':\n            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, pct_start=0.1, div_factor=1e3,\n                                                            max_lr=max_lr, epochs=epochs,\n                                                            steps_per_epoch=len(train_loader))\n        elif scheduler_type == 'reduce':\n            epoch_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=opt,\n                                                                         mode='min',\n                                                                         min_lr=1e-7,\n                                                                         patience=patience,\n                                                                         verbose=True,\n                                                                         factor=factor)\n\n        for epoch in range(epochs):\n            if epoch > 0 and epoch % batch_double_freq == 0:\n                cur_batch = cur_batch * 2\n                print(f'batch: {cur_batch}')\n                train_loader = torch.utils.data.DataLoader(train_dataset,\n                                                           batch_size=cur_batch,\n                                                           shuffle=True,\n                                                           num_workers=4)\n            train_loss = train_epoch(train_loader, model, opt, scheduler, device)\n            predictions, valid_targets, valid_loss, rmspe = evaluate(valid_loader, model, device=device)\n            print(f\"epoch {epoch}, train loss: {train_loss:.3f}, valid rmspe: {rmspe:.3f}\")\n\n            if epoch_scheduler is not None:\n                epoch_scheduler.step(rmspe)\n\n            if rmspe < best_loss:\n                print(f'new best:{rmspe}')\n                best_loss = rmspe\n                best_prediction = predictions\n                torch.save(model, os.path.join(output_dir, model_path.format(cv_idx)))\n\n        best_predictions.append(best_prediction)\n        best_losses.append(best_loss)\n        del model, train_dataset, valid_dataset, train_loader, valid_loader, X_tr, X_va, X_tr_cat, X_va_cat, y_tr, y_va, opt\n        if scheduler is not None:\n            del scheduler\n        gc.collect()\n\n    return best_losses, best_predictions, scaler\n","ceddc4ce":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\ndel df, df_train\ngc.collect()\n\ndef get_top_n_models(models, scores, top_n):\n    if len(models) <= top_n:\n        print('number of models are less than top_n. all models will be used')\n        return models\n    sorted_ = [(y, x) for y, x in sorted(zip(scores, models), key=lambda pair: pair[0])]\n    print(f'scores(sorted): {[y for y, _ in sorted_]}')\n    return [x for _, x in sorted_][:top_n]\n\n\nif PREDICT_MLP:\n    model_paths = []\n    scores = []\n    \n    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n        print('shortcut to save quota...')\n        epochs = 3\n        valid_th = 100\n    else:\n        epochs = 30\n        valid_th = NN_VALID_TH\n    \n    for i in range(NN_NUM_MODELS):\n        # MLP\n        nn_losses, nn_preds, scaler = train_nn(X, y, \n                                               [folds[-1]], \n                                               device=device, \n                                               batch_size=512,\n                                               mlp_bn=True,\n                                               mlp_hidden=256,\n                                               mlp_dropout=0.0,\n                                               emb_dim=30,\n                                               epochs=epochs,\n                                               lr=0.002,\n                                               max_lr=0.0055,\n                                               weight_decay=1e-7,\n                                               model_path='mlp_fold_{}' + f\"_seed{i}.pth\",\n                                               seed=i)\n        if nn_losses[0] < NN_VALID_TH:\n            print(f'model of seed {i} added.')\n            scores.append(nn_losses[0])\n            model_paths.append(f'artifacts\/mlp_fold_0_seed{i}.pth')\n            np.save(f'pred_mlp_seed{i}.npy', nn_preds[0])\n\n    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n    mlp_model = [torch.load(path, device) for path in model_paths]\n    print(f'total {len(mlp_model)} models will be used.')\nif PREDICT_CNN:\n    model_paths = []\n    scores = []\n        \n    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n        print('shortcut to save quota...')\n        epochs = 3\n        valid_th = 100\n    else:\n        epochs = 50\n        valid_th = NN_VALID_TH\n\n    for i in range(NN_NUM_MODELS):\n        nn_losses, nn_preds, scaler = train_nn(X, y, \n                                               [folds[-1]], \n                                               device=device, \n                                               cnn_hidden=8*128,\n                                               batch_size=1280,\n                                               model_type='cnn',\n                                               emb_dim=30,\n                                               epochs=epochs, #epochs,\n                                               cnn_channel1=128,\n                                               cnn_channel2=3*128,\n                                               cnn_channel3=3*128,\n                                               lr=0.00038, #0.0011,\n                                               max_lr=0.0013,\n                                               weight_decay=6.5e-6,\n                                               optimizer_type='adam',\n                                               scheduler_type='reduce',\n                                               model_path='cnn_fold_{}' + f\"_seed{i}.pth\",\n                                               seed=i,\n                                               cnn_dropout=0.0,\n                                               cnn_weight_norm=True,\n                                               cnn_leaky_relu=False,\n                                               patience=8,\n                                               factor=0.3)\n        if nn_losses[0] < valid_th:\n            model_paths.append(f'artifacts\/cnn_fold_0_seed{i}.pth')\n            scores.append(nn_losses[0])\n            np.save(f'pred_cnn_seed{i}.npy', nn_preds[0])\n            \n    model_paths = get_top_n_models(model_paths, scores, NN_MODEL_TOP_N)\n    cnn_model = [torch.load(path, device) for path in model_paths]\n    print(f'total {len(cnn_model)} models will be used.')\n    \nif PREDICT_TABNET:\n    tab_model = []\n    scores = []\n        \n    if SHORTCUT_NN_IN_1ST_STAGE and IS_1ST_STAGE:\n        print('shortcut to save quota...')\n        epochs = 10\n        valid_th = 1000\n    else:\n        print('train full')\n        epochs = 250\n        valid_th = NN_VALID_TH\n\n    for i in range(TABNET_NUM_MODELS):\n        nn_losses, nn_preds, scaler, model = train_tabnet(X, y,  \n                                                          [folds[-1]], \n                                                          batch_size=1280,\n                                                          epochs=epochs, #epochs,\n                                                          lr=0.04,\n                                                          patience=50,\n                                                          factor=0.5,\n                                                          gamma=1.6,\n                                                          lambda_sparse=3.55e-6,\n                                                          seed=i,\n                                                          n_a=36)\n        if nn_losses[0] < valid_th:\n            tab_model.append(model)\n            scores.append(nn_losses[0])\n            np.save(f'pred_tab_seed{i}.npy', nn_preds[0])\n            model.save_model(f'artifacts\/tabnet_fold_0_seed{i}')\n            \n    tab_model = get_top_n_models(tab_model, scores, TAB_MODEL_TOP_N)\n    print(f'total {len(tab_model)} models will be used.')","ae84526a":"del X, y\ngc.collect()","290d50dd":"X_test = get_X(df_test)\nprint(X_test.shape)","6fcbda7b":"df_pred = pd.DataFrame()\ndf_pred['row_id'] = df_test['stock_id'].astype(str) + '-' + df_test['time_id'].astype(str)\n\npredictions = {}\n\nprediction_weights = {}\n\nif PREDICT_GBDT:\n    gbdt_preds = booster.predict(X_test)\n    predictions['gbdt'] = gbdt_preds\n    prediction_weights['gbdt'] = 4\n\n\nif PREDICT_MLP and mlp_model:\n    try:\n        mlp_preds = predict_nn(X_test, mlp_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n        print(f'mlp: {mlp_preds.shape}')\n        predictions['mlp'] = mlp_preds\n        prediction_weights['mlp'] = 1\n    except:\n        print(f'failed to predict mlp: {traceback.format_exc()}')\n\n\nif PREDICT_CNN and cnn_model:\n    try:\n        cnn_preds = predict_nn(X_test, cnn_model, scaler, device, ensemble_method=ENSEMBLE_METHOD)\n        print(f'cnn: {cnn_preds.shape}')\n        predictions['cnn'] = cnn_preds\n        prediction_weights['cnn'] = 4\n    except:\n        print(f'failed to predict cnn: {traceback.format_exc()}')\n\n\nif PREDICT_TABNET and tab_model:\n    try:\n        tab_preds = predict_tabnet(X_test, tab_model, scaler, ensemble_method=ENSEMBLE_METHOD).flatten()\n        print(f'tab: {tab_preds.shape}')\n        predictions['tab'] = tab_preds\n        prediction_weights['tab'] = 1\n    except:\n        print(f'failed to predict tab: {traceback.format_exc()}')\n\n        \noverall_preds = None\noverall_weight = np.sum(list(prediction_weights.values()))\n\nprint(f'prediction will be made by: {list(prediction_weights.keys())}')\n\nfor name, preds in predictions.items():\n    w = prediction_weights[name] \/ overall_weight\n    if overall_preds is None:\n        overall_preds = preds * w\n    else:\n        overall_preds += preds * w\n        \ndf_pred['target'] = np.clip(overall_preds, 0, None)\n","ba6a6671":"sub = pd.read_csv(os.path.join(DATA_DIR, 'optiver-realized-volatility-prediction', 'sample_submission.csv'))\nsubmission = pd.merge(sub[['row_id']], df_pred[['row_id', 'target']], how='left')\nsubmission['target'] = submission['target'].fillna(0)\nsubmission.to_csv('submission.csv', index=False)","230594f3":"## Feature Engineering\n\n### Base Features","2dc67ea2":"## Inference","1249ba09":"#### Check Neighbor Indices","7ade7881":"#### Aggregate Features With Neighbors","ee33e8ad":"## NN Training","faca7bcb":"## LightGBM Training","4801ea8b":"### Nearest-Neighbor Features","527e044c":"### Misc Features","b5ef30ee":"#### Build Nearest Neighbors","810c5660":"## Reverse Engineering time-id Order & Make CV Split"}}