{"cell_type":{"94739bae":"code","d73afc5c":"code","47f0753d":"code","73fae46d":"code","4faf99f0":"code","c8f756b8":"code","596f2db7":"code","29432373":"code","8e57110f":"code","dad9c9fd":"code","e4a57dad":"code","bf195622":"code","7a9b8f91":"code","ce509f8b":"code","95042fa9":"code","3e3c66c8":"code","40184ed1":"code","853a6df7":"code","d34bbaf3":"code","3843df79":"code","47b7c9a4":"code","c9d4ac8a":"code","f47e3bd1":"code","39d65b63":"code","e289b242":"code","97a61027":"code","c8431950":"code","d700ae2e":"code","f19b8837":"code","8249c216":"code","814cda1f":"code","c7c75dab":"code","8cb6fd10":"code","b26b7a63":"code","2c6a04ce":"code","4d5a0e8b":"code","74828709":"code","7e42cc0a":"code","d9439e0f":"code","ccb7bc22":"code","4fad3d72":"code","d28e04f0":"code","e0695625":"code","40cfcc99":"code","7a168aad":"code","d8970f3c":"code","d0840629":"code","e7077c2e":"code","f4530099":"code","b7d8a688":"code","71583d91":"code","c80d0093":"code","17a50668":"code","7051833f":"code","02f6625c":"markdown","2ef2ea7a":"markdown","c6ffde84":"markdown","4fec2f05":"markdown","863ac40b":"markdown","9783a283":"markdown","9901dd8d":"markdown","03b729b8":"markdown","12d312f7":"markdown","530ee11e":"markdown","30ddc45b":"markdown","4ca4bd65":"markdown","203aa1a9":"markdown","7a9f1c3c":"markdown","03d29c88":"markdown","3a0e0045":"markdown","a72dc6e4":"markdown","cc958ab6":"markdown","2e76e55b":"markdown","9135502a":"markdown","a3bf9a4e":"markdown","f58d0984":"markdown"},"source":{"94739bae":"import json\nimport requests\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout, LSTM\nfrom sklearn import metrics ","d73afc5c":"data= pd.read_csv('..\/input\/priceprediction\/new.csv')\ndata.head(2)","47f0753d":"data=data.set_index('time')\ndata.head(2)","73fae46d":"data.isnull().sum()","4faf99f0":"type(data.index[0])","c8f756b8":"data.index=pd.to_datetime(data.index)\ndata.head(2)","596f2db7":"data.to_csv('new.csv')","29432373":"def train_test_split(df, test_size):\n    split = df.shape[0] - int(test_size * df.shape[0])\n    train_set = df.iloc[:split]\n    test_set = df.iloc[split:]\n    return train_set, test_set\n\ntrain_set, test_set =train_test_split(data, 0.2)    #checked test size 0.2 but the result for 0.3 is better\nprint('train_set.shape: ', train_set.shape)\nprint('test_set.shape: ', test_set.shape)","8e57110f":"plt.figure(figsize=(13,7))\ntrain_set['close'].plot(color='b')\ntest_set['close'].plot(color='r')\nplt.xlabel('Time', fontsize=14)\nplt.ylabel('Close Price ', fontsize=14)\nplt.legend(['Train', 'Test'], loc='best',fontsize=14 )\nplt.show()","dad9c9fd":"def zero_scaling(df):\n    \n    return df \/ df.iloc[0] - 1\n\n\n\ndef sliding_window(df, len_window, zero):\n    \n    window = []\n    for a in range(df.shape[0] - len_window):\n        sub = df[a: (a + len_window)].copy()\n        if zero:\n            sub = zero_scaling(sub)\n        window.append(sub.values)\n    return np.array(window)\n\n\n\ndef prepare_data(df, column, len_window, zero):\n    \n    train_data = train_set[[column]]\n    test_data = test_set[[column]]\n        \n    X_train = sliding_window(train_data, len_window, zero)  \n    X_test = sliding_window(test_data, len_window, zero) \n\n    y_train = train_data[column][len_window:].values\n    y_test = test_data[column][len_window:].values\n\n    if zero:\n        y_train = y_train \/ train_data[column][:-len_window].values - 1\n        y_test = y_test \/ test_data[column][:-len_window].values - 1\n\n    return train_data,  test_data, X_train, X_test,  y_train, y_test","e4a57dad":"train_data, test_data, X_train, X_test,  y_train, y_test = prepare_data(data, 'close', len_window=5, zero=True)","bf195622":"X_train.shape","7a9b8f91":"model = Sequential()\n                                   \n#use  input_shape (tuple of integers) when using this layer as the first layer in a model\n\nmodel.add(LSTM(units=100, input_shape=(X_train.shape[1], X_train.shape[2])) )  \nmodel.add(Dropout(0.2))\n\n\nmodel.add(Dense(units=1 ))  # tedade noroun ha\nmodel.add(Activation('linear'))  #activation ra besoorate layer joda add konim natije behtar ast\n\nmodel.compile(loss='mse', optimizer='adam')","ce509f8b":"# optional: from tensorflow.keras.callbacks import EarlyStopping\n# optional:  es=EarlyStopping(monitor='val_loss', mode='min', patience=25, verbose=1)\n# optional: model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1, shuffle=True ,validation_data=(X_validation, y_validation), callbacks=[es])","95042fa9":"# Fitting to the training set\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1 ,validation_data=(X_test, y_test))  #DEFAULT shuffle=True","3e3c66c8":"pd.DataFrame(model.history.history).plot(figsize=(8,6))\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Training Loss Per Epoch', fontsize=14)\nplt.show()","40184ed1":"prediction=model.predict(X_test).squeeze()  # use squeeze to convert to 1d array ","853a6df7":"assert (len(prediction)==len(y_test))","d34bbaf3":"plt.figure(figsize=(8,5))\nplt.plot(y_test, y_test, color='b')\nplt.scatter(y_test, prediction, color='r')\nplt.xlabel('y_test', fontsize=12)\nplt.ylabel('Prediction', fontsize=12)\nplt.title('Close Price Prediction, 2 Layer Model, zero scaling', fontsize=14)\nplt.show()","3843df79":"print('Mean Absolute Error: ', metrics.mean_absolute_error(y_test, prediction))","47b7c9a4":"predicted_close_price= pd.DataFrame(data=(prediction + 1) * (test_data['close'][:-5].values) ,  index=test_data[5:].index ,columns=['predicted_close_price'] )\npredicted_close_price","c9d4ac8a":"merged=pd.merge(test_data, predicted_close_price, on='time', how='left')\nmerged","f47e3bd1":"merged.isnull().sum()","39d65b63":"plt.figure(figsize=(13,7))\n\nmerged['close'][5:].plot(color='r')\nmerged['predicted_close_price'][5:].plot(color='g')\nplt.title('Close Price Prediction, 2 Layer Model, Zero Scaling',fontsize=16)\nplt.xlabel('Time', fontsize=13)\nplt.ylabel('Close Price', fontsize=13)\nplt.legend(['Actual ClosePrice', 'Predicted Close Price'], loc='best',fontsize=13)\nplt.show()","e289b242":"#size of the data we use to predict should always be at least one unit bigger than window_len\nfrom random import randint\n\ndef rand(len_window, df):\n    return randint(len_window + 1 , df.shape[0])","97a61027":"random_shape=rand(5, data)\nrandom_shape","c8431950":"new=data[['close']].iloc[0:  random_shape]\nsliding_window(new, 5, True);\nprediction=model.predict(sliding_window(new, 5, True)).squeeze()\nassert(len(prediction)==len( new['close'][:-5]))\npredicted_close_price= pd.DataFrame(data=(prediction + 1) * (new['close'][:-5].values) ,  index=new[5:].index ,columns=['predicted close'] )\npd.merge(new, predicted_close_price, on='time', how='left')[5:]","d700ae2e":"# The LSTM architecture\nmodel = Sequential()\n\n# First LSTM layer with Dropout regularisation\n#default activation` == `tanh`\n#default recurrent_activation == sigmoid.\n #return_sequences: Boolean. Whether to return the last output.Default: `False`.\n\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1],X_train.shape[2])))\nmodel.add(Dropout(0.2))\n\n# Second LSTM layer\nmodel.add(LSTM(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\n\n# Third LSTM layer\nmodel.add(LSTM(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\n\n\n# Fourth LSTM layer\nmodel.add(LSTM(units=50))\nmodel.add(Dropout(0.2))\n\n\n# The output layer\nmodel.add(Dense(units=1))\n\n\n# Compiling the RNN\nmodel.compile(optimizer='rmsprop',loss='mean_squared_error')","f19b8837":"# Fitting to the training set\nmodel.fit(X_train,y_train,epochs=30,batch_size=32)","8249c216":"pd.DataFrame(model.history.history).plot(figsize=(8,6))\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Training Loss Per Epoch', fontsize=14)\nplt.show()","814cda1f":"prediction=model.predict(X_test).squeeze()  # use squeeze to convert to 1d array \n\nassert (len(prediction)==len(y_test))","c7c75dab":"plt.figure(figsize=(8,5))\nplt.plot(y_test, y_test, color='b')\nplt.scatter(y_test, prediction, color='r')\nplt.xlabel('y_test', fontsize=12)\nplt.ylabel('Prediction', fontsize=12)\nplt.title('Close Price Prediction, 6 Layer Model, zero scaling', fontsize=14)\nplt.show()","8cb6fd10":"print('Mean Absolute Error: ', metrics.mean_absolute_error(y_test, prediction))","b26b7a63":"predicted_close_price= pd.DataFrame(data=(prediction + 1) * (test_data['close'][:-5].values) ,  index=test_data[5:].index ,columns=['predicted_close_price'] )\npredicted_close_price;\n\nmerged=pd.merge(test_data, predicted_close_price, on='time', how='left')\nmerged","2c6a04ce":"merged.isnull().sum()","4d5a0e8b":"plt.figure(figsize=(13,7))\n\nmerged['close'][5:].plot(color='r')\nmerged['predicted_close_price'][5:].plot(color='g')\nplt.title('Close Price Prediction, 6 Layer Model, Zero Scaling',fontsize=16)\nplt.xlabel('Time', fontsize=13)\nplt.ylabel('Close Price', fontsize=13)\nplt.legend(['Actual ClosePrice', 'Predicted Close Price'], loc='best',fontsize=13)\nplt.show()","74828709":"train_data = train_set[['close']]\ntest_data = test_set[['close']]\n\ntrain_data_values=train_data.values\ntest_data_values=test_data.values","7e42cc0a":"#Scaling\/Normalizing the whole Training set\nsc = MinMaxScaler(feature_range=(0,1))\ntrain_data_values_scaled = sc.fit_transform(train_data_values)","d9439e0f":"# Since LSTMs store long term memory state, we create a data structure with 5 timesteps and 1 output\n# So for each element of training set, we have 5 previous training set elements \n\nX_train = []\ny_train = []\nfor i in range(5,train_data_values.shape[0]):\n    X_train.append(train_data_values_scaled[i-5:i,0])  #window up to\n    y_train.append(train_data_values_scaled[i,0])        #one value after the window\nX_train, y_train = np.array(X_train), np.array(y_train)","ccb7bc22":"print(X_train.shape)\nprint(y_train.shape)","4fad3d72":"# Reshaping X_train for efficient modelling\nX_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_train.shape","d28e04f0":"model = Sequential()\n                                   \n#use input_shape (tuple of integers) when using this layer as the first layer in a model\n\nmodel.add(LSTM(units=100, input_shape=(X_train.shape[1], X_train.shape[2])) )  \nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(units=1 ))  \nmodel.add(Activation('linear'))  \n\nmodel.compile(loss='mse', optimizer='adam')","e0695625":"# Fitting to the training set\nmodel.fit(X_train,y_train,epochs=30,batch_size=32)","40cfcc99":"pd.DataFrame(model.history.history).plot(figsize=(8,6))\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('Loss', fontsize=12)\nplt.title('Training Loss Per Epoch', fontsize=14)\nplt.show()","7a168aad":"test_data_values_scaled = sc.fit_transform(test_data_values)  # we only do transfrom on test set not fit","d8970f3c":"X_test = []\ny_test = []\nfor i in range(5,test_set.shape[0]):\n    X_test.append(test_data_values_scaled[i-5:i,0])  #yek panjereh ja tayi ta sare\n    y_test.append(test_data_values_scaled[i,0])        #tak element bade panjere\nX_test, y_test = np.array(X_test), np.array(y_test)\n\nX_test.shape","d0840629":"X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)\n\nX_test.shape","e7077c2e":"prediction=model.predict(X_test)  #do not use squeeze, otherwise will get error in inverse scaler\nassert (len(prediction)==len(y_test))","f4530099":"plt.figure(figsize=(8,5))\nplt.plot(y_test, y_test, color='b')\nplt.scatter(y_test, prediction, color='r')\nplt.xlabel('y_test', fontsize=12)\nplt.ylabel('Prediction', fontsize=12)\nplt.title('Close Price Prediction, 2 Layer Model, MinMax scaling', fontsize=14)\nplt.show()","b7d8a688":"print('Mean Absolute Error: ', metrics.mean_absolute_error(y_test, prediction))","71583d91":"predicted_close_price = sc.inverse_transform(prediction)\n\npredicted_close_price= pd.DataFrame(data= predicted_close_price,  index=test_set[5:].index ,columns=['predicted_close_price'] )\npredicted_close_price","c80d0093":"merged=pd.merge(test_data, predicted_close_price, on='time', how='left')\nmerged","17a50668":"merged.isnull().sum()","7051833f":"plt.figure(figsize=(13,7))\n\nmerged['close'][5:].plot(color='r')\nmerged['predicted_close_price'][5:].plot(color='g')\nplt.title('Close Price Prediction, 2 Layer Model, MinMax Scaling',fontsize=16)\nplt.xlabel('Time', fontsize=13)\nplt.ylabel('Close Price', fontsize=13)\nplt.legend(['Actual ClosePrice', 'Predicted Close Price'], loc='best',fontsize=13)\nplt.show()","02f6625c":"# 6- Splitting Train & Test set","2ef2ea7a":"Now let\u2019s plot the cryptocurrency prices in Canadian dollars as a function of time using the below code:","c6ffde84":"# 13- Conclusion","4fec2f05":"# 10- Building LSTM model- 2 Layers","863ac40b":"Next, I made a couple of functions to normalize the values. Normalization is a technique often applied as part of data preparation for machine learning. The goal of normalization is to change the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values.\n\nNext, I made a function to extract data of windows which are of size 5 each as shown in the code below:\n\nI continued with making a function to prepare the data in a format to be later fed into the neural network. I used the same concept of splitting the data into two sets \u2014 training set and test set with 80% and 20% data respectively as shown in the code below:`    \n    ","9783a283":"# 3- Set time as index","9901dd8d":"Based on the Mean Absolute Error values and Close Price Prediction plots, the 2-layer predictive model executed on the data normalized by zero_scaling function has the best performance.","03b729b8":"I have used Canadian exchange rate and stored the real time data into a pandas data-frame. I used to_datetime() method to convert string Date time into Python Date time object. This is necessary as Date time objects in the file are read as a string object. ","12d312f7":"# <center >Cryptocurrency Price Prediction Using Deep Learning <center >","530ee11e":"# 11- Predicting on Brand new data","30ddc45b":"### 12-1-  LSTM model- 6 Layers + Zero scaling","4ca4bd65":"# 5- Set time as index","203aa1a9":"We can observe that there is a clear dip in prices between Jan 2020 and Oct 2020. The prices keep on increasing from Oct 2020  to Apr 2021  with fluctuations happening in the months of Jan, Feb, Mar, Apr. ","7a9f1c3c":"# 9- Normalizing Data- Zero Scaling","03d29c88":"# 4- Data cleaning","3a0e0045":"## 2- Getting real-time crptocurrency data","a72dc6e4":"It works by using special gates to allow each LSTM layer to take information from both previous layers and the current layer. The data goes through multiple gates (like forget gate, input gate, etc.) and various activation functions (like the tanh function, relu function) and is passed through the LSTM cells. The main advantage of this is that it allows each LSTM cell to remember patterns for a certain amount of time. The thing to be noted is that LSTM can remember important information and at the same time forget irrelevant information.\n\nNow let\u2019s build the model. Sequential model is used for stacking all the layers (input, hidden and output). The neural network comprises of a LSTM layer followed by 20% Dropout layer and a Dense layer with linear activation function. I complied the model using Adam as the optimizer and Mean Squared Error as the loss function.","cc958ab6":"### 12-2- LSTM model- 2 Layers + MinMaxScaler","2e76e55b":"## 1- Imports","9135502a":"# 7- Plotting train & test sets","a3bf9a4e":"Next, I split the data into two sets \u2014 training set and test set with 80% and 20% data respectively. The decision made here is just for the purpose of this tutorial. In real projects, you should always split your data into training, validation, testing (like 60%, 20%, 20%).","f58d0984":"# 12- Backup Scenarios"}}