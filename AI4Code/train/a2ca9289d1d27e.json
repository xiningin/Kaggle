{"cell_type":{"1bcc39a8":"code","9fee38f3":"code","626e9e41":"code","6157b3ec":"code","149e63fa":"code","c57ffada":"code","d893b5d4":"code","74062ae2":"code","d785dbd6":"code","6b0edbc5":"code","21675c95":"code","7100f2df":"code","848fd6a3":"code","9e80c2ce":"code","5be2291f":"code","bc2eff4c":"code","216c7779":"code","193493a2":"code","9dcba66f":"code","269b2055":"code","f79128c9":"code","ecefc5e1":"code","d1d0a6c4":"code","7ce28ff6":"code","941bf72c":"code","d1b921a6":"code","b26c4982":"code","ad9fdd24":"code","e740965a":"code","c0c16000":"code","5b874aea":"code","1cea29e1":"code","732771e9":"code","6ef5182d":"code","c7753e98":"markdown","c3a07cbf":"markdown","c203be00":"markdown","d4c824e0":"markdown","560f20ce":"markdown","1a51987b":"markdown","b98d2ac6":"markdown","6cebb490":"markdown","5a346979":"markdown","e0ed4324":"markdown","c1d5c97a":"markdown","f9370f64":"markdown","80fa8767":"markdown","b3c1a7b2":"markdown","99858081":"markdown","24abe4ce":"markdown","6fdf4358":"markdown","241c2844":"markdown","79821cae":"markdown","5748608d":"markdown","3fda4267":"markdown","6aefce44":"markdown","7c0c4837":"markdown","dbd912b9":"markdown","ac3c3cc2":"markdown","eb05a6f2":"markdown","2b531690":"markdown","631a59f3":"markdown","fe83d8b9":"markdown","df14319e":"markdown","3900ad1c":"markdown"},"source":{"1bcc39a8":"#Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.widgets import Slider, Button, RadioButtons\n\nfrom sklearn import preprocessing, svm\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold, KFold, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, confusion_matrix, classification_report\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\nimport xgboost as xgb\n\n%matplotlib inline","9fee38f3":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    \nos.chdir(\"..\/input\/pima-indians-diabetes-database\/\")","626e9e41":"#Let's upload the csv\ndiabetes = pd.read_csv(\"diabetes.csv\")\nprint(diabetes.head())","6157b3ec":"def background_check(dataframe):\n  '''\n  Summary: This function serves as a way to have a basic understanding of the data before we do any work on it\n\n  Description: The purpose of this is too develop the underlying intution using pandas dataframes in order to\n               understand the data.  As Andrej Kapathy says, \"Become one with the data\"\n\n  Parameters: dataframe\n\n  Return: None, but it does print a bunch of data for us to read\n  '''\n  \n  print(\"#\" * 100)\n  print(dataframe.head())\n\n  print(\"#\" * 100)\n  print(dataframe.info())\n\n  print(\"#\" * 100)\n  print(dataframe.describe)\n\n  print(\"#\" * 100)\n  print(dataframe.tail())\n\n  print(\"#\" * 100)\n  print(dataframe.columns)\n\n  print(\"#\" * 100)\n  print(dataframe.dtypes)\n\n  print(\"#\" * 100)\n  print(dataframe.shape)\n\nbackground_check(diabetes)","149e63fa":"def correlations_with_sns():\n  '''\n  Summary: This will make a coorelation plot in seaborn for us\n\n  Description: This correlation will allow us to understand the underlying intuition behind what data has meaning on other data\n\n  Parameters: Non\n\n  Return: None, however it will make a useful graph for us\n  '''\n\n  corr = diabetes.corr(method = \"pearson\")\n\n  fig, ax = plt.subplots(figsize = (14, 5))\n\n  #Make a heatmap and then rotate the xticks to make it easier to read\n  sns_corr = sns.heatmap(corr, annot = True, fmt = '.1g', vmin = -1, vmax = 1, center = 0, cmap = \"coolwarm\", linecolor = \"black\", linewidths = 3, cbar_kws = {\"orientation\": \"vertical\"})\n  sns_corr.set_xticklabels(sns_corr.get_xticklabels(), rotation = 35)","c57ffada":"correlations_with_sns()","d893b5d4":"def countplots():\n  fig, ax_count = plt.subplots(nrows = 2, ncols = 2, figsize = (25, 20))\n\n  pregnancies_sns_countplot = sns.countplot(x = diabetes[\"Pregnancies\"], ax = ax_count[0][0], data = diabetes, hue = \"Outcome\")\n  ax_count[0][0].set_title(\"Pregnancy Count\")\n  pregnancies_sns_countplot.tick_params(axis = \"x\", which = \"major\")\n\n  skin_thickness_sns_countplot = sns.countplot(x = diabetes[\"SkinThickness\"], ax = ax_count[0][1], data = diabetes, hue = \"Outcome\")\n  ax_count[0][1].set_title(\"SkinThickness\")\n  skin_thickness_sns_countplot.tick_params(axis = \"x\", which = \"major\")\n \n  blood_pressure_sns_countplot = sns.countplot(x = diabetes[\"BloodPressure\"], ax = ax_count[1][0], data = diabetes, hue = \"Outcome\")\n  ax_count[1][0].set_title(\"BloodPressure\")\n  blood_pressure_sns_countplot.tick_params(axis = \"x\", which = \"major\")\n\n  age_sns_countplot = sns.countplot(x = diabetes[\"Age\"], ax = ax_count[1][1], data = diabetes, hue = \"Outcome\")\n  ax_count[1][1].set_title(\"Age\")\n  age_sns_countplot.tick_params(axis = \"x\", which = \"major\")\n  \n  fig.tight_layout()\n  plt.show()","74062ae2":"countplots()","d785dbd6":"'''\n  The glucose count plot was huge so I separated it\n'''\n\n#Glucose\nfig, ax_count_glucose = plt.subplots(nrows = 1, figsize = (60, 25))\n\nglucose_sns_countplot = sns.countplot(x = diabetes[\"Glucose\"], ax = ax_count_glucose, data = diabetes, hue = \"Outcome\")\nax_count_glucose.set_title(\"Glucose Count\")\nglucose_sns_countplot.tick_params(axis = \"x\", which = \"major\")\nglucose_sns_countplot.set_xticklabels(glucose_sns_countplot.get_xticklabels(), rotation = 45)\n\n\nfig.tight_layout()\nplt.show()","6b0edbc5":"'''\n  The insulin chart was pretty big so I made it flip axes to make it easier\n'''\n\n#Insulin\ninsulin_fig, ax_count_insulin = plt.subplots(nrows = 1, figsize = (15, 25))\n\ninsulin_sns_countplot = sns.countplot(y = diabetes[\"Insulin\"], ax = ax_count_insulin, data = diabetes, hue = \"Outcome\")\nax_count_insulin.set_title(\"Insulin Count\")\ninsulin_sns_countplot.tick_params(axis = \"x\", which = \"major\")\ninsulin_sns_countplot.set_xticklabels(insulin_sns_countplot.get_xticklabels(), rotation = 45)\n\ninsulin_fig.tight_layout()\nplt.show()","21675c95":"'''\n  The BMI chart was huge also so it is separate\n'''\n\n#BMI\nbmi_fig, ax_count_bmi = plt.subplots(nrows = 1, figsize = (60, 25))\n\nbmi_sns_countplot = sns.countplot(x = diabetes[\"BMI\"], ax = ax_count_bmi, data = diabetes, hue = \"Outcome\")\nax_count_bmi.set_title(\"BMI Count\")\nbmi_sns_countplot.tick_params(axis = \"x\", which = \"major\")\nbmi_sns_countplot.set_xticklabels(bmi_sns_countplot.get_xticklabels(), rotation = 45)\n\nbmi_fig.tight_layout()\nplt.show()","7100f2df":"def histograms():\n  '''\n  Summary: This will plot the data in histograms\n\n  Description: A histogram can be a great way to see the distribution of your data\n\n  Parameters: None\n\n  Return: None, but we do see a bunch of graphs\n  '''\n  fig, ax_histogram = plt.subplots(nrows = 4, ncols = 2, figsize = (10, 10))\n\n  pregnancies_histogram = sns.histplot(data = diabetes[\"Pregnancies\"], ax = ax_histogram[0][0])\n  ax_histogram[0][0].set_title(\"Pregnancies \/ Outcome\")\n  pregnancies_histogram.tick_params(axis = \"x\")\n\n  glucose_histogram = sns.histplot(data = diabetes[\"Glucose\"], ax = ax_histogram[0][1])\n  ax_histogram[0][1].set_title(\"Glucose \/ Outcome\")\n  glucose_histogram.tick_params(axis = \"x\")\n  \n  blood_pressure_histogram = sns.histplot(data = diabetes[\"BloodPressure\"], ax = ax_histogram[1][0])\n  ax_histogram[1][0].set_title(\"Blood Pressure \/ Outcome\")\n  blood_pressure_histogram.tick_params(axis = \"x\")\n\n  skin_thickness_histogram = sns.histplot(data = diabetes[\"SkinThickness\"], ax = ax_histogram[1][1])\n  ax_histogram[1][1].set_title(\"SkinThickness \/ Outcome\")\n  skin_thickness_histogram.tick_params(axis = \"x\")\n\n  insulin_histogram = sns.histplot(data = diabetes[\"Insulin\"], ax = ax_histogram[2][0])\n  ax_histogram[2][0].set_title(\"Insulin \/ Outcome\")\n  insulin_histogram.tick_params(axis = \"x\")\n\n  bmi_histogram = sns.histplot(data = diabetes[\"BMI\"], ax = ax_histogram[2][1])\n  ax_histogram[2][1].set_title(\"BMI \/ Outcome\")\n  bmi_histogram.tick_params(axis = \"x\")\n\n  dpf_histogram = sns.histplot(data = diabetes[\"DiabetesPedigreeFunction\"], ax = ax_histogram[3][0])\n  ax_histogram[3][0].set_title(\"DiabetesPedigreeFunction \/ Outcome\")\n  dpf_histogram.tick_params(axis = \"x\")\n\n  age_histogram = sns.histplot(data = diabetes[\"Age\"], ax = ax_histogram[3][1])\n  ax_histogram[3][1].set_title(\"Age \/ Outcome\")\n  age_histogram.tick_params(axis = \"x\")\n\n  fig.tight_layout()\n  plt.show()","848fd6a3":"histograms()","9e80c2ce":"def distplots():\n  '''\n  Summary: The distplot is used to see the distributions more clearly\n\n  Description: It is important to know what data needs to be normalized and standardized and this can show us\n\n  Parameters: None\n\n  Return: None, but we do see a lot graphs\n  '''\n\n  fig, ax_distplots = plt.subplots(nrows = 3, ncols = 3, figsize = (14, 6))\n  pregnancies_distplot = sns.distplot(diabetes[\"Pregnancies\"], ax = ax_distplots[0][0])\n\n  glucose_distplot = sns.distplot(diabetes[\"Glucose\"], ax = ax_distplots[0][1])\n\n  blood_pressure_distplot = sns.distplot(diabetes[\"BloodPressure\"], ax = ax_distplots[0][2])\n\n  skin_thickness_distplot = sns.distplot(diabetes[\"SkinThickness\"], ax = ax_distplots[1][0])\n\n  insulin_distplot = sns.distplot(diabetes[\"Insulin\"], ax = ax_distplots[1][1])\n\n  bmi_distplot = sns.distplot(diabetes[\"BMI\"], ax = ax_distplots[1][2])\n\n  dpf_distplot = sns.distplot(diabetes[\"DiabetesPedigreeFunction\"], ax = ax_distplots[2][0])\n\n  age_distplot = sns.distplot(diabetes[\"Age\"], ax = ax_distplots[2][1])\n  \n  fig.tight_layout()\n  plt.show()","5be2291f":"distplots()","bc2eff4c":"diabetes.describe()","216c7779":"#We can copy the dataset so we can feel free to do whatever we want because we can always make another copy\n\n#Let's call this diabetes_normalized so we can tell the normalized one from the original\ndiabetes_normalized = diabetes.copy()\n\ndef normalize():\n  '''\n  Summary: Quickly normalize the data\n\n  Description: We need to make our data on a 0 to 1 scale to make it easier to find patterns\n\n  Parameters: None\n\n  Returns: None, but we see the normalized columns\n  '''\n  column = 'Pregnancies'\n  diabetes_normalized[column] = diabetes_normalized[column] \/ diabetes[column].abs().max() \n\n  column = 'Glucose'\n  diabetes_normalized[column] = diabetes_normalized[column] \/ diabetes[column].abs().max() \n\n  column = 'BloodPressure'\n  diabetes_normalized[column] = diabetes_normalized[column] \/ diabetes[column].abs().max() \n\n  column = 'SkinThickness'\n  diabetes_normalized[column] = diabetes_normalized[column] \/ diabetes[column].abs().max() \n\n  column = 'Insulin'\n  diabetes_normalized[column] = diabetes_normalized[column] \/ diabetes[column].abs().max() \n\n  column = 'BMI'\n  diabetes_normalized[column] = diabetes_normalized[column] \/ diabetes[column].abs().max() \n\n  column = 'Age'\n  diabetes_normalized[column] = diabetes_normalized[column] \/ diabetes[column].abs().max() \n\n  column = 'DiabetesPedigreeFunction'\n  diabetes_normalized[column] = diabetes_normalized[column] \/ diabetes[column].abs().max() \n\n\n  print(diabetes_normalized.head())","193493a2":"normalize()","9dcba66f":"def splitting_data_into_train_and_test():\n  '''\n  Summary: This splits the data into train and test sets\n\n  Description: We can call this whenever we want to train models\n\n  Parameters: None\n\n  Return: The training set and test set\n  '''\n  X = diabetes_normalized[[\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\", \"SkinThickness\", \"DiabetesPedigreeFunction\", \"Age\"]]\n  y = diabetes_normalized[\"Outcome\"]\n\n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)\n\n  return X_train, X_test, y_train, y_test","269b2055":"def standardize():\n  '''\n  Summary: Standardize the data\n\n  Description: This will standardize the data for us\n\n  Parameters: None\n\n  Return: The scaled dataset\n  '''\n  X_train, X_test, y_train, y_test = splitting_data_into_train_and_test()\n\n  #The outcome is either a 1 or a 0 so we only need to scale the training data sample\n  scaler = StandardScaler().fit(X_train) \n  X_scaled = scaler.transform(X_train)\n  print(X_scaled)\n  return\n\nstandardize()","f79128c9":"def dense_network():\n  '''\n  Summary: This is the dense network\n\n  Description: Thanks to Tensorflow \/ Keras, I am going to make a small dense network with checkpoints to save it\n\n  Parameters: None\n\n  Return: The trained model and its history for plotting use\n  '''\n\n  X_train, X_test, y_train, y_test = splitting_data_into_train_and_test()\n\n  dense_model = tf.keras.models.Sequential([\n      tf.keras.layers.Input(shape = (768, 8)),                          \n      tf.keras.layers.Dense(768, activation = \"relu\"),\n      tf.keras.layers.Dropout(0.8),\n      tf.keras.layers.Dense(1, activation = \"sigmoid\")\n  ])\n\n  dense_model.compile(optimizer = tf.keras.optimizers.Adam(0.0001),\n                      loss = \"binary_crossentropy\",\n                      metrics = [\"accuracy\"])\n  \n  history = dense_model.fit(x = X_train,\n                            y = y_train,\n                            epochs = 1000,\n                            batch_size = 10,\n                            shuffle = True,\n                            validation_split = 0.2,\n                            verbose = 1\n                            )\n  \n  return history","ecefc5e1":"dense_history = dense_network()","d1d0a6c4":"def plot_dense_network():\n  '''\n  Summary: This will plot the results of our dense network\n\n  Description: This will plot the accuracy and loss of our training and testing sets\n\n  Parameters: None\n\n  Return: None, but we do see two graphs\n  '''\n  print(dense_history.history.keys())\n\n\n  plt.plot(dense_history.history[\"accuracy\"]) #Blue \n  plt.plot(dense_history.history[\"val_accuracy\"]) #Orange\n  plt.legend([\"Train\", \"Test\"], loc = \"lower right\")\n  plt.title(\"Model Accuracy\")\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Accuracy\")\n\n  plt.show()\n\n  plt.plot(dense_history.history[\"loss\"]) #Blue\n  plt.plot(dense_history.history[\"val_loss\"]) #Orange\n  plt.legend([\"Train\", \"Test\"], loc = \"lower left\")\n  plt.xlabel(\"Model Loss\")\n  plt.xlabel(\"Epoch\")\n  plt.ylabel(\"Loss\")\n\n  fig.tight_layout()\n  plt.show()","7ce28ff6":"plot_dense_network()","941bf72c":"def random_forest_grid():\n  '''\n  Summary: This is the random forest\n\n  Description: This is the grid search cv for the random forest.  The grid search\n               is a bit overkill but I wanted to do it.\n\n  Parameters: None\n\n  Return: None, but it prints the best parameters\n  '''\n  # Step 1\n  X_train, X_test, y_train, y_test = splitting_data_into_train_and_test()\n\n  #Step 2\n  random_forest_classifier = RandomForestClassifier()\n\n  #Step 3 - The range of parameters to sweep through\n  param_grid = {\n      \"n_estimators\": [1, 5000],\n      \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n      \"max_depth\"   : [1, 2, 3, 4, 5],\n      \"criterion\"   : [\"gini\", \"entropy\"]\n  }\n\n  #Step 4\n  cv_rf = GridSearchCV(estimator = random_forest_classifier, param_grid = param_grid, cv = 5)\n\n  cv_rf.fit(X_train, y_train)\n\n  print(cv_rf.best_params_)\n\ndef random_forest_with_best_params(n_estimators, max_depth):\n  '''\n  Summary: This is the random forest with the best parameters\n\n  Description: This is the random forest we will train with the best parameters from the before method\n\n  Parameters: n_estimators, max_depth\n\n  Return: The random forest model\n  '''\n  #Step 5 - Use the best parameters to update this\n  X_train, X_test, y_train, y_test = splitting_data_into_train_and_test()\n\n  random_forest_classifier = RandomForestClassifier()\n  random_forest_classifier = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth)\n  random_forest_classifier.fit(X_train, y_train)\n  pred = random_forest_classifier.predict(X_test)\n\n  #Step 6\n  print(\"Accuracy for Random Forest on CV_1 data: \", accuracy_score(y_test, pred))\n  print(\"Mean Absolute Error of CV_1: \", mean_absolute_error(y_test, pred))\n  return random_forest_classifier","d1b921a6":"random_forest_grid()","b26c4982":"random_forest_with_best_params(n_estimators = 5000, max_depth = 5)","ad9fdd24":"def logistic_reg():\n  '''\n  Summary: This is the logistic regression model\n\n  Description: This will show us how to use grid search on a logistic regression problem\n\n  Params: None\n\n  Return: None, but it prints the best parameters like our random forest\n  '''\n  #Step 1\n  X_train, X_test, y_train, y_test = splitting_data_into_train_and_test()\n  \n  #Step 2\n  param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n  \n  #Step 3\n  logistic_regression_grid_search = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)\n  \n  lr_gs = GridSearchCV(cv=None,\n                       estimator=LogisticRegression(C=10, \n                                            intercept_scaling=1, \n                                            dual=False, \n                                            fit_intercept=True,\n                                            penalty='l2',\n                                            tol=0.0001),\n                        param_grid= param_grid)\n  \n  lr_gs.fit(X_train, y_train)\n\n  print(lr_gs.best_params_)\n\ndef logistic_reg_with_best_params():\n  '''\n  Summary: This is the logistic regression with the best parameters\n\n  Description: This will show us the logistic regression model with the apparent best parameters.\n               Are there better parameters?  Probably.\n\n  Paramters: None\n\n  Return: The logistic regression model\n  '''\n\n  #Step 5 - Use the best parameters to update this\n  X_train, X_test, y_train, y_test = splitting_data_into_train_and_test()\n\n  logistic_regresion_with_best_params = LogisticRegression()\n\n  logistic_regression_with_best_params = LogisticRegression(C = 10)\n  logistic_regression_with_best_params.fit(X_train, y_train)\n  logistic_regression_with_best_params_preds = logistic_regression_with_best_params.predict(X_test)\n\n  #Step 6\n  print(\"Accuracy for Random Forest on CV_1 data: \", accuracy_score(y_test, logistic_regression_with_best_params_preds))\n  print(\"Mean Absolute Error of CV_1: \", mean_absolute_error(y_test, logistic_regression_with_best_params_preds))\n\n  return logistic_regression_with_best_params","e740965a":"logistic_reg()","c0c16000":"logistic_reg_with_best_params()","5b874aea":"def svm_model():\n  '''\n  Summary: This is the Support Vector Machine\n\n  Description: This is the Support Vector Machine with a grid search\n\n  Parameters: None\n\n  Return: None, but it does train our model in one method since it is faster\/\n  '''\n\n  #Step 1\n  X_train, X_test, y_train, y_test = splitting_data_into_train_and_test()\n\n  svm = SVC()\n\n  #Step 2\n  param_grid = {\n      \"C\": [0.1, 1, 10, 100, 1000],\n      'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n      \"kernel\": [\"rbf\"]\n  }\n\n  #Step 3\n  svm_grid = GridSearchCV(svm, \n                          param_grid, \n                          refit = True, \n                          verbose = 3)\n  \n  svm_grid.fit(X_train, y_train)\n\n  #Step 4\n  print(svm_grid.best_params_)\n  print(svm_grid.best_estimator_)\n\n  #Step 5\n  svm_grid_predictions = svm_grid.predict(X_test)\n\n  print(classification_report(y_test, svm_grid_predictions))","1cea29e1":"svm_model()","732771e9":"def xgboost():\n  '''\n  Summary: This is the XGBoost model\n\n  Description: This is the XGBoost model I made for this project.  I have never used XGBoost so I gave it a go.\n\n  Parameters: None:\n\n  Return: None, but it does print the results of the model in one function for us\n  '''\n  #Step 1\n  X_train, X_test, y_train, y_test = splitting_data_into_train_and_test()\n\n  #Step 2\n  xg_cl = xgb.XGBClassifier(objective = \"binary:logistic\",\n                            n_estimators = 500,\n                            max_depth = 5,                        \n                            shuffle = True)\n\n  xg_cl.fit(X_train, y_train)\n\n  #Step 3\n  xgboost_preds = xg_cl.predict(X_test)\n  accuracy = float(np.sum(xgboost_preds == y_test)) \/ y_test.shape[0]\n  mae = mean_absolute_error(y_test, xgboost_preds)\n\n  print(\"Accuracy: %f\" % (accuracy))\n  print(\"Mean Absolute Error: %f\" % (mae))\n\n  #Step 4\n  kfold = KFold(n_splits = 10, random_state = 7)\n  results = cross_val_score(xg_cl, X_train, y_train, cv = kfold)\n\n  print(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean() * 100, results.std() * 100))","6ef5182d":"#This outputs an error even though it give us the metrics.\nxgboost()","c7753e98":"#Make a few graphs shall we - EDA\n\nTypes of graphs to make\n  - Corr\n  - Countplot (https:\/\/www.kaggle.com\/jchen2186\/data-visualization-with-python-seaborn\")\n  - Distplot\n  - Histogram\n  - Line (plot)\n  - Scatter\n\n\n","c3a07cbf":"This dense model is actually pretty solid.  We have under 800 rows in the dataset so these graphs are decently strong.  Sweet!  Now let's try the next model, the random forest.","c203be00":"Ok this last one was a big one to see correctly and it is still somewhat bad, but we can not see all of the columns as countplots based on the outcome hue.\n\nI apologize for how big those charts were, but I wanted to display the count plots for as much as I could.","d4c824e0":"#Random Forest Classification\n\n- We will use the GridSearchCV functionality here.  Why?  Well, it can let us do a hyperparameter sweep to see the best parameters for the RF.\n\n- Okay so what is this code?\n  - (Step 1) The first step is to call our splitting data function \n  - (Step 2) Next we create a random forest classifer\n  - (Step 3) Then we make our parameter grid\n  - (Step 4) Now we need to build and fit the GridSearchCV to find the best parameters\n  - (Step 5) Then we take the best parameters and fit the random forest on those best parameters\n  - (Step 6) Finally we make a prediction and print the accuracy and mean absolute error.\n\n- **FAIR WARNING** This will take a long time to train.  The parameter grid is combing through a wide range.","560f20ce":"This glucose chart is pretty big so I separated it from the rest.  Next up is insulin","1a51987b":"# *Displots*\n\n- A distplot is a great way to visualize the histrogram type of graph but with a kde curve","b98d2ac6":"##Import the libraires and then lets get into the data and see what we need to work on","6cebb490":"The horizontal orientation actually is a lot better for this.  The numbers along the y axis were very squished when I had them over on the y axis so the flip makes it more readable.","5a346979":"#Logistic Regression \n\n- We will use the GridSearchCV functionality here.  Why?  Well, it can let us do a hyperparameter sweep to see the best parameters for the LR similarly to the RF.\n\n- Okay so what is this code?\n  - (Step 1) The first step is to call our splitting data function \n  - (Step 2) Then we make our parameter grid\n  - (Step 3) Now we need to build and fit the GridSearchCV to find the best parameters\n  - (Step 4) Then we take the best parameters and fit the random forest on those best parameters\n  - (Step 5) Finally we make a prediction and print the accuracy and mean absoulte error.\n\n- **FAIR WARNING** This will take a long time to train.  The parameter grid is combing through a wide range.","e0ed4324":"The accuracy here is also not too bad.  We are about 80% again.","c1d5c97a":"#XGBoost\n\n\n- Okay so what is this code?\n  - (Step 1) The first step is to call our splitting data function and make a support vector machine\n  - (Step 2) We will build an XGBoost classifier and use these arguments and fit the model:\n    - objective = binary:logistic\n    - estimators = 500, \n    - max_depth = 5, \n    - seed = 123\n  - (Step 3) We need to calculate the accuracy and the mean absolute error to see how we did.\n  - (Step 4) Now we can use kfold and cross val score to measure the accuracy and mean absolute error.  Kfold and cross val score are two validation techniques so we can measure our validation metrics.\n\n- `Note` I am not using a GridSearchCV here.  I decieded not to since this was my first foray into XGBoost and I wanted to play around with it rather than use it as a serious model although it could be.","f9370f64":"#Count Plots\n\n- What is a count plot?  A count plot will feel like a bar chart but we can also use the hue to show us the relation of having diabetes or not based on the specific column.\n\n- For example, the pregnancies column will show us how many people had zero pregnanices and how many of do not have diabetes and how many of them do","80fa8767":"#Histograms\n\n- Kaggle shows us this, however, I still wanted to build the graphs to develop the underlying intuition behind which charts make sense.","b3c1a7b2":"#Models\n\nWhat models do I want to use?\n  - Dense network\n  - Random Forrest\n  - Logistic Regression\n  - SVM\n  - XGBoosst\n","99858081":"#Data Dictionary\n\n* *FYI: Everyone person in this dataset is a woman at age 21 or higher*\n\n| Column | Meaning | Thoughts |\n|------- | ------- | -------- |\n|Pregnancies | Number of times pregnant | An integer.  What is the min and what is the max?  |\n| Glucose | Plasma glucose concentration a 2 hours in an oral glucose tolerance test | Measured in mmol\/L?  1 mmol\/L = 18 mg\/dL.  I need to multiply by 18 to get the measurement for the HOMA IR Score|\n|Blood Pressure | Diastolic blood pressure | mm Hg.  Can we figure out the systolic?  The number we are given is the second one -- 120\/80 |\n|SkinThickness | Triceps skin fold thickness (mm) | This seems kinda normal|\n|Insulin | 2 hour serum insuling (muU\/ml) | Measured in muU\/ml.|\n|BMI | Body mass index (weight in kg\/(height in meters)<sup>2<\/sup> | Traditional measurement |\n|Diabetes Pedigree | Diabetes Pedigree Function | A function used to determine whether you have diabetes given your family history |\n|Age | Age in years| This is a normal whole integer |\n|Outcome | Class variable (0 or 1) 268 of 768 are 1, the others are 0|  We have an imbalance so we can handle that but let's start without that|\n\n**Future Enhancement for Feature Engineering**\nHOMA - IR Blood Code \n- Insulin * Glucose = HOMA-IR\n- Healthy is 0.5 - 1.5\n- Less than 1.0 means you are insulin- sensitive, which is optimal\n- A range of 1 - 1.9 is within \"Normal limits\"\n- Above 1.9 indicates early insuling resistance\n- Above 2.9 indicates significant signal resistance","24abe4ce":"#Dense Network\n\n- The dense network will consist of four total layers (Input, Dense, Dropout, Dense).  Many people may call this a three layer network since we do not include the input layer, but that is okay for now.\n\n- The network will use\n  - The Adam optimizer (Adam, at the moment, is the go to optimizer so let's use it.)\n  - The binary cross entropy loss function (we are classifying a 1 or 0, a binary problem so this is fitting.)\n  - We will also track the accuracy since that is the optimal metric to watch in this problem.\n  - We use 1000 epochs to ensure the model sees the data plenty of times.\n  - We use a batch size of 10\n  - We shuffle the data after every epoch to help prevent the model from memorizing the dataset\n  - The validation split is 20% (a commonly used percentage)\n  - The output is verbose so we can see it training.\n    - **FAIR WARNING** 1,000 epochs will make the notebook drag on so if you wish to change this then make the verbosity 0 to not see the training.","6fdf4358":"#Normalize and Standardize\n\nThis data needs to be properly taken care of if we are going to expect a machine learning model to find helpful patterns.\n\nFortunately, we are already dealing with numbers and have no categorical data to handle \ud83d\udc4d.\n\n","241c2844":"#Future Additions?\n\n- Feature Engineering ideas with the HOMA IR\n\n- Balance Class Imbalance\n\n- A pipeline for the random forest to normalize and standardize better\n\n- Make the splitting data function into a global variable so I do not have to call it all the time","79821cae":"# DISCLAIMER: I am by no means legally able to give any kind of health advice.  I am not a doctor of any kind, nor do I claim to be.  Take this analysis for what it is.  Please do not take this as advice or try to input information to see if you have diabetes.","5748608d":"Okay we have distplots, now what?  A distplot can show us the distribution (hence dist in distplot) of the data.  Why do we want to see this?  Well we can easily see if we will need to standardize this data or not.  We can see something such as skin thickness having two peaks  I can easily see two bumps there so we need to check the standard deviations.  We can use the dataframe.describe() method to see how many standard deviations we are.","3fda4267":"Okay we have an accuracy of about 80%.  That is not bad, but I am sure we could be better.  The dense network seems to be the best so far.","6aefce44":"#What you did not see\n\n- I am going to be transparent and say there was a lot of work I did behind the scences to make a clean and easy to read notebook.  I had tried all sorts of plots in addition to these (bar, line, pie, scatter, regplot, and violin).  I decided to remove a heavy amount to make this easy for anyone that wants to learn from this.\n\n\n- I have all of my plots in my GitHub accompanying this work.  If you wish to see some really weird plots then head over to my [GitHub](https:\/\/github.com\/alexantaki16\/diabetes_pima).  (You need to go to the master branch so switch branches to see it.  I am trying to fix this.)","7c0c4837":"#Correlations with SNS","dbd912b9":"Correlations are a great way to have a feel for the data with a few lines of code.  \n\nWe can see a few correlations such as: \n  - Pregnancies and age\n  - Glucose and your diabetic outcome\n  - Insulin and glucose\n  - Age and glucose\n  - Skin thickness and insulin\n  - Skin thickness and BMI\n\nThese correlations make sense intuitively too.  There are not too many surprises here, but it is good to know what is related to what.","ac3c3cc2":"What are the noticeable attributes?\n- Maybe the more pregnancies a woman has the more likely she is to develop diabetes later?  That is odd.  Maybe that is some kind of anomaly?\n\n- Skin thickness is showing that the thicker your skin is the more likely you are to have diabetes.\n  - Intuitively, this seems right.  The thicker your skin the more likely you are to be overweight and this leads to type 2 diabetes typically.\n\n- Blood pressure feels somewhat normal.  The healthier your blood pressure the less likely you are to develop diabetes.\n\n- Age is showing us the younger you are the more likely it is you do not have diabetes, and as you age it seems to be more likely or balance out depending on your age.","eb05a6f2":"Okay the random_forest_grid came back with \n  - n_estimator = 5000\n  - max_features = sqrt\n  - max_depth = 5\n  - criterion = entropy\n\nAre there better hyperparameters out there - probably.  However, we did a wide range sweep so we are going to be okay with what we have.","2b531690":"# Model time!\n\nOkay - we have plotted our data, found correlations, split and preped the data thanks to scikit - learn.\n\nNow lets build some models.\n\n- Model 1\n  - A small dense network \n  - Why a small dense network?  Why not.  A small dense network can be great for finding the patterns in data like this, however, a traditional machine learning model may be better.\n\n- Model 2\n  - A random forest classification\n  - A random forest is almost always good on this type of data.  The data flows in a noticeable if else statements.  If you have x amount of pregnancies traverse left else right, if you have y glucose levels traverse left else right, etc.\n\n- Model 3\n  - A logistic regression pipeline\n  - I wanted to build a pipeline to have the experince doing that in a project that was not a YouTube tutorial so here it is.\n\n- Model 4\n  - An SVM\n  - We can use this for a classification model or regression.  A classification task we assign a label to input.  We are trying to assign a 1 or 0 to the data.\n\n- Model 5\n  - An XGBoost model\n  - I have never use an XGBoost model before and I wanted to try it","631a59f3":"# SVM\n\n- We will use the GridSearchCV functionality here.  Why?  Well, it can let us do a hyperparameter sweep to see the best parameters for the SVM just like our previous two models.\n\n- Okay so what is this code?\n  - (Step 1) The first step is to call our splitting data function and make a support vector machine\n  - (Step 2) Then we make our parameter grid\n  - (Step 3) Now we need to build and fit the GridSearchCV to find the best parameters\n  - (Step 4) Then we take the best parameters and fit the support vector machine on those best parameters\n  - (Step 5) Finally we make a prediction and print the accuracy and mean absoulte error.\n\n- I kept this as one method since the training is pretty quick","fe83d8b9":"#Questions\n\n- What kind of data are dealing with - structured or unstructred?\n  - Structured data for sure\n\n- Does the data have categorical, numerical, ordinal, or time series data?\n  - Numerical\n\n- What are my feature variables (inputs) and what are my target variables (outputs)?\n  - The feature variables are here:\n    - Pregnancy count\n    - Glucose level\n    - BloodPressure\n    - SkinThickness\n    - Insulin\n    - BMI\n    - DiabetesPedigreeFunction\n    - Age\n\n- Are we missing any data?\n  - Thankful no!\n\n- Are we dealing with outliers?\n  - Sure are, we will handle this in the notebook\n","df14319e":"Yikes.  We have to standardize the data if we wish to help our model learn better.  The plots have familiarized us with the data and it is now time to normalize and standardize.","3900ad1c":"##This is the notebook that will serve as the analysis and model building of the Pima Native Americans datasets from Kaggle.\n\n<h3>Goals of this project<\/h3>\n- Develop a strong structure to follow, as in a framework so I have the ability to use this a guide for future projects.\n\n- Commit to GitHub so I can track my progress\n\n- Foster my intuition on which plots are the best for certain types of data\n\n- Standardize and normalize data quickly\n\n- Feature Engineering ideas?\n\n- Practice modularizing my code to make it easier to use"}}