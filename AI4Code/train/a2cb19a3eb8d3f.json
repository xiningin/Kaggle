{"cell_type":{"94384386":"code","6185fb01":"code","8a4c2c8f":"code","8e40d590":"code","dc74c0d9":"code","9fc7c001":"code","6b091de2":"code","799e98f0":"code","b28fb772":"code","0f8bb3af":"code","eeae693d":"code","d60eafcf":"code","e4365efb":"code","ac3c2ab9":"code","28e92208":"code","0d9f7428":"code","9dec1dd0":"code","11a9d6c7":"code","dc4a4fb4":"code","a6e74635":"code","8cb8efce":"code","67f033d7":"code","f186b6a6":"code","8eb7f4d3":"code","d6e1cb34":"code","5801af9a":"code","71afb347":"code","10a27934":"code","8ddba5e6":"code","30e72cad":"code","a7370512":"code","d858f233":"code","cc38a8cc":"code","3ed0a73b":"code","16c1778e":"code","23c8f939":"code","ff4eb2be":"markdown","ffbf59e0":"markdown","e81f4fec":"markdown","31fb1ec3":"markdown","bfecc939":"markdown","6c816ccb":"markdown","d09089a7":"markdown","23ace5f8":"markdown","8f30c010":"markdown","985be917":"markdown","9198c4d9":"markdown","640a2e0c":"markdown","fadcfb02":"markdown"},"source":{"94384386":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom PIL import Image\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ninputPaths = []\nmaskPaths = []\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/fluorescent-neuronal-cells\/all_images'):\n    for filename in filenames:\n        if filename.endswith(\"png\"):\n            fullPath = os.path.join(dirname, filename)\n            inputPaths.append(fullPath)\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/fluorescent-neuronal-cells\/all_masks'):\n    for filename in filenames:\n        if filename.endswith(\"png\"):\n            fullPath = os.path.join(dirname, filename)\n            maskPaths.append(fullPath)","6185fb01":"import multiprocessing as mp\n\n #dict<filename:string,(original:image, mask:image)>\n#fileInfo:(string, (imgPath:string, maskPath:string))\ndef LoadImgProc(fileInfo):\n    filename = fileInfo[0] \n    imgPath, maskPath = fileInfo[1] \n    img = Image.open(imgPath)\n    mask = Image.open(maskPath).convert('L')\n    return (filename, img, mask)\n\ndef loadImages(imgPathNames, maskPathNames):\n    maskFileDict = {os.path.basename(fullPath): fullPath for fullPath in maskPathNames}\n    imgMaskFilePairsDict = {os.path.basename(fullPath) : (fullPath, maskFileDict[os.path.basename(fullPath)]) for fullPath in imgPathNames}\n    \n    #processPool = mp.Pool(processes=4,maxtasksperchild=20)\n    #imgMaskPairsList = mp.Pool().map(LoadImgProc, list(imgMaskFilePairsDict.items()))\n    imgMaskPairsList = [LoadImgProc(t) for t in imgMaskFilePairsDict.items()]\n    #processPool.close()\n    return {filename: (img, mask) for filename, img, mask in imgMaskPairsList}\n\nimgMaskPairsRaw = loadImages(inputPaths, maskPaths)","8a4c2c8f":"import matplotlib.pyplot as plt\n\nimgListRaw = [t[0] for _, t in imgMaskPairsRaw.items()]\nmaskListRaw = [t[1] for _, t in imgMaskPairsRaw.items()]\n\n#imgList = [np.array(t[0]) for _, t in imgMaskPairsRaw.items()]\n#maskList = [np.array(t[1], dtype=np.uint8) for _, t in imgMaskPairsRaw.items()]\n\nplt.figure(figsize=(10,16))\nplt.imshow(maskListRaw[0])","8e40d590":"from collections import namedtuple\nfrom collections import defaultdict\n\nCellLocation = namedtuple(\"CellLocation\", [\"nPixels\", \"x\", \"y\", \"width\", \"height\"])\n\ndef bfs(r, c, nRows, nCols, traversable, groupAssignments, groupIndex):\n    frontier = [(r,c)]\n    groupAssignments[r][c] = groupIndex\n    while len(frontier) > 0:\n        curRow, curCol = frontier.pop()\n        nextNodes = [\n            (curRow + 1, curCol),\n            (curRow - 1, curCol),\n            (curRow, curCol + 1),\n            (curRow, curCol - 1)\n        ]\n        for nextRow, nextCol in nextNodes:\n            withinRange = (\n                nextRow >= 0 and nextRow < nRows and\n                nextCol >= 0 and nextCol < nCols)\n            unvisited = (\n                withinRange and \n                traversable[nextRow][nextCol] and\n                groupAssignments[nextRow][nextCol] == None)\n            if withinRange and unvisited:\n                frontier.append((nextRow, nextCol))\n                groupAssignments[nextRow][nextCol] = groupIndex\n    #end of loop, all linked nodes have been marked with \"groupIndex\"\n    return\n\n#(imgArr:TNumeric[][]) => CellLocation[]\ndef markGroups(maskArr):\n    threshold = int(255 * 0.5)\n    #Native python list to speed up per-element indexing\n    isCell = np.greater(maskArr, threshold).tolist() \n    nRows, nCols = maskArr.shape\n    group = [[None]*nCols for _ in range(nRows)]\n    groupIdx = 0\n    #flood fill & mark cell groups\n    for r in range(nRows):\n        for c in range(nCols):\n            unvisitedGroup = isCell[r][c] and (group[r][c] == None)\n            if unvisitedGroup:\n                bfs(r,c, nRows, nCols, isCell, group, groupIdx) #flood fill one cluster\n                groupIdx += 1 #Ensure group clusters are unique\n    \n    #Get size & bounding boxes\n    nGroups = groupIdx\n    sizeList = [0] * nGroups\n    \n    rightList = [0] * nGroups\n    leftList = [nCols] * nGroups\n\n    botList = [0] * nGroups\n    topList = [nRows] * nGroups\n    \n    for r in range(nRows):\n        for c in range(nCols):\n            if group[r][c] != None:\n                groupNo = group[r][c]\n                sizeList[groupNo] += 1\n                \n                rightList[groupNo] = max(rightList[groupNo], c)\n                leftList[groupNo] = min(leftList[groupNo], c)\n                \n                botList[groupNo] = max(botList[groupNo], r)\n                topList[groupNo] = min(topList[groupNo], r)\n                \n    #pack into tuples\n    cellLocList = []\n    for groupNo in range(nGroups):\n        size = sizeList[groupNo]\n        x = leftList[groupNo]\n        y = topList[groupNo]\n        width = 1 + (rightList[groupNo] - x)\n        height = 1 + (botList[groupNo] - y)\n        cellLocList.append(CellLocation(size, x, y, width, height))\n        \n    return cellLocList","dc74c0d9":"processPool = mp.Pool()\ncellLocationList = processPool.map(markGroups, [np.array(t[1], dtype=np.uint8) for _, t in imgMaskPairsRaw.items()])\nprocessPool.close()","9fc7c001":"#Sanity check\nimport matplotlib.patches as patches\n\nimageIdx = 0;\n\nplt.figure(figsize=(10,16))\n\nplt.title(\"Cell Boundaries (Based on corresponding mask)\")\nplt.imshow(imgListRaw[imageIdx])\n\nax = plt.gca();\nfor cellLoc in cellLocationList[imageIdx]:\n    rect = patches.Rectangle((cellLoc.x, cellLoc.y), cellLoc.width, cellLoc.height, linewidth=1, edgecolor='r', facecolor='none')\n    ax.add_patch(rect)\n\nplt.show()","6b091de2":"flattenedCellLocations = []\nfor li in cellLocationList:\n    flattenedCellLocations.extend(li)","799e98f0":"nTotalCells = len(flattenedCellLocations)\nprint(nTotalCells)\nsizes = [cellLoc.nPixels for cellLoc in flattenedCellLocations]\nwidths = [cellLoc.width for cellLoc in flattenedCellLocations]\nheights = [cellLoc.height for cellLoc in flattenedCellLocations]","b28fb772":"fig, ax = plt.subplots(1, 3, figsize=(16,5))\n\nnBins = 20\nax[0].hist(sizes,bins=nBins)\nax[0].set_title('Cell Area (pixels)')\n\nax[1].hist(widths,bins=nBins)\nax[1].set_title('Cell Widths(pixels)')\n\nax[2].hist(heights,bins=nBins)\nax[2].set_title('Cell Heights(pixels)')\n\nplt.tight_layout()\nplt.show()\n\npercentile = 95\n\nsizePercentile = np.percentile(sizes, percentile)\nwidthPercentile = np.percentile(widths, percentile)\nheightPercentile = np.percentile(heights, percentile)\n\nprint(\"{0}th Percentile Values for:\".format(percentile))\nprint(\" Area:{:0.2f} pixels\".format(sizePercentile))\nprint(\" Width:{:0.2f} pixels\".format(widthPercentile))\nprint(\" Height:{:0.2f} pixels\".format(heightPercentile))","0f8bb3af":"totalNoOfMarkedPixels = sum(sizes)\noverallNoOfPixels = sum((mask.size[0] * mask.size[1] for mask in maskListRaw))\ntotalNoOfUnmarkedPixels = overallNoOfPixels - totalNoOfMarkedPixels\n\nmarkedInv = overallNoOfPixels \/ totalNoOfMarkedPixels\nunmarkedInv = overallNoOfPixels \/ totalNoOfUnmarkedPixels\nnormalizationCoeff = 1 \/ (markedInv + unmarkedInv)\nclassWeights = {\n    0: unmarkedInv * normalizationCoeff, #unmarked pixels\n    1: markedInv * normalizationCoeff    #marked as cells\n}\n\nprint(\"Percentage of pixels which are neural cells: {0}%\".format((totalNoOfMarkedPixels\/ overallNoOfPixels) * 100))\nprint(classWeights)","eeae693d":"import math\n\nRect = namedtuple(\"Rect\", [\"x\", \"y\", \"width\" , \"height\"])\n\ndef EvenlySpacedPartitions(width, height, partitionLength, minOverlap):\n    def minPartitions(dimLength):\n        return math.ceil((dimLength - minOverlap) \/ (partitionLength - minOverlap))\n    nRows = minPartitions(height)\n    nCols = minPartitions(width)\n    \n    def getPartitionStartingPositions(dimLength, nPartitions):\n        overlap = (dimLength - nPartitions * partitionLength) \/ (1- nPartitions)\n        advance = partitionLength - overlap\n        return [advance * i for i in range(nPartitions)]\n    \n    rowPositions = getPartitionStartingPositions(height, nRows)\n    colPositions = getPartitionStartingPositions(width, nCols)\n    partitions = []\n    for colStart in colPositions: #x\n        for rowStart in rowPositions: #y\n            partitions.append(Rect(colStart, rowStart, partitionLength, partitionLength))\n    return partitions\n\nimageIdx = 0;\n\nplt.figure(figsize=(10,16))\n\nplt.title(\"Cell Boundaries (Based on corresponding mask)\\nRed rectangles represent the boundary of each sub-image\")\nplt.imshow(imgListRaw[imageIdx])\n\nwidth, height  = imgListRaw[imageIdx].size\n\npartitions = EvenlySpacedPartitions(width, height, 400, 100)\n\nax = plt.gca();\nfor partition in partitions:\n    rect = patches.Rectangle((partition.x, partition.y), partition.width, partition.height, linewidth=3, edgecolor='r', alpha= 0.5, linestyle=\"--\", facecolor='none')\n    ax.add_patch(rect)\n\nplt.show()","d60eafcf":"CroppedImage = namedtuple(\"CroppedImage\", [\"image\", \"mask\", \"sourceRect\"])\n\ndef CropImage(pilImage, pilMask, partitions):\n    croppedImages = []\n    for partition in partitions:\n        top, bot = (partition.y, partition.y + partition.height)\n        left, right = (partition.x, partition.x + partition.width)\n        croppedImg = pilImage.crop((left, top, right, bot))\n        croppedMask = pilMask.crop((left, top, right, bot))\n        croppedImages.append(CroppedImage(croppedImg, croppedMask, partition))\n    return croppedImages\n\ndef GetAllPartitions(pilImageList, partitionSize, minOverlap):\n    partitionList = []\n    for img in pilImageList:\n        width, height = img.size\n        partitions = EvenlySpacedPartitions(width, height, partitionSize, minOverlap)\n        partitionList.append(partitions)\n    return partitionList\n\npartitionList = GetAllPartitions(imgListRaw, 400, 100)","e4365efb":"sampleCrop = CropImage(imgListRaw[0], maskListRaw[0], partitionList[0])\n\nfor idx, croppedImagePair in enumerate(sampleCrop):\n    fig, ax = plt.subplots(1, 2, figsize=(5,2))\n\n    ax[0].imshow(croppedImagePair.image)\n    ax[0].set_title('Cropped Image #{0}'.format(idx))\n    \n    ax[1].imshow(croppedImagePair.mask)\n    ax[1].set_title('Cropped Mask #{0}'.format(idx))\n    \n    plt.tight_layout()\n    plt.show()","ac3c2ab9":"from PIL import ImageDraw\nfrom enum import Enum\nimport random\n\nTColour = namedtuple(\"TColour\", [\"r\", \"g\", \"b\"])\n\n#Sampled from the cells in the first image\ndarkYellow = TColour(101, 61, 6)\nbrightYellow = TColour(247, 237, 15)\n\n#linear lerp between the rgb values\n#frac = fraction of colour 2\ndef lerpColours(colour1, colour2, frac):\n    r = colour1.r * (1 - frac) + colour2.r * frac\n    g = colour1.g * (1 - frac) + colour2.g * frac\n    b = colour1.b * (1 - frac) + colour2.b * frac\n    return TColour(int(r),int(g),int(b))\n\nclass ENShape(Enum):\n    Ellipse = 0\n    Rect = 1\n\ndef DrawRandomEllipse(img, shape = ENShape.Ellipse):\n    imgWidth, imgHeight = img.size\n    centreX, centreY = (random.uniform(0, imgWidth), random.uniform(0, imgHeight))\n    shapeWidth, shapeHeight = (random.uniform(25, 75), random.uniform(25, 75)) #based on the cell size distribution\n    boundingBox = (\n        int(centreX - shapeWidth * 0.5),\n        int(centreY - shapeHeight * 0.5),\n        int(centreX + shapeWidth * 0.5), \n        int(centreY + shapeHeight * 0.5)\n    )\n    \n    fillColour = lerpColours(darkYellow, brightYellow, random.uniform(0, 1))\n    \n    gc = ImageDraw.Draw(img)\n    if shape == ENShape.Ellipse:\n        gc.ellipse(boundingBox, fill=fillColour, outline=None, width=1)\n    elif shape == ENShape.Rect:\n        rad = int(random.uniform(1, min(shapeWidth, shapeHeight) * 0.3))\n        gc.rounded_rectangle(boundingBox, radius=rad, fill=fillColour, outline=None, width=1)\n\nsampleImage = sampleCrop[-1].image.copy()\nDrawRandomEllipse(sampleImage, ENShape.Ellipse)\nDrawRandomEllipse(sampleImage, ENShape.Rect)\n\nplt.figure(figsize=(7,7))\nplt.title(\"Image Augmentation:\\nArtificial Shapes (Ellipses & Rectangles) Drawn\")\nplt.imshow(sampleImage)","28e92208":"#Affine transformations\ndef TranslateXForm(x,y):\n    return np.array(\n        [[1,0,x],\n         [0,1,y],\n         [0,0,1]]\n    );\n        \ndef RotateXForm(deg):\n    rad = math.radians(deg)\n    return np.array(\n        [[math.cos(rad),-math.sin(rad),0],\n         [math.sin(rad), math.cos(rad),0],\n         [0,0,1]]\n    );\n\ndef ShearXForm(cx, cy):\n    return np.array(\n        [[1,cx,0],\n         [cy,1,0],\n         [0,0,1]]\n    );\n\ndef ScaleXForm(xScale, yScale):\n    return np.array(\n        [[xScale,0,0],\n         [0,yScale,0],\n         [0,0,1]]\n    );\n\ndef GetRandomImageXForm(imgWidth, imgHeight):\n    xform = np.identity(3)\n    xform = np.matmul(TranslateXForm(imgWidth * -0.5, imgHeight * -0.5), xform) # Set image centre as origin\n    #Now go bananas\n    #shear\n    xform = np.matmul(ShearXForm(random.uniform(0, 0.20), random.uniform(0, 0.20)), xform)\n    #rotation\n    xform = np.matmul(RotateXForm(random.uniform(0, 360)), xform)\n    #scaling\n    xform = np.matmul(xform, ScaleXForm(random.uniform(0.8, 1.5), random.uniform(0.8, 1.5)))\n    #horizontal\/vertical flips\n    yFlipCoeff = 1 if bool(random.getrandbits(1)) else -1\n    xFlipCoeff = 1 if bool(random.getrandbits(1)) else -1\n    xform = np.matmul(ScaleXForm(xFlipCoeff, yFlipCoeff), xform)\n    \n    xform = np.matmul(TranslateXForm(imgWidth * 0.5, imgHeight * 0.5), xform) #Shift back to 0,0 as top left origin\n    return xform\n\nimgBgColour1 = TColour(20, 8, 0)\nimgBgColour2 = TColour(49, 25, 1)\n\ndef ApplyAffineXForm(image, xform, bgColour):\n    invXForm = np.linalg.inv(xform) #required by PIL\n    unpackedXForm = (*invXForm[0],*invXForm[1])\n    \n    xformedImg = image.transform(\n        image.size, \n        Image.AFFINE, \n        data=unpackedXForm, \n        resample=Image.BILINEAR,\n        fillcolor=bgColour)\n    return xformedImg","0d9f7428":"sampleImage = sampleCrop[-1].image\nsampleMask = sampleCrop[-1].mask\n\naugmentingXForm = GetRandomImageXForm(*sampleImage.size)\nbgColour = lerpColours(imgBgColour1, imgBgColour2, random.uniform(0, 1))\n\nxformedImg = ApplyAffineXForm(sampleImage,augmentingXForm, bgColour)\nxformedMask = ApplyAffineXForm(sampleMask,augmentingXForm, 0)\n\nfig, ax = plt.subplots(2, 2, figsize=(10,8))\n\nax[0,0].imshow(sampleImage)\nax[0,0].set_title('Original Image')\n\nax[0,1].imshow(xformedImg)\nax[0,1].set_title('Image (After Affine Transform)')\n\nax[1,0].imshow(sampleMask)\nax[1,0].set_title('Original Mask')\n\nax[1,1].imshow(xformedMask)\nax[1,1].set_title('Mask (After Affine Transform)')\n\nplt.tight_layout()\nplt.show()","9dec1dd0":"partitionedImagesSet = [CropImage(img, mask, partitionInfo) for img, mask, partitionInfo in zip(imgListRaw, maskListRaw, partitionList)]","11a9d6c7":"flattenedImages = []\nflattenedMasks = []\nfor imageMaskPairList in partitionedImagesSet:\n    for imageMaskPair in imageMaskPairList:\n        flattenedImages.append(imageMaskPair.image)\n        flattenedMasks.append(imageMaskPair.mask)","dc4a4fb4":"print(len(flattenedImages))","a6e74635":"from tensorflow.keras.utils import Sequence\nfrom tensorflow.keras.preprocessing.image import apply_affine_transform\n\nsubImgSize = 400\nfinalMaskSize = 50\n\n\n#Note: Will draw directly on the image, pass a copy if the intent isn't to alter the original\ndef drawRandomShape(pilImage):\n    nEllipses = random.randint(0, 2)\n    for _ in range(nEllipses):\n        DrawRandomEllipse(pilImage, ENShape.Ellipse)\n    nRects = random.randint(0, 2)        \n    for _ in range(nRects):\n        DrawRandomEllipse(pilImage, ENShape.Rect)\n    return pilImage\n\nclass ImageSequence(Sequence):\n\n    def __init__(self, x_set, y_set, batch_size, mask_final_dim): \n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.shuffleIndices()\n        self.mask_dim = (mask_final_dim, mask_final_dim) #Used to downscale the mask\n        self.bDrawShapes = False\n        self.bAffineTransform = False\n    \n    def shuffleIndices(self):\n        nSamples = len(self.x)\n        indices = np.arange(nSamples)\n        np.random.shuffle(indices)\n        self.indices = indices\n    \n    def on_epoch_end(self):\n        self.shuffleIndices()\n        \n    def enableFeatures(self, bDrawShapes, bAffineTransform):\n        self.bDrawShapes = bDrawShapes\n        self.bAffineTransform = bAffineTransform\n        \n    def __len__(self):\n        return math.floor(len(self.x) \/ self.batch_size)\n\n    def __getitem__(self, idx):\n        startingIdx = idx * self.batch_size\n        endingIdx = (idx + 1) * self.batch_size\n        selectedIndices = self.indices[startingIdx:endingIdx]\n        \n        splitImages = [self.x[idx] for idx in selectedIndices]\n        splitMasks = [self.y[idx] for idx in selectedIndices]\n        \n        imgWithRandomShapes = [drawRandomShape(img.copy()) if self.bDrawShapes else img \n                                   for img in splitImages]\n        masksDownscaled = [tMask.resize(self.mask_dim, Image.NEAREST ) for tMask in splitMasks]\n        \n        x_np = [np.array(img) for img in imgWithRandomShapes]\n        y_np = [np.array(img) for img in masksDownscaled]\n        \n        x_augmented = []\n        y_augmented = []\n        \n        for x, y in zip(x_np, y_np):\n            rotation = random.uniform(0, 360)\n            shear = random.uniform(-10, 10)\n            yFlipCoeff = 1 if bool(random.getrandbits(1)) else -1\n            xFlipCoeff = 1 if bool(random.getrandbits(1)) else -1\n            xScale = random.uniform(0.8, 1.5) * xFlipCoeff\n            yScale = random.uniform(0.8, 1.5) * yFlipCoeff\n            \n            bgColor = random.uniform(0,50) #can't specify separate values for RGB\n            #x_aug = apply_affine_transform(x, theta=rotation, shear=shear, zx=xScale, zy=yScale, row_axis=0, col_axis=1, channel_axis=2, fill_mode='constant', cval=bgColor, order=1)\n            x_aug = None\n            if self.bAffineTransform:\n                x_aug = apply_affine_transform(x, theta=rotation, shear=shear, zx=xScale, zy=yScale, row_axis=0, col_axis=1, channel_axis=2, fill_mode='constant', cval=bgColor, order=1)\n            else:\n                x_aug = x\n                \n            original_y_shape = y.shape\n            y_reshaped = np.reshape(y, (*original_y_shape, 1))\n            y_aug = None\n            if self.bAffineTransform:\n                y_aug = apply_affine_transform(y_reshaped, theta=rotation, shear=shear, zx=xScale, zy=yScale, row_axis=0, col_axis=1, channel_axis=2, fill_mode='constant', cval=0.0, order=1)\n            else:\n                y_aug = y_reshaped\n                \n            y_aug_reshaped = np.reshape(y_aug, original_y_shape)\n            x_augmented.append(x_aug)\n            y_augmented.append(y_aug_reshaped)\n            \n        return (np.array(x_augmented), np.array(y_augmented) \/ 255)","8cb8efce":"from sklearn.model_selection import train_test_split\n\nimg_train, img_test, mask_train, mask_test = train_test_split(flattenedImages, flattenedMasks , test_size=0.2, random_state=0xDEADBEEF)\n\nprint(len(img_train), len(img_test))","67f033d7":"#Filter out those within the training set that actually have cells\n\ntrainSetWithCells = [(img, mask) for img, mask in \n                     zip(img_train, mask_train) \n                     if (np.sum(np.array(mask)) > 255 * 5)]\nimg_train_with_cells, mask_train_with_cells = zip(*trainSetWithCells) \n\n#for weight calculation\nnUnmarkedPixels = sum([np.sum(np.array(mask) < 122) for mask in mask_train_with_cells])\nnMarkedPixels = sum([np.sum(np.array(mask) > 122) for mask in mask_train_with_cells])\n\ntotalPixels = nMarkedPixels + nUnmarkedPixels\ninvMarked = totalPixels \/ nMarkedPixels\ninvUnmarked = totalPixels \/ nUnmarkedPixels\nnormalizingCoeff = 1 \/ (invMarked + invUnmarked)\n\nlossWeights = [invUnmarked * normalizingCoeff, invMarked  * normalizingCoeff]\n\nprint(len(img_train_with_cells))\nprint(\"Unmarked: {0}, marked {1}\".format(nUnmarkedPixels, nMarkedPixels))\nprint(\"Percentage of pixels which are neural cells (filtered training set): {0}%\".format((nMarkedPixels\/ totalPixels) * 100))\nprint(lossWeights)","f186b6a6":"batch_size = 5\ngenerator = ImageSequence(img_train_with_cells, mask_train_with_cells, batch_size, finalMaskSize)\ngenerator.enableFeatures(True, True)\nimages, masks = generator.__getitem__(0)\n\nfor img, mask in zip(images, masks):\n    fig, ax = plt.subplots(1, 2, figsize=(7,2))\n\n    ax[0].imshow(img)\n    ax[0].set_title('Augmented Image (TF pipeline)')\n    \n    ax[1].imshow(mask)\n    ax[1].set_title('Augmented Mask (TF pipeline)')\n    \n    plt.tight_layout()\n    plt.show()","8eb7f4d3":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nkeras.backend.clear_session()\n\n#Loss function shamelessly copied from Sayan Dey from https:\/\/stackoverflow.com\/questions\/46009619\/keras-weighted-binary-crossentropy\ndef weighted_binary_crossentropy(zero_weight, one_weight):\n    def weighted_binary_crossentropy(y_true, y_pred):\n        b_ce = keras.backend.binary_crossentropy(y_true, y_pred)\n        # weighted calc\n        weight_vector = y_true * one_weight + (1 - y_true) * zero_weight\n        weighted_b_ce = weight_vector * b_ce\n        return keras.backend.mean(weighted_b_ce)\n\n    return weighted_binary_crossentropy\n\nmodel = keras.Sequential(\n    [\n        layers.InputLayer(input_shape=(subImgSize, subImgSize, 3)),\n        \n        layers.Lambda(lambda x : x \/ 255), #{0-255} to {0-1}\n        layers.Conv2D(196, 5, strides=(2,2), activation='relu'),\n        layers.MaxPooling2D(2),\n                \n        layers.Conv2D(256, 5,  activation='relu'),\n        layers.MaxPooling2D(2),\n        \n        layers.Conv2D(512, 3, activation='relu'),   \n        layers.Conv2D(512, 1, activation='relu'),\n        #layers.Conv2D(512, 2, activation='relu'),\n        #layers.MaxPooling2D(2),\n        \n        layers.Conv2DTranspose(1,finalMaskSize - 44,activation=\"sigmoid\")\n    ]\n)\n\nopt = keras.optimizers.Adam(learning_rate=0.001)\n#reduce weight of +ve class to reduce over-eagerness to evaluate all yellow blobs are cells\nmodel.compile(optimizer=opt, loss = weighted_binary_crossentropy(lossWeights[0], lossWeights[1] * 0.33), metrics = ['acc'])\nmodel.summary()","d6e1cb34":"#With the filtered dataset\nbatch_size = 50\ngenerator = ImageSequence(img_train_with_cells, mask_train_with_cells, batch_size, finalMaskSize)\n\nhistory = model.fit(\n    x=generator,\n    epochs=5)\n    #todo validation_data=(testDataNp, testLabels))","5801af9a":"#Not from the validation set, just used for illustration\ndef previewResults():\n    testCases = np.array([np.array(img) for img in img_train_with_cells[:5]])\n    testGroundTruth = mask_train_with_cells[:5]\n    predictions = model.predict(testCases)\n\n    for img, pred, actual in zip(testCases, predictions, testGroundTruth):\n        fig, ax = plt.subplots(1, 3, figsize=(7,2))\n\n        ax[0].imshow(img)\n        ax[0].set_title('Input')\n    \n        ax[1].imshow(pred)\n        ax[1].set_title('Predicted')\n\n        ax[2].imshow(actual)\n        ax[2].set_title('Ground Truth')\n\n        plt.tight_layout()\n        plt.show()\n\npreviewResults()","71afb347":"generator.enableFeatures(True, False) #Enable Random shapes\n\nhistory = model.fit(\n    x=generator,\n    epochs=10)","10a27934":"previewResults()","8ddba5e6":"generator = ImageSequence(img_train, mask_train, batch_size, finalMaskSize) #use full image set\ngenerator.enableFeatures(True, True) #Enable Random shapes + transforms\n\nhistory = model.fit(\n    x=generator,\n    epochs=5)","30e72cad":"previewResults()","a7370512":"#img_test, mask_test\npredictions = model.predict(np.array([np.array(img) for img in img_test]))","d858f233":"import itertools\n\nfor img, pred, actual in itertools.islice(zip(img_test, predictions, mask_test), 5):\n    fig, ax = plt.subplots(1, 3, figsize=(7,2))\n    ax[0].imshow(img)\n    ax[0].set_title('Input')\n\n    ax[1].imshow(pred)\n    ax[1].set_title('Predicted')\n    ax[2].imshow(actual)\n    ax[2].set_title('Ground Truth')\n    plt.tight_layout()\n    plt.show()","cc38a8cc":"def computeMetrics(pred, groundTruthImages):\n    actualList = []\n    predList = []\n    for p, groundTruth in zip(pred, groundTruthImages):\n        resized = groundTruth.resize((finalMaskSize,finalMaskSize), Image.BILINEAR)\n        resizedThreshold = np.array(resized, dtype=np.uint8) > 122 #0-255\n        predThreshold = (p > 0.5).astype(np.uint8) #0-1\n        actualList.append(np.reshape(resizedThreshold, -1))\n        predList.append(np.reshape(predThreshold, -1))\n    actual = np.concatenate((actualList), axis=0)\n    predictions = np.concatenate((predList), axis=0)\n    return (predictions, actual)\n\npred1D, actual1D = computeMetrics(predictions, mask_test)","3ed0a73b":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sn\n\nlabels= [\"Non-Cells\", \"Cells\"]\nprint(classification_report(actual1D, pred1D, target_names=labels))\nconfusionMatrix = confusion_matrix(actual1D, pred1D)\n\ndf_cm = pd.DataFrame(confusionMatrix, index=labels, columns=labels)\nsn.heatmap(df_cm, annot=True, annot_kws={\"size\": 16}).set_title(\"Confusion Matrix\") # font size\nplt.show()","16c1778e":"#Apply to an entire image\n#pilImage => np.array\ndef applyModelToImage(pilImage, overlap=100):\n    width, height = pilImage.size\n    partitions = EvenlySpacedPartitions(width, height, 400, overlap)\n    subImages = []#np arrays\n    for partition in partitions:\n        top, bot = (partition.y, partition.y + partition.height)\n        left, right = (partition.x, partition.x + partition.width)\n        croppedImg = pilImage.crop((left, top, right, bot))\n        subImages.append(np.array(croppedImg))\n    \n    predictions = model.predict(np.array(subImages))\n    predictionsUpscaled = []\n    for pred in predictions:\n        img = Image.fromarray(np.uint8(pred.reshape((50,50)) * 255))\n        upscaled = np.array(img.resize((400,400)), dtype=np.uint8)\n        predictionsUpscaled.append(upscaled)\n    \n    #Combine predictions into a single image\n    #Use a \"minimum\" blending function, i.e. be as conservative as positive for cell predictions\n    output = np.full((height, width), 255, dtype=np.uint8)\n    for pred, location in zip(predictionsUpscaled, partitions):\n        imgTop, imgBot = (round(location.y), round(location.y + location.height))\n        imgLeft, imgRight = (round(location.x), round(location.x + location.width))\n        output[imgTop:imgBot,imgLeft:imgRight] = np.minimum(output[imgTop:imgBot,imgLeft:imgRight], pred)\n    \n    return output","23c8f939":"idx = 15\nstitchedMask = applyModelToImage(imgListRaw[idx], 100)\n\npredictedCellLocations = markGroups(stitchedMask) #threshold = 122\n\nfig, ax = plt.subplots(1, 3, figsize=(15,15))\n\nax[0].imshow(imgListRaw[idx])\nax[0].set_title('Original Image')\n\nax[1].imshow(stitchedMask)\nax[1].set_title('Predicted Mask (after Stitching)')\nfor cellLoc in predictedCellLocations:\n    rect = patches.Rectangle((cellLoc.x, cellLoc.y), cellLoc.width, cellLoc.height, linewidth=1, edgecolor='r', facecolor='none')\n    ax[1].add_patch(rect)\n\nax[2].imshow(maskListRaw[idx])\nax[2].set_title('Ground Truth')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Number of cells (Intensity threshold=122): {0}\".format(len(predictedCellLocations)))\nfor idx, loc in enumerate(predictedCellLocations):\n    print(\" {0}) {1}\".format(idx + 1, loc))","ff4eb2be":"# Data Augmentation (Random Shapes)\n\nUsing the size distribution of the cells found earlier, ellipses and rounded rectangles (with colours similar to the marked neurons) are randomly drawn on each sub-image without affecting the mask.\nThese artificially generated shape hint to the neural network the importance of geometry as a feature for prediction.\n\n**Remarks**: While a more rigourous approach to generating similar colours would be to fit the colours of the pixels marked as neurons to a Gaussian Distribution, or even a Gaussian Mixture Distribution, I have instead opted to generate RGB values between 2 colours very scientifically selected via the process of Guesstimation<b><sup>TM<\/sup><\/b> from the first image. To further emphasize the importance of geometry over colour, black & white images could instead be fed to the neural network.","ffbf59e0":"# Training Set Filtering\n\nTo lower the class imbalance problem, the training set will first be filtered to only include sub-images that actually contain neurons. This increases the percentage of pixels which are neurons from 0.6% (entire dataset) to 2% (filtered training dataset). Filtering is done by checking which image\/mask pair contains a mask that contains at least one non-zero value.","e81f4fec":"# Model Evaluation\n\nAll sub-images in the test set are fed through the model for prediction. These results are then fed through a threshold filter of 0.5 to force the predicted values to 0 or 1 for comparison. Subsequently, results are flattened and concatenated into a 1-D array of predictions in order to be evaluated using Scipy's classification reports.","31fb1ec3":"# CNN implementation\n\nNothing fancy here, just 5 layers of convolution that progressively reduce the image size. The typical downscale-upscale architecture of most state of the art image segmentation is not adopted here since a low-resolution output mask is deemed sufficient.\n\nKeras does not natively support class weights for 2D outputs, hence a custom loss function needs to be supplied instead.\n\nThe class weight for the neuron cell class has been scaled down by a factor of 3, this is to counteract the over-eagerness of the CNN to simply identify all yellow regions as neurons.\n\nDue to the significant class imbalance (predicting pure black all the time gives a \"accuracy\" of 99.4%), the only metric that is of interest will be the Cell class' precision & recall.","bfecc939":"# Training - \\[Phase 1.1\\]\n\n5 Epochs are run on the filtered training dataset without any augmentation.\nAt this point, the CNN can be seen producing false-positives for any region that has even the slightest tinge of yellow.","6c816ccb":"# Training - \\[Phase 2\\]\n\n5 Epochs are run on the entire training dataset. All augmentations (affine transforms + random shapes) are enabled.\nFrom the predictions, it almosts seems like this resulted in degradation in performance, with a reversion back to many false positives and very diffuse boundaries.\nThis could be in part due to the low number of training cycles, but nevertheless, model evaluation will proceed.","d09089a7":"# Data Preparation (Loading)\n\nRaw images and masks are loaded.\nFor each mask, a flood-fill is performed on each collection of marked pixels (with a threshold of >122 in intensity) to get the pixel area & dimensions of each cell. The cell width & height distributions are used to determine a suitable overlap value for the sub-divided images (100 pixels). Choosing an overlap value which is larger than most cells will allow at least one sub-image to contain a full view of the cell if it rests near a boundary.","23ace5f8":"# Appliation of the trained model to an image\n\nWith the trained model, entire images can now be evaluated. Images are first partitioned into the required 400x400 pixel sub-images and evaluated. Each mask is then upscaled back to 400x400 and blended together into a single image. For overlapping regions, a \"minimum\" blend function is used to choose the lower of the two pixel values in the overlaping images. The minimum blend function helps provide a more conservative estimate about whether a pixel represents a neuron (A more prudent option given the CNN's low precision). The flood-fill algorithm can be applied to the resultant mask to perform automated counting, in addition to detecting outliers in terms of predicted area to help discard predictions that are too large\/small.\n\nOne convenient benefit of using prediction based on sub-divided images is the ability for the trained network to be deployed on arbitrarily size images (as long as the pixel dimensions of the cells follow a similar distribution).","8f30c010":"# Introduction\n\nThis kernel performs image segmentation on images of mice brain cells in fluorescence microscopy, producing a monochrome output mask marking pixels which represent brain cells. This output mask may be fed through a flood-fill algorithm to automatically count the number of cells within an image.\n\nRecall of the pixel locations of brain cells currently hovers around 80%, with a precision of ~30%. This figure was achieved with a relatively shallow CNN network (5 convolution layers), and a low number of training cycles (15 epochs under a filtered training set & 5 epochs on the entire training set). With these limitations in mind, it should be possible to achieve better recall with more training time and a more complex network. The current bottleneck is tensorflow's lacking API for image augmentation, with affine transformations only being supported using the CPU rather than the GPU, causing training cycles with the full image augmentation pipeline to run abysmally slowly.\n\n# Methodology\n\n**Data Preparation**\n\n1. Raw images\/masks are evenly split into 400 by 400 px sub-images, with an overlap of at least 100 pixels in the x and y directions. The masks are downscaled to 50 by 50 pixels.\n    * The value of 100 is based on the 95th percentile of the cell widths\/heights, and helps to ensure that the cells will be fully covered by at least one image when near the boundaries\n    \n**Augmentation Strategy**\n\n2. The following 2 augmentations strategies are deployed within a data generator pipeline, and are progressively enabled during training\n    * Drawing of randomly positioned Ellipses & Rounded rectangles\n        * Theses are drawn in similar colours and sizes to the cells\n        * They serve as a method to force the network to learn to recognize the shape characteristics of the cells, instead of just relying on the presence of a cluster of yellow pixels for predictions.\n    * Randomly generated Affine Transformations\n        * Varying degrees of:\n            * Rotation\n            * X\/Y scaling\n            * Horizontal\/Vertical Flipping\n            * Skewing\n            \n**Training Schedule**\n\nDue to the highly imbalanced dataset (only 0.6% of the total pixels mark brain cells), the following schedule was adopted:\n\n3. \\[Phase - 1\\] - Training using only images with brain cells\n   * The training dataset is first filtered down to only those that contain brain cells. This raises the percentage of brain cell pixels in the filtered dataset to ~2%.\n   * The class weightage of brain cells is computed based on the ratio of the class labels in the filtered dataset, and scaled down by one-third\n      * The scaling factor is required to prevent the network from over-eagerly evaluating all yellow blobs as neurons.\n   * \\[Phase - 1.1\\] The network is run for 5 epochs on the filtered dataset, with no augmentation.\n   * \\[Phase - 1.2\\] The network is run for 10 epochs on the filtered dataset, the following augmentation is then enabled:\n      * Drawing of randomly positioned Ellipses & Rounded rectangles\n      * This phase is used to emphasize to the network the importance of cell geometry as a feature for prediction.\n      \n4. \\[Phase - 2\\] - Training using the full training dataset\n   * The full dataset is then used, with all augmentations enabled. \n      * Drawing of randomly positioned Ellipses & Rounded rectangles\n      * Randomly generated Affine Transformations\n   * Only 5 epochs are run due to long training times (mostly bottlenecked by the data generator)\n\n**Prediction Strategy**\n\nSince the network operates on subimages of the raw-image, the following methodology is used to recombine the masks into a single image.\n\n5. Given an input image, split it evenly into 400 by 400 px sub-images, an arbitrary overlap value can be used after the model has been trained.\n6. Perform prediction on each sub-image using the trained model, upscale the predicted masks back to 400x400 px.\n7. Merge the predicted masks onto a single image, based on their corresponding locations\n   * For overlapping regions, the minimum value of each mask is taken (i.e. most conservative estimate)","985be917":"# Data Augmentation (Affine Transformations)\n\nAffine transformations are a convenient way to apply basic transformations to an image, such as:\n   * X\/Y Translations (shifting the image up\/down)\n   * X\/Y Scaling (stretching\/shrinking the image in either axis)\n   * Rotation about an abitrary point (Needs to be combined with translations to control the pivot)\n   * X\/Y Skewing - Essentially \"slanting\" the image into a trapezoidal shape\n   * X\/Y Flips (essentially just -ve X & Y scaling values)\n   \nA single 3x3 matrix can encapsulate all these operations (inclusive of their order). Generating these sequence of operations is done by matrix multiplying a matrix with the Affine Transformation matrix for a particular operation (i.e. rotation\/scaling). This is repeated to chain the operations together. Depending on whether row major or column major order representation is used, the order of multiplications may need to be reversed to get the proper sequence.\n\nThese augmentations are applied to both the Image and Mask, to ensure they remain aligned with each other.\n\nAn examples of how to use PIL to perform Affine Transformations is given below (although tensorflow's affine transform was ultimately used in the final pipeline).\n\n**Remarks:** Tensorflow's affine transformation, while simpler to use, has certain limitations (i.e. pivot of rotation is fixed to the centre and cannot be controlled). Additionally, Tensorflow's \"fill\" strategy for regions left blank by a transformation can only be used to specify a monochrome color. Moreover, it should also be noted that if your data pipeline requires a PIL image to numpy array conversion, Tensorflow's affine transformation offers no significant performance boost (both PIL & TF unfortunately run this on the CPU).","9198c4d9":"# Subdivision of source images\n\nThe subdivision of each 1600x1200px source image into 400x400px sub-image yields 25 sub-images. Subdivision is done in advance to reduce the computational load in the data generation pipeline, as well as to simplify the train\/test dataset split.","640a2e0c":"# Training - \\[Phase 1.2\\]\n\n10 Epochs are run on the filtered training dataset. Randomly drawn shapes on input images are drawn to coax the network into learning the characteristic physical contours of brain cells.\nSome improvement can be seen in the resultant images\n* Especially in the 5th preview image, where the contours are much sharper.\n* The 3rd preview image also shows smaller false-positive regions.","fadcfb02":"# Data Pipeline\n\nA Sequence based data generator is used in this case, since the augmentation strategies are capable of producing a multitude of variations. The training set is reshuffled after every epoch. By default, augmentations are turned off, and can be individually enabled via the *enableFeatures()* member function."}}