{"cell_type":{"ea0a9470":"code","0ff18b12":"code","8fda4613":"code","35378ae2":"code","51e822c0":"code","a878d84a":"code","bd54de5f":"code","a0df414b":"markdown","d6b48521":"markdown","379f0d85":"markdown","b9580dd2":"markdown","5824e1d2":"markdown","daa56272":"markdown","13e03fcf":"markdown"},"source":{"ea0a9470":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nfrom numpy.random import default_rng\nseed = 6\nrng  = default_rng(seed)","0ff18b12":"n_samples = 500\n\n# Test A (control)\nmu_of_A    = 1\nsigma_of_A = 1\ntest_A     = mu_of_A + sigma_of_A * rng.standard_normal(n_samples)\n\n# Test B\nmu_of_B    = 1.1\nsigma_of_B = 1\ntest_B     = mu_of_B + sigma_of_B * rng.standard_normal(n_samples)\n\n# Test C\nmu_of_C    = 4\nsigma_of_C = 1\ntest_C     = mu_of_C + sigma_of_C * rng.standard_normal(n_samples)","8fda4613":"fig, ax = plt.subplots(figsize=(10,5))\nsns.histplot(data=test_A,  binwidth=0.1, kde=True, alpha =0.4, color=\"red\")\nsns.histplot(data=test_B,  binwidth=0.1, kde=True, alpha =0.4, color=\"green\")\nsns.histplot(data=test_C,  binwidth=0.1, kde=True, alpha =0.3, color=\"blue\")\nax.axvline(x=1, linestyle='-', alpha=0.8, c='black', lw=3)\nax.axvline(x=1.1, linestyle='-', alpha=0.8, c='black', lw=3)\nax.axvline(x=4, linestyle='-', alpha=0.8, c='black', lw=3)\nplt.show();\nprint(\"Mean value of test A data = %.2f\" %np.mean(test_A))\nprint(\"Mean value of test B data = %.2f\" %np.mean(test_B))\nprint(\"Mean value of test C data = %.2f\" %np.mean(test_C))","35378ae2":"from statsmodels.stats.weightstats import ttest_ind\n\nt_test, p_value, deg_freedom = ttest_ind(test_A, test_B, alternative=\"two-sided\", usevar=\"unequal\")\nprint(\"p-value of %.3f\" % p_value,\"for the hypothesis that \u03bc_A = \u03bc_B\")\nt_test, p_value, deg_freedom = ttest_ind(test_A, test_C, alternative=\"two-sided\", usevar=\"unequal\")\nprint(\"p-value of %.3f\" % p_value,\"for the hypothesis that \u03bc_A = \u03bc_C\")","51e822c0":"n_samples = 5000\n# control\nmu_of_A    = 1\nsigma_of_A = 1\ntest_A     = mu_of_A + sigma_of_A * rng.standard_normal(n_samples)\n# test B\nmu_of_B    = 1.1\nsigma_of_B = 1\ntest_B = mu_of_B + sigma_of_B * rng.standard_normal(n_samples)\n# test C\nmu_of_C    = 4\nsigma_of_C = 1\ntest_C = mu_of_C + sigma_of_C * rng.standard_normal(n_samples)\n\nfig, ax = plt.subplots(figsize=(10,5))\nsns.histplot(data=test_A,  binwidth=0.1, kde=True, alpha =0.2, color=\"red\")\nsns.histplot(data=test_B,  binwidth=0.1, kde=True, alpha =0.2, color=\"green\")\nsns.histplot(data=test_C,  binwidth=0.1, kde=True, alpha =0.3, color=\"blue\")\nax.axvline(x=1, linestyle='-', alpha=0.8, c='black', lw=3)\nax.axvline(x=1.1, linestyle='-', alpha=0.8, c='black', lw=3)\nax.axvline(x=4, linestyle='-', alpha=0.8, c='black', lw=3)\nplt.show();\nprint(\"Mean value of test A data = %.2f\" %np.mean(test_A))\nprint(\"Mean value of test B data = %.2f\" %np.mean(test_B))\nprint(\"Mean value of test C data = %.2f\" %np.mean(test_C))\nt_test, p_value, deg_freedom = ttest_ind(test_A, test_B, alternative=\"two-sided\", usevar=\"unequal\")\nprint(\"p-value of %.3f\" % p_value,\"for the hypothesis that \u03bc_A = \u03bc_B\")\nt_test, p_value, deg_freedom = ttest_ind(test_A, test_C, alternative=\"two-sided\", usevar=\"unequal\")\nprint(\"p-value of %.3f\" % p_value,\"for the hypothesis that \u03bc_A = \u03bc_C\")","a878d84a":"from scipy.stats import norm\n\nalpha  = 0.05\nval    = 1 - alpha\/2\nprint(\"Z (alpha=0.05) = %.3f\" % norm.ppf(val))\n\npower   = 0.8\nprint(\"Z (power=80%%)  = %.3f\" % norm.ppf(power))\n\npower   = 0.9\nprint(\"Z (power=90%%)  = %.3f\" % norm.ppf(power))","bd54de5f":"n_samples = 2\n\n# control\nmu_of_A    = 1\nsigma_of_A = 1\ntest_A     = mu_of_A + sigma_of_A * rng.standard_normal(n_samples)\n# test B\nmu_of_B    = 1.1\nsigma_of_B = 1\ntest_B = mu_of_B + sigma_of_B * rng.standard_normal(n_samples)\n# test C\nmu_of_C    = 4\nsigma_of_C = 1\ntest_C = mu_of_C + sigma_of_C * rng.standard_normal(n_samples)\n\nfig, ax = plt.subplots(figsize=(10,5))\nsns.histplot(data=test_A,  binwidth=0.1, kde=True, alpha =0.2, color=\"red\")\nsns.histplot(data=test_B,  binwidth=0.1, kde=True, alpha =0.2, color=\"green\")\nsns.histplot(data=test_C,  binwidth=0.1, kde=True, alpha =0.3, color=\"blue\")\nkde_kws={\"color\": \"k\", \"lw\": 1, \"label\": \"KDE\",'linestyle':'--'}\nax.axvline(x=1, linestyle='-', alpha=0.8, c='black', lw=3)\nax.axvline(x=1.1, linestyle='-', alpha=0.8, c='black', lw=3)\nax.axvline(x=4, linestyle='-', alpha=0.8, c='black', lw=3)\nplt.show();\nprint(\"Mean value of test A data = %.2f\" %np.mean(test_A))\nprint(\"Mean value of test B data = %.2f\" %np.mean(test_B))\nprint(\"Mean value of test C data = %.2f\" %np.mean(test_C))\nt_test, p_value, deg_freedom = ttest_ind(test_A, test_B, alternative=\"two-sided\", usevar=\"unequal\")\nprint(\"p-value of %.3f\" % p_value,\"for the hypothesis that \u03bc_A = \u03bc_B\")\nt_test, p_value, deg_freedom = ttest_ind(test_A, test_C, alternative=\"two-sided\", usevar=\"unequal\")\nprint(\"p-value of %.3f\" % p_value,\"for the hypothesis that \u03bc_A = \u03bc_C\")","a0df414b":"Note: The vertical lines represent the ground truth mean values.\n\n### Perform a two-sample *t*-test","d6b48521":"### Results\nThe $p$-value for A\/B indicates that if $\\mu_A = \\mu_B$ then there is a 26% chance of observing the values we have seen here. In other words, there is a 26% chance that the data in green could just as well have been generated by our control group.\n\nOn the other hand, for the A\/C test, given $\\mu_A = \\mu_C$ the $p$-value indicates there is no chance of observing the distribution for Test C that we have seen here; the data we see for Test C are incompatible with the hypothesis that $\\mu_C = \\mu_A$.\n\n# Experiment #2: 5000 samples","379f0d85":"### Example: A\/C test with 80% power\nAnd how about our A\/C test?\n\n$$ n = \\frac{(1.96 + 0.842)^2  \\times (1^2+ 1^2)}{ (3)^2} \\approx 2 $$\n\nThis time it would only take a couple of samples! \n\nLet us put this to the test with a simulation:","b9580dd2":"This time we can see that the $p$-value indicates only a 0.3% chance that the data seen for Test B could have been the result of a model where $\\mu_B = \\mu_A$\nThis would lend weight to the alternative hypothesis that the two mean values are indeed different.\n\nIt is worth reiterating that both in the first and the second experiment the raw data for Test A and Test B were both drawn from the very same underlying distribution. In the first experiment we had a plausible hypothesis that both distributions had the same mean value, whereas in the second experiment it was plausible that the distributions had different means; this is all down to the quality of the data, *i.e.* the different number of samples in each simulation.\n\n# Power\nWe can see that in the second experiment, even though the mean values of Test A (1.02) and Test B (1.08) were actually closer together than they were in the first experiment, (Test A = 1.00, Test B = 1.07) it can be seen that this difference is much more statistically significant. This is due to the greater number of samples in the second experiment. In other words, the second experiment was much more 'powerful' than the first experiment,  the first experiment was 'under-powered'.\n\n#### How many samples do we need to detect what we are looking for? \n\nThe formula for calculating the minimum sample size required for calculating the difference in mean values of two independent populations is given by: \n\n\n$$ n = \\frac{(Z_{1-\\alpha\/2} + Z_{1-\\beta})^2 (\\sigma_A^2 + \\sigma_B^2)}{ (\\mu_B - \\mu_A)^2}  $$\n\nwhere $n$ is the number of samples in each test, and the values for $Z_{1-\\alpha\/2}$ and $Z_{1-\\beta}$ are obtained from the [percent point function](https:\/\/en.wikipedia.org\/wiki\/Quantile_function) of the Gaussian distribution (see below).\n\n### Example: A\/B test with 80% power\nSo, for our above example we would have\n\n$$ n = \\frac{(1.96 + 0.842)^2  \\times (1^2+ 1^2)}{ (0.1)^2} \\approx 1570 $$\n\nusing a [significance level](https:\/\/en.wikipedia.org\/wiki\/Statistical_significance) of $\\alpha = 0.05$ and a [power](https:\/\/en.wikipedia.org\/wiki\/Power_of_a_test), written as $1-\\beta$, of 80%, leading to $Z$ values of $1.96$ and $0.842$ respectively, we see we would need at least 1570 samples in each test.\n\nThis is telling us that in order to reliably see the difference between Test A and Test B, we have to have at least 1570 samples in our dataset; any less than this cut-off and our experiment is basically useless.\n\nHere is an example of how to calculate the percent point function values of the Gaussian distribution:","5824e1d2":"Indeed, with only two samples we obtain a $p$-value of 1%, so it does seem untenable to say that $\\mu_A = \\mu_C$ \n# Concluding observations\nIt is very important to first estimate the number of experimental samples that are required in order to see a significant difference in results *before* embarking on an experiment. Any experiment performed with fewer samples will probably be useless. This cut-off can be calculated using the power.\n\nHere we looked at a 10% increase in the mean value between tests A and B, and have seen how many samples we need in order to obtain *statistical* significance. However, it is up to you to decide whether this difference has any *real* significance. In some circumstances a 10% increase\ncould result in gains of millions of dollars, and thus is important, whilst in other situations it could represent a difference that is unworthy of pursuit, and as such it is insignificant.\n\nIt is worth mentioning that before embarking on an A\/B test we do not *a priori* know the values of $\\mu_A, \\sigma_A, \\mu_B$ and $ \\sigma_B$ needed to calculate the power. However, one could calculate $\\sigma_A$ and $\\mu_A$ from the current pre-test data that one has, which should be similar to the control group data, then assume that $\\sigma_B \\approx \\sigma_A$, and calculate the minimum $\\Delta \\mu$ that for you would be considered to be significant, or of value. One could also run quick simulations such as those performed above.\n\n# Related reading\n* [\"Two-Sample t-Test for Equal Means\"](https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda353.htm) NIST Engineering Statistics Handbook\n* [Martin Krzywinski and Naomi Altman \"*Significance, P values and t-tests*\", Nature Methods, volume **10** pages 1041-1042 (2013)](https:\/\/www.nature.com\/articles\/nmeth.2698.pdf) regarding the *one-sample $t$-test*\n* [Ronald L. Wasserstein and Nicole A. Lazar \"*The ASA Statement on p-Values: Context, Process, and Purpose*\", The American Statistician, volume **70** pages 129-133 (2016)](https:\/\/amstat.tandfonline.com\/doi\/full\/10.1080\/00031305.2016.1154108)\n* [Sander Greenland, Stephen J. Senn, Kenneth J. Rothman, John B. Carlin, Charles Poole, Steven N. Goodman and Douglas G. Altman \"*Statistical tests, P values, confidence intervals, and power: A guide to misinterpretations*\", European Journal of Epidemiology volume **31** pages 337-350 (2016)](https:\/\/link.springer.com\/content\/pdf\/10.1007\/s10654-016-0149-3.pdf)\n* [Martin Krzywinski and Naomi Altman \"*Power and sample size*\", Nature Methods, volume **10** pages 1139-1140 (2013)](https:\/\/www.nature.com\/articles\/nmeth.2738.pdf)\n* [Lei Cliftona,  Jacqueline Birks, and David A.Clifton \"*Comparing different ways of calculating sample size for two independent means: A worked example*\", Contemporary Clinical Trials Communications, volume **13** 100309 (2019)](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2451865418301285\/)","daa56272":"# Experiment #1: 500 samples\nTest A is our control group (in red), Test B (in green) has a mean value 10% greater than that of the control group, and just for fun Test C (in blue) has a mean 400% greater than that of the control group.","13e03fcf":"# Hypothesis testing: The two sample *t*-test, *p*-value and power\n\nIn this short notebook we use the ***two sample $t$-test***  to test whether or not the mean values of two samples are equal. The hypothesis of this test is that the values of the mean are indeed the same, *i.e.* $\\mu_A = \\mu_B$. To do this we shall simulate three datasets, and perform experiments with varying numbers of samples taken from the [Gaussian ditribution](https:\/\/en.wikipedia.org\/wiki\/Normal_distribution)\n\n$$f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi} } e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}$$\n\nwhere for simplicity $\\sigma_A = \\sigma_B = \\sigma_C = 1$ and the mean values, $\\mu$, are\n* test data A: $\\mu_A = 1 $\n* test data B: $\\mu_B = 1.1 $\n* test data C: $\\mu_C = 4 $\n\n(See my notebook [\"*Animated histogram of the central limit theorem*\"](https:\/\/www.kaggle.com\/carlmcbrideellis\/animated-histogram-of-the-central-limit-theorem) for a demonstration of how the averages of repeated samples eventually converge on a Gaussian distribution).\n\nWe shall perform two simulations; one using 500 samples, and another with 5000 samples. We shall then calculate the power of this particular experiment, and see that in order to obtain reliable results we really need to have at least 1570 samples.  "}}