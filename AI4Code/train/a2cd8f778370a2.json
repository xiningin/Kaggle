{"cell_type":{"c7ec9fa5":"code","0ee25503":"code","3bc52619":"code","0e8bffac":"code","8bcdcf2d":"code","a9fca93d":"code","3fb32a9c":"code","a19a0e8a":"code","73390148":"code","2e80f507":"code","1534aaf8":"code","c3bc3480":"code","a94fae57":"code","c2b7b400":"code","c79dc341":"code","2a5be429":"code","2bd16679":"code","1969cb22":"code","c47f7fe6":"code","cbd6ee6d":"code","2598737f":"code","ee92e916":"code","26d414a0":"code","4a644cd5":"code","88627ba4":"code","c3746ba6":"code","2b4ce330":"markdown","a4cc95a5":"markdown","946f31c8":"markdown","fadf4494":"markdown","09d06f62":"markdown","dd6cb0a1":"markdown","86582816":"markdown","a1420df9":"markdown","41c16dbe":"markdown","ffa49865":"markdown","f78c49d2":"markdown","f282d4d8":"markdown","27812f22":"markdown","942aae80":"markdown","ea9904bb":"markdown","7aef60e7":"markdown","662d8975":"markdown","020ee1ed":"markdown","706c7c08":"markdown","c03a6313":"markdown","0d6a7bd6":"markdown","64bdd8b9":"markdown","ad475bb5":"markdown","0c1e6aff":"markdown","77d164ec":"markdown","21a38c49":"markdown","11bb0097":"markdown","4af46474":"markdown","7d626427":"markdown"},"source":{"c7ec9fa5":"import pandas as pd                         # \ub370\uc774\ud130 \ubd84\uc11d \ub77c\uc774\ube0c\ub7ec\ub9ac\nimport numpy as np                          # \uacc4\uc0b0 \ub77c\uc774\ube0c\ub7ec\ub9ac\nfrom tqdm import tqdm                       # \uc9c4\ud589\ubc14\nfrom sklearn.metrics import roc_auc_score, mean_squared_error   # AUC \uc2a4\ucf54\uc5b4 \uacc4\uc0b0\nfrom sklearn.model_selection import KFold, StratifiedKFold   # K-fold CV    \nfrom bayes_opt import BayesianOptimization  # \ubca0\uc774\uc9c0\uc548 \ucd5c\uc801\ud654 \ub77c\uc774\ube0c\ub7ec\ub9ac  \nfrom functools import partial               # \ud568\uc218 \ubcc0\uc218 \uace0\uc815\nimport lightgbm as lgb                      # LightGBM \ub77c\uc774\ube0c\ub7ec\ub9ac\nimport warnings                             \nwarnings.filterwarnings(\"ignore\")           # \uacbd\uace0 \ubb38\uad6c \ubbf8\ud45c\uc2dc\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport time\n\nimport os\nimport gc\nimport re\n\n#Print control function\nprint('Set print')\ndef print_expand():\n    pd.set_option('display.max_columns', None)  # or 1000\n    pd.set_option('display.max_rows', 100)  # or 1000\n    pd.set_option('display.max_colwidth', -1)  # or 199\n    \ndef print_basic():\n    pd.set_option('display.max_columns', 30)  # or 1000\n    pd.set_option('display.max_rows', 30)  # or 1000\n    pd.set_option('display.max_colwidth', 50)  # or 199 \n\nprint_expand()\n\n## Chang_list\nchange_train = np.load('..\/input\/dacon-game\/change_train.npy')\nchange_test = np.load('..\/input\/dacon-game\/change_test.npy')","0ee25503":"def winner(train=None):\n    if train is None:\n        train = pd.read_pickle('..\/input\/dacon-game\/train.pkl')\n    y_train = train[['game_id', 'winner']].groupby('game_id')['winner'].max()\n    y_train.head()\n    return y_train\n\ntrain = pd.read_pickle('..\/input\/dacon-game\/train.pkl')\ny_train = winner(train)\n#change_train = np.load('..\/input\/dacon-game\/change_train.npy')\ngc.collect()\n# Chnage time to second\ntrain['time'] = train['time'].values\/\/1 * 60 + np.mod(train['time'].values, 1) * 100\ntrain['time'] = train['time'].fillna(0).astype(int)","3bc52619":"y_train_arr = [i if j==0 else abs(i-1) for i,j in zip(y_train, change_train) ]\ny_train_arr = pd.Series(y_train_arr)","0e8bffac":"def oneline_df(df, features, is_change=False, change_list=False):\n    '''\n    Args : game_id \ub2f9 \uac01 \ud50c\ub808\uc774\uc5b4 2 rows data -> game_id \ub2f9 1 row data \n        Input:\n            df : \ubcc0\uacbd\ud560 \ub370\uc774\ud130\ud504\ub808\uc784\n            features : \uacc4\uc0b0\ud560 column name\n            change_list : \uc885\uc871 \uc815\ub82c\uc744 \uc704\ud55c change_list\n        Output:\n            result : game_id \ub2f9 \ud558\ub098\uc758 row\ub97c \uac00\uc9c4 \ub370\uc774\ud130\ud504\ub808\uc784\n    '''\n    ### change_list\uc5d0 \ub530\ub77c\uc11c \ud50c\ub808\uc774\uc5b4\uc758 \uc21c\uc11c\ub97c \ubcc0\uacbd\ud558\ub294 \ud568\uc218 ###\n    def arrange_species(df, change_list):\n        cols = df.columns\n        v = df.values\n        for game_id in range(int(len(v)\/2)):\n            if change_list[game_id] == 1:\n                num = game_id *2\n                v[[num+1, num]] = v[[num, num+1]]\n\n        df = pd.DataFrame(v, columns=cols)\n        return df\n    \n    if is_change != False:\n        print('Change species')\n        df = arrange_species(df, change_train)\n        \n    c0 = df.loc[df.index[df.index%2 ==0], features]\n    c0.rename(columns={i: j for (i,j) in zip(c0.columns[1:], c0.columns[1:] + '_0')}, inplace=True)\n    c0.fillna(0, inplace=True)\n    \n    c1 = df.loc[df.index[df.index%2 ==1], features]\n    c1.rename(columns={i: j for (i,j) in zip(c1.columns[1:], c1.columns[1:] + '_1')}, inplace=True)\n    c1.fillna(0, inplace=True)\n    \n    #return c0, c1\n    print(\"c0 : {}, c1 : {}\".format(len(c0), len(c1)))\n    c_delta1 = pd.DataFrame((c0.values - c1.values), columns=(pd.Index(features) + '_delta'))\n    c_delta2 = pd.DataFrame((c0.values - c1.values)\/(c0.values + c1.values)*100, columns=(pd.Index(features) + '_delta\/Total'))\n    c_delta3 = pd.DataFrame((c0.values - c1.values), columns=(pd.Index(features) + '_delta\/Time'))\n    #print(len(m_time), len(c_delta3), len(c_delta2), len(c0))\n    for i in c_delta3.columns:\n        c_delta3[i] = c_delta3[i]\/ m_time.values * 100\n        \n    for df in [c_delta1,c_delta2,c_delta3]:\n        df.rename(columns={'game_id_delta' : 'game_id'}, inplace=True)\n        df['game_id'] = c0['game_id'].values\n    #return c0,c1\n    #c0 = c0 \/ (c0.values + c1.values) * 100\n    #c1 = c1 \/ (c0.values + c1.values) * 100\n    result = pd.merge(c0, c1, on='game_id')\n    result = pd.merge(result, c_delta1, on='game_id')\n    result = pd.merge(result, c_delta2, on='game_id')\n    result = pd.merge(result, c_delta3, on='game_id')\n    #print(result.columns[result.isnull().sum() != 0])\n    result.fillna(0, inplace=True)\n    return result","8bcdcf2d":"def data_augmentation(train_x, train_y, valid_x, valid_y):\n    def maxtime_aug(x_train):\n        for i in x_train['max_time'].unique():\n            if (i ==2)|(i ==3)|(i ==4):\n                temp = x_train[x_train['max_time'] == i]\n                temp = pd.concat([temp, temp.sample(frac=0.5)], axis=0).reset_index(drop=True)\n                x_train = pd.concat([x_train, temp], axis=0)\n            elif (i ==0)|(i ==1):\n                temp = x_train[x_train['max_time'] == i]\n                for i in range(2):\n                    x_train = pd.concat([x_train, temp], axis=0)\n        x_train = x_train.sample(frac=1).reset_index(drop=True)\n        return x_train\n    \n    train_x['winner'] = train_y\n    train_x = maxtime_aug(train_x.reset_index(drop=True))\n    train_y = train_x['winner']\n    train_x.drop(columns='winner', inplace=True)\n\n    valid_x['winner'] = valid_y\n    valid_x = maxtime_aug(valid_x.reset_index(drop=True))\n    valid_y = valid_x['winner']\n    valid_x.drop(columns='winner', inplace=True)\n    return train_x, train_y, valid_x, valid_y","a9fca93d":"print('Test lgb model')\ndef run_lgb(train, target, test, augmen=True, fold='stf_species'): #fold = False, 'stf_species', 'stf_time'\n    s_t = time.time()\n    #Set params\n    params = {'verbose': 100,\n              #'learning_rate': 0.006689885926154047,\n              'learning_rate': 0.015,\n              'metric': 'auc',\n              'bagging_freq': 8,\n              'boosting_type': 'gbdt',\n              'eval_metric': 'auc',\n              'lambda_l1': 1.8141668727328064,\n              'colsample_bytree': 0.6383562217487114,\n              'early_stopping_rounds': 274,\n              'max_depth': 12,\n              'lambda_l2': 44.68334861619927,\n              'bagging_fraction': 0.7505420286116447,\n              'num_leaves': 256,\n              'n_jobs': -1,\n              'n_estimators': 2000,\n              'objective': 'binary',\n              'seed': 42}\n    \n    #X = reduce_train.values\n    #y = target.values\n    features = train.columns\n    print('X_train : {} features \/ X_test : {} features'.format(len(train.columns), len(test.columns)))\n    \n    # define a GroupKFold strategy because we are predicting unknown installation_ids\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=200)\n    kf = KFold(n_splits=5, shuffle=True, random_state=2019)\n    #target = 'accuracy_group'\n    oof_pred = np.zeros(len(train))\n    train_pred = np.zeros(len(train))\n    important_features = pd.DataFrame()\n    important_features['features'] = features\n    y_pred = np.zeros(len(test))\n    \n    def model_training(tr_idx, val_idx,\n                       train=train, target=target, test=test, \n                       train_pred=train_pred, oof_pred=oof_pred, y_pred=y_pred, \n                       important_features=important_features,\n                       augmen=augmen):\n        print(\"=\"*50)\n        print('Training Start, fold : {}'.format(n_fold+1))\n        print(\"=\"*50)\n\n        ###Stratified Kfold\n        train_x, train_y = train.loc[tr_idx], target.loc[tr_idx]\n        valid_x, valid_y = train.loc[val_idx], target.loc[val_idx]\n\n        if augmen == True:\n            train_x, train_y, valid_x, valid_y = data_augmentation(train_x, train_y, valid_x, valid_y)\n        lgb_train = lgb.Dataset(train_x.values,train_y.values)\n        lgb_valid = lgb.Dataset(valid_x.values,valid_y.values)\n\n        model = lgb.train(params, lgb_train, num_boost_round = 100000, early_stopping_rounds = 200, \n                         valid_sets = [lgb_train, lgb_valid], verbose_eval = 300)\n\n        print('Validation auc score is :', roc_auc_score(valid_y.values, model.predict(valid_x.values)))\n        train_pred += model.predict(train.values) \/ 5\n        oof_pred[val_idx] = model.predict(train.loc[val_idx].values)\n        y_pred += model.predict(test.values) \/ 5\n\n        #Save feature importance\n        fold_importance = pd.DataFrame()\n        fold_importance['features'] = features\n        fold_importance['importance_{}'.format(n_fold)] = model.feature_importance()\n        important_features = pd.merge(important_features, fold_importance, on='features', how='left')\n        return model, train_pred, oof_pred, y_pred, important_features\n    \n    # train a baseline model\n    if fold == 'stf_species':\n        for n_fold, (tr_idx, val_idx) in enumerate(skf.split(train, train['species'])):\n            model, train_pred, oof_pred, y_pred, important_features = model_training(tr_idx, val_idx)\n    elif fold == 'stf_time':\n        for n_fold, (tr_idx, val_idx) in enumerate(skf.split(train, train['max_time'])):\n            model, train_pred, oof_pred, y_pred, important_features = model_training(tr_idx, val_idx)\n    else:\n        for n_fold, (tr_idx, val_idx) in enumerate(kf.split(train)):\n            model, train_pred, oof_pred, y_pred, important_features = model_training(tr_idx, val_idx)\n        \n    # calculate loss\n    loss_score = roc_auc_score(target, oof_pred)\n    important_features['mean'] = important_features[important_features.columns[1:]].mean(axis=1)\n    print('Model training Time : {}s'.format(time.time()- s_t))\n    print('Our Training auc score is :', roc_auc_score(target, model.predict(train)))\n    print('Our oof auc score is :', loss_score)\n    return train_pred, model, important_features, oof_pred, y_pred","3fb32a9c":"train_start = pd.read_csv('..\/input\/dacon-game\/train_start.csv')\ntrain_start = train_start.reset_index().rename(columns={'index':'game_id'})\ntest_start = pd.read_csv('..\/input\/dacon-game\/test_start.csv')\ntest_start = test_start.reset_index().rename(columns={'index':'game_id'})\n\ncategory_features = ['p0_s', 'p1_s', 'map', 'species', 'max_time']","a19a0e8a":"## train_count & test_count ##\n\ncount_train = pd.read_csv('..\/input\/dacon-game\/count_train_v2.csv')\ncount_test = pd.read_csv('..\/input\/dacon-game\/count_test_v2.csv')\n\nm_time = train.groupby('game_id')['time'].max()\nfeatures = count_train.columns[count_train.columns != 'player']\ntrain_count = oneline_df(count_train, features)\ntrain_count_arr = oneline_df(count_train, features, is_change=True, change_list=change_train)\n\n\ndef test_time():\n    test = pd.read_pickle('..\/input\/dacon-game\/test.pkl')\n    m_time = test.groupby('game_id')['time'].max()\n    return m_time\nm_time = test_time()\ntest_count = oneline_df(count_test, features)\ntest_count_arr = oneline_df(count_test, features, is_change=True, change_list=change_test)\ngc.collect()\n\n\n#c_c0, c_c1 \uc9c0\uc6b0\uae30\nc_c0 = train_count.columns[train_count.columns.str.contains('delta')][:8]\nc_c1 = train_count.columns[train_count.columns.str.contains('\/Total')]\nc_c2 = train_count.columns[train_count.columns.str.contains('\/Time')]\ntrain_count.drop(columns=c_c0, inplace=True)\ntrain_count.drop(columns=c_c1, inplace=True)\ntest_count.drop(columns=c_c0, inplace=True)\ntest_count.drop(columns=c_c1, inplace=True)","73390148":"## train_right & test_right ##\ndef fix_right(train_right, train_start):\n    train_right['species'] = train_start['species']\n    my_cols = train_right.columns[(train_right.columns.str.contains('my') == True)|(train_right.columns.str.contains('other') == True)]\n    right_cols = train_right.columns[(train_right.columns.str.contains('right') == True)]\n    train_right.loc[train_right[train_right['species'].isin([0,3,5]) == True].index, my_cols] = 0\n    train_right.loc[train_right[train_right['species'].isin([0,3,5]) == False].index, right_cols] = 0\n    r_c0 = train_right.columns[train_right.columns.str.contains('delta')][:8]\n    r_c1 = train_right.columns[train_right.columns.str.contains('\/Total')]\n    r_c2 = train_right.columns[train_right.columns.str.contains('\/Time')]\n    train_right.drop(columns='species', inplace=True)\n    train_right.drop(columns=r_c0, inplace=True)\n    train_right.drop(columns=r_c2, inplace=True)\n    return train_right\n\n#Train data\ntrain_right = pd.read_csv('..\/input\/dacon-game\/train_right.csv')\ntrain_right_arr = pd.read_csv('..\/input\/dacon-game\/train_right_arr.csv')\ntrain_right = fix_right(train_right, train_start)\ntrain_right_arr = fix_right(train_right_arr, train_start)\n#Test data\ntest_right = pd.read_csv('..\/input\/dacon-game\/test_right.csv')\ntest_right_arr = pd.read_csv('..\/input\/dacon-game\/test_right_arr.csv')\ntest_right = fix_right(test_right, test_start)\ntest_right_arr = fix_right(test_right_arr, test_start)","2e80f507":"def tt():\n    test = pd.read_pickle('..\/input\/dacon-game\/test.pkl')\n    max_time = test.groupby('game_id')['time'].max()\n    return max_time\n\nbins = [0,60,180,300,420,540,900]\nmax_time = train.groupby('game_id')['time'].max()\nt = pd.cut(max_time.values, bins, labels=[0,1,2,3,4,5])\ntrain_start['max_time'] = t\n\ntest_t = tt()\ngc.collect()\nt2 = pd.cut(test_t.values, bins, labels=[0,1,2,3,4,5])\ntest_start['max_time'] = t2","1534aaf8":"train_unit = pd.read_csv('..\/input\/dacon-game\/train_unit_v5_1.csv')\ntrain_unit_arr = pd.read_csv('..\/input\/dacon-game\/train_unit_v5_1_arr.csv')\ntest_unit = pd.read_csv('..\/input\/dacon-game\/test_unit_v5_1.csv')\ntest_unit_arr = pd.read_csv('..\/input\/dacon-game\/test_unit_v5_1_arr.csv')\n\ntrain_unit.drop(columns='game_id_delta', inplace=True)\ntest_unit.drop(columns='game_id_delta', inplace=True)","c3bc3480":"train_vision = pd.read_csv('..\/input\/dacon-game\/train_vision_v6.csv')\ntrain_vision_arr = pd.read_csv('..\/input\/dacon-game\/train_vision_v6_arr.csv')\ntest_vision = pd.read_csv('..\/input\/dacon-game\/test_vision_v6.csv')\ntest_vision_arr = pd.read_csv('..\/input\/dacon-game\/test_vision_v6_arr.csv')","a94fae57":"player_train = pd.read_csv('..\/input\/dacon-game\/player_unit_test_v3.csv')\n\nplayer_train.drop(columns=player_train.columns[player_train.columns.get_loc('Worker_MINERALS') :\n                                               player_train.columns.get_loc('Upgrade_Count')], inplace=True)\n\nm_time = train.groupby('game_id')['time'].max()\nfeatures = player_train.columns[player_train.columns != 'player']\ntrain_player = oneline_df(player_train, features, is_change=True, change_list=change_train)\ntrain_player.drop(columns=train_player.columns[train_player.columns.get_loc('core_c_delta'):], inplace=True)\ntrain_player_arr = oneline_df(player_train, features, is_change=True, change_list=change_train)\ntrain_player_arr.drop(columns=train_player_arr.columns[train_player_arr.columns.get_loc('core_c_delta'):], inplace=True)\n\n#Test\nplayer_test = pd.read_csv('..\/input\/dacon-game\/player_unit_train_v3.csv')\nplayer_test.drop(columns=player_test.columns[player_test.columns.get_loc('Worker_MINERALS') :\n                                               player_test.columns.get_loc('Upgrade_Count')], inplace=True)\nm_time = test_time()\ntest_player = oneline_df(player_test, features)\ntest_player.drop(columns=test_player.columns[test_player.columns.get_loc('core_c_delta'):], inplace=True)\ntest_player_arr = oneline_df(player_test, features, is_change=True, change_list=change_test)\ntest_player_arr.drop(columns=test_player_arr.columns[test_player_arr.columns.get_loc('core_c_delta'):], inplace=True)","c2b7b400":"won_train = pd.read_csv('..\/input\/dacon-game\/traindata_won.csv')\nwon_test =pd.read_csv('..\/input\/dacon-game\/testdata_won.csv')\n\nwon_train_arr = pd.read_csv('..\/input\/dacon-game\/won_Pretrain4.csv')\nwon_test_arr = pd.read_csv('..\/input\/dacon-game\/won_Pretest4.csv')\n\nwon_train_arr.drop(columns=won_train_arr.columns[:won_train_arr.columns.get_loc('delta_SetControlGroup')], inplace=True)\nwon_test_arr.drop(columns=won_test_arr.columns[:won_test_arr.columns.get_loc('delta_SetControlGroup')], inplace=True)\n\nwon_train.drop(columns=['Unnamed: 0', 'game_id', 'SUPPLY_UNITS_GAS1', 'winner'], inplace=True, errors='ignore')\nwon_test.drop(columns=['winner', 'Unnamed: 0', 'game_id', 'SUUPLY_UNITS_GAS'], inplace=True, errors='ignore')\n\nwon_train_arr.drop(columns=['winner', 'SUPPLY_UNITS_GAS1'], inplace=True)\nwon_test_arr.drop(columns='SUPPLY_UNITS_GAS.1', inplace=True)\n\n\ndef make_gameId(data):\n    data = data.reset_index()\n    data.rename(columns={'index':'game_id'}, inplace=True)\n    return data\n\nwon_train = make_gameId(won_train)\nwon_test = make_gameId(won_test)\nwon_train_arr = make_gameId(won_train_arr)\nwon_test_arr = make_gameId(won_test_arr)","c79dc341":"def make_dataset(count=True, unit=True, vision=True, player=False, won=False, right=False, arr=False):\n    except_feature = [count, unit, vision, player, won, right]\n    if arr == False:\n        train_dataset = [train_count, train_unit, train_vision, train_player, won_train, train_right]\n        test_dataset = [test_count, test_unit, test_vision, test_player, won_test, test_right]\n    elif arr == True:\n        train_dataset = [train_count_arr, train_unit_arr, train_vision_arr, train_player_arr, won_train_arr, train_right_arr]    \n        test_dataset = [test_count_arr, test_unit_arr, test_vision_arr, test_player_arr, won_test_arr, test_right_arr]\n\n    x_train = train_start.copy()\n    for dels, dataset in zip(except_feature, train_dataset):\n        if dels == False:\n            continue\n        else:\n            x_train = pd.merge(x_train, dataset, on='game_id', how='left')\n            \n    x_test = test_start.copy()\n    for dels, dataset in zip(except_feature, test_dataset):\n        if dels == False:\n            continue\n        else:\n            x_test = pd.merge(x_test, dataset, on='game_id', how='left')\n    \n    x_train[['p0_s', 'p1_s', 'map', 'species', 'max_time']] = x_train[['p0_s', 'p1_s', 'map', 'species', 'max_time']].astype('category')\n    x_test[['p0_s', 'p1_s', 'map', 'species', 'max_time']] = x_test[['p0_s', 'p1_s', 'map', 'species', 'max_time']].astype('category')\n    x_train.drop(columns='game_id', inplace=True)\n    x_test.drop(columns='game_id', inplace=True)\n    \n    return x_train, x_test","2a5be429":"del train\ntrain = []\ngc.collect()","2bd16679":"#\ubaa8\ub4e0 \uac12\uc774 \ub3d9\uc77c\ud55c feature\ub97c \uc81c\uc678\ud558\ub294 \ud568\uc218\ndef unique_columns_delete(x_train, x_test):\n    del_cols = list()\n    print(\"Before columns : {}\".format(len(x_train.columns)))\n    for i in x_train.columns:\n        if x_train[i].nunique() == 1:\n            #print(\"uniq 1 : {}\".format(i))\n            del_cols.append(i)\n        elif x_train[i].isnull().sum() == len(x_train):\n            del_cols.append(i)\n            #print(\"All null1 : {}\".format(i))\n            \n    x_train.drop(columns=del_cols, inplace=True)\n    x_test.drop(columns=del_cols, inplace=True)\n    print(\"After columns : {} \/\/ {} columns Delete!\".format(len(x_train.columns), len(del_cols)))\n    return x_train, x_test","1969cb22":"#\ub3c5\ub9bd\ubcc0\uc218\uac04 \uc0c1\uad00\uacc4\uc218\uac00 0.997 \uc774\uc0c1\uc778 feature \uc81c\uac70\ndef correlation_delete(x_train, x_test):\n    features = x_test.columns\n    features = features[features.isin(category_features)==False] #except categorical features\n    print(f\"Before columns : {len(x_test.columns)}\")\n    to_remove = []\n    except_cols = []\n    \n    def print_corr(feat_a, feat_b):\n        c = np.corrcoef(x_train[feat_a].values, x_train[feat_b].values)[0][1]\n        if c>0.997:\n            print('FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(feat_a, feat_b, c))\n            return feat_b\n    \n    for feat_a in features:\n        result = filter(None, [print_corr(feat_a, feat_b) for feat_b in features[(features.isin(except_cols)==False)&(features!=feat_a)]])\n        to_remove += result\n        except_cols += result\n        except_cols.append(feat_a)\n        \n    x_train.drop(columns=to_remove, inplace=True)\n    x_test.drop(columns=to_remove, inplace=True)\n    print(f\"After columns : {len(x_test.columns)} \/\/ {len(to_remove)} columns Delete!\")\n    return x_train, x_test","c47f7fe6":"def save_submission(pred, name='', arr=False):\n    if arr==True:\n        pred = [i if j==0 else 1-i for i,j in zip(pred, change_test)]\n    submission = pd.read_csv('..\/input\/dacon-game\/sample_submission.csv', index_col=0)\n    submission['winner'] = submission['winner'] + pred\n    name = str(f'submission_{name}.csv')\n    submission.to_csv(name)","cbd6ee6d":"#Add feature selection\nx_train, x_test = make_dataset(unit=True, vision=True, player=True, won=True, arr=True)\nx_train, x_test = unique_columns_delete(x_train, x_test)\nx_train, x_test = correlation_delete(x_train, x_test)\n\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train_arr, x_test, fold='stf_time')\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model5_arr', arr=True)","2598737f":"#Add feature selection\nx_train, x_test = make_dataset(unit=True, vision=True, player=True, won=True, right=True, arr=True)\nx_train, x_test = unique_columns_delete(x_train, x_test)\nx_train, x_test = correlation_delete(x_train, x_test)\n\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train_arr, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model7_arr(time)', arr=True)","ee92e916":"#Add feature selection\nx_train, x_test = make_dataset(unit=True, vision=True, player=True, won=True, right=True, arr=True)\nx_train, x_test = unique_columns_delete(x_train, x_test)\nx_train, x_test = correlation_delete(x_train, x_test)\n\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train_arr, x_test, fold='stf_time')\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model7_arr', arr=True)","26d414a0":"#Add feature selection\nx_train, x_test = make_dataset(unit=True, vision=True, player=True, won=True, right=True, arr=True)\nx_train, x_test = unique_columns_delete(x_train, x_test)\nx_train, x_test = correlation_delete(x_train, x_test)\n\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train_arr, x_test, fold=False)\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model7_arr', arr=True)","4a644cd5":"'''from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nunit_data = pd.concat([train_unit, test_unit], axis=0).reset_index(drop=True)\n\nunit_data.drop(columns=['is_clock_delta', 'is_detector_delta', 'is_clock_delta\/total', 'is_detector_delta\/total', 'game_id_delta'], inplace=True)\n\n#temp = pd.DataFrame(StandardScaler().fit_transform(train_unit[train_unit.columns[-85:]]))\npca = PCA(n_components=1)\ndelta_result = pd.DataFrame(index = unit_data.index)\ndel_delta = list()\nfor i in unit_data.columns[-80:-40]:\n    col_name = i + \"\/total\"\n    del_delta += [i, col_name]\n    delta_result[str(i+'_pca')] = pca.fit_transform(StandardScaler().fit_transform(unit_data[[i, col_name]]))\n\nunit_data = pd.concat([unit_data, delta_result], axis=1)\nunit_data.drop(columns=del_delta, inplace=True)\n\ncol2 = ['G_unit_MINERALS_0', 'G_unit_GAS_0', 'G_unit_SUPPLY_0', 'G_unit_Count_0']\nx_2 = StandardScaler().fit_transform(data[col2])\npca = PCA(n_components=2)\npca2 = pd.DataFrame(pca.fit_transform(x_2), columns=['G_unit_pca1_0', 'G_unit_pca2_0'])\nprint(pca.explained_variance_ratio_)\n\ncol3 = ['A_unit_MINERALS_0', 'A_unit_GAS_0', 'A_unit_SUPPLY_0', 'A_unit_Count_0']\nx_3 = StandardScaler().fit_transform(data[col3])\npca3 = pd.DataFrame(pca.fit_transform(x_3), columns=['A_unit_pca1_0', 'A_unit_pca2_0'])\nprint(pca.explained_variance_ratio_)\n\ncol4 = ['Build_MINERALS_0', 'Build_GAS_0', 'Build_Count_0']\nx_4 = StandardScaler().fit_transform(data[col4])\npca = PCA(n_components=2)\npca4 = pd.DataFrame(pca.fit_transform(x_4), columns=['Build_pca1_0', 'Build_pca2_0'])\nprint(pca.explained_variance_ratio_)\n\ncol5 = ['Research_MINERALS_0','Research_GAS_0','Research_Count_0','Upgrade_MINERALS_0','Upgrade_GAS_0','Upgrade_Count_0']\nx_5 = StandardScaler().fit_transform(data[col5])\npca5 = pd.DataFrame(pca.fit_transform(x_5), columns=['UR_pca1_0', 'RU_pca2_0'])\nprint(pca.explained_variance_ratio_)\n\nfor col, df in zip([col2, col3, col4, col5], [pca2, pca3, pca4, pca5]):\n    unit_data = pd.concat([unit_data, df], axis=1)\n    unit_data.drop(columns=col, inplace=True)\n\ncol2 = ['G_unit_MINERALS_1', 'G_unit_GAS_1', 'G_unit_SUPPLY_1', 'G_unit_Count_1']\nx_2 = StandardScaler().fit_transform(data[col2])\npca = PCA(n_components=2)\npca2 = pd.DataFrame(pca.fit_transform(x_2), columns=['G_unit_pca1_1', 'G_unit_pca2_1'])\nprint(pca.explained_variance_ratio_)\n\ncol3 = ['A_unit_MINERALS_1', 'A_unit_GAS_1', 'A_unit_SUPPLY_1', 'A_unit_Count_1']\nx_3 = StandardScaler().fit_transform(data[col3])\npca3 = pd.DataFrame(pca.fit_transform(x_3), columns=['A_unit_pca1_1', 'A_unit_pca2_1'])\nprint(pca.explained_variance_ratio_)\n\ncol4 = ['Build_MINERALS_1', 'Build_GAS_1', 'Build_Count_1']\nx_4 = StandardScaler().fit_transform(data[col4])\npca = PCA(n_components=2)\npca4 = pd.DataFrame(pca.fit_transform(x_4), columns=['Build_pca1_1', 'Build_pca2_1'])\nprint(pca.explained_variance_ratio_)\n\ncol5 = ['Research_MINERALS_1','Research_GAS_1','Research_Count_1','Upgrade_MINERALS_1','Upgrade_GAS_1','Upgrade_Count_1']\nx_5 = StandardScaler().fit_transform(data[col5])\npca5 = pd.DataFrame(pca.fit_transform(x_5), columns=['UR_pca1_1', 'RU_pca2_1'])\nprint(pca.explained_variance_ratio_)\n\nfor col, df in zip([col2, col3, col4, col5], [pca2, pca3, pca4, pca5]):\n    unit_data = pd.concat([unit_data, df], axis=1)\n    unit_data.drop(columns=col, inplace=True)\n\ncol2 = ['G_unit_MINERALS_delta_pca', 'G_unit_GAS_delta_pca', 'G_unit_SUPPLY_delta_pca', 'G_unit_Count_delta_pca']\nx_2 = StandardScaler().fit_transform(unit_data[col2])\npca = PCA(n_components=2)\npca2 = pd.DataFrame(pca.fit_transform(x_2), columns=['G_unit_pca1_delta_pca', 'G_unit_pca2_delta_pca'])\nprint(pca.explained_variance_ratio_)\n\ncol3 = ['A_unit_MINERALS_delta_pca', 'A_unit_GAS_delta_pca', 'A_unit_SUPPLY_delta_pca', 'A_unit_Count_delta_pca']\nx_3 = StandardScaler().fit_transform(unit_data[col3])\npca3 = pd.DataFrame(pca.fit_transform(x_3), columns=['A_unit_pca1_delta_pca', 'A_unit_pca2_delta_pca'])\nprint(pca.explained_variance_ratio_)\n\ncol4 = ['Build_MINERALS_delta_pca', 'Build_GAS_delta_pca', 'Build_Count_delta_pca']\nx_4 = StandardScaler().fit_transform(unit_data[col4])\npca = PCA(n_components=2)\npca4 = pd.DataFrame(pca.fit_transform(x_4), columns=['Build_pca1_delta_pca', 'Build_pca2_delta_pca'])\nprint(pca.explained_variance_ratio_)\n\ncol5 = ['Research_MINERALS_delta_pca','Research_GAS_delta_pca','Research_Count_delta_pca','Upgrade_MINERALS_delta_pca','Upgrade_GAS_delta_pca','Upgrade_Count_delta_pca']\nx_5 = StandardScaler().fit_transform(unit_data[col5])\npca5 = pd.DataFrame(pca.fit_transform(x_5), columns=['UR_pca1_delta_pca', 'RU_pca2_delta_pca'])\nprint(pca.explained_variance_ratio_)\n\nfor col, df in zip([col2, col3, col4, col5], [pca2, pca3, pca4, pca5]):\n    unit_data = pd.concat([unit_data, df], axis=1)\n    unit_data.drop(columns=col, inplace=True)\n    \ntrain_unit2 = unit_data.iloc[:38872].reset_index(drop=True)\ntest_unit2 = unit_data.iloc[38872:].reset_index(drop=True)'''","88627ba4":"'''train_pred, temp_model, important_features, oof, y_pred = run_lgb(x_train, y_train, x_test)\nprint('Auc Score : {}'.format(roc_auc_score(y_train, temp_model.predict(x_train.values))))\n\nimportant_features['mean'] = important_features[important_features.columns[1:]].mean(axis=1)\nimportant_features.sort_values('mean', ascending=False).head(20)\n\nsubmission = pd.read_csv('..\/input\/dacon-game\/sample_submission.csv', index_col=0)\nsubmission['winner'] = submission['winner'] + pred\nsubmission.to_csv('submission.csv')\n\nper_fetures = pd.read_csv('..\/input\/dacon-game\/permutation_importance.csv')'''","c3746ba6":"'''features = x_test.columns\n\ncounter = 0\nto_remove = []\nfor feat_a in features:\n    for feat_b in features:\n        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n            c = np.corrcoef(x_train[feat_a], x_train[feat_b])[0][1]\n            if c > 0.997:\n                counter += 1\n                to_remove.append(feat_b)\n                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))\n                \nx_train.drop(columns=to_remove, inplace=True)\nx_test.drop(columns=to_remove, inplace=True)'''","2b4ce330":"### Save submission","a4cc95a5":"\ucc38\uace0 \uc0ac\uc774\ud2b8 \n\n* Build order C, http:\/\/sc2calc.org\/build_order\/\n* tf.net, https:\/\/tl.net\/forum\/starcraft-2\/168348-scfusion-wol-hots-and-lotv-build-order-optimizer\n* unit stats, https:\/\/liquipedia.net\/starcraft2\/Terran_Unit_Statistics_(Wings_of_Liberty_and_Heart_of_the_Swarm)\n\n** \uc815\uaddc\ud45c\ud604\uc2dd\n\n* http:\/\/pythonstudy.xyz\/python\/article\/401-%EC%A0%95%EA%B7%9C-%ED%91%9C%ED%98%84%EC%8B%9D-Regex\n* https:\/\/whatisthenext.tistory.com\/116","946f31c8":"## Right Click","fadf4494":"* V.18 : test \ub370\uc774\ud130\n*  V.18 : Selection unit \uc218\ub97c \uace0\ub824\ud55c player unit count features, \uacc4\uc0b0\ud55c \uc720\ub2db\uc744 \ubc14\ud0d5\uc73c\ub85c \uc790\uc6d0, supply, count \uacc4\uc0b0\n*  V.12: ability event \uad00\ub828 Warp\uc640 Morph \ucd94\uac00\n* V.9, 10, 11 :  camera event \uad00\ub828 feature \uacc4\uc0b0\n* V.3 : ability evnet \uad00\ub828 feature \uacc4\uc0b0","09d06f62":"## Test Model","dd6cb0a1":"## 1. Model 1(basic F + count F)\n\nx_train, x_test = make_dataset(unit=False, vision=False, player=False)\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)\n#save_submission(pred, 'sample')","86582816":"## PCA","a1420df9":"## Max time","41c16dbe":"## V.6\n- model 5, 6 output\n- model 7 : \ub450 \ud50c\ub808\uc774\uc5b4 \uac04 \uc2dc\uac04\ucc28\uc774 \ucd94\uac00 \ubc0f categorical features\ub85c \ubcc0\ud658 \ud6c4 kfold\uc5d0 \ub123\uae30","ffa49865":"## Making train data","f78c49d2":"## Model 2. (basic F + count F + Unit F)\n\nx_train, x_test = make_dataset(unit=True, vision=False, player=False)\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)","f282d4d8":" # Modeling","27812f22":"## Unit","942aae80":"### Feature selection args","ea9904bb":"## Model 5. player_unit \ubd99\uc774\uae30\n\nx_train, x_test = make_dataset(unit=True, vision=True, won=True, player=True, arr=False)\nx_train, x_test = unique_columns_delete(x_train, x_test)\nx_train, x_test = correlation_delete(x_train, x_test)\n\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train_arr, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model5')\n\ndef extract_data(x_train, y_train, species):\n    if len(species)> 1:\n        idx = x_train[x_train['species'].isin(species)].index\n    else:\n        idx = x_train[x_train['species']== species].index\n    x_train = x_train.loc[idx].reset_index(drop=True).copy()\n    y_train = y_train.loc[idx].reset_index(drop=True).copy()\n    return x_train, y_train\n\n#Add feature selection\nx_train, x_test = make_dataset(unit=True, vision=True, player=True, won=True, arr=True)\nx_train, x_test = unique_columns_delete(x_train, x_test)\nx_train, x_test = correlation_delete(x_train, x_test)\n\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train_arr, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model5_arr', arr=True)","7aef60e7":"## Vision","662d8975":"## V.7\n\nrows \ucc28\uc774 \ucd94\uac00\n\ngame_count =  train.groupby(['game_id', 'player'])['time'].count()\ngame_count = game_count.reset_index()\ngame_count['time'] = game_count['time'].astype('int16')\n\ntrain_start['time_gap'] = np.round(((game_count.loc[game_count.index%2==0]['time'].values - game_count.loc[game_count.index%2==1]['time'].values) \/\n                           (game_count.loc[game_count.index%2==0]['time'].values + game_count.loc[game_count.index%2==1]['time'].values)) *100, 2)","020ee1ed":"## Player_unit","706c7c08":"## Count","c03a6313":"## Model 6 : traindata_won\n\nx_train = pd.read_csv('..\/input\/dacon-game\/traindata_won.csv')\nx_train.drop(columns=['Unnamed: 0', 'game_id', 'SUUPLY_UNITS_GAS', 'winner'], inplace=True, errors='ignore')\nx_test = pd.read_csv('..\/input\/dacon-game\/testdata_won.csv')\nx_test.drop(columns=['winner', 'Unnamed: 0', 'game_id', 'SUPPLY_UNITS_GAS1'], inplace=True, errors='ignore')\n\ntrain_pred, temp_model, important_features, oof, y_pred2 = run_lgb(x_train, y_train, x_test, augmen=False, fold=False)\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model6')","0d6a7bd6":"## Model 3(basic F + count F + unit F + vision F)\n\nx_train, x_test = make_dataset(unit=True, vision=True, player=False)\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)\n\nx_train, x_test = make_dataset(unit=True, vision=True, player=False, arr=True)\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train_arr, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model3_arr')","64bdd8b9":"### permutation","ad475bb5":"## Basic","0c1e6aff":"## Model baseline","77d164ec":"## Model 7 : +right","21a38c49":"## V6 : \uc2dc\uac04 \ucc28\uc774 \ubcc0\uc218 \ucd94\uac00","11bb0097":"## \uc885\uc871 \ubcc4 \uc815\ub82c \ub9de\ucdb0\ubcf4\uae30","4af46474":"## Won data","7d626427":"## Model 4. won \ubd99\uc774\uae30\n\nx_train, x_test = make_dataset(unit=True, vision=True, player=False, won=True, arr=False)\nx_train, x_test = unique_columns_delete(x_train, x_test)\nx_train, x_test = correlation_delete(x_train, x_test)\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model4')\n\n#Add feature selection\nx_train, x_test = make_dataset(unit=True, vision=True, player=False, won=True, arr=True)\nx_train, x_test = unique_columns_delete(x_train, x_test)\nx_train, x_test = correlation_delete(x_train, x_test)\ntrain_pred, temp_model, important_features, oof, pred = run_lgb(x_train, y_train_arr, x_test)\nimportant_features.sort_values('mean', ascending=False).head(20)\nsave_submission(pred, 'model4_arr')"}}