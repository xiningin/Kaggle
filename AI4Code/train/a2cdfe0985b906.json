{"cell_type":{"5fc5ce22":"code","a601d8f1":"code","776cd860":"code","8a2fb6e9":"code","34bd5b7d":"code","000f4990":"code","0436d698":"code","e108d924":"code","72dd9367":"code","4b748522":"code","35e37c6f":"code","f2998d18":"code","55d045cd":"code","b8a357d5":"code","9fda627b":"code","63dc4fa7":"code","4a9cdbbe":"code","9cedd1d9":"code","caf634a0":"code","4dce0946":"code","eb4d6a12":"code","668da002":"code","58c623f5":"code","316c3a62":"code","a94c9c12":"code","16c9b15a":"code","2257497e":"code","81e079c3":"code","6f9efb4f":"code","ec6a656a":"code","9bd63c8f":"code","366f8ab2":"code","fd4aaf6b":"code","883b2ecc":"code","f9f07bcd":"code","3771159d":"code","8634e252":"code","eb216eb9":"code","b5b0dc15":"code","3532c554":"code","19a05fe5":"code","28070b64":"code","977978ca":"code","7d89a448":"code","995d4a0d":"code","0d29085c":"code","0e1dfc24":"code","547f59c2":"code","a0aab5e3":"code","b97614a7":"code","b2269988":"code","42857def":"code","84fa9f83":"code","e0809e7a":"code","d99d1ef6":"code","8def7900":"code","762fc5a4":"code","cec00e08":"code","20ce6ea1":"code","b6810f56":"code","18301a97":"code","791ede42":"code","868d2545":"code","3cdbeeec":"code","3a6f2311":"code","92fe598a":"code","d7ff2d4d":"code","8fc0a9cc":"code","aea75712":"code","2e9fe9ac":"code","d5fc9cff":"code","17f714e8":"code","d802c600":"code","1627dc4b":"code","83723521":"code","faec8399":"code","3a353c0f":"code","bcdd1dde":"code","23c854ce":"code","d96b135f":"code","111977e4":"code","f7d2da0b":"code","88f17ddf":"code","c1ee5d78":"code","3b22f200":"code","02b7a73f":"code","d4d68692":"code","2477ccfd":"code","478efa79":"code","e791cd48":"code","c8e34fce":"code","8374949d":"code","a3d164c5":"code","f3407f47":"code","fd8d88a4":"code","1c0b1428":"code","fa12da75":"code","cf6dfd47":"code","9b3b67fe":"code","7e6fb389":"code","0c4c9f27":"code","e7f7e4b5":"code","3c0b4917":"markdown","c385fedf":"markdown","fa456707":"markdown","5b9aac30":"markdown","4e361953":"markdown","6a452ae2":"markdown","50193f85":"markdown","45a94f61":"markdown","7a25ac58":"markdown","0ee45838":"markdown","04c0e87f":"markdown","21c1a5f6":"markdown","a033a588":"markdown","32542547":"markdown","a1aee853":"markdown","4dab8b5c":"markdown","f775e066":"markdown","ee8e966d":"markdown","46be6775":"markdown","3ad726a3":"markdown","f485d595":"markdown","fe201ab2":"markdown","53d152ef":"markdown","047a4692":"markdown","989c6b0c":"markdown","3c0867b0":"markdown","ec1869b0":"markdown","372c5bca":"markdown","dbd815fb":"markdown","fd969561":"markdown","571ec303":"markdown","2cad61d4":"markdown","a7a9d91e":"markdown","65dcce66":"markdown","b789ee97":"markdown","ecbf520a":"markdown","ec1d949d":"markdown","b63c7b08":"markdown","46b99089":"markdown","427627f2":"markdown","bbf26cec":"markdown","f5468489":"markdown","f155e096":"markdown","cac65e90":"markdown","0bd44ceb":"markdown","0ee36b67":"markdown","f0d189c8":"markdown","40ad7ceb":"markdown","abf13e22":"markdown","3904db80":"markdown","ead1959f":"markdown","3f4bc796":"markdown","519f2384":"markdown","22bc3713":"markdown","b73dbc8a":"markdown","30d465ff":"markdown","2e4ca403":"markdown","2073f958":"markdown","19fdc9a0":"markdown","1783716b":"markdown","105d6103":"markdown","38568d08":"markdown","93c7bdb9":"markdown","f86372b1":"markdown","d0993ec2":"markdown","ee8be0ad":"markdown","ff27d25a":"markdown","ee3d4f67":"markdown","1f94a243":"markdown","90a62a71":"markdown","de755cb4":"markdown","1ebe2300":"markdown","5d840a1b":"markdown","864a5e5d":"markdown","98eb8264":"markdown","f7312dcf":"markdown","60d9266e":"markdown","7707f25a":"markdown","3916f52f":"markdown","b2f92670":"markdown","d6bb85a6":"markdown","5118c960":"markdown","620a115b":"markdown","fc5c62fc":"markdown","5b6d9f40":"markdown","e7aa8dde":"markdown","4642dfe8":"markdown","06f30d43":"markdown","94dfcb57":"markdown","4e636b37":"markdown","b47c0b5a":"markdown","50679c2b":"markdown","58aaa50f":"markdown","d81ce4ef":"markdown","219f6556":"markdown"},"source":{"5fc5ce22":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a601d8f1":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\n\nimport seaborn as sns\nsns.set_style('white')\n\npd.set_option('display.max_columns', None) # Display all columns \npd.set_option('display.max_rows', None) # Display all rows\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nrandom_state=0","776cd860":"path_data_description = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt'\n\nwith open(path_data_description, 'r') as file:\n    data_description = file.read()\n    print(data_description)","8a2fb6e9":"path_train = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\n\ntrain_orig = pd.read_csv(path_train)\n\n# Make a copy of the training set to use during EDA\ntrain = train_orig.copy()\n\ntrain.head()","34bd5b7d":"train.info()","000f4990":"# Change 'Id' and 'MSSubClass' to categorical type\n\ntrain['Id'] = train['Id'].astype('category')\ntrain['MSSubClass'] = train['MSSubClass'].astype('category')","0436d698":"train.dtypes.value_counts()","e108d924":"high_nans = [feature for feature in train.columns if train[feature].isnull().sum() \/ len(train) > 0.5]\nhigh_nans","72dd9367":"train.describe()","4b748522":"train.hist(bins=50, figsize=(20,12))\nplt.subplots_adjust(top=1.5)\nplt.show()","35e37c6f":"# https:\/\/www.scikit-yb.org\/en\/latest\/api\/target\/binning.html\n\nfrom yellowbrick.target import BalancedBinningReference\n\n# Instantiate the visualizer\nvisualizer = BalancedBinningReference(bins=4)\n\nvisualizer.fit(train['SalePrice'])        # Fit the data to the visualizer\nprint(visualizer.bin_edges_)\nvisualizer.show()                         # Finalize and render the figure","f2998d18":"train['SalePrice_bins'] = pd.cut(train['SalePrice'], [0, 214925, 394950, np.inf])\ntrain['SalePrice_bins'].value_counts()","55d045cd":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=random_state)\n\n# The method 'split' from the StratifiedShuffleSplit class returns 2 lists of indices \n# for slicing the data into a training set and a holdout set\n\nfor train_index, hold_index in split.split(train, train['SalePrice_bins']):\n    strat_train = train.loc[train_index]\n    strat_hold = train.loc[hold_index]\n\n# Check the proportion of entries in each bin for the holdout set\nprint('Ratio of bin sizes in training and holdout sets:')\nprint('')\nprint('Stratified train set:')\nprint(strat_train['SalePrice_bins'].value_counts(sort=False) \/ len(strat_train))\nprint('')\nprint('Stratified hold-out set:')\nprint(strat_hold['SalePrice_bins'].value_counts(sort=False) \/ len(strat_hold))","b8a357d5":"for set_ in (strat_train, strat_hold):\n    set_.drop('SalePrice_bins', axis=1, inplace=True)\n    set_.reset_index(drop=True, inplace=True)","9fda627b":"plt.figure(figsize=(12,8))\nplt.scatter(strat_train['GrLivArea'], strat_train['SalePrice'], alpha=0.5)\nplt.xlabel('Living Area: sqft (\\'GrLivArea\\')')\nplt.ylabel('SalePrice')","63dc4fa7":"# Condition for dropping\ncond = strat_train['GrLivArea'] > 4000\n\n# Select subset of training data that does not meet that condition\nstrat_train = strat_train.loc[~cond]\n\n# Reset index\nstrat_train.reset_index(drop=True, inplace=True)","4a9cdbbe":"num_types = ['int64', 'float64']\nnum_features = [feature for feature in strat_train.columns if strat_train[feature].dtype in num_types]\n\ncat_features = [feature for feature in strat_train.columns if feature not in num_features]\ncat_features.remove('Id')\n\n# By using copy(), we make sure that we can explore and manipulate without touching the original data.\ntrain_num = strat_train[num_features].copy()\ntrain_cat = strat_train[cat_features].copy()","9cedd1d9":"train_num.info()","caf634a0":"train_cat.info()","4dce0946":"# Create correlation table\ncorr_matrix = train_num.corr()\n\n# Plot heatmap\nplt.figure(figsize=(20, 20))\nsns.heatmap(corr_matrix, vmin=-1, vmax=1, center=0, cmap='coolwarm', cbar=True)\nplt.show()","eb4d6a12":"selected_features = ['EnclosedPorch', 'YearBuilt', 'BsmtFullBath', 'BsmtFinSF2', 'BsmtUnfSF',\n                    'TotRmsAbvGrd', 'GrLivArea', 'BedroomAbvGr', 'GarageArea', 'GarageCars',\n                    'TotalBsmtSF', '1stFlrSF']\n\ncorr_selected = train_num[selected_features].corr()\n\nplt.figure(figsize=(9, 9))\nsns.heatmap(corr_selected, vmin=-1, vmax=1, center=0, cmap='coolwarm', cbar=True, annot=True, fmt='.1f', square=True)\nplt.show()","668da002":"corr_matrix = train_num.corr()\n\nplt.figure(figsize=(2, 25))\nsns.heatmap(corr_matrix[['SalePrice']].sort_values(by='SalePrice', ascending=False), \n            vmin=-1, vmax=1, center=0, cmap='coolwarm', annot=True, fmt='.1f', cbar=False)\nplt.show()","58c623f5":"selected_features = [feature for feature in corr_matrix.columns if corr_matrix.loc[feature, 'SalePrice'] > 0.5]\nselected_features","316c3a62":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(train_num[selected_features], figsize=(16, 16))\nplt.show()","a94c9c12":"train_num.isna().sum()","16c9b15a":"train_num[['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].describe()","2257497e":"# Add SalePrice column to categorical dataframe so we can use it in histograms\ntrain_cat['SalePrice'] = strat_train['SalePrice'].copy()","81e079c3":"import re\n\ndef split_description():\n    \"\"\"\n    Split the description text using a regular expression.\n    \"\"\"\n    # The regex search pattern looks for spaces, followed by alpha-numeric characters, followed by ':'\n    # The parantheses contain the pattern to be returned as its own item\n    expression = '\\s+([0-9A-Za-z]+:)'\n    description_split = re.split(expression, data_description)\n        \n    return description_split\n\n\ndef join_items(list_items):\n    \"\"\"\n    Join the name of each category (e.g. 'LotFrontage:') \n        with its description (e.g. ' Linear feet of street connected to property')\n    \"\"\"\n    # The first item (idx = 0) contains name and description already. \n    # From then on, the name of each category is on an odd idx and its description is on idx + 1\n    joined_items = [list_items[0]] + [list_items[idx] + list_items[idx + 1] \n                         for idx, item in enumerate(list_items) if idx % 2 == 1]\n    \n    return joined_items\n    \n    \ndef clean_list(list_items):\n    \"\"\"\n    Keep only categorical features\n    \"\"\"\n    clean_list = []\n    \n    # Get the feature name\n    for idx, item in enumerate(list_items):\n        first_word = list_items[idx].split(':')[0]\n        \n        # Check that the name matches one of the categories\n        # If so, add name and description to new list\n        if first_word in cat_features:\n            clean_list.append(list_items[idx]) \n    \n    return clean_list\n    \n    \ndef itemize_description():\n    \"\"\"\n    Perform all the steps defined above to take the initial description text \n        and return a list of categories with their corresponding descriptions\n    \"\"\"\n    description_split = split_description()\n    joined_items = join_items(list_items=description_split)\n    desc_clean = clean_list(list_items=joined_items)\n    \n    return desc_clean\n\n\ndef box_plot(category):\n    \"\"\"\n    Display box-plot for a categorical feature with SalePrice on the y-axis\n\n    Parameters:\n        'category' : str\n            Name of a categorical feature\n    \"\"\"\n    # Plot\n    fig, ax = plt.subplots(1, 1, figsize=(14, 9))\n    train_cat.boxplot(column='SalePrice', by=category, ax=ax)\n    \n    # Format\n    fmt = '${x:,.0f}'\n    tick = mtick.StrMethodFormatter(fmt)\n    ax.yaxis.set_major_formatter(tick)\n    ax.xaxis.set_tick_params(rotation=45)\n\n    plt.show()\n    \n    \ndef plot_describe():\n    \"\"\"\n    For each categorical feature, pull its description, \n    print its value counts and missing values, and display box-plot\n    \"\"\"\n    for idx, feature in enumerate(cat_features):\n        \n        # Description\n        print(itemized_desc[idx])\n        \n        # Value counts\n        print('\\n' + 'Value Counts:')\n        print(train_cat[feature].value_counts(sort=False))\n        \n        # Missing values\n        nans = train_cat[feature].isnull().sum()\n        nan_ratio = (nans \/ len(train_cat)) * 100\n        print('\\n' + 'Missing values:')\n        print(f'{nans} ---- {nan_ratio}% of total')\n        \n        # Box-plot\n        box_plot(feature)","6f9efb4f":"itemized_desc = itemize_description()\nplot_describe()","ec6a656a":"train_cat.isna().sum()","9bd63c8f":"def start_fresh(train_or_hold, drop_id=True):\n    \"\"\"\n    Function to pull an unaltered copy of either the training or holdout data\n    \"\"\"\n    # Choose 'train' or 'hold'\n    if train_or_hold == 'train':\n        dataset = strat_train\n    elif train_or_hold == 'hold':\n        dataset = strat_hold\n    else:\n        raise ValueError('train_or_hold: invalid value')\n    \n    # Whether to include 'Id' column or not\n    if drop_id:\n        X = dataset.copy().drop(['Id', 'SalePrice'], axis=1)\n    else:\n        X = dataset.copy().drop(['SalePrice'], axis=1)\n        \n    y = dataset['SalePrice'].copy()\n    \n    return X, y","366f8ab2":"train_X, train_y = start_fresh('train')","fd4aaf6b":"# Num features\ntrain_X['LotFrontage'].fillna(0, inplace=True)\ntrain_X['MasVnrArea'].fillna(0, inplace=True)\ntrain_X['GarageYrBlt'].fillna(train_num['GarageYrBlt'].mean(), inplace=True)\n\n# Cat features\ntrain_X['MasVnrType'].fillna('None', inplace=True)\ntrain_X['BsmtQual'].fillna('NA', inplace=True)\ntrain_X['BsmtExposure'].fillna('NA', inplace=True)\ntrain_X['BsmtFinType1'].fillna('NA', inplace=True)\ntrain_X['Electrical'].fillna('SBrkr', inplace=True)\ntrain_X['FireplaceQu'].fillna('NA', inplace=True)\ntrain_X['GarageFinish'].fillna('NA', inplace=True)","883b2ecc":"def consolidate(feature, value_list, substitute, df=train_X):\n    \"\"\"\n    Consolidate several categories into one\n    \n    Parameters:\n        feature : str\n            Categorical feature to be considered\n        value_list : list\n            List of categories to be consolidated\n        substitute : str\n            Name of new category\n        df : DataFrame\n    \"\"\"\n    for x in range(len(df)):\n        if df[feature][x] in value_list:\n            df[feature][x] = substitute","f9f07bcd":"consolidate('LotShape', ['IR1', 'IR2', 'IR3'], 'IR')\nprint(train_X['LotShape'].value_counts())\nbox_plot('LotShape')","3771159d":"consolidate('Neighborhood', ['BrDale', 'BrkSide', 'Edwards', 'IDOTRR', 'MeadowV', \n                             'NAmes', 'OldTown', 'SWISU', 'Sawyer'], 'low_hood')\n\nconsolidate('Neighborhood', ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', \n                             'Mitchel', 'NWAmes', 'SawyerW', 'Somerst', 'Timber', \n                             'Veenker', 'NPkVill', 'Blueste'], 'medium_hood')\n\nconsolidate('Neighborhood', ['NoRidge', 'NridgHt', 'StoneBr'], 'high_hood')\n\nprint(train_X['Neighborhood'].value_counts())\nbox_plot('Neighborhood')","8634e252":"consolidate('HouseStyle', ['2Story', '2.5Fin'], '2Story & 2.5Fin')\nconsolidate('HouseStyle', ['1Story', '1.5Fin', 'SLvl', 'SFoyer', '1.5Unf', '2.5Unf'], 'Others')\n\nprint(train_X['HouseStyle'].value_counts())\nbox_plot('HouseStyle')","eb216eb9":"consolidate('Foundation', ['PConc', 'Wood'], 'PConc & Wood')\nconsolidate('Foundation', ['CBlock', 'BrkTil', 'Slab', 'Stone'], 'Others')\n\nprint(train_X['Foundation'].value_counts())\nbox_plot('Foundation')","b5b0dc15":"consolidate('HeatingQC', ['TA', 'Gd', 'Fa', 'Po'], 'Others')\n\nprint(train_X['HeatingQC'].value_counts())\nbox_plot('HeatingQC')","3532c554":"consolidate('Electrical', ['FuseA', 'FuseF', 'FuseP', 'Mix'], 'Others')\n\nprint(train_X['Electrical'].value_counts())\nbox_plot('Electrical')","19a05fe5":"# Dataset is from 2010\ntrain_X['HomeAge'] = 2010 - train_X['YearBuilt']\ntrain_X['RemodAge'] = 2010 - train_X['YearRemodAdd']\ntrain_X['GarageAge'] = 2010 - train_X['GarageYrBlt']\n\n# Fraction of finished basement\na = train_X['BsmtFinSF1'] + train_X['BsmtFinSF2']\nb = train_X['TotalBsmtSF']\n\ntrain_X['BsmtFinRatio'] = np.divide(a, b, out=np.zeros(a.shape, dtype=float), where=b!=0)\n\n# Total bathrooms\ntrain_X['TotalBath'] = train_X['BsmtFullBath'] + (train_X['BsmtHalfBath']\/2) + \\\n                        train_X['FullBath'] + (train_X['HalfBath']\/2)\n\n# Total porch and deck SF\ntrain_X['TotalPorchDeck'] = train_X['WoodDeckSF'] + train_X['OpenPorchSF'] + \\\n                                train_X['EnclosedPorch'] + train_X['3SsnPorch'] + \\\n                                train_X['ScreenPorch']\n\n# Total SF\ntrain_X['TotalSF'] = train_X['TotalBsmtSF'] + train_X['GrLivArea']\n\n# Fraction of home vs total lot SF\ntrain_X['HomeLotRatio'] = train_X['GrLivArea'] \/ train_X['LotArea']","28070b64":"# Season in which sale took place\nseason_dict = {'Winter': [12, 1, 2], 'Spring': [3, 4, 5], \n               'Summer': [6, 7, 8], 'Fall': [9, 10, 11]}\n\nfor i in range(len(train_X)):\n    for season in season_dict.keys():\n        if train_X.loc[i, 'MoSold'] in season_dict[season]:\n            train_X.loc[i, 'SeasonSold'] = season\n            \ntrain_X['SeasonSold'].head()","977978ca":"train_X['LotArea_log'] = np.log(train_X['LotArea'])\ntrain_X['LotArea_log'].hist(bins=20, figsize=(9, 6));","7d89a448":"train_X['GrLivArea_log'] = np.log(train_X['GrLivArea'])\ntrain_X['GrLivArea_log'].hist(bins=30, figsize=(9, 6));","995d4a0d":"def create_binary(orig_feature, new_feature):\n    \"\"\"\n    Create new feature with value 0 to represent a 0 in an existing feature,\n    or a 1 to represent any other number.\n    \"\"\"\n    for i in range(len(train_X)):\n        if train_X.loc[i, orig_feature] == 0:\n            train_X.loc[i, new_feature] = 0\n        else:\n            train_X.loc[i, new_feature] = 1\n            \n    train_X[new_feature] = train_X[new_feature].astype('category')","0d29085c":"binary_list = {'LotFrontage': 'HasLotFront', 'BsmtFinSF1': 'HasBsmt', 'BsmtFinSF2': 'Has2ndBsmt', \n               'GarageArea': 'HasGarage', '2ndFlrSF': 'Has2ndFloor', 'Fireplaces': 'HasFireplaces', \n               'WoodDeckSF': 'HasWoodDeck', 'OpenPorchSF': 'HasOpenPorch', 'EnclosedPorch': 'HasEnclosedPorch', \n               '3SsnPorch': 'Has3SsnPorch', 'ScreenPorch': 'HasScreenPorch', 'PoolArea': 'HasPoolArea'}\n\nfor orig_col, new_col in binary_list.items():\n    create_binary(orig_col, new_col)","0e1dfc24":"# features with 'object' data types could be handled faster as 'categorical'\ntrain_X[train_X.select_dtypes(['object']).columns] = train_X.select_dtypes(['object']).apply(lambda x: x.astype('category'))\n\n# YrSold and MoSold behave more as categorical features than numerical, \n# since the order of the numbers has no intrinsic value\ntrain_X['YrSold'] = train_X['YrSold'].astype('category')\ntrain_X['MoSold'] = train_X['MoSold'].astype('category')\n\ntrain_X.dtypes","547f59c2":"drop_year = ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']\ntrain_X.drop(drop_year, axis=1, inplace=True)","a0aab5e3":"drop_cats = ['MSSubClass', 'Street', 'Alley', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n                'Condition1', 'Condition2', 'BldgType', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n                'ExterCond', 'BsmtCond', 'BsmtFinType2', 'Heating', 'Functional', 'GarageType', 'GarageQual',\n                'GarageCond', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n\ntrain_X.drop(drop_cats, axis=1, inplace=True)","b97614a7":"# drop_correlated = ['GarageCars', 'GrLivArea', 'BedroomAbvGr', '1stFlrSF']\n# train_X.drop(drop_correlated, axis=1, inplace=True)","b2269988":"# corr_matrix = pd.concat([train_X, train_y], axis=1).corr()\n\n# plt.figure(figsize=(2, 25))\n# sns.heatmap(corr_matrix[['SalePrice']].sort_values(by='SalePrice', ascending=False), \n#             vmin=-1, vmax=1, center=0, cmap='coolwarm', annot=True, fmt='.1f', cbar=False)\n# plt.show()","42857def":"# Store names of columns (will come in handy later)\nnum_cols = train_X.select_dtypes(['int64', 'float64']).columns\ncat_cols = train_X.select_dtypes('category').columns","84fa9f83":"train_X.shape","e0809e7a":"train_X_dummies = pd.get_dummies(train_X)\ntrain_X_dummies.head()","d99d1ef6":"# Store column names\ntrain_X_cols = train_X_dummies.columns","8def7900":"train_X_dummies.shape","762fc5a4":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\n\nrf.fit(train_X_dummies, train_y)","cec00e08":"feat_importances = rf.feature_importances_ * 1000\n\ntree_coef_df = pd.DataFrame({'Feature': train_X_cols, 'Importance (*1000)': feat_importances})\ntree_coef_df.sort_values(by='Importance (*1000)', ascending=False, inplace=True)\ntree_coef_df.reset_index(drop=True, inplace=True)\n\ntree_coef_df","20ce6ea1":"ax = tree_coef_df.plot(x='Feature', y='Importance (*1000)', kind='barh', figsize=(10,30), grid=True)\nax.invert_yaxis()\nplt.show()","b6810f56":"# How many features we will keep\nkeep_n = 10\n\n# List of features to drop\ntree_drop = tree_coef_df.loc[keep_n:, 'Feature'].tolist()\n\n# Drop features\ntrain_X_prepared = train_X_dummies.drop(tree_drop, axis=1)\n\ntrain_X_prepared.head()","18301a97":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom xgboost import XGBRegressor\n\nlnr = LinearRegression()\nsgd = SGDRegressor(random_state=random_state)\nrdg = Ridge(random_state=random_state)\nlso = Lasso(random_state=random_state)\neln = ElasticNet(random_state=random_state)\nsvr = SVR()\nrfr = RandomForestRegressor(random_state=random_state)\netr = ExtraTreesRegressor(random_state=random_state)\ngbr = GradientBoostingRegressor(random_state=random_state)\nxgb = XGBRegressor(objective='reg:squarederror', random_state=random_state)\nadr = AdaBoostRegressor(random_state=random_state)\n\nmodels = {'LinearRegression': lnr, 'SGDRegressor': sgd, 'Ridge': rdg, 'Lasso': lso, \n          'ElasticNet': eln, 'SVR': svr, 'RandomForestRegressor': rfr, 'ExtraTreesRegressor': etr, \n          'GradientBoostingRegressor': gbr, 'XGBRegressor': xgb, 'AdaBoostRegressor': adr}","791ede42":"from sklearn.model_selection import cross_val_score\n\ndef score_models(models, X, y, verbose=0, scoring='neg_root_mean_squared_error', print_=True):\n    \n    results = {}\n\n    for model in models.keys():\n        \n        # Cross-validated score for an individual model\n        scores = -1 * cross_val_score(models[model], X, y, cv=5, \n                                verbose=verbose, scoring=scoring)\n        \n        # Mean and Standard Deviation of score\n        mean, std = np.mean(scores), np.std(scores)\n        \n        # Add mean and std to 'results' dictionary\n        results[model] = {'mean': mean, 'std': std}\n        \n        if print_:\n            print('Model: ', model)\n            print('Cross-val Mean: ', mean)\n            print('Cross-val StDev: ', std)\n            print('') \n            \n    return results","868d2545":"initial_results = score_models(models, train_X_prepared, train_y)","3cdbeeec":"train_X, train_y = start_fresh('train', drop_id=False)\n\nnum_types = ['int64', 'float64']\nnum_features = [feature for feature in train_X.columns if train_X[feature].dtype in num_types]\ncat_features = [feature for feature in train_X.columns if feature not in num_features]\n\ntrain_X.head()","3a6f2311":"# Numerical features\nimp_list_0 = num_features.copy()\nimp_list_0.remove('GarageYrBlt')\n\nimp_list_mean = ['GarageYrBlt']\n\n# Categorical features\nimp_list_None = ['MasVnrType']\nimp_list_NA = ['BsmtQual', 'BsmtExposure', 'BsmtFinType1', 'FireplaceQu', 'GarageFinish']\n\n# For the rest of the categorical features, impute using the feature's mode (most frequent value)\nimp_list_mode = [feature for feature in cat_features if feature not in (imp_list_None + imp_list_NA)]    \n\n# List of imputation lists (will come in handy later)\nimputed_cols = imp_list_0 + imp_list_mean + imp_list_None + imp_list_NA + imp_list_mode","92fe598a":"from sklearn.impute import SimpleImputer\n\n# Numerical imputers\nimputer_0 = SimpleImputer(strategy='constant', fill_value=0)\nimputer_mean = SimpleImputer()\n\n# Categorical imputers\nimputer_NA = SimpleImputer(strategy='constant', fill_value='NA')\nimputer_None = SimpleImputer(strategy='constant', fill_value='None')\nimputer_mode = SimpleImputer(strategy='most_frequent')","d7ff2d4d":"from sklearn.compose import ColumnTransformer\n\nimputer = ColumnTransformer([\n    ('imputer_0', imputer_0, imp_list_0),\n    ('imputer_mean', imputer_mean, imp_list_mean),\n    ('imputer_NA', imputer_NA, imp_list_NA),\n    ('imputer_None', imputer_None, imp_list_None),\n    ('imputer_mode', imputer_mode, imp_list_mode)\n])","8fc0a9cc":"train_X_imputed = imputer.fit_transform(train_X)\n\noutput_cols = imputed_cols.copy()\n\n# Check number of NaNs after imputation (should be 0)\npd.DataFrame(train_X_imputed, columns=output_cols).isna().values.sum()","aea75712":"def update_idx(col_list):\n    \"\"\"\n    Create dictionary of column names and their respective indices\n    \"\"\"\n    idx = {}\n    for col in col_list:\n        idx[col] = col_list.index(col)\n    \n    return idx","2e9fe9ac":"idx_imputed = update_idx(imputed_cols)\nidx_imputed","d5fc9cff":"from sklearn.base import BaseEstimator, TransformerMixin\n\ndef consolidate_pipe(feature, value_list, substitute, array, idx):\n    \"\"\"\n    Function analogous to 'consolidate' from the Feature Engineering section\n    \"\"\"\n    for x in range(len(array)):\n        if array[:, idx[feature]][x] in value_list:\n            array[:, idx[feature]][x] = substitute\n\n            \nclass Consolidator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, idx):\n        self.idx = idx\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        \n        idx = self.idx\n        \n        consolidate_pipe('LotShape', ['IR1', 'IR2', 'IR3'], 'IR', X, idx)\n        consolidate_pipe('Neighborhood', ['BrDale', 'BrkSide', 'Edwards', 'IDOTRR', 'MeadowV', \n                                     'NAmes', 'OldTown', 'SWISU', 'Sawyer'], 'low_hood', X, idx)\n        consolidate_pipe('Neighborhood', ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', \n                                     'Mitchel', 'NWAmes', 'SawyerW', 'Somerst', 'Timber', \n                                     'Veenker', 'NPkVill', 'Blueste'], 'medium_hood', X, idx)\n        consolidate_pipe('Neighborhood', ['NoRidge', 'NridgHt', 'StoneBr'], 'high_hood', X, idx)\n        consolidate_pipe('HouseStyle', ['2Story', '2.5Fin'], '2Story & 2.5Fin', X, idx)\n        consolidate_pipe('HouseStyle', ['1Story', '1.5Fin', 'SLvl', 'SFoyer', '1.5Unf', '2.5Unf'], 'Others', X, idx)\n        consolidate_pipe('Foundation', ['PConc', 'Wood'], 'PConc & Wood', X, idx)\n        consolidate_pipe('Foundation', ['CBlock', 'BrkTil', 'Slab', 'Stone'], 'Others', X, idx)\n        consolidate_pipe('HeatingQC', ['TA', 'Gd', 'Fa', 'Po'], 'Others', X, idx)\n        consolidate_pipe('Electrical', ['FuseA', 'FuseF', 'FuseP', 'Mix'], 'Others', X, idx)\n        \n        return X","17f714e8":"train_X_consolidated = Consolidator(idx=idx_imputed).fit_transform(train_X_imputed)\n\noutput_cols = imputed_cols\npd.DataFrame(train_X_consolidated, columns=output_cols).head()","d802c600":"class FeatureCreator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, idx):\n        self.idx = idx\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        \n        idx = self.idx\n        \n        # HomeAge, RemodAge, GarageAge\n        HomeAge = 2010 - X[:, idx['YearBuilt']]\n        RemodAge = 2010 - X[:, idx['YearRemodAdd']]\n        GarageAge = 2010 - X[:, idx['GarageYrBlt']]\n\n        # BasmtFinRatio\n        a = X[:, idx['BsmtFinSF1']] + X[:, idx['BsmtFinSF2']]\n        b = X[:, idx['TotalBsmtSF']]\n        BsmtFinRatio = np.divide(a, b, where=b!=0)\n\n        # TotalBath\n        TotalBath = X[:, idx['BsmtFullBath']] + (X[:, idx['BsmtHalfBath']]\/2) + \\\n                                X[:, idx['FullBath']] + (X[:, idx['HalfBath']]\/2)\n        \n        # TotalPorchDeck\n        TotalPorchDeck = X[:, idx['WoodDeckSF']] + X[:, idx['OpenPorchSF']] + \\\n                                        X[:, idx['EnclosedPorch']] + X[:, idx['3SsnPorch']] + \\\n                                        X[:, idx['ScreenPorch']]\n        \n        # TotalSF\n        TotalSF = X[:, idx['TotalBsmtSF']] + X[:, idx['GrLivArea']]\n\n        # HomeLotRatio\n        HomeLotRatio = X[:, idx['GrLivArea']] \/ X[:, idx['LotArea']]\n        \n        # SeasonSold\n        SeasonSold = np.copy(X[:, idx['MoSold']])\n\n        for i in range(len(X)):\n            for season in season_dict.keys():\n                if SeasonSold[i] in season_dict[season]:\n                    SeasonSold[i] = season\n                  \n        # Logs\n        LotArea_log = np.log(X[:, idx['LotArea']].astype('float64'))\n        GrLivArea_log = np.log(X[:, idx['GrLivArea']].astype('float64'))\n\n        return np.c_[X, HomeAge, RemodAge, GarageAge, BsmtFinRatio, TotalBath, TotalPorchDeck, \n                     TotalSF, HomeLotRatio, SeasonSold, LotArea_log, GrLivArea_log]","1627dc4b":"created_cols = ['HomeAge', 'RemodAge', 'GarageAge', 'BsmtFinRatio', 'TotalBath', 'TotalPorchDeck', \n                'TotalSF', 'HomeLotRatio', 'SeasonSold', 'LotArea_log', 'GrLivArea_log']\n\ntrain_X_created = FeatureCreator(idx=idx_imputed).fit_transform(train_X_consolidated)\n\noutput_cols += created_cols\npd.DataFrame(train_X_created, columns=output_cols).head()","83723521":"# Function to created new binary columns\n# Returns a binary (0 or 1) version of orig_col, with 0 for original value 0 and 1 for all other values\ncreate_binary_pipe = lambda orig_col, X: (X[:, idx_imputed[orig_col]] != 0).astype('int')\n\nclass BinaryCreator(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, idx):\n        self.idx = idx\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        \n        idx = self.idx\n        \n        for orig_col in binary_list.keys():\n            new_col = create_binary_pipe(orig_col, X)\n            X = np.c_[X, new_col]\n            \n        return X","faec8399":"binary_cols = [value for key, value in binary_list.items()]\n\ntrain_X_binarized = BinaryCreator(idx=idx_imputed).fit_transform(train_X_created)\n\noutput_cols += binary_cols\npd.DataFrame(train_X_binarized, columns=output_cols).head()","3a353c0f":"# Update idx dictionary\nidx_binarized = update_idx(output_cols)\n\n# Drop columns after analysis\ndrop_cols = drop_year + drop_cats + ['Id']\n\n# Gather indices of all columns to drop\ndrop_idx = [idx_binarized[col] for col in drop_cols]","bcdd1dde":"class ColumnDropper(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, drop_idx):\n        self.drop_idx = drop_idx\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        \n        X = np.delete(X, self.drop_idx, 1)\n        \n        return X","23c854ce":"train_X_dropped = ColumnDropper(drop_idx=drop_idx).fit_transform(train_X_binarized)\n\noutput_cols = list(np.delete(output_cols, drop_idx))\npd.DataFrame(train_X_dropped, columns=output_cols).head()","d96b135f":"# Update idx dictionary\nidx_dropped = update_idx(output_cols)\n\n# Get indices for numerical and categorical columns\nnum_idx = [idx_dropped[col] for col in num_cols]\ncat_idx = [idx_dropped[col] for col in cat_cols]","111977e4":"from sklearn.preprocessing import OneHotEncoder\n\none_hot_encoder = ColumnTransformer([\n    ('numerical', 'passthrough', num_idx),\n    ('encode', OneHotEncoder(), cat_idx)\n])","f7d2da0b":"train_X_encoded = one_hot_encoder.fit_transform(train_X_dropped)\n\noutput_cols = list(train_X_cols)\npd.DataFrame(train_X_encoded, columns=output_cols).head()","88f17ddf":"idx_encoded = update_idx(output_cols)\n\nsorted_features = list(tree_coef_df['Feature'])","c1ee5d78":"class FeatureSelector(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, num_features=10):\n        self.num_features = num_features\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        feature_idx = [idx_encoded[feature] for feature in sorted_features]\n        keep_idx = feature_idx[:self.num_features]\n        X_selected = X[:, keep_idx]\n        return X_selected","3b22f200":"train_X_selected = FeatureSelector().fit_transform(train_X_encoded)\ntrain_X_selected.shape","02b7a73f":"from sklearn.pipeline import Pipeline\n\n# Feature engineering pipeline\nfeat_eng_pipeline = Pipeline([\n    ('imputer', imputer),\n    ('consolidator', Consolidator(idx=idx_imputed)),\n    ('feature_creator', FeatureCreator(idx=idx_imputed)),\n    ('binary_creator', BinaryCreator(idx=idx_imputed))\n])\n\n# Feature selection pipeline\nfeat_sel_pipeline = Pipeline([\n    ('drop', ColumnDropper(drop_idx)),\n    ('encode', one_hot_encoder),\n    ('selector', FeatureSelector())\n])\n\n# Pre-processing Pipeline\npreprocessing_pipeline = Pipeline([\n    ('feature_engineering', feat_eng_pipeline),\n    ('feature_selection', feat_sel_pipeline)\n])\n\nregressor = XGBRegressor(objective='reg:squarederror', random_state=random_state)\n\n# Full Pipeline\nfull_pipeline = Pipeline([\n    ('preprocessing', preprocessing_pipeline),\n    ('regressor', regressor)\n])","d4d68692":"from sklearn import set_config\n\nset_config(display='diagram')\n\nfull_pipeline","2477ccfd":"set_config(display='text')","478efa79":"train_X, train_y = start_fresh('train', drop_id=False)","e791cd48":"from sklearn.model_selection import GridSearchCV\n\nnum_range = np.arange(1, 147, 5)\n\nparams = {'preprocessing__feature_selection__selector__num_features': num_range}\n\ngrid = GridSearchCV(full_pipeline, params, scoring='neg_mean_squared_error', verbose=1)\ngrid.fit(train_X, train_y)","c8e34fce":"# The evaluation metric for GridSearchCV was the Negative Mean Squared Error\nscores = np.sqrt(-grid.cv_results_['mean_test_score'])\n\n# Store the converted scores in a DataFrame\ngrid_results = pd.DataFrame({'CV_mean_score': scores}, index=num_range)","8374949d":"grid_results.plot(figsize=(14,8))","a3d164c5":"train_X, train_y = start_fresh('train', drop_id=False)\nhold_X, hold_y = start_fresh('hold', drop_id=False)","f3407f47":"from sklearn.metrics import mean_squared_error\n\ndef score_train_hold(model, train_data, hold_data, print_=False):\n    \"\"\"\n    Score an individual model on the entire training set and the holdout set\n    \"\"\"\n    # Fit model on entire training set\n    model.fit(train_data, train_y)\n    \n    # Evaluate model on training and holdout sets\n    rmse_train = np.sqrt(mean_squared_error(train_y, model.predict(train_data)))\n    rmse_hold = np.sqrt(mean_squared_error(hold_y, model.predict(hold_data)))\n    \n    if print_:\n        print('RMSE on training set: ', rmse_train)\n        print('RMSE on holdout set: ', rmse_hold)\n    \n    return rmse_train, rmse_hold","fd8d88a4":"score_train_hold(full_pipeline, train_X, hold_X)","1c0b1428":"num_features = 146\nmax_depth = 4\nmin_child_weight = 60\nsubsample = 0.9\nreg_lambda = 9\n\nfull_pipeline.set_params(preprocessing__feature_selection__selector__num_features = num_features,\n                         regressor__max_depth = max_depth,\n                         regressor__min_child_weight = min_child_weight,\n                         regressor__subsample = subsample,\n                         regressor__reg_lambda = reg_lambda)\n\nscore_train_hold(full_pipeline, train_X, hold_X)","fa12da75":"train_X, train_y = start_fresh('train', drop_id=False)\ntrain_X_preprocessed = preprocessing_pipeline.fit_transform(train_X)\n\nmodel = XGBRegressor(objective='reg:squarederror', num_features = 146, \n                     max_depth = 4, min_child_weight = 60, subsample = 0.9, \n                     reg_lambda = 9, random_state=random_state)\n\nmodel.fit(train_X_preprocessed, train_y)","cf6dfd47":"from xgboost import plot_tree\n\n_, ax = plt.subplots(figsize=(12,8))\nplot_tree(model, ax=ax)\nplt.show()","9b3b67fe":"path_sample_submission = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv'\n\nsample_submission = pd.read_csv(path_sample_submission)\nsample_submission.head()","7e6fb389":"path_test = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\ntest_df = pd.read_csv(path_test)\ntest_df.head()","0c4c9f27":"test_pred = full_pipeline.predict(test_df)\ntest_pred = pd.Series(data=test_pred, name='SalePrice')\ntest_pred_df = pd.concat([test_df['Id'], test_pred], axis=1)\ntest_pred_df.head()","e7f7e4b5":"test_pred_df.to_csv('my_submission.csv', index=False)","3c0b4917":"Once the preprocessing_pipeline has been fit to train_X, it's important to not call the fit method again but only the tranform method on the holdout set and later on the test set. Otherwise, we would introduce data leakage.  \n  \nNow that we have a pipeline, it's easy to evaluate different parameters for the individual steps. For example, we can plot the performance of the model on different subsets of features. We use GridSearchCV to use 5-fold cross-validation for each number of features.","c385fedf":"# Kaggle Housing Dataset","fa456707":"### Binary feature creation","5b9aac30":"It's also informative to look at the linear relationship between each dependent variable (our features) and the independent variable ('SalePrice' in our case). If we decide to use a Linear Regression model, this could be helpful as we'll be able to tell which features 'inform' our target.","4e361953":"### One-hot encoding","6a452ae2":"We take a look at the features most strongly correlated with 'SalePrice'","50193f85":"For an initial model evaluation, we will keep only 10 features.","45a94f61":"## 4. Exploratory Data Analysis","7a25ac58":"## 10. Visualize model","0ee45838":"### Add & combine features","04c0e87f":"This is not an essential part of the process, but an interesting one nonetheless. We step out of the full pipeline to train XGBRegressor on the training data and then plotting the first tree.","21c1a5f6":"<div id=\"section_4\"><\/div>","a033a588":"### Consolidate imbalanced categories","32542547":"In the Feature Selection section above, we obtained a sorted list of feature importances. We may want to select a subset of the features to train our data, so next is a tranformer that will give us the option to keep only a certain number of them.","a1aee853":"In order to properly evaluate and tune the models later on, we need to separate a subset of our training data. This subset, called the 'holdout' set, should not be available during Exploratory Data Analysis, Feature Engineering, Feature Selection, Model Selection and Model hyper-parameter Tuning, so as to avoid 'fitting' our model to it as much as possible.  \nWe do want the holdout set to be representative of the same distribution as the rest of the training set, so that the results contain meaningful information as to what changes if any we need to make to our model. For this, we create bins of the 'SalePrice' column, in effect transforming it into a new categorical column, and then splitting the training data into two subsets with the same proportion of samples for each of those bins.","4dab8b5c":"**Javier Orman** <br>\n[LinkedIn](https:\/\/www.linkedin.com\/in\/javierorman\/) | [GitHub](https:\/\/github.com\/javierorman)","f775e066":"After tuning several hyper-parameters, we find that the model does best if we keep all of the features. In the following code-block are the values that we have found work best. We are still overfitting the holdout set slightly, but the difference is quite a bit less than before.","ee8e966d":"### Explore Numerical Features","46be6775":"<div id=\"section_3\"><\/div>","3ad726a3":"Let's look at each of the numerical features with missing values:","f485d595":"### Create bins","fe201ab2":"Observe correlations with target variable:","53d152ef":"We can use a convenience function in order to efficiently score each model's performance. The scoring will be done with cross-validation using sklearn's `cross_val_score` with the default 5 folds.","047a4692":"We can see that the biggest improvement happens at 10-20 features, but the model continues to perform slightly better with more (at least on the training set).","989c6b0c":"<div id=\"section_10\"><\/div>","3c0867b0":"### Separate numerical and categorical variables","ec1869b0":"#### Correlations between features","372c5bca":"**Numerical features to investigate with extreme ranges**:  \n* LotArea  \n* MasVnrArea (Masonry veneer area in square feet)  \n* BsmtFinSF1: Type 1 finished square feet  \n* BsmtFinSF2: Type 2 finished square feet  \n* LowQualFinSF: Low quality finished square feet (all floors)  \n* WoodDeckSF  \n* OpenPorchSF  \n* EnclosedPorch  \n* 3SsnPorch: Three season porch area in square feet  \n* ScreenPorch: Screen porch area in square feet\n* PoolArea  \n* MiscVal: $Value of miscellaneous feature","dbd815fb":"The Kaggle Housing Dataset is a great alternative to the popular Boston Housing Data Set. It has more samples, more variables and sale prices that are more realistic by today's standards. \nThe dataset, compiled by Professor Dean De Cock at Truman State University in 2011, describes the sale of individual residential property in Ames, Iowa from 2006 to 2010.  \nFor more information, please read on [here](http:\/\/jse.amstat.org\/v19n3\/decock.pdf).\n","fd969561":"Gradient Boosting Trees seem to hold the most potential. Here we consider two different implementations: sklearn's GradientBoostingRegressor and XGBoost's XGBRegressor. Both perform similarly, but XGBRegressor did slightly better.","571ec303":"<div id=\"section_6\"><\/div>","2cad61d4":"## 2. Initial look at the data","a7a9d91e":"<div id=\"section_2\"><\/div>","65dcce66":"[1. Imports](#section_1)  \n[2. Initial look at the data](#section_2)  \n[3. Train-Houldout split](#section_3)  \n[4. Exploratory Data Analysis](#section_4)  \n[5. Feature Engineering](#section_5)  \n[6. Feature Selection](#section_6)  \n[7. Choose most promising models](#section_7)  \n[8. Pipeline](#section_8)  \n[9. Fine-tune model](#section_9)  \n[10. Visualize model](#section_10)  \n[11. Submission](#section_11)","b789ee97":"If you read all the way here, thank you! Please consider leaving your feedback and any suggestions in the Kaggle comments, or reach out directly.","ecbf520a":"<div id=\"section_5\"><\/div>","ec1d949d":"#### Data Types","b63c7b08":"If we want to be able to use Linear Regression in our models, then we have to be mindful of correlations between features. That is because in linear regression, we try to isolate the relationship between an independent variable and the dependent variables.  \n  \nIn other words, a Linear Regression model describes how much an independent variable 'moves' as a dependent variable changes value while holding all other variables constant. However, if two dependent variables are correlated, then by changing one we change the other.  \n  \nFor this, we observe the Pearson correlation coefficient between variables. A value of 1 means the variables are completely correlated with each other (if one goes up, the other does too and by a similar proportion). A value of -1 is similar but with movement in opposite directions. A value of 0 would mean that the variables are completely independent of each other.","46b99089":"#### Data Types","427627f2":"## 3. Train-Holdout Split","bbf26cec":"Some of the algorithms deal better with features that have normal distributions. We can create versions of LotArea and GrLivArea that better fit a normal distribution. Whether this actually makes a difference or not will be seen during training.","f5468489":"Next, we create a pre-processing pipeline based on all the steps taken above.\n\nscikit-learn provides several helpful transformer classes, but we'll have to create a few custom ones.  \nOne of the biggest challenges creating the pipeline is that the transformers only work with NumPy arrays, not pandas objects. I want to verify each transformer's output, so at each step I keep track of column indices to display the data as a DataFrame. Helpful but time-consuming!","f155e096":"We first score the model's performance on the training and holdout sets without adjusting any hyperparameters:","cac65e90":"#### Feature Importances","0bd44ceb":"#### Correlations with target variable *SalePrice*","0ee36b67":"In order to analyze each category, I will re-print their description from the 'data_description.txt' file (see above), followed by a boxplot. We need to create several functions to make this happen:","f0d189c8":"## 1. Imports","40ad7ceb":"#### Missing Values","abf13e22":"Before we go on, define a function to easily get a fresh training (or holdout) dataset:","3904db80":"By default, the transfomer only keeps the 10 top features from the list of importances. We verify that it works properly:","ead1959f":"I will create 3 bins based on the suggestions:  \n- SalePrice < 214925  \n- 214925 < SalePrice < 394950  \n- 394950 < SalePrice","3f4bc796":"The library 'Yellowbrick' provides a quick way to come up with bin edges for our categorization of 'SalePrice'.","519f2384":"<div id=\"section_11\"><\/div>","22bc3713":"<div id=\"section_8\"><\/div>","b73dbc8a":"Different features will undergo different methods of imputation.","30d465ff":"## 7. Choose most promising models","2e4ca403":"Drop 'year' columns now that we have 'age' instead.","2073f958":"We don't need 'SalePrice_bins' anymore, so we drop it from both datasets.","19fdc9a0":"## 9. Fine-tune model","1783716b":"### Feature Selector","105d6103":"Important variables such as SalePrice (which will be our target) and LotArea (which I suspect will be an influential feature) are extremely tail-heavy to the right.  \nSeveral histograms show a prevalence of value 0, which we can assume represents the absence of certain property features. For example, properties with \"GarageArea\" value 0 don't have a garage. Similarly for \"WoodDeckSF\", \"OpenPorchSF\" and others.","38568d08":"#### Some observations:","93c7bdb9":"### Create the split","f86372b1":"## 11. Submission","d0993ec2":"Drop categorical columns according to the analysis above.","ee8be0ad":"The Random Forest algorithm computes the feature importances as part of its process. This is why it can be used for 'embedded' feature selection, or even just to visualize and understand the level of contribution of each dependent variable.\n\nFirst, we need to prepare the categorical data by performing one-hot encoding.  ","ff27d25a":"### Full Pipeline","ee3d4f67":"We now utilize the holdout set to evaluate the performance of the model on a dataset it hasn't seen before. Tree-based models are known for overfitting the training set, so the goal will be to find hyperparameter values that increase regularization.  \n  \nBecause the goal will be to generalize to unseen data, we expect the performance on the training set to worsen as we reduce its variance.","1f94a243":"We can see that there are a few outliers on the right side of the plot. If our model learns patterns from these samples during fitting, it may not generalize well to other data. We will drop any sample with 'GrLivArea' higher than 4000.","90a62a71":"#### Missing Values","de755cb4":"We first take a look at how many models perform on the 10-feature subset of the dataset, without any fine-tuning. ","1ebe2300":"### Imputing","5d840a1b":"### Explore Categorical Features","864a5e5d":"* *LotFrontage*: There are no values of 0, so I'm assuming that the homes with no front lot have this field entered as empty (NaN). I'll substitute for 0.\n* *MasVnrArea*: Similarly, I will assume that the 8 missing values are missing simply because there is no Mason Veneer Area\n* *GarageYrBlt*: I will substitute NaNs with the mean.","98eb8264":"**MSSubClass**\nThe categories that are higher are, with the exception of \u201c75\t2-1\/2 STORY ALL AGES\u201d, for newer construction. This will be considered by YearBuilt already and in more precision.\n\n**MSZoning**\nRL (Residential Low Density) and FV (Floating Village Residential) are in general more expensive than the others.\nRL has tons of outliers, some very far away from the inter-quartile range.\nA vast majority of the samples are RL (920 out of 1460), with RH (Residential High Density and C (commercial) having severely under-sampled: only 9 and 13 instances respectively\n\n**Street**\nThis category is severely imbalanced (6 \u2018Gravel\u2019 vs 1162 \u2018Paved\u2019) and there is no huge distinction in prices (the difference in medians looks to be around $40,000)\n\n**Alley**\n1086 missing values: almost 93% of dataset.\nHomes with paved alleys are more expensive than those with gravel alleys with a difference in medians of about $100,000.\n\n**LotShape**\nSlightly Irregulars (IR1) tend to be more expensive than Regulars (Reg).\nLots that are Moderately Irregular (IR2) or Irregular (IR3) are severely under-sampled and tend to have a similar distribution to IR1. We should consider consolidating IR1, IR2 and IR3 into one category.\n\n**LandContour**\n\u2018Near Flat\/Level\u2019 (Lvl) category is severely oversampled: it accounts for almost 90% of instances.\nHillside and Depressed land contours tend to correspond to higher sale prices, and Banked tend to be slightly cheaper.\n\n**Utilities**\n\u2018All Public Utilities\u2019 appears in all except 1 of the instances, which happens to land close to the \u2018AllPub\u2019 median anyway. I will discard this one.\n\n**Lot Config**\n\"CulDSac\" homes are in general more expensive, but I'd be very careful to generalize since \"Inside\" and \"Corner\" have a lot of outliers. In fact, the most expensive properties are fall into the \"Corner\" category. \nPart of the problem is again the undersampling of some categories. \"Inside\" and \"Corner\", which have similar distributions account for 1052 samples in total, while \"CulDSac\" only has 74 instances.\n\n**LandSlope**\n\"Mod\" and \"Sev\" are slightly more expensive in general than \"Gtl\", with medians of about 180,000 vs 160,000. \"Gtl\" has 1104\/1168 samples, however, so I'd be careful to generalize.\n\n**Neighborhood**\nCategories are (mostly) evenly distributed and can be looked at 3 broad ranges according to the median SalesPrice:\n\n- \\\\$80,000 - \\\\$150,000: BrDale, BrkSide, Edwards, IDOTRR, MeadowV, NAmes, Oldtown, SWISU, Sayer  \n- \\\\$150,000 - \\\\$250,000: Blmngtn, ClearCr, CollgCr, Crawfor, Gilbert, Mitchel, NWAmes, SawyerW, Somerst, Timber, Veenker  \n- \\\\$250,000 +: NoRidger, NridgHt, StoneBr\n\n**Condition1**\n\"Normal\" accounts for 1010\/1168 instances, which cover all the range of prices. \"PosA\" and to a lesser extent \"PosN\" are in general more expensive than others. They have only 4 and 15 instances respectively.\n\n**Condition2**\n\"Normal\" accounts for 1155 of the instances. I'll discard this one.\n\n**BldgType** \n*1Fam* (Single-family Detached) accounts for 973\/1168 instances and cover the whole range of prices. \n*TwnhsE*, with 95 instances, corresponds to more expensive properties than *2fmCon*, *Duplex* and *Twnhs* (here I'm assuming that *Twnhs* and *TwnhI* are the same thing and it's just a type). However, its distribution seems to align well with *1Fam*. \n\n**HouseStyle**\n*2Story* (359 instances) has a slightly more expensive range than the rest, with the exception of *2.5Fin* (9 instances). I might group these two into their own category and the rest into another one.\n\n**RoofStyle**\nThere is nothing conclusive about this one. First, there's a severe class imbalance: 4 out of the 6 categories have less than 10 instances each. *Gable* has 915\/1168 instances, but it contains many outliers that might give *Hip* (the seemingly more expensive category) a run for its money. \n\n**RoofMatl**\n*CompShg* has 1150\/1168 instances and covers the whole price range. Out!\n\n**Exterior1st**\nThis one is all over the place. We can see that *AsbShng* and *AsphShn* correspond to lower SalesPrice than others, but together they only account for 18 samples! *VinylSd* has 416 samples and a higher median than most others, but the IQR is huge.\n\n**Exterior 2nd**\nSame issue as with **Exterior1st**\n\n**MasVnrType**\n8 missing values (0.7% of total)\n*Stone* usually corresponds to higher prices, *BrkCmn* to lower prices, *BrkFace* is all over the price range. *None* has most of the instances (685\/1168) and spans the range of prices as well, but slightly lower in general than *BrkFace*. \n\n**ExterQual**\nOne of the clearest separations of class so far. No missing values.\n\n**ExterCond**\nI expected this one to also have a clear correspondence with value, but *TA* (1033\/1168 instances) spans the whole price range, while the others are unremarkable. *Ex* should theoretically correspond to higher prices, but we only have 2 samples.\n\n**Foundation**\nClearly, houses with *PConc* (poured concrete) foundation tend to be pricier. *BrkTil* and *CBlock* look like they have a similar distribution. *Slab* and *Stone* are a little cheaper but it's hard to tell... they have 21 and 5 instances respectively. *Wood* only has only instance, which falls within *PConc* IQR. I may consolidate into two broad categories: *PConc & Wood* and the rest.\n\n**BsmtQual: Evaluates the height of the basement**\n31 missing values (2.65% of total)... No *Poor* or *NA*\nThe IQRs are clearly demarcated, with the expected *Ex* corresponding to more expensive houses and so on.\nWhere to put the 31 NaNs? *Gd* and *TA* have pretty different distributions and they both share a similar proportion of instances (492 & 517 respectively).\n\n**BsmtCond: Evaluates the general condition of the basement**\nSame missing values as **BsmtQual**\nAlmost all entries are *TA* (1057\/1168) and it covers the whole range of prices.\n\n**BsmtExposure: Refers to walkout or garden level walls**\n32 missing values (2.74% of total), similar to the others\nAlthough *No* has a lot of outliers on the expensive side, the IQR is clearly defined between around $130,000 and $190,000. It's the category with most instances: 755\/1168.\n\n**BsmtFinType1: Rating of basement finished area**\nSame missing values as the other **Bsmt** categories\nAll the IQRs are fairly tight\n\n**BsmtFinType2: Rating of basement finished area (if multiple types)**\n*Unf* includes most of the samples: 1000\/1168 and it covers the whole range of prices.\n\n**Heating**\nAlmost all instances (1144\/1168) belong to *GasA*\n\n**HeatingQC**\n598 instances belong to *Ex*, which also correspond generally to homes that are slightly more expensive (and also all of the very expensive ones, as seen in the boxplot). The other ranges are pretty similar, so I will combine them into one category\n\n**CentralAir**\nThe homes with no central air are definitely cheaper. Even though there is a steep class imbalance *Y* 1096 vs *N* 72, this definitely influences what the prices of the home should be.\n\n**Electrical**\n*SBrkr* includes most of the instances (1062\/1168). The other categories are cheaper in general and because there are so few instances in them, I will combine them into one category. \nThere is 1 missing value, which I will just assign to *SBrkr* (the category's mode).\n\n**KitchenQual**\n4 categories with distinct IQRs and no NaNs.\n\n**Functional: Home functionality (Assume typical unless deductions are warranted)**\n*Typ* includes 1087\/1168 instances and it covers all of the prices. Even though the median of this category is higher than others, it's not by much. I wouldn't trust it to predict prices.\n\n**FireplaceQu: Fireplace quality**\nLots of NaNs (560, 48%), which I assume represent the houses that just don't have fireplaces. The categories look promising. Might just label the NaNs something like \"NoFireplace\"\n\n**GarageType: Garage location**\n67 missing (5.7%)\n*Attchd* contains most of the samples (695\/1168) and covers almost the whole range of SalesPrice. Even though it contains lots of outliers at the top, its IQR sits between 160,000 and 240,000.\n*CarPort* corresponds with lower prices, but there are only 8 instances to go by.\n\n**GarageFinish: Interior finish of the garage**\nThis could be a better indicator of SalesPrice than GarageType (by the way, they have the same of missing values). *Finished* > *Rough Finished* > *Unfinished*. \nWe can consider the NaNs to represent homes without a garage and assign a 'NA' or 'NoGarage' category.\n\n**GarageQual**\n*TA* (Typical\/Average) has 1046\/1168 samples. In the boxplot, *Ex* corresponds to higher prices, but it only has 2 instances in the dataset.\n\n**GarageCond: Garage condition**\nSimilar to **GarageQual**\n\n**PavedDrive: Paved driveway**\n*Y* (Paved) has 1073\/1168 instances and its values cover almost the whole price range. However, I'd hesitate to discard this one: the other categories *P* (Partial Pavement) and *N* (Dirt\/Gravel) correspond to lower sales prices as expected.\n\n**PoolQC: Pool quality**\n1163 missing values (99.6%)\n\n**Fence: Fence quality**\n956 missing values (82%)\n\n**MiscFeature: Miscellaneous feature not covered in other categories**\n1124 missing values (96%)\n\n**SaleType**\nAlthough some of the categories look promising (*New* clearly corresponds to higher values), the fact that 1015\/1168 instances are in one category makes this less than ideal.\n\n**SaleCondition**\nSimilar to **SaleType**","f7312dcf":"<div id=\"section_7\"><\/div>","60d9266e":"## 6. Feature Selection","7707f25a":"<div id=\"section_9\"><\/div>","3916f52f":"#### Features with very high number of NaNs","b2f92670":"### Fill in missing values","d6bb85a6":"### Numerical features: Distribution","5118c960":"Drop numerical columns that are highly correlated to others.","620a115b":"## 5. Feature Engineering","fc5c62fc":"**Some observations:**  \n* 'YearBuilt': Even though it had a correlation of 0.5 with 'SalePrice', it seems like the relationship might be better represented by a non-linear function. I'm planning on transforming this feature into house 'Age' or something similar instead.\n* 'TotalBsmtSF': It seems to have a very strong linear correlation with 'SalePrice'. If we don't consider the outlier close to 6000 TotalBsmtSF, this relationship will show itself even more clearly.\n* 'GarageArea' has many instances with value 0. Without those, the linear relationship with 'SalePrice' will become even more apparent.  \n  \nLuckily, none of these attributes have missing values!!","5b6d9f40":"**Some strong relationships**:\n* EnclosedPorch is negatively correlated with YearBuilt\n* BsmtFullBath is positively correlated with BsmtFinSF2 (if there is a finished basement, chances are there will be a full bathroom in it) and negatively with BsmtUnfSF  (if the basement is unfinished, it will probably not have a full bathroom)\n* TotRmsAbvGrd is positively correlated with GrLivArea (above-ground living area's sqft)\n* TotRmsAbvGrd is positively correlated with BedroomAbvGr (bedrooms above ground)\n* GarageArea is positibely correlated with GarageCars\n* TotalBsmtSF is strongly positively correlated with 1stFlrSF\n\nLet's look at these relationships more closely.","e7aa8dde":"We can now see that the following correlations are very strong (>0.5 or <-0.5):\n* GarageArea\/GarageCars\n* TotRmsAbvGrd\/GrLivArea\n* TotRmsAbvGrd\/BedroomAbvGr\n* TotalBsmtSf\/1stFlrSF ","4642dfe8":"For the next Transformers, we will need column indices instead of names since the imputer will output a NumPy array, not a dataframe. That array will consist of the imputed columns, concatenated.","06f30d43":"## 8. Pipeline","94dfcb57":"### Consolidating","4e636b37":"### Select features: RandomForest","b47c0b5a":"#### Numerical features with 0s: create binary columns showing Has\/Hasn't  \nSeveral numerical features have a lot of misleading 0s. For example if a house doesn't have a Lot Front, that is represented by a 0 in the LotFrontage feature. Next, we create categorical features that track this with a 0 or a 1.","50679c2b":"### Back to the Future  \nAt this point, it's worth noting that during model selection we find out that linear models don't properly describe the data, so we end up preferring a non-linear approach. The next two code-blocks would apply if we wanted to perform Linear Regression, but a tree-based model (which is what we end up using) doesn't mind highly-correlated features and, in fact, all of these features can prove informative to such a model.","58aaa50f":"### Feature creation","d81ce4ef":"<div id=\"section_1\"><\/div>","219f6556":"### Drop features"}}