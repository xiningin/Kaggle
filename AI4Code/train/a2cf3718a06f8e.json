{"cell_type":{"07e11e69":"code","4b46e4d1":"code","111b65bc":"code","34146653":"code","19f4fc11":"code","61d6b914":"code","2b2e4977":"code","9aae6e18":"code","204ae064":"code","aa5733a9":"code","a1f79967":"code","06a897e0":"code","4e962b82":"code","88ac89c3":"code","356e2dec":"code","5bb7a198":"code","55a40bad":"code","7f84f9c8":"code","0888e872":"code","16360b8e":"code","c8f050bf":"code","007ced62":"code","4cdab6a7":"code","ee74febf":"code","497250e0":"code","0f0bc6a9":"code","589ecdfd":"code","c3245d83":"code","3ddd2836":"code","1d96edf0":"code","3d4cbaad":"code","a729e95a":"code","c8a787d2":"code","f0ecf73b":"code","df0843af":"code","dd53588c":"code","fd5e6911":"code","b11f8685":"code","aac4874d":"code","9f66a455":"code","c3f5b8bb":"code","4ef99cc8":"code","03a336fe":"code","8f886c83":"code","15b43288":"code","4e858ddc":"code","5d572a1b":"markdown","a2da1d90":"markdown","aededfa9":"markdown","c51d9ac9":"markdown","80eb5230":"markdown","00f3f4c9":"markdown","3ad2ae71":"markdown","9c794f5f":"markdown","68931765":"markdown","bcd5d1f4":"markdown","46deb6a5":"markdown","94edde72":"markdown","baec2dc5":"markdown","385a90af":"markdown","4fde8dec":"markdown","854bfeed":"markdown","504ef477":"markdown","ae3156b5":"markdown","1590c2f8":"markdown","73cd9f9f":"markdown","1b75ed6f":"markdown","6103ef9b":"markdown","696466cb":"markdown","a7630524":"markdown","466bb8a6":"markdown","14226189":"markdown","8f66a675":"markdown"},"source":{"07e11e69":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, f1_score\n\n# Resampling\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline","4b46e4d1":"bankloan = pd.read_csv('..\/input\/bankloans\/bankloans.csv')\nbankloan","111b65bc":"bankloan.describe()","34146653":"bankloan.isna().sum()\/len(bankloan.index)*100","19f4fc11":"imp_default = SimpleImputer(strategy = 'median')\nbankloan[['default']] = imp_default.fit_transform(bankloan[['default']])","61d6b914":"bankloan['default'].value_counts()\/len(bankloan['default'].index)*100","2b2e4977":"X = bankloan[['employ', 'debtinc', 'creddebt', 'othdebt']]\ny = bankloan['default']","9aae6e18":"X.shape","204ae064":"X_train_val, X_test, y_train_val, y_test = train_test_split(\n                                            X, y, \n                                            stratify = y, \n                                            test_size = 0.2, \n                                            random_state = 44)","aa5733a9":"logreg = LogisticRegression()\ntree = DecisionTreeClassifier(random_state = 44)\nknn = KNeighborsClassifier()","a1f79967":"logreg_pipe_scale = Pipeline([\n    ('scale', StandardScaler()),\n    ('logreg', logreg)\n])\n\ntree_pipe_scale = Pipeline([\n    ('tree', tree)\n])\n\nknn_pipe_scale = Pipeline([\n    ('scale', StandardScaler()),\n    ('knn', knn)\n])","06a897e0":"def model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train_val, y_train_val, cv = skfold, scoring = metric)\n    return model_cv","4e962b82":"logreg_pipe_scale_cv = model_evaluation(logreg_pipe_scale, 'f1')\ntree_pipe_scale_cv = model_evaluation(tree_pipe_scale, 'f1')\nknn_pipe_scale_cv = model_evaluation(knn_pipe_scale, 'f1')","88ac89c3":"logreg_cv = logreg_pipe_scale_cv.mean()\ntree_cv = tree_pipe_scale_cv.mean()\nknn_cv = knn_pipe_scale_cv.mean()","356e2dec":"score_list = [logreg_cv, tree_cv, knn_cv]\nmethod_name = ['Logistic Regression CV Score', 'Decision Tree Classifier CV Score',\n              'KNN Classifier CV Score']\ncv_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\ncv_summary","5bb7a198":"for model, model_name in zip([logreg_pipe_scale, tree_pipe_scale, knn_pipe_scale], \n                             ['Logistic Regression', 'Decision Tree Classifier', 'KNN Classifier']):\n    model.fit(X_train_val, y_train_val)\n    y_pred = model.predict(X_test)\n    print(model_name+ ':')\n    print(classification_report(y_test, y_pred))","55a40bad":"rus = RandomUnderSampler(random_state = 44)\nX_under, y_under = rus.fit_resample(X_train_val, y_train_val)","7f84f9c8":"logreg_pipe_scale_rus = Pipeline([\n    ('scale', StandardScaler()),\n    ('rus', rus),\n    ('logreg', logreg)\n])\n\ntree_pipe_scale_rus = Pipeline([\n    ('rus', rus),\n    ('tree', tree)\n])\n\nknn_pipe_scale_rus = Pipeline([\n    ('scale', StandardScaler()),\n    ('rus', rus),\n    ('knn', knn)\n])","0888e872":"def model_evaluation_rus(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_rus = cross_val_score(model, X_under, y_under, cv = skfold, scoring = metric)\n    return model_rus","16360b8e":"logreg_pipe_rus = model_evaluation_rus(logreg_pipe_scale_rus, 'f1')\ntree_pipe_rus = model_evaluation_rus(tree_pipe_scale_rus, 'f1')\nknn_pipe_rus = model_evaluation_rus(knn_pipe_scale_rus, 'f1')","c8f050bf":"logreg_rus = logreg_pipe_rus.mean()\ntree_rus = tree_pipe_rus.mean()\nknn_rus = knn_pipe_rus.mean()","007ced62":"score_list = [logreg_rus, tree_rus, knn_rus]\nmethod_name = ['Logistic Regression UnderSampling Score', 'Decision Tree Classifier UnderSampling Score',\n              'KNN Classifier UnderSampling Score']\nrus_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\nrus_summary","4cdab6a7":"for model, model_name in zip([logreg_pipe_scale_rus, tree_pipe_scale_rus, knn_pipe_scale_rus], \n                             ['Logistic Regression UnderSampling', 'Decision Tree Classifier UnderSampling', 'KNN Classifier UnderSampling']):\n    model.fit(X_under, y_under)\n    y_pred = model.predict(X_test)\n    print(model_name+ ':')\n    print(classification_report(y_test, y_pred))","ee74febf":"ros = RandomOverSampler(random_state = 44)\nX_over, y_over = ros.fit_resample(X_train_val, y_train_val)","497250e0":"logreg_pipe_scale_ros = Pipeline([\n    ('scale', StandardScaler()),\n    ('ros', ros),\n    ('logreg', logreg)\n])\n\ntree_pipe_scale_ros = Pipeline([\n    ('ros', ros),\n    ('tree', tree)\n])\n\nknn_pipe_scale_ros = Pipeline([\n    ('scale', StandardScaler()),\n    ('ros', ros),\n    ('knn', knn)\n])","0f0bc6a9":"def model_evaluation_ros(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_ros = cross_val_score(model, X_over, y_over, cv = skfold, scoring = metric)\n    return model_ros","589ecdfd":"logreg_pipe_ros = model_evaluation_ros(logreg_pipe_scale_ros, 'f1')\ntree_pipe_ros = model_evaluation_ros(tree_pipe_scale_ros, 'f1')\nknn_pipe_ros = model_evaluation_ros(knn_pipe_scale_ros, 'f1')","c3245d83":"logreg_ros = logreg_pipe_ros.mean()\ntree_ros = tree_pipe_ros.mean()\nknn_ros = knn_pipe_ros.mean()","3ddd2836":"score_list = [logreg_ros, tree_ros, knn_ros]\nmethod_name = ['Logistic Regression OverSampling Score', 'Decision Tree Classifier OverSampling Score',\n              'KNN Classifier OverSampling Score']\nros_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\nros_summary","1d96edf0":"for model, model_name in zip([logreg_pipe_scale_ros, tree_pipe_scale_ros, knn_pipe_scale_ros], \n                             ['Logistic Regression OverSampling', 'Decision Tree Classifier OverSampling', 'KNN Classifier OverSampling']):\n    model.fit(X_over, y_over)\n    y_pred = model.predict(X_test)\n    print(model_name+ ':')\n    print(classification_report(y_test, y_pred))","3d4cbaad":"smote = SMOTE(random_state = 44)\nX_smote, y_smote = smote.fit_resample(X_train_val, y_train_val)","a729e95a":"logreg_pipe_scale_smote = Pipeline([\n    ('scale', StandardScaler()),\n    ('smote', smote),\n    ('logreg', logreg)\n])\n\ntree_pipe_scale_smote = Pipeline([\n    ('smote', smote),\n    ('tree', tree)\n])\n\nknn_pipe_scale_smote = Pipeline([\n    ('scale', StandardScaler()),\n    ('smote', smote),\n    ('knn', knn)\n])","c8a787d2":"def model_evaluation_smote(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_smote = cross_val_score(model, X_smote, y_smote, cv = skfold, scoring = metric)\n    return model_smote","f0ecf73b":"logreg_pipe_smote = model_evaluation_smote(logreg_pipe_scale_smote, 'f1')\ntree_pipe_smote = model_evaluation_smote(tree_pipe_scale_smote, 'f1')\nknn_pipe_smote = model_evaluation_smote(knn_pipe_scale_smote, 'f1')","df0843af":"logreg_smote = logreg_pipe_smote.mean()\ntree_smote = tree_pipe_smote.mean()\nknn_smote = knn_pipe_smote.mean()","dd53588c":"score_list = [logreg_smote, tree_smote, knn_smote]\nmethod_name = ['Logistic Regression SMOTE Score', 'Decision Tree Classifier SMOTE Score',\n              'KNN Classifier SMOTE Score']\nsmote_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\nsmote_summary","fd5e6911":"for model, model_name in zip([logreg_pipe_scale_smote, tree_pipe_scale_smote, knn_pipe_scale_smote], \n                             ['Logistic Regression SMOTE', 'Decision Tree Classifier SMOTE', 'KNN Classifier SMOTE']):\n    model.fit(X_smote, y_smote)\n    y_pred = model.predict(X_test)\n    print(model_name+ ':')\n    print(classification_report(y_test, y_pred))","b11f8685":"logreg = LogisticRegression()\n\nrus = RandomUnderSampler(random_state = 44)\nX_under, y_under = rus.fit_resample(X_train_val, y_train_val)\n\nestimator = Pipeline([\n    ('scale', StandardScaler()),\n    ('rus', rus),\n    ('logreg', logreg)\n])","aac4874d":"hyperparam_space = {\n    'logreg__C': [100, 10, 1, 0.1, 0.01, 0.001],\n    'logreg__solver': ['liblinear', 'newton-cg', 'saga', 'lbfgs'],\n    'logreg__max_iter': [100, 200, 300, 400]\n}","9f66a455":"grid_search = GridSearchCV(\n                estimator,\n                param_grid = hyperparam_space,\n                cv = StratifiedKFold(n_splits = 5),\n                scoring = 'f1',\n                n_jobs = -1)","c3f5b8bb":"grid_search.fit(X_train_val, y_train_val)","4ef99cc8":"print('best score', grid_search.best_score_)\nprint('best param', grid_search.best_params_)","03a336fe":"estimator.fit(X_under, y_under)\ny_pred_estimator = estimator.predict(X_test)\nprint(classification_report(y_test, y_pred_estimator))","8f886c83":"grid_search.best_estimator_.fit(X_train_val, y_train_val)\ny_pred_grid = grid_search.best_estimator_.predict(X_test)\nprint(classification_report(y_test, y_pred_grid))","15b43288":"f1_estimator = f1_score(y_test, y_pred_estimator)\nf1_best_estimator = f1_score(y_test, y_pred_grid)","4e858ddc":"score_list = [f1_estimator, f1_best_estimator]\nmethod_name = ['Logistic Regression UnderSampling Before Tuning', 'Logistic Regression UnderSampling After Tuning']\nbest_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\nbest_summary","5d572a1b":"Usually, Hyperparameter tuning can improve the result, but in this case, **it can't**. So i have to change with another model for tuning until get the improvement.","a2da1d90":"# Cross Validation","aededfa9":"*Model Evaluation*","c51d9ac9":"*Check Missing Value and Fill*","80eb5230":"*Splitting Data*","00f3f4c9":"# Resampling: *SMOTE*","3ad2ae71":"Finally, I choose **Logistic Regression using UnderSampling** model because it has the highest of accuracy score. Let's do hyperparameter tuning to see if I can improve the score again after the imbalance data has been handled. Can it improve?","9c794f5f":"*Fitting Data*","68931765":"# Resampling: *UnderSampling*","bcd5d1f4":"*Model Evaluation*","46deb6a5":"* Best Model: Logistic Regression using UnderSampling\n* Best Estimator Score: 0.52376\n* Best C: 0.1\n* Best max_iter: 100\n* Best solver: newton-cg","94edde72":"This percentage indicates that the data is **imbalanced**.","baec2dc5":"# HyperParam Tuning","385a90af":"*Fitting Data*","4fde8dec":"I use 3 resampling methods to handle it, Under Sampling, Over Sampling and SMOTE.","854bfeed":"*fitting data*","504ef477":"# Resampling: *OverSampling*","ae3156b5":"The data contains the credit details about credit borrowers. Data Description:\n- age - Age of Customer\n- ed - Eductation level of customer\n- employ: Tenure with current employer (in years)\n- address: Number of years in same address\n- income: Customer Income\n- debtinc: Debt to income ratio\n- creddebt: Credit to Debt ratio\n- othdebt: Other debts\n- default: Customer defaulted in the past (1= defaulted, 0=Never defaulted)","1590c2f8":"*Model Evaluation*","73cd9f9f":"# Before VS After Tuning","1b75ed6f":"*Model Evaluation*","6103ef9b":"I use 0.2 as test_size score and X.shape for random_state so the data will be devided equally.","696466cb":"*Check the Imbalance*","a7630524":"# Define Model","466bb8a6":"*Fitting Data*","14226189":"Thank you for reading this notebook.","8f66a675":"# Data Cleaning"}}