{"cell_type":{"d95a049a":"code","c4d2f875":"code","af816907":"code","6f5fc1fd":"code","77d744e0":"code","4793742c":"code","c51ab4ee":"code","86819b3e":"code","b4a7c082":"code","19dc9177":"code","0920b97e":"code","c989852a":"code","b27e9261":"code","87bb2010":"code","40d04727":"code","fad816d8":"markdown","0fc614b8":"markdown"},"source":{"d95a049a":"from IPython.display import YouTubeVideo\nYOUTUBE_ID = '477Oi_a-pOk'\nYouTubeVideo(YOUTUBE_ID,width=960, height=800)","c4d2f875":"import os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.models import Sequential,load_model,save_model\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D\nfrom keras.layers import BatchNormalization\nfrom keras.optimizers import Adam","af816907":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n      rotation_range=25,\n      width_shift_range=0.2,\n      height_shift_range=0.2,\n      shear_range=0.2,\n      zoom_range=0.2,\n      horizontal_flip=True,\n      fill_mode='nearest')\n","6f5fc1fd":"batch_size = 64\ntarget_size = (64, 64)\ninput_shape=(64, 64, 3)\nseed=1337\nadam = 0.001\nfre= -20\nFC = 2048\nE = 1\npatience = 3\nverbose = 1\nfactor = 0.50\nmin_lr = 0.0001\nsteps_per_epoch=256\nvalidation_steps=256\nepochs=8","77d744e0":"test_datagen = ImageDataGenerator( rescale = 1.0\/255)\n\ntrain_generator = train_datagen.flow_from_directory('..\/input\/gender-dataset\/Dataset\/Train',\n                                                    batch_size =batch_size ,\n                                                    class_mode = 'binary',\n                                                    seed=seed,\n                                                    target_size = target_size )     \n\nvalidation_generator =  test_datagen.flow_from_directory( '..\/input\/gender-dataset\/Dataset\/Validation',\n                                                          batch_size  = batch_size,\n                                                          class_mode  = 'binary',\n                                                          seed=seed,\n                                                          target_size = target_size)","4793742c":"base_model = tf.keras.applications.VGG16(input_shape=input_shape,include_top=False,weights=\"imagenet\")","c51ab4ee":"# Freezing Layers\n\nfor layer in base_model.layers[:fre]:\n    layer.trainable=False","86819b3e":"# Building Model\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(layers.Dropout(.2))\n\nmodel.add(Conv2D(512, (3, 3),strides=(1,1), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(layers.Dropout(.1))\nmodel.add(Conv2D(128, (3, 3),strides=(1,1), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(layers.Dropout(.1))\nmodel.add(Conv2D(384, (3, 3),strides=(1,1), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(layers.Dropout(.1))\nmodel.add(Conv2D(384, (3, 3),strides=(1,1), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(layers.Dropout(.1))\nmodel.add(Conv2D(500, (3, 3),strides=(1,1), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(2,strides=(2,2), padding='same'))\n\n\n\n# Add new layers\nmodel.add(Flatten())\nmodel.add(Dense(FC , activation='relu'))\nmodel.add(layers.Dropout(.2))\nmodel.add(Dense(FC , activation='relu'))\nmodel.add(layers.Dropout(.2))\nmodel.add(Dense(FC, activation='relu'))\nmodel.add(layers.Dropout(.2))\nmodel.add(Dense(E, activation='sigmoid'))\n\nmodel.summary()\n","b4a7c082":"model.compile(optimizer=Adam(adam),\n              loss='binary_crossentropy'\n              ,metrics=['accuracy'])","19dc9177":"from tensorflow.keras.utils import plot_model\nplot_model(model,show_shapes=True, show_layer_names=True, rankdir='TB', expand_nested=True)","0920b97e":"lrd = ReduceLROnPlateau(monitor = 'val_loss',\n                        patience = patience,\n                        verbose = verbose ,\n                        factor = factor,\n                        min_lr = min_lr)\n\nmcp = ModelCheckpoint('model.h5')\n\nes = EarlyStopping(verbose=verbose, patience=patience)","c989852a":"%time\nhist = model.fit_generator(generator=train_generator,\n                           validation_data=validation_generator,\n                           steps_per_epoch=steps_per_epoch,\n                           validation_steps=validation_steps,\n                           epochs=epochs,\n                           callbacks=[lrd, mcp, es])","b27e9261":"import matplotlib.pyplot as plt\nacc = hist.history['accuracy']\nval_acc = hist.history['val_accuracy']\nloss = hist.history['loss']\nval_loss = hist.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'g', label='Training accuracy')\nplt.plot(epochs, val_acc, 'y', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","87bb2010":"import numpy as np\n\nfrom keras.preprocessing import image\n#  images test 1\npath_testmodel = \"..\/input\/testmodel\/test1.jpg\"\nimge = image.load_img(path_testmodel, target_size=target_size)\nX = image.img_to_array(imge)\nX = np.expand_dims(X, axis=0)\n\nimages = np.vstack([X])\nclasses = model.predict(images, batch_size=1)\nprint(classes[0])\nif classes[0]<0.5:\n    print(\"This is a male\")\nelse:\n    print( \"This  is a female\")\nplt.imshow(imge)","40d04727":"import numpy as np\n\nfrom keras.preprocessing import image\n#  images test 2\npath_testmodel = \"..\/input\/testmodel\/test2.jpg\"\nimge = image.load_img(path_testmodel, target_size=target_size)\nX = image.img_to_array(imge)\nX = np.expand_dims(X, axis=0)\n\nimages = np.vstack([X])\nclasses = model.predict(images, batch_size=1)\nprint(classes[0])\nif classes[0]>0.5:\n    print(\"This is a male\")\nelse:\n    print( \"This is a female\")\nplt.imshow(imge)","fad816d8":"# What is VGG16 model?\nVGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. ... By only keeping the convolutional modules, our model can be adapted to arbitrary input sizes. The model loads a set of weights pre-trained on ImageNet.\n\n<img src=\"https:\/\/storage.googleapis.com\/lds-media\/images\/vgg16-architecture.original.jpg\" width=\"800px\">\n","0fc614b8":"# Gender Classification Using VGG16+CNN\n\n\n We built a Gender (male Or female) classification model with VVG16 + convolutional neural network with 202419 images.\n\n\n<img src=\"http:\/\/img.youtube.com\/vi\/TWWsW1w-BVo\/0.jpg\" width=\"800px\">\n"}}