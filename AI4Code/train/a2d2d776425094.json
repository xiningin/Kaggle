{"cell_type":{"fe727136":"code","bb3ada12":"code","5fc51f9e":"code","8909b626":"code","23d779a2":"code","9c8d6bf2":"code","b46918da":"code","c40542af":"code","93a89042":"code","523f257e":"code","7f6b74a1":"code","44956e0d":"code","ba1df8de":"code","68a01d56":"code","480eb0e1":"code","7ab6c53a":"code","b1557d45":"code","d5aa625f":"code","9947091c":"code","ba6624ac":"code","794b0a25":"code","57e1aef3":"code","d0f1fcf3":"code","0c102810":"code","e03f487a":"code","6a5080e4":"code","d90deeb4":"code","f0a7cbbc":"code","2df64b87":"code","c8b7cbdb":"code","5aadba25":"code","d5f54e77":"code","760b6739":"code","10f8aa31":"code","4a7fa52e":"code","69205866":"code","4cc98dbc":"code","ae84074b":"code","f65ab761":"code","72cb6143":"code","e7decf20":"code","8b7c480a":"code","38dd5d87":"code","af92a2f2":"code","aa4a32e9":"code","28421740":"code","b285eef4":"code","1b373071":"code","690a212a":"code","a46dedb3":"code","6e6c51a5":"code","dc5b4a5c":"code","47e0cfe1":"code","12a0bf96":"code","53616695":"code","c2bb1f8e":"code","eb0226a3":"code","10d9fcb6":"code","0818f538":"code","8e1ed51d":"code","f110e0cf":"code","4e78c4b6":"code","fcf63b49":"code","9356c591":"code","52e7d7e0":"code","5484d53e":"code","e6cf3655":"code","788e38c3":"code","1d79970d":"code","b2d047c3":"code","cc6d4007":"code","fc15cbb9":"code","dfc851e9":"code","9f39cfb7":"code","341d0fd0":"code","c5f0a6c8":"code","d435db50":"code","eb97ce1a":"code","73e8469a":"code","0ead0858":"markdown","f52ba0bb":"markdown","dfcccb01":"markdown","2613d9d7":"markdown","ae3a7096":"markdown","3a32a062":"markdown","6ef588e3":"markdown","f27d103a":"markdown","afa3ebf5":"markdown","0e759406":"markdown","9ce213d2":"markdown","ccba0d1d":"markdown","acdfc269":"markdown","1bd031de":"markdown","291bef27":"markdown","5f9ba95a":"markdown","8318d394":"markdown","e6af2720":"markdown","e44b4bc6":"markdown","f3f188d6":"markdown","6d0b44df":"markdown","5eb20de5":"markdown","d5b94667":"markdown","f08076fc":"markdown","1ecfb6f3":"markdown","e4c074ba":"markdown","38c51226":"markdown","834a6293":"markdown","4cf02bfb":"markdown","b27e600b":"markdown","3c006ee6":"markdown","af43559d":"markdown","a9c72283":"markdown","0efe447f":"markdown","4dc56d77":"markdown","f3c15a01":"markdown","0b244a1b":"markdown","9b23fed7":"markdown","81128de6":"markdown","91191cc3":"markdown","c40c2344":"markdown","f0b49b17":"markdown","c4a309ec":"markdown","794d248d":"markdown","7aeebe3f":"markdown","2ba36232":"markdown","9fef34d8":"markdown","f0a5e511":"markdown","78746c5c":"markdown","7f45f7a3":"markdown","2d6789fb":"markdown","368257bb":"markdown","56826b8d":"markdown","23cb5e53":"markdown","b48f2620":"markdown","9d2f6211":"markdown","b40cf60c":"markdown"},"source":{"fe727136":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import svm\nfrom time import time\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, plot_confusion_matrix, confusion_matrix, f1_score\nfrom statistics import mean\nimport pickle\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras import losses\nfrom keras import utils\nfrom keras.layers.experimental.preprocessing import TextVectorization\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Bidirectional, Dropout\nfrom tensorflow.keras.models import load_model\nimport torch\nfrom tqdm.notebook import tqdm\nfrom transformers import BertTokenizer\nfrom torch.utils.data import TensorDataset\nfrom transformers import BertForSequenceClassification\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom transformers import AdamW, get_linear_schedule_with_warmup","bb3ada12":"dataset = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), shuffle=True, random_state=42)\ndf = pd.DataFrame()\ndf['text'] = dataset.data\ndf['source'] = dataset.target\nlabel=[]\nfor i in df['source']:\n    label.append(dataset.target_names[i])\ndf['label']=label","5fc51f9e":"# first few rows of the dataset\ndf.head()","8909b626":"# drop source column\ndf.drop(['source'],axis=1,inplace=True)","23d779a2":"# value count\ndf['label'].value_counts()","9c8d6bf2":"# replace to politics\ndf['label'].replace({'talk.politics.misc':'politics','talk.politics.guns':'politics',\n                     'talk.politics.mideast':'politics'},inplace=True)\n                    \n# replace to sport\ndf['label'].replace({'rec.sport.hockey':'sport','rec.sport.baseball':'sport'},inplace=True)\n                    \n# replace to religion\ndf['label'].replace({'soc.religion.christian':'religion','talk.religion.misc':'religion'},inplace=True)\n                    \n# replace to computer\ndf['label'].replace({'comp.windows.x':'computer','comp.sys.ibm.pc.hardware':'computer',\n                    'comp.os.ms-windows.misc':'computer','comp.graphics':'computer',\n                    'comp.sys.mac.hardware':'computer'},inplace=True)  \n# replace to sales\ndf['label'].replace({'misc.forsale':'sales'},inplace=True)\n\n# replace to automobile\ndf['label'].replace({'rec.autos':'automobile','rec.motorcycles':'automobile'},inplace=True)\n\n# replace to science\ndf['label'].replace({'sci.crypt':'science','sci.electronics':'science','sci.space':'science'},inplace=True)\n\n# replace to medicine\ndf['label'].replace({'sci.med':'medicine'},inplace=True)","b46918da":"# number of targets\ndf['label'].nunique()","c40542af":"# value count\ndf['label'].value_counts()","93a89042":"df['Number_of_words'] = df['text'].apply(lambda x:len(str(x).split()))\ndf.head()","523f257e":"# basic stats\ndf['Number_of_words'].describe()","7f6b74a1":"df[df['Number_of_words']==11765]","44956e0d":"no_text = df[df['Number_of_words']==0]\nprint(len(no_text))\n\n# drop these rows\ndf.drop(no_text.index,inplace=True)","ba1df8de":"plt.style.use('ggplot')\nplt.figure(figsize=(12,6))\nsns.distplot(df['Number_of_words'],kde = False,color=\"red\",bins=200)\nplt.title(\"Frequency distribution of number of words for each text extracted\", size=20)","68a01d56":"# cleaning the text\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to  datasets\ndf['cleaned_text'] = df['text'].apply(lambda x: clean_text(x))\n\n# updated text\ndf['cleaned_text'].head()","480eb0e1":"tokenizer=nltk.tokenize.RegexpTokenizer(r'\\w+')\ndf['tokens'] = df['cleaned_text'].apply(lambda x:tokenizer.tokenize(x))\ndf.head()","7ab6c53a":"# stopwords\nstopwords.words('english')[0:5]","b1557d45":"len(stopwords.words('english'))","d5aa625f":"# removing stopwords\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words \ndf['stopwordremove_tokens'] = df['tokens'].apply(lambda x : remove_stopwords(x))\ndf.head()","9947091c":"# lemmatization\nlem = WordNetLemmatizer()\ndef lem_word(x):\n    return [lem.lemmatize(w) for w in x]\n\ndf['lemmatized_text'] = df['stopwordremove_tokens'].apply(lem_word)\ndf.head()","ba6624ac":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ndf['final_text'] = df['lemmatized_text'].apply(lambda x : combine_text(x))\ndf.head()","794b0a25":"df['Final_no_of_words'] = df['final_text'].apply(lambda x:len(str(x).split()))\ndf.head()","57e1aef3":"# basic stats\ndf['Final_no_of_words'].describe()","d0f1fcf3":"# number of rows with text lenth = 0\nprint(len(df[df['Final_no_of_words']==0]))\n\n# drop those rows\ndf.drop(df[df['Final_no_of_words']==0].index,inplace=True)","0c102810":"# label_encoder object knows how to understand word labels.\nlabel_encoder = preprocessing.LabelEncoder()\n  \n# Encode labels in column 'species'.\ndf['target']= label_encoder.fit_transform(df['label'])\n  \ndf['target'].unique()","e03f487a":"# dependent and independent variable\nX = df['final_text']\ny = df['target']","6a5080e4":"X.shape,y.shape","d90deeb4":"count_vectorizer = CountVectorizer()\ncount_vector = count_vectorizer.fit_transform(X)\nprint(count_vector[0].todense())","f0a7cbbc":"tfidf_vectorizer = TfidfVectorizer(min_df = 2,max_df = 0.5,ngram_range = (1,2))\ntfidf = tfidf_vectorizer.fit_transform(X)\nprint(tfidf[0].todense())","2df64b87":"# count vector\nsmote = SMOTE(random_state = 402)\nX_smote, Y_smote = smote.fit_resample(count_vector,y)\n\n\nsns.countplot(Y_smote)","c8b7cbdb":"# tfidf\nsmote = SMOTE(random_state = 402)\nX_smote_tfidf, Y_smote_tfidf = smote.fit_resample(tfidf,y)\n\nsns.countplot(Y_smote_tfidf)","5aadba25":"# train-test split countvector\nX_train, X_test, y_train, y_test = train_test_split(X_smote, Y_smote, test_size = 0.20, random_state = 0)\nX_train.shape, X_test.shape,y_train.shape, y_test.shape","d5f54e77":"# train-test split tfidf\nX_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(X_smote_tfidf, Y_smote_tfidf , test_size = 0.20, random_state = 0)","760b6739":"training_time_container = {'linear_svm_tfidf':0,'linear_svm':0,'mnb_naive_bayes_tfidf':0,\n                         'mnb_naive_bayes':0,'random_forest_tfidf':0,'random_forest':0,\n                          'logistic_reg':0,'logistic_reg_tfidf':0}\nprediction_time_container = {'linear_svm_tfidf':0,'linear_svm':0,'mnb_naive_bayes_tfidf':0,\n                         'mnb_naive_bayes':0,'random_forest_tfidf':0,'random_forest':0,\n                            'logistic_reg':0,'logistic_reg_tfidf':0}\naccuracy_container = {'linear_svm_tfidf':0,'linear_svm':0,'mnb_naive_bayes_tfidf':0,\n                         'mnb_naive_bayes':0,'random_forest_tfidf':0,'random_forest':0,\n                     'logistic_reg':0,'logistic_reg_tfidf':0}","10f8aa31":"# on countvector\nlg = LogisticRegression(C = 1.0)\n#Fitting the model \nt0=time()\nlg.fit(X_train,y_train)\ntraining_time_container['logistic_reg']=time()-t0\n\n\n# Predicting the Test set results\nt0 = time()\ny_pred_lg = lg.predict(X_test)\nprediction_time_container['logistic_reg']=time()-t0\n\nlg_test_accuracy =  accuracy_score(y_test,y_pred_lg)\naccuracy_container['logistic_reg'] = lg_test_accuracy\n\nprint('Training Accuracy : ', accuracy_score(y_train,lg.predict(X_train)))\nprint('Testing Accuracy: ',lg_test_accuracy)\nprint(\"Training Time: \",training_time_container['logistic_reg'])\nprint(\"Prediction Time: \",prediction_time_container['logistic_reg'])\nprint(confusion_matrix(y_test,y_pred_lg))","4a7fa52e":"# on tfidf\nlg = LogisticRegression(C = 1.0)\n#Fitting the model \nt0=time()\nlg.fit(X_train_tfidf,y_train_tfidf)\ntraining_time_container['logistic_reg_tfidf']=time()-t0\n\n# Predicting the Test set results\nt0=time()\nypred_lg_tf = lg.predict(X_test_tfidf)\nprediction_time_container['logistic_reg_tfidf']=time()-t0\n\nlg_test_accuracy_tf  = accuracy_score(y_test_tfidf,ypred_lg_tf)\naccuracy_container['logistic_reg_tfidf'] = lg_test_accuracy_tf\n\nprint('Training Accuracy: ', accuracy_score(y_train_tfidf,lg.predict(X_train_tfidf)))\nprint('Testing Accuracy: ', lg_test_accuracy_tf)\nprint(\"Training Time: \",training_time_container['logistic_reg_tfidf'])\nprint(\"Prediction Time: \",prediction_time_container['logistic_reg_tfidf'])\nprint(confusion_matrix(y_test,ypred_lg_tf))","69205866":"# on countvector\nnb = MultinomialNB()\n#Fitting the model \nt0=time()\nnb.fit(X_train,y_train)\ntraining_time_container['mnb_naive_bayes']=time()-t0\n\n\n# Predicting the Test set results\nt0 = time()\ny_pred_nb = nb.predict(X_test)\nprediction_time_container['mnb_naive_bayes']=time()-t0\n\nmnb_test_accuracy =  accuracy_score(y_test,y_pred_nb)\naccuracy_container['mnb_naive_bayes'] = mnb_test_accuracy\n\nprint('Training Accuracy : ', accuracy_score(y_train,nb.predict(X_train)))\nprint('Testing Accuracy: ',mnb_test_accuracy)\nprint(\"Training Time: \",training_time_container['mnb_naive_bayes'])\nprint(\"Prediction Time: \",prediction_time_container['mnb_naive_bayes'])\nprint(confusion_matrix(y_test,y_pred_nb))","4cc98dbc":"# on tfidf\nnb = MultinomialNB()\n#Fitting the model \nt0=time()\nnb.fit(X_train_tfidf,y_train_tfidf)\ntraining_time_container['mnb_naive_bayes_tfidf']=time()-t0\n\n# Predicting the Test set results\nt0=time()\nypred_nb_tf = nb.predict(X_test_tfidf)\nprediction_time_container['mnb_naive_bayes_tfidf']=time()-t0\n\nmnb_tfidf_test_accuracy = accuracy_score(y_test_tfidf,ypred_nb_tf)\naccuracy_container['mnb_naive_bayes_tfidf'] = mnb_tfidf_test_accuracy \n\n\nprint('Training Accuracy: ', accuracy_score(y_train_tfidf,nb.predict(X_train_tfidf)))\nprint('Testing Accuracy: ',mnb_tfidf_test_accuracy )\nprint(\"Training Time: \",training_time_container['mnb_naive_bayes_tfidf'])\nprint(\"Prediction Time: \",prediction_time_container['mnb_naive_bayes_tfidf'])\nprint(confusion_matrix(y_test,ypred_nb_tf))","ae84074b":"# Used hinge loss which gives linear Support Vector Machine. Also set the learning rate to 0.0001 (also the default value)\n# which is a constant that's gets multiplied with the regularization term. For penalty, I've used L2 which is the standard\n#regularizer for linear SVMs\n\n\n# on countvector\nsvm_classifier = linear_model.SGDClassifier(loss='hinge',alpha=0.0001)\nt0=time()\nsvm_classifier.fit(X_train,y_train)\ntraining_time_container['linear_svm']=time()-t0\n\n# Predicting the Test set results\nt0=time()\ny_pred_svm = svm_classifier.predict(X_test)\nprediction_time_container['linear_svm']=time()-t0\n\nsvm_test_accuracy  = accuracy_score(y_test,y_pred_svm)\naccuracy_container['linear_svm'] = svm_test_accuracy \n\nprint('Training Accuracy : ', accuracy_score(y_train,svm_classifier.predict(X_train)))\nprint('Testing Accuracy: ',svm_test_accuracy )\nprint(\"Training Time: \",training_time_container['linear_svm'])\nprint(\"Prediction Time: \",prediction_time_container['linear_svm'])\nprint(confusion_matrix(y_test,y_pred_svm))","f65ab761":"# on tfidf\nsvm_classifier = linear_model.SGDClassifier(loss='hinge',alpha=0.0001)\n#Fitting the model \nt0=time()\nsvm_classifier.fit(X_train_tfidf,y_train_tfidf)\ntraining_time_container['linear_svm_tfidf']=time()-t0\n\n# Predicting the Test set results\nt0=time()\nypred_svm_tf = svm_classifier.predict(X_test_tfidf)\nprediction_time_container['linear_svm_tfidf']=time()-t0\n\nsvm_test_accuracy_tf  = accuracy_score(y_test_tfidf,ypred_svm_tf)\naccuracy_container['linear_svm_tfdif'] = svm_test_accuracy_tf \n\nprint('Training Accuracy: ', accuracy_score(y_train_tfidf,svm_classifier.predict(X_train_tfidf)))\nprint('Testing Accuracy: ', svm_test_accuracy_tf)\nprint(\"Training Time: \",training_time_container['linear_svm_tfidf'])\nprint(\"Prediction Time: \",prediction_time_container['linear_svm_tfidf'])\nprint(confusion_matrix(y_test,ypred_svm_tf))","72cb6143":"# on count vectorizer\nrf = RandomForestClassifier(n_estimators=50)\nt0=time()\nrf.fit(X_train,y_train)\ntraining_time_container['random_forest']=time()-t0\n\n# Predicting the Test set results\nt0=time()\ny_pred_rf = rf.predict(X_test)\nprediction_time_container['random_forest']=time()-t0\n\nrf_test_accuracy  = accuracy_score(y_test,y_pred_rf)\naccuracy_container['random_forest'] = rf_test_accuracy \n\n\nprint('Training Accuracy : ', accuracy_score(y_train,rf.predict(X_train)))\nprint('Testing Accuracy: ',rf_test_accuracy )\nprint(\"Training Time: \",training_time_container['random_forest'])\nprint(\"Prediction Time: \",prediction_time_container['random_forest'])\nprint(confusion_matrix(y_test,y_pred_rf))","e7decf20":"# on tfidf\nrf = RandomForestClassifier(n_estimators=50)\n#Fitting the model \nt0=time()\nrf.fit(X_train_tfidf,y_train_tfidf)\ntraining_time_container['random_forest_tfidf']=time()-t0\n\n# Predicting the Test set results\nt0=time()\nypred_rf_tf = rf.predict(X_test_tfidf)\nprediction_time_container['random_forest_tfidf']=time()-t0\n\nrf_test_accuracy_tf  = accuracy_score(y_test_tfidf,ypred_rf_tf)\naccuracy_container['random_forest_tfidf'] = rf_test_accuracy_tf\n\nprint('Training Accuracy: ', accuracy_score(y_train_tfidf,rf.predict(X_train_tfidf)))\nprint('Testing Accuracy: ',rf_test_accuracy_tf )\nprint(\"Training Time: \",training_time_container['random_forest_tfidf'])\nprint(\"Prediction Time: \",prediction_time_container['random_forest_tfidf'])\nprint(confusion_matrix(y_test,ypred_rf_tf ))","8b7c480a":"fig=go.Figure(data=[go.Bar(y=list(training_time_container.values()),x=list(training_time_container.keys()),\n                           marker={'color':np.arange(len(list(training_time_container.values())))}\n                          ,text=list(training_time_container.values()), textposition='auto' )])\n\nfig.update_layout(autosize=True ,plot_bgcolor='rgb(275, 275, 275)',\n                  title=\"Comparison of Training Time of different classifiers\",\n                    xaxis_title=\"Machine Learning Models\",\n                    yaxis_title=\"Training time in seconds\" )\n\nfig.data[0].marker.line.width = 3\nfig.data[0].marker.line.color = \"black\"  \nfig","38dd5d87":"fig=go.Figure(data=[go.Bar(y=list(prediction_time_container.values()),x=list(prediction_time_container.keys()),\n                           marker={'color':np.arange(len(list(prediction_time_container.values())))}\n                          ,text=list(prediction_time_container.values()), textposition='auto' )])\n\nfig.update_layout(autosize=True ,plot_bgcolor='rgb(275, 275, 275)',\n                  title=\"Comparison of Prediction Time of different classifiers\",\n                    xaxis_title=\"Machine Learning Models\",\n                    yaxis_title=\"Prediction time in seconds\" )\n\nfig.data[0].marker.line.width = 3\nfig.data[0].marker.line.color = \"black\"  \nfig","af92a2f2":"fig=go.Figure(data=[go.Bar(y=list(accuracy_container.values()),x=list(accuracy_container.keys()),\n                           marker={'color':np.arange(len(list(accuracy_container.values())))}\n                          ,text=list(accuracy_container.values()), textposition='auto' )])\n\nfig.update_layout(autosize=True ,plot_bgcolor='rgb(275, 275, 275)',\n                  title=\"Comparison of Accuracy Scores of different classifiers\",\n                    xaxis_title=\"Machine Learning Models\",\n                    yaxis_title=\"Accuracy Scores\" )\n\nfig.data[0].marker.line.width = 3\nfig.data[0].marker.line.color = \"black\"  \nfig","aa4a32e9":"svm_skcv = linear_model.SGDClassifier(loss='hinge',alpha=0.0001)\n\n# StratifiedKFold object.\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nlst_accu_stratified_svm = []\n   \nfor train_index, test_index in skf.split(X_smote_tfidf,Y_smote_tfidf):\n    x_train_fold, x_test_fold = X_smote_tfidf[train_index], X_smote_tfidf[test_index]\n    y_train_fold, y_test_fold = Y_smote_tfidf[train_index], Y_smote_tfidf[test_index]\n    svm_skcv.fit(x_train_fold, y_train_fold)\n    lst_accu_stratified_svm.append(svm_skcv.score(x_test_fold, y_test_fold))\n   \n# Print the output.\nprint('List of possible accuracy:', lst_accu_stratified_svm)\nprint('\\nMaximum Accuracy That can be obtained from this model is:',max(lst_accu_stratified_svm)*100, '%')\nprint('\\nMinimum Accuracy:', min(lst_accu_stratified_svm)*100, '%')\nprint('\\nOverall Accuracy:',mean(lst_accu_stratified_svm)*100, '%')","28421740":"rf_skcv = RandomForestClassifier(n_estimators=50)\n\n# StratifiedKFold object.\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nlst_accu_stratified_rf = []\n   \nfor train_index, test_index in skf.split(X_smote_tfidf,Y_smote_tfidf):\n    x_train_fold, x_test_fold = X_smote_tfidf[train_index], X_smote_tfidf[test_index]\n    y_train_fold, y_test_fold = Y_smote_tfidf[train_index], Y_smote_tfidf[test_index]\n    rf_skcv.fit(x_train_fold, y_train_fold)\n    lst_accu_stratified_rf.append(rf_skcv.score(x_test_fold, y_test_fold))\n   \n# Print the output.\nprint('List of possible accuracy:', lst_accu_stratified_rf)\nprint('\\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified_rf)*100, '%')\nprint('\\nMinimum Accuracy:', min(lst_accu_stratified_rf)*100, '%')\nprint('\\nOverall Accuracy:', mean(lst_accu_stratified_rf)*100, '%')","b285eef4":"nb_skcv = MultinomialNB()\n\n# StratifiedKFold object.\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)\nlst_accu_stratified_nb = []\n   \nfor train_index, test_index in skf.split(X_smote_tfidf,Y_smote_tfidf):\n    x_train_fold, x_test_fold = X_smote_tfidf[train_index], X_smote_tfidf[test_index]\n    y_train_fold, y_test_fold = Y_smote_tfidf[train_index], Y_smote_tfidf[test_index]\n    nb_skcv.fit(x_train_fold, y_train_fold)\n    lst_accu_stratified_nb.append(nb_skcv.score(x_test_fold, y_test_fold))\n   \n# Print the output.\nprint('List of possible accuracy:', lst_accu_stratified_nb)\nprint('\\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified_nb)*100, '%')\nprint('\\nMinimum Accuracy:', min(lst_accu_stratified_nb)*100, '%')\nprint('\\nOverall Accuracy:', mean(lst_accu_stratified_nb)*100, '%')","1b373071":"import joblib","690a212a":"# cv and tfidf\njoblib.dump(count_vectorizer, open('cv.pkl', 'wb'),8)\njoblib.dump(tfidf_vectorizer, open('tfidf.pkl', 'wb'),8)","a46dedb3":"# mnb \njoblib.dump(nb, open('mnb.pkl', 'wb'),8)\n\n# svm\njoblib.dump(svm_classifier, open('svm.pkl', 'wb'),8)\n\n# randomforest\njoblib.dump(rf , open('rf.pkl', 'wb'),8)","6e6c51a5":"max_features = 6433     # the maximum number of words to keep, based on word frequency\ntokenizer = Tokenizer(num_words=max_features )\ntokenizer.fit_on_texts(df['cleaned_text'].values)","dc5b4a5c":"X = tokenizer.texts_to_sequences(df['cleaned_text'].values)\nX = pad_sequences(X, padding = 'post', maxlen = 6433 )","47e0cfe1":"X","12a0bf96":"X.shape[1]","53616695":"Y = pd.get_dummies(df['label']).values","c2bb1f8e":"Y","eb0226a3":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25, random_state = 42,stratify = Y)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","10d9fcb6":"embid_dim = 300\nlstm_out = 32\n\n\nmodel = keras.Sequential()\nmodel.add(Embedding(max_features, embid_dim, input_length = X.shape[1] ))\nmodel.add(Bidirectional(LSTM(lstm_out)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(9,activation = 'softmax'))\n\nmodel.summary()","0818f538":"batch_size = 128\nearlystop = EarlyStopping(monitor='loss', min_delta=0, patience=3, verbose=0, mode='auto')\nmodel.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\nhistory = model.fit(X_train, Y_train, epochs = 10, batch_size=batch_size, verbose = 1, validation_data= (X_test, Y_test),callbacks=[earlystop])","8e1ed51d":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","f110e0cf":"model.save('lstm.h5')","4e78c4b6":"df_bert = df.sample(frac=0.5)","fcf63b49":"df_bert.reset_index(inplace=True)","9356c591":"df_bert['target'].value_counts()","52e7d7e0":"X_train, X_val, y_train, y_val = train_test_split(df_bert.index.values, \n                                                  df_bert.target.values, \n                                                  test_size=0.15, \n                                                  random_state=42, \n                                                  stratify=df_bert.target.values)","5484d53e":"df_bert['data_type'] = ['not_set']*df_bert.shape[0]\n\ndf_bert.loc[X_train, 'data_type'] = 'train'\ndf_bert.loc[X_val, 'data_type'] = 'val'","e6cf3655":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n                                          do_lower_case=True)","788e38c3":"encoded_data_train = tokenizer.batch_encode_plus(\n    df_bert[df_bert.data_type=='train'].final_text.values, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=256, \n    return_tensors='pt'\n)\n\nencoded_data_val = tokenizer.batch_encode_plus(\n    df_bert[df_bert.data_type=='val'].final_text.values, \n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=256, \n    return_tensors='pt'\n)\n\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(df_bert[df_bert.data_type=='train'].target.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nlabels_val = torch.tensor(df_bert[df_bert.data_type=='val'].target.values)","1d79970d":"dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)","b2d047c3":"# length of training and validation data \nlen(dataset_train), len(dataset_val)","cc6d4007":"model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n                                                      num_labels=12,\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)","fc15cbb9":"batch_size = 3\n\ndataloader_train = DataLoader(dataset_train, \n                              sampler=RandomSampler(dataset_train), \n                              batch_size=batch_size)\n\ndataloader_validation = DataLoader(dataset_val, \n                                   sampler=SequentialSampler(dataset_val), \n                                   batch_size=batch_size)","dfc851e9":"optimizer = AdamW(model.parameters(),\n                  lr=1e-5, \n                  eps=1e-8)","9f39cfb7":"epochs = 3\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*epochs)","341d0fd0":"def f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')","c5f0a6c8":"seed_val = 17\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","d435db50":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nprint(device)","eb97ce1a":"def evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","73e8469a":"for epoch in tqdm(range(1, epochs+1)):\n    \n    #model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n        \n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')","0ead0858":"Now we are going to combine our text, this is our final text","f52ba0bb":"DataLoader combines a dataset and a sampler and provides an iterable over the given dataset.","dfcccb01":"It's time to do lemmatization","2613d9d7":"Check the basic stats of number of words, like maximum, minimum, average number of words","ae3a7096":"Let's first import all the required libraries","3a32a062":"# Tf-Idf","6ef588e3":"- So in politics we have mideast, guns and misc sub-topics we will replace all to politics\n- We have sub-categories in sports, we will going to replace this also into sports\n- We have two sub categories in religion, we will replace them to one\n- We are going to make 9 categories in all","f27d103a":"## RandomForest ","afa3ebf5":"Now we will construct the BERT Tokenizer.Based on wordpiece.We will intantiate a pre-trained model configuration to encode our data","0e759406":"## SVM","9ce213d2":"# Save the models","ccba0d1d":"## Multinomial Naive Bayes","acdfc269":"So now we will going to make the bert model.In our kernel we have less memory so we will going to take 50% of our dataset","1bd031de":"We are treating each title as its unique sequence, so one sequence will be classified into one of the 12 labels","291bef27":"# Bag-of-Words","5f9ba95a":"Let's see the count of each label","8318d394":"Now we got encoded dataset, we can create training data and validation data","e6af2720":"We are going to make a number of words column in which there is the number of words in a particular text","e44b4bc6":"We will going to use the 20 news group dataset.Let's load the dataset in dataframe","f3f188d6":"### Save LSTM model","6d0b44df":"# Logistic Regression","5eb20de5":"# Stratified K-fold CV","d5b94667":"## RandomForest","f08076fc":"So maximu number of words text is belongs to electronics category.In our dataset we have some rows where there are no text at all i.e. the number of words is 0.We will drop those rows","1ecfb6f3":"# BERT","e4c074ba":"Now we are going to remome the stopwords from the sentences","38c51226":"Stopwords are those english words which do not add much meaning to a sentence.They are very commonly used words and we do not required those words. So we can remove those stopwords","834a6293":"We will not going to create RNN model due to its vanishing gradient problem instead of that we will going to create LSTM model.LSTMs have an additional state called \u2018cell state\u2019 through which the network makes adjustments in the information flow. The advantage of this state is that the model can remember or forget the leanings more selectively.\nFirst of all we are going to do tokenization then we will generate sequence of n-grams.After that we will going to do padding.Padding is required because all the sentences are of different length so we need to make them of same length.We will going to do this by adding 0 in the end of the text with the help of pad_sequences function of keras","4cf02bfb":"CountVectorizer is used to transform a given text into a vector on the basis of the frequency(count) of each word that occurs in the entire text.It involves counting the number of occurences each words appears in a document(text)","b27e600b":"So the maximum number of words in our dataset is 11,765.Let's have a look at it","3c006ee6":"Now our text has been cleaned, we will convert the labels into numeric values using LableEncoder()","af43559d":"### Plot Accuracy and Loss","a9c72283":"We will later use the label enocder to convert the labels (categorical value) into numeric value.So now, we will drop that column","0efe447f":"## Multinomial Naive Bayes","4dc56d77":"Let's convert our cleaned text into tokens","f3c15a01":"# Load Dataset","0b244a1b":"In machine learning, when we want to train our ML model we split our entire dataset into train set and test set using train test split class present in sklearn.Then we train our model on train set and test our model on test set. The problems that we face are, whenever we change the random_state parameter present in train_test_split(), we get different accuracy for different random_state and hence we can\u2019t exactly point out the accuracy for our model.<br>\nThe solution for the this problem is to use K-Fold Cross-Validation. But K-Fold Cross Validation also suffer from second problem i.e. random sampling.<br>\nThe solution for both first and second problem is to use Stratified K-Fold Cross-Validation.Stratified k-fold cross-validation is same as just k-fold cross-validation, But in Stratified k-fold cross-validation, it does stratified sampling instead of random sampling.","9b23fed7":"# Dependent and Independent Variable","81128de6":"- To convert all the titles from text into encoded form, we use a function called *batch_encode_plus* and we will proceed train and test data seperately.The first parameter inside the function is the text.\n- *add_special_tokens = True* means the sequences will encoded with the special tokens realtive to their model\n- *return_attention_mask=True* returns the attention mask according to the special tokenizer defined by *max_length* attribute","91191cc3":"# LSTM","c40c2344":"## SVM using Stochastic Gradient Descent","f0b49b17":"## Train-Test Split","c4a309ec":"So our model is created now it's time to train our model, we will going to use 10 epochs","794d248d":"Let's see the number of unique targets ","7aeebe3f":"# SMOTE technique to balance the dataset","2ba36232":"So we can clearly see that our dataset is imbalanced dataset.We will use SMOTE technique to balance the dataset.SMOTE is an oversampling technique where the synthetic samples are generated for the minority class.The algorithm helps to overcome the overfitting problem posed by random sampling. ","9fef34d8":"Let's check number of stopwords in nltk library","f0a5e511":"We will use f1 score as a performance metrics","78746c5c":"Text classification is the task of assigning a set of predefined categories to open-ended text. Text classifiers can be used to organize, structure, and categorize pretty much any kind of text \u2013 from documents, medical studies and files, and all over the web.We will classify the text into 9 categories.The 9 categories are:\n- computer       \n- science        \n- politics       \n- sport          \n- automobile     \n- religion        \n- medicine       \n- sales           \n- alt.atheism","7f45f7a3":"# Import Libraries","2d6789fb":"### Training loop","368257bb":"Tf-Idf stands for Term Frequency-Inverse document frequency.It is a techinque to quantify a word in documents,we generally compute a weight to each word which signifies the importance of the word which signifies the importance of the word in the document and corpus","56826b8d":"# Data Pre-Processing","23cb5e53":"Now it's time to clean our dataset, we will lower the text, remove the text in square brackets, remove links and remove words containing numbers","b48f2620":"In our dataset we have very less data in a each categorical label and there are 20 categories which are too much.We will combine the sub-categories","9d2f6211":"So our dataset is imbalanced, we split the dataset in a stratified way","b40cf60c":"So we have cleaned the dataset and remove stopwords, it's possible that there are rows in which the text length is 0.We will find those rows and remove them"}}