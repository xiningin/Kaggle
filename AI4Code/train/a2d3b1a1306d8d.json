{"cell_type":{"2a847e78":"code","2c931418":"code","33108dcb":"code","e435e593":"code","877c3b26":"code","70f043c6":"code","d6334ad6":"code","5e24ed4d":"code","16bf4473":"code","9dc7578f":"markdown","4c1b62ca":"markdown","b54fa857":"markdown","d495f33d":"markdown","d2a24081":"markdown","5ebd2c8a":"markdown","8570f3d6":"markdown","2e9d22eb":"markdown","9b3af796":"markdown"},"source":{"2a847e78":"import joblib\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\n","2c931418":"################################################\n# Helper Functions \/ utils\n################################################\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    return cat_cols, num_cols, cat_but_car\n\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe","33108dcb":"def hitters_data_prep(dataframe):\n\n    # Specifying variable types\n\n    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe, cat_th=5, car_th=20)\n\n\n    # FEATURE ENGINEERING\n    dataframe[\"new_Hits\/CHits\"] = dataframe[\"Hits\"] \/ dataframe[\"CHits\"]\n    dataframe[\"new_OrtCHits\"] = dataframe[\"CHits\"] \/ dataframe[\"Years\"]\n    dataframe[\"new_OrtCHmRun\"] = dataframe[\"CHmRun\"] \/ dataframe[\"Years\"]\n    dataframe[\"new_OrtCruns\"] = dataframe[\"CRuns\"] \/ dataframe[\"Years\"]\n    dataframe[\"new_OrtCRBI\"] = dataframe[\"CRBI\"] \/ dataframe[\"Years\"]\n    dataframe[\"new_OrtCWalks\"] = dataframe[\"CWalks\"] \/ dataframe[\"Years\"]\n\n    dataframe[\"New_Average\"] = dataframe[\"Hits\"] \/ dataframe[\"AtBat\"]\n    dataframe['new_PutOutsYears'] = dataframe['PutOuts'] * dataframe['Years']\n    dataframe[\"new_RBIWalksRatio\"] = dataframe[\"RBI\"] \/ dataframe[\"Walks\"]\n    dataframe[\"New_CHmRunCAtBatRatio\"] = dataframe[\"CHmRun\"] \/ dataframe[\"CAtBat\"]\n    dataframe[\"New_BattingAverage\"] = dataframe[\"CHits\"] \/ dataframe[\"CAtBat\"]\n\n    # remove salary bigger than up limit\n    q3 = 0.90\n    salary_up = int(dataframe[\"Salary\"].quantile(q3))\n    dataframe = dataframe[(dataframe[\"Salary\"] < salary_up)]\n\n    # One-Hot Encoding\n    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe)\n\n    dataframe = one_hot_encoder(dataframe, cat_cols)\n\n    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe, cat_th=5, car_th=20)\n\n\n    ####################################################\n    # Feature importances and Scaler Transform\n    ####################################################\n    y = dataframe[\"Salary\"]\n    X = dataframe.drop([\"Salary\"], axis=1)\n\n    X_scaled = StandardScaler().fit_transform(dataframe[num_cols])\n    dataframe[num_cols] = pd.DataFrame(X_scaled, columns=dataframe[num_cols].columns)\n    dataframe.dropna(inplace=True)\n\n\n\n    return X, y\n","e435e593":"\n# Base Models\ndef base_models(X, y):\n    print(\"Base Models....\")\n    models = [('LR', LinearRegression()),\n              (\"Ridge\", Ridge()),\n              (\"Lasso\", Lasso()),\n              (\"ElasticNet\", ElasticNet()),\n              ('KNN', KNeighborsRegressor()),\n              ('CART', DecisionTreeRegressor()),\n              ('RF', RandomForestRegressor()),\n              ('SVR', SVR()),\n              ('GBM', GradientBoostingRegressor()),\n              (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n              (\"LightGBM\", LGBMRegressor()),\n              # (\"CatBoost\", CatBoostRegressor(verbose=False))\n              ]\n\n    for name, regressor in models:\n        rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n\n\n\n","877c3b26":"\ncart_params = {'max_depth': range(1, 20),\n               \"min_samples_split\": range(2, 30)}\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [8, 15, 20],\n             \"n_estimators\": [200, 500, 1000]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01, 0.01],\n                  \"max_depth\": [5, 8, 12, 20],\n                  \"n_estimators\": [100, 200, 300, 500],\n                  \"colsample_bytree\": [0.5, 0.8, 1]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1, 0.001],\n                   \"n_estimators\": [300, 500, 1500],\n                   \"colsample_bytree\": [0.5, 0.7, 1]}\n\nregressors = [(\"CART\", DecisionTreeRegressor(), cart_params),\n              (\"RF\", RandomForestRegressor(), rf_params),\n              # ('XGBoost', XGBRegressor(objective='reg:squarederror'), xgboost_params),\n              ('LightGBM', LGBMRegressor(), lightgbm_params)]\n\n","70f043c6":"\ndef hyperparameter_optimization(X, y, cv=10, scoring=\"neg_mean_squared_error\"):\n    print(\"Hyperparameter Optimization....\")\n    best_models = {}\n    for name, regressor, params in regressors:\n        print(f\"########## {name} ##########\")\n        rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n\n        gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n\n        final_model = regressor.set_params(**gs_best.best_params_)\n        rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n        print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n        best_models[name] = final_model\n    return best_models\n\n","d6334ad6":"# Stacking & Ensemble Learning\n\ndef voting_regressor(best_models, X, y):\n    print(\"Voting Classifier...\")\n    voting_reg = VotingRegressor(estimators=[('RF', best_models[\"RF\"]),\n                                             ('LightGBM', best_models[\"LightGBM\"])])\n    voting_reg.fit(X, y)\n    np.mean(np.sqrt(-cross_val_score(voting_reg, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n    return voting_reg\n\n","5e24ed4d":"# from hitters_pipeline import hitters_data_prep\nimport os\n\ndef main():\n    df = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\n    X, y = hitters_data_prep(df)\n    base_models(X, y)\n    best_models = hyperparameter_optimization(X, y)\n    voting_reg = voting_regressor(best_models, X, y)\n    os.chdir(\".\/\")\n    joblib.dump(voting_reg, \"voting_reg_hitters.pkl\")\n    print(\"Voting_reg has been created\")\n    return voting_reg\n\nif __name__ == \"__main__\":\n    main()","16bf4473":"# from hitters_pipeline import hitters_data_prep \n# caliing data preparation file and data\ndf = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\n\nX, y = hitters_data_prep(df)\nrandom_user = X.sample(1, random_state=45)\nnew_model = joblib.load(\".\/voting_reg_hitters.pkl\")\nnew_model.predict(random_user)","9dc7578f":"<a id = \"1\"><\/a><br>\n## 1. Libraries","4c1b62ca":"<a id = \"3\"><\/a><br>\n## 3. Base Models","b54fa857":"<a id = \"2\"><\/a><br>\n## 2. Data Preprocessing & Feature Engineering","d495f33d":"<a id = \"5\"><\/a><br>\n## 5. Stacking & Ensemble Learning","d2a24081":"<a id = \"7\"><\/a><br>\n## 7. Prediction\n","5ebd2c8a":"<a id = \"4\"><\/a><br>\n## 4. Hyperparameter Optimization","8570f3d6":"# Introduction\nML models can help organizations to spot opportunities and risks, improve their business strategy, and deliver better customer experience. But gathering and processing data for ML models, using it to train and test the models, and finally operationalizing machine learning, can take a long time. \n\nCompanies want their data science teams to speed up the process so they can deliver valuable business predictions faster. \n\nThat\u2019s where ML pipelines come in. By automating workflows with machine learning pipeline monitoring, ML pipelines bring you to operationalizing machine learning models sooner. \n\nAs well as cutting down on the time it takes to produce a new ML model, machine learning pipeline orchestration also helps you improve the quality of your machine learning models. We call it a pipeline, but actual pipelines are one-way and one-time only, which isn\u2019t the case for ML pipelines.\n\n<font color = 'blue'>\nContent: \n\n1. [Libraries](#1)\n1. [Data Preprocessing & Feature Engineering](#2)\n1. [Base Models](#3)\n1. [Hyperparameter Optimization](#4)\n1. [Stacking & Ensemble Learning](#5)\n1. [Pipeline Main Function](#6)\n1. [Prediction](#7)\n1. [References](#8)","2e9d22eb":"<a id = \"6\"><\/a><br>\n## 6. Pipeline Main Function","9b3af796":"<a id = \"8\"><\/a><br>\n## 8. References\n* https:\/\/github.com\/mvahit\n* https:\/\/www.veribilimiokulu.com\/\n* https:\/\/www.kaggle.com\/azizesultanbilge\/code\n* https:\/\/www.kaggle.com\/fdemirok"}}