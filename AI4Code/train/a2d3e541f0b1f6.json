{"cell_type":{"9b1242be":"code","aae4367e":"code","24a53f64":"code","38029396":"code","ac23445a":"code","e815b97b":"code","753e71e3":"code","96450786":"code","e604e278":"code","862d48f3":"code","0ad5ee9d":"code","cb7d8801":"markdown","0fff9096":"markdown","044bab56":"markdown","f8672702":"markdown","d1ed3e07":"markdown"},"source":{"9b1242be":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nfrom sklearn.utils import shuffle","aae4367e":"def getData(balance_ones=True):\n    Y = []\n    X = []\n    first = True\n    for line in open('..\/input\/fer20131.csv'):\n        if first:\n            first = False\n        else:\n            row = line.split(',')\n            Y.append(int(row[0]))\n            X.append([int(p) for p in row[1].split()])\n\n    X, Y = np.array(X) \/ 255.0, np.array(Y)\n    \n    if balance_ones:\n        X0, Y0 = X[Y!=1, :], Y[Y!=1]\n        X1 = X[Y==1, :]\n        X1 = np.repeat(X1, 9, axis=0)\n        X = np.vstack([X0, X1])\n        Y = np.concatenate((Y0, [1]*len(X1)))\n\n    return X, Y\n\ndef getImageData():\n    X, Y = getData()\n    N, D = X.shape\n    d = int(np.sqrt(D))\n    X = X.reshape(N, d, d, 1)\n    return X, Y","24a53f64":"X, Y = getImageData()","38029396":"labels = list(set(Y))\n\nfig = plt.figure(figsize=(15, 20))\ncolumns = 5\nrows = 5\n\nfor i in labels:\n    imagens = X[Y==i]\n    qtd_imagens = len(imagens)\n    imagem = imagens[random.randint(0, qtd_imagens), :].reshape((48, 48))\n    ax = fig.add_subplot(rows, columns, i + 1)\n    ax.set_title('%d' % (i))\n    plt.imshow(imagem, cmap='gray')\nplt.show()","ac23445a":"def init_weight_and_bias(M1, M2):\n    W = np.random.randn(M1, M2) \/ np.sqrt(M1)\n    b = np.zeros(M2)\n    return W, b","e815b97b":"class HiddenLayer(object):\n    def __init__(self, M1, M2, activation=None):\n        self.M1 = M1\n        self.M2 = M2\n        self.activation = activation\n        W, b = init_weight_and_bias(M1, M2)\n        self.W = tf.Variable(W.astype(np.float32))\n        self.b = tf.Variable(b.astype(np.float32))\n        self.params = [self.W, self.b]\n    \n    def forward(self, X, is_training):\n        act_value = tf.matmul(X,self.W) + self.b\n        if self.activation is not None:\n            act_value = self.activation(act_value)\n        return act_value","753e71e3":"class ANN(object):\n    def __init__(self, hidden_layer_sizes, layer_class=HiddenLayer):\n        self.hidden_layer_sizes = hidden_layer_sizes\n        self.layer_class = layer_class\n    \n    def build_Layers(self, X, Y, activation):\n        _, D = X.shape\n        K = len(set(Y))\n        \n        self.layers = []\n        M1 = D\n        for M2 in self.hidden_layer_sizes:\n            h = self.layer_class(M1, M2, activation)\n            self.layers.append(h)\n            M1 = M2\n        \n        h = HiddenLayer(M1, K)\n        self.layers.append(h)\n        \n        self.params = []\n        for h in self.layers:\n            self.params += h.params\n        \n        return (None, D)\n    \n    def train(self, inputs, labels, Xtrain, Ytrain, Xvalid, Yvalid, epochs, n_batches, batch_sz, print_period):\n        costs = []\n        init = tf.global_variables_initializer()\n        with tf.Session() as session:\n            session.run(init)\n            for i in range(epochs):\n                Xtrain, Ytrain = shuffle(Xtrain, Ytrain)\n                for j in range(n_batches):\n                    Xbatch = Xtrain[j*batch_sz:(j*batch_sz + batch_sz)]\n                    Ybatch = Ytrain[j*batch_sz:(j*batch_sz + batch_sz)]\n                    session.run(self.train_op, feed_dict={inputs: Xbatch, labels: Ybatch})\n                    \n                    if ((j + 1) % print_period == 0):\n                        c = session.run(self.cost_op, feed_dict={inputs: Xvalid, labels: Yvalid})\n                        p = session.run(self.predict_op, feed_dict={inputs: Xvalid})\n                        costs.append(c)\n                        acc = np.mean(p != Yvalid)\n                        print(\"i:\", i, \"j:\", j, \"nb:\", n_batches, \"cost:\", c, \"error rate:\", acc)\n        return costs\n    \n    def fit(self, X, Y, activation=tf.nn.relu, learning_rate=1e-3, reg=1e-3,mu=0.99, decay=0.99999, print_period=20, epochs=20, batch_sz=100, show_fig=False):\n        learning_rate = np.float32(learning_rate)\n        mu = np.float32(mu)\n        reg = np.float32(reg)\n        decay = np.float32(decay)\n        \n        X = X.astype(np.float32)\n        Y = Y.astype(np.int32)\n        X, Y = shuffle(X, Y)\n        Xvalid  = X[-1000:]\n        Yvalid  = Y[-1000:]\n        Xtrain = X[:-1000]\n        Ytrain = Y[:-1000]\n        \n        N = Xtrain.shape[0]\n        \n        input_shape = self.build_Layers(X, Y, activation)\n        \n        if batch_sz is None:\n            batch_sz = N\n            \n        inputs = tf.placeholder(tf.float32, shape=input_shape, name='inputs')\n        labels = tf.placeholder(tf.int32, shape=(None,), name='labels')\n        logits = self.forward(inputs, is_training=True)\n        \n        self.cost_op = tf.reduce_mean(\n            tf.nn.sparse_softmax_cross_entropy_with_logits(\n                logits=logits,\n                labels=labels\n            )\n        )\n        \n        if(reg is not None):\n            rcost = reg*sum([tf.nn.l2_loss(p) for p in self.params])\n            self.cost_op += rcost\n        \n        self.train_op = tf.train.RMSPropOptimizer(learning_rate, decay=decay, momentum=mu).minimize(self.cost_op)\n        #self.train_op = tf.train.MomentumOptimizer(learning_rate, momentum=0.9, use_nesterov=True).minimize(self.cost_op)\n\n        self.predict_op = self.predict(inputs)\n        \n        n_batches = N \/\/ batch_sz\n        \n        costs = self.train(inputs, labels, Xtrain, Ytrain, Xvalid, Yvalid, \n                           epochs, n_batches, batch_sz, print_period)\n       \n        if show_fig:\n            plt.plot(costs)\n            plt.show()\n            \n    def forward(self, X, is_training):\n        out = X\n        for h in self.layers:\n            out = h.forward(out, is_training)\n        return out\n    \n    def predict(self, X):\n        pY = self.forward(X, is_training=False)\n        return tf.argmax(pY, 1)\n        \n        ","96450786":"class ConvPoolLayer(object):\n    def __init__(self, filter_width, filter_height, feature_in, feature_out, pool_sz=(2,2)):\n        self.pool_sz = pool_sz\n        self.shape = (filter_width, filter_height, feature_in, feature_out)\n        self.init_filter()\n        \n    def init_filter(self):\n        W_init = np.random.randn(*self.shape) * np.sqrt(2) \/ np.sqrt(np.prod(self.shape[:-1]) + self.shape[-1]*np.prod(self.shape[:-2] \/ np.prod(self.pool_sz)))\n        b_init = np.zeros(self.shape[-1], dtype=np.float32)\n        self.W = tf.Variable(W_init.astype(np.float32))\n        self.b = tf.Variable(b_init)\n        self.params = [self.W, self.b]\n        \n    def convpool(self, X):\n        conv_out = tf.nn.conv2d(X, self.W, strides=[1, 1, 1, 1], padding='SAME')\n        conv_out = tf.nn.bias_add(conv_out, self.b)\n        ksize = [1, self.pool_sz[0], self.pool_sz[1], 1]\n        pool_out = tf.nn.max_pool(conv_out, ksize=ksize, strides=ksize, padding='SAME')\n        return pool_out\n        \n        ","e604e278":"class CNN(ANN):\n    def __init__(self, hidden_layer_sizes, convpool_layer_sizes):\n        ANN.__init__(self, hidden_layer_sizes)\n        self.convpool_layer_sizes = convpool_layer_sizes\n        \n\n    def build_Layers(self, X, Y, activation):\n        _, H, W, C = X.shape\n        pool_sz = (2, 2)\n        K = len(set(Y))\n        \n        self.convpool_layers = []\n        self.params = []\n        self.layers = []\n        \n        feature_in = C\n        for feature_out, filter_w, filter_h in self.convpool_layer_sizes:\n            layer = ConvPoolLayer(filter_w, filter_h, feature_in, feature_out, pool_sz)\n            self.params += layer.params\n            self.convpool_layers.append(layer)\n            feature_in = feature_out\n            \n        M1 = feature_in * ((H \/\/ (len(self.convpool_layers) * pool_sz[0])) * \n                           (W \/\/ (len(self.convpool_layers) * pool_sz[1])))\n       \n        for M2 in self.hidden_layer_sizes:\n            layer = self.layer_class(M1, M2, activation)\n            self.params += layer.params\n            self.layers.append(layer)\n            M1 = M2\n        \n        h = HiddenLayer(M1, K)\n        self.params += h.params\n        self.layers.append(h)\n        \n        \n        return (None, H, W, C)\n    \n    def forward(self, X, is_training):\n        out = X\n        for layer in self.convpool_layers:\n            out = layer.convpool(out)\n        \n        out_shape = out.get_shape().as_list()\n        out = tf.reshape(tensor=out, shape=[-1, np.prod(out_shape[1:])])\n        \n        for layer in self.layers:\n            out = layer.forward(out, is_training)\n            \n        return out","862d48f3":"## Experiment 1\n\nX, Y = getData()\n\nmodel = ANN([2000, 1000, 500])\nmodel.fit(X, Y, show_fig=True, batch_sz=100, epochs=10, learning_rate=1e-2, decay=0.999)","0ad5ee9d":"## Experiment 2\n\nX, Y = getImageData()\n\nmodel = CNN(convpool_layer_sizes=[(20, 5, 5), (20, 5, 5)],\n           hidden_layer_sizes=[500, 300])\nmodel.fit(X, Y, show_fig=True, batch_sz=30, epochs=3)","cb7d8801":"### Este Notebook \u00e9 resultado de meus estudos do curso de Redes Neurais Convolucionais do Lazy Programmer. Segue: https:\/\/lazyprogrammer.me\/deep-learning-courses\/.","0fff9096":"# Creating Neural Network using Tensorflow","044bab56":"# Prepare the Data","f8672702":"## CNN","d1ed3e07":"# Experiments"}}