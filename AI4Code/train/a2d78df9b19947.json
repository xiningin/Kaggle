{"cell_type":{"5b8e03a8":"code","64517b10":"code","34abe1f8":"code","01a9ec5f":"code","d0ed9e13":"code","4147805b":"code","5567b639":"code","75e55fe6":"code","2d5d4fe0":"code","d46d03ac":"code","44984d9a":"code","f0658abf":"code","558094be":"code","5c39cf92":"code","4bbacbfd":"code","031f0f2f":"code","c4fbd625":"code","6cd72b70":"code","222c2eb6":"code","4d31a52f":"code","723f5148":"code","fbd9acd8":"code","749ad65e":"code","b1dc017f":"code","d6be7490":"code","5af4248b":"code","3ae5a6ed":"code","981ad65e":"code","27622664":"code","722bc76b":"markdown","04c9843b":"markdown","2c932dbc":"markdown","1dc220e0":"markdown"},"source":{"5b8e03a8":"import numpy as np \nimport pandas as pd \nimport os\nfrom collections import defaultdict\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score,f1_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier","64517b10":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsub_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","34abe1f8":"train_df.head()","01a9ec5f":"test_df.head()","d0ed9e13":"train_df.isnull()","4147805b":"x=train_df.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","5567b639":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(15,5))\ntrain_df_len1 = train_df[train_df['target']==1]['text'].str.len()\nax1.hist(train_df_len1,color='red')\nax1.set_title('disaster tweets')\ntrain_Df_len2 = train_df[train_df['target']==0]['text'].str.len()\nax2.hist(train_Df_len2,color = 'green')\nax2.set_title('Non Disaster Tweets')","75e55fe6":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train_df[train_df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","2d5d4fe0":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n\n","d46d03ac":"for k in range(0,len(top)):\n    print(top[k][0])","44984d9a":"x,y = zip(*top)\nprint(x)\nplt.bar(x,y)","f0658abf":"x = train_df[\"text\"]\ny = train_df[\"target\"]\n","558094be":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","5c39cf92":"X_train","4bbacbfd":"vect = CountVectorizer(stop_words = 'english')\n\nx_train_cv = vect.fit_transform(X_train)\nx_test_cv = vect.transform(X_test)\n","031f0f2f":"x_train_cv","c4fbd625":"clf = MultinomialNB()\nclf.fit(x_train_cv, y_train)","6cd72b70":"pred = clf.predict(x_test_cv)","222c2eb6":"pred","4d31a52f":"y_test","723f5148":"# y_pred = model.predict(X_test)\n\nf1score = f1_score(y_test,pred)\nprint(f\"Model Score: {f1score * 100} %\")","fbd9acd8":"confusion_matrix(y_test, pred)","749ad65e":"accuracy_score(y_test,pred)","b1dc017f":"y_test = test_df[\"text\"]\ny_test_cv = vect.transform(y_test)\npreds = clf.predict(y_test_cv)","d6be7490":"sub_df[\"target\"] = preds\nsub_df.to_csv(\"submission.csv\",index=False)","5af4248b":"sub_df.describe()","3ae5a6ed":"predicted = rf.predict(x_test_cv)\nprint(predicted)","981ad65e":"y_test1 = test_df[\"text\"]\ny_test_cv = vect.transform(y_test1)\npreds = rf.predict(y_test_cv)","27622664":"sub_df[\"target\"] = preds\nsub_df.to_csv(\"submission_final.csv\",index=False)","722bc76b":"*Seperating target and other data*","04c9843b":"**Target Distribution**","2c932dbc":"**Loading Data**","1dc220e0":"**Analysis**"}}