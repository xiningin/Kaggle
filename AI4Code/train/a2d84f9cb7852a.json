{"cell_type":{"15d7f181":"code","05bb5495":"code","6527ae0c":"code","3e416c56":"code","bb1814d6":"code","29627790":"code","f2412d41":"code","7fb66d6e":"code","c7f404c3":"code","6bff469a":"code","6db77630":"code","a665a630":"code","da4fd1e7":"code","c72d37e1":"code","c0e00418":"code","43d7e338":"code","f8e737e3":"code","431b979d":"code","8be0ec30":"code","9b7fe901":"code","96b1295e":"code","6a6d3409":"code","dc213e66":"code","5b1f4ed2":"code","39401c05":"code","cd1b9571":"code","51ecfe5b":"code","9aa9d105":"code","ca376d6e":"code","5eac9c04":"code","e1ae7ff9":"code","f4753862":"code","0a4f13de":"code","4579c668":"code","a987322e":"code","27f6ef63":"code","7f82a29f":"code","26434bc1":"code","833143a6":"code","a9110868":"code","ccf9fedf":"code","32db4c5b":"code","dbf02a4b":"code","9a3e4a79":"code","80fe7f36":"code","3be9a3d9":"code","c61c94ec":"code","d5385c5d":"code","371d2545":"code","05170b24":"code","057c7149":"code","186100f0":"code","41921c76":"code","ad5fa0c8":"code","63ec65de":"code","e9875f6c":"code","ace62257":"code","59cc1c38":"code","3ea195af":"code","f8391de8":"code","9a8d0c6f":"code","195e28bb":"code","39c51b68":"code","530e84d2":"code","fd044969":"code","864e3569":"code","ab586680":"code","125f09f6":"code","dbaf5f6f":"code","2be0e1bc":"code","d8e21db1":"code","b69a0724":"code","4f1527ae":"code","be520ebd":"code","c04b9cc2":"code","6e2ff2c4":"code","5dbb4454":"code","0d01ab1e":"code","a9dd1686":"code","c208968a":"code","e4335c7b":"code","f95d32f6":"code","55328225":"code","1633cb3d":"code","e5f94890":"code","d119c29d":"code","606add74":"code","3a02474c":"code","900b0ac9":"code","2ee7fd7a":"code","6cfc7972":"code","47ca8828":"code","b37d20f9":"code","090d1f54":"code","ab0cad83":"code","a7151793":"code","2afa4f5d":"code","106160a2":"code","4be62db7":"code","bb8db7e3":"code","c7681474":"code","f3b1684b":"code","dfbca05d":"code","6e6baf20":"code","73303c2a":"code","38f79ca4":"code","d055129d":"code","0ad920fd":"code","c8eda68f":"code","5ecc008b":"code","bf4694b3":"code","a8ad2788":"code","c6312e37":"code","7e915f19":"code","8731b3df":"code","91d549e4":"markdown","a47a563c":"markdown","45ca7d84":"markdown","b5ddcabd":"markdown","309d584e":"markdown","114cd30f":"markdown","9019fbb3":"markdown","a66c1c3e":"markdown","110250c8":"markdown","1698fbaf":"markdown","4e560db0":"markdown","b55f9347":"markdown","3ec73031":"markdown","337f6cf2":"markdown","eec6540b":"markdown","690fca3b":"markdown","0b9a9ef4":"markdown","59805a0b":"markdown","d54df07e":"markdown","ec52c3b4":"markdown","13a47c92":"markdown","a6942fab":"markdown","723bab2b":"markdown","21aa5a9c":"markdown","a2a9fb24":"markdown","57e393fe":"markdown","d622a3d0":"markdown","13b8f063":"markdown","846dfb06":"markdown","d9e1ed41":"markdown","8a1b6d79":"markdown","7c95a644":"markdown","cb118a2b":"markdown","15d386fb":"markdown","b6fe58a6":"markdown","af7528e9":"markdown"},"source":{"15d7f181":"# !pip install --upgrade pip\n# !pip install pattern --no-dependencies\n!pip install --upgrade pandarallel\n# !pip install swifter\n!pip install wordninja\n!pip install LDA2Vec","05bb5495":"# import swifter\nimport wordninja\nimport lda2vec\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport pickle\nimport heapq\nfrom collections import OrderedDict\nimport pandarallel\nfrom gensim import corpora, models\nfrom sklearn.decomposition import LatentDirichletAllocation as LDA\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import doc2vec,Doc2Vec\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import preprocess_string, strip_tags,strip_punctuation, strip_multiple_whitespaces,remove_stopwords\nfrom gensim.utils import lemmatize","6527ae0c":"pandarallel.pandarallel.initialize(progress_bar= True)\n# clean_df['top_500'] = clean_df['Clean_narrative'].parallel_apply(extract_topn_from_vector)","3e416c56":"# clean_df = pd.read_csv('\/kaggle\/input\/lda-list-of-list-of-tokens\/LDA_training_corpus.csv')","bb1814d6":"# import ast\n# list_of_list_of_tokens = ast.literal_eval(list_of_list_of_tokens)","29627790":"# clean_df['list_of_list_of_tokens']= clean_df['list_of_list_of_tokens'].parallel_apply(ast.literal_eval)","f2412d41":"# list_of_list_of_tokens = clean_df.list_of_list_of_tokens.to_list()","7fb66d6e":"# len(list_of_list_of_tokens)","c7f404c3":"# pickle_in = open('\/kaggle\/input\/lda-dictionary\/dictionary_LDA.sav',\"rb\")\n# dictionary_LDA = pickle.load(pickle_in)\n# pickle_in.close()","6bff469a":"# # dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)\n# dictionary_LDA.filter_extremes(no_below=3)\n# corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]","6db77630":"# dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)\n# dictionary_LDA.filter_extremes(no_below=3)\n# corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]\n\n# # Model by num_sub_products\n# num_topics = 10\n# lda_model = models.LdaMulticore(corpus, num_topics=num_topics, \\\n#                                   id2word=dictionary_LDA, \\\n#                                   passes=4, alpha=[0.01]*num_topics, \\\n#                                   eta=[0.01]*len(dictionary_LDA.keys()))\n# for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n#     print(str(i)+\": \"+ topic)\n#     print()\n# filename = 'lda_model_10_topics.sav'\n# pickle.dump(lda_model, open(filename, 'wb'))\n\n# coherence_model_lda = CoherenceModel(model=lda_model, texts=list_of_list_of_tokens, dictionary=dictionary_LDA, coherence='c_v')\n# coherence_lda = coherence_model_lda.get_coherence()\n# print('\\nCoherence Score: ', coherence_lda)","a665a630":"# # Get topic weights and dominant topics ------------\n# from sklearn.manifold import TSNE\n# from bokeh.plotting import figure, output_file, show\n# from bokeh.models import Label\n# from bokeh.io import output_notebook\n# import itertools\n# import matplotlib.colors as mcolors\n\n# # Get topic weights\n# topic_weights = []\n# for i, row_list in enumerate(lda_model[corpus]):\n#     topic_weights.append([w for i, w in row_list])\n\n# # Array of topic weights    \n# arr = pd.DataFrame(topic_weights).fillna(0).values\n\n# # Keep the well separated points (optional)\n# arr = arr[np.amax(arr, axis=1) > 0.35]\n\n# # Dominant topic number in each doc\n# topic_num = np.argmax(arr, axis=1)\n\n# # tSNE Dimension Reduction\n# tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n# tsne_lda = tsne_model.fit_transform(arr)\n\n# # Plot the Topic Clusters using Bokeh\n# output_notebook()\n# n_topics = 18\n# mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n\n# plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n#               plot_width=500, plot_height=500)\n# plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n# show(plot)","da4fd1e7":"# pickle_in = open('\/kaggle\/input\/lda-gensim-model-products\/lda_model_by_products.sav',\"rb\")\n# LDA_18_topics = pickle.load(pickle_in)\n# pickle_in.close()","c72d37e1":"# import pyLDAvis.gensim\n# pyLDAvis.enable_notebook()\n# vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n# vis","c0e00418":"# pickle_in = open('\/kaggle\/input\/lda-gensim-model-products\/lda_model_by_products.sav',\"rb\")\n# LDA_model = pickle.load(pickle_in)\n# pickle_in.close()","43d7e338":"# import pyLDAvis.gensim\n# pyLDAvis.enable_notebook()\n# vis = pyLDAvis.gensim.prepare(LDA_model, corpus, dictionary=LDA_model.id2word)\n# vis","f8e737e3":"# test_april = pd.read_csv('\/kaggle\/input\/testapril\/complaints-2020-05-07_09_29.csv')","431b979d":"# test_lolotokens = test_april['Consumer complaint narrative'].parallel_apply(word_tokenize)","8be0ec30":"# test_corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in test_lolotokens.to_list()]","9b7fe901":"# test_april.columns","96b1295e":"# # def LDA_predict(text):\n# #     tokens = word_tokenize(new_text)\n# for i,row_list in enumerate(LDA_model[dictionary.doc2bow[tokens]])\n# LDA_model[dictionary_LDA.doc2bow(tokens)]","6a6d3409":"# def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n#     # Init output\n#     sent_topics_df = pd.DataFrame()\n\n#     # Get main topic in each document\n#     for i, row_list in enumerate(LDA_model[test_corpus]):\n#         row = row_list[0] if ldamodel.per_word_topics else row_list            \n#         # print(row)\n#         row = sorted(row, key=lambda x: (x[1]), reverse=True)\n#         # Get the Dominant topic, Perc Contribution and Keywords for each document\n#         for j, (topic_num, prop_topic) in enumerate(row):\n#             if j == 0:  # => dominant topic\n#                 wp = ldamodel.show_topic(topic_num)\n#                 topic_keywords = \", \".join([word for word, prop in wp])\n#                 sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n#             else:\n#                 break\n#     sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n#     # Add original text to the end of the output\n#     contents = pd.Series(texts)\n#     sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n#     return(sent_topics_df)\n\n\n# df_topic_sents_keywords = format_topics_sentences(ldamodel=LDA_model, corpus=corpus, texts=test_lolotokens.to_list())\n\n# # Format\n# df_dominant_topic = df_topic_sents_keywords.reset_index()\n# df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n# df_dominant_topic.head(10)","dc213e66":"# df_dominant_topic.to_csv('LDA_prediction_on_april.csv')","5b1f4ed2":"# coherence_model_lda = CoherenceModel(model=_model, texts=list_of_list_of_tokens, dictionary=dictionary_LDA, coherence='c_v')\n# coherence_lda = coherence_model_lda.get_coherence()\n# print('\\nCoherence Score: ', coherence_lda)","39401c05":"# pickle_in = open('\/kaggle\/input\/lda-model-5-topics\/lda_model_5_topics.sav',\"rb\")\n# LDA_model_5_topics = pickle.load(pickle_in)\n# pickle_in.close()","cd1b9571":"# import pyLDAvis.gensim\n# pyLDAvis.enable_notebook()\n# vis = pyLDAvis.gensim.prepare(LDA_model_5_topics, corpus, dictionary=LDA_model_5_topics.id2word)\n# vis","51ecfe5b":"# def topics_per_document(model, corpus, start=0, end=1):\n#     corpus_sel = corpus[start:end]\n#     dominant_topics = []\n#     topic_percentages = []\n#     for i, corp in enumerate(corpus_sel):\n#         topic_percs, wordid_topics, wordid_phivalues = model[corp]\n#         dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n#         dominant_topics.append((i, dominant_topic))\n#         topic_percentages.append(topic_percs)\n#     return(dominant_topics, topic_percentages)\n\n# dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n\n# # Distribution of Dominant Topics in Each Document\n# df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n# dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n# df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n\n# # Total Topic Distribution by actual weight\n# topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n# df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n\n# # Top 3 Keywords for each Topic\n# topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n#                                  for j, (topic, wt) in enumerate(topics) if j < 3]\n\n# df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n# df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n# df_top3words.reset_index(level=0,inplace=True)","9aa9d105":"# from matplotlib.ticker import FuncFormatter\n\n# # Plot\n# fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n\n# # Topic Distribution by Dominant Topics\n# ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n# ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n# tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n# ax1.xaxis.set_major_formatter(tick_formatter)\n# ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n# ax1.set_ylabel('Number of Documents')\n# ax1.set_ylim(0, 1000)","ca376d6e":"# complaints with low contri get all contris from 18 topics, hypertune with 50k complaints, runtime  5 min, clean test and run prediction, evaluate, work on 15 months data\n# check if prod sub prod is same across 15 months","5eac9c04":"documents = pd.read_csv('\/kaggle\/input\/tagged-docs-model\/complaints-2020-06-13_12_31.csv')","e1ae7ff9":"import spacy\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\ndef clean_doc(x):\n    x = \" \".join(wordninja.split(x))\n    x = nlp(x)\n    # x = ' '.join([x.decode('utf-8')[:-3] for x in lemmatize(x,stopwords={'xx','xxxx'})])\n    x = \" \".join([token.lemma_.lower() for token in x if token.lemma_ not in ['XX','XXXX','xxxx','xx','s','00','-PRON-']])\n    # x = ' '.join(list(map(lambda x: x.decode('utf-8')[:-3], lemmatize(x,stopwords={'xx','xxxx'}))))\n    x = preprocess_string(x,[strip_tags,strip_punctuation,strip_multiple_whitespaces,remove_stopwords])\n    return x","f4753862":"%%time\n# clean_doc(documents['Consumer complaint narrative'][2])\n# wordninja.split(documents['Consumer complaint narrative'][0])","0a4f13de":"# %%time\n# train_docs = documents['Consumer complaint narrative'].parallel_apply(clean_doc)\n# train_docs = list(train_docs)\n# tagged_docs = [doc2vec.TaggedDocument(train_docs[x], [x]) for x in range(0,len(train_docs))]\n# tagged_docs","4579c668":"# pickle_out = open('train_docs.pkl','wb')\n# pickle.dump(tagged_docs,pickle_out)","a987322e":"pickle_in = open('\/kaggle\/input\/tagged-docs-model\/train_docs(1).pkl','rb')\ntagged_docs = pickle.load(pickle_in)","27f6ef63":"len(tagged_docs)","7f82a29f":"%%time\nmodel = doc2vec.Doc2Vec(vector_size=50, min_count=5, epochs=30, workers=4)\nmodel.build_vocab(tagged_docs)","26434bc1":"%%time\nmodel.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\nmodel.save('custom_Doc2vec_50d_116k.pkl')","833143a6":"model = doc2vec.Doc2Vec.load('\/kaggle\/input\/tagged-docs-model\/custom_Doc2vec_50d.pkl')\n# model = doc2vec.Doc2Vec.load('\/kaggle\/input\/tagged-docs-model\/custom_Doc2vec_200d.pkl')\n# model = doc2vec.Doc2VecKeyedVectors.load('\/kaggle\/input\/google-doc2vec\/GoogleNews-vectors-negative300.bin')","a9110868":"ranks = []\nsecond_ranks = []\nfor doc_id in range(len(tagged_docs)):\n    inferred_vector = model.infer_vector(tagged_docs[doc_id].words)\n    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n    rank = [docid for docid, sim in sims].index(doc_id)\n    ranks.append(rank)\n\n    second_ranks.append(sims[1])","ccf9fedf":"import collections\n\ncounter = collections.Counter(ranks)\nprint(counter)","32db4c5b":"x = model.infer_vector(tagged_docs[6].words)\nmodel.docvecs.most_similar([x], topn=5)","dbf02a4b":"documents['Consumer complaint narrative'][5],documents['Consumer complaint narrative'][91852]","9a3e4a79":"%%time\nfrom scipy import spatial\n\nvec1 = model.infer_vector(tagged_docs[900].words)\nvec2 = model.infer_vector(tagged_docs[31720].words)\nsimilarity = spatial.distance.cosine(vec1, vec2)\n1-similarity\n\n\n","80fe7f36":"tagged_docs[0].words","3be9a3d9":"# x = model.infer_vector(tagged_docs[0].words)\n# model.docvecs.most_similar([x], topn=5)\n# tagged_docs[0].tags\n# pd.pivot_table(data=documents,index=['Product','Sub-product',\"Issue\"],values=['clean_narrative'],aggfunc=lambda x: len(x.unique())).to_csv('class_distribution.csv')\npd.pivot_table(data=documents,index=['Sub-product',\"Issue\",'Sub-issue'],values=['Consumer complaint narrative'],aggfunc=lambda x: len(x.unique())).sort_values(by='Consumer complaint narrative').to_csv('class_distribution.csv')","c61c94ec":"# from sklearn.cluster import KMeans\n# from sklearn.decomposition import PCA\n# kmeans_model = KMeans(n_clusters=5, init='k-means++', max_iter=100) \n# X = kmeans_model.fit(model.docvecs.doctag_syn0)\n# labels=kmeans_model.labels_.tolist()\n\n# l = kmeans_model.fit_predict(model.docvecs.doctag_syn0)\n# pca = PCA(n_components=4).fit(model.docvecs.doctag_syn0)\n# datapoint = pca.transform(model.docvecs.doctag_syn0)\n\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\n# plt.figure\n# label1 = [\"#FFFF00\", \"#008000\", \"#0000FF\", \"#800080\", \"#FF00FF\",\"#98FB98\",'#006400' , '#008080']\n# # label1 = [\"#FFFF00\", \"#008000\", \"#0000FF\", \"#800080\", \"#FF00FF\"]\n# color = [label1[i] for i in labels]\n# plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n\n# centroids = kmeans_model.cluster_centers_\n# centroidpoint = pca.transform(centroids)\n# plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n# plt.show()","d5385c5d":"# PCA_components = pd.DataFrame(datapoint)","371d2545":"# ## elbow is between 4 to 8\n# ks = range(1, 20)\n# inertias = []\n# for k in ks:\n#     # Create a KMeans instance with k clusters: model\n#     elbow_model = KMeans(n_clusters=k)\n    \n#     # Fit model to samples\n#     elbow_model.fit(PCA_components.iloc[:,:4])\n    \n#     # Append the inertia to the list of inertias\n#     inertias.append(elbow_model.inertia_)\n    \n# plt.plot(ks, inertias, '-o', color='black')\n# plt.xlabel('number of clusters, k')\n# plt.ylabel('inertia')\n# plt.xticks(ks)\n# plt.show()\n","05170b24":"# PCA_components['labels'] = labels","057c7149":"# PCA_components.to_csv('doc2vec_results.csv')","186100f0":"# PCA_components.head()","41921c76":"# import plotly.graph_objs as go\n# from plotly.offline import iplot, init_notebook_mode\n\n# trace1 = go.Scatter3d(\n#     x=PCA_components[0],\n#     y=PCA_components[1],\n#     z=PCA_components[2],\n#     mode='markers',\n#     marker=dict(\n#         size=10,\n#         color=PCA_components['labels'],                # set color to an array\/list of desired values      \n#     )\n# )\n\n# data = [trace1]\n# layout = go.Layout(\n#     margin=dict(\n#         l=0,\n#         r=0,\n#         b=0,\n#         t=0  \n#     )\n    \n# )\n# fig = go.Figure(data=data, layout=layout)\n# iplot(fig)\n\n","ad5fa0c8":"# PCA_components['complaints'] = documents['Consumer complaint narrative']","63ec65de":"# PCA_components.to_csv('Doc2Vec_similarity.csv')","e9875f6c":"# documents.columns\n\n# documents['complaint_issues'] = documents['Sub-product']+\" \"+documents['Issue']+' '+documents['Sub-issue']\n# documents['complaint_issues'] = documents['Issue']+' '+documents['Sub-issue']\ndocuments['complaint_issues'] = documents['Issue']","ace62257":"import ast\ndocuments['issue_tokens'] = documents[['Issue']].values.tolist()\n# documents['issue_tokens'] = documents[['Issue','Sub-issue']].values.tolist()\nissue_tokens = [ast.literal_eval(x) for x in documents['issue_tokens'].astype(str).unique()]","59cc1c38":"full_issue = documents['complaint_issues'].unique()\nall_issues = [clean_doc(x) for x in documents['complaint_issues'].unique()]","3ea195af":"# issue_tokens\nall_issues","f8391de8":"len(all_issues)","9a8d0c6f":"# def normalize(word_vec):\n#     norm=np.linalg.norm(word_vec)\n#     if norm == 0: \n#         return word_vec\n#     else:\n#         return word_vec\/norm\n\n# list_of_issue_vectors = [normalize(x) for x in list_of_issue_vectors]","195e28bb":"list_of_issue_vectors = [model.infer_vector(x) for x in all_issues]","39c51b68":"# # test_vector = normalize(model.infer_vector(tagged_docs[0].words))\n# test_vector = model.infer_vector(tagged_docs[0].words)\n# ([metrics.pairwise.cosine_similarity([test_vector],[i]) for i in list_of_issue_vectors])","530e84d2":"%%time\nfrom scipy import spatial\ndef issue_predict(x):\n    test_vector = model.infer_vector(x)\n    score = [1 - spatial.distance.cosine(test_vector, i) for i in list_of_issue_vectors]\n    return issue_tokens[score.index(max(score))], score[score.index(max(score))]","fd044969":"%%time\nfrom sklearn import metrics\ndef issue_pred2(x):\n    test_vector = model.infer_vector(x)\n    score = [metrics.pairwise.cosine_similarity([test_vector], [i]) for i in list_of_issue_vectors]\n    # return issue_tokens[score.index(max(score))], score[score.index(max(score))]\n    return issue_tokens[score.index(max(score))]","864e3569":"x=5\nissue_predict(tagged_docs[x].words),issue_pred2(tagged_docs[x].words),documents['Consumer complaint narrative'][x]","ab586680":"issue_predict(tagged_docs[2].words),' '.join(tagged_docs[2].words)","125f09f6":"issue_predict(tagged_docs[3].words),' '.join(tagged_docs[3].words)","dbaf5f6f":"issue_predict(tagged_docs[4].words),' '.join(tagged_docs[4].words)","2be0e1bc":"issue_predict(tagged_docs[6].words),documents['Consumer complaint narrative'][6]","d8e21db1":"%%time\nissue_predict(tagged_docs[7].words),documents['Consumer complaint narrative'][7]","b69a0724":"%%time\nissue_predict(tagged_docs[8].words),documents['Consumer complaint narrative'][8]","4f1527ae":"%%time\nissue_predict(tagged_docs[9].words),documents['Consumer complaint narrative'][9]","be520ebd":"%%time\nissue_predict(tagged_docs[10].words),documents['Consumer complaint narrative'][10]","c04b9cc2":"%%time\nissue_predict(tagged_docs[11].words),documents['Consumer complaint narrative'][11]","6e2ff2c4":"%%time\nissue_predict(tagged_docs[12].words),documents['Consumer complaint narrative'][12]","5dbb4454":"%%time\nissue_predict(tagged_docs[13].words),documents['Consumer complaint narrative'][13]","0d01ab1e":"# from sklearn import metrics\nmetrics.pairwise.cosine_similarity([test_vector],[list_of_issue_vectors[3]])\n# list_of_issue_vectors\n\n# cos_sim = dot(a, b)\/(norm(a)*norm(b))","a9dd1686":"\nx = \"i have late payment charges on my credit report\"\nissue_predict(word_tokenize(x)),x","c208968a":"x = \"i need better answer about why I have unknown accounts on my credit report\"\nissue_predict(word_tokenize(x)),x\n\n","e4335c7b":"x = \"i'm a victim of fraud\"\nissue_predict(word_tokenize(x)),x","f95d32f6":"x = \"because of unknown accounts on my credit report my credit score dropped by 200 points\"\nissue_predict(word_tokenize(x)),x","55328225":"x = \"my credit score dropped by 200 points\"\nissue_predict(word_tokenize(x)),x","1633cb3d":"x = \"one customer representative harassed me over call. I'm ging to file a lawsuit\"\nissue_predict(word_tokenize(x)),x","e5f94890":"x = \"I'm happy with the bank but the customer service is really bad\"\nissue_predict(word_tokenize(x)),x","d119c29d":"x = \"You closed my paytm account, I need my money back\"\nissue_predict(word_tokenize(x)),x","606add74":"x = \"credit report disputes\"\nissue_predict(word_tokenize(x)),x","3a02474c":"x = \"will you be happy to see late payment charges in your acount?\"\nissue_predict(word_tokenize(x)),x","900b0ac9":"x = \"I used to like amazon prime but I need refund for my netflix account. I am facing hardships because of your service. Can I get a credit card\"\nissue_predict(word_tokenize(x)),x","2ee7fd7a":"x = \"My credit card was stolen. I closed it but now I can't pay all my bills what to do\"\nissue_predict(word_tokenize(x)),x","6cfc7972":"x = \"the day is really beautiful today but I am feeling it is going to rain for sure. My house can get damaged, I'm worried if my bank will allow deferment\"\nissue_predict(word_tokenize(x)),x","47ca8828":"x = \"hey my name is John Doe today I went to a mall some one pick pocketed my wallet all my cards and cash were lost. Can you close them all\"\nissue_predict(word_tokenize(x)),x","b37d20f9":"x = \"I think someone else is using my details like ssn to create fake accounts. On amazon I have lost dollar 5000\"\nissue_predict(word_tokenize(x)),x","090d1f54":"x='my husband died. I want to transfer his money to my account please help'\nissue_predict(word_tokenize(x)),x","ab0cad83":"x=\"I never openend a Digi Bank wallet. I'm getting messages for collections. Someone stole my information please report this account\"\nissue_predict(word_tokenize(x)),x","a7151793":"x='i need medical assistance please send ambulance asap'\nissue_predict(word_tokenize(x)),x","2afa4f5d":"x=\"there's a pothole on road i'm really frustrated i've been calling the sewer company since last 5 months but no one responds on time\"\nissue_predict(word_tokenize(x)),x","106160a2":"x=\"i bought an item online but my bank denied payment\"\nissue_predict(word_tokenize(x)),x","4be62db7":"x=\"I'm not going to recommend PNC bank to my friends. I raised a request to stop debt collections calls but still I'm getting way too many\"\nissue_predict(word_tokenize(x)),x","bb8db7e3":"x=\"can i stop paying emi for damaged car\"\nissue_predict(word_tokenize(x)),x","c7681474":"x=\"my cheques are bouncing even when my account has huge amount of money in it\"\nissue_predict(word_tokenize(x)),x","f3b1684b":"x = \" I am using citi credit card, I'm facing problem with a purchase shown on my statement, bank isn't resolving a dispute about a purchase on my statement\"\nissue_predict(word_tokenize(x)),x","dfbca05d":"x = \"due to covid-19, I'm facing hardship not sure if I can repay my loan\"\nissue_predict(word_tokenize(x)),x","6e6baf20":"test_data = pd.read_csv('\/kaggle\/input\/testapril\/complaints-2020-05-07_09_29.csv')","73303c2a":"test_data['Company response to consumer'].value_counts()","38f79ca4":"test_data.head(25)","d055129d":"test_result = test_data['Consumer complaint narrative'].parallel_apply(word_tokenize).parallel_apply(issue_pred2)\n","0ad920fd":"vocab = [item for sublist in tagged_docs for item in sublist]\nvocab = [item for sublist in vocab for item in sublist]\nlen(vocab)","c8eda68f":"test_actual = pd.Series(test_data[['Sub-product','Issue','Sub-issue']].values.tolist(), name='narrative')","5ecc008b":"final_result = test_result.to_frame().join(test_actual)\nfinal_result","bf4694b3":"final_result[final_result['narrative']==final_result['Consumer complaint narrative']].shape","a8ad2788":"# test_result = test_data['Consumer complaint narrative'].parallel_apply(word_tokenize).parallel_apply(issue_pred2)\n# test_actual = pd.Series(test_data[['Issue','Sub-issue']].values.tolist(), name='narrative')\n# final_result = test_result.to_frame().join(test_actual)\n# final_result","c6312e37":"final_result[final_result['narrative']==final_result['Consumer complaint narrative']].shape","7e915f19":"# test_result = test_data['Consumer complaint narrative'].parallel_apply(word_tokenize).parallel_apply(issue_pred2)\n# test_actual = pd.Series(test_data[['Issue','Sub-issue']].values.tolist(), name='narrative')\n# final_result = test_result.to_frame().join(test_actual)\n# final_result","8731b3df":"test_result = test_data['Consumer complaint narrative'].parallel_apply(word_tokenize).parallel_apply(issue_pred2)\ntest_actual = pd.Series(test_data[['Issue']].values.tolist(), name='narrative')\nfinal_result = test_result.to_frame().join(test_actual)\nfinal_result[final_result['narrative']==final_result['Consumer complaint narrative']].shape","91d549e4":"# Load LDA model with 18 topics","a47a563c":"# Step 3: prepare your list of list of token, dictionary LDA, corpus","45ca7d84":"## okayish example 1 incomplete match","b5ddcabd":"## bad example 6","309d584e":"## bad example 2","114cd30f":"# step 1: Load the list of list of token from csv ","9019fbb3":"## choosing embedding size\nhttps:\/\/stackoverflow.com\/questions\/48479915\/what-is-the-preferred-ratio-between-the-vocabulary-size-and-embedding-dimension\nhttps:\/\/stackoverflow.com\/questions\/50945820\/paragraph-vector-or-doc2vec-model-size\nhttps:\/\/stackoverflow.com\/questions\/51245689\/what-does-epochs-mean-in-doc2vec-and-train-when-i-have-to-manually-run-the-itera\n\n#### Higher dimension can lead to overfitting on smaller corpuses\n#### best dimension size is vocabulary_size**0.25","a66c1c3e":"# test on april\n# precision recall\/ auc score\n# confusion matrix","110250c8":"## partial match example 3","1698fbaf":"## bad example 9, partial match 7","4e560db0":"## partial match example 5","b55f9347":"## bad example 8, partial match 6","3ec73031":"## good example 3","337f6cf2":"## bad example 11","eec6540b":"## stress testing","690fca3b":"## good example 4","0b9a9ef4":"## bad example 10","59805a0b":"## loading doc2vec docs and model ","d54df07e":"## good example 1","ec52c3b4":"# good, partial match example 2","13a47c92":"# Step 4 : run your lda multi core model, check coherence","a6942fab":"## bad example 12","723bab2b":"## testing on april data","21aa5a9c":"## bad example 1 ","a2a9fb24":"## bad example 5","57e393fe":"## partial match 8","d622a3d0":"# good example 7, partial match 9","13b8f063":"## partial match example 4","846dfb06":"## bad example 3","d9e1ed41":"## good example 5","8a1b6d79":"## can't comment example 1","7c95a644":"# Step 2 : load your dictionary LDA","cb118a2b":"## bad example 4","15d386fb":"# Load LDA model with 5 topics","b6fe58a6":"## good match 6 taken from corpus","af7528e9":"## bad example 7"}}