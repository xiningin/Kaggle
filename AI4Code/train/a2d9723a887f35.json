{"cell_type":{"442fe8c7":"code","d7339817":"code","204970b3":"code","c8150d17":"code","33f47ad0":"code","462c430a":"code","f64ffadc":"code","f35d97fb":"code","f2a886b4":"markdown","bff9be9c":"markdown","b2f359f5":"markdown"},"source":{"442fe8c7":"# Just the needed imports for the task at hand\n# tpqdm_notebook will allow us to see the progress of image loading, which is the slowest one\n\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.image import load_img\nfrom tqdm import tqdm_notebook","d7339817":"# As in your computer data might be in a different place than in the Kaggle platform, defining this variable and appending it\n# to all paths is a convenient way to use (almost) the same notebooks in Kaggle and your platform\nDATA_DIR = '..\/input\/'\n\n# Load the train and depths csvs and join \ntrain_df = pd.read_csv(DATA_DIR + 'train.csv', index_col='id')\ndepths_df = pd.read_csv(DATA_DIR + 'depths.csv', index_col='id')\ntrain_df = train_df.join(depths_df)\n\n# A test.csv doesn't exist as such, so we use the submission information and join it with the depths information\nsubmission_df = pd.read_csv(DATA_DIR + 'sample_submission.csv', index_col=\"id\")\ntest_df = submission_df.copy()\ntest_df = test_df.join(depths_df)","204970b3":"print(\"**** Train ****\")\nprint(train_df.head())\nprint()\nprint('**** Depths ****')\nprint(depths_df.head())\nprint()\nprint(\"**** Test ****\")\nprint(test_df.head())\nprint()\nprint(\"**** Sample submission ****\")\nprint(submission_df.head())\nprint()","c8150d17":"train_df[\"images\"] = [np.array(load_img(DATA_DIR + \"train\/images\/{}.png\".format(idx), grayscale=True), dtype=np.int16) for idx in tqdm_notebook(train_df.index)]\ntrain_df[\"masks\"] = [np.array(load_img(DATA_DIR + \"train\/masks\/{}.png\".format(idx), grayscale=True), dtype=np.int16) for idx in tqdm_notebook(train_df.index)]\ntest_df['images'] = [np.array(load_img(DATA_DIR + \"test\/images\/{}.png\".format(idx), grayscale=True), dtype=np.int16) for idx in tqdm_notebook(test_df.index)]","33f47ad0":"test_df.head()","462c430a":"# Adjust the type of the z (depth) feature to make the objects smaller\ntrain_df['z'] = train_df['z'].astype(np.uint16)\ntest_df['z'] = test_df['z'].astype(np.uint16)\n\ntrain_df.info()\ntest_df.info()","f64ffadc":"# Add a boolean feature to indicate if the image has or not salt at all\ntrain_df['has_salt'] = train_df['masks'].apply(lambda x: x.sum() > 0)\ntrain_df.head()","f35d97fb":"%%time\n# Now let's save all the data in the same HF5 file\ntrain_df.to_hdf('tgs_salt.h5', key='train', mode='w')\ntest_df.to_hdf('tgs_salt.h5', key='test', mode='a')\nsubmission_df.to_hdf('tgs_salt.h5', key='submission', mode='a')","f2a886b4":"<h2>Load images and masks<\/h2>","bff9be9c":"<h2>Save the data<\/h2>","b2f359f5":"**<h1>TGS - Reading data and storing in HF5<\/h1>\n\nHaving to read from the file systems all the images and masks of this data set every time you run a notebook is very inefficient. For this reason I've created an notebook that does all the loading and some preprocessing of this information into a Pandas data frame and stores it in the more convenient format HF5, much more efficient (it just takes a few seconds to load all the data in this format).\n\nI'm sure all experienced kagglers have come up with some form of efficiently loading competition data, and I'd be glad to have some comments of better ways to do this than the one that I'm proposing, but I hope this suggestions might be useful for more inexperienced kaggles."}}