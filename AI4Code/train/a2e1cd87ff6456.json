{"cell_type":{"2a22aaec":"code","870652b1":"code","3281a215":"code","f6969417":"code","dae94b18":"code","454f5af8":"code","2dcf3ce3":"code","db23aab5":"code","704d83b6":"code","8074816c":"code","46110c02":"code","a72f412d":"code","8149e89f":"code","5c9c820e":"code","220deb49":"code","a0787c62":"code","9c20ae4e":"code","172bc651":"code","57a4e68e":"code","31c23c0c":"markdown","8ed1dfe1":"markdown","770b7db4":"markdown","bb5f4edc":"markdown","a77dec47":"markdown","4a60cfb0":"markdown","dcff827a":"markdown","7e9017fb":"markdown"},"source":{"2a22aaec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.metrics import mean_squared_error\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Lasso, LinearRegression, Ridge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","870652b1":"df = pd.read_csv(str(dirname)+'\/'+str(filename))","3281a215":"df.head(2)","f6969417":"df.isna().sum()","dae94b18":"df.drop_duplicates().reset_index(drop=True, inplace=True)","454f5af8":"def weird_division(n, d):\n    return n \/ d if d else 0","2dcf3ce3":"foo = lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 ","db23aab5":"df['CPM'] = df.apply(foo, axis=1)","704d83b6":"df = df.loc[df['CPM']>=0]","8074816c":"df.drop(['total_revenue','revenue_share_percent', 'measurable_impressions'], axis=1, inplace=True)","46110c02":"df.head(2)","a72f412d":"df['CPM'].describe()","8149e89f":"df = df[df['CPM'] < df['CPM'].quantile(.95)] ","5c9c820e":"split_data = '2019-06-22'\n\ntest = df[df.date >= split_data]\ntrain = df[df.date < split_data]","220deb49":"X = train.drop(['date','CPM'], axis=1)\ny = train['CPM']","a0787c62":"TEST_SIZE = 0.3\nRAND_STATE = 42\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SIZE, random_state=RAND_STATE)","9c20ae4e":"models = [Ridge, Lasso, RandomForestRegressor, GradientBoostingRegressor]\nmodel_names = ['Ridge', 'Lasso', 'RandomForestRegressor', 'GradientBoostingRegressor']\n\ndef estimate_model(model_cls):\n    model = model_cls(random_state=RAND_STATE)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    score = mean_squared_error(y_test, y_pred)\n    return score\n\nregression_scores = []\nfor model_cls in models:\n    score = estimate_model(model_cls)\n    regression_scores.append(score)\n\nindex = np.array(regression_scores).argmin()\nanswer = regression_scores[index]\nprint(\"best score: %s\" % model_names[index])","172bc651":"rfg = RandomForestRegressor()\nrfg.fit(X_train, y_train)\n\ny_pred_rfg = rfg.predict(X_test)","57a4e68e":"print(\"MSE: \", (mean_squared_error(y_pred_rfg, y_test)))","31c23c0c":"### 3.1 Default models","8ed1dfe1":"## 1. Load data","770b7db4":"## 2.Train test split","bb5f4edc":"### 3.2 Random Forest","a77dec47":"make CPM","4a60cfb0":"Drop duplicates","dcff827a":"take CPM >=0 and drop total_revenue, measurable_impressions, revenue_share_percent","7e9017fb":"## 3. Model"}}