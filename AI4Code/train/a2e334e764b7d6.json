{"cell_type":{"46b6f4b0":"code","ad1cb13c":"code","d3a034c8":"code","ecdc56a7":"code","ad5dce66":"code","a53634d6":"code","e03fe3d4":"code","c916a793":"code","9fab5c34":"code","134d4685":"code","a7b24dd3":"code","1e3c455b":"code","bbe50496":"code","2511d5c7":"code","72760cbf":"code","9bd2493e":"code","59a40651":"code","de144259":"code","795c9562":"code","cd16764c":"code","48c8101f":"code","d6145f71":"code","4adc2687":"code","5d024b01":"markdown","0059a258":"markdown","f1c5d72b":"markdown","c4654f77":"markdown","253e5eff":"markdown","16548e23":"markdown","f982e2a9":"markdown","c34c8c15":"markdown","39e64192":"markdown","332e5a31":"markdown","d6f7e0ef":"markdown","142fd2db":"markdown","24556a61":"markdown","8c1a8196":"markdown","a9d74408":"markdown","03262605":"markdown","9fa4ac13":"markdown","100283f5":"markdown","feeaf21f":"markdown","49000b8f":"markdown","c549e046":"markdown","24196bd8":"markdown","4b54347d":"markdown","6f49f236":"markdown","a1d8ddf1":"markdown","197d443e":"markdown","19928e8d":"markdown"},"source":{"46b6f4b0":"!pip install pyspark","ad1cb13c":"from pyspark.sql import SparkSession\nspark_ex = SparkSession.builder.getOrCreate()\nprint(spark_ex)","d3a034c8":"# Don't change this file path\nfile_path = \"..\/input\/titanic\/train.csv\"\n\n# Read in the titanic data\ntitanic = spark_ex.read.csv(file_path,header=True)\n\n# Show the data\ntitanic.show()","ecdc56a7":"titanic=titanic.drop(\"Name\", \"Ticket\",\"Cabin\")\ntitanic.show()","ad5dce66":"titanic=titanic.filter(titanic.Pclass.isNotNull())\ntitanic=titanic.filter(titanic.Sex.isNotNull())\ntitanic=titanic.filter(titanic.Age.isNotNull())\ntitanic=titanic.filter(titanic.SibSp.isNotNull())\ntitanic=titanic.filter(titanic.Fare.isNotNull())\ntitanic=titanic.filter(titanic.Parch.isNotNull())\ntitanic=titanic.filter(titanic.Embarked.isNotNull())\ntitanic.show()","a53634d6":"#total number of final records with us\ntitanic.count()","e03fe3d4":"titanic.printSchema()","c916a793":"#casting columns to numeric\ntitanic = titanic.withColumn(\"PassengerId\", titanic.PassengerId.cast(\"integer\"))\ntitanic = titanic.withColumn(\"label\", titanic.Survived.cast(\"integer\"))\ntitanic = titanic.withColumn(\"Age\", titanic.Age.cast(\"integer\"))\ntitanic = titanic.withColumn(\"SibSp\", titanic.SibSp.cast(\"integer\"))\ntitanic = titanic.withColumn(\"Parch\", titanic.Parch.cast(\"integer\"))\ntitanic = titanic.withColumn(\"Fare\", titanic.Fare.cast(\"integer\"))","9fab5c34":"titanic.printSchema()","134d4685":"from pyspark.ml.feature import StringIndexer,OneHotEncoder\n\n# Encoding Categorical Features\nPclass_indexer = StringIndexer(inputCol=\"Pclass\",outputCol=\"Pclass_index\")\n\n# Create a OneHotEncoder\nPclass_encoder = OneHotEncoder(inputCol=\"Pclass_index\",outputCol=\"Pclass_fact\")","a7b24dd3":"# Encoding Categorical Features\nSex_indexer = StringIndexer(inputCol=\"Sex\",outputCol=\"Sex_index\")\n\n# Create a OneHotEncoder\nSex_encoder = OneHotEncoder(inputCol=\"Sex_index\",outputCol=\"Sex_fact\")","1e3c455b":"# Encoding Categorical Features\nEmbarked_indexer = StringIndexer(inputCol=\"Embarked\",outputCol=\"Embarked_index\")\n\n# Create a OneHotEncoder\nEmbarked_encoder = OneHotEncoder(inputCol=\"Embarked_index\",outputCol=\"Embarked_fact\")","bbe50496":"titanic.printSchema()","2511d5c7":"from pyspark.ml.feature import VectorAssembler\nvec_assembler = VectorAssembler(inputCols=[\"Pclass_fact\", \"Sex_fact\", \"Age\", \"SibSp\", \"Parch\",\"Fare\",\"Embarked_fact\"], outputCol=\"features\")","72760cbf":"# Import Pipeline\nfrom pyspark.ml import Pipeline\n\n# Make the pipeline\ntitanic_pipe = Pipeline(stages=[Pclass_indexer, Pclass_encoder, Sex_indexer, Sex_encoder,Embarked_indexer, Embarked_encoder, vec_assembler])","9bd2493e":"# Fit and transform the data\npiped_data = titanic_pipe.fit(titanic).transform(titanic)","59a40651":"from pyspark.sql.functions import *\n# Split the data into training and test sets\ntraining, test = piped_data.randomSplit([.7,.3])","de144259":"# Import LogisticRegression\nfrom pyspark.ml.classification import LogisticRegression\n\n# Create a LogisticRegression Estimator\nlr = LogisticRegression()","795c9562":"# Import the evaluation submodule\nimport pyspark.ml.evaluation as evals\n\n# Create a BinaryClassificationEvaluator\nevaluator = evals.BinaryClassificationEvaluator(metricName=\"areaUnderROC\")","cd16764c":"# Import the tuning submodule\nimport numpy as np\nimport pyspark.ml.tuning as tune\n\n# Create the parameter grid\ngrid = tune.ParamGridBuilder()\n\n# Add the hyperparameter\ngrid = grid.addGrid(lr.regParam, np.arange(0, .1, .01))\ngrid = grid.addGrid(lr.elasticNetParam, [0,1])\n\n# Build the grid\ngrid = grid.build()","48c8101f":"# Create the CrossValidator\ncv = tune.CrossValidator(estimator=lr,\n               estimatorParamMaps=grid,\n               evaluator=evaluator\n               )","d6145f71":"# Fit cross validation models\nmodels = cv.fit(training)\n\n# Extract the best model\nbest_lr = models.bestModel\nprint(best_lr)","4adc2687":"# Use the model to predict the test set\ntest_results = best_lr.transform(test)\n\n# Evaluate the predictions\nprint(evaluator.evaluate(test_results))","5d024b01":"Now that we have our best model from all the model combinations, we'll use it to evaluate our model on test data taht we have kept aside","0059a258":"<a id=\"8\"><\/a>\n# 8. Applying Model and Cross Validation\n\nHere we will use simple Logistic to predict our survival label an validate it using cross validatio technique which works by splitting the training data into a few different partitions and testing the trained model on unseen partition of data. It calculates the scores on the basis of scoring parameter that we pass, here we will estimate our score using \"the area under the ROC\". We will further use this to to compare all the different models so you can choose the best one. We do this by choosing the hyperparameters by creating a grid of the possible pairs of values for the two hyperparameters, elasticNetParam and regParam, and using the cross validation error to compare models. ","f1c5d72b":"### Caution:\nSince there are lot of null values in the datasets, generally we should not drop these and fill them with some kind of fillers such as median, mode etc. But our main motive here is to learn how to implement a ML model via spark so lets just drop them for simplicity","c4654f77":"Finally we have the correct schema and first part of data prepration is done.","253e5eff":"<a id=\"6\"><\/a>\n# 6. Creating ML Pipeline\nLets finally create a Pipeline for our ML job, this is similar to scikit learn pipelines so you wont see anything fancy here, it just combines all the Estimators and Transformers that we created above. (label encoders and one hot encoders","16548e23":"# KEY NOTE\n\nAfter learning pyspark and cluster computing for a week, its finally time to do what we are actually here for! Machine Learning!\n\nIn this notebook, I have implemented a very basic model to predict whether the passenger has survied or not on basis of the few features, Few points to note before starting:\n\n* This is very basic implementation so we are not using complex features or doing feature engineering\n* For simplicity we will be dropping the records with null values\n* Our aim is to learn how to implement a machine learning pipeline on Big data so donot bother which model I hvae used, its just for the sake of simplicity and not a competitive approach\n\nThese are just my personal notes. I am sharing these so that it helps others too, who are trying to learn the similar concepts. Feel free to comment if you have any doubts or correct me if I am wrong anywhere in the code. Any Feedback is appreciated.\n\nIf you havent checked the pyspark notebook series, please feel free to do so:\n* [1. Scalable Data Science: Pyspark DataFrames NB1](https:\/\/www.kaggle.com\/amritvirsinghx\/scalable-data-science-pyspark-dataframes-nb1)        \n* [2. Scalable Data Science: Pyspark DataFrames NB2](https:\/\/www.kaggle.com\/amritvirsinghx\/scalable-data-science-pyspark-dataframes-nb2)    \n* [3. Scalable Data Science: PySpark Core RDD](https:\/\/www.kaggle.com\/amritvirsinghx\/scalable-data-science-pyspark-core-rdd)\n\nThis will be helpful if you are new to Spark","f982e2a9":"<a id=\"4\"><\/a>\n# 4. Preparing Categorical Features\n\nAll ML libraries only work with numeric data, so we need to encode our string features into numeric datatypes. Boolean datatypes can be easily converting into numerics but for othe rstring values we need to encode that using some kind of label encoding algorithm.\n\nThese algorithms are part of pyspark.ml features module, we first need to convert strings into corresponding encoding then we can even one hot encode them if we want.\n\n* To encode categorical we have **StringIndexer**\n* For one hot encoding we have **OneHotEncoder**\n\nThese are part of Estimator class which when applied, perform a mapping operation and return a transformer which can be applied to a DataFrame\n\nLets se how this all works for us!","c34c8c15":"<a id=\"9\"><\/a>\n# 9. Evaluating the Model\n\nLets now evaluate our model on test data that we kept aside","39e64192":"<a id=\"3\"><\/a>\n# 3. Handling Data Types\n\nLike all other ML libraries,Spark ML only handles numeric data.(intergers, or decimals)\n\nThe schema interpretted automatically by spark is not always correct as we saw above so its good idea to manually convert that schema according to their data types, to fix this we use .cast() method. The way we use it, is in combination of .withColumn() method\n\nNote that .withColumn() works on dataframe level while .cast() works on columns\n\nsynntax for .cast() method is:\n\n.cast(\"*data type*\")\n\nLets try it on our dataset now!","332e5a31":"Performing the count action of terabytes of data is probably not the good idea as I previously discussed in Pyspark notebook series,but we are safe as we only have a few records with us.\n\nLets check the schema of our dataframe now","d6f7e0ef":"<a id=\"5\"><\/a>\n# 5. Data Preparation\n\nSpaark ML library takes all the feaatures in form of single column, we can convert our features into this form of single column vector bu using VectorAssembler from pyspark.ml.feature submodule, it takes all of the columns you specify and combines them into a new vector column. Be cautious about what features you feed in, donot feed index rows! \n\nLets prepare our data for modelling","142fd2db":"<a id=\"1\"><\/a>\n# 1. Setting up the Spark Environment\n\nLets setup our spark environment in our docker to begin using our local cluster for computing. if you have gone the previous notebooks you probably have a good idea on how to do so.","24556a61":"Lets Begin with Reaidng our Data, its as simple as reading the data in our pandas DataFrame","8c1a8196":"We have a kind of problem in our schema as some of the variables which should be numeric are interpretted as string. Lets see what we can do about that!","a9d74408":"Lets use this pipeline to transform our data","03262605":"We are good to proceed now!","9fa4ac13":"### Note\nBy default the value that needs to be predicted should be renamed as \"label\" as done above for column \"Survived\".","100283f5":"Woah! AUC score of 0.88, not bad!","feeaf21f":"Lets check what our best model comes our to be","49000b8f":"<a id=\"2\"><\/a>\n# 2. Machine Learning with Spark\n\nFor mahcine learning applications, pyspark has  a dedicated libarary: pyspark.ml\n\nThe main modules of pyspark.ml are\n* Transformer\n* Estimator classes\n\nand rest eveery other module in pyspark behaves in a similar fashion\n\n* Transformer classes have a .transform() method \n\nIt takes a DataFrame and returns a new DataFrame with a new column.\n\n* Estimator classes have a .fit() method. \n\nThis as you would expect is similar to scikit learn and take a DataFrame to return a model object. This can be something like a StringIndexerModel for including categorical data saved as strings in your models, or a RandomForestModel that uses the random forest algorithm for classification or regression.\n\n","c549e046":"<a id=\"10\"><\/a>\n# 10. Epilogue\n\nCOOL! Finally we have built our ML pipeline on Spark. This was pretty simplified model on a simplified dataset, just to have an overview on how we do it on Spark Clusters. The technique is quite similar to how we do it using scikit learn and all the underlying techniques that we used remained the same. I have a learned a lot by experimenting on Spark and Bigdata techniques. I hope its the same for you.\n\n## Stay Tuned! Danke!","24196bd8":"### Caution:\n\nAs you can see Name column is a bit complex and we require use of some regex to extract some important information like Title, for the simplicity of this notebook, we ll just drop it. Similarly with the case of Ticket column\n\nAlso the Cabin column is mosly null so we ll drop that too.","4b54347d":"Create a score evaluator as follows","6f49f236":"<a id=\"7\"><\/a>\n# 7. Test Train Split\n\nLike we split our test train data in all ml jobs, we will split our data here in a similar fashion. Remember  the tools you use maybe different but the underlying job remains the same. \nWe need to separate the test data which we wont touch until our model is trained. for this we use function called randomSplit on our transformed data.","a1d8ddf1":"Now lets create a CV validator using CrossValidator from pyspark.ml.tuning and passing it the logistic regression Estimator lr, the parameter grid, and the evaluator we created previously","197d443e":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Navigation<\/h3>\n\n[1. Setting Up the Spark Environment](#1)         \n[2. Machine Learning with Spark](#2)     \n[3. Handling Data Types](#3)     \n[4. Preparing Categorical Features](#4)     \n[5. Data Preparation](#5)     \n[6. Creating ML Pipeline](#6)         \n[7. Test Train Split](#7)              \n[8. Applying Model and Cross Validation](#8)          \n[9. Evaluating the Model](#9)      \n[10. Epilogue](#10)      ","19928e8d":"Binary classifier can make (false positives and false negatives) into a simple number. \n\nFurther lets create a grid for hyper parameter tuning using ParamGridBuilder from pyspark.ml.tuning.\n\nWe will use .addGrid() and .build() methods to create a grid that you can use for cross validation. \n\n* The .addGrid() method takes a model parameter and a list of values that you want to try. \n* The .build() method takes no arguments, it just returns the grid that you'll use later."}}