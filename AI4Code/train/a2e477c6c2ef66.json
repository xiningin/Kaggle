{"cell_type":{"49f56419":"code","70efa5e0":"code","af2ae892":"code","fc082fd8":"code","c3d70196":"code","93442377":"code","9f573653":"code","1508d47a":"code","57db08b2":"code","eefbadd3":"code","8101a296":"code","8a856afa":"code","c7cbe219":"code","40295852":"code","5ec47b21":"code","c72f0090":"code","748eb616":"code","7f223cae":"code","00b88ae5":"code","74837d5b":"code","a769ef87":"code","db3a2c3c":"code","a61a50dc":"code","7399c18a":"code","a082d174":"markdown","4e1aca5b":"markdown","f75e771e":"markdown","49c6933d":"markdown","5ec086ae":"markdown","62c303b8":"markdown","e56e7f39":"markdown","45443b15":"markdown","1fab6da1":"markdown","23703c9f":"markdown","6464adfa":"markdown","629fb966":"markdown","04cc776a":"markdown","fd4eba04":"markdown","4a3c3a6e":"markdown","e0367507":"markdown","32a381b2":"markdown","b5d72a9c":"markdown","b9c3ad22":"markdown","85b5b77f":"markdown","fe9abbd2":"markdown","701b6efd":"markdown","7b9d5063":"markdown","d6923a2c":"markdown","c60fc91b":"markdown","b7008a08":"markdown"},"source":{"49f56419":"# Data Processing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns  # data visualization\nimport plotly.express as px # data visualization\nimport plotly.graph_objs as go # go object plot\n\n# Image Processing\n#import random # Data Augment\n#import cv2 # Image Preprocessing\n\n# Machine Learning\nfrom sklearn.preprocessing import OneHotEncoder # Output Preprocessing\nfrom sklearn.model_selection import train_test_split # Machine Learning Data Preprocessing\n#!pip install tensorflow --upgrade\n#!pip install -U tensorflow-gpu==2.0.0\n#!pip install -q tf-nightly-2.0-preview\n#!pip install kerastuner\nimport tensorflow as tf # Machine Learning DL Module\nfrom tensorflow import keras # Machine Learning DL Module\nfrom tensorflow.keras.models import Sequential # Machine Learning DL Module\nfrom tensorflow.keras.layers import Dense, MaxPool2D, Flatten, Conv2D, Dropout # Machine Learning DL Module\nfrom tensorflow.compat.v2.keras.utils import to_categorical # Output Processing\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator # Data Augmentation\nfrom tensorflow.keras import regularizers\n#from tensorflow.keras.utils import multi_gpu_model\n#from tensorboard.plugins.hparams import api as hp\n#from kerastuner.tuners import GridSearch #Hyperparameters\n#from kerastuner.distributions import Range, Choice #Hyperparameters\n#from tensorboard.plugins.hparams import api as hp #Hyperparameters\nfrom sklearn.model_selection import GridSearchCV #Hyperparameters\n#!pip install talos\n#import talos #Hyperparameters\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n\n#from tensorflow.contrib.opt import AdamWOptimizer \n#from tensorflow.python.keras.optimizers import TFOptimizer\n\nprint(\"Tensorflow DL Version: \" + tf.__version__)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\"\"\"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n# Any results you write to the current directory are saved as output.\nprint(\"Setup Completed\")","70efa5e0":"train_file = \"..\/input\/digit-recognizer\/train.csv\"\npredict_file = \"..\/input\/digit-recognizer\/test.csv\"\nsubmission_file = \"..\/input\/digit-recognizer\/sample_submission.csv\"\n\n#X, y = prep_data(train_file)\ntrain_data = pd.read_csv (train_file, sep=',') \npredict_data = pd.read_csv (predict_file, sep=',') \nprint(\"Files Preparation Completed\")","af2ae892":"print('**************************Train File Preliminary Investigation**************************')\nprint('1. Train File Shape:', train_data.shape)\nmissing_val_count_by_column = (train_data.isnull().sum())\nprint('2. Train File Missing Valu:', missing_val_count_by_column[missing_val_count_by_column > 0])\nprint(\"   Missing Values in Train File: {}\".format(train_data.isna().any().any()))\nprint('3. Train File Variables Information:')\nprint(train_data.info())\nprint('4. Train File Variables Unique Number:')\nprint(train_data.nunique())\nprint('                                                                                       ')\nprint('**************************Prediction File Preliminary Investigation**************************')\nprint('1. Test File Shape:', predict_data.shape)\nmissing_val_count_by_column = (predict_data.isnull().sum())\nprint('2. Test File Missing Value:', missing_val_count_by_column[missing_val_count_by_column > 0])\nprint(\"   Missing Values in Test File: {}\".format(predict_data.isna().any().any()))\nprint('3. Test File Variables Information:')\nprint(predict_data.info())\nprint('4. Test File Variables Unique Number:')\nprint(predict_data.nunique())","fc082fd8":"def datatype_check(data, type_list):\n    for type_name in type_list:\n        try:\n            temp = data[type_name]\n        except:\n            print('Some Issues in Data Type: {}!'.format(type_name))\n            pass\n    print('Data Type Check OK!')\n\n# Parameters Setting\nimg_rows, img_cols = 28, 28\nnum_classes = 10\noutput_list = ['label']\nprint(\"Parameters Setting OK\")\n\nprint('**************************Output Data List**************************')\nprint(output_list)\ndatatype_check(train_data, output_list)","c3d70196":"print('Train File:')\ntrain_data.head().T","93442377":"print('Prediction File:')\npredict_data.head().T","9f573653":"# Count Plot\ndef count_plot(data, type_list, target_list):\n    for type_name in type_list:\n        for target_name in target_list:\n            plt.figure()\n            if type_name == target_name:\n                sns.countplot(x= type_name, data=data)\n            else:\n                sns.countplot(x= type_name, hue=target_name, data=data)\n        \nprint(\"Function Read Completed\")","1508d47a":"print(\"Train File:\")\ncount_plot(train_data, ['label'], ['label'])","57db08b2":"def label_conversion(data, num_classes = 10):\n    label_array = np.array(data, dtype='uint8') #pd.get_dummies(data['label'])\n    label_array = to_categorical(label_array,num_classes=num_classes)\n    return label_array\n\ndef images_conversion(data, img_row = 28, imag_col = 28):\n    image_array = np.array(data, dtype='uint8')\n    image_array = np.reshape(image_array, (len(image_array), img_row, imag_col, 1))\n    image_array = image_array * (1. \/ 255) - 0.5\n    image_array = np.reshape(image_array, (len(image_array), img_row, imag_col, 1))\n    \n    return image_array\n\nprint(\"Function Established Completed\")","eefbadd3":"y = label_conversion(train_data[output_list[0]], num_classes = num_classes)\nX = images_conversion(train_data.drop(output_list[0], axis=1), img_row = img_rows, imag_col = img_cols)\nX_predict = images_conversion(predict_data, img_row = img_rows, imag_col = img_cols)\nprint(\"Conversion Completed\")","8101a296":"print(\"Training Data: {} \\n  Labels: {}\".format(X, y))","8a856afa":"print(\"Prediction Data: {} \\n\".format(X_predict))","c7cbe219":"# Data Augmentation Setting \ndata_aug = ImageDataGenerator(\n           featurewise_center=False,  # set input mean to 0 over the dataset\n           samplewise_center=False,  # set each sample mean to 0\n           featurewise_std_normalization=False,  # divide inputs by std of the dataset\n           samplewise_std_normalization=False,  # divide each input by its std\n           zca_whitening=False,  # apply ZCA whitening\n           rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n           zoom_range = 0.1, # Randomly zoom image \n           width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n           height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n           horizontal_flip=False,  # randomly flip images\n           vertical_flip=False)  # randomly flip images\n#data_aug.fit(X_train)\n\nprint(\"Data Augmentation Setting Completed\")","40295852":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0)\nprint(\"Data Preparation Completed\")","5ec47b21":"# Parameters Setting\nl_model_name = 'lenet5_model' # Original Model\nv_model_name = 'vgg_model' # Transfer Learning Model\nbatch_size = 64\nnum_classes = 10\nepoch_num = 6\ndropout_rate = 0.5\n\n# adam, sgd with weight decay\noz = keras.optimizers.Adam(lr=0.001, \n                           beta_1=0.9, \n                           beta_2=0.999, \n                           epsilon=1e-08, \n                           decay=1e-4, \n                           amsgrad=False)\nsgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n\n\n# Hyper Parameters Setting\n#HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n#HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.5, 0.8))\n#HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\n\nprint(\"Parameters Setting Completed\")","c72f0090":"# Callback Setting\n# Save the Best Model as High val_acc\nl_ck_callback = tf.keras.callbacks.ModelCheckpoint(l_model_name+'_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', mode='max',\n                                                  verbose=2, save_best_only=True, save_weights_only=True)\nl_best_callback = tf.keras.callbacks.ModelCheckpoint(l_model_name+'_weights.h5', monitor='val_acc', mode='max',\n                                                  verbose=2, save_best_only=True, save_weights_only=True)\n# Tensorboard Monitor\nl_tb_callback = tf.keras.callbacks.TensorBoard(log_dir='lenet_logs')\n# Adjusting the Learning Rate for the each epoch if the val_loss don't drop down\n# learning rate = factor * lr_old\nl_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=2, factor=0.5, min_lr=0.0001)\n\n# LeNet-5 Model Established\nlenet5_model = Sequential(name=l_model_name)\nlenet5_model.add(Conv2D(16, kernel_size=(5, 5),\n                 activation='relu',\n                 input_shape=(img_rows, img_cols, 1)))\nlenet5_model.add(MaxPool2D(pool_size=(2, 2),\n                 strides=None,\n                 padding='valid'))\nlenet5_model.add(Conv2D(16, kernel_size=(5, 5),\n                 activation='relu'))\nlenet5_model.add(MaxPool2D(pool_size=(2, 2),\n                 strides=None,\n                 padding='valid'))\nlenet5_model.add(Flatten())\nlenet5_model.add(Dense(120, input_shape=(400,), activation='relu'))\nlenet5_model.add(Dense(84, input_shape=(120,), activation='relu'))\nlenet5_model.add(Dense(num_classes, input_shape=(84,), activation='softmax'))\n\nlenet5_model.compile(loss=keras.losses.categorical_crossentropy,\n                     optimizer=oz,\n                     metrics=['accuracy', 'categorical_crossentropy'])\nlenet5_model.summary()","748eb616":"# Training\/ Testing\nlenet5_model_history = lenet5_model.fit_generator(data_aug.flow(X_train, y_train, batch_size=batch_size),\n                                                  epochs = epoch_num, \n                                                  validation_data = (X_test, y_test),\n                                                  verbose = 2, \n                                                  steps_per_epoch=X_train.shape[0],\n                                                  callbacks=[l_tb_callback, \n                                                             l_lr_callback, \n                                                             l_ck_callback, \n                                                             l_best_callback])\nlenet5_model.save(l_model_name+'.h5')\ndel lenet5_model\nprint(\"Training Completed!\")","7f223cae":"# Callback Setting\n# Save the Best Model as High val_acc\nv_ck_callback = tf.keras.callbacks.ModelCheckpoint(v_model_name+'_weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5', monitor='val_acc', mode='max',\n                                                  verbose=2, save_best_only=True, save_weights_only=True)\nv_best_callback = tf.keras.callbacks.ModelCheckpoint(v_model_name+'_weights.h5', monitor='val_acc', mode='max',\n                                                  verbose=2, save_best_only=True, save_weights_only=True)\n# Tensorboard Monitor\nv_tb_callback = tf.keras.callbacks.TensorBoard(log_dir='logs')\n# Adjusting the Learning Rate for the each epoch if the val_loss don't drop down\n# learning rate = factor * lr_old\nv_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=2, factor=0.5, min_lr=0.0001)\n\n# Hyperparameters Monitor\n\"\"\"\nMETRIC_ACCURACY = 'accuracy'\nwith tf.summary.create_file_writer('logs\/vgg_hparam_tuning').as_default():\n    hp.hparams_config(\n    hparams=[HP_DROPOUT, HP_OPTIMIZER], #HP_NUM_UNITS, \n    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n  )\n\"\"\"\n\n# VGG-Like Model:\nvgg_model = Sequential(name=v_model_name)\n\nvgg_model.add(Conv2D(32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape=(img_rows, img_cols, 1)))\nvgg_model.add(Conv2D(32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nvgg_model.add(MaxPool2D(pool_size=(2,2)))\nvgg_model.add(Dropout(dropout_rate*0.5))\n\nvgg_model.add(Conv2D(64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nvgg_model.add(Conv2D(64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nvgg_model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nvgg_model.add(Dropout(dropout_rate*0.5))\n\nvgg_model.add(Flatten())\nvgg_model.add(Dense(256, activation = \"relu\"))\nvgg_model.add(Dropout(dropout_rate))\nvgg_model.add(Dense(num_classes, activation = \"softmax\"))\n\nvgg_model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer=sgd,\n                  metrics=['accuracy', 'categorical_crossentropy'])\nvgg_model.summary() ","00b88ae5":"# Training\/ Testing\nvgg_model_history = vgg_model.fit_generator(data_aug.flow(X_train, y_train, batch_size=batch_size),\n                                            epochs = epoch_num, \n                                            validation_data = (X_test, y_test),\n                                            verbose = 2, \n                                            steps_per_epoch=X_train.shape[0],\n                                            callbacks=[v_tb_callback, \n                                                       v_lr_callback, \n                                                       v_ck_callback, \n                                                       v_best_callback])\nvgg_model.save(v_model_name+'.h5')\ndel vgg_model\nprint(\"Training Completed!\")","74837d5b":"def plot_history(histories, key='categorical_crossentropy', epoch = 3):\n    epoch_array = range(epoch)\n    plt.figure(figsize=(16,10))\n    for name, history in histories:\n        plt.figure(figsize=(8, 8))\n        plt.subplot(2, 1, 1)\n        plt.plot(epoch_array, history['acc'], label='Training Accuracy')\n        plt.plot(epoch_array, history['val_acc'], label='Validation Accuracy')\n        plt.legend(loc='lower right')\n        plt.ylabel('Accuracy')\n        #plt.ylim([min(plt.ylim()),1])\n        plt.grid()\n        plt.title(name.title()+' Training and Validation Accuracy')\n        \n        plt.subplot(2, 1, 2)\n        plt.plot(epoch_array, history['loss'], label='Training Loss')\n        plt.plot(epoch_array, history['val_loss'], label='Validation Loss')\n        plt.legend(loc='upper right')\n        plt.ylabel('Cross Entropy')\n        #plt.ylim([0,1.0])\n        plt.grid()\n        plt.title(name.title()+' Training and Validation Loss')\n        plt.xlabel('epoch')\n        plt.show()\n        \"\"\"\n        val = plt.plot(epoch_array, history['val_'+key],\n                       '--', label=name.title()+' Val')\n        plt.plot(epoch_array, history[key], color=val[0].get_color(),\n                 label=name.title()+' Train')\n        plt.xlabel('Epochs')\n        plt.ylabel(key.replace('_',' ').title())\n        plt.legend()\n        plt.xlim([0,max(epoch_array)])\n        plt.grid()\n        \"\"\"   \n\n#print(lenet5_model_history.history.keys())\nplot_history([(l_model_name, lenet5_model_history.history),\n              (v_model_name, vgg_model_history.history)], epoch = epoch_num)","a769ef87":"def best_model(histories, key='categorical_crossentropy'):\n    ini_flag = bool(1)\n    best_model = None\n    best_score = 0\n    for name, history in histories:\n        val_score_record = history['val_'+key]\n        for score in val_score_record:\n            if ini_flag:\n                best_score = score\n                best_model = name\n                ini_flag = bool(0)\n            \n            if best_score > score:\n                best_score = score\n                best_model = name\n        \n    return best_model\n\nbest_model_name = best_model([(l_model_name, lenet5_model_history.history),\n                             (v_model_name, vgg_model_history.history)])\nprint(best_model_name)","db3a2c3c":"# Hyperparameters Monitor\n#HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([16, 32]))\n\"\"\"\nHP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.5, 0.8))\nHP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))\nMETRIC_ACCURACY = 'accuracy'\n\nwith tf.contrib.summary.create_file_writer('logs\/hparam_tuning').as_default():\n  hp.hparams_config(\n    hparams=[HP_DROPOUT, HP_OPTIMIZER], #HP_NUM_UNITS, \n    metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],\n  )\n\"\"\"\n\"\"\"\nparam_grid = dict(optimizer = ['adam', 'sgd'], dropout_rate = [0.5, 0.8])\nhyper_best_model_name = 'best_model'\n\n# Model Load\nload_model = keras.models.load_model(best_model_name +'.h5')\nload_model.load_weights(best_model_name + '_weights.h5')\nprint(\"Check Model:\")\nload_model.evaluate(X_test, y_test)\n#load_model.compile(loss=keras.losses.categorical_crossentropy,\n#                   optimizer=param_grid, \n#                   metrics=['accuracy', 'categorical_crossentropy'])\nload_model.summary()\n\n# Grid Search\nkeras_model = KerasClassifier(build_fn=load_model, verbose = 3) #build_fn=\ngrid_search_model = GridSearchCV(estimator=keras_model, param_grid=param_grid, n_jobs=-1, cv = 5)\n#print(type(grid_search_model))\ngrid_search_model.fit(X_train, y_train)\n#load_model.save(hyper_best_model_name+'.h5')\ngrid_result.best_estimator_.model.save(grid_best_model_name)\n\"\"\"","a61a50dc":"# Load Model\nload_model = keras.models.load_model(best_model_name+'.h5')\n#load_model.load_weights(best_model_name + '_weights.h5')\nprint(\"Check Model:\")\nload_model.evaluate(X_test, y_test)\n\n# Submission File Read\nsample_submission = pd.read_csv(submission_file)\nsubmission_id = sample_submission[\"ImageId\"]\n\n# Submission File Prediction\n#print(np.argmax(load_model.predict(X), axis=1))\nsubmission = pd.DataFrame({\n    \"ImageId\": submission_id,\n    \"Label\": np.argmax(load_model.predict(X_predict), axis=1)\n    })\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission File Produced Completed\")","7399c18a":"submission.head()","a082d174":"# 5. Model Establihsed\nIn this section and next section, we show the how to use the LeNet-5 Model and the VGG-Like Model to train, test and predict the unknown data.\nFirst, we need to prepare the data for training; then, we want to established the model as following.","4e1aca5b":"This project, we cover several part:\n\n- Preparation\n  - Tool Liberary Usage\n  - Data Preparation\n- Define the Problem\n- Explore Data Analysis, EDA\n  - Preliminary Investigation\n  - Statistical Information\n- Feature Engineering\n  - Gray Images Conversion\n  - Data Augmentation\n- Model Established\n  - LeNet-5 Model\n  - VGG-Like Model\n  - Model Selection\n- Model Prediction","f75e771e":"According to the learning curve, the performance of the VGG-Like model is better than the LeNet-5 model. ","49c6933d":"Data conversion from the train file and the prediction file:","5ec086ae":"# 4. Feature Engineering\nAccording to the EDA, we have to use the two methods to established the recognition model. First, we will convert the image array to the gray image matrix. Second, we use the data augmentation technolgy to increase the images data.\n\n## Gray Images Conversion\nFirst, we need to convert the array to the gray image matrix as following as Library:","62c303b8":"The best model is:","e56e7f39":"We want to check the data:","45443b15":"## Model Selection\nWe will select the best model from the original LeNet-5 Model and the modified model from the learning curve as following:","1fab6da1":"If you have any questions or suggestions, please let me know, thank you. Enjoy it!","23703c9f":"## LeNet-5 Model \nThis Project, we use the LeNet-5 model to solve the problem. LeNet-5 paper is the fundemental CNN in Deep Learning, along side that paper. In this project, we will use the original LeNet-5 model to solve the problem, and then using VGG-like model by learning to solve the problem. We will select the best model from these models to predict the unknown data.\n\nAccording to the reference (Ref to [my GitLab](https:\/\/gitlab.com\/pcyslm\/tf_mnistrecognition_rd), using the TensorFlow module to solve the MNIST problem), there are three type layer in the LeNet-5:\n- Convolution Layer: The main function is features extraction; there are two features, local receptive fields and shared weights.\n- Max Pooling Layer: The main function is subsampling; down the sensitivity of the shift\/scale\/distortion invariance of the output.\n- Fully Connected Layer, FC: Also called multi-layer perceptrons, the main function is classfication or regression, but it has high degree sensitivity of the input\n\nWe set the callback monitor to save the model; then, define the LeNet-5 model:","6464adfa":"## Data Augmentation\nSecond, we use the tensorflow keras library to do the images augmentation as following as setting:","629fb966":"# 6. Model Prediction\nIn this section, we use the best model to predict the unknown images data:","04cc776a":"## VGG-Like Model\n According to the [keras official web](https:\/\/keras.io\/getting-started\/sequential-model-guide\/), we use the VGG-Like model to solve the problem. VGG Model is the first using the droup out layer, and using more hidden layers to training data relative LeNet-5 model. Similarly, we set the parameters for callback, and then define the model.","fd4eba04":"# 2. Define Problem\nBased on the problem description, we summarize as following as descriptions:\n\n- Problem Target: Multi-Classifications, digital 0-9\n- Train File: 251.mb data, each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. There are 785 columns, including the 784 pixels and 1 label.\n- Prediction File: 167.5 mb data, 784 pixels for each image\n\n# 3. Explore Data Analysis, EDA\n## Preliminary Investigation\nFirst, we need to know some simple information about the files:","4a3c3a6e":"# ** Basic Model for MNIST Solution, TF-v1, Top 10 **\n\nAuthor: CY Peng\n\n- First Release: 2019\/9\/23, Transfer Leaerning Model: Required More Training\n- Second Release: 2019\/9\/24, Bug Fixed \n- Third Release: 2019\/9\/25, VGG-Like Model Solution\n- Fourth Test: 2020\/1\/2, Using Tenflow-v1 to Established Model","e0367507":"Next, we will excute the training stage:","32a381b2":"Of the above investigation, there are some tips for two files:\n\n- From 1., there are 42000 images in the train file and 28000 images in the prediction file\n- From 2., we know that there are no missing values in the files! So, don't descard any image.\n- From 3., all variables type are integer, we need to convert the array to the gray image matrix.\n- From 4., we know that the file image pixel ID is from 0 to 783. Label is the digital 0-9, the problem is the multi-classification.\n\nWe set some parameters and check the data type:","b5d72a9c":"We set the some parameters:","b9c3ad22":"## Data Preparation\nHere, we read the train and prediction file.","85b5b77f":"Check the submission file:","fe9abbd2":"## Statistical Information\nHere, we show the distribution plot for the data. First, we write the some plot as following as code:","701b6efd":"From above plot, we know that the each data type are around 4000 data. According to the statistics theory, there are at least 5000 data to established the good model (statistic model); therefore, we want to use the data augmentation technology to increase the different images data as following feature engineering. ","7b9d5063":"Count Plot:","d6923a2c":"Finally, we check the some data for the train and the test files:","c60fc91b":"Training stage:","b7008a08":"# 1. Preparation\n\n## Tool Liberary Usage\nFirst, we import the libraries for this project. According to the data science, there are five steps to analyze for this project:\n- Step 1: Define the Problem\n- Step 2: Explore Data Analysis, EDA: we use the numpy, pandas and matplotlib liberaries to analyze the data.\n- Stpe 3: Feature Engineering: we use the numpy, pandas and the cv2 to process the data\n- Step 4: Model Established: we use the tensorflow to established the model\n- Step 5: Model Maintenance: this step, we use the model to predict the unknown data\n\nHere, we import these libraries:"}}