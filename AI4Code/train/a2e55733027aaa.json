{"cell_type":{"ee6619cc":"code","71c9475b":"code","6acf92b1":"code","f120c92f":"code","74a81279":"code","85d3b6c5":"code","bad1761e":"code","84f6e051":"code","a438a772":"code","2260bd4c":"code","bea9795c":"code","6bc84b71":"code","ca02ffee":"code","f7651352":"code","741b62cd":"code","6f4df939":"code","3fdb2ded":"code","4d71e56d":"code","4531b6ba":"code","d3ff2647":"code","70c9df9f":"code","b3e44e19":"code","273eeab6":"code","8bac9418":"code","b99f7844":"code","9416cfbb":"code","55b327c1":"code","e3d00ab8":"code","1682b3b8":"code","afa74d63":"markdown","0950e651":"markdown","98a88e8f":"markdown","56b502ef":"markdown","e8969aea":"markdown","5aa6022a":"markdown","6494e281":"markdown"},"source":{"ee6619cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71c9475b":"import sklearn\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","6acf92b1":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_tree as plot_xgboost","f120c92f":"df = pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\ndf.sample(5)","74a81279":"print(df['blueWins'].value_counts())  # checking the dataset is balanced","85d3b6c5":"df.dtypes # checking if any of the input values need to be turned into numbers e.g. if had categorical info. Not the case here.","bad1761e":"# checking if any entries are duplicates - all entries unique so all good\nprint(len(df['gameId'].unique()))\nprint(df.shape)","84f6e051":"df = df.drop('gameId', axis=1) # unnecessary column","a438a772":"# looking at edge cases i.e. pure white and pure black\ncorr = df.corr()\nplt.figure(figsize=(15,10))\nax= plt.subplot()\nsns.heatmap(corr, ax=ax)","2260bd4c":"# removing columns that have correlated values greater than 0.9, or less than -0.9\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9:\n            if columns[j]:\n                columns[j] = False\n        if corr.iloc[i,j] <= -0.9:\n            if columns[j]:\n                columns[j] = False\nselected_columns = df.columns[columns]\ndf = df[selected_columns]","bea9795c":"df.shape # 12 unecessary columns have been removed","6bc84b71":"X = df.drop('blueWins', axis=1)\ny = df['blueWins']","ca02ffee":"from sklearn import preprocessing\nX_scaled = preprocessing.scale(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y,random_state=1) \nclf = SVC(kernel = 'linear')\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)","f7651352":"X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1)\nrf = RandomForestClassifier(n_estimators=10, max_depth=3)\nrf.fit(X_train,y_train)\nrf.score(X_test,y_test)","741b62cd":"# 5x2cv significance test (Deittrich, 1998)\n# p value indicates no significance difference in model performance on the datasets\n# will proceed with RF as can use analysis tools (e.g. SHAP, eli5)\n\nclf1 = RandomForestClassifier(n_estimators=10, max_depth=3,random_state=1)\nclf2 = SVC(kernel='linear', random_state=1)\n\nfrom mlxtend.evaluate import paired_ttest_5x2cv\n\n\nt, p = paired_ttest_5x2cv(estimator1=clf1,\n                          estimator2=clf2,\n                          X=X, y=y, random_seed=123)\n\nprint('t statistic: %.3f' % t)\nprint('p value: %.3f' % p)","6f4df939":"# confusion matrix\n\npredictions = rf.predict(X_test)\ncm = confusion_matrix(y_test, predictions)\ncm","3fdb2ded":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ntitle = \"Learning Curve (RF)\"\ncv = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n\nestimator = RandomForestClassifier(n_estimators=10, max_depth=3)\nplot_learning_curve(estimator, title, X, y, ylim=(0.5, 1.01), cv=cv, n_jobs=4)\n\n# plt.savefig('learningcurve.png', format='png', dpi=600)\nplt.show()","4d71e56d":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(rf).fit(X_test, y_test)\ndfs = eli5.formatters.as_dataframe.explain_weights_df(perm)\ndfs.head()","4531b6ba":"# replacing feature numbers with names\n\nfeatures  = dfs.iloc[:,0]\nfeatures = features.values.tolist()\n\na = np.hstack(features)\na\nfeature = []\nfor n in range(0,27):\n    string = str(a[n])\n    cut = int(string[1:])\n    feature.append(cut)\n    \nfeature_list = []\n\nfor n in range(0,27):\n    num = feature[n] +1                # adding 1 as first column is blueWins\n    name = df.columns[num]\n    feature_list.append(name)\n    \ndfs.insert(0, 'Features', feature_list)\ndfs=dfs.drop('feature',axis=1)\ndfs.head()","d3ff2647":"import shap\nshap.initjs()\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X)\nshap.summary_plot(shap_values, X)\n\n#class 0 = Red win\n#class 1 = Blue win","70c9df9f":"# shows correlation between blue having more gold and getting more kills\n\nshap.dependence_plot(\"blueGoldDiff\", shap_values[1], X, interaction_index=\"blueKills\")","b3e44e19":"conf = []\n\nfor i in range(0,len(X_test)):\n    x = rf.predict_proba([X_test.iloc[i]])\n    y = tuple(x[0])\n    conf.append(y)\nsortedd = sorted(conf, key=lambda tup: tup[1])\nrank = [i[0] for i in sortedd]\nimport matplotlib.pyplot  as plt\nplt.figure(figsize=(5,4))\nplt.xlabel('Ranked Test Samples')\nplt.ylabel('Prediction Confidence')\nplt.title('Confidence Ranking')\nplt.plot(rank)\nplt.show()","273eeab6":"blue_wins = sorted(range(len(conf)),key = lambda k: conf[k])\nred_wins = sorted(range(len(conf)),key = lambda k: conf[k], reverse = True)\ntop_blue = blue_wins[0:10]\ntop_red = red_wins[0:10]\nprint(top_blue)\nprint(top_red)","8bac9418":"eli5.show_prediction(rf, X_test.iloc[55], show_feature_values=True)","b99f7844":"eli5.show_prediction(rf, X_test.iloc[80], show_feature_values=True)","9416cfbb":"selected_columns = selected_columns[1:]\nimport statsmodels.api as sm\ndef backwardElimination(x, Y, sl, columns):\n    numVars = len(x[0])\n    for i in range(0, numVars):\n        regressor_OLS = sm.OLS(Y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        if maxVar > sl:\n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    x = np.delete(x, j, 1)\n                    columns = np.delete(columns, j)\n                    \n    regressor_OLS.summary()\n    return x, columns\nSL = 0.05\ndata_modeled, selected_columns = backwardElimination(df.iloc[:,1:].values, df.iloc[:,0].values, SL, selected_columns)","55b327c1":"result = pd.DataFrame()\nresult['blueWin'] = df.iloc[:,0]\ndata = pd.DataFrame(data = data_modeled, columns = selected_columns)\ndata","e3d00ab8":"# remove these categories as predominatley value is 0\ndata1 = data.drop(['blueTowersDestroyed','redTowersDestroyed'], axis=1)","1682b3b8":"fig = plt.figure(figsize = (20, 25))\nj = 0\n\nfor i in data1.columns:\n    plt.subplot(6, 4, j+1)\n    j += 1\n    sns.distplot(data1[i][result['blueWin']==0], color='r', label = 'Red Win')\n    sns.distplot(data1[i][result['blueWin']==1], color='b', label = 'Blue Win')\n    plt.legend(loc='best')\nfig.suptitle('LoL Data Analysis')\nfig.tight_layout()\nfig.subplots_adjust(top=0.95)\nplt.show()","afa74d63":"Printing a list of the most confident predictions in each category, so can be further analysed","0950e651":"# RF Model Analysis","98a88e8f":"# Feature Selection  \n## Remove highly correlated features that don't add info  \ne.g. blueKills gives same info as redDeaths  \ne.g. redFirstBlood is inverse of blueFirstBlood","56b502ef":"Learning Curve  \n\nCurve indicates that model isn't overtraining, and that adding more training samples is unlikely to improve performance.","e8969aea":"# Model Selection","5aa6022a":"replacing feature numbers with names","6494e281":"blueWins = 1 ---> Blue Team Wins  \nblueWins = 0 ---> Red Team Wins"}}