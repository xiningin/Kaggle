{"cell_type":{"7c201bc8":"code","4462b0e4":"code","d54bcb0d":"code","174d5876":"code","9eae1c4b":"code","fb0e98ac":"code","109437cd":"code","2fb57948":"code","030931d4":"code","af7a64cc":"code","7a1b0de3":"code","33a2ce95":"code","d19e53e5":"code","d62e7fc0":"code","4c889302":"code","3838cce2":"code","cd993827":"code","e8a81794":"code","4f39a57a":"code","99e9e77d":"code","76f5e11c":"code","b9761d42":"code","add350d0":"code","61e995a2":"code","e61f7299":"code","2e04ec5f":"code","d7a80eea":"code","12616056":"code","3fa58144":"code","db240a66":"code","df1e827c":"code","076ed31d":"code","d49539e2":"code","c288e976":"code","28869d38":"code","c50f60e1":"code","8b9894cd":"markdown","27fc0372":"markdown","10a5e3bb":"markdown","3fe0f7ed":"markdown","e10e7195":"markdown","4c2eb786":"markdown","65170c36":"markdown","187b5893":"markdown","ab21a7ea":"markdown","5eda370b":"markdown","293b2e0c":"markdown","179258b1":"markdown","c17b0111":"markdown","1108c79c":"markdown","8f70d296":"markdown","a6989079":"markdown","a87863f5":"markdown","0fc631b5":"markdown"},"source":{"7c201bc8":"import pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc\n\nimport os\nimport string\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\nfrom bs4 import BeautifulSoup\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","4462b0e4":"df_train = pd.read_csv(\"..\/input\/drugsComTrain_raw.csv\")\ndf_test = pd.read_csv(\"..\/input\/drugsComTest_raw.csv\")\nlen_train = df_train.shape[0]","d54bcb0d":"df_train['sentiment'] = df_train[\"rating\"].apply(lambda x: 2 if x > 7 else (1 if ((x>4)&(x<=7)) else 0))\ndf_test['sentiment'] = df_test[\"rating\"].apply(lambda x: 2 if x > 7 else (1 if ((x>4)&(x<=7)) else 0))\nsolution = df_test['sentiment']","174d5876":"df_train.head()","9eae1c4b":"from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\n\n#folds = KFold(n_splits=5, shuffle=True, random_state=546789)\ntarget = df_train['sentiment']\nfeats = ['usefulCount']\n\nsub_preds = np.zeros(df_test.shape[0])\n\ntrn_x, val_x, trn_y, val_y = train_test_split(df_train[feats], target, test_size=0.2, random_state=42) \nfeature_importance_df = pd.DataFrame() \n    \nclf = LGBMClassifier(\n        n_estimators=2000,\n        learning_rate=0.05,\n        num_leaves=30,\n        #colsample_bytree=.9,\n        subsample=.9,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=2,\n        silent=-1,\n        verbose=-1,\n        )\n        \nclf.fit(trn_x, trn_y, \n        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n        eval_metric='multi_logloss', verbose=100, early_stopping_rounds=100  #30\n    )\n\nsub_preds = clf.predict(df_test[feats])\n        \nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = feats\nfold_importance_df[\"importance\"] = clf.feature_importances_\nfeature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)","fb0e98ac":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_pred=sub_preds, y_true=solution)","109437cd":"df_all = pd.concat([df_train,df_test])\ndel df_train, df_test;\ngc.collect()","2fb57948":"df_all['date'] = pd.to_datetime(df_all['date'])\ndf_all['day'] = df_all['date'].dt.day\ndf_all['year'] = df_all['date'].dt.year\ndf_all['month'] = df_all['date'].dt.month","030931d4":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\ndef review_to_words(raw_review):\n    # 1. HTML \uc81c\uac70\n    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n    # 2. \uc601\ubb38\uc790\uac00 \uc544\ub2cc \ubb38\uc790\ub294 \uacf5\ubc31\uc73c\ub85c \ubcc0\ud658\n    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n    # 3. \uc18c\ubb38\uc790 \ubcc0\ud658\n    words = letters_only.lower().split()\n    # 4. \ud30c\uc774\uc36c\uc5d0\uc11c\ub294 \ub9ac\uc2a4\ud2b8\ubcf4\ub2e4 \uc138\ud2b8\ub85c \ucc3e\ub294\uac8c \ud6e8\uc52c \ube60\ub974\ub2e4.\n    # stopwords \ub97c \uc138\ud2b8\ub85c \ubcc0\ud658\ud55c\ub2e4.\n    stops = set(stopwords.words('english'))\n    # 5. Stopwords \ubd88\uc6a9\uc5b4 \uc81c\uac70\n    meaningful_words = [w for w in words if not w in stops]\n    # 6. \uc5b4\uac04\ucd94\ucd9c\n    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n    # 7. \uacf5\ubc31\uc73c\ub85c \uad6c\ubd84\ub41c \ubb38\uc790\uc5f4\ub85c \uacb0\ud569\ud558\uc5ec \uacb0\uacfc\ub97c \ubc18\ud658\n    return( ' '.join(stemming_words))\n    #return( ' '.join(words))","af7a64cc":"%time df_all['review_clean'] = df_all['review'].apply(review_to_words)","7a1b0de3":"from textblob import TextBlob\nfrom tqdm import tqdm\nreviews = df_all['review_clean']\n\nPredict_Sentiment = []\nfor review in tqdm(reviews):\n    blob = TextBlob(review)\n    Predict_Sentiment += [blob.sentiment.polarity]\ndf_all[\"Predict_Sentiment\"] = Predict_Sentiment","33a2ce95":"df_all.head()","d19e53e5":"np.corrcoef(df_all[\"Predict_Sentiment\"], df_all[\"rating\"])","d62e7fc0":"def review_to_words(raw_review):\n    # 1. HTML \uc81c\uac70\n    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n    # 2. \uc601\ubb38\uc790\uac00 \uc544\ub2cc \ubb38\uc790\ub294 \uacf5\ubc31\uc73c\ub85c \ubcc0\ud658\n    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n    # 3. \uc18c\ubb38\uc790 \ubcc0\ud658\n    words = letters_only.lower().split()\n    # 4. \ud30c\uc774\uc36c\uc5d0\uc11c\ub294 \ub9ac\uc2a4\ud2b8\ubcf4\ub2e4 \uc138\ud2b8\ub85c \ucc3e\ub294\uac8c \ud6e8\uc52c \ube60\ub974\ub2e4.\n    # stopwords \ub97c \uc138\ud2b8\ub85c \ubcc0\ud658\ud55c\ub2e4.\n    #stops = set(stopwords.words('english'))\n    # 5. Stopwords \ubd88\uc6a9\uc5b4 \uc81c\uac70\n    #meaningful_words = [w for w in words if not w in stops]\n    # 6. \uc5b4\uac04\ucd94\ucd9c\n    #stemming_words = [stemmer.stem(w) for w in meaningful_words]\n    # 7. \uacf5\ubc31\uc73c\ub85c \uad6c\ubd84\ub41c \ubb38\uc790\uc5f4\ub85c \uacb0\ud569\ud558\uc5ec \uacb0\uacfc\ub97c \ubc18\ud658\n    #return( ' '.join(stemming_words) )\n    return( ' '.join(words))","4c889302":"%time df_all['review_clean2'] = df_all['review'].apply(review_to_words)","3838cce2":"from textblob import TextBlob\nfrom tqdm import tqdm\nreviews = df_all['review_clean2']\n\nPredict_Sentiment = []\nfor review in tqdm(reviews):\n    blob = TextBlob(review)\n    Predict_Sentiment += [blob.sentiment.polarity]\ndf_all[\"Predict_Sentiment2\"] = Predict_Sentiment","cd993827":"np.corrcoef(df_all[\"Predict_Sentiment2\"], df_all[\"rating\"])","e8a81794":"df_all.head()","4f39a57a":"list(df_all['review'].values)[0]","99e9e77d":"#\ubb38\uc7a5\uae38\uc774 (\uc904\ubc14\uafc8\ud45c\uc2dc\uac00 \uba87\ubc88\ub098\uc654\ub294\uc9c0 \uc148)\ndf_all['count_sent']=df_all[\"review\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n\n#Word count in each comment:(\ub2e8\uc5b4\uac2f\uc218)\ndf_all['count_word']=df_all[\"review_clean\"].apply(lambda x: len(str(x).split()))\ndf_all['count_word2']=df_all[\"review_clean2\"].apply(lambda x: len(str(x).split()))\n\n#Unique word count(unique\ud55c \ub2e8\uc5b4 \uac2f\uc218)\ndf_all['count_unique_word']=df_all[\"review_clean\"].apply(lambda x: len(set(str(x).split())))\ndf_all['count_unique_word2']=df_all[\"review_clean2\"].apply(lambda x: len(set(str(x).split())))\n\n#Letter count(\ub9ac\ubdf0\uae38\uc774)\ndf_all['count_letters']=df_all[\"review_clean\"].apply(lambda x: len(str(x)))\ndf_all['count_letters2']=df_all[\"review_clean2\"].apply(lambda x: len(str(x)))\n\n#punctuation count(\ud2b9\uc218\ubb38\uc790)\ndf_all[\"count_punctuations\"] = df_all[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n#upper case words count(\uc804\ubd80\ub2e4 \ub300\ubb38\uc790\uc778 \ub2e8\uc5b4 \uac2f\uc218)\ndf_all[\"count_words_upper\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n#title case words count(\uccab\uae00\uc790\uac00 \ub300\ubb38\uc790\uc778 \ub2e8\uc5b4 \uac2f\uc218)\ndf_all[\"count_words_title\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n#Number of stopwords(\ubd88\uc6a9\uc5b4 \uac2f\uc218)\nstops = set(stopwords.words('english'))\ndf_all[\"count_stopwords\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n\n#Average length of the words(\ud3c9\uade0\ub2e8\uc5b4\uae38\uc774)\ndf_all[\"mean_word_len\"] = df_all[\"review_clean\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_all[\"mean_word_len2\"] = df_all[\"review_clean2\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","76f5e11c":"df_all['season'] = df_all[\"month\"].apply(lambda x: 1 if ((x>2) & (x<6)) else(2 if (x>5) & (x<9) else (3 if (x>8) & (x<12) else 4)))","b9761d42":"df_all.head()","add350d0":"df_train = df_all[:len_train]\ndf_test = df_all[len_train:]","61e995a2":"from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\n\n#folds = KFold(n_splits=5, shuffle=True, random_state=546789)\ntarget = df_train['sentiment']\nfeats = ['usefulCount','day','year','month','Predict_Sentiment','Predict_Sentiment2', 'count_sent',\n 'count_word', 'count_word2', 'count_unique_word', 'count_unique_word2', 'count_letters', 'count_letters2', 'count_punctuations',\n 'count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len', 'mean_word_len2', 'season']\n\nsub_preds = np.zeros(df_test.shape[0])\n\ntrn_x, val_x, trn_y, val_y = train_test_split(df_train[feats], target, test_size=0.2, random_state=42) \nfeature_importance_df = pd.DataFrame() \n    \nclf = LGBMClassifier(\n        n_estimators=10000,\n        learning_rate=0.10,\n        num_leaves=30,\n        #colsample_bytree=.9,\n        subsample=.9,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=2,\n        silent=-1,\n        verbose=-1,\n        )\n        \nclf.fit(trn_x, trn_y, \n        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n        eval_metric='multi_logloss', verbose=100, early_stopping_rounds=100  #30\n    )\n\nsub_preds = clf.predict(df_test[feats])\n        \nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = feats\nfold_importance_df[\"importance\"] = clf.feature_importances_\nfeature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)","e61f7299":"cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","2e04ec5f":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_pred=sub_preds, y_true=solution)","d7a80eea":"# \ucc38\uace0\uc790\ub8cc : https:\/\/github.com\/corazzon\/KaggleStruggle\/blob\/master\/word2vec-nlp-tutorial\/tutorial-part-1.ipynb\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\n# \ud29c\ud1a0\ub9ac\uc5bc\uacfc \ub2e4\ub974\uac8c \ud30c\ub77c\uba54\ud130 \uac12\uc744 \uc218\uc815\n# \ud30c\ub77c\uba54\ud130 \uac12\ub9cc \uc218\uc815\ud574\ub3c4 \uce90\uae00 \uc2a4\ucf54\uc5b4 \ucc28\uc774\uac00 \ub9ce\uc774 \ub0a8\nvectorizer = CountVectorizer(analyzer = 'word', \n                             tokenizer = None,\n                             preprocessor = None, \n                             stop_words = None, \n                             min_df = 2, # \ud1a0\ud070\uc774 \ub098\ud0c0\ub0a0 \ucd5c\uc18c \ubb38\uc11c \uac1c\uc218\n                             ngram_range=(4, 4),\n                             max_features = 20000\n                            )\nvectorizer","12616056":"# \uc18d\ub3c4 \uac1c\uc120\uc744 \uc704\ud574 \ud30c\uc774\ud504\ub77c\uc778\uc744 \uc0ac\uc6a9\ud558\ub3c4\ub85d \uac1c\uc120\n# \ucc38\uace0 : https:\/\/stackoverflow.com\/questions\/28160335\/plot-a-document-tfidf-2d-graph\npipeline = Pipeline([\n    ('vect', vectorizer),\n])","3fa58144":"df_train.head()","db240a66":"%time train_data_features = pipeline.fit_transform(df_train['review_clean'])\n%time test_data_features = pipeline.fit_transform(df_test['review_clean'])","df1e827c":"train_data_features.shape","076ed31d":"vocab = vectorizer.get_feature_names()\nprint(len(vocab))\nvocab[:10]","d49539e2":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Bidirectional, LSTM, BatchNormalization, Dropout\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","c288e976":"#x_train, x_val, y_train, y_val = train_test_split(train_data_features, df_train['sentiment'].values, test_size=0.1, random_state=42)","28869d38":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.utils import to_categorical\nimport random\n\n# 1. \ub370\uc774\ud130\uc14b \uc0dd\uc131\ud558\uae30\n\ny_train = to_categorical(df_train['sentiment'].values, num_classes=3) # one-hot \uc778\ucf54\ub529\n\ny_test = to_categorical(solution.values, num_classes=3) # one-hot \uc778\ucf54\ub529\n\n# 2. \ubaa8\ub378 \uad6c\uc131\ud558\uae30\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=20000, activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(4, activation='relu'))\nmodel.add(Dense(3, activation='softmax'))\n\n# 3. \ubaa8\ub378 \ud559\uc2b5\uacfc\uc815 \uc124\uc815\ud558\uae30\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# 4. \ubaa8\ub378 \ud559\uc2b5\uc2dc\ud0a4\uae30\nhist = model.fit(train_data_features,y_train, epochs=100, batch_size=64)\n\n# 5. \ud559\uc2b5\uacfc\uc815 \uc0b4\ud3b4\ubcf4\uae30\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, loss_ax = plt.subplots()\n\nacc_ax = loss_ax.twinx()\n\nloss_ax.set_ylim([0.0, 3.0])\nacc_ax.set_ylim([0.0, 1.0])\n\nloss_ax.plot(hist.history['loss'], 'y', label='train loss')\nacc_ax.plot(hist.history['acc'], 'b', label='train acc')\n\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nacc_ax.set_ylabel('accuray')\n\nloss_ax.legend(loc='upper left')\nacc_ax.legend(loc='lower left')\n\nplt.show()\n\n# 6. \ubaa8\ub378 \ud3c9\uac00\ud558\uae30\nloss_and_metrics = model.evaluate(test_data_features, y_test, batch_size=32)\nprint('loss_and_metrics : ' + str(loss_and_metrics))","c50f60e1":"confusion_matrix(y_pred=sub_preds, y_true=solution)","8b9894cd":"### RandomForest\nAccuracy : 0.62","27fc0372":"Next\n- Bag of Words : \ud558\ub098\uc758 \ub2e8\uc5b4 \ubb36\uc74c https:\/\/www.kaggle.com\/theainerd\/bag-of-words-meets-bag-of-popcorn\n- Word2Vec : \ub2e8\uc5b4\uac04\uc758 \uad00\uacc4\ub97c \uc54c\ub824\uc90c\n- n-gram : \uba87 \uac1c\uc758 \ub2e8\uc5b4\ub97c \uc774\uc6a9\ud558\uc5ec \ubb36\uc74c\uc744 \ub9cc\ub4e4\uc9c0 ( n\uc744 \ub2e4\uc591\ud558\uac8c \uc2e4\ud5d8 )\n- \ub525\ub7ec\ub2dd \n- embadding : \ub2e8\uc5b4\ub97c \uc218\uce58\ud654 \ud558\ub294 \ubc29\ubc95","10a5e3bb":"add other features","3fe0f7ed":"make  reviews clean, but get strange results that are getting correlation worse. (0.34853791 -> 0.23246)","e10e7195":"from sklearn.ensemble import RandomForestClassifier\n\n\nforest = RandomForestClassifier(\n    n_estimators = 100, n_jobs = -1, random_state=2018)","4c2eb786":"TextBlob code in [Adarsh Chavakula - A gentle introduction to Sentiment Analysis\n](https:\/\/www.kaggle.com\/adarshchavakula\/a-gentle-introduction-to-sentiment-analysis)","65170c36":"code in https:\/\/www.kaggle.com\/michaelapers\/lstm-starter-notebook","187b5893":"result = forest.predict(test_data_features)\nresult[:10]","ab21a7ea":"from sklearn.model_selection import cross_val_score\n%time score = np.mean(cross_val_score(forest,train_data_features,df_train['sentiment'],cv=5,scoring='roc_auc'))","5eda370b":"### Deep Learning","293b2e0c":"code in [Jagan - Stop the S@#$ - Toxic Comments EDA](https:\/\/www.kaggle.com\/jagangupta\/stop-the-s-toxic-comments-eda)","179258b1":"%time forest = forest.fit(train_data_features, df_train['sentiment'])","c17b0111":"*TEAM - EDA is Club in Hanyang Univ. *\n\n*Our team first made the following kernel with a beginner challenge. : [Beginner Challenge : House Price](https:\/\/www.kaggle.com\/chocozzz\/beginner-challenge-house-prices)*\n\n*Objective: **Sentiment  analysis***\n\n*Member : Hyun woo Kim, ju yeon Park, ji ye lee, ju yeong lee, eun joo min, su min song*\n","1108c79c":"Only using stopwords 0.34853791 -> 0.34597794","8f70d296":"season\n- 1(spring): 3~5 month\n- 2(summer): 6~8 month\n- 3(fall): 9~11 month\n- 4(winter): 12~2 month","a6989079":"accuracy_score(result,solution)","a87863f5":"### Word2Vec, gensim, embedding \uc801\uc6a9","0fc631b5":"Reference : https:\/\/datascienceschool.net\/view-notebook\/3e7aadbf88ed4f0d87a76f9ddc925d69\/"}}