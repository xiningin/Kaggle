{"cell_type":{"e180dd43":"code","eb8c682b":"code","2c64d320":"code","545a7c0f":"code","56bee217":"code","97120ffc":"code","7700c2e3":"code","c9105198":"code","ec2eebf5":"code","0ea308b0":"code","10e63bed":"code","91bf02f8":"code","e180689b":"code","0769bbbb":"code","d0e07291":"code","1d8ccaaf":"code","fd92a128":"code","7c5ce8b3":"markdown","5c436d7b":"markdown","b4512b60":"markdown","0349f3bc":"markdown","5ef9abfc":"markdown","50d43e49":"markdown","9898a853":"markdown","16ed352b":"markdown","20a53144":"markdown","c20bd1ac":"markdown","dcbbd712":"markdown","63902db7":"markdown","8a60f5b1":"markdown","bf34a7df":"markdown","e4bf6d3d":"markdown","dfb00a25":"markdown"},"source":{"e180dd43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb8c682b":"data=pd.read_csv('\/kaggle\/input\/heart-attack-prediction\/data.csv')\ndata.shape","2c64d320":"data.dtypes","545a7c0f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True, annot=True);","56bee217":"data.rename(columns={'num       ': 'target'}, inplace=True) \ndata=data.replace('?',None)\ndata=data.replace('?',0)\ndata.head()","97120ffc":"#Getting Pandas Dummies for ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n\ndata = pd.get_dummies(data, columns = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope','thal','ca'])","7700c2e3":"#Scaling the other attributes using normal scaler\n\nstandardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndata[columns_to_scale] = standardScaler.fit_transform(data[columns_to_scale])","c9105198":"data.head()","ec2eebf5":"y = data['target']\nX = data.drop('target',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)\n\n#80% Train and 20% Test Data","0ea308b0":"#Plotting the Target (Heart Disease)\nplt.figure(figsize=(6,4))\nsns.countplot(y)\nplt.show()","10e63bed":"#Analysing the shape of X_train and X_test Data\n\nprint(X_train.shape)\nprint(X_test.shape)","91bf02f8":"from sklearn.svm import SVC\n\n#Function for storing model scores using various kernals\nsvc_scores = []\nkernel_type = ['linear', 'poly', 'rbf', 'sigmoid']\nfor type in kernel_type:\n    svc_classifier = SVC(kernel = type)\n    svc_classifier.fit(X_train, y_train)\n    svc_scores.append(svc_classifier.score(X_test, y_test))","e180689b":"#Plotting the accuracy\n\nfor i in range(len(kernel_type)):\n    label = round(svc_scores[i], 5)\n    plt.text(i, svc_scores[i], label)\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')\nplt.bar(kernel_type, svc_scores)","0769bbbb":"#Training the model on 'rbf' Kernal\n\nsvc =  SVC(kernel='rbf')\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\n\n#Printing the confussion matrix and accuracy scores\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(classification_report(y_test, svc_predicted))\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Classifier: {:.3f}\".format(svc_acc_score*100),'%\\n')","d0e07291":"from sklearn.ensemble import RandomForestClassifier\n\nmodel=RandomForestClassifier(n_estimators=500,random_state=1)\nmodel.fit(X_train,y_train)\nrfpred=model.predict(X_test)\nRF_conf_matrix = confusion_matrix(y_test, rfpred)\nrf_acc_score = accuracy_score(y_test, rfpred)\n\n#Printing the confussion matrix and accuracy scores\nprint(\"confussion matrix\")\nprint(RF_conf_matrix)\nprint(classification_report(y_test, rfpred))\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest Classifier: {:.3f}\".format(rf_acc_score*100),'%\\n')","1d8ccaaf":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\n\n#Printing the confussion matrix and accuracy scores\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(classification_report(y_test,lr_predict))\nprint(\"Accuracy of Logistic Regression: {:.3f}\".format(lr_acc_score*100),'%\\n')","fd92a128":"from sklearn.neural_network import MLPClassifier\nMLP = MLPClassifier(hidden_layer_sizes=(32), learning_rate_init=0.001, max_iter=150)\nmodel = MLP.fit(X_train, y_train)\nMLP_predict = MLP.predict(X_test)\nMLP_conf_matrix = confusion_matrix(y_test, MLP_predict)\nMLP_acc_score = accuracy_score(y_test, MLP_predict)\n\n\n#Printing the confussion matrix and accuracy scoresprint(\"confussion matrix\")\nprint(MLP_conf_matrix)\nprint(\"\\n\")\nprint(classification_report(y_test,MLP_predict))\nprint(\"Accuracy of Multilayer Perceptron classifier: {:.3f}\".format(MLP_acc_score*100),'%\\n')","7c5ce8b3":" # MODELS\n\n     1) SVM\n     2) Random Forest\n     3) Logistic Regression\n     4) Multi-layer Perceptron classifier ","5c436d7b":"We can see that the **rbf** kernel gives the maximum accuracy. Training the final model in rbf","b4512b60":"## Running SVM model with Various Kernals","0349f3bc":"# Objective\n### To predict whether a patient is at risk for a heart attack. This is a binary outcome.\n\nPositive (+) = 1, patient is at risk\nNegative (-) = 0, patient is not at risk","5ef9abfc":"# Exploring the Dataset ","50d43e49":"# Preprocessing the Data","9898a853":"# 3) Logistic Regression","16ed352b":"**OldPeak**(ST depression induced by exercise relative to rest) and **CP** ( Chest Pain Type ) have the most correlation with **target** ( diagnosis of heart disease)","20a53144":"# 4) Multi-layer Perceptron classifier ","c20bd1ac":"### We can see that the random forest gives an accuracy about 96%. This is my first public notebook in kaggle, It will be great if you guys show some love. Also, Let me know if you have any questions or suggestions below! Thanks a lot for your time :)","dcbbd712":"# Splitting data as Train and Test","63902db7":"# Final Preprocessed Data","8a60f5b1":"## visualization of Correlation in Data","bf34a7df":"# 1) Using SVM","e4bf6d3d":"# 2) Random Forest","dfb00a25":"# Understanding the Dataset\n* age (#)\n* sex : 1 = Male, 0 = Female (Binary)\n* (cp) chest pain [type (4 values, Ordinal)]: 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic\n* (trestbps) resting blood pressure (#)\n* (chol) serum cholestoral in mg\/dl (#)\n* (fbs) fasting blood sugar > 120 mg\/dl (Binary) [1 = true; 0 = false]\n* (restecg) resting electrocardiographic results [values 0,1,2]\n* (thalach) maximum heart rate achieved (#)\n* (exang) exercise induced angina (Binary) [1 = yes; 0 = no]\n* (oldpeak) = ST depression induced by exercise relative to rest (#)\n* (slope) of the peak exercise ST segment (Ordinal) [ 1: upsloping, 2: flat , 3: downsloping)\n* (ca) number of major vessels (0-3, Ordinal) colored by fluoroscopy\n* (thal) maximum heart rate achieved (Ordinal) [3 = normal; 6 = fixed defect; 7 = reversable defect]"}}