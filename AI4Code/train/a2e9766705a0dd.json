{"cell_type":{"8336afeb":"code","23074be0":"code","78095261":"code","5fb59b8c":"code","ab500781":"code","eb965d8e":"code","c727cbca":"code","9448963e":"code","0e5de39c":"code","e38943d0":"code","78ee7a4d":"code","2ed9ba0b":"code","fefb5439":"code","7f7a44bf":"code","17709b7c":"code","19d66f56":"code","da6c0ac0":"markdown"},"source":{"8336afeb":"import numpy as np\nimport matplotlib.pyplot as plt \nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization\nfrom keras.utils import to_categorical\nfrom keras.callbacks import LearningRateScheduler","23074be0":"(train_x, train_y), (test_x, test_y) = tf.keras.datasets.fashion_mnist.load_data()","78095261":"print('train_x shape => ', train_x.shape)\nprint('train_y shape => ', train_y.shape)\nprint('')\nprint('test_x shape => ', test_x.shape)\nprint('test_y shape => ', test_y.shape)","5fb59b8c":"print('no. of images in train_set => ', train_x.shape[0])\nprint('shape of an image in train_set => ', train_x[0].shape)\nprint('\\nno. of images in test_set => ', test_x.shape[0])\nprint('shape of an image in test_set => ', test_x[0].shape)","ab500781":"# Reshaping the data\ntrain_x = train_x.reshape(train_x.shape[0], 28, 28, 1)\ntest_x = test_x.reshape(test_x.shape[0], 28, 28, 1)\nprint('train_shape => ', train_x.shape)\nprint('test_shape => ', test_x.shape)","eb965d8e":"# Normalizing data\ntrain_x = train_x\/255\ntest_x = test_x\/255","c727cbca":"train_y = to_categorical(train_y)\ntest_y = to_categorical(test_y)","9448963e":"model1 = Sequential()","0e5de39c":"from keras.layers import LeakyReLU","e38943d0":"model1.add(Conv2D(32, kernel_size=(3, 3), padding = 'same' ,kernel_initializer='he_normal', input_shape=(28, 28, 1),name = 'conv1'))\nmodel1.add(LeakyReLU(alpha = 0.2))\nmodel1.add(MaxPooling2D((2, 2),name='pool1'))\nmodel1.add(Dropout(0.25, name = 'dropout1'))\nmodel1.add(BatchNormalization(name='batchnorm1'))\n\nmodel1.add(Conv2D(64, (3, 3), padding = 'same', name='conv2'))\nmodel1.add(LeakyReLU(alpha = 0.2))\nmodel1.add(MaxPooling2D(pool_size=(2, 2), name='pool2'))\nmodel1.add(Dropout(0.25, name='dropout2'))\nmodel1.add(BatchNormalization(name='batchnorm2'))\n\nmodel1.add(Conv2D(128, (3, 3), padding = 'same', name='conv3'))\nmodel1.add(LeakyReLU(alpha = 0.2))\nmodel1.add(MaxPooling2D(name='pool3'))\n#model1.add(Dropout(0.2, name='dropout3'))\nmodel1.add(BatchNormalization(name='batchnorm3'))\n\nmodel1.add(Flatten())\nmodel1.add(Dense(512, name='dense0', activation = 'relu'))\nmodel1.add(Dense(128, name='dense1', activation = 'relu'))\n#model1.add(Dropout(0.3, name='dropout4'))\nmodel1.add(Dense(10, activation='softmax', name='output'))","78ee7a4d":"model1.compile(loss=keras.losses.categorical_crossentropy,\n               optimizer=keras.optimizers.Adagrad(learning_rate=0.05),\n               metrics=['accuracy'])","2ed9ba0b":"model1.summary()","fefb5439":"from keras.callbacks import ModelCheckpoint\ncallback = ModelCheckpoint('checkpoint.h5', save_best_only = True, verbose=2)","7f7a44bf":"history = model1.fit(train_x, train_y, batch_size = 1000, epochs=80, validation_data = (test_x, test_y), verbose = 1, callbacks=[callback])","17709b7c":"model1.save('model1.adagrad_lr0.05_epoch200.h5')\n#model1.save('model1_adagrad.h5')\n#model1.save('model1_rmsprop.h5')\n#model1.save('model1_Adam.h5')","19d66f56":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","da6c0ac0":"Please Upvote the Notebook if you liked my work"}}