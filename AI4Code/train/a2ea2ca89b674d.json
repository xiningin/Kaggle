{"cell_type":{"93c325a9":"code","15fc2a7e":"code","486259c7":"code","92ccff19":"code","67b3bff6":"code","1ef2ea6f":"code","13516845":"code","fa354e2f":"code","e75944f9":"code","f02366ad":"code","3ee4e290":"code","c7011bc2":"code","03f0400c":"code","faa619c4":"code","b4b18a8a":"code","69d3ff10":"code","e2ed4774":"code","c8f8c102":"code","bf42154a":"code","6b2462f8":"code","8e0a77d4":"code","973ab3bf":"code","24ad9148":"code","cc78633b":"code","9435eda7":"code","86cf4803":"code","66ee602b":"code","eb1973c4":"code","1cb6f6da":"code","9d98956b":"code","cee14ed7":"code","e1199230":"code","3969010f":"code","b9450700":"code","6da17fb5":"code","293d47b4":"markdown","494685ea":"markdown","9bd07500":"markdown","8315a767":"markdown","18d6f054":"markdown","21ddfeae":"markdown"},"source":{"93c325a9":"import sklearn \nprint('The scikit-learn version is {}.'.format(sklearn.__version__))\nimport numpy \nprint('The scikit-learn version is {}.'.format(numpy.__version__))\nimport scipy \nprint('The scikit-learn version is {}.'.format(scipy.__version__))\nimport joblib \nprint('The scikit-learn version is {}.'.format(joblib.__version__))","15fc2a7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import tree\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import StandardScaler  \nfrom sklearn.neighbors import KNeighborsClassifier   \nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","486259c7":"df = pd.read_csv(\"..\/input\/heart.csv\")","92ccff19":"df.columns","67b3bff6":"# renaming column names to be more interpretable \n\nupdated_cols = ['Age', 'Sex', 'ChestPainType', 'RestingBP', 'SerumCholestoral', 'FastingBloodSugar', 'RestingECG', \n               'MaxHeartRate', 'ExeriseEnducedAngina', 'OldPeak', 'SlopeOldPeak', 'MajorVessels', 'Thal', 'Output']\ndf.columns = updated_cols\ndf.to_csv(\"..heart1.csv\")\ndf.head()","1ef2ea6f":"X = df.iloc[:, :-1].values  \ny = df.iloc[:, -1].values  ","13516845":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)  ","fa354e2f":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 15):  \n    clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=i, min_samples_split=2,\n                                  min_samples_leaf=1, random_state=0)\n    clf.fit(X_train, y_train)\n    pred_i = clf.predict(X_test)\n    error.append(np.mean(pred_i != y_test))","e75944f9":"plt.figure(figsize=(12, 6))  \nplt.plot(range(1, 15), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate DT Depth')  \nplt.xlabel('Depth')  \nplt.ylabel('Mean Error')  ","f02366ad":"clf = tree.DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=4, min_samples_split=2,\n                                  min_samples_leaf=1, random_state=0)\nclf = clf.fit(X_train, y_train )\n(pd.Series(clf.feature_importances_, index=df.columns[:-1])\n   .nlargest(13)\n   .plot(kind='barh'))","3ee4e290":"from sklearn.tree import export_graphviz\nexport_graphviz(clf, out_file='tree.dot', feature_names = df.columns[:-1],\n                class_names = df.columns[-1],\n                rounded = True, proportion = False, precision = 2, filled = True)","c7011bc2":"# Convert to png\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n","03f0400c":"# Display in python\nplt.figure(figsize = (37, 45))\nplt.imshow(plt.imread('tree.png'))\nplt.axis('off');\nplt.show();","faa619c4":"plt.savefig('tree.png')","b4b18a8a":"from sklearn.metrics import accuracy_score\ny_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)","69d3ff10":"scaler = StandardScaler()  \nscaler.fit(X_train)\n\nX_train_s = scaler.transform(X_train)  \nX_test_s = scaler.transform(X_test)  ","e2ed4774":"classifier = KNeighborsClassifier(n_neighbors=4)  \nclassifier.fit(X_train_s, y_train)  ","c8f8c102":"y_pred = classifier.predict(X_test_s)  ","bf42154a":"from sklearn.metrics import classification_report, confusion_matrix  \nprint(confusion_matrix(y_test, y_pred))  \nprint(classification_report(y_test, y_pred))  ","6b2462f8":"error = []\n\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):  \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train_s, y_train)\n    pred_i = knn.predict(X_test_s)\n    error.append(np.mean(pred_i != y_test))","8e0a77d4":"plt.figure(figsize=(12, 6))  \nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',  \n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')  \nplt.xlabel('K Value')  \nplt.ylabel('Mean Error')  ","973ab3bf":"sns.distplot(df['Age'],color='Red',hist_kws={'alpha':1,\"linewidth\": 2}, kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": \"KDE\"})","24ad9148":"plt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu',fmt='.2f',linewidths=2)","cc78633b":"sns.set_style('whitegrid')\nplt.figure(figsize=(30,15))\nsns.pairplot(df, hue='Output', palette='coolwarm')","9435eda7":"import numpy as np\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 4\n\n\nX = np.c_[X_train[:,0], X_train[:,7]].reshape(len(X_train),2) # we only take the first two features. We could\n                      # avoid this ugly slicing by using a two-dim dataset\ny = y_train\nh = .02  # step size in the mesh\n\n# Create color maps\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA' ])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n\nfor weights in ['uniform', 'distance']:\n    # we create an instance of Neighbours Classifier and fit the data.\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    \n    \n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # Plot also the training points\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"2-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n\nplt.show()","86cf4803":"from sklearn.cluster import KMeans\nimport numpy as np\nX_train_s = np.c_[X_train[:,4], X_train[:,7]].reshape(len(X_train_s),2)\nX_test_s =  np.c_[X_test_s[:,4], X_test_s[:,7]].reshape(len(X_test_s),2)\nkmeans = KMeans(n_clusters=2, random_state=0).fit(X_train_s)\ny_pred = kmeans.predict(X_test_s)\naccuracy_score(y_test, y_pred)","66ee602b":"kmeans.cluster_centers_\nX_train_s","eb1973c4":"plt.scatter(X_train_s[:,0], X_train_s[:,1], c=kmeans.labels_, cmap='rainbow')  \nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='black')  ","1cb6f6da":"from sklearn import datasets\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ny_pred = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Number of mislabeled points out of a total %d points : %d\"\n     % (X_test.shape[0],(y_test != y_pred).sum()))\n","9d98956b":"from sklearn import metrics\ny_pred = gnb.predict(X_test)\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","cee14ed7":"from sklearn import svm\nclf = svm.SVC(gamma='scale')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\naccuracy_score(y_test, y_pred)\n","e1199230":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.datasets import make_classification\nada=AdaBoostClassifier()\nsearch_grid={'n_estimators':[500,1000,2000],'learning_rate':[.001,0.01,.1]}\nsearch=GridSearchCV(estimator=ada,param_grid=search_grid,scoring='accuracy',n_jobs=1,cv=5)\nsearch","3969010f":"clf = AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,\n          learning_rate=1.0, n_estimators=50, random_state=None)\nclf.fit(X_train, y_train)  ","b9450700":"(pd.Series(clf.feature_importances_, index=df.columns[:-1])\n   .nlargest(13)\n   .plot(kind='barh'))","6da17fb5":"score=np.mean(cross_val_score(clf,X_train,y_train,scoring='accuracy',cv=5,n_jobs=1))\nscore","293d47b4":"**3.K-Means**","494685ea":"**5. SVM**","9bd07500":"**1. Decision Tree. CART Algorithm**","8315a767":"**4. Naive bayes**","18d6f054":"**2. K-NN**\n","21ddfeae":"**6. AdaBoost**"}}