{"cell_type":{"7368ccc8":"code","03398d99":"code","b7777566":"code","e7ec159f":"code","b504c3b9":"code","d4864279":"code","55ab7f75":"code","d0e5f106":"code","9e84f5bc":"code","b816ff18":"code","96d31909":"code","aac0d1e2":"code","b937c551":"code","9848ac0f":"code","6581083a":"markdown","3d160681":"markdown","5f58fe7e":"markdown","5e5a3aef":"markdown","770af195":"markdown","dac54a72":"markdown","3237bd0e":"markdown","b9594600":"markdown","6f9d3ac6":"markdown","7b8ef384":"markdown","c93ddf5b":"markdown","7fbc9056":"markdown","e730722b":"markdown","a6198067":"markdown","22af92f6":"markdown","f6099498":"markdown","85676710":"markdown","61e56554":"markdown","fa29c393":"markdown","eb82b402":"markdown","48fc621e":"markdown","c325feb4":"markdown","81cca594":"markdown","9f2f15ab":"markdown","3963553f":"markdown","095553ef":"markdown","2eb3f996":"markdown","9d898e58":"markdown"},"source":{"7368ccc8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Visualizing\nimport copy","03398d99":"dataset = pd.read_csv(\"..\/input\/admission-prediction\/Admission_Predict.csv\")\npd.set_option('display.max_columns',len(dataset.columns))\ndataset.head(5)# gives us the first five of the sample dataset","b7777566":"#Let's look at the exact columns' name\ndataset.columns","e7ec159f":"# we are goning to seprate the class label ( Chance of Admit ) from the rest.\nY = dataset.iloc[:,-1].values\nX = dataset.drop([\"Serial No.\",\"Chance of Admit \"],axis=1)","b504c3b9":"dataset.info()","d4864279":"X.describe()","55ab7f75":"columns = X.columns\nfig = plt.figure(figsize=(8,8))\nfor i in range(0,7):\n\n    ax = plt.subplot(3, 3, i+1)\n    ax.hist(X[columns[i]],bins = 20, color = 'blue', edgecolor = 'black')\n    \n    #set title name of each\n    ax.set_title(columns[i])\n    \nplt.tight_layout()\nplt.show()","d0e5f106":"X.corr()","9e84f5bc":"import seaborn as sb # for vuisualizing\nfig, ax = plt.subplots(figsize=(7,7))\nsb.heatmap(X.corr(),linewidth = 0.5,annot=True)","b816ff18":"from sklearn.model_selection import train_test_split #spiliting\nX_train,X_test,Y_train,Y_test = train_test_split(X.values,Y,test_size = 0.25,random_state = 1)","96d31909":"from sklearn.linear_model import LogisticRegression # Logistic Regression to predict\nclassifier = LogisticRegression(random_state =0)\nclassifier.fit(X_train,Y_train)","aac0d1e2":"from sklearn.metrics import confusion_matrix, accuracy_score # estiating the model\nfrom sklearn.linear_model import LinearRegression # use instead of logistic reg. to get probablistic output then use threshold to scale.\n\n# make an indivisual logistic_regression\ndef Logistic_Regression(X_train,X_test,Y_train,Y_test,threshold = 0.5):\n    #fitting our model for current dataset\n    regressor = LinearRegression()\n    regressor.fit(X_train,Y_train)\n    #predict\n    Y_pred = regressor.predict(X_test)\n    \n    Y_test_temp = copy.deepcopy(Y_test)\n    for index in range(0,len(Y_pred)):\n        if Y_pred[index] >= threshold:\n            Y_pred[index] = 1\n        else:\n            Y_pred[index] = 0\n    for index2 in range(0,len(Y_test)):\n        if Y_test[index2] >= threshold:\n            Y_test_temp[index2] = 1\n        else:\n            Y_test_temp[index2] = 0\n    return Y_test_temp,Y_pred\n\n# our threshold list\nthreshold_list = [.05,.1,.15,.2,.25,.3,.35,.4,.45,.5,.55,.6,.65,.7,.75,.8,.85,.9,.95,.99]\nfor i in threshold_list:\n    Y_test_temp,Y_pred = Logistic_Regression(X_train,X_test,Y_train,Y_test,threshold = i)\n  \n    #now we can get the accuracy for current model\n    print(\"ACCURACY OF \",i,\" THRESHOLD : \",accuracy_score(Y_test_temp,Y_pred),'\\n')      ","b937c551":"Y_test,Y_pred = Logistic_Regression(X_train,X_test,Y_train,Y_test,threshold = 0.75)\nprint(\"ACCURACY OF \",0.75 ,\" THRESHOLD : \",accuracy_score(Y_test,Y_pred),'\\n')","9848ac0f":"# now lets see the confusion matrix of it\ncm = confusion_matrix(Y_test,Y_pred)\nprint(cm)","6581083a":"**MEAN** : with having means and take a look at the median(50%) of each attribute,we can rest assured that our dataset does not contain outliers.\n*the outliers are dangarous enemies for linear regressions and logistic regression.*\n\nBut, we can not rely on these information without having a survey on the histograms.","3d160681":"#### The purpose is to find a good reason to use Logistic Regression and discuss about the downsides of it with respect to how we can implement it from scratch\nLets role into the problem.","5f58fe7e":"# University Admission Prediction","5e5a3aef":"Regardless of what happened so far, you can use this model to predict weather you can get your \"ADMISSION\" from desired univerdity or not. hope you best of luck;)\n\nThank you so much for reading my notebook.\nplaese send me feedback about my mistakes, i'll appreciate you, correct me.\n","770af195":"  In this exercise the goal is to predict weather a student can enter his\/her desired university or not.\n\n  The data set, as you will see, contains the information of 400 students with 9 attributes(columns) including Serial number of each students(our we can say each sample),\nthe [Graduate Record Examinations (GRE)](https:\/\/en.wikipedia.org\/wiki\/Graduate_Record_Examinations),\nthe [Test of English as a Foreign Language (TOEFL)](https:\/\/en.wikipedia.org\/wiki\/Test_of_English_as_a_Foreign_Language#:~:text=The%20TOEFL%20iBT%20test%20is,to%20determine%20the%20total%20score.),\nthe [statement of purpose (SOP)](https:\/\/blog.nomadcredit.com\/statement-purpose-explanation-international-students\/),\nthe [Letter of Recommendation (LOR)](https:\/\/whizstorm.com\/what-is-a-letter-of-recommendation),\nthe [Culmulative Grade Point Average(CGPA)](https:\/\/byjus.com\/cbse\/cgpa-calculator\/)\nscores, the University rating, and also weather the student has at least one research or not.","dac54a72":"Now we can see that there is not such a high correlation to handle it or drop some of them.\nbecause the value of each variance should at most be 1 or -1 to lead us to the conclusion of dropping the attributes that have correlation and using one of them based on our consideration of each.\n\nBut does it always work to drop the correleated attributes?","3237bd0e":"So, there are some correlations between attributes, but before we judge, lets have more clear vision of these correaltion with heatmap!!","b9594600":"# Importing Our Dataset","6f9d3ac6":"\nTo scale these probablity-based output to 1 or 0 we have to set threshold.\nfor instance consider a sigmoid function:\n![image.png](http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1534281880\/image4_gw5mmv.png)\n![image.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/88\/Logistic-curve.svg\/480px-Logistic-curve.svg.png)\nSee!! There is a curve shifting at the exact number of 0.5 of y_axis and in logistic regression usually we choose 0.5 to conclude that the all y's above the 0.5 scale to 1 and bellow this number scale to zero.\nbut that's not always our optimal cut off!!\nin the following, first we are going to use a tricky solution and after that we are going to have a scientefic reason for how to pick our optimal threshold.\n\nThen lets figuring out how to choose our threshold for labling and how to correct it?!","7b8ef384":"# THE END.\n","c93ddf5b":"### Tricky Solution","7fbc9056":"One of the most important aspect of data analyzing, is to first, have a clear vision of the dataset and finding out how we can \nrescailing, regenerating(for missing values), and most importantly imabalancing them.\n\nSo lets do this step by step...","e730722b":"I'll cut the Serial no. attribute because it's not going to affect our dataset","a6198067":"# Fitting Logistic Regression Classifier","22af92f6":"The answer is 'NO'.\n\nThe correct answer to it is laid under Multicollinearity.\nBut what is multicollinearity or collinearity?\n\n> \"_In statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy._\" ___Wikipedia___\n\nAccording to the definition, there is kind of a relation between two or more attributes. It can affect our model by small degree. The real problem is that, if we have prefect correlation between variables, while the model are trying to find a perfect coefficient for one independent variable to predict the dependent one, it affects the coefficient of the other one, which makes it hard to estimate the coefficients independently.\n\nObviously, this problem weakens the estimation of each independant variables, which leads us to not a perfect model to predict our dependant variables.\nBut to cure this problem first you have to have a good reson for that, it's all depends on severity of the problem itself. For example if the correlation of GRE and TOEFEL is 1 or -1 then we have to solve the problem not just by dropping one of them, but by many remedies that statistically can prepare, related to Feature Engineering.\n\nVisit [Wekipedia](https:\/\/en.wikipedia.org\/wiki\/Multicollinearity#:~:text=Multicollinearity%20refers%20to%20a%20situation,equal%20to%201%20or%20%E2%88%921.) for more information.\n\nLets dive into fitting our model!","f6099498":"### Heatmap","85676710":"but if you are faced issues like this and also have a probablistc prediction with label class test set, you have to consider it with only words below:\n\n### The optimal cut off would be where tpr is high and fpr is low. tpr - (1-fpr) is zero or near to zero is the optimal cut off point.","61e56554":"Histograms are a good informative plots to handle previous missunderstandings of our dataset, especially for continous varibale distribution.\nfor instance, at the first plot of GRE score, the mean is approximatley 316, but we have biomodal distribution. In others words there is two peaks for our samples in term of this variable. in data analyzing, this means maybe, you can devide this variable into the gender subplots of students, which can lead to so much more information.\n\nAnother view of the histograms of SOP, LOR they are skewed to the left. and the reason maybe its that for a good application you need to have a better scores at these exams.\n\nAnd so musch more information that you can get from histograms to analyizng your data.\n\n#### (if you can help to get more information from the plots abive ,i'll appriciate it:).","fa29c393":"## Correlations","eb82b402":"As we can see here, we do not have any missing values.\n(the Non-Null count column)","48fc621e":"# Visualizing Our Dataset","c325feb4":"# Seprating the train and test dataset","81cca594":"as we see here, we had some values but which one to pick is a challenging decision!?\n\nThe most better one doesn't always lead to better regression or model. because it can force us to over-fitting.\naccording to this, we can choose 0.6 or 0.7 for instance.","9f2f15ab":"one of a tricky solution is to estimate a range of thrsholding values:\n\nwe will now make a possible list of our threshold and using all of them to get the best answer and result in term of accuracy by fitting a linear regression model.","3963553f":"OOPS. \nWhat's the problem?\n\nAs you can see here the output of our dataset is continous to pass through for logistic regression which needs a label class; zero or one.","095553ef":"# Importing Basic Libararies","2eb3f996":"## Description","9d898e58":"### Histograms"}}