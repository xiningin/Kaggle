{"cell_type":{"9ca15b7f":"code","8dd1d2d4":"code","e45b7614":"code","b24c652c":"code","e472dbf1":"code","124c58cc":"code","d7b78c3f":"code","bfcfff6a":"code","8bedc6f2":"code","1c60a96e":"code","b22cbb48":"code","d5461018":"code","2237bbe4":"code","9e1c20bf":"code","ba896fb0":"code","53ca674a":"code","3d397b8f":"code","ec458895":"code","39bc3297":"code","77c68668":"code","1e1ba7b9":"code","6b607d11":"code","106aa3ea":"code","275b14f0":"markdown","f726a55b":"markdown","05d463d4":"markdown","14414269":"markdown","8da8ac6b":"markdown","d4b7256a":"markdown","d042a762":"markdown","212b89ee":"markdown","58c788dd":"markdown","5609e8dc":"markdown","df7fe9c9":"markdown","3aa7484c":"markdown","265c587a":"markdown","aaaf8d99":"markdown","ac531f89":"markdown","101fc1e6":"markdown","faa4fb5c":"markdown","8e3814f9":"markdown","759486db":"markdown"},"source":{"9ca15b7f":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Get version python\/keras\/tensorflow\nfrom platform import python_version\nimport keras\nimport tensorflow as tf\n\n# Folder manipulation\nfrom pathlib import Path\nimport os\n\n# Linear algebra and data processing\nimport numpy as np\nimport pandas as pd\n\n# Model evaluation\nfrom sklearn.metrics import confusion_matrix\n\n# Visualisation of picture and graph\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\n\n# For loading bar\nimport tqdm\n\n# Keras importation\nfrom tqdm import tqdm, tqdm_notebook\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras_preprocessing.image import ImageDataGenerator\nfrom keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Dense, Dropout, Flatten\nfrom keras.regularizers import l2\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping","8dd1d2d4":"print(os.listdir(\"..\/input\"))\nprint(\"Keras version : \" + keras.__version__)\nprint(\"Tensorflow version : \" + tf.__version__)\nprint(\"Python version : \" + python_version())","e45b7614":"MAIN_DIR = '..\/input\/'\nTRAIN_DIR = MAIN_DIR + \"train\/train\/\"\nTEST_DIR = MAIN_DIR + \"test\/test\/\"\nIMG_ROWS = 32\nIMG_COLS = 32\nCHANNELS = 3\nIMG_SHAPE = (IMG_ROWS, IMG_COLS, CHANNELS)\n\n# Set graph font size\nsns.set(font_scale=1.3)","b24c652c":"def plot_pictures(nb_rows=6, nb_cols=6, figsize=(14, 14)):\n    # Set up the grid\n    fig, ax = plt.subplots(nb_rows, nb_cols, figsize=figsize, gridspec_kw=None)\n    fig.subplots_adjust(wspace=0.4, hspace=0.4)\n\n    for i in range(0, nb_rows):\n        for j in range(0, nb_cols):\n            data = pd.read_csv(MAIN_DIR + \"train.csv\")\n            index = np.random.randint(0, data.shape[0])\n            file = data.loc[index]['id']\n\n            # Load picture\n            img_ = cv2.imread(TRAIN_DIR + file)\n    \n            # Hide grid\n            ax[i, j].grid(False)\n            ax[i, j].axis('off')\n            \n            # Plot picture on grid\n            ax[i, j].imshow(img_)\n            ax[i, j].set_title(\"Label : \" + str(data.loc[index]['has_cactus']))","e472dbf1":"plot_pictures(6, 6)","124c58cc":"df_train = pd.read_csv(MAIN_DIR + \"train.csv\")","d7b78c3f":"fig, ax = plt.subplots(figsize=(5, 5))\nsns.countplot(df_train['has_cactus'])","bfcfff6a":"df_train['has_cactus'].value_counts()","8bedc6f2":"def load_data(dataframe=None, batch_size=16, mode='categorical'):\n    gen = ImageDataGenerator(\n        zoom_range=0.1,\n        validation_split=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        fill_mode='nearest')\n    \n    df = pd.read_csv(MAIN_DIR + 'train.csv')\n    df['has_cactus'] = df['has_cactus'].apply(str)\n    \n    data_train = gen.flow_from_dataframe(df, \n                                          directory=TRAIN_DIR, \n                                          x_col='id', \n                                          y_col='has_cactus', \n                                          has_ext=True, \n                                          target_size=(32, 32),\n                                          class_mode=mode, \n                                          batch_size=batch_size, \n                                          shuffle=True,\n                                          subset='training')\n        \n    data_test = gen.flow_from_dataframe(df, \n                                      directory=TRAIN_DIR, # Changer les chemins\n                                      x_col='id', \n                                      y_col='has_cactus', \n                                      has_ext=True, \n                                      target_size=(32, 32),\n                                      class_mode=mode, \n                                      batch_size=batch_size, \n                                      shuffle=True, \n                                      subset='validation')\n    \n    return data_train, data_test","1c60a96e":"def create_model():\n    model = Sequential()\n    \n    model.add(Conv2D(16, (3, 3), input_shape=(32, 32, 3), padding='same', use_bias=False, name=\"first_conv\"))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(32, (3, 3), padding='same', use_bias=False, name=\"second_conv\"))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(64, (3, 3), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(64, (3, 3), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.25))\n\n    model.add(Conv2D(128, (3, 3), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(128, (3, 3), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(256, (3, 3), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(256, (3, 3), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(512, (3, 3), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Conv2D(512, (3, 3), padding='same', use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(256, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.25))\n    model.add(Dense(2, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])\n    \n    return model","b22cbb48":"def train(train_generator, val_generator):\n    model = create_model()\n\n    cbs = [ReduceLROnPlateau(monitor='loss', factor=0.5, patience=1, min_lr=1e-7, verbose=0),\n           EarlyStopping(monitor='val_loss', min_delta=0.0001, \n                         patience=10, verbose=1, mode='min', restore_best_weights=True)]\n    \n    history = model.fit_generator(train_generator, \n                        steps_per_epoch=(train_generator.n\/\/train_generator.batch_size), \n                        epochs=100, \n                        validation_data=val_generator, \n                        validation_steps=len(val_generator), \n                        shuffle=True, \n                        callbacks=cbs, \n                        verbose=1)\n    return model, history","d5461018":"train_generator, val_generator = load_data(batch_size=32)\nmodel, history = train(train_generator, val_generator)","2237bbe4":"model.summary()","9e1c20bf":"def plot_loss(history):\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n    \n    # Plot train\/val accuracy\n    ax[0].plot(history.history['acc'])\n    ax[0].plot(history.history['val_acc'])\n    ax[0].set_title('Model accuracy')\n    ax[0].set_ylabel('Accuracy')\n    ax[0].set_xlabel('Epochs')\n    ax[0].legend(['Train', 'Test'], loc='upper left')\n    \n    # Plot train\/val loss\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_ylabel('Loss')\n    ax[1].set_xlabel('Epochs')\n    ax[1].legend(['Train', 'Test'], loc='upper left')","ba896fb0":"plot_loss(history)","53ca674a":"def plot_lr(history):\n    fig, ax = plt.subplots(figsize=(7, 5))\n    \n    # Plot learning rate\n    ax.plot(history.history['lr'])\n    ax.set_title('Learning rate evolution')\n    ax.set_ylabel('Learning rate')\n    ax.set_xlabel('Epochs')\n    ax.legend(['Train', 'Test'], loc='upper left')","3d397b8f":"plot_lr(history)","ec458895":"# Code inspire from : https:\/\/gist.github.com\/oeway\/f0ed87d3df671b351b533108bf4d9d5d\ndef plot_conv_weights(model, layer):\n    W = model.get_layer(name=layer).get_weights()[0]\n    if len(W.shape) == 4:\n        W = np.squeeze(W)\n        W = W.reshape((W.shape[0], W.shape[1], W.shape[2]*W.shape[3])) \n        fig, axs = plt.subplots(5,5, figsize=(9,8))\n        fig.subplots_adjust(hspace = .5, wspace=.001)\n        axs = axs.ravel()\n        for i in range(25):\n            im = axs[i].imshow(W[:,:,i], cmap=plt.cm.get_cmap('Blues', 6))\n            axs[i].set_title(str(i))\n            \n            # Hide grid\n            axs[i].grid(False)\n            axs[i].axis('off')\n            \n    fig.subplots_adjust(right=0.8)\n    cbar_ax = fig.add_axes([0.85, 0.15, 0.03, 0.7])\n    fig.colorbar(im, cax=cbar_ax)","39bc3297":"plot_conv_weights(model, \"first_conv\")","77c68668":"plot_conv_weights(model, \"second_conv\")","1e1ba7b9":"def predict(model, sample_submission):\n    pred = np.empty((sample_submission.shape[0],))\n    for n in tqdm(range(sample_submission.shape[0])):\n        data = np.array(Image.open(TEST_DIR + sample_submission.id[n]))\n        pred[n] = model.predict(data.reshape((1, 32, 32, 3)))[0][1]\n    \n    sample_submission['has_cactus'] = pred\n    return sample_submission","6b607d11":"sample_submission = pd.read_csv(MAIN_DIR + 'sample_submission.csv')\ndf_prediction = predict(model, sample_submission)\n\ndf_prediction.to_csv('submission.csv', index=False)","106aa3ea":"df_prediction.head()","275b14f0":"### Learning curves <a id=\"learning_curves\"><\/a>","f726a55b":"# Data exploration <a id=\"data_exploration\"><\/a>","05d463d4":"## Training <a id=\"training\"><\/a>","14414269":"# Prediction <a id=\"prediction\"><\/a>","8da8ac6b":"# Modelisation <a id=\"modelisation\"><\/a>","d4b7256a":"### Weights <a id=\"weights\"><\/a>","d042a762":"## Results <a id=\"results\"><\/a>","212b89ee":"# Informations <a id=\"informations\"><\/a>","58c788dd":"## Datasets (CSV file) <a id=\"datasets\"><\/a>","5609e8dc":"## Pictures <a id=\"pictures\"><\/a>","df7fe9c9":"## Aerial cactus identification with Keras CNN","3aa7484c":"# Table of contents","265c587a":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/13435\/logos\/header.png?t=2019-03-07-17-24-10\"><\/img>","aaaf8d99":"# Context <a id=\"context\"><\/a>","ac531f89":"### Learning rate <a id=\"learning_rate\"><\/a>","101fc1e6":"# Importations <a id=\"importations\"><\/a>","faa4fb5c":"# Set parameters <a id=\"set_parameters\"><\/a>","8e3814f9":"* [Context](#context)\n* [Importations](#importations)\n* [Informations](#informations)\n* [Set parameters](#set_parameters)\n* [Data exploration](#data_exploration)\n    * [Pictures](#pictures)\n    * [Datasets (CSV files)](#datasets)\n* [Modelisation](#modelisation)\n    * [Training](#training)\n    * [Results](#results)\n        * [Learning curves](#learning_curves)\n        * [Learning rate](#learning_rate)\n        * [Weights](#weights)\n* [Prediction](#prediction)","759486db":"<p style=\"text-align:justify;\">To assess the impact of climate change on Earth's flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the VIGIA project, which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas.<\/p>\n\n<p style=\"text-align:justify;\">**In this competition, we are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery.**<\/p>"}}