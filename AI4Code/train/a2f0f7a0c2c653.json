{"cell_type":{"291d4f79":"code","20034529":"code","724b0d38":"code","e700c295":"code","5d641732":"code","4b9d011a":"code","ecac1369":"code","e7ff8d94":"code","df0d4983":"code","09a01132":"code","1440f9b6":"code","096e91fd":"code","77eaaabf":"code","cfaa4a8a":"code","fb93d395":"code","3bb62f06":"code","b0738349":"code","0d895b30":"code","4e75e3d3":"code","1c146cf3":"code","03b6c68a":"code","8ddccc6e":"code","8fcd78b7":"markdown"},"source":{"291d4f79":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tensorflow.python.keras import models, layers, optimizers\nimport tensorflow\nfrom tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport bz2\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nimport re\n\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom keras import metrics\n\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))","20034529":"!python -m spacy download fr_core_news_lg","724b0d38":"train_data = pd.read_json('..\/input\/rightinnov\/dataset_fr_train.json', lines=True)\n\ntest_data = pd.read_json('..\/input\/rightinnov\/dataset_fr_test.json', lines=True)\n\nvalid_data= pd.read_json('..\/input\/rightinnov\/dataset_fr_dev.json', lines=True)","e700c295":"# convert starts into sentiments\n\nsentiments_dict = {1: 0,\n            2: 0,\n            3: 1,    \n            4: 2,\n            5: 2}\n\ndef stars_to_sentiment(dataset):\n    dataset['sentiment'] = dataset['stars'].map(sentiments_dict)\n    dataset = dataset[['review_body','sentiment']]\n    return dataset\n\ndf_train = stars_to_sentiment(train_data)\ndf_test = stars_to_sentiment(test_data)\ndf_valid = stars_to_sentiment(valid_data)","5d641732":"train_labels, train_texts = df_train['sentiment'].values.tolist(), df_train['review_body'].values.tolist()\ntest_labels, test_texts = df_test['sentiment'].values.tolist(), df_test['review_body'].values.tolist()\nval_labels, val_texts = df_valid['sentiment'].values.tolist(), df_valid['review_body'].values.tolist()","4b9d011a":"import re\nNON_ALPHANUM = re.compile(r'[\\W]')\nNON_ASCII = re.compile(r'[^a-z0-1\\s]')\ndef normalize_texts(texts):\n    normalized_texts = []\n    for text in texts:\n        lower = text.lower()\n        no_punctuation = NON_ALPHANUM.sub(r' ', lower)\n        no_non_ascii = NON_ASCII.sub(r'', no_punctuation)\n        normalized_texts.append(no_non_ascii)\n    return normalized_texts\n        \ntrain_texts = normalize_texts(train_texts)\ntest_texts = normalize_texts(test_texts)\nval_texts = normalize_texts(val_texts)","ecac1369":"# convert labels to one-hot encoding\n\nfrom keras.utils import to_categorical\nfrom numpy import array\n\ntrain_labels = to_categorical(array(train_labels))\ntest_labels = to_categorical(array(test_labels))\nval_labels = to_categorical(array(val_labels))","e7ff8d94":"MAX_FEATURES = 9000\ntokenizer = Tokenizer(num_words=MAX_FEATURES)\ntokenizer.fit_on_texts(train_texts)\ntrain_texts = tokenizer.texts_to_sequences(train_texts)\nval_texts = tokenizer.texts_to_sequences(val_texts)\ntest_texts = tokenizer.texts_to_sequences(test_texts)","df0d4983":"# Adding 1 because of reserved 0 index\nvocab_size = len(tokenizer.word_index) + 1\n\nmaxlen = 100\n\ntrain_texts = pad_sequences(train_texts, padding='post', maxlen=maxlen)\ntest_texts = pad_sequences(test_texts, padding='post', maxlen=maxlen)\nval_texts = pad_sequences(val_texts, padding='post', maxlen=maxlen)","09a01132":"# preparing the embedding dictionnary\n\nimport fr_core_news_lg\nnlp = fr_core_news_lg.load()","1440f9b6":"embedding_matrix = np.zeros((vocab_size, 300))\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = nlp(word).vector\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","096e91fd":"def create_model(dropout=0.4, gru_nparams=128, dense_nparams=64, activation='relu', optimizer='adam'):\n    sequences = layers.Input(shape=(maxlen,))\n    embedded = layers.Embedding(vocab_size,\n                            300,\n                            weights=[embedding_matrix],\n                            input_length=maxlen,\n                            trainable=False)(sequences)\n    x = layers.SpatialDropout1D(dropout)(embedded)\n    #x = layers.CuDNNGRU(gru_nparams)(x)\n    x = layers.GRU(gru_nparams)(x)\n    x = layers.Dense(dense_nparams, activation=activation)(x)\n    predictions = layers.Dense(3, activation='softmax')(x)\n    model = models.Model(inputs=sequences, outputs=predictions)\n    model.compile(\n        optimizer=optimizer,\n        loss='categorical_crossentropy',\n        metrics=[\n            'categorical_accuracy', \n            metrics.Precision(), \n            metrics.Recall()]\n    )\n    return model","77eaaabf":"kears_estimator = KerasClassifier(build_fn=create_model, verbose=1)","cfaa4a8a":"# using SMOTE for unballanced class\nfrom imblearn.over_sampling import SMOTE","fb93d395":"from imblearn.pipeline import Pipeline as PP\n\nestimator = PP([\n    ('oversample', SMOTE(random_state=12)),\n    ('kc', kears_estimator)\n    ])","3bb62f06":"# define the grid search parameters\nparam_grid = {\n    'kc__epochs': [10, 100],\n    'kc__batch_size':[16, 64, 128],\n    'kc__dense_nparams': [32, 128, 256, 512],\n    'kc__gru_nparams' : [64, 128, 192],\n    'kc__optimizer':['SGD', 'RMSprop', 'Adam'],\n    'kc__dropout': [0.4, 0.3, 0.2, 0.1, 0],\n    'kc__activation': ['softmax', 'relu', 'sigmoid']\n}","b0738349":"from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard","0d895b30":"my_callbacks = [\n    EarlyStopping(monitor='val_loss', min_delta=0, \n                         patience=5, verbose=1, mode='auto',\n                         baseline=None, restore_best_weights=True),\n    ModelCheckpoint(filepath='.\/model.{epoch:02d}-{val_accuracy:.2f}.h5', \n                   monitor='val_accuracy', verbose=1, save_best_only=True, mode='max'),\n    TensorBoard(log_dir='.\/logs'),\n]","4e75e3d3":"kfold_splits = 5\n\ngrid = RandomizedSearchCV(estimator,\n                              param_grid,\n                              cv=kfold_splits, \n                              verbose=3,\n                              n_jobs=-1,random_state=1)","1c146cf3":"grid_result = grid.fit(np.array(train_texts), np.array(train_labels), \n                       kc__callbacks=my_callbacks)","03b6c68a":"# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","8ddccc6e":"# save the best model\n\nbest_model = grid_result.best_estimator_.model\nmodel2json = best_model.to_json()\nwith open('.\/best_model.json', 'w') as json_file:\n    json_file.write(model2json)\nbest_model.save_weights('.\/best_model.h5')","8fcd78b7":"# Parameters Tuning"}}