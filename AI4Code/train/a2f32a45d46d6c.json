{"cell_type":{"48d70157":"code","fab9e995":"code","b22bd62b":"code","5ae5061b":"code","1eb1fcff":"code","1f1c15a4":"code","6075b66f":"code","8761db24":"code","6ca7264f":"code","f589d901":"code","c1dca2ed":"code","e41185e4":"markdown","f3a2d2d1":"markdown","18887138":"markdown","41eb00ff":"markdown"},"source":{"48d70157":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport soundfile as sf\nimport os, random, librosa\nfrom tqdm.auto import tqdm\nfrom IPython import display\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, utils, callbacks\nfrom keras.preprocessing.image import ImageDataGenerator\nsns.set_style('darkgrid')","fab9e995":"class config:\n    CLASSES = np.array(['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine'])\n    MAX_EPOCHS = 500","b22bd62b":"def read_and_pad_sound_wave(filepath, audio_length=16000):\n    waveform, sr = sf.read(filepath)\n    zero_padding = np.zeros([audio_length - len(waveform)])\n    waveform = np.concatenate([waveform, zero_padding], 0)\n    return waveform, sr\n\ndef sound_wave_to_mel_spectrogram(sound_wave, sample_rate, spec_h=128, spec_w=128, length=1):\n    NUM_MELS = spec_h\n    HOP_LENGTH = int(sample_rate * length \/ (spec_w - 1)) \n    mel_spec = librosa.feature.melspectrogram(y=sound_wave, sr=sample_rate, hop_length=HOP_LENGTH, n_mels=NUM_MELS)\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    return mel_spec_db\n\nclass_distribution = []\nspectrogram_samples = []\naudio_samples = []\n\nROOT = '..\/input\/speech-commands-classification-dataset'\nDATA = 'mel_spectrograms'\nif not os.path.exists(DATA):\n    os.mkdir(DATA)\n      \nfor label in tqdm(config.CLASSES):\n    src_dir = os.path.join(ROOT, label)\n    dst_dir = os.path.join(DATA, label)\n    if not os.path.exists(dst_dir):\n        os.mkdir(dst_dir)\n        \n    for i, filename in enumerate(os.listdir(src_dir)):\n        class_distribution.append(label)\n        src_path = os.path.join(src_dir, filename)\n        dst_path = f\"{os.path.join(dst_dir, filename[:-4])}.png\"\n        plt.imsave(dst_path, sound_wave_to_mel_spectrogram(*read_and_pad_sound_wave(src_path)), cmap='gray')\n        if i == 0:\n            spectrogram_samples.append(dst_path)\n            audio_samples.append(src_path)\nprint('done')            ","5ae5061b":"fig, ax = plt.subplots(figsize=(20, 6))\npd.Series(class_distribution).value_counts().plot.bar()\nfig.suptitle('Class Distribution', fontsize=24);","1eb1fcff":"for i in range(10):\n    display.display(display.Audio(read_and_pad_sound_wave(audio_samples[i])[0], rate=16000))","1f1c15a4":"for i in range(10):\n    fig, ax = plt.subplots(1,2, figsize=(20, 4))\n    ax[0].plot(read_and_pad_sound_wave(audio_samples[i])[0])\n    ax[1].imshow(plt.imread(spectrogram_samples[i]), cmap='gray')\n    fig.suptitle(config.CLASSES[i], fontsize=15);","6075b66f":"image_size = (128, 128, 1)\ndatagen = ImageDataGenerator(validation_split=0.1)\n\ntraining_set = datagen.flow_from_directory(\n    DATA, target_size=image_size[:2],  batch_size=32, class_mode='categorical', color_mode='grayscale', subset='training'\n)\nvalidation_set = datagen.flow_from_directory(\n    DATA, target_size=image_size[:2],  batch_size=32, class_mode='categorical', color_mode='grayscale', subset='validation'\n)","8761db24":"model = models.Sequential([\n    layers.Conv2D(32, 3, activation='relu', input_shape=image_size, padding='same'),\n    layers.Conv2D(32, 3, activation='relu', padding='same'),\n    layers.MaxPooling2D(padding='same'),\n    layers.Dropout(0.25),\n    layers.Conv2D(64, 3, activation='relu', padding='same'),\n    layers.Conv2D(64, 3, activation='relu', padding='same'),\n    layers.MaxPooling2D(padding='same'),\n    layers.Dropout(0.25),\n    layers.GlobalAveragePooling2D(),\n    layers.Dense(len(config.CLASSES), activation='softmax'),\n])\n\nmodel.summary()\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)","6ca7264f":"utils.plot_model(model, show_shapes=True, expand_nested=True)","f589d901":"es = callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n\nrlp = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=1e-10, mode='min', verbose=1)\n\nhistory = model.fit(\n    training_set, validation_data=validation_set,\n    epochs=config.MAX_EPOCHS, callbacks=[es, rlp], batch_size=8\n)","c1dca2ed":"fig, ax = plt.subplots(2, 1, figsize=(20, 8))\ndf = pd.DataFrame(history.history)\ndf[['accuracy', 'val_accuracy']].plot(ax=ax[0])\ndf[['loss', 'val_loss']].plot(ax=ax[1])\nax[0].set_title('Model Accuracy', fontsize=15)\nax[1].set_title('Model Loss', fontsize=15)\nfig.suptitle('Learning Curve', fontsize=24);","e41185e4":"# Data Preparation\nThese audio files are 1 sec long and have a sample rate 16000 ie the audio is sampled 16000 times per second. `read_and_pad_sound_wave` function readds the sound file and ensures the length of the file is consistent (1sec in this case) and returns the signal along with its sample rate\n\nWe'll convert the waveform into a mel spectrogram, which shows frequency changes over time and can be represented as a 2D image, which will be used as an input to the CNN, `sound_wave_to_mel_spectrogram` converts an audio into a mel spectrogram of the required dimesnsions\n","f3a2d2d1":"# Modellling","18887138":"# Sample Audio","41eb00ff":"# Data Generator"}}