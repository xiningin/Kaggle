{"cell_type":{"d2914f77":"code","0fb36eea":"code","728fced0":"code","729060a2":"code","4408b0df":"code","0444d8ec":"code","354e662d":"code","7a02d6c3":"code","0550ba15":"code","46064bac":"code","088d8ae0":"code","ee4504c9":"code","64b33219":"code","64145e49":"code","de55bb38":"code","d45b2644":"code","67b9a6b5":"code","a27fb210":"code","030d1d6b":"code","24c81485":"code","ffacd8df":"code","ddc8147c":"code","56e041a7":"code","91db9f7b":"code","e31c6d93":"code","1437d6ab":"code","b6b99dbb":"code","e52b1d38":"code","27bc864b":"code","7f9aea95":"code","187789da":"code","76076717":"code","c865b0f9":"code","38ccac48":"code","2e81b035":"code","2368134d":"code","83760259":"code","f1a7fc39":"code","edfeb317":"code","34248999":"code","68589d77":"code","109db707":"code","87361fb5":"code","1eb3790c":"code","ae856cf4":"markdown","25eda056":"markdown","4bf2adc4":"markdown","d86afa56":"markdown","c0c50201":"markdown","9367f07e":"markdown","8298cda9":"markdown","8cfb8514":"markdown"},"source":{"d2914f77":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt \n#import seqlearn","0fb36eea":"## global pyplot settings -- can be changed for a specific instance\nplt.rcParams['figure.figsize'] = (8, 5.28) \nplt.rcParams[\"axes.labelsize\"] = 12\nplt.rcParams[\"axes.titlesize\"] = 13\nplt.rcParams[\"axes.titleweight\"] = 600\n\nsns.set_color_codes(\"pastel\")","728fced0":"## reading in files\nmon = pd.read_csv(\"\/kaggle\/input\/cicids2017latest-from-unbca-site\/Monday-WorkingHours.pcap_ISCX.csv\", encoding=\"utf-8\")\ntues = pd.read_csv(\"\/kaggle\/input\/cicids2017latest-from-unbca-site\/Tuesday-WorkingHours.pcap_ISCX.csv\", encoding=\"utf-8\")\nwed = pd.read_csv(\"\/kaggle\/input\/cicids2017latest-from-unbca-site\/Wednesday-workingHours.pcap_ISCX.csv\", encoding=\"utf-8\")\nthur_web = pd.read_csv(\"\/kaggle\/input\/cicids2017latest-from-unbca-site\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\", encoding=\"utf-8\")\nthur_inf = pd.read_csv(\"\/kaggle\/input\/cicids2017latest-from-unbca-site\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\", encoding=\"utf-8\")\nfrid_ddos = pd.read_csv(\"\/kaggle\/input\/cicids2017latest-from-unbca-site\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\", encoding=\"utf-8\")\nfrid_port = pd.read_csv(\"\/kaggle\/input\/cicids2017latest-from-unbca-site\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\", encoding=\"utf-8\")\nfrid_morn = pd.read_csv(\"\/kaggle\/input\/cicids2017latest-from-unbca-site\/Friday-WorkingHours-Morning.pcap_ISCX.csv\", encoding=\"utf-8\")","729060a2":"# Prezentacja danych z ka\u017cdego dnia \nprint(\"Rozmiar ruchu z poniedzia\u0142ku to {} wierszy i {} kolumn\".format(mon.shape[0],mon.shape[1]))\nprint(\"Rozmiar ruchu z wtorku to {} wierszy i {} kolumn\".format(tues.shape[0],tues.shape[1]))\nprint(\"Rozmiar ruchu ze \u015brody to {} wierszy i {} kolumn\".format(wed.shape[0],wed.shape[1]))\nprint(\"Rozmiar ruchu z czwartku to {} wierszy i {} kolumn\".format(thur_web.shape[0] + thur_inf.shape[0],thur_web.shape[1]))\nprint(\"Rozmiar ruchu z pi\u0105tku to {} wierszy i {} kolumn\".format(frid_ddos.shape[0] + frid_port.shape[0] + frid_morn.shape[0],frid_ddos.shape[1]))","4408b0df":"### all labels are of numerical type, despite Label column\nmon.info()  # same format applies to other files","0444d8ec":"# searching for NA values - empty \"\" are not considered NA - but that is of no concern in this dataset \npd.options.mode.use_inf_as_na = True ## so that inf is also treated as NA value\nprint(\"mon NA values\")\nprint(mon.loc[:, mon.isnull().any()].isnull().sum())\nprint(\"=====================\")\nprint(\"tues NA values\")\nprint(tues.loc[:, tues.isnull().any()].isnull().sum())\nprint(\"=====================\")\nprint(\"wed NA values\")\nprint(wed.loc[:, wed.isnull().any()].isnull().sum())\nprint(\"=====================\")\nprint(\"thur NA values\")\nprint(thur_web.loc[:, thur_web.isnull().any()].isnull().sum())\nprint(\"=====================\")\nprint(thur_inf.loc[:, thur_inf.isnull().any()].isnull().sum())\nprint(\"=====================\")\nprint(\"Friday NA values\")\nprint(frid_ddos.loc[:, frid_ddos.isnull().any()].isnull().sum())\nprint(\"=====================\")\nprint(frid_port.loc[:, frid_port.isnull().any()].isnull().sum())\nprint(\"=====================\")\nprint(frid_morn.loc[:, frid_morn.isnull().any()].isnull().sum())","354e662d":"### NA values are minority of the dataset. Because they cannot be processed by algorithms, they can be easily discarded \nmon = mon.dropna()\ntues = tues.dropna()\nwed = wed.dropna()\nthur_web = thur_web.dropna()\nthur_inf = thur_inf.dropna()\nfrid_ddos = frid_ddos.dropna()\nfrid_port = frid_port.dropna()\nfrid_morn = frid_morn.dropna()","7a02d6c3":"### check the current size after dropping rows containing NA values\nprint(\"Rozmiar ruchu po usuni\u0119ciu NA z poniedzia\u0142ku to {} wierszy i {} kolumn\".format(mon.shape[0],mon.shape[1]))\nprint(\"Rozmiar ruchu po usuni\u0119ciu NA z wtorku to {} wierszy i {} kolumn\".format(tues.shape[0],tues.shape[1]))\nprint(\"Rozmiar ruchu po usuni\u0119ciu NA ze \u015brody to {} wierszy i {} kolumn\".format(wed.shape[0],wed.shape[1]))\nprint(\"Rozmiar ruchu po usuni\u0119ciu NA z czwartku to {} wierszy i {} kolumn\".format(thur_web.shape[0] + thur_inf.shape[0],thur_web.shape[1]))\nprint(\"Rozmiar ruchu po usuni\u0119ciu NA z pi\u0105tku to {} wierszy i {} kolumn\".format(frid_ddos.shape[0] + frid_port.shape[0] + frid_morn.shape[0],frid_ddos.shape[1]))","0550ba15":"## join all data into one DataFrame\nall_data = pd.concat([mon, tues, wed, thur_web, thur_inf, frid_ddos, frid_port, frid_morn], ignore_index=True)\n\n#all_data","46064bac":"## feature selection phase - find cols w\/ all zero values \n\n#all_datav2.describe().iloc[1:].apply(np.sum, index=[])\ndescribe_info = all_data.describe()\nall_zeroes_cols = describe_info.loc[:,(describe_info.iloc[1:] == 0).all()]\nall_zeroes_cols","088d8ae0":"### feature selection phase - lets's cut out features that are reduntant - all zero values \n## removing [8] features from previous cell - they are reduntant \n\nall_data.drop(columns=all_zeroes_cols, inplace=True)\n\nall_data.shape # check if resulting DataFrame valid","ee4504c9":"### just rename Labels that contain non-printable characters \nprint(\"Before...\")\nprint(all_data.loc[:,\" Label\"].unique())\n\nall_data.loc[:,\" Label\"].replace({\"Web Attack \ufffd XSS\" : \"XSS\", \"Web Attack \ufffd Sql Injection\": \"Sql Injection\", \"Web Attack \ufffd Brute Force\": \"Brute Force\"}, inplace=True)\nprint(\"After..\")\nprint(all_data.loc[:,\" Label\"].unique())\n\n## remove trailing && leading spaces from all the labels\nrename_cols = lambda col_lbl: col_lbl.strip()\nall_data.rename(rename_cols, axis=1, inplace=True, errors=\"raise\")","64b33219":"## temporarily add new column to distinguish traffic type between Normal \/ Attack \ntrf_type = all_data.loc[:, \"Label\"].map(lambda lbl: \"Normalny\" if lbl == \"BENIGN\" else \"Atak\")\ntrf_type.name = \"traffic type\"\nall_data.loc[:, trf_type.name] = trf_type","64145e49":"## check if column correctly added to the DataFrame\n\n#all_data\nprint(all_data.shape)","de55bb38":"### plot distrtibution of Normal traffic and Attacks in the whole dataset \n\n#from matplotlib.ticker import StrMethodFormatter, MultipleLocator\n#fmt = StrMethodFormatter(\"{x:,}\")  #below lines are just another approach on formatting yaxis\n#ax = plt.gca()\n#locator = MultipleLocator(1*10**5)\n#ax.yaxis.set_major_locator(locator)\n#ax.yaxis.set_major_formatter(fmt)\nplt.ticklabel_format(axis='y', useMathText=True, useOffset=False)  # change def ScalarFormatter\n\nsns.countplot(x=\"traffic type\", data=all_data, palette=[\"g\",\"r\"])\n#plt.title(\"Traffic type distribution in whole dataset\")\nplt.title(\"Rozk\u0142ad ruchu w ca\u0142ym zbiorze\")\nplt.xlabel(\"typ ruchu\")\nplt.ylabel(\"liczba pr\u00f3bek\")\nplt.savefig(\"distribution1.png\", dpi=200, format='png')\nplt.show()","d45b2644":"### Normal traffic greatly outweights Evil traffic. To avoid for the model to be biased towards one type of traffic, it has to be downsampled\nall_data.loc[:, \"traffic type\"].value_counts()\n\n## DOWNSAMPLING\nfrom imblearn.under_sampling import RandomUnderSampler\n\nrus = RandomUnderSampler(random_state=10, sampling_strategy=0.85) # equals traffic unless ratio is specified\n\n\nall_data.drop([\"traffic type\"], axis=1, inplace=True) # temporarily rm the categorical column for underSampling\n\nall_data_res, trf_type_res = rus.fit_sample(all_data, trf_type)\n\n#all_datav2 = pd.concat([all_data_res, trf_type_res], join=\"inner\")  # another way to join dataframe w\/ series\nall_datav2 = all_data_res.join(trf_type_res, how=\"inner\")\n\nall_datav2.shape # check if resulting DaraFrame valid\n","67b9a6b5":"### show distribution chart after downsampling huge Normal traffic \n\nplt.ticklabel_format(axis='y', useMathText=True, useOffset=False)  # change def ScalarFormatter\nsns.countplot(x=\"traffic type\", data=all_datav2, order=[\"Normalny\", \"Atak\"],  palette=[\"g\",\"r\"])\n\n#plt.title(\"Traffic type distribution in whole dataset after random downsampling\")\nplt.title(\"Rozk\u0142ad ruchu w ca\u0142ym zbiorze po pr\u00f3bkowaniu w d\u00f3\u0142\")\nplt.xlabel(\"typ ruchu\")\nplt.ylabel(\"liczba pr\u00f3bek\")\nplt.savefig(\"distribution2.png\", dpi=200, format='png')\nplt.show()","a27fb210":"## TESTING ONLY CELL \n\n#multi_trf_type = all_datav2[\"Label\"]\n#all_datav2.info()\n#all_datav2_cp.drop([\"Label\", \"traffic type\"], axis=1, inplace=True)\n# def _assert_all_finite(X):\n#     \"\"\"Like assert_all_finite, but only for ndarray.\"\"\"\n#     X = np.asanyarray(X)\n#     # First try an O(n) time, O(1) space solution for the common case that\n#     # everything is finite; fall back to O(n) space np.isfinite to prevent\n#     # false positives from overflow in sum method.\n#     if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n#             and not np.isfinite(X).all()):\n#         raise ValueError(\"Input contains NaN, infinity\"\n#                          \" or a value too large for %r.\" % X.dtype)\n# _assert_all_finite(all_datav2_cp) ","030d1d6b":"## feature selection phase - prepare data for RFC\nlbls = all_datav2.loc[:, \"Label\"]\ndata_w_o_cat_attrs = all_datav2.iloc[:, :-2]","24c81485":"data_w_o_cat_attrs.info()","ffacd8df":"## feature selection phase - let's use RFC on our data\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(random_state=10, n_jobs=-1) # 100 trees in forest\n\n# fit random forest classifier on the dataset\nrfc.fit(data_w_o_cat_attrs, lbls)\n","ddc8147c":"## plotting features' importance in the whole dataset graph \n# extract important features\nscore = np.round(rfc.feature_importances_,5)\nimportances = pd.DataFrame({'cechy':data_w_o_cat_attrs.columns,'poziom wa\u017cno\u015bci':score})\nimportances = importances.sort_values('poziom wa\u017cno\u015bci',ascending=False).set_index('cechy')\n\n# plot importances\nsns.barplot(x=importances.index, y=\"poziom wa\u017cno\u015bci\", data=importances, color=\"b\")\nplt.xticks(rotation=\"vertical\")\nplt.gcf().set_size_inches(14,5)\nplt.savefig(\"importances.png\", dpi=200, format='png', bbox_inches = \"tight\", pad_inches=0.2)\nplt.show()\n\n#importances.plot.bar()  # alt way to achieve similar plot","56e041a7":"## Let's leave only the most important features \nthreshold = 0.001 # importance threshold\n\n#bl_thresh = importances.loc[importances[\"importance\"] < threshold]\nbl_thresh = importances.loc[importances[\"poziom wa\u017cno\u015bci\"] < threshold]\nprint(\"there are {} features to delete, as they are below chosen threshold\".format(bl_thresh.shape[0]))\nprint(\"these features are the following:\")\nfeats_to_del = [feat for feat in bl_thresh.index]\nprint(\"\\n\".join(feats_to_del))\n\n## removing these not important features \nall_datav2.drop(columns=feats_to_del, inplace=True) # dropping columns\n","91db9f7b":"## check if DataFrame still valid \nall_datav2","e31c6d93":"## feature selection phase - let's find highly correlated feature pairs \nall_data_corr_mtrx = all_datav2.corr()","1437d6ab":"## plotting a correlation heatmap \n\nplt.gcf().set_size_inches(60, 60)\nhm = sns.heatmap(all_data_corr_mtrx, annot=True, linewidths=.8, annot_kws={\"fontsize\": 15}, fmt=\".2f\")\nhm.set_yticklabels(hm.get_ymajorticklabels(), fontsize = 25)\nhm.set_xticklabels(hm.get_xmajorticklabels(), fontsize = 25)\nplt.savefig(\"corr_heatmap.png\", dpi=200, format='png', bbox_inches = \"tight\", pad_inches=0.4)\nplt.show()","b6b99dbb":"# check matrix\nall_data_corr_mtrx","e52b1d38":"## process correlation matrix, list highly correlated feature pairs \ndef srt_corr(mtrx):\n    corr_ustack = mtrx.unstack().abs()\n    #corr_ustack\n    corr_srted = corr_ustack.sort_values(ascending=False)\n    return corr_srted\nsrt_corr(all_data_corr_mtrx)","27bc864b":"from collections import OrderedDict\nthres_corr = 0.95\nepoch=0\nhighly_corr = {\"dummy\": \"dummy\"}\nfeats_deled = []\n\nall_data_corr_mtrx2 = all_data_corr_mtrx.copy()  # for easy\ndef add_to_dct(l, ft, ft2):\n    try:\n        l[ft].append(ft2)\n    except KeyError:\n        l[ft] = [ft2]\n\nget_imp = lambda feat: importances.loc[feat][0]\nsrt_key = lambda elem: get_imp(elem[0])  # gets imp of first elem\n\ndef what_to_del(dct_srt):    \n    to_del = []  #least imp feature\n    for k, val in dct_srt.items():\n        ## get all indexes lower than current k\n        feats_lw_imp = importances[importances.index.slice_indexer(k)].index\n        if set(val) - set(feats_lw_imp):  # feat k creates a corr pair w\/ feature of higher importance --- delete feat k\n            if k not in to_del: to_del.append(k)\n        else:  # feat k creates a corr pair w\/ features of lower importnace --- delete one w\/ lowest imp \n            for ft in feats_lw_imp[::-1]:  # searching from least important\n                if ft in val and ft not in to_del:\n                    to_del.append(ft)\n                    break  ## deleting first founud feat of lowest possible importance\n    return to_del\n        \n            \nwhile highly_corr:\n    count = 0\n    highly_corr.clear()\n    for feats, val in srt_corr(all_data_corr_mtrx2).iteritems():\n        if val > thres_corr and feats[0] != feats[1]:\n            count += 1\n            add_to_dct(highly_corr, feats[0], feats[1])\n    if not highly_corr: break  # no more highly corr pairs\n    highly_corr_srt = OrderedDict(sorted(highly_corr.items(), key=srt_key))  # sorted based on imp\n\n    to_del = what_to_del(highly_corr_srt)\n    feats_deled += to_del\n    epoch +=1 # first epoch will be 1 not 0! \n    print(\"there are {} higly correlated pairs in {} iteration\".format(count, epoch))\n    all_data_corr_mtrx2.drop(to_del, axis=1, inplace=True)\n    all_data_corr_mtrx2.drop(to_del, axis=0, inplace=True)  # need to remove the feat from both cols and index\n\nprint(\"deleting: {} feature\".format(len(feats_deled)))\nprint(\"finally deleted:\\n\"+ \"\\n\".join(feats_deled))","7f9aea95":"## plotting a correlation heatmap after removing highly correlated pairs\n\nplt.gcf().set_size_inches(40, 40)\nhm2 = sns.heatmap(all_data_corr_mtrx2, annot=True, linewidths=.8, annot_kws={\"fontsize\": 15}, fmt=\".2f\")\nhm2.set_yticklabels(hm.get_ymajorticklabels(), fontsize = 20)\nhm2.set_xticklabels(hm.get_xmajorticklabels(), fontsize = 20)\nplt.savefig(\"corr_heatmap2.png\", dpi=200, format='png', bbox_inches = \"tight\", pad_inches=0.4)\nplt.show()","187789da":"all_datav3 = all_datav2.copy() ## just for ease of cells executing \n\nall_datav3.drop(feats_deled, axis=1, inplace=True)\n\nall_datav3_cp = all_datav3.copy()\n\nall_datav3 # just check","76076717":"## scaling numerical data using Quantile scaling\nfrom sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(random_state=10) # number of quantiles can be set, default n_quantiles=1000\n\natt_type = all_datav3.loc[:, \"Label\"]\nbin_trff_type = all_datav3.loc[:, \"traffic type\"]\nall_datav3.drop([\"Label\", \"traffic type\"], axis=1, inplace=True) ## drop categorical columns\nall_data_scled = qt.fit_transform(all_datav3)\n\nall_data_scled ## check    ","c865b0f9":"### splitting dataset into training and test set \nfrom sklearn.model_selection import train_test_split\n\ntrain_data, test_data, train_lbl, test_lbl  = train_test_split(all_data_scled, att_type, random_state=10, train_size=0.7)\n## additional held-out validation set for evaluating neural nets predicting on upsampled training set --- the validation set need to be split b4 upsampling\nneural_train_data, neural_validation, neural_train_lbl, neural_validation_lbl = train_test_split(train_data, train_lbl, random_state=10, train_size=0.8) ## will be shuffled in the same order as train_data above\n\ntrain_bin_trff_lbl = train_lbl.map(lambda lbl: \"Normalny\" if lbl == \"BENIGN\" else \"Atak\") \nneural_train_bin_trff_lbl = neural_train_lbl.map(lambda lbl: \"Normalny\" if lbl == \"BENIGN\" else \"Atak\") ## train_lbl for upsampled neural nets\ntest_bin_trff_lbl = test_lbl.map(lambda lbl: \"Normalny\" if lbl == \"BENIGN\" else \"Atak\") \nneural_validation_bin_trff_lbl = neural_validation_lbl.map(lambda lbl: \"Normalny\" if lbl == \"BENIGN\" else \"Atak\") ## validation lbl for upsampled neural nets","38ccac48":"## check the current distribution of each traffic type in training set\na = train_lbl.value_counts()\nall_samples = a.sum()\nprint(a)\nprint(\"Total: {}\".format(all_samples))\n## TODO ZROBI\u0106 Z TEGO WYKRESY dla zbioru treningowego tylko","2e81b035":"## traffic type distribution in training set before oversampling\norder = a.index\npalette = {}\nfor key in order:\n    palette[key] = \"g\" if key == \"BENIGN\" else \"r\"\nax = sns.countplot(x=train_lbl, order=order, palette=palette)\nplt.xticks(rotation=\"vertical\")\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x(), p.get_height()))\n    \nplt.title(\"Rozk\u0142ad ruchu w zbiorze treningowym przed skalowaniem w g\u00f3r\u0119\")\nplt.xlabel(\"typ ruchu\")\nplt.ylabel(\"liczba pr\u00f3bek\")\nplt.savefig(\"distribution_up1.png\", dpi=200, format='png', bbox_inches = \"tight\")\nplt.show()","2368134d":"## TEST CELL ONLY\ntrain_lbl.value_counts().loc[\"BENIGN\"]","83760259":"## The distribution of each category is very imbalanced -- usign OverSampling to even the distribution\n## oversampling only the training set so that not to leak information (important patterns) to the test dataset\nfrom imblearn.over_sampling import SMOTE\nfrom math import ceil\nmin_thresh = 0.005  # it is a percent of the whole traffic after underSampling\n\nglob_cls_distr = None\ndef over_sample_new(y):\n    global glob_cls_distr\n    cls_distr = {}\n    for trf_cls in np.unique(y):\n        curr_size = a.loc[trf_cls]  # global a == train_lbl.value_counts()\n        if (curr_size \/ all_samples) < min_thresh:\n            cls_distr[trf_cls] = ceil(min_thresh * all_samples)\n        else:\n            cls_distr[trf_cls] = curr_size\n    print(\"class distribution after over sampling:\")\n    glob_cls_distr = cls_distr\n    print(glob_cls_distr)\n    return cls_distr\n\ndef over_sample_bin(dct):\n    sm = 0\n    for key, val in dct.items():\n        if key != \"BENIGN\":\n            sm += val\n        else: benign = val\n    return {\"Normalny\": benign, \"Atak\": sm}\n\n#dct = {'FTP-Patator': 7935, 'SSH-Patator': 6057, 'DoS slowloris': 6057, 'DoS Slowhttptest': 6057, 'DoS Hulk': 230124, 'DoS GoldenEye': 10293, 'Heartbleed': 6057, 'Brute Force': 6057, 'XSS': 6057, 'Sql Injection': 6057, 'Infiltration': 6057, 'DDoS': 128025, 'PortScan': 158804, 'Bot': 6057, 'BENIGN': 654771}\nsmote = SMOTE(random_state=10, k_neighbors=3, sampling_strategy=over_sample_new, n_jobs=-1)  # todo can resample w\/ k_neigh only for heartbleed\n#print(glob_cls_distr)\n\nup_train_data, up_train_lbl = smote.fit_sample(train_data, train_lbl)\n\n#up_train_bin_trff_lbl = up_train_lbl.map(lambda lbl: \"Normalny\" if lbl == \"BENIGN\" else \"Atak\")  # invalid way to oversample binary traffic\n","f1a7fc39":"## same as above but done for the neural nets multiouput classification (need to held out additional validation set b4 upsampling)\nneural_thresh = 0.005  # it is a percent of the whole traffic [neural_train_data] after underSampling\n\nneural_glob_cls_distr = None\ndef over_sample_neural(y):\n    all_samples = neural_train_lbl.value_counts().sum()\n    global neural_glob_cls_distr\n    cls_distr = {}\n    for trf_cls in np.unique(y):\n        curr_size = neural_train_lbl.value_counts().loc[trf_cls]\n        if (curr_size \/ all_samples) < neural_thresh:\n            cls_distr[trf_cls] = ceil(neural_thresh * all_samples)\n        else:\n            cls_distr[trf_cls] = curr_size\n    print(\"class distribution after over sampling for neural nets:\")\n    neural_glob_cls_distr = cls_distr\n    print(neural_glob_cls_distr)\n    return cls_distr\n\nneural_smote = SMOTE(random_state=10, k_neighbors=2, sampling_strategy=over_sample_neural, n_jobs=-1)  # todo can resample w\/ k_neigh only for heartbleed\nup_neural_train_data, up_neural_train_lbl = neural_smote.fit_sample(neural_train_data, neural_train_lbl)","edfeb317":"## upsampling minority data in the case of classification between of Normal \/ Evil traffic\n## Attack samples should be equal to the sum of all attacks after upsampling in the previous step\n#ratio = get_sum(glob_cls_distr) \/ a.loc[\"BENIGN\"]  # minority class after resampling over majority class\nratio = over_sample_bin(glob_cls_distr)\n#print(ratio)\nsmote_bin = SMOTE(random_state=10, k_neighbors=3, sampling_strategy=ratio, n_jobs=-1)\nup_train_bin_data, up_train_bin_trff_lbl = smote_bin.fit_sample(train_data, train_bin_trff_lbl)","34248999":"## same as above but done for neural nets binary classification (need to keep additional validation set b4 upsampling)\nneural_ratio = over_sample_bin(neural_glob_cls_distr)\n#print(ratio)\nneural_smote_bin = SMOTE(random_state=10, k_neighbors=3, sampling_strategy=neural_ratio, n_jobs=-1)\nup_neural_train_bin_data, up_neural_train_bin_trff_lbl = neural_smote_bin.fit_sample(neural_train_data, neural_train_bin_trff_lbl)","68589d77":"## traffic type distribution in training set after oversampling\norder = ['BENIGN', 'DoS Hulk', 'PortScan', 'DDoS', 'DoS GoldenEye',\n       'FTP-Patator', 'DoS slowloris', 'SSH-Patator', 'DoS Slowhttptest',\n       'Bot', 'Brute Force', 'XSS', 'Infiltration', 'Sql Injection',\n       'Heartbleed']\npalette = {}\nfor key in order:\n    palette[key] = \"g\" if key == \"BENIGN\" else \"r\"\nax = sns.countplot(x=up_train_lbl, order=order, palette=palette)\nplt.xticks(rotation=\"vertical\")\nfor p in ax.patches:\n    ax.annotate('{}'.format(p.get_height()), (p.get_x(), p.get_height()))\n    \nplt.title(\"Rozk\u0142ad ruchu w zbiorze treningowym po skalowaniu w g\u00f3r\u0119\")\nplt.xlabel(\"typ ruchu\")\nplt.ylabel(\"liczba pr\u00f3bek\")\nplt.savefig(\"distribution_up2.png\", dpi=200, format='png', bbox_inches = \"tight\")\nplt.show()","109db707":"neural_validation_bin_lbl","87361fb5":"## encoding labels using OneHotEncoder. Attack types should not imply any hierachical relation between them \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\ntest_rshped = test_lbl.values.reshape(-1,1)\ntrain_rshped = train_lbl.values.reshape(-1,1)\nup_train_rshped = up_train_lbl.values.reshape(-1,1)\nup_neural_train_rshped = up_neural_train_lbl.values.reshape(-1,1)\nneural_validation_rshped = neural_validation_lbl.values.reshape(-1,1)\n\n\n# cats = np.array(['BENIGN', 'DoS Hulk', 'DDoS', 'PortScan', 'DoS slowloris',\n#        'Brute Force', 'DoS GoldenEye', 'Bot', 'DoS Slowhttptest',\n#        'SSH-Patator', 'FTP-Patator', 'XSS', 'Sql Injection',\n#        'Infiltration', 'Heartbleed'])\nohenc = OneHotEncoder()\nlenc = LabelEncoder()\n\ntest_lbl_enc = ohenc.fit_transform(test_rshped).toarray()  # one-hot encoded test set lbls\ntrain_lbl_enc = ohenc.fit_transform(train_rshped).toarray()  # one-hot encoded train set labels\nup_train_lbl_enc = ohenc.fit_transform(up_train_rshped).toarray()  # one-hot encoded upsampled train set lbls\nup_neural_train_lbl_enc = ohenc.fit_transform(up_neural_train_rshped).toarray()  # one-hot encoded upsampled train set lbls for neural nets predicting upsampled traffic\nneural_validation_lbl_enc = ohenc.fit_transform(neural_validation_rshped).toarray()  # one-hot encoded neural nets validation set for multiouput classification\n\ntest_bin_trff_lbll_enc = lenc.fit_transform(test_bin_trff_lbl)  # label encoded test set binary lbls\ntrain_bin_trff_lbll_enc = lenc.fit_transform(train_bin_trff_lbl) # label encoded train set binary lbls\nup_train_bin_trff_lbl_enc = lenc.fit_transform(up_train_bin_trff_lbl)  # label encoded upsampled train set binary lbls\nup_neural_train_bin_trff_lbl_enc = lenc.fit_transform(up_neural_train_bin_trff_lbl)  # label encoded upsampled train set binary lbls for neural nets predicting upsampled traffic\nneural_validation_bin_trff_lbl_enc = lenc.fit_transform(neural_validation_bin_trff_lbl)  # label encoded neural nets validation set for binary classification","1eb3790c":"## saving output for future use \npd.DataFrame(train_data).to_csv(\"train_set_df.csv\", index=False)  # raw train set\npd.DataFrame(up_train_data).to_csv(\"upsmpl_train_set_df.csv\", index=False) # upsampled train set for multi-output classification\npd.DataFrame(up_neural_train_data).to_csv(\"upsmpl_neural_train_set_df.csv\", index=False) # upsampled train set for multi-output classification for neural nets\npd.DataFrame(up_train_bin_data).to_csv(\"upsmpl_train_bin_set_df.csv\", index=False)  # upsampled train set for binary classification\npd.DataFrame(up_neural_train_bin_data).to_csv(\"upsmpl_neural_train_bin_set_df.csv\", index=False)  # upsampled train set for binary classification for neural nets\npd.DataFrame(test_data).to_csv(\"test_set_df.csv\", index=False)  #raw test set\n\ntrain_lbl.to_csv(\"train_lbl.csv\")  # raw labels for multi-output classification\nup_train_lbl.to_csv(\"upsmpl_train_lbl.csv\")  # raw lbls for upsampled multi-output classification\npd.DataFrame(data=train_lbl_enc).to_csv(\"train_lbl_enc.csv\", index=False) # one-hot encoded labels for multi-output classification\npd.DataFrame(data=up_train_lbl_enc).to_csv(\"upsmpl_train_lbl_enc.csv\", index=False) # one-hot encoded lbls for upsampled multi-output classification\npd.DataFrame(data=up_neural_train_lbl_enc).to_csv(\"upsmpl_neural_train_lbl_enc.csv\", index=False) # one-hot encoded lbls for upsampled multi-output classification 4 neural\n\ntest_lbl.to_csv(\"test_lbl.csv\")  # raw test labels for multi-output classification\npd.DataFrame(data=test_lbl_enc).to_csv(\"test_lbl_enc.csv\", index=False)  # one-hot encoded test lbls for multi-ouput classification\n\ntrain_bin_trff_lbl.to_csv(\"train_bin_trff_lbl.csv\")  # raw lbls for binary classification\nup_train_bin_trff_lbl.to_csv(\"upsmpl_train_bin_trff_lbl.csv\")  # raw lbls for upsampled binary classification\npd.DataFrame(data=train_bin_trff_lbll_enc).to_csv(\"train_bin_trff_lbl_enc.csv\", index=False)  # label encoded lbls for binary classification\npd.DataFrame(data=up_train_bin_trff_lbl_enc).to_csv(\"upsmpl_train_bin_trff_lbl_enc.csv\", index=False)  # label encoded lbls for upsampled binary classification\npd.DataFrame(data=up_neural_train_bin_trff_lbl_enc).to_csv(\"upsmpl_neural_train_bin_trff_lbl_enc.csv\", index=False)  # label encoded lbls for upsampled binary classification 4 neural\n\ntest_bin_trff_lbl.to_csv(\"test_bin_trff_lbl.csv\")  # raw test lbls for binary classification\npd.DataFrame(data=test_bin_trff_lbll_enc).to_csv(\"test_bin_trff_lbl_enc.csv\", index=False)  # label encoded test lbls for binary classification \n\npd.DataFrame(neural_validation).to_csv(\"neural_validation.csv\", index=False)  # validation set for neural nets predicting on upsampled training set\npd.DataFrame(data=neural_validation_bin_trff_lbl_enc).to_csv(\"neural_validation_bin_trff_lbl_enc.csv\", index=False)  # label encoded validation set lbl 4 neural 4 binary \npd.DataFrame(data=neural_validation_lbl_enc).to_csv(\"neural_validation_lbl_enc\", index=False)  # one-hot encoded validation set lbl 4 neural 4 multioutput classification","ae856cf4":"## DELETE COLUMNS CONTAINING ONLY 0s","25eda056":"## DELETE NA VALUES","4bf2adc4":"## TRAFFIC UNDERSAMPLING ","d86afa56":"## OverSampling training dataset","c0c50201":"## QUANTILE SCALING NUMERICAL FEATURES","9367f07e":"## DETECTING HIGHLY CORRELATED PAIRS","8298cda9":"## ENCODING CATEGORICAL FEATURES - TWO APPROACHES","8cfb8514":"## CALCULATING FEATURES' IMPORTANCE"}}