{"cell_type":{"e474668d":"code","1bc9dfe9":"code","64f50d20":"code","db22457b":"code","5dd34884":"code","58251c8b":"code","1b5800db":"code","14b55321":"code","24826ca5":"code","f9e0d8e0":"code","e1a440ee":"code","920cd430":"code","7fdb525d":"code","4a3ed4d0":"code","dfec5b7e":"code","d69d5fae":"code","88cadc40":"code","d02ce7de":"code","8838eca3":"code","178edcb1":"code","06529242":"code","488e4f29":"code","e7066f7f":"code","b8e5c8f0":"code","5eff35ad":"code","dbd7c02d":"code","a965384f":"markdown","dfd94793":"markdown","8b9b781e":"markdown","3ae6d2c7":"markdown","523dfb1e":"markdown","8f15504c":"markdown"},"source":{"e474668d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1bc9dfe9":"data = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\ndata.shape","64f50d20":"data.describe()","db22457b":"data.isnull().sum()","5dd34884":"data_clean= data.dropna()\ndata_clean.reset_index(drop=True, inplace=True)\ndata_clean2=data.fillna(data.mean())\ndata_clean.shape","58251c8b":"# Checking for outliers\nq25, q50, q75 = np.percentile(data_clean, [25, 50, 75] )\niqr = q75 - q25\nmini = q25 - 1.5*(iqr)\nmaxi = q75 + 1.5*(iqr)\nfor cols in data_clean.columns :\n    out=[x for x in data_clean[cols] if x > maxi]\n    print(len(out))\nfor cols in data_clean.columns :\n    out=[x for x in data_clean[cols] if x < mini]\n    print(len(out))","1b5800db":"one = data[data['Potability']== 1]\nzero = data[data['Potability']== 0]\none.shape , zero.shape","14b55321":"# Definition to make a sublot of histograms\ndef hist_plots( data: pd.DataFrame,\n               rows: int,\n              cols: int,\n              figsize: tuple):\n    fig, axes = plt.subplots(rows,cols, figsize=figsize)\n    for i, ax in enumerate(axes.flatten()):\n        if i < len(data.columns):\n            data[sorted(data.columns)[i]].plot.hist(bins=30, ax=ax)\n            ax.set_title(f'{sorted(data.columns)[i]} distribution', fontsize=10)\n            ax.tick_params(axis='x', labelsize=10)\n            ax.tick_params(axis='y', labelsize=10)\n            ax.get_yaxis().get_label().set_visible(False)\n        else:\n            fig.delaxes(ax=ax)\n    fig.tight_layout()","24826ca5":"hist_plots(data=data_clean,\n          rows=3,\n          cols=4,\n          figsize=(20,10))","f9e0d8e0":"hist_plots(data=data_clean2,\n          rows=3,\n          cols=4,\n          figsize=(20,10))","e1a440ee":"cols = data.columns[:-1]\ncorr_values = data[cols].corr()\n\n# Simplify by emptying all the data below the diagonal\ntril_index = np.tril_indices_from(corr_values)\n\n# Make the unused values NaNs\nfor coord in zip(*tril_index):\n    corr_values.iloc[coord[0], coord[1]] = np.NaN\n    \n# Stack the data and convert to a data frame\ncorr_values = (corr_values\n               .stack()\n               .to_frame()\n               .reset_index()\n               .rename(columns={'level_0':'feature1',\n                                'level_1':'feature2',\n                                0:'correlation'}))\n\n# Get the absolute values for sorting\ncorr_values['abs_correlation'] = corr_values.correlation.abs()","920cd430":"# Correlation distribution\nsns.set_context('talk')\nsns.set_style('white')\n\nax = corr_values.abs_correlation.hist(bins=20, figsize=(12, 8))\nax.set(xlabel='Absolute Correlation', ylabel='Frequency');","7fdb525d":"from sklearn.model_selection import StratifiedShuffleSplit\ncols= data_clean.columns[:-1]\n# Get the split indexes\nstrat_shuf_split = StratifiedShuffleSplit(n_splits=3, \n                                          test_size=0.3, \n                                          random_state=42)\n\ntrain_idx, test_idx = next(strat_shuf_split.split(data_clean[cols], data_clean['Potability']))\n\n# Create the dataframes\nX_train = data_clean.loc[train_idx, cols]\ny_train = data_clean.loc[train_idx, 'Potability']\n\nX_test  = data_clean.loc[test_idx, cols]\ny_test  = data_clean.loc[test_idx, 'Potability']\n\nX_train2 = data_clean2.loc[train_idx, cols]\ny_train2 = data_clean2.loc[train_idx, 'Potability']\n\nX_test2  = data_clean2.loc[test_idx, cols]\ny_test2  = data_clean2.loc[test_idx, 'Potability']","4a3ed4d0":"from sklearn.preprocessing import MinMaxScaler\n# Scaling features\nmm = MinMaxScaler() \nX_train= mm.fit_transform(X_train) \nX_test= mm.transform(X_test)","dfec5b7e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Tune hyperparameters\nparams = {\"C\":np.logspace(-3,3,20), \"penalty\":[\"l1\",\"l2\"]}\ngrid = GridSearchCV(LogisticRegression(solver='liblinear'), params)\n\n#Fit to thw train data\nGR = grid.fit(X_train, y_train)\nGR.best_params_ ,GR.best_score_","d69d5fae":"y_pred=grid.predict(X_test)\ny_prob=grid.predict_proba(X_test)","88cadc40":"from sklearn.metrics import classification_report, roc_auc_score\n\ncr = classification_report(y_test, y_pred)\nprint(cr)\n\nauc= roc_auc_score(y_test, y_pred),\n                        \nprint('auc: ', auc)\n","d02ce7de":"from sklearn.tree import DecisionTreeClassifier\nparam_grid = {'max_depth':range(1,300, 2),\n              'max_features': range(1, len(cols))}\n\nGR2 = GridSearchCV(DecisionTreeClassifier(random_state=42),\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  n_jobs=-1)\n\nGR2 = GR2.fit(X_train, y_train)\nGR2.best_params_ ,GR2.best_score_, GR2.best_estimator_.tree_.node_count","8838eca3":"y_pred_gr2 = GR2.predict(X_test)","178edcb1":"cr2 = classification_report(y_test, y_pred_gr2)\nprint(cr2)\n\nauc2= roc_auc_score(y_test, y_pred_gr2),\n                        \nprint('auc: ', auc2)","06529242":"from sklearn.ensemble import RandomForestClassifier\nparam_grid = {'n_estimators':range(1,400, 2)}\nRF = RandomForestClassifier(oob_score=True, \n                            random_state=42, \n                            warm_start=True,\n                            n_jobs=-1)\nrf = GridSearchCV(RF,\n                  param_grid=param_grid)\n\nrf = rf.fit(X_train, y_train)\nrf.best_params_ ,rf.best_score_","488e4f29":"y_pred_rf = rf.predict(X_test)","e7066f7f":"cr3 = classification_report(y_test, y_pred_rf)\nprint(cr3)\n\nauc3= roc_auc_score(y_test, y_pred_rf),\n                        \nprint('auc: ', auc3)","b8e5c8f0":"param_grid = {'n_estimators':range(1,400, 2)}\nRF = RandomForestClassifier(oob_score=True, \n                            random_state=42, \n                            warm_start=True,\n                            n_jobs=-1)\nrf = GridSearchCV(RF,\n                  param_grid=param_grid)\nrf2 = rf.fit(X_train2, y_train2)\nrf2.best_params_ ,rf2.best_score_","5eff35ad":"y_pred_rf2 = rf2.predict(X_test2)","dbd7c02d":"cr4 = classification_report(y_test2, y_pred_rf2)\nprint(cr4)\n\nauc4= roc_auc_score(y_test2, y_pred_rf2),\n                        \nprint('auc: ', auc4)","a965384f":"The third column shows outliers equals to the number of observations and as shown on the distributions bellow there are no outliers so I am going to ignore them.","dfd94793":"The models hyperparameters are tuned with GridSearchCV\n\n# # Model 1 Logistic Regression","8b9b781e":"In this notebook I study if we can categorize water as potable or non-potable given the  data at hand. I use 3 classification models: Logistic regression, Decision trees and Random forest. I also study the discrepancies between deleting missing values and inputing them with the mean. ","3ae6d2c7":"# # Model 2 Decision Tree ","523dfb1e":"# # Model 3 Random Forest ","8f15504c":"There is a lot of missiing values. I am going to remove and to replace the values with the mean and check for differences with one of the models."}}