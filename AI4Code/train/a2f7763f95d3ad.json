{"cell_type":{"3f53239c":"code","1a47fc4b":"code","c65140e2":"code","865298bc":"code","c5dbc8b0":"code","a2ddee48":"code","f103da6c":"code","58315b25":"code","c7a6e77f":"code","962b574f":"code","b913a594":"code","5c712e14":"code","572fe2bd":"code","15f62b64":"code","e3887643":"code","deabfc89":"code","d0d2221a":"code","68e44ec7":"code","dfb8511f":"code","4cc0d4ba":"code","627c7c1b":"code","b224cc24":"code","0b999f96":"markdown"},"source":{"3f53239c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport gc #garbage collection\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom hyperopt import hp, fmin, tpe, Trials\nfrom hyperopt.pyll.base import scope\nfrom tqdm import tqdm","1a47fc4b":"# Load data\npath = '..\/input\/tabular-playground-series-jan-2021\/'\ntrain = pd.read_csv(path+'train.csv')\ntrain.set_index('id',drop=True,inplace=True)\ntrain.drop(284103,inplace=True) # looks like an error\/outlier, label = 0.0","c65140e2":"# train.head()\n# train.isna().sum() \n# found no missing values","865298bc":"features = [col for col in train.columns if 'cont' in col]\nlabel = 'target'","c5dbc8b0":"plt.hist(train[label],bins=100)\nplt.title('Distribution of target')\nplt.show()","a2ddee48":"fig, ax = plt.subplots(int(len(features)\/2),2, figsize= (20,20))\nfig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\ncounter = 0 \nfor i,feature in enumerate(features):\n    ax[i%7,counter\/\/7].hist(train[feature],bins=100)\n    ax[i%7,counter\/\/7].set_title('Distribution of feature ' + str(feature), fontsize=12)\n    counter +=1","f103da6c":"fig.clear()\nplt.close(fig)","58315b25":"# train\/eval set split\nX_train,X_valid, y_train,y_valid = train_test_split(train[features],train[label],test_size=0.2)\n\nd_tr = xgb.DMatrix(X_train, y_train)\nd_val = xgb.DMatrix(X_valid,y_valid)","c7a6e77f":"# Un-tunded base case estimator to compare score\nparams_base = {'objective': 'reg:squarederror',\n               'tree_method': 'gpu_hist',\n               'random_state': 0}\nbase_model = xgb.train(params = params_base,\n                       dtrain = d_tr,\n                       num_boost_round = 1000,\n                       evals = [(d_val,'eval')],\n                       early_stopping_rounds=10,\n                       verbose_eval = 20)\ny_pred_base = base_model.predict(d_val)\nbase_score = mean_squared_error(y_valid, y_pred_base,squared=False)\nprint(base_score)","962b574f":"# Simple Cross Val score as function to be optimised\n\ndef score(params):\n    \n    ps = {'learning_rate': params['learning_rate'],\n         'max_depth': params['max_depth'], \n         'gamma': params['gamma'], \n         'min_child_weight': params['min_child_weight'], \n         'subsample': params['subsample'], \n         'colsample_bytree': params['colsample_bytree'], \n         'verbosity': 1, \n         'objective': 'reg:squarederror',\n         'eval_metric': 'rmse', \n         'tree_method': 'gpu_hist', \n         'random_state': 27,\n        }\n    model = xgb.train(ps,d_tr, params['n_round'], [(d_val, 'eval')], early_stopping_rounds=10, verbose_eval = False)\n    y_pred = model.predict(d_val)\n    score = mean_squared_error(y_valid, y_pred,squared=False)\n\n    return score","b913a594":"# Define parameter space\nparam_space = {'learning_rate': hp.uniform('learning_rate', 0.01, 0.3), \n               'n_round': scope.int(hp.quniform('n_round', 200, 3000, 100)),\n               'max_depth': scope.int(hp.quniform('max_depth', 5, 16, 1)), \n               'gamma': hp.uniform('gamma', 0, 10), \n               'min_child_weight': hp.uniform('min_child_weight', 0, 10),\n               'subsample': hp.uniform('subsample', 0.1, 1), \n               'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1)\n              }","5c712e14":"# Run optimiser with tpe\n%time\ntrials = Trials()\n\nhopt = fmin(fn = score,\n            space = param_space, \n            algo = tpe.suggest, \n            max_evals = 1000, ## 100\n            trials = trials, \n           )","572fe2bd":"params_best = hopt\nparams_best['max_depth'] = int(hopt['max_depth'])\nn_rounds_best = int(hopt['n_round'])\ndel params_best['n_round']\nprint(params_best)\nprint(n_rounds_best)","15f62b64":"# score(params_best)","e3887643":"# # trails object stores results on all trials\n# trials.trials","deabfc89":"# plot parameter choice in trials for selected parameters\nparams_to_plot = ['n_round','learning_rate', 'max_depth', 'min_child_weight']\nfig, ax = plt.subplots(len(params_to_plot),1, figsize= (10,15))\nfig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\nfor i,param in enumerate(params_to_plot):\n    xs = [t['tid'] for t in trials.trials]\n    ys = [t['misc']['vals'][param] for t in trials.trials]\n    ax[i].scatter(xs, ys, s=20, linewidth=0.01, alpha=0.75)\n    ax[i].set_title(str(param) + ' vs t ', fontsize=18)\n    ax[i].set_xlabel('id', fontsize=16)\n    ax[i].set_ylabel(str(param), fontsize=16)","d0d2221a":"f, ax = plt.subplots(1)\nxs = [t['tid'] for t in trials.trials]\nys = [t['result']['loss'] for t in trials.trials]\nax.scatter(xs, ys, s=20, linewidth=0.01, alpha=0.75)\nax.set_title('loss over time', fontsize=18)\nax.set_xlabel('trials', fontsize=16)\nax.set_ylabel('loss', fontsize=16)","68e44ec7":"%time\n# Train with full dataset and best params\nparams_best['tree_method'] = 'gpu_hist'\nd = xgb.DMatrix(train[features], train[label])\nxgb_final = xgb.train(params_best,d,n_rounds_best)","dfb8511f":"y_pred_final = xgb_final.predict(d)\nscore_final = np.sqrt(mean_squared_error(train[label], y_pred_final))\nprint(score_final) #sanity check","4cc0d4ba":"# Load test data\ntest = pd.read_csv(path + 'test.csv')\ntest.set_index('id',drop=True,inplace=True)\nd_tst = xgb.DMatrix(test[features])\n# test.head()","627c7c1b":"# Predictions for test data\nmodels = []\n\nfor seed in range(0,10):\n    params_best['seed'] = seed\n    xgb_final = xgb.train(params_best,d,num_boost_round = n_rounds_best)\n    models.append(xgb_final)\n    \nxgb_pred = xgb_final.predict(d_tst)","b224cc24":"# Save test predictions to file\nids = test.index\noutput = pd.DataFrame({'id': ids,\n                       'target': xgb_pred})\noutput.to_csv('submission.csv', index=False)","0b999f96":"# XGBoost Parameter Tuning with Hyperopt"}}