{"cell_type":{"edb91798":"code","b93176ac":"code","1eb33de6":"code","dac26153":"code","3a4f5fb0":"code","73a76a77":"code","acc3e347":"code","8869238f":"code","4bc35d68":"code","eab6b714":"code","d66bc39f":"code","54f156d2":"code","a403d6a7":"markdown","2a6e9f02":"markdown","2e054cbc":"markdown","e1885544":"markdown","cf39ab75":"markdown","1cc60705":"markdown","bddbd66e":"markdown","ee3de5cb":"markdown","98eacf70":"markdown","5615e063":"markdown","bb49b6ce":"markdown","3a20b74b":"markdown","667b8510":"markdown","d19e6c76":"markdown"},"source":{"edb91798":"import pandas as pd\nimport numpy as np\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom scipy.stats import multivariate_normal  # Estimating Gaussian distribution","b93176ac":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","1eb33de6":"train.head()","dac26153":"fig = plt.gcf()\nfig.set_size_inches(8, 5)\n\nsns.scatterplot(x=\"radius_mean\", y=\"texture_mean\", hue=\"diagnosis\", palette=[\"red\", \"blue\"], data=train)\nplt.title(\"Texture Mean vs. Radius Mean\")","3a4f5fb0":"# Standardize each feature before analysis\ndef standardize(train, test):\n    \"\"\"\n    Standardizes training and testing set data based on the means and standard\n    deviations of the training set. x = (x-\u00b5)\/\u03c3\n    \n    :param train: training set\n    :param test: testing set\n    :returns: standardized training and testing set\n    \"\"\"\n    for col in train.columns:\n        if col == \"diagnosis\":\n            continue\n        mean = np.mean(train[col])\n        std = np.std(train[col])\n        train[col] = (train[col]-mean)\/std  # standardize training data\n        test[col] = (test[col]-mean)\/std  # standardize testing data\n    return train, test\n\ntrain, test = standardize(train, test)","73a76a77":"# Split data into two classes\nbenign = train[train[\"diagnosis\"] == \"B\"]\nmalignant = train[train[\"diagnosis\"] == \"M\"]","acc3e347":"# Calculate Covariance Matrix for each class\ndef covariance_matrix(data):\n    \"\"\"\n    Calculates the covariance matrix of all features for the dataset.\n    \n    :param data: dataset for covariance matrix\n    :return: covariance matrix of data\n    \"\"\"\n    df = data.drop([\"diagnosis\"], axis=1)\n    for col in df.columns:\n        df[col] = df[col] - np.mean(df[col])\n    X = df.to_numpy()\n    n = len(data)\n    return (1\/n)*np.dot(X.T, X)\n\nC1 = covariance_matrix(benign)  # benign covariance matrix\nC2 = covariance_matrix(malignant)  # malignant covariance matrix","8869238f":"# Overall Covariance Matrix with weighted average\nn1 = len(benign)\nn2 = len(malignant)\nC = (n1*C1+n2*C2)\/(n1+n2)\nC  # joined covariance matrix","4bc35d68":"def _LDA(features):\n    \"\"\"\n    Performs LDA classification on one data point, given array of its features.\n    \n    :param features: array of feature data\n    :return: label prediction\n    \"\"\"\n    p_benign = len(benign)\/len(train)  # probability of benign \n    p_malignant = len(malignant)\/len(train)  # probability of malignant\n    \n    mean_benign = np.array(benign.mean(axis=0))  # mean of benign\n    mean_malignant = np.array(malignant.mean(axis=0))  # mean of malignant\n    \n    pdf_benign = multivariate_normal.pdf(features, mean_benign, C)  # conditional probability given benign\n    pdf_malignant = multivariate_normal.pdf(features, mean_malignant, C)  # conditional probability given malignant\n    if pdf_benign * p_benign > pdf_malignant * p_malignant:\n        return \"B\"\n    else:\n        return \"M\"","eab6b714":"X_train = train.drop([\"diagnosis\"], axis=1).to_numpy()\ny_train = train[\"diagnosis\"]\nX_test = test.drop([\"diagnosis\"], axis=1).to_numpy()\ny_test = test[\"diagnosis\"]\n\ndef LDA(data):\n    \"\"\"\n    Performs LDA classification on an array of data.\n    \n    :param data: array of data to make predictions on\n    :return: array of predictions on data\n    \"\"\"\n    preds = np.array([])\n    for i in data:\n        preds = np.append(preds, _LDA(i))\n    return preds\n\nLDA_preds_train = LDA(X_train)  # LDA predictions for training set\nprint(\"LDA Training Accuracy: \", np.sum(LDA_preds_train==y_train)\/len(train))\nprint(\"LDA Training Error: \", np.sum(LDA_preds_train!=y_train)\/len(train))\n\nLDA_preds_test = LDA(X_test)   # LDA predictions for testing set\nprint(\"LDA Testing Accuracy: \", np.sum(LDA_preds_test==y_test)\/len(test))\nprint(\"LDA Testing Error: \", np.sum(LDA_preds_test!=y_test)\/len(test))","d66bc39f":"def _QDA(features):\n    \"\"\"\n    Performs QDA classification on one data point, given array of its features.\n    \n    :param features: array of feature data\n    :return: label prediction\n    \"\"\"\n    p_benign = len(benign)\/len(train)  # probability of benign \n    p_malignant = len(malignant)\/len(train)  # probability of malignant\n    \n    mean_benign = np.array(benign.mean(axis=0))  # mean of benign\n    mean_malignant = np.array(malignant.mean(axis=0))  # mean of malignant\n    \n    pdf_benign = multivariate_normal.pdf(features, mean_benign, C1)  # conditional probability of benign\n    pdf_malignant = multivariate_normal.pdf(features, mean_malignant, C2)  # conditional probability of malignant\n    if pdf_benign * p_benign > pdf_malignant * p_malignant:\n        return \"B\"\n    else:\n        return \"M\"","54f156d2":"def QDA(data):\n    \"\"\"\n    Performs QDA classification on an array of data.\n    \n    :param data: array of data to make predictions on\n    :return: array of predictions on data\n    \"\"\"\n    preds = np.array([])\n    for i in data:\n        preds = np.append(preds, _QDA(i))\n    return preds\n\nQDA_preds_train = QDA(X_train)  # LDA predictions for training set\nprint(\"QDA Training Accuracy: \", np.sum(QDA_preds_train==y_train)\/len(train))\nprint(\"QDA Training Error: \", np.sum(QDA_preds_train!=y_train)\/len(train))\n\nQDA_preds_test = QDA(X_test)   # LDA predictions for testing set\nprint(\"QDA Testing Accuracy: \", np.sum(QDA_preds_test==y_test)\/len(test))\nprint(\"QDA Testing Error: \", np.sum(QDA_preds_test!=y_test)\/len(test))","a403d6a7":"We will calculate the covariance matrix of the data with all 30 features. Then we will have 2 different convariance matrices made from the data for both classes.","2a6e9f02":"Before performing Linear Discriminant Analysis, it's important to standardize the data for each feature in order to get more accurate results. Larger data values tends to greatly impact the estimated probabilities coming from the Gaussian distribution, which we can help mitigate by standardizing beforehand.","2e054cbc":"Again, very similar to LDA, we perform QDA by calculating conditional probabilities of each class given the data point feature vector. We can calculate the conditional probabilities for each class by using Bayes' Rule:\n\n\ud835\udc43(\ud835\udc34|\ud835\udc35)=\ud835\udc43(\ud835\udc35|\ud835\udc34)\u2217\ud835\udc43(\ud835\udc34)\/\ud835\udc43(\ud835\udc35) \nwhere A represents the class label and B represents the feature vector.\n\nSince the denominators are the same for each class because we are using the same feature vector for both probabilities, we find the label that maximizes  \ud835\udc43(\ud835\udc35|\ud835\udc34)\u2217\ud835\udc43(\ud835\udc34)  To calculate the conditional probability, we can estimate it with a multivariate Gaussian distribution, using different covariance matrices amongst the different classes. From this, we can estimate the likelihood of our feature vector occuring in the distribution described by the training data we fit.","e1885544":"# Quadratic Discriminant Analysis","cf39ab75":"In Linear Discriminant Analysis, we assume that the diagonal covariance matrix is shared amongst the classes. Since we have two different covariance matrices calculated, we can join these two matrcies together by using the weighted mean and get one joined, shared, covariance matrix.","1cc60705":"The prediction made by LDA depends on the conditional probabilities of each class, given the features of the test data. We can calculate the conditional probabilities for each class by using Bayes' Rule:\n\n$P(A|B) =  P(B|A)*P(A)\/P(B)$\n\nwhere A represents the class label and B represents the feature vector.\n\nSince the denominators are the same for each class because we are using the same feature vector for both probabilities, we find the label that maximizes $P(B|A)*P(A)$ To calculate the conditional probability, we can estimate it with a multivariate Gaussian distribution, using the shared covariance matrix we computed above, and the means of each feature in the data. From this, we can estimate the likelihood of our feature vector occuring in the distribution described by the training data we fit.\n\nBecause we are using a shared covariance matrix between the classes, the resulting decision boundary of our model will be linear, hence the name.","bddbd66e":"# Feature Visualization\nBefore getting started, we should visualize the differences in the values of the features between the two class labels, malignant and benign. Features are more useful when there is a clear distinction between the data points of the two labels.","ee3de5cb":"Now we can start making predictions on the testing set and evaluate the performance of our model.","98eacf70":"Quadratic Discriminant Analysis introduces a slightly more complex model than LDA. We already have the covariance matrices for each class, benign and malignant, calculated above as *C1* and *C2*, respectively. In QDA, the difference is that the covariance matrices used in fitting the Gaussian are different between the 2 classes. This results in a decision boundary that is quadratic, hence the name. So, we will use C1 and C2 to calculate conditional probabilities for the multivariate Gaussian distribution, rather than the collective covariance matrix.","5615e063":"Now we can split the data into the two classes to calculate conditional probabilities.","bb49b6ce":"# Linear Discriminant Analysis","3a20b74b":"# Linear & Quadratic Discriminant Analysis","667b8510":"We have data about various tumors and whether they are Malignant, \"M\", or Benign, \"B\", as the diagnosis. Each tumor has measures recorded like their radius, perimeter, area, etc. In this notebook, we will use linear and quadratic discriminant analysis **(LDA\/QDA)** to classify tumors as either malignant or benign. To do so, we will use estimate probabilities with multivariate Gaussian distributions.","d19e6c76":"Here is the radius mean vs. the texture mean of malignant and benign tumors. We can fit a Gaussian to just these two features of course, but it's more useful to fit multivariate Gaussians of larger dimensions and use all features in our data."}}