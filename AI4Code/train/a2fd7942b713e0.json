{"cell_type":{"86662ea3":"code","4108d9d0":"code","4c8f6bf9":"code","7021a99d":"code","676297fb":"code","6d389d33":"code","bf2dbfdd":"code","8d43455b":"code","4cd19320":"code","16506c0f":"code","3b6224cb":"code","08123e17":"code","6ea88432":"code","7794ecd2":"code","519ee24a":"code","35ccdfcb":"code","0a98db1a":"code","97c3b67c":"code","25bb3197":"code","2ea4cbf5":"code","e9add649":"markdown","aefee2f0":"markdown","5f499279":"markdown","85e935a4":"markdown"},"source":{"86662ea3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4108d9d0":"! cp -rf \/kaggle\/input\/aerial-cactus-identification\/train.csv -d \/kaggle\/working\n! unzip -o \/kaggle\/input\/aerial-cactus-identification\/train.zip -d \/kaggle\/working\n! unzip \/kaggle\/input\/aerial-cactus-identification\/test.zip -d \/kaggle\/working","4c8f6bf9":"import os\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport keras\n\n\nfrom keras.datasets import mnist\nfrom sklearn.model_selection import train_test_split\n\nprint(\"tf version : \", tf.__version__)\n\n# GPU test \ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n    raise SystemError('GPU device not found')\n\nprint('Found GPU at: {}'.format(device_name))","7021a99d":"df = pd.read_csv('train.csv')\n#df.has_cactus = np.where(df.has_cactus == 1, 'yes', 'no')\ndf.sample(3)\ndf.has_cactus.value_counts().plot.bar()","676297fb":"from keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.utils import to_categorical\n\nfilename = df.id[10]\nprint(filename)\nimage = load_img(\".\/train\/\"+filename)\n\nplt.imshow(image)","6d389d33":"train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)\ntrain_df = train_df.reset_index(drop=True)\nvalidate_df = validate_df.reset_index(drop=True)","bf2dbfdd":"train_datagen = ImageDataGenerator(\n    rotation_range=45,\n    rescale=1.\/32,\n    zoom_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=True,\n#     width_shift_range=0.1,\n#     height_shift_range=0.1\n\n)\n\n\n#train_datagen = ImageDataGenerator()","8d43455b":"BATCH_SIZE = 128\nIMAGE_SIZE = (32,32)\n\nINPUT_SHAPE=(32, 32, 3)\nBATCH_SIZE=2**10\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df, \n    directory=\".\/train\",\n    x_col='id',\n    y_col='has_cactus',\n    target_size=IMAGE_SIZE,\n    color_mode='rgb',\n    batch_size=BATCH_SIZE,\n    class_mode=\"raw\"\n)\n\n\nvalidation_generator = train_datagen.flow_from_dataframe(\n    dataframe=validate_df, \n    directory=\".\/train\",\n    x_col='id',\n    y_col='has_cactus',\n    target_size=IMAGE_SIZE,\n    color_mode='rgb',\n    batch_size=BATCH_SIZE,\n    class_mode=\"raw\"\n)","4cd19320":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Dropout, AveragePooling2D\n\n\nmodel = Sequential([\n                    Conv2D(filters=64, kernel_size=(4,4), strides=(1,1), activation='relu', input_shape=(32, 32, 3), padding=\"same\"),\n                    BatchNormalization(),\n                    AveragePooling2D( pool_size=(3, 3)), \n                    Dropout(0.2),\n\n#                     Conv2D(128, (4, 4), activation='relu',  padding=\"same\"),\n#                     BatchNormalization(),\n#                     AveragePooling2D( pool_size=(2, 2)), \n#                     #Dropout(0.2),\n    \n        \n#                     Conv2D(64, (2, 2), activation='relu',  padding=\"same\"),\n#                     BatchNormalization(),\n#                     AveragePooling2D( pool_size=(2, 2)), \n#                     Dropout(0.3),\n    \n    \n#                     Conv2D(32, (2, 2), activation='relu',  padding=\"same\"),\n#                     BatchNormalization(),\n#                     AveragePooling2D( pool_size=(2, 2)), \n#                     Dropout(0.3),\n\n                    Flatten(),\n                    Dense(128, activation='relu'),\n                    Dense(64, activation='relu'),\n                    Dense(32, activation='relu'),\n                    Dropout(0.45),\n                    Dense(1, activation='sigmoid')\n])\n\n\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nearlystop = EarlyStopping(patience=4)\nmodel.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\ncallbacks = [earlystop]\n\nmodel.summary()","16506c0f":"%%time\nhistory = model.fit(\n    train_generator, \n    epochs=30,\n    validation_data=validation_generator,\n    #validation_steps=validate_df.shape[0]\/\/BATCH_SIZE,\n    #steps_per_epoch=train_df.shape[0]\/\/BATCH_SIZE,\n    callbacks=callbacks\n)","3b6224cb":"pd.DataFrame(history.history).plot()","08123e17":"df = pd.DataFrame()\ndf['id'] = os.listdir('test')\ndf.head()\n\nfrom keras.preprocessing import image_dataset_from_directory\n\ntest_gen = ImageDataGenerator(rescale=1.\/255)\ntest_generator = test_gen.flow_from_dataframe(\n    df,\n    \"test\", \n    x_col='id',\n    y_col=None,\n    class_mode=None,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    shuffle=False\n)\n\n\npred=model.predict(test_generator)\n\n\ndf['has_cactus'] =np.transpose(pred)[0] #np.argmax(pred, axis=-1)\ndf.sample(5)","6ea88432":"pred","7794ecd2":"np.transpose(pred)[0]","519ee24a":"df.has_cactus.max()","35ccdfcb":"submission = df.copy()\nsubmission.to_csv('submission.csv', index=False)","0a98db1a":"! ls ..\/","97c3b67c":"submission.head()","25bb3197":"submission.has_cactus.describe()","2ea4cbf5":"! rm -rf train test train.csv","e9add649":"## Data Preprocessing","aefee2f0":"## Data extraction","5f499279":"## Model:  less is better \n\n","85e935a4":"### Test & submit"}}