{"cell_type":{"3e0fc404":"code","af90164c":"code","d54e5b0b":"code","892c3eaa":"code","f4aa6ae4":"code","42dce4f7":"code","a7220960":"code","c4ed3575":"code","0f434df0":"code","130cc7a6":"code","413ce94e":"code","631aed2c":"code","344cc9af":"code","6d7f619f":"code","c18ae16e":"code","eecd40d5":"code","a30640c6":"code","5a5b0a20":"code","324741ff":"code","59f51c93":"code","f5de6c79":"code","f85721e4":"code","80252e8a":"code","a579543c":"markdown","f21f0bf3":"markdown","0766aa18":"markdown","c7b00a7f":"markdown","7f9334a7":"markdown","f0bde678":"markdown","ef97df82":"markdown","43fb99b4":"markdown","5cdaa899":"markdown","5ba28719":"markdown","06b06e1a":"markdown","9afe74d1":"markdown","3569b8a9":"markdown","b09e1f1a":"markdown","a656c350":"markdown","67e42468":"markdown","219ec957":"markdown","7d1c8d06":"markdown","d6e2a419":"markdown"},"source":{"3e0fc404":"import pandas as pd\n\ndf=pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndf.tail()","af90164c":"df.drop(\"id\", axis=1, inplace=True)\nprint(df.info())","d54e5b0b":"df.dropna(subset=[\"bmi\"], inplace=True)","892c3eaa":"df.describe()","f4aa6ae4":"import seaborn as sns\nfrom matplotlib import pyplot as plt\n\nsns.pairplot(df)\nplt.show()","42dce4f7":"fig, ax=plt.subplots(nrows=6, ncols=2, figsize=(20, 30))\nfig.tight_layout(h_pad=9, w_pad=7)\n\nsns.countplot(x=df.gender, ax=ax[0][0])\ninfo_text=\"Female: \"+str(df.gender.value_counts().Female)+\"\\nMale: \"+str(df.gender.value_counts().Male)+\"\\nOther: \"+str(df.gender.value_counts().Other)\nax[0][0].legend(title=info_text, labels=[])\nax[0][0].set_xlabel(\"Gender\")\nax[0][0].set_ylabel(\"Count\")\nax[0, 0].set_title(\"Gender\", fontsize=20)\n\nsns.histplot(df.age, ax=ax[0, 1], kde=True)\nax[0, 1].set_xlabel(\"Age\")\nax[0, 1].set_ylabel(\"Count\")\nax[0, 1].set_title(\"Age\", fontsize=20)\nplt.sca(ax[0, 1])\nplt.legend(labels=[], title=\"Line: Kernel Density Estimation\")\n\nsns.countplot(x=df.hypertension, ax=ax[1, 0])\nplt.sca(ax[1, 0])\nplt.xticks([0, 1], [\"No\", \"Yes\"])\ninfo_text=\"No: \"+str(df.hypertension.value_counts()[0])+\"\\nYes: \"+str(df.hypertension.value_counts()[1])\nplt.legend(labels=[], title=info_text)\nax[1, 0].set_xlabel(\"Answer\")\nax[1, 0].set_ylabel(\"Count\")\nax[1, 0].set_title(\"Hypertension\", fontsize=20)\n\nsns.countplot(x=df.heart_disease, ax=ax[1, 1])\nplt.sca(ax[1, 1])\nplt.xticks([0, 1], [\"No\", \"Yes\"])\ninfo_text=\"No: \"+str(df.heart_disease.value_counts()[0])+\"\\nYes: \"+str(df.heart_disease.value_counts()[1])\nplt.legend(labels=[], title=info_text)\nax[1, 1].set_xlabel(\"Answer\")\nax[1, 1].set_ylabel(\"Count\")\nax[1, 1].set_title(\"Heart Disease\", fontsize=20)\n\nsns.countplot(x=df.ever_married, ax=ax[2, 0], order=[\"No\", \"Yes\"])\nax[2, 0].set_xlabel(\"Answer\")\nax[2, 0].set_ylabel(\"Count\")\nax[2, 0].set_title(\"Ever Married?\", fontsize=20)\n\nplt.sca(ax[2, 1])\nax[2, 1].set_title(\"Work Type\", fontsize=20)\ndf.work_type.value_counts().index\nrenamed_labels=['Private', 'Self Employed', 'Children', 'Government Jobs', 'Never Worked']\nplt.pie(x=df.work_type.value_counts(), labels=renamed_labels, autopct=\"%1.1f%%\", \n        explode=df.work_type.nunique()*[.03])\n\nplt.sca(ax[3, 0])\nax[3, 0].set_title(\"Residence Type\", fontsize=20)\ndf.Residence_type.value_counts().index\nplt.pie(x=df.Residence_type.value_counts(), labels=df.Residence_type.value_counts().index, autopct=\"%1.1f%%\", \n        explode=df.Residence_type.nunique()*[.03])\n\nsns.histplot(x=df.avg_glucose_level, ax=ax[3, 1], kde=True)\nax[3, 1].set_title(\"Average Glucose Level In Blood\", fontsize=20)\nax[3, 1].set_xlabel(\"Level\")\nax[3, 1].set_ylabel(\"Count\")\n\nsns.histplot(x=df.bmi, kde=True, ax=ax[4, 0])\nax[4, 0].set_title(\"Body Mass Index\", fontsize=20)\nax[4, 0].set_xlabel(\"\")\nax[4, 0].set_ylabel(\"Count\")\n\nsns.countplot(x=df.smoking_status, ax=ax[4, 1])\nplt.sca(ax[4, 1])\nax[4, 1].set_title(\"Smoking Status\", fontsize=20)\nax[4, 1].set_xlabel(\"\")\nax[4, 1].set_ylabel(\"Count\")\n\nplt.sca(ax[5, 0])\nax[5, 0].set_title(\"Stroke\", fontsize=20)\nplt.pie(x=df.stroke.value_counts(), labels=[\"No\", \"Yes\"], autopct=\"%1.1f%%\")\n\nax[5, 1].set_visible(False)\n\nplt.show()","a7220960":"df.dtypes","c4ed3575":"categorical_df=df.copy()\ndictionary_of_encodes={}\n\nfor column in categorical_df.select_dtypes(\"object\").columns:\n    categorical_df[column]=categorical_df[column].astype(\"category\") #Changing dtype.\n    dictionary_of_encodes[column]=dict( enumerate(categorical_df[column].cat.categories ) ) #Saving the encoding dictionary.\n    categorical_df[column]=categorical_df[column].cat.codes #Encoding the dataframe.\n    \ncategorical_df.dtypes","0f434df0":"categorical_df[[\"age\", \"avg_glucose_level\", \"bmi\"]].describe()","130cc7a6":"categorical_df.age=pd.cut(x=categorical_df.age, bins=[x for x in range(0, 101, 10)])\ncategorical_df.avg_glucose_level=pd.cut(x=categorical_df.avg_glucose_level, bins=[x for x in range(55, 301, 20)])\ncategorical_df.bmi=pd.cut(x=categorical_df.bmi, bins=[x for x in range(10, 101, 10)])\n\nfor column in [\"age\", \"avg_glucose_level\", \"bmi\"]:\n    dictionary_of_encodes[column]=dict( enumerate(categorical_df[column].cat.categories ) ) #Saving the encoding dictionary.\n    categorical_df[column]=categorical_df[column].cat.codes\n\ncategorical_df[[\"age\", \"avg_glucose_level\", \"bmi\"]]","413ce94e":"categorical_df.stroke.value_counts()","631aed2c":"#Adjusting Train Data\n\ntype1indices=[]\ntype0indices=[]\n\nfor x in range(categorical_df.shape[0]):\n    if categorical_df.stroke.iloc[x]==1:\n        type1indices.append(x)\n    else:\n        type0indices.append(x)\n        \nimport numpy as np\n\nnp.random.shuffle(type0indices)\nnp.random.shuffle(type1indices)\n\ntrain_x=categorical_df.iloc[type0indices[0:101]+type1indices[0:101], 0:-1]\ntrain_y=categorical_df.iloc[type0indices[0:101]+type1indices[0:101], -1]\nval_x=categorical_df.iloc[type0indices[101:]+type1indices[101:], 0:-1]\nval_y=categorical_df.iloc[type0indices[101:]+type1indices[101:], -1]","344cc9af":"from sklearn.ensemble import RandomForestRegressor\n\nmodel=RandomForestRegressor(random_state=1)\nmodel.fit(train_x, train_y)\npred_y=model.predict(val_x)\n\npred_y","6d7f619f":"comparing_results_df=pd.DataFrame(val_y) #Creating new DF.\ncomparing_results_df.columns=[\"actual\"] \ncomparing_results_df[\"pred\"]=pred_y #Adding new column for predicted values.\ncomparing_results_df['pred'] = np.where(comparing_results_df['pred']<=np.mean(pred_y), 0, 1) #adjusting values by the obtained probability.","c18ae16e":"results_df=pd.concat([val_x, comparing_results_df], axis=1).reset_index(drop=True)\n\nfor row_n in range(results_df.shape[0]):\n    for column in dictionary_of_encodes.keys():\n        results_df.loc[row_n, column]=dictionary_of_encodes[column][results_df.loc[row_n, column]]","eecd40d5":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ndef plotAndSaveConfusionMatrix(name_of_model, name_of_pic, actual_arr, pred_arr):\n    confmat = confusion_matrix(actual_arr, pred_arr)\n    fig, ax=plt.subplots(ncols=2, nrows=1, figsize=(20, 7))\n\n    sns.heatmap(confmat, cbar=False, annot=True, fmt=\"g\", ax=ax[0])\n    plt.suptitle(\"Confusion Matrix of \"+name_of_model+\" Model\", fontsize=23)\n    plt.sca(ax[0])\n    plt.ylabel(\"True Values\")\n    plt.xlabel(\"Predicted Values\")\n\n    ax[1].set_axis_off()\n    ax[1].text(0, 0, classification_report(actual_arr, pred_arr), fontsize=20)\n    plt.savefig(name_of_pic+'.png')\n    plt.show()\n    \nplotAndSaveConfusionMatrix(\"Random Forest\", \"cm_rf\", comparing_results_df['actual'], comparing_results_df['pred'])\nresults_df[results_df.actual==1].tail(n=20)","a30640c6":"from sklearn.linear_model import LogisticRegression \n\nmodel=LogisticRegression(max_iter=440)\nmodel.fit(train_x, train_y)\npred_y=model.predict(val_x)\nresults_df.pred=pred_y\n\npred_y","5a5b0a20":"plotAndSaveConfusionMatrix(\"Logistic Regression\", \"cm_lr\", results_df['actual'], pred_y)\nresults_df[results_df.actual==1].tail(n=20)","324741ff":"from sklearn.naive_bayes import MultinomialNB\n\nmodel=MultinomialNB(alpha=1)\nmodel.fit(train_x, train_y)\npred_y=model.predict(val_x)\nresults_df.pred=pred_y\n\npred_y","59f51c93":"plotAndSaveConfusionMatrix(\"Naive Bayes\", \"cm_nb\", results_df['actual'], pred_y)\nresults_df[results_df.actual==1].tail(n=20)","f5de6c79":"from sklearn.neighbors import KNeighborsClassifier\nfrom scipy import stats\n\nbest_score=-1\nbest_n=-1\n\nfor x in range(1, 101):\n    model=KNeighborsClassifier(n_neighbors=x)\n    model.fit(train_x, train_y)\n    pred_y=model.predict(val_x)\n    \n    score=np.sum(pred_y==val_y)\/len(val_y)\n    if score>best_score:\n        best_score=score\n        best_n=x\n        \nmodel=KNeighborsClassifier(n_neighbors=best_n)\nmodel.fit(train_x, train_y)\npred_y=model.predict(val_x)\nresults_df.pred=pred_y\n\nprint(stats.describe(pred_y), \"\\nBest N: \",best_n, sep=\"\")","f85721e4":"plotAndSaveConfusionMatrix(\"K-Nearest Neighbors\", \"cm_knb\", results_df['actual'], pred_y)\nresults_df[results_df.actual==1].tail(n=20)","80252e8a":"import matplotlib.image as mpimg\n\nfig, ax=plt.subplots(nrows=4, ncols=1, figsize=(20, 40))\nfig.tight_layout(h_pad=-80)\n\nax[0].imshow(mpimg.imread('cm_rf.png'))\nax[0].set_axis_off()\n\nax[1].imshow(mpimg.imread('cm_lr.png'))\nax[1].set_axis_off()\n\nax[2].imshow(mpimg.imread('cm_nb.png'))\nax[2].set_axis_off()\n\nax[3].imshow(mpimg.imread('cm_knb.png'))\nax[3].set_axis_off()","a579543c":"\nWe can also see that we only have missing data in the body mass index column, being a small amount we can get rid of those rows.","f21f0bf3":"## Precision Display","0766aa18":"# Data Cleaning\n\nBefore we could see that the column \"id\" will not be of much help to us, so we can eliminate it.","c7b00a7f":"# Classification By K-Nearest Neighbors","7f9334a7":"# Classification By Naive Bayes","f0bde678":"# Classification By Logistic Regression","ef97df82":"# Visualizing The Data","43fb99b4":"# Classification By Random Forest","5cdaa899":"### Decoding Values From Encoded Columns","5ba28719":"## Precision Display","06b06e1a":"## Precision Display","9afe74d1":"# Encoding Process","3569b8a9":"#### Creating Certain Ranges For Numeric Columns","b09e1f1a":"# Visualization of The Precision of All The Algorithms Used","a656c350":"# Visualizing The Distribution of The Data","67e42468":"# Importing and Looking at The Last 5 Rows of The Dataset","219ec957":"## Precision Display","7d1c8d06":"# Adjusting Training Data\n\nAfter having seen the distribution of the data we know that we have little data on people who had strokes.\n \nInstead of doing an oversampling we will collect the same amount of data from people who had and did not have strokes in a totally random way, then a test will be done with the remaining data (despite having a large majority of test data from people that they did not have strokes, this process is being done with the intention of being able to see the precision of some classification algorithms).","d6e2a419":"## Creation of a New Data Frame"}}