{"cell_type":{"1e425de4":"code","dbcbe07f":"code","8b51e89b":"code","ca3b3a1f":"code","34a41987":"code","57f0b740":"code","3e61b19d":"code","607f2f9e":"code","96acd835":"code","aad122e9":"code","806e5d26":"code","77db2a47":"code","b8d0c144":"code","bab83432":"code","988c51d2":"code","5a97130b":"code","f85e279d":"markdown","1a8656af":"markdown","f6ed19c0":"markdown","aaef4791":"markdown","55819431":"markdown","d0bb7587":"markdown","898a875a":"markdown","34c4315b":"markdown","7584c604":"markdown","0609c7f5":"markdown","434124a4":"markdown","a819e14f":"markdown","100a46f3":"markdown","74fb98e5":"markdown","5c5274e7":"markdown","126bd729":"markdown"},"source":{"1e425de4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dbcbe07f":"data = pd.read_csv('..\/input\/column_2C_weka.csv')","8b51e89b":"data.head()","ca3b3a1f":"# As you can see there is no labels in data\nx = data['pelvic_radius']\ny = data['degree_spondylolisthesis']\nplt.figure(figsize=(13,5))\nplt.scatter(x,y)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","34a41987":"df = data.loc[:, ['degree_spondylolisthesis', 'pelvic_radius']]\ndf.head()","57f0b740":"# which k value to choose\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(df)\n    wcss.append(kmeans.inertia_) # kmeans.inertia : calculate wcss\n    \nplt.plot(range(1,15), wcss, '-o')\nplt.xlabel('number of k (cluster) value')\nplt.ylabel('wcss')\nplt.show()","3e61b19d":"#if we choose k=2\nfrom sklearn.cluster import KMeans\nkmeans4 = KMeans(n_clusters = 2)\nclusters =kmeans4.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters\n\n# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","607f2f9e":"#if we choose k=3\nfrom sklearn.cluster import KMeans\nkmeans4 = KMeans(n_clusters = 3)\nclusters =kmeans4.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters\n\n# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","96acd835":"# if we choose k=4\nfrom sklearn.cluster import KMeans\nkmeans4 = KMeans(n_clusters = 5)\nclusters =kmeans4.fit_predict(df) # fit first and then predict\n\n# add labels for df\ndf['label'] = clusters\n\n# plot\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = clusters)\nplt.xlabel('pelvic_radius')\nplt.xlabel('degree_spondylolisthesis')\nplt.show()","aad122e9":"# plot\ncolors = [0 if i=='Abnormal' else 1 for i in data['class']] # to create colors\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'],c = colors)\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","806e5d26":"data = pd.read_csv('..\/input\/column_2C_weka.csv')\ndata3 = data.drop('class',axis = 1)","77db2a47":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 2)\npipe = make_pipeline(scalar,kmeans)\npipe.fit(data3)\nlabels = pipe.predict(data3)\ndf = pd.DataFrame({'labels':labels,\"class\":data['class']})\nct = pd.crosstab(df['labels'],df['class'])\nprint(ct)","b8d0c144":"# DENDOGRAM \n# here we will try to predict how many clusters we have \nfrom scipy.cluster.hierarchy import linkage, dendrogram # linkage: create dendrogram\ndf1 = data.loc[:, ['pelvic_radius', 'degree_spondylolisthesis']]\nmerg = linkage(df1, method='ward') # ward: cluster icindeki yayilimlari minimize et (wcss gibi bisey)\ndendrogram(merg, leaf_rotation=90)\nplt.xlabel('data points')\nplt.ylabel('euclidian distance')\nplt.show()","bab83432":"# PCA\nfrom sklearn.decomposition import PCA\nmodel = PCA()\nmodel.fit(data3)\ntransformed = model.transform(data3)\nprint('Principle components: ',model.components_)","988c51d2":"# PCA variance\nscaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler,pca)\npipeline.fit(data3)\n\nplt.bar(range(pca.n_components_), pca.explained_variance_)\nplt.xlabel('PCA feature')\nplt.ylabel('variance')\nplt.show()","5a97130b":"# apply PCA\ncolor_list=[\"red\",\"blue\"]\npca = PCA(n_components = 2)\npca.fit(data3)\ntransformed = pca.transform(data3)\nx = transformed[:,0]\ny = transformed[:,1]\nplt.scatter(x,y,c = color_list)\nplt.show()","f85e279d":"<a id=\"7\"><\/a> \n### STANDARDIZATION\n* Standardizaton is important for both supervised and unsupervised learning\n* Do not forget standardization as pre-processing\n* As we already have visualized data so you got the idea. Now we can use all features for clustering.\n* We can use pipeline like supervised learning.","1a8656af":"<a id=\"9\"><\/a> \n## Principle Component Analysis (PCA)\n1. Fundemental dimension reduction technique\n2. first step is decorrelation:\n3. rotates data samples to be aligned with axes\n4. shifts data asmples so they have mean zero\n5. no information lost\n6. fit() : learn how to shift samples\n7. transform(): apply the learned transformation. It can also be applies test data\n8. Resulting PCA features are not linearly correlated\n9. Principle components: directions of variance","f6ed19c0":"Reference : \n* https:\/\/www.displayr.com\/what-is-hierarchical-clustering\/\n* https:\/\/www.kaggle.com\/kanncaa1\/machine-learning-tutorial-for-beginners","aaef4791":"<a id=\"3\"><\/a> \n## Unsupervised Algorithms","55819431":"<a id=\"1\"><\/a> \n### Introduction\n * Unsupervised learning: It uses data that has unlabeled and uncover hidden patterns from unlabeled data. Example, there are orthopedic patients data that do not    have labels. You do not know which orthopedic patient is normal or abnormal.\n * As you know orthopedic patients data is labeled (supervised) data. It has target variables. In order to work on unsupervised learning, lets drop target variables and to visualize just consider pelvic_radius and degree_spondylolisthesis","d0bb7587":"<a href=\"https:\/\/ibb.co\/xztqkwv\"><img src=\"https:\/\/i.ibb.co\/MPxkvKT\/21.jpg\" alt=\"21\" border=\"0\"><\/a><br \/><a target='_blank' href='https:\/\/freeonlinedice.com\/'>play craps for free<\/a><br \/>\n\n<a href=\"https:\/\/ibb.co\/mhrgcyX\"><img src=\"https:\/\/i.ibb.co\/nQqtL1n\/22.jpg\" alt=\"22\" border=\"0\"><\/a>\n\n<a href=\"https:\/\/ibb.co\/bJGQ4n1\"><img src=\"https:\/\/i.ibb.co\/pykbHp2\/23.jpg\" alt=\"23\" border=\"0\"><\/a>","898a875a":"* Second step: intrinsic dimension: number of feature needed to approximate the data essential idea behind dimension reduction\n* PCA identifies intrinsic dimension when samples have any number of features\n* intrinsic dimension = number of PCA feature with significant variance\n* In order to choose intrinsic dimension try all of them and find best accuracy","34c4315b":"<a id=\"2\"><\/a> \n## Differences between Supervised and Unsupervised Algorithm \n\n<a href=\"https:\/\/ibb.co\/MpqvNM9\"><img src=\"https:\/\/i.ibb.co\/xGVk5J3\/1001.jpg\" alt=\"1001\" border=\"0\"><\/a><br \/><a target='_blank' href='https:\/\/babynamesetc.com\/boy\/unusual'>baby name malou<\/a><br \/>\n\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/i.ibb.co\/0GL7vNt\/100.jpg\" alt=\"100\" border=\"0\"><\/a>\n\n","7584c604":"<a id=\"10\"><\/a>\n## Conclusion\n","0609c7f5":"<a id=\"6\"><\/a> \n### Find optimum k value","434124a4":"<a id=\"8\"><\/a> \n## Hierarcical Clustering\n* Assign each data point as a cluster\n* Create a new cluster by choosing the closest two clusters\n* Repeat 2 until it remains only one cluster","a819e14f":"<a id=\"4\"><\/a> \n### K Means Algorithm\n\n* We choose a k value\n* Then it is randomly created k centroids\n* Every data point is clusterd according to the nearest centroid\n* By taking average of all data points that belog to a centroid, it is created new centroids.\n* Using these new centroids repeat 3 and 4\n* Finally, when centroids remain stationary, the algorith stops there.\n* As a result, according to these centroids, data is clustered\n\n<a href=\"https:\/\/ibb.co\/GcwmGn7\"><img src=\"https:\/\/i.ibb.co\/GcwmGn7\/102.jpg\" alt=\"102\" border=\"0\"><\/a>","100a46f3":"#### Original data is as follow","74fb98e5":"* According to elbow rule we can select 2,3 or 4  but the elbow point is not quite obvious here","5c5274e7":"<a id=\"5\"><\/a> \n### Selection of k value\n\n1. For k=1, run KMeans algorithm\n2. For each cluster (k cluster we have), it is calculated WCSS (within cluster sum of squares) value\n3. repeat 1 and 2 for 1<k<15\n4. obtain k vs WCSS plot\n5. Using elbow rule, choose the optimum k value to be used in K Means Algorithm","126bd729":"1. [Introduction)](#1)\n1. [Differences between Supervised and Unsupervised](#2)\n 1. [Unsupervised Algorithms](#3)\n    1. [K Means Algorithm](#4)\n       1. [Selection of k value](#5)\n       1. [Find optimum K value](#6)\n  1. [Standardization](#7)\n  1. [Hierarcial Clustering](#8)\n  1. [Principle Component Analysis](#9)\n1. [Conclusion](#10)  "}}