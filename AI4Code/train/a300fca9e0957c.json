{"cell_type":{"dc06bb7e":"code","5109e2bb":"code","ca61a6f4":"code","56a2e933":"code","fa9b9fa8":"code","aafa0a65":"code","81f45a2d":"code","f43924f8":"code","807246b2":"code","291c60a7":"code","70a852c3":"code","d3180562":"code","0e2e0a3a":"code","d27d954c":"code","0d721a0a":"code","440e3678":"code","61f536f2":"code","b170e2ca":"code","53045358":"code","adb2919d":"markdown","6d39ae71":"markdown","b93ec637":"markdown","9a4dd785":"markdown","663058bd":"markdown","f9ea84e5":"markdown","64e4516e":"markdown","3711fc74":"markdown","8c72e6cc":"markdown","7f659086":"markdown","dc6199de":"markdown","e5749b3a":"markdown","ffdea551":"markdown","8c8cfd4f":"markdown","aea403ca":"markdown","cd5a8f3d":"markdown","6dcb3b25":"markdown","0fcae8c3":"markdown","eaffe1a5":"markdown","ec1377af":"markdown","12744888":"markdown","efee0f20":"markdown","380a2108":"markdown","afccaef9":"markdown","1431adc2":"markdown"},"source":{"dc06bb7e":"#basic modules\nimport numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import Ridge","5109e2bb":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test  = pd.read_csv('..\/input\/test.csv')\n\nX = df_train.drop(['SalePrice', 'Id'], axis = 1)\ntrain_labels = df_train['SalePrice'].values\nX_test = df_test.drop(['Id'], axis = 1)","ca61a6f4":"#df_data_types = X.dtypes\n\nfor colname in X.columns:\n    print(colname + ': ' + str(X[colname].dtype))","56a2e933":"def prepro_change_column_type(x):\n    x['MSSubClass'] = x['MSSubClass'].astype('str')\n\n    \nprepro_change_column_type(X)\n\nprepro_test_list =[]\nprepro_test_list.append(prepro_change_column_type)\n    \n# df_train['MSSubClass'] = df_train['MSSubClass'].astype('str')\n# df_test['MSSubClass']  = df_test['MSSubClass'].astype('str')","fa9b9fa8":"#proportion of NaN in each column\ncount_var_nulls = X.isnull().sum(axis = 0)\/X.shape[0]\nvariable_nulls = count_var_nulls[count_var_nulls >0]\n\nprint('variables with NaN:')\nprint(variable_nulls)\nprint('-----')\n#remove columns with more than 50% of NaN\nremove_variables_index = list(variable_nulls[variable_nulls > 0.5].index)\nvariable_nulls.drop(remove_variables_index, inplace = True)  #prepro remove_variables_index\n\ndef prepro_nan_columns(x):\n    x.drop(remove_variables_index, axis =1,  inplace = True)\n\nprint('remaining variables with NaN after dropping those with more than 50% missing:')\nprint(variable_nulls)  \n\nprepro_nan_columns(X)\nprepro_test_list.append(prepro_nan_columns)","aafa0a65":"num_columns = X.select_dtypes(include=np.number).columns.tolist() \ncat_columns = X.select_dtypes(include=['object', 'category']).columns.tolist() ","81f45a2d":"count_obs_nulls = X.isnull().sum(axis = 1)\/X.shape[1]\nobs_nulls = count_obs_nulls[count_obs_nulls >0]\nremove_obs_index = list(obs_nulls[obs_nulls > 0.5].index)\nX.drop(remove_obs_index, axis = 1, inplace = True)\nprint(len(remove_obs_index),' observations removed because of having more than 50% of null values' )","f43924f8":"#prepro\ndef prepro_nan_objective_imputing(x):\n    #categorical\n    x['MasVnrType'].fillna('None', inplace = True)\n   \n    aux_list = ['BsmtQual', \n                  'BsmtCond', \n                  'BsmtExposure',\n                  'BsmtFinType1',\n                  'BsmtFinType2',\n                  'GarageType', \n                  'GarageFinish',\n                  'GarageQual', \n                  'GarageCond',\n                  'FireplaceQu']\n    \n    for i in aux_list:\n        x[i].fillna('NA', inplace = True)  \n        \n    #numerical\n    x['MasVnrArea'].fillna(0, inplace = True)    \n    \n  \n       \n    x.loc[x['BsmtCond'] == 'NA', ['BsmtUnfSF', \n                                   'TotalBsmtSF', \n                                   'BsmtFullBath', \n                                   'BsmtHalfBath', \n                                   'BsmtFinSF1', \n                                   'BsmtFinSF2' ]] = 0\n    \nprepro_nan_objective_imputing(X)\nprepro_test_list.append(prepro_nan_objective_imputing)","807246b2":"X = X.loc[\n    (  X['LotArea']<100000) \n    | (X['LotFrontage']<250)\n    | (X['1stFlrSF']<4000)\n    | (X['BsmtFinSF1']<5000) \n    | (X['BsmtFinSF2']<1400) \n    | (X['EnclosedPorch']<500)\n    | (X['GrLivArea']<5000)\n    | (X['TotalBsmtSF']<6000), ]","291c60a7":"numeric_transformer = Pipeline([\n                                (\"median\", SimpleImputer(strategy='median')),\n                                (\"standard\", StandardScaler())\n                               ]\n                              )\ncategorical_transformer = Pipeline([\n                                (\"mostfreq\", SimpleImputer(strategy = 'most_frequent')),\n                                (\"onehot\", OneHotEncoder(handle_unknown='ignore'))\n                                   ]\n                                  )","70a852c3":"prepro_transformer = ColumnTransformer([('num', numeric_transformer, num_columns),\n                                       ('cat', categorical_transformer, cat_columns)])","d3180562":"y = np.log10(train_labels)","0e2e0a3a":"pipeline = Pipeline([('prepro', prepro_transformer), ('estimator', Ridge())])","d27d954c":"param_grid = dict(estimator__alpha =  np.linspace(10,100, 100))\ngrid_search = GridSearchCV(pipeline, param_grid, scoring = 'neg_mean_squared_error', cv = 5, verbose = 0)","0d721a0a":"grid_search.fit(X, y)","440e3678":"best_model = grid_search.best_estimator_","61f536f2":"for func in prepro_test_list:\n    func(X_test)","b170e2ca":"y_pred   = 10**best_model.predict(X_test)\ndf_submit = pd.DataFrame({'Id':df_test['Id'], 'SalePrice': y_pred})\ndf_submit.to_csv('df_submit.csv', index = False)","53045358":"grid_search.best_params_","adb2919d":"There are a few cases where missing values can be imputed objectively from the knowledge of this particular dataset. We will do that for these cases instead of using an automatic imputation strategy at a later stage.","6d39ae71":"### Automatic missing values imputation and normalization","b93ec637":"## 1. Preprocessing","9a4dd785":"We show the type of each column","663058bd":"## 2. Modeling","f9ea84e5":"We remove variables with more than 50% of missing values since they will not add value to the dataset","64e4516e":"### Outliers for numerical variables","3711fc74":"We now look at outliers. We will remove a few of the rows presenting outliers in the train set for fitting a more robust model. The following code chunk (need to uncomment) generates a histogram for each numerical variable that is of assistance in choosing appropriate thresholds for declaring values as outliers.","8c72e6cc":"We now prepare transformers for automatic imputation of missing values in numeric and categorical variables. The strategies will be the median and most frequent value imputation respectively. After that We will standardize the numeric variables and one-hot encode the categorical ones.\n\nSince we are going to fit our model using cross validation we cannot do this in an objective way with the original dataset, since for each cross validation round these median and most common value may vary. Hence the need for an automatic process that can be included in a pipeline.\n","7f659086":"We start by loading the necessary libraries","dc6199de":"### Objective treatment of missing values","e5749b3a":"We first preprocess the test set in the same way as the train set","ffdea551":"We log-transform the target variable since the metric used by Kaggle for this competition is RMSLE and create a pipeline encapsulating the preprocessing steps and the final estimator that will be used.","8c8cfd4f":"# Kaggle - House Prices \/ Advanced Regression Techniques","aea403ca":"### Objective Imputation of NaN in variables","cd5a8f3d":"We make the predictions and prepare the csv file for submitting to Kaggle","6dcb3b25":"## 3 Predicting","0fcae8c3":"We remove observations with more than 50% of missing values since they will not add value to the dataset","eaffe1a5":"as we see, the variable MSSubClass has the wrong type, since it should be categorical. We change its type to string in both the training and test set","ec1377af":"We create lists with the remaining column names for numeric and categorical variables, for later reference","12744888":"### Loading modules and data","efee0f20":"### Choosing models and tuning parameters","380a2108":"This notebook deals with the Kaggle competition \"House Prices: Advanced Regression Techniques\" (https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) and generates a response dataset that, once submitted, gets a score below 0.12 RMSLE. It is possible to obtain better results using ensemble methods encapsulating several algorithms. However the purpose of this notebook is to be able to tune efficiently the hyperparameters of a linear regression algorithm in order to get a good result.","afccaef9":"We apply a Grid Search approach to find optimal hyperparameters for the algorithm.","1431adc2":"We now load the data"}}