{"cell_type":{"3afc0434":"code","f2d3b1f3":"code","8fbfaeec":"code","83794768":"code","f41d3c76":"code","fabb6fab":"code","11ea6673":"code","c9cdb032":"code","7fedf1aa":"code","080682f1":"code","8abe5322":"code","07dea2f6":"code","1acb83c4":"code","c7fa9db1":"code","2fc1d153":"code","127337de":"code","c2a5dca3":"code","de688f86":"code","47a4d3ce":"code","a92dc9ff":"code","d1f6a772":"code","52158cee":"code","526f3893":"code","e232fa02":"code","56a5028b":"code","f8d00d26":"code","1d623f45":"code","2f57fa3e":"code","98e02d09":"code","95f3f82c":"code","19f9286a":"code","228486fc":"code","f0e21de0":"code","1bd055d6":"code","78cae2ec":"code","37bb881d":"code","28436a37":"code","bcf1aa50":"code","49e1947a":"code","bbf229f0":"code","03f41757":"code","bf0ae522":"code","6accb293":"code","b75d746b":"code","1e5274a7":"code","9fb556b9":"code","3679f33b":"code","b0981ed2":"code","2d3ec810":"code","6ca22861":"code","193bfd46":"code","b3230186":"code","ea81e331":"markdown","9888bb78":"markdown","6359deb6":"markdown","f45fd611":"markdown","c8a10c1c":"markdown","0dc24bf9":"markdown","12b43c52":"markdown","e0051636":"markdown","bdbce34d":"markdown","d8805a92":"markdown","e773f9e7":"markdown","6d56490e":"markdown","a29e6dc2":"markdown","24820568":"markdown","a7927fbc":"markdown","5e179b54":"markdown","54c3e8db":"markdown","9f3995a1":"markdown","e7741925":"markdown","5e5cc78d":"markdown","1aa6d9c5":"markdown","0c24f1c1":"markdown"},"source":{"3afc0434":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2d3b1f3":"import tensorflow as tf\nimport numpy as np\nimport os\nimport pandas as pd\n\n%matplotlib inline\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport itertools\nfrom tensorflow import keras","8fbfaeec":"tf.version.VERSION","83794768":"import os\nbase_dir = os.path.join('\/kaggle\/input\/pokemon-images-dataset\/pokemon_jpg')\npokemon_pictures = os.listdir(os.path.join(base_dir, \"pokemon_jpg\"))\nprint(pokemon_pictures[:10])","f41d3c76":"print(\"Dataset contains \" + str(len(pokemon_pictures)) + \" pictures\")","fabb6fab":"from PIL import ImageOps, Image\nsize = 64, 64\n\nfor f in os.listdir(os.path.join(base_dir, \"pokemon_jpg\")):\n    im = Image.open(os.path.join(base_dir, \"pokemon_jpg\", f)).resize(size, Image.ANTIALIAS)\n    break\n\nbig_arr = np.array([np.array(im)]).reshape(1, 64, 64, 3)\nfor f in os.listdir(os.path.join(base_dir,\"pokemon_jpg\"))[1:]:\n    big_arr = np.append(big_arr, [np.array(Image.open(os.path.join(base_dir, \"pokemon_jpg\", f)).resize(size, Image.ANTIALIAS)).reshape(64, 64, 3)], axis=0)\n    #i+=1\n    \nbig_arr = big_arr\/255","11ea6673":"x_train = big_arr[:600]\nx_test = big_arr[600:800]\nprint(\"full array shape: \" + str(big_arr.shape))\nprint(\"train shape: \" + str(x_train.shape))\nprint(\"test shape: \" + str(x_test.shape) + \"\\n\")\n\nprint(\"Example Image: \")\nplt.imshow(big_arr[0])\nplt.axis('off')\nplt.show()","c9cdb032":"big_arr3 = big_arr.reshape((len(big_arr), np.prod(big_arr.shape[1:])))\nx_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\nx_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n\nprint(\"full array shape: \" + str(big_arr3.shape))\nprint(\"x_train shape: \" + str(x_train.shape))\nprint(\"x_test shape: \" + str(x_test.shape))","7fedf1aa":"from tensorflow.keras import regularizers\ninput_shape = 64*64*3\n#encoding_dim = 100\n\ninput_img = keras.Input(shape=(input_shape,))\nencoded = keras.layers.Dense(64, activation='relu')(input_img)\nencoded = keras.layers.Dense(32, activation='relu')(encoded)\n\ndecoded = keras.layers.Dense(64, activation='relu')(encoded)\ndecoded2 = keras.layers.Dense(input_shape, activation='sigmoid')(decoded)\n\n# model which maps input with reconstruction\nautoencoder = keras.Model(input_img, decoded2)","080682f1":"autoencoder.summary()","8abe5322":"autoencoder.compile(optimizer='adam',loss='binary_crossentropy')\nautoencoder.fit(x_train, x_train, epochs=50, batch_size=10, shuffle=True, validation_data=(x_test, x_test), verbose=10)","07dea2f6":"decoded_imgs = autoencoder.predict(x_test)\ndecoded_imgs.shape","1acb83c4":"import matplotlib.pyplot as plt\n\nn = 10  # How many pictures we will display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(64, 64, 3))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(64, 64, 3))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","c7fa9db1":"input_img = tf.keras.Input(shape=(64, 64, 3))\n\nx = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\nx = keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nencoded = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n\nx = keras.layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\ndecoded = keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = keras.Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')","2fc1d153":"autoencoder.summary()","127337de":"x_train = big_arr[:600]\nx_test = big_arr[600:800]\n\nprint(\"x_train shape: \" + str(x_train.shape))\nprint(\"x_test shape: \" + str(x_test.shape))","c2a5dca3":"# Load the TensorBoard notebook extension\n%load_ext tensorboard\nimport datetime","de688f86":"log_dir = \".\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)","47a4d3ce":"autoencoder.fit(x_train, x_train,\n               epochs=50,\n               batch_size=128,\n               shuffle=True,\n               validation_data=(x_test, x_test),\n               verbose = 3,\n               callbacks=[tensorboard_callback])","a92dc9ff":"%tensorboard --logdir logs\/fit","d1f6a772":"decoded_imgs = autoencoder.predict(x_test)","52158cee":"import matplotlib.pyplot as plt\n\nn = 10  # How many pictures to display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test[i].reshape(64, 64, 3))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(64, 64, 3))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()\n","526f3893":"noise_factor = 0.5\nx_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \nx_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n\nx_train_noisy = np.clip(x_train_noisy, 0., 1.)\nx_test_noisy = np.clip(x_test_noisy, 0., 1.)","e232fa02":"n = 10\nplt.figure(figsize=(20, 2))\nfor i in range(1, n+1):\n    ax = plt.subplot(1, n, i)\n    plt.imshow(x_test_noisy[i].reshape(64, 64, 3))\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","56a5028b":"input_img = tf.keras.Input(shape=(64, 64, 3))\n\nx = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\nx = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\nx = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nencoded = keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n\nx = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\nx = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\nx = tf.keras.layers.UpSampling2D((2, 2))(x)\ndecoded = keras.layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n\nautoencoder = keras.Model(input_img, decoded)\nautoencoder.compile(optimizer='adam', loss='binary_crossentropy')","f8d00d26":"autoencoder.summary()","1d623f45":"autoencoder.fit(x_train_noisy, x_train,\n               epochs=50,\n               batch_size=128,\n               shuffle=True,\n               validation_data=(x_test_noisy, x_test),\n               callbacks=[tensorboard_callback])","2f57fa3e":"decoded_imgs = autoencoder.predict(x_test_noisy)","98e02d09":"n = 10  # How many pictures to display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    # Display original\n    ax = plt.subplot(2, n, i + 1)\n    plt.imshow(x_test_noisy[i].reshape(64, 64, 3))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # Display reconstruction\n    ax = plt.subplot(2, n, i + 1 + n)\n    plt.imshow(decoded_imgs[i].reshape(64, 64, 3))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","95f3f82c":"def define_discriminator(in_shape=(64, 64, 3)):\n    model = tf.keras.Sequential()\n    model.add(keras.layers.Conv2D(64, (3, 3), strides=(2,2), padding='same', input_shape=in_shape))\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(keras.layers.Conv2D(64, (3, 3), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    model.add(tf.keras.layers.Dropout(0.4))\n    model.add(tf.keras.layers.Flatten())\n    model.add(keras.layers.Dense(1, activation='sigmoid'))\n    # compile model\n    opt = keras.optimizers.Adam(lr=0.0002, beta_1=0.5)\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model","19f9286a":"model = define_discriminator()","228486fc":"# the aggressive 2x2 stride acts to down-sample the input image, first from 64x64 to 32x32, then to 16x16 before the model makes\n# the output prediction\nmodel.summary()","f0e21de0":"x_train = big_arr[:600]\nx_test = big_arr[600:800]","1bd055d6":"import random\ndef generate_real_samples(dataset, n_samples):\n    rand_selection = []\n    for i in range(n_samples):\n        rand_selection.append(random.randint(0, dataset.shape[0]-1))\n    X = dataset[rand_selection]\n    y = np.ones((n_samples, 1))\n    return X, y","78cae2ec":"real_samples_to_generate = 128\ntest_x_train, test_y_train = generate_real_samples(x_train, real_samples_to_generate)\n\nprint(\"test_x_train shape: \" + str(test_x_train.shape))\nprint(\"test_y_train shape: \" + str(test_y_train.shape))","37bb881d":"# next, I want to be able to generate fake samples with images comprised of random pixel values\ndef generate_fake_samples(n_samples):\n    # generate uniform random numbers in [0,1]\n    X = np.random.rand(64 * 64 * 3 * n_samples)\n    # reshape into a batch of images\n    X = X.reshape((n_samples, 64, 64, 3))\n    # generate 'fake' class labels (0)\n    y = np.zeros((n_samples, 1))\n    return X, y","28436a37":"# train the discriminator model\ndef train_discriminator(model, dataset, n_iter=100, n_batch = 200):\n    half_batch = int(n_batch\/2)\n    # manually enumerate epochs\n    for i in range(n_iter):\n        # get randomly selected 'real' samples\n        X_real, y_real = generate_real_samples(x_train, half_batch)\n        # update the discriminator on real samples\n        _, real_acc = model.train_on_batch(X_real, y_real)\n        # generate 'fake' examples\n        X_fake, y_fake = generate_fake_samples(half_batch)\n        # update discriminator on fake samples\n        _, fake_acc = model.train_on_batch(X_fake, y_fake)\n        # summarize performance\n        print('>%d real=%.0f%% fake=%.0f%%' % (i+1, real_acc*100, fake_acc*100))","bcf1aa50":"train_discriminator(model, x_train)","49e1947a":"# define the standalone generator model\n# https:\/\/machinelearningmastery.com\/how-to-develop-a-generative-adversarial-network-for-an-mnist-handwritten-digits-from-scratch-in-keras\/\ndef define_generator(latent_dim):\n    model = tf.keras.Sequential()\n    # foundation for the 16x16 image\n    n_nodes = 128 * 16 * 16\n    model.add(tf.keras.layers.Dense(n_nodes, input_dim = latent_dim))\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    model.add(tf.keras.layers.Reshape((16, 16, 128)))\n    # upsample to 32x32\n    # with a Conv2DTranspose layer, it is good practice to use a kernel size that is a factor of the stride (double)\n    # to avoid a checkerboard pattern that can be observed when upsampling\n    model.add(tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    # upsample to 64x64\n    model.add(tf.keras.layers.Conv2DTranspose(128, (4, 4), strides=(2,2), padding='same'))\n    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n    # the output layer is a Conv2D with 3 filters and a kernel size of 7x7 and 'same' padding, designed to create a\n    # 3 feature map and preserve its dimensions at 64x64 pixels\n    # a sigmoid activation is used to ensure output values are in the desired range of [0,1]\n    model.add(tf.keras.layers.Conv2D(3, (7, 7), activation='sigmoid', padding='same'))\n    return model","bbf229f0":"latent_dim = 100\nmodel = define_generator(latent_dim)\nmodel.summary()","03f41757":"def generate_latent_points(latent_dim, n_samples):\n    # generate points in the latent space\n    x_input = np.random.randn(latent_dim * n_samples)\n    # reshape into a batch of inputs for the network\n    x_input = x_input.reshape(n_samples, latent_dim)\n    return x_input","bf0ae522":"# use the generator to generate n fake examples, with class labels\ndef generate_fake_samples(g_model, latent_dim, n_samples):\n    # generate points in latent space\n    x_input = generate_latent_points(latent_dim, n_samples)\n    X = g_model.predict(x_input)\n    # create 'fake' class labels\n    y = np.zeros((n_samples, 1))\n    return X, y","6accb293":"#import pyplot as plt\n# size of the latent space\nlatent_dim = 100\n# define the generator model\nmodel = define_generator(latent_dim)\n# generate the samples\nn_samples = 25\nX, _ = generate_fake_samples(model, latent_dim, n_samples)\n# plot the generated samples\nfor i in range(n_samples):\n    ax = plt.subplot(5, 5, i+1)\n    plt.imshow(X[i].reshape(64, 64, 3))\n    #plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","b75d746b":"# define the combined generator and dscriminator model for updating the generator\ndef define_gan(g_model, d_model):\n    # make weight in the discriminator not trainable\n    d_model.trainable = False\n    # connect them\n    model = tf.keras.Sequential()\n    model.add(g_model)\n    model.add(d_model)\n    # I'm going to add an Adam optimizer with lr of 0.0002 and momentum of 0.5, which is recommended\n    # when training deep convolutional GANs\n    opt = tf.keras.optimizers.Adam(lr = 0.0002, beta_1=0.5)\n    model.compile(loss='binary_crossentropy', optimizer=opt)\n    return model","1e5274a7":"# size of the latent space\nlatent_dim = 100\n# create the discriminator\nd_model = define_discriminator()\n# create the generator\ng_model = define_generator(latent_dim)\n# create the gan\ngan_model = define_gan(g_model, d_model)","9fb556b9":"# summarize gan model\ngan_model.summary()","3679f33b":"# train the composite model\n# the number of batches within an epoch is defined by how many times the batch size divides into the training set\n# Since I have a dataset size of 600 samples and batch_size  of 200, there are 3 batches per epoch\ndef train_gan(gan_model, latent_dim, n_epochs=30, n_batch=3):\n    for i in range(n_epochs):\n        # prepare points in latent space as input for the generator\n        x_gan = generate_latent_points(latent_dim, n_batch)\n        # create inverted labels for the fake samples\n        y_gan = np.ones((n_batch, 1))\n        # update the generator via the discriminator's error\n        gan_model.train_on_batch(x_gan, y_gan)","b0981ed2":"# evaluate the discriminator, plot generated images, save generator model\ndef summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=100):\n    # prepare real samples\n    X_real, y_real = generate_real_samples(dataset, n_samples)\n    # evaluate disciminator on real samples\n    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n    # prepare fake examples\n    x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\n    # evaluate discriminator on fake examples\n    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n    # summarize discriminator performance\n    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))","2d3ec810":"\n# train the generator and discriminator\ndef train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=30, n_batch=3):\n    bat_per_epo = int(dataset.shape[0] \/ n_batch)\n    half_batch = int(n_batch \/ 2)\n    # manually enumerate epochs\n    for i in range(n_epochs):\n        # enumerate batches over the training set\n        for j in range(bat_per_epo):\n            # get randomly selected 'real' samples\n            X_real, y_real = generate_real_samples(dataset, half_batch)\n            # generate fake examples\n            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n            # create training set for discriminator\n            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n            # update discriminator model weights\n            d_loss, _ = d_model.train_on_batch(X, y)\n            # prepare points in latent space as input for the generator\n            X_gan = generate_latent_points(latent_dim, n_batch)\n            # create inverted labels for the fake samples\n            y_gan = np.ones((n_batch, 1))\n            # update the generator via the discriminator's error\n            g_loss = gan_model.train_on_batch(X_gan, y_gan)\n            # summarize loss on this batch\n            print('>%d, %d%d, d=%.3f, g=%.3f' % (i+1, j+1, bat_per_epo, d_loss, g_loss))\n            # evaluate model performance, somtimes\n            if (i+1) % 10 == 0:\n                summarize_performance(i , g_model, d_model, dataset, latent_dim)","6ca22861":"# size of the latent space\nlatent_dim = 100\n# create the discriminator\nd_model = define_discriminator()\n# create the generator\ng_model = define_generator(latent_dim)\n# create the gan\ngan_model = define_gan(g_model, d_model)\n# load image data\n#dataset = load_real_samples()\n# train model\ntrain(g_model, d_model, gan_model, x_train, latent_dim)","193bfd46":"latent_points = generate_latent_points(100, 25)\ntest_this = model.predict(latent_points)","b3230186":"\nn = 10 # how many pictures to display\nplt.figure(figsize=(20, 4))\nfor i in range(n):\n    ax = plt.subplot(1, n, i+1)\n    plt.imshow(test_this[i].reshape(64, 64, 3))\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\nplt.show()","ea81e331":"## Training the Generator Model\nThe weights in the generator model are updated based on the performance of the discriminator model. When the discriminator model is good at detecting fake samples, teh generator is updated more, and when the discriminator model is relatively poor or confused when detecting fake samples, the generator model is updated less.\n\nI'm going to do this by creating a new model that combines the generator and discriminator models. I'll do this by stacking the generator and discriminator so that the generator receives as input random points in the latent space and generates samples that are fed into the discriminator model directly and then are classified and the output of this larger model can be used to update the model weights of the generator.\n\nThe discriminator is only concerned with distinguishing between real and fake samples, so it can be trained in a standalone manner on examples of each.\n\nThe generator model is only concerned with the discriminator's performance on fake samples. Therefore, I will mark all of the layers in the discriminator as not trainable when it is part of the GAN model so that they can not be updated and overtrained on fake examples.\n\nWhen training the generator in this GAN model, I want the discriminator to think that the samples output by the generator are real, not fake. Therefore, when the generator is trained as part of the GAN model, I will mark the generated samples as real (class 1)","9888bb78":"## Create generator to generate fake samples\nNext, I want to write a function that will generate fake samples. These fake samples will be created","6359deb6":"Below, I can see the original images compared with the images fed through the autoencoder. The images are fuzzy and some of the color has been removed, but the autoencoder is able to pick up on the main features.","f45fd611":"Feed the test images, which have never been seen before, through the autoencoder.","c8a10c1c":"The first step is to generate new points in the latent space. This can be done by calling np.randn() for generating arrays of random numbers drawn from a standard Gaussian. The array of random numbers can then be reshaped into samples, which is n rows with 100 elements per row. I will do this with the generate_latent_points() function","0dc24bf9":"## Evaluating GAN Model Performance\nCurrently, there are no objective ways to evaluate the performance of a GAN model, so the images must be subjectively evaluated for quality by a human operator. So, I cannot know when to stop training without looking at examples of generated images. Due to the adversarial nature of the training process, the generator is changing after every batch. So once \"good enough\" images can be generated, the subjective quality of the images may then begin to vary, improve, or degrade with subsequent updates.\n\nIn order to handle this complex training situation, I will:\n\nperiodically evaluate the classification accuracy of the discriminator on real and fake images\nperiodically generate many images and save them for subjective review\nperiodically save the generator model","12b43c52":"I can then plot the generated samples by calling the imshow() function. Since the model is not trained, the generated images are complately random pixel values","e0051636":"# Application to image denoising\nSince the aim of an autoencoder is to learn a representation for a set of data, the autoencoder needs to focus on the main features of the data. Therefore, autoencoders are good at removing noise from images. Below, I'm going to create a dataset of images with gaussian noise. Then I'm going to feed those images through a convolutional autoencoder to train it. Finally, I'm going to test how well the autoencoder performed.","bdbce34d":"# GANs\nA Generative Adversarial Network (GAN) is an algorithmic architecture that uses two neural networks, putting one against the other in order to generate new, synthetic instances of data that can pass for real data. One neural network, called the generator, generates new data instances. The other neural network, called the discriminator, evaluates them for authenticity\n\n## Define and Train the Discriminator Model\nFirst, I'm going to create a neural network that can discriminate between the real images and the fake images. This will take the (64, 64, 3) images and feed them through a neural network. I'm going to use convolutions because they perform better. Also, I'm going to use dropout for regularization. This is a binary classification task, so the output layer will be composed of a single neuron with a sigmoid activation function.","d8805a92":"# Importing the data as an array\nFirst, I set up the base directory and find the number of pictures that I have. Then I iterate through all of the images, open them, resize them to be 64x64x3, divide the pixel density by 255 so every pixel is in the range of 0:1, add them to an array of arrays, and then split into the training and test set.","e773f9e7":"In order to improve the quality of the reconstructed, I'm going to use a slightly different model with more filters per layer","6d56490e":"Next, I will define a generate_fake_samples() function that returns both the generated samples and the associated class labels","a29e6dc2":"# Convolutional Autoencoder\nNext, I'm going to test how a convolutional autoencoder will perform. I feed the original image through two Conv2D\/MaxPooling2D layers in order to encode the image down to 32 neurons. Then I use two Conv2D\/UpSampling2D layers to decode the image back to its original size. I'm setting the input shape as (64, 64, 3), so I will need to reshape my x_train and x_test back to a shape of (64, 64, 3).","24820568":"Next, I'm going to fit the model. The input is x_train and the output is also x_train since the goal is for the autoencoder to encode the images, feed them through a bottleneck, and then decode them to be as close as possible to the original image.","a7927fbc":"First, I will update the discriminator model with real and fake samples, then update the generator via the composite model. I'll be doing this via the train_discriminator() function and then the train_gan() function.\n\nThe discriminator model is updated once per batch by combining one half of a batch of fake and real examples into a single batch.\n\nFinally, I will report the loss each batch so I can keep an eye on the loss over batches. The reason for this is that a crash in the discriminator loss indicates that the generator model has started generating bad examples that the discriminator can easily discriminate.","5e179b54":"In order to compare different runs, I'm going to use tensorboard and the tensorboard_callback.","54c3e8db":"## Create generator to generate real samples\nNow I'm going to write a function that will take the training dataset as an argument and will select a random subsample of images. It will also return class labels for the sample, specifically a class label of 1, to indicate real images.","9f3995a1":"Below, I can see that in this case, the loss remains stable over the course of training","e7741925":"Here, I can see the model expects the pokemon images as input and predicts a single value as output","5e5cc78d":"The generator model is responsible for creating new, fake but plausible images by taking a point from the latent space as input and outputting a square grayscale image. The latent space is an arbitrarily defined vector space of Gaussian-distributed values. It has no meaning, but by drawing points from this space randomly and providing them to the generator model during training, the generator model will assign meaning to the latent points until, at the end of training, the latent vector space represents a compressed representation of the output space of pokemon images that only the generator knows how to turn into plausible pokemon images.\n\nThe generator model is not compiled and does not specify a loss function or optimization algorithm. This is because the generator is not trained directly.","1aa6d9c5":"# Pokemon Images Autoencoder and GAN\nIn this project, I experiment with creating autoencoders using dense layers and convolutional \/ pooling layers. Then I use the autoencoder in order to remove gaussian noise from images. After that, I build and train a GAN.","0c24f1c1":"\n# Creating a supervised learning problem out of an unlabeled dataset\nI can take an unlabeled dataset and frame it as a supervised learning problem tasked with outputting x, a reconstruction of the original input x. This network can be trained by minimizing the reconstruction error, L(x, x^), which measures the differences between the original input and the consequent reconstruction. A bottleneck constrains the amount of information that can traverse the full network, forcing a learned compression of the input data. In this case the image, which contains 12288 is fed through a bottleneck which contains only 32 neurons. Then it is reconstructed back to its original size\n\nFor this first autoencoder, I'm using a series of dense layers to encode and decode the images."}}