{"cell_type":{"5b17b42e":"code","b37a6041":"code","2cb77cf5":"code","64e295d5":"code","d0237f4d":"code","6e90ec1f":"code","a8389956":"code","05f1b5d5":"code","c20b1051":"code","75abe36a":"code","5915144d":"code","ce355a19":"code","3968c554":"code","eb4ddbe7":"code","f2486154":"code","158bd0df":"code","ed9ffc12":"code","b3407d20":"code","b6fb7247":"code","67b48c30":"code","45d12e6b":"code","ed17106d":"code","5c264756":"code","35136f62":"code","6650ee38":"code","c1851a34":"code","ec647d0e":"code","6dae3f30":"code","d6188331":"code","e5464b80":"code","bd746830":"code","4e18ec03":"code","cc9e9683":"code","2911b86a":"code","c4e3d12a":"code","dc5c9af4":"code","00b6ec2b":"code","2a2f7ed2":"code","f558d377":"code","7b20b852":"code","54f68bd9":"code","6c09efa4":"code","cece4009":"code","bfe223e1":"code","864ed563":"code","fa9ec2f9":"code","2b4c0a90":"code","c0e8a929":"code","0f4c43ac":"code","50d02a94":"markdown","d2a81ea5":"markdown","5bed6c6d":"markdown","21dbc287":"markdown","3b79a0a1":"markdown","ae734eba":"markdown","c1b5959b":"markdown","d9be95fa":"markdown","ffd82b08":"markdown","8e9a96c3":"markdown","7937723f":"markdown","28d45275":"markdown","327a6580":"markdown","c0ff2ad2":"markdown","adf32481":"markdown","21aeeddb":"markdown","2027df0f":"markdown","2ea69c02":"markdown","ea75d43b":"markdown"},"source":{"5b17b42e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path \npath = Path('\/kaggle\/input\/store-sales-time-series-forecasting\/')","b37a6041":"!pip install pytorch-forecasting","2cb77cf5":"df_train = pd.read_csv(path \/ 'train.csv', parse_dates=['date'], infer_datetime_format=True)\ndf_train.shape","64e295d5":"df_test = pd.read_csv(path \/ 'test.csv', parse_dates=['date'], infer_datetime_format=True)\ndf_test['date'].value_counts()","d0237f4d":"prediction_steps = df_test['date'].nunique()","6e90ec1f":"stores = pd.read_csv(path \/ 'stores.csv')\nstores = stores.rename(columns={\"type\": \"store_type\", 'cluster': 'store_cluster'})\nstores.head()","a8389956":"df_train = pd.merge(df_train, stores, on='store_nbr', how='left')\ndf_train.shape\n","05f1b5d5":"holidays = pd.read_csv(path \/ 'holidays_events.csv', parse_dates=['date'], infer_datetime_format=True)\n#Keep only not transferrred holidays\nholidays = holidays.loc[holidays['transferred'] == False]\n\nholidays_nat = holidays[holidays['locale']=='National'].drop_duplicates(subset='date')\nholidays_reg = holidays[holidays['locale']=='Regional'].drop_duplicates(subset='date')\nholidays_loc = holidays[holidays['locale']=='Local'].drop_duplicates(subset='date')\n\ndf_train = pd.merge(df_train, holidays_nat[['date','description']], on='date', how='left').rename(columns={'description': 'holiday_nat'})\ndf_train = pd.merge(df_train, holidays_reg[['date', 'locale_name', 'description']], left_on=['date', 'state'], right_on=['date', 'locale_name'], how='left').rename(columns={'description': 'holiday_reg'}).drop(columns=['locale_name'])\ndf_train = pd.merge(df_train, holidays_loc[['date', 'locale_name', 'description']], left_on=['date', 'city'], right_on=['date', 'locale_name'], how='left').rename(columns={'description': 'holiday_loc'}).drop(columns=['locale_name'])\n\ndf_train[[\"holiday_nat\", \"holiday_reg\", \"holiday_loc\"]] = df_train[[\"holiday_nat\", \"holiday_reg\", \"holiday_loc\"]].fillna(\"No\")\n\ndf_train.shape","c20b1051":"oil = pd.read_csv(path\/ 'oil.csv', parse_dates=['date'], infer_datetime_format=True)","75abe36a":"df_train = pd.merge(df_train, oil, on='date', how='left')\ndf_train.shape","5915144d":"transactions = pd.read_csv(path\/ 'transactions.csv', parse_dates=['date'], infer_datetime_format=True)\ntransactions.tail()","ce355a19":"df_train = pd.merge(df_train, transactions, on=['store_nbr', 'date'], how='left')\ndf_train.shape","3968c554":"from scipy.stats import skewnorm\nearthquake = pd.DataFrame()\nearthquake[\"date\"] = pd.date_range(\"2016-04-17\",\"2016-05-16\")\nearthquake['earthquake_effect'] =  [2*skewnorm.pdf(i\/20, 0.5) for i in range(len(earthquake))]\n\n\ndf_train = pd.merge(df_train, earthquake, on='date', how='left')\ndf_train['earthquake_effect'].fillna(0, inplace=True)\ndf_train.shape","eb4ddbe7":"df_train","f2486154":"import plotly.express as px\npx.line(df_train[(df_train['date'] > pd.to_datetime(\"2016-03-16\"))&(df_train['date'] < pd.to_datetime(\"2016-06-16\"))&(df_train['store_nbr']==2)&(df_train['family']=='AUTOMOTIVE')], x='date', y=['earthquake_effect', 'sales'])","158bd0df":"\ndef get_distance_from_paydays(date):\n    end_of_month = date.daysinmonth\n    distance_to_1st = 0 if date.day >=15 else 15 - date.day\n    distance_to15th = 0 if date.day < 15 else end_of_month - date.day\n    return distance_to_1st + distance_to15th\n\ndf_train['days_from_payday'] = df_train['date'].apply(get_distance_from_paydays)","ed9ffc12":"df_train['average_sales_by_family'] = df_train.groupby([\"date\", 'family'], observed=True).sales.transform('mean')\ndf_train['average_sales_by_store'] = df_train.groupby([\"date\", 'store_nbr'], observed=True).sales.transform('mean')","b3407d20":"df_train['dcoilwtico'] = df_train['dcoilwtico'].interpolate().fillna(method='bfill')\ndf_train['transactions'] = df_train['transactions'].interpolate().fillna(method='bfill')\ndf_train['dayofweek'] = df_train['date'].dt.dayofweek.astype('str').astype('category')\ndf_train['month'] = df_train['date'].dt.month.astype('str').astype('category')\ndf_train['dayofyear'] = df_train['date'].dt.dayofyear.astype('str').astype('category')\n\nfor cat_col in ['holiday_nat', 'holiday_reg', 'holiday_loc','city','state' , 'store_type', 'store_cluster', 'store_nbr', 'family']:\n    df_train[cat_col] = df_train[cat_col].astype(str).astype('category')\n\n\ndf_train['time_idx'] = (df_train['date'].dt.date - df_train['date'].dt.date.min()).dt.days","b6fb7247":"df_train.info()","67b48c30":"df_train.isna().sum()","45d12e6b":"from pytorch_forecasting import TimeSeriesDataSet, Baseline, TemporalFusionTransformer\nfrom pytorch_forecasting.data import GroupNormalizer","ed17106d":"max_prediction_length = prediction_steps\nmax_encoder_length = 60 # Go back  60 Days \ntraining_cutoff = df_train[\"time_idx\"].max() - max_prediction_length","5c264756":"\n\ntraining = TimeSeriesDataSet(\n    df_train[lambda x: x.time_idx <= training_cutoff],\n    time_idx=\"time_idx\",\n    target=\"sales\",\n    group_ids=[\"store_nbr\", \"family\"],\n    min_encoder_length=max_encoder_length \/\/ 2,  # keep encoder length long (as it is in the validation set)\n    max_encoder_length=max_encoder_length,\n    min_prediction_length=1,\n    max_prediction_length=max_prediction_length,\n    static_categoricals=[\"store_nbr\", \n                         \"family\", \n                         \"city\", \n                        # \"state\", \n                         \"store_cluster\", \n                         \"store_type\"],\n    time_varying_known_categoricals=[\"holiday_nat\", \n                                     \"holiday_reg\", \n                                     \"holiday_loc\", \n                                     \"month\", \n                                     \"dayofweek\",\n                                     \"dayofyear\"\n                                    ],\n    time_varying_known_reals=[\"time_idx\", \"onpromotion\", 'days_from_payday', 'dcoilwtico', \"earthquake_effect\"\n],\n    time_varying_unknown_categoricals=[],\n    time_varying_unknown_reals=[\n        \"sales\",\n       # \"transactions\",\n        \"average_sales_by_family\",\n        \"average_sales_by_store\",\n    ],\n    target_normalizer=GroupNormalizer(\n        groups=[\"store_nbr\", \"family\"], transformation=\"softplus\"\n    ),  # use softplus and normalize by group\n    add_relative_time_idx=True,\n    add_target_scales=True,\n    add_encoder_length=True,\n    allow_missing_timesteps=True\n)\n\n# create validation set (predict=True) which means to predict the last max_prediction_length points in time\n# for each series\nvalidation = TimeSeriesDataSet.from_dataset(training, df_train, predict=True, stop_randomization=True)\n","35136f62":"\n# create dataloaders for model\nbatch_size = 128  # set this between 32 to 128\ntrain_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=2, drop_last=True)\nval_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=2,drop_last=True)","6650ee38":"import torch\n\n# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\nactuals = torch.cat([y for x, (y, weight) in iter(val_dataloader)])\nbaseline_predictions = Baseline().predict(val_dataloader)\n(actuals - baseline_predictions).abs().mean().item()","c1851a34":"import pytorch_lightning as pl\nfrom pytorch_forecasting.metrics import QuantileLoss\n","ec647d0e":"import pickle\n\nfrom pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n\n# create study\nstudy = optimize_hyperparameters(\n    train_dataloader,\n    val_dataloader,\n    model_path=\"optuna_test\",\n    n_trials=100,\n    max_epochs=30,\n    gradient_clip_val_range=(0.01, 1.0),\n    hidden_size_range=(8, 64),\n    hidden_continuous_size_range=(8, 64),\n    attention_head_size_range=(1, 4),\n    learning_rate_range=(0.001, 0.1),\n    dropout_range=(0.1, 0.3),\n    trainer_kwargs=dict(limit_train_batches=30, log_every_n_steps=15, gpus=1),\n    reduce_on_plateau_patience=4,\n    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n    timeout=7200\n)\n\n\n\n# show best hyperparameters\nprint(study.best_trial.params)","6dae3f30":"#Early Stopping \nMIN_DELTA  = 1e-4\nPATIENCE = 10\n\n#PL Trainer\nMAX_EPOCHS = 150\nGPUS = 1\nGRADIENT_CLIP_VAL=study.best_trial.params['gradient_clip_val']\nLIMIT_TRAIN_BATCHES=30\n\n#Fusion Transformer\nLR = study.best_trial.params['learning_rate']\nHIDDEN_SIZE = study.best_trial.params['hidden_size']\nDROPOUT = study.best_trial.params['dropout']\nATTENTION_HEAD_SIZE = study.best_trial.params['attention_head_size']\nHIDDEN_CONTINUOUS_SIZE = study.best_trial.params['hidden_continuous_size']\nOUTPUT_SIZE=7\nREDUCE_ON_PLATEAU_PATIENCE=5\n","d6188331":"from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n\n# configure network and trainer\nearly_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=MIN_DELTA, patience=PATIENCE, verbose=False, mode=\"min\")\nlr_logger = LearningRateMonitor()  # log the learning rate\n\ntrainer = pl.Trainer(\n    max_epochs=MAX_EPOCHS,\n    gpus=GPUS,\n    weights_summary=\"top\",\n    gradient_clip_val=GRADIENT_CLIP_VAL,\n    limit_train_batches=LIMIT_TRAIN_BATCHES,#oment in for training, running valiation every 30 batches\n    #fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n    callbacks=[lr_logger, early_stop_callback],\n    log_every_n_steps=10\n    \n)\n\n\ntft = TemporalFusionTransformer.from_dataset(\n    training,\n    learning_rate=LR,\n    hidden_size=HIDDEN_SIZE,\n    attention_head_size=ATTENTION_HEAD_SIZE,\n    dropout=DROPOUT,\n    hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,\n    output_size=OUTPUT_SIZE,# 7 quantiles by default\n    loss=QuantileLoss(),\n    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n    reduce_on_plateau_patience=REDUCE_ON_PLATEAU_PATIENCE,\n)\nprint(f\"Number of parameters in network: {tft.size()\/1e3:.1f}k\")","e5464b80":"import tensorflow as tf \nimport tensorboard as tb \ntf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n\n","bd746830":"# fit network\ntrainer.fit(\n    tft,\n    train_dataloaders=train_dataloader,\n    val_dataloaders=val_dataloader,\n)","4e18ec03":"# load the best model according to the validation loss\n# (given that we use early stopping, this is not necessarily the last epoch)\nbest_model_path = trainer.checkpoint_callback.best_model_path\nbest_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)","cc9e9683":"# calcualte mean absolute error on validation set\nactuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\npredictions = best_tft.predict(val_dataloader)\n(actuals - predictions).abs().mean()","2911b86a":"# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\nraw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)","c4e3d12a":"for idx in range(5):  # plot 10 examples\n    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);","dc5c9af4":"predictions, x = best_tft.predict(val_dataloader, return_x=True)\npredictions_vs_actuals = best_tft.calculate_prediction_actual_by_variable(x, predictions)\nbest_tft.plot_prediction_actual_by_variable(predictions_vs_actuals);","00b6ec2b":"df_test","2a2f7ed2":"df_test = pd.merge(df_test, stores, on='store_nbr')\n\n","f558d377":"df_test","7b20b852":"\ndf_test = pd.merge(df_test, holidays_nat[['date','description']], on='date', how='left').rename(columns={'description': 'holiday_nat'})\ndf_test = pd.merge(df_test, holidays_reg[['date', 'locale_name', 'description']], left_on=['date', 'state'], right_on=['date', 'locale_name'], how='left').rename(columns={'description': 'holiday_reg'}).drop(columns=['locale_name'])\ndf_test = pd.merge(df_test, holidays_loc[['date', 'locale_name', 'description']], left_on=['date', 'city'], right_on=['date', 'locale_name'], how='left').rename(columns={'description': 'holiday_loc'}).drop(columns=['locale_name'])\n\ndf_test[[\"holiday_nat\", \"holiday_reg\", \"holiday_loc\"]] = df_test[[\"holiday_nat\", \"holiday_reg\", \"holiday_loc\"]].fillna(\"No\")\n\ndf_test = pd.merge(df_test, oil, on='date', how='left')\ndf_test['dcoilwtico'] = df_test['dcoilwtico'].interpolate().fillna(method='bfill')\n\ndf_test['dayofweek'] = df_test['date'].dt.dayofweek.astype('str').astype('category')\ndf_test['month'] = df_test['date'].dt.month.astype('str').astype('category')\ndf_test['dayofyear'] = df_test['date'].dt.dayofyear.astype('str').astype('category')\n\n\nfor cat_col in ['holiday_nat', 'holiday_reg', 'holiday_loc','city','state' , 'store_type', 'store_cluster', 'store_nbr', 'family']:\n    df_test[cat_col] = df_test[cat_col].astype(str).astype('category')\n\ndf_test['earthquake_effect'] = 0\n\ndf_test['days_from_payday'] = df_test['date'].apply(get_distance_from_paydays)\ndf_test['time_idx'] = (df_test['date'].dt.date - df_train['date'].dt.date.min()).dt.days","54f68bd9":"df_test","6c09efa4":"# select last 30 days from data (max_encoder_length is 24)\nencoder_data = df_train[lambda x: x.time_idx > x.time_idx.max() - max_encoder_length]\n\n\nlast_data = df_train[df_train['time_idx'].isin([idx  -  prediction_steps for idx in df_test['time_idx'].unique()])]\nlast_data['time_idx'] = last_data['time_idx'] + prediction_steps\ndecoder_data = pd.merge(df_test[[col for col in df_test.columns if 'sales' not in col]], \n        last_data[['time_idx','store_nbr', 'family', 'sales', 'average_sales_by_family', 'average_sales_by_store' , 'transactions']],\n        on = ['time_idx', 'store_nbr', 'family',]\n        )\n\n# combine encoder and decoder data\nnew_prediction_data = pd.concat([encoder_data, decoder_data], ignore_index=True)","cece4009":"decoder_data","bfe223e1":"new_raw_predictions, new_x = best_tft.predict(new_prediction_data, mode=\"raw\", return_x=True)\n\nfor idx in range(10):  # plot 10 examples\n    best_tft.plot_prediction(new_x, new_raw_predictions, idx=idx, show_future_observed=False);","864ed563":"interpretation = best_tft.interpret_output(new_raw_predictions, reduction=\"sum\")\nbest_tft.plot_interpretation(interpretation)","fa9ec2f9":"predictions = best_tft.predict(new_prediction_data, mode=\"prediction\", return_x=False)\n","2b4c0a90":"predictions = pd.DataFrame(predictions.numpy()).T\npredictions['date'] = sorted(df_test['date'].unique())\npredictions = pd.melt(predictions, id_vars=['date'])\npredictions = predictions.sort_values(['date', 'variable']).reset_index(drop=True)\ndf_test[['date', 'id', 'store_nbr', 'family']].sort_values(['date', 'store_nbr', 'family']).reset_index(drop=True)\ndf_test = df_test.join(predictions['value'])","c0e8a929":"import plotly.graph_objects as go \nimport plotly.express as px\nlist_colors = px.colors.qualitative.Plotly\ndef plot_train_prediction(df_train, df_predictions, store=\"3\", n_families=10, date_begin=\"2017-07-15\", pred_time_col='date' , pred_col='value'):\n    df_train_viz = df_train[(df_train['date'] > pd.to_datetime(\"2017-07-15\"))&(df_train['store_nbr']==store)]\n    fig = go.Figure()\n\n    for i, family in enumerate(df_train_viz['family'].unique()[:10]):\n        train = df_train_viz[df_train_viz['family']==family]\n        pred = df_predictions[(df_predictions['family']==family)&(df_predictions['store_nbr']==store)]\n        fig.add_trace(go.Scatter(x =train[\"date\"], y=train[\"sales\"], mode='lines', name=f'{family}_train', line=dict(color=list_colors[i])))\n        fig.add_trace(go.Scatter(x =pred[pred_time_col], y=pred[pred_col], mode='lines', name=f'{family}_pred',  line=dict(color=list_colors[i])))\n\n    fig.show()\n    \nplot_train_prediction(df_train, df_test, store=\"4\")","0f4c43ac":"df_test[df_test['family']=='BOOKS', 'value'] = 0 # SEEMs to be 0 Sales of books \ndf_test[['id', 'value']].rename(columns={\"value\": \"sales\"}).to_csv('submission.csv', index=False)","50d02a94":"### Holidays","d2a81ea5":"### Reformat predictions for submission","5bed6c6d":"### Payday ","21dbc287":"### EarthQuake \n\nFrom the info of the competition the earthquake from 2016 April 16 had an impact on sales so we model this using a variable that is a skewed distribution after the event (time for the help to come and slowly fading out)","3b79a0a1":"### Casting and preparing for Pytorch Forecasting TimeSeriesDataSet","ae734eba":"<a id=\"section-3\"><\/a>\n# Prediction","c1b5959b":"<a id=\"section-1\"><\/a>\n# Loading Data ","d9be95fa":"### Evaluation","ffd82b08":"A lot better than baseline !","8e9a96c3":"### Retrain A full Model ","7937723f":"### Transactions","28d45275":"<a id=\"section-2\"><\/a>\n## Pytorch Forecasting : Training\/Tuning\/Evaluating TFT ","327a6580":"### Build Dataset","c0ff2ad2":"# Using Pytorch Forecasting to train a TemporalFusionTransformer\n\n**Litteraly the most swag name for a Machine Learning Model** \ud83d\ude4c\n\n=> To understand what's going on under this crazy appelation : [video explanation ](https:\/\/www.youtube.com\/watch?v=M7O4VqRf8s4) \n\n=> This is litterally a copy of the [Pytorch Forecasting Doc](https:\/\/pytorch-forecasting.readthedocs.io\/en\/latest\/tutorials\/stallion.html), all rights reserved to the authors \ud83d\ude03\n\n### Table of contents \n- [Loading and merging data](#section-1) \n- [Training TFT ](#section-2)\n- [Prediction and submission](#section-3)","adf32481":"### Baseline","21aeeddb":"### Stores","2027df0f":"### Derivates from sales ","2ea69c02":"### Hyperparameter Optimization","ea75d43b":"### Oil"}}