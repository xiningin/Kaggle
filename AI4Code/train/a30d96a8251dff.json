{"cell_type":{"a2acd0cb":"code","93461b6d":"code","17cc5daa":"code","88d2b3ba":"code","59d57587":"code","4fd51935":"code","157e143a":"code","89d3f88c":"code","c8bc6d03":"code","13f961e6":"markdown","e2375dc5":"markdown","97f64b75":"markdown","f7fc92ca":"markdown","9bd27d0a":"markdown"},"source":{"a2acd0cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","93461b6d":"import pandas\nimport numpy\n\n\ndf = pandas.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\nnum_records = len(df)\n\nids = numpy.arange(num_records)\nids = numpy.random.permutation(ids)\n\ntrain_size = 0.8\npartition = int(num_records * train_size)\n\ntrain_ids, valid_ids = ids[:partition], ids[partition:]\n\ndf.loc[train_ids].to_csv(\".\/processed_train.csv\", index=False)\ndf.loc[valid_ids].to_csv(\".\/processed_valid.csv\", index=False)\n","17cc5daa":"from typing import Any, Dict, Iterable, MutableMapping, Optional\nfrom urllib.parse import urlparse\n\nfrom allennlp.data import DatasetReader\nfrom allennlp.data import Tokenizer\nfrom allennlp.data.fields.field import Field\nfrom allennlp.data.fields import ArrayField, TextField\nfrom allennlp.data.instance import Instance\nfrom allennlp.data.token_indexers import SingleIdTokenIndexer\nfrom allennlp.data.token_indexers.token_indexer import TokenIndexer\nfrom allennlp.data.tokenizers.token_class import Token\nimport pandas\nimport numpy\nfrom overrides import overrides\n\n\n@DatasetReader.register(\"commonlit_reader\")\nclass CommonlitDatasetReader(DatasetReader):\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        excerpt_token_indexers: Optional[Dict[str, TokenIndexer]] = None,\n        hostname_token_indexers: Optional[Dict[str, TokenIndexer]] = None,\n    ) -> None:\n\n        super().__init__()\n\n        self.tokenizer = tokenizer\n        self.excerpt_token_indexers: Dict[str, TokenIndexer] = excerpt_token_indexers or {\n            \"tokens\": SingleIdTokenIndexer(),\n        }\n        self.hostname_token_indexers: Dict[str, TokenIndexer] = hostname_token_indexers or {\n            \"tokens\": SingleIdTokenIndexer(),\n        }\n\n    def _read(self, file_path: str) -> Iterable[Instance]:\n        instances = []\n\n        dataframe = pandas.read_csv(file_path)\n        dataframe[\"hostname\"] = dataframe \\\n            .url_legal \\\n            .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else \"EMPTY_HOSTNAME\")\n\n        for _, row in dataframe.iterrows():\n            excerpt = row.excerpt\n            hostname = row.hostname\n            target = row.target if hasattr(row, \"target\") else None\n            instances.append(self.text_to_instance(excerpt, hostname, target))\n\n        return instances\n\n    @overrides\n    def text_to_instance(self, excerpt: str, hostname: str, target: Optional[float] = None) -> Instance:\n        excerpt_tokens = self.tokenizer.tokenize(excerpt)\n        hostname_tokens = [Token(text=hostname)]\n        fields: MutableMapping[str, Field[Any]] = {\n            \"excerpt\": TextField(excerpt_tokens),\n            \"hostname\": TextField(hostname_tokens),\n        }\n        if target is not None:\n            fields[\"target\"] = ArrayField(numpy.asarray(target, dtype=numpy.float32))\n        return Instance(fields=fields)\n\n    def apply_token_indexers(self, instance: Instance) -> None:\n        assert isinstance(instance.fields[\"excerpt\"], TextField)\n        instance.fields[\"excerpt\"].token_indexers = self.excerpt_token_indexers\n        assert isinstance(instance.fields[\"hostname\"], TextField)\n        instance.fields[\"hostname\"].token_indexers = self.hostname_token_indexers\n\n\nfrom typing import Dict, Optional\nfrom allennlp.data.vocabulary import Vocabulary\nfrom allennlp.models import Model\nfrom allennlp.modules import TextFieldEmbedder\nfrom allennlp.modules import Seq2VecEncoder\nfrom allennlp.nn.util import get_text_field_mask\nfrom allennlp.data.fields.text_field import TextFieldTensors\nfrom overrides.overrides import overrides\nfrom torch import FloatTensor\nfrom torch.functional import Tensor\nfrom torch.nn.functional import mse_loss\nfrom torch import cat\nfrom torch import sqrt\nfrom torch.nn import Linear\n\n\nEPS = 1e-8\n\n\n@Model.register(\"naive\")\nclass NaiveRegressor(Model):\n\n    def __init__(\n        self,\n        vocab: Vocabulary,\n        excerpt_embedder: TextFieldEmbedder,\n        excerpt_encoder: Seq2VecEncoder,\n        hostname_embedder: Optional[TextFieldEmbedder] = None,\n    ) -> None:\n\n        super().__init__(vocab)\n\n        self.vocab = vocab\n        self.excerpt_embedder = excerpt_embedder\n        self.excerpt_encoder = excerpt_encoder\n        self.hostname_embedder = hostname_embedder\n\n        in_features = self.excerpt_encoder.get_output_dim()\n        if hostname_embedder is not None:\n            in_features += hostname_embedder.get_output_dim()\n\n        self.classification_layer = Linear(\n            in_features=in_features,\n            out_features=1,\n        )\n\n    @overrides\n    def forward(\n        self,\n        excerpt: TextFieldTensors,\n        hostname: Optional[TextFieldTensors] = None,\n        target: Optional[FloatTensor] = None,\n    ) -> Dict[str, Tensor]:\n\n        mask = get_text_field_mask(excerpt)\n        excerpt_emb = self.excerpt_embedder(excerpt)\n        hidden_state = self.excerpt_encoder(excerpt_emb, mask=mask)\n\n        if self.hostname_embedder is not None and hostname is not None:\n            hostname_emb = self.hostname_embedder(hostname)\n            hidden_state = cat((hidden_state, hostname_emb.squeeze(dim=1)), dim=1)\n\n        logit = self.classification_layer(hidden_state)\n\n        output_dict = {\"logit\": logit}\n        if target is not None:\n            output_dict[\"loss\"] = sqrt(mse_loss(logit.view(-1), target) + EPS)\n\n        return output_dict\n\n    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n        return {}\n\n\nfrom allennlp.common.util import JsonDict\nfrom allennlp.data.instance import Instance\nfrom allennlp.predictors import Predictor\n\n\n@Predictor.register(\"regressor_predictor\")\nclass RegressorPredictor(Predictor):\n    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n        return self._dataset_reader.text_to_instance(**json_dict)  # type: ignore\n","88d2b3ba":"jsonnet_text = \"\"\"\\\n{\n    dataset_reader: {\n        type: \"commonlit_reader\",\n        tokenizer: {\n            type: \"pretrained_transformer\",\n            model_name: \"..\/input\/roberta-base\",\n        },\n        excerpt_token_indexers: {\n            tokens: {\n                type: \"pretrained_transformer\",\n                model_name: \"..\/input\/roberta-base\",\n            },\n        },\n    },\n    train_data_path: \".\/processed_train.csv\",\n    validation_data_path: \".\/processed_valid.csv\",\n    model: {\n        type: \"naive\",\n        excerpt_embedder: {\n            type: \"basic\",\n            token_embedders: {\n                tokens: {\n                    type: \"pretrained_transformer\",\n                    model_name: \"..\/input\/roberta-base\",\n                },\n            },\n        },\n        excerpt_encoder: {\n            type: \"bert_pooler\",\n            pretrained_model: \"..\/input\/roberta-base\",\n        },\n        hostname_embedder: {\n            type: \"basic\",\n            token_embedders: {\n                tokens: {\n                    embedding_dim: 50,\n                },\n            },\n        },\n    },\n    trainer: {\n        num_epochs: 15,\n        learning_rate_scheduler: {\n            type: \"slanted_triangular\",\n            num_epochs: 10,\n            num_steps_per_epoch: 3088,\n            cut_frac: 0.06\n        },\n        optimizer: {\n            type: \"huggingface_adamw\",\n            lr: 5e-7,\n            weight_decay: 0.05,\n        },\n        validation_metric: \"-loss\"\n    },\n    data_loader: {\n        batch_size: 8,\n        shuffle: true\n    }\n}\n\"\"\"\n\nf = open(\"baseline.jsonnet\", \"w\")\nf.write(jsonnet_text)\nf.close()","59d57587":"import allennlp.commands\n\nallennlp.commands.train.train_model_from_file(\n    parameter_filename=\".\/baseline.jsonnet\",\n    serialization_dir=\".\/serialization\/1\",\n)","4fd51935":"from allennlp.models.archival import load_archive\n\narchive = load_archive(\"serialization\/1\/model.tar.gz\")\npredictor = RegressorPredictor.from_archive(archive)","157e143a":"from urllib.parse import urlparse\n\n\ntest_df = pandas.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\nprint(test_df.head())\n\ntest_df[\"hostname\"] = test_df \\\n    .url_legal \\\n    .apply(lambda url: urlparse(url).hostname if isinstance(url, str) else \"EMPTY_HOSTNAME\")\n\nbatch_json = test_df.apply(lambda row: {\"excerpt\": row.excerpt, \"hostname\": row.hostname}, axis=1).tolist()\npredictor.predict_batch_json(batch_json)","89d3f88c":"class BatchIterator:\n        def __init__(self, data, batch_size):\n                self.data = data\n                self.batch_size = batch_size\n                self.cur = 0\n            \n        def __iter__(self):\n                return self\n            \n        def __next__(self):\n                batch = self.data[self.cur:self.cur+self.batch_size]\n                self.cur += self.batch_size\n                if len(batch) == 0:\n                    raise StopIteration\n                return batch\n\n\npredictions = []\nbatch_iterator = BatchIterator(batch_json, batch_size=1)\n\nfor batch in batch_iterator:\n    predictions += predictor.predict_batch_json(batch)","c8bc6d03":"test_df[\"target\"] = list(map(lambda p: p[\"logit\"][0], predictions))\ntest_df[[\"id\", \"target\"]].to_csv(\"submission.csv\", index=False)\ntest_df","13f961e6":"## Modules: Dataset reader, Model, Predictor\n\nModel is simple that consists of two types of features: 1 categorical feature (hostname) and 1 text feature (excerpt).\n\nA hostname is embedded into 50 dimensional and excerpt is fed into RoBERTa (base).\n\nEach representation are concatenated and projected to a scalar.","e2375dc5":"## Config file\n\nI'm not sure if this is the best way but I create a config file in this notebook.\n\nExecuting the following cell creates a Jsonnet config.","97f64b75":"## Training\n\nInstead of running `allennlp train` on a shell,\nwe can directly invoke `allennlp.commands.train.train_model_from_file`.","f7fc92ca":"## What is this\n\nThis notebook shows an example of [AllenNLP](https:\/\/github.com\/allenai\/allennlp) Jsonnet API for training\/inference.\n\nI publish this example because I couldn't find an example using AllenNLP with Jsonnet.\n(Examples using AllenNLP I found didn't use Jsonnet config file)\n\n\nIn notebook only competition, it is relatively hard to use AllenNLP with Jsonnet config file,\nbecause training is launched by running a command `allennlp train` on the shell and it loads\nmodules in a repository. (it means that we have to write scripts outside the notebook).\n\nIn this example, I invoke `allennlp.commands.train` on the notebook to mitigate this limitation.\nEach modules are defined in the notebook.\n\n\nUnfortunately, the submission using this notebook failed with the error `Notebook Exceeded Allowed Compute`.\nAlthough I was trying to figure out the cause of memory\/disk problem, I couldn't solve it.","9bd27d0a":"### Inference"}}