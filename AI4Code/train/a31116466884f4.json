{"cell_type":{"41cdc452":"code","a4b071f3":"code","4cd2067c":"code","ff54937a":"code","850ca570":"code","6111b30c":"code","1fb651f9":"code","0566a55c":"code","12a14519":"code","688e9bdf":"code","d3e79e3a":"code","4636cb86":"code","f7885cc5":"code","309a77b9":"code","f777ffaf":"code","87efafba":"code","a103e087":"code","98643180":"code","352a2726":"code","a0a7694b":"code","31808947":"code","2d81e0f4":"code","74b402ac":"code","f0c97b87":"code","d4f166d6":"code","d142a297":"code","99943afd":"code","b605c20e":"code","c5481d5c":"code","a769f652":"code","4223e3af":"code","2e91d8d4":"code","f54470c0":"code","7cb2d6a6":"code","8e7dc804":"code","67018fb3":"code","f1b78fd6":"code","285d6316":"code","b80acd63":"code","fd7c4e9b":"code","1a14a24f":"code","4f0bdaa3":"code","f8b2f519":"code","b5a3c23b":"code","4ea57982":"code","3481c5ed":"code","c2715110":"code","9ea79a84":"code","d057ec4a":"code","3a2a10e8":"code","9f4dbd5b":"code","842201f2":"code","fd27edd1":"code","a674284c":"code","10efaeec":"code","21c88e2c":"code","47df466d":"code","7adfc598":"code","b77d369f":"code","dae6c6b1":"code","c753e0b0":"code","d122f625":"code","e33ce8ef":"code","f9305a96":"code","3b110200":"code","65acc7fd":"code","23112d42":"code","7638fa63":"code","911c2dc8":"code","4906365c":"code","01cc8cfc":"code","9264c798":"code","6f9f5be0":"code","e07e98f9":"code","bc36b008":"code","25382029":"code","1d9a90ac":"code","4d076593":"code","b6af48b8":"code","837b4d0a":"code","75c1bf3c":"code","86166b7c":"code","ef957f3b":"code","51fcee6a":"markdown","027e0a56":"markdown","a2a9a185":"markdown","d6e9ca56":"markdown","cae7bcaf":"markdown","e39e5a38":"markdown","9a54466b":"markdown","820fe518":"markdown","d3dc1bf7":"markdown","4263e605":"markdown","41b7073c":"markdown","e2b70db8":"markdown","b57413d7":"markdown","46649e32":"markdown","bc608362":"markdown","01f92b66":"markdown","05f3c5d8":"markdown"},"source":{"41cdc452":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a4b071f3":"def show(_df, count=2):\n    print(_df.shape)\n    size = _df.shape[0]\n    count = count if size > count else size\n    display(_df.head(count))\n    \n    \n    display(_df.sample(count))\n    display(_df.tail(count))","4cd2067c":"import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport array as arr\nimport pandas_profiling\nfrom datetime import timedelta\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n","ff54937a":"test_file = '\/test.csv'\ntrain_file = '\/train.csv'\ntrain_df = pd.read_csv(dirname + train_file)\ntest_df = pd.read_csv(dirname + test_file)\n","850ca570":"#define variables \npredCol = 'TargetValue'\nidCol = 'Id'\nfidCol = 'ForecastId'\nignoreCols = [predCol, idCol, fidCol, 'Date']\nctry = 'Country_Region'\nste = 'Province_State'\ncty = 'County'\nlcn='Location'","6111b30c":"\nd2 = pd.to_datetime('2020-12-31')\nlr = test_df[test_df[ctry]=='India'].tail(1)[['ForecastId','Date']].values[0]\nd1 = pd.to_datetime(lr[1]) + timedelta(1)\nid = lr[0]\nprint(d1)\nprint (d2)\nppd = pd.DataFrame(\n    [[d]\n     for d  in pd.date_range(d1, d2, freq='D')])\nppd.tail(2)","1fb651f9":"\n\ndef getid(id1):\n    global id\n    id1 = id + 1\n    id = id1\n    return id1;\n\nprint(id)\ncc= pd.DataFrame(\n    [[getid(id), np.NaN,np.NaN, 'India',1295210000,0.04766, d, 'ConfirmedCases']\n     for d  in pd.date_range(d1, d2, freq='D')],\n    columns=['ForecastId', cty, ste, ctry, 'Population', 'Weight', 'Date', 'Target']\n)\nprint(cc.tail(2))\n\nff= pd.DataFrame(\n    [[getid(id), np.NaN,np.NaN, 'India',1295210000,0.47660, d, 'Fatalities']\n     for d in pd.date_range(d1, d2, freq='D')],\n    columns=['ForecastId', cty, ste, ctry, 'Population', 'Weight', 'Date', 'Target']\n)\nprint(ff.head(2))\n\n\nmoretest = pd.concat([cc,ff])\n#moretest['ForecastId'] = moretest.apply(lambda x: getid(id))['ForecastId']\n\nmoretest[(moretest[ctry]=='India') & (moretest['Date']== '2020-06-10')]","0566a55c":"test_df =pd.concat([test_df, moretest])\n#test_df.drop_duplicates()","12a14519":"test_df[(test_df[ctry]=='India') & (test_df['Date']== '2020-06-10')]","688e9bdf":"show(test_df)","d3e79e3a":"df_all = pd.concat([train_df, test_df], sort=False)\nshow(df_all, 5)","4636cb86":"df_all[lcn] = df_all[ctry] + '_' +  df_all[ste].fillna('NA') + '_' + df_all[cty].fillna('NA')","f7885cc5":"country = [\"US\", \"India\",\"China\", \"Spain\", \"Italy\", \"Pakistan\", \"Mexico\", \"United Kingdom\", \"France\", \"Japan\", \"South Korea\", \"Russia\", \"Cananda\", \"Peru\", \"Turkey\"]\ncountry =[x + \"_NA_NA\" for x in country]\nprint(country)","309a77b9":"\nif len(country) >0 :\n    df_all = df_all[df_all[lcn].isin(country )]\n\nshow(df_all, 2)","f777ffaf":"locMin = df_all[df_all[predCol]>0 | df_all[predCol].notnull()].groupby([lcn])['Date'].min()\nlocMin","87efafba":"def replace_startdate(_ctry, _dt):\n    idex = locMin[locMin.index.str.startswith(_ctry)].index\n    print(idex)\n    _dt = pd.to_datetime(_dt, infer_datetime_format=True)\n    print(_dt.strftime(\"%d-%m\"))\n    if len(idex) ==1:\n        print(locMin.at[idex[0]])\n        locMin.at[idex[0]] = _dt\n        print(locMin.at[idex[0]])\n\n\nreplace_startdate('Russia', '03-05-2020')\nreplace_startdate('Italy', '02-20-2020')\nreplace_startdate('Germany', '02-24-2020')\nreplace_startdate('Turkey', '03-15-2020')\nreplace_startdate('Canada_NA_NA', '02-05-2020')","a103e087":"df_all= pd.merge(df_all, locMin, on=[lcn,lcn])\nshow(df_all)","98643180":"import time\ndf_all['Date_x'] = pd.to_datetime(df_all['Date_x'], infer_datetime_format=True)\ndf_all['Date_y'] = pd.to_datetime(df_all['Date_y'], infer_datetime_format=True)","352a2726":"df_all = df_all[df_all['Date_x']>=df_all['Date_y']]","a0a7694b":"df_all['Days'] = df_all.apply(lambda x: ((x['Date_x'] -  x['Date_y']).days +1) ,axis=1)\ndf_all","31808947":"from datetime import timedelta\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\ndef trim_axs(axs, N):\n    axs = axs.flat\n    for ax in axs[N:]:\n        ax.remove()\n    return axs[:N]\ndef makeMonotoic(df):\n    state = df - df.shift(1)\n    state = state.fillna(0)\n    for st in state:\n        if (st < 0):\n            df = df.shift(1);\n\ndef topCharts(_dg, _n=[0,1], _s=0, _d=0, _w=18, _h=5, _m=400,_dif=2, _l=''):\n    \n    dg = _dg[_dg[predCol].notnull()]\n    \n    if _l != '':\n        dg = dg[dg[lcn].str.startswith(_l)]\n    dg = dg.groupby([lcn, 'Target' ]) \\\n        .sum() \\\n        .sort_values(by=predCol, ascending=False) \\\n        .reset_index()\n    dg1 = dg[(dg['Target']=='ConfirmedCases') | (dg['Target'] == 1)]\n    dg1 = dg1.groupby([lcn]).sum()\n    dg1 = dg1.sort_values(predCol, ascending=False)\n    top_c = dg1[_n[0]:_n[0] + _n[1]]\n    if top_c.shape[0] == 0 :\n        print(\"Nothing to plot for any country\")\n        return\n    \n  \n    _c = 4 if _n[1] > 4 else _n[1] \n    #show(top_c,1)\n    top_10 = top_c.index\n    #vfunc = np.vectorize(visualize, excluded=['_dg','_days'])\n    vt = 4\n    rows = len(top_10) \/\/ _c + 1\n    figsize = (_w,_h*rows)\n    fig ,axes= plt.subplots(rows, _c, figsize=figsize, constrained_layout=True)\n    plt.grid(True)\n    axes = trim_axs(axes, len(top_10))\n    for ax, _ctry in zip(axes, top_10):\n        ax2 = ax.twinx()\n   \n        tr = _dg[_dg[lcn]==_ctry]\n        if _d > 0 :\n            tr = tr.iloc[_s:_d*2]\n        tr = tr.sort_values(by='Days', ascending= True)\n        tr = tr[tr[predCol].notnull()]\n\n        cases = tr[(tr['Target']=='ConfirmedCases') | (tr['Target'] == 1)]\n        cases_dates = np.array(cases.apply(lambda x:  x['Date_x'].strftime(\"%d-%m\") , axis=1).unique())\n        days = np.array(tr.apply(lambda x:  x['Days'], axis=1).unique())\n        confirmedCases = np.array(cases[predCol]).cumsum()\n        \n        \n        \n        fatals = tr[(tr['Target']=='Fatalities')  | (tr['Target'] == 2)]\n        fatals_dates = np.array(fatals.apply(lambda x:  x['Date_x'].strftime(\"%d-%m\") , axis=1).unique())\n        Fatalities = np.array(fatals[predCol]).cumsum()\n\n\n        \n        #print(cc05)\n     \n        #print(Fatalities)\n\n        #low = np.ma.masked_where(confirmedCases<=10, confirmedCases)\n        #high = np.ma.masked_where(confirmedCases>10, confirmedCases)\n        #ax.plot(low, high)\n        \n                    \n        ax.plot(cases_dates, confirmedCases, '-b.', color='purple')\n        qf= 'q95'\n        qf1 = 'q05'\n        if 'q05' in cases:\n            cc05 = np.array( (cases[predCol] - cases[qf1]).cumsum())\n            ax.plot(cases_dates, cc05, color='purple', linestyle='--')\n            cc95 = np.array( (cases[predCol] + cases[qf]).cumsum())\n            ax.plot(cases_dates, cc95, color='purple', linestyle='--')\n            \n                    \n        if 'q05' in fatals:\n            ff = fatals[predCol] - fatals[qf1]\n            # print(fatals[predCol])\n            #print(fatals[qf])\n            # print(ff)\n            ff05 = np.array( ff.cumsum() )\n            ax2.plot(fatals_dates, ff05, color='orange', linestyle='--')\n            ff95 = np.array((fatals[predCol] + fatals[qf]).cumsum())\n            ax2.plot(fatals_dates, ff95, color='orange', linestyle='--')\n        \n        #if 'cc05' in locals() and 'cc95' in locals():\n            #ax.bar(cases_dates, cc05,  color='')\n            #ax.bar(cases_dates, cc95, bottom=cc05, color='grey')\n            \n       # if 'ff05' in locals() and 'ff95' in locals():\n           \n            #ax2.bar(fatals_dates, ff05,  color='pink')\n            #ax2.bar(fatals_dates, ff95, bottom=ff05, color='red')\n        \n        ax.set_ylabel(\"Cases\",fontsize=14,color='blue')\n        \n        #ax.text(days, confirmedCases, str(confirmedCases))\n        \n\n        ax2.plot(fatals_dates, Fatalities, '-b.', color='orange', linestyle='-')\n\n        flim = ax2.get_ylim()\n        ax2.set_ylim([-10, flim[1]*1.3])\n        vt = int(len(fatals_dates)*_c\/50 + 1)\n        ax2.set_ylabel(\"Fatal\",fontsize=14,color='blue')\n        ax.set_title(_ctry + '(' + tr['Date_y'].iloc[0].strftime(\"%d-%m\") + ')')\n        ax.set_xticklabels(fatals_dates[0::vt], rotation=90)\n        plt.xticks(fatals_dates[0::vt], rotation=90)\n        #ax2.text(dates, Fatalities)\n        ax.grid(True)\n        ax2.grid(True)\n        secax = ax.secondary_xaxis('top')\n        \n        secax.set_xlabel('days')\n        \n        lbly = (ax.get_ylim()[1]\/_h) ;\n        p = 0\n        if _dif > 0:\n            for i, v in enumerate(confirmedCases):\n                if(v < _m and v - p >= _dif) :\n                    ax.text(days[i] -1, v ,  str(v.astype(int)) , rotation=90, ha='left')\n                    p = v\n\n    \n#topCharts(dfv1, _dif=10000, _l='India')","2d81e0f4":"topCharts(df_all, _n=[0,2], _d=10)","74b402ac":"topCharts(df_all, _n=[2,1], _d=90)","f0c97b87":"topCharts(df_all, _n=[3,1], _d=40)","d4f166d6":"topCharts(df_all, _n=[4,1], _d=40)","d142a297":"topCharts(df_all, _n=[5,1], _d=20, _dif=1)","99943afd":"topCharts(df_all, _n=[6,4], _d=15,_dif=0)","b605c20e":"topCharts(df_all, _n=[10,4], _d=15,_dif=0)","c5481d5c":"topCharts(df_all, _n=[14,1], _d=30,_dif=0)","a769f652":"topCharts(df_all, _n=[16,1], _d=30,_dif=0)","4223e3af":"topCharts(df_all, _n=[17,2], _d=30,_dif=0)","2e91d8d4":"topCharts(df_all, _n=[19,2], _d=30,_dif=1)","f54470c0":"topCharts(df_all, _n=[21,4], _d=10,_dif=5)","7cb2d6a6":"topCharts(df_all, _n=[25,4], _d=10,_dif=5)","8e7dc804":"topCharts(df_all, _n=[29,4], _d=10,_dif=0)","67018fb3":"topCharts(df_all, _n=[33,16], _d=30,_dif=0)","f1b78fd6":"ecol = df_all.isna().sum().sort_values(ascending=False)[df_all.isna().sum()>0].index;\nprint(ecol)","285d6316":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ndflcn = df_all[[lcn, ctry, cty, ste]]\n\ndf_all[lcn] = encoder.fit_transform(df_all[lcn])\ndflcn['code'] = df_all[lcn]\n#pd.get_dummies(df, columns=['Location'], drop_first=True)\n\nshow(dflcn)","b80acd63":"df_all['Target'] = df_all['Target'].apply(lambda s: 1 if s== \"ConfirmedCases\" else 2 )","fd7c4e9b":"from sklearn import svm\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import tree\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble.weight_boosting import AdaBoostRegressor\nfrom sklearn.linear_model.base import LinearRegression\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveRegressor\nfrom sklearn.linear_model.theil_sen import TheilSenRegressor\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import ExtraTreesRegressor\n\n","1a14a24f":"models = [\n    ExtraTreesRegressor(n_estimators=500,n_jobs=-1,verbose=1),\n    XGBRegressor(n_estimators = 2300 , alpha = 0, gamma = 0, learning_rate = 0.04,  random_state = 42 , max_depth = 23),\n    #LGBMRegressor(),\n    #RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',max_depth=None, max_features='auto', max_leaf_nodes=None,max_samples=None, min_impurity_decrease=0.0,                      min_impurity_split=None, min_samples_leaf=1,                      min_samples_split=2, min_weight_fraction_leaf=0.0,                      n_estimators=100, n_jobs=None, oob_score=False,                      random_state=None, verbose=0, warm_start=False),\n    #KNeighborsRegressor(),\n    #AdaBoostRegressor(),\n    #PassiveAggressiveRegressor(),\n    #TheilSenRegressor()\n]","4f0bdaa3":"\ntrain = df_all[df_all[predCol].notnull()].drop(columns=['ForecastId'])\n\ntest = df_all[df_all[predCol].isna()].drop([predCol], axis=1).drop(columns=['Id'])\n\ntest.rename(columns={'ForecastId':'Id'}, inplace=True)\n","f8b2f519":"cols = ['Id', 'Population', 'Weight', 'Location', 'Days', 'Target']\n\ntest= test[cols]\ntrain= train[cols + ['TargetValue']]\n","b5a3c23b":"Xtrn, Xtest, Ytrn, Ytest = train_test_split(train.drop([predCol], axis=1), train[[predCol]], test_size=0.2, random_state=42)","4ea57982":"def handle_predictions (predictions):\n    predictions[predictions < 0] = 0   \n    return predictions","3481c5ed":"TestModels = pd.DataFrame()\ntmp = {}\n \nfor model in models:\n    # get model name\n    \n    tmp['Model'] = str(model)\n    # fit model on training dataset\n    model.fit(Xtrn, Ytrn[predCol])\n    pred= model.predict(Xtest)\n    pred = handle_predictions(pred)\n\n   \n    act = pred\n    targ = handle_predictions(Ytest[predCol])\n    \n    tmp['accuracy'] = r2_score(targ, act)\n    tmp['rmsle'] = (mean_squared_log_error(targ,act))\n    tmp['rmse'] = (mean_squared_error(targ, act))\n    # write obtained data\n    TestModels = TestModels.append([tmp])\n        \nTestModels.set_index('Model', inplace=True)\nTestModels","c2715110":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom scipy.interpolate import make_interp_spline, BSpline\nimport math \n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(TestModels)\nprint(scaled_data.min())\nscaled_data = scaled_data + abs(scaled_data.min())+1\naccuracy = scaled_data[0:,0:1]\nrmsle = scaled_data[0:,1:2]\nrmse =   scaled_data[0:,2:]\nallmet =  1\/accuracy\n\nprint(scaled_data)\nprint(1\/accuracy)\nfig, axes = plt.subplots(figsize=(15, 6))\naxes.plot(range(len(accuracy)),accuracy, color='blue' , marker='.')\naxes.plot(range(len(accuracy)),rmsle, color='green' , marker='.')\naxes.plot(range(len(accuracy)),rmse, color='orange' , marker='.')\n\naxes.plot(range(len(accuracy)),allmet, color='red' , marker='.')\n\nplt.show()","9ea79a84":"bestModel = models[allmet.argmin()]\nTestModels.iloc[allmet.argmin():allmet.argmin()+1].index[0]","d057ec4a":"bestModelName = TestModels.iloc[allmet.argmin():allmet.argmin()+1].index[0]\nbestScore = allmet[allmet.argmin()]      \nprint('Best Score : ' + str(bestScore))\nselectedModel = models[allmet.argmin()]","3a2a10e8":"show(test)","9f4dbd5b":"selectedModel.fit(train.drop([predCol], axis=1), train[predCol])\nprediction =selectedModel.predict(test)","842201f2":"test[predCol] = prediction\ntest['istrain'] = 0\ntrain['istrain'] = 1\nprint(test.shape)\nprint(train.shape)","fd27edd1":"cols = [lcn, 'Days','Target']\n#print(train[(train['Date_x']=='2020-05-17') & (train[ctry]=='India')])\ntrain1=train.rename(columns={'Id':'tid'})\n#print(train1[(train1['Date_x']=='2020-05-17') & (train1[ctry]=='India')])\n\nintersect = pd.merge(train1[ [predCol, 'tid'] + cols], test[cols + ['Id']], how='inner')#.set_index('tid')\ntidx = intersect['tid']\ntrain = train[~train['Id'].isin(tidx)]\n#show(intersect,2)\n#print(intersect[(intersect['Days']==77) & (intersect[lcn]==142) ])\nintersect.drop(columns=['tid'],axis=1, inplace=True)\n\nintersect= intersect.set_index('Id')\ntt = test#[(test['Date_x']=='2020-05-17') & (test[ctry]=='India')]\ntt = tt.drop(columns=[predCol])#.set_index('Id')\n#print(tt[(tt['Days']==77) & (tt[lcn]==142)])\n#print(intersect[(intersect['Days']==77) & (intersect[lcn]==142)])\n\nintersect = pd.merge(tt, intersect, how='inner')\n\ntest = test[~test['Id'].isin(intersect['Id'])]\n#print(intersect)\n#print()\nprint(intersect[(intersect['Days']==77) & (intersect[lcn]==142)])\ntest1 = test\ntest1 = pd.concat([intersect, test1], sort=False)\n#print(test1[(test1['Date_x']=='2020-05-17') & (test1[ctry]=='India')])\ntest = test1.drop_duplicates()\n#print(test1[(test1['Date_x']=='2020-05-17') & (test1[ctry]=='India')])","a674284c":"dflcn = dflcn.drop_duplicates()\n\ndfv = pd.concat([train, test ], sort=False)\n\ndfv = pd.merge(dfv, dflcn, left_on=lcn, right_on='code',how='left')\ndfv.rename(columns={'Location_y':lcn}, inplace=True)\n\n","10efaeec":"dfv['Date_x'] = dfv.apply(lambda x: pd.to_datetime(locMin.at[x[lcn],]) + timedelta(x['Days']) , axis=1)\ndfv['Date_y'] = dfv.apply(lambda x: pd.to_datetime(locMin.at[x[lcn],]) , axis=1)\ndfv","21c88e2c":"dfv['Target'] = dfv['Target'].apply(lambda s: \"ConfirmedCases\" if s == 1 else \"Fatalities\" )\ndfv","47df466d":"dfv1= dfv\n\nwinsize = len(dfv1[lcn].unique())\nprint (winsize)\ndfv1['q05'] = dfv1.groupby([lcn, 'Target'])[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=winsize).quantile(0.05)).reset_index(name='q05')['q05']\ndfv1['q95'] = dfv1.groupby([lcn, 'Target'])[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=winsize).quantile(0.95)).reset_index(name='q95')['q95']\ndfv1['q50'] = dfv1.groupby([lcn, 'Target'])[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=winsize).quantile(0.5)).reset_index(name='q50')['q50']\n\n#dfv1['q05'] = dfv1.groupby(['Id'])[predCol].quantile(0.05).reset_index(name='q05')['q05']\n#dfv1['q95'] = dfv1.groupby(['Id'])[predCol].quantile(0.95).reset_index(name='q95')['q95']\n#dfv1['q50'] = dfv1.groupby(['Id'])[predCol].quantile(0.5).reset_index(name='q50')['q50']\n\n#dfv1[predCol] = dfv1['q50']\n\ndfv1[dfv1['Target']=='ConfirmedCases'].tail(20)","7adfc598":"dfv1[dfv1[lcn]=='India_NA_NA']","b77d369f":"from datetime import date\n\ntoday = pd.to_datetime(date.today())\noffset = today - timedelta (1)\n\ndfv1.loc[(dfv1['Date_x'] < offset), 'q05'] = 0\ndfv1.loc[(dfv1['Date_x'] < offset), 'q50'] = 0\ndfv1.loc[(dfv1['Date_x'] < offset), 'q95'] = 0\n","dae6c6b1":"test_r = dfv1[dfv1['istrain']==0]\n#test_r['q50'] = test_r[predCol]\nsub=pd.melt(test_r, id_vars=['Id'], value_vars=['q05','q50','q95'])\nsub['variable']=sub['variable'].str.replace(\"q\",\"0.\", regex=False)\nsub['ForecastId_Quantile']=sub['Id'].astype(int).astype(str)+'_'+sub['variable']\nsub['TargetValue']=sub['value']\nsub=sub[['ForecastId_Quantile','TargetValue']]\nsub.reset_index(drop=True,inplace=True)\nsub","c753e0b0":"dfc = dfv[dfv[lcn]=='Italy_NA_NA']\ndfc[dfc['Date_x']=='2020-04-28']\n#dfc = dfc[dfc['Target']=='Fatalities']\n#dfc = dfc.groupby('Date_x').count()\n#dfc[dfc['code']!=1]","d122f625":"topCharts(dfv1, _n=[0,1], _s=10, _d=180, _dif=0)","e33ce8ef":"topCharts(dfv1, _n=[1,1])","f9305a96":"topCharts(dfv1, _n=[2,1], _s=10, _d=180, _dif=0)","3b110200":"topCharts(dfv1, _n=[3,4], _s=10, _d=180, _dif=0)","65acc7fd":"topCharts(dfv1, _n=[7,3], _s=10, _d=180, _dif=0)","23112d42":"topCharts(dfv1, _n=[10,3], _s=10, _d=180, _dif=0)","7638fa63":"topCharts(dfv1, _n=[13,3], _s=10, _d=180, _dif=0)","911c2dc8":"topCharts(dfv1, _n=[16,3], _s=10, _d=180, _dif=0)","4906365c":"topCharts(dfv1, _n=[19,3], _s=10, _d=180, _dif=0)","01cc8cfc":"topCharts(dfv1, _n=[22,3], _s=10, _d=180, _dif=0)","9264c798":"topCharts(dfv1, _n=[25,3], _s=10, _d=180, _dif=0)","6f9f5be0":"topCharts(dfv1, _l='US')","e07e98f9":"topCharts(dfv1, _l='Bra',  _dif=10000)","bc36b008":"topCharts(dfv1, _dif=10000, _l='India')","25382029":"topCharts(dfv1, _dif=10000, _l='Russia')","1d9a90ac":"topCharts(dfv1, _dif=10000, _l='United K')","4d076593":"topCharts(dfv1, _dif=10000, _l='Spain')","b6af48b8":"topCharts(dfv1, _dif=10000, _l='Italy')","837b4d0a":"topCharts(dfv1, _dif=10000, _l='China')","75c1bf3c":"topCharts(dfv1, _dif=10000, _l='Mexi')","86166b7c":"topCharts(dfv1, _dif=10000, _l='Pak')","ef957f3b":"topcc = pd.DataFrame(dfv[(dfv[ste].isnull()) & (dfv['Target']=='ConfirmedCases')].groupby(lcn)[predCol].sum())\n#result['countries'] =.index\ntopcc['fatals'] = dfv[(dfv[ste].isnull()) & (dfv['Target']=='Fatalities')].groupby(lcn)[predCol].sum().values\ntopcc.sort_values(by=predCol, ascending=False).head(20)","51fcee6a":"Lets merge three location columns. We will also merge the two data set so that we get uniform location feature code.","027e0a56":"We know empty values exist in these columns. Lets make location as feature columns","a2a9a185":"Put a filter so that it executes fast. Later remove the filter.","d6e9ca56":"Why do we have state and country. I thought we have to analyze at country level. Let's drill down.","cae7bcaf":"dfv1= dfv.groupby([lcn, 'Target']).apply(lambda x: x.set_index('Date_x').resample('1D').first())\n\ndfv1['q05'] = dfv1.groupby(level=1)[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=8).quantile(0.05)).reset_index(name='q05')['q05']\ndfv1['q95'] = dfv1.groupby(level=1)[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=8).quantile(0.95)).reset_index(name='q95')['q95']\n\ndfv1.groupby(level=1)[predCol].apply(lambda x: x.shift().rolling(min_periods=1,window=8).quantile(0.05)).reset_index(name='q05')","e39e5a38":"drop records which are zero before pandamic starts","9a54466b":"Lets join three regional columns into one and convert it to feature column. We also have to merge test and train to have uniform feature columns","820fe518":"**Visualize the growth**","d3dc1bf7":"Set new start date above and start ntebook again.","4263e605":"# Begin Modelling","41b7073c":"Assign no of days to each entries from start of case in the area.","e2b70db8":"Data Validation : Check empty columns","b57413d7":"    Let's see the train data ","46649e32":"* We have US data starting march 10, by then US already more 200 cases. So we ignore it.\n* Russia had first 2 cases on Jan 31, but actual spread can be considered from Mar 5, when next days it went up from 5 to 13. \n* Italy started on Feb 20\n* Germany had no or less spread for first 25 days. Lets consider Feb 24 as start date \n* Turkey - 15 Mar\n* Canada - 5 Feb\n","bc608362":"Based on analysis below following are the new dates of spread start.","01f92b66":"Find the begining of pandamic ","05f3c5d8":"Lets analyze top 20 locations for initial 30 days and \/ or first 400 cases to change actual spread start date."}}