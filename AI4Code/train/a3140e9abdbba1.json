{"cell_type":{"8e845a8c":"code","ec41dfef":"code","f0d7a873":"code","57a348c7":"code","e55b1114":"code","7032aab1":"code","3bd477b8":"code","6a4fcea3":"code","3d962810":"code","f3fb8ae4":"code","b1db9cd3":"code","c444cb4f":"code","e0e4ac94":"code","34ec337b":"code","86961efb":"code","c3295ec9":"code","bf6fb35a":"code","e0790bf3":"code","2c58106c":"code","1e54a60c":"code","c7fd47b1":"code","f019222f":"code","ebb8dff0":"markdown","d27c833e":"markdown","7c81165f":"markdown","de37b34e":"markdown","31942e54":"markdown","22a9faa0":"markdown","ca6e0eff":"markdown","5b4f2a9f":"markdown","55a6e06b":"markdown","79081533":"markdown","1201d311":"markdown","c1ee2988":"markdown","c32ef7c7":"markdown","adb50ba5":"markdown","fccdf60f":"markdown","b32f17cf":"markdown","d99225a6":"markdown","871499c2":"markdown"},"source":{"8e845a8c":"# Import necessary libraries:\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\n\n# Setting necessary display options:\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)","ec41dfef":"# Import dataset\n\ndf_ = pd.read_csv(\"..\/input\/hitters-baseball-data\/Hitters.csv\")\ndf = df_.copy()","f0d7a873":"# Quick check to the dataset:\n\ndef check_df(dataframe):\n    print(\"##################### Shape #####################\")\n    print(f\"Rows: {dataframe.shape[0]}\")\n    print(f\"Columns: {dataframe.shape[1]}\")\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"####################### NA ######################\")\n    print(dataframe.isnull().sum())\n    print(\"################### Quantiles ###################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n    print(\"##################### Head ######################\")\n    print(dataframe.head())\n\ncheck_df(df)","57a348c7":"# Grab column names by their type:\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","e55b1114":"# Setting Outlier Thresholds and Checking Outliers:\n\ndef outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\ncheck_outlier(df, num_cols)","7032aab1":"# We need to check the details of the outliers in our dataset:\n\ndef grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n\n    if dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 10:\n        print(\"#####################################################\")\n        print(str(col_name) + \" variable have too much outliers: \" + str(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0]))\n        print(\"#####################################################\")\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].head(15))\n        print(\"#####################################################\")\n        print(\"Lower threshold: \" + str(low) + \"   Lowest outlier: \" + str(dataframe[col_name].min()) +\n              \"   Upper threshold: \" + str(up) + \"   Highest outlier: \" + str(dataframe[col_name].max()))\n        print(\"#####################################################\")\n    elif (dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] < 10) & \\\n            (dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0] > 0):\n        print(\"#####################################################\")\n        print(str(col_name) + \" variable have less than 10 outlier values: \" + str(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].shape[0]))\n        print(\"#####################################################\")\n        print(dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))])\n        print(\"#####################################################\")\n        print(\"Lower threshold: \" + str(low) + \"   Lowest outlier: \" + str(dataframe[col_name].min()) +\n              \"   Upper threshold: \" + str(up) + \"   Highest outlier: \" + str(dataframe[col_name].max()))\n        print(\"#####################################################\")\n    else:\n        print(\"#####################################################\")\n        print(str(col_name) + \" variable does not have outlier values\")\n        print(\"#####################################################\")\n\n    if index:\n        print(str(col_name) + \" variable's outlier indexes\")\n        print(\"#####################################################\")\n        outlier_index = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))].index\n        return outlier_index\n\n\nfor col in num_cols:\n    grab_outliers(df, col)","3bd477b8":"# Replacing outliers with thresholds:\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n\nfor col in num_cols:\n    replace_with_thresholds(df, col)","6a4fcea3":"# Filling \"Salary\" NaN values with KNN Imputer:\n\ndff = pd.get_dummies(df[cat_cols + num_cols], drop_first=True)\nscaler = MinMaxScaler()\ndff = pd.DataFrame(scaler.fit_transform(dff), columns=dff.columns)\nimputer = KNNImputer(n_neighbors=5)\ndff = pd.DataFrame(imputer.fit_transform(dff), columns=dff.columns)\ndff = pd.DataFrame(scaler.inverse_transform(dff), columns=dff.columns)\ndf[\"Salary\"] = dff[[\"Salary\"]]","3d962810":"# Now I have decided to search for hidden outliers based on Local Outlier Factor.\n\ndff = df.select_dtypes(include=['float64', 'int64'])\nclf = LocalOutlierFactor(n_neighbors=20)\nclf.fit_predict(dff)\ndff_scores = clf.negative_outlier_factor_\nscores = pd.DataFrame(np.sort(dff_scores))\nscores.plot(stacked=True, xlim=[0, 60], style='.-')\nplt.show()\nth = np.sort(dff_scores)[4]\nclf_index = df[dff_scores < th].index\ndf.iloc[(clf_index)]\ndf.drop(index=clf_index, inplace=True)","f3fb8ae4":"# Feature Engineering\n\n\n# Runs\/AtBat - Points won per hits in 1986-1987 season\ndf[\"Runs_AtBat\"] = df[\"Runs\"] \/ df[\"AtBat\"] * 100\n\n# HmRun\/AtBat - Perfect hits per hits in 1986-1987 season\ndf[\"HmRun_AtBat\"] = df[\"HmRun\"] \/ df[\"AtBat\"] * 100\n\n# CHits\/CAtBats - Successful hits per total hits of player's entire career\ndf[\"CHits_CAtBat\"] = df[\"CHits\"] \/ df[\"CAtBat\"] * 100\n\n# CHmRun\/CAtBat - Perfect hits per total hits of player's entire career\ndf[\"CHmRun_CAtBat\"] = df[\"CHmRun\"] \/ df[\"CAtBat\"] * 100\n\n# Errors\/CWalks - Player's error per mistakes done to the opponent\ndf[\"Error_Mean\"] = df[\"Errors\"] \/ df[\"CWalks\"] * 100\ndf['Error_Mean'] = df['Error_Mean'].replace(np.inf, 0)\n\n# Ultimate_Power - Mean of player's hits, perfect hits and scores won in it's entire career:\ndf[\"Ultimate_Power\"] = (df[\"CHits\"] + df[\"CHmRun\"] + df[\"CRuns\"]) \/ 3\n\n\n# Grab column names again:\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","b1db9cd3":"# One-Hot Econding\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=True):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe\n\ndf = one_hot_encoder(df, cat_cols)\n\n\n# Grab column names:\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","c444cb4f":"# Setting independent varaibles (X) and dependent variable (y):\n\nX = df.drop('Salary', axis=1)\ny = df[[\"Salary\"]]\n\n\n# Setting train and test parts with Holdout method:\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\n\n# Setting up the model and fitting it to the train part:\n\nreg_model = LinearRegression()\nreg_model.fit(X_train, y_train)","e0e4ac94":"# Now we have found the bias value (b):\n\nprint(\"Bias value: \" + str(reg_model.intercept_[0]))\n","34ec337b":"# Weight values (w):\n\nprint(\"Weight values: \" + str(reg_model.coef_[0]))","86961efb":"# RMSE value for Train part:\n\ny_pred = reg_model.predict(X_train)\nprint(\"Salary y_pred Train RMSE: \" + str(np.sqrt(mean_squared_error(y_train, y_pred))))","c3295ec9":"print(\"R-square value for Train part: \" + str(reg_model.score(X_train, y_train)))","bf6fb35a":"# Train part regplot:\n\ng = sns.regplot(x=y_train, y=y_pred, scatter_kws={'color': 'b', 's': 5},\n                ci=False, color=\"r\")\ng.set_title(f\"Train Model R2: = {reg_model.score(X_train, y_train):.4f}\")\ng.set_ylabel(\"Predicted Salary\")\ng.set_xlabel(\"Salary\")\nplt.xlim(-5, 2700)\nplt.ylim(bottom=0)\nplt.show()","e0790bf3":"# RMSE value for Test part:\n\ny_pred = reg_model.predict(X_test)\nprint(\"Salary y_pred Test RMSE: \" + str(np.sqrt(mean_squared_error(y_test, y_pred))))","2c58106c":"print(\"R-square value for Test part: \" + str(reg_model.score(X_test, y_test)))","1e54a60c":"# Test part regplot:\n\ng = sns.regplot(x=y_test, y=y_pred, scatter_kws={'color': 'b', 's': 5},\n                ci=False, color=\"r\")\ng.set_title(f\"Test Model R2: = {reg_model.score(X_test, y_test):.4f}\")\ng.set_ylabel(\"Predicted Salary\")\ng.set_xlabel(\"Salary\")\nplt.xlim(-5, 2700)\nplt.ylim(bottom=0)\nplt.show()","c7fd47b1":"print(\"10-fold cross model validation value - RMSE: \" + str(np.mean(np.sqrt(-cross_val_score(reg_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))))","f019222f":"# Plot importance \n\ndef plot_importance(save=False):\n    feature_imp = pd.DataFrame({\"Value\": reg_model.coef_[0], \"Feature\": X.columns})\n    plt.figure(figsize=(10,10))\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp, color=\"k\")\n    plt.title(\"Variable importance plot map\")\n    plt.grid(axis=\"x\")\n    plt.tight_layout()\n    if save:\n        plt.savefig(\"pic_imp.png\")\n    plt.show()\n\nplot_importance()","ebb8dff0":"**Comment:**\n\nNow because we have categoric (str) variables, we should apply One-Hot Encoding to them.","d27c833e":"**Comment:**\n\nUntil now, we have fitted Linear Regression to our dataset. Later applied Holdout method to split dataset to train and test parts. Train part has been evaluated and found the R-square value for Train part.\n\nNow we have to find out how actually our model will work on test part which is actually the most important part of the Linear Regression analysis.\n\nPlease find below codes for RMSE, R-square value and regplot for Test part.","7c81165f":"**Comment:**\n\nI have decided to select 4th value as the threshold between outlier and non-outliers. Values below 4th value deleted from the dataset.\n\nNow it's time to create new variables (Feature Engineering) to rise the accuracy of the Linear Regression model.\n\nI have created 6 new variables below by using already existing variables.","de37b34e":"Please find below the regplot of Test model:","31942e54":"Please find below R-square value of the Train model. Accuracy of model for Train part:","22a9faa0":"**Comment:**\n\nNow we can put a outlier thresholds and check for outliers in dataset based on 10\/90 thresholds.","ca6e0eff":"**Comment:** \n\n10-fold cross model validation value has been calculated at 289.70 RMSE. This is ~10% higher than our RMSE calculated with Holdout method. In that case out model is able to predict \"Salary\" variable with +-10% difference of %66.27 accuracy.\n\n**Now let's see the plot importance scores of our variables (the level of each variables contributing explaining the model):**","5b4f2a9f":"**Comment:**\n\nAs we can see, our graphs are showing the R-square value for Test part at 0.6627 level and with the accuracy of 268.89 RMSE.\n\nBut this has been found with Holdout method, which isn't as much reliable as K-fold Cross Model Validation.\n\nBelow, I have decided to use 10-fold cross model validation method:","55a6e06b":"# Linear Regression with Hitters dataset\n\n**What is Linear Regression?**\n\nA Linear Regression model describes the relationship between a dependent variable (y) and one or more independent variables (X). If there is one X, this model is called Simple Linear Regression and if there is more than one X, this model is called Multiple Linear Regression.\n\n**Why Hitters dataset should be evaluated with Linear Regression?**\n\nHitters dataset's dependent variable (y) is Salary and it's numerical, so it should be evaluated with Linear Regression. If the dataset would have a categoric dependent variable (0, 1) than we would use Logistic Regression.\n\n**Business Problem:**\n\nHitters dataset contains Salary information and a batch of information about baseball players in 1986.\nCan a machine learning project be carried out to estimate player's salary?\n\n**Dataset Story:**\n\nThis dataset was originally taken from the StatLib library at Carnegie Mellon University. The dataset is part of the data used in the 1988 ASA Graphics Division Poster Session. Salary data are from Sports Illustrated, April 20, 1987.\n\n**Variables:**\n\nAtBat: 1986-1987 the number of times a baseball bat hits the ball\n\nHits: 1986-1987 hits\n\nHmRun: 1986-1987 perfect hits\n\nRuns: 1986-1987 points won to the team\n\nRBI: The number of players a batsman walked when he hit\n\nWalks: Number of mistakes made by the opposing player\n\nYear: How long the player has played in the major league (years)\n\nCAtBat: Number of ball hits throughout the player's career\n\nCHits: Number of hits hit during the player's career\n\nCHmRun: Number of perfect hits during the player's career\n\nCRuns: Player scoring points for his team throughout his career\n\nCRBI: The number of players the player has made to walk during his career\n\nCWalks : The number of mistakes the player has made to the opposing player during his career\n\nLeague: Has a factor that indicates ownership of the player's seasonal levels (A or N)\n\nDivision: A factor with additional contributing indicator E and W levels for 1986 season\n\nPutOuts: Helping your teammate in-game\n\nAssists: 1986-1987 counts as assists\n\nErrors: Number of player errors in 1986-1987\n\nSalary: The player earned money 1986-1987 (thousand)\n\nNewLeague: A factor showing the league of a player used in 1987 and has A & N levels","79081533":"**Thank you for your reading!**\n\n**This work has been done with the support of** [VBO](https:\/\/www.veribilimiokulu.com\/), [Miuul](https:\/\/miuul.com\/),  [Vahit Keskin](https:\/\/www.linkedin.com\/in\/vahitkeskin\/), [O\u011fuz Erdo\u011fan](https:\/\/www.linkedin.com\/in\/oguzerdo\/), [Hande K\u00fc\u00e7\u00fckbulut](https:\/\/www.linkedin.com\/in\/hande-kucukbulut\/), [Mehmet Tuzcu](https:\/\/www.linkedin.com\/in\/mehmettuzcu\/) & [Burak Do\u011frul](https:\/\/www.linkedin.com\/in\/burakdogrul\/).","1201d311":"**Comment:**\n\nIt's time to set the Linear Regression model to our dataset:","c1ee2988":"# Test part analysis:","c32ef7c7":"# Train part analysis:","adb50ba5":"We have found bias and weights from dataset. Now it's time to test their accuracy (RMSE).\n\nPlease find below prediction of \"Salary\" dependent variable (y_pred) from modeled X_train part.","fccdf60f":"**Comment:**\n\nNow we should fill the \"Salary\" variable's NaN values with KNN Imputer. I have decided to consider mean of 5 closest neighbors of NaN values.","b32f17cf":"Please find below the regplot of Train model:","d99225a6":"**Comment:**\n\nAs we can see, only few varaibles have very few outliers, which if replaced with thresholds won't effect the overall model result.","871499c2":"**Comment:**\n\nAs we can see above the dataset's Salary variable has 59 NaN values. This values will be filled latter.\n\nAlso, quantiles are indicating that there are outliers above %90. This outliers will be described first and than we should decide what to do with these outliers. But first we should grab column names by their type."}}