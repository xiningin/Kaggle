{"cell_type":{"2cdbc6fa":"code","0d4d26eb":"code","fed03c7d":"code","ba2ac7c7":"code","613f8a76":"code","ab0bd0a8":"code","a2f59683":"code","c0323781":"code","03cc87ef":"code","c195977d":"code","4981ec3f":"code","f541c2e1":"code","d1af96a2":"code","a97d9c29":"code","1a2a8d50":"code","fa273558":"code","d543c58c":"code","718cea7b":"code","5ae99992":"code","9e14920f":"code","772afbfb":"code","94568040":"code","6e317859":"code","e2b547ba":"code","ee71b320":"code","a8ff8b11":"code","9975b250":"code","60c788ce":"code","db2521e4":"code","e2913481":"code","d10c173a":"code","a2fdcd7c":"code","6deb5d36":"code","abd26398":"code","5eb443be":"code","c76734de":"code","a95fe6b8":"markdown","82964327":"markdown"},"source":{"2cdbc6fa":"## Competi\u00e7\u00e3o DSA de Machine Learning - Edi\u00e7\u00e3o Junho\/2019\n# MARCIO DE LIMA\n\n# As submiss\u00f5es para esta competi\u00e7\u00e3o ser\u00e3o avaliadas pelo RMSE (Root Mean Squared Error).\n","0d4d26eb":"# Nesta competi\u00e7\u00e3o, voc\u00ea desenvolver\u00e1 algoritmos para identificar e atender as oportunidades mais relevantes \n# para os indiv\u00edduos, revelando sinais de lealdade dos clientes. Sua contribui\u00e7\u00e3o melhorar\u00e1 a vida dos \n# clientes e ajudar\u00e1 a reduzir as campanhas indesejadas, a fim de criar uma e experi\u00eancia mais personalizada \n# para cada cliente e consequentemente aumentar a satisfa\u00e7\u00e3o e claro, as vendas.","fed03c7d":"# Importando as bibliotecas\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.core.pylabtools import figsize\nimport seaborn as sns\nimport warnings\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","ba2ac7c7":"# Carregando os arquivos\ndf = pd.read_csv('..\/input\/dataset_treino.csv')\ndf_teste = pd.read_csv('..\/input\/dataset_teste.csv')\n\ndf.head(5)","613f8a76":"# Mostrando as estruturas dos Datasets\ndf.info()","ab0bd0a8":"df_teste.info()","a2f59683":"#Limpando espa\u00e7os do texto caso existam\ndf['first_active_month'] = df['first_active_month'].str.strip()\ndf_teste['first_active_month'] = df_teste['first_active_month'].str.strip()\n","c0323781":"#Checando valores NA nos dados\ndf.isna().any()[lambda x: x]\ndf_teste.isna().any()[lambda x: x]","03cc87ef":"df_teste.dropna()\ndf_teste.isna().any()[lambda x: x]","c195977d":"# Ajustando colunas e limpeza dos dados nos DataSets\n\n#Criando novas colunas\ndf['Month'] = df.first_active_month.apply(lambda dt: dt[5:7])\ndf['Year'] = df.first_active_month.apply(lambda dt: dt[:4])\ndf_teste['Month'] = df_teste.first_active_month.apply(lambda dt: '01' if isinstance(dt,float)  else dt[5:7])\ndf_teste['Year'] = df_teste.first_active_month.apply(lambda dt: '2000' if isinstance(dt,float)  else dt[:4])\n\n#Ajustando a tipagem da coluna\ndf['Month'] = df['Month'].apply(pd.to_numeric, downcast='integer')\ndf['Year'] = df['Year'].apply(pd.to_numeric, downcast='integer')\ndf_teste['Month'] = df['Month'].apply(pd.to_numeric, downcast='integer')\ndf_teste['Year'] = df['Year'].apply(pd.to_numeric, downcast='integer')\n\ndf.dtypes","4981ec3f":"# Criando coluna = calculo das features\ndf['feature_4'] = (df['feature_1'] * df['feature_1'].mean()) + (df['feature_2']  * df['feature_2'].mean()) + (df['feature_3'] * df['feature_3'].mean())\ndf_teste['feature_4'] = (df_teste['feature_1'] * df_teste['feature_1'].mean()) + (df_teste['feature_2']  * df_teste['feature_2'].mean()) + (df_teste['feature_3'] * df_teste['feature_3'].mean())\ndf.head(5)","f541c2e1":"# Correla\u00e7\u00e3o com a Variavel TARGET\ndf[df.columns.drop('target')].corrwith(df.target)\n","d1af96a2":"# Dados estatisticos\ndf.describe()","a97d9c29":"#Construindo um gr\u00e1fico de HEATMAP\nf, ax = plt.subplots(figsize=(15, 12))\nsns.heatmap(df.corr(),linewidths=.5, ax=ax)","1a2a8d50":"#Gerando gr\u00e1ficos para analise das variaveis\n\n#Histogramas\ndf.plot(kind = 'hist', subplots = True, layout = (7,3), sharex = False, figsize=(20,70))\nplt.show()","fa273558":"df.plot(kind = 'density', subplots = True, layout = (7,3), sharex = False, figsize=(20,70))\nplt.show()","d543c58c":"df.plot(kind = 'box', subplots = True, layout = (7,3), sharex = False, sharey = False, figsize=(20,70))\nplt.show()","718cea7b":"#Funcoes utilit\u00e1rias para medir a performance dos modelos\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\ndef rmspe(y_test, y_pred):\n\n    mse = mean_squared_error(y_test, y_pred)\n    rmspe = sqrt(mse)   \n    \n    return rmspe\n\n# Treinamento e resultado do modelo - funcao generica\ndef treine_e_avalie(model, X, y, X_test, y_test):\n    \n    \n    # Predicao\n    model_pred = treino_e_predicao(model, X, y, X_test)\n    #Performance\n    model_rmspe = rmspe(y_test, model_pred)\n    \n    # Retorno da Performance do modelo\n    return model_rmspe\n\ndef treino_e_predicao(model, X, y, X_test):\n    \n    # FIT\n    model.fit(X, y)\n    # Predicao\n    return model.predict(X_test)","5ae99992":"df.shape","9e14920f":"#Gerando dados de Treino e de Teste para os modelos\nfrom sklearn.model_selection import train_test_split\n\nseed = 1313\n\narray = df.values\nX = array[:,6:9]\nY = df.target.values\n\nX_treino, X_teste, y_treino, y_teste = train_test_split(X, Y, test_size = 0.30, random_state = seed)\n","772afbfb":"X_treino","94568040":"# Importando os modelos\n\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor","6e317859":"# Modelo 1 - Regressao Linear Simples\n\nlr = LinearRegression()\nlr_rmspe = treine_e_avalie(lr, X_treino, y_treino, X_teste, y_teste)\n\nprint('Modelo 1 - Regressao Linear => RMSPE = %0.4f' % lr_rmspe)","e2b547ba":"# Modelo 2 - KNN\nknn = KNeighborsRegressor(n_neighbors=5)\nknn_rmspe = treine_e_avalie(knn, X_treino, y_treino, X_teste, y_teste)\n\nprint('Modelo 2 - KNN => RMSPE = %0.4f' % knn_rmspe)","ee71b320":"# Modelo 3 - GradientBoostingRegressor\ngradient_boosted = GradientBoostingRegressor(random_state=60)\ngradient_boosted_rmspe = treine_e_avalie(gradient_boosted, X_treino, y_treino, X_teste, y_teste)\n\nprint('Modelo 3 - GradientBoostingRegressor = %0.4f' % gradient_boosted_rmspe)\n","a8ff8b11":"# Otimizando o modelo 3 - Otimiza\u00e7\u00e3o de Hyperpar\u00e2metro\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Modelo para testar a otimiza\u00e7\u00e3o\ngbr = GradientBoostingRegressor(random_state=13)\n#Parametros da otimiza\u00e7\u00e3o\nparam_grid = {\n        'n_estimators': [100, 200, 500],\n        'max_features': ['auto', 'sqrt', 'log2', None],\n        'max_depth': [2, 3, 5, 10, 15],\n        'learning_rate': [0.1],\n        'loss': ['ls', 'lad', 'huber'],\n        'subsample': [1]\n}\n\n#Modelo para melhor scoring para o RMSE\nmodelo_otm = RandomizedSearchCV(estimator=gbr,\n                               param_distributions=param_grid,\n                               cv=2, n_iter=1, \n                               scoring = 'neg_mean_absolute_error',\n                               n_jobs = -1, verbose = 1, \n                               return_train_score = True,\n                               random_state=60)\n","9975b250":"#Treinando o modelo otimizado\nmodelo_otm.fit(X_treino, y_treino)\n","60c788ce":"#Resultado do modelo otimizado\nprint('Melhores Params:')\nprint(modelo_otm.best_params_)\nprint('Melhor CV Score:')\nprint(-modelo_otm.best_score_)","db2521e4":"# Modelo 4 - GradientBoostingRegressor Otimizado\n# Melhores parameters\n#gradient_boosted1_otm = GradientBoostingRegressor(max_depth=2, max_features='sqrt',\n#                                                  n_estimators=500, loss='lad', random_state=60, \n#                                                  learning_rate=0.1, verbose=1, subsample=1)\n\ngradient_boosted1_otm = GradientBoostingRegressor( max_depth=3, max_features='sqrt', \n                                                   n_estimators=100, \n                                                   criterion='mse',\n                                                   learning_rate=0.05,\n                                                   random_state=60)\n\nmodelo_pred_otm = treino_e_predicao(gradient_boosted1_otm, X_treino, y_treino, X_teste)\ngradient_boosted_rmspe_otm = rmspe(y_teste, modelo_pred_otm)\n\nprint('Modelo 4 - GradientBoostingRegressor - Otimizado = %0.4f' % gradient_boosted_rmspe_otm)\n","e2913481":"#Gerando os dados para o Arquivo de submissao\narray = df_teste.values\nX_teste1 = array[:,5:9]\n","d10c173a":"X_teste1","a2fdcd7c":"#Gerando Arquivo de Submissao\ndf_submission = pd.DataFrame()\ndf_submission['card_id'] = df_teste['card_id']\n\nresultado_otm = gradient_boosted1_otm.predict(X_teste1)","6deb5d36":"resultado_otm","abd26398":"df_submission['target'] = resultado_otm","5eb443be":"df_submission.head(10)","c76734de":"#Gravando Arquivo de Submissao\ndf_submission.to_csv('submission.csv', index=False)","a95fe6b8":"## Melhor modelo - GradientBoostingRegressor - 3.8544","82964327":"## FIM\n## Obrigado"}}