{"cell_type":{"ae7fd956":"code","4564e5a7":"code","915f41f2":"code","e27a2b18":"code","7430a2e3":"code","22fe03d0":"code","3b627e44":"code","b553ffa4":"code","3a2abf57":"code","d4044708":"code","2f955d48":"code","a3a8d092":"code","e0e108bc":"code","afa394bd":"code","0de97173":"code","dc902cfc":"code","155e6b94":"code","c3b68a22":"code","346322d3":"code","d13879b0":"code","e7c43176":"code","7035585d":"code","6b6d01e3":"code","9111b65c":"code","762923c4":"code","7f77e5a3":"code","eda8bd0f":"markdown","d70d302c":"markdown","29dd852d":"markdown","55dfc1cb":"markdown","fd195e43":"markdown","639ef080":"markdown","91da3a28":"markdown","e95a6248":"markdown","e8b2f603":"markdown","5b2f315d":"markdown","b0dee945":"markdown","ca55ded6":"markdown","77796625":"markdown","f2ebb22d":"markdown","23c54b67":"markdown","c2034cca":"markdown"},"source":{"ae7fd956":"# Dependencies\n\nimport numpy as np\nimport pandas as pd\n\nimport os\nimport json\nfrom joblib import Parallel, delayed\nfrom PIL import Image\nfrom tqdm import tqdm\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.applications.densenet import preprocess_input, DenseNet121\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D, \\\n    MaxPooling1D, Dense, BatchNormalization, Dropout, Embedding, Reshape, Concatenate\nfrom tensorflow.keras import losses\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.backend as K\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD, NMF\nfrom sklearn.metrics import cohen_kappa_score, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\n\nimport lightgbm as lgb\n\nfrom gensim.models import KeyedVectors\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4564e5a7":"# Random seed function (thanks to Benjamin Minixhofer)\n\nseed = 73\n\ndef seed_everything(seed=seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.set_random_seed(seed)\n    np.random.seed(seed)","915f41f2":"# Load dataframes\ntrain_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/train\/train.csv')\ntest_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/test\/test.csv')\n\nbreeds_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')\ncolors_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/color_labels.csv')\nstates_df = pd.read_csv('..\/input\/petfinder-adoption-prediction\/state_labels.csv')","e27a2b18":"# Correct possible data errors\n\n# Replace Breed1 with Breed2\ntrain_df['Breed1'].replace(0, train_df['Breed2'], inplace=True)\n\n# Replace Breed1 with 0\nids = ['1bc0f89d8', '15a206d0d', 'f8654865f', '36b20cfb5',\n       '699a81c51', '85ec1aac0','6a72cfda7'] \ntrain_df.loc[train_df['PetID'].isin(ids), 'Breed1'] = 0\n\n# Replace Breed2 with 0\nids = ['f8654865f', '699a81c51', '6a72cfda7']\ntrain_df.loc[train_df['PetID'].isin(ids), 'Breed2'] = 0\n\n# Change Type to 1\ntrain_df.loc[train_df['PetID'] == '6c399cb06', 'Type'] = 1","7430a2e3":"# Extraction functions\n\ndef get_metadata_features(pet_id, dataset):\n    \"\"\"\n    Collects the following features from the image metadata for profile images.\n    \n    1. Image resolution.\n    2. Top 3 dominant colors by score.\n    \"\"\"\n    json_path = '..\/input\/petfinder-adoption-prediction\/{}_metadata\/{}-1.json'.format(dataset, pet_id)\n    image_path = '..\/input\/petfinder-adoption-prediction\/{}_images\/{}-1.jpg'.format(dataset, pet_id)\n    \n    if not os.path.exists(json_path):\n        # Test sample with no profile picture\n        if os.path.exists('..\/input\/petfinder-adoption-prediction\/{}_metadata\/{}-2.json'.format(dataset, pet_id)):\n            json_path = '..\/input\/petfinder-adoption-prediction\/{}_metadata\/{}-2.json'.format(dataset, pet_id)\n            image_path = '..\/input\/petfinder-adoption-prediction\/{}_images\/{}-2.jpg'.format(dataset, pet_id)\n        else:\n            return\n    \n    row = {}\n    \n    row['PetID'] = pet_id\n    \n    with open(json_path) as fp:\n        row_json = json.load(fp)\n    \n    try:\n        image = Image.open(image_path)\n        row['img_width'], row['img_height'] = image.size\n    except:\n        row['img_width'], row['img_height'] = np.nan, np.nan\n    \n    try:\n        colors = row_json['imagePropertiesAnnotation']['dominantColors']['colors']\n        reds, greens, blues, scores = [], [], [], []\n        for color in colors:\n            reds.append(color['color'].get('red', 0))\n            greens.append(color['color'].get('green', 0))\n            blues.append(color['color'].get('blue', 0))\n            scores.append(color.get('score', 0))\n        colors_df = pd.DataFrame({'red': reds, 'green': greens, 'blue': blues, 'score': scores})\n        row.update(dict(zip(['img_color_1_red', 'img_color_1_green', 'img_color_1_blue',\n                             'img_color_2_red', 'img_color_2_green', 'img_color_2_blue',\n                             'img_color_3_red', 'img_color_3_green', 'img_color_3_blue'],\n                            colors_df.sort_values('score', ascending=False).iloc[:3, :-1].values.ravel())))\n    except:\n        row.update(dict(zip(['img_color_1_red', 'img_color_1_green', 'img_color_1_blue',\n                             'img_color_2_red', 'img_color_2_green', 'img_color_2_blue',\n                             'img_color_3_red', 'img_color_3_green', 'img_color_3_blue'], [np.nan] * 9)))\n    \n    return row\n\n\ndef get_sentiment_features(filename, dataset):\n    \"\"\"\n    Collects the following features from the sentiment data.\n    \n    1. Sentences scores mean and variance weighted by magnitude.\n    2. Document sentiment magnitude and score.\n    \"\"\"\n    path = '..\/input\/petfinder-adoption-prediction\/' + dataset + '_sentiment'\n    with open(os.path.join(path, filename)) as fp:\n        row_json = json.load(fp)\n    row = {}\n\n    row['PetID'] = filename.replace('.json', '')\n    \n    try:\n        magnitudes, scores = [], []\n        for sentence in row_json['sentences']:\n            magnitudes.append(sentence['sentiment']['magnitude'])\n            scores.append(sentence['sentiment']['score'])\n        sentences_df = pd.DataFrame({'magnitude': magnitudes, 'score': scores})\n        sentences_df['score'] = sentences_df['magnitude'] * sentences_df['score']\n        epsilon = np.finfo(np.float32).eps\n        sentences_df['score'] = sentences_df['magnitude'] \/ (sentences_df['magnitude'].sum() + epsilon)\n        row['sentence_score_mean'] = sentences_df['score'].mean()\n        row['sentence_score_var'] = sentences_df['score'].var()\n    except:\n        row['sentence_score_mean'] = np.nan\n        row['sentence_score_var'] = np.nan \n\n    try:\n        row['document_magnitude'] = row_json['documentSentiment']['magnitude']\n        row['document_score'] = row_json['documentSentiment']['score']\n    except:\n        row['document_magnitude'] = np.nan\n        row['document_score'] = np.nan\n    \n    return row\n\n# Use parallel processing\ntrain_metadata_rows = Parallel(n_jobs=-1, verbose=2)(\n    delayed(get_metadata_features)(pet_id, 'train') for pet_id in train_df['PetID'])\ntrain_metadata_df = pd.DataFrame([row for row in train_metadata_rows if row is not None])\ntest_metadata_rows = Parallel(n_jobs=-1, verbose=2)(\n    delayed(get_metadata_features)(pet_id, 'test') for pet_id in test_df['PetID'])\ntest_metadata_df = pd.DataFrame([row for row in test_metadata_rows if row is not None])\n\ntrain_sentiment_df = pd.DataFrame(Parallel(n_jobs=-1, verbose=2)(\n    delayed(get_sentiment_features)(filename,'train') for filename in os.listdir(\n        '..\/input\/petfinder-adoption-prediction\/train_sentiment')))\ntest_sentiment_df = pd.DataFrame(Parallel(n_jobs=-1, verbose=2)(\n    delayed(get_sentiment_features)(filename, 'test') for filename in os.listdir(\n        '..\/input\/petfinder-adoption-prediction\/test_sentiment')))\n\n# Merge everything\ntrain_merged = pd.merge(train_df, train_metadata_df, how='left', on='PetID')\ntrain_merged = pd.merge(train_merged, train_sentiment_df, how='left', on='PetID')\ntest_merged = pd.merge(test_df, test_metadata_df, how='left', on='PetID')\ntest_merged = pd.merge(test_merged, test_sentiment_df, how='left', on='PetID')","22fe03d0":"# New features\n\n# Add name length\ntrain_merged['name_len'] = train_merged['Name'].map(len, na_action='ignore')\ntest_merged['name_len'] = test_merged['Name'].map(len, na_action='ignore')\n\n# Add description length\ntrain_merged['desc_len'] = train_merged['Description'].map(len, na_action='ignore')\ntest_merged['desc_len'] = test_merged['Description'].map(len, na_action='ignore')\n\n# Add RescuerID count\ntrain_merged['rescuer_count'] = train_merged['RescuerID'].replace(train_merged.groupby('RescuerID').size())\ntest_merged['rescuer_count'] = test_merged['RescuerID'].replace(test_merged.groupby('RescuerID').size())","3b627e44":"# We simply average pretrained FastText vectors for description\n\nmodel = KeyedVectors.load_word2vec_format('..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec')\n\npet_ids = train_df[~train_df['Description'].isna()]['PetID']\nvects = []\nfound_pet_ids = []\nfor pet_id in pet_ids:\n    desc = train_df[train_df['PetID'] == pet_id]['Description'].values[0].split(' ')\n    word_vectors = []\n    for word in desc:\n        try:\n            word_vectors.append(model.get_vector(word))\n        except KeyError:\n            pass\n    if word_vectors:\n        mean_vect = np.mean(word_vectors, axis=0)\n        vects.append(mean_vect)\n        found_pet_ids.append(pet_id)\nfasttext_train_df = pd.DataFrame(np.array(vects)).add_prefix('fasttext_')\nfasttext_train_df['PetID'] = found_pet_ids\ntrain_merged = pd.merge(train_merged, fasttext_train_df, how='left', on='PetID')\n\npet_ids = test_df[~test_df['Description'].isna()]['PetID']\nvects = []\nfound_pet_ids = []\nfor pet_id in pet_ids:\n    desc = test_df[test_df['PetID'] == pet_id]['Description'].values[0].split(' ')\n    word_vectors = []\n    for word in desc:\n        try:\n            word_vectors.append(model.get_vector(word))\n        except KeyError:\n            pass\n    if word_vectors:\n        mean_vect = np.mean(word_vectors, axis=0)\n        vects.append(mean_vect)\n        found_pet_ids.append(pet_id)\nfasttext_test_df = pd.DataFrame(np.array(vects)).add_prefix('fasttext_')\nfasttext_test_df['PetID'] = found_pet_ids\ntest_merged = pd.merge(test_merged, fasttext_test_df, how='left', on='PetID')","b553ffa4":"text_columns = ['Description']\n\n# Fill nans with empty text\ntrain_merged[text_columns] = train_merged[text_columns].fillna('')\ntest_merged[text_columns] = test_merged[text_columns].fillna('')\n\n# Text feature extractor class\n# We use TF-IDF vectorizer and then extract SVD and NMF vectors with 13 components each\n\nclass TextFeatureExtractor():\n    \"\"\"Extracts text features from text columns.\"\"\"\n    def __init__(self, n_components):\n        self.tfidf = TfidfVectorizer(min_df=2, max_features=None,\n                          strip_accents='unicode', analyzer='word', token_pattern='\\w+',\n                          ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1)\n        self.svd = TruncatedSVD(n_components=n_components, random_state=seed)\n        self.nmf = NMF(n_components=n_components, random_state=seed)\n        \n    def fit_transform(self, X_text):\n        text_features = []\n        for col in X_text.columns:\n            tfidf_col = self.tfidf.fit_transform(X_text[col])\n            \n            svd_col = self.svd.fit_transform(tfidf_col)\n            svd_col = pd.DataFrame(svd_col)\n            svd_col = svd_col.add_prefix('SVD_{}_'.format(col))\n            text_features.append(svd_col)\n            \n            nmf_col = self.nmf.fit_transform(tfidf_col)\n            nmf_col = pd.DataFrame(nmf_col)\n            nmf_col = nmf_col.add_prefix('NMF_{}_'.format(col))\n            text_features.append(nmf_col)\n            \n        text_features = pd.concat(text_features, axis=1)\n        \n        return text_features\n    \n    def transform(self, X_text):\n        text_features = []\n        for col in X_text.columns:\n            tfidf_col = self.tfidf.transform(X_text[col])\n            \n            svd_col = self.svd.transform(tfidf_col)\n            svd_col = pd.DataFrame(svd_col)\n            svd_col = svd_col.add_prefix('SVD_{}_'.format(col))\n            text_features.append(svd_col)\n            \n            nmf_col = self.nmf.transform(tfidf_col)\n            nmf_col = pd.DataFrame(nmf_col)\n            nmf_col = nmf_col.add_prefix('NMF_{}_'.format(col))\n            text_features.append(nmf_col)\n            \n        text_features = pd.concat(text_features, axis=1)\n        \n        return text_features\n\n    \ntext_feature_extractor = TextFeatureExtractor(n_components=13)","3a2abf57":"# We extract image features using DenseNet121 and apply Average Pooling\n# with window_size=4 for profile images and window_size=8 for second images.\n\nweights_path = '..\/input\/densenet121weights\/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\nclass ImageFeatureExtractor():\n    def __init__(self,\n                 shape=[256, 256, 3],\n                 average_pooling_window=4):\n        self.shape = shape\n        self.size = self.shape[:2]\n        input_tensor = Input(shape)\n        densenet = DenseNet121(input_tensor=input_tensor,\n                               weights=weights_path,\n                               include_top=False)\n        out = densenet.output\n        out = GlobalAveragePooling2D()(out)\n        out = Lambda(lambda x: K.expand_dims(x, axis=-1))(out)\n        if average_pooling_window:\n            out = AveragePooling1D(average_pooling_window)(out)\n        out = Lambda(lambda x: x[:,:,0])(out)\n        \n        self.model = Model(input_tensor, out)\n        self.feats_shape = list(map(int, self.model.output.shape[1:]))\n\n    def resize_to_square(self, img):\n        return img.resize(self.size) \n\n    def resize_saving_ratio(self, img):\n        # works if self.size represents a square\n        # resize initial image\n        max_dim = max(img.width, img.height)\n        k = self.size[0] \/ max_dim\n        width = int(img.width * k)\n        height = int(img.height * k)\n        img = img.resize([width, height])\n        # concat with black rectangle\n        res_img = Image.new('RGB', self.size)\n        res_img.paste(img, (0, 0))\n        return res_img\n\n    def load_image_by_path(self, filepath, resize_method='square'):\n        img = Image.open(filepath)\n        if resize_method == 'square':\n            img = self.resize_to_square(img)\n        else:\n            img = self.resize_saving_ratio(img)\n        img = np.array(img).astype(np.float32)\n        img = preprocess_input(img)\n        if len(img.shape) == 2:\n            img = np.repeat(np.expand_dims(img, axis=2), repeats=3, axis=2)\n        return img\n\n    def extract(self, filepath, resize_method='square'):\n        img = self.load_image_by_path(filepath, resize_method='square')\n        return self.model.predict(np.expand_dims(img, axis=0))\n\n    def extract_all(self, filepaths, batch_size=16, resize_method='square'):\n        res_feats = np.empty(shape=[0]+self.feats_shape, dtype=np.float32)\n        num_batches = int(np.ceil(len(filepaths) \/ batch_size))\n        for it in tqdm(range(num_batches)):\n            batch_filepaths = filepaths[it * batch_size: (it + 1) * batch_size]\n            batch = []\n            for fp in batch_filepaths:\n                img = self.load_image_by_path(fp, resize_method=resize_method)\n                batch.append(img)\n            batch = np.array(batch)\n            feats = self.model.predict(batch)\n            res_feats = np.append(res_feats, feats, axis=0)\n        return res_feats\n\n\ndef get_image_filepaths(pet_ids, dataset, img_number):\n    filepaths = []\n    found_pet_ids = []\n    for pet_id in pet_ids:\n        path = '..\/input\/petfinder-adoption-prediction\/{}_images\/{}-{}.jpg'.format(dataset,\n                                                                                   pet_id,\n                                                                                   img_number)\n        if os.path.exists(path):\n            filepaths.append(path)\n            found_pet_ids.append(pet_id)\n        elif os.path.exists('..\/input\/petfinder-adoption-prediction\/{}_images\/{}-2.jpg'.format(dataset, pet_id)):\n            path = '..\/input\/petfinder-adoption-prediction\/{}_images\/{}-2.jpg'.format(dataset,\n                                                                                      pet_id)\n            filepaths.append(path)\n            found_pet_ids.append(pet_id)\n    return filepaths, found_pet_ids\n\n# Profile images\nimage_feature_extractor = ImageFeatureExtractor()\n\ntrain_img_filepaths, train_found_pet_ids = get_image_filepaths(train_df['PetID'], 'train', 1)\ntest_img_filepaths, test_found_pet_ids = get_image_filepaths(test_df['PetID'], 'test', 1)\n\ntrain_img_feats = image_feature_extractor.extract_all(train_img_filepaths,\n                                                      resize_method='square')\ntrain_img_feats_df = pd.DataFrame(train_img_feats).add_prefix('img_feat_')\ntrain_img_feats_df['PetID'] = train_found_pet_ids\ntest_img_feats = image_feature_extractor.extract_all(test_img_filepaths,\n                                                     resize_method='square')\ntest_img_feats_df = pd.DataFrame(test_img_feats).add_prefix('img_feat_')\ntest_img_feats_df['PetID'] = test_found_pet_ids\n    \ntrain_merged = pd.merge(train_merged, train_img_feats_df, how='left', on='PetID')\ntest_merged = pd.merge(test_merged, test_img_feats_df, how='left', on='PetID')","d4044708":"# Second images\nimage_feature_extractor = ImageFeatureExtractor(average_pooling_window=8)\n\ntrain_img_filepaths, train_found_pet_ids = get_image_filepaths(train_df['PetID'], 'train', 2)\ntest_img_filepaths, test_found_pet_ids = get_image_filepaths(test_df['PetID'], 'test', 2)\n\ntrain_img_feats = image_feature_extractor.extract_all(train_img_filepaths,\n                                                      resize_method='square')\ntrain_img_feats_df = pd.DataFrame(train_img_feats).add_prefix('img2_feat_')\ntrain_img_feats_df['PetID'] = train_found_pet_ids\ntest_img_feats = image_feature_extractor.extract_all(test_img_filepaths,\n                                                     resize_method='square')\ntest_img_feats_df = pd.DataFrame(test_img_feats).add_prefix('img2_feat_')\ntest_img_feats_df['PetID'] = test_found_pet_ids\n    \ntrain_merged = pd.merge(train_merged, train_img_feats_df, how='left', on='PetID')\ntest_merged = pd.merge(test_merged, test_img_feats_df, how='left', on='PetID')","2f955d48":"# Regression objective\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\n# Competition metric\ndef qwk(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')","a3a8d092":"# We tried different rounding techniques for threshold optimization.\n# For us rounding by following train distribution gave best results.\n\ndef get_thresholds_from_dist(y_true, y_pred):\n    \"\"\"Calculates thresholds for raw predictions\n    so as to follow the true distribution.\n    \"\"\"\n    idxs = np.cumsum(np.bincount(y_true))[:-1]\n    idxs = (idxs * y_pred.size \/ y_true.size).astype(int)\n    return np.sort(y_pred)[idxs]\n\ndef allocate_to_rate(y_pred, thresholds):\n    \"\"\"Allocates raw predictions to adoption rates.\"\"\"\n    rates = np.zeros(y_pred.size, dtype=int)\n    for i in range(4):\n        rates[y_pred >= thresholds[i]] = i + 1\n    return rates","e0e108bc":"# Final datasets arrangement\n\nX_train = train_merged.drop(columns=['PetID', 'AdoptionSpeed'])\ny_train = train_merged['AdoptionSpeed']\n\nX_test = test_merged.drop(columns=['PetID'])\n\n# We also add the most frequent breed of each rescuer as feature.\nX_train['rescuer_breed_mode'] = X_train['RescuerID'].map(X_train.groupby('RescuerID')['Breed1'].agg(\n    lambda x:x.value_counts().index[0]))\nX_test['rescuer_breed_mode'] = X_test['RescuerID'].map(X_test.groupby('RescuerID')['Breed1'].agg(\n    lambda x:x.value_counts().index[0]))\n\ncat_feats = ['Type', 'Breed1', 'Breed2', 'Vaccinated',\n             'Dewormed', 'Sterilized', 'State', 'rescuer_breed_mode']\n\nX_train = X_train.drop(columns=['Name', 'RescuerID', 'Description'])\nX_test = X_test.drop(columns=['Name', 'RescuerID', 'Description'])","afa394bd":"# Some useful dicts\n\nonehot_feats = ['Type', 'Gender',\n             'Color1', 'Color2', 'Color3', 'Vaccinated',\n             'Dewormed', 'Sterilized']\nonehot_sizes = dict(X_train[onehot_feats].nunique())\nonehot_sizes['Color2'] = onehot_sizes['Color1']\nonehot_sizes['Color3'] = onehot_sizes['Color1']\ncat_feats = ['Breed1', 'Breed2', 'rescuer_breed_mode', 'State']\nX_concat = pd.concat([X_train, X_test])\nembedding_sizes = {\n    'Breed1': 32,\n    'Breed2': 32,\n    'rescuer_breed_mode': 32,\n    'State': 8\n}\ncat_feats_sizes = {\n    'Breed1': X_concat['Breed1'].nunique(),\n    'Breed2': X_concat['Breed2'].nunique(),\n    'rescuer_breed_mode': X_concat['rescuer_breed_mode'].nunique(),\n    'State': X_concat['State'].nunique()\n}\n\ncat_feats_mappings = {}\nfor cat_feat in cat_feats:\n    mapping = {}\n    vals = X_concat[cat_feat].unique()\n    vals.sort()\n    for i, feat in enumerate(vals):\n        mapping[feat] = i\n    cat_feats_mappings[cat_feat] = mapping\n\nimg_feats = ['img_feat_{}'.format(i) for i in range(256)]\ntext_feats = ['fasttext_{}'.format(i) for i in range(300)]\nnumerical_feats = [col for col in X_train.columns \n                   if not col in img_feats and\n                   not col in text_feats and\n                   not col in onehot_feats and\n                   not col in cat_feats]","0de97173":"# Numerical feats\nnum_scaler = StandardScaler()\nX_train[numerical_feats] = num_scaler.fit_transform(X_train[numerical_feats])\nX_test[numerical_feats] = num_scaler.transform(X_test[numerical_feats])\n\n# Image feats\nimg_scaler = StandardScaler()\nX_train[img_feats] = img_scaler.fit_transform(X_train[img_feats])\nX_test[img_feats] = img_scaler.transform(X_test[img_feats])\n\n# Text feats\ntext_scaler = StandardScaler()\nX_train[text_feats] = text_scaler.fit_transform(X_train[text_feats])\nX_test[text_feats] = text_scaler.transform(X_test[text_feats])\n\nX_train[X_train.isna()] = 0\nX_test[X_test.isna()] = 0","dc902cfc":"def get_model():\n    K.clear_session()\n    # Numerical feats\n    num_input = Input(shape=[len(numerical_feats)])\n\n    num_out = Dense(32, activation='relu')(num_input)\n    num_out = BatchNormalization()(num_out)\n    num_out = Dropout(rate=0.66)(num_out)\n\n    # Image feats\n    img_input = Input(shape=[256])\n\n    img_out = Lambda(lambda x: K.expand_dims(x, axis=-1))(img_input)\n    img_out = AveragePooling1D(4)(img_out)\n    img_out = Lambda(lambda x: x[:,:,0])(img_out)\n    img_out = Dense(128, activation='relu')(img_out)\n    img_out = BatchNormalization()(img_out)\n    img_out = Dropout(rate=0.66)(img_out)\n\n    # Text feats\n    text_input = Input(shape=[300])\n\n    text_out = Dense(128, activation='relu')(text_input)\n    text_out = BatchNormalization()(text_out)\n    text_out = Dropout(rate=0.66)(text_out)\n\n    # Categorical feats\n    cat_inputs = []\n    cat_outs = []\n    for cat_feat in cat_feats:\n        cat_input = Input(shape=[1])\n\n        cat_out = Embedding(input_dim=cat_feats_sizes[cat_feat],\n                            output_dim=embedding_sizes[cat_feat],\n                            input_length=1)(cat_input)\n        cat_out = Reshape(target_shape=[embedding_sizes[cat_feat]])(cat_out)\n        cat_out = Dense(embedding_sizes[cat_feat], activation='relu')(cat_out)\n        cat_out = BatchNormalization()(cat_out)\n        cat_out = Dropout(rate=0.66)(cat_out)\n\n        cat_inputs.append(cat_input)\n        cat_outs.append(cat_out)\n\n    feat_inputs = []\n    feat_outs = []\n    for onehot_feat in onehot_feats:\n        feat_input = Input(shape=[onehot_sizes[onehot_feat]])\n\n        feat_out = Dense(8, activation='relu')(feat_input)\n        feat_out = BatchNormalization()(feat_out)\n        feat_out = Dropout(rate=0.66)(feat_out)\n\n        feat_inputs.append(feat_input)\n        feat_outs.append(feat_out)\n\n    cat_outs += feat_outs\n    cats_out = Concatenate()(cat_outs)\n    cats_out = Dense(64, activation='relu')(cats_out)\n    cats_out = BatchNormalization()(cats_out)\n    cats_out = Dropout(rate=0.66)(cats_out)\n\n    # Concatenate dense outputs from different features\n    out = Concatenate()([num_out, img_out, text_out, cats_out])\n    out = Dense(192, activation='relu')(out)\n    out = BatchNormalization()(out)\n    out = Dropout(rate=0.66)(out)\n    out = Dense(64, activation='relu')(out)\n    out = BatchNormalization()(out)\n    out = Dropout(rate=0.66)(out)\n    out = Dense(1)(out)\n\n    inputs = [num_input] + [img_input] + [text_input] + cat_inputs + feat_inputs\n    outputs = [out]\n    model = Model(inputs=inputs,\n                  outputs=outputs)\n\n    return model","155e6b94":"# Transforming data\ntrain_num_data = X_train[numerical_feats].values\ntrain_img_data = X_train[img_feats].values\ntrain_text_data = X_train[text_feats].values\ntrain_cat_data = X_train[cat_feats].values.T\n\ntrain_onh_data = []\n# Mapping onehot categories to vectors\nfor onh in onehot_feats:\n    vals = X_train[onh].values\n    vals = to_categorical(np.clip(vals - 1, 0, np.inf).astype(np.uint8), num_classes=onehot_sizes[onh])\n    train_onh_data.append(vals)\n\n# Mapping cat_feats using cat_feats_mappings\nfor i, cat_feat in enumerate(cat_feats):\n    train_feats = train_cat_data[i]\n    for j in range(len(train_feats)):\n        train_feats[j] = cat_feats_mappings[cat_feat][train_feats[j]]","c3b68a22":"# Reshape to list of 15 arrays\ntrain_data = \\\n[train_num_data] + \\\n[train_img_data] + \\\n[train_text_data] + \\\n[d for d in train_cat_data] + \\\ntrain_onh_data","346322d3":"# Transforming data\ntest_num_data = X_test[numerical_feats].values\ntest_img_data = X_test[img_feats].values\ntest_text_data = X_test[text_feats].values\ntest_cat_data = X_test[cat_feats].values.T\n\n# categories range should start from 0\n\ntest_onh_data = []\n# Mapping onehot categories to vectors\nfor onh in onehot_feats:\n    vals = X_test[onh].values\n    vals = to_categorical(np.clip(vals - 1, 0, np.inf).astype(np.uint8), num_classes=onehot_sizes[onh])\n    test_onh_data.append(vals)\n\n# Mapping cat_feats using cat_feats_mappings\nfor i, cat_feat in enumerate(cat_feats):\n    test_feats = test_cat_data[i]\n    for j in range(len(test_feats)):\n        test_feats[j] = cat_feats_mappings[cat_feat][test_feats[j]]","d13879b0":"# Reshape to list of 15 arrays\ntest_data = \\\n[test_num_data] + \\\n[test_img_data] + \\\n[test_text_data] + \\\n[d for d in test_cat_data] + \\\ntest_onh_data","e7c43176":"def rmse_loss(y_true, y_pred):\n    diff = y_true - y_pred\n    return K.sqrt(K.mean(K.square(diff)))\n\ndef map_to_int(y_true, y_pred, preds):\n    thresholds = get_thresholds_from_dist(y_true, y_pred)\n    return allocate_to_rate(preds, thresholds)","7035585d":"# CV\nseed_everything()\n\nn_splits = 5\nearly_stopping_steps = 5\nepochs = 100\n\nX_train = train_data\nX_test = test_data\n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                               min_delta=1e-4,\n                               patience=early_stopping_steps,\n                               restore_best_weights=True)\ncallbacks = [early_stopping]\n\ngr_kfold_split = GroupKFold(n_splits=n_splits).split([0] * len(train_df),\n                                              y_train,\n                                              groups=train_df['RescuerID'])\n\noof_train = np.zeros(shape=[len(train_df)])\noof_test = np.zeros(shape=[len(test_df), n_splits])\n\nqwks = []\nrmses = []\n\nembeddingses = {}\nfor cat_feat in cat_feats:\n    embeddingses[cat_feat] = []\n    \nfor i, (train_inds, test_inds) in enumerate(gr_kfold_split):\n    print('---- Fold {} ----'.format(i))\n\n    X_tr = []\n    X_val = []\n    for X_inp in X_train:\n        X_tr.append(X_inp[train_inds])\n        X_val.append(X_inp[test_inds])\n        \n    y_tr = np.array(y_train)[train_inds]\n    y_val = np.array(y_train)[test_inds]\n          \n    model = get_model()\n    model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n                  loss='mse')\n    \n    model.fit(X_tr, y_tr,\n              batch_size=32,\n              validation_data=(X_val, y_val),\n              epochs=epochs,\n              callbacks=callbacks)\n    \n    embedding_layers = [l for l in model.layers if isinstance(l, Embedding)]\n    for cat_feat, emb_layer in zip(cat_feats, embedding_layers):\n        embeddingses[cat_feat].append(emb_layer.get_weights()[0])\n    \n    tr_pred = model.predict(X_tr)\n    val_pred = model.predict(X_val)\n    \n    tr_pred = np.squeeze(tr_pred)\n    val_pred = np.squeeze(val_pred)\n    \n    tr_rates = map_to_int(y_tr, tr_pred, tr_pred)\n    val_rates = map_to_int(y_tr, tr_pred, val_pred)\n    \n    tr_rmse = rmse(y_tr, tr_pred)\n    tr_qwk = qwk(y_tr, tr_rates)\n    \n    print('TR___RMSE: {:7.5F}___QWK: {:7.5F}'.format(tr_rmse, tr_qwk))\n    \n    val_rmse = rmse(y_val, val_pred)\n    val_qwk = qwk(y_val, val_rates)\n    \n    print('VAL___RMSE: {:7.5F}___QWK: {:7.5F}'.format(val_rmse, val_qwk))\n    \n# Out-of-fold predictions\n    oof_train[test_inds] = val_pred\n    oof_test_pred = model.predict(X_test)\n    oof_test_pred = np.squeeze(oof_test_pred)\n    oof_test[:, i] = oof_test_pred\n    \n    qwks.append(val_qwk)\n    rmses.append(val_rmse)\n    \nprint('QWK CV: {} +\/- {}'.format(np.mean(qwks), np.std(qwks)))\nprint('RMSE CV: {} +\/- {}'.format(np.mean(rmses), np.std(rmses)))\n\nfor cat_feat in cat_feats:\n    embeddingses[cat_feat] = np.mean(np.array(embeddingses[cat_feat]), axis=0)","6b6d01e3":"# Final datasets arrangement\n\nX_train = train_merged.drop(columns=['PetID', 'AdoptionSpeed'])\ny_train = train_merged['AdoptionSpeed']\n\nX_test = test_merged.drop(columns=['PetID'])\n\n# The most frequent breed of each rescuer as feature.\nX_train['rescuer_breed_mode'] = X_train['RescuerID'].map(X_train.groupby('RescuerID')['Breed1'].agg(\n    lambda x:x.value_counts().index[0]))\nX_test['rescuer_breed_mode'] = X_test['RescuerID'].map(X_test.groupby('RescuerID')['Breed1'].agg(\n    lambda x:x.value_counts().index[0]))\n\ncat_feats = ['Type', 'Vaccinated',\n             'Dewormed', 'Sterilized'] \n\nX_train = X_train.drop(columns=['Name', 'RescuerID'])\nX_test = X_test.drop(columns=['Name', 'RescuerID'])","9111b65c":"# Extracting the embeddings and inserting them into the dataframes\n\nbreeds = pd.concat([X_train['Breed1'], X_test['Breed1']]).unique()\nembed_list = []\nfor breed in breeds:\n    embed_list.append(embeddingses['Breed1'][cat_feats_mappings['Breed1'][breed]])\nembed_list = np.array(embed_list)\nbreed1_embed_df = pd.DataFrame(embed_list).add_prefix('breed1_embed_')\nbreed1_embed_df['Breed1'] = breeds\nX_train = X_train.merge(breed1_embed_df, how='left', on='Breed1')\nX_test = X_test.merge(breed1_embed_df, how='left', on='Breed1')\n\nbreeds = pd.concat([X_train['Breed2'], X_test['Breed2']]).unique()\nembed_list = []\nfor breed in breeds:\n    embed_list.append(embeddingses['Breed2'][cat_feats_mappings['Breed2'][breed]])\nembed_list = np.array(embed_list)\nbreed1_embed_df = pd.DataFrame(embed_list).add_prefix('breed2_embed_')\nbreed1_embed_df['Breed2'] = breeds\nX_train = X_train.merge(breed1_embed_df, how='left', on='Breed2')\nX_test = X_test.merge(breed1_embed_df, how='left', on='Breed2')\n\nbreeds = pd.concat([X_train['rescuer_breed_mode'], X_test['rescuer_breed_mode']]).unique()\nembed_list = []\nfor breed in breeds:\n    embed_list.append(embeddingses['rescuer_breed_mode'][cat_feats_mappings['rescuer_breed_mode'][breed]])\nembed_list = np.array(embed_list)\nbreed1_embed_df = pd.DataFrame(embed_list).add_prefix('rescuer_breed_mode_embed_')\nbreed1_embed_df['rescuer_breed_mode'] = breeds\nX_train = X_train.merge(breed1_embed_df, how='left', on='rescuer_breed_mode')\nX_test = X_test.merge(breed1_embed_df, how='left', on='rescuer_breed_mode')\n\nbreeds = pd.concat([X_train['State'], X_test['State']]).unique()\nembed_list = []\nfor breed in breeds:\n    embed_list.append(embeddingses['State'][cat_feats_mappings['State'][breed]])\nembed_list = np.array(embed_list)\nbreed1_embed_df = pd.DataFrame(embed_list).add_prefix('state_embed_')\nbreed1_embed_df['State'] = breeds\nX_train = X_train.merge(breed1_embed_df, how='left', on='State')\nX_test = X_test.merge(breed1_embed_df, how='left', on='State')\n\nX_train = X_train.drop(columns=['Breed1', 'Breed2', 'State', 'rescuer_breed_mode'])\nX_test = X_test.drop(columns=['Breed1', 'Breed2', 'State', 'rescuer_breed_mode'])","762923c4":"# Using LightGBM regression\n\nparams = {'objective': 'mse',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 10,\n          'max_depth': 5,\n          'min_data_in_leaf': 60,\n          'learning_rate': 0.01,\n          'bagging_fraction': 0.5,\n          'bagging_freq': 1,\n          'feature_fraction': 0.3,\n          'feature_fraction_seed': 73,\n          'lambda_l1': 0,\n          'lambda_l2': 0.3,\n          'verbosity': -1,\n          'seed': seed}\n\nseed_everything()\n\ndef cross_validation(X_train, y_train,\n                     params, \n                     n_splits=5,\n                     early_stopping_rounds=500,\n                     verbose_eval=100,\n                     num_boost_round=10000,\n                     seed=seed):\n\n    gr_kfold_split = GroupKFold(n_splits=5).split(X_train, y_train,\n                                                  groups=train_df['RescuerID'])\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n    qwks, rmses = [], []\n    importances = []\n    i = 0\n    for train_index, valid_index in gr_kfold_split:\n        X_tr = X_train.iloc[train_index, :]\n        y_tr = y_train[train_index].values\n        X_val = X_train.iloc[valid_index, :]\n        y_val = y_train[valid_index].values\n        \n        # Text features \n        X_tr_text = text_feature_extractor.fit_transform(X_tr[text_columns]).set_index(X_tr.index)\n        X_val_text = text_feature_extractor.transform(X_val[text_columns]).set_index(X_val.index)       \n        X_tr = pd.concat([X_tr.drop(columns=text_columns),\n                          X_tr_text], axis=1)\n        X_val = pd.concat([X_val.drop(columns=text_columns),\n                          X_val_text], axis=1) \n        \n        # LGB datasets\n        d_train = lgb.Dataset(X_tr, label=y_tr)\n        d_valid = lgb.Dataset(X_val, label=y_val)\n        valid_sets = [d_train, d_valid]\n\n        # Training\n        print('Fold {}\/{}'.format(i + 1, n_splits))\n        model = lgb.train(params,\n                          train_set=d_train,\n                          num_boost_round=num_boost_round,\n                          valid_sets=valid_sets,\n                          verbose_eval=verbose_eval,\n                          early_stopping_rounds=early_stopping_rounds,\n                          categorical_feature=cat_feats)\n\n        # Predictions\n        tr_pred = model.predict(X_tr)\n        val_pred = model.predict(X_val)\n       \n        # Rounding\n        thresholds = get_thresholds_from_dist(y_tr, tr_pred)\n        val_pred_rounded = allocate_to_rate(val_pred, thresholds)\n        \n        # Evaluation\n        qwk_val = qwk(y_val, val_pred_rounded)\n        rmse_val = rmse(y_val, val_pred)\n        qwks.append(qwk_val)\n        rmses.append(rmse_val)\n\n        # Out-of-fold predictions\n        oof_train[valid_index] = val_pred\n        \n        # Test predictions\n        X_test_text = text_feature_extractor.transform(X_test[text_columns]).set_index(X_test.index)\n        X_test_val = pd.concat([X_test.drop(columns=text_columns),\n                                X_test_text], axis=1)\n        test_pred = model.predict(X_test_val)\n        oof_test[:, i] = test_pred\n            \n        importance = model.feature_importance('gain') \n        importances.append(pd.Series(dict(zip(X_tr.columns, importance))))\n\n        i += 1\n\n        print('QWK: {}, RMSE: {}\\n'.format(qwk_val, rmse_val))\n    \n    return qwks, rmses, oof_train, oof_test, importances\n\nqwks, rmses, oof_train, oof_test, importances = cross_validation(X_train, y_train, params)\n\nprint('QWK CV: {} +\/- {}'.format(np.mean(qwks), np.std(qwks)))\nprint('RMSE CV: {} +\/- {}'.format(np.mean(rmses), np.std(rmses)))","7f77e5a3":"def submit(oof_train, oof_test):\n    \"\"\"Generates submission from test OOF predictions.\"\"\"\n    preds = oof_test.mean(axis=1)\n    \n    thresholds = get_thresholds_from_dist(y_train, preds)\n    preds = allocate_to_rate(preds, thresholds)\n\n    preds = preds.astype(np.int32)\n    submission = pd.DataFrame({'PetID': test_df['PetID'].values, 'AdoptionSpeed': preds})\n    submission.to_csv('submission.csv', index=False)\n    \n    return preds\n    \npreds = submit(oof_train, oof_test)","eda8bd0f":"<a id=\"1\"><\/a> \n## 1. Preparations","d70d302c":" ### 3.2 Image Features","29dd852d":"### 3.1. Text Features","55dfc1cb":"![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Petfinder\/PetFinder%20-%20Logo.png)\n![](https:\/\/s3.amazonaws.com\/cdn-origin-etr.akc.org\/wp-content\/uploads\/2017\/11\/12232719\/Golden-Retriever-On-White-05.jpg)","fd195e43":"#### 4.3.1 Neural Net Embeddings\nWe trained a neural net with embeddings for some categorical features. <br>\nWe then used the embeddings to train the LightGBM model. <br>\nThis part was mainly done by my teammate.","639ef080":"# Kaggle [PetFinder.my Adoption Prediction](https:\/\/www.kaggle.com\/c\/petfinder-adoption-prediction) Competition Solution\n","91da3a28":"#### 4.3.2 LightGBM Training","e95a6248":"### 4.1. Metrics","e8b2f603":"### 4.2. Thresholds Optimization","5b2f315d":"<a id=\"2\"><\/a> \n## 2. Feature Extraction from Sentiment and Image Metadata","b0dee945":"<a id=\"4\"><\/a> \n## 4. Modeling","ca55ded6":"<a id=\"3\"><\/a> \n## 3. Text and Image Features","77796625":" <a id=\"top\"><\/a> <br>\n## Contents\n1. [Preparations](#1)\n2. [Feature Extraction from Sentiment and Image Metadata](#2)\n3. [Text and Image Features](#3)\n4. [Modeling](#4)\n5. [Submission](#5)","f2ebb22d":"This is our solution for PetFinder.my Kaggle competition which [me](https:\/\/www.kaggle.com\/aruchomu) and my teammate [Dmitry Voynov](https:\/\/www.kaggle.com\/vainof) submitted. <br>\nThe solution scored 0.40767 of [Quadratic Weighted Kappa](https:\/\/stats.stackexchange.com\/questions\/59798\/quadratic-weighted-kappa-versus-linear-weighted-kappa?rq=1) (QWK) and reached top 33% on the private leaderboard. <br>\n","23c54b67":"### 4.3. Training","c2034cca":"<a id=\"5\"><\/a> \n## 5. Submission"}}