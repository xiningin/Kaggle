{"cell_type":{"e41b0e2f":"code","5a9ff1de":"code","8d738033":"code","60b8bf41":"code","a7a2db34":"code","f3c93b8c":"code","bda29944":"code","bbc323d3":"code","1bb8b6a5":"code","8cf19cab":"code","77df59ad":"code","992d118b":"code","172a992b":"code","1c35c0bb":"code","d9c84900":"code","163dad4b":"code","ad94a5e6":"code","cbf0c38c":"code","76cdbe8f":"code","44af6cbc":"code","1924c578":"code","2afbc053":"code","b9cfcd90":"code","c41ef041":"code","fbd78a1a":"code","227d2f15":"code","4717dc28":"code","c40fd552":"code","da5eba3d":"code","59434943":"code","36924579":"code","e9add334":"code","66f008cd":"code","919cd49c":"code","6d139644":"code","bc5fcc3c":"code","5eafa3ea":"code","0fa68206":"code","ec0005e6":"code","8de1d1fc":"code","08d7173a":"markdown","ec47480e":"markdown","3a9b7a37":"markdown"},"source":{"e41b0e2f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5a9ff1de":"import numpy as np \nimport pandas as pd \nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib_venn import venn2\nimport category_encoders as ce\n%matplotlib inline\n\n#from xfeat import (SelectCategorical, LabelEncoder, Pipeline, ConcatCombination, SelectNumerical, \n#                   ArithmeticCombinations, TargetEncoder, aggregation, GBDTFeatureSelector, GBDTFeatureExplorer)\n\nfrom catboost import CatBoost\nfrom catboost import CatBoostClassifier\nfrom catboost import Pool\nfrom catboost import cv\nfrom sklearn.metrics import mean_squared_log_error\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom tqdm import tqdm\n\nimport os\nfrom glob import glob\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\nimport shap\n\nfrom optuna.integration import _lightgbm_tuner as lgb_tuner\nimport optuna\nfrom collections import Counter\npd.set_option('display.max_columns', 100)\n\nimport warnings\nwarnings.filterwarnings('ignore')","8d738033":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")","60b8bf41":"train_df.head()","a7a2db34":"test_df.head()","f3c93b8c":"fig = plt.figure(figsize = (16,8))\nfor i in range(0,10):\n    ax = fig.add_subplot(2,5,i+1)\n    sns.countplot(train_df[\"cat\"+str(i)])\n    plt.title(\"train\")\n    plt.tight_layout()","bda29944":"fig = plt.figure(figsize = (16,8))\nfor i in range(0,10):\n    ax = fig.add_subplot(2,5,i+1)\n    sns.countplot(test_df[\"cat\"+str(i)])\n    plt.title(\"test\")\n    plt.tight_layout()","bbc323d3":"fig = plt.figure(figsize = (16,16))\nfor i in range(0,14):\n    ax = fig.add_subplot(4,4,i+1)\n    sns.distplot(train_df[\"cont\"+str(i)], label='train')\n    sns.distplot(test_df[\"cont\"+str(i)], label='test')\n    plt.legend()\n    plt.title(\"cont\"+str(i))\n    plt.tight_layout()","1bb8b6a5":"# https:\/\/www.guruguru.science\/competitions\/13\/discussions\/41b4ac2d-690b-4ba5-8ff7-be3639578bc1\/\n\n# BaseBlock \nclass BaseBlock(object):\n    def fit(self, input_df, y=None):\n        return self.transform(input_df)\n    \n    def transform(self, input_df):\n        raise NotImplementedError()\n\n# OneHotEncoding\nclass OneHotEncodingBlock(BaseBlock):\n    def __init__(self, cols):\n        self.cols = cols\n        self.encoder = None\n        \n    def fit(self, input_df, y=None):\n        self.encoder = ce.OneHotEncoder(use_cat_names=True)\n        self.encoder.fit(input_df[self.cols])\n        return self.transform(input_df[self.cols])\n    \n    def transform(self, input_df):\n        return self.encoder.transform(input_df[self.cols]).add_prefix(\"OHE_\")\n    \n# CountEncoding\nclass CountEncodingBlock(BaseBlock):\n    def __init__(self, cols):\n        self.cols = cols\n        self.encoder = None\n    \n    def fit(self, input_df, y=None):\n        return self.transform(input_df[self.cols])\n\n    def transform(self, input_df):\n        self.encoder = ce.CountEncoder()\n        self.encoder.fit(input_df[self.cols])\n        return self.encoder.transform(input_df[self.cols]).add_prefix(\"CE_\")\n    \n# OrdinalEncoding\nclass OrdinalEncodingBlock(BaseBlock):\n    def __init__(self, cols):\n        self.cols = cols\n        self.encoder = None\n        \n    def fit(self, input_df, y=None):\n        self.encoder = ce.OrdinalEncoder()\n        self.encoder.fit(input_df[self.cols])\n        return self.transform(input_df[self.cols])\n    \n    def transform(self, input_df):\n        return self.encoder.transform(input_df[self.cols]).add_prefix(\"OE_\")","8cf19cab":"def get_ce_features(input_df):\n    _input_df = pd.concat([input_df], axis=1)\n\n    cols = [\n        \"cat0\",\n        \"cat1\",\n        \"cat2\",\n        \"cat3\",\n        \"cat4\",\n        \"cat5\",\n        \"cat6\",\n        \"cat7\",\n        \"cat8\",\n        \"cat9\",\n    ]\n    encoder = CountEncodingBlock(cols=cols)\n    output_df = encoder.fit(_input_df.astype(str))\n    return output_df\n\ndef get_oe_features(input_df):\n    _input_df = pd.concat([input_df])\n    cols = [\n        \"cat0\",\n        \"cat1\",\n        \"cat2\",\n        \"cat3\",\n        \"cat4\",\n        \"cat5\",\n        \"cat6\",\n        \"cat7\",\n        \"cat8\",\n        \"cat9\",\n    ]\n    encoder = OrdinalEncodingBlock(cols=cols)\n    output_df = encoder.fit(input_df)\n    return output_df\n\n\ndef get_ohe_features(input_df):\n    cols = [\n        \"cat0\",\n        \"cat1\",\n        \"cat2\",\n        \"cat3\",\n        \"cat4\",\n        \"cat5\",\n        \"cat6\",\n        \"cat7\",\n        \"cat8\",\n        \"cat9\",\n    ]\n    encoder = OneHotEncodingBlock(cols=cols)\n    output_df = encoder.fit(input_df)\n    return output_df","77df59ad":"def create_continuous_features(input_df):\n    use_columns = [\"cont0\",\"cont1\",\"cont2\",\"cont3\",\"cont4\",\n                   \"cont5\",\"cont6\",\"cont7\",\"cont8\",\"cont9\",\n                   \"cont10\",\"cont11\",\"cont12\",\"cont13\"\n                  ]\n    output_df = input_df[use_columns]\n    return output_df","992d118b":"# propress\ndef to_features(train, test):\n    input_df = pd.concat([train, test]).reset_index(drop=True)\n\n    processes = [\n        get_oe_features,\n        get_ce_features,\n        get_ohe_features,\n        create_continuous_features\n    ]\n\n    output_df = pd.DataFrame()\n    for func in tqdm(processes):\n        _df = func(input_df)\n        assert len(_df) == len(input_df), func.__name__\n        output_df = pd.concat([output_df, _df], axis=1)\n\n    train_x = output_df.iloc[:len(train)] \n    test_x = output_df.iloc[len(train):].reset_index(drop=True)\n    return train_x, test_x","172a992b":"target_data = \"target\" \n\ntrain_x, test_x = to_features(train_df, test_df)\ntrain_ys = train_df[target_data]","1c35c0bb":"train_x.info()","d9c84900":"train_ys","163dad4b":"from contextlib import contextmanager\nfrom time import time\n\n@contextmanager\ndef timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n    if prefix: format_str = str(prefix) + format_str\n    if suffix: format_str = format_str + str(suffix)\n    start = time()\n    yield\n    d = time() - start\n    out_str = format_str.format(d)\n    if logger:\n        logger.info(out_str)\n    else:\n        print(out_str)","ad94a5e6":"def fit_lgbm(X, y, cv, params: dict=None, verbose: int=50):\n    metric_func = mean_squared_error\n\n    if params is None:\n        params = {}\n\n    models = []\n\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        clf = lgb.LGBMRegressor(**params)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            clf.fit(x_train, y_train, \n                    eval_set=[(x_valid, y_valid)],  \n                    early_stopping_rounds=verbose,\n                    verbose=verbose)\n\n        pred_i = clf.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(clf)\n\n        print(f'Fold {i} RMSE: {metric_func(y_valid, pred_i) ** .5:.4f}')\n        \n    score = metric_func(y, oof_pred) ** .5\n    print('FINISHED | Whole RMSE: {:.4f}'.format(score))\n    return oof_pred, models","cbf0c38c":"def fit_xgb(X, y, cv, params: dict=None, verbose: int=50):\n    metric_func = mean_squared_error\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        model_xgb = xgb.XGBRegressor(**params)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            model_xgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)])\n            \n        #print(model_xgb.best_score())\n        \n        pred_i = model_xgb.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(model_xgb)\n\n        print(f'Fold {i} RMSE: {metric_func(y_valid, pred_i) ** .5:.4f}')\n\n    score = metric_func(y, oof_pred) ** .5\n    print('FINISHED | Whole RMSE: {:.4f}'.format(score))\n    return oof_pred, models","76cdbe8f":"def fit_cb(X, y, cv, params: dict=None, verbose: int=50):\n    metric_func = mean_squared_error\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        train_pool = Pool(x_train, label = y_train)\n        valid_pool = Pool(x_valid, label = y_valid)\n        \n        model_cb = CatBoost(params)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            model_cb.fit(train_pool,\n              # valid_data\n              eval_set = valid_pool,\n              use_best_model = True,\n              silent = True,\n              plot = False)\n            \n        print(model_cb.get_best_score())\n        \n        pred_i = model_cb.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(model_cb)\n\n        print(f'Fold {i} RMSE: {metric_func(y_valid, pred_i) ** .5:.4f}')\n\n    score = metric_func(y, oof_pred) ** .5\n    print('FINISHED | Whole RMSE: {:.4f}'.format(score))\n    return oof_pred, models","44af6cbc":"def create_stratified_folds_for_regression(data_df, n_splits=5):\n    \"\"\"\n    @param data_df: training data to split in Stratified K Folds for a continous target value\n    @param n_splits: number of splits\n    @return: the training data with a column with kfold id\n    \"\"\"\n    data_df['kfold'] = -1\n    # randomize the data\n    data_df = data_df.sample(frac=1).reset_index(drop=True)\n    # calculate the optimal number of bins based on log2(data_df.shape[0])\n    num_bins = np.int(np.floor(1 + np.log2(len(data_df))))\n    print(f\"Num bins: {num_bins}\")\n    # bins value will be the equivalent of class value of target feature used by StratifiedKFold to \n    # distribute evenly the classed over each fold\n    data_df.loc[:, \"bins\"] = pd.cut(pd.to_numeric(data_df['target'], downcast=\"signed\"), bins=num_bins, labels=False)\n    kf = StratifiedKFold(n_splits=n_splits)\n    \n    # set the fold id as a new column in the train data\n    for f, (t_, v_) in enumerate(kf.split(X=data_df, y=data_df.bins.values)):\n        data_df.loc[v_, 'kfold'] = f\n    \n    # drop the bins column (no longer needed)\n    data_df = data_df.drop(\"bins\", axis=1)\n    \n    return data_df","1924c578":"def kfold_splits(n_splits, train_df):\n    \"\"\"\n    Returns a collection of (fold, train indexes, validation indexes)\n    @param n_splits: number of splits\n    @param train_df: training data\n    @return: a collection of (fold, train indexes, validation indexes)\n    \"\"\"\n    \n    # not append \"fold\" => my function\n    all_folds = list(range(0, n_splits))\n    kf_splits = []\n    for fold in range(0, n_splits):\n        train_folds = [x for x in all_folds if x != fold]\n        trn_idx = train_df[train_df.kfold!=fold].index\n        val_idx = train_df[train_df.kfold==fold].index\n        kf_splits.append((trn_idx, val_idx))\n    return kf_splits","2afbc053":"train_df_re = pd.concat([train_x, train_ys], axis=1)","b9cfcd90":"n_splits = 9\ntrain_df_re = create_stratified_folds_for_regression(train_df_re, n_splits)","c41ef041":"stratified_cv = kfold_splits(n_splits, train_df_re)","fbd78a1a":"def fit_lgbm_param_optuna(X, \n             y, \n             cv, \n             params: dict=None, \n             verbose: int=50):\n    metric_func = mean_squared_error\n\n    if params is None:\n        params = {}\n\n    models = []\n\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n\n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n\n        clf = lgb.LGBMRegressor(**params)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            clf.fit(x_train, y_train, \n                    eval_set=[(x_valid, y_valid)],  \n                    early_stopping_rounds=verbose,\n                    verbose=verbose)\n\n        pred_i = clf.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(clf)\n\n    score = metric_func(y, oof_pred) ** .5\n    return score\n\ndef objective(trial):\n    \n    #fold = KFold(n_splits=5, shuffle=True, random_state=71)\n    #cv = list(fold.split(train_x, train_ys))\n    optuna_paramas_lgb = {\n        'num_leaves': trial.suggest_int('num_leaves', 32, 512),\n        'boosting_type': 'gbdt',\n        'max_bin': trial.suggest_int('max_bin', 700, 900),\n        'objective': 'huber',\n        'metric': 'mae',\n        'learning_rate': trial.suggest_float('learning_rate',0.0155,0.05),\n        'random_state' : 71,\n        'max_depth': trial.suggest_int('max_depth', 4, 16),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 16),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 8),\n        'min_child_samples': trial.suggest_int('min_child_samples', 4, 80),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 1.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 1.0),\n        'early_stopping_rounds': 10\n            \n}\n    score = fit_lgbm_param_optuna(train_x.values,  train_ys, stratified_cv, params=optuna_paramas_lgb)\n    \n    return score\n\n#study = optuna.create_study(direction=\"minimize\", study_name='lgbm_train')\n#study.optimize(objective, n_trials=50)","227d2f15":"#study.best_params\n\n\"\"\"\nlgb\n{'num_leaves': 385,\n 'max_bin': 887,\n 'learning_rate': 0.049867328104748844,\n 'max_depth': 14,\n 'min_child_weight': 10,\n 'feature_fraction': 0.4511004151880547,\n 'bagging_fraction': 0.6559039807249963,\n 'bagging_freq': 2,\n 'min_child_samples': 77,\n 'lambda_l1': 4.638151021025029e-08,\n 'lambda_l2': 0.2937304195136803}\"\"\"","4717dc28":"def fit_xgb_optuna(X, y, cv, params: dict=None, verbose: int=50):\n    metric_func = mean_squared_error\n    if params is None:\n        params = {}\n\n    models = []\n    oof_pred = np.zeros_like(y, dtype=np.float)\n\n    for i, (idx_train, idx_valid) in enumerate(cv): \n        x_train, y_train = X[idx_train], y[idx_train]\n        x_valid, y_valid = X[idx_valid], y[idx_valid]\n        \n        model_xgb = xgb.XGBRegressor(**params)\n\n        with timer(prefix='fit fold={} '.format(i + 1)):\n            model_xgb.fit(x_train, y_train, eval_set=[(x_valid, y_valid)])\n            \n        #print(model_xgb.best_score())\n        \n        pred_i = model_xgb.predict(x_valid)\n\n        oof_pred[idx_valid] = pred_i\n        models.append(model_xgb)\n\n    score = metric_func(y, oof_pred) ** .5\n\n    return score\n\ndef objective_xgb(trial):\n    \n    fold = KFold(n_splits=5, shuffle=True, random_state=71)\n    cv = list(fold.split(train_x, train_ys))\n    optuna_paramas_xgb = {\n        'booster': 'gbtree',\n        'max_bin': trial.suggest_int('max_bin', 700, 900),\n        'objective': 'reg:squarederror',\n        'eval_metric': 'mae',\n        'learning_rate': trial.suggest_float('learning_rate',0.0155,0.05),\n        'random_state' : 71,\n        'max_depth': trial.suggest_int('max_depth', 4, 16),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 16),\n        'subsample': trial.suggest_uniform('subsample', 0.4, 1.0),\n        'lambda': trial.suggest_loguniform('lambda', 1e-8, 1.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-8, 1.0),\n        'early_stopping_rounds': 10\n    }\n    \n    score = fit_xgb_optuna(train_x.values,  train_ys, stratified_cv, params=optuna_paramas_xgb)\n    \n    return score\n\n#study = optuna.create_study(direction=\"minimize\", study_name='xgb_train')\n#study.optimize(objective_xgb, n_trials=10)","c40fd552":"#study.best_params\n\"\"\"\n{'max_bin': 830,\n 'learning_rate': 0.048518442248912635,\n 'max_depth': 15,\n 'min_child_weight': 7,\n 'subsample': 0.9080463485454009,\n 'lambda': 5.370896698434827e-07,\n 'alpha': 0.005799175899438967}\"\"\"","da5eba3d":"params_best = {\n    'num_leaves': 385,\n    'max_bin': 887,\n    'learning_rate': 0.049867328104748844,\n    'max_depth': 14,\n    'min_child_weight': 10,\n    'feature_fraction': 0.4511004151880547,\n    'bagging_fraction': 0.6559039807249963,\n    'bagging_freq': 2,\n    'min_child_samples': 77,\n    'lambda_l1': 4.638151021025029e-08,\n    'lambda_l2': 0.2937304195136803,\n    \"random_state\": 71,\n    \"num_boost_round\": 50000,\n    \"early_stopping_rounds\": 100,\n    'objective': 'regression',\n    'metric': 'rmse',\n    \"boosting\": \"gbdt\",\n}\n\n#fold = KFold(n_splits=5, shuffle=True, random_state=71)\n#cv = list(fold.split(train_x, train_ys))\n\noof, models = fit_lgbm(train_x.values, train_ys, stratified_cv, params=params_best)","59434943":"import xgboost as xgb\nparams_xgb = {\n        'max_bin': 830,\n 'learning_rate': 0.048518442248912635,\n 'max_depth': 15,\n 'min_child_weight': 7,\n 'subsample': 0.9080463485454009,\n 'lambda': 5.370896698434827e-07,\n 'alpha': 0.005799175899438967\n}\n\nfold = KFold(n_splits=5, shuffle=True, random_state=71)\ncv = list(fold.split(train_x, train_ys))\n\noof_xgb, models_xgb = fit_xgb(train_x.values, train_ys, stratified_cv, params=params_xgb)","36924579":"params_cb = {\n    'loss_function': 'RMSE',\n    'max_depth': 3, \n    'learning_rate': 0.08, \n    'subsample': 0.8, \n    #'colsample_bytree': 0.7,\n    'num_boost_round': 1000,\n    'early_stopping_rounds': 100,\n}\n\noof_cb, models_cb = fit_cb(train_x.values, train_ys, stratified_cv, params=params_cb)","e9add334":"def visualize_importance(models, feat_train_df):\n\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importances_\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n\n    fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n    sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax, palette='viridis')\n    ax.tick_params(axis='x', rotation=90)\n    ax.grid()\n    fig.tight_layout()\n    return fig, ax","66f008cd":"fig, ax = visualize_importance(models, train_x)","919cd49c":"pred_lgb = np.array([model.predict(test_x.values) for model in models])\npred_lgb = np.mean(pred_lgb, axis=0)\npred_lgb = np.where(pred_lgb < 0, 0, pred_lgb)","6d139644":"pred_xgb = np.array([model.predict(test_x.values) for model in models_xgb])\npred_xgb = np.mean(pred_xgb, axis=0)\npred_xgb = np.where(pred_xgb < 0, 0, pred_xgb)","bc5fcc3c":"pred_cb = np.array([model.predict(test_x.values) for model in models_cb])\npred_cb = np.mean(pred_cb, axis=0)\npred_cb = np.where(pred_cb < 0, 0, pred_cb)","5eafa3ea":"#oof_em = (oof+oof_xgb+oof_cb)\/3\noof_em = oof*0.3+oof_xgb*0.1+oof_cb*0.6\n\nmetric_func = mean_squared_error\n\nscore = metric_func(train_ys, oof_em) ** .5\n\nprint(score)\n#0.8439838083404265","0fa68206":"#pred = (pred_lgb + pred_xgb + pred_cb)\/3\npred_em = pred_lgb*0.3 + pred_xgb*0.1 +pred_cb*0.6","ec0005e6":"submission[\"target\"] = pred_em\nsubmission.to_csv('.\/submission.csv', index=False)","8de1d1fc":"fig, ax = plt.subplots(figsize=(8, 8))\nsns.distplot(oof, label='Test Predict')\nsns.distplot(submission[\"target\"], label='Out Of Fold')\nax.legend()\nax.grid()","08d7173a":"# optuna","ec47480e":"# preprocess","3a9b7a37":"# Stratified_folds_for_regression\n\nthanks for good information!\nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-feb-2021\/discussion\/216576"}}