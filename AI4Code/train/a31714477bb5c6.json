{"cell_type":{"34eb060a":"code","d8555e31":"code","92a80034":"code","391a3a8e":"code","954cc7b0":"code","09b2f03a":"code","e1ea89a6":"code","99dbc6e7":"code","e4d34009":"code","b628da37":"code","b9818d5e":"code","ee5f0624":"code","816ace12":"code","2e43bb77":"code","455eb0fb":"code","8f3a00e8":"code","4d257a6a":"code","b9418699":"code","d7d9d465":"code","4d931357":"code","4b3788d1":"code","3f3bbfb8":"code","b4fb1d43":"code","89b19d2d":"code","db65135c":"code","c5342a62":"code","b0f416b0":"code","04361c57":"code","d50f8b2a":"code","1d09dc98":"code","5846cfad":"code","02cbff77":"code","8880cc17":"code","25638b68":"code","00bed02b":"code","5dd15433":"code","3c55ee09":"code","9b5fc1b8":"code","b3b3de45":"code","f876a8ea":"code","6a7c1679":"code","c4c7204c":"code","5c5b3a2c":"code","bef318be":"code","6c550585":"code","ff9b0a35":"code","9825699f":"code","0fb3fd4b":"code","94e8a345":"code","b4c93bdc":"code","99e34317":"code","91a82214":"code","f7c66350":"code","3afab17b":"code","082dddcd":"code","bda60e48":"code","4e844f3d":"markdown","99bf412d":"markdown","4f31812b":"markdown","98d1592e":"markdown","ca140d63":"markdown","5aeb0bf6":"markdown","6c404975":"markdown","703ecc93":"markdown","38600917":"markdown","a7a06d82":"markdown","4e7ef18e":"markdown","403ac41a":"markdown","eced1a1d":"markdown","a6f1e165":"markdown","ea811a9b":"markdown","c673a4a3":"markdown","d2f50b5b":"markdown","f7da8af0":"markdown","ff08564a":"markdown","13699532":"markdown","f633d224":"markdown","f0a2c898":"markdown","f6738836":"markdown","c6594dcf":"markdown","bf2446d6":"markdown","79e12e44":"markdown","22dbe7ab":"markdown","d85b117a":"markdown","10e5c316":"markdown","0a0e0cbb":"markdown","86ab27f8":"markdown","7c0ab878":"markdown","2cb1fb2a":"markdown","33f93613":"markdown"},"source":{"34eb060a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8555e31":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom PIL import Image\nfrom wordcloud import WordCloud,STOPWORDS\n\nimport plotly.offline as pyo\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nimport spacy\nfrom spacy import displacy\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords, wordnet\n\nfrom collections import Counter\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import MultinomialNB\n","92a80034":"root_path = \"..\/input\/nlp-getting-started\"\ntrain_path = os.path.join(root_path,\"train.csv\")\ntrain = pd.read_csv(train_path)\ntrain","391a3a8e":"test_path = os.path.join(root_path,\"test.csv\")\ntest = pd.read_csv(test_path)\ntest.shape","954cc7b0":"train_location_uniques = [name for name in train.location.value_counts().index.sort_values()]\ntrain_location_uniques[0:25] # location content first 25 rows","09b2f03a":"def draw_word_cloud(image_path, fig_title, words, fig_size, max_words):\n    stop_words = set(STOPWORDS)\n    mask_img = np.array(Image.open(image_path))\n    wordcloud_masked = WordCloud(background_color='white', max_words=max_words, stopwords=stop_words,min_font_size=10,\n                                 contour_width=5, contour_color='gray',mask=mask_img, width=mask_img.shape[1],\n                                height=mask_img.shape[0]).generate(words)\n    #Plotting the WordCloud\n    plt.figure(figsize=fig_size)\n    plt.imshow(wordcloud_masked)\n    plt.title(fig_title, fontsize=30)\n    plt.axis(\"off\")\n    plt.show() \n    return None","e1ea89a6":"location_path ='..\/input\/locationpoint\/location_pin.png'\ndraw_word_cloud(location_path, 'Location content', str(train_location_uniques), (35,12), 500)","99dbc6e7":"def plot_donut(labels, values, fig_height, fig_title, legend_title):\n\n    fig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.6)])\n    fig.update_layout(\n        height=fig_height,\n        title=fig_title,\n        legend_title=legend_title,\n        font=dict(\n            size=18,\n            color=\"purple\"\n        )\n    )\n    fig.show();\n\n","e4d34009":"keyword_data = train.groupby('keyword').size().sort_values().tail(15)\/train['keyword'].count()*100\nlabels = keyword_data.index\nvalues = keyword_data.values\nplot_donut(labels, values, 600, 'Most appeared keywords', 'keyword' )","b628da37":"# function to create bar plot and display the values on the bars  \ndef plot_bar(labels, values, fig_title, x_axis_name, fig_size):\n    plt.figure(figsize=fig_size)\n    plt.xticks(rotation=0)\n    ax = sns.barplot(x=labels, y=values)\n    ax.tick_params(labelsize=12)\n    ax.set_title(fig_title, fontsize=20)\n    ax.set_xlabel(x_axis_name, fontsize=12)\n    for p in ax.patches:\n      ax.annotate(\"%.2f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n          ha='center', va='center', fontsize=12, color='purple', xytext=(0, 4),\n          textcoords='offset points')\n    ","b9818d5e":"plot_bar(['Not Real Disaster', 'Real Disaster'], train.target.value_counts().values, 'Train Dataset w.r.to Target feature', 'Type of Tweet' , [6,5])","ee5f0624":"def spacy_ner(content):\n    spacy_tok = spacy.load('en_core_web_sm')\n    parsed_content = spacy_tok(str(content))\n    spacy.displacy.render(parsed_content, style='ent', jupyter=True)\n    return None","816ace12":"# explore first 30 real disaster tweets\nspacy_ner(train[train.target == 1]['text'][:30])","2e43bb77":"# explore first 30 not-real type disaster tweets\nspacy_ner(train[train.target == 0]['text'][:30])","455eb0fb":"missing_data_sum = train.isnull().sum()\nmissing_data = pd.DataFrame({'total_missing_values': missing_data_sum,'percentage_of_missing_values': (missing_data_sum\/train.shape[0])*100})\nplt.figure(figsize=(12, 5))\nplt.title(\"Missing values percentages\", fontsize=20)\nchart = sns.barplot(x =missing_data['percentage_of_missing_values'] , y = train.columns, orientation='horizontal')","8f3a00e8":"train[\"text_length\"]= train.text.apply(lambda x : len(x))","4d257a6a":"def hist_plotly(target_type):\n    fig_title = 'Frequency of Text Length for Not-Real Disaster Tweets'\n    xaxis_name = 'Not-Real Disaster Tweets Text Length'\n    if target_type == 1:\n        fig_title = 'Frequency of Text Length for Real Disaster Tweets'\n        xaxis_name = 'Real Disaster Tweets Text Length'\n    plt2 = go.Histogram(x = train[train.target == target_type]['text_length'])\n    lyt2 = go.Layout(title=fig_title, xaxis=dict(title=xaxis_name, range=[0,180]), yaxis=dict(title='Frequency'))\n    fig2 = go.Figure(data=[plt2], layout=lyt2)\n    iplot(fig2)\n    return None\nhist_plotly(1)\nhist_plotly(0)","b9418699":"upper_text = []\npunc_text = []\nswd_text = []\n\ndef iterate_dataset():\n    for row in train.text:\n        for word in row.split():\n            if word.isupper():\n                upper_text.append(word)\n            if word in list(string.punctuation):\n                punc_text.append(word)\n            if word in set(stopwords.words('english')):\n                swd_text.append(word)\n    return None\niterate_dataset() ","d7d9d465":"# Identifying the uppercase words exists in the tweets  \nprint('Length of uppercase words : {}'.format(len(upper_text)))\n\n#removing duplicates\nupper_text_result = [] \n[upper_text_result.append(x) for x in upper_text if x not in upper_text_result] \nprint('Length of uppercase words after removal of duplicate entries : {}'.format(len(upper_text_result)))","4d931357":"# word cloud to see upper case words\n_path ='..\/input\/tweetimage\/comment_icon.png'\ndraw_word_cloud(_path, 'Uppercase content', str(upper_text_result),(30, 8), max_words=300 )","4b3788d1":"# Converting to lowercase\ntrain['text'] = train.text.str.lower()\ntest['text'] = test.text.str.lower()","3f3bbfb8":"string.punctuation","b4fb1d43":"# Identifying the Punctuations exists in the tweets\n\nprint('Number of  Punctuations : {}'.format(len(punc_text)))\n# Use nltk to get the frequency\npunc_frequency = nltk.FreqDist(punc_text)\npunc_frequency","89b19d2d":"# Get lebels and values as seperate lists \npunc_fixed_list = [x for x in punc_frequency.items()]\nlabels,values = zip(*punc_fixed_list)\nplot_donut(labels, values, 650, 'Percentage of Punctuations', 'punctuation' )","db65135c":"def remove_punctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))\n\n# removing punctuations\ntrain['text'] = train['text'].apply(lambda x: remove_punctuations(x))\ntest['text'] = test['text'].apply(lambda x: remove_punctuations(x))\n\ntrain.head()","c5342a62":"english_vocab = set(w.lower() for w in nltk.corpus.words.words()) #vocab taken from nltk corpus\nour_text_vocab = set(w.lower() for w in train.text if w.lower().isalpha())\nunknown_content = our_text_vocab.difference(english_vocab) \nunknown_content","b0f416b0":"swd_frequency = nltk.FreqDist(swd_text)\n# Get lebels and values as seperate lists \nswd_fixed_list = [x for x in swd_frequency.items()]\nlabels,values = zip(*swd_fixed_list)\nplot_donut(labels, values, 650, 'Percentage of Stop Words', 'stopword' )","04361c57":"def remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in set(stopwords.words('english'))])\n\n# removing stop words\ntrain['text'] = train['text'].apply(lambda x: remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x: remove_stopwords(x))\n\ntrain.head()\n","d50f8b2a":"# Lets identify most common\/frequent words after removal of stop words\ncommon_cnt = Counter()\n\nfor row in train.text:\n    for word in row.split():\n        common_cnt[word] += 1\n        ","1d09dc98":"most_common_wd_frequency = nltk.FreqDist(common_cnt.most_common(10))\n# Get lebels and values as seperate lists \nmcwds_fixed_list = [x for x in most_common_wd_frequency.items()]\nlabels,values = zip(*mcwds_fixed_list)\nplot_donut(labels, values, 650, 'Percentage of Top 10 Common Words after removal of stopwords', 'common word' )","5846cfad":"freqt_words = set([word for (word, count) in common_cnt.most_common(10)])\n\ndef remove_freqtwords(text):\n    return \" \".join([word for word in str(text).split() if word not in freqt_words])\n\n# removing common\/frequent words\ntrain['text'] = train['text'].apply(lambda x: remove_freqtwords(x))\ntest['text'] = test['text'].apply(lambda x: remove_freqtwords(x))\n\ntrain.head()\n","02cbff77":"num_rare_words = 50\n\nrare_words = set([word for (word, counter) in common_cnt.most_common()[:-num_rare_words-1:-1]])\n","8880cc17":"# word cloud to see upper case words\n_path ='..\/input\/tweetimage\/comment_icon.png'\ndraw_word_cloud(_path, 'Rare words content', str(rare_words),(30, 8), max_words=50 )","25638b68":"def remove_rarewords(text):\n    return \" \".join([word for word in str(text).split() if word not in rare_words])\n\n# removing rare words\ntrain['text'] = train['text'].apply(lambda x: remove_freqtwords(x))\ntest['text'] = test['text'].apply(lambda x: remove_freqtwords(x))\n\ntrain.head()","00bed02b":"stemmer = PorterStemmer()\n\ndef word_stemmer(text):\n    return \" \".join([stemmer.stem(word) for word in text.split()])\n\ntrain['text'] = train['text'].apply(lambda x: word_stemmer(x))\ntest['text'] = test['text'].apply(lambda x: word_stemmer(x))\n\ntrain.head()\n","5dd15433":"lemmatizer = WordNetLemmatizer()\n\ndef word_lemmatizer(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n\ntrain['text'] = train['text'].apply(lambda x: word_lemmatizer(x))\ntest['text'] = test['text'].apply(lambda x: word_lemmatizer(x))\n\ntrain.head()\n\n","3c55ee09":"lemmatizer = WordNetLemmatizer()\n\nwordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n\ndef word_lemmatizer(text):\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n\ntrain['text'] = train['text'].apply(lambda x: word_lemmatizer(x))\ntest['text'] = test['text'].apply(lambda x: word_lemmatizer(x))\n\ntrain.head()","9b5fc1b8":"def remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ntrain['text'] = train['text'].apply(lambda x: remove_urls(x))\ntest['text'] = test['text'].apply(lambda x: remove_urls(x))\n\ntrain.head()","b3b3de45":"def remove_html_tags(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)\n\ntrain['text'] = train['text'].apply(lambda x: remove_html_tags(x))\ntest['text'] = test['text'].apply(lambda x: remove_html_tags(x))\n\ntrain.head()","f876a8ea":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain['text'] = train['text'].apply(lambda x: remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x: remove_emoji(x))\n\ntrain.head()","6a7c1679":"# After cleaning the text, lets check the length\n\ntrain['cleaned_text_length'] = train.text.apply(lambda x : len(x))\n","c4c7204c":"plt.figure(figsize=(25,10))\nplt.title('Visualizing tweets length before and after data cleanup', fontsize=24)\nplt.plot(train.text_length, label='before')\nplt.plot(train.cleaned_text_length, label='after')\nplt.legend();","5c5b3a2c":"# Applying Bag of Words model \n\ncount_vectorizer=CountVectorizer(analyzer='word',binary=True)\ncount_vectorizer.fit(train.text)\n\ntrain_cnt_vec = count_vectorizer.fit_transform(train.text)\ntest_cnt_vec = count_vectorizer.transform(test.text)\n","bef318be":"# Spliting 80:20 ratio\n\nX_train, X_test, y_train, y_test = train_test_split(train_cnt_vec.todense(), train.target, test_size = 0.2, random_state = 42)","6c550585":"# function to get fit the model and save the scores\ntrain_score = []\ntest_score = []\ndef fit_model(model, model_name):\n    _model = model.fit(X_train, y_train)\n    train_score_val = model.score(X_train, y_train)\n    test_score_val = model.score(X_test,y_test)\n    train_score.append(train_score_val)\n    test_score.append(test_score_val)\n    print(f\"{model_name} Train score :\", train_score_val)\n    print(f\"{model_name} Test score is:\",test_score_val)\n    return _model","ff9b0a35":"fit_model(GaussianNB(), 'Gaussian Naive Bayes')","9825699f":"fit_model(DecisionTreeClassifier(criterion=\"entropy\", max_depth = 4), 'Decision Tree Classifier')","0fb3fd4b":"fit_model(LogisticRegression(max_iter=1000000), 'Logistic Regression')","94e8a345":"fit_model(GradientBoostingClassifier(random_state=0), 'Gradient Boosting Classifier')","b4c93bdc":"fit_model(MultinomialNB(), 'Multinomial Naive Bayes')","99e34317":"model_name = ['Gaussian NB', 'Decision Tree', 'Logistic Regression', 'Gradient Boosting Classifier', 'Multinomial NB']\nplot_bar(model_name, train_score, 'Train Score w.r.to Model', 'model name', [14,5])","91a82214":"plot_bar(model_name, test_score, 'Test Score w.r.to Model', 'model name', [14,5])","f7c66350":"# lets apply MNB model on complete train data \nmnb_model = MultinomialNB()\nmnb_model.fit(train_cnt_vec.todense(), train.target)","3afab17b":"# predicting test.csv data\n\npredictions = mnb_model.predict(test_cnt_vec.todense())\npredictions_test_ids=test.id.values.ravel()","082dddcd":"submission_df = pd.DataFrame({\"id\":predictions_test_ids,\"target\":predictions})\nsubmission_df","bda60e48":"submission_df.to_csv('submission.csv', index=False)","4e844f3d":"> Take away : MultinomialNB model is generalised when compared other models","99bf412d":"### Text feature length distribution","4f31812b":"### target feature","98d1592e":"### Covert text to Lower Case","ca140d63":"> Re-apply Lemmatization to preseve the meaning of the word","5aeb0bf6":"# Missing values\n","6c404975":"# Explore Train Data","703ecc93":"# Read\/Load Test Data","38600917":"> This means apart from the above words remaining all are identifying from default nltk corus english vocabulary. \nHence use English language for avoiding stop words","a7a06d82":"![nertype.png](attachment:nertype.png)","4e7ef18e":"> both the distributions are symmetric","403ac41a":"### Removal of Punctuations","eced1a1d":"## Feedback\n* Your feedback is much appreciated\n* Comment if you have any doubts or you found any errors in the notebook\n\n**Happy learning!!**","a6f1e165":"### Word Cloud on location content","ea811a9b":"### NER(Named Entity Recognition) using Spacy","c673a4a3":"### Removal of most common\/frequent words","d2f50b5b":"* location content is the mixture of country names & hash tags with some juck characters.","f7da8af0":"### Removal of stopwords\n* Lets first look into the language used in text feature","ff08564a":"# Submission\n### Disaster Prediction for Test Data and Submission","13699532":"> Let's ignore keyword and location and explore actual text data\n","f633d224":"# Data Pre-Processing\/Cleaning","f0a2c898":"# Read\/Load Train Data","f6738836":"### Remove emojis","c6594dcf":"# Conclusion\n\n* Explored Text data. Visualized data using word clouds\n* Able to identified named entity recognizers using spacy\n* Done data cleaning like conversion of lower case, removal of puntuations, stop words, most used words, rare words, html tags and emogies\n* Tokenized and applied BOW(Bag of Words) techique\n* Verified foundational models like LR, GNB, DT, GBC and MNB\n* Predicted target values for the given test dataset","bf2446d6":"### Apply Stemming","79e12e44":"# Model inputs readiness","22dbe7ab":"### keyword feature","d85b117a":"### Removal of HTML tags\n","10e5c316":"### Removal of URLs\n","0a0e0cbb":"### location feature","86ab27f8":"# Libraries","7c0ab878":"### Apply Lemmatization","2cb1fb2a":"# Goal: Real Disaster Tweet or Not\n* Predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0\n\n### Dataset\n* kaggle competitions download -c nlp-getting-started\n\n### Columns\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a particular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n","33f93613":"###  Removal of Rare words"}}