{"cell_type":{"9ef35a62":"code","f45f53ee":"code","3580312f":"code","1244a109":"code","1682a905":"code","e0230b6c":"code","4fa9fa0a":"code","d45b7340":"code","ec49d754":"code","e360f7ac":"code","7e334763":"code","9f10f096":"code","e328469f":"code","00250185":"code","7faeca81":"code","3d029a7a":"code","ff79f5bc":"code","fa152acc":"code","9faac60d":"code","a68d5366":"code","07642cc3":"code","2aac397f":"code","d43c7488":"code","02e16586":"code","2e1ab357":"code","7623fae1":"code","f496a7f4":"markdown","df24f4ed":"markdown","8847f24a":"markdown","a7433d50":"markdown","277714cf":"markdown","779984fd":"markdown","b10e8b02":"markdown"},"source":{"9ef35a62":"! cp ..\/input\/tokenization\/tokenization.py  \/kaggle\/working","f45f53ee":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport tensorflow as tf \nimport cudf,cupy,cuml\nimport cv2 as cv \nfrom PIL import Image\nimport os\nfrom tqdm import tqdm\nimport re \nimport string\nimport tensorflow_hub as hub\nimport tokenization\nfrom tensorflow.keras.optimizers import SGD \nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.utils import Sequence\nimport cv2 as cv\nimport gc","3580312f":"test = pd.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\n#test_cudf = cudf.read_csv(\"..\/input\/shopee-product-matching\/test.csv\")\nimages_path = \"..\/input\/shopee-product-matching\/test_images\"\nif len(test) <= 3 :\n    test = pd.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\n    #test_cudf = cudf.read_csv(\"..\/input\/shopee-product-matching\/train.csv\")\n    images_path = \"..\/input\/shopee-product-matching\/train_images\"","1244a109":"test.head()","1682a905":"targ = test.groupby(\"label_group\").posting_id.unique()","e0230b6c":"test[\"target\"] = test.label_group.map(targ)","4fa9fa0a":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.target,row[col]) )\n        return 2*n \/ (len(row.target)+len(row[col]))\n    return f1score","d45b7340":"def clean(title):\n    \"\"\"This function, allows to clean title from useless characters and symbols.\n    \n    @ params :\n    title(str) : the title text that the function will clean up.\n    \n    @ returns :\n    title(str) : cleaned title\n\n    \n    \"\"\"\n    title = re.sub(r\"\\-\",\" \",title)\n    title = re.sub(r\"\\+\",\" \",title)\n    title = re.sub (r\"&\",\"and\",title)\n    title = re.sub(r\"\\|\",\" \",title)\n    title = re.sub(r\"\\\\\",\" \",title)\n    title = re.sub(r\"\\W\",\" \",title)\n    for p in string.punctuation :\n        title = re.sub(r\"f{p}\",\" \",title)\n    \n    title = re.sub(r\"\\s+\",\" \",title)\n    \n    return title","ec49d754":"def combine_matches(row):\n    return \" \".join(row.pred)","e360f7ac":"def combine(row):\n    x = np.concatenate([row.pred_img,row.pred_text])\n   \n    return np.unique(x)","7e334763":"label_group = test.groupby(\"label_group\").posting_id.unique()\ntest[\"target\"] = test.label_group.map(label_group)","9f10f096":"test[\"cleaned_title\"] = test.title.map(clean)","e328469f":"class BertEmbedding :\n    def __init__(self):\n        self.max_length = 0\n        self.bert = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\\\n                        trainable = False)\n        vocab_file = self.bert.resolved_object.vocab_file.asset_path.numpy()\n        do_lower_case = self.bert.resolved_object.do_lower_case.numpy()\n        self.tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n       \n        \n    def length(self,text):\n        mx = 0 \n        for tx in text :\n            mx = max(len(tx.split()),mx)\n        self.max_length = mx \n    def encode(self,text):\n        self.length(text)\n        all_mask = []\n        all_tokens = []\n        all_segments = []\n        for tx in text :\n            tokens = self.tokenizer.tokenize(tx)\n            tokens = ['[CLS]'] + tokens[:self.max_length - 2] + ['[SEP]']\n            tokens = self.tokenizer.convert_tokens_to_ids(tokens)\n            l = len(tokens)\n            pad_len = self.max_length - l\n            tokens = tokens + [0] * pad_len \n            mask_ids = [1] * l + [0] * pad_len\n            segments_ids = [0] * self.max_length\n            all_tokens.append(tokens)\n            all_mask.append(mask_ids)\n            all_segments.append(segments_ids)\n        return np.asarray(all_tokens),np.asarray(all_mask),np.asarray(all_segments)\n    def embedding_model(self):\n        input_words_ids = tf.keras.layers.Input(shape=(self.max_length,),dtype=tf.int32)\n        input_mask = tf.keras.layers.Input(shape=(self.max_length,),dtype=tf.int32)\n        segments_ids = tf.keras.layers.Input(shape=(self.max_length,),dtype=tf.int32)\n        pooled_output,sequence_output = self.bert([input_words_ids,input_mask,segments_ids])\n        x = sequence_output[:,0,:]\n        model = tf.keras.Model(inputs=[input_words_ids,input_mask,segments_ids],outputs= x)\n        model.compile(optimizer =SGD(1e-3,momentum=0.8),loss=\"binary_crossentropy\",metrics=['accuracy'])\n        return model\n    def predict(self,text):\n        data = self.encode(text)\n        model = self.embedding_model()\n        return model.predict(data)","00250185":"bert_text_embedding = BertEmbedding()\ntx_embedding = bert_text_embedding.predict(test[\"cleaned_title\"].values)","7faeca81":"from cuml.neighbors import NearestNeighbors\nnb = NearestNeighbors(n_neighbors=50)\nnb.fit(tx_embedding)","3d029a7a":"chunk = 1024 *4 \ncl = len(test) \/\/ chunk\ncl += int((len(test)% chunk) != 0)\npr=[]\nfor i in range(cl) :\n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    distances,indices = nb.kneighbors(tx_embedding[a:b,])\n    for j in range(b-a):\n        distance = distances[j,]\n        ind = np.where(distance < 3)[0]\n        ind = indices[j,ind]\n        pr.append(test.iloc[ind].posting_id.values)\ntest[\"pred_text\"] = pr","ff79f5bc":"test[\"f1\"] = test.apply(getMetric(\"pred_text\"),axis=1)\n    \nprint('CV score for tf embedding text =',test.f1.mean())","fa152acc":"class DataGenerator(Sequence):\n    def __init__(self,df,batch_size,path,img_size):\n        self.df = df \n        self.batch_size = batch_size \n        self.path = path \n        self.img_size = img_size\n        self.indexes = np.arange(len(df))\n    def __len__(self):\n        cl = (len(self.df) \/\/ self.batch_size)\n        cl += int((len(self.df) % self.batch_size) !=0)\n        return cl \n    def __getitem__(self,ind):\n        indices = self.indexes[ind * self.batch_size : (ind +1) * self.batch_size]\n        X = self.__data_generation(indices)\n        return X\n    def __data_generation(self,indices):\n        ddf = self.df.iloc[indices]\n        images = np.zeros((len(ddf),self.img_size,self.img_size,3),dtype=\"float32\")\n        for i, (j,row) in enumerate(ddf.iterrows()) :\n            img = cv.imread(os.path.join(self.path,row.image))\n            img = cv.resize(img,(self.img_size,self.img_size))\n            images[i,] = img \n        return images\n                            ","9faac60d":"WGT = \"..\/input\/effnetb0\/efficientnetb0_notop.h5\"\nmodel = EfficientNetB0(weights=WGT,include_top=False,pooling=\"avg\",input_shape=None)","a68d5366":"chunk = 1024 * 4 \ncls = len(test) \/\/ chunk \ncls += int (len(test) % chunk != 0)\nimage_embedding = []\nfor i in tqdm(range(cls)) :\n    \n    a = i * chunk \n    b = (i+1) * chunk \n    b = min(b,len(test))\n    data = DataGenerator(test.iloc[a:b],32,images_path,256)\n    emb = model.predict(data,use_multiprocessing=True,workers = 4)\n    image_embedding.append(emb)\n\ndel(model)\nimage_embedding = np.concatenate(image_embedding,axis=0)\ngc.collect()","07642cc3":"from cuml.neighbors import NearestNeighbors\nneighbors = NearestNeighbors(n_neighbors=50,metric=\"cosine\")\nneighbors.fit(image_embedding)","2aac397f":"chunk = 4 * 1024\ncl = len(test) \/\/ chunk \ncl += int ((len(test) % chunk) !=0)\nprediction = []\nfor i in tqdm(range(cl)):\n    a = i * chunk\n    b = (i+1) * chunk\n    b = min(b,len(test))\n    distances , indices = neighbors.kneighbors(image_embedding[a:b,])\n    for j in range(b-a):\n        distance = distances[j,:]\n        ind = np.where(distance < 0.2)[0]\n        IND = indices[j,ind]\n        #IND = cupy.asnumpy(IND)\n        prediction.append(test.iloc[IND].posting_id.values)\ntest[\"pred_img\"] = prediction","d43c7488":"test[\"f1\"] = test.apply(getMetric(\"pred_img\"),axis=1)\n    \nprint('CV score for tf embedding text =',test.f1.mean())","02e16586":"test[\"pred\"] = test.apply(combine,axis=1)\ntest[\"f\"] = test.apply(getMetric(\"pred\"),axis=1)\n    \nprint('CV score for baseline =',test.f.mean())","2e1ab357":"test[\"matches\"] = test.apply(combine_matches,axis=1)","7623fae1":"test[[\"posting_id\",\"matches\"]].to_csv(\"submission.csv\",index = False)\nsub = pd.read_csv('submission.csv')\nsub.head()","f496a7f4":"# 2.Useful functions :","df24f4ed":"tx_embedding = cupy.array(tx_embedding)","8847f24a":"### Images matching :","a7433d50":"# 1. Load datas:","277714cf":"# Modeling :","779984fd":"# Text matching :","b10e8b02":"descriptors = products_descriptors[0][1]\nfor tup in tqdm(products_descriptors[1:]) :\n    descriptors = np.vstack((descriptors,tup[1]))"}}