{"cell_type":{"e3a6f1b2":"code","08d831e8":"code","c93df7a0":"code","cdece7ec":"code","a43b45e1":"code","d725888c":"code","88e817a4":"code","670d47d3":"code","d6d46819":"code","804867e6":"code","2253342a":"code","c793d8f4":"code","142d6c1e":"code","7db63b77":"code","a5a5154f":"code","ac295c10":"code","18aff83a":"code","4f16327d":"code","80ee282f":"code","fe85ba32":"code","a5941897":"code","4e998603":"code","56a735da":"code","a019febc":"code","42271849":"code","d59958e4":"code","1a23e062":"code","480c2a36":"code","12abc37d":"code","9312571b":"code","899516ba":"code","fba08cd8":"code","d920cda5":"code","c42e0956":"code","bf1b1ffd":"code","a4d538e7":"code","02a49e7d":"code","2a633198":"code","71add5f6":"code","277f5158":"markdown","6731b9aa":"markdown","c0387217":"markdown","5e0e0b30":"markdown","602944fa":"markdown","9430d868":"markdown","4c7d23e1":"markdown","e4a99f79":"markdown"},"source":{"e3a6f1b2":"import numpy as np\nimport os\nfrom os.path import isfile\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Dense, Bidirectional, LSTM, Dropout, Activation, GRU\nfrom keras.layers import Conv2D, concatenate, MaxPooling2D, Flatten, Embedding, Lambda\n\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau\nfrom keras import backend as K\nfrom keras.utils import np_utils\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\nfrom keras import regularizers\n\n\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\n%matplotlib inline","08d831e8":"dict_genres = {'Electronic':0, 'Experimental':1, 'Folk':2, 'Hip-Hop':3, \n               'Instrumental':4,'International':5, 'Pop' :6, 'Rock': 7  }\n\n\nreverse_map = {v: k for k, v in dict_genres.items()}\nprint(reverse_map)","c93df7a0":"npzfile = np.load('..\/input\/load-data1\/shuffled_train.npz.npz\uff08\u526f\u672c\uff09')\nprint(npzfile.files)\nX_train = npzfile['arr_0']\ny_train = npzfile['arr_1']\nprint(X_train.shape, y_train.shape)","cdece7ec":"npzfile = np.load('..\/input\/load-data1\/shuffled_valid.npz')\nprint(npzfile.files)\nX_valid = npzfile['arr_0']\ny_valid = npzfile['arr_1']\nprint(X_valid.shape, y_valid.shape)","a43b45e1":"X_train_list = []\ny_train_list = []\nfor i,j in zip(X_train,y_train):\n    X_train_list.append(i[0:128])\n    X_train_list.append(i[128:256])\n    X_train_list.append(i[256:384])\n    X_train_list.append(i[384:512])\n    X_train_list.append(i[512:])\n    y_train_list.append(j)\n    y_train_list.append(j)\n    y_train_list.append(j)\n    y_train_list.append(j)\n    y_train_list.append(j)","d725888c":"X_train = np.array(X_train_list)\ny_train = np.array(y_train_list)\nX_train.shape,y_train.shape","88e817a4":"del X_train_list,y_train_list\nimport gc\ngc.collect()","670d47d3":"X_valid_list = []\ny_valid_list = []\nfor i,j in zip(X_valid,y_valid):\n    X_valid_list.append(i[0:128])\n    X_valid_list.append(i[128:256])\n    X_valid_list.append(i[256:384])\n    X_valid_list.append(i[384:512])\n    X_valid_list.append(i[512:])\n    y_valid_list.append(j)\n    y_valid_list.append(j)\n    y_valid_list.append(j)\n    y_valid_list.append(j)\n    y_valid_list.append(j)\nX_valid = np.array(X_valid_list)\ny_valid = np.array(y_valid_list)\nX_valid.shape,y_valid.shape\n\ndel X_valid_list,y_valid_list\nimport gc\ngc.collect()","d6d46819":"num = 5300\nspectogram = X_train[num]\ngenre = np.argmax(y_train[num])\nprint(reverse_map[genre])\nplt.figure(figsize=(10, 5))\nlibrosa.display.specshow(spectogram.T, y_axis='mel', x_axis='time')\nplt.colorbar(format='%+2.0f dB')\nplt.title('Test Melspectogram')\nplt.tight_layout()","804867e6":"batch_size = 32\nnum_classes = 8\nn_features = X_train.shape[2]\nn_time = X_train.shape[1]","2253342a":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n\nfrom tensorflow.keras import backend as K\nK.clear_session()\n\nBATCH_SIZE = 64\nEPOCH_COUNT = 50\nL2_regularization = 0.001","c793d8f4":"MobileNetV3 = tf.keras.applications.MobileNetV3Small\n\ninput_shape = (128,128, 1)\nmodel_input = Input(input_shape, name='input')\n\ninput_layer =tf.keras.layers.Concatenate(axis=3)([model_input, model_input,model_input])\n\n\ninc_model = MobileNetV3(include_top=False)(input_layer)\n\n#Adding custom Layers\n# x = inc_model.output\nx = GlobalAveragePooling2D()(inc_model)\nx = Dense(1024, activation=\"relu\")(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation=\"relu\")(x)\npredictions = Dense(8, activation=\"softmax\")(x)\n\n# creating the final model \nmodel = Model(inputs=model_input, outputs=predictions)\n\n# Lock initial layers to do not be trained\n# for layer in model.layers[:20]:\n#     layer.trainable = False\n\nmodel.compile(optimizer=Adam(lr=0.0001)\n                    , loss='categorical_crossentropy'\n                    , metrics=['accuracy'])","142d6c1e":"checkpointer = ModelCheckpoint(filepath='weights.best.hdf5', \n                               verbose=1, save_best_only=True)\n\nhistory = model.fit(X_train, y_train, \n                              batch_size=8,\n                              epochs=EPOCH_COUNT,\n                              validation_data=(X_valid, y_valid),\n                              callbacks=[checkpointer],\n                              verbose=1\n                    )","7db63b77":"# !pip install keras_applications","a5a5154f":"# import numpy as np\n# from tensorflow.keras.utils import plot_model\n# from keras_applications.imagenet_utils import _obtain_input_shape\n# from tensorflow.keras.utils import get_source_inputs\n# from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, GlobalMaxPooling2D, GlobalAveragePooling2D\n# from tensorflow.keras.layers import Activation, Dense\n# from tensorflow.keras.models import Model\n# import keras.backend as K\n# from shutil import copyfile\n\n# # copy our file into the working directory (make sure it has .py suffix)\n# copyfile(src = \"..\/input\/utils\/utils2.py\", dst = \"..\/working\/utils2.py\")\n\n# from utils2 import block","ac295c10":"# def ShuffleNetV2(include_top=True,\n#                  input_tensor=None,\n#                  scale_factor=1.0,\n#                  pooling='max',\n#                  input_shape=(224,224,3),\n#                  load_model=None,\n#                  num_shuffle_units=[3,7,3],\n#                  bottleneck_ratio=1,\n#                  classes=1000):\n#     if K.backend() != 'tensorflow':\n#         raise RuntimeError('Only tensorflow supported for now')\n#     name = 'ShuffleNetV2_{}_{}_{}'.format(scale_factor, bottleneck_ratio, \"\".join([str(x) for x in num_shuffle_units]))\n#     input_shape = _obtain_input_shape(input_shape, default_size=224, min_size=28, require_flatten=include_top,\n#                                       data_format=K.image_data_format())\n#     out_dim_stage_two = {0.5:48, 1:116, 1.5:176, 2:244}\n\n#     if pooling not in ['max', 'avg']:\n#         raise ValueError('Invalid value for pooling')\n#     if not (float(scale_factor)*4).is_integer():\n#         raise ValueError('Invalid value for scale_factor, should be x over 4')\n#     exp = np.insert(np.arange(len(num_shuffle_units), dtype=np.float32), 0, 0)  # [0., 0., 1., 2.]\n#     out_channels_in_stage = 2**exp\n#     out_channels_in_stage *= out_dim_stage_two[bottleneck_ratio]  #  calculate output channels for each stage\n#     out_channels_in_stage[0] = 24  # first stage has always 24 output channels\n#     out_channels_in_stage *= scale_factor\n#     out_channels_in_stage = out_channels_in_stage.astype(int)\n\n#     if input_tensor is None:\n#         img_input = Input(shape=input_shape)\n#     else:\n#         if not K.is_keras_tensor(input_tensor):\n#             img_input = Input(tensor=input_tensor, shape=input_shape)\n#         else:\n#             img_input = input_tensor\n\n#     # create shufflenet architecture\n#     x = Conv2D(filters=out_channels_in_stage[0], kernel_size=(3, 3), padding='same', use_bias=False, strides=(2, 2),\n#                activation='relu', name='conv1')(img_input)\n#     x = MaxPool2D(pool_size=(3, 3), strides=(2, 2), padding='same', name='maxpool1')(x)\n\n#     # create stages containing shufflenet units beginning at stage 2\n#     for stage in range(len(num_shuffle_units)):\n#         repeat = num_shuffle_units[stage]\n#         x = block(x, out_channels_in_stage,\n#                    repeat=repeat,\n#                    bottleneck_ratio=bottleneck_ratio,\n#                    stage=stage + 2)\n\n#     if bottleneck_ratio < 2:\n#         k = 1024\n#     else:\n#         k = 2048\n#     x = Conv2D(k, kernel_size=1, padding='same', strides=1, name='1x1conv5_out', activation='relu')(x)\n\n#     if pooling == 'avg':\n#         x = GlobalAveragePooling2D(name='global_avg_pool')(x)\n#     elif pooling == 'max':\n#         x = GlobalMaxPooling2D(name='global_max_pool')(x)\n\n#     if include_top:\n#         x = Dense(classes, name='fc')(x)\n#         x = Activation('softmax', name='softmax')(x)\n\n#     if input_tensor:\n#         inputs = get_source_inputs(input_tensor)\n\n#     else:\n#         inputs = img_input\n\n#     model = Model(inputs, x, name=name)\n\n#     if load_model:\n#         model.load_weights('', by_name=True)\n\n#     return model","18aff83a":"from tensorflow.keras import backend as K\nK.clear_session()","4f16327d":"# input_shape = (128,128, 1)\n# model_input = Input(input_shape, name='input')\n\n# input_layer =tf.keras.layers.Concatenate(axis=3)([model_input, model_input,model_input])\n\n\n# inc_model = ShuffleNetV2(include_top=False, input_shape=(128,128, 3),load_model=None, classes=8)(input_layer)\n# #Adding custom Layers\n# # x = inc_model.output\n# #x = GlobalAveragePooling2D()(inc_model)\n# x = Dense(1024, activation=\"relu\")(inc_model)\n# x = Dropout(0.5)(x)\n# x = Dense(512, activation=\"relu\")(x)\n# x = Dropout(0.5)(x)\n# x = Dense(256, activation=\"relu\")(x)\n# predictions = Dense(8, activation=\"softmax\")(x)\n\n# # creating the final model \n# model = Model(inputs=model_input, outputs=predictions)\n\n# #Lock initial layers to do not be trained\n# # for layer in model.layers[:50]:\n# #     layer.trainable = False\n\n# model.compile(optimizer=Adam(lr=0.0001)\n#                     , loss='categorical_crossentropy'\n#                     , metrics=['accuracy'])","80ee282f":"# model.summary()","fe85ba32":"# checkpointer = ModelCheckpoint(filepath='weights.best.hdf5', \n#                                verbose=1, save_best_only=True)\n\n# history = model.fit(X_train, y_train, \n#                               batch_size=8,\n#                               epochs=EPOCH_COUNT,\n#                               validation_data=(X_valid, y_valid),\n#                               callbacks=[checkpointer],\n#                               verbose=1\n#                     )","a5941897":"from tensorflow.keras import backend as K\nK.clear_session()","4e998603":"# !pip install -U git+https:\/\/github.com\/leondgarse\/keras_efficientnet_v2","56a735da":"# import keras_efficientnet_v2\n# import tensorflow as tf\n\n# input_shape = (640,128, 1)\n# model_input = Input(input_shape, name='input')\n\n# input_layer =tf.keras.layers.Concatenate(axis=3)([model_input, model_input,model_input])\n\n\n# inc_model = keras_efficientnet_v2.EfficientNetV2B0(input_shape=(None, None, 3),drop_connect_rate=0.5, num_classes=0, pretrained=\"imagenet21k-ft1k\")(input_layer)\n\n# #Adding custom Layers\n# # x = inc_model.output\n# x = GlobalAveragePooling2D()(inc_model)\n# x = Dense(1024, activation=\"relu\")(x)\n# x = Dropout(0.5)(x)\n# x = Dense(512, activation=\"relu\")(x)\n# x = Dropout(0.5)(x)\n# x = Dense(256, activation=\"relu\")(x)\n# predictions = Dense(8, activation=\"softmax\")(x)\n\n# # creating the final model \n# model = Model(inputs=model_input, outputs=predictions)\n\n# #Lock initial layers to do not be trained\n# # for layer in model.layers[:50]:\n# #     layer.trainable = False\n\n# model.compile(optimizer=Adam(lr=0.0001)\n#                     , loss='categorical_crossentropy'\n#                     , metrics=['accuracy'])","a019febc":"# model.summary()","42271849":"# checkpointer = ModelCheckpoint(filepath='weights.best.hdf5', \n#                                verbose=1, save_best_only=True)\n\n# history = model.fit(X_train, y_train, \n#                               batch_size=BATCH_SIZE,\n#                               epochs=EPOCH_COUNT,\n#                               validation_data=(X_valid, y_valid),\n#                               callbacks=[checkpointer],\n#                               verbose=1\n#                     )","d59958e4":"# nb_filters1=16 \n# nb_filters2=32 \n# nb_filters3=64\n# nb_filters4=64\n# nb_filters5=64\n# ksize = (3,1)p'\n# pool_size_1= (2,2) \n# pool_size_2= (4,4)\n# pool_size_3 = (4,2)\n\n# dropout_prob = 0.20\n# dense_size1 = 128\n# lstm_count = 64\n# num_units = 120\n\n# BATCH_SIZE = 64\n# EPOCH_COUNT = 50\n# L2_regularization = 0.001\n\n# def conv_recurrent_model_build(model_input):\n#     print('Building model...')\n#     layer = model_input\n    \n#     ### Convolutional blocks\n#     conv_1 = Conv2D(filters = nb_filters1, kernel_size = ksize, strides=1,\n#                       padding= 'valid', activation='relu', name='conv_1')(layer)\n#     pool_1 = MaxPooling2D(pool_size_1)(conv_1)\n\n#     conv_2 = Conv2D(filters = nb_filters2, kernel_size = ksize, strides=1,\n#                       padding= 'valid', activation='relu', name='conv_2')(pool_1)\n#     pool_2 = MaxPooling2D(pool_size_1)(conv_2)\n\n#     conv_3 = Conv2D(filters = nb_filters3, kernel_size = ksize, strides=1,\n#                       padding= 'valid', activation='relu', name='conv_3')(pool_2)\n#     pool_3 = MaxPooling2D(pool_size_1)(conv_3)\n    \n    \n#     conv_4 = Conv2D(filters = nb_filters4, kernel_size = ksize, strides=1,\n#                       padding= 'valid', activation='relu', name='conv_4')(pool_3)\n#     pool_4 = MaxPooling2D(pool_size_2)(conv_4)\n    \n    \n#     conv_5 = Conv2D(filters = nb_filters5, kernel_size = ksize, strides=1,\n#                       padding= 'valid', activation='relu', name='conv_5')(pool_4)\n#     pool_5 = MaxPooling2D(pool_size_2)(conv_5)\n\n#     flatten1 = Flatten()(pool_5)\n#     ### Recurrent Block\n    \n#     # Pooling layer\n#     pool_lstm1 = MaxPooling2D(pool_size_3, name = 'pool_lstm')(layer)\n    \n#     # Embedding layer\n\n#     squeezed = Lambda(lambda x: K.squeeze(x, axis= -1))(pool_lstm1)\n# #     flatten2 = K.squeeze(pool_lstm1, axis = -1)\n# #     dense1 = Dense(dense_size1)(flatten)\n    \n#     # Bidirectional GRU\n#     lstm = Bidirectional(GRU(lstm_count))(squeezed)  #default merge mode is concat\n    \n#     # Concat Output\n#     concat = concatenate([flatten1, lstm], axis=-1, name ='concat')\n    \n#     ## Softmax Output\n#     output = Dense(num_classes, activation = 'softmax', name='preds')(concat)\n    \n#     model_output = output\n#     model = Model(model_input, model_output)\n    \n# #     opt = Adam(lr=0.001)\n#     opt = RMSprop(lr=0.0005)  # Optimizer\n#     model.compile(\n#             loss='categorical_crossentropy',\n#             optimizer=opt,\n#             metrics=['accuracy']\n#         )\n    \n#     print(model.summary())\n#     return model","1a23e062":"# def train_model(x_train, y_train, x_val, y_val):\n    \n#     n_frequency = 128\n#     n_frames = 640\n#     #reshape and expand dims for conv2d\n# #     x_train = x_train.reshape(-1, n_frequency, n_frames)\n#     x_train = np.expand_dims(x_train, axis = -1)\n    \n    \n# #     x_val = x_val.reshape(-1, n_frequency, n_frames)\n#     x_val = np.expand_dims(x_val, axis = -1)\n    \n    \n#     input_shape = (n_frames, n_frequency, 1)\n#     model_input = Input(input_shape, name='input')\n    \n#     model = conv_recurrent_model_build(model_input)\n    \n# #     tb_callback = TensorBoard(log_dir='.\/logs\/4', histogram_freq=1, batch_size=32, write_graph=True, write_grads=False,\n# #                               write_images=False, embeddings_freq=0, embeddings_layer_names=None,\n# #                               embeddings_metadata=None)\n#     checkpoint_callback = ModelCheckpoint('.\/weights.best.h5', monitor='val_acc', verbose=1,\n#                                           save_best_only=True, mode='max')\n    \n#     reducelr_callback = ReduceLROnPlateau(\n#                 monitor='val_acc', factor=0.5, patience=10, min_delta=0.01,\n#                 verbose=1\n#             )\n#     callbacks_list = [checkpoint_callback, reducelr_callback]\n\n#     # Fit the model and get training history.\n#     print('Training...')\n#     history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCH_COUNT,\n#                         validation_data=(x_val, y_val), verbose=1, callbacks=callbacks_list)\n\n#     return model, history\n","480c2a36":"def show_summary_stats(history):\n    # List all data in history\n    print(history.history.keys())\n\n    # Summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()\n\n    # Summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","12abc37d":"# model, history  = train_model(X_train, y_train, X_valid, y_valid)","9312571b":"show_summary_stats(history)","899516ba":"from sklearn.metrics import classification_report\n\ny_true = np.argmax(y_valid, axis = 1)\nX_valid = np.expand_dims(X_valid, axis = -1)\ny_pred = model.predict(X_valid)\ny_pred = np.argmax(y_pred, axis=1)\nlabels = [0,1,2,3,4,5,6,7]\ntarget_names = dict_genres.keys()\n\nprint(y_true.shape, y_pred.shape)\nprint(classification_report(y_true, y_pred, target_names=target_names))\n","fba08cd8":"from sklearn.metrics import accuracy_score\n\nprint(accuracy_score(y_true, y_pred))","d920cda5":"npzfile = np.load('..\/input\/load-data1\/test_arr.npz')\nprint(npzfile.files)\nX_test = npzfile['arr_0']\ny_test = npzfile['arr_1']\nprint(X_test.shape, y_test.shape)","c42e0956":"from keras.models import load_model\n\nweights_path = '.\/weights.best.hdf5'\nmodel = load_model(weights_path)","bf1b1ffd":"y_test -= 1\nprint(np.amin(y_test), np.amax(y_test), np.mean(y_test))","a4d538e7":"X_test_raw = librosa.core.db_to_power(X_test, ref=1.0)\nprint(np.amin(X_test_raw), np.amax(X_test_raw), np.mean(X_test_raw))","02a49e7d":"X_test = np.log(X_test_raw)\n\n\nfrom sklearn.metrics import classification_report\n\ny_true = y_test\nX_test = np.expand_dims(X_test, axis = -1)\ny_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)\nlabels = [0,1,2,3,4,5,6,7]\ntarget_names = dict_genres.keys()\n\nprint(y_true.shape, y_pred.shape)\nprint(classification_report(y_true, y_pred, target_names=target_names))","2a633198":"from sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\n\nmat = confusion_matrix(y_true, y_pred)\nsns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n            xticklabels=dict_genres.keys(),\n            yticklabels=dict_genres.keys())\nplt.xlabel('true label')\nplt.ylabel('predicted label');","71add5f6":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(y_true, y_pred))","277f5158":"### Check by plotting a Spectogram","6731b9aa":"### MobileNetV3 ","c0387217":"## moded ","5e0e0b30":"### efficientnetv2","602944fa":"### ShuffleNet","9430d868":"### Look at the Test Set","4c7d23e1":"bset 0.6313","e4a99f79":"### Load training and Validation arrays"}}