{"cell_type":{"4c9d0785":"code","4ba71d95":"code","749c9ae2":"code","a8da0862":"code","ea6450c5":"code","145f15b8":"code","55995b63":"code","9779c307":"code","6a003f13":"code","aa1125ec":"code","df0e28cc":"code","d639473d":"code","37dfa1cf":"code","cce8bf96":"code","4eaf9725":"code","0fa2e79e":"code","f069748f":"code","b46ce74d":"code","4f89d956":"code","d5df6b4d":"code","a11547b1":"code","f4ba5f59":"code","b880f7bd":"code","de2d3424":"code","334bada3":"code","73364a45":"code","11f5c629":"code","452c7f8f":"code","dae2662c":"code","058a5bf1":"code","9d3256cc":"code","03a10df5":"code","b30279fe":"code","c7cea3b5":"code","aea887ea":"code","b23c876c":"code","bc8a8ea5":"code","f95baa36":"code","bb769fa7":"code","675d3511":"code","20454270":"code","ed782cb0":"code","79947f49":"code","e792c352":"code","0f418dfb":"code","51789b4f":"code","c75b8277":"code","f4097197":"code","b34337df":"code","58f9a8a1":"code","96ac0f41":"code","3a508890":"code","be077a7d":"code","ddb2449a":"code","502be780":"code","a210c738":"code","148aba71":"code","17eda8a6":"code","27c588ce":"code","2b46e000":"code","eb1c80bf":"code","0fbe51db":"code","85e17ea3":"code","ae0a002f":"code","f9cf9c19":"markdown","53934c53":"markdown","dfe79938":"markdown","a64d936b":"markdown","fc199896":"markdown","ba3feb64":"markdown","b4a5d006":"markdown","d21d4f59":"markdown","3ea4ed6f":"markdown","a8dc2584":"markdown","de928f13":"markdown","40630614":"markdown","49a5d21e":"markdown","aff85cda":"markdown","06ead290":"markdown","e673f84b":"markdown","e9ce296b":"markdown","dd8d2fb6":"markdown","e8848f88":"markdown","569a5467":"markdown","2bebf416":"markdown","e1129357":"markdown","57904021":"markdown","05e2934a":"markdown"},"source":{"4c9d0785":"import numpy as np\nfrom collections import defaultdict\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n%matplotlib inline\n\nimport re\nimport string\nimport spacy\n\n# misc:\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tqdm as tqdm\nimport os","4ba71d95":"class Config:\n    \n    def __init__(self):\n        \n        # hyperparameters\n        \n        # paths\n        self.train_file_path = '..\/input\/feedback-prize-2021\/train.csv'\n        self.train_path = '..\/input\/feedback-prize-2021\/train'\n        self.test_path = '..\/input\/feedback-prize-2021\/test'\n        self.model_save_path = '.'\n        \n        # others\n        ","749c9ae2":"conf = Config()","a8da0862":"train_df = pd.read_csv(conf.train_file_path)","ea6450c5":"#pd.set_option('display.max_colwidth', -1)  # to make dataframe take up the whole width of the screen","145f15b8":"train_df.head()","55995b63":"train_df.predictionstring[0]","9779c307":"train_df.info()","6a003f13":"# total number of unique text files\nlen(train_df['id'].unique())","aa1125ec":"# sample discourse text\ntrain_df['discourse_text'][0]","df0e28cc":"# utility for printing the text\ndef print_text(txt_id):\n    with open(f'{conf.train_path}\/{txt_id}.txt', 'r') as fp:\n        text = fp.readlines()\n    print(''.join(text))","d639473d":"# corresponding text for above discourse text\nprint_text('423A1CA112E2')","37dfa1cf":"# classes\ntrain_df['discourse_type'].unique().tolist()","cce8bf96":"classes = ['Lead', 'Position', 'Evidence', 'Claim', 'Counterclaim', 'Rebuttal', 'Concluding Statement']","4eaf9725":"discourse_type_num = train_df['discourse_type_num'].unique().tolist()\ndiscourse_type_num[0:5]","0fa2e79e":"# This function returns a list, where each index in the list correspond to a particular class (as in the list \n# classes declared above). Each entry in the returned list represents the maximum number of times a class has appeared in some\n# text(s).\n\ndef get_list_of_max(classes, discourse_type_num):\n    list_of_max = []\n\n    for _class in classes:\n        mx=0\n        for _type in discourse_type_num:\n            if _type[-1:].isdigit() and _type[-2:].isdigit():\n                if _class == _type[:-3]:\n                    curr = _type[-2:]\n                    mx = max(mx, int(curr))\n            else:\n                if _class == _type[:-2]:\n                    curr = _type[-1:]\n                    mx = max(mx, int(curr))\n            \n        list_of_max.append(mx)\n        \n    return list_of_max","f069748f":"# get the maximum frequencies of all classes\nlist_of_max = get_list_of_max(classes, discourse_type_num)","b46ce74d":"plt.figure(figsize = (7,5))\n\nax = sns.barplot(x=list_of_max, y=classes)\nax.set_xlabel('max frequency')\nax.set_ylabel('classes')\nax.set_title('Maximum number of times a class can be present in a text')\n\n# we can see from below graph that evidence and claim have appeared a maximum of 12 times in some text(s)","4f89d956":"plt.figure(figsize=(9,7))\nsns.set_context('paper',font_scale=1.5)\nax = sns.countplot(x='discourse_type', data=train_df)\nax.set_xticklabels(classes, rotation=40, ha='right');","d5df6b4d":"dframe = train_df.drop_duplicates(subset = ['id', 'discourse_type'], keep = 'first', inplace = False)","a11547b1":"dframe.head()","f4ba5f59":"len(dframe)","b880f7bd":"plt.figure(figsize=(9,7))\nsns.set_context('paper',font_scale=1.5)\nax = sns.countplot(x='discourse_type', data=dframe)\nax.set_xticklabels(classes, rotation=40, ha='right');","de2d3424":"df = train_df.copy()","334bada3":"df['full_text'] = df['discourse_text'].groupby(df['id']).transform(lambda x: ' '.join(x))","73364a45":"text_length = df['full_text'].drop_duplicates().apply(len)\n\nfig = plt.figure(figsize=(9,7))\n\nax = sns.distplot(text_length, kde=False, bins=100, color='r')\nax.set_title('Distribution of Text Length')\nax.set_xlabel(\"Text Length\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","11f5c629":"word_count = df['full_text'].drop_duplicates().apply(lambda x: len(str(x).split()))\n\nfig = plt.figure(figsize=(9,7))\n\nax = sns.distplot(word_count, kde=False, bins=100, color='r')\nax.set_title('Distribution of Word Count')\nax.set_xlabel(\"Word Count\")\nax.set_ylabel(\"Frequency\")\n\nplt.show()","452c7f8f":"df = train_df.copy()","dae2662c":"df['discourse_len'] = df['discourse_text'].apply(lambda x: len(x.split()))","058a5bf1":"fig = plt.figure(figsize=(6,5))\n\nax = fig.add_axes([0,0,1,1])\nax = df.groupby('discourse_type')['discourse_len'].mean().sort_values().plot(kind=\"barh\", color='lightseagreen')\nax.set_title(\"Average number of words versus Discourse Type\", fontsize=14)\nax.set_xlabel(\"Average number of words\", fontsize = 12)\nax.set_ylabel(\"\")","9d3256cc":"labels = ['Evidence', 'Concluding Statement', 'Lead', 'Rebuttal', 'Counterclaim', 'Position', 'Claim']\nfeature = 'discourse_len'\n\ndata = []\nfor i in range(len(labels)):\n    _data = df.loc[df['discourse_type'] == labels[i]][feature]\n    data.append(_data)\n    \nrows, cols = 4, 2\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize = (15, 27))\nfor idx, axis in enumerate(axes.reshape(-1)):\n    if idx<len(labels):\n        ax = sns.distplot(data[idx], ax = axis, color = 'lightseagreen')\n        ax.set_title(labels[idx])\n    if idx == len(labels):\n        axis.axis('off')  # for turning off the axis of the very last plot","03a10df5":"plt.figure(figsize = (12, 5))\nax = sns.kdeplot(data[0])\nax = sns.kdeplot(data[1])\nax = sns.kdeplot(data[2])\nax = sns.kdeplot(data[3])\nax = sns.kdeplot(data[4])\nax = sns.kdeplot(data[5])\nax = sns.kdeplot(data[6])\nax.legend(labels)","b30279fe":"df['unique_words_count'] = train_df['discourse_text'].apply(lambda x: len(set(str(x).split())))","c7cea3b5":"fig = plt.figure(figsize=(6,5))\n\nax = fig.add_axes([0,0,1,1])\nax = df.groupby('discourse_type')['unique_words_count'].mean().sort_values().plot(kind=\"barh\", color='mediumturquoise')\nax.set_title(\"Unique number of words versus Discourse Type\", fontsize=14)\nax.set_xlabel(\"Unique number of words\", fontsize = 12)\nax.set_ylabel(\"\")","aea887ea":"labels = ['Evidence', 'Concluding Statement', 'Lead', 'Rebuttal', 'Counterclaim', 'Position', 'Claim']\nfeature = 'unique_words_count'\n\ndata = []\nfor i in range(len(labels)):\n    _data = df.loc[df['discourse_type'] == labels[i]][feature]\n    data.append(_data)\n    \nrows, cols = 4, 2\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize = (15, 27))\nfor idx, axis in enumerate(axes.reshape(-1)):\n    if idx<len(labels):\n        ax = sns.distplot(data[idx], ax = axis, color = 'teal')\n        ax.set_title(labels[idx])\n    if idx == len(labels):  \n        axis.axis('off')  # for turning off the axis of the very last plot","b23c876c":"plt.figure(figsize = (12, 5))\nax = sns.kdeplot(data[0])\nax = sns.kdeplot(data[1])\nax = sns.kdeplot(data[2])\nax = sns.kdeplot(data[3])\nax = sns.kdeplot(data[4])\nax = sns.kdeplot(data[5])\nax = sns.kdeplot(data[6])\nax.legend(labels)","bc8a8ea5":"df['avg_word_length'] = train_df['discourse_text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","f95baa36":"fig = plt.figure(figsize=(6,5))\n\nax = fig.add_axes([0,0,1,1])\nax = df.groupby('discourse_type')['avg_word_length'].mean().sort_values().plot(kind=\"barh\", color='steelblue')\nax.set_title(\"Average word length versus Discourse Type\", fontsize=14)\nax.set_xlabel(\"Average word length\", fontsize = 12)\nax.set_ylabel(\"\")","bb769fa7":"labels = ['Evidence', 'Concluding Statement', 'Lead', 'Rebuttal', 'Counterclaim', 'Position', 'Claim']\nfeature = 'avg_word_length'\n\ndata = []\nfor i in range(len(labels)):\n    _data = df.loc[df['discourse_type'] == labels[i]][feature]\n    data.append(_data)\n    \nrows, cols = 4, 2\nfig, axes = plt.subplots(nrows=rows, ncols=cols, figsize = (15, 27))\nfor idx, axis in enumerate(axes.reshape(-1)):\n    if idx<len(labels):\n        ax = sns.distplot(data[idx], ax = axis, color = 'steelblue')\n        ax.set_title(labels[idx])\n    if idx == len(labels):  \n        axis.axis('off')  # for turning off the axis of the very last plot","675d3511":"plt.figure(figsize = (12, 5))\nax = sns.kdeplot(data[0])\nax = sns.kdeplot(data[1])\nax = sns.kdeplot(data[2])\nax = sns.kdeplot(data[3])\nax = sns.kdeplot(data[4])\nax = sns.kdeplot(data[5])\nax = sns.kdeplot(data[6])\nax.legend(labels)","20454270":"data_df = train_df.groupby(\"discourse_type\")[['discourse_end', 'discourse_start']].mean().reset_index().sort_values(\n    by = 'discourse_start', \n    ascending = False)\n\ndata_df.plot(x='discourse_type',\n        kind='barh',\n        stacked=False,\n        title='Average start and end position absolute',\n        figsize=(12,4))\n\nplt.show()","ed782cb0":"# creating temporary corpus for analysing punctuations and stopwords:\ndef temporary_corpus(target = None):\n    corpus=[]\n    \n    if target == None:\n        text_data = train_df['discourse_text'].str.split()\n    else:\n        text_data = train_df[train_df['discourse_type']==target]['discourse_text'].str.split()\n        \n    for text in text_data:\n        for char in text:\n            corpus.append(char)\n    return corpus","79947f49":"plt.figure(figsize=(10,5))\ncorpus=temporary_corpus()\n\ndic=defaultdict(int)\nspecial = string.punctuation\nfor token in corpus:\n    for character in token:\n        if character in special:\n            dic[character]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y, color='cadetblue')","e792c352":"import en_core_web_lg\nnlp=en_core_web_lg.load()","0f418dfb":"corpus = temporary_corpus()","51789b4f":"def get_frequent_stopwords(corpus):\n    \n    dic=defaultdict(int)\n    for word in corpus:\n        if word in nlp.Defaults.stop_words:\n            dic[word]+=1\n\n    # getting the top 20 most frequent stop words        \n    top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]\n    \n    return top, dic","c75b8277":"top, dic = get_frequent_stopwords(corpus)","f4097197":"print(f'Total number of stop words in spacy: {len(nlp.Defaults.stop_words)}')\nprint(f'Total number of stop words in the given text data which are part of the spacy\\'s stop words list: {len(dic)}')","b34337df":"# top 20 most frequent stop words in our corpus:\nplt.figure(figsize = (15,4))\nx,y=zip(*top)\nplt.bar(x,y, color='goldenrod')","58f9a8a1":"def get_count(text):\n    count=0\n    for ch in text:\n        if ch in nlp.Defaults.stop_words:\n            count+=1\n    return count","96ac0f41":"df = train_df.copy()\ndf['stop_words_count'] = train_df['discourse_text'].apply(lambda x: get_count(x))","3a508890":"labels = ['Evidence', 'Concluding Statement', 'Lead', 'Rebuttal', 'Counterclaim', 'Position', 'Claim']\n\ndata = []\nfor i in range(len(labels)):\n    _data = df.loc[df['discourse_type'] == labels[i]]['stop_words_count']\n    data.append(_data)","be077a7d":"plt.figure(figsize = (12, 5))\nax = sns.kdeplot(data[0])\nax = sns.kdeplot(data[1])\nax = sns.kdeplot(data[2])\nax = sns.kdeplot(data[3])\nax = sns.kdeplot(data[4])\nax = sns.kdeplot(data[5])\nax = sns.kdeplot(data[6])\nax.legend(labels)","ddb2449a":"from sklearn.feature_extraction.text import CountVectorizer","502be780":"def get_n_grams(n_grams, top_n = 10):\n    \n    df_words = pd.DataFrame()\n    \n    for dt in train_df['discourse_type'].unique():\n        \n        df = train_df.query('discourse_type == @dt')\n        texts = df['discourse_text'].tolist()\n        vec = CountVectorizer(lowercase = True, stop_words = 'english',\\\n                              ngram_range=(n_grams, n_grams)).fit(texts)\n        bag_of_words = vec.transform(texts)\n        sum_words = bag_of_words.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n        cvec_df = pd.DataFrame.from_records(words_freq,\\\n                                            columns= ['words', 'counts']).sort_values(by=\"counts\", ascending=False)\n        cvec_df.insert(0, \"Discourse_type\", dt)\n        cvec_df = cvec_df.iloc[:top_n,:]\n        df_words = df_words.append(cvec_df)\n        \n    return df_words","a210c738":"unigrams = get_n_grams(n_grams = 1, top_n=10)\nunigrams.head()","148aba71":"def plot_ngram(df, type = \"unigrams\"):\n    \n    plt.figure(figsize=(15, 12))\n    plt.subplots_adjust(hspace=0.5)\n\n    for n, dt in enumerate(df.Discourse_type.unique()):\n        ax = plt.subplot(4, 2, n + 1)\n        ax.set_title(f\"Most used {type} in {dt}\")\n        data = df.query('Discourse_type == @dt')[['words', 'counts']].set_index(\"words\").sort_values(by = \"counts\", ascending = True)\n        data.plot(ax=ax, kind = 'barh', color = 'steelblue')\n        plt.ylabel(\"\")\n    plt.tight_layout()\n    plt.show()","17eda8a6":"# unigrams\nplot_ngram(unigrams)","27c588ce":"# bigrams\nbigrams = get_n_grams(n_grams = 2, top_n=10)\nplot_ngram(bigrams, 'bigrams')","2b46e000":"# trigrams\ntrigrams = get_n_grams(n_grams = 3, top_n=10)\nplot_ngram(trigrams, 'trigrams')","eb1c80bf":"train_files = os.listdir(conf.train_path)\ntest_files = os.listdir(conf.test_path)\n\nfor file in range(len(train_files)):\n    train_files[file] = str(conf.train_path) + \"\/\" +  str(train_files[file])\nfor file in range(len(test_files)):\n    test_files[file] = str(conf.test_path) + \"\/\" +  str(test_files[file])","0fbe51db":"train_files[40]","85e17ea3":"train_files[40][35:-4]","ae0a002f":"r = 20\nents = []\n \nfor i, row in train_df[train_df['id'] == train_files[r][35:-4]].iterrows():\n    ents.append({\n                    'start': int(row['discourse_start']),   \n                     'end': int(row['discourse_end']), \n                     'label': row['discourse_type']\n                })\n\nwith open(train_files[r], 'r') as file: \n    data = file.read()\n\ndoc2 = {\n    \"text\": data,\n    \"ents\": ents,\n}\n\ncolors = {'Lead': 'turquoise',\n          'Position': '#f9d5de',\n          'Claim': '#adcfad',\n          'Evidence': 'wheat',\n          'Counterclaim': '#bdf2fa',\n          'Concluding Statement': '#eea69e',\n          'Rebuttal': '#d1f8f4'}\n\noptions = {\"ents\": train_df.discourse_type.unique().tolist(), \"colors\": colors}\nspacy.displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","f9cf9c19":"# Exploration","53934c53":"Distribution of discourse types over discourse length:","dfe79938":"<b>Let's have a look at class distribution when considering only single instances of classes (discourse_type) in any given text.<\/b>","a64d936b":"<b>Stopwords (with reference to spacy's stopwords list):<\/b>","fc199896":"### To be continued..","ba3feb64":"Visulization in a single plot:","b4a5d006":"Distribution of discourse types over count of unique words in discourse texts:","d21d4f59":"<b>Distribution of the length of complete texts:<\/b>","3ea4ed6f":"#### Path and configuration class","a8dc2584":"<b>Punctuations:<\/b>","de928f13":"<b>n-grams analysis:<\/b>","40630614":"#### Text Visualization","49a5d21e":"<b>Relation between discourse types and frequency of unique words in each discourse text:<\/b> ","aff85cda":"Visualization in a single plot:","06ead290":"<b>Let's see how many times a particular class appeared at maximum in some text(s) with maximum taken across all texts:<\/b>","e673f84b":"<b>Relation between discourse types and discourse length:<\/b>","e9ce296b":"Distribution of discourse types over stopword counts:","dd8d2fb6":"<b>Distribution for word count:<\/b>","e8848f88":"Distribution of discourse types over the average word lengths of discourse texts:","569a5467":"Note: The terms 'discourse type(s)' and 'classe(s)' are used interchangeably.","2bebf416":"<b>Distribution of classes across all rows of the dataframe:<\/b><br><br>\nNote that the train_df also contains multiple rows for the same text, and a particular discourse type, as we know, can occur multiple times (as shown in the column discourse_type_num) in any given text, therefore the count axis in the below plot takes into account all those multiple occurences of a discourse type of the same text.\n<br><br>\nWe see that the evidence and the claim are the dominant discourse types with rebuttal and concluding statement being the rare ones.","e1129357":"<b>Relation between discourse types and average word length of discourse texts:<\/b>","57904021":"Visualization in a single plot:","05e2934a":"<b>Visualizing the start and end positions of discourse texts:<\/b>"}}