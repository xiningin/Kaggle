{"cell_type":{"eccc9e79":"code","21527688":"code","8a6f6a8c":"code","2cfb8560":"code","dfff89a8":"code","ef18c182":"code","86dc026f":"code","9976214c":"code","b9077955":"code","c2bd8c26":"code","24ac3a18":"code","7f919eda":"code","a808a49b":"code","9c7fd804":"code","41227049":"code","6fb56ffc":"code","14a664fd":"code","40aa6c3b":"code","4e7c84a7":"code","2ad196e0":"code","e15cc288":"code","8ea103c9":"code","c7cd2627":"code","edfdc517":"code","8cf0e266":"code","1f68e0d0":"code","d4106208":"code","af8f95fc":"code","0460f54b":"code","4201af00":"code","ee3de74b":"code","2fa5e0ed":"code","63f5f0a4":"code","d5e25af0":"code","eb88e2ed":"code","efae311c":"markdown","1fd9474a":"markdown","e250b049":"markdown","653f6c6c":"markdown","8d4c1e0c":"markdown","29c9c07e":"markdown","5b64a54d":"markdown","1ec05bce":"markdown","042c2444":"markdown","07424ed7":"markdown","b62a7769":"markdown","a7d69c7f":"markdown"},"source":{"eccc9e79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21527688":"#importing library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport scipy.stats as stats\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\nfrom sklearn.preprocessing import OneHotEncoder,OrdinalEncoder,StandardScaler,MinMaxScaler,PowerTransformer,FunctionTransformer\nfrom sklearn.model_selection import train_test_split,cross_val_score\nfrom sklearn.linear_model import LinearRegression,LassoCV,RidgeCV,ElasticNet\nfrom sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline ","8a6f6a8c":"#laod the data\ndf=pd.read_csv(\"..\/input\/car-price-prediction\/CarPrice_Assignment.csv\")\ndf.shape","2cfb8560":"#check the data\ndf.head()","dfff89a8":"df.info()","ef18c182":"#chicking the null value.\ndf.isnull().sum()","86dc026f":"df.describe()","9976214c":"#checking the correlation between the pair of columns\ndf.corr()","b9077955":"# graphycally represent the correaletion with heatmap \nplt.figure(figsize=(16,8))\nsns.heatmap(df.corr(),annot=True)\n","c2bd8c26":"#with the function we can select highly correlated  independent features \n#it will remove the first feature that is correlated with anything other feature \ndef correlation(df,threshold):\n    coll_corr=set() #set of all the names of correlated columns \n    corr_matrix=df.corr()\n    for i in range (len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i,j])> threshold:\n                colname=corr_matrix.columns[i] #getting the name of columns\n                coll_corr.add(colname)\n    return coll_corr           ","24ac3a18":"#calling the fuction\ncorr_feature = correlation(df.iloc[:,:-1],0.90)\nlen(set(corr_feature))","7f919eda":"#these are the highly correletd independent features ,we should remove this... \ncorr_feature","a808a49b":"#dropping the highly correlated features\ndf=df.drop(['highwaympg'],axis =1)\ndf.shape\n","9c7fd804":"#getting the categorical and numerical columns from df\nnumeric_col=[col for col in df if df[col].dtype !=\"object\" ]\ncategorical_col=[col for col in df if df[col].dtype==\"object\" ]","41227049":"numeric_col","6fb56ffc":"#removing car_id and price col \nnumeric_col.remove(\"car_ID\")\nnumeric_col.remove(\"price\")","14a664fd":"categorical_col","40aa6c3b":"#removing the carname columns \ncategorical_col.remove(\"CarName\")","4e7c84a7":"#analysis the categorical columns with count and box plot \n#using count plot we perform univariant analysis \n#using box plot we perform bivariant alalysis\ni=1\nplt.figure(figsize=(30,120))\nfor col in categorical_col:\n    plt.subplot(10,2,i)\n    sns.countplot(df[col])\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.xlabel(col,fontsize=25)\n    plt.ylabel(\"count\",fontsize=25)\n    \n    i=i+1\n    plt.subplot(10,2,i)\n    sns.boxplot(x=df[col],y=df[\"price\"])\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.xlabel(col,fontsize=25)\n    plt.ylabel(\"price\",fontsize=25)\n    i=i+1\n\nplt.show()","2ad196e0":"#analysis the numerical columns with dist and box plot \n\ni=1\nplt.figure(figsize=(30,120))\nfor col in numeric_col:\n    plt.subplot(14,2,i)\n    sns.distplot(df[col],color=\"r\")\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.xlabel(col,fontsize=25)\n    plt.ylabel(\"count\",fontsize=25)\n    \n    i=i+1\n    plt.subplot(13,2,i)\n    sns.boxplot(df[col],color=\"green\")\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.xlabel(col,fontsize=25)\n    i=i+1\n\nplt.show()","e15cc288":"#analysis the numerical col with scatter plot \ni=1\nplt.figure(figsize=(30,120))\nfor col in numeric_col:\n    plt.subplot(13,2,i)\n    sns.scatterplot(df[col],df[\"price\"],color=\"r\")\n    plt.xticks(fontsize=25)\n    plt.yticks(fontsize=25)\n    plt.xlabel(col,fontsize=25)\n    plt.ylabel(\"price\",fontsize=25)\n    \n    i=i+1\n\nplt.show()","8ea103c9":"sns.displot(df[\"price\"],color=\"r\",kde=True)","c7cd2627":"df[\"price\"].skew()","edfdc517":"#transform the price col using log transformer\nft=FunctionTransformer(func=np.log1p)\ndf[\"price\"]=ft.fit_transform(df[\"price\"])","8cf0e266":"#after transform the price col\nsns.displot(df[\"price\"],color=\"r\",kde=True)","1f68e0d0":"#get features and terget columns \nX=df[['symboling','wheelbase','carwidth','carheight','boreratio','stroke','compressionratio','horsepower','peakrpm',\n      'citympg','fueltype','aspiration','doornumber','carbody',\n 'drivewheel','enginelocation','enginetype','cylindernumber','fuelsystem','carlength','curbweight','enginesize']]\nY=df.price\nX.shape","d4106208":"#finding skewed columns using this function\ndef distribution(data):\n    #let find the skewed col and fix them\n    skew_limit=0.75 # limit for skewed col\n    skew_vals=data[numeric_col].skew()\n    skew_col=skew_vals[abs(skew_vals)>skew_limit].sort_values(ascending =False)\n    \n    #graphically represent the skewed col\n    i=1    \n    print(\"Columns names: \",skew_col.index)\n    print(skew_col)\n    plt.figure(figsize=(30,30))\n    for col in skew_col.index:\n        plt.subplot(5,2,i)\n        sns.distplot(X[col],color=\"r\")\n        plt.xticks(fontsize=25)\n        plt.yticks(fontsize=25)\n        plt.xlabel(col,fontsize=25)\n        i=i+1\n        \n\n    plt.show()","af8f95fc":"#calling the function\ndistribution(X)","0460f54b":"#fix the skewed columns using function transformer\n\nskew_col=['compressionratio','horsepower','wheelbase', 'carwidth',\"enginesize\"] #these two col perform well for power transform\nft=FunctionTransformer(func=np.log1p)\nX[skew_col]=ft.fit_transform(X[skew_col])\nX.head()","4201af00":"#Encoding the categorical columns into numerical columns \ncat_col=[col for col in X if X[col].dtype==\"object\" ] #get the cat col\nX=pd.get_dummies(X,columns=cat_col,drop_first=True) #using pandas function\n\n#scaling all the columns \nsc=StandardScaler()\nX=sc.fit_transform(X)\nX=pd.DataFrame(X)","ee3de74b":"#finally our features look like this ,take a look!!!!!!\nX","2fa5e0ed":"#train test split  \nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.2,random_state=42)\nprint(\"X_train shape: \",x_train.shape)\nprint(\"X_test shape: \",x_test.shape)\nprint(\"Y_train shape: \",y_train.shape)\nprint(\"Y_test shape: \",y_test.shape)\n\n","63f5f0a4":"#evaluate and disply the performance of models\ndef evaluate(model):\n    model.fit(x_train,y_train)\n    y_pred_m=model.predict(x_test) \n    \n    #(Y_pred_m) and (y_test) contain log transformed values not real valued price ..\n    #so we inversely apply log transform to get real price value...\n    ft=FunctionTransformer(func=np.log1p,inverse_func=np.exp)\n    inv_y_pred=ft.inverse_transform(y_pred_m)\n    inv_y_test=ft.inverse_transform(y_test)\n    \n    #printing the model name and accuracy !!!!!\n    print(\"Model name: \",model)\n    print(\"R2 score :--->>\",r2_score(inv_y_test,inv_y_pred))\n    print(\"MSE :--->>\",mean_squared_error(inv_y_test,inv_y_pred))\n    print(\"MAE :--->>\",mean_absolute_error(inv_y_test,inv_y_pred))\n    print(\"<<<<------------------------------------------------------------->>>>\")\n    ","d5e25af0":"#Initialize the models \nlr=LinearRegression() #Linear regressor\nlasso=LassoCV() #Lasso regression\nR=RidgeCV() #Ridge regression\nknn=KNeighborsRegressor() \nsvr=SVR() # support vector regressor\ndtr=DecisionTreeRegressor()\n\nmodels=[lr,lasso,R,knn,svr,dtr] #create a list of models \n\nfor model in models:\n    evaluate(model)\n    \n#It's show time !!!!!!!!!!!","eb88e2ed":"# lets apply our Ensamble models on this data\nrf=RandomForestRegressor(n_estimators=200,random_state=42)\nada=AdaBoostRegressor(random_state=42)\ngbr=GradientBoostingRegressor(random_state=42)\nxgb=XGBRegressor()\n\nmodls=[rf,ada,gbr,xgb] #list of ensamble models \nfor model in modls:\n    evaluate(model)\n    \n#It's show time ,Yaaaaaah","efae311c":"price col is right skewed ,we should fix it !!!!!!!","1fd9474a":"There are 5 skewed columns in features columns","e250b049":"# If you like it then plz consider a upvote for me ..Thank you and happy analysing !!!!!!!!!!","653f6c6c":"From all, the dicision tree regressor is the best model.great !!!!, our linear regressor done a great job ,90% r2_score it obtained..\nLasso and Ridge perform quit well ...  well done !!!!!","8d4c1e0c":"  we don't use the carname col in our analysis so we remove it !!!!!!\n  ","29c9c07e":"our features are ready to fit in the model ,all the step is done,great !!!!!!!!!!!!","5b64a54d":"Above all our best model is Random forest regressor,2nd best model is gradinat boosting regressor ,all the ensamble models are perform quit good ......","1ec05bce":"# Evaluate models with our Data","042c2444":"we remove the car_id col as we don't use it also we remove the price col,we analyes it leter !!!!!!","07424ed7":"# applying EDA for data analysis","b62a7769":"here we got some skewed col ,we want to tranformed this using power transformer or function transformer , we fixed it leter!!!!!\n\nlet's look at the price col","a7d69c7f":"No null values present .great !!!!!!"}}