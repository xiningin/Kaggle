{"cell_type":{"5bb1985e":"code","6d2ceeef":"code","6739e281":"code","b01f0c36":"code","d2390d0c":"code","d2c8b56c":"code","baecdbad":"code","e0023c39":"code","d69de432":"code","3e95e3c6":"code","06f1bad1":"code","274f86bd":"code","d9a2483d":"code","9e61fb9c":"code","be32c160":"code","573c06e8":"code","faf75922":"code","e1a9bf30":"code","e1535415":"code","b921ffd2":"code","c123cb98":"code","428da543":"code","bb053fab":"code","d420e205":"code","cfaf6097":"code","bf654f6c":"code","076dcfc8":"code","0343acc0":"code","033cba9a":"code","44df7ca4":"code","22a6f7be":"code","7ab7d42e":"code","268e4dbf":"markdown","c40725df":"markdown","278ce007":"markdown","25bcec39":"markdown","3f7c80d7":"markdown","dabd8f48":"markdown","db0cd0e9":"markdown","e1671d8c":"markdown","86734388":"markdown","02f5da32":"markdown","a5cbcafe":"markdown","32b59460":"markdown"},"source":{"5bb1985e":"!nvidia-smi","6d2ceeef":"# DON\"T USE WANDB, theres a bug with it handling distributed training\n#!pip uninstall -y wandb\n!pip install --upgrade wandb\n\n#INSTALLING WANDB ALTERNATIVE\n#!pip install comet_ml\n\n## INSTALLING TRANSFORMERS FROM SOURCE\n!pip install git+https:\/\/github.com\/huggingface\/transformers.git@master\n#!pip install --upgrade transformers\n\n!pip install --upgrade numpy tokenizers rouge_score tqdm\n!pip install datasets\n\n## INSTALLING PyTorch\/XLA using pip\n#!pip install cloud-tpu-client==0.10 https:\/\/storage.googleapis.com\/tpu-pytorch\/wheels\/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n\n## INSTALLING PyTorch\/XLA from source\n#!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n#!python pytorch-xla-env-setup.py --version 1.9 --apt-packages libomp5 libopenblas-dev\n\n#!pip install git+https:\/\/github.com\/pytorch\/pytorch.git@master\n#!pip install --upgrade torch\n\n#!pip install --upgrade deepspeed\n!pip install git+https:\/\/github.com\/microsoft\/DeepSpeed.git@master","6739e281":"from subprocess import run\nUSE_DEEPSPEED_OR_TPU = \"DEEPSPEED\" ## OPTIONS: \"DEEPSPEED\", \"TPU\", \"NONE\"\nPY_TPU_VERSION = \"1.10\"\nMONITORING_SYSTEM = \"Wandb\" ## OPTIONS: \"Wandb\", \"Cometml\"\n\n### INITIALIZING ENVIRONMENT VARIABLES FOR TPU OR DEEPSPEED:\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\nif USE_DEEPSPEED_OR_TPU==\"TPU\":\n    os.environ[\"XLA_USE_BF16\"] = \"1\"\n    \nelif USE_DEEPSPEED_OR_TPU==\"DEEPSPEED\":\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n    os.environ['RANK'] = \"0\"\n    os.environ['LOCAL_RANK'] = \"0\"\n    os.environ['WORLD_SIZE'] = \"1\"\n\nimport gc\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhuggingface_hub_token = user_secrets.get_secret(\"huggingface-hub-token\")\nos.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"wandb-key\")\nos.environ[\"COMET_API_KEY\"] = user_secrets.get_secret(\"comet-key\")\n\nif MONITORING_SYSTEM == \"Cometml\":\n    import comet_ml\nelse:\n    import wandb\nfrom tqdm.notebook import tqdm\nimport transformers\nimport tokenizers\n#import datasets\nimport tensorflow as tf\nimport glob\nimport numpy as np\nimport json\nimport pandas as pd\nfrom functools import partial\n\nfrom datasets import (\n    load_dataset,\n    load_metric,\n    load_from_disk,\n)\n\nfrom transformers import(\n    Trainer,\n    TrainingArguments,\n    is_torch_tpu_available,\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoModel,\n    DataCollatorWithPadding,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n)\nfrom transformers.modeling_outputs import Seq2SeqLMOutput\nfrom transformers.optimization import (\n    Adafactor,\n    AdafactorSchedule,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n)\n\nfrom dataclasses import dataclass, field\nimport nltk\nnltk.download(\"punkt\")\n\nimport torch\nfrom torch.utils.data import Dataset\nimport torch.nn as nn\n\nfrom kaggle_datasets import KaggleDatasets\n\nif is_torch_tpu_available():\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n    import torch_xla.utils.serialization as xser\nelse:\n    import deepspeed\n\nfrom typing import Dict, List, Optional\n\nfrom subprocess import run\n\n\nrouge_metric = load_metric(\"rouge\")","b01f0c36":"from platform import python_version\nprint(python_version())\nprint(tf.__version__)\nprint(transformers.__version__)\nprint(torch.__version__)\n\nif not is_torch_tpu_available():\n    print(deepspeed.__version__)","d2390d0c":"TPU_CORES=0\nN_GPUS=0\n\nif is_torch_tpu_available():\n    TPU_CORES = 8\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 32\n    GRAD_ACCU_STEPS = 1\n    FP16 = False\n    DS_Config=None\nelif USE_DEEPSPEED_OR_TPU==\"DEEPSPEED\":\n    N_GPUS = 1\n    TRAIN_BATCH_SIZE = 10\n    VALID_BATCH_SIZE = 10\n    GRAD_ACCU_STEPS = 1\n    FP16=True\n    DS_Config=\"zero3\"\n    #DS_Config=\"zero3_infinity\"\n    #DS_Config=\"zero2\"\nelse:\n    N_GPUS = 1\n    TRAIN_BATCH_SIZE = 4\n    VALID_BATCH_SIZE = 4\n    GRAD_ACCU_STEPS = 1\n    FP16=True\n    DS_Config=None\n    \nclass ConfigInitializer:\n    \n    ## DATA CONFIGS\n    use_fast_tokenizer=True\n    #prefix=\"summarize: \"\n    prefix=\"\"\n    \n    valid_test_tpu_support=True\n    train_required_columns=[\"input_ids\", \"attention_mask\",\n                            \"decoder_attention_mask\",\n                            \"labels\"]\n    eval_required_columns = [\"input_ids\", \"attention_mask\", \n                             \"decoder_attention_mask\",\n                             \"labels\"]\n    max_input_len=256\n    max_output_len=64\n    truncation=True\n    padding=\"max_length\"\n    return_tensors=\"pt\"\n    input_column=\"document\"\n    output_column=\"summary\"\n\n    train_batch_size=TRAIN_BATCH_SIZE\n    valid_batch_size=VALID_BATCH_SIZE\n    test_batch_size=4\n\n    drop_last_train=True\n    drop_last_valid=False\n    drop_last_test=False\n\n    num_workers_train=2\n    num_workers_valid=2\n    num_workers_test=2\n\n    datasets_main_dir=os.path.join(os.curdir, \"my_datasets\")\n    train_ds_path=os.path.join(datasets_main_dir, \"train_ds.pt\")\n    valid_ds_path=os.path.join(datasets_main_dir, \"valid_ds.pt\")\n    test_ds_path=os.path.join(datasets_main_dir, \"test_ds.pt\")\n    \n    ## TRAINING CONFIGS\n    max_steps=1000\n    optimizer_scale_parameter=False\n    optimizer_relative_step=False\n    optimizer_warmup_init=False\n    lr=1e-3\n    initial_lr=1e-3\n    epochs=1\n    logging_ratio = 0.01\n    warmup_ratio=0.1\n    saving_ratio = 0.2\n    eval_ratio = 0.1\n    total_train_steps=100\n    logging_steps=None\n    eval_steps=None\n    save_steps=None\n    save_strategy=\"no\"\n    evaluation_strategy=\"steps\"\n    output_dir=os.path.join(os.curdir, \"results\")\n    logging_dir=os.path.join(os.curdir, \"log\")\n    weight_decay=0.0\n    num_beams=4\n    skip_special_tokens_in_decode=True\n    use_stemmer_for_compute=True\n    adam_epsilon=1e-8\n    gradient_accumulation_steps=GRAD_ACCU_STEPS\n    n_gpu=N_GPUS\n    tpu_cores=TPU_CORES\n    seed=42\n    use_ada_factor=True\n    compute_metrics=False\n    fp16=FP16\n    rouge_before_training=False\n    limited_host_mem=False\n    use_ds_optimizer_scheduler=False\n    \n    def __init__(self, gcs_path, project_name, model_checkpoint,\n                 model_name,dataset_name, dataset_dir, csv_folder=None, train_len=None,\n                 valid_len=200, test_len=20, monitoring_system=\"Wandb\"):\n        self.dataset_name = dataset_name\n        self.project_name = project_name\n\n        if csv_folder:\n            self.train_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, csv_folder, f\"{dataset_name}_train.csv\")\n            self.valid_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, csv_folder, f\"{dataset_name}_valid.csv\")\n            self.test_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, csv_folder, f\"{dataset_name}_test.csv\")\n        else:\n            self.train_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, f\"{dataset_name}_train.csv\")\n            self.valid_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, f\"{dataset_name}_valid.csv\")\n            self.test_files = os.path.join(gcs_path, \"Sci_Summarizer_Data\", dataset_dir, f\"{dataset_name}_test.csv\")\n\n        self.model_checkpoint = model_checkpoint\n        self.model_name = model_name\n\n        self.xser_save_path=f\"{model_name.replace('-', '_')}_{dataset_name}\"\n        self.config_json_path=f\"{self.xser_save_path}.json\"\n        self.tokenizer_save_path=f\"{self.xser_save_path}_tokenizer\"\n\n        train_file_path = os.path.join(os.curdir,f\"{self.dataset_name}_train.csv\")\n        valid_file_path = os.path.join(os.curdir,f\"{self.dataset_name}_valid.csv\")\n        test_file_path  = os.path.join(os.curdir,f\"{self.dataset_name}_test.csv\")\n        \n        train_file_check = os.path.exists(train_file_path)\n        valid_file_check = os.path.exists(valid_file_path)\n        test_file_check = os.path.exists(test_file_path)\n        \n        file_check_list = [(self.train_files, train_file_check),\n                           (self.valid_files, valid_file_check),\n                           (self.test_files, test_file_check)]\n\n        for data_file, file_check in file_check_list:\n            if not file_check:\n                run([\"gsutil\", \"cp\", data_file, \".\"])\n\n        self.train_files = os.path.join(os.curdir,f\"{self.dataset_name}_train.csv\")\n        self.valid_files = os.path.join(os.curdir,f\"{self.dataset_name}_valid.csv\")\n        self.test_files = os.path.join(os.curdir,f\"{self.dataset_name}_test.csv\")\n        \n\n        self.train_len=train_len\n        self.valid_len=valid_len\n        self.test_len=test_len\n        self.total_train_step=None\n        \n        if DS_Config:\n            self.deepspeed_config = os.path.join(os.curdir, f\"ds_config_{DS_Config}.json\")\n        else:\n            self.deepspeed_config = DS_Config\n\n        if monitoring_system == \"Cometml\":\n            comet_ml.init(project_name=self.project_name)\n        else:\n            pass\n            #wandb.init(project=self.project_name)","d2c8b56c":"zero3_config_dict = {\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \n    \"zero_allow_untested_optimizer\": True,\n\n    \"\"\"\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n    \"\"\"\n    \"\"\"\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n    \"\"\"\n    \n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": False\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": True\n        },\n        \"overlap_comm\": False,\n        \"contiguous_gradients\": True,\n        \"sub_group_size\": 1e5,\n        \"allgather_bucket_size\": 1e8,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e5,\n        \"stage3_max_reuse_distance\": 1e5,\n        \"stage3_gather_fp16_weights_on_model_save\": True\n    },\n    \n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": False\n}\n\n\nwith open(\"ds_config_zero3.json\", \"w\") as config_file:\n    json.dump(zero3_config_dict, config_file)","baecdbad":"zero3_infinity_config_dict = {\n    \"fp16\": {\n        \"enabled\": \"auto\",\n        \"loss_scale\": 0,\n        \"loss_scale_window\": 1000,\n        \"initial_scale_power\": 16,\n        \"hysteresis\": 2,\n        \"min_loss_scale\": 1\n    },\n    \n    \"zero_allow_untested_optimizer\": True,\n    \n    \"zero_optimization\": {\n        \"stage\": 3,\n        \"offload_optimizer\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"\/local_nvme\",\n            \"pin_memory\": True,\n            \"buffer_count\": 4,\n            \"fast_init\": False\n        },\n        \"offload_param\": {\n            \"device\": \"nvme\",\n            \"nvme_path\": \"\/local_nvme\",\n            \"pin_memory\": True,\n            \"buffer_count\": 5,\n            \"buffer_size\": 1e8,\n            \"max_in_cpu\": 1e9\n        },\n        \"aio\": {\n            \"block_size\": 262144,\n            \"queue_depth\": 32,\n            \"thread_count\": 1,\n            \"single_submit\": False,\n            \"overlap_events\": True\n        },\n        \"overlap_comm\": True,\n        \"contiguous_gradients\": True,\n        \"sub_group_size\": 1e9,\n        \"reduce_bucket_size\": \"auto\",\n        \"stage3_prefetch_bucket_size\": \"auto\",\n        \"stage3_param_persistence_threshold\": \"auto\",\n        \"stage3_max_live_parameters\": 1e9,\n        \"stage3_max_reuse_distance\": 1e9,\n        \"stage3_gather_fp16_weights_on_model_save\": True\n    },\n\n    \"gradient_accumulation_steps\": \"auto\",\n    \"gradient_clipping\": \"auto\",\n    \"steps_per_print\": 2000,\n    \"train_batch_size\": \"auto\",\n    \"train_micro_batch_size_per_gpu\": \"auto\",\n    \"wall_clock_breakdown\": False\n}\n\nwith open(\"ds_config_zero3_infinity.json\", \"w\") as config_file:\n    json.dump(zero3_infinity_config_dict, config_file)","e0023c39":"GCS_PATH = KaggleDatasets().get_gcs_path(\"sci-summarizer-data\")\nprint(GCS_PATH)\n\nConfig = ConfigInitializer(project_name=\"t5_large_xsum\",\n                           monitoring_system=MONITORING_SYSTEM,\n                           #model_checkpoint=\"microsoft\/prophetnet-large-uncased-cnndm\",\n                           #model_checkpoint=\"google\/pegasus-cnn_dailymail\",\n                           #model_checkpoint=\"facebook\/bart-large-cnn\",\n                           model_checkpoint=\"t5-large\",\n                           #model_checkpoint=\"t5-base\",\n                           #model_checkpoint=\"t5-small\",\n                           \n                           #model_name=\"prophetnet\",\n                           #model_name=\"pegasus\",\n                           #model_name=\"bart-large\",\n                           #model_name=\"bart\",\n                           model_name=\"t5-large\",\n                           #model_name=\"t5-base\",\n                           #model_name=\"t5-small\",\n                           \n                           dataset_name=\"xsum\",\n                           gcs_path=GCS_PATH,\n                           dataset_dir=\"xsum_data\",\n                           train_len=None,\n                           valid_len=200,\n                           test_len=20)","d69de432":"def compute_rouge(model, tokenizer,\n                  tokenized_dataset,\n                  gen_kwargs=None,\n                  Config=Config):\n    \n    def postprocess_text(preds, labels):\n        preds = [pred.strip() for pred in preds]\n        labels = [label.strip() for label in labels]\n\n        # rougeLSum expects newline after each sentence\n        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n        return preds, labels\n    \n\n    for batch in tqdm(tokenized_dataset,\n                      total=len(tokenized_dataset),\n                      unit=\"batchs\"):\n        \n        if is_torch_tpu_available():\n            temp_batch = {\n                \"input_ids\": batch[\"input_ids\"],\n                \"attention_mask\": batch[\"attention_mask\"],\n            }\n        else:\n            temp_batch={\n                \"input_ids\": batch[\"input_ids\"].to(\"cuda\"),\n                \"attention_mask\": batch[\"attention_mask\"].to(\"cuda\"),\n            }\n            \n        labels = batch[\"labels\"]\n        temp_batch.update(gen_kwargs)\n        generated_tokens = model.generate(**temp_batch)\n\n        if isinstance(generated_tokens, tuple):\n            generated_tokens = generated_tokens[0]\n\n\n        decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=Config.skip_special_tokens_in_decode)\n        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=Config.skip_special_tokens_in_decode)\n        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n        rouge_metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n\n    result = rouge_metric.compute(use_stemmer=Config.use_stemmer_for_compute)\n\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n\n    result = {k: round(v, 4) for k, v in result.items()}\n    return result","3e95e3c6":"tokenizer = AutoTokenizer.from_pretrained(Config.model_checkpoint, use_fast=Config.use_fast_tokenizer)","06f1bad1":"class MyXSum(Dataset):\n    \n    def __init__(self, Config, tokenizer, main_ds, split_type):  \n        \n        self.model_name = Config.model_checkpoint\n        self.dataset = main_ds[split_type]\n        self.tokenizer = tokenizer\n        self.Config = Config\n        \n        if split_type in set([\"validation\", \"test\"]):\n            self.required_columns = Config.eval_required_columns\n            if split_type == \"validation\":\n                num_samples = Config.valid_len\n            else:\n                num_samples = Config.test_len\n        else:\n            self.required_columns = Config.train_required_columns\n            num_samples = Config.train_len\n            \n        if num_samples:\n            self.dataset = self.dataset.select(list(range(0, num_samples)))\n  \n    def __len__(self):\n        return self.dataset.shape[0]\n    \n    def preprocess_function(self, examples):\n\n        _inputs = [self.Config.prefix + examples[self.Config.input_column]]\n        _target = [examples[self.Config.output_column]]\n        \n        model_inputs = self.tokenizer(_inputs, max_length=self.Config.max_input_len,\n                                      truncation=self.Config.truncation, padding=self.Config.padding,\n                                      return_tensors=self.Config.return_tensors)\n\n        # Setup the tokenizer for targets\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(_target, max_length=self.Config.max_output_len,\n                                    truncation=self.Config.truncation, padding=self.Config.padding,\n                                    return_tensors=self.Config.return_tensors)\n            \n\n\n        model_inputs = {\n            \"input_ids\": model_inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": model_inputs[\"attention_mask\"].squeeze(),\n            \"decoder_input_ids\": labels[\"input_ids\"].squeeze(),\n            \"decoder_attention_mask\": labels[\"attention_mask\"].squeeze(),\n            \"labels\": labels[\"input_ids\"].squeeze(),\n        }\n        \n        model_inputs = {k: model_inputs[k] for k in self.required_columns}\n        \n        return model_inputs\n    \n  \n    def __getitem__(self, index):\n\n        return self.preprocess_function(self.dataset[index])","274f86bd":"def get_datasets(Config, main_dataset, tokenizer):\n    \n    train_ds = MyXSum(Config, tokenizer, main_dataset, \"train\")\n    valid_ds = MyXSum(Config, tokenizer, main_dataset, \"validation\")\n    test_ds = MyXSum(Config, tokenizer, main_dataset, \"test\")\n    \n    data_dict = {\n        \"train\": train_ds,\n        \"validation\": valid_ds,\n        \"test\": test_ds,\n    }\n    return data_dict ","d9a2483d":"xsum_ds = load_dataset(\"csv\", data_files={\"train\": Config.train_files,\n                                          \"validation\": Config.valid_files,\n                                          \"test\": Config.test_files})\n\nos.makedirs(Config.datasets_main_dir, exist_ok=True)\n\ndata_dict = get_datasets(Config, xsum_ds, tokenizer)\n\ntorch.save(data_dict[\"train\"], Config.train_ds_path)\ntorch.save(data_dict[\"validation\"], Config.valid_ds_path)\ntorch.save(data_dict[\"test\"], Config.test_ds_path)","9e61fb9c":"del xsum_ds\ndel data_dict\ngc.collect()","be32c160":"@dataclass\nclass T2TDataCollator(DataCollatorWithPadding):\n    \n    def collate_batch(self, batch: List) -> Dict[str, torch.Tensor]:\n        \"\"\"\n        Take a list of samples from a Dataset and collate them into a batch.\n        Returns:\n            A dictionary of tensors\n        \"\"\"\n        input_ids = torch.stack([example['input_ids'] for example in batch])\n        labels = torch.stack([example['decoder_input_ids'] for example in batch])\n        labels[labels[:, :] == 0] = -100\n        attention_mask = torch.stack([example['attention_mask'] for example in batch])\n        decoder_attention_mask = torch.stack([example['decoder_attention_mask'] for example in batch])\n        \n\n        return {\n            'input_ids': input_ids.squeeze(), \n            'attention_mask': attention_mask.squeeze(),\n            'labels': labels.squeeze(), \n            'decoder_attention_mask': decoder_attention_mask.squeeze()\n        }","573c06e8":"train_ds = torch.load(Config.train_ds_path)\nvalid_ds = torch.load(Config.valid_ds_path)\ntest_ds = torch.load(Config.test_ds_path)\n\nConfig.train_len = len(train_ds)\nConfig.valid_len = len(valid_ds)\nConfig.test_len = len(test_ds)","faf75922":"model_config  = AutoConfig.from_pretrained(Config.model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(Config.model_checkpoint, config=model_config)","e1a9bf30":"#with deepspeed.zero.Init():\nmodel_config  = AutoConfig.from_pretrained(Config.model_checkpoint)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(Config.model_checkpoint, config=model_config)\n    \nmodel.train()\n\nif is_torch_tpu_available():\n    WRAPPED_MODEL = xmp.MpModelWrapper(model)\nelif Config.deepspeed_config:\n    WRAPPED_MODEL = model\nelse:\n    WRAPPED_MODEL = model.to(\"cuda\")\n\ndata_collator = T2TDataCollator(tokenizer=tokenizer)","e1535415":"Config.epochs=4\n\nif not Config.total_train_steps:\n    Config.total_train_steps = (\n                (Config.train_len \/\/ (Config.train_batch_size * max(1, Config.tpu_cores if Config.tpu_cores else 0)))\n                \/\/ Config.gradient_accumulation_steps\n                * float(Config.epochs)\n            )\n\nprint(\"TOTAL TRAIN STEPS:\", Config.total_train_steps)\n\nConfig.eval_steps = int(Config.total_train_steps * Config.eval_ratio)\nprint(f\"Eval Ratio: {Config.eval_ratio}, Eval Steps: {Config.eval_steps}\")\n\nConfig.logging_steps = int(Config.total_train_steps * Config.logging_ratio)\nprint(f\"Logging Ratio: {Config.logging_ratio}, Logging Steps: {Config.logging_steps}\")\n\nConfig.save_steps = int(Config.total_train_steps * Config.saving_ratio)\nprint(f\"Save Ratio: {Config.saving_ratio}, Save Steps: {Config.save_steps}\")\n\nConfig.warmup_steps = int(Config.total_train_steps * Config.warmup_ratio)\nprint(f\"Warmup Ratio: {Config.warmup_ratio}, Warmup Steps: {Config.warmup_steps}\")\n\nConfig.optimizer_warmup_init=False","b921ffd2":"proxy_valid_ds = valid_ds\nproxy_test_ds = test_ds\n\nvalid_subset_len = Config.valid_batch_size * 3\ntest_subset_len = Config.test_batch_size * 2\n\n#if is_torch_tpu_available():\nproxy_valid_ds = torch.utils.data.Subset(valid_ds, range(valid_subset_len))\nproxy_test_ds = torch.utils.data.Subset(test_ds, range(test_subset_len))\n\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    proxy_valid_ds,\n    batch_size=Config.valid_batch_size,\n    drop_last=Config.drop_last_valid,\n    num_workers=Config.num_workers_valid,\n)\n\ntest_data_loader = torch.utils.data.DataLoader(\n    proxy_test_ds,\n    batch_size=Config.test_batch_size,\n    drop_last=Config.drop_last_test,\n    num_workers=Config.num_workers_test,\n)","c123cb98":"print(len(valid_data_loader))\nprint(len(test_data_loader))","428da543":"if Config.rouge_before_training:\n    gen_kwargs = {\n        \"num_beams\": Config.num_beams,\n        \"max_length\":Config.max_output_len,\n    }\n\n    valid_results = compute_rouge(model, tokenizer, valid_data_loader, gen_kwargs=gen_kwargs, Config=Config)\n    test_results = compute_rouge(model, tokenizer, test_data_loader, gen_kwargs=gen_kwargs, Config=Config)\n\n    print(\"VALID RESULTS: \", valid_results)\n    print(\"TEST RESULTS: \", test_results )\n\n    for idx, batch in enumerate(test_ds):\n\n        if is_torch_tpu_available():\n            temp_batch={\n                \"input_ids\": batch[\"input_ids\"].unsqueeze(0),\n                \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0),\n            }\n        else:\n            temp_batch={\n                \"input_ids\": batch[\"input_ids\"].unsqueeze(0).to(\"cuda\"),\n                \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0).to(\"cuda\"),\n            }\n\n        labels = batch[\"labels\"]\n\n        temp_batch.update(gen_kwargs)\n\n        output_ids = model.generate(**temp_batch)\n        decoded_input=tokenizer.decode(temp_batch[\"input_ids\"].squeeze(), skip_special_tokens=False)\n        decoded_output = tokenizer.decode(output_ids.squeeze(), skip_special_tokens=False)\n        decoded_labels = tokenizer.decode(labels.squeeze(), skip_special_tokens=False)\n\n        print(f\"{'-'*60}Example-{idx}{'-'*60}\")\n        print(\"INPUT: \", decoded_input[:400], \".\"*10, decoded_input[-100:], \"\\n\")\n        print(\"LABEL: \", decoded_labels, \"\\n\")\n        print(\"PREDICTION: \", decoded_output, \"\\n\")\n        print(\"\\n\")\n\n        if idx == 3:\n            break","bb053fab":"from transformers.file_utils import (\n    is_sagemaker_mp_enabled,\n)\nfrom transformers.trainer_utils import (\n    ShardedDDPOption,\n)\nfrom transformers.deepspeed import (\n    is_deepspeed_zero3_enabled\n)\nfrom transformers.modeling_utils import (\n    PreTrainedModel,\n    unwrap_model\n)\n\nclass TPUTrainer(Trainer):\n    \n    def __init__(self, limited_host_mem=None, **kwargs):\n        super().__init__(**kwargs)\n        self.limited_host_mem=limited_host_mem\n        \n    def _tune_save_checkpoint(self):\n        from ray import tune\n\n        if not self.use_tune_checkpoints:\n            return\n        with tune.checkpoint_dir(step=self.state.global_step) as checkpoint_dir:\n            output_dir = os.path.join(checkpoint_dir, f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\")\n            self.save_model(output_dir)\n            if self.args.should_save:\n                self.state.save_to_json(os.path.join(output_dir, \"trainer_state.json\"))\n                torch.save(self.optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                torch.save(self.lr_scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n    \n    def save_model(self, output_dir: Optional[str] = None, limited_host_mem=False):\n        \"\"\"\n        Will save the model, so you can reload it using :obj:`from_pretrained()`.\n\n        Will only save from the main process.\n        \"\"\"\n        if self.limited_host_mem:\n            limited_host_mem=self.limited_host_mem\n\n        if output_dir is None:\n            output_dir = self.args.output_dir\n\n        if is_torch_tpu_available():\n            self._save_tpu(output_dir, limited_host_mem)\n        elif is_sagemaker_mp_enabled():\n            # Calling the state_dict needs to be done on the wrapped model and on all processes.\n            state_dict = self.model_wrapped.state_dict()\n            if self.args.should_save:\n                self._save(output_dir, state_dict=state_dict)\n        elif (\n            ShardedDDPOption.ZERO_DP_2 in self.args.sharded_ddp or ShardedDDPOption.ZERO_DP_3 in self.args.sharded_ddp\n        ):\n            state_dict = self.model.state_dict()\n\n            if self.args.should_save:\n                self._save(output_dir, state_dict=state_dict)\n        elif self.deepspeed:\n\n            # this takes care of everything as long as we aren't under zero3\n            if self.args.should_save:\n                self._save(output_dir)\n\n            if is_deepspeed_zero3_enabled():\n                # It's too complicated to try to override different places where the weights dump gets\n                # saved, so since under zero3 the file is bogus, simply delete it. The user should\n                # either user deepspeed checkpoint to resume or to recover full weights use\n                # zero_to_fp32.py stored in the checkpoint.\n                if self.args.should_save:\n                    file = os.path.join(output_dir, WEIGHTS_NAME)\n                    if os.path.isfile(file):\n                        # logger.info(f\"deepspeed zero3: removing {file}, see zero_to_fp32.py to recover weights\")\n                        os.remove(file)\n\n                # now save the real model if stage3_gather_fp16_weights_on_model_save=True\n                # if false it will not be saved.\n                # This must be called on all ranks\n                self.deepspeed.save_fp16_model(output_dir, WEIGHTS_NAME)\n\n        elif self.args.should_save:\n            self._save(output_dir)\n    \n    def _save_tpu(self, output_dir: Optional[str] = None, limited_host_mem=False):\n        output_dir = output_dir if output_dir is not None else self.args.output_dir\n        logger.info(f\"Saving model checkpoint to {output_dir}\")\n\n        if xm.is_master_ordinal():\n            os.makedirs(output_dir, exist_ok=True)\n            torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n\n        # Save a trained model and configuration using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        xm.rendezvous(\"saving_checkpoint\")\n        if not isinstance(self.model, PreTrainedModel):\n            if isinstance(unwrap_model(self.model), PreTrainedModel):\n                unwrap_model(self.model).save_pretrained(\n                    output_dir,\n                    save_config=self.args.should_save,\n                    state_dict=self.model.state_dict(),\n                    save_function=xm.xser if limited_host_mem else xm.save,\n                )\n            else:\n                logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n                state_dict = self.model.state_dict()\n                if limited_host_mem:\n                    xm.xser(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n                else:\n                    xm.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n        else:\n            self.model.save_pretrained(output_dir, save_config=self.args.should_save,\n                                       save_function=xm.xser if limited_host_mem else xm.save)\n        if self.tokenizer is not None and self.args.should_save:\n            self.tokenizer.save_pretrained(output_dir)\n\n    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n        # If we are executing this function, we are the process zero, so we don't check for that.\n        output_dir = output_dir if output_dir is not None else self.args.output_dir\n        os.makedirs(output_dir, exist_ok=True)\n        logger.info(f\"Saving model checkpoint to {output_dir}\")\n        # Save a trained model and configuration using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        if not isinstance(self.model, PreTrainedModel):\n            if isinstance(unwrap_model(self.model), PreTrainedModel):\n                if state_dict is None:\n                    state_dict = self.model.state_dict()\n                unwrap_model(self.model).save_pretrained(output_dir, state_dict=state_dict)\n            else:\n                logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n                if state_dict is None:\n                    state_dict = self.model.state_dict()\n                torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n        else:\n            self.model.save_pretrained(output_dir, state_dict=state_dict)\n        if self.tokenizer is not None:\n            self.tokenizer.save_pretrained(output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))","d420e205":"def compute_metrics(eval_pred):\n    rouge_metric = load_metric(\"rouge\")\n    predictions, labels = eval_pred\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    # Replace -100 in the labels as we can't decode them.\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # Rouge expects a newline after each sentence\n    \n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n    \n    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n    # Extract a few results\n    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n    \n    # Add mean generated length\n    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n    result[\"gen_len\"] = np.mean(prediction_lens)\n    \n    del predictions\n    del decoded_preds\n    del labels\n    del decoded_labels\n    del eval_pred\n    del prediction_lens\n    del rouge_metric\n    gc.collect()\n    \n    return {k: round(v, 4) for k, v in result.items()}","cfaf6097":"def get_trainer(Config=Config, optimizer=None, lr_scheduler=None):\n    training_args = TrainingArguments(\n        max_steps=Config.total_train_steps,\n        output_dir=Config.output_dir,\n        num_train_epochs=Config.epochs,\n        save_strategy=Config.save_strategy,\n        evaluation_strategy=Config.evaluation_strategy,\n        logging_dir=Config.logging_dir,\n        eval_steps=Config.eval_steps,\n        save_steps=Config.save_steps,\n        logging_steps=Config.logging_steps,\n        per_device_train_batch_size=Config.train_batch_size,\n        per_device_eval_batch_size=Config.valid_batch_size,\n        fp16=Config.fp16,\n        deepspeed=Config.deepspeed_config,\n        adafactor=Config.use_ada_factor\n    )\n    \n    if is_torch_tpu_available():\n        trainer = TPUTrainer(\n            limited_host_mem=Config.limited_host_mem,\n            model=model,\n            args=training_args,\n            compute_metrics=compute_metrics if Config.compute_metrics else None,\n            data_collator=data_collator,\n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            optimizers=(optimizer, lr_scheduler),    \n        )\n    else:\n        trainer = Trainer(\n            model=model,\n            args=training_args,\n            compute_metrics=compute_metrics if Config.compute_metrics else None,\n            data_collator=data_collator,\n            train_dataset=train_ds,\n            eval_dataset=valid_ds,\n            optimizers=(optimizer, lr_scheduler),\n        )\n    return trainer\n\n\ndef get_optimizer_n_scheduler(model, Config=Config):\n    \n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    \n    grouped_params = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": Config.weight_decay,\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        }\n    ]\n\n    optimizer = Adafactor(grouped_params, scale_parameter=Config.optimizer_scale_parameter, \n                          relative_step=Config.optimizer_relative_step, warmup_init=Config.optimizer_warmup_init,\n                          lr=Config.lr)\n    lr_scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                   num_training_steps=Config.total_train_steps,\n                                                   num_warmup_steps =Config.warmup_steps)\n\n    return optimizer, lr_scheduler","bf654f6c":"def _mp_fn_boiler_plate(index, Config=Config):\n    \n    device = xm.xla_device()\n    model = WRAPPED_MODEL.to(device)\n\n    optimizer, lr_scheduler = get_optimizer_n_scheduler(model, Config)\n    \n    print(\"Loading datasets... \", end=\"\")\n\n    trainer = get_trainer(Config, optimizer, lr_scheduler)\n    trainer.place_model_on_device = False\n    trainer.train()\n\n    xser.save(trainer.model.state_dict(), Config.xser_save_path)\n    trainer.model.config.to_json_file(Config.config_json_path)\n    tokenizer.save_pretrained(Config.tokenizer_save_path)   ","076dcfc8":"run([\"rm\", \"-rf\", Config.xser_save_path])","0343acc0":"Config.epochs=4\n\nif not Config.total_train_steps:\n    Config.total_train_steps = (\n                (Config.train_len \/\/ (Config.train_batch_size * max(1, Config.tpu_cores if Config.tpu_cores else 0)))\n                \/\/ Config.gradient_accumulation_steps\n                * float(Config.epochs)\n            )\n\nprint(\"TOTAL TRAIN STEPS:\", Config.total_train_steps)\n\nConfig.eval_steps = int(Config.total_train_steps * Config.eval_ratio)\nprint(f\"Eval Ratio: {Config.eval_ratio}, Eval Steps: {Config.eval_steps}\")\n\nConfig.logging_steps = int(Config.total_train_steps * Config.logging_ratio)\nprint(f\"Logging Ratio: {Config.logging_ratio}, Logging Steps: {Config.logging_steps}\")\n\nConfig.save_steps = int(Config.total_train_steps * Config.saving_ratio)\nprint(f\"Save Ratio: {Config.saving_ratio}, Save Steps: {Config.save_steps}\")\n\nConfig.warmup_steps = int(Config.total_train_steps * Config.warmup_ratio)\nprint(f\"Warmup Ratio: {Config.warmup_ratio}, Warmup Steps: {Config.warmup_steps}\")\n\nConfig.optimizer_warmup_init=False","033cba9a":"Config.compute_metrics = False\n\nif is_torch_tpu_available():\n    _mp_fn = partial(_mp_fn_boiler_plate, Config=Config)\n    xmp.spawn(_mp_fn, start_method=\"fork\")\n\n    \"\"\"\n    ckpts_path = os.path.join(os.curdir, \"results\")\n    model_ckpts = os.listdir(ckpts_path)\n    ckpt_nums = [int(ckpt[11:]) for ckpt in model_ckpts]\n    best_ckpt_num = max(ckpt_nums)\n    best_ckpt_path = os.path.join(ckpts_path, f\"checkpoint-{best_ckpt_num}\")\n    tokenizer.save_pretrained(best_ckpt_path)\n    \"\"\"\n\nelse:\n    if Config.deepspeed_config and Config.use_ds_optimizer_scheduler:\n        optimizer = None\n        lr_scheduler = None\n    else:\n        optimizer, lr_scheduler = get_optimizer_n_scheduler(WRAPPED_MODEL, Config)\n        #if Config.deepspeed_config\n        \n    #print(optimizer)\n    trainer = get_trainer(Config, optimizer, lr_scheduler)\n    #print(trainer.optimizer)\n    trainer.train()\n    \n    #ckpts_path = os.path.join(os.curdir, \"results\")\n    #model_ckpts = os.listdir(ckpts_path)\n    #ckpt_nums = [int(ckpt[11:]) for ckpt in model_ckpts]\n    #best_ckpt_num = max(ckpt_nums)\n    #best_ckpt_path = os.path.join(ckpts_path, f\"checkpoint-{best_ckpt_num}\")\n    #tokenizer.save_pretrained(best_ckpt_path)","44df7ca4":"if is_torch_tpu_available():\n    torch.cuda.empty_cache()","22a6f7be":"if is_torch_tpu_available():\n    tokenizer = AutoTokenizer.from_pretrained(Config.tokenizer_save_path,\n                                              local_files_only=True,\n                                              use_fast=Config.use_fast_tokenizer)\n    \n    model = AutoModelForSeq2SeqLM.from_pretrained(Config.model_checkpoint)\n    state_dict = xser.load(Config.xser_save_path)\n    model.load_state_dict(state_dict)\nelse:\n    ckpts_path = os.path.join(os.curdir, \"results\")\n    model_ckpts = os.listdir(ckpts_path)\n    ckpt_nums = [int(ckpt[11:]) for ckpt in model_ckpts]\n    best_ckpt_num = max(ckpt_nums)\n    best_ckpt_path = os.path.join(ckpts_path, f\"checkpoint-{best_ckpt_num}\")\n    tokenizer.save_pretrained(best_ckpt_path)\n    model = AutoModelForSeq2SeqLM.from_pretrained(best_ckpt_path, local_files_only=True)\n    model = model.to(\"cuda\")","7ab7d42e":"gen_kwargs = {\n    \"num_beams\": Config.num_beams,\n    \"max_length\":Config.max_output_len,\n}\n\nvalid_results = compute_rouge(model, tokenizer, valid_data_loader, gen_kwargs=gen_kwargs, Config=Config)\ntest_results = compute_rouge(model, tokenizer, test_data_loader, gen_kwargs=gen_kwargs, Config=Config)\n\nprint(\"VALID RESULTS: \", valid_results)\nprint(\"TEST RESULTS: \", test_results)\n\nfor idx, batch in enumerate(test_ds):\n    \n    if is_torch_tpu_available():\n        temp_batch={\n            \"input_ids\": batch[\"input_ids\"].unsqueeze(0),\n            \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0),\n        }\n    else:\n        temp_batch={\n            \"input_ids\": batch[\"input_ids\"].unsqueeze(0).to(\"cuda\"),\n            \"attention_mask\": batch[\"attention_mask\"].unsqueeze(0).to(\"cuda\"),\n        }\n        \n    labels = batch[\"labels\"]\n    \n    temp_batch.update(gen_kwargs)\n\n    output_ids = model.generate(**temp_batch)\n    decoded_input=tokenizer.decode(temp_batch[\"input_ids\"].squeeze(), skip_special_tokens=False)\n    decoded_output = tokenizer.decode(output_ids.squeeze(), skip_special_tokens=False)\n    decoded_labels = tokenizer.decode(labels.squeeze(), skip_special_tokens=False)\n\n    print(f\"{'-'*60}Example-{idx}{'-'*60}\")\n    print(\"INPUT: \", decoded_input[:400], \".\"*10, decoded_input[-100:], \"\\n\")\n    print(\"LABEL: \", decoded_labels, \"\\n\")\n    print(\"PREDICTION: \", decoded_output, \"\\n\")\n    print(\"\\n\")\n\n    if idx == 3:\n        break","268e4dbf":"    ## If you don't want to use AdaFactor then, copy the below dict into config json\n    \n    \"optimizer\": {\n        \"type\": \"AdamW\",\n        \"params\": {\n            \"lr\": \"auto\",\n            \"betas\": \"auto\",\n            \"eps\": \"auto\",\n            \"weight_decay\": \"auto\"\n        }\n    },\n\n    \"scheduler\": {\n        \"type\": \"WarmupLR\",\n        \"params\": {\n            \"warmup_min_lr\": \"auto\",\n            \"warmup_max_lr\": \"auto\",\n            \"warmup_num_steps\": \"auto\"\n        }\n    },\n    \n    ## And remove the below line from config json:\n    \n    \"zero_allow_untested_optimizer\": true","c40725df":"### Rouge Before Training:","278ce007":"### Zero3-infinity:","25bcec39":"### Rouge After Training:","3f7c80d7":"# Initializing the Model: ","dabd8f48":"# Initializing the tokenizer and tokenizing the data:","db0cd0e9":"## DeepSpeed Config files:","e1671d8c":"    from prettytable import PrettyTable\n\n    def count_parameters(model):\n        table = PrettyTable([\"Modules\", \"Parameters\"])\n        total_params = 0\n        for name, parameter in model.named_parameters():\n            if not parameter.requires_grad: continue\n            param = parameter.numel()\n            table.add_row([name, param])\n            total_params+=param\n        print(table)\n        print(f\"Total Trainable Params: {total_params}\")\n        return total_params\n\n    count_parameters(model)","86734388":"### My Custom TPU Trainer which implements serialized saving when limited host memory available.","02f5da32":"    zero2_config_dict = {\n        \"fp16\": {\n            \"enabled\": \"auto\",\n            \"loss_scale\": 0,\n            \"loss_scale_window\": 1000,\n            \"initial_scale_power\": 16,\n            \"hysteresis\": 2,\n            \"min_loss_scale\": 1\n        },\n\n        \"zero_allow_untested_optimizer\": True,\n\n        \"\"\"\n        \"optimizer\": {\n            \"type\": \"AdamW\",\n            \"params\": {\n                \"lr\": \"auto\",\n                \"betas\": \"auto\",\n                \"eps\": \"auto\",\n                \"weight_decay\": \"auto\"\n            }\n        },\n        \"\"\"\n        \"\"\"\n        \"scheduler\": {\n            \"type\": \"WarmupLR\",\n            \"params\": {\n                \"warmup_min_lr\": \"auto\",\n                \"warmup_max_lr\": \"auto\",\n                \"warmup_num_steps\": \"auto\"\n            }\n        },\n        \"\"\"\n\n        \"zero_optimization\": {\n            \"stage\": 2,\n            \"\"\"\n            \"offload_optimizer\": {\n                \"device\": None,\n                \"pin_memory\": True\n            },\n            \"\"\"\n            \"allgather_partitions\": True,\n            \"allgather_bucket_size\": 5e8,\n            \"overlap_comm\": True,\n            \"reduce_scatter\": True,\n            \"reduce_bucket_size\": 5e8,\n            \"contiguous_gradients\": True\n        },\n\n        \"gradient_accumulation_steps\": \"auto\",\n        \"gradient_clipping\": \"auto\",\n        \"steps_per_print\": 2000,\n        \"train_batch_size\": \"auto\",\n        \"train_micro_batch_size_per_gpu\": TRAIN_BATCH_SIZE,\n        \"wall_clock_breakdown\": False\n    }\n\n    with open(\"ds_config_zero2.json\", \"w\") as config_file:\n        json.dump(zero2_config_dict, config_file)","a5cbcafe":"### Zero2:","32b59460":"        \"offload_optimizer\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },\n        \"offload_param\": {\n            \"device\": \"cpu\",\n            \"pin_memory\": true\n        },"}}