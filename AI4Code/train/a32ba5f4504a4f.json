{"cell_type":{"ea5c13a5":"code","fab109b1":"code","1fc2c2a6":"code","4742cdf9":"code","3dc5f35f":"code","e0deb281":"code","358de08c":"code","224e82a4":"code","b41a04a0":"code","1eab9670":"code","af7dbb5c":"code","46ad1d0a":"code","ffe56e2c":"code","a7c61da9":"code","a06be5da":"code","644dc35a":"code","aaf77561":"code","43aaed80":"code","10960430":"code","02162454":"code","7775b2bd":"code","447f76b0":"code","5f7a11c5":"code","9ccf9143":"code","514cd816":"code","cbfe2c27":"code","238bdc42":"code","58a1be0c":"code","0491e37f":"code","fb9c1907":"code","f6448a20":"code","49608ef6":"code","5071aa42":"code","76815303":"code","68710c31":"code","6e435f1c":"code","ab7e4ae3":"code","3ebf032b":"code","26df5183":"markdown","faa26592":"markdown","3290cc65":"markdown","9a87b546":"markdown","376d743c":"markdown","ef979773":"markdown","2b95e72f":"markdown","e2946de4":"markdown","71bac57e":"markdown","51bd67ac":"markdown","aa461eff":"markdown","9c48c287":"markdown"},"source":{"ea5c13a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fab109b1":"import os\nos.getcwd()","1fc2c2a6":"import sqlite3\ncon = sqlite3.connect('..\/input\/database.sqlite')","4742cdf9":"import pandas as pd\nfiltered_data = pd.read_sql_query('''SELECT * FROM Reviews WHERE Score != 3''', con)\ntype(filtered_data)\nfiltered_data.head()","3dc5f35f":"def posneg(x):\n    if x<3:\n        return \"Negative\"\n    return \"Positive\"\nactual_score = filtered_data['Score']\nPosNeg = list(map(posneg, actual_score))\nfiltered_data['Score'] = PosNeg\nfiltered_data.shape","e0deb281":"filtered_data.head()","358de08c":"filtered_data[filtered_data.isnull().any(axis = 1)]","224e82a4":"dup_data = pd.read_sql_query('''SELECT * FROM Reviews WHERE Score != 3 AND UserId = 'AR5J8UI46CURR' ORDER BY ProductId''', con)\ndup_data","b41a04a0":"filtered = filtered_data.sort_values(by = 'ProductId', ascending = True, axis = 0, inplace = False)\nfiltered.head()","1eab9670":"filtered.tail()\nfiltered.shape","af7dbb5c":"final_data= filtered_data.drop_duplicates(subset = {'UserId', 'ProfileName', 'Time', 'Text'}, keep = 'first', inplace = False)\nfinal_data.shape","46ad1d0a":"(final_data['Id'].size\/filtered_data['Id'].size) * 100","ffe56e2c":"final_data.head()","a7c61da9":"final_data[final_data.HelpfulnessNumerator > final_data.HelpfulnessDenominator]","a06be5da":"final = final_data[final_data.HelpfulnessNumerator <= final_data.HelpfulnessDenominator]\nfinal.shape","644dc35a":"final['Score'].value_counts()","aaf77561":"import re\ni = 0\ncount = 0\nfor sent in final['Text'].values:\n    if(re.findall('<.*?>', sent)):\n        print(i)\n        print(sent)\n        break\n    i +=1\n           \n   \n    ","43aaed80":"def cleanHtml(sentence):\n    cleaner = re.compile('<.*?>')\n    clean_html = re.sub(cleaner, ' ', sentence)\n    return clean_html\ndef cleanPunc(sentence):\n    cleaned = re.sub(r'[?|!|\\\\]',r'',sentence)\n    cleaned = re.sub(r'[.|,|)|(|\\\\|\/]',r' ',cleaned)\n    return cleaned \nimport string\nimport nltk                     \nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nstop = set(stopwords.words('english'))\nsno = nltk.stem.SnowballStemmer('english')\nprint(stop)\nprint(sno.stem('tasty'))\n                     ","10960430":"i = 0\nstr1 = ' '\nfinal_string = []\nall_positive_words = []\nall_negative_words = []\ns = ''\nfor sent in final['Text'].values:\n    filtered_sentence = []\n    sent = cleanHtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanPunc(w).split():\n            if ((cleaned_words.isalpha()) & (len(cleaned_words) > 2)):\n                if (cleaned_words.lower() not in stop):\n                    s = (sno.stem(cleaned_words.lower())).encode('utf8')\n                    filtered_sentence.append(s)\n                    if(final['Score'].values)[i] == 'Positive':\n                        all_positive_words.append(s)\n                    if(final['Score'].values)[i] == 'Negative': \n                        all_negative_words.append(s)\n                else:\n                    continue\n            else:\n                continue\n    str1 = b\" \".join(filtered_sentence)\n    final_string.append(str1)\n    i += 1\n","02162454":"final['Cleaned_text'] = final_string\nfinal['Cleaned_text'] = final['Cleaned_text'].str.decode('utf-8')\nfinal.head(3)","7775b2bd":"final['Cleaned_text'].values","447f76b0":"from sklearn.feature_extraction.text import CountVectorizer\ncount_vect = CountVectorizer()\nfinal_counts1 = count_vect.fit(final['Cleaned_text'].values)\ntype(final_counts1)","5f7a11c5":"final_counts = count_vect.transform(final['Cleaned_text'].values)\ntype(final_counts)","9ccf9143":"final_counts.shape","514cd816":"freq_dist_pos = nltk.FreqDist(all_positive_words)\nfreq_dist_neg = nltk.FreqDist(all_negative_words)\nprint(\"Most Common Positive words\", freq_dist_pos.most_common(20))\nprint(\"Most Common Negative words\", freq_dist_neg.most_common(20))","cbfe2c27":"print(stop)\nstop.remove('not')\nprint('*' * 50)\nprint(stop)\n#stop1.remove('not')\ni = 0\nstr2 = ' '\nfinal_string1 = []\nall_positive_words1 = []\nall_negative_words1 = []\ns1 = ''\nfor sent1 in final['Text'].values:\n    filtered_sentence1 = []\n    sent1 = cleanHtml(sent1)\n    for w1 in sent1.split():\n        for cleaned_words1 in cleanPunc(w1).split():\n            if ((cleaned_words1.isalpha()) & (len(cleaned_words1) > 2)):\n                if (cleaned_words1.lower() not in stop):\n                    s1 = (sno.stem(cleaned_words1.lower())).encode('utf8')\n                    filtered_sentence1.append(s1)\n                    if(final['Score'].values)[i] == 'Positive':\n                        all_positive_words1.append(s1)\n                    if(final['Score'].values)[i] == 'Negative': \n                        all_negative_words1.append(s1)\n                else:\n                    continue\n            else:\n                continue\n    str2 = b\" \".join(filtered_sentence1)\n    final_string1.append(str2)\n    i += 1    ","238bdc42":"final['cleaned_not'] = final_string1\nfinal['cleaned_not'] = final['cleaned_not'].str.decode('utf-8')\nprint(final.shape)\nprint(final['cleaned_not'].values)","58a1be0c":"freq_dist_pos1 = nltk.FreqDist(all_positive_words1)\nfreq_dist_neg1 = nltk.FreqDist(all_negative_words1)\nprint(\"Most Common Positive words with 'not' stopwrd\", freq_dist_pos1.most_common(20))\nprint(\"Most Common Negative words with 'not' stopwrd\", freq_dist_neg1.most_common(20))","0491e37f":"# BiGrams\ncount_vect = CountVectorizer(ngram_range = (1,2))\nfinal_counts_bigrams = count_vect.fit_transform(final['cleaned_not'].values)\nprint(final_counts_bigrams.shape)\nprint(type(final_counts_bigrams))","fb9c1907":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_count = TfidfVectorizer(ngram_range = (1,2))\nfinal_counts_tfidf = tfidf_count.fit_transform(final['cleaned_not'].values)\nprint(final_counts_tfidf.shape)\nprint(type(final_counts_tfidf))","f6448a20":"# To get feature names\nfeatures = tfidf_count.get_feature_names()\nprint(len(features))\nprint(features[100000:100010])\n","49608ef6":"# Train your own Word2Vec model using your own text corpus\nimport gensim\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nlist_of_sent=[]\nfor sent in final['Cleaned_text'].values:\n    list_of_sent.append(sent.split())\nprint(final['Cleaned_text'].values[0])\nprint(\"*****************************************************************\")\nprint(list_of_sent[0])","5071aa42":"# min_count = 5 considers only words that occured atleast 5 times\nW2V_model = Word2Vec(list_of_sent, min_count = 5, size = 50, workers = 4)\nw2v_words = list(W2V_model.wv.vocab)\nprint(\"Number of words that occured minimum 5 times\", len(w2v_words))\nprint(\"Sample words\", w2v_words[0:25])\n","76815303":"W2V_model.wv.most_similar('tasti')","68710c31":"count_vect_feat = count_vect.get_feature_names()\nprint(count_vect_feat[count_vect_feat.index('like')])","6e435f1c":"import numpy as np\nsent_vectors = []\nfor sent in list_of_sent:\n    sent_vec = np.zeros(50)\n    cnt_words = 0\n    for word in sent:\n        if word in w2v_words:\n            vec = W2V_model.wv[word]\n            sent_vec += vec\n            cnt_words +=1\n    if cnt_words != 0:\n        sent_vec \/= cnt_words\n    sent_vectors.append(sent_vec)\nprint(len(sent_vectors))\nprint(len(sent_vec))\nprint(sent_vectors[0])","ab7e4ae3":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_count1 = TfidfVectorizer()\nfinal_counts1_tfidf = tfidf_count1.fit_transform(final['Cleaned_text'].values)\nprint(final_counts1_tfidf.shape)                                   ","3ebf032b":"import numpy as np\ntfidf_feat = tfidf_count1.get_feature_names()\ntype(tfidf_feat)\ntfidf_sent_vectors = []\nrow = 0\nfor sent in list_of_sent[:100]:\n    sent_vec = np.zeros(50)\n    weighted_sum = 0\n    for word in sent:\n        if word in w2v_words:\n            vec = W2V_model.wv[word]\n            tf_idf = final_counts1_tfidf[row, tfidf_feat.index(word)]\n            sent_vec = sent_vec + (vec * tf_idf)\n            weighted_sum += tf_idf\n    if weighted_sum != 0:\n        sent_vec \/= weighted_sum\n    tfidf_sent_vectors.append(sent_vec)\n    row += 1\nprint(len(tfidf_sent_vectors))\nprint(len(sent_vec))\nprint(tfidf_sent_vectors[0])  \n","26df5183":"Data Mining\n1. Remove HTML tags ","faa26592":"TFIDF Vectorizer \n1. TF increases if a word is present more frequemtly in a review\/text. It repersents the probability of occurence of word in a review.\n2. IDF increases if a word is most rarely used in the corpus of reviews.","3290cc65":"**Word2vec**\nIt will convert each word in to a 300 dimensional vector.","9a87b546":"Need to remove the above two rows where Helpfulness Numerator > helpfulness Denominator","376d743c":"Number of Unique words = 68,984","ef979773":"**Avg W2V***\n1. Calculate vector for each word in the sentence and sum up all the word vectors of a sentence and divide it by number of words for that sentence present in word corpus.","2b95e72f":"Code for implementing step-by-step the checks mentioned in the pre-processing phase","e2946de4":"Note: Using bigrams number of features has increased to 2816967.","71bac57e":"Vectorization Techniques:\n1. Bag Of Words(BOW)\n   --> Create an instance for CountVectorizer\n   --> Fit will learn Vocabulay to fit  from unique word corpus of all the review data\n   --> Trnasform will create a Sparse matrix by counting tokens from raw documents fitted to the fot function ","51bd67ac":"**TFIDF Weighted Word2Vec**","aa461eff":"Observation:\nHere the most common words for Positve and Negative reviews are 'like', 'good', 'taste'. Actually for negative it should be prefixed with 'not'. But as the text is cleaned by removing Stop words in which 'not' is one of the words. So to overcome this, n-grams can be used where the sequential text meaning is considered as important.","9c48c287":"Observation: 'not' can be seen as most common word in both Positive and Negative words. Probably positve review may contain words like 'not sure' ."}}