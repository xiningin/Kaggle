{"cell_type":{"9b2a5f83":"code","03a240e1":"code","32a6f864":"code","aa415160":"code","19d0d191":"code","007b8728":"code","700003ea":"code","b4781b73":"code","0558a8b9":"code","f6bfccd3":"code","97d7ff48":"code","df96c152":"code","23b660e7":"code","61a35277":"code","fa5936a9":"code","93149099":"code","4aea6be2":"code","ed9df0b4":"code","cd7b5b9a":"code","5b225dfb":"code","fa3a5e7b":"code","6f1dcf1a":"code","3ff2b034":"code","360df254":"code","39b963b7":"code","675bdcec":"code","b5edf706":"code","47926e49":"code","4e524763":"code","354011e5":"code","419970c6":"code","633cc02a":"code","2a718c78":"code","a4dcbca4":"code","6d3307f9":"code","f5113d4c":"code","b658e183":"code","c9cb428f":"code","1a034ba8":"code","6cdef3af":"code","afda830d":"code","8c896eec":"code","91872bbc":"code","26c3bc51":"code","36dba44a":"code","0c099d1c":"code","43ab452b":"code","985f05d3":"code","4fb5a26d":"markdown","0ac8568c":"markdown","41f32ffe":"markdown","63fb951b":"markdown","a1384ab3":"markdown","cbebd324":"markdown","e5de43ed":"markdown","553b98c3":"markdown","961b4129":"markdown","e4750405":"markdown","4106f05e":"markdown","ab89a751":"markdown","5a88c1cf":"markdown","c6e84cc1":"markdown","7ef107cb":"markdown","3f44b877":"markdown","ec2f911a":"markdown","b2ab48e6":"markdown","db499f9a":"markdown","9f45aee7":"markdown","c08d53ff":"markdown","2b9d6d0c":"markdown","e6bb364d":"markdown"},"source":{"9b2a5f83":"!pip install sweetviz\n!pip install nb_black==1.0.7","03a240e1":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\nimport sweetviz as sv\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split,StratifiedKFold, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import SelectPercentile, chi2\nfrom sklearn.metrics import plot_confusion_matrix,auc,confusion_matrix,roc_auc_score,roc_curve\n\nfrom yellowbrick.classifier import ConfusionMatrix,ROCAUC\nfrom yellowbrick.model_selection import LearningCurve\n\nfrom pytz import timezone\nfrom kaggle_secrets import UserSecretsClient\n","32a6f864":"# For reproducible results\nrandom_seed = 42","aa415160":"# Read in data as pandas dataframe and display the info and a small sample\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\", index_col=0)\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\", index_col=0)\n\nprint(\"Titanic Training Dataframe\")\ntrain.info()\nprint()\nprint(\"Titanic Testing Dataframe\")\ntest.info()\nprint()\ntest.sample(3)","19d0d191":"# Concatenate the training and the testing dataset's for cleaning and analysis\n\ndef concat_df(train, test):\n    # Returns a concatenated df of training and test set\n    return pd.concat([train, test], sort=True)\n\ndf_all = concat_df(train, test)\n\ndf_all.info()","007b8728":"# Create a SweetViz EDA Report and set y to the Survived Panda's Series\n\ny = train['Survived']\n\nmy_report = sv.compare([train, \"Training Data\"], [test, \"Test Data\"], \"Survived\")\n\nmy_report.show_notebook(h=\"full\")","700003ea":"# Generate another SweetViz Report to compare Male vs Female survival rates\n\nfeature_config = sv.FeatureConfig(skip=\"Sex\")\n\nmy_report_sex = sv.compare_intra(train, train[\"Sex\"] == 'male', [\"Male\", \"Female\"], 'Survived',feature_config)\n\nmy_report_sex.show_notebook(h=\"full\")","b4781b73":"#What are the median ages by sex and Pclass?\ndf_all.groupby(['Sex', 'Pclass'])['Age'].median()","0558a8b9":"# Filling the missing values in Age with the medians of Sex and Pclass groups\ndf_all['Age'] = df_all.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n\n# Filling the missing values in Embarked with S strictly due to the small amount of missing values and the majority leaving from Southampton  \ndf_all['Embarked'] = df_all['Embarked'].fillna('S')\n\n# Bin Age into 10 bins\ndf_all['Age'] = pd.qcut(df_all['Age'], 10, duplicates='drop').cat.codes\n\ndf_all.info()","f6bfccd3":"# What is the single missing Fare?\ndf_all[df_all['Fare'].isnull()]","97d7ff48":"# Find the mean Fare price per Pclass and fill in the missing value\nprint(\"The mean price per Fare for each Pclass:\")\nprint(df_all.groupby(['Pclass'])['Fare'].mean())\ndf_all['Fare'].fillna(13.3, inplace=True)\nprint()\nprint(\"The missing value for passenger id 1043 has been updated to 13.3\")\ndf_all.loc[1044]","df96c152":"# Bin Fare into 13 bins\ndf_all['Fare'] = pd.qcut(df_all['Fare'], 13, duplicates='drop').cat.codes","23b660e7":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Fare', hue='Survived', data=df_all,  palette=['#005AFF', '#FFA500'])\n\nplt.xlabel('Fare', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","61a35277":"# Creating Deck column from the first letter of the Cabin column (M stands for Missing)\n\ndf_all['Deck'] = df_all['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\ndf_all_decks = df_all.groupby(['Deck', 'Pclass']).count().drop(columns=['Survived', 'Sex', 'Age', 'SibSp', 'Parch', \n                                                                        'Fare', 'Embarked', 'Cabin', 'Ticket']).rename(columns={'Name': 'Count'}).transpose()\n\ndef get_pclass_dist(df):\n    \n    # Creating a dictionary for every passenger class count in every deck\n    deck_counts = {'A': {}, 'B': {}, 'C': {}, 'D': {}, 'E': {}, 'F': {}, 'G': {}, 'M': {}, 'T': {}}\n    decks = df.columns.levels[0]    \n    \n    for deck in decks:\n        for pclass in range(1, 4):\n            try:\n                count = df[deck][pclass][0]\n                deck_counts[deck][pclass] = count \n            except KeyError:\n                deck_counts[deck][pclass] = 0\n                \n    df_decks = pd.DataFrame(deck_counts)    \n    deck_percentages = {}\n\n    # Creating a dictionary for every passenger class percentage in every deck\n    for col in df_decks.columns:\n        deck_percentages[col] = [(count \/ df_decks[col].sum()) * 100 for count in df_decks[col]]\n        \n    return deck_counts, deck_percentages","fa5936a9":"def display_pclass_dist(percentages):\n    \n    df_percentages = pd.DataFrame(percentages).transpose()\n    deck_names = ('A', 'B', 'C', 'D', 'E', 'F', 'G', 'M', 'T')\n    bar_count = np.arange(len(deck_names))  \n    bar_width = 0.85\n    \n    pclass1 = df_percentages[0]\n    pclass2 = df_percentages[1]\n    pclass3 = df_percentages[2]\n    \n    plt.figure(figsize=(10, 5))\n    plt.bar(bar_count, pclass1, color='#005AFF', edgecolor='White', width=bar_width, label='Passenger Class 1')\n    plt.bar(bar_count, pclass2, bottom=pclass1, color='#B3CEFF', edgecolor='white', width=bar_width, label='Passenger Class 2')\n    plt.bar(bar_count, pclass3, bottom=pclass1 + pclass2, color='#FFA500', edgecolor='white', width=bar_width, label='Passenger Class 3')\n\n    plt.xlabel('Deck', size=15, labelpad=20)\n    plt.ylabel('Passenger Class Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, deck_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Passenger Class Distribution in Decks', size=18, y=1.05)   \n    \n    plt.show()    \n\nall_deck_count, all_deck_per = get_pclass_dist(df_all_decks)\ndisplay_pclass_dist(all_deck_per)\n\nprint(all_deck_per)\n\nprint()\nprint(\"Count of Passengers by Deck\")\nprint(df_all.groupby('Deck')['Pclass'].count())\nprint()\nprint('Distribution by Class')\ndf_all_decks","93149099":"# Reduce the number of categories based on the class distribution\n\n# Move the one Passenger in the T deck to A due to the high cardinality along with the fact that there is only 1 person in T\nidx = df_all[df_all['Deck'] == 'T'].index\ndf_all.loc[idx, 'Deck'] = 'A'\n\ndf_all['Deck'] = df_all['Deck'].replace(['A', 'B', 'C'], 'ABC') # A,B and C have only first class passengers\ndf_all['Deck'] = df_all['Deck'].replace(['D', 'E'], 'DE') # D,E have simialr class distributions\ndf_all['Deck'] = df_all['Deck'].replace(['F', 'G'], 'FG') # F,G have simialr class distributions\n\n# Leave the M category for missing cabin info\ndf_all['Deck'].value_counts()","4aea6be2":"df_all['Ticket_Frequency'] = df_all.groupby('Ticket')['Ticket'].transform('count')","ed9df0b4":"fig, axs = plt.subplots(figsize=(10, 7))\nsns.countplot(x='Ticket_Frequency', hue='Survived', data=df_all, palette=['#005AFF', '#FFA500'])\n\nplt.xlabel('Ticket Frequency', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Ticket Frequency'), size=15, y=1.05)\n\nplt.show()","cd7b5b9a":"# Add features\n\ndf_all['Family_Size'] = df_all['SibSp'] + df_all['Parch'] + 1\n\ndef is_alone(s):\n    for data in s:\n        s['Is_Alone'] = 0\n        s.loc[s['Family_Size']==1, 'Is_Alone'] = 1\n        \ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If title exists extract it\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nis_alone(df_all)","5b225dfb":"family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf_all['Family_Size_Grouped'] = df_all['Family_Size'].map(family_map)\ndf_all.drop(columns=['Family_Size'], inplace=True, axis=1)","fa3a5e7b":"df_all['Title'] = df_all['Name'].apply(get_title)\ndf_all['Title'] = df_all['Title'].replace(['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer', 'Dona'], 'Rare')","6f1dcf1a":"df_all['Title'] = df_all['Title'].replace('Mlle','Miss')\ndf_all['Title'] = df_all['Title'].replace('Ms','Miss')\ndf_all['Title'] = df_all['Title'].replace('Mme','Mrs')","3ff2b034":"# The Mrs Title has the highest surviving rate over any other female title\ndf_all['Is_Married'] = 0\ndf_all['Is_Married'] = np.where(df_all['Title'].isin(['Mrs']),1, df_all['Is_Married'])","360df254":"def divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:891], all_data.loc[892:].drop(['Survived'], axis=1)\n\n\ntrain, test = divide_df(df_all)","39b963b7":"X = train.drop(['Survived'], axis='columns')\ny = train.Survived","675bdcec":"column_trans = make_column_transformer(\n    (OneHotEncoder(), ['Age','Sex','Deck','Embarked','Title','Family_Size_Grouped']),('drop',['Name','Cabin','Ticket','Fare']),\n    remainder='passthrough')\nclf =  RandomForestClassifier()","b5edf706":"pipe = make_pipeline(column_trans, clf)","47926e49":"default_params = cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\nprint(default_params)","4e524763":"# keep x% of features with the best chi-squared scores\nselection = SelectPercentile(chi2, percentile=90)","354011e5":"# Create new pipline with feature selection % as a criteria \npipe = make_pipeline(column_trans, selection, clf)\ncross_val_score(pipe, X, y, cv=5,scoring='accuracy').mean()","419970c6":"pipe","633cc02a":"params = {\"selectpercentile__percentile\":[80, 85, 90, 95],\n              \"randomforestclassifier__max_depth\": [2,6,10],\n              \"randomforestclassifier__max_features\": ['auto', 'sqrt'],\n              \"randomforestclassifier__min_samples_split\": [2, 5, 10,15],\n              \"randomforestclassifier__min_samples_leaf\": [1, 2, 4, 8],\n              \"randomforestclassifier__n_estimators\": [10,200,800,1000,1500]}","2a718c78":"# instantiate RandomSearchCV\nrandom = RandomizedSearchCV(pipe , params, cv=3, scoring='accuracy',random_state=random_seed)","a4dcbca4":"random.fit(X,y)","6d3307f9":"random.best_params_","f5113d4c":"# convert results into a DataFrame\nresults = pd.DataFrame(random.cv_results_)","b658e183":"# sort by test score\nresults.sort_values('rank_test_score')","c9cb428f":"clf =  RandomForestClassifier(n_estimators = 1500, max_depth = 10, \n                               min_samples_split = 2, min_samples_leaf = 4,max_features='auto', bootstrap= True)\n\nselection = SelectPercentile(chi2, percentile=95)\n\npipe = make_pipeline(column_trans, clf)","1a034ba8":"tuned_params = cross_val_score(pipe, X, y, cv=5, scoring='accuracy').mean()\nprint(tuned_params)","6cdef3af":"# Tuning pickup over default params\ntuned_params - default_params","afda830d":"# pass in a trained pieline model: it makes predictions for X and compares them to y\npipe.fit(X, y)\ndisp = plot_confusion_matrix(pipe, X, y, cmap='Blues', values_format='d')","8c896eec":"#test = test.drop(['index'], axis=1)\npipe.predict(test)","91872bbc":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(pipe, X, y)","26c3bc51":"preds = pipe.predict(test)","36dba44a":"test.reset_index(inplace=True)","0c099d1c":"f_submission = \"baseline_submission.csv\"\ndf_test_preds = pd.DataFrame({\"PassengerId\": test[\"PassengerId\"], \"Survived\": preds})\ndf_test_preds['Survived'] = df_test_preds['Survived'].astype('int64')\ndf_test_preds.to_csv(f_submission, index=False)","43ab452b":"import os\nimport json\n\nuser_secrets = UserSecretsClient()\ncreds_dict = {\n    \"username\": user_secrets.get_secret(\"username\"),\n    \"key\": user_secrets.get_secret(\"key\"),\n}\n\nsecrets_path = \"\/root\/.kaggle\"\nif not os.path.exists(secrets_path):\n    os.makedirs(secrets_path)\n\nwith open(f\"{secrets_path}\/kaggle.json\", \"w\") as f:\n    json.dump(creds_dict, f)\n\n!chmod 600 \/root\/.kaggle\/kaggle.json","985f05d3":"from datetime import datetime\ndatetime_fmt = \"%Y-%m-%d %H:%M:%S\"\n\ncurrent_datetime = datetime.now(timezone(\"US\/Pacific\"))\nsubmission_message = \"Iteration 4 model submission: %s\" % (\n    current_datetime.strftime(datetime_fmt)\n)\n\n!kaggle competitions submit -c titanic -f {f_submission} -m \"{submission_message}\"","4fb5a26d":"# Random Seed","0ac8568c":"# Confusion Matrix","41f32ffe":"#### Title and Is_Married","63fb951b":"# Titanic - Machine Learning from Disaster","a1384ab3":"## Cleaning the data","cbebd324":"#### Display deck level cut from the Aft of the Titanic","e5de43ed":"#### Split Dataframes back out","553b98c3":"### Create a concatenated Dataframe to clean the data\n\n<ul>\n<li>It is convenient to work on concatenated training and test set while dealing with missing values, otherwise filled data may overfit to training or test set samples.<\/li>\n<li>The count of missing values in Age, Embarked and Fare are smaller compared to total sample, but roughly 80% of the Cabin is missing.<\/li>\n<li>Missing values in Age, Embarked and Fare can be filled with descriptive statistical measures but that wouldn't work for Cabin.<\/li>\n<\/ul>- Kaggler gunesevitan","961b4129":"# Make Predictions and ROC Curve","e4750405":"### Fare","4106f05e":"# What can we gleen from the SweetViz Reports?","ab89a751":"<ul>\n<li>Only 38% of the passengers survived, the other 62% perished (Training Data)<\/li>\n<li>When looking at Gender 74% of the Females Survived while only ~19% of Males Survived (Training Data)<\/li>\n<li>Within the three classes First Class Survival Rate was 63% (97% of Female First Class Survived), Second Class was 47% (92% of Female First Class Survived) and the Third Class Survival Rate was 24% the lowest by far (Training Data)<\/li>\n<li>While Southampton had the lowest % Survival Rate at 34% or 217 passengers, it was also the greates port of entry at 644 out of the 889 non blank entries<\/li>\n<li>The greatest number of missing values is in the Cabin Field<\/li>\n<\/ul>","5a88c1cf":"### Cabin","c6e84cc1":"# Load Libraries\/Packages and Titanic Data","7ef107cb":"# Added Features or Feature Engineering","3f44b877":"![Titanic.jpg](attachment:e396f50c-a2a8-464c-ac05-7bc0ae9e6c88.jpg)","ec2f911a":"### Age and Embarked","b2ab48e6":"# Exploratory Data Analysis","db499f9a":"# Reference Table\n<ul>\n    <li><strong><em>PassengerId<\/em><\/strong> is the unique id of the row for each passenger<\/li>\n    <li><strong><em>Survived<\/em><\/strong> is the target variable we are trying to predict (0 Died or 1 Survived)<\/li>\n    <li><strong><em>Pclass (Passenger Class)<\/em><\/strong> is the socio-economic status of the passenger and it is a categorical ordinal feature which has 3 unique values (1, 2 or 3)<\/li>\n    <li><strong><em>Name, Sex and Age<\/em><\/strong> are self-explanatory<\/li>\n    <li><strong><em>SibSp<\/em><\/strong> is the total number of the passengers' siblings and spouse<\/li>\n    <li><strong><em>Parch<\/em><\/strong> is the total number of the passengers' parents and children<\/li>\n    <li><strong><em>Ticket<\/em><\/strong> is the ticket number of the passenger<\/li>\n    <li><strong><em>Fare<\/em><\/strong> is the passenger fare<\/li>\n    <li><strong><em>Cabin<\/em><\/strong> is the cabin number of the passenger<\/li>\n    <li><strong><em>Embarked<\/em><\/strong> is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):<\/li>\n    <ul>\n        <li>C = Cherbourg<\/li>\n        <li>Q = Queenstown<\/li>\n        <li>S = Southampton<\/li>\n     <\/ul>\n<\/ul>","9f45aee7":"### ROC Curve\n\nQuoting Wikipedia :\n\u201cA receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\u201d","c08d53ff":"### Ticket","2b9d6d0c":"###  Confusion Matrix\n\n<ul>\n    Accuracy is the popular model evaluation method used for the majority of the classification models in supervised learning algorithms.\n    When we are having accuracy as a measure for knowing the performance of the classification models then why we need another measure to quantify the performance of the model?\n    Accuracy is not able to explain the below question:\n    <ul>\n        <li>How many actual positive targets are predicted as positive?<\/li>\n        <li>How many actual positive targets are predicted as negative?<\/li>\n        <li>How many actual negative targets are predicted as positive?<\/li>\n        <li>How many actual negative targets are predicted as negative?<\/li>\n     <\/ul>\n<\/ul>\n","e6bb364d":"# La Fine !!!!!\n"}}