{"cell_type":{"bb7ab476":"code","e1ef250f":"code","9cdcf9c9":"code","4248ba5c":"code","d8461ef1":"code","b70e35fa":"code","0baed4ef":"code","3d269bdb":"code","21710c41":"code","d0f0611c":"code","11193be0":"code","dc31d58c":"code","a9baef83":"code","15fdd312":"code","7e0c8de6":"code","29cb545f":"code","c71575c0":"code","79343962":"code","d521330f":"code","6a0b82e9":"code","b625961e":"code","86a06f2e":"code","c6e5f447":"code","e15b5c7b":"code","c8c6455d":"code","6cab90e9":"code","4e4b74aa":"code","e4f3e808":"code","df94408d":"code","f5504ca7":"code","46edaf12":"code","ae3aac51":"code","f6202f1c":"code","45122ee0":"code","bd71f58d":"code","5892d744":"code","f203c534":"code","c1eaaceb":"code","b4c80d73":"code","7ab4af57":"code","9d117744":"code","dbbdd63c":"code","d981397e":"code","9c24c03e":"code","1745a929":"code","db9c7eab":"code","471a7e5b":"code","af0f6c33":"code","4d5352b2":"code","25f033da":"code","05b26cb7":"code","5ac4d4bf":"code","5fadad85":"code","45b9ee1d":"code","61520e0f":"code","5f9ab58c":"code","92539d31":"code","3e29f848":"code","8a102352":"code","3f098f3e":"code","29ecb795":"code","ca5eb9b3":"code","3d3d4c69":"code","8b34d756":"code","5cbb482c":"markdown","d2c5f073":"markdown","ee37532a":"markdown","06559c2a":"markdown","13c5d110":"markdown","aeedd14a":"markdown","58f2f308":"markdown","834ba8a2":"markdown","f77beaf4":"markdown","a558d236":"markdown","00058d31":"markdown","48b24c50":"markdown","9235eca7":"markdown","8b573529":"markdown","d6a62e5a":"markdown","0e7ac061":"markdown","7181c596":"markdown","ee756041":"markdown","5f562c3c":"markdown","6a2c5a74":"markdown","51e61e37":"markdown"},"source":{"bb7ab476":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e1ef250f":"train_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ncombine = [train_data, test_data]","9cdcf9c9":"train_data.info()","4248ba5c":"train_data.head(10)","d8461ef1":"test_data.head(10)","b70e35fa":"print(train_data.shape)\nprint(test_data.shape)","0baed4ef":"train_data.describe()","3d269bdb":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women survivors:\", rate_women)","21710c41":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men survivors:\", rate_men)","d0f0611c":"train_data.isnull().sum()","11193be0":"for dataset in combine:\n    dataset.drop(['Cabin','Ticket'],axis = 1,inplace = True)\n    print(dataset.shape)","dc31d58c":"for dataset in combine:\n    dataset['Title'] = train_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])","a9baef83":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","15fdd312":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()","7e0c8de6":"train_data.drop(['Name','PassengerId'],axis = 1,inplace = True)\ntest_data.drop(['Name'],axis = 1,inplace = True)\ncombine = [train_data, test_data]\nprint(train_data.shape)\nprint(test_data.shape)","29cb545f":"sns.countplot(x=\"Survived\",data=train_data)","c71575c0":"sns.countplot(x=\"Survived\", hue = 'Pclass',data=train_data)","79343962":"sns.countplot(x=\"Survived\", hue = 'Sex',data=train_data)","d521330f":"sns.countplot(x=\"Survived\", hue = 'SibSp',data=train_data)","6a0b82e9":"sns.countplot(x=\"Survived\", hue = 'Parch',data=train_data)","b625961e":"sns.violinplot(x=\"Survived\", y=\"Age\", data = train_data, size = 9)","86a06f2e":"sns.countplot(x=\"Survived\",hue = \"Embarked\",data=train_data)","c6e5f447":"#filling missing values for 'Embarked' with most frequent one\nfreq_port = train_data.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e15b5c7b":"for dataset in combine:\n    mean = train_data[\"Age\"].mean()\n    std = test_data[\"Age\"].std()\n    is_null = dataset[\"Age\"].isnull().sum()\n    # compute random numbers between the mean, std and is_null\n    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n    # fill NaN values in Age column with random values generated\n    age_slice = dataset[\"Age\"].copy()\n    age_slice[np.isnan(age_slice)] = rand_age\n    dataset[\"Age\"] = age_slice\n    dataset[\"Age\"] = train_data[\"Age\"].astype(int)","c8c6455d":"train_data.isnull().sum()","6cab90e9":"test_data.isnull().sum()","4e4b74aa":"# Filling the one missing value from Fare in test_data\ntest_data['Fare'].fillna(test_data['Fare'].dropna().median(), inplace=True)\ntest_data.isnull().sum()","e4f3e808":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","df94408d":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","f5504ca7":"train_data.head()","46edaf12":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","ae3aac51":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","f6202f1c":"train_data.head()","45122ee0":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 36), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 36) & (dataset['Age'] <= 50), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\ntrain_data.head()","bd71f58d":"train_data['FareBand'] = pd.qcut(train_data['Fare'], 4)\ntest_data['FareBand'] = pd.qcut(test_data['Fare'], 4)\ntrain_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\ntest_data.head()","5892d744":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_data = train_data.drop(['FareBand'], axis=1)\ntest_data = test_data.drop(['FareBand'], axis=1)\ncombine = [train_data, test_data]","f203c534":"train_data.head()","c1eaaceb":"test_data.head()","b4c80d73":"X_train = train_data.drop(['Survived'], axis = 1).values\nY_train = train_data['Survived'].values\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()","7ab4af57":"print(X_train.shape)\nprint(Y_train.shape)\nprint(X_test.shape)","9d117744":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X_train,Y_train,test_size=0.2)","dbbdd63c":"from sklearn.linear_model import LogisticRegression\nlr_clf = LogisticRegression()\nlr_clf.fit(x_train, y_train)","d981397e":"pred_train = lr_clf.predict(x_train)\npred_test = lr_clf.predict(x_test)","9c24c03e":"from sklearn.metrics import accuracy_score\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","1745a929":"from sklearn.ensemble import RandomForestClassifier\nrf_clf = RandomForestClassifier(n_jobs= -1, n_estimators = 100, warm_start= True, max_depth= 5, min_samples_leaf= 2, max_features = 'sqrt',verbose = 0)\nrf_clf.fit(x_train, y_train)","db9c7eab":"pred_train = rf_clf.predict(x_train)\npred_test = rf_clf.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","471a7e5b":"from sklearn.ensemble import AdaBoostClassifier\nadb_clf = AdaBoostClassifier(n_estimators = 100, learning_rate = 0.5)\nadb_clf.fit(x_train, y_train)","af0f6c33":"pred_train = adb_clf.predict(x_train)\npred_test = adb_clf.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","4d5352b2":"from sklearn.ensemble import GradientBoostingClassifier\ngdb_clf = GradientBoostingClassifier(n_estimators = 100, max_depth = 3, min_samples_leaf = 2, verbose = 0)\ngdb_clf.fit(x_train, y_train)","25f033da":"pred_train = gdb_clf.predict(x_train)\npred_test = gdb_clf.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","05b26cb7":"from sklearn.ensemble import ExtraTreesClassifier\net_clf = ExtraTreesClassifier(n_jobs = -1, n_estimators = 100, max_depth = 5, min_samples_leaf = 2, verbose = 0)\net_clf.fit(x_train,y_train)","5ac4d4bf":"pred_train = et_clf.predict(x_train)\npred_test = et_clf.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","5fadad85":"from sklearn.svm import SVC\nsvc_clf = SVC(kernel = 'linear', C = 0.025)\nsvc_clf.fit(x_train,y_train)","45b9ee1d":"pred_train = svc_clf.predict(x_train)\npred_test = svc_clf.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","61520e0f":"from xgboost import XGBClassifier\nxgb_clf = XGBClassifier(n_estimators= 50, max_depth= 5, min_samples_leaf= 2)\nxgb_clf.fit(x_train, y_train)","5f9ab58c":"pred_train = xgb_clf.predict(x_train)\npred_test = xgb_clf.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","92539d31":"import lightgbm as lgb\nlgb_clf = lgb.LGBMClassifier(n_estimators=100)\nlgb_clf.fit(x_train,y_train)","3e29f848":"pred_train = lgb_clf.predict(x_train)\npred_test = lgb_clf.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","8a102352":"from sklearn.ensemble import VotingClassifier\nvt_classifier = VotingClassifier(estimators = [('lr', lr_clf),\n                                               ('rf',rf_clf),\n                                               ('adb',adb_clf),\n                                               ('gdb',gdb_clf),\n                                               ('etc',et_clf),\n                                               ('svc',svc_clf),\n                                               ('xgb',xgb_clf),\n                                               ('lgbm',lgb_clf),], voting = 'hard')","3f098f3e":"vt_classifier.fit(x_train,y_train)","29ecb795":"pred_train = vt_classifier.predict(x_train)\npred_test = vt_classifier.predict(x_test)\ntrain_accuracy = accuracy_score(y_train,pred_train)\ntest_accuracy = accuracy_score(y_test,pred_test)\nprint(\"Training Accuracy: \", train_accuracy)\nprint(\"Testing Accuracy: \", test_accuracy)","ca5eb9b3":"final_pred = vt_classifier.predict(X_train)\ntrain_accuracy = accuracy_score(final_pred,Y_train)\nprint(\"Training Accuracy: \", train_accuracy)","3d3d4c69":"X_test = X_test.values\nfinal_pred = final_model.predict(X_test)","8b34d756":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = final_pred\nsubmission.to_csv('titanic_submission3.csv', index=False)","5cbb482c":"# Visualization and Analysis","d2c5f073":"# Dropping Name Column as we have extracted the Titles","ee37532a":"# Final Model (Ensembling Stacking all the models)","06559c2a":"# Handling categorical variables","13c5d110":"# Creating new features FamilySize and IsAlone from Parch ana SibSp","aeedd14a":"# Checking Missing Values","58f2f308":"## Logistic Regression","834ba8a2":"# Creating bands for age and Fare","f77beaf4":"# Replacing Titles with common ones or Rare and coverting them to ordinal values","a558d236":"# Creating a new feature 'Title' by extracting title from names","00058d31":"## Random Forest Classifier","48b24c50":"## Ada Boost Classifier","9235eca7":"# Creating The Models and Checking individual performances","8b573529":"## XGBoost Classifier","d6a62e5a":"## Support Vector Classifier","0e7ac061":"## LightGBM Classifier","7181c596":"# Handling missing values for Age and Embarked","ee756041":"# Importing and Studying Data","5f562c3c":"## Extra Trees Classifier","6a2c5a74":"# Dropping Cabin(too many missing values) and Ticket(no contribution to survival)","51e61e37":"## Gradient Boosting Classifier"}}