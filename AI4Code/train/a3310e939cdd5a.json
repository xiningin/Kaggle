{"cell_type":{"51209dd8":"code","6d6b51fb":"code","e49efc1a":"code","26f630d1":"code","3b6dc528":"code","6eb5c68a":"code","fe8e7b76":"code","3123a261":"code","99f9cfea":"code","51f97344":"code","9aa54ff8":"code","2298a285":"code","c7c88beb":"code","724ac50e":"code","389ab51f":"code","c1a29042":"code","3314706c":"code","60ecfb00":"code","6f6db965":"code","bd580df7":"code","db25fc1e":"code","d12bd8f2":"code","f7d44845":"code","834f08eb":"code","936ea35b":"code","0b51c8ed":"markdown","ca5615ee":"markdown","e19727e0":"markdown","80508044":"markdown","dfb79b28":"markdown","9787ce0c":"markdown","504492c7":"markdown","6473bab6":"markdown","d347f9db":"markdown","9bf8f7ad":"markdown","188186b4":"markdown","95a9ece3":"markdown","4582e094":"markdown","f795654e":"markdown","d0960514":"markdown","a8af8bfc":"markdown"},"source":{"51209dd8":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import linear_model, naive_bayes, neighbors, svm","6d6b51fb":"drugs = pd.read_csv('..\/input\/drug-classification\/drug200.csv')\ndrugs.head()","e49efc1a":"sns.distplot(drugs['Age'])","26f630d1":"sns.distplot(drugs['Na_to_K'])","3b6dc528":"drugs.describe()","6eb5c68a":"age_groups = []\nfor i in drugs['Age']:\n    if i <= 30:\n        age_groups.append('0-30')\n    if i > 30 and i <= 40:\n        age_groups.append('30-40')\n    if i > 40 and i <= 50:\n        age_groups.append('40-50')\n    if i > 50 and i <= 60:\n        age_groups.append('50-60')\n    if i > 60:\n        age_groups.append('60+')\n        \ndrugs['AgeGroup'] = age_groups","fe8e7b76":"na_to_k_groups = []\nfor i in drugs['Na_to_K']:\n    if i <= 10:\n        na_to_k_groups.append('5-10')\n    if i > 10 and i <= 15:\n        na_to_k_groups.append('10-15')\n    if i > 15 and i <= 20:\n        na_to_k_groups.append('15-20')\n    if i > 20 and i <= 25:\n        na_to_k_groups.append('20-25')\n    if i > 25 and i <= 30:\n        na_to_k_groups.append('25-30')\n    if i > 30:\n        na_to_k_groups.append('30+')\n        \ndrugs['Na_to_K_groups'] = na_to_k_groups","3123a261":"drugs = drugs[['AgeGroup', 'Sex', 'BP', 'Cholesterol', 'Na_to_K_groups', 'Drug']]\ndrugs.head()","99f9cfea":"sns.set_theme(style=\"darkgrid\")\nsns.countplot(x=\"AgeGroup\", data=drugs, palette='Spectral', order=['0-30', '30-40', '40-50', '50-60', '60+'])","51f97344":"sns.countplot(x=\"Sex\", data=drugs, palette='Spectral')","9aa54ff8":"sns.countplot(x=\"BP\", data=drugs, palette='Spectral')","2298a285":"sns.countplot(x=\"Cholesterol\", data=drugs, palette='Spectral')","c7c88beb":"sns.countplot(x=\"Na_to_K_groups\", data=drugs, palette='Spectral', order=['5-10', '10-15', '15-20', '20-25', '30+'])","724ac50e":"sns.countplot(x=\"Drug\", data=drugs, palette='Spectral')","389ab51f":"x, y = drugs.values[:, :-1], drugs.values[:, -1]","c1a29042":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)","3314706c":"x_train = pd.get_dummies(pd.DataFrame(x_train))\nx_test = pd.get_dummies(pd.DataFrame(x_test))","60ecfb00":"x_train, y_train = SMOTE().fit_resample(x_train, y_train)","6f6db965":"ax = sns.countplot(x=y_train, data=drugs, palette='Spectral')\nax.set(xlabel=\"Drug\")","bd580df7":"print(\"ORIGINAL dataset:\", len(drugs), \"\\n EXTENDED dataset:\",len(y))","db25fc1e":"log_reg = linear_model.LogisticRegression(max_iter = 5000)\nlog_reg.fit(x_train, y_train)\nlog_reg_acc = 100*log_reg.score(x_test, y_test)\nprint('Logistic Regression Predictions: \\n', log_reg.predict(x_test), '\\n Accuracy:', log_reg_acc, '%')","d12bd8f2":"nb = naive_bayes.GaussianNB()\nnb.fit(x_train, y_train)\nnb_acc = 100*nb.score(x_test, y_test)\nprint('Naive Bayes Predictions: \\n', nb.predict(x_test), '\\n Accuracy:', nb_acc, '%')","f7d44845":"knn = neighbors.KNeighborsClassifier(n_neighbors=25)\nknn.fit(x_train, y_train)\nknn_acc = 100*knn.score(x_test, y_test)\nprint('K-Nearest Neighbours Predictions: \\n', knn.predict(x_test), '\\n Accuracy:', knn_acc, '%')","834f08eb":"svm = svm.SVC(kernel='linear')\nsvm.fit(x_train, y_train)\nsvm_acc = 100*svm.score(x_test, y_test)\nprint('SVM Predictions: \\n', svm.predict(x_test), '\\n Accuracy:', svm_acc, '%')","936ea35b":"pd.DataFrame(data={'Model': ['Logistic Regression', 'Gaussian Naive Bayes', 'K-Nearest Neighbours', 'Support Vector Machine (SVM)'], 'Accuracy %': [log_reg_acc, nb_acc, knn_acc, svm_acc]})","0b51c8ed":"The drug frequencies are very unbalanced. For a classification problem it would be ideal to have similar numbers of the target variable. Therefore we will use SMOTE (Synthetic Minority Oversampling Technique). SMOTE will oversample drugA, drugB, drugC and drugX to have the same number of samples as DrugY. The synthetic aspect of this oversampling technique helps to avoid overfitting, as it's not just repeating existing data. First we need to split the dataset into training and testing sets and transform the data into dummies.","ca5615ee":"The variable descriptions are as follows:\n\n* Age: Age of the patient in years\n* Sex: Sex of the patient - male or female\n* BP: Blood Pressure of patient - high, low, or normal\n* Cholesterol - Cholesterol level of the patient - normal or high\n* Na_to_K - Sodium to Potassium ratio in patient's blood\n* Drug - The drug type that the patient was prescribed - drugA, drugB, drugC, drugX or drugY","e19727e0":"## Gaussian Naive Bayes","80508044":"## Support Vector Machine","dfb79b28":"## Grouping data","9787ce0c":"## Logistic Regression","504492c7":"We can see here that the minimum and maximum ages are 15 and 74 respectively and the minimum and maximum sodium to potassium ratios are 6.269 and 38.247 respectively.\n\n**Age groups:**\n* Under 30s\n* 30-40\n* 40-50\n* 50-60\n* Over 60s\n\n**Sodium to Potassium ratios:**\n* 5-10\n* 10-15\n* 15-20\n* 20-25\n* 25-30\n* 30+","6473bab6":"## K-Nearest Neighbours","d347f9db":"## Summary","9bf8f7ad":"## Conclusion\n\nLogistic Regression, Naive Bayes and SVM all successfully predicted 100% of the drug types that the patients should be assigned. K-Nearest Neighbours had a much lower success rate of only 77%. This makes sense as KNN is known as a 'Lazy Learner' meaning it doesn't actually 'learn' anything in the training period - it just stores training data. Nevertheless, the other models were successful!","188186b4":"## Imports","95a9ece3":"# Drug Classification\n\n**Project Goal:**\n\nTo create a model capable of determining which drug type a patient should be prescribed based on a number of features, using a variety of classification algorithms.","4582e094":"We're only using SMOTE on the training data. The synthetic data shouldn't be so similar to the original data that it causes overfitting, but by only oversampling the training data, we will know it if does.","f795654e":"We will group the Age and Sodium to Potassium Ratio (Na_to_K) values, depending on the max and min values in each column. Let's have a look at their distributions.","d0960514":"Dataset has been extended by 255.","a8af8bfc":"## Visualising the variable distributions"}}