{"cell_type":{"1c2887e4":"code","9f2910aa":"code","0a243367":"code","64942c4f":"code","a256bda1":"code","fdcca9ee":"code","2df8e6e7":"code","5d593630":"code","df91a190":"code","3a3dc9a1":"code","33378947":"code","d6655453":"code","312217ae":"code","e17246fd":"code","e96f69cb":"code","fe3132e9":"code","92f730d9":"markdown","cf6bf309":"markdown","f823d3ac":"markdown","87d7e8a2":"markdown","3de6d7db":"markdown","f73137fe":"markdown","d0d73166":"markdown","daab0f80":"markdown","cb31eeb9":"markdown","dca7bcc6":"markdown","0093e13a":"markdown","6a8f5aab":"markdown","1b80d1b5":"markdown","eefe3109":"markdown","b1c16cf0":"markdown","9eab1978":"markdown","9282e12d":"markdown","b02ce763":"markdown"},"source":{"1c2887e4":"import os\nimport json\nimport numpy as np # Linear algebra\nimport pandas as pd # For data manipulation\nimport matplotlib.pyplot as plt # For visualization\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import StratifiedKFold # For evaluation and hyperparameter tuning\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report # For evaluation\nfrom scipy.ndimage import rotate, shift, zoom # For data augmentation\nfrom keras import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom IPython.display import FileLink # For downloading the output file\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9f2910aa":"train_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nsubmission_df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/sample_submission.csv\")","0a243367":"train_df.info()","64942c4f":"test_df.info()","a256bda1":"X_train = train_df.iloc[:, 1:].values\ny_train = train_df.iloc[:, 0].values\nX_test = test_df.values\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")","fdcca9ee":"some_digit = X_train[40]\n\nsome_digit_image = some_digit.reshape(28, 28)\nprint(f\"Label: {y_train[40]}\")\nplt.imshow(some_digit_image, cmap=\"binary\")\nplt.show()","2df8e6e7":"def shift_in_one_direction(image, direction):\n    \"\"\"\n    Shifts an image by one pixel in the specified direction\n    \"\"\"\n    if direction == \"DOWN\":\n        image = shift(image, [1, 0])\n    elif direction == \"UP\":\n        image = shift(image, [-1, 0])\n    elif direction == \"LEFT\":\n        image = shift(image, [0, -1])\n    else:\n        image = shift(image, [0, 1])\n\n    return image\n\n\ndef shift_in_all_directions(image):\n    \"\"\"\n    Shifts an image in all the directions by one pixel\n    \"\"\"\n    reshaped_image = image.reshape(28, 28)\n\n    down_shifted_image = shift_in_one_direction(reshaped_image, \"DOWN\")\n    up_shifted_image = shift_in_one_direction(reshaped_image, \"UP\")\n    left_shifted_image = shift_in_one_direction(reshaped_image, \"LEFT\")\n    right_shifted_image = shift_in_one_direction(reshaped_image, \"RIGHT\")\n\n    return (down_shifted_image, up_shifted_image,\n            left_shifted_image, right_shifted_image)\n\n\ndef rotate_in_all_directions(image, angle):\n    \"\"\"\n    Rotates an image clockwise and anti-clockwise\n    \"\"\"\n    reshaped_image = image.reshape(28, 28)\n    \n    rotated_images = (rotate(reshaped_image, angle, reshape=False),\n                      rotate(reshaped_image, -angle, reshape=False))\n    \n    return rotated_images\n\n\ndef clipped_zoom(image, zoom_ranges):\n    \"\"\"\n    Clips and zooms an image at the specified zooming ranges\n    \"\"\"\n    reshaped_image = image.reshape(28, 28)\n    \n    h, w = reshaped_image.shape\n    \n    zoomed_images = []\n    for zoom_range in zoom_ranges:\n        zh = int(np.round(h \/ zoom_range))\n        zw = int(np.round(w \/ zoom_range))\n        top = (h - zh) \/\/ 2\n        left = (w - zw) \/\/ 2\n        \n        zoomed_images.append(zoom(reshaped_image[top:top+zh, left:left+zw],\n                                  zoom_range))\n    \n    return zoomed_images\n\ndef alter_image(image):\n    \"\"\"\n    Alters an image by shifting, rotating, and zooming it\n    \"\"\"\n    shifted_images = shift_in_all_directions(image)\n    rotated_images = rotate_in_all_directions(image, 10)\n    zoomed_images = clipped_zoom(image, [1.1, 1.2])\n            \n    return np.r_[shifted_images, rotated_images, zoomed_images]\n\nX_train_add = np.apply_along_axis(alter_image, 1, X_train).reshape(-1, 784)\ny_train_add = np.repeat(y_train, 8)\n\nprint(f\"X_train_add shape: {X_train_add.shape}\")\nprint(f\"y_train_add shape: {y_train_add.shape}\")","5d593630":"X_train_combined = np.r_[X_train, X_train_add]\ny_train_combined = np.r_[y_train, y_train_add]\n\ndel X_train\ndel X_train_add\ndel y_train\ndel y_train_add\n\nprint(f\"X_train_combined shape: {X_train_combined.shape}\")\nprint(f\"y_train_combined shape: {y_train_combined.shape}\")","df91a190":"class ImageReshaper(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Reshapes the data to the shape required by the KerasClassifier\n    \"\"\"\n    def __init__(self, shape):\n        self.shape = shape\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        return X.reshape(self.shape)","3a3dc9a1":"def build_lenet5_model():\n    \"\"\"\n    Builds and returns the model based on LeNet-5 architecture\n    \"\"\"\n    model = Sequential()\n    # Adding layers to the model\n    model.add(Conv2D(6, kernel_size=5, activation='relu',\n                     input_shape=(28,28,1)))\n    model.add(MaxPooling2D())\n    \n    model.add(Conv2D(16, kernel_size=5, activation='relu'))\n    model.add(MaxPooling2D())\n    \n    model.add(Flatten())\n    \n    model.add(Dense(400, activation='relu'))\n    model.add(Dense(120, activation='relu'))\n    model.add(Dense(10, activation='softmax'))\n    \n    # Specifying the loss function and optimizer\n    model.compile(loss='categorical_crossentropy', optimizer='adam',\n                  metrics=['accuracy'])\n    \n    return model","33378947":"def build_custom_lenet5_model():\n    \"\"\"\n    Builds and returns the model based on a modified LeNet-5 architecture\n    \"\"\"\n    model = Sequential()\n    # Adding layers to the model\n    model.add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(28,28,1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32,kernel_size=3,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(64,kernel_size=3,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64,kernel_size=3,activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    model.add(Dense(10, activation='softmax'))\n    \n    # Specifying the loss function and optimizer\n    model.compile(loss='categorical_crossentropy', optimizer='adam',\n                  metrics=['accuracy'])\n    \n    return model","d6655453":"stratified_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, indices in enumerate(stratified_fold.split(X_train_combined, y_train_combined)):\n    # Creating datasets for training and testing the model \n    X_train_, y_train_ = X_train_combined[indices[0]], y_train_combined[indices[0]]\n    X_test_, y_test_ = X_train_combined[indices[1]], y_train_combined[indices[1]]\n    \n    model_pipeline = Pipeline([\n        ('min_max_scaler', MinMaxScaler()),\n        ('image_reshaper', ImageReshaper(shape=(-1, 28, 28, 1))),\n        ('model', KerasClassifier(build_lenet5_model, epochs=5, batch_size=32))\n    ])\n    \n    model_pipeline.fit(X_train_, y_train_)\n    predictions = model_pipeline.predict(X_test_)\n    \n    print(f\"Classification report for Fold {fold + 1}:\")\n    print(classification_report(y_test_, predictions, digits=3), end=\"\\n\\n\")\n    \n    print(f\"Confusion Matrix for Fold {fold + 1}:\")\n    print(confusion_matrix(y_test_, predictions), end=\"\\n\\n\")\n    \n    del X_train_\n    del X_test_\n    del y_train_\n    del y_test_","312217ae":"lenet5_model = Pipeline([\n    ('min_max_scaler', MinMaxScaler()),\n    ('image_reshaper', ImageReshaper(shape=(-1, 28, 28, 1))),\n    ('model', KerasClassifier(build_lenet5_model, epochs=5, batch_size=32))\n])\n\ncustom_lenet5_model = Pipeline([\n    ('min_max_scaler', MinMaxScaler()),\n    ('image_reshaper', ImageReshaper(shape=(-1, 28, 28, 1))),\n    ('model', KerasClassifier(build_custom_lenet5_model, epochs=20, batch_size=32))\n])\n\n\nlenet5_model.fit(X_train_combined, y_train_combined)\n# Getting the estimated probabilities for each class\nlenet5_model_predictions = lenet5_model.predict_proba(X_test)\n\ncustom_lenet5_model.fit(X_train_combined, y_train_combined)\n# Getting the estimated probabilities for each class\ncustom_lenet5_model_predictions = custom_lenet5_model.predict_proba(X_test)","e17246fd":"predictions = lenet5_model_predictions + custom_lenet5_model_predictions\n\npredictions = np.argmax(predictions, axis=1)","e96f69cb":"submission_df[\"Label\"] = predictions\nsubmission_df.to_csv('submissions.csv', index=False)\nFileLink('submissions.csv')","fe3132e9":"submission_df.head()","92f730d9":"Visualizing a digit from the training data as a 28 X 28 image","cf6bf309":"Loading the datasets into dataframes","f823d3ac":"### Data Exploration","87d7e8a2":"Knowing about the features in the datasets","3de6d7db":"Combining the model results","f73137fe":"Generating the submission file","d0d73166":"Combining the original images and the synthesized images to form a new dataset","daab0f80":"## Digit Recognizer\n\nIn this notebook, I used an ensemble of **Convolutional Neural Network** models based on the **LeNet-5** architecture and the architecture inspired from this [notebook](https:\/\/www.kaggle.com\/cdeotte\/how-to-choose-cnn-architecture-mnist) to recognize the hand-written digits. I also used **Data Augmentation** or **Artificial Data Synthesis** technique to synthesize images in addition to the training data to boost the model's performance on the test data. \n\nAchieved an accuracy of **99.7%** (Leaderboard) on the test data by using the images in the Kaggle's training set and not on the entire MNIST dataset externally available.\n\nPlease **upvote** this notebook if you like the implementation and share your valuable feedback to improve.\n\nYou can find my other notebooks below:\n\n* [Disaster Tweets Classification](https:\/\/www.kaggle.com\/gauthampughazh\/disaster-or-not-plotly-use-tfidf-h2o-ai-automl)\n* [House Sales Price Prediction](https:\/\/www.kaggle.com\/gauthampughazh\/house-sales-price-prediction-svr)\n* [Titanic Survival Classification](https:\/\/www.kaggle.com\/gauthampughazh\/titanic-survival-prediction-pandas-plotly-keras)\n* [Digit Recognition using KNN](https:\/\/www.kaggle.com\/gauthampughazh\/digit-recognition-using-knn)","cb31eeb9":"Building a custom transformer to reshape the scaled images as required by the KerasClassifier","dca7bcc6":"### Result Generation","0093e13a":"### Modelling","6a8f5aab":"Converting the train and test dataframes into numpy arrays","1b80d1b5":"Using **StratifiedKFold** to ensure that the test data represents samples from all classes (digits) and for cross-validating the model. Using the classification report and confusion matrix to understand the model's performance on each fold.","eefe3109":"Function to build a model based on a modified LeNet-5 architecture from the above mentioned notebook","b1c16cf0":"### Data Augmentation","9eab1978":"Each image in the training set is \n\n* shifted down, up, left and right by one pixel\n* rotated clockwise and anti-clockwise \n* clipped and zoomed at two different ranges\n\ngenerating eight different images. The image is clipped before zooming to preserve the image size.","9282e12d":"Function to build a model based on LeNet-5 architecture","b02ce763":"Fitting models to the combined dataset with custom pipelines"}}