{"cell_type":{"2a64f3ee":"code","4386f31b":"code","d0fd57d1":"code","9c147c0f":"code","9e5bd5b4":"code","ddeffa6e":"code","de8aef20":"code","a526e70b":"code","60967101":"code","086df31a":"code","113f2f16":"code","3d26c8c1":"code","df5993fb":"code","13d6c7f5":"code","31991769":"code","78bbefac":"code","f3cc38fd":"code","27fba175":"code","85125de8":"markdown","1005ffca":"markdown","f37ed0ee":"markdown","9942e7f7":"markdown","0dbdd222":"markdown","d2fcfd61":"markdown","ad7da15b":"markdown","1a18c9ab":"markdown","f8af0958":"markdown","e05ea086":"markdown"},"source":{"2a64f3ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4386f31b":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","d0fd57d1":"# Read data\n\ndata = pd.read_csv('..\/input\/prostate-cancer\/Prostate_Cancer.csv')","9c147c0f":"data.head()","9e5bd5b4":"data.info()","ddeffa6e":"data.describe()","de8aef20":"data = data.drop(['id'], axis=1)","a526e70b":"data_dr = data['diagnosis_result'].value_counts()\n\nlabel = [data_dr.index.tolist()]\nplt.pie(data_dr, labels=label[0], shadow=True, explode=(0.0, 0.2), autopct='%1.1f%%', startangle=90)\nplt.gcf().set_size_inches(12,6)\nplt.show()","60967101":"data['diagnosis_result'].replace({'M':0,'B':1},inplace=True)","086df31a":"# Data correlation matrix\ncorr_metrics = data.corr()\ncorr_metrics.style.background_gradient()","113f2f16":"data = data.drop(['fractal_dimension', 'texture', 'perimeter'], axis=1)","3d26c8c1":"# split the data in train and test function\n\nfrom sklearn.model_selection import train_test_split\n\nX = data.drop(['diagnosis_result'], axis=1) # Features\ny = data['diagnosis_result'] # Labels\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=10)","df5993fb":"# Logistic Regression model\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\npred_log = logreg.predict(X_test)","13d6c7f5":"# Random Forest Classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators = 50)\nforest.fit(X_train,y_train)\npred_forest = forest.predict(X_test)","31991769":"# KNeighbors Classifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nkn = KNeighborsClassifier(n_neighbors=2)\nkn.fit(X_train,y_train)\npred_kn = kn.predict(X_test)","78bbefac":"# Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier()\ntree.fit(X_train,y_train)\npred_tree = tree.predict(X_test)","f3cc38fd":"from sklearn.metrics import classification_report\n\nclass_rep_log = classification_report(y_test, pred_log)\nclass_rep_forest = classification_report(y_test, pred_forest)\nclass_rep_kn = classification_report(y_test, pred_kn)\nclass_rep_tree = classification_report(y_test, pred_tree)\n\nprint(\"Logistic Regression: \\n\", class_rep_log)\nprint(\"Forest Classifier: \\n\", class_rep_forest)\nprint(\"KNeighbors Classifier: \\n\", class_rep_kn)\nprint(\"Decision Tree: \\n\", class_rep_tree)","27fba175":"from sklearn.model_selection import KFold, cross_val_score\n\nkf = KFold(10)\n\nlogit_score = cross_val_score(logreg, X, y, cv=kf)\nforest_score = cross_val_score(forest, X, y, cv=kf)\nKNeighbors_score = cross_val_score(kn, X, y, cv=kf)\ntree_score = cross_val_score(tree, X, y, cv=kf)\n\n\n# Print the mean of each array of scores\nprint(\"Logistic Regression:\", np.mean(logit_score), '\\n'\n      \"Forest Classification:\", np.mean(forest_score), '\\n'\n      \"KNeighbors Classification:\", np.mean(KNeighbors_score), '\\n'\n      \"Decision Tree:\", np.mean(tree_score), '\\n'\n     )","85125de8":"The Forest Classificatio model has the highest average precision of 0.9, and both variables are well balanced (benigne: 1 and maligne: 0). \n\nThe Decision Tree model has also a good average precision. However in this model the maligne results are better classified.","1005ffca":"The highest score is obtained with the Forest Classification, followed by the Decision tree.","f37ed0ee":"There is no missing data.","9942e7f7":"## 1. Import and check the Dataset","0dbdd222":"# **Objective: generate models for predicting prostate cancer based on indicators**","d2fcfd61":"Fractal dimension and texture present a low correlation with the remaining variables. Therefore we can eliminate them from our DB.\nAdditional, because perimeter and area have a strong correlation, it will be elimintated to avoid redudancy.","ad7da15b":"### 3. ML models","1a18c9ab":"### 2. Train & test sets","f8af0958":"The id column can be eliminated. This column adds no value to the data.","e05ea086":"The majority number of cases is found to be maligne. \nThis variable can be set to 0 for maligne and 1 for benigne."}}