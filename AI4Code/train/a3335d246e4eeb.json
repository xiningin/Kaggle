{"cell_type":{"f31c2093":"code","5ffde0ec":"code","f90dfc9a":"code","5c32791d":"code","8377f12e":"code","468dc6ed":"code","6892afa1":"code","955f2b60":"code","c51f9853":"code","86f25a54":"code","f0486758":"code","a0ca428c":"code","dc11b965":"code","e10b8c28":"code","8a07f619":"code","5329c1bb":"code","59765aca":"code","6f86f4f9":"code","38b333e0":"code","3ac322b0":"code","21b45b7f":"code","1db9ad5b":"code","06b84524":"code","291da2d8":"code","dfaeb069":"code","0050085a":"code","01c8c538":"code","768316ea":"code","9de02ce5":"code","3c4635ea":"code","8cea2ae3":"code","80aba67f":"code","406cd7c6":"code","787c5892":"code","c6ad4dfd":"code","9d98d980":"code","832e23ef":"code","6d205ff5":"code","2c4074ab":"markdown","a40047ad":"markdown","5b6a2fe2":"markdown","a5e7967e":"markdown","a652a81e":"markdown","1dba3217":"markdown","d5a8728e":"markdown","5264d216":"markdown","db94d6ae":"markdown","62a6bded":"markdown","78e20663":"markdown","cf10132c":"markdown","edc3ab9b":"markdown","3730109d":"markdown","a4ff02c7":"markdown","b328cffc":"markdown","239b417b":"markdown","11256c74":"markdown","aeddb839":"markdown","cd794863":"markdown","a50370b3":"markdown","c3ab237c":"markdown","da689292":"markdown","d86a8aa6":"markdown","553ff425":"markdown","4f095520":"markdown","4a322ed3":"markdown","6b9dfe97":"markdown","95ee15f5":"markdown","4e97d1df":"markdown","1b4a0d12":"markdown","6c45f469":"markdown","4d347532":"markdown","76edb09f":"markdown","8d4c98bc":"markdown"},"source":{"f31c2093":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt #plottting\nimport seaborn as sns # more plotting\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","5ffde0ec":"# Load in both the training and test data \ntrain_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n# Add the missing column to the test data but just fill it with NaNs so we can separate it later\ntest_data['SalePrice'] = np.nan\n\n# Combine the two dataframes\nall_data = pd.concat([train_data, test_data])\n\n# Check the lengths are as expected\nprint(\"Rows in train data = {}\".format(len(train_data)))\nprint(\"Rows in test data = {}\".format(len(test_data)))","f90dfc9a":"# Check for NaNs in the data\nprint(\"NaNs in each training Feature\")\ndfNull = all_data.isnull().sum().to_frame('nulls')\nprint(dfNull.loc[dfNull['nulls'] > 0]) # Print only features that have Null values","5c32791d":"# Drop the features with lots of NaNs\nall_data.drop(columns=['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], inplace=True)","8377f12e":"# Create mapping which works for Kitchen, Garage and Basement\nmapping = {'Ex': 5.0, 'Gd': 4.0, 'TA': 3.0, 'Fa': 2.0, 'Po': 1.0}\n\n# Remap values\nall_data.replace({'KitchenQual': mapping}, inplace=True)\nall_data.replace({'BsmtQual': mapping}, inplace=True)\nall_data.replace({'BsmtCond': mapping}, inplace=True)\nall_data.replace({'GarageQual': mapping}, inplace=True)\nall_data.replace({'GarageCond': mapping}, inplace=True)","468dc6ed":"# Plot the distributions\nkeys = ['MSZoning', 'Utilities', 'Electrical', 'Functional']\nfig, axs = plt.subplots(4,1, figsize=(8,16))\naxs = axs.flatten()\nfor i in range(len(keys)):\n    all_data[keys[i]].value_counts().plot(kind='bar', rot=20.0, fontsize=16, color='darkblue', \n                                          title=keys[i], ax=axs[i])\nfig.subplots_adjust(hspace=0.5)\nplt.show()","6892afa1":"# MSZoning: Fill the NaNs with RL\nall_data['MSZoning'].fillna('RL', inplace=True)\n\n# Utilities: Drop the feature\nall_data.drop(columns=['Utilities'], inplace=True)\n\n# Electrical: Fill the NaNs with SBrkr\nall_data['Electrical'].fillna('SBrkr', inplace=True)\n\n# Functional: Fill the NaNs with Typ\nall_data['Functional'].fillna('Typ', inplace=True)","955f2b60":"# Plot the distributions\nall_data['Exterior1st'].value_counts().plot(kind='bar', rot=90.0, fontsize=16, color='darkblue', title='Exterior1st')\nplt.show()\n\nall_data['Exterior2nd'].value_counts().plot(kind='bar', rot=90.0, fontsize=16, color='darkblue', title='Exterior2nd')\nplt.show()","c51f9853":"# Check if both exteriors are missing for the same property\nprint(all_data['Exterior2nd'].values[all_data['Exterior1st'].isnull() == True])","86f25a54":"# Fill the NaNs with VinylSd\nall_data['Exterior1st'].fillna('VinylSd', inplace=True)\nall_data['Exterior2nd'].fillna('VinylSd', inplace=True)","f0486758":"# Plot the distribution\nall_data['MasVnrType'].value_counts().plot(kind='bar', rot=90.0, fontsize=16, color='darkblue', title='MasVnrType')\nplt.show()","a0ca428c":"# Find the vales of MasVnrArea when MasVnrType is Nan\nprint(all_data['MasVnrArea'].values[all_data['MasVnrType'].isnull() == True])","dc11b965":"# Find the indices that correspond to the NaNs in MasVnrType but values in MasVnrArea (i.e. the MasVnrArea=198 index)\nindices = (np.argwhere((all_data['MasVnrType'].isnull().values) & (all_data['MasVnrArea'].isnull().values == False))).flatten()\nprint(indices)\n\n# Now fill the MasVnrType corresponding to MasVnrArea != NaN with BrkFace\nall_data.iloc[indices[0], all_data.columns.get_loc('MasVnrType')] = 'BrkFace'\n\n# And fill the remaining MasVnrType and MasVnrArea\nall_data['MasVnrType'].fillna('None', inplace=True)\nall_data['MasVnrArea'].fillna(0.0, inplace=True)","e10b8c28":"# Plot the distributions\nkeys = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','BsmtFullBath', 'BsmtHalfBath']\nfig, axs = plt.subplots(4,2, figsize=(12,16))\naxs = axs.flatten()\nfor i in range(len(keys)):\n    all_data[keys[i]].value_counts().plot(kind='bar', rot=20.0, fontsize=16, color='darkblue', \n                                          title=keys[i], ax=axs[i])\nfig.subplots_adjust(hspace=0.5)\nfig.delaxes(axs[-1]) # remove the extra subplot (only need 7 but created 8)\nplt.show()","8a07f619":"# Fill the BsmtCond values with 3.0\nall_data['BsmtCond'].fillna(3.0, inplace=True)\n\n# Fill the rest\nall_data['BsmtQual'].fillna(3.0, inplace=True)\nall_data['BsmtExposure'].fillna('No', inplace=True)\nall_data['BsmtFinType1'].fillna('Unf', inplace=True)\nall_data['BsmtFinType2'].fillna('Unf', inplace=True)\nall_data['BsmtFullBath'].fillna(0.0, inplace=True)\nall_data['BsmtHalfBath'].fillna(0.0, inplace=True)","5329c1bb":"# Plot the distribution\nall_data['KitchenQual'].value_counts().plot(kind='bar', rot=90.0, fontsize=16, color='darkblue', title='KitchenQual')\nplt.show()","59765aca":"# Find the 5 largest correlations to KitchenQual\nbigCorr = all_data.corr().nlargest(5, 'KitchenQual')['KitchenQual']\nprint(bigCorr)\n\n# Find the 5 largest anti-correlations to KitchenQual\nbigAnti = all_data.corr().nsmallest(5, 'KitchenQual')['KitchenQual']\nprint(bigAnti)","6f86f4f9":"# Calculate the mean of the KitchenQual feature and round to the nearest integer (i.e. nearest quality category)\nmeanKitchenQual = np.rint(all_data.groupby(['OverallQual'])['KitchenQual'].mean())\nprint(meanKitchenQual)\n\n# Check what OverallQual is for the missing KitchenQual value\nprint(all_data['OverallQual'].values[np.isnan(all_data['KitchenQual'].values)])\n\n# Fill the missing KitchenQual value\nall_data['KitchenQual'].fillna(3.0, inplace=True)","38b333e0":"# Plot the categorical features\nfig, axs = plt.subplots(3,2, figsize=(13,15))\naxs = axs.flatten()\nkeys = ['GarageType', 'GarageFinish', 'GarageCars', 'GarageQual', 'GarageCond']\nfor i in range(len(keys)):\n    all_data[keys[i]].value_counts().plot(kind='bar', rot=40.0, fontsize=16, color='darkblue', \n                                                   title=keys[i], ax=axs[i])\nfig.subplots_adjust(hspace=0.5)\nfig.delaxes(axs[-1]) # remove the extra subplot (only need 5 but created 6)\nplt.show()","3ac322b0":"# Fill with most common values \nall_data['GarageType'].fillna('Attchd', inplace=True)\nall_data['GarageFinish'].fillna('Unf', inplace=True)\nall_data['GarageCars'].fillna(2.0, inplace=True)\nall_data['GarageQual'].fillna(3.0, inplace=True)\nall_data['GarageCond'].fillna(3.0, inplace=True)","21b45b7f":"# Sort this plotting out so it looks good\nkeys = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF']\n# Plot the continuous distributions\nfig, axs = plt.subplots(2,2, figsize=(12,12))\naxs = axs.flatten()\nfor i in range(len(keys)):\n    all_data[keys[i]].hist(color='darkblue', ax=axs[i], bins=40)\n    axs[i].set_title(keys[i])\nfig.subplots_adjust(hspace=0.5)\nplt.show()","1db9ad5b":"# Check which indices are NaN for each features\nfor i in keys:\n    print(np.argwhere(np.isnan(all_data[i].values)))","06b84524":"# Calculate median\nmedian_total = np.median(all_data['TotalBsmtSF'].values[~np.isnan(all_data['TotalBsmtSF'].values)])\nmedian_SF1 = np.median(all_data['BsmtFinSF1'].values[~np.isnan(all_data['BsmtFinSF1'].values)])\nmedian_Unf = np.median(all_data['BsmtUnfSF'].values[~np.isnan(all_data['BsmtUnfSF'].values)])\nprint(median_total, median_SF1, median_Unf)\n\n# Fill the missing values\nall_data['BsmtFinSF1'].fillna(median_SF1, inplace=True)\nall_data['BsmtFinSF2'].fillna(0.0, inplace=True)\nall_data['BsmtUnfSF'].fillna(median_Unf, inplace=True)\nall_data['TotalBsmtSF'].fillna(median_total, inplace=True)","291da2d8":"keys = [ 'GarageYrBlt', 'GarageArea']\n# Plot the distribution\nall_data['GarageArea'].hist(color='darkblue', bins=20)\nplt.show()","dfaeb069":"# Find the NaNs\ngarage_yr_built = all_data['GarageYrBlt'].values\nindices = np.argwhere(np.isnan(garage_yr_built))\n\n# Update the data\nyearbuilt = all_data['YearBuilt'].values[indices]\ngarage_yr_built[indices] = yearbuilt\n\n# Save to the dataframe\nall_data['GarageYrBlt'] = garage_yr_built\n\n\n# Fill area with median\nmedian = np.median(all_data['GarageArea'].values[~np.isnan(all_data['GarageArea'].values)])\nprint(median)\nall_data['GarageArea'].fillna(median, inplace=True)","0050085a":"# Plot the distribution\nall_data['SaleType'].value_counts().plot(kind='bar', rot=90.0, fontsize=16, color='darkblue', title='SaleType')\nplt.show()\n\n# Fill with the most common value\nall_data['SaleType'].fillna('WD', inplace=True)","01c8c538":"# Fill Frontage with median\nmedian = np.median(all_data['LotFrontage'].values[~np.isnan(all_data['LotFrontage'].values)])\nprint(median)\nall_data['LotFrontage'].fillna(median, inplace=True)","768316ea":"# Check everything has been filled as expected\nprint(all_data.isnull().sum().values)","9de02ce5":"# ==== Get features ready for one-hot encoding\n# MoSold\nmapping = {1: 'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', \n           8:'Aug', 9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'}\nall_data.replace({'MoSold': mapping}, inplace=True)\n\n# MSSubClass\nmapping = {20:'A', 30:'B', 40:'C', 45:'D', 50:'E', 60:'F', 70:'G', 75:'H', 80:'I', 85:'J', \n           90:'K', 120:'L', 150:'M', 160:'N', 180:'O', 190:'P'}\nall_data.replace({'MSSubClass': mapping}, inplace=True)\n\n\n# ==== Create some new features\nall_data['sale_age'] = 2020 - all_data['YrSold'] \nall_data['house_age'] = 2020 - all_data['YearBuilt'] \nall_data['remodel_age'] = 2020 - all_data['YearRemodAdd']\nall_data['garage_age'] = 2020 - all_data['GarageYrBlt']\nall_data.drop(columns=['YrSold', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt'], inplace=True)\n\n\nall_data['TotalArea'] = (all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n                         + all_data['GrLivArea'] + all_data['GarageArea'])\nall_data['TotalBathrooms'] = all_data['FullBath'] + all_data['HalfBath']*0.5 ","3c4635ea":"# Specify which columns to one hot encode\ncolumns = ['MSZoning', 'MSSubClass', 'PavedDrive', 'GarageFinish', 'Foundation', 'Functional', 'LandContour', 'Condition1', \n           'Condition2', 'Street', 'LotShape', 'ExterQual', 'ExterCond', 'LotConfig', 'LandSlope', 'Neighborhood','BldgType',\n           'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st','Exterior2nd', 'MasVnrType', 'BsmtExposure', 'BsmtFinType1',\n           'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'GarageType', 'SaleType', 'SaleCondition', \n           'MoSold']\n\n# Do the encoding\none_hot = pd.get_dummies(all_data.loc[:, columns], drop_first=True)\nall_data.drop(columns=columns, inplace=True)\nfor i in range(len(one_hot.columns.values)):\n    all_data[one_hot.columns.values[i]] = one_hot[one_hot.columns.values[i]].values","8cea2ae3":"# Separate the data into prediction and model data\npredict_data = all_data.loc[np.isnan(all_data['SalePrice'].values)]\nmodel_data = all_data.loc[~np.isnan(all_data['SalePrice'].values)]\npredict_ids = predict_data['Id'].values\n\n# Remove ID so that the model is not trained on this. We add this back into the data before submission\npredict_data.drop(columns=['Id'], inplace=True)\nmodel_data.drop(columns=['Id'], inplace=True)","80aba67f":"# Remove outliers from the training data \nfrom collections import Counter\n\ndef detect_outliers(df,n,features):\n    outlier_indices = []\n    for col in features:\n        Q1 = np.percentile(df[col], 25) # 1st quartile (25%)\n        Q3 = np.percentile(df[col],75) # 3rd quartile (75%)\n        IQR = Q3 - Q1 # Interquartile range (IQR)\n        outlier_step = 1.5 * IQR # outlier step\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n    # select observations containing more than n outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers \n\n# Detect outliers from SalePrice, LotArea, GarageArea\nOutliers_to_drop = detect_outliers(model_data,1,[\"SalePrice\", 'LotArea', 'GarageArea'])\nmodel_data = model_data.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","406cd7c6":"# Scale the data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(model_data.drop(columns=['SalePrice']))\nX_train = scaler.transform(model_data.drop(columns=['SalePrice']))\ny_train = model_data['SalePrice'] \n\nX_predict_data_scaled = scaler.transform(predict_data.drop(columns=['SalePrice']))","787c5892":"# Import some required modules for the fitting\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV","c6ad4dfd":"\"\"\"\n# Search for the optimal hyperparameters for RF\nparam_grid = { \n    'criterion' : ['mse'],\n    'n_estimators': [90, 100, 110],\n    'max_features': ['auto', 'log2'],\n    'max_depth' : [7, 9, 11, 13]    \n                }\n\nrandomForest_CV = GridSearchCV(estimator = RandomForestRegressor(), param_grid = param_grid, cv = 3)\nrandomForest_CV.fit(X_train, y_train)\nprint(randomForest_CV.best_params_)\n\n# Print the best score\nprint(randomForest_CV.best_score_)\n\n# Get the parameters from the best fit\ncriterion = randomForest_CV.best_params_.get('criterion')\nnRF = randomForest_CV.best_params_.get('n_estimators')\nmax_features = randomForest_CV.best_params_.get('max_features')\nmax_depth = randomForest_CV.best_params_.get('max_depth')\n\n# Train the classifier on all the data for predicting the test data (use best hyperparams)\nbestRF = RandomForestRegressor(n_estimators=nRF, max_depth=max_depth, max_features=max_features, criterion=criterion)\nbestRF.fit(X_train, y_train)\n\"\"\"","9d98d980":"\"\"\"\nimport xgboost as xgb\nxgboost = xgb.XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                    max_depth=3)\nxgboost.fit(X_train,y_train)\n\"\"\"","832e23ef":"import tensorflow as tf\n\nmodel = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(500, activation=tf.nn.relu, input_dim = X_train.shape[1]))\nmodel.add(tf.keras.layers.Dense(500, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(500, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(500, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(1, activation='linear'))\nmodel.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])\n\n# Use 20% of data for validation \ntraining = model.fit(X_train, y_train, epochs=20, validation_split=0.1)\n\n#import matplotlib.pyplot as plt\n#plt.figure()\n#plt.plot(training.history['loss'])\n#plt.plot(training.history['val_loss'])","6d205ff5":"# Use the models to predict the output of the test data\n#testPredictionsRF = bestRF.predict(X_predict_data_scaled)\n#testPredictionsXGB = xgboost.predict(X_predict_data_scaled)\ntestPredictionsNN = model.predict(X_predict_data_scaled)\n\n\n# Save the outputs to csv files\n\n#outputRF = pd.DataFrame({'Id': predict_ids, 'SalePrice': testPredictionsRF})\n#outputRF.to_csv('my_submissionRF.csv', index=False)\n\noutputNN = pd.DataFrame({'Id': predict_ids, 'SalePrice': testPredictionsNN.flatten()})\noutputNN.to_csv('my_submissionNN.csv', index=False)\n\n#outputXGB = pd.DataFrame({'Id': predict_ids, 'SalePrice': testPredictionsXGB})\n#outputXGB.to_csv('my_submissionXGB.csv', index=False)","2c4074ab":"Now check if the NaNs are all for the same property","a40047ad":"Okay. So we can see that the missing values are all for the same property. Let's just fill the values with 0 for BsmtFinSF2. We can fill the missing values of BsmtFinSF1, TotalBsmtSF and BsmtUnfSF with the median of the distribution.","5b6a2fe2":"We can see that most of the MasVnrArea values are NaN too, but there is one that is not. Let's fill the MasVnrArea=198 case with the most common MasVnrType (which is BrkFace) and fill the rest with MasVnrArea=0.0 and MasVnrType=None.","a5e7967e":"Check the KitchenQual correlations","a652a81e":"# 5. Generating Submission Files\n\nNow that we have trained our models, we need to use them to make predictions and then generate submission files with these predictions.","1dba3217":"Okay so we know that OverallQual is closely correlated to the KitchenQual and so we can fill missing values using the OverallQual.\n\n","d5a8728e":"### 3.2.6. Garage Categorical Features","5264d216":"4.4. Neural Network","db94d6ae":"# 2. Data Cleaning\n\nLet's start by removing columns that have a lot of missing data and therefore would be very difficult to fill. This just keeps the dataframe smaller and easier to handle.","62a6bded":"### 3.2.2. Exterior1st and Exterior2nd (1 missing value each)","78e20663":"And that's everything for now. This notebook (as of writing) scores in the top 32% (XGBoost model), which is not bad considering that we have not done anything too complicated. **If you have made it this far, please give the notebook an upvote! It really helps.**","cf10132c":"If GarageYrBlt is not available just set this as the same year the house was built (YearBuilt)","edc3ab9b":"### 3.2.5. KitchenQual (1 missing value)","3730109d":"Now as BsmtCond is almost always 3.0, let's replace the NaNs with 3.0. Let's fill the rest with the most common values too.","a4ff02c7":"### 3.2.3. MasVnrType and MasVnrArea (24 and 23 missing values respectively)","b328cffc":"## 4.3. XGBoost Regression\n\nNow we can also try the XGBoostRegressor and see how this performs. We haven't bothered tuning the hyperparamters here but this is something that could be implemented.","239b417b":"# 4. Fitting Models to the Data\n\n## 4.1. Preparing the Data\n\nWe now need to get our data ready for the model. To do this, we should separate the data used for training and testing our model from the data used to make house price predictions.","11256c74":"Okay so this tells us that the Exterior features are missing for the same property. This is also one of the properties that we are trying to predict the SalePrice of so we cannot remove it. Let's just fill it with the most common Exterior value for now.","aeddb839":"### 3.2.10. LotFrontage (486 missing values)","cd794863":"### 3.2.8. Garage Continuous Features","a50370b3":"The over whelming majority of data for the MSZoning feature takes a value of \"RL\" so let's fill the missing data with this value. As we only have 4 data points to fill this is probably sufficient for this feature but we could always come back and do a more technical filling of the data based on correlations.\n\nAs the Utilities feature almost always takes the value \"AllPub\" we can drop this feature rather than filling in the missing values. This is because if all the data takes the same value of a feature, that feature does not provide any informaion for the model.\n\nAs we only have one missing Electrical value we can fill it with the most common value (\"SBrkr\").\n\nThe Functional feature takes the value \"Typ\" the majority of the time so let's fill the missing values with this.","c3ab237c":"# House Price Competition - Predicting Prices\n\nThis notebook is designed to predict prices for the House Price Competition. If you use this notebook or find it helpful, **please give it an upvote!**\n\nI have created a step-by-step data exploration notebook for this competition [here](https:\/\/www.kaggle.com\/thomaswoolley\/house-price-data-exploration). If you have not read that one, I would strongly recommend doing so before working through this notebook.\n\nIf you like my work my other, notebooks on various topics can be found on Kaggle [here](https:\/\/www.kaggle.com\/thomaswoolley\/notebooks) and GitHub [here](https:\/\/github.com\/Woolley12345\/kaggle-competitions).\n\n\n### Competition Overview\n\nThe aim of this competition is to predict the sales price of various houses given a number of features. This makes this a regression problem (predicting a continuous variable e.g. price) rather than a classification problem (predicting distinct classes\/groups e.g. survived or not). In order to approach this competition, it was essential to first understand the data that we are working with. This notebook builds on that understanding and uses it to make predictions about the final house prices.\n\n### Notebook Structure\nThis notebook is structured as follows:\n\n1. Load Modules and Data\n2. Data Cleaning\n3. Feature Engineering\n4. Fitting Models to the Data\n5. Generating Submission Files\n\n\n\n# 1. Load Modules and Data\n\nAs the first step, we have to load in the data that we want to use. It is also important to load in some modules. Modules help us to do additional things with Python such as plotting, mathematics and data processing. Some commonly used modules are imported below to help us explore the data we have been given. For more information on the modules that are used here, check out the documentation at the following links:\n\n* [Numpy](https:\/\/numpy.org)\n* [Pandas](https:\/\/pandas.pydata.org)\n* [MatplotLib](https:\/\/matplotlib.org)\n* [Seaborn](https:\/\/seaborn.pydata.org)","da689292":"### 3.2.9. SaleType (1 missing value)","d86a8aa6":"We can also remove some outliers to improve how our model is trained. This code is adapted from this notebook: https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling","553ff425":"Here we can see that there are no NaNs except for the last feature. This last feature is the SalePrice that we are trying to predict for the test set.\n\n## 3.3. Expanding Existing Features\n\nLet's expand some of the features which could provide some helpful information for the model. This part can take the most time and can really improve model accuracy. Here we will just do some fairly simple things but it is possible to do much more advanced feature enegineering.","4f095520":"Fill with the most common result which is None. This also means the MasVnrArea feature will be zero for these if it takes a NaN value. Let's check what values of the MasVnrArea feature are available when we have NaNs for the MasVnrType feature.","4a322ed3":"### 3.2.4. Basement Categorical Features\nThe categorical features relating to the basement of the houses are:\n1. BsmtQual (81 missing values)\n2. BsmtCond (82 missing values)\n3. BsmtExposure (82 missing values)\n4. BsmtFinType1 (79 missing values)\n5. BsmtFinType2 (80 missing values)\n6. BsmtFullBath (2 missing values)\n7. BsmtHalfBath (2 missing values)\n\nLet's check out the distributions of these.","6b9dfe97":"## 3.1. Converting Quality and Condition to Numerical Fields\nThere are some features which have quality and condition measured as a number and others with them measured with text-based categories. Here we should convert them all to numbers to make it more consistent. The features that we remap in this section are:\n1. KitchenQual\n2. BsmtQual\n3. BsmtCond\n4. GarageQual\n5. GarageCond","95ee15f5":"## 3.4. Handling Categorical Text Data \n\nOne way of handling categorical text data is to use one-hot encoding. This creates a new column for each possible value of a feature. The columns are filled with a 1 or a 0 depending on whether the feature takes that value.\n\nBelow we do one-hot encoding on all the text based categorical features in the dataset.","4e97d1df":"Now we need to scale the training and prediction datasets by the same amount. This scales all the data to take values between 0 and 1, which ensures features with larger numerical values are not incorrectly favoured over features with smaller numerical values.","1b4a0d12":"## 4.2. Random Forest Regression\n\nLet's now fit the RandomForestRegressor to the model dataset in order to make predictions about the house prices. First we need to import the required modules. We use GridSearchCV here to search for the best model hyperparameters to fit the data. ","6c45f469":"## 3.2. Filling Missing Values\n\nIn this section we fill the missing values. Most of the missing values are filled using the most common value for brevity however, we could test out other ways of filling missing values to improve the models. This has been left for future notebooks.\n\n### 3.2.1. MSZoning, Utilities and Electrical (4, 2 and 1 missing values respectively)\nLet's start with these features and check their distributions.\n","4d347532":"### 3.2.7. Basement Continuous Features\nThe following continuous features related to the basement:\n1. BsmtFinSF1 (1 missing value)\n2. BsmtFinSF2 (1 missing value)\n3. BsmtUnfSF (1 missing value)\n4. TotalBsmtSF (1 missing value)","76edb09f":"# 3. Feature Engineering\n\nLet's fill some of the missing values and make the data ready for models.","8d4c98bc":"The output of the above cell provides us with the paths to the data files that we wish to import. They are saved as *.csv* files and so all we have to do is use the *read_csv* option in the *Pandas* module. The following code saves the data as a *Pandas Dataframe* (you can imagine it like a table of data) with the variable name *all_data*. \n\nWe choose to combine the training and test data here so that we only have to write the feature engineering code once. However, we will track the test data (as we can fill the SalePrice features with NaN values in the Dataframe), and separate it before fitting our models."}}