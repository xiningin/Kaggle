{"cell_type":{"e7567851":"code","23300242":"code","b9b37af9":"code","b5127729":"code","2eefa594":"code","7a9768f8":"code","a5fd75a0":"code","698d8b60":"code","1fc149e2":"code","8c7a866a":"code","daf65f3f":"code","d9afb463":"code","e7f57a47":"code","1be8263b":"code","327a76d2":"code","a5e511e3":"markdown","7566697c":"markdown","c7cad674":"markdown","9ab693ca":"markdown","da88bf9a":"markdown","9be21cee":"markdown","418ab2db":"markdown","88cf1c23":"markdown","301472c0":"markdown","c3bd30dc":"markdown","a03c42ce":"markdown","f1428309":"markdown","100172f7":"markdown","89e93bde":"markdown"},"source":{"e7567851":"import warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Dense, Dropout, BatchNormalization, Activation\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\n# Set seeds to make the experiment more reproducible.\nfrom tensorflow import set_random_seed\nfrom numpy.random import seed\nset_random_seed(0)\nseed(0)\n\n%matplotlib inline\nsns.set(style=\"whitegrid\")\nwarnings.filterwarnings(\"ignore\")","23300242":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","b9b37af9":"print('Train set shape:', train.shape)\nprint('Test set shape:', test.shape)\nprint('Train set overview:')\ndisplay(train.head())","b5127729":"f, ax = plt.subplots(figsize=(6, 6))\nax = sns.countplot(x=\"target\", data=train, label=\"Label count\")\nsns.despine(bottom=True)","2eefa594":"def plot_distribution():\n    f, axes = plt.subplots(1, 3, figsize=(20, 8), sharex=True)\n    for feature in train.columns[1:31]:\n        sns.distplot(train[feature], ax=axes[0], axlabel='First 30 features').set_title(\"Complete set\")\n        sns.distplot(train[train['target']==1][feature], ax=axes[1], axlabel='First 30 features').set_title(\"target = 1\")\n        sns.distplot(train[train['target']==0][feature], ax=axes[2], axlabel='First 30 features').set_title(\"target = 0\")\n    sns.despine(left=True)\n    plt.tight_layout()\n\nplot_distribution()","7a9768f8":"train = pd.concat([train, pd.get_dummies(train['wheezy-copper-turtle-magic'], prefix='magic', drop_first=True)], axis=1).drop(['wheezy-copper-turtle-magic'], axis=1)\ntest = pd.concat([test, pd.get_dummies(test['wheezy-copper-turtle-magic'], prefix='magic', drop_first=True)], axis=1).drop(['wheezy-copper-turtle-magic'], axis=1)","a5fd75a0":"labels = train['target']\ntrain.drop('target', axis=1, inplace=True)\ntrain.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\nX_train, X_val, Y_train, Y_val = train_test_split(train, labels, test_size=0.2, random_state=1)","698d8b60":"non_cat_features = list(train.filter(regex='^(?!magic_)'))\nscaler = MinMaxScaler()\nX_train[non_cat_features] = scaler.fit_transform(X_train[non_cat_features])\nX_val[non_cat_features] = scaler.transform(X_val[non_cat_features])\ntest[non_cat_features] = scaler.transform(test[non_cat_features])","1fc149e2":"BATCH_SIZE = 128\nEPOCHS = 30\nLEARNING_RATE = 0.01\nES_PATIENCE = 5","8c7a866a":"model = Sequential()\nmodel.add(Dense(1024, input_dim=X_train.shape[1]))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=ES_PATIENCE)\ncallback_list = [es]\n\noptimizer = optimizers.Adam(lr=0.0001)\nmodel.compile(optimizer=optimizer, loss=\"binary_crossentropy\",  metrics=['binary_accuracy'])\nmodel.summary()","daf65f3f":"history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n                    callbacks=callback_list, \n                    epochs=EPOCHS, \n                    batch_size=BATCH_SIZE, \n                    verbose=2)","d9afb463":"sns.set_style(\"whitegrid\")\nfig, (ax1, ax2) = plt.subplots(1, 2, sharex='col', figsize=(20, 7))\n\nax1.plot(history.history['loss'], label='Train loss')\nax1.plot(history.history['val_loss'], label='Validation loss')\nax1.legend(loc='best')\nax1.set_title('Loss')\n\nax2.plot(history.history['binary_accuracy'], label='Train Accuracy')\nax2.plot(history.history['val_binary_accuracy'], label='Validation accuracy')\nax2.legend(loc='best')\nax2.set_title('Accuracy')\n\nplt.xlabel('Epochs')\nsns.despine()\nplt.show()","e7f57a47":"train_pred = model.predict_classes(X_train)\nval_pred = model.predict_classes(X_val)\n\nf, axes = plt.subplots(1, 2, figsize=(16, 5), sharex=True)\ntrain_cnf_matrix = confusion_matrix(Y_train, train_pred)\nval_cnf_matrix = confusion_matrix(Y_val, val_pred)\n\ntrain_cnf_matrix_norm = train_cnf_matrix \/ train_cnf_matrix.sum(axis=1)[:, np.newaxis]\nval_cnf_matrix_norm = val_cnf_matrix \/ val_cnf_matrix.sum(axis=1)[:, np.newaxis]\n\ntrain_df_cm = pd.DataFrame(train_cnf_matrix_norm, index=[0, 1], columns=[0, 1])\nval_df_cm = pd.DataFrame(val_cnf_matrix_norm, index=[0, 1], columns=[0, 1])\n\nsns.heatmap(train_df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=axes[0]).set_title(\"Train\")\nsns.heatmap(val_df_cm, annot=True, fmt='.2f', cmap=\"Blues\", ax=axes[1]).set_title(\"Validation\")\nplt.show()","1be8263b":"print('Train AUC %.2f' % roc_auc_score(Y_train.values, train_pred))\nprint('Validation AUC %.2f' % roc_auc_score(Y_val.values, val_pred))","327a76d2":"predictions = model.predict(test)\ndf = pd.read_csv('..\/input\/sample_submission.csv')\ndf['target'] = predictions\ndf.to_csv('submission.csv', index=False)\ndf.head(10)","a5e511e3":"# Dependencies","7566697c":"# Model evaluation\n\n## Confusion matrix","c7cad674":"### Train\/validation random split (80% train \/ 20% validation)","9ab693ca":"# Load data","da88bf9a":"# Model\n\n## Model parameters","9be21cee":"## Metrics ROC AUC","418ab2db":"### Normalize data using MinMaxScaler","88cf1c23":"# EDA","301472c0":"The features seems to be normalized.\n\n## Process data for model\n\n### Turn \"wheezy-copper-turtle-magic\" into a categorical feature","c3bd30dc":"## Target distribution","a03c42ce":"Also we have a balanced target distribution.\n\n## Feature distribution","f1428309":"### Model graph loss","100172f7":"On both our train and test sets all columns are numerical, and we also don't have any missing data.","89e93bde":"# Test predictions"}}