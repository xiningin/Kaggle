{"cell_type":{"a8367a88":"code","314046c2":"code","4f0d4217":"code","ae1a9a91":"code","9df66766":"code","114f68e8":"code","62f86efe":"code","d2c6843f":"code","fd35432a":"code","a0bb302a":"code","972c6b09":"code","7ee53308":"code","07d1147f":"code","5ec88532":"code","99f4928e":"code","3d0eba65":"code","30119f15":"code","b3f85692":"code","65c3b305":"code","3cb52ef5":"code","74cf1ea6":"code","dfb521f3":"code","29d82957":"code","309fa571":"code","6c85f2e7":"code","787d0b2f":"code","e4857dd3":"code","5c70bb32":"code","ccac29e1":"code","32c8420c":"markdown","d1374812":"markdown","53632006":"markdown","18ace798":"markdown","a6b84ffa":"markdown","818c632b":"markdown","79265152":"markdown","30dd53ad":"markdown","94ba2b27":"markdown","cdb1ebba":"markdown","b4f451d8":"markdown","f4f8610c":"markdown"},"source":{"a8367a88":"# LOAD LIBRARIES\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport gc\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\n\nfrom time import time\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nimport lightgbm as lgb\nimport sklearn.utils\n\nimport eli5\nfrom IPython.display import display\nfrom eli5.permutation_importance import get_score_importances\nfrom eli5.sklearn import PermutationImportance","314046c2":"# DATA CHECK\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# SET RANDOM SEED\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)    \n\n# CONSTANT VARIABLES\nSEED = 42\nNFOLDS = 3\nMODEL_TEST = True\nTARGET = 'income'\n\nseed_everything(SEED)","4f0d4217":"# reduce_mem_usage()\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2   \n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n                    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    \n    if verbose: \n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n        \n    return df","ae1a9a91":"# DEFINE BASE PATH\nBASE_PATH = '\/kaggle\/input\/kakr-4th-competition\/'\n\n# LOAD DATASET\ndf_train = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\ndf_test  = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))\n\ndf_train[TARGET] = np.where(df_train[TARGET] == '>50K', 1, 0)\n#df_train = df_train.drop('income', axis=1)\n\nprint('df_train shape is: ', df_train.shape)\nprint('df_test  shape is: ', df_test.shape)","9df66766":"# LIST OF FEATURE\nremove_features = [\n    'id', 'education_num',\n    TARGET\n]\n\nbase_columns = [col for col in list(df_train) if col not in remove_features]\nbase_columns","114f68e8":"# MODEL PARAMETERS\nparams = {\n            'objective': 'binary',          \n            'boosting_type': 'gbdt',        \n            'n_jobs': -1,                   \n            'learning_rate': 0.01,          \n            'num_leaves': 2**7,             \n            'max_depth': -1,                \n            'min_child_samples': 30,        \n            'colsample_bytree': 0.7,       \n            'subsample_freq': 1,            \n            'subsample': 0.7,             \n            'n_estimators': 10000,         \n            'verbose': -1,                \n            'seed': SEED,\n}","62f86efe":"# SCIKIT-LEARN WRAPPER LGB F1 SCORE METRIC\ndef lgb_f1_score(y_hat, data):\n    y_true = np.round(data).astype(int)\n    y_hat  = np.round(y_hat).astype(int)\n    \n    f1 = f1_score(y_true, y_hat, average='micro')\n    \n    return ('micro f1', f1, True)\n\n# PYTHON WRAPPER LGB F1 SCORE METRIC\n# def lgb_f1_score(y_hat, data):\n#     y_true = data.get_label()\n#     y_hat  = np.round(y_hat)\n    \n#     return 'f1', f1_score(y_true, y_hat, average='micro'), True","d2c6843f":"## MODEL\ndef make_predictions(tr_df, tt_df, target, lgb_params):\n    base_columns = [col for col in list(tr_df) if col not in remove_features]\n    new_columns = set(list(tr_df)).difference(base_columns + remove_features)\n    features_columns = base_columns + list(new_columns)\n    print(\"##### Feature List #####\")\n    print(list(features_columns))\n    print('\\n')\n    \n    tr_df = reduce_mem_usage(tr_df)\n    tt_df = reduce_mem_usage(tt_df)\n    \n    df_perm_importance = pd.DataFrame()\n    tt_df[target] = 0\n    \n    # LABEL ENCODING\n    for col in features_columns:\n        if tr_df[col].dtype.name == 'object' or tt_df[col].dtype.name == 'object':\n            le = LabelEncoder()\n            le.fit(list(tr_df[col].values) + list(tt_df[col].values))\n            tr_df[col] = le.transform(list(tr_df[col].values))\n            tt_df[col] = le.transform(list(tt_df[col].values))\n    \n    # StratifiedKFold\n    folds = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    \n    X_tr, y_tr = tr_df[features_columns], tr_df[target]       \n    X_tt, y_tt = tt_df[features_columns], tt_df[target]       \n    \n    tt_df = tt_df[['id', target]]\n    predictions = np.zeros(len(tt_df))\n    oof = np.zeros(len(tr_df))\n    score = 0     \n\n    for fold, (tr_idx, vl_idx) in enumerate(folds.split(X_tr, y_tr)):\n        print('#'*10 + ' Fold: ', fold+1)\n        \n        tr_x, tr_y = X_tr.iloc[tr_idx, :], y_tr[tr_idx]       \n        vl_x, vl_y = X_tr.iloc[vl_idx, :], y_tr[vl_idx]       \n        print(len(X_tr), len(tr_x), len(vl_x))\n\n        clf = lgb.LGBMClassifier(\n            **lgb_params, \n            silent=-1,\n            metric = 'None',\n        )\n        \n        clf.fit(\n            tr_x, tr_y,\n            eval_set = [(tr_x, tr_y), (vl_x, vl_y)],\n            verbose = 100,\n            early_stopping_rounds = 200,\n            eval_metric = lgb_f1_score\n        )\n        \n        tr_y_pred = clf.predict(tr_x)\n        vl_y_pred = clf.predict(vl_x)\n        \n        tr_y_pred = [int(v >= 0.5) for v in tr_y_pred]\n        vl_y_pred = [int(v >= 0.5) for v in vl_y_pred]\n        \n        oof[vl_idx] = vl_y_pred\n        \n        print(f\"\\nFold {fold + 1}\")\n        print(f\"Train f1 score: {f1_score(tr_y, tr_y_pred, average='micro')}\")\n        print(f\"Valid f1 score: {f1_score(vl_y, vl_y_pred, average='micro')}\\n\")\n        \n        score += f1_score(vl_y, vl_y_pred, average='micro') \/ NFOLDS\n        predictions += clf.predict(X_tt) \/ NFOLDS\n        \n        # FEATURE IMPORTANCE\n        feature_imp = pd.DataFrame(sorted(zip(clf.feature_importances_, X_tr.columns), reverse=True), columns=['Value', 'Feature'])\n        print(feature_imp)\n        \n        # PERMUTATION IMPORTANCE\n        perm = PermutationImportance(clf, scoring='f1', n_iter=1, random_state=42, cv=None, refit=False).fit(vl_x, vl_y, verbose=100)\n        tmp = eli5.show_weights(perm)\n        display(eli5.show_weights(perm, top=len(list(X_tr)), feature_names = list(features_columns)))\n        \n        df_perm_fold = eli5.explain_weights_df(perm, top = len(list(features_columns)), feature_names = list(features_columns))\n        df_perm_fold['fold'] = fold + 1\n        df_perm_importance = pd.concat([df_perm_importance, df_perm_fold], axis=0)\n    \n        del tr_x, tr_y, vl_x, vl_y, clf\n        gc.collect()\n        \n    tt_df[target] = predictions\n    print(f'\\nMean F1 score = {score}')\n    print(f\"OOF F1 score = {f1_score(y_tr, oof, average='micro')}\\n\")\n    \n    pd.set_option('display.max_rows', len(df_perm_importance['feature']))\n    df_perm_importance = df_perm_importance.drop(['fold'], axis=1)\n    df_perm_importance = df_perm_importance.groupby(['feature'], as_index=False).mean()\n    df_perm_importance['feature rank'] = df_perm_importance['weight'].rank(ascending=0)\n    df_perm_importance = df_perm_importance.sort_values('feature rank', ascending=True)\n    print(df_perm_importance)\n    \n    return tt_df","fd35432a":"df_train.loc[df_train['occupation'].isin(['Armed-Forces', 'Priv-house-serv']), 'occupation'] = 'Priv-house-serv'\ndf_test.loc[df_test['occupation'].isin(['Armed-Forces', 'Priv-house-serv']), 'occupation'] = 'Priv-house-serv'\n\nworkclass_other = ['Without-pay', 'Never-worked']\ndf_train['workclass'] = df_train['workclass'].apply(lambda x: 'Other' if x in workclass_other else x)\ndf_test['workclass'] = df_test['workclass'].apply(lambda x: 'Other' if x in workclass_other else x)","a0bb302a":"print(df_train[(df_train['sex'] == 'Female') & (df_train['relationship'] == 'Husband')])\nidx = df_train[(df_train['sex'] == 'Female') & (df_train['relationship'] == 'Husband')].index.values\ndf_train.drop(idx, inplace = True)\n\nprint('=' * 20)\nprint(df_train[(df_train['sex'] == 'Male') & (df_train['relationship'] == 'Wife')])\nidx = df_train[(df_train['sex'] == 'Male') & (df_train['relationship'] == 'Wife')].index.values\ndf_train.drop(idx, inplace = True)\n\ndf_train.reset_index(drop=True, inplace=True)","972c6b09":"has_na_columns = ['workclass', 'occupation', 'native_country']","7ee53308":"for c in has_na_columns:\n    df_train.loc[df_train[c] == '?', c] = df_train[c].mode()[0]\n    df_test.loc[df_test[c] == '?', c] = df_test[c].mode()[0]","07d1147f":"# Mean F1 score = 0.8709352740661096\n# OOF F1 score = 0.8709355445506546\nif MODEL_TEST:\n    test_predictions = make_predictions(df_train, df_test, TARGET, params)\n    test_predictions[TARGET] = [int(v >= 0.5) for v in test_predictions[TARGET]]","5ec88532":"# AGE GROUP\ndef age_group(age):\n    if age <= 29:\n        return 'youth'\n    elif age <= 59:\n        return 'middle-age'\n    else:\n        return 'old'\n    \ndf_train['age_group'] = df_train['age'].apply(age_group)\ndf_test['age_group'] = df_test['age'].apply(age_group)\n\n\n# EDUCATION GROUP\nedu_group_3 = ['Bachelors', 'Doctorate', 'Prof_school', 'Masters']\nedu_group_2 = ['HS-grad', 'Some-college', 'Assoc_acdm', 'Assoc_voc']\nedu_group_1 = ['Preshool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th']\n\ndef education_group(edu):\n    if edu in edu_group_1:\n        return 'group_1_education'\n    elif edu in edu_group_2:\n        return 'group_2_education'\n    else:\n        return 'group_3_education'\n\ndf_train['education_group'] = df_train['education'].apply(education_group)\ndf_test['education_group'] = df_test['education'].apply(education_group)\n\n# MARITAL STATUS GROUP\nlive_together = ['Married-civ-spouse', 'Married-AF-spouse']\nlive_solo = ['Divorced', 'Married-spouse-absent', 'Never-married', 'Separated', 'Widowed']\n\ndef marital_status_group(m):\n    if m in live_solo:\n        return 'group_1_marital'\n    else:\n        return 'group_2_marital'\n\ndf_train['marital_status_group'] = df_train['marital_status'].apply(marital_status_group)\ndf_test['marital_status_group'] = df_test['marital_status'].apply(marital_status_group)\n\n# WORKCLASS GROUP\npublic = ['State-gov', 'Local-gov', 'Federal-gov']\nself_emp = ['Self-emp-not-inc', 'Self-emp-inc']\nprivate = ['Private', 'Other']\n\ndef workclass_group(p):\n    if p in private:\n        return 'private'\n    elif p in public:\n        return 'public'\n    else:\n        return 'self_emp'\n\ndf_train['workclass_group'] = df_train['workclass'].apply(workclass_group)\ndf_test['workclass_group']  = df_test['workclass'].apply(workclass_group)\n\n# OCCUPATION\noccupation_group_1 = ['Priv-house-serv', 'Other-service', 'Handlers-cleaners' '?', 'Farming-fishing', 'Machine-op-inspct', 'Adm-clerical']\noccupation_group_2 = ['Transport-moving', 'Craft-repair', 'Sales', 'Tech-support', 'Protective-serv']\noccupation_group_3 = ['Exec-managerial', 'Prof-specialty']\n\ndef occupation_group(o):\n    if o in occupation_group_1:\n        return 'group1'\n    elif o in occupation_group_2:\n        return 'group2'\n    else:\n        return 'group3'\n\ndf_train['occupation_group'] = df_train['occupation'].apply(occupation_group)\ndf_test['occupation_group'] = df_test['occupation'].apply(occupation_group)\n\n\n# RELATIONSHIP\ncouple = ['Husband', 'Wife']\n\ndef relationship_group(c):\n    if c in couple:\n        return 'couple'\n    else:\n        return 'no_couple'\n\ndf_train['relationship_group'] = df_train['relationship'].apply(relationship_group)\ndf_test['relationship_group']  = df_test['relationship'].apply(relationship_group)   ","99f4928e":"# MODEL_TEST = True\nif MODEL_TEST:\n    test_predictions = make_predictions(df_train, df_test, TARGET, params)\n    test_predictions[TARGET] = [int(v >= 0.5) for v in test_predictions[TARGET]]","3d0eba65":"df_train['log_capital_gain'] = df_train['capital_gain'].map(lambda x: np.log(x) if x != 0 else 0)\ndf_test['log_capital_gain'] = df_test['capital_gain'].map(lambda x: np.log(x) if x != 0 else 0)\n\ndf_train['log_capital_loss'] = df_train['capital_loss'].map(lambda x: np.log(x)  if x != 0 else 0)\ndf_test['log_capital_loss'] = df_test['capital_loss'].map(lambda x: np.log(x) if x != 0 else 0)","30119f15":"mm_scaler = MinMaxScaler()\nst_scaler = StandardScaler()\n\ndf_train['age'] = st_scaler.fit_transform(df_train['age'].values.reshape(-1, 1))\ndf_test['age'] = st_scaler.fit_transform(df_test['age'].values.reshape(-1, 1))\n\ndf_train['fnlwgt'] = st_scaler.fit_transform(df_train['fnlwgt'].values.reshape(-1, 1))\ndf_test['fnlwgt'] = st_scaler.fit_transform(df_test['fnlwgt'].values.reshape(-1, 1))\n\ndf_train['hours_per_week'] = st_scaler.fit_transform(df_train['hours_per_week'].values.reshape(-1, 1))\ndf_test['hours_per_week'] = st_scaler.fit_transform(df_test['hours_per_week'].values.reshape(-1, 1))\n\ndf_train['log_capital_gain'] = st_scaler.fit_transform(df_train['log_capital_gain'].values.reshape(-1, 1))\ndf_test['log_capital_gain'] = st_scaler.fit_transform(df_test['log_capital_gain'].values.reshape(-1, 1))\n\ndf_train['log_capital_gain'] = st_scaler.fit_transform(df_train['log_capital_gain'].values.reshape(-1, 1))\ndf_test['log_capital_gain'] = st_scaler.fit_transform(df_test['log_capital_gain'].values.reshape(-1, 1))\n\n\nremove_features += ['capital_gain', 'capital_loss', 'native_country', 'race', 'fnlwgt', 'workclass']","b3f85692":"# MODEL TEST\nif MODEL_TEST:\n    test_predictions = make_predictions(df_train, df_test, TARGET, params)\n    test_predictions[TARGET] = [int(v >= 0.5) for v in test_predictions[TARGET]]","65c3b305":"def frequency_encoding(df_train, df_test, columns, self_encoding=False):\n    for col in columns:\n        df_temp = pd.concat([df_train[[col]], df_test[[col]]])\n        fq_encode = df_temp[col].value_counts(dropna=False).to_dict()\n        if self_encoding:\n            df_train[col] = df_train[col].map(fq_encode)\n            df_test[col]  = df_test[col].map(fq_encode)\n        else:\n            df_train[col + '_fq_encode'] = df_train[col].map(fq_encode)\n            df_test[col + '_fq_encode']  = df_test[col].map(fq_encode)\n            \n    return df_train, df_test","3cb52ef5":"i_cols = [\n 'education',\n 'marital_status',\n 'occupation',\n 'relationship',\n 'workclass',\n 'sex',\n 'native_country',\n]\n\ndf_train, df_test = frequency_encoding(df_train, df_test, i_cols, self_encoding=False)","74cf1ea6":"if MODEL_TEST:\n    test_predictions = make_predictions(df_train, df_test, TARGET, params)\n    test_predictions[TARGET] = [int(v >= 0.5) for v in test_predictions[TARGET]]","dfb521f3":"def uid_aggregation(df_train, df_test, main_columns, uids, aggregations):\n    for main_column in main_columns:\n        for uid in uids:\n            for agg in aggregations:\n                new_col_name = uid + '_' + main_column + '_' + agg\n                df_temp = pd.concat([df_train[[uid, main_column]], df_test[[uid, main_column]]])\n                df_temp = df_temp.groupby([uid])[main_column].agg([agg]).reset_index().rename(columns = {agg: new_col_name})\n                df_temp.index = list(df_temp[uid])\n                df_temp = df_temp[new_col_name].to_dict()\n                \n                df_train[new_col_name] = df_train[uid].map(df_temp)\n                df_test[new_col_name]  = df_test[uid].map(df_temp)\n                \n    return df_train, df_test","29d82957":"main_columns = ['age', 'hours_per_week']\nuids = ['relationship_group', 'occupation_group', 'education_group', 'marital_status_group', 'workclass_group']\naggregations = ['mean']   # std\n\ndf_train, df_test = uid_aggregation(df_train, df_test, main_columns, uids, aggregations)","309fa571":"#MODEL TEST\nif MODEL_TEST:\n    test_predictions = make_predictions(df_train, df_test, TARGET, params)\n    test_predictions[TARGET] = [int(v >= 0.5) for v in test_predictions[TARGET]]","6c85f2e7":"df_train['uid1'] = df_train['relationship_group'].astype(str) + '_' + df_train['occupation_group'].astype(str)\ndf_test['uid1']  = df_test['relationship_group'].astype(str) + '_' + df_test['occupation_group'].astype(str)\n\ndf_train['uid2'] = df_train['relationship_group'].astype(str) + '_' + df_train['education_group'].astype(str)\ndf_test['uid2']  = df_test['relationship_group'].astype(str) + '_' + df_test['education_group'].astype(str)\n\ndf_train['uid3'] = df_train['relationship_group'].astype(str) + '_' + df_train['marital_status_group'].astype(str)\ndf_test['uid3']  = df_test['relationship_group'].astype(str) + '_' + df_test['marital_status_group'].astype(str)\n\ndf_train['uid4'] = df_train['education_group'].astype(str) + '_' + df_train['occupation_group'].astype(str)\ndf_test['uid4']  = df_test['education_group'].astype(str) + '_' + df_test['occupation_group'].astype(str)\n\ndf_train['uid5'] = df_train['education_group'].astype(str) + '_' + df_train['marital_status_group'].astype(str)\ndf_test['uid5']  = df_test['education_group'].astype(str) + '_' + df_test['marital_status_group'].astype(str)\n\ndf_train['uid6'] = df_train['marital_status_group'].astype(str) + '_' + df_train['occupation_group'].astype(str) \ndf_test['uid6']  = df_test['marital_status_group'].astype(str) + '_' + df_test['occupation_group'].astype(str)\n\nnew_columns = ['uid1', 'uid2', 'uid3', 'uid4', 'uid5', 'uid6']\n# remove_features += new_columns\n\nprint('Most common uids: ')\nfor col in new_columns:\n    print('#'*10, col)\n    print(df_train[col].value_counts()[:10])","787d0b2f":"main_columns = ['age', 'hours_per_week']\nuids = ['uid1', 'uid2', 'uid4', 'uid5', 'uid6']\naggregations = ['mean']   # std\n\n# remove_features += ['uid2_hours_per_week_std', 'uid1_hours_per_week_std', 'uid2_age_std', 'uid1_age_std']\n\ndf_train, df_test = uid_aggregation(df_train, df_test, main_columns, uids, aggregations)","e4857dd3":"#MODEL TEST\nif MODEL_TEST:\n    test_predictions = make_predictions(df_train, df_test, TARGET, params)\n    test_predictions[TARGET] = [int(v >= 0.5) for v in test_predictions[TARGET]]","5c70bb32":"target = df_train[TARGET]\n\nbase_columns = [col for col in list(df_train) if col not in remove_features]\nnew_columns = set(list(df_train)).difference(base_columns + remove_features)\nfeatures_columns = base_columns + list(new_columns)\nprint(\"feature columns list\")\nprint(list(features_columns))\n\ndf_train = df_train[features_columns]\ndf_test  = df_test[features_columns]\n\ndf_train = pd.concat([df_train, target], axis=1)\n\ndf_train.to_csv('train_2_fe.csv', index=False)\ndf_test.to_csv('test_2_fe.csv', index=False)","ccac29e1":"submission = pd.read_csv(os.path.join(BASE_PATH, 'sample_submission.csv'))\nsubmission['prediction'] = test_predictions[TARGET]\nsubmission.to_csv('submission_test.csv', index=False)","32c8420c":"---","d1374812":"## Feature scaling","53632006":"## Group aggregation","18ace798":"---","a6b84ffa":"## Group feature","818c632b":"## Submissions","79265152":"## Baseline","30dd53ad":"---","94ba2b27":"---","cdb1ebba":"## Frequency encoding","b4f451d8":"## [Kakr 4th] LGB FE + Permutation importance","f4f8610c":"---"}}