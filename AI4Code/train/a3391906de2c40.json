{"cell_type":{"7c349c74":"code","07e42e22":"code","44396dc5":"code","4f9a1d5b":"code","a8c8e6cc":"code","d9b99341":"code","5cf7f413":"code","121ffe80":"code","e553eba4":"code","c383ca83":"code","f6dfce2d":"code","0bd5cb21":"code","dc322d4a":"code","09f82f0b":"code","2f4afaf3":"code","d65f6e56":"code","017818d9":"code","259fb05e":"code","a9fadc16":"code","aa85cfc8":"code","2416babf":"code","9c490f53":"code","3be6bdb8":"code","504c2521":"code","befd029b":"code","c8f91a65":"code","feec8ef1":"code","f536c522":"markdown","a770aaa7":"markdown","3a2f4d25":"markdown","89ae1291":"markdown","b8066c9f":"markdown","e26eca5d":"markdown","1a064641":"markdown","b72285c5":"markdown","43f61ae9":"markdown","99b01637":"markdown","3ce9148f":"markdown","53233d2e":"markdown","ed413ba0":"markdown","c7712585":"markdown","6569bf6a":"markdown","07b06ea9":"markdown","1bc542bf":"markdown","b8dff43f":"markdown","b5b79e88":"markdown"},"source":{"7c349c74":"import tensorflow as tf\nimport numpy as np\nimport os\nimport glob\nimport math\nimport matplotlib.pyplot as plt\nimport random\nfrom xml.etree import ElementTree\nfrom PIL import Image, ImageOps, ImageDraw\n\nfrom tensorflow.keras.utils import Progbar\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, GlobalMaxPooling2D, Input\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras import applications\n\n%matplotlib inline","07e42e22":"print(tf.__version__)","44396dc5":"random_seed = 42\n\nimg_height = 64\nimg_width = 64\nimg_channels = 3\n\nnp.random.seed(random_seed)\ntf.random.set_seed(random_seed)\nrandom.seed(random_seed)","4f9a1d5b":"images_dir = \"\/kaggle\/input\/medical-masks\/images\"\nmeta_dir = \"\/kaggle\/input\/medical-masks\/labels\"\ndata_dir = \".\/source\"\ntrain_dir = os.path.join(data_dir, \"train\")\ntest_dir = os.path.join(data_dir, \"test\")","a8c8e6cc":"!rm -rf {data_dir}\n!mkdir {data_dir}","d9b99341":"def random_image(images_dir):\n    files = glob.glob(os.path.join(images_dir, \"*\"), recursive=True)\n    return random.choice(files)\n\nimage_path = random_image(images_dir)\nimage_path","5cf7f413":"def load_image(image_path):\n    return Image.open(image_path).convert(\"RGB\")\n\nimage = load_image(image_path)\nplt.imshow(image)","121ffe80":"def image_meta_path(image_path, meta_dir):\n    image_filename = os.path.basename(image_path)\n    basename = os.path.splitext(image_filename)[0]\n    return os.path.join(meta_dir, f\"{basename}.xml\")\n\nimage_meta_path(image_path, meta_dir)","e553eba4":"def get_regions_meta(image_path):\n    meta_file = image_meta_path(image_path, meta_dir)\n    root = ElementTree.parse(meta_file).getroot()\n    regions = []\n    for object_tag in root.findall(\"object\"):\n        name = object_tag.find(\"name\").text\n        xmin = int(object_tag.find(\"bndbox\/xmin\").text)\n        xmax = int(object_tag.find(\"bndbox\/xmax\").text)\n        ymin = int(object_tag.find(\"bndbox\/ymin\").text)\n        ymax = int(object_tag.find(\"bndbox\/ymax\").text)\n        regions.append({ \"name\": name, \"coordinates\": (xmin, ymin, xmax, ymax) })\n    return regions\n\nregions_meta = get_regions_meta(image_path)\nregions_meta","c383ca83":"def process_image(image, size=None, crop=None):\n    if not crop is None:\n        image = image.crop(crop)\n    if not size is None:\n        image = image.resize(size)\n    return image\n\ndef crop_regions(image, regions_meta, size=None):\n    return list(map(lambda region_meta: (region_meta[\"name\"], process_image(image, crop=region_meta[\"coordinates\"], size=size)), regions_meta))\n\nregions = crop_regions(image, regions_meta, size=(img_width, img_height))\n\ndef plot_regions(regions):\n    fig = plt.figure(figsize=(10, 10))\n    fig.subplots_adjust(hspace=0.2, wspace=0.1)\n    columns = 4\n    rows = math.ceil(len(regions) \/ columns)\n    \n    for i, region in enumerate(regions):\n        label, image = region\n        ax = fig.add_subplot(rows, columns, i + 1)\n        ax.set_title(label)\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n        plt.imshow(image)\n        \n    plt.show()\n\nplot_regions(regions)","f6dfce2d":"test_split = 0.2\n\ndef image_dir(label):\n    if not label in ['good', 'bad']:\n        return None\n\n    if random.random() > test_split:\n        split_dir = train_dir\n    else:\n        split_dir = test_dir\n  \n    return os.path.join(split_dir, label)\n\nimage_dir('good')","0bd5cb21":"def prepare_image(image_path, image_index, size=None):\n    regions_meta = get_regions_meta(image_path)\n    image = load_image(image_path)\n    regions = crop_regions(image, regions_meta, size=size)\n    for region_index, region in enumerate(regions):\n        label, image = region\n        region_dir = image_dir(label)\n        if region_dir == None:\n            continue\n        if not os.path.isdir(region_dir):\n            os.makedirs(region_dir)\n        region_filename = f\"{image_index}_{region_index}.jpg\"\n        image.save(os.path.join(region_dir, region_filename), \"JPEG\")","dc322d4a":"def prepare_images(images_dir, size=None):\n    images_paths = glob.glob(os.path.join(images_dir, '*'), recursive=True)\n    progbar = Progbar(target = len(images_paths))\n    progbar.update(0)\n    for image_index, image_path in enumerate(images_paths):\n        prepare_image(image_path, image_index, size=size)\n        progbar.update(image_index + 1)\n\nprepare_images(images_dir, size=(img_width, img_height))","09f82f0b":"!ls -R | grep \":$\" | sed -e 's\/:$\/\/' -e 's\/[^-][^\\\/]*\\\/\/--\/g' -e 's\/^\/   \/' -e 's\/-\/|\/'","2f4afaf3":"batch_size = 16\n\ntrain_data = ImageDataGenerator(\n    rescale=1.\/255,\n    rotation_range=15,\n    width_shift_range=0.3,\n    height_shift_range=0.3,\n    brightness_range=(0.67, 1.0),\n    shear_range=0.3,\n    zoom_range=0.3,\n    horizontal_flip=True,\n    fill_mode=\"nearest\"\n).flow_from_directory(\n    batch_size=batch_size,\n    directory=train_dir,\n    shuffle=True,\n    target_size=(img_height, img_width), \n    class_mode=\"binary\"\n)\n\nvalidation_data = ImageDataGenerator(\n    rescale=1.\/255\n).flow_from_directory(\n    batch_size=batch_size,\n    directory=test_dir,\n    shuffle=True,\n    target_size=(img_height, img_width), \n    class_mode=\"binary\"\n)","d65f6e56":"def analyze(history):\n    best_accuracy = np.max(history.history[\"val_accuracy\"])\n    best_loss = np.min(history.history[\"val_loss\"])\n    print(f\"Best accuracy: {best_accuracy}\")\n    print(f\"Best logloss: {best_loss}\")\n    \n    acc = history.history[\"accuracy\"]\n    val_acc = history.history[\"val_accuracy\"]\n\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    \n    plt.figure(figsize=(8, 8))\n    plt.subplot(2, 1, 1)\n    plt.plot(acc, label=\"Training Accuracy\")\n    plt.plot(val_acc, label=\"Validation Accuracy\")\n    plt.plot(np.argmax(history.history[\"val_accuracy\"]), best_accuracy, \"o\", color=\"green\")\n    plt.legend(loc=\"lower right\")\n    plt.ylabel(\"Accuracy\")\n    plt.ylim([min(plt.ylim()),1])\n    plt.title(\"Training and Validation Accuracy\")\n\n    plt.subplot(2, 1, 2)\n    plt.plot(loss, label=\"Training Loss\")\n    plt.plot(val_loss, label=\"Validation Loss\")\n    plt.plot(np.argmin(history.history[\"val_loss\"]), best_loss, \"o\", color=\"green\")\n    plt.legend(loc=\"upper right\")\n    plt.ylabel(\"Cross Entropy\")\n    plt.ylim([min(plt.ylim()),max(plt.ylim())])\n    plt.title(\"Training and Validation Loss\")\n    plt.xlabel(\"epoch\")\n    plt.show()","017818d9":"epochs = 50\nverbose = 0","259fb05e":"plain_model = Sequential([\n    Input((img_height, img_width, img_channels)),\n    Flatten(),\n    Dense(16),\n    Dense(1)\n])\n\nplain_model.summary()","a9fadc16":"train_data.reset()\nvalidation_data.reset()\n\nplain_model.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\nplain_model_history = plain_model.fit(\n    train_data,\n    epochs=epochs,\n    steps_per_epoch=len(train_data),\n    validation_data=validation_data,\n    validation_steps=len(validation_data),\n    verbose=verbose\n)","aa85cfc8":"analyze(plain_model_history)","2416babf":"base_transfer_model = applications.Xception(include_top=False, weights=\"imagenet\")\nbase_transfer_model.trainable = False\n\ntransfer_model = Sequential([\n    Input((img_height, img_width, img_channels)),\n    base_transfer_model,\n    GlobalMaxPooling2D(),\n    Dense(64),\n    Dense(1)\n])\n\ntransfer_model.summary()","9c490f53":"train_data.reset()\nvalidation_data.reset()\n\ntransfer_model.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\ntransfer_model_history = transfer_model.fit(\n    train_data,\n    epochs=epochs,\n    steps_per_epoch=len(train_data),\n    validation_data=validation_data,\n    validation_steps=len(validation_data),\n    verbose=verbose\n)","3be6bdb8":"analyze(transfer_model_history)","504c2521":"custom_model = Sequential([\n    Input((img_height, img_width, img_channels)),\n    Conv2D(32, (2, 2), activation=\"relu\", padding=\"same\"),\n    MaxPooling2D(2, 2),\n    Conv2D(64, (2, 2), activation=\"relu\", padding=\"same\"),\n    MaxPooling2D(2, 2),\n    Conv2D(64, (2, 2), activation=\"relu\", padding=\"same\"),\n    MaxPooling2D(2, 2),\n    Conv2D(64, (2, 2), activation=\"relu\", padding=\"same\"),\n    MaxPooling2D(2, 2),\n    Flatten(),\n    Dropout(0.5),\n    Dense(128, activation=\"relu\"),\n    Dense(32, activation=\"relu\"),\n    Dense(1, activation=\"sigmoid\")\n])\n\ncustom_model.summary()","befd029b":"train_data.reset()\nvalidation_data.reset()\n\ncustom_model.compile(\n    optimizer=\"adam\",\n    loss=\"binary_crossentropy\",\n    metrics=[\"accuracy\"]\n)\n\ncustom_model_history = custom_model.fit(\n    train_data,\n    epochs=epochs,\n    steps_per_epoch=len(train_data),\n    validation_data=validation_data,\n    validation_steps=len(validation_data),\n    verbose=verbose\n)","c8f91a65":"analyze(custom_model_history)","feec8ef1":"plain_model_acc = plain_model_history.history[\"val_accuracy\"]\ntransfer_model_acc = transfer_model_history.history[\"val_accuracy\"]\ncustom_model_acc = custom_model_history.history[\"val_accuracy\"]\n\nplt.plot(plain_model_acc, label=\"Plain Model Accuracy\")\nplt.plot(transfer_model_acc, label=\"Transfer Model Accuracy\")\nplt.plot(custom_model_acc, label=\"Custom Model Accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.ylim([min(plt.ylim()),1])\nplt.title(\"Model comparison\")","f536c522":"### Load, crop and resize image","a770aaa7":"### Generate random image path","3a2f4d25":"## Comparison","89ae1291":"## Transfer model (trainable top)","b8066c9f":"## Analyze","e26eca5d":"### Parse image metadata from XML file","1a064641":"### Show directory tree","b72285c5":"# Medical Masks Dataset Crop faces and classify\nThis kernel consists two parts:\n1. Crop faces from photos using XML metadata and put them in folders to prepared for `flow_from_directory`;\n2. Classify faces in two categories 'good' and 'bad'.","43f61ae9":"### Crop faces from image and save them in proper directory ","99b01637":"### Process all images","3ce9148f":"## Data","53233d2e":"## Custom Model\n","ed413ba0":"### Get metadata XML filename for an image","c7712585":"## Generators\nPrepare train and validation image data generators","6569bf6a":"### Crop faces from image","07b06ea9":"# Prepare","1bc542bf":"### Determine directory for a particular image\nDirectory is determined according to image label and random train\/test split ratio. Only 'good' and 'bad' images processed, other images are droppped.","b8dff43f":"## Plain model","b5b79e88":"# Model"}}