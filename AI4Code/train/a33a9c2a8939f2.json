{"cell_type":{"54dff6be":"code","6a4dfa2e":"code","8da60e7c":"code","cbcbea04":"code","ec34d1af":"code","acf3b627":"code","99e870ba":"code","f0d7456a":"code","52110505":"code","9e198cf3":"code","0fee06b8":"code","92a4afe7":"code","15b58391":"code","b52a8ff4":"code","7a00922f":"code","0c322ed3":"code","7c93c787":"code","28da8b7f":"code","35ff52da":"code","71e8a8c8":"code","e8627ee8":"code","b2c8faf1":"code","2ed8950a":"code","c9bedc32":"code","125cf0b7":"code","7a3984e1":"code","55b2201e":"code","179ac264":"code","e6912ad0":"code","be05c182":"code","df0d0309":"code","2d654817":"code","0b88ff3a":"code","c0f5fa02":"code","94dea5f9":"code","151a6914":"code","76dd0221":"code","64a85e36":"code","90805164":"code","3fda9340":"code","8c454574":"code","222eae17":"code","9b152ed5":"code","66bc129d":"code","7b4806ca":"markdown","4dea0d1d":"markdown","ba5c3f64":"markdown","ac3fb0b2":"markdown","e4206261":"markdown","1f1600a8":"markdown","547659f0":"markdown","68aad8e3":"markdown","d72e4c1c":"markdown","e4c908cf":"markdown","47fc3c25":"markdown"},"source":{"54dff6be":"#calling the libraries which would need for this analysis\nimport pandas as pd\nfrom pandas import Series, DataFrame \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split","6a4dfa2e":"#uploading the files\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8da60e7c":"#pulling out the necessary files\ntrain_ID=pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv') \ntrain_trans=pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_ID=pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv') \ntest_trans=pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')","cbcbea04":"#check their shapes(numbers of rows and columns)\nprint(train_ID.shape)\nprint(train_trans.shape)\nprint(test_ID.shape)\nprint(test_trans.shape)","ec34d1af":"train_set=pd.merge(train_ID, train_trans, on='TransactionID', how='outer')\ntest_set=pd.merge(test_ID, test_trans, on='TransactionID', how='outer')","acf3b627":"#delete the original uploaded files to save the memory \ndel train_ID, train_trans, test_ID, test_trans","99e870ba":"#drop the rows with na values by setting the threshold of 250 as out of total total vlues in both datasets, train and test\n#train_set=train_set.dropna(thresh=250)\n#test_set=test_set.dropna(thresh=250)","f0d7456a":"#then check the size of datasets again to see how many we have left \nprint(train_set.shape)\nprint(test_set.shape)","52110505":"#As the datasets are still large to manage, here dropping the columns that would have nulls which is 80% of the whole data vlues.\ntrain_set = train_set.loc[:, train_set.isnull().sum() < 0.8*train_set.shape[0]]\n","9e198cf3":"#As the datasets are still large to manage, here dropping the columns that would have nulls which is 80% of the whole data vlues.\ntest_set = test_set.loc[:, test_set.isnull().sum() < 0.8*test_set.shape[0]]","0fee06b8":"#Then those train_set and test_set are combined \nboth_data = pd.concat([train_set, test_set], axis=0, sort=False)","92a4afe7":"#created the bar plot to see how is the frequency of targerted variable \"isFraud\"\nboth_data['isFraud'].value_counts().plot(kind = 'bar')","15b58391":"#then delete the old versions of files as we dont need to use them for training models later.\ndel train_set, test_set","b52a8ff4":"#To avoid dropping the main variable \"isFraud\" column, we just dropped the columns by calling individualy in the function\nboth_data=both_data.drop(['id-01', 'id-02', 'id-05', 'id-06', 'id-11', 'id-12', 'id-13', 'id-15', 'id-16', 'id-17', 'id-19', 'id-20',\n'id-28', 'id-29', 'id-31', 'id-35', 'id-36', 'id-37', 'id-38'], axis=1)\n","7a00922f":"#Since we cant drop any na in dataset as that could lead to zero row left, we again set the conditon to drop certain columns in datset again\nboth_data = both_data.loc[:, both_data.isnull().sum() < 0.7*both_data.shape[0]]","0c322ed3":"#count the values of targeted variable in the mergered dataset\nboth_data.isFraud.value_counts() ","7c93c787":"#dropping the random rows again to make the dataset more managable when running the models\nnp.random.seed(10)\nremove_n = 100000\ndrop_indices = np.random.choice(both_data.index, remove_n, replace=False)\nboth_data = both_data.drop(drop_indices)\nboth_data.shape","28da8b7f":"#As it is mentioned above, there are some categorical fields in dataset and we have to encode all those categorical variables.pd get dummies function would save your times a lot rather than using column transform function here.\n\nboth_data = pd.get_dummies(both_data)\nprint(both_data.shape)\nboth_data.head(2)","35ff52da":"#clean the newly generated columns of the dataset as there are some weird words in the column headings\nimport re\nboth_data = both_data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))","71e8a8c8":"both_data.head(5)","e8627ee8":"#removing dulpicate columns because that could help you run the chosen models smoothly\n_, i = np.unique(both_data.columns, return_index=True) \nboth_data=both_data.iloc[:, i] ","b2c8faf1":"#count the values of targeted variable in the mergered dataset\nboth_data.isFraud.value_counts() ","2ed8950a":"#separating our data into features dataset X and our target dataset y \nX=both_data.drop('isFraud',axis=1) \ny=both_data.isFraud ","c9bedc32":"y.fillna(y.mode()[0], inplace=True)","125cf0b7":"X = X.fillna(X.mean())\n","7a3984e1":"#delete the old version of dataset after splited it into two dataset as X and y accordingly\ndel both_data","55b2201e":"#Now splitting our datasets into test and train to apply into the models\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.5)","179ac264":"X_test.shape","e6912ad0":"#importing linear model for the purpose of creating the correlatlion plot, we dont really need to do fit this model here .\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nreg.fit(X_train, y_train)","be05c182":"#computing the coefficient ..\nreg.coef_","df0d0309":"#then create the correlation plot\n%matplotlib inline\nfeat_importances = pd.Series(reg.coef_, index=X.columns)\nfeat_importances.nlargest(30).plot(kind='barh')","2d654817":"#delete the old versions of datasets as we wouldnot need them for furtur purposes \ndel X,y","0b88ff3a":"#XGB model was applied\nfrom xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)","c0f5fa02":"##wvaluate model on test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]","94dea5f9":"#getting the accuracy for training the model\nfrom sklearn.metrics import accuracy_score\naccur = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accur * 100.0))","151a6914":"# building the lightgbm model\nimport lightgbm as lgb\nmodel2 = lgb.LGBMClassifier()\nmodel2.fit(X_train, y_train)\n","76dd0221":"# predict the results\ny_pred_lgb=model2.predict(X_test)","64a85e36":"#getting the accuracy for training the model\naccur2=accuracy_score(y_pred_lgb,y_test)\nprint(\"Accuracy: %.2f%%\" % (accur2 * 100.0))","90805164":"#Creating a dataframe \u2018comparison_df\u2019 for comparing the performance of Lightgbm and xgb.\ncomparison = {'accuracy score':(accur2,accur)}\ncomparison_df = DataFrame(comparison) \ncomparison_df.index= ['LightGBM','xgboost'] \ncomparison_df","3fda9340":"#Prob pred with XGB model\ntest_pred_Prob = model.predict_proba(X_test)\nprint(test_pred_Prob[:20])\n","8c454574":"submission = pd.DataFrame({'TransactionID' : X_test.TransactionID,    'isFraud' : test_pred_Prob[:,1]})\nsubmission.head(20)","222eae17":"submission.shape","9b152ed5":"submission.head()","66bc129d":"\n\nss=pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')\nss.head(2)\n#ss.loc[:, 'isFraud'] = submission['isFraud']\nss=ss.drop(['isFraud'], axis=1)\nss.head(2).head(2)\nmy_submission =pd.concat([ss,submission], axis=0)\nfinal=my_submission.dropna()\nfinal.shape\n\nfinal.to_csv('submission_pmm.csv',index=False)\n\n","7b4806ca":"There are four files we need to load for checking through the features of the datasets","4dea0d1d":"We generated the largest 30 features that have high correlation with the targeted variable. It can be seen that R_emaildomain has most influence factor in this case. Probably, if the ones who are using the similar email domain names would be the same group of hackers to do certain frauds. Device Info is more correlated than Device type and the variable types of M4, V1 , ... ,etc are also invloved in the influenct factor for the targeted variable.","ba5c3f64":"As we can see the files sizes are too big as follow. And we are going to merge train_ID file and train_trans file because they have different variables and we need all those features in one dataframe to run the models by targeting \"isFraud\" as dependent varaible.","ac3fb0b2":"Following categorical features in Transaction_ID dataset and \nProductCD\n- card1 - card6\n- addr1, addr2\n- P_emaildomain\n- R_emaildomain\n- M1 - M9\n\nFollowing categorical Features in Identity dataset are now in one combained datasets; train_set and test_set.\n- DeviceType\n- DeviceInfo\n- id_12 - id_38\n\n- The TransactionDT feature is a timedelta from a given reference datetime (not an actual timestamp).\n\nAfter observing the files, we can say that columns of both datasets have same fields.  Following process are to deal with the missing values in the merged files.","e4206261":"Hoorayyy!!! Finally,we can now split that combined dataset into featres and target datasets accordingly","1f1600a8":"Tremendous again!! the accuracy of training the LGB model is 98%. It cant be said much different which one is better over one another as the different is only small points between XGB and LGB. We can make a little more complicated application by having parameters in the model functions if you want to explore more.","547659f0":"When we called the combined dataset of those four files, the following fields have many null values which is almost 90% of the value of the whole dataset.","68aad8e3":"Firstly, lets create train and test set by merging the respective data files accordingly. ","d72e4c1c":"In this project, the target variable is \"isFraud\" which is categorical variable. The rest of the the columns in the dataset are used as independent variables in this case.\nAs the given datasets are too big, there are many steps proceeded to clean and construct the data to be more readble and handled enough to train the planned models in this processes of analysis.\n\nWe are going to explore XGB and LGB machine learning models to train as the size of this project's data is enormous and it would be difficult for traditional data algorithms to give faster results. We will be doing XGB which is also an implementation of gradient boosting machines to see what model would be the best fit for this maching learning observation.\n","e4c908cf":"Awesome!! The accuracy rate is 98.12% and the model can be seen as a real good one for this analysis. But as we want to explore more and compare the results, we are going to train light gradient boosting.","47fc3c25":"Ref:\n\nhttps:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n\nhttps:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/\nhttps:\/\/towardsdatascience.com\/lightgbm-vs-xgboost-which-algorithm-win-the-race-1ff7dd4917d"}}