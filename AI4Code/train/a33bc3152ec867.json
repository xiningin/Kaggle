{"cell_type":{"3e941d6a":"code","9607fe48":"code","c0b3590c":"code","5b8531ab":"code","6807c180":"code","4e28f2fc":"code","438e7714":"code","a91b706b":"code","b9188e44":"code","e0500901":"code","be68d7f0":"code","7055651c":"code","eabf85d0":"code","cbefe40d":"code","5e0f87f1":"code","74eb1742":"code","13f95aa1":"code","9a70fe96":"code","917a5845":"code","3e2087b2":"code","4a154f6e":"code","b65e58ab":"code","d1ced4bc":"code","51786df1":"code","be38d148":"code","0aceb3b2":"code","cfc90510":"code","d72866ea":"code","2cc78e17":"code","865c553a":"code","a12d2da7":"code","fc35df36":"markdown","a0b3da4a":"markdown","9dd92e6e":"markdown","2def55f8":"markdown","0986df73":"markdown","04213039":"markdown","58dd3dcc":"markdown","d97ec92a":"markdown","88d69003":"markdown","e643922c":"markdown","58984dc3":"markdown","080f11ba":"markdown","0a2dffd1":"markdown"},"source":{"3e941d6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9607fe48":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\n\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score","c0b3590c":"path = \"\/kaggle\/input\/electric-power-consumption-data-set\/\"\ndf = pd.read_csv(path + \"household_power_consumption.txt\",\n                  sep = \";\" ,na_values = [\"nan\",\"?\"],\n                  parse_dates = {\"dt\" : [\"Date\",\"Time\"]},\n                  infer_datetime_format = True,\n                  index_col = \"dt\")","5b8531ab":"df.head()","6807c180":"df.shape","4e28f2fc":"df.dtypes","438e7714":"df.describe()","a91b706b":"df.columns","b9188e44":"# Check unique values in all columns\n\nfor col in df.columns:\n    print(\"Column : {} , Unique Values : {} \\n Values : {} \\n\\n\".format(col, df[col].nunique(), df[col].unique()))","e0500901":"# Number of na values\ndf.isna().sum()","be68d7f0":"df = df.fillna(df.mean())","7055651c":"# Number of na values\ndf.isna().sum()","eabf85d0":"df_resampled=df.resample('h').mean()","cbefe40d":"scaler = MinMaxScaler(feature_range=(0, 1))\nscaled = scaler.fit_transform(df_resampled)\ndf_scaled =pd.DataFrame(scaled)","5e0f87f1":"df_resampled[\"target\"] = df_resampled.Global_active_power.shift(-1)\ndf_resampled = df_resampled.iloc[:-1,:] # remove last value as it is shifted upward","74eb1742":"df_resampled.tail()","13f95aa1":"df_resampled.tail()","9a70fe96":"values = df_resampled.values\nnum_train = 365*24\n\ntrain = values[:num_train, :]\ntest = values[num_train:, :]\n\nX_train, y_train = train[:,1:], train[:,0]\nX_test, y_test = test[:,1:], test[:,0]\n","917a5845":"# Reshaping train and test sets\n\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))","3e2087b2":"print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)","4a154f6e":"model = tf.keras.Sequential()\ninitializer = tf.keras.initializers.HeNormal()\nmodel.add(tf.keras.layers.LSTM(100,\n                               activation='relu',\n                               kernel_initializer=initializer,\n                               input_shape=(X_train.shape[1],\n                                            X_train.shape[2])))\nmodel.add(tf.keras.layers.Dropout(0.2))\n\nmodel.add(tf.keras.layers.Dense(1))\n\nmodel.compile(loss='mean_squared_error', optimizer='adam')","b65e58ab":"model.summary()","d1ced4bc":"history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_data=(X_test, y_test), shuffle=False)","51786df1":"history.history.keys()","be38d148":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","0aceb3b2":"y_pred = model.predict(X_test)","cfc90510":"X_test.shape","d72866ea":"# lets scale the X_test too\nX_test = X_test.reshape(X_test.shape[0], 7)","2cc78e17":"# Invert scaling for pred\ninv_x = np.concatenate((y_pred, X_test[:, -6:]), axis=1)\ninv_x = scaler.inverse_transform(inv_x)\ninv_y_pred = inv_x[:,0]\n","865c553a":"# invert scaling for actual\ny_test = y_test.reshape((len(y_test), 1))\ninv_y = np.concatenate((y_test, X_test[:, -6:]), axis=1)\ninv_y = scaler.inverse_transform(inv_y)\ninv_y = inv_y[:,0]","a12d2da7":"# calculate RMSE\nrmse = np.sqrt(mean_squared_error(inv_y, inv_y_pred))\nprint('RMSE value : {}'.format(rmse))\n\n# calculate R2 Score\nr2 = r2_score(inv_y, inv_y_pred)\nprint(\"R2 Score : {}\".format(r2))","fc35df36":"## Import the data and preprocess","a0b3da4a":"##  Clean up na values","9dd92e6e":"## Resampling data to hours","2def55f8":"# Train Test split","0986df73":"## Import the packages","04213039":"- We can see that there are nan values in columns","58dd3dcc":"- Filling missing values with mean","d97ec92a":"## Making Predictions","88d69003":"## Scaling Data ","e643922c":"# LSTM Data Preparation and feature engineering\n\n- We need to shift our target variable one step back , as LSTM uses past value for prediction\n- Target Column = Global_active_power","58984dc3":"# Model Architecture\n\n1. LSTM with 100 neurons in the first visible layer (**Q:** would you like ot change it and why?)\n2. dropout 20%\n4. 1 neuron in the output layer for predicting Global_active_power. \n5. The input shape will be 1 time step with 7 features.\n6. Use the Mean Absolute Error (MAE) loss function and the efficient Adam gradient descent.\n7. The model will be fit for suitable training epochs with a suitable batch size.","080f11ba":"# Fit the network","0a2dffd1":"- Taking last 1 year values for test set, and rest for train set"}}