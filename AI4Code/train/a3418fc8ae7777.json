{"cell_type":{"34703fc2":"code","7cccdd3a":"code","b3de95e1":"code","dc595dd5":"code","12a09d46":"code","de9db699":"code","2d5ca0ce":"code","fc8f3cab":"code","65a56f6f":"code","33be929e":"code","3421a8ec":"code","dc9b4f75":"code","e1b04cc1":"code","b1d016ac":"code","eed6e56e":"code","c623db77":"code","df1e6c29":"code","7902cdbe":"code","ff5ba976":"code","73751871":"code","9bf7c62d":"code","b144d45f":"code","59f5c680":"code","e9ec34cd":"code","6a908a3d":"code","59cb9b40":"code","f0b8434a":"code","3a115b84":"code","b584a6c9":"code","f7c84b7c":"code","bcb15b88":"code","a3d0558f":"code","4f34dfc7":"code","e8b06e4c":"code","18ec2923":"markdown","8459362c":"markdown","81261699":"markdown","71b3b120":"markdown","6ef98ba7":"markdown","cacdffe5":"markdown","aacf35a0":"markdown","ceb025f2":"markdown","100bc9ab":"markdown","0c19e302":"markdown","056b55f0":"markdown","f143cea7":"markdown","2fa4068d":"markdown","fc9d3f87":"markdown","fa153a15":"markdown","87281e60":"markdown"},"source":{"34703fc2":"#Import important libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split,cross_val_score,RandomizedSearchCV\nfrom sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n\nsns.set_style('whitegrid')","7cccdd3a":"# Load the dataset\ndata = pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/car data.csv')\nprint(data.shape)\ndata.head(2)","b3de95e1":"# check in case of any duplicate records and then drop it\nprint(data.duplicated().sum())\ndata.drop_duplicates(inplace=True)","dc595dd5":"# check for any missing value\nplt.figure(figsize=(8,3))\nsns.heatmap(data.isnull(),yticklabels=False,cmap='viridis',cbar=False)","12a09d46":"# categorising columns based on column type\nNominal_cat_var =['Car_Name','Fuel_Type','Seller_Type','Transmission']\nOrdinal_cat_var = ['Owner']\nNum_var = ['Selling_Price','Present_Price','Kms_Driven']\nDate_colum = ['Year']","de9db699":"# Distribution of all the numerical variables\nfor column in Num_var :\n    plt.hist(data[column])\n    plt.xlabel(column)\n    plt.ylabel('Count')\n    plt.show()","2d5ca0ce":"# Distribution of all the categorical variables\nfor column in Nominal_cat_var :\n    sns.countplot(data[column])\n    plt.xlabel(column)\n    plt.ylabel('Count')\n    plt.show()","fc8f3cab":"# Analysing date column\nplt.figure(figsize=(12,3))\nsns.countplot(data.Year)","65a56f6f":"# Creating new year column\ndata['No_of_year'] = 2020 - data['Year'] ","33be929e":"# One hot encoding\nNominal =['Fuel_Type','Seller_Type','Transmission']\ndata = pd.get_dummies(data,columns=Nominal,drop_first=True)","3421a8ec":"# Dropping unwanted columns\ndrop_var = ['Car_Name','Year']\ndata.drop(drop_var,axis=1,inplace=True)","dc9b4f75":"data.head(2)","e1b04cc1":"# Checking the corelation between the variables\nsns.heatmap(data.corr(),annot=True,cmap='viridis')","b1d016ac":"# Splitting data into dependent and independent variables\nX = data.iloc[:,1:]\ny = data.iloc[:,0]","eed6e56e":"# ExtrTressRegressor to see the feature importance\nImp_reg = ExtraTreesRegressor()\nImp_reg.fit(X,y)","c623db77":"# Important features\nImportnt_feat = pd.Series(Imp_reg.feature_importances_,X.columns).sort_values(ascending=False)\nprint(Importnt_feat)","df1e6c29":"# Let's visualise it\nImportnt_feat.plot(kind='barh')","7902cdbe":"models = [LinearRegression(),RandomForestRegressor()]\nmean = []\nstd = []\n\nfor model in models:\n    mean_score = cross_val_score(model,X,y,scoring='neg_mean_squared_error',cv=5)\n    mean.append(mean_score)\n    std.append(mean_score)","ff5ba976":"classifiers=['Linear Regression', 'Random Forest']\n\nfor i in range(len(mean)):\n    sns.distplot(mean[i],hist=False, kde_kws={\"shade\": True})\nplt.title(\"Distribution of each classifier's Accuracy\", fontsize=15)\nplt.legend(classifiers)\nplt.xlabel(\"Neg_mean_squared_error\", labelpad=20)\nplt.yticks([])\n    \n    ","73751871":"# Train and test split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)","9bf7c62d":"Lr_reg = LinearRegression()\nLr_reg.fit(X_train,y_train)","b144d45f":"# For train data\npred_lr_train = Lr_reg.predict(X_train)\nprint('The r2_score for linear regression is {}'.format(r2_score(y_train,pred_lr_train)))\nprint('The MAE for linear regression is {}'.format(mean_absolute_error(y_train,pred_lr_train)))\nprint('The MSE for linear regression is {}'.format(mean_squared_error(y_train,pred_lr_train)))\nprint('The RMSE for linear regression is {}'.format(np.sqrt(mean_squared_error(y_train,pred_lr_train))))","59f5c680":"# For test data\npred_lr = Lr_reg.predict(X_test)\nprint('The r2_score for linear regression is {}'.format(r2_score(y_test,pred_lr)))\nprint('The MAE for linear regression is {}'.format(mean_absolute_error(y_test,pred_lr)))\nprint('The MSE for linear regression is {}'.format(mean_squared_error(y_test,pred_lr)))\nprint('The RMSE for linear regression is {}'.format(np.sqrt(mean_squared_error(y_test,pred_lr))))","e9ec34cd":"# Coeff table\nCoeff_table = pd.DataFrame(Lr_reg.coef_,X.columns,columns=['Coeff'])\nCoeff_table.sort_values(by='Coeff',ascending=False)","6a908a3d":"from sklearn.linear_model import Ridge\nrid = Ridge()\nparams = {'alpha':[0,1,0.01]}\nresult = RandomizedSearchCV(rid,params,scoring='neg_mean_squared_error',cv=5,n_jobs = 1,n_iter = 10)\nresult.fit(X,y)","59cb9b40":"print(result.best_params_)\nprint(result.best_score_)","f0b8434a":" #Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# max_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","3a115b84":"# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","b584a6c9":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestRegressor()","f7c84b7c":"# Random search of parameters, using 5 fold cross validation, \n\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","bcb15b88":"grid_result = rf_random.fit(X,y)","a3d0558f":"# Printing the best params\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","4f34dfc7":"# Running the regresson model for best param\nrf_best = RandomForestRegressor(n_estimators= 700,min_samples_split=15, min_samples_leaf= 1,max_features= 'auto', max_depth=20)","e8b06e4c":"# Fitting the model on training data\nrf_best.fit(X_train,y_train)\npred_rf = rf_best.predict(X_test) \nprint('The r2_score for random forest regression is {}'.format(r2_score(y_test,pred_rf)))\nprint('The MAE for random forest regression is {}'.format(mean_absolute_error(y_test,pred_rf)))\nprint('The MSE for random forest regression is {}'.format(mean_squared_error(y_test,pred_rf)))\nprint('The RMSE for random forest regression is {}'.format(np.sqrt(mean_squared_error(y_test,pred_rf))))","18ec2923":"## Model Building","8459362c":"## Data Exploration","81261699":"    Note:\n    \n    1) We will drop the column car name, as it has many options.\n    2) we will convert columns - 'Fuel_Type','Seller_Type','Transmission' using One hot encoding","71b3b120":"In Machine Learning there are several ways the split your data into training and test sets in order to determine how a\nmodel performs on them, making sure that our model performs well no matter how the data is partitioned.\nSuppose we have a model with one or more unknown parameters, and a data set to which the model can be fit \n(the training data set). The fitting process optimizes the model parameters to make the model fit the training data \nas well as possible. If we then take an independent sample of validation data from the same population as the training \ndata, it will generally turn out that the model does not fit the validation data as well as it fits the training data. \nThe size of this difference is likely to be large especially when the size of the training data set is small, or when the\nnumber of parameters in the model is large. Cross-validation is a way to estimate the size of this effect.\n\n\nPlease note cross validation is a validating technique.It is not a technique to split the data into test and train to\nbuild the model. It is suggested to run this validation across multiple classifiers to finally choose one. Gernerally, train\nand test split will divide the data based random state, This may lead to different accuracy. Cross validation, will help us \nundersatnd the variation in the accuracy. The classifier which has less deviation will be selected as it ies more stable","6ef98ba7":"### Random Forest Regressor","cacdffe5":"###### Kaggle dataset link - https:\/\/www.kaggle.com\/nehalbirla\/vehicle-dataset-from-cardekho","aacf35a0":"# Car Price Prediction Case Study","ceb025f2":"### Linear Regression","100bc9ab":"## Cross Validation","0c19e302":"    There were 2 duplicate records which are now removed from the dataset","056b55f0":"      We will convert the date column to a numerical field by subtracting it from 2020, as cars have depreciation value","f143cea7":"## Feature Importance","2fa4068d":"## Feature Engineering","fc9d3f87":"    All the numerical variables are right skewed","fa153a15":"### Ridge Regression (L1) (Just for learning purpose)","87281e60":"Feature importance scores play an important role in a predictive modeling project, including providing insight into the data,\ninsight into the model, and the basis for dimensionality reduction and feature selection that can improve the efficiency and \neffectiveness of a predictive model on the problem.\n\nFeature importance scores can provide insight into the dataset. The relative scores can highlight which features may be most \nrelevant to the target, and the converse, which features are the least relevant. This may be interpreted by a domain expert \nand could be used as the basis for gathering more or different data."}}