{"cell_type":{"6fa52027":"code","83f83a76":"code","499fd084":"code","f02e245a":"code","ef413793":"code","4d5b6004":"code","b98f2b50":"code","c6a820c7":"code","705f9c6c":"code","11175b72":"code","40be2ee2":"code","e679efde":"code","57b54257":"code","609472c5":"code","b4461689":"code","2baba7c6":"code","fb4bdd48":"code","630af556":"code","0716ae2d":"code","1a2a5835":"code","ff66d095":"code","807c8af6":"code","227c47ea":"code","da6ed6e3":"code","4ebe8fad":"markdown","9596721b":"markdown","d7ac6e5b":"markdown","1df060d8":"markdown","7536c005":"markdown","c36b8060":"markdown","8d9d5ad7":"markdown","d967f1c5":"markdown","d9e047be":"markdown"},"source":{"6fa52027":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve\n\n#models and tuning\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom xgboost import plot_importance\n\n# For tuning\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nimport optuna \n\n# Metrics for models evaluation\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n","83f83a76":"df = pd.read_csv('..\/input\/mushroom-classification\/mushrooms.csv')","499fd084":"df.head()","f02e245a":"df.info()","ef413793":"df.describe()","4d5b6004":"labelencoder=LabelEncoder()\n\nfor column in df.columns:\n    df[column] = labelencoder.fit_transform(df[column])","b98f2b50":"df.describe()","c6a820c7":"df.drop('veil-type', axis = 1, inplace=True)","705f9c6c":"rat_df = df['class'].value_counts()\n\n# ratio is created if it's needed in models\nratio = rat_df[1] \/ rat_df[0]\n\nweights_dict = {0: ratio,\n               1:1}","11175b72":"X = df.drop('class',axis=1) \ny = df['class']","40be2ee2":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","e679efde":"# sc = StandardScaler()\n\n# X_train = sc.fit_transform(X_train)\n# X_test = sc.transform(X_test)","57b54257":"neighbors = np.arange(4,16)\ntrain_accuracy =np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i,k in enumerate(neighbors):\n    #Setup a knn classifier with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    print('Training KNN ', k)\n    #Fit the model\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n    \n    #Compute accuracy on the test set\n    test_accuracy[i] = knn.score(X_test, y_test)","609472c5":"plt.title('k-NN Varying number of neighbors')\nplt.plot(neighbors, test_accuracy, label='Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label='Training accuracy')\nplt.legend()\nplt.xlabel('Number of neighbors')\nplt.ylabel('Accuracy')\nplt.show()","b4461689":"knn = KNeighborsClassifier(n_neighbors=6,p=1, n_jobs=-1)\nknn.fit(X_train, y_train)\n\ny_pred = knn.predict(X_test)\n\nprint('Accuracy:', accuracy_score(y_test ,y_pred))\nprint('F1-score:', f1_score(y_test ,y_pred))\nprint('Confusion matrix:')\nprint(confusion_matrix(y_test,y_pred))","2baba7c6":"# Find best hyperparameters (roc_auc)\nrandom_state = 42\n\nlog_clf = LogisticRegression(random_state = random_state)\nparam_grid = {'class_weight' : ['auto', weights_dict, 'None'], \n                'penalty' : ['l2','l1'],  \n                'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid = GridSearchCV(estimator = log_clf, param_grid = param_grid , scoring = 'roc_auc', verbose = 1, n_jobs = -1)\n\ngrid.fit(X_train,y_train)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))\n\nbest_parameters = grid.best_params_","fb4bdd48":"log_reg = LogisticRegression(**best_parameters)\nlog_reg.fit(X_train, y_train)\n\ny_pred = log_reg.predict(X_test)\n\nprint('Accuracy:', accuracy_score(y_test ,y_pred))\nprint('F1-score:', f1_score(y_test ,y_pred))\nprint('Confusion matrix:')\nprint(confusion_matrix(y_test,y_pred))","630af556":"lgb_model = LGBMClassifier(nthread=-1, silent=True)\n\n#Fit to training data\nlgb_model.fit(X_train, y_train)\n#Generate Predictions\ny_pred=lgb_model.predict(X_test)\n\naccuracy = accuracy_score(y_test ,y_pred)\nf1 = f1_score(y_test ,y_pred)\n\nprint('Accuracy:', accuracy)\nprint('F1-score:', f1)\nprint('Confusion matrix:')\nprint(confusion_matrix(y_test,y_pred))","0716ae2d":"lgb.plot_importance(lgb_model, figsize=(12,8))","1a2a5835":"lgb_fimp = pd.DataFrame(sorted(zip(lgb_model.feature_importances_,X.columns)), columns=['Value_LGB','Feature_LGB'])\nlgb_fimp = lgb_fimp.sort_values(by=['Value_LGB'], ascending=False).reset_index(drop=True)\nlgb_fimp","ff66d095":"gbm = xgb.XGBClassifier(max_depth=3, n_estimators=300, learning_rate=0.05)\ngbm.fit(X_train, y_train)\ny_pred = gbm.predict(X_test)\n\naccuracy = accuracy_score(y_test ,y_pred)\nf1 = f1_score(y_test ,y_pred)\n\nprint('Accuracy:', accuracy)\nprint('F1-score:', f1)\nprint('Confusion matrix:')\nprint(confusion_matrix(y_test,y_pred))","807c8af6":"plot_importance(gbm, max_num_features=30) # top 10 most important features\nplt.show()","227c47ea":"dct = gbm.get_booster().get_score(importance_type=\"gain\")\nxgb_fimp = pd.DataFrame(dct.items(), columns=['Feature_XGB', 'Value_XGB']).sort_values(by=['Value_XGB'], ascending=False)\n\n\nxgb_fimp,lgb_fimp\n\nd = {'XGBoost_Feature' : xgb_fimp['Feature_XGB'], \n     'XGBoost_Value' : xgb_fimp['Value_XGB'],\n      'LGBM_Feature' : lgb_fimp['Feature_LGB'],\n      'LGBM_Value' : lgb_fimp['Value_LGB'],\n    }\n\nfeatures = pd.DataFrame(d)\n","da6ed6e3":"features","4ebe8fad":"### All columns are categorical","9596721b":"## Conclusion","d7ac6e5b":"## Logistic regression","1df060d8":"## LGBM","7536c005":"## XGBoost","c36b8060":"### Checking class balance","8d9d5ad7":"## KN-neighbors","d967f1c5":"### KNN (k = 6), LGBM, XGBM showed a 100% accuracy and 1.0 F1-Score.\n\nIn case with LGBM, XGBM parameters tuning was not necessary and even excessive.\n\nThe most important features for LGBM and XGBM are:","d9e047be":"### Using LabelEncoder to encode all columns' values"}}