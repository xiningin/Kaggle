{"cell_type":{"86bef78c":"code","a11ac2b1":"code","be2a0519":"code","a7bf5c9f":"code","94702f8c":"code","af4a6f07":"code","c1e3b77d":"code","ac4b5874":"code","ca20aed8":"code","ecff813a":"code","68ddca60":"code","0c778c40":"code","5e772e44":"code","1ff2e8f5":"code","bc480f5c":"markdown","c99998d6":"markdown","7e7dfebc":"markdown","524990df":"markdown"},"source":{"86bef78c":"import os\n%matplotlib inline\nimport matplotlib.pyplot as plt  \nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold\n#Importing most common alogorithms \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC#we will not be using SVM due tot he huge training time required on our dataset.\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis,LinearDiscriminantAnalysis\n\nfrom sklearn import model_selection #Cross-validation multiple scoring function\nimport warnings\nwarnings.simplefilter(\"ignore\")","a11ac2b1":"# Only load those columns in order to save space\nkeep_cols = ['event_id', 'game_session', 'installation_id', 'event_count', 'event_code', 'title', 'game_time', 'type', 'world']\n\ntrain = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv', usecols=keep_cols)\ntest = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv', usecols=keep_cols)\ntrain_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')\ntrain.shape","be2a0519":"train.head()","a7bf5c9f":"def group_and_reduce(df):\n    # group1 and group2 are intermediary \"game session\" groups,\n    # which are reduced to one record by game session. group1 takes\n    # the max value of game_time (final game time in a session) and \n    # of event_count (total number of events happened in the session).\n    # group2 takes the total number of event_code of each type\n    group1 = df.drop(columns=['event_id', 'event_code']).groupby(\n        ['game_session', 'installation_id', 'title', 'type', 'world']\n    ).max().reset_index()\n\n    group2 = pd.get_dummies(\n        df[['installation_id', 'event_code']], \n        columns=['event_code']\n    ).groupby(['installation_id']).sum()\n\n    # group3, group4 and group5 are grouped by installation_id \n    # and reduced using summation and other summary stats\n    group3 = pd.get_dummies(\n        group1.drop(columns=['game_session', 'event_count', 'game_time']),\n        columns=['title', 'type', 'world']\n    ).groupby(['installation_id']).sum()\n\n    group4 = group1[\n        ['installation_id', 'event_count', 'game_time']\n    ].groupby(\n        ['installation_id']\n    ).agg([np.sum, np.mean, np.std])\n\n    return group2.join(group3).join(group4).reset_index()","94702f8c":"train = group_and_reduce(train)\ntest = group_and_reduce(test)\n\nprint(train.shape)\ntrain.head()","af4a6f07":"labels = train_labels[['installation_id', 'accuracy_group']]\ntrain = train.merge(labels, how='left', on='installation_id').dropna()\ntrain.shape","c1e3b77d":"train.columns","ac4b5874":"x_train, y_train  = train.drop(['installation_id', 'accuracy_group'], axis=1),train['accuracy_group']\n ","ca20aed8":"seed = 42\nnp.random.seed(seed)\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(multi_class='auto',n_jobs=-1)))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('QDA',QuadraticDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier(7, n_jobs=-1)))\nmodels.append(('CART', DecisionTreeClassifier(max_depth=10)))\nmodels.append(('NB', GaussianNB()))\n#models.append(('Gaussian Process Classifier', GaussianProcessClassifier()))\n#models.append(('SVM_linear', SVC(kernel=\"linear\", C=0.025)))#we will not be using SVM due to the huge training time required for this dataset\n#models.append(('SVM_',SVC(gamma=2, C=1)))\nmodels.append(('RandomForest',RandomForestClassifier( n_estimators=100, n_jobs=-1)))\nmodels.append(('MLP',MLPClassifier(alpha=0.0001)))\nmodels.append(('ADABoost',AdaBoostClassifier()))\nmodels.append(('One-Vs-Rest LR', OneVsRestClassifier(LogisticRegression(multi_class='auto',n_jobs=-1),n_jobs=-1)))\nmodels.append(('One-Vs-Rest LDA', OneVsRestClassifier(OneVsRestClassifier(LinearDiscriminantAnalysis(),n_jobs=-1))))\nmodels.append(('One-Vs-Rest QDA',OneVsRestClassifier(QuadraticDiscriminantAnalysis())))\nmodels.append(('One-Vs-Rest KNN', OneVsRestClassifier(KNeighborsClassifier(7, n_jobs=-1))))\nmodels.append(('One-Vs-Rest CART', OneVsRestClassifier(DecisionTreeClassifier(max_depth=10),n_jobs=-1)))\nmodels.append(('One-Vs-Rest NB', OneVsRestClassifier(GaussianNB(),n_jobs=-1)))\n#models.append(('One-Vs-Rest Gaussian Process Classifier', OneVsRestClassifier(GaussianProcessClassifier(),n_jobs=-1)))\n#models.append(('One-Vs-Rest SVM_linear', SVC(kernel=\"linear\", C=0.025)))#we will not be using SVM due to the huge training time required for this dataset\n#models.append(('One-Vs-Rest SVM_',SVC(gamma=2, C=1)))\nmodels.append(('One-Vs-Rest RandomForest',OneVsRestClassifier(RandomForestClassifier( n_estimators=100, n_jobs=-1),n_jobs=-1)))\nmodels.append(('One-Vs-Rest MLP',OneVsRestClassifier(MLPClassifier(alpha=0.0001),n_jobs=-1)))\nmodels.append(('One-Vs-Rest ADABoost',OneVsRestClassifier(AdaBoostClassifier(),n_jobs=-1)))\n\n# evaluate each model in turn\n\nresults = []\nscoring = ['accuracy',\n          'precision_weighted',\n          'recall_weighted',\n          'f1_weighted']\nnames = []\n\nsk = model_selection.StratifiedKFold(n_splits=10, random_state=42)\nfor name, model in models:\n    cv_results = model_selection.cross_validate(model, x_train, y_train, cv=sk, scoring=scoring) \n    results.append(cv_results)\n    names.append(name)\n    msg ='-------------------------------------------------------------------------------------------------------------\\n'\n    msg = \"Model : %s \\n\" % (name)\n    msg = msg +'\\n'\n    msg =  msg + \"Accuracy :  %f (%f)\\n\" % (cv_results['test_accuracy'].mean(),cv_results['test_accuracy'].std())\n    msg =  msg + \"Precision score :  %f (%f)\\n\" % (cv_results['test_precision_weighted'].mean(),cv_results['test_precision_weighted'].std())\n    msg =  msg + \"Recall score :  %f (%f)\\n\" % (cv_results['test_recall_weighted'].mean(),cv_results['test_recall_weighted'].std())\n    msg =  msg + \"F1 score :  %f (%f)\\n\" % (cv_results['test_f1_weighted'].mean(),cv_results['test_f1_weighted'].std())\n    msg = msg + '------------------------------------------------------------------------------------------------------------\\n'\n    print(msg)\n","ecff813a":"Accuracy = []\nPrecision = []\nRecall = []\nF1 = []\nfor idx,scores in enumerate(results):\n    Accuracy.append(scores['test_accuracy'])\n    Precision.append(scores['test_precision_weighted'])\n    Recall.append(scores['test_recall_weighted'])\n    F1.append(scores['test_f1_weighted'])","68ddca60":"fig = plt.figure(figsize=(30,30))\nfig.suptitle('Algorithms Comparison')\nax = fig.add_subplot(221)\nplt.boxplot(Accuracy)\nplt.title('Accuracy score')\nax.set_xticklabels(names)\nax = fig.add_subplot(222)\nplt.boxplot(Precision)\nplt.title('Precision Score')\nax.set_xticklabels(names)\nax = fig.add_subplot(223)\nplt.boxplot(Recall)\nax.set_xticklabels(names)\nplt.title('Recall score')\nax = fig.add_subplot(224)\nplt.title('F1 score')\nplt.boxplot(F1)\nax.set_xticklabels(names)\n\nplt.show()","0c778c40":"from time import time\n\nmodel = OneVsRestClassifier(AdaBoostClassifier(),n_jobs=-1)\n\ntic = time()\nmodel.fit(x_train,y_train)\ntoc = time()\n\nprint(\"Classifier : One-Vs-Rest AdaBoostClassifier ===> Training duration : {} sec\".format( toc - tic))","5e772e44":"y_test = model.predict(test.drop(['installation_id'], axis=1).fillna(0))#.argmax(axis=1)\ny_test.shape","1ff2e8f5":"test['accuracy_group'] = y_test\ntest[['installation_id', 'accuracy_group']].to_csv('submission.csv', index=False)","bc480f5c":"# Load Data","c99998d6":"It seems that the 'One vs the Rest' Classifier approach give a little boost on performance to all the classification aproach on diferent folds of the validation set, but we have a clear winner wich is AdaBoost algorthms. Let's see in performance on the test set trough the leaderboard.","7e7dfebc":"# Training models","524990df":"# Group and Reduce"}}