{"cell_type":{"bd52f6b0":"code","8e98eec4":"code","f82dd7fe":"code","a601c6d5":"code","137cf79d":"code","6a653b0f":"code","b66c74db":"code","f6c47498":"code","aca2baeb":"code","2a8c6e00":"code","d0bcc147":"code","df8c1dc5":"code","2e208e76":"code","d882a183":"code","4a35dd8b":"code","05e42420":"code","b7da6894":"code","aab92f0a":"code","a6b864aa":"code","e7ed3c9f":"code","96bbd714":"code","802f47bd":"code","e32cbb20":"code","f0fc6b5a":"code","43e474f5":"code","f3e45a59":"code","4139390c":"code","c8ef03f3":"code","d4bce9de":"code","d890d586":"code","f65b3d67":"code","2671e3eb":"code","dfb11b14":"code","05a33fe0":"code","15cc10dc":"code","318f28c8":"code","db37d89f":"code","fdd2c2a0":"code","a292a65a":"code","4549a9f6":"code","4d2702e1":"markdown","101e2e5a":"markdown","19001e3f":"markdown","be2dc9c9":"markdown","05602387":"markdown","d99dc051":"markdown","0b65a47c":"markdown","e0f456ce":"markdown","33b49206":"markdown","0e7a8353":"markdown","26af173b":"markdown","6495b9d0":"markdown","c44578c6":"markdown"},"source":{"bd52f6b0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8e98eec4":"! pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html","f82dd7fe":"!pip install iterative-stratification\n","a601c6d5":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","137cf79d":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","6a653b0f":"train = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsubmission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\nremove_vehicle = True\n\nif remove_vehicle:\n    train_features = train.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\nelse:\n    train_features = train","b66c74db":"train_features","f6c47498":"# check the number of categorical features for train\/test\ncol_features = list(train_features.columns)[1:]\nprint(train_features[col_features[0]].value_counts())\nprint(test_features[col_features[0]].value_counts())\nprint(train_features[col_features[1]].value_counts())\nprint(test_features[col_features[1]].value_counts())\nprint(train_features[col_features[2]].value_counts())\nprint(test_features[col_features[2]].value_counts())\n","aca2baeb":"# check nan value and inf value ?\nprint(test_features[col_features].isna().sum().values.sum())\nprint(np.isinf(test_features[col_features[3:]].values).sum()) # only for numerical value\nprint(train_features[col_features].isna().values.sum())\nprint(np.isinf(train_features[col_features[3:]].values).sum()) # only for numerical value","2a8c6e00":"train_targets_scored","d0bcc147":"# ratio for each label\n\ndef get_ratio_labels(df):\n    columns = list(df.columns)\n    columns.pop(0)\n    ratios = []\n    toremove = []\n    for c in columns:\n        counts = df[c].value_counts()\n        if len(counts) != 1:\n            ratios.append(counts[0]\/counts[1])\n        else:\n            toremove.append(c)\n    print(f\"remove {len(toremove)} columns\")\n    \n    for t in toremove:\n        columns.remove(t)\n    return columns, np.array(ratios).astype(np.int32)\n\ncolumns, ratios = get_ratio_labels(train_targets_scored)\nprint(ratios)\n","df8c1dc5":"train_targets_nonscored","2e208e76":"columns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)\nprint(ratios_nonscored)\nprint(len(columns_nonscored), len(ratios_nonscored))","d882a183":"print(train_features[col_features[3:]].max().values.max())\nprint(train_features[col_features[3:]].min().values.min())\nprint(test_features[col_features[3:]].min().values.min())\nprint(test_features[col_features[3:]].max().values.max())","4a35dd8b":"len(col_features[3:])","05e42420":"mapping = {\"cp_type\":{\"trt_cp\": 1, \"ctl_vehicle\":2},\n               \"cp_time\":{48:1, 72:2, 24:3},\n               \"cp_dose\":{\"D1\":1, \"D2\":2}}\n\ndef transform_data(train, test, col, mapping, normalize=True, removed_vehicle=False):\n    \"\"\"\n        the first 3 columns represents categories, the others numericals features\n    \"\"\"\n\n    \n    if removed_vehicle:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n    else:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n    \n    max_ = 10.\n    min_ = -10.\n   \n    if removed_vehicle:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    else:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    if normalize:\n        numerical_tr = (numerical_tr-min_)\/(max_ - min_)\n        numerical_test = (numerical_test-min_)\/(max_ - min_)\n    return categories_tr, categories_test, numerical_tr, numerical_test\ncol_features = list(train_features.columns)[1:]\ncat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, mapping, normalize=False, removed_vehicle=remove_vehicle)\ntargets_tr = train_targets_scored[columns].values.astype(np.float32)\ntargets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)","b7da6894":"class MOADataset(Dataset):\n    def __init__(self, x_cats, x_nums, y=None, y2=None):\n        self.cats = x_cats\n        self.nums = x_nums\n        self.y = y\n        self.y2 = y2\n        \n    def __len__(self):\n        return len(self.cats)\n\n    def __getitem__(self, index):\n        x1 = torch.as_tensor(self.cats[index], dtype=torch.long)\n        x2 = torch.as_tensor(self.nums[index], dtype=torch.float)\n        \n        if self.y is not None:\n            label = torch.as_tensor(self.y[index], dtype=torch.float)\n            if self.y2 is not None:\n                label2 = torch.as_tensor(self.y2[index], dtype=torch.float)\n                return x1, x2, label, label2\n            return  x1, x2, label\n        return  x1, x2","aab92f0a":"class MOA_MLP(nn.Module):\n    def __init__(self, num_cats=[2,3,2] , cats_emb_size=[2,2,2], num_numericals=872, hidden_size_numericals=2048,\n                num_class=206, aux=None):\n        super().__init__()\n        self.cat_emb1 = nn.Embedding(num_cats[0], cats_emb_size[0], padding_idx=0)\n        self.cat_emb2 = nn.Embedding(num_cats[1], cats_emb_size[1], padding_idx=0)\n        #self.cat_emb3 = nn.Embedding(num_cats[2], cats_emb_size[2], padding_idx=0)\n\n        self.norms = nn.BatchNorm1d(sum(cats_emb_size) +num_numericals)\n        self.dropout = nn.Dropout(0.2)\n        \n        self.proj = nn.utils.weight_norm(nn.Linear(sum(cats_emb_size) + num_numericals, hidden_size_numericals))\n        self.norm_proj = nn.BatchNorm1d(hidden_size_numericals)\n        self.dropout2 = nn.Dropout(0.5)\n        \n        hd_1 = hidden_size_numericals\/\/2\n        hd_2 = hd_1\/\/2\n        self.extractor = nn.Sequential(nn.utils.weight_norm(nn.Linear(hidden_size_numericals, hd_1)),\n                                        nn.PReLU(),\n                                        nn.BatchNorm1d(hd_1),\n                                        nn.Dropout(0.5),\n                                        #nn.utils.weight_norm(nn.Linear(hd_1, hd_2)),\n                                        #nn.PReLU(),\n                                        #nn.BatchNorm1d(hd_2),\n                                        #nn.Dropout(0.5)\n        )\n        self.cls = nn.utils.weight_norm(nn.Linear(hd_1, num_class))\n        self.cls_aux=None\n        if aux is not None:\n            self.cls_aux = nn.utils.weight_norm(nn.Linear(hd_1, aux))\n    def forward(self, x_cat, x_num):\n        cat_features = torch.cat([self.cat_emb1(x_cat[:,0]), self.cat_emb2(x_cat[:,1])], dim=1)\n        all_features = torch.cat([cat_features, x_num], dim=1)\n        all_features = self.norms(all_features)\n        all_features = self.dropout(all_features)\n        \n        proj_features = self.proj(all_features)\n        proj_features = self.norm_proj(F.relu(proj_features))\n        proj_features = self.dropout2(proj_features )\n        \n        \n        \n        features_reduced = self.extractor(proj_features)\n        \n        outputs = self.cls(features_reduced)\n        if self.cls_aux is not None:\n            outputs2 = self.cls_aux(features_reduced)\n            return outputs, outputs2\n        return outputs\n    \nclass MOA_MLPv2(nn.Module):\n    def __init__(self, num_cats=[2,3,2] , cats_emb_size=[2,2,2], num_numericals=872, hidden_size_numericals=2048,\n                num_class=206, aux=None):\n        super().__init__()\n        self.cat_emb1 = nn.Embedding(num_cats[0], cats_emb_size[0], padding_idx=0)\n        self.cat_emb2 = nn.Embedding(num_cats[1], cats_emb_size[1], padding_idx=0)\n        self.cat_emb3 = nn.Embedding(num_cats[2], cats_emb_size[2], padding_idx=0)\n\n        self.projection_numericals = nn.Linear(num_numericals, hidden_size_numericals)\n        self.norm_numericals = nn.BatchNorm1d(hidden_size_numericals)\n        self.dropout = nn.Dropout(0.5)\n        \n        self.proj = nn.Linear(sum(cats_emb_size) + hidden_size_numericals, 2048)\n        self.norm_proj = nn.BatchNorm1d(2048)\n        \n        hd_1 = hidden_size_numericals\/\/2\n        hd_2 = hd_1\/\/2\n        self.extractor = nn.Sequential(nn.Linear(2048, hd_1),\n                                       nn.ReLU(),\n                                      nn.BatchNorm1d(hd_1),\n                                      nn.Dropout(0.25),\n                                      nn.Linear(hd_1, hd_2),\n                                      nn.ReLU(),\n                                      nn.BatchNorm1d(hd_2),\n                                      nn.Dropout(0.25))\n        \n        self.cls = nn.Linear(hd_2, num_class)\n        self.cls_aux=None\n        if aux is not None:\n            self.cls_aux = nn.Linear(hd_2, aux)\n    def forward(self, x_cat, x_num):\n        cat_features = torch.cat([self.cat_emb1(x_cat[:,0]), self.cat_emb2(x_cat[:,1]), self.cat_emb3(x_cat[:,2])], dim=1)\n        \n        num_features = self.projection_numericals(x_num)\n        num_features = self.norm_numericals(F.relu(num_features))\n        \n        all_features = torch.cat([cat_features, num_features], dim=1)\n        all_features = self.dropout(all_features)\n\n        all_features = F.relu(self.proj(all_features))\n        all_features = self.norm_proj(all_features)\n        \n        features_reduced = self.extractor(all_features)\n        \n        outputs = self.cls(features_reduced)\n        if self.cls_aux is not None:\n            outputs2 = self.cls_aux(features_reduced)\n            return outputs, outputs2\n        return outputs","a6b864aa":"from torch.cuda.amp import GradScaler, autocast\ndef train_one_epoch(model, dataloader, cfg, optimizer, loss_fn, loss_fn_aux=None, accumulation=1, with_aux_class=False, verbose=True):\n    model.train()\n    scaler = GradScaler()\n    optimizer.zero_grad()\n    N = 0.\n    total_loss = 0.\n    t=tqdm(dataloader, disable=~verbose)\n    for i, batch in enumerate(t):\n        \n        x1 = batch[0]\n        x2 = batch[1]\n        labels = batch[2]\n        \n        x1 = x1.to(cfg.device)\n        x2 = x2.to(cfg.device)\n        labels = labels.to(cfg.device)\n        \n        if with_aux_class:\n            labels2 = batch[3]\n            labels2 = labels2.to(cfg.device)\n            \n        with autocast(cfg.use_apex):\n            if with_aux_class:\n                outputs, outputs2 = model(x1, x2)\n                loss1 = loss_fn(outputs, labels).mean(0).mean()\n                loss2 = loss_fn_aux(outputs2, labels2).mean(0).mean()\n                loss = loss1 + 0.5*loss2\n\n            else:\n                outputs = model(x1, x2)\n                loss = loss_fn(outputs, labels).mean(0).mean()\n        \n        N += len(x1)\n        total_loss += (loss.item() * len(x1))  \n        \n        if cfg.use_apex:\n            loss = loss\/accumulation\n            scaler.scale(loss).backward()\n        else:\n            loss = loss\/accumulation\n            loss.backward()\n\n\n\n        \n        if (i+1)%accumulation == 0 or i-1 == len(dataloader):\n            if cfg.use_apex:\n                scaler.step(optimizer)\n\n                # Updates the scale for next iteration.\n                scaler.update()\n                optimizer.zero_grad()\n            else:                \n                optimizer.step()\n                optimizer.zero_grad()\n\n\n            t.set_description(\"Loss : {0}\".format(total_loss\/N))\n            t.refresh()\n            \n            \ndef evals(model, dataloader, cfg, loss_fn, loss_fn_aux=None, with_aux_class=False, verbose=True):\n    model.eval()\n    N = 0.\n    total_loss = 0.\n\n    y_preds = []\n    y_targets = []\n    t=tqdm(dataloader, disable=~verbose)\n    with torch.no_grad():\n        for i, batch in enumerate(t):\n\n            x1 = batch[0]\n            x2 = batch[1]\n            labels = batch[2]\n\n            x1 = x1.to(cfg.device)\n            x2 = x2.to(cfg.device)\n            labels = labels.to(cfg.device)\n\n            if with_aux_class:\n                labels2 = batch[3]\n                labels2 = labels2.to(cfg.device)\n\n            with autocast(cfg.use_apex):\n                if with_aux_class:\n                    outputs, outputs2 = model(x1, x2)\n                    loss1 = loss_fn(outputs, labels).mean(0).mean()\n                    loss2 = loss_fn_aux(outputs2, labels2).mean(0).mean()\n                    loss = loss1 #+ 0.5*loss2\n\n                else:\n                    outputs = model(x1, x2)\n                    loss = loss_fn(outputs, labels).mean(0).mean()\n\n            N += len(x1)\n            total_loss += (loss.item() * len(x1))  \n\n            t.set_description(\"Loss : {0}\".format(total_loss\/N))\n            t.refresh()\n            \n            y_preds.append(torch.sigmoid(outputs).detach().cpu().numpy())\n            y_targets.append(labels.detach().cpu().numpy())\n    y_preds = np.concatenate(y_preds, axis=0)\n    y_targets = np.concatenate(y_targets, axis=0)\n    score = log_loss_multi(y_targets, y_preds)\n    #print(\"Logloss = \", score)\n    return y_preds, y_targets, score\n\n\n\ndef inference_fn(model, dataloader, cfg, with_aux_class=True, verbose=True):\n    model.eval()\n    N = 0.\n    \n    y_preds = []\n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(dataloader, disable=~verbose)):\n\n            x1 = batch[0]\n            x2 = batch[1]\n            \n\n            x1 = x1.to(cfg.device)\n            x2 = x2.to(cfg.device)\n\n            \n\n            with autocast(cfg.use_apex):\n                if with_aux_class:\n                    outputs, outputs2 = model(x1, x2)\n                    \n                else:\n                    outputs = model(x1, x2)\n\n            \n            y_preds.append(torch.sigmoid(outputs).detach().cpu().numpy())\n    y_preds = np.concatenate(y_preds, axis=0)\n    return y_preds","e7ed3c9f":"def log_loss_score(actual, predicted,  eps=1e-15):\n\n        \"\"\"\n        :param predicted:   The predicted probabilities as floats between 0-1\n        :param actual:      The binary labels. Either 0 or 1.\n        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n        \"\"\"\n\n        \n        p1 = actual * np.log(predicted+eps)\n        p0 = (1-actual) * np.log(1-predicted+eps)\n        loss = p0 + p1\n\n        return -loss.mean()","96bbd714":"np.log(0)*0","802f47bd":"def log_loss_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n    return results.mean()\n        ","e32cbb20":"def train_fold(fold, model, tr_dataloader, val_dataloader, cfg, optimizer, reducer, loss_fn, loss_fn_aux=None, accumulation=1, with_aux_class=False):\n    best_score = np.inf\n    best_preds = None\n    best_targets = None\n    for e in range(cfg.EPOCHS):\n        train_one_epoch(model, tr_dataloader, cfg, optimizer, loss_fn, loss_fn_aux=loss_fn_aux, accumulation=accumulation, with_aux_class=with_aux_class, verbose=cfg.verbose)\n        preds, targets, score = evals(model, val_dataloader, cfg, loss_fn, loss_fn_aux=loss_fn_aux, with_aux_class=with_aux_class, verbose=cfg.verbose)\n        reducer.step(score)\n        if score < best_score:\n            print(\"## Epochs {0} : Improvement from {1} to {2}\".format(e, best_score, score))\n            best_score = score\n            best_preds = preds\n            best_targets= targets\n            torch.save(model.state_dict(), cfg.save_name + f\"_{fold}.pth\")\n    print(\"## FOLD {0} : best results : {1}\".format(fold, best_score))\n    return best_preds, best_targets, score","f0fc6b5a":"def inference_fold(folds, model, test_loader,cfg):\n    preds = []\n    for fold in range(folds):\n        name = cfg.save_name + f\"_{fold}.pth\"\n        model.load_state_dict(torch.load(name))\n        p = inference_fn(model, test_loader, cfg)\n        preds.append(p)\n    \n    return preds","43e474f5":"def check_targets(targets):\n    ### check if targets are all binary in training set\n    \n    for i in range(targets.shape[1]):\n        if len(np.unique(targets[:,i])) != 2:\n            return False\n    return True","f3e45a59":"def auc_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        try:\n            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n        except:\n            pass\n    return results.mean()","4139390c":"def check_labels(A):\n    w = np.zeros(A.shape[1])\n    for i in range(A.shape[1]):\n        if len(np.unique(A[:,i])) == 2:\n            w[i] = 1\n    return w.reshape(1, -1)","c8ef03f3":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold, MultilabelStratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score","d4bce9de":"class Config(object):\n    def __init__(self):\n        self.num_class = targets_tr.shape[1]\n        self.aux_class = targets2_tr.shape[1]\n        self.use_apex=False\n        self.verbose=False\n        #\n        self.batch_size = 128\n        self.device = \"cpu\"\n        self.SPLITS = 10\n        self.EPOCHS = 100\n        # Parameters model\n        self.num_cats=[3+1,2+1] if remove_vehicle else [2+1,3+1,2+1] \n        self.cats_emb_size=[1]* cat_tr.shape[1] #to choose\n        self.num_numericals= len(col_features[3:])\n        self.hidden_size_numericals=1024 # to choose\n        self.num_numericals= numerical_tr.shape[1]\n        self.hidden_size_numericals=2048 # to choose\n        self.num_ensembling = 1\n        # save\n        self.seed = 42\n        self.save_name = f\"MOA_mlp-KFOLD{self.SPLITS}\"\n        \n        self.strategy = \"KFOLD\" # or \ncfg = Config()\ncfg.with_aux_class = True if cfg.aux_class is not None else False\nprint(cfg.num_class, cfg.aux_class,cfg.with_aux_class)","d890d586":"loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")#, pos_weight=torch.as_tensor(ratios))\nloss_fn_aux = nn.BCEWithLogitsLoss(reduction=\"none\")#, pos_weight=torch.as_tensor(ratios))","f65b3d67":"test_dataset = MOADataset(cat_test, numerical_test)\ntest_dataloader= DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)","2671e3eb":"if cfg.strategy == \"KFOLD\":\n    oof_preds_all = []\n    oof_targets_all = []\n    scores_all = []\n    scores_auc_all = []\n    preds_test = []\n    masks = []\n    for seed in range(cfg.num_ensembling):\n        mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=cfg.seed+seed, shuffle=True)\n        oof_preds = []\n        oof_targets = []\n        scores = []\n        scores_auc = []\n        p = []\n        temp_mask = []\n        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n            print(\"FOLDS : \", j)\n\n            ## model\n\n            model = MOA_MLP(num_cats=cfg.num_cats , cats_emb_size=cfg.cats_emb_size, num_numericals=cfg.num_numericals, hidden_size_numericals=cfg.hidden_size_numericals,\n                        num_class=cfg.num_class, aux=cfg.aux_class).to(cfg.device)\n            \n            optimizer = optim.Adam(model.parameters(), lr = 5e-3, weight_decay=1e-5, amsgrad=True)#optim.SGD(model.parameters(), lr=1e-2, weight_decay=5e-4, momentum=0.9, nesterov=True) \n            reducer = ReduceLROnPlateau(optimizer, mode='min', factor=0.1,threshold=1e-3, patience=3,  min_lr=5e-6, eps=1e-08, verbose=True)\n            ## Create dataset then dataloader\n            if cfg.with_aux_class:\n                train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx], y2=targets2_tr[train_idx])\n                val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx], y2=targets2_tr[val_idx])\n            else:\n                train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx])\n                val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx] )\n            train_dataloader =    DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n            val_dataloader =    DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n\n            ## train fold\n            preds , targets, score = train_fold(f\"_{j}_{seed}\", model, train_dataloader, val_dataloader, cfg, optimizer, reducer, loss_fn, \n                                                loss_fn_aux=loss_fn_aux, accumulation=1, with_aux_class=cfg.with_aux_class)\n\n            ## save oof to compute the CV later\n            temp_mask.append(check_labels(targets_tr[train_idx]))\n            oof_preds.append(preds)\n            oof_targets.append(targets)\n            scores.append(score)\n            scores_auc.append(auc_multi(targets,preds))\n            p.append(inference_fn(model, test_dataloader, cfg,verbose=False))\n\n        oof_preds_all.append(np.concatenate(oof_preds))\n        oof_targets_all.append(np.concatenate(oof_targets))\n        scores_all.append(scores)\n        scores_auc_all.append(scores_auc)\n        preds_test.append(np.array(p))\n        masks.append(np.stack(temp_mask))\n        \n\n    preds_test = np.stack(preds_test)\n    oof_preds_all = np.stack(oof_preds_all)\n    oof_targets_all = np.stack(oof_targets_all)\n    scores_all = np.stack(scores_all)\n    scores_auc_all = np.stack(scores_auc_all)\n    masks = np.stack(masks)","dfb11b14":"if cfg.strategy == \"KFOLD\":\n    for i in range(oof_preds_all.shape[0]):\n        print(\"CV score : \", log_loss_multi(oof_targets_all[i], oof_preds_all[i]))\n        print(\"auc mean : \", sum(scores_auc_all[i])\/len(scores_auc_all[i]))","05a33fe0":"if cfg.strategy != \"KFOLD\":\n    i = 0\n    mskf = MultilabelStratifiedShuffleSplit(n_splits=1000, test_size=0.1, random_state=0)\n    oof_preds = []\n    oof_targets = []\n    scores = []\n    scores_auc = []\n    for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n        if i == cfg.SPLITS:\n            break\n            \n        if not check_targets(targets_tr[train_idx]):\n            continue\n        print(\"FOLDS : \", i, j)\n\n        ## model\n\n        model = MOA_MLPv2(num_cats=cfg.num_cats , cats_emb_size=cfg.cats_emb_size, num_numericals=cfg.num_numericals, hidden_size_numericals=cfg.hidden_size_numericals,\n                    num_class=cfg.num_class, aux=cfg.aux_class)\n        optimizer = optim.Adam(model.parameters(), lr = 1e-3, amsgrad=True)#optim.SGD(model.parameters(), lr=1e-2, weight_decay=5e-4, momentum=0.9, nesterov=True) \n        reducer = ReduceLROnPlateau(optimizer, mode='min', factor=0.1,threshold=1e-3, patience=3,  min_lr=1e-5, eps=1e-08, verbose=True)\n        ## Create dataset then dataloader\n        if cfg.with_aux_class:\n            train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx], y2=targets2_tr[train_idx])\n            val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx], y2=targets2_tr[val_idx])\n        else:\n            train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx])\n            val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx] )\n        train_dataloader =    DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n        val_dataloader =    DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n\n        ## train fold\n        preds , targets, score = train_fold(i, model, train_dataloader, val_dataloader, cfg, optimizer, reducer, loss_fn, \n                                            loss_fn_aux=loss_fn_aux, accumulation=1, with_aux_class=cfg.with_aux_class)\n\n        ## save oof to compute the CV later\n        oof_preds.append(preds)\n        oof_targets.append(targets)\n        scores.append(score)\n        scores_auc.append(auc_multi(targets,preds))\n        i+=1\n        #break","15cc10dc":"if cfg.strategy != \"KFOLD\":\n    oof_preds = np.concatenate(oof_preds)\n    oof_targets = np.concatenate(oof_targets)\n    print(\"CV score : \", log_loss_multi(oof_targets, oof_preds))\n    print(\"auc mean : \", sum(scores_auc)\/len(scores_auc))\n    print(oof_preds.shape, oof_targets.shape, targets_tr.shape)","318f28c8":"print(scores)","db37d89f":"print(model)","fdd2c2a0":"preds_test2 = preds_test.sum(1).sum(0)\/masks.sum(1).sum(0) ","a292a65a":"submission[columns] = preds_test2#preds_test.mean(1).mean(0)\nsubmission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0","4549a9f6":"submission","4d2702e1":"# script","101e2e5a":"# Model","19001e3f":"# Utils","be2dc9c9":"# Source \n\n- Pytorch 1.6 : https:\/\/pytorch.org\/docs\/stable\/\n- iterative-stratification : https:\/\/github.com\/trent-b\/iterative-stratification for stratified K fold multilabel","05602387":"# Dataset","d99dc051":"# Approach :\nInference script : \nhttps:\/\/www.kaggle.com\/ludovick\/inference-moa-baseline-mlp-kfold-10\/edit\/run\/41997446\n\nNeural Network to classify a multi labels tasks with pytorch\n- Stratified K Fold (10 folds) or shufflesplit\n- BCE Loss\n- optional labels are used for the training, not for inference though (after filtering)\n- gradient accumulation (not tested yet)\n- version 14 : add weight_norm from https:\/\/www.kaggle.com\/nicohrubec\/pytorch-multilabel-neural-network","0b65a47c":"As we can see, the labels are hightly imbalanced, it may be necessary to use a weighted loss function to help the model ?","e0f456ce":"we could later normalize our numerical value but we will see that for another version","33b49206":"# Targets data","0e7a8353":"### seems that some optional labels have only one labels, so we discard them","26af173b":"Check the different values than each category can take and if there are nan\/inf values","6495b9d0":"# Dataloader","c44578c6":"# Data\n"}}