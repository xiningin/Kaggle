{"cell_type":{"b25a7233":"code","328568b9":"code","6e3fb69b":"code","983769d4":"code","c4733bb6":"code","fd8319d0":"code","f009607c":"markdown","ba016a26":"markdown","f1d898b0":"markdown","dceab499":"markdown","6aa84472":"markdown","b365c195":"markdown","0fde8cef":"markdown","40472642":"markdown","12469ee2":"markdown"},"source":{"b25a7233":"import warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport numpy as np \nimport pandas as pd \nimport xgboost\n\npreprocessed_train = pd.read_csv('..\/input\/preprocessed-train-data\/preprocessed_train_data.csv')\npreprocessed_test = pd.read_csv('..\/input\/preprocessed-test-data\/preprocessed_test_data.csv')\n'''Split the data for training and testing'''\nx_train, y_train = preprocessed_train[preprocessed_train.columns[:-1]], preprocessed_train['SalePrice']","328568b9":"def cross_validate(preprocessed_train, model):\n    '''5-Fold Cross Validation'''\n    kfold = KFold(n_splits=5, random_state=42)\n    print(f'Model: {model.__class__.__name__}',end='\\n')\n    mse_errors = []\n    for i, (train_idx, test_idx) in enumerate(kfold.split(preprocessed_train)):\n        X_train, Y_train = x_train.loc[train_idx,:], y_train.loc[train_idx]\n        X_test, Y_test = x_train.loc[test_idx,:], y_train.loc[test_idx]\n        '''Fit the model'''\n        model.fit(X_train,Y_train)\n        '''Compute the predictions'''\n        predictions = model.predict(X_test)\n        '''Calculate Mean Squared Error and append to a list'''\n        errors = mean_squared_error(Y_test, predictions)\n        mse_errors.append(errors)\n        print(f'Fold:{i+1}, MSE:{errors}',end='\\n')\n    return mse_errors\n\nmodel = LinearRegression()\nprint(f'Variance of LinearRegression: {np.var(cross_validate(preprocessed_train, model))}',end='\\n\\n')\n\nmodel = xgboost.XGBRegressor(objective=\"reg:squarederror\", random_state=42)\nmse_errors = cross_validate(preprocessed_train, model)\n'''Compute the variance of XGBoost model'''\nvariance = np.var(mse_errors) \n'''\nThe XGBoost model's variance would be low when compared to Linear Regression so \nXGBoost has low variance and can generalize well.\n'''\nprint(f'Variance of XGBRegressor: {variance}')","6e3fb69b":"lasso = Lasso(alpha = 0.4)\nmse_errors_lasso = cross_validate(preprocessed_train, lasso)\nvariance_lasso = np.var(mse_errors_lasso)\n'''The variance is much less when compared to Linear Regression'''\nprint(f'Variance of LASSO: {variance_lasso}')","983769d4":"ridge = Ridge(alpha = 0.4)\nmse_errors_ridge = cross_validate(preprocessed_train, ridge)\nvariance_ridge = np.var(mse_errors_ridge)\n'''The variance is much less when compared to Linear Regression'''\nprint(f'Variance of Ridge: {variance_ridge}')","c4733bb6":"elasticnet = ElasticNet(alpha= 0.4, l1_ratio= 0.9)\nmse_errors_en = cross_validate(preprocessed_train, elasticnet)","fd8319d0":"'''Kernel initializer denotes the distribution in which the weights of the neural networks are initialized'''\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(128, kernel_initializer='normal', activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(256, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dropout(0.4),\n    tf.keras.layers.Dense(384, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dense(384, kernel_initializer='normal',activation='relu'),\n    tf.keras.layers.Dropout(0.6),\n    tf.keras.layers.Dense(1, kernel_initializer='normal',activation='linear')\n])\n\nmsle = tf.keras.losses.MeanSquaredLogarithmicError()\nmodel.compile(loss= msle, optimizer='adam', metrics=[msle])\nmodel.fit(x_train.values, y_train.values, epochs=15, batch_size=64, validation_split = 0.2)","f009607c":"# Dimensionality Reduction\nDimensionality reduction is the process of reducing the available features. Model could not be applied on entire set of features directly which may lead to spurious predictions and generalization issues. In order to prevent these issues dimensionality reduction is applied.\n\n* **Feature Selection**: Feature selection methods attempts to reduce the features by discarding the least important features.\n\n* **Feature Extraction**: Feature extraction methods attempts to reduce the features by combining the features and transforming it to the specified number of features.\n\n## Methods of Feature Selection\n* Filter methods\n* Wrapper methods\n* Embedded methods\n* Feature Importance\n\n## Techniques of Feature Extraction\n* PCA (Principal Component Analysis)\n* LDA (Linear Discriminant Analysis)\n* T-SNE (T-distributed Stochastic Neighbour Embedding)\n\nTo know further about feature selection and feature extraction refer to my notebooks in the link below,\n\n-> [Link to Techniques of Feature Extraction](https:\/\/www.kaggle.com\/srivignesh\/techniques-of-feature-extraction)\n\n-> [Link to Feature Selection Techniques](https:\/\/www.kaggle.com\/srivignesh\/feature-selection-techniques)","ba016a26":"## Dropout Regularization for Neural Networks\n\nDropout regularization attempts to reduce overfitting by dropping down random neurons in a layer according to the specified probability. \n* Dropout regularization has a similar effect of L2 regularization which shrinks the coefficients. \n* Dropout regularization distributes values for weights equally to all the neurons since it doesn't know which neuron would persist and which would not.\n\n![](https:\/\/miro.medium.com\/max\/1800\/0*VgOp3MXtsRmXSD50.png)\n\nTo know further about Artificial Neural Network refer to my notebook below,\n\n[Link to Introduction to ANN in Tensorflow](https:\/\/www.kaggle.com\/srivignesh\/introduction-to-ann-in-tensorflow)","f1d898b0":"## Lasso Regression\nLasso Regression is also called as L1 Regularization which is a linear model. Lasso tends to produce zero values for coefficients so it is also used in feature selection techniques. Lasso attempts to minimize the following cost function,\n\nL(w) = $\\sum\\limits_{i=1}^{M}(y_i - \\hat{y_i})^2 + \\alpha\\sum\\limits_{j=1}^{N} |w_j| $\n\n* $\\alpha$ = penalty that ranges from 0 to 1.\n* $|w_j|$ = absolute value for the coefficients of the model\n* $y_i$ = target of the data\n* $\\hat{y_i}$ = prediction of the model","dceab499":"## Identifying the problem of Overfitting\n\nThe problem of Overfitting can be identified using a technique called cross-validation. \n* Cross-validation attempts to split the training data in 'K' fold and utilizes one of its fold as test set. \n* The process repeats until each of the fold is utilized as test set.\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/b5\/K-fold_cross_validation_EN.svg)\n","6aa84472":"# Understanding Bias and Variance\nPrior to understanding what overfitting is, we need to have a firm foundation of what Bias and Variance are. Bias and Variance are the two central problems of Machine Learning and they are the inherent properties of the model. The Bias increases when the Variance decreases and vice versa which indicates that there exists a trade-off between Bias and Variance. \n\n* The phenomenon in which there exists High Bias and Low Variance is called **Underfitting**\n* The phenomenon in which there exists Low Bias and High Variance is called **Overfitting**\n\n***Error of the model*** can be broken down into,\n\n$Bias ^ 2 + Variance + \\sigma^2$ \n* $\\sigma^2$ = Irreducible error.\n\n![](https:\/\/www.educative.io\/api\/edpresso\/shot\/6668977167138816\/image\/5033807687188480)\n\n\n### Bias\n\n* Bias is statistically defined as the difference between the prediction and the actual value of the target\/ dependent variable. \n* One of the main reasons for Bias is the usage of Single Imputation techniques such as Mean and Mode Imputation. Another reason is the usage of Inflexible model such as Linear Regression.\n\n#### **Bias of the model**  =  $\\sum\\limits_{i=1}^{N}(y_i - \\hat{y_i})$\n\n* y - Actual value of the target\n* $\\hat{y}$ - Prediction of the target\n* N - The number of samples in the training set.\n\n### Variance\n\n* Variance is defined as the variance in predictions when training data changes. Variance can be caused due to Model flexibility (models such as Decision Trees, Polynomial Regression etc..) and due to the noise in data. \n* A model with high variance has low train error and high test error and therefore does not generalize well to the unseen, real-world data.\n\n#### **Variance of the model** = $E[(\\hat{y} - E[\\hat{y}])^2]$\n\n* E - Expectation\n* E[$\\hat{y}$] - Expectation of the Prediction\n\n\n## Causes of Overfitting\n* **Model Flexibility**\n    -> Flexible models such as Polynomial Regression and Decision Trees can cause overfitting.\n* **Noise in data**\n    -> Noisy data can jeopardize the coefficient estimates.\n* **Highly correlated features in the data**\n    -> Correlated features can cause severe problems in the estimation of model coefficients\n* **More features** and **Less samples** in the data.\n\n\n## Techniques to prevent Overfitting\n* Make use of **Ensembling** techniques such as Bagging and Boosting models.\n* Employ **Regularization** techniques such as LASSO and Ridge Regression.\n* Use less flexible models.\n* Collect more samples.\n* Resolve **Multicollinearity** issues (features with high correlation).\n* Remove features with high variance. \n* Employ Dimensionality Reduction techniques such as **Feature selection and Feature Extraction techniques**.","b365c195":"# Regularization\n\nRegularization is a process that involves adding penalties to the cost function and attempting to minimize that cost function. Overfitted model coefficients tend to have large coefficients and in order to reduce large coefficients we use regularizations. \nThe most commonly used regularization techniques are, \n\n-> Lasso Regression \n\n-> Ridge Regression\n\n-> Elastic Net Regression","0fde8cef":"## Elastic Net Regression\nElastic Net Regression is a combination of L1 and L2 regularization that attempts to minimize the following cost function,\n\nL(w) = $\\sum\\limits_{i=1}^{M}(y_i - \\hat{y_i})^2 + \\alpha_1\\sum\\limits_{j=1}^{N} |w_j| + \\alpha_2\\sum\\limits_{j=1}^{N} w_j^2 $\n\n* $\\alpha_1$ & $\\alpha_2$ = penalties that ranges from 0 to 1.\n* $|w_j|$ = absolute value for the coefficients of the model\n* $w_j^2$ = square value for the coefficients of the model\n* $y_i$ = target of the data\n* $\\hat{y_i}$ = prediction of the model","40472642":"# Load the preprocessed data\n\nThe data you feed to the ANN must be preprocessed thouroughly to yield reliable results. The training data has been preprocessed already. The preprocessing steps involved are,\n\n* MICE (Multiple Imputation by Chained Equation) Imputation\n* Log transformation\n* Square root transformation\n* Ordinal Encoding\n* Target Encoding\n* Z-Score Normalization\n\nThe preprocessing is done to House Price Prediction data \n\n-> [Data Source](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data)\n\nFor detailed implementation of the above mentioned steps refer my notebook on data preprocessing:\n\n-> [Link to the Notebook](https:\/\/www.kaggle.com\/srivignesh\/data-preprocessing-for-house-price-prediction)\n\n\n","12469ee2":"## Ridge Regression\nRidge Regression is also called L2 Regularization which is a linear model that attempts to minimize the following cost function,\n\nL(w) = $\\sum\\limits_{i=1}^{M}(y_i - \\hat{y_i})^2 + \\alpha\\sum\\limits_{j=1}^{N} w_j^2 $\n* $\\alpha$ = penalty that ranges from 0 to 1.\n* $w_j^2$ = square value for the coefficients of the model\n* $y_i$ = target of the data\n* $\\hat{y_i}$ = prediction of the model"}}