{"cell_type":{"ec7c2dc0":"code","ec377e84":"code","80f09739":"code","b4830762":"code","0128e390":"code","de869b03":"code","927a69a8":"code","b5af721f":"code","b9e14d9a":"code","042abeef":"code","45e62e17":"code","052d6ea3":"code","b648fa3e":"code","c5f9b560":"code","fd9e4e25":"markdown"},"source":{"ec7c2dc0":"import numpy as np\nimport pandas as pd   \nimport matplotlib.pyplot as plt\nimport random\nimport os\n\nfrom sklearn.metrics import mean_absolute_error\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.callbacks import * \nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras import backend as K\n\n# from kerashypetune import KerasGridSearch","ec377e84":"### READ DATA ###\n\n# df = pd.read_csv('Punta_Salute_2009.csv', sep=';') # orignal data had date and hour\ndf = pd.read_csv('..\/input\/venezia\/venezia.csv',parse_dates=[\"datetime\"],infer_datetime_format=True)\n# match format:\ndf[\"Data\"] = df[\"datetime\"].dt.date\ndf[\"Ora solare\"] = df[\"datetime\"].dt.hour\ndf.drop([\"datetime\"],axis=1,inplace=True, errors=\"ignore\")\ndf = df.dropna()\n\nprint(df.shape)\ndf.head()","80f09739":"### PLOT WEEKLY TREND ###\n\ndf[:7*24]['level'].plot(\n    y='level', x='Ora solare', figsize=(8,6))","b4830762":"### DEFINE T2V LAYER ###\n\nclass T2V(Layer):\n    \n    def __init__(self, output_dim=None, **kwargs):\n        self.output_dim = output_dim\n        super(T2V, self).__init__(**kwargs)\n        \n    def build(self, input_shape):\n\n        self.W = self.add_weight(name='W',\n                                shape=(input_shape[-1], self.output_dim),\n                                initializer='uniform',\n                                trainable=True)\n\n        self.P = self.add_weight(name='P',\n                                shape=(input_shape[1], self.output_dim),\n                                initializer='uniform',\n                                trainable=True)\n\n        self.w = self.add_weight(name='w',\n                                shape=(input_shape[1], 1),\n                                initializer='uniform',\n                                trainable=True)\n\n        self.p = self.add_weight(name='p',\n                                shape=(input_shape[1], 1),\n                                initializer='uniform',\n                                trainable=True)\n\n        super(T2V, self).build(input_shape)\n        \n    def call(self, x):\n        \n        original = self.w * x + self.p\n        sin_trans = K.sin(K.dot(x, self.W) + self.P)\n        \n        return K.concatenate([sin_trans, original], -1)","0128e390":"### CREATE GENERATOR FOR LSTM AND T2V ###\n\nsequence_length = 24 # orig: 24 .  note, 24*7 = 168\n\ndef gen_sequence(id_df, seq_length, seq_cols):\n    \n    data_matrix = id_df[seq_cols].values\n    num_elements = data_matrix.shape[0]\n\n    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n        yield data_matrix[start:stop, :]\n\ndef gen_labels(id_df, seq_length, label):\n    \n    data_matrix = id_df[label].values\n    num_elements = data_matrix.shape[0]\n    \n    return data_matrix[seq_length:num_elements, :]","de869b03":"### DEFINE MODEL STRUCTURES ###\n\ndef set_seed_TF2(seed):\n    \n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    \n\ndef T2V_NN(param, dim):\n    \n    inp = Input(shape=(dim,1))\n    x = T2V(param['t2v_dim'])(inp)\n    x = LSTM(param['unit'], activation=param['act'])(x)\n    x = Dense(1)(x)\n    \n    m = Model(inp, x)\n    m.compile(loss='mse', optimizer=Adam(lr=param['lr']))\n    \n    return m\n\n\ndef NN(param, dim):\n    \n    inp = Input(shape=(dim,1))\n    x = LSTM(param['unit'], activation=param['act'])(inp)\n    x = Dense(1)(x)\n    \n    m = Model(inp, x)\n    m.compile(loss='mse', optimizer=Adam(lr=param['lr']))\n    \n    return m","927a69a8":"### PREPARE DATA TO FEED MODELS ###\n\nX, Y = [], []\nfor sequence in gen_sequence(df, sequence_length, ['level']):\n    X.append(sequence)\n    \nfor sequence in gen_labels(df, sequence_length, ['level']):\n    Y.append(sequence)\n    \nX = np.asarray(X)\nY = np.asarray(Y)","b5af721f":"### TRAIN TEST SPLIT ###\n\ntrain_dim = int(0.7*len(df))\nX_train, X_test = X[:train_dim], X[train_dim:]\ny_train, y_test = Y[:train_dim], Y[train_dim:]\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","b9e14d9a":"### DEFINE PARAM GRID FOR HYPERPARM OPTIMIZATION ###\n\nparam_grid = {\n    'unit': [64,32],\n    't2v_dim': [128,64,16],\n    'lr': [1e-2,1e-3], \n    'act': ['elu','relu'], \n    'epochs': 100, # 200,\n    'batch_size': [128,512,1024]\n}\n\n\nbase_param = {\n    'unit': 64,\n    't2v_dim': 64,\n    'lr': 2e-3, \n    'act': 'relu', \n    'epochs': 100, # 200,\n    'batch_size': 256\n}","042abeef":"### FIT T2V + LSTM ###\n### this is muc hslower if using CPU and not GPU\nes = EarlyStopping(patience=3, verbose=0, min_delta=0.0005, monitor='val_loss', mode='auto', restore_best_weights=True)\n### orig - hyperparam grid search\n# hypermodel = lambda x: T2V_NN(param=x, dim=sequence_length)\n# kgs_t2v = KerasGridSearch(hypermodel, param_grid, monitor='val_loss', greater_is_better=False, tuner_verbose=1)\n# kgs_t2v.set_seed(set_seed_TF2, seed=33)\n# kgs_t2v.search(X_train, y_train, validation_split=0.2, callbacks=[es], shuffle=False)\n###\n## new : skip search, default model\nmodel = T2V_NN(param=base_param, dim=sequence_length)\nmodel.fit(X_train, y_train, validation_split=0.2, callbacks=[es], shuffle=False)\n\n# pred_t2v = kgs_t2v.best_model.predict(X_test).ravel()\npred_t2v = model.predict(X_test).ravel()\nprint(\"MAE\")\nmean_absolute_error(y_test.ravel(), pred_t2v)","45e62e17":"# print(\"model with bigger dim\")\n# ## requires processing data to match new dim\n# model = T2V_NN(param=base_param, dim=168) # 24*7 = 168\n# model.fit(X_train, y_train, validation_split=0.2, callbacks=[es], shuffle=False)\n\n# pred_t2v = model.predict(X_test).ravel()\n# print(\"MAE\")\n# mean_absolute_error(y_test.ravel(), pred_t2v)","052d6ea3":"### VISUALIZE TEST PREDICTIONS ###\n\nplt.figure(figsize=(8,5))\n\nplt.plot(pred_t2v[:365], label='prediction')\nplt.plot(y_test.ravel()[:365], label='true')\nplt.title('T2V plus LSTM'); plt.legend()","b648fa3e":"### FIT SIMPLE LSTM ###\n\ndel param_grid['t2v_dim']\n\n# es = EarlyStopping(patience=5, verbose=0, min_delta=0.001, monitor='val_loss', mode='auto', restore_best_weights=True)\n\n# hypermodel = lambda x: NN(param=x, dim=sequence_length)\n# kgs = KerasGridSearch(hypermodel, param_grid, monitor='val_loss', greater_is_better=False, tuner_verbose=1)\n# kgs.set_seed(set_seed_TF2, seed=33)\n# kgs.search(X_train, y_train, validation_split=0.2, callbacks=[es], shuffle=False)\n\nkgs = NN(param=base_param, dim=sequence_length)\nkgs.fit(X_train, y_train, validation_split=0.2, callbacks=[es], shuffle=False)\n\n# pred_t2v = kgs_t2v.best_model.predict(X_test).ravel()\npred_nn = kgs.predict(X_test).ravel()\nprint(\"MAE\")\nmean_absolute_error(y_test.ravel(), pred_nn)","c5f9b560":"### VISUALIZE TEST PREDICTIONS ###\n\nplt.figure(figsize=(8,5))\n\nplt.plot(pred_nn[:365], label='prediction')\nplt.plot(y_test.ravel()[:365], label='true')\nplt.title('single LSTM'); plt.legend()","fd9e4e25":"* Based on the Time2Vec method + the notebook\/implementation from: https:\/\/github.com\/cerlymarco\/MEDIUM_NoteBook\/blob\/master\/Time2Vec\/Time2Vec.ipynb\n\n* ToDo: try bilstm , try longer sequence\/seasonality. \n* Com pare to simply adding in DoW, date or hour of day, or cyclic embedding of hour of day as feature(s)"}}