{"cell_type":{"10ae795b":"code","8c0d72c9":"code","4e09e8e7":"code","78e76206":"code","fbd0ba2d":"code","723f54b6":"code","f852bdb8":"code","6fa32d18":"code","1dba8cdb":"code","d3235bf4":"code","b657e5fd":"code","23b51c33":"code","9b6a181f":"code","46581b3d":"code","f97b9375":"code","3809ec30":"code","35eb4879":"code","d86b92e0":"code","a6e1c7ae":"code","2be91961":"code","5909ddb0":"code","e4ba39b4":"code","43ce31c3":"code","1ad82386":"code","82f1c4b5":"code","430ed55e":"code","f01919ae":"code","f4acd8e3":"code","cc2bf4c2":"code","4b44302e":"code","f9c89add":"code","204d8d3f":"code","486b1019":"code","e3445ea1":"code","59c3bba0":"code","785efb09":"code","233a6c37":"code","0d42da4a":"code","28176eeb":"code","3acd0d4a":"code","d3e2b3d5":"code","72393374":"code","fa65e75e":"code","0b3e12aa":"code","fad6f582":"code","df7c94ab":"code","bd93d14d":"code","698c531c":"code","e2f034a9":"code","bb74cd02":"code","e65ab2ff":"code","03f80b51":"code","1681d072":"code","1aed0af7":"code","c9db52de":"code","7b406d19":"code","5cf846c3":"code","2d321f10":"code","d8ae0f20":"code","7e060217":"code","08d15ca0":"code","c6f12f92":"code","27032d7b":"code","f54b09ad":"code","edb5bfe9":"code","6eb38292":"code","12023a2d":"markdown","10c742a4":"markdown","588b26b5":"markdown","ce5d7fa7":"markdown","e631d055":"markdown","372087bb":"markdown","47f6746f":"markdown","091b54fe":"markdown","121c40c7":"markdown","9edef5fc":"markdown","3eb62dfe":"markdown","2a608e4e":"markdown","9f36dd86":"markdown","4dd724c8":"markdown","5c720581":"markdown","6ed428ad":"markdown","5bfad946":"markdown","50d190f9":"markdown","b8148f84":"markdown","3a37453a":"markdown","e4723ea7":"markdown","5124254d":"markdown","4c4d9223":"markdown","f1c40167":"markdown","38b77077":"markdown","9e5f269c":"markdown","de3de1b4":"markdown","3e52d162":"markdown","4998d9cd":"markdown","0255f4db":"markdown","d78650d3":"markdown","ec29b069":"markdown","7b781820":"markdown","603ebf84":"markdown","38b2a30d":"markdown","b0c9b8f0":"markdown"},"source":{"10ae795b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c0d72c9":"# Let us import the libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n#data visualization\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns # data visualization\nimport bokeh as bk\n\n\n# Machine Learning Algo\/Models\nfrom sklearn import linear_model\nfrom sklearn import tree, metrics,ensemble\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split \nfrom xgboost import XGBClassifier\nfrom sklearn.decomposition import PCA\nimport graphviz\n","4e09e8e7":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","78e76206":"train_data.info()","fbd0ba2d":"train_data.describe()","723f54b6":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","f852bdb8":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","6fa32d18":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","1dba8cdb":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=1000, random_state=100)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\nmodel.score(X,y)\nacc_random_forest=round(model.score(X,y)*100,2)\nprint(acc_random_forest)\n\n    ","d3235bf4":"import numpy as np\nimport pandas as pd\nimport squarify\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\ntitanic =  pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n#drop and create relevant columns\ntitanic.drop(['Name', 'Ticket'], axis=1, inplace=True)\ntitanic['Cabin_ind'] = np.where(titanic['Cabin'].isnull(), 0, 1)\ngender_num = {'male': 0, 'female': 1}\ntitanic['Sex'] = titanic['Sex'].map(gender_num)\n#titanic.drop(['Cabin', 'Embarked'], axis=1, inplace=True)\nX_train_data = pd.DataFrame(titanic)\nbins= [10,20,30,40,50,60]\nlabels = ['<20','21-30','31-40','41-50','>50']\nX_train_data['AgeGroup'] = pd.cut(X_train_data['Age'], bins=bins, \n                                  labels=labels, right=False)\n# Try a Pie chart\npie_data = (titanic.groupby('Pclass')['Fare'].sum()).to_frame()\npie_data['Fare'] = round((pie_data.Fare\/sum(pie_data.Fare))*100)\nplt.pie(pie_data.Fare, labels=pie_data.index,  \n        startangle=90, autopct='%.1f%%')\nplt.title(\"Chart-1 A pie chart of Fare Class\",\n          fontsize=16,fontweight=\"bold\")\nplt.show()\n# Try a Doughnut chart\ndonut_data = (titanic.groupby('Pclass')['Fare'].sum()).to_frame()\ndonut_data['Fare'] = (donut_data.Fare\/sum(donut_data.Fare))*100\nmy_circle=plt.Circle( (0,0), 0.6, color='white')\nplt.pie(donut_data.Fare, labels=donut_data.index, autopct='%1.1f%%',\n        colors=['red','green','blue'])\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title(\"Chart-2 Titanic Fare Groups in a Donut chart\",\n          fontsize=16,fontweight=\"bold\")\nplt.show()\n# Try a Tree Map chart\nlbl = donut_data.index.join(map(str, donut_data['Fare']))\nlbl = lbl.join(map(str,'%'))\nnorm = mpl.colors.Normalize(vmin=min(donut_data.Fare), \n                            vmax=max(donut_data.Fare))\ncolors = [plt.cm.Spectral(norm(value)) for value in donut_data.Fare]\nsquarify.plot(label=lbl,sizes=donut_data.Fare, \n              color = colors, alpha=.6)\nplt.title(\"Chart-3 Titanic Classes in a Tree Map\",\n          fontsize=16,fontweight=\"bold\")\nplt.axis('off');\nplt.show()\n#Try a chart of the fare by age group\npie_data = (titanic.groupby('AgeGroup')['Fare'].count()).to_frame()\npie_data['AgeGroup'] = round((pie_data.Fare\/sum(pie_data.Fare))*100, 2)\nplt.pie(pie_data.Fare, labels=pie_data.index,  \n        startangle=90, autopct='%.1f%%');\nplt.title(\"Chart-4 Titanic Age Groups in a pie chart\",\n          fontsize=16,fontweight=\"bold\")\nplt.show()\n# Doughnut chart\ndonut_data = (titanic.groupby('AgeGroup')['Fare'].count()).to_frame()\ndonut_data['AgeGroup'] = (donut_data.Fare\/sum(donut_data.Fare))*100\nmy_circle=plt.Circle( (0,0), 0.6, color='white')\nplt.pie(donut_data.Fare, labels=donut_data.index, autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title(\"Chart-5 Titanic Age Groups in a Donut chart\",\n          fontsize=16,fontweight=\"bold\")\nplt.show()\n# Change color\nnorm = mpl.colors.Normalize(\n    vmin=min(donut_data.AgeGroup), \n    vmax=max(donut_data.AgeGroup)\n)\ncolors = [plt.cm.Spectral(norm(value)) for value in donut_data.AgeGroup]\ncolors[1] = \"#FBFCFE\"\nplt.figure(figsize=(10, 6))\nplt.rc('font', size=13)\nplt.title(\"Chart-6 Titanic Agewise Travel Group - In a Treemap\",\n          fontsize=16,fontweight=\"bold\")\nplt.axis('off');\nperc = [str('{:5.2f}'.format(i\/donut_data.AgeGroup.sum()*100)) \n        + \"%\" for i in donut_data['AgeGroup']]\nlbl = [el[0] + \" = \" + el[1] for el in zip(donut_data.index, perc)]\nsquarify.plot(sizes=donut_data['AgeGroup'], label=lbl, alpha=.8, \n              edgecolor=\"white\", linewidth=2)#, color=colors)\nplt.axis('off')\nplt.show()\n# Make data: I have 3 groups and 7 subgroups\ngroup_names=['First Class', 'Second Class', 'Third Class']\ngroup_size=[63.3,13.2,24.4]\nsubgroup_names = labels\nsubgroup_size = []\nfor agegroup in pie_data.AgeGroup:\n    subgroup_size.append(agegroup)\n# Create colors\na, b, c=[plt.cm.Blues, plt.cm.Reds, plt.cm.Greens]\nperc2 = [str('{:5.2f}'.format(i\/donut_data.Fare.sum()*100)) \n         + \"%\" for i in donut_data['Fare']]\nlbl2 = [el[0] + \" = \" + el[1] for el in zip(donut_data.index, perc2)]\n# First Ring (outside)\nfig, ax = plt.subplots()\nax.axis('equal')\nmypie, _ = ax.pie(group_size,\n                  radius=1.3, \n                  labels=group_names,\n                  colors=[a(0.6),b(0.7),c(0.8)]\n                 )\nplt.setp( mypie, width=0.3, edgecolor='white')\nperc = [str('{:5.2f}'.format(i\/donut_data.AgeGroup.sum()*100))\n        + \"%\" for i in donut_data['AgeGroup']]\nlbl = [el[0] + \" = \" + el[1] for el in zip(donut_data.index, perc)]\n# Second Ring (Inside)\nmypie2, _ = ax.pie(subgroup_size, \n                   radius=1.3-0.3, \n                   labels=lbl, #subgroup_names, \n                   labeldistance=0.7, \n                   colors=[c(0.5), c(0.4), c(0.3), b(0.5), b(0.4), \n                           a(0.6), a(0.5), a(0.4), a(0.3), a(0.2)])\nplt.setp( mypie2, width=0.4, edgecolor='white')\nplt.margins(0,0)\nplt.title(\"Chart-7 Titanic Agewise Travel Group \\n In a multi donut chart\\n\",\n          fontsize=16,fontweight=\"bold\")\n# show it\nplt.show()","b657e5fd":"df=titanic","23b51c33":"import plotly.express as px\nfig = px.treemap(df.dropna(subset=['Embarked']), path=['Embarked'], values='Pclass', maxdepth=3, branchvalues='total', color='Pclass', color_continuous_scale='piyg_r')\nfig\n\n","9b6a181f":"fig = px.parallel_categories(df.dropna(subset=['Embarked']), dimensions=['Survived', 'Pclass', 'Sex', 'Parch', 'SibSp', 'Embarked'], color='Survived', color_continuous_scale='tealrose', labels={}, title='Titanic Survivors - Parallel Category Chart', template='ggplot2')\nfig.update_layout(autosize=True)\nfig.update_layout(legend_title_text='Legend title')\nfig\n","46581b3d":"fig = px.treemap(df.dropna(subset=['Embarked']), path=['Survived', 'Sex', 'Pclass', 'Embarked', 'SibSp'], color='Pclass', color_continuous_scale='agsunset')\nfig\n","f97b9375":"fig = px.line_3d(df.dropna(subset=['Embarked']), x='Survived', y='Pclass', z='Sex', color='Embarked', color_discrete_sequence=px.colors.qualitative.Alphabet, template='ggplot2')\nfig.update_layout(autosize=False)\nfig.update_layout(legend_title_text='Legend title')\nfig","3809ec30":"fig = px.density_contour(df.dropna(subset=['Age', 'Embarked']), x='Age', y='Pclass', z='Sex', color='Survived', facet_col='Embarked', hover_name='Survived', color_discrete_sequence=px.colors.qualitative.Vivid, trendline_color_override='lime', template='ggplot2')\nfig.update_layout(legend_title_text='Titanic Survivors Density Contour Chart')\nfig","35eb4879":"X_train_data = pd.DataFrame(titanic)\nbins= [10,20,30,40,50,60]\nlabels = ['Below 20','21 to 30','31 to 40','41 to 50',' Above 50']\nX_train_data['AgeGroup'] = pd.cut(X_train_data['Age'], bins=bins, \n                                  labels=labels, right=False)\n# Try a Histogram chart\nax1 = titanic['Age'].hist(bins=25)\nax1.set_xlabel('Age')\nax1.set_ylabel('Count')\nax1.set_title('Distribution by Age')\nplt.show()","d86b92e0":"ax2 = titanic['Fare'][titanic['Sex']=='male'].hist(bins=25, label='Male')\ntitanic['Fare'][titanic['Sex']=='female'].hist(bins=25, ax = ax2, label='Female')\nax2.set_xlabel('Fare')\nax2.set_ylabel('Count')\nax2.set_title('Distribution by Fare')\nax2.legend()\nplt.show()","a6e1c7ae":"X_train_data = pd.DataFrame(titanic)\nbins= [10,20,30,40,50,60]\nlabels = ['Below 20','21 to 30','31 to 40','41 to 50',' Above 50']\nX_train_data['AgeGroup'] = pd.cut(X_train_data['Age'], bins=bins, \n                                  labels=labels, right=False)\n# Try a Histogram chart\nax1 = titanic['Age'].hist(bins=25)\nax1.set_xlabel('Age')\nax1.set_ylabel('Count')\nax1.set_title('Distribution by Age')\nplt.show()","2be91961":"titanic['Survived']=titanic['Survived'].map({0: 'Dead', 1: 'Survived'})\ntitanic.groupby(['Sex','Survived']).size().unstack().plot(kind='bar', stacked=True)\nax.set_xlabel('Gender')\nplt.ylabel('Count')\nplt.title('Survival rate by Gender')","5909ddb0":"pclass_count =titanic['Pclass'].value_counts()\nax = pclass_count.plot.bar()\npclass_count.rename({1: 'First Class', 2:'Second Class', 3:'Third Class'}, inplace=True)\nax.set_ylabel('Count', rotation=90)\nax.set_xlabel('Passenger Class')\nax.set_title('Pclass vs Count')","e4ba39b4":"titanic.boxplot(by='Pclass', column=['Age'])","43ce31c3":"titanic.plot(kind='density', subplots=True, layout=(3,3), \n             lw=5,ls='--',sharex=False , figsize=(12,10))","1ad82386":"titanic.plot(kind='box', subplots=True, layout=(3,3), sharex=False,sharey=False ,figsize =(10,10))","82f1c4b5":"titanic['Fare'].plot.kde(lw=5,ls='-.')\nax.set_xlabel('Fare')\nax.set_ylabel('Density')\nax.set_title('Distribution by Fare')\nax.legend()","430ed55e":"titanic['Age'].hist(bins=100,lw=5,ls='-.')","f01919ae":"colors = [\"#FFCA00\", \"#15FF58\",\"#E70488\", \"#66CA00\", \"#150458\",\"#E70FF8\" ,\"#880458\", \"#88CA00\", \"#870488\"]\npie_data = (titanic.groupby('AgeGroup')['Fare'].count()).to_frame()\npie_data['AgeGroup'] = round((pie_data.Fare\/sum(pie_data.Fare))*100, 2)\npie_data['AgeGroup'].plot.pie(labels=pie_data.index, subplots=True,\n        startangle=90, autopct='%.1f%%',colors=colors)","f4acd8e3":"colors = [\"#FFCA00\", \"#15FF58\",\"#E70488\", \"#66CA00\", \"#150458\",\"#E70FF8\" ,\"#880458\", \"#88CA00\", \"#870488\"]\npie_data = (titanic.groupby('Sex')['Fare'].count()).to_frame()\npie_data['Gender'] = round((pie_data.Fare\/sum(pie_data.Fare))*100, 2)\n\npie_data['Gender'].plot.pie(labels=pie_data.index, subplots=True,\n        startangle=90, autopct='%.1f%%',colors=colors)","cc2bf4c2":"pie_data = (titanic.groupby('Survived')['Fare'].count()).to_frame()\npie_data['Surv'] = round((pie_data.Fare\/sum(pie_data.Fare))*100, 2)\n\npie_data['Surv'].plot.pie(labels=pie_data.index, subplots=True,\n        startangle=90, autopct='%.1f%%',colors=colors[:2])","4b44302e":"pd.plotting.autocorrelation_plot(titanic.Fare)","f9c89add":"from pandas.plotting import radviz\ntitanic_radviz=titanic.filter(['Age','Fare','Survived','Pclass'],axis=1)\ntitanic_radviz.head()\ntitanic_radviz.ffill(axis=0, inplace=True, limit=None, downcast=None)\ntitanic_radviz.head(40)\nradviz(titanic_radviz, 'Survived')","204d8d3f":"pd.plotting.parallel_coordinates(titanic_radviz, 'Survived', colormap='prism')","486b1019":"titanic.info","e3445ea1":"from matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 16,12\nplt.style.use('fivethirtyeight')\n\ntitanic2 =  pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic2.drop(['PassengerId'], axis=1, inplace=True)\ntitanic_PT_1 = pd.pivot_table(data=titanic2,index=['Sex'])\ntitanic_PT_1","59c3bba0":"titanic_PT_1.plot(kind='bar')","785efb09":"plt.style.use('fivethirtyeight')\ntitanic_pivot = pd.pivot_table(titanic2,index=['Survived'],# 'Sex', #'Pclass'],\n                        columns=['Embarked'],\n                        values=['Age'],\n                        aggfunc=np.mean,\n                        fill_value=np.mean(titanic2['Age']))\ntitanic_pivot","233a6c37":"titanic_pivot.plot(kind='bar')","0d42da4a":"!pip -q install graphviz\n","28176eeb":" # http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nclf = XGBClassifier()\nclf = clf.fit(X, y)","3acd0d4a":"#Test","d3e2b3d5":"# Predict the full data set\nexpected = y\npredicted = clf.predict(X)","72393374":"# Display metrics\n# Precision measures the impact of false positives: TP\/(TP+FP)\n# Recall measures the impact of false negatives : TP\/(TP+FN)\n# F1 is the weighted average of precision and recall: (2*Recall*Precision)\/(Recall+Precision)\nprint(metrics.classification_report(expected, predicted))","fa65e75e":"# Display confusion matrix\nprint(metrics.confusion_matrix(expected, predicted))","0b3e12aa":" # Display feature importance\nprint(clf.feature_importances_)","fad6f582":"model = LogisticRegression(solver='liblinear', multi_class='ovr')\nmodel.fit(X, y)","df7c94ab":" # Predict\nexpected = y\npredicted = model.predict(X)","bd93d14d":"# Display metrics\n# Precision measures the impact of false positives: TP\/(TP+FP)\n# Recall measures the impact of false negatives : TP\/(TP+FN)\n# F1 is the weighted average of precision and recall: (2*Recall*Precision)\/(Recall+Precision)\nprint(metrics.classification_report(expected, predicted))","698c531c":"# Display confusion matrix\nprint(metrics.confusion_matrix(expected, predicted))","e2f034a9":"# Display feature importance\nprint(clf.feature_importances_)","bb74cd02":"# http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\nclf = tree.DecisionTreeClassifier(max_depth=3)\nclf = clf.fit(X, y)","e65ab2ff":"model = LogisticRegression(solver='liblinear', multi_class='ovr')\nmodel.fit(X, y)\n\nY_pred = model.predict(X)\nacc_log = round(model.score(X, y) * 100, 2)\nacc_log","03f80b51":"# Display metrics\n# Precision measures the impact of false positives: TP\/(TP+FP)\n# Recall measures the impact of false negatives : TP\/(TP+FN)\n# F1 is the weighted average of precision and recall: (2*Recall*Precision)\/(Recall+Precision)\nprint(metrics.classification_report(expected, predicted))","1681d072":"# Display confusion matrix\nprint(metrics.confusion_matrix(expected, predicted))","1aed0af7":"# Display feature importance\nprint(clf.feature_importances_)","c9db52de":"svc = SVC()\nsvc.fit(X, y)\nY_pred = svc.predict(X)\nacc_svc = round(svc.score(X,y) * 100, 2)\nacc_svc","7b406d19":"gaussian = GaussianNB()\ngaussian.fit(X, y)\nY_pred = gaussian.predict(X)\nacc_gaussian = round(gaussian.score(X, y) * 100, 2)\nacc_gaussian","5cf846c3":"\n#model = KMeans(n_clusters=3)\n#model.fit(X)\n\nknn_model = KNeighborsClassifier(n_neighbors = 3)\nknn_model.fit(X, y)\nY_pred = knn_model.predict(X)\nval_knn = round(knn_model.score(X, y) * 100, 2)\nprint(val_knn)","2d321f10":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\ntitanic_pca = pca.fit(X).transform(X)\n\nprint(titanic_pca.shape)\nprint('explained variance ratio: %s' % str(pca.explained_variance_ratio_))","d8ae0f20":"X.columns","7e060217":"#Importing warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error\nfrom sklearn.linear_model import LogisticRegression,Lasso,LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn import metrics\nfrom scipy.stats import zscore\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor,GradientBoostingRegressor\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\nfrom sklearn.gaussian_process.gpr import GaussianProcessRegressor\nfrom  sklearn.isotonic import IsotonicRegression\nfrom sklearn.linear_model.bayes import ARDRegression\nfrom sklearn.linear_model.huber import HuberRegressor\nfrom sklearn.linear_model.base import LinearRegression\nfrom sklearn.linear_model.passive_aggressive import PassiveAggressiveRegressor \n#from sklearn.linear_model.randomized_l1 import RandomizedLogisticRegression\nfrom sklearn.linear_model.stochastic_gradient import SGDRegressor\nfrom sklearn.linear_model.theil_sen import TheilSenRegressor\nfrom sklearn.linear_model.ransac import RANSACRegressor\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.neighbors.regression import KNeighborsRegressor\nfrom sklearn.neighbors.regression import RadiusNeighborsRegressor\nfrom sklearn.neural_network.multilayer_perceptron import MLPRegressor\nfrom sklearn.tree.tree import DecisionTreeRegressor\nfrom sklearn.tree.tree import ExtraTreeRegressor\nfrom sklearn.svm.classes import SVR\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.cross_decomposition import CCA\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Lars\nfrom sklearn.linear_model import LarsCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import LassoLars\nfrom sklearn.linear_model import LassoLarsIC\nfrom sklearn.linear_model import LassoLarsCV\nfrom sklearn.linear_model import MultiTaskElasticNet\nfrom sklearn.linear_model import MultiTaskElasticNetCV\nfrom sklearn.linear_model import MultiTaskLasso\nfrom sklearn.linear_model import MultiTaskLassoCV\nfrom sklearn.svm import NuSVR\nfrom sklearn.linear_model import OrthogonalMatchingPursuit\nfrom sklearn.linear_model import OrthogonalMatchingPursuitCV\nfrom sklearn.cross_decomposition import PLSCanonical\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR","08d15ca0":"sc = StandardScaler()\na = sc.fit_transform(X)\ndf_x = pd.DataFrame(a,columns=X.columns)\ndf_x.head()\n","c6f12f92":"#Splitting the data into training and testing data\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(df_x,y,test_size=0.20,random_state=45)","27032d7b":"model = [AdaBoostRegressor(),\n         ARDRegression(),\n         BayesianRidge(),\n         CCA(),\n         DecisionTreeRegressor(),\n         ElasticNet(),\n         ElasticNetCV(),\n         ExtraTreeRegressor(),\n         GaussianProcessRegressor(),\n         GradientBoostingRegressor(),\n         HuberRegressor(),\n         #IsotonicRegression(), #Not feasible with the dataset\n         KernelRidge(),\n         KNeighborsRegressor(),\n         #LabelEncoder(), #Not feasible with the dataset\n         Lars(),\n         LarsCV(),\n         Lasso(),\n         LassoCV(),\n         LassoLars(),\n         LassoLarsCV(),\n         LassoLarsIC(),\n         LinearRegression(),\n         LinearSVR(),\n         MLPRegressor(),\n         #MultiOutputRegressor(), #Not feasible with the dataset\n         #MultiTaskElasticNet(),\n         #MultiTaskElasticNetCV(),\n         #MultiTaskLasso(),\n         #MultiTaskLassoCV(),\n         OrthogonalMatchingPursuit(),\n         OrthogonalMatchingPursuitCV(),\n         PassiveAggressiveRegressor(),\n         #PCA(), #Not feasible with the dataset\n         PLSCanonical(),\n         PLSRegression(),\n         RadiusNeighborsRegressor(),\n         #RandomizedLogisticRegression(), #Not feasible with the dataset\n         #RANSACRegressor(),\n         Ridge(),\n         RidgeCV(),\n         SGDRegressor(),\n         #StandardScaler(),#Not feasible with the dataset\n         SVR(),\n         TheilSenRegressor(),\n         NuSVR()\n]\nmodel_name = []\nmodel_score = []\nmodel_mae = []\nmodel_mse = []\nmodel_rmse = []\nmodel_r2= []\n\nfor m in model:\n    #print(m)\n    m.fit(x_train,y_train)\n    score=m.score(x_train,y_train)\n    predm=m.predict(x_test)\n    model_name.append(m) \n    model_score.append(score) \n    model_mae.append(mean_absolute_error(y_test,predm))\n    model_mse.append(mean_squared_error(y_test,predm))\n    model_rmse.append(np.sqrt(mean_squared_error(y_test,predm)))\n    model_r2.append(r2_score(y_test,predm))\n\nlist_of_models = list(zip(model_name,\n                          model_score,\n                          model_mae,\n                          model_mse,\n                          model_rmse,\n                          model_r2\n                         )\n                     )\nlist_of_models\ndfm = pd.DataFrame(list_of_models,columns=['Model','Score','MAE','MSE','RMSE','R2 Score'])\ndfm.sort_values('Score', ascending=False)\ndfm.style.background_gradient(cmap ='RdYlGn')\n","f54b09ad":"print(\"*\" * 50)\nprint(\"The Best Model is : \")\nprint(\"*\" * 50)\nprint(dfm.loc[dfm['Score'].idxmax()])\nprint(\"*\" * 50)","edb5bfe9":"print(\"*\" * 50)\nprint(\"The Best R2 Score is : \")\nprint(\"*\" * 50)\nprint(dfm.loc[dfm['R2 Score'].idxmax()])\nprint(\"*\" * 50)","6eb38292":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","12023a2d":"#### Titanic Survival : Survival Vs Dead Pie Chart","10c742a4":"### Let us load the data","588b26b5":"## Let us try some Plotly","ce5d7fa7":"#### Logistic Regression ","e631d055":"# Titanic Dataset - Analysis of a Disaster","372087bb":"**Random Forest Classifier**","47f6746f":"#### Decision Trees Method","091b54fe":"#### XGBoost Method","121c40c7":"### Let us try different types of regression","9edef5fc":"#### Support Vector Machines","3eb62dfe":"#### Titanic Survival : Histogram based on Bins","2a608e4e":"### Let us load Kaggle Environment","9f36dd86":"#### Titanic Survival : Survival Vs Dead Stacked Bar chart by Gender","4dd724c8":"### Let us see some information","5c720581":"#### Titanic Survival : RadViz Chart","6ed428ad":"#### Titanic Survival : Density Plot with Subplots","5bfad946":"# Some EDA #","50d190f9":"#### Titanic Survival : Simple Bar Chart","b8148f84":"#### Titanic Survival : Survival Vs Dead Gender Based Pie Chart","3a37453a":"#### PAC","e4723ea7":"## Let us use some Pivot Table Concepts","5124254d":"#### Titanic Survival : Kernel Density Estimate ","4c4d9223":"### Let us load all the libraries","f1c40167":"### Let us see survival stats","38b77077":"## Let us try using Pandas","9e5f269c":"#### Titanic Survival : A Parallel Coordinates Chart","de3de1b4":"As you can see from the info  *2 Embarked, ~687 Cabin 177 Age columns are null*","3e52d162":"#### Titanic Survival : Autocorrelation Chart","4998d9cd":"### Let us try all types of regression using Scikit Learn library\n> model = [AdaBoostRegressor(), ARDRegression(), BayesianRidge(), CCA(), DecisionTreeRegressor(), ElasticNet(), ElasticNetCV(), ExtraTreeRegressor(), GaussianProcessRegressor(), GradientBoostingRegressor(), HuberRegressor(),\n\n>     #IsotonicRegression(), #Not feasible with the dataset\n>     KernelRidge(),\n>     KNeighborsRegressor(),\n>     #LabelEncoder(), #Not feasible with the dataset\n>     Lars(),\n    LarsCV(),\n    Lasso(),\n    LassoCV(),\n    LassoLars(),\n    LassoLarsCV(),\n    LassoLarsIC(),\n    LinearRegression(),\n    LinearSVR(),\n    MLPRegressor(),\n    #MultiOutputRegressor(), #Not feasible with the dataset\n    MultiTaskElasticNet(),\n    MultiTaskElasticNetCV(),\n    MultiTaskLasso(),\n    MultiTaskLassoCV(),\n    OrthogonalMatchingPursuit(),\n    OrthogonalMatchingPursuitCV(),\n    PassiveAggressiveRegressor(),\n    #PCA(), #Not feasible with the dataset\n    PLSCanonical(),\n    PLSRegression(),\n    RadiusNeighborsRegressor(),\n    #RandomizedLogisticRegression(), #Not feasible with the dataset\n    RANSACRegressor(),\n    Ridge(),\n    RidgeCV(),\n    SGDRegressor(),\n    #StandardScaler(),#Not feasible with the dataset\n    SVR(),\n    TheilSenRegressor(),\n    NuSVR()\n]","0255f4db":"#### Titanic Survival : Box Plot with no share of Axis","d78650d3":"#### K-Means","ec29b069":"### Let us do some Exploratory Data Analysis ###","7b781820":"#### Titanic Survival : Box Plot based on Travel Class","603ebf84":"## Let us try Matplotlib","38b2a30d":"#### Titanic Survival : Age Group Pie Chart","b0c9b8f0":"#### Titanic Survival :Let us try a Pivot Table"}}