{"cell_type":{"14b9a5e9":"code","25a14873":"code","5802b7f8":"code","afb1162a":"code","8379a007":"code","571c6e65":"code","0c201884":"code","4a5d7a6a":"code","a2fe59b9":"code","fc05477a":"code","92d45522":"code","38a74efc":"code","b6799a84":"code","5712cf8d":"code","35b46611":"code","42534248":"code","c6bb6c4b":"code","2ad90c61":"code","60b9dc98":"code","9f248234":"code","9b1db919":"code","01a68f62":"code","e8984875":"code","fa223b64":"code","11d740f3":"code","4dccb9b1":"markdown","559e41dd":"markdown","c322f1f2":"markdown","c19c31c4":"markdown","c9ed8ba9":"markdown","65a56b4e":"markdown","91bb2307":"markdown","8967e822":"markdown","d3e95650":"markdown","39905195":"markdown","e24c5cad":"markdown"},"source":{"14b9a5e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","25a14873":"df = pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')\ndf.head()","5802b7f8":"sns.heatmap(df.isnull(),)","afb1162a":"df.info(), len(df)","8379a007":"cats = df.select_dtypes('O')\nnumerical = df.select_dtypes(['int', 'float'])\ntarget = 'charges'\nnumerical = numerical.drop(columns = target)\ncat = cats.columns\ncats.columns, numerical.columns","571c6e65":"for i, column in enumerate(cats):\n    sns.catplot(x = column, y = target, kind = 'violin', data = df)","0c201884":"sns.boxplot(x = target, data = df)\nlen(df.loc[df.charges > 45000])\/len(df)","4a5d7a6a":"len(df.loc[df['charges'] > 45000])\/len(df)","a2fe59b9":"fig, ax = plt.subplots(3, 3, figsize = (15, 15))\nax = ax.flatten()\nfor i, column in enumerate(numerical):\n    sns.scatterplot(x = column, y = target, hue = cat[0], ax = ax[3 * i], data = df)\n    sns.scatterplot(x = column, y = target, hue = cat[1], ax = ax[3 * i + 1], data = df)\n    sns.scatterplot(x = column, y = target, hue = cat[2], ax = ax[3 * i + 2], data = df)","fc05477a":"sns.catplot(x = 'smoker', y = target, hue = 'sex', kind = 'violin', split = True, data = df)\nsns.catplot(x = 'sex', y = 'age', kind = 'violin', data = df)\nsns.catplot(x = 'sex', y = 'bmi', hue = 'smoker', split = True, kind = 'violin', data = df)#sns.scatterplot(x = 'age', y = target, hue = 'smoker', style = 'sex', data = df)","92d45522":"from collections import defaultdict\n\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.svm import SVR\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline","38a74efc":"def get_scores(X, y, z, folds, models, model_names):\n    train_rmse = defaultdict(list)\n    \n    val_rmse = defaultdict(list)\n    \n    mmx = MinMaxScaler()\n    for train_idx, val_idx in folds.split(X, z):\n        for model, name in zip(models, model_names):\n            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n            model.fit(X_train, y_train)\n            train_preds = model.predict(X_train)\n            train_rmse[name].append(mean_squared_error(y_train, train_preds, squared = False))\n            \n            val_preds = model.predict(X_val)\n            \n            val_rmse[name].append(mean_squared_error(y_val, val_preds, squared = False))\n            \n    return train_rmse, val_rmse","b6799a84":"def print_scores(name, scores, score_name, typ):\n    npscores = np.array(scores)\n    avg = np.average(npscores)\n    mx = np.max(npscores)\n    mn = np.min(npscores)\n    print(f'{typ} {name}:\\n + Average {score_name}: {avg} \\n Max {score_name}: {mx} \\n Min {score_name}: {mn}')","5712cf8d":"X = df[['smoker', 'bmi', 'age']]\ny = df[target]\nz = y.map(lambda x: x \/\/ 10000)\nX.loc[:,'smoker'] = X.loc[:,'smoker'].map(lambda x: 1 if x == 'yes' else 0)\nxgb = XGBRegressor()\nforest = RandomForestRegressor()\nridge = Ridge() \nsvr = SVR()\nknn = KNeighborsRegressor()\n\nmodels = [ridge, forest, xgb, svr, knn]\nmodel_names = ['Ridge', 'Forest', 'XGB', 'SVR', 'KNN']\n\nskf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 2)\n\ntrain_rmse, val_rmse = get_scores(X, y, z, skf, models, model_names)\n\nfor name in model_names:\n    print_scores(name, train_rmse[name], 'RMSE', 'Train')\n    print_scores(name, val_rmse[name], 'RMSE', 'Val')","35b46611":"X = df[['smoker', 'bmi', 'age', 'sex']]\nX.loc[:,'smoker'] = X.loc[:,'smoker'].map(lambda x: 1 if x == 'yes' else 0)\nX.loc[:,'sex'] = X.loc[:,'sex'].map(lambda x: 1 if x == 'male' else 0)\n\n\nmodels = [ridge, forest, xgb]\nmodel_names = ['Ridge', 'Forest', 'XGB']\n\ntrain_rmse, val_rmse = get_scores(X, y, z, skf, models, model_names)\n\nfor name in model_names:\n    print_scores(name, train_rmse[name], 'RMSE', 'Train')\n    print_scores(name, val_rmse[name], 'RMSE', 'Val')","42534248":"X = df[['smoker', 'bmi', 'age', 'region']]\nX.loc[:,'smoker'] = X.loc[:,'smoker'].map(lambda x: 1 if x == 'yes' else 0)\n\nct = ColumnTransformer([('OneHotEncoding', OneHotEncoder(), ['region'])], remainder = 'passthrough')\npiped_models = []\nmodels = [ridge, forest, xgb]\nfor model in models:\n    piped_models.append(Pipeline([('Onehot', ct), ('Model', model)]))\nmodel_names = ['Ridge', 'Forest', 'XGB']\n\ntrain_rmse, val_rmse = get_scores(X, y, z, skf, piped_models, model_names)\n\nfor name in model_names:\n    print_scores(name, train_rmse[name], 'RMSE', 'Train')\n    print_scores(name, val_rmse[name], 'RMSE', 'Val')","c6bb6c4b":"#ct = ColumnTransformer([('Scaling', MinMaxScaler(), ['bmi']), ('OneHotEncoding', OneHotEncoder(), ['region'])], remainder = 'passthrough')\n#pipe = Pipeline([('Column Transformer', ct), ('forest', RandomForestRegressor())])\n#^Legacy stuff keeping for future references\nX = df[['smoker', 'bmi', 'age']]\nX.loc[:,'smoker'] = X.loc[:,'smoker'].map(lambda x: 1 if x == 'yes' else 0)\nparam_grid = {'n_estimators': [25, 50, 75], 'max_leaf_nodes': [8, 10, 12]}\ngrid = GridSearchCV(RandomForestRegressor(), param_grid = param_grid, scoring = 'neg_mean_squared_error', cv = skf.split(X, z))\ngrid.fit(X, y)","2ad90c61":"grid.best_params_, np.sqrt(-grid.best_score_)","60b9dc98":"grid_results = pd.DataFrame(grid.cv_results_)\ngrid_results.loc[:, 'split0_test_score' : 'mean_test_score'] = grid_results.loc[:, 'split0_test_score': 'mean_test_score'].apply(lambda x: np.sqrt(-x))\ngrid_results","9f248234":"#ct = ColumnTransformer([('Scaling', MinMaxScaler(), ['bmi']), ('OneHotEncoding', OneHotEncoder(), ['region'])], remainder = 'passthrough')\n#pipe = Pipeline([('Column Transformer', ct), ('ridge', Ridge())])\nparam_grid = {'alpha': [0, 0.01, 0.05, 0.1, 0.5, 1, 2, 5, 10]}\ngrid = GridSearchCV(Ridge(), param_grid = param_grid, scoring = 'neg_mean_squared_error', cv = skf.split(X, z))\ngrid.fit(X, y)","9b1db919":"grid.best_params_, np.sqrt(-grid.best_score_)","01a68f62":"#ct = ColumnTransformer([('Scaling', MinMaxScaler(), ['bmi']), ('OneHotEncoding', OneHotEncoder(), ['region'])], remainder = 'passthrough')\n#X = df[['smoker', 'bmi', 'age', 'region']]\n#X.loc[:,'smoker'] = X.loc[:,'smoker'].map(lambda x: 1 if x == 'yes' else 0)\nforest = RandomForestRegressor(max_leaf_nodes = 10, n_estimators = 50)\n#pipe = Pipeline([('Transformer', ct), ('forest', forest)])\ntrain_rmse, val_rmse = get_scores(X, y, z, skf, [forest], ['forest'])\n\nprint_scores(forest, train_rmse['forest'], 'RMSE', 'Train')\nprint_scores(forest, val_rmse['forest'], 'RMSE', 'Val')","e8984875":"#Legacy Stuff\n#from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\n\n#rskf = RepeatedStratifiedKFold(n_splits = 3, n_repeats = 5, random_state = 2)\n#no_grid_forest = RandomForestRegressor()\n#no_grid_pipe = Pipeline([('Transformer', ct), ('no_forest', no_grid_forest)])\n#grid_pipe = Pipeline([('Transformer', ct), ('forest', forest)])\n\n#no_grid_score = cross_val_score(no_grid_pipe, X, y, scoring = 'neg_mean_squared_error', cv = rskf.split(X, z))\n#grid_score = cross_val_score(grid_pipe, X, y, scoring = 'neg_mean_squared_error', cv = rskf.split(X, z))\n#no_grid_score = cross_val_score(no_grid_pipe, X, y, scoring = 'neg_mean_squared_error', cv = 5)\n#grid_score = cross_val_score(grid_pipe, X, y, scoring = 'neg_mean_squared_error', cv = 5)\n\n\n","fa223b64":"X_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8)\nforest.fit(X_train, y_train)\npreds = forest.predict(X_val)\n\nsns.scatterplot(x = X_val['age'], y = y_val, color = 'b')\nsns.scatterplot(x = X_val['age'], y = preds, color = 'r')","11d740f3":"mean_absolute_error(y_val, preds)","4dccb9b1":"Non smoking males and females have essentially the same cost distributions. Smokers have two 'central' charges, one hear 20k, and one near 40k. Male smokers are more likely to be at the 40k region, than the female smokers, and this is not due to any differences in the age distributions (which also could account for the differences in charges). So the bands in the charges with age can be seen as non-smokers (lowest band), mostly smoking women with some smoking men.\n\nIn the bottom graph, we see that women who smoke have a lower bmi, while men who smoke have a higher bmi. BMI is correlated with the insurance prices for smokers, so this may explain why smoking women have a distribution that favors lower costs.","559e41dd":"Males and Females have a similar charge distribution, however, males have a slightly larger distribution at costs near 45k, while having a slightly smaller distribution near charges of 5k. We should keep this parameter\nSmoking has a great effect on the surance rages, accounting for most, if not all of the charges abouve 35k, while no smoking accounts for all of the charges around <5k. This is a very important parameter to keep\nRegion has very little effect on the charges. There is a slight change in distribution among the regions, similar to the male\/female split. I'll add this in a different model to see if it makes any difference.","c322f1f2":"The fit looks pretty good for the topmost band, and seems fine for the inmddle band, however, we are typically overestimating the lowest band for some reason. Finally let's look at the MAE which shouldn't exaggerate the effects of outliers","c19c31c4":"We increased the avg ","c9ed8ba9":"Charges > 45 K make up 3% of our data, however, it is tied closely with high age and smoking. I'll keep it around to help predict those values","65a56b4e":"\nThe important parameters appear to be:\n1. Smoking, this clearly has the largest effect\n2. BMI, there's a strong effect with increasing BMI if the client is a smoker\n3. Age\n\nParameters that might have some contributions:\n1. Sex, the distributions for smoking charges are different for men and women, however this is likely due to smoking women having a lower bmi than men.\n2. Region, ","91bb2307":"We can see that the Random Forest and XGB are oveerfitting the data, based on the average scores, however they perform better than the Ridge Regressor. What is more concerning, however, is that the overfitting is causing unstable results, for example the VAL RMSE for the forest regressor ranges from 1823 - 4882, which is 2.5 times the smalled rmse. I'm going to check a few more parameters before I start playing with the estimators to try to get them to be more stable.\n\nKNN and SVR seem like really bad model choices","8967e822":"So we can see that sex is fairly unimportant, as expected","d3e95650":"The region is also a fairly unimportant feature, which is somewhat expected","39905195":"Now instead of heavily overfitting the training data, we are only slightly overfitting it, with the avg Train RMSE ~ 5% higher than the avg Val RMSE. As a result, our val RMSE decreased by 10%, a fairly big difference.","e24c5cad":"Our target variable is 'Charges' which I will interchange with costs\n\n1. The top three graphs show an increase with charges as the insured ages. By eye, the effect is at 25% for the highest band, and maybe 400% for the lower band\n    However, the charges per each age is highly segregated into three bands, with the highest and lowest bands being attributable to the smoking\/nonsmoking\n    divide. At teh youngest ages, the higher band is an order of magnitude higher in cost then the lowest band. The reasonining behind the middle band cannot be\n    determined by the current graphs. It does appear that men pay higher than women, but maybe this is a smoking correlation? I'll check to see\n    what women\/men smoking looks like, and if there are separate bands for this\n2. BMI is not really a factor for charges if you don't smoke, otherwise there is a strong correlatoin between bmi and smoking\n3. Having more kids doesn't seem to have an effect until after three children, although this might be due to a lack of samples\n4. The effect of regions is difficult to see due to the muddied graphs. For example, orange (southeast) appear to have less children, and a higher bmi\n    especially when compared to green (northwest).\n    \nMore investigation"}}