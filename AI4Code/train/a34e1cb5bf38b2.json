{"cell_type":{"380d16d6":"code","c0f835cc":"code","5a7da5f8":"code","6a7ccb3f":"code","d4b36eb0":"code","7c167dd3":"code","c2c11ba1":"code","91931683":"code","b17bbcdd":"code","000e07a8":"code","88433b77":"code","64e0bf6e":"code","b6cf41ad":"code","f16508a1":"code","2abfbda9":"code","414a1b43":"code","3505c259":"code","5888b929":"code","bca076aa":"code","4ff74d78":"code","0980d017":"code","e2441385":"code","ac673154":"code","93f8d533":"code","ed22a7d0":"code","57aac116":"code","2be7b9a4":"code","0f713e12":"code","493e74f4":"code","d9f9188c":"code","e48c9f82":"code","078ffee3":"code","ac8e0936":"code","3402206c":"code","4ae2ee31":"code","a160942e":"code","46b73e41":"code","39505b19":"code","fc124e0e":"code","cd441b34":"code","bf3a29ce":"code","75727255":"code","c3150a9c":"code","a02848fa":"code","3c5bc164":"code","4b76b2be":"code","d82bea7c":"code","0adb8b8a":"markdown","b5bee5e0":"markdown","65f4ea18":"markdown","004d6109":"markdown","c784a876":"markdown","c9050032":"markdown","b06070e1":"markdown","f6790d7c":"markdown","540acfba":"markdown","92089e6f":"markdown"},"source":{"380d16d6":"pip install newspaper3k","c0f835cc":"# Please import necessary packages\nimport numpy as np\nimport pandas as pd\nfrom time import sleep\nimport newspaper\nfrom newspaper import Article\nfrom google.cloud import bigquery\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom nltk import FreqDist\nfrom nltk import bigrams\nfrom nltk.stem import SnowballStemmer\nimport itertools\nimport collections","5a7da5f8":"## lets collect most recent articles from the verge\n\npaper = newspaper.build('https:\/\/arstechnica.com\/', memoize_articles=False)","6a7ccb3f":"len(paper.articles)","d4b36eb0":"j=0\nfor article in paper.articles[:30]:\n    print(j, article.url)\n    j=j+1","7c167dd3":"# Client is needed for configuring API requests. Leaving it empty will initiate Kaggle's public dataset BigQuery integration.\nclient = bigquery.Client()","c2c11ba1":"# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","91931683":"# Let's create our first SQL query on HN database. \n\nquery = \"\"\"\n    #standardSQL\n    SELECT REGEXP_EXTRACT(url, '\/\/([^\/]*)\/?') domain, COUNT(*) c\n    FROM `bigquery-public-data.hacker_news.full`\n    WHERE url!='' AND (REGEXP_CONTAINS(text, r\"(p|P)rivacy\") OR REGEXP_CONTAINS(title, r\"(P|p)rivacy\")) AND timestamp > '2018-01-01' AND type='story' \n    GROUP BY domain ORDER BY c DESC LIMIT 30\n\"\"\"","b17bbcdd":"query","000e07a8":"# For more details on using BQ see: https:\/\/www.kaggle.com\/michapaliski\/hackernews-analysis-with-bigquery\n\n# Set up the query\nquery_job = client.query(query)\n# API request - run the query, and return a pandas DataFrame\ndf = query_job.to_dataframe()","88433b77":"df","64e0bf6e":"query = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE (REGEXP_CONTAINS(text, r\"(p|P)rivacy\") OR REGEXP_CONTAINS(title, r\"(P|p)rivacy\")) AND timestamp > '2018-01-01' AND type='story'\n        \"\"\"","b6cf41ad":"# Set up the query\nquery_job = client.query(query)\n# API request - run the query, and return a pandas DataFrame\ndf = query_job.to_dataframe()","f16508a1":"# Let's see how many stories don't have urls\nprint(df.shape)\ndf=df[~df['url'].isna()]\nprint(df.shape)","2abfbda9":"# Some users might refer to the same story. We get rid of duplicates.\ndf=df.drop_duplicates(subset=['url'])\ndf.reset_index(inplace=True)\nlen(df)","414a1b43":"# An example story\ndf['url'][0]","3505c259":"# We point newspaper to the first story's url\narticle = Article(df['url'][0])\n# Next, we download the source code\narticle.download()\n# we parse the html\narticle.parse()","5888b929":"# now we can access different elements of the article like:","bca076aa":"# publication date\narticle.publish_date","4ff74d78":"# or article's body\narticle.text[:1000]","0980d017":"# We can also use some cool nlp features provided by the newspaper3k\narticle.nlp()","e2441385":"# e.g. we can retrieve the keywords\narticle.keywords","ac673154":"# or use newspaper3k for creating an automated summary of the article for us\narticle.summary","93f8d533":"# We sort stories by their score\ndf=df.sort_values('score',ascending=False)[:1000]\nli=df['url'].tolist()","ed22a7d0":"# # Collect articles and their metadata (authors, titles and publication dates)\n\n# date=[]\n# auths=[]\n# titles=[]\n# text=[]\n\n# for no, l in enumerate(li):\n#     article = Article(l)\n#     try:\n#         article.download()\n#         article.parse()\n#         date.append(article.publish_date)\n#         auths.append(article.authors)\n#         titles.append(article.title)\n#         text.append(article.text)\n#     except:\n#         date.append(np.nan)\n#         auths.append(np.nan)\n#         titles.append(np.nan)\n#         text.append(np.nan)\n#     if no%100==0:\n#         print(no)\n#     sleep(.5)\n\n# res={\n#     'title': titles,\n#     'link':li,\n#     'date':date,\n#     'authors':auths,\n#     'text':text\n#     }\n# df=pd.DataFrame(res)\n\n# df.to_csv('.\/hn_newspaper.csv')","57aac116":"df=pd.read_csv('..\/input\/hn-newspaper\/hn_newspaper(1).csv')","2be7b9a4":"df.head()","0f713e12":"print(\"We have retrieved texts of: \"+ str((len(df[~df['text'].isna()])\/1000)*100)+'% of the stories \\n Not bad!')","493e74f4":"df","d9f9188c":"text=df['text'][0]","e48c9f82":"text","078ffee3":"text=word_tokenize(text)","ac8e0936":"text[:10]","3402206c":"text=[i.lower() for i in text]","4ae2ee31":"stopword_list=stopwords.words('english')","a160942e":"stopword_list[:10]","46b73e41":"text=[i for i in text if i not in stopword_list]","39505b19":"st = SnowballStemmer('english')","fc124e0e":"st.stem('exciting')","cd441b34":"text=[st.stem(i) for i in text]","bf3a29ce":"text[:10]","75727255":"df=df.dropna(subset=['text'])","c3150a9c":"df['text_token']=df['text'].apply(lambda x: word_tokenize(x))\ndf['text_token']=df['text_token'].apply(lambda row: [i.lower() for i in row])\ndf['text_token']=df['text_token'].apply(lambda row: [i for i in row if i not in stopword_list and len(i)>1] )\ndf['text_token_st']=df['text_token'].apply(lambda row: [st.stem(i) for i in row ] )","a02848fa":"df.head()","3c5bc164":"all_frequencies = Counter()\nfor i, row in df.iterrows():\n    counts=Counter(row['text_token_st'])\n    all_frequencies.update(counts)","4b76b2be":"all_frequencies.most_common()[:10]","d82bea7c":"temp=[]\nfor i, row in df.iterrows():\n    for j in row['text_token_st']:\n        \n        temp.append(j)\nCounter(temp).most_common()[:10]","0adb8b8a":"### We use pip for installing newspaper3k. For more details on installation see: https:\/\/newspaper.readthedocs.io\/en\/latest\/.","b5bee5e0":"# HackerNews analysis with newspaper3k\n\n- This notebook is a continuation of our Hacker News analysis.\n- In the [previous installment](https:\/\/www.kaggle.com\/michapaliski\/hackernews-analysis-with-bigquery) we have shown how you can use BigQuery and basic SQL for retrieving HN stories and comments.\n- In this tutorial we build on that and present how you can **collect articles** to which HN stories are referring.\n- We will use a great python library called [newspaper3k](https:\/\/github.com\/codelucas\/newspaper) for **scraping the articles and their metadata**\n\n### First part presents how you can:\n- connect to BQ from the Kaggle kernel\n- run basic SQL queries against the HN dataset\n\n**output: Top30 domains - outlets publishing stories on online privacy which were found worth sharing by HN users**\n\n### Second part focuses on:\n- introducing basic features of the newspaper3k\n- collecting the most popular HN stories related to online privacy issues\n\n**output: collection of popular articles on online privacy and their metadata**\n\n---","65f4ea18":"### We build a similar SQL query. But this time we collect all columns.","004d6109":"### Top30 domains: outlets publishing stories on online privacy which were found worth sharing by HN users  ","c784a876":"# Data cleaning","c9050032":"# Basic SQL queries against the HN dataset\n\nWe start our analysis with investigating the top domains that HN users use as sources. \n\nSteps:\n1. Extract domains from the stories' urls using regexp. \n2. Exclude stories without urls\n3. Include stories published after '2018-01-01' containing word 'privacy' or 'Privacy' in their titles or texts.\n4. `COUNT `top 30 domains and store the results in the column `c` ","b06070e1":"### Now when you are familiar with the basic capabilities of the newspaper3k library. Let's use it for a bulk request. In the following we will try to collect top 1k stories","f6790d7c":"# The most popular HN stories related to online privacy issues","540acfba":"# Connect to BQ from the Kaggle kernel\n","92089e6f":"# Basic features of the newspaper3k"}}