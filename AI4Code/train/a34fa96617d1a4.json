{"cell_type":{"f70f033c":"code","2c8b4203":"code","ce83e7d5":"code","d73f2a7e":"code","fca19476":"code","01e5cf7c":"code","6b8c748a":"code","da6a15bb":"code","3c6e5c90":"code","cb04154f":"code","10ca2f30":"code","7d9c95b7":"code","7196dd7b":"code","1d53026e":"code","ca0ecc7d":"code","6a0c5e50":"code","8d0f54c5":"markdown","350e0ca8":"markdown","d59fd167":"markdown","b70ca074":"markdown","e3fb2552":"markdown","3aeb22d2":"markdown"},"source":{"f70f033c":"import os\nimport time\nimport random\nimport glob\nimport numpy as np\n\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt","2c8b4203":"###====================== HYPER-PARAMETERS ===========================###\n## Adam\nbatch_size = 5  \nlr_v = 2e-4\ntot_sample= 100  # Totall traning images\n## adversarial learning (SRGAN)\nn_epoch = 500\n## initialize G\nn_epoch_init = n_epoch\/\/10\n\n# create folders to save result images and trained models\nsave_dir = \"samples\"\ncheckpoint_dir = \"models\"\n#track image as per index\nsave_ind= 16\n\nif not os.path.exists(save_dir):\n    os.makedirs(save_dir)\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)","ce83e7d5":"def load(path,shape):\n    img= cv2.imread(path)\n    img= cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img= cv2.resize(img, shape)\n    return img\n\n\ndef get_data(path):\n    X=[]\n    Y=[]\n    for folder in glob.glob(path+ str('\/*')):\n        for img_path in glob.glob(folder+ str('\/*')):      \n            if folder == os.path.join(path, 'HR'):\n                X.append(load(img_path, (384, 384)))\n            elif folder == os.path.join(path, 'LR'):\n                Y.append(load(img_path, (96,96)))\n\n    X= np.array(X)\n    Y= np.array(Y)\n    return X\/255.0, Y\/255.0\n    ","d73f2a7e":"HR_train, LR_train= get_data('..\/input\/super-image-resolution\/Data')\nHR_train.shape, LR_train.shape","fca19476":"plt.imshow(HR_train[65])","01e5cf7c":"f, ax= plt.subplots(1,2, figsize=(16, 6))\nax[0].imshow(LR_train[save_ind], aspect='auto')\nax[1].imshow(HR_train[save_ind], aspect='auto')\nplt.show()","6b8c748a":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, add,\\\n                                    BatchNormalization, Activation, LeakyReLU, Layer\n\nfrom tensorflow.keras.models import Model","da6a15bb":"class SubpixelConv2D(Layer):\n    \"\"\" Subpixel Conv2D Layer\n    upsampling a layer from (h, w, c) to (h*r, w*r, c\/(r*r)),\n    where r is the scaling factor, default to 4\n    # Arguments\n    upsampling_factor: the scaling factor\n    # Input shape\n        Arbitrary. Use the keyword argument `input_shape`\n        (tuple of integers, does not include the samples axis)\n        when using this layer as the first layer in a model.\n    # Output shape\n        the second and the third dimension increased by a factor of\n        `upsampling_factor`; the last layer decreased by a factor of\n        `upsampling_factor^2`.\n    # References\n        Real-Time Single Image and Video Super-Resolution Using an Efficient\n        Sub-Pixel Convolutional Neural Network Shi et Al. https:\/\/arxiv.org\/abs\/1609.05158\n    \"\"\"\n\n    def __init__(self, upsampling_factor=2, **kwargs):\n        super(SubpixelConv2D, self).__init__(**kwargs)\n        self.upsampling_factor = upsampling_factor\n\n    def build(self, input_shape):\n        last_dim = input_shape[-1]\n        factor = self.upsampling_factor * self.upsampling_factor\n        if last_dim % (factor) != 0:\n            raise ValueError('Channel ' + str(last_dim) + ' should be of '\n                             'integer times of upsampling_factor^2: ' +\n                             str(factor) + '.')\n\n    def call(self, inputs, **kwargs):\n        return tf.nn.depth_to_space( inputs, self.upsampling_factor )\n\n    def get_config(self):\n        config = { 'upsampling_factor': self.upsampling_factor, }\n        base_config = super(SubpixelConv2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n    def compute_output_shape(self, input_shape):\n        factor = self.upsampling_factor * self.upsampling_factor\n        input_shape_1 = None\n        if input_shape[1] is not None:\n            input_shape_1 = input_shape[1] * self.upsampling_factor\n        input_shape_2 = None\n        if input_shape[2] is not None:\n            input_shape_2 = input_shape[2] * self.upsampling_factor\n        dims = [ input_shape[0],\n                 input_shape_1,\n                 input_shape_2,\n                 int(input_shape[3]\/factor)\n               ]\n        return tuple( dims )","3c6e5c90":"#Generator\ndef get_G(input_shape):\n    # w_init = tf.random_normal_initializer(stddev=0.02)\n    g_init = tf.random_normal_initializer(1., 0.02)\n    relu= Activation('relu')\n\n    nin= Input(shape= input_shape)\n    n= Conv2D(64, (3,3), padding='SAME', activation= 'relu',\n                        kernel_initializer='HeNormal')(nin)\n    temp= n\n\n\n    # B residual blocks\n    for i in range(3):\n        nn= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n        nn= BatchNormalization(gamma_initializer= g_init)(nn)\n        nn= relu(nn)\n        nn= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n        nn= BatchNormalization(gamma_initializer= g_init)(nn)\n\n        nn= add([n, nn])\n        n= nn\n\n    n= Conv2D(64, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n    n= BatchNormalization(gamma_initializer= g_init)(n)\n    n= add([n, temp])\n    # B residual blacks end\n\n    n= Conv2D(256, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n    n= SubpixelConv2D(upsampling_factor=2)(n)\n    n= relu(n)\n\n    n= Conv2D(256, (3,3), padding='SAME', kernel_initializer='HeNormal')(n)\n    n= SubpixelConv2D(upsampling_factor=2)(n)\n    n= relu(n)\n\n    nn= Conv2D(3, (1,1), padding='SAME', kernel_initializer='HeNormal', activation= 'tanh')(n)\n\n\n    G = Model(inputs=nin, outputs=nn, name=\"generator\")\n    return G\n\n\n# discriminator\ndef get_D(input_shape):\n\n    g_init= tf.random_normal_initializer(1., 0.02)\n    ly_relu= LeakyReLU(alpha= 0.2)\n    df_dim = 16\n\n    nin = Input(input_shape)\n    n = Conv2D(64, (4, 4), (2, 2), padding='SAME', kernel_initializer='HeNormal')(nin)\n    n= ly_relu(n)\n\n    for i in range(2, 6):\n        n = Conv2D(df_dim*(2**i),(4, 4), (2, 2), padding='SAME', kernel_initializer='HeNormal')(n)\n        n= ly_relu(n)\n        n= BatchNormalization(gamma_initializer= g_init)(n)\n\n    n= Conv2D(df_dim*16, (1, 1), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n    n= ly_relu(n)\n    n= BatchNormalization(gamma_initializer= g_init)(n)\n\n    n= Conv2D(df_dim*8, (1, 1), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n    n= BatchNormalization(gamma_initializer= g_init)(n)\n    temp= n\n\n    n= Conv2D(df_dim*4, (3, 3), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n    n= ly_relu(n)\n    n= BatchNormalization(gamma_initializer= g_init)(n)\n\n    n= Conv2D(df_dim*8, (3, 3), (1, 1), padding='SAME', kernel_initializer='HeNormal')(n)\n    n= BatchNormalization(gamma_initializer= g_init)(n)\n\n    n= add([n, temp])\n\n    n= Flatten()(n)\n    no= Dense(units=1, kernel_initializer='HeNormal', activation= 'sigmoid')(n)\n    D= Model(inputs=nin, outputs=no, name=\"discriminator\")\n\n    return D\n\n\n# VGG19\ndef get_vgg19():\n    vgg= tf.keras.applications.VGG19( include_top=False, weights='imagenet', \n                                    input_tensor=None, input_shape=(384, 384, 3),\n                                    pooling=None, classes=1000, classifier_activation='softmax' )\n\n    inp= Input(shape=(384, 384, 3))\n    x= vgg.layers[0](inp)\n    for ly in vgg.layers[1:17]:\n        x= ly(x)\n    VGG19= Model(inp, x)\n\n    return VGG19","cb04154f":"G = get_G((96, 96, 3))\nD = get_D((384, 384, 3))\nvgg= get_vgg19()","10ca2f30":"# Optimizers\ng_optimizer_init = tf.optimizers.Adam(lr_v)\ng_optimizer = tf.optimizers.Adam(lr_v)\nd_optimizer = tf.optimizers.Adam(lr_v)","7d9c95b7":"n_step_epoch = round(n_epoch_init \/\/ batch_size)\nfor epoch in range(n_epoch_init):\n  i,j= ((epoch)*batch_size)%tot_sample, (((epoch+1))*batch_size)%tot_sample\n  if j== 0:\n    j= -1\n  X, Y= LR_train[i: j], HR_train[i: j]\n  with tf.GradientTape() as tape:\n      ypred= G(X)\n      mse_loss= tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(Y, ypred), axis=-1))\n      grad = tape.gradient(mse_loss, G.trainable_weights)\n      g_optimizer_init.apply_gradients(zip(grad, G.trainable_weights))\n        \n  print(\"Epoch: [{}\/{}] step: mse: {:.3f} \".format(\n            epoch, n_epoch_init , mse_loss))\n  if epoch%10 ==0:\n    img= G.predict(LR_train[np.newaxis, save_ind])[0]\n    #img= (img-img.mean())\/img.std()\n    img= Image.fromarray(np.uint8(img*255))\n    img.save(os.path.join(save_dir, 'init_g_{}.png'.format(epoch)))\n","7196dd7b":"f, ax= plt.subplots(1,5, figsize=(16, 6))\nfor i, file in enumerate(glob.glob('.\/samples\/init*')):\n    img= load(file, shape=(384, 384))\n    ax[i].imshow(img)\nplt.show()    ","1d53026e":"n_epoch= n_epoch-200\nfor epoch in range(n_epoch):\n        i,j= ((epoch)*batch_size)%tot_sample, (((epoch+1))*batch_size)%tot_sample\n        if j== 0:\n            j= -1\n        X, Y= LR_train[i: j], HR_train[i: j]\n        with tf.GradientTape(persistent=True) as tape:\n            fake_img= G(X)\n            fake_logits= D(fake_img)\n            real_logits= D(Y)\n            fake_feature= vgg(fake_img)\n            real_feature= vgg(Y)\n\n            #D. loss\n            d_loss1= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits , tf.zeros_like(fake_logits)))\n            d_loss2= tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(real_logits,tf.ones_like(real_logits)))\n            d_loss= d_loss1 + d_loss2\n\n            #G. loss\n            g_gan_loss= 2e-3*tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(fake_logits , tf.ones_like(fake_logits)))\n            mse_loss=  2e-1* tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(fake_img, Y), axis=-1))\n            vgg_loss = 2e-6 * tf.reduce_mean(tf.reduce_mean(tf.math.squared_difference(fake_feature, real_feature), axis=-1))\n            g_loss = mse_loss + vgg_loss + g_gan_loss\n\n            grad = tape.gradient(g_loss, G.trainable_weights)\n            g_optimizer.apply_gradients(zip(grad, G.trainable_weights))\n            grad = tape.gradient(d_loss, D.trainable_weights)\n            d_optimizer.apply_gradients(zip(grad, D.trainable_weights))\n\n        print(\"Epoch: [{}\/{}] step: D.loss: {:.3f}: G.loss: {:.3f}\".format(\n                epoch, n_epoch , d_loss, g_loss))\n\n\n        if epoch%20 ==0:\n            img= G.predict(LR_train[np.newaxis, save_ind])[0]\n            # if not sigmoid\n            #img= (img-img.mean())\/img.std()\n            img= Image.fromarray(np.uint8(img*255))\n            img.save(os.path.join(save_dir, 'train_g_{}.png'.format(epoch)))","ca0ecc7d":"f, ax= plt.subplots(3,5, figsize=(14, 16))\nfor i, file in enumerate(glob.glob('.\/samples\/train*')[:15]):\n    img= load(file, shape=(384, 384))\n    ax[i\/\/5][i%5].imshow(img, aspect='auto')\nplt.show()    ","6a0c5e50":"f, ax= plt.subplots(1,3, figsize=(16, 6))\nax[0].imshow(LR_train[save_ind], aspect='auto')\nax[1].imshow(load(glob.glob('.\/samples\/train*')[-1], (384, 384)), aspect='auto')\nax[2].imshow(HR_train[save_ind], aspect='auto')\nplt.show()","8d0f54c5":"### 2. Model creation","350e0ca8":"### Initialize learning (G)","d59fd167":"### End-to-End Keras implementation on top of Tensorflow 2.3","b70ca074":"### Results","e3fb2552":"### G.+ D. learning","3aeb22d2":"## GAN-Model\n\n### 1. Pre-Functions"}}