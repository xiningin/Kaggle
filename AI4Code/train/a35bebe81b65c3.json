{"cell_type":{"99477ae4":"code","3ac8ce1d":"code","7e9cab47":"code","6186f84e":"code","b96b6995":"code","6f10dd7c":"code","ba881e17":"code","033de90c":"code","b2381683":"code","3d9da1b0":"code","4033397f":"code","de1baa97":"code","41e931a0":"code","597a2cad":"code","84aa72aa":"code","b4e3744f":"code","17d5a562":"code","8292f1c0":"code","e8117925":"code","77205214":"code","90ee2a96":"code","12644926":"code","ad7d6e5c":"code","7e986e1d":"code","2ad5a73e":"code","66d88984":"code","55c7ad0f":"code","87ba5551":"code","9e2c38df":"code","99d46edf":"code","c2dd4168":"code","82e32e88":"code","ed821e34":"code","b18c60ed":"code","2cc0ce9a":"code","0eebab78":"code","5f4e3636":"code","0d9f304d":"code","74f29408":"code","ae0b9fd4":"code","0492f946":"code","0c86dcd2":"code","17d3208d":"code","6bcd44f3":"code","33e31ac7":"code","64aaa64d":"code","44d992bd":"code","f608a0e3":"code","0981f466":"code","8a9b628f":"code","e7418cee":"code","999928c0":"code","f81c7288":"code","e8721831":"code","1648b28b":"code","e7a96d91":"code","d1c5ad14":"code","80bc24cd":"code","8aae816e":"code","10f21df7":"code","d584bdc7":"code","0d7faf9c":"code","a24a7101":"code","36588ff8":"code","5d1ffab5":"code","d1361a80":"code","10cc1809":"code","aeafc0da":"code","91cd66bb":"code","f50c7386":"code","652aabb9":"markdown","205b427e":"markdown","e83f547d":"markdown","386cca31":"markdown","5be7539b":"markdown","947a02e4":"markdown","48856886":"markdown","9751dd12":"markdown","fa32d200":"markdown","6dbea121":"markdown","341ee606":"markdown","241d7012":"markdown","f49c946c":"markdown","31c0641f":"markdown","be9f62a8":"markdown","a30512f0":"markdown","a7b7f614":"markdown","b8c5186d":"markdown","b5e6ead0":"markdown","1a9e070f":"markdown","c96a1228":"markdown","e0fca831":"markdown","6d102f20":"markdown","18172de2":"markdown","7406255c":"markdown","8a49f095":"markdown","68ddb915":"markdown","6109946a":"markdown","79f81c67":"markdown","e94b1c6c":"markdown","7a9e725d":"markdown","9ee0da01":"markdown","afa7f88e":"markdown","2495b826":"markdown","6681da93":"markdown","c5788ca9":"markdown","92c53a6c":"markdown","3efe8ca1":"markdown","d754f823":"markdown","144828df":"markdown","17b77cca":"markdown","9fa6d5a6":"markdown","e31d15a9":"markdown","7d64928a":"markdown","679c0cea":"markdown","7f5e72ff":"markdown","036bdd0a":"markdown","7ff2202e":"markdown","c1650ce3":"markdown","8de06803":"markdown","e00aa3fb":"markdown","a5ea031e":"markdown","402cbd7e":"markdown","56330820":"markdown","9f4b2f5f":"markdown"},"source":{"99477ae4":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport sqlite3\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nimport numpy as np\nfrom sklearn import cross_validation\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.cross_validation import cross_val_score\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sb\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nimport gensim\nfrom gensim.models import Word2Vec, KeyedVectors","3ac8ce1d":"# Connection to the dataset\ncon = sqlite3.connect('..\/input\/database.sqlite')\n\n# It is given that the table name is 'Reviews'\n# Creating pandas dataframe and storing into variable 'dataset' by help of sql query\ndataset = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\n\"\"\", con)\n\n# Getting the shape of actual data: row, column\ndisplay(dataset.shape)","7e9cab47":"# Displaying first 5 data points\ndisplay(dataset.head())","6186f84e":"# Considering only those reviews which score is either 1,2 or 4,5\n# Since, 3 is kind of neutral review, so, we are eliminating it\nfiltered_data = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\nWHERE Score != 3\n\"\"\", con)","b96b6995":"# Getting shape of new dataset\ndisplay(filtered_data.shape)","6f10dd7c":"# Changing the scores into 'positive' or 'negative'\n# Score greater that 3 is considered as 'positive' and less than 3 is 'negative'\ndef partition(x):\n    if x>3:\n        return 'positive'\n    return 'negative'\n\nactual_score = filtered_data['Score']\npositiveNegative = actual_score.map(partition)\nfiltered_data['Score'] = positiveNegative","ba881e17":"# Sorting data points according to the 'ProductId'\nsorted_data = filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n\n# Eliminating the duplicate data points based on: 'UserId', 'ProfileName', 'Time', 'Summary'\nfinal = sorted_data.drop_duplicates(subset={'UserId', 'ProfileName', 'Time', 'Summary'}, keep='first', inplace=False)\n\n# Eliminating the row where 'HelpfulnessDenominator' is greater than 'HelpfulnessNumerator' as these are the wrong entry\nfinal = final[final['HelpfulnessDenominator'] >= final['HelpfulnessNumerator']]\n\n# Getting shape of final data frame\ndisplay(final.shape)","033de90c":"%%time\n\n# Creating the set of stopwords\nstop = set(stopwords.words('english'))\n\n# For stemming purpose\nsnow = nltk.stem.SnowballStemmer('english')\n\n# Defining function to clean html tags\ndef cleanhtml(sentence):\n    cleaner = re.compile('<.*>')\n    cleantext = re.sub(cleaner, ' ', sentence)\n    return cleantext\n\n# Defining function to remove special symbols\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|.|!|*|@|#|\\'|\"|,|)|(|\\|\/]', r'', sentence)\n    return cleaned\n\n\n# Important steps to clean the text data. Please trace it out carefully\ni = 0\nstr1 = ''\nall_positive_words = []\nall_negative_words = []\nfinal_string = []\ns=''\nfor sent in final['Text'].values:\n    filtered_sentence = []\n    sent = cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if ((cleaned_words.isalpha()) & (len(cleaned_words)>2)):\n                if (cleaned_words.lower() not in stop):\n                    s = (snow.stem(cleaned_words.lower())).encode('utf-8')\n                    filtered_sentence.append(s)\n                    if (final['Score'].values)[i] == 'positive':\n                        all_positive_words.append(s)\n                    if (final['Score'].values)[i] == 'negative':\n                        all_negative_words.append(s)\n                else:\n                    continue\n            else:\n                continue\n    str1 = b\" \".join(filtered_sentence)\n    final_string.append(str1)\n    i += 1\n    \n# Adding new column into dataframe to store cleaned text\nfinal['CleanedText'] = final_string\nfinal['CleanedText'] = final['CleanedText'].str.decode('utf-8')\n\n# Creating new dataset with cleaned text for future use\nconn = sqlite3.connect('final.sqlite')\nc = conn.cursor()\nconn.text_factory = str\nfinal.to_sql('Reviews', conn, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None)\n\n# Getting shape of new datset\nprint(final.shape)","b2381683":"# Creating connection to read from database\nconn = sqlite3.connect('.\/final.sqlite')\n\n# Creating data frame for visualization using sql query\nfinal = pd.read_sql_query(\"\"\"\nSELECT *\nFROM Reviews\n\"\"\", conn)","3d9da1b0":"# Displaying first 3 indices\ndisplay(final.head(3))","4033397f":"# Sampling positive and negative reviews\npositive_points = final[final['Score'] == 'positive'].sample(\n    n=10000, random_state=0)\nnegative_points = final[final['Score'] == 'negative'].sample(\n    n=10000, random_state=0)\ntotal_points = pd.concat([positive_points, negative_points])\n\n# Sorting based on time\ntotal_points['Time'] = pd.to_datetime(\n    total_points['Time'], origin='unix', unit='s')\ntotal_points = total_points.sort_values('Time')\nsample_points = total_points['CleanedText']\nlabels = total_points['Score']#.map(lambda x: 1 if x == 'positive' else 0).values","de1baa97":"# Splitting into train and test\nX_train, X_test, Y_train, Y_test = train_test_split(\n    sample_points, labels, test_size=0.30, random_state=0)","41e931a0":"count_vect = CountVectorizer(ngram_range=(1, 1))\nX_train = count_vect.fit_transform(X_train)\nX_test = count_vect.transform(X_test)","597a2cad":"%%time\n\nneighbors = list(range(20, 80, 4))\ncv_score = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n    cv_score.append(scores.mean())","84aa72aa":"MSE = [1 - x for x in cv_score]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\nplt.plot(neighbors, MSE)\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.show()","b4e3744f":"%%time\noptimal_model = KNeighborsClassifier(n_neighbors=optimal_k)\noptimal_model.fit(X_train, Y_train)\nprediction = optimal_model.predict(X_test)\n\ntraining_accuracy = optimal_model.score(X_train, Y_train)\ntraining_error = 1 - training_accuracy\ntest_accuracy = accuracy_score(Y_test, prediction)\ntest_error = 1 - test_accuracy\n\nprint(\"_\" * 101)\nprint(\"Training Accuracy: \", training_accuracy)\nprint(\"Train Error: \", training_error)\nprint(\"Test Accuracy: \", test_accuracy)\nprint(\"Test Error: \", test_error)\nprint(\"_\" * 101)","17d5a562":"print(\"_\" * 101)\nprint(\"Classification Report: \\n\")\nprint(classification_report(Y_test, prediction))\nprint(\"_\" * 101)","8292f1c0":"conf_matrix = confusion_matrix(Y_test, prediction)\nclass_label = ['negative', 'positive']\ndf_conf_matrix = pd.DataFrame(\n    conf_matrix, index=class_label, columns=class_label)\nsb.heatmap(df_conf_matrix, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nprint(\"_\" * 101)","e8117925":"%%time\n\nneighbors = list(range(20, 80, 4))\ncv_score = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k, algorithm='kd_tree')\n    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n    cv_score.append(scores.mean())","77205214":"MSE = [1 - x for x in cv_score]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\nplt.plot(neighbors, MSE)\n#for xy in zip(neighbors, np.round(MSE, 3)):\n#    plt.annotate(\"%s %s\" %xy, xy=xy, textcoords='data')\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.show()","90ee2a96":"%%time\n\noptimal_model = KNeighborsClassifier(\n    n_neighbors=optimal_k, algorithm='kd_tree')\noptimal_model.fit(X_train, Y_train)\nprediction = optimal_model.predict(X_test)\n\ntraining_accuracy = optimal_model.score(X_train, Y_train)\ntraining_error = 1 - training_accuracy\ntest_accuracy = accuracy_score(Y_test, prediction)\ntest_error = 1 - test_accuracy\n\nprint(\"_\" * 101)\nprint(\"Training Accuracy: \", training_accuracy)\nprint(\"Train Error: \", training_error)\nprint(\"Test Accuracy: \", test_accuracy)\nprint(\"Test Error: \", test_error)\nprint(\"_\" * 101)","12644926":"print(\"Classification Report: \\n\")\nprint(classification_report(Y_test, prediction))","ad7d6e5c":"conf_matrix = confusion_matrix(Y_test, prediction)\nclass_label = ['negative', 'positive']\ndf_conf_matrix = pd.DataFrame(\n    conf_matrix, index=class_label, columns=class_label)\nsb.heatmap(df_conf_matrix, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nprint(\"_\" * 101)","7e986e1d":"# Splitting into train and test\nX_train, X_test, Y_train, Y_test = train_test_split(\n    sample_points, labels, test_size=0.30)\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","2ad5a73e":"# Initializing tfidf vectorizer\ntfidf_vect = TfidfVectorizer(ngram_range=(1, 1))\n\n# Fitting for tfidf vectorization\nX_train = tfidf_vect.fit_transform(X_train)\nX_test = tfidf_vect.transform(X_test)\n\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","66d88984":"%%time\n\nneighbors = list(range(20, 80, 4))\ncv_score = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n    cv_score.append(scores.mean())","55c7ad0f":"MSE = [1 - x for x in cv_score]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\nplt.plot(neighbors, MSE)\n#for xy in zip(neighbors, np.round(MSE, 3)):\n#    plt.annotate(\"%s %s\" %xy, xy=xy, textcoords='data')\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.show()","87ba5551":"%%time\n\noptimal_model = KNeighborsClassifier(\n    n_neighbors=optimal_k, algorithm='kd_tree')\noptimal_model.fit(X_train, Y_train)\nprediction = optimal_model.predict(X_test)\n\ntraining_accuracy = optimal_model.score(X_train, Y_train)\ntraining_error = 1 - training_accuracy\ntest_accuracy = accuracy_score(Y_test, prediction)\ntest_error = 1 - test_accuracy\n\nprint(\"_\" * 101)\nprint(\"Training Accuracy: \", training_accuracy)\nprint(\"Train Error: \", training_error)\nprint(\"Test Accuracy: \", test_accuracy)\nprint(\"Test Error: \", test_error)\nprint(\"_\" * 101)","9e2c38df":"print(\"_\" * 101)\nprint(\"Classification Report: \\n\")\nprint(classification_report(Y_test, prediction))\nprint(\"_\" * 101)","99d46edf":"conf_matrix = confusion_matrix(Y_test, prediction)\nclass_label = ['negative', 'positive']\ndf_conf_matrix = pd.DataFrame(\n    conf_matrix, index=class_label, columns=class_label)\nsb.heatmap(df_conf_matrix, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","c2dd4168":"%%time\n\nneighbors = list(range(20, 80, 4))\ncv_score = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k, algorithm='kd_tree')\n    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n    cv_score.append(scores.mean())","82e32e88":"MSE = [1 - x for x in cv_score]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\nplt.plot(neighbors, MSE)\n#for xy in zip(neighbors, np.round(MSE, 3)):\n#    plt.annotate(\"%s %s\" %xy, xy=xy, textcoords='data')\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.show()","ed821e34":"%%time\n\noptimal_model = KNeighborsClassifier(\n    n_neighbors=optimal_k, algorithm='kd_tree')\noptimal_model.fit(X_train, Y_train)\nprediction = optimal_model.predict(X_test)\n\ntraining_accuracy = optimal_model.score(X_train, Y_train)\ntraining_error = 1 - training_accuracy\ntest_accuracy = accuracy_score(Y_test, prediction)\ntest_error = 1 - test_accuracy\n\nprint(\"_\" * 101)\nprint(\"Training Accuracy: \", training_accuracy)\nprint(\"Train Error: \", training_error)\nprint(\"Test Accuracy: \", test_accuracy)\nprint(\"Test Error: \", test_error)\nprint(\"_\" * 101)","b18c60ed":"print(\"_\" * 101)\nprint(\"Classification Report: \\n\")\nprint(classification_report(Y_test, prediction))\nprint(\"_\" * 101)","2cc0ce9a":"conf_matrix = confusion_matrix(Y_test, prediction)\nclass_label = ['negative', 'positive']\ndf_conf_matrix = pd.DataFrame(\n    conf_matrix, index=class_label, columns=class_label)\nsb.heatmap(df_conf_matrix, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nprint(\"_\" * 101)","0eebab78":"sample_points = total_points['Text']\n#labels = total_points['Score']\nX_train, X_test, Y_train, Y_test = train_test_split(\n    sample_points, labels, test_size=0.3, random_state=0)","5f4e3636":"import re\n\ndef cleanhtml(sentence):\n    cleantext = re.sub('<.*>', '', sentence)\n    return cleantext\n\ndef cleanpunc(sentence):\n    cleaned = re.sub(r'[?|!|\\'|#|@|.|,|)|(|\\|\/]', r'', sentence)\n    return cleaned","0d9f304d":"train_sent_list = []\nfor sent in X_train:\n    train_sentence = []\n    sent = cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if (cleaned_words.isalpha()):\n                train_sentence.append(cleaned_words.lower())\n            else:\n                continue\n    train_sent_list.append(train_sentence)","74f29408":"test_sent_list = []\nfor sent in X_test:\n    train_sentence = []\n    sent = cleanhtml(sent)\n    for w in sent.split():\n        for cleaned_words in cleanpunc(w).split():\n            if (cleaned_words.isalpha()):\n                train_sentence.append(cleaned_words.lower())\n            else:\n                continue\n    test_sent_list.append(train_sentence)","ae0b9fd4":"train_w2v_model = gensim.models.Word2Vec(\n    train_sent_list, min_count=5, size=50, workers=4)\ntrain_w2v_words = train_w2v_model[train_w2v_model.wv.vocab]","0492f946":"test_w2v_model = gensim.models.Word2Vec(\n    test_sent_list, min_count=5, size=50, workers=4)\ntest_w2v_words = test_w2v_model[test_w2v_model.wv.vocab]","0c86dcd2":"print(train_w2v_words.shape, test_w2v_words.shape)","17d3208d":"import numpy as np\ntrain_vectors = []\nfor sent in train_sent_list:\n    sent_vec = np.zeros(50)\n    cnt_words = 0\n    for word in sent:\n        try:\n            vec = train_w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    train_vectors.append(sent_vec)\ntrain_vectors = np.nan_to_num(train_vectors)","6bcd44f3":"test_vectors = []\nfor sent in test_sent_list:\n    sent_vec = np.zeros(50)\n    cnt_words = 0\n    for word in sent:\n        try:\n            vec = test_w2v_model.wv[word]\n            sent_vec += vec\n            cnt_words += 1\n        except:\n            pass\n    sent_vec \/= cnt_words\n    test_vectors.append(sent_vec)\ntest_vectors = np.nan_to_num(test_vectors)","33e31ac7":"X_train = train_vectors\nX_test = test_vectors","64aaa64d":"%%time\n\nneighbors = list(range(20, 50, 2))\ncv_score = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n    cv_score.append(scores.mean())","44d992bd":"MSE = [1 - x for x in cv_score]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\nplt.plot(neighbors, MSE)\n#for xy in zip(neighbors, np.round(MSE, 3)):\n#    plt.annotate(\"%s %s\" %xy, xy=xy, textcoords='data')\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.show()","f608a0e3":"%%time\n\noptimal_model = KNeighborsClassifier(\n    n_neighbors=optimal_k, algorithm='kd_tree')\noptimal_model.fit(X_train, Y_train)\nprediction = optimal_model.predict(X_test)\n\ntraining_accuracy = optimal_model.score(X_train, Y_train)\ntraining_error = 1 - training_accuracy\ntest_accuracy = accuracy_score(Y_test, prediction)\ntest_error = 1 - test_accuracy\n\nprint(\"_\" * 101)\nprint(\"Training Accuracy: \", training_accuracy)\nprint(\"Train Error: \", training_error)\nprint(\"Test Accuracy: \", test_accuracy)\nprint(\"Test Error: \", test_error)\nprint(\"_\" * 101)","0981f466":"print(\"_\" * 101)\nprint(\"Classification Report: \\n\")\nprint(classification_report(Y_test, prediction))\nprint(\"_\" * 101)","8a9b628f":"conf_matrix = confusion_matrix(Y_test, prediction)\nclass_label = ['negative', 'positive']\ndf_conf_matrix = pd.DataFrame(\n    conf_matrix, index=class_label, columns=class_label)\nsb.heatmap(df_conf_matrix, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nprint(\"_\" * 101)","e7418cee":"%%time\n\nneighbors = list(range(20, 50, 4))\ncv_score = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k, algorithm='kd_tree')\n    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n    cv_score.append(scores.mean())","999928c0":"MSE = [1 - x for x in cv_score]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\nplt.plot(neighbors, MSE)\n#for xy in zip(neighbors, np.round(MSE, 3)):\n#    plt.annotate(\"%s %s\" %xy, xy=xy, textcoords='data')\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.show()","f81c7288":"%%time\n\noptimal_model = KNeighborsClassifier(\n    n_neighbors=optimal_k, algorithm='kd_tree')\noptimal_model.fit(X_train, Y_train)\nprediction = optimal_model.predict(X_test)\n\ntraining_accuracy = optimal_model.score(X_train, Y_train)\ntraining_error = 1 - training_accuracy\ntest_accuracy = accuracy_score(Y_test, prediction)\ntest_error = 1 - test_accuracy\n\nprint(\"_\" * 101)\nprint(\"Training Accuracy: \", training_accuracy)\nprint(\"Train Error: \", training_error)\nprint(\"Test Accuracy: \", test_accuracy)\nprint(\"Test Error: \", test_error)\nprint(\"_\" * 101)","e8721831":"print(\"_\" * 101)\nprint(\"Classification Report: \\n\")\nprint(classification_report(Y_test, prediction))\nprint(\"_\" * 101)","1648b28b":"conf_matrix = confusion_matrix(Y_test, prediction)\nclass_label = ['negative', 'positive']\ndf_conf_matrix = pd.DataFrame(\n    conf_matrix, index=class_label, columns=class_label)\nsb.heatmap(df_conf_matrix, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nprint(\"_\" * 101)","e7a96d91":"X_train, X_test, Y_train, Y_test = train_test_split(\n    sample_points, labels, test_size=0.3)\nprint(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)","d1c5ad14":"tfidf_vect = TfidfVectorizer(ngram_range=(1, 1))\ntrain_tfidf_w2v = tfidf_vect.fit_transform(X_train)\ntest_tfidf_w2v = tfidf_vect.transform(X_test)\nprint(train_tfidf_w2v.shape, test_tfidf_w2v.shape)","80bc24cd":"%%time\n\ntfidf_feat = tfidf_vect.get_feature_names()\ntrain_tfidf_w2v_vectors = []\nrow = 0\nfor sent in train_sent_list:\n    sent_vec = np.zeros(50)\n    weight_sum = 0\n    for word in sent:\n        if word in train_w2v_words:\n            vec = train_w2v_model.wv[word]\n            tf_idf = train_tfidf_w2v[row, tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    train_tfidf_w2v_vectors.append(sent_vec)\n    row += 1","8aae816e":"%%time\n\ntfidf_feat = tfidf_vect.get_feature_names()\ntest_tfidf_w2v_vectors = []\nrow = 0\nfor sent in test_sent_list:\n    sent_vec = np.zeros(50)\n    weighted_sum = 0\n    for word in sent:\n        if word in test_w2v_words:\n            vec = test_w2v_model[word]\n            tf_idf = test_tfidf_w2v[row, tfidf_feat.index(word)]\n            sent_vec += (vec * tf_idf)\n            weight_sum += tf_idf\n    if weight_sum != 0:\n        sent_vec \/= weight_sum\n    test_tfidf_w2v_vectors.append(sent_vec)\n    row += 1","10f21df7":"X_train = train_tfidf_w2v_vectors\nX_test = test_tfidf_w2v_vectors","d584bdc7":"%%time\n\nneighbors = list(range(1, 50, 2))\ncv_score = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')\n    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n    cv_score.append(scores.mean())","0d7faf9c":"MSE = [1 - x for x in cv_score]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\nplt.plot(neighbors, MSE)\n#for xy in zip(neighbors, np.round(MSE, 3)):\n#    plt.annotate(\"%s %s\" %xy, xy=xy, textcoords='data')\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.show()","a24a7101":"%%time\n\noptimal_model = KNeighborsClassifier(\n    n_neighbors=optimal_k, algorithm='kd_tree')\noptimal_model.fit(X_train, Y_train)\nprediction = optimal_model.predict(X_test)\n\ntraining_accuracy = optimal_model.score(X_train, Y_train)\ntraining_error = 1 - training_accuracy\ntest_accuracy = accuracy_score(Y_test, prediction)\ntest_error = 1 - test_accuracy\n\nprint(\"_\" * 101)\nprint(\"Training Accuracy: \", training_accuracy)\nprint(\"Train Error: \", training_error)\nprint(\"Test Accuracy: \", test_accuracy)\nprint(\"Test Error: \", test_error)\nprint(\"_\" * 101)","36588ff8":"print(\"_\" * 101)\nprint(\"Classification Report: \\n\")\nprint(classification_report(Y_test, prediction))\nprint(\"_\" * 101)","5d1ffab5":"conf_matrix = confusion_matrix(Y_test, prediction)\nclass_label = ['negative', 'positive']\ndf_conf_matrix = pd.DataFrame(\n    conf_matrix, index=class_label, columns=class_label)\nsb.heatmap(df_conf_matrix, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nprint(\"_\" * 101)","d1361a80":"%%time\n\nneighbors = list(range(1, 50, 2))\ncv_score = []\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k, algorithm='kd_tree')\n    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n    cv_score.append(scores.mean())","10cc1809":"MSE = [1 - x for x in cv_score]\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"_\" * 101)\nprint(\"Optimal number of neighbors: \", optimal_k)\nprint(\"_\" * 101)\nprint(\"Missclassification error for each k values: \", np.round(MSE, 3))\nprint(\"_\" * 101)\n\nplt.plot(neighbors, MSE)\n#for xy in zip(neighbors, np.round(MSE, 3)):\n#    plt.annotate(\"%s %s\" %xy, xy=xy, textcoords='data')\nplt.title(\"Number of neighbors and error\")\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Missclassification error\")\nplt.show()","aeafc0da":"%%time\n\noptimal_model = KNeighborsClassifier(\n    n_neighbors=optimal_k, algorithm='kd_tree')\noptimal_model.fit(X_train, Y_train)\nprediction = optimal_model.predict(X_test)\n\ntraining_accuracy = optimal_model.score(X_train, Y_train)\ntraining_error = 1 - training_accuracy\ntest_accuracy = accuracy_score(Y_test, prediction)\ntest_error = 1 - test_accuracy\n\nprint(\"_\" * 101)\nprint(\"Training Accuracy: \", training_accuracy)\nprint(\"Train Error: \", training_error)\nprint(\"Test Accuracy: \", test_accuracy)\nprint(\"Test Error: \", test_error)\nprint(\"_\" * 101)","91cd66bb":"print(\"_\" * 101)\nprint(\"Classification Report: \\n\")\nprint(classification_report(Y_test, prediction))\nprint(\"_\" * 101)","f50c7386":"conf_matrix = confusion_matrix(Y_test, prediction)\nclass_label = ['negative', 'positive']\ndf_conf_matrix = pd.DataFrame(\n    conf_matrix, index=class_label, columns=class_label)\nsb.heatmap(df_conf_matrix, annot=True, fmt='d')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\nprint(\"_\" * 101)","652aabb9":"### Classification Report","205b427e":"### Classificatoin Report","e83f547d":"- After executing models and hyper parameter tuning, it is found out that, KNN is giving train and test accuracy around 70% in for both W2V and TF-IDF.\n- As K increased, training accuracy always increases but test accuracy is limited around 70% so it's obvious to adjust the K properly so that model doesn't over fit.\n- Model is always over fitting for Average W2V dataset independent of k value so is not suitable in this case.\n- Model is always lenient towards one class for TFIDF-W2V so clearly it's not suitable in this case too.\n- It is very time consuming to have data with very high dimension. Very small subset were taken but still we are getting high delay due to large dimension and time complexity of KNN.","386cca31":"## KD tree KNN","5be7539b":"### Misclassification Error","947a02e4":"# TFIDF-Word2Vec","48856886":"### Classification Report","9751dd12":"# Final Report","fa32d200":"- After trying out lot of hyper parameter tuning, model is always bias towards only one class. It clearly indicates that, the model  Brute Ford KNN is not suitable for TFIDF-Word2Vec in our Amazon Fine Food Reviews data set.","6dbea121":"- After trying out lot of hyper parameter tuning, model is always bias towards only one class. It clearly indicates that, the model KD Tree KNN is not suitable for TFIDF-Word2Vec in our Amazon Fine Food Reviews data set.","341ee606":"### Accuracy Score","241d7012":"### Misclassification Error","f49c946c":"### Misclassification Error","31c0641f":"### Misclassification Error","be9f62a8":"# KNN\n\n## Introduction\n\n## Objective:\n- Get Amazon Fine Foor Review dataset and prepare 4 categories of datasets: \n      (i) BoW, \n      (ii) TF-IDF, \n      (iii) Word2Vec, \n      (iv) TFIDF-W2V\n      \n- Apply following on all of the above:\n        1. Split the data into train data(70%) and test data(30%) using timebased slicing.\n        2. Perform 10 fold cross validation to find optimal K in KNN.\n        3. Apply KNN, both: bruteforce and kd-tree.\n        4. Report: Accuracy score with best K, f1 measure, Confusion Matrix","a30512f0":"### Confusion Matrix","a7b7f614":"## Brute force KNN","b8c5186d":"# BoW","b5e6ead0":"### Accuracy Score","1a9e070f":"### Accuracy Score","c96a1228":"### Missclassification Error","e0fca831":"### Misclassification Error","6d102f20":"### Accuracy Score","18172de2":"### Accuracy Score","7406255c":"### Confusion Matrix","8a49f095":"### Accuracy Score","68ddb915":"- Accuracy Report in %:\n<table>\n   <tbody>\n    <tr>      \n      <th><center> <\/center><\/th>\n      <th colspan='4'><center> Brute Force KNN <\/center><\/th>\n      <th colspan='4'><center> KD Tree KNN <\/center><\/th>      \n    <\/tr>\n  \n  <tr>\n    <td> <\/td>\n    <td rowspan=\"1\"><b><center> Train Accuracy <\/center><\/b><\/td>    \n    <td rowspan=\"1\"><b><center> Train Error <\/center><\/b><\/td>\n    <td rowspan=\"1\"><b><center> Test Accuracy <\/center><\/b><\/td>\n    <td rowspan=\"1\"><b><center> Test Error <\/center><\/b><\/td>\n    <td rowspan=\"1\"><b><center> Train Accuracy <\/center><\/b><\/td>    \n    <td rowspan=\"1\"><b><center>Train  Error <\/center><\/b><\/td>\n    <td rowspan=\"1\"><b><center> Test Accuracy <\/center><\/b><\/td>\n    <td rowspan=\"1\"><b><center> Test Error <\/center><\/b><\/td>\n  <\/tr>\n  <tr>\n    <td><b><center> Bow <\/center><\/b><\/td>\n    <td><center> 72.65 <\/center><\/td>\n    <td><center> 27.34<\/center><\/td>\n    <td><center> 70.65 <\/center><\/td>\n    <td><center> 29.35 <\/center><\/td>\n<td><center> 72.65 <\/center><\/td>\n    <td><center> 27.34<\/center><\/td>\n    <td><center> 70.65 <\/center><\/td>\n    <td><center> 29.35 <\/center><\/td>\n  <\/tr>\n<tr>\n    <td><b><center> TF-IDF <\/center><\/b><\/td>\n    <td><center> 73.37 <\/center><\/td>\n    <td><center> 26.62 <\/center><\/td>\n    <td><center> 71.80 <\/center><\/td>\n    <td><center> 28.20 <\/center><\/td>\n   <td><center> 73.37 <\/center><\/td>\n    <td><center> 26.62 <\/center><\/td>\n    <td><center> 71.80 <\/center><\/td>\n    <td><center> 28.20 <\/center><\/td>\n  <\/tr>\n<tr>\n    <td><b><center> Avg-W2V <\/center><\/b><\/td>\n    <td><center> 75.00 <\/center><\/td>\n    <td><center> 25.00 <\/center><\/td>\n    <td><center> 57.91 <\/center><\/td>\n    <td><center> 42.08 <\/center><\/td>\n     <td><center> 75.00 <\/center><\/td>\n    <td><center> 25.00 <\/center><\/td>\n    <td><center> 57.91 <\/center><\/td>\n    <td><center> 42.08 <\/center><\/td>\n  <\/tr>\n<tr>\n    <td><b><center> TFIDF-W2V <\/center><\/b><\/td>\n    <td><center> 50.20 <\/center><\/td>\n    <td><center> 49.79 <\/center><\/td>\n    <td><center> 49.51 <\/center><\/td>\n    <td><center> 50.48 <\/center><\/td>\n    <td><center> 50.20 <\/center><\/td>\n    <td><center> 49.79 <\/center><\/td>\n    <td><center> 49.51 <\/center><\/td>\n    <td><center> 50.48 <\/center><\/td>\n  <\/tr>\n  <\/tbody>\n<\/table>\n","6109946a":"### Classification Report","79f81c67":"### Missclassification Error","e94b1c6c":"###  Confusion Matrix","7a9e725d":"### Confusion Matrix","9ee0da01":"### Confusion Matrix","afa7f88e":"# TF-IDF","2495b826":"- After trying out lot of hyper parameter tuning, model is always overfitting. It clearly indicates that, the model  KD Tree KNN is bias towards Average Word2Vec in our Amazon Fine Food Reviews data set.","6681da93":"# Conclusion","c5788ca9":"## Brute force KNN","92c53a6c":"## KD tree KNN","3efe8ca1":"### Confusion Matrix","d754f823":"## KD tree KNN","144828df":"## Brute force KNN","17b77cca":"### Classification Report","9fa6d5a6":"# Avg Word2Vec","e31d15a9":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#KNN\" data-toc-modified-id=\"KNN-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><\/li><li><span><a href=\"#Objective:\" data-toc-modified-id=\"Objective:-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Objective:<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#BoW\" data-toc-modified-id=\"BoW-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>BoW<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Brute-force-KNN\" data-toc-modified-id=\"Brute-force-KNN-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Brute force KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Missclassification-Error\" data-toc-modified-id=\"Missclassification-Error-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;<\/span>Missclassification Error<\/a><\/span><\/li><li><span><a href=\"#Accuracy-Score\" data-toc-modified-id=\"Accuracy-Score-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;<\/span>Accuracy Score<\/a><\/span><\/li><li><span><a href=\"#Classificatoin-Report\" data-toc-modified-id=\"Classificatoin-Report-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;<\/span>Classificatoin Report<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#KD-tree-KNN\" data-toc-modified-id=\"KD-tree-KNN-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>KD tree KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Misclassification-Error\" data-toc-modified-id=\"Misclassification-Error-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;<\/span>Misclassification Error<\/a><\/span><\/li><li><span><a href=\"#Accuracy-Score\" data-toc-modified-id=\"Accuracy-Score-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;<\/span>Accuracy Score<\/a><\/span><\/li><li><span><a href=\"#Classification-Report\" data-toc-modified-id=\"Classification-Report-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;<\/span>Classification Report<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#TF-IDF\" data-toc-modified-id=\"TF-IDF-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>TF-IDF<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Brute-force-KNN\" data-toc-modified-id=\"Brute-force-KNN-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Brute force KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Missclassification-Error\" data-toc-modified-id=\"Missclassification-Error-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;<\/span>Missclassification Error<\/a><\/span><\/li><li><span><a href=\"#Accuracy-Score\" data-toc-modified-id=\"Accuracy-Score-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;<\/span>Accuracy Score<\/a><\/span><\/li><li><span><a href=\"#Classificatoin-Report\" data-toc-modified-id=\"Classificatoin-Report-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;<\/span>Classificatoin Report<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#KD-tree-KNN\" data-toc-modified-id=\"KD-tree-KNN-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>KD tree KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Misclassification-Error\" data-toc-modified-id=\"Misclassification-Error-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;<\/span>Misclassification Error<\/a><\/span><\/li><li><span><a href=\"#Accuracy-Score\" data-toc-modified-id=\"Accuracy-Score-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;<\/span>Accuracy Score<\/a><\/span><\/li><li><span><a href=\"#Classificatoin-Report\" data-toc-modified-id=\"Classificatoin-Report-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;<\/span>Classificatoin Report<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Word2Vec\" data-toc-modified-id=\"Word2Vec-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Word2Vec<\/a><\/span><\/li><li><span><a href=\"#Avg-Word2Vec\" data-toc-modified-id=\"Avg-Word2Vec-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Avg Word2Vec<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Brute-force-KNN\" data-toc-modified-id=\"Brute-force-KNN-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Brute force KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Misclassification-Error\" data-toc-modified-id=\"Misclassification-Error-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;<\/span>Misclassification Error<\/a><\/span><\/li><li><span><a href=\"#Accuracy-Score\" data-toc-modified-id=\"Accuracy-Score-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;<\/span>Accuracy Score<\/a><\/span><\/li><li><span><a href=\"#Classification-Report\" data-toc-modified-id=\"Classification-Report-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;<\/span>Classification Report<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#KD-tree-KNN\" data-toc-modified-id=\"KD-tree-KNN-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>KD tree KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Misclassification-Error\" data-toc-modified-id=\"Misclassification-Error-5.2.1\"><span class=\"toc-item-num\">5.2.1&nbsp;&nbsp;<\/span>Misclassification Error<\/a><\/span><\/li><li><span><a href=\"#Accuracy-Score\" data-toc-modified-id=\"Accuracy-Score-5.2.2\"><span class=\"toc-item-num\">5.2.2&nbsp;&nbsp;<\/span>Accuracy Score<\/a><\/span><\/li><li><span><a href=\"#Classification-Report\" data-toc-modified-id=\"Classification-Report-5.2.3\"><span class=\"toc-item-num\">5.2.3&nbsp;&nbsp;<\/span>Classification Report<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-5.2.4\"><span class=\"toc-item-num\">5.2.4&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#TFIDF-Word2Vec\" data-toc-modified-id=\"TFIDF-Word2Vec-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>TFIDF-Word2Vec<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Brute-force-KNN\" data-toc-modified-id=\"Brute-force-KNN-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Brute force KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Misclassification-Error\" data-toc-modified-id=\"Misclassification-Error-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;<\/span>Misclassification Error<\/a><\/span><\/li><li><span><a href=\"#Accuracy-Score\" data-toc-modified-id=\"Accuracy-Score-6.1.2\"><span class=\"toc-item-num\">6.1.2&nbsp;&nbsp;<\/span>Accuracy Score<\/a><\/span><\/li><li><span><a href=\"#Classification-Report\" data-toc-modified-id=\"Classification-Report-6.1.3\"><span class=\"toc-item-num\">6.1.3&nbsp;&nbsp;<\/span>Classification Report<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-6.1.4\"><span class=\"toc-item-num\">6.1.4&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#KD-tree-KNN\" data-toc-modified-id=\"KD-tree-KNN-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>KD tree KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Misclassification-Error\" data-toc-modified-id=\"Misclassification-Error-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;<\/span>Misclassification Error<\/a><\/span><\/li><li><span><a href=\"#Accuracy-Score\" data-toc-modified-id=\"Accuracy-Score-6.2.2\"><span class=\"toc-item-num\">6.2.2&nbsp;&nbsp;<\/span>Accuracy Score<\/a><\/span><\/li><li><span><a href=\"#Classification-Report\" data-toc-modified-id=\"Classification-Report-6.2.3\"><span class=\"toc-item-num\">6.2.3&nbsp;&nbsp;<\/span>Classification Report<\/a><\/span><\/li><li><span><a href=\"#Confusion-Matrix\" data-toc-modified-id=\"Confusion-Matrix-6.2.4\"><span class=\"toc-item-num\">6.2.4&nbsp;&nbsp;<\/span>Confusion Matrix<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Final-Report\" data-toc-modified-id=\"Final-Report-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Final Report<\/a><\/span><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/div>","7d64928a":"## Brute force KNN","679c0cea":"## KD tree KNN","7f5e72ff":"### Accuracy Score","036bdd0a":"### Confusion Matrix","7ff2202e":"### Classificatoin Report","c1650ce3":"### Classificatoin Report","8de06803":"### Classification Report","e00aa3fb":"# Word2Vec","a5ea031e":"- After trying out lot of hyper parameter tuning, model is always overfitting. It clearly indicates that, the model Brute Force KNN is bias towards Average Word2Vec in our Amazon Fine Food Reviews data set.","402cbd7e":"### Misclassification Error","56330820":"### Confusion Matrix","9f4b2f5f":"### Accuracy Score"}}