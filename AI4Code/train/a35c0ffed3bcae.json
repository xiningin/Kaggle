{"cell_type":{"581a5a4f":"code","78cdfda9":"code","ea74cda6":"code","1a5730ad":"code","0f58d99c":"code","c4f87d4c":"code","d975a4be":"code","2d6123f2":"code","1c5527ce":"code","d713d4d0":"code","40b0589a":"code","099444e2":"code","58bbd2ba":"code","77128bcb":"code","b24ecce2":"code","5ee42c73":"code","b22012ab":"code","e44a5944":"code","a435fb27":"code","bc402820":"code","fb2aefd4":"code","718d024d":"code","c2018bef":"code","167aa575":"code","48c2b695":"code","f5cdfe77":"code","1de66f3c":"code","053d6473":"code","a9672300":"markdown","169913bb":"markdown","a7ad6f72":"markdown","743269e1":"markdown","173c2f00":"markdown","8f1cb798":"markdown","f56d8691":"markdown","d3257273":"markdown","427ab2d4":"markdown","eca60a43":"markdown","a81cb423":"markdown","91e531a6":"markdown","6f9bea73":"markdown","990dd1ef":"markdown","7289fb5c":"markdown","12003e49":"markdown"},"source":{"581a5a4f":"%%capture\n\n## Import Libraries\nimport numpy as np # linear algebra\nfrom numpy.random import seed \nimport math \n\nfrom scipy.stats import normaltest\n\nimport pandas as pd # data processing \npd.options.display.max_rows = 100\npd.options.display.max_seq_items = 2000\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\n\nimport datetime as dt\n\n!pip -q install mplfinance\nimport mplfinance as mpf\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport matplotlib.dates as mpl_dates\nplt.rcParams.update({'font.size': 14})\nimport seaborn as sns\nplt.style.use('seaborn')\nsns.set_style('whitegrid')\n\n!pip install talib-binary # install talib for feature engineering \nimport talib\nfrom talib import RSI, BBANDS, MACD, ATR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.pipeline import Pipeline\n\nimport warnings # Supress warnings \nwarnings.filterwarnings('ignore')\n\nimport statsmodels as sm\n\nimport joblib\n\n# Fix seed for reproducible results\nSEED = 42\nnp.random.seed(SEED)","78cdfda9":"## Import data\ndata_folder = \"..\/input\/g-research-crypto-forecasting\/\"\n!ls $data_folder","ea74cda6":"example_sample_submission = pd.read_csv(data_folder + \"example_sample_submission.csv\")\nprint(example_sample_submission.shape)\nexample_sample_submission.head()","1a5730ad":"%%time\n\ntest_df = pd.read_csv(data_folder + \"example_test.csv\")\ntest_df['timestamp'] = pd.to_datetime(test_df['timestamp'], unit='s') \nprint(test_df.shape)\ntest_df.head()","0f58d99c":"%%time \n\ndef reduce_mem_usage(df): ## Copied directly from other Kagglers! ##\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#         else:\n#             df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","c4f87d4c":"%%time \nassets_df = pd.read_csv(data_folder + \"asset_details.csv\")\nassets_df = assets_df.sort_values(by='Asset_ID')\n\ntickers = ['BNB', 'BTC', 'BCH', 'ADA', 'DOGE', 'EOS', 'ETH', 'ETC', 'IOTA', 'LTC', 'MKR', 'XMR', 'XLM', 'TRX']\n\n# mappings\nmapping_name = dict(assets_df[['Asset_ID', 'Asset_Name']].values)\nmapping_weight = dict(assets_df[['Asset_ID', 'Weight']].values)\nmapping_tickers = dict(enumerate(tickers))\n\n# data types\ndtypes = {\n    'timestamp': str,\n    'Asset_ID': np.int8,\n    'Count': np.int32,\n    'Open': np.float64,\n    'High': np.float64,\n    'Low': np.float64,\n    'Close': np.float64,\n    'Volume': np.float64,\n    'VWAP': np.float64,\n    'Target': np.float64,\n}\n\n# formatting\ntext_formats = {\n   'PURPLE': '\\033[95m',\n   'CYAN': '\\033[96m',\n   'DARKCYAN': '\\033[36m',\n   'BLUE': '\\033[94m',\n   'GREEN': '\\033[92m',\n   'YELLOW': '\\033[93m',\n   'RED': '\\033[91m',\n   'BOLD': '\\033[1m',\n   'UNDERLINE': '\\033[4m',\n   'END': '\\033[0m'\n}\n\nRED = text_formats['RED']\nBOLD = text_formats['BOLD']\nEND = text_formats['END']\n\ndf_train_raw = pd.read_csv(data_folder + \"train.csv\", dtype=dtypes)\n# df_train_raw = reduce_mem_usage(df_train_raw) # Memory reduced by 34.8%\ndf_train_raw.head()","d975a4be":"%%time\n\ndef prepare_train_data(df):\n    \"\"\"This function takes in the raw training data and wrangles the data in the shape and types \n    required for further research\"\"\"\n    \n    CUTOFF_DATE = '2021-06-13 00:00:00'\n    \n    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')       # convert from unix to datetime timestamp\n    df = df.set_index('timestamp')                                    # set date as index column\n    \n    dates = df.index.get_level_values('timestamp')\n    df['date'] = pd.to_datetime(dates.date)\n    df['date'] = df['date'].dt.strftime('%Y%m%d').astype(int)         # SLOWS IT DOWN BY 3 MINS! NEED STH BETTER\n    df['year'] = dates.year\n    df['month'] = dates.month\n    \n    df[\"asset_name\"] = df[\"Asset_ID\"].map(mapping_name)\n    df[\"asset_weight\"] = df[\"Asset_ID\"].map(mapping_weight)\n    df[\"ticker\"] = df[\"Asset_ID\"].map(mapping_tickers)\n    \n    df = df.rename({'Count': 'interval_trade_count'}, axis=1)\n    df.columns = map(str.lower, df.columns)\n    \n    df.sort_index()\n    df = df[(df.index < CUTOFF_DATE)]\n    \n    total_num_records = df.shape[0]\n    print(f\"Total number of records in train dataset: {total_num_records}\")\n    \n    return df\n\ndf_train = prepare_train_data(df_train_raw)\nprint(df_train.dtypes)\ndf_train.head()","2d6123f2":"df_vwap_inf = np.where(np.isinf(df_train['vwap']))\nprint(df_vwap_inf)\nprint(len(df_vwap_inf))\n\n","1c5527ce":"def find_missing_rows(df, start, end, asset):\n    \"\"\"This function calculates how many missing rows each crypto asset has.\"\"\"\n    \n    num_rows = len(df.index)\n    num_missing_rows = len(pd.date_range(start=start, end=end, freq='min').difference(df.index))\n    print(f\"{asset} total number of records: \" + BOLD + f\"{num_rows}\" + END +  \\\n          \", Total number of missing minute records: \" + BOLD + f\"{num_missing_rows}\" + END)\n    \ndef populate_missing_rows(df):\n    \"\"\"This function populates missing minute data rows with Null values.\"\"\"\n    \n    asset, start_date, end_date = df.iloc[0]['asset_name'], df.index[0], df.index[-1]\n    print(f\"{asset} start date: {start_date}, end date: {end_date}\")\n    find_missing_rows(df, start_date, end_date, asset)\n    \n    df = df.asfreq(freq='T', method='ffill')  # forward fill missing time series data\n    print(f\"Populating missing {asset} data...\")\n    find_missing_rows(df, start_date, end_date, asset)\n    print('\\n')\n    \n    return df\n    \ncrypto_dfs = list()\n    \nfor i in list(range(14)):\n    symbol_df = df_train[df_train['asset_id'] == i]\n    crypto_dfs.append(populate_missing_rows(symbol_df))\n\n\ndf_train = pd.concat(crypto_dfs).sort_index()  \n# print(df_train.dtypes)\n# print(len(df_train.index))\n# df_train.head()","d713d4d0":"%%time\n# Generate dictionary of Dataframes for all tickers and store in dict..\ndef generate_crypto_dfs():\n    \"\"\"Returns a dictionary of dataframes with key(Ticker):value(Dataframe that belongs to that ticker)\"\"\"\n    \n    return {ticker:df_train.query(\"ticker == @ticker\") for ticker in tickers}\n\nall_cryptos = generate_crypto_dfs()","40b0589a":"## Filter on a few data frames to work with (BTC, ETH, DOGE)\nbtc = all_cryptos['BTC']\neth = all_cryptos['ETH']\ndoge = all_cryptos['DOGE']\nbnb = all_cryptos['BNB']\n\nprint(f\"DOGE unique years: {doge.year.unique()}\")\nprint(f\"BTC unique years: {btc.year.unique()}\")\nprint(f\"Number of BTC records: {len(btc.index)}\")\n\n# doge_filtered = doge.query('year >= 2020')\n# btc_filtered = btc.query('year >= 2020')","099444e2":"# print(len(btc_filtered.index))\n# print(len(doge_filtered.index))\n\n\n# doge.index[-1]","58bbd2ba":"# Function that gives opportunity to resample a time series data frame\n\ndef resample_timeseries(df, offset='H'):\n    \"\"\"Resampes a timeseries dataframe given the offset forwarded in the function\"\"\"\n    \n    df = df.reset_index().groupby(['asset_id', 'ticker']).resample(offset, on='timestamp', origin='start').agg(\n        {\"open\": \"first\", \n         \"close\": \"last\", \n         \"low\": \"min\", \n         \"high\": \"max\",\n         \"volume\": \"sum\",\n         \"vwap\": \"max\"\n        }\n    ).dropna()[['open', 'high', 'low', 'close', 'volume','vwap']]\n\n    return df.reset_index().set_index('timestamp')","77128bcb":"def plot_rolling_correlation(df, ticker0, ticker1, window, offset, prices):\n    \"\"\"Computes and plots the rolling correlations\"\"\"\n    \n    rolling_corr = df[ticker0].rolling(window).corr(df[ticker1]).to_frame('correlation')\n    \n    plt.figure(figsize=(20,8))\n    sns.lineplot(x=\"timestamp\", y=\"correlation\", data=rolling_corr).set(title=f\"{window} {offset} Rolling Correlation between {ticker0} and {ticker1} using {prices}\")\n    \n    \ndef merge_series(series0, series1, ticker0, ticker1):\n    return pd.DataFrame({ticker0: series0, ticker1: series1})\n\ndef test_correlation(df0, df1, window0, window1, offset, use_returns=True):\n    \"\"\" Computes the correlation between 2 series for the whole series\n    and the correlation on a rolling window\"\"\"\n    \n    ticker0, ticker1 = df0.iloc[0]['ticker'], df1.iloc[0]['ticker']\n    if use_returns:\n        df0_close, df1_close = df0.close.pct_change(), df1.close.pct_change()\n        price_type = 'Returns'\n    else:\n        df0_close, df1_close = df0.close, df1.close\n        price_type = 'Absolute Prices'\n        \n    df = merge_series(df0_close, df1_close, ticker0, ticker1)\n    \n    # Compute correlations\n    corr_spearman = df[ticker0].corr(df[ticker1], method='spearman')\n    corr_pearson = df[ticker0].corr(df[ticker1], method='pearson')\n    \n    # Print correlations\n    print(f\" Correlation between {ticker0} and {ticker1} using {offset}+{price_type} with Spearman method is: \" +  (RED+BOLD) + str(corr_spearman) + END)\n    print(f\" Correlation between {ticker0} and {ticker1} using {offset}+{price_type} with Pearson method is: \" + (RED+BOLD) + str(corr_pearson) + END)\n    \n    # Compute and plot rolling correlations \n    plot_rolling_correlation(df, ticker0, ticker1, window0, offset, price_type)   \n    print('\\n')\n    plot_rolling_correlation(df, ticker0, ticker1, window1, offset, price_type)    \n    \n# Test correlation with Daily crypto data\ntest_correlation(resample_timeseries(btc, 'D'), resample_timeseries(eth, 'D'), 30, 60, 'day'.capitalize(), False)     # BTC and ETH are highly positively correlated with a value of 0.797\ntest_correlation(resample_timeseries(btc, 'D'), resample_timeseries(eth, 'D'), 30, 60, 'day'.capitalize())\n\n# Test correlation with Monthly crypto data\ntest_correlation(resample_timeseries(btc, 'M'), resample_timeseries(eth, 'M'), 3, 6, 'month'.capitalize(), False)        # BTC and ETH are highly positively correlated with a value of 0.797\ntest_correlation(resample_timeseries(btc, 'M'), resample_timeseries(eth, 'M'), 3, 6, 'month'.capitalize())\n\n# # Test correlation with Daily crypto data between BTC\/DOGE to see if they are less correlated compared to BTC\/ETH\n# test_correlation(resample_timeseries(btc_filtered, 'D'), resample_timeseries(doge_filtered, 'D'), 30, 60, 'day'.capitalize())\n\n# Test correlation with Daily crypto data between BTC\/BNB to see if they are less correlated compared to BTC\/ETH\ntest_correlation(resample_timeseries(btc, 'D'), resample_timeseries(bnb, 'D'), 30, 60, 'day'.capitalize())","b24ecce2":"from statsmodels.tsa.stattools import adfuller\n\nFORMATTER = mpl_dates.DateFormatter('%Y-%m-%d')\nbtc_hourly = resample_timeseries(btc) \neth_hourly = resample_timeseries(eth)\ndoge_hourly = resample_timeseries(doge) \n\n\ndef plot_series(series, asset):\n    \n    plt.figure(figsize=(15,8))\n    plt.plot(series, linestyle='solid')\n    plt.gca().xaxis.set_major_formatter(FORMATTER) # set date format\n    \n    plt.title(f\"{asset} closing prices\", fontweight='bold')\n    plt.xlabel('Date')\n    plt.ylabel('Price')\n    plt.show()\n\ndef check_stationarity(series, asset):\n\n    result = adfuller(series.values)\n    \n    print(BOLD + f\"Stationary Test for {asset}\" + END + '\\n')\n    print('ADF Statistic: %f' % result[0])\n    print('p-value: %f' % result[1])\n    print('Critical Values:')\n    for key, value in result[4].items():\n        print('\\t%s: %.3f' % (key, value))\n\n    if (result[1] <= 0.05) & (result[4]['5%'] > result[0]):\n        print(RED + \"Stationary\" + END)\n    else:\n        print(RED + \"Non-stationary\" + END)\n    print('\\n')\n    \n    plot_series(series, asset)\n           \ncheck_stationarity(btc_hourly.close, 'BTC'), check_stationarity(eth_hourly.close, 'ETH'), check_stationarity(doge_hourly.close, 'DOGE')","5ee42c73":"from math import sqrt, log\n\n\ndef close_close_volatility(df,window_size, N=365): # N=365 because crypto markets don't close \n    # Compute log returns using close prices \n    df['log_returns'] = np.log(df['close'] \/ df['close'].shift(1))\n    \n    # Compute historical volatility\n    return df.log_returns.rolling(window_size).std() * np.sqrt(N)\n\ndef calc_daily_features(df):\n    \n    df['date'] = pd.to_datetime(df.index.date)\n    df['date'] = df['date'].dt.strftime('%Y%m%d').astype(int)\n    \n    # Momentum \n    df['rsi_14d'] = RSI(df.close, timeperiod=14)\n    df['bbands_upper'], df['bbands_middle'], df['bbands_lower'] = BBANDS(df.close, timeperiod=10, nbdevup=2, nbdevdn=2)\n    \n    # SMA\n    df['sma_10d'] = df.close.rolling(window=10).mean()\n    df['sma_20d'] = df.close.rolling(window=20).mean()\n    df['sma_30d'] = df.close.rolling(window=30).mean()\n    \n    # Volume moving average\n    df['avg_volume_10d'] = df['volume'].rolling(window=10).mean()\n    df['avg_volume_20d'] = df['volume'].rolling(window=20).mean()\n    df['avg_volume_30d'] = df['volume'].rolling(window=30).mean()\n\n    # Volatility\n    df['close_close_vol_10d'] = close_close_volatility(df,10)\n    df['close_close_vol_20d'] = close_close_volatility(df,20)\n    df['close_close_vol_30d'] = close_close_volatility(df,30)\n    df['atr_14d'] = ATR(df.high, df.low, df.close, timeperiod=14)\n    \n    return df.iloc[:, 8:] \n    \n\ndef calc_features(df, daily=True):\n    \n#     df = reduce_mem_usage(df)\n    \n    CLOSE = df.close\n    DF_DAILY = resample_timeseries(df, 'D')\n    \n       # Price transformation features\n    df['avg_price'] = (df['close'] + df['open'] + df['low'] + df['high']) \/ 4\n    df['weighted_close'] = ((df['close'] * 2) + df['high'] + df['low']) \/ 4        # extra weight on the close pri\n    df['typical_price'] = (df['high'] + df['low'] + df['close']) \/ 3 \n    df['median_price'] = (df['high'] + df['low']) \/ 2\n    \n    df['dollar_volume'] = df['close'].mul(df['volume'])\n    df['volume_per_trade'] = df['volume'].div(df['interval_trade_count'])\n    df['dollar_volume_per_trade'] = df['dollar_volume'].div(df['interval_trade_count'])\n    \n    df_features_daily = calc_daily_features(DF_DAILY) \n\n    # Join daily features back to original df which contains minute data\n    if daily:\n        return df_features_daily\n    else:\n        df['timestamp'] = df.index\n        df = df.merge(df_features_daily, on='date', how='left').set_index(df['timestamp']).drop(columns='timestamp')\n        df = df.dropna(subset=['close_close_vol_30d'])\n        return df ","b22012ab":"btc_feat = calc_features(btc, False) \ndoge_feat = calc_features(doge, False)\nbtc_feat.head()","e44a5944":"# %%time\n# # Generate dictionary of Dataframes for all tickers and store in dict..\n# def generate_crypto_dfs_with_features():\n#     \"\"\"Returns a dictionary of dataframes with key(Ticker):value(Dataframe with generated features that belongs to that ticker)\"\"\"\n    \n#     return {ticker:generate_features(df_train.query(\"ticker == @ticker\"), False) for ticker in tickers}\n\n# all_cryptos_feat = generate_crypto_dfs_with_features()","a435fb27":"## How many targets of BTC are null\nprint(f\"Number of BTC target values that are null: {btc_feat.target.isnull().sum()}\")\nprint(f\"Number of DOGE target values that are null: {doge_feat.target.isnull().sum()}\")\n\n## How many total null values\nbtc_feat.isnull().sum()\nbtc_feat.isna().sum()\n","bc402820":"## Recreate target\ndef recreate_target(df):\n    price_column = df.close\n    df['p1'] = df[price_column].shift(freq='-1T')\n    df['p16'] = df[price_column].shift(freq='-16T')\n    df['r'] = np.log(df.p16\/ df.p1)\n    df.drop(['p1', 'p16'], axis=1, inplace=True)\n    df.reset_index(inplace=True)\n    \n    return df\n\n\n\n","fb2aefd4":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nbtc_feat = btc_feat.dropna(how=\"any\")\nbtc_y = btc_feat['target']\nfeatures = ['vwap', 'weighted_close', 'avg_price', 'median_price', 'typical_price', 'sma_10d', 'sma_20d', 'rsi_14d', 'atr_14d', 'close_close_vol_10d', \\\n           'close_close_vol_20d'] \n\n\nbtc_y = btc_feat['target']\nbtc_x = pd.DataFrame(btc_feat, columns = features)\n\nprint(btc_y.shape)\nprint(btc_x.shape)\nbtc_x.head()\n","718d024d":"X_train, X_test, y_train, y_test = \\\n    train_test_split(btc_x.values, btc_y.values, test_size = 0.25, random_state = 1)\n\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\nprint(y_train)","c2018bef":"scaler = StandardScaler()\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test).astype(float)\n\nprint(X_train_scaled)\nprint(X_train_scaled.shape)\nprint(X_test_scaled.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n# X_train_scaled.head()","167aa575":"## Linear Regression\n\n# from sklearn.linear_model import LinearRegression\n\n# # implement basic ML baseline (one per asset)\n# lr = LinearRegression()\n# lr.fit(X_train_scaled, y_train)\n# y_pred_lr = lr.predict(X_test_scaled)\n\n# models\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\n# parameters\nparams = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'learning_rate': 0.01\n        }\n\n# train (full model)\nmodel = lgb.LGBMRegressor(**params)\nmodel.fit(X_train_scaled, y_train, verbose=-1)\n\ny_pred = model.predict(X_test_scaled)","48c2b695":"first_pred = y_pred[0]\nprint(first_pred)","f5cdfe77":"## Feature importance\n# feature importance\n\ndef feature_importance():\n    fi_df = pd.DataFrame()\n    fi_df['features'] = features\n    f_df = fi_df[:-1]\n    fi_df['importance'] = model.booster_.feature_importance(importance_type=\"gain\")\n    \n\n    # plot feature importance\n    fig, ax = plt.subplots(1, 1, figsize=(7, 15))\n    sns.barplot(\n    x='importance', \n    y='features',\n    data=fi_df.sort_values(by=['importance'], ascending=False),\n    ax=ax)\n\nfeature_importance()","1de66f3c":"## Evaluate\nfrom sklearn.metrics import r2_score\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom math import sqrt\n\ndef mae(observation, forecast):    \n    error = mean_absolute_error(observation, forecast)\n    print('Mean Absolute Error (MAE): {:.3g}'.format(error))\n    return error\n\ndef mape(observation, forecast): \n    observation, forecast = np.array(observation), np.array(forecast)\n    # Might encounter division by zero error when observation is zero\n    error = np.mean(np.abs((observation - forecast) \/ observation)) * 100\n    print('Mean Absolute Percentage Error (MAPE): {:.3g}'.format(error))\n    return error\n\ndef rmse(observation, forecast):\n    error = sqrt(mean_squared_error(observation, forecast))\n    print('Root Mean Square Error (RMSE): {:.3g}'.format(error))\n    return error\n\ndef evaluate(pd_dataframe, observation, forecast):\n    \"\"\"\n    :params\n    :pd_dataframe pandas dataframe\n    :observation column name of expected values\n    :forecast column name of forecasted values\n    :return the results of MAE, MAPE and RMSE, respectively\n    \"\"\"\n    first_valid_date = pd_dataframe[forecast].first_valid_index()\n    mae_error = mae(pd_dataframe[observation].loc[first_valid_date:, ], pd_dataframe[forecast].loc[first_valid_date:, ])\n    mape_error = mape(pd_dataframe[observation].loc[first_valid_date:, ], pd_dataframe[forecast].loc[first_valid_date:, ])\n    rmse_error = rmse(pd_dataframe[observation].loc[first_valid_date:, ], pd_dataframe[forecast].loc[first_valid_date:, ]) \n\n    ax = pd_dataframe.loc[:, [observation, forecast]].plot()\n    ax.xaxis.label.set_visible(False)\n    \n#     return mae_error, mape_error, rmse_error\n#     print()\n\n# evaluate(X_test_scaled, y_test, y_pred)\n\n\n","053d6473":"# env = gresearch_crypto.make_env()\n# iter_test = env.iter_test()\n\n# for i, (df_test, df_pred) in enumerate(iter_test):\n#     for j , row in df_test.iterrows():        \n#         if new_models[row['Asset_ID']] is not None:\n#             try:\n#                 model = new_models[row['Asset_ID']]\n#                 x_test = get_features(row)\n#                 y_pred = model.predict(pd.DataFrame([x_test]))[0]\n#                 df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n#             except:\n#                 df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n#                 traceback.print_exc()\n#         else: \n#             df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0  \n    \n#     env.predict(df_pred)\n\n# import gresearch_crypto\n# env = gresearch_crypto.make_env()   # initialize the environment\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n# for (test_df, sample_prediction_df) in iter_test:\n#     print(test_df)\n#     print('\\n')\n#     print(sample_prediction_df)\n#     # feature engineering\n#     test_df = calc_features(test_df, False)\n    \n#     # inference\n#     sample_prediction_df['Target'] = model.predict(test_df[features])  # make your predictions here\n    \n#     # register your predictions\n#     env.predict(sample_prediction_df)  \n\n# import gresearch_crypto\n# env = gresearch_crypto.make_env()\n# iter_test = env.iter_test()\n\n# for i, (df_test, df_pred) in enumerate(iter_test):\n#     for _, row in df_test.iterrows():\n#         asset_id = int(row['Asset_ID'])\n#         if list_models[asset_id] is not None:\n#             try:\n#                 model  = list_models[asset_id]\n#                 x_test = x_feature(row)\n#                 y_pred = model.predict(pd.DataFrame([x_test]))[0]\n#                 df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = y_pred\n#             except:\n#                 df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n#         else: \n#             df_pred.loc[df_pred['row_id'] == row['row_id'], 'Target'] = 0\n        \n#     env.predict(df_pred)","a9672300":"## Prepare first train data\nTaking in the raw train data, we wrangle the data in a first step perform the following steps:\n* Transform unix timestamp to a regular timestamp\n* Extract simple date values like date, year, and month\n* Add colomns from asset details including the ticket, which makes the Data Frames more easy to filter\n* Rename count to interval_trade_count\n","169913bb":"# <b>TODO<\/b>","a7ad6f72":"![gresearch.png](attachment:8f9e16fc-c54c-4f53-8a06-5e816ea1b10b.png)  <p style=\"text-align:center;\"><span style=\"font-size:80px;\"><span style=\"color:orange;\"> <i>Crypto Forecasting<\/i> <\/span><\/span><\/p>\n  \n<span style=\"font-size:18px;\"><span style=\"font-family:cursive;\">\n  <b>Author: Vincent Weimer <br>\n      Date: 2022-01-13<\/b>\n    <\/span>\n  \n<hr><\/hr> ","743269e1":"## Correlation","173c2f00":"### Example Sample Submission","8f1cb798":"# Feature Engineering\n\nFeatures considered:\n\n* Price transformation features\n* RSI\n* Historical Volatility\n* Momentum features\n* Rolling Average Volume","f56d8691":"## Preprocessing train data","d3257273":"## Linear Regression","427ab2d4":"# Statistical Analysis","eca60a43":"<b><i>Important!! Test data starts at 2021-06-13 and training data ends at 2021-09-21. To prevent data leakage, we need to filter train_df before the test data starts.<\/i><\/b>","a81cb423":"# Modeling ","91e531a6":"# Evaluation","6f9bea73":"## Stationarity","990dd1ef":"The data contains more than 24 million rows! We need to reduce its memory using the following function.","7289fb5c":"# Submission","12003e49":"### Example Test"}}