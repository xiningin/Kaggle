{"cell_type":{"70dda336":"code","0da96082":"code","ee04a975":"code","2200e1dc":"code","30d5e24a":"code","acb52ea7":"code","95a59039":"code","575f3dcc":"code","a3a0e4c6":"code","99736a2a":"code","010eb1a7":"code","21904732":"code","f3eeeb1f":"code","00be12c1":"code","0f83e9d2":"code","09527437":"code","d7be6b59":"code","79de2dce":"code","f1f13a2d":"code","77446e5d":"code","f4a30d66":"code","f2f2a0e7":"code","40dba3ec":"code","ed91f919":"code","712d850f":"code","26e53fae":"code","35dcd25f":"markdown","a8232965":"markdown","d79e06f6":"markdown","3526dc81":"markdown","92f05aca":"markdown","bf8bb289":"markdown","dc3c2d84":"markdown","c1128b21":"markdown","f7a765da":"markdown","bfc2d8b3":"markdown","85d7b587":"markdown","4b8bb93a":"markdown","63e557ee":"markdown","d1d8c810":"markdown","fc472948":"markdown","d8b72eea":"markdown","ca5a2e21":"markdown","068c8d50":"markdown","641b1a3a":"markdown","8ee9e85d":"markdown","22fdb63d":"markdown"},"source":{"70dda336":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport seaborn as sns\nimport matplotlib.pyplot as plt","0da96082":"data_types_dict = {\n#     'time_id': 'int32',\n    'investment_id': 'int16',\n    \"target\": 'float16',\n}\n\nfeatures = [f'f_{i}' for i in range(300)]\n\nfor f in features:\n    data_types_dict[f] = 'float16'\n    \ntarget = 'target'","ee04a975":"train_df = pd.read_csv('\/kaggle\/input\/ubiquant-market-prediction\/train.csv', \n#                        nrows=5 * 10**3,\n                       usecols = data_types_dict.keys(),\n                       dtype=data_types_dict)","2200e1dc":"train_df","30d5e24a":"train_df['target'].hist(bins = 100, figsize = (20,10))","acb52ea7":"for f in np.random.choice(train_df['investment_id'].unique(), 10):\n    train_df[train_df['investment_id'] == f]['target'].hist(bins = 100, alpha = 0.2, figsize = (20,10))","95a59039":"train_df['investment_id'].nunique()","575f3dcc":"train_df['investment_id'].value_counts().plot(kind = 'bar',figsize = (20,10))","a3a0e4c6":"f = 'f_67'\ntrain_df[f].hist(bins = 100, figsize = (20,10))","99736a2a":"f = 'f_109'\ntrain_df[f].hist(bins = 100, figsize = (20,10))","010eb1a7":"f = 'f_62'\ntrain_df[f].hist(bins = 100, figsize = (20,10))","21904732":"f = 'f_234'\ntrain_df[f].hist(bins = 100, figsize = (20,10))","f3eeeb1f":"f = 'f_164'\ntrain_df[f].hist(bins = 100, figsize = (20,10))","00be12c1":"train_df[features].nunique().hist()","0f83e9d2":"sample_df = train_df.sample(frac = 0.01)\nsample_df","09527437":"correlation = sample_df[[target] + features].corr()","d7be6b59":"correlation['target'].iloc[1:].hist(bins = 20, figsize = (20,10))","79de2dce":"sns.clustermap(correlation, figsize=(20, 20))","f1f13a2d":"top_feautures = ['f_74', 'f_153', 'f_145', 'f_108', 'f_231']","77446e5d":"from seaborn import pairplot\nsample_df = train_df.sample(10000).reset_index()\npairplot(sample_df[top_feautures + ['target']])","f4a30d66":"gen_features = []\n\nfor i, f1 in enumerate(top_feautures[:-1]):\n    for j, f2 in enumerate(top_feautures[i+1:]):\n        train_df[f\"{f1}*{f2}\"] = train_df[f1] * train_df[f2]\n        train_df[f\"{f1}\/{f2}\"] = train_df[f1] \/ train_df[f2]\n        \n        gen_features.append(f\"{f1}*{f2}\")\n        gen_features.append(f\"{f1}\/{f2}\")","f2f2a0e7":"from lightgbm import LGBMRegressor","40dba3ec":"features += gen_features","ed91f919":"from sklearn.model_selection import StratifiedKFold \nseed = 0\nfolds = 15\nmodels = []\n\nskf = StratifiedKFold(folds, shuffle = True, random_state = seed)\n\nfor train_index, test_index in skf.split(train_df, train_df['investment_id']):\n    train = train_df.iloc[train_index]\n    valid = train_df.iloc[test_index]\n    \n    lgbm = LGBMRegressor(\n        num_leaves=31,\n        n_estimators = 1500,\n        min_child_samples = 1000, \n        subsample=0.7, \n        subsample_freq=1,\n        n_jobs= -1\n    )\n\n    lgbm.fit(train[features], train[target], eval_set = (valid[features], valid[target]), early_stopping_rounds = 10)\n    models.append(lgbm)","712d850f":"import lightgbm\nlightgbm.plot_importance(lgbm, figsize = (20, 60))","26e53fae":"import ubiquant\nenv = ubiquant.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (test_df, sample_prediction_df) in iter_test:\n    \n    for i, f1 in enumerate(top_feautures[:-1]):\n        for j, f2 in enumerate(top_feautures[i+1:]):\n            test_df[f\"{f1}*{f2}\"] = test_df[f1] * test_df[f2]\n            test_df[f\"{f1}\/{f2}\"] = test_df[f1] \/ test_df[f2]\n    \n    test_df['target']  = 0\n    \n    for lgbm in models:\n        test_df['target'] += lgbm.predict(test_df[features])\n    test_df['target'] \/= len(models)\n    env.predict(test_df[['row_id','target']])","35dcd25f":"# Model training","a8232965":"We will do analysis on a smaller random 1% samle of the dataset to speed up the process.","d79e06f6":"We have 3579 different investments, and most of them have a substantial amount of data points and probably don't require any filtering so far.","3526dc81":"All features have a lot of unique values, so they either float or have some added noise to hide the integer\/categorical nature.","92f05aca":"# Basic EDA","bf8bb289":"On a high-level target for each investment_id also looks ok.","dc3c2d84":"## Features interaction","c1128b21":"It is hard to analyze all features one by one, but let's do so aggregated analysis. First of all, let's just look at some features distributions.","f7a765da":"There are definitely some clusters of highly correlated features that can be later analyzed together.","bfc2d8b3":"## Feature engeneering","85d7b587":"I will use LGBMRegressor to train a simple baseline model.","4b8bb93a":"Some features look normal, but most have outliers, skewed distribution, and multiple modes. Probably the analysis of features one by one will bring a lot of value later in the competition, but we will not go deep into it in this notebook.","63e557ee":"# Features","d1d8c810":"## Investment_id","fc472948":"Let's load the data and look a the high-level data structure.","d8b72eea":"Let's take some top features from the last run of the notebook, look at them and generate some interactions.","ca5a2e21":"There is no strong correlation between features and target. Let's look at the correlation of features with each other.","068c8d50":"Our dataset contains 300 anonymous features that don't have any description, `investment_id,` and target that is also some anonymous float value.","641b1a3a":"This notebook contains an initial EDA of train dataset for the [Ubiquant Market Prediction competition](https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction) as well as a simple baseline model training and inference. Here I analyze the overall structure of the dataset, the distributions of different features, and the correlation of features and target.","8ee9e85d":"The target values look quite normal without any outliers or long tails. We should not have any problems working with it. Let's also plot distributions of targets of a few random features:","22fdb63d":"## Target"}}