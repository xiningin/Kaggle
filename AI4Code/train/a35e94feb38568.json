{"cell_type":{"c6c09a64":"code","b5846482":"code","529ea7f9":"code","2fa0d73b":"code","ed0f4da2":"code","e6a262fd":"code","03ec9c09":"code","f98fd503":"code","f22e8305":"code","eec5e8d4":"code","4ae447a6":"code","97bd745e":"code","42929a32":"code","857feeac":"code","6068cf68":"code","f8d51a51":"code","0583bf8e":"code","df65a39e":"code","94c9fad5":"code","210419ea":"code","f448189c":"code","fe1c34fc":"code","250e1237":"code","cc14e5c6":"code","cab71dc5":"code","44ba765b":"code","971ae36f":"code","53c0856d":"code","0a410d50":"code","7818a338":"code","2f6c1740":"code","afb32e70":"code","8e76c23d":"code","9c91104e":"code","94d9da95":"code","32517544":"code","0f58c722":"code","57657d2e":"code","5a69b12c":"code","e04bd891":"code","8ceceddf":"code","2f7c54d4":"code","7fef1b0d":"code","33d2891b":"code","24519795":"code","a9bf94eb":"code","16910ecd":"code","bcb00504":"code","863c3656":"code","fab79b31":"code","36421f67":"code","ce2ad37f":"code","74da507e":"code","f2c68b26":"code","54200ee3":"code","131527d3":"code","18e97935":"code","d6c70aea":"code","5e9607d1":"code","50b71ab3":"code","78458ef5":"code","7b3d1a10":"code","6a4790a7":"code","d7d36e06":"code","6924f044":"code","a69745df":"code","305dcddf":"code","cc667823":"code","bdc29c20":"code","9505f262":"code","cf7cbefb":"code","4aa7e32c":"code","795853ed":"code","edbbb548":"code","601e4eb8":"code","ed93a1b2":"code","f7da5e28":"code","d7112d86":"code","eb42f595":"code","f37345b8":"code","e4dc7661":"code","71f5ac33":"code","fe1d6d1c":"code","c1b1debe":"code","e1806393":"code","8b8653af":"code","c9abdbd5":"code","95901968":"code","509b6d67":"code","b3140beb":"code","dcf98cf2":"code","b06b1ac9":"code","6e39b281":"code","5630c873":"code","fd0de559":"code","4344fd79":"code","80d40983":"code","1be6a3ac":"code","431ea165":"code","3bf144e9":"code","4890e145":"code","0b66767b":"code","713129c3":"code","4e997670":"code","70a6679f":"code","3072f540":"code","af392c9e":"code","e616a193":"code","ff441c1a":"code","9e342e61":"code","43e78711":"code","8ca3ca20":"code","7601cf09":"code","a2261d64":"code","80641e38":"code","16809a0d":"code","6e5b055d":"code","eca70aef":"code","5839e319":"code","96c2f107":"code","2d89e0f2":"code","81ea28b5":"code","eb68ca5d":"code","ae73fb58":"code","68d7e75f":"code","40d017d8":"code","70100c32":"code","4cd436e0":"code","4c25681c":"code","d834ce4a":"code","aa4cb8b4":"code","619c4143":"code","537c621c":"code","9ca4a468":"code","a77bfe34":"code","96700130":"code","5a1aed2d":"code","ae684cce":"code","742d19d1":"code","c7d9c54d":"code","8d7e811f":"code","a035449e":"code","baf36fa1":"code","95011aa5":"code","21119777":"code","f2e504c7":"code","5e92a777":"code","8c8783ef":"code","1d949079":"code","4d4e9847":"code","25879c0d":"code","7b9c8214":"code","e1ed948e":"code","00b60009":"code","408749c3":"code","b1708200":"code","a2990c01":"code","0f0c6c44":"code","8ea837f9":"code","867ed7a2":"code","10728590":"code","3c9367dc":"code","69d8246e":"code","6a98bfac":"code","f3dfcc8b":"code","adc0f50c":"code","ce600906":"code","3526a41a":"code","c75d032d":"code","13ad3718":"code","c99804a2":"code","e43dce2f":"code","c73166a3":"code","04d6ed24":"code","18c37ce3":"code","dce46cb5":"code","e47680fa":"code","9816472e":"code","8a245eea":"code","fcdb8185":"code","7a9dc0a7":"code","0428ff96":"code","8113e0ef":"code","8d22fcbb":"code","75e96a7c":"code","98cc8416":"code","d15a755e":"code","16686e8d":"code","0ee7b39e":"code","e23e7b9e":"code","23448fe7":"code","4815e859":"code","842e91db":"code","903b5172":"code","876b87c6":"code","7aedef22":"code","cab5338b":"code","1f2706f0":"code","c34d5ba0":"code","41b76513":"code","d8de813d":"code","69eb8852":"code","2d20bd2c":"code","ca278825":"code","56249dad":"code","37d30276":"code","5501565c":"code","1a64c94c":"code","e160cb52":"code","1be4da77":"code","8bd3b3f9":"code","1415c54f":"code","a1c4a9a6":"code","77dbd783":"code","9bd8e9e9":"code","e95d7c1d":"code","e3b142cf":"code","922202c0":"code","9d309f3d":"code","474bcc6e":"code","85de710c":"code","cb3c5304":"code","9f97262d":"code","8f3cbd66":"code","3a759ffe":"code","7871d8e6":"code","dca6e811":"code","f5ccf880":"code","5ddd31c9":"code","8eebf738":"code","a09a33f0":"code","d76c5b55":"code","fc7deb78":"code","24014785":"code","b96919f4":"code","38724517":"code","12243c50":"code","0776e784":"code","ee7b2a40":"code","5532adf6":"code","c2667f74":"code","fd5d8fe0":"code","1e9a1b5f":"code","be9ce555":"code","4c1e75a7":"code","44902005":"code","fc7d1663":"code","21097316":"code","47fabe02":"code","20e6da23":"code","5fa706a7":"code","344bbc7a":"code","88fce42b":"code","9dad9e80":"code","ba3b6319":"code","c1dc07eb":"code","29522f96":"code","ac036d7b":"code","11fd35c7":"code","4eaf6d05":"code","f2ccc5a6":"code","3992a9c7":"code","e3096ae8":"code","dec7e5ea":"code","eaa1e476":"code","2f31987b":"code","d732f9e3":"code","d671a7e1":"code","b12f4af7":"code","da4ee0e0":"code","adb95d70":"code","8e711ae1":"code","85d011a8":"code","ee188bab":"code","21f6030a":"code","9c4c65e8":"code","3357ac02":"code","be39ec20":"code","741687a0":"code","d5a2447c":"code","d14e2fdf":"code","cc813833":"code","207cb05e":"code","850b5bc7":"code","c050f1e7":"code","c43309f5":"code","30dc817a":"code","1ff4a071":"code","ed23c681":"code","8342aa51":"code","d6a36eaf":"code","6a9b50ff":"code","2152ed49":"code","d7fdfa88":"code","8174a3f8":"code","699a26ef":"code","f3cd094c":"code","886aad7e":"code","d8842f20":"code","5d111225":"code","e18a0fbf":"code","4416b86b":"code","074a7c88":"markdown","54b666f9":"markdown","fad14409":"markdown","de6d8053":"markdown","28001515":"markdown","9e805e94":"markdown","c38e136b":"markdown","83a82dd1":"markdown","b53662b1":"markdown","d05afe54":"markdown","18d052b0":"markdown","2d5ac5e2":"markdown","931d3579":"markdown","995a7929":"markdown","defddccc":"markdown","d8f21130":"markdown","093c3bd3":"markdown","bd99bde1":"markdown","3400b392":"markdown","56fbc4be":"markdown","f42e7e98":"markdown","f1df4855":"markdown","ac1a5b45":"markdown","6c065b34":"markdown","cbdd42f5":"markdown","e8b5ccdf":"markdown","13c81ec0":"markdown","a07bc11e":"markdown","80f9431e":"markdown","a90dba7a":"markdown","a03d1b0d":"markdown","33c407e2":"markdown","2e7d0a07":"markdown","537e4d67":"markdown","f347a28b":"markdown","9baec40b":"markdown","f5f88efc":"markdown","fcc0161d":"markdown","c68d798a":"markdown","caef7652":"markdown","accd9c3d":"markdown","76ba867f":"markdown","6530b516":"markdown","24550f3e":"markdown","c3ac3094":"markdown","2dff9496":"markdown","ce93f7f1":"markdown","4d71d107":"markdown","c865f60d":"markdown","804f1b72":"markdown","b9315353":"markdown","e881ba92":"markdown","e487bcba":"markdown","06ba1f8b":"markdown","9e633b8a":"markdown","49d0aa2f":"markdown","eaf990a1":"markdown","5bce39c5":"markdown","a40028f9":"markdown","4399ee35":"markdown","e249cfd3":"markdown","e72bfc5b":"markdown","8ade211a":"markdown","6efdde50":"markdown","0dd3350b":"markdown","534b88ba":"markdown","55cd584d":"markdown","fd60a31c":"markdown","84ac3a98":"markdown","1bc22ac1":"markdown","6f9d3a79":"markdown","585b0d2d":"markdown","d7e9c6d0":"markdown","8305b209":"markdown","43b608ae":"markdown","81acc633":"markdown","0226cd7b":"markdown","5eac9b4b":"markdown","4e8386b2":"markdown","7fb77858":"markdown","7a687a9f":"markdown","1fde5078":"markdown","485c5400":"markdown","a2e10f52":"markdown","e7e4c3fe":"markdown","a932820c":"markdown","3e22b2ff":"markdown","920a3ab3":"markdown","78cc1562":"markdown","13f346f3":"markdown","12675eba":"markdown","7aa1be72":"markdown","539ca655":"markdown","eb520e2a":"markdown","3308492d":"markdown","fd4bb1cd":"markdown","df87852d":"markdown","911af4c2":"markdown","a1521d68":"markdown","de9ff4ea":"markdown","abb79989":"markdown","acc24006":"markdown","b65def72":"markdown","d32cf3ae":"markdown"},"source":{"c6c09a64":"# importing the required packages and libraries\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport graphviz\nimport pydotplus\nfrom IPython.display import Image\nfrom sklearn.externals.six import StringIO\n\nimport sklearn\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import IncrementalPCA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.externals.six import StringIO\nfrom sklearn.tree import export_graphviz\n\nfrom sklearn.utils import resample\nimport io\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action='ignore', category=FutureWarning)","b5846482":"# Read the data - please update the path of the csv in the statement below\n\npath = r'..\/input\/telecom_churn_data.csv'\ntelecom = pd.read_csv(path)\n\n# creating a copy of the dataframe, just in case it is needed further down the assignment\ntelecom_copy = telecom.copy(deep = True)\n\n# viewing the top few records\ntelecom.head()","529ea7f9":"telecom.shape","2fa0d73b":"telecom.info()","ed0f4da2":"telecom.describe()","e6a262fd":"# Having a look at the list of columns of the dataset before proceeding to clean the data\n\nprint(list(telecom.columns))","03ec9c09":"# Renaming the columns\n\ntelecom = telecom.rename(columns={'aug_vbc_3g': 'vbc_3g_8', 'jul_vbc_3g': 'vbc_3g_7',\n                                  'jun_vbc_3g': 'vbc_3g_6', 'sep_vbc_3g': 'vbc_3g_9'})","f98fd503":"# Checking the list of values in the circle \ntelecom['circle_id'].value_counts()","f22e8305":"plt.figure(figsize=(2,5))\nsns.countplot(telecom['circle_id'])\nplt.show()","eec5e8d4":"telecom = telecom.drop('circle_id', axis = 1)","4ae447a6":"'''\n\n    Since there are a lot of columns in the data, the null values will need to handled iteratively.\n    The following function takes the dataframe as input, and returns the null percentage in the data in the descending order\n    \n'''\n\ndef null_pct(df):\n    x = round((df.isnull().sum()\/len(df.index) * 100),2).sort_values(ascending = False)\n    return x","97bd745e":"print(null_pct(telecom))","42929a32":"'''\n    This method takes, three parameters -:\n        1) A dataframe\n        2) Lower range\n        3) Upper range\n        \n    This function then returns a list of all the columns which have null values in the given range.\n\n'''\n\ndef check_null_columns(df,a,b):\n    try:\n        x = (df.isna().sum() * 100 \/ len(df)).sort_values(ascending=False)\n        y = x[x.between(a,b)]\n        y = str(y).split()\n        col = y[::2][:-1]\n        if len(col) ==1:\n            return []\n        return col\n    except:\n        print(\"error in the function, no tracebacking :)\")","857feeac":"t1 = check_null_columns(telecom, 70,80)\nprint(t1)","6068cf68":"'''\n\n    The following function accepts three parameters\n        1. The dataframe in which the columns need to be imputed\n        2. The list of columns that need to be imputed\n        3. The list of values to be imputed in the respective columns (passed as the second parameter)\n\n'''\n\ndef impute_col(df,col,val):\n    for (i,j) in zip(col,val):\n        df[i] = df[i].fillna(j)\n    return df","f8d51a51":"# looking at the number of 2g recharges done in June\n\ntelecom['count_rech_2g_6'].describe()","0583bf8e":"plt.figure(figsize=(15,5))\nsns.countplot(telecom['count_rech_2g_6'])\nplt.show()","df65a39e":"# Two lists are being created - one for the name of the column and one for the value it needs to be imputed with(for the null data)\n\ncol_imp = ['count_rech_2g_6']\nval_imp = [0] ","94c9fad5":"# looking at the values in the date of last recharge for data in June\n\ntelecom['date_of_last_rech_data_6'].value_counts()","210419ea":"for i in t1:\n    if 'date' in i:\n        print(i)","f448189c":"drop_col = ['date_of_last_rech_data_6','date_of_last_rech_data_7','date_of_last_rech_data_8', 'date_of_last_rech_data_9']","fe1c34fc":"# looking at the number of recharges done for a 3g pack in June\ntelecom['count_rech_3g_6'].describe()","250e1237":"plt.figure(figsize=(15,5))\nsns.countplot(telecom['count_rech_3g_6'])\nplt.show()","cc14e5c6":"# appending the column in col_imp list and the value that needs to be imputed is appende din the val_imp list\ncol_imp.append('count_rech_3g_6')\nval_imp.append(0)\nprint(t1)","cab71dc5":"telecom['av_rech_amt_data_6'].describe()","44ba765b":"print(telecom[telecom['av_rech_amt_data_6'].isna()]['vol_2g_mb_6'].value_counts())\nprint(telecom[telecom['av_rech_amt_data_6'].isna()]['vol_3g_mb_6'].value_counts())","971ae36f":"# similarly appending the column and the value to the required lists\ncol_imp.append('av_rech_amt_data_6')\nval_imp.append(0)\ntelecom['max_rech_data_6'].describe()","53c0856d":"# The column max_rech_data_6 needs to be dropped as it is not adding much value to the model.\ndrop_col.append('max_rech_data_6')","0a410d50":"telecom['total_rech_data_6'].describe()","7818a338":"telecom['arpu_3g_6'].describe()","2f6c1740":"# Imputing the column arpu_3g_6 with the median, as it makes sense to impute the missing values with the median.\n\ncol_imp.append('arpu_3g_6')\nval_imp.append(telecom['arpu_3g_6'].median())","afb32e70":"telecom['arpu_2g_6'].describe()","8e76c23d":"# Imputing the column arpu_2g_6 with the median, as it makes sense to impute the missing values with the median.\n\ncol_imp.append('arpu_2g_6')\nval_imp.append(telecom['arpu_2g_6'].median())","9c91104e":"telecom['night_pck_user_6'].describe()","94d9da95":"plt.figure(figsize=(3,5))\nsns.countplot(telecom['night_pck_user_6'])\nplt.show()","32517544":"# Imputing the column night_pck_user_6 with the median, as it makes sense to impute the missing values with the median.\n\ncol_imp.append('night_pck_user_6')\nval_imp.append(0)","0f58c722":"telecom['fb_user_6'].value_counts()","57657d2e":"plt.figure(figsize=(3,5))\nsns.countplot(telecom['fb_user_6'])\nplt.show()","5a69b12c":"# Getting the value counts for fb_user_6, to understand how is it related to the column vol_2g_mb_6.\n\nprint(telecom[telecom['fb_user_6'].isna()]['vol_2g_mb_6'].value_counts())\nprint(telecom[telecom['fb_user_6'].isna()]['vol_3g_mb_6'].value_counts())","e04bd891":"# Imputing the column fb_user_6 with the median, as it makes sense to impute the missing values with the median.\n\ncol_imp.append('fb_user_6')\nval_imp.append(0)","8ceceddf":"telecom['count_rech_2g_7'].describe()","2f7c54d4":"plt.figure(figsize=(15,5))\nsns.countplot(telecom['count_rech_2g_7'])\nplt.show()","7fef1b0d":"# Imputing the column count_rech_2g_7 with 0 as its value.\n\ncol_imp.append('count_rech_2g_7')\nval_imp.append(0)","33d2891b":"telecom['count_rech_3g_7'].describe()","24519795":"plt.figure(figsize=(15,5))\nsns.countplot(telecom['count_rech_3g_7'])\nplt.show()","a9bf94eb":"# Here we are appending a column and its value with count_rech_3g_7 and 0 in the required lists. \n# These lists will later be used for imputing.\n\ncol_imp.append('count_rech_3g_7')\nval_imp.append(0)","16910ecd":"telecom['av_rech_amt_data_7'].describe()","bcb00504":"# Getting the value counts for av_rech_amt_data_7, to understand how is it related to the column vol_2g_mb_7.\n\nprint(telecom[telecom['av_rech_amt_data_7'].isna()]['vol_2g_mb_7'].value_counts())\nprint(telecom[telecom['av_rech_amt_data_7'].isna()]['vol_3g_mb_7'].value_counts())","863c3656":"# Below lines will help in imputing the column av_rech_amt_data_7 with the vaue 0\ncol_imp.append('av_rech_amt_data_7')\nval_imp.append(0)","fab79b31":"telecom['max_rech_data_7'].describe()","36421f67":"# we are dropping this column as well. Since it is not making much sense.\ndrop_col.append('max_rech_data_7')","ce2ad37f":"telecom['total_rech_data_7'].describe()\n#This will be handled later","74da507e":"telecom['arpu_3g_7'].describe()","f2c68b26":"# Here we have imputed the column arpu_3g_7 with it's median value.\n\ncol_imp.append('arpu_3g_7')\nval_imp.append(telecom['arpu_3g_7'].median())","54200ee3":"telecom['arpu_2g_7'].describe()","131527d3":"# Below lines will help in imputing the column arpu_2g_7 with it's median value.\n\ncol_imp.append('arpu_2g_7')\nval_imp.append(telecom['arpu_2g_7'].median())","18e97935":"telecom['night_pck_user_7'].describe()","d6c70aea":"plt.figure(figsize=(3,5))\nsns.countplot(telecom['night_pck_user_7'])\nplt.show()","5e9607d1":"# the column night_pck_user_7 will be imputed with it's median vaue.\n\ncol_imp.append('night_pck_user_7')\nval_imp.append(0)","50b71ab3":"telecom['fb_user_7'].value_counts()","78458ef5":"plt.figure(figsize=(3,5))\nsns.countplot(telecom['fb_user_7'])\nplt.show()","7b3d1a10":"# Understanding the relation of fb_user_7, when it is null. What happens to the column vol_2g_mb_7. This will help us in \n# indentifying the value with which we can impute the column fb_user_7\n\nprint(telecom[telecom['fb_user_7'].isna()]['vol_2g_mb_7'].value_counts())\nprint(telecom[telecom['fb_user_7'].isna()]['vol_3g_mb_7'].value_counts())","6a4790a7":"# Imputing the column fb_user_7 with a 0 value.\n\ncol_imp.append('fb_user_7')\nval_imp.append(0)","d7d36e06":"telecom['count_rech_2g_8'].describe()","6924f044":"plt.figure(figsize=(15,5))\nsns.countplot(telecom['count_rech_2g_8'])\nplt.show()","a69745df":"# Since, most of the data for the column count_rech_2g_8 is 0. Hence we are imputing the values with 0.\n\ncol_imp.append('count_rech_2g_8')\nval_imp.append(0)","305dcddf":"telecom['count_rech_3g_8'].describe()","cc667823":"plt.figure(figsize=(15,5))\nsns.countplot(telecom['count_rech_3g_8'])\nplt.show()","bdc29c20":"# Since, most of the data for the column count_rech_3g_8 is lying around 0. Hence we are imputing the values with 0.\ncol_imp.append('count_rech_3g_8')\nval_imp.append(0)","9505f262":"telecom['av_rech_amt_data_8'].describe()","cf7cbefb":"# It seems if the column av_rech_amt_data_8 is null, the volume of the data used is 0. \n# Hence we will impute the column av_rech_amt_data_8 with 0.\n\nprint(telecom[telecom['av_rech_amt_data_8'].isna()]['vol_2g_mb_8'].value_counts())\nprint(telecom[telecom['av_rech_amt_data_8'].isna()]['vol_3g_mb_8'].value_counts())","4aa7e32c":"# Iputing the column av_rech_amt_data_8 with a zero value.\ncol_imp.append('av_rech_amt_data_8')\nval_imp.append(0)","795853ed":"telecom['max_rech_data_8'].describe()","edbbb548":"# dropping the column  max_rech_data_8 as it does not seem to be making much sense.\ndrop_col.append('max_rech_data_8')","601e4eb8":"telecom['total_rech_data_8'].describe()\n#This is being handled later","ed93a1b2":"telecom['arpu_3g_8'].describe()","f7da5e28":"# Below lines will help in imputing the column arpu_3g_8 with it's median value.\n\ncol_imp.append('arpu_3g_8')\nval_imp.append(telecom['arpu_3g_8'].median())","d7112d86":"telecom['arpu_2g_8'].describe()","eb42f595":"# Below lines will help in imputing the column arpu_2g_8 with it's median value.\n\ncol_imp.append('arpu_2g_8')\nval_imp.append(telecom['arpu_2g_8'].median())","f37345b8":"telecom['night_pck_user_8'].describe()","e4dc7661":"plt.figure(figsize=(3,5))\nsns.countplot(telecom['night_pck_user_8'])\nplt.show()","71f5ac33":"# Here we have added the column to the col_imp list and the value also to the val_imp list. \n# So, that they an later on be imputed.\ncol_imp.append('night_pck_user_8')\nval_imp.append(telecom['night_pck_user_8'].median())","fe1d6d1c":"telecom['fb_user_8'].value_counts()","c1b1debe":"plt.figure(figsize=(3,5))\nsns.countplot(telecom['fb_user_8'])\nplt.show()","e1806393":"# Understanding the relation of fb_user_8, when it is null. What happens to the column vol_2g_mb_8. This will help us in \n# indentifying the value with which we can impute the column fb_user_8\n\nprint(telecom[telecom['fb_user_8'].isna()]['vol_2g_mb_8'].value_counts())\nprint(telecom[telecom['fb_user_8'].isna()]['vol_3g_mb_8'].value_counts())","8b8653af":"# imputing the column vol_2g_mb_8 with a zero value.\ncol_imp.append('fb_user_8')\nval_imp.append(0)","c9abdbd5":"telecom['count_rech_2g_9'].describe()","95901968":"plt.figure(figsize=(15,5))\nsns.countplot(telecom['count_rech_2g_9'])\nplt.show()","509b6d67":"# hre We have added the column to the col_imp list and the value also to the val_imp list. SO, that they an later on be imputed.\n\ncol_imp.append('count_rech_2g_9')\nval_imp.append(0)","b3140beb":"telecom['count_rech_3g_9'].describe()","dcf98cf2":"plt.figure(figsize=(15,5))\nsns.countplot(telecom['count_rech_3g_9'])\nplt.show()","b06b1ac9":"# Here we are adding the column count_rech_3g_9 to a list and the value 0 to a value list. They can be used for imputing.\n\ncol_imp.append('count_rech_3g_9')\nval_imp.append(0)","6e39b281":"telecom['av_rech_amt_data_9'].describe()","5630c873":"# Understanding the relation between av_rech_amt_data_9 and vol_2g_mb_9 and then imputing the column av_rech_amt_data_9.\n\nprint(telecom[telecom['av_rech_amt_data_9'].isna()]['vol_2g_mb_9'].value_counts())\nprint(telecom[telecom['av_rech_amt_data_9'].isna()]['vol_3g_mb_9'].value_counts())","fd0de559":"# Imputing the column av_rech_amt_data_9 with a value 0.\ncol_imp.append('av_rech_amt_data_9')\nval_imp.append(0)","4344fd79":"telecom['max_rech_data_9'].describe()","80d40983":"# Dropping the column max_rech_data_9 as this does not seem to be making sense.\n\ndrop_col.append('max_rech_data_9')","1be6a3ac":"telecom['total_rech_data_9'].describe()","431ea165":"telecom['arpu_3g_9'].describe()","3bf144e9":"# Imputing the column arpu_3g_9 with it's median.\n\ncol_imp.append('arpu_3g_9')\nval_imp.append(telecom['arpu_3g_9'].median())","4890e145":"telecom['arpu_2g_9'].describe()","0b66767b":"# Imputing the column arpu_2g_9 with it's median.\n\ncol_imp.append('arpu_2g_9')\nval_imp.append(telecom['arpu_2g_9'].median())","713129c3":"telecom['night_pck_user_9'].describe()","4e997670":"plt.figure(figsize=(3,5))\nsns.countplot(telecom['night_pck_user_9'])\nplt.show()","70a6679f":"# Imputing the column night_pck_user_9 with it's median.\n\ncol_imp.append('night_pck_user_9')\nval_imp.append(telecom['night_pck_user_9'].median())","3072f540":"telecom['fb_user_9'].value_counts()","af392c9e":"plt.figure(figsize=(3,5))\nsns.countplot(telecom['fb_user_9'])\nplt.show()","e616a193":"# Understand the relation between fb_user_9 and vol_2g_mb_9. \n\nprint(telecom[telecom['fb_user_9'].isna()]['vol_2g_mb_9'].value_counts())\nprint(telecom[telecom['fb_user_9'].isna()]['vol_3g_mb_9'].value_counts())","ff441c1a":"# Imputing the column fb_user_9 with the value 0.\ncol_imp.append('fb_user_9')\nval_imp.append(0)","9e342e61":"# here we are passing the dataframe, col_imp and val_imp list to a function impute_col. Which will\n\ntelecom = impute_col(telecom,col_imp, val_imp)\n\ntelecom_copy_2 = telecom.copy(deep=True)","43e78711":"# filling all the total recharge data values in case they have NA with the sum of their respective month's 2g and 3g value.\ntelecom['total_rech_data_6'].fillna((telecom['count_rech_3g_6'] + telecom['count_rech_2g_6']), inplace = True)\ntelecom['total_rech_data_7'].fillna((telecom['count_rech_3g_7'] + telecom['count_rech_2g_7']), inplace = True)\ntelecom['total_rech_data_8'].fillna((telecom['count_rech_3g_8'] + telecom['count_rech_2g_8']), inplace = True)\ntelecom['total_rech_data_9'].fillna((telecom['count_rech_3g_9'] + telecom['count_rech_2g_9']), inplace = True)","8ca3ca20":"# checking all the columns which have null values between 70 and 80 %\nprint(sorted(check_null_columns(telecom,70,80)))","7601cf09":"# dropping all the date related columns\nprint(sorted(list(drop_col)))","a2261d64":"telecom = telecom.drop(drop_col, axis = 1)","80641e38":"# since we have removed all the date columns, and the other values have been imputed , hence now we have no null values\n# for any columns in the range 70 - 80 %\nprint(check_null_columns(telecom,70,80))","16809a0d":"# Similarly there is no column in the range 60 - 70 with Null values\nprint(check_null_columns(telecom,60,70))","6e5b055d":"# THere is no column in the range 50 - 60 with Null values also\nprint(check_null_columns(telecom,50,60))","eca70aef":"# checking for the maximum percentage of NA values in the columns\nprint(null_pct(telecom))","5839e319":"# fetching the null values list of columns which lies in the range 7 - 10.\nt1 = check_null_columns(telecom,7,10)\nprint(sorted(t1))","96c2f107":"telecom = telecom.drop(t1, axis = 1)","2d89e0f2":"# Again checking for maximum null valuesin the dataframe.\nprint(null_pct(telecom))","81ea28b5":"'''\n    Here we still have lot of columns which have null values. Hence we will split the range intervals further\n'''\nprint(list(telecom.columns))","eb68ca5d":"'''\n    The null range interval is 4-6. Here we will get a list of all those columns which have null values\n'''\nt1 = check_null_columns(telecom,4,6)\nprint(sorted(t1))","ae73fb58":"telecom[t1].describe()","68d7e75f":"# impute all the values with the respective median. Almost all the columns have outliers, and imputing the null values with mean\n# will distort the interpretations and modeling\n\ncol_imp=[]\nval_imp=[]\ncol_imp.extend(['ic_others_8', 'isd_ic_mou_8', 'isd_og_mou_8', 'loc_ic_mou_8', 'loc_ic_t2f_mou_8', 'loc_ic_t2m_mou_8', \n                'loc_ic_t2t_mou_8', 'loc_og_mou_8', 'loc_og_t2c_mou_8', 'loc_og_t2f_mou_8', 'loc_og_t2m_mou_8', \n                'loc_og_t2t_mou_8', 'offnet_mou_8', 'og_others_8', 'onnet_mou_8', 'roam_ic_mou_8', 'roam_og_mou_8', \n                'spl_ic_mou_8', 'spl_og_mou_8', 'std_ic_mou_8', 'std_ic_t2f_mou_8', 'std_ic_t2m_mou_8', 'std_ic_t2o_mou_8', \n                'std_ic_t2t_mou_8', 'std_og_mou_8', 'std_og_t2c_mou_8', 'std_og_t2f_mou_8', 'std_og_t2m_mou_8', \n                'std_og_t2t_mou_8'])\n\n'''\n    In the below for loop we are imputing all the remaining columns with their median values. Also, we are not doing any\n    kind of imputation if the column ends with _9 i.e. if the column is for Septembet month here.\n'''\n\nfor i in col_imp:\n    if \"_9\" in i:\n        continue\n    val_imp.append(telecom[i].median())","40d017d8":"'''\n    Here, we are passing the lists created in the above cell, containing columns and their median values. With this we can\n    get all the columns imputed with their median vlaues.\n'''\ntelecom = impute_col(telecom,col_imp, val_imp)","70100c32":"'''\n    Now, we donot see any null values in the range 4-6\n'''\nt1 = check_null_columns(telecom, 4, 6)\nprint(t1)","4cd436e0":"t1 = check_null_columns(telecom, 3, 4)\nprint(t1)","4c25681c":"# impute all the values with the respective median. Almost all the columns have outliers, and imputing the null values with mean\n# will distort the interpretations and modeling\n\ncol_imp=[]\nval_imp=[]\ncol_imp.extend(['loc_ic_t2f_mou_6', 'std_og_t2c_mou_6', 'roam_og_mou_6', 'std_og_mou_6', 'std_ic_t2m_mou_6', 'loc_og_t2t_mou_6',\n                'std_ic_mou_6', 'loc_og_t2m_mou_6', 'loc_og_t2f_mou_6', 'roam_ic_mou_6', 'std_og_t2f_mou_6', 'std_ic_t2f_mou_6',\n                'loc_og_t2c_mou_6', 'loc_og_mou_6', 'std_og_t2m_mou_6', 'loc_ic_t2t_mou_6', 'std_ic_t2o_mou_6', \n                'std_og_t2t_mou_6', 'isd_og_mou_6', 'isd_ic_mou_6', 'spl_ic_mou_6', 'offnet_mou_6', 'std_ic_t2t_mou_6', \n                'spl_og_mou_6', 'onnet_mou_6', 'loc_ic_t2m_mou_6', 'og_others_6', 'loc_ic_mou_6', 'ic_others_6', 'spl_ic_mou_7',\n                'std_og_t2m_mou_7', 'std_ic_t2o_mou_7', 'std_og_t2f_mou_7', 'isd_ic_mou_7', 'ic_others_7', 'std_ic_mou_7', \n                'spl_og_mou_7', 'std_og_t2c_mou_7', 'isd_og_mou_7', 'std_og_mou_7', 'std_ic_t2f_mou_7', 'std_og_t2t_mou_7', \n                'std_ic_t2m_mou_7', 'loc_ic_t2f_mou_7', 'loc_ic_t2m_mou_7', 'loc_ic_mou_7', 'onnet_mou_7', 'offnet_mou_7', \n                'std_ic_t2t_mou_7', 'loc_og_mou_7', 'roam_og_mou_7', 'loc_og_t2t_mou_7', 'roam_ic_mou_7', 'og_others_7', \n                'loc_ic_t2t_mou_7', 'loc_og_t2c_mou_7', 'loc_og_t2m_mou_7', 'loc_og_t2f_mou_7'])\n'''\n    In the below for loop we are imputing all the columns with their median values. Also, we are not doing any\n    kind of imputation if the column ends with _9 i.e. if the column is for Septembet month here.\n'''\n\nfor i in col_imp:\n    if \"_9\" in i:\n        continue\n    val_imp.append(telecom[i].median())","d834ce4a":"'''\n    Here, we are passing the lists created in the above cell, containing columns and their median values. With this we can\n    get all the columns imputed with their median vlaues.\n'''\n\ntelecom = impute_col(telecom,col_imp, val_imp)","aa4cb8b4":"'''\n    Even in the range 0.1 -3 we find few columns with the null vlaues.\n'''\nt1 = check_null_columns(telecom, 0.1, 3)\nprint(t1)","619c4143":"col_imp=[]\nval_imp=[]\ncol_imp=['loc_og_t2o_mou', 'std_og_t2o_mou', 'loc_ic_t2o_mou'] # the other columns in the above list are date columns, \n                                                                # and therfore not being included\n\n'''\n    In the below for loop we are imputing all the remaining columns with their median values. Also, we are not doing any\n    kind of imputation if the column ends with _9 i.e. if the column is for Septembet month here.\n'''\n\nfor i in col_imp:\n    if \"_9\" in i:\n        continue\n    val_imp.append(telecom[i].median())","537c621c":"'''\n    Here, we are passing the lists created in the above cell, containing columns and their median values. With this we can\n    get all the columns imputed with their median vlaues.\n'''\n\ntelecom = impute_col(telecom,col_imp, val_imp)\nprint(telecom.shape)","9ca4a468":"'''\n    Checking if, there are still columns left for imputation.\n'''\nprint(null_pct(telecom))","a77bfe34":"'''\n    Since, we have only date columns left with Null values. So, we will be removing them\n'''\ntelecom = telecom.drop(['date_of_last_rech_9', 'date_of_last_rech_8', 'date_of_last_rech_7', 'last_date_of_month_9', \n                         'date_of_last_rech_6', 'last_date_of_month_8', 'last_date_of_month_7'], axis = 1)","96700130":"# checking the null percentage between 0.1 and 100\nprint(check_null_columns(telecom,0.1,100))","5a1aed2d":"# checking if any columns in the data have null values at all\nprint(null_pct(telecom))","ae684cce":"telecom['rech_amt_avg_6_7'] = (telecom['total_rech_amt_6'] + telecom['total_rech_amt_7'])\/2\ntelecom[['total_rech_amt_6', 'total_rech_amt_7', 'rech_amt_avg_6_7']].head(10)","742d19d1":"temp = telecom['rech_amt_avg_6_7'].quantile(0.70)\ntelecom_HVC = telecom[telecom['rech_amt_avg_6_7'] > temp ]\ntelecom_HVC.shape","c7d9c54d":"telecom_HVC['temp_col'] = (telecom_HVC['total_ic_mou_9'] + telecom_HVC['total_og_mou_9'] + telecom_HVC['vol_2g_mb_9'] + telecom_HVC['vol_3g_mb_9'])\ntelecom_HVC['churn'] = telecom_HVC['temp_col'].apply(lambda x: 1 if x == 0 else 0 )\ntelecom_HVC = telecom_HVC.drop('temp_col', axis = 1)","8d7e811f":"# looking at the churn value in the dataset\ntelecom_HVC['churn'].head()","a035449e":"t1 = telecom_HVC[['arpu_6','arpu_7', 'arpu_8', 'churn']]\nsns.pairplot(t1)\nplt.show()","baf36fa1":"t1 = telecom_HVC[['total_rech_num_6','total_rech_num_7', 'total_rech_num_8', 'churn']]\nsns.pairplot(t1)\nplt.show()","95011aa5":"# identifying the columns that belong to September. These end with _9\ndrop_9_columns = telecom.filter(regex='_9')\nprint(telecom.drop(list(drop_9_columns.columns),axis=1,inplace=True))","21119777":"drop_9_columns.columns","f2e504c7":"# Creating a new column as the difference of the avg values in \"good phase\" and values in \"action phase\"\n\ntelecom_HVC['avg_rech_amt_diff'] = (telecom['total_rech_amt_6'] + telecom_HVC['total_rech_amt_7'])\/2 - telecom['total_rech_amt_8']\ntelecom_HVC['avg_rech_num_diff'] = (telecom['total_rech_num_6'] + telecom_HVC['total_rech_num_7'])\/2 - telecom['total_rech_num_8']\ntelecom_HVC['avg_og_mou_diff'] = (telecom['total_og_mou_6'] + telecom_HVC['total_og_mou_7'])\/2 - telecom['total_og_mou_8']\ntelecom_HVC['max_rech_amt_diff'] = (telecom_HVC['max_rech_amt_6']+telecom_HVC['max_rech_amt_7'])\/2 - telecom_HVC['max_rech_amt_8']\ntelecom_HVC['avg_vbc_3g_diff'] = (telecom_HVC['vbc_3g_6']+telecom_HVC['vbc_3g_7'])\/2 - telecom_HVC['vbc_3g_8']\n\n# Age of customer on the network is given. \n# This can be used to derive the loyalty status of the customer\ntelecom_HVC['loyalty_temp'] = round(telecom_HVC['aon']\/365)","5e92a777":"telecom_HVC['loyalty_temp'].head(10)","8c8783ef":"telecom_HVC['loyalty_temp'].value_counts()","1d949079":"# creating a categoricalderived feature termed as loyalty.\n\nbins = [0,4,8,13]\nlabels = ['not loyal','loyal','very loyal']\ntelecom_HVC['loyalty'] = pd.cut(telecom_HVC['loyalty_temp'], bins=bins, labels=labels)","4d4e9847":"telecom_HVC['loyalty'].head(10)","25879c0d":"telecom_HVC['loyalty'].value_counts()","7b9c8214":"print(list(telecom_HVC.columns))","e1ed948e":"print(\"\\nLoaylty split for non-churned customers\")\nprint(telecom_HVC[telecom_HVC['churn']==0]['loyalty'].value_counts())\nprint(\"\\nLoaylty split for churned customers\")\nprint(telecom_HVC[telecom_HVC['churn']==1]['loyalty'].value_counts())","00b60009":"sns.catplot(x = \"loyalty\", y =\"churn\",kind = \"violin\", data=telecom_HVC)\nplt.show()","408749c3":"drop_9_columns = telecom_HVC.filter(regex='_9')\nprint(telecom_HVC.drop(list(drop_9_columns.columns),axis=1,inplace=True))\n\ndrop_9_columns = telecom_HVC.filter(regex='last_date_of_month')\nprint(telecom_HVC.drop(list(drop_9_columns.columns),axis=1,inplace=True))\ntelecom_HVC.head()","b1708200":"t_dummies_df = pd.get_dummies(telecom_HVC['loyalty'], drop_first = True)\n\ntelecom_HVC = pd.concat([telecom_HVC, t_dummies_df], axis = 1)\ntelecom_HVC = telecom_HVC.drop(['loyalty', 'loyalty_temp'], axis = 1)\ntelecom_HVC.shape","a2990c01":"telecom_HVC.head()","0f0c6c44":"'''\n\n    Here, we pass a dataframe and all the columns for which we want to check for outliers.\n    Post execution of the function we get a dataframe, which has the outliers removed.\n    \n'''\n\ndef remove_outliers(df,col):\n    for i in col:\n        Q1 = df[i].quantile(0.05)\n        Q3 = df[i].quantile(0.995)\n        df = df[(df[i] >=Q1) &(df[i] <=Q3)]\n    return df","8ea837f9":"df_no_outlier = remove_outliers(telecom_HVC, list(telecom_HVC.columns))\ndf_no_outlier.shape","867ed7a2":"# creating a copy of the dataframe to use it for identifying the number Principal components required\ntelecom_HVC_copy = telecom_HVC.copy(deep = True)","10728590":"# creating an index column with the mobile number as its value\ny_ind = telecom_HVC_copy.pop('mobile_number')","3c9367dc":"# storing the target column in the variable \"y\"\ny = telecom_HVC_copy.pop('churn')","69d8246e":"# instantiating an object of the class StandardScaler\nscaler = StandardScaler()\n\n# fitting and transforming the dataset\ntelecom_HVC_scaled = scaler.fit_transform(telecom_HVC_copy)\n\ntelecom_HVC_scaled.shape","6a98bfac":"t_HVC_scaled = pd.DataFrame(telecom_HVC_scaled)\nt_HVC_scaled.columns = telecom_HVC_copy.columns\nt_HVC_scaled.head()","f3dfcc8b":"# All the relevant libraries were imported in the beginning\n# instantiating an object of the class PCA\npca = PCA(random_state=42)\n\n# fitting on the scaled dataset\npca.fit(t_HVC_scaled)","adc0f50c":"# Taking a look at the resultant PCA components\npca.components_","ce600906":"# taking a look at the explained variance ratio by the Principal components\npca.explained_variance_ratio_","3526a41a":"# plotting the explained variance ratio \nplt.bar(range(1,len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\nplt.show()","c75d032d":"var_cumu = np.cumsum(pca.explained_variance_ratio_)\nvar_cumu","13ad3718":"plt.figure(figsize=(10,8))\nplt.vlines(x = 75, ymax=1, ymin=0.1, colors=\"r\", linestyles=\"--\")\nplt.vlines(x = 55, ymax=1, ymin=0.1, colors=\"b\", linestyles=\"--\")\nplt.hlines(y = 0.90, xmax=120, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.hlines(y = 0.96, xmax=120, xmin=0, colors=\"b\", linestyles=\"--\")\nplt.plot(var_cumu)\nplt.ylabel(\"Cumulative variance explained\")\nplt.show()","c99804a2":"# creating a PCA object for 55 PCs now\npca_final = IncrementalPCA(n_components = 55)\n\n# this PCA object will now be used to fit and transform the scaled country dataset\n# when transforming is done, the inverse of the PCs is multiplied with the scaled dataset and the result is the corresponding values \n# of the scaled dataset with the PCs as the new basis\nnew_var = pca_final.fit_transform(t_HVC_scaled)\nnew_var","e43dce2f":"# To work further with the new dataset, a dataframe needs to be created\n\ncolumns=[]\nn_components = 55\nfor i in range(1,n_components+1):\n    columns.append(\"PC\"+str(i))\n\n\ndf = pd.DataFrame(new_var, columns =columns)\ndf.head()","c73166a3":"cols = list(telecom_HVC_copy.columns)\ntemp = pd.DataFrame({'PC1':pca_final.components_[0], 'PC2':pca_final.components_[1],\n                     'PC3':pca_final.components_[2], 'PC4':pca_final.components_[3],\n                     'PC5':pca_final.components_[4], 'Feature':cols})\ntemp.head()","04d6ed24":"df_train, df_test = train_test_split(telecom_HVC, test_size=0.2, random_state = 100)","18c37ce3":"# looking athe counts of the \"churn\" in the dataset\ntelecom_HVC['churn'].value_counts()","dce46cb5":"# calculating the imbalance percentage in the dataset\nimbal = round((telecom_HVC['churn'] == 1).sum()\/len(telecom_HVC) * 100,2)\nimbal","e47680fa":"# Separate majority and minority classes\nt_majority = df_train[df_train.churn==0]\nt_minority = df_train[df_train.churn==1]\n \n# Upsample minority class\nt_minority_upsampled = resample(t_minority, \n                                 replace=True,     # sample with replacement\n                                 n_samples=len(t_majority),    # to match majority class\n                                 random_state=123) # reproducible results\n \n# Combine majority class with upsampled minority class\nt_upsampled = pd.concat([t_majority, t_minority_upsampled])\n \n# Display new class counts\nt_upsampled.churn.value_counts()","9816472e":"df_train = t_upsampled.copy(deep = True)\ny_train = df_train.pop('churn')\ny_ind = df_train.pop('mobile_number')\nX_train = df_train.copy(deep=True)","8a245eea":"y_test = df_test.pop('churn')\ny_test_ind = df_test.pop('mobile_number')\nX_test = df_test.copy(deep = True)","fcdb8185":"X_train_c = X_train.copy(deep = True)\nX_test_c = X_test.copy(deep = True)\ny_train_c = y_train.copy(deep = True)\ny_test_c = y_test.copy(deep = True)","7a9dc0a7":"scaler = StandardScaler() \n  \nX_train = scaler.fit_transform(X_train) \nX_test = scaler.transform(X_test) ","0428ff96":"pca_final = PCA(n_components = 55) \n\n\nX_train_pca = pca_final.fit_transform(X_train)\nX_test_pca = pca_final.transform(X_test) \n  \nexplained_variance = pca_final.explained_variance_ratio_ ","8113e0ef":"# Fitting Logistic Regression To the training set \n\nlr_pca = LogisticRegression(random_state = 0) \nlr_pca.fit(X_train_pca, y_train) ","8d22fcbb":"y_train.head()","75e96a7c":"t = pd.DataFrame(y_train)\nt.columns = ['churn']\nt['churn'].value_counts()","98cc8416":"t1 = pd.DataFrame(y_test)\nt1.columns = ['churn']\nt1['churn'].value_counts()","d15a755e":"y_train_pred_pca = lr_pca.predict(X_train_pca) ","16686e8d":"# Predicting the test set result using  \n# predict function under LogisticRegression  \n\nlr_y_test_pred_pca = lr_pca.predict(X_test_pca) ","0ee7b39e":"y_train_pred_pca = pd.DataFrame(y_train_pred_pca)\ny_train_pred_pca.head()","e23e7b9e":"lr_y_test_pred_pca = pd.DataFrame(lr_y_test_pred_pca)\nlr_y_test_pred_pca.columns = ['churn']\nlr_y_test_pred_pca.head()","23448fe7":"# making confusion matrix between train set of Y and predicted value. \n\ncm_train_lr_pca = confusion_matrix(y_train, y_train_pred_pca) \ncm_train_lr_pca","4815e859":"# making confusion matrix between the test set of Y and predicted value\n  \ncm_test_lr_pca = confusion_matrix(y_test, lr_y_test_pred_pca) \ncm_test_lr_pca","842e91db":"'''\n    This function takes the confusion metrics as an input and then prints -:\n        1) Accuracy\n        2) Sensitivity\/Recall\/TPR\n        3) Specificity\n        4) FPR and Precision for the same.\n\n'''\n\ndef calculate_all_metrics(confusion):\n    \n    print(\"Confusion matrix obtained is \\n{val}\".format(val=confusion))\n    \n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    \n    print(\"\\nAccuracy score obtained is {val}%\".format(val=round(((TP+TN)\/(TP+TN+FP+FN))*100,2)))\n    print(\"\\nSensitivity\/Recall\/True Positive Rate for the above confusion matrix obtained is = {val}%\".format(val=round((TP\/(TP+FN))*100,2)))\n    print(\"\\nSpecificity for the above confusion matrix obtained is = {val}%\".format(val=round((TN\/(TN+FP))*100,2)))\n    print(\"\\nFalse Positive Rate for the above confusion matrix obtained is = {val}%\".format(val=round((FP\/(TN+FP))*100,2)))\n    print(\"\\nPrecision for the above confusion matrix obtained is = {val}%\".format(val=round((TP\/(TP+FP))*100,2)))","903b5172":"# evaluating all the metrics(sensitivity\/specificity\/precision\/recall\/TPR\/FPR) for cm_train_lr_pca\ncalculate_all_metrics(cm_train_lr_pca)","876b87c6":"# evaluating all the metrics(sensitivity\/specificity\/precision\/recall\/TPR\/FPR) for cm_test_lr_pca\ncalculate_all_metrics(cm_test_lr_pca)","7aedef22":"X_train_pca_dt = pd.DataFrame(X_train_pca)\nX_test_pca_dt = pd.DataFrame(X_test_pca) \n\n\n# Fitting the decision tree with default hyperparameter max_depth is 3;\ndt_pca_1 = DecisionTreeClassifier(max_depth = 3)\ndt_pca_1.fit(X_train_pca, y_train)","cab5338b":"# Putting features\nfeatures = list(X_train_pca_dt.columns[0:])","1f2706f0":"# Making predictions\ndt_y_test_pred_pca_1 = dt_pca_1.predict(X_test_pca_dt)\n\n# Printing classification report\nprint(classification_report(y_test, dt_y_test_pred_pca_1))","c34d5ba0":"'''\n    \n    Here, we take a decision tree and its features as an input. This function returns the image of a decision tree\n    which is helpful from the analysis perspective.\n\n'''\ndef draw_decision_tree(dt,features):\n    dot_data = io.StringIO()  \n    export_graphviz(dt, out_file=dot_data,\n                feature_names=features, filled=True,rounded=True)\n\n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    return(Image(graph.create_png()))","41b76513":"# evaluating all the metrics(sensitivity\/specificity\/precision\/recall\/TPR\/FPR) for cm_pca_dt_1\n\ncm_pca_dt_1 = confusion_matrix(y_test,dt_y_test_pred_pca_1)\ncalculate_all_metrics(cm_pca_dt_1)","d8de813d":"'''\n    here we have changed a few hyperparameters to understand how\n    the confusion matrix and the other evaluation metrics change\n'''\ndt_pca_2 = DecisionTreeClassifier(max_depth=7,min_samples_split=200)\ndt_pca_2.fit(X_train_pca, y_train)","69eb8852":"# Making predictions\ndt_y_test_pred_pca_2 = dt_pca_2.predict(X_test_pca_dt)\n# Printing classification report\nprint(classification_report(y_test, dt_y_test_pred_pca_2))","2d20bd2c":"# evaluating all the metrics(sensitivity\/specificity\/precision\/recall\/TPR\/FPR) for cm_pca_dt_2\n\ncm_pca_dt_2 = confusion_matrix(y_test,dt_y_test_pred_pca_2)\ncalculate_all_metrics(cm_pca_dt_2)","ca278825":"'''\n    here we have changed a few hyperparameters to understand how\n    the confusion matrix and the other evaluation metrics change\n'''\ndt_pca_3 = DecisionTreeClassifier(max_depth=5,min_samples_split=150,min_samples_leaf=25,random_state=100,max_leaf_nodes=15,\n                                 criterion='entropy')\ndt_pca_3.fit(X_train_pca, y_train)","56249dad":"# Making predictions\ndt_y_test_pred_pca_3 = dt_pca_3.predict(X_test_pca_dt)\n# Printing classification report\nprint(classification_report(y_test, dt_y_test_pred_pca_3))","37d30276":"# evaluating all the metrics(sensitivity\/specificity\/precision\/recall\/TPR\/FPR) for cm_pca_dt_3\n\ncm_pca_dt_3 = confusion_matrix(y_test,dt_y_test_pred_pca_3)\ncalculate_all_metrics(cm_pca_dt_3)","5501565c":"'''\n    Plotting the simple decision tree with a depth 4, the first Decision Tree\n\n'''\ndraw_decision_tree(dt_pca_1,features)","1a64c94c":"'''\n    Plotting a complex decision tree with depth 7, the second Decision Tree\n\n'''\ndraw_decision_tree(dt_pca_2,features)","e160cb52":"'''\n    \n    Plotting a complex decision tree with depth 5, the third Decision Tree.\n\n'''\ndraw_decision_tree(dt_pca_3,features)","1be4da77":"# Running the random forest with default parameters.\nrfc = RandomForestClassifier()","8bd3b3f9":"# fit\nrfc.fit(X_train_pca,y_train)","1415c54f":"# Making predictions for train dataset\npredictions_train = rfc.predict(X_train_pca)\nprint(classification_report(y_train,predictions_train))","a1c4a9a6":"# Making predictions for test dataset\nrf_y_test_pred_pca_1 = rfc.predict(X_test_pca)","77dbd783":"# Let's check the report of our default model\nprint(classification_report(y_test, rf_y_test_pred_pca_1))","9bd8e9e9":"cm_rfc_pca = confusion_matrix(y_test, rf_y_test_pred_pca_1)\ncalculate_all_metrics(cm_rfc_pca)","e95d7c1d":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5, 10]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)","e3b142cf":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)","922202c0":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","9d309f3d":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             max_features=10,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             n_estimators=200)","474bcc6e":"# fiting the model\nrfc.fit(X_train,y_train)","85de710c":"# predict\nrf_y_test_pred_pca_2 = rfc.predict(X_test)","cb3c5304":"print(classification_report(y_test, rf_y_test_pred_pca_2))","9f97262d":"cm_rfc_pca_2 = confusion_matrix(y_test, rf_y_test_pred_pca_2)","8f3cbd66":"calculate_all_metrics(cm_rfc_pca_2)","3a759ffe":"'''\n    This method takes 4 parameters -\n    1. The actual value of the target variable\n    2. The predicted value of the target variable from the first model\n    3. The predicted value of the target variable from the second model\n    4 The predicted value of the target variable from the third model\n    \n    This method then plots thres ROC curves, one for each model, b\/w the actual taregt value and the predicted value\n'''\ndef draw_roc_compare( actual, prob1, prob2, prob3):\n    fpr1, tpr1, thresholds1 = metrics.roc_curve( actual, prob1, drop_intermediate = False )\n    fpr2, tpr2, thresholds2 = metrics.roc_curve( actual, prob1, drop_intermediate = False )\n    fpr3, tpr3, thresholds3 = metrics.roc_curve( actual, prob1, drop_intermediate = False )\n    auc_score1 = metrics.roc_auc_score( actual, prob1)\n    auc_score2 = metrics.roc_auc_score( actual, prob2)\n    auc_score3 = metrics.roc_auc_score( actual, prob3)\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr1, tpr1, label='Model 1 (area = %0.2f)' % auc_score1, color = \"r\" )\n    plt.plot( fpr2, tpr2, label='Model 2 (area = %0.2f)' % auc_score2, color = \"b\" )\n    plt.plot( fpr3, tpr3, label='Model 3 (area = %0.2f)' % auc_score3, color = \"g\" )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","7871d8e6":"draw_roc_compare(pd.DataFrame(y_test)['churn'] , pd.DataFrame(dt_y_test_pred_pca_1). iloc[:, 0],pd.DataFrame(dt_y_test_pred_pca_2). iloc[:, 0], pd.DataFrame(dt_y_test_pred_pca_3). iloc[:, 0])","dca6e811":"draw_roc_compare(pd.DataFrame(y_test)['churn'] , lr_y_test_pred_pca['churn'],pd.DataFrame(dt_y_test_pred_pca_3). iloc[:, 0],pd.DataFrame(rf_y_test_pred_pca_2). iloc[:, 0])","f5ccf880":"# creating X_train, y_train, X_test and y_test from the respective copies created earlier\nX_train = X_train_c.copy(deep = True)\nX_test = X_test_c.copy(deep = True)\ny_train = y_train_c.copy(deep = True)\ny_test = y_test_c.copy(deep = True)","5ddd31c9":"scaler = StandardScaler() \n\ncols = X_train.columns\nX_train[cols] = scaler.fit_transform(X_train[cols]) \nX_test[cols] = scaler.transform(X_test[cols]) ","8eebf738":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","a09a33f0":"logreg = LogisticRegression()\nrfe = RFE(logreg, 45)\nrfe = rfe.fit(X_train, y_train)\nrfe.support_","d76c5b55":"col = X_train.columns[rfe.support_]","fc7deb78":"X_train.columns[~rfe.support_]","24014785":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","b96919f4":"# calculate the VIF \n\nvif = pd.DataFrame()\nvif['Features'] = X_train_sm.columns\nvif['VIF'] = [variance_inflation_factor(X_train_sm.values,i) for i in range(X_train_sm.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","38724517":"'''\n    This function takes, a dataframe and its columns as input and removes all columns one by one having VIF greater than 5.\n    It returns a dataframe at the end, where in the multicollinearity is removed.\n    \n'''\n\ndef calculate_vif(df,col):\n    vif = pd.DataFrame()\n    X = df[col]\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    if vif.iloc[0][1] < 5 :\n        return X\n    else:\n        Y = X.drop(vif.iloc[0][0], axis =1)\n        print(\"column dropped as part of VIF is  {val}\".format(val = vif.iloc[0][0]))\n        return(calculate_vif(X,Y.columns))","12243c50":"df_num = np.array(X_train[col].select_dtypes(include=[np.number]).columns.values)\ndf_vif = calculate_vif(X_train[col], df_num)","0776e784":"len(df_vif.columns)","ee7b2a40":"# calculate the VIF again\n\nvif = pd.DataFrame()\nvif['Features'] = df_vif.columns\nvif['VIF'] = [variance_inflation_factor(df_vif.values,i) for i in range(df_vif.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5532adf6":"X_train_sm_2 = sm.add_constant(df_vif)\nlogm2 = sm.GLM(y_train,X_train_sm_2, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","c2667f74":"df_vif = df_vif.drop(\"ic_others_8\", axis = 1)\nX_train_sm_2 = sm.add_constant(df_vif)\nlogm2 = sm.GLM(y_train,X_train_sm_2, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","fd5d8fe0":"df_vif = df_vif.drop(\"max_rech_amt_diff\", axis = 1)\nX_train_sm_2 = sm.add_constant(df_vif)\nlogm2 = sm.GLM(y_train,X_train_sm_2, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","1e9a1b5f":"df_vif = df_vif.drop(\"std_ic_t2t_mou_7\", axis = 1)\nX_train_sm_2 = sm.add_constant(df_vif)\nlogm2 = sm.GLM(y_train,X_train_sm_2, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","be9ce555":"df_vif = df_vif.drop(\"loc_ic_t2t_mou_7\", axis = 1)\nX_train_sm_2 = sm.add_constant(df_vif)\nlogm2 = sm.GLM(y_train,X_train_sm_2, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","4c1e75a7":"y_train_pred_lr = res.predict(X_train_sm_2)\ny_train_pred_lr[:10]","44902005":"y_train_pred_lr = y_train_pred_lr.values.reshape(-1)\ny_train_pred_lr[:10]","fc7d1663":"y_train_array = np.array(y_train.values)\ny_train_array = y_train_array.reshape(-1)\ny_train_array[:10]","21097316":"y_ind = y_ind.reset_index()\ny_ind = y_ind.drop(\"index\", axis = 1)\ny_ind['mobile_number'].head()","47fabe02":"y_train_pred_final_lr = pd.DataFrame({'Churn':y_train_array, 'Churn_Prob':y_train_pred_lr})\ny_train_pred_final_lr['mobile_number'] = y_ind['mobile_number']\ny_train_pred_final_lr.head()","20e6da23":"y_train_pred_final_lr['predicted_Churn'] = y_train_pred_final_lr.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final_lr.head()","5fa706a7":"# Confusion matrix \ncm_lr_train_1 = metrics.confusion_matrix(y_train_pred_final_lr.Churn, y_train_pred_final_lr.predicted_Churn )","344bbc7a":"calculate_all_metrics(cm_lr_train_1)","88fce42b":"'''\n    this method takes two column values as parameters - 1. the actual target variable and \n    2. the predicted value of the target variable from the model built\n    \n    This method then plots a ROC curve between the two values\n'''\ndef draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","9dad9e80":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final_lr.Churn, y_train_pred_final_lr.Churn_Prob, \n                                         drop_intermediate = False )","ba3b6319":"#Finding Optimal Cutoff Point\n#Optimal cutoff probability is that prob where we get balanced sensitivity and specificity\ndraw_roc(y_train_pred_final_lr.Churn, y_train_pred_final_lr.Churn_Prob)","c1dc07eb":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final_lr[i]= y_train_pred_final_lr.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final_lr.head()","29522f96":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final_lr.Churn, y_train_pred_final_lr[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\ncutoff_df.head()","ac036d7b":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.vlines(x = 0.55, ymax=1, ymin=0.1, colors=\"b\", linestyles=\"--\")\nplt.vlines(x = 0.52, ymax=1, ymin=0.1, colors=\"r\", linestyles=\"--\")\nplt.show()","11fd35c7":"y_train_pred_final_lr['final_predicted'] = y_train_pred_final_lr.Churn_Prob.map( lambda x: 1 if x > 0.45 else 0)\ny_train_pred_final_lr.head()","4eaf6d05":"# Confusion matrix \ncm_lr_train_2 = metrics.confusion_matrix(y_train_pred_final_lr.Churn, y_train_pred_final_lr.final_predicted )","f2ccc5a6":"calculate_all_metrics(cm_lr_train_2)","3992a9c7":"X_train_sm_2.columns","e3096ae8":"X_train_sm_c = X_train_sm_2.copy(deep = True)\nX_train_sm_c = X_train_sm_c.drop('const',axis=1)\ncol = X_train_sm_c.columns\nX_test_sm = sm.add_constant(X_test[col])\ny_test_pred_lr = res.predict(X_test_sm)\ny_test_pred_lr[:10]","dec7e5ea":"y_test_pred_lr = y_test_pred_lr.values.reshape(-1)\ny_test_pred_lr[:10]","eaa1e476":"y_test_array = np.array(y_test.values)\ny_test_array = y_test_array.reshape(-1)\ny_test_array[:10]","2f31987b":"y_test_pred_final_lr = pd.DataFrame({'Churn':y_test_array, 'Churn_Prob':y_test_pred_lr})\ny_test_pred_final_lr['mobile_number'] = y_ind['mobile_number']\ny_test_pred_final_lr.head()","d732f9e3":"y_test_pred_final_lr['final_predicted'] = y_test_pred_final_lr.Churn_Prob.map( lambda x: 1 if x > 0.45 else 0)\ny_test_pred_final_lr.head()","d671a7e1":"cm_lr_test = metrics.confusion_matrix(y_test_pred_final_lr.Churn, y_test_pred_final_lr.final_predicted )\ncalculate_all_metrics(cm_lr_test)","b12f4af7":"# The final summary of the model\nres.summary()","da4ee0e0":"# Looking at the top 10 coefficients\nlr_coeffs = pd.DataFrame(res.params)\nlr_coeffs.reset_index(inplace = True)\nlr_coeffs.columns = ['Feature', 'coeff value']\ntop_10 = lr_coeffs.sort_values(['coeff value'], ascending = False).head(10)\ntop_10","adb95d70":"# Looking at the bottom 10 coefficients\nbot_10 = lr_coeffs.sort_values(['coeff value'], ascending = False).tail(10)\nbot_10","8e711ae1":"# Combining the coefficients to come up with the features that affect the model the most\nlr_coef = pd.concat([top_10, bot_10], axis=0, sort=False)\nlr_coef['absolute coeff'] = abs(lr_coef['coeff value'])\nlr_coef.sort_values(by = 'absolute coeff', ascending = False).head(10)","85d011a8":"# Plotting the coefficients\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\nsns.barplot(x = 'coeff value', y = 'Feature', data = lr_coef)\nplt.title(\"Telecom Churn Coefficients\")\nplt.grid()\nplt.show()","ee188bab":"# Fitting the decision tree with default hyperparameter max_depth = 3 \n# The Decision Tree model need not be given the scaled data. \n# Therefore, the paramters passed would be the original training dataset where data imbalance was handled\ndt_1 = DecisionTreeClassifier(max_depth = 3)\ndt_1.fit(X_train_c, y_train_c)","21f6030a":"# Importing required packages for visualization\nfrom IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nfrom sklearn.tree import export_graphviz\nimport pydotplus, graphviz\n\n# Putting features\nfeatures = list(X_train_c.columns[0:])\nfeatures","9c4c65e8":"# plotting tree with max_depth=3\ndot_data = StringIO()  \nexport_graphviz(dt_1, out_file=dot_data,\n                feature_names=features, filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n#graph.write_pdf(\"TelecomChurn.pdf\")","3357ac02":"# to draw it here, the following command can be used\ndraw_decision_tree(dt_1,features)","be39ec20":"# Making predictions\ndt_y_test_pred1 = dt_1.predict(X_test_c)\n\n# Printing classification report\nprint(classification_report(y_test_c, dt_y_test_pred1))","741687a0":"# evaluating all the metrics(sensitivity\/specificity\/precision\/recall\/TPR\/FPR) for cm_pca_dt_3\n\ncm_dt = confusion_matrix(y_test_c, dt_y_test_pred1)\ncalculate_all_metrics(cm_dt)","d5a2447c":"'''\n    here we have changed a few hyperparameters to understand how\n    the confusion matrix and the other evaluation metrics change\n'''\n\ndt_2 = DecisionTreeClassifier(max_depth=7,min_samples_split=200)\ndt_2.fit(X_train_c, y_train_c)","d14e2fdf":"# to draw it here, the following command can be used\ndraw_decision_tree(dt_2,features)","cc813833":"# Making predictions\ndt_y_test_pred2 = dt_2.predict(X_test_c)\n\n# Printing classification report\nprint(classification_report(y_test_c, dt_y_test_pred2))","207cb05e":"# evaluating all the metrics(sensitivity\/specificity\/precision\/recall\/TPR\/FPR) for cm_pca_dt_3\n\ncm_dt = confusion_matrix(y_test_c, dt_y_test_pred2)\ncalculate_all_metrics(cm_dt)","850b5bc7":"'''\n    here we have changed a few hyperparameters to understand how\n    the confusion matrix and the other evaluation metrics change\n'''\n\ndt_3 = DecisionTreeClassifier(max_depth=5,min_samples_split=150,min_samples_leaf=25,random_state=100,max_leaf_nodes=15,\n                                 criterion='entropy')\ndt_3.fit(X_train_c, y_train_c)","c050f1e7":"# to draw it here, the following command can be used\ndraw_decision_tree(dt_3,features)","c43309f5":"# Making predictions\ndt_y_test_pred3 = dt_3.predict(X_test_c)\n\n# Printing classification report\nprint(classification_report(y_test_c, dt_y_test_pred3))","30dc817a":"# evaluating all the metrics(sensitivity\/specificity\/precision\/recall\/TPR\/FPR) for cm_pca_dt_3\n\ncm_dt = confusion_matrix(y_test_c, dt_y_test_pred3)\ncalculate_all_metrics(cm_dt)","1ff4a071":"draw_roc_compare(pd.DataFrame(y_test_c)['churn'] , pd.DataFrame(dt_y_test_pred1). iloc[:, 0],pd.DataFrame(dt_y_test_pred2). iloc[:, 0],pd.DataFrame(dt_y_test_pred3). iloc[:, 0])","ed23c681":"# just rechecking the shapes of the dataset\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8342aa51":"# base estimator: a weak learner with max_depth = 9\nshallow_tree = DecisionTreeClassifier(max_depth = 9, random_state = 100)\n# fit the shallow decision tree \nshallow_tree.fit(X_train, y_train)\n\n# prediction on test dataset\ny_pred = shallow_tree.predict(X_test)\ncm_boosting_test_1 = confusion_matrix(y_test, y_pred)\ncalculate_all_metrics(cm_boosting_test_1)","d6a36eaf":"from sklearn.ensemble import AdaBoostClassifier\nestimators = list(range(1, 20, 2))\n\nabc_scores = []\nfor n_est in estimators:\n    ABC = AdaBoostClassifier(\n    base_estimator=shallow_tree, \n    n_estimators = n_est)\n    \n    ABC.fit(X_train, y_train)\n    y_pred = ABC.predict(X_test)\n    cm = confusion_matrix(y_test, y_pred) \n    TP = cm[1,1]\n    FN = cm[1,0]\n    abc_scores.append(round((TP\/(TP+FN))*100,2))\n    print(\"\\n\\nThe value for all the relevant metrics for number of estimators {} \\n\".format(n_est))\n    calculate_all_metrics(cm)","6a9b50ff":"# plot test scores and n_estimators\n\nplt.plot(estimators, abc_scores)\nplt.xlabel('n_estimators')\nplt.ylabel('Sensitivity')\n\nplt.show()","2152ed49":"# base estimator: a weak learner with max_depth = 11\nshallow_tree1 = DecisionTreeClassifier(max_depth = 11, random_state = 100)\n# fit the shallow decision tree \nshallow_tree1.fit(X_train_c, y_train_c)\n\n# test error\nboosting_y_pred = shallow_tree1.predict(X_test_c)\ncm = confusion_matrix(y_test_c, boosting_y_pred)\ncalculate_all_metrics(cm)","d7fdfa88":"draw_decision_tree(shallow_tree1,features)","8174a3f8":"draw_roc_compare(pd.DataFrame(y_test)['churn'] , y_test_pred_final_lr['final_predicted'],pd.DataFrame(dt_y_test_pred2). iloc[:, 0],pd.DataFrame(boosting_y_pred). iloc[:, 0])","699a26ef":"# The metrics of the best predictor RF model are \ncalculate_all_metrics(cm_rfc_pca_2)","f3cd094c":"# plotting the second decision tree and saving it as a pdf at the location same as the python notebook \ndot_data = StringIO()  \nexport_graphviz(dt_2, out_file=dot_data,feature_names=features, filled=True,rounded=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n#graph.write_pdf(\"TelecomChurn.pdf\")","886aad7e":"# to draw it here, the following command can be used\ndraw_decision_tree(dt_2,features)","d8842f20":"importances = dt_2.feature_importances_\nimportances","5d111225":"importance_dict = dict(zip(X_train.columns, dt_2.feature_importances_))","e18a0fbf":"# Getting the top 10 features of the decision tree\nlistofTuples = sorted(importance_dict.items() , reverse=True, key=lambda x: x[1])\ncnt = 0\nlst = []\n\n# Iterate over the sorted sequence\nfor elem in listofTuples :\n    lst.append([elem[0], elem[1]])\n\n#converting the list to dataframe\ndt_coef = pd.DataFrame(lst)\ndt_coef.columns = [\"Feature\", \"Weightage\"]\ndt_coef_top10 = dt_coef.head(10)\ndt_coef.head(10)","4416b86b":"# plotting the weightage\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\nsns.barplot(x = 'Weightage', y = 'Feature', data = dt_coef_top10)\nplt.title(\"Telecom Churn Coefficients\")\nplt.grid()\nplt.show()","074a7c88":"Inference - The Logistic Regression model performs decently well on the train data","54b666f9":"When 'fb_user_6' is null, it is evident that no volume of data has been used for 2g or 3g. Thefore, the null data should be imputed with 0","fad14409":"* Inference - As expected, after the hyperparameter tuning, the RandomForest model has done well on predicting the \"churned\" cases, while still being able to maintain the prediction on the \"non-churned\" ones. This model is certainly better than the first Random Forest model built\n\n* ROC Curve to compare the perfomance of the three models\n>       An ROC curve demonstrates several things:\n        * It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n        *  The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n        *  The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.\n        *  It can also be used to compare different models. The model with maximum value under the curve is the best model to predict the target variable. \n        \n*In the current assignment, since sensitivity (or True Positive Rate) is one of the crucial parameters, comparing the models based on ROC is a good idea","de6d8053":"As evident from the above output, this column has a lot of outliers, therefore the null data will be imputed by median because the mean will corrupt the results and interpretations","28001515":"# Data Understanding","9e805e94":"The above list of columns has null data more than 70% and lesser than 80%\n\nIt is worth observing the the same columns for the months June, July, August and September are present in the above list. These can be handled in a similar manner as detailed below","c38e136b":"# August data - the imputation would be carried out as it was done for the June data","83a82dd1":"The above graph shows that 90% of the variance can be explained by 55 principal components","b53662b1":"This column has just one value throughout the dataset, and therefore does not contribute in the customer behaviour analysis. This column can therefore be dropped","d05afe54":"Checking the churn data count in train and test datasets","18d052b0":"It was mentioned in the problem statement that after filtering the High Value Customers, we would be left with about 29.9k rows. The output of the above cell concurs with the problem statement\n\nDeriving the target variable \"churn\" using the attributes from September\nThe problem statement defines this derivation as: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\n        * total_ic_mou_9\n        * total_og_mou_9\n        * vol_2g_mb_9\n        * vol_3g_mb_9","2d5ac5e2":"* Inference - The sensitivity is lesser than the Logistic Regression Model. Some hyperparameter tuning might help better it. the accuracy and specificity are quite good\n* Creating another tree by tuning some hyperparameters","931d3579":"# July data - the imputation would be carried out as it was done for the June data","995a7929":"# Predicting the results on the test data","defddccc":"@Author: Garima and Tushar\n# Problem Statement\n* The customer data from June 2014 - September 2014 for the prepaid customers in the Asian Market of a Telecom operator is given. A customer is said to have churned when he leaves the network. Getting a new customer on-board the network is far more cumbersome than retaining an existing one. Therefore the Telecom operator wants us to analyse the data from June - August and predict the likelihood of the customer churning in September.\n* The data from June and July is considered to be \"good phase\" data, while the one from August is considered to be in \"action phase\". It is believed that the variation in customer behavior from good phase to action phase drives the cause for a customer to churn.\n* The business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months, and this needs to be done for the High-Value customers","d8f21130":"* As concluded in the Inferences above, the second and third Decision Trees have the same area under the ROC curve, but since the third Decision Tree had the best sensitivity, it will be used to compare the performance of the Logistic Regression and Random Forest Models.\n\n* Note: Since the values of area under ROC curve are quite close, all the three curves are not visible","093c3bd3":"* The above output gives the weightage of the various features in the Decision Tree. it is evident that the total minutes of incoming calls in the month of August has 56% weightage in determining the churn behaviour of the customer. This is followed by the total outgoing minutes of usage in roamin category for the month of August, followed by the total recharge amount in August.\n\n* A few inferences and recommendations based on the decision tree are present in the upcoming cells","bd99bde1":"# Step 7 - Predictive Modeling\n* Step 7.1 - Model 1 - Logistic regression Model using PCA\u00b6","3400b392":"As evident, the VIF of all the columns is now under accepatble limit. So we can proceed further with building the model.","56fbc4be":"The data has approximately 1 lakh customer records and stores information in 226 columns","f42e7e98":"When 'fb_user_7' is null, it is evident that no volume of data has been used for 2g or 3g. Thefore, the null data should be imputed with 0","f1df4855":"# Step 6 - Handling Data imbalance\n* Splitting the dataset into test and train datasets","ac1a5b45":"* As expected, the second decision tree gives the best results. Like in the earlier graphs, since the areas under the curve are similar, all the lines are not visible\n\n# Step 8.3 - Model 3 - Boosting without PCA","6c065b34":"As mentioned above, the cleaning of the columns would be handled month-wise.\n\n* June data - data will be looked at one column at a time to decide how to handle the null data","cbdd42f5":"From the graph, it is quite clear that not a lot of people recharged for a 3g pack in July\n\nContinuing with approach taken above, this column can be imputed with 0","e8b5ccdf":"# Comparing the models using ROC curve to decide which is the best interpretable model","13c81ec0":"* Inference - The Random Forest Model has done excellently well when it comes to results of the \"non-churned\" cases, however, for the \"churned\" cases, the model has not perfomed well.\n\n**Grid-search is used to find the optimal hyperparameters of a model which results in the best predictions the model can give. Since the results from the above Random Forest model do not seem to be good, grid search needs to be done to better the model**","a07bc11e":"From the graph, it is quite clear that not a lot of people recharged for a 3g pack in September\n\nContinuing with approach taken above, this column can be imputed with 0","80f9431e":"Intuitively, it can be concluded that the date field is not of a lot of significance in predicting the churn rate. Therfore, all the date fields can be dropped. These are stored in a temporary variable in the cell below","a90dba7a":"***Let's create a Random Forest Model to see if it can do better than the Logistic Regression and Decision Tree models*\n**\n# Step 7.3 - Model 3 - Random Forest using PCA","a03d1b0d":"When the data recharge amount is null, the mobile internet usage of 2g and 3g data is 0. Therefore, it makes sense to impute 'av_rech_amt_data_6' with 0","33c407e2":"# Step 9. Conclusion, Interpretation and Recommendation\n\n> Conclusion\n    * The best predictive model is the tuned Random Forest Model\n    * The final model that can be used to predict the target variable is a Random Forest Model built using the following parameters\n    * bootstrap = True\n    * max_depth = 10\n    * max_features = 10\n    * min_samples_leaf = 100\n    * min_samples_split = 200\n    * n_estimators = 200","2e7d0a07":"# Step 7.2 - Model 2 - Decision Tree model using PCA here\u00b6","537e4d67":"This column specifies the number of recharges that were made for 2g pack in September. Just like in June as well as July and August, from the graph, it is evident that not a lot of customer did more than 5 recharges for 2g pack in September as well\n\nIntuitively, a null in this coulmn could mean that no 2g recharge was done by the customer. Therefore, the nulls can be replaced by 0 as it was done for June, July and August","f347a28b":"By executing the code in a couple of cells earlier, we know that the maximum null percentage is approximately 75%\n\nSo, handling of null values can be started by checking the columns that have a null percentage between 70% and 80%","9baec40b":"* Inference - All the metrics have slacked compared to the second decision tree.\n* Regardless, let's compare the ROC curves to see which Decision Tree was the best","f5f88efc":"> Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\n* Let's assign a random cut-off value for the predicted probabilty and see how the model behaves.","fcc0161d":"*The best sensitivity is achieved with n_estimators = 1, i.e with just one decision tree.*","c68d798a":"Let's first compare which Decision Tree does the best prediction and then use that to compare it with Logistic Regression and Random Forest models","caef7652":"* It would be expected that customers that contribute to a high revenue in the month would be loyal towards the network and not churn. \n* From the graph, it is evident that the customers towards the lower side of the revenue are churning (churn=1) \n* However, even on the lower side of the graph, the customers are seen to still stay strong with the network (churn=0), implying that although revenue is a factor in deciding churn behaviour, it needs to be concatenated with other factors as well","accd9c3d":"This is being handled later","76ba867f":"Of the values present, seems like most of the users utilize service scheme to avail services of Facebook and similar social networking sites. We might need to look at this column in concatenation with some other columns to understand what to impute the null values with","6530b516":"This columns is being handled later","24550f3e":"* Inference - Although the accuracy is good, the sensitivity is not quite up to the mark. This model of the Decision Tree does not seem to be performing per our requirements\n\n* Creating another Decision Tree by tuning the hyperparameters","c3ac3094":"* The best interpretable model is the second Decision Tree model","2dff9496":"For all practical purposes, the test data would not have balanced information. Therefore, data imbalance did not need to be handled for test data","ce93f7f1":"Intuitively, it makes sense to impute the null data with 0.","4d71d107":"> All the p-values now seem to be under the acceptable limit and therefore these coefficients are significant\n\n# Predicting the train and test results\u00b6","c865f60d":"Of the values present, seems like most of the users utilize service scheme to avail services of Facebook and similar social networking sites. We might need to look at this column in concatenation with some other columns to understand what to impute the null values with","804f1b72":"> As evident from the above table and plot, the features that affect the churn behaviour the most are:\n        * og_others_8\n        * loc_ic_t2m_mou_8\n        * total_rech_num_8\n        * loc_og_t2t_mou_8\n        * std_og_t2t_mou_8\n        * fb_user_8\n        * last_day_rch_amt_8\n        * spl_ic_mou_8\n        * avg_rech_amt_diff\n        * offnet_mou_8","b9315353":"# Step 4 - Outliers' treatment","e881ba92":"As evident from the above output, this column has a lot of outliers, therefore the null data will be imputed by median because the mean will corrupt the results and interpretations","e487bcba":"> Assessing the model with StatsModels\nRFE has helped us come with the most important features. These can now be assessed by building a StatsModels model using the above features.","06ba1f8b":"Of the 226 columns, 214 hold numeric information. These could be categorical fields that store numbers as categories. This will be handled further down the assignment","9e633b8a":"* One would anticipate that a customer who does more number of recharges in a month would not churn. This is not completely obvious from the above graphs implying that the number recharges cannot solely contribute towards the churn behaviour of a customer\n* Since the columns for September are not required for futher processing, these columns can be deleted.","49d0aa2f":"# Inferences and Recommendations\n\n* A few inferences derived from the Decision Tree:\n\n          * total_ic_mou_8 <= 0.185; \n          * av_rech_amt_data_8 <= 146.0; \n          * std_og_mou_8 <= 287.905; \n          * loc_ic_mou_8 <= 0.105; \n          * avg_vbc_3g_diff <= -71.943;\n          * loc_og_mou_7<= 923.11 ========> customer is most likely to churn\n        \n          * total_ic_mou_8 <= 0.185; \n          * av_rech_amt_data_8 <= 146.0; \n          * std_og_mou_8 <= 287.905; \n          * loc_ic_mou_8 > 0.105; \n          * av_rech_amt_data_6 <= 1280.0;\n          * vol_3g_mb_7 <= 6333.33; ======> customer is most likely to churn\n              \n          * total_ic_mou_8 > 30.02; \n          * roam_og_mou_8 <= 0.02; \n          * total_rech_amt_8 > 236.5; \n          * last_day_rch_amt_8 > 2.5; \n          * avg_og_mou_diff <= 231.905;\n          * avg_rech_amt_diff > -1351.75; \n          * roam_ic_mou_8 <= 0.455 ========> customer is most likely to not churn\n          \n          * total_ic_mou_8 > 30.02; \n          * roam_og_mou_8 <= 0.02; \n          * total_rech_amt_8 > 236.5; \n          * last_day_rch_amt_8 > 2.5; \n          * avg_og_mou_diff > 231.905;\n          * total_rech_amt_8 <= 597.5; \n          * avg_vbc_3g_diff <= 15.757 =======> churn is most likely to not churn\n          \n          * total_ic_mou_8 > 30.02; \n          * roam_og_mou_8 > 0.02; \n          * loc_ic_mou_8 <= 159.37; \n          * roam_og_mou_7 <= 1.395; \n          * loc_og_t2f_mou_6 <= 0.235; \n          * roam_og_mou_6 <= 68.545; \n          * spl_ic_mou_8 <= 0.17 ; ======> customer is most likely to churn\n\n> Recommendations:\n       * If total_ic_mou_8 < 30, then likelihood of the customer churning from the network is on the higher side.\n       * The tree shows that if roam_og_mou_8 is a significant number then the customer is likely to churn. \n       * One logical reason would be the high roaming charges that were levied on the customer. Therefore, giving a rebate on the outgoimg\n         roaming charges to a customer could help him be retained with the network.\n\n> If the total recharge made by the customer in August is more than 236.5 units, then he is likely to retain with the network. This is quite logical. So, for customers who have recharged with amount lesser than 236.5 units in August, the operator can provide lucrative schemes on recharge, possibly something like cashback, coupons, vouchers, etc could help in retaining the customer.\n\n> If the recharge amount on the last day of the month is less, obviously there are more chances of the customer churning. Therefore, in order to retain him, the network operator can provide some alluring deal like freebies, free minutes, free messaging, etc if the customer recharges with at least a certain amount. Since this is a prepaid customer that we are talking about, if the customer recharges on the last dy in August, he shall have the balance amount left in September, thereby reducing the chances of him churning\n\n> If the difference in the recharge amounts between the good and action phase is high, the customer will most likely churn. However, if the customer has been consistenly recharging with a similar amount in both the good and action phases, he is more likely to stay on with the network. Therefore, the telecom operator should ensure that towards the end of August, the total recharge made by the customer should be almost as much as that in the good phase. If not, provide the customer with some offers to increase the recharge amount\n\n> As the number of outgoing calls (both local and std) have increased from the good phase to action phase, it is observed that the likelihood of the customer churning is high. Therefore, a reduction in the outgoing calls' cost or giving some free outgoing minutes could be beneficial for the telecom operator in retaining the customer","eaf990a1":"* The acceptable p-value (to ensure a coeffiecient is significant) is under 0.05\n\n* In the above output, a lot of p-values are greater than this value. We shall begin with deleting the feature with the largest p-value and rebuilding the model. This will be repeated until all the coefficients are significant","5bce39c5":"This is the dataset of the High value customers. The further analysis needs to be done on this dataset","a40028f9":"Of the values present, seems like most of the users utilize service scheme to avail services of Facebook and similar social networking sites. We might need to look at this column in concatenation with some other columns to understand what to impute the null values with","4399ee35":"Because there are a lot of columns in the dataset, the entire list of columns with their respective null percentage is not being displayed.\n\nThis process needs to be broken down and each piece addressed separately. To do so, we have created a function that accepts the dataframe, the lower and upper limits as the input parameters are returns the columns that have the null percentage between these limits","e249cfd3":"Handling the null values between 60% to 70%","e72bfc5b":"This column specifies the number of recharges that were made for 2g pack in August. Just like in June as well as July, from the graph, it is evident that not a lot of customer did more than 5 recharges for 2g pack in August as well\n\nIntuitively, a null in this coulmn could mean that no 2g recharge was done by the customer. Therefore, the nulls can be replaced by 0 as it was done for June and July","8ade211a":"* Inference - The most critical metric for the problem is sensitivity. Its value is reducing with every iteration. The first iteration gives the best value of sensitivity. At this value, all the metrics seem to show acceptable values","6efdde50":"# Creating some derived features for brevity","0dd3350b":"* Inference - The model has performed well on the test data","534b88ba":"This column specifies the number of recharges that were made for 2g pack in June. From the graph, it is evident that not a lot of customer did more than 5 recharges for 2g pack in June\n\nIntuitively, a null in this coulmn could mean that no 2g recharge was done by the customer. Therefore, the nulls can be replaced by 0. A similar approach would also be followed for the same column for July and August","55cd584d":"The last 3 columns - 'aug_vbc_3g', 'jul_vbc_3g', 'jun_vbc_3g', 'sep_vbc_3g' give the \"Volume based cost\" information of the respective months per the column name. But these column names are not consistent with the names pf the remaining columns in the data. So, these columns should be renamed to maintain consistency of column names in the dataset","fd60a31c":"8.64% of the customers in the High value dataset churn. This dataset is highly imbalanced. If continued to be analyzed as-is, the result would be highly skewed and not useful. Therefore, it is necessary to handle this imbalance before proceeding further\n\nThe imbalance will be treated by creating synthetic records of the minor class (in this case, when churn = 1)","84ac3a98":"This column does not add a lot of value. Therefore, it can be dropped","1bc22ac1":"Intuitively, it makes sense to impute this column with 0","6f9d3a79":"Of the values present, seems like most of the users utilize service scheme to avail services of Facebook and similar social networking sites. We might need to look at this column in concatenation with some other columns to understand what to impute the null values with","585b0d2d":"From the curve above, 0.52 is the optimum point to take it as a cutoff probability. --- Despite that, we chose the cut-off as 0.45 because the critical evaluation metric for the problem at hand is sensitivity. From the table above, it is evident that at threhold = 0.45, the sensitivity is quite high and accuracy seems to be a tab bit lesser. So at threshold = 0.45, both the metrics would be acceptable","d7e9c6d0":"# Data Cleaning","8305b209":"* Inference - Although the accuracy takes a minor hit in the test data results is lesser than the train data, the sensitivity is better in the former and specificity is not quite lesser.\n\n* Conclusion - The Logistic Regression does a decent job in both train and test data predictions.","43b608ae":"Imputing the columns in col_imp using the values in val_imp","81acc633":"From the graph, it is quite clear that not a lot of people recharged for a 3g pack in June\n\nContinuing with approach taken above, this column can be imputed with 0","0226cd7b":"* Inference - Although this Decision Tree improved the Sensitivity, it compromised on the Accuracy and Specificity.\n\n* Conclusion - The second Decision Tree seems to have done the best job.","5eac9b4b":"* The train data is balanced","4e8386b2":"This column specifies the number of recharges that were made for 2g pack in July. Just like in June, from the graph, it is evident that not a lot of customer did more than 5 recharges for 2g pack in July as well\n\nIntuitively, a null in this coulmn could mean that no 2g recharge was done by the customer. Therefore, the nulls can be replaced by 0 as it was done for June","7fb77858":"# Boosting now","7a687a9f":"# The problem statement mentions - Note that is is more important to identify churners than the non-churners accurately - choose an appropriate evaluation metric which reflects this business goal.\n\n* Accuracy is a measure of the number of cases(both \"churned\" and \"not churned\") rightly predicted by the model. Sensitivity is a measure of the number of \"churned\" cases rightly predicted by the model. Specificity is a measure of how precisely the model is able to predict the number of \"churned\" cases.\n\n* Therefore, Sensitivity is the most imperative metric. The aim would be to maximize the sensitivity while ensuring accuracy and specificity do not take a big hit either.\n\n* Also, sensitivity is same as Recall or True Positive Rate","1fde5078":"As evident from the above output, this column has a lot of outliers, therefore the null data will be imputed by median because the mean will corrupt the results and interpretations","485c5400":"Now the features can be handled one at a time\n\nThe mobile_number is the mobile number of the customer and therefore need not be analyzed. Let's look at the circle_id, which is the \"Telecom circle area to which the customer belongs to\"","a2e10f52":"> Conclusion -\nThe hyperparameter tuned Random Forest does the best prediction of the target variable. It maintains good balance between both - sensitivity and specificity without quite compromising on the accuracy of the model\n\n* Note: Since the values of area under the ROC curve are quite close, all the three curves are not visible\n\n# Predictive Model information -\n* *The final model that can be used to predict the target variable is a Random Forest Model built using the following parameters*\n* bootstrap = True\n* max_depth = 10\n* max_features = 10\n* min_samples_leaf = 100\n* min_samples_split = 200\n* n_estimators = 200\n\n# Step 8 : Interpretable Model building\n> The model built using PCA does not have the original features because they would have been replaced with the Principal Components. Therefore, a new interpretable model is being built using the original features. This model can help us identify the most important features that the affect a customer behaviour and can help the organization channel its efforts in the right direction to help retain their High value customers\n\n# Step 8.1 - Model 1 - Logistic Regression without using PCA","e7e4c3fe":"# September data - the imputation would be carried out as it was done for the June data","a932820c":"From the graph, it is quite clear that not a lot of people recharged for a 3g pack in August\n\nContinuing with approach taken above, this column can be imputed with 0","3e22b2ff":"# Bivariate Analysis\n* Analysing a few columns pairwise in the dataset of High Value Customers","920a3ab3":"The other columns will be looked at and handled in a similar fashion","78cc1562":"# Steps to be followed:\n*  Reading and understanding the data\n*  Data cleaning\n*  Filtering the High-value customers\n*  Treating the outliers\n*  PCA\n*  Handling imbalance in the data\n*  Predictive Modeling:\n          *  Model 1 - Logistic Regression using PCA\n          *   Model 2 - Decision Tree using PCA\n          *   Model 3 - Random Forest using PCA\n*  Interpretable Model:\u00b6\n          *   Model 1 - Logistic Regression without PCA\n          *   Model 2 - Decision Tree without PCA\n          *   Model 3 - Boosting\n*  Conclusion, Interpretation and Recommendation","13f346f3":"> The entire dataset has been used for modeling. Obviously all these variables would not be needed for modeling. Recursive Feature Elimination, an automated method of modeling can be used to identify the top few features for modeling. Coupling this with manual method of removing features as needed would be an efficient way to come up with an interpretable model\n\n# Feature Selection Using RFE","12675eba":"* Since most of the records are getting removed as part of the outlier treatment, Hence it does not make sense to remove the outliers in this assignment\n# Step 5 - PCA (Principal Component Analysis)\n* Since there are a lot of variables present in the dataset, PCA is a good technique to reduce the dimensions while not losing a lot of information","7aa1be72":"# Step 1: Reading and understanding the data","539ca655":"* Inference - The accuracy, sensitivity and specificity are all better than the first decision tree model. The precision still does not seem to be good though. Let's see if the result can still be made better because the Logistic Regression model gave a better sensitivity while still maintaining decent accuracy and specificity\n\n# Creating another Decision Tree by tuning the hyperparameters","eb520e2a":"> **VIF<5 is an acceptable industry norm. As evident from the above output, there are a lot of features with VIF>5 Getting rid of these features before proceeding further is a good way to build a reliable and interpretable mod**","3308492d":"Inference - All the evaluation metrics look good .","fd4bb1cd":"As evident from the output of the above 2 cells, the churn of customers who are \"very loyal\" is very low while the ones who are \"not loyal\" is the highest","df87852d":"# Scaling the data\n* Standardization is the technique used to scale the data here","911af4c2":"When the data recharge amount is null, the mobile internet usage of 2g and 3g data is 0. Therefore, it makes sense to impute 'av_rech_amt_data_7' with 0","a1521d68":"There are no more Null records. So we can proceed with the next step\n\n# Step 3 - Creating the High value customers' dataset\nThis dataset is created by filtering the customers to contribute to the top 70% of the average recharge amounts in \"good phase\".\n\nThe problem statement defines High Value Customers as - Those who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).","de9ff4ea":"Since these values belong to September, and these will not be used to determine the High value customers, these columns can be deleted","abb79989":"# Plotting the ROC Curve","acc24006":"Scaling the data","b65def72":"> VIF\n* Calculating VIF is extremely necessary to ensure absence of multicollinearilty. Models in which the predictor variables are themselves dependent on each other are not quite interpretable","d32cf3ae":"# Step 8.2 - Model 2 - Decison Tree without using PCA"}}