{"cell_type":{"907aa99a":"code","c0ce93cf":"code","1370591e":"code","34f76416":"code","cdf12e69":"code","e2e5c570":"code","e1a69b3b":"code","50564607":"code","0f326d99":"code","2fd6dd78":"code","8f7a7891":"code","986902de":"code","78d8e980":"code","e3f9a231":"code","26fdd0cc":"markdown","795403d6":"markdown","da25eeba":"markdown","9a22185a":"markdown","00cc21aa":"markdown"},"source":{"907aa99a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c0ce93cf":"import sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1370591e":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf","34f76416":"#df.isna().sum()\ndf.dtypes","cdf12e69":"import pandas_profiling\n\nprofile = pandas_profiling.ProfileReport(df)\nprofile","e2e5c570":"pip install ppscore","e1a69b3b":"def heatmap(df):\n    df = df[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\n    fig, ax = plt.subplots(figsize=(25,25)) \n    ax = sns.heatmap(df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5, annot=True)\n    ax.set_title(\"PPS matrix\")\n    ax.set_xlabel(\"feature\")\n    ax.set_ylabel(\"target\")\n    return ax\n\nimport ppscore as pps\nmatrix = pps.matrix(df)\nheatmap(matrix)","50564607":"from sklearn.preprocessing import RobustScaler\n\nrs = RobustScaler()\n\ndf['Amount'] = rs.fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['Time'] = rs.fit_transform(df['Time'].values.reshape(-1, 1))\ndf.head(10)","0f326d99":"#splitting data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['Class'], axis = 1)\ny = df['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)","2fd6dd78":"#making a function for evaluation model results, using confusion_matrix, classification_report\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ndef evaluate(y_actual, y_hat):\n    matrix = confusion_matrix(y_actual, y_hat)\n\n    sns.heatmap(pd.DataFrame(matrix), annot = True, cmap ='PuBu', fmt = 'g')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    \n    labels = ['Non-Fraud', 'Fraud']\n    print(classification_report(y_actual, y_hat, target_names = labels))","8f7a7891":"scale_num = int(y_train.value_counts().values[0]\/y_train.value_counts().values[1])\nscale_num","986902de":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_estimators = 300, verbosity = 1, use_label_encoder=False, scale_pos_weight = scale_num)\nxgb.fit(X_train, y_train)\n\npredictions_xgb = xgb.predict(X_test)\nevaluate(y_test, predictions_xgb)","78d8e980":"features = ['V10', 'V12', 'V14', 'V16', 'V17']\nX1 = df[features]\ny1 = df['Class']\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size = 0.20, random_state = 42)","e3f9a231":"xgb1 = XGBClassifier(n_estimators = 300, verbosity = 1, use_label_encoder=False, scale_pos_weight = scale_num)\nxgb1.fit(X_train1, y_train1)\n\npredictions_xgb1 = xgb1.predict(X_test1)\nevaluate(y_test1, predictions_xgb1)","26fdd0cc":"We can see that using all the features gives us higher precision and lower recall. Depending on business question, what is more important - correct classifying of Frauds or not labeling Non-Fraud clients as Fraud one should choose the best model. ","795403d6":"Okay, so far we have seen a few things:\n1. There are no missing values.\n2. Features except Time and Amount are scaled.\n3. There is no linear relationship between V-features and there is a correlation between V-features and Class (target variable\n4. The dataset is very imbalanced, there are 492 Frauds and 284315 Not Frauds.\nLet's have a look at predictive power score matrix for any other relationships between features.","da25eeba":"We will use XGBoost for building a model because it has scale_pos_weight parameter that allows to deal with imbalanced data sets. This parameter is a proportion between negative and positive values, so we have to calculate it at first for the algorithm.","9a22185a":"Well, that looks interesting. It seems, there is pretty strong correlation between V10, V12, V14, V16,V17 and Class. We will try to use it later when we'll build a model. \nAnd before that we need to do some processing of features Amount and Time.","00cc21aa":"Now, let's try selected features that have high prediction value of Class and train another model."}}