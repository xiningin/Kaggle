{"cell_type":{"a7c6eb91":"code","0b887105":"code","122747be":"code","57b81e4b":"code","24ae8f21":"code","079c33b8":"code","06be81e4":"code","7a5abb1f":"code","89450ecd":"code","ae5e8cc7":"code","137f9cc1":"code","c5ea1dce":"code","7d543fa5":"code","6743d24a":"code","b02e6408":"code","e02aaf02":"code","0a6b8a3f":"code","2bb9f139":"code","be031686":"code","4420dfe4":"code","5a6da9bc":"code","47c0ff15":"code","35c955b2":"code","b04c82ca":"code","e169bf56":"code","6488c409":"code","5a52c68b":"code","63e501d7":"code","43421101":"code","d7d2dfa0":"code","796c9235":"code","3d74e0ad":"code","748c624f":"code","79ce6b51":"code","0d424448":"code","d8571548":"code","8088d571":"code","6795f5ab":"code","71d88020":"code","2ddd11cf":"code","6dcab009":"code","ae86b103":"code","9cbe3039":"code","7c0e1b38":"code","0fd62744":"code","c8d84e98":"code","77010a8b":"code","970ed092":"code","8369fa8b":"code","e8a1651a":"code","f9f4197b":"code","bdf7148b":"code","3c0ce1a1":"code","d5bafcc5":"code","03719131":"code","9c91f4f8":"code","421bbd2e":"code","c966cd4b":"code","d24928fc":"code","2d899ddc":"code","7f93b386":"code","074f69ae":"code","5dff8bd9":"code","2a1e34ae":"code","0b9540ee":"code","ad48453a":"code","2acffdb4":"code","bdd1b76a":"code","3faf8a9d":"code","c5c6d5b0":"code","18f3fe01":"code","bc940d8a":"code","e3406a98":"code","2a25d686":"code","8efac1b2":"markdown","7634cb9d":"markdown","7634c1c3":"markdown","60901f7d":"markdown","e117621f":"markdown","76169b2a":"markdown","20947dab":"markdown","6a3784d2":"markdown","83123a59":"markdown","5d9ce925":"markdown","c383ecde":"markdown","d94b8ba0":"markdown","7c810741":"markdown","26558f1a":"markdown","0a6fd5f9":"markdown","4996242f":"markdown","c032d8dd":"markdown","43d1c503":"markdown","1e56177a":"markdown","ac903b58":"markdown"},"source":{"a7c6eb91":"import pandas as pd\nimport numpy as np\nimport random\nfrom datetime import datetime, timedelta\nimport time\nimport matplotlib.pyplot as plt\nimport seaborn as sns  \nimport matplotlib as mpl\nfrom IPython.display import Image\nfrom sklearn.metrics import mean_squared_error\n#import seaborn as sns   \nimport warnings\n\n%matplotlib inline","0b887105":"# Loading the database\ndf_uci_diabetic = pd.read_csv('..\/input\/diabetic-data-cleaning\/diabetic_data.csv', decimal=b',')\n\n# Criando um novo dataframe a partir do df_uci_diabetic\ndf = df_uci_diabetic.copy (deep = True)","122747be":"print('O Dataframe diabetic_data possui ' + str(df.shape[0]) + ' linhas e ' + str(df.shape[1]) + ' colunas')","57b81e4b":"# Checking Data Types and Descriptive Statistics\nprint (df.info ()) \nprint (df.describe ())","24ae8f21":"# Viewing the first 10 rows of the dataframe\ndf.head(10)","079c33b8":"df.describe()","06be81e4":"# Checking for missing data\nfor col in df.columns:\n    if df[col].dtype == object:\n        if df[col][df[col] == '?'].count() > 0:\n            print(col,df[col][df[col] == '?'].count(),' Correspondendo a ',np.around((df[col][df[col] == '?'].count()\/df[col].count())*100,2), '% das observa\u00e7\u00f5es')","7a5abb1f":"# Evaluating the distribution of data in each attribute (has missing data)\nfor col in df.columns:\n    if df[col].dtype == object:\n        if df[col][df[col] == '?'].count() != 0:       \n            print(df.groupby([col])[col].count())\n            print('')","89450ecd":"# Evaluating the distribution of data in each attribute (no missing data)\nfor col in df.columns:\n    if df[col].dtype == object:\n        if df[col][df[col] == '?'].count() == 0:       \n            print(df.groupby([col])[col].count())\n            print('')","ae5e8cc7":"# Checking the median\nfor col in df.columns:\n    if df[col].dtype != object:\n        print(col, df[col].median())\n        print('')","137f9cc1":"# Deleting columns that will not be used\ndf.drop(['encounter_id', 'patient_nbr', 'weight', 'payer_code', 'examide', 'citoglipton', 'medical_specialty'], axis = 1, inplace = True)","c5ea1dce":"Image('..\/input\/imagem-1\/Agrupamento_CID_9.png')","7d543fa5":"# Creating new columns to assign transformed values\ndf['d1'] = df['diag_1']\ndf['d2'] = df['diag_2']\ndf['d3'] = df['diag_3']\ndf['classe'] = -1\ndf['change_t'] = -1\ndf['gender_t'] = -1\ndf['diabetesMed_t'] = -1","6743d24a":"# Regrouping the main diagnosis\ndf['d1'] = df.apply(lambda row: 1 if (row['diag_1'][0:3].zfill(3) >= '390') and (row['diag_1'][0:3].zfill(3) <= '459' ) or  (row['diag_1'][0:3].zfill(3) == '785' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 2 if (row['diag_1'][0:3].zfill(3) >= '460') and (row['diag_1'][0:3].zfill(3) <= '519' ) or  (row['diag_1'][0:3].zfill(3) == '786' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 3 if (row['diag_1'][0:3].zfill(3) >= '520') and (row['diag_1'][0:3].zfill(3) <= '579' ) or  (row['diag_1'][0:3].zfill(3) == '787' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 4 if (row['diag_1'][0:3].zfill(3) == '250') else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 5 if (row['diag_1'][0:3].zfill(3) >= '800') and (row['diag_1'][0:3].zfill(3) <= '999' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 6 if (row['diag_1'][0:3].zfill(3) >= '710') and (row['diag_1'][0:3].zfill(3) <= '739' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 7 if (row['diag_1'][0:3].zfill(3) >= '580') and (row['diag_1'][0:3].zfill(3) <= '629' ) or  (row['diag_1'][0:3].zfill(3) == '788' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 8 if (row['diag_1'][0:3].zfill(3) >= '140') and (row['diag_1'][0:3].zfill(3) <= '239' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 9 if (row['diag_1'][0:3].zfill(3) >= '790') and (row['diag_1'][0:3].zfill(3) <= '799' ) or  (row['diag_1'][0:3].zfill(3) == '780' ) or  (row['diag_1'][0:3].zfill(3) == '781' ) or  (row['diag_1'][0:3].zfill(3) == '784' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 10 if (row['diag_1'][0:3].zfill(3) >= '240') and (row['diag_1'][0:3].zfill(3) <= '249' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 10 if (row['diag_1'][0:3].zfill(3) >= '251') and (row['diag_1'][0:3].zfill(3) <= '279' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 11 if (row['diag_1'][0:3].zfill(3) >= '680') and (row['diag_1'][0:3].zfill(3) <= '709' ) or  (row['diag_1'][0:3].zfill(3) == '782' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 12 if (row['diag_1'][0:3].zfill(3) >= '001') and (row['diag_1'][0:3].zfill(3) <= '139' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '290') and (row['diag_1'][0:3].zfill(3) <= '319' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:1] >= 'E') and (row['diag_1'][0:1] <= 'V' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '280') and (row['diag_1'][0:3].zfill(3) <= '289' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '320') and (row['diag_1'][0:3].zfill(3) <= '359' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '630') and (row['diag_1'][0:3].zfill(3) <= '679' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '360') and (row['diag_1'][0:3].zfill(3) <= '389' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 13 if (row['diag_1'][0:3].zfill(3) >= '740') and (row['diag_1'][0:3].zfill(3) <= '759' ) else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: 0 if (row['diag_1'][0:3].zfill(3)  == '783' or row['diag_1'][0:3].zfill(3)  == '789') else row['d1'], axis=1)\ndf['d1'] = df.apply(lambda row: -1 if (row['diag_1'][0:1] == '?') else row['d1'], axis=1)                           ","b02e6408":"# Regrouping of the first secondary diagnosis\ndf['d2'] = df.apply(lambda row: 1 if (row['diag_2'][0:3].zfill(3) >= '390') and (row['diag_2'][0:3].zfill(3) <= '459' ) or  (row['diag_2'][0:3].zfill(3) == '785' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 2 if (row['diag_2'][0:3].zfill(3) >= '460') and (row['diag_2'][0:3].zfill(3) <= '519' ) or  (row['diag_2'][0:3].zfill(3) == '786' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 3 if (row['diag_2'][0:3].zfill(3) >= '520') and (row['diag_2'][0:3].zfill(3) <= '579' ) or  (row['diag_2'][0:3].zfill(3) == '787' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 4 if (row['diag_2'][0:3].zfill(3) == '250') else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 5 if (row['diag_2'][0:3].zfill(3) >= '800') and (row['diag_2'][0:3].zfill(3) <= '999' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 6 if (row['diag_2'][0:3].zfill(3) >= '710') and (row['diag_2'][0:3].zfill(3) <= '739' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 7 if (row['diag_2'][0:3].zfill(3) >= '580') and (row['diag_2'][0:3].zfill(3) <= '629' ) or  (row['diag_2'][0:3].zfill(3) == '788' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 8 if (row['diag_2'][0:3].zfill(3) >= '140') and (row['diag_2'][0:3].zfill(3) <= '239' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 9 if (row['diag_2'][0:3].zfill(3) >= '790') and (row['diag_2'][0:3].zfill(3) <= '799' ) or  (row['diag_2'][0:3].zfill(3) == '780' ) or  (row['diag_2'][0:3].zfill(3) == '781' ) or  (row['diag_2'][0:3].zfill(3) == '784' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 10 if (row['diag_2'][0:3].zfill(3) >= '240') and (row['diag_2'][0:3].zfill(3) <= '249' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 10 if (row['diag_2'][0:3].zfill(3) >= '251') and (row['diag_2'][0:3].zfill(3) <= '279' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 11 if (row['diag_2'][0:3].zfill(3) >= '680') and (row['diag_2'][0:3].zfill(3) <= '709' ) or  (row['diag_2'][0:3].zfill(3) == '782' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 12 if (row['diag_2'][0:3].zfill(3) >= '001') and (row['diag_2'][0:3].zfill(3) <= '139' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '290') and (row['diag_2'][0:3].zfill(3) <= '319' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:1] >= 'E') and (row['diag_2'][0:1] <= 'V' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '280') and (row['diag_2'][0:3].zfill(3) <= '289' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '320') and (row['diag_2'][0:3].zfill(3) <= '359' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '630') and (row['diag_2'][0:3].zfill(3) <= '679' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '360') and (row['diag_2'][0:3].zfill(3) <= '389' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 13 if (row['diag_2'][0:3].zfill(3) >= '740') and (row['diag_2'][0:3].zfill(3) <= '759' ) else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: 0 if (row['diag_2'][0:3].zfill(3)  == '783' or row['diag_2'][0:3].zfill(3)  == '789') else row['d2'], axis=1)\ndf['d2'] = df.apply(lambda row: -1 if (row['diag_2'][0:1] == '?') else row['d2'], axis=1)                           ","e02aaf02":"# Regrouping the second secondary diagnosis\ndf['d3'] = df.apply(lambda row: 1 if (row['diag_3'][0:3].zfill(3) >= '390') and (row['diag_3'][0:3].zfill(3) <= '459' ) or  (row['diag_3'][0:3].zfill(3) == '785' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 2 if (row['diag_3'][0:3].zfill(3) >= '460') and (row['diag_3'][0:3].zfill(3) <= '519' ) or  (row['diag_3'][0:3].zfill(3) == '786' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 3 if (row['diag_3'][0:3].zfill(3) >= '520') and (row['diag_3'][0:3].zfill(3) <= '579' ) or  (row['diag_3'][0:3].zfill(3) == '787' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 4 if (row['diag_3'][0:3].zfill(3) == '250') else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 5 if (row['diag_3'][0:3].zfill(3) >= '800') and (row['diag_3'][0:3].zfill(3) <= '999' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 6 if (row['diag_3'][0:3].zfill(3) >= '710') and (row['diag_3'][0:3].zfill(3) <= '739' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 7 if (row['diag_3'][0:3].zfill(3) .zfill(3)>= '580') and (row['diag_3'][0:3].zfill(3) <= '629' ) or  (row['diag_3'][0:3].zfill(3) == '788' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 8 if (row['diag_3'][0:3].zfill(3) >= '140') and (row['diag_3'][0:3].zfill(3) <= '239' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 9 if (row['diag_3'][0:3].zfill(3) >= '790') and (row['diag_3'][0:3].zfill(3) <= '799' ) or  (row['diag_3'][0:3].zfill(3) == '780' ) or  (row['diag_3'][0:3].zfill(3) == '781' ) or  (row['diag_3'][0:3].zfill(3) == '784' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 10 if (row['diag_3'][0:3].zfill(3) >= '240') and (row['diag_3'][0:3].zfill(3) <= '249' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 10 if (row['diag_3'][0:3].zfill(3) >= '251') and (row['diag_3'][0:3].zfill(3) <= '279' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 11 if (row['diag_3'][0:3].zfill(3) >= '680') and (row['diag_3'][0:3].zfill(3) <= '709' ) or  (row['diag_3'][0:3].zfill(3) == '782' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 12 if (row['diag_3'][0:3].zfill(3) >= '001') and (row['diag_3'][0:3].zfill(3) <= '139' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '290') and (row['diag_3'][0:3].zfill(3) <= '319' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:1] >= 'E') and (row['diag_3'][0:1] <= 'V' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '280') and (row['diag_3'][0:3].zfill(3) <= '289' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '320') and (row['diag_3'][0:3].zfill(3) <= '359' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '630') and (row['diag_3'][0:3].zfill(3) <= '679' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '360') and (row['diag_3'][0:3].zfill(3) <= '389' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 13 if (row['diag_3'][0:3].zfill(3) >= '740') and (row['diag_3'][0:3].zfill(3) <= '759' ) else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: 0 if (row['diag_3'][0:3].zfill(3)  == '783' or row['diag_3'][0:3].zfill(3)  == '789') else row['d3'], axis=1)\ndf['d3'] = df.apply(lambda row: -1 if (row['diag_3'][0:1] == '?') else row['d3'], axis=1)                           ","0a6b8a3f":"print(df.groupby(['d1', 'diag_1']).d2.count())","2bb9f139":"print(df.groupby(['d2', 'diag_2']).d2.count())","be031686":"print(df.groupby(['d3', 'diag_3']).d3.count())","4420dfe4":"df = df[(df.d1 > -1) | (df.d2 > -1) | (df.d3 > -1)]","5a6da9bc":"# Deleting the original columns from diagnostics\ndf.drop(['diag_1'], axis = 1, inplace = True)\ndf.drop(['diag_2'], axis = 1, inplace = True)\ndf.drop(['diag_3'], axis = 1, inplace = True)","47c0ff15":"# Assigns the class the values 1 or 0, 1 corresponding to readmission occurrences in less than 30 days\ndf['classe'] = df.apply(lambda row: 0 if (row['readmitted'][0:3] == '>30' or row['readmitted'][0:2] == 'NO') else row['classe'], axis=1) \ndf['classe'] = df.apply(lambda row: 1 if (row['readmitted'][0:3] == '<30') else row['classe'], axis=1)\ndf.drop(['readmitted'], axis = 1, inplace = True)","35c955b2":"df['change_t'] = df.apply(lambda row: 1 if (row['change'] == 'Ch') else -1, axis=1)\ndf['change_t'] = df.apply(lambda row: 0 if (row['change'] == 'No') else row['change_t'], axis=1)\ndf.drop(['change'], axis = 1, inplace = True)","b04c82ca":"df['gender_t'] = df.apply(lambda row: 1 if (row['gender'] == 'Male') else -1, axis=1)\ndf['gender_t'] = df.apply(lambda row: 0 if (row['gender'] == 'Female') else row['gender_t'], axis=1)\ndf.drop(['gender'], axis = 1, inplace = True)","e169bf56":"df['diabetesMed_t'] = df.apply(lambda row: 1 if (row['diabetesMed'] == 'Yes') else -1, axis=1)\ndf['diabetesMed_t'] = df.apply(lambda row: 0 if (row['diabetesMed'] == 'No') else row['diabetesMed_t'], axis=1)\ndf.drop(['diabetesMed'], axis = 1, inplace = True)","6488c409":"m = 0\nmedicacoes = ['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'glipizide', 'glyburide', \n              'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'insulin', 'glyburide-metformin', 'tolazamide', \n              'metformin-pioglitazone','metformin-rosiglitazone', 'glimepiride-pioglitazone', \n              'glipizide-metformin', 'troglitazone', 'tolbutamide', 'acetohexamide']\nfor col in df.columns:\n    if col in medicacoes:       \n        colname = 'Med' + str(m) + '_t'\n        df[colname] = df.apply(lambda row: 0 if (row[col] == 'No') else 1, axis=1)\n        df.drop([col], axis = 1, inplace = True)\n        m = m + 1","5a52c68b":"df['A1Cresult_t'] = df.apply(lambda row: 0 if (row['A1Cresult'][0:4] == 'Norm') else -1, axis=1) \ndf['A1Cresult_t'] = df.apply(lambda row: 1 if (row['A1Cresult'][0:2] == '>7' or row['A1Cresult'][0:2] == '>8') else row['A1Cresult_t'], axis=1) \ndf.drop(['A1Cresult'], axis = 1, inplace = True)","63e501d7":"df['max_glu_serum_t'] = df.apply(lambda row: 0 if (row['max_glu_serum'][0:4] == 'Norm') else -1, axis=1) \ndf['max_glu_serum_t'] = df.apply(lambda row: 1 if (row['max_glu_serum'][0:2] == '>7' or row['max_glu_serum'][0:2] == '>8') else row['max_glu_serum_t'], axis=1) \ndf.drop(['max_glu_serum'], axis = 1, inplace = True)","43421101":"df['age_faixa'] = df.apply(lambda row: 0 if (row['age'] == '[0-10)') else -1, axis=1) \ndf['age_faixa'] = df.apply(lambda row: 1 if (row['age'] == '[10-20)') else row['age_faixa'], axis=1)\ndf['age_faixa'] = df.apply(lambda row: 2 if (row['age'] == '[20-30)') else row['age_faixa'], axis=1) \ndf['age_faixa'] = df.apply(lambda row: 3 if (row['age'] == '[30-40)') else row['age_faixa'], axis=1)\ndf['age_faixa'] = df.apply(lambda row: 4 if (row['age'] == '[40-50)') else row['age_faixa'], axis=1) \ndf['age_faixa'] = df.apply(lambda row: 5 if (row['age'] == '[50-60)') else row['age_faixa'], axis=1)\ndf['age_faixa'] = df.apply(lambda row: 6 if (row['age'] == '[70-80)') else row['age_faixa'], axis=1) \ndf['age_faixa'] = df.apply(lambda row: 7 if (row['age'] == '[80-90)') else row['age_faixa'], axis=1)\ndf['age_faixa'] = df.apply(lambda row: 8 if (row['age'] == '[90-100)') else row['age_faixa'], axis=1)\ndf.drop(['age'], axis = 1, inplace = True)","d7d2dfa0":"df['race_t'] = df.apply(lambda row: 0 if (row['race'] == '?') else -1, axis=1) \ndf['race_t'] = df.apply(lambda row: 1 if (row['race'] == 'AfricanAmerican') else row['race_t'], axis=1)\ndf['race_t'] = df.apply(lambda row: 2 if (row['race'] == 'Asian') else row['race_t'], axis=1) \ndf['race_t'] = df.apply(lambda row: 3 if (row['race'] == 'Caucasian') else row['race_t'], axis=1)\ndf['race_t'] = df.apply(lambda row: 4 if (row['race'] == 'Hispanic') else row['race_t'], axis=1) \ndf['race_t'] = df.apply(lambda row: 5 if (row['race'] == 'Other') else row['race_t'], axis=1)\ndf.drop(['race'], axis = 1, inplace = True)","796c9235":"# Saving the dataset with the transformations\ndf.to_csv('.\/diabetes_data_modificado.csv', index=False)","3d74e0ad":"# Loading the transformed database\ndf = pd.read_csv('diabetes_data_modificado.csv', decimal=b',')","748c624f":"df.head(10)","79ce6b51":"print (df.info ()) ","0d424448":"print(df.groupby(['classe']).classe.count())","d8571548":"# Data Manipulation Packages\nimport sklearn \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA, RandomizedPCA\nfrom sklearn.preprocessing import scale, MinMaxScaler, MultiLabelBinarizer, QuantileTransformer, Normalizer, StandardScaler, MaxAbsScaler, RobustScaler\n\n# Keras e TensorFlow\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.models import load_model\nfrom keras.optimizers import SGD, Adam, RMSprop\nimport tensorflow as tf\n\n# Pacotes para Confusion Matrix e Balanceamento de Classes\n#from pandas_ml import ConfusionMatrix\n#import pandas_ml as pdml\nimport imblearn\n\nLABELS = [\"Normal\", \"Readmiss\u00e3o\"]","8088d571":"import matplotlib.pyplot as plt\nimport itertools\nfrom sklearn.metrics import classification_report\n\ndef pretty_print_conf_matrix(y_true, y_pred, \n                             classes,\n                             normalize=False,\n                             title='Confusion matrix',\n                             cmap=plt.cm.Blues):\n    \"\"\"\n    refer\u00eancia: http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n\n    \"\"\"\n\n    cm = confusion_matrix(y_true, y_pred)\n\n    # Configure Confusion Matrix Plot Aesthetics (no text yet) \n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(cm, cmap=plt.cm.Blues)\n    fig.colorbar(cax)\n    plt.title(title, fontsize=14)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('True label', fontsize=12)\n    plt.xlabel('Predicted label', fontsize=12)\n\n    # Calculate normalized values (so all cells sum to 1) if desired\n    if normalize:\n        cm = np.round(cm.astype('float') \/ cm.sum(),2) #(axis=1)[:, np.newaxis]\n\n    # Place Numbers as Text on Confusion Matrix Plot\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\",\n                 fontsize=12)\n\n\n    # Add Precision, Recall, F-1 Score as Captions Below Plot\n    rpt = classification_report(y_true, y_pred)\n    rpt = rpt.replace('avg \/ total', '      avg')\n    rpt = rpt.replace('support', 'N Obs')\n\n    plt.annotate(rpt, \n                 xy = (0,0), \n                 xytext = (-50, -140), \n                 xycoords='axes fraction', textcoords='offset points',\n                 fontsize=12, ha='left')    \n\n    # Plot\n    plt.tight_layout()","6795f5ab":"# Function for the statistics of accuracy, inaccuracy, false negative and false positive rates\ndef estatisticas(y_true, y_pred):\n    false_neg = 0\n    false_pos = 0\n    incorrect = 0\n    y2_true = np.array(y_true)\n    total = len(y_true)\n    for i in range(len(y_true)):        \n        if y_pred[i] != y2_true[i]:\n            incorrect += 1\n            if y2_true[i] == 1 and y_pred[i] == 0:\n                false_neg += 1\n            else:\n                false_pos += 1\n\n    inaccuracy = incorrect \/ total\n\n    print('Inacur\u00e1cia:', inaccuracy)\n    print('Acur\u00e1cia:', 1 - inaccuracy)\n    if incorrect > 0:\n        print('Taxa de Falsos Negativos:', false_neg\/incorrect)\n        print('Taxa de Falsos Positivos:', false_pos \/ incorrect )    \n    print('Falsos Negativos\/total:', false_neg\/total)\n    return inaccuracy, incorrect","71d88020":"#df['classe'].hist()\n#plt.show()\ncount_classes = pd.value_counts(df['classe'], sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Distribui\u00e7\u00e3o\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Classe\")\nplt.ylabel(\"Frequ\u00eancia\");","2ddd11cf":"print('O Dataframe diabetic_data_modificado possui ' + str(df.shape[0]) + ' linhas e ' + str(df.shape[1]) + ' colunas')","6dcab009":"readmissoes = df.loc[df['classe'] == 1]\nnao_readmissoes = df.loc[df['classe'] == 0]\nprint(\"Temos\", len(readmissoes), \"pontos de dados como readmiss\u00f5es e\", len(nao_readmissoes), \"pontos de dados considerados normais.\")","ae86b103":"# Assigning Values to the X and Y Variables of the Model\nX = df.iloc[:,:-1]\ny = df['classe']\n\n# Aplicando Scala e Redu\u00e7\u00e3o de dimensionalidade com PCA\nX = scale(X)\npca = PCA(n_components = 10, random_state=38)\nX = pca.fit_transform(X)\n\n# Gerando dados de treino, teste e valida\u00e7\u00e3o\nX1, X_valid, y1, y_valid = train_test_split(X, y, test_size = 0.10, random_state = 0)\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size = 0.26, random_state = 0)","9cbe3039":"print(\"Tamanho do Dataset de Treino: \", X_train.shape)","7c0e1b38":"print(\"Tamanho do Dataset de Valida\u00e7ao: \", X_valid.shape)","0fd62744":"print(\"Tamanho do Dataset de Test: \", X_test.shape)","c8d84e98":"model = Sequential()\nmodel.add(Dense(10, input_dim = 10, activation = 'relu'))     \nmodel.add(Dense(1, activation = 'sigmoid'))                \nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\nmodel.summary()","77010a8b":"model.fit(X_train, y_train, epochs = 1, validation_data=(X_valid, y_valid))","970ed092":"print(\"Erro\/Acur\u00e1cia: \", model.evaluate(X_valid, y_valid, verbose = 0))","8369fa8b":"y_predicted = model.predict(X_valid).T[0].astype(int)","e8a1651a":"# Plot Confusion Matrix\nwarnings.filterwarnings('ignore')\npretty_print_conf_matrix(y_valid, y_predicted, \n                         classes= ['0', '1'],\n                         normalize=False, \n                         title='Confusion Matrix')","f9f4197b":"estatisticas(y_valid, y_predicted)","bdf7148b":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=42, ratio='minority')\nX2, y2 = smote.fit_sample(X, y)","3c0ce1a1":"count_classes = pd.value_counts(y2, sort = True)\ncount_classes.plot(kind = 'bar', rot=0)\nplt.title(\"Distribui\u00e7\u00e3o\")\nplt.xticks(range(2), LABELS)\nplt.xlabel(\"Classe\")\nplt.ylabel(\"Frequ\u00eancia\");","d5bafcc5":"# Generating training data based on balanced data\nX2_train, X_test_, y2_train, y_test_ = train_test_split(X2, y2, test_size = 0.33, random_state = 0)","03719131":"from keras.callbacks import EarlyStopping\nfrom tensorflow import set_random_seed\nimport keras as keras\nfrom sklearn.metrics import precision_score, recall_score","9c91f4f8":"#OPTIMIZER = Adam(lr=0.01, beta_1=0.99, beta_2=0.999, amsgrad=True) # otimizador\nOPTIMIZER = RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)","421bbd2e":"# Class to calculate metric of accuracy based on recall\nclass Metrics(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, y_val = self.validation_data[0], self.validation_data[1]\n        y_predict = np.round(model2.predict(X_val)).T[0]\n    \n        self._data.append({\n            'val_recall': recall_score(y_val, np.round(model2.predict(X_val)).T[0]),\n            'val_precision': precision_score(y_val, np.round(model2.predict(X_val)).T[0]),\n        })\n        return\n\n    def get_data(self):\n        return self._data","c966cd4b":"batch_size = 8790\nseed = 100\nset_random_seed(seed)\nmetrics = Metrics()","d24928fc":"model2 = Sequential()\nmodel2.add(Dense(10, input_dim = 10,   kernel_initializer='ones', activation = 'tanh')) \nmodel2.add(Dense(1024, activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(512, activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(16,  activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(8,  activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(4,  activation = 'tanh'))\nmodel2.add(Dropout(0.40))\nmodel2.add(Dense(1,  activation = 'sigmoid'))\nmonitor = EarlyStopping(monitor = 'val_loss', min_delta = 1e-3, patience = 5, verbose = 1, mode = 'auto')   \nmodel2.compile(loss = 'binary_crossentropy', optimizer = OPTIMIZER, metrics = ['accuracy'])\nmodel2.summary()","2d899ddc":"history = model2.fit(X2_train, y2_train, epochs = 100, batch_size = batch_size, validation_data=(X_valid, y_valid), callbacks = [monitor, metrics], shuffle=False)","7f93b386":"# Perform the training until you achieve the best recall accuracy, mandating the balance of total accuracy\nLastrecall = 0\nMaxrecall = 0\nMaxprecision = 0\nfor i in range(5980,9790,10):\n    batch_size = i\n    print(i)\n    metrics = Metrics()\n    history = model2.fit(X2_train, y2_train, epochs = 100,  batch_size = batch_size, validation_data=(X_valid, y_valid), callbacks = [monitor, metrics], shuffle=False)\n    if recall_score(y_test,np.round(model2.predict(X_test)).T[0]) > Maxrecall and precision_score(y_test,np.round(model2.predict(X_test)).T[0]) > Maxprecision:\n        print(recall_score(y_test,np.round(model2.predict(X_test)).T[0]), i)\n        Maxrecall = recall_score(y_test,np.round(model2.predict(X_test)).T[0])\n        Maxprecision = precision_score(y_test,np.round(model2.predict(X_test)).T[0])\n        if Maxrecall > Lastrecall:\n            Lastrecall = Maxrecall\n            model2.save('.\/best_model.h5')   \n#    metrics.get_data()","074f69ae":"# load model from single file\nmodel2 = load_model('best_model.h5')","5dff8bd9":"# Evaluating the Model\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model train vs validation loss'), \nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","2a1e34ae":"print(\"Loss: \", model2.evaluate(X_valid, y_valid, verbose=0))","0b9540ee":"from sklearn.metrics import recall_score","ad48453a":"from sklearn import metrics","2acffdb4":"probs = model2.predict_proba(X_valid)\npreds = probs[:,0]\nfpr, tpr, threshold = metrics.roc_curve(y_valid, preds)\nroc_auc = metrics.auc(fpr, tpr)","bdd1b76a":"plt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True  Positive rate')\nplt.xlabel('False Positive rate')\nplt.show()","3faf8a9d":"y2_predicted = np.round(model2.predict(X_test)).T[0]\ny2_correct = y_test","c5c6d5b0":"np.setdiff1d(y2_predicted, y2_correct)","18f3fe01":"inaccuracy, incorrect = estatisticas(y2_correct, y2_predicted)","bc940d8a":"print('Validation Results')\nprint(recall_score(y_valid,np.round(model2.predict(X_valid)).T[0]))\nprint('\\nTest Results')\nprint(1 - inaccuracy)\nprint(recall_score(y_test,np.round(model2.predict(X_test)).T[0]))","e3406a98":"print(incorrect)","2a25d686":"# Plot Confusion Matrix\nwarnings.filterwarnings('ignore')\n#plt.style.use('classic')\n#plt.figure(figsize=(5,5))\npretty_print_conf_matrix(y2_correct, y2_predicted, \n                         classes= ['0', '1'],\n                         normalize=False, \n                         title='Confusion Matrix')","8efac1b2":"### Aplicando a transforma\u00e7\u00e3o dos dados","7634cb9d":"### Gerando dados de Treino\n\nOs dados balanceados servir\u00e3o para gerar apenas o conjunto de dados treino, para que os dados sint\u00e9ticos gerados n\u00e3o vazem para os conjuntos de teste e valida\u00e7\u00e3o.","7634c1c3":"### Introdu\u00e7\u00e3o\nO objetivo desta an\u00e1lise \u00e9 criar um modelo de Rede Neural Profunda (Deep Learning) capaz prever, com o mais alto grau de precis\u00e3o poss\u00edvel, os atendimentos prop\u00edcios a ocorr\u00eancia de Readmiss\u00e3o Hospitalar.","60901f7d":"### Carga de dados","e117621f":"\n#### Referencias\n\nData Science Acabemy: Forma\u00e7\u00e3o Inteligencia Artificial           \nhttps:\/\/www.datascienceacademy.com.br\/pages\/formacao-inteligencia-artificial\n\nFator de Qualidade: dados de readmiss\u00e3o hospitalar devem ser informados \u00e0 ANS   \nhttp:\/\/www.ans.gov.br\/aans\/noticias-ans\/qualidade-da-saude\/3167-fator-de-qualidade-dados-de-readmissao-hospitalar-devem-ser-informados-a-ans\n\n3 formas \u00fanicas de diminuir a readmiss\u00e3o hospitalar       \nhttps:\/\/saudebusiness.com\/noticias\/3-formas-diminuir-readmissao-hospitalar\/\n\nResampling strategies for imbalanced datasets       \nhttps:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\n\nThe Right Way to Oversample in Predictive Modeling       \nhttps:\/\/beckernick.github.io\/oversampling-modeling\/\n\nScikit-learn - Confusion Matrix   \nhttp:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n","76169b2a":"### An\u00e1lise Explorat\u00f3ria","20947dab":"### Constru\u00e7\u00e3o do Modelo Preditivo","6a3784d2":"### Come\u00e7ando com uma Rede Neural Simples","83123a59":"Podemos observar que apesar do modelo est\u00e1 demonstrando uma acur\u00e1cia de aproximadamente 88%, analisando a ConfusionMatrix percebemos que os resultados n\u00e3o foram satisfat\u00f3rio, consequencia do desbalanciamento dos dados. Ent\u00e3o vamos aplicar um oversampling para corrigir um vi\u00e9s no conjunto de dados original, empregando Synthetic Minority Over-sampling Technique para balancear os dados. ","5d9ce925":"### Apresenta\u00e7\u00e3o dos dados\n\nPara realizar este trabalho utilizei um conjunto de dados dispon\u00edvel publicamente no reposit\u00f3rio da UCI [Link]( https:\/\/archive.ics.uci.edu\/ml\/datasets\/diabetes+130-us+hospitals+for+years+1999-2008# ). Este conjunto de dados representa 10 anos (1999-2008) de atendimento cl\u00ednico em 130 hospitais dos EUA, contendo 101.766 observa\u00e7\u00f5es em 10 anos. Inclui mais de 50 atributos, que representam caracter\u00edsticas do paciente, diagn\u00f3sticos, exames, etc. As informa\u00e7\u00f5es foram extra\u00eddas do banco de dados contendo registros que satisfizeram os seguintes crit\u00e9rios. \n \n* 1) Cada atendimento representa uma interna\u00e7\u00e3o hospitalar. \n* 2) Contempla apenas atendimentos de pacientes diab\u00e9ticos, ou seja, aquele em que qualquer tipo de diabetes foi introduzido no sistema como um diagn\u00f3stico. \n* 3) O tempo de perman\u00eancia foi de no m\u00ednimo 1 dia e no m\u00e1ximo 14 dias. \n* 4) Testes laboratoriais foram realizados durante o atendimento. \n* 5) Medicamentos foram administrados durante o atendimento.\n \nOs dados cont\u00eam atributos como n\u00famero do paciente, ra\u00e7a, g\u00eanero, idade, tipo de interna\u00e7\u00e3o, tempo no hospital, n\u00famero de exames laboratoriais realizados, resultado do exame de HbA1c, diagn\u00f3sticos, n\u00famero de medicamentos utilizados, se usa medicamentos para diab\u00e9tico e quais, n\u00famero de pacientes ambulatoriais , interna\u00e7\u00e3o e visitas de emerg\u00eancia no ano anterior \u00e0 hospitaliza\u00e7\u00e3o, etc. Alguns desses atributos ser\u00e3o desconsiderados nesta an\u00e1lise pois n\u00e3o ter\u00e3o relev\u00e2ncia para o objetivo do trabalho.\n","c383ecde":"###  Importando bibliotecas","d94b8ba0":"O indicador de Readmiss\u00e3o Hospitalar  mede a taxa de pacientes que retornaram ao hospital em at\u00e9 30 dias desde a \u00faltima alta hospitalar correspondente a primeira admiss\u00e3o. Este indicador avalia a capacidade progressiva do prestador de servi\u00e7o em ajudar na recupera\u00e7\u00e3o do paciente. Nos Estados unidos, v\u00e1rias iniciativas j\u00e1 foram tomadas para garantir o sucesso da recupera\u00e7\u00e3o da sa\u00fade de seus pacientes, usando t\u00e9cnicas de trabalho em equipe a tecnologia para diminuir a readmiss\u00e3o hospitalar.\n\nA taxa de readmiss\u00e3o hospitalar \u00e9 frequentemente usada como uma medida da qualidade assistencial de um hospital, segundo determina a ANS, o indicador de Readmiss\u00e3o Hospitalar \u00e9 um dos crit\u00e9rios para o estabelecimento alcan\u00e7ar o reajuste de 100% do IPCA, consequentemente uma alta taxa de readmiss\u00e3o pode afetar o \u00edndice de reajuste dos contratos firmados entre operadoras de planos de sa\u00fade e prestadores de servi\u00e7o. Segundo a diretora-adjunta de Desenvolvimento Setorial da ANS, Michelle Mello \u201cEsse \u00e9 um indicador internacional cl\u00e1ssico para avalia\u00e7\u00e3o da qualidade de atendimento e cuidado prestados ao paciente nos hospitais. Quanto menor for a reincid\u00eancia de interna\u00e7\u00e3o, ou seja, quanto menor for a readmiss\u00e3o potencialmente evit\u00e1vel, melhor \u00e9 considerado o atendimento prestado pela unidade hospitalar\u201d.\n\nUm dos grandes desafios dos hospitais \u00e9 identificar as readmiss\u00f5es que poderiam ser evitadas. Ser capaz de prever quais pacientes ser\u00e3o readmitidos pode ajudar os hospitais e operadoras de plano de sa\u00fade a economizar milh\u00f5es de reais e melhorar a qualidade dos cuidados e recupera\u00e7\u00e3o dos pacientes\n\nO objetivo deste trabalhe \u00e9 implementar um modelo de Deep Learning, capaz de classificar os pacientes que ser\u00e3o readmitidos, com o mais alto grau de precis\u00e3o poss\u00edvel. Um dos desafios ao analisar este conjunto de dados \u00e9 o enorme desequil\u00edbrio da vari\u00e1vel target: as readmiss\u00f5es com menos de 30 dias correspondem apenas 11,16% dos atendimentos. Nesse caso, \u00e9 muito pior ter falsos negativos do que falsos positivos em nossas previs\u00f5es, pois falsos negativos significam que algum paciente foi readmitido, por\u00e9m o modelo n\u00e3o foi capaz de prever, isso poder\u00e1 comprometer os idicadores de qualidade da institui\u00e7\u00e3o. ","7c810741":"#### Categoriza\u00e7\u00e3o de diagn\u00f3sticos\nNo conjunto de dados existem tr\u00eas diagnosticos, um principal e dois secund\u00e1rios, contendo em m\u00e9dia 752 codigos distintos em cada um, por isso resolvi realizar um reagrupamento com base numa an\u00e1lise realizada por Strack et al. em 2014, sobre o mesmo tema e utilizando o mesmo conjunto de dados, publicado em ( https:\/\/www.hindawi.com\/journals\/bmri\/2014\/781670\/abs\/ ). ","26558f1a":"Conforme consta na documenta\u00e7\u00e3o, trata-se de um do conjunto de dados contendo atendimentos onde qualquer tipo de diabetes foi introduzido no sistema como um diagn\u00f3stico, ent\u00e3o eliminarei as observa\u00e7\u00f5es onde n\u00e3o existe nenhum diagn\u00f3stico registrado.","0a6fd5f9":"### Aumentando o N\u00famero de Camadas na Rede Neural","4996242f":"### Limpeza e Transforma\u00e7\u00e3o dos dados\n\nOs crit\u00e9rios de elimina\u00e7\u00e3o de atributos e observa\u00e7\u00f5es depende muito da interpreta\u00e7\u00e3o dos dados feita pelo Cientista de Dados na fase de explora\u00e7\u00e3o. Estes crit\u00e9rios passam pela avalia\u00e7\u00e3o de cada atributo do conjunto de dados, verifica\u00e7\u00e3o da distribui\u00e7\u00e3o de frequ\u00eancia, analise de correla\u00e7\u00f5es entre vari\u00e1veis, modelo preditivo que ser\u00e1 aplicado, al\u00e9m de um certo conhecimento do neg\u00f3cio em estudo, para ent\u00e3o decidir quais atributos e\/ou observa\u00e7\u00f5es devem ser descartados. Considerando o conjunto de dados em quest\u00e3o, decidi eliminar alguns atributos nos quais avalio que n\u00e3o impactar\u00e1 no resultado das an\u00e1lises preditivas. Descartarei os atributos \"encounter_id\", \"patient_nbr\", \"weight\", \"payer_code\", \"examide\", \"citoglipton\" e \"medical_specialty\". Por exemplo o atributo \"weight\", que corresponde ao peso do paciente, seria um atributo muito importante a ser considerado na an\u00e1lise, por\u00e9m em 97% das observa\u00e7\u00e3o este atributo est\u00e1 sem valor, tornando-se um dado insuficientemente consistente para o modelo aplicar algum tipo de generaliza\u00e7\u00e3o.\n\nCom base na consulta a documenta\u00e7\u00e3o disponibilizada pelo reposit\u00f3rio dos dados e entendimento de cada atributo do conjunto de dados, decidi tamb\u00e9m eliminar algumas observa\u00e7\u00f5es que acredito n\u00e3o impactar no objetivo proposta neste trabalho. Precisarei tamb\u00e9m transformar alguns dados, com o objetivo de prepar\u00e1-los para serem entregue ao modelo preditivo proposto.","c032d8dd":"### por Antonildo Santos ","43d1c503":"### Conclus\u00e3o\nO modelo de Deep Learning conseguiu atingir um excelente \u00edndice de acur\u00e1cia no recall da classe minorit\u00e1ria, demonstrando ser uma ferramenta eficaz na identifica\u00e7\u00e3o antecipada de pacientes que necessitar\u00e3o de uma maior aten\u00e7\u00e3o da equipe assistencial, por possuir uma alta probabilidade de ocorr\u00eancia de readmiss\u00e3o.\n","1e56177a":"## Previs\u00e3o de readmiss\u00e3o hospitalar utilizando modelo de Deep Learning","ac903b58":"### Aplicando Oversampling"}}