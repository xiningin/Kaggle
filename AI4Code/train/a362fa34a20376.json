{"cell_type":{"94723ecc":"code","a73c0d26":"code","a37fbe67":"code","0280285e":"code","f0660d97":"code","a4ca9322":"code","cd68db25":"code","bec481ae":"code","2568d3fa":"markdown","5d8acbd1":"markdown","b16485ac":"markdown","859d6ec8":"markdown","48871b80":"markdown"},"source":{"94723ecc":"%%capture\n! pip install -U keras-tuner\n! pip install wandb","a73c0d26":"import kerastuner as kt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport wandb","a37fbe67":"def build_model(hp):\n    \"\"\"\n    Builds a convolutional model.\n    \n    Args:\n      hp: Hyperparamet object, This is the object that helps\n        us sample hyperparameter for a particular trial.\n    \n    Returns:\n      model: Keras model, Returns a keras model.\n    \"\"\"\n    inputs = tf.keras.Input(shape=(28, 28, 1))\n    x = inputs\n    # In this example we also get to look at\n    # conditional heyperparameter settings.\n    # Here the `kernel_size` is conditioned\n    # with the for loop counter. \n    for i in range(hp.Int('conv_layers', 1, 3)):\n        x = tf.keras.layers.Conv2D(\n            filters=hp.Int('filters_' + str(i), 4, 32, step=4, default=8),\n            kernel_size=hp.Int('kernel_size_' + str(i), 3, 5),\n            activation='relu',\n            padding='same')(x)\n        # choosing between max pool and avg pool\n        if hp.Choice('pooling' + str(i), ['max', 'avg']) == 'max':\n            x = tf.keras.layers.MaxPooling2D()(x)\n        else:\n            x = tf.keras.layers.AveragePooling2D()(x)\n        \n        x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.ReLU()(x)\n\n    if hp.Choice('global_pooling', ['max', 'avg']) == 'max':\n        x = tf.keras.layers.GlobalMaxPooling2D()(x)\n    else:\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n    \n    model = tf.keras.Model(inputs, outputs)\n    return model","0280285e":"class MyTuner(kt.Tuner):\n  \"\"\"\n  Custom Tuner subclassed from `kt.Tuner`\n  \"\"\"\n  def run_trial(self, trial, train_ds):\n    \"\"\"\n    The overridden `run_trial` function\n\n    Args:\n      trial: The trial object that holds information for the\n        current trial.\n      train_ds: The training data.\n    \"\"\"\n    hp = trial.hyperparameters\n    # Batching the data\n    train_ds = train_ds.batch(\n        hp.Int('batch_size', 32, 128, step=32, default=64))\n    # The models that are created\n    model = self.hypermodel.build(trial.hyperparameters)\n    # Learning rate for the optimizer\n    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log', default=1e-3)\n\n    if hp.Choice('optimizer', ['adam', 'sgd']) == 'adam':\n        optimizer = tf.keras.optimizers.Adam(lr)\n    else:\n        optimizer = tf.keras.optimizers.SGD(lr)\n\n    epoch_loss_metric = tf.keras.metrics.Mean()\n\n    # build the train_step\n    @tf.function\n    def run_train_step(data):\n        \"\"\"\n        The run step\n        \n        Args:\n            data: the data that needs to be fit\n        \n        Returns:\n            loss: Returns the loss for the present batch\n        \"\"\"\n        images = tf.dtypes.cast(data['image'], 'float32') \/ 255.\n        labels = data['label']\n        with tf.GradientTape() as tape:\n            logits = model(images)\n            loss = tf.keras.losses.sparse_categorical_crossentropy(\n                labels, logits)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        epoch_loss_metric.update_state(loss)\n        return loss\n    \n    # WANDB INITIALIZATION\n    # Here we pass the configuration so that\n    # the runs are tagged with the hyperparams\n    # This also directly means that we can\n    # use the different comparison UI widgets in the \n    # wandb dashboard off the shelf.\n    run = wandb.init(entity='entity', project='project', config=hp.values)\n    for epoch in range(10):\n        self.on_epoch_begin(trial, model, epoch, logs={})\n        for batch, data in enumerate(train_ds):\n            self.on_batch_begin(trial, model, batch, logs={})\n            batch_loss = run_train_step(data)\n            self.on_batch_end(trial, model, batch, logs={'loss': batch_loss})   \n            if batch % 100 == 0:\n                loss = epoch_loss_metric.result().numpy()\n                # Log the batch loss for WANDB\n                run.log({f'e{epoch}_batch_loss':loss})\n        # Epoch loss logic\n        epoch_loss = epoch_loss_metric.result().numpy()\n        # Log the epoch loss for WANDB\n        run.log({'epoch_loss':epoch_loss, 'epoch':epoch})\n        # `on_epoch_end` has to be called so that \n        # we can send the logs to the `oracle` which handles the\n        # tuning.\n        self.on_epoch_end(trial, model, epoch, logs={'loss': epoch_loss})\n        epoch_loss_metric.reset_states()\n    # Finish the wandb run\n    run.finish()        ","f0660d97":"# Initialize the tuner\ntuner = MyTuner(\n    oracle=kt.oracles.BayesianOptimization(\n        objective=kt.Objective('loss', 'min'),\n        max_trials=10),\n    hypermodel=build_model,\n    directory='results',\n    project_name='mnist_custom_training')\n\nmnist_data = tfds.load('mnist')\nmnist_train, mnist_test = mnist_data['train'], mnist_data['test']\nmnist_train = mnist_train.shuffle(1000)\n\n# Search the space of hyperparams\ntuner.search(train_ds=mnist_train)","a4ca9322":"import pprint\npp = pprint.PrettyPrinter(indent=4)","cd68db25":"best_hps = tuner.get_best_hyperparameters()[0]\npp.pprint(best_hps.values)","bec481ae":"best_model = tuner.get_best_models()[0]\nbest_model.summary()","2568d3fa":"# Installation\nThe following packages are not provided by Colab hence must be downloaded by `pip`:\n1. keras-tuner\n2. wandb","5d8acbd1":"# Best\nHere we will harness the best hyperparameters from the search space.","b16485ac":"# Building\nWe can build keras models in two ways which helps us integrating the `kerastuner` library off the shelf.\n1. building methods via functions\n2. subclassing `HyperModel` and use the kerastuner in the `build` method\n\nWith this example code, we will build the model with a function.","859d6ec8":"# Tuner\nThe essesnce of the keras tuner package lies in the `tuner`.","48871b80":"# Imports\nThe following packages are used in the notebook:\n1. kerastuner\n2. tensorflow\n3. tensorflow_datasets\n4. wandb"}}