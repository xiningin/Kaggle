{"cell_type":{"e8bfdca6":"code","af135836":"code","804cf8ba":"code","d936a518":"code","5510103c":"code","e326880c":"code","978e9c1e":"code","2bb63f9c":"code","6dbc23d1":"code","b5b8893c":"code","2a6d7ddf":"code","d5256ebe":"code","c31d8feb":"code","9e351f51":"code","f2460e6c":"code","0dda5300":"code","1e1b2209":"code","a521dd57":"code","02ed9971":"code","3d0bf3a8":"code","42a16654":"code","f198d84c":"code","c7104a33":"code","3bf0cd1e":"code","a0c309b6":"code","f841460f":"code","a277ddc5":"code","eeb26157":"code","47a5b222":"markdown","3911930b":"markdown","e5b6d615":"markdown"},"source":{"e8bfdca6":"def meta_define():\n    \"\"\"\n    input : none\n    output : Corrected metadata\n    \"\"\"\n    import pandas as pd\n    import os \n    #os.chdir(\"\/kaggle\/input\/hah-data-science-challenge\/\")\n    df_train = pd.read_csv(\"..\/input\/hah-data-science-challenge\/train.csv\", index_col=False)\n    df_test = pd.read_csv(\"..\/input\/hah-data-science-challenge\/test.csv\",index_col=False)\n    \n    ##################################################\n    #\u4ee5\u4e0b\u8f9e\u66f8\u3084\u5909\u6570\u306e\u5b9a\u7fa9\n    #\u5404\u30c7\u30fc\u30bf\u4fee\u6b63\u7528\u306e\u8f9e\u66f8\n    bolt_dict = {\n        '\u5927':\"big\",\n        '\u5c0f':\"small\"\n    }\n\n    plate_dict = {\n        '\u5927':\"big\",\n        '\u5c0f':\"small\"\n    }\n\n    record_dict = {\n        'PC\u5185\u81d3':\"pc_built_in\",\n        'PC\u5185\u8535':\"pc_built_in\",\n        'USB1':\"usb1\", \n        'USB2':\"usb2\", \n        'USB3':\"usb3\", \n        'USB4':\"usb4\", \n        '\u30b9\u30de\u30db':\"smart_phone\",\n        '\u30b9\u30de\u30db\u306e\u30dc\u30a4\u30b9\u30ec\u30b3\u30fc\u30c0':\"smart_phone\",\n        '\u5185\u8535\u30de\u30a4\u30af':\"pc_built_in\",\n        }\n\n    distance_dict = {\n        '10cm': 0.1, \n        '10\u339d': 0.1, \n        '1M': 1.0, \n        '20cm': 0.2, \n        '20\u339d': 0.2, \n        '2M': 2.0, \n        '2m': 2.0, \n        '30cm': 0.3, \n        '30cn': 0.3, \n        '30\u339d': 0.3, \n        '3m': 3.0, \n        '40cm': 0.4, \n        '40\u339d': 0.4, \n        '50cm': 0.5, \n        '50\u339d': 0.5, \n        '5cm': 0.05,\n        '8cm': 0.08, \n        '\uff11\uff2d': 1.0   \n    }\n\n    cvt_dict = {\n        \"\u306d\u3058\" : bolt_dict, \n        '\u30d7\u30ec\u30fc\u30c8' : plate_dict, \n        '\u9332\u97f3\u65b9\u6cd5' : record_dict, \n        '\u30de\u30a4\u30af\u8ddd\u96e2' : distance_dict\n    }\n    \n    #df_train\u65e5\u672c\u8a9e\u30ab\u30e9\u30e0\u540d : ['ID', '\u306d\u3058', '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2', '\u30d5\u30a1\u30a4\u30eb', 'Target']\n    col_train = ['id', 'bolt', 'plate', 'record', 'mic_dist', 'file', 'Target']\n    #df_test\u65e5\u672c\u8a9e\u30ab\u30e9\u30e0\u540d : ['ID', '\u306d\u3058', '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2', '\u30d5\u30a1\u30a4\u30eb', 'Target']\n    col_test = ['id', 'bolt', 'plate', 'record', 'mic_dist', 'file']\n    \n    tgt_col = [\"\u306d\u3058\", '\u30d7\u30ec\u30fc\u30c8', '\u9332\u97f3\u65b9\u6cd5', '\u30de\u30a4\u30af\u8ddd\u96e2']\n    ##################################################\n    \n    for col in tgt_col:#Target\u306f\u5909\u63db\u5bfe\u8c61\u5916\n        df_train[col] = df_train[col].map(cvt_dict[col])\n        df_test[col] = df_test[col].map(cvt_dict[col])\n        \n    df_train.columns = col_train\n    df_test.columns = col_test\n    \n    return df_train, df_test","af135836":"!pip install japanize-matplotlib","804cf8ba":"import torch\nimport torchvision\nfrom torchvision import transforms, models\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torch.optim as optim\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport umap\nimport category_encoders as ce\nimport os\nimport japanize_matplotlib","d936a518":"df_train, df_test = meta_define()\n\n###################\ndf_train[\"flag\"] = \"\"\ndf_train.loc[df_train[\"Target\"]==0, \"flag\"] = \"normal\"\ndf_train.loc[df_train[\"Target\"]==1, \"flag\"] = \"anormal\"\ndf_train.loc[df_train.Target.isnull()==True, \"flag\"] = \"unlabel\"\ndf_test['flag'] = 'test'\n###################\n\n#\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u4f5c\u6210##################\ndf = pd.concat([df_train, df_test])\ndf[\"class\"] = df[\"bolt\"]+\"_\"+df[\"plate\"] + \"_\" + df[\"record\"]\n\n\n#\u4f5c\u6210\u3057\u305f\u6b63\u89e3\u30e9\u30d9\u30eb\u3092\u6570\u5024\u306b\u30a8\u30f3\u30b3\u30fc\u30c9#####\ncate_col = ['class']\noe = ce.OrdinalEncoder(cols=cate_col, \n                       drop_invariant=True)\noe_df = oe.fit_transform(df[cate_col])\noe_df.columns = [\"class_id\"]\ndf = pd.concat([df, oe_df], axis=1)\ndf[\"class_id\"] = df[\"class_id\"]-1#oe\u3067\u306f1\u59cb\u307e\u308a\u3067\u30a8\u30f3\u30b3\u30fc\u30c9\u3055\u308c\u308b\u306e\u3067\u30010\u59cb\u307e\u308a\u3068\u3059\u308b\u305f\u3081\u30011\u3092\u5f15\u304f\nsns.countplot(data = df, x=\"class_id\")\nplt.title(\"\u5168\u30c7\u30fc\u30bf\u306e\u30e9\u30d9\u30eb\u5206\u5e03\")\n\ndf.head()\n","5510103c":"df_normal = df[df[\"flag\"]==\"normal\"].copy()\ndf_normal = df_normal.reset_index(drop=True)\n\ndf_anormal = df[df[\"flag\"]==\"anormal\"].copy()\ndf_anormal = df_anormal.reset_index(drop=True)\n\ndf_unlabel = df[df[\"flag\"]==\"unlabel\"].copy()\ndf_unlabel = df_unlabel.reset_index(drop=True)\n\ndf_test = df[df[\"flag\"]==\"test\"].copy()\ndf_test = df_test.reset_index(drop=True)\n\nsns.countplot(data = df_normal, x=\"class_id\")\nplt.title(\"\u6b63\u5e38\u30e9\u30d9\u30eb\u304c\u4ed8\u4e0e\u3055\u308c\u305f\u30c7\u30fc\u30bf\u306e\u30e9\u30d9\u30eb\u5206\u5e03\")\nprint(\"df_normal.shape :\", df_normal.shape, \"df_anormal.shape :\", df_anormal.shape, \"df_unlabel.shape :\", df_unlabel.shape, \"df_test.shape :\", df_test.shape)","e326880c":"#\u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u306e\u4f5c\u6210\nclass HahDataset(Dataset):\n    def __init__(self, df,train=True):\n        self.df = df\n        self.train = train\n        self.label = df[\"class_id\"].to_list()\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        tgt_label = self.label[index]\n        row = self.df.loc[index]\n        path = row[\"file\"].split(\".\")[0]\n        if self.train:\n            img = np.load(f\"..\/input\/20210911-feature-mel-256\/train\/{path}.npy\")\n        else:\n            img = np.load(f\"..\/input\/20210911-feature-mel-256\/test\/{path}.npy\")\n        img = img.reshape(-1,img.shape[0],img.shape[1])\n        return torch.tensor(img).float(), tgt_label","978e9c1e":"#train, val\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\ntorch.manual_seed(1234)\nnp.random.seed(1234)\n\ntrainset = HahDataset(df_normal)\ntrain_indices, val_indices = train_test_split(list(range(len(trainset.label))), test_size=0.1, stratify=trainset.label)\n\nprint(len(train_indices), len(val_indices))\n\ntrain_dataset = torch.utils.data.Subset(trainset, train_indices)\nval_dataset = torch.utils.data.Subset(trainset, val_indices)\n\ntrain_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)\ndataloaders_dict = {\"train\":train_data_loader, \"val\":val_data_loader}","2bb63f9c":"#anormal, unlabel, test\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u4f5c\u6210\nanormal_dataset = HahDataset(df_anormal)\nanormal_data_loader = torch.utils.data.DataLoader(anormal_dataset, batch_size=16, shuffle=False)\n\nunlabel_dataset = HahDataset(df_unlabel)\nunlabel_data_loader = torch.utils.data.DataLoader(unlabel_dataset, batch_size=16, shuffle=False)\n\ntest_dataset = HahDataset(df_test, train=False)\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)\n\nprint(len(anormal_dataset), len(unlabel_dataset), len(test_dataset))","6dbc23d1":"#\u8a13\u7df4\u5bfe\u8c61\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5b9a\u7fa9\ndef res50(out_features=100):\n    model = models.resnet50(pretrained=False)\n    model.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    num_ftrs= model.fc.in_features #num_ftrs=2048\n    model.fc = nn.Linear(num_ftrs, 100)\n    return model\n      \nclass ArcFace(nn.Module):\n    def __init__(self, num_features, num_classes, s=30.0, m=0.50):\n        super(ArcFace, self).__init__()\n        self.num_features = num_features\n        self.n_classes = num_classes\n        self.s = s\n        self.m = m\n        self.W = nn.Parameter(torch.FloatTensor(num_classes, num_features))\n        nn.init.xavier_uniform_(self.W)\n\n    def forward(self, input, label=None):\n        # normalize features\n        x = F.normalize(input)\n        # normalize weights\n        W = F.normalize(self.W)\n        # dot product\n        logits = F.linear(x, W)\n        if label is None:\n            return logits\n        # add margin\n        theta = torch.acos(torch.clamp(logits, -1.0 + 1e-7, 1.0 - 1e-7))\n        target_logits = torch.cos(theta + self.m)\n        one_hot = torch.zeros_like(logits)\n        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n        output = logits * (1 - one_hot) + target_logits * one_hot\n        # feature re-scale\n        output *= self.s\n\n        return output","b5b8893c":"#\u52d5\u4f5c\u30c6\u30b9\u30c8\u7528\u306b\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\n#\u7279\u5fb4\u91cf\u62bd\u51fa\u5668(resnet50)\u3067100\u6b21\u5143\u307e\u3067\u5727\u7e2e\u3057\u3001\u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u3092arcface\u3067\u5206\u985e\u3059\u308b\u3002\nmodel = res50()#\u7279\u5fb4\u91cf\u4f5c\u6210\u90e8\u5206\nmetric_fc = ArcFace(model.fc.out_features, num_classes=11)\n#\u5165\u529b\u3068\u51fa\u529b\u306e\u78ba\u8a8d\nsample = torch.randn(16, 1, 256, 256 )\nout = model(sample)\nprint(\"resnet_out.shape : \", out.shape)\nout = metric_fc(out)\nprint(\"metric_fc_out.shape : \", out.shape)","2a6d7ddf":"#\u5b66\u7fd2\u7528\u306e\u95a2\u6570\ndef train_model(net, adacos, criterion, dataloaders_dict, optimizer, num_epochs):\n    log = pd.DataFrame(index=[],\n                   columns=[ 'epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n    \n    train_loss = None\n    train_acc = None\n    \n    best_loss = float('inf')\n    not_improvement = 0\n    \n    for epoch in range(num_epochs):\n        print(f\"Epoch {epoch+1}\/{num_epochs}, Best_loss : {best_loss}\")\n        print(\"not_improvement : \", not_improvement)\n        if not_improvement >= 5:\n            print(\"Done!!\")\n            break\n        \n        for phase in [\"train\",\"val\"]:\n            if phase == \"train\":\n                net.train()\n                adacos.train()\n            else:\n                net.eval()\n                adacos.train()\n                \n            epoch_loss = 0.0\n            epoch_corrects = 0\n      \n            if (epoch==0) and (phase==\"train\"):\n              continue\n                \n            for inputs, labels in tqdm(dataloaders_dict[phase]):\n                optimizer.zero_grad()\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                with torch.set_grad_enabled(phase==\"train\"):\n                    out = net(inputs)\n                    \n                    adacos_train_out = adacos(out, labels)#adacos\u306e\u51fa\u529b(\u5b66\u7fd2\u306b\u5229\u7528)\n                    loss = criterion(adacos_train_out, labels)\n                    \n                    _, preds= torch.max(adacos(out), 1)#\u5358\u7d14\u306a\u5185\u7a4d(\u63a8\u8ad6\u306b\u5229\u7528)\n                    \n                    if phase == \"train\":\n                        loss.backward()\n                        optimizer.step()\n                        \n                        \n                    epoch_loss += loss.item() * inputs.shape[0]\n                    epoch_corrects += torch.sum(preds == labels.data)\n                    \n            epoch_loss = epoch_loss\/len(dataloaders_dict[phase].dataset)\n            epoch_acc = epoch_corrects.double() \/ len(dataloaders_dict[phase].dataset)\n        \n            \n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n            if phase == \"train\":\n                train_loss = epoch_loss\n                train_acc = float(epoch_acc.cpu().numpy())\n            else:\n                val_loss = epoch_loss\n                val_acc = float(epoch_acc.cpu().numpy())\n                \n                if epoch_loss < best_loss:\n                    print(\"saving_weight\")\n                    os.makedirs(f\".\/weight\", exist_ok=True)\n                    torch.save(model.state_dict(), \".\/weight\/resnet50_cat_10.pth\") \n                    torch.save(metric_fc.state_dict(), \".\/weight\/arcface_cat_10.pth\") \n                    not_improvement = 0\n                    best_loss = epoch_loss\n                else:\n                    not_improvement += 1\n                    \n        tmp = pd.Series([\n            epoch+1,\n            train_loss,\n            train_acc,\n            val_loss,\n            val_acc,\n            ], index=[ 'epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc'])\n        \n        log = log.append(tmp, ignore_index=True)\n        os.makedirs(f\".\/log\", exist_ok=True)\n        log.to_csv(\".\/log\/resnet_50_cat_11.csv\", index=False)\n                    \n    return log","d5256ebe":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","c31d8feb":"#\u8a13\u7df4\u5bfe\u8c61\u306eNN\u3092\u30a4\u30f3\u30b9\u30bf\u30f3\u30b9\u5316\nmodel = res50().to(device)#\u7279\u5fb4\u91cf\u4f5c\u6210\u90e8\u5206\nmetric_fc = ArcFace(model.fc.out_features, num_classes=11).to(device)","9e351f51":"#optimizer\u306e\u8a2d\u5b9a\u306f\u66ab\u5b9a\u306a\u306e\u3067\u3001\u6539\u826f\u306e\u4f59\u5730\u3042\u308a\u3002\noptimizer = optim.Adam(list(model.parameters()) \n                       + list(metric_fc.parameters()))\ncriterion = nn.CrossEntropyLoss()\nlog = train_model(model, metric_fc, criterion, dataloaders_dict, optimizer, 10000)","f2460e6c":"#\u5b66\u7fd2\u306e\u7d50\u679c\u306e\u78ba\u8a8d\n#val\u306e\u5206\u6563\u304c\u5927\u304d\u3044\u306e\u3067\u3001lr\u3092\u5c0f\u3055\u304f\u3059\u308b\u3001\u5c64\u6bce\u306b\u5b66\u7fd2\u7387\u3092\u5909\u3048\u308b\u3001\u30b9\u30b1\u30b8\u30e5\u30fc\u30e9\u3092\u4f7f\u3046\u306a\u3069\u3001\u6539\u826f\u306e\u4f59\u5730\u3042\u308a\u3002\nresult_df = pd.read_csv(\".\/log\/resnet_50_cat_11.csv\")\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nsns.lineplot(data=result_df, x=\"epoch\", y = \"train_loss\", ax=axes[0])\nsns.lineplot(data=result_df, x=\"epoch\", y = \"val_loss\", ax=axes[0])\naxes[0].legend(labels=[\"train_loss\",\"val_loss\"])\n\nsns.lineplot(data=result_df, x=\"epoch\", y = \"train_acc\", ax=axes[1])\nsns.lineplot(data=result_df, x=\"epoch\", y = \"val_acc\", ax=axes[1])\naxes[1].legend(labels=[\"train_acc\",\"val_acc\"])","0dda5300":"def check_train_res(net, adacos, dataloader):\n    \"\"\"\n    \u8a13\u7df4\u30c7\u30fc\u30bf\u7d50\u679c\u306e\u78ba\u8a8d\u7528\u95a2\u6570\n    \"\"\"\n    correct_label = []\n    pred_label = []\n    for inputs, labels in tqdm(dataloader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        out = net(inputs)\n        _, preds= torch.max(adacos(out), 1)\n        correct_label.extend(list(labels.cpu().numpy()))\n        pred_label.extend(list(preds.cpu().numpy()))\n    \n    return correct_label, pred_label","1e1b2209":"#\u6df7\u540c\u884c\u5217\u3092\u4f5c\u6210\u3057\u3066\u3001\u5b66\u7fd2\u7d50\u679c\u3092\u78ba\u8a8d\n#\u307e\u3042\u307e\u3042\u3044\u3044\u611f\u3058\ncorrect_label, pred_label = check_train_res(model, metric_fc, val_data_loader)\ncm = confusion_matrix(correct_label, pred_label)\nsns.heatmap(cm, square=True, cbar=True, annot=True, cmap='Blues')","a521dd57":"def embedding_data(net, dataloader):\n    \"\"\"\n    \u7279\u5fb4\u91cf\u62bd\u51fa\u5668\u306e\u63a8\u8ad6\u7d50\u679c\u3092\u8fd4\u3059\u95a2\u6570\n    Umap\u3067\u306e\u53ef\u8996\u5316\u7528\n    \"\"\"\n    correct_label = []\n    embed_feat = []\n    for inputs, labels in tqdm(dataloader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        out = net(inputs)\n        correct_label.extend(list(labels.cpu().numpy()))\n        embed_feat.extend(list(out.cpu().detach().numpy()))\n\n    return np.array(embed_feat), np.array(correct_label)\n  \ndef convert_unit_vec(feature):\n    \"\"\"\n    \u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3059\u308b\u95a2\u6570\n    (Umap)\u3067\u306e\u53ef\u8996\u5316\u306b\u5229\u7528\n    \"\"\"\n    #\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u5909\u63db\u7528\u306e\u95a2\u6570\n    norm = np.linalg.norm(feature, axis=1)\n    feature_init_vec = feature \/ norm.reshape(len(feature),  -1)\n    return feature_init_vec\n\ndef calc_cossim(net, adacos, dataloader):\n    \"\"\"\n    \u6b63\u89e3\u306e\u30d9\u30af\u30c8\u30eb\u3068\u306e\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\u3059\u308b\u95a2\u6570\n    \"\"\"\n    correct_label = []\n    calc_res = []\n    for inputs, labels in tqdm(dataloader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        out = net(inputs)\n        ret = adacos(out)\n        correct_label.extend(list(labels.cpu().numpy()))\n        calc_res.extend(list(ret.cpu().detach().numpy()))\n        \n    calc_res = np.array(calc_res)\n    correct_label = np.array(correct_label)\n        \n    ret = []\n    for res, c_label in zip(calc_res, correct_label):\n        tgt_cossim = res[c_label]\n        ret.append(tgt_cossim)\n    \n    return np.array(ret)","02ed9971":"train_embed_feat, train_correct_label = embedding_data(model, train_data_loader)\nprint(train_embed_feat.shape)\nprint(train_correct_label.shape)\n\nval_embed_feat, val_correct_label = embedding_data(model, val_data_loader)\nprint(val_embed_feat.shape)\nprint(val_correct_label.shape)\n\nanormal_embed_feat, anormal_correct_label = embedding_data(model, anormal_data_loader)\nprint(anormal_embed_feat.shape)\nprint(anormal_correct_label.shape)\n\nunlabel_embed_feat, unlabel_correct_label = embedding_data(model, unlabel_data_loader)\nprint(unlabel_embed_feat.shape)\nprint(unlabel_correct_label.shape)\n\ntest_embed_feat, test_correct_label = embedding_data(model, test_data_loader)\nprint(test_embed_feat.shape)\nprint(test_correct_label.shape)","3d0bf3a8":"#umap\u3067\u306e\u53ef\u8996\u5316\u306b\u3042\u305f\u308a\u3001\u5358\u4f4d\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\ntrain_embed_feat_unit = convert_unit_vec(train_embed_feat)\nval_embed_feat_unit = convert_unit_vec(val_embed_feat)\nanormal_embed_feat_unit = convert_unit_vec(anormal_embed_feat)\nunlabel_embed_feat_unit = convert_unit_vec(unlabel_embed_feat)\ntest_embed_feat_unit = convert_unit_vec(test_embed_feat)","42a16654":"reducer = umap.UMAP(random_state=1234)\nreducer.fit(train_embed_feat_unit)","f198d84c":"embedding_train = reducer.transform(train_embed_feat_unit)\nembedding_val = reducer.transform(val_embed_feat_unit)\nembedding_anormal = reducer.transform(anormal_embed_feat_unit)\nembedding_unlabel = reducer.transform(unlabel_embed_feat_unit)\nembedding_test = reducer.transform(test_embed_feat_unit)","c7104a33":"embedding_anormal\nprint(anormal_correct_label)","3bf0cd1e":"# umap\u3067\u53ef\u8996\u5316\nfig, axes = plt.subplots(3,2, figsize=(2*14,3*14), sharex=True, sharey=True)\n\nsns.scatterplot(data=pd.DataFrame(embedding_train), x=0, y=1, hue=train_correct_label, ax=axes[0,0], s=20, palette='bright')\naxes[0,0].scatter(embedding_anormal[0,0], embedding_anormal[0,1], c=\"Red\", s=50, marker=\"$9$\")\naxes[0,0].scatter(embedding_anormal[1,0], embedding_anormal[1,1], c=\"Red\", s=50, marker=\"$10$\")\naxes[0,0].set_title(\"\u6b63\u5e38\u30c7\u30fc\u30bf\u3068\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u5206\u5e03(\u7570\u5e38\u30c7\u30fc\u30bf\u306f\u6570\u5b57\u3067\u8a18\u8f09)\")\n\n\nsns.scatterplot(data=pd.DataFrame(embedding_val), x=0, y=1, hue=val_correct_label, ax=axes[0,1], s=20, palette='bright')\naxes[0,1].set_title(\"\u8a55\u4fa1\u30c7\u30fc\u30bf\u306e\u5206\u5e03\")\n\nsns.scatterplot(data=pd.DataFrame(embedding_anormal), x=0, y=1, hue=anormal_correct_label, ax=axes[1,0], s=20, palette='bright')\naxes[1,0].set_title(\"\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u307f\u306e\u5206\u5e03\")\n\nsns.scatterplot(data=pd.DataFrame(embedding_unlabel), x=0, y=1, hue=unlabel_correct_label, ax=axes[1,1], s=20, palette='bright')\naxes[1,1].set_title(\"\u30e9\u30d9\u30eb\u306a\u3057\u30c7\u30fc\u30bf\u306e\u5206\u5e03\")\n\nsns.scatterplot(data=pd.DataFrame(embedding_test), x=0, y=1, hue=test_correct_label, ax=axes[2,0], s=20, palette='bright')\naxes[1,1].set_title(\"\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u5206\u5e03\")","a0c309b6":"train_cossim= calc_cossim(model, metric_fc, train_data_loader)\nval_cossim= calc_cossim(model, metric_fc, val_data_loader)\nanormal_cossim= calc_cossim(model, metric_fc, anormal_data_loader)\nunlabel_cossim= calc_cossim(model, metric_fc, unlabel_data_loader)\ntest_cossim= calc_cossim(model, metric_fc, test_data_loader)","f841460f":"fig, axes = plt.subplots(3, 2, sharex=True, figsize=(10, 15))\naxes[0,0].hist(train_cossim, bins=50, range=(-1, 1))\naxes[0,0].set_title(\"\u6b63\u5e38\u30c7\u30fc\u30bf\u306e\u5206\u5e03\")\n\naxes[0,1].hist(val_cossim, bins=50, range=(-1, 1))\naxes[0,1].set_title(\"\u8a55\u4fa1\u30c7\u30fc\u30bf\u306e\u5206\u5e03\")\n\naxes[1,0].hist(anormal_cossim, bins=50, range=(-1, 1))\naxes[1,0].set_title(\"\u7570\u5e38\u30c7\u30fc\u30bf\u306e\u5206\u5e03\")\n\naxes[1,1].hist(unlabel_cossim, bins=50, range=(-1, 1))\naxes[1,1].set_title(\"\u30e9\u30d9\u30eb\u306a\u3057\u30c7\u30fc\u30bf\u306e\u5206\u5e03\")\n\naxes[2,0].hist(test_cossim, bins=50, range=(-1, 1))\naxes[1,1].set_title(\"\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u5206\u5e03\")\n\nplt.show()\nprint(anormal_cossim)","a277ddc5":"#\u7570\u5e38\u5ea6\u306b\u5909\u63db\nscore_test = -1*test_cossim\n\nscore_test_std = (score_test - score_test.min()) \/ (score_test.max() - score_test.min())\nplt.hist(score_test_std)\nplt.title(\"\u7570\u5e38\u5ea6\u306e\u5206\u5e03\")\nplt.show()","eeb26157":"df_sub = pd.read_csv('\/kaggle\/input\/hah-data-science-challenge\/sample_submission.csv', index_col=0)\ndf_sub['Target'] = score_test_std\ndf_sub.to_csv('\/kaggle\/working\/submission.csv')\ndf_sub.head()","47a5b222":"### \u3053\u306enotebook\u3067\u5b9f\u65bd\u3057\u3066\u3044\u308b\u3053\u3068\n- \u30c7\u30a3\u30b9\u30ab\u30c3\u30b7\u30e7\u30f3\u3067\u3054\u7d39\u4ecb\u3044\u305f\u3060\u3044\u305f\u3001\u4ee5\u4e0b\u306e\u624b\u6cd5\u3092\u5b9f\u88c5\n    - https:\/\/www.youtube.com\/watch?v=u2O69EIGb30\n    - \u6982\u8981\u306f\u7570\u5e38\u691c\u77e5\u3092\u5206\u985e\u554f\u984c\u306b\u66f8\u304d\u63db\u3048\u308b\u3053\u3068\n    - \u52d5\u753b\u5185\u3067\u306f\u3001\u8907\u6570\u306e\u6a5f\u5668\u306e\u7570\u5e38\u3092\u691c\u77e5\u3059\u308b\u969b\u3001\u30c7\u30fc\u30bf\u304b\u3089\u3001\u5f53\u8a72\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u305f\u6a5f\u5668\u3092\u5206\u985e\u3059\u308b\u554f\u984c\u3092\u89e3\u3044\u3066\u3044\u308b\u3002\n    - \u6a5f\u5668\u306e\u72b6\u614b\u304c\u5909\u5316\u3059\u308b\u3068\u3001\u8aa4\u5206\u985e(=\u5f53\u8a72\u30c7\u30fc\u30bf\u3092\u751f\u6210\u3057\u305f\u6a5f\u5668\u3092\u6b63\u3057\u304f\u7279\u5b9a\u3067\u304d\u306a\u3044)\u304c\u8d77\u3053\u308a\u3001\u305d\u308c\u3092\u7570\u5e38\u306e\u6307\u6a19\u3068\u3059\u308b\n    \n- \u4eca\u56de\u306e\u30b1\u30fc\u30b9\u3067\u306f\u4ee5\u4e0b\u306e\u65b9\u6cd5\u3092\u5b9f\u65bd\n    - \u554f\u984c\u3092\u5206\u985e\u554f\u984c\u306b\u5909\u66f4\u3059\u308b\u305f\u3081\u306b\u3001\u5404\u30c7\u30fc\u30bf\u306b\u300c\u30dc\u30eb\u30c8\u7a2e\u985e\u300d\u30fb\u300c\u30d7\u30ec\u30fc\u30c8\u7a2e\u985e\u300d\u30fb\u300c\u9332\u97f3\u65b9\u6cd5\u300d\u306e\u6761\u4ef6\u3092\u5408\u308f\u305b\u305f\u306e\u30e9\u30d9\u30eb\u3092\u4ed8\u4e0e(11\u5206\u985e\u3078\u554f\u984c\u3092\u66f8\u304d\u63db\u3048)\n    - \u6b63\u5e38\u30e9\u30d9\u30eb\u306e\u4ed8\u4e0e\u3055\u308c\u3066\u3044\u308b\u30c7\u30fc\u30bf\u306b\u3064\u3044\u3066\u3001\u5165\u529b\u300c20210911_feature_mel_256\u3067\u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u300d\u3001\u51fa\u529b\u300c\u4e0a\u8a18\u306e11\u6761\u4ef6\u30e9\u30d9\u30eb\u300d\u3068\u3057\u3066\u3001\u5206\u985e\u6a5f\u3092\u5b66\u7fd2\n    - \u30b5\u30f3\u30d7\u30eb\u30c7\u30fc\u30bf\u6570\u304c\u5c11\u306a\u3044\u305f\u3081\u3001\u7279\u5fb4\u91cf\u62bd\u51fa\u5668\u3068\u3057\u3066**resnet50**\u3092\u3001\u6700\u7d42\u7684\u306a\u5206\u985e\u5668\u3068\u3057\u3066**arcface**\u3092\u5229\u7528\u3057\u305f\u3002\n    - \u6700\u7d42\u7684\u306a\u7570\u5e38\u5ea6\u306f\u3001\u6b63\u89e3\u30e9\u30d9\u30eb\u306earcface\u306e\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3068\u3001resnet50\u306b\u3088\u308a\u753b\u50cf\u3092\u5909\u63db\u3057\u305f\u30d9\u30af\u30c8\u30eb\u306e\u5185\u7a4d\u306e\u9006\u6570\u3068\u3059\u308b\u3002\n    - \u7d50\u679c\u3068\u3057\u3066\u30b9\u30b3\u30a2\u306b\u306f\u7d50\u3073\u4ed8\u304b\u305a\u3002\u3002\u3002\uff08\u3084\u306f\u308a\u30c7\u30fc\u30bf\u3092\u9078\u3076\u306e\u304b\uff1f\uff09\n    \n    ","3911930b":"### \u5404\u884c\u3078\u306e\u6b63\u89e3\u30e9\u30d9\u30eb\u306e\u4ed8\u4e0e\u3068\u5206\u5e03\u306e\u78ba\u8a8d","e5b6d615":"### pytorch\u3067\u306e\u5b66\u7fd2\u6e96\u5099"}}