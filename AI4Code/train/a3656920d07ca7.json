{"cell_type":{"908be87d":"code","49a8d7d5":"code","47db190c":"code","689efa1c":"code","f538fb92":"code","8558ff17":"code","b46ae625":"code","98b52ca5":"code","eaed46eb":"code","09e4fbb9":"code","a04d5d85":"code","e2fa98d9":"code","c31c8c4b":"code","0cb75eb4":"code","d1b53b23":"code","a097e5ef":"code","19d6c7f5":"code","f54b65df":"code","789bfe70":"code","4a07dbf8":"code","0785b027":"code","bfc16f48":"code","ee2d3629":"code","65b2ac1f":"code","2fc5752c":"code","d63a0b82":"code","b1778b0e":"code","4b54b0ec":"code","42bcfe74":"code","99696e74":"code","7020200c":"code","a242f537":"code","e7cf873d":"code","6c0ed464":"code","b444aa83":"code","e45f1520":"code","ae881018":"code","61cb5659":"code","12ee09f4":"code","d267069d":"code","4cd14d3f":"code","67094e2f":"code","eb7b7f24":"code","0a584d4a":"code","bad8f91c":"code","a451fc66":"code","787ce567":"code","f351de4e":"code","441fde51":"code","747654a4":"code","41ad50c4":"code","062cfab7":"code","bd57081d":"code","8b5558c6":"code","afded551":"code","f7914153":"code","12f34c7c":"code","1c5f0409":"code","597ff7f8":"code","acf75d4f":"code","2aa014ab":"code","fbed4514":"code","242791b7":"code","66936a76":"code","7bcc287f":"code","01915f7a":"code","d1106a15":"code","409244ac":"code","b29f2915":"code","3448debb":"code","33a8bc0a":"code","1b73ab2d":"code","290ba876":"code","c9366c7b":"code","976a48ff":"code","86686654":"code","b4daa295":"code","c774f9ed":"code","98ce5d15":"code","f22e0fd1":"code","c2f2909d":"code","55abaaf2":"code","f73aabb6":"markdown","0134a1cf":"markdown","13c7a478":"markdown","9ceff444":"markdown","c0c4802f":"markdown","cf00fbb9":"markdown","e72a02c0":"markdown","95555b21":"markdown","c336ffde":"markdown","b2cc1180":"markdown","85c39045":"markdown","b99d648c":"markdown","948552d3":"markdown","f8f044b0":"markdown","5cfc7c2e":"markdown","db050e80":"markdown","b5745140":"markdown","db906508":"markdown","2b2191c9":"markdown","a83dd72a":"markdown","d20ff4c1":"markdown","52c7acb3":"markdown","e40d0af3":"markdown","f4fff191":"markdown","31c97223":"markdown","607665c4":"markdown","742fc51b":"markdown","d00c7154":"markdown","bf38ed62":"markdown","08e29550":"markdown","d3298c8f":"markdown","4e04c18a":"markdown","96c39074":"markdown","810e19b0":"markdown","09471bed":"markdown","c025861a":"markdown","ee1b06d6":"markdown","3ccc5039":"markdown","01d8140f":"markdown","18296dcb":"markdown","eb928e91":"markdown","85082b82":"markdown","9c830479":"markdown","08b2f2e4":"markdown","d821799e":"markdown","c7eee780":"markdown","da16aba2":"markdown","19c8eb6a":"markdown","41e7fd4f":"markdown","6473fc5d":"markdown","6b7e69cb":"markdown","a24f893b":"markdown","1e186141":"markdown","2498b44a":"markdown","e3089ebb":"markdown","e97f2821":"markdown","ddb4071f":"markdown","5e487510":"markdown","7f4ca159":"markdown","bc107fea":"markdown","2b0771af":"markdown","22dbd520":"markdown","317d1a20":"markdown","ab97b815":"markdown","806771c7":"markdown","51f7ad48":"markdown","cf3c38b1":"markdown","26b2fc27":"markdown","6603e228":"markdown","414e7c13":"markdown","83089445":"markdown","eaeb4eab":"markdown","33050018":"markdown","926486a9":"markdown","dbe6b3a0":"markdown","e630fb51":"markdown","2a5d1b1d":"markdown","cf6d7fd9":"markdown","692d10c2":"markdown","6de4d209":"markdown","61b4920b":"markdown","aa8a5e5a":"markdown","9609c2db":"markdown","3f697708":"markdown","a59e9426":"markdown","4611abbc":"markdown","5b5900e1":"markdown","3efaa7cc":"markdown"},"source":{"908be87d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport os\nimport warnings\n%matplotlib inline","49a8d7d5":"# Set some default options\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 100)\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 12\nmatplotlib.rcParams['figure.figsize'] = (10, 8)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'\nwarnings.filterwarnings('ignore')","47db190c":"raw_df = pd.read_csv('..\/input\/suicide-rates-overview-1985-to-2016\/master.csv')\nraw_df.head()","689efa1c":"raw_df.info()","f538fb92":"raw_df.rename(columns={' gdp_for_year ($) ':'gdp_for_year ($)'}, inplace=True)\nraw_df.columns","8558ff17":"# Check missing value and drop duplicate rows for the same country and same year\nraw_df.loc[raw_df['HDI for year'].isna(), ['country', 'year']].drop_duplicates()","b46ae625":"hdi_df = pd.read_csv('..\/input\/humandevelopmentindex\/hdi.csv', index_col='Country')\nhdi_df.head()","98b52ca5":"hdi_df.info()","eaed46eb":"# Iterate over each row in raw_df and impute the missing hdi value if applicable\nfor row in raw_df.itertuples():\n    # Exclude rows with year before 1990 since the earliest hdi available in hdi_df is 1990\n    if row.year >= 1990:\n        try:\n            hdi = hdi_df.loc[row.country, str(row.year)]\n            raw_df.at[row.Index, 'HDI for year'] = hdi\n        except KeyError:\n            pass","09e4fbb9":"raw_df.loc[raw_df['HDI for year'].isna(), ['country', 'year']].drop_duplicates()","a04d5d85":"# Group HDI by country and filter the minimum value\nmin_hdi_df = raw_df[['country', 'HDI for year']].groupby('country').agg({\"HDI for year\": \"min\"})\nmin_hdi_df.head()","e2fa98d9":"# Convert the dataframe to dict\nmin_hdi_dict = min_hdi_df.to_dict()\n\n# Map missing HDI value according to the input dict\nmask = raw_df['HDI for year'].isna()\nraw_df.loc[mask, 'HDI for year'] = raw_df.loc[mask, 'country'].apply(lambda x: min_hdi_dict['HDI for year'][x])","c31c8c4b":"# Compute a pandas series with mean hdi of each year\nmean_hdi = hdi_df.mean()\n\n# Create bool mask for rows with missing HDI (before 1990 and on\/after 1990)\nmask_bf_1990 = (raw_df['HDI for year'].isna()) & (raw_df['year'] < 1990)\nmask_aft_1990 = (raw_df['HDI for year'].isna()) & (raw_df['year'] >= 1990)\n\n# Impute missing HDI using average HDI (on\/after 1990). If missing HDI is before 1990, impute using average HDI of 1990\nraw_df.loc[mask_aft_1990, 'HDI for year'] = raw_df.loc[mask_aft_1990, 'year'].apply(lambda x: mean_hdi[str(x)])\nraw_df.loc[mask_bf_1990, 'HDI for year'] = mean_hdi['1990']","0cb75eb4":"raw_df['HDI for year'].isna().sum()","d1b53b23":"raw_df.drop(columns=['country-year'], inplace=True)\nraw_df.columns","a097e5ef":"# Reference: https:\/\/geopandas.org\/docs\/user_guide\/mapping.html\nimport geopandas","19d6c7f5":"# Load the map\nworld = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\nworld.head()","f54b65df":"# Group the data by country and find the sum of suicides number and population\nsuicide_by_country_df = raw_df.groupby('country')[['suicides_no', 'population']].sum().reset_index()\n\n# Calculate suicides\/100k pop\nsuicide_by_country_df['suicides\/100k pop'] = 100000 * suicide_by_country_df['suicides_no'] \/ suicide_by_country_df['population']\n\n# Sort the rows by suicides\/100k pop from highest to lowest\nsuicide_by_country_df.sort_values(by='suicides\/100k pop', ascending=False, inplace=True)\nsuicide_by_country_df.head()","789bfe70":"# Modify country name due to a difference in two dataframes\ncountry_match = {'Russia': 'Russian Federation', 'United States of America': 'United States', 'South Korea': 'Republic of Korea', \n                 'Bosnia and Herz.': 'Bosnia and Herzegovina'}\nworld['name'].replace(country_match, inplace=True)","4a07dbf8":"# Merge the two dataframes together\ntable = world.merge(suicide_by_country_df, how='left', left_on=['name'], right_on=['country'])\n\n# Create the map\ntable.plot(column='suicides\/100k pop', \n           cmap='hot',\n           legend=True, \n           legend_kwds={'label': 'number of suicides per 100k population',\n                        'orientation': 'horizontal'},\n           missing_kwds={'color': 'lightgrey'});","0785b027":"# x-axis from year 1985 to 2016\nx = np.arange(1985, 2017)\n\n# y-axis is the mean suicide no. per 100k population\ny = raw_df.groupby('year')['suicides\/100k pop'].mean()\n\n# Create a line plot\nsns.lineplot(x=x, y=y)\nplt.xlabel('Year')\nplt.ylabel('number of suicides per 100k population')\nplt.title('Suicide Incidence Over the Years');","bfc16f48":"# Create a barplot \nsns.barplot(data=raw_df, x='age', y='suicides\/100k pop', hue='sex')\nplt.xlabel('Age group')\nplt.ylabel('# of suicides per 100k population')\nplt.title('Suicide Incidence by Age and Gender (1985 - 2016)');","ee2d3629":"sns.heatmap(raw_df.corr(), annot=True, cmap='YlOrBr', fmt='.2f');","65b2ac1f":"# Create additional column for suicide risk and set the default value as low\nraw_df['suicide_risk'] = 'low' \n\n# Set the risk as high if suicides\/100k pop is higher than the mean\nraw_df.loc[raw_df['suicides\/100k pop'] > raw_df['suicides\/100k pop'].mean(), 'suicide_risk'] = 'high' \nraw_df['suicide_risk'].value_counts()","2fc5752c":"sns.countplot(x='year', data=raw_df)\nplt.xticks(rotation='90');","d63a0b82":"# Create a series containing number of data per year\nyear_dt = raw_df.year.value_counts()\nyear_dt.sort_index(inplace=True)\ncum_pct = 100 * year_dt.cumsum() \/ year_dt.sum()\ncum_pct","b1778b0e":"train_df = raw_df.loc[raw_df['year'] <= 2004]\nval_df = raw_df.loc[(raw_df['year'] >= 2005) & (raw_df['year'] <=2010)]\ntest_df = raw_df.loc[raw_df['year'] >= 2011]\n\nprint('train_df.shape: ', train_df.shape)\nprint('val_df.shape: ', val_df.shape)\nprint('test_df.shape: ', test_df.shape)","4b54b0ec":"input_cols = ['country', 'sex', 'age', 'HDI for year', 'gdp_per_capita ($)']\ntarget_col = 'suicide_risk'","42bcfe74":"train_inputs = train_df.loc[:, input_cols]\ntrain_target = train_df.loc[:, target_col]\n\nval_inputs = val_df.loc[:, input_cols]\nval_target = val_df.loc[:, target_col]\n\ntest_inputs = test_df.loc[:, input_cols]\ntest_target = test_df.loc[:, target_col]","99696e74":"numeric_cols = list(train_inputs.select_dtypes(include=np.number).columns)\ncategorical_cols = list(train_inputs.select_dtypes(include='object').columns)\n\nprint('numeric columns: ', numeric_cols)\nprint('categorical columns: ', categorical_cols)","7020200c":"from sklearn.preprocessing import MinMaxScaler\n\n# Fit the scaler to all the data\nscaler = MinMaxScaler().fit(raw_df.loc[:, numeric_cols])\n\n# Scale the train, validation and test sets \ntrain_inputs.loc[:, numeric_cols] = scaler.transform(train_inputs.loc[:, numeric_cols])\nval_inputs.loc[:, numeric_cols] = scaler.transform(val_inputs.loc[:, numeric_cols])\ntest_inputs.loc[:, numeric_cols] = scaler.transform(test_inputs.loc[:, numeric_cols])","a242f537":"train_inputs[numeric_cols].describe()","e7cf873d":"# Check the number of unique values of each categorical column\nraw_df[categorical_cols].nunique()","6c0ed464":"from sklearn.preprocessing import OneHotEncoder\n\nencoder = OneHotEncoder(handle_unknown='ignore').fit(raw_df[categorical_cols])","b444aa83":"encoded_cols = list(encoder.get_feature_names(categorical_cols))\n\n# Encode the columns\ntrain_inputs[encoded_cols] = encoder.transform(train_inputs.loc[:, categorical_cols]).toarray()\nval_inputs[encoded_cols] = encoder.transform(val_inputs.loc[:, categorical_cols]).toarray()\ntest_inputs[encoded_cols] = encoder.transform(test_inputs.loc[:, categorical_cols]).toarray()","e45f1520":"X_train = train_inputs[numeric_cols + encoded_cols]\nX_val = val_inputs[numeric_cols + encoded_cols]\nX_test = test_inputs[numeric_cols + encoded_cols]","ae881018":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=1)\ntree.fit(X_train, train_target)","61cb5659":"from sklearn.metrics import accuracy_score, confusion_matrix\n\ntrain_pred = tree.predict(X_train)\naccuracy_score(train_target, train_pred)","12ee09f4":"val_pred = tree.predict(X_val)\naccuracy_score(val_target, val_pred)","d267069d":"val_target.value_counts() \/ len(val_target)","4cd14d3f":"from sklearn.tree import plot_tree\n\nplt.figure(figsize=(80, 20))\nplot_tree(tree, feature_names=X_train.columns, max_depth=2, filled=True);","67094e2f":"importance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': tree.feature_importances_\n}).sort_values(by='importance', ascending=False)\n\n# Visualize the feature importance\nsns.barplot(data=importance_df.head(10), x='importance', y='feature')\nplt.title('Feature Importance (Decision Tree)');","eb7b7f24":"tree.tree_.max_depth","0a584d4a":"def max_depth_tuning(md):\n    tree = DecisionTreeClassifier(random_state=1, max_depth=md)\n    tree.fit(X_train, train_target)\n    train_pred = tree.predict(X_train)\n    val_pred = tree.predict(X_val)\n    train_error = 1 - accuracy_score(train_target, train_pred)\n    val_error = 1 - accuracy_score(val_target, val_pred)\n    return {'max_depth': md, 'train_error': train_error, 'val_error': val_error}","bad8f91c":"# Create a dataframe to store the tuning result\nerrors_df = pd.DataFrame([max_depth_tuning(md) for md in range(2, 43, 2)])","a451fc66":"# Create a line plot to visualize training error and validation error against max_depth\nfig, ax = plt.subplots()\nsns.lineplot(data=errors_df, x='max_depth', y='train_error', ax=ax)\nsns.lineplot(data=errors_df, x='max_depth', y='val_error', ax=ax)\nplt.ylabel('Prediction error (1 - accuracy)')\nplt.xticks(range(1, 44, 2))\nplt.title('Training Error vs. Validation Error')\nplt.legend(['Training', 'Validation'])\n\n# Annotate the training error and validation error \nplt.axvline(x=30, color='r', linestyle='--')\nax.annotate('validation error: 0.118',\n            xy=(30, 0.117988),\n            xycoords='data',\n            fontsize=12,\n            xytext=(10, 10),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\", color='black',\n                            connectionstyle=\"arc3\"))\nax.annotate('train error: 0.009',\n            xy=(30, 0.009398),\n            xycoords='data',\n            fontsize=12,\n            xytext=(10, 10),\n            textcoords='offset points',\n            arrowprops=dict(arrowstyle=\"->\", color='black',\n                            connectionstyle=\"arc3\"))\nplt.show()","787ce567":"param_dict = {\n    \"max_depth\": range(2, 40, 2),\n    \"min_samples_split\": range(2, 10, 2)\n}","f351de4e":"X = pd.concat([X_train, X_val])\nY = pd.concat([train_target, val_target])\nprint(X.shape, Y.shape)","441fde51":"from sklearn.model_selection import GridSearchCV\n\ntree = DecisionTreeClassifier(random_state=1)\ngrid = GridSearchCV(tree, param_grid=param_dict)\ngrid.fit(X, Y)","747654a4":"print('best params: ', grid.best_params_)\nprint('accuracy score: ', grid.best_score_)","41ad50c4":"test_pred = grid.predict(X_test)\naccuracy_score(test_target, test_pred)","062cfab7":"import joblib","bd57081d":"suicide_risk_dt = {\n    'model': grid,\n    'scaler': scaler,\n    'encoder': encoder,\n    'input_cols': input_cols,\n    'target_col': target_col,\n    'numeric_cols': numeric_cols,\n    'categorical_cols': categorical_cols,\n    'encoded_cols': encoded_cols\n}","8b5558c6":"joblib.dump(suicide_risk_dt, 'suicide_risk_dt.joblib')","afded551":"suicide_risk_dt_clf = joblib.load('suicide_risk_dt.joblib')","f7914153":"def predict_input(model, single_input):\n    input_df = pd.DataFrame([single_input])\n    input_df[model['numeric_cols']] = model['scaler'].transform(input_df[model['numeric_cols']])\n    input_df[model['encoded_cols']] = model['encoder'].transform(input_df[model['categorical_cols']]).toarray()\n    X_input = input_df[model['numeric_cols'] + model['encoded_cols']]\n    pred = model['model'].predict(X_input)[0]\n    prob = model['model'].predict_proba(X_input)[0][list(model['model'].classes_).index(pred)]\n    return pred, prob","12f34c7c":"new_input = {\n    'country': 'Republic of Korea',\n    'year': 2020,\n    'sex': 'male',\n    'age': '75+ years',\n    'suicides_no': 1400,\n    'population': 900000,\n    'suicides\/100k pop': 155.55,\n    'HDI for year': 0.9,\n    'gdp_for_year ($)': '1,000,000,000',\n    'gdp_per_capita ($)': 29000,\n    'generation': 'Silent' \n}","1c5f0409":"predict_input(suicide_risk_dt_clf, new_input)","597ff7f8":"new_input_2 = {\n    'country': 'Singapore',\n    'year': 2020,\n    'sex': 'female',\n    'age': '15-24 years',\n    'suicides_no': 14,\n    'population': 250000,\n    'suicides\/100k pop': 5.6,\n    'HDI for year': 0.7,\n    'gdp_for_year ($)': '300,000,000,000',\n    'gdp_per_capita ($)': 80000,\n    'generation': 'Millenials' \n}","acf75d4f":"predict_input(suicide_risk_dt_clf, new_input_2)","2aa014ab":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_jobs=-1, random_state=1)\nrf.fit(X_train, train_target)","fbed4514":"train_pred = rf.predict(X_train)\nprint('training accuracy: ', accuracy_score(train_target, train_pred))\n\nval_pred = rf.predict(X_val)\nprint('validation accuracy: ', accuracy_score(val_target, val_pred))","242791b7":"importance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': rf.feature_importances_\n}).sort_values(by='importance', ascending=False)\n\n# Visualize the feature importance\nsns.barplot(data=importance_df.head(10), x='importance', y='feature')\nplt.title('Feature Importance (Random Forest)');","66936a76":"len(rf.estimators_)","7bcc287f":"plt.figure(figsize=(80, 20))\nplot_tree(rf.estimators_[0], feature_names=X_train.columns, max_depth=2, filled=True);","01915f7a":"plt.figure(figsize=(80, 20))\nplot_tree(rf.estimators_[1], feature_names=X_train.columns, max_depth=2, filled=True);","d1106a15":"def train_and_evaluate(X_train, train_target, X_val, val_target, **params):\n    rf = RandomForestClassifier(n_jobs=-1, random_state=1, **params)\n    rf.fit(X_train, train_target)\n    train_pred = rf.predict(X_train)\n    val_pred = rf.predict(X_val)\n    train_error = 1 - accuracy_score(train_target, train_pred)\n    val_error = 1 - accuracy_score(val_target, val_pred)\n    return {'model': rf, **params, 'train_error': train_error, 'val_error': val_error}","409244ac":"errors_list = []\n\nfor n_estimators in [10, 100, 500]:\n    for max_features in ['sqrt', 'log2']:\n        outcome = train_and_evaluate(X_train, train_target, X_val, val_target, n_estimators=n_estimators, max_features=max_features)\n        errors_list.append(outcome)","b29f2915":"errors_df = pd.DataFrame(errors_list).sort_values(by='val_error')\nerrors_df","3448debb":"selected_rf = errors_df.iloc[5]['model']","33a8bc0a":"test_pred = selected_rf.predict(X_test)\naccuracy_score(test_target, test_pred)","1b73ab2d":"suicide_risk_rf = {\n    'model': selected_rf,\n    'scaler': scaler,\n    'encoder': encoder,\n    'input_cols': input_cols,\n    'target_col': target_col,\n    'numeric_cols': numeric_cols,\n    'categorical_cols': categorical_cols,\n    'encoded_cols': encoded_cols\n}","290ba876":"joblib.dump(suicide_risk_rf, 'suicide_risk_rf.joblib')","c9366c7b":"suicide_risk_rf_clf = joblib.load('suicide_risk_rf.joblib')","976a48ff":"new_input = {\n    'country': 'Albania',\n    'year': 2020,\n    'sex': 'female',\n    'age': '15-24 years',\n    'suicides_no': 10,\n    'population': 250000,\n    'suicides\/100k pop': 4,\n    'HDI for year': 0.75,\n    'gdp_for_year ($)': '11,000,000,000',\n    'gdp_per_capita ($)': 4400,\n    'generation': 'Millenials' \n}","86686654":"predict_input(suicide_risk_rf_clf, new_input)","b4daa295":"dt_probs = suicide_risk_dt_clf['model'].predict_proba(X_test)\nrf_probs = suicide_risk_rf_clf['model'].predict_proba(X_test)\n\n# Keep probabilities for positive outcome only\ndt_probs = dt_probs[:, 1]\nrf_probs = rf_probs[:, 1]","c774f9ed":"from sklearn.metrics import roc_curve, roc_auc_score\n\n# The probability estimates correspond to the probability of the class with the greater label, i.e. estimator.classes_[1] -> 'low' suicide risk\ndt_auc = roc_auc_score(test_target, dt_probs)\nrf_auc = roc_auc_score(test_target, rf_probs)\n\nprint('Decision Tree (AUROC): {:.3f}'.format(dt_auc))\nprint('Random Forest (AUROC): {:.3f}'.format(rf_auc))","98ce5d15":"# Compute ROC curves for both models\ndt_fpr, dt_tpr, dt_t = roc_curve(test_target, dt_probs, pos_label='low')\nrf_fpr, rf_tpr, rf_t = roc_curve(test_target, rf_probs, pos_label='low')\n\n# Plot ROC curves\nplt.plot(dt_fpr, dt_tpr, color='darkorange', label='Decision Tree (AUROC): {:.3f}'.format(dt_auc))\nplt.plot(rf_fpr, rf_tpr, color='green', label='Random Forest (AUROC): {:.3f}'.format(rf_auc))\nplt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.title('ROC plot')\nplt.xlabel('False Positive Rate (FPR)')\nplt.ylabel('True Positive Rate (TPR)')\nplt.legend(loc='lower right')\nplt.show();","f22e0fd1":"# Get the best threshold for random forest model\nJ_rf = rf_tpr - rf_fpr\nix = np.argmax(J_rf)\nbest_thresh = rf_t[ix]\nprint('Best Threshold for random forest: %f' % (best_thresh))\n\n# Get the best threshold for decision tree model\nJ_dt = dt_tpr - dt_fpr\nix = np.argmax(J_dt)\nbest_thresh = dt_t[ix]\nprint('Best Threshold for decision tree: %f' % (best_thresh))","c2f2909d":"# Set the prediction to 'low' if probability estimate of positive outcome (low suicide risk) > threshold\nrf_test_pred = np.where(rf_probs >= 0.600, 'low', 'high')\nprint('Test accuracy (random forest) (threshold = 0.600): ', accuracy_score(test_target, rf_test_pred))\nprint('Test accuracy (random forest) (default threshold): ', accuracy_score(test_target, suicide_risk_rf_clf['model'].predict(X_test)), '\\n')\n\ndt_test_pred = np.where(dt_probs >= 0.666667, 'low', 'high')\nprint('Test accuracy (decision tree) (threshold = 0.667): ', accuracy_score(test_target, dt_test_pred))\nprint('Test accuracy (decision tree) (default threshold): ', accuracy_score(test_target, suicide_risk_dt_clf['model'].predict(X_test)))\n","55abaaf2":"raw_df['suicide_risk'].value_counts()","f73aabb6":"Let's also identify the numeric and categorical columns.","0134a1cf":"Let's look at the types of data and number of null values under each column.","13c7a478":"# Random Forest\n\nNext, let's train a random forest model using the same train\/validation\/test dataset. Random forest trains several decision trees based on randomly selected data samples, gets predictions from each tree and makes the best prediction by means of voting.\n\nIn general, a random forest model is highly robust and can reduce overfitting as the final prediction was made by averaging the predictions from multiple decision trees. The drawback is the speed because all the trees in the forest need to make a prediction before voting.\n\nWe'll use `RandomForestClassifier` from `sklearn.ensemble`.","9ceff444":"Suicide is a major global health problem. According to World Health Organization (WHO) [suicide data](https:\/\/www.who.int\/teams\/mental-health-and-substance-use\/suicide-data), more than 700,000 people die by suicide every year.\n\nIn this project, we'll use machine learning algorithms (decision tree and random forest) to predict suicide risk. We use a dataset on [Kaggle](https:\/\/www.kaggle.com\/russellyates88\/suicide-rates-overview-1985-to-2016), which contains information from 101 countries such as age group, gender, gross domestic product (GDP) per capita, and human development index (HDI) during the period of 1985 to 2016.","c0c4802f":"Let's evaluate the random forest model using `n_estimators` of (10, 100, 500) and `max_features` of (sqrt, log2). We will compile the training errors and validation errors and use them to select the optimal model.","cf00fbb9":"Let's look at the best combination of hyperparameters and the accuracy score. ","e72a02c0":"In this section, let's try to clean up the data and remove\/add columns we need for subsequent analysis.\n\nWe see earlier that there are a lot of missing values in `HDI for year`. Let's start by manipulating this column.","95555b21":"We've completed the EDA. ","c336ffde":"Now, let's group the suicide data by country and calculate the aggregated number of suicides per 100k population for each country.","b2cc1180":"The performance of the random forest model is actually pretty good. When using decision tree model earlier, our validation accuracy is around 85-89% with different tuning. Let's check the feature importance and see if it is comparable with the important features identifed from our single decision tree model. ","85c39045":"Before we proceed further, there is one point we need to take note of. When using `GridSearchCV` function, it handles the split of dataset into train\/validation set and performs 5-fold cross validation splitting strategy by default. It is possible to explicitly specify the train\/validation set ([reference](https:\/\/stackoverflow.com\/questions\/48390601\/explicitly-specifying-test-train-sets-in-gridsearchcv)), but it will be more tedious. \n\nHere, we choose to let `GridSearchCV` to handle the split. So, let's combine the train set and validation set before running the function.","b99d648c":"Currently, we have 100 decision trees in the forest.","948552d3":"# Prepare the Data for Training\n\nWe'll follow the general steps below (where applicable) to prepare dataset for training.\n* Create the target column\n* Create a train\/validation\/test split\n* Identify input and target columns\n* Identify numeric and categorical columns\n* Impute missing numeric values\n* Scale numeric values \n* Encode categorical columns","f8f044b0":"Let's convert the `errors_list` (list of dictionary) into a Pandas dataframe and sort by validation error.","5cfc7c2e":"There is no much difference in the test accuracy of both models before *vs.* after we tune the threshold.\n\nInstead of using ROC curve to find the optimal threshold, we can use another curve, called precision-recall curve, which is recommended when there is an imbalance in the obversations between two classes. In our dataset, we have approx. 2x more data of low suicide risk class than high suicide risk class.\n\nThe principle is similar to the ROC-AUC curve - more information can be found [here](https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/).","db050e80":"## Create Train, Validation and Test Sets\n\nLet's split our dataset into three parts.\n\n1.   Train set: used to train a model\n2.   Validation set: used to tune model hyperparameters (e.g., regularization parameter) and choose between models during training\n3.   Test set: used to compare different models and report the model's final accuracy\n\nIn general, we can pick random subset of rows to create train\/validation\/test set in a distribution of 60-20-20 rule. However, when working with time-series data, it's better to split the dataset based on time, so the model was trained using the data in the past, and evaluated using the data in the future.\n\nLet's look at the distribution fo our data by `year`.","b5745140":"We can use the best threshold (instead of the default threshold of 0.5) to tune our models and see if the model accuracy improves.\n\nTo do this, we need to \"manually\" create the predicted test outcomes by checking if the probabilities of positive outcome are larger than the defined threshold.","db906508":"## Hyperparameter tuning\n\nLet's try to tune a few important hyperparameters for random forest and see if we can further improve the model accuracy. Here, we look at:\n\n1. `n_estimators`: number of trees in the forest\n2. `max_features`: number of features to consider when looking for the best split\n\nWe'll evaluate a combination of these two hyperparameters and see which combination yields the best prediction. To do this, let's first define a helper function.","2b2191c9":"Let's check if the number of missing values decreases. ","a83dd72a":"Let's calculate the AUROC of both models.","d20ff4c1":"## Suicide Over Time\n\nLet's take a look at the suicide incidence over the past 20 years.","52c7acb3":"The test set accuracy score is pretty good at about 91%! Let's save our model (including weights, hyperparameters) so we do not need to retrain the model from scratch every time we need to use it. Along with the model, we also save the important elements such as encoder, scaler, and column names.","e40d0af3":"## Identify Input and Target Columns\n\nOften, not all the columns in the dataset are useful for training a model. We will ignore the following columns:\n\n*   `year`: this is not relevant as we are predicting the suicide risk of a particular person in the future\n*   `suicides_no`, `population`: contains redundant information as `suicides\/100k pop`\n*   `suicides\/100k pop`: contains redundant information as `suicide_risk`\n*   `gdp_for_year ($)`: contains redundant information as `gdp_per_capita ($)`\n*   `generation`: contains redundant information as  `age`. Each generation is corresponding to a specific age group under `age`.\n","f4fff191":"The validation set accuracy is about 88%, which is better than always predicting \"low\".","31c97223":"## Create the target column\n\nBefore we split the dataset into train\/validation\/test set, let's create our target column `suicide_risk` (i.e. high \/ low risk) using the information from `suicides\/100k pop`. \n\nIf the value of `suicides\/100k pop` is higher than the mean of `suicides\/100k pop`, we will classify the suicide risk as high, and low otherwise. ","607665c4":"## Evaluation\n\nLet's look at the training accuracy and validation accuracy.","742fc51b":"Let's try to impute the missing HDI information in `raw_df`\nby using the information in `hdi_df`. \n\nWhile this might not fully impute the missing values as there are still missing HDI for some countries and years in the `hdi_df`, it should be able to reduce quite a lot of missing HDI in `raw_df`. ","d00c7154":"There doesn't seem to be strong relationship between numerical variables and suicide. The correlation coefficient between `population` and `suicides_no` is quite high (0.62), but actually there is no \"real\" correlation (coefficient = 0.01) after normalizing the number of suicides by population.","bf38ed62":"# Data Cleaning and Manipulation","08e29550":"## Visualization\n\nLet's also visualize the first few layers of the decision tree. ","d3298c8f":"The file `master.csv` contains the data. Let's load it into a Pandas dataframe.","4e04c18a":"Random forest model has a better separation capacity (AUROC = 0.943) than the decision tree model (AUROC = 0.916). Let's also plot the ROC curve of both models.","96c39074":"Both training error and validation error shows a decreasing trend with the increase of `max_depth`. It is not so obvious from the graph, but the validation error is the lowest when the `max_depth` is around 30. Beyond `max_depth` of 30, the validation error increases slightly. ","810e19b0":"We can define a helper function to make predictions on new inputs.","09471bed":"The test set accuracy is 87.9% (lower than the validation accuracy).","c025861a":"We have pretty much imputed most of the missing HDI. For the remaining missing values, let's impute using average HDI across the countries of each year. ","ee1b06d6":"# Exploratory Data Analysis","3ccc5039":"Great! We've sufficiently addressed all the missing HDI. Now, let's also drop the column `country-year` since the information is already available under columns `country` and `year`.","01d8140f":"## Save Trained Model\n\nWe can use `joblib` [module](https:\/\/joblib.readthedocs.io\/en\/latest\/) to save and load Python objects on the disk.","18296dcb":"### Max Depth Tuning\n\nIf a tree is too deep, it will overfit training data. Let's explore the max depth to optimize the bias-variance trade off, so we do not create partitions which are for \"noisy\" data (outlier) in the training dataset.\n\nLet's first take a look at the `max_depth` of the current decision tree.","eb928e91":"It's a bit challenging to decide which year we should use as a split point by looking at the count. Let's look at the cumulative percentage instead.","85082b82":"Older adults have a higher tendency of committing suicide. This might be related to factors such as loneliness, social connectedness, chronic diseases, etc. *(further reading [here](https:\/\/www.sprc.org\/populations\/older-adults))*.\n\nAcross all age groups, suicide occurred almost 3-4 times more often in males than females. This could be because male suicide methods are often more violent, making them more likely to be completed before anyone can intervene. There might also be social stigma that men cannot be weak, which causes them not to share their struggles openly. *(further reading [here](https:\/\/www.bbc.com\/future\/article\/20190313-why-more-men-kill-themselves-than-women))*.","9c830479":"Model #5 is giving the highest validation accuracy (~ 90.8%). This is slightly higher than the validation accuracy (~90.2%) before tuning the hyperparameters. Let's evaluate the test set accuracy using this model.","08b2f2e4":"## Age and Gender Distribution of People Committing Suicide\n\nLet's visualize the number of suicides by age and gender.","d821799e":"# References\n\nHere are some useful references:\n\n*   [Hyperparameter tuning for classification ML algorithms](https:\/\/machinelearningmastery.com\/hyperparameters-for-classification-machine-learning-algorithms\/)\n*   [Tuning a decision tree](https:\/\/towardsdatascience.com\/how-to-tune-a-decision-tree-f03721801680)\n*   [ROC Curve](https:\/\/machinelearningmastery.com\/roc-curves-and-precision-recall-curves-for-classification-in-python\/)\n*   [Threshold moving for imbalanced classification](https:\/\/machinelearningmastery.com\/threshold-moving-for-imbalanced-classification\/)\n*   WHO suicide [fact sheet](https:\/\/www.who.int\/news-room\/fact-sheets\/detail\/suicide)\n","c7eee780":"We'll skip the step of missing value imputation as we have handled the missing values of `HDI for year` before performing exploratory data analysis. There is no null data in other columns.","da16aba2":"The suicide incidence of the countries in our dataset peaked in 1995 and followed a downtrend since then. However, in 2015, there was a sudden surge of number of suicides. ","19c8eb6a":"## Evaluation\n\nThe decision tree has been trained. Let's evaluate its performance using the accuracy score.","41e7fd4f":"## Feature Importance\n\nBased on the Gini index computation, each feature is assigned a \"Gini importance\", which is computed as the (normalized) total reduction of Gini index brought by that feature. Let's see which feature is more important to predict the suicide risk. \n\n","6473fc5d":"Let's explore and visualize the dataset to get some useful insights before training our machine learning models.","6b7e69cb":"## Encode categorical columns\n\nTo train the machine learning models, we need to transform the values of categorical columns into numbers. There are different ways to encode the data, such as label encoding and one-hot encoding. \n\nHere, we will use [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) from `sklearn.preprocessing`. What it does is to convert each category value into a new column and assign a `1` or `0` (True\/False) value to the column. This has the benefit of not weighting a value improperly.","a24f893b":"# Download the Dataset","1e186141":"## Make Predictions Based on New Inputs\n\nLet's load the random forest model from `joblib` object.","2498b44a":"From the map, the suicide incidence is higher in Russian Federation and some European countries, compared to Australia and America.\n\nWe can also see that we do not have suicide data for most of the Asia and Africa countries, and our machine learning model will not generalize well to these countries.","e3089ebb":"On the United Nations Development Programme [website](http:\/\/hdr.undp.org\/en\/data), we can access HDI of 190 countries from 1990-2019. I have generated the report in a csv format (`hdi.csv`) from the website. Let's load it into a Pandas dataframe.","e97f2821":"Let's modify some important predictors of suicide risk (e.g., `sex_female`, `HDI for year`) and see if the prediction changes. ","ddb4071f":"# Machine Learning Project - Suicide Risk Prediction","5e487510":"## Numeric Variables Correlated with Suicide\n\nAfter exploring the relationship between categorical variables and suicide, let's see the correlation between numeric variables and suicide. We can visualize the relationship using a heatmap.","7f4ca159":"## Suicide Incidence Across the Countries\n\nFirst, we'll visualize the suicide rate across the countries on a map using `geopandas` library.","bc107fea":"We can merge `world` and `suicide_by_country_df` to plot the suicide incidence of each country. Before that, we need to modify some country names in `world` as there are discrepancies between the two dataframes. ","2b0771af":"`gdp_per_capita ($)` turns out to be the most important feature in random forest model (4th important feature in decision tree model). The other important features (e.g., `sex_female`, `HDI for year`, `age_5-14 years`) are largely similar to the results from decision tree model.","22dbd520":"Let's experiment with different `max_depth` using a helper function `max_depth_tuning`. We'll test the `max_depth` for a range from 2 to 42, with a step of 2.","317d1a20":"We can see that each decision tree is using different features to split the samples. This is because the dataset was built from various sub-samples of the dataset (attribute `bootstrap` was set as `True` by default). This \"randomness\" helps control overfitting. ","ab97b815":"# Conclusion & Next Steps","806771c7":"Approximately 60% of data are before 2004 (cumulative percentage = 60.4%), and 20% of data are between 2005 and 2010 (cumulative percentage = 82.7%).\n\nLet's split the data into train\/validation\/test set using year 2004 and year 2010 as breakpoint.","51f7ad48":"## Hyperparameter Tuning\n\nNext, let's tune some hyperparameters to see if we can improve the accuracy of the model on the validation set.\n\nOne common problem of decision tree is that it tends to overfit the training data. In our case, the training accuracy is 100%, which basically means that the model has memorized all the train data (=overfitting). In order to make the model generalize better to unseen data, let's try to tune two hyperparameters.\n\n1.   `max_depth`: controls the overall complexity of a decision tree (adequate assuming that the tree built is symmetric)\n2.   `min_samples_split`: minimum number of samples required to split an internal node\n\n","cf3c38b1":"We can use the `predict_input` function, which was defined in the 'Decision Tree' section to make predictions based on new inputs.","26b2fc27":"Let's import some required libraries before we begin.","6603e228":"The most important feature to predict the suicide risk is `sex_female` (0.18), followed by `HDI for year` and `age_5-14 years`, which both have similar importance of around 0.13. Other important features include `gdp_per_capita` and other age groups.","414e7c13":"Next, we are going to clean up the data and explore the data to gain some quick insights. ","83089445":"## Save Trained Model\n\nLet's save the random forest model using `joblib`.\n","eaeb4eab":"## Visualization\n\nLet's take a look at a few decision trees from the random forest. The decision trees can be accessed via `estimators_` attribute.","33050018":"We can see that the model classifies the input data by a series of decisions. If a split results in information gain (measured by Gini Impurity Index - a lower Gini Index indicates a better split), the split will happen.\n\nIn our decision tree model, the input data was first split by sex, followed by age group, HDI for year, and country. ","926486a9":"## Feature Importance","dbe6b3a0":"### Best Combination of Max Depth and Min Samples Split\n\nWe've explored the range of max depth values earlier and identified max_depth of 30 to be the optimal value. Now, let's include another hyperparameter `min_samples_split` in the tuning process. `Min_samples_split` is the minimum number of samples required to split an internal node. For example, if we specify the `min_samples_split` as 2, the split resulting in a leaf with 1 sample will not be allowed. Too high a `min_samples_split` value can lead to underfitting.\n\nHere, we'll use [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) function from `sklearn.model_selection`. It can help us find the optimal combination of the specified hyperparameters which gives the best prediction. Let's first specify the type and range of hyperparameters we would like to test.\n","e630fb51":"## Scale Numeric Features\n\nLet's scale the numeric features (i.e., `HDI for year`, `gdp_per_capita`) to a range of value of 0 to 1. This ensures that no particular numeric features has a disproportionate impact on the model's loss. It also smoothens the training process.\n\nLet's use `MinMaxScaler` from `sklearn.preprocessing` to scale values to (0,1) range.\n\n*Refer [here](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/feature-scaling-machine-learning-normalization-standardization\/) for difference between normalization and standardization.*","2a5d1b1d":"The optimal hyperparameter combination is `max_depth` of 34 and `min_samples_split` of 2. Let's evaluate the performance of this model on test set.","cf6d7fd9":"# Decision Tree\n\nLet's train a decision tree classifier to classify the suicide risk into high or low based on the input data.","692d10c2":"There are 6 numeric columns and 6 categorical columns. The column `HDI for year` contains a lot of missing values. Later, we'll see how we can impute the data or extract the missing information from other sources.\n\nNow, let's try to understand the meaning of some of the \"less intuitive\" features in the dataset.\n* `population`: number of people of the corresponding age group \n* `suicides_no`: number of suicides in the population of a particular age group\n*  `HDI for year`: human development index (HDI) of the year. HDI is a composite index measuring average achievement in three basic dimensions of human development \u2014 a long and healthy life, knowledge and a decent standard of living.\n* `gdp_for_year`: gross domestic product (GDP) of the year. GDP is the total monetary or market value of all the finished goods and services produced within a country's borders. \n* `gdp_per_capita`: a country's GDP divided by its total population\n","6de4d209":"As a final step, let's drop the textual categorical columns, so that we're left with just numerical data.","61b4920b":"## Tune Optimal Threshold for the Model\n\nBy default, in a binary classification problem, `scikit-learn` uses the threshold of 0.5 for classification. For example, if the predicted probability is >=0.5, the sample is predicted to be class 1. However, the optimal threshold for separation might not always be 0.5. \n\nLet's try to locate the optimal threshold with the optimal balance between TPR and FPR using a method called '[Youden's J Statistic](https:\/\/en.wikipedia.org\/wiki\/Youden%27s_J_statistic)'.\n\nIn essence, J statistic = sensitivity + specificity - 1 = true positive rate - false positive rate.","aa8a5e5a":"We've successfully trained a decision tree and a random forest model to predict the suicide risk based on factors such as age group, gender, HDI per year (composite index of life expectancy, education, and per capita income), GDP per capita and country.\n\nIt is important to recall that we do not have suicide data for most Asian and African countries in our training dataset. Hence, the ML models shall not be generalized beyond the countries available in our dataset. \n\nHere are a few next steps we might take to better make predictions \/ generalize our model:\n\n* Collect data from more Asia and Africa countries\n* Obtain additional features such as employment status, financial situation (debt), any chronic health issues, country's mental health infrastructure, etc.\n\nIt's very difficult to obtain all the data for all the countries across the globe. We can also zoom into a particular country and collect \/ analyze the features more comprehensively. This might help us design targeted suicide prevention strategies for the higher-suicide risk group (e.g., include mental health education in secondary school, organize more community events for elderly, etc.).","9609c2db":"## Make Prediction on New Inputs\n\nLet's load our decision tree model back using `joblib.load`. ","3f697708":"# Compare Decision Tree and Random Forest Model Using AUC-ROC Curve\n\nAUC (Area Under the Curve) - ROC (Receiver Operating Characteristics) curve measures the model performance for the classification problem at various threshold settings by summarizing the trade-off between the true positive rate (TPR) and false positive rate (FPR).\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/36\/Roc-draft-xkcd-style.svg)\n\n<center><i>Image source: Wikipedia <\/i><\/center>\n\nROC is a probability curve. AUC represents the degree of separability. The higher the AUC, the better the model is at predicting 0 class as 0 and 1 class as 1. \n\n*   AUC near to 1: excellent model with good measure of separability\n*   AUC = 0.5: model with no class separation capacity (=making random guesses)\n*   AUC near to 0: perfectly incorrect model with worst measure of separability\n\nLet's compare the performance of our decision tree and random forest model using AUC-ROC curve. First, we need to calculate the predicted probabilities of the 1 class (=low suicide risk).\n\n","a59e9426":"We can see the number of countries and years with missing HDI has dropped from 1,624 rows to 408 rows. These missing HDI are either before 1990 or not available on the United Nations Programme website.  \n\nFrom `hdi_df`, we see that HDI generally increases over the years. Since the remaining missing HDI are the values in the earlier years, a reasonable approach would be to impute these missing values using the minimum HDI of each country.","4611abbc":"The training accuracy is 100%. Let's evaluate the model using the validation set.","5b5900e1":"Let's generate column names for each individual category using `get_feature_names` and add all the columns to `train_inputs`, `val_inputs`, and `test_inputs`. ","3efaa7cc":"There are leading and trailing spaces in the column name `gdp_for_year ($)`. Let's fix it. "}}