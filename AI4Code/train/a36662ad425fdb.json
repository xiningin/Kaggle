{"cell_type":{"2f1eee72":"code","a74b16c1":"code","a8632c7a":"code","ea8c4d13":"code","0d3ee916":"code","6f454805":"code","a05dbb03":"code","cd638198":"code","c9c7459f":"code","ba7d02aa":"code","3d3baa26":"code","1771f471":"code","cbe08f1a":"code","5bf9f4d1":"code","d5c8c092":"code","e756dddc":"code","6846df23":"code","224bc3fa":"code","f89d1be4":"code","8a3174d2":"code","db5a6fb0":"markdown","d0c46024":"markdown","604e926d":"markdown","1cc3cf4d":"markdown","1b41910c":"markdown","87ba5ff6":"markdown","7601a7c7":"markdown","a427b09e":"markdown","7d88c125":"markdown","a1f2b0d1":"markdown"},"source":{"2f1eee72":"%matplotlib inline \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\nimport nltk\nimport csv\nfrom PIL import Image\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n    ","a74b16c1":"# \u0110\u1ecdc v\u00e0o d\u1eef li\u1ec7u train v\u00e0 test \n\ntrain_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/quora-insincere-questions-classification\/test.csv')\n\n# th\u1eed in ra v\u00e0i d\u00f2ng \u0111\u1ea7u c\u1ee7a t\u1eadp train\nprint(train_data.head())\n\n# th\u1eed in ra v\u00e0i d\u00f2ng \u0111\u1ea7u c\u1ee7a t\u1eadp \nprint(test_data.head())","a8632c7a":"# bi\u1ec3u di\u1ec5n s\u1ed1 l\u01b0\u1ee3ng c\u00e1c lo\u1ea1i c\u00e2u h\u1ecfi \u1edf t\u1eadp train \u1edf d\u1ea1ng bi\u1ec3u \u0111\u1ed3 c\u1ed9t\n\nsns.countplot(x='target', data=train_data)\nplt.show()","ea8c4d13":"# ki\u1ec3m tra xem t\u1eadp train c\u00f3 ch\u1ee9a null values ho\u1eb7c duplicates kh\u00f4ng\n\nprint(\"Total Missing values in the Dataset :\",train_data.isnull().sum().sum())\nprint(\"Total Duplicated values in the Dataset :\",train_data.duplicated().sum())","0d3ee916":"# \u0110\u1ed1i v\u1edbi t\u1eadp train \n# \u0111\u1ed9 d\u00e0i c\u00e2u h\u1ecfi\ntrain_data['qlen'] = train_data['question_text'].str.len() \n\n# s\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong c\u00e2u h\u1ecfis \ntrain_data['n_words'] = train_data['question_text'].apply(lambda row: len(row.split(\" \")))\n\n# s\u1ed1 l\u01b0\u1ee3ng t\u1eeb d\u1ea1ng s\u1ed1 trong c\u00e2u h\u1ecfi\ntrain_data['numeric_words'] = train_data['question_text'].apply(lambda row: sum(c.isdigit() for c in row))\n\n# s\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t trong c\u00e2u h\u1ecfi\ntrain_data['sp_char_words'] = train_data['question_text'].str.findall(r'[^a-zA-Z0\u20139 ]').str.len()\n\n# s\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 trong c\u00e2u h\u1ecfi\ntrain_data['char_words'] = train_data['question_text'].apply(lambda row: len(str(row)))\n\n# s\u1ed1 l\u01b0\u1ee3ng t\u1eeb duy nh\u1ea5t trong c\u00e2u h\u1ecfi\ntrain_data['unique_words'] = train_data['question_text'].apply(lambda row: len(set(str(row).split())))\n\n# th\u1eed in ra v\u00e0i d\u00f2ng \u0111\u1ea7u \u0111\u1ec3 ki\u1ec3m tra\nprint(train_data.head())","6f454805":"# \u0110\u1ed1i v\u1edbi t\u1eadp test\n# \u0111\u1ed9 d\u00e0i c\u00e2u h\u1ecfi\ntest_data['qlen'] = test_data['question_text'].str.len() \n\n# s\u1ed1 l\u01b0\u1ee3ng t\u1eeb trong c\u00e2u h\u1ecfi \ntest_data['n_words'] = test_data['question_text'].apply(lambda row: len(row.split(\" \")))\n\n# s\u1ed1 l\u01b0\u1ee3ng t\u1eeb d\u1ea1ng s\u1ed1 trong c\u00e2u h\u1ecfi\ntest_data['numeric_words'] = test_data['question_text'].apply(lambda row: sum(c.isdigit() for c in row))\n\n# s\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t trong c\u00e2u h\u1ecfi\ntest_data['sp_char_words'] = test_data['question_text'].str.findall(r'[^a-zA-Z0\u20139 ]').str.len()\n\n# s\u1ed1 l\u01b0\u1ee3ng k\u00fd t\u1ef1 trong c\u00e2u h\u1ecfi\ntest_data['char_words'] = test_data['question_text'].apply(lambda row: len(str(row)))\n\n# s\u1ed1 l\u01b0\u1ee3ng t\u1eeb duy nh\u1ea5t trong c\u00e2u h\u1ecfi\ntest_data['unique_words'] = test_data['question_text'].apply(lambda row: len(set(str(row).split())))\n\n# th\u1eed in ra v\u00e0i d\u00f2ng \u0111\u1ea7u \u0111\u1ec3 ki\u1ec3m tra\nprint(test_data.head())","a05dbb03":"# M\u1ed9t danh s\u00e1ch \u0111\u1ea7y \u0111\u1ee7 c\u00e1c d\u1ea5u v\u00e0 k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ebft \u0111\u01b0\u1ee3c li\u1ec7t k\u00ea trong m\u1ea3ng puncts\npuncts=[',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', \n        '\u2022', '~', '@', '\u00a3', '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`', '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a', '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', \n        '\u2588', '\u2026', '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u25ba', '\u2212', '\u00a2', '\u00ac', '\u2591', '\u00a1', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', \n        '\u2014', '\u2039', '\u2500', '\u2592', '\uff1a', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00b8', '\u22c5', '\u2018', '\u221e', \n        '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u2264', '\u2021', '\u221a', '\u25c4', '\u2501', \n        '\u21d2', '\u25b6', '\u2265', '\u255d', '\u2661', '\u25ca', '\u3002', '\u2708', '\u2261', '\u263a', '\u2714', '\u21b5', '\u2248', '\u2713', '\u2663', '\u260e', '\u2103', '\u25e6', '\u2514', '\u201f', '\uff5e', '\uff01', '\u25cb', \n        '\u25c6', '\u2116', '\u2660', '\u258c', '\u273f', '\u25b8', '\u2044', '\u25a1', '\u2756', '\u2726', '\uff0e', '\u00f7', '\uff5c', '\u2503', '\uff0f', '\uffe5', '\u2560', '\u21a9', '\u272d', '\u2590', '\u263c', '\u263b', '\u2510', \n        '\u251c', '\u00ab', '\u223c', '\u250c', '\u2109', '\u262e', '\u0e3f', '\u2266', '\u266c', '\u2727', '\u232a', '\uff0d', '\u2302', '\u2716', '\uff65', '\u25d5', '\u203b', '\u2016', '\u25c0', '\u2030', '\\x97', '\u21ba', \n        '\u2206', '\u2518', '\u252c', '\u256c', '\u060c', '\u2318', '\u2282', '\uff1e', '\u2329', '\u2399', '\uff1f', '\u2620', '\u21d0', '\u25ab', '\u2217', '\u2208', '\u2260', '\u2640', '\u2654', '\u02da', '\u2117', '\u2517', '\uff0a', \n        '\u253c', '\u2740', '\uff06', '\u2229', '\u2642', '\u203f', '\u2211', '\u2023', '\u279c', '\u251b', '\u21d3', '\u262f', '\u2296', '\u2600', '\u2533', '\uff1b', '\u2207', '\u21d1', '\u2730', '\u25c7', '\u266f', '\u261e', '\u00b4', \n        '\u2194', '\u250f', '\uff61', '\u25d8', '\u2202', '\u270c', '\u266d', '\u2523', '\u2534', '\u2513', '\u2728', '\\xa0', '\u02dc', '\u2765', '\u252b', '\u2120', '\u2712', '\uff3b', '\u222b', '\\x93', '\u2267', '\uff3d', \n        '\\x94', '\u2200', '\u265b', '\\x96', '\u2228', '\u25ce', '\u21bb', '\u21e9', '\uff1c', '\u226b', '\u2729', '\u272a', '\u2655', '\u061f', '\u20a4', '\u261b', '\u256e', '\u240a', '\uff0b', '\u2508', '\uff05', \n        '\u254b', '\u25bd', '\u21e8', '\u253b', '\u2297', '\uffe1', '\u0964', '\u2582', '\u272f', '\u2587', '\uff3f', '\u27a4', '\u271e', '\uff1d', '\u25b7', '\u25b3', '\u25d9', '\u2585', '\u271d', '\u2227', '\u2409', '\u262d', \n        '\u250a', '\u256f', '\u263e', '\u2794', '\u2234', '\\x92', '\u2583', '\u21b3', '\uff3e', '\u05f3', '\u27a2', '\u256d', '\u27a1', '\uff20', '\u2299', '\u2622', '\u02dd', '\u220f', '\u201e', '\u2225', '\u275d', '\u2610', \n        '\u2586', '\u2571', '\u22d9', '\u0e4f', '\u2601', '\u21d4', '\u2594', '\\x91', '\u279a', '\u25e1', '\u2570', '\\x85', '\u2662', '\u02d9', '\u06de', '\u2718', '\u272e', '\u2611', '\u22c6', '\u24d8', '\u2752', '\u2623', '\u2709', '\u230a', '\u27a0', '\u2223', '\u2751', '\u25e2', '\u24d2', '\\x80', '\u3012', '\u2215', '\u25ae', '\u29bf', '\u272b', '\u271a', '\u22ef', '\u2669', '\u2602', '\u275e', '\u2017', '\u0702', '\u261c', \n        '\u203e', '\u271c', '\u2572', '\u2218', '\u27e9', '\uff3c', '\u27e8', '\u0387', '\u2717', '\u265a', '\u2205', '\u24d4', '\u25e3', '\u0361', '\u201b', '\u2766', '\u25e0', '\u2704', '\u2744', '\u2203', '\u2423', '\u226a', '\uff62', \n        '\u2245', '\u25ef', '\u263d', '\u220e', '\uff63', '\u2767', '\u0305', '\u24d0', '\u2198', '\u2693', '\u25a3', '\u02d8', '\u222a', '\u21e2', '\u270d', '\u22a5', '\uff03', '\u23af', '\u21a0', '\u06e9', '\u2630', '\u25e5', \n        '\u2286', '\u273d', '\u26a1', '\u21aa', '\u2741', '\u2639', '\u25fc', '\u2603', '\u25e4', '\u274f', '\u24e2', '\u22b1', '\u279d', '\u0323', '\u2721', '\u2220', '\uff40', '\u25b4', '\u2524', '\u221d', '\u264f', '\u24d0', \n        '\u270e', '\u037e', '\u2424', '\uff07', '\u2763', '\u2702', '\u2724', '\u24de', '\u262a', '\u2734', '\u2312', '\u02db', '\u2652', '\uff04', '\u2736', '\u25bb', '\u24d4', '\u25cc', '\u25c8', '\u275a', '\u2742', '\uffe6', \n        '\u25c9', '\u255c', '\u0303', '\u2731', '\u2556', '\u2749', '\u24e1', '\u2197', '\u24e3', '\u267b', '\u27bd', '\u05c0', '\u2732', '\u272c', '\u2609', '\u2589', '\u2252', '\u2625', '\u2310', '\u2668', '\u2715', '\u24dd', \n        '\u22b0', '\u2758', '\uff02', '\u21e7', '\u0335', '\u27aa', '\u2581', '\u258f', '\u2283', '\u24db', '\u201a', '\u2670', '\u0301', '\u270f', '\u23d1', '\u0336', '\u24e2', '\u2a7e', '\uffe0', '\u274d', '\u2243', '\u22f0', '\u264b', \n        '\uff64', '\u0302', '\u274b', '\u2733', '\u24e4', '\u2564', '\u2595', '\u2323', '\u2738', '\u212e', '\u207a', '\u25a8', '\u2568', '\u24e5', '\u2648', '\u2743', '\u261d', '\u273b', '\u2287', '\u227b', '\u2658', '\u265e', \n        '\u25c2', '\u271f', '\u2320', '\u2720', '\u261a', '\u2725', '\u274a', '\u24d2', '\u2308', '\u2745', '\u24e1', '\u2667', '\u24de', '\u25ad', '\u2771', '\u24e3', '\u221f', '\u2615', '\u267a', '\u2235', '\u235d', '\u24d1', \n        '\u2735', '\u2723', '\u066d', '\u2646', '\u24d8', '\u2236', '\u269c', '\u25de', '\u0bcd', '\u2739', '\u27a5', '\u2195', '\u0333', '\u2237', '\u270b', '\u27a7', '\u220b', '\u033f', '\u0367', '\u2505', '\u2964', '\u2b06', '\u22f1', \n        '\u2604', '\u2196', '\u22ee', '\u06d4', '\u264c', '\u24db', '\u2555', '\u2653', '\u276f', '\u264d', '\u258b', '\u273a', '\u2b50', '\u273e', '\u264a', '\u27a3', '\u25bf', '\u24d1', '\u2649', '\u23e0', '\u25fe', '\u25b9', \n        '\u2a7d', '\u21a6', '\u2565', '\u2375', '\u230b', '\u0589', '\u27a8', '\u222e', '\u21e5', '\u24d7', '\u24d3', '\u207b', '\u239d', '\u2325', '\u2309', '\u25d4', '\u25d1', '\u273c', '\u264e', '\u2650', '\u256a', '\u229a', \n        '\u2612', '\u21e4', '\u24dc', '\u23a0', '\u25d0', '\u26a0', '\u255e', '\u25d7', '\u2395', '\u24e8', '\u261f', '\u24df', '\u265f', '\u2748', '\u21ac', '\u24d3', '\u25fb', '\u266e', '\u2759', '\u2664', '\u2209', '\u061b', \n        '\u2042', '\u24dd', '\u05be', '\u2651', '\u256b', '\u2553', '\u2573', '\u2b05', '\u2614', '\u2638', '\u2504', '\u2567', '\u05c3', '\u23a2', '\u2746', '\u22c4', '\u26ab', '\u030f', '\u260f', '\u279e', '\u0342', '\u2419', '\u24e4', '\u25df', '\u030a', '\u2690', '\u2719', '\u2199', '\u033e', '\u2118', '\u2737', '\u237a', '\u274c', '\u22a2', '\u25b5', '\u2705', '\u24d6', '\u2628', '\u25b0', '\u2561', '\u24dc', '\u2624', '\u223d', '\u2558', \n        '\u02f9', '\u21a8', '\u2659', '\u2b07', '\u2671', '\u2321', '\u2800', '\u255b', '\u2755', '\u2509', '\u24df', '\u0300', '\u2656', '\u24da', '\u2506', '\u239c', '\u25dc', '\u26be', '\u2934', '\u2707', '\u255f', '\u239b', \n        '\u2629', '\u27b2', '\u279f', '\u24e5', '\u24d7', '\u23dd', '\u25c3', '\u2562', '\u21af', '\u2706', '\u02c3', '\u2374', '\u2747', '\u26bd', '\u2552', '\u0338', '\u265c', '\u2613', '\u27b3', '\u21c4', '\u262c', '\u2691', \n        '\u2710', '\u2303', '\u25c5', '\u25a2', '\u2750', '\u220a', '\u2608', '\u0965', '\u23ae', '\u25a9', '\u0bc1', '\u22b9', '\u2035', '\u2414', '\u260a', '\u27b8', '\u030c', '\u263f', '\u21c9', '\u22b3', '\u2559', '\u24e6', \n        '\u21e3', '\uff5b', '\u0304', '\u219d', '\u239f', '\u258d', '\u2757', '\u05f4', '\u0384', '\u259e', '\u25c1', '\u26c4', '\u21dd', '\u23aa', '\u2641', '\u21e0', '\u2607', '\u270a', '\u0bbf', '\uff5d', '\u2b55', '\u2798', \n        '\u2040', '\u2619', '\u275b', '\u2753', '\u27f2', '\u21c0', '\u2272', '\u24d5', '\u23a5', '\\u06dd', '\u0364', '\u208b', '\u0331', '\u030e', '\u265d', '\u2273', '\u2599', '\u27ad', '\u0700', '\u24d6', '\u21db', '\u258a', \n        '\u21d7', '\u0337', '\u21f1', '\u2105', '\u24e7', '\u269b', '\u0310', '\u0315', '\u21cc', '\u2400', '\u224c', '\u24e6', '\u22a4', '\u0313', '\u2626', '\u24d5', '\u259c', '\u2799', '\u24e8', '\u2328', '\u25ee', '\u2637', \n        '\u25cd', '\u24da', '\u2254', '\u23e9', '\u2373', '\u211e', '\u250b', '\u02fb', '\u259a', '\u227a', '\u0652', '\u259f', '\u27bb', '\u032a', '\u23ea', '\u0309', '\u239e', '\u2507', '\u235f', '\u21ea', '\u258e', '\u21e6', '\u241d', \n        '\u2937', '\u2256', '\u27f6', '\u2657', '\u0334', '\u2644', '\u0368', '\u0308', '\u275c', '\u0321', '\u259b', '\u2701', '\u27a9', '\u0bbe', '\u02c2', '\u21a5', '\u23ce', '\u23b7', '\u0332', '\u2796', '\u21b2', '\u2a75', '\u0317', '\u2762', \n        '\u224e', '\u2694', '\u21c7', '\u0311', '\u22bf', '\u0316', '\u260d', '\u27b9', '\u294a', '\u2041', '\u2722']\ndef clean_punct(question):\n    for punct in puncts:\n        if punct in question:\n            question = question.replace(punct, '')\n    return question","cd638198":"def clean_numbers(question):\n    if bool(re.search(r'\\d', question)):  # x\u00e1c \u0111\u1ecbnh xem trong c\u00e2u c\u00f3 ch\u1ee9a s\u1ed1 hay kh\u00f4ng\n        question = re.sub('[0-9]{5,}', '#####', question) #Lo\u1ea1i b\u1ecf c\u00e1c s\u1ed1,thay b\u1eb1ng c\u00e1c '#'\n        question = re.sub('[0-9]{4}', '####', question)\n        question = re.sub('[0-9]{3}', '###', question)\n        question = re.sub('[0-9]{2}', '##', question)\n        question = re.sub('[0-9]{1}', '#', question)\n    return question","c9c7459f":"# T\u1ea1o m\u1ed9t directory c\u00f3 key l\u00e0 c\u00e1c t\u1eeb vi\u1ebft sai ch\u00ednh t\u1ea3 v\u00e0 value l\u00e0 c\u00e1c t\u1eeb vi\u1ebft \u0111\u00fang c\u1ee7a key\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'bitcoin', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization', \n                'electroneum':'bitcoin','nanodegree':'degree','hotstar':'star','dream11':'dream','ftre':'fire','tensorflow':'framework','unocoin':'bitcoin',\n                'lnmiit':'limit','unacademy':'academy','altcoin':'bitcoin','altcoins':'bitcoin','litecoin':'bitcoin','coinbase':'bitcoin','cryptocurency':'cryptocurrency',\n                'simpliv':'simple','quoras':'quora','schizoids':'psychopath','remainers':'remainder','twinflame':'soulmate','quorans':'quora','brexit':'demonetized',\n                'iiest':'institute','dceu':'comics','pessat':'exam','uceed':'college','bhakts':'devotee','boruto':'anime',\n                'cryptocoin':'bitcoin','blockchains':'blockchain','fiancee':'fiance','redmi':'smartphone','oneplus':'smartphone','qoura':'quora','deepmind':'framework','ryzen':'cpu','whattsapp':'whatsapp',\n                'undertale':'adventure','zenfone':'smartphone','cryptocurencies':'cryptocurrencies','koinex':'bitcoin','zebpay':'bitcoin','binance':'bitcoin','whtsapp':'whatsapp',\n                'reactjs':'framework','bittrex':'bitcoin','bitconnect':'bitcoin','bitfinex':'bitcoin','yourquote':'your quote','whyis':'why is','jiophone':'smartphone','dogecoin':'bitcoin','onecoin':'bitcoin','poloniex':'bitcoin','7700k':'cpu','angular2':'framework','segwit2x':'bitcoin','hashflare':'bitcoin','940mx':'gpu',\n                'openai':'framework','hashflare':'bitcoin','1050ti':'gpu','nearbuy':'near buy','freebitco':'bitcoin','antminer':'bitcoin','filecoin':'bitcoin','whatapp':'whatsapp',\n                'empowr':'empower','1080ti':'gpu','crytocurrency':'cryptocurrency','8700k':'cpu','whatsaap':'whatsapp','g4560':'cpu','payymoney':'pay money',\n                'fuckboys':'fuck boys','intenship':'internship','zcash':'bitcoin','demonatisation':'demonetization','narcicist':'narcissist','mastuburation':'masturbation',\n                'trignometric':'trigonometric','cryptocurreny':'cryptocurrency','howdid':'how did','crytocurrencies':'cryptocurrencies','phycopath':'psychopath',\n                'bytecoin':'bitcoin','possesiveness':'possessiveness','scollege':'college','humanties':'humanities','altacoin':'bitcoin','demonitised':'demonetized',\n                'bras\u00edlia':'brazilia','accolite':'accolyte','econimics':'economics','varrier':'warrier','quroa':'quora','statergy':'strategy','langague':'language',\n                'splatoon':'game','7600k':'cpu','gate2018':'gate 2018','in2018':'in 2018','narcassist':'narcissist','jiocoin':'bitcoin','hnlu':'hulu','7300hq':'cpu',\n                'weatern':'western','interledger':'blockchain','deplation':'deflation', 'cryptocurrencies':'cryptocurrency', 'bitcoin':'blockchain cryptocurrency',}\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n# t\u1ea1o th\u00eam m\u1ed9t danh s\u00e1ch mispellings_re ch\u1ee9a c\u00e1c key c\u1ee7a directory ch\u00ednh l\u00e0 c\u00e1c t\u1eeb vi\u1ebft sai ch\u00ednh t\u1ea3 \n\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\n# n\u1ebfu trong c\u00e2u h\u1ecfi ch\u1ee9a t\u1eeb sai ch\u00ednh t\u1ea3 thay th\u1ebf b\u1eb1ng value t\u01b0\u01a1ng \u1ee9ng\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n    return mispellings_re.sub(replace, text)","ba7d02aa":"# T\u1ea1o m\u1ed9t directory c\u00f3 key l\u00e0 c\u00e1c t\u1eeb vi\u1ebft t\u1eaft v\u00e0 value l\u00e0 c\u00e1c t\u1eeb kh\u00f4ng vi\u1ebft t\u1eaft c\u1ee7a key\ncontraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n                    \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \n                    \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n                    \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\",\n                    \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n                    \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                    \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n                    \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n                    \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n                    \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                    \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                    \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \n                    \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                    \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n                    \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n                    \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n                    \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n\ndef _get_contractions(contraction_dict):\n    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n    return contraction_dict, contraction_re\n\n# t\u1ea1o th\u00eam m\u1ed9t danh s\u00e1ch contractions_re ch\u1ee9a c\u00e1c key c\u1ee7a directory ch\u00ednh l\u00e0 c\u00e1c t\u1eeb vi\u1ebft t\u1eaft\ncontractions, contractions_re = _get_contractions(contraction_dict)\n\n## n\u1ebfu trong c\u00e2u h\u1ecfi ch\u1ee9a t\u1eeb vi\u1ebft t\u1eaft thay th\u1ebf b\u1eb1ng value t\u01b0\u01a1ng \u1ee9ng\ndef replace_contractions(text):\n    def replace(match):\n        return contractions[match.group(0)]\n    return contractions_re.sub(replace, text)","3d3baa26":"\ndef to_lower(question):\n    return question.lower()","1771f471":"# H\u00e0m n\u00e0y \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 g\u1ecdi t\u1ea5t c\u1ea3 c\u00e1c h\u00e0m x\u1eed l\u00fd m\u00e0 ch\u00fang ta \u0111\u00e3 x\u00e1c \u0111\u1ecbnh tr\u01b0\u1edbc \u0111\u00f3 tr\u00ean question_text.\n\ndef clean_sentence(question):\n    question = clean_punct(question)\n    question = clean_numbers(question)\n    question = replace_typical_misspell(question)\n    question = replace_contractions(question)\n    question = to_lower(question)\n    question = question.replace(\"'\", \"\")\n    return question\n    ","cbe08f1a":"# ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u c\u1ee7a t\u1eadp train\ntrain_data['preprocessed_question_text'] = train_data['question_text'].apply(lambda x: clean_sentence(x))\nprint(train_data.preprocessed_question_text.head())","5bf9f4d1":"# ti\u1ec1n x\u1eed l\u00fd d\u1eef li\u1ec7u c\u1ee7a t\u1eadp test\ntest_data['preprocessed_question_text'] = test_data['question_text'].apply(lambda x: clean_sentence(x))\ntest_data.preprocessed_question_text.head()","d5c8c092":"# random 90% l\u00e0m m\u1ea3ng train, 10% l\u00e0m m\u1ea3ng validation\ntrain_data, val_data = train_test_split(train_data, test_size=0.1, random_state=123)\n\n## some config values \nembed_size = 300 # k\u00edch th\u01b0\u1edbc m\u1ed9t word vector\nmax_features = 50000 # s\u1ed1 l\u01b0\u1ee3ng t\u1eeb unique \u0111c s\u1eed d\u1ee5ng (s\u1ed1 d\u00f2ng c\u1ee7a embedding vector)\nmaxlen = 100 # s\u1ed1 t\u1eeb nhi\u1ec1u nh\u1ea5t trong m\u1ed9t c\u00e2u\n\n## thay c\u00e1c missing values trong c\u1ed9t c\u00e2u h\u1ecfi b\u1eb1ng gi\u00e1 tr\u1ecb 'na'\ntrain_X = train_data[\"preprocessed_question_text\"].fillna(\"_na_\").values\nval_X = val_data[\"preprocessed_question_text\"].fillna(\"_na_\").values\ntest_X = test_data[\"preprocessed_question_text\"].fillna(\"_na_\").values\n\n## s\u1eed d\u1ee5ng thu\u1eadt to\u00e1n t\u00e1ch t\u1eeb v\u00e0 chuy\u1ec3n \u0111\u1ed5i ch\u00fang th\u00e0nh chu\u1ed7i vector\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad c\u00e2u: \u0111\u01b0a s\u1ed1 t\u1eeb trong c\u00e2u v\u1ec1 maxlen b\u1eb1ng vi\u1ec7c th\u00eam v\u00e0o c\u00e1c s\u1ed1 0\ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## l\u1ea5y ra c\u00e1c target\ntrain_y = train_data['target'].values\nval_y = val_data['target'].values","e756dddc":"# kh\u1edfi t\u1ea1o input cho model\ninp = Input(shape=(maxlen,))\n\n# kh\u1edfi t\u1ea1o output cho model\nx = Embedding(max_features, embed_size)(inp)\nx = Bidirectional(LSTM(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\n\n# kh\u1edfi t\u1ea1o moodel\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","6846df23":"# ti\u1ebfn h\u00e0nh train model\nmodel.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))","224bc3fa":"# th\u1eed d\u1ef1 \u0111o\u00e1n v\u00e0 \u0111\u00e1nh gi\u00e1 model tr\u00ean t\u1eadp validation\npred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\nfor thresh in np.arange(0.1, 0.501, 0.01):\n    thresh = np.round(thresh, 2)\n    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))","f89d1be4":"# ti\u1ebfn h\u00e0nh d\u1ef1 \u0111o\u00e1n tr\u00ean t\u1eadp test\npred_noemb_test_y = model.predict([test_X], batch_size=1024, verbose=1)","8a3174d2":"# t\u1ea1o file submission\npred_noemb_test_y = (pred_noemb_test_y > 0.33).astype(int)\nout_df = pd.DataFrame({\"qid\":test_data[\"qid\"].values})\nout_df['prediction'] = pred_noemb_test_y\nout_df.to_csv(\"submission.csv\", index=False)\nprint('Successfully saved submission')\npred_noemb_test_y","db5a6fb0":"*Lo\u1ea1i b\u1ecf c\u00e1c d\u1ea5u v\u00e0 k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t trong c\u00e2u*","d0c46024":"Lo\u1ea1i b\u1ecf c\u00e1c d\u1eef li\u1ec7u kh\u00f4ng c\u1ea7n thi\u1ebft, l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u \u0111\u1ec3 vi\u1ec7c d\u1ef1 \u0111o\u00e1n \u0111\u1ea1t k\u1ebft qu\u1ea3 t\u1ed1t nh\u1ea5t\n* Lo\u1ea1i b\u1ecf c\u00e1c d\u1ea5u v\u00e0 k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t trong c\u00e2u\n* Lo\u1ea1i b\u1ecf c\u00e1c s\u1ed1 trong c\u00e2u\n* S\u1eeda c\u00e1c t\u1eeb sai ch\u00ednh t\u1ea3 th\u01b0\u1eddng g\u1eb7p\n* Lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb vi\u1ebft t\u1eaft","604e926d":"**Data Exploration**","1cc3cf4d":"Chia t\u1eadp train v\u00e0 t\u1eadp validation","1b41910c":"*S\u1eeda c\u00e1c t\u1eeb sai ch\u00ednh t\u1ea3 th\u01b0\u1eddng g\u1eb7p*","87ba5ff6":"**Bidirectional GRU**","7601a7c7":"*Feature Enginerring*\n\n    T\u1eeb t\u1eadp d\u1eef li\u1ec7u th\u00f4 ban \u0111\u1ea7u, ta tr\u00edch xu\u1ea5t ra th\u00eam m\u1ed9t s\u1ed1 \u0111\u1eb7c tr\u01b0ng (features) \u0111\u1ec3 hi\u1ec3u th\u00eam v\u00e0 gi\u00fap bi\u1ec3u di\u1ec5n t\u1eadp d\u1eef li\u1ec7u ban \u0111\u1ea7u t\u1ed1t h\u01a1n tr\u01b0\u1edbc khi \u0111i v\u00e0o l\u00e0m s\u1ea1ch d\u1eef li\u1ec7u","a427b09e":"*Lo\u1ea1i b\u1ecf c\u00e1c s\u1ed1 trong c\u00e2u*","7d88c125":"*Lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb vi\u1ebft t\u1eaft*","a1f2b0d1":"**Data Preprocessing**"}}