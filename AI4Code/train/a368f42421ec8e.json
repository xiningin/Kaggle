{"cell_type":{"d5f0cc05":"code","692e1076":"code","95bc8e0c":"code","a87a456c":"code","f60324b0":"code","56aff5e6":"code","39ceaf49":"code","a02dc011":"markdown","90c3115c":"markdown","7c5a862f":"markdown","13dc0c32":"markdown","70cb7a9a":"markdown","9978ef3b":"markdown"},"source":{"d5f0cc05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","692e1076":"from numpy import array\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.embeddings import Embedding\n","95bc8e0c":"# define documents\ndocs = ['Well done!',\n\t\t'Good work',\n\t\t'Great effort',\n\t\t'nice work',\n\t\t'Excellent!',\n\t\t'Weak',\n\t\t'Poor effort!',\n\t\t'not good',\n\t\t'poor work',\n\t\t'Could have done better.']\n# define class labels\nlabels = array([1,1,1,1,1,0,0,0,0,0])\n","a87a456c":"# integer encode the documents\nvocab_size = 50\nencoded_docs = [one_hot(d, vocab_size) for d in docs]\nprint(encoded_docs)\n","f60324b0":"# pad documents to a max length of 4 words\nmax_length = 4\npadded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\nprint(padded_docs)\n","56aff5e6":"# define the model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 8, input_length=max_length))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())\n","39ceaf49":"# fit the model\nmodel.fit(padded_docs, labels, epochs=50, verbose=0)\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\nprint('Accuracy: %f' % (accuracy*100))","a02dc011":"# Define Embedding Layer\nWe are now ready to define our Embedding layer as part of our neural network model.\n\nThe Embedding layer has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions.\n\nThe model is a simple binary classification model. Importantly, the output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. We flatten this to a one 32-element vector to pass on to the Dense output layer.","90c3115c":"# Integer Encode Document\n\nNext, we can integer encode each document. This means that as input the Embedding layer will have sequences of integers. We could experiment with other more sophisticated bag of word model encoding like counts or TF-IDF.\n\nKeras provides the one_hot() function that creates a hash of each word as an efficient integer encoding. We will estimate the vocabulary size of 50, which is much larger than needed to reduce the probability of collisions from the hash function.","7c5a862f":"# Padding\n\nThe sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. We will pad all input sequences to have the length of 4. Again, we can do this with a built in Keras function, in this case the pad_sequences() function.","13dc0c32":"# Imports","70cb7a9a":"# Problem Definition\n\nWe will define a small problem where we have 10 text documents, each with a comment about a piece of work a student submitted. Each text document is classified as positive \u201c1\u201d or negative \u201c0\u201d. This is a simple sentiment analysis problem.\n\nFirst, we will define the documents and their class labels.","9978ef3b":"# Training and Evaluation\nFinally, we can fit and evaluate the classification model."}}