{"cell_type":{"bd1150a1":"code","1849dd1c":"code","ce56e004":"code","db24cc90":"code","a6768cb5":"code","473d80bc":"code","19b67718":"code","b29c4c50":"code","ed98b498":"code","2bba83ba":"code","86b3c67e":"code","5dae752c":"code","d2645c07":"code","23f50d09":"code","40afa071":"code","083d87e7":"code","9f33d31e":"code","3e1d8eb0":"code","f0c73ac7":"code","bc833754":"code","5a35782a":"code","bac45364":"code","e551fdf4":"code","b03e6ad2":"code","c49e2ac6":"code","e25d35a5":"code","c82dec0e":"code","81f8d907":"code","cc537fde":"code","ac3ab034":"code","71b326a7":"code","cd272e76":"code","20b7209c":"code","8fc6720a":"code","56211864":"code","8d5eccab":"code","ed6f2542":"code","c9d4269e":"code","c0bda6f6":"code","f8c2f5e9":"code","cf62a13d":"code","97c2139b":"code","893b2469":"code","7e940adc":"code","313b14d6":"code","59d6552e":"code","687d3a8f":"code","76bce887":"code","6781046b":"code","0da7b807":"code","38cd8797":"code","d23a3134":"code","0538e283":"code","77665c8e":"code","cf10e9a5":"code","1c3ef0fc":"code","f6dd214b":"code","71fa1b58":"code","c14e7cc9":"code","d1046fea":"code","de51c346":"code","47e30d3a":"code","65d46f2e":"code","70c989de":"code","8aed8274":"markdown","193f001e":"markdown","f60be846":"markdown","63876dd6":"markdown","6191dc11":"markdown","c1eb4599":"markdown","476a6967":"markdown","8dfb4dc5":"markdown","6d8a2797":"markdown","67d3bcce":"markdown","1aa67af0":"markdown","c3905942":"markdown","1925d019":"markdown","6ef41736":"markdown","d3228f33":"markdown","42dc86dd":"markdown","7f866a2a":"markdown","83ab2684":"markdown","4031d919":"markdown","e70b5753":"markdown","12deebe4":"markdown","8ab7f62b":"markdown","e5b2fcec":"markdown","453c70ea":"markdown","7be0cf06":"markdown","c36eb1fe":"markdown","93ca7615":"markdown","e34ef636":"markdown","a1acbd81":"markdown","835a472f":"markdown","27ca87b5":"markdown","d9378578":"markdown","7adf8963":"markdown","b52e17cd":"markdown","88b8a77e":"markdown","9c6211d0":"markdown","8631ae7d":"markdown","ad1c43fd":"markdown","6454a6a5":"markdown","1b6a1e99":"markdown","4f07ad75":"markdown","8cf9cf75":"markdown","d9f3cb4b":"markdown"},"source":{"bd1150a1":"%%capture\n!pip install contractions\n!pip install textstat","1849dd1c":"import numpy as np\nimport pandas as pd\nimport regex as re\nimport os\n\nimport contractions\nimport urllib.parse\n\nfrom tabulate import tabulate\nfrom collections import defaultdict, Counter\nfrom termcolor import colored, cprint\n\nimport nltk\nfrom nltk.util import ngrams\nfrom wordcloud import WordCloud\nimport textstat\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(0)","ce56e004":"# Set font size\nplt.rcParams.update({'font.size': 12})\n\n# Set seaborn plot axis style\nsns.set_style('ticks')\n\n# Set color pallete\nPAL = sns.color_palette('BrBG', 9)\nsns.palplot(PAL)","db24cc90":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntrain_df.head()","a6768cb5":"train_df.info()","473d80bc":"# A summary for numerical columns\ntrain_df.describe().T","19b67718":"# A summary for nonnumerical columns\ntrain_df.select_dtypes(include=[object]).describe().T","b29c4c50":"def tab_data(df):\n    headers = ['Column', 'Null Count', 'Unique Count']\n    meta_list = []\n    cols = [i for i in df.columns]\n    for col in cols:\n        temp = []\n        temp.append(col)\n        temp.append(df[col].isna().sum())\n        temp.append(df[col].nunique())\n        meta_list.append(temp)\n    print(tabulate(meta_list, headers, tablefmt='rst'))\n\nprint('Train Set: Missing and Unique Values Summary')\ntab_data(train_df)","ed98b498":"# Check if the url_legal and license columns share the same rows with missing values\nurl_no_license = len(train_df.loc[train_df['url_legal'].notna() & train_df['license'].isna()])\nprint(f'Number of URLs without a license: {url_no_license}')","2bba83ba":"# Check the excerpts refer to the same URL\nurl_count = train_df.groupby(['url_legal']).agg({'url_legal': 'count'})\nurl_count.columns = ['url_count']\nurl_count.loc[url_count.url_count > 1].drop_duplicates(subset='url_count')","86b3c67e":"# Check the rows with the same values in target and standard_error column\ntrain_df.loc[train_df['target'] == train_df['standard_error']]","5dae752c":"test_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\ntest_df.head()","d2645c07":"test_df.info()","23f50d09":"print('Test Set: Missing and Unique Values Summary')\ntab_data(test_df)","40afa071":"def plot_error_vs_target(df, x, y, ax):\n    x_label = x.replace('_', ' ').title()\n    y_label = y.replace('_', ' ').title()\n    sns.scatterplot(\n        df[x],\n        df[y],\n        color=PAL[8],\n        alpha=.7,\n        ax=ax\n    )\n    sns.kdeplot(\n        df[x],\n        df[y],\n        levels=4,\n        color=PAL[0],\n        linewidths=2,\n        ax=ax\n    )\n    ax.set_title(\n        f'{y_label} vs {x_label}', \n        fontsize=20, \n        fontweight='bold', \n        fontfamily='serif'\n    )\n    ax.set_xlabel(f'{x_label}')\n    ax.set_ylabel(f'{y_label}')\n    \n    if y == 'standard_error':\n        ax.set_ylim([0.4, None])\n    if x == 'target':\n        limit_x = [\n            round(df[x].min(), 2),\n            round(df[x].max(), 2)\n        ]\n        ax.set_xticks(list(ax.get_xticks()) + limit_x)\n        ax.set_xlim([-4, 2])\n\ndef plot_grid(df, feats, n_rows=3, n_cols=2, clr=[0, 1, 7, 8]):\n    \n    fig = plt.figure(figsize=(n_cols*7, n_rows*7))\n    fig.patch.set_facecolor(PAL[4])\n    \n    grid = plt.GridSpec(n_rows, n_cols)\n    \n    means = {feat: round(df[f'{feat}'].mean(), 2) for feat in feats}\n    \n    for col, (feat, mean) in enumerate(means.items()):\n        for row in range(n_rows):\n            ax = plt.subplot(grid[row, col])\n            label = feat.replace('_', ' ')\n            if col == 1:\n                clr.sort(reverse=True)\n            if row == 0:\n                sns.histplot(\n                    df[feat],\n                    color=PAL[clr[3]],\n                    alpha=0.8,\n                    bins=20,\n                    ax=ax,\n                    label=label,\n                    kde=True,\n                )\n                ax.axvline(\n                    mean,\n                    color=PAL[clr[0]],\n                    linestyle='--',\n                    linewidth=2,\n                    label=f'mean: {mean}',\n                )\n                ax.legend(loc='upper left')\n                ax.set_xlabel(label.title())\n                ax.set_title(\n                    f'{label.title()} Distribution',\n                    fontsize=20,\n                    fontweight='bold',\n                    fontfamily='serif'\n                )\n            elif row == 1:\n                sns.boxplot(\n                    df[feat],\n                    orient='h',\n                    color=PAL[clr[2]],\n                    ax=ax\n                )\n                ax.set_xlabel(label.title())\n                ax.set_title(\n                    f'Box Plot {label.title()}',\n                    fontsize=20, \n                    fontweight='bold', \n                    fontfamily='serif'\n                )\n            elif row == 2:\n                ax =  plt.subplot(grid[row, :])\n                plot_error_vs_target(df, feats[0], feats[1], ax)\n    \n    plt.show()\n","083d87e7":"plot_grid(train_df, ['target', 'standard_error'])","9f33d31e":"# Binning the target to three levels\ntarget_bins = [i for i in range(-4, 3, 2)]\ntarget_labels = ['complex', 'medium', 'simple']\ntrain_df['level'] = pd.cut(\n    train_df['target'], bins=target_bins, labels=target_labels\n).astype('str')\n\n# Set text types\ntrain_df['text_type'] = 'classic'\ntrain_df.loc[train_df['url_legal'].notna(), 'text_type'] = 'modern'\n\n# Count level by text type and merge with train dataframe\nlevel_count = train_df.groupby(['text_type', 'level']).agg({'level': 'count'})\nlevel_count.columns = ['level_count']\ntrain_df = train_df.merge(level_count, how='left', on=['text_type', 'level'])\n\n# Create plot supporting dataframe\nlevel_df = train_df[['level', 'text_type', 'level_count']].copy()\nlevel_df = level_df.drop_duplicates().reset_index(drop=True)\nlevel_df = level_df.pivot(index='text_type', columns='level', values='level_count')\n\n# Get pecentage values\ntotals = level_df.sum(axis=1).tolist()\nlevels = [i for i in level_df]\nfor level in levels:\n    level_df[f'{level}_%'] = (level_df[level] \/ totals * 100).astype('float16')\n\n#level_df.head()","3e1d8eb0":"# Create figure\nfig = plt.figure(figsize=(15, 15))\nfig.patch.set_facecolor(PAL[4])\n\n# Define grid and subplots\ngrid = plt.GridSpec(2, 2)\nax1 = plt.subplot(grid[0, 0])\nax2 = plt.subplot(grid[0, 1])\nax3 = plt.subplot(grid[1, :])\n\ncount_plot = level_df[['simple', 'medium', 'complex']].plot(\n    kind='bar',\n    stacked=True,\n    color=[PAL[i] for i in [8, 1, 0]],\n    alpha=0.8,\n    rot=0,\n    xlabel='Text Type',\n    ylabel='Count',\n    ax=ax1,\n)\n\nfor container in count_plot.containers:\n    count_plot.bar_label(\n        container,\n        label_type='center',\n        fontsize=12,\n    )\n\nax1.set_title(\n    'Level Count vs Text Type',\n    fontsize=20,\n    fontweight='bold',\n    fontfamily='serif',\n)\nax1.legend(title='Text Level')\n\npercent_plot = level_df[['simple_%', 'medium_%', 'complex_%']].plot(\n    kind='bar',\n    stacked=True,\n    color=[PAL[i] for i in [8, 1, 0]],\n    alpha=0.7,\n    rot=0,\n    xlabel='Text Type',\n    ylabel='Percent',\n    ax=ax2,\n)\n\nfor container in percent_plot.containers:\n    percent_plot.bar_label(\n        container,\n        label_type='center',\n        fontsize=12,\n        fmt='%.2f%%'\n    )\n\nax2.set_title(\n    'Level Percentage vs Text Type',\n    fontsize=20,\n    fontweight='bold',\n    fontfamily='serif',\n)\nax2.get_legend().remove()\n\n\n# Plot most common domains vs target\nsns.kdeplot(\n    train_df['target'],\n    hue=train_df['text_type'],\n    fill=PAL,\n    color=PAL,\n    palette=([PAL[i] for i in [1, 7]]),\n    lw=0.1,\n    alpha=0.8,\n    ax=ax3,\n)\n\nax3.set_title(\n    'Text Type Distributions Comparison',\n    fontsize=20,\n    fontweight='bold',\n    fontfamily='serif',\n)\nax3.set_xlabel('Target')\nax3.get_legend().set_title('Text Type')\n\nplt.show()","f0c73ac7":"# Get domain name\ndef get_domain(url):\n    parsed_url = urllib.parse.urlparse(url)\n    domain = parsed_url.netloc\n    return domain\n\n\ntrain_df.loc[train_df['url_legal'].notna(), 'domain'] = train_df.loc[\n    train_df['url_legal'].notna(), 'url_legal'\n].apply(get_domain)\n\n# Domain count\ndomain_count = train_df['domain'].value_counts()\ndomain_dc = domain_count.to_dict()\n\ntrain_df['domain_count'] = train_df['domain'].map(domain_dc)\n\nprint(tabulate(domain_dc.items(), headers=['Domain Name', 'Count']))","bc833754":"wiki = ['simple.wikipedia.org', 'en.wikipedia.org', 'en.wikibooks.org']\nkids = ['kids.frontiersin.org', 'www.africanstorybook.org', 'freekidsbooks.org']\ntrain_df['wiki_kids'] = train_df['domain']\ntrain_df['wiki_kids'] = train_df['wiki_kids'].replace(wiki, 'Wikipedia')\ntrain_df['wiki_kids'] = train_df['wiki_kids'].replace(kids, 'Literature for Kids')","5a35782a":"# Create figure\nfig = plt.figure(figsize=(14, 21))\nfig.patch.set_facecolor(PAL[4])\n\n# Define grid and subplots\ngrid = plt.GridSpec(3, 2)\nax1 = plt.subplot(grid[0, :])\nax2 = plt.subplot(grid[1, :])\nax3 = plt.subplot(grid[2, :])\n\n# Domain plot\ndomain_plot = domain_count.plot(kind='barh', color=PAL[8], alpha=0.8, ax=ax1)\nax1.set_title(\n    'Domain Names',\n    fontsize=20,\n    fontweight='bold',\n    fontfamily='serif'\n)\n\nax1.set_xlabel('Count')\n\nfor container in domain_plot.containers:\n    domain_plot.bar_label(\n        container,\n        fontsize=12,\n        color=PAL[0],\n        padding=4,\n    )\n\nax1.invert_yaxis()\n\n# Most common domains vs target\nsns.kdeplot(\n    train_df['target'],\n    hue=train_df.loc[train_df['domain_count'] > 100]['domain'],\n    fill=PAL,\n    color=PAL,\n    palette=([PAL[i] for i in [1, 0, 7, 8]]),\n    lw=0.1,\n    alpha=0.8,\n    ax=ax2,\n)\n\nax2.set_title(\n    'Target Distributions of the Most Common Domain Names',\n    fontsize=20,\n    fontweight='bold',\n    fontfamily='serif',\n)\nax2.set_xlabel('Tagret')\nax2.get_legend().set_title('Domain Names')\n\n\n# Wikipedia and Kids Literature vs target\nsns.kdeplot(\n    train_df['target'],\n    hue=train_df.loc[train_df['wiki_kids'].isin(['Wikipedia', 'Literature for Kids'])]['wiki_kids'],\n    fill=PAL,\n    color=PAL,\n    palette=([PAL[i] for i in [1, 8]]),\n    lw=0.1,\n    alpha=0.8,\n    ax=ax3,\n)\n\nax3.set_title(\n    'Wikipedia and Literature for Kids',\n    fontsize=20,\n    fontweight='bold',\n    fontfamily='serif',\n)\nax3.set_xlabel('Tagret')\nax3.get_legend().set_title('Domain Groups')\n\nplt.show()","bac45364":"license_count = train_df['license'].value_counts()\nlicense_dc = license_count.to_dict()\ntrain_df['license_count'] = train_df['license'].map(license_dc)\n\nprint(tabulate(license_dc.items(), headers=['License', 'Count']))","e551fdf4":"# Create figure\nfig = plt.figure(figsize=(15, 15))\nfig.patch.set_facecolor(PAL[4])\n\n# Define grid and subplots\ngrid = plt.GridSpec(2, 1)\nax1 = plt.subplot(grid[0, 0])\nax2 = plt.subplot(grid[1, 0])\n\n# Licenses plot\nlicense_plot = license_count.plot(\n    kind='barh',\n    color=PAL[0],\n    alpha=0.8,\n    ax=ax1,\n)\n\nax1.set_title(\n    'Licenses',\n    fontsize=20,\n    fontweight='bold',\n    fontfamily='serif'\n)\n\nax1.set_xlabel('Count')\n\nfor container in license_plot.containers:\n    license_plot.bar_label(\n        container,\n        fontsize=12,\n        color=PAL[8],\n        padding=4,\n    )\n\nax1.invert_yaxis()\n\n\n# Most common licenses vs target\nsns.kdeplot(\n    train_df['target'],\n    hue=train_df.loc[train_df['license_count'] > 100]['license'],\n    fill=PAL,\n    color=PAL,\n    palette=([PAL[i] for i in [7, 0, 8]]),\n    lw=0.1,\n    alpha=0.8,\n    ax=ax2,\n)\n\nax2.set_title(\n    'Target Distributions of the Most Common Licenses',\n    fontsize=20,\n    fontweight='bold',\n    fontfamily='serif',\n)\nax2.set_xlabel('Target')\n\nplt.show()","b03e6ad2":"domain_4_dc = train_df[train_df.license == 'CC BY 4.0']['domain'].value_counts().to_dict()\nprint(tabulate(domain_4_dc.items(), headers=['Domain', 'CC BY 4.0']))","c49e2ac6":"def get_samples(df, col_1='level', col_2='text_type'):\n    samples = {}\n    level = df[col_1].unique().tolist()\n    level.sort(reverse=True)\n    text_type = df[col_2].unique().tolist()\n    for i in level:\n        for j in text_type:\n            idx = df.loc[(df[col_1] == i) & (df[col_2] == j)].sample().index[0]\n            target = round(df['target'][idx], 2)\n            excerpt = df['excerpt'][idx]\n            samples[f'{i} {j}'] = [target, excerpt]\n    return samples","e25d35a5":"# Extract basic metadata from excerpts\ndef get_meta(df, col):\n    df['sentences_per_excerpt'] = df[col].apply(lambda x: x.count('.'))\n    df['words_per_excerpt'] = df[col].str.split().map(lambda x: len(x))\n    df['characters_per_excerpt'] = df[col].apply(lambda x: len(x))\n\n    df['words_per_sentence'] = df[col].str.split('.').apply(\n    lambda x: [len(i.split()) for i in x]).map(\n    lambda x: np.mean(x[:-1])\n    )\n    df['characters_per_sentense'] = df[col].str.split('.').apply(\n    lambda x: [len(i) for i in x]).map(\n    lambda x: np.mean(x[:-1])\n    )\n    df['characters_per_word'] = df[col].str.split().apply(\n    lambda x: [len(i) for i in x]).map(\n    lambda x: np.mean(x)\n    )\n    return df\n\ntrain_df = get_meta(train_df, 'excerpt')","c82dec0e":"samples = get_samples(train_df)\nn_words = train_df['words_per_excerpt'].tolist()\n\nprint(\n    colored(\n        f'Train contains {train_df.shape[0]}' \n        + f' excerpts, ranging from {min(n_words)} to {max(n_words)}' \n        + f' (avg {round(np.mean(n_words))}) words long.',\n        'yellow',\n        attrs=['bold']\n    )\n)\n\nfor level, (target, text) in samples.items():\n    if target > 0:\n        color = 'green'\n    elif target < -2:\n        color = 'red'\n    else:\n        color = 'yellow'\n    print(colored('---' * 10, color))\n    level = colored(level.upper(), color, attrs=['bold'])\n    target = colored(target, color, attrs=['bold'])\n    print(f'Train sample of {level} text with the target value: {target}')\n    print(colored('---' * 10, color))\n    print(text)","81f8d907":"def plot_stats(df, features, target, clr=[0,8]):\n\n    # Mean values\n    means = {}\n    for feat in features:\n        means[feat] = round(df[feat].mean())\n\n    # Create figure\n    n_rows = len(features)\n    n_cols = 2\n    fig = plt.figure(figsize=(n_cols * 7, n_rows * 7))\n    fig.patch.set_facecolor(PAL[4])\n\n    # Define grid and subplots\n    grid = plt.GridSpec(n_rows, n_cols)\n    for row, (feat, mean) in enumerate(means.items()):\n        title = feat.replace('_', ' ').title().replace(' Per ', ' per ')\n        for col in range(n_cols):\n            ax = plt.subplot(grid[row, col])\n            if col == 0:\n                sns.histplot(\n                    df[feat],\n                    color=PAL[clr[1]],\n                    alpha=0.8,\n                    bins=10,\n                    ax=ax,\n                    label=title.lower(),\n                    kde=True,\n                )\n                ax.axvline(\n                    mean,\n                    color=PAL[clr[0]],\n                    linestyle='--',\n                    linewidth=2,\n                    label=f'mean: {mean}',\n                )\n                ax.legend(loc='upper right')\n                ax.set_xlabel(title)\n                ax.set_ylabel('Excerpt Count')\n                ax.set_title(\n                    title,\n                    fontsize=20,\n                    fontweight='bold',\n                    fontfamily='serif'\n                )\n\n            elif col == 1:\n                sns.regplot(\n                    df[feat],\n                    df[target],\n                    order=1,\n                    color=PAL[clr[1]],\n                    line_kws={'color': PAL[clr[0]]},\n                    scatter_kws={'alpha': 0.3},\n                    ax=ax,\n                )\n                ax.set_xlabel(title)\n                ax.set_ylabel(target.capitalize())\n                ax.set_title(\n                    f'{target.capitalize()} vs {title}',\n                    fontsize=20,\n                    fontweight='bold',\n                    fontfamily='serif',\n                )\n\n            else:\n                print(f'Column with value {col} is out of plot grid')\n\n    plt.show()","cc537fde":"excerpt_features = [i for i in train_df.columns[-6:]]\nplot_stats(train_df, excerpt_features, 'target')","ac3ab034":"def get_stat(df, col):\n    df['flesch_reading_ease'] = df[col].apply(lambda x: textstat.flesch_reading_ease(x))\n    df['flesch_kincaid_grade'] = df[col].apply(lambda x: textstat.flesch_kincaid_grade(x))\n    df['gunning_fog'] = df[col].apply(lambda x: textstat.gunning_fog(x))\n    df['smog_index'] = df[col].apply(lambda x: textstat.smog_index(x))\n    df['automated_readability_index'] = df[col].apply(lambda x: textstat.automated_readability_index(x))\n    df['coleman_liau_index'] = df[col].apply(lambda x: textstat.coleman_liau_index(x))\n    df['linsear_write_formula'] = df[col].apply(lambda x: textstat.linsear_write_formula(x))\n    df['dale_chall_readability_score'] = df[col].apply(lambda x: textstat.dale_chall_readability_score(x))\n    df['text_standard'] = df[col].apply(lambda x: textstat.text_standard(x, float_output=True))\n    return df","71b326a7":"train_df = get_stat(train_df, 'excerpt')\ntextstat_features = train_df.columns[-9:].tolist()\nplot_stats(train_df, textstat_features, 'target', clr=[1, 7])","cd272e76":"# Tokenize \ndef tokenize(text):\n    return re.findall(r'[\\w-]*\\p{L}[\\w-]*', text)\n\n\n# Remove stop words\nstopwords = nltk.corpus.stopwords.words('english')\n\ndef remove_stopwords(tokens):\n    return [token for token in tokens if token not in stopwords]\n\n\n# Piplene\npipeline = [contractions.fix, str.lower, tokenize, remove_stopwords]\n\n\n# Preprocess\ndef preprocess(text, pipeline):\n    tokens = text\n    for transform in pipeline:\n        tokens = transform(tokens)\n    return tokens\n\ntrain_df['tokens'] = train_df['excerpt'].apply(preprocess, pipeline=pipeline)","20b7209c":"def count_words(df, column='tokens', preprocess=None, min_freq=2):\n\n    # Process tokens and update counter\n    def update(doc):\n        tokens = doc if preprocess is None else preprocess(doc)\n        counter.update(tokens)\n\n    # Create counter and run through all data\n    counter = Counter()\n    df[column].map(update)\n\n    # Transform counter into data frame\n    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['frequency'])\n    freq_df = freq_df.query('frequency >= @min_freq')\n    freq_df.index.name = column\n    \n    return freq_df.sort_values('frequency', ascending=False)","8fc6720a":"freq_df = count_words(train_df)","56211864":"def plot_freq(df, n_items=20):\n    \n    fig = plt.figure(figsize=(15, 10))\n    fig.patch.set_facecolor(PAL[4])\n    ax = plt.subplot()\n\n    diagram = df.head(n_items).plot(\n        kind='barh',\n        color=PAL[8],\n        alpha = 0.7,\n        ax=ax\n    )\n    \n    title = df.index.name.title()\n    ax.set_title(\n             f'{n_items} Most Common {title}', \n             fontsize=20, \n             fontweight='bold', \n             fontfamily='serif'\n    )\n\n    ax.set_xlabel('Count')\n    ax.set_ylabel(title)\n\n    for container in diagram.containers:\n        diagram.bar_label(\n            container,\n            fontsize=12,\n            color=PAL[0],\n            padding=4,\n        )\n    \n    ax.invert_yaxis()\n    plt.show()","8d5eccab":"plot_freq(freq_df)","ed6f2542":"def plot_wordcloud(word_freq, max_words=200):\n    \n    wordcloud = WordCloud(\n        width=800,\n        height=400,\n        background_color= 'white',\n        max_font_size=150,\n        max_words=max_words\n    )\n    \n    wordcloud.generate_from_frequencies(word_freq)\n    \n    fig = plt.figure(figsize=(15, 7))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()","c9d4269e":"plot_wordcloud(freq_df.frequency.to_dict(), max_words=200)","c0bda6f6":"def ngrams(tokens, n=2):\n    return [' '.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])]","f8c2f5e9":"train_df['bigrams'] = train_df['tokens'].apply(ngrams, n=2)\nbigrams = count_words(train_df, 'bigrams')","cf62a13d":"plot_freq(bigrams)","97c2139b":"plot_wordcloud(bigrams.frequency.to_dict(), max_words=150)","893b2469":"train_df['trigrams'] = train_df['tokens'].apply(ngrams, n=3)\ntrigrams = count_words(train_df, 'trigrams')","7e940adc":"plot_freq(trigrams)","313b14d6":"plot_wordcloud(trigrams.frequency.to_dict(), max_words=150)","59d6552e":"freq_dict = freq_df.frequency.to_dict()\n\nrare_words = {k: v for k, v in freq_dict.items() if v <= 5}\nprint('Examples of Rare Words:', list(rare_words.keys())[:5])\n\ncommon_words = {k: v for k, v in freq_dict.items() if v > 100}\nprint('Examples of Common Words:', list(common_words.keys())[:5])\n\nlong_df = count_words(\n    train_df,\n    column='excerpt',\n    preprocess=lambda text: re.findall(r'\\w{6,}', text),\n)\nlong_words = long_df.frequency.to_dict()\nprint('Examples of Long Words:', list(long_words.keys())[:5])","687d3a8f":"def count_freq(corpus, vocab=rare_words):\n    counter = 0\n    for word in vocab:\n        if word in corpus:\n            counter += 1\n    return counter","76bce887":"%%time\ndef get_tokenstat(df, col):\n    df['tokens_per_corpus'] = df[col].map(len)\n    df['characters_per_corpus'] = df[col].apply(\n        lambda x: sum([len(i) for i in x])\n    )\n    df['characters_per_token'] = df[col].apply(\n        lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)\n    )\n    df['rare_tokens'] = df[col].apply(count_freq)\n    df['common_tokens'] = df[col].apply(count_freq, vocab=common_words)\n    df['long_tokens'] = df[col].apply(count_freq, vocab=long_words)\n    \n    return df\n\ntrain_df = get_tokenstat(train_df, 'tokens')","6781046b":"token_features = [i for i in train_df.columns[-6: ]]\nplot_stats(train_df, token_features, 'target', clr=[8,0])","0da7b807":"features = excerpt_features + textstat_features + token_features","38cd8797":"# Compute the correlation matrix\ncorr = train_df[features + ['target']].corr()\n\n# Create figure\nfig, ax = plt.subplots(figsize=(15, 15))\n\n# Generate colormap\ncmap = sns.color_palette('BrBG', 20, as_cmap=True)\n\n# Draw and configurate the heatmap\nsns.heatmap(corr, cmap=cmap, annot=True, fmt='.2f', center=0,\n            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.5})\n\nplt.show()","d23a3134":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, BayesianRidge, ARDRegression","0538e283":"X = train_df[features].values\ny = train_df.target.values.reshape(-1, 1)\nX.shape, y.shape","77665c8e":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","cf10e9a5":"models = [LinearRegression(), Ridge(), BayesianRidge(), ARDRegression()]\nperformance = {}\nfor model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_valid)\n    rmse = mean_squared_error(y_pred, y_valid, squared=True)\n    performance[model] = rmse\nprint(tabulate(performance.items(), headers=['Model', 'RMSE']))","1c3ef0fc":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.pipeline import FeatureUnion","f6dd214b":"# Transformers \ntransforms = list()\ntransforms.append(('mms', MinMaxScaler()))\ntransforms.append(('ss', StandardScaler()))\ntransforms.append(('rs', RobustScaler()))\ntransforms.append(('mas', MaxAbsScaler()))\ntransforms.append(('qt', QuantileTransformer(n_quantiles=100, output_distribution='normal')))\ntransforms.append(('kbd', KBinsDiscretizer(n_bins=20, encode='ordinal', strategy='uniform')))\ntransforms.append(('pca', PCA(n_components=7)))\ntransforms.append(('svd', TruncatedSVD(n_components=7)))\n\n# Concatenate the results\nfu = FeatureUnion(transforms)","71fa1b58":"trans_features = fu.fit_transform(train_df[features])\ntrans_features.shape","c14e7cc9":"%%capture\n!pip install -U sentence-transformers\nfrom sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('paraphrase-MiniLM-L6-v2')","d1046fea":"%%time\ntrain_embeddings = model.encode(train_df['excerpt'])","de51c346":"train_embeddings.shape","47e30d3a":"# Concatenate obtained features with original\nX = np.concatenate((trans_features, train_embeddings, X), axis=1)\ny = train_df.target.values.reshape(-1, 1)\nX.shape, y.shape","65d46f2e":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)","70c989de":"models = [LinearRegression(), Ridge(), BayesianRidge(), ARDRegression()]\nperformance = {}\nfor model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_valid)\n    rmse = mean_squared_error(y_pred, y_valid, squared=True)\n    performance[model] = rmse\nprint(tabulate(performance.items(), headers=['Model', 'RMSE']))","8aed8274":"## Text Type and Complexity Level Summary\n\n---\n\nIn the graphs above, the excerpts are filtered by source existence and sorted by level from simple to complex. The texts without sources are labeled as \"classic\", and the passages supported by URLs are called \"modern\". This is just an assumption that needs to be refined and tested during the prototyping phase to possible use within the feature generating loop.\n\nIn the \"modern\" text distribution (excerpts from the Internet), there are almost half as many complex passages and almost twice as many simple ones in comparison with the \"classic\" text distribution (excerpts without source). To find the possible reason for this, let's take a closer look at the `url_leagal` distribution, extract the domain names, and examine the level of complexity of the texts inside.\n\n**[Back to Table of Contents](#contents)**","193f001e":"## <a id=\"domain\">Domain Names<\/a>\n\n---\n","f60be846":"## <a id=\"readability\">Readability, Complexity, and Grade Level<\/a>\n\n---\n\nFor this tasks we'll use [Texstat](https:\/\/pypi.org\/project\/textstat\/) library. It helps us determine readability, complexity, and grade level of the excerpts. A brief features description we'll use: \n\n- `flesch_reading_ease`: returns the [Flesch Reading Ease Score](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch_reading_ease). While the maximum score is 121.22, there is no limit on how low the score can be. A negative score is valid;\n- `flesch_kincaid_grade`: returns the [Flesch-Kincaid Grade](https:\/\/en.wikipedia.org\/wiki\/Flesch%E2%80%93Kincaid_readability_tests#Flesch%E2%80%93Kincaid_grade_level) of the given text. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document;\n- `gunning_fog`: returns the [FOG index](https:\/\/en.wikipedia.org\/wiki\/Gunning_fog_index) of the given text. The Fog Scale (Gunning FOG Formula) grade formula in that a score of 9.3 means that a ninth grader would be able to read the document;\n- `smog_index`: returns the [SMOG index](https:\/\/en.wikipedia.org\/wiki\/SMOG) of the given text. This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document; Texts of fewer than 30 sentences are statistically invalid, because the SMOG formula was normed on 30-sentence samples. textstat requires at least 3 sentences for a result;\n- `automated_readability_index`: returns the [ARI (Automated Readability Index)](https:\/\/en.wikipedia.org\/wiki\/Automated_readability_index) which outputs a number that approximates the grade level needed to comprehend the text. For example if the ARI is 6.5, then the grade level to comprehend the text is 6th to 7th grade;\n- `coleman_liau_index`: returns the grade level of the text using the [Coleman-Liau Formula](https:\/\/en.wikipedia.org\/wiki\/Coleman%E2%80%93Liau_index). This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document;\n- `linsear_write_formula`: returns the grade level using the Linsear [Write Formula](https:\/\/en.wikipedia.org\/wiki\/Linsear_Write). This is a grade formula in that a score of 9.3 means that a ninth grader would be able to read the document;\n- `dale_chall_readability_score`: different from other tests, since it uses a lookup table of the most commonly used 3000 English words. Thus it returns the grade level using the [New Dale-Chall Formula](https:\/\/en.wikipedia.org\/wiki\/Dale%E2%80%93Chall_readability_formula);\n- `text_standard`: based upon all the above tests, returns the estimated school grade level required to understand the text. Optional float_output allows the score to be returned as a float. Defaults to False: *textstat.text_standard(text, float_output=False)*.","63876dd6":"# <a id=overview>Overview of the Data<\/a>\n\n---\n\n","6191dc11":"# <a id=\"further\">Further Exploration<\/a>\n\n---\n\n","c1eb4599":"**[Back to Table of Contents](#contents)**","476a6967":"**[Back to Table of Contents](#contents)**","8dfb4dc5":"## <a id=\"embeddings\">Sentence Embeddings<\/a>\n\n---\n\nThe second thing how we can extend the feature space is to generate sentence embeddings for each excerpt in the dataset and so get several hundred new features. One of the handy libraries for this is Sentence Transformers. We'll use a small model here - for our purpose is more than enough, but you can go ahead, play around, and chose the best one from over a hundred [pre-trained models](https:\/\/www.sbert.net\/docs\/pretrained_models.html) available. Some models are general purpose models, while others produce embeddings for specific use cases. Pre-trained models can be loaded by just passing the model name: `SentenceTransformer('model_name')`. More useful information can be found on [GitHub](https:\/\/github.com\/UKPLab\/sentence-transformers) and\/or [Sentence Transformers documentation](https:\/\/www.sbert.net\/docs\/quickstart.html)","6d8a2797":"## <a id=\"train\">Training Set<\/a>\n\n---","67d3bcce":"## <a id=\"improved\">Performance with Extended Features<\/a>\n\n---\n","1aa67af0":"## Test Dataset Summary\n\n---\n\nThe public test set has only 7 entries with 4 columns (`id`, `url_legal`, `license`, `excerpt`), but this shouldn't mislead you, the size of the hidden test set, as [was mentioned by the competition host](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/236335), is around 2000.\n\n**[Back to Table of Contents](#contents)**","c3905942":"# <a id=\"final\">Final Thoughts<\/a>\n\n---\n\n### Work in progress\n\n**[Back to Table of Contents](#contents)**","1925d019":"## Excerpts Summary \n\n---\n\nBy **comparing passages by hand**, we can confirm that there are seen readability boundaries among the groups of texts divided by the complexity level. As for assumptions about text types, we do not find great differences between the so-called \"modern\" and \"classic\" texts within the same groups.\n\nWe can also indicate that the `targets` are not always reliable. Sometimes texts with different target values not much distinguished from each other, on the other hand, some pairs with near the same target scores are clearly have not the same ease of reading. Of course, this is a subjective observation, but from time to time we meet obvious cases. For example, look at the excerpts `a666c1db9` and `b55026bd9` with almost the same target scores:\n\n<div class=\"alert alert-block alert-info\">\n<b>Target: 0.2316<\/b> \n<p>In capitalism, people may sell or lend their property, and other people may buy or borrow it. If one person wants to buy, and another person wants to sell to them, they do not need to get permission from higher power. People can have a market (buying and selling with each other) without anyone else telling them to. People who own capital are sometimes called capitalists (people who support capitalism are called capitalists, too). They can hire anyone who wants to work in their factories, shops or lands for them for the pay they offer.The word capital can be used to mean things that produce more things or money. For example, lands, factories, shops, tools and machines are capital. If someone has money that can be invested, that money is capital too. In capitalist systems, many people are workers (or proletarians). They are employed to earn money for living. People can choose to work for anyone who will hire them in a free market.<\/p>\n<\/div>\n\n<div class=\"alert alert-block alert-warning\">\n<b>Target: 0.2303<\/b> \n<p>This is Cat. This is Dog. Cat and Dog live in a house. A house with a door. A house with a roof. Cat and Dog have a ball. The ball is red and blue and green. Cat and Dog play with the ball. Cat throws the ball to Dog. Dog catches the ball. Dog throws the ball to Cat. Cat catches the ball. Then Cat throws the ball very high. Oh! oh! The ball is on the roof. The ball is on the roof of the house. Cat and Dog can see the ball. Cat and Dog cannot get to the ball. Cat and Dog cry. Then Elephant comes by. Elephant is big. Elephant can see the ball. Elephant can get to the ball. Elephant gets the ball from the roof. Elephant takes the ball from the roof of the house. Elephant gives the ball to Cat and Dog. Cat and Dog smile. Elephant smiles. Cat and Dog and Elephant smile.<\/p>\n<\/div>\n\nHopefully, these cases are not widespread and can be considered as noise. But we need to think about how to identify and approach such errors, taking into account the presence of the same noise in the test dataset.\n\n**Talking about statistics** we've extracted from the raw texts, we can note almost all correlated with the target and can perform as signals of the complexity of the texts. At first glance, some relationships are not obvious, such as `Sentence for Excerpt`, which indicates an inverse correlation with the target value (the more sentences in a passage, the easier it is to read). But if we get a closer look at related features such as the number of `Words per Excerpt`, `Words per Sentence`, etc., we verify that everything is reliable. In our case, fewer sentences in a passage means longer and more complex sentences, therefore, such texts are less readable and more difficult for students.\n\n**[Back to Table of Contents](#contents)**","6ef41736":"## Target and Standard Error Summary\n\n---\n\nTarget distribution is very close to normal with the values ranged from -3.68 to 1.71, and mean value -0.96. Most of the values in the standard error distribution range from 0.43 to 0.57 with a mean of 0.49, and as we can clarify from the box plot, there are quite a few outliers.\n\nGiven that the number of pairwise comparisons is the same for all passages, the outliers in the standard error distribution can be explained by the distribution of the evaluators. As mentioned earlier, excerpts were assessed by teachers in grades 3\u201312. Most of them teach in grades 6-10, a minority work with students in grades 3-5 and 11-12, respectively. The difference in the scores between these two subgroups from the minority may cause outliers corresponding to the complexity of the text on its poles.\n\n**[Back to Table of Contents](#contents)**","d3228f33":"## <a id=\"ngrams\">Ngrams Analysis<\/a>\n\n---\n","42dc86dd":"## <a id=\"target\">Target and Standard Error<\/a>\n\n---\n","7f866a2a":"# <a id=\"baseline\">Simple Baseline<\/a>\n\n---\n\nSo far we have twenty-one features that correlated to the target and we can create our first baseline model. It could be one of the regressors from the standard scikit-learn library. Here we'll train and compare four of them: two linear (LinearRegression, Ridge), and two bayesian (BayesianRidge, ARDRegression) regressors.","83ab2684":"## Plot Extracted Metadata\n\n---","4031d919":"**[Back to Table of Contents](#contents)**","e70b5753":"## <a id=\"word_cloud\">Word Clouds<\/a>\n\n---","12deebe4":"## <a id=\"corpus_stats\">Corpus Statistics<\/a>\n\n---\n","8ab7f62b":"## <a id=\"test\">Test Dataset<\/a>\n\n---","e5b2fcec":"## <a id=\"prep\">Text Preprocessing Pipeline<\/a>\n\n---\n\nFor further analysis, we need to preprocess our excerpts. First, we'll expand contractions (this reduces data redundancy and makes the code computationally cheaper), then convert strings to all lowercase, tokenize them, and finally remove all stopwords. For the EDA purposes, these should be sufficient, so our simple preprocessing pipeline will consist of four steps.","453c70ea":"## <a id=\"corr\">Correlation Matrix<\/a>\n\n---","7be0cf06":"**[Back to Table of Contents](#contents)**","c36eb1fe":"## Training Set Overview Summary\n\nThe training set represented by 2834 records in the 6 columns shown in the table above. \n\n### Missing Values \n\nTwo columns `url_legal` and `license` contain 2004 missing values represented in the same rows. All 830 URLs are provided with licenses.\n\nNext, I want to quote a fragment from the data description mentioned the nature of the text:\n\n> Note that the test set includes a slightly larger proportion of **modern** texts (the type of texts we want to generalize to) than the training set.\n\nWe have not explanation and can only guess what is meant by **modern** texts. But let's step back and try to think what the possible reason of the missing values in the dataset? \n\nThe first thing that comes to mind is that the missing values can be explained by another way of collecting\/processing data. For example, it could be excerpts from classical literature, school textbooks, or other teaching materials that were manually collected and included in the dataset. Thus, such passages are, so to speak, **\"classic\"**, and the texts scraped from the web sites are **\"modern\"**. \n\nOn the other hand we have a warning:\n\n> Also note that while licensing information is provided for the public test set (because the associated excerpts are available for display \/ use), **the hidden private test set includes only blank license \/ legal information.**\n\nSo as far as I understand, we have neither licenses nor URLs in the hidden test set. That means that the information in the `url_legal` and `license` columns are completely meaningless. \n\nFor the sake of completeness, we'll look at these columns in the EDA and even go a little deeper, but our assumption about the nature of **\"modern\"** and **\"classic\"** texts does not seem promising in this case. At least, simple filtering by source will not be enough to sort the texts.\n\n### Unique Values\n\nAmong the 2834 rows of the training dataset we have 667 unique URLs, supported by 15 types of digital licenses. The rest values in the columns are absolutely unique including the `target` and `standard_error` columns represented by continues numerical values. In light of the fact that we are already know about how the information was collected and ranked it seems to be sane. \n\n667 unique URLs out of 830 URLs in total means that some excerpts have the same URL and their targets should be very close to each other. But after testing this assumption, we found that only two excerpts (`195bb7384`, `7a1723dd0`) share to the same web page. Another 164 texts are refer to https:\/\/www.africanstorybook.org\/ and can be explained by how the API of this site works (all books and pages within them are linked to the same URL)\n\nFinally, in the training dataset, we have one row in which the target and standard error values are the same and equal to zero. As [was mentioned by one of the competition hosts](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/236403), it\u2019s nothing but the baseline for all other comparisons. \n\n**[Back to Table of Contents](#contents)**","93ca7615":"## <a id=\"licenses\">Licenses<\/a>\n\n---\n","e34ef636":"## <a id=\"excerpts\">Excerpts<\/a>\n\n___\n\nFirst, let's define a function to generate random samples with different text type and complexity level, and extract basic metadata from the excepts to build statistics.","a1acbd81":"## License Findings\n\n---\n\nThree licenses cover almost 94% of all excerpts provided with legal information. The most common is CC BY 4.0, with a distribution very similar to the \"Literature for Kids\" domain group distribution we examined earlier. And after a quick check, we just approved this assumption. However, legal information does not appear in the hidden test set, so it is even more useless than URLs and our search for a modern\/classic pattern.\n\n## So What About Modern and Classic Excerpst?\n\n---\n\nAfter exploring the basics from the training dataset we can admit that continuing to search in this direction is not the greatest idea, and here's why:\n\nMost \"modern\" excerpts are given by several websites, so the target distribution is biased and its shift towards simple texts is not a useful finding. Even if all texts in the hiding test set would have the URLs and they are the same as we observed in the training set there is no chance that they have the same distribution. More likely, that in addition, we'll get the excerpts from the rare URLs such as osu.edu, ck12.org, etc. And this part of the so-called \"modern\" excerpts will definitely be different.\n\nIn the next step, we'll finally take a close look at the excerpts themselves and use the concept of modern\/classic text types one last time.\n\n**[Back to Table of Contents](#contents)**","835a472f":"## Corpus Statistics Summary\n\n---\n\n**[Back to Table of Contents](#contents)**","27ca87b5":"## Bigrams Wordcloud","d9378578":"## <a id=\"trans\">Feature Scaling and Transformation<\/a>\n\n---\n\nTo increase the number of features we can perform some basic preprocessing techniques like scaling, binning, linear dimensionality reduction, and so on. Here we'll use just a several, for more information on how they perform fill free to explore scikit-learn documentation.","7adf8963":"## Trigrams Wordcloud","b52e17cd":"## <a id=\"reading\">Viewing Excerpts<\/a>\n\n---\n\nTo study the passages, we will generate and print random samples of texts of different types and complexity level. By re-running the cell below each time you will receive new set of excerpts and can compare them one by one. ","88b8a77e":"**[Back to Table of Contents](#contents)**","9c6211d0":"## <a id=\"word_freq\">Word Frequency Diagram<\/a>\n\n---\n","8631ae7d":"# <a id=\"ref\">References<\/a>\n\n---\n\n1. [CommonLit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize)\n2. [Latex Syntax](https:\/\/en.wikibooks.org\/wiki\/LaTeX\/Mathematics)\n3. [Mean Squared Error](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_error.html)\n3. [Additional Information About Dataset](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240423)\n4. [Bradley-Terry Model](https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model)\n5. [How Bradley-Terry Model Works](https:\/\/www.kaggle.com\/shaz13\/code-how-bradley-terry-model-works\/)\n7. [Contractions](https:\/\/github.com\/kootenpv\/contractions)\n8. [WordCloud API](https:\/\/amueller.github.io\/word_cloud\/generated\/wordcloud.WordCloud.html#wordcloud.WordCloud)\n9. [Textstat - Readability, Complexity, and Grade Level](https:\/\/pypi.org\/project\/textstat\/)\n10. [Sentence Transformers Documentation](https:\/\/www.sbert.net\/docs\/quickstart.html)\n\n**[Back to Table of Contents](#contents)**","ad1c43fd":"## <a id=\"text_level\">Text Type and Complexity Level<\/a>\n\n---","6454a6a5":"**[Back to Table of Contents](#contents)**","1b6a1e99":"# <a id=about>About<\/a>\n    \n---\n\nThis is an Exploratory Data Analysis (EDA) for the [CommonLit Readability Prize](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize), which I hope will provide you with meaningful insights and help you create better machine learning solutions.\n\n## <a id=statement>The Problem Statement<\/a>\n    \n---\n\nThe aim of this Natural Language Processing (NLP) challenge is to improve readability rating methods in order to make the process of learning English more effective and engaging. This is a code competition, and our specific goal is to build an algorithm that can predict the complexity level of a text while meeting the following requirements:\n    \n- CPU Notebook not more than 3 hours run-time\n- GPU Notebook not more than hours run-time\n- Internet access disabled\n- Freely and publicly available external data is allowed, including pre-trained models\n- Submission file must be named `submission.csv`\n\nThis is a regression problem in which we need to extract features from the text excerpts and predict the continuous target values. \n\n## <a id=metric>Evaluation Metric<\/a>\n    \n---    \n\nThe evaluation metric is the root mean squared error (RMSE) which defined as:\n    \n$$RMSE = \\sqrt{\\frac{1}{n}{\\sum_{i=1}^{n}(y_i-\\hat{y_i})^2}}$$\n    \nwhere $y_i$ is the predicted value, $\\hat{y_i}$ is the original value, and $n$ is the number of rows in the test data.\n    \nIn practice, we can write our own function or just use the `mean_squared_error` from the [scikit-learn library](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_squared_error.html), setting the `squared` parameter equal **False** (if True returns MSE value, if False returns RMSE value):\n    \n```python\nfrom sklearn.metrics import mean_squared_error\n\nrmse = mean_squared_error(y_true, y_pred, squared=False)\n```\n    \n## <a id=data>Data Description<\/a>\n    \n--- \n    \n#### Files\n\n- **train.csv** - the training\n- **test.csv** - the test set \n- **sample_submission.csv** - a sample submission file in the correct format\n\n#### Columns\n\n- `id` - unique ID for excerpt\n- `url_legal` - URL of source - this is blank in the test set.\n- `license` - license of source material - this is blank in the test set.\n- `excerpt` - text to predict reading ease of\n- `target` - reading ease\n- `standard_error` - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n    \nAs was mentioned in [description](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/data), the texts taken from various domains, there are passages from several time periods and a wide range of reading ease scores.\n\n#### Additional Information\n    \nFrom one of the competition hosts, [Scott Crossley](https:\/\/www.kaggle.com\/cookiecutters), we also know that:\n    \n>The target value is the result of a Bradley-Terry analysis of more than 111,000 pairwise comparisons between excerpts. Teachers spanning grades 3-12 (a majority teaching between grades 6-10) served as the raters for these comparisons.\n\n>Standard error is included as an output of the Bradley-Terry analysis because individual raters saw only a fraction of the excerpts, while every excerpt was seen by numerous raters. The test and train sets were split after the target scores and s.e. were computed.\n\nAs Scott Crossley said in the [same discussion](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/240423), teachers grading passages were only asked one question: **Which text is easier for the students to understand?** \n    \nMore about the [Bradley-Terry](https:\/\/en.wikipedia.org\/wiki\/Bradley%E2%80%93Terry_model) analysis and pairwise comparison you can find in [this notebook](https:\/\/www.kaggle.com\/shaz13\/code-how-bradley-terry-model-works\/).\n\n**[Back to Table of Contents](#contents)**\n\nSo let\u2019s start!","4f07ad75":"## Domain Names Summary\n\n---\n\nWe can see that most of the URLs (over 87%) come from the top four domain names. In addition, among the domains, we can distinguish two groups of three elements each, representing 91% of all texts in the distribution. Wikipedia (\"simple.wikipedia.org\", \"en.wikipedia.org\", \"en.wikibooks.org\") provided a total of 380 ex\u0441erpts and almost repeat the target distribution of the training dataset. Other tree domains (\u201ckids.frontiersin.org\u201d, \u201cwww.africanstorybook.org\u201d, \u201cfreekidsbooks.org\u201d), which appear to be sources of children's literature, give us another 374 texts with a shifted complexity level towards simple. The vast amount of children's literature can explain the previously observed differences in the distribution of \"modern\" and \"classic\" texts.\n\n**[Back to Table of Contents](#contents)**","8cf9cf75":"## Get Metadata\n\n---","d9f3cb4b":"# Exploratory Data Analysis \n#### [CommonLit Readability Prize competition](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize)\n\n---\n\n# <a id=contents>Table of Contents<\/a>\n\n---\n\n#### [About](#about)\n- [The Problem Statement](#statement)\n- [Evaluation Metric](#metric)\n- [Data Description](#data)\n\n#### [Overview of the Data](#overview)\n- [Training Set](#train)\n- [Test Set](#test)\n- [Target and Standard Error](#target)\n- [Text Type and Complexity Level](#text_level)\n- [Domain Names](#domain)\n- [Licenses](#licenses)\n- [Excerpts](#excerpts)\n\n#### [Further Exploration](#further)\n- [Readability, Complexity, and Grade Level](#readability)\n- [Text Preprocessing Pipeline](#prep)\n- [Word Frequency Diagram](#word_freq)\n- [Word Clouds](#word_cloud)\n- [Ngrams Analysis](#ngrams)\n- [Corpus Statistics](#corpus_stats)\n- [Correlation Matrix](#corr)\n\n#### [Simple Baseline](#baseline)\n- [Feature Scaling and Transformation](#trans)\n- [Sentence Embedding](#embeddings)\n- [Performance with Extended Features](#improved)\n\n#### [Final Thoughts](#final)\n\n#### [References](#ref)"}}