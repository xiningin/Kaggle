{"cell_type":{"7b9eae50":"code","20eba263":"code","63c2286f":"code","aed7afa5":"code","dec61d39":"code","0dbcae49":"code","129975d5":"code","7c520b94":"code","766038ee":"code","5f875d8d":"code","4c924d2a":"code","6c679e65":"code","dbb0acf4":"code","540ec8cc":"code","ba25e076":"code","3f61b999":"code","6f8312b8":"code","4fd64cad":"code","c1187214":"code","e02c0161":"code","53115e04":"code","4d15d033":"code","aac5a59d":"code","929c913a":"code","9451f7b0":"code","6d5b2521":"code","20235723":"code","51f70e1f":"code","4f955c36":"code","6f72f696":"code","40aaa192":"code","b1dbaa07":"code","41c7f750":"code","7b50a0a4":"code","0ec49d00":"markdown","c4fb7f44":"markdown","8a2bd399":"markdown","93a3f531":"markdown","07b2f12a":"markdown","fed9a374":"markdown","a995e0e9":"markdown","972d1ba0":"markdown","12483551":"markdown","adb5c208":"markdown","59af71df":"markdown","16c5a836":"markdown","96b92a05":"markdown","0e4c2e0e":"markdown","580db0fb":"markdown","2c431da7":"markdown","3b64d978":"markdown","f56df000":"markdown","0851a841":"markdown","62735740":"markdown","d441b46b":"markdown","80489844":"markdown","413cc0eb":"markdown","219e07e7":"markdown","6c15d66c":"markdown","cc47f56b":"markdown","6aaffcc2":"markdown","359112e7":"markdown"},"source":{"7b9eae50":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for drawing\nimport seaborn as sns # for drawing as well\nfrom scipy.optimize import minimize # will use for training\nfrom sklearn.metrics import accuracy_score, f1_score # some metrics\nfrom functools import partial # better google what it is\nfrom tqdm import tqdm as tqdm # nice trackbar\n\nimport torch # this is the best library for neural networks (IMHO)\nimport torch.nn as nn # high-level API, useful for deep learning\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nos.listdir('\/kaggle\/input\/digit-recognizer')","20eba263":"train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\ntrain.head()","63c2286f":"pixels = np.array(train.iloc[0]) # get first line\npixels = pixels[1:] # remove label\npixels = pixels.reshape((28,28)) # reshape to normal image shape\n\nplt.imshow(pixels, cmap='gray');","aed7afa5":"# max pixel value is 255, but let's make it from 0 to 1\npixels01 = pixels \/ 255\n\n# make plot a bit bigger\nplt.figure(figsize=(14,11))\n\nsns.heatmap(np.round(pixels01, 1), annot=True, cmap='gray');","dec61d39":"X_train = np.array(train.drop('label', 1)) \/ 255\ny_train = np.array(train['label'])\n\nX_test = np.array(test) \/ 255\n\nprint('X_train.shape', X_train.shape)\nprint('y_train.shape', y_train.shape)\nprint('X_test.shape', X_train.shape)","0dbcae49":"# Let's make a random neuron\n\nneuron_r = np.random.normal(size=(784,))\nneuron_r \/= np.sqrt(784) # less variance\n\ndef sigmoid(x):\n    ex = np.exp(x)\n    return ex \/ (1 + ex)","129975d5":"# Let's apply a random neuron to some image:\nx = X_train[0]\noutput = sigmoid(x @ neuron_r)\nprint(output)","7c520b94":"# Let's make a new training set\ny_train1 = (y_train == 1).astype(int)\n\nprint(y_train1[:20])","766038ee":"predictions = sigmoid(X_train @ neuron_r)\nprint('Predictions:', predictions[:10])\nprint('Truth:', y_train1[:10])","5f875d8d":"def cross_entropy(y_true, y_pred):\n    out1 = y_true * np.log(y_pred + 1e-12) # small number for numerical stability\n    out2 = (1 - y_true) * np.log(1 - y_pred + 1e-12)\n    return -(out1 + out2).mean()\n\nprint(cross_entropy(y_train1, predictions))","4c924d2a":"%%time\n\ndef _loss(neuron, X, y):\n    predictions = sigmoid(X @ neuron)\n    error = cross_entropy(y, predictions)\n    return error\n\n# let's use only 500 examples, because otherwise it takes really long (later will solve it)\nerror = partial(_loss, X=X_train[:500], y=y_train1[:500])\n\nneuron_1 = minimize(error, x0=neuron_r).x","6c679e65":"predictions = sigmoid(X_train @ neuron_1)\nprint('Error:', cross_entropy(y_train1, predictions))","dbb0acf4":"print('Accuracy:', accuracy_score(y_train1, predictions > 0.5))\nprint('F1:', f1_score(y_train1, predictions > 0.5))","540ec8cc":"# this is array of weights for each pixel\nneuron_1\n\n# let's print it\nneuron_image = neuron_1.reshape(28, 28)\nsns.heatmap(neuron_image);","ba25e076":"%%time\n\nneurons = []\n\nfor digit in range(10):\n    print('Training neuron', digit)\n    y = y_train == digit\n    error = partial(_loss, X=X_train[:100], y=y[:100])\n    neuron = minimize(error, x0=neuron_r).x\n    neurons.append(neuron)","3f61b999":"fig, axes = plt.subplots(1, 10, figsize=(30,30))\n\nfor i in range(10):\n    axes[i].set_title(i)\n    neuron_img = neurons[i].reshape(28, 28)\n    axes[i].imshow(neuron_img)","6f8312b8":"digit_probabilities = sigmoid(X_train @ np.array(neurons).T)\nprint('digit_probabilities.shape:', digit_probabilities.shape)\n# Now we have probabilities for each image for each digit\n\n# Let's take maximum as prediction\npredictions = np.argmax(digit_probabilities, axis=1)\nprint('predictions.shape:', predictions.shape)\nprint('\\nTrain accuracy:', accuracy_score(y_train, predictions))","4fd64cad":"# Let's make test predictions\ndigit_probabilities = sigmoid(X_test @ np.array(neurons).T)\ntest_predictions = np.argmax(digit_probabilities, axis=1)\n\nsubmission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\nsubmission.Label = test_predictions\n\n# Save it as csv file\nsubmission.to_csv('submission_10n.csv', index=False)\n\nsubmission.head()","c1187214":"# Let's look at the example\nA = torch.tensor(10)\nA","e02c0161":"B = torch.tensor([1, 2, 3, 5])\nprint(B)\nC = torch.tensor([0, 1, 1, 2])\nprint(C)\nD = B + C\nprint(D)\nprint(D.shape, D.dtype)","53115e04":"# Let's write some strange function\nf = lambda x: (x**2 + 4*x + 6) * (x<0) + (x**2 - 2*x + 6) * (x>0)\n\nx = np.linspace(-4,6, 100)\nplt.plot(x, f(x));","4d15d033":"# Let's say, parameter x with default value 5\nx = torch.tensor(5.0, requires_grad=True)\n\noptimizer = torch.optim.SGD(params=[x], lr=0.1) # SGD = stochastic gradient descent\n\n# Will record x values here\nx_history = [x.data.numpy().copy()]\n\n# Let's make gradient descent steps\nfor i in range(30):\n    # Compute the function (forward pass)\n    loss = f(x)\n    # Compute gradients (backward pass of backpropagation)\n    optimizer.zero_grad()\n    loss.backward()\n    # Change x a bit\n    optimizer.step()\n    # Add to history\n    x_history.append(x.data.numpy().copy())\n\nx_history = np.array(x_history)\nprint('x =', x.data.numpy())","aac5a59d":"plt.figure(figsize=(14,10))\nx = np.linspace(-4,6, 100)\nplt.plot(x, f(x), label='f(x)')\nplt.grid()\nplt.scatter(x_history, f(x_history), color='orange', label='GD steps')\nplt.legend();","929c913a":"%%time\n# Weights of all neurons\nx0 = np.random.normal(size=(784, 10)) \/ np.sqrt(784)\nneurons = torch.tensor(x0, requires_grad=True)\n\n# We will need our dataset as torch.tensor\nX = torch.tensor(X_train)\ny = torch.tensor(y_train)\n\noptimizer = torch.optim.SGD(params=[neurons], lr=1) # SGD = stochastic gradient descent\n\nloss_history = []\n\n# Let's make gradient descent steps\nfor i in range(100):\n    # Compute the function (forward pass)\n    predictions = X @ neurons\n    # Compute loss\n    loss = torch.nn.functional.cross_entropy(predictions, y) # it already includes activation function\n    # Compute gradients (backward pass of backpropagation)\n    optimizer.zero_grad()\n    loss.backward()\n    # Change neurons a bit\n    optimizer.step()\n    loss_history.append(loss.data.numpy())","9451f7b0":"plt.title('Training history')\nplt.ylabel('Loss')\nplt.xlabel('step')\nplt.plot(loss_history);","6d5b2521":"fig, axes = plt.subplots(1, 10, figsize=(30,30))\n\nfor i in range(10):\n    axes[i].set_title(i)\n    neuron_img = neurons[:,i].data.numpy().reshape(28, 28)\n    axes[i].imshow(neuron_img)","20235723":"# The same code as before\n\ndigit_probabilities = sigmoid((X @ neurons).data.numpy())\nprint('digit_probabilities.shape:', digit_probabilities.shape)\n# Now we have probabilities for each image for each digit\n\n# Let's take maximum as prediction\npredictions = np.argmax(digit_probabilities, axis=1)\nprint('predictions.shape:', predictions.shape)\nprint('\\nTrain accuracy:', accuracy_score(y_train, predictions))","51f70e1f":"# Let's make test predictions\ndigit_probabilities = sigmoid(X_test @ neurons.data.numpy())\ntest_predictions = np.argmax(digit_probabilities, axis=1)\n\nsubmission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\nsubmission.Label = test_predictions\n\n# Save it as csv file\nsubmission.to_csv('submission_10n_torch.csv', index=False)\n\nsubmission.head()","4f955c36":"# Will use torch's high level API: nn\n# When we create a neural network in pytorch, we usually create a class which derives from torch.nn.Module\n# It is easy to train and further use network defined in such way\n# This network has 210 neurons\n\nclass MyFirstNN(nn.Module):\n    def __init__(self, n_hidden_neurons=200):\n        super().__init__()\n        \n        # Here we define the model's trainable paramerers\n        \n        # First layer weights\n        init_1 = np.random.normal(size=(784, n_hidden_neurons)) \/ np.sqrt(784) # google for \"xavier initialization\"\n        self.neurons_layer1 = torch.tensor(init_1, requires_grad=True) # weights for the first layer of neurons\n        self.neurons_layer1 = nn.Parameter(self.neurons_layer1)\n        \n        # Second layer weights\n        init_2 = np.random.normal(size=(n_hidden_neurons, 10)) \/ np.sqrt(n_hidden_neurons)\n        self.neurons_layer2 = torch.tensor(init_2, requires_grad=True)\n        self.neurons_layer2 = nn.Parameter(self.neurons_layer2)\n        \n    def forward(self, x):\n        # Here we do all the computations\n        \n        # First layer\n        h = x @ self.neurons_layer1\n        # Activation function of a hidden layer\n        h = torch.relu(h)\n        # Output layer\n        out = h @ self.neurons_layer2\n        return out\n\n# People use more high-level features, such as nn.Linear, to make the code shorter\n# This is a simple and explicit example","6f72f696":"model = MyFirstNN()\n\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)","40aaa192":"N_EPOCHS = 501\n\n# We will need our dataset as torch.tensor\nX_tr = torch.tensor(X_train[1000:])\ny_tr = torch.tensor(y_train[1000:])\nX_dev = torch.tensor(X_train[:1000])\ny_dev = torch.tensor(y_train[:1000])\n\ntrain_loss_history = []\ndev_loss_history = []\n\n# Let's make gradient descent steps\nfor i in tqdm(range(N_EPOCHS)):\n    # Compute the function (forward pass)\n    predictions = model(X_tr)\n    # Compute loss\n    loss = torch.nn.functional.cross_entropy(predictions, y_tr) # it already includes activation function\n    # Compute gradients (backward pass of backpropagation)\n    optimizer.zero_grad()\n    loss.backward()\n    # Change neurons a bit\n    optimizer.step()\n    train_loss_history.append(loss.data.numpy())\n    \n    # Evaluate the model\n    if i % 10 == 0:\n        predictions = model(X_dev)\n        loss = torch.nn.functional.cross_entropy(predictions, y_dev)\n        dev_loss_history.append(loss.data.numpy())","b1dbaa07":"plt.figure(figsize=(16, 8))\nplt.title('Training history')\nplt.ylabel('Loss')\nplt.xlabel('step')\nplt.plot(range(N_EPOCHS), train_loss_history, label='train loss')\nplt.plot(range(N_EPOCHS)[::10], dev_loss_history, label='dev loss')\nplt.legend();","41c7f750":"# Evaluate accuracy on dev set\n\n# Same code as before (but use dev set)\ndigit_probabilities = model(X_dev).data.numpy()\nprint('digit_probabilities.shape:', digit_probabilities.shape)\n# Now we have probabilities for each image for each digit\n\n# Let's take maximum as prediction\npredictions = np.argmax(digit_probabilities, axis=1)\nprint('predictions.shape:', predictions.shape)\nprint('\\Dev accuracy:', accuracy_score(y_train[:1000], predictions))","7b50a0a4":"# Let's make test predictions\ndigit_probabilities = model(torch.tensor(X_test)).data.numpy()\ntest_predictions = np.argmax(digit_probabilities, axis=1)\n\nsubmission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\nsubmission.Label = test_predictions\n\n# Save it as csv file\nsubmission.to_csv('submission_NN.csv', index=False)\n\nsubmission.head()","0ec49d00":"Why is there 784 pixels?? Because it's 28x28 picture, lol.  \nLet's print one of them:","c4fb7f44":"Now error is smaller! Let's measure accuracy and f1","8a2bd399":"Looks like numbers!  \n\nWe can now do classificatiion using these 10 neurons:","93a3f531":"## Import all modules","07b2f12a":"Accuracy was 67%, and now it is 90%. Again, because we used more data.","fed9a374":"But what we did and why???  \nWe made a neuron which takes a vector and outputs one number, and this number makes no sense.  \n\nWe need to train our neuron!  \nLet's give it a simpler task: **output 1 when image is digit 1, and output 0 otherwise**.  ","a995e0e9":"# What's next?\n## 1. Neural network?\n![](https:\/\/i.imgflip.com\/3gavrz.jpg)\nNo, it isn't.  \nIn neural network, neurons are connected into a... network.  \n\nA real network looks like this:  \n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/9\/99\/Neural_network_example.svg\/360px-Neural_network_example.svg.png)\nEach circle on this image is a neuron, and each arrow is a weighted connection.  ","972d1ba0":"Let's find minimum of this function using gradient descent","12483551":"Here we see weights of each pixel. Positive means that pixel has positive impact on predicting \"1\".  \nThis looks a bit like a shadow of digit \"1\", doesn't it?  \n\n## Now let's train 10 neurons!\nOne for each digit","adb5c208":"## 2. Faster training?\nYes!  \nNow it's time to get familiar with PyTorch:\n![](https:\/\/github.com\/pytorch\/pytorch\/raw\/master\/docs\/source\/_static\/img\/pytorch-logo-dark.png)","59af71df":"*So, just like numpy, right?*\n\nYes, this is just like numpy, but instead of `np.array` we use `torch.tensor`.  \nThe main difference is that pytorch will save gradients everytime you make any operation.  \n\nAnd now gradient descent example:","16c5a836":"By iteratively changing x, we found a local minimum. As you see, we didn't explicitly write the derivative of $f(x)$. But pytorch still knows which direction to go.\n\nThis will help us in training neural networks. So let's train our 10 neurons!","96b92a05":"let's look at our neurons","0e4c2e0e":"Now we have `X_train`, which is an array of 42000 vectors (images) with 784 values (pixels) each  \nAlso `y_train`, which is an array of 42000 correct labels  \nAnd `X_test` which is an array of images for which we don't know the target","580db0fb":"Now we want to **train** our neuron. This means we want to make predictions closer to the true values.\n\nFor this we will use [Cross Entropy](https:\/\/en.wikipedia.org\/wiki\/Cross_entropy) - function which shows us how close two distributions are.","2c431da7":"Looks like \"one\", I guess...  \nAnd it is! If you look at the label of the first line, it is 1\n\nBut where did the numbers go?  \nHere they are:","3b64d978":"Ok, now you know the rules: 0 is black, 1 is white, and 0.5 is gray\n\nLet's prepare training set in the numpy format","f56df000":"How to make predictions for the whole dataset? Let's use matrix multiplication again","0851a841":"# Read the data","62735740":"# Now what's next?\nReal neural networks!  \nActually, what I showed you is called Logistic Regression, which is not NN.  \nBut pytorch can do much more!  \n\nI will show you just a little example how to make a NN using pytorch:","d441b46b":"It took 12 seconds and we used the whole dataset!","80489844":"Looks like numbers, but a little different than before, because here we used much much more data","413cc0eb":"# But What is a Neuron? Or not-really-deep Learning\n*This is a completely beginner tutorial. You will need to know what is numpy and, of course, python.*\n\nTo better understand what is **a neural network**, it is good to know first what is **a neuron**.  \nIn this tutorial we'll try to make a **neural network made of only one neuron!**  \nWe'll also learn about **PyTorch** - a library which makes it easy to build neural networks.  \n\n*One-neuron networks is not smth that is used in real life.  \nThis is made only to explain the basic concepts of neural networks.*","219e07e7":"*Ok, A is 10, what else?*","6c15d66c":"Nice results!  \nBut what is this neuron, let's see","cc47f56b":"# Now neurons!!\nWe could train logistic regression, or decision tree, or even xgboost... But this is boring!  \n\nWe could train neural network, but this is difficult.........  \n\nLet's train just one neuron \u00af\\(\u30c4)\/\u00af\n\n### 1. Biological neuron\nSo, what is a neuron:\n![neuron](https:\/\/scx1.b-cdn.net\/csz\/news\/800\/2018\/2-whyareneuron.jpg)\nThis is a neural cell of an animal brain. It's basic function is **to transmit signal**.  \n\nNeuron has:  \n1) **inputs** (called dendrites). They can recieve signal from other neurons  \n2) **output** (called axon). Neuron usually has one output. It transmits signal to other neurons\n3) Body.\n\nBut what is the logic of neuron?  \nWell... this is a very simple explanation, but it will work for now:  \n* Neuron has it's own **electric charge**\n* When a signal is recieved, the charge changes by some value\n* This value depends on which dendrite recieved the signal\n* Some dendrites will upper the charge *(excitatory dendrite)*, and some will lower it *(inhibitory dendrite)*\n* When the charge it big enough, **discharge** happens: neuron transmits signal to other neurons through its axon (output)\n\n### 2. Mathematical neuron\nNow we want artificial neuron:\n![](https:\/\/cdn-images-1.medium.com\/freeze\/max\/1000\/1*tMuOsWWRX3fR84xoSeJcAw.png?q=20)\nThe idea is the same. The neuron has:  \n* Multiple inputs $x_1, x_2, ..., x_{784}$. This will be our **pixel values**!\n* Weights $w_1, w_2, ..., w_{784}$ for each of the input. Positive weights correspond to *excitatory dendrites* and negative weights - to *inhibitory dendrites*\n* Sum operation: $S = x_1w_1 + x_2w_2 + ... + x_Nw_N$. Or $S=\\vec{x} \\cdot \\vec{w}$.\n* Activation function. It will map $S$ to a number from 0 to 1: $\\sigma (S) \\in [0,1]$. This is called sigmoid:\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/88\/Logistic-curve.svg\/400px-Logistic-curve.svg.png)\n$$\\sigma(x) = \\frac{e^x}{1 + e^x}$$","6aaffcc2":"Let's recap what we did:  \n* Defined loss function: cross-entropy\n* Wanted it to be as small as possible\n* Used `scipy.optimize.minimize` to find the best neurons\n* It took 25 seconds, and we used only 100 training examples!  \n\nWhat if we want to use all 42000 examples? According to my calculations it's gonna take approximately forever.\n\n# Gradient descent\n**We need a faster method**. And it exists. And it is called [Gradient Descent](https:\/\/en.wikipedia.org\/wiki\/Gradient_descent).  \nBut to use gradient descent, **we need to know gradients**, e.g. the derivatives of the loss function with respect to the neuron parameters (weights).\n\nNow, we could try to write some formulas and compute those derivatives.  \nBut, luckily, there is a library which will do it for us.","359112e7":"So this is cross entropy. We want it to be smaller.  \n\nLet's create a neuron which minimizes the cross entropy!"}}