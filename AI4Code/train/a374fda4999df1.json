{"cell_type":{"32edf674":"code","341f4391":"code","f2dae278":"code","9b1c9738":"code","51ae7909":"code","cb36309f":"code","a225795f":"code","1e4be929":"code","94964c13":"code","a3e4fb55":"code","02c10e87":"code","43c7e5ba":"code","6b83a98b":"code","8036d33c":"code","9a5a7c71":"code","5239cca6":"code","c5866c3a":"code","125b9f4b":"code","ba32072f":"code","d63cc234":"code","6c772bd9":"code","70ed1d93":"code","7e82f078":"code","750524af":"code","c3283713":"code","972eb776":"code","de47089e":"code","401503b0":"code","92990533":"code","40815d11":"code","094828f9":"code","d0ecba46":"code","3a37a624":"code","4fb2ad36":"code","2963f005":"code","2af7457e":"code","fe7cac5b":"code","ad6d3d90":"code","5d158d3f":"code","d9d25a5d":"code","c5fadb66":"code","7252b0b3":"code","add04e7d":"code","678fdd82":"code","75dd9a7d":"code","d7333882":"code","6ae4757a":"code","fb6222d7":"code","ec62f71c":"code","808f6833":"code","1041a09e":"code","30b6731f":"code","33f1454c":"code","8cc2f78c":"markdown","67ea40fc":"markdown","3b50a4b3":"markdown","42e0f9f8":"markdown","53eeaf0e":"markdown","2a59f2f6":"markdown","9aa2e105":"markdown","8d21649c":"markdown","8bb2b0eb":"markdown","81359e39":"markdown","f98e4454":"markdown","43724d7b":"markdown","7c9c00fa":"markdown","218327dd":"markdown","86ef8207":"markdown","5244ca75":"markdown","330fe437":"markdown","3a32501e":"markdown","668e3bcb":"markdown","0c1addd9":"markdown","08fb9f1e":"markdown","5cf1872e":"markdown","27033c58":"markdown","58218408":"markdown","16eaea22":"markdown","4c248414":"markdown","d071af35":"markdown","bbc0a76f":"markdown","89706ff9":"markdown","f590d5f7":"markdown","2487bfab":"markdown","49b8c9f1":"markdown","7b75f6be":"markdown","144760e2":"markdown","d6c2d445":"markdown","1d66d4cc":"markdown","9a3136e7":"markdown","fea3ecca":"markdown","88c58ccb":"markdown","ba6379fe":"markdown","dafe0c6e":"markdown","b099bc3b":"markdown","10790b94":"markdown","d2c9ec41":"markdown","1e6a3edd":"markdown","9cc9527b":"markdown","2c7d28ea":"markdown","12c88d36":"markdown","6e6ad777":"markdown","fd4f763d":"markdown","3862567e":"markdown"},"source":{"32edf674":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport collections\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit, GridSearchCV, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.metrics import make_scorer, precision_score, recall_score, classification_report, confusion_matrix\nfrom collections import Counter\nfrom sklearn.preprocessing import RobustScaler\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndata = pd.read_csv('..\/input\/creditcard.csv',sep=',')\ndata.head()","341f4391":"print(data.columns)","f2dae278":"data.shape","9b1c9738":"data.info()","51ae7909":"#Lets start looking the difference by Normal and Fraud transactions\nprint(\"Distribuition of Normal(0) and Frauds(1): \")\nprint(data[\"Class\"].value_counts())\nprint('')\n\n# The classes are heavily skewed we need to solve this issue later.\nprint('Non-Frauds', round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Frauds', round(data['Class'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')\n\nplt.figure(figsize=(7,5))\nsns.countplot(data['Class'])\nplt.title(\"Class Count\", fontsize=18)\nplt.xlabel(\"Is fraud?\", fontsize=15)\nplt.ylabel(\"Count\", fontsize=15)\nplt.show()","cb36309f":"plt.figure(figsize=(16,4))\ndata.iloc[:,:-1].boxplot()\nplt.title('(Raw) Distribution of Features', fontsize=17)\nplt.show()\n\nplt.figure(figsize=(16,4))\nnp.log(data.iloc[:,:-1]).boxplot()\nplt.title('(Log) Distribution of Features', fontsize=17)\nplt.show()","a225795f":"#Now look at Fraud Amounts\nplt.figure(figsize=(16,5))\nsns.boxplot(x=data.Amount[data.Class == 1])\nplt.title('Distribution of (Fraud) Amounts',fontsize=17)\nplt.show()\n#Now look at Non-Fraud Amounts\nplt.figure(figsize=(16,5))\nsns.boxplot(x=data.Amount[data.Class == 0])\nplt.title('Distribution of (Non-Fraud) Amounts',fontsize=17)\nplt.show()","1e4be929":"print('Top 85% of transaction amounts:', round(data.Amount.quantile(.85),2))\nprint('Top 1% of transaction amounts:', round(data.Amount.quantile(.99),2))\nprint('Largest transaction amount:', round(data.Amount.quantile(1),2))\nprint('80% of Frauds are less than:', round(data.Amount[data.Class==1].quantile(.80),2))","94964c13":"#First look at Time\nplt.figure(figsize=(11,6))\nsns.distplot(data.Time,kde=False)\nplt.title('Distribution of Time', fontsize=17)\nplt.show()","a3e4fb55":"# Create a EDA dataframe for the time units and visualizations\neda = pd.DataFrame(data.copy())\n\n# Tell timedelta to interpret the Time as second units\ntimedelta = pd.to_timedelta(eda['Time'], unit='s')\n\n# Create a hours feature from timedelta\neda['Time_hour'] = (timedelta.dt.components.hours).astype(int)","02c10e87":"#Exploring the distribuition by Class types through seconds\nplt.figure(figsize=(12,5))\nsns.distplot(eda[eda['Class'] == 0][\"Time\"], \n             color='g')\nsns.distplot(eda[eda['Class'] == 1][\"Time\"], \n             color='r')\nplt.title('(Density Histogram) Fraud VS Normal Transactions by Second', fontsize=17)\nplt.xlim([-2000,175000])\nplt.show()","43c7e5ba":"#Exploring the distribuition by Class types through hours\nplt.figure(figsize=(12,5))\nsns.distplot(eda[eda['Class'] == 0][\"Time_hour\"], \n             color='g')\nsns.distplot(eda[eda['Class'] == 1][\"Time_hour\"], \n             color='r')\nplt.title('(Density Histogram) Fraud VS Normal Transactions by Hour', fontsize=17)\nplt.xlim([-1,25])\nplt.show()","6b83a98b":"# Define outcome and predictors to split into train and test groups\ny = data['Class']\nX = data.drop('Class', 1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\n# Class balance in test group\nprint(\"TEST GROUP\")\nprint('Size:',y_test.count())\nprint(\"Frauds percentage:\",\n      y_test.value_counts()[1]\/y_test.count())\nprint(\"Nonfrauds percentage:\",\n      y_test.value_counts()[0]\/y_test.count())\n\n# Class balance in train group\nprint(\"\\nTRAIN GROUP\")\nprint('Size:',y_train.count())\nprint(\"Frauds percentage:\",\n      y_train.value_counts()[1]\/y_train.count())\nprint(\"Nonfrauds percentage:\",\n      y_train.value_counts()[0]\/y_train.count())","8036d33c":"# Invoke classifier\nclf = LogisticRegression()\n\n# Cross-validate on the train data\ntrain_cv = cross_val_score(X=X_train,y=y_train,estimator=clf,cv=3)\nprint(\"TRAIN GROUP\")\nprint(\"\\nCross-validation accuracy scores:\",train_cv)\nprint(\"Mean score:\",train_cv.mean())\n\n# Now predict on the test group\nprint(\"\\nTEST GROUP\")\ny_pred = clf.fit(X_train, y_train).predict(X_test)\nprint(\"\\nAccuracy score:\",clf.score(X_test,y_test))\n\n# Classification report\nprint('\\nClassification report:\\n')\nprint(classification_report(y_test, y_pred))\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\nsns.heatmap(conf_matrix, annot=True,fmt='d', cmap=plt.cm.copper)\nplt.show()\n","9a5a7c71":"features = pd.DataFrame()","5239cca6":"plt.figure(figsize=(12,6))\n\n# Visualize where Time is less than 100,000\nplt.subplot(1,2,1)\nplt.title(\"Time < 100,000\")\ndata[data['Time']<100000]['Time'].hist()\n\n# Visualize where Time is more than 100,000\nplt.subplot(1,2,2)\nplt.title(\"Time >= 100,000\")\ndata[data['Time']>=100000]['Time'].hist()\n\nplt.tight_layout()\nplt.show()","c5866c3a":"# Create a feature from normal distributions above\nfeatures['100k_time'] = np.where(data.Time<100000, 1,0)","125b9f4b":"plt.figure(figsize=(12,6))\n\nplt.subplot(1,2,1)\nplt.title(\"Non-Frauds, Hour <= 4\")\neda.Time_hour[(eda.Class == 0) & (eda.Time_hour <= 4)].plot(kind='hist',bins=15)\n\nplt.subplot(1,2,2)\nplt.title(\"Non-Frauds, Hour > 4\")\neda.Time_hour[(eda.Class == 0) & (eda.Time_hour > 4)].plot(kind='hist',bins=15)\n\nplt.tight_layout()\nplt.show()","ba32072f":"# Create a feature from distributions above\nfeatures['4_hour'] = np.where((eda.Class == 0) & (eda.Time_hour > 4), 1,0)","d63cc234":"# how many frauds are actually 0 dollars?\nprint(\"Non-Fraud Zero dollar Transactions:\")\ndisplay(data[(data.Amount == 0) & (data.Class == 0)]['Class'].count())\nprint(\"Fraudulent Zero dollar Transactions:\")\ndisplay(data[(data.Amount == 0) & (data.Class == 1)]['Class'].count())","6c772bd9":"# Capture where transactions have a $0 amount\nfeatures['amount0'] = np.where(data.Amount == 0,1,0)","70ed1d93":"rob_scaler = RobustScaler()\n\nfeatures['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\nfeatures['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))","7e82f078":"# Add the PCA components to our features DataFrame.\nfeatures = features.join(data.iloc[:,1:-1].drop('Amount',axis=1))\n\n# Add 'Class' to our features DataFrame.\nfeatures = features.join(data.Class)\n\n# Nice! These are the final features I'll settle for.\nfeatures.head()","750524af":"# Define outcome and predictors USE FEATURE-ENGINEERED DATA\ny = features['Class']\nX = features.drop('Class', 1)\n\n# Split X and y into train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42)\n\n# Class balance in test group\nprint(\"TEST GROUP\")\nprint('Size:',y_test.count())\nprint(\"Frauds percentage:\",\n      y_test.value_counts()[1]\/y_test.count())\nprint(\"Nonfrauds percentage:\",\n      y_test.value_counts()[0]\/y_test.count())\n\n# Class balance in train group\nprint(\"\\nTRAIN GROUP\")\nprint('Size:',y_train.count())\nprint(\"Frauds percentage:\",\n      y_train.value_counts()[1]\/y_train.count())\nprint(\"Nonfrauds percentage:\",\n      y_train.value_counts()[0]\/y_train.count())","c3283713":"# Invoke classifier\nclf = LogisticRegression()\n\n# Make a scoring callable from recall_score\nrecall = make_scorer(recall_score)\n\n# Cross-validate on the train data\ntrain_cv = cross_val_score(X=X_train,y=y_train,estimator=clf,scoring=recall,cv=3)\nprint(\"TRAIN GROUP\")\nprint(\"\\nCross-validation recall scores:\",train_cv)\nprint(\"Mean recall score:\",train_cv.mean())\n\n# Now predict on the test group\nprint(\"\\nTEST GROUP\")\ny_pred = clf.fit(X_train, y_train).predict(X_test)\nprint(\"\\nRecall:\",recall_score(y_test,y_pred))\n\n# Classification report\nprint('\\nClassification report:\\n')\nprint(classification_report(y_test, y_pred))\n\n# Confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\nsns.heatmap(conf_matrix, annot=True,fmt='d', cmap=plt.cm.copper)\nplt.show()","972eb776":"# Balancing Classes before checking for correlation\n\n# Join the train data\ntrain = X_train.join(y_train)\n\nprint('Data shape before balancing:',train.shape)\nprint('\\nCounts of frauds VS non-frauds in previous data:')\nprint(train.Class.value_counts())\nprint('-'*40)\n\n# Oversample frauds. Imblearn's ADASYN was built for class-imbalanced datasets\nX_bal, y_bal = ADASYN(sampling_strategy='minority',random_state=0).fit_resample(\n    X_train,\n    y_train)\n\n# Join X and y\nX_bal = pd.DataFrame(X_bal,columns=X_train.columns)\ny_bal = pd.DataFrame(y_bal,columns=['Class'])\nbalanced = X_bal.join(y_bal)\n\n\nprint('-'*40)\nprint('Data shape after balancing:',balanced.shape)\nprint('\\nCounts of frauds VS non-frauds in new data:')\nprint(balanced.Class.value_counts())","de47089e":"print('Distribution of the Classes in the subsample dataset')\nprint(balanced.Class.value_counts()\/len(train))\n\nsns.countplot('Class', data=balanced)\nplt.title('Class Distribution', fontsize=14)\nplt.show()\n","401503b0":"# Compare correlation of raw train data VS balanced train data\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Imbalanced DataFrame\ncorr = train.corr()\nsns.heatmap(corr, annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (Biased)\", fontsize=14)\n\n# Balanced DataFrame\nbal_corr = balanced.corr()\nsns.heatmap(bal_corr, annot_kws={'size':20}, ax=ax2)\nax2.set_title('Balanced Correlation Matrix', fontsize=14)\nplt.show()","92990533":"# Each feature's correlation with Class\nbal_corr.Class","40815d11":"no_outliers=pd.DataFrame(balanced.copy())","094828f9":"# Removing Outliers from high-correlation features\n\ncols = bal_corr.Class.index[:-1]\n\n# For each feature correlated with Class...\nfor col in cols:\n    # If absolute correlation value is more than X percent...\n    correlation = bal_corr.loc['Class',col]\n    if np.absolute(correlation) > 0.1:\n        \n        # Separate the classes of the high-correlation column\n        nonfrauds = no_outliers.loc[no_outliers.Class==0,col]\n        frauds = no_outliers.loc[no_outliers.Class==1,col]\n\n        # Identify the 25th and 75th quartiles\n        all_values = no_outliers.loc[:,col]\n        q25, q75 = np.percentile(all_values, 25), np.percentile(all_values, 75)\n        # Get the inter quartile range\n        iqr = q75 - q25\n        # Smaller cutoffs will remove more outliers\n        cutoff = iqr * 7\n        # Set the bounds of the desired portion to keep\n        lower, upper = q25 - cutoff, q75 + cutoff\n        \n        # If positively correlated...\n        # Drop nonfrauds above upper bound, and frauds below lower bound\n        if correlation > 0: \n            no_outliers.drop(index=nonfrauds[nonfrauds>upper].index,inplace=True)\n            no_outliers.drop(index=frauds[frauds<lower].index,inplace=True)\n        \n        # If negatively correlated...\n        # Drop nonfrauds below lower bound, and frauds above upper bound\n        elif correlation < 0: \n            no_outliers.drop(index=nonfrauds[nonfrauds<lower].index,inplace=True)\n            no_outliers.drop(index=frauds[frauds>upper].index,inplace=True)\n        \nprint('\\nData shape before removing outliers:', balanced.shape)\nprint('\\nCounts of frauds VS non-frauds in previous data:')\nprint(balanced.Class.value_counts())\nprint('-'*40)\nprint('-'*40)\nprint('\\nData shape after removing outliers:', no_outliers.shape)\nprint('\\nCounts of frauds VS non-frauds in new data:')\nprint(no_outliers.Class.value_counts())","d0ecba46":"no_outliers.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\nplt.title('Distributions with Less Outliers', fontsize=17)\nplt.show()","3a37a624":"feat_sel =pd.DataFrame(no_outliers.copy())","4fb2ad36":"# Make a dataframe with the class-correlations before removing outliers\ncorr_change = pd.DataFrame()\ncorr_change['correlation']= bal_corr.Class\ncorr_change['origin']= 'w\/outliers'\n\n# Make a dataframe with class-correlations after removing outliers \ncorr_other = pd.DataFrame()\ncorr_other['correlation']= feat_sel.corr().Class\ncorr_other['origin']= 'no_outliers'\n\n# Join them\ncorr_change = corr_change.append(corr_other)\n\nplt.figure(figsize=(14,6))\nplt.xticks(rotation=90)\n\n# Plot them\nsns.set_style('darkgrid')\nplt.title('Class Correlation per Feature. With VS W\/out Outliers', fontsize=17)\nsns.barplot(data=corr_change,x=corr_change.index,y='correlation',hue='origin')\nplt.show()","2963f005":"# Feature Selection based on correlation with Class\n\nprint('\\nData shape before feature selection:', feat_sel.shape)\nprint('\\nCounts of frauds VS non-frauds before feature selection:')\nprint(feat_sel.Class.value_counts())\nprint('-'*40)\n\n# Correlation matrix after removing outliers\nnew_corr = feat_sel.corr()\n\nfor col in new_corr.Class.index[:-1]:\n    # Pick desired cutoff for dropping features. In absolute-value terms.\n    if np.absolute(new_corr.loc['Class',col]) < 0.1:\n        # Drop the feature if correlation is below cutoff\n        feat_sel.drop(columns=col,inplace=True)\n\nprint('-'*40)\nprint('\\nData shape after feature selection:', feat_sel.shape)\nprint('\\nCounts of frauds VS non-frauds in new data:')\nprint(feat_sel.Class.value_counts())","2af7457e":"feat_sel.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\nplt.title('Distribution of Features Selected', fontsize=17)\nplt.show()","fe7cac5b":"# Undersample model for efficiency and balance classes.\n\nX_train = feat_sel.drop('Class',1)\ny_train = feat_sel.Class\n\n# After feature-selection, X_test needs to include only the same features as X_train\ncols = X_train.columns\nX_test = X_test[cols]\n\n# Undersample and balance classes\nX_train, y_train = RandomUnderSampler(sampling_strategy={1:5000,0:5000}).fit_resample(X_train,y_train)\n\nprint('\\nX_train shape after reduction:', X_train.shape)\nprint('\\nCounts of frauds VS non-frauds in y_train:')\nprint(np.unique(y_train, return_counts=True))","ad6d3d90":"# DataFrame to store classifier performance\nperformance = pd.DataFrame(columns=['Train_Recall','Test_Recall','Test_Specificity'])","5d158d3f":"# Load simple classifiers\nclassifiers = [SVC(max_iter=1000),LogisticRegression(),\n               DecisionTreeClassifier(),KNeighborsClassifier()]\n\n# Get a classification report from each algorithm\nfor clf in classifiers:    \n    \n    # Heading\n    print('\\n','-'*40,'\\n',clf.__class__.__name__,'\\n','-'*40)\n    \n    # Cross-validate on the train data\n    print(\"TRAIN GROUP\")\n    train_cv = cross_val_score(X=X_train, y=y_train, \n                               estimator=clf, scoring=recall,cv=3)\n    print(\"\\nCross-validation recall scores:\",train_cv)\n    print(\"Mean recall score:\",train_cv.mean())\n\n    # Now predict on the test group\n    print(\"\\nTEST GROUP\")\n    y_pred = clf.fit(X_train, y_train).predict(X_test)\n    print(\"\\nRecall:\",recall_score(y_test,y_pred))\n    \n    # Print confusion matrix\n    conf_matrix = confusion_matrix(y_test,y_pred)\n    sns.heatmap(conf_matrix, annot=True,fmt='d', cmap=plt.cm.copper)\n    plt.show()\n    \n    # Store results\n    performance.loc[clf.__class__.__name__+'_default',\n                    ['Train_Recall','Test_Recall','Test_Specificity']] = [\n        train_cv.mean(),\n        recall_score(y_test,y_pred),\n        conf_matrix[0,0]\/conf_matrix[0,:].sum()\n    ]","d9d25a5d":"# Scores obtained\nperformance","c5fadb66":"# Parameters to optimize\nparams = [{\n    'solver': ['newton-cg', 'lbfgs', 'sag'],\n    'C': [0.3, 0.5, 0.7, 1],\n    'penalty': ['l2']\n    },{\n    'solver': ['liblinear','saga'],\n    'C': [0.3, 0.5, 0.7, 1],\n    'penalty': ['l1','l2']\n}]\n\nclf = LogisticRegression(\n    n_jobs=-1, # Use all CPU\n    class_weight={0:0.1,1:1} # Prioritize frauds\n)\n\n# Load GridSearchCV\nsearch = GridSearchCV(\n    estimator=clf,\n    param_grid=params,\n    n_jobs=-1,\n    scoring=recall\n)\n\n# Train search object\nsearch.fit(X_train, y_train)\n\n# Heading\nprint('\\n','-'*40,'\\n',clf.__class__.__name__,'\\n','-'*40)\n\n# Extract best estimator\nbest = search.best_estimator_\nprint('Best parameters: \\n\\n',search.best_params_,'\\n')\n\n# Cross-validate on the train data\nprint(\"TRAIN GROUP\")\ntrain_cv = cross_val_score(X=X_train, y=y_train, \n                           estimator=best, scoring=recall,cv=3)\nprint(\"\\nCross-validation recall scores:\",train_cv)\nprint(\"Mean recall score:\",train_cv.mean())\n\n# Now predict on the test group\nprint(\"\\nTEST GROUP\")\ny_pred = best.fit(X_train, y_train).predict(X_test)\nprint(\"\\nRecall:\",recall_score(y_test,y_pred))\n\n# Get classification report\nprint(classification_report(y_test, y_pred))\n\n# Print confusion matrix\nconf_matrix = confusion_matrix(y_test,y_pred)\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap=plt.cm.copper)\nplt.show()\n    \n# Store results\nperformance.loc[clf.__class__.__name__+'_search',\n                ['Train_Recall','Test_Recall','Test_Specificity']] = [\n    train_cv.mean(),\n    recall_score(y_test,y_pred),\n    conf_matrix[0,0]\/conf_matrix[0,:].sum()\n]","7252b0b3":"performance","add04e7d":"pd.DataFrame(search.cv_results_).iloc[:,4:].sort_values(by='rank_test_score').head()","678fdd82":"# Make a scoring function that improves specificity while identifying all frauds\ndef recall_optim(y_true, y_pred):\n    \n    conf_matrix = confusion_matrix(y_true, y_pred)\n    \n    # Recall will be worth a greater value than specificity\n    rec = recall_score(y_true, y_pred) * 0.8 \n    spe = conf_matrix[0,0]\/conf_matrix[0,:].sum() * 0.2 \n    \n    # Imperfect recalls will lose a penalty\n    # This means the best results will have perfect recalls and compete for specificity\n    if rec < 0.8:\n        rec -= 0.2\n    return rec + spe \n    \n# Create a scoring callable based on the scoring function\noptimize = make_scorer(recall_optim)","75dd9a7d":"scores = []\nfor rec, spe in performance[['Test_Recall','Test_Specificity']].values:\n    rec = rec * 0.8\n    spe = spe * 0.2\n    if rec < 0.8:\n        rec -= 0.20\n    scores.append(rec + spe)\nperformance['Optimize'] = scores\ndisplay(performance)","d7333882":"def score_optimization(params,clf):\n    # Load GridSearchCV\n    search = GridSearchCV(\n        estimator=clf,\n        param_grid=params,\n        n_jobs=-1,\n        scoring=optimize\n    )\n\n    # Train search object\n    search.fit(X_train, y_train)\n\n    # Heading\n    print('\\n','-'*40,'\\n',clf.__class__.__name__,'\\n','-'*40)\n\n    # Extract best estimator\n    best = search.best_estimator_\n    print('Best parameters: \\n\\n',search.best_params_,'\\n')\n\n    # Cross-validate on the train data\n    print(\"TRAIN GROUP\")\n    train_cv = cross_val_score(X=X_train, y=y_train, \n                               estimator=best, scoring=recall,cv=3)\n    print(\"\\nCross-validation recall scores:\",train_cv)\n    print(\"Mean recall score:\",train_cv.mean())\n\n    # Now predict on the test group\n    print(\"\\nTEST GROUP\")\n    y_pred = best.fit(X_train, y_train).predict(X_test)\n    print(\"\\nRecall:\",recall_score(y_test,y_pred))\n\n    # Get classification report\n    print(classification_report(y_test, y_pred))\n\n    # Print confusion matrix\n    conf_matrix = confusion_matrix(y_test,y_pred)\n    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=plt.cm.copper)\n    plt.show()\n\n    # Store results\n    performance.loc[clf.__class__.__name__+'_optimize',:] = [\n        train_cv.mean(),\n        recall_score(y_test,y_pred),\n        conf_matrix[0,0]\/conf_matrix[0,:].sum(),\n        recall_optim(y_test,y_pred)\n    ]\n    # Look at the parameters for the top best scores\n    display(pd.DataFrame(search.cv_results_).iloc[:,4:].sort_values(by='rank_test_score').head())\n    display(performance)","6ae4757a":"# Parameters to optimize\nparams = [{\n    'solver': ['newton-cg', 'lbfgs', 'sag'],\n    'C': [0.3, 0.5, 0.7, 1],\n    'penalty': ['l2'],\n    'class_weight':[{1:1,0:0.3},{1:1,0:0.5},{1:1,0:0.7}]\n    },{\n    'solver': ['liblinear','saga'],\n    'C': [0.3, 0.5, 0.7, 1],\n    'penalty': ['l1','l2'],\n    'class_weight':[{1:1,0:0.3},{1:1,0:0.5},{1:1,0:0.7}]\n}]\n\nclf = LogisticRegression(\n    n_jobs=-1 # Use all CPU\n)\n\nscore_optimization(clf=clf,params=params)","fb6222d7":"# Parameters to optimize\nparams = {\n    'criterion':['gini','entropy'],\n    'max_features':[None,'sqrt'],\n    'class_weight':[{1:1,0:0.3},{1:1,0:0.5},{1:1,0:0.7}]\n    }\n\nclf = DecisionTreeClassifier(\n)\n\nscore_optimization(clf=clf,params=params)","ec62f71c":"# Parameters to optimize\nparams = {\n    'kernel':['rbf','linear'],\n    'C': [0.3,0.5,0.7,1],\n    'gamma':['auto','scale'],\n    'class_weight':[{1:1,0:0.3},{1:1,0:0.5},{1:1,0:0.7}]\n    }\n\n# Load classifier\nclf = SVC(\n    cache_size=3000,\n    max_iter=1000, # Limit processing time\n)\nscore_optimization(clf=clf,params=params)","808f6833":"# Parameters to compare\nparams = {\n    \"n_neighbors\": list(range(2,6,1)), \n    'leaf_size': list(range(20,41,10)),\n    'algorithm': ['ball_tree','auto'],\n    'p': [1,2] # Regularization parameter. Equivalent to 'l1' or 'l2'\n}\n\n# Load classifier\nclf = KNeighborsClassifier(\n    n_jobs=-1\n)\nscore_optimization(clf=clf,params=params)","1041a09e":"# Parameters to compare\nparams = {\n    'class_weight':[{1:1,0:0.3},{1:1,0:0.4},{1:1,0:0.5},{1:1,0:0.6},{1:1,0:7}],\n    'sampling_strategy':['all','not majority','not minority']\n}\n\n# Implement the classifier\nclf = BalancedRandomForestClassifier(\n    criterion='entropy',\n    max_features=None,\n    n_jobs=-1\n)\nscore_optimization(clf=clf,params=params)","30b6731f":"# Parameters to compare\nparams = {\n    'criterion':['entropy','gini'],\n    'class_weight':[{1:1,0:0.3},{1:1,0:0.4},{1:1,0:0.5},{1:1,0:0.6},{1:1,0:7}]\n}\n\n# Implement the classifier\nclf = RandomForestClassifier(\n    n_estimators=100,\n    max_features=None,\n    n_jobs=-1,\n)\n\nscore_optimization(clf=clf,params=params)","33f1454c":"# Let's get the mean between test recall and test specificity\nperformance['Mean_RecSpe'] = (performance.Test_Recall+performance.Test_Specificity)\/2\nperformance","8cc2f78c":"- So this removed a few features from our 'processed' dataset. Aside from its large size, it should be ready for predictions.","67ea40fc":"### Feature: Time_hour > 4\n- Feature for non-frauds, where 'Time_hour' is above 4. This seems to have a clear differentiation.","3b50a4b3":"### SKlearn' RandomForestClassifier- Optimized\n- This is the good ol\u2019 RandomForestClassifier from Sklearn. It\u2019s a less specialized implementation. We\u2019ll see how it stacks against Imblearn\u2019s implementation.","42e0f9f8":"### Balancing Classes\n**There's several methods for balancing classes:** Im mostly interested in these...\n\n---\n- Random-Undersampling of Majority Class.\n\nYou reduce the size of majority class to match size of minority class. Disadvantage is that you may end up with very little data.\n    \n---\n- SMOTE- Synthetic Minority Oversampling Technique.\n\nAlgorithm that creates a larger sample of minority class to match the size of majority class.\n \n---\n- Inverting Class Ratios. (Turning minority into majority)\n\nIf you turn the minority into the majority, you may skew results towards better recall scores(detecting frauds correctly), as opposed to better specificity scores.(detecting non-frauds correctly)\n\n---\n\n**For now, I'll balance with a variant implementation of SMOTE, to see correlations.**","53eeaf0e":"## Test and Compare Classifiers\n**Approach:**\n\n- I'll evaluate improvements based on **fraud recall**, since its crucial to prevent frauds. This might come at the expense of more false-alarms, which would decrease the overall accuracy. **The main purpose of this project will be to identify all frauds, while minimizing false-positives.**\n\n- I'll define outcomes and predictors, reduce model size, and classify.","2a59f2f6":"## Feature Engineering\n[Back to Outline](#Outline)\n\n**Before fixing the class imbalance, there are other things that need to be addressed:**\n- Classification algorithms expect to receive normalized features. There are two features in the data that aren't normalized. ('Time' and 'Amount')\n- New features could be created from those unprocessed features, if they capture a pattern correlated to 'Class'.\n\n**'Features' DataFrame**\n- In this dataframe I'll store the features intended for predictive modeling of frauds.\n- 'data' will be left as the raw dataset.","9aa2e105":"### Support Vector Classifier- Optimized\n\n- The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC\n\n- C is 1 by default and it\u2019s a reasonable default choice. If you have a lot of noisy observations you should decrease it. It corresponds to regularize more the estimation. https:\/\/scikit-learn.org\/stable\/modules\/svm.html#tips-on-practical-use\n","8d21649c":"- Now add the optimized scores to the existing performance DataFrame","8bb2b0eb":"- Outliers from high-correlation features are now gone. However, this created a class-imbalance again. \n- I will balance the classes later when I reduce the model size. Reduction is important because classifiers may lag on high-dimensional datasets. ","81359e39":"### Removing High-Correlation Outliers\n- This step must be taken after balancing classes. Otherwise, correlations will echo class-distributions. To illustrate, I'll include two versions of the correlation matrix.\n- Based on a correlation matrix, we'll identify features with high correlations, and remove any transactions with outlying values in these.\n- High correlation features have a high capacity to influence the algorith prediction. Therefore it's important to control their anomalies.\n- This approach will reduce prediction bias because our algorithm will learn from more normally-distributed features. ","f98e4454":"- All the `None` parameters performed better.\n- By looking at the top split_scores, several are less than `0.8`, which means not-perfect recalls. No wonder it didn't nail all the frauds.\n- DecisionTreeClassifier seems to be better at predicting non-frauds than others, but consistently misses a few frauds.\n- Between default and optimize scores, DecisionTree lost accuracy. Well, some algorithms have their limitations.\n","43724d7b":"# 1. Introduction to Dataset\n\n**From https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\/home :** \n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\n**PCA Features:**\n\nIt contains only numerical input variables which are the result of a PCA transformation. Features V1, V2, ... V28 are the principal components obtained with PCA.\n\n**Time:**\n\nFeature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. \n\n**Amount:** \n\nThe feature 'Amount' is the transaction Amount.\n\n**Class:**\n\nFeature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n","7c9c00fa":"### First-Run: Predictions on Default Parameters\n\n- Here, I'll try a few simple classifiers and compare their performance. ","218327dd":"- SVC's scores have the most disparity between train and test sets. Train splits had perfect recall, but test set was very poor.\n- First three have `param_C` to `1`, followed by `0.7`. That's very conclusive I'd say.\n- Compared with its default settings, its score also decreased. SVC can be very good at learning from train data, but it\u2019s very sensitive when tested in different data.","86ef8207":"# 3. Modeling Outcome of Interest\n[Back to Outline](#Outline)\n\n## The Problem of Imbalanced Data (How NOT to do it...)\n\n- Here I'll do a base-line prediction of frauds using default settings on the data without any modifications.\n- This serves to show the need for techniques on Class Imbalance.\n---\n**Approach**\n- Below I split data into train and test groups. \n- I'll make sure the groups maintain the same class balance as the whole set. That way they can better represent the whole, for testing purposes.","5244ca75":"**Approach to removing outliers:**\n\n**For features of high positive correlation...**\nRemove non-fraud outliers on the top range, (improve recall) and remove fraud outliers on the bottom range. (improve specificity)\n\n**For features of high negative correlation...**\nRemove non-fraud outliers on the bottom range, (improve recall) and remove fraud outliers on the top range. (improve specificity)","330fe437":"- Yes!! With our optimize function, specificity in LogisticRegression improved from 93% to 97%, while still having perfect recall.\n- By looking at these results, there's no doubt that `liblinear, l1` is the best combination, regardless of `C_param`.\n- Also, `class_weight` for non-frauds set to 0.5 (`1:1,0:5`) seem to rank better. This is likely the result of the custom scoring which now rewards higher precisions.","3a32501e":"**Understanding the scores**\n\nSensitivity (or Recall) is the percentage of positives correctly identified.\n\nSpecificity is just the opposite, the percentage of negatives correctly identified.\n\nThe confusion matrix and classification reports reveal that **the high scores are merely a reflection of the class imbalance.** Since we're using a generalized scoring method, accuracy reflects the recall of both frauds and non-frauds. However, since frauds are so few,(`0.0017%`) their poor recall(`53%`) isn't reflected in the overall accuracy score.\n\n**On the test set**\n- Of `98` fraud cases in the test set, `52` were correctly labeled as frauds. And almost a half, `46` were mislabeled as non-frauds.\n- All except `11` non-frauds were correctly labeled as non-frauds, from a total of `56,864`. That's nearly perfect, but the priority should be to prevent frauds. Therefore, this is rather a secondary metric for us.","668e3bcb":"# Outline\n\n[1. Introduction to Dataset](#1.-Introduction-to-Dataset)\n\n[2. Exploratory Data Analysis](#2.-Exploratory-Data-Analysis)\n\n[3. Modeling Outcome of Interest](#3.-Modeling-Outcome-of-Interest)\n    \n- [3.1. Feature Engineering](#Feature-Engineering)\n- [3.2. Data Processing](#Data-Processing)\n- [3.3. Optimize Specificity while Maintaining Perfect Recall](#Optimize-Specificity-while-Maintaining-Perfect-Recall)\n\n[4. Research Question](#4.-Research-Question)\n\n[5. Choosing Model](#5.-Choosing-Model)\n\n[6. Practical Use for Audiences of Interest](#6.-Practical-Use-for-Audiences-of-Interest)\n\n[7. Weak Points & Shortcomings](#7.-Weak-Points-&-Shortcomings)","0c1addd9":"- From the feature engineered variables, it looks like `4_hour` has a very strong (negative) correlation with 'Class'. Well, at least one was useful.\n- Now let's see some actual numbers for feature correlations.","08fb9f1e":"## 'Amount' Distribution\n\n- Variable isn't normalized.\n- There's high concentrations of small-amount transactions. And many dispersed large-amount outliers, all the way up to \\$25,000\n- 85\\% of data is below \\$140\n- Top 1% of transaction amounts are between 1017.97 and 25691.16\n\n**Amount of frauds**\n- 80% of Frauds are less than: \\$152.34.","5cf1872e":"# 2. Exploratory Data Analysis\n[Back to Outline](#Outline)","27033c58":"### Feature Selection\n\n- I'll use the correlation matrix again, but this time I'll filter out features with low predictive power, instead of outliers.\n\nBut first, let's see what the outlier removal did to the correlations.","58218408":"### Classification Improvements after Feature Engineering\n\n- We've added some features, and re-coded two existing features. Let's see how classification performs now.\n- In this classification I'll define `X` and `y`, as well as `train` and `test` samples from the `features` DataFrame, which has the feature-engineered version of the data.\n- Also, I'll use `recall_score` as the scoring function for cross-validation. This represents the percentage of frauds correctly identified.","16eaea22":"## Class Imbalance\n- This is the most unique quality about this dataset. Most of the steps taken later will be about multiple ways of dealing with imbalanced data.","4c248414":"## Data Processing\n[Back to Outline](#Outline)\n\nData processing will include class-balancing, removing outliers, and feature-selection.","d071af35":"- Our best overall scores on test group. Recal wasn\u2019t perfect, but it has the highest combination of scores.","bbc0a76f":"## Visualizing distributions. \n- Features have different central tendencies and need to be normalized to make better sense of them.\n- 'Time' is encoded in seconds, out of a 24Hr day. We'll need to transform it in order to visualize it properly.","89706ff9":"# 4. Research Question\n[Back to Outline](#Outline)\n\n**What is the best way to predict frauds? (Pick an approach...)**\n\n- Focus on reducing false negatives.\n\nVS\n\n- Focus on reducing false positives.\n\nVS\n\n- Focus on a custom balance?\n\n# 5. Choosing Model\n[Back to Outline](#Outline)\n\n### Perfect Recall\n\n- <u>Judged by perfect recall and high specificity<\/u>, LogisticRegression had the highest optimized score with 97% specificity and 100% recall.\n\n### Best Overall\n\n- <u>For a more flexible approach<\/u>, RandomForestClassifier had the highest combined recall and specificity with only one missed fraud and 99% specificity.\n\n# 6. Practical Use for Audiences of Interest\n[Back to Outline](#Outline)\n\n- **Bank\u2019s fraud-prevention mechanisms.**\n(Annoying: Transactions canceled when traveling)\n\n- **Data Science students.**\nAddition to the pool of Kaggle\u2019s forks on this Dataset.\n\n# 7. Weak Points & Shortcomings\n[Back to Outline](#Outline)\n\n- **Model Processing-** Involves many steps. Steps depend immensely on the data. This doesn\u2019t lend itself to quick iterations. \n> Could\u2019ve used a processing pipeline function, but that\u2019s a more advanced method I haven\u2019t experimented with.\n- **Need for Data Reduction-** 270,000 non-frauds were undersampled to 5,000\u2026 Definitely affected accuracy. A supercomputer might handle complete set without the need for reduction. \nSVM and Kneighbors took the longest, even after undersampling the train data.\n","f590d5f7":"### Iteration Function\n\nSince I'll apply the new settings to several classifiers, I'll define a function to reuse several times.\n- It'll take the parameters you want to compare, and the classifier you want to try.\n- It'll determine best parameters based on custom scoring, do cross-validation for recall on train data, then train and predict the test set. \n- It'll show us the recall scores for train and test, a confusion matrix, a classification report, the GridSearch' top combinations, and a view of the performance DataFrame.","2487bfab":"## Checking for Missing Data.\n- Fortunately, data integrity is perfect. All non-null, and no mixed types.","49b8c9f1":"- I'll make a loop that checks each feature for correlation value, and if greater than that, it'll remove outliers for that variable following a certain cutoff.","7b75f6be":"### Imblearn' BalancedRandomForest- Optimized\n- This algorithm incorporates a RandomForestClassifier with a RandomUndersampling algorithm to balance classes according to the `sampling_strategy` parameter.","144760e2":"## Optimize Specificity while Maintaining Perfect Recall\n[Back to Outline](#Outline)\n\n- In this section I'll implement a few ideas to minimize false-positives (non-frauds identified as frauds), while still predicting all frauds correctly. \n\n### Custom Scoring Function\n\n- Parameter search functions use a scoring parameter to determine the best parameter combination. In the previous experiments we've used recall score as the basis. Now we want to pick a parameter combination that also takes specificity into account, while ensuring perfect recall.","d6c2d445":"- It's obvious that most features gained correlation power, regardless of direction. Positive correlations went higher up, negative correlations went lower down. Also, the highest correlations flattened out, while the smallest ones rose to relevance.\n- It is clearly an indicator that the outliers were causing noise, and therefore dimming the correlation-potential of each feature.","1d66d4cc":"### KNeighborsClassifier- Optimized","9a3136e7":"- It seems like the top 5 combinations had a perfect `recall_score`, which explain why they all have a rank of `1`. This means there was no need for a real comparison for the 'best' parameters, because they all were perfect. We simply got the parameters that were first on the list of **perfect** combinations.\n- Since we wanted to prioritize fraud recall, we set a very skewed `class_weight` parameter. This is why the results produced such perfect recall scores, at the expense of specificity.\n- Let's find the right balance between perfect recall and higher specificity.","fea3ecca":"## 'Time' Distribution\n- I'll convert 'Time' to hours and minutes, which will allow for better visualization.\n- 'Time' distribution (by second) shows two normal curves, which might reveal something meaningful for predicting purposes. This will be the basis for a time-based feature engineering.","88c58ccb":"### Add the Rest: PCA and Class\n","ba6379fe":"### LogisticRegression- Optimized.","dafe0c6e":"### Feature: $0 Fraud Amounts...?\n- Many transactions are zero dollars. This might be confusing for our model's predictive ability. It is arguable these don't need to be prevented. \n    - One approach could be to simply discard these transactions. \n    - The second approach is to ignore it and focus on predicting transactions labeled as 'frauds', regardless of them having no dollar-value.\n    \n**For now, I'll use this as basis for a feature. Later I'll compare results between different approaches**","b099bc3b":"### Time-Based Features\n- There seem to be two normal distributions in the feature Time. Let's isolate them so we can create features from them.","10790b94":"**Scores**\n- Now the cross_val scores reflect the fraud recall on three folds of the train data. These numbers are more informative for us now.\n- The mean recall from train data is also very consistent with the test recall. This is evidence of the model's certainty.\n- Fraud Recall went up from `53%` to `83%`. That's pretty good already, but it's far from perfect. We still have `17` frauds in the test set that aren't being predicted.\n\n**What's next**\nThe main obstacles for high accuracy are currently class-imbalance, outliers and noise. Fixing these involves changing the length of the data, meaning we won't have the same datapoints present afterwards. For that reason, we'll only use the features' `train` data to make these transformations, and use the features' `test` data to make predictions.","d2c9ec41":"### Normalize Time and Amount\n- Although we already captured some features from 'Time' and 'Amount', before decidedly dropping them, I'd like to normalize and test them in the model.","1e6a3edd":"- `GridSearch` allows us to see the results that informed the choice of best parameters, based on our scoring function. In this case, `recall_score`. Let's see how they compare.","9cc9527b":"- It's clear that `Time` and `Amount` are in a different range compared to the `PCA` features.","2c7d28ea":"- Now we have much more data because the frauds were oversampled to match the size of non-frauds.\n- Notice that ADASYN isn't perfectly matching the number of frauds to the majority class. This is good enough though. ","12c88d36":"- These results are very promising for a first run, considering I didn't tweak any of the parameters.\n- Now let's do a GridSearchCV to find the best parameters for these classifiers.","6e6ad777":"### DecisionTreeClassifier- Optimized","fd4f763d":"## Pyrrhic Victory- \n**A victory that inflicts such a devastating toll on the victor that it is tantamount to defeat. Someone who wins a Pyrrhic victory has also taken a heavy toll that negates any true sense of achievement.**\n\n- Well, fraud recall improved on Logistic Regression.\n- However, this has come at the cost of horribly low specificity.","3862567e":"### Logistic Regression- GridSearch & Recall Score. \n\n- `GridSearchCV` compares parameter combinations to find the highest score, determined by the user. I'll set `recall_score` to be the determinant factor for the best parameter combination.\n\n- The `class_weight` parameter greatly skews the classification emphasis from focusing on frauds at the expense of more non-fraud errors. For now, I'll prioritize fraud prevention. Later, I'll improve on specificity.\n\n**About the parameters to optimize**\n- Solvers `'newton-cg', 'lbfgs', and 'sag'` handle only `L2`-penalty. So we'll have to do this using two parameter grids: First for `L2`-only solvers, and then for `L1 and L2`-solvers."}}