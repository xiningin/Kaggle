{"cell_type":{"ed751340":"code","48ffd30d":"code","4fd0c304":"code","473ebe39":"code","428bf5f6":"code","ffb3aeee":"code","5b994159":"code","7366692f":"markdown","70eaaab6":"markdown","9c161e83":"markdown","80ac5603":"markdown","e059a689":"markdown","267a9be3":"markdown","bd752d89":"markdown","e4df76e4":"markdown","73dc2473":"markdown"},"source":{"ed751340":"!pip install praat-textgrids","48ffd30d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory","4fd0c304":"import textgrids\n\nFRAME_DURATION = 30 # 30 msec\nOVERLAP_RATE = 0.0 # frames don't overlap\n\ndef readFile(path):\n    '''\n    Read the file and return the list of SPEECH\/NONSPEECH labels for each frame\n    '''\n        \n    labeled_list  = []\n    grid = textgrids.TextGrid(path)\n\n    for interval in grid['silences']:\n        label = int(interval.text)\n\n        dur = interval.dur\n        dur_msec = dur * 1000 # sec -> msec\n        num_frames = int(round(dur_msec \/30)) # the audio is divided into 30 msec frames\n        print(dur_msec)\n        for i in range(num_frames):\n            \n            labeled_list.append(label)\n\n    return labeled_list","473ebe39":"!pip install librosa\nimport librosa\n\nroot ='\/Female\/TIMIT\/SA2'\nannotation_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Annotation\/Female\/TMIT\/SI2220.TextGrid\"\naudio_path = \"\/kaggle\/input\/speech-activity-detection-datasets\/Data\/Audio\/Female\/TMIT\/SI2220.wav\"\n\n# read annotaion\nlabel_list = readFile(annotation_path)\n\n# read wav file\ndata, fs = librosa.load(audio_path)","428bf5f6":"\n# define time axis\nNs = len(data)  # number of sample\nTs = 1 \/ fs  # sampling period\nt = np.arange(Ns) * 1000 * Ts  # time axis\n\n\nshift = 1 - OVERLAP_RATE\nframe_length = int(np.floor(FRAME_DURATION * fs \/ 1000)) # frame length in sample\nframe_shift = round(frame_length * shift)# frame shift in sample\n","ffb3aeee":"import matplotlib.pyplot as plt\n\nfigure = plt.Figure(figsize=(10, 7), dpi=85)\nplt.plot(t, data)\n\nfor i, frame_labeled in enumerate(label_list):\n    idx = i * frame_shift\n    if (frame_labeled == 1):\n        plt.axvspan(xmin= t[idx], xmax=t[idx + frame_length-1], ymin=-1000, ymax=1000, alpha=0.4, zorder=-100, facecolor='g', label='Speech')\n\nplt.title(\"Ground truth labels\")\nplt.legend(['Signal', 'Speech'])\nplt.show()","5b994159":"print(len(label_list))\nprint(len(t))\nprint(len(data))","7366692f":"### Define the function extracting the ground truth labels","70eaaab6":"### Install library","9c161e83":"# Example of usage ","80ac5603":"## Load a file","e059a689":"The green parts indicates the frames where an human speech is detected.","267a9be3":"## Plot signal","bd752d89":"### Load data from kaggle","e4df76e4":"It's important for this kind of tasks to perform short time analysis on the signal, so it needs to assign the lables (SPEECH\/NONSPEECH) to very little portions of the signal. I decide to split the data into portion of 30 milliseconds.","73dc2473":"### Preparing the variable"}}