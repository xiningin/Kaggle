{"cell_type":{"79bfb59f":"code","6bb0524d":"code","fb1338af":"code","60909113":"code","edb62d9c":"code","b79b4065":"code","1180d65f":"code","57723156":"code","6d29b7d2":"code","7b0ee5d5":"code","d1e9e697":"code","df36e215":"code","021b4e25":"code","65b9c20a":"code","adff2fdd":"code","ac2b8395":"code","ca5f46e3":"code","1747523c":"code","83f4e5a7":"code","78bfa550":"code","c3cdf7ea":"code","3447dfed":"code","3471be10":"code","a2cd2434":"code","dd419ddd":"code","43b2e248":"code","52e4418b":"code","2de54d06":"code","bbbc28f9":"code","869b1a98":"code","db8ee169":"code","debf4aea":"code","b680285b":"markdown","f8be2ffe":"markdown","34b299e5":"markdown","e52477cc":"markdown","661a0c8b":"markdown","5531a5b9":"markdown","2d63b199":"markdown","62922dcd":"markdown","1e568c32":"markdown","eb7987de":"markdown","8650e27b":"markdown","1aed4eca":"markdown","c6cca6e2":"markdown","3758c510":"markdown","8025df69":"markdown","6629bacb":"markdown","38c46b32":"markdown"},"source":{"79bfb59f":"#REFERENCES\n#https:\/\/tinyurl.com\/r7z663r -- Twitter Sentiment Analysis usig Naive Bayes\n#https:\/\/www.kaggle.com\/arefoestrada\/nlp-classifying-disaster-tweets\n#https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n#https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n#https:\/\/www.kaggle.com\/manjeetsingh\/eda-for-disaster-tweets\n#https:\/\/www.kaggle.com\/arefoestrada\/nlp-classifying-disaster-tweets\/notebook\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom nltk import corpus\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nimport re\nimport string\n\n\n\n#importing dataset\nsample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","6bb0524d":"train.head()","fb1338af":"test.head()","60909113":"train.isnull().sum()","edb62d9c":"test.isnull().sum()","b79b4065":"values = {'keyword': \"no_keyword\", 'location': \"no_location\"}\ntrain.fillna(value=values)","1180d65f":"test.fillna(value=values)","57723156":"x=train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","6d29b7d2":"freq = pd.Series(' '.join(train['text']).split()).value_counts()[:20]\nfreq","7b0ee5d5":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","d1e9e697":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(train['text'])[:10]\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","df36e215":"#removes URL from text\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ntrain['text']=train['text'].apply(lambda x : remove_URL(x))","021b4e25":"#removes HTML tags from text\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ntrain['text']=train['text'].apply(lambda x : remove_html(x))","65b9c20a":"#removing emojis\n# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ntrain['text']=train['text'].apply(lambda x: remove_emoji(x))","adff2fdd":"#removing punctuations\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ntrain['text']=train['text'].apply(lambda x : remove_punct(x))","ac2b8395":"#using pyspellchecker to remove spelling errors\n\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\ntrain['text']=train['text'].apply(lambda x : correct_spellings(x))","ca5f46e3":"# Importing train_test_split\nfrom sklearn.model_selection import train_test_split","1747523c":"X_train, X_test, y_train, y_test = train_test_split(train['text'], train['target'])","83f4e5a7":"# Initializing the vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvect = TfidfVectorizer()","78bfa550":"# Learning the vocabulary inside the datasets and transform the train's train dataset into matrix\nX_train_vect = vect.fit_transform(X_train)\nX_test_vect = vect.transform(X_test)","c3cdf7ea":"# Importing Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\n\nmodel = MultinomialNB()","3447dfed":"# Training the train dataset\nmodel.fit(X_train_vect, y_train)","3471be10":"# Predicting the target (0 for non-disaster tweet, 1 for disaster tweet)\ny_predict = model.predict(X_test_vect)","a2cd2434":"# Estimating the accuracy of the model\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix","dd419ddd":"# Classification report\nprint(classification_report(y_test, y_predict))","43b2e248":"# Confusion matrix\nprint(confusion_matrix(y_test, y_predict))","52e4418b":"# Accuracy score\nprint(accuracy_score(y_test, y_predict))","2de54d06":"# Extracting the tweets from the test dataset\ntext_test = test['text']","bbbc28f9":"# Transforming the tweets into matrix\ntext_test_trans = vect.transform(text_test)\n\n# Predicting the tweets\nresult = model.predict(text_test_trans)\n\n# Putting the result into the submission's dataframe\nsample_submission['target'] = result\n\n","869b1a98":"sample_submission.head()\n","db8ee169":"sample_submission.to_csv('submission.csv', index = False)","debf4aea":"import pandas as pd\nsample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","b680285b":"##### 1. A glimpse of the training data:","f8be2ffe":"Most common word:\n\nThe most common words here are mostly stopwords. Words that have no meaning. \nThis indicates the data needs to be cleaned.","34b299e5":"### Basic EDA\n\n1. Finding class distribution","e52477cc":"#### 2. A glimpse of test data:","661a0c8b":"### 3. Missing Values\nIn both training and test data, we can see that the location and keyword column has missing values.\n\n1. Total number of null values in keywords & location columns in training set","5531a5b9":"Removing URLS:","2d63b199":"The distribution shows that there are more non-disaster tweeets than disaster ones.","62922dcd":"#### Cleaning the text column:","1e568c32":"Since the number of missing values are too large to just omit\/drop the rows altogether. \nMissing values in those features are filled with no_keyword and no_location respectively.","eb7987de":"### Data Cleaning","8650e27b":"# Practice Classifier for Disaster Tweeets","1aed4eca":"#### Vectorizing Dataset\n\nTfidf vectorizer is used to create a corpus and vectorize the train's train dataset and test dataset.","c6cca6e2":"The analysis of top Bi-gram shows that those words mostly have links\/urls.","3758c510":"### Inferencing from Test Dataset","8025df69":"#### Ngram Analysis:\nThe most common bigrams in tweets.","6629bacb":"2. Total number of null values in keywords & location columns in test set","38c46b32":"#### Classifying Each Tweet\n\nMultinomial Naive Bayes classifier, which is suitable for classification on text."}}