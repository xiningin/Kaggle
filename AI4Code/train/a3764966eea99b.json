{"cell_type":{"613460c0":"code","8bd404cb":"code","e2177abd":"code","ed4108a0":"code","1c90bc32":"code","68929924":"code","efc557c8":"code","9c52b3db":"code","48d10b4b":"code","10c4f771":"code","4cf54dbe":"code","5a23a7cc":"code","80b12156":"code","dccc1f58":"code","5e5cf3f1":"code","be5dda4b":"code","f15956bb":"code","18d02056":"code","58580eb5":"code","d521a635":"code","98269a33":"markdown","d4ce8df7":"markdown","a84fc45d":"markdown","8d57c20a":"markdown","011b9526":"markdown","abf7d68e":"markdown"},"source":{"613460c0":"!pip install texthero","8bd404cb":"!pip install -U spacy","e2177abd":"!pip install tldextract","ed4108a0":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport texthero as hero  # text processing\nimport seaborn as sns # plotting\nimport tldextract # url domain extraction\n# from tqdm.auto import tqdm # progress bars\n# from lime.lime_text import LimeTextExplainer # Model Explanation\n# from sklearn import metrics # from here and below used for machine learning\n# from sklearn.naive_bayes import GaussianNB, MultinomialNB\n# from sklearn.svm import LinearSVC\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.pipeline import Pipeline, FeatureUnion\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.preprocessing import MaxAbsScaler, FunctionTransformer\n# from sklearn.model_selection import cross_validate, train_test_split\n# from sklearn.feature_extraction.text import TfidfVectorizer","1c90bc32":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport sys\nimport os\n\nfor dirname, _, filenames in os.walk(\"\/kaggle\/input\"):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68929924":"# load data\ndf = pd.read_csv(\n    '\/kaggle\/input\/albanian-fake-news-corpus\/alb-fake-news-corpus.csv',\n    parse_dates=['publication_datetime']\n)","efc557c8":"def preprocess_text(s):\n    \"\"\"A text processing pipeline for cleaning up text using the hero package.\"\"\"\n    s = hero.fillna(s)\n    s = hero.lowercase(s)\n    s = hero.remove_digits(s)\n    s = hero.remove_punctuation(s)\n    s = hero.remove_diacritics(s)\n    s = hero.remove_whitespace(s)\n    return s\n\n# A list of stopwords taken from https:\/\/github.com\/arditdine\/albanian-nlp\nSTOPWORDS = [\"e\", \"te\", \"i\", \"me\", \"qe\", \"ne\", \"nje\", \"a\", \"per\", \"sh\", \"nga\", \"ka\", \"u\", \"eshte\", \"dhe\", \"shih\", \"nuk\",\n             \"m\", \"dicka\", \"ose\", \"si\", \"shume\", \"etj\", \"se\", \"pa\", \"sipas\", \"s\", \"t\", \"dikujt\", \"dike\", \"mire\", \"vet\",\n             \"bej\", \"ai\", \"vend\", \"prej\", \"ja\", \"duke\", \"tjeter\", \"kur\", \"ia\", \"ku\", \"ta\", \"keq\", \"dy\", \"ben\", \"bere\",\n             \"behet\", \"dickaje\", \"edhe\", \"madhe\", \"la\", \"sa\", \"gjate\", \"zakonisht\", \"pas\", \"veta\", \"mbi\", \"disa\", \"iu\",\n             \"mos\", \"c\", \"para\", \"dikush\", \"gje\", \"be\", \"pak\", \"tek\", \"fare\", \"beri\", \"po\", \"bie\", \"k\", \"do\", \"gjithe\",\n             \"vete\", \"mund\", \"kam\", \"le\", \"jo\", \"beje\", \"tij\", \"kane\", \"ishte\", \"jane\", \"vjen\", \"ate\", \"kete\", \"neper\",\n             \"cdo\", \"na\", \"marre\", \"merr\", \"mori\", \"rri\", \"deri\", \"b\", \"kishte\", \"mban\", \"perpara\", \"tyre\", \"marr\",\n             \"gjitha\", \"as\", \"vetem\", \"nen\", \"here\", \"tjera\", \"tjeret\", \"drejt\", \"qenet\", \"ndonje\", \"nese\", \"jap\",\n             \"merret\", \"rreth\", \"lloj\", \"dot\", \"saj\", \"nder\", \"ndersa\", \"cila\", \"veten\", \"ma\", \"ndaj\", \"mes\", \"ajo\",\n             \"cilen\", \"por\", \"ndermjet\", \"prapa\", \"mi\", \"tere\", \"jam\", \"ashtu\", \"kesaj\", \"tille\", \"behem\", \"cilat\",\n             \"kjo\", \"menjehere\", \"ca\", \"je\", \"aq\", \"aty\", \"prane\", \"ato\", \"pasur\", \"qene\", \"cilin\", \"teper\", \"njera\",\n             \"tej\", \"krejt\", \"kush\", \"bejne\", \"ti\", \"bene\", \"midis\", \"cili\", \"ende\", \"keto\", \"kemi\", \"sic\", \"kryer\",\n             \"cilit\", \"atij\", \"gjithnje\", \"andej\", \"siper\", \"sikur\", \"ketej\", \"ciles\", \"ky\", \"papritur\", \"ua\",\n             \"kryesisht\", \"gjithcka\", \"pasi\", \"kryhet\", \"mjaft\", \"ketij\", \"perbashket\", \"ata\", \"atje\", \"vazhdimisht\",\n             \"kurre\", \"tone\", \"keshtu\", \"une\", \"sapo\", \"rralle\", \"vetes\", \"ishin\", \"afert\", \"tjetren\", \"ketu\", \"cfare\",\n             \"to\", \"anes\", \"jemi\", \"asaj\", \"secila\", \"kundrejt\", \"ketyre\", \"pse\", \"tilla\", \"mua\", \"nepermjet\", \"cilet\",\n             \"ndryshe\", \"kishin\", \"ju\", \"tani\", \"atyre\", \"dic\", \"yne\", \"kudo\", \"sone\", \"sepse\", \"cilave\", \"kem\", \"ty\",\n             \"t'i\", \"nbsp\", \"tha\", \"re\", \"the\"]\n\n\ndef make_clf(classifier, scaler=None, feature_extractor=None, use_dense=False):\n    \"\"\"Function for generating pipelines for our experiment.\"\"\"\n    steps = []\n\n    if feature_extractor is not None:\n        steps.append([\"feature_extractor\", feature_extractor])\n    if use_dense:\n        steps.append(\n            [\"to_dense\", FunctionTransformer(lambda x: x.todense(), accept_sparse=True)]\n        )\n    if scaler is not None:\n        steps.append([\"scaler\", scaler])\n    steps.append([\"classifier\", classifier])\n\n    return Pipeline(steps)","9c52b3db":"df[\"fake_news\"] = df[\"fake_news\"].astype(bool)","48d10b4b":"df[\"preprocessed_content\"] = preprocess_text(df[\"content\"])","10c4f771":"# Remove stopwords\ndf[\"preprocessed_content_without_stopwords\"] = df[\"preprocessed_content\"].apply(\n    lambda x: \" \".join([word for word in x.split() if word not in (STOPWORDS)])\n)","4cf54dbe":"print(df.loc[df[\"fake_news\"]==1][\"content\"].values[0])","5a23a7cc":"print(df.loc[df[\"fake_news\"]==0][\"content\"].values[0])","80b12156":"# Defining colors for both sources to be used in our plots\npalette = [\"#d62728\", \"#1f77b4\"]","dccc1f58":"# Fake News Wordcloud\nhero.wordcloud(\n    df.loc[df[\"fake_news\"] == 1][\"preprocessed_content_without_stopwords\"],\n    max_words=100,\n    width=1900,\n    height=400,\n    background_color=palette[0],\n)","5e5cf3f1":"# Non-Fake News Wordcloud\nhero.wordcloud(\n    df.loc[df[\"fake_news\"] == 0][\"preprocessed_content_without_stopwords\"],\n    max_words=100,\n    width=1900,\n    height=400,\n    background_color=palette[1],\n)","be5dda4b":"sns.set(rc={'figure.figsize':(20,10)})\nsns.set_theme(style=\"whitegrid\")","f15956bb":"# texthero pipeline and plot\ndf[\"pca\"] = (\n    df[\"preprocessed_content_without_stopwords\"].pipe(hero.tfidf).pipe(hero.pca)\n)\n\nplot_values = np.stack(df[\"pca\"], axis=1)\nsns.scatterplot(\n    data=df,\n    x=plot_values[0],\n    y=plot_values[1],\n    hue=\"fake_news\",\n    style=\"fake_news\",\n    markers={True: \"X\", False: \"s\"},\n    alpha=0.5,\n    palette=palette.reverse(),\n)","18d02056":"df[\"article_word_count\"] = df[\"content\"].apply(lambda x: len(x.split()))\n\nax = sns.histplot(data=df, x=\"article_word_count\", hue=\"fake_news\", palette=palette)\nax.set(xlabel='Article Word Count', ylabel='Frequency')","58580eb5":"df[\"publisher\"] = df[\"web_page_url\"].apply(lambda x: tldextract.extract(x).domain)","d521a635":"# Publishers with the most fake news in the dataset\nsns.countplot(y='publisher', data=df.loc[df[\"fake_news\"]==1], order=df.loc[df[\"fake_news\"]==1].publisher.value_counts().index)","98269a33":"## Explorative Data Analysis\n\nLet's start with an example article from each source:","d4ce8df7":"## Preprocessing\n\nTo proceed, we only want to consider satirical news articles collected from the satire website Kungulli.com, and to have some genuine news data, the same number of news articles were taken from Kallxo.com as a credible source of news present in the dataset.","a84fc45d":"## Utility code","8d57c20a":"## Imports","011b9526":"## Load data","abf7d68e":"# Albanian Fake News Corpus Explorative Data Analysis\n\n\n## Getting started\n\nTo begin, we must install the necessary packages."}}