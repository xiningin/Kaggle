{"cell_type":{"2cc9e97c":"code","c3c98d82":"code","de9052f3":"code","ffa88641":"code","f12f4844":"code","3d4778e3":"code","ecd9727b":"code","8ca49a1b":"code","74ccf144":"code","d8ad3a6b":"code","d8b606b0":"code","5c9ba4bc":"code","e105f88f":"code","fcdbc30f":"code","7c182250":"code","c5a889d7":"code","7600aa63":"code","b06e16b9":"code","6ff05849":"code","a65f1d5e":"code","47de456d":"code","0f313969":"code","a4e1101a":"code","1782a609":"code","be7539a1":"code","40be29ae":"code","6a534d4c":"code","a2ac173a":"code","aec8197e":"code","870a4c31":"code","2ad6817f":"code","70728788":"code","6f21ecd8":"code","0b71e35b":"code","481790d1":"code","b36b0c8c":"code","7a0fa3e0":"code","8a7358ca":"code","498d0756":"code","c113af09":"code","82510a3d":"code","135c5c5e":"code","cd23aeda":"code","9e5e9589":"code","75e9cdcc":"code","03ed3c68":"code","96a859d3":"code","87db5e5b":"code","26d63113":"code","11a768b0":"code","7418b7b4":"code","53e5476e":"code","f0463509":"code","b62ac2c1":"code","5660da29":"code","547e87e5":"code","c98ed8dc":"code","26d919fe":"code","84765f0a":"code","b0ea90f1":"code","b59c8913":"code","eddbb1c1":"code","a79be3e6":"code","f4463440":"code","690fef6e":"code","3b14d36b":"code","96dbaff1":"code","d8acb73b":"code","8fcde3fe":"code","68ab52d2":"code","58219fc3":"code","3a29076a":"code","5c36d7b3":"code","a6e3025d":"code","e19be692":"code","e543c5d5":"code","cf7d73b0":"code","5a38933a":"code","2971c5ea":"code","e74850c1":"code","a6dc37bf":"markdown","e8a2a150":"markdown","c21658fe":"markdown","259020a3":"markdown","9e3798b6":"markdown","db02abaf":"markdown","1365173a":"markdown","fdc5b3ec":"markdown","b82f814c":"markdown","651d199c":"markdown","2bf6206d":"markdown","1731452e":"markdown","20bf7f99":"markdown","83e2e791":"markdown","be562738":"markdown","0ae88792":"markdown","85dea7bb":"markdown","feaf8eb4":"markdown","70ec18ce":"markdown","39bdb708":"markdown","cbf286c4":"markdown","0f457c71":"markdown","ce7f0f7b":"markdown","ad809612":"markdown","509573ec":"markdown","272ac876":"markdown","393680d5":"markdown","a29f67cb":"markdown","f63a6e30":"markdown","40d9e9ec":"markdown","d8703d6b":"markdown","41cb7472":"markdown","7a4e18c1":"markdown","a88e6a99":"markdown","ee90b8be":"markdown","39b4566e":"markdown","3368110b":"markdown","e9fc8608":"markdown","46ba973f":"markdown","a7057763":"markdown","72d6eb05":"markdown","508c9db0":"markdown","a40e61f7":"markdown","25ee53be":"markdown"},"source":{"2cc9e97c":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\nimport warnings\nwarnings.simplefilter(action =\"ignore\")\n\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import the necessary packages\nimport numpy as np\nimport pandas as pd\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\n# Algorithms\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import learning_curve\nprint(\"All Libraries have imported successfully!!!\")","c3c98d82":"# Load Dataset\ndata = pd.read_csv('..\/input\/fetal-health-classification\/fetal_health.csv')\ndata","de9052f3":"# Print the first 5 rows of the dataframe.\n\ndata.head()","ffa88641":"print(f\"Dataset Columns:\\n{data.columns}\")","f12f4844":"# Analyse statically insight of data\ndata.describe().T","3d4778e3":"data.info(verbose=True)","ecd9727b":"print(f\"The dataset size: {data.shape}\")","8ca49a1b":"# Count the missing and null values for dataset fetal healt.\nmiss_values = data.columns[data.isnull().any()]\nprint(f\"Missing values:\\n{data[miss_values].isnull().sum()}\")\n\nnull_values = data.columns[data.isna().any()]\nprint(f\"Null values:\\n{data[null_values].isna().sum()}\")","74ccf144":"# Null count analysis\nnull_plot = msno.bar(data, color = \"#5F9EA0\")","d8ad3a6b":"data[\"fetal_health\"].describe()","d8b606b0":"total = data[\"fetal_health\"].sum()\nnormal = total - 471\nsuspect = total - 1831\npathological = total - 1950\n\nprint(data[\"fetal_health\"].value_counts())\n\nplt.figure(figsize = (10,5))\nplt.subplot(121)\n# sns.countplot(x=\"fetal_health\", data=data)\nvis_fetal_health = data.fetal_health.value_counts().plot(figsize=(10, 5), kind=\"bar\", color = [\"#5F9EA0\", \"#B0E0E6\", \"#ADD8E6\"])\nplt.title(\"Fetal health count\")\nplt.xlabel(\"Fetal helth\")\nplt.ylabel(\"Cases\")\n\n\nplt.subplot(122)\n# plt.pie([normal, suspect, pathological], labels=[\"Normal\", \"Suspect\", \"Pathological\"], autopct=\"%1.0f%%\")\nplt.title(\"Fetal state\") \n\nvis_pie_fetal_health = plt.pie([normal, suspect, pathological], labels=[\"Normal\", \"Suspect\", \"Pathological\"], colors = [\"#5F9EA0\", \"#B0E0E6\", \"#ADD8E6\"], autopct=\"%1.0f%%\")\nplt.title(\"Fetal health count\")\nplt.xlabel(\"Fetal helth\")\nplt.ylabel(\"Cases\")\n\n\nplt.show()","5c9ba4bc":"data_hist_plot = data.hist(figsize = (20,20), color = \"#5F9EA0\")","e105f88f":"numeric_data = data.select_dtypes(exclude=\"object\")\nnumeric_corr = numeric_data.corr()\nf,ax=plt.subplots(figsize=(25,1))\nsns.heatmap(numeric_corr.sort_values(by=[\"fetal_health\"], ascending=False).head(1), cmap=\"GnBu\")\nplt.title(\"Numerical features correlation with the fetal_health\", weight=\"bold\", fontsize=18, color=\"#5F9EA0\")\nplt.yticks(weight=\"bold\", color=\"darkgreen\", rotation=0)\n\nplt.show()","fcdbc30f":"Num_feature = numeric_corr[\"fetal_health\"].sort_values(ascending=False).head(20).to_frame()\n\ncm = sns.light_palette(\"#5F9EA0\", as_cmap=True)\n\nstyle = Num_feature.style.background_gradient(cmap=cm)\nstyle","7c182250":"from pandas.plotting import scatter_matrix\nscatterMatrix = scatter_matrix(data,figsize=(50, 50), color = \"#5F9EA0\")","c5a889d7":"# Set the size of figure to 12 by 10.\nplt.figure(figsize=(15,15))  \n\n# Seaborn has very simple solution for heatmap\np=sns.heatmap(data.corr(), annot=True, cmap = \"GnBu\")  ","7600aa63":"columns = ['baseline value', 'accelerations', 'fetal_movement',\n       'uterine_contractions', 'light_decelerations', 'severe_decelerations',\n       'prolongued_decelerations', 'abnormal_short_term_variability',\n       'mean_value_of_short_term_variability',\n       'percentage_of_time_with_abnormal_long_term_variability',\n       'mean_value_of_long_term_variability', 'histogram_width',\n       'histogram_min', 'histogram_max', 'histogram_number_of_peaks',\n       'histogram_number_of_zeroes', 'histogram_mode', 'histogram_mean',\n       'histogram_median', 'histogram_variance', 'histogram_tendency']\nscale_X = StandardScaler()\nX =  pd.DataFrame(scale_X.fit_transform(data.drop([\"fetal_health\"],axis = 1),), columns = columns)","b06e16b9":"X.head()","6ff05849":"y = data[\"fetal_health\"]","a65f1d5e":"# Importing train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify = y)","47de456d":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","0f313969":"# Baseline model of Logistic Regression with default parameters:\n\nlogistic_regression = linear_model.LogisticRegression()\nlogistic_regression_mod = logistic_regression.fit(X_train, y_train)\nprint(f\"Baseline Logistic Regression: {round(logistic_regression_mod.score(X_test, y_test), 3)}\")\n\npred_logistic_regression = logistic_regression_mod.predict(X_test)","a4e1101a":"cv_method = StratifiedKFold(n_splits=3, \n                            random_state=42\n                            )","1782a609":"# Cross validate Logistic Regression model\nscores_Logistic = cross_val_score(logistic_regression, X_train, y_train, cv =cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Logistic Regression model:\\n{scores_Logistic}\")\nprint(f\"CrossValMeans: {round(scores_Logistic.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_Logistic.std(), 3)}\")","be7539a1":"params_LR = {\"tol\": [0.0001,0.0002,0.0003],\n            \"C\": [0.01, 0.1, 1, 10, 100],\n            \"intercept_scaling\": [1, 2, 3, 4]\n              }","40be29ae":"GridSearchCV_LR = GridSearchCV(estimator=linear_model.LogisticRegression(), \n                                param_grid=params_LR, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","6a534d4c":"# Fit model with train data\nGridSearchCV_LR.fit(X_train, y_train);","a2ac173a":"best_estimator_LR = GridSearchCV_LR.best_estimator_\nprint(f\"Best estimator for LR model:\\n{best_estimator_LR}\")","aec8197e":"best_params_LR = GridSearchCV_LR.best_params_\nprint(f\"Best parameter values for LR model:\\n{best_params_LR}\")","870a4c31":"print(f\"Best score for LR model: {round(GridSearchCV_LR.best_score_, 3)}\")","2ad6817f":"# The grid search returns the following as the best parameter set\nlogistic_regression = linear_model.LogisticRegression(C=10, intercept_scaling=1, tol=0.0001, penalty=\"l2\", solver=\"liblinear\", random_state=42)\nlogistic_regression_mod = logistic_regression.fit(X_train, y_train)\npred_logistic_regression = logistic_regression_mod.predict(X_test)\n\nmse_logistic_regression = mean_squared_error(y_test, pred_logistic_regression)\nrmse_logistic_regression = np.sqrt(mean_squared_error(y_test, pred_logistic_regression))\nscore_logistic_regression_train = logistic_regression_mod.score(X_train, y_train)\nscore_logistic_regression_test = logistic_regression_mod.score(X_test, y_test)","70728788":"print(f\"Mean Square Error for Logistic Regression = {round(mse_logistic_regression, 3)}\")\nprint(f\"Root Mean Square Error for Logistic Regression = {round(rmse_logistic_regression, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_logistic_regression_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_logistic_regression_test, 3)}\")","6f21ecd8":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_logistic_regression))","0b71e35b":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_logistic_regression))","481790d1":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_logistic_regression), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Normal\", \"Suspect\", \"Pathological\"]);","b36b0c8c":"# Baseline model of K-Nearest Neighbors with default parameters:\n\nknn = KNeighborsClassifier()\nknn_mod = knn.fit(X_train, y_train)\nprint(f\"Baseline K-Nearest Neighbors: {round(knn_mod.score(X_test, y_test), 3)}\")\n\npred_knn = knn_mod.predict(X_test)\n","7a0fa3e0":"# Cross validate K-Nearest Neighbors model\ncv_method = StratifiedKFold(n_splits=3, \n                            random_state=42\n                            )\n\nscores_knn = cross_val_score(knn, X_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for K-Nearest Neighbors model:\\n{scores_knn}\")\nprint(f\"CrossValMeans: {round(scores_knn.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_knn.std(), 3)}\")","8a7358ca":"params_knn = {\"leaf_size\": list(range(1,30)),\n              \"n_neighbors\": list(range(1,21)),\n              \"p\": [1,2]}","498d0756":"GridSearchCV_knn = GridSearchCV(estimator=KNeighborsClassifier(), \n                                param_grid=params_knn, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=-1,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","c113af09":"# Fit model with train data\nGridSearchCV_knn.fit(X_train, y_train);","82510a3d":"best_estimator_knn = GridSearchCV_knn.best_estimator_\nprint(f\"Best estimator for KNN model:\\n{best_estimator_knn}\")","135c5c5e":"best_params_knn = GridSearchCV_knn.best_params_\nprint(f\"Best parameter values:\\n{best_params_knn}\")","cd23aeda":"best_score_knn = GridSearchCV_knn.best_score_\nprint(f\"Best score for GNB model: {round(best_score_knn, 3)}\")","9e5e9589":"# Test with new parameter for KNN model\nknn = KNeighborsClassifier(leaf_size=1, n_neighbors=3 , p=1)\nknn_mod = knn.fit(X_train, y_train)\npred_knn = knn_mod.predict(X_test)\n\nmse_knn = mean_squared_error(y_test, pred_knn)\nrmse_knn = np.sqrt(mean_squared_error(y_test, pred_knn))\nscore_knn_train = knn_mod.score(X_train, y_train)\nscore_knn_test = knn_mod.score(X_test, y_test)","75e9cdcc":"print(f\"Mean Square Error for K_Nearest Neighbor  = {round(mse_knn, 3)}\")\nprint(f\"Root Mean Square Error for K_Nearest Neighbor = {round(rmse_knn, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_knn_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_knn_test, 3)}\")","03ed3c68":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_knn))","96a859d3":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_knn))","87db5e5b":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_knn), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Normal\", \"Suspect\", \"Pathological\"]);","26d63113":"# Baseline model of RF with default parameters:\n\nrandom_forest = RandomForestClassifier()\nrandom_forest_mod = random_forest.fit(X_train, y_train)\nprint(f\"Baseline Random Forest: {round(random_forest_mod.score(X_test, y_test), 3)}\")\n\npred_random_forest = random_forest_mod.predict(X_test)","11a768b0":"# Cross validate Random forest model\nscores_RF = cross_val_score(random_forest, X_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Random forest model:\\n{scores_RF}\")\nprint(f\"CrossValMeans: {round(scores_RF.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_RF.std(), 3)}\")","7418b7b4":"params_RF = {\"min_samples_split\": [2, 6, 20],\n              \"min_samples_leaf\": [1, 4, 16],\n              \"n_estimators\" :[100,200,300,400],\n              \"criterion\": [\"gini\"]             \n              }","53e5476e":"GridSearchCV_RF = GridSearchCV(estimator=RandomForestClassifier(), \n                                param_grid=params_RF, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","f0463509":"# Fit model with train data\nGridSearchCV_RF.fit(X_train, y_train);","b62ac2c1":"best_estimator_RF = GridSearchCV_RF.best_estimator_\nprint(f\"Best estimator for RF model:\\n{best_estimator_RF}\")","5660da29":"best_params_RF = GridSearchCV_RF.best_params_\nprint(f\"Best parameter values for RF model:\\n{best_params_RF}\")","547e87e5":"best_score_RF = GridSearchCV_RF.best_score_\nprint(f\"Best score for RF model: {round(best_score_RF, 3)}\")","c98ed8dc":"random_forest = RandomForestClassifier(criterion=\"gini\", n_estimators=100, min_samples_leaf=1, min_samples_split=2, random_state=42)\nrandom_forest_mod = random_forest.fit(X_train, y_train)\npred_random_forest = random_forest_mod.predict(X_test)\n\nmse_random_forest = mean_squared_error(y_test, pred_random_forest)\nrmse_random_forest = np.sqrt(mean_squared_error(y_test, pred_random_forest))\nscore_random_forest_train = random_forest_mod.score(X_train, y_train)\nscore_random_forest_test = random_forest_mod.score(X_test, y_test)","26d919fe":"print(f\"Mean Square Error for Random Forest = {round(mse_random_forest, 3)}\")\nprint(f\"Root Mean Square Error for Random Forest = {round(rmse_random_forest, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_random_forest_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_random_forest_test, 3)}\")","84765f0a":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_random_forest))","b0ea90f1":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_random_forest))","b59c8913":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_random_forest), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Normal\", \"Suspect\", \"Pathological\"]);","eddbb1c1":"# Baseline model of gradient boosting classifier with default parameters:\ngbc = GradientBoostingClassifier()\ngbc_mod = gbc.fit(X_train, y_train)\nprint(f\"Baseline gradient boosting classifier: {round(gbc_mod.score(X_test, y_test), 3)}\")\n\npred_gbc = gbc_mod.predict(X_test)","a79be3e6":"# Cross validate Gradient Boosting Classifier model\nscores_GBC = cross_val_score(gbc, X_train, y_train, cv = cv_method, n_jobs = 2, scoring = \"accuracy\")\n\nprint(f\"Scores(Cross validate) for Gradient Boosting Classifier model:\\n{scores_GBC}\")\nprint(f\"CrossValMeans: {round(scores_GBC.mean(), 3)}\")\nprint(f\"CrossValStandard Deviation: {round(scores_GBC.std(), 3)}\")","f4463440":"params_GBC = {\"loss\": [\"deviance\"],\n              \"learning_rate\": [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1], \n              \"n_estimators\": [250, 500],\n              \"max_depth\": [3, 5, 8]\n              }","690fef6e":"GridSearchCV_GBC = GridSearchCV(estimator=GradientBoostingClassifier(), \n                                param_grid=params_GBC, \n                                cv=cv_method,\n                                verbose=1, \n                                n_jobs=2,\n                                scoring=\"accuracy\", \n                                return_train_score=True\n                                )","3b14d36b":"# Fit model with train data\nGridSearchCV_GBC.fit(X_train, y_train);","96dbaff1":"# Get the best estimator values.\nbest_estimator_GBC = GridSearchCV_GBC.best_estimator_\nprint(f\"Best estimator values for GBC model:\\n{best_estimator_GBC}\")","d8acb73b":"# Get the best parameter values.\nbest_params_GBC = GridSearchCV_GBC.best_params_\nprint(f\"Best parameter values for GBC model:\\n{best_params_GBC}\")","8fcde3fe":"# Best score for GBC by using the best_score attribute.\nbest_score_GBC = GridSearchCV_GBC.best_score_\nprint(f\"Best score value foe GBC model: {round(best_score_GBC, 3)}\")","68ab52d2":"# Test with new parameter for GBC model\ngbc = GradientBoostingClassifier(criterion=\"friedman_mse\", learning_rate=1, loss=\"deviance\", max_depth=5, max_features=\"log2\", min_samples_leaf=0.2, min_samples_split=0.5, n_estimators=200, random_state=42)\ngbc_mod = gbc.fit(X_train, y_train)\npred_gbc = gbc_mod.predict(X_test)\n\nmse_gbc = mean_squared_error(y_test, pred_gbc)\nrmse_gbc = np.sqrt(mean_squared_error(y_test, pred_gbc))\nscore_gbc_train = gbc_mod.score(X_train, y_train)\nscore_gbc_test = gbc_mod.score(X_test, y_test)","58219fc3":"print(f\"Mean Square Error for Gradient Boosting Classifier = {round(mse_gbc, 3)}\")\nprint(f\"Root Mean Square Error for Gradient Boosting Classifier = {round(rmse_gbc, 3)}\")\nprint(f\"R^2(coefficient of determination) on training set = {round(score_gbc_train, 3)}\")\nprint(f\"R^2(coefficient of determination) on testing set = {round(score_gbc_test, 3)}\")","3a29076a":"print(\"Classification Report\")\nprint(classification_report(y_test, pred_gbc))","5c36d7b3":"print(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, pred_gbc))","a6e3025d":"ax= plt.subplot()\nsns.heatmap(confusion_matrix(y_test, pred_gbc), annot=True, ax = ax, cmap = \"BuGn\");\n\n# labels, title and ticks\nax.set_xlabel(\"Predicted labels\");\nax.set_ylabel(\"True labels\"); \nax.set_title(\"Confusion Matrix\"); \nax.xaxis.set_ticklabels([\"Normal\", \"Suspect\", \"Pathological\"]);","e19be692":"# Plot learning curve\ndef plot_learning_curve(estimator, title, x, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, x, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"#80CBC4\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"#00897B\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","e543c5d5":"# Logistic Regression\nplot_learning_curve(GridSearchCV_LR.best_estimator_,title = \"Logistict Regression learning curve\", x = X_train, y = y_train, cv = cv_method);","cf7d73b0":"plot_learning_curve(GridSearchCV_knn.best_estimator_,title = \"KNN Classifier learning curve\", x = X_train, y = y_train, cv = cv_method);","5a38933a":"# Random forest\nplot_learning_curve(GridSearchCV_RF.best_estimator_,title = \"Random Forest learning curve\", x = X_train, y = y_train, cv = cv_method);","2971c5ea":"# Gradient Boosting Classifier\nplot_learning_curve(GridSearchCV_GBC.best_estimator_,title = \"Gradient Boosting Classifier learning curve\", x = X_train, y = y_train, cv = cv_method);","e74850c1":"results = pd.DataFrame({\n                        \"Model\": [\"Logistic Regression\",\n                                  \"KNN\",\n                                  \"Random Forest\",\n                                  \"Gradient Boosting Classifier\"],\n                        \"Score\": [logistic_regression_mod.score(X_train, y_train),\n                                  knn_mod.score(X_train, y_train),\n                                  random_forest_mod.score(X_train, y_train),\n                                  gbc_mod.score(X_train, y_train),\n                                    ]\n                        })\nresult_df = results.sort_values(by=\"Score\", ascending=False)\nresult_df = result_df.set_index(\"Score\")\nresult_df.head()","a6dc37bf":"## Correlation Numeric featurs with output variable(fetal_health)\n*Correlation & Correlation Matrix*\n\nHere, we want to show the correlation between numerical features and the target \"fetal_health\", in order to have a first idea of the connections between features. Just by looking at the heatmap below we can see some features have the dark colors, Those features have high correlation with the target.","e8a2a150":"## Gradient Boosting classifier (GBC)\n","c21658fe":"Property **DataFrame.shape** returns a tuple representing the dimensionality of the DataFrame.","259020a3":"### We can see three features: \"prolongued_decelerations\", \"abnormal_short_term_variability\", \"percentage_of_time_with_abnormal_long_term_variability\" have high correlation with the target culumn (fetal_health).","9e3798b6":"## Test Train Split and Cross Validation methods\n* **Train Test Split** : To have unknown datapoints to test the data rather than testing with the same points with which the model was trained. This helps capture the model performance much better.\n* **Cross-validation**, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.\n","db02abaf":"Data visualizations of **\"fetal_health\"** column makes easier to understand the fetal state, and visualization also makes it easier to detect patterns of the fetal state (Normal, Suspect, Pathological). ","1365173a":"**DataFrame.describe()** method generates descriptive statistics for only numeric values not for categorical values. This method summarizes the central tendency, dispersion, and shape of a dataset\u2019s distribution, excluding NaN values. \n\nNow, let's understand the statistics that are generated by the describe() method:\n* Count tells us the number of NoN-empty rows in a feature. As we can see there are no NoN-empty rows.\n* Mean tells us the mean value of that feature.\n* Std tells us the Standard Deviation Value of that feature.\n* Min tells us the minimum value of that feature.\n* 25%, 50%, and 75% are the percentile\/quartile of each features. This quartile information helps us to detect Outliers.\n* Max tells us the maximum value of that feature.","fdc5b3ec":"Data visualizations of \"fetal_health\" column shows us the percentage of fetal health state. ","b82f814c":"As the result, In this case, there is *neither null values nor missing values* in the dataset.","651d199c":"Second, we pass the GradientBoostingClassifier() and params_GBC as the model and the parameter dictionary into the GridSearchCV function.\n","2bf6206d":"### Here, we are going to tune the baseline model to boost the model.","1731452e":"## Tuning parameters\nMany strategies exist on how to tune parameters. Most data scientist see **number of trees, tree depth and the learning rate** as most crucial parameters.\n* **Number of trees**: A high number of trees can be computationally expensive. Generally, with a change in learning rate,n_estimators should also be adjusted (10-fold decrease in learning_rate should go in line with a approx. 10-fold increase in n_estimators.\n* **Learning rate**: Determines the contribution of each tree on the final outcome and controls how quickly the algorithm proceeds down the gradient descent (learns). This hyperparameter is also called shrinkage. Generally, the smaller this value, the more accurate the model can be but also will require more trees in the sequence.\n* **Tree depth**: Controls the depth of the individual trees. Typical values range from a depth of 3\u20138 but it is not uncommon to see a tree depth of 1.\n\nParameter tuning is a crucial task in finding the model with the highest predictive power. The code below how to tune parameters in a gradient boosting model for classification.\n\nIt's hyperparameter tuning time. First, we need to define a dictionary of GBC parameters for the grid search.","20bf7f99":"### Here, we are going to tune the baseline model to boost the model.","83e2e791":"## Scatter matrix\n> A scatter plot matrix is a grid (or matrix) of scatter plots used to visualize bivariate relationships between combinations of variables. Each scatter plot in the matrix visualizes the relationship between a pair of variables, allowing many relationships to be explored in one chart. ","be562738":"## Model Performance Analysis","0ae88792":"# Scaling the data\n> Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model. Scaling can make a difference between a weak machine learning model and a better one. The most common techniques of feature scaling are Normalization and Standardization. Normalization is used when we want to bound our values between two numbers, typically, between [0,1] or [-1,1]. While Standardization transforms the data to have zero mean and a variance of 1, they make our data unitless. Refer to the below diagram, which shows how data looks after scaling in the X-Y plane.\n\n![](https:\/\/pariaagharabi.github.io\/images\/fetal-health-image.png)\n\n*NOTE*: \n> To learn more about scaling techniques: [](http:\/\/)https:\/\/towardsdatascience.com","85dea7bb":"**DataFrame.info()** method prints a concise summary of a DataFrame.This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.","feaf8eb4":"## Model Selection\n","70ec18ce":"<br><br>","39bdb708":"## Analyze and visualize the target column (fetal_health)","cbf286c4":"The results of the model selection phase are summarized in Table above. The Random Forest with 0.99 score has high percentage among models. Logistic Regression has lowest score (0.90).","0f457c71":"We use the **describe() method** for our target column to show the descriptive statistics include those that summarize the central tendency, dispersion, and shape of a dataset\u2019s distribution, excluding NaN values.","ce7f0f7b":"## Plotting the learning curve\n* Learning curves are plots that show changes in learning performance over time in terms of experience.\n* Learning curves of model performance on the train and validation datasets can be used to diagnose an underfit, overfit, or well-fit model.\n* Learning curves of model performance can be used to diagnose whether the train or validation datasets are not relatively representative of the problem domain.","ad809612":"## Exploratory Data Analysis (EDA) and Analysis\nIn this step we want to get basic information about the data types, columns, null value counts, memory usage, etc. EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.","509573ec":"## Count the missing and null values\nHere is good to count the **missing** and **null** values.\nIn the case of a real-world dataset, it is very common that some values in the dataset are missing. We represent these missing values as NaN (Not a Number) values. But to build a good machine learning model our dataset should be complete. That\u2019s why we use some imputation techniques to replace the NaN values with some probable values.","272ac876":"## Heatmap\n> A heat map is a two-dimensional representation of information with the help of colors. Heat maps can help the user visualize simple or complex information. Correlation heatmaps are ideal for comparing the measurement for each pair of dimension values.","393680d5":"*Thank you for opening this kernel!*\n\n\n# Predicting Fetal Health Classification:\n> In this work, we use machine learning for the prediction of fetal health to prevent child and maternal mortality.\n\n## PROJECT CONTENT:\n* Import Necessary Libraries\n* Data Exploration\/ Analysis\/ Visualization\n* Correlation & Correlation Matrix\n* Predictive Modeling \n* Confusion Matrix\n* Precision and Recall\n* Hyperparameters Tuning\n\n## Goal:\n> The goal of this competition is to predict Fetal Health. We will practice Classification Algorithms to achieve the lowest prediction error.\n\n## Machine learning methods:\nI have applied consolidated methodologies to identify the most suitable machine learning model for the task from a pool of candidate methodologies.\nI have taken into consideration a pool of four state-of-the-art machine learning models, that are briefly reviewed in the following:\n* **Logistic Regression (LR)**: is the baseline model in this Kernel.\n\n**NOTE: The key advantages of LR are its simplicity, the scalability to very large datasets and the interpretation it provides in terms of how unitary changes in an input feature influence the log-odds of the associated linear parameter. **\n\n* **K-nearest neighbors (KNN)**: k-Nearest Neighbor is a memory-based model, where predictions are performed by the similarity of the current sample to k nearest elements in the training set, according to the given distance metric. \n\n**NOTE: The key advantage of this method lies in its sheer simplicity, compensated by the difficulties in robustly determining the most appropriate similarity function as well as the choice of the k meta-parameter. **\n\n* **Random Forest (RF)**: is a type of ensemble methods in which multiple learning models are combined together to improve generalization.\n\n* **Gradient Boosting Machine (GBM)**: is another ensemble method combining a series of weak learners to obtain a stronger predictor.\n\nThe **rationale** behind this selection of candidate models was to provide reasonable coverage of different methodologies, to achieve the lowest prediction error.","a29f67cb":"Base on the result above, after tuning our model (LR), We could boost the model just a little bit. So we keep going with other models.","f63a6e30":"## Visualize missing values (NaN) using Missingno Library:\nWe use **Missingno library** which offers a very nice way to visualize the distribution of NaN values. Missingno is a Python library and compatible with Pandas.\n ","40d9e9ec":"* **Confusion Matrix**:\nThe confusion matrix is a technique used for summarizing the performance of a classification algorithm i.e. it has binary outputs.\n","d8703d6b":"Thank you for taking the time to read through my kernel. For the moment, let me know if you found this notebook useful or you just liked it: I would really appreciate it!","41cb7472":"<br><br>","7a4e18c1":"## K-Nearest Neighbors (KNN)","a88e6a99":"## The important things we could learn about the above plot is Skewness.  Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. There are three types of skewed distributions. A right (or positive) skewed distribution, left (or negative) skewed distribution, and normal distribution.\n* A left-skewed distribution has a long left tail. Left-skewed distributions are also called negatively-skewed distributions. That\u2019s because there is a long tail in the negative direction on the number line. The mean is also to the left of the peak.\n* A right-skewed distribution has a long right tail. Right-skewed distributions are also called positive-skew distributions. That\u2019s because there is a long tail in the positive direction on the number line. The mean is also to the right of the peak.\n* The skewness for a normal distribution is zero and looks a bell curve.","ee90b8be":"## Random Forest (RF)","39b4566e":"## Building Machine Learning Models:\n1. Logistic Regression (LR)\n2. K-nearest neighbors (KNN)\n3. Random Forest (RF)\n4. Gradient Boosting Machine (GBM)\n\nNOTE: To improve all scores for each ML model, we want to search the set of \"hyperparameters\" by using the common approach \"Grid search\" for four models above. \n**Hyper-parameters** are a set of additional, model-dependent parameters that are not inferred automatically by the learning algorithm but need to be specified before the learning phase: a common example of **hyper-parameter** is the value of k in k-Nearest Neighbor or the number of hidden units in a Neural Network. \nHence, finding sub-optimal values of the hyper-parameters is crucial to ensure proper generalization. *The hyper-parameter optimization procedure*, which was repeated separately for each candidate learning methodology, encompassed the following steps: \n* Firstly, a set of suitable hyper-parameters to optimize was identified; for each of them, a range of candidate values was specified. These choices are dependent both on our expertise and on the computational cost needed to train the models. \n* Secondly, a predictor was learned for all the possible combinations of hyper-parameters and its out-of-sample performance was estimated using 5-fold Cross Validation (CV), i.e. \nWe trained the model with 70% of the total training set size and validated its performance in the remaining 30%. \n\n**GridSearch** exhaustively searches through all possible combinations of hyperparameters during training the phase. Before we proceed further, we shall cover another cross-validation (CV) methods since tuning hyperparameters via grid search is usually cross-validated to avoid overfitting.\nHence, For accelerating the running GridSearchCV we set: n-splits=3, n_jobs=2.","3368110b":"* **Classification Report:** Report which includes Precision, Recall and F1-Score.\n\n\n1. **Precision** - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n\n    Precision = TP\/TP+FP\n\n2. **Recall (Sensitivity)** - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. \n\n    Recall = TP\/TP+FN    \n\n3. **F1 score** - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it\u2019s better to look at both Precision and Recall.\n\n    F1 Score = 2(Recall Precision) \/ (Recall + Precision)","e9fc8608":"<br><br>","46ba973f":"<br><br>","a7057763":"## Bar Chart :\nThis bar chart gives us an idea about how many missing values are there in each column. ","72d6eb05":"## The **principle** of this challenge is: \n> Classify fetal health in order to prevent child and maternal mortality. So get the info about the column of \"fetal_health\" which were classified by three expert obstetritians into 3 classes:\n\n* Normal\n* Suspect\n* Pathological\n\nTo do so, we're gonna **analyze and visualize** the target column (fetal_health).","508c9db0":"## Result Visualisation of the learning curve","a40e61f7":"## Logistic Regression (LR)","25ee53be":"## Import Necessary Libraries and Data Sets."}}