{"cell_type":{"9c38d4f3":"code","1976eab2":"code","491abc1d":"code","ee469dad":"code","102d380c":"code","67324325":"code","d3fd5659":"code","7f6c27a8":"code","9b29bd41":"code","448a9738":"code","86de39fb":"code","d412696e":"code","f32798bf":"code","cbb97512":"code","37cf3c90":"code","3b926ff1":"code","39f7abee":"code","b678d46b":"code","dca0bb17":"code","cd0abbd0":"code","b8d2faeb":"code","52680aa6":"code","97e560d2":"code","c302257e":"code","cdd9171c":"code","32516296":"code","73694dbe":"code","301dbe97":"markdown","8cd42eb2":"markdown","8a77c23a":"markdown","e5644a82":"markdown","df02cb25":"markdown","defb9978":"markdown","0f0e6153":"markdown","208ef008":"markdown","23cc9efb":"markdown","10e1b9f5":"markdown","2ec2a22d":"markdown","e23bb31a":"markdown","73318232":"markdown","9504629c":"markdown","a147c9c1":"markdown","374a7e24":"markdown","e8cb4f47":"markdown","a23c40b4":"markdown"},"source":{"9c38d4f3":"# Load Libraries\nfrom __future__ import unicode_literals, print_function, division\nfrom io import open\nimport unicodedata\nimport string\nimport re\nimport random\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nimport torch.nn.functional as F\nimport os","1976eab2":"# Confirmed Global Variables","491abc1d":"# Draft Global Variables","ee469dad":"# Kaggle Utils\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","102d380c":"# Confirmed Classes","67324325":"# Confirmed Functions","d3fd5659":"# Other Confirmed Programmes\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","7f6c27a8":"SOS_token = 0 #Start of sentence\nEOS_token = 1 #End of sentence\n\n\nclass Lang:\n    def __init__(self, name):\n        #create vocabulary dictionary\n        self.name = name\n        self.word2index = {}\n        self.word2count = {}\n        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n        self.n_words = 2\n\n    def addSentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addWord(word)\n\n    def addWord(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.n_words\n            self.word2count[word] = 1\n            self.index2word[self.n_words] = word\n            self.n_words += 1\n        else:\n            self.word2count[word] += 1","9b29bd41":"# Turn a Unicode string to plain ASCII, thanks to\n# https:\/\/stackoverflow.com\/a\/518232\/2809427\ndef unicodeToAscii(s):\n    return ''.join(\n        c for c in unicodedata.normalize('NFD', s)\n        if unicodedata.category(c) != 'Mn'\n    )\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    return s","448a9738":"def readLangs(direc, reverse=False):\n    print(\"Reading lines...\")\n\n    # Read the file and split into lines\n    lines = open(direc, encoding='utf-8').\\\n        read().strip().split('\\n')\n\n    # Split every line into pairs and normalize\n    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n\n    # Reverse pairs, make Lang instances\n    \"\"\"\n    if reverse:\n        pairs = [list(reversed(p)) for p in pairs]\n        input_lang = Lang(lang2)\n        output_lang = Lang(lang1)\n    else:\n    \"\"\"\n    input_lang = Lang(\"question\")\n    output_lang = Lang(\"answer\")\n\n    return input_lang, output_lang, pairs","86de39fb":"MAX_LENGTH = 30\n\n\"\"\"\neng_prefixes = (\n    \"i am \", \"i m \",\n    \"he is\", \"he s \",\n    \"she is\", \"she s \",\n    \"you are\", \"you re \",\n    \"we are\", \"we re \",\n    \"they are\", \"they re \"\n)\n\"\"\"\n\n\ndef filterPair(p):\n    return len(p[0].split(' ')) < MAX_LENGTH and \\\n        len(p[1].split(' ')) < MAX_LENGTH\n\ndef filterPairs(pairs):\n    return [pair for pair in pairs if filterPair(pair)]","d412696e":"def prepareData(direc, reverse=False):\n    input_lang, output_lang, pairs = readLangs(direc, reverse)\n    print(\"Read %s sentence pairs\" % len(pairs))\n    pairs = filterPairs(pairs)\n    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n    print(\"Counting words...\")\n    for pair in pairs:\n        input_lang.addSentence(pair[0])\n        output_lang.addSentence(pair[1])\n    print(\"Counted words:\")\n    print(input_lang.name, input_lang.n_words)\n    print(output_lang.name, output_lang.n_words)\n    return input_lang, output_lang, pairs\n\n\ninput_lang, output_lang, pairs = prepareData('..\/input\/questionanswer\/question-answer.txt', False)\nprint(random.choice(pairs))","f32798bf":"class EncoderRNN(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(EncoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n\n    def forward(self, input, hidden):\n        embedded = self.embedding(input).view(1, 1, -1)\n        output = embedded\n        output, hidden = self.gru(output, hidden)\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","cbb97512":"class DecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size):\n        super(DecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.gru = nn.GRU(hidden_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n        self.softmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, input, hidden):\n        output = self.embedding(input).view(1, 1, -1)\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n        output = self.softmax(self.out(output[0]))\n        return output, hidden\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","37cf3c90":"class AttnDecoderRNN(nn.Module):\n    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n        super(AttnDecoderRNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.output_size = output_size\n        self.dropout_p = dropout_p\n        self.max_length = max_length\n\n        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n        self.dropout = nn.Dropout(self.dropout_p)\n        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n        self.out = nn.Linear(self.hidden_size, self.output_size)\n\n    def forward(self, input, hidden, encoder_outputs):\n        embedded = self.embedding(input).view(1, 1, -1)\n        embedded = self.dropout(embedded)\n\n        attn_weights = F.softmax(\n            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n                                 encoder_outputs.unsqueeze(0))\n\n        output = torch.cat((embedded[0], attn_applied[0]), 1)\n        output = self.attn_combine(output).unsqueeze(0)\n\n        output = F.relu(output)\n        output, hidden = self.gru(output, hidden)\n\n        output = F.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden, attn_weights\n\n    def initHidden(self):\n        return torch.zeros(1, 1, self.hidden_size, device=device)","3b926ff1":"def indexesFromSentence(lang, sentence):\n    return [lang.word2index[word] for word in sentence.split(' ')]\n\n\ndef tensorFromSentence(lang, sentence):\n    indexes = indexesFromSentence(lang, sentence)\n    indexes.append(EOS_token)\n    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n\n\ndef tensorsFromPair(pair):\n    input_tensor = tensorFromSentence(input_lang, pair[0])\n    target_tensor = tensorFromSentence(output_lang, pair[1])\n    return (input_tensor, target_tensor)","39f7abee":"teacher_forcing_ratio = 0.5\n\n\ndef train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n    encoder_hidden = encoder.initHidden()\n\n    encoder_optimizer.zero_grad()\n    decoder_optimizer.zero_grad()\n\n    input_length = input_tensor.size(0)\n    target_length = target_tensor.size(0)\n\n    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n    loss = 0\n\n    for ei in range(input_length):\n        encoder_output, encoder_hidden = encoder(\n            input_tensor[ei], encoder_hidden)\n        encoder_outputs[ei] = encoder_output[0, 0]\n\n    decoder_input = torch.tensor([[SOS_token]], device=device)\n\n    decoder_hidden = encoder_hidden\n\n    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n\n    if use_teacher_forcing:\n        # Teacher forcing: Feed the target as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            loss += criterion(decoder_output, target_tensor[di])\n            decoder_input = target_tensor[di]  # Teacher forcing\n\n    else:\n        # Without teacher forcing: use its own predictions as the next input\n        for di in range(target_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            topv, topi = decoder_output.topk(1)\n            decoder_input = topi.squeeze().detach()  # detach from history as input\n\n            loss += criterion(decoder_output, target_tensor[di])\n            if decoder_input.item() == EOS_token:\n                break\n\n    loss.backward()\n\n    encoder_optimizer.step()\n    decoder_optimizer.step()\n\n    return loss.item() \/ target_length","b678d46b":"import time\nimport math\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))","dca0bb17":"def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n    start = time.time()\n    plot_losses = []\n    print_loss_total = 0  # Reset every print_every\n    plot_loss_total = 0  # Reset every plot_every\n\n    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n    training_pairs = [tensorsFromPair(random.choice(pairs))\n                      for i in range(n_iters)]\n    criterion = nn.NLLLoss()\n\n    for iter in range(1, n_iters + 1):\n        training_pair = training_pairs[iter - 1]\n        input_tensor = training_pair[0]\n        target_tensor = training_pair[1]\n\n        loss = train(input_tensor, target_tensor, encoder,\n                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n        print_loss_total += loss\n        plot_loss_total += loss\n\n        if iter % print_every == 0:\n            print_loss_avg = print_loss_total \/ print_every\n            print_loss_total = 0\n            print('%s (%d %d%%) %.4f' % (timeSince(start, iter \/ n_iters),\n                                         iter, iter \/ n_iters * 100, print_loss_avg))\n\n        if iter % plot_every == 0:\n            plot_loss_avg = plot_loss_total \/ plot_every\n            plot_losses.append(plot_loss_avg)\n            plot_loss_total = 0\n\n    showPlot(plot_losses)\n    plt.savefig(\"loss.png\")","cd0abbd0":"import matplotlib.pyplot as plt\nplt.switch_backend('agg')\nimport matplotlib.ticker as ticker\nimport numpy as np\n\n\ndef showPlot(points):\n    plt.figure()\n    fig, ax = plt.subplots()\n    # this locator puts ticks at regular intervals\n    loc = ticker.MultipleLocator(base=0.2)\n    ax.yaxis.set_major_locator(loc)\n    plt.plot(points)","b8d2faeb":"def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n    with torch.no_grad():\n        input_tensor = tensorFromSentence(input_lang, sentence)\n        input_length = input_tensor.size()[0]\n        encoder_hidden = encoder.initHidden()\n\n        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n\n        for ei in range(input_length):\n            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n                                                     encoder_hidden)\n            encoder_outputs[ei] += encoder_output[0, 0]\n\n        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n\n        decoder_hidden = encoder_hidden\n\n        decoded_words = []\n        decoder_attentions = torch.zeros(max_length, max_length)\n\n        for di in range(max_length):\n            decoder_output, decoder_hidden, decoder_attention = decoder(\n                decoder_input, decoder_hidden, encoder_outputs)\n            decoder_attentions[di] = decoder_attention.data\n            topv, topi = decoder_output.data.topk(1)\n            if topi.item() == EOS_token:\n                decoded_words.append('<EOS>')\n                break\n            else:\n                decoded_words.append(output_lang.index2word[topi.item()])\n\n            decoder_input = topi.squeeze().detach()\n\n        return decoded_words, decoder_attentions[:di + 1]","52680aa6":"def evaluateRandomly(encoder, decoder, n=10):\n    for i in range(n):\n        pair = random.choice(pairs)\n        print('>', pair[0])\n        print('=', pair[1])\n        output_words, attentions = evaluate(encoder, decoder, pair[0])\n        output_sentence = ' '.join(output_words)\n        print('<', output_sentence)\n        print('')","97e560d2":"hidden_size = 256\nencoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\nattn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n\ntrainIters(encoder1, attn_decoder1, 75000, print_every=5000)","c302257e":"evaluateRandomly(encoder1, attn_decoder1)","cdd9171c":"print(normalizeString(\"Good morning, how are you?\"))\noutput_words, attentions = evaluate(\n    encoder1, attn_decoder1, normalizeString(\"Good morning, how are you?\"))\nplt.matshow(attentions.numpy())\nplt.savefig(\"attentions.png\")","32516296":"def showAttention(input_sentence, output_words, attentions):\n    # Set up figure with colorbar\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    cax = ax.matshow(attentions.numpy(), cmap='bone')\n    fig.colorbar(cax)\n\n    # Set up axes\n    ax.set_xticklabels([''] + input_sentence.split(' ') +\n                       ['<EOS>'], rotation=90)\n    ax.set_yticklabels([''] + output_words)\n\n    # Show label at every tick\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n\n    plt.show()\n    plt.savefig(str(input_sentence)+\".png\")\n\n\ndef evaluateAndShowAttention(input_sentence):\n    output_words, attentions = evaluate(\n        encoder1, attn_decoder1, input_sentence)\n    print('input =', input_sentence)\n    print('output =', ' '.join(output_words))\n    showAttention(input_sentence, output_words, attentions)\n\n\nevaluateAndShowAttention(\"good morning\")\n\nevaluateAndShowAttention(\"how are you\")\n\nevaluateAndShowAttention(\"what do you like\")\n\nevaluateAndShowAttention(\"i guess you are bad\")","73694dbe":"# \u5904\u7406\u6570\u636e\npos = \"..\/input\/conversation-json\/conversation.json\"\nimport json\ndata = None\nwith open(pos, \"r\") as fr:\n    rd = fr.read()\n    data = json.loads(rd)\ndata = data['conversations']\nnewdata = []\nwith open(\".\/question-answer.txt\", \"w\") as fw:\n    for dataa in data:\n        for i in range(len(dataa)-1):\n            j = i + 1\n            fw.write(dataa[i]+\"\\t\"+dataa[j]+\"\\n\")","301dbe97":"### KevJ12\u59dc\u5955\u8fb0\n\u5206\u5de5\uff1a","8cd42eb2":"### chenykfrank\u9648\u715c\u5764\n\u5206\u5de5\uff1a","8a77c23a":"## V. BUG\u53ca\u7591\u95ee\u5217\u8868","e5644a82":"### Lazenander Yang\u6768\u6dcf\u7136\n\u5206\u5de5\uff1a","df02cb25":"## \u91cd\u8981\u901a\u77e5\u5217\u8868","defb9978":"### \u516c\u5171\u4ee3\u7801\u57572\uff1a\u5168\u5c40\u5e38\u91cf\n\u8bf7\u5c06\u6240\u6709\u7684\u5168\u5c40\u5e38\u91cf\u5199\u5728Draft Global Variables\u4ee3\u7801\u5757\u5904\uff08\u610f\u56fe\u662f\u5b58\u653e\u4e00\u4e9b\u5e38\u91cf\u4f5c\u4e3a\u6a21\u578b\u8bad\u7ec3\u7684\u53c2\u6570\uff0c\u6bd4\u5982epoches=8\uff0cbatch=256\u4e4b\u7c7b\u7684\uff09\u5e76\u52a1\u5fc5**\u5199\u597d\u884c\u6ce8\u91ca**\uff0c\u5df2\u786e\u8ba4\u7684\u53d8\u91cf\u4f1a\u88ab\u79fb\u5230Confirmed Global Variables","0f0e6153":"## IV. \u4e2a\u4eba\u4ee3\u7801\u5757","208ef008":"### RamboXia\u590f\u5ec9\u535a\n\u5206\u5de5\uff1a","23cc9efb":"## I. \u9879\u76ee\u6587\u6863\n### \u4efb\u52a1\u6982\u8ff0\n\uff08\u6682\u5b9a\uff0c\u968f\u65f6\u4fee\u6539\uff09\n\u8fd9\u662fComputerization\u793e\u56e2\uff08\u4fe1\u606f\u5316\u793e\uff09AI\u4eba\u5de5\u667a\u80fd\u90e8\u95e8\u4e8e2020\u5e749\u67081\u65e5\u5f00\u59cb\u5f00\u5c55\u7684\u5927\u578b\u9879\u76ee\uff0c\u9700\u8981\u5168\u4f53\u793e\u5458\u7684\u914d\u5408\u3002  \n\u672c\u9879\u76ee\u5173\u6ce8\u4e2d\u56fd\u5883\u5185\u5b64\u5be1\u8001\u4eba\u7b49\u8001\u5e74\u7fa4\u4f53\u65e0\u4eba\u966a\u4f34\u3001\u65e0\u4eba\u4ea4\u6d41\u6240\u4ea7\u751f\u7684\u5b64\u72ec\u95ee\u9898\uff0c\n\u65e8\u5728\u901a\u8fc7\u4eba\u5de5\u667a\u80fd**\u8bed\u97f3\u8bc6\u522b**\u3001**\u81ea\u7136\u8bed\u8a00\u5904\u7406**\u3001**\u5f3a\u5316\u5b66\u4e60**\u7b49\u6a21\u578b\u5efa\u7acb\u4e00\u4e2a\u804a\u5929\u673a\u5668\u4eba\uff0c  \n\u4f7f\u5176\uff1a\n1. \u80fd\u591f**\u6a21\u4eff\u8001\u5e74\u4eba\u7684\u53e3\u543b**\u4e0e\u8001\u5e74\u4eba\u4ea4\u6d41\uff0c\u5b9e\u73b0\u8fd9\u4e2a\u76ee\u6807\u6709\u4e24\u4e2a\u65b9\u5f0f\uff1a  \n   a) \u4e00\u5f00\u59cb\u5bfb\u627e\u6570\u636e\u65f6\u627e\u4e0e\u8001\u4eba\u6709\u5173\u7684\u6570\u636e\uff08\u8fd9\u4e2a\u601d\u8def\u66f4\u6613\u884c\uff0c\u4f46\u9700\u8981\u8001\u4eba\u6709\u5173\u7684\u5927\u91cf\u6570\u636e\u96c6\uff09  \n   b) \u8bad\u7ec3\u4e00\u4e2a\u901a\u7528\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u98ce\u683c\u8fc1\u79fb\u3001\u5f3a\u5316\u5b66\u4e60\u4f7f\u5176\u6a21\u4eff\u8001\u4eba\u7684\u53e3\u543b\uff08\u8fd9\u4e2a\u601d\u8def\u66f4\u96be\uff0c\u4f46\u662f\u6240\u9700\u6570\u636e\u5c11\uff0c\u4e14\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff09\n2. \u80fd\u591f\u5173\u6ce8\u8001\u5e74\u4eba\u7684**\u65e5\u5e38\u5fc5\u8981\u4e8b\u5b9c**\uff0c\u5982\u4e70\u836f\u3001\u770b\u533b\u751f\u7b49\uff0c\u53ef\u4ee5\u7b80\u5355\u7684\u901a\u8fc7\u8bb0\u5f55\u548c\u679a\u4e3e\u6240\u9700\u4efb\u52a1\u5b9e\u73b0\n3. \u80fd\u591f\u5728\u4e0e\u8001\u5e74\u4eba\u7684\u9010\u6b65\u4ea4\u6d41\u4e2d\u66f4\u52a0**\u7ec6\u8282\u5730\u5f62\u6210\u4e0e\u4e0e\u5176\u5bf9\u8bdd\u8001\u5e74\u4eba\u53e3\u543b\u76f8\u7b26\u5408\u3001\u804a\u5929\u65b9\u5411\u7edf\u4e00\u7684\u804a\u5929\u6a21\u5f0f**\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a**\u8bb0\u5fc6\u6a21\u578b**\uff0c\u4f7f\u5f97\u673a\u5668\u4eba\u53ef\u4ee5\u63a7\u5236\u8bb0\u4f4f\u54ea\u4e9b\u91cd\u8981\u7684\u4fe1\u606f\uff0c\u4ece\u800c\u5c06\u5176\u89c6\u4e3a\u672a\u6765\u4e0e\u8001\u5e74\u4eba\u804a\u5929\u7684**\u7d20\u6750**\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0  \n\n\u8fd9\u662f\u4e00\u4e2a\u957f\u671f\u7684\u9879\u76ee\uff0c\u53ef\u4ee5\u8003\u8651\u6cbf\u793e\u56e2\u4f20\u627f\u4e0b\u53bb\uff0c\u6211\u4eec\u8ba4\u4e3a\u5408\u7406\u7684\u6a21\u5f0f\u662f\uff1a\u52a0\u901f10\u5e74\u7ea7\u7684\u77e5\u8bc6\u50a8\u5907\u8fc7\u7a0b\u3001\u51cf\u5c11\u5bf9\u77e5\u8bc6\u539f\u7406\u7684\u8981\u6c42\u3001\u589e\u5f3a\u5bf9\u5b9e\u8df5\u7684\u8981\u6c42\uff1b10\u5e74\u7ea7\u4e0b\u534a\u5b66\u671f10\u5e74\u7ea7\u52a0\u5165\u5f00\u53d1\uff1b11\u5e74\u7ea7\u6574\u4e2a\u5b66\u671f\u4fdd\u6301\u5f00\u53d1\uff0c\u5e76\u5b9a\u671f\u5199\u62a5\u544a\u548c\u505acode review\u5de5\u4f5c\uff0c\u540c\u65f6\u5728\u6574\u4e2a\u4ee3\u7801\u5f00\u53d1\u8fc7\u7a0b\u505a\u597d\u6ce8\u91ca\uff0c\u4fdd\u6301code style\u7684\u4e00\u81f4\u3002  \n12\u5e74\u7ea7\u539f\u5219\u4e0a\u4e0d\u8fdb\u884c\u65b0\u7684\u5f00\u53d1\u5de5\u4f5c\uff0c\u4f46\u76f4\u5230\u81ea\u5df1\u7684\u4ee3\u7801\u5b8c\u5168\u4ea4\u63a5\u524d\u4ecd\u9700\u8981\u5bf9\u4e4b\u524d\u81ea\u5df1\u5199\u7684bug\u548c\u4ee3\u7801\u90e8\u5206\u8d1f\u8d23  \n\n\u6574\u4e2a\u9879\u76ee\u53ef\u8bb0C\u3001S\u65f6\u95f4\uff0c\u4f46\u8bb0S\u9700\u8981\u4fdd\u8bc1\u5b9a\u671f\u91c7\u8bbf\u5b64\u5be1\u8001\u4eba\uff0c\u4e0e\u4ed6\u4eec\u4ea4\u6d41\u83b7\u53d6\u4ed6\u4eec\u60f3\u8981\u7684\u529f\u80fd\u4e0e\u4ed6\u4eec\u7684\u60f3\u6cd5\n\n### \u5206\u5de5\n\u6bcf\u4e00\u5c4a**\u4e3b\u8d1f\u8d23**\u7531AI\u90e8\u8d1f\u8d23\u4eba\u62c5\u4efb\uff08\u56e0\u6b64\u672c\u4efb\u52a1\u7684\u5b8c\u6210\u60c5\u51b5\u5c06\u6210\u4e3a\u8d1f\u8d23\u4eba\u9009\u62d4\u7684\u91cd\u8981\u6807\u51c6\u4e4b\u4e00\uff09\uff0c**\u526f\u8d1f\u8d23**\u7531\u4e3b\u8d1f\u8d23\u4eba\u6311\u9009\uff0c\u9700\u8981\u5bf9AI\u6a21\u578b\u548c\u4ee3\u7801\u90fd\u6709\u8db3\u591f\u7684\u4e86\u89e3\u624d\u53ef\u62c5\u5f53  \n\u4e3b\u8d1f\u8d23\u4eba\u7684\u6838\u5fc3\u5de5\u4f5c\u5982\u4e0b\uff1a\u8d44\u91d1\u7ba1\u7406\u53ca\u7533\u62a5\u3001\u5206\u5de5\u8c03\u6574\u3001\u8fdb\u5ea6\u63a7\u5236\u3001\u6a21\u578b\u6846\u67b6\u7684\u786e\u7acb\u4e0e\u8c03\u6574\u3001\u5b9a\u671f\u5f00\u4f1a\u300110\u5e74\u7ea7\u6388\u8bfe\u4efb\u52a1\u5206\u914d  \n\u526f\u8d1f\u8d23\u4eba\u9700\u8f85\u52a9\u603b\u8d1f\u8d23\u4eba\u5b8c\u6210\u4e0a\u8ff0\u4efb\u52a1\u3001\u5e76\u8fdb\u884c\u4f1a\u8bae\u8bb0\u5f55\u3001\u8d1f\u8d23\u4e0e\u5b64\u5be1\u8001\u4eba\u7684\u8054\u7cfb\u7ef4\u6301\u5de5\u4f5c\u3001\u670d\u52a1\u5668\u8d1f\u8d23  \n\u4e3b\u3001\u526f\u8d1f\u8d23\u4eba\u540c\u6837\u9700\u8981\u64b0\u5199\u4ee3\u7801\u548c\u5b8c\u6210\u5176\u5b83\u5de5\u4f5c\uff0c\u7531\u5177\u4f53\u7684\u5206\u5de5\u6765\u51b3\u5b9a  \n\uff08\u6682\u5b9a\uff0c\u7ec6\u8282\u5f85\u8ba1\u5212\u4e0e\u6a21\u578b\u6846\u67b6\u521d\u7a3f\u786e\u7acb\u540e\u518d\u52a0\u8fdb\u53bb\uff09  \n\u6a21\u578b\u4ee3\u7801\u7f16\u5199\u7684\u7ec6\u5316\uff1axx\u6a21\u578b\u9884\u8bad\u7ec3\u3001\u8bad\u7ec3\u3001\u6d4b\u8bd5\u3001\u753b\u56fe  \n\u5176\u5b83\u6280\u672f\u8d1f\u8d23\u7684\u7ec6\u5316\uff1a\u5e94\u7528\u5f00\u53d1\u3001\u7f51\u7ad9\u5f00\u53d1\u3001\u673a\u5668\u4eba\u642d\u5efa\u3001\u670d\u52a1\u5668\u8d1f\u8d23  \n\u5176\u5b83\u5206\u5de5\u7684\u7ec6\u5316\uff1axx\u5c4a\u4e3b\u8d1f\u8d23\u3001\u526f\u8d1f\u8d23\u3001\u6570\u636e\u641c\u96c6\u3001LOGO\u8bbe\u8ba1\u3001UI\u8bbe\u8ba1\u3001\u4e0e\u8001\u4eba\u8bbf\u8c08\uff08\u7b80\u8bb0\u4e3a\u8bbf\u8c08\uff09\n\u5177\u4f53\u5206\u5de5\u5728\u6bcf\u4eba\u4e2a\u4eba\u4ee3\u7801\u5757\u4e0b\u65b9\u6807\u51fa\n\n### \u65f6\u95f4\u8282\u70b9\n\uff08\u6682\u5b9a\uff09  \n1. \u671f\u4e2d\u8003\u540e\u4e00\u5468\uff1a\u5b8c\u6210\u77e5\u8bc6\u50a8\u5907\u3001\u786e\u7acb\u521d\u7a3f\u6a21\u578b\u4efb\u52a1\u3001\u5f00\u4f1a\u5468\u671f\u3001\u5206\u5de5\u5b89\u6392\n2. \u6708\u8003\u524d\u540e\uff1a\u5b8c\u6210\u6570\u636e\u91c7\u96c6\u53ca\u521d\u6b65\u7684\u539f\u578b\u642d\u5efa\n3. \u671f\u672b\u8003\u524d\uff1a\u5b8c\u6210\u4fee\u6539\uff0c\u5e76\u8bbf\u8c08\u548c\u8fdb\u884c\u6d4b\u8bd5\n4. \u5f00\u5b66\u524d\uff1a\u5b8c\u6210\u6700\u7ec8\u7248\u7684\u6a21\u578b\u642d\u5efa\uff0c\u4e3b\u8d1f\u8d23\u4eba\u3001\u526f\u8d1f\u8d23\u4eba\u4ea4\u63a5\n\n### \u4f7f\u7528\u987b\u77e5\n1. \u4efb\u4f55\u65f6\u5019\u4e0d\u8981\u4f7f\u7528Run All\u6307\u4ee4\uff0c\u4f7f\u7528\u4e86\u5bb9\u6613\u6709\u4ee3\u7801\u51b2\u7a81\uff0c\u5982\u82e5\u51fa\u73b0\u4e86\u83ab\u540d\u5176\u5999\u7684bug\u53ef\u80fd\u662f\u8fd9\u7c7b\u95ee\u9898\uff0c\u53ef\u4ee5\u91cd\u65b0\u542f\u52a8kernel\u540e\u7ee7\u7eed\n2. \u8bf7\u9632\u6b62\u4e0e\u522b\u7684\u540c\u5b66\u7684\u53d8\u91cf\u540d\u3001\u51fd\u6570\u540d\u6216\u7c7b\u540d\u76f8\u51b2\u7a81\uff0c\u82e5\u51b2\u7a81\u8bf7\u53ca\u65f6\u66f4\u6362\uff1b\u5168\u5c40\u53d8\u91cf\u5efa\u8bae\u5199\u5728\u516c\u5171\u4ee3\u7801\u57572\u4e2d\n3. \u591a\u5199\u6ce8\u91ca\uff0c\u5305\u62ec\u6587\u6863\u6ce8\u91ca\u548c\u9010\u884c\u6ce8\u91ca\n4. \u4f7f\u7528PyTorch\u5e93\u8fdb\u884c\u5f00\u53d1\uff0c\u4e0d\u5141\u8bb8\u4f7f\u7528Tensorflow\n5. \u4fdd\u6301\u4ee3\u7801\u98ce\u683c\u7684\u7edf\u4e00\n6. \u5bfc\u5165\u5e93\u7684\u8fc7\u7a0b\u8bf7\u5199\u5230\u516c\u5171\u4ee3\u7801\u57571\u4e2d\uff0c\u5982\u6240\u9700\u5e93\u672a\u5b89\u88c5\u53ef\u4f7f\u7528!pip install xxx\u7684\u6307\u4ee4\u8fdb\u884c\u5b89\u88c5\uff0c\u5b89\u88c5\u597d\u540e\u8bf7\u5220\u9664\u611f\u53f9\u53f7\u5f00\u5934\u7684\u4ee3\u7801\n7. \u9664\u5bfc\u5165\u5e93\u53ca\u5168\u5c40\u53d8\u91cf\u53ef\u4ee5\u5728\u516c\u5171\u4ee3\u7801\u5757\u4e2d\u7f16\u5199\uff0c\u5176\u4f59\u4e00\u5f8b\u53ea\u80fd\u5728\u4e2a\u4eba\u7684\u4ee3\u7801\u5757\u4e2d\u7f16\u5199\uff08\u53ef\u4ee5\u81ea\u884c\u6dfb\u52a0\u591a\u4e2a\u4ee3\u7801\u5757\uff09\uff0c\u4e14\u4e25\u7981\u5199\u5230\u5df2\u786e\u8ba4\u4ee3\u7801\u4e2d\n8. \u5feb\u6377\u952e\uff1aCtrl+Enter\u8fd0\u884c\uff0cb\u65b0\u5efa\u4ee3\u7801\u5757\uff0center\u8fdb\u5165\u9009\u4e2d\u7684\u4ee3\u7801\u5757\uff0cdd\u5220\u9664\u4ee3\u7801\u5757\uff08\u53ef\u4ee5\u64a4\u9500\uff09\n9. \u4e0d\u8981\u968f\u610f\u5f00GPU\u751a\u81f3TPU\uff0c\u8981\u5f00\u8bf7\u4e0e\u4e3b\u3001\u526f\u8d1f\u8d23\u4eba\u8054\u7cfb\uff0c\u5f97\u5230\u5141\u8bb8\u540e\u518d\u5f00\uff0c\u5b8c\u6210\u4efb\u52a1\u540e\u7acb\u523b\u5173\u95ed\n10. \u5728\u6bcf\u5468\u4f1a\u8bae\u4e2d\uff0c\u9009\u51fa\u7f16\u5199\u597d\u7684\u4ee3\u7801\u52a0\u5165\u5230\u5df2\u786e\u8ba4\u4ee3\u7801\u4e2d\uff0c\u8fdb\u884c\u96c6\u4e2d\u7684bug\u4fee\u6539\uff08\u5982\u82e5\u4e2a\u4eba\u65e0\u6cd5\u89e3\u51b3\uff09\uff0c\u4e4b\u540e\u5982\u65e0\u5fc5\u8981\u8bf7\u5c06\u4e2a\u4eba\u4ee3\u7801\u5757\u4e2d\u7684\u76f8\u5173\u4ee3\u7801\u5220\u9664\u4ee5\u514d\u51b2\u7a81\uff0c\u540c\u65f6\u8fdb\u884cSave Version\uff0c\u6807\u9898\u4e3aconfirmed_mm_dd_yyyy_bx\uff0c\u4ee3\u8868yyyy.mm.dd\u786e\u8ba4\u7684\u7d2f\u8ba1\u7b2cx\u6b21\u4e0a\u4f20\u7684\u6d4b\u8bd5\u7248\u672c\uff0c\u4e00\u5207\u64cd\u4f5c\u4ec5\u53ef\u4e3b\u3001\u526f\u8d1f\u8d23\u4eba\u8fdb\u884c\n11. \u8bf7\u4e0d\u8981\u81ea\u5df1\u8fdb\u884cSave Version\u64cd\u4f5c\n12. \u53ef\u4ee5\u4efb\u610f\u4e0a\u4f20\u6570\u636e\uff0c\u4f46\u4e0d\u53ef\u968f\u610f\u5220\u9664\u6570\u636e\uff08\u81ea\u5df1\u4e0a\u4f20\u7684\u672a\u88ab\u786e\u8ba4\u7684\u9664\u5916\uff09\uff0c\u4e0a\u4f20\u65f6\u9700\u8981\u6253\u5f00vpn\n13. \u4efb\u4f55\u6210\u5458\u6709\u4e49\u52a1\u548c\u6743\u5229\u4e3e\u62a5\u8fdd\u53cd\u4e0a\u8ff0\u89c4\u5219\u7684\u6210\u5458\uff0c\u5bf9\u8fdd\u89c4\u7684\u6210\u5458\u89c6\u60c5\u51b5\u8fdb\u884c\u5c01\u7981\u4e00\u5b9a\u65e5\u671f\u7684\u60e9\u7f5a\uff0c\u4e25\u91cd\u8005\u505a\u9000\u793e\u5904\u7406\uff0c\u8bf7\u8c28\u8bb0\n14. \u4efb\u4f55BUG\u53ca\u7591\u95ee\u8bf7\u6dfb\u52a0\u5230V. BUG\u53ca\u7591\u95ee\u5217\u8868\u4e2d\uff0c\u6309\u4f18\u5148\u7ea7\u53ca\u6dfb\u52a0\u987a\u5e8f\u6392\u5217","10e1b9f5":"### \u5176\u4f59\u5df2\u786e\u8ba4\u4ee3\u7801\n**\u4e25\u7981\u975e\u8d1f\u8d23\u4eba\u4fee\u6539\uff0c\u5982\u53d1\u73b0bug\u8bf7\u53ca\u65f6\u4e0e\u8d1f\u8d23\u4eba\u62a5\u544a\uff0c\u8d1f\u8d23\u4eba\u4f1a\u6253\u56de\u4ee3\u7801\u6216\u81ea\u884c\u4fee\u6539**  \nConfirmed Classes\u4e3a\u5df2\u786e\u8ba4\u7684\u7c7b\uff0c\u5982\u81ea\u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u7c7b  \nConfirmed Functions\u4e3a\u5df2\u786e\u8ba4\u7684\u5168\u5c40\u51fd\u6570  \n\u5176\u4f59\u5df2\u786e\u8ba4\u4ee3\u7801\u5728Other Confirmed Programmes\u5904  ","2ec2a22d":"### CptnPhntm\u4e01\u6c9b\n\u5206\u5de5\uff1a\u5409\u7965\u7269","e23bb31a":"### DoremySwee\u80e1\u97ec\u7407\n\u5206\u5de5\uff1a","73318232":"### Yuqi Lu\u9646\u96e8\u5947\n\u5206\u5de5\uff1a","9504629c":"## II. \u6a21\u578b\u67b6\u6784","a147c9c1":"## III. \u516c\u5171\u4ee3\u7801\n### \u516c\u7528\u4ee3\u7801\u57571\uff1a\u52a0\u8f7d\u5e93\n\u8bf7\u5c06\u6240\u6709\u7684\u52a0\u8f7d\u5e93\u7684\u8fc7\u7a0b\u5199\u5728\u8fd9\u91cc","374a7e24":"### ALPHAXU\u8bb8\u6893\u9038\n\u5206\u5de5\uff1a**19\u5c4a\u526f\u8d1f\u8d23**","e8cb4f47":"### Brian Guo\u90ed\u57f9\u626c\n\u5206\u5de5\uff1a**19\u5c4a\u4e3b\u8d1f\u8d23**  \nDone\uff1a\u628aboyu \u7684\u8bd7\u6b4cseq2seq\u4ee3\u7801\u6458\u4e86\u8fc7\u6765  \nTodo\uff1a\u6574\u4e00\u7248pytorch\u7684\uff0c\u5c06boyu\u7684\u8bd7\u6b4cseq2seq\u6539\u6210\u73b0\u4ee3\u5bf9\u8bdd\u6587\u672cseq2seq\uff08seqlen\u4e0d\u56fa\u5b9a\u7248\u672c\uff09","a23c40b4":"## VI. \u4f1a\u8bae\u8bb0\u5f55"}}