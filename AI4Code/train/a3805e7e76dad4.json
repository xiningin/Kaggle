{"cell_type":{"81ec0e97":"code","a0bf49e5":"code","47f926fd":"code","6aa68cfc":"code","99aa9e57":"code","8fdee4dd":"code","66b677d1":"code","b3d9017b":"code","ee99c5ba":"code","eb892e30":"code","d80f6bbf":"code","a81da43f":"code","9013dce3":"code","88a67f66":"code","25add9ec":"code","ca5e54fe":"code","f07ddad3":"code","3fd5a0a5":"code","63f00b24":"code","3cee9819":"code","b4d391bc":"code","b6dd8d8b":"code","b919d061":"code","665c6ac7":"code","1d41ad44":"code","0a8511b8":"code","9c2823df":"code","c9de85ca":"code","adc47e46":"code","064cd42c":"code","27d5c82b":"code","33e66317":"markdown","1d8af289":"markdown","93249edc":"markdown","967fe3e4":"markdown","9a5ddf2b":"markdown","88079746":"markdown","abb0bfaf":"markdown","72d4e6e1":"markdown","e9120bcc":"markdown","009f992f":"markdown","bf6d0e0a":"markdown","89e08563":"markdown","20739883":"markdown"},"source":{"81ec0e97":"import warnings\nwarnings.filterwarnings(action='ignore', category=UserWarning)","a0bf49e5":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\n\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    \"\"\"\n    This function calculates the upper limit and lower limit values for outlier analysis.\n\n    IQR = Q1 - Q3\n    lower_limit = Q1 - 1.5 * IQR\n    upper_limit = Q3 + 1.5 * IQR\n\n     Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    col_name: str\n        Column name to be processed\n    q1: int, optional\n        Percentage value for Q1 (default 0.25)\n    q3: int, optional\n        Percentage value for Q3 (default 0.75)\n\n    Returns\n    -------\n    lower_limit, upper_limit: int, int\n        Returns the lower limit and upper limit values used to identify outliers\n\n    Examples\n    -------\n    import seaborn as sns\n    df = sns.load_dataset(\"tips\")\n    lower_limit, upper_limit = outlier_thresholds(df, 'total_bill')\n    print(lower_limit, upper_limit)\n\n    \"\"\"\n    quantile1 = dataframe[col_name].quantile(q1)\n    quantile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quantile3 - quantile1\n    upper_limit = quantile3 + 1.5 * interquantile_range\n    lower_limit = quantile1 - 1.5 * interquantile_range\n    return lower_limit, upper_limit\n\n\ndef check_outlier(dataframe, col_name, **kwargs):\n    \"\"\"\n    This function checks for outliers in the dataframe\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    col_name: str\n        Column name to be processed\n    kwargs:\n        It is used when you want to change the Q1 and Q3 values. (default q1=0.25, q3=0.75)\n\n    Returns\n    -------\n    Returns boolean value\n\n    Examples\n    -------\n    import seaborn as sns\n    df = sns.load_dataset(\"tips\")\n    check_outlier(df, 'Age')\n    # If you want change q1 and q3\n    check_outlier(df, 'Age', q1=0.10, q3=0.90)\n\n    \"\"\"\n    if len(kwargs) == 0:\n        lower_limit, upper_limit = outlier_thresholds(dataframe, col_name)\n    else:\n        lower_limit, upper_limit = outlier_thresholds(dataframe, col_name, q1=kwargs['q1'], q3=kwargs['q3'])\n    if dataframe[(dataframe[col_name] > upper_limit) | (dataframe[col_name] < lower_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n\ndef grab_outliers(dataframe, col_name, index=False, **kwargs):\n    \"\"\"\n    This function is used to observe outliers.\n    When it is desired to reach the indexes of outliers, the index values can be returned by setting the parameter.\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    col_name: str\n        Column name to be processed\n    index: boolean, optional\n        Used to access indexes of outliers (default False)\n    kwargs:\n        It is used when you want to change the Q1 and Q3 values. (default q1=0.25, q3=0.75)\n\n    Returns\n    -------\n    outlier_index: list\n        If index=True returns indexes of outliers\n\n    Examples\n    -------\n    import seaborn as sns\n    df = sns.load_dataset(\"tips\")\n    grab_outliers(df, 'Age')\n    # If you want change q1 and q3\n    grab_outliers(df, 'Age', q1=0.10, q3=0.90)\n    # If you want to access indexes of outliers\n    outlier_index = grab_outliers(df, 'Age', q1=0.10, q3=0.90 index=True)\n\n    \"\"\"\n    if len(kwargs) == 0:\n        lower_limit, upper_limit = outlier_thresholds(dataframe, col_name)\n    else:\n        lower_limit, upper_limit = outlier_thresholds(dataframe, col_name, kwargs['q1'], kwargs['q3'])\n    if dataframe[((dataframe[col_name] < lower_limit) | (dataframe[col_name] > upper_limit))].shape[0] > 10:\n        print(dataframe[((dataframe[col_name] < lower_limit) | (dataframe[col_name] > upper_limit))].head())\n    else:\n        print(dataframe[((dataframe[col_name] < lower_limit) | (dataframe[col_name] > upper_limit))])\n    if index:\n        outlier_index = dataframe[((dataframe[col_name] < lower_limit) | (dataframe[col_name] > upper_limit))].index\n        return outlier_index\n\n\ndef remove_outlier(dataframe, col_name, **kwargs):\n    \"\"\"\n    This function deletes outliers\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    col_name: str\n        Column name to be processed\n    kwargs:\n        It is used when you want to change the Q1 and Q3 values. (default q1=0.25, q3=0.75)\n\n    Returns\n    -------\n    df_without_outliers: DataFrame\n        Processed dataframe\n\n    Examples\n    -------\n    import seaborn as sns\n    df = sns.load_dataset(\"tips\")\n    remove_outlier(df, 'Age')\n    # If you want change q1 and q3\n    remove_outlier(df, 'Age', q1=0.10, q3=0.90)\n\n\n    \"\"\"\n    if len(kwargs) == 0:\n        lower_limit, upper_limit = outlier_thresholds(dataframe, col_name)\n    else:\n        lower_limit, upper_limit = outlier_thresholds(dataframe, col_name, kwargs['q1'], kwargs['q3'])\n    df_without_outliers = dataframe[~((dataframe[col_name] < lower_limit) | (dataframe[col_name] > upper_limit))]\n    return df_without_outliers\n\n\ndef replace_with_thresholds(dataframe, col_name, **kwargs):\n    \"\"\"\n    This function replaces outliers with lower limit and upper limit values.\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    col_name: str\n        Column name to be processed\n    kwargs:\n        It is used when you want to change the Q1 and Q3 values. (default q1=0.25, q3=0.75)\n\n    Examples\n    -------\n    import seaborn as sns\n    df = sns.load_dataset(\"tips\")\n    replace_with_thresholds(df, 'Age')\n    # If you want change q1 and q3\n    replace_with_thresholds(df, 'Age', q1=0.10, q3=0.90)\n\n    \"\"\"\n    if len(kwargs) == 0:\n        lower_limit, upper_limit = outlier_thresholds(dataframe, col_name)\n    else:\n        lower_limit, upper_limit = outlier_thresholds(dataframe, col_name, kwargs['q1'], kwargs['q3'])\n    dataframe.loc[(dataframe[col_name] < lower_limit), col_name] = lower_limit\n    dataframe.loc[(dataframe[col_name] > upper_limit), col_name] = upper_limit\n\n\ndef missing_values_table(dataframe, na_name=False):\n    \"\"\"\n    This function calculates the numbers and ratios of the missing value.\n    Returns a list of names of columns with missing values with parameter setting.\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    na_name: bool\n        Used to access column names with missing values. (default False)\n\n    Returns\n    -------\n    na_columns: list\n        If na_name=True returns a list of column names with missing values.\n\n    Examples\n    -------\n    import pandas as pd\n    import numpy as np\n    import seaborn as sns\n    df = sns.load_dataset(\"titanic\")\n    missing_values_table(df)\n    # or\n    na_columns = missing_values_table(df, na_name=True)\n\n    \"\"\"\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns\n\n\ndef missing_vs_target(dataframe, target, na_columns):\n    \"\"\"\n    This function is used to analyze missing values.\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    target: str\n        Column name to be processed (target variable)\n    na_columns: list\n        List of columns with NA value\n\n    Examples\n    -------\n    import seaborn as sns\n    df = sns.load_dataset(\"tips\")\n    na_columns = missing_values_table(df, na_name=True)\n    missing_vs_target(df, \"total_bill\", na_columns)\n\n    \"\"\"\n    temp_df = dataframe.copy()\n    for col in na_columns:\n        temp_df[col + '_NA_FLAG'] = np.where(temp_df[col].isnull(), 1, 0)\n    na_flag = temp_df.loc[:, temp_df.columns.str.contains('_NA_')].columns\n    for col in na_flag:\n        print(pd.DataFrame({'TARGET_MEAN': temp_df.groupby(col)[target].mean(),\n                            \"Count\": temp_df.groupby(col)[target].count()}), end=\"\\n\\n\\n\")\n\n\ndef label_encoder(dataframe, binary_col):\n    \"\"\"\n    This function converts categorical variables with binary class into numeric variables with label encoding.\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    binary_col: str\n        name of column with binary class\n\n    Returns\n    -------\n        dataframe: DataFrame\n            Processed dataframe\n\n    Examples\n    -------\n    import seaborn as sns\n    df = sns.load_dataset(\"tips\")\n    df = label_encoder(df, \"sex\")\n\n    \"\"\"\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe\n\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False, dummy_na=False):\n    \"\"\"\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    categorical_cols: str\n        The name of the categorical column\n    drop_first: boolean, optional\n        get_dummies() parameter\n    dummy_na: boolean, optional\n        get_dummies() parameter\n    Returns\n    -------\n    dataframe: DataFrame\n        Processed dataframe\n\n    Notes\n    -------\n\n    If drop_first=True, label encoding is also done.\n\n    Examples\n    -------\n    import seaborn as sns\n    df = sns.load_dataset(\"tips\")\n    df = one_hot_encoder(df, \"day\")\n    # or\n    df = one_hot_encoder(df, \"day\", drop_first=True, dummy_na=True)\n\n    \"\"\"\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first, dummy_na=dummy_na)\n    return dataframe\n\n\ndef rare_analyser(dataframe, target, categorical_cols):\n    \"\"\"\n\n    Parameters\n    ----------\n    dataframe: DataFrame\n        Dataframe to be processed\n    target: str\n        The name of the target value\n    categorical_cols: list\n        List of category columns\n\n    Examples\n    -------\n    import seaborn as sns\n    from helpers.eda import grab_col_names\n    df = sns.load_dataset(\"titanic\")\n    categorical_cols, _, _ = grab_col_names(df)\n    rare_analyser(df, 'survived', categorical_cols)\n\n    \"\"\"\n    for col in categorical_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            'TARGET_MEDIAN': dataframe.groupby(col)[target].median(),\n                            \"TATGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\n# def rare_encoder(dataframe, rare_perc):\n#     \"\"\"\n#\n#     Parameters\n#     ----------\n#     dataframe: DataFrame\n#         Dataframe to be processed\n#     rare_perc: float\n#         Threshold value to be used to identify rare variables\n#\n#     Returns\n#     -------\n#     dataframe: DataFrame\n#         Processed dataframe\n#\n#     Examples\n#     --------\n#     import seaborn as sns\n#     df = sns.load_dataset(\"titanic\")\n#     rare_encoder(df, 0.20)\n#\n#     \"\"\"\n#     temp_df = dataframe.copy()\n#     rare_columns = [col for col in temp_df.columns if (temp_df[col].nunique() > 2)\n#                     and (temp_df[col].nunique() < 10)\n#                     and (temp_df[col].value_counts() \/ len(temp_df) < rare_perc).any(axis=None)]\n#\n#     for var in rare_columns:\n#         tmp = temp_df[var].value_counts() \/ len(temp_df)\n#         rare_labels = tmp[tmp < rare_perc].index\n#         temp_df[var] = np.where(temp_df[var].isin(rare_labels), 'Rare', temp_df[var])\n#\n#     return temp_df\n\n\ndef rare_encoder(dataframe, rare_perc, cat_cols):\n    temp_df = dataframe.copy()\n    rare_columns = [col for col in cat_cols if (dataframe[col].value_counts() \/ len(temp_df) < .001).sum() > 1]\n    for col in rare_columns:\n        temp_df = dataframe[col].value_counts() \/ len(dataframe)\n        rare_labels = temp_df[temp_df < rare_perc].index\n        dataframe[col] = np.where(dataframe[col].isin(rare_labels), 'Rare', dataframe[col])\n    return dataframe","47f926fd":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ndef check_dataframe(dataframe, head=5, tail=5):\n    print(\"######################### Shape #############################\")\n    print(dataframe.shape)\n    print(\"######################### Types #############################\")\n    print(dataframe.dtypes)\n    print(\"######################### Head #############################\")\n    print(dataframe.head(head))\n    print(\"######################### Tail #############################\")\n    print(dataframe.tail(tail))\n    print(\"######################### NA #############################\")\n    print(dataframe.isnull().sum())\n    print(\"######################### Describe #############################\")\n    print(dataframe.describe().T)\n    print(\"######################### Quantiles ###############################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"####################################\")\n    if plot:\n        sns.countplot(dataframe[col_name], data=dataframe)\n        plt.show()\n\n\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n\n    Veri setindeki kategorik, numerik ve kategorik fakat kardinal de\u011fi\u015fkenlerin isimlerini verir.\n    Not: Kategorik de\u011fi\u015fkenlerin i\u00e7erisine numerik g\u00f6r\u00fcn\u00fcml\u00fc kategorik de\u011fi\u015fkenler de dahildir.\n\n    Parameters\n    ------\n        dataframe: dataframe\n                De\u011fi\u015fken isimleri al\u0131nmak istenilen dataframe\n        cat_th: int, optional\n                numerik fakat kategorik olan de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n        car_th: int, optinal\n                kategorik fakat kardinal de\u011fi\u015fkenler i\u00e7in s\u0131n\u0131f e\u015fik de\u011feri\n\n    Returns\n    ------\n        cat_cols: list\n                Kategorik de\u011fi\u015fken listesi\n        num_cols: list\n                Numerik de\u011fi\u015fken listesi\n        cat_but_car: list\n                Kategorik g\u00f6r\u00fcn\u00fcml\u00fc kardinal de\u011fi\u015fken listesi\n\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n\n\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = toplam de\u011fi\u015fken say\u0131s\u0131\n        num_but_cat cat_cols'un i\u00e7erisinde.\n        Return olan 3 liste toplam\u0131 toplam de\u011fi\u015fken say\u0131s\u0131na e\u015fittir: cat_cols + num_cols + cat_but_car = de\u011fi\u015fken say\u0131s\u0131\n\n    \"\"\"\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\n\ndef target_summary_with_cat(dataframe, target, categorical_col):\n    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\ndef target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\n\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\")\n        plt.show()\n    return drop_list\n\n","6aa68cfc":"from catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate,RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","99aa9e57":"pd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n# pd.set_option('display.max_rows', None)","8fdee4dd":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf = train.append(test).reset_index(drop=True)\ndf.head()","66b677d1":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","b3d9017b":"df[\"SalePrice\"].describe([0.05, 0.10, 0.25, 0.50, 0.75, 0.80, 0.90, 0.95, 0.99])","ee99c5ba":"# ba\u011f\u0131ml\u0131 de\u011fi\u015fkene g\u00f6re korelasyonlar\u0131 hesaplayacak y\u00fczde 60 da nb\u00fcy\u00fck ve k\u00fc\u00e7\u00fck\ndef find_correlation(dataframe, numeric_cols, corr_limit=0.60):\n    high_correlations = []\n    low_correlations = []\n    for col in numeric_cols:\n        if col == \"SalePrice\":\n            pass\n        else:\n            correlation = dataframe[[col, \"SalePrice\"]].corr().loc[col, \"SalePrice\"]\n            print(col, correlation)\n            if abs(correlation) > corr_limit:\n                high_correlations.append(col + \": \" + str(correlation))\n            else:\n                low_correlations.append(col + \": \" + str(correlation))\n    return low_correlations, high_correlations\n\n\nlow_corrs, high_corrs = find_correlation(df, num_cols)","eb892e30":"df.head()","d80f6bbf":"# Year\nfrom datetime import date\ntodays_date = date.today()\ntodays_date.year\n\n# Lot Frontage\ndf[\"LOTFRONTAGE_RATIO\"] = df[\"LotFrontage\"] \/ df[\"LotArea\"] * 100\ndf[\"LOTFRONTAGE_RATIO\"].fillna(0, inplace=True)\n\n# Calculating the Age of the House by 2021\ndf[\"NEW_BUILDING_AGE\"] = todays_date.year - df[\"YearBuilt\"]\n\n# Grouping of Calculated Ages\ndf[\"NEW_BUILDING_AGE_CAT\"] = pd.qcut(df[\"NEW_BUILDING_AGE\"], 4, labels=[\"New_house\", \"Middle_aged\", \"Middle_Old\", \"Old\"])\n\n# Difference between year of sale and year of manufacture\ndf[\"NEW_SOLD_DIFF\"] = df[\"YrSold\"] - df[\"YearBuilt\"]\n\n# House Demand \ndf[\"NEW_HOUSE_DEMAND\"] = pd.qcut(df[\"NEW_SOLD_DIFF\"], 4, labels=[\"High_Demand\", \"Normal_Demand\", \"Less_Demand\", \"Least_Demand\"])\n\n# Garage age\ndf[\"NEW_GARAGE_AGE\"] = df[\"GarageYrBlt\"] - df[\"YearBuilt\"]\n\n# Garage year difference\ndf[\"NEW_GARAGE_YEAR_DIFF\"] = df[\"GarageYrBlt\"] - df[\"YearBuilt\"]\n\n# First floor ratio\ndf[\"NEW_FIRST_FLOOR_RATIO\"] = df[\"1stFlrSF\"] \/ df[\"GrLivArea\"] * 100\n\n# Uncomplete ratio:\ndf[\"NEW_UNCOMP_BSMT_RATIO\"] = df[\"BsmtUnfSF\"] \/ df[\"TotalBsmtSF\"] * 100\n\n# Total bath\ndf[\"NEW_TOTAL_BATH\"] = (df[\"BsmtHalfBath\"] + df[\"HalfBath\"]) * 0.5 + df[\"BsmtFullBath\"] + df[\"FullBath\"]\ndf[\"NEW_TOTAL_FULL_BATH\"] = df[\"FullBath\"] + df[\"BsmtFullBath\"]\ndf[\"NEW_TOTAL_HALF_BATH\"] = df[\"HalfBath\"] + df[\"BsmtHalfBath\"] * 0.5\n\n# Other Rooms\ndf[\"NEW_NEW_NUMBER_OF_OTHER_ROOM\"] = df[\"TotRmsAbvGrd\"] - df[\"KitchenAbvGr\"] - df[\"BedroomAbvGr\"]\n\n# Average Room Area\ndf[\"NEW_AVERAGE_ROOM_AREA\"] = df[\"GrLivArea\"] \/ (df[\"TotRmsAbvGrd\"] + df[\"NEW_TOTAL_BATH\"])\n\n# Total porch area\ndf[\"NEW_TOTAL_PORCH_AREA\"] = df[\"WoodDeckSF\"] + df[\"OpenPorchSF\"] + df[\"EnclosedPorch\"] + df[\"3SsnPorch\"] + df[\n    \"ScreenPorch\"]\n\n# Garage ratio\ndf[\"NEW_GARAGE_RATIO\"] = df[\"GarageArea\"] \/ df[\"LotArea\"] * 100\n\n# Garage Area Per car\ndf[\"NEW_GARAGE_AREA_PER_CAR\"] = df[\"GarageArea\"] \/ df[\"GarageCars\"]\n\n# Total Garden Area:\ndf[\"NEW_GARDEN_AREA\"] = df[\"LotArea\"] - df[\"GarageArea\"] - df[\"NEW_TOTAL_PORCH_AREA\"] - df[\"TotalBsmtSF\"]\ndf[\"NEW_GARDEN_RATIO\"] = df[\"NEW_GARDEN_AREA\"] \/ df[\"LotArea\"] * 100\ndf[\"NEW_LOTAREA_CAT\"] = pd.qcut(df[\"LotArea\"],4,[\"Small\",\"Medium\",\"Big\",\"Huge\"])\n\n# Total SF\ndf['TotalSF'] = (df['BsmtFinSF1'] + df['BsmtFinSF2'] + df['1stFlrSF'] + df['2ndFlrSF'])\n\n# 1. Floor and Basement m^2\ndf[\"NEW_SF\"] = df[\"1stFlrSF\"] + df[\"TotalBsmtSF\"]\n\n# Total m^2\ndf[\"NEW_TOTAL_M^2\"] = df[\"NEW_SF\"] + df[\"2ndFlrSF\"]\n\n# Garage + total area \ndf[\"NEW_SF_G\"] = df[\"NEW_TOTAL_M^2\"] + df[\"GarageArea\"]\n\n# New living area\ndf['NEW_TOTAL_LVNGAR'] = (df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'])\n\n# Others\ndf[\"NEW_YEAR_OVERALL\"] = df[\"YearRemodAdd\"] * df[\"OverallQual\"]\ndf[\"NEW_TOTALQUAL\"] = df[\"OverallQual\"] * df[\"GarageArea\"] * df[\"GrLivArea\"]\ndf[\"NEW_YEAR_REMOD\"] = df[\"YearBuilt\"] - df[\"YearRemodAdd\"]\ndf[\"NEW_AREA\"] = df[\"GrLivArea\"] + df[\"GarageArea\"]\n","a81da43f":"\"\"\"list = [\n    [\"MSSubClass\", \"MSZoning\"],\n    [\"MSSubClass\", \"BUILDING_AGE_CAT\"],\n    [\"Neighborhood\", \"HouseStyle\"],\n    [\"HouseStyle\", \"OverallQual\"],\n    [\"HouseStyle\", \"OverallCond\"],\n    [\"HouseStyle\", \"YearRemodAdd\"],\n    [\"HouseStyle\", \"RoofStyle\"],\n    [\"HouseStyle\", \"Exterior1st\"],\n    [\"HouseStyle\", \"MasVnrType\"],\n    [\"SaleType\", \"SaleCondition\", \"HouseStyle\"],\n    [\"SaleType\", \"HouseStyle\", \"MSSubClass\"],\n    [\"LotConfig\", \"LotShape\"],\n    [\"LotConfig\", \"Neighborhood\"],\n    [\"LotArea_Cat\", \"Neighborhood\"],\n    [\"LandContour\", \"Neighborhood\"]\n]\n\n\ndef colon_bros(dataframe, list):\n\n    for row in list:\n        colon = [col for col in dataframe.columns if col in row]\n        dataframe[\"_\".join(map(str, row))] = [\"_\".join(map(str, i)) for i in dataframe[colon].values]\n        # print(dataframe[\"_\".join(map(str, row))].head(15))\n\n\ncolon_bros(df, list)\"\"\"","9013dce3":"df.columns = [col.upper() for col in df.columns]\ndf.head()","88a67f66":"# RARE ANALYZER\ndef rare_analyser(dataframe, target, rare_perc):\n    rare_columns = [col for col in dataframe.columns if len(dataframe[col].value_counts()) <= 20\n                    and (dataframe[col].value_counts() \/ len(dataframe) < rare_perc).any(axis=None)]\n    for var in rare_columns:\n        print(var, \":\", len(dataframe[var].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[var].value_counts(),\n                            \"RATIO\": dataframe[var].value_counts() \/ len(dataframe),\n                            \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median()}), end=\"\\n\\n\\n\")","25add9ec":"rare_analyser(df, \"SALEPRICE\", 0.01)","ca5e54fe":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\n\ndf = rare_encoder(df, 0.01, cat_cols)\n\nuseless_cols = [col for col in cat_cols if df[col].nunique() == 1 or\n                (df[col].nunique() == 2 and (df[col].value_counts() \/ len(df) <= 0.01).any(axis=None))]\n\ncat_cols = [col for col in cat_cols if col not in useless_cols]\n\nfor col in useless_cols:\n    df.drop(col, axis=1, inplace=True)","f07ddad3":"cat_cols = cat_cols + cat_but_car\n\ndf = one_hot_encoder(df, cat_cols, drop_first=False)\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nuseless_cols_new = [col for col in cat_cols if (df[col].value_counts() \/ len(df) <= 0.01).any(axis=None)]\n\ndf[useless_cols_new].head()\n\n\nfor col in useless_cols_new:\n    df.drop(col, axis=1, inplace=True)\n\n# for col in useless_cols_new:\n#   cat_summary(df, col)","3fd5a0a5":"missing_values_table(df)\n# missing_values_table(train)\n#test.shape\nna_cols = [col for col in df.columns if df[col].isnull().sum() > 0 and \"SalePrice\".upper() not in col]\n#df[na_cols] = df[na_cols].apply(lambda x: x.fillna(x.median()), axis=0)\n\n\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\ndf_filled = imputer.fit_transform(df[na_cols])\ndf[na_cols] = pd.DataFrame(df_filled, columns=df[na_cols].columns)\n\nmissing_values_table(df)","63f00b24":"for col in num_cols:\n    print(col, check_outlier(df, col, q1=0.01, q3=0.99))\n\n#for col in num_cols:\n    #replace_with_thresholds(df, col, q1=0.01, q3=0.99)","3cee9819":"train_df = df[df['SalePrice'.upper()].notnull()]\ntest_df = df[df['SalePrice'.upper()].isnull()].drop(\"SalePrice\".upper(), axis=1)\n\n# y = train_df[\"SalePrice\"]\ny = np.log1p(train_df['SalePrice'.upper()])\nX = train_df.drop([\"ID\",\"SalePrice\".upper()], axis=1)","b4d391bc":"# Base Models\nmodels = [\n    ('LR', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('KNN', KNeighborsRegressor()),\n          ('CART', DecisionTreeRegressor()),\n          ('RF', RandomForestRegressor()),\n          ('SVR', SVR()),\n          ('GBM', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n          (\"LightGBM\", LGBMRegressor()),\n#         (\"CatBoost\", CatBoostRegressor(task_type = \"GPU\",verbose=False))\n]\n\nfor name, regressor in models:\n    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\n    print(f\"RMSE: {round(rmse, 4)} ({name}) \")","b6dd8d8b":"from sklearn.model_selection import GridSearchCV, cross_val_score, cross_validate,RandomizedSearchCV\n\nlgb_model = LGBMRegressor(random_state=17)\n\nlgb_random_params = {\"num_leaves\" : np.random.randint(2, 10, 5),\n                     \"max_depth\": np.random.randint(2, 20, 10),\n                     \"n_estimators\": [int(x) for x in np.linspace(start=200, stop=2000, num=50)],\n                     \"min_child_samples\": np.random.randint(5, 20, 10),\n                     \"reg_alpha\": [0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9],\n                     \"reg_lambda\": [0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9],\n                     \"learning_rate\": [0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9,1,3,5,7],\n                     \"colsample_bytree\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n                     \"min_child_weight\" : [0.001,0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9],\n                    \"max_bin\": np.random.randint(2, 50, 10),\n                    'bagging_fraction': [0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9],\n                    'bagging_freq': np.random.randint(2, 10, 5),\n                    'min_sum_hessian_in_leaf' : [0.02,0.01]\n                     }\n\nlgb_random = RandomizedSearchCV(estimator=lgb_model,param_distributions=lgb_random_params,\n                                n_iter=100,\n                                cv=3,\n                                verbose=True,\n                                random_state=42,\n                                n_jobs=-1)\n\n\nlgb_random.fit(X, y)","b919d061":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(50, 50))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')","665c6ac7":"final_model = lgb_model.set_params(**lgb_random.best_params_).fit(X, y)\nrmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=5, scoring=\"neg_mean_squared_error\")))\nrmse","1d41ad44":"feature_imp = pd.DataFrame({'Value': final_model.feature_importances_, 'Feature': X.columns})\nnum_summary(feature_imp, \"Value\", True)\nfeature_imp[feature_imp[\"Value\"] > 0].shape\nfeature_imp[feature_imp[\"Value\"] < 1].shape\nzero_imp_cols = feature_imp[feature_imp[\"Value\"] < 10][\"Feature\"].values\nselected_cols = [col for col in X.columns if col not in zero_imp_cols]","0a8511b8":"lgb_random.best_params_","9c2823df":"lgbm_model = LGBMRegressor(random_state=17)\n\nlgbm_params = {\"num_leaves\" : [6,8],\n               \"max_depth\": [6,9],\n               \"n_estimators\": [1044,1779],\n               \"min_child_samples\": [6,10],\n               \"reg_alpha\": [0.1,0.2],\n               \"reg_lambda\": [0.1,0.02],\n               \"learning_rate\": [0.02],\n               \"colsample_bytree\": [0.2],\n               \"min_child_weight\" : [0.01,0.001],\n               \"max_bin\": [32],\n               'bagging_freq': [5],\n               'bagging_fraction': [0.5],\n              'min_sum_hessian_in_leaf': [0.00245,0.01]\n              }\n\nlgbm_gs_best = GridSearchCV(lgbm_model,\n                            lgbm_params,\n                            cv=3,\n                            n_jobs=-1,\n                            verbose=True).fit(X[selected_cols], y)\n\n\nfinal_model = lgbm_model.set_params(**lgbm_gs_best.best_params_).fit(X[selected_cols], y)\nrmse = np.mean(np.sqrt(-cross_val_score(final_model, X[selected_cols], y, cv=5, scoring=\"neg_mean_squared_error\")))\nrmse","c9de85ca":"test_df['Id'] = test_df['ID'].astype('int64')\nsubmission_df = pd.DataFrame()\nsubmission_df['Id'] = test_df[\"Id\"]\ny_pred_sub = final_model.predict(test_df[selected_cols])\ny_pred_sub = np.expm1(y_pred_sub)\nsubmission_df['SalePrice'] = y_pred_sub\nsubmission_df.to_csv('submission_v9.csv', index=False)","adc47e46":"\"\"\"\n\nrf_model = RandomForestRegressor(random_state=17)\n\nrf_random_params = {\"max_depth\": np.random.randint(5, 20, 10),\n                    \"max_features\": np.random.randint(2, 50, 20),\n                    \"min_samples_split\": np.random.randint(2, 20, 10),\n                    \"n_estimators\": [int(x) for x in np.linspace(start=100, stop=1500, num=40)],\n                    \"min_samples_leaf\" : np.random.randint(2, 50, 20),\n                    \"min_weight_fraction_leaf\" : [0.01,0.1,0.2,0.3,0.02,0.5],\n                    \"min_impurity_decrease\":[0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9],\n                    \"max_samples\":[0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9]}\n\n\nrf_random = RandomizedSearchCV(estimator=rf_model,\n                               param_distributions=rf_random_params,\n                               n_iter=100,  # denenecek parametre say\u0131s\u0131\n                               cv=3,\n                               verbose=True,\n                               random_state=42,\n                               n_jobs=-1)\n\nrf_random.fit(X, y)\n\n# En iyi hiperparametre de\u011ferleri:\nrf_random.best_params_\n\n# En iyi skor\nrf_random.best_score_\n\n\"\"\"","064cd42c":"\"\"\"\n\nlgb_model = LGBMRegressor(random_state=17)\n\nlgb_random_params = {\"num_leaves\" : np.random.randint(2, 10, 5),\n                     \"max_depth\": np.random.randint(2, 20, 10),\n                     \"n_estimators\": [int(x) for x in np.linspace(start=200, stop=2000, num=50)],\n                     \"min_child_samples\": np.random.randint(5, 20, 10),\n                     \"reg_alpha\": [0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9],\n                     \"reg_lambda\": [0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9],\n                     \"learning_rate\": [0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9,1,3,5,7],\n                     \"colsample_bytree\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n                     \"min_child_weight\" : [0.001,0.01,0.1,0.2,0.3,0.02,0.5,0.7,0.9]}\n\n\nlgb_random = RandomizedSearchCV(estimator=lgb_model,param_distributions=lgb_random_params,\n                                n_iter=100,  # denenecek parametre say\u0131s\u0131\n                                cv=3,\n                                verbose=True,\n                                random_state=42,\n                                n_jobs=-1)\n\n\nlgb_random.fit(X, y)\n\n# En iyi hiperparametre de\u011ferleri:\nlgb_random.best_params_\n\n# En iyi skor\nlgb_random.best_score_\n\n\"\"\"","27d5c82b":"\"\"\"\n\nxgb_model = XGBRegressor(random_state=17, tree_method='gpu_hist')\n\n\nxgb_random_params = {\"max_depth\": np.random.randint(2, 20, 20),\n                     \"n_estimators\": [int(x) for x in np.linspace(start=100, stop=1000, num=20)],\n                     \"min_child_weight\": [0.3,0.02,0.5,0.7,0.9],\n                     \"learning_rate\": [0.02,0.5,0.7,0.9],\n                     \"colsample_bytree\": [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n                     \"min_child_weight\" : [0.02,0.5,]}\n\n\nxgb_random = RandomizedSearchCV(estimator=xgb_model,param_distributions=xgb_random_params,\n                                n_iter=100,  # denenecek parametre say\u0131s\u0131\n                                cv=3,\n                                verbose=True,\n                                random_state=42,\n                                n_jobs=-1)\n\n\nxgb_random.fit(X, y)\n\n\n# En iyi hiperparametre de\u011ferleri:\nxgb_random.best_params_\n\n\n# En iyi skor\nxgb_random.best_score_\n\n\"\"\"","33e66317":"##################\n# Label Encoding & One-Hot Encoding\n##################","1d8af289":"######################################\n# Exploratory Data Analysis\n######################################","93249edc":"# Rare Analysis","967fe3e4":"##################\n# Target Analysis\n##################","9a5ddf2b":"# XGB Random Search","88079746":"# randomcv LGBM","abb0bfaf":"#######################################\n# Sonu\u00e7lar\u0131n Y\u00fcklenmesi\n#######################################","72d4e6e1":"# randomcv RF","e9120bcc":"##################\n# Outliers\n##################","009f992f":"##################\n# Missing Values\n##################","bf6d0e0a":"##################\n# Rare Encoding\n##################","89e08563":"######################################\n# Modeling\n######################################","20739883":"######################################\n# Data Preprocessing & Feature Engineering\n######################################"}}