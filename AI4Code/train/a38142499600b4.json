{"cell_type":{"56d1f8cc":"code","072a22cc":"code","e77bf1ff":"code","0c1844ee":"code","f77d77e2":"code","e2135af8":"code","5454cd37":"code","73589890":"code","bcd1ed77":"code","ab1aaff1":"code","c8c10f9c":"code","21bdd30c":"code","51e8bf34":"code","549aea41":"code","2792615e":"code","c49a3c94":"code","c3ef0299":"code","f2bff59d":"code","4d6d13bd":"code","89cf6ce1":"code","060f09eb":"code","afd83fbe":"code","befc6219":"code","a448696e":"code","9ef3eeac":"markdown","820e79d4":"markdown","e138c877":"markdown","6f026548":"markdown","fede4b9c":"markdown","e6de7410":"markdown","9a921587":"markdown","0aa097f1":"markdown","520ca2eb":"markdown","eabd7de0":"markdown"},"source":{"56d1f8cc":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Classifier\/Regressor\nfrom xgboost import XGBRegressor, DMatrix\n\n# Model selection\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import make_scorer\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt.callbacks import DeadlineStopper, DeltaYStopper\nfrom skopt.space import Real, Categorical, Integer\n\n# Data processing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","072a22cc":"# Loading data \nX_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","e77bf1ff":"X_train.head()","0c1844ee":"# Preparing data as a tabular matrix\ny_train = X_train.target\nX_train = X_train.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","f77d77e2":"# Stratifying the target\ny_stratified = pd.cut(y_train.rank(method='first'), bins=10, labels=False)","e2135af8":"y_stratified","5454cd37":"# Winsorizing lower bounds\nfrom scipy.stats.mstats import winsorize\ny_train = np.array(winsorize(y_train, [0.002, 0.0]))","73589890":"non_important_features=[\"cat9\",\"cat7\",\"cat4\",\"cat2\",\"cat3\",\"cat6\",\"cont4\",\"cont3\",\"cont0\"]\nX_train = X_train[non_important_features]\nX_test = X_test[non_important_features]","bcd1ed77":"# Pointing out categorical features\ncategoricals = [item for item in X_train.columns if 'cat' in item]","ab1aaff1":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(X_train.append(X_test)[categoricals])\nX_train[dummies.columns] = dummies.iloc[:len(X_train), :]\nX_test[dummies.columns] = dummies.iloc[len(X_train): , :]\ndel(dummies)","c8c10f9c":"# Dealing with categorical data using OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\nX_train[categoricals] = ordinal_encoder.fit_transform(X_train[categoricals])\nX_test[categoricals] = ordinal_encoder.transform(X_test[categoricals])","21bdd30c":"X_train.head()","51e8bf34":"# Feature selection (https:\/\/www.kaggle.com\/lucamassaron\/tutorial-feature-selection-with-boruta-shap)\n\"\"\"\nimportant_features = ['cat8_E', 'cont0', 'cont5', 'cont7', 'cont8', 'cat1_A', 'cont2', 'cont13', \n                      'cont3', 'cont10', 'cont1', 'cont9', 'cont11', 'cat1', 'cat8_C', 'cont6', \n                      'cont12', 'cat5', 'cat3_C', 'cont4', 'cat8']\n                      \"\"\"\n\nX_train.head() \n","549aea41":"# Reporting util for different optimizers\ndef report_perf(optimizer, X, y, title=\"model\", callbacks=None):\n    \"\"\"\n    A wrapper for measuring time and performances of different optmizers\n    \n    optimizer = a sklearn or a skopt optimizer\n    X = the training set \n    y = our target\n    title = a string label for the experiment\n    \"\"\"\n    start = time()\n    \n    if callbacks is not None:\n        optimizer.fit(X, y, callback=callbacks)\n    else:\n        optimizer.fit(X, y)\n        \n    d=pd.DataFrame(optimizer.cv_results_)\n    best_score = optimizer.best_score_\n    best_score_std = d.iloc[optimizer.best_index_].std_test_score\n    best_params = optimizer.best_params_\n    \n    print((title + \" took %.2f seconds,  candidates checked: %d, best CV score: %.3f \"\n           + u\"\\u00B1\"+\" %.3f\") % (time() - start, \n                                   len(optimizer.cv_results_['params']),\n                                   best_score,\n                                   best_score_std))    \n    print('Best parameters:')\n    pprint.pprint(best_params)\n    print()\n    return best_params","2792615e":"# Setting the scoring function\nscoring = make_scorer(partial(mean_squared_error, squared=False), \n                      greater_is_better=False)","c49a3c94":"# Setting the validation strategy\nskf = StratifiedKFold(n_splits=7,\n                      shuffle=True, \n                      random_state=0)\n\ncv_strategy = list(skf.split(X_train, y_stratified))","c3ef0299":"# Setting the basic regressor\nreg = XGBRegressor(random_state=0, booster='gbtree', objective='reg:squarederror', tree_method='gpu_hist')","f2bff59d":"# Setting the search space\nsearch_spaces = {'learning_rate': Real(0.01, 1.0, 'uniform'),\n                 'max_depth': Integer(2, 12),\n                 'subsample': Real(0.1, 1.0, 'uniform'),\n                 'colsample_bytree': Real(0.1, 1.0, 'uniform'), # subsample ratio of columns by tree\n                 'reg_lambda': Real(1e-9, 100., 'uniform'), # L2 regularization\n                 'reg_alpha': Real(1e-9, 100., 'uniform'), # L1 regularization\n                 'n_estimators': Integer(50, 5000)\n   }","4d6d13bd":"# Wrapping everything up into the Bayesian optimizer\nopt = BayesSearchCV(estimator=reg,                                    \n                    search_spaces=search_spaces,                      \n                    scoring=scoring,                                  \n                    cv=cv_strategy,                                           \n                    n_iter=120,                                       # max number of trials\n                    n_points=1,                                       # number of hyperparameter sets evaluated at the same time\n                    n_jobs=1,                                         # number of jobs\n                    iid=False,                                        # if not iid it optimizes on the cv score\n                    return_train_score=False,                         \n                    refit=False,                                      \n                    optimizer_kwargs={'base_estimator': 'GP'},        # optmizer parameters: we use Gaussian Process (GP)\n                    random_state=0)                                   # random state for replicability","89cf6ce1":"# Running the optimizer\noverdone_control = DeltaYStopper(delta=0.0001)                    # We stop if the gain of the optimization becomes too small\ntime_limit_control = DeadlineStopper(total_time=60*60*4)          # We impose a time limit (7 hours)\n\nbest_params = report_perf(opt, X_train, y_train,'XGBoost_regression', \n                          callbacks=[overdone_control, time_limit_control])","060f09eb":"# Transferring the best parameters to our basic regressor\nreg = XGBRegressor(random_state=0, booster='gbtree', objective='reg:squarederror', tree_method='gpu_hist', **best_params)","afd83fbe":"# Cross-validation prediction\nfolds = 10\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=0)\n\npredictions = np.zeros(len(X_test))\nrmse = list()\n\nfor k, (train_idx, val_idx) in enumerate(skf.split(X_train, y_stratified)):\n    reg.fit(X_train.iloc[train_idx, :], y_train[train_idx])\n    val_preds = reg.predict(X_train.iloc[val_idx, :])\n    val_rmse = mean_squared_error(y_true=y_train[val_idx], y_pred=val_preds, squared=False)\n    print(f\"Fold {k} RMSE: {val_rmse:0.5f}\")\n    rmse.append(val_rmse)\n    predictions += reg.predict(X_test).ravel()\n    \npredictions \/= folds\nprint(f\"repeated CV RMSE: {np.mean(rmse):0.5f} (std={np.std(rmse):0.5f})\")","befc6219":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': predictions})\n\nsubmission.to_csv(\"submission.csv\", index = False)","a448696e":"submission","9ef3eeac":"We define a search space, expliciting the key hyper-parameters to optimize and the range where to look for the best values.\n","820e79d4":"First, we create a wrapper function to deal with running the optimizer and reporting back its best results.","e138c877":"We then define the Bayesian optimization engine, providing to it our XGBoost, the search spaces, the evaluation metric, the cross-validation. We set a large number of possible experiments and some parallelism in the search operations.","6f026548":"We then define the evaluation metric, using the Scikit-learn function make_scorer allows us to convert the optimization into a minimization problem, as required by Scikit-optimize. We set squared=False by means of a partial function to obtain the root mean squared error (RMSE) as evaluation.","fede4b9c":"Finally we runt the optimizer and wait for the results. We have set some limits to its operations: we required it to stop if it cannot get consistent improvements from the search (DeltaYStopper) and time dealine set in seconds (we decided for 6 hours).","e6de7410":"# Setting up optimization","9a921587":"Having got the best hyperparameters for the data at hand, we instantiate a XGBoost using such values and train our model on all the available examples.\n\nAfter having trained the model, we predict on the test set and we save the results on a csv file.","0aa097f1":"We set up a generic XGBoost regressor.","520ca2eb":"# Prediction on test data","eabd7de0":"We set up a  7-fold cross validation"}}