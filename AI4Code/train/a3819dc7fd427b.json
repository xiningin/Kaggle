{"cell_type":{"190fa7b5":"code","651f0e72":"code","a96b8b30":"code","8500bc0d":"code","0868b0e6":"code","47f3e9c4":"code","7e5b63d1":"code","072f249c":"code","360cb8e0":"code","2f582d20":"code","13cc422f":"code","ea479e4e":"code","5807d97b":"code","7464f1d8":"code","4698f9ed":"code","19f61698":"code","14f693ae":"code","d7395cd6":"markdown","252a363b":"markdown","16b7258a":"markdown","1d40dd8d":"markdown","83dab3bf":"markdown"},"source":{"190fa7b5":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neural_network import MLPRegressor\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nsns.set(style=\"white\")\n%matplotlib inline\n\ndef rmse(y_test, y_pred):\n      return np.sqrt(mean_squared_error(y_test, y_pred))\n\ndef rmsle(y_test, y_pred):\n    return np.sqrt(mean_squared_log_error(y_test, y_pred))","651f0e72":"df_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","a96b8b30":"df_train.head(10)","8500bc0d":"df_train.drop('Id', axis=1, inplace=True)\ndf_test.drop('Id', axis=1, inplace=True)\n\nsize = len(list(df_train.columns))-1\n\n# df_train.dropna(axis=1, inplace=True)\n# f_columns = list(df_train.columns)\n# size = len(f_columns)-1\n# del f_columns[size]\n# df_test = df_test[f_columns]\n# df_train.head()","0868b0e6":"def preprocessing(df):\n    simputer = SimpleImputer(strategy=\"most_frequent\")\n    \n    df = df.replace(np.nan, 0).replace(np.inf, 1e+5).replace(-np.inf, -1e+5)\n    for column in df.columns:\n        if df[column].dtype.name == \"object\":\n            df[column] = pd.Categorical(df[column]).codes\n\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    num_cols = list(df.select_dtypes(include=numerics).columns)\n    df[num_cols] = simputer.fit_transform(df[num_cols])\n    return df\n\ndef normalization(df, norm):\n    columns = df.columns\n    return pd.DataFrame(norm.transform(df), columns=columns)\n\ncolumns = df_train.columns\nx_train = pd.DataFrame(df_train.to_numpy()[:, :size], columns=columns[:size])\ny_train = df_train.to_numpy()[:, size:].ravel().astype(np.float64)\n\ndf_train = preprocessing(x_train).astype(np.int)\ndf_test = preprocessing(df_test).astype(np.int)\n\n# norm = StandardScaler().fit(df_train)\n# df_train = normalization(df_train, norm).astype(np.float64)\n# df_test = normalization(df_test, norm).astype(np.float64)\n\n# norm = MinMaxScaler().fit(df_train)\n# df_train = normalization(df_train, norm).astype(np.float64)\n# df_test = normalization(df_test, norm).astype(np.float64)\n\nprint(df_train.shape)\nprint(df_test.shape)\nprint(y_train.shape)","47f3e9c4":"df_train.head(10)","7e5b63d1":"concat = np.c_[df_train.to_numpy(), y_train.reshape(len(y_train), 1)]\ndf = pd.DataFrame(concat, columns=list(df_train.columns) + [\"Label\"]).astype(np.float64)\ncorr = df.corr()\ncmap = sns.diverging_palette(10, 255, as_cmap=True)\n\nplt.figure(figsize=(45, 45))\nplt.subplot(2, 1, 1)\nplt.title(\"Pearson Correlation\")\nax = sns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=1, annot=False, cbar_kws={\"shrink\": .5})\nax.set_ylim(size, 0)\nax.set_xlim(0, size)\nplt.tight_layout()\nplt.show()","072f249c":"columns = df_train.columns\ncolumns = columns[tuple(np.where(columns != \"Label\"))]\npca = PCA(n_components=len(columns))\npca.fit(df_train[columns])\n\nprint(\"PCA (Principal component analysis):\")\nprint(\"Singular Values:\")\nprint(np.round(pca.singular_values_, 5))","360cb8e0":"columns = list(df.columns[corr[\"Label\"].ravel() > 0.05].ravel())\ncolumns += list(df.columns[corr[\"Label\"].ravel() < -0.05].ravel())\ncolumns = np.array(sorted(columns))\ncolumns = columns[tuple(np.where(columns != \"Label\"))]\nprint(\"Columns with strong linear correlation:\")\nprint(len(columns))\nprint(columns)","2f582d20":"pca = PCA(n_components=len(columns))\npca.fit(df_train[columns])\n\nprint(\"PCA (Principal component analysis):\")\nprint(\"Singular Values:\")\nprint(np.round(pca.singular_values_, 5))","13cc422f":"pX_train, pX_test, py_train, py_test = train_test_split(df_train[columns], y_train, test_size=0.33, random_state=42)","ea479e4e":"print(\"Random Forest Regression:\")\nmd = RandomForestRegressor(**{\n    'criterion': 'mse',\n    'max_features': 'auto',\n    'max_leaf_nodes': 500,\n    'n_estimators': 500,\n    'n_jobs': 4,\n    'random_state': 42\n})\nmd.fit(pX_train, py_train)\ny_pred = md.predict(pX_test)\nprint(f\"R^2: {md.score(pX_test, py_test)}\")\nprint(f\"RMSE: {rmse(py_test, y_pred)}\")\nprint(f\"RMSLE: {rmsle(py_test, y_pred)}\")\nprint(f\"Log MSE: {mean_squared_log_error(py_test, y_pred)}\")\nprint(f\"MAE: {mean_absolute_error(py_test, y_pred)}\")","5807d97b":"# print(\"MLP Regression:\")\n# md = MLPRegressor(**{\n#     \"hidden_layer_sizes\": (100, 10),\n#     \"learning_rate\": \"constant\",\n#     \"learning_rate_init\": 1e-3,\n#     \"activation\": \"relu\",\n#     \"alpha\": 1e-2,\n#     \"batch_size\": 32,\n#     \"solver\": \"lbfgs\",\n#     \"max_iter\": 500,\n#     \"random_state\": 42\n# })\n# md.fit(pX_train, py_train)\n# y_pred = md.predict(pX_test)\n# print(f\"R^2: {md.score(pX_test, py_test)}\")\n# print(f\"RMSE: {rmse(py_test, y_pred)}\")\n# print(f\"RMSLE: {rmsle(py_test, y_pred)}\")\n# print(f\"Log MSE: {mean_squared_log_error(py_test, y_pred)}\")\n# print(f\"MAE: {mean_absolute_error(py_test, y_pred)}\")","7464f1d8":"pca = PCA(n_components=30)\nx_pca = pca.fit_transform(df_train[columns])\nprint(\"PCA (Principal component analysis):\")\nprint(\"Singular Values:\")\nprint(np.round(pca.singular_values_, 5))\n\npX_train, pX_test, py_train, py_test = train_test_split(x_pca, y_train, test_size=0.33, random_state=42)","4698f9ed":"print(\"Random Forest Regression:\")\nmd = RandomForestRegressor(**{\n    'criterion': 'mse',\n    'max_features': 'auto',\n    'max_leaf_nodes': 500,\n    'n_estimators': 500,\n    'n_jobs': 4,\n    'random_state': 42\n})\nmd.fit(pX_train, py_train)\ny_pred = md.predict(pX_test)\nprint(f\"R^2: {md.score(pX_test, py_test)}\")\nprint(f\"RMSE: {rmse(py_test, y_pred)}\")\nprint(f\"RMSLE: {rmsle(py_test, y_pred)}\")\nprint(f\"Log MSE: {mean_squared_log_error(py_test, y_pred)}\")\nprint(f\"MAE: {mean_absolute_error(py_test, y_pred)}\")","19f61698":"# print(\"MLP Regression:\")\n# md = MLPRegressor(**{\n#     \"hidden_layer_sizes\": (100, 10),\n#     \"learning_rate\": \"constant\",\n#     \"learning_rate_init\": 1e-3,\n#     \"activation\": \"relu\",\n#     \"alpha\": 1e-4,\n#     \"batch_size\": 32,\n#     \"solver\": \"lbfgs\",\n#     \"max_iter\": 500,\n#     \"random_state\": 42\n# })\n# md.fit(pX_train, py_train)\n# y_pred = md.predict(pX_test)\n# print(f\"R^2: {md.score(pX_test, py_test)}\")\n# print(f\"RMSE: {rmse(py_test, y_pred)}\")\n# print(f\"RMSLE: {rmsle(py_test, y_pred)}\")\n# print(f\"Log MSE: {mean_squared_log_error(py_test, y_pred)}\")\n# print(f\"MAE: {mean_absolute_error(py_test, y_pred)}\")","14f693ae":"md = RandomForestRegressor(**{\n    'criterion': 'mse',\n    'max_features': 'auto',\n    'max_leaf_nodes': 500,\n    'n_estimators': 2500,\n    'n_jobs': 4,\n    'random_state': 42\n})\nmd.fit(df_train[columns], y_train)\ny_pred = np.round(md.predict(df_test[columns]).ravel(), 3)\n\n# md = MLPRegressor(**{\n#     \"hidden_layer_sizes\": (200, 5),\n#     \"learning_rate\": \"adaptive\",\n#     \"learning_rate_init\": 1e-1,\n#     \"activation\": \"relu\",\n#     \"alpha\": 1e-5,\n#     \"batch_size\": 32,\n#     \"solver\": \"lbfgs\",\n#     \"max_iter\": 2000,\n#     \"random_state\": 42\n# })\n# md.fit(x_pca, y_train)\n# y_pred = np.round(md.predict(t_pca).ravel(), 3)\n\ni_df_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsub = pd.DataFrame()\nsub[\"Id\"] = i_df_test[\"Id\"]\nsub[\"SalePrice\"] = y_pred.astype(np.int)\nsub.to_csv('submission.csv', index=False)","d7395cd6":"Remove columns that have low correlation with the final price.","252a363b":"Doing some Person Correlation on the data, to see Linear relations between each columns and the final price.","16b7258a":"Let's do some PCA to see the energy of each column...","1d40dd8d":"Given the above, let's run our Regressors on the Correlation and on small dataset using PCA...","83dab3bf":"Let's apply PCA on those new selected columns to see if we still have lot's of energy."}}