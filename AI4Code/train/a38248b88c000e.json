{"cell_type":{"5c7305e4":"code","d5e74e86":"code","eeb66943":"code","85c4c46f":"code","f1b88e84":"code","f70b1f72":"code","54687eb9":"code","5fad0d69":"code","e020b052":"code","fa672049":"code","d28fc34a":"code","bc221ff3":"code","24962ef5":"code","aef9b2e0":"code","01653163":"code","ef7a1ae4":"code","2beac282":"code","8673483d":"code","3f217fe6":"code","08de4fc0":"code","db52e656":"code","1647aada":"code","0efb8229":"code","73e082a7":"code","192b72ab":"code","6ae91d25":"code","0a301c5c":"code","22e609a2":"code","ebef91ac":"code","462acad0":"code","0c1b825c":"code","615c5570":"code","5e48688a":"code","119881cf":"code","a0f68ca7":"code","1321b341":"code","03ac7618":"code","41a58a75":"code","68c1f45d":"code","9354f131":"code","88cad230":"code","3ce2424f":"code","0dec64ed":"code","158a3ae3":"code","9ea9ab32":"code","88a0646a":"code","58501676":"code","8b66371f":"code","802f94b1":"code","b685b14d":"code","66362c11":"code","2c928e90":"code","267864c8":"code","99e15faf":"code","c90fff92":"markdown","23cc945e":"markdown","80df343d":"markdown","3e767540":"markdown","2112b741":"markdown","61ce6de0":"markdown","9e901ffe":"markdown","768fc201":"markdown","879ff480":"markdown","d7da1130":"markdown","ee536d9e":"markdown","9f9e2351":"markdown","84adb83d":"markdown","2c3c70d2":"markdown","78a1135d":"markdown","4c613cf5":"markdown","dcd25c6f":"markdown","4e586c28":"markdown","a94cb2da":"markdown","244d15d9":"markdown","069eff70":"markdown","6e4c5aa8":"markdown","e908860d":"markdown","57ae8bb9":"markdown","27f70ea6":"markdown","5ec7d267":"markdown","59f2c164":"markdown"},"source":{"5c7305e4":"import pandas as pd \nimport numpy as np \nimport tensorflow as tf\nimport matplotlib.pyplot as plt \nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nimport os\nimport time\nimport math","d5e74e86":"PATH = \"..\/input\/applications-of-deep-learning-wustl-spring-2021b\/mc\/\"\nPATH_TRAIN = os.path.join(PATH, \"train.csv\")\nPATH_TEST = os.path.join(PATH, \"test.csv\")","eeb66943":"df_train = pd.read_csv(PATH_TRAIN)\ndf_test = pd.read_csv(PATH_TEST)\ndf_train['fake'] = df_train['fake'].astype('str')\ndf_train.head()","85c4c46f":"df_test.head()","f1b88e84":"fig,ax = plt.subplots(figsize=(8,5))\nax.bar(df_train.fake.value_counts().index,df_train.fake.value_counts().values,width=0.5)\nax.set_title('Target')","f70b1f72":"img = Image.open('..\/input\/applications-of-deep-learning-wustl-spring-2021b\/mc\/mc-1.jpg')\n#img = img.resize((512,512),Image.ANTIALIAS)\nimg_arr = np.asarray(img)\nfig,ax = plt.subplots(figsize=(6,6))\nax.imshow(img_arr)\nax.set_title('Original Figure')","54687eb9":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            horizontal_flip = True,\n            vertical_flip = True,\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img_arr,axis=0), batch_size=1)\n\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Horizonal\/Vertical Flipped')","5fad0d69":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            width_shift_range=0.1, \n            height_shift_range=0.1,\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img_arr,axis=0), batch_size=1)\n\nfig, ax = plt.subplots(nrows=1,ncols=3,figsize=(15,15))\n# generate batch of images\nfor i in range(3):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Vertical\/Horizontal Shift')","e020b052":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            rotation_range = 45,\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img_arr,axis=0), batch_size=1)\n\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Rotation')","fa672049":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            brightness_range = [0.8,1.1],\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img_arr,axis=0), batch_size=1)\n\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Brighten')","d28fc34a":"img2 = Image.open('..\/input\/applications-of-deep-learning-wustl-spring-2021b\/mc\/mc-1000.jpg')\n#img = img.resize((IMAGE_WIDTH,IMAGE_HEIGHT),Image.ANTIALIAS)\nimg2_arr = np.asarray(img2)\nfig,ax = plt.subplots(figsize=(6,6))\nax.imshow(img2_arr)\nax.set_title('Original Figure')","bc221ff3":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            brightness_range = [0.8,1.1],\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img2_arr,axis=0), batch_size=1)\n\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Brighten')","24962ef5":"datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            zoom_range = [0.5,1.5],\n            fill_mode = 'nearest')\naug_iter = datagen.flow(np.expand_dims(img_arr,axis=0), batch_size=1)\n\nfig, ax = plt.subplots(nrows=1,ncols=5,figsize=(15,15))\n# generate batch of images\nfor i in range(5):\n    image = next(aug_iter)[0].astype('uint8')\n    ax[i].imshow(image)\n    ax[i].axis('off')\n    ax[i].set_title('Zoom')","aef9b2e0":"train_X,val_X,train_y,val_y = train_test_split(df_train.drop(columns='fake',axis=1),\n                                               df_train['fake'],\n                                               test_size=0.2,\n                                               shuffle=True,\n                                               random_state=98)\ndf_train_cut = pd.concat([train_X,train_y],axis=1)\ndf_val_cut = pd.concat([val_X,val_y],axis=1)","01653163":"def DataGeneration(Height, Width, BatchSize):\n    start_time = time.time()\n    training_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n                rescale = 1.\/255,\n                horizontal_flip = True,\n                #vertical_flip = True,\n                #rotation_range = 30,\n                #brightness_range = [0.8,1.5],\n                #zoom_range = [0.8,1.2],\n                fill_mode = 'nearest')\n    train_generator = training_datagen.flow_from_dataframe(\n            dataframe = df_train_cut,\n            directory = PATH,\n            x_col = \"filename\",\n            y_col = \"fake\",\n            target_size = (Height, Width),\n            batch_size = BatchSize,\n            class_mode = 'binary',\n            shuffle = True,\n            seed = 98)\n\n    validation_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n                rescale = 1.\/255)\n    val_generator = validation_datagen.flow_from_dataframe(\n            dataframe = df_val_cut,\n            directory = PATH,\n            x_col = \"filename\",\n            y_col = \"fake\",\n            target_size = (Height, Width),\n            batch_size = BATCH_SIZE,\n            class_mode = 'binary',\n            shuffle = True,\n            seed = 98)\n\n    print(f'----{time.time()-start_time} seconds----')\n    return train_generator, val_generator","ef7a1ae4":"## Warmup with Expotential Decay Learning Rate Scheduler\ndef scheduler(epoch):\n    if epoch<= WARMUP_EPOCH:\n        lr = INITIAL_LEARNINGRATE *epoch\/WARMUP_EPOCH\n    \n    else:\n        lr = INITIAL_LEARNINGRATE * DECAY_RATE**(epoch-WARMUP_EPOCH)\n    \n    return lr","2beac282":"HEIGHT = 128\nWIDTH = 128\nBATCH_SIZE = 128\n\ntrain_generator, val_generator = DataGeneration(Height = HEIGHT, \n                                                Width = WIDTH,\n                                                BatchSize = BATCH_SIZE)","8673483d":"train_generator[0][0][0].shape","3f217fe6":"tf.keras.backend.clear_session()","08de4fc0":"EPOCHS = 50\n#STEPS_PER_EPOCH = train_generator.n\/\/BATCH_SIZE\nSTEPS_PER_EPOCH = 50\n#VALIDATION_STEPS = val_generator.n\/\/BATCH_SIZE\nVALIDATION_STEPS = 25\nLEARNING_RATE = 1e-3\n\nmodel = tf.keras.models.Sequential([\n\n    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(HEIGHT, WIDTH, 3)),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    #tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    #tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    #tf.keras.layers.Dropout(0.5),\n    #tf.keras.layers.BatchNormalization(),\n    #tf.keras.layers.MaxPooling2D(2,2),\n    \n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","db52e656":"model.compile(loss = 'binary_crossentropy', \n              optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n              metrics = 'accuracy')\nmonitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                           min_delta=1e-4, \n                                           patience=10, \n                                           verbose=1, \n                                           mode='min',\n                                           restore_best_weights=True)\n#lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\nlr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n                                                    factor=0.2,\n                                                    patience=5,\n                                                    verbose=1,\n                                                    mode='min',\n                                                    min_delta=1e-4,\n                                                    cooldown=0,\n                                                    min_lr=1e-5,)\n\ncheckpoint = tf.keras.callbacks.ModelCheckpoint('CNN_best_weight.h5', \n                                                monitor='val_loss', \n                                                verbose=1, \n                                                save_best_only=True, \n                                                save_weights_only=True, \n                                                mode='min',\n                                                save_freq = 'epoch')\n\nhistory = model.fit(train_generator, \n                    epochs=EPOCHS, \n                    steps_per_epoch=STEPS_PER_EPOCH, \n                    validation_data = val_generator, \n                    verbose = 1, \n                    validation_steps=VALIDATION_STEPS,\n                    callbacks = [monitor,lr_scheduler,checkpoint])","1647aada":"def BuildModel(ModelName):\n    print('--------------Building The Model...--------------')\n    if ModelName == 'ResNet50' or ModelName == 'Xception':\n        if ModelName == 'ResNet50':\n            base_model = tf.keras.applications.ResNet50(include_top=False,\n                                                    weights='imagenet',\n                                                    input_shape=(HEIGHT,WIDTH,3))\n        \n        elif ModelName == 'Xception':\n            base_model = tf.keras.applications.Xception(include_top=False,\n                                                weights='imagenet',\n                                                input_shape=(HEIGHT,WIDTH,3))\n        \n        base_model.trainable = True\n        print(\"\\nNumber of layers in the base model: \", len(base_model.layers))\n        #print(base_model.layers[0].output_shape)\n        x = base_model.output\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x = tf.keras.layers.Dense(1024,activation='relu')(x)\n        #x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dense(512,activation='relu')(x)\n        #x = tf.keras.layers.Dropout(0.5)(x)\n        x = tf.keras.layers.Dense(64,activation='relu')(x)\n        out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n        model = tf.keras.models.Model(inputs=base_model.input, outputs=out)\n    \n    elif ModelName == 'EfficientNetB0':\n        base_model = tf.keras.applications.EfficientNetB0(include_top=False,\n                                                weights='imagenet',\n                                                input_shape=(HEIGHT,WIDTH,3))\n        base_model.trainable = True\n        print(\"\\nNumber of layers in the base model: \", len(base_model.layers))\n        #print(base_model.layers[0].output_shape)\n        x = base_model.output\n        x = tf.keras.layers.GlobalAveragePooling2D()(x)\n        x = tf.keras.layers.Dense(512,activation='relu')(x)\n        #x = tf.keras.layers.BatchNormalization()(x)\n        x = tf.keras.layers.Dense(64,activation='relu')(x)\n        out = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n        model = tf.keras.models.Model(inputs=base_model.input, outputs=out)\n    \n    print('\\n--------------Done!--------------')\n    return model","0efb8229":"def ModelDeployment(model, LearningRate, ModelName,  NbOfEpoch, StepsPerEpoch, ValidationSteps):\n    ## Freeze all the layers before the `fine_tune_at` layer\n    #for layer in base_model.layers[:fine_tune_at]:\n    #    layer.trainable =  False\n    print('--------------Deploying the Model...--------------')\n    model.compile(loss = 'binary_crossentropy', \n                  optimizer=tf.keras.optimizers.Adam(learning_rate=LearningRate),\n                  metrics = 'accuracy')\n    monitor = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                               min_delta=1e-4, \n                                               patience=10, \n                                               verbose=1, \n                                               mode='min',\n                                               restore_best_weights=True)\n    #lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose=1)\n    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n                                                        factor=0.5,\n                                                        patience=3,\n                                                        verbose=1,\n                                                        mode='min',\n                                                        min_delta=1e-4,\n                                                        cooldown=0,\n                                                        min_lr=MIN_LR,)\n\n    filepath= ModelName+\"-{epoch:02d}-{val_loss:.4f}.hdf5\"\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath, \n                                                    monitor='val_loss', \n                                                    verbose=1, \n                                                    save_best_only=False, \n                                                    save_weights_only=True, \n                                                    mode='min',\n                                                    save_freq = 'epoch')\n    print('--------------Deployed Successfully--------------')\n    print('--------------Training Begins--------------')\n    history = model.fit(train_generator, \n                            epochs = NbOfEpoch, \n                            steps_per_epoch = StepsPerEpoch, \n                            validation_data = val_generator, \n                            verbose = 1, \n                            validation_steps = ValidationSteps,\n                            callbacks = [monitor,lr_scheduler,checkpoint])\n    return history","73e082a7":"def HistoryPlotting(history):\n    fig,ax = plt.subplots(figsize=(8,5))\n    ax.plot(history.history['loss'],label='train',color='blue')\n    ax.plot(history.history['val_loss'],label='val',color='green')\n    ax.set_title('model loss',fontsize=14)\n    #ax.set_yticks()\n    ax.set_ylabel('loss')\n    ax.set_xlabel('epoch')\n    #ax.set_xticks(range(0,20))\n    ax.legend(loc='upper right',fontsize=13)","192b72ab":"HEIGHT = 256\nWIDTH = 256\nBATCH_SIZE = 32\n\ntrain_generator, val_generator = DataGeneration(Height = HEIGHT, \n                                                Width = WIDTH,\n                                                BatchSize = BATCH_SIZE)","6ae91d25":"tf.keras.backend.clear_session()","0a301c5c":"Resnet_based_model = BuildModel(ModelName='ResNet50')\nResnet_based_model.summary()","22e609a2":"STEPS_PER_EPOCH = train_generator.n\/\/BATCH_SIZE\n#STEPS_PER_EPOCH = 200\nVALIDATION_STEPS = val_generator.n\/\/BATCH_SIZE\n#VALIDATION_STEPS = 50\nLEARNING_RATE = 1e-5\nNB_EPOCH = 50\n#FINETUNE_LEARNING_RATE = LEARNING_RATE\/10\n#MIN_LR = 1e-8 \n#fine_tune_at = 200\nResnet50_history = ModelDeployment(model = Resnet_based_model, \n                                   LearningRate = LEARNING_RATE, \n                                   ModelName = 'ResNet50',  \n                                   NbOfEpoch = NB_EPOCH, \n                                   StepsPerEpoch = STEPS_PER_EPOCH, \n                                   ValidationSteps = VALIDATION_STEPS)","ebef91ac":"tf.keras.backend.clear_session()","462acad0":"HEIGHT = 256\nWIDTH = 256\nBATCH_SIZE = 32\n\ntrain_generator, val_generator = DataGeneration(Height = HEIGHT, \n                                                Width = WIDTH,\n                                                BatchSize = BATCH_SIZE)","0c1b825c":"Xception_based_model = BuildModel(ModelName='Xception')\nXception_based_model.summary()","615c5570":"STEPS_PER_EPOCH = train_generator.n\/\/BATCH_SIZE\n#STEPS_PER_EPOCH = 200\nVALIDATION_STEPS = val_generator.n\/\/BATCH_SIZE\n#VALIDATION_STEPS = 50\nLEARNING_RATE = 1e-5\nNB_EPOCH = 50\n\nXception_history = ModelDeployment(model = Xception_based_model, \n                                   LearningRate = LEARNING_RATE, \n                                   ModelName = 'Xception',  \n                                   NbOfEpoch = NB_EPOCH, \n                                   StepsPerEpoch = STEPS_PER_EPOCH, \n                                   ValidationSteps = VALIDATION_STEPS)","5e48688a":"tf.keras.backend.clear_session()","119881cf":"HEIGHT = 256\nWIDTH = 256\nBATCH_SIZE = 32\n\ntrain_generator, val_generator = DataGeneration(Height = HEIGHT, \n                                                Width = WIDTH,\n                                                BatchSize = BATCH_SIZE)","a0f68ca7":"EfficientNet_based_model = BuildModel(ModelName='EfficientNetB0')\nEfficientNet_based_model.summary()","1321b341":"STEPS_PER_EPOCH = train_generator.n\/\/BATCH_SIZE\n#STEPS_PER_EPOCH = 200\nVALIDATION_STEPS = val_generator.n\/\/BATCH_SIZE\n#VALIDATION_STEPS = 50\nLEARNING_RATE = 1e-5\nNB_EPOCH = 50\n\nEfficientNet_history = ModelDeployment(model = EfficientNet_based_model, \n                                   LearningRate = LEARNING_RATE, \n                                   ModelName = 'EfficientNetB0',  \n                                   NbOfEpoch = NB_EPOCH, \n                                   StepsPerEpoch = STEPS_PER_EPOCH, \n                                   ValidationSteps = VALIDATION_STEPS)","03ac7618":"val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        rescale = 1.\/255)\nval_generator = val_datagen.flow_from_dataframe(\n        dataframe = df_val_cut,\n        directory = PATH,\n        x_col=\"filename\",\n        target_size = (HEIGHT, WIDTH),\n        batch_size = 1,\n        shuffle = False,\n        class_mode = None)\n## very important\nval_generator.reset()","41a58a75":"Resnet_based_model = BuildModel(ModelName='ResNet50')\nXception_based_model = BuildModel(ModelName='Xception')\nEfficientNet_based_model = BuildModel(ModelName='EfficientNetB0')","68c1f45d":"Resnet_based_model.load_weights('..\/input\/resnet50bestweight2\/ResNet50-02-0.1628.hdf5')\nXception_based_model.load_weights('..\/input\/xceptionbestweight\/Xception-02-0.14.hdf5')\nEfficientNet_based_model.load_weights('..\/input\/efficientnetb0bestweight\/EfficientNetB0-24-0.1663.hdf5')\n\nResnet_val_pred = Resnet_based_model.predict(val_generator,steps=len(df_val_cut))\nXception_val_pred = Xception_based_model.predict(val_generator,steps=len(df_val_cut))\nEfficientNet_val_pred = EfficientNet_based_model.predict(val_generator,steps=len(df_val_cut))","9354f131":"val_y = df_val_cut['fake'].astype(np.float32).values\nResnet_val_logloss = log_loss(val_y,Resnet_val_pred.flatten())\nXception_val_logloss = log_loss(val_y,Xception_val_pred.flatten())\nEfficientNet_val_logloss = log_loss(val_y,EfficientNet_val_pred.flatten())\nprint(f'Before Clipping,\\nResnet_Logloss: {Resnet_val_logloss}\\nXception_Logloss: {Xception_val_logloss}\\nEfficientNet_Logloss: {EfficientNet_val_logloss}')","88cad230":"ensemble_pred_1 = 0.3*Resnet_val_pred + 0.4*Xception_val_pred+0.3*EfficientNet_val_pred\nprint(f'Before Clipping, Blending_Logloss: {log_loss(val_y,ensemble_pred_1)}')","3ce2424f":"clipped_ensemble_pred_1 = np.clip(ensemble_pred_1,0.03,0.97)\nprint(f'After Clipping, Blending_Logloss: {log_loss(val_y,clipped_ensemble_pred_1)}')","0dec64ed":"Resnet_clipped_val_pred = np.clip(Resnet_val_pred.flatten(),0.04,0.96)\nXception_clipped_val_pred = np.clip(Xception_val_pred.flatten(),0.03,0.97)\nEfficientNet_clipped_val_pred = np.clip(EfficientNet_val_pred.flatten(),0.04,0.96)\n\nResnet_val_logloss = log_loss(val_y,Resnet_clipped_val_pred)\nXception_val_logloss = log_loss(val_y,Xception_clipped_val_pred)\nEfficientNet_val_logloss = log_loss(val_y,EfficientNet_clipped_val_pred)\nprint(f'After Clipping,\\nResnet_Logloss: {Resnet_val_logloss}\\nXception_Logloss: {Xception_val_logloss}\\nEfficientNet_Logloss: {EfficientNet_val_logloss}')","158a3ae3":"ensemble_pred_2 = 0.3*Resnet_clipped_val_pred + 0.4*Xception_clipped_val_pred + 0.3*EfficientNet_clipped_val_pred\n#ensemble_pred_2 = (Resnet_clipped_val_pred + Xception_clipped_val_pred + EfficientNet_clipped_val_pred)\/3\nlog_loss(val_y,ensemble_pred_2)","9ea9ab32":"pred_class = np.round(ensemble_pred.flatten())\nwrong_idx = np.argwhere(val_y!=pred_class).flatten()\ndf_val_cut = df_val_cut.reset_index(drop=True)\nval_image_paths = [os.path.join(PATH, filename) for filename in df_val_cut.filename[wrong_idx[:10]]]","88a0646a":"fig,ax = plt.subplots(nrows=2,ncols=5,figsize=(25,10))\nfor idx,path in enumerate(val_image_paths):\n    img = Image.open(path)\n    #img = img.resize((IMAGE_WIDTH,IMAGE_HEIGHT),Image.ANTIALIAS)\n    img_arr = np.asarray(img)\n    ax[divmod(idx,5)[0],divmod(idx,5)[1]].imshow(img_arr)\n    ax[divmod(idx,5)[0],divmod(idx,5)[1]].set_title(f'Pred:{pred_class[wrong_idx[idx]]}  Real:{1-pred_class[wrong_idx[idx]]}')\n    ax[divmod(idx,5)[0],divmod(idx,5)[1]].axis('off')","58501676":"start_time = time.time()\ntest_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        rescale = 1.\/255)\ntest_generator = test_datagen.flow_from_dataframe(\n        dataframe = df_test,\n        directory = PATH,\n        x_col=\"filename\",\n        target_size = (HEIGHT, WIDTH),\n        batch_size = 1,\n        shuffle = False,\n        class_mode = None)\nprint(f'----{time.time()-start_time} seconds----')\n## very important\ntest_generator.reset()","8b66371f":"#Xception_based_model.load_weights('.\/Xception-02-0.16.hdf5')\n#Resnet_based_model.load_weights('.\/ResNet50-02-0.1628.hdf5')\n#EfficientNet_based_model.load_weights('..\/input\/efficientnetb0bestweight\/EfficientNetB0-24-0.1663.hdf5')\n\nResnet_pred = Resnet_based_model.predict(test_generator,steps=len(df_test))\nXception_pred = Xception_based_model.predict(test_generator,steps=len(df_test))\nEfficientNet_pred = EfficientNet_based_model.predict(test_generator,steps=len(df_test))","802f94b1":"#ensemble_pred = 0.3*Resnet_pred + 0.4*Xception_pred + 0.3*EfficientNet_pred\n#clipped_ensemble_pred = np.clip(ensemble_pred,0.03,0.97)","b685b14d":"Resnet_clipped_pred = np.clip(Resnet_pred.flatten(),0.04,0.96)\nXception_clipped_pred = np.clip(Xception_pred.flatten(),0.03,0.97)\nEfficientNet_clipped_pred = np.clip(EfficientNet_pred.flatten(),0.04,0.96)\n\nensemble_pred = 0.3*Resnet_clipped_pred + 0.4*Xception_clipped_pred + 0.3*EfficientNet_clipped_pred\n#ensemble_pred = (Resnet_clipped_pred + Xception_clipped_pred + EfficientNet_clipped_pred)\/3","66362c11":"test_image_paths = [os.path.join(PATH, filename) for filename in df_test.filename[:10]]\nfig,ax = plt.subplots(nrows=2,ncols=5,figsize=(25,10))\nfor idx,path in enumerate(test_image_paths):\n    img = Image.open(path)\n    #img = img.resize((IMAGE_WIDTH,IMAGE_HEIGHT),Image.ANTIALIAS)\n    img_arr = np.asarray(img)\n    ax[divmod(idx,5)[0],divmod(idx,5)[1]].imshow(img_arr)\n    #ax[divmod(idx,5)[0],divmod(idx,5)[1]].set_title(np.round(ensemble_pred.flatten())[idx])\n    ax[divmod(idx,5)[0],divmod(idx,5)[1]].set_title(np.round(Xception_pred.flatten())[idx])\n    ax[divmod(idx,5)[0],divmod(idx,5)[1]].axis('off')","2c928e90":"#df_submit = pd.DataFrame({\"id\":df_test['id'],'fake':pred.flatten()})\n#df_submit['fake'] = np.clip(df_submit['fake'],0.04,0.96)\n#df_submit = pd.DataFrame({\"id\":df_test['id'],'fake':Resnet_pred.flatten()})\ndf_submit = pd.DataFrame({\"id\":df_test['id'],'fake':ensemble_pred.flatten()})\n#df_submit = pd.DataFrame({\"id\":df_test['id'],'fake': clipped_ensemble_pred.flatten()})\ndf_submit.to_csv(\"submision.csv\",index = False)","267864c8":"np.min(df_submit['fake']),np.max(df_submit['fake'])","99e15faf":"df_submit.head(20)","c90fff92":"Take a look at first ten misclassified pictures.","23cc945e":"## EfficientNet","80df343d":"## ResNet50","3e767540":"## Clipping + Blending","2112b741":"## Rotation","61ce6de0":"Not sure. But worth a try.","9e901ffe":"# Transfer Learning","768fc201":"#  Submission","879ff480":"## Zoom ","d7da1130":"## Horizontal\/Vertical Shift","ee536d9e":"Relatively balanced dataset","9f9e2351":"Visualize the validation result and test different clipping strategy.","84adb83d":"## Xception","2c3c70d2":"## Horizontal\/Vertical Flip","78a1135d":"# Data Preparation","4c613cf5":"# Validation Evaluation","dcd25c6f":"Let's see how brighten affect dark images","4e586c28":"# Data (Image Augmentation) Visualization","a94cb2da":"# CNN Stater","244d15d9":"## Original Picture","069eff70":"Seems not bad","6e4c5aa8":"clipping strategy:\n\n(0.03,0.97) for ResNet50 & Xception, (0.04,0.96) for EfficientNetB0","e908860d":"Horizontal Flipping could be a good augmentation method.","57ae8bb9":"## Blending + Clipping","27f70ea6":"Take a loot at first ten test images prediction result","5ec7d267":"## Brightness","59f2c164":"CNN Model from scratch is trained to compare the effects of implementing different real-time image augmentation methods because it is fast to train. Thanks for the professor's starter notebook."}}