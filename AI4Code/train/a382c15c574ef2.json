{"cell_type":{"80a1a944":"code","eb89662b":"code","de597ee1":"code","6af56cad":"code","4d2489f2":"code","ca2ff03d":"code","13ef475f":"code","fdc76d18":"code","7a658dac":"code","7d218a33":"code","1aef25a7":"code","b8814577":"code","e2ccc53d":"code","4217eb97":"code","febc5123":"code","1183db35":"code","46ee4b53":"markdown","933d15b5":"markdown","91a82481":"markdown","9fb773e6":"markdown","8fa74f62":"markdown","301bc782":"markdown","789d884b":"markdown","c062689f":"markdown","48202227":"markdown","53fcddeb":"markdown","7d1fb34b":"markdown","1fdfb8e1":"markdown","217ac275":"markdown","f7736310":"markdown"},"source":{"80a1a944":"import numpy as np \nimport pandas as pd\nimport os\nimport zipfile\nimport cv2\nfrom matplotlib import pyplot as plt\nimport shutil \nfrom tqdm import tqdm\nimport torch\nimport torchvision\nimport time\nimport copy\nfrom torchvision import transforms, models","eb89662b":"print(os.listdir('..\/input'),'\\n') \nwith zipfile.ZipFile('..\/input\/platesv2\/plates.zip', 'r') as zip_obj:\n        zip_obj.extractall('\/kaggle\/working\/') \nprint(os.listdir('\/kaggle\/working\/'))\n\ndata_root = '\/kaggle\/working\/plates\/' \nprint(data_root)\nprint(os.listdir(data_root))","de597ee1":"class Remove_background_and_crop:\n    \n    def __init__(self,img):\n        self.x00 = 0\n        self.x00 = 0\n        self.r00 = 0\n        self.img = img\n        self.mask = img\n                \n    def crop (self): \n            c_r_crop = (1.42*self.r00\/2)\n            self.img = self.img[int(self.y00)-int(c_r_crop):int(self.y00)+int(c_r_crop),int(self.x00)-int(c_r_crop):int(self.x00)+int(c_r_crop)]\n            self.img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n            crop = self.img\n            cv2.imwrite(image_folder,crop)\n            h,w = self.img.shape[:2] \n            c = min(h,w)     \n            for i in range (5,int(c\/3),5): \n                    crop_img = self.img[i:h-i,i:w-i]    \n                    cv2.imwrite(image_folder[:-4] +  '_Crop_' + str(i) + '.jpg',crop_img)\n            image1 = self.img[0:int(h\/\/2),0:int(w\/\/2)]\n            #\u0414\u0435\u043b\u0438\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0430 4 \u0447\u0430\u0441\u0442\u0438\n            cv2.imwrite(image_folder[:-4] +  'image1' + '.jpg',image1)\n            image2 = self.img[int(h\/\/2):h,int(w\/\/2):w]\n            cv2.imwrite(image_folder[:-4] +  'image2' + '.jpg',image2)\n            image3 = self.img[int(h\/\/2):h,0:int(w\/\/2)]\n            cv2.imwrite(image_folder[:-4] +  'image3' + '.jpg',image3)\n            image4 = self.img[0:int(h\/\/2),int(w\/\/2):w]\n            cv2.imwrite(image_folder[:-4] +  'image4' + '.jpg',image4)\n            \n    def crop_test (self):\n        c_r_crop = (1.42*self.r00\/2)\n        self.img = self.img[int(self.y00)-int(c_r_crop):int(self.y00)+int(c_r_crop),int(self.x00)-int(c_r_crop):int(self.x00)+int(c_r_crop)]\n        self.img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n        cv2.imwrite(image_folder,self.img)       \n            \n    def find_circle(self):\n        output = self.img.copy()    \n        img = cv2.convertScaleAbs(self.img, alpha=1.2, beta=0.0)\n        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, 1, 10,param1=10,param2=5,minRadius=40,maxRadius=250)\n        \n        if circles is not None: \n            #print('\u041a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0446\u0435\u043d\u0442\u0440\u0430:',self.x00,self.y00)\n            circles = np.round(circles[0, :]).astype(\"int\")\n            #print('\u0412\u043e\u0437\u043c\u043e\u0436\u043d\u044b\u0435 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0446\u0435\u043d\u0442\u0440\u0430 \u0438 \u0440\u0430\u0434\u0438\u0443\u0441\u044b:')\n            #print(circles) \n                             \n            for x, y, r in circles:\n                if ((self.x00-15)<x<(self.x00+15)) and ((self.y00-15)<y<(self.y00+15)):\n                    if r > self.r00: \n                        self.x00 = x\n                        self.y00 = y\n                        self.r00 = r\n                        #print('\u041d\u0430\u0439\u0434\u0435\u043d\u043e \u0441\u043e\u0432\u043f\u0430\u0434\u0435\u043d\u0438\u0435:',x,y,r)\n                    \n            if self.r00==0: \n                #print('\u0422\u0430\u0440\u0435\u043b\u043a\u0430 \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u043d\u0430','\\n')\n                ret,thresh = cv2.threshold(self.mask,235,255,0)\n                contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)   \n                c = max(contours, key=cv2.contourArea)\n                (self.x00, self.y00), self.r00 = cv2.minEnclosingCircle(c)\n            #print('\u0412\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u0435 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0446\u0435\u043d\u0442\u0440\u0430:',self.x00,self.y00,self.r00)\n            \n    def findCoordinates(self):  \n        ret,thresh = cv2.threshold(self.mask,235,255,0)        \n        M = cv2.moments(thresh)\n        self.x00 = int(M[\"m10\"] \/ M[\"m00\"])\n        self.y00 = int(M[\"m01\"] \/ M[\"m00\"])\n        \n#    def findCenter(self):\n#        c_r_crop = 124\n#        self.img = self.img[int(self.y00)-int(c_r_crop):int(self.y00)+int(c_r_crop),int(self.x00)-int(c_r_crop):int(self.x00)+int(c_r_crop)]\n                \n    def remove_background(self):  \n        mainRectSize = .08\n        fgSize = .01\n        img = self.img\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        new_h, new_w = img.shape[:2]\n        mask = np.zeros(img.shape[:2], np.uint8)\n        bg_w = round(new_w * mainRectSize)\n        bg_h = round(new_h * mainRectSize)\n        bg_rect = (bg_w, bg_h, new_w - bg_w, new_h - bg_h)\n        fg_w = round(new_w * (1 - fgSize) \/ 2)\n        fg_h = round(new_h * (1 - fgSize) \/ 2)\n        fg_rect = (fg_w, fg_h, new_w - fg_w, new_h - fg_h)\n        cv2.rectangle(mask, fg_rect[:2], fg_rect[2:4], color=cv2.GC_FGD, thickness=-1)\n        bgdModel1 = np.zeros((1, 65), np.float64)\n        fgdModel1 = np.zeros((1, 65), np.float64)\n        cv2.grabCut(img, mask, bg_rect, bgdModel1, fgdModel1, 3, cv2.GC_INIT_WITH_RECT)\n        cv2.rectangle(mask, bg_rect[:2], bg_rect[2:4], color=cv2.GC_PR_BGD, thickness=bg_w * 3)\n        cv2.grabCut(img, mask, bg_rect, bgdModel1, fgdModel1, 10, cv2.GC_INIT_WITH_MASK)   \n        mask_result = np.where((mask == 1) + (mask == 3), 255, 0).astype('uint8')\n        masked = cv2.bitwise_and(img, img, mask=mask_result)\n        masked[mask_result < 2] = [255, 255, 255] \n        self.img = masked\n        self.mask = mask_result\n    \nfor image_index in range (20):\n    print (\"Complete dirty: \",\"{0:04}\".format(image_index),\"\/0019\", end=\"\\r\")\n    image_folder = '\/kaggle\/working\/plates\/train\/dirty\/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = Remove_background_and_crop(img)\n    out_img.remove_background()\n    out_img.findCoordinates()\n    out_img.find_circle()\n    out_img.crop()    \nprint (\"\\n\\r\", end=\"\")    \n    \nfor image_index in range (20):\n    print (\"Complete cleaned: \",\"{0:04}\".format(image_index),\"\/0019\", end=\"\\r\")\n    image_folder = '\/kaggle\/working\/plates\/train\/cleaned\/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = Remove_background_and_crop(img)\n    out_img.remove_background()\n    out_img.findCoordinates()\n    out_img.find_circle()\n    out_img.crop()\nprint (\"\\n\\r\", end=\"\")\n\nfor image_index in range (744):\n    print (\"Complete test: \",\"{0:04}\".format(image_index),\"\/0743\", end=\"\\r\")\n    image_folder = '\/kaggle\/working\/plates\/test\/{0:04}.jpg'.format(image_index) \n    img = cv2.imread(image_folder)\n    out_img  = Remove_background_and_crop(img)\n    out_img.remove_background()\n    out_img.findCoordinates()\n    out_img.find_circle()\n    out_img.crop_test()\nprint (\"\\n\\r\", end=\"\") ","6af56cad":"train_dir = 'train' # \u043d\u0430 \u044d\u0442\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0431\u0443\u0434\u0435\u043c \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c\nval_dir = 'val' #\u043d\u0430 \u044d\u0442\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u0431\u0443\u0434\u0435\u043c \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043a\u0430\u043a\u0443\u044e accuracy \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430\u0448\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \nclass_names = ['cleaned', 'dirty']\n \nfor dir_name in [train_dir, val_dir]:\n    for class_name in class_names:\n        os.makedirs(os.path.join(dir_name, class_name), exist_ok=True)\n \nfor class_name in class_names:\n    source_dir = os.path.join(data_root, 'train', class_name)\n    for i, file_name in enumerate(tqdm(os.listdir(source_dir))):\n        if i % 6 != 0:\n            dest_dir = os.path.join(train_dir, class_name) \n        else:\n            dest_dir = os.path.join(val_dir, class_name)\n        shutil.copy(os.path.join(source_dir, file_name), os.path.join(dest_dir, file_name))","4d2489f2":"train_transforms = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.09, p=0.75, interpolation=3, fill=255),\n    transforms.Resize((224, 224)),    \n    transforms.ColorJitter(hue=(-0.5,0.5)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(), \n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #\u0422\u0440\u0430\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u043a\u0438. \u0418\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u0432\u0438\u0434\u0443, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043e\u0431\u0443\u0447\u0430\u043b\u0441\u044f \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439 ResNet. \u0412\u044b\u0447\u0435\u0442\u0430\u0435\u043c \u043e\u0442 \u043a\u0440\u0430\u0441\u043d\u043e\u0433\u043e, \u0437\u0435\u043b\u0451\u043d\u043e\u0433\u043e \u0438 \u0441\u0438\u043d\u0435\u0433\u043e \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b 0.485, 0.456, 0.406 \u0438 \u0434\u0435\u043b\u0438\u043c \u043d\u0430 0.229, 0.224, 0.225 (\u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u043c\u0430\u043d\u0443\u0430\u043b\u0430 ResNet)\n])\n\nval_transforms = transforms.Compose([\n    transforms.RandomPerspective(distortion_scale=0.1, p=0.8, interpolation=3, fill=255),\n    transforms.Resize((224, 224)),\n    transforms.ColorJitter(hue=(-0.5,0.5)),\n    transforms.RandomHorizontalFlip(),     \n    transforms.RandomVerticalFlip(), \n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) #\u0422\u0440\u0430\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u043a\u0438. \u0418\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u0432\u0438\u0434\u0443, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043e\u0431\u0443\u0447\u0430\u043b\u0441\u044f \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0439 ResNet. \u0412\u044b\u0447\u0435\u0442\u0430\u0435\u043c \u043e\u0442 \u043a\u0440\u0430\u0441\u043d\u043e\u0433\u043e, \u0437\u0435\u043b\u0451\u043d\u043e\u0433\u043e \u0438 \u0441\u0438\u043d\u0435\u0433\u043e \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b 0.485, 0.456, 0.406 \u0438 \u0434\u0435\u043b\u0438\u043c \u043d\u0430 0.229, 0.224, 0.225 (\u0434\u0430\u043d\u043d\u044b\u0435 \u0438\u0437 \u043c\u0430\u043d\u0443\u0430\u043b\u0430 ResNet)\n])   \n\ndataset_transforms = {\n                      'orig': transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n\n                      '140': transforms.Compose([\n    transforms.CenterCrop(140),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                     '135': transforms.Compose([\n    transforms.CenterCrop(135),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]), \n                      '130': transforms.Compose([\n    transforms.CenterCrop(130),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '125': transforms.Compose([\n    transforms.CenterCrop(125),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '120': transforms.Compose([\n    transforms.CenterCrop(120),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '115': transforms.Compose([\n    transforms.CenterCrop(115),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '110': transforms.Compose([\n    transforms.CenterCrop(110),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '105': transforms.Compose([\n    transforms.CenterCrop(105),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '100': transforms.Compose([\n    transforms.CenterCrop(100),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                     '95': transforms.Compose([\n    transforms.CenterCrop(95),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '90': transforms.Compose([\n    transforms.CenterCrop(90),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '85': transforms.Compose([\n    transforms.CenterCrop(85),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                       '80': transforms.Compose([\n    transforms.CenterCrop(80),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),\n                      '75': transforms.Compose([\n    transforms.CenterCrop(75),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),                                         \n                       '70': transforms.Compose([\n    transforms.CenterCrop(70),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(), \n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n]),                                                           \n                     }\n \ntrain_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n\nbatch_size = 16 # \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u0431\u0430\u0442\u0447\u0435\ntrain_dataloader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=batch_size)\n\nval_dataloader = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=batch_size)","ca2ff03d":"def show_input(input_tensor, title=''):\n    image = input_tensor.permute(1, 2, 0).numpy() #\u043e\u0431\u0440\u0430\u0442\u043d\u0430\u044f \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f \u043a ToTensor - .permute(1, 2, 0) Channels,H,W -> H,W,Channels \u043f\u043e\u0442\u043e\u043c \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u043c \u0432 numpy array .numpy()\n    image = std * image + mean #\u0414\u0435\u043b\u0430\u0435\u043c \u043e\u0431\u0440\u0430\u0442\u043d\u0443\u044e \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u043a\u0438\n    plt.imshow(image.clip(0, 1))\n    plt.title(title)\n    plt.show()\n    plt.pause(0.1)\n \nX_batch, y_batch = next(iter(train_dataloader)) \nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\nfor x_item, y_item in zip(X_batch, y_batch):\n    show_input(x_item, title=class_names[y_item])","13ef475f":"def train_model(model, loss, optimizer, scheduler, num_epochs):\n \n    loss_hist = {'train':[], 'val':[]}\n    acc_hist = {'train':[], 'val':[]}\n \n    for epoch in range(num_epochs):\n        print(\"Epoch {}\/{}:\".format(epoch, num_epochs - 1), end=\"\")\n        for phase in ['train', 'val']:\n            if phase == 'train': #\u0415\u0441\u043b\u0438 \u0444\u0430\u0437\u0430 == \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438  \n                dataloader = train_dataloader #\u0431\u0435\u0440\u0435\u043c train_dataLoader\n                scheduler.step() #\u0414\u0435\u043b\u0430\u0435\u043c 1 \u0448\u0430\u0433 (\u043f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u043e\u0434\u043d\u0430 \u044d\u043f\u043e\u0445\u0430)\n                model.train()  # \u041c\u043e\u0434\u0435\u043b\u044c \u0432 training mode - \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 (\u0424\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c, \u0438\u043d\u0430\u0447\u0435 \u0443 \u043d\u0430\u0441 \u043c\u043e\u0433\u0443\u0442 \u0438\u0437\u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u043b\u043e\u044f \u0431\u0430\u0442\u0447-\u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u0441\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043a\u0430 \u0441 \u0442\u0435\u0447\u0435\u043d\u0438\u0435\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u0438)\n            else: #\u0415\u0441\u043b\u0438 \u0444\u0430\u0437\u0430 == \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \n                dataloader = val_dataloader #\u0431\u0435\u0440\u0435\u043c val_dataLoader \n                model.eval()   # \u041c\u043e\u0434\u0435\u043b\u044c \u0432 evaluate mode - \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f (\u0424\u0438\u043a\u0441\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c, \u0438\u043d\u0430\u0447\u0435 \u0443 \u043d\u0430\u0441 \u043c\u043e\u0433\u0443\u0442 \u0438\u0437\u043c\u0435\u043d\u044f\u0442\u044c\u0441\u044f \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0441\u043b\u043e\u044f \u0431\u0430\u0442\u0447-\u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0438\u0437\u043c\u0435\u043d\u0438\u0442\u0441\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043a\u0430 \u0441 \u0442\u0435\u0447\u0435\u043d\u0438\u0435\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u0438)\n \n            running_loss = 0. \n            running_acc = 0.\n \n            # \u0418\u0442\u0435\u0440\u0438\u0440\u0443\u0435\u043c\u0441\u044f \u043f\u043e dataloader\n            for inputs, labels in tqdm(dataloader):\n                inputs = inputs.to(device) # \u0422\u0435\u043d\u0437\u043e\u0440 \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043d\u0430 GPU \n                labels = labels.to(device) # \u0422\u0435\u043d\u0437\u043e\u0440 \u0441 \u043b\u0435\u0439\u0431\u043b\u0430\u043c\u0438 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043d\u0430 GPU \n \n                optimizer.zero_grad() # \u041e\u0431\u043d\u0443\u043b\u044f\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442,\u0447\u0442\u043e\u0431\u044b \u043e\u043d \u043d\u0435 \u043d\u0430\u043a\u0430\u043f\u043b\u0438\u0432\u0430\u043b\u0441\u044f \n \n                with torch.set_grad_enabled(phase == 'train'): #\u0415\u0441\u043b\u0438 \u0444\u0430\u0437\u0430 train \u0442\u043e \u0430\u043a\u0442\u0438\u0432\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b (\u0442\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0437\u0430\u043c\u043e\u0440\u043e\u0436\u0435\u043d\u044b) (\u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u0438\u0441\u0442\u043e\u0440\u0438\u044e loss)\n                    preds = model(inputs) # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u044b, input \u043f\u0435\u0440\u0435\u0434\u0430\u0435\u043c \u0432 \u043c\u043e\u0434\u0435\u043b\u044c\n                    loss_value = loss(preds, labels) #\u041f\u043e\u0441\u0447\u0438\u0442\u0430\u043b\u0438  Loss    \n                    preds_class = preds.argmax(dim=1) # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441,\u0431\u0435\u0440\u0435\u043c .argmax(dim=1) \u043d\u0435\u0439\u0440\u043e\u043d \u0441 \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0430\u043a\u0442\u0438\u0432\u0430\u0446\u0438\u0435\u0439\n                \n                    if phase == 'train':\n                        loss_value.backward() # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442 \n                        optimizer.step() # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u0448\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430\n \n                # \u0421\u0442\u0430\u0442\u0438\u0441\u0442\u0438\u043a\u0430\n                running_loss += loss_value.item() #\u0441\u0447\u0438\u0442\u0430\u0435\u043c Loss\n                running_acc += (preds_class == labels.data).float().mean().data.cpu().numpy()  #\u0441\u0447\u0438\u0442\u0430\u0435\u043c accuracy\n \n            epoch_loss = running_loss \/ len(dataloader)  # Loss'\u044b \u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u043a\u043e\u043b-\u0432\u043e \u0431\u0430\u0447\u0435\u0439 \u0432 \u044d\u043f\u043e\u0445\u0435 \n            epoch_acc = running_acc \/ len(dataloader) #\u0441\u0447\u0438\u0442\u0430\u0435\u043c Loss \u043d\u0430 \u043a\u043e\u043b-\u0432\u043e \u0431\u0430\u0447\u0435\u0439 \u0432 \u044d\u043f\u043e\u0445\u0435\n \n            print(\"{} Loss: {:.4f} Acc: {:.4f} \".format(phase, epoch_loss, epoch_acc), end=\"\")\n            \n            loss_hist[phase].append(epoch_loss)\n            acc_hist[phase].append(epoch_acc)\n        \n    return model, loss_hist, acc_hist","fdc76d18":"model = models.resnet152(pretrained=True) #\u0424\u043e\u0440\u043c\u0430\u0442 pretrained=True - \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u044b \u0432\u0435\u0441\u0430, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043f\u043e\u043b\u0443\u0447\u0438\u043b\u0438\u0441\u044c \u0432\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u044d\u0442\u043e\u0433\u043e ResNet, \u043d\u0430 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 ImageNet\n          \n# \u0417\u0430\u043c\u043e\u0440\u0430\u0436\u0438\u0432\u0430\u0435\u043c \u0432\u0435\u0441\u0430, \u0447\u0442\u043e\u0431\u044b \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u0438\u0448\u043d\u0438\u0435 \u0432\u0435\u0441\u0430 \u0432 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438, \u0430 \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u0441\u043b\u043e\u0439\nfor param in model.parameters(): #\u041f\u0440\u043e\u0445\u043e\u0434\u0438\u043c \u043f\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430\u043c \u043c\u043e\u0434\u0435\u043b\u0438 (\u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 - \u044d\u0442\u043e \u043a\u0430\u0436\u0434\u044b\u0439 \u0441\u043b\u043e\u0439, model.parameters \u043d\u0430\u043c \u043e\u0442\u0434\u0430\u0441\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0438\u0442\u0435\u0440\u0430\u0442\u043e\u0440 \u043f\u043e \u0441\u043b\u043e\u044f\u043c)\n   param.requires_grad = False #\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0430 \u0438 \u0441\u043b\u043e\u044f:\"requires grad = False\", \u0442\u043e \u0435\u0441\u0442\u044c \u0443\u0436\u0435 \u043d\u0435 \u0442\u0440\u0435\u0431\u0443\u0435\u0442\u0441\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u0430 \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0433\u043e \u0441\u043b\u043e\u044f. \u0418 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f, \u0447\u0442\u043e \u0443 \u043d\u0430\u0441 \u0432\u0441\u044f \u0441\u0435\u0442\u043a\u0430 \u0431\u0443\u0434\u0435\u0442 \u0437\u0430\u043c\u043e\u0440\u043e\u0436\u0435\u043d\u0430, \u0442\u043e \u0435\u0441\u0442\u044c \u043c\u044b \u043d\u0435 \u0441\u043c\u043e\u0436\u0435\u043c \u0432\u043e\u043e\u0431\u0449\u0435 \u043d\u0438\u0447\u0435\u0433\u043e \u043e\u0431\u0443\u0447\u0430\u0442\u044c.\n    \n#\u041c\u0435\u043d\u044f\u0435\u043c \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0439 \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0439 \u0441\u043b\u043e\u0439, \u0432 ResNet \u043e\u043d \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u0443\u0435\u0442 \u043d\u0430 \u0442\u044b\u0441\u044f\u0447\u0443 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u0430 \u0443 \u043d\u0430\u0441 \u043a\u043b\u0430\u0441\u0441\u0430 \u0432\u0441\u0435\u0433\u043e 2.\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2) # C\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0439 torch.nn.Linear, \u044d\u0442\u043e \u043f\u043e\u043b\u043d\u043e\u0441\u0432\u044f\u0437\u043d\u044b\u0439 \u0441\u043b\u043e\u0439, \u043d\u0430 \u0432\u0445\u043e\u0434 \u043e\u043d \u043f\u0440\u0438\u043d\u0438\u043c\u0430\u0435\u0442 model fc_in features. \u0418 \u043e\u043d \u0435\u0434\u0438\u043d\u0441\u0432\u0435\u043d\u043d\u044b\u0439 - \u0440\u0430\u0437\u043c\u043e\u0440\u043e\u0436\u0435\u043d.\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = model.to(device)\n \n#\u041e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c Loss \u0444\u0443\u043d\u043a\u0446\u0438\u044e\n#\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0441\u043b\u0443\u0447\u0430\u0435 - \u044d\u0442\u043e \u0431\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043a\u0440\u043e\u0441\u0441-\u044d\u043d\u0442\u0440\u043e\u043f\u0438\u044f CrossEntropyLoss (\u0443 \u043d\u0430\u0441 \u0432\u0441\u0435\u0433\u043e 2 \u043a\u043b\u0430\u0441\u0441\u0430) \nloss = torch.nn.CrossEntropyLoss()\n# \u041c\u0435\u0442\u043e\u0434 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 Adam\noptimizer = torch.optim.Adam(model.parameters(), amsgrad=True, lr=0.001) #lr - (learning rate - \u0448\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430)\n# \u0423\u043c\u0435\u043d\u044c\u0448\u0430\u0435\u043c \u0448\u0430\u0433 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u043a\u0430\u0436\u0434\u044b\u0435 7 \u044d\u043f\u043e\u0445\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","7a658dac":"model, loss, acc = train_model(model, loss, optimizer, scheduler, num_epochs=30); #\u0417\u0430\u043f\u0443\u0441\u043a \u0444\u0443\u043d\u043a\u0446\u0438\u0438 Train (\u041c\u043e\u0434\u0435\u043b\u044c= ResNet,Loss-\u0444\u0443\u043d\u043a\u0446\u0438\u044f= CrossEntropyLoss(\u0431\u0438\u043d\u0430\u0440\u043d\u0430\u044f \u043a\u0440\u043e\u0441\u0441-\u044d\u043d\u0442\u0440\u043e\u043f\u0438\u044f),\u041c\u0435\u0442\u043e\u0434 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430= Adam, \u0423\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u043e\u0433\u043e \u0441\u043f\u0443\u0441\u043a\u0430 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043a\u043e\u043b-\u0432\u0430 \u044d\u043f\u043e\u0445, \u041a\u043e\u043b-\u0432\u043e \u044d\u043f\u043e\u0445= 30) )","7d218a33":"plt.rcParams['figure.figsize'] = (14, 7)\nfor experiment_id in acc.keys():\n    plt.plot(acc[experiment_id], label=experiment_id)\nplt.legend(loc='upper left')\nplt.title('Model Accuracy')\nplt.xlabel('Epoch num', fontsize=15)\nplt.ylabel('Accuracy value', fontsize=15);\nplt.grid(linestyle='--', linewidth=0.5, color='.7')","1aef25a7":"plt.rcParams['figure.figsize'] = (14, 7)\nfor experiment_id in loss.keys():\n    plt.plot(loss[experiment_id], label=experiment_id)\nplt.legend(loc='upper left')\nplt.title('Model Loss')\nplt.xlabel('Epoch num', fontsize=15)\nplt.ylabel('Loss function value', fontsize=15)\nplt.grid(linestyle='--', linewidth=0.5, color='.7')","b8814577":"#\u041a\u043e\u0441\u0442\u044b\u043b\u044c ImageFolder, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043d\u0435 \u043c\u043e\u0436\u0435\u0442 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u043f\u0443\u0442\u044c \u043a \u043f\u0430\u043f\u043a\u0435 \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u0439 \u0443\u0436\u0435 \u0441\u0440\u0430\u0437\u0443 \u043b\u0435\u0436\u0430\u0442 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n#\u041a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u0432\u0441\u044e \u043f\u0430\u043f\u043a\u0443 test \u0432 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u044e test\\unknown\ntest_dir = 'test'\nshutil.copytree(os.path.join(data_root, 'test'), os.path.join(test_dir, 'unknown'))","e2ccc53d":"#\u041c\u044b \u043d\u0435 \u0437\u043d\u0430\u0435\u043c, \u043a\u0430\u043a\u0438\u0435 ID, \u043a\u0430\u043a\u0438\u0435 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0443 \u043d\u0430\u0441 \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u0442\u0441\u044f, \u043a\u043e\u0433\u0434\u0430 \u043c\u044b \u043f\u0440\u043e\u0441\u0438\u043c \u0443 DataLoader -- \"\u0434\u0430\u0439 \u043d\u0430\u043c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0431\u0430\u0442\u0447\".\n#\u041e\u043d\u0438 \u043f\u043e \u0430\u043b\u0444\u0430\u0432\u0438\u0442\u0443 \u0438\u0434\u0443\u0442, \u043f\u043e \u0434\u0430\u0442\u0435 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f, \u0438\u043b\u0438 \u043f\u0440\u043e\u0441\u0442\u043e \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c -- \u043d\u0435\u043f\u043e\u043d\u044f\u0442\u043d\u043e.\n#\u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u043f\u0438\u0441\u0430\u0442\u044c \u043d\u0435\u043c\u043d\u043e\u0436\u043a\u043e ImageFolder, \u0447\u0442\u043e\u0431\u044b \u043e\u043d \u043d\u0430\u043c \u043e\u0442\u0434\u0430\u0432\u0430\u043b \u043d\u0435 \u043f\u0440\u043e\u0441\u0442\u043e tuple, \u0441 \u0441\u0430\u043c\u0438\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\u043c \u0438 \u0435\u0433\u043e \u043c\u0435\u0442\u043a\u043e\u0439, \u0430 \u0435\u0449\u0451, \u0447\u0442\u043e\u0431\u044b \u043e\u043d \u043e\u0442\u0434\u0430\u0432\u0430\u043b \u0438\u043c\u044f, \u043d\u0443, \u043b\u0438\u0431\u043e -- \u043f\u0443\u0442\u044c \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044e.\nclass ImageFolderWithPaths(torchvision.datasets.ImageFolder): #\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u043b\u0430\u0441\u0441, \u043e\u043d \u043d\u0430\u0441\u043b\u0435\u0434\u0443\u0435\u0442\u0441\u044f \u043e\u0442 ImageFolder, \u043d\u043e \u0438\u0437\u043c\u0435\u043d\u044f\u0435\u0442 \u0435\u0433\u043e \u0444\u0443\u043d\u043a\u0446\u0438\u044e get_item\n    def __getitem__(self, index):\n        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index) #\u0414\u043e\u043f\u043e\u043b\u043d\u044f\u0435\u043c original_tuple \u043f\u0443\u0442\u0435\u043c \u0434\u043b\u044f \u0444\u0430\u0439\u043b\u0430 (.__getitem__(index))\n        path = self.imgs[index][0]\n        tuple_with_path = (original_tuple + (path,))\n        return tuple_with_path\n    \ndf = pd.DataFrame\n#\u0418\u0442\u0435\u0440\u0438\u0440\u0443\u0435\u043c\u0441\u044f \u043f\u043e Crop'\u0430\u043c test \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\nfor (i,tranforms) in dataset_transforms.items():\n    test_dataset = ImageFolderWithPaths('\/kaggle\/working\/test', tranforms) #\u0411\u0435\u0440\u0435\u043c \u043d\u043e\u0432\u044b\u0439 \u043a\u043b\u0430\u0441\u0441 \u0438 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c tuple \u0438\u0437 3\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n    test_dataloader = torch.utils.data.DataLoader(\n        test_dataset, batch_size=batch_size, shuffle=False, num_workers=0) #\u041d\u043e\u0432\u044b\u0439 \u0434\u0430\u0442\u0430\u043b\u043e\u0430\u0434\u0435\u0440 \u0441 \u043f\u0443\u0442\u044f\u043c\u0438 \u0434\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\n    \n    model.eval() #\u041f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u0432 \u0441\u043e\u0441\u0442\u043e\u044f\u043d\u0438\u0435 eval\n    test_predictions = []  #\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u0443\u0441\u0442\u043e\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \n    test_img_paths = [] #\u041f\u0443\u0442\u0438 \u0434\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\n    for inputs, labels, paths in tqdm(test_dataloader): #\u0426\u0438\u043a\u043b \u043f\u043e test_dataloader inputs - \u0431\u0430\u0442\u0447 \u0441 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435\u043c, lable - \u0442\u0443\u0442 none, paths - \u043f\u0443\u0442\u0438 \u0434\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f  \n        inputs = inputs.to(device) \n        labels = labels.to(device)  \n        with torch.set_grad_enabled(False):\n            preds = model(inputs) # \u0421\u0447\u0438\u0442\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0438\u043a\u0448\u0435\u043d\u044b\n        test_predictions.append(\n            torch.nn.functional.softmax(preds, dim=1)[:,1].data.cpu().numpy()) #\u0421 \u043f\u043e\u043c\u043e\u0449\u044c\u044e torch.nn.functional.softmax \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438, \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 [:,1], \u043f\u0435\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u0442\u0435\u043d\u0437\u043e\u0440 \u0432 .data, \u043d\u0430 .cpu(), \u0432 numpy \n        test_img_paths.extend(paths)\n    test_predictions = np.concatenate(test_predictions)\n    \n    \n    submission_df = pd.DataFrame.from_dict({'id': test_img_paths, 'label': test_predictions})\n    submission_df['id'] = submission_df['id'].str.replace('\/kaggle\/working\/test\/unknown\/', '')\n    submission_df['id'] = submission_df['id'].str.replace('.jpg', '')\n    submission_df.set_index('id', inplace=True)\n    \n    try : df = df.merge(submission_df, how='inner', on='id') #\u041e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0432 \u043e\u0434\u0438\u043d \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\n    except BaseException: # \u0414\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0439 \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0438\n        df = submission_df \n    #submission_df['label'] = submission_df['label'].map(lambda pred: 'dirty' if pred > 0.50 else 'cleaned')\n    #submission_df.to_csv('submission_predict_{0}.csv'.format(i))\ndf.head(8)","4217eb97":"df['mean'] = df.mean(axis=1)\ndf.drop(df.columns[:-1], axis='columns', inplace=True)\ndf['label'] = df['mean'].map(lambda pred: 'dirty' if pred > 0.50 else 'cleaned')\ndf.drop(df.columns[:-1], axis='columns', inplace=True)\ndf.head(10)","febc5123":"df.to_csv('submission.csv')","1183db35":"!rm -rf train val","46ee4b53":"\u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u043c\u043e\u0434\u0435\u043b\u0438","933d15b5":"Train","91a82481":"\u0415\u0441\u043b\u0438 \u0434\u0430\u043d\u043d\u043e\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435 \u0412\u0430\u043c \u043f\u043e\u043c\u043e\u0433\u043b\u043e, \u043d\u0435 \u0437\u0430\u0431\u0443\u0434\u044c\u0442\u0435 \u043f\u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c ^ (upvote) \u0432 \u043f\u0440\u0430\u0432\u043e\u043c \u0432\u0435\u0440\u0445\u043d\u0435\u043c \u0443\u0433\u043b\u0443","9fb773e6":"1. \u0423\u0434\u0430\u043b\u044f\u0435\u043c \u0444\u043e\u043d  - remove_background\n2. \u041d\u0430\u0445\u043e\u0434\u0438\u043c \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u044b \u0446\u0435\u043d\u0442\u0440\u0430 \u0444\u0438\u0433\u0443\u0440\u044b - findCoordinates\n3. \u0418\u0449\u0435\u043c \u043a\u0440\u0443\u0433\u0438 \u0432 \u043f\u0440\u0435\u0434\u0435\u043b\u0430\u0445 \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442 - find_circle\n4. \u041e\u0431\u0440\u0435\u0437\u0430\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u0443\u0432\u0435\u043b\u0438\u0447\u0435\u043d\u0438\u044f train \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 - crop \n5. \u0414\u043b\u044f test \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0432\u044b\u0440\u0435\u0437\u0430\u043d\u043d\u043e\u0435 \u043f\u043e \u0432\u043f\u0438\u0441\u0430\u043d\u043d\u043e\u043c\u0443 (\u0432 \u043d\u0430\u0439\u0434\u0435\u043d\u043d\u044b\u0439 \u043a\u0440\u0443\u0433) \u043a\u0432\u0430\u0434\u0440\u0430\u0442\u0443 - crop_test","8fa74f62":"\u0413\u0440\u0430\u0444\u0438\u043a loss","301bc782":"\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u043f\u043e \u0432\u0441\u0435\u043c crop'\u0430\u043c","789d884b":"\u0413\u0440\u0430\u0444\u0438\u043a Accuracy","c062689f":"\u0420\u0430\u0441\u043f\u0430\u043a\u043e\u0432\u043a\u0430 \u0438\u0441\u0445\u043e\u0434\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","48202227":"\u0417\u0430\u043f\u0443\u0441\u043a\u0430\u0435\u043c \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435","53fcddeb":"\u0410\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445","7d1fb34b":"\u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0432\u044b\u0432\u043e\u0434\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439","1fdfb8e1":"\u0421\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c","217ac275":"\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043f\u0430\u043f\u043a\u0443 train \u0438 val (\u0438\u0437 \u043f\u0430\u043f\u043a\u0438 train \u0431\u0435\u0440\u0435\u043c \u043a\u0430\u0436\u0434\u0443\u044e \u0448\u0435\u0441\u0442\u0443\u044e \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044e)","f7736310":"\u0418\u043c\u043f\u043e\u0440\u0442 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a"}}