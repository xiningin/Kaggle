{"cell_type":{"01640d7e":"code","508c1224":"code","7f621477":"code","9270b47e":"code","6274057b":"code","0cbe018f":"code","41d17acf":"code","64a8434e":"code","cf81f5d4":"code","4e21a1c1":"code","f0d54e1c":"code","b1040ea5":"code","8913726e":"code","27fd2ad9":"code","7eca74e5":"code","c0908b68":"code","e852e9c1":"code","e3a5e940":"markdown","c068c9c1":"markdown","ac3a7925":"markdown","3af96d28":"markdown","72b7c811":"markdown","e420d197":"markdown","dcbba0f7":"markdown","d1acdbca":"markdown","8d093790":"markdown","bb7471a2":"markdown","78162cdd":"markdown","515a29f9":"markdown","85ac4294":"markdown","40eb847a":"markdown","a02fa836":"markdown","aaefd415":"markdown","51fc4743":"markdown","c2c6d039":"markdown","11d56e87":"markdown"},"source":{"01640d7e":"from IPython.display import clear_output\n!pip install pycaret --user\nclear_output()","508c1224":"import numpy as np\nimport pandas as pd\nimport pycaret\nfrom pycaret.classification import *\nfrom sklearn.model_selection import train_test_split","7f621477":"TARGET = \"Survived\"\nSESSION_ID = 2021\nSEED = 42\nFEATURE = \"feature\"\nEXPERIMENT_NAME = 'titanic_prediction'","9270b47e":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","6274057b":"setup(data = train, \n      target = TARGET, \n      session_id=SESSION_ID, \n      experiment_name=EXPERIMENT_NAME,\n      silent=True\n     )","0cbe018f":"lightgbm = create_model('lightgbm')","41d17acf":"print(lightgbm)","64a8434e":"lgbm_params = {'num_leaves': np.arange(10,200,1),\n                'max_depth': np.arange(1,100,1),\n                'learning_rate': np.arange(0.0001,1,0.0001),\n                'n_estimators':np.arange(1,2000,1),\n                'min_child_samples':np.arange(1,100,1),\n                'subsample_freq':np.arange(0,100,1)\n             }\n\ntuned_lightgbm = tune_model(lightgbm, custom_grid = lgbm_params)","cf81f5d4":"print(tuned_lightgbm)","4e21a1c1":"plot_model(tuned_lightgbm)","f0d54e1c":"plot_model(tuned_lightgbm, plot = 'pr')","b1040ea5":"plot_model(tuned_lightgbm, plot = 'confusion_matrix')","8913726e":"plot_model(tuned_lightgbm, plot=FEATURE)","27fd2ad9":"evaluate_model(tuned_lightgbm)","7eca74e5":"final_lightgbm = finalize_model(tuned_lightgbm)\nprint(final_lightgbm)","c0908b68":"test_pred = predict_model(final_lightgbm, data=test)\ntest_pred.head()","e852e9c1":"submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nsubmission.Survived = test_pred[\"Label\"]\nsubmission.to_csv(\"submission.csv\",index=False)\nsubmission.head(10)","e3a5e940":"# 8.Predict test data by final model","c068c9c1":"# 5.2 Precision-Recall Curve","ac3a7925":"# 7.Get final model ","3af96d28":"# 5.1 AUC Plot","72b7c811":"#  0.3 global variables","e420d197":"# 1.load data","dcbba0f7":"# 2.setup pycaret ","d1acdbca":"\nThe setup() function initializes the environment in pycaret and creates the transformation pipeline to prepare the data for modeling and deployment. setup() must be called before executing any other function in pycaret. It takes two mandatory parameters: a pandas dataframe and the name of the target column. All other parameters are optional and are used to customize the pre-processing pipeline","8d093790":"# 5.4 Feature importance plot ","bb7471a2":"# 6.Evaluate model","78162cdd":"# 0.1 install pycaret ","515a29f9":"# 9.Submission ","85ac4294":"# 3.create lightgbm model (default model)","40eb847a":"# 0.2 import libraries","a02fa836":"# 5.3 Confusion matrix","aaefd415":"\nModel finalization is the last step in the experiment. A normal machine learning workflow in PyCaret starts with setup(), followed by comparing all models using compare_models() and shortlisting a few candidate models (based on the metric of interest) to perform several modeling techniques such as hyperparameter tuning, ensembling, stacking etc. This workflow will eventually lead you to the best model for use in making predictions on new and unseen data.","51fc4743":"# 4.tune lightgbm ","c2c6d039":"When a model is created using the create_model() function it uses the default hyperparameters to train the model. In order to tune hyperparameters, the tune_model() function is used. This function automatically tunes the hyperparameters of a model using Random Grid Search on a pre-defined search space. The output prints a score grid that shows Accuracy, AUC, Recall, Precision, F1, Kappa, and MCC by fold for the best model. To use the custom search grid, you can pass custom_grid parameter in the tune_model function","11d56e87":"When setup() is executed, PyCaret's inference algorithm will automatically infer the data types for all features based on certain properties. The data type should be inferred correctly but this is not always the case. To account for this, PyCaret displays a table containing the features and their inferred data types after setup() is executed. If all of the data types are correctly identified enter can be pressed to continue or quit can be typed to end the expriment. Ensuring that the data types are correct is of fundamental importance in PyCaret as it automatically performs a few pre-processing tasks which are imperative to any machine learning experiment. These tasks are performed differently for each data type which means it is very important for them to be correctly configured."}}