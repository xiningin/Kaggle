{"cell_type":{"67eeb162":"code","a60ea6ac":"code","0e523df1":"code","ebadb438":"code","f6323a16":"code","16135edc":"code","49da2890":"code","1418efaa":"code","39eb7d86":"code","ba275f01":"code","c21d21c5":"code","8fed309a":"code","8eeeb80c":"code","d184e1d3":"code","798858be":"code","e781d905":"code","0101201e":"code","ed3daa5d":"markdown","d8a2d5ae":"markdown","56ae6a96":"markdown","637ef83f":"markdown","c0d45e55":"markdown","5dfc522c":"markdown","004097eb":"markdown","ebf50cdd":"markdown","24232b72":"markdown","12d094d4":"markdown","049c8b18":"markdown","f94b13b9":"markdown","3b32966c":"markdown","5f779e6d":"markdown","0b1ffdb9":"markdown"},"source":{"67eeb162":"# import package\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.datasets import load_boston","a60ea6ac":"#download boston dataset\nboston = load_boston()\nbostonDF=pd.DataFrame(boston.data,columns=boston.feature_names)\n\nbostonDF['PRICE']=boston.target\n\nprint('The shape of boston dataset : ',bostonDF.shape)\nbostonDF.head(5)","0e523df1":"bostonDF.info() # all features are non-null, float","ebadb438":"fig, axs = plt.subplots(figsize=(32,8),ncols=7,nrows=2)\nfeatures = boston.feature_names\nfor i, feature in enumerate(features):\n    row = int(i\/7)\n    col = i%7\n    \n    sns.regplot(x=feature,y='PRICE',data=bostonDF,ax=axs[row][col])","f6323a16":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\ny_target = bostonDF['PRICE']\nX_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)\n\nX_train, X_test, y_train, y_test = train_test_split(X_data,y_target,test_size=0.3,\n                                                   random_state=156)\n\nlr=LinearRegression()\nlr.fit(X_train,y_train)\ny_preds=lr.predict(X_test)\nmse = mean_squared_error(y_test,y_preds)\nrmse = np.sqrt(mse)\n\nprint('MSE: {0:.3f}, RMSE : {1:.3f}'.format(mse,rmse))\nprint('Variance score: {0:.3f}'.format(r2_score(y_test,y_preds)))\nprint('intercept: ',lr.intercept_)\nprint('coefficient: ',lr.coef_)","16135edc":"coeff = pd.Series(data=np.round(lr.coef_,1),index=X_data.columns)\ncoeff.sort_values(ascending=False)","49da2890":"from sklearn.model_selection import cross_val_score\n\ny_target = bostonDF['PRICE']\nX_data = bostonDF.drop(['PRICE'],axis=1,inplace=False)\nlr = LinearRegression()\n\nneg_mse_scores = cross_val_score(lr,X_data,y_target\n                                 ,scoring=\"neg_mean_squared_error\",cv=5)\nrmse_scores = np.sqrt(-1*neg_mse_scores)\navg_rmse=np.mean(rmse_scores)\n\nprint('each RMSE score of 5 folds : ', np.round(rmse_scores,2))\nprint('mean RMSE score of 5 folds : ', np.round(avg_rmse,2))","1418efaa":"# We only use 'LSTAT' feature as X\nplt.plot(X_train['LSTAT'],y_train,\"b.\")\nplt.ylabel('PRICE')\nplt.xlabel('LSTAT')\nplt.show()","39eb7d86":"# quadratic polynomial\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_features = PolynomialFeatures(degree=2)\nX_poly = poly_features.fit_transform(X_train['LSTAT'].values.reshape(len(X_train['LSTAT']),1)) # we have to make series into M*1 array\n\nprint(X_train['LSTAT'][49]) # first value\nprint(X_poly[0])","ba275f01":"lr2=LinearRegression()\nlr2.fit(X_poly,y_train)\nX_poly_test = poly_features.fit_transform(X_test['LSTAT'].values.reshape(len(X_test['LSTAT']),1))\ny_preds=lr2.predict(X_poly_test)\nmse = mean_squared_error(y_test,y_preds)\nrmse = np.sqrt(mse)\n\nprint('MSE: {0:.3f}, RMSE : {1:.3f}'.format(mse,rmse))\nprint('Variance score: {0:.3f}'.format(r2_score(y_test,y_preds)))\nprint('intercept: ',lr2.intercept_)\nprint('coefficient: ',lr2.coef_)","c21d21c5":"X_new = X_train['LSTAT']\ny_new = lr2.predict(X_poly)\n\nplt.plot(X_train['LSTAT'],y_train,\"b.\")\nplt.plot(X_new, y_new, \"r.\", linewidth=2, label=\"Predictions\")\nplt.ylabel('PRICE')\nplt.xlabel('LSTAT')\nplt.legend(loc=\"upper left\", fontsize=14)\nplt.show()","8fed309a":"from sklearn.linear_model import Ridge\n\nalphas = [0,0.1,1,10,100]\nfor alpha in alphas:\n    ridge = Ridge(alpha=alpha)\n    \n    neg_mse_scores = cross_val_score(ridge,X_data,y_target\n                                 ,scoring=\"neg_mean_squared_error\",cv=5)\n    rmse_scores = np.sqrt(-1*neg_mse_scores)\n    avg_rmse=np.mean(rmse_scores)\n    print('--------alpha = ',alpha,'----------')\n    print('each RMSE score of 5 folds : ', np.round(rmse_scores,2))\n    print('mean RMSE score of 5 folds : ', np.round(avg_rmse,2))","8eeeb80c":"fig, axs = plt.subplots(figsize=(18,6), nrows = 1, ncols = 5)\n\ncoeff_df = pd.DataFrame()\n\nfor pos, alpha in enumerate(alphas):\n    ridge = Ridge(alpha=alpha)\n    ridge.fit(X_data,y_target)\n    \n    coeff = pd.Series(data=ridge.coef_, index = X_data.columns)\n    colname ='alpha : '+str(alpha)\n    coeff_df[colname] = coeff\n    \n    coeff = coeff.sort_values(ascending=False)\n    axs[pos].set_title(colname)\n    axs[pos].set_xlim(-3,6)\n    sns.barplot(x=coeff.values, y=coeff.index, ax=axs[pos])\n\nplt.show()","d184e1d3":"from sklearn.linear_model import Lasso\n\nalphas = [0.07,0.1,0.5,1,3]\nfor alpha in alphas:\n    lasso = Lasso(alpha=alpha)\n    \n    neg_mse_scores = cross_val_score(lasso,X_data,y_target\n                                 ,scoring=\"neg_mean_squared_error\",cv=5)\n    rmse_scores = np.sqrt(-1*neg_mse_scores)\n    avg_rmse=np.mean(rmse_scores)\n    print('--------alpha = ',alpha,'----------')\n    print('each RMSE score of 5 folds : ', np.round(rmse_scores,2))\n    print('mean RMSE score of 5 folds : ', np.round(avg_rmse,2))","798858be":"fig, axs = plt.subplots(figsize=(18,6), nrows = 1, ncols = 5)\ncoeff_df = pd.DataFrame()\n\nfor pos, alpha in enumerate(alphas):\n    lasso = Lasso(alpha=alpha)\n    lasso.fit(X_data,y_target)\n    \n    coeff = pd.Series(data=lasso.coef_, index = X_data.columns)\n    colname ='alpha : '+str(alpha)\n    coeff_df[colname] = coeff\n    \n    coeff = coeff.sort_values(ascending=False)\n    axs[pos].set_title(colname)\n    axs[pos].set_xlim(-3,6)\n    sns.barplot(x=coeff.values, y=coeff.index, ax=axs[pos])\n\nplt.show()","e781d905":"from sklearn.linear_model import ElasticNet\n\n# set l1_ratio to 0.7\nelastic_alphas = [0.07,0.1,0.5,1,3]\n\nfor alpha in elastic_alphas:\n    elasticnet = ElasticNet(alpha=alpha, l1_ratio=0.7)\n    \n    neg_mse_scores = cross_val_score(elasticnet,X_data,y_target\n                                 ,scoring=\"neg_mean_squared_error\",cv=5)\n    rmse_scores = np.sqrt(-1*neg_mse_scores)\n    avg_rmse=np.mean(rmse_scores)\n    print('--------alpha = ',alpha,'----------')\n    print('each RMSE score of 5 folds : ', np.round(rmse_scores,2))\n    print('mean RMSE score of 5 folds : ', np.round(avg_rmse,2))","0101201e":"fig, axs = plt.subplots(figsize=(18,6), nrows = 1, ncols = 5)\ncoeff_df = pd.DataFrame()\n\nelastic_alphas = [0.07,0.1,0.5,1,3]\n\nfor pos, alpha in enumerate(elastic_alphas):\n    elasticnet = ElasticNet(alpha=alpha, l1_ratio=0.7)\n    elasticnet.fit(X_data,y_target)\n    \n    coeff = pd.Series(data=elasticnet.coef_, index = X_data.columns)\n    colname ='alpha : '+str(alpha)\n    coeff_df[colname] = coeff\n    \n    coeff = coeff.sort_values(ascending=False)\n    axs[pos].set_title(colname)\n    axs[pos].set_xlim(-3,6)\n    sns.barplot(x=coeff.values, y=coeff.index, ax=axs[pos])\n\nplt.show()","ed3daa5d":"It seems that feature 'RM' and 'LSTAT' have large impact on 'PRICE'! 'RM' has positive linearity with 'PRICE',and 'LSTAT' has negative linearity with 'PRICE'.","d8a2d5ae":"**Ridge regression**\n> In ridge regression, we have to set alpha parameter. If alpha is 0, the cost function in min(RSS(W)). If alpha is infinite, W have to close to 0. ","56ae6a96":"**2. visuallize X impact on Y**","637ef83f":"**Elastic Net regression**\n> Elastic combine L2, L1 regularization.","c0d45e55":"![1](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ec\/Anscombe%27s_quartet_3.svg)","5dfc522c":"# Regression\n> Regression is the one of the most important key of modern statistic. It is the method that modeling relationship between dependent variable and independent variable. We can divide regression as the number of independent variable and linearity","004097eb":"You can see that as alpha get bigger, coefficient get smaller. Especially, coefficient of 'NOX' decline extrimly. ","ebf50cdd":"You can see some of the coefficient become 0.","24232b72":"|The number of independent variable|linearity|\n|------|------|\n|1 : single regression |linear : linear regression|\n|more than 2 : multiple regression|nonlinear : nonlinear regression|","12d094d4":"**Lasso regression**\n> Lasso apply L1 regularization into linear regression. L1 regularization include only valid feature and make useless feature into 0.","049c8b18":"# 3\ufe0f\u20e3 third model : Regularization\n> We can divide linear regression through regularization method. Regularization method is the method that give penalty to coefficient to avoid overfitting problem.\n\n* Ridge : linear regression + L2 regularization(reduce the influence of relatively large coefficient)\n* Lasso : linear regression + L1 regularization(make coefficient that have little influence into 0)\n* ElasticNet : linear regression + L1 + L2","f94b13b9":"# 2\ufe0f\u20e3 second model : Polynomial regression\n> First model was made by linear equation, But we can't express all the relationship between dependent and independent variable into linear. Therefore, we can use polynomial regression which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. One thing you should be aware of is that polynomial regression is not nonlinear but linear regression. So we can apply polynomial model into linear model.","3b32966c":"**1. download the dataset**","5f779e6d":"# \ud83c\udfe1Boston dataset\n> Sklearn package provide various regression method through class. Using dataset provided by sklearn, we can make regression model.","0b1ffdb9":"# 1\ufe0f\u20e3 First model : very simple linear regression\n> First, we will make very simple linear regression model that contain all the feature and no regulation. We can simpliy make it using scikit-learn package"}}