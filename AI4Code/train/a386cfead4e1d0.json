{"cell_type":{"b5cd3dc9":"code","c849ff3e":"code","40f59ede":"code","f87ffd34":"code","25802cc6":"code","224b8077":"code","0bee90ba":"code","49658e13":"code","f602f751":"code","aaed2871":"code","85e8de70":"code","b8eb4b07":"code","30074e68":"code","ed36c11d":"code","17547f75":"code","daf80300":"code","e445fbce":"code","7923ee86":"code","c1259753":"code","8491d5ca":"code","c9546a5a":"code","65b3acae":"code","88c907be":"code","1b7ca608":"code","4356a639":"code","4ecf0e5a":"code","7c93ef35":"code","8463311f":"code","7bd73a4f":"code","62da06ab":"code","2391c19a":"code","c3c374a3":"code","230f794e":"markdown","72321d34":"markdown","071cf58d":"markdown","19547408":"markdown","cd471e27":"markdown","4e2f4f88":"markdown","7aaee828":"markdown","c4c91309":"markdown","db889833":"markdown","34690f6f":"markdown","c15969cf":"markdown","ff7308ee":"markdown","78195603":"markdown","5c65d8bb":"markdown","4469bad2":"markdown","fa9b2c30":"markdown","741ff1d6":"markdown","00d353f0":"markdown"},"source":{"b5cd3dc9":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nplt.style.use(\"ggplot\")\n%matplotlib inline","c849ff3e":"data = pd.read_csv(\"..\/input\/titanic-data\/titanic.csv\")\ndata.head()","40f59ede":"def Sigmoid_function(x):    \n    return 1\/ (1 + np.exp(-x))","f87ffd34":"def Cost_function(theta, x, y):\n    m = len(y)\n    hypothesis = Sigmoid_function(np.dot(x , theta))\n    error = (y * np.log(hypothesis)) + ((1 - y) * np.log(1 - hypothesis))\n    cost = -1 \/ m * sum(error)\n    gradient = 1 \/ m * np.dot(x.transpose(), (hypothesis - y))\n    return cost[0] , gradient","25802cc6":"def gradient_descent(x, y, theta, alpha, iterations):\n    costs = []\n    for i in range(iterations):\n        cost, gradient = Cost_function(theta, x, y)\n        theta -= (alpha * gradient)\n        print(cost)\n        costs.append(cost)\n    return theta, costs","224b8077":"features = data[['Age','Fare','Sex','Cabin']].values  #predictors\nresults = data['Survived'].values ","0bee90ba":"mean_features = np.mean(features, axis=0)\nstd_features = np.std(features, axis=0)\nfeatures = (features - mean_features) \/ std_features ","49658e13":"rows = features.shape[0]\ncols = features.shape[1]\n\nX = np.append(np.ones((rows, 1)), features, axis=1) \ny = results.reshape(rows, 1)\n\ntheta_init = np.zeros((cols + 1, 1))\ncost, gradient = Cost_function(theta_init, X, y)\n\nprint(\"Cost at initialization\", cost)\nprint(\"Gradient at initialization:\", gradient)","f602f751":"theta, costs = gradient_descent(X, y, theta_init, 0.1, 100)\n","aaed2871":"print(\"Theta after running gradient descent:\", theta)\nprint(\"Resulting cost:\", costs[-1])","85e8de70":"plt.plot(costs)\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"$J(\\Theta)$\")\nplt.title(\"Values of Cost Function over iterations of Gradient Descent\");","b8eb4b07":"def predict(theta, x):\n    results = x.dot(theta)\n    return results > 0\n\n\np = predict(theta, X)\nprint(\"Training Accuracy:\", (sum(p==y)[0])\/len(data)*100,\"%\")","30074e68":"theta","ed36c11d":"predict(theta,np.array([1, 50 , 53,  0,  7]) )","17547f75":"y_pred=predict(theta,X )","daf80300":"def confusion_matrix(y, y_true):\n  TP,TN,FP,FN = 0,0,0,0\n  for actual, predicted in zip(*(y_true,y)):\n    if actual == predicted:\n      if actual == 1:\n        TP+=1\n      else:\n        TN+=1\n    else:\n      if predicted == 1:\n        FP+=1\n      else:\n        FN+=1 \n  return TP,TN,FP,FN\n#F1 score\nTP,TN,FP,FN = confusion_matrix(y_pred, y)\n\"TP= {0}, TN= {1}, FP={2}, FN={3}\".format(TP,TN,FP,FN)","e445fbce":"recall=TP\/(FN+TP)\nrecall","7923ee86":"precision=TP\/(FP+TP)\nprecision","c1259753":"f1_score = TP\/(TP + (FN+FP)\/2)\nf1_score","8491d5ca":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()","c9546a5a":"X = data[['Age','Fare','Sex','Cabin']]  #predictors\ny = data['Survived']","65b3acae":"lr.fit(X,y)","88c907be":"lr.intercept_,lr.coef_","1b7ca608":"y_pred=lr.predict(X)","4356a639":"from sklearn.metrics import accuracy_score\nprint('training accuracy: ',accuracy_score(y_pred,y))","4ecf0e5a":"test = np.array(np.array([[50 , 53,  0,  7]]))\nlr.predict(test)","7c93ef35":"from sklearn.metrics import confusion_matrix,f1_score\nconfusion_matrix(y,y_pred)","8463311f":"from sklearn.metrics import recall_score,precision_score\nrecall_score(y,y_pred),precision_score(y,y_pred)","7bd73a4f":"f1_score(y,y_pred)","62da06ab":"from sklearn.metrics import roc_auc_score,roc_curve,plot_roc_curve,det_curve,RocCurveDisplay,precision_recall_curve","2391c19a":"prob_score=lr.predict_proba(X)[:, 1]\nroc_auc_score(y,prob_score)","c3c374a3":"plot_roc_curve(lr,X,y)","230f794e":"So our model has predicted that person will survive with given input of [1, 50 , 53,  0,  7]","72321d34":"Now our model is ready. Below we are giving a sample input of age, fare,sex and cabin and predicting whether a person will survive or not","071cf58d":"The main goal of Gradient descent is to minimize the cost value. i.e. min J(\u03b8).\nNow to minimize our cost function we need to run the gradient descent function on each parameter","19547408":"###  Sigmoid Function $\\sigma(z)$\n\n $$\\sigma(z) = \\frac{1}{1+e^{-z}}$$","cd471e27":"Gradient Descent is the process of minimizing a function by following the gradients of the cost function. This involves knowing the form of the cost as well as the derivative so that from a given point you know the gradient and can move in that direction, e.g. downhill towards the minimum value.","4e2f4f88":"You learnt about the cost function J(\u03b8) in the lecture, the cost function represents optimization objective i.e. we create a cost function and minimize it so that we can develop an accurate model with minimum error.","7aaee828":"$h_\\theta(x) = \\sigma(z)$, where $\\sigma$ is the logistic sigmoid function and $z = \\theta^Tx$\n\nWhen $h_\\theta(x) \\geq 0.5$ the model predicts class \"1\":\n\n$\\implies \\sigma(\\theta^Tx) \\geq 0.5$\n\n$\\implies \\theta^Tx \\geq 0$ predict class \"1\" \n\n","c4c91309":"**Gradient descent**\n\n$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}$ (simultaneously update $\\theta_j$ for all $j$)","db889833":"**cost function**\n\n$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [ y^{(i)}log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)})log(1 - (h_{\\theta}(x^{(i)}))]$$\n\n**gradient**\n\n$$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$$","34690f6f":"# Logistic Regression in Machine Learning without using scikit-learn","c15969cf":"The objective of gradient descent is to find out optimal parameters that result in optimising a given function. In the Logistic Regression algorithm, the optimal parameters \u03b8 are found by minimising the value of cost function","ff7308ee":"Here, try changing values of alpha and no of iterations and observe J(theta)","78195603":"I hope this notebook was helpful to understand the math behind logistic regression.","5c65d8bb":"Please [Click here to watch  ](https:\/\/www.youtube.com\/watch?v=-la3q9d7AKQ&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=33&ab_channel=ArtificialIntelligence-AllinOne)the whole logistic regression lecture by AndrewNg  before moving further.\nWe are going to build a model based on the mathematics explained in these videos by AndrewNg","4469bad2":"survived = (results == 1).reshape(891, 1)\nnot_survived = (results == 0).reshape(891, 1)\n\nax = sns.scatterplot(x = features[survived[:, 0], 0],\n                     y = features[survived[:, 0], 1],\n                     marker = \"^\",\n                     color = \"green\",\n                     s = 60)\nsns.scatterplot(x = features[not_survived[:, 0], 0],\n                y = features[not_survived[:, 0], 1],\n                marker = \"X\",\n                color = \"red\",\n                s = 60)\n\nax.set(xlabel=\"Age\", ylabel=\"Fare\")\nax.legend([\"survived\", \"not_survived\"])\nplt.show();","fa9b2c30":"# Now lets implement the same using sklearn.**","741ff1d6":"Logistic regression is a regression analysis that predicts the probability of an outcome that can only have two values. A logistic regression produces a logistic curve, which is limited to values between 0 and 1. Logistic regression models the probability that each input belongs to a particular category. For this particular notebook we will try to predict whether a person will survive or not but without using scikit learn .","00d353f0":"## Standardization (Z-score Normalization):\n\nsubtract the mean from each feature. Then we divide the values of each feature by its standard deviation.\n\nThis will return a normalized value (z-score) based on the mean and standard deviation. A z-score, or standard score, is used for standardizing scores on the same scale by dividing a score\u2019s deviation by the standard deviation in a data set. The result is a standard score. It measures the number of standard deviations that a given data point is from the mean.\n\nA z-score can be negative or positive. A negative score indicates a value less than the mean, and a positive score indicates a value greater than the mean. The average of every z-score for a data set is zero."}}