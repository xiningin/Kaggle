{"cell_type":{"c6fc103c":"code","649b2292":"code","1b08d8a9":"code","e50d367a":"code","cb47e0b4":"code","3e4e0c7c":"code","bb83d51f":"code","f087ca6e":"code","3bdc8582":"code","aedadd70":"code","46b19ad6":"code","86b1882a":"code","8df2dea9":"code","987b0074":"code","d7009ef2":"code","eb23785e":"code","175d6dbc":"code","dede0a6b":"code","067af80a":"code","ee425239":"code","fd3d25a7":"code","bb519c26":"code","a1790e3b":"code","4728a76d":"code","d694240f":"markdown","4d75237b":"markdown","130b33d6":"markdown","2f682ca9":"markdown","12922c0f":"markdown","70cf0a4a":"markdown"},"source":{"c6fc103c":"#import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io\n","649b2292":"def sigmoid(z):\n    \n    \n    return (1\/(1+np.exp(-z)))\n    \n    \n    \n    ","1b08d8a9":"def relu(z):\n    \n    \n    return np.maximum(0,z)\n    ","e50d367a":"def initialize_parameters(layers_dims):\n    \n    \n    L = len(layers_dims)\n    \n    params = {}\n    \n    for i in range(1,L):\n        \n        params[\"W\"+str(i)] = np.random.rand(layers_dims[i] , layers_dims[i-1])\/np.sqrt(2\/layers_dims[i-1])\n        params[\"b\"+str(i)] = np.zeros((layers_dims[i] , 1))\n        \n    return params\n    \n    ","cb47e0b4":"def forward_propagation(X , params):\n    \n    W1 = params[\"W1\"]\n    b1 = params[\"b1\"]\n    W2 = params[\"W2\"]\n    b2 = params[\"b2\"]\n    W3 = params[\"W3\"]\n    b3 = params[\"b3\"]\n    \n    Z1 = np.dot(W1 , X) + b1\n    A1 = relu(Z1)\n    Z2 = np.dot(W2 , A1)+b2\n    A2 = relu(Z2)\n    Z3 = np.dot(W3 , A2)+b3\n    A3 = sigmoid(Z3)\n    \n    cache = (Z1,A1,W1,b1,Z2,A2,W2,b2,Z3,A3,W3,b3)\n    \n    return A3 , cache\n    \n    \n    ","3e4e0c7c":"def back_propagation(X,Y,cache):\n    \n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1.\/m * np.dot(dZ3, A2.T)\n    db3 = 1.\/m * np.sum(dZ3, axis=1, keepdims = True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1.\/m * np.dot(dZ2, A1.T)\n    db2 = 1.\/m * np.sum(dZ2, axis=1, keepdims = True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1.\/m * np.dot(dZ1, X.T)\n    db1 = 1.\/m * np.sum(dZ1, axis=1, keepdims = True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients\n    ","bb83d51f":"def update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters:\n                    parameters['W' + str(i)] = Wi\n                    parameters['b' + str(i)] = bi\n    grads -- python dictionary containing your gradients for each parameters:\n                    grads['dW' + str(i)] = dWi\n                    grads['db' + str(i)] = dbi\n    learning_rate -- the learning rate, scalar.\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    \n    n = len(parameters) \/\/ 2 # number of layers in the neural networks\n\n    # Update rule for each parameter\n    for k in range(n):\n        parameters[\"W\" + str(k+1)] = parameters[\"W\" + str(k+1)] - learning_rate * grads[\"dW\" + str(k+1)]\n        parameters[\"b\" + str(k+1)] = parameters[\"b\" + str(k+1)] - learning_rate * grads[\"db\" + str(k+1)]\n        \n    return parameters","f087ca6e":"def predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  n-layer neural network.\n    \n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n    \n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    \n    m = X.shape[1]\n    p = np.zeros((1,m), dtype = np.int)\n    \n    # Forward propagation\n    a3, caches = forward_propagation(X, parameters)\n    \n    # convert probas to 0\/1 predictions\n    for i in range(0, a3.shape[1]):\n        if a3[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    # print results\n\n    #print (\"predictions: \" + str(p[0,:]))\n    #print (\"true labels: \" + str(y[0,:]))\n    print(\"Accuracy: \"  + str(np.mean((p[0,:] == y[0,:]))))\n    \n    return p","3bdc8582":"def compute_cost(a3, Y):\n    \"\"\"\n    Implement the cost function\n    \n    Arguments:\n    a3 -- post-activation, output of forward propagation\n    Y -- \"true\" labels vector, same shape as a3\n    \n    Returns:\n    cost - value of the cost function\n    \"\"\"\n    m = Y.shape[1]\n    \n    logprobs = np.multiply(-np.log(a3),Y) + np.multiply(-np.log(1 - a3), 1 - Y)\n    cost = 1.\/m * np.nansum(logprobs)\n    \n    return cost","aedadd70":"def predict_dec(parameters, X):\n    \"\"\"\n    Used for plotting decision boundary.\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (m, K)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n    \"\"\"\n    \n    # Predict using forward propagation and a classification threshold of 0.5\n    a3, cache = forward_propagation(X, parameters)\n    predictions = (a3>0.5)\n    return predictions","46b19ad6":"def plot_decision_boundary(model , X , Y):\n    \n    x_min , x_max = X[0,:].min()-1 , X[0,:].max()+1\n    Y_min , Y_max = X[1,:].min()-1 , X[1,:].max()+1\n    h = 0.01\n    \n    xx , yy = np.meshgrid(np.arange(x_min , x_max,h) , np.arange(Y_min , Y_max,h))\n    \n    Z = model(np.c_[xx.ravel() , yy.ravel()])\n    \n    Z = Z.reshape(xx.shape)\n    \n    plt.contourf(xx,yy,Z,cmap = plt.cm.Spectral)\n    plt.ylabel(\"x2\")\n    plt.xlabel(\"x1\")\n    plt.scatter(X[0 , :] , X[1 , :] , c = Y , cmap = plt.cm.Spectral)\n    plt.show()\n    \n    ","86b1882a":"def load_2D_dataset():\n    data = scipy.io.loadmat('..\/input\/pos-data\/data.mat')\n    train_X = data['X'].T # contain position of team player and opponents players\n    train_Y = data['y'].T # contain label such that this position is good or not to pass ball\n    test_X = data['Xval'].T\n    test_Y = data['yval'].T\n\n    plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral);\n    \n    return train_X, train_Y, test_X, test_Y","8df2dea9":"train_x,train_y,test_x,test_y = load_2D_dataset()","987b0074":"def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):\n    \"\"\"\n    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot \/ 0 for red dot), of shape (output size, number of examples)\n    learning_rate -- learning rate of the optimization\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- If True, print the cost every 10000 iterations\n    lambd -- regularization hyperparameter, scalar\n    keep_prob - probability of keeping a neuron active during drop-out, scalar.\n    \n    Returns:\n    parameters -- parameters learned by the model. They can then be used to predict.\n    \"\"\"\n        \n    grads = {}\n    costs = []                            # to keep track of the cost\n    m = X.shape[1]                        # number of examples\n    layers_dims = [X.shape[0], 20, 3, 1]\n    \n    # Initialize parameters dictionary.\n    parameters = initialize_parameters(layers_dims)\n\n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n        if keep_prob == 1:\n            a3, cache = forward_propagation(X, parameters)\n        elif keep_prob < 1:\n            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)\n        \n        # Cost function\n        if lambd == 0:\n            cost = compute_cost(a3, Y)\n        else:\n            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)\n            \n        # Backward propagation.\n        assert (lambd == 0 or keep_prob == 1)   # it is possible to use both L2 regularization and dropout, \n                                                # but this assignment will only explore one at a time\n        if lambd == 0 and keep_prob == 1:\n            grads = back_propagation(X, Y, cache)\n        elif lambd != 0:\n            grads = backward_propagation_with_regularization(X, Y, cache, lambd)\n        elif keep_prob < 1:\n            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)\n        \n        # Update parameters.\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the loss every 10000 iterations\n        if print_cost and i % 10000 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, cost))\n        if print_cost and i % 1000 == 0:\n            costs.append(cost)\n    \n    # plot the cost\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('iterations (x1,000)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","d7009ef2":"parameters = model(train_x, train_y)\nprint (\"On the training set:\")\npredictions_train = predict(train_x, train_y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_x, test_y, parameters)","eb23785e":"plt.title(\"Model without regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_x, train_y)","175d6dbc":"# GRADED FUNCTION: compute_cost_with_regularization\n\ndef compute_cost_with_regularization(A3, Y, parameters, lambd):\n    \"\"\"\n    Implement the cost function with L2 regularization. See formula (2) above.\n    \n    Arguments:\n    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    parameters -- python dictionary containing parameters of the model\n    \n    Returns:\n    cost - value of the regularized loss function (formula (2))\n    \"\"\"\n    m = Y.shape[1]\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n    W3 = parameters[\"W3\"]\n    \n    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost\n    \n    \n    L2_regularization_cost = (np.sum(np.square(W1)) + np.sum(np.square(W2))+np.sum(np.square(W3))) * (lambd \/ (2*m))\n    \n    cost = cross_entropy_cost + L2_regularization_cost\n    \n    return cost","dede0a6b":"# GRADED FUNCTION: backward_propagation_with_regularization\n\ndef backward_propagation_with_regularization(X, Y, cache, lambd):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n    \n    Arguments:\n    X -- input dataset, of shape (input size, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation()\n    lambd -- regularization hyperparameter, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1\/m * np.dot(dZ3 , A2.T) + lambd\/m*W3\n    \n    db3 = 1. \/ m * np.sum(dZ3, axis=1, keepdims=True)\n    \n    dA2 = np.dot(W3.T, dZ3)\n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1\/m * np.dot(dZ2 , A1.T) + lambd\/m*W2\n    \n    db2 = 1. \/ m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1\/m * np.dot(dZ1 , X.T) + lambd\/m*W1\n    \n    db1 = 1. \/ m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients","067af80a":"parameters = model(train_x, train_y, lambd = 0.7)\nprint (\"On the train set:\")\npredictions_train = predict(train_x, train_y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_x, test_y, parameters)","ee425239":"plt.title(\"Model with L2-regularization\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_x, train_y)","fd3d25a7":"# GRADED FUNCTION: forward_propagation_with_dropout\n\ndef forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):\n    \"\"\"\n    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n                    W1 -- weight matrix of shape (20, 2)\n                    b1 -- bias vector of shape (20, 1)\n                    W2 -- weight matrix of shape (3, 20)\n                    b2 -- bias vector of shape (3, 1)\n                    W3 -- weight matrix of shape (1, 3)\n                    b3 -- bias vector of shape (1, 1)\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    A3 -- last activation value, output of the forward propagation, of shape (1,1)\n    cache -- tuple, information stored for computing the backward propagation\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    # retrieve parameters\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    W3 = parameters[\"W3\"]\n    b3 = parameters[\"b3\"]\n    \n    Z1 = np.dot(W1, X) + b1\n    A1 = relu(Z1)\n    \n    D1 = np.random.rand(A1.shape[0] , A1.shape[1])\n    D1 = (D1 < keep_prob).astype(int)\n    A1 = np.multiply(A1,D1)\n    A1 = A1\/keep_prob\n    \n    \n    Z2 = np.dot(W2, A1) + b2\n    A2 = relu(Z2)\n    D2 = np.random.rand(A2.shape[0] , A2.shape[1])\n    D2 = (D2<keep_prob).astype(int)\n    A2 = np.multiply(A2,D2)\n    A2 = A2\/keep_prob\n    \n    Z3 = np.dot(W3, A2) + b3\n    A3 = sigmoid(Z3)\n    \n    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)\n    \n    return A3, cache","bb519c26":"# GRADED FUNCTION: backward_propagation_with_dropout\n\ndef backward_propagation_with_dropout(X, Y, cache, keep_prob):\n    \"\"\"\n    Implements the backward propagation of our baseline model to which we added dropout.\n    \n    Arguments:\n    X -- input dataset, of shape (2, number of examples)\n    Y -- \"true\" labels vector, of shape (output size, number of examples)\n    cache -- cache output from forward_propagation_with_dropout()\n    keep_prob - probability of keeping a neuron active during drop-out, scalar\n    \n    Returns:\n    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n    \"\"\"\n    \n    m = X.shape[1]\n    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache\n    \n    dZ3 = A3 - Y\n    dW3 = 1.\/m * np.dot(dZ3, A2.T)\n    db3 = 1.\/m * np.sum(dZ3, axis=1, keepdims=True)\n    dA2 = np.dot(W3.T, dZ3)\n    dA2 = dA2 * D2\n    dA2 = dA2 \/ keep_prob\n    \n    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n    dW2 = 1.\/m * np.dot(dZ2, A1.T)\n    db2 = 1.\/m * np.sum(dZ2, axis=1, keepdims=True)\n    \n    dA1 = np.dot(W2.T, dZ2)\n    dA1 = dA1 * D1\n    dA1 = dA1 \/ keep_prob\n    \n    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n    dW1 = 1.\/m * np.dot(dZ1, X.T)\n    db1 = 1.\/m * np.sum(dZ1, axis=1, keepdims=True)\n    \n    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n    \n    return gradients","a1790e3b":"parameters = model(train_x, train_y, keep_prob = 0.86, learning_rate = 0.3)\n\nprint (\"On the train set:\")\npredictions_train = predict(train_x, train_y, parameters)\nprint (\"On the test set:\")\npredictions_test = predict(test_x, test_y, parameters)","4728a76d":"plt.title(\"Model with dropout\")\naxes = plt.gca()\naxes.set_xlim([-0.75,0.40])\naxes.set_ylim([-0.75,0.65])\nplot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_x, train_y)","d694240f":"# first using non regularized model","4d75237b":"Of course, because you changed the cost, you have to change backward propagation as well! All the gradients have to be computed with respect to this new cost. ","130b33d6":"# Using L2 regularization","2f682ca9":"# Using dropout regularization","12922c0f":"recommend positions where France's goal keeper should kick the ball so that the French team's players can then hit it with their head.","70cf0a4a":"If the dot is blue, it means the French player managed to hit the ball with his\/her head\nIf the dot is red, it means the other team's player hit the ball with their head"}}