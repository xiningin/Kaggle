{"cell_type":{"cc7d2ae8":"code","becfbabe":"code","eab4e9bf":"code","f6f39cc6":"code","6ba1d286":"code","d70d3959":"code","eba1d13c":"code","402070f3":"code","a43a2e42":"code","cfe4969e":"code","44dcc411":"code","f33b0385":"code","b48a8c30":"code","b0dc6496":"code","74688608":"code","3842bbaa":"code","c9a0acab":"code","0c2266a7":"code","29860090":"code","ff8bd1b4":"code","7aecc641":"code","8fc0c9d7":"code","267153a8":"code","187a82be":"code","24e811e8":"code","a31461da":"code","4ecea6eb":"code","8a24c294":"code","5fcaafd2":"code","8acecc3d":"code","cb44e362":"code","7bffd3d8":"code","02910217":"code","74f31398":"code","5136c29e":"code","27c771ab":"code","bcb120c3":"code","9a8c0bf3":"code","4a97187c":"code","bb0ad475":"code","f9712a1d":"code","f333868c":"markdown","39e9ca77":"markdown","8e0e2c29":"markdown","521521ca":"markdown","ee0842ce":"markdown","e809e153":"markdown","0bdd744a":"markdown","d65967e2":"markdown","26a52542":"markdown","68f4b3ab":"markdown","0c36feba":"markdown","d4aae5eb":"markdown","3f77403e":"markdown","6195b8c5":"markdown","4c086cf7":"markdown","b0bf8eb2":"markdown","da74a925":"markdown"},"source":{"cc7d2ae8":"import pandas as pd\npd.options.display.max_rows = 999\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\n\nprint('Import Complete')","becfbabe":"data = pd.read_csv('..\/input\/fish-market\/Fish.csv')\ndata.head()","eab4e9bf":"print('Shape of data:',data.shape)\ndata.isnull().sum()","f6f39cc6":"data.info(),data.describe()","6ba1d286":"#there is zero weigth observed in weight , weight would never be zero \n#raw_data[raw_data['Weight']==0] \n\nraw_data = data.copy()\nraw_data\n#Fixing the outlier. Mean of the species weight is taken and assigned\nmean_fish = data['Weight'][(data['Species'] =='Roach') & (data['Weight'] != 0)].mean()\nraw_data.loc[40,'Weight'] = mean_fish\nraw_data.loc[40,'Weight']","d70d3959":"# there is no null values and next is to check the outliers\n\n# to check Outliers\nfor i in raw_data.select_dtypes('float64').columns:\n    plt.figure(figsize=(7,5))\n    sns.distplot(raw_data[i])\n    plt.show()\n    print('Skewness is', raw_data[i].skew())\n    #print('Kurtosis is', raw_data[i].kurtosis())                ","eba1d13c":"#there is no outliers because skewness value of all columns are in acceptable range\n# next we go our preprocessing of data\nsns.barplot(x=raw_data['Species'],y=raw_data['Weight'])\nplt.title('Species Vs Weight')\n","402070f3":"f, (ax1, ax2, ax3,ax4, ax5) = plt.subplots(5,1,sharey=True, figsize =(10,40))\nax1.scatter(raw_data['Length1'],raw_data['Weight'])\nax1.set_title('length1 and Weight')\nax2.scatter(raw_data['Length2'],raw_data['Weight'])\nax2.set_title('length2 and Weight')\nax3.scatter(raw_data['Length3'],raw_data['Weight'])\nax3.set_title('length1 and Weight')\nax4.scatter(raw_data['Height'],raw_data['Weight'])\nax4.set_title('Height and Weight')\nax5.scatter(raw_data['Width'],raw_data['Weight'])\nax5.set_title('Width and Weight')","a43a2e42":"#take log to linearity\nraw_data['LogWeight']= np.log(raw_data['Weight'])\n#[np.log[i] for i in raw_data['Weight'] if i!=0 ]\n#\nraw_data['LogWeight']","cfe4969e":"\n\nf, (ax1, ax2, ax3,ax4, ax5) = plt.subplots(5,1,sharey=True, figsize =(10,40))\nax1.scatter(raw_data['Length1'],raw_data['LogWeight'])\nax1.set_title('length1 and Log Weight')\nax2.scatter(raw_data['Length2'],raw_data['LogWeight'])\nax2.set_title('length2 and Log Weight')\nax3.scatter(raw_data['Length3'],raw_data['LogWeight'])\nax3.set_title('length1 and Log Weight')\nax4.scatter(raw_data['Height'],raw_data['LogWeight'])\nax4.set_title('Height and Log Weight')\nax5.scatter(raw_data['Width'],raw_data['LogWeight'])\nax5.set_title('Width and Log Weight')","44dcc411":"raw_data['LogLength1']=np.log(raw_data['Length1'])\nraw_data['LogLength2']=np.log(raw_data['Length2'])\nraw_data['LogLength3']=np.log(raw_data['Length3'])\nraw_data['LogHeight']=np.log(raw_data['Height'])\nraw_data['LogWidth']=np.log(raw_data['Width'])\nraw_data.head(7)","f33b0385":"f, (ax1, ax2, ax3,ax4, ax5) = plt.subplots(5,1,sharey=True, figsize =(10,40))\nax1.scatter(raw_data['LogLength1'],raw_data['LogWeight'])\nax1.set_title('Loglength1 and Log Weight')\nax2.scatter(raw_data['LogLength2'],raw_data['LogWeight'])\nax2.set_title('Loglength2 and Log Weight')\nax3.scatter(raw_data['LogLength3'],raw_data['LogWeight'])\nax3.set_title('Loglength1 and Log Weight')\nax4.scatter(raw_data['LogHeight'],raw_data['LogWeight'])\nax4.set_title('LogHeight and Log Weight')\nax5.scatter(raw_data['LogWidth'],raw_data['LogWeight'])\nax5.set_title('LogWidth and Log Weight')","b48a8c30":"cleaned_data = raw_data.drop(['Weight','Length1','Length2','Length3','Height','Width'],axis=1)","b0dc6496":"# only 'species' column inthe data set is categorical ,we need to label the column \nraw_data['Species'].unique()","74688608":"#selct only the categorical column to impute\n\nlabel_data = cleaned_data.copy()\n\nencoder = LabelEncoder()\nlabel_data['Species'] = encoder.fit_transform(raw_data['Species'])\n#raw_data['Species'].nunique()\nlabel_data.head(5)\n#label_data['Species'].nunique()","3842bbaa":"label_data.describe()","c9a0acab":"#dependant and independant variable\ny= label_data['LogWeight']\nx= label_data.drop(['LogWeight'],axis=1)\n","0c2266a7":"import statsmodels.api as sm\nx_const = sm.add_constant(x)\nresults = sm.OLS(y,x).fit()\nresults.summary()","29860090":"#drop the \"length1\" that has more p-value to get better results\n\ny= label_data['LogWeight']\nx1= label_data.drop(['LogWeight','LogLength3'],axis=1)\n","ff8bd1b4":"import statsmodels.api as sm\nx_const = sm.add_constant(x1)\nresults2 = sm.OLS(y,x1).fit()\nresults2.summary()","7aecc641":"#so we have to drop column 'LogLenth3' from the dataset\nx.drop('LogLength3',axis=1)\nx.head(7)","8fc0c9d7":"# train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=365)\nprint(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","267153a8":"lreg= LinearRegression()\nlreg.fit(x_train,y_train)","187a82be":"\ny_pred = lreg.predict(x_train)\ny_pred","24e811e8":"# The simplest way to compare the targets (y_train) and the predictions (y_pred) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_train, y_pred , alpha=0.5)\n# Let's also name the axes\nplt.xlabel('Targets (y_train)',size=18)\nplt.ylabel('Predictions (y_pred)',size=18)\n# Sometimes the plot will have different scales of the x-axis and the y-axis\n# This is an issue as we won't be able to interpret the '45-degree line'\n# We want the x-axis and the y-axis to be the same\nplt.xlim(0,8)\nplt.ylim(0,8)\nplt.show()","a31461da":"# Another useful check of our model is a residual plot\n# We can plot the PDF of the residuals and check for anomalies\nsns.distplot(y_train - y_pred)\n\n# Include a title\nplt.title(\"Residuals PDF\", size=18)","4ecea6eb":"# checking the Score\n\nlreg.score(x_train,y_train)","8a24c294":"lreg.coef_","5fcaafd2":"lreg.intercept_","8acecc3d":"from sklearn.metrics import mean_absolute_error\n\nmean_absolute_error(y_train,y_pred)","cb44e362":"y_pred_test = lreg.predict(x_test)\ny_pred","7bffd3d8":"lreg.score(x_test,y_test)","02910217":"lreg.intercept_","74f31398":"lreg.coef_","5136c29e":"from sklearn.metrics import mean_absolute_error\n\nmean_absolute_error(y_test,y_pred_test)","27c771ab":"# The simplest way to compare the targets (y_test) and the predictions (y_pred) is to plot them on a scatter plot\n# The closer the points to the 45-degree line, the better the prediction\nplt.scatter(y_test, y_pred_test , alpha=0.5)\n# Let's also name the axes\nplt.xlabel('Targets (y_test)',size=18)\nplt.ylabel('Predictions (y_pred_test)',size=18)\n# Sometimes the plot will have different scales of the x-axis and the y-axis\n# This is an issue as we won't be able to interpret the '45-degree line'\n# We want the x-axis and the y-axis to be the same\nplt.xlim(0,8)\nplt.ylim(0,8)\nplt.show()","bcb120c3":"# Another useful check of our model is a residual plot\n# We can plot the PDF of the residuals and check for anomalies\nsns.distplot(y_test - y_pred_test)\n\n# Include a title\nplt.title(\"Residuals PDF\", size=18)","9a8c0bf3":"# Finally, let's manually check these predictions\n# To obtain the actual prices, we take the exponential of the log_price\ndf_pf = pd.DataFrame(np.exp(y_pred_test), columns=['Prediction'])\ndf_pf.head(7)","4a97187c":"# reset the index to create the tabel\ny_test = y_test.reset_index(drop=True)\n# Let's overwrite the 'Target' column with the appropriate values\n# Again, we need the exponential of the test log price\ndf_pf['Target'] = np.exp(y_test)\n\n# Additionally, we can calculate the difference between the targets and the predictions\n# Note that this is actually the residual (we already plotted the residuals)\ndf_pf['Residual'] = df_pf['Target'] - df_pf['Prediction']\n\n# Since OLS is basically an algorithm which minimizes the total sum of squared errors (residuals),\n# this comparison makes a lot of sense\ndf_pf['Difference%'] = np.absolute(df_pf['Residual']\/df_pf['Target']*100)\ndf_pf.head(7)","bb0ad475":"# Exploring the descriptives here gives us additional insights\ndf_pf.describe()","f9712a1d":"# To see all rows, we use the relevant pandas syntax\npd.options.display.max_rows = 999\n# Moreover, to make the dataset clear, we can display the result with only 2 digits after the dot \npd.set_option('display.float_format', lambda x: '%.2f' % x)\n# Finally, we sort by difference in % and manually check the model\ndf_pf.sort_values(by=['Difference%'])","f333868c":"# Analysing the data","39e9ca77":"This is my first Machine Learning algorithm notebook on kaggle and  feel free to ask me any queries about this notebook\n\nComment your thoughts about this notebook and suggest me some methods to Improve the model\n\nThank You !..","8e0e2c29":"# Creating the summary","521521ca":"## Train_Test_Split","ee0842ce":"### Wow ! Perfect... R.Score is 99% it is Perfectly fit...","e809e153":"# Import Relevant Libraries","0bdd744a":"#  Preprocessing the data ","d65967e2":"Content:\n\n    1.Import Relevant libraries\n    2.Import Data\n    3.Analyze the Data\n    4.Preprocessing the Data\n    5.Checking the data using StatsModels\n    6.Model Creation \n    7.Testing the model","26a52542":"# Checking the data using StatsModels","68f4b3ab":"# Testing the model","0c36feba":"#       Regression Analysis of predicting Fish Weight","d4aae5eb":"# Create the Model","3f77403e":"We take log Transformation to aceive linearity of the Model\n\nClick the link to know more about LogTransformation\nhttps:\/\/www.real-statistics.com\/multiple-regression\/multiple-regression-log-transformations\/","6195b8c5":"## Dealing with Categorical Variables","4c086cf7":"    Sometimes categorical varibales also play a amjor role in predicting the results,so we have to deal the categorical variables\n 1.Drop the Categorical Columns\n 2.Imputing the categorical Variable(Label Encoder)\n 3.OneHot Encoding\n \n https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables","b0bf8eb2":"# Import Data","da74a925":"### OLS Model"}}