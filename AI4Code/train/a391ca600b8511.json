{"cell_type":{"fd65854d":"code","71cafa9a":"code","edcbe020":"code","e7cc2750":"code","c463d18b":"code","61f02dcf":"code","5987df8b":"code","2826619a":"code","f090f90a":"code","fc47a9e5":"code","09302611":"code","5a1d3ec8":"code","8c44dda6":"code","d29e9773":"code","88f1e01f":"code","2e6862bd":"code","51f5a1d7":"code","e67ac3f1":"code","b9d8dac1":"code","b08926cd":"code","84006f86":"markdown","4c7e2eba":"markdown","0f8220b1":"markdown","c36c75d1":"markdown","03b930a8":"markdown","bbcc1b70":"markdown","a14efc00":"markdown","eb8c0ac7":"markdown","e9fb3932":"markdown"},"source":{"fd65854d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","71cafa9a":"holiday=pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/holidays_events.csv\")\noil=pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/oil.csv\")\nsample_submission=pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/sample_submission.csv\")\nstores=pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/stores.csv\")\ntest=pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/test.csv\")\ntrain=pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/train.csv\")\ntransactions=pd.read_csv(\"..\/input\/store-sales-time-series-forecasting\/transactions.csv\")","edcbe020":"print (\"Training Data Shape: \", train.shape)\nprint (\"Testing Data Shape: \", test.shape)\ntrain.head()","e7cc2750":"train1 = train.merge(oil, on = 'date', how='left')\ntrain1 = train1.merge(holiday, on = 'date', how='left')\ntrain1 = train1.merge(stores, on = 'store_nbr', how='left')\ntrain1 = train1.merge(transactions, on = ['date', 'store_nbr'], how='left')\ntrain1 = train1.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\n\ntest1 = test.merge(oil, on = 'date', how='left')\ntest1 = test1.merge(holiday, on = 'date', how='left')\ntest1 = test1.merge(stores, on = 'store_nbr', how='left')\ntest1 = test1.merge(transactions, on = ['date', 'store_nbr'], how='left')\ntest1 = test1.rename(columns = {\"type_x\" : \"holiday_type\", \"type_y\" : \"store_type\"})\ntrain1.head()","c463d18b":"test1.head()","61f02dcf":"train1[\"family\"].value_counts()","5987df8b":"train1[\"city\"].value_counts()","2826619a":"train1[\"state\"].value_counts()","f090f90a":"train1[\"onpromotion\"].value_counts()","fc47a9e5":"train1[\"store_type\"].value_counts()","09302611":"import seaborn as sns\ncorr = train1.corr()\nsns.heatmap(corr)","5a1d3ec8":"sns.set(rc={'figure.figsize':(20,8.27)})\nsns.barplot(x = 'store_nbr',y = 'sales',data = train1,palette = \"Blues\")","8c44dda6":"sns.set(rc={'figure.figsize':(20,8.27)})\nsns.barplot(x = 'store_nbr',y = 'transactions',data = train1,palette = \"Blues\")","d29e9773":"sns.set(rc={'figure.figsize':(20,8.27)})\nsns.lineplot(x = \"transactions\",y = 'sales',data = train1,palette = \"Blues\")","88f1e01f":"sns.set(rc={'figure.figsize':(20,8.27)})\nsns.lineplot(x = \"onpromotion\",y = 'sales',data = train1,palette = \"Blues\")","2e6862bd":"sns.set(rc={'figure.figsize':(20,8.27)})\nsns.barplot(x = 'cluster',y = 'transactions',data = train1,palette = \"Blues\")","51f5a1d7":"from sklearn.model_selection import train_test_split\nfeatures=['date','store_nbr','family','onpromotion','dcoilwtico','holiday_type','locale','locale_name','description','transferred','city','state','store_type','cluster','transactions']\nX=train1[features]\ny=train1.sales\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)","e67ac3f1":"# linear regression feature importance\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom matplotlib import pyplot\n# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# define the model\nmodel = LinearRegression()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.coef_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\n    pyplot.bar([x for x in range(len(importance))], importance)\n    pyplot.show()","b9d8dac1":"def feature_eng(data):\n    data['date'] = pd.to_datetime(data['date'])\n    data['dayofweek'] = data['date'].dt.dayofweek\n    data['quarter'] = data['date'].dt.quarter\n    data['month'] = data['date'].dt.month\n    data['year'] = data['date'].dt.year\n    data['dayofyear'] = data['date'].dt.dayofyear\n    data['dayofmonth'] = data['date'].dt.day\n    return data\n    \ntrain1 = feature_eng(train1)\ntest1 = feature_eng(test1)\ntrain1.head()","b08926cd":"train1.to_csv(\"train_m_fe.csv\", index = False)\ntest1.to_csv(\"test_m_fe.csv\", index = False)","84006f86":"# Correlation b\/w features","4c7e2eba":"# Feature importance plot","0f8220b1":"# Spilliting Dataset","c36c75d1":"# Load Datasets","03b930a8":"# Merging Datasets","bbcc1b70":"# Visualizing closely correlated features","a14efc00":"# Feature Engineering","eb8c0ac7":"# Value Count for each feature","e9fb3932":"Several supplement files are provided which contain addition features, which can be cobined to training and test datasets(original)."}}