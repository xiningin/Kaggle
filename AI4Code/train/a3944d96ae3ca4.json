{"cell_type":{"15ed1265":"code","03906f5d":"code","62def7df":"code","d9684665":"code","d6d8e512":"code","b9bdcd6f":"code","901e2ace":"code","1070e868":"code","199acf19":"code","cc786cdd":"code","bedbfcd2":"code","e2947421":"code","eb557244":"code","1b8c0ffc":"code","add8821c":"code","edfaf503":"code","99a2d62d":"code","4dde7dfe":"code","579ddb13":"code","ce928aae":"code","fc38e8be":"code","90899737":"code","44f10434":"code","20605f6e":"code","f7218c2b":"code","57f492e1":"code","c76ba9a4":"code","0a25578c":"code","85fe6757":"code","43281419":"code","633a9af4":"code","f0a54f45":"code","73cdedbb":"code","b2d307b8":"code","aca57b92":"code","892c5ec5":"markdown","f036135a":"markdown","6ff851e8":"markdown","168befe3":"markdown","ee743841":"markdown","c44e0a05":"markdown","4c0cd0e4":"markdown","8b7127ac":"markdown","ea7fd8e8":"markdown","5b936f9d":"markdown"},"source":{"15ed1265":"import pandas as pd  # for dataframe\nimport numpy as np   # for dealing with array\nimport matplotlib.pyplot as plt   # for visualising\nimport seaborn as sns  # for visualising","03906f5d":"data = pd.read_csv(\"..\/input\/multi-organ-failure-prediction\/mods_dataset.csv\")","62def7df":"data.head(5)","d9684665":"data.isnull().sum()       # To check the NaN values","d6d8e512":"data.keys()   # To the name of the features","b9bdcd6f":"l = []\nfor i in data.keys():\n    l.append(data[i].isnull().sum())       # To check the NaN values","901e2ace":"for i in l:\n    print(i,end=\" \")","1070e868":"data.info()","199acf19":"data.head(4)","cc786cdd":"data.corr()","bedbfcd2":"data.fillna(0,inplace = True)","e2947421":"data.isnull().sum()","eb557244":"from sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()","1b8c0ffc":"data['Label'] = encode.fit_transform(data['Label'])","add8821c":"x = data.iloc[:,:-1]\ny = data.iloc[:,[-1]]","edfaf503":"x","99a2d62d":"y","4dde7dfe":"from sklearn.impute import SimpleImputer\n\nimpute = SimpleImputer()","579ddb13":"x = impute.fit_transform(x)","ce928aae":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 1\/4)","fc38e8be":"import warnings\nwarnings.filterwarnings('ignore')\n\n# Grid search cross validation\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nlogreg=LogisticRegression()\nlogreg_cv=GridSearchCV(logreg,grid,cv=3)\nlogreg_cv.fit(x_train,y_train)\nprint(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\nprint(\"accuracy :\",logreg_cv.best_score_)","90899737":"regex = LogisticRegression(C=1000,penalty = 'l2')","44f10434":"regex.fit(x_train,y_train)","20605f6e":"pred = regex.predict(x_test)","f7218c2b":"from sklearn.metrics import accuracy_score\n\nprint(accuracy_score(y_test,pred))","57f492e1":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(y_test, pred))","c76ba9a4":"from sklearn.ensemble import RandomForestClassifier","0a25578c":"rfc=RandomForestClassifier(random_state=42)\n","85fe6757":"param_grid = { \n    'n_estimators': [100,125,150,175,200,250,300,350,400,450,500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}","43281419":"import warnings\nwarnings.filterwarnings('ignore')\n\nCV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\nCV_rfc.fit(x_train, y_train)","633a9af4":"CV_rfc.best_params_","f0a54f45":"rfc=RandomForestClassifier(criterion = 'entropy',\n max_depth = 4,\n max_features = 'log2',\n n_estimators = 125,\nrandom_state=42)","73cdedbb":"rfc.fit(x_train,y_train)","b2d307b8":"pred_f = rfc.predict(x_test)","aca57b92":"print(classification_report(y_test,pred_f))","892c5ec5":"# Make a classifier model","f036135a":"**So I have performed gridcv on my Logistic model to check for best parameters that I can get**","6ff851e8":"## Let's import the data on which we are going to work","168befe3":"# Let's regularise our data","ee743841":"# Before moving ahead lets deal with the categorical values","c44e0a05":"**As you can see that there is 61 total entries and from those 61 there are many columns which have the NA so now we have two options either we can replace it or delete but we cannot delete it as there is already less entries, so we will look at the pattern and replace NaN values with the most suitted one.**","4c0cd0e4":"## Start with import the libraries","8b7127ac":"**We got .625 accuracy but it is not good so lets move to build more complex model**","ea7fd8e8":"**If any column contains NaN values then that means that person might not have that feature, so we will fill NaN with 0**","5b936f9d":"# Making different sets from our dataset to train the model"}}