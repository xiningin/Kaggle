{"cell_type":{"ed708ceb":"code","b556ff47":"code","0619456c":"code","289b851b":"code","36b32ef5":"code","e4691f29":"code","cf8d2540":"code","6c338d46":"code","ecca2848":"code","347d9812":"code","2870f507":"markdown","095643b1":"markdown","7ebf5389":"markdown","9e396206":"markdown","efcb02dd":"markdown","d3fde186":"markdown","3c78c06c":"markdown"},"source":{"ed708ceb":"!pip install rank_bm25","b556ff47":"#from ktb import *\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.cluster import DBSCAN\nfrom nltk.corpus import stopwords\nfrom collections import  Counter\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport pyLDAvis.gensim\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport pyLDAvis\nimport gensim\nimport spacy\nimport os\nfrom gensim.models import KeyedVectors,  keyedvectors\nimport multiprocessing as mp\nimport re\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom annoy import AnnoyIndex\nfrom functools import lru_cache \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pathlib import Path, PurePath\nimport pandas as pd\nimport requests\nfrom requests.exceptions import HTTPError, ConnectionError\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download(\"punkt\")\nimport re\nfrom rank_bm25 import BM25Okapi","0619456c":"from typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n\nimport pandas as pd\nimport os\nimport pickle\nimport gzip\nimport numpy as np\nimport random as rn\nfrom datetime import datetime\n\nSEED = 777\n\noutput_dir = \"..\/Results\"\n\ndef write_submission(df, cols = None):\n    if cols is None:\n        cols = df.columns\n    time_now = datetime.strftime(datetime.now(), \"%Y-%m-%d_%H-%M-%S\")\n    df[cols].to_csv(os.path.join(output_dir,f'submission-{time_now}.csv'), index=False, float_format='%.4f')\n    \ndef read_object(file):\n    with gzip.open(file, \"rb\") as f:\n        return pickle.load(f)\n\ndef write_object(file, obj):\n    with gzip.open(file, \"wb\") as f:\n        pickle.dump(obj, f)\n\ndef cache_func(func,key):\n    if not os.path.exists(f\"cache\"):\n        os.mkdir(\"cache\")\n    key = key+func.__name__\n    def inner_func(*args, **kwargs):\n        try: \n            if os.path.exists(f\"cache\/{key}\"):\n                return read_object(f\"cache\/{key}\")\n        except:\n            pass\n        obj = func(*args, **kwargs)\n        write_object(f\"cache\/{key}\", obj)\n        return obj\n    return inner_func\n\ndef seed_everything(seed):\n    rn.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\ndef reduce_mem_usage(df: pd.DataFrame,\n                     verbose: bool = True) -> pd.DataFrame:\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if (c_min > np.iinfo(np.int8).min\n                        and c_max < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif (c_min > np.iinfo(np.int16).min\n                      and c_max < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif (c_min > np.iinfo(np.int32).min\n                      and c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min\n                      and c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                        and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > np.finfo(np.float32).min\n                      and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    reduction = (start_mem - end_mem) \/ start_mem\n    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n    if verbose:\n        print(msg)\n    return df\n\ndef maddest(d : Union[np.array, pd.Series, List], axis : Optional[int]=None) -> np.array:  \n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef batching(df : pd.DataFrame,\n             batch_size : int,\n             add_index : Optional[bool]=True) -> pd.DataFrame :\n\n    df['batch_'+ str(batch_size)] = df.groupby(df.index\/\/batch_size, sort=False)[df.columns[0]].agg(['ngroup']).values + 1\n    df['batch_'+ str(batch_size)] = df['batch_'+ str(batch_size)].astype(np.uint16)\n    if add_index:\n        df['batch_' + str(batch_size) +'_idx'] = df.index  - (df['batch_'+ str(batch_size)] * batch_size)\n        df['batch_' + str(batch_size) +'_idx'] = df['batch_' + str(batch_size) +'_idx'].astype(np.uint16)\n    return df\n\ndef flattern_values(obj, func=None):\n    res = []\n    if isinstance(obj, dict):\n        for v in obj.values:\n            res.extend(flattern_values(v, func))\n    elif isinstance(obj, list):\n        for v in obj:\n            res.extend(flattern_values(v, func))\n    else:\n        if func is not None:\n            res.extend(flattern_values(func(obj), None))\n        else:\n            res.append(obj)\n\n    return res\n\n\ndef apply2values(obj, func):\n    res = None\n    if isinstance(obj, dict):\n        res = {k:apply2values(v, func) for k,v in obj.items}\n    elif isinstance(obj, list):\n        res = [apply2values(v, func) for v in obj]\n    else:\n        res = func(obj)\n    return res\n\nseed_everything(SEED)\n\n\n\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import KeyedVectors,keyedvectors\nimport numpy as np\nimport re\nfrom annoy import AnnoyIndex\nfrom collections import defaultdict\nimport unidecode\nfrom gensim.summarization.bm25 import BM25\nfrom nltk.corpus import stopwords\nfrom tqdm.notebook import tqdm\n\nDEFAULT_REGEX = \"[-,.\\\\\/!@#$%^&*))_+=\\(|\\)|:|,|;|\\.|\u2019|\u201d|\u201c|\\?|%|>|<]+\"\n\nstopwords_en = set(stopwords.words('english'))\n\ndef tokenize(text, regex=DEFAULT_REGEX):\n    if text is None or (not isinstance(text,str) and np.isnan(text)):\n        return []\n    return [x for y in word_tokenize(str(text)) for x in re.split(regex, y)]\n\ndef analyze(tokens):\n    tokens = [unidecode.unidecode(t).lower() for t in tokens \n        if len(t) > 1 and t not in stopwords_en and (not t.isnumeric() or t.isalpha())]\n    return tokens\n   \n\nlemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer()\n\ndef lemmatize(word):\n    return  lemmatizer.lemmatize(str(word))\n\ndef stem(word):\n    return  lemmatizer.lemmatize(str(word))\n\ndef lemmatize_data(obj):\n    return apply2values(obj, lemmatize)\n\ndef stem_data(obj):\n    return apply2values(obj, stem)\n\ndef reduce_w3v_size(data, model_file, tokenization_regex = DEFAULT_REGEX, use_lemmatizer = True, use_stemmer = False):\n    words_model = None\n    data = set(flattern_values(data, lambda x: analyze(tokenize(x, tokenization_regex))))\n    \n    with open(model_file, 'rb') as fin:\n        words_model = KeyedVectors.load_word2vec_format(fin, binary=True)\n    def get_vec(word):\n        if word in words_model:\n            return words_model.word_vec(word) \n        if use_lemmatizer:\n            word = lemmatize(word) \n            if word in words_model:\n                return words_model.word_vec(word) \n        if use_stemmer:\n            word = stem(word)\n            if word in words_model:\n                return words_model.word_vec(word) \n        return words_model.word_vec(\"unk\")\n\n    vecs = {w: get_vec(w) for w in data}\n    m = keyedvectors.Word2VecKeyedVectors(vector_size=words_model.vector_size)\n    m.add(list(vecs.keys()), list(vecs.values()))\n    return m\n\ndef rank_items(scores, n_indexs, w = None, and_weight= 0.7):\n    items = []\n    for item, sc  in scores.items():\n        sc = [w[k] * v if w is not None and k in w else v for k,v in sc.items()]\n        score = min(sc) * (and_weight) + (1-and_weight) * (np.sum(sc) + (n_indexs -len(sc)) * max(sc))\/n_indexs\n        items.append((item, score))\n    items = sorted(items, key = lambda x: x[1])\n    return items\n        \n\n\nclass AnnoySearch:\n    def __init__(self, w2v_model, columns, use_lemmatizer = True, use_stemmer= False):\n        self.words_model = w2v_model\n        self.use_lemmatizer = use_lemmatizer\n        self.use_stemmer = use_stemmer\n        self.index = {c:AnnoyIndex(w2v_model.vector_size, 'dot') for c in columns}\n        \n    def get_vector(self, text):\n        def get_vec(word):\n            if word in self.words_model:\n                return self.words_model.word_vec(word) \n            if self.use_lemmatizer:\n                word = lemmatize(word) \n                if word in self.words_model:\n                    return self.words_model.word_vec(word) \n            if self.use_stemmer:\n                word = stem(word)\n                if word in self.words_model:\n                    return self.words_model.word_vec(word) \n            return self.words_model.word_vec(\"unk\")\n        \n        text = analyze(tokenize(text))\n        if len(text) == 0:\n            text = [\"unk\"]\n        vector = np.mean([get_vec(w) for w in text] ,axis=0)\n        return vector\n    \n    def build(self, df, n_trees=1000):\n        self.ids_index = df.index.to_list()\n        self.ids = dict(zip(self.ids_index, range(len(df))))\n        self.df = df\n        for i, row in tqdm(df.iterrows(), total = len(df)):\n            for c, idx in self.index.items():\n                idx.add_item(self.ids[i], self.get_vector(row[c]))\n        for c, idx in self.index.items():\n            idx.build(n_trees)\n        \n    def save(self):\n        write_object(\"annoy_search.gz\", self)\n        \n    def load(self):\n        newObj =  read_object(\"annoy_search.gz\")\n        self.__dict__.update(newObj.__dict__)\n    \n    def query(self, q, n_items, w = None, and_weight= 0.7, include_distances= False):\n        q_vec = self.get_vector(q)\n        score_dict = defaultdict(dict)\n        for c, idx in self.index.items():\n            ids =  idx.get_nns_by_vector(q_vec, n_items, include_distances= True)\n            for i, s in list(zip(list(ids[0]),list(ids[1]))):\n                score_dict[i].update({c:s})\n        sorted_res = rank_items(score_dict, len(self.index), w, and_weight)\n        \n        if not include_distances:\n            sorted_res = [k for k,v in sorted_res]\n        return sorted_res\n\n        \nclass HybridSearch:\n    def __init__(self, w2v_model, columns, use_lemmatizer = True, use_stemmer= True, use_lemmatizer_annoy = True, use_stemmer_annoy= False):\n        self.use_lemmatizer = use_lemmatizer\n        self.use_stemmer = use_stemmer\n        self.annoy_search = AnnoySearch(w2v_model, columns, use_lemmatizer_annoy, use_lemmatizer_annoy)\n        self.bm25 = {c:None for c in columns}\n\n    def process_text(self, text):\n        text = analyze(tokenize(text))\n        if self.use_lemmatizer:\n            text = lemmatize_data(text)\n        if self.use_stemmer:\n            text = stem_data(text)\n        return text\n\n    def build(self, df, n_trees=1000):\n        for c in tqdm(self.bm25.keys()):\n            self.bm25[c] = BM25(df[c].transform(self.process_text).to_list())\n        self.annoy_search.build(df, n_trees)\n    \n    def save(self):\n        write_object(\"hybrid_search.gz\", self)\n        \n    def load(self):\n        newObj =  read_object(\"hybrid_search.gz\")\n        self.__dict__.update(newObj.__dict__) \n    \n    def query(self, q, n_items, w = None, and_weight= 0.7, include_distances=False, additive_semantic_weight = 0.1):\n        score_dict = defaultdict(dict)\n        q_tokens = self.process_text(q)\n        for c, idx in self.bm25.items():\n            scores =  idx.get_scores(q_tokens)\n            for i, s in zip(range(len(scores)), scores):\n                score_dict[i].update({c: -s})\n        sorted_res = rank_items(score_dict, len(self.bm25), w, and_weight)\n        bm25_dict = dict(sorted_res)\n        annoy_res = self.annoy_search.query(q, n_items, w, and_weight, True)\n        for item,score in annoy_res:\n             bm25_dict[item] = score*additive_semantic_weight + bm25_dict[item]*score\n        sorted_res = sorted(bm25_dict.items(), key = lambda x: x[1])\n        results = self.annoy_search.df.iloc[[idx for idx,_ in sorted_res]][list(self.bm25.keys())]\n        if include_distances:\n            results[\"Score\"] = [score for _,score in sorted_res]\n        return results[:n_items]","289b851b":"data_path = \"\/kaggle\/input\/CORD-19-research-challenge\"\nall_sources = pd.read_csv(os.path.join(data_path,\"metadata.csv\"), dtype={'Microsoft Academic Paper ID': str,\n                                      'pubmed_id': str})\ncsv_data_path = \"\/kaggle\/input\/cord-19-eda-parse-json-and-generate-clean-csv\"\nall_text_data = []\nfor file in os.listdir(csv_data_path):\n    if os.path.splitext(file)[-1] == \".csv\":\n        print(f\"loading {file}\")\n        all_text_data.append(pd.read_csv(os.path.join(csv_data_path, file)))\nall_text_data = pd.concat(all_text_data)\nduplicate_paper = ~(all_sources.title.isnull() | all_sources.abstract.isnull()) & (all_sources.duplicated(subset=['title', 'abstract']))\nall_sources = all_sources[~duplicate_paper].reset_index(drop=True)\nall_data = pd.merge(all_sources[list(set(all_sources.columns).difference(set(all_text_data.columns)))], all_text_data, how='left', left_on=['sha'], right_on = ['paper_id'])\nprint(f\"all_sources shape {all_sources.shape}\")\nprint(f\"all_text_data shape {all_text_data.shape}\")\nprint(f\"all_data shape {all_data.shape}\")","36b32ef5":"all_data.head()","e4691f29":"search_columns = [\"title\", \"abstract\", \"text\", 'title', 'abstract', 'doi', 'authors', 'journal']","cf8d2540":"w2v_model = KeyedVectors.load(\"\/kaggle\/input\/covid19-bio-w2v\/small_w2v_model.wv\")","6c338d46":"#Hyperd model cant work on kaggle kernel because of memroy limit if you want to run it run it localy\n#search = HybridSearch(w2v_model,columns=[\"title\", \"abstract\", \"text\"])\n\nsearch = AnnoySearch(w2v_model,columns=search_columns)\n%timeit\nsearch.build(all_data)","ecca2848":"from ipywidgets import interact\nimport ipywidgets as widgets\nimport pandas as pd\n\ndef set_column_width(ColumnWidth, MaxRows):\n    pd.options.display.max_colwidth = ColumnWidth\n    pd.options.display.max_rows = MaxRows\n    print('Set pandas dataframe column width to', ColumnWidth, 'and max rows to', MaxRows)\n    \ninteract(set_column_width, \n         ColumnWidth=widgets.IntSlider(min=50, max=400, step=50, value=200),\n         MaxRows=widgets.IntSlider(min=50, max=500, step=100, value=100));","347d9812":"from IPython.display import display\n\ndef search_papers(SearchTerms: str):\n    search_results = search.query(SearchTerms, n_items=100, include_distances=True)\n    results = all_data.iloc[[x for x,_ in search_results]]\n    results[\"score\"] = [x for _,x in search_results]\n    if len(results) > 0:\n        display(results) \n    return results\n\nsearchbar = widgets.interactive(search_papers, SearchTerms='viruse')\nsearchbar","2870f507":"building the model","095643b1":"### Loading Data","7ebf5389":"### Functions from [https:\/\/github.com\/ABasharEter\/KaggleToolbox](https:\/\/github.com\/ABasharEter\/KaggleToolbox)","9e396206":"# Importing libraris","efcb02dd":"loading the ","d3fde186":"In order to use Annoy for semantic search we need to have a vector representation of the document. I will use [BioWordVec](https:\/\/github.com\/ncbi-nlp\/BioSentVec\/) which a biomedical word and sentence embeddings. We will use the word2vec model of it and will generate vector for each document as the mean of its words vectors. To do that we need to download the model and run it.\n\nThe problem is that the model is huge 13GB and will use around 32 GB of ram when it will be loaded. Thus, I have loaded it locally and fetched only the vectors of the words that we have in our data and stored them into another model file that I have provided here.\n\nFollowing is the code that I have used to reduce word vectors number:\n``` python\nsearch_columns = [\"title\", \"abstract\", \"text\", 'title', 'abstract', 'doi', 'authors', 'journal']\nall_text = [all_data[col].to_list() for col in search_columns]\nw2v_model = reduce_w3v_size(all_text\n                            ,'BioWordVec_PubMed_MIMICIII_d200.vec.bin')\nw2v_model.save(\"small_bio_w2v_model.wv\")\n```","3c78c06c":"# Hybrid Search Model - Annoy bio-w2v + BM25\n**Hay kagglers. This is my fisrt public kernal. Please upvote it if you like it**\n## Gole:\nIn this kernel I will create a sematic index for the reaserch papers which will help us in searching the data and clustring it to find meaningfull information. The way I have build the index by ranking papers with bm25 similarity as well as dot product of feature vectors that are the mean of the word to vec vectors of their words. The word 2 vec model is the [BioWordVec](https:\/\/github.com\/ncbi-nlp\/BioSentVec\/) which is trained on biological research papers thus it's more helpfull for us to use it. Moreover, I am indexing the vectors for each document into [Annoy Index](https:\/\/github.com\/spotify\/annoy) which will help us searching in the vector space model. You can use this kernel to search for similar document even if the exact word is not presendted in the document\n\nIn this code I will utiles some libraries that I have build and collected from multiple open source resources. You can fin at as a library on [https:\/\/github.com\/ABasharEter\/KaggleToolbox](https:\/\/github.com\/ABasharEter\/KaggleToolbox)\n\n### Note:\n This kernel is not yet finished. I am faceing memory limit issues on kaggle for the bm25 model but it is working locally. If you have any ideas regarding that please tell me.\n Thanks for: \n 1. [DwightGunning kernel](https:\/\/www.kaggle.com\/dgunning\/browsing-research-papers-with-a-bm25-search-engine) for his kernel for bm25 search model. I have added here the full atrical text if it is there also I am not removing duplicate words from the test to boost words by term frequncy. \n 2. [xhlulu kernel](https:\/\/www.kaggle.com\/xhlulu\/cord-19-eda-parse-json-and-generate-clean-csv) for data preprocesing and cleaning.\n"}}