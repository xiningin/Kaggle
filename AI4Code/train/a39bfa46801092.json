{"cell_type":{"5d5600bd":"code","64356f9c":"code","64d656ea":"code","2a766c72":"code","8162624f":"code","50c09ee8":"code","0700678f":"code","52f805cd":"code","14d6979d":"code","cfa66212":"code","f274089b":"code","75b1cc7a":"code","b3074fe1":"code","11c99fe6":"code","5ae705a3":"code","10cd06c0":"code","75094e79":"code","eb695079":"code","090337d0":"code","45665484":"code","86d956eb":"markdown","7845e657":"markdown","e32206ee":"markdown","36899cb6":"markdown","592ce6b2":"markdown","74c9222a":"markdown","987877f6":"markdown","8b6d06ed":"markdown","e53b6ddb":"markdown","8dedc127":"markdown","7b476967":"markdown","2a37e6ab":"markdown","3d688203":"markdown","0b75e338":"markdown","59badff4":"markdown","b93936ae":"markdown","90a867b7":"markdown","4cb440bb":"markdown","b74d5383":"markdown","8a0641f5":"markdown"},"source":{"5d5600bd":"# Import libraries \nimport librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport glob \nfrom sklearn.metrics import confusion_matrix\nimport IPython.display as ipd  # To play sound in the notebook\nimport os\nimport sys\nimport warnings\n# ignore warnings \nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","64356f9c":"#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\nTESS = \"\/kaggle\/input\/toronto-emotional-speech-set-tess\/tess toronto emotional speech set data\/TESS Toronto emotional speech set data\/\"\nRAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"\nSAVEE = \"\/kaggle\/input\/surrey-audiovisual-expressed-emotion-savee\/ALL\/\"\nCREMA = \"\/kaggle\/input\/cremad\/AudioWAV\/\"\n\n# Run one example \ndir_list = os.listdir(SAVEE)\ndir_list[0:5]","64d656ea":"# Get the data location for SAVEE\ndir_list = os.listdir(SAVEE)\n\n# parse the filename to get the emotions\n# remind: they are all male speakers only.\nemotion=[]\npath = []\nfor i in dir_list:\n    if i[-8:-6]=='_a':\n        emotion.append('male_angry')\n    elif i[-8:-6]=='_d':\n        emotion.append('male_disgust')\n    elif i[-8:-6]=='_f':\n        emotion.append('male_fear')\n    elif i[-8:-6]=='_h':\n        emotion.append('male_happy')\n    elif i[-8:-6]=='_n':\n        emotion.append('male_neutral')\n    elif i[-8:-6]=='sa':\n        emotion.append('male_sad')\n    elif i[-8:-6]=='su':\n        emotion.append('male_surprise')\n    else:\n        emotion.append('male_error') \n    path.append(SAVEE + i)","2a766c72":"# Now check out the label count distribution \nSAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\nSAVEE_df['source'] = 'SAVEE'\nSAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])],axis=1)\nNSAVEE=SAVEE_df.labels.value_counts()\nSAVEE_df.head()","8162624f":"NSAVEE","50c09ee8":"# fear emotion track\n# use the well known Librosa library for this task \nfname = SAVEE + 'DC_f11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","0700678f":"# a happy eotion track\nfname = SAVEE + 'DC_h11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","52f805cd":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')#split filename to catch emotion and gender code\n        emotion.append(int(part[2]))#change string to int for emoticon code\n        temp = int(part[6])#gender code to integer\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '\/' + f)\ngender[1375:1385],path[1375:1385],len(gender)","14d6979d":"RAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'neutral', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'],axis=1)\nRAV_df.labels.value_counts()\n#RAV_df.head()","cfa66212":"# Pick a fearful track\nfname = RAV + 'Actor_14\/03-01-06-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","f274089b":"# Pick a happy track\nfname = RAV + 'Actor_14\/03-01-03-02-02-02-14.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","75b1cc7a":"dir_list = os.listdir(TESS)\ndir_list.sort()\ndir_list, TESS","b3074fe1":"path = []\nemotion = []\n\nfor i in dir_list:\n    fname = os.listdir(TESS + i)\n    for f in fname:\n        if i == 'OAF_angry' or i == 'YAF_angry':\n            emotion.append('female_angry')\n        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n            emotion.append('female_disgust')\n        elif i == 'OAF_Fear' or i == 'YAF_fear':\n            emotion.append('female_fear')\n        elif i == 'OAF_happy' or i == 'YAF_happy':\n            emotion.append('female_happy')\n        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n            emotion.append('female_neutral')                                \n        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n            emotion.append('female_surprise')               \n        elif i == 'OAF_Sad' or i == 'YAF_sad':\n            emotion.append('female_sad')\n        else:\n            emotion.append('Unknown')\n        path.append(TESS + i + \"\/\" + f)\n\nTESS_df = pd.DataFrame(emotion, columns = ['labels'])\nTESS_df['source'] = 'TESS'\nTESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nTESS_df.labels.value_counts()","11c99fe6":"# lets play a fearful track \nfname = TESS + 'YAF_fear\/YAF_dog_fear.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","5ae705a3":"# lets play a happy track \nfname =  TESS + 'YAF_happy\/YAF_dog_happy.wav' \n\ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","10cd06c0":"dir_list = os.listdir(CREMA)\ndir_list.sort()\nprint(dir_list[0:10])","75094e79":"gender = []\nemotion = []\npath = []\nfemale = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n\nfor i in dir_list: \n    part = i.split('_')\n    if int(part[0]) in female:\n        temp = 'female'\n    else:\n        temp = 'male'\n    gender.append(temp)\n    if part[2] == 'SAD' and temp == 'male':\n        emotion.append('male_sad')\n    elif part[2] == 'ANG' and temp == 'male':\n        emotion.append('male_angry')\n    elif part[2] == 'DIS' and temp == 'male':\n        emotion.append('male_disgust')\n    elif part[2] == 'FEA' and temp == 'male':\n        emotion.append('male_fear')\n    elif part[2] == 'HAP' and temp == 'male':\n        emotion.append('male_happy')\n    elif part[2] == 'NEU' and temp == 'male':\n        emotion.append('male_neutral')\n    elif part[2] == 'SAD' and temp == 'female':\n        emotion.append('female_sad')\n    elif part[2] == 'ANG' and temp == 'female':\n        emotion.append('female_angry')\n    elif part[2] == 'DIS' and temp == 'female':\n        emotion.append('female_disgust')\n    elif part[2] == 'FEA' and temp == 'female':\n        emotion.append('female_fear')\n    elif part[2] == 'HAP' and temp == 'female':\n        emotion.append('female_happy')\n    elif part[2] == 'NEU' and temp == 'female':\n        emotion.append('female_neutral')\n    else:\n        emotion.append('Unknown')\n    path.append(CREMA + i)\n    \nCREMA_df = pd.DataFrame(emotion, columns = ['labels'])\nCREMA_df['source'] = 'CREMA'\nCREMA_df = pd.concat([CREMA_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nCREMA_df.labels.value_counts()","eb695079":"# use the well known Librosa library for this task \nfname = CREMA + '1012_IEO_HAP_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","090337d0":"# A fearful track\nfname = CREMA + '1012_IEO_FEA_HI.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Lets play the audio \nipd.Audio(fname)","45665484":"df = pd.concat([SAVEE_df, RAV_df, TESS_df, CREMA_df], axis = 0)\nprint(df.labels.value_counts())\ndf.head()\ndf.to_csv(\"Data_path.csv\",index=False)","86d956eb":"<a id=\"tess\"><\/a>\n##  <center> 3. TESS dataset <center>\nIt's only based on 2 speakers, **a young female and an older female** which hopefully balance out the male dominant speakers that we have on SAVEE. Its got the same 7 key emotions we're interested in. But what is slightly different about this dataset compared to the previous two above, is that the **addition of 'pleasant surprise' emotion**. But We going to work with the assumption for now that for the RADVESS and SAVEE dataset,its a pleasant surprise.","7845e657":"<a id=\"savee\"><\/a>\n##  <center> 1. SAVEE dataset <center>\nThe audio files (**all male speakers only**) are named in such a way that the prefix letters describes the emotion classes as follows:\n- 'a' = 'anger'\n- 'd' = 'disgust'\n- 'f' = 'fear'\n- 'h' = 'happiness'\n- 'n' = 'neutral'\n- 'sa' = 'sadness'\n- 'su' = 'surprise' ","e32206ee":"<a id=\"savee_con\"><\/a>\n### Conclusion\nThe wave form is distinctively different from the fear one which is good for our model. However, the sentence uttered is different so its not exactly an apple to apple comparison but it still gives us a good early indication of what we're dealing with.","36899cb6":"And I'm not so sure if it'd consider the emotion happy. Sounds to me more like neutral. But then again, could be due to the audio quality. Lets listen to another one, a fearful one.  ","592ce6b2":"<a id=\"ravdess_explore\"><\/a>\n### Explore the data","74c9222a":"<a id=\"crema\"><\/a>\n##  <center> 4. CREMA-D dataset <center>\nIts **a very large dataset** which we need. And it has **a good variety of different speakers**, apparently taken from movies. And the speakers are of different ethnicities. What we are **missing from this dataset is the \"surprise\" emotion** but no biggie, we can use the rest.","987877f6":"<a id=\"ravdess\"><\/a>\n## <center>2. RAVDESS dataset<\/center>\n\n**Female and male speakers**.The filename identifiers as per the official RAVDESS website:\n\n- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n- Vocal channel (01 = speech, 02 = song).\n- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n- Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nSo, here's an example of an audio filename. \n_02-01-06-01-02-01-12.mp4_\n\nThis means the meta data for the audio file is:\n- Video-only (02)\n- Speech (01)\n- Fearful (06)\n- Normal intensity (01)\n- Statement \"dogs\" (02)\n- 1st Repetition (01)\n- 12th Actor (12) - Female (as the actor ID number is even)","8b6d06ed":"<a id=\"final\"><\/a>\n##  <center> 5. Final thoughts<center>\nAll 4 dataset are good datasets and based on some really rough inspections, we can combine all of them. We hope to hold-out a high accuracy and well work on new unseen dataset as well since the classifier will be trained on the various dataset with the various circumstances that the dataset was obtained or produced, (eg. audio quality, speaker repetition, duration and sentence uttered). Furthermore, to enable it to do well on new datasets, it needs to be given noise, make it work hard to find the real distinguishing characteristics of the emotion.\n\nNext, final steps are to combine all the meta-data together as one. We saved the paths for all the audio files,so this will be handy when we need to read all 4 data sources in different folder structures.","e53b6ddb":"# <center>Audio Emotion classifier<\/center>\n## <center>Part 1 - Data Exploration<\/center>\nSo Part 1 here, we are going to check out a few data sources which are all open sourced:\n\n- Surrey Audio-Visual Expressed Emotion [(SAVEE)](https:\/\/www.kaggle.com\/ejlok1\/surrey-audiovisual-expressed-emotion-savee)\n- Ryerson Audio-Visual Database of Emotional Speech and Song [(RAVDESS)](https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio)\n- Toronto emotional speech set [(TESS)](https:\/\/www.kaggle.com\/ejlok1\/toronto-emotional-speech-set-tess)\n- Crowd-sourced Emotional Mutimodal Actors Dataset [(CREMA-D)](https:\/\/www.kaggle.com\/ejlok1\/cremad)","8dedc127":"<a id=\"savee_explore\"><\/a>\n### Explore the data\nThe below wave plot is a graphical representation of a sound wave vibration overtime. Its in this wave that we need to find the key pattern that will help us distinguish the different emotions. ","7b476967":"<a id=\"ravdess_con\"><\/a>\n### Conclusion\nLooking at the wave plot between the 2 files, I notice the only real difference is the amplitute wherein this happy track has a higher amplituted at various points. But, could be coincidence, who knows at this stage.The audio quality is good and we need females in the dataset. If we don't include females, we'll end up with an AI that is bias towards \/ aagainst one gender, and its unethical. Unless there's a good reason, I'm not taking it out.  ","2a37e6ab":"<a id=\"tess_explore\"><\/a>\n### Explore the data\n400 files for each key emotion. ","3d688203":"<a id=\"savee_load\"><\/a>\n###  Load the dataset ","0b75e338":"The 4 sources of the datasets are all on Kaggle, import them into the workspace with the directory path are below:","59badff4":"<a id=\"crema_con\"><\/a>\n### Conclusion\nIf we go back to listen a few more random tracks, we will noticed that this CREMA-D dataset is highly varied in its quality. Some are crisp clear and some are really muffled or echoey. Also there's lots of silence as well. All in all, slightly 'dirtier' version of the data.","b93936ae":"<a id=\"crema_load\"><\/a>\n###  Load the dataset \nThe speakers and the emotions like all previous datasets, are tagged in the audio filename itself. However, what we are missing is the Gender, which is kept as a seperate csv file that maps the actors. Instead of reading it and doing some matching, I'm just going to hardcode it here instead. Not the best practice but can do for now. ","90a867b7":"<a id=\"tess_con\"><\/a>\n### Conclusion\nNotice the amplitute is pretty high too on a few data points? We saw that on the RAVDESS dataset too. Perhaps that could be one of the few distinguishing factors? Because the speakers are the same, and the sentence uttered are the same, its an apples to apples comparison. ","4cb440bb":"<a id=\"crema_explore\"><\/a>\n### Explore the data","b74d5383":"<a id=\"tess_load\"><\/a>\n###  Load the dataset \nThe speakers and the emotions are organised in seperate folders which is very convenient","8a0641f5":"<a id=\"ravdess_load\"><\/a>\n###  Load the dataset \nBecause of the way the entire data was packaged for us, and the format of the audio filename, there's a few more parsing steps required for the RAVDESS dataset compared to SAVEE "}}