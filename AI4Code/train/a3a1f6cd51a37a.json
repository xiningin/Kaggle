{"cell_type":{"9acdb0be":"code","1ee2db4a":"code","dec7c441":"code","8547d209":"code","70c40f3e":"code","503316b4":"code","f5ed4cc5":"code","a7341029":"code","e82504bf":"code","09b2ed70":"code","a92f5a1c":"code","fb8c889d":"code","c1a296cf":"code","3664716c":"code","94d1780a":"code","4cb9912a":"code","a8545a23":"code","956a92a5":"code","2b7b40a7":"code","e52c87be":"code","2fee0043":"code","24d66002":"code","cf52fd25":"code","e71cd6d1":"code","04f5b656":"code","e1a4b25b":"code","4ccbf71a":"code","a55a2e93":"code","41d248fd":"code","1cd18d86":"code","af4f4207":"code","253cb7c5":"code","70795504":"code","8c52ef08":"code","0a0dc582":"code","9cf6dc8a":"code","681f7558":"code","08bbb604":"code","85151f8d":"code","0f52f52b":"code","6b6c0536":"code","a84e6c52":"code","c6158ea5":"code","2c16eea0":"code","f3e447f2":"code","98194eb2":"code","b78367f9":"code","20ff46bc":"code","26bbad42":"code","e4d78eec":"code","2176de77":"code","ad379d3a":"code","a6b85111":"code","1e0fcf21":"code","b7534f93":"code","3b922df3":"code","2a52ab7c":"code","f2798948":"code","ffe3cd6c":"code","6011bf8b":"code","cdfdf85d":"code","f8afee95":"code","4545dcdf":"code","9e58e0c7":"code","06fd16c4":"code","58b3cac1":"code","64777d82":"code","6b54f34a":"code","4bef7b08":"code","6b531d1d":"code","1d36d0f6":"code","1bb7e0ac":"code","81235d7c":"code","6bbfbea5":"code","526458e3":"code","62a66d67":"code","543bc113":"code","c4455034":"markdown","019aa968":"markdown","c0edf606":"markdown","ae33f4a5":"markdown","43ddb0ab":"markdown","04d5fb92":"markdown"},"source":{"9acdb0be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ee2db4a":"import pandas as pd\n# Train data \ndf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf.head()\n","dec7c441":"# Test Data\ndf1 = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf1.head()","8547d209":"df.columns","70c40f3e":"# importig the required Libraries for Data explorations #\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","503316b4":"df.shape\n","f5ed4cc5":"df1.shape\n# SalesPrice needs to be predicted which is a missing column in test dataset","a7341029":"df['LotFrontage'].isnull().sum()","e82504bf":"# Lets identify what uis the percentage of missing values in each of this columns #\n\n\nfeatures_missing = [i for i in df.columns if df[i].isnull().sum() >=1]\nfeatures_missing","09b2ed70":"features_missing1 = [i for i in df1.columns if df1[i].isnull().sum() >=1]\nfeatures_missing1","a92f5a1c":"for i in features_missing:\n    print(i,round(df[i].isnull().mean(),2),'% of missing values')","fb8c889d":"for i in features_missing1:\n    print(i,round(df1[i].isnull().mean(),2),'% of missing values')","c1a296cf":"import numpy as np\n\nfor i in features_missing:\n    data = df.copy()\n    # Replace the missing values by 1, else make it zero\n    data[i] = np.where(data[i].isnull(),1,0)\n    data.groupby(i)['SalePrice'].median().plot.bar()\n    plt.title(i)\n    plt.show()\n    \n    # As per the observation, we will replace thus missing values through feature engineering steps","3664716c":"# Identify the list of numerical variables #\n\nfeatures_numerical = [i for i in df.columns if df[i].dtype!='O']\n\n\nfeatures_numerical","94d1780a":"# Same way we can identify the datatime featurws, we can help us to identify the no. of days  \/ no. of years between\n# the car was built and when it was sold #\n\nyear_feature =[i for i in df.columns if 'Yr' in i or 'Year' in i]\nyear_feature","4cb9912a":"for i in year_feature:\n    print(df[i].head())","a8545a23":"# Letsa analyze the relation between the year sold and sales price (time series analysis)\n\ndf.groupby('YrSold')['SalePrice'].median().plot()\nplt.title('Yrsold Vs. Selling Price')\nplt.show()","956a92a5":"# Lets try to understand the relation between year sold with other 3 variables :\n# yearbuilt, 'YearRemodAdd', 'GarageYrBlt to understant teh relationship with the selling proce of house \n\nfor i in year_feature:\n    if i!='YrSold':\n        data=df.copy()\n    # this will give the no. of days #\n        print(\"for the feature\",i)\n        data[i] = data['YrSold'] - data[i]\n        plt.scatter(data[i],data['SalePrice'])\n        plt.xlabel(i)\n        plt.ylabel('SalePrice')\n        plt.show()\n        \n# From this graphs it can interpreted that as the no. of years passed by are less, the selling price\n# turns to be very high  and vice versa #","2b7b40a7":"df.info()","e52c87be":"len(df['ScreenPorch'].unique())","2fee0043":"#From the numerical variables, we cna have 2 types of variables : Discrete and continous\n\n# Discrete variables #\n\ndiscrete_features = [i for i in features_numerical if len(df[i].unique()) <25 and i not in year_feature]\n\nprint(df[discrete_features].head())","24d66002":"len(discrete_features)","cf52fd25":"# Lets plot relatoon between target variable Salesprice and discrete features #\n\nfor i in discrete_features:\n    data = df.copy()\n    data.groupby(i)['SalePrice'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('SalePrice')\n    plt.show()","e71cd6d1":"# Lets try ti understand the distributon if this continous variables #\n\ncontinous_features = [i for i in features_numerical if i not in discrete_features and i not in year_feature+['Id']]\nlen(continous_features)\n","04f5b656":"# Plotting the distribution if this variables #\n\nfor i in continous_features:\n    df[i].hist(bins=25)\n    plt.xlabel(i)\n    plt.ylabel('count')\n    plt.show()\n\n\n# only the few oif the variables have the gaussian distrivution, rest of them have skewed distribution # ","e1a4b25b":"# Before applying the logarthmic trasnformations, if we check the scatter plot #\n\n\nfor i in continous_features:\n    data = df.copy()\n    if 0 in df[i].unique():\n        pass\n    else:\n        plt.scatter(data['SalePrice'],data[i])\n        plt.xlabel(i)\n        plt.ylabel('SalePrice')\n        plt.title(i)\n        plt.show()\n        ","4ccbf71a":"# As some of yhis continous variables are having skewed distributions, we will apply log transformations\n#to this variables #\n\nfor i in continous_features:\n    data = df.copy()\n    if 0 in df[i].unique():\n        pass\n    else:\n        data[i] = np.log(data[i])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.scatter(data['SalePrice'],data[i])\n        plt.xlabel(i)\n        plt.ylabel('SalePrice')\n        plt.show()","a55a2e93":"# checking the outliers in the data #\n\nfor i in continous_features:\n    data = df.copy()\n    if 0 in data[i].unique():\n        pass\n    else:\n        data[i] =np.log(data[i])\n        data.boxplot(column=i)\n        plt.ylabel(i)\n        plt.show()","41d248fd":"# Categorical variables #\n\ncategorical_features = [i for i in df.columns if df[i].dtypes=='O']\n\nlen(categorical_features)\n\n# Cardinality : How many categories are there in each of the variables #\n","1cd18d86":"# Cardinality of the variables #\n\nfor i in categorical_features:\n    print(i, 'has',df[i].nunique(),'unique categories')\n    \n# Some of the varuables has a large no. of cardinalities (largw no. of categories, which will create \n#a lot of data complexities while doung the one hot encoding)\n# Such variables needs to be handled separately instead of using one hot encoding #","af4f4207":"for i in categorical_features:\n    data = df.copy()\n    data.groupby(i)['SalePrice'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('SalePrice')\n    plt.show()","253cb7c5":"# We will droop the ID column and the variables : Alley, fence, poolQc, fence and Miscfeature where the missjng values are greater than 50%\n# in both the train and test data\n# Train data\ndf_n = df.drop(['Id','Alley','PoolQC','Fence','MiscFeature'],axis=1)\ndf_n.shape","70795504":"# test data\ndf1_n = df1.drop(['Id','Alley','PoolQC','Fence','MiscFeature'],axis=1)\ndf1_n.shape","8c52ef08":"# Train Data # Imputing the categorical variables with mode values \n\nfor i in df_n.columns:\n    if df_n[i].dtype=='O':\n        df_n[i]= df_n[i].fillna(df_n[i].mode()[0])","0a0dc582":"# test Data # Imouting the categorical variables with mode values \n\nfor i in df1_n.columns:\n    if df1_n[i].dtype=='O':\n        df1_n[i]= df1_n[i].fillna(df1_n[i].mode()[0])","9cf6dc8a":"# Train data, for the numerical variables where we will replace missing \n# values with median instead of mean as from EDA, we came to know that there are outliers in the data #\n\nfor i in df_n.columns:\n    if df_n[i].dtype=='float64' or df_n[i].dtype=='int64':\n        df_n[i]= df_n[i].fillna(df_n[i].median())","681f7558":"# test data, for the numerical variables where we will replace missing \n# values with median instead of mean as from EDA, we came to know that there are outliers in the data #\n\nfor i in df1_n.columns:\n    if df1_n[i].dtype=='float64' or df1_n[i].dtype=='int64':\n        df1_n[i]= df1_n[i].fillna(df1_n[i].median())","08bbb604":"print(df1_n.isnull().sum().sum())\nprint(df_n.isnull().sum().sum())","85151f8d":"print(df_n.duplicated().sum())\n\nprint(df1_n.duplicated().sum())","0f52f52b":"# Temporal variables #\n# Train data #\nfor i in year_feature:\n    if i!='YrSold':\n    # this will give the no. of days #\n        print(\"for the feature\",i)\n        df_n[i] = df_n['YrSold'] - df_n[i]\ndf_n.head()","6b6c0536":"df_n[year_feature].head()","a84e6c52":"# Test data #\nfor i in year_feature:\n    if i!='YrSold':\n    # this will give the no. of days #\n        df1_n[i] = df1_n['YrSold'] - df1_n[i]\ndf1_n.head()","c6158ea5":"df1_n[year_feature].head()","2c16eea0":"# Feature Transformation : Converting all the categorical variables into numerical type #\n\n#From checking categories of categorical variables for train and test data it can be understood \n# that are some additional categories mentioned for test data categorical variables whicgh are not mentioned \n# in the train data \n\n# So we if do dummy encoding for both train and test data individually, then model will not ne able to understand\n# some of the categories if the test data, in such cases its better to combine this data and perform dummt encoding#","f3e447f2":"y_train = df_n['SalePrice']\nx_train = df_n.drop(['SalePrice'],axis=1)\nx_train.shape","98194eb2":"y_train.head()","b78367f9":"# Lets combine rhe data from train and test #\n\ndf_new = pd.concat([x_train,df1_n],axis=0)\ndf_new.tail()","20ff46bc":"df_new.shape","26bbad42":"df_cat = pd.get_dummies(df_new,drop_first=True)\ndf_cat.head()","e4d78eec":"# Splitting the total daata set back into train and test back\n\n\nx_train = df_cat.iloc[:1460,:]\n\nx_train.shape","2176de77":"x_test =  df_cat.iloc[1460:,:]\n\nx_test.shape","ad379d3a":"print(x_train.columns)\nprint(x_test.columns)\n","a6b85111":"from sklearn.preprocessing import MinMaxScaler\nmn = MinMaxScaler()\n\nnorm = mn.fit(x_train)\n\nx = pd.DataFrame(norm.transform(x_train),columns=x_train.columns)\nx.head()\n","1e0fcf21":"x_test = pd.DataFrame(norm.transform(x_test),columns = x_test.columns)\nx_test.head()\n\n# test dataset #","b7534f93":"# Importing the keras libraries #\nimport keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import ReLU\nfrom keras.optimizers import SGD\n","3b922df3":"from tensorflow.keras.optimizers import Adam","2a52ab7c":"# Initializing and add the multiple layers in the ANN #\n\nclassifier = Sequential()\nclassifier.add(Dense(units=384,activation='relu',input_dim=236,kernel_initializer='he_uniform'))\nclassifier.add(Dense(units=256,activation='relu',kernel_initializer='he_uniform'))\nclassifier.add(Dense(units=32,activation='relu',kernel_initializer='he_uniform'))\nclassifier.add(Dense(units=32,activation='relu',kernel_initializer='he_uniform'))\nclassifier.add(Dense(units=32,activation='relu',kernel_initializer='he_uniform'))\nclassifier.add(Dense(units=1,activation='linear'))","f2798948":"# Compiling the ANN Model #\n\nclassifier.compile(loss='mean_absolute_error',optimizer=Adam(learning_rate=0.01))","ffe3cd6c":"ann_mo = classifier.fit(x, y_train, validation_split=0.20, epochs=1, batch_size=20,verbose=0)","6011bf8b":"y_pred = classifier.predict(x_test)","cdfdf85d":"# Converting this array into data frame #\n\ny_pr = pd.DataFrame(y_pred)\ny_pr.head()","f8afee95":"# Creating a sample submission file #\n\nsub_7 = pd.concat([df1['Id'],y_pr],axis=1)\nsub_7.head()","4545dcdf":"sub_7.columns = ['Id','SalePrice']\nsub_7.head()","9e58e0c7":"sub_7.to_csv('sub_7.csv',index=False)\n\n#Score of 0.155","06fd16c4":"# For Keras tuner, we require the version of tensorflow to be greater than 2.0\nimport tensorflow as tf\nprint(tf.__version__)","58b3cac1":"pip install keras-tuner","64777d82":"from kerastuner.tuners import RandomSearch\nfrom tensorflow.keras import layers\nfrom tensorflow import keras","6b54f34a":"def build_model(hp):\n    model = keras.Sequential()\n    for i in range(hp.Int('num_layers', 2, 10)):\n        model.add(layers.Dense(units=hp.Int('units_' + str(i),\n                                            min_value=32,\n                                            max_value=512,\n                                            step=32),\n                               activation='relu'))\n    model.add(layers.Dense(1, activation='linear'))\n    model.compile(\n        optimizer=keras.optimizers.Adam(\n            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),\n        loss='mean_absolute_error',\n        metrics=['mean_absolute_error'])\n    return model","4bef7b08":"tuner = RandomSearch(\n    build_model,\n    objective='val_mean_absolute_error',\n    max_trials=10,\n    executions_per_trial=3,\n    directory='my_dir',\n    project_name='House_Price_Prediction')","6b531d1d":"tuner.search_space_summary()","1d36d0f6":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test = train_test_split(x,y_train,test_size=0.20)","1bb7e0ac":"tuner.search(X_train,Y_train,batch_size=20,epochs=600,validation_data=(X_test,Y_test))","81235d7c":"# Predicting the test data set #\n\ny_pred = model.predict(x_test)\ny_pred","6bbfbea5":"# Converting this array into data frame #\n\ny_pr = pd.DataFrame(y_pred)\ny_pr.head()","526458e3":"# Creating a sample submission file #\n\nsub_3 = pd.concat([df1['Id'],y_pr],axis=1)\nsub_3.head()","62a66d67":"sub_3.columns = ['Id','SalePrice']\nsub_3.head()","543bc113":"sub_3.to_csv('sub_3.csv',index=False)\n# Score of 0.1322, rank of #1657","c4455034":"# Steps for building solution for this problem statement\n\na) Problem Statement : To predict the price if the house through regression techniques\nb) Exploratory Data Analysis : Understanding the hidden relationship within the data\nc) Data Cleaning : Missing values, Outliers, duplicates\n\ni) Handling Missing Values : Delete the fields \/ columns which have greater than 50% of missing values and for rest of the columns impute them with mean, median and mode\nii) Check for duplicate values : Remove the duplicate enteries in the data\niii) Handling Outliers : If we are applying tree based algorithms we need not handle outliers separately\nd) Feature Engineeering : Transformation, scaling, extraction, etc. \nin order to make the data ready for the model building\ni) Temporal Values (We convert this year variables into difference between yr. sold and ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'] to understand the duration when the house had been restructured\nii. Transforming the Categorical variables the. one hot encoding \/ dummy encoding\niii. Standardize the variables values (Min max, standard scaler) - Feature Scaling\nAs we are dealing with ANN which consists of gradient descent optimization techniques, need to apply feature scaling  in order to converge the model faster to global minima\ne) Model Building - ANN\nf) Hyper parameter tuning \ng) Model Validation","019aa968":"# Data Cleaning # ","c0edf606":"# Model Building - Artificial Neural Network#","ae33f4a5":"# Hyper parameter tuning #","43ddb0ab":"# Feature Engineering #","04d5fb92":"# Exploratory Data Analysis "}}