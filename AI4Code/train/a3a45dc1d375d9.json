{"cell_type":{"f2e01e5a":"code","9705e86a":"code","0a8d47c7":"code","86c18382":"code","3ec0d77b":"code","b6db1871":"code","6e7a115b":"code","ca0a5d36":"code","1e9af0f9":"code","e3371264":"code","872e2f89":"code","beb89ad6":"code","f01a01b3":"code","56daf579":"code","e5433a7b":"code","cf899f76":"code","555b8478":"code","44a0e565":"code","0ef2b6e3":"code","8e7222f6":"code","31267264":"code","b7166606":"code","797fec3b":"code","673e02b1":"code","5d768270":"code","e30fef6f":"code","e36db077":"code","012233c7":"code","11976028":"code","d8648790":"code","93cb32d5":"code","b2c94441":"code","30eb10c3":"code","719406a5":"code","80eb9466":"code","7beee397":"code","efd6a779":"code","81faae39":"code","9efa7f09":"code","55c43ae0":"code","31b3fcdc":"code","0d37dd1d":"code","8b24dcae":"code","cb04490c":"code","a111117d":"code","a177d430":"code","aeb8f696":"code","16c8a2d6":"code","ad4e5527":"code","7181568d":"code","c76b4c44":"code","92ade8a9":"code","2fe5db84":"code","788084da":"code","e578b402":"code","bfba04bb":"code","221d7471":"code","a6c34edb":"code","59adc0e7":"code","fdc306f1":"markdown","64f36716":"markdown","776fd8d6":"markdown","e100e5f7":"markdown","845b4232":"markdown","f58662bd":"markdown","68ce6d03":"markdown","f5818477":"markdown","552406a9":"markdown","9ada4c64":"markdown","044b9f3c":"markdown","93b601e4":"markdown","f2ce6740":"markdown","d2da8168":"markdown","29e2c4d7":"markdown","79f28c34":"markdown","3f336f61":"markdown","5ea7d482":"markdown","9eb66753":"markdown","1ebf19fa":"markdown","f7540327":"markdown","af82fb0c":"markdown","ac5f0d54":"markdown","87bcc380":"markdown","0b5b117c":"markdown","0c33e70d":"markdown","6c878e2b":"markdown","359f9400":"markdown","82699be7":"markdown","813cc52b":"markdown","76d46996":"markdown","1f00637c":"markdown","0f58b314":"markdown","c0d4e833":"markdown","e9d38a4f":"markdown","fd68e97c":"markdown","0657cb26":"markdown","e24e1956":"markdown","4ebcbba9":"markdown","32e0dca3":"markdown","66a27520":"markdown","fc37208e":"markdown","b4916515":"markdown","9b7381be":"markdown"},"source":{"f2e01e5a":"# For data analysis, model building, evaluating\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,KFold\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score, f1_score, roc_curve, auc,precision_recall_curve\nfrom sklearn import preprocessing\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 200) # before I forget\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#For loading data\nimport os\n\n# For plots\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams","9705e86a":"print(os.listdir(\"..\/input\"))\n\ninput_path = '..\/input'\n\n%matplotlib inline\n\nRANDOM_SEED = 42\nnan_replace = -999","0a8d47c7":"print(os.listdir('..\/input\/previous-submissions'))","86c18382":"baseline_random_forest = pd.read_csv('..\/input\/previous-submissions\/baseline_random_forest.csv', index_col='TransactionID') # .878\nbaseline_xgboost = pd.read_csv('..\/input\/previous-submissions\/baseline_xgboost.csv', index_col='TransactionID') # .938\nxgboost_with_tuning = pd.read_csv('..\/input\/previous-submissions\/xgboost_with_tuning.csv', index_col='TransactionID') # .928\npreprocessed2_xgboost = pd.read_csv('..\/input\/previous-submissions\/preprocessed2_xgboost.csv', index_col='TransactionID') # .934","3ec0d77b":"assert(baseline_random_forest.shape == baseline_xgboost.shape == xgboost_with_tuning.shape == preprocessed2_xgboost.shape)","b6db1871":"ensemble = .025*baseline_random_forest + .05*xgboost_with_tuning + .075*preprocessed2_xgboost + .85*baseline_xgboost\nensemble.head()","6e7a115b":"ensemble.to_csv('ensemble6.csv')","ca0a5d36":"# %%time\n# train_identity =  pd.read_csv(os.path.join(input_path, 'train_identity.csv'), index_col='TransactionID')\n# train_transaction = pd.read_csv(os.path.join(input_path, 'train_transaction.csv'), index_col='TransactionID')\n# test_identity = pd.read_csv(os.path.join(input_path, 'test_identity.csv'), index_col='TransactionID')\n# test_transaction = pd.read_csv(os.path.join(input_path, 'test_transaction.csv'), index_col='TransactionID')","1e9af0f9":"# print('train_identity shape:', train_identity.shape)\n# print('train_transaction shape:', train_transaction.shape)\n# print('test_identity shape:', test_identity.shape)\n# print('test_transaction shape:', test_transaction.shape)","e3371264":"# train_transaction.head()","872e2f89":"# train_identity.head()","beb89ad6":"# train = pd.merge(train_transaction, train_identity, how='left', on='TransactionID')\n# test = pd.merge(test_transaction, test_identity, how='left', on='TransactionID')\n\n# # see if transaction and identity variables one train table (should be same for test)\n# train.head()","f01a01b3":"# # clear up RAM\n# del train_transaction, train_identity, test_transaction, test_identity","56daf579":"# print('train shape:', train.shape)\n# print('test shape:', test.shape)","e5433a7b":"# num_train = train.shape[0]\n# num_test = test.shape[0]\n# num_features = test.shape[1]\n\n# print('Test data is {:.2%}'.format(num_test\/(num_train+num_test)), 'of total train\/test data')","cf899f76":"# import xgboost as xgb\n# print('XGBoost version:', xgb.__version__)","555b8478":"# y_train = train['isFraud']\n# X_train = train.drop('isFraud', axis=1)\n# X_test = test\n\n\n# X_train = X_train.fillna(nan_replace)\n# X_test = X_test.fillna(nan_replace)\n\n# del train, test\n\n# # Label Encoding\n# for f in X_train.columns:\n#     if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n#         lbl = preprocessing.LabelEncoder()\n#         lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n#         X_train[f] = lbl.transform(list(X_train[f].values))\n#         X_test[f] = lbl.transform(list(X_test[f].values))","44a0e565":"# clf = xgb.XGBClassifier(n_estimators=500,\n#     max_depth=9,\n#     learning_rate=0.05,\n#     subsample=0.9,\n#     colsample_bytree=0.9,\n#     missing=nan_replace,\n#     random_state=RANDOM_SEED,\n#     tree_method='gpu_hist')","0ef2b6e3":"# %%time\n# clf.fit(X_train, y_train)","8e7222f6":"# sample_submission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'), index_col='TransactionID')","31267264":"# del X_train, y_train","b7166606":"# y_pred = clf.predict_proba(X_test)\n\n# sample_submission['isFraud'] = y_pred[:,1]\n# sample_submission.to_csv('baseline_xgboost.csv')","797fec3b":"# for feature in train.columns[:20]:\n#     print(feature, '\\t', train[feature].dtype)","673e02b1":"# cat_features = ['ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'DeviceInfo', 'isFraud']\n\n# # add card1-card6\n# for i in range(1, 7):\n#     cat_features.append('card'+str(i))\n    \n    \n# # add M1-M9\n# for i in range(1, 10):\n#     cat_features.append('M'+str(i))\n    \n    \n# # add id12-38\n# for i in range(12, 39):\n#     cat_features.append('id_'+str(i))","5d768270":"# # Convert categorical features to data type 'object'\n# def convert_to_object(df, cat_features):\n#     \"\"\"\n#     Converts features to data type 'object', so that all categorical features in dataframe are of type 'object'\n    \n#     Args:\n#         df (pd.Dataframe)\n#         cat_features (list): the categorical features as strings\n        \n#     Returns:\n#         df (pd.Dataframe): where new df has categorical features as type 'object'\n#     \"\"\"\n#     for feature in cat_features:\n#         if feature not in df.columns:\n#             print('ERROR:', feature)\n#         else:\n#             df[feature] = df[feature].astype('object')\n                        \n#     return df","e30fef6f":"# train = convert_to_object(train, cat_features)\n# test = convert_to_object(test, cat_features)\n\n# #Verify\n# for feature in train.columns[:20]:\n#     print(feature, '\\t', train[feature].dtype)\n    \n# print('\\n')\n# print('-'*50)\n# print('\\nTest: \\n')\n\n# for feature in test.columns[:20]:\n#     print(feature, '\\t', test[feature].dtype)","e36db077":"# train.isnull().sum()[:15]","012233c7":"# test.isnull().sum()[:15]","11976028":"# for feature in train.columns[:20]:\n#     if train[feature].dtype == 'object':\n#         print(feature, '\\t Unique categories:', train[feature].describe()[1])\n#         print('-'*40)","d8648790":"# y_train = train.isFraud\n# train = train.drop(['isFraud'], axis=1)\n# num_fraud = y_train.sum()\n\n# print('# of fraudulent transactions:', num_fraud, '\\n# of training examples:', num_train)","93cb32d5":"# plt.bar([1, 2], height=[num_fraud, num_train])\n# plt.title('Class Imbalance')\n# plt.show()","b2c94441":"# train_fraud = train[y_train == 1]\n# train_not_fraud = train[y_train == 0]\n\n# train_fraud.head(10)","30eb10c3":"# def get_mean_of_feature(df, feature):\n#     \"\"\"\n#     Calculates and returns mean value of a numerical feature variable\n    \n#     Args:\n#         df (pd.DataFrame): the dataframe\n#         feature (str): the name of the numerical feature\/variable as a string\n        \n#     Returns:\n#         mean (float)\n#     \"\"\"\n#     return df[feature].mean()\n\n# def get_categorical_distribution_of_feature(df, feature):\n#     \"\"\"\n#     Calculates and returns distribution of a categorical feature variable\n    \n#     Args:\n#         df (pd.DataFrame): the dataframe\n#         feature (str): the name of the categorical feature\/variable as a string\n        \n#     Returns:\n#         categorical dist (pd.Series)\n#     \"\"\"\n#     return df[feature].value_counts() \/ df[feature].value_counts().sum()","719406a5":"# def compare_dataframes(df1, df1_description, df2, df2_description):\n#     \"\"\"\n#     Analyze each feature and compare the difference between fraud and not fraud table\n    \n#     Args:\n#         train_fraud (pd.DataFrame): contains the fraudulent transactions\n#         train_not_fraud (pd.DataFrame): contains the non-fraud transactions\n        \n#     Returns:\n        \n#     \"\"\"\n    \n#     # features that look interesting from visual inspection\n#     features = ['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card4', 'card6', \n#                 'P_emaildomain', 'R_emaildomain', 'id_29', 'id_30', 'id_31', 'DeviceType', 'DeviceInfo']\n    \n#     # Use this if analyzing ALL features of dataframes\n#     # make sure have same features in both dataframes\n#     #assert(sorted(train_not_fraud.columns) == sorted(train_fraud.columns))\n#     #features = train_fraud.columns \n    \n#     for feature in features:\n#         # numerical feature\n#         if df1[feature].dtype == 'int64' or df1[feature].dtype == 'float64':\n#             print('\\nNumerical feature (' + str(df1_description), ')\\tFeature name:', feature, '\\nmean:', get_mean_of_feature(df1, feature))\n#             print('\\nNumerical feature (' + str(df2_description), ')\\tFeature name:', feature, '\\nmean:', get_mean_of_feature(df2, feature))\n#         # categorical feature\n#         elif df1[feature].dtype == 'object': # object, a string\n#             print('\\nCategorical feature(' + str(df1_description), ')\\tFeature name:', feature, '\\nDistribution:\\n', get_categorical_distribution_of_feature(df1,feature)[:10])\n#             print('\\nCategorical feature(' + str(df2_description), ')\\tFeature name:', feature, '\\nDistribution:\\n', get_categorical_distribution_of_feature(df2,feature)[:10])","80eb9466":"# compare_dataframes(train_fraud, 'Train Fraud', train_not_fraud, 'Train Not Fraud')","7beee397":"# # Clear up RAM (10.3GB -> 8.6GB)\n# del train_fraud, train_not_fraud","efd6a779":"# compare_dataframes(train, 'Train set', test, 'Test set')","81faae39":"# plt.hist(train['TransactionDT'], label='train')\n# plt.hist(test['TransactionDT'], label='test')\n# plt.legend()\n# plt.title('Distribution of TransactionDT dates')","9efa7f09":"# # could correct for time difference in later iteration, for now, just drop column\n# train.drop(['TransactionDT'], axis=1)\n# test.drop(['TransactionDT'], axis=1)\n\n# print('dropped TransactionDT')","55c43ae0":"# drop_cols = [c for c in train.columns if (train[c].isnull().sum() \/  num_train) > .80]\n\n# # also dropping V107 (though float values and VESTA did not say it was categorical, it really looks categorical in {0,1})\n# # it caused problems in making predictions, after further analysis, it seemed to have weak correlation with target variable\n# drop_cols.append('V107')\n\n# print('Dropping', len(drop_cols), 'columns.')\n# print('Including...', drop_cols[0:10])","31b3fcdc":"# train = train.drop(drop_cols, axis=1)\n# test = test.drop(drop_cols, axis=1)","0d37dd1d":"# def replace_nans(df):\n#     \"\"\"\n#     Replaces missing values (NaNs) with the mean (if numerical) and with most\n#     common category (if categorical)\n    \n#     Args:\n#         df (pd.DataFrame)\n        \n#     Returns:\n#         df (pd.DataFrame): transformed dataframe\n#     \"\"\"\n#     # NOTE: fillna did not work well here, recommend using replace\n    \n#     for feature in df.columns:\n#         # replace categorical variable with most frequent\n#         if df[feature].dtype == 'object':\n#             df[feature] = df[feature].replace(np.nan, df[feature].value_counts().index[0])\n        \n#         # replace NaN in numerical columns with mean (could try median)\n#         else:\n#             df[feature] = df[feature].replace(np.nan, df[feature].mean()) # most common category\n            \n#     return df","8b24dcae":"# # train = replace_nans(train)\n# # test = replace_nans(test)\n\n# # fill in -999 where vars have NaNs\n# train = train.replace(np.nan, nan_replace)\n# test = test.replace(np.nan, nan_replace)","cb04490c":"# train.head() # nice and pretty","a111117d":"# train.isnull().sum().sum() # 0","a177d430":"# test.isnull().sum().sum() # 0","aeb8f696":"# %%time\n# from sklearn.preprocessing import LabelEncoder\n\n# # When trying to encode variabels, receive the following ValueError: y contains previously unseen labels: [nan, nan, nan,... , nan]\n# for feature in train.columns:\n#     if train[feature].dtype == 'object' or test[feature].dtype == 'object':\n#         le = LabelEncoder()\n#         le.fit(list(train[feature].values) + list(test[feature].values))\n#         train[feature] = le.transform(list(train[feature].values))\n#         test[feature] = le.transform(list(test[feature].values))","16c8a2d6":"# # Don't see any strings here, looks like the encoding worked\n# train.head()","ad4e5527":"# def normalize(df):\n#     \"\"\"\n#     Normalize numerical variables\n    \n#     Args:\n#         df (pd.DataFrame): dataframe to be normalized\n        \n#     Returns:\n#         df (pd.Dataframe): dataframe where each column has mean 0\n#     \"\"\"\n#     for feature in df.columns:\n#         if df[feature].dtype != 'object': # if it is numerical\n#             mu = df[feature].mean()\n#             sd = df[feature].std()\n#             df[feature] = (df[feature] - mu) \/ sd\n            \n#             # verify mean is 0\n#             mu_after = df[feature].mean()\n#             #print(feature, mu_after) # checks out\n            \n#     return df\n            ","7181568d":"# train = normalize(train)\n# test = normalize(test)","c76b4c44":"# print(train.memory_usage().sum() \/ 1024**3, 'GB')\n# print(test.memory_usage().sum() \/ 1024**3, 'GB')","92ade8a9":"# def reduce_mem_usage(df, verbose=True):\n#     numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n#     start_mem = df.memory_usage().sum() \/ 1024**2    \n#     for col in df.columns:\n#         col_type = df[col].dtypes\n#         if col_type in numerics:\n#             c_min = df[col].min()\n#             c_max = df[col].max()\n#             if str(col_type)[:3] == 'int':\n#                 if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n#                     df[col] = df[col].astype(np.int8)\n#                 elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n#                     df[col] = df[col].astype(np.int16)\n#                 elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n#                     df[col] = df[col].astype(np.int32)\n#                 elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n#                     df[col] = df[col].astype(np.int64)  \n#             else:\n#                 if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n#                     df[col] = df[col].astype(np.float16)\n#                 elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n#                     df[col] = df[col].astype(np.float32)\n#                 else:\n#                     df[col] = df[col].astype(np.float64)    \n#     end_mem = df.memory_usage().sum() \/ 1024**2\n#     if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n#     return df","2fe5db84":"# train = reduce_mem_usage(train)\n# test = reduce_mem_usage(test)","788084da":"# import xgboost as xgb\n# from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV, RandomizedSearchCV","e578b402":"# y = y_train.astype('category')","bfba04bb":"# # grid of parameters, use GridSearch to find best combination\n# n_estimators = [400, 550, 700]\n# gamma = [.5, 1, 3]\n# max_depth = [6, 8, 10]","221d7471":"# import time\n\n# start = time.time()\n\n# # try all combinations of parameters\n# for n_est in n_estimators:\n#     for md in max_depth: \n#         for g in gamma:               \n#                 # train\/test split, hopefully with a large dataset this is sufficient to estimate roc auc\n#                 X_train, X_test, y_train, y_valid = train_test_split(train, y, test_size=.3, random_state=RANDOM_SEED, shuffle=True)\n                \n#                 # fit\n#                 clf = xgb.XGBClassifier(n_estimators=n_est,\n#                                         gamma=g,\n#                                         max_depth=md,\n#                                         missing=nan_replace,\n#                                         subsample=.8,\n#                                         colsample_bytree=.8,\n#                                         scale_pos_weight=20, # to correct for class imbalance\n#                                         random_state=RANDOM_SEED,\n#                                         tree_method='gpu_hist')\n                \n#                 # fit with these parameters\n#                 clf.fit(X_train, y_train)\n#                 del X_train, y_train\n                \n#                 # predict on test\/ estimate roc_auc, pick model with\n#                 y_pred = clf.predict_proba(X_test)\n#                 del X_test, clf\n                \n#                 print(roc_auc_score(y_valid, y_pred[:,1]), 'with parameters n_estimators={}, max_depth={}, gamma={},'.format(n_est, md, g))\n#                 del y_valid, y_pred\n                \n#                 now = time.time()\n#                 print('ELAPSED TIME:', now-start, 'seconds')\n                \n#                 # print(train.memory_usage().sum() \/ 1024**3, 'GB')\n#                 # print(y.memory_usage() \/ 1024**3, 'GB\\n')\n                \n#                 # train = reduce_mem_usage(train)\n                \n#                 # give RAM time to clear\n#                 time.sleep(10)\n                ","a6c34edb":"# %%time\n\n# # define xgboost classifier\n# clf = xgb.XGBClassifier(n_estimators=400,\n#                             gamma=1,\n#                             max_depth=6,\n#                             missing=nan_replace,\n#                             subsample=.8,\n#                             colsample_bytree=.8,\n#                             scale_pos_weight=20, # to correct for class imbalance\n#                             random_state=RANDOM_SEED,\n#                             tree_method='gpu_hist')\n    \n# # fit classifier\n# clf.fit(train, y_train)\n# del train, y_train","59adc0e7":"# sample_submission = pd.read_csv(os.path.join(input_path, 'sample_submission.csv'), index_col='TransactionID')\n\n# y_pred = clf.predict_proba(test)\n# del clf, test\n\n# sample_submission['isFraud'] = y_pred[:,1]\n# del y_pred\n# sample_submission.to_csv('xgboost_with_tuning2.csv')","fdc306f1":"## Analyze Categorical Variables","64f36716":"### Any remaining missing values?","776fd8d6":"### Thoughts\n* There are several categorical variables with many categories, suggesting that 1-Hot encoding might be too high dimensional\n* It may be more prudent to do label encoding (1,2,3,..) to limit dimensionality\n* Drawback with Label Encoding: softly implies there is some order to the categories since the categories are now numbered","e100e5f7":"## Takeaways from EDA\n### There are lots of missing values\n### There is significant class imbalance (Only ~20,000 out of 590,000 are fraudulent, or 3.5 %)\n* Thus, a classifier that always predicts not fraud (0) would have 96.5% accuracy (on the training set, the test set is similar)\n\n\n### TRAIN SET: Comparing means of numerical features among fraud and non-fraud transactions:\n* `TransactionDT` - fraudulent transactions 4.5% higher\n* `TransactionAmt` - fraudulent transactions 11% more expensive\n\n### TRAIN SET: Comparing distributions of categorical variables among fraud and non-fraud transactions:\n* Take a look at the above cell to see the comparison\n* Some of these may spurious, but with 20,000 fraudulent examples, they could imply something\n* `ProductCD` - 39% of fraud transactions are 'C', but only 11% of non-fraud transactions are 'C'\n* `card1` - looks similar\n* `card4` - distribution looks similar\n* `card6` - fraud transactions distributed evenly (52\/48) between debit and credit whereas non-fraud transactions are mostly debit (76%)\n* `P_emaildomain` - 13% of fraud comes from hotmail email vs. 9% non-fraud is hotmail email \n* `R_emaildomain` - 60% of emails on receiving end of fraud are gmail vs. only 40% for non-fraud\n* `id_29` - 70% are 'Found' in the fraud examples vs. 52% in the non-fraud\n* `id_30` - Though MAC OS versions show up on non-fraud top 10, do not show up in top 10 for fraud, implying fraud less common on MAC\n* `DeviceType` - fraud was about evenly distributed (50\/50) between mobile and desktop, most non-fraud on desktop (61%)\n* `DeviceInfo` - similar to what id_30 implied, MAC used for 11% of non-fraud transactions but just 3% of fraud transactions\n\n\n### Comparing train distribution and test distribution\n* Remember, train size is $560,000$ and test size is $500,000$\n* Other than `TransactionDT`, the distributions look similar\n* Note that since the test set is later in time, there are some features where the distributions are almost certain to be different\n* e.g. `id_31` represents the browser used. For the train set, the most common browser was **chrome 63** at 16%. In the test set, the most common was **chrome 70**.\n7 versions later and **chrome 63** did not even show up in the top 10 most common browser for the test set, unsurprisingly.\n* Should I drop `id_31` and other columns affected by time or let the model weight it?\n* Also, looking at `DeviceType`, 60% of transactions in the train set were done on desktop vs. 54% on desktop in test set. \nCould this represent the increasing usage of mobile? Is there that much of a time difference between the train and test set?","845b4232":"# Fraud Detection \n* Author: Grant Gasser\n* Last Edit: 8\/18\/2019\n* Kaggle: \"In this competition you are predicting the probability that an online transaction is fraudulent, as denoted by the binary target `isFraud.`\"","f58662bd":"### Note TransactionDT has no overlap\n* As mentioned: https:\/\/www.kaggle.com\/robikscube\/ieee-fraud-detection-first-look-and-eda\n* Not sure what to do here. Maybe transform so that each value is relative to its range?","68ce6d03":"### Data Types\n* Before diving into EDA, look at data types of current features and see if they need to be changed","f5818477":"### XGBoost AUC = .938","552406a9":"### Hyperparameter tuning with GridSearch and RandomizedSearch\n* [XGBoost parameters](https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html)\n* [GridSearch](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV) - **Takes too much RAM**, exhaustive search of the parameters, expensive but finds the optimal set\n* set `scale_pos_weight` to adjust for class imbalance, common to do `sum(neg samples) \/ sum(pos samples)` which would be about 30 in this data set\n* control overfitting: `max_depth`, `min_child_weight`, `gamma` per xgboost docs","9ada4c64":"## Load and explore data","044b9f3c":"### Reduce memory usage before fitting XGBoost\n* Thanks to https:\/\/www.kaggle.com\/iasnobmatsu\/xgb-model-with-feature-engineering","93b601e4":"### In next iteration: come back and inspect columns being dropped\n* See if some should be kept and receive imputed values","f2ce6740":"### View tables","d2da8168":"### Replacing with Missing Values\n* Using code from: https:\/\/www.kaggle.com\/inversion\/ieee-simple-xgboost","29e2c4d7":"### Thoughts\n* Some of these should not be numerical data (e.g. card1-card6 should be 'object' types, not int64 or float64)\n* The next few cells changes this","79f28c34":"## Fit the XGBoost Classifier Again using Cross Validation\n* See how the performance differs after imputing values and normalizing data\n* The baseline score was `.938`","3f336f61":"## Compare train and test","5ea7d482":"### Remove features with large amounts of missing data\n* For computational purposes, removing features that have 80% (arbitrary number) or more missing values in the training set\n* May come back and try different values for this cutoff\n* Fill in remaining missing values with mean or median","9eb66753":"### Normalize Variables\n* For speed of convergence and numerical stability\n* Also to ensure variables with larger numbers do not dominate the model (e.g. TransactionAmt)\n* Normalize numerical variables: $x_{i,j} = \\frac{x_{i,j} - \\mu_j}{\\sigma_j}$ where $i$ is the row, $j$ is the column, $\\mu_j$ is the mean of the column and $\\sigma_j$ is the std of the col\n* After the transformation, we will have $\\mu_j = 0$ and $\\sigma_j = 1$ for each numerical column\/feature $j$\n* Could also try Min-Max scaling too which gives $x_j \\in (0,1)$ for all $i$.","1ebf19fa":"## XGBoost\n* https:\/\/developer.ibm.com\/code\/2018\/06\/20\/handle-imbalanced-data-sets-xgboost-scikit-learn-python-ibm-watson-studio\/\n* XGBoost is an extreme gradient boosting algorithm based on trees that tends to perform very well out of the box compared to other ML algorithms.\n* XGBoost is popular with data scientists and is one of the most common ML algorithms used in Kaggle Competitions.\n* XGBoost allows you to tune various parameters.\n* XGBoost allows parallel processing.","f7540327":"### XGBoost Classifier\n* See [notebook](https:\/\/www.kaggle.com\/inversion\/ieee-simple-xgboost) for starter code","af82fb0c":"### Weighted Avg\n* Based on scores (more weight for model outputs that had better scores)\n* `.05, .05, .1, .8` -> `.9392`, minor improvement from the best model's score of `.9381`, file: `ensemble5.csv`","ac5f0d54":"## Check Missing Values\n* Hint: there are lots of them","87bcc380":"## Explore Labels\n* Note the class imbalance\n* About 3.5% of train examples are fraudulent","0b5b117c":"## Ensemble\n* Will have to comment out everything below this\n* Load previous submissions","0c33e70d":"## Skip normalize to see effect on performance\n* Pre-processed XGBoost score `.878` vs. `.938` with no pre-processing\n* **Note:** After removing normalization for XGBoost, performance jumped from `.878` to `.932`. Normalization may only be necessary or helpful with neural nets and similar algorithms","6c878e2b":"**Categorical Features:**\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* Pemaildomain Remaildomain\n* M1 - M9\n* DeviceType\n* DeviceInfo\n* id12 - id38","359f9400":"### ^ Stopped early\n* Changing `gamma` does not seem to affect the performance\n* Adding more estimators and more max depth will improve performance on a subset of the test set, but has not led to improvement on the test set","82699be7":"## ENSEMBLING:\n* Averaging out my previous predictions using the files listed above ^ \n* data: `https:\/\/www.kaggle.com\/grantgasser\/previous-submissions`","813cc52b":"## TODO\n* GridSearchCV and RandomizedSearchCV take too much RAM\n* Will write my own grid search loop and be more efficient with RAM","76d46996":"## Train XGBoost model","1f00637c":"# Merge identity and transaction tables\n* Per Kaggle: \"The data is broken into two files `identity` and `transaction`, which are joined by `TransactionID`. Not all transactions have corresponding identity information.\n* Merge identity and transaction tables with `TransactionID` as the key\"\n* Since \"not all transactions have corresponding identity information,\" we will use a (left) outer join, using pandas merge function since a key might not appear in both tables","0f58b314":"## Label Encoding\n* Change categorical variable data to numbers so that computer can understand\n* e.g. if the encoding is: `['mastercard', 'discover', 'visa']` based on index, then data like `['visa', 'visa', 'mastercard', 'discover', 'mastercard']` would be encoded as `[2, 2, 0, 1, 0]`","c0d4e833":"## Replace missing values with mean (numerical) and most common category (categorical)","e9d38a4f":"## Summary\n**This is my first serious attempt at a Kaggle competition. As such, I would really appreciate some feedback or tips for improving performance. If you enjoyed this notebook and it helped you, please leave a thumbs up! Though I've written most of the code myself, I have found the other public kernels very helpful and would encourage you do browse through them to look for other good ideas.**\n\n* **Public Leaderboard Results:** (I plan on using some of the previous submission files for ensembling)\n* Random Forest filled NaNs with -999: `.872`\n* XGBoost filled NaNs with -999: `.938`, submission file: `baseline_xgboost.csv`\n* XGBoost impute mean for numerical NaNs and most common cat for categorical NaNs, also normalized numerical vars: `.878`\n* XGBoost impute mean for numerical NaNs and most common cat for categorical NaNs, no normalization: `.932`, submission file: `preprocessed_xgboost`\n* XGBoost, impute mean for numerical NaNs, do not impute most common category for categorical NaNs, no normalization: `.934`, file: `preprocessed2_xgboost`. **NOTE**: imputing mean for numerical NaNs and most common category for categorical NaNs did not seem to help for XGBoost. \n* Version 21: hyperparameter tuning with XGBoost (Grid Search or Random Search), `.9284`, `xgboost_with_tuning`\n* `.9226`, `xgboost_with_tuning2`","fd68e97c":"# Baseline Model with minimal pre-processing (.938)","0657cb26":"# Data Description \n* As provided by VESTA: https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-586800\n\n#### Transaction Table\n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distances between (not limited) billing address, mailing address, zip code, IP address, phone area, etc.\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n** Categorical Features: **\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* Pemaildomain Remaildomain\n* M1 - M9\n\n---\n\n#### Identity Table\nVariables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions. \nThey're collected by Vesta\u2019s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n** Categorical Features: **\n* DeviceType\n* DeviceInfo\n* id12 - id38","e24e1956":" ### Reminder\n **NOTE:** with fillna, replace and other pandas functions, make sure you set the variable, because it returns the transformed object\n * e.g. `df[feature] = df[feature].replace()` instead of just `df[feature].replace()`\n * **Note:** Just replacing NaNs with -999 and letting XGBoost handle it, will use this function if try different models such as Neural Network","4ebcbba9":"# EDA","32e0dca3":"# Pre-processing","66a27520":"## Compare fraud and non-fraud (within training set)\n1. Compare the difference in means of numerical features between the fraud and non-fraud transactions. \n\n2. Compare the difference in distributions of categorical features between the fraud and non-fraud transactions. \n ","fc37208e":"### Look at a few fraudulent transactions","b4916515":"## Libraries","9b7381be":"## View provided files"}}