{"cell_type":{"b4a86f81":"code","28e13087":"code","6c89798e":"code","c8b9d3f8":"code","41a05233":"code","b5f03b10":"code","2dbaa734":"code","3c88fb4e":"code","e8bfcb07":"code","484d9d40":"code","3c306d4b":"code","b1a4111c":"markdown","b2e68588":"markdown","ee4f2c12":"markdown"},"source":{"b4a86f81":"import torch\nimport transformers\n\nMODEL_URLS = {\n    \"original\": \"https:\/\/github.com\/unitaryai\/detoxify\/releases\/download\/v0.1-alpha\/toxic_original-c1212f89.ckpt\"\n}\n\nPRETRAINED_MODEL = None\n\n\ndef get_model_and_tokenizer(\n    model_type, model_name, tokenizer_name, num_classes, state_dict\n):\n    model_class = getattr(transformers, model_name)\n    model = model_class.from_pretrained(\n        pretrained_model_name_or_path=\"..\/input\/get-huggingface-models\/bert-base-uncased\/\", # clone huggingface model via another online notebook\n        num_labels=num_classes,\n        state_dict=state_dict,\n        local_files_only=True\n    )\n    tokenizer = getattr(transformers, tokenizer_name).from_pretrained(\n        \"..\/input\/get-huggingface-models\/bert-base-uncased\/\", # clone huggingface model via another online notebook\n        local_files_only=True,\n        model_max_length=512\n    )\n\n    return model, tokenizer\n\n\ndef load_checkpoint(model_type=\"original\", checkpoint=None, device='cpu'):\n    if checkpoint is None:\n        checkpoint_path = MODEL_URLS[model_type]\n        loaded = torch.hub.load_state_dict_from_url(checkpoint_path, map_location=device)\n    else:\n        loaded = torch.load(checkpoint)\n        if \"config\" not in loaded or \"state_dict\" not in loaded:\n            raise ValueError(\n                \"Checkpoint needs to contain the config it was trained \\\n                    with as well as the state dict\"\n            )\n    class_names = loaded[\"config\"][\"dataset\"][\"args\"][\"classes\"]\n    # standardise class names between models\n    change_names = {\n        \"toxic\": \"toxicity\",\n        \"identity_hate\": \"identity_attack\",\n        \"severe_toxic\": \"severe_toxicity\",\n    }\n    class_names = [change_names.get(cl, cl) for cl in class_names]\n    model, tokenizer = get_model_and_tokenizer(\n        **loaded[\"config\"][\"arch\"][\"args\"], state_dict=loaded[\"state_dict\"]\n    )\n\n    return model, tokenizer, class_names\n\n\ndef load_model(model_type, checkpoint=None):\n    if checkpoint is None:\n        model, _, _ = load_checkpoint(model_type=model_type)\n    else:\n        model, _, _ = load_checkpoint(checkpoint=checkpoint)\n    return model\n\n\nclass Detoxify:\n    def __init__(self, model_type=\"original\", checkpoint=PRETRAINED_MODEL, device=\"cpu\"):\n        super(Detoxify, self).__init__()\n        self.model, self.tokenizer, self.class_names = load_checkpoint(\n            model_type=model_type, checkpoint=checkpoint, device=device\n        )\n        self.device = device\n        self.model.to(self.device)\n\n\n    @torch.no_grad()\n    def predict(self, text):\n        self.model.eval()\n        inputs = self.tokenizer(\n            text, return_tensors=\"pt\", truncation=True, padding=True\n        ).to(self.model.device)\n        out = self.model(**inputs)[0]\n        scores = torch.sigmoid(out).cpu().detach().numpy()\n        results = {}\n        for i, cla in enumerate(self.class_names):\n            results[cla] = (\n                scores[0][i]\n                if isinstance(text, str)\n                else [scores[ex_i][i].tolist() for ex_i in range(len(scores))]\n            )\n        return results","28e13087":"# make sure that all the dependencies match to what detoxify is requiring\n!pip install ..\/input\/detoxifygithub18112021\/ > \/dev\/null # include github repo of detoxify into the data","6c89798e":"# load checkpoint of a required model\n!mkdir \/root\/.cache\/torch\n!mkdir \/root\/.cache\/torch\/hub\n!mkdir \/root\/.cache\/torch\/hub\/checkpoints\n!cp ..\/input\/detoxifyoriginal\/toxic_original-c1212f89.ckpt \/root\/.cache\/torch\/hub\/checkpoints\/toxic_original-c1212f89.ckpt","c8b9d3f8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","41a05233":"# test\nmodel = Detoxify('original', device='cuda')\nmodel.predict('I hate you')['severe_toxicity']","b5f03b10":"comments_to_score = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsample_submission = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/sample_submission.csv\")\nvalidation_data = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")","2dbaa734":"# create new column \"score\" with the scores from the pre-trained model\ncomments_to_score['score'] = comments_to_score.text.apply(lambda x: model.predict(x)['severe_toxicity']).values","3c88fb4e":"ax = sns.distplot(comments_to_score['score'])","e8bfcb07":"comments_to_score[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","484d9d40":"validation_data['score_less'] = validation_data.less_toxic.apply(lambda x: model.predict(x)['severe_toxicity']).values\nvalidation_data['score_more'] = validation_data.more_toxic.apply(lambda x: model.predict(x)['severe_toxicity']).values","3c306d4b":"# Average Agreement with Annotators -- score by organizers\nranking_results = (np.array(validation_data.score_less.values) < np.array(validation_data.score_more.values))\nranking_results.mean()","b1a4111c":"# Because notebook is not allowed to have intertet, I had to add this (copied from detoxify library):","b2e68588":"# Validation","ee4f2c12":"# Now, actual code begins"}}