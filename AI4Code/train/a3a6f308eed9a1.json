{"cell_type":{"ad87714e":"code","5b838c7c":"code","01c956fb":"code","5e78c586":"code","a04a3951":"code","2859f694":"code","9c323a63":"code","ce7846cb":"code","aa69d126":"code","a1c519ce":"code","b59643c8":"code","9998ffdd":"code","223811f3":"code","daa8e0c8":"code","8ae72505":"code","714baff9":"code","8023c0fd":"code","012afad6":"code","e1776852":"code","03991903":"code","6bffa524":"code","d3a23715":"code","2a2a2433":"code","8ad3a5ac":"code","965dd55d":"code","18017052":"code","16d952cb":"code","0a2fe623":"code","39ab6e65":"code","2876129f":"code","092ae8cd":"code","fe3f82ea":"code","8995631e":"code","7f13a24c":"code","7e9c6ad2":"code","29a71a4c":"code","70dccb4b":"code","4213ab98":"code","36c060f5":"code","a4888edb":"markdown","5f886c97":"markdown","1ba23c91":"markdown","cbcc57ce":"markdown","be536804":"markdown","473fd9f0":"markdown","6f65ee17":"markdown","8decec01":"markdown","1c6384c5":"markdown","d3181d2d":"markdown","17f1becb":"markdown","86f26bc1":"markdown","ce2c19a7":"markdown","e607b81a":"markdown","982a8799":"markdown","4a0b38bb":"markdown"},"source":{"ad87714e":"## Importing the libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","5b838c7c":"## Importing the dataset\ndataset = pd.read_csv('..\/input\/videogamesales\/vgsales.csv')","01c956fb":"dataset.info()","5e78c586":"dataset.head(10)","a04a3951":"# Checking for null values in the dataset\ndataset.isnull().values.any()","2859f694":"## Checking which columns contain null values\nprint(dataset['Rank'].isnull().values.any())\nprint(dataset['Name'].isnull().values.any())\nprint(dataset['Platform'].isnull().values.any())\nprint(dataset['Year'].isnull().values.any())\nprint(dataset['Genre'].isnull().values.any())\nprint(dataset['Publisher'].isnull().values.any())\nprint(dataset['NA_Sales'].isnull().values.any())\nprint(dataset['EU_Sales'].isnull().values.any())\nprint(dataset['JP_Sales'].isnull().values.any())\nprint(dataset['Other_Sales'].isnull().values.any())\nprint(dataset['Global_Sales'].isnull().values.any())","9c323a63":"#Checking the number of missing value rows in the dataset\nprint(dataset['Year'].isnull().sum())\nprint(dataset['Publisher'].isnull().sum())","ce7846cb":"# Removing the missing value rows in the dataset\ndataset = dataset.dropna(axis=0, subset=['Year','Publisher'])","aa69d126":"dataset.isnull().values.any()","a1c519ce":"## Defining the features and the dependent variable\nx = dataset.iloc[:,1:-1].values\ny = dataset.iloc[:,-1].values\nprint(x[0])\nprint(y)","b59643c8":"## Determining the relevancy of features using heatmap in calculating the outcome variable\ncorrmat = dataset.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(10,10))\n#Plotting heat map\ng=sns.heatmap(dataset[top_corr_features].corr(),annot=True,linewidths=.5)\nb, t = plt.ylim() # Finding the values for bottom and top\nb += 0.5 \nt -= 0.5 \nplt.ylim(b, t) \nplt.show() ","9998ffdd":"# Retaining only the useful features of the dataset\n# From the heatmap, we can decipher that the columns NA_Sales,JP_Sales,EU_Sales and Other_Sales are the most useful features\n# in determining the global sales\nx = dataset.iloc[:,6:-1].values\nprint(x[0])","223811f3":"## Splitting the dataset into independent and dependent vaiables\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)","daa8e0c8":"print(x_train)\nprint(x_test)\nprint(y_train)\nprint(y_test)","8ae72505":"## Training the multiple linear regression on the training set\nfrom sklearn.linear_model import LinearRegression\nregressor_MultiLinear = LinearRegression()\nregressor_MultiLinear.fit(x_train,y_train)","714baff9":"## Predicting test results\ny_pred = regressor_MultiLinear.predict(x_test)","8023c0fd":"# Calculating r2 score\nfrom sklearn.metrics import r2_score\nr2_MultiLinear = r2_score(y_test,y_pred)\nprint(r2_MultiLinear)","012afad6":"## Finding out the optimal degree of polynomial regression\nfrom sklearn.preprocessing import PolynomialFeatures\nsns.set_style('darkgrid')\nscores_list = []\npRange = range(2,6)\nfor i in pRange :\n    poly_reg = PolynomialFeatures(degree=i)\n    x_poly = poly_reg.fit_transform(x_train)\n    poly_regressor = LinearRegression()\n    poly_regressor.fit(x_poly,y_train)\n    y_pred = poly_regressor.predict(poly_reg.fit_transform(x_test))\n    scores_list.append(r2_score(y_test,y_pred))\nplt.plot(pRange,scores_list,linewidth=2)\nplt.xlabel('Degree of polynomial')\nplt.ylabel('r2 score with varying degrees')\nplt.show()","e1776852":"## Training the polynomial regression on the training model\npoly_reg = PolynomialFeatures(degree=2)\nx_poly = poly_reg.fit_transform(x_train)\npoly_regressor = LinearRegression()\npoly_regressor.fit(x_poly,y_train)\ny_pred = poly_regressor.predict(poly_reg.fit_transform(x_test))\nr2_poly = r2_score(y_test,y_pred)\nprint(r2_poly)","03991903":"## Finding the optimal number of neighbors for KNN regression\nfrom sklearn.neighbors import KNeighborsRegressor\nknnRange = range(1,11,1)\nscores_list = []\nfor i in knnRange:\n    regressor_knn = KNeighborsRegressor(n_neighbors=i)\n    regressor_knn.fit(x_train,y_train)\n    y_pred = regressor_knn.predict(x_test)\n    scores_list.append(r2_score(y_test,y_pred))\nplt.plot(knnRange,scores_list,linewidth=2,color='green')\nplt.xticks(knnRange)\nplt.xlabel('No. of neighbors')\nplt.ylabel('r2 score of KNN')\nplt.show()    ","6bffa524":"# Training the KNN model on the training set\nregressor_knn = KNeighborsRegressor(n_neighbors=7)\nregressor_knn.fit(x_train,y_train)\ny_pred = regressor_knn.predict(x_test)\nr2_knn = r2_score(y_test,y_pred)\nprint(r2_knn)","d3a23715":"# Training the Decision Tree regression on the training model\nfrom sklearn.tree import DecisionTreeRegressor\nregressor_Tree = DecisionTreeRegressor(random_state=0)\nregressor_Tree.fit(x_train,y_train)","2a2a2433":"# Predicting test results\ny_pred = regressor_Tree.predict(x_test)","8ad3a5ac":"# Calculating r2 score\nr2_tree = r2_score(y_test,y_pred)\nprint(r2_tree)","965dd55d":"# Finding out the optimal number of trees for Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\nforestRange=range(50,500,50)\nscores_list=[]\nfor i in forestRange: \n    regressor_Forest = RandomForestRegressor(n_estimators=i,random_state=0)\n    regressor_Forest.fit(x_train,y_train)\n    y_pred = regressor_Forest.predict(x_test)\n    scores_list.append(r2_score(y_test,y_pred))\nplt.plot(forestRange,scores_list,linewidth=2,color='maroon')\nplt.xticks(forestRange)\nplt.xlabel('No. of trees')\nplt.ylabel('r2 score of Random Forest Reg.')\nplt.show()    ","18017052":"# Training the Random Forest regression on the training model\nregressor_Forest = RandomForestRegressor(n_estimators=100,random_state=0)\nregressor_Forest.fit(x_train,y_train)\ny_pred = regressor_Forest.predict(x_test)\nr2_forest = r2_score(y_test,y_pred)\nprint(r2_forest)","16d952cb":"## Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nx_train = sc_x.fit_transform(x_train)\nx_test = sc_x.transform(x_test)\nsc_y = StandardScaler()\ny_train = sc_y.fit_transform(np.reshape(y_train,(len(y_train),1)))\ny_test = sc_y.transform(np.reshape(y_test,(len(y_test),1)))","0a2fe623":"print(x_train)\nprint(x_test)\nprint(y_test)\nprint(y_train)","39ab6e65":"## Training the Linear SVR model on the training set\nfrom sklearn.svm import SVR\nregressor_SVR = SVR(kernel='linear')\nregressor_SVR.fit(x_train,y_train)","2876129f":"## Predicting test results\ny_pred = regressor_SVR.predict(x_test)","092ae8cd":"## Calculating r2 score\nr2_linearSVR = r2_score(y_test,y_pred)\nprint(r2_linearSVR)","fe3f82ea":"## Training the Non-linear SVR model on the training set\nfrom sklearn.svm import SVR\nregressor_NonLinearSVR = SVR(kernel='rbf')\nregressor_NonLinearSVR.fit(x_train,y_train)","8995631e":"## Predicting test results\ny_pred = regressor_NonLinearSVR.predict(x_test)","7f13a24c":"## Calculating r2 score\nr2_NonlinearSVR = r2_score(y_test,y_pred)\nprint(r2_NonlinearSVR)","7e9c6ad2":"## Applying XGBoost Regression model on the training set\nfrom xgboost import XGBRegressor\nregressor_xgb = XGBRegressor()\nregressor_xgb.fit(x_train,y_train)","29a71a4c":"## Predicting test results\ny_pred = regressor_xgb.predict(x_test)","70dccb4b":"## Calculating r2 score\nr2_xgb = r2_score(y_test,y_pred)\nprint(r2_xgb)","4213ab98":"## Comparing the r2 scores of different models\nlabelList = ['Multiple Linear Reg.','Polynomial Reg.','K-NearestNeighbors','Decision Tree','Random Forest',\n             'Linear SVR','Non-Linear SVR','XGBoost Reg.']\nmylist = [r2_MultiLinear,r2_poly,r2_knn,r2_tree,r2_forest,r2_linearSVR,r2_NonlinearSVR,r2_xgb]\nfor i in range(0,len(mylist)):\n    mylist[i]=np.round(mylist[i]*100,decimals=3)\nprint(mylist)","36c060f5":"plt.figure(figsize=(14,8))\nax = sns.barplot(x=labelList,y=mylist)\nplt.yticks(np.arange(0, 101, step=10))\nplt.title('r2 score comparison among different regression models',fontweight='bold')\nfor p in ax.patches:\n    width, height = p.get_width(), p.get_height()\n    x, y = p.get_xy() \n    ax.annotate('{:.3f}%'.format(height), (x +0.25, y + height + 0.8))\nplt.show()","a4888edb":"### Decision Tree Regression","5f886c97":"### K-Nearest Neighbors Regression","1ba23c91":"---","cbcc57ce":"---","be536804":"---","473fd9f0":"---","6f65ee17":"### Random Forest Regression","8decec01":"---","1c6384c5":"### Linear Support Vector Regression","d3181d2d":"### Non-linear Support Vector Regression","17f1becb":"----","86f26bc1":"---","ce2c19a7":"---","e607b81a":"### Polynomial Regression","982a8799":"---","4a0b38bb":"### Multiple Linear Regression"}}