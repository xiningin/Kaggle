{"cell_type":{"70a50056":"code","609dff85":"code","fa693ae5":"code","39bc86ee":"code","db054450":"code","55746895":"code","9dc3ed5b":"code","107c0947":"code","138f5752":"code","8f10e462":"code","b5de3777":"code","237c4703":"code","b7af0ef4":"code","aa20cd05":"code","3a6f6af4":"code","2214c674":"code","d9a2eb72":"code","8254d659":"code","35086ce7":"code","d0d602d0":"code","b245462f":"code","62546314":"code","f5480b19":"code","c631cfb0":"code","c99baf5e":"code","fa9d3a77":"code","f4b52810":"code","c73a6e43":"code","9b823ac2":"code","0a02822a":"code","2451fff7":"code","f29e53f6":"code","4f6f9f17":"code","31ed4de0":"code","df0ee0fd":"code","2b7516f0":"code","c377634b":"code","fea3a2d7":"code","f4b64ddc":"code","ce15ca07":"code","cb5f678f":"code","f2cb014c":"code","1ee115e9":"code","e0a6f28e":"code","7b15cadb":"code","fc8fa7b9":"code","7cb5e9db":"code","81523bf6":"code","07938950":"code","88072728":"code","f2e949db":"code","005f6b20":"code","25a9244f":"code","a14d863a":"code","30cc08d5":"code","79ae36a5":"code","875eede2":"code","5ef59ffa":"code","9f068bb4":"code","5d8cc40e":"code","bff5ad65":"code","fce75eb7":"code","3c7c57ee":"code","662c0bba":"code","a0e95160":"code","8e14ca5b":"code","8009358f":"markdown","a203bbee":"markdown","68813f7d":"markdown","73dca959":"markdown","a0ed8352":"markdown","3b9c1a36":"markdown","80e2591c":"markdown","fc93f855":"markdown","878bac06":"markdown","851e0ff8":"markdown","094a3902":"markdown","0da3b7d5":"markdown","d4bc0d3f":"markdown","98e753d0":"markdown","952c7b9f":"markdown","8590b5f9":"markdown","4184e1dd":"markdown","57f8e9b5":"markdown","0bb95393":"markdown","8e8baca8":"markdown","c741e45b":"markdown","5ae685b1":"markdown","8905aef3":"markdown","f7549c50":"markdown","33f6f4d2":"markdown","f48d79a3":"markdown","12973314":"markdown","1b6217f8":"markdown","073673fb":"markdown","45fd4820":"markdown","3d1ae400":"markdown","0e8702b8":"markdown","b3a23df9":"markdown","ed83b80b":"markdown","398ba2ae":"markdown","8390b736":"markdown","1daa43d8":"markdown","d76fafc5":"markdown","87794410":"markdown","40811e98":"markdown","b11c3ace":"markdown","54a039df":"markdown","9c7b46a2":"markdown","e7e66a5f":"markdown","7442205b":"markdown","9e7905ff":"markdown","00a818b4":"markdown","d8c94d46":"markdown","3631202b":"markdown","8f849eb2":"markdown","df640f84":"markdown","52a93115":"markdown","73228db6":"markdown","a1f08c2c":"markdown","b79e214f":"markdown","d9a79f12":"markdown","f632b3e4":"markdown","f92660da":"markdown","71bf8f16":"markdown","dfec6966":"markdown","4bec07f3":"markdown","23e0e830":"markdown","e5bc0794":"markdown","6a528cab":"markdown","0a8325bb":"markdown","f604adc2":"markdown","b3bb2057":"markdown","27f79d64":"markdown","590b6bdb":"markdown","47d74d9d":"markdown","8a4051bb":"markdown","0efe1662":"markdown","3cbf4ad4":"markdown","d580f417":"markdown","92c110bf":"markdown","a56b07e0":"markdown","54b02271":"markdown","e149a5f9":"markdown","609b0694":"markdown","7ad1a7f1":"markdown","3fe3cebe":"markdown","3ab546f5":"markdown","cb9a0ed3":"markdown","9741a227":"markdown","19d5860e":"markdown","cbd5208a":"markdown","7a0bdfb2":"markdown","4ad8a1f2":"markdown","0a142c90":"markdown","15a72834":"markdown","c4738f33":"markdown","49e68fb1":"markdown","233607d1":"markdown","2e097329":"markdown","3a590c71":"markdown","7b4a97f6":"markdown","ac9034f7":"markdown","a89e20f4":"markdown","b1cf2777":"markdown","e8655c63":"markdown","8654bc0b":"markdown","fe36de84":"markdown","a1c15861":"markdown"},"source":{"70a50056":"!pip install dataprep -q -U\n!pip install seaborn -q -U\n!pip install scikit-learn==0.24.2 -q -U\n!pip install git+https:\/\/github.com\/QuentinSoubeyran\/scikit-optimize.git@fix-plot_evaluations -q -U\n# !pip install git+https:\/\/github.com\/QuentinSoubeyran\/scikit-optimize.git@multimetric -q -U\n# !pip install git+https:\/\/github.com\/scikit-optimize\/scikit-optimize.git -q\n!pip install -U kaleido -q\n# !pip install explainerdashboard -q -U","609dff85":"import numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nfrom yellowbrick.classifier.threshold import discrimination_threshold\nimport plotly.express as px\nfrom dataprep.eda import plot, create_report\n\n# Preprocessing\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import RobustScaler #, StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n# from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n# from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.pipeline import make_pipeline, Pipeline\n\n# Feature Engineering\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import SelectFromModel\n\n# Models\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n# from sklearn.linear_model import LogisticRegression, RidgeClassifier\nfrom sklearn.svm import SVC as SVClassifier\nfrom xgboost import XGBClassifier\nfrom imblearn.ensemble import BalancedRandomForestClassifier, RUSBoostClassifier\n\n# Metrics & scoring\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.experimental import enable_halving_search_cv\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.calibration import calibration_curve\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import plot_confusion_matrix, plot_precision_recall_curve, plot_roc_curve\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.inspection import plot_partial_dependence\nfrom sklearn.model_selection import learning_curve\n\n# tunning tools \nfrom skopt import BayesSearchCV\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import plot_objective, plot_histogram, plot_evaluations\n\n# diplays HTML representation in a jupyter context\nfrom sklearn import set_config\nset_config(display='diagram')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", module=\"FutureWarning\")\nfrom time import sleep\nimport joblib\nfrom math import ceil\nfrom IPython.display import Image\nsubmitting = True","fa693ae5":"class txt_style:\n    PURPLE = '\\033[95m'\n    CYAN = '\\033[96m'\n    DARKCYAN = '\\033[36m'\n    BLUE = '\\033[94m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    RED = '\\033[91m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\n    END = '\\033[0m'\n\ndef reduce_mem_usage(original_df, conserve=False, inplace=False, report=True, low_card_as_cat=10):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n        if conserve : avoid loss of precision by disabling float convertion  \n        if inplace : don't make a copy, change the passed dataframe\n        if report : print reduced amount of memory\n        low_card_as_cat : if a numeric variable has a lower cardinality than this threshold it'll be considered categorical, \n                          0 means all numeric variables would stay numeric\n    \"\"\"\n    # cases of when to backup or copy data\n    if inplace:\n        df = original_df\n        if not conserve : backup = original_df.copy()\n    else:\n        df = original_df.copy()\n        if not conserve : backup = original_df\n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    if report:\n        print(f'Initial memory usage of dataframe is {start_mem:.2f} MB')\n    \n    # set the right dtype for each column based on the range of its values\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object and df[col].nunique()>low_card_as_cat:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            elif not conserve:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max :\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    if report:\n        end_mem = df.memory_usage().sum() \/ 1024**2\n        print(f'Memory usage after optimization is: {end_mem:.2f} MB')\n        print(txt_style.GREEN+txt_style.BOLD+f'Memory consumption decreased by {100 * (start_mem - end_mem)\/start_mem:.1f}%'+txt_style.END)\n        if conserve :\n            print(txt_style.BLUE+'All values conserved, 0% loss of information'+txt_style.END)\n        else:\n            print('Mean absolute percentage loss of information in float columns :')\n            print(\n                100 *\n                (backup.select_dtypes(include=['float16', 'float32', 'float64']) -\n                 df.select_dtypes(include=['float16', 'float32'])).sum().abs() \/\n                backup.select_dtypes('float').sum())\n    return df","39bc86ee":"import os\nimport inspect\n\ndef get_default_args(func):\n    signature = inspect.signature(func)\n    return {\n        k: v.default\n        for k, v in signature.parameters.items()\n        if v.default is not inspect.Parameter.empty\n    }\n\nclass ForceRunException(Exception): pass\nfrom functools import wraps\n\ndef lazy_run(joblib_Filename, compress=9): #meta decorator to enable passing argument to the decorator\n    def real_decorator(function): #actual decorator that adds the functionality of `lazy_run`\n        @wraps(function)\n        def wrapper(*args, **kwargs):\n            force_run_arg = kwargs.get('force_run', None)\n            default_args = get_default_args(function)\n            default_force_run = default_args.get('force_run', False)\n            if force_run_arg is None:\n                force_run = default_force_run\n            else :\n                force_run = force_run_arg\n            assert type(force_run) is bool\n            try:\n                if force_run:\n                    raise ForceRunException\n                execution_results = joblib.load(joblib_Filename)\n                warnings.warn(\"function call and arguments are not evaluated, and might be invalide !!\")\n                print(\"Stored results from an old execution has been found and successfully imported\")\n            except (ForceRunException, FileNotFoundError) as e:\n                if e.__class__ == ForceRunException:\n                    print(\"Explicit execution requested,\", end=' ')\n                else:\n                    print(\"data not found,\", end=' ')\n                print(\"Executing calculations...\")\n                execution_results = function(*args, **kwargs)\n                print(\"Computations are done.\", \"Saving data to\", repr(joblib_Filename))\n                try:\n                    joblib.dump(execution_results, joblib_Filename, compress=compress)\n                except OSError:\n                    path, file = joblib_Filename.rsplit('\/', 1)\n                    print(\"Write access denied to\", repr(path), \".. Now saving to\", os.getcwd(), \"(default)\")\n                    joblib.dump(execution_results, file, compress=compress)\n                print(\"data has been saved successfully!\")\n            finally: #return the computation results regardless of the occurred exceptions\n                return execution_results\n        return wrapper\n    return real_decorator","db054450":"from time import perf_counter\n\ndef repr_time(sec):\n    minutes, seconds = divmod(sec, 60)\n    hours, minutes = divmod(minutes, 60)\n    seconds = f\"{seconds:.2f}s\" if hours==minutes==0 else f\"{seconds:.0f}s\"\n    minutes = f\"{minutes:.0f}min \" if minutes>0 else ''\n    hours = f\"{hours:.0f}h \" if hours>0 else ''\n    return hours+minutes+seconds","55746895":"#Preventing myself from accidentally runing the cell \ndef confirm_execution(method):\n    if submitting:\n        return\n    val = input(f\"Are you sure you want to re-run this cell? (this would erase old stored {method} results) [y-yes-Y] to confirm\\n\")\n    if val not in {'y', 'yes', 'Y'}: \n        raise Exception(\"Phew that was close\")","9dc3ed5b":"def type_name(estimator):\n    \"\"\"Getting the class name of an instance\"\"\"\n    return type(estimator).__name__","107c0947":"data_filepath = \"..\/input\/credit-card-customers\/BankChurners.csv\"\nchurn_data = pd.read_csv(data_filepath, index_col='CLIENTNUM', usecols = range(21))\nreduced_churn_data = reduce_mem_usage(churn_data, conserve=False, report=False)\nconserved_churn_data = reduce_mem_usage(churn_data, conserve=True, report=True)","138f5752":"churn_data.head()","8f10e462":"# Check missing values\npd.DataFrame(churn_data.isnull().sum(), columns=['missing_count']).T","b5de3777":"cat_cols, num_cols = [], []\nfor col in reduced_churn_data.columns:\n    if reduced_churn_data[col].dtype.name == \"category\":\n        cat_cols.append(col)\n    else:\n        num_cols.append(col)","237c4703":"# move numeric categories to the end for plots convenience\ncat_cols = [col for col in cat_cols if reduced_churn_data[col].cat.categories.dtype.name[:3] != 'int'] + \\\n[col for col in cat_cols if reduced_churn_data[col].cat.categories.dtype.name[:3] == 'int']","b7af0ef4":"colors = (\"palegreen\", \"lightblue\", \"paleturquoise\", \"lightsteelblue\", \"plum\", \"darksalmon\", \"darkcyan\", \"peru\", \"lightslategray\", \"lightgray\")\nprint(f\"categorical features : ({len(cat_cols)})\")\nfor col, color in zip(cat_cols, colors):\n    print(f\"\\n{col} [cardinality : {reduced_churn_data[col].nunique()}]\")\n    res = pd.DataFrame({\"count\" : reduced_churn_data[col].value_counts(), \n                        \"percentage\" : (100*reduced_churn_data[col].value_counts(normalize=True)).round(2).astype(str)+' %'})\n    if reduced_churn_data[col].cat.categories.dtype.name[:3] == 'int' :\n        display(res.sort_index().style.bar(subset=['count'], align='mid', color=color))\n    else:\n        display(res.style.bar(subset=['count'], align='mid', color=color))","aa20cd05":"cmap = sns.light_palette(\"lightgray\", as_cmap=True)\n# droping 'count' as we already seen there is no missing values\nchurn_data[num_cols].describe().drop('count').T.sort_values('50%', ascending=False).style.format('{:.3f}').background_gradient(cmap=cmap)","3a6f6af4":"dataprep_report = create_report(conserved_churn_data, display=[\"Overview\", \"Variables\"], progress=False)\ndataprep_report","2214c674":"to_plot = conserved_churn_data.apply(lambda col : col.astype('object') if col.dtype.name == 'category' else col)\ndisp = [\"Nested Bar Chart\", \"Stacked Bar Chart\"]\nfor col in (cat_cols + num_cols):\n    if col != 'Attrition_Flag':\n        plot(to_plot, col, 'Attrition_Flag', display=disp, progress=False).show()\n        if col=='Contacts_Count_12_mon': disp = None","d9a2eb72":"to_plot = to_plot.apply(lambda col : col.astype('int')  if col.dtype.name == 'object' and col.astype(str).str.isdigit().all() else col)\n\nfig = px.scatter_matrix(reduced_churn_data,\n    dimensions=num_cols,#cat_cols[1:]+\n    color=cat_cols[0],\n    opacity=0.1)\nfig.update_traces(showupperhalf=False)\nfig.update_layout(\n    title='Interactions scatter matrix',\n    width=2400,\n    height=2000,\n)\nimg_bytes = fig.to_image(format=\"png\")\ndisplay(Image(img_bytes, width = 1400, height = 1000))","8254d659":"df = reduced_churn_data.loc[:, cat_cols]\ndf.loc[:, cat_cols[0]] = df.loc[:, cat_cols[0]].replace({\"Existing Customer\":0, \"Attrited Customer\":1}).astype('int')\nfig = px.parallel_categories(df.iloc[:,:6].loc[df.Attrition_Flag==1], dimensions=cat_cols[1:6], \n#                              color=cat_cols[0], color_continuous_scale=[[0, 'lightgray'], [1, 'darkblue']],\n                             width=1000, height=500, title=\"Churning customers groups\")\nfig.update_layout(title_x=0.5)\nimg_bytes = fig.to_image(format=\"png\")\ndisplay(Image(img_bytes))","35086ce7":"# The data on which we'll train our models \nmodel_data = reduce_mem_usage(churn_data, conserve=True, report=False, low_card_as_cat=0)\nX = model_data.copy()\ntarget_labels = ['Existing Customer','Attrited Customer']\ny = X.pop('Attrition_Flag').map(dict(zip(target_labels, (0,1))))\n# Train\/Test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)","d0d602d0":"# Defining the encoding variables\nordinal_enc_variables = [\"Education_Level\", \"Income_Category\", \"Card_Category\"]\none_hot_enc_variables = [\"Gender\",\"Marital_Status\"]\n\n# Defining ordinal categories\nedu_lvl = [\"Unknown\", \"Uneducated\", \"High School\", \"College\", \"Graduate\", \"Post-Graduate\", \"Doctorate\"]\nincome_cat = [\"Unknown\", \"Less than $40K\", \"$40K - $60K\", \"$80K - $120K\", \"$60K - $80K\", \"$120K +\"]\ncard_cat = [\"Blue\", \"Silver\", \"Gold\", \"Platinum\"]\n\n# Calling the encoder classes\nord_enc = OrdinalEncoder(categories=[edu_lvl, income_cat, card_cat])\none_hot_enc = OneHotEncoder(handle_unknown=\"ignore\")\n\n# Make column transformer\ncat_transformer = make_column_transformer((ord_enc, ordinal_enc_variables),\n                                          (one_hot_enc, one_hot_enc_variables),\n                                          remainder=\"passthrough\")","b245462f":"# PCA\next = ExtraTreesClassifier(n_estimators=50, random_state=42)\nselector = SelectFromModel(ext, threshold=\"1.5*median\")\nfeature_maker = FeatureUnion(\n    [(\"join\", make_pipeline('passthrough')),\n     (\"best_pca\", make_pipeline(PCA(random_state=42), selector))\n    ], n_jobs=-1)","62546314":"# Make pipeline\npipeline = Pipeline(\n    steps=[('cat_encod', cat_transformer),\n           ('up_samp', SMOTETomek(random_state=42)), \n           ('PCA_placeholder', 'passthrough'), #feature_maker\n           ('scaler', RobustScaler()), \n           ('clf', XGBClassifier())]) #model placeholder\n\n# Make custom scoring metric\nscorer = 'f1' #make_scorer(fbeta_score, beta=2, pos_label=1)","f5480b19":"def get_feature_names(column_transformer, as_list=True):\n    \"\"\"Get feature names from all transformers.\n    Returns\n    -------\n    feature_names : list of strings\n        Names of the features produced by transform.\n    \"\"\"\n    # Remove the internal helper function\n    #check_is_fitted(column_transformer)\n    \n    # Turn loopkup into function for better handling with pipeline later\n    def get_names(trans):\n        # >> Original get_feature_names() method\n        if trans == 'drop' or (\n                hasattr(column, '__len__') and not len(column)):\n            return []\n        if trans == 'passthrough':\n            if hasattr(column_transformer, '_df_columns'):\n                if ((not isinstance(column, slice))\n                        and all(isinstance(col, str) for col in column)):\n                    return column\n                else:\n                    return column_transformer._df_columns[column]\n            else:\n                indices = np.arange(column_transformer._n_features)\n                return ['x%d' % i for i in indices[column]]\n        if not hasattr(trans, 'get_feature_names'):\n        # >>> Change: Return input column names if no method avaiable\n            # Turn error into a warning\n#             warnings.warn(\"Transformer %s (type %s) does not \"\n#                                  \"provide get_feature_names. \"\n#                                  \"Will return input column names if available\"\n#                                  % (str(name), type(trans).__name__))\n            # For transformers without a get_features_names method, use the input\n            # names to the column transformer\n            if column is None:\n                return []\n            else:\n                return [name + \"__\" + f for f in column]\n\n        return [name + \"__\" + f for f in trans.get_feature_names()]\n    \n    ### Start of processing\n    feature_names = []\n    \n    # Allow transformers to be pipelines. Pipeline steps are named differently, so preprocessing is needed\n    if type(column_transformer) == Pipeline:\n        l_transformers = [(name, trans, None, None) for step, name, trans in column_transformer._iter()]\n    else:\n        # For column transformers, follow the original method\n        l_transformers = list(column_transformer._iter(fitted=True))\n    \n    \n    for name, trans, column, _ in l_transformers: \n        if type(trans) == Pipeline:\n            # Recursive call on pipeline\n            _names = get_feature_names(trans)\n            # if pipeline has no transformer that returns names\n            if len(_names)==0:\n                _names = [name + \"__\" + f for f in column]\n            feature_names.extend(_names)\n        else:\n            feature_names.extend(get_names(trans))\n    if as_list:\n        return list(map(lambda s: s.split('__')[-1], list(feature_names)))\n    feature_dict = {}\n    for elem in map(lambda s: s.split('__'), list(feature_names)):\n        key = '__'.join(elem[:-1]) if len(elem)>1 else 'passthrough'\n        if key in feature_dict:\n            feature_dict[key].append(elem[-1])\n        else:\n            feature_dict[key] = elem[-1:]\n\n    return feature_dict\n\ndef get_names_from_pipe(pipe):\n    transf=pipe.named_steps['cat_encod']\n    for step in transf.transformers_:\n        if type(step[1])== Pipeline :\n            if type(step[1][0])==OneHotEncoder:\n                oh_feat_names=step[2]\n                break\n    features=get_feature_names(transf)\n    count=0\n    for i,elt in enumerate(features):\n        if elt[0]=='x':\n            if int(elt[1])!=count :\n                count+=1\n            features[i]=one_hot_enc_variables[count]+elt[2:]\n    pca_pipe = pipe.named_steps.get('PCA_placeholder', 'passthrough')\n    if pca_pipe != 'passthrough':\n        features.extend(['PC_'+str(i+1) for i in pca_pipe.transformer_list[-1][-1][-1].get_support(indices=True)])\n    return features","c631cfb0":"search_spaces = [\n    {'clf': [SVClassifier(probability=True, random_state=42)],\n     'clf__C': [2.5, 1.0],\n     'clf__gamma': ['scale', 0.77],\n     'PCA_placeholder': [feature_maker],\n    }, \n    {'clf': [RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)],\n     'clf__n_estimators': [250, 500],\n     'clf__max_depth': [30, None], \n     'clf__max_features': [0.5, \"sqrt\"]\n    }, \n    {'clf': [ExtraTreesClassifier(n_jobs=-1, random_state=42)],\n     'clf__n_estimators': [250, 500],\n     'clf__min_samples_split': [2, 5],\n     'clf__max_depth': [30, None], \n     'clf__max_features': [0.5, \"sqrt\"]\n    }, \n    {'clf': [HistGradientBoostingClassifier(random_state=42)],\n     'clf__max_iter': [700, 2000],\n     'clf__learning_rate': [0.02, 0.12],\n     'clf__max_depth' : [30, None]\n    }\n]","c99baf5e":"## Light grid search to get an idea about the overall performance\n@lazy_run(\"..\/input\/bayes-search-res-list\/grid_search_res_list.joblib\")\ndef light_grid_search(pipeline, search_spaces, scoring, *, force_run=False):\n    confirm_execution(\"GridSearchCV\")\n    grid_res = [] #GridSearch results\n    for search_space in search_spaces:\n        model_name = type_name(search_space['clf'][0])\n        print(txt_style.BLUE+f\"\\nModel: {model_name}\"+txt_style.END)\n        t0 = perf_counter()\n        grid_search_cv = HalvingGridSearchCV(pipeline, search_space, scoring=scoring, \n                                             verbose=2, n_jobs=-1, random_state=42)\n        grid_res.append(grid_search_cv.fit(X_train, y_train))\n        print(txt_style.GREEN+txt_style.BOLD+f\"\\n{model_name} finished in {repr_time(perf_counter() - t0)}\\n\"+txt_style.END)\n    return grid_res","fa9d3a77":"grid_res = light_grid_search(pipeline, search_spaces, scoring=scorer)","f4b52810":"for df_res in grid_res:\n    res = pd.DataFrame(df_res.cv_results_).sort_values(\"mean_test_score\", ascending=False)\n    res['clf_type'] = res.param_clf.apply(type_name)\n    param_cols = [param for param in res.columns if 'param_clf__' in param or 'param_PCA_placeholder' in param]\n    metrics_cols = ['iter', 'n_resources', 'clf_type', 'mean_test_score', 'std_test_score', 'rank_test_score', 'mean_train_score', 'std_train_score']\n    display(res.sort_values(\"mean_test_score\", ascending=False).loc[:, param_cols+metrics_cols].head(5))","c73a6e43":"# from scipy import stats\n# def uniformint(values):\n#     return stats.rv_discrete(values=(values, np.repeat(1\/len(values), len(values))))\n\nbayes_search_dists = [\n    {'clf': [ExtraTreesClassifier(n_jobs=-1, random_state=42)],\n     'clf__n_estimators': Categorical([250, 500], name='n_estimators'),\n     'clf__max_depth': Categorical([30, 100, None], name='max_depth'), \n     'clf__min_samples_split': Integer(2, 20, name='min_samples_split'),\n     'clf__max_features': Real(0.3, 0.95, 'uniform', name='max_features')\n    }, \n    {'clf': [SVClassifier(probability=True, random_state=42)],\n     'clf__C': Real(1.1, 3.9, 'uniform', name='C'), \n     'clf__gamma': Real(0.05, 0.1, 'uniform', name='gamma'),\n     'PCA_placeholder': [feature_maker],\n    }, \n    {'clf': [RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)],\n     'clf__n_estimators': Categorical([250, 500], name='n_estimators'), \n     'clf__max_depth': Categorical([30, 100, None], name='max_depth'),\n     'clf__max_features': Real(0.3, 0.9, 'uniform', name='max_features')\n    }, \n    {'clf': [HistGradientBoostingClassifier(random_state=42)],\n     'clf__max_iter': Categorical([700, 1300, 2000], name='max_iter'),  \n     'clf__learning_rate': Real(1e-2, 0.3, 'uniform', name='learning_rate'), \n     'clf__max_depth' : Categorical([15, 50, 90, None], name='max_depth')\n    }, \n#     {'clf': [XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1, random_state=42, tree_method='hist')],\n#      'clf__learning_rate': Real(1e-3, 0.4, 'log-uniform', name='learning_rate'),\n#      'clf__n_estimators': Categorical([250, 500], name='n_estimators'),\n#      'clf__max_depth': Integer(2, 30, name='max_depth')\n#     }\n]","9b823ac2":"## bayes search to find the best possible parameter\n@lazy_run(\"..\/input\/bayes-search-res-list\/compressed_bayes_search_res_list.joblib\")\ndef full_bayes_search(pipeline, bayes_search_dists, scoring, *, force_run=False):\n    confirm_execution(\"BayesSearchCV\")\n    bayes_res = [] #BayesSearch results\n    for bayes_search_dist in bayes_search_dists:\n        model_name = type_name(bayes_search_dist['clf'][0])\n        print(txt_style.BLUE+f\"\\nModel: {model_name}\"+txt_style.END)\n        t0 = perf_counter()\n        bayes_search_cv = BayesSearchCV(pipeline, bayes_search_dist, scoring=scoring, n_jobs=-1, \n                                        verbose=1, n_iter=50, random_state=42, return_train_score=True)\n        bayes_res.append(bayes_search_cv.fit(X_train, y_train))\n        print(txt_style.GREEN+txt_style.BOLD+f\"\\n{model_name} finished in {repr_time(perf_counter() - t0)}\\n\"+txt_style.END)\n    return bayes_res","0a02822a":"bayes_res = full_bayes_search(pipeline, bayes_search_dists, scoring=scorer)","2451fff7":"bayes_res_dfs = []\nfor model_res in bayes_res:\n    res = pd.DataFrame(model_res.cv_results_).sort_values(\"mean_test_score\", ascending=False)\n    res['clf_type'] = res.param_clf.apply(type_name)\n    param_cols = [param for param in res.columns if 'param_clf__' in param]\n    metrics_cols = ['mean_test_score', 'std_test_score', 'rank_test_score', 'mean_train_score', 'std_train_score']\n    res = res.loc[:, ['clf_type']+param_cols+metrics_cols]\n    bayes_res_dfs.append(res)\n    display(res.head())","f29e53f6":"def res_convergence_plot(bayes_res_dfs):\n    \"\"\"plot convergence plots of the BayesSearchCV tunning of multiple models\n        bayes_res_dfs: list of DataFrames of cv_results_\n    \"\"\"\n    combines_res_df = pd.concat([df.copy().sort_index() for df in bayes_res_dfs])\n    combines_res_df['iteration'] = combines_res_df.index\n    cum_scores = combines_res_df.loc[:, ['iteration', 'mean_test_score', 'mean_train_score', 'clf_type']]\n    cum_scores['minimal loss'] = 1 - cum_scores.groupby('clf_type').mean_test_score.cummax()\n    rel = sns.lmplot(x=\"iteration\", y=\"mean_test_score\", col=\"clf_type\", hue=\"clf_type\", \n               data=combines_res_df, facet_kws=dict(sharey=False), height=3.7, aspect=1.1).set(xlabel=None)#.set(xticklabels=[])\n    rel.fig.suptitle('Score by iterations',y=1.05)\n    rel = sns.relplot(x=\"iteration\", y=\"minimal loss\", col=\"clf_type\", hue=\"clf_type\", style=\"clf_type\",\n                      data=cum_scores, facet_kws=dict(sharey=False), kind='line', dashes=False, markers=True, \n                      legend=False, height=3.7, aspect=1.1).set(title='')\n    rel.fig.suptitle('Minimum loss by iteration',y=1.05);","4f6f9f17":"res_convergence_plot(bayes_res_dfs)","31ed4de0":"is_clf_param = lambda param: 'clf__' in param\ndef res_hyperparams_plot(bayes_res):\n    \"\"\"\n    plot the partial dependence plots and histograms of the tried hyperparameters \n    of BayesSearchCV tunning of multiple models\n        bayes_res: list of fitted BayesSearchCV\n    \"\"\"\n    plot_figs = []\n    for model_res in bayes_res:\n        model_name = type_name(model_res.best_estimator_[-1])\n        eval_ax = plot_evaluations(model_res.optimizer_results_[0])\n        eval_ax[0,0].set_title(f'\\n    Evaluations plot of {model_name} parameters', loc='left', pad=15, fontsize=14)\n        eval_ax[0,0].figure.savefig('tmp_fig1.png', bbox_inches='tight')\n        fig1 = plt.imread('tmp_fig1.png')\n        obj_ax = plot_objective(model_res.optimizer_results_[0], n_minimum_search=int(1e8))\n        obj_ax[0,0].set_title(f'\\n    Partial Dependence plot of {model_name} parameters', loc='left', pad=15, fontsize=14)\n        obj_ax[0,0].figure.savefig('tmp_fig2.png', bbox_inches='tight')\n        fig2 = plt.imread('tmp_fig2.png')\n        plot_figs.append((fig1, fig2))\n    os.remove('tmp_fig1.png')\n    os.remove('tmp_fig2.png')\n    return plot_figs","df0ee0fd":"%%capture\nres_hyperparams = res_hyperparams_plot(bayes_res)","2b7516f0":"for fig1, fig2 in res_hyperparams:\n    dpi = 60\n    height, width, depth = fig2.shape\n    figsize = 2*width \/ dpi, height \/ dpi\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n    ax1.imshow(fig1)\n    ax1.grid()\n    ax1.axis('off')\n    ax2.imshow(fig2)\n    ax2.grid()\n    ax2.axis('off')\n    plt.show()","c377634b":"from sklearn.preprocessing import minmax_scale\n\ndef hacky_parallel_coordinates(df, ax, col, title, prefix='param_clf__'):\n    df.sort_values(col)\n    df[col+'_copy'] = df[col]\n    l, u =  df[col].min(), df[col].max()\n    df.iloc[:, :-1] = pd.DataFrame(minmax_scale(df.iloc[:, :-1].astype(float), feature_range=(l, u)),\n                                   columns=df.iloc[:, :-1].columns)\n    df.drop(col, axis=1, inplace=True)\n    df[col+'_copy2'] = df[col+'_copy']\n    fixed_cols = [c.replace(prefix, '') for c in df.columns[:-2]]+[col]\n    pd.plotting.parallel_coordinates(df, col+'_copy', ax=ax, sort_labels=False, \n                                     colormap=sns.color_palette(\"coolwarm\", as_cmap=True), lw=1)\n    ax.set(xticklabels=fixed_cols)\n    ax.yaxis.set_label_position(\"right\")\n    ax.yaxis.tick_right()\n    ax.yaxis.set_major_locator(plt.MaxNLocator(6))\n    ax.set_title(title, pad=12)\n    ax.get_legend().remove()","fea3a2d7":"fig, axs = plt.subplots(2, 2, figsize=(17,8))\nfor ax, bayes_res_df in zip(axs.flat, bayes_res_dfs):\n    bayes_df = bayes_res_df.sort_values('mean_test_score')\n    bayes_df['iteration'] = bayes_df.index\n    param_cols = [param for param in bayes_res_df.columns if 'param_clf__' in param]\n    plot_scores = bayes_df.loc[:, ['iteration']+param_cols+['mean_test_score']]\n    if 'param_clf__max_depth' in plot_scores:\n        plot_scores.param_clf__max_depth = plot_scores.param_clf__max_depth.fillna(300)    \n    hacky_parallel_coordinates(plot_scores, ax, col='mean_test_score', title=bayes_res_df['clf_type'][0])\nplt.tight_layout()\nplt.show()","f4b64ddc":"from io import BytesIO\n\ndef display_plots_images(img_bytes_dict):\n    dpi = 80\n    img_dict = {}\n    for model_name, img_bytes in img_bytes_dict.items():\n        img_dict[model_name] = plt.imread(BytesIO(img_bytes))[10:350, 50:780, :]\n    fig_len = ceil(len(img_dict)\/2)\n    height, width, depth = list(img_dict.values())[0].shape\n    figsize = 1.4*fig_len*width \/ dpi, 1.4*fig_len*height \/ dpi\n    fig, axs = plt.subplots(fig_len, 2, figsize=figsize)\n    for ax, model_name, img_data in zip(axs.flat, img_dict.keys(), img_dict.values()):\n        ax.imshow(img_data, cmap='gray')\n        ax.set_title(model_name, pad=10, fontsize=14)\n        ax.axis('off')\n    fig.subplots_adjust(wspace=-0.13, hspace=0.15)\n    return fig\n\ndef res_parallel_coordinates(bayes_res_dfs):\n    \"\"\"\n    plot parallel coordinates plots the tried hyperparameters \n    of the BayesSearchCV tunning of multiple models.\n        bayes_res_dfs: list of DataFrames of cv_results_\n    \"\"\"\n    plot_imgs = {}\n    for bayes_res_df in bayes_res_dfs:\n        model_name = bayes_res_df.clf_type[0]\n        bayes_df = bayes_res_df.sort_values('mean_test_score')\n        bayes_df['iteration'] = bayes_df.index\n        param_cols = [param for param in bayes_res_df.columns if 'param_clf__' in param]\n        plot_scores = bayes_df.loc[:, ['iteration']+param_cols+['mean_test_score']]\n        plot_scores = plot_scores.astype(float)\n        if 'param_clf__max_depth' in plot_scores:\n            plot_scores.param_clf__max_depth = plot_scores.param_clf__max_depth.fillna(300)\n        fixed_cols = {col: col.replace('param_clf__', '') for col in plot_scores.columns}\n        fig = px.parallel_coordinates(plot_scores, color=\"mean_test_score\", labels=fixed_cols, #title=model_name,\n                                      color_continuous_scale=['rgb'+str(cm.coolwarm(i, bytes=True)[:-1]) for i in range(0,255,127)], \n                                      width=800, height=400,\n                                      color_continuous_midpoint=plot_scores.mean_test_score.quantile(0.25)\n                                     )\n        img_bytes = fig.to_image(format=\"png\")\n        plot_imgs[model_name] = img_bytes\n    return display_plots_images(plot_imgs)","ce15ca07":"res_parallel_coordinates(bayes_res_dfs)\nplt.show()","cb5f678f":"from sklearn.preprocessing import normalize\nfrom pandas.plotting import table\n\ndef df_from_clf_report(clf_report, target_names):\n    clf_report['avgerage'] = {'f1': clf_report.pop('avg_f1'), 'geo': clf_report.pop('avg_geo'), \n                              'iba': clf_report.pop('avg_iba'), 'pre': clf_report.pop('avg_pre'), \n                              'rec': clf_report.pop('avg_rec'), 'spe': clf_report.pop('avg_spe'), \n                              'sup': clf_report.pop('total_support')}\n    df_report = pd.DataFrame.from_dict(clf_report, orient='index').drop('spe', axis=1)\n    df_report.index = target_names + ['weighted avgerage']\n    ref = {'f1': 'f1-score', 'geo': 'geometric\\nmean', 'iba': 'balanced\\naccuracy', \n           'pre': 'precision', 'rec': 'recall', 'sup': 'support'}\n    df_report.columns = df_report.columns.map(ref)\n    df_report.loc['weighted avgerage', 'support']\n    return df_report\n\ndef classifier_performance_plot(classifier):\n    model_name = type_name(classifier[-1])\n    fig, axs = plt.subplots(2, 3, figsize=(17,8.8))\n    #Confusion matrix\n    y_pred = classifier.predict(X_test)\n    cnf_mat = plot_confusion_matrix(classifier, X_test, y_test, labels=[0,1], #normalize='true', \n                                    display_labels=target_labels, ax=axs[0,0], cmap='Blues')\n    normalized_cnf = normalize(cnf_mat.confusion_matrix, norm='l1', axis=1).round(4)\n    colors_rgb = [(234, 240, 247), (28, 65, 120), (28, 65, 120), (28, 65, 120)]\n    positions = [(-0.2, 0.17), (0.8, 0.17), (-0.2, 1.17), (0.8, 1.17)]\n    for col, cord, pos in zip(colors_rgb, positions, [(0,0), (0,1), (1,0), (1,1)]):\n        axs[0,0].text(*cord, f\"({normalized_cnf[pos]})\".rjust(8), fontsize=10, c=np.array(col)\/255)\n    axs[0,0].set_title(\"Confusion Matrix\", pad=13)\n    axs[0,0].grid(False)\n    #Class prediction error\n    cnf_mat_df = pd.DataFrame(cnf_mat.confusion_matrix, columns=target_labels, index=target_labels)\n    cnf_mat_df.plot.bar(stacked=True, rot=False, \n                        xlabel=\"actual class\", ylabel=\"number of predicted class\", ax=axs[0,1])\n    axs[0,1].set_title(\"Class prediction error for \"+model_name, pad=13)\n    axs[0,1].grid(axis='x')\n    #ROC curve\n    roc = plot_roc_curve(classifier, X_test, y_test, name=\"ROC of class \"+target_labels[1], ax=axs[0,2], pos_label=1)\n    roc = plot_roc_curve(classifier, X_test, y_test, name=\"ROC of class \"+target_labels[0], ax=axs[0,2], pos_label=0)\n    axs[0,2].set_title(\"Receiver operating characteristic (ROC) curve\", pad=13)\n    axs[0,2].plot([0,1],[0,1], 'r--', label=\"Random guess\")\n    axs[0,2].set_xlabel(axs[0,2].get_xlabel().split(' (', 1)[0])\n    axs[0,2].set_ylabel(axs[0,2].get_ylabel().split(' (', 1)[0])\n    axs[0,2].legend(loc='lower right')\n    #Precision-Recall curve\n    disp = plot_precision_recall_curve(classifier, X_test, y_test, name=model_name, ax=axs[1,2])\n    disp.ax_.fill_between(disp.line_._x, disp.line_._y, color=\"steelblue\", alpha=0.15)\n    disp.ax_.axhline(y=disp.average_precision, color='r', linestyle='--', label=\"Average Precision\")\n    axs[1,2].legend(loc='lower right')\n    axs[1,2].set_title(\"Precision-Recall curve\", pad=13)\n    #Classification report\n    y_pred = classifier.predict(X_test)\n    clf_report = classification_report_imbalanced(y_test, y_pred, target_names=target_labels, output_dict=True)\n    for ax in axs[1,:-1]: ax.remove()\n    ax = fig.add_subplot(2,3, (4,5))\n#     acc = clf_rapport.pop('accuracy')\n    df_report = df_from_clf_report(clf_report, target_labels)\n    df_report.loc['weighted avgerage', 'support'] = 'total:   '+str(df_report.loc['weighted avgerage', 'support'])\n#     df_rapport = pd.DataFrame.from_dict(clf_rapport, orient='index')\n    tb_report = table(ax, df_report.round(4).astype(str), loc=\"center right\", colWidths=[0.048]*7, \n                      rowColours=['whitesmoke']*df_report.shape[0], colColours=['whitesmoke']*df_report.shape[1])\n    tb_report.set_fontsize(12)\n    tb_report.scale(3, 4)\n#     ax.text(-0.02, 0.83, f\"accuracy = {round(acc,4)}\", fontsize=12)\n    ax.axis('off')\n    fig.suptitle(f\"\\n\\nModel: {model_name}\\n\", fontsize=13, color=\"royalblue\", \n                 family='monospace', y=0.95, x=0.04, ha='left')\n    return fig\n\nfrom copy import deepcopy\ndef calibrate_pipe(pipe):\n    pipe_copy = deepcopy(pipe)\n    pipe_copy.steps.append(['clf', CalibratedClassifierCV(pipe_copy.steps.pop(-1)[1], method='isotonic')])\n    return pipe_copy\n\ndef res_classifier_performance(bayes_res, calibrated=False):\n    \"\"\"\n    plot classifier performance plots of best_estimator_ of BayesSearchCV tunning of multiple models\n        bayes_res: list of fitted BayesSearchCV\n    \"\"\"\n    for model_res in bayes_res:\n        best_model = model_res.best_estimator_\n        \n        if calibrated:\n            best_model = calibrate_pipe(best_model)\n            best_model.fit(X_train, y_train)\n        classifier_performance_plot(best_model)\n        plt.tight_layout()\n        plt.show()","f2cb014c":"%%time\nres_classifier_performance(bayes_res)","1ee115e9":"def res_discrimination_threshold(bayes_res, n_trials=10, calibrated=False):\n    \"\"\"\n    plot discrimination_threshold plots of best_estimator_ of BayesSearchCV tunning of multiple models\n        bayes_res: list of fitted BayesSearchCV\n    \"\"\"\n    fig, axs = plt.subplots(1, len(bayes_res), figsize=(len(bayes_res)*5.5,5))\n    if type(axs)!= np.ndarray: axs = [axs]\n    for ax, model_res in zip(axs, bayes_res):\n        best_model = model_res.best_estimator_\n        model_name = type_name(best_model[-1])\n        if calibrated:\n            best_model = calibrate_pipe(best_model)\n        disp = discrimination_threshold(best_model, X_test, y_test, is_fitted=True, n_trials=n_trials,\n                                        random_state=42, ax=ax, show=False, title=f\"Threshold Plot for {model_name}\")\n    return fig","e0a6f28e":"%%time\nres_discrimination_threshold(bayes_res, n_trials=10)\nplt.show()","7b15cadb":"def res_calibration_curve(bayes_res, n_bins=10, calibrated=False, axs=None):\n    \"\"\"\n    plot calibration_curve plots of best_estimator_ of BayesSearchCV tunning of multiple models\n        bayes_res: list of fitted BayesSearchCV\n    \"\"\"\n    if axs is None:\n        fig, axs = plt.subplots(1, len(bayes_res)+1, figsize=(len(bayes_res)*5.5,8))\n        axs = axs.flatten()\n    l = len(axs)\/\/2\n    calib = {True: 'calibrated', False: 'uncalibrated'}\n    prob_list = []\n    for ax, pax, model_res in zip(axs[:l], axs[l:], bayes_res):\n        best_model = model_res.best_estimator_\n        model_name = type_name(best_model[-1])\n        if calibrated:\n            best_model = calibrate_pipe(best_model)\n            best_model.fit(X_train, y_train)\n        try:\n            probs = best_model.predict_proba(X_test)[:,1]\n        except AttributeError:\n            probs = best_model.decision_function(X_test)\n        prob_list.append(probs)\n        pax.hist(probs, range=(0, 1), bins=n_bins, label=f\"{calib[calibrated]} {model_name} probabilities\",\n                 histtype=\"step\", lw=2)\n        pax.set_title(\"Predicted probabilities dictribution\", pad=10)\n        pax.legend()\n        fop, mpv = calibration_curve(y_test, probs, n_bins=n_bins, normalize=True)\n        ax.plot([0, 1], [0, 1], linestyle='--', label=\"perfectly calibrated\")\n        ax.plot(mpv, fop, marker='.', label=\"calibration curve\")\n        ax.set_title(f\"{calib[calibrated]} {model_name}\", pad=10)\n        ax.set_xlabel(\"relative observed frequency\")\n        ax.set_ylabel(\"predicted probability frequency\")\n        ax.legend()","fc8fa7b9":"imb_search_spaces = [\n    {'clf': [BalancedRandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)],\n     'clf__n_estimators': [250, 500],\n     'clf__max_depth': [30, None], \n     'clf__max_features': [0.5, \"sqrt\"],\n     'clf__sampling_strategy': ['auto', 'not majority']\n    }, \n    {'clf': [RUSBoostClassifier(algorithm='SAMME.R', random_state=42)],\n     'clf__n_estimators': [250, 500],\n     'clf__learning_rate': [0.1, 0.3, 0.5, 0.7, 0.9],\n     'clf__sampling_strategy': ['auto', 'not majority']\n    }\n]\nimb_pipeline = Pipeline(\n    steps=pipeline.steps[:1] + pipeline.steps[2:]) #exclude resampling\n# imb_pipeline","7cb5e9db":"## Light grid search to get an idea about the overall performance of imb models\n@lazy_run(\"..\/input\/bayes-search-res-list\/imb_grid_search_res_list.joblib\")\ndef imb_grid_search(imb_pipeline, imb_search_spaces, scoring, *, force_run=False):\n    confirm_execution(\"GridSearchCV\")\n    imb_grid_res = [] #GridSearch results\n    for imb_search_space in imb_search_spaces:\n        model_name = type_name(imb_search_space['clf'][0])\n        print(txt_style.BLUE+f\"\\nModel: {model_name}\"+txt_style.END)\n        t0 = perf_counter()\n        grid_search_cv = HalvingGridSearchCV(imb_pipeline, imb_search_space, scoring=scorer, \n                                             verbose=2, n_jobs=-1, random_state=42)\n        imb_grid_res.append(grid_search_cv.fit(X_train, y_train))\n        print(txt_style.GREEN+txt_style.BOLD+f\"\\n{model_name} finished in {repr_time(perf_counter() - t0)}\\n\"+txt_style.END)\n    return imb_grid_res","81523bf6":"imb_grid_res = imb_grid_search(imb_pipeline, imb_search_spaces, scorer)","07938950":"for df_res in imb_grid_res:\n    res = pd.DataFrame(df_res.cv_results_).sort_values(\"mean_test_score\", ascending=False)\n    res['clf_type'] = res.param_clf.apply(type_name)\n    param_cols = [param for param in res.columns if 'param_clf__' in param or 'param_PCA_placeholder' in param]\n    metrics_cols = ['iter', 'n_resources', 'clf_type', 'mean_test_score', 'std_test_score', 'rank_test_score', 'mean_train_score', 'std_train_score']\n    display(res.sort_values(\"mean_test_score\", ascending=False).loc[:, param_cols+metrics_cols].head())","88072728":"imb_search_bayes_dists = [\n    {'clf': [BalancedRandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)],\n     'clf__n_estimators': Categorical([250, 500]),\n     'clf__max_depth': Categorical([30, 100, None]), \n     'clf__min_samples_split': Integer(2, 20),\n     'clf__max_features': Real(0.3, 0.95, 'uniform'),\n     'clf__sampling_strategy': Categorical(['auto', 'not majority'])\n    },\n    {'clf': [RUSBoostClassifier(algorithm='SAMME.R', random_state=42)],\n     'clf__n_estimators': Categorical([250, 500]), \n     'clf__learning_rate': Real(1e-2, 0.9, 'uniform'), \n     'clf__sampling_strategy': Categorical(['auto', 'not majority'])\n    }\n]","f2e949db":"## bayes search for imbalanced models to find the best possible parameters\n@lazy_run(\"..\/input\/bayes-search-res-list\/imb_bayes_search_res_list.joblib\")\ndef imb_bayes_search(imb_pipeline, imb_search_bayes_dists, scoring, *, force_run=False):\n    confirm_execution(\"BayesSearchCV\")\n    imb_bayes_res = [] #BayesSearch results\n    for imb_search_bayes_dist in imb_search_bayes_dists:\n        model_name = type_name(imb_search_bayes_dist['clf'][0])\n        print(txt_style.BLUE+f\"\\nModel: {model_name}\"+txt_style.END)\n        t0 = perf_counter()\n        bayes_search_cv = BayesSearchCV(imb_pipeline, imb_search_bayes_dist, scoring=scorer, n_jobs=-1, \n                                        verbose=1, n_iter=50, random_state=42, return_train_score=True)\n        imb_bayes_res.append(bayes_search_cv.fit(X_train, y_train))\n        print(txt_style.GREEN+txt_style.BOLD+f\"\\n{model_name} finished in {repr_time(perf_counter() - t0)}\\n\"+txt_style.END)\n    return imb_bayes_res","005f6b20":"imb_bayes_res = imb_bayes_search(imb_pipeline, imb_search_bayes_dists, scorer)","25a9244f":"imb_bayes_res_dfs = []\nfor imb_model_res in imb_bayes_res:\n    res = pd.DataFrame(imb_model_res.cv_results_).sort_values(\"mean_test_score\", ascending=False)\n    res['clf_type'] = res.param_clf.apply(type_name)\n    param_cols = [param for param in res.columns if 'param_clf__' in param]\n    metrics_cols = ['mean_test_score', 'std_test_score', 'rank_test_score', 'mean_train_score', 'std_train_score']\n    res = res.loc[:, ['clf_type']+param_cols+metrics_cols]\n    imb_bayes_res_dfs.append(res)\n    display(res.head())","a14d863a":"%%time\nres_classifier_performance(imb_bayes_res, calibrated=True)","30cc08d5":"%%time\nres_discrimination_threshold(imb_bayes_res, n_trials=10, calibrated=True)\nplt.show()","79ae36a5":"%%time\nfig, axs = plt.subplots(2, len(imb_bayes_res)+1, figsize=(20, len(imb_bayes_res)*4), sharey=False)\nres_calibration_curve(imb_bayes_res, n_bins=8, calibrated=False, axs=[axs[0,0], axs[1,0], axs[0,-1], axs[1,-1]])\nres_calibration_curve(imb_bayes_res, n_bins=8, calibrated=True, axs=[axs[0,1], axs[1,1], axs[0,-1], axs[1,-1]])\nplt.tight_layout()\nplt.show()","875eede2":"def res_permutation_importance(bayes_res, n_repeats=10, calibrated=False):\n    \"\"\"\n    plot permutation_importance plots of best_estimator_ of BayesSearchCV tunning of multiple models\n        bayes_res: list of fitted BayesSearchCV\n    \"\"\"\n    n_models = len(bayes_res)\n    if type(calibrated) is bool:\n        calibrated = [calibrated]*n_models\n    n_rows = ceil(n_models\/2)\n    fig, axs = plt.subplots(n_rows, 2, figsize=(15, 4.2*n_rows))\n    if type(axs) != np.ndarray:\n        axs = [axs]\n    for i, ax, model_res in zip(range(n_models), axs.flat, bayes_res):\n        best_model = model_res.best_estimator_\n        model_name = type_name(best_model[-1])\n        if calibrated[i]:\n            best_model = calibrate_pipe(best_model)\n            best_model.fit(X_train, y_train)\n        result = permutation_importance(best_model, X_test, y_test, \n                                        n_repeats=n_repeats, random_state=42, n_jobs=-1)\n        sorted_idx = result.importances_mean.argsort()[::-1]\n        sns.boxplot(data=pd.DataFrame(result.importances[sorted_idx].T, columns=X_test.columns[sorted_idx]), \n                    orient=\"h\", palette=\"coolwarm\", ax=ax).set_title(model_name)\n        ax.grid(axis='y')\n    fig.tight_layout()\n    return fig","5ef59ffa":"%%time\nres_permutation_importance(bayes_res+imb_bayes_res, n_repeats=10, calibrated=[False]*4+[True]*2)\nplt.show()","9f068bb4":"from scipy.stats import spearmanr\nfrom scipy.cluster import hierarchy\nfrom scipy.spatial.distance import squareform\nfig, axs = plt.subplots(1, 2, figsize=(17, 8))\ncorr = spearmanr(X_train).correlation\n# Ensure the correlation matrix is symmetric\ncorr = (corr + corr.T)\/2\nnp.fill_diagonal(corr, 1)\n# We convert the correlation matrix to a distance matrix before performing\n# hierarchical clustering using Ward's linkage.\ndistance_matrix = 1 - np.abs(corr)\ndist_linkage = hierarchy.ward(squareform(distance_matrix))\ndendro = hierarchy.dendrogram(dist_linkage, labels=X_train.columns,\n                              ax=axs[1], leaf_rotation=90, show_contracted=True)\naxs[1].tick_params(axis='x', which='major', labelsize=10)\ndendro_idx = np.arange(0, len(dendro['ivl']))\nsns.heatmap(pd.DataFrame(corr[dendro['leaves'], :][:, dendro['leaves']], columns=dendro['ivl'], index=dendro['ivl']).round(2),\n            annot=True, ax=axs[0], cmap='coolwarm', annot_kws={'size': 8})\nfig.tight_layout()\nplt.show()","5d8cc40e":"def res_partial_dependence(bayes_res, kind='average', subsample=0.1, grid_resolution=10, n_cols=3, calibrated=False):\n    \"\"\"\n    plot permutation_importance plots of best_estimator_ of BayesSearchCV tunning of multiple models\n        bayes_res: list of fitted BayesSearchCV\n    \"\"\"\n    n_models = len(bayes_res)\n    if type(calibrated) is bool:\n        calibrated = [calibrated]*n_models\n    n_rows = ceil(n_models\/2)\n    fig, axs = plt.subplots(n_rows, min(2, n_models), figsize=(15, 8*n_rows))\n    if type(axs) != np.ndarray:\n        axs = np.array([axs])\n    for i, ax, model_res in zip(range(n_models), axs.flat, bayes_res):\n        best_model = model_res.best_estimator_\n        model_name = type_name(best_model[-1])\n        if calibrated[i]:\n            best_model = calibrate_pipe(best_model)\n            best_model.fit(X_train, y_train)\n        best_model = Pipeline([step for step in best_model.steps if step[-1]!='passthrough'])  # exclude 'passthrough'\n        disp = plot_partial_dependence(\n            best_model, X_train, features=X_train.select_dtypes(exclude=['category', 'object']).columns, \n            n_jobs=-1, kind=kind, n_cols=n_cols, ax=ax, subsample=subsample, grid_resolution=grid_resolution, random_state=42)\n        disp.figure_.subplots_adjust(wspace=0.15, hspace=0.4)\n        disp.figure_.tight_layout()\n        ax.set_title(f\"{model_name} partial_dependence\", pad=10)\n    return fig","bff5ad65":"%%time\nres_partial_dependence(bayes_res[-1:]+imb_bayes_res[:1], kind='both', subsample=0.05, \n                       grid_resolution=30, calibrated=[False, True], n_cols=4)\nplt.show()","fce75eb7":"import shap  # package used to calculate Shap values\nshap.initjs()\ndef res_churn_factors(bayes_res, customer_index=0, outcome=-1, calibrated=False):\n    \"\"\"\n    plot the shap values plots of best_estimator_ of BayesSearchCV tunning of multiple models\n        bayes_res: list of fitted BayesSearchCV\n        outcome : 0 for showing shap values of negative outcome, -1 for negative outcome\n    \"\"\"\n    approx = False\n    n_models = len(bayes_res)\n    if type(calibrated) is bool:\n        calibrated = [calibrated]*n_models\n    for i, model_res in enumerate(bayes_res):\n        best_model = model_res.best_estimator_\n        model_name = type_name(best_model[-1])\n        feature_names = get_names_from_pipe(best_model)\n        if calibrated[i]:\n            best_model = calibrate_pipe(best_model)\n            best_model.fit(X_train, y_train)\n        best_model = Pipeline([step for step in best_model.steps if step[-1]!='passthrough'])  # exclude 'passthrough'\n        data_for_prediction = pd.Series(\n            best_model[:-1].transform(X_test.iloc[customer_index:customer_index+1])[0], \n            index= feature_names).astype(float)\n        try:\n            explainer = shap.TreeExplainer(best_model[-1])\n        except Exception as e:\n            if \"Model type not yet supported\" in str(e):\n                approx = True\n                data_for_prediction_ = pd.DataFrame(\n                    best_model[:-1].transform(X_train.iloc[:400]), \n                    columns= feature_names).astype(float)\n                explainer = shap.KernelExplainer(best_model[-1].predict_proba, data_for_prediction_)\n            else: \n                raise\n        shap_values = explainer.shap_values(data_for_prediction) #data_for_prediction\n        if type(shap_values) is not list:\n            shap_values = [shap_values]\n        print(f\"Churn factors {'(approximated)' if approx else ''} found by {model_name} for customer #{X_test.index[customer_index]} :\\n\")\n        display(shap.force_plot(explainer.expected_value[outcome], shap_values[outcome], data_for_prediction))","3c7c57ee":"%%time\nres_churn_factors(bayes_res[:1]+imb_bayes_res[:1], customer_index=100, outcome=-1, calibrated=[False, True])\nres_churn_factors(bayes_res[:1]+imb_bayes_res[:1], customer_index=200, outcome=-1, calibrated=[False, True])","662c0bba":"def plot_learning_curve(estimator, title, X, y, scoring=None, axes=None, ylim=None, cv=None,\n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate 3 plots: the test and training learning curve, the training\n    samples vs fit times curve, the fit times vs score curve.\n    more details on scikit-learn.org\/0.24\/auto_examples\/model_selection\/plot_learning_curve.html\n    \"\"\"\n    if axes is None:\n        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n        estimator, X, y, scoring=scoring, cv=cv, n_jobs=n_jobs, \n        train_sizes=train_sizes, return_times=True, random_state=42)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n    # Plot learning curve\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std, \n                         train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n    # Plot n_samples vs fit_times\n    axes[1].plot(train_sizes[:-1], fit_times_mean[:-1], 'o-')\n    axes[1].fill_between(train_sizes[:-1], fit_times_mean[:-1] - fit_times_std[:-1],\n                         fit_times_mean[:-1] + fit_times_std[:-1], alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n    # Plot fit_time vs score\n    axes[2].plot(fit_times_mean[:-1], test_scores_mean[:-1], 'o-')\n    axes[2].fill_between(fit_times_mean[:-1], test_scores_mean[:-1] - test_scores_std[:-1],\n                         test_scores_mean[:-1] + test_scores_std[:-1], alpha=0.1)\n    axes[2].set_xlabel(\"fit_times\")\n    axes[2].set_ylabel(\"Score\")\n    axes[2].set_title(\"Performance of the model\")\n    return plt\n\ndef res_learning_curves(bayes_res, calibrated=False):\n    \"\"\"\n    plot permutation_importance plots of best_estimator_ of BayesSearchCV tunning of multiple models\n        bayes_res: list of fitted BayesSearchCV\n    \"\"\"\n    n_models = len(bayes_res)\n    if type(calibrated) is bool:\n        calibrated = [calibrated]*n_models\n    n_rows = n_models\n    fig, axs = plt.subplots(n_rows, 3, figsize=(15, 5*n_rows))\n    for i, ax, model_res in zip(range(n_models), axs.reshape(-1, 3), bayes_res):\n        best_model = model_res.best_estimator_\n        model_name = type_name(best_model[-1])\n        if calibrated[i]:\n            best_model = calibrate_pipe(best_model)\n            best_model.fit(X_train, y_train)\n        best_model = Pipeline([step for step in best_model.steps if step[-1]!='passthrough'])  # exclude 'passthrough'\n        plot_learning_curve(best_model, model_name, X_train, y_train, scoring=scorer,\n                            axes=ax, ylim=(0.75, 1.01), cv=5, n_jobs=-1)\n    fig.tight_layout()\n    return fig","a0e95160":"%%time\nres_learning_curves(bayes_res[-1:]+imb_bayes_res[:1], calibrated=[False, True])\nplt.show()","8e14ca5b":"if not submitting:\n    sleep(21600) #wait for 6h of inactivity before stoping (instead of the default 20min)","8009358f":"## Understand individual predictions","a203bbee":"The better calibrated or more reliable a forecast, the closer the points will appear along the main diagonal from the bottom left to the top right of the plot.\n\nThe position of the points or the curve relative to the diagonal can help to interpret the probabilities; for example:\n\n* Below the diagonal: The model has over-forecast; the probabilities are too large.\n* Above the diagonal: The model has under-forecast; the probabilities are too small.\n\n`RUSBoostClassifier` benefits the most from calibration, as the probabilities of the uncalibrated classifier show a large bias, the histograms show that the probabilities are centred arround 0.5, and as a result, the calibration curve shows a characteristic sigmoid shape (or S-shaped), this means that the model is over-forecasting low probabilities and under-forecasting high probabilities.","68813f7d":"The goal of this section is to:\n* Try multiple models with cross validation to get an estimate of their performance on unseen data, and identifying the most promising models that we could keep investigating.\n* For each model find the best combination of hyper-parameters and preprocessing steps that maximize our scoring metric, and ultimately leads to the optimal performance.","73dca959":"### Interactive exploration of univariate propreties","a0ed8352":"As we have seen after importing the data, it has a mixed types variables, i.e categorical and numeric variables.  \nSo we'll split them and analyse each type with the appropiate methods.","3b9c1a36":"Next, to understand how the parameters were sampled from the input dimension, we will use two plots:  \n* **Evaluations plot:** this helps with visualizing the location and order in which samples are evaluated. The diagonal shows histograms for each of the dimensions. In the lower triangle a two dimensional scatter plot of all points is shown. The order in which points were evaluated is encoded in the color of each point. Darker\/purple colors correspond to earlier samples and lighter\/yellow colors correspond to later samples. A red point shows the location of the minimum found by the optimization process.\n* **Partial dependence plot:** this shows the partial dependence of the loss function, as represented by the surrogate model, for each dimension and as pairs of the input dimensions. Partial dependence plots are a method for interpreting the importance of input features used in gradient boosting machines. The idea is to visualize how the value of each variable influences the function after averaging out the influence of all other variables.","80e2591c":"Now to find the best possible combination of hyperparameters we will use the bayesian optimisation to model the search space and arrive at optimal parameter values combination as soon as possible.\n\nThis would require us to input the search space as dimensions instead of lists of values.","fc93f855":"### Initial grid search","878bac06":"### Chaining data transformation and modeling steps: Pipeline\n\nPipelines are a simple way to keep your data preprocessing and modeling code organized. Specifically, a pipeline bundles preprocessing and modeling steps so you can use the whole bundle as if it were a single step.\n\nPipeline serves multiple purposes and have some important benefits. Those include:\n\n* **Cleaner Code and Convenience:** Accounting for data at each step of preprocessing can get messy. With a pipeline, you won't need to manually keep track of your training and validation data at each step. instead, you only have to call fit and predict once on your data to fit a whole sequence of estimators.\n* **Fewer Bugs:** There are fewer opportunities to misapply a step or forget a preprocessing step.\n* **Avoid Data Leakage:** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n* **Joint Parameter Selection:** You can grid search over parameters of all estimators in the pipeline at once.\n\nIn our pipeline, we chained the preprocessing steps as follows:  \n1. Categorical features encoding, \n2. Resampling using SMOTE and Tomek links, \n3. a placeholder for PCA, so we can apply it only when it improves the model,\n4. Robust scaling: removing the median and scaling the data according to the Interquartile Range, this is the same as the usual Standardization (with the mean and standard deviation) i.e it tries to get the features in the same order of magnitude so they don't dominate each others while training the models, except that it doesn't get influenced by outliers,\n5. a placeholder for models, this is to allow us trying various models with the same pipeline (later explained in model selection section)","851e0ff8":"Good news, the data dosen't contain any missing values!","094a3902":"### Validation of the predicted probabilities","0da3b7d5":"In certain domains, a model needs a certain level of interpretability before it can be deployed. A model that is exhibiting performance issues needs to be debugged for one to understand the model\u2019s underlying issue. \n\nBecause model interpretability is one of the high-priority issues in the field of banking, in this section we will try to understand the predictions from a model and what affects them. This can be used to evaluate assumptions and biases of a model, design a better model, or to diagnose issues with model performance.","d4bc0d3f":"So far, the only score that we looked at is the cross validation score, but we can't rely on it to estimate the model generalization error, since the hypermarameters were choosed to minimise the CV score, which may introduce some overfitting.  \nFor this reason, we will consider only the performance metrics on the test set (unseen data). ","98e753d0":"### Numeric variables","952c7b9f":"The following interaction report can quickly answer the question \"*which features have direct impact on customer churn? and how they can help us on churn prediction?*\"  \n\n*Note:* Stacked Bar Chart can show the actual difference in distribution of categorical variables, rather than seing by differences in the counts of each category. **But** they are only reliable when there is enough observations on each category to make conclusions. ","8590b5f9":"Classification predictive modeling typically involves predicting a class label. Nevertheless, many machine learning algorithms are capable of predicting a probability or scoring of class membership, it can be mapped to a crisp class label and also gives you some kind of confidence on the prediction.\n\nPredicted probabilities can be improved in two ways;\n\n* Tuning the Classification Threshold, explained bellow.\n* Calibrating Probabilities, this will be used in a later section with another type of models.","4184e1dd":"## Evaluation of models performance","57f8e9b5":"We can see that indeed, a lot of features are strogly related, which confirms the multicollinearity issue.\n\nFrom the dendogram it seems that a threshold of 0.5 on the dendogram should be enough to fix the multicollinearity without losing too much of important features.  \nif we were to apply this, it would means to drop those features:  \n`Avg_Utilization_Ratio`, `Gender`, `Avg_Open_To_Buy`, `Customer_Age` and `Total_Trans_Amt`  ","0bb95393":"### Interactions between categorical variables","8e8baca8":"#### Feature extraction\nFor feature extraction purposes, we will use PCA (Principal component analysis) to decompose the dataset in a set of successive orthogonal components that explain a maximum amount of the variance, feeding those components to the model as additional variables can, in some cases, reveal additional information that the model was not able to detect before.  \nTo select the most useful components for prediction, first we will make a Meta-estimator that is fitted to all the features, and then selecting the  most important ones according to the estimation coefficients.","c741e45b":"### Optimal hyperparameters by bayesian optimisation","5ae685b1":"After we get a general idea about how our data looks like, here we can dive deeper in the statistical proprities of each variable, and thanks to this interactive exploration, we can look exactly into what we're intrested in or curious about.","8905aef3":"For small amounts of data, the training score of both models is much greater than the validation score. Adding more training samples will most likely increase generalization.\n\nThese plots show also that `HistGradientBoostingClassifier` is not only better in terms of predictive performance, but also in terms of computational cost, as we can see that he is about 3 times faster than the `BalancedRandomForestClassifier`, thus if we can get more data we don't have to worry about the efficiency of our model.","f7549c50":"#### Calibrating Probabilities\nSome models can give you poor estimates of the class probabilities and some even do not support probability prediction. The sklearn's calibration module allows you to better calibrate the probabilities of a given model, or to add support for probability prediction.\n\n**Well calibrated** classifiers are probabilistic classifiers for which the output of the predict_proba method can be directly interpreted as a confidence level. For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class.\n\nSince those models only handle the data imbalance issue internaly, and are not feeded with pre-balanced data, the skewed class distribution may result in even more bias in the predicted probabilities. So for next evaluation steps we will use the calibrated models (this decision was also motivated by the fact that calibrating those models has improved the CV scores in my previous attempts, specially the recall by about 1.5%), and in the end we will look at the effect of calibration on the predicted probabilities.","33f6f4d2":"#### Tuning the Classification Threshold","f48d79a3":"**Some key takeaways:**\n* for each model, we succesfuly achieved a consistent *mean_test_score* in the top5 results, wich means that our search algorithm coverges nicely with 50 iterations.\n* the tunning improved the overall performance of all models.\n* the *mean_train_score* is consistently higher than the *mean_test_score*, which I think is fine since the training is done on the balanced dataset (to give equal priority to each class), while the test is done on the actual imbalanced testing set (to correctly mesure how well our model can generalize).     *please correct me if I'm wrong!*","12973314":"**Comments and analysis:**  \n* The first thing we can note is that the plots for both models tell approximately the same story, this is a good sign of the reliability of the results.    \n* We can see on the PDPs (thick blue line) that on average, those features which have a flat line does not have a strong influence on the customer churn, while the others who have interesting shapes are the most important:\n    * The more the Total Transaction Count (`Total_Trans_Ct`) gets Higher than 50, the less likely the customer would leave the bank.\n    * Customers with No. of Contacts in the last 12 months (`Contacts_Count_12_mon`) higher than five, are most likely to leave the bank.\n    * ...\n* The ICE curves (light blue lines) complement the analysis: we can see that there are some exceptions when:\n    * having just one or more `Contacts_Count_12_mon` increase the likelihood of churn.\n    * Attrited customers are more between the ages 40-50.\n    * ...\n    \n  Therefore, ICE plots show some individual effect which are attenuated by taking the averages.","1b6217f8":"Likewise, for categorical features, we'll use parallel categories plot to highlight the main characteristics that distinguish the churning customers.  \n\n*Note:* parallel categories plot (and also the parallel_coordinates that we'll use in a later section) is known to be more useful when interactive, but unfortunately it slows down the page, so to keep the browser happy they'll be used only in the static mode.","073673fb":"The goal of this section is to break down how the model works for an individual prediction.\n\nFor this we are going to use the SHAP Values (an acronym from SHapley Additive exPlanations), they break down a prediction to show the impact of each feature.  \nThis is particulary useful as the bank wants to identify what factors are driving each customer churn probability, so it can make the right decisions and take appropriate actions.\n\nWe will show some examples of how the bank could use the SHAP with our models to obtain the final product: **Accurate**, **Confidence-mesured** and **Explainable** churn predictions for each customer.  \nLet's see how this looks for a churning and non-churning customer by two different models.","45fd4820":"**The dependence of variables:**  \nWhen features are collinear, permutating one feature will have little effect on the models performance because it can get the same information from a correlated feature. One way to handle multicollinear features is by performing hierarchical clustering on the Spearman rank-order correlations, picking a threshold, and keeping a single feature from each cluster. \n\nHere won't re-fit the models but let's see which features caused this issue by looking at the correlations heatmap and the dendrogram:","3d1ae400":"### Initial grid search","0e8702b8":"### Feature Engineering","b3a23df9":"The next steps of modeling and evaluating models would be basically the same, thus they won't be commented with as much details.","ed83b80b":"The first thing we may observe is that our variables are at different scales, indicating that depending on what model we will use, we may need to consider using some scalling transformations.  \nSecond thing is that some variables may suffer from skewed distribution, but we will look into that in more details alongside with other properties in the following section.","398ba2ae":"### Categorical variables","8390b736":"### Interactive exploration of the relations with the target variable","1daa43d8":"#### Resampling\nImbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class. This results in models that have poor predictive performance, specifically for the minority class. This is a problem, because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class, which is the case for churn prediction. We'll discuss what makes a good model later in evaluation metrics section. \n\nTo counter this problem we need to balance our dataset, and for that there are different resampling strategies :\n- **Under-sampling:** Undersampling consists of deleting observations from your over-represented class. In this case, those would be the active customers. This makes sense to do when you have a large amount of data where the information loss due to removing observations would have a minimal impactthe information loss due to removing observations.\n- **Over-sampling:** Oversampling consists of adding copies of observations from your under-represented class. In this case, those would be the churned customers. A better known version is to rather than taking copies of observations, we use a distance measure to create synthetic samples of data points, this method is called SMOTE (Synthetic Minority Over-sampling Technique).\n- **Combination of over- and under-sampling:** Although using SMOTE can help us retain more information on the dataset, this method can generate noisy samples by interpolating new points between marginal outliers and inliers. This issue can be solved by cleaning the space resulting from over-sampling. For this purpose we will use Tomek\u2019s link (removing the instances of the majority class in pairs of very close instances, but of opposite classes) to increases the space between the two classes, and facilitate the classification process.","d76fafc5":"<h1>Predictive Marketing in banking || Customer Churn analysis and Prediction<\/h1>","87794410":"# Introduction\n## The business problem\nChurn prediction is one of the most common use cases in data science. It often leads to the establishment of a predictive score that is used to trigger marketing and sales actions. <br>\nThe availability of historical customer data, and knowing that retention costs are often lower than acquisition costs, make predicting attrition one of the first jobs of a team of data scientists. <br>\nThe case of the bank, where the client portfolio must remain balanced, is a special case of the application of attrition prevention. Indeed, the departure of a category of customers would lead to an imbalance in the portfolio, requiring tariff changes, possibly leading to further changes in the customer base (by customer arrivals and departures). This feared instability in this particular sector favors the prevention of leaving, even if it means keeping clients at risk or not paying much.\n\n## The project goal\nMotivated by the reasons above, our goal would be:\n- Try to detect the potential patterns that may explain why the customers decided to leave the bank.\n- Build a solide model that can predict attrited customers, and generate a relevant predictive score mesuring how likely each customer would leave or stay.\n- Support the model with Machine Learning explaination tools to explain each prediction, and hence explain the basis that drives each decision.","40811e98":"> *Before we demand more of our data, we need to demand more of ourselves.* - Nate Silver","b11c3ace":"### Predictive power","54a039df":"# How much our models would benefit from adding more data ?","9c7b46a2":"### Interactions between numeric variables","e7e66a5f":"## Partial dependence","7442205b":"## Importing Data","9e7905ff":"This project code is meant to be compatible as much as possible with the scikit-learn ecosystem, to take advantage of a wide range of available features that we'll discover along the notebook.","00a818b4":"## Permutation importance","d8c94d46":"### List of models to try\n\nInitially before this submition I tried some linear models (Linear Regression and Ridge classifier) but they was always far behind the tree based models, so I decided to eliminate them and only keep the models that were able to fit our data and shows consistently a decent score, those are:\n* **Support Vector Machine:** Can handle unbalanced problems, Support custom Kernels for the decision function, Effective in high dimensional spaces\n* **Random Forest:** Bootstraped decision trees, reduces overfitting in decision trees and helps to improve the accuracy.\n* **Extremely Randomized Trees:** Randomized decision trees, uses the entire input sample instead of a bootstrap replica will reduce bias, and chooses randomly the split point of each node to reduce variance. (also much faster than Random Forest)\n* **Histogram based Gradient Boosting:** Faster than  normal Gradient Boosting, native support for missing values and categorical features.\n\nXGboost has also achieved a promising performance, but because of his expansive computation, I decided to keep it for another time when I might use the GPU acceleration to run the tunning process.","3631202b":"# Modeling","8f849eb2":"### Convergence diagnosis","df640f84":"## Model selection and Tuning hyperparameters","52a93115":"It seems that there is no problem in importing the data.","73228db6":"In this section we will focus on some models that are designed specifically to handle imbalanced data. those methods woks by generating under-sampled subsets combined inside an ensemble.\n\nWe will try both a Bagging and Boosting algorithms:\n* `RUSBoostClassifier`: Random under-sampling integrated in the learning of AdaBoost.\n* `BalancedRandomForestClassifier`: A balanced random forest classifier.","a1f08c2c":"***Final toughts:***  \nThis project was a great opportunity to learn:\n- a lot of interesting theorical concepts,\n- some techniques for dealing with existing problems in real-word data,\n- how to design utility functions to automate commun data sience and machine learning tasks, and\n- how to read and interpret the results, and define additional tasks based on theses findings. \n\n*Special thanks to:*<br>\n\\- The kaggle community for sharing knowledge and being a great ressource for learning, <br>\n\\- The developers and contributors to the great tools used here, and  <br>\n\\- Everyone who had supported me and encouraged me to keep going.\n\n*Credits:*  \n<small>Some of the Code\/Explanations in this notebook were taken from or inpired by:<\/small>  \n\\- [Scikit-learn User guide](https:\/\/scikit-learn.org\/stable\/user_guide.html)  \n\\- [Kaggle Courses](https:\/\/www.kaggle.com\/learn\/)  \n\\- [machine learning mastery | Imbalanced Classification](https:\/\/machinelearningmastery.com\/category\/imbalanced-classification\/)  \n\\- [kdnuggets tutorials](https:\/\/www.kdnuggets.com\/tutorials\/index.html)","b79e214f":"## Models with native handling of imbalanced data","d9a79f12":"Again, first note is the consistency of results, although we used two different models, we can still get similar insights about the contribution of each feature for estimating the churn probability.  \n***Note:*** the numbers next to each feature are not directly interpretable since they show the values of the transformed data. But as soon as we get the propotions of impact of each feature, we could easily go back to our data and inspect the original value before any transformation was done.","f632b3e4":"Before jumping to the fancy fine tuning concepts, let's try runing a minimal grid search over only some few parameters, and see if we can get any insights that would help us in the next step. \n\nTo further accelerate this process, we will use successive halving (SH), it's like a tournament among candidate parameter combinations. SH is an iterative selection process where all candidates (the parameter combinations) are evaluated with a small number of training samples at the first iteration. Only some of these candidates (1\/3 of them in our case) are selected for the next iteration, which will be evaluated on more number of training samples.","f92660da":"These plots are suggesting that `SVClassifier` and `HistGradientBoostingClassifier` are able to achieve higher F1-score by tunning the threshold to an optimal value that is relatively far from the default threshold 0.5.  \nWorth noting also that the relative improvements from changing the treeshold are only significant for SVM.","71bf8f16":"**Primary observations:**\n\n* not specifying `max_depth` tends to result in better models.\n* smaller `learning_rate` is consistently better.\n* higher `n_estimators` and `max_iter` is genearaly better.\n* setting `gamma=\"scale\"` in SVC will often improve the model","dfec6966":"* From evaluations plots, we can see that points start clustering around the location of the miminum error. The histograms show that the objective is evaluated more often at locations near to one of the minima.  \n* From the partial dependence plots, we can read the impact of each feature controling for the others, this is **useful** since it makes us understand what is the effect of changing each hyperparameter and allows us to make intelligent decisions based on our requirements rather that blindly choosing the one value suggested by the tunning results.","4bec07f3":"## Statistical proprities and Features interactions","23e0e830":"**Before we begin:** <br>\nThis is my first data science project with python on kaggle, if you have any suggestion on how I can make my work better, I'll be glad to learn more and improve myself. <br>\nand if you find this notebook useful, you can upvote it and leave me your feedback in the comments.","e5bc0794":"## Tuning results analysis","6a528cab":"### Inspection of the tuning path","0a8325bb":"Every estimator has its advantages and drawbacks. Its generalization error can be decomposed in terms of bias, variance and noise. The bias of an estimator is its average error for different training sets. The variance of an estimator indicates how sensitive it is to varying training sets. Noise is a property of the data.\n\nOne way to reduce the variance of a model is to use more training data. However, you should only collect more training data if the true function is too complex to be approximated by an estimator with a lower variance.\n\nA learning curve shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error.","f604adc2":"# Conclusion\n\nThe ability to identify customers that aren\u2019t happy with provided solutions allows businesses to learn about product or pricing plan weak points, operation issues, as well as customer preferences and expectations to proactively reduce reasons for churn.\n\nBy integrating the solutions presented in this project in the bank system, we can build an alert system to identify customers at risk and their specific reasons. this would allow the bank to upgrade their approache of dealing with customer churn problem, from a reactive approach (that focuses on winning back the customer after they unsubscribe) to a proactive approach: to try to keep its at-risk customers BEFORE they leave.\n\nAfter identifying those unhappy cutomers, because an algorithm without action has no value, we have to put concrete actions in place:\n\n- Examine their profile, identify their characteristics, analyze past interactions with the services and products.\n- Communicate the most recent offers and services likely to meet their needs.\n- Concentrate retention efforts, material and financial resources on profitable customers.\n\nWelcome to the era of predictive marketing!","b3bb2057":"This figure is showing how much and how fast each model improve with every tunning iteration step.  \nWe can see that in general the tunning keeps chosing a better combination for the next iteration, also it seems that the amount of additional improvement in the minimal loss is getting lower each time, indicating the convergence of our tunning.","27f79d64":"# What features our model think are important? - Feature Importance","590b6bdb":"Another straightforward way to see which cobinations performe better and which perform worse is by looking at the parallel cordinates plots, this can quickly give you some insights without having to link and interpret multiple figures.","47d74d9d":"The balanced models have succeeded in achieving relatively good scores, without the need to generate aditional data or to remove observations.\n\nSo it seems to be a great strategy to use all available data, and also to avoid the use of complicated computationally expensive solutions.  ","8a4051bb":"This interactive repport can cover a wide range of interesting proprities, and highlight the issues or the useful informations about each variable, such as the picks, skews, multi modalities and other easy to catch insights in the data.  \nHowever, for prediction purposes, what can be more important for us is how each variable can interact with our target variable, and what factors are mostly related with the attrition problem.","0efe1662":"#### Choosing the scoring metric\n\nWe will predict class labels of whether a customer will leave or not. Therefore, we need a measure that is appropriate for evaluating the predicted class labels.\n\nThe focus of the task is on the positive class (churning customers). Precision and recall are a good place to start. Maximizing precision will minimize the false positives and maximizing recall will minimize the false negatives in the predictions made by a model.\n\n* Precision = TruePositives \/ (TruePositives + FalsePositives)\n* Recall = TruePositives \/ (TruePositives + FalseNegatives)\n\nUsing the F-Score will calculate the harmonic mean between precision and recall. This is a good single number that can be used to compare and select a model on this problem.\n\n* F-Score = (2 * Precision * Recall) \/ (Precision + Recall)\n\nA better choice could be the use of Fbeta-Score with beta>1, here is what F**2**-Score looks like:\n\n* F2-Score = ((1 + **2**^2) * Precision * Recall) \/ (**2**^2 * Precision + Recall)\n\nThis score for example is assuming that false negatives are twice (or beta times) more damaging than false positives, and weight more attention on recall than precision by a factor of beta.  \nBut for now, let's use F1-Score, and I will try to update this later with the other tasks in the TO-DO list.","3cbf4ad4":"for skopt's BayesSearch, we can pass directly the whole search space, since it handles it as follows :\n> If a list of dictionary objects is given, then the search is performed sequentially for every parameter space with maximum number of evaluations set to n_iter.","d580f417":"### Optimal hyperparameters using bayesian optimisation","92c110bf":"## Describing data","a56b07e0":"# Exploratory data analysis - EDA","54b02271":"Before we begin our modeling process or doing any changes on the data, we'll need to make sure to hold out a part of the original data as a test set to properly evaluate later the model performance on real unseen data.","e149a5f9":"Permutation feature importance is a model inspection technique that tries to answer the question: What features have the biggest impact on predictions?\nThe permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being model agnostic and can be calculated many times with different permutations of the feature, and it is especially useful for non-linear or opaque estimators.","609b0694":"To investigate and analyse the results of our models tuning, we will next use a collection of diagnostic plots","7ad1a7f1":"So, visually, the features that seems to have more direct impact on churn are:  \n`Card category`, `Total_Relationship_Count`, `Months_Inactive_12_mon`, `Total_Revolving_Bal`, `Total_Trans_Amt`, `Total_Trans_Ct`, `Total_Ct_Chng_Q4_Q1`, `Avg_Utilization_Ratio`.\n\nAgain it is not enough to look only at the univariate interactions (i.e one feature at a time), as real word data often has more complicated relationships between variables. For this reason we'll see next the interactions between the variables and how they impact our target variable.","3fe3cebe":"For processing time sake, we're going the reduce the data memory usage, by setting each column data type to a less memory consuming one (if possible), this can make some computations runs faster and saves us some memory.","3ab546f5":"Here we can see the most common propreties among the attrited customers, for instance, Graduate Females have relatively high chance of attrition.","cb9a0ed3":"![Customer Churn](https:\/\/abdelgha-4.github.io\/Portfolio\/images\/churn.png)","9741a227":"#### Rliability diagrams\n\nCalibration curves (also known as reliability diagrams) compare how well the probabilistic predictions of a binary classifier are calibrated. It plots the true frequency of the positive label against its predicted probability, for binned predictions. The x axis represents the average predicted probability in each bin. The y axis is the fraction of positives, i.e. the proportion of samples whose class is the positive class (in each bin). \n\nThe two calibration curve plots in the left shows the calculated the per bin average predicted probabilities and fraction of positives for each model before and after calibration.   \nThe histogram in the right gives some insight into the behavior of each classifier by showing the number of samples in each predicted probability bin.","19d5860e":"For these calibrated classifiers, by maintaining the default threshod we're already very close to the optimal points, so changing it wouldn't make a big difference.","cbd5208a":"### Missing values","7a0bdfb2":"**Bonus:** *(feel free to skip this)*\n\nFor a more interactive experience, you can try this easy two-steps approach to access an ineractive data visuliazer:\n1. click on the \"launch\" button bellow, this will send you to a public jupyter notebook (wait a bit for it to load).\n2. run the first cell, and click on the link that will appear.\n\nNow you are in to visuliazer, try to explore the interface and some interisting features (look at *charts*, *Feature Analysis*, *Correlation* and try *`code export`* with each feature) and see if you can find anything interesting.\n\n<a href=\"https:\/\/mybinder.org\/v2\/gh\/Abdelgha-4\/Churn-data-visualizer\/main?urlpath=tree\/churn%20data%20visualizer.ipynb\">\n    <img align=\"left\" src=\"https:\/\/mybinder.org\/badge_logo.svg\">\n<\/a>","4ad8a1f2":"#### Encoding\nMost machine learning algorithms require numerical inputs to work, the goal of categorical encoding is to produce variables we can use to train machine learning models and build predictive features from categories.  \nIn our case we'll use the most two popular techniques:  \n\\- Ordinal Encoding: for categorical variables that have a natural rank ordering, known as ordinal variables. This is the case for `Education_Level`, `Income_Category`, and `Card_Category`.  <br>\n\\- One-Hot Encoding: for categorical variables that do not have a natural rank ordering, known as nominal variables. in our data they are `Gender`, and `Marital_Status` variables.","0a142c90":"## building a pipeline","15a72834":"**Interpretation:**  \nLike most things in data science, there is some randomness to the exact performance change from a shuffling a column. We measure the amount of randomness in our permutation importance calculation by repeating the process with multiple shuffles. and present the mesures variations in boxplots, those can tell how performance varied from one-reshuffling to the next.\n\nNegative values for permutation importances means that the predictions on the shuffled (or noisy) data happened to be more accurate than the real data. This happens when the feature didn't matter (should have had an importance close to 0), but random chance caused the predictions on shuffled data to be more accurate.\n\nSo, is that all? About the half of our features dosen't matter and have no impact on customer churn?  \nNo, we're not done yet! there is a common pitfall that we had encountred here: *Multicollinear Features*, let's treat that.","c4738f33":"#### Tuning the Classification Threshold","49e68fb1":"**Comments:**  \n* We did get an AUC between 0.97 and 0.99, thus we can confirm our intuition of getting a good predictive power.  \n* The best model is `HistGradientBoostingClassifier`, it is outperforming the other models in most perfomance metrics.\n* SVM can compete with other models in terms of Sensitivity (recall), but its relatively low precision makes it behind the others.\n* Since we obtained a higher precision than recall in most models, we may want to trade off a slight desrease in precsion for getting a better recall, a possible way of acheiving this is by changing the probability threshold to a better value. we will look into this in the next section.","233607d1":"<big><big><strong><i> To-Do: <\/i><\/strong><\/big><\/big>\n\n- Separate computation from plotting (Specially the calibration of models).\n- Add Xgboost with hyperparameter tuning using GPU.\n- Add global Feature Importance with SHAP, and the two-way partial dependence plots.\n- Add more extensive EDA and Feature engineering.\n- Repeat the tunning using F2-score instead of F1-score.\n- Reduce redundant and repetitive features.\n- Add more explanation.\n- Try explainerdashboard when it becomes compatible with imbalanced-learn pipelines.\n- Upgrade to scikit-learn 1.0 when scikit-optimize becomes compatible with it.\n- Try `HistGradientBoostingClassifier`'s native handling of categorical features.\n- Try considering *'Unknown'* class in `Education_Level`, `Marital_Status `, and `Income_Category` as missing values,  \n  and apply imputation techniques to see if this can further improve the performance.","2e097329":"To avoid redundancy, we'll look on all the interactions in one place, for numeric variables we'll use a scatter matrix that can serve our purpose very well.","3a590c71":"#### Predictive power","7b4a97f6":"**Insights:**  \n- The target variable `Attrition_Flag` has unequal distribution of classes, thus we are facing an imbalanced classification problem, so we'll need later to apply the necessary techniques for dealing with imbalanced data.<br>\n- We can observe the ordering of the customers characteristics by popularity. For instance:  \n    - Customers with higher income are rarer, and more than third of our customer base are in `Less than $40K` category,  \nand because also most of the customers are at `Graduate` level, this might indicate that a large porpotion of this bank customers are at a younger age.  \n    - the dataset is balanced with respect to gender.  \netc...","ac9034f7":"\nthe predicted probabilities must be interpreted before they can be mapped to a class labels. This is achieved by using a threshold, such as 0.5, where all values equal or greater than the threshold are mapped to the positive class and all other values are mapped to the negative class.\n\nFor those classification problems that have a severe class imbalance, the default threshold can result in poor performance. As such, a simple and straightforward approach to improving the performance of a classifier that predicts probabilities on an imbalanced classification problem is to tune the threshold used to map probabilities to class labels.\n\nTo determine the effect of moving the threshold we will use a visualization of precision, recall, f1 score, and queue rate (This metric describes the percentage of instances that must be reviewed, i.e percentage of predicted positives) with respect to the discrimination threshold of a binary classifier","a89e20f4":"We can see above a lot of interesting patterns in our data, and more interestingly a good seperation between our classes, this is suggesting that we can have a lot of predivtive power, and we can already hope for a good classification results ","b1cf2777":"Partial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze interaction between the target response and a set of input features of interest.\n\nPartial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the \u2018complement\u2019 features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest.\n\nindividual conditional expectation (ICE) plot shows the dependence between the target function and an input feature of interest. However, unlike a PDP, which shows the average effect of the input feature, an ICE plot visualizes the dependence of the prediction on a feature for each sample separately with one line per sample. \n\nWhile the PDPs are good at showing the average effect of the input features, they can obscure a heterogeneous relationship created by interactions. When interactions are present the ICE plot will provide many more insights.\n\nIt is commun to plot an overlay of both of the plots, taking advantages of both proprities:","e8655c63":"Among what we can see here:  \n\\- In general, the higher the iteration is, the higher the score we get.  \n\\- Some parameters have a clear impact on the model performance, while others dosen't seem to make a significant difference.  \n\\- Some models can have multiple local optima and might benefit from increasing the iterations to avoid getting stuck in a local optimum.","8654bc0b":"*Terminology:*  \n\\- in our case, the positive class represent the churning customers.  \n\\- true positive (TP), correctly detected as positive class.  \n\\- true negative (TN), correctly rejected.  \n\\- false positive (FP), false alarm.  \n\\- false negative (FN), false rejection.\n\nTo evaluate our models performance, this metrics\/plots were chosen:  \n* **Imbalanced classification report:** this is a table showing the main classification metrics used for imbalanced data: \n    - precision: the ratio `TP \/ (TP + FP)`. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n    - recall: the ratio `TP \/ (TP + FN)`. The recall is intuitively the ability of the classifier to find all the positive samples.\n    - geometric mean: This measure tries to maximize the accuracy on each of the classes while keeping these accuracies balanced.\n    - balanced accuracy: This is a correction for the usual accuracy, accoucting for the unbalance in target classes.\n    - F1-score: as explained above in \"chosoing the scoring metric\" section.\n\n  *Be careful:* to correctly read the recall and precision, you need to look at `Attrited Customer` row (the positive class), because the recall and precision for the negative class have a different interpretation.\n\n\n* **Confusion matrix:** simple, yet effective plot to see and compare TP, TN, FP, and FN.\n* **Class prediction error:** similar to the confusion matrix, except that it shows the prediction class in a readable barplot presentation.\n* **ROC curve:** a plot showing the diagnostic ability of a binary classifier system as its discrimination threshold is varied.\n* **Precision recall curve:** a plot that shows the tradeoff between precision and recall for different threshold.\n\nNote: if you are seeing this from kaggle, you can fold the table of content to get a wider view.","fe36de84":"# Istalling and importing necessary tools ","a1c15861":"### Splitting data"}}