{"cell_type":{"7c0e6720":"code","11769af9":"code","76815b68":"code","891db9cc":"code","11353642":"code","4a3b2a93":"code","4723d4f0":"code","14b0ac08":"code","f10eac11":"code","22336c40":"code","0d9ecc37":"code","3765862a":"code","04d36028":"code","36cd8b6c":"code","ca573ada":"code","69709430":"code","663ac4be":"code","34262dad":"code","cb8b8985":"code","1d90e559":"code","68dff1f7":"code","7c10eb08":"code","259ade4f":"code","78e9bb5c":"code","304b1f80":"code","6765b931":"code","c5dfc240":"code","dbda5828":"code","696b8c0e":"code","f4c9ec90":"code","42edc973":"code","275ba50e":"code","bce0c7c3":"code","4234347c":"code","eac580ce":"code","4a590e0d":"code","ac2f6ec5":"code","44f8c5d8":"code","7e2ae725":"code","dea701bd":"code","c92a3fa7":"markdown","b3fcc7be":"markdown","7d1d2327":"markdown","c4b17ccc":"markdown","687e713f":"markdown","602208f5":"markdown","c9694cb1":"markdown","bbe64506":"markdown","744f31e1":"markdown","8e16aedd":"markdown","08d8135e":"markdown","edf725dd":"markdown","4f1fafeb":"markdown","fabf7b5f":"markdown","56ccf15f":"markdown","5e837d64":"markdown","db406299":"markdown","7553f19a":"markdown","55159118":"markdown"},"source":{"7c0e6720":"from skimage.transform import resize\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport cv2 \nimport random\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense, Dropout,GlobalAveragePooling2D, Activation, Flatten, Conv2D, MaxPooling2D, MaxPool2D, BatchNormalization\nfrom keras import models, layers, optimizers\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom keras.optimizers import SGD, Adam, RMSprop\nfrom keras.models import Sequential\nfrom keras.callbacks import ModelCheckpoint,Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras import backend as K\nimport tensorflow as tf\nrandom.seed(180)","11769af9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))","76815b68":"traindir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Train'\nvaliddir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Validation'\ntestdir='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Test'","891db9cc":"myList = os.listdir(traindir)\nprint(\"Total Number of Classes Detected :\",len(myList))\nnoOfclasses= len(myList)\nprint(myList)","11353642":"path1='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Test\/WithMask'\n\nprint(\"Images With Mask\")\nplt.figure(figsize=(20,10))\nfor i in range(5):\n    file=random.choice(os.listdir(path1))\n    img_path=os.path.join(path1,file)\n    image=cv2.imread(img_path)\n    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    ax=plt.subplot(1,5,i+1)\n    plt.imshow(image)","4a3b2a93":"path2='..\/input\/face-mask-12k-images-dataset\/Face Mask Dataset\/Test\/WithoutMask'\n\nprint(\"Images Without Mask\")\nplt.figure(figsize=(20,10))\nfor i in range(5):\n    file=random.choice(os.listdir(path2))\n    img_path=os.path.join(path2,file)\n    image=cv2.imread(img_path)\n    image=cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n    ax=plt.subplot(1,5,i+1)\n    plt.imshow(image)","4723d4f0":"classes=['WithMask','WithoutMask']\nprint(\"Importing Dataset...\")","14b0ac08":"def load_data(image_list,label_list,path):\n    classes=['WithMask','WithoutMask']\n    for category in classes:\n        picList= os.listdir(path+\"\/\"+str(category))\n        for pic in tqdm(picList):\n            image= cv2.imread(path+\"\/\"+str(category)+\"\/\"+pic)\n            image= cv2.resize(image,(128,128))\n            image = img_to_array(image)\/255.0\n            image_list.append(image)\n            label_list.append(classes.index(category))\n    return image_list,label_list","f10eac11":"train_images=[]\ntrain_labels=[]\ntrain_images,train_labels= load_data(train_images,train_labels,traindir)","22336c40":"val_images=[]\nval_labels=[]\nval_images,val_labels= load_data(val_images,val_labels,validdir)","0d9ecc37":"test_images=[]\ntest_labels=[]\ntest_images,test_labels= load_data(test_images,test_labels,testdir)","3765862a":"def convert_to_array(train_images,train_labels):\n    images = np.array(train_images)\n    labels = np.array(train_labels)\n    return images,labels","04d36028":"x_train,y_train= convert_to_array(train_images,train_labels)\nx_val,y_val= convert_to_array(val_images,val_labels)\nx_test,y_test= convert_to_array(test_images,test_labels)","36cd8b6c":"del train_images\ndel train_labels\ndel val_images\ndel val_labels\ndel test_images\ndel test_labels","ca573ada":"print(x_train.shape, y_train.shape, type(x_train), x_train.dtype,y_train.dtype)\nprint(x_val.shape, y_val.shape)\nprint(x_test.shape, y_test.shape)","69709430":"print(y_train[650])\nprint(y_val[650])\nprint(y_test[650])","663ac4be":"y_train= to_categorical(y_train)\ny_val= to_categorical(y_val)\ny_test= to_categorical(y_test)","34262dad":"print(y_train[650])\nprint(y_val[650])\nprint(y_test[650])","cb8b8985":"augmentation = ImageDataGenerator(  \n    height_shift_range= 0.2, \n    width_shift_range=0.2, \n    rotation_range=20,\n    shear_range=0.2,\n    zoom_range=0.15,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)","1d90e559":"file_path=\".\/model_weight.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\nearly = EarlyStopping(monitor=\"val_loss\", mode='auto', patience=10,restore_best_weights=True)\ncallbacks_list = [checkpoint,early]","68dff1f7":"model5=Sequential()\nmodel5.add(Conv2D(32,(3,3),activation='relu', input_shape=(128, 128, 3),padding='same'))\nmodel5.add(BatchNormalization())\nmodel5.add(Conv2D(32,(3,3),activation='relu', padding='same'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D((2, 2)))\nmodel5.add(Dropout(0.2))\n\nmodel5.add(Conv2D(64,(3,3),activation='relu',padding='same'))\nmodel5.add(BatchNormalization())\nmodel5.add(Conv2D(64,(3,3),activation='relu',padding='same'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D((2, 2)))\nmodel5.add(Dropout(0.2))\n\n\nmodel5.add(Conv2D(128,(3,3),activation='relu',padding='same'))\nmodel5.add(BatchNormalization())\nmodel5.add(Conv2D(128,(3,3),activation='relu',padding='same'))\nmodel5.add(BatchNormalization())\nmodel5.add(MaxPooling2D((2, 2)))\nmodel5.add(Dropout(0.2))\n\n\nmodel5.add(Flatten())\nmodel5.add(Dense(128, activation='relu'))\nmodel5.add(BatchNormalization())\nmodel5.add(Dropout(0.2))\nmodel5.add(Dense(2,activation='softmax'))\nmodel5.summary()","7c10eb08":"model_img_file = '.\/model_image.png'\ntf.keras.utils.plot_model(model5, to_file=model_img_file, show_shapes=True, show_layer_names=True)","259ade4f":"INIT_LR=0.0001\nEPOCHS=100\nBS=32\nmodel5.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.0001,decay= INIT_LR\/EPOCHS),metrics=['accuracy'])","78e9bb5c":"from sklearn.utils import shuffle\nx_val,y_val= shuffle(x_val,y_val)","304b1f80":"history = model5.fit(\n    augmentation.flow(x_train,y_train,batch_size=BS,shuffle=True),\n    steps_per_epoch= len(x_train)\/\/BS,\n    validation_data=(x_val,y_val),\n    validation_steps= len(x_val)\/\/BS,\n    epochs= EPOCHS,\n    callbacks=[callbacks_list],\n    verbose=1\n)","6765b931":"def plot_learning_curve(history):\n    plt.figure(figsize=(18,9))\n    plt.subplot(1,2,1)\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.savefig('.\/accuracy_curve.png')\n    plt.subplot(1,2,2)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.savefig('.\/loss_curve.png')","c5dfc240":"plot_learning_curve(history)\nplt.show()","dbda5828":"model5.load_weights(\".\/model_weight.hdf5\")\nscore=model5.evaluate(x_test,y_test)\nprint(\"Loss: {} \\t\\tAccuracy: {}\".format(score[0],score[1]))","696b8c0e":"y_pred = model5.predict(x_test,batch_size=BS)\nY_pred = np.argmax(y_pred,axis = 1)\nY_true = np.argmax(y_test,axis = 1)","f4c9ec90":"from sklearn.metrics import classification_report\nprint(classification_report(Y_true,Y_pred,target_names=['WithMask','WithoutMask']))","42edc973":"import itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.savefig('.\/confusion_matrix.png')","275ba50e":"# confusion matrix\ncm = confusion_matrix(Y_true, Y_pred)        \n# plot confusin matrix\nplt.figure(figsize=(6,6))\nplt.grid(b=False)\nplot_confusion_matrix(cm, classes=['WithMask','WithoutMask'], normalize=False, \n                      title='Confusion matrix', cmap = plt.cm.Blues)\nplt.show()","bce0c7c3":"model5.save('.\/face_model.h5')","4234347c":"# from keras import models\n# loaded_model = models.load_model('.\/face_model.h5')\n# score=loaded_model.evaluate(x_test,y_test)\n# print(score[1])","eac580ce":"x_test[98]","4a590e0d":"y_test[98]","ac2f6ec5":"face = np.expand_dims(x_test[98], axis=0)\n(mask, withoutMask) = model5.predict(face)[0]\nlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\nprint(label)","44f8c5d8":"y_test[790]","7e2ae725":"face = np.expand_dims(x_test[790], axis=0)\n(mask, withoutMask) = model5.predict(face)[0]\nlabel = \"Mask\" if mask > withoutMask else \"No Mask\"\nprint(label)","dea701bd":"# path=input()\n# image= cv2.imread(path)\n# image= cv2.resize(image,(128,128))\n# image = img_to_array(image)\/255.0\n# image=np.array(image)\n# image = np.expand_dims(image, axis=0)\n# (mask, withoutMask) = model5.predict(image)[0]\n# print(mask,withoutMask)\n# label = \"Mask\" if mask > withoutMask else \"No Mask\"\n# print(label)","c92a3fa7":"#### Importing Libraries","b3fcc7be":"#### One hot encoding","7d1d2327":"#### Accuracy and Loss Curve","c4b17ccc":"#### Model Visualization","687e713f":"#### Image Augmentation","602208f5":"#### Visualizing Data","c9694cb1":"#### Load the model","bbe64506":"#### Data Paths","744f31e1":"#### Convert to NumPy array","8e16aedd":"#### Model Compiling and Training","08d8135e":"### If the kernel impress you,give an <font size=\"+3\" color=\"blue\"><b>Upvote<\/b><\/font>.<br>","edf725dd":"#### Loading Data","4f1fafeb":"#### CNN Model Creation","fabf7b5f":"#### Conclusion\nCreated and trained a CNN model to detect face mask in images. The model obtained an accuracy of 99.7%.","56ccf15f":"#### Model Callbacks","5e837d64":"#### Prediction From Manual Input \nUncomment the code below to run.","db406299":"## The Sparks Foundation-GRIP-Data Science and Business Analytics-August'2021\n### Task 4: Detection of face mask\n### Author: Saifur Rahman Shatil","7553f19a":"#### Model Evaluation","55159118":"#### Predicting Image Mask or No Mask"}}