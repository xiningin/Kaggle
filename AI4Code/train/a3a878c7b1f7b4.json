{"cell_type":{"b3013b12":"code","3a20dd82":"code","ab0755ed":"code","57c24c61":"code","0c3dbd5d":"code","1bbc1995":"code","11931934":"code","29ff2d69":"code","7a882529":"code","c446059b":"code","174e4598":"code","703875fd":"code","95cb7480":"code","14b6fc1b":"code","6e254bf9":"code","b8a5788f":"code","843675e4":"code","e87a579a":"code","578d918f":"code","17ea5ca6":"code","b7a09875":"code","ed671049":"code","5ef8f895":"code","f5f55c50":"code","ad1ba09b":"code","7988384c":"code","bfb4f579":"code","c5c23753":"markdown","45b4e080":"markdown","cecb8be7":"markdown","ce4a8f20":"markdown","5cdf9a00":"markdown","a8657b11":"markdown","b9f18e1a":"markdown","5fea56a5":"markdown","ded3a953":"markdown","97ee64c0":"markdown","6524679f":"markdown","286f93a1":"markdown","bbb3aebb":"markdown","b81db38d":"markdown","db6ab260":"markdown","dfd38cc7":"markdown","a00ec5c9":"markdown"},"source":{"b3013b12":"# Lets download the spacy library\n#!pip install spacy","3a20dd82":"#Importing the Required Python Packages\nimport os\nimport shutil\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\npd.set_option('display.max_colwidth', -1)","ab0755ed":"# Lets load the default english model of spacy\nnlp = spacy.load('en_core_web_sm')","57c24c61":"# Lets load the train dataset.\ntrain = pd.read_csv('Data\/train.csv')","0c3dbd5d":"#Lets check the dataset\ntrain.head()","1bbc1995":"# Lets create a function to create a new feature 'ABV' from dataset\ndef createFeature(df):    \n    return [x.split(' ')[y] for x,y in zip(df['TEXT'], df['LOCATION'])]","11931934":"train['ABV'] = createFeature(train)","29ff2d69":"grouped = train.groupby(by=['ABV', 'LABEL'], as_index = False, sort = False).count()\ngrouped = grouped.sort_values(by='TEXT', ascending = False)\ngrouped","7a882529":"#Lets take top 20 Abbreviations for further processing\ntopAbv = grouped['ABV'][:20]\ntopAbv","c446059b":"train = train[train['ABV'].isin(topAbv)]\ntrain.shape","174e4598":"#Lets create a function to convert all the text in lowercase\ndef tolower(df):\n    return [t.lower() for t in df['TEXT']]","703875fd":"# Lets create a function to remove all the Punctuations from Text\ndef removePunctuation(df):\n    return [t.translate(str.maketrans('','',string.punctuation)) for t in df['TEXT']]","95cb7480":"# Lets create a function to Tokenize the Text column of dataset\ndef createTokens(df):\n    return df['TEXT'].apply(lambda x: x.split(' '))","14b6fc1b":"#Lets create a function to drop \"Abstract_id\", \"Location\" and \"TEXT\" columns from dataset\ndef dropCols(df):\n    return df.drop(columns=['ABSTRACT_ID', 'LOCATION', 'TEXT'])","6e254bf9":"# Lets create a function to remove stop words from the Text column\ndef removeStop(df):\n    stopWords = spacy.lang.en.stop_words.STOP_WORDS\n    # Remove any stopwords which appear to be an Abbreviation\n    [stopWords.remove(t) for t in df['ABV'].str.lower() if t in stopWords]\n    return df['TOKEN'].apply(lambda x: [item for item in x if not item in stopWords])","b8a5788f":"def preProcessData(df):   \n    df['TEXT'] = tolower(df)\n    df['TEXT'] = removePunctuation(df)\n    df['TOKEN'] = createTokens(df)\n    df = dropCols(df)\n    df['TOKEN'] = removeStop(df)\n    return df","843675e4":"preProcessData(train).to_csv('Train\/train_final.csv', index = False)","e87a579a":"# Lets load the Valid and test datasets as well.\nvalid = pd.read_csv('Data\/valid.csv')\ntest = pd.read_csv('Data\/test.csv')","578d918f":"valid.head(3)","17ea5ca6":"test.head(3)","b7a09875":"# Create ABV feature for valid and test sets.\nvalid['ABV'] = createFeature(valid)\ntest['ABV'] = createFeature(test)","ed671049":"# Filter the valid and test datasets based on the topAbv list and check their shapes\nvalid = valid[valid['ABV'].isin(topAbv)]\ntest = test[test['ABV'].isin(topAbv)]\nprint('Valid:', valid.shape)\nprint('Test:', test.shape)","5ef8f895":"valid = valid[:10000]\ntest = test[:10000]","f5f55c50":"valid = preProcessData(valid)","ad1ba09b":"test = preProcessData(test)","7988384c":"valid.to_csv('Validation\/valid_final.csv', index = False)","bfb4f579":"test.to_csv('Test\/test_final.csv', index = False)","c5c23753":"### Again, due to Hardware limitations, lets use 10K rows for validation and test sets.","45b4e080":"### Apply the above preProcessData function to train set","cecb8be7":"## Step# 2b: Convert the data to lowercase","ce4a8f20":"### Lets load the Validation and Test set for Preprocessing.","5cdf9a00":"## Step# 1: Load the datasets","a8657b11":"## Step 2f: Remove Stop words","b9f18e1a":"## Step# 2c: Remove Punctuations","5fea56a5":"## Step# 2a: Create a new feature 'ABV'","ded3a953":"## Step# 2: Data Preprocessing: Following preprocessing steps will be performed on the Dataset:\n\n    a. Create a new feature 'ABV' for abbreviation directly deriving it from Location and Text columns.-- Already done above\n    b. Convert text to lowercase.\n    c. Remove Punctuations from the Text Column.\n    d. Tokenize the Text column.\n    e. Dropping Abstract_id, Location and Text columns.\n    f. Remove stop words from the Text Column.\n\nSo, lets start with the Data Preprocessing sub-steps.","97ee64c0":"### Lets check which Abbreviations occur the most and lets take top 20 such Abbreviations for further processing due to Hardware limitations","6524679f":"### Lets apply the preprocessing steps to Valid and Test datasets","286f93a1":"## Step# 2e: Dropping the columns not needed","bbb3aebb":"## Lets Create a function to apply all the above preprocessing steps to the dataset","b81db38d":"# Abbreviation Disambiguation in Medical Texts - Data Preprocessing\n\nThis Notebook is in continuation of the notebook- 'Step 1- Data Wrangling and EDA' and lists down:\n\n1. Data Preprocessing done on the dataset.\n2. Exploring the Preprocessed data.","db6ab260":"### Let's only use the Abbreviations for which we are training the model","dfd38cc7":"## Step# 2d: Tokenize the text column and save the tokenized data in a new column 'TOKEN'.","a00ec5c9":"### Lets extract only the above 20 Abbreviations from train set for futher processing."}}