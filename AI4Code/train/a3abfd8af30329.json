{"cell_type":{"c2626cc1":"code","eefd6342":"code","41b3ab77":"code","315429a6":"code","5d464130":"code","b77c5c9f":"code","9f138fb8":"code","2a227c67":"code","d777da7b":"code","5d1faadc":"code","9a9278c5":"code","3645764e":"code","37103cdf":"code","0e6bae14":"code","ffa789b6":"code","862d375c":"code","6eaf29bd":"code","73eb1b76":"code","b13e98b0":"code","bda96918":"code","6e7d55ed":"code","10c6c362":"code","079539e7":"code","bc0b0084":"markdown","ba518051":"markdown","0461e526":"markdown","60d53d8d":"markdown","f8bcc285":"markdown","717b82f2":"markdown","7a25e744":"markdown","4545e630":"markdown","c230cb48":"markdown","cff91074":"markdown","dd4e8656":"markdown","6e4f109f":"markdown","37097d33":"markdown","7dd45019":"markdown","4e092267":"markdown","d0bdb277":"markdown","22e638d3":"markdown","d98cfde2":"markdown","5be2735b":"markdown","087d5be8":"markdown","f052f3d9":"markdown","db9076f4":"markdown"},"source":{"c2626cc1":"!pip install seaborn --upgrade #Update Seaborn for plotting","eefd6342":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Plotting Functions\nimport matplotlib.pyplot as plt\n\n#Aesthetics\nimport seaborn as sns\nsns.set_style('ticks') #No grid with ticks\nprint(sns.__version__)","41b3ab77":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","315429a6":"heart=pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')\nheart.info()\nheart.head()","5d464130":"heart_list=['sex','cp', 'fbs', 'restecg','exang', 'slope','ca','thal']\n\nfor label in heart_list:\n    if heart[label].max()==1:\n        print(label, 'is only categorical!')\n    else:\n        print(label, 'is ordinal with values of', sorted(heart[label].unique()))","b77c5c9f":"def Plotter(plot, x_label, y_label, x_rot=None, y_rot=None,  fontsize=12, fontweight=None, legend=None, save=False,save_name=None):\n    \"\"\"\n    Helper function to make a quick consistent plot with few easy changes for aesthetics.\n    Input:\n    plot: sns or matplot plotting function\n    x_label: x_label as string\n    y_label: y_label as string\n    x_rot: x-tick rotation, default=None, can be int 0-360\n    y_rot: y-tick rotation, default=None, can be int 0-360\n    fontsize: size of plot font on axis, defaul=12, can be int\/float\n    fontweight: Adding character to font, default=None, can be 'bold'\n    legend: Choice of including legend, default=None, bool, True:False\n    save: Saves image output, default=False, bool\n    save_name: Name of output image file as .png. Requires Save to be True.\n               default=None, string: 'Insert Name.png'\n    Output: A customized plot based on given parameters and an output file\n    \n    \"\"\"\n    #Ticks\n    ax.tick_params(direction='out', length=5, width=3, colors='k',\n               grid_color='k', grid_alpha=1,grid_linewidth=2)\n    plt.xticks(fontsize=fontsize, fontweight=fontweight, rotation=x_rot)\n    plt.yticks(fontsize=fontsize, fontweight=fontweight, rotation=y_rot)\n    \n    #Legend\n    if legend==None:\n        pass\n    elif legend==True:\n        plt.legend()\n        ax.legend()\n        pass\n    else:\n        ax.legend().remove()\n        \n    #Labels\n    plt.xlabel(x_label, fontsize=fontsize, fontweight=fontweight, color='k')\n    plt.ylabel(y_label, fontsize=fontsize, fontweight=fontweight, color='k')\n\n    #Removing Spines and setting up remianing, preset prior to use.\n    ax.spines['top'].set_color(None)\n    ax.spines['right'].set_color(None)\n    ax.spines['bottom'].set_color('k')\n    ax.spines['bottom'].set_linewidth(3)\n    ax.spines['left'].set_color('k')\n    ax.spines['left'].set_linewidth(3)\n    \n    if save==True:\n        plt.savefig(save_name)","9f138fb8":"fig, ax=plt.subplots()#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.countplot(data=heart, x='target', hue='sex', palette=sns.color_palette(\"mako\"))\nPlotter(plot, 'target', 'count', legend=True, save=True, save_name='Heart Disease Count.png')","2a227c67":"fig, ax=plt.subplots()\nplot=sns.histplot(data=heart, x='age', hue='target', element='step',stat='density',kde=True, palette=sns.color_palette(\"mako\",2))\nPlotter(plot, 'age', 'density', legend=None, save=True, save_name='Age_Hist.png')#For histplots set legend to None. I do not know why the function does not work properly for histplots","d777da7b":"fig, ax=plt.subplots()\nplot=sns.scatterplot(data=heart, x='age', y='trestbps',hue='target', palette=sns.color_palette(\"mako\",2))\nPlotter(plot, 'age', 'trestbps', legend=True, save=True, save_name='Age_Trest.png')","5d1faadc":"fig, ax=plt.subplots()\nplot=sns.boxplot(data=heart, x='thal', y='thalach',hue='target', palette=sns.color_palette(\"mako\",2))\nPlotter(plot, 'thal', 'thalach', legend=True, save=True, save_name='target_thal.png')","9a9278c5":"sns.pairplot(data=heart, hue='target',palette=sns.color_palette(\"mako\",2))","3645764e":"fig, ax=plt.subplots(figsize=(9,9))#Required outside of function. This needs to be activated first when plotting in every code block\nplot=sns.heatmap(heart.corr(),annot=True, cmap='Blues', linewidths=1)\nPlotter(plot, None, None, 90,legend=False, save=True, save_name='Corr.png')","37103cdf":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(heart.drop(['target'], axis=1), heart['target'],test_size=0.30, random_state=0)\n\n#Scaling the Data\nfrom sklearn import preprocessing\n\nscaler=preprocessing.StandardScaler()\nX_train_ss=scaler.fit_transform(X_train)\nX_test_ss=scaler.transform(X_test)","0e6bae14":"from mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom sklearn.ensemble import RandomForestClassifier as RFC\n\nfeature_names=list(X_train.columns)\nfeature_names\nrfc=RFC(n_estimators=250, criterion='entropy', random_state=1)\n\n\n#Setting SFS\nsfs=SFS(estimator=rfc,\n       k_features='best',\n       forward=False,#Backwards elimination\n       floating=True,#floating true, takes whole feature set\n       scoring='accuracy',\n       cv=5,\n       verbose=0)#Shows progress, I set this to 0 to save space\n\n#Fitting the model\nsfs.fit(X_train_ss, y_train, custom_feature_names=feature_names)\n\n#Results\nprint('Best Features are', sfs.k_feature_names_)\nprint('Best Features by index are', sfs.k_feature_idx_)\nprint('Best Score', sfs.k_score_)\n\n#Transforming data\nX_train_fet=sfs.transform(X_train_ss)#Set new variables\nX_test_fet=sfs.transform(X_test_ss)\nprint('New training dimensions are',X_train_fet.shape, 'While testing dimensions are', X_test_fet.shape)\n\noutput=pd.DataFrame.from_dict(sfs.get_metric_dict()).T\noutput.sort_values('avg_score', ascending=False)#Print table with metrics","ffa789b6":"from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV #Paramterizers\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix #Accuracy metrics\nimport itertools #Used for iterations","862d375c":"def Searcher(estimator, param_grid, search, train_x, train_y, test_x, test_y,label=None):\n    \"\"\"\n    This is a helper function for tuning hyperparameters using the two search methods.\n    Methods must be GridSearchCV or RandomizedSearchCV.\n    Inputs:\n        estimator: Any Classifier\n        param_grid: Range of parameters to search\n        search: Grid search or Randomized search\n        train_x: input variable of your X_train variables \n        train_y: input variable of your y_train variables\n        test_x: input variable of your X_test variables\n        test_y: input variable of your y_test variables\n        label: str to print estimator, default=None\n    Output:\n        Returns the estimator instance, clf\n    \"\"\"   \n    \n    try:\n        if search == \"grid\":\n            clf = GridSearchCV(\n                estimator=estimator, \n                param_grid=param_grid, \n                scoring=None,\n                n_jobs=-1, \n                cv=10, #Cross-validation at 10 replicates\n                verbose=0,\n                return_train_score=True\n            )\n        elif search == \"random\":           \n            clf = RandomizedSearchCV(\n                estimator=estimator,\n                param_distributions=param_grid,\n                n_iter=10,\n                n_jobs=-1,\n                cv=10,\n                verbose=0,\n                random_state=1,\n                return_train_score=True\n            )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0) #Exits program if not grid or random\n        \n    # Fit the model\n    clf.fit(X=train_x, y=train_y)\n    \n    #Testing the model\n    try:\n        if search=='grid':\n            cfmatrix=confusion_matrix(\n            y_true=test_y, y_pred=clf.predict(test_x))\n        \n            #Defining prints for accuracy metrics of grid\n            print(\"**Grid search results of\", label,\"**\")\n            print(\"The best parameters are:\",clf.best_params_)\n            print(\"Best training accuracy:\\t\", clf.best_score_)\n            print('Classification Report:')\n            print(classification_report(y_true=test_y, y_pred=clf.predict(test_x))\n             )\n        elif search == 'random':\n            cfmatrix=confusion_matrix(\n            y_true=test_y, y_pred=clf.predict(test_x))\n\n            #Defining prints for accuracy metrics of grid\n          \n            print(\"**Random search results of\", label,\"**\")\n            print(\"The best parameters are:\",clf.best_params_)\n            print(\"Best training accuracy:\\t\", clf.best_score_)\n            print('Classification Report:')\n            print(classification_report(y_true=test_y, y_pred=clf.predict(test_x))\n                 )\n    except:\n        print('Search argument has to be \"grid\" or \"random\"')\n        sys.exit(0) #Exits program if not grid or random\n        \n    return clf, cfmatrix; #Returns a trained classifier with best parameters","6eaf29bd":"def plot_confusion_matrix(cm, label,color=None,title=None):\n    \"\"\"\n    Plot for Confusion Matrix:\n    Inputs:\n        cm: sklearn confusion_matrix function for y_true and y_pred as seen in https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html\n        title: title of confusion matrix as a 'string', default=None\n        label: the unique label that represents classes for prediction can be done as sorted(dataframe['labels'].unique()).\n        color: confusion matrix color, default=None, set as a plt.cm.color, based on matplot lib color gradients\n    \"\"\"\n    classes=sorted(label)\n    plt.imshow(cm, interpolation='nearest', cmap=color)\n    plt.title(title)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    thresh = cm.mean()\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j]), \n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] < thresh else \"black\") ","73eb1b76":"from sklearn.linear_model import LogisticRegression as LR\n\n#Setting searcher parameters\n#Grid Search\nlog_param={\"C\":[.01, .1, 1, 5, 10, 100],#Specific parameters to be tested at all combinations\n          \"max_iter\":[100,250,500,750,1000],\n          \"random_state\":[1]}\n\nlog_grid, cfmatrix_grid= Searcher(LR(), log_param, \"grid\", X_train_fet, y_train, X_test_fet, y_test,label='LogReg')\n\nprint('_____'*20)\n\n#Randomized Search \nlog_dist = {\n    \"C\": np.arange(0.01,100, 0.01),   #By using np.arange it will select from randomized values\n    \"max_iter\": np.arange(100,1000, 5),\n    \"random_state\": [1]}\n\nlog_rand, cfmatrix_rand= Searcher(LR(), log_dist, \"random\", X_train_fet, y_train, X_test_fet, y_test, label='LogReg')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=heart['target'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=heart['target'].unique(), color=plt.cm.cividis) #randomized matrix function\n\nplt.savefig('LogReg_confusion.png')","b13e98b0":"from sklearn.tree import DecisionTreeClassifier as DTC\n\ndepth=np.arange(1,20, 1)\n#Setting searcher parameters\n#Grid Search\ndtc_param={\"criterion\":['entropy',\"gini\"],\n           'max_depth':[None, depth],\n    \"min_samples_split\":np.arange(2,20, 1),\n          \"min_samples_leaf\":np.arange(2,20, 1),\n          \"random_state\":[1]}\n\ndtc_grid, cfmatrix_grid= Searcher(DTC(), dtc_param, \"grid\", X_train_fet, y_train, X_test_fet, y_test,label='Tree')\n\nprint('_____'*20)\n\n#Randomized Search \ndtc_dist = {\n    \"criterion\":['entropy',\"gini\"],\n           'max_depth':[None, depth],\n    \"min_samples_split\":np.arange(2,20, 1),\n          \"min_samples_leaf\":np.arange(2,20, 1),\n          \"random_state\":[1]}\n\ndtc_rand, cfmatrix_rand= Searcher(DTC(), dtc_dist, \"random\", X_train_fet, y_train, X_test_fet, y_test, label='Tree')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=heart['target'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=heart['target'].unique(), color=plt.cm.cividis) #randomized matrix function\n\nplt.savefig('DTC_confusion.png')","bda96918":"from sklearn.neighbors import KNeighborsClassifier as KNN\n\n#Setting searcher parameters\n#Grid Search\nknn_param={\"n_neighbors\":[1,2,3,4,5, 10, 15, 20],\n           'weights':['uniform','distance'],\n    \"algorithm\":['ball_tree', 'kd_tree', 'brute'],\n          \"p\":[1,2],\n          }\n\nknn_grid, cfmatrix_grid= Searcher(KNN(), knn_param, \"grid\", X_train_fet, y_train, X_test_fet, y_test,label='KNN')\n\nprint('_____'*20)\n\n#Randomized Search \nknn_dist = {\"n_neighbors\":np.arange(1,40,1),\n           'weights':['uniform','distance'],\n    \"algorithm\":['ball_tree', 'kd_tree', 'brute'],\n          \"p\":[1,2],\n          }\n\nknn_rand, cfmatrix_rand= Searcher(KNN(), knn_dist, \"random\", X_train_fet, y_train, X_test_fet, y_test, label='KNN')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=heart['target'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=heart['target'].unique(), color=plt.cm.cividis) #randomized matrix function\n\nplt.savefig('KNN_confusion.png')","6e7d55ed":"from sklearn.svm import SVC #Support Vector Classifier\n\n#Grid Search SVM Parameters\nsvm_param = {\n    \"C\": [.01, .1, 1, 5, 10, 100], #Specific parameters to be tested at all combinations\n    \"gamma\": [0, .01, .1, 1],\n    \"kernel\": [\"rbf\",\"linear\",\"poly\"],\n    \"degree\": [3,4],\n    \"random_state\": [1]}\n\n#Randomized Search SVM Parameters\nsvm_dist = {\n    \"C\": np.arange(0.01,100, 0.01),   #By using np.arange it will select from randomized values\n    \"gamma\": np.arange(0,1, 0.01),\n    \"kernel\": [\"rbf\",\"linear\",\"poly\"],\n    \"degree\": [3,4],\n    \"random_state\": [1]}\n\n#Grid Search \nsvm_grid, cfmatrix_grid= Searcher(SVC(), svm_param, \"grid\", X_train_fet, y_train, X_test_fet, y_test,label='SVC')\n\nprint('_____'*20)#Spacer\n\n#Random Search\nsvm_rand, cfmatrix_rand= Searcher(SVC(), svm_dist, \"random\", X_train_fet, y_train, X_test_fet, y_test,label='SVC')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=heart['target'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=heart['target'].unique(), color=plt.cm.cividis) #randomized matrix function\nplt.savefig('SVM_confusion.png')","10c6c362":"#Grid Search RFC Parameters\nrfc_param = {\n    \"n_estimators\": [10, 50, 75, 100, 150,200], #Specific parameters to be tested at all combinations\n    \"criterion\": ['entropy','gini'],\n    \"random_state\": [1],\n    \"max_depth\":np.arange(1,16,1)}\n\n#Randomized Search RFC Parameters\nrfc_dist = {\n    \"n_estimators\": np.arange(10,200, 10),   #By using np.arange it will select from randomized values\n    \"criterion\": ['entropy','gini'],\n    \"random_state\": [1],\n    \"max_depth\":np.arange(1,16,1)}\n\n#Grid Search RFC\nrfc_grid, cfmatrix_grid= Searcher(RFC(), rfc_param, \"grid\", X_train_fet, y_train, X_test_fet, y_test,label='RFC')\n\nprint('_____'*20)#Spacer\n\n#Random Search RFC\nrfc_rand, cfmatrix_rand= Searcher(RFC(), rfc_dist, \"random\", X_train_fet, y_train, X_test_fet, y_test,label='RFC')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=heart['target'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=heart['target'].unique(), color=plt.cm.cividis) #randomized matrix function\nplt.savefig('RFC_confusion.png')","079539e7":"from sklearn.neural_network import MLPClassifier as MLP\n\n#Grid Search MLP Parameters\nmlp_param = {\n    \"hidden_layer_sizes\": [(9,),(9,6),(9,6,3),(9,6,3,1)], #Specific parameters to be tested at all combinations\n    \"activation\": ['identity', 'logistic', 'tanh', 'relu'],\n    \"max_iter\":[200,400,600,800,1000],\n    \"solver\":['lbfgs', 'sgd', 'adam'],\n    \"learning_rate_init\":[0.01],\n    \"learning_rate\":['constant','adaptive'],\n    \"random_state\": [1]}\n\n#Randomized Search MLP Parameters\nsini=np.arange(6,9,1)\non_si=np.arange(1,6,1)\non_th=np.arange(1,3,1)\nmlp_dist = {\n    \"hidden_layer_sizes\": [(9,),(9,6),(9,6,3),(9,6,3,1)], #Specific parameters to be tested at all combinations\n    \"activation\": ['identity', 'logistic', 'tanh', 'relu'],\n    \"max_iter\":np.arange(100,1000, 100),\n    \"solver\":['lbfgs', 'sgd', 'adam'],\n    \"learning_rate_init\":np.arange(0.001,0.01,0.001),\n    \"learning_rate\":['constant','adaptive'],\n    \"random_state\": [1]}\n\n#Grid Search SVM\nmlp_grid, cfmatrix_grid= Searcher(MLP(), mlp_param, \"grid\", X_train_fet, y_train, X_test_fet, y_test,label='MLP')\n\nprint('_____'*20)#Spacer\n\n#Random Search SVM\nmlp_rand, cfmatrix_rand= Searcher(MLP(), mlp_dist, \"random\", X_train_fet, y_train, X_test_fet, y_test,label='MLP')\n\n#Plotting the confusion matrices\nplt.subplots(1,2)\nplt.subplots_adjust(left=-0.5, bottom=None, right=None, top=None, wspace=0.5, hspace=None)\nplot_confusion_matrix(cfmatrix_rand, title='Random Search Confusion Matrix',label=heart['target'].unique(), color=plt.cm.cividis) #grid matrix function\nplt.subplot(121)\nplot_confusion_matrix(cfmatrix_grid, title='Grid Search Confusion Matrix', label=heart['target'].unique(), color=plt.cm.cividis) #randomized matrix function\nplt.savefig('MLP_confusion.png')","bc0b0084":"# Importing the data\nLets first look at the heart disease uci data. We will print the info to check for null values then show the first 5 rows of the data set.","ba518051":"### Random Forest (again)\nSince we took our selected features from random forest lets try this model out anyway. Random forest is an ensemble method that estimates several weak decision trees and combines the mean to create an uncorrelated forest at the end. The uncorrelated forest should be able to predict more accurately than an individual tree.","0461e526":"### Confusion Matrix Function\nSetting aesthetics for confusion matrices is also easy through using functions as well. As can be seen below. When using this function, all inputs are required except title.","60d53d8d":"Again, we do not see much different here. We can summarize the data in a multivariate manner by plotting a pairplot.","f8bcc285":"Now that we know what type of data we have lets move right into exploratory data analysis (EDA).\n# EDA\nThe purpose of EDA is to get a feel for the data. You should not start modeling until feeling out the data first. I will first show a script for a plotter function I use will makes easy reproducible figures that are publication quality. They are based on seaborn plotting and require few lines of code whenever you do want to plot. \n\n## Plotter","717b82f2":"Now that we have our features selected lets model and predict the data.\n# Modeling the Data for Prediction\nWe will first set up a grid and randomized search functions below.\n## Search Function\nWe can evalute multiple parameters at one using Grid or Randomization Search functions. Grid Search evalutes several input parameters at all combinations input while randomized search looks for the best. Cross-validation is the models self assemessment when trying to find the best parameters on the training data and can be done in \"n\" amount of replicates.","7a25e744":"### Decision Tree\nVery simple to understand model that makes decision rulings to predict target values. However, limited by overfitting and sensitive to data variation. (I do not have much hope for this one!).","4545e630":"Now that the data is split and scaled. Lets move into feature selection.\nThe MLxtend library provided various tools for feature selection. I decided to look at sequential feature selection as oppossed to recursive feature selection cause I want to algorithm to either eliminate or add features not just remove. (May take a few minutes)","c230cb48":"### Multi-Layer Perceptron\nFeed-forward neural network. Very simple compared to using tensor flow or keras,however, may not be as powerful. The number of nodes are determined by (2\/3 * input feature count) + (number of output + 2). The number of layers were decided by 2\/3 of the first and 1\/2 the second layer. We can paramterize plenty of activator functions and set this up with the search function above. Running the searches with this code may take about 30-60min.","cff91074":"### K-Nearest Neighbors\nSeeks the closest distance of nearest neighbors to make a prediction based on a majority vote of nearest data points.","dd4e8656":"We see there is no null values. Upon inspection of the table we see that there are only 0-3. Lets print out those features to see how high they go.","6e4f109f":"# Conclusion\nWe observed that:\n* We can use sequential feature selection in tandem with a classifier to provide us with the best features for machine learning.\n* Using simplistic models can guide us for the need for more sophisticated models\n* Again, there is no one sole method to machine learning as I will show in parts two and three.\n* Our modeling overall has scored around ~80% at 30% train-test-split, therefore some parts of the data may just have some overlap that are outliers.\n\n# Next Steps\nIn the next two notebooks we will see the application of KBest feature selection and compare dimensionality reduction techniques such as PCA and LDA to cluster data. These will be other approaches to selecting features.\n\n# Notebooks:\nVarious Approaches to ML: Part 2: Soon to be published  \nVarious Approaches to ML: Part 3: Not yet started!\n\n## Suggestions?\n\n### If you stuck around to the end please leave a comment for feedback or upvote!<\/b>\n\nLike what I have done? Check out my other notebooks here: https:\/\/www.kaggle.com\/christopherwsmith  \n\nSelected Notebooks That Are Helpful!  \nhttps:\/\/www.kaggle.com\/christopherwsmith\/classify-that-penguin-100-accuracy\n\nhttps:\/\/www.kaggle.com\/christopherwsmith\/tutorial-quick-custom-and-helpful-functions\n\nhttps:\/\/www.kaggle.com\/christopherwsmith\/how-to-predict-lung-cancer-levels-100-accuracy","37097d33":"We really do not see too much separation. Lets look at pearson correlation coefficients to see if there is any dependencies within the data.","7dd45019":"There really is not too much of a difference here. Lets look at other factors.","4e092267":"Plotting for our target for heart disease, we see that the data is almost balanced meaning we will not have to worry about target bias in modeling later. Plotting by gender as well shows that there are more cases of gender 1, this may hint that it may not be too important later. Lets put age into a histogram and see if it makes a difference.","d0bdb277":"Even though a better training accuracy was slightly achieved we are having a difficult time distinguishing the remaining 20% of the test data. Lets try some more sophisticated models.\n### Support Vector Machine\nUsing the support vector machines classifiers (SVC) it can handle higher dimensional data and generate hyperplanes for separation and score on a yes (1) no (1) basis.","22e638d3":"# Various Approaches to ML: Part 1\n\n## Hello World\n\nHello, this is a series of notebooks dedicated to approaching machine learning (ML) problems in different ways. The overall goal of machine learning is to being able to develop a reliable model that has high prediction accuarcy. I have noticed many kaggelers asking questions such as how to approach ML problems. There is no wrong way to approaching but there are ways to getting great results (given the data is not junk). One thing to keep in mind is that machine learning is dynamic, therefore not one approach is going to give you the right answer everytime and may require trial by fire.<b> The goal of this series is to demonstrate a few different approaches while incorporating a general flow. <\/b> Typically you want to clean the data, perform EDA, then apply some sort of preprocess-feature selection (either order) then model and set hyperparameters. So I hope you will stick around to see different ways that I approach problems. I will be incoporating different data sets into this series.\n\n<b> Please upvote or comment if you see something that I could do different or that you like! <\/b>\n\n## Part 1: Heart Classification Data\n## Part 2: Fetal Health Data\n## Part 3: Wisconsin Breast Cancer Data\n\n![image.png](attachment:image.png)\n\n# Importing relevant libraries.\nLets first import relevant libraries to get us started.","d98cfde2":"Based on the correlation matrix there is some moderate correlation on target in presence of certain features. We will apply a feature selection tool to tell us but first we will run train-test split.\n\n# Splitting the Data\nAs part of ML we split our data set into a training and validation datasets, or in this case a training and testing set as there is no separate file provided. We will then scale the data and move into feature selection.","5be2735b":"It looks like those between the age of 40-55 are most at risk of getting the disease. Lets continue to bivariate.\n## Bivariate\nNow we will look at two variables at the same time and see if there is separation.","087d5be8":"## Modeling with Various Algoirthms\nFirst we will try with simple models such as Logistic Regression, Decision Trees and K-Nearest Neighbors. If those fail then we will look at more powerful models.\n### Logistic Regression\nTypically used for binary outputs, uses a sigmoid function to determine the probability of a specific binary class (yes or no).","f052f3d9":"### Searcher","db9076f4":"## Univariate\nLets look at our categorical and numerical data first individually to understand it."}}