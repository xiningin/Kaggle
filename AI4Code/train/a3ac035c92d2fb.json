{"cell_type":{"25bd3923":"code","f7aa494d":"code","b1a1c625":"code","319041c8":"code","6b5e7769":"code","e0626fef":"code","0637476d":"code","15662aff":"code","7f35f4d1":"code","77da4b57":"code","ca3d81fd":"code","b20ab57a":"code","946edcdf":"code","b2b0cbee":"code","21619f32":"code","e94c1714":"code","2ed7e3de":"code","9307988e":"code","63e86ec8":"code","83bf5a79":"code","1b2165c7":"code","7ace15ab":"code","efde9ef1":"code","288d720e":"code","87a8ecc0":"code","0375cdb9":"code","653ea873":"code","72256227":"code","32424e4e":"code","b0ff071e":"code","654738a4":"code","6ac8e56c":"code","e339a3b6":"code","fcbd8745":"markdown","39cff7af":"markdown","9df8bd37":"markdown","f098d1cd":"markdown","d5e7210d":"markdown","572641d6":"markdown","66f06e18":"markdown","5aef8182":"markdown"},"source":{"25bd3923":"import gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\nimport re\nimport nltk\nimport spacy\nimport string\nfrom textblob import TextBlob\nfrom collections import Counter\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adamax\nfrom tensorflow.keras.initializers import LecunNormal\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.layers import LayerNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Attention, Dropout\nfrom tensorflow.keras.layers import Dense, Input, Conv1D\nfrom tensorflow_addons.layers import WeightNormalization\nfrom tensorflow.keras.layers import GlobalAveragePooling1D\nfrom tensorflow.keras.layers import LSTM, Flatten, Bidirectional\nfrom tensorflow.keras.layers import Activation, SpatialDropout1D\nfrom tensorflow.keras.layers import GlobalMaxPool1D, Concatenate\n\nfrom transformers import RobertaTokenizer, TFRobertaModel, RobertaConfig","f7aa494d":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df.drop(['url_legal','license','standard_error'], inplace=True, axis=1)\ntrain_df.set_index(\"id\", inplace=True)\nprint(f\"train_df: {train_df.shape}\\n\")\ntrain_df.head()","b1a1c625":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df.drop(['url_legal','license'], inplace=True, axis=1)\ntest_df.set_index(\"id\", inplace=True)\nprint(f\"test_df: {test_df.shape}\\n\")\ntest_df.head()","319041c8":"_, ax = plt.subplots(1, 2, figsize=(15, 5))\nsns.boxplot(x='target', data=train_df, ax=ax[0])\nsns.histplot(x='target', data=train_df, ax=ax[1])\nax[0].title.set_text('Box Plot - target')\nax[1].title.set_text('Hist Plot - target')","6b5e7769":"Ytrain = train_df['target'].values\nYtrain_strat = pd.qcut(train_df['target'].values, q=5, labels=range(0,5))\ntrain_df.drop(['target'], inplace=True, axis=1)\nprint(\"Ytrain: {}\".format(Ytrain.shape))","e0626fef":"def contraction_count(sent):\n    count = 0\n    count += re.subn(r\"won\\'t\", '', sent)[1]\n    count += re.subn(r\"can\\'t\", '', sent)[1]\n    count += re.subn(r\"n\\'t\", '', sent)[1]\n    count += re.subn(r\"\\'re\", '', sent)[1]\n    count += re.subn(r\"\\'s\", '', sent)[1]\n    count += re.subn(r\"\\'d\", '', sent)[1]\n    count += re.subn(r\"\\'ll\", '', sent)[1]\n    count += re.subn(r\"\\'t\", '', sent)[1]\n    count += re.subn(r\"\\'ve\", '', sent)[1]\n    count += re.subn(r\"\\'m\", '', sent)[1]\n    return count","0637476d":"def pos_count(sent):\n    nn_count = 0   #Noun\n    pr_count = 0   #Pronoun\n    vb_count = 0   #Verb\n    jj_count = 0   #Adjective\n    uh_count = 0   #Interjection\n    cd_count = 0   #Numerics\n    \n    sent = nltk.word_tokenize(sent)\n    sent = nltk.pos_tag(sent)\n\n    for token in sent:\n        if token[1] in ['NN','NNP','NNS']:\n            nn_count += 1\n\n        if token[1] in ['PRP','PRP$']:\n            pr_count += 1\n\n        if token[1] in ['VB','VBD','VBG','VBN','VBP','VBZ']:\n            vb_count += 1\n\n        if token[1] in ['JJ','JJR','JJS']:\n            jj_count += 1\n\n        if token[1] in ['UH']:\n            uh_count += 1\n\n        if token[1] in ['CD']:\n            cd_count += 1\n    \n    return pd.Series([nn_count, pr_count, vb_count, jj_count, uh_count, cd_count])","15662aff":"def dialog_parser(text):\n    \n    tokenized = nltk.word_tokenize(text)\n    \n    # let's set up some lists to hold our pieces of narrative and dialog\n    parsed_dialog = []\n    parsed_narrative = []\n    \n    # and this list will be a bucket for the text we're currently exploring\n    current = []\n\n    # now let's set up values that will help us loop through the text\n    length = len(tokenized)\n    found_q = False\n    counter = 0\n    quote_open, quote_close = '``', \"''\"\n\n    # now we'll start our loop saying that as long as our sentence is...\n    while counter < length:\n        word = tokenized[counter]\n\n        # until we find a quotation mark, we're working with narrative\n        if quote_open not in word and quote_close not in word:\n            current.append(word)\n\n        # here's what we do when we find a closed quote\n        else:\n            # we append the narrative we've collected & clear our our\n            # current variable\n            parsed_narrative.append(current)\n            current = []\n            \n            # now current is ready to hold dialog and we're working on\n            # a piece of dialog\n            current.append(word)\n            found_q = True\n\n            # while we're in the quote, we're going to increment the counter\n            # and append to current in this while loop\n            while found_q and counter < length-1:\n                counter += 1\n                if quote_close not in tokenized[counter]:\n                    current.append(tokenized[counter])\n                else:\n                    # if we find a closing quote, we add our dialog to the\n                    # appropriate list, clear current and flip our found_q\n                    # variable to False\n                    current.append(tokenized[counter])\n                    parsed_dialog.append(current)\n                    current = []\n                    found_q = False\n\n        # increment the counter to move us through the text\n        counter += 1\n    \n    if len(parsed_narrative) == 0:\n        parsed_narrative.append(current)\n    \n    mean_dialog_word_len = 0\n    \n    if len(parsed_dialog) > 0:\n        for text in parsed_dialog:\n            join_text = \" \".join(text)\n            join_text = join_text.replace('\"','')\n            join_text = join_text.replace(\"''\",\"\")\n            mean_dialog_word_len += len(join_text.split())\n        \n        mean_dialog_word_len \/= float(len(parsed_dialog))\n    \n    mean_narrative_word_len = 0\n    \n    if len(parsed_narrative) > 0:\n        for text in parsed_narrative:\n            join_text = \" \".join(text)\n            join_text = join_text.replace('\"','')\n            join_text = join_text.replace(\"''\",\"\")\n            mean_narrative_word_len += len(join_text.split())\n        \n        mean_narrative_word_len \/= float(len(parsed_narrative))\n\n    return len(parsed_dialog), len(parsed_narrative), mean_dialog_word_len, mean_narrative_word_len","7f35f4d1":"def decontraction(phrase):\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","77da4b57":"def remove_punctuations(text):\n    for punctuation in list(string.punctuation):\n        text = text.replace(punctuation, '')\n    return text","ca3d81fd":"def lemmatize_words(text):\n    lemmatizer = WordNetLemmatizer()\n    wordnet_map = {\n        \"N\": wordnet.NOUN, \n        \"V\": wordnet.VERB, \n        \"J\": wordnet.ADJ, \n        \"R\": wordnet.ADV\n    }\n    pos_tagged_text = nltk.pos_tag(text.split())\n    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])","b20ab57a":"combined_df = train_df.append(test_df, sort=False, ignore_index=False)\n\ndel train_df\ndel test_df\ngc.collect()\n\ncombined_df.head()","946edcdf":"cnt = Counter()\nfor text in combined_df[\"excerpt\"].values:\n    for word in text.split():\n        cnt[word] += 1\n\n\nfreq_words = []\nfor (w, wc) in tqdm(cnt.most_common()):\n    if wc > 3500:\n        freq_words.append(w)\n\nlen(freq_words)","b2b0cbee":"combined_df[\"excerpt_num_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x).split()))\ncombined_df[\"excerpt_num_unique_words\"] = combined_df[\"excerpt\"].apply(lambda x: len(set(str(x).split())))\ncombined_df[\"excerpt_num_chars\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x)))\ncombined_df[\"excerpt_num_stopwords\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\ncombined_df[\"excerpt_num_punctuations\"] =combined_df['excerpt'].apply(lambda x: len([c for c in str(x) if c in list(string.punctuation)]))\ncombined_df[\"excerpt_num_words_upper\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ncombined_df[\"excerpt_num_words_title\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ncombined_df[\"excerpt_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ncombined_df[\"excerpt_num_paragraphs\"] = combined_df[\"excerpt\"].apply(lambda x: len(x.split('\\n')))\ncombined_df[\"excerpt_num_sentences\"] = combined_df[\"excerpt\"].apply(lambda x: len(str(x).split('.')))\ncombined_df[\"excerpt_num_contractions\"] = combined_df[\"excerpt\"].apply(contraction_count)\ncombined_df[\"excerpt_freq_words\"] = combined_df[\"excerpt\"].apply(lambda x: len([w for w in str(x).split() if w in freq_words]))\ncombined_df[\"excerpt_num_dialog\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[0])\ncombined_df[\"excerpt_num_narrative\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[1])\ncombined_df[\"excerpt_dialog_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[2])\ncombined_df[\"excerpt_narrative_mean_word_len\"] = combined_df[\"excerpt\"].apply(lambda x: dialog_parser(x)[3])\ncombined_df['excerpt_polarity'] = combined_df['excerpt'].apply(lambda x: TextBlob(x).sentiment[0])\ncombined_df['excerpt_subjectivity'] = combined_df['excerpt'].apply(lambda x: TextBlob(x).sentiment[1])\ncombined_df[['nn_count','pr_count','vb_count','jj_count','uh_count','cd_count']] = combined_df['excerpt'].apply(pos_count)\ncombined_df.head()","21619f32":"df = combined_df[:Ytrain.shape[0]].copy()\ndf['target'] = Ytrain\nplt.figure(figsize=(20, 15))\nsns.heatmap(df.corr(), annot=True, fmt='.2f', cmap=\"RdYlGn\")","e94c1714":"# Convert to lower case\ncombined_df['excerpt'] = combined_df['excerpt'].apply(lambda x: str(x).lower().replace('\\\\', '').replace('_', ' '))\n\n# Remove double spaces\ncombined_df['excerpt'] = combined_df['excerpt'].apply(lambda x: re.sub('\\s+',  ' ', x))\n\n# Replace contractions (\"don't\" with \"do not\" and \"we've\" with \"we have\")\ncombined_df['excerpt'] = combined_df['excerpt'].apply(lambda x: decontraction(x))\n\n# Remove punctuations\ncombined_df['excerpt'] = combined_df['excerpt'].apply(remove_punctuations)\n\n# Lemmatize words\ncombined_df['excerpt'] = combined_df['excerpt'].apply(lambda text: lemmatize_words(text))\n\ncombined_df.head()","2ed7e3de":"# Extract excerpts\ntrain_list = combined_df[:Ytrain.shape[0]]['excerpt'].tolist()\ntest_list = combined_df[Ytrain.shape[0]:]['excerpt'].tolist()\nMAX_LEN = combined_df['excerpt_num_words'].max() + 11\nprint(f\"Train excerpts: {len(train_list)} \\nTest excerpts: {len(test_list)} \\nMAX_LEN: {MAX_LEN}\")","9307988e":"selected_features = ['excerpt_num_words','excerpt_num_unique_words','excerpt_num_chars',\n                     'excerpt_num_punctuations','excerpt_num_words_title','excerpt_mean_word_len',\n                     'excerpt_num_paragraphs','excerpt_num_contractions','excerpt_freq_words',\n                     'excerpt_num_dialog','excerpt_narrative_mean_word_len','nn_count','pr_count',\n                     'vb_count','jj_count','uh_count','cd_count']\n\ncombined_df = combined_df[selected_features].copy()","63e86ec8":"Xtrain = combined_df[:Ytrain.shape[0]].copy()\nXtest = combined_df[Ytrain.shape[0]:].copy()\nprint(f\"Xtrain: {Xtrain.shape} \\nXtest: {Xtest.shape}\")\n\ndel combined_df\ngc.collect()","83bf5a79":"DISTILROBERTA_BASE = \"..\/input\/huggingface-roberta-variants\/distilroberta-base\/distilroberta-base\"","1b2165c7":"def sent_encode(texts, tokenizer):\n    input_ids = []\n    attention_mask = []\n\n    for text in tqdm(texts):\n        tokens = tokenizer.encode_plus(text, max_length=MAX_LEN, truncation=True, \n                                       padding='max_length', add_special_tokens=True, \n                                       return_attention_mask=True, return_token_type_ids=False, \n                                       return_tensors='tf')\n        \n        input_ids.append(tokens['input_ids'])\n        attention_mask.append(tokens['attention_mask'])\n\n    return np.array(input_ids), np.array(attention_mask)","7ace15ab":"tokenizer = RobertaTokenizer.from_pretrained(DISTILROBERTA_BASE)","efde9ef1":"Xtrain_id, Xtrain_mask = sent_encode(train_list, tokenizer)\n\nXtrain_id = Xtrain_id.reshape((Xtrain_id.shape[0], Xtrain_id.shape[2]))\nXtrain_mask = Xtrain_mask.reshape((Xtrain_mask.shape[0], Xtrain_mask.shape[2]))\n    \nprint(f\"Input-ids: {Xtrain_id.shape} \\nAttention Mask: {Xtrain_mask.shape}\")","288d720e":"Xtest_id, Xtest_mask = sent_encode(test_list, tokenizer)\n\nXtest_id = Xtest_id.reshape((Xtest_id.shape[0], Xtest_id.shape[2]))\nXtest_mask = Xtest_mask.reshape((Xtest_mask.shape[0], Xtest_mask.shape[2]))\n    \nprint(f\"Input-ids: {Xtest_id.shape} \\nAttention Mask: {Xtest_mask.shape}\")","87a8ecc0":"def rmse_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, dtype=tf.float32)\n    y_pred = tf.cast(y_pred, dtype=tf.float32)\n    return tf.math.sqrt(tf.math.reduce_mean((y_true - y_pred)**2))","0375cdb9":"def distilroberta_model(transformer_model):\n    \n    input_id = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n    attention_mask = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"attention_mask\")\n\n    embed = transformer_model(input_id, attention_mask=attention_mask)[0]    \n    embed = LayerNormalization(name='Embedding')(embed)\n    \n    x = WeightNormalization(\n            Conv1D(filters=384, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(embed)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = WeightNormalization(\n            Conv1D(filters=192, kernel_size=5, \n                   strides=2, padding='same', \n                   kernel_regularizer=l2(0.0001),\n                   kernel_initializer='he_uniform'))(x)\n    x = LayerNormalization()(x)\n    x = Activation('relu')(x)\n    x = SpatialDropout1D(rate=0.25)(x)\n    \n    x = Flatten()(x)\n    x = Dropout(rate=0.5)(x)\n    \n    x = Dense(units=1, kernel_initializer='lecun_normal')(x)\n\n    model = Model(inputs=[input_id, attention_mask], outputs=x, \n                  name='Pretrained_DistilRoberta_Model')\n    return model","653ea873":"def build_model(distilroberta_embeddings, stat_input, seed=0):\n    \n    x1 = Bidirectional(LSTM(units=96, activation='tanh',\n                            return_sequences=True, dropout=0.25,\n                            kernel_regularizer=l2(0.0001),\n                            kernel_initializer=LecunNormal(seed=seed)))(distilroberta_embeddings)\n    x1 = LayerNormalization(epsilon=1e-5)(x1)\n    \n    x2 = Conv1D(filters=96, kernel_size=5, \n                strides=2, padding='same', \n                kernel_regularizer=l2(0.0003),\n                kernel_initializer=LecunNormal(seed=seed))(x1)\n    x2 = Activation('relu')(x2)\n    x2 = LayerNormalization(epsilon=1e-5)(x2)\n    x2 = SpatialDropout1D(rate=0.25)(x2)\n    \n    x2 = Conv1D(filters=192, kernel_size=3, \n                strides=2, padding='same', \n                kernel_regularizer=l2(0.0003),\n                kernel_initializer=LecunNormal(seed=seed))(x2)\n    x2 = Activation('relu')(x2)\n    x2 = LayerNormalization(epsilon=1e-5)(x2)\n    x2 = SpatialDropout1D(rate=0.25)(x2)\n    \n    attn = Attention()([x2, x1])\n    \n    avg_pool2 = GlobalAveragePooling1D()(x2)\n    avg_pool3 = GlobalAveragePooling1D()(attn)\n    \n    max_pool2 = GlobalMaxPool1D()(x2)\n    max_pool3 = GlobalMaxPool1D()(attn)\n    \n    x3 = Dense(units=32, kernel_initializer=LecunNormal(seed=seed), \n               kernel_regularizer=l2(0.0003))(stat_input)\n    x3 = Activation('relu')(x3)\n    x3 = BatchNormalization()(x3)\n    \n    x4 = Concatenate()([avg_pool2, avg_pool3, max_pool2, max_pool3])\n    x4 = BatchNormalization()(x4)\n    x5 = Dropout(rate=0.35)(x4)\n    \n    x5 = Dense(units=96, kernel_initializer=LecunNormal(seed=seed), \n               kernel_regularizer=l2(0.0003))(x5)\n    x5 = Activation('relu')(x5)\n    x5 = BatchNormalization()(x5)\n    \n    x = Concatenate()([x5, x3, x4])\n    x = BatchNormalization()(x)\n    x = Dropout(rate=0.35)(x)\n    \n    x = Dense(units=24, kernel_initializer=LecunNormal(seed=seed), \n              kernel_regularizer=l2(0.0001))(x)\n    x = Activation('relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(rate=0.15)(x)\n    \n    x = Dense(units=1, kernel_initializer=LecunNormal(seed=seed))(x)\n    \n    return x","72256227":"config = RobertaConfig.from_pretrained(DISTILROBERTA_BASE)\nconfig.output_hidden_states = False\ntransformer_model = TFRobertaModel.from_pretrained(DISTILROBERTA_BASE, config=config)","32424e4e":"pretrained_model = distilroberta_model(transformer_model)\npretrained_model.trainable = False","b0ff071e":"stat_input = Input(shape=(Xtrain.shape[1],), dtype=tf.int32, name=\"numerical_input\")\nmodel = Model(inputs=[pretrained_model.input, stat_input], \n              outputs=build_model(pretrained_model.get_layer('Embedding').output, stat_input),\n              name='CommonLit_Readability_Model')\nmodel.summary()","654738a4":"plot_model(\n    model, to_file='.\/CommonLit_Readability_Model.png', \n    show_shapes=True, show_layer_names=True\n)","6ac8e56c":"FOLD = 5\nVERBOSE = 1\nMINI_BATCH_SIZE = 16\nSEEDS = [2020]\n\ncounter = 0\noof_score1 = 0\noof_score2 = 0\ny_pred_final = 0\n\n\nfor sidx, seed in enumerate(SEEDS):\n    seed_score1 = 0\n    seed_score2 = 0\n    \n    kfold = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=seed)\n\n    for idx, (train, val) in enumerate(kfold.split(Xtrain, Ytrain_strat)):\n        counter += 1\n\n        train_x_id, train_x_mask = Xtrain_id[train], Xtrain_mask[train]\n        val_x_id, val_x_mask = Xtrain_id[val], Xtrain_mask[val]\n        train_x2, val_x2 = Xtrain.iloc[train], Xtrain.iloc[val]\n        train_y, val_y = Ytrain[train], Ytrain[val]\n        \n        \n        #==================================================================\n        #                 Load pretrained DistilBert Model\n        #==================================================================\n        \n        pretrained_model = distilroberta_model(transformer_model)\n        pretrained_model.load_weights(f'..\/input\/commonlit-roberta-variants-p1\/DistilRoberta-Base\/CLRP_DistilRoberta_Base_{(idx+1)}C.h5')\n        pretrained_model.trainable = False\n        \n        \n        #==================================================================\n        #                              Model-1\n        #==================================================================\n        \n        tf.random.set_seed(seed+idx+13)\n\n        stat_input = Input(shape=(Xtrain.shape[1],), dtype=tf.int32, name=\"numerical_input\")\n        model1 = Model(inputs=[pretrained_model.input, stat_input], \n                       outputs=build_model(pretrained_model.get_layer('Embedding').output, stat_input, seed+idx+13),\n                       name='CommonLit_Readability_Model')\n        \n        model1.compile(loss=rmse_loss,\n                       metrics=[RootMeanSquaredError(name='rmse')],\n                       optimizer=Adamax(lr=8e-3))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=9, verbose=VERBOSE)\n\n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.25, \n                                      min_lr=1e-6, patience=4, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'.\/CommonLit_Readability_Model1_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model1.fit(\n            [train_x_id, train_x_mask, train_x2], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=50, \n            verbose=VERBOSE, \n            workers=5,\n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x2], val_y)\n        )\n        \n        model1.load_weights(f'.\/CommonLit_Readability_Model1_{counter}C.h5')\n\n        y_pred = model1.predict([val_x_id, val_x_mask, val_x2])\n        score1 = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score1 += score1\n        seed_score1 += score1\n        print(\"Model-1 | Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score1))\n        \n        \n        #==================================================================\n        #                              Model-2\n        #==================================================================\n        \n        tf.random.set_seed(seed+idx+23)\n        \n        stat_input = Input(shape=(Xtrain.shape[1],), dtype=tf.int32, name=\"numerical_input\")\n        model2 = Model(inputs=[pretrained_model.input, stat_input], \n                       outputs=build_model(pretrained_model.get_layer('Embedding').output, stat_input, seed+idx+23),\n                       name='CommonLit_Readability_Model')\n        \n        model2.compile(loss=rmse_loss,\n                       metrics=[RootMeanSquaredError(name='rmse')],\n                       optimizer=Adamax(lr=8e-3))\n\n        early = EarlyStopping(monitor=\"val_rmse\", mode=\"min\", \n                              restore_best_weights=True, \n                              patience=9, verbose=VERBOSE)\n\n        reduce_lr = ReduceLROnPlateau(monitor=\"val_rmse\", factor=0.25, \n                                      min_lr=1e-6, patience=4, \n                                      verbose=VERBOSE, mode='min')\n\n        chk_point = ModelCheckpoint(f'.\/CommonLit_Readability_Model2_{counter}C.h5', \n                                    monitor='val_rmse', verbose=VERBOSE, \n                                    save_best_only=True, mode='min',\n                                    save_weights_only=True)\n        \n        history = model2.fit(\n            [train_x_id, train_x_mask, train_x2], train_y, \n            batch_size=MINI_BATCH_SIZE,\n            epochs=50, \n            verbose=VERBOSE, \n            workers=5,\n            callbacks=[reduce_lr, early, chk_point], \n            validation_data=([val_x_id, val_x_mask, val_x2], val_y)\n        )\n        \n        model2.load_weights(f'.\/CommonLit_Readability_Model2_{counter}C.h5')\n\n        y_pred = model2.predict([val_x_id, val_x_mask, val_x2])\n        score2 = np.sqrt(mean_squared_error(val_y, y_pred))\n        oof_score2 += score2\n        seed_score2 += score2\n        print(\"Model-2 | Seed-{} | Fold-{} | OOF Score: {}\".format(seed, idx, score2))\n        \n        if score1 > score2:\n            y_pred_final += model2.predict([Xtest_id, Xtest_mask, Xtest])\n            print(\"Predictions made using Model-2\\n\")\n        \n        elif score1 < score2:\n            y_pred_final += model1.predict([Xtest_id, Xtest_mask, Xtest])\n            print(\"Predictions made using Model-1\\n\")\n        \n        else:\n            y_pred_final += model1.predict([Xtest_id, Xtest_mask, Xtest])\n            print(\"Predictions made using Model-1\\n\")\n    \n    print(\"\\nModel-1 | Seed: {} | Aggregate OOF Score: {}\".format(seed, (seed_score1 \/ FOLD)))\n    print(\"Model-2 | Seed: {} | Aggregate OOF Score: {}\\n\\n\".format(seed, (seed_score2 \/ FOLD)))\n\n\ny_pred_final = y_pred_final \/ float(counter)\noof_score1 \/= float(counter)\noof_score2 \/= float(counter)\noof_score = (oof_score1 + oof_score2) \/ 2.0\nprint(\"Model-1 OOF Score: {}\".format(oof_score1))\nprint(\"Model-2 OOF Score: {}\".format(oof_score2))\nprint(\"Aggregate OOF Score: {}\".format(oof_score))","e339a3b6":"submit_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsubmit_df['target'] = y_pred_final\nsubmit_df.to_csv(\".\/submission.csv\", index=False)\nsubmit_df.head()","fcbd8745":"## Load source datasets","39cff7af":"### **Experiment Log:**\n\n|Version |Models Used |CV Score |LB Score| Changes Made\n| --- | --- | --- | --- | --- |\n|v1 |Lasso, ElasticNet | 0.8361 | 0.843 | Baseline\n|v2 |Lasso, ElasticNet | 0.8361 | 0.843 | TF-IDF\n|v3 |Lasso, ElasticNet | 0.8361 | NA | TF-IDF (-) <br> Glove (+)\n|v4 |Lasso, ElasticNet <br> LightGBM, XGBoost | 0.7485 | 0.654 | Glove\n|v5 |Conv1D, Bidirectional LSTM | 0.6344 | 0.603 | Glove\n|v6 |Conv1D, Bidirectional LSTM | 0.6436 | NA | Glove + FastText <br> Model architecture modified\n|v7 |Conv1D, Bidirectional LSTM | 0.6443 | 0.583 | Model architecture modified\n|v8 |Conv1D, Bidirectional LSTM | 0.6652 | 0.629 | Bert Embeddings\n|v9 |Conv1D, Bidirectional LSTM | 0.6416 | 0.596 | Bert Embeddings (-) <br> Glove + FastText (+)\n|v10 |Transformer block with attention | 0.6466 | NA | Glove + FastText\n|v11 |Transformer block with attention | 0.6497 | 0.596 | Changed TF initialization process\n|v12 |Conv1D, Bidirectional LSTM | 0.6482 | NA | Reverting to v7 model architecture <br> (with TF initialization changes)\n|v13 |Conv1D, Bidirectional LSTM | 0.6496 | 0.575 | Bug fix in predictions\n|v14 |Conv1D, Bidirectional LSTM | 0.3216 | NA | Pretrained DistilBert Embeddings <br> (with same model backbone)\n|v15 |Conv1D, Bidirectional LSTM | 0.3283 | 0.561 | Pretrained DistilBert Embeddings <br> (training for single seed)\n|v16 |Conv1D, Bidirectional LSTM | 0.6712 | 0.593 | Glove + FastText (Common Crawl)\n|v17 |Bidirectional LSTM & GRU <br> (with attention) | 0.6312 | 0.592 | Model architecture modified\n|v18 |Bidirectional LSTM & GRU <br> (with attention) | 0.6313 | 0.583 | Glove + FastText (Wiki-News)\n|v19 |Bidirectional LSTM & GRU <br> (with attention) | 0.6413 | 0.597 | Fix for OOV tokens\n|v20 |Conv1D, Bidirectional LSTM | 0.6563 | 0.615 | Switched to v13 model architecture\n|v21 |Conv1D, Bidirectional LSTM | 0.6372 | NA | Glove + FastText + Word2Vec <br> (Weighted embeddings) <br> Model architecture modified\n|v22 |Conv1D, Bidirectional LSTM | 0.6367 | 0.589 | Glove + FastText <br> (Weighted embeddings) <br> Model architecture modified\n|v23 |Conv1D, Bidirectional LSTM | 0.6568 | 0.593 | Glove + FastText <br> (Weighted embeddings) <br> v13 Model architecture\n|v24 |Conv1D, Bidirectional LSTM | TBD | TBD | Pretrained DistilRoberta Embeddings","9df8bd37":"## Generate word token and attention mask","f098d1cd":"## Feature Engineering","d5e7210d":"## Import libraries","572641d6":"## Base Model\n\n* **DistilRoberta Embeddings**\n* **Conv1D + Bidirectional LSTM**","66f06e18":"## Extract target label","5aef8182":"## Create submission file"}}