{"cell_type":{"40493908":"code","71c48b11":"code","93183dbe":"code","2ac949da":"code","0fa20350":"code","23455666":"code","0dc975f6":"code","a1abb150":"code","738df742":"code","5371200f":"code","4f145e1c":"code","630eba25":"code","78714742":"code","77ed325a":"code","b0d9516c":"code","f41d4158":"code","107f7662":"code","b49bddfa":"code","30bb910d":"code","f96415df":"code","0d0dcf68":"code","497cf497":"code","18075e75":"code","b8a600af":"code","e6a52408":"code","d18ffbff":"code","0a9ed54d":"code","d936fa2e":"code","ee730576":"code","fc28f63a":"code","7e78551f":"code","34db4d8c":"code","241a151c":"code","f8b38c63":"code","2c1263b4":"code","486f6f31":"code","6953347d":"code","c672190a":"code","c0f6f4ac":"code","c5dcbd28":"code","15edd014":"code","b3f878a2":"code","69e27079":"code","f3502811":"code","ad9d0171":"code","8b6e8efb":"code","9037c00f":"code","7bdd1f5b":"code","922bd79a":"code","822f2aed":"code","338ef43c":"markdown","1a8e9e07":"markdown","4426fa8a":"markdown","a682957c":"markdown","a49f79e2":"markdown","18361bd8":"markdown","cea7d669":"markdown","0cc91ee4":"markdown","eeb4921e":"markdown","45bd94dd":"markdown","08f772a2":"markdown","94c426b5":"markdown","b423e4fd":"markdown","efc11d9f":"markdown","e7f765d5":"markdown","725fbd3d":"markdown","e6c1aaf9":"markdown","0e0447cf":"markdown","34e60c19":"markdown","09716c4c":"markdown","773c3645":"markdown","01d23b9d":"markdown","d53aa1ac":"markdown","41bd834b":"markdown","73b94426":"markdown","0a01809d":"markdown","00613244":"markdown","5ab90c48":"markdown","675fd5a4":"markdown","5fe4b9d2":"markdown"},"source":{"40493908":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71c48b11":"import pandas as pd\nimport numpy as np\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(color_codes=True)\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom pyearth import Earth\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\n\n\n\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\nimport warnings\nwarnings.simplefilter('ignore')","93183dbe":"# Load Data\ndata_train = pd.read_csv(\"\/kaggle\/input\/are-your-employees-burning-out\/train.csv\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/are-your-employees-burning-out\/test.csv\")\nprint(\"Shape of Training Data:\", data_train.shape)\nprint(\"Shape of Test Data:\", data_test.shape)\ndata_train.head()","2ac949da":"# Data Information\nprint(data_train.info())\nprint(data_test.info())","0fa20350":"#from datetime import datetime as dt\nimport datetime as dt\ndata_train[\"Date of Joining\"] =  pd.to_datetime(data_train[\"Date of Joining\"])\ndata_test[\"Date of Joining\"] =  pd.to_datetime(data_test[\"Date of Joining\"])\ndata_train.dtypes","23455666":"dt_today = dt.date.today()\ndata_train[\"today\"]=dt_today\ndata_test[\"today\"]=dt_today\ndata_train.head()","0dc975f6":"data_train[\"today\"] =  pd.to_datetime(data_train[\"today\"])\ndata_test[\"today\"] =  pd.to_datetime(data_test[\"today\"])","a1abb150":"data_train['tenure'] = data_train['today'] - data_train[\"Date of Joining\"] \ndata_test['tenure'] = data_test['today'] - data_test[\"Date of Joining\"] \ndata_train.head()","738df742":"data_train['tenure'] = data_train['tenure'].astype(int) \ndata_test['tenure'] = data_test['tenure'].astype(int)","5371200f":"data_train = data_train.drop([\"Employee ID\", \"Date of Joining\", \"today\"], axis=1)\ndata_test = data_test.drop([\"Employee ID\", \"Date of Joining\", \"today\"], axis=1)\nprint(data_train.info())\nprint(data_test.info())","4f145e1c":"# Drop rows where target variable \"Burn Rage\" is missing.\ndata_train = data_train.dropna(subset=['Burn Rate'])\ndata_train.info()","630eba25":"# Impute remaining missing values with medians\ndf_train = data_train\nfor col in ['Resource Allocation', 'Mental Fatigue Score']:\n    df_train[col] = df_train[col].fillna(df_train[col].median())\n\ndf_train.info()","78714742":"# Descriptive statistics\ndf_train.describe()","77ed325a":"# Representing categorical data using Letter Value Boxplots\nfig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(10,5))\nsns.countplot(df_train[\"Gender\"], palette=\"winter\", ax=ax[0])\nsns.countplot(df_train[\"Company Type\"], palette=\"winter\", ax=ax[1])\nsns.countplot(df_train[\"WFH Setup Available\"], palette=\"winter\", ax=ax[2])\nplt.tight_layout()\nplt.show()","b0d9516c":"# Representing categorical data using Letter Value Boxplots\nfig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(10,5))\nsns.boxenplot(x=\"Gender\", y=\"Burn Rate\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[0])\nsns.boxenplot(x=\"Company Type\", y=\"Burn Rate\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[1])\nsns.boxenplot(x=\"WFH Setup Available\", y=\"Burn Rate\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[2])\nplt.tight_layout()\nplt.show()","f41d4158":"# Representing categorical data using Letter Value Boxplots\nfig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(14,5))\nsns.boxenplot(x=\"Gender\", y=\"Burn Rate\", hue=\"Company Type\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[0])\nsns.boxenplot(x=\"Gender\", y=\"Burn Rate\", hue=\"WFH Setup Available\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[1])\nsns.boxenplot(x=\"WFH Setup Available\", y=\"Burn Rate\", hue=\"Company Type\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[2])\nplt.tight_layout()\nplt.show()","107f7662":"# Create a list of continuous variables\ncont = [\"Burn Rate\", \"Resource Allocation\", \"Mental Fatigue Score\", \"tenure\"]\n\n# Create a dataframe of continuous variables\ndf_cont = df_train[cont]","b49bddfa":"# Visualize correlation between continuous variables\n\n# Compute the correlation matrix\n#corr = df_cont.corr()\n\n# Generate a mask for the upper triangle\n#mask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\n#f, ax = plt.subplots(figsize=(4, 3))\n\n# Draw the heatmap with the mask and correct aspect ratio\n#sns.heatmap(corr, mask=mask, cmap=sns.diverging_palette(128, 240,as_cmap=True), \n#            vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)","30bb910d":"# Visualize regression between each feature and output variable\n\n# The number of variables\nk = 4\nfig = plt.figure(figsize=(14,5))\n# Correlations between each variable\ncorrmat = df_cont.corr()\n# Take k elements in descending order of coefficient \ncols = corrmat.nlargest(k, \"Burn Rate\")[\"Burn Rate\"].index\n# Calculate correlation\nfor i in np.arange(1,k):\n    regline = df_cont[cols[i]]\n    ax = fig.add_subplot(1,3,i)\n    sns.regplot(x=regline, y=df_train['Burn Rate'], scatter_kws={\"color\": \"royalblue\", \"s\": 3},\n                line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","f96415df":"# Representing categorical data using Letter Value Boxplots\nfig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(14,5))\nsns.scatterplot(x=\"Mental Fatigue Score\", y=\"Burn Rate\", hue=\"Gender\", data=df_train, linewidth=0.0, ax=ax[0])\nsns.scatterplot(x=\"Resource Allocation\", y=\"Burn Rate\", hue=\"Gender\", data=df_train, linewidth=0.0, ax=ax[1])\nsns.scatterplot(x=\"tenure\", y=\"Burn Rate\", hue=\"Gender\", data=df_train, linewidth=0.0, ax=ax[2])\nplt.tight_layout()\nplt.show()","0d0dcf68":"# Representing categorical data using Letter Value Boxplots\nfig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(14,5))\nsns.scatterplot(x=\"Mental Fatigue Score\", y=\"Burn Rate\", hue=\"WFH Setup Available\", data=df_train, linewidth=0.0, ax=ax[0])\nsns.scatterplot(x=\"Resource Allocation\", y=\"Burn Rate\", hue=\"WFH Setup Available\", data=df_train, linewidth=0.0, ax=ax[1])\nsns.scatterplot(x=\"tenure\", y=\"Burn Rate\", hue=\"WFH Setup Available\", data=df_train, linewidth=0.0, ax=ax[2])\nplt.tight_layout()\nplt.show()","497cf497":"# Representing categorical data using Letter Value Boxplots\nfig, ax = plt.subplots(nrows=1, ncols=3, sharey=True, figsize=(10,5))\nsns.boxenplot(x=\"Gender\", y=\"Mental Fatigue Score\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[0])\nsns.boxenplot(x=\"Company Type\", y=\"Mental Fatigue Score\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[1])\nsns.boxenplot(x=\"WFH Setup Available\", y=\"Mental Fatigue Score\", data=df_train, palette=\"winter\", linewidth=0.0, ax=ax[2])\nplt.tight_layout()\nplt.show()","18075e75":"import category_encoders as ce\n\nlist_cols = ['Gender','Company Type','WFH Setup Available']\n\nce_ohe = ce.OneHotEncoder(cols=list_cols)\ndf_train = ce_ohe.fit_transform(df_train)\ndf_train.head()\n\nce_ohe2 = ce.OneHotEncoder(cols=list_cols)\ndata_test = ce_ohe2.fit_transform(data_test)","b8a600af":"# Split X and y\nX = df_train.drop(['Burn Rate','Gender_2','Company Type_2','WFH Setup Available_2'], axis=1)\ny = df_train['Burn Rate']\n\ndata_test = data_test.drop(['Gender_2','Company Type_2','WFH Setup Available_2'], axis=1)","e6a52408":"# Create dummies for categorical variables\n\n# subset all categorical variables\n#categorical = X.select_dtypes(include=['object'])\n# convert into dummies\n#dummies = pd.get_dummies(categorical, drop_first=True)\n# drop categorical variables \n#X = X.drop(list(categorical.columns), axis=1)\n#data_test = data_test.drop(list(categorical.columns), axis=1)\n# concat dummy variables with X\n#X = pd.concat([X, cars_dummies], axis=1)\n#data_test = data_test([data_test, cars_dummies], axis=1)","d18ffbff":"# split into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7,test_size = 0.3, random_state=100)","0a9ed54d":"from sklearn.preprocessing import StandardScaler\n\ncols = X.columns\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform (X_test)\n\nX_train = pd.DataFrame(X_train)\nX_test = pd.DataFrame(X_test)\n\nX_train.columns = cols\nX_test.columns = cols","d936fa2e":"# Instantiate the model\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Make prediction\nlm_y_pred = lm.predict(X_test)\n\n# Performance metrics\nlr_r2= r2_score(y_test, lm_y_pred)\nlr_mae = mean_absolute_error(y_test, lm_y_pred)\n\n# Show the metrics\nprint(\"Linear Regression R2: \", lr_r2)\nprint(\"Linear Regression MAE: \", lr_mae)","ee730576":"# Evaluate the model based on the assumption of linear regression:\n\n# Assumption 1. The error terms are normally distributed with mean approximately 0.\n\nfig = plt.figure()\nsns.distplot((y_test - lm_y_pred),bins=50, color=\"blue\")\nfig.suptitle('Error Terms', fontsize=14)                  \nplt.xlabel('y_test-y_pred', fontsize=12)                  \nplt.ylabel('Index', fontsize=12)                          \nplt.show()","fc28f63a":"# Assumption 2: Homoscedasticity, i.e. the variance of the error term (y_true-y_pred) is constant.\n\nc = [i for i in range(len(lm_y_pred))]\nfig = plt.figure()\nplt.plot(c,y_test - lm_y_pred, color=\"blue\", linewidth=2.5, linestyle=\"-\", alpha=0.4)\nfig.suptitle('Error Terms', fontsize=14)               \nplt.xlabel('Index', fontsize=12)                      \nplt.ylabel('ytest-ypred', fontsize=12)                \nplt.show()","7e78551f":"# Assumption 3: There is little correlation between the predictors. i.e., Multicollinearity:\n\n# Compute the correlation matrix\ncors = X.loc[:, list(X.columns)].corr()\n\n# Generate a mask for the upper triangle\nmask_2 = np.triu(np.ones_like(cors, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(9, 6))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(cors, mask=mask_2, cmap=\"coolwarm\",\n            vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=False)\nplt.show()","34db4d8c":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","241a151c":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f8b38c63":"# Initiate the model\nmars = Earth()\n\n# By default, we do not need to set any of the algorithm hyperparameters.\n# The algorithm automatically discovers the number and type of basis functions to use.\n\n# Fit the model\nmars.fit(X_train, y_train)\n\n# Making predictions\nmars_y_pred = mars_model.predict(X_test)\n\n# Performance Metrics\nmars_r2 = r2_score(y_test, mars_y_pred)\nmars_mae = mean_absolute_error(y_test, mars_y_pred)\n\n# Show the model performance\nprint(\"MARS R2: \", mars_r2)\nprint(\"MARS MAE: \", mars_mae)","2c1263b4":"# Initiate the model\nknn = KNeighborsRegressor()\n\n# Fit the model\nknn.fit(X_train, y_train)\n\n# Make predictions\nknn_y_pred = knn.predict(X_test)\n\n# Performance metrics\nknn_r2 = r2_score(y_test, knn_y_pred)\nknn_mae = mean_absolute_error(y_test, knn_y_pred)\n\n# Show the model performance\nprint(\"KNN R2: \", knn_r2)\nprint(\"KNN MAE: \", knn_mae)","486f6f31":"# Initiate the model\ndt = DecisionTreeRegressor()\n\n# Fit the model\ndt.fit(X_train, y_train)\n\n# Make predictions\ndt_y_pred = dt.predict(X_test)\n\n# Performance metrics\ndt_r2 = r2_score(y_test, dt_y_pred)\ndt_mae = mean_absolute_error(y_test, dt_y_pred)\n\n# Show the model performance\nprint(\"DT R2: \", dt_r2)\nprint(\"DT MAE: \", dt_mae)","6953347d":"# Initiate the model\nrf = RandomForestRegressor()\n\n# Fit the model\nrf.fit(X_train, y_train)\n\n# Make predictions\nrf_y_pred = rf.predict(X_test)\n\n# Performance metrics\nrf_r2 = r2_score(y_test, rf_y_pred)\nrf_mae = mean_absolute_error(y_test, rf_y_pred)\n\n# Show the model performance\nprint(\"RF R2: \", rf_r2)\nprint(\"RF MAE: \", rf_mae)","c672190a":"# Initiate the model\ngb = GradientBoostingRegressor()\n\n# Fit the model\ngb.fit(X_train, y_train)\n\n# Make predictions\ngb_y_pred = gb.predict(X_test)\n\n# Performance metrics\ngb_r2 = r2_score(y_test, gb_y_pred)\ngb_mae = mean_absolute_error(y_test, gb_y_pred)\n\n# Show the model performance\nprint(\"GB R2: \", gb_r2)\nprint(\"GB MAE: \", gb_mae)","c0f6f4ac":"# Initiate the model\net = ExtraTreesRegressor()\n\n# Fit the model\net.fit(X_train, y_train)\n\n# Make predictions\net_y_pred = et.predict(X_test)\n\n# Performance metrics\net_r2 = r2_score(y_test, et_y_pred)\net_mae = mean_absolute_error(y_test, et_y_pred)\n\n# Show the model performance\nprint(\"ET R2: \", et_r2)\nprint(\"ET MAE: \", et_mae)","c5dcbd28":"# Initiate the model\nxg = XGBRegressor()\n\n# Fit the model\nxg.fit(X_train, y_train)\n\n# Make predictions\nxg_y_pred = xg.predict(X_test)\n\n# Performance metrics\nxg_r2 = r2_score(y_test, xg_y_pred)\nxg_mae = mean_absolute_error(y_test, xg_y_pred)\n\n# Show the model performance\nprint(\"XGB R2: \", xg_r2)\nprint(\"XGB MAE: \", xg_mae)","15edd014":"# Initiate the model\nlg = LGBMRegressor()\n\n# Fit the model\nlg.fit(X_train, y_train)\n\n# Make predictions\nlg_y_pred = lg.predict(X_test)\n\n# Performance metrics\nlg_r2 = r2_score(y_test, lg_y_pred)\nlg_mae = mean_absolute_error(y_test, lg_y_pred)\n\n# Show the model performance\nprint(\"LGBM R2: \", lg_r2)\nprint(\"LGBM MAE: \", lg_mae)","b3f878a2":"# Initiate the model\ncb = CatBoostRegressor()\n\n# Fit the model\ncb.fit(X_train, y_train)\n\n# Make predictions\ncb_y_pred = cb.predict(X_test)\n\n# Performance metrics\ncb_r2 = r2_score(y_test, cb_y_pred)\ncb_mae = mean_absolute_error(y_test, cb_y_pred)\n\n# Show the model performance\nprint(\"CATB R2: \", cb_r2)\nprint(\"CATB MAE: \", cb_mae)","69e27079":"# Define a DNN\ndef create_model(optimizer='adam'):\n    model = Sequential()\n    model.add(Dense(X_train.shape[1], input_dim=X_train.shape[1], kernel_initializer='normal', activation='relu'))\n    model.add(Dense(4, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n\n    model.compile(loss='mean_squared_error', optimizer=optimizer)\n    return model","f3502811":"# Initiate DNN\ndnn = KerasRegressor(build_fn=create_model, epochs=1000, batch_size=20, verbose=1)\n\n# Fit DNN\ndnn_history = dnn.fit(X_train, y_train)","ad9d0171":"# Visualize the DNN learning\nloss_train = dnn_history.history['loss']\nepochs = range(1,1001)\nplt.figure(figsize=(8,6))\nplt.plot(epochs, loss_train, 'royalblue', label='Training loss', linewidth=3)\nplt.title('Training loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","8b6e8efb":"# Make predictions\ndnn_y_pred = dnn.predict(X_test)\n\n# Performance metrics\ndnn_r2 = r2_score(y_test, dnn_y_pred)\ndnn_mae = mean_absolute_error(y_test, dnn_y_pred)\n\n# Show the model performance\nprint(\"DNN R2: \", dnn_r2)\nprint(\"DNN MAE: \", dnn_mae)","9037c00f":"results_table = pd.DataFrame([[np.mean(lr_r2), np.mean(lr_mae)],\n                              [np.mean(mars_r2), np.mean(mars_mae)],\n                              [np.mean(knn_r2), np.mean(knn_mae)],\n                              [np.mean(dt_r2), np.mean(dt_mae)],\n                              [np.mean(rf_r2), np.mean(rf_mae)],\n                              [np.mean(gb_r2), np.mean(gb_mae)],\n                              [np.mean(et_r2), np.mean(et_mae)],\n                              [np.mean(xg_r2), np.mean(xg_mae)],\n                              [np.mean(lg_r2), np.mean(lg_mae)],\n                              [np.mean(cb_r2), np.mean(cb_mae)],\n                              [np.mean(dnn_r2), np.mean(dnn_mae)]],\n                            columns=['R2', 'MAE'],\n                            index=[\"Linear Regression\",\"MARS\",\"KNN\",\"Decision Tree\",\"Random Forest\",\"Gradient Boosting\",\n                                   \"Extra Trees\",\"XGBoost\",\"LightGBM\",\"CatBoost\",\"DNN\"])\npd.options.display.precision = 3\nresults_table","7bdd1f5b":"pred_table = pd.DataFrame({\"Linear Regression: Predicted Price\": lm_y_pred,\n                           \"MARS: Predicted Price\": mars_y_pred,\n                           \"KNN: Predicted Price\": knn_y_pred,\n                           \"Decision Tree: Predicted Price\": dt_y_pred,\n                           \"Random Forest: Predicted Price\": rf_y_pred,\n                           \"Gradient Boosting: Predicted Price\": gb_y_pred,\n                           \"Extra Trees: Predicted Price\": et_y_pred,\n                           \"XGBoost: Predicted Price\": xg_y_pred,\n                           \"LightGBM: Predicted Price\": lg_y_pred,\n                           \"CatBoost: Predicted Price\": cb_y_pred,\n                           \"DNN: Predicted Price\": dnn_y_pred,\n                           \"Actual Price\": y_test})","922bd79a":"# Visualize the predicted price and actual price\nfig = plt.figure(figsize=(16,10))\nplt.subplot(3,4,1)\nsns.regplot(x = 'Linear Regression: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,2)\nsns.regplot(x = 'MARS: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,3)\nsns.regplot(x = 'KNN: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,4)\nsns.regplot(x = 'Decision Tree: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,5)\nsns.regplot(x = 'Random Forest: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,6)\nsns.regplot(x = 'Gradient Boosting: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,7)\nsns.regplot(x = 'Extra Trees: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,8)\nsns.regplot(x = 'XGBoost: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,9)\nsns.regplot(x = 'LightGBM: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,10)\nsns.regplot(x = 'CatBoost: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.subplot(3,4,11)\nsns.regplot(x = 'DNN: Predicted Price', y = 'Actual Price', data = pred_table,\n           color = 'royalblue',scatter_kws={\"s\": 5}, line_kws={\"color\": \"turquoise\"})\nplt.tight_layout()\nplt.show()","822f2aed":"# Feature Importances\nfti = lg.feature_importances_\n\nprint('Feature Importances:')\nfor i, feat in enumerate(X.columns):\n    print('\\t{0:10s} : {1:>12.4f}'.format(feat, fti[i]))","338ef43c":"# 2. Imputing Missing Values","1a8e9e07":"The first assumption seems to be met.","4426fa8a":"## 2.2. Exploration of Categorical Variables","a682957c":"# 1. Import Libraries and Load Dataset","a49f79e2":"The test data does not contain output variable \"Burn Rate\", so let's training models using the training data, and predict the Burn Rate in the test data using the trained models.","18361bd8":"# 5. Feature Importance","cea7d669":"## 2.3. Exploration of Continuous Variables","0cc91ee4":"# 3. Regression","eeb4921e":"## 3.3. KNN Regression","45bd94dd":"# 4. Summary of Results","08f772a2":"All VIF are below 5. So, there is no concern for multicollinearity.","94c426b5":"There are no missing values in the test set. So we need to impute missing data only for the training data.","b423e4fd":"The second assumption seems to be met.","efc11d9f":"## 3.7. ExtraTreesRegressor","e7f765d5":"## 2.4. Data Preparation for Modeling","725fbd3d":"## 3.10. CatBoost Regression","e6c1aaf9":"## 3.6. Gradient Boosting Regression","0e0447cf":"## 3.1. Linear Regression","34e60c19":"Some features are highly correlated. So let's check the multicolliearity by VIF.","09716c4c":"### Mental Fatigue and Tenure are important features.","773c3645":"## 3.5. Deep Neural Network","01d23b9d":"- Drop rows where target variable \"Burn Rate\" is missing.\n- Impute other missing values with median because all the missing values are float.","d53aa1ac":"## 3.5. Random Forest Regression ","41bd834b":"## 3.4. Decision Tree Regression","73b94426":"## 3.2. Multivariate Adaptive Regression Splines (MARS)","0a01809d":"- Regression Models:\n  - Linear Regression\n  - Multivariate Adaptive Regression Splines\n  - KNN Regressor\n  - Decision Tree Regressor\n  - Random Forest Regressor\n  - Gradient Boosting Regressor\n  - Extra Trees Regressor\n  - XGBoost Regressor\n  - LightGBM Regressor\n  - CatBoost Regressor\n  - Deep Neural Network\n\n- Performace Metrics:\n  - R-Squared\n  - Mean Absolute Error","00613244":"## 3.8. XGB Regression","5ab90c48":"# Employee Burnout Prediction","675fd5a4":"## 3.9. Light GBM Regression","5fe4b9d2":"# 2. Data Exploration and Preprocessing"}}