{"cell_type":{"578a2a71":"code","bbe07097":"code","895b699c":"code","8da04463":"code","c43c9f10":"code","d4ae20dd":"code","6f49ad51":"code","dc38345e":"code","1fe776da":"code","a18a282f":"code","d3d51f04":"code","b45a5f8a":"code","6563d9ae":"code","4a0853dc":"code","ece8f530":"code","e1cf4e91":"code","ba9ebbc3":"code","bce10984":"code","4c4c25fc":"code","59c7e19b":"code","617a01e1":"code","940c3318":"code","f7e087d8":"code","d6030c6f":"code","5c4a1a5c":"code","f2336bf1":"code","5e50c881":"code","0593fe1d":"code","88c06242":"code","7a65035a":"code","cbbb37bd":"code","b9f9947b":"code","19672333":"code","1904c0a3":"code","75c869a7":"code","a0f74c7c":"markdown","505e7c09":"markdown","ee190ea4":"markdown","522c1f7e":"markdown","deefb659":"markdown","18120683":"markdown","e851d75d":"markdown","8f4bec36":"markdown","51c5ffca":"markdown"},"source":{"578a2a71":"!pip install openpyxl  --quiet","bbe07097":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict\nimport string\nimport tensorflow as tf\nimport re\nimport os\nimport time\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split","895b699c":"ENCODER_LEN = 100\nDECODER_LEN = 20\nBATCH_SIZE = 64\nBUFFER_SIZE = BATCH_SIZE*8","8da04463":"news = pd.read_excel(\"..\/input\/inshorts-news-data\/Inshorts Cleaned Data.xlsx\",engine = 'openpyxl')\nnews.drop(['Source ', 'Time ', 'Publish Date'], axis=1, inplace=True)\nnews.head()","c43c9f10":"article = news['Short']\nsummary = news['Headline']\narticle = article.apply(lambda x: '<SOS> ' + x + ' <EOS>')\nsummary = summary.apply(lambda x: '<SOS> ' + x + ' <EOS>')","d4ae20dd":"def preprocess(text):\n    text = re.sub(r\"&.[1-9]+;\",\" \",text)\n    return text\narticle = article.apply(lambda x: preprocess(x))\nsummary = summary.apply(lambda x: preprocess(x))","6f49ad51":"filters = '!\"#$%&()*+,-.\/:;=?@[\\\\]^_`{|}~\\t\\n'\noov_token = '<unk>'\narticle_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\nsummary_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters=filters, oov_token=oov_token)\narticle_tokenizer.fit_on_texts(article)\nsummary_tokenizer.fit_on_texts(summary)\ninputs = article_tokenizer.texts_to_sequences(article)\ntargets = summary_tokenizer.texts_to_sequences(summary)","dc38345e":"ENCODER_VOCAB = len(article_tokenizer.word_index) + 1\nDECODER_VOCAB = len(summary_tokenizer.word_index) + 1\nprint(ENCODER_VOCAB, DECODER_VOCAB)","1fe776da":"inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=ENCODER_LEN, padding='post', truncating='post')\ntargets = tf.keras.preprocessing.sequence.pad_sequences(targets, maxlen=DECODER_LEN, padding='post', truncating='post')\ninputs = tf.cast(inputs, dtype=tf.int64)\ntargets = tf.cast(targets, dtype=tf.int64)","a18a282f":"dataset = tf.data.Dataset.from_tensor_slices((inputs, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","d3d51f04":"def get_angles(position, i, d_model):\n    angle_rates = 1 \/ np.power(10000, (2 * (i \/\/ 2)) \/ np.float32(d_model))\n    return position * angle_rates\n\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(\n        np.arange(position)[:, np.newaxis],\n        np.arange(d_model)[np.newaxis, :],\n        d_model\n    )\n\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)\n\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    return seq[:, tf.newaxis, tf.newaxis, :]\n\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask\n\ndef scaled_dot_product_attention(q, k, v, mask):\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)  \n\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n\n    output = tf.matmul(attention_weights, v)\n    return output, attention_weights\n","b45a5f8a":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model \/\/ self.num_heads\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n        \n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)\n        k = self.wk(k)\n        v = self.wv(v)\n\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n        \n        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n        output = self.dense(concat_attention)\n            \n        return output, attention_weights\n    \ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        tf.keras.layers.Dense(dff, activation='relu'),\n        tf.keras.layers.Dense(d_model)\n    ])","6563d9ae":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)\n\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)\n\n        return out2","4a0853dc":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n    \n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n\n        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)\n\n        ffn_output = self.ffn(out2)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)\n\n        return out3, attn_weights_block1, attn_weights_block2","ece8f530":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n        \n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n    \n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n    \n        return x\n    \nclass Decoder(tf.keras.layers.Layer):\n        \n    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n    \n    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n\n            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n    \n        return x, attention_weights\n    ","e1cf4e91":"class Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n        super(Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n\n        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n    \n    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n\n        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n\n        final_output = self.final_layer(dec_output)\n\n        return final_output, attention_weights","ba9ebbc3":"num_layers = 3\nd_model = 128\ndff = 512\nnum_heads = 4\ndropout_rate = 0.2\nEPOCHS = 15","bce10984":"class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n\n        self.warmup_steps = warmup_steps\n    \n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)","4c4c25fc":"learning_rate = CustomSchedule(d_model)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)","59c7e19b":"temp_learning_rate_schedule = CustomSchedule(d_model)\n\nplt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\nplt.ylabel(\"Learning Rate\")\nplt.xlabel(\"Train Step\")","617a01e1":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_)\/tf.reduce_sum(mask)\n\n\ndef accuracy_function(real, pred):\n    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n    #accuracies = tf.cast(accuracies, dtype= tf.float32)\n\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    accuracies = tf.math.logical_and(mask, accuracies)\n\n    accuracies = tf.cast(accuracies, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n    return tf.reduce_sum(accuracies)\/tf.reduce_sum(mask)","940c3318":"train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.Mean(name='train_accuracy')","f7e087d8":"transformer = Transformer(\n    num_layers=num_layers,\n    d_model=d_model,\n    num_heads=num_heads,\n    dff=dff,\n    input_vocab_size=ENCODER_VOCAB,\n    target_vocab_size=DECODER_VOCAB,\n    pe_input=1000,\n    pe_target=1000,\n    rate=dropout_rate)","d6030c6f":"def create_masks(inp, tar):\n    enc_padding_mask = create_padding_mask(inp)\n    dec_padding_mask = create_padding_mask(inp)\n\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n  \n    return enc_padding_mask, combined_mask, dec_padding_mask","5c4a1a5c":"checkpoint_path = \"checkpoints\"\n\nckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!')","f2336bf1":"@tf.function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(\n            inp, tar_inp, \n            True, \n            enc_padding_mask, \n            combined_mask, \n            dec_padding_mask\n        )\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, transformer.trainable_variables)    \n    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(accuracy_function(tar_real, predictions))","5e50c881":"for epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n  \n    for (batch, (inp, tar)) in enumerate(dataset):\n        train_step(inp, tar)\n    \n        if batch % 100 == 0:\n            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n      \n    if (epoch + 1) % 5 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n   \n    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))","0593fe1d":"def evaluate(input_article):\n    input_article = article_tokenizer.texts_to_sequences([input_article])\n    input_article = tf.keras.preprocessing.sequence.pad_sequences(input_article, maxlen=ENCODER_LEN, \n                                                                   padding='post', truncating='post')\n\n    encoder_input = tf.expand_dims(input_article[0], 0)\n\n    decoder_input = [summary_tokenizer.word_index['<sos>']]\n    output = tf.expand_dims(decoder_input, 0)\n    \n    for i in range(DECODER_LEN):\n        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n\n        predictions, attention_weights = transformer(\n            encoder_input, \n            output,\n            False,\n            enc_padding_mask,\n            combined_mask,\n            dec_padding_mask\n        )\n\n        predictions = predictions[: ,-1:, :]\n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n\n        if predicted_id == summary_tokenizer.word_index['<eos>']:\n            return tf.squeeze(output, axis=0), attention_weights\n\n        output = tf.concat([output, predicted_id], axis=-1)\n\n    return tf.squeeze(output, axis=0), attention_weights","88c06242":"def summarize(input_article):\n    summarized = evaluate(input_article=input_article)[0].numpy()\n    summarized = np.expand_dims(summarized[1:], 0)  \n    return summary_tokenizer.sequences_to_texts(summarized)[0]","7a65035a":"article[5]","cbbb37bd":"print(\"Real Headline : \", summary[5][5:-5],\"\\n Predicted Summary : \", summarize(article[5]))","b9f9947b":"article[16]","19672333":"print(\"Real Headline : \", summary[16][5:-5],\"\\nPredicted Summary : \", summarize(article[16]))","1904c0a3":"article[23]","75c869a7":"print(\"Real Headline : \", summary[23][5:-5],\"\\nPredicted Summary : \", summarize(article[23]))","a0f74c7c":"# Evaluation","505e7c09":"# Custom Loss and Accuracy","ee190ea4":"# Acknowledgements\n\nThe code for the transformer model is take from  this tutorial https:\/\/www.tensorflow.org\/text\/tutorials\/transformer","522c1f7e":"# Installing Packages needed and Importing Libraries","deefb659":"# Custom Learning Rate","18120683":"# Transformer Model\n\nThe next several blocks of code contain the vanilla Transformer model.\n\nIf you want to know about what they are and how they work I suggest this video: https:\/\/www.youtube.com\/watch?v=4Bdc55j80l8\n\nIt does an excellent job of giving an overview about them and helped me in understanding them.","e851d75d":"# Predictions\n\nBelow me make predictions on some texts to see how the model is performimg. Since this was a very basic approach the model wont perform that well but it can surely be improved.","8f4bec36":"# Dataset\n\nAfter creating the dataframe we apply Start of Sentence(<SOS>) and End of Sentence(<EOS>) tokens. \nThese sentences are then tokenized and padded to fix length.","51c5ffca":"# Training the Model"}}