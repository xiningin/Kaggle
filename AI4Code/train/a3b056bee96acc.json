{"cell_type":{"896899ee":"code","a1731bb4":"code","297f80a4":"code","2188ab5c":"code","529ab0fb":"code","defc74a7":"code","37bbf536":"code","c46b9b6e":"code","9077f25a":"code","3ac7ac07":"code","92b7736a":"code","64bb9e8b":"code","7e6e254c":"code","aba3189b":"code","87aab607":"code","b95b77ca":"code","2033fda7":"code","a7af0525":"code","7a138270":"code","2f9f0083":"code","b991daee":"code","8b2d5017":"code","319e1a36":"code","ae4b9201":"code","890b86c6":"code","a67ca24f":"code","530d43d8":"code","af6d136c":"code","324bd8da":"code","a9c0b6d0":"code","145676c0":"code","56a927cc":"code","51af5b2f":"code","9bb67c3d":"markdown","47e3f453":"markdown","9657522a":"markdown"},"source":{"896899ee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","a1731bb4":"import json\nwith open(\"..\/input\/service0206-0806\/service1906_1506.json\") as of:\n    data = json.load(of)","297f80a4":"computes = [c for c in data.keys() if c!=\"timespan\"]\nvariables = [v for v in data[computes[0]] if v!='index' and v!='arrJob_scheduling']","2188ab5c":"#Check empty array\ndef getEmptyArr(data, c):\n    cObj = data[c]\n    cDf = pd.DataFrame()\n    cDf['compute'] = [c for _ in data['timespan']]\n    cDf['timespan'] = data['timespan']\n    for v in variables:\n        vArr = np.array(cObj[v])\n        if len(vArr)==0:\n            print('c=', c)\n            print('v=', v)\nfor c in computes:\n    getEmptyArr(data, c)","529ab0fb":"def addTarget(cDf, predictedVar, predictedStep):\n    cDf[target] = cDf[predictedVar].shift(-predictedStep)\n    cDf.dropna(inplace=True)","defc74a7":"def getComputeDf(data, c, predictedVar, predictedStep):\n    cObj = data[c]\n    cDf = pd.DataFrame()\n    cDf['compute'] = [c for _ in data['timespan']]\n    cDf['timespan'] = data['timespan']\n    for v in variables:\n        vArr = np.array(cObj[v])\n        if len(vArr)==0:\n            return None\n        else:\n            for i in range(len(vArr[0])):\n                cDf[v+str(i)] = vArr[:, i]\n    cDf['timespan'] = pd.to_datetime(cDf['timespan'])\n    addTarget(cDf, predictedVar, predictedStep)\n    return cDf","37bbf536":"predictedVar = 'arrTemperature0'\ntarget = predictedVar + \"_target\"\npredictedSteps = 4\ndf = pd.concat([x for x in [getComputeDf(data, c, predictedVar, predictedSteps) for c in computes] if type(x)!=\"NoneType\"])","c46b9b6e":"df = df.reset_index().drop('index', axis=1)","9077f25a":"features = [x for x in df.columns if x not in ['compute', 'timespan', 'arrTemperature0_target']]","3ac7ac07":"features","92b7736a":"from matplotlib import pyplot as plt\nimport seaborn as sns","64bb9e8b":"def plotAttrDataOfId(data, compute, features):\n    plt.figure(figsize=(30, 20))\n    for i, v in enumerate(features):\n        plt.subplot(10, 3, i+1)\n        cDf = df[df['compute']==compute]\n        plt.plot(cDf['timespan'], cDf[v])\n        plt.title(v)\n        plt.tight_layout()","7e6e254c":"for x in np.random.randint(0, len(computes), 3):\n    plotAttrDataOfId(df, computes[x], features)","aba3189b":"def plotDataDistribution(data, features):\n    plt.figure(figsize=(30, 10))\n    for i, v in enumerate(features):\n        plt.subplot(3, 10, i+1)\n        sns.distplot(list(data[v].values))\n        plt.title(v)\n    plt.tight_layout()","87aab607":"plotDataDistribution(df, features)","b95b77ca":"X_dfs = []\ny = []\nnumberOfSequences = 400\nsequenceSteps = 5\n# generate training data.\nfor compute in computes:\n    cDf = df[df['compute']==compute]\n    if(len(cDf) > sequenceSteps):\n        randSteps = np.random.randint(0, len(cDf)-sequenceSteps, numberOfSequences)\n        for randStep in randSteps:\n            X_dfs.append(cDf.iloc[randStep:randStep+sequenceSteps])\n            y.append(X_dfs[-1][target].values[-1])","2033fda7":"from sklearn.model_selection import train_test_split\nX_train_dfs, X_test_dfs, y_train, y_test = train_test_split(X_dfs, y, test_size=0.33)","a7af0525":"# combine the training data to create a scaler\ntrain_dfs = pd.concat(X_train_dfs)","7a138270":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(train_dfs[features].values)","2f9f0083":"X_train = np.array([scaler.transform(item[features].values) for item in X_train_dfs])\nX_test = np.array([scaler.transform(item[features].values) for item in X_test_dfs])","b991daee":"y_train = np.array(y_train)\ny_test = np.array(y_test)","8b2d5017":"sns.distplot(y_train)","319e1a36":"sns.distplot(y_test)","ae4b9201":"from keras import regularizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\n\n# from keras import backend as K\n# K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=36, inter_op_parallelism_threads=36)))\n\n\ndef createModel(l1Nodes, l2Nodes, d1Nodes, d2Nodes, inputShape):\n    # input layer\n    lstm1 = LSTM(l1Nodes, input_shape=inputShape, return_sequences=True, kernel_regularizer=regularizers.l2(0.1))\n    do1 = Dropout(0.2)\n    \n    lstm2 = LSTM(l2Nodes, return_sequences=True, kernel_regularizer=regularizers.l2(0.1))\n    do2 = Dropout(0.2)\n    \n    flatten = Flatten()\n    \n    dense1 = Dense(d1Nodes, activation='relu')\n    do3 = Dropout(0.2)\n    \n    dense2 = Dense(d2Nodes, activation='relu')\n    do4 = Dropout(0.2)\n    \n    # output layer\n    outL = Dense(1, activation='relu')\n    # combine the layers\n#     layers = [lstm1, do1, lstm2, do2, dense1, do3, dense2, do4, outL]\n    layers = [lstm1, lstm2, flatten,  dense1, dense2, outL]\n    # create the model\n    model = Sequential(layers)\n    model.compile(optimizer='adam', loss='mse')\n    return model","890b86c6":"from keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import load_model\n# ten fold\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(n_splits=3, shuffle=True)\nfrom keras.models import load_model\nmsescores = []\ncounter= 0\nfor trainIdx, testIdx in kfold.split(X_train, y_train):\n    counter = counter + 1\n    # create callbacks\n    model_path = 'best_model_fold'+str(counter)+'.h5'\n    mc = ModelCheckpoint(model_path, monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n    es = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1)\n    # create model\n    model = createModel(64, 64, 8, 8, (X_train.shape[1], X_train.shape[2]))\n    model.fit(X_train[trainIdx], y_train[trainIdx], validation_data=(X_train[testIdx], y_train[testIdx]), batch_size=32, epochs=40, callbacks=[mc, es])\n    # Done load the best model of this fold\n    saved_model = load_model(model_path)\n    msescores.append({'path': model_path, 'mse': saved_model.evaluate(X_train[testIdx], y_train[testIdx])})","a67ca24f":"msescores","530d43d8":"for md in msescores:\n    saved_model = load_model(md['path'])\n    print(saved_model.evaluate(X_test, y_test))","af6d136c":"best_model = load_model(msescores[np.argmin([sc['mse'] for sc in msescores])]['path'])","324bd8da":"predicted = saved_model.predict(X_test)","a9c0b6d0":"baseline = np.array([df[predictedVar].values[-1] for df in X_test_dfs])","145676c0":"plt.figure(figsize=(50, 10))\nplt.plot(range(50), predicted[:50], 'x', label='predicted')\nplt.plot(range(50), baseline[:50], 'v', label='baseline')\nplt.plot(range(50), y_test[:50], 'o', label='actual')\nplt.legend()","56a927cc":"from sklearn.metrics import mean_squared_error\nmse = mean_squared_error(y_test, predicted)\nmsebaseline = mean_squared_error(y_test, baseline)","51af5b2f":"print('mse=', mse)\nprint('msebaseline=', msebaseline)","9bb67c3d":"# Plot and see data distribution","47e3f453":"# Scale","9657522a":"# Generate sequences\nMay need to fill forward (time sampling)"}}