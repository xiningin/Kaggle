{"cell_type":{"6a6cbc62":"code","92d3d87e":"code","ff9883ce":"code","084c4966":"code","3c48d697":"code","1ee6a68d":"code","6a9ea725":"code","ff8100b1":"code","70792358":"code","7bdacf3e":"code","dfd5a6af":"code","886aaee5":"code","3fefca2c":"code","cf47ddfd":"code","404fc294":"code","60baa645":"code","da7ae067":"code","bbdc9bb1":"code","25e8f751":"code","b1e99a6e":"code","1c0af6e9":"code","d54a63ec":"code","25442e37":"code","8588280f":"code","cdcdbc99":"code","8dae370b":"code","f7c6a9bd":"code","900451d8":"code","40238ea1":"code","7a6bffcd":"code","4efe2091":"code","18fb1f50":"markdown","bf0502c5":"markdown","5baa6332":"markdown","4282b3e0":"markdown","10c494e5":"markdown","5c55d859":"markdown","b3cd5064":"markdown","0939e9fe":"markdown","9b93568d":"markdown"},"source":{"6a6cbc62":"from IPython.display import Image\nImage(\"..\/input\/credit-card-image\/are-credit-cards-the-same-in-the-us-and-canada.jpg\")","92d3d87e":"import pandas as pd \nimport numpy as np \nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc,roc_auc_score,average_precision_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline","ff9883ce":"df=pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","084c4966":"df.head()","3c48d697":"df.describe()","1ee6a68d":"df.shape","6a9ea725":"df.dtypes","ff8100b1":"df.isnull().sum()","70792358":"sns.countplot(data=df,x='Class');\n\n#Highly unblanced class ","7bdacf3e":"from sklearn.preprocessing import RobustScaler\n\n\ncol=[col for col in df.columns if col not in ['Time','Class']]\n\nfor i in col:\n    sc=RobustScaler()\n    df[i]=sc.fit_transform(df[i].values.reshape(-1,1))\n\nX=df.drop(labels=['Time','Class'],axis=1)","dfd5a6af":"# Lets visualize the dataset in 2D and see where the anamolies are \n\nfrom sklearn.decomposition import PCA \n\npca=PCA(n_components=2)\nX2D=pca.fit_transform(X)\n\nX2D=pd.DataFrame(X2D,columns=['pca1','pca2'])\n\nX2D['Class']=df['Class']\n\n#The Explained vairance ratio is pretty low in 2 dimension and I assume this is because the dataset has already PCA \n#perfromed for anonymization\n\nprint(pca.explained_variance_ratio_)","886aaee5":"samp=X2D.sample(frac=1)\n\nplt.scatter(samp['pca1'],samp['pca2'],c=samp['Class'],alpha=0.1);\nplt.title('PCA for abnormality detection');\nplt.xlabel('PCA_1');\nplt.ylabel('PCA_2');","3fefca2c":"from sklearn.decomposition import PCA \n\npca=PCA(n_components=3)\nX3D=pca.fit_transform(X)\n\nX3D=pd.DataFrame(X3D,columns=['pca1','pca2','pca3'])\nX3D['Class']=df['Class']\n\n# We can change the frac value to reduce the points plotted \nsamp=X3D.sample(frac=0.02)","cf47ddfd":"import plotly.express as px\nfig = px.scatter_3d(samp, x='pca1', y='pca2', z='pca3',\n              color='Class')\nfig.show()","404fc294":"!pip install chart_studio","60baa645":"import chart_studio\n\nusername = 'sumsaw' # your username\napi_key = 'zW5PflAEvKULQOvQc1u6' # your api key - go to profile > settings > regenerate key\nchart_studio.tools.set_credentials_file(username=username, api_key=api_key)","da7ae067":"import chart_studio.plotly as py\npy.plot(fig, filename = 'fraud_detection', auto_open=True)","bbdc9bb1":"\nfrom sklearn.mixture import GaussianMixture\n\n#bic=[]\n#aic=[]\n\n#for component in np.arange(2,20):\n\n #   gm=GaussianMixture(n_components=component,n_init=10,covariance_type='spherical')\n\n  #  gm.fit(X)\n   # bic.append(gm.bic(X))\n    #aic.append(gm.aic(X))\n\n\n#sns.lineplot(x=np.arange(2,20),y=bic,markers=True)\n#sns.lineplot(x=np.arange(2,20),y=aic,markers=True)","25e8f751":"# So in the GM model when we do score_sample we get the log of PDF for each instance . Higher values means the point is located \n#in a higher density region by and setting the threshold at 10 percentile we return all points below that thereshold value or\n#return points in lowest 10% density \n\ngm=GaussianMixture(n_components=10,n_init=10,covariance_type='spherical')\n\ngm.fit(X)\n\ndensities=gm.score_samples(X)\ndensity_threshold=np.percentile(densities,10)","b1e99a6e":"df['Gm_predict']=0\n\ndf.loc[df[densities<density_threshold].index,'Gm_predict']=1","1c0af6e9":"# So we where able to identify 446 anomaly points out of 492 in the original dataset. Also we have kept \n#the threshold value a bit higher to so we will catch a lot of False positive as well. \n\ndf[(df['Class']==1) & (df['Gm_predict']==1)].shape","d54a63ec":"\n#  Function source ; H20.ai blog \n\n%matplotlib notebook\nfrom sklearn.metrics import roc_curve, precision_recall_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef get_auc(labels, scores):\n    fpr, tpr, thresholds = roc_curve(labels, scores)\n    auc_score = auc(fpr, tpr)\n    return fpr, tpr, auc_score\n\n\ndef get_aucpr(labels, scores):\n    precision, recall, th = precision_recall_curve(labels, scores)\n    aucpr_score = np.trapz(recall, precision)\n    return precision, recall, aucpr_score\n\n\ndef plot_metric(ax, x, y, x_label, y_label, plot_label, style=\"-\"):\n    ax.plot(x, y, style, label=plot_label)\n    ax.legend()\n    \n    ax.set_ylabel(x_label)\n    ax.set_xlabel(y_label)\n\n\n\ndef prediction_summary(labels, predicted_score, predicted_class, info, plot_baseline=True, axes=None):\n    if axes is None:\n        axes = [plt.subplot(1, 2, 1), plt.subplot(1, 2, 2)]\n\n    fpr, tpr, auc_score = get_auc(labels, predicted_score)\n    plot_metric(axes[0], fpr, tpr, \"False positive rate\",\n                \"True positive rate\", \"{} AUC = {:.4f}\".format(info, auc_score))\n    if plot_baseline:\n        plot_metric(axes[0], [0, 1], [0, 1], \"False positive rate\",\n                \"True positive rate\", \"baseline AUC = 0.5\", \"r--\")\n\n    precision, recall, aucpr_score = get_aucpr(labels, predicted_score)\n    plot_metric(axes[1], recall, precision, \"Recall\",\n                \"Precision\", \"{} AUCPR = {:.4f}\".format(info, aucpr_score))\n    if plot_baseline:\n        thr = sum(labels)\/len(labels)\n        plot_metric(axes[1], [0, 1], [thr, thr], \"Recall\",\n                \"Precision\", \"baseline AUCPR = {:.4f}\".format(thr), \"r--\")\n\n    plt.show()\n    return axes\n","25442e37":"print('AUC score is {}'.format(roc_auc_score(df['Class'],-1*gm.score_samples(X))))\nprint('AUCPR score is {}'.format(average_precision_score(df['Class'],-1*gm.score_samples(X))))","8588280f":"axes = prediction_summary(\n    df[\"Class\"], -1*gm.score_samples(X), df[\"Gm_predict\"], \"sklearn\")","cdcdbc99":"from sklearn.ensemble import IsolationForest\n\nIF=IsolationForest(n_estimators=100,n_jobs=-1,contamination=0.1,random_state=7,max_samples=0.25)\n\ndf['isolation_forest_predict']=IF.fit_predict(X)\n\ndf['isolation_forest_predict'].replace({1:0,-1:1},inplace=True)","8dae370b":"df['isolation_forest_predict'].value_counts()","f7c6a9bd":"df['Class'].value_counts()","900451d8":"# So we where able to identify 444 anomaly points out of 492 in the original dataset. Also we have kept \n#the threshold value a bit higher to so we will catch a lot of False positive as well. \n\ndf[(df['Class']==1) & (df['isolation_forest_predict']==1)].shape","40238ea1":"df['score_sample_IF']=IF.score_samples(X)*-1","7a6bffcd":"print('AUC score is {}'.format(roc_auc_score(df['Class'],-1*IF.score_samples(X))))\nprint('AUCPR score is {}'.format(average_precision_score(df['Class'],-1*IF.score_samples(X))))","4efe2091":"axes = prediction_summary(\n    df[\"Class\"], df[\"score_sample_IF\"], df[\"isolation_forest_predict\"], \"sklearn\")","18fb1f50":"We do see a small patch of anomaly but there are not well seperated with lot of overlap with normal points ","bf0502c5":"### Lets check out the algorithm Isloation Forest \n\nThe below model was tried with different values estimators,contimination and max_sample sizes . We are casting the \ncontamination higher than 0.001% which the valued from our dataset and accourding to the above EDA we know many anaomaly points are overlapping the good points so as to capture them we need to flag more data as fraud ","5baa6332":"So there is no clear region where we can see the Fraud points in 2 dimension ","4282b3e0":"### Motivation \n\nIn this notebook I am going th explore some of the various machine learning models used for Anomaly detection. <br>\n\nAnomaly detection is very important as it can be applied to detect fraud in a bank transcation , catching defects in a manufacturing product or even detect outliers in a dataset. <br> It can also be used for data labeling \n\nThe challenging part in anomaly detection is that its dataset is higly unbalanced that is the anaomaly occurs around 1% of the total instances in the dataset. <br>\n\n### Approach \n\nThere could be three aproached for the given problem . \n* Treating as supervised learning problem : Fitting various models on the dataset to classify<br> \n* Treating as semi-supervised learning : Where you just map the correct data and any data points outside this domain are anomaly<br>\n* Treating is a unsupervised learning : Where you try to isolate the fraud points from proper transaction without using labels <br>\n\nEven though this dataset has labels but I am going to apply various anaomaly detection algorithms in this notebook and then via the help of the labels check out if I got all the anomalies correctly classified. \n\n### Dataset \n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.<br>","10c494e5":"### Scaling the feature column","5c55d859":"For using the Gaussian Mixture algorithm we need to provide the number of cluster . So I computed the AIC and BIC scores to see what's the clusted number which minimizes score but I could not spot a clear minimum value and the score kept on dropping as I increased the cluster to 20 . \n\nJust taking a clue from slope change and computation hardware limitation I have settled on 10 cluster .Uncomment the below cell and increase the upper limit number to see the effect ","b3cd5064":"### Scoring the Models \n\nFor imbalance dataset we should consider the Area Under the Receiver Operating Characteristic Curve (AUC) and area under precision-recall curve(AUCPR). For AUC the baseline is 0.5 which is random guessing and perfect model is 1 . Similary the AUCPR the perfect model score is 1 and the baseline score is the relative count of the positive class so for this dataset 0.00172\n","0939e9fe":"## Please Upvote if you like . Comment below for any issues ","9b93568d":"Lets visualize the same graph in 3D to for some extra prespestive "}}