{"cell_type":{"dd457459":"code","5641124b":"code","d42d4639":"code","8a57504c":"code","76801784":"code","42e614b7":"code","cfcf201d":"code","dcd51953":"code","a0fa8197":"code","5118e225":"code","ed43793c":"code","fc810b3c":"code","60426ca6":"code","f992ef26":"code","4314f321":"code","c8a54d52":"code","27ef3e41":"code","e8c51adc":"code","82dc1583":"code","62c08760":"code","7da4f717":"code","ca94510c":"code","e262da31":"code","5606fdcd":"code","2fe9f973":"code","ca33dd2c":"code","283d6b78":"code","9fa888f9":"code","ba5052d9":"code","211b5606":"code","5ea423a5":"code","25da01fa":"code","ecd79538":"code","5926586d":"code","6e268f9f":"code","07b0c34e":"code","4874b6b6":"code","047da580":"code","5e944734":"code","9e9b8a44":"markdown","4c35ad9b":"markdown","5334e3ed":"markdown","05a345d2":"markdown","1ce70ef7":"markdown","83b5f8e6":"markdown","75f2b1ca":"markdown","b4fba5cf":"markdown","7e0ed8fc":"markdown","acf10e37":"markdown","22afc11b":"markdown","ec4dc029":"markdown","e015d0a4":"markdown","74e9b43e":"markdown","307c3825":"markdown","e75805de":"markdown","26165b7d":"markdown","cb1da5a7":"markdown","17ceec3c":"markdown","dc8aea14":"markdown","8bcf78b8":"markdown","83a16199":"markdown","da96cf3b":"markdown","b41591fd":"markdown","f3f99a6b":"markdown"},"source":{"dd457459":"# Importing all required packages\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\n#import modin.pandas as pd\n#import ray\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport seaborn as sns\nimport datetime\nimport cufflinks as cf\nimport plotly as py\nimport plotly.graph_objs as go\nimport ipywidgets as widgets\nfrom pandas.api.types import is_object_dtype,is_string_dtype, is_numeric_dtype\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n%matplotlib inline\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler,RobustScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import RFE,RFECV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.preprocessing import StandardScaler,PolynomialFeatures\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor,StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn import tree\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import make_pipeline\n\nimport os\n\n# hide warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set(style=\"whitegrid\")\npd.set_option('display.max_columns', 300)\npd.set_option('display.max_rows', 10000)\npy.offline.init_notebook_mode(connected=True) # plotting in offilne mode \ncf.set_config_file(offline=False, world_readable=True, theme='ggplot')\npd.set_option('display.max_colwidth', 1) # make sure data and columns are displayed correctly withput purge\npd.options.display.float_format = '{:20,.2f}'.format # display float value with correct precision ","5641124b":"# Generic Functions\n\n# Method to get Meta-Data about any dataframe passed \ndef getMetadata(dataframe) :\n    metadata_matrix = pd.DataFrame({\n                    'Datatype' : dataframe.dtypes, # data types of columns\n                    'Total_Element': dataframe.count(), # total elements in columns\n                    'Null_Count': dataframe.isnull().sum(), # total null values in columns\n                    'Null_Percentage': round(dataframe.isnull().sum()\/len(dataframe) * 100,2) ,# percentage of null values\n                    'Unique_Value': dataframe.nunique()\n                       })\n    return metadata_matrix\n\ndef getVIF(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif\n\ndef binary_map(x):\n    return x.map({'yes': 1, \"no\": 0})\n\ndef cross_validation(X_train,y_train,lm):\n    folds = KFold(n_splits = 2, shuffle = True, random_state = 100)\n    hyper_params = [{'n_features_to_select': list(range(len(X_train.columns)))}]\n    lm.fit(X_train, y_train)\n    rfe = RFE(lm)             \n    model_cv = GridSearchCV(estimator = rfe, \n                            param_grid = hyper_params, \n                            scoring= 'r2', \n                            cv = folds, \n                            verbose = 1,\n                            return_train_score=True)      \n\n    model_cv.fit(X_train, y_train)                  \n    cv_results = pd.DataFrame(model_cv.cv_results_)\n    plt.figure(figsize=(16,6))\n    plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_test_score\"])\n    plt.plot(cv_results[\"param_n_features_to_select\"], cv_results[\"mean_train_score\"])\n    plt.xlabel('number of features')\n    plt.ylabel('r-squared')\n    plt.title(\"Optimal Number of Features\")\n    plt.legend(['test score', 'train score'], loc='upper left')\n    \ndef plot_bar_chart(plotting_frame,x_column,y_column) :\n            \n        x_axis_title = x_column.title()\n        y_axis_title = y_column.title()\n        \n        graph_title = \"Bar Chart [\" + x_axis_title.title() + \" Vs \" + y_axis_title.title() + \"]\"\n        \n        layout = go.Layout(\n             title = graph_title,\n             yaxis=dict(\n                title=y_axis_title\n             ),\n             xaxis=dict(\n                 title=x_axis_title\n             )\n        )\n\n        data_to_be_plotted = [\n            go.Bar(\n                x=plotting_frame[x_column], \n                y=plotting_frame[y_column]\n            )\n        ]\n\n\n        figure = go.Figure(data=data_to_be_plotted,layout=layout)\n        py.offline.iplot(figure)\n        \n        \ndef plot_pie_chart(plotting_frame,x_column,y_column) : \n        \n        labels = plotting_frame[x_column].tolist()\n        values = plotting_frame[y_column].tolist()\n\n        trace = go.Pie(labels=labels, values=values)\n\n        py.offline.iplot([trace])\n\n        \ndef plot_box_chart(dataframe) :\n    data = []\n    for index, column_name in enumerate(dataframe) :\n        data.append(\n        go.Box(\n            y=dataframe.iloc[:, index],\n            name=column_name\n         ))   \n        \n    layout = go.Layout(\n    yaxis=dict(\n        title=\"Frequency\",\n        zeroline=False\n    ),\n       boxmode='group'\n    )\n    \n    fig = go.Figure(data=data, layout=layout)    \n    py.offline.iplot(fig) \n    \ndef plot_group_bar_chart(plot,col,hue) : \n    hue_col = pd.Series(data = hue)\n    fig, ax = plt.subplots()\n    width = len(plot[col].unique()) + 6 + 5*len(hue_col.unique())\n    fig.set_size_inches(width , 10)\n    ax = sns.countplot(data = loan_plot, x= col, order=plot[col].value_counts().index,hue = hue,palette=\"Set2\") \n    \n    for p in ax.patches:\n                # Some segment wise value we are getting as Nan as respective value not present to tackle the Nan using temp_height\n                temp_height = p.get_height()\n                \n                if math.isnan(temp_height):\n                    temp_height = 0.01\n                    \n                \n                ax.annotate('{:1.1f}%'.format((temp_height*100)\/float(len(loan_plot))), (p.get_x()+0.05, temp_height+20)) \n    \n    plt.show()\n\ndef col_list(df):\n    num_list = []\n    cat_list = []\n    for column in df:\n        if is_numeric_dtype(df[column]):\n            num_list.append(column)\n        elif is_object_dtype(df[column]):\n            cat_list.append(column)    \n    return cat_list,num_list\n\ndef outliers(df,num_list):\n    oc = []\n    noc=[]\n    l=[]\n    u=[]\n    for c in num_list:\n        data=df[c].values\n        lower, upper = np.mean(data) - (np.std(data) * 3), np.mean(data) + (np.std(data) * 3)\n        outliers=len([x for x in data if x < lower or x > upper])\n        non_outliers=len([x for x in data if x >= lower and x <= upper])\n        l.append(lower)\n        u.append(upper)\n        oc.append(outliers)\n        noc.append(non_outliers)\n    oc_metric = pd.Series(oc, name = 'Outliers')\n    noc_metric = pd.Series(noc, name = 'Non-Outliers')\n    lower_limit = pd.Series(l, name = 'Lower Limit')\n    uper_limit = pd.Series(u, name = 'Upper Limit')    \n    outl = pd.DataFrame(num_list,columns = ['Columns'])\n    final_metric = pd.concat([outl, oc_metric, noc_metric,lower_limit,uper_limit], axis = 1)\n #   final_metric.set_index(\"Columns\", inplace = True)\n    return final_metric    \n\ndef assumption_graph(y_train,y_pred_train):\n    \n    ### Assumption of Error Terms Being Independent\n    y_res_train = y_train - y_pred_train\n    plt.scatter( y_pred_train , y_res_train)\n    plt.axhline(y=0, color='r', linestyle=':')\n    plt.xlabel(\"Predictions\")\n    plt.ylabel(\"Residual\")\n    plt.show()\n    \n    # Distribution of errors\n    p = sns.distplot(y_res_train,kde=True)\n    p = plt.title('Normality of error terms\/residuals')\n    plt.xlabel(\"Residuals\")\n    plt.show()\n    \n    #### Variance\n    sns.regplot(x=y_train, y=y_pred_train)\n    plt.title('Predicted Points Vs. Actual Points', fontdict={'fontsize': 20})\n    plt.xlabel('Actual Points', fontdict={'fontsize': 15})\n    plt.ylabel('Predicted Points', fontdict={'fontsize': 15})\n    plt.show()\n    print(\"Shape after outlier correction \",price_df.shape ,\"rows & columns.\")\n\ndef prediction_matrix(model,X_train,X_test,y_train,y_test):\n    begin = time.time()\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    end = time.time()\n    time_taken=end-begin\n    print('Time taken to predict train and test {} sec'.format(time_taken))\n    metric = []\n    r2_train_lr = r2_score(y_train, y_pred_train)\n    rss_train_lr = np.sum(np.square(y_train - y_pred_train))\n    mse_train_lr = mean_squared_error(y_train, y_pred_train)\n    adjusted_r2_train_lr= (1 - (1-model.score(X_train, y_train))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\n    \n    r2_test_lr = r2_score(y_test, y_pred_test)\n    rss_test_lr = np.sum(np.square(y_test - y_pred_test))\n    mse_test_lr = mean_squared_error(y_test, y_pred_test)\n    adjusted_r2_test_lr= (1 - (1-model.score(X_test, y_test))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1)) \n    \n    \n    metric.append(r2_train_lr)\n    metric.append(r2_test_lr)\n    metric.append(adjusted_r2_train_lr)\n    metric.append(adjusted_r2_test_lr)\n    metric.append(rss_train_lr)\n    metric.append(mse_test_lr)\n    metric.append(mse_train_lr**0.5)\n    metric.append(mse_test_lr**0.5)\n\n    y_res_train = y_train - y_pred_train\n    y_res_test = y_test - y_pred_test\n    \n    plt.figure(figsize=(20, 12))\n    sns.set(font_scale= 1)\n    sns.set_style('whitegrid')\n    \n    plt.subplot(2,3,1)\n#    plt.scatter( y_pred_train , y_res_train)\n    sns.scatterplot(x=y_pred_test,y=y_res_test,color='Blue')\n    plt.axhline(y=0, color='r', linestyle=':')\n    plt.xlabel(\"Train Predictions\")\n    plt.ylabel(\"Train Residual\")\n    \n    plt.subplot(2,3,2)\n    p = sns.distplot(y_res_train,kde=True,color='Blue')\n    p = plt.title('Normality of error terms\/residuals on Train Data')\n    plt.xlabel(\"Residuals\")\n    \n\n    plt.subplot(2,3,3)\n    p=sns.regplot(x=y_train, y=y_pred_train,color='Green')\n    p=plt.title('Predicted Points Vs. Actual Points on Train Data')\n    plt.xlabel('Actual Points')\n    plt.ylabel('Predicted Points')\n\n    plt.subplot(2,3,4)\n    sns.scatterplot(x=y_pred_test,y=y_res_test,color='Green')\n  #  plt.scatter(y_pred_test , y_res_test)\n    plt.axhline(y=0, color='r', linestyle=':')\n    plt.xlabel(\"Test Predictions\")\n    plt.ylabel(\"Test Residual\")\n\n    plt.subplot(2,3,5)\n    p = sns.distplot(y_res_test,kde=True,color='Green')\n    p = plt.title('Normality of error terms\/residuals on Test Data')\n    plt.xlabel(\"Residuals\")\n\n    plt.subplot(2,3,6)\n    p=sns.regplot(x=y_test, y=y_pred_test,color='Green')\n    p=plt.title('Predicted Points Vs. Actual Points on Test Data')\n    plt.xlabel('Actual Points')\n    plt.ylabel('Predicted Points') \n    \n    sns.despine()\n    \n#    assumption_graph(y_train,y_pred_train)\n#    assumption_graph(y_test,y_pred_test)\n    return metric\n\n\ndef feature_importance(model,df,title):\n# Scatter plot \n    trace = go.Scatter(\n        y = model,\n        x = df.columns,\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1.3,\n            size = 12,\n            color = model,\n            colorscale='Portland',\n            showscale=True\n        ),\n        text = df.columns\n    )\n    data = [trace]\n\n    layout= go.Layout(\n        autosize= True,\n        title= title,\n        hovermode= 'closest',\n         xaxis= dict(\n             ticklen= 5,\n             showgrid=False,\n            zeroline=False,\n            showline=False\n         ),\n        yaxis=dict(\n            title= 'Feature Importance',\n            showgrid=False,\n            zeroline=False,\n            ticklen= 5,\n            gridwidth= 2\n        ),\n        showlegend= False\n    )\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig,filename='scatter')","d42d4639":"def cleaning(df):\n\n    print(\"Records before dropping duplicates  : \" + str(df.shape[0]))\n    df.drop_duplicates(keep=False,inplace=True)\n    print(\"Records after dropping duplicates  : \" + str(df.shape[0]))    \n    \n    print(\"Null Value Analysis\")\n    price_metadata = getMetadata(df)\n    price_metadata_group = price_metadata.groupby(\"Null_Percentage\").count().reset_index()\n    price_metadata_group.sort_values([\"Null_Percentage\"], axis=0,ascending=False, inplace=True)\n    plot_pie_chart(price_metadata_group,\"Null_Percentage\",\"Null_Count\")\n\n    print('Completely Missing Data')\n    completly_missing_data = price_metadata[price_metadata[\"Null_Percentage\"] == 100.0]\n    drop_missing_column = completly_missing_data.index.tolist()\n    print(\"Null Columns before deleting  : \" + str(df.shape[1]))\n    df.drop(drop_missing_column,inplace=True,axis=1)\n    print(\"Null Columns after deleting : \" + str(df.shape[1]))\n    \n    print('80%+ Missing Data')\n    missing_data_greater_80 = price_metadata[(price_metadata[\"Null_Percentage\"] > 80.0) & \n                                         (price_metadata[\"Null_Percentage\"] < 100.0)]\n    drop_missing_column_80 = missing_data_greater_80.index.tolist()\n    #df.drop(drop_missing_column_80, axis =1, inplace=True)\n    display(drop_missing_column_80)\n    print(\"Shape after deleting unique value columns \",df.shape ,\"rows & columns.\")\n    \n    print('Identify and drop columns having single value as they will not add any value to our analysis')\n    unique_value = df.nunique()\n    col_with_only_one_value = unique_value[unique_value.values == 1]\n    col_to_drop = col_with_only_one_value.index.tolist()\n    display(col_to_drop)\n    df.drop(col_to_drop, axis =1, inplace=True)\n    print(\"Shape after deleting unique value columns \",df.shape ,\"rows & columns.\")\n    \n    print('Datatype Check')\n    price_data_type = getMetadata(df)\n    display(price_data_type[\"Datatype\"].value_counts())\n    \n    price_numeric = df.select_dtypes(include=['object'])\n    print('Object Records:')\n    display(price_numeric.head(5))\n\n    print('Non Object Records:')\n    price_object = df.select_dtypes(exclude=['object'])\n    display(price_object.head(5))\n\n    print('Columns requiring imputation:')\n    impute_columns = getMetadata(df)\n    impute_columns = impute_columns[impute_columns[\"Null_Count\"] > 0]\n    display(impute_columns.sort_values(by=\"Null_Count\",ascending = False))\n    \n    return df\n\ndef outlier_treatment(df,columns):\n    for col in columns:\n        lower_limit=(outlier_info[(outlier_info.Columns==col)][\"Lower Limit\"].values)[0]\n        upper_limit=(outlier_info[(outlier_info.Columns==col)][\"Upper Limit\"].values)[0]\n        df[col]=np.where(df[col]>upper_limit,upper_limit,df[col])\n        df[col]=np.where(df[col]<lower_limit,lower_limit,df[col])\n    display(df[num_list].describe())\n\n    ### Post fixing outliers\n    outlier_columns=outlier_info[outlier_info.Outliers>0][\"Columns\"]\n    i=int(len(outlier_columns)\/3)\n    plt.figure(figsize=(30,30))\n    sns.set(font_scale=1.0)\n    sns.set_style(\"whitegrid\")\n    j=1\n    for p,c in enumerate(columns):\n        plt.subplot(i,i,j)\n        sns.boxplot(y=df[c],orient=\"h\")\n        plt.ylabel(c)\n        j=j+1\n    plt.show()\n    print(\"Shape after outlier correction \",df.shape ,\"rows & columns.\")\n    return df\n    \ndef impute(df):\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    for name in df.select_dtypes(\"object\"):\n        df[name] = df[name].fillna(\"None\")\n    return df\n\ndef encode(df,nominal_feature,ordinal_feature):\n    numeric_feature=list(df.select_dtypes(exclude=['object']).columns)\n    display(len(ordinal_feature),len(nominal_feature),len(numeric_feature))\n    num_list=numeric_feature.copy()\n    price_ordinal=df[ordinal_feature]\n    price_nominal=df[nominal_feature]\n    price_numeric=df[num_list]\n    #### Label Encoding\n    df[ordinal_feature]=df[ordinal_feature].astype('category')\n    for catg in ordinal_feature:\n        df[catg]=df[catg].cat.codes\n    display(\"Shape after dummy encoding \",df.shape ,\"rows & columns.\")\n    ### One Hot Encoding\n    price_dummies = pd.get_dummies(price_nominal, drop_first=True)\n    df = df.drop(list(price_nominal.columns), axis=1)\n    df = pd.concat([price_dummies,df],axis = 1)\n    display(df.head())\n    print(\"Shape after dummy encoding \",df.shape ,\"rows & columns.\")\n    return df\n\ndef baseline(X_train,y_train,X_test,y_test,models):\n    cross_metric_train = []\n    cross_metric_test =[]\n    for i,x in enumerate(models):\n        score=cross_val_score(x,X_train,y_train,cv=10,scoring='r2')\n        score2=cross_val_score(x,X_test,y_test,cv=10,scoring='r2')\n        cross_metric_train.append(score.mean())\n        cross_metric_test.append(score2.mean())\n    y=pd.Series(cross_metric_test,name='Test')\n    lr_table = {'Metric': ['LR','Lasso','Ridge'],\n            'Train': cross_metric_train\n            }\n    lr_df=pd.DataFrame(lr_table,columns=[\"Metric\",\"Train\"])\n    baseline_metric=pd.concat([lr_df,y],axis=1)\n    display(baseline_metric)\n\n\ndef loadData():\n    input_path='..\/input\/house-prices-advanced-regression-techniques\/'\n    train=input_path +'train.csv'\n    test=input_path +'test.csv'\n    train_df=pd.read_csv(train, index_col='Id')\n    test_df=pd.read_csv(test,index_col='Id')\n    df=pd.concat([train_df,test_df],axis=0)\n    print('Top Five Records')\n    display(df.head())\n    print('Shape',df.info())\n    price_metadata=getMetadata(df)\n    display(price_metadata)   \n    cleaning(df)\n    \n    return df","8a57504c":"price_df=loadData()","76801784":"price_df[\"MasVnrArea\"].fillna((price_df[\"MasVnrArea\"].median()),inplace=True)\nprice_df[\"LotFrontage\"].fillna((price_df[\"LotFrontage\"].median()),inplace=True)\n#price_df[\"GarageYrBlt\"].fillna((price_df[\"YearBuilt\"]),inplace=True)\nprice_df[\"PoolQC\"].fillna('NA',inplace=True)\nprice_df[\"MiscFeature\"].fillna('NA',inplace=True)\nprice_df[\"Alley\"].fillna('NA',inplace=True)\nprice_df[\"Fence\"].fillna('NA',inplace=True)\nprice_df[\"FireplaceQu\"].fillna('NA',inplace=True)\nprice_df[\"GarageType\"].fillna('NA',inplace=True)\nprice_df[\"GarageFinish\"].fillna('NA',inplace=True)\nprice_df[\"GarageQual\"].fillna('NA',inplace=True)\nprice_df[\"GarageCond\"].fillna('NA',inplace=True)\nprice_df[\"BsmtExposure\"].fillna('NA',inplace=True)\nprice_df[\"BsmtFinType2\"].fillna('NA',inplace=True)\nprice_df[\"BsmtFinType1\"].fillna('NA',inplace=True)\nprice_df[\"BsmtCond\"].fillna('NA',inplace=True)\nprice_df[\"BsmtQual\"].fillna('NA',inplace=True)\nprice_df[\"MasVnrType\"].fillna('None',inplace=True)\nprice_df[\"Electrical\"].fillna((price_df[\"Electrical\"].mode()[0]),inplace=True)\ntmp2=price_df[['SalePrice','GarageYrBlt']]\n\ncol_to_drop=['SalePrice','GarageYrBlt']\nprice_df.drop(col_to_drop, axis=1, inplace=True)\ntmp1=impute(price_df)\nprice_df=pd.concat([tmp1,tmp2],axis=1)\n\nimpute_columns = getMetadata(price_df)\nimpute_columns = impute_columns[impute_columns[\"Null_Count\"] > 0]\nimpute_columns.sort_values(by=\"Null_Count\",ascending = False)","42e614b7":"price_df['MSSubClass'].replace({20:\"1-STORY 1946 & NEWER\",\n                               30:\"1-STORY 1945 & OLDER\",\n                               40:\"1-STORY W\/FINISHED\",\n                               45:\"1-1\/2 STORY - UNFINISHED\",\n                               50:\"1-1\/2 STORY FINISHED\",\n                               60:\"2-STORY 1946 & NEWER\",\n                               70:\"2-STORY 1945 & OLDER\",\n                               75:\"2-1\/2 STORY ALL AGES\",\n                               80:\"SPLIT OR MULTI-LEVEL\",\n                               85:\"SPLIT FOYER\",\n                               90:\"DUPLEX\",\n                               120:\"1-STORY PUD\",\n                               150:\"1-1\/2 STORY PUD\",\n                               160:\"2-STORY PUD\",\n                               180:\"PUD - MULTILEVEL\",\n                               190:\"2 FAMILY CONVERSION\"                         \n                              },inplace=True)\nprice_df['MoSold'].replace({1:\"Jan\",2:\"Feb\",3:\"Mar\",4:\"Apr\",5:\"May\",6:\"Jun\",\n                         7:\"Jul\",8:\"Aug\",9:\"Sep\",10:\"Oct\",11:\"Nov\",12:\"Dec\"}\n                        ,inplace=True)\n\nprint(\"Shape after correcting categorical columns \",price_df.shape ,\"rows & columns.\")\nprice_df.head(5)","cfcf201d":"cat_list,num_list=col_list(price_df)\nprint(\"Category Columns:\",cat_list)\nprint(\"Continous Columns:\",num_list)\nprice_df[num_list].describe()\noutlier_info=outliers(price_df,num_list)\ndisplay(outlier_info)\noutlier_columns_fix=outlier_info[(outlier_info[\"Upper Limit\"]>20) & (outlier_info.Outliers>0)][\"Columns\"]\ndisplay(list(outlier_columns_fix))\nprice_df=outlier_treatment(price_df,list(outlier_columns_fix))","dcd51953":"price_df[\"Age\"]=price_df[\"YrSold\"]-price_df[\"YearBuilt\"]\nprice_df[\"GarageAge\"]=price_df[\"YrSold\"] - price_df[\"GarageYrBlt\"]\nprice_df[\"GarageAge\"].fillna(99,inplace=True)\nprice_df[\"LivLotRatio\"] = price_df.GrLivArea \/ price_df.LotArea\nprice_df[\"Spaciousness\"] = (price_df[\"1stFlrSF\"] + price_df[\"2ndFlrSF\"]) \/ price_df.TotRmsAbvGrd\nprice_df[\"MedNhbdArea\"] = price_df.groupby(\"Neighborhood\")[\"GrLivArea\"].transform(\"median\")\nprice_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Story',''))\nprice_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Fin',''))\nprice_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Unf',''))\nprice_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Lvl',''))\nprice_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('Foyer',''))\nprice_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].apply(lambda x: x.replace('S','1'))\nprice_df[\"HouseStyle\"]=price_df[\"HouseStyle\"].astype('float64')\nMSClass=[k for k,v in zip(list((price_df[\"MSSubClass\"].value_counts()).index),\n                          list((price_df[\"MSSubClass\"].value_counts()).values)) if v<70]\nNeig=[k for k,v in zip(list((price_df[\"Neighborhood\"].value_counts()).index),\n                       list((price_df[\"Neighborhood\"].value_counts()).values)) if v<50]\nprice_df[\"MSSubClass\"]=price_df[\"MSSubClass\"].apply(lambda x: \"Others\" if x in MSClass else x)\nprice_df[\"Neighborhood\"]=price_df[\"Neighborhood\"].apply(lambda x: \"Others\" if x in Neig else x)\ncol_to_drop=[\"YrSold\",\"YearBuilt\",\"GarageYrBlt\",\"YearRemodAdd\"]\nprice_df.drop(col_to_drop,inplace=True,axis=1)\ndisplay(price_df.head())\nprint(\"Shape after dervived columns \",price_df.shape ,\"rows & columns.\")\nx = price_df[\"SalePrice\"]\nsns.set_style(\"whitegrid\")\nsns.distplot(x)\nplt.show()\nprice_df[\"SalePrice_log\"] = np.log(price_df.SalePrice)\nx = price_df.SalePrice_log\nsns.distplot(x)\nplt.show()","a0fa8197":"impute_columns = getMetadata(price_df)\nimpute_columns = impute_columns[impute_columns[\"Null_Count\"] > 0]\nimpute_columns.sort_values(by=\"Null_Count\",ascending = False)","5118e225":"cat_list,num_list=col_list(price_df)\nprint(\"Category Columns:\",cat_list)\nprint(\"Continous Columns:\",num_list)\ndisplay(len(cat_list),len(num_list))\nplt.figure(figsize = (25, 15))\nsns.heatmap(price_df[num_list].corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","ed43793c":"#### Visualising the continous columns columns\nQuality_features = ['LotFrontage', 'LotArea', 'MasVnrArea', 'TotalBsmtSF', 'GrLivArea',\n                    'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', \n                    'TotRmsAbvGrd', 'Fireplaces', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', \n                    'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal','Age','GarageAge']\nplt.figure(figsize=(20, 30))\nsns.set(font_scale= 1.2)\nsns.set_style('darkgrid')\n\nfor i, feature in enumerate(Quality_features):\n    plt.subplot(7, 4, i+1)\n    sns.scatterplot(data=price_df, x=feature, y='SalePrice', palette=\"ch:.10\")         \nsns.despine()","fc810b3c":"# We will plot some joint histogram and scatter grphs to look at correlated features in more detail\ny = price_df[\"SalePrice\"]\nfeatures = [\n    \"MasVnrArea\",\n    \"BsmtFinSF1\",\n    \"TotalBsmtSF\",\n    \"1stFlrSF\",\n    \"GrLivArea\",\n    \"FullBath\",\n    \"TotRmsAbvGrd\",\n    \"Fireplaces\",\n    \"GarageCars\",\n    \"GarageArea\",\n    \"LotArea\",\n    \"LotFrontage\",\n]\n\nfor features in features:\n    sns.set_style(\"whitegrid\")\n    plt.figure(figsize=(10, 10))\n    x = price_df[features]\n    sns.jointplot(x=x, y=y, data=price_df)","60426ca6":"#### Visualising the categorical columns\nQuality_features = ['MSZoning','LandContour','Utilities','HouseStyle','OverallCond','RoofStyle','Exterior1st','ExterCond',\n                    'RoofMatl', 'ExterQual', 'BsmtQual', 'HeatingQC', 'CentralAir', \n                    'Electrical', 'KitchenQual', 'GarageQual','GarageType','SaleCondition','PoolQC','Alley','Fence']\n\nplt.figure(figsize=(30, 20))\nsns.set(font_scale= 1.2)\nsns.set_style('darkgrid')\n\nfor i, feature in enumerate(Quality_features):\n    plt.subplot(6, 4, i+1)\n    sns.barplot(data=price_df, x=feature, y='SalePrice', palette=\"ch:.10\")         \nsns.despine()","f992ef26":"'''\nfeatures = price_df.select_dtypes(include=['object']).columns\nplt.figure(figsize=(30, 20))\nsns.set_style('darkgrid')\n\nfor feature in features:\n    g = sns.FacetGrid(price_df[~price_df.SalePrice.isnull()], col=feature)\n    g.map(plt.hist, 'SalePrice');\n    sns.despine()\n'''","4314f321":"binn_col=['MSZoning','Street','Alley','LandContour',\n'Condition1','Condition2','BldgType','RoofStyle','RoofMatl','Heating','CentralAir','Electrical',\n'PavedDrive','Fence','MiscFeature','SaleType','SaleCondition']\nx=dict()\nx[\"MSZoning\"]=300\nx[\"Street\"]=10\nx[\"Alley\"]=60\nx[\"LandContour\"]=70\nx[\"LandSlope\"]=70\nx[\"Condition1\"]=100\nx[\"Condition2\"]=10\nx[\"BldgType\"]=120\nx[\"RoofStyle\"]=300\nx[\"RoofMatl\"]=15\nx[\"Heating\"]=20\nx['Electrical']=100\nx['Functional']=40\nx['PavedDrive']=100\nx['Fence']=160\nx['MiscFeature']=50\nx['SaleType']=130\nx['SaleCondition']=130\nfor p,y in x.items():\n    val=[k for k,v in zip(list((price_df[p].value_counts()).index),\n                       list((price_df[p].value_counts()).values)) if v<y]\n    price_df[p]=price_df[p].apply(lambda x: \"Others\" if x in val else x)\nprice_df.head()\n","c8a54d52":"ordinal_feature=['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC',\n                 'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond','LotShape', 'LandSlope', 'BsmtExposure', \n                 'BsmtFinType1', 'BsmtFinType2', 'Functional','GarageFinish','Utilities','PoolQC'\n                 ]\nnominal_feature= [\"MSSubClass\", \"MSZoning\", \"Street\", \"LandContour\", \n                \"LotConfig\", \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \n                 \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \n                \"MasVnrType\", \"Foundation\", \"Heating\", \"CentralAir\", \"GarageType\", \n                 \"SaleType\", \"SaleCondition\",\"PavedDrive\",'Electrical','MoSold','Alley','Fence','MiscFeature']\ndf=encode(price_df,ordinal_feature,nominal_feature)","27ef3e41":"train=df[~df.SalePrice.isnull()]\ntest=df[df.SalePrice.isnull()]\ndisplay(train.shape,test.shape)","e8c51adc":"np.random.seed(0)\ndf_train, df_validation = train_test_split(train, train_size = 0.7, test_size = 0.3, random_state = 100)","82dc1583":"X_train = df_train.drop([\"SalePrice_log\",\"SalePrice\"], axis = 1)\ny_train = df_train[\"SalePrice_log\"]\nX_validation = df_validation.drop([\"SalePrice_log\",\"SalePrice\"], axis = 1)\ny_validation = df_validation[\"SalePrice_log\"]\nX_test = test.drop([\"SalePrice_log\",\"SalePrice\"], axis = 1)","62c08760":"num_list=list(X_train.select_dtypes(exclude=['object']).columns)","7da4f717":"num_list.remove('LivLotRatio')","ca94510c":"### Scaling\nscaler = RobustScaler()\n#scaler=StandardScaler()\nX_train[num_list] = scaler.fit_transform(X_train[num_list])\nX_validation[num_list] = scaler.transform(X_validation[num_list])\nX_train.head()","e262da31":"X_test[num_list]=scaler.transform(X_test[num_list])","5606fdcd":"models=[LinearRegression(),Lasso(),Ridge()]\nbaseline(X_train,y_train,X_validation,y_validation,models)","2fe9f973":"lm = LinearRegression()\nlm.fit(X_train, y_train)\nlm_metric=prediction_matrix(lm,X_train,X_validation,y_train,y_validation)","ca33dd2c":"#cross_validation(X_train,y_train,lm)\nmin_features_to_select = 1  # Minimum number of features to consider\nrfecv = RFECV(estimator=Ridge(), step=1, cv=10,\n              scoring='r2',\n              min_features_to_select=min_features_to_select)\nrfecv.fit(X_train, y_train)\nprint(\"Optimal number of features : %d\" % rfecv.n_features_)\n# Plot number of features VS. cross-validation scores\nplt.figure()\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score\")\nplt.plot(range(min_features_to_select,\n               len(rfecv.grid_scores_) + min_features_to_select),\n         rfecv.grid_scores_)\nplt.show()","283d6b78":"### selecting columns post rfe\ncol = X_train.columns[rfecv.support_]\nlen(list(col))\nX_train_rfe = X_train[col]\nX_validation_rfe  = X_validation[col]\nlm_rfe=LinearRegression()\nlm_rfe.fit(X_train_rfe,y_train)\nlm_rfe_metric=prediction_matrix(lm_rfe,X_train_rfe,X_validation_rfe,y_train,y_validation)","9fa888f9":"# list of alphas to tune - if value too high it will lead to underfitting, if it is too low, \n# it will not handle the overfitting\nparams = {'alpha': [0.001, 0.01, 0.1, 1.0,10.0,20,50,100,150,200,500]}\nestimator = Ridge()\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = estimator, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error',  \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train_rfe, y_train) \ndisplay(model_cv.best_params_)\nalpha = model_cv.best_params_['alpha']\nridge = Ridge(alpha=alpha)\nridge.fit(X_train_rfe, y_train)\nlm_ridge_metric=prediction_matrix(ridge,X_train_rfe,X_validation_rfe,y_train,y_validation)","ba5052d9":"# list of alphas to tune - if value too high it will lead to underfitting, if it is too low, \n# it will not handle the overfitting\nparams = {'alpha': [0.00001,0.0001,0.001, 0.01, 0.1, 1.0,10.0,20,50,100,150,200,500]}\n#params = {'alpha': [0.001, 0.0001, 0.0005, 0.005,0.003 ]}\nestimator = Lasso()\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = estimator, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error',  \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nmodel_cv.fit(X_train, y_train) \ndisplay(model_cv.best_params_)\nalpha = model_cv.best_params_['alpha']\nlasso = Lasso(alpha=alpha)\nlasso.fit(X_train, y_train) \nlm_lasso_metric=prediction_matrix(lasso,X_train,X_validation,y_train,y_validation)","211b5606":"params = {\n    'criterion':['mse'],\n    'splitter':['best'],\n    'max_depth':[5,10,15,20,40,50],\n    'min_samples_split':[2,5,10,20,50,100],\n    'min_samples_leaf':[1,2,3,5,10,20],\n    'random_state':[42,100],\n}\nestimator = tree.DecisionTreeRegressor()\n# cross validation\nfolds = 5\nmodel_cv = GridSearchCV(estimator = estimator, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error',  \n                        cv = folds, \n                        return_train_score=False,\n                        verbose = 1)  \nmodel_cv.fit(X_train, y_train)\ndisplay(model_cv.best_estimator_)\nlm_dt=model_cv.best_estimator_\nlm_dt.fit(X_train, y_train) \nlm_dt_metric=prediction_matrix(lm_dt,X_train,X_validation,y_train,y_validation)","5ea423a5":"params = {\n    'criterion':['mse'],\n    'max_depth':[5,10,15,20],\n    'min_samples_split':[5,10],\n    'min_samples_leaf':[3,5,10],\n  #  'random_state':[42,100],\n    'max_features': [50,75,140],\n    'n_estimators':[30,50,100],\n    'n_jobs':[-1],\n    'oob_score':[True]\n    \n}\nestimator = RandomForestRegressor()\n# cross validation\nfolds = 3\nmodel_cv = GridSearchCV(estimator = estimator, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error',  \n                        cv = folds, \n                        return_train_score=False,\n                        verbose = 1)  \nmodel_cv.fit(X_train, y_train) \ndisplay(model_cv.best_estimator_)\nlm_rfc=model_cv.best_estimator_\nlm_rfc.fit(X_train, y_train) \nlm_rfc_metric=prediction_matrix(lm_rfc,X_train,X_validation,y_train,y_validation)","25da01fa":"lm_knn=KNeighborsRegressor(n_neighbors=5)\nlm_knn.fit(X_train, y_train) \nlm_knn_metric=prediction_matrix(lm_knn,X_train,X_validation,y_train,y_validation)","ecd79538":"lm_gbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.01, max_depth=1, random_state=31)\nlm_gbr.fit(X_train, y_train) \nlm_gbr_metric=prediction_matrix(lm_gbr,X_train,X_validation,y_train,y_validation)","5926586d":"params = dict(\n    max_depth=[3,9] ,          # maximum depth of each tree - try 2 to 10\n    learning_rate=[0.001,0.01] ,   # effect of each tree - try 0.0001 to 0.1\n    n_estimators=[1000,5000] ,    # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=[1,5] ,   # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=[0.7],  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=[0.7],         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=[0,5,10],         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=[1,5,8]  ,      # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=[1],   # set > 1 for boosted random forests\n)\nestimator = XGBRegressor()\n# cross validation\nfolds = 3\nmodel_cv = GridSearchCV(estimator = estimator, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error',  \n                        cv = folds, \n                        return_train_score=False,\n                        verbose = 1)  \nmodel_cv.fit(X_train, y_train) \ndisplay(model_cv.best_estimator_)\nlm_xgb = model_cv.best_estimator_\nlm_xgb.fit(X_train, y_train) \nlm_xgb_metric=prediction_matrix(lm_xgb,X_train,X_validation,y_train,y_validation)","6e268f9f":"'''\nxgb_params = dict(\n    max_depth=6,           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,   # set > 1 for boosted random forests\n)\n\nlm_xgb = XGBRegressor(**xgb_params)\nlm_xgb.fit(X_train, y_train) \nlm_xgb_metric=prediction_matrix(lm_xgb,X_train,X_validation,y_train,y_validation)\n'''","07b0c34e":"estimators = [\n    ('lr', lm),\n    (\"lasso\",lasso),\n    ('ridge',ridge),\n    ('dt',lm_dt),\n    ('rfc',lm_rfc),\n    ('gbr',lm_gbr),\n    ('xgb',lm_xgb)\n]\nlm_stack = StackingRegressor(estimators=estimators,final_estimator=LinearRegression())\nlm_stack.fit(X_train, y_train)\nlm_stack_metric=prediction_matrix(lm_stack,X_train,X_validation,y_train,y_validation)","4874b6b6":"# Creating a table which contain all the metrics\nlr_table = {'Metric': ['R2 Score (Train)','R2 Score (Test)','Adjusted R2 Score (Train)','Adjusted R2 Score (Test)','RSS (Train)','RSS (Test)',\n                       'RMSE (Train)','RMSE (Test)'], \n        'Linear Regression': lm_metric\n        }\n\nmetric_lm = pd.DataFrame(lr_table ,columns = ['Metric', 'Linear Regression'] )\nmetric_lm_rfe = pd.Series(lm_rfe_metric,name = 'RFE Linear Regression')\nmetric_lm_ridge = pd.Series(lm_ridge_metric, name = 'Ridge Regression')\nmetric_lm_lasso = pd.Series(lm_lasso_metric, name = 'Lasso Regression')\nmetric_lm_dt = pd.Series(lm_dt_metric,name = 'Decision Tree Regression')\nmetric_lm_rfc = pd.Series(lm_rfc_metric, name = 'Random Forest Regression')\nmetric_lm_knn= pd.Series(lm_knn_metric, name = 'KNN Regression')\nmetric_lm_gbr = pd.Series(lm_gbr_metric, name = 'Gradient Boosting Regression')\nmetric_lm_xgb = pd.Series(lm_xgb_metric, name = 'XGB Regression')\nmetric_lm_stack = pd.Series(lm_stack_metric, name = 'Stacked Regression')\nfinal_metric = pd.concat([metric_lm,metric_lm_rfe,metric_lm_ridge,metric_lm_lasso,metric_lm_dt,metric_lm_rfc,metric_lm_knn,metric_lm_gbr,metric_lm_xgb,metric_lm_stack], axis = 1)\nprint(\"Model Peformance Metric:\")\ndisplay(final_metric)","047da580":"feature_importance(lm.coef_,X_train,'Linear')\nfeature_importance(lm_rfe.coef_,X_train_rfe,'RFE Linear')\nfeature_importance(ridge.coef_,X_train_rfe,'Ridge')\nfeature_importance(lasso.coef_,X_train,'Lasso')\nfeature_importance(lm_dt.feature_importances_,X_train,'Decision Tree')\nfeature_importance(lm_rfc.feature_importances_,X_train,'Random Forest')\nfeature_importance(lm_gbr.feature_importances_,X_train,'Gradient Boosting')\nfeature_importance(lm_xgb.feature_importances_,X_train,'Xtreme Gradient Boosting')","5e944734":"temp=X_test\ntemp=temp.reset_index()\noutput_df=temp['Id']\nmodels=dict([('LM',lm),('Lasso',lasso),('DecisionTree',lm_dt),('RandomForest',lm_rfc),\n            ('KNN',lm_knn),('GradientBoosting',lm_gbr),('XGBoost',lm_xgb),('StackedReg',lm_stack)])\nfor i,m in models.items():\n    filename=str(i)+'_submission_file.csv'\n    lm_price=m.predict(X_test)\n    mp = pd.Series(lm_price, name = 'SalePrice')\n    final_metric = pd.concat([output_df,mp], axis = 1)\n    final_metric['SalePrice']=final_metric['SalePrice'].apply(lambda x: np.exp(x))\n    final_metric.to_csv(filename,index =False)","9e9b8a44":"**Analysis** - Lots of categorical columns have data skewed to one\/two category like:\n'MSZoning','Street','Alley','LandContour','Utilities','LandSlope',\n'Condition1','Condition2','BldgType','RoofStyle','RoofMatl','ExterCond','BsmtCond',\n'BsmtFinType2','Heating','CentralAir','Electrical','Functional',\n'GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature','SaleType','SaleCondition'\nand needs to binned properly","4c35ad9b":"#### x. XGBRegressor","5334e3ed":"## Step 5: Model Building & Evaluation","05a345d2":"#### iii. RFE Implementation for feature selection","1ce70ef7":"## 1.a Load Data and Perform Data Cleaning ","83b5f8e6":"#### Populating categorical values","75f2b1ca":"## Step 6: Feature Importance","b4fba5cf":"## House Price Prediction Assignment\n\n### Problem Statement:\n\nA US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. \n\nWe are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\n\n### Business Goals:\n\nThe company wants to know:\n1. Which variables are significant in predicting the price of a house, and\n2. How well those variables describe the price of a house.\n\nAlso, determine the optimal value of lambda for ridge and lasso regression.\n\n\n### Steps:\n\n#### 1. Data Sourcing\n\n    1. Checking the encoding of the file\n    2. Loading the data\t\t\n\n#### 2. Data Exploring & Cleaning\n\n    A. Null Values Analysis\n        1. Identify and drop columns with 100% missing data\n        2. Identify and drop columns with more than 80% missing data\n        3. Identify and drop columns having single unique values as they will not any value to the analysis\n        4. Identify and drop unnecessary columns (like text based, Applicant Loan Behaviour)\n\n    B. Datatype Check \n\n    C. Datatype Conversion\n        1. Converting int to object\n    \n    D. Drop Records\n        1. Drop Duplicates\n    \n    E. Impute Null Values\n    \n    F. Populating the categorical columns with correct mapping\n    \n    G. Outliers handling\n    \n    G. Derived Metrics\n    \n#### 3. Data Visualisation\n#### 4. Data Preparation\n#### 5. Splitting and Scaling the data\n#### 6. Model Building & Evaluation\n#### 7. Regualisation using Ridge and Lasso\n#### 8. Making Predictions Using the Final Model on the test data\n######################################################################","7e0ed8fc":"#### ii. Linear Regression","acf10e37":"#### i. Baseline Score","22afc11b":"### 1.c Outlier Treatment","ec4dc029":"### 1.d Feature Engineering","e015d0a4":"#### viii. KNN Regression","74e9b43e":"### 1.b Imputation\n#### Imputing values","307c3825":"#### xi. Stacked Regression","e75805de":"#### vii. Random Forest","26165b7d":"#### v. Lasso Regression","cb1da5a7":"#### iv. Ridge Regression","17ceec3c":"## 3. Data Prep","dc8aea14":"## 2. Data Visualisation:","8bcf78b8":"#### Dividing into X and Y sets for the model building","83a16199":"## Step 6: Making Predictions Using the Final Model on the test data","da96cf3b":"#### ix. Gradient Boosting","b41591fd":"## Step 4: Splitting and Scaling the data","f3f99a6b":"#### vi. Decision Tree"}}