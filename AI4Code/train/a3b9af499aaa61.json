{"cell_type":{"c3fc0215":"code","91747779":"code","61583f38":"code","d4102bea":"code","4846f5a7":"code","a6fedeaa":"code","0839f9be":"code","e0351bee":"code","dc01a851":"code","5928c423":"code","571819d3":"code","256d3ae5":"code","61c7bbde":"code","1fba409b":"code","541e2b95":"code","9bb6419d":"code","68eddbd3":"code","e3411f10":"code","67badc24":"code","336e31ef":"code","99babf9c":"code","2de159e8":"markdown","913ee8a1":"markdown","b14e371a":"markdown","1e151dae":"markdown","a20070ee":"markdown","6f3210b9":"markdown","d52b9819":"markdown","fe33a9b1":"markdown","0fc59e99":"markdown","8f1ee147":"markdown","14b2ca16":"markdown","3627dea2":"markdown","8739f636":"markdown","81766d5f":"markdown","35877b62":"markdown","0eb6ca5e":"markdown","56f0402c":"markdown","8b6374ff":"markdown","9fb38c8f":"markdown","11a66cc5":"markdown","64a74714":"markdown","0cd7f5cc":"markdown","ced311e5":"markdown","bec58835":"markdown","920f4a8b":"markdown","be2d9662":"markdown"},"source":{"c3fc0215":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot  as plt\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import BernoulliNB,GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n","91747779":"# ## Read Dataset\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","61583f38":"train_df.info()\nprint(\"----------------------------\")\ntest_df.info()","d4102bea":"fig, (first, second, third) = plt.subplots(1, 3, figsize=(15, 5))\nsns.countplot(x='Embarked', data=train_df, ax=first)\nsns.countplot(x='Survived', hue=\"Embarked\", data=train_df, order=[1, 0], ax=second)\nmean_embarked = train_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],                                                                           as_index=False).mean()\nsns.barplot(x='Embarked', y='Survived', data=mean_embarked, order=['S', 'C', 'Q'], ax=third)\n","4846f5a7":"train_df['Embarked']= train_df['Embarked'].fillna('S')\nembarked_one_hot = pd.get_dummies(train_df['Embarked'], prefix='Port of Embarkation')\ntrain_df = pd.concat([train_df, embarked_one_hot], axis=1)\nembarked_one_hot = pd.get_dummies(test_df['Embarked'], prefix='Port of Embarkation')\ntest_df = pd.concat([test_df, embarked_one_hot], axis=1)\ntrain_df = train_df.drop( \"Embarked\", axis=1)\ntest_df = test_df.drop(\"Embarked\", axis=1)","a6fedeaa":"train_corr = train_df.corr(method='pearson')\nplt.figure(figsize=(12, 9))\nsns.set(font_scale=0.9)\nsns.heatmap(train_corr,\n            annot=True,\n            linecolor='white',\n            linewidths=0.01,\n            cmap='magma')","0839f9be":"train_df['FamilySize'] = train_df['SibSp'] + train_df['Parch'] + 1\ntest_df['FamilySize'] = test_df['SibSp'] + test_df['Parch'] + 1\ntrain_df = train_df.drop( [\"SibSp\",\"Parch\"], axis=1)\ntest_df = test_df.drop([\"SibSp\",\"Parch\"], axis=1)","e0351bee":"_, (first, second, third) = plt.subplots(1, 3, figsize=(15, 5))\nsns.countplot(x='Sex', data=train_df, ax=first)\nsns.countplot(x='Survived', hue=\"Sex\", data=train_df, order=[1, 0], ax=second)\nmean_embarked = train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean()\nsns.barplot(x='Sex', y='Survived', data=mean_embarked, order=['male', 'female'], ax=third)\nsex_one_hot = pd.get_dummies(train_df['Sex'], prefix='Gender')\ntrain_df = pd.concat([train_df, sex_one_hot], axis=1)\nsex_one_hot = pd.get_dummies(test_df['Sex'], prefix='Gender')\ntest_df = pd.concat([test_df, sex_one_hot], axis=1)\ntrain_df = train_df.drop( \"Sex\", axis=1)\ntest_df = test_df.drop(\"Sex\", axis=1)","dc01a851":"train_age_mean = train_df['Age'].mean()\ntrain_df['Age'] = train_df['Age'].fillna(train_age_mean)\ntest_age_mean = test_df['Age'].mean()\ntest_df['Age'] = test_df['Age'].fillna(test_age_mean)","5928c423":"PClass_one_hot = pd.get_dummies(train_df['Pclass'], prefix='Pclass')\ntrain_df = pd.concat([train_df, PClass_one_hot], axis=1)\nPClass_one_hot = pd.get_dummies(test_df['Pclass'], prefix='Pclass')\ntest_df = pd.concat([test_df, PClass_one_hot], axis=1)\ntrain_df = train_df.drop( \"Pclass\", axis=1)\ntest_df = test_df.drop(\"Pclass\", axis=1)","571819d3":"train_age_mean = train_df['Fare'].mean()\ntrain_df['Fare'] = train_df['Fare'].fillna(train_age_mean)\ntest_age_mean = test_df['Fare'].mean()\ntest_df['Fare'] = test_df['Fare'].fillna(test_age_mean)","256d3ae5":"train_corr = train_df.corr(method='pearson')\nplt.figure(figsize=(12, 9))\nsns.set(font_scale=0.9)\nsns.heatmap(train_corr,\n            annot=True,\n            linecolor='white',\n            linewidths=0.01,\n            cmap='magma')","61c7bbde":"train_df = train_df.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\ntest_df = test_df.drop([\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)","1fba409b":"train_df_norm = (train_df - train_df.min()) \/ (train_df.max() - train_df.min())\ntrain_df_norm.info()\nprint(\"*********************\")\ntest_df_norm = (test_df - test_df.min()) \/ (test_df.max() - test_df.min())\ntest_df_norm.info()","541e2b95":"y_train_1 = train_df_norm['Survived'].values\ntrain_df_norm_t = train_df_norm.drop('Survived', axis=1)\nx_train_1 = train_df_norm_t.iloc[:, 1:].values\nx_test = test_df_norm.iloc[:, 1:].values\nx_train, x_valid, y_train, y_valid = train_test_split(x_train_1, y_train_1, test_size=0.1, random_state=1)","9bb6419d":"models = [('bernoulli_nb', BernoulliNB()),\n          ('log_reg', LogisticRegression(max_iter = 10000)),\n          ('linear_svc', LinearSVC(max_iter = 10000)),\n          ('knn', KNeighborsClassifier(n_neighbors=100)),\n          ('rf', RandomForestClassifier(n_estimators=100))]","68eddbd3":"for m_name, model in models:\n    pipeline = Pipeline([('scaler', StandardScaler()),(m_name,model)])\n    pipeline.fit(x_train,y_train)\n    test_y = pipeline.predict(x_valid)\n    print(\"model = \",model,\"\\n\")\n    print(classification_report(y_valid,test_y,digits=3))","e3411f10":"\n\nmodel = RandomForestClassifier()\npipeline = Pipeline([('scaler', StandardScaler()), ('clf', model)])\nparam_grid = { \n    'clf__n_estimators': [200, 500],\n    'clf__max_features': ['auto', 'sqrt', 'log2'],\n    'clf__max_depth' : [4,5,6,7,8],\n    'clf__criterion' :['gini', 'entropy']\n}\n\ngrid_search = GridSearchCV(pipeline,param_grid,verbose=1)\ngrid_search.fit(x_train_1, y_train_1)\nprint('LR CVScore ', grid_search.best_score_)\nprint('LR C', grid_search.best_params_.get('clf__C'))\nprint('LR Max Iterations', grid_search.best_params_.get('clf__max_iter'))\n\n","67badc24":"test_y = grid_search.best_estimator_.predict(x_test)\ndf = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')","336e31ef":"print(classification_report(df['Survived'].values,test_y,digits=3))","99babf9c":"df['Survived'] = test_y.astype(int)\ndf.to_csv('submission.csv',index = False)","2de159e8":"### Drop unnecessary feature and Normalize Features","913ee8a1":"#### Analyze of the train and test dataset: \n1. ##### In the training dataset, I have 891 different person; however, some of them contains NAN value: \n    1. ##### In the Age Feature: 891 - 714 = 177.\n    2. ##### In the Cabin Feature: 891 - 204 = 687.\n    3. ##### In the Embarked Feature: 891 - 889 = 2.\n2. ##### In the test dataset, I have 418 different person; however, some of them contains NAN value: \n    1. ##### In the Age Feature: 418 - 332 = 86.\n    2. ##### In the Cabin Feature: 418 - 91 = 327.\n    3. ##### In the Fare Feature: 418 - 417 = 1.\n    \n3. ##### Name, Sex, Ticket, Cabin, and Embarked features are string; therefore, I should apply one hot representation. ","b14e371a":"## Read Dataset","1e151dae":"## Training","a20070ee":"## Libraries","6f3210b9":"> #### As can be seen in the correlation matrix, correlation of PassengerId and Survived are too low; therefore we can drop it. \n> #### We can should Name and Ticket features since these are string and they cannot represented with integer value. \n> #### Also, we cannot use Cabin features since their most of the values are NAN-Null in the training and test dataset. ","d52b9819":"> ### As can be seen that siblings\/spouse and parent\/child features are highly correlated","fe33a9b1":"### Port of Embarkation Analayze and One-hot-representation","0fc59e99":"1. #### I check whether train dataset contains null value or not.\n2. #### Port of embarkation is not numeric therefore I apply one-hot representation and I visualize it.\n3. #### I used Pearson's Correlation Matrix to understand correlation of siblings\/supouse & parent\/child then I gather them one feature called family size.\n4. #### I visualize gender feature and I represent gender with one-hot representation.\n5. #### I visualize fare feature to understand the interval of its values.\n6. #### As I understand, the features\u2019 value should be normalized and I normalize, namely rescale,them. I used that formulation to rescale them in interval 1 to 0 :\n![](https:\/\/miro.medium.com\/max\/4800\/0*8btMQlMD6O50pUDP)\n","8f1ee147":"### Models","14b2ca16":"### Fill the NAN varilable in the Fare","3627dea2":"### Feature normalization is needed for comparing feature importance since the data may have varying range and this circumstance has impact on feature importance. \n### In the formulation of the logistic regression, all features are multiply by their weights (xi * wi) and If the values of the features is not scaled in thesame interval, this affects feature importance.\n### Therefore, I normalize and prepare training data with following methods:","8739f636":"### Pearson's Correlation Matrix \n> ##### We should look at Pearson's Correlation with our new Features to understand the correlation of other features and Survived. ","81766d5f":"> ### All features are now represented by numerical value; however, It has variying ranges.Therefore, we should normalize the data. Also, we should drop feature which is used to obtain numerical data etc.","35877b62":"## Split Data","0eb6ca5e":"### Pearson's Correlation Matrix and siblings\/supouse & parent\/child","56f0402c":"## Data Analyze","8b6374ff":"> #### Fill the NAN variables with 'S' since the number of 'S' in the dataset are higher than others. \n> #### Then, apply one - hot representation.","9fb38c8f":"## Feature Normalization","11a66cc5":"### Normalization and Rescaling","64a74714":"### Training Results","0cd7f5cc":"### Fill the NAN variable in Age Feature","ced311e5":"### Hyperparameters","bec58835":"### One - Hot representation of PClass","920f4a8b":"### One-Hot Representation of Gender","be2d9662":"> ### RandomForestClassifier model performs more accurately. Therefore, I choose RandomForestClassifier as a model. \n> ### However, we did not make hyperparameter tuning yet."}}