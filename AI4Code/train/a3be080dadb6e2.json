{"cell_type":{"2b875e7a":"code","31efb75f":"code","46e7bc28":"code","b5e171be":"code","8b8f1ae6":"code","36eb217d":"code","b6b73af3":"code","2b94731e":"code","7c6530bd":"code","92ee164f":"code","68e7458c":"code","a11bce35":"code","3d9f9c30":"code","28d7cf3d":"code","7e2deaa6":"code","a261fc8b":"code","ea98606c":"code","2248e769":"code","9ae90ed8":"code","1dec0dd0":"code","45abe949":"code","cf010c7b":"code","771b541f":"code","67dafb7e":"code","bb70eb60":"code","90be5fa2":"code","4bdc780b":"code","43082048":"code","301a9a47":"code","0174a091":"code","1ce012ae":"code","6b95facc":"code","d1a796e8":"code","698eab99":"code","c26e7529":"code","c2dcc442":"code","5ae960b9":"code","184f56ac":"code","ad5979f5":"markdown","fcf11c18":"markdown","dfd47582":"markdown","5d63fcd5":"markdown","1ff490d9":"markdown","5fecd795":"markdown","5a83cbd8":"markdown","d4ca7072":"markdown","cfaa200e":"markdown","f9e0a8b9":"markdown","dcc19f1f":"markdown","7642e6ea":"markdown","c828349d":"markdown","85daed60":"markdown","568bc70e":"markdown","dc9c5a86":"markdown","687a8e7d":"markdown","fdb6eb6a":"markdown","cbc02bc3":"markdown","a7483f28":"markdown","38f3e5d0":"markdown","0b900a9e":"markdown","506c06ec":"markdown"},"source":{"2b875e7a":"!pip install git+https:\/\/github.com\/fastai\/fastai_dev ","31efb75f":"from fastai2.torch_basics import *\nfrom fastai2.test import *\nfrom fastai2.data.all import *\nfrom fastai2.vision.core import *\nfrom fastai2.notebook.showdoc import *","46e7bc28":"source = Path('..\/input\/the-oxfordiiit-pet-dataset\/images')","b5e171be":"items = get_image_files(source)\nsplit_idx = RandomSplitter()(items)","8b8f1ae6":"def resized_image(fn:Path, sz=128):\n    x = PILImage.create(fn).convert('RGB').resize((sz,sz))\n    return tensor(array(x)).permute(2,0,1).float()\/255.","36eb217d":"class TitledImage(Tuple):\n    def show(self, ctx=None, **kwargs): show_titled_image(self, ctx=ctx, **kwargs)","b6b73af3":"img = resized_image(items[0])","2b94731e":"TitledImage(img,'test').show()","7c6530bd":"class PetTfm(Transform):\n    def __init__(self, items, train_idx):\n        self.items,self.train_idx = items,train_idx\n        self.labeller = RegexLabeller(pat = r'\/([^\/]+)_\\d+.jpg$')\n        vals = map(self.labeller, items[train_idx])\n        self.vocab,self.o2i = uniqueify(vals, sort=True, bidir=True)\n\n    def encodes(self, i):\n        o = self.items[i]\n        return resized_image(o), self.o2i[self.labeller(o)]\n    \n    def decodes(self, x): return TitledImage(x[0],self.vocab[x[1]])","92ee164f":"pets = PetTfm(items, split_idx[0])","68e7458c":"x,y = pets(0)\nx.shape,y","a11bce35":"dec = pets.decode((x,y))\ndec.show()","3d9f9c30":"class SiameseImage(Tuple):\n    def show(self, ctx=None, **kwargs): \n        img1,img2,same_breed = self\n        return show_image(torch.cat([img1,img2], dim=2), title=same_breed, ctx=ctx)","28d7cf3d":"SiameseImage(img,img,True).show();","7e2deaa6":"class SiamesePair(Transform):\n    def __init__(self,items,labels):\n        self.items,self.labels,self.assoc = items,labels,self\n        sortlbl = sorted(enumerate(labels), key=itemgetter(1))\n        # dict of (each unique label) -- (list of indices with that label)\n        self.clsmap = {k:L(v).itemgot(0) for k,v in itertools.groupby(sortlbl, key=itemgetter(1))}\n        self.idxs = range_of(self.items)\n        \n    def encodes(self,i):\n        \"x: tuple of `i`th image and a random image from same or different class; y: True if same class\"\n        othercls = self.clsmap[self.labels[i]] if random.random()>0.5 else self.idxs\n        otherit = random.choice(othercls)\n        return SiameseImage(self.items[i], self.items[otherit], self.labels[otherit]==self.labels[i])","a261fc8b":"OpenAndResize = TupleTransform(resized_image)\nlabeller = RegexLabeller(pat = r'\/([^\/]+)_\\d+.jpg$')\nsp = SiamesePair(items, items.map(labeller))\npipe = Pipeline([sp, OpenAndResize], as_item=True)\nx,y,z = t = pipe(0)\nx.shape,y.shape,z","ea98606c":"for _ in range(3): pipe.show(pipe(0))","2248e769":"class ImageResizer(Transform):\n    order=10\n    \"Resize image to `size` using `resample\"\n    def __init__(self, size, resample=Image.BILINEAR):\n        if not is_listy(size): size=(size,size)\n        self.size,self.resample = (size[1],size[0]),resample\n\n    def encodes(self, o:PILImage): return o.resize(size=self.size, resample=self.resample)\n    def encodes(self, o:PILMask):  return o.resize(size=self.size, resample=Image.NEAREST)","9ae90ed8":"tfms = [[PILImage.create, ImageResizer(128), ToTensor(), ByteToFloatTensor()],\n        [labeller, Categorize()]]\ndsrc = DataSource(items, tfms)","1dec0dd0":"t = dsrc[0]\ntype(t[0]),type(t[1])","45abe949":"x,y = dsrc.decode(t)\nx.shape,y","cf010c7b":"dsrc.show(t);","771b541f":"tfms = [[PILImage.create], [labeller, Categorize()]]\ndsrc = DataSource(items, tfms)\ntdl = TfmdDL(dsrc, bs=1, after_item=[ImageResizer(128), ToTensor(), ByteToFloatTensor()])","67dafb7e":"t = tdl.one_batch()\nx,y = tdl.decode_batch(t)[0]\nx.shape,y","bb70eb60":"dsrc.show((x,y));","90be5fa2":"pets = DataSource(items, tfms, splits=split_idx)","4bdc780b":"x,y = pets.subset(1)[0]\nx.shape,y","43082048":"x2,y2 = pets.valid[0]\ntest_eq(x.shape,x2.shape)\ntest_eq(y,y2)","301a9a47":"xy = pets.valid.decode((x,y))\nxy[1]","0174a091":"xy2 = decode_at(pets.valid, 0)\ntest_eq(type(xy2[1]), Category)\ntest_eq(xy2, xy)","1ce012ae":"pets.show((x,y));","6b95facc":"ds_img_tfms = [ImageResizer(128), ToTensor()]\ndl_tfms = [Cuda(), ByteToFloatTensor()]\n\ntrn_dl = TfmdDL(pets.train, bs=9, after_item=ds_img_tfms, after_batch=dl_tfms)\nb = trn_dl.one_batch()\n\ntest_eq(len(b[0]), 9)\ntest_eq(b[0][0].shape, (3,128,128))\ntest_eq(b[0].type(), 'torch.cuda.FloatTensor' if default_device().type=='cuda' else 'torch.FloatTensor')","d1a796e8":"bd = trn_dl.decode_batch(b)\n\ntest_eq(len(bd), 9)\ntest_eq(bd[0][0].shape, (3,128,128))","698eab99":"_,axs = plt.subplots(3,3, figsize=(9,9))\ntrn_dl.show_batch(ctxs=axs.flatten())","c26e7529":"dbch = pets.databunch(bs=9, after_item=ds_img_tfms, after_batch=dl_tfms)\ndbch.train_dl.show_batch()","c2dcc442":"cv_source = Path('..\/input\/camvid\/camvid\/CamVid')\ncv_items = get_image_files(cv_source\/'train')\ncv_splitter = RandomSplitter(seed=42)\ncv_split = cv_splitter(cv_items)\ncv_label = lambda o: cv_source\/'train_labels'\/f'{o.stem}_L{o.suffix}'","5ae960b9":"tfms = [[PILImage.create], [cv_label, PILMask.create]]\ncamvid = DataSource(cv_items, tfms, splits=cv_split)\ntrn_dl = TfmdDL(camvid.train,  bs=4, after_item=ds_img_tfms, after_batch=dl_tfms)","184f56ac":"_,axs = plt.subplots(2,2, figsize=(6,6))\ntrn_dl.show_batch(ctxs=axs.flatten())","ad5979f5":"Once released, the module will be `fastai`, but for now it's called `fastai2`.","fcf11c18":"...or equivalently:","dfd47582":"We create a `SiamesePair` transform that creates the tuple we'll need for a `SiameseImage`.","5d63fcd5":"We can use this to feed a `DataLoader` and view a batch. It's faster to convert to float on GPU, so we'll do it as a DataLoader transform, after CUDA in `after_batch`.","1ff490d9":"Note that `decodes` is intended to operate on the return value of `encodes`. In the case above, `decodes` takes in a tuple consisting of a Tensor representing the image and an integer being the class idx and returns an instance of `TitledImage`.\n\nIt's important to give the type that can show itself to fully decoded elements because when in a `Pipeline`, we stop decoding as soon as we can find a `show` method.","5fecd795":"## Loading the Pets dataset using only `Transform`\n\nLet's see how to use `fastai.data` to process the Pets dataset. We use *source* to refer to the underlying source of our data (e.g. a directory on disk, a database connection, a network connection, etc).","5a83cbd8":"We can now create a `Transform` that converts from an index to our `x` and `y` for modeling.","d4ca7072":"By using the same transforms in `after_item` and `after_batch` but a different kind of targets (here segmentation masks), the targets are automatically processed as they should with the type-dispatch system.","cfaa200e":"## Using `DataSource`\n\n`DataSource` applies a list of list of transforms (or list of `Pipeline`s) lazily to items of a collection, creating one output per list of transforms\/`Pipeline`. This makes it easier for us to separate out steps of a process, so that we can re-use them and modify the process more easily. For instance, we could add data augmentation, data normalization, etc. Here we separate out the steps of the basic pets process.","f9e0a8b9":"## Segmentation","dcc19f1f":"## Using `Pipeline` to create Siamese model dataset\n\nA *Siamese* model needs a dataset with two images as input, and a boolean output that is `True` if the two images are the \"same\" (e.g. are pictures of the same breed of pet). Custom structures like this are often easiest to create using your own `Pipeline`, which makes no assumptions about the structure of your input or output data.\n\nWe'll be creating a dataset that returns two images and a boolean. So let's first define a showable type for a tuple with those things:","7642e6ea":"First we grab the items and split indices:","c828349d":"The types being properly propagated and dispatched, we can do the same thing with `ImageResizer`, `ImageToByteTensor`, `ByteToFloatTensor` being passed as tranforms over the tuple. This is done in `TfmdDL` by passing them to `after_item`. They won't do anything to the category but will only be applied to the inputs.","85daed60":"Then the `Pipeline` will compose our two transforms, and create the `SiameseImage`.","568bc70e":"When you create a `Datasource`, you can pass along `splits` indices that represent the split between train and validation set (there can be multiple validation sets) on top of the items and tfms.","dc9c5a86":"## Adding splits","687a8e7d":"Before we can create a `Transform`, we need a type that knows how to show itself. Here we define a `TitledImage`:","fdb6eb6a":"# Tutorial: fastai v2 low level data APIs\n\n> Using `DataSource`, `Pipeline`, `TfmdList` and `Transform`\n\nNote that this tutorial is for advanced features of fastai v2 for working with data formats that are not supported directly by fastai applications. However most common applications are supported by the Data Blocks API, which is more suitable for beginners and for rapid development.\n\n**NB: fastai v2 is not officially released at this point. This is using the pre-release**. We have to install directly from GitHub since it isn't yet released on PyPi or any conda channel.","cbc02bc3":"Then we want to open and resize the image filenames but not the boolean. This kind of filtering is done with type annotations. We indicated with the return type annotation of `SiamesePair.encodes` the types of our objects and we can then create a new transform that opens and resizes `Path` objects but leaves other types untouched. ","a7483f28":"We're now ready to show our items.","38f3e5d0":"We can decode an element for display purposes, either passing a tuple to `decode` or by passing an index to `decode_at`.","0b900a9e":"We'll use this function to create consistently sized tensors from image files:","506c06ec":"To access an element we need to specify the subset (either with `train`\/`valid` or with `subset(i)`):"}}