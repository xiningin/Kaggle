{"cell_type":{"5c028988":"code","8eee3f50":"code","254075af":"code","53bddc21":"code","a0665853":"code","02a8ec14":"code","c4bf1852":"code","21a226c8":"code","ab94a19d":"code","0ff2fa5d":"code","fd7be691":"code","dcccc381":"code","bad7eb37":"code","1a41b496":"code","b111773c":"code","2b54cf1a":"code","91a09f73":"code","e3eace3f":"code","5a769d72":"code","59355f30":"code","ccc1e0f9":"code","95059747":"markdown","5be1ae8d":"markdown","3d6dd7bf":"markdown","d160c4c4":"markdown"},"source":{"5c028988":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8eee3f50":"# For visualization\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom wordcloud import WordCloud\n# For preprocessing \nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\nfrom string import punctuation \nfrom nltk.stem import LancasterStemmer, SnowballStemmer, PorterStemmer,WordNetLemmatizer\n# For preparing the data \nfrom sklearn.model_selection import train_test_split\n# For model build \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n# For model evaluation \nfrom sklearn.metrics import classification_report,f1_score","254075af":"FILEPATH = '..\/input\/sms-spam-collection-dataset\/spam.csv'","53bddc21":"data = pd.read_csv(FILEPATH,encoding='ISO-8859-1')\ndata.head()","a0665853":"data.isnull().sum()[data.isnull().sum()>0]","02a8ec14":"# Since only the unnamed columns have the null values, hence we can safely drop the columns \ndata.dropna(axis=1, inplace=True)\n# Reverse the columns for content to be first and then labels\ndata = data.iloc[:,::-1]\n# Name the columns appropriately\ndata.columns = ['Content','Label']\ndata.head()","c4bf1852":"edaData = data.copy()\nedaData['NumWords'] = edaData['Content'].apply(lambda x: len(x))","21a226c8":"fig, ax = plt.subplots(figsize=(16,6))\nsns.histplot(data=edaData, x='NumWords',hue='Label',kde=True,ax=ax)\nax.set_xlabel('Number of Words in the email')\nax.set_ylabel('Number of emails')\nax.set_title('Studying the distribution of number of words in spam and ham emails')\nplt.show()","ab94a19d":"fig, ax = plt.subplots(figsize=(8,8))\nedaData['Label'].value_counts().plot(\n    kind='pie',\n    title='Proportion of spam and ham emails in the dataset',\n    autopct = '%1.0f%%',\n    ax=ax\n)\nplt.show()","0ff2fa5d":"spamData = edaData[edaData['Label'] == 'spam']\nhamData = edaData[edaData['Label'] == 'ham']\nspamWords = ' '.join([message['Content'] for _,message in spamData.iterrows()])\nhamWords = ' '.join([message['Content'] for _,message in hamData.iterrows()])\nspamCloud =WordCloud(collocations = False, background_color = 'white').generate(spamWords)\nhamCloud = WordCloud(collocations=False, background_color='white').generate(hamWords)\nfig,ax = plt.subplots(figsize=(16,8),nrows=1,ncols=2)\nfor axis in ax:\n    axis.axis('off')\nax[0].imshow(spamCloud)\nax[0].set_title('Most common words in Spam emails')\nax[1].imshow(hamCloud)\nax[1].set_title('Most common words in Ham emails')\nplt.show()","fd7be691":"class Preprocess:\n    def __init__(self,method='WordNetLemmatizer')->None:\n        \"\"\"\n        Preprocesses the inputs in your dataset to be suitable for machine learning models \n        Preprocessing involves the following steps:\n        1) Convert all words to lower case for uniformity (normalization)\n        2) Split each word in the sentence and remove stop words and punctuation marks (tokenization)\n        3) Stem each word (stemming)\n        4) Uses tf-idf vectorization technique to preserve the semantic meaning of the words in the sentence\n        5) Label encode the categories \n        \n        Attributes:\n        self.method(str) - The method by which you would want to stem\n        self.methods(list) - The possible methods which you can use for stemmming\n        self.stemmers(dict) - The appropriate nltk stemmers matching to the methods\n        self.stemmer(nltk.stem.stemmer) - The stemmer object which will be used to stem the words\n        self.stuff_to_be_removed(list) - The list of characters which should be removed \n        self.isFitted(bool) - Indicates if the label encoders and tf-idf vectorizers are fitted\n        \n        Params:\n        method(str) - the method by which you would like to stem your inputs\n        Returns:\n        None \n        \"\"\"\n        self.method = method \n        self.methods = ['LancesterStemmer','PorterStemmer','SnowballStemmer','WordNetLemmatizer']\n        if method not in self.methods:\n            raise ValueError(f'The method should be from the following methods {self.methods}')\n        self.stuff_to_be_removed = list(stopwords.words('english'))+list(punctuation)\n        self.stemmers = {\n            'PorterStemmer':PorterStemmer(),\n            'LancesterStemmer':LancasterStemmer(),\n            'SnowballStemmer':SnowballStemmer(language='english'),\n            'WordNetLemmatizer':WordNetLemmatizer()\n        }\n        self.stemmer = self.stemmers[self.method]\n        self.isFitted = False\n    def preprocess(self,message:str)->str:\n        \"\"\"\n        Stems and removes stopwords and punctuation from the given message \n        Params:\n        message(str) - The message which you want to preprocess\n        Returns:\n        str - The preprocessed message        \n        \"\"\"\n        # Convert message to lower case \n        message = message.lower()\n        # Remove all the links from the messages \n        message = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', message)\n        # Remove all the mentions\n        message =re.sub(\"(@[A-Za-z0-9_]+)\",\"\", message)\n        # Remove stopwords and perform stemming \n        if self.method == 'WordNetLemmatizer':\n            message = ' '.join([self.stemmer.lemmatize(word) for word in message.split() if word not in self.stuff_to_be_removed])\n        else:\n            message = ' '.join([self.stemmer.stem(word) for word in message.split() if word not in self.stuff_to_be_removed])\n        # Return the message \n        return message \n    def fit(self,X:pd.Series,y:pd.Series)->None:\n        \"\"\"\n        Fits the tf-idf vectorizer and label encoders according the training data \n        Params:\n        X(pd.Series) - The column of the dataframe containing the message contents\n        y(pd.Series) - The column of the dataframe containing the labels\n        Returns:\n        None\n        \"\"\"\n        # Preprocess the message first\n        X = X.apply(lambda x: self.preprocess(x))\n        # Initialize the label encoder and tfidf vectorizer \n        self.labelEncoder = LabelEncoder()\n        self.vectorizer = TfidfVectorizer()\n        # Fit the label encoder and vectorizer \n        self.vectorizer.fit(X)\n        self.labelEncoder.fit(y)\n        self.isFitted=True\n    def transform(self,X:pd.Series,y:pd.Series)->tuple:\n        \"\"\"\n        Transforms the given messages and labels to be suitable for machine learning models.\n        Removes stop words and punctuation from the message and then stems it and applies \n        tf-idf vectorization to the message.\n        Label encodes the message category\n        Params:\n        X(pd.Series or str) - The message or column of messages you want to transform\n        y(pd.Series or list or np.array) - The labels which you want to transform \n        Returns:\n        tuple containing the transformed messages and labels        \n        \"\"\"\n        # Check if it is fitted\n        if not self.isFitted:\n            raise NotImplementedError('Please use fit function first')\n        # Preprocess the messages first and apply tfidf vectorization\n        if isinstance(X,pd.Series):\n            X = X.apply(lambda x: self.preprocess(x))\n            vector = self.vectorizer.transform(X)\n        else:\n            X = self.preprocess(X)\n            vector = self.vectorizer.transform([X])\n        # convert tfidf sparse matrix to an array\n        vector = vector.toarray()\n        # Apply label encoding \n        if y is not None:\n            labels = self.labelEncoder.transform(y)\n            return vector,labels\n        else:\n            return vector\n    def fit_transform(self,X:pd.Series,y:pd.Series)->tuple:\n        \"\"\"\n        Fits and transforms the data to be suitable for machine learning models\n        Params:\n        X(pd.Series or str) - The message or column of messages you want to transform\n        y(pd.Series or list or np.array) - The labels which you want to transform \n        Returns:\n        tuple containing the transformed messages and labels   \n        \"\"\"\n        # Call the fit function first\n        self.fit(X,y)\n        # Call the transform function \n        vectors,labels = self.transform(X,y)\n        return vectors,labels    ","dcccc381":"preprocess = Preprocess()\nX = data['Content']\ny = data['Label']\n(X,y) = preprocess.fit_transform(X=X,y=y)","bad7eb37":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)","1a41b496":"def evaluate_model(modelName, model):\n    model.fit(X_train, y_train)\n    trainPreds = model.predict(X_train)\n    testPreds = model.predict(X_test)\n    trainReport = classification_report(y_true=y_train, y_pred=trainPreds)\n    testReport = classification_report(y_true=y_test, y_pred=testPreds)\n    f1Train = f1_score(y_true=y_train, y_pred=trainPreds)\n    f1Test = f1_score(y_true=y_test, y_pred=testPreds)\n    print('='*50, f'EVALUATING {modelName}', '='*50)\n    print('Classification report on train set is')\n    print(trainReport)\n    print('Classification report on test set is')\n    print(testReport)\n    return model,f1Train,f1Test","b111773c":"model_dict = {\n    'LogisticRegression':LogisticRegression(class_weight='balanced'),\n    'NaiveBayes':GaussianNB(),\n    'DecisionTree':DecisionTreeClassifier(class_weight='balanced'),\n    'RandomForest':RandomForestClassifier(class_weight='balanced',n_jobs=-1),\n    'XGBoost':XGBClassifier(\n    objective='binary:logistic',\n    n_jobs=-1,\n    eval_metric='logloss',\n    use_label_encoder=False)\n}","2b54cf1a":"fittedModelDict = {}\nmetricsDf = pd.DataFrame()\ntrainF1List = []\ntestF1List = []\nfor name,model in model_dict.items():\n    fittedModelDict[name],trainF1,testF1 = evaluate_model(modelName=name,model=model)\n    trainF1List.append(trainF1)\n    testF1List.append(testF1)\nmetricsDf['TrainF1'] = trainF1List\nmetricsDf['TestF1'] = testF1List\nmetricsDf.index = list(fittedModelDict.keys())","91a09f73":"fig,ax=plt.subplots(figsize=(16,6))\nmetricsDf.plot(kind='bar',ax=ax,title='Comparison between models on train and test F1 scores')\nplt.show()","e3eace3f":"xgbModel = fittedModelDict['XGBoost']","5a769d72":"def predictNewMessage(message):\n    # Preprocess the message \n    message = preprocess.preprocess(message)\n    # Transform the message into a tf-idf vector\n    to_predict = preprocess.transform(message,y=None)\n    # Predict using the best model\n    pred = xgbModel.predict(to_predict)\n    # Inverse transform the labels by using the label encoder \n    pred = preprocess.labelEncoder.inverse_transform(pred)[0]\n    # return the predictions\n    return pred","59355f30":"spamMessage = \"Amazon is sending you a refund of 32.64 rupees. Please reply with your bank account and routing number to recieve your refund\"\n\npred = predictNewMessage(spamMessage)\nprint(pred)","ccc1e0f9":"hamMessage = \"Planning to catch up at 10?\"\npred = predictNewMessage(hamMessage)\nprint(pred)","95059747":"Since we see that XGBoost model has the highest f1 score for both spam and ham emails on both train and test sets,\nwe can say that XGBoost is the best model which we can use for prediction","5be1ae8d":"# Data Preprocessing","3d6dd7bf":"# Exploratory Data Analysis","d160c4c4":"# Imports"}}