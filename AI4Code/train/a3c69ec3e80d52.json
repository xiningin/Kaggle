{"cell_type":{"2c03d045":"code","57a5079b":"code","97c29617":"code","27807c73":"code","7a6f8adb":"code","91ce2e7c":"code","789fdbf1":"code","349f12df":"code","447cc839":"code","c371f0e5":"code","75e4dccf":"code","ada6c102":"code","34342cfd":"code","49984915":"code","7c93b0f4":"code","08f9658d":"code","c123d71d":"code","c3a5079d":"code","e40f88d7":"code","2747060b":"code","7deca8d3":"code","fb99a4ca":"code","5f314878":"code","164d47a1":"code","aceb33dc":"code","5cc6a88e":"code","d90d5666":"code","aab5bc71":"code","e5a740de":"code","7a69dd66":"code","a07745f0":"code","780e4a9b":"code","7a55e259":"code","cfbc448e":"code","3bda499a":"code","8e5641d2":"code","e79910ba":"code","6aa8187f":"code","7f63d460":"code","f7374dde":"code","8cdb6dde":"code","db6f582f":"code","86f91e2e":"code","7475e755":"code","614f3787":"code","a8038eaf":"code","b9aefaa5":"code","893922f6":"code","b49806e1":"code","688fdc41":"code","a19979e0":"code","76e1e4aa":"markdown","d5d64dd9":"markdown","67c23832":"markdown","0236b895":"markdown","20b85abb":"markdown","710ae409":"markdown","b2cef631":"markdown","9d8d3691":"markdown","31620945":"markdown","3bec2409":"markdown","ed74d10b":"markdown","16243d81":"markdown","560093b7":"markdown","2bebd675":"markdown","441e76b2":"markdown","75be8acd":"markdown","9a2eb627":"markdown","1ae5aeb7":"markdown","ae06c2d3":"markdown","409699eb":"markdown","8dbde980":"markdown","cbf40c91":"markdown","19a47a77":"markdown","5ecd5f14":"markdown","76259167":"markdown","7834bce8":"markdown","875e3e28":"markdown","58537c8f":"markdown","1d037091":"markdown","a600028a":"markdown","6c2da52c":"markdown"},"source":{"2c03d045":"from IPython.display import Image","57a5079b":"Image(\"..\/input\/nlp-lectures-images\/tokenization.png\")","97c29617":"Image(\"..\/input\/nlp-lectures-images\/embeddings.png\")","27807c73":"Image(\"..\/input\/nlp-lectures-images\/classification.png\")","7a6f8adb":"Image(\"..\/input\/nlp-lectures-images\/ner.png\")","91ce2e7c":"Image(\"..\/input\/nlp-lectures-images\/seq2seq-teacher-forcing.png\")","789fdbf1":"Image(\"..\/input\/nlp-lectures-images\/textmining.png\")","349f12df":"Image(\"..\/input\/nlp-lectures-images\/nlp.png\")","447cc839":"Image('..\/input\/nlp-lectures-images\/tweet_screenshot.png')","c371f0e5":"import pandas as pd\n\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col='id')\ntrain.head()","75e4dccf":"train = train.drop(columns=['keyword', 'location'])\ntrain.head()","ada6c102":"import matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS","34342cfd":"print(train.shape)\n\ntrain['target'].value_counts(normalize=True)","49984915":"disaster_samples = train[train['target'] == 1]\nfake_samples = train[train['target'] == 0]","7c93b0f4":"for sample in disaster_samples['text'].sample(3, random_state=42):\n    print(sample)\n    print('\\n=======\\n')","08f9658d":"for sample in fake_samples['text'].sample(3, random_state=42):\n    print(sample)\n    print('\\n=======\\n')","c123d71d":"text = disaster_samples['text'].values\n\nwc = WordCloud(max_font_size=60, background_color=\"black\", max_words=2000, stopwords=STOPWORDS)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(12,6))\nplt.axis(\"off\")\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17),interpolation=\"bilinear\")\nplt.show()","c3a5079d":"text = fake_samples['text'].values\n\nwc = WordCloud(max_font_size=60, background_color=\"black\", max_words=2000, stopwords=STOPWORDS)\nwc.generate(\" \".join(text))\nplt.figure(figsize=(12,6))\nplt.axis(\"off\")\nplt.imshow(wc.recolor(colormap= 'viridis' , random_state=17),interpolation=\"bilinear\")\nplt.show()","e40f88d7":"from sklearn.metrics import accuracy_score","2747060b":"constant_prediction = [0 for text in train['text']]\n\nconstant_accuracy_score = accuracy_score(train['target'], constant_prediction)\nprint(f'accuracy_score : {constant_accuracy_score}')","7deca8d3":"from collections import Counter\n\ndisaster_text = ' '.join(disaster_samples['text'].tolist())\ndisaster_words = disaster_text.split()\ndisaster_count = Counter(disaster_words)\ndisaster_count = pd.Series(disaster_count)\ndisaster_count = disaster_count.sort_values(ascending=False)\n\nfake_text = ' '.join(fake_samples['text'].tolist())\nfake_words = fake_text.split()\nfake_count = Counter(fake_words)\nfake_count = pd.Series(fake_count)\nfake_count = fake_count.sort_values(ascending=False)","fb99a4ca":"disaster_count.iloc[:10]","5f314878":"#removing stopwords\ndisaster_count[~disaster_count.index.str.lower().isin(STOPWORDS)].iloc[:20]","164d47a1":"print('Disaster', disaster_count[~disaster_count.index.str.lower().isin(STOPWORDS)].iloc[:30].index)\nprint('Fake', fake_count[~fake_count.index.str.lower().isin(STOPWORDS)].iloc[:30].index)","aceb33dc":"disaster_words = ['fire', 'killed', 'suicide', 'disaster', 'bomb', 'crash', 'families', 'police', 'buildings', 'fatal', 'train', 'burning']\n\nbasic_prediction = [any([d_w in text.lower() for d_w in disaster_words]) for text in train['text']]\n\nbasic_accuracy_score = accuracy_score(train['target'], basic_prediction)\nprint(f'accuracy_score : {basic_accuracy_score}')","5cc6a88e":"Image(\"..\/input\/nlp-lectures-images\/machine_learning_overview.jpg\")","d90d5666":"import numpy as np\nnp.random.seed(42)\n\nX_train = np.random.normal(0, 1, (500, 2))\nnoise_train = np.random.normal(0, 1., 500)\n\nX_test = np.random.normal(0, 1, (100, 2))\nnoise_test = np.random.normal(0, 1., 100)\n\ny_train = (np.sin(X_train) + np.cos(2*X_train)).sum(axis=1) + noise_train\ny_test = (np.sin(X_test) + np.cos(2*X_test)).sum(axis=1) + noise_test","aab5bc71":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\n\ny_train_preds = lr.predict(X_train)\ny_test_preds = lr.predict(X_test)\n\nprint(f'MSE on train: {mean_squared_error(y_train, y_train_preds)}\\nMSE on test: {mean_squared_error(y_test, y_test_preds)}')","e5a740de":"lr = LinearRegression()\n\nX_train_poly = np.hstack([X_train, X_train**2])\nX_test_poly = np.hstack([X_test, X_test**2])\n\nlr.fit(X_train_poly, y_train)\n\ny_train_preds = lr.predict(X_train_poly)\ny_test_preds = lr.predict(X_test_poly)\n\nprint(f'MSE on train: {mean_squared_error(y_train, y_train_preds)}\\nMSE on test: {mean_squared_error(y_test, y_test_preds)}')","7a69dd66":"lr = LinearRegression()\n\nX_train_poly = np.hstack([X_train**(p) for p in range(10)])\nX_test_poly = np.hstack([X_test**(p) for p in range(10)])\n\nlr.fit(X_train_poly, y_train)\n\ny_train_preds = lr.predict(X_train_poly)\ny_test_preds = lr.predict(X_test_poly)\n\nprint(f'MSE on train: {mean_squared_error(y_train, y_train_preds)}\\nMSE on test: {mean_squared_error(y_test, y_test_preds)}')","a07745f0":"from matplotlib import pyplot as plt\n\ntrain_result = []\ntest_result = []\nfor pow_n in range(1, 10):\n    lr = LinearRegression()\n\n    X_train_poly = np.hstack([X_train**(p) for p in range(pow_n)])\n    X_test_poly = np.hstack([X_test**(p) for p in range(pow_n)])\n\n    lr.fit(X_train_poly, y_train)\n\n    y_train_preds = lr.predict(X_train_poly)\n    y_test_preds = lr.predict(X_test_poly)\n    \n    train_result.append(mean_squared_error(y_train, y_train_preds))\n    test_result.append(mean_squared_error(y_test, y_test_preds))\n    \nplt.plot(list(range(1, 10)), train_result, list(range(1, 10)), test_result)\nplt.show()","780e4a9b":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer","7a55e259":"Image('..\/input\/nlp-lectures-images\/countvectorizer.png')","cfbc448e":"Image('..\/input\/nlp-lectures-images\/countvectorizer2.jpg')","3bda499a":"#split to train test for validation purposes\nX, y = train['text'], train['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","8e5641d2":"#init model\ncount_vectorizer = CountVectorizer(binary=True)\nlog_reg = LogisticRegression(solver='liblinear', random_state=42)","e79910ba":"#fit model\ncount_vectorizer.fit(X_train)\nX_train_vectorized = count_vectorizer.transform(X_train)\n\nlog_reg.fit(X_train_vectorized, y_train);","6aa8187f":"#prediction\n\nX_test_vectorized = count_vectorizer.transform(X_test)\n\nprediction_train = log_reg.predict(X_train_vectorized)\nprediction_test = log_reg.predict(X_test_vectorized)","7f63d460":"accuracy_score(y_train, prediction_train), accuracy_score(y_test, prediction_test),","f7374dde":"Image('..\/input\/nlp-lectures-images\/pipeline.png')","8cdb6dde":"from sklearn.pipeline import Pipeline, FeatureUnion\n\n#init\ncount_vectorizer = CountVectorizer()\nlog_reg = LogisticRegression(solver='liblinear', random_state=42)\n\nmodel = Pipeline([('count_vectorizer', count_vectorizer),  ('log_reg', log_reg)])","db6f582f":"#fit\nmodel.fit(X_train, y_train);","86f91e2e":"#predict\naccuracy_score(y_test, model.predict(X_test))","7475e755":"Image('..\/input\/nlp-lectures-images\/tf_idf.png')","614f3787":"Image('..\/input\/nlp-lectures-images\/tf-idf-example.jpg')","a8038eaf":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nword_vectorizer = TfidfVectorizer(\n    analyzer='word',\n    stop_words='english',\n    ngram_range=(1, 3),\n    lowercase=True,\n    min_df=5,\n    max_features=30000)\n\nchar_vectorizer = TfidfVectorizer(\n    analyzer='char',\n    stop_words='english',\n    ngram_range=(3, 6),\n    lowercase=True,\n    min_df=5,\n    max_features=50000)\n\n\nlog_reg = LogisticRegression(solver='liblinear', random_state=42)\n\nfu = FeatureUnion([('word_vectorizer', word_vectorizer),  ('char_vectorizer', char_vectorizer)])\nmodel = Pipeline([('vectorizers', fu),  ('log_reg', log_reg)])","b9aefaa5":"model.fit(X_train, y_train);","893922f6":"accuracy_score(y_test, model.predict(X_test))","b49806e1":"import eli5\n\nlog_reg = model.named_steps['log_reg']\nvectorizers = model.named_steps['vectorizers']\nfeature_names = vectorizers.get_feature_names()","688fdc41":"eli5.explain_weights(log_reg, feature_names=feature_names, top=100)","a19979e0":"print(X_test.iloc[18])\neli5.show_prediction(log_reg, X_test.iloc[18], vec=vectorizers, feature_names=feature_names)","76e1e4aa":"## 5. Sequence to Sequence Tasks\n\nGoal: generate text for some given text <br>\nApplications: machine translation, chatbots, Q&A systems, summarization <br>\nRepresentation: sequences of embedding vectors of words (preserves word order)","d5d64dd9":"#### TF-IDF calculation example:","67c23832":"# Model Explainer","0236b895":"### Trivial Solution","20b85abb":"# Model Improvement\nLet's calculate tf-idf measure instead of simple counts for each term","710ae409":"# Machine Learning Approach\n![mlvd](https:\/\/github.com\/mephistopheies\/mlworkshop39_042017\/raw\/a6426fd652faa38864c3ea4538e000539106fb56\/1_ml_intro\/ipy\/images\/bengio.png)","b2cef631":"We can also finetune some parameters for vectorizer in scikit-learn","9d8d3691":"## 3. Text Classification Tasks\n\nGoal: predict classes (categories) for given text. <br>\nApplications: spam detection, toxic detection, domain classification, importance detection. <br>\nRepresentation: bag of words, count and tf-idf vectorizers (does not preserve word order), embeddings (preserve word order)","31620945":"# Task: Real or Not? NLP with Disaster Tweets\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview\">Kaggle Competition<\/a> <br>\nTwitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster. Take this example:","3bec2409":"### Popular words differences","ed74d10b":"So, as we've writen before, there can be actually a huge difference between results on train and test data! And we consider test results as a benchmark ones.","16243d81":"## Baseline solutions","560093b7":"## Data Loading\nWhat am I predicting?<br>\nYou are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.","2bebd675":"# IASA Natural Language Processing Workshop (Part 1)\n\nThe purpose of this workshop is a general overview of various techniques and methods of text analytics, and in particular natural language processing from classical approaches to modern state of the art ones. After completing this course attentive students will have a solid general understanding of the NLP domain and will be able to implement various basic algorithms for text data processing. However, for a deeper understanding, a more detailed study of proposed materials will be necessary. \n\nThese particular materials were created by <a href=\"https:\/\/www.kaggle.com\/abazdyrev\">Anton Bazdyrev<\/a> and <a href=\"https:\/\/www.kaggle.com\/yakuben\">Oleksii Yakubenko<\/a> for IASA students and inspired by <a href=\"https:\/\/mlcourse.ai\/\">MLCOURSE.AI<\/a> and <a href=\"https:\/\/ods.ai\/\">ODS.AI<\/a> by <a href=\"https:\/\/www.kaggle.com\/kashnitsky\">\nYury Kashnitsky<\/a>.","441e76b2":"# Final thoughts\n\nFor various text classification tasks we can achive pretty decent results without any domain knowledge, comlicated algorithms and powerful computational resources. We need just a few lines of code for scikit-learn pipeline: TF-IDF Vectorizer -> Logistic Regression, that is very easy to use.","75be8acd":"The author explicitly uses the word \u201cABLAZE\u201d but means it metaphorically. This is clear to a human right away, especially with the visual aid. But it\u2019s less clear to a machine.\n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. You\u2019ll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.","9a2eb627":"# Scikit-Learn Pipeline API","1ae5aeb7":"## Supervised learning\n\n### Model\n\n*Model* - parametric space of functions (hypotheses):\n\n$\\large \\mathcal{H} = \\left\\{ h\\left(x, \\theta\\right) | \\theta \\in \\Theta \\right\\}$\n\n* where\n    * $\\large h: X \\times \\Theta \\rightarrow Y$    \n    * $\\large \\Theta$ - parametric space\n    * $\\large X$ - factor space (exogenous variables)\n    * $\\large Y$ - target space\n    \n### Training algorithm\n\n\n*Training algorithm* - map from data space to hypotheses space:\n\n$\\large \\mathcal{M}: X \\times Y \\rightarrow \\mathcal{H}$\n\nThere are 2 steps in algorithm:\n1. Selection of hypothesis: $\\large h = \\mathcal{M}\\left(D\\right)$, where $\\large D$ - our particular dataset\n2. Testing for given example $\\large x$ calculation of model prediction $\\large \\hat{y} = h\\left(x\\right)$\n\n### Selection of hypothesis\n\nDefine *loss function*:\n$\\large L: Y \\times Y \\rightarrow \\mathbb{R}$ <br>\n\nWith loss function we can measure how the prediction $\\large \\hat {y}$ differs from the ground truth values $\\large y$.\n\n$\\large Q_{\\text{emp}}\\left(h\\right) = \\frac{1}{n} \\sum_{i=1}^n L\\left(h\\left(x_i\\right), y_i\\right)$, \nwhere $\\large \\mathcal{D} = \\left\\{ \\left(x_i, y_i\\right) \\right\\}$ - our training dataset, $\\large h$ - hypothesis (function)\n\nWe should select hypothesis, that minimizes average loss:\n$\\large \\hat{h} = \\arg \\min_{h \\in \\mathcal{H}} Q_{\\text{emp}}\\left(h\\right)$\n\nExamples of loss functions:\n* classification: $\\large L\\left(\\hat{y}, y\\right) = \\text{I}\\left[\\hat{y} = y\\right]$\n* regression: $\\large L\\left(\\hat{y}, y\\right) = \\left(\\hat{y} - y\\right)^2$\n\n## Example: Logistic Regression\n\n\n### Model\nGiven $\\large \\mathcal{D} = \\left\\{ \\left(x_i, y_i\\right) \\right\\}$ - our training dataset, $\\large \\mathcal{H} = \\left\\{ h\\left(x, \\theta\\right) | \\theta \\in R^n \\right\\}$ - logistic regression model\n\n* where\n    * $\\large x_i \\in  R^n$    \n    * $\\large y_i \\in  \\left\\{0, 1\\right\\}$\n    * $\\large h(x_i, \\theta) = \\sigma \\left(\\theta ^ T x_i \\right) = \\dfrac{1}{1 + e ^ \\left( - \\theta ^ T x_i \\right)} = \\dfrac{1}{1 + e ^ \\left( - \\sum_{j=1}^n \\theta_j x_{ij} \\right)},$ $\\large h(x_i, \\theta) \\in \\left(0, 1\\right)$\n    * $\\sigma \\left(z\\right) = \\dfrac{1}{1 + e ^ \\left(-z \\right)}$ - logistic function, aka sigmoid\n    \n### Loss function\n$y_i \\sim Bernoulli(p)$, where p is an unknown parameter <br>\nLet's estimate conditional probability $Pr\\left\\{y_i = 1 | x_i \\right\\} = \\large h(x_i, \\theta)$ with Maximum Likelihood Estimation.<br> Obviously, that $Pr\\left\\{y_i = 0 | x_i \\right\\} = 1 - \\large h(x_i, \\theta)$ <br>\nAlso we can define $Pr\\left\\{y = y_i | x_i \\right\\} = \\large h(x_i, \\theta)^{y_i} \\left(1 - \\large h(x_i, \\theta)\\right)^{1 - y_i}$ <br> \nWe now calculate the likelihood function assuming that all the observations in the sample are independently Bernoulli distributed: <br>\n\n$L\\left(\\theta | x\\right) = \\prod_{i=1}^m Pr\\left\\{y = y_i | x_i \\right\\} = \\prod_{i=1}^m h\\left(x_i, \\theta\\right)^{y_i} \\left(1 - h\\left(x_i, \\theta\\right)\\right)^{1 - y_i}$ <br>\n\n\nTypically, the log likelihood is maximized: \n\n$log\\left(L\\left(\\theta | x\\right) \\right) = log\\left(\\prod_{i=1}^m Pr\\left\\{y = y_i | x_i \\right\\}\\right) = log\\left(\\prod_{i=1}^m h\\left(x_i, \\theta\\right)^{y_i} \\left(1 - h\\left(x_i, \\theta\\right)\\right)^{1 - y_i}\\right) = \\sum_{i=1}^m log\\left(h\\left(x_i, \\theta\\right)^{y_i} \\left(1 - h\\left(x_i, \\theta\\right)\\right)^{1 - y_i}\\right) = \\sum_{i=1}^m y_ilog\\left(h\\left(x_i, \\theta\\right) \\right) + \\left(1 - y_i\\right) log\\left(1 - h\\left(x_i, \\theta\\right) \\right)$ <br>\n\nSo, finally, we can define our classification loss function as $\\large L\\left(\\hat{y}, y\\right) = - ylog\\left(\\hat{y} \\right) - \\left(1 - y\\right) log\\left(1 - \\hat{y} \\right)$ where $\\hat{y} = h\\left(x_i, \\theta\\right) $\n\nNow, when we have dataset, model and loss function, we can perform training and find the best estimation $\\large \\hat{h} =\\arg \\min_{h \\in \\mathcal{H}} \\left(\\frac{1}{n} \\sum_{i=1}^n L\\left(h\\left(x_i\\right), y_i\\right)\\right)$ <br>\n\n### Optimization\nWe can find it using Gradient Descent method: <br>\nGradient descent is based on the observation that if the multi-variable function $\\large F$ is defined and differentiable in a neighborhood of a point $\\large a$, then $\\large F$ decreases fastest if one goes from a in the direction of the negative gradient of $\\large F$ at $\\large a$. <br>\n\nDefine $\\vec{w_{n + 1}} = \\vec{w_{n}} - \\alpha\\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} \\left(\\vec{w_{n}}\\right)$, where $\\alpha$ is a small number, called learning rate. <br>\n\nIf $\\vec{w_{n}}$ converges, then it converges to local minimum. If $\\mathcal{L}$ is a convex function, then $\\vec{w_{n}}$ converges to the global minimum.\n\nIn our case of optimization our loss function (log-loss) for logistic regression model: <br>\n\n$$\\large \\begin{array}{rcl} - \\frac{\\partial \\mathcal{L}}{\\partial \\vec{w}} &=& \\frac{\\partial}{\\partial \\vec{w}}\\sum_{i=1}^n y_i \\ln \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right) + \\left(1 - y_i\\right) \\ln \\left(1 - \\sigma\\left(\\vec{w}^T \\vec{x}_i\\right)\\right) \\\\\n&=& \\sum_{i=1}^n y_i \\frac{1}{\\sigma} \\sigma \\left(1 - \\sigma\\right) \\vec{x}_i + \\left(1 - y_i\\right) \\frac{1}{1 - \\sigma} \\left(-1\\right)\\sigma \\left(1 - \\sigma\\right) \\vec{x}_i \\\\\n&=& \\sum_{i=1}^n y_i \\left(1 - \\sigma\\right) \\vec{x}_i - \\left(1 - y_i\\right) \\sigma \\vec{x}_i \\\\\n&=& \\sum_{i=1}^n \\vec{x}_i \\left(y_i - \\sigma\\right)\n\\end{array}$$","ae06c2d3":"Maybe we can improve results by adding comlexity such as polynomial features to our model?","409699eb":"## 4. Sequence Processing Tasks\n\nGoal: language modeling - predict next\/previous word(s), text generation, sequence labeling <br>\nApplications: sequence tagging (predict POS tags for each word in sequence), named entity recognition <br>\nRepresentation: sequences of embedding vectors of words (preserves word order) ","8dbde980":"## Data overview","cbf40c91":"# Validation\n\nIf we have a few different models how can we define wich one performs better for our task? <br>\nWe can measure the quality of predictions using some metric or loss functions. <br>\n\nLet's look through the example: <br>\nWe have regression task with target variable y. We can observe x0, x1 and we assume that we can estimate y as function of x0, x1. Also, we know that there may be some \"noise\" in our data and also there may also be other factors that affect the target y. We have 100 examples in training set and 20 examples in evaluation set.","19a47a77":"In fact, the thing is, that when we use more complex models, then we can \"explain\" functional dependency in our training data, even if we actually don't have it. In real world we always face situations when we have such kind of \"noise\" in real training data, so very important thing in model validation is to validate it using unseen in train phase data.","5ecd5f14":"Hmmm... That looks really weird isn't it? Why we've got huge improvements on train and such a terrible result on test?","76259167":"Wow! Thats great, we've got some improvements by just adding squared features to our model. But can we improve it more? Let's add more powers up to 10!\n\n","7834bce8":"## Roadmaps:","875e3e28":"Let's build some simple Linear Regression model and consider Mean Squared Error as a target metric","58537c8f":"## 2. Text Meaning\/Representation Tasks\n\nGoal: represent meaning and context <br>\nApplications: creation of words and sentence embeddings, finding similar words (similar vectors) <br>\nRepresentation: word vectors, the mapping of words to vectors (n-dimensional numeric vectors) aka embeddings\n","1d037091":"# Text Mining and NLP roadmaps and tasks overview\n## 1. Low Level Tasks\n\n1. Tokenization <br>\n    Goal: Split given sentense to sequence of tokens.\n    \n2. Sentence boundary detection <br>\n    Goal: Split given text to sequence of sentences.","a600028a":"# Back to the original task: Modeling and Validation","6c2da52c":"Seems that if we have words like fire, killed, suicide, disaster, bomb, crash, families, police, buildings, fatal, train, burning and so on in text it means that they re rather disaster than fake."}}