{"cell_type":{"8fb5594c":"code","cd286642":"code","a53c0e3f":"code","cdc2cec1":"code","cc96fde4":"code","77f4765e":"code","c758017b":"code","fe9608af":"code","702d73f3":"code","c228a8f5":"code","c9e2f956":"code","9fd97d19":"code","8ff28abb":"code","1726879c":"code","77e8bc63":"code","f38c0f06":"code","671a5018":"code","486f0846":"code","65b13588":"code","7069dcc4":"code","451103b5":"code","769c83c6":"markdown","64514a9e":"markdown","c48bf9d3":"markdown","9e866d9b":"markdown","2336f907":"markdown","ff027533":"markdown","29a91a4f":"markdown","0e79bd32":"markdown","085cc395":"markdown"},"source":{"8fb5594c":"import os\nimport glob","cd286642":"image_path = glob.glob('\/kaggle\/input\/image-localization-dataset\/training_images\/*.jpg')","a53c0e3f":"import numpy as np\nfrom PIL import Image, ImageDraw","cdc2cec1":"input_dimen = 228\nimages = []\n\nfor imagefile in image_path:\n    image = Image.open(imagefile).resize((input_dimen, input_dimen))\n    image = np.asarray(image) \/ 255.0\n    images.append(image)","cc96fde4":"import xmltodict\n\nbboxes = []\nclasses_raw = []\nannotations_path = glob.glob('\/kaggle\/input\/image-localization-dataset\/training_images\/*.xml')\n\nfor xmlfile in annotations_path:\n    x = xmltodict.parse(open(xmlfile, 'rb'))\n    bndbox = x['annotation']['object']['bndbox']\n    bndbox = np.array([ int(bndbox['xmin']), int(bndbox['ymin']), int(bndbox['xmax']), int(bndbox['ymax'])])\n    bndbox2 = [None] * 4\n    bndbox2[0] = bndbox[0]\n    bndbox2[1] = bndbox[1]\n    bndbox2[2] = bndbox[2]\n    bndbox2[3] = bndbox[3]\n    bndbox2 = np.array(bndbox2)\/input_dimen\n    bboxes.append(bndbox2)\n    classes_raw.append( x['annotation']['object']['name'])","77f4765e":"from sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split","c758017b":"boxes = np.array(bboxes)\nencoder = LabelBinarizer()\nclasses_onehot = encoder.fit_transform(classes_raw)\n\ny = np.concatenate( [boxes, classes_onehot], axis=1)\nx = np.array(images)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1)","fe9608af":"from keras import backend as K","702d73f3":"def calculate_iou( target_boxes, pred_boxes ):\n    xA = K.maximum( target_boxes[ ... , 0], pred_boxes[ ... , 0] )\n    yA = K.maximum( target_boxes[ ... , 1], pred_boxes[ ... , 1] )\n    xB = K.minimum( target_boxes[ ... , 2], pred_boxes[ ... , 2] )\n    yB = K.minimum( target_boxes[ ... , 3], pred_boxes[ ... , 3] )\n    \n    interArea = K.maximum( 0.0, xB - xA ) * K.maximum( 0.0, yB - yA )\n    boxA_Area = (target_boxes[ ... , 2] - target_boxes[ ... , 0]) * (target_boxes[ ... , 3] - target_boxes[ ... , 1])\n    boxB_Area = (pred_boxes[ ... , 2] - pred_boxes[ ... , 0]) * (pred_boxes[ ... , 3] - pred_boxes[ ... , 1])\n    \n    iou = interArea \/ ( boxA_Area + boxB_Area - interArea)\n    return iou","c228a8f5":"import tensorflow as tf\n\ndef custom_loss( y_true, y_pred):\n    mse = tf.losses.mean_squared_error( y_true, y_pred)\n    iou = calculate_iou( y_true, y_pred)\n    return mse + ( 1 - iou )","c9e2f956":"def iou_metric( y_true, y_pred):\n    return calculate_iou(y_true, y_pred)","9fd97d19":"import keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.optimizers import Adam\nfrom keras.constraints import max_norm\nfrom keras.initializers import he_uniform\nfrom keras.regularizers import l2","8ff28abb":"num_classes = 3\npred_vector_length = 4 + num_classes\nalpha = 0.25\ninput_shape = (input_dimen, input_dimen, 3)\n\nmodel = keras.Sequential()\n\n'''\n\nstrides=1, padding = 'same', kernel_initializer = he_uniform(), kernel_regularizer = l2(0.001)\n\n'''\n\nmodel.add(keras.layers.Conv2D(16, kernel_size = (3,3), input_shape = input_shape))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(16, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.2))\n\nmodel.add(keras.layers.Conv2D(32, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(32, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.2))\n\nmodel.add(keras.layers.Conv2D(64, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(64, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.2))\n\nmodel.add(keras.layers.Conv2D(128, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(128, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.2))\n\nmodel.add(keras.layers.Conv2D(256, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Conv2D(256, kernel_size = (3,3)))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.MaxPooling2D(pool_size = (2,2)))\nmodel.add(keras.layers.Dropout(0.2))\n\nmodel.add(keras.layers.Flatten())\n\nmodel.add(keras.layers.Dense(1280))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\nmodel.add(keras.layers.Dense(640))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(480))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(120))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(62))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))\n#model.add(keras.layers.BatchNormalization())\n\nmodel.add(keras.layers.Dense(pred_vector_length))\nmodel.add(keras.layers.LeakyReLU(alpha=alpha))","1726879c":"adam = Adam(learning_rate=0.0001, beta_1 = 0.9, beta_2 = 0.999, amsgrad=False)\n\nmodel.compile(optimizer = adam, loss = custom_loss, metrics = [iou_metric])","77e8bc63":"'''\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndatagenerator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\ngen = datagenerator.flow(x_train, y_train, batch_size=3)\nsteps = int(x_train.shape[0] \/ 3)\n\nes = EarlyStopping(monitor = 'val_loss', mode='min', verbose=1, patience=20)\nmc = ModelCheckpoint('best_model.h5', monitor='val_iou_metric', mode='max', verbose=1, save_best_only=True)\n\nhistory = model.fit_generator(gen, steps_per_epoch = steps, validation_data = (x_test, y_test), epochs = 120, callbacks=[es, mc], verbose=2)\n\n_, acc = model.evaluate_generator(gen, steps = steps, callbacks=[es, mc], verbose=2)\nprint('> %.3f' % (acc*100.0))\n\n'''","f38c0f06":"es = EarlyStopping(monitor = 'val_loss', mode='min', verbose=0, patience=20)\nmc = ModelCheckpoint('best_model.h5', monitor='val_iou_metric', mode='max', verbose=0, save_best_only=True)","671a5018":"history = model.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 80, batch_size = 2, callbacks=[es, mc], verbose=2)","486f0846":"_, acc = model.evaluate(x_train, y_train, callbacks=[es, mc], verbose=0)\nprint('> %.3f' % (acc*100.0))","65b13588":"import matplotlib.pyplot as plt\n\ndef summary_plot(history):\n    # plot loss\n    plt.subplot(211)\n    plt.title('Custom Loss')\n    plt.plot(history.history['loss'], color='blue', label='train')\n    plt.plot(history.history['val_loss'], color='red', label='test')\n    # plot Accuracy\n    plt.subplot(212)\n    plt.title('Iou Metric')\n    plt.plot(history.history['iou_metric'], color='blue', label='train')\n    plt.plot(history.history['val_iou_metric'], color='red', label='test')\n\n\nsummary_plot(history)","7069dcc4":"!rm -r inference_images\n!mkdir -v inference_images\n\nboxes = model.predict( x_test )\nfor i in range( boxes.shape[0] ):\n    b = boxes[ i , 0 : 4 ] * input_dimen\n    img = x_test[i] * 255\n    source_img = Image.fromarray( img.astype( np.uint8 ) , 'RGB' )\n    draw = ImageDraw.Draw( source_img )\n    draw.rectangle( b , outline=\"black\" )\n    source_img.save( 'inference_images\/image_{}.png'.format( i + 1 ) , 'png' )","451103b5":"def calculate_avg_iou( target_boxes , pred_boxes ):\n    xA = np.maximum( target_boxes[ ... , 0], pred_boxes[ ... , 0] )\n    yA = np.maximum( target_boxes[ ... , 1], pred_boxes[ ... , 1] )\n    xB = np.minimum( target_boxes[ ... , 2], pred_boxes[ ... , 2] )\n    yB = np.minimum( target_boxes[ ... , 3], pred_boxes[ ... , 3] )\n    interArea = np.maximum(0.0, xB - xA ) * np.maximum(0.0, yB - yA )\n    boxAArea = (target_boxes[ ... , 2] - target_boxes[ ... , 0]) * (target_boxes[ ... , 3] - target_boxes[ ... , 1])\n    boxBArea = (pred_boxes[ ... , 2] - pred_boxes[ ... , 0]) * (pred_boxes[ ... , 3] - pred_boxes[ ... , 1])\n    iou = interArea \/ ( boxAArea + boxBArea - interArea )\n    return iou\n\ntarget_boxes = y_test * input_dimen\npred = model.predict( x_test )\npred_boxes = pred[ ... , 0 : 4 ] * input_dimen\n\niou_scores = calculate_avg_iou( target_boxes , pred_boxes )\nprint( 'Mean IOU score {}'.format( iou_scores.mean() ) )","769c83c6":"### get all .jpg images","64514a9e":"### Training","c48bf9d3":"### Saving predicted images","9e866d9b":"### create training and testing data","2336f907":"### normalize the images","ff027533":"### convert xml files to dict to extract bounding box dimensions","29a91a4f":"### calculate Intersection over Union","0e79bd32":"### Create Model","085cc395":"### Compile model"}}