{"cell_type":{"e8f40f6f":"code","997d70a4":"code","d0780406":"code","cb891b6a":"code","0758cdcd":"code","56936c65":"code","6cb7dfca":"code","3228c7b4":"code","60fa0a3f":"code","00753018":"code","e38eeac8":"code","9de4f193":"code","81be715c":"code","6821ebff":"code","0c876c25":"code","b6508c81":"code","e2e8bf22":"code","5f24c5d7":"code","8da88ff3":"code","5c576ad3":"code","60de339e":"code","f93c1321":"code","da42f636":"code","957a53aa":"code","9c7f4e6a":"code","1579d97b":"code","47854a44":"code","cddafbf9":"code","60e91912":"code","d8b9a2f9":"code","0ad4c885":"markdown","36ab6443":"markdown","eb56f1cb":"markdown","9aa5a3d2":"markdown","71e5c38e":"markdown","213e087d":"markdown","35ccd23d":"markdown","7e92c860":"markdown"},"source":{"e8f40f6f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","997d70a4":"import seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np \nimport random as rand\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder","d0780406":"df = pd.read_csv('\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","cb891b6a":"df.head()","0758cdcd":"df.columns","56936c65":"df.isnull().sum()","6cb7dfca":"cat_features=[\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\",\"DEATH_EVENT\"]\ncat_without_label=[\"anaemia\",\"diabetes\",\"high_blood_pressure\",\"sex\",\"smoking\"]\nnum_features=pd.Series(df.columns)\nnum_features=num_features[~num_features.isin(cat_features)]","3228c7b4":"df.describe()","60fa0a3f":"df[\"DEATH_EVENT\"].value_counts().plot(kind='bar')\nplt.title='Death Events Distribution'","00753018":"sns.heatmap(data=df[num_features].corr(), annot=True)","e38eeac8":"r=c=0\nfig,ax = plt.subplots(4,2,figsize=(14,25))\nfor n, i in enumerate(num_features):\n    sns.boxplot(x='DEATH_EVENT', y=i, data=df, ax=ax[r,c])\n    ax[r,c].set_title(i.upper()+\" by \"+\"DEATH_EVENT\")\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","9de4f193":"df[num_features].hist(figsize=(10,10))\nplt.show()","81be715c":"r=c=0\nfig,ax = plt.subplots(3,2,figsize=(14, 14))\nfor n, i in enumerate(cat_without_label):\n    sns.countplot(x=i, hue='DEATH_EVENT', data=df, ax=ax[r,c])\n    ax[r,c].set_title(i.upper())\n    c+=1\n    if (n+1)%2==0:\n        r+=1\n        c=0\nax[r,c].axis(\"off\")\nplt.show()","6821ebff":"X=df.iloc[:, :-1]\nY=df.iloc[:, -1]","0c876c25":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)","b6508c81":"classifier = RandomForestClassifier(n_estimators=5000,random_state=42)\nclassifier.fit(x_train,y_train)\n\nprint('Train Accuracy: %f' % classifier.score(x_train, y_train))\nprint('Test Accuracy: %f' % classifier.score(x_test, y_test))","e2e8bf22":"feature_names = cat_features\nfeature_names = np.r_[feature_names, num_features]\n\nfeat_imp = pd.DataFrame(classifier.feature_importances_)\nfeat_imp.index = pd.Series(df.iloc[:,:-1].columns)\nfeat_imp = (feat_imp*100).copy().sort_values(by=0,ascending=False)\nfeat_imp = feat_imp.reset_index()\nfeat_imp.columns = [\"Feature\",\"Importance_score\"]\n\nfig = plt.figure(figsize=(6,10))\nsns.scatterplot(data=feat_imp,x=5,y=np.linspace(100,0,12),size=\"Importance_score\",sizes=(200,2000),legend=False)\nfor i,feat,imp in zip(np.linspace(100,0,12),feat_imp[\"Feature\"],feat_imp[\"Importance_score\"]):\n    plt.text(x=5.05,y=i-1,s=feat)\n    plt.text(x=4.89,y=i-1,s=np.round(imp,2))\nplt.axis(\"off\")\n\nplt.show()\n","5f24c5d7":"from sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import SMOTE\n\n\nfor var in np.arange(feat_imp.shape[0],6,-1):\n    X_new = X[feat_imp.iloc[:var,0]].copy()\n    X_train, X_test, y_train,y_test = train_test_split(X_new,Y,test_size=0.2,random_state=11)\n    smote = SMOTE(random_state = 11) \n    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n    final_rf = RandomForestClassifier(random_state=11)\n    \n    \n    gscv = GridSearchCV(estimator=final_rf,param_grid={\n        \"n_estimators\":[100,500,1000,5000],\n        \"criterion\":[\"gini\",\"entropy\"]\n    },cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\n    gscv.fit(X_train_smote,y_train_smote)\n    print(str(var)+\" variables:  \"+str(gscv.best_estimator_)+\"  F1 score: \"+str(gscv.best_score_))\n    \n    \n","8da88ff3":"X_new = X[feat_imp.iloc[:8,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,Y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\nfinal_rf = RandomForestClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_rf,param_grid={\n    \"n_estimators\":[100,500,1000,5000],\n    \"criterion\":[\"gini\",\"entropy\"]\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\ngscv.fit(X_train_smote,y_train_smote)\nfinal_model = gscv.best_estimator_\n\nfinal_model.score(X_train_smote, y_train_smote)","5c576ad3":"pred=final_model.predict(X_test)\nprint(pred)","60de339e":"from sklearn.model_selection import learning_curve\n\ntrain_size, train_acc, test_acc = learning_curve(final_model, X_train_smote, y_train_smote, cv=5)\nlearn_df = pd.DataFrame({\"Train_size\":train_size,\"Train_Accuracy\":train_acc.mean(axis=1),\"Test_Accuracy\":test_acc.mean(axis=1)}).melt(id_vars=\"Train_size\")\nsns.lineplot(x='Train_size', y='value', data=learn_df, hue='variable')\nplt.ylabel('Accuracy');","f93c1321":"from sklearn.metrics import classification_report, confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n\ncm = confusion_matrix(y_test, pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, cmap='Blues')\nplt.xlabel('Prediction')\nplt.ylabel('Actual');","da42f636":"X_new = X[feat_imp.iloc[:8,0]].copy()\nX_train, X_test, y_train,y_test = train_test_split(X_new,Y,test_size=0.2,random_state=11)\nsmote = SMOTE(random_state = 11) \nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\nfinal_rf = RandomForestClassifier(random_state=11)\ngscv = GridSearchCV(estimator=final_rf,param_grid={\n    \"n_estimators\":[5000,7000],\n    \"criterion\":[\"gini\",\"entropy\"],\n    \"max_depth\":[3,5,7],\n    \"min_samples_split\":[80,100],\n    \"min_samples_leaf\":[40,50],\n},cv=5,n_jobs=-1,scoring=\"f1_weighted\")\n\ngscv.fit(X_train_smote,y_train_smote)\nfinal_model = gscv.best_estimator_\n\nfinal_model.score(X_train_smote, y_train_smote)","957a53aa":"train_pred = final_model.score(X_train_smote)\n\nprint(classification_report(y_train_smote, train_pred))","9c7f4e6a":"final_model.score(X_test, y_test)","1579d97b":"final_pred = final_model.predict(X_test)\nprint(final_pred)","47854a44":"print(classification_report(y_test, final_pred))","cddafbf9":"plot_roc_curve(final_model, X_test, y_test)\nplt.show()","60e91912":"plot_precision_recall_curve(final_model, X_test, y_test)\nplt.show()","d8b9a2f9":"cm = confusion_matrix(y_test, final_pred)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, cmap='Blues')\nplt.xlabel('Prediction')\nplt.ylabel('Actual');","0ad4c885":"Use learning curve to see bias and variance and see if additional data will help the model","36ab6443":"Good true positive rate!","eb56f1cb":"Learned this next part from: https:\/\/www.kaggle.com\/ksvmuralidhar\/heart-failure-prediction-auc-0-98 ","9aa5a3d2":"Good resource for SMOTE: https:\/\/towardsdatascience.com\/how-to-effortlessly-handle-class-imbalance-with-python-and-smote-9b715ca8e5a7","71e5c38e":"Feature Importance Reference: https:\/\/scikit-learn.org\/stable\/auto_examples\/inspection\/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py","213e087d":"We can see that the training score and test score are similar, so the model isn't overfitting anymore.","35ccd23d":"Here we are tuning some hyperparameters to overcome the overfitting. \n\n1. Use large number of n_estimators - grows more trees, prevents overfitting\n2. Use low number for max_depth - prevents model from growing\n3. Use large number for max_samples - ensures the leaf has good amount of samples","7e92c860":"Learning curve confirms the model is overfitting the data. Let's use confusion matrix to confirm overfitting"}}