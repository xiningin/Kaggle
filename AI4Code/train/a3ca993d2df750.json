{"cell_type":{"02c1891e":"code","900a28cd":"code","8eed4ccb":"code","5f275744":"code","09cc26f1":"code","087946ab":"code","d9441a2e":"code","36ac336f":"code","d79d4da7":"code","23846980":"code","da506029":"code","43bc6edc":"code","22757fd5":"code","509bdd81":"code","2c4b06f4":"code","6eb890d5":"code","9cea13c9":"code","5df7e3b8":"code","a69df4af":"code","a2f9dd79":"code","13fcfacd":"code","648265ab":"code","d3df9429":"code","fb275195":"code","12de3cd8":"code","8cb206b4":"code","d893d1ee":"code","8167a5ee":"code","9487b8d0":"code","623fb739":"code","bc19449a":"code","fe6503be":"code","56efc94b":"code","aa5eac78":"code","fdaab462":"code","de3ae22e":"code","2053bf70":"code","f32bc851":"code","bf5045aa":"code","eac57b3e":"code","9de125ff":"code","0f856556":"code","af4c79bc":"code","9a4ec967":"code","e3724350":"code","b0cac8ca":"code","4fbd7227":"code","0642c05b":"code","447c90f7":"code","e6bf7a2f":"code","1736e3b3":"code","60c1c293":"code","226875ea":"code","50b2788f":"code","570c8e51":"code","0e696907":"code","37343599":"code","2fe4476f":"code","79a108ca":"code","3474bbda":"code","27839007":"code","33908675":"code","5aeddf09":"code","1d044f7f":"code","1424de17":"code","504f0058":"code","c258a75d":"code","e03e4be7":"code","7c8c5f80":"code","259e3398":"code","66927a8c":"markdown","4e928cb5":"markdown","a65c1dc6":"markdown","5fa7db6b":"markdown","0ab30863":"markdown","59205a7a":"markdown"},"source":{"02c1891e":"import requests\nimport bs4\nimport csv","900a28cd":"res = requests.get(\"https:\/\/www.imdb.com\/list\/ls025933303\/\")","8eed4ccb":"soup = bs4.BeautifulSoup(res.text,\"lxml\")","5f275744":"head1= soup.select(\".lister-item-header\")","09cc26f1":"head1","087946ab":"Name,Year =[],[]\nfor i in soup.select('h3 > a'):\n    Name.append(i.text)\nfor i in range(len(head1)):\n    Year.append(head1[i].select(\"span\")[1].text)","d9441a2e":"Name","36ac336f":"Year","d79d4da7":"head2= soup.select(\".runtime\")","23846980":"runtime=[]\nfor i in head2:\n    runtime.append(i.text)","da506029":"runtime","43bc6edc":"rating=[]\ni=0\nwhile i < 575:\n    print(soup.select(\".ipl-rating-star__rating\")[i].text)\n    rating.append(soup.select(\".ipl-rating-star__rating\")[i].text)\n    i += 23","22757fd5":"rating","509bdd81":"term = soup.select(\".genre\")","2c4b06f4":"genre=[]\nfor i in range(len(term)):\n    genre.append(term[i].text) ","6eb890d5":"genre","9cea13c9":"data = []","5df7e3b8":"for i in range(len(Name)):\n    data.append({'Name' : Name[i],'Year':Year[i],'Runtime': runtime[i],'Rating': rating[i],'Genre': genre[i]})\n        ","a69df4af":"csv_columns = ['Name','Year','Runtime','Rating','Genre']\n\ncsv_file = \"data.csv\"\ntry:\n    with open(csv_file, 'w') as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n        writer.writeheader()\n        for data in data:\n            writer.writerow(data)\nexcept IOError:\n    print(\"I\/O error\")  ","a2f9dd79":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","13fcfacd":"df = pd.read_csv('data.csv')","648265ab":"df.head()","d3df9429":"df[\"Genre\"] = df[\"Genre\"].apply(lambda x: x.split()[0])\ndf[\"Genre\"] = df[\"Genre\"].apply(lambda x: x.replace(r\"\/\", \"\"))\ndf[\"Genre\"] = df[\"Genre\"].apply(lambda x: x.replace(r\",\", \"\"))\n\ndf[\"Genre\"] = df[\"Genre\"].apply(lambda x: x.replace(r\"\\n\", \"\"))\ndf.sample(5)","fb275195":"plt.style.use('seaborn-whitegrid')\nplt.title(\"Genres\")\nsns.countplot(y = df[\"Genre\"])\nplt.xlabel(\"Count\")\nplt.ylabel(\"Genres\")\nplt.show()","12de3cd8":"plt.title(\"Rating distribution\")\nsns.distplot(df[\"Rating\"])\nplt.xlabel(\"Rating\")\nplt.show()","8cb206b4":"df['Runtime'] = df[\"Runtime\"].apply(lambda x: x.split()[0])","d893d1ee":"df['Runtime'].astype(int)","8167a5ee":"plt.title(\"Runtime distribution\")\nsns.distplot(df[\"Runtime\"])\nplt.xlabel(\"Runtime\")\nplt.show()","9487b8d0":"df[\"Year\"] = df[\"Year\"].apply(lambda x: x[1:5])\n","623fb739":"df.Year.astype(int)","bc19449a":"plt.style.use('seaborn-whitegrid')\nplt.title(\"Year\")\nsns.countplot(y = df[\"Year\"])\nplt.xlabel(\"Count\")\nplt.ylabel(\"Year\")\nplt.show()","fe6503be":"df.dtypes","56efc94b":"for i in df.columns:\n    print(f'length of unique values in {i}',len(set(df[i])))\n    print(f'some of the unique values in {i}',list(set(df[i]))[0:5])\n    print('---------------------------------------------------------')","aa5eac78":"df.loc[df['Genre'] =='Drama', 'Genre'] = 0\ndf.loc[df['Genre'] =='Action', 'Genre'] = 1\ndf.loc[df['Genre'] =='Comedy', 'Genre'] = 2","fdaab462":"for i in ['Year','Runtime','Genre']:\n    df[i] = pd.to_numeric(df[i],errors='coerce')","de3ae22e":"f, ax = plt.subplots(figsize=(9,9))\nsns.heatmap(df.corr(), annot=True ,linewidth=0.5, fmt='.1f',ax=ax)","2053bf70":"data = pd.read_csv('..\/input\/kollywood-movie-dataset-2011-2017\/Kollywood Movie Dataset (2011 - 2017).csv')","f32bc851":"data","bf5045aa":"data.isna().sum()","eac57b3e":"avg_rating = np.sum(data.Rating)\/(416-7)","9de125ff":"data['Rating'].fillna(value=avg_rating, inplace=True)","0f856556":"data.head()","af4c79bc":"data = data.drop(['Language'],axis =1)","9a4ec967":"for i in data.columns:\n    print(f'length of unique values in {i}',len(set(data[i])))\n    print(f'some of the unique values in {i}',list(set(data[i]))[0:5])\n    print('-'*100)","e3724350":"plt.style.use('seaborn-whitegrid')\nplt.title(\"Year\")\nsns.countplot(y = data['Release Year'])\nplt.xlabel(\"Count\")\nplt.ylabel(\"Year\")\nplt.show()","b0cac8ca":"data","4fbd7227":"plt.title(\"Rating\")\nsns.distplot(data[\"Rating\"])\nplt.xlabel(\"Rating\")\nplt.show()","0642c05b":"data['Genre'] = data['Genre'].apply(lambda x: x.split(' ')[0])","447c90f7":"data[\"Genre\"] = data[\"Genre\"].apply(lambda x: x.replace(r\"\/\", \"\"))\ndata[\"Genre\"] = data[\"Genre\"].apply(lambda x: x.replace(r\",\", \"\"))\n\ndata[\"Genre\"] = data[\"Genre\"].apply(lambda x: x.replace(r\"\\n\", \"\"))","e6bf7a2f":"data.Genre.unique()","1736e3b3":"import re\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n","60c1c293":"def process_text(text):\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    text_tokens = tokenizer.tokenize(text)\n\n    texts_clean = []\n    for word in text_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            stem_word = stemmer.stem(word)  # stemming word\n            texts_clean.append(stem_word)\n\n\n    return texts_clean","226875ea":"def build_freqs(texts, ys):\n\n    yslist = np.squeeze(ys).tolist()\n\n    freqs = {}\n    for y, text in zip(yslist, texts):\n        for word in process_text(text):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","50b2788f":"data['Status'] = np.select([(data['Rating'] >= 5.5),(data['Rating'] < 5.5)],[1,0])","570c8e51":"data.columns","0e696907":"data['text'] = data.apply(lambda row: row.Director+ \n                                  (row.Plot), axis = 1) ","37343599":"from sklearn.model_selection import train_test_split","2fe4476f":"y = data.Status\ndata =data.drop(['Status'],axis=1)","79a108ca":"X_train = data[0:332]\nX_test = data[332:]\ny_train= y[0:332]\ny_test = y[332:]","3474bbda":"X_train.columns","27839007":"freqs_buil = build_freqs(X_train.text,y_train)","33908675":"def extract_features(text, freqs):\n    word_l = process_text(text)\n    \n    x = np.zeros((1, 3)) \n    \n    x[0,0] = 1 \n    \n    \n    for word in word_l:\n        \n        x[0,1] += freqs.get((word,1.0), 0)\n        \n        x[0,2] += freqs.get((word,0.0), 0)\n        \n    assert(x.shape == (1, 3))\n    return x\n","5aeddf09":"X = np.zeros((len(X_train), 3))\nfor i in range(0,len(X_train)):\n    X[i,:]= extract_features(X_train.text[i],freqs_buil)","1d044f7f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, plot_confusion_matrix, accuracy_score\n","1424de17":"model = LogisticRegression(penalty= 'l2' ,random_state= 42 ,max_iter=20,solver='liblinear',class_weight= 'balanced')\nmodel.fit(X,y_train)","504f0058":"X_t = np.zeros((len(X_test), 3))\n","c258a75d":"i=332\nj=0\nwhile i <416:\n    X_t[j,:]= extract_features(X_test.text[i],freqs_buil)\n    i+=1\n    j+=1","e03e4be7":"y_pred = model.predict(X_t)","7c8c5f80":"accuracy = accuracy_score(y_test,y_pred)\naccuracy","259e3398":"plot_confusion_matrix(model, X_t, y_test,labels=[0,1],normalize= 'true')\n","66927a8c":"# Big Data Boss","4e928cb5":"Here the 7 rating values are null lets fill them with the average value of Rating","a65c1dc6":"#### Data collected by me (Very very small data)","5fa7db6b":"### This notebook contains\n1) Work on the Data Collected byu webscraping(Very Very less data)\n\n2) Work on the Data taken from Internet","0ab30863":"Using NLP","59205a7a":"## Data Visualisations of data collected from internet "}}