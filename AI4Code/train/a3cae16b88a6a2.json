{"cell_type":{"83f679e9":"code","174e24a7":"code","dfb98850":"code","a2042326":"code","06b2cbe1":"code","f2121269":"code","32c40757":"markdown","19d62140":"markdown","4a976824":"markdown","6ea948db":"markdown","fe69833b":"markdown"},"source":{"83f679e9":"#Loading Training and testing data to a dataframe.\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\",header = 0 , dtype ={'Age' : np.float64})\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\" , header = 0 ,dtype = {'Age' : np.float64})\nfull_data = [train,test]\nPassengerId = test['PassengerId']\ntrain.head(100)\n# print(PassengerId)\n# print(test.info())","174e24a7":"# 1. Pclass - Checking people who selects the particular class and mean Survived.\n\n#print(train[['Pclass','Survived']].groupby(['Pclass']).mean())\n\n# 2. Sex - Mean amount of male and female who survived\n\n#print(train[['Sex','Survived']].groupby(\"Sex\").mean())\n\n# 3. Sibling and Spouse - SibSp && Parent and Children - Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch']\n#print(train[['FamilySize','Survived']].groupby('FamilySize').mean())\n\n#-------Is alone or not? -----------\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1 , 'IsAlone'] = 1\n#print (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())\n    \n# 4. Embarked - Fill the Missing values with most occured value 'S'2\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n#print(train[['Embarked','Survived']].groupby(['Embarked']).mean()) \n\n# 5. Fare - Dividing Fare into four Categories and getting the mean\nfor dataset in full_data:\n        dataset['Fare'] = dataset['Fare'].fillna(train['Fare'])\ntrain['CategoricalFare'] = pd.qcut(train['Fare'],4) #divide into 4 cuts and pandas decides.\n#print(train[['CategoricalFare','Survived']].groupby(['CategoricalFare']).mean())\n\n# 6. Age - 86+177 = 263 null values detected\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    #print(age_avg , age_std , age_null_count)\n    #random null values which is in range 15 - 45 (avg-std , avg_std)\n    age_null_random_list = np.random.randint(age_avg - age_std , age_avg + age_std , size = age_null_count)\n    #print(age_null_random_list)\n    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\ntrain['CategorialAge'] = pd.cut(train['Age'],5)\n#print (train[['CategorialAge', 'Survived']].groupby(['CategorialAge'], as_index=False).mean())\n#to test whether there is any null attribute\n# 7 . Name\ndef get_title(name):\n    for dataset in full_data:\n        title_search = re.search('([A-Za-z]+)\\.',name)\n        #print(title_search)\n        if title_search:\n            return title_search.group(1)\n        return \"\"\nfor dataset in full_data: \n    dataset['Title'] = dataset['Name'].apply(get_title)\n#print(pd.crosstab(test['Title'],train['Sex']))\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'],'rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n#print (train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())\n#train['Age'].isnull().sum() # to check null values for entire dataset\n\ntrain.head(3)","dfb98850":"# Mapping values to key attributes\nfor dataset in full_data:\n    #mapping Sex\n#     dataset['Sex'] = dataset['Sex'].fillna('Test')\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1 } )\n    #mapping Title\n    title_mapping = {\"Mr\" : 1 , \"Miss\" : 2 , \"Mrs\" : 3 , \"Master\" : 4 , \"Rare\" : 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'].fillna(0)\n    #Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map({ 'S' : 0 , 'C' : 1 , 'Q' : 2 })\n    #Mapping Fare \n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare']\n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']                           = 4","a2042326":"#Feature Selection - It is trying to drop all the Attributes from test and train dataset - PassengerId,Name,Ticket,Cabin,SibSp,Parch,FamilySize,CategoricalAge,CategoricalFare\n\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp',\\\n                 'Parch', 'FamilySize']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n\ntest  = test.drop(drop_elements, axis = 1)\n#---INSTEAD WE WILL TRY LIKE THIS AS IT IS GIVING ERROR AGAIN AND AGAIN\n\nkeep_elements = ['Survived','Pclass','Sex','Age','Fare','Embarked','IsAlone','Title']\ntrain = train[keep_elements]\ntest = train[keep_elements]\ntrain = train.values\ntest  = test.values\ndisplay(train)","06b2cbe1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n\tAdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog \t = pd.DataFrame(columns=log_cols)\n\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.1, random_state=0)\n\nX = train[0::, 1::]\ny = train[0::, 0]\n\nacc_dict = {}\n\nfor train_index, test_index in sss.split(X, y):\n\tX_train, X_test = X[train_index], X[test_index]\n\ty_train, y_test = y[train_index], y[test_index]\n\t\n\tfor clf in classifiers:\n\t\tname = clf.__class__.__name__\n\t\tclf.fit(X_train, y_train)\n\t\ttrain_predictions = clf.predict(X_test)\n\t\tacc = accuracy_score(y_test, train_predictions)\n\t\tif name in acc_dict:\n\t\t\tacc_dict[name] += acc\n\t\telse:\n\t\t\tacc_dict[name] = acc\n\nfor clf in acc_dict:\n\tacc_dict[clf] = acc_dict[clf] \/ 10.0\n\tlog_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n\tlog = log.append(log_entry)\n\nplt.xlabel('Accuracy')\nplt.title('Classifier Accuracy')\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","f2121269":"import pandas as pd\ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")","32c40757":"##  **FEATURE ENGINEERING**","19d62140":"## **DATA CLEANING**","4a976824":"## **Feature Exploration , Engineering and Cleaning**\n \n ** BELOW CODES ARE ONLY FOR DATA CLEANING , FEATURE EXPLORATION AND SELECTION OF ML ALGORITHM**","6ea948db":"**BELOW ARE THE CODE FOR DATA CLEANING AND SELECTING THE MACHINE LEARNING ALGORITHM. ALTHOUGH  FROM THE BLOG I REFFERED , SVM was the most preffered and high accuracy dataset for this project.**","fe69833b":"## **LEARNINGS**\n * A dataset should be clean for machine learning and data analysis - we can use pandas .fillna to fill null values or missing values should be replaced for analysis."}}