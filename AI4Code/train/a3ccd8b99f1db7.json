{"cell_type":{"33cf30a8":"code","2397cc85":"code","c17d0852":"code","a86db432":"code","ec4710fc":"code","a6caeec5":"code","f43ccbb2":"code","8c0b1b85":"code","19d5d1ba":"code","eab00f7b":"code","8e48fd2a":"code","ddf3a679":"code","bf7ce5f5":"code","9ac4965d":"code","0a14257f":"code","72ce54f7":"code","c70c59b7":"code","c6960a97":"code","89520665":"code","ead1531b":"code","23575643":"code","197ff920":"code","9bbd941f":"code","14e8cc58":"code","3675a015":"code","5e0e51bf":"code","65c33b37":"code","b215c87f":"code","a2b11d5a":"code","69096080":"code","e3f0926e":"markdown","730ce160":"markdown","88ed41a5":"markdown","0a2811bb":"markdown","d681356b":"markdown","67c17592":"markdown","711181c6":"markdown","20f8db18":"markdown","76a5fab7":"markdown","ce9ac986":"markdown","80a9f496":"markdown","fb46b7fb":"markdown","58090e00":"markdown","e6d709ea":"markdown","f0bdfd11":"markdown","65296fc1":"markdown","49614cc8":"markdown","29900f21":"markdown","7b2aed99":"markdown"},"source":{"33cf30a8":"DEBUG=False","2397cc85":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport json\nimport glob\nimport seaborn as sns\n\nfrom tqdm import tqdm","c17d0852":"replay_dir = \"..\/input\/santa-replay-s-shohei\/replay\/\"\nTEAMNAME = \"s_shohei\"","a86db432":"json_paths=[]\njson_info_paths=[]\nagent_set = set()\nfor cnt,path in enumerate(glob.glob(replay_dir+\"*\")):   \n    agent=int(path.split(\"_\")[0].split(\"\/\")[-1])  \n    agent_set.add(str(agent))\n    if path.count(\"info\")!=0:\n        json_info_paths.append(path)\n    else:\n        json_paths.append(path)\n        cnt+=1\n    if DEBUG and cnt>30:\n        break\nlen(json_paths)","ec4710fc":"json_paths[0:5]","a6caeec5":"json_open = open(json_paths[0], 'r')\njson_load = json.load(json_open)\njson_load[\"info\"][\"TeamNames\"]","f43ccbb2":"def get_df(json_load):\n    left_hit=[]\n    left_action=[]\n\n    right_hit=[]\n    right_action=[]\n\n    left_last_reward=0\n    right_last_reward=0\n\n    rows_list=[]\n    \n    if TEAMNAME in json_load[\"info\"][\"TeamNames\"][0]:\n        team=0\n    elif TEAMNAME in json_load[\"info\"][\"TeamNames\"][1]:\n        team=1\n    else:\n        return None\n\n    for step in range(1,2000):\n        d=dict()\n        left = json_load[\"steps\"][step][team]\n        right = json_load[\"steps\"][step][1-team]\n\n        d[\"left_hit\"] = left[\"reward\"] != left_last_reward\n        d[\"right_hit\"] = right[\"reward\"] != right_last_reward\n\n        d[\"left_action\"] = left[\"action\"]\n        d[\"right_action\"] = right[\"action\"]\n        rows_list.append(d)\n        \n        left_last_reward = left[\"reward\"] \n        right_last_reward = right[\"reward\"]    \n        \n    return pd.DataFrame(rows_list)","8c0b1b85":"X_df = get_df(json_load)\nX_df.shape","19d5d1ba":"X_df.head()","eab00f7b":"def topological_sort(all_node_list, inc_list, link):\n    s=[]\n    for node in all_node_list:\n        if inc_list[node]==0:\n            s.append(node)\n    \n    s.reverse()\n    res=defaultdict(int)\n    cnt=0\n    \n    while s:\n        node=s.pop()\n        res[node]=cnt\n        cnt+=1\n        \n        for nxt in link[node].keys():\n            if link[node][nxt]==0:\n                continue\n            inc_list[nxt] -= link[node][nxt]\n            if inc_list[nxt]==0:\n                s.append(nxt)\n    return res","8e48fd2a":"from collections import deque\n\ndef bfs(src_node,link):\n    visited=set()\n    \n    visited.add(src_node)\n    Q = deque()\n    Q.append(src_node)\n    \n    while Q:\n        now = Q.popleft()\n        for nxt in link[now].keys():\n            if nxt in visited or link[now][nxt]==0:\n                continue\n            visited.add(nxt)\n            Q.append(nxt)\n    return visited","ddf3a679":"# encode node [0,0,0] -> \"0_0_0\"\ndef node_num(node):\n    return str(int(node[0]))+\"_\" + str(int(round(node[1])))+\"_\"+str(int(node[2]))\n\ndef add_edge(src_node_num, node_list, link, r_link, inc_list):\n    src_node = node_num(node_list[src_node_num])\n    adding_dst_nodes = []\n    \n    for i in range(100):\n        dst_node = node_num(node_list[i])\n        \n        if link[dst_node][src_node] == 0:\n            # just add this\n            adding_dst_nodes.append(dst_node)\n        else:\n            # ignore this edge and \"cancel\" existing edge\n            link[dst_node][src_node] -= 1\n            r_link[src_node][dst_node] -= 1\n            inc_list[src_node] -= 1\n            \n            if link[dst_node][src_node] < 0:\n                raise BaseException(\"link must be >= 0\")\n            if r_link[src_node][dst_node] < 0:\n                raise BaseException(\"r_link must be >= 0\")\n            if inc_list[src_node] < 0:\n                raise BaseException(\"inc_list must be >= 0\")\n    \n    # check parents to prevent add loop\n    parents_node_set = bfs(src_node,r_link) # reverse bfs   \n    \n    # add new edge\n    for dst_node in adding_dst_nodes:\n        if dst_node in parents_node_set or src_node == dst_node:\n            # Topological sort requires DAG(directed acyclic graph), so don't add edge to its parent\n            continue \n            \n        inc_list[dst_node] += 1\n        link[src_node][dst_node] +=1\n        r_link[dst_node][src_node] += 1","bf7ce5f5":"from collections import defaultdict\nimport dill\n\ndef create_dict(json_path):\n    json_open = open(json_path, 'r')\n    json_load = json.load(json_open)\n\n    if len(json_load[\"steps\"]) != 2000: # invalid episode\n        return [],[]\n\n    X_df = get_df(json_load)\n\n    if X_df is None:\n        return [],[]\n    \n    link   = defaultdict(lambda:defaultdict(int)) # link like [2,2,0] -> [0,0,0]\n    r_link = defaultdict(lambda:defaultdict(int)) # reverse link\n    inc_list = defaultdict(int) # number of incoming edge for topological sort\n    \n    node_list = np.zeros((100,3)) # my_pull, my_hit, opp_pull\n\n    for i in range(X_df.shape[0]):\n        row = X_df.iloc[i]\n        \n        # update link\n        add_edge(row.left_action,node_list,link,r_link,inc_list)\n\n        # update state of node\n        # my pull node\n        my_pull, my_hit, opp_pull = node_list[row.left_action]\n        my_hit += row.left_hit\n        my_pull+=1\n        node_list[row.left_action]  = my_pull, my_hit, opp_pull\n\n        # opp pull node\n        my_pull, my_hit, opp_pull = node_list[row.right_action] \n        opp_pull+=1\n        node_list[row.right_action] = my_pull, my_hit, opp_pull\n\n    all_node_list = list(link.keys())\n    group = topological_sort(all_node_list, inc_list, link)\n    \n    return group,link","9ac4965d":"len(json_paths)","0a14257f":"%%time\nfrom joblib import Parallel, delayed\nsub = Parallel(n_jobs=-1, verbose=10)( [delayed(create_dict)(j) for j in json_paths[0:500]] )","72ce54f7":"# first replay has 2275 state\nlink = sub[0][1]\nlen(link.keys())","c70c59b7":"# node state(name) example\nlist(link.keys())[-10:]","c6960a97":"# dictionary\nd = sub[0][0]\nlen(d.keys())","89520665":"# let's see topological sort order.\n# lower order means better node, not normalized yet\nprint(d[\"0_0_0\"])\nprint(d[\"0_0_2\"])\nprint(d[\"2_2_0\"])\nprint(d[\"2_2_2\"])\nprint(d[\"23_6_11\"])","ead1531b":"node_set = set()\nfor i in range(len(sub)):\n    if len(sub[i][0])==0:\n        continue\n    node_list = sub[i][0].keys()\n    node_set = node_set | set(node_list)\n\nprio_dict = defaultdict(lambda: 1) # unknown node returns worst priority:1\nfor node in tqdm(node_set):\n    prio_tmp_lis = []\n    for i in range(len(sub)):\n        if len(sub[i][0])==0:\n            continue\n        if not node in sub[i][0].keys():\n            continue\n        dict_len = len(sub[i][0].keys())\n\n        p = sub[i][0][node]\/dict_len # normalize\n        if p!=0:\n            prio_tmp_lis.append(p)\n\n    if len(prio_tmp_lis) == 0:\n        prio_dict[node]=1\n    else:\n        prio_dict[node] = np.mean(prio_tmp_lis) # mean of all replay's priority","23575643":"!mkdir -p \/kaggle_simulations\/agent\nfname = \"priority_dict.dill\"\ndill.dump(prio_dict, open(fname,'wb'))\n!mv $fname \/kaggle_simulations\/agent\/","197ff920":"print(prio_dict[\"0_0_0\"]) # untouched node\nprint(prio_dict[\"1_1_0\"])\nprint(prio_dict[\"2_2_0\"]) # one of the best state\nprint(prio_dict[\"3_3_0\"])\nprint(prio_dict[\"4_4_0\"])\nprint()\nprint(prio_dict[\"23_6_11\"]) # bad state\nprint(prio_dict[\"100_100_100\"]) # returns 1 for unknown node","9bbd941f":"print(prio_dict[\"0_0_0\"]) # untouched node\nprint(prio_dict[\"0_0_1\"]) # avoid node opp_pull == 1\nprint(prio_dict[\"0_0_2\"]) # must try\nprint(prio_dict[\"0_0_3\"]) # must try\nprint(prio_dict[\"0_0_4\"])","14e8cc58":"print(prio_dict[\"23_6_11\"])\nprint(prio_dict[\"23_7_11\"])\nprint(prio_dict[\"23_8_11\"])\nprint(prio_dict[\"23_9_11\"])\nprint(prio_dict[\"23_10_11\"])","3675a015":"num1=50 # my_pull\nnum2=50 # my_hit\narr = [[0 for _ in range(num2)] for _ in range(num1)]\n\nfor i in range(num1): # my_pull\n    for j in range(num2): # my_hit\n        p = prio_dict[node_num([i,j,5])] # opp_pull==5\n        arr[i][j] = p\n\nplt.figure(figsize = (10,7))\nsns.heatmap(arr, annot=False,square=True,vmin=0,vmax=1,cmap=\"ocean\")\nplt.xlabel(\"my_hit\")\nplt.ylabel(\"my_pull\")","5e0e51bf":"num1=50 # my_pull\nnum2=50 # my_hit\narr = [[0 for _ in range(num2)] for _ in range(num1)]\n\nfor i in range(num1): # my_pull\n    for j in range(num2): # my_hit\n        p = prio_dict[node_num([i,j,30])] # opp_pull==30\n        arr[i][j] = p\n\nplt.figure(figsize = (10,7))\nsns.heatmap(arr, annot=False,square=True,vmin=0,vmax=1,cmap=\"ocean\")\nplt.xlabel(\"my_hit\")\nplt.ylabel(\"my_pull\")","65c33b37":"!pip install kaggle-environments --upgrade","b215c87f":"%%writefile agent3.py\n\n# https:\/\/www.kaggle.com\/iehnrtnc\/santa2020\n\nimport math, random\n\nhistory = {\n    \"turn\": 0,\n    \"cnts\": [0] * 100,\n    \"ocnts\": [0] * 100,\n    \"hits\": [0] * 100,\n    \"osteps\": [0] * 100,\n    \"la\": -1,\n}\n\ndef agent3(observation, configuration):\n    global history\n\n    N = 100\n    p = [0.5, 0.000138, 1.24]\n    step = observation[\"step\"]\n    if step == 0:\n        pass\n    else:\n        la = history[\"la\"]\n        ola = sum(observation['lastActions']) - la\n        history[\"osteps\"][ola] = step\n        if sum(history[\"hits\"]) < observation['reward']:\n            history[\"hits\"][la] += 1 \/ pow(0.97, history[\"cnts\"][la] + history[\"ocnts\"][la])\n        history[\"cnts\"][la] += 1\n        history[\"ocnts\"][ola] += 1\n\n    tau = p[0] \/ (step + 1) + p[1]\n    ea = [0] * N\n    hits = history[\"hits\"]\n    cnts = history[\"cnts\"]\n    ocnts = history[\"ocnts\"]\n    osteps = history[\"osteps\"]\n\n    tv = sorted([(-ocnts[i], osteps[i], i) for i in range(N)])\n    ot = [0] * N\n    for i in range(N):\n        ot[tv[i][2]] = 99 - i\n\n    for i in range(N):\n        if cnts[i] == 0:\n            if ocnts[i] > 1:\n                ea[i] = math.exp(min(500, ot[i] \/ 100 * pow(0.97, ocnts[i]) \/ tau))\n            else:\n                ea[i] = math.exp(min(500, 0.99 * pow(0.97, ocnts[i]) \/ tau))\n        else:\n            w = pow(cnts[i], p[2])\n            wo = ocnts[i]\n            if ocnts[i] < 2:\n                wo = 0\n            r = hits[i] \/ cnts[i]\n            ro = ot[i] \/ 100\n            ea[i] = math.exp(min(500, (r * w + ro * wo) \/ (w + wo) * pow(0.97, cnts[i] + ocnts[i]) \/ tau))\n\n    se = sum(ea)\n    r = random.random() * se\n    t = 0\n    la = 99\n    for i in range(N):\n        t += ea[i]\n        if t >= r:\n            la = i\n            break\n\n    history[\"la\"] = la\n    return la","a2b11d5a":"%%writefile \/kaggle_simulations\/agent\/main.py\n\nimport numpy as np\nimport dill\n\nfname = \"\/kaggle_simulations\/agent\/priority_dict.dill\"\nloaded_dict = dill.load(open(fname,'rb'))\n\ndef node_name(node):\n    return str(int(node[0]))+\"_\" + str(int(round(node[1])))+\"_\"+str(int(node[2]))\n\ndef get_priority(node):\n    node = node_name(node)\n    return loaded_dict[node]\n    \nnode_list = np.zeros((100,3)) # my_pull, my_hit, opp_pull\n\ntotal_reward = 0\nmy_last_act=0\n\ndef agent(observation, configuration):\n    global node_list,total_reward,my_last_act\n    \n    if observation.step == 0:\n        node_list = np.zeros((100,3))\n    else:\n        r = observation.reward - total_reward # 1 if my_last_act was hit\n        total_reward = observation.reward\n        opp_last_act = sum(observation['lastActions']) - my_last_act \n        \n        # update node's state\n        node_list[my_last_act][1] += r\n        node_list[my_last_act][0] += 1\n        \n        node_list[opp_last_act][2] += 1\n    \n    node_score_list=[0]*100\n    for i in range(100):\n        # actually no need to update all bandit.\n        # update my_last_act and opp_last_act is enough.\n        node_score_list[i] = get_priority(node_list[i])\n\n    my_last_act = int(np.argmin(node_score_list))\n\n    return my_last_act","69096080":"from kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\noutput = env.run([\"\/kaggle_simulations\/agent\/main.py\", \"agent3.py\"])\nprint('Left player: reward = %s, status = %s, info = %s' % (output[-1][0]['reward'], output[-1][0]['status'], output[-1][0]['info']))\nprint('Right player: reward = %s, status = %s, info = %s' % (output[-1][1]['reward'], output[-1][1]['status'], output[-1][1]['info']))\nenv.render(mode=\"ipython\", width=500, height=500)","e3f0926e":"## Build graph","730ce160":"## Functions","88ed41a5":"## Summary","0a2811bb":"Left agent pulled bandit 14 at first round, and got no reward.","d681356b":"Normalize its priority to fit [0-1]\n* dict[3,3,0] == 0\/9 = 0\n* dict[2,2,0] == 1\/9 = 0.111\n* ...\n* dict[1,0,1] == 9\/0 = 1","67c17592":"## evaluate","711181c6":"#### Average dict\nI built a graph and dictionary from one replay.  \nThen average all dictionary and get final priority_dict.  \n\nMy agent just lookup this dictionary for each bandits' state, and pick argmin.","20f8db18":"## Average all dictionary","76a5fab7":"## Code","ce9ac986":"## Check performance","80a9f496":"more my_hit -> lower priority (good node)  \nmore my_pull -> higher priority (bad node)  \nmore opp_pull -> higher priority  ","fb46b7fb":"## Visualize\nYou can see priority gradation.  \nIn this notebook, lower priority value means better node.","58090e00":"* 17th place\n* Copy top agent's behavior by graph theory.\n* My agent: https:\/\/www.kaggle.com\/iiyamaiiyama\/santa2020-s-shohei-lb11001200","e6d709ea":"![santa%20%281%29.png](attachment:santa%20%281%29.png)","f0bdfd11":"Sparring partner  \nhttps:\/\/www.kaggle.com\/iehnrtnc\/santa2020 ","65296fc1":"![santa%20%282%29.png](attachment:santa%20%282%29.png)","49614cc8":"#### Edge\nAs you see top LB agent's early game behavior, you immediatery notice that  \n[1,1,0] prioritize over [0,0,0] : if hit, pull it again  \n[0,0,0] prioritize over [1,0,0] : if missed, pull other untouched bandit  \n\nThis relationship can be represented as Edges.  \n[1,1,0] -> [0,0,0] -> [1,0,0]\n\nIf opponent pulled same bandit twice, we should try that bandit instead of trying new one.  \n[0,0,2] -> [0,0,0]\n\nWe don't want to pull bandit that opponent pulled only once, which is probably bad one.  \n[0,0,0] -> [0,0,1]\n\nthen we can build a priority graph like below.","29900f21":"#### Node\nEach bandit has its state.  \nNotation: [my pull, my_hit, oppnent_pull]  \n\ne.g.  \nbandit [5,3,2]: I already pulled it 5 times, rewarded 3 times, and opponent pulled it 2 times.  \n\nThere are 100 bandit with state [0,0,0] at first round of episode,  \nand my agent updates them throughout episode.","7b2aed99":"#### Topological sort\nNow we got graph structure of priority, then do topological sort on this graph.  \nAfter topological sort, we can see how good the bandit is.  \n[3,3,0] is the best bandit in this graph.  "}}