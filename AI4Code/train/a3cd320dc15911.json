{"cell_type":{"0aa41c5b":"code","8734127c":"code","bd3d490e":"code","059fd0e9":"code","bc69e9b1":"code","deda4711":"code","63b160bc":"code","220d4d30":"code","5df42f92":"code","32f325b1":"code","2bd8be6a":"code","2b1e9c89":"code","9b31c3b1":"code","4a6658f5":"code","56062de2":"code","046597d1":"code","a39301f6":"code","046ae923":"code","83218821":"code","386c2973":"code","af2239a5":"code","8e79c41b":"code","01a66ed1":"code","cd197025":"code","7e85779e":"code","c81a2f5c":"code","12090b42":"code","0041949b":"code","b3b5e52a":"code","22444ed6":"code","cafb7d0e":"code","68281bbc":"code","ed5021b0":"code","2596fc2d":"code","a5dbe314":"code","fe429c69":"code","0d0ef650":"code","e07e6d51":"code","4084c5a9":"code","88266bef":"code","0c87c1c9":"code","6438ca9a":"code","e6e42e1a":"code","286dc787":"code","64009308":"code","2dd25e0f":"code","196077f2":"code","212e8e7f":"code","8ae8be7b":"code","4bc9ea78":"code","a5695810":"code","b6f00645":"code","3a008057":"code","8501febb":"code","adee6aa8":"code","de99de51":"code","6f594130":"code","be46f243":"code","5446c97f":"code","e55ab69a":"code","4f9e0359":"code","399ccc93":"code","9ed46458":"code","deefe7b0":"code","86be5198":"code","bd157c62":"code","e2320be3":"code","c384ecf4":"code","d4eed6f6":"code","ffb7111f":"code","76e50e79":"code","e5886dea":"code","9f97b863":"code","7a866965":"code","492a47ca":"code","d37881d6":"code","cfd4f6d8":"code","26aa013a":"code","13410fd7":"code","4f0574ac":"code","bb0b17c2":"code","fbbfcf8f":"code","c95bd4b4":"code","1924547e":"code","83ae86ba":"code","117ee118":"code","6da198f8":"code","621df723":"code","d7609bb6":"code","6460d403":"code","c333a262":"code","c7457a21":"code","b430dd34":"code","4c03105e":"code","618b95e6":"code","5bec7686":"code","cba4ce59":"code","c2bb6dd5":"code","22e046db":"markdown","5629d1c0":"markdown","d6d06d94":"markdown","01d2ea5b":"markdown","fe36435e":"markdown","1198f0ee":"markdown","1d90d635":"markdown","6fa69fc2":"markdown","7b6c97b2":"markdown","4f6b11b5":"markdown","22acced1":"markdown","482ebd0c":"markdown","a07783cc":"markdown","3f2a698b":"markdown","5faf4054":"markdown","08bcf58b":"markdown","5f4147a9":"markdown","8c3c5a9d":"markdown","4d4f1228":"markdown","76b60305":"markdown","a1343d9c":"markdown","9c25c774":"markdown","d1e1e49e":"markdown","ccd9eb42":"markdown","177c411c":"markdown","d3326047":"markdown","64292281":"markdown","6b74b7f9":"markdown","63f1408d":"markdown","9a4f20ba":"markdown","1f31132c":"markdown","0acb4a53":"markdown"},"source":{"0aa41c5b":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd","8734127c":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\ncombine = [train_df, test_df]","bd3d490e":"print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n\ntrain_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncombine = [train_df, test_df]\n\n\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape","059fd0e9":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_df['Title'], train_df['Sex'])","bc69e9b1":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","deda4711":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_df.head()","63b160bc":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape","220d4d30":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n\ntrain_df.head()","5df42f92":"guess_ages = np.zeros((2,3))\nguess_ages","32f325b1":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df.head()","2bd8be6a":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","2b1e9c89":"for dataset in combine:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\ntrain_df.head()","9b31c3b1":"train_df = train_df.drop(['AgeBand'], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()","4a6658f5":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","56062de2":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n\ntrain_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","046597d1":"train_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ncombine = [train_df, test_df]\n\ntrain_df.head()","a39301f6":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)","046ae923":"freq_port = train_df.Embarked.dropna().mode()[0]\nfreq_port","83218821":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","386c2973":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","af2239a5":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\ntest_df.head()","8e79c41b":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","01a66ed1":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncombine = [train_df, test_df]\n    \ntrain_df.head(10)","cd197025":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier","7e85779e":"from sklearn.model_selection import train_test_split","c81a2f5c":"X_train, X_test, y_train, y_test = train_test_split(train_df.drop(\"Survived\", axis=1), train_df[\"Survived\"], test_size=0.3)","12090b42":"from sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","0041949b":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    '''\n    print the accuracy score, classification report and confusion matrix of classifier\n    '''\n    if train:\n        '''\n        training performance\n        '''\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        '''\n        test performance\n        '''\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))","b3b5e52a":"clf = DecisionTreeClassifier(random_state=42)\n\nclf.fit(X_train, y_train)\n\nprint_score(clf, X_train, y_train, X_test, y_test, train=True)\n\nprint_score(clf, X_train, y_train, X_test, y_test, train=False) # Test","22444ed6":"bag_clf = BaggingClassifier(base_estimator=clf, n_estimators=1000,\n                            bootstrap=True, n_jobs=-1,\n                            random_state=42)\n\nbag_clf.fit(X_train, y_train)\n\nprint_score(bag_clf, X_train, y_train, X_test, y_test, train=True)\n\nprint_score(bag_clf, X_train, y_train, X_test, y_test, train=False)","cafb7d0e":"bag_clf = BaggingClassifier(base_estimator=clf, n_estimators=1000,\n                            bootstrap=True, oob_score=True,\n                            n_jobs=-1, random_state=42)","68281bbc":"bag_clf.fit(X_train, y_train)","ed5021b0":"bag_clf.oob_score_","2596fc2d":"print_score(bag_clf, X_train, y_train, X_test, y_test, train=True)","a5dbe314":"print_score(bag_clf, X_train, y_train, X_test, y_test, train=False)","fe429c69":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","0d0ef650":"def print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    '''\n    print the accuracy score, classification report and confusion matrix of classifier\n    '''\n    if train:\n        '''\n        training performance\n        '''\n        print(\"Train Result:\\n\")\n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_train, clf.predict(X_train))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_train, clf.predict(X_train))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_train, clf.predict(X_train))))\n\n        res = cross_val_score(clf, X_train, y_train, cv=10, scoring='accuracy')\n        print(\"Average Accuracy: \\t {0:.4f}\".format(np.mean(res)))\n        print(\"Accuracy SD: \\t\\t {0:.4f}\".format(np.std(res)))\n        \n    elif train==False:\n        '''\n        test performance\n        '''\n        print(\"Test Result:\\n\")        \n        print(\"accuracy score: {0:.4f}\\n\".format(accuracy_score(y_test, clf.predict(X_test))))\n        print(\"Classification Report: \\n {}\\n\".format(classification_report(y_test, clf.predict(X_test))))\n        print(\"Confusion Matrix: \\n {}\\n\".format(confusion_matrix(y_test, clf.predict(X_test))))","e07e6d51":"rf_clf = RandomForestClassifier(random_state=42)","4084c5a9":"rf_clf.fit(X_train, y_train)","88266bef":"print_score(rf_clf, X_train, y_train, X_test, y_test, train=True)","0c87c1c9":"print_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","6438ca9a":"from sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import GridSearchCV","e6e42e1a":"rf_clf = RandomForestClassifier(random_state=42)","286dc787":"params_grid = {\"max_depth\": [3, None],\n               \"min_samples_split\": [2, 3, 10],\n               \"min_samples_leaf\": [1, 3, 10],\n               \"bootstrap\": [True, False],\n               \"criterion\": ['gini', 'entropy']}","64009308":"grid_search = GridSearchCV(rf_clf, params_grid,\n                           n_jobs=-1, cv=5,\n                           verbose=1, scoring='accuracy')","2dd25e0f":"grid_search.fit(X_train, y_train)","196077f2":"grid_search.best_score_","212e8e7f":"grid_search.best_estimator_.get_params()","8ae8be7b":"print_score(grid_search, X_train, y_train, X_test, y_test, train=True)","4bc9ea78":"print_score(grid_search, X_train, y_train, X_test, y_test, train=False)","a5695810":"from sklearn.ensemble import ExtraTreesClassifier","b6f00645":"xt_clf = ExtraTreesClassifier(random_state=42)\n\nxt_clf.fit(X_train, y_train)\n\nprint_score(xt_clf, X_train, y_train, X_test, y_test, train=True)\n\nprint_score(xt_clf, X_train, y_train, X_test, y_test, train=False)","3a008057":"Y_pred = xt_clf.predict(test_df.drop('PassengerId',axis=1))\n\nY_pred\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submissions_xt.csv', index=False)","8501febb":"from sklearn.ensemble import AdaBoostClassifier\n\nada_clf = AdaBoostClassifier()\n\nada_clf.fit(X_train, y_train)\n\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=True)\n\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=False)","adee6aa8":"Y_pred = ada_clf.predict(test_df.drop('PassengerId',axis=1))\n\nY_pred\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submissions_ada.csv', index=False)","de99de51":"from sklearn.ensemble import RandomForestClassifier","6f594130":"ada_clf = AdaBoostClassifier(RandomForestClassifier())\n\nada_clf.fit(X_train, y_train)\n\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=True)\n\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=False)\n\nada_clf = AdaBoostClassifier(base_estimator=RandomForestClassifier())\n\nada_clf.fit(X_train, y_train)\n\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=True)\n\nprint_score(ada_clf, X_train, y_train, X_test, y_test, train=False)","be46f243":"Y_pred = ada_clf.predict(test_df.drop('PassengerId',axis=1))\n\nY_pred\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submissions_ada_random.csv', index=False)","5446c97f":"from sklearn.ensemble import GradientBoostingClassifier","e55ab69a":"gbc_clf = GradientBoostingClassifier()\ngbc_clf.fit(X_train, y_train)","4f9e0359":"print_score(gbc_clf, X_train, y_train, X_test, y_test, train=True)","399ccc93":"print_score(gbc_clf, X_train, y_train, X_test, y_test, train=False) # Test","9ed46458":"Y_pred = gbc_clf.predict(test_df.drop('PassengerId',axis=1))\n\nY_pred\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submissions_gbc.csv', index=False)","deefe7b0":"import xgboost as xgb","86be5198":"xgb_clf = xgb.XGBClassifier(max_depth=5, n_estimators=10000, learning_rate=0.3,\n                            n_jobs=-1)","bd157c62":"xgb_clf.fit(X_train, y_train)","e2320be3":"print_score(xgb_clf, X_train, y_train, X_test, y_test, train=True)","c384ecf4":"print_score(xgb_clf, X_train, y_train, X_test, y_test, train=False)","d4eed6f6":"Y_pred = xgb_clf.predict(test_df.drop('PassengerId',axis=1))\n\nY_pred\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submissions_xgb.csv', index=False)","ffb7111f":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier()\ntree_clf.fit(X_train, y_train)\n\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","76e50e79":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train.ravel())\n\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","e5886dea":"en_en = pd.DataFrame()","9f97b863":"tree_clf.predict_proba(X_train)","7a866965":"en_en['tree_clf'] = pd.DataFrame(tree_clf.predict_proba(X_train))[1]\nen_en['rf_clf'] =  pd.DataFrame(rf_clf.predict_proba(X_train))[1]\ncol_name = en_en.columns\nen_en = pd.concat([en_en, pd.DataFrame(y_train).reset_index(drop=True)], axis=1)","492a47ca":"en_en.head()","d37881d6":"tmp = list(col_name)\ntmp.append('ind')\nen_en.columns = tmp","cfd4f6d8":"from sklearn.linear_model import LogisticRegression\n\nm_clf = LogisticRegression(fit_intercept=False)\n\nm_clf.fit(en_en[['tree_clf', 'rf_clf']], en_en['ind'])","26aa013a":"en_test = pd.DataFrame()","13410fd7":"en_test['tree_clf'] = pd.DataFrame(tree_clf.predict_proba(X_test))[1]\nen_test['rf_clf'] =  pd.DataFrame(rf_clf.predict_proba(X_test))[1]\ncol_name = en_en.columns\nen_test['combined'] = m_clf.predict(en_test[['tree_clf', 'rf_clf']])","4f0574ac":"col_name = en_test.columns\ntmp = list(col_name)\ntmp.append('ind')","bb0b17c2":"tmp","fbbfcf8f":"en_test = pd.concat([en_test, pd.DataFrame(y_test).reset_index(drop=True)], axis=1)","c95bd4b4":"en_test.columns = tmp","1924547e":"print(pd.crosstab(en_test['ind'], en_test['combined']))","83ae86ba":"print(round(accuracy_score(en_test['ind'], en_test['combined']), 4))","117ee118":"print(classification_report(en_test['ind'], en_test['combined']))","6da198f8":"#For self:\n#df.Attrition.value_counts() \/ df.Attrition.count()","621df723":"from sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.ensemble import BaggingClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier","d7609bb6":"pd.Series(list(y_train)).value_counts() \/ pd.Series(list(y_train)).count()","6460d403":"class_weight = {0:0.61, 1:0.38}","c333a262":"forest = RandomForestClassifier(class_weight=class_weight)","c7457a21":"ada = AdaBoostClassifier(base_estimator=forest, n_estimators=100,\n                         learning_rate=0.5, random_state=42)","b430dd34":"ada.fit(X_train, y_train.ravel())","4c03105e":"print_score(ada, X_train, y_train, X_test, y_test, train=True)\nprint_score(ada, X_train, y_train, X_test, y_test, train=False)","618b95e6":"bag_clf = BaggingClassifier(base_estimator=ada, n_estimators=50,\n                            max_samples=1.0, max_features=1.0, bootstrap=True,\n                            bootstrap_features=False, n_jobs=-1,\n                            random_state=42)","5bec7686":"bag_clf.fit(X_train, y_train.ravel())","cba4ce59":"print_score(bag_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(bag_clf, X_train, y_train, X_test, y_test, train=False)","c2bb6dd5":"Y_pred = bag_clf.predict(test_df.drop('PassengerId',axis=1))\n\nY_pred\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submissions_bag_last.csv', index=False)","22e046db":"## AdaBoost with Random Forest","5629d1c0":"### **B**ootstrap **Agg**regat**ing** or [Bagging](https:\/\/en.wikipedia.org\/wiki\/Bootstrap_aggregating)\n* [Scikit- Learn Reference](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#bagging)\n* Bootstrap sampling: Sampling with replacement\n* Combine by averaging the output (regression)\n* Combine by voting (classification)\n* Can be applied to many classifiers which includes ANN, CART, etc.","d6d06d94":"## Ensemble Methods:\n\n### **B**ootstrap **Agg**regat**ing** or [Bagging](https:\/\/en.wikipedia.org\/wiki\/Bootstrap_aggregating)\n* [Scikit- Learn Reference](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#bagging)\n* Bootstrap sampling: Sampling with replacement\n* Combine by averaging the output (regression)\n* Combine by voting (classification)\n* Can be applied to many classifiers which includes ANN, CART, etc.\n\n### [Pasting](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html)\n* Sampling without replacement\n\n### [Boosting](https:\/\/en.wikipedia.org\/wiki\/Boosting_(machine_learning)\n* Train weak classifiers \n* Add them to a final strong classifier by weighting. Weighting by accuracy (typically)\n* Once added, the data are reweighted\n  * Misclassified samples gain weight \n  * Correctly classified samples lose weight (Exception: Boost by majority and BrownBoost - decrease the weight of repeatedly misclassified examples). \n  * Algo are forced to learn more from misclassified samples\n  \n    \n### [Stacking](http:\/\/blog.kaggle.com\/2016\/12\/27\/a-kagglers-guide-to-model-stacking-in-practice\/)\n* Also known as Stacked generalization\n* [From Kaggle:](http:\/\/blog.kaggle.com\/2016\/12\/27\/a-kagglers-guide-to-model-stacking-in-practice\/) Combine information from multiple predictive models to generate a new model. Often times the stacked model (also called 2nd-level model) will outperform each of the individual models due its smoothing nature and ability to highlight each base model where it performs best and discredit each base model where it performs poorly. For this reason, stacking is most effective when the base models are significantly different. \n* Training a learning algorithm to combine the predictions of several other learning algorithms. \n  * Step 1: Train learning algo\n  * Step 2: Combiner algo is trained using algo predictions from step 1.","01d2ea5b":"The bagging model has an average accuracy of 0.767","fe36435e":"# Ensemble Learning Methods Introduction using the Titanic dataset","1198f0ee":"## Grid Search","1d90d635":"# XGBoost (Extreme Gradient Boosting)\n\n[Documentation](http:\/\/xgboost.readthedocs.io\/en\/latest\/)\n\n[tqchen github](https:\/\/github.com\/tqchen\/xgboost\/tree\/master\/demo\/guide-python)\n\n[dmlc github](https:\/\/github.com\/dmlc\/xgboost)\n\n* \u201cGradient Boosting\u201d is proposed in the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. \n* XGBoost is based on this original model. \n\n* Supervised Learning\n\n## Objective Function : Training Loss + Regularization\n\n$$Obj(\u0398)=L(\u03b8)+\u03a9(\u0398)$$\n\n* $L$ is the training loss function, and \n* $\u03a9$ is the regularization term. \n\n### Training Loss\n\nThe training loss measures how predictive our model is on training data.\n\nExample 1, Mean Squared Error for Linear Regression:\n\n$$L(\u03b8)= \\sum_i(y_i-\\hat{y}_i)^2$$\n\nExample 2, Logistic Loss for Logistic Regression:\n\n$$ L(\u03b8) = \\sum_i \\large[ y_i ln(1 + e^{-\\hat{y}_i}) + (1-y_i) ln(1 + e^{\\hat{y}_i}) \\large] $$\n\n### Regularization Term\n\nThe regularization term controls the complexity of the model, which helps us to avoid overfitting.","6fa69fc2":"So our decision tree has an accuracy of 0.76","7b6c97b2":"# Boosting (Hypothesis Boosting)\n\n* Combine several weak learners into a strong learner. \n\n* Train predictors sequentially","4f6b11b5":"## Model 1 : Decision Trees","22acced1":"Setting oob True also generated same score.","482ebd0c":"# Data processing","a07783cc":"### Other Ensemble Methods:\n\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning)\n* Bayes optimal classifier\n  * An ensemble of all the hypotheses in the hypothesis space. \n  * Each hypothesis is given a vote proportional to the likelihood that the training dataset would be sampled from a system if that hypothesis were true. \n  * To facilitate training data of finite size, the vote of each hypothesis is also multiplied by the prior probability of that hypothesis. \n* Bayesian parameter averaging\n  * an ensemble technique that seeks to approximate the Bayes Optimal Classifier by sampling hypotheses from the hypothesis space, and combining them using Bayes' law.\n  * Unlike the Bayes optimal classifier, Bayesian model averaging (BMA) can be practically implemented. \n  * Hypotheses are typically sampled using a Monte Carlo sampling technique such as MCMC. \n* Bayesian model combination\n  * Instead of sampling each model in the ensemble individually, it samples from the space of possible ensembles (with model weightings drawn randomly from a Dirichlet distribution having uniform parameters). \n  * This modification overcomes the tendency of BMA to converge toward giving all of the weight to a single model. \n  * Although BMC is somewhat more computationally expensive than BMA, it tends to yield dramatically better results. The results from BMC have been shown to be better on average (with statistical significance) than BMA, and bagging.\n* Bucket of models\n  * An ensemble technique in which a model selection algorithm is used to choose the best model for each problem. \n  * When tested with only one problem, a bucket of models can produce no better results than the best model in the set, but when evaluated across many problems, it will typically produce much better results, on average, than any model in the set.\n\n\nR released\n* BMS (an acronym for Bayesian Model Selection) package\n* BAS (an acronym for Bayesian Adaptive Sampling) package\n* BMA package","3f2a698b":"# The best model is XGB in these runs","5faf4054":"### Fit model","08bcf58b":"![](http:\/\/)## Bagging (oob_score=False)","5f4147a9":"## Bagging (oob_score=True)\n\nUse out-of-bag samples to estimate the generalization accuracy","8c3c5a9d":"**Step 1. **\n\n  $$Y = F(x) + \\epsilon$$\n\n**Step 2. **\n\n  $$\\epsilon = G(x) + \\epsilon_2$$\n\n  Substituting (2) into (1), we get:\n  \n  $$Y = F(x) + G(x) + \\epsilon_2$$\n    \n**Step 3. **\n\n  $$\\epsilon_2 = H(x)  + \\epsilon_3$$\n\nNow:\n  \n  $$Y = F(x) + G(x) + H(x)  + \\epsilon_3$$\n  \nFinally, by adding weighting  \n  \n  $$Y = \\alpha F(x) + \\beta G(x) + \\gamma H(x)  + \\epsilon_4$$\n\nGradient boosting involves three elements:\n\n* **Loss function to be optimized**: Loss function depends on the type of problem being solved. In the case of regression problems, mean squared error is used, and in classification problems, logarithmic loss will be used. In boosting, at each stage, unexplained loss from prior iterations will be optimized rather than starting from scratch.\n\n* **Weak learner to make predictions**: Decision trees are used as a weak learner in gradient boosting.\n\n* **Additive model to add weak learners to minimize the loss function**: Trees are added one at a time and existing trees in the model are not changed. The gradient descent procedure is used to minimize the loss when adding trees.","4d4f1228":"# Ensemble of ensembles - model stacking\n\n* **Ensemble with different types of classifiers**: \n  * Different types of classifiers (E.g., logistic regression, decision trees, random forest, etc.) are fitted on the same training data\n  * Results are combined based on either \n    * majority voting (classification) or \n    * average (regression)\n  \n\n* **Ensemble with a single type of classifier**: \n  * Bootstrap samples are drawn from training data \n  * With each bootstrap sample, model (E.g., Individual model may be decision trees, random forest, etc.) will be fitted \n  * All the results are combined to create an ensemble. \n  * Suitabe for highly flexible models that is prone to overfitting \/ high variance. \n\n***\n\n## Combining Method\n\n* **Majority voting or average**: \n  * Classification: Largest number of votes (mode) \n  * Regression problems: Average (mean).\n  \n  \n* **Method of application of meta-classifiers on outcomes**: \n  * Binary outcomes: 0 \/ 1 from individual classifiers\n  * Meta-classifier is applied on top of the individual classifiers. \n  \n  \n* **Method of application of meta-classifiers on probabilities**: \n  * Probabilities are obtained from individual classifiers. \n  * Applying meta-classifier","76b60305":"# Using Single Classifier","a1343d9c":"# Lets start with the examples","9c25c774":"# That is all for bagging. Now moving onto boosting","d1e1e49e":"# AdaBoost \/ Adaptive Boosting\n\n[Robert Schapire](http:\/\/rob.schapire.net\/papers\/explaining-adaboost.pdf)\n\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/AdaBoost)\n\n[Chris McCormick](http:\/\/mccormickml.com\/2013\/12\/13\/adaboost-tutorial\/)\n\n[Scikit Learn AdaBoost](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#adaboost)\n\n1995\n\nAs above for Boosting:\n* Similar to human learning, the algo learns from past mistakes by focusing more on difficult problems it did not get right in prior learning. \n* In machine learning speak, it pays more attention to training instances that previously underfitted.\n\nSource: Scikit-Learn:\n\n* Fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. \n* The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction.\n* The data modifications at each so-called boosting iteration consist of applying weights $w_1, w_2, \u2026, w_N$ to each of the training samples. \n* Initially, those weights are all set to $w_i = 1\/N$, so that the first step simply trains a weak learner on the original data. \n* For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. \n* At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. \n* As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence.","ccd9eb42":"The accuracy came as .768 but we can tune the parameters using grid search.","177c411c":"## Decision Tree","d3326047":"**Note: Ensemble methods**\n\n* Work best with indepedent predictors\n\n* Best to utilise different algorithms","64292281":"# Meta Classifier","6b74b7f9":"Works for both regression and classification\n\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Gradient_boosting)\n\n* Sequentially adding predictors\n* Each one correcting its predecessor\n* Fit new predictor to the residual errors\n\nCompare this to AdaBoost: \n* Alter instance weights at every iteration","63f1408d":"# Extra-Trees (Extremely Randomized Trees) Ensemble\n\n[scikit-learn](http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#bagging)\n\n* Random Forest is build upon Decision Tree\n* Decision Tree node splitting is based on gini or entropy or some other algorithms\n* Extra-Trees make use of random thresholds for each feature unlike Decision Tree","9a4f20ba":"## Model 2: Random Forest","1f31132c":"# Random Forest\n\n[paper](http:\/\/ect.bell-labs.com\/who\/tkh\/publications\/papers\/odt.pdf)\n\n* Ensemble of Decision Trees\n\n* Training via the bagging method (Repeated sampling with replacement)\n  * Bagging: Sample from samples\n  * RF: Sample from predictors. $m=sqrt(p)$ for classification and $m=p\/3$ for regression problems.\n\n* Utilise uncorrelated trees\n\nRandom Forest\n* Sample both observations and features of training data\n\nBagging\n* Samples only observations at random\n* Decision Tree select best feature when splitting a node","0acb4a53":"# Bagging Machine Learning Algorithm"}}