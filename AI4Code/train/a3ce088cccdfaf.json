{"cell_type":{"f4536134":"code","000979e5":"code","5e7531a3":"code","89902cac":"code","d2c9c871":"code","99e659ac":"code","5aceb5a7":"code","22d4b39d":"code","0b3949e7":"code","de255854":"code","d936ceff":"code","40fde485":"code","c26b5714":"markdown","691154fb":"markdown","53009ea2":"markdown","cf664dc7":"markdown","7976e044":"markdown","bdd9138c":"markdown","33aedeef":"markdown","e2cd62d9":"markdown","b45a156b":"markdown"},"source":{"f4536134":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport collections\nfrom collections import Counter \nfrom typing import List, Dict, Any, Generator\nfrom collections import OrderedDict\nimport gensim.downloader as api\nfrom IPython.display import Markdown, display\nfrom rake_nltk import Rake\nfrom collections import Counter\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nimport en_core_web_md\nword_vectors = api.load(\"glove-wiki-gigaword-100\")\nnlp = en_core_web_md.load()\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\n#for dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","000979e5":"def get_keyword(task:str):\n    # Use Rake from nltk to extract keyword from task question\n    stops_words=stopwords.words('english')+['virus', 'know', 'known']\n    r = Rake(stopwords=stops_words, min_length=2) # Uses stopwords for english from NLTK, and all puntuation characters.\n    r.extract_keywords_from_text(task)\n    return r.get_ranked_phrases() # To get keyword phrases ranked highest to lowest.\n    \n    \ndef get_keyword2(task:str):\n    # Use spacy model to to extract keyword from task question\n    result = []\n    pos_tag = ['PROPN', 'NOUN'] \n    nlp.vocab[\"virus\"].is_stop = True\n    nlp.vocab[\"disease\"].is_stop = True\n    doc = nlp(task.lower()) \n    for token in doc:\n        if(token.text in nlp.Defaults.stop_words or token.text in punctuation or token.is_stop):\n            continue\n        if(token.pos_ in pos_tag):\n            result.append(token.text)\n                \n    return result     ","5e7531a3":"task='what we know about Range of incubation periods for the disease in humans (and how this varies across age and health status) and how long individuals are contagious, even after recovery.'\nprint('key sentences extraction using rake_nltk: ')\nprint(get_keyword(task))\n\nprint('\\nkeywords extraction using spacy: ')\nprint(get_keyword2(task))","89902cac":"def get_file_paths(data_path: str) -> List[str]:\n    \"\"\" Return all JSON file from a folder \"\"\"\n    file_paths = [os.path.join(\n        data_path, file_name) for file_name in os.listdir(\n            data_path) if file_name.endswith(\".json\")]\n    print(f\"Found {len(file_paths)} files.\")\n    return file_paths\n\ndef read_file(file_path: str) -> Dict[str, Any]:\n    \"\"\" Open JSON file and return dict() data \"\"\"\n    print(f\"Reading file {file_path}.\")\n    with open(file_path, \"r\") as handler:\n        json_data = json.loads(handler.read(), object_pairs_hook=OrderedDict)\n    return json_data","d2c9c871":"def get_paths(json_data: Dict[str, Any]) -> List[List[str]]:\n    \"\"\" Return all paths defined by JSON keys \"\"\"\n    paths = []\n    if isinstance(json_data, collections.MutableMapping):\n        for k, v in json_data.items():\n            paths.append([k])\n            paths += [[k] + x for x in get_paths(v)]\n    elif isinstance(json_data, collections.Sequence) and not isinstance(json_data, str):\n        for i, v in enumerate(json_data):\n            paths.append([i])\n            paths += [[i] + x for x in get_paths(v)]\n    return paths\n\ndef extract_section_text_from_paths(paths: List[List[str]], section: str) -> Generator[List[str], None, None]:\n    \"\"\" Yield paths of abstract, body_text, ref_entries or back_matter \"\"\"\n    section_paths = (path for path in paths if path[0] == section and path[-1] == \"text\")\n    return section_paths\n\ndef extract_text_from_path(paths: List[str], data: Dict[str, Any]) -> Generator[str, None, None]:\n    \"\"\" Use paths (list of keys to follow) to yield texts from JSON \"\"\"\n    for path in paths:\n        node_data = data\n        for key in path:\n            node_data = node_data[key]\n        if len(node_data) > 20:\n            yield(node_data)","99e659ac":"def preprocess_paragraphs(dict_of_paragraphs: Dict[str, Generator[str, None, None]]):\n\n    print(\"TEXT PRE-PROCESSINGTO BE IMPLEMENTED\")\n    abstract=[]\n    body=[]\n    legends=[]\n    #print(dict_of_paragraphs)\n    for k, l in dict_of_paragraphs.items():  \n        #print(f\"== {k} ==\")  # Abstract, body or figures legend as keys\n        for s in l:  # Generatos of paragraphs as values\n            if k=='abstract':\n                abstract.append(s) \n            elif k=='body':\n                body.append(s)\n            else:\n                legends.append(s)\n                \n    #print(len(abstract))  # Sentences as str\n    #print(len(body))   \n    #print(len(legends))\n            \n    return [abstract, body, legends]","5aceb5a7":"data_path=['\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json',\n'\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pdf_json',\n'\/kaggle\/input\/CORD-19-research-challenge\/comm_use_subset\/comm_use_subset\/pmc_json',\n'\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pdf_json',\n'\/kaggle\/input\/CORD-19-research-challenge\/noncomm_use_subset\/noncomm_use_subset\/pmc_json']\nfiles_paths=[]\nfor fold in data_path:\n    files_paths= files_paths+ get_file_paths(fold) \n\nprint(len(files_paths))    ","22d4b39d":"    \nlist_file=[]\nlist_titles=[]\nlist_abstract=[]\nlist_body=[]\nlist_legends=[]\nfor file_path in files_paths:\n\n    # Read JSON and extract key paths from JSON root to leaves\n    json_file_content: Dict[str, Any] = read_file(file_path)\n    list_titles.append(json_file_content['metadata']['title'])     \n    paths: List[List[str]] = get_paths(json_data=json_file_content)\n\n    # Extract paths in JSON data leading to a text from different sections\n    abstracts_paths = extract_section_text_from_paths(paths=paths, section=\"abstract\")\n    body_paths = extract_section_text_from_paths(paths=paths, section=\"body_text\")\n    figures_legends_paths = extract_section_text_from_paths(paths=paths, section=\"ref_entries\")\n\n    # Get paragraphs (defined by the JSON structure) from these parts\n    paragraphs = {\n        \"abstract\": extract_text_from_path(paths=abstracts_paths, data=json_file_content),\n        \"body\": extract_text_from_path(paths=body_paths, data=json_file_content),\n        \"legends\": extract_text_from_path(paths=figures_legends_paths, data=json_file_content)}\n\n    # Preprocess them\n    preprocessed_paragraphs: Dict[str, List[str]] = preprocess_paragraphs(dict_of_paragraphs=paragraphs)\n    list_file.append(file_path.split('\/')[-1].split('.')[0])\n    list_abstract.append(' '.join([str(elem) for elem in preprocessed_paragraphs[0]]))\n    list_body.append(' '.join([str(elem) for elem in preprocessed_paragraphs[1]]))\n    list_legends.append(' '.join([str(elem) for elem in preprocessed_paragraphs[2]]))    ","0b3949e7":"paper_table = pd.DataFrame(list(zip(list_file, list_titles, list_abstract, list_body, list_legends)), \n               columns =['ID', 'title','abstract','body', 'legends'])\ndel(list_file)\ndel(list_titles)\ndel(list_abstract)\ndel(list_body)\ndel(list_legends)\npaper_table.head()","de255854":"def filterTheDict(dictObj, callback):\n    newDict = dict()\n    # Iterate over all the items in dictionary\n    for (key, value) in dictObj.items():\n        # Check if item satisfies the given condition then add to new dict\n        if callback((key, value)):\n            newDict[key] = value\n    return newDict\n\ndef filter_similar_paper(df, task):\n    simil_dict={}\n    for id in df['ID']:\n        #print(id)\n        sentence=df[df['ID']==id]['abstract'].values[0]\n        simil_dict[id]= word_vectors.wmdistance(sentence, task)\n    simil_dict=filterTheDict(simil_dict, lambda elem: elem[1] <2)\n        \n    return df[df['ID'].isin(list(simil_dict.keys())) ]","d936ceff":"def get_task_information(task:str, paper_table):\n    # function that display for each extract word  the part of the paper_id and \n    #the part of the abstract that contains that word\n    \n    paper_table= filter_similar_paper(paper_table, task)\n    print(paper_table.shape)\n    \n    #key_terms=get_keyword(task)+get_keyword2(task)\n    key_terms=get_keyword(task)\n    \n    index=0\n    for abstract in paper_table.iloc[:,3]:\n        term_in=False\n        for terms in key_terms:\n            #print(terms)\n            terms2=terms\n            if terms2 in abstract.lower() and (\"covid-19\" in abstract.lower() or \"coronavirus\" in abstract.lower()): \n                term_in=True\n        if term_in:\n            if len(paper_table.iloc[index,1])>0:\n                display(Markdown('<b>'+\"In the paper:\"+' ' + paper_table.iloc[index,1]+'<b>'))\n            else:\n                display(Markdown('<b>'+\"In the paper:\"+' ' + paper_table.iloc[index,0]+'<b>'))\n            \n            #display(Markdown(df.iloc[index,2]))        \n            #display(Markdown('<b>'+'In conclusion:'+'<b>'))\n            sentence3=\"<ul>\"\n            for sentence in abstract.split('. '):\n                sentence2=sentence.lower()\n                matched2=False\n                for terms in key_terms: \n                    terms2=terms\n                    if terms2 in sentence2: \n                        matched2=True\n                        sentence2=sentence2.replace(terms, '<b>'+terms+'<\/b>')\n                        #sentence2='<b>'+sentence2+'<b>'\n                if matched2: \n                    sentence3=sentence3+\"<li>\"+sentence2.replace(\"\\n\",\"\")+\"<\/li>\"\n            display(Markdown(sentence3+\"<\/ul>\"))       \n        \n        index+=1","40fde485":"get_task_information(\"what we know about  incubation periods\", paper_table)","c26b5714":"### Example of outputs:\n\n#### This example is taken from the final execution of the notebook to give the reader an idea of what to expect as a final result\n\n##### Given question: \n> **What we know about incubation period ?**\n\n##### Outputs:\n\n**In the paper: Epidemiological characteristics of 1212 COVID-19 patients in Henan, China. medRxiv**\n    \nFirst excerpt :\n> the following findings are obtained: 1) covid-19 patients in henan show gender (55% vs 45%) and age (81% aged between 21 and 60) preferences, possible causes were explored; 2) statistical analysis on 483 patients reveals that the estimated average, mode and median **incubation periods** are 7.4, 4 and 7 days; incubation periods of 92% patients were no more than 14 days; 3) the epidemic of covid-19 in henan has undergone three stages and showed high correlations with the numbers of patients that recently return from wuhan; 4) network analysis on the aggregate outbreak phenomena of covid-19 revealed that 208 cases were clustering infected, and various people's hospital are the main force in treating patients\n\n**In the paper: Is a 14-day quarantine period optimal for effectively controlling coronavirus disease 2019 (COVID-19)?**\n\nFirst excerpt :\n> the full range of **incubation periods** of the covid-19 cases ranged from 0 to 33 days among 2015 cases.\n\nSecond excerpt :\n> this cohort contained 4 transmission generations, and **incubation periods** of the cases between generations were not significantly different, suggesting that the virus has not been rapidly adapted to human beings.\n\nThird excerpt :\n> interestingly, **incubation periods** of 233 cases (11.6%) were longer than the who-established quarantine period (14 days).\n","691154fb":"Here we choose the question **What is known about incubation Periods?** from the task 1","53009ea2":"## Creating Pandas Data Frame containing all previous loaded files\nWe create a Data frame including the paper_id, the title,  the absctract, the body and the legends for each paper","cf664dc7":"## Loading all files\nWe load all the abstract, body_text, and legends in all existing file in the five  five folder:\n1. biorxiv_medrxiv\/pdf_json\n2. comm_use_subset\/pdf_json\n3. comm_use_subset\/pmc_json\n4. noncomm_use_subset\/pdf_json\n5. noncomm_use_subset\/pmc_json\n\nIn total we have 25004 files","7976e044":"## Summary\n\nThis notebook is the result of the collaborative work of a group of engineers at Atos BDS France.\n\nIn this notebook we treat the different questions of the COVID-19 Open Research Dataset Challenge task: **What is known about transmission, incubation, and environmental stability?**\n\nOur objective was to **quickly identify sentences\/excerpts that are most likely to answer specific questions** we find in the several tasks and the articles they are coming from.","bdd9138c":"## Utils functions for Data Proccessing\nThe following python methods are written to help us to process, load, parse the json file in each folder of data.","33aedeef":"## Approach\nOur approach is divided into three parts:\n\n1. For a given question we start by selecting the most similar papers to this question. Those papers are likely those which can contain answers for the questions:\n\n    *In this step we will start by importing and preprocess the different documents then we will use **Glove** which is a nlp pretrained model on wikipedia corpus that will provide us we embedding for the abstract of each document and an other embedding for the question. Those two vecoriel resprenstation (embeddings) will be used to calculate the similarity between each document and the given question. then  we keep the most similar paper with the question (those paper which has the a score of similarity greater than a given threshold).*\n    \n2. Extract keywords and key sentences from the given question (using a nlp model).\n\n    *In this step we will extract keywords and key sentences form the question. For that we will use the Spacy en_core_web_md model which is an english multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl, to extract keywords. We will also use nltk_rake to extract key sentences form the question.*\n\n3. Output sentences\n\n    *This step will provide us with the output of the notebook. \n    For each keyword or key sentence and each similar paper we will return the sentences from the pape's body which contain the keyword\/key sentence if and only if the paper contains together this keyword and the word covid19 or coronavirus.*\n","e2cd62d9":"## Extract Information About Task question\nThe following methods will use the Glove pretrained model to filter papers that are more similar to the question and the match the body of the paper with the extracted key words and sentences in order to provide us with the part of the body that likely answer the question as expalined in the Approach part. ","b45a156b":"## Methods and example of keywords and key sentences extraction from the task question using spacy and nltk_rake Packages"}}