{"cell_type":{"2a693a7e":"code","abbac876":"code","d1c8f38b":"code","cf746095":"code","9fa39651":"code","b280764b":"code","4ec3467b":"code","e6971750":"code","6b8592e9":"code","6c8b709e":"code","d5ddb1b0":"code","8f5f41e2":"code","302371ee":"code","d238588b":"code","f840a40a":"code","c0a06594":"code","d0e62c41":"code","d0528c2b":"markdown","3358ebb6":"markdown","d2330d49":"markdown","31ba08af":"markdown","c0a2f63e":"markdown","f9aad9d2":"markdown","f38ad8f5":"markdown","18ddf47a":"markdown","fab1bea4":"markdown","6940f136":"markdown","63c7f899":"markdown","4fd0b8fe":"markdown","2c9b750f":"markdown","594ccf61":"markdown","e45c56d4":"markdown","a45dffe9":"markdown","fc6aaf90":"markdown","e76dbd1a":"markdown"},"source":{"2a693a7e":"import numpy as np\nimport datatable as dt\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb","abbac876":"def reduce_memory_usage(df, verbose=True):\n    numerics = [\"int8\", \"int16\", \"int32\", \"int64\", \"float16\", \"float32\", \"float64\",\"object\"]\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (\n                    c_min > np.finfo(np.float16).min\n                    and c_max < np.finfo(np.float16).max\n                ):\n                    df[col] = df[col].astype(np.float16)\n                elif (\n                    c_min > np.finfo(np.float32).min\n                    and c_max < np.finfo(np.float32).max\n                ):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","d1c8f38b":"def preprocess_data(df):\n    num_missing = df.isna().sum(axis=1)\n    # Fill missing values in training data with 0.0\n    numerical_transformer = SimpleImputer(strategy='constant', fill_value=0.0)\n    imputed_df1 = pd.DataFrame(numerical_transformer.fit_transform(df))\n    imputed_df1.columns = df.columns\n    \n    df1 = reduce_memory_usage(imputed_df1)\n    \n    df1['missing'] = num_missing\n    \n    return df1","cf746095":"train_df = preprocess_data(dt.fread('..\/input\/tabular-playground-series-sep-2021\/train.csv').to_pandas())\ntest_df = preprocess_data(dt.fread('..\/input\/tabular-playground-series-sep-2021\/test.csv').to_pandas())","9fa39651":"test_df.id = test_df.id.astype('int32') # Makes sure they are int and not float, which happens during imputing\n\n# X holds the training data\nX = train_df.drop(columns=['id','claim'])\n# y holds the target\/dependent variable, the 'claim' column\ny = train_df['claim'].astype('int16')\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1)","b280764b":"X_train.shape","4ec3467b":"rf_model = RandomForestClassifier(n_estimators=10, max_samples=100000, n_jobs=-1)","e6971750":"rf_model.fit(X_train, y_train)\n\nrf_score = roc_auc_score(y_valid, rf_model.predict_proba(X_valid)[:, 1])\n\nprint(\"On the validation set, rf_model had a score(auc) of:\", rf_score)","6b8592e9":"xgb_model = xgb.XGBClassifier(n_estimators=20000, learning_rate=0.13, tree_method='gpu_hist',\n                              max_depth=2, n_jobs=-1, gamma=0,\n                              reg_alpha=0, reg_lambda=1, subsample=0.9, colsample_bytree=0.9, \n                              max_bin=256, objective='binary:logistic', eval_metric='auc',\n                              max_delta_step=0, predictor='gpu_predictor', use_label_encoder=False,\n                              random_state=459)","6c8b709e":"xgb_model.fit(X_train, y_train, early_stopping_rounds=1000, eval_set=[(X_valid, y_valid)], verbose=2000)\n\nxgb_preds = xgb_model.predict_proba(X_valid)[:,1]\n\nxgb_score = roc_auc_score(y_valid, xgb_preds)\n\nprint(\"On the validation set, xgb_model had a score(auc) of:\", xgb_score)","d5ddb1b0":"plot_confusion_matrix(xgb_model, X_valid, y_valid)","8f5f41e2":"from catboost import CatBoostClassifier","302371ee":"cat_model = CatBoostClassifier(iterations=20000, depth=6, task_type=\"GPU\", \n                               thread_count=-1, loss_function='Logloss', \n                               eval_metric='AUC', od_type='Iter', \n                               early_stopping_rounds=1000, \n                               use_best_model=True, verbose=2000,\n                               random_state=459)","d238588b":"cat_model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n\ncat_preds = cat_model.predict_proba(X_valid)[:,1]\n\ncat_score = roc_auc_score(y_valid, cat_preds)\n\nprint(\"On the validation set, cat_model had a score(auc) of:\", cat_score)","f840a40a":"avg_preds = (xgb_preds + cat_preds) \/ 2.0","c0a06594":"combined_score = roc_auc_score(y_valid, avg_preds)\n\nprint(\"On the validation set, the combined model had a score(auc) of:\", combined_score)","d0e62c41":"drop_test = test_df.drop(columns=['id'])\n\ncombined_output = (cat_model.predict_proba(drop_test)[:,1] + xgb_model.predict_proba(drop_test)[:,1]) \/ 2.0\n\noutput = pd.DataFrame({\n    'id': test_df.id,\n    'claim': combined_output\n})\noutput.to_csv('submission.csv', index=False)","d0528c2b":"I am doing a very naive version of ensembling, but it does improve the AUC score slightly. This is because the XGBoost and CatBoost likely make different errors in their predictions. The average of their predictions should be a little closer to the ground truth, which is the likellihood of an insurance claim resulting from the information in the record.","3358ebb6":"## Random Forest Classifier - Scikit Learn","d2330d49":"The following is a function to reduce the memory of a pandas DataFrame by changing the datatypes of the columns. The new data type is set to the smallest datatype that can store the values in the column. [Here is a link to the source notebook](https:\/\/www.kaggle.com\/somang1418\/tuning-hyperparameters-under-10-minutes-lgbm#Bayesian-Optimization-with-LightGBM) and [heres a notebook with helpful tips including this one](https:\/\/www.kaggle.com\/bextuychiev\/how-to-work-w-million-row-datasets-like-a-pro).","31ba08af":"### Turn below cell into a code cell in order to see baseline accuracy of random forest\nUsing bootstrap (default) strategy for the RandomForestClassifier, meaning that it will use random subsets of 10000 samples to train each of the 100 individual decision tree estimators.","c0a2f63e":"## XGB Classifier - XGBoost","f9aad9d2":"Parameter search spaces used:\n\n{'learning_rate':[0.01, 0.1, 0.3], 'max_depth':[3,5,7,9], 'reg_alpha':[0.1,1e-4,1e-5], 'reg_lambda':[0.5,1.0,1.5], \n    'subsample':[0.6,0.7,0.9], 'colsample_bytree':[0.6,0.7,0.9], 'gamma':[0,1,2], 'max_bin':[256,320,512]}\n    \nNB: I did randomly change the values more to experiment and find different arrangements of parameters after seeing some other notebooks. Increasing the number of estimators and decreasing the max_depth of the trees increased the roc_auc score by less than a percent. Also Optuna makes hyperparameter tuning much easier, I would recommend using it instead.\n\nOnly two parameters were searched at a time with GridSearchCV. Turn below cell into code cell in order to optimize hyperparameters.","f38ad8f5":"## Preprocessing","18ddf47a":"We can check the shape of the dataset to get a better idea for what models may have the best prediction","fab1bea4":"CatBoost is another gradient boosting method like XGBoost","6940f136":"%%time\nxgb_params = {} # Fill params here for testing\n\nxgb_cv = GridSearchCV(xgb_model, xgb_params, n_jobs=-1, cv=2)\nxgb_cv.fit(imputed_X_train, y_train)\n\nresults = pd.DataFrame(xgb_cv.cv_results_)\nprint(results)","63c7f899":"**There are 119 columns in this dataset, 118 features not including the id column. Random Forests are effective classifiers for datsets with many rows. A scikit-learn implementation of a Random Forest will be used to set a baseline of performance.**\n\n**XGBoost will be used to train an even more accurate classifier and the parameters will be tuned with GridSearchCV.**","4fd0b8fe":"The confusion matrix allows you to see how the model is making its classification error","2c9b750f":"## Combining Methods","594ccf61":"# September Tabular Data Challenge - Binary Classification","e45c56d4":"In initial tests, the constant imputing strategy performed better than the mean strategy so we will use it here. The SimpleImputer will replace every missing value in the dataset with 0.0. The behavior would be the same if I used the same SimpleImputer, since it is only filling in 0's for missing values. The target column is not missing values, which we could check using the same logic as previous code cell.","a45dffe9":"## CatBoost Classifier - CatBoost","fc6aaf90":"## This is my first public notebook, please let me know of any errors I made in the explanation\/code. Thanks for reading! ","e76dbd1a":"The library datatable can read csv files faster than pandas, so we will use it here and then convert the datatable objects into pandas DataFrames."}}