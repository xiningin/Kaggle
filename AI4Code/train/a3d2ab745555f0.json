{"cell_type":{"a260efd4":"code","f2a10bd4":"code","bbb6c272":"code","f19981e4":"code","ff983d39":"code","7da14b71":"code","4f8a9cef":"code","8692647c":"code","f158197d":"code","811c43d3":"code","22e2b7bb":"code","69d8f977":"code","17e3119f":"code","805723f7":"code","d6093de3":"code","75a62a1f":"code","5692e503":"code","b378ab43":"code","ee944cbb":"code","13b58fc6":"code","23fd2ccb":"code","824fed2f":"code","cf3ed570":"code","6f961965":"code","baac4f04":"code","02fe7eb5":"code","643b9f46":"code","9a156f80":"code","8fede655":"code","2362f731":"code","e50efccb":"code","c446a668":"code","797135b5":"code","2fc85bac":"code","47c9ada0":"code","3a1eb4c2":"code","7143e1e3":"code","156009c9":"code","301befa1":"code","99712b73":"code","e0716fcf":"code","e63a3f26":"code","eb1ab7e5":"code","7d655807":"code","aa5798b9":"code","084cb734":"code","57874209":"code","3a1e00d8":"code","26ab2aaa":"code","024f1e50":"code","01607397":"code","a1ae5516":"code","73594e66":"code","a24fa45a":"code","81ac3a21":"code","96721be4":"markdown","7d42560c":"markdown","b8ec65e4":"markdown","f8f1871a":"markdown","f7652167":"markdown","df623770":"markdown","003c4552":"markdown","dc74a62f":"markdown","2ace1ef5":"markdown","38c0330c":"markdown","f93d6148":"markdown","7e3d6aa0":"markdown","d4d4d69e":"markdown","2b3724b0":"markdown","165c0e47":"markdown","df62a37a":"markdown","a3929e19":"markdown","90722e59":"markdown","76c3c621":"markdown"},"source":{"a260efd4":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            \n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n\n    def __repr__(self):\n         return f'{self.paper_id}: \\n{self.abstract[:400]}...\\n{self.body_text[:400]}...'\n\ndef doi_url(d): return f'http:\/\/{d}' if d.startswith('doi.org') else f'http:\/\/doi.org\/{d}'\n\ndef load_data_from_kaggle():\n    file_counter = 0\n    outer_loop = True\n\n    for dirname, _, filenames in os.walk(DATA_PATH):\n        for filename in filenames:\n            file_counter += 1\n            print(filename)\n            if file_counter > 10:\n                outer_loop = False\n                break\n        else:\n            continue\n        break\n            \n        \n\n    # Any results you write to the current directory are saved as output.\n\n    root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\n    all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\n    filename = f'{root_path}metadata.csv'\n    print(f'Metadata Filename: {filename}')\n    len(all_json)\n\n    # load in metadata\n    #\n    meta_data = pd.read_csv(filename, dtype={\n            'doi': str,\n            'pubmed_id': str,\n            'Microsoft Academic Paper ID': str\n            })\n\n    print(len(meta_data))\n    meta_data.head(2)\n\n    # working with relevent fields\n    #\n    meta_data = meta_data[['sha','title','doi','abstract','publish_time','authors','journal']]\n\n    meta_data.abstract = meta_data.abstract.fillna(meta_data.title)\n    print(f'record count: {len(meta_data)}')\n    meta_data.head(2)\n\n    # remove older years\n    meta_data['publish_time_year'] = meta_data['publish_time'].str[:4]\n    meta_data = meta_data[meta_data['publish_time_year'] >= '2012']\n    meta_data = meta_data.reset_index(drop=True) \n    \n    # removal of null and duplicate\n    #\n    duplicate_paper = ~(meta_data.title.isnull() | meta_data.abstract.isnull() | meta_data.doi.isnull()) & (meta_data.duplicated(subset=['title', 'abstract']))\n    meta_data = meta_data[~duplicate_paper].reset_index(drop=True)\n    len(meta_data)\n    meta_data.doi = meta_data.doi.fillna('').apply(doi_url)\n    first_row = FileReader(all_json[0])\n    print(first_row)\n\n    # Load the data into dataframe\n    #\n    dict_ = {'paper_id': [], 'body_text': []}\n    counter = 0\n    for idx, entry in enumerate(all_json):\n        if idx % (len(all_json) \/\/ 10) == 0:\n            print(f'Processing index: {idx} of {len(all_json)}')\n    \n        try:\n            content = FileReader(entry)\n            dict_['paper_id'].append(content.paper_id)\n            dict_['body_text'].append(content.body_text)\n            \n        except:\n            counter+= 1       \n\n    dataframe = pd.DataFrame(dict_, columns=['paper_id', 'body_text'])\n    print(f'Total records rejected due to wrong structure: {counter}')\n    dataframe.head()\n\n    # perform join between metadata and json files\n    #\n    left = meta_data[['sha','title','doi','abstract','publish_time','authors','journal']]\n    right = dataframe[['paper_id','body_text']]\n    dataset = pd.merge(left, right, left_on='sha', right_on='paper_id', how='left')\n    print(f'dataset ->: {len(dataframe)}')\n    print(f'left ->: {len(left)}')\n    print(f'right ->: {len(right)}')\n    print(f'final ->: {len(dataset)}')\n\n \n    gc.collect() \n    \n    return dataset","f2a10bd4":"import csv\nimport gc\nimport glob\nimport heapq\nimport json\nimport pickle\nimport os\nimport re\nimport string\nimport sys\n\n#\n# Libraries licensed under BSD\n#\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import HTML\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\nDATA_PATH = '\/kaggle\/input\/CORD-19-research-challenge'\nW2V_PATH = '\/kaggle\/input\/covid19-w2v\/'\n\ndf_orig_data = load_data_from_kaggle()","bbb6c272":"df_covid=df_orig_data\ndf_covid['body_text']=pd.Series(df_covid['body_text'], dtype=\"str\")\ndf_covid['abstract']=pd.Series(df_covid['abstract'], dtype=\"str\")\ndf_covid['title']=pd.Series(df_covid['title'], dtype=\"str\")","f19981e4":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import wordnet as wn\nfrom langdetect import detect\n# Utilize English only documents\ndef detect_lang(text):\n    try:\n        portion=text[0:400]\n        lang=detect(portion)\n    except Exception:\n        lang=None\n  \n    return lang \n    ","ff983d39":"\ndf_covid.drop_duplicates(['body_text'], inplace=True)\n","7da14b71":"df_covid.dropna(inplace=True)\n","4f8a9cef":"\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\ndf_covid.head()","8692647c":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nplt.hist(np.clip(df_covid['body_word_count'], 0, 50000), bins=100, density=True)\nplt.xlabel('Total words')\nplt.show()","f158197d":"df_covid['column'] = np.where(df_covid['publish_time'] < '2019-12-31', 'Before_Covid', 'After_Covid')","811c43d3":"df_covid['column'].value_counts().plot(kind='bar', figsize=(7, 6), rot=0)\nplt.ylabel(\"Count of Papers\", labelpad=14)\nplt.title(\"Count of papers before covid vs after covid\", y=1.02);","22e2b7bb":"df_covid['column_1'] = np.where(df_covid['abstract'].isnull() & df_covid['title'].notnull() , 'Abstract Not Present', 'Abstract Present')","69d8f977":"df_covid['column_1'].value_counts().plot(kind='bar', figsize=(7, 6), rot=0)\nplt.ylabel(\"Count of Papers\", labelpad=14)\nplt.title(\"Total count of abstracts if title is not Null\", y=1.02);","17e3119f":"import plotly.express as px\nvalue_counts = df_covid['journal'].value_counts()\nvalue_counts_df = pd.DataFrame(value_counts)\nvalue_counts_df['journal_name'] = value_counts_df.index\nvalue_counts_df['count'] = value_counts_df['journal']\nfig = px.bar(value_counts_df[0:10], \n             x=\"count\", \n             y=\"journal_name\",\n             title='Most Common Journals in CORD-19 Dataset',\n             orientation='h')\nfig.show()","805723f7":"def lower_case(input_str):\n    input_str = input_str.lower()\n    return input_str\n\ndf_covid['body_text'] = df_covid['body_text'].apply(lambda x: lower_case(x))","d6093de3":"def splitDataFrameIntoSmaller(df, chunkSize = 1000): \n    listOfDf = list()\n    numberChunks = len(df) \/\/ chunkSize + 1\n    for i in range(numberChunks):\n        listOfDf.append(df[i*chunkSize:(i+1)*chunkSize])\n    return listOfDf","75a62a1f":"covid19_synonyms = ['covid','covid-19','covid19','sarscov2',\n                    'coronavirus disease 19',\n                    'sars cov 2', # Note that search function replaces '-' with ' '\n                    '2019 ncov',\n                    '2019ncov',\n                    r'2019 n cov\\b',\n                    r'2019n cov\\b',\n                    'ncov 2019',\n                    r'\\bn cov 2019',\n                    'coronavirus 2019',\n                    'wuhan pneumonia',\n                    'wuhan virus',\n                    'wuhan coronavirus',\n                    r'coronavirus 2\\b']","5692e503":"text_split=splitDataFrameIntoSmaller(df_covid)","b378ab43":"for i in range(0,37) :\n    text_split[i]['flagCol'] = np.where(text_split[i].body_text.str.contains('|'.join(covid19_synonyms)),1,0)\n    text_split[i] = text_split[i][text_split[i]['flagCol']==1]","ee944cbb":"final_text=pd.concat(text_split, axis=0)","13b58fc6":"final_text=final_text.dropna(subset=['flagCol','body_text'])\nfinal_text.drop( final_text[ final_text['body_word_count'] < 500 ].index , inplace=True)","23fd2ccb":"final_text.describe(include='all')","824fed2f":"final_text.to_csv('filtered_data_1.csv',index=False)","cf3ed570":"from keras.preprocessing.text import Tokenizer\nfrom gensim.models.fasttext import FastText\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport nltk\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize\nfrom nltk import WordPunctTokenizer","6f961965":"import nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('stopwords')\nen_stop = set(nltk.corpus.stopwords.words('english'))\nimport spacy","baac4f04":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nnltk.download('wordnet') \nfrom nltk.stem.wordnet import WordNetLemmatizer\nimport string\n","02fe7eb5":"stop_words = set(stopwords.words('english'))\n\n\n# pos_words and extend words are some common words to be removed from body_text\n\npos_words = ['highest','among','either','seven','six','plus','strongest','worst','doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n,'greatest','every','better','per','across','throughout','except','fewer','trillion','fewest','latest','least','manifest','unlike','eight','since','toward','largest','despite','via','finest','besides','easiest','must','million','oldest','behind','outside','smaller','nest','longest','whatever','stronger','worse','two','another','billion','best','near','nine','around','nearest','wechat','lowest','smallest','along','higher','three','older','greater','neither','inside','newest','lower','may','although','though','earlier','upon','five','ca','larger','us','whether','beyond','onto','might','one','out','unless','four','whose','can','fastest','without','ecobooth','broadest','easier','within','like', 'could','biggest','bigger','would','thereby','yet','timely','thus','also','avoid','know','usually','time','year','go','welcome','even','date',\n             'used', 'following', 'go', 'instead', 'fundamentally', 'first', 'second', 'alone',\n               'everything', 'end', 'also', 'year', 'made', 'many', 'towards', 'truly', 'last','introduction', 'abstract', 'section', 'edition', 'chapter','and', 'the', 'is', 'any', 'to', 'by', 'of', 'on','or', 'with', 'which', 'was','be','we', 'are', 'so',\n                    'for', 'it', 'in', 'they', 'were', 'as','at','such', 'no', 'that', 'there', 'then', 'those',\n                    'not', 'all', 'this','their','our', 'between', 'have', 'than', 'has', 'but', 'why', 'only', 'into',\n                    'during', 'some', 'an', 'more', 'had', 'when', 'from', 'its', \"it's\", 'been', 'can', 'further',\n                    'above', 'before', 'these', 'who', 'under', 'over', 'each', 'because', 'them', 'where', 'both',\n                     'just', 'do', 'once', 'through', 'up', 'down', 'other', 'here', 'if', 'out', 'while', 'same',\n                    'after', 'did', 'being', 'about', 'how', 'few', 'most', 'off', 'should', 'until', 'will', 'now',\n                    'he', 'her', 'what', 'does', 'itself', 'against', 'below', 'themselves','having', 'his', 'am', 'whom',\n                    'she', 'nor', 'his', 'hers', 'too', 'own', 'ma', 'him', 'theirs', 'again', 'doing', 'ourselves',\n                     're', 'me', 'ours', 'ie', 'you', 'your', 'herself', 'my', 'et', 'al', 'may', 'due', 'de',\n                     'one','two', 'three', 'four', 'five','six','seven','eight','nine','ten', 'however',\n                     'i', 'ii', 'iii','iv','v', 'vii', 'viii', 'ix', 'x', 'xi', 'xii','xiii', 'xiv' \n               'often', 'called', 'new', 'date', 'fully', 'thus', 'new', 'include', 'http', \n               'www','doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et',\n               'al', 'author', 'figure','rights', 'reserved', 'permission', 'used', 'using', 'biorxiv',\n               'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI','-PRON-']\nextend_words =['used', 'following', 'go', 'instead', 'fundamentally', 'first', 'second', 'alone', 'everything', 'end', 'also', 'year', 'made', 'many', 'towards', 'truly', 'last', 'often', 'called', 'new', 'date', 'fully', 'thus', 'new', 'include', 'http', 'www','doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure','rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI','-PRON-']\n\npos_words.extend(extend_words)\npos_words\nstop_words = stop_words.union(pos_words)\n\ndef text_preprocess(text):\n    lemma = nltk.wordnet.WordNetLemmatizer()\n    \n    #Convert to lower\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text)\n\n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    \n    #Remove punctuation\n    table = str.maketrans('', '', string.punctuation)\n    text = [w.translate(table) for w in text.split()]\n    \n    lemmatized = []\n    #Lemmatize non-stop words and save\n    other_words = ['virus','study','viral','human','infection'] # common words to remove specific to these articles\n    for word in text:\n        if word not in stop_words:\n            x = lemma.lemmatize(word)\n            if x not in other_words:\n                lemmatized.append(x)\n   \n    result = \" \".join(lemmatized)\n    return result","643b9f46":"\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom scipy.spatial import distance","9a156f80":"final_text['language'] = final_text['abstract'].apply(lambda x: detect_lang(x))\nfinal_text = final_text[final_text['language'] == 'en'] ","8fede655":"final_text['body_text_processed'] = final_text['body_text'].apply(text_preprocess)","2362f731":"final_text['body_text_processed_1'] = final_text['body_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","e50efccb":"import gensim\ntokenized_doc = []\nfor d in final_text['body_text_processed']:\n    tokenized_doc.append(word_tokenize(d.lower()))","c446a668":"tagged_data = [gensim.models.doc2vec.TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]","797135b5":"\nmodel_docs = gensim.models.doc2vec.Doc2Vec(vector_size=200, min_count=4,sample=0.0008, epochs=50,dm=0,dbow_words=0)","2fc85bac":"# Build the Volabulary\nmodel_docs.build_vocab(tagged_data)\n# Train the Doc2Vec model\nmodel_docs.train(tagged_data, total_examples=model_docs.corpus_count, epochs=model_docs.epochs)","47c9ada0":"test_data = (\"artificial intelligence and deep learning treatment\")\ntest_data=word_tokenize(text_preprocess(test_data))\nivec = model_docs.infer_vector(test_data, steps=100,alpha=0.001)\nsimilar=model_docs.docvecs.most_similar(positive=[ivec], topn=10)","3a1eb4c2":"def Extract(lst): \n    return list(list(zip(*lst))[0])\ntop_articles_1=final_text.iloc[Extract(similar)]\ntop_articles_1.iloc[:,[12,0,2]]","7143e1e3":"\nfrom gensim.similarities.index import AnnoyIndexer\nannoy_index = AnnoyIndexer(model_docs, 500)\napproximate_neighbors = model_docs.docvecs.most_similar(positive=[model_docs.infer_vector(test_data)],topn=20\n, indexer=annoy_index)","156009c9":"#1) open code of summarization\ntop_articles=final_text.iloc[Extract(similar)].iloc[:2,2].str.cat(sep=', ')\nimport re\ntext_first=top_articles\ntext_first = re.sub(r'\\[[0-9]*\\]', ' ', text_first)\ntext_first = re.sub(r'\\s+', ' ', text_first)\nformatted_article_text = re.sub('[^a-zA-Z]', ' ', text_first)\nformatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\nsentence_list = nltk.sent_tokenize(text_first)\nstopwords = nltk.corpus.stopwords.words('english')\n\nword_frequencies = {}\nfor word in nltk.word_tokenize(formatted_article_text):\n    if word not in stopwords:\n        if word not in word_frequencies.keys():\n            word_frequencies[word] = 1\n        else:\n            word_frequencies[word] += 1\nmaximum_frequncy = max(word_frequencies.values())\n\nfor word in word_frequencies.keys():\n    word_frequencies[word] = (word_frequencies[word]\/maximum_frequncy)\nsentence_scores = {}\nfor sent in sentence_list:\n    for word in nltk.word_tokenize(sent.lower()):\n        if word in word_frequencies.keys():\n            if len(sent.split(' ')) < 30:\n                if sent not in sentence_scores.keys():\n                    sentence_scores[sent] = word_frequencies[word]\n                else:\n                    sentence_scores[sent] += word_frequencies[word]\nimport heapq\nsummary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n\nsummary = ' '.join(summary_sentences)\nprint(summary)","301befa1":"# 2) use of neural network technique\nfrom gensim.summarization.summarizer import summarize\nprint(summarize(top_articles,0.09))","99712b73":"task_ques_list = ['deep learning treatment',\\\n                  'ECMO death',\\\n                  'Resources long term care facilities.', \\\n                  'Mobilization of surge medical staff to address shortages in overwhelmed communities', \\\n                  'Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with\/without other organ failure \u2013 particularly for viral etiologies', \\\n                  'Outcome mechanical ventilation', \\\n                  'Knowledge of the frequency, manifestations, and course of extrapulmonary manifestations of COVID-19, including, but not limited to, possible cardiomyopathy and cardiac arrest.', \\\n                  'Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level.', \\\n                  'Approaches for encouraging and facilitating the production of elastomeric respirators, which can save thousands of N95 masks.', \\\n                  'Best telemedicine practices, barriers and faciitators, and specific actions to remove\/expand them within and across state boundaries.', \\\n                  'Guidance on the simple things people can do at home to take care of sick people and manage disease', \\\n                  'Oral medications that might potentially work', \\\n                  'Best practices and critical challenges and innovative solutions and technologies in hospital flow and organization, workforce protection, workforce allocation, community-based support resources, payment, and supply chain management to enhance capacity, efficiency, and outcome', \\\n                  'Efforts to define the natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials', \\\n                  'Efforts to develop a core clinical outcome set to maximize usability of data across a range of trials', \\\n                  'Efforts to determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)']\n","e0716fcf":"def Extract(lst): \n    return list(list(zip(*lst))[0])\ndef Extract_1(lst): \n    return list(list(zip(*lst))[1])","e63a3f26":"from gensim.summarization.summarizer import summarize\nfinal_docs = pd.DataFrame(columns=[ 'Similarity score','Question', 'Title','Summarize', 'Authors', 'Published_Date', 'Link'])\nfor i, info in enumerate(task_ques_list):\n    test_data=word_tokenize(text_preprocess(info))\n    ivec = model_docs.infer_vector(test_data, steps=100,alpha=0.001)\n    similar=model_docs.docvecs.most_similar(positive=[ivec], topn=5)\n    df =  final_text.iloc[Extract(similar)]\n    df['Body_text_summarize'] = df['body_text'].apply(summarize)\n    abstracts = df['abstract']\n    Summarization=df['Body_text_summarize']\n    titles = df['title']\n    similar_1=Extract(similar)\n    similar_2=Extract_1(similar)\n    for l in range(len(similar)):\n        final_docs = final_docs.append({ 'Similarity score': similar_2[l] ,'Summarize': Summarization.iloc[l], 'Question': info[:100], 'Title': titles.iloc[l], \\\n                                        'Authors': df['authors'].iloc[l], 'Published_Date': df['publish_time'].iloc[l], \\\n                                        'Link': df['doi'].iloc[l] },ignore_index=True)\n        ","eb1ab7e5":"final_docs","7d655807":"# Function to take an URL string and text string and generate a href for embedding\n#\ndef href(val1,val2):\n    return '<a href=\"{}\" target=\"_blank\">{}<\/a>'.format(val1,val2)\n#\n# Function to add column width for a particular column within the HTML string\n#\ndef setColWidth(html, col, width):\n    html = re.sub('<th>'+ col,  '<th width=\"'+ width+ '%\">'+col, html)\n    return html\n\n# Function to replace additional authors with 'et al.'\ndef etal(val):\n    if isinstance(val, float): \n        return_val = \" \"\n    else:\n        if ';' in val:\n            if ',' in val:\n                return_val = re.sub(',.*', ' et al.', val)\n            else:\n                return_val = re.sub(';.*', ' et al.', val)\n        else:\n            return_val = \" \"\n    return return_val\n\ndef setCaption(html, caption):\n    html = re.sub('mb-0\">', 'mb-0\"><caption>' + caption + '<\/caption>', html)\n    return html\n\ndef format_answer(val):\n    val = val.replace(\"\\n\",\"\")\n    val = val.replace(\"https:\",\"\")\n    val = val.replace(\"http:\",\"\")\n    val = val.replace(\"www.\",\"\")\n    return val\n\n  \n#\n# Function to generate HTML string\n#\ndef createHtmlTable(df,prefix):\n    # CSS string to justify text to the left\n    css_str ='<style>.dataframe thead tr:only-child th {text-align: left;}.dataframe thead th {text-align: left;}.dataframe tbody tr th:only-of-type {vertical-align: middle;}.dataframe tbody tr th {vertical-align: top;}.dataframe tbody tr td {text-align: left;}.dataframe caption {display: table-caption;text-align: left;font-size: 12px;color: black;font-weight: bold;}<\/style>'\n    \n    # Create a new Title column and combines the URL link to allow the user to open the source document in another tab\n    df['Title'] = df.apply(lambda row : href (row['Link'], row['Title']), axis=1)\n    df['Authors'] = df.apply(lambda row : etal (row['Authors']), axis=1)  \n    df['Summarize'] = df.apply(lambda row : format_answer (row['Summarize']), axis=1)\n    \n    # Generate HTML table string    \n    html_str = df[[ 'Title', 'Authors', 'Published_Date',  'Summarize' ]].to_html(render_links=True, index=False,  classes=\"table table-bordered table-striped mb-0\")\n    html_str = html_str + '<hr>'\n    \n    # Set table caption\n    html_str = setCaption (html_str, 'Top Published Documents')\n    \n    # Perform a few adjustments on the HTML string to make it even better\n    html_str = re.sub('&lt;', '<', html_str)\n    html_str = re.sub('&gt;', '>', html_str)\n    html_str = setColWidth(html_str, 'Title', '31')\n    html_str = setColWidth(html_str, 'Authors', '13')\n    html_str = setColWidth(html_str, 'Published_Date', '11')\n    html_str = setColWidth(html_str, 'Summarize', '45')  \n    \n    \n    # Return the final HTML table string for display\n    return css_str + prefix + html_str\n\n#\n#Function to generate HTML Q&A + table that can be displayed\n#\npd.set_option('mode.chained_assignment', None)\ndef create_html_per_question(df, ques_list):\n    i=0\n    for ques in task_ques_list:\n        i=i+1\n        new_section = 1\n        prefix = '<h4>#' + str(i) + ': ' + ques + '<\/h4>'\n        df = (final_docs.loc[final_docs['Question'] == ques])\n        for index, data in df.iterrows():\n            if data['Question'] == ques and new_section == 1:\n                answer_w2v = data['Question']\n                prefix = prefix + '<p>' + answer_w2v + '<\/p>'\n                new_section = 0\n                small_table = final_docs.loc[final_docs[\"Question\"] == ques]\n                display(HTML(createHtmlTable(small_table,prefix)))","aa5798b9":"create_html_per_question(final_docs, task_ques_list)","084cb734":"#running word2vec to create sentence vectors\nfrom gensim.models import Word2Vec\nmodel_ww = Word2Vec( min_count=5,\n                     size=300,\n                     sample=6e-5, \n                     alpha=0.03, \n                     min_alpha=0.0007, \n                     negative=20,)","57874209":"model_ww.build_vocab(tokenized_doc)","3a1e00d8":"model_ww.train(tokenized_doc, total_examples=model_ww.corpus_count,epochs=10)","26ab2aaa":"model_ww.wv.most_similar(positive=[\"risk\"])","024f1e50":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom gensim.summarization.summarizer import summarize\nfinal_docs_1 = pd.DataFrame(columns=[ 'Similarity score','Question', 'Title','Summarize', 'Authors', 'Published_Date', 'Link'])\nfor i, info in enumerate(task_ques_list):\n    test_data=word_tokenize(text_preprocess(info))\n    ivec = model_docs.infer_vector(test_data, steps=100,alpha=0.001)\n    similar=model_docs.docvecs.most_similar(positive=[ivec], topn=15)\n    df =  final_text.iloc[Extract(similar)]\n    df['Body_text_summarize'] = df['body_text'].apply(summarize)\n    abstracts = df['abstract']\n    Summarization=df['Body_text_summarize']\n    titles = df['title']\n    similar_1=Extract(similar)\n    similar_2=Extract_1(similar)\n    for l in range(len(similar)):\n        final_docs_1 = final_docs_1.append({ 'Similarity score': similar_2[l] ,'Summarize': Summarization.iloc[l], 'Question': info[:100], 'Title': titles.iloc[l], \\\n                                        'Authors': df['authors'].iloc[l], 'Published_Date': df['publish_time'].iloc[l], \\\n                                        'Link': df['doi'].iloc[l] },ignore_index=True)","01607397":"from collections import Counter\n\ncnt = Counter({k:v.count for k, v in model_ww.wv.vocab.items()})\n\nfrom itertools import chain\nimport spacy\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nfrom functools import partial\nfrom tqdm import tqdm_notebook\nall_sents = chain(*map(sent_tokenize,final_docs_1['Summarize']))","a1ae5516":"all_sents = list(all_sents)\nmy_nums = list(filter(lambda s: len(s.split()) >= 5, all_sents))","73594e66":"def sent2vector(sent):\n    words = word_tokenize(sent.lower())\n    \n    # Here we weight-average each word in sentence by 1\/log(count[word])\n    emb = [model_ww[w] for w in words if w in model_ww]\n    weights = [1.\/cnt[w] for w in words if w in model_ww]\n    \n    if len(emb) == 0:\n        return np.zeros(300, dtype=np.float32)\n    else:\n        return np.dot(weights, emb) \/ np.sum(weights)","a24fa45a":"sent_vectors = np.array(list(map(sent2vector, tqdm_notebook(my_nums))))\nfrom sklearn.neighbors import KDTree\nkdtree = KDTree(sent_vectors)\ndef search(sent, k=3):\n    sent_vec = sent2vector(sent)\n    closest_sent = kdtree.query(sent_vec[None], k)[1][0]\n    \n    return [all_sents[i] for i in closest_sent]\n","81ac3a21":"search(\"ecmo death\",9)","96721be4":"# using indexer to compare with cosine similarity results","7d42560c":"Tokenization","b8ec65e4":"# Using doc2vec on one of the sub task to test","f8f1871a":"* ## Removing Missing Values","f7652167":"# Descriptive Analysis","df623770":"# Using Body text to filter the articles that are related to COVID 19 only","003c4552":"For infer_vector the resuls were kind of better with more steps and decreasing the alpha rate from 0.01 to 0.025.","dc74a62f":"# **COVID analysis using Doc2Vec and Sent2Vec**\n#  *Created by a [TransUnion](https:\/\/www.transunion.com\/) data scientist that believes that information can be used to change our world for the better. #InformationForGood*","2ace1ef5":"## Clean Duplicates","38c0330c":"## Read the data from JSON files and Metadata","f93d6148":"# Creating HTML document for final report \nTook a reference from this Kaggle Kernel : https:\/\/www.kaggle.com\/emelaon\/ericsson-cord-19-challenge-task-8","7e3d6aa0":"# Method 2: Using word2vec to create sentence embeddings and finding top sentences for each task","d4d4d69e":"# Text pre processing","2b3724b0":"Select the documents that are written in English","165c0e47":"# Method 1 : Doc2Vec","df62a37a":"**Combining top 3 articles and applying two different methods of summarization**","a3929e19":"* Vector Size ranging from 100 to 1000 and for smaller data less than 100 more than enough.\n* Sample is very useful in yper tuning as that decreases the effect of high frequency words.( Tried changing it and range is 1e-3 to 1e-5)\n* If using DM then the epochs should be more.\n* DM or skip gram is generally used when there are more infrequent words.\n* Try to compare pure DBOW with hybrid DBOW (change dbow_words = 1)\n* Tried changing the default alpha rates but the results werent that great.","90722e59":"# **Creating final dataset by adding Summarized Body text for each sub question**","76c3c621":"**Final dataset is created by filtering articles that only has covid synonyms and have more than 500 words in the body text** "}}