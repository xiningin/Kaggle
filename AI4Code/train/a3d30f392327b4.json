{"cell_type":{"42603d43":"code","cf461d8b":"code","6ce5b77b":"code","d192915b":"code","0a06f890":"code","0fa53a83":"code","247181de":"code","e0c803b1":"code","8d8e03c8":"code","2298a421":"code","ddbfdd69":"code","71e3de67":"code","97437940":"code","eefa4e8d":"code","c73af29e":"code","249fc622":"code","4eabaf8e":"code","03cdbded":"code","c94a690d":"code","eddee01c":"markdown","383580ad":"markdown","fae80b67":"markdown","a40eb292":"markdown","3ff57034":"markdown","46bf76c4":"markdown","57d8cd36":"markdown","939dd9fa":"markdown"},"source":{"42603d43":"%config Completer.use_jedi = False","cf461d8b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow, imread\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold\nfrom sklearn.metrics import mean_squared_error, accuracy_score, roc_auc_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.impute import SimpleImputer\nimport scikitplot as skplt\n\nimport scipy.stats as stats\n\nimport lightgbm as lgb\nimport warnings\n\nimport optuna","6ce5b77b":"R_SEED = 37","d192915b":"submission_ex = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\ntrain_data_a = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_data_a  = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\ntrain_data_b = pd.read_csv('\/kaggle\/input\/0042-ann32ds-tps-sep-2021\/train_data_nn_features.csv')\ntest_data_b  = pd.read_csv('\/kaggle\/input\/0042-ann32ds-tps-sep-2021\/test_data_nn_features.csv')","0a06f890":"target_data = train_data_a[['claim']].copy()\ntrain_data_a.drop(['id', 'claim'], axis=1, inplace=True)\nsubmit_data = test_data_a[['id']].copy()\ntest_data_a.drop(['id'], axis=1, inplace=True)","0fa53a83":"all_data = pd.concat([train_data_a, test_data_a])\nall_data.reset_index(drop=True, inplace=True)","247181de":"all_n_missing = all_data.isna().sum(axis=1)\nall_std = all_data.std(axis=1)","e0c803b1":"all_data_normalized = StandardScaler().fit_transform(all_data)\nall_data = pd.DataFrame(all_data_normalized, columns=all_data.columns)","8d8e03c8":"##### all_data['n_missing'] = all_n_missing\n##### all_data['std'] = all_std\n\n# imputer = SimpleImputer(strategy='constant', fill_value=0) # add_indicator=True, \n\n# old_features = list(all_data.columns)\n# all_data = pd.DataFrame(imputer.fit_transform(all_data))\n# features = old_features # + ['ind_for_' + str(e) for e in imputer.indicator_.features_]\n# all_data.columns = features","2298a421":"train_data_a, test_data_a = all_data.iloc[:train_data_a.shape[0],:].copy(), all_data.iloc[train_data_a.shape[0]:,:].copy()\n\nn_missing = all_n_missing[:train_data_a.shape[0]]","ddbfdd69":"pca = PCA(n_components=5)\npca.fit(pd.concat([train_data_b, test_data_b]))\ntrain_data_c = pd.DataFrame(pca.transform(train_data_b), columns=['pca_b_f' + str(i) for i in range(5)])\ntest_data_c = pd.DataFrame(pca.transform(test_data_b), columns=['pca_b_f' + str(i) for i in range(5)])","71e3de67":"print(pca.explained_variance_ratio_)","97437940":"def plot_gmm(model, data, ax, c):\n    weights = model.weights_\n    means = model.means_\n    covars = model.covariances_\n\n    x = np.arange(np.min(data), np.max(data), (np.max(data) - np.min(data)) \/ 100)\n    for i in range(len(weights)):\n        ax.plot(x - 1, weights[i] * stats.norm.pdf(x,means[i],np.sqrt(covars[i])[0]), alpha = 0.7, linewidth = 3, color=c)","eefa4e8d":"fig = plt.figure(figsize = (10, 5))\nax = fig.gca()\n\ndata0 = np.expand_dims(n_missing[target_data['claim'] == 0], axis=1)\ngm = GaussianMixture(n_components = 1, n_init = 5)\ngm.fit(data0)\nplot_gmm(gm, data0, ax, 'red')\n#---\ndata1 = np.expand_dims(n_missing[target_data['claim'] == 1], axis=1)\ngm = GaussianMixture(n_components = 1, n_init = 5)\ngm.fit(data1)\nplot_gmm(gm, data1, ax, 'blue')\n\nax.set_title('NaN_values')\nax.legend(['probability for 0', 'probability for 1'])\n\nplt.show()","c73af29e":"train_data = pd.concat([train_data_a, train_data_b, train_data_c], axis=1)\ntest_data = pd.concat([test_data_a.reset_index(drop=True), test_data_b, test_data_c], axis=1)","249fc622":"del train_data_a, train_data_b, train_data_c, test_data_a, test_data_b, test_data_c, all_data","4eabaf8e":"params_1 = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'n_estimators': 20000,\n    'learning_rate': 0.005,\n    'num_leaves': 627, \n    'min_child_samples': 1952, \n    'feature_fraction': 0.4, \n    'bagging_fraction': 0.8,\n    'bagging_freq': 2,\n    'importance_type': 'gain'}\n\nparams_2 = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'n_estimators': 1000, \n    'learning_rate': 0.05, \n    'num_leaves': 89, \n    'min_child_samples': 48, \n    'feature_fraction': 0.35, \n    'bagging_fraction': 0.9, \n    'bagging_freq': 1}\n    \nmodel_1 = lgb.LGBMRegressor(**params_1,\n                          n_jobs=-1,\n                          random_state = R_SEED)\n\nmodel_2 = lgb.LGBMRegressor(**params_2,\n                          n_jobs=-1,\n                          random_state = R_SEED)","03cdbded":"kfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = R_SEED)\npred = []\nlgb_oof_1 = np.zeros(train_data.shape[0])\nlgb_oof_2 = np.zeros(train_data.shape[0])\n\nfor train_index, test_index in kfolds.split(X=train_data, y=n_missing):\n\n    X_train, X_val = train_data.iloc[train_index], train_data.iloc[test_index]\n    X_train_2, X_val_2 = np.expand_dims(n_missing[train_index], axis=1), np.expand_dims(n_missing[test_index], axis=1)\n    y_train, y_val = target_data.iloc[train_index], target_data.iloc[test_index]\n    \n    print(y_train.shape[0], y_train['claim'].sum())\n    \n    model_1.fit(\n        X_train, \n        np.ravel(y_train), \n        eval_metric = \"auc\", \n        eval_set = [(X_val, y_val)],\n        verbose = 100,\n        early_stopping_rounds = 500)\n    \n    oof_pred_1 = model_1.predict(X_val)\n    lgb_oof_1[test_index] = oof_pred_1\n        \n    _p = model_1.predict(test_data)\n    pred.append(_p)\n    \n    model_2.fit(\n        X_train_2, \n        np.ravel(y_train), \n        eval_metric = \"auc\", \n        eval_set = [(X_val_2, y_val)],\n        verbose = 100,\n        early_stopping_rounds = 200)\n    \n    oof_pred_2 = model_2.predict(X_val_2)\n    lgb_oof_2[test_index] = oof_pred_2\n    \nfinal_p = np.sum(pred, axis = 0) \/ len(pred)\n\nsubmit_data['claim'] = final_p\nsubmit_data.to_csv('submission.csv', index=False)","c94a690d":"p_1 = [(1-e, e) for e in lgb_oof_1]\np_2 = [(1-e, e) for e in lgb_oof_2]\n\nfig = plt.figure(figsize = (10, 10))\nax = fig.gca()\n\nskplt.metrics.plot_roc(target_data.claim.values, p_1, plot_micro=False, plot_macro=False, classes_to_plot=[1], ax=ax, cmap='Reds')\n# skplt.metrics.plot_roc(target_data.claim.values, p_1, plot_micro=False, plot_macro=False, classes_to_plot=[0], ax=ax, cmap='ocean')\nskplt.metrics.plot_roc(target_data.claim.values, p_2, plot_micro=False, plot_macro=False, classes_to_plot=[1], ax=ax, cmap='Blues')\nplt.show()","eddee01c":"#### Just experimenting\nMaybe some of pca components will be useful. PCA has been applied on ann dataset.","383580ad":"Curve is drawn with oof data","fae80b67":"It seems a bit unnatural :). The blue one is of number_of_missing \"dataset\".  \nleft side is a bit to spacious. It's sign that we have in upper side mixing of 1s and 0s. So, part closer to 0 is better than part closer to 1 (as far as classification is concerned)","a40eb292":"#### Run\nTwo models are trained, One on prepared dataset and one on number_of_missing array.  \nJust want to see how looks on graph.","3ff57034":"#### Additional ann dataset\nMy excursion with neural net ended with bad result. In try to get something out of it, I collected data from layer before the last. So, version1 of this dataset has 32 features and version2 has 16 features. Dataset is [public](https:\/\/www.kaggle.com\/ivankontic\/0042-ann32ds-tps-sep-2021), if someone want to try.","46bf76c4":"#### NaN","57d8cd36":"Little more freely drawn graph, but it should be right.  \nAs we can see, if there are NaN in row, probability is on the side of 1s. I'll try later something with this probabilities.","939dd9fa":"I decided to not do anything with missing values (for now). Let lgbm deal with it."}}