{"cell_type":{"fa7f2959":"code","9735db86":"code","5aa5eb8c":"code","835e9239":"code","28998979":"code","464bbcc7":"code","4de50b39":"code","cd243b26":"code","74e495d5":"code","f148a4fb":"code","791689c7":"code","c0232212":"code","08cfe6f8":"code","4025c7b1":"code","ecbb550b":"code","b54431a1":"code","6ee1b546":"code","ed71dfb1":"code","20334913":"code","b814b4f9":"code","10c80bb8":"code","7a1f4be2":"code","276ea388":"code","f0df8df5":"code","dab156e7":"code","62551cc1":"code","66bcf639":"code","158cd25f":"code","47a1fb73":"code","02a791c2":"code","902f2534":"code","baf1f1b4":"code","9b608c00":"code","d56053d0":"code","43abb14a":"code","a11c48fd":"code","ed75f6ee":"code","e46ff286":"code","5799e1bf":"code","de55091e":"code","86a52b72":"code","7f1c9bdb":"code","07e98573":"code","360af023":"code","4abae7e6":"code","73caaef7":"code","fb8c66bf":"code","ef1a58ab":"markdown","171a01e1":"markdown","0cd6ea45":"markdown","a8d1e2b8":"markdown","9b9fe41f":"markdown","5980ed02":"markdown","8786cee5":"markdown","d5aa9b46":"markdown","45437461":"markdown","3ada1c30":"markdown","6febe00e":"markdown","6bb55fe8":"markdown","bef8fde2":"markdown","79a0cf50":"markdown","6f7243d2":"markdown","a841af4f":"markdown","a394e9f1":"markdown","ed120ed6":"markdown","6e027bde":"markdown","f3309194":"markdown","50a1855d":"markdown","fe05c689":"markdown","3e759429":"markdown","8a30a284":"markdown","023e3297":"markdown","d9657ccf":"markdown","db5c84e3":"markdown","036117bf":"markdown","d1ad7433":"markdown"},"source":{"fa7f2959":"import numpy as np \nimport pandas as pd \nimport pandas_summary as ps\nfrom category_encoders import WOEEncoder","9735db86":"# Lgbm\nimport lightgbm as lgb\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom sklearn.linear_model import LogisticRegression\n\n# Hyper_opt\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nimport hyperopt.pyll\nfrom hyperopt.pyll import scope\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plots\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams\n\n# Others\nimport shap","5aa5eb8c":"folder = '..\/input\/titanic\/'\ntrain_df = pd.read_csv(folder + 'train.csv')\ntest_df = pd.read_csv(folder + 'test.csv')\nsub_df = pd.read_csv(folder + 'gender_submission.csv')","835e9239":"print('train')\nprint('All: ', train_df.shape)\nprint('test')\nprint('All: ', test_df.shape)\nprint('sub')\nprint('sub ', sub_df.shape)","28998979":"train_df.head()","464bbcc7":"test_df.head()","4de50b39":"sub_df.head()","cd243b26":"train_df['Sex'] = train_df['Sex'].apply(lambda x: 1 if str(x) == 'male' else 0)\ntest_df['Sex'] = test_df['Sex'].apply(lambda x: 1 if str(x) == 'male' else 0)","74e495d5":"train_df.drop('PassengerId', axis=1, inplace=True)\ntest_df.drop('PassengerId', axis=1, inplace=True)","f148a4fb":"# Check correlation","791689c7":"train_corr = train_df.corr()\n# plot the heatmap and annotation on it\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(train_corr, xticklabels=train_corr.columns, yticklabels=train_corr.columns, annot=True, ax=ax);","c0232212":"test_corr = test_df.corr()\n# plot the heatmap and annotation on it\nfig, ax = plt.subplots(figsize=(12,12))\nsns.heatmap(test_corr, xticklabels=test_corr.columns, yticklabels=test_corr.columns, annot=True, ax=ax);","08cfe6f8":"y = train_df['Survived']\ntrain_df.drop('Survived', axis=1, inplace=True)","4025c7b1":"y.value_counts()","ecbb550b":"y.value_counts(normalize=True)","b54431a1":"y.hist(bins=2);","6ee1b546":"dfs = ps.DataFrameSummary(train_df)\nprint('categoricals: ', dfs.categoricals.tolist())\nprint('numerics: ', dfs.numerics.tolist())\ndfs.summary()","ed71dfb1":"dfs = ps.DataFrameSummary(test_df)\nprint('categoricals: ', dfs.categoricals.tolist())\nprint('numerics: ', dfs.numerics.tolist())\ndfs.summary()","20334913":"train_df.hist(figsize=(25, 20));","b814b4f9":"sns.pairplot(data=train_df);","10c80bb8":"test_df.hist(figsize=(25, 20));","7a1f4be2":"sns.pairplot(data=test_df);","276ea388":"sns.barplot(data=train_df, x='Pclass', y=y);","f0df8df5":"sns.barplot(data=train_df, x='Sex', y=y);","dab156e7":"sns.barplot(data=train_df, x='SibSp', y=y);","62551cc1":"sns.barplot(data=train_df, x='Parch', y=y);","66bcf639":"for data in (train_df, test_df):\n    data['FamilySize'] = data['Parch'] + data['SibSp']","158cd25f":"sns.barplot(data=train_df, x='FamilySize', y=y);","47a1fb73":"for data in (train_df, test_df):\n    data['Parch'] = data['Parch'] \/ data['FamilySize']\n    data['Parch'].fillna(-1, inplace=True)\n    data['SibSp'] = data['SibSp'] \/ data['FamilySize']\n    data['SibSp'].fillna(-1, inplace=True)","02a791c2":"for data in (train_df, test_df):\n    data['Fare'].fillna(train_df.groupby(['Embarked', 'Pclass'])['Fare'].transform('median'), inplace=True)\n\nfor data in (train_df, test_df):\n    data['Embarked'].fillna(train_df['Embarked'].mode(), inplace=True)","902f2534":"class AgeFeature(BaseEstimator, TransformerMixin):\n    '''AgeFeature - works with df only'''\n    \n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X, y=None):\n        # sex, name\n        X['Initial'] = 0\n        for i in X:\n            X['Initial'] = X.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n    \n        X['Initial'].replace(\n            ['Dona','Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n            ['Miss','Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],\n            inplace=True\n        )\n        print(X.groupby('Initial')['Age'].median()) # lets check the average age by Initials\n\n        ## Assigning the NaN Values with the Ceil values of the median ages\n        X.loc[(X.Age.isnull()) & (X.Initial=='Mr'), 'Age'] = 30\n        X.loc[(X.Age.isnull()) & (X.Initial=='Mrs'), 'Age'] = 35\n        X.loc[(X.Age.isnull()) & (X.Initial=='Master'), 'Age'] = 3.5\n        X.loc[(X.Age.isnull()) & (X.Initial=='Miss'), 'Age'] = 21.5\n        X.loc[(X.Age.isnull()) & (X.Initial=='Other'), 'Age'] = 51\n        return X['Age'].as_matrix().reshape(-1, 1)\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X).transform(X)\n\nager = AgeFeature().fit(train_df)\ntrain_df['Age'] = ager.transform(train_df)\ntest_df['Age'] = ager.transform(test_df)","baf1f1b4":"for data in (train_df, test_df):\n    data['Is_Married'] = 0\n    data['Is_Married'].loc[data['Initial'] == 'Mrs'] = 1","9b608c00":"train_df['Cabin'] = train_df['Cabin'].apply(lambda x: str(x)[0]).apply(lambda x: 'n' if x == 'T' else x)\ntest_df['Cabin'] = test_df['Cabin'].apply(lambda x: str(x)[0]).apply(lambda x: 'n' if x == 'T' else x)\ntrain_df['Cabin'].unique(), test_df['Cabin'].unique()","d56053d0":"woe_encoder = WOEEncoder(cols=['Cabin', 'Embarked', 'Initial'])\nwoe_encoder.fit(train_df, y)\ntrain_df = woe_encoder.transform(train_df)\ntest_df = woe_encoder.transform(test_df)","43abb14a":"train_df.drop(['Name', 'Ticket'], axis=1, inplace=True)\ntest_df.drop(['Name', 'Ticket'], axis=1, inplace=True)","a11c48fd":"train_df.columns.to_list()","ed75f6ee":"train_df.columns.to_list()","e46ff286":"dfs = ps.DataFrameSummary(train_df)\ndfs.summary()","5799e1bf":"dfs = ps.DataFrameSummary(test_df)\ndfs.summary()","de55091e":"scaler = StandardScaler().fit(train_df)\nscaled_train_df = scaler.transform(train_df)\nscaled_test_df = scaler.transform(test_df)","86a52b72":"tsne = TSNE(perplexity=30)\n\ntrain_tsne_transformed = tsne.fit_transform(scaled_train_df)\ntest_tsne_transformed = tsne.fit_transform(scaled_test_df)","7f1c9bdb":"plt.figure(figsize=(10, 10))\nplt.scatter(train_tsne_transformed[:, 0], train_tsne_transformed[:, 1], c=y);","07e98573":"plt.figure(figsize=(10, 10))\nplt.scatter(test_tsne_transformed[:, 0], test_tsne_transformed[:, 1]);","360af023":"# different between train and test penalty power:\np = 0.9\n# number of hyperopt iterarions\nk = 350\ncat=[]\nmodels = dict()\ntrains = []\npreds = []\nfolds = []\nvalid_scores =dict()\nskf = StratifiedKFold(n_splits=9, shuffle=True, random_state=7)\n\nfor num, spl in enumerate(skf.split(train_df, y)):\n    train_index, val_index = spl[0], spl[1]\n    x_train_1, x_valid_1 = train_df.iloc[train_index, :], train_df.iloc[val_index, :]\n    y_train_1, y_valid_1 = y.iloc[train_index], y.iloc[val_index]\n    \n    print('fold: ', num+1)\n    \n    def score(params):\n        w=[]\n        best_iter = []\n        skf_1 = StratifiedKFold(n_splits=2, shuffle=True, random_state=77)\n\n        for train_index_2, val_index_2 in skf_1.split(x_train_1, y_train_1):\n            x_train_2, x_valid_2 = x_train_1.iloc[train_index_2, :], x_train_1.iloc[val_index_2, :]\n            y_train_2, y_valid_2 = y_train_1.iloc[train_index_2], y_train_1.iloc[val_index_2]\n            train_data = lgb.Dataset(x_train_2, label=y_train_2,)\n            val_data = lgb.Dataset(x_valid_2, label=y_valid_2, reference=train_data)\n            gbm = lgb.train(params, train_data, valid_sets = [train_data, val_data], valid_names=['train', 'val'],\n                            num_boost_round = 5800, verbose_eval = False,)\n            w.append([gbm.best_score['train']['auc'], gbm.best_score['val']['auc']])\n            best_iter.append(gbm.best_iteration)\n        nrounds = np.mean(best_iter)\n        res = list(np.mean(w, axis=0))\n        return {'loss': -res[1]+np.power(np.square(res[0]-res[1]), p), 'status': STATUS_OK, \n                'mean_auc_train': res[0], 'mean_auc_test': res[1], 'best_iter': int(nrounds)}\n\n    def optimize(trials):\n        space = {\n        #'max_depth': hp.choice('max_depth', [-1, 6, 7]),\n        'max_depth': -1,\n        'max_bin': scope.int(hp.uniform('max_bin', 2, 200)),\n        'num_leaves': scope.int(hp.uniform('num_leaves', 2, 16)),\n        'min_data_in_leaf': scope.int(hp.uniform('min_data_in_leaf', 2, 75)),\n        'lambda_l1': hp.quniform('lambda_l1', 0, 6.5, 0.25),\n        'lambda_l2': hp.quniform('lambda_l2', 0, 10.5, 0.25),\n        'learning_rate': hp.quniform('learning_rate', 0.01 , 0.05, 0.005),\n        'min_child_weight': hp.quniform('min_child_weight', 0.05, 25.95, 0.05),\n        'feature_fraction': hp.quniform('feature_fraction', 0.4, 0.8, 0.1),\n        'bagging_fraction': hp.quniform('bagging_fraction', 0.45, 0.95, 0.05),\n        'metric': ('auc',),\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'nthread': 8,\n        'early_stopping_rounds': 10,\n        'silent':1,\n        }\n        best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=k)\n        #print(best)\n        \n    trials = Trials()\n    optimize(trials)\n    params_for = trials.best_trial['misc']['vals']\n    params_for['num_leaves'] = int(params_for['num_leaves'][0])\n    params_for['max_bin'] = int(params_for['max_bin'][0])\n    params_for['min_data_in_leaf'] = int(params_for['min_data_in_leaf'][0])\n    params_for['objective'] = 'binary'\n    params_for['metric'] = {'auc'}\n    \n    train_data = lgb.Dataset(x_train_1, label=y_train_1)\n    val_data = lgb.Dataset(x_valid_1, label=y_valid_1, reference=train_data)\n    gbm_main = lgb.train(params_for, train_data, valid_sets = [train_data, val_data], valid_names=['train', 'val'],\n                         num_boost_round = 10000, verbose_eval = 100, early_stopping_rounds = 20)\n    trains.append(gbm_main.predict(train_df))\n    preds.append(gbm_main.predict(test_df))\n    folds.append(gbm_main.best_score['val']['auc'])\n    print('best score: ', gbm_main.best_score['val']['auc'])\n    print()\n    valid_scores[num] = gbm_main.best_score['val']['auc']\n    models['lgb_fold'+str(num+1)] = gbm_main","4abae7e6":"model = LogisticRegression(penalty='l1', solver='saga')\n#pd.Series(np.mean(np.array(preds), axis=0))\nmodel.fit(pd.DataFrame(np.array(trains)).T, y)\ntarget = model.predict(pd.DataFrame(np.array(preds)).T)","73caaef7":"sub_df['Survived'] = target","fb8c66bf":"sub_df.to_csv(\"submission.csv\", index=False)","ef1a58ab":"### Drop Passenger Id","171a01e1":"for train","0cd6ea45":"### Class target average probability for Pclass","a8d1e2b8":"### Class target average probability for Parch","9b9fe41f":"# EDA","5980ed02":"## Check columns list","8786cee5":"### Class target average probability for FamilySize","d5aa9b46":"This is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n\nThe competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\nRead on or watch the video below to explore more details. Once you\u2019re ready to start competing, click on the \"Join Competition button to create an account and gain access to the competition data. Then check out Alexis Cook\u2019s Titanic Tutorial that walks you through step by step how to make your first submission!","45437461":"# Fill NA","3ada1c30":"### Data Set Column Descriptions\n- pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- survived: Survival (0 = No; 1 = Yes)\n- name: Name\n- sex: Sex\n- age: Age\n- sibsp: Number of siblings\/spouses aboard\n- parch: Number of parents\/children aboard\n- fare: Passenger fare (British pound)\n- embarked: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n- cabin:Cabin number\n- ticket:Ticket number","6febe00e":"for test","6bb55fe8":"### Binary Sex","bef8fde2":"### Describe features","79a0cf50":"## Goal\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.\nFor each in the test set, you must predict a 0 or 1 value for the variable.\n\n## Metric\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.","6f7243d2":"# Load Datasets","a841af4f":"![fb485619f6a24cd791db6792433c1bf0.jpeg](attachment:fb485619f6a24cd791db6792433c1bf0.jpeg)","a394e9f1":"### Is Married flag","ed120ed6":"## Histograms of features","6e027bde":"# The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n## What Data Will I Use in This Competition?\nIn this competition, you\u2019ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\nThe `test.csv` dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes.\n\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.\n\nCheck out the \u201cData\u201d tab to explore the datasets even further. Once you feel you\u2019ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.","f3309194":"# Import some libs","50a1855d":"### Class target average probability for SibSp","fe05c689":"### Weight of Evidence Encoding\nWeight of Evidence (WoE) is a measure of the \u201cstrength\u201d of a grouping technique to separate good and bad. This method was developed primarily to build a predictive model to evaluate the risk of loan default in the credit and financial industry. Weight of evidence (WOE) is a measure of how much the evidence supports or undermines a hypothesis.","3e759429":"### Parch & SibSp will be in percentage","8a30a284":"# Let's look on final dataset in 2d dimension by tSNE\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is a (prize-winning) technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets. The technique can be implemented via Barnes-Hut approximations, allowing it to be applied on large real-world datasets. ","023e3297":"# Let's get target","d9657ccf":"### Class target average probability for Sex","db5c84e3":"## Titanic schema:\n![schema.jpeg](attachment:schema.jpeg)","036117bf":"# Machine learning\nMachine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.","d1ad7433":"#### I think, trees will be the best algorithm."}}