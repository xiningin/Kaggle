{"cell_type":{"4d1a167f":"code","8d041b9f":"code","5fe049e0":"code","a1632a4d":"code","d6f75d26":"code","8f847a32":"code","d3e6cfa3":"code","1f8b92eb":"code","7e3c0f66":"code","09ef9e68":"code","108fca16":"code","65e7b904":"code","dca7395a":"code","e042cde2":"code","ff2a6ad3":"code","b3f5c1ee":"code","63e345fd":"markdown","a5e4b2bc":"markdown","0f5d4dbd":"markdown","993a1128":"markdown"},"source":{"4d1a167f":"# data manipulation\nimport numpy as np\nimport pandas as pd\nimport random\n\n# high-level neural networks API - running on top of TensorFlow\nimport keras\n# Sequential is a linear stack of layers\nfrom keras.models import Sequential\n# Dense, Flatten - type of layers, Dropout - tool, which decrease chance of overfitting\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras import backend as K\n\n# data visualisation, live training loss plot\nimport matplotlib.pyplot as plt\n\nimport time\nfrom sklearn.model_selection import train_test_split","8d041b9f":"# load dataset\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","5fe049e0":"y_train = train[\"label\"]\nX_train = train.drop(labels = [\"label\"],axis = 1) ","a1632a4d":"# shape of data\nX_train.shape","d6f75d26":"def vis_rand_dig():\n    # size of pixcel\n    plt.figure(figsize = (10, 10))\n    # print random digit\n    rand_indexes = np.random.randint(0, X_train.shape[0], 16)\n    for index,im_index in enumerate(rand_indexes):\n        plt.subplot(4, 4, index+1)\n        plt.imshow(X_train.values[im_index].reshape(28,28), cmap = 'ocean', interpolation = 'none')\n        plt.title('Class %d' % y_train[im_index])\n    plt.tight_layout()","8f847a32":"vis_rand_dig()","d3e6cfa3":"def prep_data(X_train, y_train, test):\n   \n    X_train = X_train.astype('float32') \/ 255\n    test = test.astype('float32')\/255\n    X_train = X_train.values.reshape(-1,28,28,1)\n    test = test.values.reshape(-1,28,28,1)\n    y_train = keras.utils.np_utils.to_categorical(y_train)\n    classes = y_train.shape[1]\n    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = int(time.time()))\n    \n    return X_train, y_train, X_test, y_test, classes, test","1f8b92eb":"X_train, y_train, X_test, y_test, out_neurons, test = prep_data(X_train, y_train, test)","7e3c0f66":"def cnn():\n    cnn = Sequential([\n        Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu', input_shape = (28,28,1)),\n        Conv2D(32, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        MaxPool2D(pool_size = (2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        MaxPool2D(pool_size = (2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(128, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        Conv2D(128, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        MaxPool2D(pool_size = (2, 2)),\n        Dropout(0.25),\n        \n        Flatten(),\n        \n        Dense(512, activation = 'relu'),\n        Dropout(0.5),\n        Dense(256, activation = 'relu'),\n        Dropout(0.5),\n        Dense(out_neurons, activation = 'softmax')\n    ])\n    return cnn\n\nmodel = cnn()\nmodel.summary()\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'Adam' , metrics = ['accuracy'])","09ef9e68":"model.fit(X_train, y_train,\n          batch_size = 512,\n          epochs = 11,\n          validation_data = (X_test, y_test),\n          verbose = 0);","108fca16":"result = model.evaluate(X_test, y_test, verbose = 0)\nprint('Accuracy: ', result[1])\nprint('Error: %.2f%%' % (100- result[1]*100))","65e7b904":"y_pred = model.predict(test, verbose=0)","dca7395a":"def error_predict(y_test, y_pred):\n    for idx, (a, b) in enumerate(zip(y_test, y_pred)):\n        if np.argmax(a) == np.argmax(b): continue\n        yield idx, np.argmax(a), tuple(np.argsort(b)[-2:])","e042cde2":"def display_errors():\n    random_errors = random.sample(list(error_predict(y_test, y_pred)), 12)\n\n    plt.figure(figsize=(10, 10))\n    X_test_plot = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2] )\n\n    for index, (im_index, y_test_val, (y_pred_2, y_pred_1)) in enumerate(random_errors):\n            plt.subplot(4,4,index+1)\n            plt.imshow(X_test_plot[im_index], cmap='ocean', interpolation='none')\n            plt.title('True value: {0}\\nFirst predicted: {1}\\nSecond predicted: {2}'.format(y_test_val, y_pred_1, y_pred_2))\n            plt.tight_layout()","ff2a6ad3":"display_errors()","b3f5c1ee":"solution = np.argmax(y_pred,axis = 1)\nsolution = pd.Series(solution, name=\"Label\").astype(int)\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),solution],axis = 1)\nsubmission.to_csv(\"mnist_cnn.csv\",index=False)","63e345fd":"# Digit Recognizer\n\nTarget: Take an image of a handwritten single digit, and determine what that digit is.\n\nSource: https:\/\/www.kaggle.com\/c\/digit-recognizer","a5e4b2bc":"# Where we have mistake?","0f5d4dbd":"# CNN\n\nConvolutional neural network expects additional parameter - number of channels. In computer science we have two type of standards. We may add channel at the first or last element. It is dependent on backend. I use TensorFlow, so I add channel at last.","993a1128":"In my opiniom model make mistake in very difficult cases.\n\n# Conclusions\n\n- The best score: 0.9952(accuracy), 0,48%(error);\n- The best architecture of CNN: \n        Conv2D(32, kernel_size = (3, 3), padding = 'same', activation = 'relu',\n                input_shape = in_dim),\n        Conv2D(32, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        MaxPool2D(pool_size = (2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        Conv2D(64, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        MaxPool2D(pool_size = (2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(128, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        Conv2D(128, kernel_size = (3, 3), activation = 'relu', padding = 'same'),\n        MaxPool2D(pool_size = (2, 2)),\n        Dropout(0.25),\n        \n        Flatten(),\n        \n        Dense(512, activation = 'relu'),\n        Dropout(0.5),\n        Dense(256, activation = 'relu'),\n        Dropout(0.5),\n        Dense(out_dim, activation = 'softmax')\n        \n- Parameters: 1,010,666;\n- Time: 16 min 47 s;\n- Hardware: CPU - Ryzen 2700X  (I know that GPU is better for deep learning, but I didn't have opportunity to work on GPU);\n- Batch size: 512;\n- Epochs: 11;\n\nI tried several configuration. The results varied between 0,48% - 0,84% (error). Generally I changed number of layers, number of kernels, size of batch_size and number of epochs. \nI noticed, that: \n- More layer increased computing time and growing number of parameters;\n- More batch_size affect the reduced computing time;\n- More expanded section of MLP (number of layers and number of neurones) influences on more parameters;\n- More CNN layers = less parameters;\n- More numbers of epochs affect on growing compute time;\n- Worse score after used BatchNormalization;\n- CNN vs MLP for digit recognition? Decisively CNN for digit recognizer!"}}