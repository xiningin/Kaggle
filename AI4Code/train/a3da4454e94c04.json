{"cell_type":{"80fbd620":"code","6b93ab3a":"code","fa74e59f":"code","ab765caa":"code","4dba44b2":"code","315c2484":"code","ff22ba00":"code","832bfe44":"code","fa0038c3":"code","f5f7cbe7":"code","4bc8cd10":"code","2f551f3c":"code","3ab19079":"code","b5026c4e":"code","5f359c73":"code","43ce887b":"code","175ab1c7":"code","7aa1f2dd":"code","72982b98":"code","8f28c437":"code","9101cc00":"code","8e48651f":"code","9661a088":"code","ab998a60":"code","cdd1554d":"code","67d3ad66":"code","b3889cbc":"code","1f46f4ae":"code","c637d6c2":"code","bdfaf578":"code","fc8362c0":"code","257b8fe0":"code","895a903b":"code","3ad37aa7":"code","3846003d":"markdown","2fcc0b3d":"markdown","63e1c285":"markdown","6d444167":"markdown","62ff580c":"markdown","8df3b73e":"markdown","0578e8bd":"markdown","4bbcf07e":"markdown","d574f5b9":"markdown","cbb6be76":"markdown","44e779c2":"markdown","fa141037":"markdown","0293bcd0":"markdown","db3d2ff2":"markdown","6ab8621f":"markdown","e71d6f5d":"markdown","266b8aca":"markdown","cbfa4022":"markdown","ab094d9e":"markdown","86f129c2":"markdown","e3bdcec2":"markdown","07752305":"markdown","5b428b4d":"markdown","13f5d632":"markdown","6886e33b":"markdown","3f18c24d":"markdown","8b37fca5":"markdown","d8c71142":"markdown","a1f374cd":"markdown","68aef050":"markdown","2e81a411":"markdown","6b68e8e4":"markdown","8b8f238a":"markdown","b5424886":"markdown","a781f3ad":"markdown"},"source":{"80fbd620":"### Import required libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\nimport warnings\nwarnings.filterwarnings('ignore')","6b93ab3a":"# Read train and test files\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","fa74e59f":"# training set\nprint (\"Training set:\")\nn_data  = len(train_df)\nn_features = train_df.shape[1]\nprint (\"Number of Records: {}\".format(n_data))\nprint (\"Number of Features: {}\".format(n_features))\n\n# testing set\nprint (\"\\nTesting set:\")\nn_data  = len(test_df)\nn_features = test_df.shape[1]\nprint (\"Number of Records: {}\".format(n_data))\nprint (\"Number of Features: {}\".format(n_features))","ab765caa":"train_df.head(n=10)","4dba44b2":"train_df.info()","315c2484":"test_df.head(n=10)","ff22ba00":"test_df.info()","832bfe44":"#### Check if there are any NULL values in Train Data\nprint(\"Total Train Features with NaN Values = \" + str(train_df.columns[train_df.isnull().sum() != 0].size))\nif (train_df.columns[train_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(train_df.columns[train_df.isnull().sum() != 0])))\n    train_df[train_df.columns[train_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","fa0038c3":"#### Check if there are any NULL values in Test Data\nprint(\"Total Test Features with NaN Values = \" + str(test_df.columns[test_df.isnull().sum() != 0].size))\nif (test_df.columns[test_df.isnull().sum() != 0].size):\n    print(\"Features with NaN => {}\".format(list(test_df.columns[test_df.isnull().sum() != 0])))\n    test_df[test_df.columns[test_df.isnull().sum() != 0]].isnull().sum().sort_values(ascending = False)","f5f7cbe7":"X_train = train_df.drop([\"ID\", \"target\"], axis=1)\ny_train = np.log1p(train_df[\"target\"].values)\n\nX_test = test_df.drop([\"ID\"], axis=1)","4bc8cd10":"# check and remove constant columns\ncolsToRemove = []\nfor col in X_train.columns:\n    if X_train[col].std() == 0: \n        colsToRemove.append(col)\n        \n# remove constant columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True)\n\n# remove constant columns in the test set\nX_test.drop(colsToRemove, axis=1, inplace=True) \n\nprint(\"Removed `{}` Constant Columns\\n\".format(len(colsToRemove)))\nprint(colsToRemove)","2f551f3c":"# Check and remove duplicate columns\ncolsToRemove = []\ncolsScaned = []\ndupList = {}\n\ncolumns = X_train.columns\n\nfor i in range(len(columns)-1):\n    v = X_train[columns[i]].values\n    dupCols = []\n    for j in range(i+1,len(columns)):\n        if np.array_equal(v, X_train[columns[j]].values):\n            colsToRemove.append(columns[j])\n            if columns[j] not in colsScaned:\n                dupCols.append(columns[j]) \n                colsScaned.append(columns[j])\n                dupList[columns[i]] = dupCols\n                \n# remove duplicate columns in the training set\nX_train.drop(colsToRemove, axis=1, inplace=True) \n\n# remove duplicate columns in the testing set\nX_test.drop(colsToRemove, axis=1, inplace=True)\n\nprint(\"Removed `{}` Duplicate Columns\\n\".format(len(dupList)))\nprint(dupList)","3ab19079":"print(\"Train set size: {}\".format(X_train.shape))\nprint(\"Test set size: {}\".format(X_test.shape))","b5026c4e":"# Find feature importance\nclf_gb = GradientBoostingRegressor(random_state = 42)\nclf_gb.fit(X_train, y_train)\nprint(clf_gb)","5f359c73":"# GradientBoostingRegressor feature importance - top 100\nfeat_importances = pd.Series(clf_gb.feature_importances_, index=X_train.columns)\nfeat_importances = feat_importances.nlargest(100)\nplt.figure(figsize=(16,15))\nfeat_importances.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","43ce887b":"# GradientBoostingRegressor feature importance - top 25\nfeat_importances_gb = pd.Series(clf_gb.feature_importances_, index=X_train.columns)\nfeat_importances_gb = feat_importances_gb.nlargest(25)\nplt.figure(figsize=(16,8))\nfeat_importances_gb.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","175ab1c7":"print(pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(10))","7aa1f2dd":"# Find feature importance\n#Don't forget to do feature importance usinge another regressor\nclf_rf = RandomForestRegressor(random_state = 42)\nclf_rf.fit(X_train, y_train)\nprint(clf_rf)","72982b98":"# RandomForestRegressor feature importance - top 25\nfeat_importances_rf = pd.Series(clf_rf.feature_importances_, index=X_train.columns)\nfeat_importances_rf = feat_importances_rf.nlargest(25)\nplt.figure(figsize=(16,8))\nfeat_importances_gb.plot(kind='barh')\nplt.gca().invert_yaxis()\nplt.show()","8f28c437":"print(pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(10))","9101cc00":"plt.figure()\nfig, ax = plt.subplots(1, 2, figsize=(16,6))\nfeat_importances_gb.plot(kind='barh', ax=ax[0])\nfeat_importances_rf.plot(kind='barh', ax=ax[1])\nax[0].invert_yaxis()\nax[1].invert_yaxis()\nplt.show()","8e48651f":"s1 = pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(10).index\ns2 = pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(10).index\n\ncommon_features = pd.Series(list(set(s1).intersection(set(s2)))).values\n\nprint(common_features)","9661a088":"df_plot = X_train[['f190486d6', 'eeb9cd3aa', '58e2e02e6', '58232a6fb', '15ace8c9f', '9fd594eec']]\ndf_plot['target'] = y_train\n\ng = sns.pairplot(df_plot, diag_kind=\"kde\", palette=\"BuGn_r\")\ng.fig.suptitle('Pairplot of Top 6 Important Features',fontsize=26)","ab998a60":"# PLot Correlation HeatMap for top 20 features from GB and RF Models\ns1 = pd.Series(clf_gb.feature_importances_, index=X_train.columns).nlargest(20).index\ns2 = pd.Series(clf_rf.feature_importances_, index=X_train.columns).nlargest(20).index\n\ncommon_features = pd.Series(list(set(s1).union(set(s2)))).values\n\nprint(common_features)","cdd1554d":"df_plot = pd.DataFrame(X_train, columns = common_features)\ncorr = df_plot.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(16, 16))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title(\"Correlation HeatMap\", fontsize=15)\nplt.show()","67d3ad66":"X_train_cpy = X_train.copy()\npca = PCA(n_components=3)\nX_train_cpy = pca.fit_transform(X_train_cpy)","b3889cbc":"print(pca.components_)","1f46f4ae":"print(pca.explained_variance_)","c637d6c2":"colors = np.random.random((4459, 3))\n\nfig = plt.figure(1, figsize=(8, 6))\nax = Axes3D(fig, elev=-150, azim=110)\n\nax.scatter(X_train_cpy[:, 0], X_train_cpy[:, 1], X_train_cpy[:, 2], c=colors,\n           cmap=plt.cm.Set1, edgecolor=colors, alpha=0.5, s=40)\nax.set_title(\"First three PCA directions\")\nax.set_xlabel(\"1st eigenvector\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd eigenvector\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd eigenvector\")\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","bdfaf578":"dev_X, val_X, dev_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)","fc8362c0":"def run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.005,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=50, \n                      evals_result=evals_result)\n    \n    pred_test_y = np.expm1(model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","257b8fe0":"# Training LGB\npred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, X_test)\nprint(\"LightGBM Training Completed...\")","895a903b":"# feature importance\nprint(\"Features Importance...\")\ngain = model.feature_importance('gain')\nfeatureimp = pd.DataFrame({'feature':model.feature_name(), \n                   'split':model.feature_importance('split'), \n                   'gain':100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\nprint(featureimp[:15])","3ad37aa7":"sub = pd.read_csv('..\/input\/sample_submission.csv')\nsub[\"target\"] = pred_test\nprint(sub.head())\nsub.to_csv('sub_lgb.csv', index=False)","3846003d":"### Load Data","2fcc0b3d":"We have more features than records in the Training set.\n\n- We have a total of `4993` features in the `Train` set.\n- The number of records in `Train` set is just `4459`\n\n\n- We have a total of `4992` features in the `Test` set.\n- The number of records in `Test` set is just `49342`","63e1c285":"Below we will see some visualizations related to the top features.","6d444167":"As we have lot of features, we will look for ways to trim down on the number of features. From the above sample records we can see that many of the records and features contains **`0`**. Also as there are many features, we will try to see if there are any features with constant values.","62ff580c":"### Feature Importance from RandomForestRegressor","8df3b73e":"## Check and Remove Constant Features","0578e8bd":"Below we will list the `top 10 features` with their feature importances.","4bbcf07e":"### GradientBoostingRegressor vs RandomForestRegressor Top 25 Features","d574f5b9":"## Correlation HeatMap","cbb6be76":"As we have `4730 features` it would be very difficult to visualize and analyze all of them. So, we will try to analyze only some of the top features.","44e779c2":"## Prepare the Data","fa141037":"## Check for Missing Values","0293bcd0":"We will plot the top 25 features from RandomForestRegressor to see how similar they are with the top features given by GradientBoostingRegressor.","db3d2ff2":"So there are a total of 4993 features out of which 1845 are of type float64, 3147 are int64 and 1 is object (ID is the object column).\n\nThe sad thing is that, as mentioned earier, we do not have meaningful names for the features. So, we have to rely on other ways to analyze and understand the data.","6ab8621f":"### Feature Importance from GradientBoostingRegressor","e71d6f5d":"## Data Visualization","266b8aca":"Below, we will use the GradientBoostingRegressor to see the top features in our dataset.","cbfa4022":"We are lucky enough to get some data that has got no `NULLS`. So, we can go ahead and start anaylzing the data.","ab094d9e":"### LightGBM","86f129c2":"<img src=\"https:\/\/dynl.mktgcdn.com\/p\/OioPDkijUSBehXPo5nCC_CEd-0hZkZRv94-HHnJj-eA\/2326x832.jpg\" width=600\/>","e3bdcec2":"## PCA Visualization","07752305":"### Test Data Info","5b428b4d":"### Test Data","13f5d632":"So, we can see that there are a total of just 6 common features in top 25 features of RandomForestRegressor and GradientBoostingRegressor. So, we should be careful in choosing what feature sto pick for further analysis.","6886e33b":"The above plot looks very cluttered. Instead, we will take a look at the `top 25 features`.","3f18c24d":"### Train Data","8b37fca5":"So there are a total of 4992 features in the test set out of which 4991 are of type float64 and 1 is object (ID is the object column)","d8c71142":"In this competition, Santander Group is asking us to help them identify the value of transactions for each potential customer.\nWe are provided with an anonymized dataset containing numeric feature variables, the numeric target column, and a string ID column. Our task is to predict the value of target column in the test set.\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error. The data set consists of train.csv and test.csv","a1f374cd":"## Load Libraries","68aef050":"## Modeling","2e81a411":"### Train Data Info","6b68e8e4":"## Exploratory Data Analysis","8b8f238a":"# Santander Value Prediction Challenge","b5424886":"## Data Summary","a781f3ad":"## Check and Remove Duplicate Columns"}}