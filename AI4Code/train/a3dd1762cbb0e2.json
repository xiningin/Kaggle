{"cell_type":{"51056b85":"code","96f0f742":"code","69ce3635":"code","c264b04c":"code","51fcaef0":"code","5f6eb9b5":"code","dab2fd07":"code","1292813e":"code","a21e233d":"code","0aaae4a3":"code","7e3bd8da":"code","4c25c66a":"code","c8302a77":"code","0277fa94":"code","4115fd85":"code","27047e35":"code","128c7b8d":"code","2e1a2402":"code","33b50356":"code","80e24b2f":"code","cf5289c0":"code","942f5219":"code","9ae935e9":"code","4328cc97":"code","ab42c5a3":"code","3755b207":"code","a4a08d10":"code","ede87794":"code","91a26eea":"code","587c40e5":"code","967eb7b3":"code","5d610dc3":"code","801e2e60":"code","d6225645":"code","38c7debc":"code","9cb7aeee":"markdown","2b17da17":"markdown","46f39fe7":"markdown","3e454120":"markdown","ed41fb01":"markdown","72090e2a":"markdown","6031868c":"markdown","80d8cacc":"markdown","7008251a":"markdown","c969c7df":"markdown"},"source":{"51056b85":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns","96f0f742":"## We will consider both years data\n\ndf2 = pd.read_excel(\"..\/input\/online-retail\/online_retail_II.xlsx\", sheet_name = \"Year 2010-2011\")\ndf1 = pd.read_excel(\"..\/input\/online-retail\/online_retail_II.xlsx\", sheet_name = \"Year 2009-2010\")","69ce3635":"df = pd.concat([df1, df2], axis=0)\ndf.shape","c264b04c":"df.info()","51fcaef0":"df.describe()","5f6eb9b5":"df['Country'].value_counts()","dab2fd07":"## check missing values\n\ndf.isnull().sum()","1292813e":"df = df.dropna()","a21e233d":"## Let's drop returns as our interest is sales. Also price below 0.01 could be a mistake.\n\ndf = df[df['Quantity']>0]\ndf = df[df['Price']>=0.01]","0aaae4a3":"## calculate Amount per product\n\ndf['Amount'] = df['Quantity']*df['Price']","7e3bd8da":"## calculate Amount by customer across the total period to get 'Monetary' value\n\nmonetary = df.groupby('Customer ID')['Amount'].agg('sum')\nmonetary.head()","4c25c66a":"## Note Invoice repeat for same transaction. Hence, we will dedup the data by customer and invoice\n\ndf3 = df[['Customer ID', 'Invoice']]\ndf3 = df3.drop_duplicates()\nfreq = df3.groupby('Customer ID')['Invoice'].agg('count')\n\nfreq.head()","c8302a77":"### get latest date in the dataset\n\nmax_dt = max(df['InvoiceDate'])\nmax_dt","0277fa94":"df['rec'] = max_dt - df['InvoiceDate']\ndf['rec'] = df['rec'].apply(lambda x: x.days)","4115fd85":"## how many days back a customer transacted (based on latest date in the dataset)? \n## Although correct, we don't want to have recency 0. Hence increase recency by 1.\n\nrecency = df.groupby('Customer ID')['rec'].agg('min')\nrecency = recency+1\n\nrecency.head()","27047e35":"## merge all three data to get RFM\n\nRFM = pd.concat([monetary, recency, freq], join='inner', axis=1 )\n\nRFM.head()","128c7b8d":"RFM.shape","2e1a2402":"RFM.describe()","33b50356":"RFM = RFM.rename(columns={'Invoice':'Freq', 'rec':'Recency'})","80e24b2f":"cols = list(RFM.columns)\n\nfor col in cols:\n    RFM[col].plot(kind='box')\n    plt.show()","cf5289c0":"q1 = RFM.quantile(0.25)\nq3 = RFM.quantile(0.75)\n\nIQR = q3-q1\n\nRFM = RFM[~((RFM< (q1-1.5*IQR)) | (RFM> (q3+1.5*IQR)))]\nRFM.shape","942f5219":"RFM = RFM.dropna()\nRFM.shape","9ae935e9":"RFM.info()","4328cc97":"## standardization is critical for distance based algorithm like Cluster\n\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nRFMs = ss.fit_transform(RFM)","ab42c5a3":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nkmeans = KMeans()\n\nssd = []\nsscore = []\n\nfor i in range(2, 10):\n    kmeans= KMeans(n_clusters=i, random_state=3)\n    kmeans.fit(RFMs)\n    ss1 = kmeans.inertia_\n    ssd.append(ss1)\n    sscore1 = silhouette_score(RFMs, kmeans.labels_)\n    sscore.append(sscore1)","3755b207":"fig, ax = plt.subplots(figsize=(10,5))\nax.plot(range(2,10), ssd)\nax.set_ylabel('SSD')\nax.set_xlabel('No. Clusters')\nax1=ax.twinx()\nax1.plot(range(2,10), sscore, 'r-')\nax1.set_ylabel('SSCORE')\nplt.show()","a4a08d10":"def svisualizer(x, ncluster):\n    import matplotlib.pyplot as plt\n    from sklearn.cluster import KMeans\n    import numpy as np\n    from matplotlib import cm\n    from sklearn.metrics import silhouette_samples\n\n    km = KMeans(n_clusters=ncluster, init='k-means++', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n    y_km = km.fit_predict(x)\n\n    cluster_labels = np.unique(y_km)\n    n_clusters = cluster_labels.shape[0]\n    silhouette_vals = silhouette_samples(x, y_km, metric='euclidean')\n    y_ax_lower, y_ax_upper = 0, 0\n\n    yticks = []\n    for i, c in enumerate(cluster_labels):\n        c_silhouette_vals = silhouette_vals[y_km==c]\n        c_silhouette_vals.sort()\n        y_ax_upper += len(c_silhouette_vals)\n        color = cm.jet(i \/ n_clusters)\n        plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n\n        yticks.append((y_ax_lower + y_ax_upper) \/ 2)\n        y_ax_lower += len(c_silhouette_vals)\n\n    silhouette_avg = np.mean(silhouette_vals)\n    plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\") \n\n    plt.yticks(yticks, cluster_labels + 1)\n    plt.ylabel('Cluster')\n    plt.xlabel('Silhouette coefficient')\n\n    plt.tight_layout()\n    plt.show()\n","ede87794":"svisualizer(RFMs, 3)","91a26eea":"svisualizer(RFMs, 4)","587c40e5":"svisualizer(RFMs, 5)","967eb7b3":"kmeans=KMeans(n_clusters=3, random_state=3)\nkmeans.fit(RFMs)\n\nRFM['cluster'] = kmeans.labels_","5d610dc3":"RFM['cluster'].value_counts()","801e2e60":"RFM.groupby(['cluster']).agg(['mean', 'count'])","d6225645":"rec = RFM.groupby(['cluster'])['Recency'].agg(['mean'])\nfreq = RFM.groupby(['cluster'])['Freq'].agg(['mean'])\nmonetary = RFM.groupby(['cluster'])['Amount'].agg(['mean'])","38c7debc":"fig, axis = plt.subplots(1, 3, figsize=(10, 5))\nrec.plot.bar(ax=axis[0])\nfreq.plot.bar(ax=axis[1])\nmonetary.plot.bar(ax=axis[2])\nplt.show()","9cb7aeee":"## Profile clusters","2b17da17":"Mostly it is from UK","46f39fe7":"Elbow analysis shows elbows at 3 and 4 clusters. However, Silhouette score is not conclusive. Hence silhouette distribution is compared for 3, 4 and 5 clusters.\n\nTo criteria for evaluating Silhouette distribution are:-\n    - All clusters should cross average level\n    - Size of all clusters should be same as much as possible ","3e454120":"## Segmenting Customers based on RFM","ed41fb01":"Due to outlier removal about 700 rows are reduced","72090e2a":"There are serious outliers for Amount and Freq. It will be removed using IQR","6031868c":"Cluster -0\nThis group consists of customers who have shopped long time back. Freq and Monetary are at low level. Most likely they have switched to another online retailer. Good case for reactivation campaign.\n\nCluster-1\nThis group of customers shopped with the retailer recently. Also got the highest Frequency and Monetary. The best customer for the retailer. Should take proactive steps to retain them through loyalty programs.\n\nCluster-2\nThis group shopped with the retailer recently. But the frequency and monetary are not as high as Cluster-1. This group too is valuable and frequncy and monetary could increase in future. Good case for offering discounts etc for increasing sales.\n","80d8cacc":"Considering all the three cluster solutions, 3 clusters seems to be the best solution. For 4 and 5, few clusters are too small in size.\n\nHence, we will continue the analysis with 3 cluster solution.","7008251a":"Let's take a look at the distribution of country","c969c7df":"Customer ID is critical for this analysis and it is not possible to impute. Hence we will have to drop missing customer IDs"}}