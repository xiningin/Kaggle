{"cell_type":{"a5632fd5":"code","030bcdf0":"code","c02fb6bd":"code","44093821":"code","bf58339e":"code","c0430601":"code","989d8ef1":"code","8b8606e7":"code","dc82c28f":"code","0a92ea6a":"code","5cc3cfac":"code","f7a10f3e":"markdown","03b5342a":"markdown","3d9ab28b":"markdown","36109f27":"markdown"},"source":{"a5632fd5":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","030bcdf0":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport tensorflow_addons as tfa\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom tqdm.notebook import tqdm","c02fb6bd":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","44093821":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    del df['sig_id']\n    return df\n\ntrain = preprocess(train_features)\ntest = preprocess(test_features)\n\ndel train_targets['sig_id']\n\ntrain_targets = train_targets.loc[train['cp_type']==0].reset_index(drop=True)\ntrain = train.loc[train['cp_type']==0].reset_index(drop=True)","bf58339e":"def nn_model(num_columns):\n   \n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(num_columns),\n\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation='elu')),\n\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1048, activation='elu')),\n\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.35),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(510, activation='elu')),\n\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(510, activation='elu')),\n        \n     tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(510, activation='elu')),\n    \n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(256, activation='elu')),\n\n    #============ Final Layer =================\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tfa.optimizers.AdamW(lr = 1e-3, weight_decay = 1e-5, clipvalue = 700), \n                  loss='binary_crossentropy', \n                  )\n    return model    ","c0430601":"\nnum_features = ['g-0','g-2','g-3','g-5','g-6','g-7','g-8'\n,'g-9','g-11','g-12','g-13','g-15','g-16','g-17','g-18','g-20','g-21'\n,'g-22','g-24','g-25','g-26','g-27','g-28','g-29','g-30','g-31','g-32'\n,'g-33','g-34','g-36','g-37','g-38','g-39','g-41','g-42','g-43','g-45'\n,'g-47','g-48','g-49','g-50','g-51','g-52','g-53','g-54','g-55','g-56'\n,'g-57','g-58','g-60','g-61','g-62','g-63','g-65','g-66','g-67','g-68'\n,'g-69','g-70','g-71','g-72','g-73','g-75','g-76','g-77','g-78','g-79'\n,'g-80','g-81','g-83','g-84','g-85','g-86','g-87','g-89','g-90','g-91'\n,'g-92','g-93','g-94','g-96','g-97','g-98','g-100','g-101','g-102','g-103'\n,'g-104','g-105','g-106','g-107','g-108','g-109','g-110','g-111','g-112'\n,'g-113','g-114','g-115','g-116','g-117','g-118','g-119','g-120','g-121'\n,'g-122','g-123','g-124','g-125','g-126','g-127','g-129','g-130','g-131'\n,'g-132','g-133','g-134','g-135','g-136','g-137','g-138','g-139','g-140'\n,'g-141','g-142','g-143','g-144','g-146','g-147','g-148','g-149','g-150'\n,'g-151','g-152','g-154','g-156','g-157','g-158','g-160','g-161','g-162'\n,'g-163','g-164','g-165','g-166','g-167','g-169','g-170','g-172','g-173'\n,'g-174','g-175','g-177','g-178','g-179','g-180','g-181','g-183','g-184'\n,'g-185','g-186','g-187','g-188','g-189','g-190','g-192','g-194','g-195'\n,'g-196','g-199','g-200','g-202','g-203','g-205','g-206','g-207','g-208'\n,'g-209','g-210','g-211','g-212','g-215','g-216','g-217','g-218','g-219'\n,'g-221','g-222','g-224','g-225','g-226','g-227','g-228','g-229','g-230'\n,'g-231','g-233','g-235','g-236','g-237','g-238','g-239','g-240','g-241'\n,'g-242','g-243','g-245','g-246','g-247','g-248','g-250','g-251','g-252'\n,'g-253','g-254','g-255','g-256','g-257','g-258','g-260','g-262','g-263'\n,'g-265','g-267','g-268','g-269','g-270','g-272','g-273','g-274','g-276'\n,'g-279','g-280','g-283','g-284','g-285','g-286','g-287','g-291','g-292'\n,'g-293','g-294','g-296','g-297','g-298','g-299','g-300','g-301','g-302'\n,'g-303','g-305','g-306','g-307','g-308','g-309','g-310','g-312','g-313'\n,'g-314','g-317','g-318','g-319','g-321','g-322','g-323','g-324','g-325'\n,'g-326','g-327','g-328','g-329','g-330','g-331','g-332','g-335','g-336'\n,'g-337','g-338','g-340','g-341','g-342','g-343','g-344','g-346','g-347'\n,'g-348','g-349','g-350','g-352','g-353','g-354','g-355','g-356','g-357'\n,'g-358','g-359','g-360','g-361','g-362','g-363','g-365','g-366','g-367'\n,'g-368','g-369','g-371','g-372','g-373','g-374','g-375','g-376','g-377'\n,'g-379','g-380','g-381','g-382','g-383','g-384','g-385','g-386','g-387'\n,'g-388','g-389','g-390','g-391','g-392','g-394','g-395','g-396','g-397'\n,'g-398','g-400','g-402','g-403','g-404','g-405','g-407','g-408','g-409'\n,'g-410','g-411','g-412','g-414','g-415','g-416','g-417','g-418','g-419'\n,'g-420','g-421','g-422','g-423','g-424','g-425','g-426','g-427','g-428'\n,'g-429','g-430','g-431','g-432','g-433','g-434','g-435','g-438','g-439'\n,'g-440','g-441','g-442','g-443','g-444','g-445','g-446','g-447','g-449'\n,'g-450','g-451','g-453','g-454','g-455','g-456','g-457','g-458','g-459'\n,'g-460','g-461','g-462','g-463','g-465','g-466','g-468','g-469','g-470'\n,'g-471','g-472','g-473','g-474','g-475','g-476','g-479','g-480','g-482'\n,'g-483','g-484','g-485','g-486','g-488','g-489','g-491','g-492','g-493'\n,'g-497','g-498','g-499','g-500','g-502','g-503','g-504','g-506','g-507'\n,'g-508','g-509','g-510','g-511','g-513','g-514','g-515','g-516','g-518'\n,'g-520','g-522','g-523','g-524','g-525','g-526','g-527','g-528','g-529'\n,'g-530','g-531','g-533','g-534','g-535','g-536','g-537','g-538','g-539'\n,'g-540','g-541','g-542','g-543','g-544','g-546','g-547','g-550','g-551'\n,'g-552','g-553','g-554','g-555','g-556','g-557','g-558','g-559','g-560'\n,'g-561','g-562','g-563','g-564','g-566','g-567','g-568','g-569','g-570'\n,'g-571','g-572','g-574','g-577','g-578','g-579','g-580','g-583','g-584'\n,'g-587','g-588','g-589','g-590','g-592','g-593','g-594','g-595','g-596'\n,'g-597','g-598','g-599','g-600','g-602','g-604','g-605','g-606','g-608'\n,'g-609','g-610','g-611','g-612','g-613','g-614','g-616','g-619','g-620'\n,'g-622','g-624','g-627','g-628','g-629','g-630','g-631','g-632','g-634'\n,'g-635','g-636','g-639','g-640','g-641','g-642','g-643','g-644','g-646'\n,'g-647','g-648','g-649','g-651','g-652','g-655','g-656','g-657','g-658'\n,'g-659','g-660','g-661','g-663','g-664','g-665','g-666','g-667','g-669'\n,'g-671','g-672','g-673','g-674','g-675','g-677','g-678','g-679','g-681'\n,'g-682','g-683','g-684','g-685','g-686','g-688','g-689','g-691','g-692'\n,'g-693','g-694','g-696','g-697','g-698','g-699','g-700','g-701','g-702'\n,'g-704','g-705','g-706','g-708','g-709','g-710','g-711','g-712','g-713'\n,'g-714','g-720','g-722','g-724','g-725','g-726','g-727','g-728','g-729'\n,'g-731','g-733','g-734','g-735','g-736','g-737','g-738','g-739','g-740'\n,'g-741','g-742','g-743','g-744','g-745','g-746','g-747','g-748','g-749'\n,'g-750','g-751','g-752','g-753','g-755','g-756','g-757','g-758','g-759'\n,'g-760','g-761','g-762','g-763','g-764','g-766','g-767','g-768','g-769'\n,'g-771','c-0','c-5','c-6','c-7','c-8','c-9','c-10','c-12','c-13','c-15'\n,'c-18','c-20','c-22','c-24','c-25','c-26','c-30','c-33','c-34','c-36'\n,'c-37','c-38','c-41','c-44','c-45','c-46','c-47','c-48','c-50','c-51'\n,'c-52','c-54','c-56','c-57','c-58','c-59','c-60','c-62','c-63','c-64'\n,'c-65','c-66','c-67','c-69','c-70','c-71','c-72','c-73','c-75','c-76'\n,'c-77','c-79','c-80','c-81','c-83','c-85','c-86','c-87','c-89','c-92'\n,'c-93','c-95','c-96','c-98','c-99']\n\nprint(len(num_features))\n","989d8ef1":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(log_loss(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float), labels=[0,1]))\n    return np.mean(metrics)","8b8606e7":"N_STARTS = 9\n\ntf.random.set_seed(43)\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(KFold(n_splits=5, random_state=(seed+1)**2, shuffle=True).split(train_targets)):\n        print(f'Fold {n}')\n    \n        model = nn_model(len(num_features))\n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-5, mode='min')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        \n       \n        \n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience= 5)\n        \n        model.fit(train[num_features].values[tr],\n                  train_targets.values[tr],\n                  validation_data=(train[num_features].values[te], train_targets.values[te]),\n                  epochs=75, batch_size=128,\n                  steps_per_epoch = len(train.values[tr]) \/\/ 128 + 1,\n                  validation_steps = len(train.values[te]) \/\/ 128 + 1,\n                  callbacks=[reduce_lr_loss, cb_checkpt, early], verbose=2\n                 )\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test[num_features].values)\n        val_predict = model.predict(train[num_features].values[te])\n        \n        ss.loc[:, train_targets.columns] += test_predict\n        res.loc[te, train_targets.columns] += val_predict\n        print('')\n    \nss.loc[:, train_targets.columns] \/= ((n+1) * N_STARTS)\nres.loc[:, train_targets.columns] \/= N_STARTS","dc82c28f":"print(f'OOF Metric: {metric(train_targets, res)}')","0a92ea6a":"ss.loc[test['cp_type']==1, train_targets.columns] = 0","5cc3cfac":"ss.to_csv('submission.csv', index=False)","f7a10f3e":"## MODEL","03b5342a":"If you find this work helpful please **Upvote**","3d9ab28b":"### Training","36109f27":"This kernel is an improvement of that [kernel](https:\/\/www.kaggle.com\/simakov\/keras-multilabel-neural-network-v1-2)\n\n**Changes I done**\n\n1) Improved The Architecture (Neurons, Dropout, Early Stopping)\n\n2) Added Elu as an activation function\n\n3) Learning rate adjusted, k folds and seed averaging to 9 with 100 epochs\n\n4) Using AdamW as an optimizer"}}