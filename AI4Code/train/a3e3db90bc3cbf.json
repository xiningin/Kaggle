{"cell_type":{"b5f24508":"code","f1955046":"code","e68c854c":"code","3d27b49c":"code","0d9ba445":"code","b6534be6":"code","63faa0e3":"code","ddb31cb5":"code","8e59a0b4":"code","ff8b8af3":"code","0e114bf7":"code","81837462":"code","8e7d6062":"code","dbf8acab":"code","f9697dec":"code","5287d673":"code","1ae36568":"code","80c370eb":"code","1633aee4":"code","480bb473":"code","3e165dc7":"code","fee7acd9":"code","3164b0a4":"code","d7a01b03":"code","9712978c":"code","314fd39e":"code","814a90e7":"code","308f015f":"code","056db3c2":"code","81a5dcc0":"code","e15d97cc":"code","555573a1":"code","5258f6fa":"code","2e62cf2c":"code","cc8cd585":"code","18c496a6":"code","3f428175":"code","b9a78f3c":"code","17a34550":"code","6bfc7339":"code","7c38a14a":"code","49be21a0":"code","9aee0b9f":"code","fdeb345e":"code","2b67e7dd":"code","a1d6960f":"code","0149cbaf":"code","38ac912b":"code","ccc0156c":"code","ae6a2281":"code","fb3ab959":"code","c4fc4994":"code","cace60a7":"code","6f1c91f5":"code","5cdf7778":"code","193e10f9":"code","c8a6b6d6":"code","515a1a5e":"code","9ab9488f":"code","b298fdea":"code","39e41703":"code","eac6acf0":"code","dbbaf873":"code","e3ce1beb":"code","6bdcdc5f":"code","bc8acabc":"code","dc6f5cb6":"code","4279799f":"code","c17ca27f":"code","88351539":"code","53524f5e":"code","1fc299c6":"code","a279a885":"code","375373a6":"code","8b89be69":"code","2f03c562":"code","b9995a0f":"code","ca570165":"code","e7e1dcd5":"code","d4e2d7c1":"code","35f2419c":"code","05e12421":"markdown","07e24ea8":"markdown","f4c819d2":"markdown","68abb12b":"markdown","7abb8936":"markdown","28e49b2c":"markdown","ebcf9324":"markdown","b04fe7c5":"markdown","a1dbf458":"markdown","0c753b4f":"markdown","7fefa29a":"markdown","6b8329c7":"markdown","ba3c5b9c":"markdown","7b55ed6b":"markdown","84ce6fcd":"markdown","0c3855b8":"markdown","f2dfefe9":"markdown","011f85e3":"markdown","4644307c":"markdown","83d500b8":"markdown","5da1c653":"markdown","905f5131":"markdown","c87ef00f":"markdown","9bfa3504":"markdown","89948cb0":"markdown","2eafc1e7":"markdown","d4ee53a7":"markdown","8034cea6":"markdown","808881e6":"markdown","0302bfe9":"markdown","325571a2":"markdown","b687bfd2":"markdown","6bb4b0ed":"markdown","034f13ce":"markdown","9434133d":"markdown","f4c35964":"markdown","9a159ba4":"markdown","9b40fb7d":"markdown","5b019c60":"markdown","7b03c859":"markdown","e3d75c23":"markdown"},"source":{"b5f24508":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","f1955046":"# Importing required libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error as MAE\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.feature_selection import RFE\n#import scipy.stats as stats\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","e68c854c":"# Reading csv and setting column index\ndf = pd.read_csv('..\/input\/bikesharing\/day.csv', index_col=0)\n\n# View first 5 rows\ndf.head()","3d27b49c":"# Drop unnecessary column\ndf = df.drop('dteday', axis=1)","0d9ba445":"# Print shape of dataframe\nog_df_shape = df.shape\nprint(og_df_shape)","b6534be6":"# Describe dataframe\ndf.describe()","63faa0e3":"# View info of dataframe\ndf.info()","ddb31cb5":"# Check percantage of null values in the data\ndf.isnull().sum()\/len(df)*100","8e59a0b4":"# Check duplicate values in the data\ndf.duplicated().sum()","ff8b8af3":"# Mapping categories in yr\ndf.yr = df.yr.map({0:'2018', 1:'2019'})\ndf.yr.value_counts()","0e114bf7":"# Mapping categories in season\ndf.season = df.season.map({1:'Spring', 2:'Summer', 3:'Fall', 4:'Winter'})\ndf.season.value_counts()","81837462":"# Mapping categories in weathersit\ndf.weathersit = df.weathersit.map({1:'Clear', 2:'Misty', 3:'Light_rain_snow', 4:'Heavy_rain_snow'})\ndf.weathersit.value_counts()","8e7d6062":"# Mapping categories in mnth\ndf.mnth = df.mnth.map({1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', 7:'Jul', 8:'Aug',\n                       9:'Sep', 10:'Oct', 11:'Nov', 12:'Dec'})\ndf.mnth.value_counts()","dbf8acab":"# Mapping categories in weekday\ndf.weekday = df.weekday.map({0:'Sun', 1:'Mon', 2:'Tue', 3:'Wed', 4:'Thu', 5:'Fri', 6:'Sat'})\ndf.weekday.value_counts()","f9697dec":"df.info()","5287d673":"# Distplot of Numeric Variables\n\nplt.figure(figsize=[20,15])\nplt.subplot(3,3,1)\nsns.distplot(df.temp)\nplt.subplot(3,3,2)\nsns.distplot(df.atemp)\nplt.subplot(3,3,3)\nsns.distplot(df.hum)\nplt.subplot(3,3,4)\nsns.distplot(df.windspeed)\nplt.subplot(3,3,5)\nsns.distplot(df.registered)\nplt.subplot(3,3,6)\nsns.distplot(df.casual)\nplt.subplot(3,3,7)\nsns.distplot(df.cnt)\nplt.suptitle('Distplot of Numeric Variables',fontsize=20)\nplt.show()","1ae36568":"# Scatter Pairplot of Numeric Variables\n\nsns.pairplot(df, vars=['temp', 'atemp', 'hum', 'windspeed', 'registered', 'casual', 'cnt'], corner=True)\nplt.suptitle('Scatter Pairplot of Numeric Variables',fontsize=20)\nplt.show()","80c370eb":"# Correlation Heatmap Of The Dataframe\nplt.figure(figsize=[20,15])\nsns.heatmap(df.corr(), annot=True,cmap=\"vlag\",linewidths=.4)\nplt.title('Correlation Heatmap Of All Variables', fontsize=20)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.show()","1633aee4":"# dropping registered and casual columns as business goal is to find total count only, not specific. Dropping highly correlated atemp\ndf = df.drop(columns=['registered','casual','atemp'])","480bb473":"# Visualizing Binary Numeric Variables\n\nplt.figure(figsize=[15,10])\nplt.subplot(2,3,1)\nsns.scatterplot(x = 'yr', y = 'cnt', data = df)\nplt.subplot(2,3,2)\nsns.scatterplot(x = 'holiday', y = 'cnt', data = df)\nplt.subplot(2,3,3)\nsns.scatterplot(x = 'workingday', y = 'cnt', data = df)\nplt.subplot(2,3,4)\nsns.countplot(df.yr)\nplt.subplot(2,3,5)\nsns.countplot(df.holiday)\nplt.subplot(2,3,6)\nsns.countplot(df.workingday)\nplt.suptitle('Visualizing Binary Numeric Variables',fontsize=20)\nplt.show()","3e165dc7":"# Create variable for categorical data\ndf_categorical = df.select_dtypes(exclude=['float64', 'int64'])\nprint(\"The categorical columns in the dataframe are:\", df_categorical.columns)","fee7acd9":"# Season and Weather vs Count boxplots\nplt.figure(figsize=[15,7])\nplt.subplot(1,2,1)\nsns.boxplot(x='season', y='cnt', data=df)\nplt.subplot(1,2,2)\nsns.boxplot(x='weathersit', y='cnt', data=df)\nplt.suptitle('Season and Weather vs Count', fontsize=20)\nplt.show()","3164b0a4":"# Time (Day Of The Week and Month) boxplots\nplt.figure(figsize=[15,10])\nplt.subplot(2,1,1)\nsns.boxplot(x='weekday', y='cnt', data=df)\nplt.subplot(2,1,2)\nsns.boxplot(x='mnth', y='cnt', data=df)\nplt.suptitle('Time (Day Of The Week and Month)', fontsize=20)\nplt.show()","d7a01b03":"# Create dummies\ndf = pd.get_dummies(df, drop_first=True)\ndf.head()","9712978c":"print(\"Shape of dataframe before creating dummies:\", og_df_shape)\nprint(\"Shape of dataframe after creating dummies:\", df.shape)","314fd39e":"#Splitting into training and testing sets\n\ndf_train, df_test = train_test_split(df, train_size=0.7, random_state=1)","814a90e7":"print(\"Size of training data:\", df_train.size)\nprint(\"Size of testing data:\", df_test.size)","308f015f":"df_train.head()","056db3c2":"# Using MinMaxScaler\nscaler = MinMaxScaler()","81a5dcc0":"# Applying scaler to all the columns except the 0\/1 binary and 'dummy' variables\nnum_variables = ['cnt', 'windspeed', 'hum', 'temp']\n\ndf_train[num_variables] = scaler.fit_transform(df_train[num_variables])","e15d97cc":"df_train.describe()","555573a1":"# Correlation Heatmap Of All Training Variables\n\nplt.figure(figsize=[25,20])\nsns.heatmap(df_train.corr(), annot=True,cmap=\"vlag\",linewidths=.4)\nplt.title('Correlation Heatmap Of All Training Variables', fontsize=20)\nplt.xticks(rotation=90)\nplt.yticks(rotation=0)\nplt.show()","5258f6fa":"# Dividing Training Data into X and y\ny_train = df_train.pop('cnt')\nX_train = df_train","2e62cf2c":"# Check columns in X_train\nX_train.columns","cc8cd585":"# Shape of X_train\nX_train.shape","18c496a6":"# Running RFE with the output number of the variable equal to 18\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 18)\nrfe = rfe.fit(X_train, y_train)","3f428175":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","b9a78f3c":"# True variables for model\ncol = X_train.columns[rfe.support_]\ncol","17a34550":"# Remove False variables\nX_train.columns[~rfe.support_]","6bfc7339":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","7c38a14a":"# Creating a linear model and adding constant as Statsmodel doesn't add by default\nX_train_lm = sm.add_constant(X_train_rfe)\n\nlr = sm.OLS(y_train, X_train_lm).fit()\nlr.summary()","49be21a0":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif_df = pd.DataFrame()\nvif_df['Features'] = X_train_rfe.columns\nvif_df['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif_df['VIF'] = round(vif_df['VIF'], 2)\nvif_df = vif_df.sort_values(by = \"VIF\", ascending = False)\nvif_df","9aee0b9f":"# workingday variable has high p-value and very high VIF so we'll drop it first\nX_train_new = X_train_rfe.drop('workingday', axis=1)","fdeb345e":"#Build second fitted model\nX_train_lm2 = sm.add_constant(X_train_new)\n\nlr_2 = sm.OLS(y_train, X_train_lm2).fit()\nlr_2.summary()","2b67e7dd":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif_df = pd.DataFrame()\nvif_df['Features'] = X_train_new.columns\nvif_df['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif_df['VIF'] = round(vif_df['VIF'], 2)\nvif_df = vif_df.sort_values(by = \"VIF\", ascending = False)\nvif_df","a1d6960f":"# weekday_Sat variable has high p-value so we'll drop it\nX_train_new = X_train_new.drop('weekday_Sat', axis=1)","0149cbaf":"#Build third fitted model\nX_train_lm3 = sm.add_constant(X_train_new)\n\nlr_3 = sm.OLS(y_train, X_train_lm3).fit()\nlr_3.summary()","38ac912b":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif_df = pd.DataFrame()\nvif_df['Features'] = X_train_new.columns\nvif_df['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif_df['VIF'] = round(vif_df['VIF'], 2)\nvif_df = vif_df.sort_values(by = \"VIF\", ascending = False)\nvif_df","ccc0156c":"# season_Summer variable has high p-value so we'll drop it\nX_train_new = X_train_new.drop('season_Summer', axis=1)","ae6a2281":"# Build fourth fitted model\nX_train_lm4 = sm.add_constant(X_train_new)\n\nlr_4 = sm.OLS(y_train, X_train_lm4).fit()\nlr_4.summary()","fb3ab959":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif_df = pd.DataFrame()\nvif_df['Features'] = X_train_new.columns\nvif_df['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif_df['VIF'] = round(vif_df['VIF'], 2)\nvif_df = vif_df.sort_values(by = \"VIF\", ascending = False)\nvif_df","c4fc4994":"# hum variable has very high VIF so we'll drop it\nX_train_new = X_train_new.drop('hum', axis=1)","cace60a7":"# Build fifth fitted model\nX_train_lm5 = sm.add_constant(X_train_new)\n\nlr_5 = sm.OLS(y_train, X_train_lm5).fit()\nlr_5.summary()","6f1c91f5":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif_df = pd.DataFrame()\nvif_df['Features'] = X_train_new.columns\nvif_df['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif_df['VIF'] = round(vif_df['VIF'], 2)\nvif_df = vif_df.sort_values(by = \"VIF\", ascending = False)\nvif_df","5cdf7778":"# season_Winter variable has very high p value\nX_train_new = X_train_new.drop('season_Winter', axis=1)","193e10f9":"# Build sixth fitted model\nX_train_lm6 = sm.add_constant(X_train_new)\n\nlr_6 = sm.OLS(y_train, X_train_lm6).fit()\nlr_6.summary()","c8a6b6d6":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif_df = pd.DataFrame()\nvif_df['Features'] = X_train_new.columns\nvif_df['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif_df['VIF'] = round(vif_df['VIF'], 2)\nvif_df = vif_df.sort_values(by = \"VIF\", ascending = False)\nvif_df","515a1a5e":"# windspeed variable has very high p value\nX_train_new = X_train_new.drop('windspeed', axis=1)","9ab9488f":"# Build seventh fitted model\nX_train_lm7 = sm.add_constant(X_train_new)\n\nlr_7 = sm.OLS(y_train, X_train_lm7).fit()\nlr_7.summary()","b298fdea":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif_df = pd.DataFrame()\nvif_df['Features'] = X_train_new.columns\nvif_df['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif_df['VIF'] = round(vif_df['VIF'], 2)\nvif_df = vif_df.sort_values(by = \"VIF\", ascending = False)\nvif_df","39e41703":"# mnth_Jul variable has high p value\nX_train_new = X_train_new.drop('mnth_Jul', axis=1)","eac6acf0":"# Build eigth fitted model\nX_train_lm8 = sm.add_constant(X_train_new)\n\nlr_8 = sm.OLS(y_train, X_train_lm8).fit()\nprint(lr_8.summary())","dbbaf873":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif_df = pd.DataFrame()\nvif_df['Features'] = X_train_new.columns\nvif_df['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif_df['VIF'] = round(vif_df['VIF'], 2)\nvif_df = vif_df.sort_values(by = \"VIF\", ascending = False)\nprint(vif_df)","e3ce1beb":"y_train_pred = lr_8.predict(X_train_lm8)","6bdcdc5f":"res = y_train-y_train_pred","bc8acabc":"# Plot the histogram of the error terms\n\nplt.figure(figsize=[15,7])\nsns.distplot(res, bins = 20)\nplt.suptitle('Error Terms', fontsize = 30)\nplt.xlabel('Errors', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.show()","dc6f5cb6":"num_variables = ['cnt', 'windspeed', 'hum', 'temp']\n\ndf_test[num_variables] = scaler.transform(df_test[num_variables])","4279799f":"df_test.describe()","c17ca27f":"#  Dividing Test Data into X and y\ny_test = df_test.pop('cnt')\nX_test = df_test","88351539":"# Using columns from final train model for test\nX_train_cols = X_train_new.columns\nX_test = X_test[X_train_cols]\nX_test.info()","53524f5e":"# Adding constant variable to test dataframe\nX_test_m = sm.add_constant(X_test)","1fc299c6":"# y predictions\ny_pred = lr_8.predict(X_test_m)","a279a885":"# Calculate residuals of actual and predictions\nres = y_test - y_pred","375373a6":"# Plot the histogram of the error terms\n\nsns.set(style=\"darkgrid\")\nsns.distplot(res, bins = 25)\nplt.title('Residual Distplot', fontsize = 15)\nplt.xlabel('Errors', fontsize = 15)\nplt.ylabel('Density', fontsize = 15)\nplt.show()","8b89be69":"# Residual QQ Plot\n\nsm.qqplot(res, line='s')\nplt.title('Residual QQ Plot', fontsize = 15)\nplt.show()","2f03c562":"# Plotting y_test and y_pred to understand the spread\n\nplt.figure(figsize=[15,7])\nsns.regplot(y_test, y_pred, line_kws={\"color\": \"red\"})\nplt.xlabel('Actual', fontsize = 15)\nplt.ylabel('Predicted', fontsize = 15)\nplt.title('y_test vs y_pred', fontsize = 20)\nplt.show()","b9995a0f":"# Plotting res and y_pred to understand the spread\n\nplt.figure(figsize=[15,7])\nsns.regplot(y_pred,res, line_kws={\"color\": \"red\"})\nplt.xlabel('Predicted', fontsize = 15)\nplt.ylabel('Residuals', fontsize = 15)\nplt.title('y_pred vs res', fontsize = 20)\nplt.show()","ca570165":"# Distplot of Predictions vs Actual\n\nplt.figure(figsize=[15,7])\nfig = sns.kdeplot(y_pred, color=\"r\", label='Predictions')\nfig = sns.kdeplot(y_test, shade=True, color=\"b\", label='Actual')\nplt.title('Distplot of Predictions vs Actual', fontsize=20)\nplt.legend()\nplt.show()","e7e1dcd5":"# Calculating MAE and MSE\nprint('\\nThe Mean Absolute Error of y_test, y_pred is:', MAE(y_test, y_pred))\nprint('\\nThe Mean Squared Error of y_test, y_pred is:', MSE(y_test, y_pred))","d4e2d7c1":"# Calculating r2 score\nprint('The r2 score of y_test, y_pred is:',r2_score(y_test, y_pred))","35f2419c":"# Calculating adjusted r2 score\nn=len(X_test)\nk=len(X_test.columns)\nadj_r2_score = 1 - ((1-r2_score(y_test, y_pred))*(n-1)\/(n-k-1))\nprint('The adjusted r2 score of y_test, y_pred is:', adj_r2_score)","05e12421":"#### MODEL 7","07e24ea8":"# Assigning Features and Target (X and y)","f4c819d2":"# Problem Statement\n\nThis assignment is a programming assignment wherein you have to build a multiple linear regression model for the prediction of demand for shared bikes.\nUsing Statsmodel, sklearn LinearRegression, RFE.\n\n \n### Problem Statement\n\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\n    Which variables are significant in predicting the demand for shared bikes.\n    How well those variables describe the bike demands\n\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n\n### Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","68abb12b":"# Exploratory Data Analysis","7abb8936":"**Inference:** Normal distribution can be seen of error terms with slight skewness","28e49b2c":"# Creating Dummies","ebcf9324":"# Model Statistical Analysis","b04fe7c5":"# Feature Scaling","a1dbf458":"# Model Building\n## Using Statsmodel To Get Detailed Statistical Analysis","0c753b4f":"# Final Observations","7fefa29a":"**Inference:** some unequal error terms can be seen towards the higher end and a few towards the lower end as well","6b8329c7":"# Residual Analysis","ba3c5b9c":"**Inference:** hum and windspeed show normal distribution.","7b55ed6b":"## Scaling test set to match train set","84ce6fcd":"# Mapping categories to numerical variables that represent different categories","0c3855b8":"**Inference:** very few customers are seen during non-holiday. year 2019 has much higher and wider distribution of cnt than 2018","f2dfefe9":"* P-value of variables was less than 0.05 and their VIF below 5.\n* The Prob (F-statistic) was almost 0 while F-statistic was significant. Both these are good indicators for the model.\n* Residuals\/error terms were calculated and graphed. They showed normal distribution.\n* High r-squared of test data ~0.84 and the difference between r-squared and adjusted r-squared of test data also happened to be very negligible and therefore good.\n\n","011f85e3":"#### MODEL 5","4644307c":"# Reading and Understanding Data","83d500b8":"Generally VIF above 5 suggests some multicollinearity might exist, VIF above 10 suggests very strong one.\nWe will start dropping highly correlated or insignificant variables one by one until all variables have desired levels","5da1c653":"**Inference:** temp and atemp are very highly correlated. registered and casual are highly correlated as well.","905f5131":"#### MODEL 8","c87ef00f":"#### MODEL 6","9bfa3504":"**Inference:** negative correlation between spring season and cnt can be seen here.","89948cb0":"**Inference:** Sunday has highest customer count however median is similar for all days. May to October have highest cnt and median levels.","2eafc1e7":"# Splitting The Data Into Training And Testing Sets","d4ee53a7":"**Inference:** Fall season and clear weather attract highest customer count. Opposite in case of light rain\/snow and Spring","8034cea6":"**Inference:** linearity can be seen between: registered-temp, temp-cnt, atemp-cnt, registered-cnt","808881e6":"### Visualizing Categorical Variables","0302bfe9":"# Residual analysis on Train data","325571a2":"**Inference:** Displot showing decent overlap of predictions and actual data","b687bfd2":"#### MODEL 1","6bb4b0ed":"#### MODEL 2","034f13ce":"#### MODEL 4","9434133d":"#### MODEL 3","f4c35964":"**Inference:** Normal distribution can be seen of error terms with slight skewness","9a159ba4":"### Visualizing Binary Numeric Variables","9b40fb7d":"**Inference:** Decently equal distribution with no visible increase or decrease in variance","5b019c60":"### Visualizing Non-Binary Numeric Variables","7b03c859":"# Test Data Predictions Using Final Model","e3d75c23":"**Inference:** QQ plot of error terms showing some skewness as well"}}