{"cell_type":{"297128fd":"code","5299aa29":"code","ebf35a39":"code","e73bcabe":"code","e2565433":"code","d90036d5":"code","d03420c5":"code","a3846397":"code","552b3989":"code","9c298fc6":"code","786fbd1a":"code","8ef9d943":"code","79723315":"code","b861a64b":"markdown","72fa3a18":"markdown","f173f689":"markdown","5edee25a":"markdown","085afd71":"markdown","8013a4fb":"markdown","a1c47ec4":"markdown","c61983b5":"markdown","f431430c":"markdown"},"source":{"297128fd":"import random\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom scipy.sparse import csr_matrix\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator, array_to_img\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical   \n\nfrom keras import datasets, layers, models\nfrom keras.callbacks import ModelCheckpoint\n\nfrom matplotlib import pyplot as plt\n\npd.set_option('max_colwidth', 150)","5299aa29":"captions = pd.read_csv(\"..\/input\/flickr8k\/captions.txt\")\ncaptions","ebf35a39":"def display_random_data(count=5, seed=1):\n    np.random.seed(seed)\n    # random choose images == count\n    images = np.random.choice(captions['image'].unique(), count)    \n    # display and their captions\n    for image in images:\n        # display image\n        display(Image.open(f'..\/input\/flickr8k\/Images\/{image}'))\n        # display caption\n        img_captions = captions.loc[captions['image']==image, 'caption'].tolist()\n        for cap in img_captions:\n            print(cap)\ndisplay_random_data(5)","e73bcabe":"# constants\nconfig = {\n    'max_vocab': 10000,\n    'test_ratio': 0.1,\n    'batch_size': 32,\n    'steps_per_epoch': 1000,\n    'epochs': 80\n}","e2565433":"# create the captions with start and end tag\ny = captions['caption'].apply(lambda x: '<start> ' + x.lower() + ' <end>')\n# tokenize the captions\ntokenizer = Tokenizer(num_words=config['max_vocab'], \n                      filters='!\"#$%&()*+,-.\/:;=?@[\\\\]^_`{|}~\\t\\n',\n                      oov_token=0)\ntokenizer.fit_on_texts(y) \nsequences = tokenizer.texts_to_sequences(y)\n# get constant\nconfig['max_len'] = max([len(x) for x in sequences]) \n# pad the sequneces\nsequences = pad_sequences(sequences, maxlen=config['max_len'], dtype='int32', padding='post', truncating='post', value=0)\n# add sequences to the caption df\ncaptions['sequences'] = sequences.tolist()","d90036d5":"# get train\/test split, with holdout = 20%\nrandom.seed(10)\ntest_indices = random.sample(list(captions.index), int(config['test_ratio']*captions.shape[0]))\ntrain_indices = [x for x in list(captions.index) if x not in test_indices]","d03420c5":"# create datagenerator\ndef data_generator(pick_from_caption_indices, batch_size=32, reproduce=False):\n    while True:\n        img_2_arrs, one_hot_captions = [], []\n        # get the indices\n        if reproduce:\n            random.seed(1)\n        indices = random.sample(pick_from_caption_indices, batch_size)\n        # get the relevant rows\n        df = captions.loc[indices, ['image', 'sequences']]\n        # for each batch size\n        for row in df.to_dict(orient='records'):\n            # load the image\n            image = load_img(f\"..\/input\/flickr8k\/Images\/{row['image']}\", color_mode=\"rgb\", target_size=(229, 229, 3))\n            img_2_arr = img_to_array(image) \/ 255\n            one_hot_caption = to_categorical(row['sequences'], num_classes=config['max_vocab'])    \n            # append\n            img_2_arrs.append(img_2_arr)\n            one_hot_captions.append(one_hot_caption)\n        # return\n        yield np.array(img_2_arrs), np.array(one_hot_captions)\n\n# create the data generators\ntrain_data_gen = data_generator(train_indices, config['batch_size'])\ntest_data_gen = data_generator(test_indices, 5, True)\n","a3846397":"# start\nmodel = models.Sequential()\n# CNN part\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(229, 229, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(16, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(16, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(8, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\n# Flatten\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(256, activation='relu'))\n# NLP part\nmodel.add(layers.RepeatVector(config['max_len']))\nmodel.add(layers.LSTM(256, return_sequences=True))\nmodel.add(layers.TimeDistributed(layers.Dense(config['max_vocab'], activation='softmax')))\n# summary\nmodel.summary()","552b3989":"# import keras\n# # start\n# model = models.Sequential()\n# # Pretarined CNN models (Keras Applications)\n# app = keras.applications.Xception(\n#     include_top=False,\n#     weights=\"imagenet\",\n#     input_shape=(229, 229, 3),\n#     pooling=\"avg\",\n# )\n# app.trainable = False\n# model.add(app)\n# model.add(layers.Dense(256, activation='relu'))\n\n# # NLP part\n# model.add(layers.RepeatVector(config['max_len']))\n# model.add(layers.Bidirectional(layers.LSTM(256, return_sequences=True)))\n# model.add(layers.TimeDistributed(layers.Dense(config['max_vocab'], activation='softmax')))\n# # summary\n# model.summary()","9c298fc6":"# define the callbacks\ncheckpoint = ModelCheckpoint(\"checkpoint_model_{epoch:02d}.hdf5\", monitor='loss', verbose=1,\n    save_best_only=False, mode='auto', period=10)\nearlystopping = keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n\n# compile the model\nmodel.compile(optimizer='adam', loss=\"categorical_crossentropy\")\n\n# fit the model\nhistory = model.fit(train_data_gen, \n                    epochs=config['epochs'],\n                    steps_per_epoch=config['steps_per_epoch'],\n                    verbose=1, \n                    callbacks=[checkpoint, earlystopping]\n                   )","786fbd1a":"plt.plot(history.history['loss'])","8ef9d943":"# local save\nmodel.save(\"image_captioning_model\") ","79723315":"# get a sample image\nimgs, one_hots = next(test_data_gen)\n# perform predction\npredicted = model.predict(imgs)\n# for each \nfor i, (img, pred) in enumerate(zip(imgs, predicted)):\n    # display the image\n    display(array_to_img(img))\n    # one hot to sentence\n    sent = \" \".join([tokenizer.index_word.get(np.argmax(x), '#') for x in pred])\n    print(\"Caption: \\n\", sent)","b861a64b":"### Import","72fa3a18":"## Test the model","f173f689":"## Model creation\n\n- Encoder: Custom CNN \n- Decoder: LSTM","5edee25a":"## Save the model","085afd71":"## Prepare data for training\n\n- Prepare the image data: \n    - Resize all images to a specific size.\n    - Load the data and save to a variable.\n    - [Later] Use augmented images to increase data hence generality.\n- Prepare the caption data\n    - Prepare for Recurrent layer input.\n    - Usual preprocessing (lower, punctuations, ..)\n    - Limit the size and add <start>, <end> prefix and suffix resp.","8013a4fb":"## Plot the history","a1c47ec4":"## Model creation v2\n- Encoder: Pre-trained Keras CNN model\n- Decoder: biLSTM","c61983b5":"## Image Captioning on Flickr8k dataset\n\n- Helper code for Image captioning article @ mohitmayank.com OR https:\/\/medium.com\/@mohitmayank\n\n- **Idea:** Learning the image captioning space (combination of CV and NLP space)\n- **Plan:**\n    - v1: Simple CNN as ecnoder and LSTM as decoder. \n    - v2: Use Pretrained\/Existing architecture as CNN and bidirectional LSTM as decoder.","f431430c":"### Load data"}}