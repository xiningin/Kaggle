{"cell_type":{"3f39db1f":"code","3cec3ef7":"code","5ca2451b":"code","0dc345c8":"code","17d6134d":"code","fc2251d2":"code","f49bf8cc":"code","73a3795c":"code","5a65ab7f":"code","e4d1de6e":"code","7c89f875":"code","eac872ad":"code","e242d960":"code","2f02c468":"code","9cf64107":"code","8d5d40f2":"code","15fe2df8":"code","29328408":"code","8279058b":"code","33c3e636":"code","4c71b07a":"code","b76863bf":"markdown","67ac9346":"markdown","8a380a31":"markdown","394f5c3a":"markdown","e166e594":"markdown","bec7af1c":"markdown","9a85fdfe":"markdown","7da79a60":"markdown","64edb85f":"markdown","71392a7c":"markdown","10fbd2dc":"markdown","42c8a9ad":"markdown"},"source":{"3f39db1f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3cec3ef7":"from matplotlib import pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_columns', None)","5ca2451b":"train_data = pd.read_csv(r\"\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv\")\ntest_data = pd.read_csv(r\"\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv\")","0dc345c8":"train_data.head()","17d6134d":"test_data.head()","fc2251d2":"print(\"shape of training set is \" + str(train_data.shape))\nprint(\"shape of test set is \" + str(test_data.shape))","f49bf8cc":"train_data.describe()","73a3795c":"test_data.describe()","5a65ab7f":"missing_data_train = train_data.isna().sum()\nmissing_data_train[missing_data_train != 0]","e4d1de6e":"missing_data_test = train_data.isna().sum()\nmissing_data_test[missing_data_train != 0]","7c89f875":"### https:\/\/stackoverflow.com\/questions\/29803093\/check-which-columns-in-dataframe-are-categorical \ntrain_data.drop(columns=['ID_code', 'target']).select_dtypes(include=['category', 'object', 'int'])","eac872ad":"test_data.select_dtypes(include=['category', 'object', 'int'])","e242d960":"### http:\/\/seaborn.pydata.org\/tutorial\/categorical.html?highlight=bar%20plot\nsns.countplot(data=train_data, x='target')","2f02c468":"# PCA\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\npca_train_X = pca.fit_transform(train_data.drop(columns = ['target', 'ID_code']))","9cf64107":"pca_train_X.shape","8d5d40f2":"from matplotlib import pyplot as plt\n\nplt.scatter(pca_train_X[:,0], pca_train_X[:,1], c=train_data['target'])\nplt.show()","15fe2df8":"import math\n\n## Violin plot for training data\nnum_features=5 \nnum_columns = 3\n\ncolumns = list(filter(lambda col: col not in ['ID_code'] + ['target'], train_data.columns))\nnum_graphs = math.ceil(len(columns)\/num_features)\nnum_rows = math.ceil(num_graphs\/num_columns)\n\nfor row in range(num_rows):\n    print(\"plotting for variables %d - %d\" % (row * num_features * num_columns, row * num_features * num_columns+15))\n    fig, axes = plt.subplots(1, num_columns)\n    fig.set_figheight(5)\n    fig.set_figwidth(8 * num_columns)\n    for col in range(num_columns):\n        curr_cols = columns[row * num_features * num_columns + col * num_features:row * num_features * num_columns + col * num_features + num_features]\n        if len(curr_cols) == 0:\n            break\n        else:\n            curr_cols = curr_cols + ['target']\n        df = train_data[curr_cols]\n        df = df.melt(id_vars = ['target'], var_name = 'Vars', value_name = 'Values')\n        sns.violinplot(x=\"Vars\",y=\"Values\",data=df, hue = 'target', split=True, inner=\"quart\", ax=axes[col])\n    plt.show()","29328408":"# Violin plot for train vs test dataset\ncopy_train_data = train_data.copy()\ncopy_test_data = test_data.copy()\n\ncopy_train_data['dataset'] = 'train'\ncopy_test_data['dataset'] = 'test'\n\ncopy_train_data = copy_train_data.drop(columns=['target'])\n\ncolumns = list(filter(lambda col: col not in ['ID_code'] + ['dataset'], copy_train_data.columns))\n\nnum_graphs = math.ceil(len(columns)\/num_features)\nnum_rows = math.ceil(num_graphs\/num_columns)\n\nfor row in range(num_rows):\n    print(\"plotting for variables %d - %d\" % (row * num_features * num_columns, row * num_features * num_columns+15))\n    fig, axes = plt.subplots(1, num_columns)\n    fig.set_figheight(5)\n    fig.set_figwidth(8 * num_columns)\n    for col in range(num_columns):\n        curr_cols = columns[row * num_features * num_columns + col * num_features:row * num_features * num_columns + col * num_features + num_features]\n        if len(curr_cols) == 0:\n            break\n        else:\n            curr_cols = curr_cols + ['dataset']\n        train_df = copy_train_data[curr_cols]\n        test_df = copy_test_data[curr_cols]\n        df = train_df.append(test_df)\n        df = df.melt(id_vars = ['dataset'], var_name = 'Vars', value_name = 'Values')\n        sns.violinplot(x=\"Vars\",y=\"Values\",data=df, hue = 'dataset', split=True, inner=\"quart\", ax=axes[col])\n    plt.show()","8279058b":"## plot the histogram of corr coef across all variables\n\n# this function calculates the correlation coefficient for large dataset\ndef corr_coef(df):\n    mean = np.mean(df, axis=0)\n    std = np.std(df, axis=0)\n    scaled = (df-mean)\/std\n    return np.matmul(scaled.T, scaled)\/df.shape[0]\n\ndef plot_corrcoef(mat):\n    res = []\n    for i in range(len(mat-1)):\n        for j in range(i+1,len(mat)):\n            res.append(mat[i,j])\n    \n    plt.hist(res, bins=100)\n    plt.show()","33c3e636":"## training set\ncorr = corr_coef(train_data.drop(columns=['target', 'ID_code']))\nplot_corrcoef(corr.to_numpy())","4c71b07a":"## test set\ntest_corr = corr_coef(test_data.drop(columns=['ID_code']))\nplot_corrcoef(test_corr.to_numpy())","b76863bf":"### 5. Plots","67ac9346":"The summary statistics for the training and test data are similar","8a380a31":"There are no missing data in both the training and test set","394f5c3a":"### 4. Distribution of target variable","e166e594":"We cannot see any pattern from the plot. Try violin plot now.","bec7af1c":"### 3. Check categorical values","9a85fdfe":"##### The dataset is imbalanced. Since there are plenty of data available, we will undersample data with target class 0 before training","7da79a60":"### 2. Check null values","64edb85f":"The violin plots look identical for training and test dataset. We expect that a model performing well for training set should perform well for test set as well.","71392a7c":"### 1. Summary Statistics","10fbd2dc":"There are no categorical data in this dataset","42c8a9ad":"The variables exhibits little or no linear correlation with each other."}}