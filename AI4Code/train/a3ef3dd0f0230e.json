{"cell_type":{"847af756":"code","9af5966a":"code","06314ed9":"code","b02b03c4":"code","b7392a0b":"code","3a43b2db":"code","d6d65d89":"code","1579c2f5":"code","38f39eb0":"code","8c13ed67":"markdown","709a7f77":"markdown","5c7b59d0":"markdown","7a2aaba0":"markdown","82bd1f90":"markdown","2c23c993":"markdown","5c7869b5":"markdown"},"source":{"847af756":"import torch\nfrom torch.utils import data\nimport numpy as np\nfrom keras.preprocessing import sequence","9af5966a":"class TextDataset(data.Dataset):\n    '''\n    Simple Dataset\n    '''\n    def __init__(self,X,y=None):\n        self.X = X\n        self.y = y\n\n    def __len__(self):\n        return len(self.X)\n\n    def __getitem__(self, idx):\n        if self.y is not None:\n            return [self.X[idx],self.y[idx]]\n        return self.X[idx]","06314ed9":"class MyCollator(object):\n    '''\n    Yields a batch from a list of Items\n    Args:\n    test : Set True when using with test data loader. Defaults to False\n    percentile : Trim sequences by this percentile\n    '''\n    def __init__(self,test=False,percentile=100):\n        self.test = test\n        self.percentile = percentile\n    def __call__(self, batch):\n        if not self.test:\n            data = [item[0] for item in batch]\n            target = [item[1] for item in batch]\n        else:\n            data = batch\n        lens = [len(x) for x in data]\n        max_len = np.percentile(lens,self.percentile)\n        data = sequence.pad_sequences(data,maxlen=int(max_len))\n        data = torch.tensor(data,dtype=torch.long)\n        if not self.test:\n            target = torch.tensor(target,dtype=torch.float32)\n            return [data,target]\n        return [data]","b02b03c4":"sample_size = 1024\nsizes = np.random.normal(loc=200,scale=50,size=(sample_size,)).astype(np.int32)\nX = [np.ones((sizes[i])) for i in range(sample_size)]\nY = np.random.rand(sample_size).round()","b7392a0b":"sizes.max()","3a43b2db":"batch_size = 128\ndataset = TextDataset(X,Y)\ntest_dataset = TextDataset(X)","d6d65d89":"collate = MyCollator(percentile=100)\nloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True ,collate_fn=collate)\nfor X,Y in loader:\n    print(X.shape,Y.shape)","1579c2f5":"test_collate = MyCollator(test=True,percentile=100)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False , collate_fn=test_collate)\nfor X in test_loader:\n    print(X[0].shape)","38f39eb0":"collate = MyCollator(percentile=95)\nloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True ,collate_fn=collate)\nfor X,Y in loader:\n    print(X.shape,Y.shape)","8c13ed67":"If we choose to pad this data by maximum length in the whole data, this is the length all sequences will be padded to","709a7f77":"Example : Running on test set","5c7b59d0":"However, this is not ideal. <br>\nLet's try padding the sequence to maximum length per batch instead of the whole dataset","7a2aaba0":"To Furthur reduce running times, you can choose to pad by **Nth** percentile of lenghts, keeping **N** close to 100. This may or may not affect model performance, your mileage may vary. <br>\nFor example, **N = 95** gave a good balance between speed and performance for quora challenge's winning team. <br>","82bd1f90":"**Sequence sizes are smaller overall! <br>**\n*Note that the size reduction depends on the distribution of sequence sizes in the actual dataset. <br>*\n*Here, I've used a normally distributed dummy dataset. *","2c23c993":"The [winning solution of Quora Insincere Questions Classification](https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification\/discussion\/80568#latest-516532) used a clever way to pad sequences per batch on the fly. <br> I would like to share the same for PyTorch.<br> This should help in improving run times without affecting model performance ( in theory ).","5c7869b5":"Let's create a sample dataset to test our new collate function"}}