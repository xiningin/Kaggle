{"cell_type":{"eebe9660":"code","b568465b":"code","a7419e91":"code","27fc4770":"code","f2d2832b":"code","2a5dee6e":"code","0768ab6b":"code","79c289af":"code","9da997ae":"code","7fbb43f6":"code","78549810":"code","f87c1c39":"code","d930776b":"code","01bce1d7":"code","686ef6b1":"code","f1b9a70a":"code","210b797f":"code","d30b897e":"code","4b806c54":"code","1fdaa8f7":"code","65404317":"code","647a8c66":"code","a5fd4019":"code","c7d75635":"code","7876b9fd":"code","59d9f41a":"code","8220f073":"code","b5f2b74d":"code","501cc661":"code","560c7290":"code","0abc9bbb":"code","c041393e":"code","2a73ce68":"code","ed309da2":"code","5c288dcf":"code","a0e4cb99":"code","5271cf34":"markdown"},"source":{"eebe9660":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b568465b":"import pandas as pd\nimport numpy as np\nimport warnings \nwarnings.filterwarnings('ignore')","a7419e91":"train = pd.read_csv(r'..\/input\/loan-prediction-analytics-vidhya\/train_ctrUa4K.csv')\ntest = pd.read_csv(r'..\/input\/loan-prediction-analytics-vidhya\/test_lAUu6dG.csv')","27fc4770":"train.shape, test.shape","f2d2832b":"train['source'] = 'train'\ntest['source'] = 'test'\ndata = pd.concat([train,test],ignore_index=True)","2a5dee6e":"data.shape","0768ab6b":"data.dtypes","79c289af":"data.isnull().sum()","9da997ae":"# filling missing values of categorical variables with mode\n\ndata['Gender'].fillna(data['Gender'].mode()[0], inplace=True)\n\ndata['Married'].fillna(data['Married'].mode()[0], inplace=True)\n\ndata['Dependents'].fillna(data['Dependents'].mode()[0], inplace=True)\n\ndata['Self_Employed'].fillna(data['Self_Employed'].mode()[0], inplace=True)\n\ndata['Loan_Amount_Term'].fillna(data['Loan_Amount_Term'].mode()[0], inplace=True)\n\ndata['Credit_History'].fillna(data['Credit_History'].mode()[0], inplace=True)","7fbb43f6":"# filling missing values of continuous variables with mean\ndata['LoanAmount'].fillna(data['LoanAmount'].mean(), inplace=True)","78549810":"data.isnull().sum()","f87c1c39":"# converting the categories into numbers using map function\ndata['Gender'] = data['Gender'].map({'Male': 0, 'Female': 1})\ndata['Married'] = data['Married'].map({'No': 0, 'Yes': 1})\ndata['Dependents'] = data['Dependents'].map({'0': 0, '1': 1, '2': 2, '3+': 3})\ndata['Education'] = data['Education'].map({'Graduate': 1, 'Not Graduate': 0})\ndata['Self_Employed'] = data['Self_Employed'].map({'No': 0, 'Yes': 1})\ndata['Property_Area'] = data['Property_Area'].map({'Rural': 0, 'Semiurban': 1, 'Urban': 2})\ndata['Loan_Status'] = data['Loan_Status'].map({'N': 0, 'Y': 1})","d930776b":"data.head()","01bce1d7":"# bringing variables in the range 0 to 1\ndata['Dependents']=(data['Dependents']-data['Dependents'].min())\/(data['Dependents'].max()-data['Dependents'].min())","686ef6b1":"train_modified = data.loc[data['source']=='train']\ntest_modified = data.loc[data['source']=='test']\n\ntrain_modified.drop('source',axis=1,inplace=True)\ntest_modified.drop(['source','Loan_Status'],axis=1,inplace=True)","f1b9a70a":"train_modified.drop('Loan_ID',axis=1,inplace=True)","210b797f":"# applying for loop to bring all the variables in range 0 to 1\n\nfor i in train_modified.columns[1:]:\n    train_modified[i] = (train_modified[i] - train_modified[i].min()) \/ (train_modified[i].max() - train_modified[i].min())","d30b897e":"X = train_modified.drop('Loan_Status',axis=1)\ny = train_modified['Loan_Status']","4b806c54":"X.shape, y.shape","1fdaa8f7":"from sklearn.model_selection import train_test_split\nX_train,X_test , y_train,y_test = train_test_split(X,y,stratify=train_modified['Loan_Status'],random_state=10,test_size=0.2)","65404317":"from keras.models import Sequential\nfrom keras.layers import InputLayer, Dense","647a8c66":"# defining input neurons\ninput_neurons = X_train.shape[1]","a5fd4019":"# number of output neurons\n# since loan prediction is a binary classification problem, we will have single neuron in the output layer \n# define number of output neurons\noutput_neurons = 1","c7d75635":"# number of hidden layers and hidden neurons\n# It is a hyperparameter and we can pick the hidden layers and hidden neurons on our own\n# define hidden layers and neuron in each layer\nnumber_of_hidden_layers = 2\nneuron_hidden_layer_1 = 10\nneuron_hidden_layer_2 = 5","7876b9fd":"# defining the architecture of the model\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(input_neurons,)))\nmodel.add(Dense(units=neuron_hidden_layer_1, activation='relu'))\nmodel.add(Dense(units=neuron_hidden_layer_2,activation='relu'))\nmodel.add(Dense(units=output_neurons, activation='sigmoid'))","59d9f41a":"# summary of the model\nmodel.summary()","8220f073":"# number of parameters between input and first hidden layer\n\ninput_neurons*neuron_hidden_layer_1","b5f2b74d":"# number of parameters between input and first hidden layer\n# adding the bias for each neuron of first hidden layer\n\ninput_neurons*neuron_hidden_layer_1 + 10","501cc661":"# number of parameters between first and second hidden layer\n\nneuron_hidden_layer_1*neuron_hidden_layer_2 + 5","560c7290":"# number of parameters between second hidden and output layer\n\nneuron_hidden_layer_2*output_neurons + 1","0abc9bbb":"# compiling the model\n\n# loss as binary_crossentropy, since we have binary classification problem\n# defining the optimizer as adam\n# Evaluation metric as accuracy\n\nmodel.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])","c041393e":"# training the model\n\n# passing the independent and dependent features for training set for training the model\n\n# validation data will be evaluated at the end of each epoch\n\n# setting the epochs as 50\n\n# storing the trained model in model_history variable which will be used to visualize the training process\n\nmodel_history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50)","2a73ce68":"# getting predictions for the validation set\nprediction = model.predict_classes(X_test)","ed309da2":"# calculating the accuracy on validation set\nfrom sklearn.metrics import accuracy_score\naccuracy_score(y_test, prediction)","5c288dcf":"import matplotlib.pyplot as plt\n%matplotlib inline\n# summarize history for loss\nplt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","a0e4cb99":"# summarize history for accuracy\nplt.plot(model_history.history['accuracy'])\nplt.plot(model_history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","5271cf34":"activation function of different layers\n\nfor now I have picked relu as an activation function for hidden layers, you can change it as well\nsince it is a binary classification problem, I have used sigmoid activation function in the final layer"}}