{"cell_type":{"1b938416":"code","0c442e1a":"code","ec010a35":"code","30d31665":"code","c0ff7f9b":"code","0353b88e":"code","b4592079":"code","4fe03316":"code","395a1981":"code","fc67a760":"code","4c57d78f":"code","547941af":"code","19d6bbdc":"code","68fa5842":"markdown","d35e0972":"markdown","a26f11b9":"markdown","ac6c5b49":"markdown","100f38d1":"markdown","07c071fd":"markdown","8b69ee17":"markdown","54a158fd":"markdown","486b2383":"markdown","502fa98a":"markdown","4c399e50":"markdown"},"source":{"1b938416":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c442e1a":"data=pd.read_csv('\/kaggle\/input\/marks.csv')\ndata.head()","ec010a35":"data.shape","30d31665":"plt.figure(figsize=(12,8))\nsns.scatterplot(x=data['e1_score'],y=data['e2_score'],hue=data.result)\nplt.title('Exam data')\nplt.show()","c0ff7f9b":"X=data[['e1_score','e2_score']].values   # converted into numpy array\ny=data['result'].values.reshape(1,141)   # converted into numpy array and reshaped as (1,number_of_examples)","0353b88e":"class LogisticRegression():\n    def __init__(self):\n        self.w=None\n        self.b=None\n        self.m=None\n        \n    def sigmoid(self,z):\n        return 1\/(1+np.exp(-z))\n\n    \n    def predict(self,X):\n        try:\n            z=np.dot(self.w.T,X)+self.b\n            y=self.sigmoid(z)\n            self.m=X.shape[1]\n            return y\n        except:\n            print('First fit the data and then predict.')\n        \n    def compute_loss(self,y_pred,y_true):\n        cost=-(np.sum((y_true*np.log(y_pred))+(1-y_true)*np.log(1-y_pred)))\/self.m\n        return cost\n        \n    def fit(self,X,y,learning_rate=0.001,iterations=1000000):\n        loss=[]\n        self.w=np.zeros((X.shape[0],1))\n        self.b=0\n        y_pred=self.predict(X)\n        loss.append(self.compute_loss(y_pred,y))\n        print('loss at iteration number '+str(1)+\" is: \",loss[0])\n        for i in range(iterations):\n            dz=(y_pred-y)\n            dw=np.dot(X,dz.T)\/self.m   \n            db=np.sum(dz)\/self.m\n            self.w=self.w-learning_rate*dw\n            self.b=self.b-learning_rate*db\n            y_pred=self.predict(X)\n            loss.append(self.compute_loss(y_pred,y))\n            if (i+1)%10000==0:\n                print('loss at iteration number '+str(i+1)+\" is: \",loss[i])\n        plt.plot(loss)\n            \n        \n        ","b4592079":"lr=LogisticRegression()","4fe03316":"lr.fit(X.T,y,learning_rate=0.001,iterations=300000)","395a1981":"y_pred=lr.predict(X.T)","fc67a760":"print(lr.compute_loss(y_pred,y))","4c57d78f":"l1=[]\nfor i in y_pred[0]:\n    if i>=0.5:\n        l1.append(1)\n    else:\n        l1.append(0)","547941af":"sns.scatterplot(x=data['e1_score'],y=data['e2_score'],hue=l1)","19d6bbdc":"sns.scatterplot(x=data['e1_score'],y=data['e2_score'],hue=data['result'])","68fa5842":"### Plot of real data","d35e0972":"# Visualizing predicted data and real data","a26f11b9":"# Model of Logistic Regression\n### Class Logistic Regression has four functions:-\n1. Sigmoid Function:-\n    * Takes input z and returns its sigmoid.\n2. Predict Function:-\n    * Takes input X and returns predicted y.\n3. Compute_loss Function:-\n    * Takes input 'y_true' and 'y_pred' and returns loss using above mentioned formula.\n4. Fit function:-\n    * fit function takes input X, y_true and optional arguments number_of_iterations, learning_rate.\n    * fit function works as follows\n        1. initialization of weights and bias\n        2. predicting output y_pred\n        3. computing loss\n        4. updating parameters\n        5. atlast plots graph loss vs iterations","ac6c5b49":"### We got output in range of 0 & 1, converting output as 0 or 1","100f38d1":"# Visualizing data\n* Our data represents marks of 141 students in two exams and result of those exams as pass or fail.\n* e1_score -> marks scored in first exam.\n* e2_score -> marks scored in second exam.\n* result -> pass or fail represented as 1 or 0 respectively.","07c071fd":"# Logistic Regression\n* Logistic regression is named for the function used at the core of the method, the logistic function.\n* Logistic Regression is used for classification purpose like email classification as span or non-spam.\n* Sigmoid function:- \n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcSa6_tRnNi-t45hIxvgi4w2Q_x1JCJAtDVz0A&usqp=CAU)\n\n* Graph of sigmoid function:-\n![](https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2016\/03\/Logistic-Function.png)","8b69ee17":"# How logistic regression works with Gradient Descent as optimzation algorithm?\n1. Input X\n    * Input X has shape as (number_of_features, number_of_examples) -> (n,m)\n    \n    ![X.jpg](attachment:X.jpg)\n    ---------------------------------------------------------------------------\n    \n    * Output y has shape as (1, number of examples) -> (1,m)\n    \n    ![y.jpg](attachment:y.jpg)\n\n\n2. Forward propagation and predict output a.\n    * z= w1*x1 + w2*x2 +.......+b where,\n    w & b are weights and bias respectively, and \n    x1,x2....xn are input features.       \n    * a=f(z) , where f is sigmoid function\n    \n    * Below image shows complete representation of forward propagation (note that image has only 3 input features):-\n     ![](https:\/\/datascienceintuition.files.wordpress.com\/2018\/01\/single_neuron.png?w=1100)\n    * Image represents everything in vectorised form, so weight vector 'w' represent w1,w2,w3....wn and has shape (n,1)\n    \n    \n3. Compute loss.\n    * Formula to compute loss:-\n    ![loss.jpg](attachment:loss.jpg)\n    \n    \n4. Backward propagation and updating parameters using learning rate alpha.\n    * w = w - alpha*dJ\/dw            \n    * b = b - alpha*dJ\/db\n    * note that these are the partial derivatives\n    \n    \n\n\n       \n       \n   ","54a158fd":"### Both plots looks quite similar and has loss of 0.229","486b2383":"## If this notebook was helpful to you please do upvote and suggest changes if any.\n* if anyone wants to know the calculus part, please let me know.","502fa98a":"### Plot of predicted data","4c399e50":"# Loading data and importing packages"}}