{"cell_type":{"4d58879f":"code","2b926803":"code","f8b2474b":"code","0cffe2ac":"code","ed85a5d4":"code","a637d4f2":"code","71ffad05":"code","86595234":"code","d71f4ae1":"code","b6cf36d9":"code","4f3ec093":"code","ee35e52c":"code","c3bcae13":"code","223ea291":"code","51e29a7b":"markdown","a0c356a9":"markdown","993875b0":"markdown","ae12fbca":"markdown","e25bbdc4":"markdown"},"source":{"4d58879f":"from __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\nimport os\nimport time\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, Activation, Dropout, MaxPooling2D\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.applications import mobilenet_v2, inception_v3\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import layers\nimport matplotlib.pylab as plt\nfrom keras import backend as K\nfrom keras.models import load_model\nimport cv2\nimport numpy as np\n","2b926803":"data_dir = '..\/input\/plantleaf\/PlantVillage'\ntrain_dir = '..\/input\/plantleaf\/PlantVillage'\nvalidation_dir = '..\/input\/plantleaf\/PlantVillage'\n\n\ndef count(dir, counter=0):\n    \"returns number of files in dir and subdirs\"\n    for pack in os.walk(dir):\n        for f in pack[2]:\n            counter += 1\n    return dir + \" : \" + str(counter) + \"files\"\n\n\nprint('total images for training :', count(train_dir))\nprint('total images for validation :', count(validation_dir))\n\n\nIMAGE_SIZE = (256, 256)\nBATCH_SIZE = 10","f8b2474b":"classes = [\"Pepper__bell___Bacterial_spot\",\n           \"Pepper__bell___healthy\",\n           \"Potato___Early_blight\",\n           \"Potato___healthy\",\\\n           \"Potato___Late_blight\",\n           \"Tomato__Target_Spot\",\n           \"Tomato__Tomato_mosaic_virus\",\n           \"Tomato__Tomato_YellowLeaf__Curl_Virus\",\n           \"Tomato_Bacterial_spot\",\\\n           \"Tomato_Early_blight\",\n           \"Tomato_healthy\",\n           \"Tomato_Late_blight\",\n           \"Tomato_Leaf_Mold\",\n           \"Tomato_Septoria_leaf_spot\",\n           \"Tomato_Spider_mites_Two_spotted_spider_mite\"]\n\nprint(classes)\n\nprint('Number of classes:', len(classes))\n","0cffe2ac":"train_data_gen = ImageDataGenerator(\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_data_gen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_data_gen.flow_from_directory(\n    train_dir,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical')\n\nvalidation_generator = test_data_gen.flow_from_directory(\n    validation_dir,\n    target_size=IMAGE_SIZE,\n    batch_size=BATCH_SIZE,\n    class_mode='categorical')","ed85a5d4":"inputShape = (256,256,3)\nchanDim = -1\nif K.image_data_format() == \"channels_first\":\n    inputShape = (3,256,256)\n    chanDim = 1\nn_classes = 15","a637d4f2":"# RUN WHEN TRAINING \n\n# model = Sequential()\n# model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n# model.add(MaxPooling2D(pool_size=(3, 3)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(64, (3, 3), padding=\"same\"))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n\n# model.add(Conv2D(64, (3, 3), padding=\"same\"))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(128, (3, 3), padding=\"same\"))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n\n# model.add(Conv2D(128, (3, 3), padding=\"same\"))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n\n# model.add(Dropout(0.25))\n# model.add(Flatten())\n# model.add(Dense(512))\n# model.add(Activation(\"relu\"))\n\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(n_classes))\n# model.add(Activation(\"softmax\"))\n\n# model.save('kaggle_plant_model.h5')\n\n\n# model.summary()","71ffad05":"# RUN FOR TRAINING \n\n# opt = Adam(lr=0.001)\n\n# model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n\n# EPOCHS = 100\n\n\n# history = model.fit_generator(\n#     train_generator,\n#     steps_per_epoch=train_generator.samples \/\/ BATCH_SIZE,\n#     epochs=EPOCHS,\n#     validation_data=validation_generator,\n#     validation_steps=validation_generator.samples \/\/ BATCH_SIZE,\n#     verbose=1)\n","86595234":"# RUN FOR LOADING \n\n# model = Sequential()\n# model.add(Conv2D(32, (3, 3), padding=\"same\",input_shape=inputShape))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n# model.add(MaxPooling2D(pool_size=(3, 3)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(64, (3, 3), padding=\"same\"))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n\n# model.add(Conv2D(64, (3, 3), padding=\"same\"))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n# model.add(Dropout(0.25))\n\n# model.add(Conv2D(128, (3, 3), padding=\"same\"))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n\n# model.add(Conv2D(128, (3, 3), padding=\"same\"))\n# model.add(Activation(\"relu\"))\n# model.add(BatchNormalization(axis=chanDim))\n# model.add(MaxPooling2D(pool_size=(2, 2)))\n\n# model.add(Dropout(0.25))\n# model.add(Flatten())\n# model.add(Dense(512))\n# model.add(Activation(\"relu\"))\n\n# model.add(BatchNormalization())\n# model.add(Dropout(0.5))\n# model.add(Dense(n_classes))\n# model.add(Activation(\"softmax\"))\n\n#opt = Adam(lr=0.001)\n\n#model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])","d71f4ae1":"# RUN FOR LOADING\n# model.load_model(<path>)","b6cf36d9":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(EPOCHS)","4f3ec093":"plt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\nplt.ylabel(\"Accuracy (training and validation)\")\nplt.xlabel(\"Training Steps\")\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.ylabel(\"Loss (training and validation)\")\nplt.xlabel(\"Training Steps\")\nplt.show()","ee35e52c":"# def prepare(img_path):\n#     img = cv2.imread(img_path)\n#     x = np.asarray(img)\n#     x = 1.\/255\n#     return np.expand_dims(x, axis=0)","c3bcae13":"img = cv2.imread('..\/input\/plantleaf\/PlantVillage\/Tomato_Leaf_Mold\/50e1906e-24da-493d-88b0-3110e778a26a___Crnl_L.Mold 7135.JPG')\nplt.imshow(img)\nimg = img.reshape(1,256,256,3)\nprint(classes[model.predict_classes(img)[0]])","223ea291":"# result = model.predict_classes(\n#     [prepare('\/content\/drive\/My Drive\/LEAF_PROJECT\/test_images\/1.jpg')])\n# disease = cv2.imread('\/content\/drive\/My Drive\/LEAF_PROJECT\/test_images\/1.jpg')\n# plt.imshow(disease)\n# print(classes[int(result)])","51e29a7b":"## ------------ FOR LOADING MODEL ONLY----------------------","a0c356a9":"Import neccessary packages","993875b0":"## ------------ FOR TRAINING ONLY----------------------","ae12fbca":"## ____________________","e25bbdc4":"### Predicting"}}