{"cell_type":{"dbc97353":"code","7ce84585":"code","10291754":"code","4b626fb0":"code","cee3f251":"code","7e2b27a5":"code","902d9e30":"code","bd83c6d0":"code","5500e8c3":"code","a493a0fa":"code","28500d90":"code","a6ee2d88":"code","ec13450d":"code","058670a7":"code","d23e4d0d":"code","f1e75499":"code","9ed9ed0c":"code","ae73703c":"code","3b7576e7":"code","81dbdcaf":"code","e2a5e46b":"code","2dba721d":"code","ed12117e":"code","4bbb7cb4":"code","ce867708":"code","e800ab0d":"code","16506d9b":"code","6c53eae1":"code","23836ac0":"code","c5eb575b":"code","257f4a00":"code","440d7f27":"code","2f3d4cee":"code","693b6f0d":"code","bae72d49":"code","ed2da146":"code","08382298":"code","fc545129":"code","54d2d559":"code","9b3383c1":"code","32819573":"code","fd3a4f35":"code","9711aa6d":"code","240c3bd7":"code","f3486773":"code","01ec147b":"code","28a33036":"code","094d9be7":"code","e25e287d":"code","59308393":"code","90c7e0e0":"code","071200d3":"code","72db1709":"code","ca87e632":"code","1c4a8085":"code","731abc36":"code","6941b17e":"code","986356de":"code","62e39c46":"code","d61457fd":"code","f3d3a505":"code","43647eea":"code","aecb2b71":"code","5773740b":"code","92f194b3":"code","51e02f12":"code","02de3de3":"code","c16a86e2":"code","3fb4596a":"code","5b6818d9":"code","2b61f3f3":"code","3488fd63":"code","0770dd0d":"code","5569fe09":"code","a9546fb1":"code","5b7fdccf":"code","865c294a":"code","573766cf":"code","0922e0e9":"code","af3c8079":"code","ba48d577":"code","f54a3183":"code","75e7dbac":"code","1f50f987":"code","9f0d9d29":"code","21c9857c":"code","08d62265":"code","4d26533f":"code","2082b8ac":"code","b2691a1a":"code","f06390ae":"code","c1b11129":"code","f2207047":"code","9c34b8b6":"code","16dfa116":"code","684e497d":"code","39cb9a66":"code","a7b4066e":"code","1159c637":"code","92bf1756":"code","11961db2":"code","5a1db8bc":"markdown","21337670":"markdown","7839a54e":"markdown","0ba13b0a":"markdown","4b2566bd":"markdown","641afee1":"markdown","f323e582":"markdown","d7eceabe":"markdown","84536cb7":"markdown","cded54c7":"markdown","ca358040":"markdown","2c14d2d7":"markdown","b8da4881":"markdown","5414b18b":"markdown","7f1b5279":"markdown","fe45f0cb":"markdown","131b3811":"markdown","c4a5d5f0":"markdown","6758588f":"markdown","a4a035f7":"markdown","aca2ddbd":"markdown","c7619959":"markdown","26bb795d":"markdown","d32b0da6":"markdown","0b974230":"markdown","6bf215d9":"markdown","944778ea":"markdown","f574550f":"markdown","54e82b81":"markdown","976f26a3":"markdown","c8c314dc":"markdown","5414946d":"markdown","8d9cae8f":"markdown","1ecbae85":"markdown","7b3ae891":"markdown","aca31764":"markdown","e1eda1b8":"markdown","532f746e":"markdown","a31a887f":"markdown","e2bed677":"markdown","1cddd0fe":"markdown","4f132ceb":"markdown","b26aeeb4":"markdown","41809d18":"markdown","5269fb2f":"markdown","80cc8f53":"markdown","ed85cc75":"markdown","dddfee8f":"markdown","03731d27":"markdown","081ed3e2":"markdown","9ceb9586":"markdown","b83c48e9":"markdown","5b50432a":"markdown","391508ae":"markdown","fbf7c1e6":"markdown","03edd0cd":"markdown","3ebe91af":"markdown","e634ec33":"markdown","cd83780f":"markdown","94497283":"markdown","0970eb60":"markdown","c4b12d9d":"markdown","25dee813":"markdown","fa4062ff":"markdown","9aba53d6":"markdown","dd2cf0f2":"markdown","de560d72":"markdown","a98e4b83":"markdown","358150c3":"markdown","3a5ba342":"markdown","6e48c4da":"markdown","54d8d5e8":"markdown","71ac7ca8":"markdown","adcaf2b3":"markdown","26d838dd":"markdown","cdedc322":"markdown","76c6c3f0":"markdown","85fef44f":"markdown","a25aa841":"markdown","e4a4ffeb":"markdown","5127276f":"markdown","5146602e":"markdown","73de49f6":"markdown","fbe86b3d":"markdown","9aaf6edc":"markdown","efd2affc":"markdown","75fe5b4a":"markdown","11839ea5":"markdown","632779e3":"markdown","87079cba":"markdown","ba0db03e":"markdown","f5f4d0e2":"markdown","1cf60d0d":"markdown","a5a3d35b":"markdown","bd369db5":"markdown","c691d3b5":"markdown"},"source":{"dbc97353":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.dummy import DummyClassifier\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import fbeta_score\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.over_sampling import SMOTE\n","7ce84585":"df = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\ndf","10291754":"df.info()","4b626fb0":"# bmi missing values\ndf.isna().sum()","cee3f251":"df.describe()","7e2b27a5":"df[df['stroke']==1].describe()","902d9e30":"df[df['stroke']==0].describe()","bd83c6d0":"atttibutes_hist = df[[\"age\", \"avg_glucose_level\", \"bmi\"]].hist(bins=20, figsize=(20,15))\natttibutes_hist","5500e8c3":"fig, ax = plt.subplots(4,2, figsize = (12,12))\n((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8)) = ax\n\nlabels = df['gender'].value_counts().index.tolist()[:2]\nvalues = df['gender'].value_counts().tolist()[:2]\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.05])\nax1.set_title(\"Gender Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = [\"Not hypertension\", \"hypertension\"]\nvalues = df['hypertension'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2])\nax2.set_title(\"Hypertension Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = [\"There is not heart disease\", \"There is heart disease\"]\nvalues = df['heart_disease'].value_counts().tolist()\nax3.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2])\nax3.set_title(\"Heart disease Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = [\"married\", \"never married\"]\nvalues = df['ever_married'].value_counts().tolist()\nax4.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.05])\nax4.set_title(\"Marriage Distribution Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = [\"Private Job\", \"Self-employed\", \"Children\", \"Goverment Job\", \"Never Worked Before\"]\nvalues = df['work_type'].value_counts().tolist()\nax5.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0.1, 0.1, 0.1, 0.1, 0.2])\nax5.set_title(\"Work Type Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = [\"Urban Residence\", \"Rural Residence\"]\nvalues = df['Residence_type'].value_counts().tolist()\nax6.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.05])\nax6.set_title(\"Residence Type Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = [\"Never Smoked Before\", \"Unknown\", \"Smoked in the past\", \"Currently Smokes\"]\nvalues = df['smoking_status'].value_counts().tolist()\nax7.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0.03, 0.03, 0.03, 0.03])\nax7.set_title(\"Smoking Status Pie Chart\", fontdict={'fontsize': 14})\n\nlabels = [\"Didn't have Stroke\", \"Had Stroke\"]\nvalues = df['stroke'].value_counts().tolist()\nax8.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True, explode=[0, 0.2])\nax8.set_title(\"Stroke Pie Chart\", fontdict={'fontsize': 14})\n\nplt.tight_layout()\nplt.show()","a493a0fa":"bmi_mean = df[\"bmi\"].mean()\ndf[\"bmi\"].fillna(bmi_mean, inplace=True)\n\ndf.isna().sum() #just check","28500d90":"print(df['gender'].value_counts())\ndf = df[df['gender'] != \"Other\"]\n","a6ee2d88":"# just check:\ndf['gender'].value_counts()","ec13450d":"df_copy = df.copy()\ndf_result = pd.get_dummies(df_copy, columns=['hypertension', 'heart_disease', 'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], prefix=['hypertension', 'heart_disease', 'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'] )\ndf_result.rename(columns={\"hypertension_0\": \"no_hypertension\", \"hypertension_1\": \"yes_hypertension\", \"heart_disease_0\": \"no_heart_disease\", \"heart_disease_1\": \"yes_heart_disease\"}, inplace=True)\n\nprint(df_result.columns.tolist())\ndf_result","058670a7":"df_result.corr()","d23e4d0d":"corr_matrix = df_result.corr()\ncorr_matrix[\"stroke\"].sort_values(ascending = False)\n","f1e75499":"fig, ax = plt.subplots(figsize=(10,8))\nsns.heatmap(df_result[['stroke', 'age', 'avg_glucose_level', 'bmi']].corr(),annot=True)\n","9ed9ed0c":"fig, ax = plt.subplots(2,2, figsize = (12,12))\n((ax1, ax2), (ax3, ax4)) = ax\n\n# the \"no_\" attributes is the opposite to the \"yes_\" attributes so the correlation to stroke will be the same but negative.\nsns.heatmap(df_result[['stroke', 'yes_hypertension', 'yes_heart_disease']].corr(),annot=True, ax=ax1)\nsns.heatmap(df_result[['stroke', 'gender_Male', 'ever_married_Yes', 'Residence_type_Urban']].corr(),annot=True, ax=ax2)\nsns.heatmap(df_result[['stroke', 'work_type_Govt_job', 'work_type_Never_worked', 'work_type_Private', 'work_type_Self-employed', 'work_type_children']].corr(),annot=True, ax=ax3)\nsns.heatmap(df_result[['stroke', 'smoking_status_Unknown', 'smoking_status_formerly smoked', 'smoking_status_never smoked', 'smoking_status_smokes']].corr(),annot=True, ax=ax4)\n\nplt.tight_layout()\nplt.show()","ae73703c":"df = df_result\n\nfig, ax = plt.subplots(1,2, figsize = (14,5), )\n((ax1, ax2)) = ax\n\ndf.plot(ax=ax1, kind='scatter', x='age', y='stroke', alpha = 0.2)\ndf.plot(ax=ax2, kind='scatter', x='avg_glucose_level', y='stroke', alpha = 0.2)\n\nplt.tight_layout()\nplt.show()","3b7576e7":"fig, ax = plt.subplots(1,2, figsize = (12,12))\n((ax1, ax2)) = ax\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[df['yes_hypertension']==1]['stroke'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax1.set_title(\"stroke ratio - there is hypertention\", fontdict={'fontsize': 14})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[df['no_hypertension']==1]['stroke'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax2.set_title(\"stroke ratio - there isn't hypertention\", fontdict={'fontsize': 14})\n\nplt.tight_layout()\nplt.show()","81dbdcaf":"fig, ax = plt.subplots(1,2, figsize = (12,12))\n((ax1, ax2)) = ax\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[df['yes_heart_disease']==1]['stroke'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax1.set_title(\"stroke ratio - there is heart disease\", fontdict={'fontsize': 14})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[df['no_heart_disease']==1]['stroke'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax2.set_title(\"stroke ratio - there isn't heart disease\", fontdict={'fontsize': 14})\n\nplt.tight_layout()\nplt.show()","e2a5e46b":"fig, ax = plt.subplots(1,2, figsize = (12,12))\n((ax1, ax2)) = ax\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[df['ever_married_Yes']==1]['stroke'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax1.set_title(\"stroke ratio - merried\", fontdict={'fontsize': 14})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[df['ever_married_No']==1]['stroke'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax2.set_title(\"stroke ratio - not merried\", fontdict={'fontsize': 14})\n\nplt.tight_layout()\nplt.show()","2dba721d":"corr_matrix = df_result.corr()\nbmi_corr = corr_matrix[\"bmi\"].sort_values(ascending = False).drop('bmi')\nprint(bmi_corr[bmi_corr>0.15])\nprint(bmi_corr[bmi_corr<-0.15])","ed12117e":"df.plot.scatter( x='bmi', y='stroke', alpha = 0.05, title=\"stroke by bmi\")","4bbb7cb4":"# just a little check that the pattern above is realy exist and not because the plot density:\n\nvalues_30plusminusBMI = df[(df['bmi']>27) & (df['bmi']<33)]['stroke'].value_counts().tolist()\nvalues_stroke = df['stroke'].value_counts().tolist()\n\nprint(\"-+30bmi without stroke cases : all wothiut stroke cases (ratio) = \" + str(values_30plusminusBMI[0]\/values_stroke[0]))\nprint(\"-+30bmi : all observations (ratio) = \" + str(sum(values_30plusminusBMI)\/sum(values_stroke)))\nprint(\"-+30bmi with stroke cases : all stroke cases (ratio) = \" + str(values_30plusminusBMI[1]\/values_stroke[1]))\nprint(\"as we can see, among 1\/2 of the stroke cases the bmi is around 30. In contrast to cases where there is no stroke where the ratio is significantly lower, only 1\/3.\")","ce867708":"df = df_result\n\nfig, ax = plt.subplots(1,2, figsize = (14,5))\n((ax1, ax2)) = ax\n\ndf[df['stroke'] ==0].plot.scatter(ax=ax1, x='bmi', y='avg_glucose_level', alpha = 0.2, c='gray', label='no stroke')\ndf[df['stroke'] ==1].plot.scatter(ax=ax1, x='bmi', y='avg_glucose_level', alpha = 0.8, c='orange', label='stroke')\nax1.legend()\nax1.set_title('stroke by combination of bmi and avg_glucose_level')\n\ndf[df['stroke'] ==0].plot.scatter(ax=ax2, x='bmi', y='age', alpha = 0.3, c='gray', label='no stroke')\ndf[df['stroke'] ==1].plot.scatter(ax=ax2, x='bmi', y='age', alpha = 0.6, c='orange', label='stroke')\nax2.legend()\nax2.set_title('stroke by combination of bmi and age')\n\nplt.tight_layout()\nplt.show()","e800ab0d":"fig, ax = plt.subplots(2,2, figsize = (12,12))\n((ax1, ax2), (ax3, ax4)) = ax\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['ever_married_Yes']==1) & (df['bmi']>27) & (df['bmi']<33)]['stroke'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax1.set_title(\"stroke ratio - merried and have 27<BMI<33\", fontdict={'fontsize': 14})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['ever_married_Yes']==1) & ((df['bmi']<27) | (df['bmi']>33))]['stroke'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax2.set_title(\"stroke ratio - merried and have 27>BMI or BMI>33\", fontdict={'fontsize': 14})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['ever_married_No']==1) & (df['bmi']>27) & (df['bmi']<33)]['stroke'].value_counts().tolist()\nax3.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax3.set_title(\"stroke ratio - not merried and have 27<BMI<33\", fontdict={'fontsize': 14})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['ever_married_No']==1) & ((df['bmi']<27) | (df['bmi']>33))]['stroke'].value_counts().tolist()\nax4.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax4.set_title(\"stroke ratio - not merried and have 27>BMI or BMI>33\", fontdict={'fontsize': 14})\n\nplt.tight_layout()\nplt.show()","16506d9b":"fig, ax = plt.subplots(2,2, figsize = (12,12))\n((ax1, ax2), (ax3, ax4)) = ax\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['yes_hypertension']==1) & (df['bmi']>27) & (df['bmi']<33)]['stroke'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax1.set_title(\"stroke ratio - have hypertension and have 27<BMI<33\", fontdict={'fontsize': 10})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['yes_hypertension']==1) & ((df['bmi']<27) | (df['bmi']>33))]['stroke'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax2.set_title(\"stroke ratio - have hypertension and have 27>BMI or BMI>33\", fontdict={'fontsize': 10})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['no_hypertension']==1) & (df['bmi']>27) & (df['bmi']<33)]['stroke'].value_counts().tolist()\nax3.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax3.set_title(\"stroke ratio -do not have hypertension and have 27<BMI<33\", fontdict={'fontsize': 10})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['no_hypertension']==1) & ((df['bmi']<27) | (df['bmi']>33))]['stroke'].value_counts().tolist()\nax4.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax4.set_title(\"stroke ratio - do not have hypertension and have 27>BMI or BMI>33\", fontdict={'fontsize': 10})\n\nplt.tight_layout()\nplt.show()","6c53eae1":"fig, ax = plt.subplots(2,2, figsize = (12,12))\n((ax1, ax2), (ax3, ax4)) = ax\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['yes_heart_disease']==1) & (df['bmi']>27) & (df['bmi']<33)]['stroke'].value_counts().tolist()\nax1.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax1.set_title(\"stroke ratio - have heart disease background and have 27<BMI<33\", fontdict={'fontsize': 10})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['yes_heart_disease']==1) & ((df['bmi']<27) | (df['bmi']>33))]['stroke'].value_counts().tolist()\nax2.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax2.set_title(\"stroke ratio - have heart disease background and have 27>BMI or BMI>33\", fontdict={'fontsize': 10})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['no_heart_disease']==1) & (df['bmi']>27) & (df['bmi']<33)]['stroke'].value_counts().tolist()\nax3.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax3.set_title(\"stroke ratio -do not have heart disease background and have 27<BMI<33\", fontdict={'fontsize': 10})\n\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = df[(df['no_heart_disease']==1) & ((df['bmi']<27) | (df['bmi']>33))]['stroke'].value_counts().tolist()\nax4.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax4.set_title(\"stroke ratio - do not have heart disease background and have 27>BMI or BMI>33\", fontdict={'fontsize': 10})\n\nplt.tight_layout()\nplt.show()","23836ac0":"X = df.drop(columns=['stroke', 'id'])\nY = df['stroke']\nsplit = StratifiedShuffleSplit(n_splits=1, random_state=42, test_size=0.2)\nfor train_index, test_index in split.split(X, Y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\nX = {'train': X_train, 'test': X_test}\ny = {'train': y_train, 'test': y_test}","c5eb575b":"print(y_train.describe()[:3], y_test.describe()[:3])","257f4a00":"scaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()","440d7f27":"scaler.fit(X_train[['age', 'avg_glucose_level', 'bmi']])\nX_train_scaled[['age', 'avg_glucose_level', 'bmi']] = scaler.transform(X_train_scaled[['age', 'avg_glucose_level', 'bmi']])\n\nX_train_scaled.describe()","2f3d4cee":"# the scaling is with the the same fitted scaler (by the train data)\nX_test_scaled[['age', 'avg_glucose_level', 'bmi']] = scaler.transform(X_test_scaled[['age', 'avg_glucose_level', 'bmi']])","693b6f0d":"# This function will serve us well in the next section:\n\ndef cf_matrix_show(cf_matrix, beta = 4):\n\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cf_matrix.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')\n\n    accuracy  = np.trace(cf_matrix) \/ float(np.sum(cf_matrix))\n    precision = cf_matrix[1,1] \/ sum(cf_matrix[:,1])\n    recall    = cf_matrix[1,1] \/ sum(cf_matrix[1,:])\n    f1_score  = 2*precision*recall \/ (precision + recall)\n    fb_score  = (1+beta**2)*precision*recall \/ ((beta**2)*precision + recall)\n    stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\\nFb Score={:0.3f}\".format(\n                    accuracy,precision,recall,f1_score, fb_score)\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label' + stats_text)","bae72d49":"# try K=1 through K=30 and plot testing accuracy\nk_range = list(range(1, 31))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled, y_train)\n    y_pred = knn.predict(X_test_scaled)\n    scores.append(metrics.accuracy_score(y_test, y_pred))\n    \nplt.plot(k_range, scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Testing Accuracy')","ed2da146":"# try K=1 through K=30 and plot testing accuracy\nk_range = list(range(1, 31))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled, y_train)\n    y_pred = knn.predict(X_train_scaled)\n    scores.append(metrics.accuracy_score(y_train, y_pred))\n    \nplt.plot(k_range, scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Testing Accuracy')","08382298":"max_score = max(scores)\nbest_K = scores.index(max(scores))+1\n\nprint(\"the first k value with best score is \" + str(best_K) + \" with accuracy of \" + str(max_score))","fc545129":"k_range = list(range(1, 31))\nk_mean_scores = []\nk_std_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train_scaled, y_train, cv=10, scoring='accuracy')\n    k_mean_scores.append(scores.mean())\n    k_std_scores.append(scores.std())\n\nd = {'k_mean': k_mean_scores, 'k_std': k_std_scores}\nKs = pd.DataFrame(data = d, index=k_range)","54d2d559":"fig, ax = plt.subplots(1,2, figsize = (12,5))\n((ax1, ax2)) = ax\n\nax1.plot(k_range, k_mean_scores)\nax1.set_title('mean acurracy by K')\nax1.set_xlabel('Value of K for KNN')\nax1.set_ylabel('Testing Mean Accuracy')\n\nax2.plot(k_range, k_std_scores)\nax2.set_title('std scores by K')\nax2.set_xlabel('Value of K for KNN')\nax2.set_ylabel('Testing STD')\n\nplt.tight_layout()\nplt.show()\n\nmax_score = max(k_mean_scores)\nmin_std = min(k_std_scores)\nbest_Ks = Ks[(Ks['k_mean']==max_score) & (Ks['k_std']==min_std)]\nprint(\"best mean acuarracy is: \" +str(max_score))\nprint(\"the best K's are:\")\nprint(best_Ks)\nprint(\"in this case I will choose the smallest k between those K's for the model, to make it the as simple as we can.\")","9b3383c1":"min(best_Ks['k_mean'].index)","32819573":"k_range = list(range(1, 31))\nk_mean_scores = []\nk_std_scores = []\n\nfeatures = list(X_train_scaled.columns.values)\nfor feature in features:\n    knn = KNeighborsClassifier(n_neighbors= min(best_Ks['k_mean'].index))\n    scores = cross_val_score(knn, X_train_scaled.drop(columns=feature), y_train, cv=10, scoring='accuracy')\n    k_mean_scores.append(scores.mean())\n    k_std_scores.append(scores.std())\n\n","fd3a4f35":"# check if there is an feature worthe to remove\nfig = plt.gcf()\nplt.plot(features, k_mean_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('Testing Accuracy')\nplt.xticks(rotation='vertical')\n\nfig.set_size_inches(18.5, 10.5)","9711aa6d":"knn = KNeighborsClassifier(n_neighbors=min(best_Ks['k_mean'].index))\nknn.fit(X_train_scaled, y_train)\ny_predict = knn.predict(X_test_scaled)\ncf_matrix_show(confusion_matrix(y_test, y_predict))","240c3bd7":"oversample = SMOTE()\nX, y = oversample.fit_resample(X_train_scaled, y_train)\nupsampled_df = X.assign(Stroke = y)\n\nfig, ax = plt.subplots(1,1, figsize = (12,12))\nlabels = [\"deosn't have stroke\", \"have stroke\"]\nvalues = upsampled_df['Stroke'].value_counts().tolist()\n\nax.pie(x=values, labels=labels, autopct=\"%1.2f%%\", shadow=True)\nax.set_title(\"stroke ratio - after over sampeling:\", fontdict={'fontsize': 15})\nplt.show()\nprint(\"there are now equal number of cases with stroke and without: \" +str(values))","f3486773":"# split the target and features\nX_train_scaled_upsample = upsampled_df.drop(columns='Stroke')\ny_train_upsamle = upsampled_df['Stroke']","01ec147b":"print(\"here we can se the 95% as false accuracy, without any true positive, so this is useless dummy model.\")\n\nplt.subplot(1,2,1)\nplt.title('most_frequent (not have stroke always)')\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train_scaled, y_train)\ny_predict_dummy = dummy_clf.predict(X_test_scaled)\ncf_matrix_show(confusion_matrix(y_test, y_predict_dummy))\n\nplt.subplot(1,2,2)\nplt.title('constant (have stroke)')\ndummy_clf = DummyClassifier(strategy=\"constant\", constant=1)\ndummy_clf.fit(X_train_scaled, y_train)\ny_predict_dummy = dummy_clf.predict(X_test_scaled)\ncf_matrix_show(confusion_matrix(y_test, y_predict_dummy))\n\nplt.tight_layout()\nplt.show()","28a33036":"plt.subplot(1,2,1)\nplt.title('stratified')\ndummy_clf = DummyClassifier(strategy=\"stratified\")\ndummy_clf.fit(X_train_scaled, y_train)\ny_predict_dummy = dummy_clf.predict(X_test_scaled)\ncf_matrix_show(confusion_matrix(y_test, y_predict_dummy))\n\nplt.subplot(1,2,2)\nplt.title('uniform')\ndummy_clf = DummyClassifier(strategy=\"uniform\")\ndummy_clf.fit(X_train_scaled_upsample, y_train_upsamle)\ny_predict_dummy = dummy_clf.predict(X_test_scaled)\ncf_matrix_show(confusion_matrix(y_test, y_predict_dummy))\n\nplt.tight_layout()\nplt.show()","094d9be7":"# ideal result\ncf_matrix_show(confusion_matrix(y_test, y_test))","e25e287d":"# try K=1 through K=130 and plot testing F-beta\nk_range = list(range(1, 131, 15))\nbeta=4\n\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled_upsample, y_train_upsamle)\n    y_pred = knn.predict(X_test_scaled)\n    scores.append(metrics.fbeta_score(y_test, y_pred, beta=beta))\nplt.plot(k_range, scores)\n\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled_upsample, y_train_upsamle)\n    y_pred = knn.predict(X_train_scaled_upsample)\n    scores.append(metrics.fbeta_score(y_train_upsamle, y_pred, beta=beta))\nplt.plot(k_range, scores)\n\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train_scaled_upsample, y_train_upsamle)\n    y_pred = knn.predict(X_train_scaled)\n    scores.append(metrics.fbeta_score(y_train, y_pred, beta=beta))\n    \nplt.plot(k_range, scores)\n\nplt.xlabel('Value of K for KNN')\nplt.ylabel('F beta')\nplt.legend(labels=['test set', 'upsample train set', 'non-upsample train set'])\nplt.show()","59308393":"from imblearn.pipeline import make_pipeline\n\nbeta = 4\nk_range = range(1,180, 15)\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\nrecall_scores = []\nprecision_scores = []\nFb_scores = []\n\nfor k in k_range:\n    imba_pipeline = make_pipeline(SMOTE(random_state=42), \n                              KNeighborsClassifier(n_neighbors= k))\n\n    \n    recall = cross_val_score(imba_pipeline, X_train_scaled, y_train, scoring='recall', cv=kf).mean()\n    recall_scores.append(recall)\n    precision = cross_val_score(imba_pipeline, X_train_scaled, y_train, scoring='precision', cv=kf).mean()\n    precision_scores.append(precision)\n    fb_score = (1+beta**2)*precision*recall \/ ((beta**2)*precision + recall)\n    Fb_scores.append(fb_score)\n\npd.set_option('display.max_rows', df.shape[0]+1)    \nd = {'Fb': Fb_scores, 'recall': recall_scores, 'precision': precision_scores}\nKs = pd.DataFrame(data = d, index=k_range)\nKs","90c7e0e0":"beta = 4\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\nk_range = range(1 ,161, 10)\ncorr_limits = [0, 0.01, 0.02, 0.04, 0.05, 0.06, 0.08, 0.1]\n\nFb_mean = []\nFb = []\n\nfor limit in corr_limits:\n    X_tr_copy = X_train_scaled.copy()\n    corr_matrix = df_result.corr()\n    stroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\n    s = (stroke_corr < limit)&(stroke_corr>-limit)\n    s.drop(labels=['id', 'stroke'], inplace=True)\n    X_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\n\n    Fb_scores = []\n    \n    for k in k_range:\n        imba_pipeline = make_pipeline(SMOTE(random_state=42), \n                                  KNeighborsClassifier(n_neighbors= k))\n\n        recall = cross_val_score(imba_pipeline, X_tr_copy, y_train, scoring='recall', cv=kf).mean()\n        precision = cross_val_score(imba_pipeline, X_tr_copy, y_train, scoring='precision', cv=kf).mean()\n        fb_score = (1+beta**2)*precision*recall \/ ((beta**2)*precision + recall)\n        Fb_scores.append(fb_score)\n    \n    Fb_mean.append(sum(Fb_scores)\/len(Fb_scores))\n    Fb.append(Fb_scores)\n","071200d3":"d = {'Fb': Fb_mean}\ncorr_lim_Fb = pd.DataFrame(data = d, index=corr_limits)\nbest_corr_lim = corr_lim_Fb['Fb'].idxmax()\nbest_mean_Fb = corr_lim_Fb['Fb'].max()\nprint(\"best correlation limit is: \" + str(best_corr_lim) + \" , with mean F-beta of \" + str(best_mean_Fb))\ncorr_lim_Fb\n","72db1709":"recall_scores = []\nprecision_scores = []\nFb_scores = []\n\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\n\nlimit = best_corr_lim\nX_tr_copy = X_train_scaled.copy()\ncorr_matrix = df_result.corr()\nstroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\ns = (stroke_corr < limit)&(stroke_corr>-limit)\ns.drop(labels=['id', 'stroke'], inplace=True)\nX_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\n\nk_range = range(1 ,161)\nfor k in k_range:\n    imba_pipeline = make_pipeline(SMOTE(random_state=42), \n                              KNeighborsClassifier(n_neighbors= k))\n    \n    recall = cross_val_score(imba_pipeline, X_tr_copy, y_train, scoring='recall', cv=kf).mean()\n    recall_scores.append(recall)\n    precision = cross_val_score(imba_pipeline, X_tr_copy, y_train, scoring='precision', cv=kf).mean()\n    precision_scores.append(precision)\n    fb_score = (1+beta**2)*precision*recall \/ ((beta**2)*precision + recall)\n    Fb_scores.append(fb_score)  \n\npd.set_option('display.max_rows', df.shape[0]+1)    \nd = {'Fb': Fb_scores, 'recall': recall_scores, 'precision': precision_scores}\nKs_best_corr_lim = pd.DataFrame(data = d, index=k_range)","ca87e632":"fig = plt.gcf()\nfig.set_size_inches(18.5, 10.5)\nplt.plot(k_range, recall_scores)\nplt.plot(k_range, precision_scores)\nplt.xlabel('Value of K for KNN')\nplt.ylabel('re-call VS precision score')\n\nprint(\"re-call VS precision score by k:\")\n\nplt.show()","1c4a8085":"best_Fb = Ks_best_corr_lim['Fb'].max()\nbest_K = Ks_best_corr_lim['Fb'].idxmax()\nprint(\"best K is: \" + str(best_K) + \" , with Fbeta of \" + str(best_Fb))\nKs_best_corr_lim","731abc36":"print(\"here it is:\")\nKs_best_corr_lim.loc[[best_K]]","6941b17e":"best_imba_pipeline = make_pipeline(SMOTE(random_state=42), \n                              KNeighborsClassifier(n_neighbors= best_K))\n\nlimit = best_corr_lim\nX_tr_copy = X_train_scaled.copy()\ncorr_matrix = df_result.corr()\nstroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\ns = (stroke_corr < limit)&(stroke_corr>-limit)\ns.drop(labels=['id', 'stroke'], inplace=True)\nX_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\n\npredict_probability = cross_val_predict(best_imba_pipeline, X_tr_copy, y_train, cv=kf, method='predict_proba')\npredict_probability\n\n# keep probabilities for the positive outcome only\nstroke_prob = predict_probability[:, 1]\nprecision, recall, thresholds = precision_recall_curve(y_train, stroke_prob)\n# convert to f-beta score\nfb_score = ((1+beta**2) * precision * recall) \/ ((beta**2)*precision + recall)\n# locate the index of the largest fb score\nnp.nan_to_num(fb_score, nan=0, copy=False)\nix = fb_score.argmax(axis=0)\n\nprint('Best Threshold = %f with Fb-Score of %.3f' % (thresholds[ix], fb_score[ix]))","986356de":"best_imba_pipeline = make_pipeline(SMOTE(random_state=42), \n                              KNeighborsClassifier(n_neighbors= best_K))\nlimit = best_corr_lim\nX_tr_copy = X_train_scaled.copy()\nX_test_copy = X_test_scaled.copy()\ncorr_matrix = df_result.corr()\nstroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\ns = (stroke_corr < limit)&(stroke_corr>-limit)\ns.drop(labels=['id', 'stroke'], inplace=True)\nX_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\nX_test_copy = X_test_copy[X_test_copy.columns[~s]]\n\nbest_imba_pipeline.fit(X_tr_copy, y_train)\ny_prob = best_imba_pipeline.predict_proba(X_test_copy)\n# keep probabilities for the positive outcome only\ny_prob = y_prob[:, 1]\ny_predict = np.where(y_prob < thresholds[ix], 0, 1)\n\ncf_matrix_show(confusion_matrix(y_test, y_predict), beta=4)","62e39c46":"limit = best_corr_lim\nX_tr_copy = X_train_scaled_upsample.copy()\nX_test_copy = X_test_scaled.copy()\ncorr_matrix = df_result.corr()\nstroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\ns = (stroke_corr < limit)&(stroke_corr>-limit)\ns.drop(labels=['id', 'stroke'], inplace=True)\nX_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\nX_test_copy = X_test_copy[X_test_copy.columns[~s]]\n\nknn = KNeighborsClassifier(n_neighbors=best_K)\nknn.fit(X_tr_copy, y_train_upsamle)\ny_predict = knn.predict(X_test_copy)\n\ncf_matrix_show(confusion_matrix(y_test, y_predict), beta=4)","d61457fd":"limit = best_corr_lim\nX_tr_copy = X_train_scaled.copy()\nX_test_copy = X_test_scaled.copy()\ncorr_matrix = df_result.corr()\nstroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\ns = (stroke_corr < limit)&(stroke_corr>-limit)\ns.drop(labels=['id', 'stroke'], inplace=True)\nX_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\nX_test_copy = X_test_copy[X_test_copy.columns[~s]]","f3d3a505":"from imblearn.pipeline import Pipeline\n\nimba_pipeline = Pipeline([\n    ('sampling', SMOTE(random_state=42)), \n    ('classification', KNeighborsClassifier())])","43647eea":"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import fbeta_score, make_scorer\nf_4_scorer = make_scorer(fbeta_score, beta=4)\n\nknn_param_grid = [\n    \n    {\n     'classification__n_neighbors': list(range(20,261)),\n     'classification__weights':['uniform','distance'],\n     'classification__metric': ['euclidean','manhattan','minkowski'],\n    }\n    \n]\n\ngrid_search_knn = GridSearchCV(\n    imba_pipeline,\n    param_grid=knn_param_grid,\n    cv=kf,\n    scoring=f_4_scorer,\n    return_train_score=True,\n)\ngrid_search_knn.fit(X=X_tr_copy,y=y_train)","aecb2b71":"grid_search_knn.best_score_","5773740b":"best = grid_search_knn.best_estimator_\nbest","92f194b3":"best_knn_pipeline = make_pipeline(SMOTE(random_state=42), \n                              best)\n\nlimit = best_corr_lim\nX_tr_copy = X_train_scaled.copy()\nX_test_copy = X_test_scaled.copy()\ncorr_matrix = df_result.corr()\nstroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\ns = (stroke_corr < limit)&(stroke_corr>-limit)\ns.drop(labels=['id', 'stroke'], inplace=True)\nX_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\nX_test_copy = X_test_copy[X_test_copy.columns[~s]]\n\nbest_knn_pipeline.fit(X_tr_copy, y_train)\ny_predict = best.predict(X_test_copy)\n\ncf_matrix_show(confusion_matrix(y_test, y_predict), beta=4)","51e02f12":"best_knn_pipeline = make_pipeline(SMOTE(random_state=42), \n                              best)\nlimit = best_corr_lim\nX_tr_copy = X_train_scaled.copy()\ncorr_matrix = df_result.corr()\nstroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\ns = (stroke_corr < limit)&(stroke_corr>-limit)\ns.drop(labels=['id', 'stroke'], inplace=True)\nX_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\n\npredict_probability = cross_val_predict(best_knn_pipeline, X_tr_copy, y_train, cv=kf, method='predict_proba')\n\nbeta=4\n# keep probabilities for the positive outcome only\nstroke_prob = predict_probability[:, 1]\nprecision, recall, thresholds = precision_recall_curve(y_train, stroke_prob)\n# convert to f-beta score\nfb_score = ((1+beta**2) * precision * recall) \/ ((beta**2)*precision + recall)\n# locate the index of the largest fb score\nnp.nan_to_num(fb_score, nan=0, copy=False)\nix = fb_score.argmax(axis=0)\n\nprint('Best Threshold = %f with Fb-Score of %.3f' % (thresholds[ix], fb_score[ix]))","02de3de3":"# test the final model:\n\nlimit = best_corr_lim\nX_tr_copy = X_train_scaled.copy()\nX_test_copy = X_test_scaled.copy()\ncorr_matrix = df_result.corr()\nstroke_corr = corr_matrix[\"stroke\"].sort_values(ascending = False)\ns = (stroke_corr < limit)&(stroke_corr>-limit)\ns.drop(labels=['id', 'stroke'], inplace=True)\nX_tr_copy = X_tr_copy[X_tr_copy.columns[~s]]\nX_test_copy = X_test_copy[X_test_copy.columns[~s]]\n\nbest_knn_pipeline.fit(X_tr_copy, y_train)\ny_prob = best.predict_proba(X_test_copy)\n# keep probabilities for the positive outcome only\ny_prob = y_prob[:, 1]\ny_predict = np.where(y_prob <= thresholds[ix], 0, 1)\n\ncf_matrix_show(confusion_matrix(y_test, y_predict), beta=4)\n","c16a86e2":"list(best_imba_pipeline)[1].get_params()","3fb4596a":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=200) # randomly choose\nclf.fit(X_train_scaled_upsample, y_train_upsamle)\n \n# Predict on training set\npred_y = clf.predict(X_train_scaled) \n\n# How's our Fbeta?\nprint('on train-set scores (over-fitting):')\nprint('test-set')\nprint('Fb: '+ str(metrics.fbeta_score(y_train, pred_y, beta=4)) )\nprint('recall:' + str(metrics.recall_score(y_train, pred_y)) )\nprint('precision:' + str(metrics.precision_score(y_train, pred_y)) )\nprint()\n\n# Predict on training set\npred_y = clf.predict(X_test_scaled)\n\n# How's our Fbeta?\nprint('on test-set scores (very low):')\nprint('Fb: '+ str(metrics.fbeta_score(y_test, pred_y, beta=4)) )\nprint('recall:' + str(metrics.recall_score(y_test, pred_y)) )\nprint('precision:' + str(metrics.precision_score(y_test, pred_y)) )\nprint()","5b6818d9":"#### feature importance:","2b61f3f3":"feature_imp = pd.Series(clf.feature_importances_,index=X_train_scaled_upsample.columns).sort_values(ascending=False)\nfeature_imp","3488fd63":"# copy X with columns wich grater than limit:\ndef copy_by_imp_limit(X, lim, limits):\n    X_copy = X.copy()\n    s = (feature_imp < lim)\n    X_copy = X_copy[X_copy.columns[~s]]\n    return X_copy","0770dd0d":"def Fb_recall_precision(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    pred_y = clf.predict(X_test)\n    fb_score = metrics.fbeta_score(y_test, pred_y, beta=4) \n    recall = metrics.recall_score(y_test, pred_y) \n    precision = metrics.precision_score(y_test, pred_y)\n    return fb_score, recall, precision\n\nbeta = 4\nkf = KFold(n_splits=5, random_state=42, shuffle=True)\ndef Fb_recall_precision_CV(model, X_train, y_train):\n    recall = cross_val_score(model, X_train, y_train, scoring='recall', cv=kf).mean()\n    precision = cross_val_score(model, X_train, y_train, scoring='precision', cv=kf).mean()\n    fb_score = (1+beta**2)*precision*recall \/ ((beta**2)*precision + recall)\n    return fb_score, recall, precision","5569fe09":"def df_Fb_recall_precision(Fb_list, recall_list, precision_list):\n    d = {'Fb': Fb_list, 'recall': recall_list, 'precision': precision_list}\n    return pd.DataFrame(data = d, index=importance_limit)","a9546fb1":"importance_limit = [0, 0.01, 0.02, 0.04, 0.13, 0.132, 0.14]\n    \nFb_list = []\nrecall_list = []\nprecision_list = []\n\nfor limit in importance_limit:\n    X_tr_copy = copy_by_imp_limit(X_train_scaled_upsample, limit, feature_imp)\n    X_test_copy = copy_by_imp_limit(X_test_scaled, limit, feature_imp)\n    \n    fb_score, recall, precision = Fb_recall_precision(clf, X_tr_copy, y_train_upsamle, X_test_copy, y_test)\n\n    Fb_list.append(fb_score)\n    recall_list.append(recall)\n    precision_list.append(precision)\n    ","5b7fdccf":"df_scores_limits = df_Fb_recall_precision(Fb_list, recall_list, precision_list)\nbest_imp_lim = df_scores_limits['Fb'].idxmax()\nbest_imp_Fb = df_scores_limits['Fb'].max()\ndf_scores_limits","865c294a":"print(\"best importance limit is: \" + str(best_imp_lim) + \" , with mean F-beta of \" + str(best_imp_Fb))","573766cf":"from imblearn.pipeline import Pipeline\n\nf_4_scorer = make_scorer(fbeta_score, beta=4)\nimba_pipeline_RFC = Pipeline([\n    ('sampling', SMOTE(random_state=42)), \n    ('classification', RandomForestClassifier())])","0922e0e9":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'classification__n_estimators': n_estimators,\n               'classification__max_features': max_features,\n               'classification__max_depth': max_depth,\n               'classification__min_samples_split': min_samples_split,\n               'classification__min_samples_leaf': min_samples_leaf,\n               'classification__bootstrap': bootstrap}","af3c8079":"kf = KFold(n_splits=5, random_state=42, shuffle=True)\n# search across 100 different combinations, and use all availimba_pipelineores\nrf_random = RandomizedSearchCV(estimator = imba_pipeline_RFC, param_distributions = random_grid, n_iter = 100, cv = kf, verbose=2, random_state=42, n_jobs = -1, scoring=f_4_scorer)\n# Fit the random search model\nrf_random.fit(X_train_scaled, y_train)","ba48d577":"print(\"best score is:\")\nprint(rf_random.best_score_)","f54a3183":"print(\"current best params:\")\nrf_random.best_params_","75e7dbac":"rf = rf_random.best_estimator_\nrf.fit(X_train_scaled_upsample, y_train_upsamle)\ny_predict = rf.predict(X_test_scaled)\ncf_matrix_show(confusion_matrix(y_test, y_predict), beta=4)","1f50f987":"current_best_model = list(rf)[1]\nfeature_imp = pd.Series(current_best_model.feature_importances_,index=X_train_scaled_upsample.columns).sort_values(ascending=False)\nfeature_imp","9f0d9d29":"importance_limit = [0, 0.01, 0.02, 0.03, 0.04, 0.06, 0.08, 0.083, 0.09]  \n    \nFb_list = []\nrecall_list = []\nprecision_list = []\n\ncurrent_best_pipeline_RFC = Pipeline([\n    ('sampling', SMOTE(random_state=42)), \n    ('classification', current_best_model)])\n\nfor limit in importance_limit:\n    X_tr_copy = copy_by_imp_limit(X_train_scaled, limit, feature_imp)\n    \n    Fb_score, recall, precision = Fb_recall_precision_CV(current_best_pipeline_RFC, X_tr_copy, y_train)\n\n    Fb_list.append(Fb_score)\n    recall_list.append(recall)\n    precision_list.append(precision)\n\nlimits = df_Fb_recall_precision(Fb_list, recall_list, precision_list)\nlimits","21c9857c":"X_train_copy = copy_by_imp_limit(X_train_scaled, 0.09, feature_imp)\nX_test_copy = copy_by_imp_limit(X_test_scaled, 0.09, feature_imp)\n\ncurrent_best_pipeline_RFC.fit(X_train_copy, y_train)\ny_predict = current_best_pipeline_RFC.predict(X_test_copy)\n\ncf_matrix_show(confusion_matrix(y_test, y_predict), beta=4)","08d62265":"main_scores = pd.DataFrame(rf_random.cv_results_)[['rank_test_score', 'mean_test_score', 'std_test_score', 'param_classification__n_estimators', 'param_classification__min_samples_split', 'param_classification__min_samples_leaf', 'param_classification__max_features', 'param_classification__max_depth', 'param_classification__bootstrap']]\nmain_scores.sort_values(by=['rank_test_score']).head()","4d26533f":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 600, num = 100)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [5, 10]\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2,3,4,5,6]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [4,6,8]\n\n# Method of selecting samples for training each tree\nbootstrap = [True]\n\n# Create the param grid\nrfc_param_grid = [{'classification__n_estimators': n_estimators,\n               'classification__max_features': max_features,\n               'classification__max_depth': max_depth,\n               'classification__min_samples_split': min_samples_split,\n               'classification__min_samples_leaf': min_samples_leaf,\n               'classification__bootstrap': bootstrap}]","2082b8ac":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start =100 , stop = 500, num = 100)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [6, 10]\n# Minimum number of samples required to split a node\nmin_samples_split = [2,5]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [4,8]\n# Method of selecting samples for training each tree\nbootstrap = [True]\n# Create the param grid\nrfc_param_grid = [{'classification__n_estimators': n_estimators,\n               'classification__max_features': max_features,\n               'classification__max_depth': max_depth,\n               'classification__min_samples_split': min_samples_split,\n               'classification__min_samples_leaf': min_samples_leaf,\n               'classification__bootstrap': bootstrap}]","b2691a1a":"kf = KFold(n_splits=3, random_state=42, shuffle=True)\n\ngrid_search_RFC = GridSearchCV(\n    imba_pipeline_RFC,\n    param_grid=rfc_param_grid,\n    cv=kf,\n    scoring=f_4_scorer,\n    return_train_score=True,\n)\ngrid_search_RFC.fit(X_train_scaled, y_train)","f06390ae":"main_RFC_scores = pd.DataFrame(grid_search_RFC.cv_results_)[['rank_test_score', 'mean_test_score', 'std_test_score', 'param_classification__n_estimators', 'param_classification__min_samples_split', 'param_classification__min_samples_leaf', 'param_classification__max_features', 'param_classification__max_depth', 'param_classification__bootstrap']]\nmain_RFC_scores.sort_values(by=['rank_test_score']).head()","c1b11129":"grid_search_RFC.best_score_","f2207047":"print(\"best params:\")\ngrid_search_RFC.best_params_","9c34b8b6":"best_RFC = grid_search_RFC.best_estimator_\nbest_pipeline_RFC = Pipeline([\n    ('sampling', SMOTE(random_state=42)), \n    ('classification', best_RFC)])\n\nbest_pipeline_RFC.fit(X_train_scaled, y_train)\ny_pred = best_pipeline_RFC.predict(X_test_scaled)\n\ncf_matrix_show(confusion_matrix(y_test, y_pred), beta=4)","16dfa116":"best_rfc_model = list(best_RFC)[1]\nfeature_imp = pd.Series(best_rfc_model.feature_importances_,index=X_train_scaled_upsample.columns).sort_values(ascending=False)\nfeature_imp","684e497d":"importance_limit = [0, 0.01, 0.015, 0.02, 0.03, 0.04, 0.05, 0.06, 0.065, 0.08, 0.11]  \n    \nFb_list = []\nrecall_list = []\nprecision_list = []\n\ncurrent_best_pipeline_RFC = Pipeline([\n    ('sampling', SMOTE(random_state=42)), \n    ('classification', best_rfc_model)])\n\nfor limit in importance_limit:\n    X_tr_copy = copy_by_imp_limit(X_train_scaled, limit, feature_imp)\n    X_test_copy = copy_by_imp_limit(X_test_scaled, limit, feature_imp)\n    \n    Fb_score, recall, precision = Fb_recall_precision_CV(current_best_pipeline_RFC, X_tr_copy, y_train)\n\n    Fb_list.append(Fb_score)\n    recall_list.append(recall)\n    precision_list.append(precision)\n\nlimits = df_Fb_recall_precision(Fb_list, recall_list, precision_list)\nlimits","39cb9a66":"X_train_copy = copy_by_imp_limit(X_train_scaled, 0.015, feature_imp)\nX_test_copy = copy_by_imp_limit(X_test_scaled, 0.015, feature_imp)\n\nbest_pipeline_RFC.fit(X_train_copy, y_train)\ny_predict = best_pipeline_RFC.predict(X_test_copy)\n\ncf_matrix_show(confusion_matrix(y_test, y_predict), beta=4)","a7b4066e":"kf = KFold(n_splits=5, random_state=42, shuffle=True)\npredict_probability = cross_val_predict(best_pipeline_RFC, X_train_copy, y_train, cv=kf, method='predict_proba')\nbeta=4\n# keep probabilities for the positive outcome only\nstroke_prob = predict_probability[:, 1]\nprecision, recall, thresholds = precision_recall_curve(y_train, stroke_prob)\n# convert to f-beta score\nfb_score = ((1+beta**2) * precision * recall) \/ ((beta**2)*precision + recall)\n# locate the index of the largest fb score\nnp.nan_to_num(fb_score, nan=0, copy=False)\nix = fb_score.argmax(axis=0)\n\nprint('Best Threshold = %f with Fb-Score of %.3f' % (thresholds[ix], fb_score[ix]))","1159c637":"best_pipeline_RFC.fit(X_train_copy, y_train)\ny_prob = best_pipeline_RFC.predict_proba(X_test_copy)\n# keep probabilities for the positive outcome only\ny_prob = y_prob[:, 1]\n\ny_predict = np.where(y_prob <= thresholds[ix], 0, 1)\ncf_matrix_show(confusion_matrix(y_test, y_predict), beta=4)","92bf1756":"current_best_model.get_params()\n","11961db2":"current_best_model","5a1db8bc":"- this model is just a dummy model! we have to fix the imbalance classes!\n\nThe problem with imbalanced classification is that there are too few examples of the minority class for a model to effectively learn the decision boundary.<br>\nOne way to solve this problem is to oversample the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model.<br>\nAn improvement on duplicating examples from the minority class is to synthesize new examples from the minority class. This is a type of data augmentation for tabular data and can be very effective.<br>\nPerhaps the most widely used approach to synthesizing new examples is called the Synthetic Minority Oversampling TEchnique, or SMOTE for short. This technique was described by Nitesh Chawla, et al. in their 2002 paper named for the technique titled \u201cSMOTE: Synthetic Minority Over-sampling Technique.\u201d<br>\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.<br>\n(source - https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/)\n\n\n### Over sample using SMOTE","21337670":"![confusion_matrix_bin_claasification.webp](attachment:confusion_matrix_bin_claasification.webp)","7839a54e":"- Background of heart disease increases the risk of stroke by 13%.","0ba13b0a":"### basic infromation of the observaions which have stroke:","4b2566bd":"## Can we push the model even further?","641afee1":"- take a look at the importances list:","f323e582":"##  feature selecting","d7eceabe":"# _classification notebook_\n\n# <ins> A. Task definition and general information <\/ins> \n\n## Classificastion task - Predict Stroke\nOur top priority in this health problem is to identify patients with a stroke. Therefore, we would like to identify any observation that may be a stroke.<br>\nThe data represents the population that comes to the emergency room with some suspicion of stroke. About 1\/20 of them actually have a stroke. The aim of the model is to identify as many as possible of those who actually have a stroke and at the same time increase as much as possible the 1 \\ X ratio between those who have a stroke and those who do not have a stroke (20> X as much as possible) of all people who identify the model as having a stroke.<br>\nI chose this data and this task out of a personal touch on the matter. As an MDA medic, I think that already during the initial treatment, with the help of the ML model, the patient's problem can be better established.\nIn addition, the model will be able to provide a quick response to physicians and thus save time, reduce the amount of people with a reasonable suspicion of stroke and treat stroke cases more quickly.","84536cb7":"### basic infromation of the observaions which don't have stroke:","cded54c7":"# Random Forest Classifier","ca358040":"# <ins> B. Basic familiarity with the Datast <\/ins> \n\n\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about a patient.\n\n## attributes information\n\nThe data contains 5110 observations with 12 attributes.\n\n- id: unique identifier.\n- gender: \"Male\", \"Female\" or \"Other\".\n- age: age of the patient.\n- hypertension: hypertension means high blood pressure. 0 if the patient doesn't have hypertension, 1 if the patient has hypertension.\n- heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n- ever_married: \"No\" or \"Yes\".\n- work_type: \"children\", \"Govt_job\", \"Never_worked\", \"Private\" or \"Self-employed\".\n- Residence_type: \"Rural\" or \"Urban\".\n- avg_glucose_level: average glucose level in blood.\n- bmi: body mass index, As a measure of obesity.\n- smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*.\n- stroke: 1 if the patient had a stroke or 0 if not.<br>\n\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient.","2c14d2d7":"###  - heatmap corraltions of numerical attributes:","b8da4881":"## BMI - can we find some patterns here?\nthat's a bit strange. A lot of studies point to a high BMI as a stroke risk. It could be that the data is not sufficiently representative, or that there really is no significant link between high BMI and stroke. Furthermore, high BMI observations may be abnormal and OUTLIERS, therefore, they may be worth ignoring. However, because our data is not large enough, we will not ignore these observations and treat this information as representative.\n\n### although the BMI correlation with stroke is low, it has a majors effects about othe attributes which have a high correlation with stroke. So, it's worth a try to uncover few patterns here also - maybe combination of specific bmi values with another attribute can be a major stroke factor.\n\nmajor correlations with bmi:","5414b18b":"1.\nTrue Positives (TP) is defined by the total number of accurate outputs when the actual class of the data object was True and the prediction was also the True value.\n\n2.\nTrue Negatives (TN) is defined by the total number of accurate outputs when the actual class of the data object was False and the predicted is also the False value.\n\n3.\nFalse Positives (FP) when the actual class of the data object was False and the output value was the True value\n\n4.\nFalse Negatives (FN) when the actual class of the data object was True and the output value was the False value.\n\nMetrics computed from a confusion matrix\nA confusion matrix gives a useful information about how well the model does. However, its elements can be used to calculate many performance metrics to get even more information. Among the most popular:\n\n1. Accuracy is the most intuitive performance measure, and defined as the ratio of the number of correctly classified objects to the total number of objects evaluated.\n\n2. Precision it is simply a ratio of correctly predicted positive data objects to the total predicted positive data objects.\n\n3. Recall it is defined by the number of correct positive results divided by the total number of relevant samples (all samples that should have been identified as positive).\n\n4. F-score it can be defined as a weighted average of the precision and recall. An F-score is considered perfect when reaches its best value at 1, while the model is a total failure when it reaches the 0 value.\n\n5. The F-beta-score is a generalization of the F-score that adds a configuration parameter called beta. A default beta value is 1.0, which is the same as the F-score. A smaller beta value, such as 0.5, gives more weight to precision and less to recall, whereas a larger beta value, such as 2.0, gives less weight to precision and more weight to recall in the calculation of the score.\n\n(source - https:\/\/link.springer.com\/article\/10.1007\/s42452-019-1356-9)","7f1b5279":"- take a look at the importances list:","fe45f0cb":"### attributes which have main correlations with stroke (>0.10):\n\n- age\n- avg glucose level\n- hypertension\n- heart disease background\n- ever married\n\n### let's take a closer glance at each of those attributes! what can we see?","131b3811":"### first, I will use StratifiedShuffleSplit to make test set and train set","c4a5d5f0":"### let's take a look at the numeric attributes histograma:","6758588f":"### before selecting our best K, I will check if there are a features that can be removed:\n- I will remove features using linear correlation.","a4a035f7":"### now, let's take a look at the combinations by the correlations we saw above.\n\n#### right plot - bmi and age\n#### left plot - bmi and glucose","aca2ddbd":"- this score is great and use only one feature!\n- but, we can do even better if we take a step.\n### I will find better model using the grid search<br> with the knowledge from the random search:","c7619959":"#### after define our dummy models, how would look an ideal model?","26bb795d":"#### At the right plot (above) - we can see that the combination of BMI and AGE is very powerful facor to predict stroke. At the left plot (above) - alghouth there is a pattern it's not so simple. We saw already the very high and cery low glucose level is major factor. this plot puts the stroke cases at the center with the help of BMI.\n\n#### next plots - bmi and marriage (high corr), bmi and hypertension (high corr), bmi and heart disease background (low corr). as we can see below, BMI around 30 increase the probability to have stroke to those who had merried or have hypertension. but this is not significant or uniqe pattern, because the percentage difference (to have stroke) between those who have merried \/ hypertension and not - remains relatively the same. In addition, this increase is also occur in the attribute of heart disease in a similar way.\n\n##### bmi and marriage","d32b0da6":"More over, it is clear that using a test set can reflect over-fitting to the test data so we will use the training data.\n    #### find the best k for the model using only the train set and testing using it:","0b974230":"### features selection - is there feature that is not necceserily? maybe even it harms the model?","6bf215d9":"# <ins> F. Test Set and Train Test <\/ins>","944778ea":"### basic infromation of all the data:","f574550f":"### note: it is important to scaling the test set by the train set!","54e82b81":"# <ins>D. Dig into the DATA - correlations and patterns <\/ins>\n\n## Let's try so uncover some patterns.\n\n#### although linear correlations are not the only correlations we can find, it can gives us a good start. I will use Pearson\u2019s correlation coefficient in the next matrixes.\n\n### Correlation Matrix:","976f26a3":"## this is amazing! 3 features are all what we need! \n### now, let's do it the right way - using pipline, smote and cross_validation.","c8c314dc":"### test the final model:","5414946d":"### A confusion matrix for binary classification:","8d9cae8f":"## - Handeling text and categorial attributes\n\n### first of all, I will use \"get_dummies\" function to \"convert\" every categorial attribute.\n### now, we have seperate column for each value. note that the stroke column (targets) as is. let's see the columns:","1ecbae85":"<br>\nlet's take a look at the attributes categories:","7b3ae891":"### now we have stratified train and test sets","aca31764":"let's take a look at the F-beta scores with this corralation limit!","e1eda1b8":"Obviously such a test can reflect over-fitting. Therefor, I will use cross-validation. advantage of this method - it is more \"efficient\" use of data (every observation is used for both training and testing).\n#### find the best k for the model by cross-validation:","532f746e":"### First try - I will try random forest randomly - I will use alse the test set sometimes just to understand the base and where are we standing.\nnote: I will use n=200 randomly just to check in generalize.","a31a887f":"## Best  Threshold:","e2bed677":"# <ins> C. Clean and prepare the data <\/ins> ","1cddd0fe":"we want to get the highest F-beta we can. so:\n- first, we want to get the highest recall we can\n- second, we want to get the highest precision we can","4f132ceb":"# <ins> I. Best model is: <\/ins>","b26aeeb4":"#### this is the 5 best models that the random search found:","41809d18":"those correlation can give as a better unserstanding at the role of the BMI as an indirect factor to stroke. let's check if those correlations realated to stroke also.\n\n#### first, we can see that specific bmi values, around 30, have the most stroke cases:","5269fb2f":"- there is a big difference between importance list to linear correlation list ! for example, BMI is important as we saw in the data analysis.\n#### feature selection by importance:","80cc8f53":"# <ins> G. K-NN Model <\/ins>","ed85cc75":"- People who get married have a 5% higher risk of having a stroke.","dddfee8f":"### using grid-search:","03731d27":"#### although this is not by the book, I will use the test set so we can understand where we're standing:","081ed3e2":"### The solution is to use cross-validation and only after the split do over-sampling!\n- I will use imblearn.pipeline.make_pipeline for this task.","9ceb9586":"### define dummy model:","b83c48e9":"## general knowledge:\n\nA stroke is a medical condition in which poor blood flow to the brain causes cell death. There are two main types of stroke: ischemic, due to lack of blood flow, and hemorrhagic, due to bleeding. Both cause parts of the brain to stop functioning properly.\n\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths, accounting for 6.2 million deaths. Approximately 17 million people had a stroke in 2010 and 33 million people have previously had a stroke and were still alive. Between 1990 and 2010 the number of strokes decreased by approximately 10% in the developed world and increased by 10% in the developing world.\n\nThe main risk factor for stroke is high blood pressure. Other risk factors include tobacco smoking, obesity, high blood cholesterol, diabetes mellitus, a previous TIA, end-stage kidney disease, and atrial fibrillation.\n\n(source - https:\/\/en.wikipedia.org\/wiki\/Stroke)\n\nHigh blood pressure, high cholesterol, smoking, obesity, and diabetes are leading causes of stroke. 1 in 3 US adults has at least one of these conditions or habits. This information may help as to build a model.\n(source - https:\/\/www.cdc.gov\/stroke\/facts.htm)\n","5b50432a":"## final Random Forest Classifier model:","391508ae":"#### - this is awasome! such a dimentionality reduction!<br> just by the age we can get F-beta of 0.557 and find 70% of the stroke cases!\n\n#### test this model:","fbf7c1e6":"### scaling the sets:","03edd0cd":"#### fine tuning by choosing best treshold:","3ebe91af":"### let's look at the categorial attributes histograma (as pies):","e634ec33":"### Now we have scaled test set and train set, we can continue to find a good model!\n<br> <br>","cd83780f":"## this is a little bit shocking - our best model is using only the age feature!\n- with F beta of ~0.62, this model find 80% of the stroke cases.","94497283":"we can see that: gender, ever_married, Residence_type, smoking_status are a textual attributes. those and more are categorial, we will handle it soon and convert is to numeric values by \"get dummies\" function.\nalso, the bmi attribute has 201 missing values. It's need to be handled too.","0970eb60":"## best score:","c4b12d9d":"There are few options for performence measure. I decided to choose a combination of two (re-call and percision) using F-beta-score:\n$$ re-call = \\frac{TP}{TP+FN}$$<br>\n$$ precision = \\frac{TP}{TP+FP} $$\n<br>The re-call must be highest as we can, becaue we need to predict the most stroke cases we can, so that FN will be as close to 0 as posible.<br>\nThe re-call is much more important than the percision, because we need to first identify as many strokes as possible. Therefore, we will use F beta when beta = 4. Supposedly, re-call is 4 times more important than precision.\n\n$$ Fbeta(4) = \\frac{(1+4^2) X precision X recall}{(4^2) X precision+recall} $$\n\nAccuracy will not be appropriate for 2 reasons: a. It is impossible to make sure that we predict all the strokes cases. B. If we define the zero model according to the division of yes\/no stroke in the data or if we define that it will always say that there is no stroke - it will succeed in about 95% of the cases (because of the distribution of targets attribute). 95% is a false accuracy that will be very difficult to circumvent.\nF-SCORE will not fit because the recall and percision not equal in weight in this task.","25dee813":"### test:","fa4062ff":"#### there are few missing values in 'bmi' attribute. we will see soon that the correlation between stroke and bmi is low, so we will update all the missing value of the bmi with the mean.","9aba53d6":"The most common techniques of feature scaling are Normalization and Standardization.\nNormalization is used when we want to bound our values between two numbers, typically, between [0,1] or [-1,1]. While Standardization transforms the data to have zero mean and a variance of 1, they make our data unitless. Refer to the below diagram, which shows how data looks after scaling in the X-Y plane. (source - https:\/\/towardsdatascience.com\/all-about-feature-scaling-bcc0ad75cb35) \n<br>I will use standardization. why? because the affect of outliers is less significant.","dd2cf0f2":"- We can see that we found 82% of the stroke cases, which is pretty good! The cost is that the model indicates about 33% as having stroke. However, this is a significant improvement, because in data 1\/20 is a case with a stroke and the model improves the ratio to about 1\/8. Not bad at all.\n\n### we can see that the fine tuning (choosing threshold) was good, because without it - the score is lower:","de560d72":"##### bmi and hypertension","a98e4b83":"#### test this model:","358150c3":"#### lets push this model higher:","3a5ba342":"this is a little bit too much information in one table.<br>\nwhat can we do?\n- look just at the main coulmn.\n- organize it in heatmaps by realated attributes (realated by common sense).\n\n### - all the correlations with stroke:","6e48c4da":"#### important information from the tables above:\n- 4.87% of the observation in this dataset had stroke.\n- there is a big difference between those who have stroke and those who don't have stroke! In those who have a stroke - the average age and average glucose level is significantly higher, the number of people with heart disease and hypertension is significantly higher.","54d8d5e8":"#### find best threshold:","71ac7ca8":"- The older a person is, the more likely he have a stroke.\n- Blood glucose level is dangerous at both edges (the safe zone is between 120 to 180 approximately.","adcaf2b3":"### note: the next steps are not correct, I will show them for make the process understandable.I will also provide explanations for those mistakes to show my way of thinking and progress in finding a good model.\n\n### although acurracy is not the our measure method, I started with this to get some information about the best k for the model. this is the first mistake.\n#### find the best k for the model using train set and test set:","26d838dd":"This plot made me realize that there is a significant problem. How can it be that when significant features such as age are removed, the accuracy of the model does not fall?\nSome conclusions:\n- The use of an accuracy test is also not suitable for an initial test because this is not the purpose of the model.\n- More importantly - my model functions like a stupid model if I use an accuracy test! It almost completely predicts that there is simply no stroke.\n- Perhaps most importantly - unbalanced classes. The model behaves like a stupid model because there are unbalanced classes. There are only 5% of strokes and the model almost completely ignores them by simply stating that there is always no stroke and thus achieving supposedly very high accuracy percentages.\n\n#### You can see at the confusion matrix below the problem clearly:","cdedc322":"# <ins> E. Select a Performance Measure <\/ins>","76c6c3f0":"- so the grid search didn't help as we hope.\n### the best KNN model is:","85fef44f":"with those params:\n","a25aa841":"## top 5 usind grid search:","e4a4ffeb":"- I will use random seacrh to idenify to ranges of best parameters.","5127276f":"## - Unique values\n\n### as we can see, in the 'gender' attribute, there is one observation that is not female or male. because it's just one I will remove this observation.","5146602e":"### grid search for best hyperparameter:","73de49f6":"- it was a nice try, but feature selection and threshold choosing didn't help as so much.\n- maybe better solution will be to insert the feture-selection step into the grid-search because every model use the params and the features differently.<br>I would have do that, but my computer barely survived this grid_search so it will stay as future mision.","fbe86b3d":"best params:\n- n_estimators: 200 and 400 (lowest).\n- min_samples_split: both (2,5).\n- min_samples_leaf: 4 (highest).\n- max_features: both.\n- max_depth: 10 (lowest).\n- bootstrap: True.\n\nI will check around those results using grid search:","9aaf6edc":"## Thanks for reading!\n### I would love to get comments, reviews and suggestions for improvement!","efd2affc":"#### let's find our KNN model!\nnote: we cant use the data after upsample for testing our model because it is balanced compared to the TEST SET which is unbalanced. more over, Even if we use the training data before over-sampling, so the ratio is the same, there is a risk of making a mistake due to over-fitting.\n- example:","75fe5b4a":"### imports","11839ea5":"#### Find best importance limit using cross validation (and not the test set):","632779e3":"##### bmi and heart disease background","87079cba":"### patterns and corralaion conclusion:\n##### attributes which have main correlations with stroke (>0.10):\n- age\n- avg glucose level\n- hypertension\n- heart disease background\n- ever married\n\n##### stroke patterns:\n- The older a person is, the more likely he have a stroke.\n- Blood glucose level is dangerous at both edges (the safe zone is between 120 to 180 approximately.\n- Background of heart disease increases the risk of stroke by 13%.\n- Hypertension increases the risk of stroke by 10%.\n- 1\/2 of the stroke cases are around 30 BMI.\n- combination of BMI and AGE is very powerful facor to predict stroke (high age, around 30 bmi).","ba0db03e":"- Hypertension increases the risk of stroke by 10%.\n","f5f4d0e2":"#### Find best importance limit using cross validation (and not the test set):","1cf60d0d":"### - heatmap corraltions of categorial attributes: ","a5a3d35b":"- we can see that, approximatly, the mean is 0 and the std is 1.\n\n#### scaling the numerical features of X_train\n","bd369db5":"#### scaling the numerical features of X_train","c691d3b5":"- note: scoring by train-test provides a high variance estimate since changing which observations happen to be in the testing set can significantly change testing accuracy."}}