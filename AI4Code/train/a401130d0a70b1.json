{"cell_type":{"b258f6b0":"code","9e49a4eb":"code","aca4a1c2":"code","a7776bff":"code","adc2f862":"code","128c7e2c":"code","8b110ceb":"code","a370e5c5":"code","22609be6":"code","da38f888":"code","cabd6664":"code","90e45108":"code","6d5f23ba":"code","96b5e5ca":"code","2b2e8c67":"code","2651ed57":"code","1e10a1a7":"code","ea319727":"code","db1e4b37":"code","454115ea":"code","524a8ef1":"code","f8b27877":"code","91c3c5bc":"code","4d4bf15b":"code","e0b2b54e":"code","46d4d103":"code","a3cd9a73":"code","30971457":"code","e173a561":"code","977a04b8":"code","41f44c3b":"code","94b24d43":"code","a6f32f07":"code","b8e33842":"code","e8ec27de":"code","ab7a8da7":"code","5fdbfbdc":"code","2f2a8ab4":"code","0c619e9f":"code","2d41e219":"code","9b23030c":"code","ca38394f":"code","740fb12d":"code","b7ba53cc":"code","b477481a":"code","61b83e0e":"code","2f0cf9e7":"markdown","033ba796":"markdown","8f6c0e01":"markdown","be348a8c":"markdown","5aea4fd3":"markdown","86ca5ff6":"markdown","319eb6a4":"markdown","d68131c2":"markdown","5e0ea8ef":"markdown","558d598e":"markdown","bfd2be7e":"markdown","ef15d785":"markdown","3490ddfa":"markdown","7e2093ee":"markdown","76c65c7a":"markdown","7b58559f":"markdown","468d9b76":"markdown","5d0d408a":"markdown","8c6cd94f":"markdown","b3aa18e6":"markdown","28bb2a91":"markdown","f50618b6":"markdown","9508b0e8":"markdown","44011e28":"markdown","816e63b0":"markdown","d93e051d":"markdown","9022b62f":"markdown","dbddbe5a":"markdown","5f04831f":"markdown","38789dc1":"markdown","f97bad0f":"markdown","bd9f193b":"markdown","e8840df6":"markdown","40e44f82":"markdown","49a150ba":"markdown","b6bd3d24":"markdown","140fcee5":"markdown"},"source":{"b258f6b0":"#Import libraries:\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom xgboost import XGBRegressor\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\n\npd.pandas.set_option('display.max_columns',None)","9e49a4eb":"#Read in data:\ndata = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")\nX_test_id = X_test['Id']\ndata.head()","aca4a1c2":"data.info()","a7776bff":"#Examine missing values:\n\nmiss_perc = data.isnull().sum()*100\/len(data)\nmiss_perc = miss_perc[miss_perc>0].sort_values(ascending=False)\nsns.barplot(x=miss_perc.index, y=miss_perc.values)\nplt.title(\"Percentage of missing values\", fontsize=15)\nplt.xticks(rotation=90)\nplt.show()\n","adc2f862":"#Explore relationship between outcome and missing status:\nplt.subplots(5,4,figsize=(11,12))\nplt.tight_layout()\nj=1\nfor i in miss_perc.index:\n    temp = data.copy()\n    temp[i] = np.where(temp[i].isnull(),1,0)\n    plt.subplot(5,4,j)\n    sns.boxplot(x=temp[i],y=temp['SalePrice'], fliersize=2)\n    plt.title(i+\"_miss\", fontsize=15)\n    plt.xlabel(\"\")\n    j+=1\n    #plt.show()\n","128c7e2c":"#The numeric variables MSSubClass and MoSold are categorical so need to be reclassified: \ndata['MSSubClass'] = data['MSSubClass'].astype('object')\nX_test['MSSubClass'] = X_test['MSSubClass'].astype('object')\ndata['MoSold'] = data['MoSold'].astype('object')\nX_test['MoSold'] = X_test['MoSold'].astype('object')","8b110ceb":"#Create lists of numeric and categorical variables:\n\nnum_cols = [col for col in data.columns if data[col].dtype in ['float64','int64']]\ncat_cols = [col for col in data.columns if data[col].dtype=='object']\n\nprint(\"num_cols:\\n\", num_cols)\nprint(\"cat_cols:\\n\", cat_cols)","a370e5c5":"#Histograms of numeric variables\nplt.subplots(9,4,figsize=(11,22))\nplt.tight_layout()\nj=1\nfor i in num_cols:\n    plt.subplot(9,4,j)\n    sns.histplot(data[i], kde=False)\n    plt.title(i, fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","22609be6":"#Create lists of continuous and discrete numeric variables:\ncont_num = [col for col in num_cols if data[col].nunique()>15 or col=='PoolArea']\ndisc_num = [col for col in num_cols if col not in cont_num]\n\nprint(\"cont_num:\\n\", cont_num)\nprint(\"disc_num:\\n\", disc_num)","da38f888":"#Explore continuous variables and SalePrice:\nplt.subplots(6,4,figsize=(11,14))\nplt.tight_layout()\n\nj=1\nfor i in cont_num:\n    plt.subplot(6,4,j)\n    sns.scatterplot(x=data[i],y=data['SalePrice'], alpha=0.3)\n    plt.title(i, fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","cabd6664":"#Explore discrete variables and SalePrice:\nplt.subplots(3,4,figsize=(11,10))\nplt.tight_layout()\n\nj=1\nfor i in disc_num:\n    plt.subplot(3,4,j)\n    sns.boxplot(x=data[i],y=data['SalePrice'], fliersize=2, color='steelblue')\n    plt.title(i, fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","90e45108":"#Plot correlation matrix of numeric variables:\ncorrmat = data[num_cols].corr(method='spearman')\n\nmask = np.zeros_like(corrmat,dtype=np.bool)\nmask[np.triu_indices_from(mask)]=True\n\nplt.figure(figsize=(18,14))\nsns.heatmap(corrmat,annot=True,mask=mask,cmap=sns.diverging_palette(240,10,as_cmap=True),vmin=-1,vmax=1)\nplt.title(\"Feature Correlation Matrix\",fontsize=20)\nplt.show()","6d5f23ba":"#Countplots of all categorical variables:\nplt.subplots(12,4,figsize=(11,30))\nplt.tight_layout()\n\nj=1\nfor i in cat_cols:\n    plt.subplot(12,4,j)\n    sns.countplot(x=data[i])\n    plt.title(i,fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","96b5e5ca":"#Boxplots of catagorical variables vs price:\nplt.subplots(12,4,figsize=(11,32))\nplt.tight_layout()\n\nj=1\nfor i in cat_cols:\n    plt.subplot(12,4,j)\n    sns.boxplot(x=data[i], y=np.log1p(data['SalePrice']), fliersize=2)\n    plt.title(i,fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","2b2e8c67":"y = data['SalePrice'].copy()\nX = data.copy()\nX = X.drop(['Id','SalePrice'], axis=1)\nX_test = X_test.drop('Id',axis=1)\n\n#Update variable lists:\nnum_cols = [col for col in X.columns if data[col].dtype in ['float64','int64']]\ncont_num = [col for col in num_cols if X[col].nunique()>15 or col=='PoolArea']\ndisc_num = [col for col in num_cols if col not in cont_num]\n\ncat_cols = [col for col in data.columns if data[col].dtype=='object']","2651ed57":"#Fill 'None' entries:\nnone_cats = ['GarageType','GarageFinish','BsmtFinType2','BsmtExposure','BsmtFinType1', \n       'GarageCond','GarageQual','BsmtCond','BsmtQual','FireplaceQu','Alley','Fence','PoolQC','MiscFeature']\n\nX[none_cats] = X[none_cats].fillna(\"None\")\nX_test[none_cats] = X_test[none_cats].fillna(\"None\")\n\n#Replace missing MasVnrArea with 0\nX['MasVnrArea'] = X['MasVnrArea'].fillna(0)\nX_test['MasVnrArea'] = X_test['MasVnrArea'].fillna(0)\n\n#Fill these 5 others with 0 as investigation shows there is no garage\/basement\nzero_fill = ['BsmtFinSF1', 'GarageArea','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF']\nX_test[zero_fill] = X_test[zero_fill].fillna(0)\n\n#Again, fill with 0 for no basement\/garage\nzero_fill_disc = ['BsmtHalfBath','BsmtFullBath','GarageCars']\nX_test[zero_fill_disc] = X_test[zero_fill_disc].fillna(0)","1e10a1a7":"#Check what other missing values we have in X:\nmiss_perc = X.isnull().sum()*100\/len(X)\nmiss_perc = miss_perc[miss_perc>0].sort_values(ascending=False)\nsns.barplot(x=miss_perc.index, y=miss_perc.values)\nplt.title(\"Training Percentage Missing\",fontsize=15)\nplt.ylim(0,100)\nplt.show()","ea319727":"#Check what other missing values we have in X_test:\nmiss_perc = X_test.isnull().sum()*100\/len(X_test)\nmiss_perc = miss_perc[miss_perc>0].sort_values(ascending=False)\nsns.barplot(x=miss_perc.index, y=miss_perc.values)\nplt.title(\"Test Percentage Missing\",fontsize=15)\nplt.xticks(rotation=90)\nplt.ylim(0,100)\nplt.show()","db1e4b37":"#Create a Total SF variable:\nX['TotalSF'] = X['TotalBsmtSF'] + X['GrLivArea']\nX_test['TotalSF'] = X_test['TotalBsmtSF'] + X_test['GrLivArea']\nsns.histplot(x=X['TotalSF'], bins=50)\nplt.title(\"Total Square Footage\", fontsize=15)\nplt.show()","454115ea":"#Total Number of bathrooms:\nX['TotalBath'] = X['FullBath'] + X['BsmtFullBath'] + X['HalfBath']\/2 + X['BsmtHalfBath']\/2\nX_test['TotalBath'] = X_test['FullBath'] + X_test['BsmtFullBath'] + X_test['HalfBath']\/2 + X_test['BsmtHalfBath']\/2\nsns.scatterplot(x=X['TotalBath'], y=y)\nplt.title(\"Total Number of Bathrooms\", fontsize=15);","524a8ef1":"#TotalPorch:\nX['TotalPorch'] = X['OpenPorchSF'] + X['EnclosedPorch'] + X['3SsnPorch'] + X['ScreenPorch']\nX_test['TotalPorch'] = X_test['OpenPorchSF'] + X_test['EnclosedPorch'] + X_test['3SsnPorch'] + X_test['ScreenPorch']\nsns.histplot(x=X['TotalPorch'], bins=50)\nplt.title(\"Total Porch Square Footage\", fontsize=15);","f8b27877":"sns.scatterplot(x=X['TotalPorch'], y=y)\nplt.title(\"Total Porch Square Footage vs Price\", fontsize=15);","91c3c5bc":"#Update variable lists:\nnum_cols = [col for col in X.columns if X[col].dtype in ['float64','int64']]\ncont_num = [col for col in num_cols if X[col].nunique()>15 or col=='PoolArea']\ndisc_num = [col for col in num_cols if col not in cont_num]\n\ncat_cols = [col for col in X.columns if X[col].dtype=='object']","4d4bf15b":"#Plot cont_num variables:\n\nplt.subplots(6,4,figsize=(11,15))\nplt.tight_layout()\nj=1\nfor i in cont_num:\n    plt.subplot(6,4,j)\n    sns.histplot(X[i], kde=False)\n    plt.title(i, fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","e0b2b54e":"#Histograms of log transformed continuous numeric variables:\nplt.subplots(6,4,figsize=(11,15))\nplt.tight_layout()\nj=1\n\nfor i in cont_num:\n    plt.subplot(6,4,j)\n    sns.histplot(np.log1p(X[i]), kde=False)\n    plt.title(i, fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","46d4d103":"from scipy.stats import normaltest\n\nlog_cols = []\nfor i in cont_num:\n    p = normaltest(X[i]).pvalue\n    plog = normaltest(np.log1p(X[i])).pvalue\n    #Add i to transform list if plog larger than p\n    if plog>p:\n        log_cols.append(i)\n    else:\n        pass\n\nlog_cols","a3cd9a73":"#Transform skewed variables:\nfor i in log_cols:\n    if i=='SalePrice':\n        X[i] = np.log1p(X[i])\n    else:\n        X[i] = np.log1p(X[i])\n        X_test[i] = np.log1p(X_test[i])","30971457":"# Plot Price and LogPrice\n\nsns.histplot((y), kde=False)\nplt.title(\"SalePrice\", fontsize=15)\nplt.xlabel(\"SalePrice\")\nplt.show()\n\nsns.histplot((np.log1p(y)), kde=False)\nplt.title(\"LogSalePrice\", fontsize=15)\nplt.xlabel(\"SalePrice\")\nplt.show()","e173a561":"#Transform SalePrice:\ny = np.log1p(y)","977a04b8":"#Create lists of nominal or ordinal categorical variables:\nqual_ord_cats = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC',\n             'KitchenQual','FireplaceQu','GarageQual','GarageCond','PoolQC']\nother_ord_cats = ['LandSlope','BsmtExposure','Functional','GarageFinish','BsmtFinType1','BsmtFinType2']\n\nnom_cats = [col for col in cat_cols if col not in (qual_ord_cats+other_ord_cats)]","41f44c3b":"#Reclass small nominal levels:\nfor i in nom_cats:\n    temp = X[i].value_counts()\/len(X)\n    keep_levs = temp[temp>0.02].index\n    if X[i].nunique()-len(keep_levs)>1:\n        X[i] = np.where(X[i].isin(keep_levs),X[i],'other')\n        X_test[i] = np.where(X_test[i].isin(keep_levs),X_test[i],'other')\n    else:\n        pass","94b24d43":"for i in nom_cats:\n    print(i, \"levels=\",X[i].nunique())","a6f32f07":"#Custom encode ordinal variables:\n#MAPPING - map the Ordinal cat vars with dictionaries:\nquality = {'Ex': 5,'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'None':0}\nfinished = {'GLQ': 6,'ALQ': 5,'BLQ': 4,'Rec': 3,'LwQ': 2,'Unf': 1, 'None': 0}\n\nslope = {\"Gtl\":2, \"Mod\":1, \"Sev\":0}\nfunction = {\"Typ\":7, \"Min1\":6, \"Min2\":5,\"Mod\":4, \"Maj1\":3, \"Maj2\":2,\"Sev\":1,\"Sal\":0}\ng_fin = {\"Fin\":3,\"RFn\":2,\"Unf\":1,\"None\":0}\nexpose = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0}\n\n\nfor i in qual_ord_cats:\n    X[i] = X[i].map(quality)\n    X_test[i] = X_test[i].map(quality)\n    \nfinished_cols = ['BsmtFinType1','BsmtFinType2']\nfor i in finished_cols:\n    X[i] = X[i].map(finished)\n    X_test[i] = X_test[i].map(finished)\n    \nX['LandSlope'] = X['LandSlope'].map(slope)\nX_test['LandSlope'] = X_test['LandSlope'].map(slope)\n\nX['Functional'] = X['Functional'].map(function)\nX_test['Functional'] = X_test['Functional'].map(function)\n\nX['GarageFinish'] = X['GarageFinish'].map(g_fin)\nX_test['GarageFinish'] = X_test['GarageFinish'].map(g_fin)\n\nX['BsmtExposure'] = X['BsmtExposure'].map(expose)\nX_test['BsmtExposure'] = X_test['BsmtExposure'].map(expose)\n\n#Check mapping:\nX['BsmtExposure'].head(10)","b8e33842":"#Re-examine ordinal relationship with Price:\nplt.subplots(4,4,figsize=(11,10))\nplt.tight_layout()\n\nj=1\nfor i in qual_ord_cats+other_ord_cats:\n    plt.subplot(4,4,j)\n    sns.boxplot(x=X[i], y=y, fliersize=2,color='steelblue')\n    plt.title(i,fontsize=15)\n    plt.xlabel(\"\")\n    j+=1","e8ec27de":"#Dropping cols:\ndrop_cols = ['BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', \n             'PoolArea', 'MiscVal','Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', \n             'PoolQC','GarageYrBlt', '1stFlrSF','TotRmsAbvGrd','GarageArea', 'MSSubClass','MoSold']\nX.drop(drop_cols, axis=1, inplace=True)\nX_test.drop(drop_cols, axis=1, inplace=True)","ab7a8da7":"#Update variable lists:\nnum_cols = [col for col in X.columns if X[col].dtype in ['float64','int64']]\ncont_num = [col for col in num_cols if X[col].nunique()>15 or col=='PoolArea']\ndisc_num = [col for col in num_cols if col not in cont_num]\n\ncat_cols = [col for col in X.columns if X[col].dtype=='object']\nnom_cats = [col for col in cat_cols if col not in drop_cols]","5fdbfbdc":"#Create a modeling pipeline:\n# Create num and cat transformers for imputing and cat encoding then bundle into a preprocessor:\n\ncont_num_transformer = Pipeline( steps=[('imputer',SimpleImputer(strategy='median')),\n                                        ('scaler',StandardScaler())\n                                       ])\ndisc_num_transformer = Pipeline( steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                                         ('scaler',StandardScaler())\n                                       ])\n\ncat_transformer = Pipeline( steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n                                   ('onehot',OneHotEncoder(handle_unknown='ignore'))\n                                  ])\n\n\npreprocessor = ColumnTransformer( transformers = [('num_c',cont_num_transformer, cont_num),\n                                                  ('num_d',disc_num_transformer, disc_num),\n                                                  ('nom_cat',cat_transformer, cat_cols)\n                                                 ])","2f2a8ab4":"#Define the LASSO model and bundle into a processing and modelling pipeline:\n\nmodel = Lasso(max_iter=100000)\n\npipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', model)\n                            ])","0c619e9f":"#Tune alpha with GridSearchCV:\n#Tuned alpha=0.001\n\nhparams = {'modelling__alpha':[0.001]}\n\ngrid = GridSearchCV(pipe,param_grid = hparams,cv=5,scoring='neg_mean_squared_error',n_jobs=3,verbose=1)\ngrid.fit(X,y)\nprint(\"Best params\",grid.best_params_)\nprint(\"Best score:\",grid.best_score_)\n","2d41e219":"model = LinearRegression()\n\nreg_pipe = Pipeline(steps = [('preprocessing',preprocessor),\n                             ('selector', SelectFromModel(Lasso(alpha=0.001))),\n                            ('modelling', model)\n                            ])\n\nscores = -1*cross_val_score(reg_pipe, X,y,cv=5, scoring = 'neg_mean_squared_error')\nscores.mean()","9b23030c":"model = RandomForestRegressor(random_state=0)\n\nreg_pipe = Pipeline(steps = [('preprocessing',preprocessor),\n                             ('selector', SelectFromModel(Lasso(alpha=0.001))),\n                            ('modelling', model)\n                            ])\n\nscores = -1*cross_val_score(reg_pipe, X,y,cv=5, scoring = 'neg_mean_squared_error')\nscores.mean()","ca38394f":"# Tuned n_estimators=700,max_depth=8\n\nhparams = {'modelling__n_estimators':[700],\n          'modelling__max_depth':[8]}\n\ngrid = GridSearchCV(reg_pipe,param_grid = hparams,cv=5,scoring='neg_mean_squared_error',n_jobs=3,verbose=1)\ngrid.fit(X,y)\nprint(\"Best params\",grid.best_params_)\nprint(\"Best score:\",grid.best_score_)","740fb12d":"model = XGBRegressor(random_state=0)\n\nreg_pipe = Pipeline(steps = [('preprocessing',preprocessor),\n                             ('selector', SelectFromModel(Lasso(alpha=0.001))),\n                            ('modelling', model)\n                            ])\n\nscores = -1*cross_val_score(reg_pipe, X,y,cv=5, scoring = 'neg_mean_squared_error')\nscores.mean()","b7ba53cc":"#Previously put the pipeline through a GridSearchCV to tune the learning_rate and n_estimators:\n# Best params were learning_Rate=0.03, n_estimators = 700\nmodel = XGBRegressor(learning_rate=0.03, n_estimators=700,random_state=0)\n\nreg_pipe = Pipeline(steps = [('preprocessing',preprocessor),\n                             ('selector', SelectFromModel(Lasso(alpha=0.001))),\n                            ('modelling', model)\n                            ])\n\nscores = -1*cross_val_score(reg_pipe, X,y,cv=5, scoring = 'neg_mean_squared_error')\nscores.mean()","b477481a":"model = Lasso(alpha=0.001, max_iter=100000)\n\npipe = Pipeline(steps = [('preprocessing',preprocessor),\n                            ('modelling', model)\n                            ])\n\npipe.fit(X,y)\ny_preds = pipe.predict(X_test)","61b83e0e":"#transform saleprice predictions back from log\npredictions = np.expm1(y_preds)\n\n\noutput = pd.DataFrame({'Id':X_test_id,'SalePrice':predictions})\noutput.to_csv('submission.csv',index=False)\nprint(\"Output saved!\")","2f0cf9e7":"### 3.2.1 Tune RF Model:","033ba796":"## 1.2 Numeric Variables","8f6c0e01":"## 2.4 Engineering Categorical Variables","be348a8c":"# 3. Machine Learning","5aea4fd3":"# 1. EDA","86ca5ff6":"### 3.3.1 Tune XGBoost Model:","319eb6a4":"### 3.3 Fit XGBoost Model to Regularized Pipeline","d68131c2":"# House Price Competition\n\n","5e0ea8ef":"## 2.3 Transformations\n\nSeveral of the continuous numeric varables have skewed distributions. As we are using Lasso regression, which relies on an assumption of normality, we need to transform these features. The target, SalePrice, is also skewed so we will transform this too.","558d598e":"There are a number of variables with mainly one value, such as street and utilities. We wil add them to our list of variables to drop. Also, a few variables have several low count levels which we will group together. ","bfd2be7e":"## 2.6 Feature Selection Using Lasso Regularization","ef15d785":"There is clearly a difference in outcome for the missing value group in many of the variables. We will need to give this some thought when imputing these missing values later.","3490ddfa":"There's a very nice monotonic relationship between OverallQuall and SalePrice. Again, we can see many outliers.","7e2093ee":"There are clear differences in price over some categorical variables such as Neighborhood.  It's also clear that many of the categorical variables are ordinal, so we will further separate variables into nominal and ordinal features. Relationships can be seen above between SalePrice and some of these ordinal variables such as ExterQual, but custom encoding these features will make it easier to identify any relationships with SalePrice. ","76c65c7a":"Remaining missing values will be filled through imputation in a pipeline below. Continuous numeric variables will be filled with their median and the discrete and categorical variables will be filled with their modes. ","7b58559f":"## 1.1 Missing Values","468d9b76":"**Context:**\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\n**Dataset:**\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. With 79 explanatory variables, it describes (almost) every aspect of residential homes in Ames, Iowa, from the number of bedrooms to the quality of the pool. The target is the actual price of the property achieved at sale.  \n\n**Project Aim:**\nThe aim of this competition is to predict the sale price of each home using a predictive model. As the outcome is a continuous response we will apply supervised regression techniques.\n\n","5d0d408a":"### 3.4 Refit the Lasso Model to all Training Data","8c6cd94f":"## 1.3 Categorical Variables","b3aa18e6":"## 2.1 Handling Missing 'None' Values\n\nMany of the categorical variables have missing values when they should have 'None' because they don't have that feature, e.g. values in BsmtQual where there is no basement. We will replace these values with 'None'.","28bb2a91":"### 2.4.1 Grouping Small Levels - Nominal Features\n\nMany of the categorical variables have high cardinality with several low frequncy levels. For the nominal categorical variables we will group the small levels.","f50618b6":"### 3.1 Fit Linear Regression Model to Regularized Pipeline","9508b0e8":"### 1.2.1 Numeric Variables Relationship with Outcome and Each Other","44011e28":"There are some nice clear relationships here, like with ExterQual.","816e63b0":"We have an Id, 79 features and the outcome, SalePrice. We have a mixture of datatypes in our features and a number of missing values.","d93e051d":"The Lasso regression gave the lowest cross validated mean squared error score. We will refit this model to obtain predictions.","9022b62f":"### 3.2 Fit Random Forest Model to Regularized Pipeline","dbddbe5a":"There are quite a few missing values that we will have to deal with in feature engineering.","5f04831f":"There are some nice clear relationships with SalePrice here like GrLivArea and YearBuilt and many that have no correlation with SalePrice. We can see there are quite a few outliers too, such as LotFrontage>300.","38789dc1":"We will sort these variables into continuous and discrete variables. A number of the continuous variables, including the outcome, SalePrice, are skewed so we will consider transformations. Variables such as ['BsmtFinSF2', 'LowQualFinSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'] add little information. We will drop them during feature engineering. We will also drop Id. Several variables are candidates for feature generation such as number of bath and half baths or combining the porch variables into an AnyPorchSF variable.","f97bad0f":"### 1.3.1 Categorical Variables Relationship with Price","bd9f193b":"We have a few correlated features - we will drop one from each pair, the one with the lowere correlation with SalePrice. Drop GarageYrBlt, 1stFlrSF,TotRmsAbvGrd and GarageArea.","e8840df6":"### 2.4.2 Custom Encoding Ordinal Variables\n\nFor the ordinal variables we will custom encode the levels and then regroup any low frequency levels.","40e44f82":"## Contents\n\n- 1) Exploratory Data Analysis\n- 2) Feature Engineering\n- 3) Machine Learning","49a150ba":"# 2. Feature Engineering\nWe will perform a number of processes here:\n* Creat X and y, Update our variable lists, drop Id\n* Handle missing 'None' values \n* Generate features\n* Engineer numeric features such as transforming continuous variables where necessary \n* Engineer categorical features such as combining low frequency categorical levels","b6bd3d24":"## 2.5 Drop Unhelpful Columns\nWe will drop the unhelpful columns identified earlier. We will also drop MSSubClass and MoSold too as this was leading to problems with numeric categorical values.","140fcee5":"## 2.2 Feature Generation\n\nWe will create 3 new features, Total Square Footage of the property, Total Number of Bathrooms and Total Porch Area."}}