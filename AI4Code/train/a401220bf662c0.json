{"cell_type":{"ffa7ae88":"code","e2a53320":"code","1cbe6773":"code","43aed68f":"code","bd7da2a0":"code","80014de1":"code","db4be4a9":"code","9adc678f":"code","cac09274":"code","c1ab5474":"code","0d3ff6d7":"code","f249d6da":"code","ba9d380f":"code","0f217859":"code","81a32850":"code","ee5d3c80":"code","34f2e847":"code","56e978bf":"markdown","6d079d77":"markdown","41a0f916":"markdown","998b0617":"markdown","c25540cc":"markdown","1d385575":"markdown","5d7c4c3e":"markdown","d32d9290":"markdown","06608c15":"markdown","ea361bbd":"markdown","2949605f":"markdown","60b96d2f":"markdown","79b96539":"markdown","49f3302a":"markdown"},"source":{"ffa7ae88":"# Ignore warnings when using Tensorflow\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nos.environ[\"KMP_SETTINGS\"] = \"false\"\n\nimport csv\nimport pickle\nimport collections\nimport numpy as np\nimport tensorflow as tf\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Model, layers\nfrom tensorflow_transform import vocabulary, apply_vocabulary\nfrom tensorflow_addons.metrics import F1Score","e2a53320":"tokenizer = TweetTokenizer()\n\nwith open(\"\/kaggle\/input\/nlp-getting-started\/train.csv\", \"r\") as file:\n    data = [line[3:5] for line in csv.reader(file)][1:]  # text and target columns\n    sentences = [tokenizer.tokenize(d[0]) for d in data]  # tokenized strings\n    targets = [int(d[1]) for d in data]  # integer targets\n\nsize = len(sentences)\nprint(f\"The dataset contains {size} tweets.\")","1cbe6773":"collections.Counter(targets)","43aed68f":"words = [word for sentence in sentences for word in sentence]\nword_freq = collections.Counter(words)\nwords = list(set([word for word, freq in word_freq.items() if freq > 1]))\n\nvsize = len(words) + 1\nprint(f\"The dataset contains {vsize} unique words appearing at least twice.\")\n\nwith open(\"\/kaggle\/working\/vocabulary.pkl\", \"wb\") as file:\n    pickle.dump(words, file)","bd7da2a0":"class VocabEncoder():\n    def __init__(self, file=\"\/kaggle\/working\/vocabulary.pkl\"):\n        with open(file, \"rb\") as file:\n            self.words = pickle.load(file)\n            \n    def __call__(self, sentence):\n        return [self.words.index(word)+1 if word in self.words else 0 for word in sentence]","80014de1":"encoder = VocabEncoder()\nencoded = [encoder(sentence) for sentence in sentences]","db4be4a9":"X_train, X_validation, target_train, target_validation = train_test_split(encoded, targets, test_size=0.2, stratify=targets)","9adc678f":"train = tf.ragged.stack([tf.convert_to_tensor(x) for x in X_train]).with_row_splits_dtype(tf.int64)\ny_train = tf.one_hot(target_train, depth=2)\nvalidation = tf.ragged.stack([tf.convert_to_tensor(x) for x in X_validation]).with_row_splits_dtype(tf.int64)\ny_validation = tf.one_hot(target_validation, depth=2)","cac09274":"def define_model(**kwargs):\n    input = layers.Input([None], dtype=tf.int32, ragged=True, name=\"input\")\n    x = layers.Embedding(vsize, 4, name=\"embedding\")(input)\n    x = tf.reduce_mean(x, axis=1, name=\"reduce\")\n    x = layers.Dense(2, activation=\"softmax\", name=\"output\")(x)\n    return Model([input], [x])\n\nmodel = define_model()\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[F1Score(num_classes=2)])\nmodel.summary()","c1ab5474":"model.fit(train, y_train, validation_data=(validation, y_validation), batch_size=64, epochs=30)","0d3ff6d7":"def define_model(**kwargs):\n    input = layers.Input([None], dtype=tf.int32, ragged=True, name=\"input\")\n    x = layers.Embedding(vsize, 4, name=\"embedding\")(input)\n    x = layers.LSTM(16, name=\"reduce\")(x)\n    x = layers.Dense(2, activation=\"softmax\", name=\"output\")(x)\n    return Model([input], [x])\n\nmodel = define_model()\nmodel.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[F1Score(num_classes=2)])\nmodel.summary()","f249d6da":"model.fit(train, y_train, validation_data=(validation, y_validation), batch_size=64, epochs=10)","ba9d380f":"embedding_dim = 4\n\ndef positional_encoding(length, dim):\n    positions = np.arange(length)[:, np.newaxis]\n    dims = np.arange(dim)[np.newaxis, :]\n    encoding = positions \/ np.power(10000, (2 * (dims\/\/2)) \/ dim)\n    encoding[:, 0::2] = np.sin(encoding[:, 0::2])\n    encoding[:, 1::2] = np.cos(encoding[:, 1::2])\n    return tf.stack(encoding)\n\npos_train = tf.concat([positional_encoding(len(x), embedding_dim) for x in X_train], axis=0)\npos_train = tf.RaggedTensor.from_row_lengths(pos_train, row_lengths=train.row_lengths())\npos_validation = tf.concat([positional_encoding(len(x), embedding_dim) for x in X_validation], axis=0)\npos_validation = tf.RaggedTensor.from_row_lengths(pos_validation, row_lengths=validation.row_lengths())","0f217859":"def define_transformer_block(size, hidden_dim, name=\"transformer\"):\n    input = layers.Input([None, size], dtype=tf.float32, ragged=True, name=name+\"_input\")\n    x = input.to_tensor(shape=[None, 128, size])\n    mask = tf.reduce_all(x == 0, axis=-1)\n    \n    # Self-Attention\n    query = layers.Dense(size, use_bias=False)(x)\n    value = layers.Dense(size, use_bias=False)(x)\n    key = layers.Dense(size, use_bias=False)(x)\n    x = x + layers.Attention()([query, value, key], [mask, mask])\n    x = layers.LayerNormalization(axis=(1,2))(x)\n    \n    # MLP\n    x = tf.ragged.boolean_mask(x, mask)\n    mlp = layers.TimeDistributed(layers.Dense(hidden_dim, activation=\"relu\"))(x)\n    mlp = layers.TimeDistributed(layers.Dense(hidden_dim, activation=\"relu\"))(mlp)\n    x = x + layers.TimeDistributed(layers.Dense(size))(mlp)\n    x = x.to_tensor(shape=[None, 128, size])\n    x = layers.LayerNormalization(axis=(1,2))(x)\n    x = tf.ragged.boolean_mask(x, mask)\n    return Model([input], [x], name=name)\n    \ndef define_model(**kwargs):\n    input = layers.Input([None], dtype=tf.int32, ragged=True, name=\"input\")\n    positions = layers.Input([None, embedding_dim], dtype=tf.float32, ragged=True, name=\"positions\")\n    x = layers.Embedding(vsize, embedding_dim, name=\"embedding\")(input) + positions\n    x = define_transformer_block(4, 4, name=\"transformer2\")(x)\n    x = layers.LSTM(16, name=\"reduce\")(x)\n    x = layers.Dense(2, activation=\"softmax\", name=\"output\")(x)\n    return Model([input, positions], [x])\n\nmodel = define_model()\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[F1Score(num_classes=2)])\nmodel.summary()","81a32850":"model.fit((train, pos_train), y_train, validation_data=((validation, pos_validation), y_validation), batch_size=64, epochs=10)","ee5d3c80":"full = tf.concat([train, validation], axis=0)\ny_full = tf.concat([y_train, y_validation], axis=0)\npos_full = tf.concat([pos_train, pos_validation], axis=0)\n\nmodel = define_model()\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[F1Score(num_classes=2)])\nmodel.fit((full, pos_full), y_full, batch_size=64, epochs=10)","34f2e847":"with open(\"\/kaggle\/input\/nlp-getting-started\/test.csv\", \"r\") as file:\n    test_data = [(line[0], line[3]) for line in csv.reader(file)][1:]  # text and target columns\n    test_id = [d[0] for d in test_data]  # row ids\n    test_sentences = [tokenizer.tokenize(d[1]) for d in test_data]  # tokenized sentences\n    \ntest_encoded = [encoder(sentence) for sentence in test_sentences]\ntest = tf.ragged.stack([tf.convert_to_tensor(x) for x in test_encoded]).with_row_splits_dtype(tf.int64)\npos_test = tf.concat([positional_encoding(len(x), embedding_dim) for x in test_encoded], axis=0)\npos_test = tf.RaggedTensor.from_row_lengths(pos_test, row_lengths=test.row_lengths())\ny_test = model.predict((test, pos_test))\ntest_pred = np.argmax(y_test, axis=1)\n\nwith open(\"submission.csv\", \"w\") as file:\n    writer = csv.writer(file)\n    writer.writerow((\"id\", \"target\"))\n    for id, pred in zip(test_id, test_pred):\n        writer.writerow((id, pred))","56e978bf":"#### Recurrent Neural Network\n\nIn order to customize the reduction operation a recurrent neural network can be used. It uses the initialization $\\mathbf{h}_0 = 0$ and computes hidden states recursively $\\mathbf{h}_i = F(\\mathbf{x}_i, \\mathbf{h}_{i-1}), \\, i = 1, \\dots, s$ with a learnable transition operation $F$. The reduced sequence is then given by the final hidden state $\\mathbf{z} = \\mathbf{h}_s$.\n\nThe reduction operation is now a generic block that computes a trainable function.\n\n![disaster_lstm.png](attachment:70456622-883c-4896-8d0f-2e5b2b2120a0.png)","6d079d77":"## Data Preprocessing\n\nThe dataset is read and transformed into a suitable format to be processed by Tensorflow. Tweet texts can easily be tokenized using the [TweetTokenizer](https:\/\/www.nltk.org\/api\/nltk.tokenize.html) from `nltk` and we can then build a vocabulary that is used in the input pipline of new data.","41a0f916":"#### Building the Vocabulary\n\nIn order to encode the words as string we need to have a look at the whole vocabulary in the dataset. We then concentrate on the words that appear at least twice and give them an individual index whilst all words appearing only once are encoded as 0.\n\nAfter all important words have been assigned an index we need to store the mapping in a file such that it can be reused for new words.","998b0617":"#### Simple Transformer\n\nSince we want to reduce the input sequence to the probabilites of being a tweet about a true disaster we are fine with using transformer encoder blocks to transform the sequence before classifing it. The apply self-attention over the whole sequence to get residual interdependent updates $\\mathbf{x}_i \\leftarrow \\mathbf{x}_i + \\sum_{j=1}^s w_{ij}\\mathbf{v}_j$ where the attention weights $w_i$ are computed using Luong-style multiplicative attention. It also passes each sequence element through a multi-layer perceptron to increase\n\nUnfortunately, Tensorflow has no support for attention mechanisms and layer normalizations on ragged tensors. For that reason we pad the sequences with zero in the transformer block to a length of 128 which is more than enough to fit any sentece in the training set. \n\n![disaster_transformer.png](attachment:fc50c52e-5dbf-49f8-aa81-e1bc175a180a.png)\n\nSince the attention layer is not recurrent, it normally does not receive any information about the position of the elements in the input sequence. For that reason we compute a positional encoding that serves as an additional input and is then added to the embedded input words. The encoding of position $k$ in component $i < d$ of the $d$-dimensional embedding vector is given by \n$$\n    \\sin(10000^{-\\frac{2i}{d}}k), \\enspace\\text{$i$ even}, \\qquad \\cos(10000^{-\\frac{2i-1}{d}}k), \\enspace\\text{$i$ odd} \\enspace.\n$$","c25540cc":"# Disaster Tweets with a simple Transformer","1d385575":"#### Embedding and Logistic Regression\n\nA sentence is represented by the integer sequence of encodings $(w_1, \\dots, w_s) \\in \\mathbb{N}_0^{\\times s}$ for sentence length $s \\in \\mathbb{N}$. We first transform each component to a real number using an embedding $w \\mapsto \\mathbf{x} \\in \\mathbb{R}^d$ where we find that embedding dimension $d = 4$ is sufficient. \n\nTo deal with the variable length of the sentences the sequence of embedded vectors $(\\mathbf{x}_1, \\dots, \\mathbf{x}_s)$ needs to be reduced to one single vector. The easiest form of such a reduction is to just take the average\n$$ \\mathbf{z} = \\frac{1}{s} \\sum_{i=1}^s \\mathbf{x}_i \\in \\mathbb{R}^s \\enspace. $$\n\nFinally, the probabilities of the tweet being about a real disaster or not are computed using a linear transform to the two-dimensional space and a softmax\n$$\n    p_0 = \\frac{e^{\\mathbf{w}_0^T\\mathbf{z} + b_0}}{e^{\\mathbf{w}_0^T\\mathbf{z} + b_0} + e^{\\mathbf{w}_1^T\\mathbf{z} + b_1}} \\enspace, \\qquad\n    p_1 = \\frac{e^{\\mathbf{w}_1^T\\mathbf{z} + b_1}}{e^{\\mathbf{w}_0^T\\mathbf{z} + b_0} + e^{\\mathbf{w}_1^T\\mathbf{z} + b_1}} \\enspace. \n$$\n\nThis model can be visualized as a simple sequence of embedding, reduction and softmax operations.\n\n![disaster_embedding.png](attachment:e0215977-f101-42b2-ba95-65f523a9bd36.png)","5d7c4c3e":"#### Splitting into Train and Validation Set\n\nSince we are not particularily interested in the viability of the word encoding we split into train and validation set only now, after the vocabulary has been built. We choose to split 20% of the data off as validation set and stratify by label.\n\nImmediately afterwards we can convert the train and validation set to Tensorflow ragged tensords and build the datasets that can be used for training the models.","d32d9290":"By quickly counting the number of samples of each class we can confirm that the dataset is not unbalanced to the point where we need to take action and we can proceed.","06608c15":"Using transformer blocks overfits the training set quite easily. However, it seems like generalization can still not be improved over using the embedding layer alone.","ea361bbd":"## Training the Models","2949605f":"We now define a preprocessor object that loads the string to index mappings created before and can be used to encode new sentences as integer vectors. For now we just take a list of strings as input and return the mapped list of integers.\n\nThe sentences in the whole dataset can then be transformed into integer vectors such that they can later easily be passed to tensorflow.","60b96d2f":"## Inference\n\nWe first retrain the transformer model on the whole training dataset until we reach a loss function value that has proven to give the best generalization before. Then we can apply the preprocessing pipeline to the test dataset and run the model on them.","79b96539":"The model can now be trained for 30 epochs after which an F1 score of about 0.8 is achieved.","49f3302a":"Although the reduction operation is now trained, the model performance cannot be improved compared to simply using the sequence average."}}