{"cell_type":{"d4124cf9":"code","a592070b":"code","b0b755fe":"code","6ed44c9f":"code","174d8936":"code","85918044":"code","a5730f38":"code","bd5c17c5":"code","ad064d68":"code","c3aa14d1":"markdown","b4e579ee":"markdown","273deb62":"markdown","f541c466":"markdown","82df05c2":"markdown","cbbd7408":"markdown","50f4fbdb":"markdown","b973047d":"markdown","06780393":"markdown","7dc62ded":"markdown","0b993cce":"markdown","be02b1ab":"markdown","0d896f65":"markdown"},"source":{"d4124cf9":"from __future__ import print_function\nfrom keras.datasets import cifar10\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input, Reshape, Dense, Dropout, Activation, Flatten, PReLU\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers.normalization import BatchNormalization as BN\nfrom keras.layers import GaussianNoise as GN\nfrom keras.optimizers import SGD\nimport keras.backend as K\nimport numpy as np\nimport os\nimport tarfile\nimport sys\nimport pickle\nprint(os.listdir(\"..\/input\/cifar-10-python\/cifar-10-batches-py\"))\nimport keras","a592070b":"def load_batch(fpath, label_key='labels'):\n    \"\"\"Internal utility for parsing CIFAR data.\n\n    # Arguments\n        fpath: path the file to parse.\n        label_key: key for label data in the retrieve\n            dictionary.\n\n    # Returns\n        A tuple `(data, labels)`.\n    \"\"\"\n    with open(fpath, 'rb') as f:\n        if sys.version_info < (3,):\n            d = pickle.load(f)\n        else:\n            d = pickle.load(f, encoding='bytes')\n            # decode utf8\n            d_decoded = {}\n            for k, v in d.items():\n                d_decoded[k.decode('utf8')] = v\n            d = d_decoded\n    data = d['data']\n    labels = d[label_key]\n\n    data = data.reshape(data.shape[0], 3, 32, 32)\n    return data, labels\n\ntrain_num = 50000\nx_train = np.zeros(shape=(train_num,3,32,32))\ny_train = np.zeros(shape=(train_num))\n\ntest_num = 10000\nx_test = np.zeros(shape=(test_num,3,32,32))\ny_test = np.zeros(shape=(test_num))\n\ndef load_data():\n    for i in range(1,6):\n        begin = (i-1)*10000\n        end = i*10000\n        x_train[begin:end,:,:,:],y_train[begin:end] = load_batch(\"..\/input\/cifar-10-python\/cifar-10-batches-py\/data_batch_\"+str(i))\n    \n    x_test[:],y_test[:] = load_batch(\"..\/input\/cifar-10-python\/cifar-10-batches-py\/test_batch\")\n\nload_data()\nif K.image_data_format() == 'channels_last':\n    x_test = x_test.transpose(0, 2, 3, 1)\n    x_train = x_train.transpose(0, 2, 3, 1)","b0b755fe":"num_classes=10\n#(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\nx_train \/= 255\nx_test \/= 255\n\nprint(x_train.shape)\nprint(x_test.shape)\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","6ed44c9f":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(\n  featurewise_center=True,\n  featurewise_std_normalization=True,\n  width_shift_range=0.2,\n  height_shift_range=0.2,\n  rotation_range=20,\n  zoom_range=[1.0,1.2],\n  horizontal_flip=True)\n\ndatagen.fit(x_train)\n\ntestdatagen = ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n)\n\ntestdatagen.fit(x_train)","174d8936":"from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\nimport math\n\n# learning rate schedule\ndef step_decay(epoch,lr):\n    initial_lrate = 0.1\n    drop = 0.5\n    epochs_drop = 20.0\n    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n    return lrate\n\ndef scheduler(epoch):\n  if epoch < 25:\n    return .1\n  elif epoch < 50:\n    return 0.01\n  else:\n    return 0.001\n  \ndef nin_scheduler(epoch):\n  if epoch <= 60:\n      return 0.05\n  if epoch <= 120:\n      return 0.01\n  if epoch <= 160:\n      return 0.002\n  return 0.0004\n\nlrate = LearningRateScheduler(step_decay)\nlrate_nin = LearningRateScheduler(nin_scheduler)\nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              factor=0.2,\n                              min_lr=0.0001,\n                              patience=4)","85918044":"from keras.callbacks import ModelCheckpoint\nchkpoint = ModelCheckpoint('wideresnet_28x10-best.hdf5', monitor='val_acc', save_best_only=True)","a5730f38":"from keras.layers import Add, ZeroPadding2D\n\ndef bn_act(x, activation='relu'):\n    l = BN()(x)\n    if activation=='prelu':\n        l = PReLU()(l)\n    else:\n        l = Activation('relu')(l)\n    return l\n        \ndef res_block(convs, identity=True, k=1):\n    def inner(x):\n        if not identity:\n            strides = (2,2)\n        else:\n            strides = (1,1)\n            \n        act = bn_act(x)\n        l = Conv2D(convs*k, 3, strides=strides, padding='same', kernel_initializer='he_normal')(act)\n        \n        l = bn_act(l)\n        l = Dropout(0.5)(l)\n        l = Conv2D(convs*k, 3, strides=(1,1), padding='same', kernel_initializer='he_normal')(l)\n        \n        if not identity or x.shape[3]!=convs*k:\n            shortcut = Conv2D(convs*k, 1, strides=strides, padding='same', kernel_initializer='he_normal')(act)\n            l = Add()([l,shortcut])\n        else:\n            l = Add()([l,x])\n        return l\n    return inner\n\ndef wide_resnet(input_shape):\n    inpt = Input(shape = input_shape)\n\n    # stage 1\n    x = Conv2D(16, 3, strides=(1,1), padding='same', kernel_initializer='he_normal')(inpt)\n    #x = bn_act(x)\n    #x = MaxPooling2D(pool_size=(3,3),strides=2)(x)\n    \n    k=10 # widening factor\n    n=28 # depth or total number of layers\n    N = (n-4)\/\/6\n    # stage 2\n    for i in range(N):\n        x = res_block(16,k=k)(x)\n\n    # stage 3\n    x = res_block(32,False,k=k)(x)\n    for i in range(1,N):\n        x = res_block(32,k=k)(x)\n\n    # stage 4\n    x = res_block(64,False,k=k)(x)\n    for i in range(1,N):\n        x = res_block(64,k=k)(x)\n    \n    x = bn_act(x)\n    x = GlobalAveragePooling2D()(x)\n    outpt = Dense(num_classes, activation=\"softmax\")(x)\n    model = Model(inpt,outpt)\n\n    return model\n\n\ndef res_scheduler(epoch):\n    if epoch < 60:\n        return 0.1\n    if epoch < 120:\n        return 0.02\n    if epoch < 160:\n        return 0.004\n    return 0.0008\n\nlrate_res = LearningRateScheduler(res_scheduler)\n\nwideresnet = wide_resnet(x_train.shape[1:])\nwideresnet.summary()","bd5c17c5":"batch_size=128\nepochs=200\n\nopt = SGD(lr=0.1, momentum=0.9, nesterov=True)\nwideresnet.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nhistory=wideresnet.fit_generator(datagen.flow(x_train, y_train,batch_size=batch_size),\n                            steps_per_epoch=len(x_train) \/ batch_size, \n                            epochs=epochs,\n                            validation_data=testdatagen.flow(x_test, y_test),\n                            validation_steps=len(x_test) \/ batch_size,\n                            callbacks=[lrate_res,chkpoint],\n                            verbose=2)\n\n# scores = model.evaluate(x_test, y_test, verbose=1)\n#print('Test loss:', scores[0])\n#print('Test accuracy:', scores[1])\n","ad064d68":"import matplotlib.pyplot as plt\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('ResNet accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","c3aa14d1":"# Topologia","b4e579ee":"# Callbacks","273deb62":"## ResNet","f541c466":"Descargar el dataset de amazon y preprocesarlo para darle la forma que requiere la red. Cambiar el tipo de dato a float de 32 bits y normalizar a valores entre 0 y 1 para las im\u00e1genes de entrada. Para las salidas transoformarlas a \"one-hot\" vector.","82df05c2":"# Dataset","cbbd7408":"Se prueban distintas t\u00e9cicas de enfriamiento del factor de aprendizaje.\n\n\n1.   Decaimiento por pasos, reducir a la mitad cada 20 epocas.\n2.   Enfriamiento programado donde especificamos en que \u00e9poca concreta queremos bajar el factor de aprendizaje y a que valor concreto.\n\n\n\n","50f4fbdb":"Importando las capas necesarias para este problema","b973047d":"Callback para que guarde el modelo seiempre que mejore la precisi\u00f3n en el conjunto de validaci\u00f3n.","06780393":"## Enfriamiento LR","7dc62ded":"## Guardar modelos","0b993cce":"# Entrenamiento y test","be02b1ab":"# Imports","0d896f65":"# Data Augmentation\n"}}