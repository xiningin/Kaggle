{"cell_type":{"1bf98ad2":"code","fd03edd0":"code","51375251":"code","f8d637cb":"code","13c84533":"code","da185243":"code","89b0adac":"code","07c044a9":"code","83569e2e":"code","1d3ec552":"code","afe592e9":"code","9e3e3246":"code","0a180ef4":"code","6fe35596":"code","71234369":"code","3fbac84c":"code","be11438a":"code","d02cd445":"code","eef4ef2d":"code","8c498b5c":"code","00732fa6":"code","d57e4e8c":"code","7c7367a6":"code","425bdd89":"code","79b3a3ec":"code","290c2ba2":"code","c79c60f9":"code","bd93fb4e":"code","b0a941d7":"code","164d1711":"code","33cb3aa2":"code","7a1578ed":"code","d4096d17":"code","58d715e9":"code","8c625171":"code","fdc4d23e":"code","f33ab2d2":"code","62f19805":"code","111fcd56":"code","a72f2656":"code","dcdbb48c":"code","2512be0e":"code","8b061dc6":"code","570c57a9":"code","046505fa":"code","08c295c9":"code","ff88f855":"code","c6fe0f25":"code","1e966e35":"code","3120a4d2":"code","bbe67d18":"code","7d0ef082":"code","d7b4e6c8":"code","c06edddb":"code","741e21b9":"code","6e01f658":"code","f4670ace":"code","45b4146b":"code","b6a40c12":"code","60ee6a7d":"code","54db102e":"code","b8aeba2f":"code","599d5ee3":"code","c083269c":"code","ebd58eb7":"code","52926848":"code","92d9f669":"code","437b68f8":"code","77af5560":"code","dfc0164c":"code","df47cdfc":"code","94f5ec81":"code","ce1f5b6b":"markdown","925639aa":"markdown","9aab80b5":"markdown","1fddf465":"markdown","42687007":"markdown","116e019c":"markdown","f4319fb7":"markdown","9c8901b1":"markdown","0a92523e":"markdown","b7a5a571":"markdown","330ca49c":"markdown","9e4a970d":"markdown","87204ef7":"markdown","ccaccd93":"markdown","d81ad4d4":"markdown","cfce4df7":"markdown","db6ed453":"markdown","ecbe95a5":"markdown","66e8536e":"markdown","920fe867":"markdown","dd5896e3":"markdown","3bf188f1":"markdown","697b5200":"markdown","1f49ad38":"markdown","224011f3":"markdown"},"source":{"1bf98ad2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_predict, cross_val_score\nimport scipy.stats as stats\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import precision_score, recall_score, matthews_corrcoef, roc_auc_score, confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier,GradientBoostingClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import LabelEncoder","fd03edd0":"dataset = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\ndataset.head(5)","51375251":"dataset.shape","f8d637cb":"dataset.isnull().sum()","13c84533":"dataset.info()","da185243":"dataset.describe(include = 'object')","89b0adac":"dataset.describe()","07c044a9":"dataset.nunique()","83569e2e":"cat_cols = dataset.select_dtypes(include = 'object')\nfor x in cat_cols.columns:\n    plt.figure(figsize = (10,6))\n    sns.countplot(dataset[x],hue = dataset['Attrition'],palette = 'viridis')\n    plt.xticks(rotation = 90)\n    plt.title(x,fontweight='bold',size=15)\n    plt.show()","1d3ec552":"dataset['Attrition'] = dataset['Attrition'].replace({'Yes':1,'No':0})","afe592e9":"cat_cols = dataset.select_dtypes(include = 'object')\nfor x in cat_cols.columns:\n    z = pd.crosstab(dataset[x],dataset['Attrition'])\n    plt.figure(figsize = (10,6))\n    plt.pie(z[1], labels = z.index, autopct='%1.0f%%')\n    plt.title(x,fontweight='bold',size=15)\n    plt.show()","9e3e3246":"cat_cols = dataset.select_dtypes(include = 'object')\nfor x in cat_cols.columns:\n    z = pd.crosstab(dataset[x],dataset['Attrition'])\n    z['Sum'] = z.T.sum().values\n    for i in z.columns:\n        z[i] = (z[i]\/z['Sum'])*100\n    z.drop('Sum',1,inplace = True)\n    z.plot(kind = 'bar', stacked = True, color = ['teal','gold'])\n    plt.title(x,fontweight='bold',size=15)\n    plt.show()","0a180ef4":"num_col = dataset.select_dtypes(exclude = 'object')\nfor y in num_col.columns:\n    sns.boxplot(dataset['Attrition'],dataset[y],palette = 'tab20c_r')\n    plt.title(y,fontweight='bold',size=15)\n    plt.show()","6fe35596":"correlation_matrix = dataset.corr()\nplt.figure(figsize = (18,14))\nsns.heatmap(correlation_matrix,cmap = 'tab20c_r')\nplt.title('Correlation Matrix')\nplt.show()","71234369":"dataset.drop(['EmployeeCount','Over18','StandardHours','EmployeeNumber'],1,inplace = True)","3fbac84c":"num_col = dataset.select_dtypes(exclude = 'object')\nfeatures_to_drop = []\nfeatures_to_transform = []\nfor y in num_col.columns:\n    value_0 = dataset[dataset['Attrition']==0][y]\n    value_1 = dataset[dataset['Attrition']==1][y]\n    ttest_ind = stats.ttest_ind(value_1,value_0)\n    mann_whitney_u = stats.mannwhitneyu(value_1,value_0)\n    if ttest_ind[1]>0.05 and mann_whitney_u[1]>0.05:\n        features_to_drop.append(y)\n    elif ttest_ind[1]>0.05 and mann_whitney_u[1]<0.05:\n        features_to_transform.append(y)\n    else:\n        continue","be11438a":"cat_col = dataset.select_dtypes(include = 'object')\nfor y in cat_col.columns:\n    crosstab_matrix = pd.crosstab(dataset[y],dataset['Attrition'])\n    test_stat, pvalue, DOF, expected_value = stats.chi2_contingency(crosstab_matrix)\n    if pvalue>0.05:\n        features_to_drop.append(y)\n    else:\n        continue","d02cd445":"dataset.drop(features_to_drop,1,inplace = True)","eef4ef2d":"dataset['YearsSinceLastPromotion']= np.power(dataset[features_to_transform],0.3)","8c498b5c":"X = dataset.drop('Attrition',1)\ny = dataset['Attrition']","00732fa6":"categorical_X = X.select_dtypes(include = 'object')\nnumerical_X = X.select_dtypes(exclude = 'object')","d57e4e8c":"lb = LabelEncoder()\nX1 = pd.DataFrame()\nfor x in range(categorical_X.shape[1]):\n    label_X = lb.fit_transform(categorical_X.iloc[:,x])\n    X1[categorical_X.columns[x]] = label_X","7c7367a6":"combined_X = pd.concat([X1,numerical_X],1)\ncombined_X","425bdd89":"sc = StandardScaler()\ntransformed_numeric = sc.fit_transform(combined_X)\ntransformed_numeric_df = pd.DataFrame(transformed_numeric)\ntransformed_numeric_df.columns = combined_X.columns","79b3a3ec":"final_dataset = pd.concat([transformed_numeric_df,y],1)\nfinal_dataset.head(5)","290c2ba2":"X = final_dataset.drop('Attrition',1)\ny = final_dataset['Attrition']","c79c60f9":"logr=LogisticRegression()\nrfe=RFECV(logr, scoring = 'f1')\nrfe_fe=rfe.fit(X,y)\n\nrfe_rank=pd.DataFrame()\nrfe_rank['Feature']=X.columns\nrfe_rank['Rank']=rfe_fe.ranking_\nrfe_feature=rfe_rank[rfe_rank['Rank']==1]","bd93fb4e":"features = rfe_feature['Feature'].values\nfinal_X = X[features]","b0a941d7":"xtr,xte,ytr,yte = train_test_split(final_X,y,test_size = 0.3, random_state = 46)","164d1711":"# Logistic Regression\nlogr = LogisticRegression(random_state = 46)\nlogr.fit(xtr,ytr)","33cb3aa2":"y_pred = logr.predict(xte)\nprint(classification_report(yte,y_pred))","7a1578ed":"math_logr = matthews_corrcoef(yte,y_pred)\nmath_logr","d4096d17":"mcc_scores = []\nmcc_scores.append(math_logr)","58d715e9":"confusion_matrix(yte,y_pred)","8c625171":"# Regularized Decision Tree\ndt = DecisionTreeClassifier(random_state = 46)\nparams = {'max_depth': np.arange(1,15),'criterion':['entropy','gini']}\ngs = GridSearchCV(dt,params,cv = 5)\ngs.fit(xtr,ytr)","fdc4d23e":"gs.best_params_","f33ab2d2":"dt_reg = DecisionTreeClassifier(max_depth = 2, criterion = 'gini')\ndt_reg.fit(xtr,ytr)","62f19805":"y_pred = dt_reg.predict(xte)\nprint(classification_report(yte,y_pred))","111fcd56":"math_dt_reg = matthews_corrcoef(yte,y_pred)\nmath_dt_reg","a72f2656":"mcc_scores.append(math_dt_reg)","dcdbb48c":"confusion_matrix(yte,y_pred)","2512be0e":"# Random Forest\nrf = RandomForestClassifier(random_state = 46)\nparams = {'n_estimators': np.arange(1,15)}\ngs = GridSearchCV(rf,params,cv = 5)\ngs.fit(xtr,ytr)","8b061dc6":"gs.best_params_","570c57a9":"rf_reg = RandomForestClassifier(n_estimators = 12)\nrf_reg.fit(xtr,ytr)\n","046505fa":"y_pred = rf_reg.predict(xte)\nprint(classification_report(yte,y_pred))","08c295c9":"math_rf = matthews_corrcoef(yte,y_pred)\nmath_rf","ff88f855":"mcc_scores.append(math_rf)","c6fe0f25":"confusion_matrix(yte,y_pred)","1e966e35":"# K Nearest Neighbors\nknn = KNeighborsClassifier()\nparams = {'n_neighbors':np.arange(1,100), 'weights': ['distance','uniform']}\ngs = GridSearchCV(knn,params,cv = 5)\ngs.fit(xtr,ytr)","3120a4d2":"gs.best_params_","bbe67d18":"knn_tuned = KNeighborsClassifier(n_neighbors = 11, weights = 'distance')\nknn_tuned.fit(xtr,ytr)","7d0ef082":"y_pred = knn_tuned.predict(xte)\nprint(classification_report(yte,y_pred))","d7b4e6c8":"math_knn = matthews_corrcoef(yte,y_pred)\nmath_knn","c06edddb":"mcc_scores.append(math_knn)","741e21b9":"confusion_matrix(yte,y_pred)","6e01f658":"#Bagged Logistic Regression\nlogr = LogisticRegression(random_state = 20)\nbagged_logr = BaggingClassifier(logr,random_state = 20)\nbagged_logr.fit(xtr,ytr)","f4670ace":"y_pred = bagged_logr.predict(xte)\nprint(classification_report(yte,y_pred))","45b4146b":"math_bagged_logr = matthews_corrcoef(yte,y_pred)\nmath_bagged_logr","b6a40c12":"mcc_scores.append(math_bagged_logr)","60ee6a7d":"confusion_matrix(yte,y_pred)","54db102e":"# Boosted Logistic Regression\nboosted_logr = AdaBoostClassifier(logr,random_state = 46)\nboosted_logr.fit(xtr,ytr)","b8aeba2f":"y_pred = boosted_logr.predict(xte)\nprint(classification_report(yte,y_pred))","599d5ee3":"math_boosted_logr = matthews_corrcoef(yte,y_pred)\nmath_boosted_logr","c083269c":"mcc_scores.append(math_boosted_logr)","ebd58eb7":"confusion_matrix(yte,y_pred)","52926848":"# Gradient Boosting\ngb = GradientBoostingClassifier(random_state = 46)\ngb.fit(xtr,ytr)","92d9f669":"y_pred = gb.predict(xte)\nprint(classification_report(yte,y_pred))","437b68f8":"math_gboost = matthews_corrcoef(yte,y_pred)\nmath_gboost","77af5560":"mcc_scores.append(math_gboost)","dfc0164c":"confusion_matrix(yte,y_pred)","df47cdfc":"models = ['Logistic Regression','Regularized Decision Tree','Random Forest','K Nearest Neighbors','Bagged Logistic Regression','Boosted Logistic Regression','Gradient Boosting']\nresults = pd.DataFrame()\nresults['Matthews correlation coefficient Scores'] = mcc_scores\nresults.index = models","94f5ec81":"results","ce1f5b6b":"<a id=\"c\"> <\/a>\n## Data Visualization","925639aa":"### Correlation Heatmap","9aab80b5":"### **Employee Attrition is a common problem in every organization, and this sometimes leads to great resource and opportunity loss. Being able to identify employees who may leave the organization will help the HR department in better planning and allocation of resources.**\n\n### **In this particular problem, it is equally important to minimize both False Positives and False Negatives because the organization would not want to extend additional support to the employees who are planning to leave the organization anytime soon. It would also not want to unnecessarily penalize employees who have no plans of leaving the organization.**","1fddf465":"### Attrition in Various Categories(Absolute Terms)","42687007":"### Evaluation Metric\n\n**The Matthews correlation coefficient (MCC) is said to be a more reliable metric in case of imbalanced dataset because it produces a high score only if the prediction obtains good results in all of the four confusion matrix categories (true positives, false negatives, true negatives, and false positives), proportionally both to the size of positive elements and the size of negative elements in the dataset.**\n**A score of 1 represents perfect predictions and a score of -1 represents opposite predictions.**\n\n**It is calculated as :**\n![image.png](attachment:image.png)","116e019c":"# IBM Employee Attrition Prediction\n## By **[Deeksha Sureka](https:\/\/www.linkedin.com\/in\/deeksha-sureka-602b29136\/)**","f4319fb7":"<a id=\"d\"> <\/a>\n## **Statistical Analysis for Feature Selection**","9c8901b1":"<a id=\"j\"> <\/a>\n\n## Inferences\n\nBased on the Matthews correlation coefficient Scores, Bagged Logistics Regression is a clear winner. It is equally important to minimimize both False Negatives as well as False Positives and this trade off is best depicted by the Bagged Logistic Regression Model.\nAccording to the Bagged Logistic Model:\n\nTrue Nagatives = 351\n\nFalse Positives = 9\n\nTrue Positives = 31\n\nFalse Negative = 50\n\nThis analysis shows that predicting exact attrition is very difficult however it can be controlled if proper steps are taken to improve employee satisfaction. From the visualizations above, I have identified some reasons that could lead to attrition and they are:\n1. Business Travels - It is a business commitment, however, its also very important that we contemplate on finding alternatives to reduce business travels. This would not only cut cost but might also help in lowering the attrition rate.\n2. Overtime - Employee Satisfaction dips when they are made to do overtime on a regular basis. Occasional Overtimes are understandable but if happening regularly, thats when the HR team needs to ponder upon the workforce planning and management.\n3. Job Role -  38% of the total attrition is caused by Sales Representatives and Sales Executives followed by 26% by Laboratory Technitians. Its very important that the HR team takes regular feedback from the employees in these departments.\n\n","0a92523e":"## Dropping Columns with 1 or all unique values","b7a5a571":"<a id=\"a\"> <\/a>\n## Importing Libraries and Dataset","330ca49c":"<a id=\"i\"> <\/a>\n## Model Performance in a Nutshell","9e4a970d":"<a id=\"f\"> <\/a>\n## Final Feature Selection using RFECV","87204ef7":"### Attrition Percentage Within Categories","ccaccd93":"**AGE**\tNumerical Value\n\n**ATTRITION**\tEmployee leaving the company (0=no, 1=yes)\n\n**BUSINESS TRAVEL**\t(1=No Travel, 2=Travel Frequently, 3=Tavel Rarely)\n\n**DAILY RATE**\tNumerical Value - Salary Level\n\n**DEPARTMENT**\t(1=HR, 2=R&D, 3=Sales)\n\n**DISTANCE FROM HOME**\tNumerical Value - THE DISTANCE FROM WORK TO HOME\n\n**EDUCATION**\tNumerical Value\n\n**EDUCATION FIELD**\t(1=HR, 2=LIFE SCIENCES, 3=MARKETING, 4=MEDICAL SCIENCES, 5=OTHERS, 6= TEHCNICAL)\n\n**EMPLOYEE COUNT**\tNumerical Value\n\n**EMPLOYEE NUMBER**\tNumerical Value - EMPLOYEE ID\n\n**ENVIROMENT SATISFACTION**\tNumerical Value - SATISFACTION WITH THE ENVIROMENT\n\n**GENDER**\t(1=FEMALE, 2=MALE)\n\n**HOURLY RATE**\tNumerical Value - HOURLY SALARY\n\n**JOB INVOLVEMENT**\tNumerical Value - JOB INVOLVEMENT\n\n**JOB LEVEL**\tNumerical Value - LEVEL OF JOB\n\n**JOB ROLE**\t(1=HC REP, 2=HR, 3=LAB TECHNICIAN, 4=MANAGER, 5= MANAGING DIRECTOR, 6= REASEARCH DIRECTOR, 7= RESEARCH SCIENTIST, 8=SALES EXECUTIEVE, 9= SALES REPRESENTATIVE)\n\n**JOB SATISFACTION**\tNumerical Value - SATISFACTION WITH THE JOB\n\n**MARITAL STATUS**\t(1=DIVORCED, 2=MARRIED, 3=SINGLE)\n\n**MONTHLY INCOME**\tNumerical Value - MONTHLY SALARY\n\n**MONTHY RATE**\tNumerical Value - MONTHY RATE\n\n**NUMCOMPANIES WORKED**\tNumerical Value - NO. OF COMPANIES WORKED AT\n\n**OVER 18**\t(1=YES, 2=NO)\n\n**OVERTIME**\t(1=NO, 2=YES)\n\n**PERCENT SALARY HIKE**\tNumerical Value - PERCENTAGE INCREASE IN SALARY\n\n**PERFORMANCE RATING**\tNumerical Value - ERFORMANCE RATING\n\n**RELATIONS SATISFACTION**\tNumerical Value - RELATIONS SATISFACTION\n\n**STANDARD HOURS**\tNumerical Value - STANDARD HOURS\n\n**STOCK OPTIONS LEVEL**\tNumerical Value - STOCK OPTIONS\n\n**TOTAL WORKING YEARS**\tNumerical Value - TOTAL YEARS WORKED\n\n**TRAINING TIMES LAST YEAR**\tNumerical Value - HOURS SPENT TRAINING\n\n**WORK LIFE BALANCE**    Numerical Value - TIME SPENT BEWTWEEN WORK AND OUTSIDE\n\n**YEARS AT COMPANY**\tNumerical Value - TOTAL NUMBER OF YEARS AT THE COMPNAY\n\n**YEARS IN CURRENT ROLE**\tNumerical Value -YEARS IN CURRENT ROLE\n\n**YEARS SINCE LAST PROMOTION**\tNumerical Value - LAST PROMOTION\n\n**YEARS WITH CURRENT MANAGER**\tNumerical Value - YEARS SPENT WITH CURRENT MANAGER","d81ad4d4":"<a id=\"b\"> <\/a>\n## Basic Data Investigation","cfce4df7":"## Train Test Split","db6ed453":"## Data Dictionary","ecbe95a5":"<a id=\"e\"> <\/a>\n## Data Cleaning and Transformation","66e8536e":"<a id=\"g\"> <\/a>\n## Applying Classification Models and Evaluating Performance","920fe867":"### Inference from Heatmap\nThe target column has weak to no correlation with the input features. This means none of these input columns would be contributing much to the final predictions and this could be a possible reason that our machine learning models dont perform well.","dd5896e3":"## Table of Content\n1. **[Importing Libraries and Dataset](#a)**\n2. **[Basic Data Investigation](#b)**\n3. **[Data Visualization](#c)**\n4. **[Statistical Analysis for Feature Selection](#d)**\n5. **[Data Cleaning and Transformation](#e)**\n6. **[Final Feature Selection Using RFECV](#f)**\n7. **[Applying Classification Models and Evaluating Performance](#g)**\n8. **[Ensemble Techiniques for Model Tuning](#h)**\n9. **[Model Performance in a Nutshell](#i)**\n10. **[Inferences](#j)**","3bf188f1":"#  **Problem Statement**","697b5200":"<a id=\"h\"> <\/a>\n## Ensemble Techiniques for Model Tuning","1f49ad38":"### Attrition Vs Numerical Features","224011f3":"### Percentage Contribution to Attrition"}}