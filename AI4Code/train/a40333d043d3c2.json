{"cell_type":{"6629ddf2":"code","9cf3ebc3":"code","0d657094":"code","28308366":"code","c606a4ce":"code","4a67bdb2":"code","d943b368":"code","5d613ec9":"code","22394ff6":"code","4816e0d1":"code","38ad70e3":"code","5c5c4c80":"code","8f2863c0":"code","c8a031af":"code","c943eccf":"code","4eadbcef":"code","a78c85e8":"code","bd91ea46":"code","ea46a427":"code","d7aa925c":"code","825a6bf6":"code","fe14e484":"code","a2b7cee9":"code","3ae71875":"code","91cc6735":"code","0b5d2ee4":"code","d6fae7b6":"markdown","10110c72":"markdown"},"source":{"6629ddf2":"import xgboost as xgb\nimport numpy as np\nimport pandas as pd\nimport random\nimport optuna\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error","9cf3ebc3":"train = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")","0d657094":"train.head()","28308366":"df=train\nfrom sklearn.preprocessing import LabelEncoder\nfor c in df.columns:\n    if df[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values))\n        df[c] = lbl.transform(df[c].values)\ntrain=df","c606a4ce":"train","4a67bdb2":"df=test\nfor c in df.columns:\n    if df[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df[c].values))\n        df[c] = lbl.transform(df[c].values)\ntest=df","d943b368":"test","5d613ec9":"Name0=train['target'].unique()\nName=sorted(Name0)\nprint(Name)","22394ff6":"N=[]\nfor i in range(2):\n    N+=[i]\n    \nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","4816e0d1":"target = train['target'].map(normal_mapping)\ndata = train.drop(['target','id'],axis=1)\ntest = test.drop('id',axis=1)","38ad70e3":"columns=data.columns.to_list()\nprint(columns)","5c5c4c80":"def objective(trial,data=data,target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2,random_state=42)\n    param = {\n        'objective': trial.suggest_categorical('objective',['reg:logistic','reg:tweedie']), \n        'tree_method': trial.suggest_categorical('tree_method',['hist']),  # 'gpu_hist','hist'\n        'lambda': trial.suggest_loguniform('lambda',1e-3,10.0),\n        'alpha': trial.suggest_loguniform('alpha',1e-3,10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018,0.02]),\n        'n_estimators': trial.suggest_categorical('n_estimators', [1000,2000,4000,8000]),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24,48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1,300),\n        'use_label_encoder': trial.suggest_categorical('use_label_encoder',[False])\n    }\n    model = xgb.XGBClassifier(**param)      \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(test_x)\n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse","8f2863c0":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=16)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","c8a031af":"study.trials_dataframe()","c943eccf":"# shows the scores from all trials\noptuna.visualization.plot_optimization_history(study)","4eadbcef":"# interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","a78c85e8":"# shows the evolution of the search\noptuna.visualization.plot_slice(study)","bd91ea46":"# parameter interactions on an interactive chart.\noptuna.visualization.plot_contour(study, params=['colsample_bytree','max_depth'])","ea46a427":"# Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","d7aa925c":"# Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","825a6bf6":"Best_trial=study.best_trial.params\nprint(Best_trial)","fe14e484":"sample = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/sample_submission.csv\")\nsample","a2b7cee9":"preds = np.zeros((sample.shape[0]))\nkf = KFold(n_splits=5,random_state=48,shuffle=True)\nfor trn_idx, test_idx in kf.split(train[columns],target):\n    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n    y_tr,y_val=target.iloc[trn_idx],target.iloc[test_idx]\n    model = xgb.XGBClassifier(**Best_trial)\n    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    preds+=model.predict(test[columns])\/kf.n_splits   ###### predict_proba\n    rmse=mean_squared_error(y_val, model.predict(X_val),squared=False)\n    print(rmse)","3ae71875":"model","91cc6735":"print(preds.shape)\nprint(preds[0])","0b5d2ee4":"subm = sample\nsubm['target'] = np.where(preds<0.5,0,1).astype(int)\nsubm.to_csv('submission.csv',index=False)\nsubm","d6fae7b6":"# XGBoost with Optuna tuning\n* doc: \nhttps:\/\/github.com\/optuna\/optuna","10110c72":"### Objective candidate for XGBoost\n* Objective candidate: survival:aft\n* Objective candidate: binary:hinge\n* Objective candidate: multi:softmax\n* Objective candidate: multi:softprob\n* Objective candidate: rank:pairwise\n* Objective candidate: rank:ndcg\n* Objective candidate: rank:map\n* Objective candidate: reg:squarederror\n* Objective candidate: reg:squaredlogerror\n* Objective candidate: reg:logistic\n* Objective candidate: reg:pseudohubererror\n* Objective candidate: binary:logistic\n* Objective candidate: binary:logitraw\n* Objective candidate: reg:linear\n* Objective candidate: count:poisson\n* Objective candidate: survival:cox\n* Objective candidate: reg:gamma\n* Objective candidate: reg:tweedie"}}