{"cell_type":{"076af730":"code","f5e5dce1":"code","d60aa70f":"code","f7f87696":"code","4b3d15f9":"code","8bad8223":"code","87d4e159":"code","5088dadc":"code","aebaba51":"code","35da116a":"code","04055fa1":"code","4d75818f":"code","3b24e06b":"code","228458a5":"code","5687dd29":"code","48318535":"code","e6ec267d":"code","cd14d7a1":"code","a428e9d2":"code","c192e485":"code","20e430e8":"code","3257d2d9":"code","ca9457f6":"code","5a8377fd":"code","d2ee96ec":"code","453cfdb3":"code","cad34d03":"code","a846f2fd":"code","e4e2a7bd":"markdown","e9a676bb":"markdown","42f14f9e":"markdown","b0b5e7f5":"markdown","38a662f0":"markdown","614295d7":"markdown","bd296897":"markdown","9a9bd109":"markdown","e0c7cf40":"markdown","398ebba0":"markdown","0f9d8a83":"markdown","ca84f781":"markdown","b8137951":"markdown","bbfbfc8c":"markdown","314509c9":"markdown","d9688e32":"markdown","4f0a06ea":"markdown","4a47e975":"markdown"},"source":{"076af730":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f5e5dce1":"from sqlalchemy import create_engine\nimport sqlite3\nfrom datetime import timedelta\nfrom matplotlib.dates import DateFormatter\nimport matplotlib.dates as mdates\nimport datetime as dt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math","d60aa70f":"engine = create_engine('sqlite:\/\/\/..\/input\/clubhouse-dataset\/Clubhouse_Dataset_v5.db')\nengine.table_names()","f7f87696":"query = \"\"\"\nSELECT *\nFROM user\n\"\"\"\ndf=pd.read_sql(query, engine)","4b3d15f9":"df_og=df.copy(deep=True)\ndf_copy=df.copy(deep=True)\ndf=df.merge(df_copy, how='left', left_on='user_id', right_on='invited_by_user_profile')\ndf","8bad8223":"df = df.drop([\"name_x\", \"username_x\", \"photo_url_x\", \"num_following_x\", \"twitter_x\", \"instagram_x\", \"invited_by_club_x\", \"username_y\", \"name_y\", \"photo_url_y\", \"twitter_y\", \"instagram_y\", \"num_followers_y\", \"num_following_y\", \"invited_by_club_y\"], axis=1)\ndf","87d4e159":"df.loc[(pd.isnull(df['time_created_y'])), 'time_created_y'] = df['time_created_x']\ndf.rename(columns={'time_created_y': 'activity'}, inplace=True)\ndf","5088dadc":"def get_day(x): \n    return dt.datetime(x.year, x.month, x.day)\n\ndef get_month(x):\n    return dt.datetime(x.year, x.month, 1)\n\ndf['time_created_x']= pd.to_datetime(df['time_created_x'])\ndf['activity']= pd.to_datetime(df['activity'])\ndf['activity_date'] = df['activity'].apply(get_day)\ngroup_first_d = df.groupby('user_id_x')['time_created_x']\ngroup_last_d = df.groupby('user_id_x')['activity']\ndf['first_date'] = group_first_d.transform('min')\ndf['last_date'] = group_last_d.transform('max')\ndf['activity_month'] = df['activity'].apply(get_month)\ngroup_first_m = df.groupby('user_id_x')['time_created_x']\ndf['first_month'] = group_first_m.transform('min').apply(get_month)\ndf","aebaba51":"def get_ymd (df, column):\n    year=df[column].dt.year\n    month=df[column].dt.month\n    day=df[column].dt.day\n    return year, month, day\n\nlast_year, last_month, _ = get_ymd(df, 'last_date')\nfirst_year, first_month, _ = get_ymd(df, 'first_date')\n\nyears_diff = last_year - first_year\nmonths_diff = last_month - first_month\n\ndf['duration_month'] = years_diff * 12 + months_diff * 1 + 1\ndf","35da116a":"group_cohort = df.groupby(['first_month','duration_month'])\ncohort_data = group_cohort['user_id_x'].apply(pd.Series.nunique).reset_index()\ncohort_data","04055fa1":"cohort_counts = cohort_data.pivot(index='first_month', columns='duration_month', values='user_id_x')\ncohort_sizes = cohort_counts.iloc[:,0]\nretention = cohort_counts.divide(cohort_sizes, axis=0)\nretention.round(4)*100","4d75818f":"pd.to_datetime(retention.index)\nplt.subplots(figsize=(16, 8))\nsns.set()\nax = sns.heatmap(retention, annot=True, fmt='.1%', vmin=0.0, vmax=0.4, cmap='YlOrBr')\nax.set_yticklabels(retention.iloc[:].index.strftime('%b-%d-%Y'))\nplt.yticks()\nplt.title('Retention Rate')\nplt.show()","3b24e06b":"print(df['time_created_x'].min())\nprint(df['time_created_x'].max())","228458a5":"last_year, last_month, last_day = get_ymd(df, 'last_date')\nyears_diff_last = 2021 - last_year\nmonths_diff_last = 6 - last_month\ndays_diff_last = 10 - last_day\ndf['recency'] = years_diff_last * 365 + months_diff_last * 30 + days_diff_last\ndf","5687dd29":"data_recency = df.groupby('user_id_x')['recency'].apply(min).reset_index()","48318535":"data_frequency = df.groupby('user_id_x').size().reset_index(name='count')","e6ec267d":"data_influence = df.groupby('user_id_x')['num_followers_x'].apply(max).reset_index()","cd14d7a1":"data_rf = pd.merge(left=data_recency, right=data_frequency, how='inner', left_on='user_id_x', right_on='user_id_x')\ndata_rfi = pd.merge(left=data_rf, right=data_influence, how='inner', left_on='user_id_x', right_on='user_id_x')\ndata_rfi.rename(columns={'user_id_x': 'id', 'count': 'frequency', 'num_followers_x': 'influence'}, inplace=True)\ndata_rfi = data_rfi.set_index('id')\ndata_rfi","a428e9d2":"print(data_rfi.mean().round(1))","c192e485":"print('Top 5% frequency quantile:', data_rfi['frequency'].quantile(q = 0.95))\ndata_rfi['frequency'].quantile(q = 0.95)\ndata_5qf = data_rfi[data_rfi['frequency'] >= data_rfi['frequency'].quantile(q = 0.95)]\ndata_5qf_14d=data_5qf[data_5qf['recency']<=14]\ndata_5qf_14d","20e430e8":"print(data_5qf_14d.mean().round(1))","3257d2d9":"all_colors = ['silver'] * 3\nqf_colors = ['Orange'] * 3\nCategories = ['Recency', 'Frequency', 'Influence']\nall = [116.8,1.5,101]\nqf = [13.8, 23.4, 1520.2]\ntrace1 = go.Bar(\n    x = Categories,\n    y = all,\n    name = 'All Users',\n    marker_color = all_colors\n)\ntrace2 = go.Bar(\n    x = Categories,\n    y = qf,\n    name = 'Top 5% freq. active in 14 days',\n    marker_color = qf_colors\n)\ndata = [trace1, trace2]\nlayout = go.Layout(barmode = 'group')\nfig = go.Figure(data = data, layout = layout)\nfig.update_yaxes(type=\"log\")\nfig.update_xaxes(\n        title_text = \"Categories\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_yaxes(\n        title_text = \"Average on a log scale\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.show()","ca9457f6":"print('Top 0.5% recency:', data_rfi['recency'].quantile(q = 0.005))\nprint('Top 0.5% frequency:', data_rfi['frequency'].quantile(q = 0.995))\nprint('Top 0.5% influence:', data_rfi['influence'].quantile(q = 0.995))\n\nsel_data_05qr = data_rfi['recency'] <= data_rfi['recency'].quantile(q = 0.005)\nsel_data_05qf = data_rfi['frequency'] >= data_rfi['frequency'].quantile(q = 0.995)\nsel_data_05qi = data_rfi['influence'] >= data_rfi['influence'].quantile(q = 0.995)\ndata_05rfi = data_rfi[sel_data_05qr & sel_data_05qf & sel_data_05qi]\ndata_05rfi","5a8377fd":"print(data_05rfi.mean().round(1))","d2ee96ec":"all_colors = ['silver'] * 3\nqf_colors = ['Orange'] * 3\nqfi_colors = ['YellowGreen'] * 3\nCategories = ['Recency', 'Frequency', 'Influence']\nall = [116.8,1.5,101]\nqf = [13.8, 23.4, 1520.2]\nqfi = [14.4, 75.7, 20968.2]\ntrace1 = go.Bar(\n    x = Categories,\n    y = all,\n    name = 'All Users',\n    marker_color = all_colors\n)\ntrace2 = go.Bar(\n    x = Categories,\n    y = qf,\n    name = 'Top 5% freq. active in 14 days',\n    marker_color = qf_colors\n)\ntrace3 = go.Bar(\n    x = Categories,\n    y = qfi,\n    name = 'Top 0.5% quantile',\n    marker_color = qfi_colors\n)\ndata = [trace1, trace2, trace3]\nlayout = go.Layout(barmode = 'group')\nfig = go.Figure(data = data, layout = layout)\nfig.update_yaxes(type=\"log\")\nfig.update_xaxes(\n        title_text = \"Categories\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.update_yaxes(\n        title_text = \"Average on a log scale\",\n        title_font = {\"size\": 16},\n        title_standoff = 12)\nfig.show()","453cfdb3":"print('Top 0.03% recency:', data_rfi['recency'].quantile(q = 0.0003))\nprint('Top 0.03% frequency:', data_rfi['frequency'].quantile(q = 0.9997))\nprint('Top 0.03% influence:', data_rfi['influence'].quantile(q = 0.9997))\n\nsel_data_003qr = data_rfi['recency'] <= data_rfi['recency'].quantile(q = 0.0003)\nsel_data_003qf = data_rfi['frequency'] >= data_rfi['frequency'].quantile(q = 0.9997)\nsel_data_003qi = data_rfi['influence'] >= data_rfi['influence'].quantile(q = 0.9997)\ndata_003rfi = data_rfi[sel_data_003qr & sel_data_003qf & sel_data_003qi]\ndata_top10 = data_003rfi.sort_values('influence', ascending=False).head(10)\ndata_top10","cad34d03":"print(data_top10.mean().round(1))","a846f2fd":"r_colors=['Orange', 'YellowGreen', 'DodgerBlue']\nfig = make_subplots(rows=1, cols=3, subplot_titles=['Recency', 'Frequency', 'Influence'])\nfig.add_trace(\n    go.Bar(x = ['Top 5% freq.', 'Top 0.5%', 'Top 10'], \n    y = [13.8, 14.4, 11.5], \n    marker_color = r_colors), row=1, col=1\n)\nfig.add_trace(\n    go.Bar(x = ['Top 5% freq.', 'Top 0.5%', 'Top 10'], \n    y = [23.4, 75.7, 812.3], \n    marker_color = r_colors), row=1, col=2\n)\nfig.add_trace(\n    go.Bar(x = ['Top 5% freq.', 'Top 0.5%', 'Top 10'], \n    y = [1520.2, 20968.2, 60098.1], \n    marker_color = r_colors), row=1, col=3\n)\nfig.update_layout({'title':{\n    'text':'Comparison between 3 Groups (Log Scale)', 'x':0.5, 'y':0.9}\n}, showlegend=False)\nfig.update_yaxes(type=\"log\")\nfig.update_yaxes(\n        title_text = \"Average on a Log Scale\",\n        title_font = {\"size\": 12},\n        title_standoff = 0)\nfig.show()","e4e2a7bd":"- Plot a bar chart to see the difference between this group and the total users in terms of the average number of 3 different subjects. (The chart uses a log scale.)","e9a676bb":"## 1. Introduction\nThe given dataset is the user profiles of Clubhouse. The total number of users in this dataset are 8,427,058 users. Clustering is the subject of this analysis.\n\n## 2. Objectives\nTo know the clusters, drawing out the figures would help understand the cohorts. Thus, here comes the questions that this analysis would like to demonstrate.\n\n1. What is the retention rate of the users in terms of the date since these users sign up.\n2. Who are the top 5% frequency quantile with valid activity in 14 days? Who are the top 0.5% quantile in recency, frequency, and influence? Who are the top 10 most valuable users?\n3. Visuals to understand the different numbers between users groups.\n\n## 3. Method\n- Python\n\nUsing Python to cluster the customers by a simple linear quantile method.\n\n## 4. Prepare\n- First of all, import functions that will be used.","42f14f9e":"## 5. Process\n- In order to calculate valid activities, here to self-join the data on the user id and the invitation by users. After joining, the total number of rows goes up to 12,686,728 rows. ","b0b5e7f5":"- Calculate the duration month","38a662f0":"- Since some users have never invite others, fill the NA value with the date that the account is created as an initial activity.","614295d7":"- Plot a heatmap to visualize the retention rates in terms of the dutation and the date that the account is created. ","bd296897":"- Lastly, plot bar charts in different subjects to compare different user groups. (log scales)","9a9bd109":"- Generate a new dataframe for recency, frequency, and influence(Follower numbers). It will be more friendly for the system to run the data. Otherwise, the original data frame is to huge to process.","e0c7cf40":"## 6. Analyze\n- Add columns for the first activity and the last activity dates. ","398ebba0":"- Here is to subset the data by the top 0.5% quantile in recency, frequency, and influence. The total number of users is 438.","0f9d8a83":"## 7. Conclusion\u00b6\nIn sum, here are viewpoints as below.\n\n1. Users who joined from Dec 2020 to Feb 2021 have a relatively higher duration rate.\n\n2. 1,845 users are in the top 5% frequency quantile with an activity in the past two weeks. 438 users are in the top 0.5% quantile in recency, frequency and influence. The IDs of the top 10 most valuable users are shown in the table above.\n\n3. Top 10 valuable users have a huge gap between the top 0.05% quantile users in both frequency and influence. A strategy is needed to escalate the top 0.05% quantile users as frequently active users in order to attract more and more new users to join the game.\n\nSuggested further analysis.\n\n1. Due to the huge dataset consuming the platform resource, some types of analysis are restricted. Loading the data in chunks would help to extend the analysis.\n\n2. Clustering the users by using machine learning such as KMeans.\n\n3. Other visual types to explore more insights.","ca84f781":"- Get the number of users in terms of how many month they have been with valid activities. Then, generate the matrix by using a pivot table.","b8137951":"- Load SQL data into a data frame.","bbfbfc8c":"- Again, plot a bar chart to compare different groups. (The chart uses a log scale)","314509c9":"- Find out the last date that the last account is created. The last date is June 9th, 2021. Thus, here is to set June 10th, 2021, as the date of this analysis is processing in order to calculate the recency. (The lower recency, the better)","d9688e32":"- Here is to subset the data by the top 5% frequency quantile and the last activity in 2 weeks. The total number of users is 1,845.","4f0a06ea":"- Subset the data by 0.03% quantile in the recency, frequency, and indfluence to get 11 users. Then, sort the value by influence to pick the top 10 users.","4a47e975":"- Remove unnecessary columns"}}