{"cell_type":{"ddc9db8a":"code","396cca67":"code","1b65eca3":"code","ba5e2e36":"code","7264e96e":"code","828a755b":"code","186863ab":"code","19712a82":"code","820d7e92":"code","918de08e":"code","9c8c5863":"code","3b8d870a":"code","96768d84":"code","b027b7b3":"code","5b0e687c":"code","685e0721":"code","758823af":"code","fc06042c":"code","07e0d9f6":"code","dcd00878":"code","11ba353d":"markdown","84c9dd01":"markdown","f7a6a649":"markdown","1040f6b0":"markdown","4149b0e4":"markdown","577d4639":"markdown","b29a7da6":"markdown","df86a077":"markdown","582b963a":"markdown"},"source":{"ddc9db8a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","396cca67":"data = pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndata[\"class\"] = [1 if each == \"Abnormal\" else 0 for each in data[\"class\"]]","1b65eca3":"data.info()","ba5e2e36":"data.describe()","7264e96e":"def detect_outliers(data,features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(data[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(data[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # Detect outlier and their indices\n        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n        # Store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i,v in outlier_indices.items() if v > 2)\n        \n    return multiple_outliers\n\ncolumns = data.iloc[:,0:]\noutliers = data.loc[detect_outliers(data,columns)]","828a755b":"outliers","186863ab":"data = data.drop(detect_outliers(data,columns),axis = 0).reset_index(drop = True)","19712a82":"data.info()","820d7e92":"data.describe()","918de08e":"abnormal = data[data[\"class\"] == 1]\nnormal = data[data[\"class\"] == 0]","9c8c5863":"# scatter plot\nplt.scatter(abnormal.pelvic_incidence,abnormal.pelvic_radius,color=\"red\",label = \"Abnormal\",alpha = 0.6)\nplt.scatter(normal.pelvic_incidence,normal.pelvic_radius,color=\"green\",label = \"Normal\",alpha = 0.5)\nplt.xlabel(\"pelvic?incidence\")\nplt.ylabel(\"pelvic_tilt\")\nplt.show()","3b8d870a":"y = data[\"class\"].values\nx_data = data.drop([\"class\"],axis=1)","96768d84":"y","b027b7b3":"x_data.head()","5b0e687c":"x = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","685e0721":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=42)","758823af":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(x_train,y_train)\npred = knn.predict(x_test)","fc06042c":"pred","07e0d9f6":"print(\"{} nn score:{}\".format(1,knn.score(x_test,y_test)))","dcd00878":"# find k value\n\nscore_list = []\nfor i in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = i)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()","11ba353d":"As can be seen, when our k value is 1, our accuracy rate is maximum.","84c9dd01":"## Drop Outliers","f7a6a649":"<a id=\"4\"><a><br>\n# Train Test Split","1040f6b0":"<a id=\"5\"><a><br>\n# KNN Model:\n    1. Pick a value for K.\n    2. Search for the K observations in the training data that are \"nearest\" to the measurements of the unknown iris\n    3. Use the most popular response value from the K nearest neighbors as the predicted response value for the unknown iris\n* This would always have 100% accuracy, because we are testing on the exact same data, it would always make correct predictions\n* KNN would search for one nearest observation and find that exact same observation\n    * KNN has memorized the training set\n    * Because we testing on the exact same data, it would always make the same prediction\n","4149b0e4":"<a id=\"1\"><a><br>\n# Read Data","577d4639":"<a id = \"2\"><a><br>\n# Outlier Detection","b29a7da6":"I said an estimated 1. Here, a method can be used to find the n_neighbors value.","df86a077":"<a id=\"3\"><a><br>\n# Normalization","582b963a":"# Contents:\n   * [Read Data](#1)\n   * [Outlier Detection](#2)\n   * [Normalization](#3)\n   * [Train Test Split](#4)\n   * [KNN Model](#5)"}}