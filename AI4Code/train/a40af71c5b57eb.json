{"cell_type":{"b28639a7":"code","8c48b538":"code","1261f19e":"code","c01fb86b":"code","2a29c1e8":"code","d11058a0":"code","65d79100":"code","ae4b6fe6":"code","9ebd8afa":"code","b9f0a9c8":"code","f301e8ba":"code","fca66bf8":"code","9f87d05a":"markdown","b7055209":"markdown","644d6fd4":"markdown","0e4aded5":"markdown","dbbbd789":"markdown","1bee1de7":"markdown","1a801018":"markdown","6405a98a":"markdown","46d68557":"markdown","5d9aa87e":"markdown","cf188e46":"markdown","a8caf93e":"markdown","492da23a":"markdown","9ecb2502":"markdown","f49b6fec":"markdown","82d55d0b":"markdown"},"source":{"b28639a7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8c48b538":"df = pd.read_csv('..\/input\/tsf-datasets\/Iris.csv')","1261f19e":"df.head()","c01fb86b":"df = df.drop('Id', axis = 1)","2a29c1e8":"df.head()","d11058a0":"df.isnull().sum()","65d79100":"#Pair-Plot\nsns.pairplot(df, hue = 'Species', diag_kind = 'hist')\nplt.plot()","ae4b6fe6":"#Correlation\nsns.heatmap(df.corr(), annot = True)\nplt.plot()","9ebd8afa":"x = df.iloc[:, [0, 1, 2, 3]].values\n\nfrom sklearn.cluster import KMeans\nwcss=[]             #within cluster sum of squares\n\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)","b9f0a9c8":"plt.plot(range(1,11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","f301e8ba":"kmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_kmeans = kmeans.fit_predict(x)","fca66bf8":"#Plotting CLusters\nplt.rcParams[\"figure.figsize\"] = 10, 10\n\nplt.scatter(x[y_kmeans == 0, 0], x[y_kmeans == 0, 1], s = 100, c = 'red', label = 'Iris-setosa')\n\nplt.scatter(x[y_kmeans == 1, 0], x[y_kmeans == 1, 1], s = 100, c = 'green', label = 'Iris-versicolour')\n\nplt.scatter(x[y_kmeans == 2, 0], x[y_kmeans == 2, 1], s = 100, c = 'blue', label ='Iris-virginica')\n\n#Plotting Centroids of the CLusters\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 250, c = 'yellow', label = 'Centroids')\n\nplt.legend()\nplt.show()","9f87d05a":"### Finding the optimum number of clusters for k-means classification","b7055209":"# Author : Akash Kothare","644d6fd4":"### Dropping ID Column","0e4aded5":"From this, we choose the <b>number of clusters as 3<\/b>","dbbbd789":"### Checking for Null Values","1bee1de7":"In this task, we have to develop a classifier for the 'Iris' dataset and predict an optimum numbers of clusters and thus viusalizing them.","1a801018":"The dataset has no null values, thus no need to clean it","6405a98a":"### Visualising the clusters - On the first two columns","46d68557":"### Plotting the results onto a line graph to observe 'The Elbow'\n","5d9aa87e":"### Using Seaborn features : Pair-Plot and Correlation to check dependencies","cf188e46":"You can clearly see why it is called 'The elbow method' from the above graph, the optimum clusters is where the elbow occurs. This is when the within cluster sum of squares (WCSS) doesn't decrease significantly with every iteration.","a8caf93e":"## Task 2: Prediction using Unsupervised ML\n","492da23a":"### Loading Dataset","9ecb2502":"Data Science & Business Analytics Intern (Batch - Dec'20)","f49b6fec":"### Importing Libraries","82d55d0b":"### Creating the KMeans Classifier"}}