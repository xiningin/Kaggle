{"cell_type":{"d7b9d884":"code","b56b01fe":"code","90aed273":"code","cc353f7f":"code","4aa6e7e4":"code","0a7343b0":"code","fd43f710":"code","695bc7ee":"markdown","1f9333ea":"markdown","d6234bb0":"markdown","81683ccb":"markdown","7608b14e":"markdown","8d7bd9d7":"markdown","8c0bc776":"markdown","72f25644":"markdown","58a1c20c":"markdown","66a39084":"markdown","a3e508bd":"markdown","9f640452":"markdown","27695108":"markdown","1746aac0":"markdown","e8bde657":"markdown"},"source":{"d7b9d884":"# Import the required packages\nimport scipy as sp\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn')\nsns.set_style(\"darkgrid\")\n\n# Define the colour scheme\nc1 = \"#173f5f\"\nc2 = \"#20639b\"\nc3 = \"#3caea3\"\nc4 = \"#f6d55c\"\nc5 = \"#ed553b\"\n\nprint(\"Imported the required packages and defined the colour scheme\")","b56b01fe":"### Parameters\nloc = 0\nscale = 1\nsize = 50\nrandom_state = 123\n\n### Sampling from normal distribution\nsample = sp.stats.norm.rvs(loc=loc, scale=scale, size=size, random_state=random_state)\nsample_sorted = np.sort(sample)\n\n### Print the results\nprint(\"Number of random variates sampled: {:,.0f}\".format(size))\nprint(\"First 3 random variates sampled: {}\".format(sample[:3]))","90aed273":"### Theoretical PDF\nx_pdf = np.linspace(sample.min(), sample.max(), 100)\ny_pdf = sp.stats.norm.pdf(x=x_pdf, loc=loc, scale=scale)\n\n### Theoretical CDF\nx_cdf = np.linspace(sample.min(), sample.max(), 100)\ny_cdf = sp.stats.norm.cdf(x=x_pdf, loc=loc, scale=scale)\n\n### Emperical CDF\nx_cdf_sample = sample_sorted\nn = x_cdf_sample.size\ny_cdf_sample = np.arange(1, n+1) \/ n\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].hist(sample, bins=15, color=c1, density=True, label=\"Emperical\")\nax[0].plot(x_pdf, y_pdf, color=c5, label=\"Theoretical\")\nax[0].set(title=\"Probability Density Function\", xlabel=\"Random Variates\", ylabel=\"Density\")\nax[0].legend()\n\nax[1].step(x_cdf_sample, y_cdf_sample, where='post', color=c1, label=\"Emperical\")\nax[1].plot(x_cdf, y_cdf, color=c5, label=\"Theoretical\")\nax[1].set(title=\"Cumulative Distribution Function\", xlabel=\"Random Variates\", ylabel=\"Cumulative Probability\")\nax[1].legend()\nplt.show()","cc353f7f":"# Calculate the percentile area\nsample_index = np.arange(1, size+1, 1)\npercentile_area = sample_index \/ size\n\n# Theoretical Z-scores\nz_theoretical = sp.special.ndtri(percentile_area)\n\n# Experical Z-scores\nexperical_mean_diff = sample_sorted - np.mean(sample_sorted)\nexperical_std = np.std(sample_sorted, ddof=0)\nz_emperical = experical_mean_diff \/ experical_std\n\n# Max range of Z-Scores\nmax_z = np.max((abs(np.min((z_theoretical[:-1], z_emperical[:-1]))), np.max((z_theoretical[:-1], z_emperical[:-1]))))\n\n# Theroetical Percentiles\npercentile_theoretical = sp.stats.norm.cdf(x=sample_sorted, loc=loc, scale=scale)\n\n# Plot Theoretical vs. Emperical\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].plot([0, 1], [0, 1], color=c5, label=\"Theoretical Normal\")\nax[0].scatter(y_cdf_sample, percentile_theoretical, color=c1, label=\"Sample Distribution\")\nax[0].set(title=\"P-P Plot\", xlabel=\"Emperical Percentile\", ylabel=\"Theoretical Percentile\")\nax[0].legend()\n\nax[1].plot([-max_z, max_z], [-max_z, max_z], color=c5, label=\"Theoretical Normal\")\nax[1].scatter(z_emperical, z_theoretical, color=c1, label=\"Sample Distribution\")\nax[1].set(title=\"Q-Q Plot\", xlabel=\"Emperical Z-Score\", ylabel=\"Theoretical Z-Score\")\nax[1].legend()\nplt.show()","4aa6e7e4":"### Emperical CDF\nx_cdf_sample = sample_sorted\nn = x_cdf_sample.size\ny_cdf_sample = np.arange(1, n+1) \/ n\n\n### Theoretical CDF\nx_theoretical_cdf = np.linspace(sample_sorted.min(), sample.max(), 100)\ny_theoretical_cdf = sp.stats.norm.cdf(x=x_pdf, loc=loc, scale=scale)\ny_theoretical_cdf_discrete = sp.stats.norm.cdf(x=x_cdf_sample, loc=loc, scale=scale)\n\n### Differnce between the emperical and theoretical CDF\ncdf_diff = abs(y_cdf_sample - y_theoretical_cdf_discrete)\nmax_cdf_diff = max(cdf_diff)\nks_statistic, p_value = sp.stats.kstest(sample_sorted, 'norm')\n\nprint(\"KS Statistic (p-value): {:.4f} ({:.4f})\".format(ks_statistic, p_value))\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].step(x_cdf_sample, y_cdf_sample, where='post', color=c1, label=\"Emperical\")\nax[0].plot(x_theoretical_cdf, y_theoretical_cdf, color=c5, label=\"Theoretical\")\nax[0].set(title=\"Cumulative Distribution Function\", xlabel=\"Random Variates\", ylabel=\"Cumulative Probability\")\nax[0].legend()\n\nax[1].plot(x_cdf_sample, cdf_diff, color=c1, label=\"_nolabel_\")\nax[1].fill_between(x_cdf_sample, 0, cdf_diff, color=c1, alpha=0.3, label=\"_nolabel_\")\nax[1].set(title=\"Kolmogorov Smirnov Plot\", xlabel=\"Random Variates\", ylabel=\"Emperical CDF - Theoretical CDF\")\nax[1].axhline(y=max_cdf_diff, color=c2, lw=1, ls=\"--\")\nax[1].text(-3, 0.087, 'KS = {:,.4f}'.format(ks_statistic), color=c2)\nax[1].legend()\nplt.show()","0a7343b0":"# Create a Log-Normal sample \nshape = 1; loc = 0; scale = 1; size = 100; random_state = 123 \nsample = sp.stats.lognorm.rvs(s=shape, loc=loc, scale=scale, size=size, random_state=random_state)\nsample_sorted = np.sort(sample)\n\n### Emperical CDF\nx_cdf_sample = sample_sorted\nn = x_cdf_sample.size\ny_cdf_sample = np.arange(1, n+1) \/ n\n\n### Plot the sampled PDF and CDF against the theoretical distribution\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].hist(sample, bins=15, color=c1, density=True, label=\"Emperical\")\nax[0].set(title=\"Probability Density Function\", xlabel=\"Random Variates\", ylabel=\"Density\")\n\nax[1].step(x_cdf_sample, y_cdf_sample, where='post', color=c1, label=\"Emperical\")\nax[1].set(title=\"Cumulative Distribution Function\", xlabel=\"Random Variates\", ylabel=\"Cumulative Probability\")\nplt.show()","fd43f710":"# Fit a Normal Distribution\nfit_normal = sp.stats.norm.fit(sample_sorted)\n\n# Fit a Log-Normal Distribution\nfit_log_normal = sp.stats.lognorm.fit(sample_sorted)\n\n### Emperical CDF\nx_cdf_sample = sample_sorted\nn = x_cdf_sample.size\ny_cdf_sample = np.arange(1, n+1) \/ n\n\n### Theoretical Normal CDF\nx_norm_cdf = np.linspace(sample_sorted.min(), sample.max(), 100)\ny_norm_cdf = sp.stats.norm.cdf(x=x_norm_cdf, loc=fit_normal[0], scale=fit_normal[1])\ny_norm_cdf_discrete = sp.stats.norm.cdf(x=x_cdf_sample, loc=fit_normal[0], scale=fit_normal[1])\n\n### Theoretical Log-Normal CDF\nx_lognorm_cdf = np.linspace(sample_sorted.min(), sample.max(), 100)\ny_lognorm_cdf = sp.stats.lognorm.cdf(x=x_lognorm_cdf, s=fit_log_normal[0], loc=fit_log_normal[1], scale=fit_log_normal[2])\ny_lognorm_cdf_discrete = sp.stats.lognorm.cdf(x=x_cdf_sample, s=fit_log_normal[0], loc=fit_log_normal[1], scale=fit_log_normal[2])\n\n### KS Distance\nnormal_diff = abs(y_cdf_sample - y_norm_cdf_discrete).max()\nlognormal_diff = abs(y_cdf_sample - y_lognorm_cdf_discrete).max()\n\nprint(\"Normal Fit: loc={:,.2f}, scale={:,.2f}, KS-Distance={:,.2f}\".format(fit_normal[0], fit_normal[1], normal_diff))\nprint(\"Log-Normal Fit: shape={:,.2f}, loc={:,.2f}, scale={:,.2f}, KS-Distance={:,.2f}\".format(fit_log_normal[0], fit_log_normal[1], fit_log_normal[2], lognormal_diff))\n\n### Plot the sampled CDF against the Theoretical Normal and Log-Normal\nfig, ax = plt.subplots(figsize=(16,6), nrows=1, ncols=2)\n\nax[0].step(x_cdf_sample, y_cdf_sample, where='post', color=c1, label=\"Sample\")\nax[0].plot(x_norm_cdf, y_norm_cdf, color=c3, label=\"Fitted Normal Distribution\")\nax[0].set(title=\"Probability Density Function\", xlabel=\"Random Variates\", ylabel=\"Density\")\nax[0].legend()\n\nax[1].step(x_cdf_sample, y_cdf_sample, where='post', color=c1, label=\"Emperical\")\nax[1].plot(x_lognorm_cdf, y_lognorm_cdf, color=c5, label=\"Fitted Log-Normal Distribution\")\nax[1].set(title=\"Cumulative Distribution Function\", xlabel=\"Random Variates\", ylabel=\"Cumulative Probability\")\nax[1].legend()\nplt.show()","695bc7ee":"# 1. Evaluating a Distribution Sample <a class=\"anchor\" id=\"DistributionSampling\"><\/a>\n---\nThis section will explain the basics you will need to know in order to sample from a distribution in python and store the results as a NumPy array. The following points will be covered in this section:\n\n1. Syntax for sampling from a Normal distribution;\n2. Plotting the Probability Density Functions (PDF), Cumulative Density Functions (CDF), as well as the Probability-Probability plot (P-P Plot) and Quantile-Quantile plot (Q-Q Plot); and\n3. Important distribution fillting statisitcs;\n\n","1f9333ea":"### Probability-Probability (<a href=\"https:\/\/en.wikipedia.org\/wiki\/P%E2%80%93P_plot\">P-P<\/a>) Plot & Quantile-Quantile (<a href=\"https:\/\/en.wikipedia.org\/wiki\/Q%E2%80%93Q_plot\">Q-Q<\/a>) Plot\n\nThe P-P and Q-Q plots also show that the sample distribution is representative of the Theoretical Standard Normal Diustribution.\n","d6234bb0":"# Introduction <a class=\"anchor\" id=\"Introduction\"><\/a>\n---\nThe purpose of this notebook is to explore the applications of distribution analysis in quantitative finance. More specifically, the Value-at-Risk (VaR) which is one of the most important statistic that measures and quantifies the level of financial risk within a firm, portfolio or position. VaR often relies on dimulating distributions to understand the likelihood of possible profit\/loss events occuring at some point in the future. \n\nThis notebook will cover the basics of how to construct and simulate from a distribution in Python before applying this knowledge to an example data set to calculate the VaR for a hypothetical portfolio.\n","81683ccb":"### Aditional Statistics\n\nIn the event that the visualization techniques discussed above are not conclusive, Hypothesis Testing such as the Kolmogorov Smirnov test, Shapiro Wilk test and Anderson-Darling test may be able to provide more statistical confidence. We are just going to cover the KS Statistic in this section since it is a general test which can be easily visualised.\n\n**<a href=\"https:\/\/en.wikipedia.org\/wiki\/Kolmogorov%E2%80%93Smirnov_test\">Kolmogorov Smirnov test:<\/a>**\nThe Kolmogorov\u2013Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution. To make the KS statistic easier to visualise, we can plot the difference between the enperical and theoretical CDF as shown below.\n","7608b14e":"# Table of Content\n\n* [Introduction](#Introduction)\n* [Environment Setup](#EnvironmentSetup)\n* [**1. Evaluating a Distribution Sample**](#DistributionSampling)\n* [**2. Fitting Distributions**](#DistributionFitting)\n","8d7bd9d7":"If the observed data perfectly follow a normal distribution, the value of the KS statistic will be 0. The P-Value is used to decide whether the difference is large enough to reject the null hypothesis:\n\n- If the P-Value of the KS Test is larger than 0.05, we assume a normal distribution\n- If the P-Value of the KS Test is smaller than 0.05, we do not assume a normal distribution\n\nIn this case the KS Statistic is 0.0978 which is close to zero and the p-value is 0.7212 which is greater than 0.05. Therefore we would conclude that the sample comes from the Standard Normal Distribution with a high degree of confidence.","8c0bc776":"## Normal Distribution\n\nThe Probability Density Function (PDF) for a <a href=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.norm.html\">Normal distribution<\/a> is generally defined in terms of the mean $\\mu$ and variance $\\sigma^2$. In this case the PDF takes the form:\n\n$$ f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{1}{2}\\left( \\frac{x-\\mu}{\\sigma} \\right)^2} $$\n\nwhere $\\mu \\in \\mathbb{R}$ is the mean and $\\sigma^2>0$ is the variance. Within SciPy, the standardised notation for the PDF is used with the location equal to the mean (`loc` $=\\mu$) and scale equal to the standard deviation (`scale` $=\\sigma$). The SciPy syntax which will be used in this example includes:\n\n- **<a href=\"https:\/\/en.wikipedia.org\/wiki\/Independent_and_identically_distributed_random_variables\">Random Variate Sampling<\/a>:** `sp.stats.norm.rvs(loc=loc, scale=scale, size=size, random_state=random_state)`\n- **<a href=\"https:\/\/en.wikipedia.org\/wiki\/Probability_density_function\">Probability Density Function<\/a>:** `sp.stats.norm.pdf(x=x, loc=loc, scale=scale)`\n- **<a href=\"https:\/\/en.wikipedia.org\/wiki\/Cumulative_distribution_function\">Cumulative Distribution Fuction<\/a>:** `sp.stats.norm.cdf(x=x, loc=loc, scale=scale)`\n- **<a href=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.special.ndtri.html\">Area under a Gaussian Distribution Function<\/a>:** `sp.special.ndtri(percentile_area)`\n\n### Define the Fixed Parameters and Sample\nFor this example we are setting the mean of the distribution to zero (`loc = 0`) and the standard deviation to one (`scale = 1`), i.e. the Standard Normal Distribution. We sample 50 times from this distribution (`size = 50`) before we can plot our sample against the theoretical Standard Normal Distribution.","72f25644":"# Environment Setup <a class=\"anchor\" id=\"EnvironmentSetup\"><\/a>\n---\nThis notebook leverages a number of key Python packages including <a href=\"https:\/\/numpy.org\/\">Numpy<\/a>, <a href=\"https:\/\/www.scipy.org\/\">SciPy<\/a> and <a href=\"https:\/\/matplotlib.org\/\">Matplotlib<\/a>. A custom colour scheme has also been defined to ensure a consistent theme for all of the analysis. I personally use <a href=\"https:\/\/coolors.co\/\">Coolors<\/a> to find a suitable colour scheme.","58a1c20c":"Assume hyperthetically that the underlying distribution of the overall population in this example is unkown. Then we would need to assume a range of distributions and assess their fit against the sample. For the purpose of this example, we are going to compare the fit of a Normal and Log-Normal distribution to our sample.","66a39084":"From plotting the PDF, CDF and P-P\/Q-Q plots, we have strong evidence to suggest that the sample of 50 random variates in this example come from a Standard Normal Distribution. However for more complex distributions, these chart may not provide enough statistical evidence to draw such conclusions.","a3e508bd":"![My%20Banner.png](attachment:My%20Banner.png) \n\n<a href=\"https:\/\/linkedin.com\/in\/ljhealy1992\">Liam Healy<\/a> $\\cdot$ Feb 11, 2020 $\\cdot$ 30 min read\n\n---","9f640452":"### Probability Density Function (<a href=\"https:\/\/en.wikipedia.org\/wiki\/Probability_density_function\">PDF<\/a>) & Cumulative Probability Function (<a href=\"https:\/\/en.wikipedia.org\/wiki\/Cumulative_distribution_function\">CDF<\/a>)\n\nThe charts below compare our sampled variates (emperical distribution) against the theoretical Standard Normal Distribution. With only a sample size of 50, we can begin to see that the Emperical CDF has a very similar shape to the Theoretical CDF which suggest that our sample is representative of the theoretical 'full population'.","27695108":"### Conclusions\n\nWithin this section we have sampled from a Normal distribution and shown that our sample is representative of the overall population through a number of charts of statistics. This foundation will be leveraged when fitting and evaluating more complex distributions.","1746aac0":"From the CDF charts above it is clear that the fitted Log-Normal distribution represents the sample much more accurately than the Normal distribution. The KS Statistic also confirms this conclusion with a Log-Normal KS_Distance of 0.06 compared to 0.21 for the Normal distribution.","e8bde657":"# 2. Fitting Distributions <a class=\"anchor\" id=\"DistributionFitting\"><\/a>\n---\nThis section will briefly discuss a framework for fitting a distribution to a set of data. This data for example, could represent historical profit and losses of an investment fortfolio over time. The main objective of this section will be to fit a range of distributions to a samle of Log-Normal random raviates and perform the statistical test discussed in [section 1](#DistributionSampling).\n\nConsider a <a href=\"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.lognorm.html\">Log-Normal distribution<\/a> with the PDF:\n\n$$ f(x) = \\frac{1}{x\\sigma \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2} \\right) $$\n\nwhere $\\mu \\in \\mathbb{R}$ is the mean and $\\sigma^2>0$ is the variance. Within SciPy, the standardised notation for the PDF is used with the location parameter `loc` is the minimum value of the distribution, the shape parameter `s` is the standard deviation and the scale parameter `scale` is the exponential of the mean (i.e. $scale = \\exp(\\mu)$).\n\nWe begin by constructing a dummy data set that we know will follow a Log-Normal distribution with `shape=2`, `loc=0`, and `scale=5`. The sample of 100 random variates is plotted below as a PDF and CDF."}}