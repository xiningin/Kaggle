{"cell_type":{"6a426aa1":"code","0e0242e1":"code","c18f3f82":"code","1433c04c":"code","d85202ab":"code","9f34dce3":"code","1f52025b":"code","c8a66a5b":"code","d27961c0":"code","0ae94855":"code","9d50ee4e":"code","aba1154a":"code","e16c110f":"code","27d09a97":"markdown","62dd7dfc":"markdown","080b78e8":"markdown","da1d1eaf":"markdown","c73bc02f":"markdown","d1ce9307":"markdown","c5019f9d":"markdown","e60b548e":"markdown","364ff40e":"markdown","5f7787b6":"markdown","8663f81d":"markdown","21ac9e09":"markdown","030c1dcb":"markdown","d3deaf37":"markdown","f5fb6e15":"markdown","443f3786":"markdown"},"source":{"6a426aa1":"import pandas as pd\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import regexp_tokenize\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom nltk.stem import WordNetLemmatizer","0e0242e1":"text = \"Very orderly and methodical he looked, with a hand on each knee, and a loud watch ticking a sonorous sermon under his flapped newly bought waist-coat, as though it pitted its gravity and longevity against the levity and evanescence of the brisk fire.\"\n\nprint(\"Text : \", len(text))\nprint(\"------------------\")\nprint(text)","c18f3f82":"document = \"At nine o'clock I visited him myself. It looks like religious mania, and he'll soon think that he himself is God.\"\nprint(\"Text : \", len(document))\nprint(\"------------------\")\nprint(text)","1433c04c":"message = \"i recently watched this show called mindhunters:). i totally loved it \ud83d\ude0d. it was gr8 <3. #bingewatching #nothingtodo \ud83d\ude0e\"\nprint(\"Text : \", len(message))\nprint(\"------------------\")\nprint(message)","d85202ab":"tokens = word_tokenize(text.lower())\nprint(\"Tokens : \", len(tokens))\nprint(\"------------------\")\nprint(tokens)","9f34dce3":"words = word_tokenize(document)\nprint(\"Words : \", len(words))\nprint(\"------------------\")\nprint(words)","1f52025b":"sentences = sent_tokenize(document)\nprint(\"Sentences : \", len(sentences))\nprint(\"------------------\")\nprint(sentences)","c8a66a5b":"tweets = TweetTokenizer().tokenize(message)\nprint(\"Tweets : \", len(tweets))\nprint(\"------------------\")\nprint(tweets)","d27961c0":"pattern = \"#[\\w]+\"\n\nwords_regex = regexp_tokenize(message, pattern)\nprint(\"Words : \", len(words_regex))\nprint(\"------------------\")\nprint(words_regex)","0ae94855":"stemmer = PorterStemmer()\nporter_stemmed = [stemmer.stem(token) for token in tokens]\n\nprint(\"Tokens : \", len(porter_stemmed))\nprint(\"------------------\")\nprint(porter_stemmed)","9d50ee4e":"stemmer = SnowballStemmer(\"english\")\nsnowball_stemmed = [stemmer.stem(token) for token in tokens]\n\nprint(\"Tokens : \", len(snowball_stemmed))\nprint(\"------------------\")\nprint(snowball_stemmed)","aba1154a":"wordnet_lemmatizer = WordNetLemmatizer()\nlemmatized = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n\nprint(\"Tokens : \", len(lemmatized))\nprint(\"------------------\")\nprint(lemmatized)","e16c110f":"df = pd.DataFrame({\"tokens\": tokens, \"porter_stemmed\" : porter_stemmed, \"snowball_stemmed\" : snowball_stemmed, \"lemmatized\" : lemmatized})\ndf_new = df[(df.tokens != df.porter_stemmed) | (df.tokens != df.snowball_stemmed) | (df.tokens != lemmatized)]\n\nprint(\"Differences : \", len(df_new))\nprint(\"------------------\")\ndf_new","27d09a97":"#### <font color='#4a8bad'>Reading Sample Texts<\/font>\n<a id=\"reading\"><\/a>","62dd7dfc":"#### <font color='#4a8bad'>Snowball Stemmer<\/font>\n<a id=\"snowball\"><\/a>","080b78e8":"## <font color='#4a8bad'>Stemmer<\/font>\n***\n<a id=\"stemmer\"><\/a>","da1d1eaf":"## <font color='#4a8bad'>Tokenisation<\/font>\n***\n<a id=\"tokenisation\"><\/a>","c73bc02f":"#### <font color='#4a8bad'>Tweet Tokenisation<\/font>\n<a id=\"tweet\"><\/a>\n\nA problem with word tokeniser is that it fails to tokeniser emojis and other complex special characters such as word with hashtags. Emojis are common these days and people use them all the time.\nEmojis have their own significance in areas like sentiment analysis where a happy face and sad face can salone prove to be a really good predictor of the sentiment. Similarly, the hashtags are broken into two tokens. A hashtag is used for searching specific topics or photos in social media apps such as Instagram and facebook. So there, you want to use the hashtag as is.\n\nLet's use the tweet tokeniser of nltk to tokenise this message.","d1ce9307":"#### <font color='#4a8bad'>Word Tokenisation<\/font>\n<a id=\"word\"><\/a>\n\nNLTK's word tokeniser not only breaks on whitespaces but also breaks contraction words such as he'll into \"he\" and \"'ll\". On the other hand it doesn't break \"o'clock\" and treats it as a separate token.","c5019f9d":"## <font color='#4a8bad'>Conclusion<\/font>\n***\n<a id=\"conclusion\"><\/a>","e60b548e":"#### <font color='#4a8bad'>Wordnet Lemmatizer<\/font>\n<a id=\"wordnet\"><\/a>","364ff40e":"## <font color='#4a8bad'>Lemmatizer<\/font>\n***\n<a id=\"lemmatizer\"><\/a>","5f7787b6":"* [Prerequisite](#pre)\n    * [Importing Libraries](#importing)\n    * [Reading Sample Texts](#reading)\n    * [Extracting Tokens](#extracting)\n* [Tokenisation](#tokenisation)\n    * [Word Tokenisation](#word)\n    * [Sentence Tokenisation](#sentence)\n    * [Tweet Tokenisation](#tweet)\n    * [Custom Tokenisation (using Regex)](#regex)\n* [Stemmer](#stemmer) \n    * [Porter Stemmer](#porter)\n    * [Snowball Stemmer](#snowball)\n* [Lemmatizer](#lemmatizer)\n    * [Wordnet Lemmatizer](#wordnet)\n* [Conclusion](#conclusion)","8663f81d":"#### <font color='#4a8bad'>Extracting Tokens<\/font>\n<a id=\"extracting\"><\/a>","21ac9e09":"## <font color='#4a8bad'>Prerequisite<\/font>\n***\n<a id=\"pre\"><\/a>","030c1dcb":"#### <font color='#4a8bad'>Importing Libraries<\/font>\n<a id=\"importing\"><\/a>","d3deaf37":"#### <font color='#4a8bad'>Custom Tokenisation (using Regex)<\/font>\n<a id=\"regex\"><\/a>\n\nNow, there is a tokeniser that takes a regular expression and tokenises and returns result based on the pattern of regular expression.\n\nLet's look at how you can use regular expression tokeniser.","f5fb6e15":"#### <font color='#4a8bad'>Porter Stemmer<\/font>\n<a id=\"porter\"><\/a>","443f3786":"#### <font color='#4a8bad'>Sentence Tokenisation<\/font>\n<a id=\"sentence\"><\/a>\n\nTokenising based on sentence requires you to split on the period ('.'). Let's use nltk sentence tokeniser."}}