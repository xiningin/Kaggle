{"cell_type":{"3883aa5a":"code","1c984f59":"code","157bc412":"code","adecde71":"code","ff737ade":"code","6f3e60e8":"code","5e6c50ba":"code","68890b1d":"code","c8c749d8":"code","10433ca2":"code","5c3a9399":"code","06c076cd":"code","22e39ed1":"code","67fa9670":"code","397e2c50":"code","3fa95ef0":"code","a6a3971b":"code","0002fa4d":"code","f000b8ac":"code","5a6198d6":"code","e10fc85d":"code","1e602390":"code","d2a79a9c":"code","e3dc338a":"code","26f32af2":"code","48ece29e":"code","6b6a9ed5":"code","d6bb16c8":"code","6f977b99":"code","ab9007f9":"code","eb6cd332":"code","d8a9521d":"code","29cdc797":"code","7d0c414f":"code","ff049bc9":"code","921d0ce6":"code","4ed596a3":"code","8dac14ce":"code","b1e7ae2b":"code","ce547b1b":"code","cb246c93":"code","eea43e5f":"code","18eca890":"code","190ead39":"code","191c43dc":"code","a229106a":"code","eec471ca":"code","db3d9389":"code","4c043232":"code","44caddde":"code","b59c91b9":"markdown","35452cc2":"markdown","f48c35f2":"markdown","cfe05ebb":"markdown","5c0b1084":"markdown","2d4af3ed":"markdown","d8bae5f9":"markdown","d2a7cea2":"markdown","e4c597af":"markdown","3a2c197c":"markdown","7f36ff88":"markdown","40e429bb":"markdown","9f1fe257":"markdown","c3337cca":"markdown","3ad58f4a":"markdown","f664d891":"markdown","464f82c7":"markdown","3feaf9eb":"markdown","8cb87e51":"markdown","f990e218":"markdown","636c2c7c":"markdown"},"source":{"3883aa5a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport time\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\nfrom sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import LeaveOneOut as loocv\n","1c984f59":"from plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.offline as py\nfrom plotly.graph_objs import Scatter, Layout\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff","157bc412":"%time\ntrain = pd.read_csv('..\/input\/training.csv')\ntest = pd.read_csv('..\/input\/testing.csv')\nprint(\"Rows and Columns(Train): \",train.shape)\nprint(\"Rows and Columns(Test) : \",test.shape)","adecde71":"train.info()","ff737ade":"train.head()","6f3e60e8":"# we have no missing values\ntrain.isnull().any().any()","5e6c50ba":"#'duplicated()' function in pandas return the duplicate row as True and othter as False\n#for counting the duplicate elements we sum all the rows\nsum(train.duplicated())","68890b1d":"p = train.describe().T\np = p.round(4)\ntable = go.Table(\n    columnwidth=[0.8]+[0.5]*8,\n    header=dict(\n        values=['Attribute'] + list(p.columns),\n        line = dict(color='#506784'),\n        fill = dict(color='lightblue'),\n    ),\n    cells=dict(\n        values=[p.index] + [p[k].tolist() for k in p.columns[:]],\n        line = dict(color='#506784'),\n        fill = dict(color=['rgb(173, 216, 220)', '#f5f5fa'])\n    )\n)\npy.iplot([table], filename='table-of-mining-data')","c8c749d8":"print(train['class'].value_counts())\n\nf,ax=plt.subplots(1,2,figsize=(20,8))\ntrain['class'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0])\nax[0].set_title('Distribution of Different Classes (Pie Chart)')\nax[0].set_ylabel('')\nsns.countplot('class',data=train,ax=ax[1])\nax[1].set_title('Distribution of Different Classes (Bar Plot)')\nplt.show()","10433ca2":"from collections import Counter\n\ndef detect_outliers(train_data,n,features):\n    outlier_indices = []\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(train_data[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(train_data[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        outlier_step = 1.5 * IQR\n        outlier_list_col = train_data[(train_data[col] < Q1 - outlier_step) | (train_data[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers\nlist_atributes = train.drop('class',axis=1).columns\nOutliers_to_drop = detect_outliers(train,2,list_atributes)","5c3a9399":"train.loc[Outliers_to_drop]","06c076cd":"group_map = {\"grass \":0,\"building \":1,'concrete ':2,'tree ':3,'shadow ':4,'pool ':5,'asphalt ':6,'soil ':7,'car ':8}\n\ntrain['class'] = train['class'].map(group_map)\ntest['class'] = test['class'].map(group_map)\ntrain['class'].unique()","22e39ed1":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[1:148]\nplt.title(\"Distribution of Mean Values Per Row in the Train and Test Set\",fontsize=15)\nsns.distplot(train[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","67fa9670":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of Mean Values Per Column in the Train and Test Set\",fontsize=15)\nsns.distplot(train[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","397e2c50":"sns.pairplot(train, vars=['class', 'BrdIndx','Area','Round','Bright','Compact'], hue='class')\nplt.show()","3fa95ef0":"correlations = train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head()","a6a3971b":"correlations = correlations.loc[correlations[0] == 1]\nremovable_features = set(list(correlations['level_1']))\ncorrelations.shape\n","0002fa4d":"test.head()","f000b8ac":"X_train = train.drop(['class'], axis=1)\n#X_train = X_train.drop(removable_features, axis=1)\ny_train = pd.DataFrame(train['class'].values)\nX_test = test.drop(['class'], axis=1)\n#X_test = X_test.drop(removable_features, axis=1)\ny_test = test['class']\n\nscaler = StandardScaler()\nX_train_std = scaler.fit_transform(X_train)\nX_test_std = scaler.transform(X_test)","5a6198d6":"clfs = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),AdaBoostClassifier(),GaussianNB(),XGBClassifier()]\ntotal_accuracy = {}\ntotal_accuracy_std = {}\nfor model in clfs:\n    model.fit(X_train, y_train)\n    pred = model.predict(X_test)\n    total_accuracy[str((str(model).split('(')[0]))] = accuracy_score(pred,y_test)\n    \nfor model in clfs:\n    model.fit(X_train_std, y_train)\n    pred = model.predict(X_test_std)\n    total_accuracy_std[str((str(model).split('(')[0]))] = accuracy_score(pred,y_test)","e10fc85d":"data = total_accuracy.values()\nlabels = total_accuracy.keys()\ndata1 = total_accuracy_std.values()\nlabels1 = total_accuracy_std.keys()\n\nfig = plt.figure(figsize=(20,5))\nplt.subplot(121)\nplt.plot([i for i, e in enumerate(data)], data); plt.xticks([i for i, e in enumerate(labels)], [l[:] for l in labels])\nplt.title(\"Accuracy Score Without Preprocessing\",fontsize = 14)\nplt.xlabel('Model',fontsize = 13)\nplt.xticks(rotation = 10)\nplt.ylabel('Accuracy',fontsize = 13)\n\nplt.subplot(122)\nplt.plot([i for i, e in enumerate(data1)], data1); plt.xticks([i for i, e in enumerate(labels1)], [l[:] for l in labels1])\nplt.title(\"Accuracy Score After Preprocessing\",fontsize = 14)\nplt.xlabel('Model',fontsize = 13)\nplt.xticks(rotation = 10)\nplt.show()","1e602390":"n_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=True, random_state=42)","d2a79a9c":"prediction = np.zeros(len(X_test))\ntotal_acc = []\noof = np.zeros(len(X_train))\nfor fold_n, (train_index, valid_index) in enumerate(folds.split(X_train,y_train)):\n    print('Fold', fold_n, 'started at', time.ctime(),end = \"  \")\n    X_train_, X_valid = X_train.iloc[train_index], X_train.iloc[valid_index]\n    y_train_, y_valid = y_train.iloc[train_index], y_train.iloc[valid_index]\n        \n    clf_rfc2 = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=0)\n    clf_rfc2.fit(X_train_, y_train_)\n    oof[valid_index] = clf_rfc2.predict(X_train.iloc[valid_index])\n            \n    prediction = clf_rfc2.predict(X_test)\n    print(\"Validation Score: \",accuracy_score(y_test,prediction))\n    total_acc.append(accuracy_score(y_test,prediction))\nprint(\"CV score: {:<8.5f}\".format(accuracy_score(y_train, oof)))\nprint(\"Mean Testing Score: \",np.mean(total_acc))","e3dc338a":"clf = RandomForestClassifier(class_weight = 'balanced', random_state=0)\nparameters = {'n_estimators':[1000], \n              'max_depth':[4, 5, 6],\n              'criterion':['gini', 'entropy'], \n              'max_leaf_nodes':[5,11],\n              'min_samples_leaf':[2,3]\n              #'max_features': ['auto', 'sqrt', 'log2']\n             }\n\ngrid_obj = GridSearchCV(clf, parameters, scoring='accuracy', verbose=1, cv=10)\n\ngrid_fit = grid_obj.fit(X_train, y_train)\nbest_clf = grid_fit.best_estimator_\nprint(best_clf)","26f32af2":"best_clf.fit(X_train, y_train)\ny_pred = best_clf.predict(X_test)\naccuracy_score(y_pred,y_test)","48ece29e":"selector = RFE(best_clf, 30, step=1)\nselector.fit(X_train,y_train)","6b6a9ed5":"y_pred = selector.predict(X_test)\naccuracy_score(y_pred,y_test)","d6bb16c8":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(best_clf, random_state=0).fit(X_train, y_train)\n\neli5.show_weights(perm, top=50,feature_names = X_train.columns.tolist())","6f977b99":"clf = DecisionTreeClassifier(class_weight = 'balanced', random_state=0)\nparameters = {'max_depth':[ 1, 3, 5, 7, 9, 11 ,13,15,17,19],\n              'criterion':['gini', 'entropy'], \n              #'max_leaf_nodes':[2,3,5,7,11,9,13],\n              #'min_samples_leaf':[2,3,5,7,11,9,13]\n              'min_samples_split':[2],\n              'max_leaf_nodes':[11],\n              'min_samples_leaf':[2],\n              #'max_features': ['auto', 'sqrt', 'log2']\n             }\n\ngrid_obj = GridSearchCV(clf, parameters, scoring='accuracy', verbose=1, cv=10)\n\ngrid_fit = grid_obj.fit(X_train, y_train)\nbest_clf = grid_fit.best_estimator_\nprint(best_clf)","ab9007f9":"best_clf.fit(X_train, y_train)\ny_pred = best_clf.predict(X_test)\naccuracy_score(y_pred,y_test)","eb6cd332":"mod = xgb.XGBClassifier(learning_rate=0.02,booster = \"gbtree\",  objective= \"multi:softmax\")\n\nparams = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5,7,9]\n        }\n\nfolds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nr_search = RandomizedSearchCV(mod, params, scoring='accuracy', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3)\nr_search.fit(X_train, y_train)\nprint('\\n Best hyperparameters:')\nprint(r_search.best_params_)","d8a9521d":"mod = xgb.XGBClassifier(learning_rate=0.02, n_estimators=350,booster = \"gbtree\",subsample=0.6,objective=\"multi:softmax\",max_depth=3,\n                       gamma=0.5,colsample_bytree=0.6,min_child_weight=1,eval_metric='merror')","29cdc797":"mod.fit(X_train,y_train)","7d0c414f":"y_pred = mod.predict(X_test)\naccuracy_score(y_pred,y_test)","ff049bc9":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\ntrain_scaled = scaler.fit_transform(X_train)         \nPCA_train_x = PCA().fit_transform(train_scaled)\nplt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=train['class'], cmap=\"copper_r\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","921d0ce6":"train_scaled = scaler.fit_transform(X_train)         \nPCA_train_x = PCA(4).fit_transform(train_scaled)","4ed596a3":"from sklearn.decomposition import KernelPCA\n\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\n\nplt.figure(figsize=(15, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n       \n    PCA_train_x = PCA(2).fit_transform(train_scaled)\n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=train['class'], cmap=\"nipy_spectral_r\")\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()","8dac14ce":"X_train = train.drop('class',axis=  1)\ny_train = train['class']","b1e7ae2b":"from sklearn.datasets import load_boston\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrfc = RandomForestClassifier(n_estimators=500, class_weight='balanced', max_depth=5, random_state=42)\nselector = RFE(rfc, n_features_to_select=50)\nselector.fit(X_train, y_train)","ce547b1b":"selector.get_support()","cb246c93":"selected = X_train.columns[selector.get_support()]","eea43e5f":"clfs = [KNeighborsClassifier(),DecisionTreeClassifier(),RandomForestClassifier(),AdaBoostClassifier(),GaussianNB(),XGBClassifier()]\nfor model in clfs:\n    model.fit(train[selected], y_train)\n    pred = model.predict(test[selected])\n    print(accuracy_score(pred,y_test))","18eca890":"import shap\nfrom eli5.sklearn import PermutationImportance\nimport eli5","190ead39":"from sklearn.svm import SVC","191c43dc":"rfc = SVC(kernel='linear')\nrfc.fit(X_train,y_train)\npred = rfc.predict(test.drop('class',axis=1))\nprint(accuracy_score(pred,test['class']))","a229106a":"perm = PermutationImportance(rfc, random_state=1).fit(X_train, y_train)\neli5.show_weights(perm, top=50)","eec471ca":"explainer = shap.LinearExplainer(rfc, X_train)\nshap_values = explainer.shap_values(X_train)\n\nshap.summary_plot(shap_values, X_train)","db3d9389":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA","4c043232":"lda = LDA(n_components=1)  \nX_train = lda.fit_transform(X_train_std, y_train)  \nX_test = lda.transform(X_test_std) ","44caddde":"classifier = RandomForestClassifier(max_depth=5, random_state=0)\n\nclassifier.fit(X_train_std, y_train)  \ny_pred = classifier.predict(X_test_std) \naccuracy_score(y_test, y_pred)","b59c91b9":"> **Since PCA hasn't been useful, I decided to proceed with the existing dataset**","35452cc2":"### Feature Selection","f48c35f2":"> #### XGBoost","cfe05ebb":"<a id='toc'><\/a>\n# Table of Content\n\n* [Introduction](#intro)\n* [Importing packages and loading data](#pac)\n* [Data cleaning and data wrangling](#clean)\n* [Exploratory Data Analysis (EDA)](#eda)\n    * [Basic Statistical Details](#bsd)\n    * [Violin plot of features by diagnosis](#vpf)\n    * [Kde Plot For Each Mean Feature](#kde)\n    * [Relationship Between Features](#rbf)\n        * [Correlation Heatmap](#chm)\n        * [Feature Pair](#fp)\n        * [Positively Correlated Features](#pcf)\n        * [Un-Correlated Features](#ucf)\n        * [Negatively Correlated Features](#ncf)\n* [Statistical Analysis(Outliers)](#sa)\n    * [Box Plot](#bp)\n    * [Remove Outliers Using IQR](#iqr)\n* [Principal Component Analysis(PCA)](#pca)\n* [Machine Learning Classification](#ml)\n    * [Building Feature Set](#sp)\n    * [Support Vector Machine (SVM)](#svm)\n        * [Kernel Selection Using Learning Curve ](#ks)\n        * [Selection of Regularization parameter(C)](#srp)\n        * [Confusion Metrix and ROC Curve](#cmrc)\n* [Summary of models performance](#sum)","5c0b1084":"### eli5 provides a way to compute feature importances for any black-box estimator by measuring how score decreases when a feature is not available; the method is also known as \u201cpermutation importance\u201d or \u201cMean Decrease Accuracy (MDA)\u201d.","2d4af3ed":"#### Walkthrough of SKlearn Classification Algs (Not worrying about overfitting yet)","d8bae5f9":"### Hyperparameter Tunning","d2a7cea2":"<a id='intro'><\/a>\n## Introduction<br>\n> ### Dataset Information\nContains training and testing data for classifying a high resolution aerial image into 9 types of urban land cover. Multi-scale spectral, size, shape, and texture information are used for classification. There are a low number of training samples for each class (14-30) and a high number of classification variables (148), so it may be an interesting data set for testing feature selection methods. The testing data set is from a random sampling of the image. <br>\n> Class is the target classification variable. The land cover classes are: trees, grass, soil, concrete, asphalt, buildings, cars, pools, shadows.\n<br>\n> ### Attributes Deatil\n- Class    : Land cover class (nominal) \n- BrdIndx  : Border Index (shape variable) \n- Area     : Area in m2 (size variable) \n- Round    : Roundness (shape variable) \n- Bright   : Brightness (spectral variable) \n- Compact  : Compactness (shape variable) \n- ShpIndx  : Shape Index (shape variable) \n- Mean_G   : Green (spectral variable) \n- Mean_R   : Red (spectral variable) \n- Mean_NIR : Near Infrared (spectral variable) \n- SD_G     : Standard deviation of Green (texture variable) \n- SD_R     : Standard deviation of Red (texture variable) \n- SD_NIR   : Standard deviation of Near Infrared (texture variable) \n- LW       : Length\/Width (shape variable) \n- GLCM1    : Gray-Level Co-occurrence Matrix (texture variable) \n- Rect     : Rectangularity (shape variable) \n- GLCM2    : Another Gray-Level Co-occurrence Matrix attribute (texture variable) \n- Dens     : Density (shape variable) \n- Assym    : Assymetry (shape variable) \n- NDVI     : Normalized Difference Vegetation Index (spectral variable) \n- BordLngth: Border Length (shape variable) \n- GLCM3    : Another Gray-Level Co-occurrence Matrix attribute (texture variable) \n\n> **Note: These variables repeat for each coarser scale (i.e. variable_40, variable_60, ...variable_140). **","e4c597af":"> #### RandomForestClassifier[](http:\/\/)","3a2c197c":"> #### RandomForestClassifier","7f36ff88":"<a id='clean'><\/a>\n## Data Cleaning and Data Wrangling","40e429bb":"## Featurs Correlation","9f1fe257":"<a id='eda'><\/a>\n## Exploratory Data Analysis (EDA)","c3337cca":"<div style=\"background: linear-gradient(to bottom, #77009f, #9f0079); border: 2px; box-radius: 20px\"><br><h1 style=\"color: #220104; text-align: center\"> Urban Land Cover<br><\/h1><\/div>","3ad58f4a":"> #### DecisionTreeClassifier","f664d891":"### Cross Validation","464f82c7":"These are the values that we already observed earlier that are mostly centered around 0. We can see that train is actually showing a larger spread of these values, while test values have a smaller deviation and a distribution closer to a normal one.\n\nLet's show the distribution of standard deviation of values per row for train and test datasets.","3feaf9eb":"<a id='bsd'><\/a>\n> ### Basic Statistical Details","8cb87e51":"<a id='data'><\/a>\n### Load the Data","f990e218":"### Outliers Detection","636c2c7c":"<a id='pac'><\/a>\n## Importing Packages and Loading Data"}}