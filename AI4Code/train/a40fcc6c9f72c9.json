{"cell_type":{"3c6ff747":"code","1f1f732b":"code","075d0fb6":"code","685eab9a":"code","65003e3d":"code","e761c84d":"code","8d40b897":"code","3e7d9f8e":"code","f9a763aa":"code","f1b11ce1":"code","2c0cbc13":"code","0b5985db":"code","5f91faaf":"code","3d0364a3":"code","a7917306":"code","abe0d772":"code","1f60adf5":"code","a399d949":"code","599f884c":"code","8d77e5d7":"code","297d21dc":"code","2b28818b":"code","dd6ed096":"code","e05b7c33":"code","d4de03ae":"code","27ed5985":"code","5ac91384":"code","273000ee":"code","0e30bd45":"code","efa94c2c":"code","554a140b":"code","3c6c5b90":"code","1747af54":"code","6b6b60e7":"code","a3c8c6eb":"code","ad72e143":"code","4a93c3c5":"code","92caa419":"code","353a299f":"code","d4cc58f0":"code","881d3e75":"code","ccc0bf63":"code","badfb8ac":"code","cf0905a7":"code","fd89e234":"code","ffbd55c0":"code","93714801":"code","28d1b717":"code","7d1b103d":"code","a4c271e4":"code","5fdbf0f9":"code","48b87ca4":"code","3579a018":"code","788be0d2":"code","9d752add":"code","7f8d9c22":"code","78f63335":"code","0c1f7838":"code","38e85139":"code","1b488a40":"code","7caf09ed":"code","22760d1f":"code","d304e466":"code","ded60371":"code","6567d930":"code","a1ca9737":"code","c96e490b":"code","f69a166a":"code","7fb1a972":"code","b003453d":"code","d8f4b1e9":"code","3a059f1e":"code","09c1401b":"code","cb6ca175":"code","3040af59":"code","23a3dfa9":"code","e15a1a5d":"code","65bf7bfe":"code","d560cf11":"code","9a50845c":"code","bad88a2a":"code","1e557b4c":"code","87a85c3e":"code","483e6d5d":"code","53292970":"code","c510da98":"code","aad652fc":"code","54669d27":"code","469c1938":"code","449fc3cf":"code","e0620d89":"code","17d1954e":"code","55c2acf2":"code","3fbc928c":"code","0b84da90":"code","746ba605":"code","71a63c60":"code","d5e3190a":"code","f5ce5fac":"code","9f47e325":"code","410f4163":"code","432d5618":"code","b223f2f5":"code","45793bdb":"code","d9a37d11":"code","9a45f14d":"code","a361a91c":"code","1dce44ea":"code","db125101":"code","3fe8b867":"code","e588817f":"code","bc52374d":"code","0715376e":"code","c4b8bdf2":"code","252d46ff":"code","ffc9f6cb":"code","bc4f29b8":"code","4c37393e":"code","4822d1f0":"code","6131876c":"code","cf9f3925":"code","e45694ca":"code","57900f95":"code","441c4008":"code","083313e9":"code","b1ce69dc":"code","379e69cf":"code","35e5a2a5":"code","4a740c06":"code","6cc899fc":"code","c9f4526a":"code","cc02554d":"code","3f0ba77a":"markdown","75061323":"markdown","35b7e9b3":"markdown","c4e0ed47":"markdown","41e6e19a":"markdown","fc394854":"markdown","a5cec00e":"markdown","e88a4690":"markdown","b1590c09":"markdown","4ad1a74e":"markdown","2f587315":"markdown","63faea49":"markdown","89786c1a":"markdown","21f7b464":"markdown","2e9eb0de":"markdown","5003c5a1":"markdown","48e89487":"markdown","3e4eba82":"markdown","0a98d718":"markdown","3ecd43ec":"markdown","069aabe6":"markdown","8e448fcc":"markdown","b27af6db":"markdown","3fcdf42e":"markdown","470d3d53":"markdown","74973c93":"markdown","d8a9257a":"markdown","9db34449":"markdown","df519474":"markdown","4e53e4a1":"markdown","cdfb821d":"markdown","f6ba245e":"markdown","7b668350":"markdown","d9b0c948":"markdown","8590df3b":"markdown","a498c21a":"markdown","dd7b4c86":"markdown","49490142":"markdown","85326184":"markdown","06a56f4a":"markdown","dcf50c5d":"markdown","3c8bbc6b":"markdown","9e4453cb":"markdown","cb833e08":"markdown","c55de2f1":"markdown","723da58c":"markdown","f30dabc4":"markdown","380336d7":"markdown","22aca6c6":"markdown","16c13af3":"markdown","19f290e5":"markdown","e633124a":"markdown","e2dd767e":"markdown","e33d0548":"markdown","268ba7d2":"markdown","ba8c688f":"markdown","89da3531":"markdown","1ff1e87a":"markdown","b0c5e1c8":"markdown","5bd281a3":"markdown","f4f9f128":"markdown","fbebf8ef":"markdown","827d7bcd":"markdown","c9a0e740":"markdown","7ece1a02":"markdown","5c3f536f":"markdown","8702081b":"markdown","89d636f6":"markdown","ad00fbee":"markdown","0d721d0c":"markdown","41c024fa":"markdown","ef6855f9":"markdown","b1d645e0":"markdown","ab9a0aa7":"markdown","24edbd58":"markdown","760b13fe":"markdown","16546458":"markdown","ecae6c1a":"markdown","deafe3e9":"markdown","7677fe19":"markdown","a63b96a4":"markdown","e5a727a6":"markdown","aa6e07d3":"markdown","b6d6d2f4":"markdown","26f6a0f4":"markdown","ac8e38cf":"markdown","30de4d41":"markdown","26d00265":"markdown","3e8a21ce":"markdown","1528e454":"markdown","50a95faf":"markdown","ed07c7fe":"markdown","564337cd":"markdown","19f3c1f4":"markdown","7b844d56":"markdown","b31a5dcb":"markdown","e074d7e2":"markdown","62f62a00":"markdown","191553a4":"markdown","39fd83a9":"markdown","00877627":"markdown","c6bbb514":"markdown","500ed8c0":"markdown","715c68a0":"markdown","2bd19446":"markdown","aee6ead7":"markdown","8cc18801":"markdown","f1c62155":"markdown","038df379":"markdown","5b797b75":"markdown","1ae11122":"markdown","12e56dda":"markdown","c1d19db1":"markdown","0e4f79c6":"markdown","1a9dab3b":"markdown","a053a5d8":"markdown"},"source":{"3c6ff747":"import os, random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('max_columns', 200)","1f1f732b":"from sklearn.covariance import EllipticEnvelope\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM\nfrom sklearn import preprocessing","075d0fb6":"# !pip install plotly\n# !pip install fbprophet","685eab9a":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\nimport datetime as dt\nfrom statsmodels.tsa.arima_model import ARIMA\n\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly\n\nimport plotly.offline as py\nfrom matplotlib import pyplot\npy.init_notebook_mode()\n","65003e3d":"%%HTML\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color: #00b899; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n\ndiv.h2 {\n    background-color: #00b899; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 25px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\ndiv.h3 {\n    color: #00b899;\n    font-size: 16px; \n    margin-top: 20px; \n    margin-bottom:4px;\n}\ndiv.h4 {\n    font-size: 15px; \n    margin-top: 20px; \n    margin-bottom: 8px;\n}\nspan.note {\n    font-size: 5; \n    color: gray; \n    font-style: italic;\n}\nspan.captiona {\n    font-size: 5; \n    color: dimgray; \n    font-style: italic;\n    margin-left: 130px;\n    vertical-align: top;\n}\nhr {\n    display: block; \n    color: gray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n}\nhr.light {\n    display: block; \n    color: lightgray\n    height: 1px; \n    border: 0; \n    border-top: 1px solid;\n}\n\n<\/style>","e761c84d":"import pandas as pd\nno2_weather = pd.read_csv(\"..\/input\/s5p-data-csv\/no2_weather.csv\")\ns5p_no2_pictures_df = pd.read_csv(\"..\/input\/s5p-data-csv\/s5p_no2_pictures_df.csv\")\nweather_pictures_df = pd.read_csv(\"..\/input\/s5p-data-csv\/weather_pictures_df.csv\")","8d40b897":"import rasterio as rio\ndef split_column_into_new_columns(dataframe,column_to_split,new_column_one,begin_column_one,end_column_one):\n    for i in range(0, len(dataframe)):\n        dataframe.loc[i, new_column_one] = dataframe.loc[i, column_to_split][begin_column_one:end_column_one]\n    return dataframe","3e7d9f8e":"power_plants = pd.read_csv('\/kaggle\/input\/ds4g-environmental-insights-explorer\/eie_data\/gppd\/gppd_120_pr.csv')\npower_plants = split_column_into_new_columns(power_plants,'.geo','latitude',50,66)\npower_plants = split_column_into_new_columns(power_plants,'.geo','longitude',31,48)\npower_plants['latitude'] = power_plants['latitude'].astype(float)\na = np.array(power_plants['latitude'].values.tolist()) # 18 instead of 8\npower_plants['latitude'] = np.where(a < 10, a+10, a).tolist() \n\npower_plants_df = power_plants.sort_values('capacity_mw',ascending=False).reset_index()\npower_plants_df['img_idx_lt']=(((18.6-power_plants_df.latitude)*148\/(18.6-17.9))).astype(int)\npower_plants_df['img_idx_lg']=((67.3+power_plants_df.longitude.astype(float))*475\/(67.3-65.2)).astype(int)\npower_plants_df['plant']=power_plants_df.name.str[:3]+power_plants_df.name.str[-1]+'_'+power_plants_df.primary_fuel\npower_plants=power_plants_df[['name','latitude','longitude','primary_fuel','capacity_mw','img_idx_lt','img_idx_lg','plant']]\npower_plants","f9a763aa":"no2_weather.shape","f1b11ce1":"no2_weather.head()","2c0cbc13":"s5p_no2_pictures_df.shape","0b5985db":"s5p_no2_pictures_df.head()","5f91faaf":"weather_pictures_df.shape","3d0364a3":"weather_pictures_df.head()","a7917306":"x = weather_pictures_df['date']\ny = weather_pictures_df[\"total_precipitation_surface_mean\"]\nplt.plot(x,y)\nplt.show()","abe0d772":"def parser(x):\n    return dt.datetime.strptime(x, \"%Y-%m-%d\")\n\npath= '..\/input\/s5p-data-csv\/no2_weather.csv' \ndata = pd.read_csv(path, header=0, parse_dates=[0], squeeze=True, date_parser=parser)\ndata = data[['start_date','no2_emission_sum']]\ndata[\"start_date\"] = data[\"start_date\"].dt.strftime('%Y%m%d').astype(float)","1f60adf5":"data.info()","a399d949":"data.dropna(axis=0, inplace=True)\nprint(data.shape)","599f884c":"data = data.set_index('start_date')\ndata.head()","8d77e5d7":"weather_pictures_df.head(2)","297d21dc":"weather_pictures_df.shape","2b28818b":"no2_weather['start_date'] = pd.to_datetime(no2_weather['start_date'])\nno2_weather['no2_emission_sum'] = (no2_weather['no2_emission_sum'] - 32) * 5\/9\n# plot the data\nno2_weather.plot(x='start_date', y='no2_emission_sum')","dd6ed096":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\nHTML('<iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/8DfXJUDjx64\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","e05b7c33":"# An estimation of anomly population of the dataset (necessary for several algorithm)\noutliers_fraction = 0.01\n\nno2_weather['day'] = no2_weather['start_date'].dt.day\n# the day of the week (Monday=0, Sunday=6) and if it's a week end day or week day.\nno2_weather['DayOfTheWeek'] = no2_weather['start_date'].dt.dayofweek","d4de03ae":"# creation of 6 distinct categories that seem useful (week end\/day)\nno2_weather['catDayEmission'] = no2_weather['DayOfTheWeek']\n\na = no2_weather.loc[no2_weather['catDayEmission'] == 0, 'no2_emission_sum']\nb = no2_weather.loc[no2_weather['catDayEmission'] == 1, 'no2_emission_sum']\nc = no2_weather.loc[no2_weather['catDayEmission'] == 2, 'no2_emission_sum']\nd = no2_weather.loc[no2_weather['catDayEmission'] == 3, 'no2_emission_sum']\ne = no2_weather.loc[no2_weather['catDayEmission'] == 4, 'no2_emission_sum']\nf = no2_weather.loc[no2_weather['catDayEmission'] == 5, 'no2_emission_sum']\ng = no2_weather.loc[no2_weather['catDayEmission'] == 6, 'no2_emission_sum']\n\n","27ed5985":"# creation of 6 distinct categories that seem useful (week end\/day)\nno2_weather['catDayEPrecSurfaceMean'] = no2_weather['DayOfTheWeek']\n\na2 = no2_weather.loc[no2_weather['catDayEPrecSurfaceMean'] == 0, 'total_precipitation_surface_mean']\nb2 = no2_weather.loc[no2_weather['catDayEPrecSurfaceMean'] == 1, 'total_precipitation_surface_mean']\nc2 = no2_weather.loc[no2_weather['catDayEPrecSurfaceMean'] == 2, 'total_precipitation_surface_mean']\nd2 = no2_weather.loc[no2_weather['catDayEPrecSurfaceMean'] == 3, 'total_precipitation_surface_mean']\ne2 = no2_weather.loc[no2_weather['catDayEPrecSurfaceMean'] == 4, 'total_precipitation_surface_mean']\nf2 = no2_weather.loc[no2_weather['catDayEPrecSurfaceMean'] == 5, 'total_precipitation_surface_mean']\ng2 = no2_weather.loc[no2_weather['catDayEPrecSurfaceMean'] == 6, 'total_precipitation_surface_mean']","5ac91384":"no2_weather['time_epoch'] = (no2_weather['start_date'].astype(np.int64)\/100000000000).astype(np.int64)","273000ee":"# Take useful feature and standardize them\ndata_IF = no2_weather[['time_epoch','DayOfTheWeek','day','no2_emission_sum','temperature_2m_above_ground_mean','specific_humidity_2m_above_ground_mean','relative_humidity_2m_above_ground_mean','u_component_of_wind_10m_above_ground_mean','v_component_of_wind_10m_above_ground_mean','total_precipitation_surface_mean']]\n\ndata_IF.dropna(axis=0, inplace=True)\nprint(data_IF.shape)\n\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data_IF)\ndata_IF = pd.DataFrame(np_scaled)","0e30bd45":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\nHTML('<iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/mh6rAYA0e7Q\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","efa94c2c":"# qq = no2_weather.loc[no2_weather['catDayEmission'] == 0, 'no2_emission_sum']\n# qq \nno2_weather['catDayEmission'].value_counts()","554a140b":"df_class0 = no2_weather.loc[no2_weather['catDayEmission'] == 0, 'no2_emission_sum']\ndf_class1 = no2_weather.loc[no2_weather['catDayEmission'] == 1, 'no2_emission_sum']\ndf_class2 = no2_weather.loc[no2_weather['catDayEmission'] == 2, 'no2_emission_sum']\ndf_class3 = no2_weather.loc[no2_weather['catDayEmission'] == 3, 'no2_emission_sum']\ndf_class4 = no2_weather.loc[no2_weather['catDayEmission'] == 4, 'no2_emission_sum']\ndf_class5 = no2_weather.loc[no2_weather['catDayEmission'] == 5, 'no2_emission_sum']\ndf_class6 = no2_weather.loc[no2_weather['catDayEmission'] == 6, 'no2_emission_sum']\n","3c6c5b90":"fig, axs = plt.subplots(4,2)\ndf_class0.hist(ax=axs[0,0],bins=28)\ndf_class1.hist(ax=axs[0,1],bins=28)\ndf_class2.hist(ax=axs[1,0],bins=28)\ndf_class3.hist(ax=axs[1,1],bins=28)\ndf_class4.hist(ax=axs[2,0],bins=28)\ndf_class5.hist(ax=axs[2,1],bins=28)\ndf_class6.hist(ax=axs[3,0],bins=28)\n","1747af54":"df_class0.dropna(axis=0, inplace=True)\ndf_class1.dropna(axis=0, inplace=True)\ndf_class2.dropna(axis=0, inplace=True)\ndf_class3.dropna(axis=0, inplace=True)\ndf_class4.dropna(axis=0, inplace=True)\ndf_class5.dropna(axis=0, inplace=True)\ndf_class6.dropna(axis=0, inplace=True)","6b6b60e7":"print('df_class0.shape ',df_class0.shape)\nprint('df_class1.shape ',df_class1.shape)\nprint('df_class2.shape ',df_class2.shape)\nprint('df_class3.shape ',df_class3.shape)\nprint('df_class4.shape ',df_class4.shape)\nprint('df_class5.shape ',df_class5.shape)\nprint('df_class6.shape ',df_class6.shape)\nprint('total data no2_weather.shape ',no2_weather.shape)\n","a3c8c6eb":"## apply ellipticEnvelope(gaussian distribution) at each categories\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class0.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class0 = pd.DataFrame(df_class0)\ndf_class0['deviation'] = envelope.decision_function(X_train)\ndf_class0['anomaly'] = envelope.predict(X_train)\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class1.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class1 = pd.DataFrame(df_class1)\ndf_class1['deviation'] = envelope.decision_function(X_train)\ndf_class1['anomaly'] = envelope.predict(X_train)\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class2.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class2 = pd.DataFrame(df_class2)\ndf_class2['deviation'] = envelope.decision_function(X_train)\ndf_class2['anomaly'] = envelope.predict(X_train)\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class3.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class3 = pd.DataFrame(df_class3)\ndf_class3['deviation'] = envelope.decision_function(X_train)\ndf_class3['anomaly'] = envelope.predict(X_train)","ad72e143":"envelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class4.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class4 = pd.DataFrame(df_class4)\ndf_class4['deviation'] = envelope.decision_function(X_train)\ndf_class4['anomaly'] = envelope.predict(X_train)\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class5.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class5 = pd.DataFrame(df_class5)\ndf_class5['deviation'] = envelope.decision_function(X_train)\ndf_class5['anomaly'] = envelope.predict(X_train)\n\nenvelope =  EllipticEnvelope(contamination = outliers_fraction) \nX_train = df_class6.values.reshape(-1,1)\nenvelope.fit(X_train)\ndf_class6 = pd.DataFrame(df_class6)\ndf_class6['deviation'] = envelope.decision_function(X_train)\ndf_class6['anomaly'] = envelope.predict(X_train)","4a93c3c5":"a0 = df_class0.loc[df_class0['anomaly'] == 1, 'no2_emission_sum']\nb0 = df_class0.loc[df_class0['anomaly'] == -1, 'no2_emission_sum']\n\na1 = df_class1.loc[df_class1['anomaly'] == 1, 'no2_emission_sum']\nb1 = df_class1.loc[df_class1['anomaly'] == -1, 'no2_emission_sum']\n\na2 = df_class2.loc[df_class2['anomaly'] == 1, 'no2_emission_sum']\nb2 = df_class2.loc[df_class2['anomaly'] == -1, 'no2_emission_sum']\n\na3 = df_class3.loc[df_class3['anomaly'] == 1, 'no2_emission_sum']\nb3 = df_class3.loc[df_class3['anomaly'] == -1, 'no2_emission_sum']\n\na4 = df_class4.loc[df_class4['anomaly'] == 1, 'no2_emission_sum']\nb4 = df_class4.loc[df_class4['anomaly'] == -1, 'no2_emission_sum']\n\na5 = df_class5.loc[df_class5['anomaly'] == 1, 'no2_emission_sum']\nb5 = df_class5.loc[df_class5['anomaly'] == -1, 'no2_emission_sum']\n\na6 = df_class6.loc[df_class6['anomaly'] == 1, 'no2_emission_sum']\nb6 = df_class6.loc[df_class6['anomaly'] == -1, 'no2_emission_sum']","92caa419":"\nfig, axs = plt.subplots(2,2)\naxs[0,0].hist([a0,b0], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[0,1].hist([a1,b1], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[1,0].hist([a2,b2], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[1,1].hist([a3,b3], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\nplt.legend()\nplt.show()","353a299f":"a0.describe()","d4cc58f0":"a1.describe()","881d3e75":"a3.describe()","ccc0bf63":"a4.describe()","badfb8ac":"fig, axs = plt.subplots(2,2)\naxs[0,0].hist([a4,b4], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[0,1].hist([a5,b5], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\naxs[1,0].hist([a6,b6], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\n\nplt.legend()\nplt.show()","cf0905a7":"a4.describe()","fd89e234":"a5.describe()","ffbd55c0":"a6.describe()","93714801":"# add the data to the main \ndf_class = pd.concat([df_class0, df_class1, df_class2, df_class3])\nno2_weather['anomaly22'] = df_class['anomaly']\nno2_weather['anomaly22'] = np.array(no2_weather['anomaly22'] == -1).astype(int) ","28d1b717":"# visualisation of anomaly throughout time\nfig, ax = plt.subplots()\n\na = no2_weather.loc[no2_weather['anomaly22'] == 1, ('time_epoch', 'no2_emission_sum')] #anomaly\n\nax.plot(no2_weather['time_epoch'], no2_weather['no2_emission_sum'], color='blue')\nax.scatter(a['time_epoch'],a['no2_emission_sum'], color='red')\nplt.show()","7d1b103d":"# visualisation of anomaly with temperature repartition\na = no2_weather.loc[no2_weather['anomaly22'] == 0, 'no2_emission_sum']\nb = no2_weather.loc[no2_weather['anomaly22'] == 1, 'no2_emission_sum']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])\nplt.legend()\nplt.show()","a4c271e4":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\nHTML('<iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/5p8B2Ikcw-k\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","5fdbf0f9":"# train isolation forest \nmodel =  IsolationForest(contamination = outliers_fraction)\nmodel.fit(data_IF)\n  \nno2_weather['anomaly25'] = pd.Series(model.predict(data_IF))\nno2_weather['anomaly25'] = no2_weather['anomaly25'].map( {1: 0, -1: 1} )\nprint(no2_weather['anomaly25'].value_counts())\n","48b87ca4":"a = no2_weather.loc[no2_weather['anomaly25'] == 0, 'total_precipitation_surface_mean']\nb = no2_weather.loc[no2_weather['anomaly25'] == 1, 'total_precipitation_surface_mean']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label = ['normal', 'anomaly'])\nplt.legend()\nplt.show()","3579a018":"fig, ax = plt.subplots()\n\na = no2_weather.loc[no2_weather['anomaly25'] == 1, ['time_epoch', 'total_precipitation_surface_mean']] #anomaly\n\nax.plot(no2_weather['time_epoch'], no2_weather['total_precipitation_surface_mean'], color='blue')\nax.scatter(a['time_epoch'],a['total_precipitation_surface_mean'], color='red')\nplt.show()","788be0d2":"a = no2_weather.loc[no2_weather['anomaly25'] == 0, 'no2_emission_sum']\nb = no2_weather.loc[no2_weather['anomaly25'] == 1, 'no2_emission_sum']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label = ['normal', 'anomaly'])\nplt.legend()\nplt.show()","9d752add":"fig, ax = plt.subplots()\n\na = no2_weather.loc[no2_weather['anomaly25'] == 1, ['time_epoch', 'no2_emission_sum']] #anomaly\n\nax.plot(no2_weather['time_epoch'], no2_weather['no2_emission_sum'], color='blue')\nax.scatter(a['time_epoch'],a['no2_emission_sum'], color='red')\nplt.show()","7f8d9c22":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\nHTML('<iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/086OcT-5DYI\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","78f63335":"# Take useful feature and standardize them \ndata_SVM = no2_weather[['time_epoch','DayOfTheWeek','day','no2_emission_sum','temperature_2m_above_ground_mean','specific_humidity_2m_above_ground_mean','relative_humidity_2m_above_ground_mean','u_component_of_wind_10m_above_ground_mean','v_component_of_wind_10m_above_ground_mean','total_precipitation_surface_mean']]\ndata_SVM.dropna(axis=0, inplace=True)\nprint(data_SVM.shape)\n\n\nmin_max_scaler = preprocessing.StandardScaler()\nnp_scaled = min_max_scaler.fit_transform(data_SVM)\n# train one class SVM \nmodel =  OneClassSVM(nu=0.95 * outliers_fraction) #nu=0.95 * outliers_fraction  + 0.05\ndata_SVM = pd.DataFrame(np_scaled)\nmodel.fit(data_SVM)\n# add the data to the main  \nno2_weather['anomaly26'] = pd.Series(model.predict(data_SVM))\nno2_weather['anomaly26'] = no2_weather['anomaly26'].map( {1: 0, -1: 1} )\nprint(no2_weather['anomaly26'].value_counts())","0c1f7838":"a = no2_weather.loc[no2_weather['anomaly26'] == 0, 'total_precipitation_surface_mean']\nb = no2_weather.loc[no2_weather['anomaly26'] == 1, 'total_precipitation_surface_mean']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label = ['normal', 'anomaly'])\nplt.legend()\nplt.show()","38e85139":"fig, ax = plt.subplots()\n\na = no2_weather.loc[no2_weather['anomaly26'] == 1, ['time_epoch', 'total_precipitation_surface_mean']] #anomaly\n\nax.plot(no2_weather['time_epoch'], no2_weather['total_precipitation_surface_mean'], color='blue')\nax.scatter(a['time_epoch'],a['total_precipitation_surface_mean'], color='red')\nplt.show()","1b488a40":"a = no2_weather.loc[no2_weather['anomaly26'] == 0, 'no2_emission_sum']\nb = no2_weather.loc[no2_weather['anomaly26'] == 1, 'no2_emission_sum']\n\nfig, axs = plt.subplots()\naxs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label = ['normal', 'anomaly'])\nplt.legend()\nplt.show()","7caf09ed":"fig, ax = plt.subplots()\n\na = no2_weather.loc[no2_weather['anomaly26'] == 1, ['time_epoch', 'no2_emission_sum']] #anomaly\n\nax.plot(no2_weather['time_epoch'], no2_weather['no2_emission_sum'], color='blue')\nax.scatter(a['time_epoch'],a['no2_emission_sum'], color='red')\nplt.show()","22760d1f":"no2_weather['yr'] = no2_weather.start_date.dt.year\nno2_weather['mt'] = no2_weather.start_date.dt.month\nno2_weather['d'] = no2_weather.start_date.dt.day\n\nno2_weather['weekday'] = no2_weather.start_date.dt.weekday\nno2_weather['weekday_mean'] = no2_weather.weekday.replace(no2_weather[:199].groupby('weekday')['no2_emission_sum'].mean().to_dict())","d304e466":"no2_weather.head(2)","ded60371":"timeLags = np.arange(1,10*48*7)\nautoCorr = [no2_weather.no2_emission_sum.autocorr(lag=dt) for dt in timeLags]\nplt.figure(figsize=(19,8))\nplt.plot(1.0\/(48*7)*timeLags, autoCorr)\nplt.xlabel('time lag [weeks]')\nplt.ylabel('correlation coeff', fontsize=12)","6567d930":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\nHTML('<iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/9zhrxE5PQgY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","a1ca9737":"data_ = no2_weather.loc[:,['start_date','no2_emission_sum']] \ndata_['start_date'] = pd.to_datetime(no2_weather['start_date'])\ndata_.set_index('start_date', inplace=True)\ndata_ = data_.resample(\"1D\").sum() # day sum","c96e490b":"#Create a new dataframe with only the 'no2_emission_sum column\ndata_2 = data_.filter(['no2_emission_sum'])\n#Convert the dataframe to a numpy array\ndataset = data_2.values\n#Get the number of rows to train the model on\ntraining_data_len = int(np.ceil( len(dataset) * .8 ))\ntraining_data_len","f69a166a":"#Scale the data\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(dataset)\n# scaled_data","7fb1a972":"train_data = scaled_data[0:int(training_data_len), :]\n#Split the data into x_train and y_train data sets\nx_train = []\ny_train = []\n\nfor i in range(129, len(train_data)):\n    x_train.append(train_data[i-129:i, 0])\n    y_train.append(train_data[i, 0])\n#     if i<= 61:\n#         print(x_train)\n#         print(y_train)\n#         print()","b003453d":"# Convert the x_train and y_train to numpy arrays \nx_train, y_train = np.array(x_train), np.array(y_train)","d8f4b1e9":"#Reshape the data\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))","3a059f1e":"model = Sequential()\nmodel.add(LSTM(50, return_sequences=True, input_shape= (x_train.shape[1], 1)))\nmodel.add(LSTM(50, return_sequences= False))\nmodel.add(Dense(25))\nmodel.add(Dense(1))","09c1401b":"\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.fit(x_train, y_train, batch_size=1, epochs=1)","cb6ca175":"#Create the testing data set\ntest_data = scaled_data[training_data_len - 129: , :]\n#Create the data sets x_test and y_test\nx_test = []\ny_test = dataset[training_data_len:, :]\nfor i in range(129, len(test_data)):\n    x_test.append(test_data[i-129:i, 0])\n    \n# Convert the data to a numpy array\nx_test = np.array(x_test)","3040af59":"# Reshape the data\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n# Get the models predicted values \npredictions = model.predict(x_test)\npredictions = scaler.inverse_transform(predictions)\n# Get the root mean squared error (RMSE)\nrmse = np.sqrt(np.mean(((predictions - y_test) ** 2)))\nrmse","23a3dfa9":"# Plot the data\ntrain = data_[:training_data_len]\nvalid = data_[training_data_len:]\nvalid['Predictions'] = predictions","e15a1a5d":"# Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('start_date', fontsize=18)\nplt.ylabel('NO2 Emission Sum', fontsize=18)\nplt.plot(train['no2_emission_sum'])\nplt.plot(valid[['no2_emission_sum', 'Predictions']])\nplt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\nplt.show()","65bf7bfe":"f, ax = plt.subplots(figsize=(14,8))\npd.plotting.register_matplotlib_converters() # Add this \ndata_.plot(ax=ax, color='C0')\ndata_.rolling(window=30, center=True).mean().plot(ax=ax, ls='-', lw=3, color='C3')\nax.grid(ls=':')\nax.legend(['daily values','30 days No2 average'], frameon=False, fontsize=14)\n\n[l.set_fontsize(13) for l in ax.xaxis.get_ticklabels()]\n[l.set_fontsize(13) for l in ax.yaxis.get_ticklabels()]\nax.set_xlabel('date', fontsize=15)\nax.set_ylabel('N02 values', fontsize=15);\n# ax.axvline('2018', color='0.8', lw=8, zorder=-1)","d560cf11":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\nHTML('<iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/zlZaOnBbpUg?list=PL436A4F939FBE10D7\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","9a50845c":"from statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\narimaM = ARIMA(data, order=(5,1,0))\narimaMfit = arimaM.fit(disp=0)\nprint(arimaMfit.summary())","bad88a2a":"errors = pd.DataFrame(arimaMfit.resid)\nerrors.plot()\npyplot.show()\nerrors.plot(kind='kde')\npyplot.show()\nprint(errors.describe())","1e557b4c":"X = data.values\nsize = int(len(X) * 0.70)\nlimitCount = 40\ntrain, test = X[0:size], X[size:size+limitCount]\nhistory = [x for x in train]","87a85c3e":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","483e6d5d":"pred = []\nfor t in range(len(test)):\n    model = ARIMA(history, order=(5,1,0))\n    model_fit = model.fit(disp=0)\n    output = model_fit.forecast()\n    yhat = output[0]\n    pred.append(yhat)\n    obs = test[t]\n    history.append(obs)\n    print('pred=%f, exp=%f' % (yhat, obs))\nerror = mean_squared_error(test, pred)\nerror2 = rmsle(pred,test)\n\nprint('Mean Squared Error: %.3f' % error)\nprint('RMSLE: %.3f' % error)\n","53292970":"pyplot.plot(test)\npyplot.plot(pred, color='red')\npyplot.show()","c510da98":"# Suppress warnings \nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\nHTML('<iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/pOYAXv15r3A\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe>')","aad652fc":"X= no2_weather[['start_date','no2_emission_sum','temperature_2m_above_ground_mean','specific_humidity_2m_above_ground_mean','relative_humidity_2m_above_ground_mean',\n            'u_component_of_wind_10m_above_ground_mean','v_component_of_wind_10m_above_ground_mean','total_precipitation_surface_mean']]\ny=no2_weather['no2_emission_sum']","54669d27":"train_dataset= pd.DataFrame()\ntrain_dataset['ds'] = pd.to_datetime(no2_weather[\"start_date\"])\ntrain_dataset['y']=y\ntrain_dataset.head(2)","469c1938":"prophet_basic = Prophet()\nprophet_basic.fit(train_dataset)","449fc3cf":"future= prophet_basic.make_future_dataframe(periods=30)\nfuture.tail()","e0620d89":"forecast=prophet_basic.predict(future)","17d1954e":"fig1 =prophet_basic.plot(forecast)","55c2acf2":"fig1 = prophet_basic.plot_components(forecast)","3fbc928c":"from fbprophet.diagnostics import cross_validation, performance_metrics\ndf_cv = cross_validation(prophet_basic, horizon='30 days')\ndf_p = performance_metrics(df_cv)\ndf_p.head(30)","0b84da90":"from fbprophet.plot import plot_cross_validation_metric\nfig_mape = plot_cross_validation_metric(df_cv, metric='mape')\nfig_rmse = plot_cross_validation_metric(df_cv, metric='rmse')\n","746ba605":"from fbprophet.plot import add_changepoints_to_plot\nfig = prophet_basic.plot(forecast)\na = add_changepoints_to_plot(fig.gca(), prophet_basic, forecast)","71a63c60":"prophet_basic.changepoints[:10]","d5e3190a":"pro_change= Prophet(changepoint_range=0.9)\nforecast = pro_change.fit(train_dataset).predict(future)\nfig= pro_change.plot(forecast);\na = add_changepoints_to_plot(fig.gca(), pro_change, forecast)","f5ce5fac":"pro_change= Prophet(n_changepoints=20, yearly_seasonality=True, changepoint_prior_scale=0.08)\nforecast = pro_change.fit(train_dataset).predict(future)\nfig= pro_change.plot(forecast);\na = add_changepoints_to_plot(fig.gca(), pro_change, forecast)","9f47e325":"train_dataset['temperature_2m_above_ground_mean'] = X['temperature_2m_above_ground_mean']\ntrain_dataset['specific_humidity_2m_above_ground_mean'] = X['specific_humidity_2m_above_ground_mean']\ntrain_dataset['relative_humidity_2m_above_ground_mean'] = X['relative_humidity_2m_above_ground_mean']\ntrain_dataset['u_component_of_wind_10m_above_ground_mean'] = X['u_component_of_wind_10m_above_ground_mean']\ntrain_dataset['v_component_of_wind_10m_above_ground_mean'] = X['v_component_of_wind_10m_above_ground_mean']\ntrain_dataset['total_precipitation_surface_mean'] = X['total_precipitation_surface_mean']\n\ntrain_X= train_dataset[:200]\ntest_X= train_dataset[200:]\n","410f4163":"#Additional Regressor\npro_regressor= Prophet()\npro_regressor.add_regressor('temperature_2m_above_ground_mean')\npro_regressor.add_regressor('specific_humidity_2m_above_ground_mean')\npro_regressor.add_regressor('relative_humidity_2m_above_ground_mean')\npro_regressor.add_regressor('u_component_of_wind_10m_above_ground_mean')\npro_regressor.add_regressor('v_component_of_wind_10m_above_ground_mean')\npro_regressor.add_regressor('total_precipitation_surface_mean')\n#Fitting the data\npro_regressor.fit(train_X)\nfuture_data = pro_regressor.make_future_dataframe(periods=30) # 30 days\n#forecast the data for Test  data\nforecast_data = pro_regressor.predict(test_X)\npro_regressor.plot(forecast_data);","432d5618":"df_cv_reg = cross_validation(pro_regressor, horizon='30 days')\ndf_p_reg = performance_metrics(df_cv_reg)\ndf_p_reg.head(30)","b223f2f5":"fig_mape_reg = plot_cross_validation_metric(df_cv_reg, metric='mape')\nfig_mape_reg = plot_cross_validation_metric(df_cv_reg, metric='rmse')\n","45793bdb":"from fbprophet.plot import plot_plotly, add_changepoints_to_plot\nimport plotly.offline as py\n\nfig_d_reg = plot_plotly(pro_regressor, forecast_data)\n\npy.iplot(fig_d_reg) \n\nfig_d_reg = pro_regressor.plot(forecast_data,xlabel='Date',ylabel='N02 values')\n","d9a37d11":"import pickle\nwith open('forecast_model_No2.pckl', 'wb') as fout:\n    pickle.dump(pro_regressor, fout)\nwith open('forecast_model_No2.pckl', 'rb') as fin:\n    m2 = pickle.load(fin)\n","9a45f14d":"from datetime import datetime\nfiles=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/ds4g-environmental-insights-explorer\/eie_data\/s5p_no2'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\n\n# read all the absorbing aerosol index data into one list of arrays\nno2_first_day=[]\nno2_first_key=[]\nno2_arr=[]\nband=0 #  NO2_column_number_density\nfor i in range(0,len(files)):\n    no2_first_day.append(datetime.strptime(files[i][76:91], '%Y%m%dT%H%M%S').date())\n    no2_first_key.append(datetime.strptime(files[i][76:91], '%Y%m%dT%H%M%S').toordinal()+1) # correction of + 1 day in order to sync on climate data\n    no2_arr.append(rio.open(files[i]).read(band+1))\n\n","a361a91c":"from pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()\n\na=[]\na_pos=[]\nfor i in range(0,len(no2_arr)): \n    a.append(np.nanmean(no2_arr[i]))\n    a_pos.append(np.nanmean(np.clip(no2_arr[i],0,10000)))\n    \nno2_rgn=pd.DataFrame({ 'start_date': no2_first_day,'no2_rgn' : a_pos, 'key_date' : no2_first_key })\nno2_rgn=no2_rgn.sort_values('start_date')\nno2_rgn=no2_rgn.reset_index()","1dce44ea":"# read only the NO2 index arrays with a nan-percentage <5% into one list of arrays for calculation of local NO2 data\nfiles=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/ds4g-environmental-insights-explorer\/eie_data\/s5p_no2'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\n\nno2_first_day=[]\nno2_first_key=[]\nno2_arr=[]\nband=0 # NO2_column_number_density\nfor i in range(0,len(files)):\n    a=rio.open(files[i]).read(band+1)\n    if pd.isnull(a).sum().sum() < 3515:\n        no2_first_day.append(datetime.strptime(files[i][76:91], '%Y%m%dT%H%M%S').date())\n        no2_first_key.append(datetime.strptime(files[i][76:91], '%Y%m%dT%H%M%S').toordinal()+1) # correction of + 1 day in order to sync on climate data\n        no2_arr.append(np.clip(a,0,10000))  # clip negative values to zero","db125101":"gray= power_plants[['name','primary_fuel','capacity_mw','img_idx_lt','img_idx_lg','plant']].copy() \ngray.head()","3fe8b867":"pollute_clean_primary_fuel= power_plants.loc[((power_plants['primary_fuel']=='Coal') | (power_plants['primary_fuel']=='Oil') | (power_plants['primary_fuel']=='Gas')),['name','primary_fuel','capacity_mw','img_idx_lt','img_idx_lg','plant']]\npollute_clean_primary_fuel.head()","e588817f":"# NO2_column_number_density value in proximity of all plants with all locations in location mask - proximity is +\/- n points from location of plant\nn=11\nno2_=[]\nfor j in range(0,len(gray)):\n    idx_lt=gray.iloc[j,3]\n    idx_lg=gray.iloc[j,4]\n    no2_j=[]\n    for i in range(0,len(no2_arr)):\n        no2_j.append(np.nanmean(no2_arr[i][idx_lt-n:idx_lt+n,idx_lg-n:idx_lg+n])) # calculate average of no2 for location of plant\n    \n    no2_.append(no2_j)","bc52374d":"aa=pd.DataFrame({'key_date':np.array(no2_first_key), 'start_date': no2_first_day}) \n\nfor j in range(0,len(gray)):\n    aa[gray.iloc[j,5]]=no2_[j]  # add average of N02 for location of plant to dataframe with column name from df gray.plant\n\nprint('size of dataframe with aai data for gray-energy power-plant locations: ',aa.shape)\n# sorting dataframe on date to produce ordered time series\naa=aa.sort_values('key_date')\naa=aa.reset_index()\naa=aa.drop(columns=['index'])\naa=aa.fillna(0)\naa.head()","0715376e":"gray.loc[:,'EF_wght']=1\nww2=pd.DataFrame({'start_date':no2_first_day})\nXX2=pd.DataFrame({})\n\nfor j in range(0,len(gray)):\n    ww2[gray.iloc[j,0]]=no2_j[j]  # add average of N02 Description for location of plant to dataframe\n\n    x=ww2.groupby(by='start_date').agg(['mean'])\n    X2=pd.merge(aa.loc[:,['start_date',gray.iloc[j,5]]],x, how='inner', on='start_date')\n    X2=X2.rename(columns = {gray.iloc[j,5]:'no2_density_locationofplant'})\n    c=gray.iloc[j,5]   \n    X2[c]=np.ones((len(X2)))*gray.iloc[j,6] # addition of EF_wght for each plant to the dataframe\n    XX=pd.concat([XX2,X2], axis=0, sort=False) # aggregation of dataframe per plant_location\nXX=XX.fillna(0) \nXX=XX.reset_index()","c4b8bdf2":"for i in range(XX.shape[1]):\n    if i==0 or i==1:\n        pass\n    else:\n        XX  = XX.rename( columns= {XX.columns[i] :str(XX.columns[i]).replace(\"'\",'').replace('(','').replace(')','').replace(',','').replace(' ','_') })\nXX.head()","252d46ff":"XX=XX.drop(columns=['index','Tor2_Hydro'])\nXX.to_csv('no2_density_estimation.csv', index=False)","ffc9f6cb":"X= XX[['start_date','no2_density_locationofplant','Aguirre_mean','Costa_Sur_mean','San_Juan_CC_mean','Palo_Seco_mean','EcoEl\u00e9ctrica_mean','A.E.S._Corp._mean','Cambalache_mean','Mayag\u00fcez_mean','Santa_Isabel_Wind_Farm_mean','Oriana_Solar_Farm_mean','Yabucoa_mean','Daguao_mean','Jobos_mean','Vega_Baja_mean','San_Fermin_Solar_Farm_mean','Loiza_Solar_Park_mean','Yauco_1_mean','AES_Ilumina_mean','Punta_Lima_mean','Caonillas_1_mean','Salinas_mean','Dos_Bocas_mean','Carite_1_mean','Yauco_2_mean','Toro_Negro_1_mean','Garzas_1_mean','Vieques_EPP_mean','Garzas_2_mean','R\u00edo_Blanco_mean','Windmar_Ponce_mean','Caonillas_2_mean','Toro_Negro_2_mean']]\ny=XX['no2_density_locationofplant']\nprint(XX.shape)\n       ","bc4f29b8":"train_dataset= pd.DataFrame()\ntrain_dataset['ds'] = pd.to_datetime(XX[\"start_date\"])\ntrain_dataset['y']=y\ntrain_dataset.head(2)","4c37393e":"train_dataset['Aguirre_mean'] = X['Aguirre_mean']\ntrain_dataset['Costa_Sur_mean'] = X['Costa_Sur_mean']\ntrain_dataset['San_Juan_CC_mean'] = X['San_Juan_CC_mean']\ntrain_dataset['Palo_Seco_mean'] = X['Palo_Seco_mean']\ntrain_dataset['EcoEl\u00e9ctrica_mean'] = X['EcoEl\u00e9ctrica_mean']\ntrain_dataset['A.E.S._Corp._mean'] = X['A.E.S._Corp._mean']\ntrain_dataset['Cambalache_mean'] = X['Cambalache_mean']\ntrain_dataset['Mayag\u00fcez_mean'] = X['Mayag\u00fcez_mean']\ntrain_dataset['Santa_Isabel_Wind_Farm_mean'] = X['Santa_Isabel_Wind_Farm_mean']\ntrain_dataset['Oriana_Solar_Farm_mean'] = X['Oriana_Solar_Farm_mean']\ntrain_dataset['Yabucoa_mean'] = X['Yabucoa_mean']\ntrain_dataset['Daguao_mean'] = X['Daguao_mean']\ntrain_dataset['Jobos_mean'] = X['Jobos_mean']\ntrain_dataset['Vega_Baja_mean'] = X['Vega_Baja_mean']\ntrain_dataset['San_Fermin_Solar_Farm_mean'] = X['San_Fermin_Solar_Farm_mean']\ntrain_dataset['Loiza_Solar_Park_mean'] = X['Loiza_Solar_Park_mean']\ntrain_dataset['Yauco_1_mean'] = X['Yauco_1_mean']\ntrain_dataset['AES_Ilumina_mean'] = X['AES_Ilumina_mean']\ntrain_dataset['Punta_Lima_mean'] = X['Punta_Lima_mean']\ntrain_dataset['Salinas_mean'] = X['Salinas_mean']\ntrain_dataset['Dos_Bocas_mean'] = X['Dos_Bocas_mean']\ntrain_dataset['Carite_1_mean'] = X['Carite_1_mean']\ntrain_dataset['Yauco_2_mean'] = X['Yauco_2_mean']\ntrain_dataset['Toro_Negro_1_mean'] = X['Toro_Negro_1_mean']\ntrain_dataset['Garzas_1_mean'] = X['Garzas_1_mean']\ntrain_dataset['Vieques_EPP_mean'] = X['Vieques_EPP_mean']\ntrain_dataset['Garzas_2_mean'] = X['Garzas_2_mean']\ntrain_dataset['R\u00edo_Blanco_mean'] = X['R\u00edo_Blanco_mean']\ntrain_dataset['Windmar_Ponce_mean'] = X['Windmar_Ponce_mean']\ntrain_dataset['Caonillas_2_mean'] = X['Caonillas_2_mean']\ntrain_dataset['Toro_Negro_2_mean'] = X['Toro_Negro_2_mean']\n\ntrain_X= train_dataset[:200]\ntest_X= train_dataset[200:]","4822d1f0":"#Additional Regressor\npro_regressor= Prophet()\npro_regressor.add_regressor('Aguirre_mean')\npro_regressor.add_regressor('Costa_Sur_mean')\npro_regressor.add_regressor('San_Juan_CC_mean')\npro_regressor.add_regressor('Palo_Seco_mean')\npro_regressor.add_regressor('EcoEl\u00e9ctrica_mean')\npro_regressor.add_regressor('A.E.S._Corp._mean')\npro_regressor.add_regressor('Cambalache_mean')\npro_regressor.add_regressor('Mayag\u00fcez_mean')\npro_regressor.add_regressor('Santa_Isabel_Wind_Farm_mean')\npro_regressor.add_regressor('Oriana_Solar_Farm_mean')\npro_regressor.add_regressor('Yabucoa_mean')\npro_regressor.add_regressor('Daguao_mean')\npro_regressor.add_regressor('Jobos_mean')\npro_regressor.add_regressor('Vega_Baja_mean')\npro_regressor.add_regressor('San_Fermin_Solar_Farm_mean')\npro_regressor.add_regressor('Loiza_Solar_Park_mean')\npro_regressor.add_regressor('Yauco_1_mean')\npro_regressor.add_regressor('AES_Ilumina_mean')\npro_regressor.add_regressor('Punta_Lima_mean')\npro_regressor.add_regressor('Salinas_mean')\npro_regressor.add_regressor('Dos_Bocas_mean')\npro_regressor.add_regressor('Carite_1_mean')\npro_regressor.add_regressor('Yauco_2_mean')\npro_regressor.add_regressor('Toro_Negro_1_mean')\npro_regressor.add_regressor('Garzas_1_mean')\npro_regressor.add_regressor('Vieques_EPP_mean')\npro_regressor.add_regressor('Garzas_2_mean')\npro_regressor.add_regressor('R\u00edo_Blanco_mean')\npro_regressor.add_regressor('Windmar_Ponce_mean')\npro_regressor.add_regressor('Caonillas_2_mean')\npro_regressor.add_regressor('Toro_Negro_2_mean')\n\n#Fitting the data\npro_regressor.fit(train_X)\nfuture_data = pro_regressor.make_future_dataframe(periods=30) # 30 days\n#forecast the data for Test  data\nforecast_data = pro_regressor.predict(test_X)\npro_regressor.plot(forecast_data);","6131876c":"df_cv_reg = cross_validation(pro_regressor, horizon='30 days')\ndf_p_reg = performance_metrics(df_cv_reg)\ndf_p_reg.head(30)","cf9f3925":"fig_mape_reg = plot_cross_validation_metric(df_cv_reg, metric='mape')\nfig_mape_reg = plot_cross_validation_metric(df_cv_reg, metric='rmse')","e45694ca":"from fbprophet.plot import plot_plotly, add_changepoints_to_plot\nimport plotly.offline as py\n\nfig_d_reg = plot_plotly(pro_regressor, forecast_data)\n\npy.iplot(fig_d_reg) \n\nfig_d_reg = pro_regressor.plot(forecast_data,xlabel='Date',ylabel='no2 Density by Location of Plant values')","57900f95":"import pickle\nwith open('forecast_model_No2Density.pckl', 'wb') as fout:\n    pickle.dump(pro_regressor, fout)\nwith open('forecast_model_No2Density.pckl', 'rb') as fin:\n    m2 = pickle.load(fin)","441c4008":"power_plants_df[['name','primary_fuel','plant']][power_plants_df['primary_fuel']=='Coal']","083313e9":"train_dataset['A.E.S._Corp._mean'] = X['A.E.S._Corp._mean']\n# Additional Regressor\npro_regressor= Prophet()\npro_regressor.add_regressor('A.E.S._Corp._mean')\ntrain_X= train_dataset[:200]\ntest_X= train_dataset[200:]\n\n#Fitting the data\npro_regressor.fit(train_X)\nfuture_data = pro_regressor.make_future_dataframe(periods=30) # 30 days\n#forecast the data for Test  data\nforecast_data = pro_regressor.predict(test_X)\npro_regressor.plot(forecast_data);","b1ce69dc":"df_cv_reg = cross_validation(pro_regressor, horizon='30 days')\ndf_p_reg = performance_metrics(df_cv_reg)\ndf_p_reg.head(30)","379e69cf":"fig_mape_reg = plot_cross_validation_metric(df_cv_reg, metric='mape')\nfig_mape_reg = plot_cross_validation_metric(df_cv_reg, metric='rmse')","35e5a2a5":"month_p_fuel = X[['start_date', 'no2_density_locationofplant', 'A.E.S._Corp._mean']].copy()\nmonth_p_fuel['date'] = pd.to_datetime(X[\"start_date\"])\nmonth_p_fuel['date'] = month_p_fuel['date'].dt.month\nmonth_p_fuel = month_p_fuel.groupby(['date','A.E.S._Corp._mean']).sum()\nmonth_p_fuel\n","4a740c06":"month_p_fuel_agg = month_p_fuel.groupby(['date', 'A.E.S._Corp._mean']).agg(['sum'])\nmonth_p_fuel_agg = month_p_fuel_agg.reset_index()\nlevel_0 = month_p_fuel_agg.columns.droplevel(0)\nlevel_1 = month_p_fuel_agg.columns.droplevel(1)\nlevel_0 = ['' if x == '' else '-' + x for x in level_0]\n\nmonth_p_fuel_agg.columns = level_1 + level_0\nmonth_p_fuel_agg.rename_axis(None, axis=1)\n# month_p_fuel_agg.head()","6cc899fc":"from plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\n\nfig_total = px.line(month_p_fuel_agg, x='date', y='no2_density_locationofplant-sum', color='A.E.S._Corp._mean', render_mode='svg')\nfig_total.update_layout(title='Total NO2 aspect in A.E.S._Corp._mean - 12 months')\nfig_total.show()","c9f4526a":"dayweek_p_fuel = X[['start_date', 'no2_density_locationofplant', 'A.E.S._Corp._mean']].copy()\ndayweek_p_fuel['dateofweek'] = pd.to_datetime(X[\"start_date\"])\ndayweek_p_fuel['dateofweek'] = dayweek_p_fuel['dateofweek'].dt.dayofweek\ndayweek_p_fuel = dayweek_p_fuel.groupby(['dateofweek','A.E.S._Corp._mean']).sum()\n\ndayofweek_p_fuel_agg = dayweek_p_fuel.groupby(['dateofweek', 'A.E.S._Corp._mean']).agg(['sum'])\ndayofweek_p_fuel_agg = dayofweek_p_fuel_agg.reset_index()\nlevel_0 = dayofweek_p_fuel_agg.columns.droplevel(0)\nlevel_1 = dayofweek_p_fuel_agg.columns.droplevel(1)\nlevel_0 = ['' if x == '' else '-' + x for x in level_0]\n\ndayofweek_p_fuel_agg.columns = level_1 + level_0\ndayofweek_p_fuel_agg.rename_axis(None, axis=1)","cc02554d":"from plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\n\nfig_total = px.line(dayofweek_p_fuel_agg, x='dateofweek', y='no2_density_locationofplant-sum', color='A.E.S._Corp._mean', render_mode='svg')\nfig_total.update_layout(title='Total NO2 aspect in A.E.S._Corp._mean - day of week')\nfig_total.show()","3f0ba77a":"\n\n<div class=\"h3\">Our purpose is to detect these abnormal observations in advance!<\/div>\n","75061323":"[\ud83c\udf0f\ud83c\udf3fGreen Future: Analysis and Solution](https:\/\/www.kaggle.com\/caesarlupum\/green-future-analysis-and-solution\/)\n\n<div class=\"h3\"> Submissions: <\/div>\n\nFollowing are parts of Kernels Submissions in order:\n<ul>\n    <li>\n        <a href=\"https:\/\/www.kaggle.com\/caesarlupum\/ds4g-go-to-the-green-future\" target=\"_blank\">Part 1: \ud83c\udf0f\ud83c\udf3fDS4G: Go to the Green Future! - A Gentle Introduction <\/a>  \n    <\/li>\n    <li>\n        <a href=\"https:\/\/www.kaggle.com\/maxlenormand\/saving-the-power-plants-csv-to-geojson\" target=\"_blank\">Part 2: Saving the Power Plants CSV to GeoJSON - EDA Analysis - Tutorial, analytics <\/a>  \n    <\/li>\n    <li>\n        <a href=\"https:\/\/www.kaggle.com\/caesarlupum\/ds4g-anomaly-analysis\" target=\"_blank\">Part 3: \ud83c\udf0f\ud83c\udf3fGreen Future: Anomaly Analysis & Time Series - A Deep Analysis <\/a>  \n    <\/li>\n\n<\/ul>\n\n<div align='center'><font size=\"5\" color=\"#00b899\">\ud83c\udf0f\ud83c\udf3fGreen Future: Anomaly Analysis & Time Series<\/font><\/div>\n<div align='center'>Other Parts: <a href='https:\/\/www.kaggle.com\/caesarlupum\/ds4g-go-to-the-green-future'>Part 1<\/a> | <a href='https:\/\/www.kaggle.com\/maxlenormand\/saving-the-power-plants-csv-to-geojson'>Part 2<\/a> | <a href='https:\/\/www.kaggle.com\/caesarlupum\/ds4g-anomaly-analysis'>Part 3<\/a>  \n\n<\/div>","35b7e9b3":"<div class=\"h2\">One class SVM<\/div>\n<a id=\"OUTLIER3\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","c4e0ed47":"Good detections of extreme values.","41e6e19a":"\n<div class=\"h3\">Visualisation of anomaly throughout time<\/div>\n<a id=\"IF\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","fc394854":"We get a line plot of the residual errors, suggesting that there may still be some trend information not captured by the model and \n\nwe get a density plot of the residual error values, suggesting the errors are Gaussian, but may not be centered on zero.","a5cec00e":"#### Build the LSTM model","e88a4690":"Recurrent Networks can be improved to remember long range dependencies by using whats called a Long-Short Term Memory (LSTM) Cell. Let's build one using just numpy! I'll go over the cell components as well as the forward and backward pass logic\n  ","b1590c09":"### Forecast quality evaluation for region\nLet's evaluate the quality of the algorithm by calculating the error metrics for the last 30 days that we predicted. For this, we will need the observations yi and the corresponding predicted values y^i \n","4ad1a74e":"<div class=\"h3\">creation of 6 distinct categories emissions<\/div>\n","2f587315":"The Analysis of Time Series","63faea49":"<div class=\"h2\">Save The Model<\/div>\n<a id=\"PRO4\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n\nThe model should be re-trained when new data becomes available. There is no point to re-train model, if data is not changed. Save model instead and use it again, when user wants to call predict function. Use pickle functionality for that:\n","89786c1a":"A line plot is created showing the expected values (blue) compared to the rolling forecast predictions (red). \nWe can see the values show some trend and are in the correct scale","21f7b464":"Printing a summary of the fit model.\nThis summarizes the coefficient values used as well as the skill of the fit on the on the in-sample observations.","2e9eb0de":"<div class=\"h3\"> shape and head s5p  <\/div>\n<a id=\"P\"><\/a>\n  ","5003c5a1":"<div class=\"h3\"> # plot residual errors <\/div>\n<a id=\"P\"><\/a>\n  ","48e89487":"<div class=\"h3\"> drop nan values  <\/div>\n<a id=\"P\"><\/a>\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","3e4eba82":"Friday NO2 emission stats.","0a98d718":"It shows that 10 days forecast results in around ~7% error.","3ecd43ec":"<div class=\"h2\">Forecast quality evaluation<\/div>\n<a id=\"PRO3\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n\n\nLet's evaluate the quality of the algorithm by calculating the error metrics for the last 30 days that we predicted. For this, we will need the observations  yi  and the corresponding predicted values  y^i .\n","069aabe6":"The distribution of the residual errors is displayed. \nThe results show that indeed there is a bias in the prediction (a non-zero mean in the residuals).","8e448fcc":"The sum, faceted for A.E.S.Corp._ mean aspect shows some aberrant values. For example, in general, Thursday (3) has the bigger aberrant value. Tuesday (1) has the least N02 value.","b27af6db":"<div class=\"h3\">plot the temperature repartition by catDayEmission<\/div>\n","3fcdf42e":"# <div class=\"h3\">Imports <\/div>\n<a id=\"IMPORT\"><\/a>\n[Back to Table of Contents](#top)\n\nWe are using a stack: ``numpy``, ``pandas``, ``sklearn``, ``matplotlib``, ``rasterio``, ``plotly``.","470d3d53":"Anomaly Detection Problem | Motivation \u2014 [ Machine Learning | Andrew Ng ]","74973c93":"<div class=\"h3\">Advantages of using Prophet<\/div>\n- Accommodates seasonality with multiple periods\n- Prophet is resilient to missing values\n- Best way to handle outliers in Prophet is to remove them\n- Fitting of the model is fast\n- Intuitive hyper parameters which are easy to tune","d8a9257a":"\n\n<div class=\"h3\">30 days NO2 average<\/div>\n","9db34449":"<div class=\"h3\">Predicting the values for the future<\/div>\n\nFor predicting the values using Prophet, we need to create a dataframe with ds(datetime stamp) containing the dates for which we want to make the predictions.\nWe use make_future_dataframe() to which we specify the number of days to extend into the future. By default it includes dates from the history","df519474":"Define Prophet dataset","4e53e4a1":"<div class=\"h3\">Time lag feature - week X Correlation coef <\/div>","cdfb821d":"ax.scatter(a['time_epoch'],a['no2_emission_sum'], color='red')\nno2_emission_sum","f6ba245e":"<div class='h3'>Plotting the predicted data<\/div>\n","7b668350":"<div class=\"h3\"> shape and head no2 weather <\/div>\n<a id=\"P\"><\/a>\n  ","d9b0c948":"Owing to accurate future air quality estimates, need for detecting the anomalously high increase in concentration of pollutants cannot be adjourned. The presence of NO2 concentration in air is investigated in this notebook, considering its constant increase over years as well as its inevitable health risks. Furthermore, spatiotemporal segments with anomalously high NO2 concentrations for  Porto Rico.\n","8590df3b":"<div class=\"h3\">Regional NO2 Density<\/div>","a498c21a":"Friday, Saturday, Sunday.","dd7b4c86":"   <hr>\n<a id='ds5'><\/a>\n# <div class=\"h2\">About the data<\/div>\n<a id=\"ABOUTTHEDATA\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n<hr>\n\n[Global Power Plant database ](https:\/\/developers.google.com\/earth-engine\/datasets\/catalog\/WRI_GPPD_power_plants) by WRI\n> Description\nThe Global Power Plant Database is a comprehensive, open source database of power plants around the world. It centralizes power plant data to make it easier to navigate, compare and draw insights for one\u2019s own analysis. The database covers approximately 30,000 power plants from 164 countries and includes thermal plants (e.g. coal, gas, oil, nuclear, biomass, waste, geothermal) and renewables (e.g. hydro, wind, solar). Each power plant is geolocated and entries contain information on plant capacity, generation, ownership, and fuel type. It will be continuously updated as data becomes available.\n\n[Sentinel 5P OFFL NO2](https:\/\/developers.google.com\/earth-engine\/datasets\/catalog\/COPERNICUS_S5P_OFFL_L3_NO2) by [EU\/ESA\/Copernicus](https:\/\/sentinel.esa.int\/web\/sentinel\/user-guides\/sentinel-5p-tropomi\/document-library)\n> Sentinel-5 Precursor\nSentinel-5 Precursor is a satellite launched on 13 October 2017 by the European Space Agency to monitor air pollution. The onboard sensor is frequently referred to as Tropomi (TROPOspheric Monitoring Instrument). The OFFL\/NO2 is a dataset that provides offline high-resolution imagery of **NO2 concentration**.\n\n[Global Forecast System 384-Hour Predicted Atmosphere Data](https:\/\/developers.google.com\/earth-engine\/datasets\/catalog\/NOAA_GFS0P25) by NOAA\/NCEP\/EMC\n> The Global Forecast System (GFS) is a weather forecast model produced by the National Centers for Environmental Prediction (NCEP). The GFS dataset consists of selected model outputs (described below) as gridded forecast variables. The 384-hour forecasts, with 3-hour forecast interval, are made at 6-hour temporal resolution (i.e. updated four times daily). Use the 'creation_time' and 'forecast_time' properties to select data of interest.\n\n[Global Land Data Assimilation System](https:\/\/developers.google.com\/earth-engine\/datasets\/catalog\/NASA_GLDAS_V021_NOAH_G025_T3H) by NASA\n> Global Land Data Assimilation System (GLDAS) ingests satellite and ground-based observational data products. Using advanced land surface modeling and data assimilation techniques, it generates optimal fields of land surface states and fluxes. his dataset provided by NASA ingest satellite.\n\nParticipants may also consider using other public datasets related to trade commodities for fuel types, total fuel consumed, and\/or data from the [US Energy Information Agency (EIA)](https:\/\/www.eia.gov\/state\/data.php?sid=RQ#CarbonDioxideEmissions).","49490142":"\n<div class=\"h2\">Get the root mean squared error (RMSE)<\/div>\n<a id=\"LSTM2\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n\n","85326184":"<div class=\"h3\"> shape and head weather  <\/div>\n<a id=\"P\"><\/a>\n  ","06a56f4a":"#### Create the training data set andC create the scaled training data set","dcf50c5d":"Creating features","3c8bbc6b":"Monday, Tuesday, Wednesday, Thursday","9e4453cb":"Wednesday NO2 emission stats.\n ","cb833e08":"<div class=\"h2\">Forecast quality evaluation for Power Plant over the year<\/div>\n<a id=\"M2\"><\/a>\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n\nLet's evaluate the quality of the algorithm by calculating the error metrics for the last 30 days that we predicted. For this, we will need the observations yi and the corresponding predicted values y^i \n","c55de2f1":"Creating the data set for Prophet","723da58c":"The trend shows that the days as Monday, Weednesday have high values and the last month of year have high values. ","f30dabc4":"<div class=\"h1\">Anomaly Analysis<\/div>\n\n<a id=\"OUTLIER\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","380336d7":"It shows that 10 days forecast results in around 10% error.","22aca6c6":"<div class=\"h2\">Incorporating the effects of weather condition<\/div>\n<a id=\"PRO2\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n\n\n\n> Now we add  as extra regressors in the fbprophet model","16c13af3":"<div class=\"h2\">Forecast quality evaluation<\/div>\n<a id=\"PRO1\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n\n\nLet's evaluate the quality of the algorithm by calculating the error metrics for the last 30 days that we predicted. For this, we will need the observations  yi  and the corresponding predicted values  y^i .\n\nLet's look into the object forecast that the library created for us:","19f290e5":"- ds \u2014 forecast date\n- yhat \u2014 forecast value for the given date\n- yhat_lower \u2014 lower forecast boundary for the given date\n- yhat_uppet \u2014 upper forecast boundary for the given date\nCalling plot function for Prophet model displays how the model was trained according to training data (black points \u2014 training data, blue line \u2014 forecast value, light blue area \u2014 forecast boundaries):","e633124a":"Anomaly Detection | Developing And Evaluating An Anomaly Detection System","e2dd767e":"   <hr>\nInspired by: [Exploratory Data Analysis and Factor Model](https:\/\/www.kaggle.com\/ragnar123\/exploratory-data-analysis-and-factor-model-idea),\n[Modelling of emissions of power plants](https:\/\/www.kaggle.com\/tiurii\/ds4g-modelling-of-emissions-of-power-plants)\n\nsource: [Survey](https:\/\/www.tandfonline.com\/doi\/full\/10.1080\/10962247.2019.1577314?scroll=top&needAccess=true),[Arima python](https:\/\/machinelearningmastery.com\/arima-for-time-series-forecasting-with-python\/), [anomaly detection](https:\/\/github.com\/Vicam\/Unsupervised_Anomaly_Detection\/blob\/master\/Anomaly%20detection%2C%20different%20methods%20on%20a%20simple%20example.ipynb), [Intro LSTM](https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/),[LSTM](https:\/\/www.kaggle.com\/faressayah\/stock-market-analysis-prediction-using-lstm), [prophet facebook](https:\/\/towardsdatascience.com\/time-series-prediction-using-prophet-in-python-35d65f626236), [forecast in python](https:\/\/towardsdatascience.com\/forecasting-in-python-with-facebook-prophet-29810eb57e66)","e33d0548":"The RMSE for 30 days its 0.984912.","268ba7d2":"#### Compile and Train the model","ba8c688f":"# Ending note","89da3531":"Incorporating the primary_fuel conditions","1ff1e87a":"We can also calculate a final mean squared error score (MSE) and (RMSLE) for the predictions, providing a point of comparison for other ARIMA configurations.","b0c5e1c8":"Save the Data","5bd281a3":" <div class=\"h3\"> The day of the week (Monday=0, Sunday=6) and if it's a week end day or week day.<\/div>\n\nMonday NO2 emission stats. IN general Monday have more variation that other day of week","f4f9f128":"Decreasing the changepoint_prior_scale to 0.001 to make the trend less flexible","fbebf8ef":"## We can see that the Best Approach is Prophet.","827d7bcd":"total precipitation surface mean","c9a0e740":"### Outlier Analysis of Power Plant - Coal day of week","7ece1a02":"<div class=\"h2\">Outlier Analysis of Power Plant - Coal over the year<\/div>\n<a id=\"M3\"><\/a>\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n   \n\n\nIdentifying outliers for A.E.S._Corp ","5c3f536f":"no2 emission sum","8702081b":"Create features for analysing **no2_emission_sum**, **total_precipitation_surface_mean** for each **day of week**","89d636f6":"<div class=\"h1\"> Reading S5p data Whether and No2 <\/div>\n<a id=\"READS5P\"><\/a>\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  \nYou can verify the data with more details here: [Prepare Data for Modeling](https:\/\/www.kaggle.com\/caesarlupum\/ds4g-go-to-the-green-future#-Satellite-Information)","ad00fbee":" \n<div class=\"h3\">Day Emission with anomalies<\/div>","0d721d0c":"Predicted data is the blue shaded region at the end.","41c024fa":"We can change the inferred changepoint range by setting the changepoint_range","ef6855f9":"<div class=\"h2\"> Glimpse of Data - Power Plants <\/div>\n<a id=\"PREPARATION\"><\/a>\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","b1d645e0":"\n<div class=\"h3\">Visualisation of anomaly throughout time<\/div>\n<a id=\"IF\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","ab9a0aa7":"The sum, facetted for A.E.S._Corp._mean aspect, shows some aberrant values, for example in month 4, April. August have the least N02 value. ","24edbd58":"# Save The Model for primary_fuel","760b13fe":"## Our modeling investigate regions, primary_fuel model  and we can decompose emission factors between plants,over time and identifying anomaly events.","16546458":"Anomaly Detection | Gaussian Distribution \u2014 [ Machine Learning | Andrew Ng ]\n","ecae6c1a":"<div class=\"h2\">Isolation Forest <\/div>\n<a id=\"OUTLIER2\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","deafe3e9":"We can view the dates where the chagepoints occurred","7677fe19":"Delivered by Sean Taylor (Facebook) at the 2018 New York R Conference at Work-Bench on April April 20 and 21","a63b96a4":" \n<div class=\"h3\">ellipticEnvelope(gaussian distribution) for each catDayEmission<\/div>\n","e5a727a6":"## In this notebook we investigated the presence of NO2 concentration in air, considering its constant increase over days, years. Owing to accurate future air quality estimates, the need for detecting the anomalously high increase in the concentration of pollutants cannot be adjourned. This study is helpful in educating the government for decision making and people about spatiotemporal, geographical, and economic conditions responsible for anomalously high NO2 concentrations in air. In this work, we modeling the solution and analyze the impacts of air pollution for each region in Porto Rico for each primary_fuel in the year.","aa6e07d3":"### Visualize the data\n","b6d6d2f4":"<div class=\"h3\">time with int to plot easily<\/div>\n","26f6a0f4":"<div class=\"h2\"> Visuals  <\/div>\n<a id=\"V1\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","ac8e38cf":"Predicted data is the blue shaded region at the end.","30de4d41":" <div class=\"h3\">plot the N02 Day Emission with anomalies<\/div>","26d00265":"\n<div class=\"h2\"> Rolling Forecast ARIMA Model<\/div>\n<a id=\"ARP2\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n","3e8a21ce":"<a class=\"anchor\" id=\"top\"><\/a>\n<a id='dsf4'><\/a>\n# <div class=\"h2\">  Table of contents<\/div>\n\n1. [Glimpse of Data](#PREPARATION)\n    * [Import packages](#IMPORT)\n2. [Reading S5p data Whether and No2](#READS5P)\n3. [Visuals](#V1)\n4. [Anomaly Analysis](#OUTLIER)\n    4.1 [Gaussian](#OUTLIER1)\n    4.2 [Isolation Forest](#OUTLIER2)\n    4.3 [One Class SVM](#OUTLIER3)\n\n5. [Prediction using LSTM with Python](#LSTM1)\n    5.1 [Get the root mean squared error (RMSE)](#LSTM1)\n\n6. [Arima with Python](#AR)\n    6.1. [Rolling Forecast ARIMA Model](#AR2)\n\n7. [Time series prediction using Prophet in Python](#PRO)\n    7.1. [Forecast quality evaluation](#PRO1)\n    7.2. [Incorporating the Effects of Weather Condition](#PRO2)\n    7.3. [Forecast quality evaluation](#PRO3)\n    7.4. [Save The Model](#PRO4)\n8. [Prediction of  NO2 density for each primary_fuel throughout the year](#M1)\n    8.1. [Forecast quality evaluation for Power Plant over the year](#M2)\n    8.3. [Outlier Analysis of Power Plant - Coal over the year](#M3)\n       \n9. [About the data](#ABOUTDATA)  \n10. [Ending note](#END)  \n\n  <hr>","1528e454":"<div class=\"h3\">Local NO2 Density<\/div>","50a95faf":"Our prediction contains historical dates with 30 days.","ed07c7fe":"Saturday NO2 emission stats.","564337cd":"Power plants on Puerto Rico","19f3c1f4":"### Creating the data set for Prophet","7b844d56":"total_precipitation_surface_mean","b31a5dcb":"<div class=\"h3\"> data info  <\/div>\n<a id=\"DF\"><\/a>\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","e074d7e2":"<hr>\n<a id='ds5'><\/a>\n# <div class=\"h2\">Don't hesitate to give your suggestions in the comment section.<\/div>\n<a id=\"theend\"><\/a>\n<a id='ds5'><\/a>\n# <div class=\"h3\">Remember the upvote button is next to the fork button, and it's free too! ;)<\/div>\n<a id=\"theend\"><\/a>","62f62a00":"<a id='ds0'><\/a>\n#  <div class=\"h1\">  DS4G: Environmental Insights Explorer \ud83c\udf0f<\/div>\n### Exploring alternatives for emissions factor calculations\n    \n    ","191553a4":"<div class='h3'>Plotting the forecasted components<\/div>\nWe can plot the trend and seasonality, components of the forecast.","39fd83a9":"<div class=\"h1\"> ARIMA with Python<\/div>\n<a id=\"ARP\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","00877627":"<div class=\"h1\">Time series prediction using Prophet in Python<\/div>\n<a id=\"PRO\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n","c6bbb514":"#### Adding ChangePoints to Prophet\nChangepoints are the datetime points where the time series have abrupt changes in the trajectory.","500ed8c0":"Thursday NO2 emission stats.\n","715c68a0":"### Outlier Analysys - day of the week with Monday=0, Sunday=6","2bd19446":"### Another Example - Verification A.E.S.Corp ","aee6ead7":"<div class=\"h1\"> Prediction using LSTM with Python<\/div>\n<a id=\"LSTM1\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","8cc18801":"The RMSE for 30 days its 0.1.778483e-11.","f1c62155":"Unsupervised Anomaly Detection with Isolation Forest - Elena Sharov","038df379":"<div class=\"h2\">Gaussian<\/div>\n\n<a id=\"OUTLIER1\"><\/a>\n\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)\n  ","5b797b75":"high NO2 mean in September","1ae11122":"AutoCorrelation 10 weeks depth\n\nThe NO2 demand seems to be driven by a weekly trend: on certain days of the week, is higher than the others. We simply prove this computing autocorrelation.","12e56dda":"NO2_column_number_density value in proximity of all plants with all locations in location mask - proximity is +\/- n points from location of plant. More information [here](https:\/\/www.kaggle.com\/tiurii\/ds4g-modelling-of-emissions-of-power-plants), @tiurii\n","c1d19db1":"Sunday NO2 emission stats.","0e4f79c6":"<div class=\"h3\">Feature engineering<\/div>\nExtracting some features\n","1a9dab3b":"Tuesday NO2 emission stats.\n","a053a5d8":"<div class=\"h1\">Prediction of  NO2 density for each primary_fuel throughout the year<\/div>\n<a id=\"M1\"><\/a>\n[Back to Table of Contents](#top)\n\n[General Findinds](#theend)"}}