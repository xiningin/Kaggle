{"cell_type":{"32d5d0a2":"code","ff2e4a29":"code","9db46009":"code","415bddd5":"code","8e6f4c92":"code","14b2b9fc":"code","3bf03f67":"code","ce652b94":"code","4e689243":"code","7fe839e2":"code","9cf64de4":"code","e543d7d0":"code","7ad50357":"code","5a747a0c":"code","c1a2bf12":"code","06b6568f":"code","bda15e7e":"code","5a18592c":"code","cecad7b7":"code","cdbcadeb":"code","bd716829":"code","c9c6e56f":"code","a63b6988":"code","962bb2c8":"code","93bbcde0":"code","e6402717":"code","e9fe9378":"code","dfcbb606":"code","e12efb31":"code","5891410f":"code","59e47a50":"code","b7ab1773":"code","26621f4b":"code","580d5240":"code","b92a6306":"code","3dfd5da9":"code","76289c27":"code","38bcb991":"code","76ef9554":"code","39e8651e":"code","848e4715":"code","7b428c06":"code","05136d55":"markdown","16227b93":"markdown","0cc4944f":"markdown","09f2f5dc":"markdown","48bd9b18":"markdown","c706c17f":"markdown","dc8e0724":"markdown","ff08fd96":"markdown","5cc18e2b":"markdown","ad5c21bb":"markdown","0dd31974":"markdown","a67f6a25":"markdown","7e4b0700":"markdown","c6bf31c8":"markdown","9a1beeef":"markdown","e9810a4f":"markdown","d569990f":"markdown","1110c5f8":"markdown","17b590bd":"markdown","9d79acd7":"markdown","7fc9d7f7":"markdown","a8ae0312":"markdown","309c2d40":"markdown","7c459eab":"markdown"},"source":{"32d5d0a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff2e4a29":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport spacy","9db46009":"train=pd.read_csv(r'\/kaggle\/input\/train.csv')","415bddd5":"train.head()","8e6f4c92":"train.shape","14b2b9fc":"nlp=spacy.load('en_core_web_sm')\nstopwords=spacy.lang.en.stop_words.STOP_WORDS #List of stop words.","3bf03f67":"train.isnull().sum()","ce652b94":"train.dropna(subset=['text'],inplace=True) # We will make our model based on the news content only,that why we are only worry about the missing data in text columns.","4e689243":"train=train[['text','label']]\ntrain.head()","7fe839e2":"doc=nlp(train.text[0]) \n#Split the whole news into chunks of words,means this is for tockenization, this instance \n#also have many functionality later on you get familiar with these all.\n#We can access everything tockens,parts of peech tag, named entities by interating over doc instance, and that the thing we will use in all Function.\n# Now i can hope you will able to understand everyting.\n","9cf64de4":"def only_alpha(text):  # Keep only alphabets,keep in mind that it will keep alphabets of any country.\n    doc=nlp(text)   \n    alpha=[w.text for w in doc if w.text.isalpha() and w not in stopwords] #W.text to access tocken\n    return ' '.join(alpha)\n\ndef count_pos_tag(text): #Returns the count of noun,proper noun and pronouns for each news.\n    doc=nlp(text)\n    tags=[w.pos_ for w in doc] # W.pos_ gives us part of speech tag for that word.\n    return tags.count('NOUN'),tags.count('PROPN'),tags.count('PRON')\n\ndef count_named_entity(text): # Returns the numbers of person,location,organisation,political groups ,mentioned in the news content.\n    doc=nlp(text)\n    entity=[w.label_ for w in doc.ents] # W.label_ tell us entity eg. person or place or organisation\n    return entity.count('PERSON'),entity.count('GPE'),entity.count('NORP'),entity.count('ORG')\n\ndef lemma(text):      #Lemmatize the word\n    doc=nlp(text)\n    lemma=[w.lemma_ for w in doc] #W.lemma_ lemmatize the word.\n    return \" \".join(lemma)\n\ndef preprocessing(data): #A combined function to all the text mining works.\n    data=pd.DataFrame(data,columns=['text'])\n    data.text=data.text.apply(only_alpha)\n    data.text=data.text.str.lower()\n    print('wooh, have Only alphabetic char ')\n    data['noun_count'],data['pnoun_count'],data['pron_count']=np.array(list(data.text.apply(count_pos_tag))).T\n    print('Tagged pos')\n    data['num_person'],data['num_places'],data['num_national_gr'],data['num_organisation']=np.array(list(data.text.apply(count_named_entity))).T\n    print('lemmatisation starts')\n    data.text=data.text.apply(lemma)\n    return data\n    \n","e543d7d0":"feature=train.text #On which we will performe text cleaning.\nlabel=train.label","7ad50357":"import timeit\n%timeit preprocessing(feature[:1])","5a747a0c":"500\/7*len(feature)\/1000\/60 #It will take approx half an hour to complete all steps.","c1a2bf12":"feature=feature.apply(only_alpha) #Removes all non-alphabetic chars","06b6568f":"# You can see all the punctuation have been removed.\nfeature[:5]","bda15e7e":"feature=feature.str.lower() #Converts to lower case.\nfeature[:5]","5a18592c":"feature=pd.DataFrame(feature)\n#Adding columns for number of noun,proper noun and pronouns.\nfeature['noun_count'],feature['pnoun_count'],feature['pron_count']=np.array(list(feature.text.apply(count_pos_tag))).T\nfeature.head()","cecad7b7":"# Adding columns for count of each entity(person,place,organisation etc.)\nfeature['num_person'],feature['num_places'],feature['num_national_gr'],feature['num_organisation']=np.array(list(feature.text.apply(count_named_entity))).T","cdbcadeb":"feature.text=feature.text.apply(lemma)#Lemmatize the words(tockens)\nfeature.head()","bd716829":"#Lets see how these all steps could be done by a single of code using preprocessing function\npreprocessing(train.text[:10])","c9c6e56f":"train=feature.join(label)\ntrain.to_csv(r'trainv2.0.csv',index=False)#keep a copy of preprocessed data","a63b6988":"train.head()","962bb2c8":"train.isnull().sum()","93bbcde0":"train.dropna(inplace=True)","e6402717":"def compare_plot(feature,ax=None):\n    Fake_mean=train.loc[train.label==1,feature].mean()\n    Real_mean=train.loc[train.label==0,feature].mean()\n    sns.barplot(x=['Fake','Real'],y=[Fake_mean,Real_mean],ax=ax)\n_,ax=plt.subplots(nrows=1,ncols=3,figsize=(15,5)) \nax=ax.flatten()\ncompare_plot('noun_count',ax[0])\ncompare_plot('pnoun_count',ax[1])\ncompare_plot('pron_count',ax[2],)\nfor end,title in enumerate(['noun','propernoun','pronoun']):\n    ax[end].set(title=title)","e9fe9378":"_,ax=plt.subplots(nrows=1,ncols=3,figsize=(15,5)) \nax=ax.flatten()\ncompare_plot('num_person',ax[0])\ncompare_plot('num_place',ax[1])\ncompare_plot('num_organisation',ax[2],)\nfor end,title in enumerate(['person','place','organisation']):\n    ax[end].set(title=title)","dfcbb606":"train.index=list(range(len(train)))\nfeature=train.drop('label',axis=1)\nlabel=train.label\nfeature.shape","e12efb31":"from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n#We, are already familiar with CountVectorizer and its working ,below is a short note on Tfidf.\n#tfidf also works ecaxtly like the count vectorixer but give more weightaage to the rare words,thats the difference.","5891410f":"tfidf_vectorizer=TfidfVectorizer(ngram_range=(1,1),stop_words='english')\nvectors=tfidf_vectorizer.fit_transform(feature.text)","59e47a50":"vectors \n# This is a sparce matrix of tfidf corpus,it contains a huge amount of data of shape approx (200680,155000),\n#so a 8gb ram unable to allocate that huge memory.We can review this to make another tfidf vector on lesser data.\n#lets review it","b7ab1773":"tfidf_vectorizer.transform(feature.text[:1]).A #we can see must of the columns having 0 values,mut definetlly not all.so the dimensality reduction becomes a import steps","26621f4b":"from sklearn.decomposition import NMF\nfrom sklearn.preprocessing import Normalizer\nnorm=Normalizer()\nnmf=NMF(n_components=50)\nvectors=norm.fit_transform(vectors)\nvectors=nmf.fit_transform(vectors)","580d5240":"vectors=pd.DataFrame(vectors)\nvectors=vectors.join(feature.drop('text',axis=1))\nvectors.head()","b92a6306":"from sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score\n\nlgb=LGBMClassifier()\nX_train,X_test,y_train,y_test=train_test_split(vectors,label)\nlgb.fit(X_train,y_train)\naccuracy_score(y_test,lgb.predict(X_test))","3dfd5da9":"from sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import StratifiedKFold","76289c27":"skf=StratifiedKFold(n_splits=5)\npipeline=make_pipeline(tfidf_vectorizer,norm,nmf)","38bcb991":"accuracy=np.array([])\nfor trainind,testind in skf.split(feature,label):\n    X_train,X_test,y_train,y_test=feature.iloc[trainind],feature.iloc[testind],label[trainind],label[testind]\n    vector_train=pipeline.fit_transform(X_train.text)\n    \n    vector_train=pd.DataFrame(vector_train)\n    vector_train.join(X_train.drop('text',axis=1))\n    \n    vector_val=pipeline.transform(X_test.text)\n    vector_val=pd.DataFrame(vector_val)\n    X_test.index=list(range(len(X_test)))\n    vector_val.join(X_test.drop('text',axis=1))\n    \n    lgb.fit(vector_train,y_train)\n    \n    y_pred=lgb.predict(vector_val)\n    accuracy=np.append(accuracy,accuracy_score(y_test,y_pred))  ","76ef9554":"accuracy","39e8651e":"training_data=vectors #As we know we stored tfidf data of whole dataset in vectors.","848e4715":"training_data.to_csv(r'trainv2.1.csv')","7b428c06":"lgb.fit(training_data,label)","05136d55":"### Preparing data","16227b93":"training=pipeline.fit_transform(feature.text)\ntraining=pd.DataFrame(training)\ntraining=training.join(feature.drop('text',axis=1))\ntraining.head()","0cc4944f":"### Comparison of Noun,proper noun and pronouns used in the title.\nwe can clearly see that the fake news contains more number of  pronouns than the proper noun(named entity).So, it can contribute in classification.","09f2f5dc":"Now we will performe 2 steps of text cleaning, Removing punctuations and converting to lower case, then based on this we will generate some important features, after that we performe rest of the steps i.e stop words removal and lemmatization.","48bd9b18":"## Vectorization\nVectorization means creating numeric representation of the documents. Lets take Coutvectorizer as example, Countvectorizer creats a list of all unique vocabulary from the entire documents(dataset),set them as columns and count the number of occurences of each word(vocab) in each row and put it as values,which represents the text of that row.![vector.png](attachment:vector.png)","c706c17f":"# Some traditional text cleaning\nIn text cleaning we will remove stop words(so,and,the etc.), Punctuations and performe Lemmatization (converts words to its root form eg.working->work) on words. For accomplisement of all these stuff first we will have to tockenize (split the document into words or sentences) the ducuments.","dc8e0724":"If notice that this validation method is not fair, as we creat tfidf vectors without separating train and test data,here data leakage have been done.now we will performe Stratified model validation and create separate tfidf for X_train and X_test. To make this easy we first make a pipeline for this whole work","ff08fd96":"### Real life news truth prediction\nrun the below cell ,and give a whole news content as input, the output will be the label of news.","5cc18e2b":"![tfidf.png](attachment:tfidf.png)","ad5c21bb":"## Dimensality reduction by NMF\nNMF decomposes multivariate data by creating a user-defined number of features. Each feature is a linear combination of the original attribute set; the coefficients of these linear combinations are non-negative.\n\nNMF decomposes a data matrix V into the product of two lower rank matrices W and H so that V is approximately equal to W times H. NMF uses an iterative procedure to modify the initial values of W and H so that the product approaches V. The procedure terminates when the approximation error converges or the specified number of iterations is reached![NMF.png](attachment:NMF.png)","0dd31974":"test=[input('enter the text of news')]\ntest=preprocessing(test)\ntest_=pd.DataFrame(pipeline.transform(test.text))\ntest_.join(test.drop('text',axis=1))\nif lgb.predict(test_)[0]==1:\n    print('prediction: News is Fake')\nelse:\n    print('prediction: news is Real')","a67f6a25":"## Kfold validation","7e4b0700":"## reading datasets","c6bf31c8":"# Importing necessary module","9a1beeef":"## Demonstration of model working.\nHere, you will get a broad idea that how this pretrained model work,so that you can easily understand the below functions.","e9810a4f":"### Loading a pretrained model for english language.\nFor tockenization we need a model so, here we will load a pretrained model from spacy, we can make our one using regrex.","d569990f":"## Bird's eye view.","1110c5f8":"doc=nlp(train.text[0])\nspacy.displacy.serve(doc,style='ent')","17b590bd":"## Performing text cleaning\nWe can do all the steps by applying above preprocessing function,but for now we will do all steps separately for better understading.Durring pipeline generation we will use that function.so keep petience and move forward.","9d79acd7":"Purpose of this notebook to share all the bassic steps to approach any nlp problem.Here we will got femiliar with these all steps with making a robust Fake news classifier.","7fc9d7f7":"If you noticed that, we left with the last processing step i.e stopwords removel, we will sone this by just specifying a argument in tfidf","a8ae0312":"## functions for the text mining.","309c2d40":"## Training a lightlgm model on full dataset","7c459eab":"## Model validation part-I"}}