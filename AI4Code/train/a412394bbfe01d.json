{"cell_type":{"af0e930c":"code","f806db2b":"code","28577cf5":"code","76f2ead0":"code","715c57f3":"code","56462d0e":"code","c03108c7":"code","9048524b":"code","1593f41e":"code","a3fa2f3a":"code","833ab46f":"code","28691cb3":"code","a8c086ef":"code","92dd6ee6":"code","691e9fbb":"code","78ded4a3":"code","b24029a6":"code","378e56e0":"code","f4d839af":"code","572fc6a4":"code","65277750":"code","0146d5cf":"code","3c008c72":"code","edcdc8bd":"code","d8e36eb6":"code","f484463d":"code","eb21e3fb":"code","eb5e0a08":"code","3d6774f3":"code","7a619d23":"code","c01d371b":"code","8c9a33fc":"code","4cf0096d":"code","199973ef":"code","cc594983":"code","eb1371f3":"code","01af240c":"code","111a5fc3":"code","2a951af6":"code","3f1bda61":"markdown","b8972927":"markdown","e4cc3f22":"markdown","6a1fcf82":"markdown","3288c41e":"markdown","6c62b3f3":"markdown","abe69799":"markdown","df7f21af":"markdown","ce765f44":"markdown","032aec01":"markdown","bfcd9581":"markdown","ed542377":"markdown"},"source":{"af0e930c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import skew,norm\nfrom scipy.stats.stats import pearsonr","f806db2b":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","28577cf5":"#save the ID column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Drop the ID column since it is unnecessary for the prediction process\ntrain.drop(\"Id\",axis =1,inplace = True)\ntest.drop(\"Id\",axis =1,inplace= True)\n","76f2ead0":"print (\"Train data: \\n\")\nprint (\"Number of columns: \" + str (train.shape[1]))\nprint (\"number of rows: \" + str (train.shape[0]))\n\nprint('\\nTest data: \\n')\nprint (\"number of columns:\" + str (test.shape[1]))\nprint (\"Number of columns:\" +  str (test.shape[0]))","715c57f3":"train.head()","56462d0e":"#descriptive statistics summary\ntrain['SalePrice'].describe()\n","c03108c7":"# kernel density plot\nsns.distplot(train.SalePrice,fit=norm);\nplt.ylabel =('Frequency')\nplt.title = ('SalePrice Distribution');\n#Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice']);\n#QQ plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()\nprint(\"skewness: %f\" % train['SalePrice'].skew())\nprint(\"kurtosis: %f\" % train ['SalePrice'].kurt())","9048524b":"#log transform the target \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Kernel Density plot\nsns.distplot(train.SalePrice,fit=norm);\nplt.ylabel=('Frequency')\nplt.title=('SalePrice distribution');\n#Get the fitted parameters used by the function\n(mu,sigma)= norm.fit(train['SalePrice']);\n#QQ plot\nfig =plt.figure()\nres =stats. probplot(train['SalePrice'], plot=plt)\nplt.show()\n","1593f41e":"print(\"skewness: %f\" % train['SalePrice'].skew())\nprint(\"kurtosis: %f\" % train ['SalePrice'].kurt())","a3fa2f3a":"#correration matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat,vmax=0.9, square=True)\nplt.show();","833ab46f":"cols = corrmat.nlargest(10, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values,\n                 xticklabels=cols.values)\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\nplt.show()","28691cb3":"var ='TotalBsmtSF'\ndata = pd.concat([train['SalePrice'],train[var]],axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim =0.800000);\nplt.show()","a8c086ef":"#scatter plot LotArea\/salePrice\nvar = 'LotArea'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x= var, y='SalePrice', ylim =(0,800000));\nplt.show();","92dd6ee6":"#scatter plot GrLivArea\/salePrice\nvar ='GrLivArea'\ndata =pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice',ylim=(0,800000));\nplt.show()","691e9fbb":"#scatter plot GarageArea\/SalePrice\nvar = 'GarageArea'\ndata =pd.concat([train['SalePrice'], train[var]], axis=1)\ndata.plot.scatter(x=var,y='SalePrice', ylim= (0,800000));\nplt.show()","78ded4a3":"#Deleting Outliers of GrLivArea\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n","b24029a6":"#box plot overallqual\/salePrice\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax =plt.subplots(figsize=(8,6))\nfig = sns.boxplot(x=var, y='SalePrice', data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.show();","378e56e0":"#year built\nvar  = 'YearBuilt'\ndata= pd.concat([train['SalePrice'], train[var]], axis =1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y='SalePrice', data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90);\nplt.show();","f4d839af":"train.head()","572fc6a4":"all_data = pd.concat((train.loc[:, 'MSSubClass': 'SaleCondition'],\n                     test.loc[:,'MSSubClass':'SaleCondition']))\nprint(\"all_data size is: {} \".format(all_data.shape))\nall_data_na = (all_data.isnull().sum()\/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending = False)\n# [:30]\nmissing_data =pd.DataFrame({'Missing Raio':all_data_na})\nmissing_data.head(20)","65277750":"for col in ('PoolQC','MiscFeature','GarageType','Alley','Fence','FireplaceQu','GarageFinish',\n           'GarageQual','GarageCond','MasVnrType','MSSubClass'):\n    all_data[col] = all_data[col].fillna('None')","0146d5cf":"#Replacing missing value with 0(since no garage = no cars in such garage)\nfor col in ('GarageYrBlt','GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\n#missing values are likely zero for no basement \nfor col in ('BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF','BsmtFullBath',\n            'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n\n#\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n#for below categorical basement-related feature NaN means that there is no basement \nfor col in ('BsmtQual', 'BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n    \n#group by Neigborhood and fill missing value with median Lot frontage of all the neighboorhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\nlambda x: x.fillna(x.median()))\n    ","3c008c72":"#msZoning classification: 'RL' is common\nall_data ['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n#functional: NA is typical\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna('Typ')\n\n#Electrical\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\n#KitchenQual\nall_data['KitchenQual'] =all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\n#Extrerior !st and Exterior 2nd\nall_data ['Exterior1st']= all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd']= all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\n#sale type\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","edcdc8bd":"#dropping as same value 'AllPub' for all records except 2NA and 1 'NoSeWa'\nall_data = all_data.drop(['Utilities'], axis=1)","d8e36eb6":"#Transforming required numerical features to categorical \nall_data['MSSubClass']= all_data['MSSubClass'].apply(str)\nall_data['OverallCond'] =all_data['OverallCond'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)\n","f484463d":"#Label Encoding some categorical variables\n#for information in their ordering set\n\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n#apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(all_data[c].values))\n    all_data[c] = lbl.transform(list(all_data[c].values))\n#shape\nprint('Shape all_data: {}'.format(all_data.shape))\n\n","eb21e3fb":"#add total surface area as TotalSf = basement + firstflr + secondflr\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n","eb5e0a08":"#log transform skewed numeric features \nnumeric_features = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_features = all_data[numeric_features].apply(lambda x : skew (x.dropna())).sort_values(ascending=False)\n#compute skewness\nprint (\"\\skew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_features})   \nskewness.head(7)                                              \n\n                                 ","3d6774f3":"skewness = skewness[abs(skewness) > 0.75]\nprint (\"There are {} skewed numerical features to box cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p \nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    all_data[feat] = boxcox1p(all_data[feat], lam)","7a619d23":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","c01d371b":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train= train.SalePrice.values\ntrain = pd.DataFrame(all_data[:ntrain])\ntest = pd.DataFrame(all_data[ntrain:])","8c9a33fc":"from sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\n","4cf0096d":"#validation function\nn_folds = 5\n\ndef RMSLE_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\",\ncv = kf))\n    return(rmse)\n","199973ef":"#lasso\nlasso = make_pipeline(RobustScaler(), Lasso(alpha = 0.0005, random_state = 1))\n\n#Gradient Boosting Regression\nGBoost = GradientBoostingRegressor(loss='huber', learning_rate=0.05, n_estimators=3000,\n                                   min_samples_split=10, min_samples_leaf=15,max_depth=4,\n                                   random_state=5,max_features='sqrt')\n\n","cc594983":"#Lasso\nscore = RMSLE_cv(lasso)\nprint (\"\\n Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(),score.std()))\n\n#Gradient Boosting Regression\nscore = RMSLE_cv(GBoost)\nprint (\"\\n GBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(),score.std()))","eb1371f3":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   \n\n\n    \n# Averaged base models score\n\naveraged_models = AveragingModels(models = (GBoost, lasso))\n\nscore = RMSLE_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","01af240c":"#defining RMSLE evaluation function\ndef RMSLE (y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n","111a5fc3":"#final training and prediction of the stacked regressor\n\naveraged_models.fit(train.values, y_train) \nstacked_train_pred = averaged_models.predict(train.values)\nstacked_pred = np.expm1(averaged_models.predict(test.values))\nprint(\"RMSLE score on the train data:\") \nprint(RMSLE(y_train,stacked_train_pred))\nprint(\"Accuracy score:\") \naveraged_models.score(train.values, y_train)","2a951af6":"ensemble = stacked_pred *1\nsubmit = pd.DataFrame()\nsubmit['id'] = test_ID\nsubmit['SalePrice'] = ensemble\nsubmit.to_csv('sample_submission.csv', index = False)\nsubmit.head()","3f1bda61":"\n#### stacking the models ","b8972927":"### Relation exploration for categorical features","e4cc3f22":"#### adding dummy categorical features\n","6a1fcf82":"#### Exploring the variables","3288c41e":"#### Linear regression Modeling\nI will be implementing\n1. Lasso Regression\n2. Gradient Boosting Regression ","6c62b3f3":"#### Box cox transformation of highly skewed features\n","abe69799":"##### Setting Mode values for missing entries","df7f21af":"### inputing missing values","ce765f44":"###### Based on feature description provide, A feature that has NA means it is absent ","032aec01":"### Inputing Missing Values","bfcd9581":"###### scores from the above models\n","ed542377":"###### averaging base model\n"}}