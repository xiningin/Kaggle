{"cell_type":{"536b7ced":"code","412a4c02":"code","241ec4fc":"code","faa9c575":"code","cde220fb":"code","40857e98":"code","d7fdd07f":"code","6994c782":"code","b288ee67":"code","dae32e5a":"code","9af3ee02":"code","ee9df4cf":"code","5a8c3fee":"code","415d04fc":"code","fdb48858":"code","7d98f49e":"code","3996b1ad":"code","7fd9ed9a":"code","0f2e45a6":"code","9cc747b6":"code","953f7098":"code","36d6658f":"code","ca20f134":"code","67cc1193":"code","a745238d":"code","75a612b5":"code","6b3d5d71":"code","bc9cb7a5":"code","b8a24121":"code","ee37eddf":"code","610890e9":"code","006daa02":"code","f9a5deca":"code","18563c93":"code","ee63f2e2":"code","111cf7b4":"markdown","f02eb6b4":"markdown","0a65ae73":"markdown","3d3c997b":"markdown","6da515c1":"markdown","7c870a40":"markdown","355c6969":"markdown","e01d70d3":"markdown","b44ad2e5":"markdown","f4d437c3":"markdown","00b9f203":"markdown","3e100e77":"markdown","00ea6530":"markdown","0fb14474":"markdown","d0d43445":"markdown"},"source":{"536b7ced":"import gc, math, pickle, datetime, os, random\nimport numpy as np \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas.plotting import register_matplotlib_converters\nregister_matplotlib_converters()","412a4c02":"def reduce_mem_usage(df, verbose=True):\n    \"\"\"\n    :param df: Dataframe with columns unprocessed so they use more memory than needed\n    \n    :returns:\n        df -> Dataframe with lower memory use\n    \"\"\"\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef clean_timestamps(df):\n    \"\"\"\n    :param df: Dataframe containing a \"timestamp\" field which will be broken down in hour, year, day,...\n    \"\"\"\n    df['timestamp'] = pd.to_datetime(df['timestamp'])\n    df[\"year\"] = df[\"timestamp\"].dt.year.astype(np.uint16)\n    df[\"month\"] = df[\"timestamp\"].dt.month.astype(np.uint8)\n    df[\"day\"] = df[\"timestamp\"].dt.day.astype(np.uint8)\n    df[\"hour\"] = df[\"timestamp\"].dt.hour.astype(np.uint8)\n    df[\"weekend\"] = df[\"timestamp\"].dt.weekday.astype(np.uint8)\n    \ndef drop_cols(df):\n    \"\"\"\n    :param df: Dataframe with unnecessary cols\n    \n    :returns:\n            df -> dataframe containing only the desired columns\n    \"\"\"\n    #drop_cols = ['timestamp','primary_use', 'site_id', 'floor_count',\"precip_depth_1_hr\", \"sea_level_pressure\", \"wind_direction\", \"wind_speed\", \"building_id\"]\n    drop_cols = ['timestamp']\n    df = df.drop(drop_cols, axis = 1)\n    return df\n\n# Predictions lower than zero are turned zero\ndef fix_predictions(y):\n    \"\"\"\n    :param y: Column with predictions\n    \"\"\"\n    y[y < 0] = 0\n\n# Predictibility in our predictions\ndef seed_everything(seed=0):\n    \"\"\"\n    :param seed: Value for seeding random functions\n    \"\"\"\n    random.seed(seed)\n    np.random.seed(seed)\n\n# Fill given categories with their average values\ndef fill_averages(df):\n    \"\"\"\n    :param df: Dataframe containing normal and nan values\n    \"\"\"\n    data_ratios = df.count()\/len(df)\n    cols = data_ratios[data_ratios < 1.0].index\n    for col in cols:\n        df[col] = df[col].fillna(-1)\n        df[col] = df[col].astype(np.int8)\n        more_zero = df[col] >= 0\n        less_zero = df[col] < 0\n        mean = df[more_zero][col].mean()\n        df.loc[less_zero, col] = mean","241ec4fc":"SEED = 5\nseed_everything(SEED)","faa9c575":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cde220fb":"#PATH = '..\/input\/ashrae-energy-prediction\/'\n#train_df = reduce_mem_usage(pd.read_csv(PATH + 'train.csv'))\n#building = reduce_mem_usage(pd.read_csv(PATH + 'building_metadata.csv'))\n#weather_train = reduce_mem_usage(pd.read_csv(PATH + 'weather_train.csv'))\n\n#train_merged = train_df.merge(building, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n#train = train_merged.merge(weather_train, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"])","40857e98":"PATH = '\/kaggle\/input\/'\ntrain = reduce_mem_usage(pd.read_csv(PATH + 'lgb-train-test\/train.csv'))\ntest = reduce_mem_usage(pd.read_csv(PATH + 'lgb-train-test\/test.csv'))","d7fdd07f":"print(train.shape)\nprint(test.shape)","6994c782":"clean_timestamps(train)","b288ee67":"plt.figure(figsize=(10, 8))\nsns.countplot(x=\"floor_count\",data=train, order = train['floor_count'].value_counts().index)\nplt.title('Floor count feature column')\nplt.tight_layout()\nplt.show()","dae32e5a":"plt.figure(figsize=(10, 8))\nax = sns.countplot(x=\"primary_use\",data=train, order = train['primary_use'].value_counts().index)\nplt.title('Buiding type count (primary_use) feature column')\nax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\nplt.show()","9af3ee02":"# A\u00f1adir leyenda con diccionario\nbuilding_id = 213\nplt.figure(figsize=(14, 8))\nax = sns.lineplot(x=\"timestamp\", y=\"meter_reading\", hue=\"meter\", data=train[train['building_id'] == building_id])\nplt.title('Meter readings from building_id {}'.format(building_id))\nplt.show()","ee9df4cf":"train = drop_cols(train)\n\ntrain_y = np.log1p(train['meter_reading'])\n\ntrain_x = train.drop('meter_reading', axis=1)\ntrain_x['primary_use'] = LabelEncoder().fit_transform(train_x['primary_use'])\n\ntrain_x.head()","5a8c3fee":"del train\n#del train_x\n#del train_y \n\ngc.collect()","415d04fc":"# Train and Validation splits\ntest_size = 0.20\nX_train, X_val, y_train, y_val = train_test_split(train_x, train_y, test_size=test_size, random_state=SEED)","fdb48858":" # lgb_params = {\n #                   'objective':'regression',\n #                   'boosting_type':'gbdt',\n #                   'metric':'rmse',\n #                   'n_jobs':-1,\n #                   'learning_rate':0.07,\n #                   'num_leaves': 2**8,\n #                   'max_depth':-1,\n #                   'tree_learner':'serial',\n #                   'colsample_bytree': 0.7,\n #                   'subsample_freq':1,\n #                   'subsample':0.5,\n #                   'n_estimators':8500,\n #                   'max_bin':255,\n #                   'verbose':1,\n #                   'seed': SEED,\n #                   'early_stopping_rounds':3500, \n #               } ","7d98f49e":"del train_x\ndel train_y\n\ngc.collect()","3996b1ad":"# load model\nprint('Loading model')\ngbm = lgb.Booster(model_file= PATH + 'lbg-model\/lgb_classifier_20-10-2019_0.20834363390357943.txt')","7fd9ed9a":"#lgb_train = lgb.Dataset(X_train, y_train)\n#lgb_eval = lgb.Dataset(X_val, y_val)\n#gbm = lgb.train(\n#            lgb_params,\n#            lgb_train,\n#            num_boost_round=5000,\n#            valid_sets=(lgb_train, lgb_eval),\n#            verbose_eval = 50\n#            )","0f2e45a6":"y_pred = gbm.predict(X_val, num_iteration=gbm.best_iteration)","9cc747b6":"fix_predictions(y_pred)","953f7098":"rmsle = np.sqrt(mean_squared_log_error(y_pred, (y_val)))\nprint('RMSLE: ', rmsle)","36d6658f":"del y_val\ndel y_train\ndel X_val\ndel X_train\n#del y_pred\n\ngc.collect()","ca20f134":"# save model\n\n#gbm.save_model('lgb_classifier_{}_{}.txt'.format(datetime.datetime.now().strftime(\"%d-%m-%Y\"), rmsle), num_iteration=gbm.best_iteration)","67cc1193":"feature_imp = pd.DataFrame(sorted(zip(gbm.feature_importance(), gbm.feature_name()),reverse = True), columns=['Value','Feature'])\nplt.figure(figsize=(10, 5))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()","a745238d":"#building = reduce_mem_usage(pd.read_csv(PATH + 'building_metadata.csv'))\n#test = reduce_mem_usage(pd.read_csv(PATH + \"test.csv\"))\n#weather_test = reduce_mem_usage(pd.read_csv(PATH + \"weather_test.csv\"))\n\n#test = test.merge(building, left_on = \"building_id\", right_on = \"building_id\", how = \"left\")\n#test = test.merge(weather_test, left_on = [\"site_id\", \"timestamp\"], right_on = [\"site_id\", \"timestamp\"], how=\"left\")\n\n#del weather_test\n#del building\ngc.collect()","75a612b5":"clean_timestamps(test)\ntest = drop_cols(test)\ntest = test.drop('row_id', axis = 1)\ntest['primary_use'] = LabelEncoder().fit_transform(test['primary_use'])","6b3d5d71":"submission = pd.read_csv(PATH+'ashrae-energy-prediction\/sample_submission.csv')","bc9cb7a5":"test_1 = test[:len(test)\/\/3]\ny_pred_1 = gbm.predict(test_1, num_iteration=gbm.best_iteration)\n\ndel test_1\n\ngc.collect()","b8a24121":"test_2 = test[len(test)\/\/3:(len(test)*2)\/\/3]\ny_pred_2 = gbm.predict(test_2, num_iteration=gbm.best_iteration)\n\ndel test_2\n\ngc.collect()","ee37eddf":"test_3 = test[(len(test)*2)\/\/3:]\ny_pred_3 = gbm.predict(test_3, num_iteration=gbm.best_iteration)\n\ndel test_3\n\ngc.collect()","610890e9":"y_pred_test = np.concatenate([y_pred_1, y_pred_2, y_pred_3], axis=0)\n\ndel y_pred_1\ndel y_pred_2\ndel y_pred_3\n\ngc.collect()","006daa02":"y_pred_test = np.expm1(y_pred_test)\nfix_predictions(y_pred_test)\nsubmission['meter_reading'] = y_pred_test\nsubmission","f9a5deca":"np.log1p(submission['meter_reading']).hist(bins=30)","18563c93":"sns.distplot(y_pred, color=\"blue\", label=\"train prediction\")\nsns.distplot(np.log1p(y_pred_test), color=\"green\", label=\"test prediction\")\nplt.legend()","ee63f2e2":"submission.to_csv('submission.csv', index=False)","111cf7b4":"### Saving the model","f02eb6b4":"## Functions","0a65ae73":"![building_picture](https:\/\/www.tetratech.com\/en\/images\/ne18-051-sustainable-design-1-650.jpg)","3d3c997b":"RMSLE error calculation using sklearn library","6da515c1":"It does not make sense to have a prediction of energy consumption lower than 0.0 KWh","7c870a40":"## Submission file","355c6969":"### Imports","e01d70d3":"## Test preparation","b44ad2e5":"### Feature importance","f4d437c3":"## LightGBM model","00b9f203":"## Train preparation","3e100e77":"You can find some commented code so people can recompile the train.csv, test.csv and their own models using the competition files. I compiled train, test and the model itself so it will produce less memory overhead without the merges or training.\n\nThis **competition** is about:\n\n* *Predicting energy consumption to improve building efficiencies to reduce costs and emissions*","00ea6530":"### Basic preprocessing","0fb14474":"### Data visualization","d0d43445":"## Introduction"}}