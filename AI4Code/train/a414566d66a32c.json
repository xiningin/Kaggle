{"cell_type":{"e7710994":"code","9be47cf8":"code","1227e464":"code","9f71d834":"code","86db3b8a":"code","4344d6ef":"code","43832abd":"code","303cda1c":"code","93b4e151":"code","e8d8262b":"code","02171b90":"code","0a87414d":"code","d226a1b3":"code","27c6fb76":"code","5e615b4e":"code","fe2c9b26":"code","1bf0efee":"code","76b06347":"code","6bbf294c":"code","7e5b21f1":"code","a1a884ab":"code","f664e372":"code","1cd24ee4":"code","665a9eac":"code","bdd57f61":"markdown","0ed73894":"markdown"},"source":{"e7710994":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9be47cf8":"base_path = \"\/kaggle\/input\/nlp-getting-started\/\"\ntrain_raw = pd.read_csv(base_path + \"train.csv\")\ntest_raw = pd.read_csv(base_path + \"test.csv\")","1227e464":"# removing emoji's\nimport emoji\ndef removeEmojis(text):\n    allchars = [c for c in text]\n    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI[\"en\"]]\n    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n    return clean_text","9f71d834":"test_string = \"Hi \ud83e\udd14 How is your \ud83d\ude48 and \ud83d\ude0c. Have a nice weekend \ud83d\udc95\ud83d\udc6d\ud83d\udc59\"\n(test_string,removeEmojis(test_string))","86db3b8a":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nprint(len(stopwords.words(\"english\")))\n\ndef removeStopwords(text):\n    text = text.lower()\n    clean_text = ' '.join([c for c in word_tokenize(text) if not any(i == c for i in stopwords.words(\"english\"))])\n    return clean_text","4344d6ef":"test_string = \"Hi \ud83e\udd14 How is your \ud83d\ude48 and \ud83d\ude0c. Have a nice weekend \ud83d\udc95\ud83d\udc6d\ud83d\udc59\".lower()\n(test_string,removeStopwords(test_string))","43832abd":"import string\n\ndef removePunctuations(text):\n    return text.translate(str.maketrans('', '', string.punctuation))","303cda1c":"test_string = \"Hi' \ud83e\udd14 How is your \ud83d\ude48 and \ud83d\ude0c. Have a nice weekend \ud83d\udc95\ud83d\udc6d\ud83d\udc59\".lower()\n(test_string,removePunctuations(test_string))","93b4e151":"import re\ndef removeNumbers(text):\n    clean_text = re.sub(r'\\d+', '', text)\n    return clean_text","e8d8262b":"test_string = \"Hi \ud83e\udd14 123How is your \ud83d\ude48 and \ud83d\ude0c. 123 Have a nice weekend \ud83d\udc95\ud83d\udc6d\ud83d\udc59\".lower()\n(test_string,removeNumbers(test_string))","02171b90":"def clean(text):\n    text = removeStopwords(text)\n    text = removePunctuations(text)\n    text = removeEmojis(text)\n    text = removeNumbers(text)\n    return text","0a87414d":"test_string = \"Hi', \ud83e\udd14 123How is your \ud83d\ude48 and \ud83d\ude0c. 123 Have a nice weekend \ud83d\udc95\ud83d\udc6d\ud83d\udc59\".lower()\n(test_string,clean(test_string))","d226a1b3":"import random\nidx = random.randint(0,len(train_raw))\n(train_raw.text[idx],clean(train_raw.text[idx]))","27c6fb76":"train_raw['clean'] = train_raw.text.apply(clean)\ntest_raw['clean'] = test_raw.text.apply(clean)","5e615b4e":"train_raw.sample(7)","fe2c9b26":"(len(train_raw),len(train_raw.drop_duplicates(subset=['clean','target'],keep='first')),\n len(train_raw.drop_duplicates(subset=['clean','target'],keep=False)))","1bf0efee":"train_clean = train_raw.drop_duplicates(subset=['clean','target'],keep='first')\nlen(train_clean)","76b06347":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ndef baselinemodel(train):\n    cv = CountVectorizer(ngram_range=(1,1))\n    doc_counts = cv.fit_transform(train['clean'])\n    \n    X_train,X_test,y_train,y_test = train_test_split(doc_counts.todense(),train[\"target\"],test_size=0.3,random_state=42)\n    \n    nb_classifier = GaussianNB()\n    nb_classifier.fit(X_train, y_train)\n    \n    y_pred = nb_classifier.predict(X_train)\n    train_accr_scr = accuracy_score(y_pred,y_train)\n    \n    validation_accr_scr = accuracy_score(nb_classifier.predict(X_test),y_test)\n    return cv,nb_classifier,train_accr_scr,validation_accr_scr\n\ndef predictions(vocab,model,test):\n    test_counts = vocab.transform(test[\"clean\"])\n    y_final_pred = model.predict(test_counts.todense())\n    test['target'] = y_final_pred\n    return test","6bbf294c":"vocab,model,train_accuracy, validation_accuracy = baselinemodel(train_clean)\nprint(model)\nprint(\"train accuracy {:04.2f}%\".format(train_accuracy*100))\nprint(\"validation accuracy {:04.2f}%\".format(validation_accuracy*100))","7e5b21f1":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\n\ndef multinomialNBModel(train):\n    cv = CountVectorizer(ngram_range=(1,2))\n    doc_counts = cv.fit_transform(train['clean'])\n    \n    X_train,X_test,y_train,y_test = train_test_split(doc_counts.todense(),train[\"target\"],test_size=0.3,random_state=42)\n    \n    nb_classifier = MultinomialNB()\n    nb_classifier.fit(X_train, y_train)\n    \n    y_pred = nb_classifier.predict(X_train)\n    train_accr_scr = accuracy_score(y_pred,y_train)\n    \n    validation_accr_scr = accuracy_score(nb_classifier.predict(X_test),y_test)\n    return cv,nb_classifier,train_accr_scr,validation_accr_scr","a1a884ab":"vocab,model,train_accuracy, validation_accuracy = multinomialNBModel(train_clean)\nprint(model)\nprint(\"train accuracy {:04.2f}%\".format(train_accuracy*100))\nprint(\"validation accuracy {:04.2f}%\".format(validation_accuracy*100))","f664e372":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\ndef lrModelWithTFIDF(train):\n    cv = TfidfVectorizer(sublinear_tf=True,max_features=60000, min_df=1, norm='l2',  ngram_range=(1,2))\n    doc_counts = cv.fit_transform(train['clean'])\n    \n    X_train,X_test,y_train,y_test = train_test_split(doc_counts.todense(),train[\"target\"],test_size=0.3,random_state=42)\n    \n    lr_classifier = LogisticRegression(max_iter=1000,C=3)\n    lr_classifier.fit(X_train, y_train)\n    \n    y_pred = lr_classifier.predict(X_train)\n    train_accr_scr = accuracy_score(y_pred,y_train)\n    \n    validation_accr_scr = accuracy_score(lr_classifier.predict(X_test),y_test)\n    return cv,lr_classifier,train_accr_scr,validation_accr_scr","1cd24ee4":"vocab,model,train_accuracy, validation_accuracy = lrModelWithTFIDF(train_clean)\nprint(model)\nprint(\"train accuracy {:04.2f}%\".format(train_accuracy*100))\nprint(\"validation accuracy {:04.2f}%\".format(validation_accuracy*100))","665a9eac":"test_raw = predictions(vocab,model,test_raw)\ntest_raw[[\"id\",\"target\"]].to_csv(\"submissions.csv\",index=False)","bdd57f61":"Preprocessing\n1. stopword removal\n2. punctuation removal\n3. remove the emojis ","0ed73894":"1. Basic Gaussian model - 61.17%\n2. Basic multinomial model - 79.14%\n3. Basic multinomial model + bigrams- 78.69%\n4. Basic multinomial model + tfidf - 79.72%\n5. Basic multinomial model + tfidf + bigrams- 78.91%\n6. Basic Logistic regression model + tfidf - 79.76%\n6. Basic Logistic regression model + tfidf + bigrams - 79.49%"}}