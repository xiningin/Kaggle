{"cell_type":{"91e796a4":"code","576413ef":"code","1b51a86f":"code","32b5dc01":"code","5441e6ae":"code","c7928cdd":"code","f88a80d3":"code","dd212cdf":"code","dd924f0c":"code","4b7048f1":"code","a458f612":"code","ac5e5e8f":"code","fb1ccff1":"code","6b69f23c":"code","01ac2921":"code","296f28f0":"code","41ee2500":"code","aeebf606":"code","9ea4ce20":"code","61da52ad":"code","a713e31c":"code","11dac1b5":"code","61745aa4":"code","783bee56":"code","ed8c7efb":"code","58d0f5f4":"code","791aaf28":"code","45bdae2c":"code","afe0f58b":"code","86e7aec1":"code","53f4924c":"code","a3760531":"code","9a148478":"code","7e1511e6":"code","4393161d":"code","c0d7e8ab":"code","e6676197":"code","476eb290":"code","70ee5e7e":"code","6235a57a":"markdown","784c7ba3":"markdown","ef1de0de":"markdown"},"source":{"91e796a4":"import tensorflow as tf\ntf.enable_eager_execution()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport tensorflow as tf\nfrom keras.utils.data_utils import get_file\nimport tarfile\n\nimport numpy as np\nimport os\nimport time","576413ef":"filename='MovieSummaries.tar.gz' #download data\npath = get_file(\n    filename,\n    origin='https:\/\/www.cs.cmu.edu\/~ark\/personas\/data\/MovieSummaries.tar.gz')\nstatinfo = os.stat(path)\nprint('Succesfully downloaded', filename, ', size: ', statinfo.st_size\/1024\/1024, 'Mbytes.')","1b51a86f":"tar = tarfile.open(path) #exctract files\ntar.extractall(path=\"movies\")\ntar.close()","32b5dc01":"dir_dat=os.path.realpath('movies')\npath=dir_dat+'\/MovieSummaries\/'\nos.listdir(path)","5441e6ae":"plots=pd.read_fwf(path+'plot_summaries.txt', delimiter='\\n', header=None) # load movie plots to pandas\nplots.columns=['movies']\nplots = plots.movies.str.split('\\t', 1, expand=True)\nplots.columns=['ID','plot']\nplots.head()","c7928cdd":"# id is object type, so convert it into numeric\nplots['ID']=plots['ID'].apply(pd.to_numeric, errors='coerce')","f88a80d3":"# load movie metadata\ntags=[ 'ID',\n'Freebase movie ID',\n'Movie name',\n'release date',\n'box office revenue',\n'runtime',\n'Movie languages',\n'Movie countries',\n'genres']\nmetadat=pd.read_csv(path+'movie.metadata.tsv', sep='\\t', lineterminator='\\n', names=tags)\nmov_names=metadat[['ID', 'Movie name', 'genres']]\nplot_genre=pd.merge(plots, mov_names, how='left', on='ID') # merge both tables on ID\nprint(plot_genre.shape)\nplot_genre.head()","dd212cdf":"plot_genre.dropna(inplace=True)","dd924f0c":"import re # regex for cleaning the genre strings\ndef clean_genres(line):\n    #print(line)\n    a=re.sub('[\"\"{}]', '', line)\n    b=re.split(': |,',a)\n    new=[]\n    [new.append(b[i]) for i in range(len(b))   if i % 2 != 0]\n    str1=''\n    str1 = ','.join(str(e) for e in new)\n    return str1","4b7048f1":"plot_genre['genres']=plot_genre['genres'].apply(lambda x: clean_genres(x))","a458f612":"import re\nimport string\nfrom string import digits\nplot_genre['plot']=plot_genre['plot'].apply(lambda x: re.sub(r\"[^a-zA-Z,.]\",' ', x)).apply(lambda x: re.sub(\",\", ' , ', x)).apply(lambda x: re.sub(\"\\.\", ' . ', x))\nplot_genre['genres']=plot_genre['genres'].apply(lambda x: re.sub(r\"[^a-zA-Z,.]\",' ', x)).apply(lambda x: re.sub(\",\", ' , ', x)).apply(lambda x: re.sub(\"\\.\", ' . ', x)) \nremove_digits = str.maketrans('', '', digits)\nplot_genre['plot']=plot_genre['plot'].apply(lambda x: x.translate(remove_digits)).apply(lambda x: x.lower())\nplot_genre['genres']=plot_genre['genres'].apply(lambda x: x.translate(remove_digits)).apply(lambda x: x.lower())\nplot_genre['plot']=plot_genre['plot'].apply(lambda x: re.sub('\\'',' \\' ', x))\ndel plots, metadat,mov_names\nprint(plot_genre.shape)\nplot_genre.head()","ac5e5e8f":"plot1=\"family film|thriller|comedy\"\nplots=plot_genre[plot_genre['genres'].str.contains(plot1)]['plot']\nprint('Number of movies with genres: ',plot1, plots.shape)\n\nsub=plots.sample(n=20000, random_state=14)\nmask = (sub.str.len() <= 1000)\nnot_huge_plots = sub.loc[mask]\nprint('Number of movies with less than 1000 chars in plots: ',(not_huge_plots).shape)","fb1ccff1":"#not_huge_plots.to_csv('plots.txt', sep='\\t', encoding='utf-8', index=False, header=False)","6b69f23c":"#os.listdir('..\/working')\nstrings=not_huge_plots.values.T.tolist()\nplotz = ''.join(str(e) for e in strings)","01ac2921":"print ('Length of text: {} characters'.format(len(plotz)))","296f28f0":"all_plots=set() #create vocabulary of unique words\n\nfor txt in plotz.split():\n    for word in txt.split():\n        if word not in all_plots:\n            all_plots.add(word)","41ee2500":"len(all_plots) # number of unique words in plots","aeebf606":"vocab = sorted(list(all_plots))\nword2idx = dict(\n    [(word, i) for i, word in enumerate(vocab)])","9ea4ce20":"for word,_ in zip(word2idx, range(20)):\n    print('{:6s} ---> {:4d}'.format(repr(word), word2idx[word]))","61da52ad":"idx2word = np.array(vocab)\n\ntext_as_int = np.array([word2idx[c] for c in plotz.split()])","a713e31c":"print ('{} ---- characters mapped to int ---- > {}'.format(plotz.split()[:10], text_as_int[:10]))","11dac1b5":"# The maximum length sentence we want for a single input in characters\nseq_length = 100\n\n# Create training examples \/ targets\nchunks = tf.data.Dataset.from_tensor_slices(text_as_int).batch(seq_length+1, drop_remainder=True)\n\nfor item in chunks.take(2):\n  print(repr(' '.join(idx2word[item.numpy()])))\n","61745aa4":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = chunks.map(split_input_target)","783bee56":"for input_example, target_example in  dataset.take(1):\n  print ('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n  print ('Target data:', repr(' '.join(idx2word[target_example.numpy()])))","ed8c7efb":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2word[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2word[target_idx])))","58d0f5f4":"BATCH_SIZE = 64\nBUFFER_SIZE = 10000","791aaf28":"dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","45bdae2c":"class Model(tf.keras.Model):\n  def __init__(self, vocab_size, embedding_dim, units):\n    super(Model, self).__init__()\n    self.units = units\n\n    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n\n    if tf.test.is_gpu_available():\n      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n                                          return_sequences=True, \n                                          recurrent_initializer='glorot_uniform',\n                                          stateful=True)\n    else:\n      self.gru = tf.keras.layers.GRU(self.units, \n                                     return_sequences=True, \n                                     recurrent_activation='sigmoid', \n                                     recurrent_initializer='glorot_uniform', \n                                     stateful=True)\n\n    self.fc = tf.keras.layers.Dense(vocab_size)\n        \n  def call(self, x):\n    embedding = self.embedding(x)\n    \n    # output at every time step\n    # output shape == (batch_size, seq_length, hidden_size) \n    output = self.gru(embedding)\n    \n    # The dense layer will output predictions for every time_steps(seq_length)\n    # output shape after the dense layer == (seq_length * batch_size, vocab_size)\n    prediction = self.fc(output)\n    \n    # states will be used to pass at every step to the model while training\n    return prediction","afe0f58b":"vocab_size = len(vocab)\n\n# The embedding dimension \nembedding_dim = 256\n\n# Number of RNN units\nunits = 512\n\nmodel = Model(vocab_size, embedding_dim, units)","86e7aec1":"# Using adam optimizer with default arguments\noptimizer = tf.train.AdamOptimizer()\n\n# Using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\ndef loss_function(real, preds):\n    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)","53f4924c":"model.build(tf.TensorShape([BATCH_SIZE, seq_length]))\nmodel.summary()","a3760531":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")","9a148478":"EPOCHS=300\n# Training loop\nfor epoch in range(EPOCHS):\n    start = time.time()\n    \n    # initializing the hidden state at the start of every epoch\n    # initally hidden is None\n    hidden = model.reset_states()\n    \n    for (batch, (inp, target)) in enumerate(dataset):\n          with tf.GradientTape() as tape:\n              # feeding the hidden state back into the model\n              # This is the interesting step\n              predictions = model(inp)\n              loss = loss_function(target, predictions)\n              \n          grads = tape.gradient(loss, model.variables)\n          optimizer.apply_gradients(zip(grads, model.variables))\n\n          #if (epoch+1) % 2 == 0:\n          #    print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch,loss))\n    # saving (checkpoint) the model every 5 epochs\n    if (epoch + 1) % 10 == 0:\n      model.save_weights(checkpoint_prefix)\n      print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n      print ('Time taken for 10 epoch {} sec\\n'.format(time.time() - start))","7e1511e6":"model.save_weights(checkpoint_prefix)","4393161d":"model = Model(vocab_size, embedding_dim, units)\n\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n\nmodel.build(tf.TensorShape([1, None]))","c0d7e8ab":"num_generate = 400\n\n# You can change the start string to experiment\nstart_string = 'now'\n\n# Converting our start string to numbers (vectorizing) \ninput_eval = [word2idx[s] for s in start_string]\ninput_eval = tf.expand_dims(input_eval, 0)\n\n# Empty string to store our results\ntext_generated = []\n\n# Low temperatures results in more predictable text.\n# Higher temperatures results in more surprising text.\n# Experiment to find the best setting.\ntemperature = 1.0","e6676197":" #Evaluation loop.\n\n# Here batch size == 1\nmodel.reset_states()\nfor i in range(num_generate):\n    predictions = model(input_eval)\n    # remove the batch dimension\n    predictions = tf.squeeze(predictions, 0)\n\n    # using a multinomial distribution to predict the word returned by the model\n    predictions = predictions \/ temperature\n    predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n    \n    # We pass the predicted word as the next input to the model\n    # along with the previous hidden state\n    input_eval = tf.expand_dims([predicted_id], 0)\n    \n    text_generated.append(idx2word[predicted_id])\n\n","476eb290":"# make the text readable by capitalizing each sentence\ngenerated=' '.join(text_generated)\nnew=re.sub(\" ,\", ',', generated)\nnew=re.sub(\" \\.\", '.', new)\nsentence=new.split('. ')\nfor i, line in enumerate(sentence):\n    sentence[i]=line.capitalize()\nprint('. '.join(sentence))","70ee5e7e":"import shutil\nshutil.rmtree(dir_dat)","6235a57a":"# word level, no encoder decoder using tf example","784c7ba3":"# to be continued","ef1de0de":"# Data preprocessing"}}