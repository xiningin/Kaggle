{"cell_type":{"6bc9ae73":"code","18eda97a":"code","09b12ab9":"code","85362544":"code","b05f1899":"code","d2ec8733":"code","4ae61bfd":"code","37639cf3":"code","54f974b5":"code","3ada9e4f":"code","3c16c180":"code","0abf0119":"code","fb2173eb":"code","645e4452":"code","de615821":"code","e742b162":"code","4e3311b6":"code","47665e17":"code","3b5ab1d1":"code","382b0ffb":"code","81de6082":"code","d44c9fc0":"code","fbc56b26":"code","e8b8de1d":"markdown","3b90062c":"markdown","35db3572":"markdown","9ca137cc":"markdown","4ae90d72":"markdown","f790d547":"markdown","70cbd5c8":"markdown","121361a7":"markdown","0dfa8837":"markdown"},"source":{"6bc9ae73":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport datetime as dt\n\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n","18eda97a":"# read the dataset\ncc_df = pd.read_csv(\"..\/input\/ccdata\/CC GENERAL.csv\", sep=\",\", encoding=\"ISO-8859-1\", header=0)\ncc_df.head()","09b12ab9":"cc_df.shape\n","85362544":"cc_df.info()","b05f1899":"cc_df = cc_df.dropna()\ncc_df.shape","d2ec8733":"sns.boxplot(cc_df['BALANCE'])\nplt.show()","4ae61bfd":"sns.boxplot(cc_df['PURCHASES'])\nplt.show()","37639cf3":"sns.boxplot(cc_df['INSTALLMENTS_PURCHASES'])\nplt.show()","54f974b5":"sns.boxplot(cc_df['CREDIT_LIMIT'])\nplt.show()","3ada9e4f":"sns.boxplot(cc_df['MINIMUM_PAYMENTS'])\nplt.show()","3c16c180":"sns.boxplot(cc_df['PURCHASES_FREQUENCY'])\nplt.show()","0abf0119":"cc_df.describe()","fb2173eb":"from sklearn.neighbors import NearestNeighbors\nfrom random import sample\nfrom numpy.random import uniform\nimport numpy as np\nfrom math import isnan\n \ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) # heuristic from article [1]\n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H\n","645e4452":"hopkins(cc_df.drop('CUST_ID',axis=1))","de615821":"#DRopping the unwanted features for the clustering\ncc_df_data=cc_df.drop(['CUST_ID','BALANCE_FREQUENCY','PURCHASES_FREQUENCY','INSTALLMENTS_PURCHASES','ONEOFF_PURCHASES_FREQUENCY','CASH_ADVANCE','PURCHASES_INSTALLMENTS_FREQUENCY','CASH_ADVANCE_TRX','TENURE','PURCHASES_TRX','CASH_ADVANCE_FREQUENCY','PRC_FULL_PAYMENT'],axis=1)\ncc_df_data.head()","e742b162":"scaler = StandardScaler()\n\n# fit_transform\ncc_df_scaled = scaler.fit_transform(cc_df_data)\ncc_df_scaled.shape","4e3311b6":"cc_df_scaled = pd.DataFrame(cc_df_scaled)\ncc_df_scaled.columns = cc_df_data.columns\ncc_df_scaled.head()","47665e17":"# elbow-curve\/SSD\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(cc_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\n# ssd\nplt.plot(ssd)","3b5ab1d1":"# silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(cc_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(cc_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n    \n    ","382b0ffb":"# final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50,random_state=50)\nkmeans.fit(cc_df_scaled)","81de6082":"# assign the label\ncc_df['cluster_id'] = kmeans.labels_\ncc_df.head()","d44c9fc0":"#CLuster Profiling\ncc_df[['cluster_id','PURCHASES','BALANCE','ONEOFF_PURCHASES','CREDIT_LIMIT']].groupby('cluster_id').mean().plot(kind='bar')\nplt.show()","fbc56b26":"#plot data with seaborn\nsns.scatterplot(x = 'CREDIT_LIMIT', y = 'PURCHASES', hue = 'cluster_id', data = cc_df, palette = 'Set1')\nplt.show()","e8b8de1d":"- cluster2 customers uses credit card more \n- cluster0 customers uses credit card very less\n- cluster1 customers uses credit card more than cluster0 but less than cluster 1","3b90062c":"## Scaling the Data","35db3572":"**Data set can be clustered as the hopkins score is 90 percentage on the average**","9ca137cc":"## Optimal Value of K","4ae90d72":"Here in this case study it's better not to handle outliers because we want to analyse all types of customers so better not to handle them","f790d547":"**Problem Statement**\n\nThis data set has details about customers credit cards and we are going to cluster the customers data based on customer characterstics.","70cbd5c8":"## Hopkin Score","121361a7":"## Outlier Handling","0dfa8837":"## EDA"}}