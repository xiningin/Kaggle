{"cell_type":{"6d07518b":"code","69c59630":"code","7a89ec38":"code","d0405fd6":"code","bb20eb09":"code","39c3c31f":"code","d6cc7a9a":"code","3eed2430":"code","fca7b75e":"code","f0d4a2fa":"code","a18224ec":"code","f4745b1d":"code","1def0e3f":"code","b22673bf":"code","22965648":"code","5b554fe1":"code","efb6004a":"code","62eacadd":"code","370ab475":"code","116ae812":"code","78a9491f":"code","8694b178":"code","577964bd":"code","cac49a47":"code","d2bbb244":"code","1bb201fa":"code","65667b2d":"code","5c45116d":"code","c9bc60b3":"code","0bb64704":"code","09f448d9":"code","183558ec":"code","759eb3bb":"code","7bdcf6f6":"code","b41f6a25":"code","7df25b9c":"code","29a71a36":"code","4f69651e":"code","31692a39":"code","feef8da5":"code","bee29539":"markdown","2cfa0328":"markdown","acdab5be":"markdown","ae3e488f":"markdown","4c95b9e0":"markdown","4085cef6":"markdown","210dbfa9":"markdown","00f11706":"markdown","d0c1612d":"markdown","b5d4db84":"markdown","fdf33ba2":"markdown","ee691d28":"markdown","f81ea18c":"markdown","0dcfb3c9":"markdown","337b1940":"markdown","0791f40d":"markdown","93cec68b":"markdown","fafa332c":"markdown","6e7e876e":"markdown","3a84fe91":"markdown","0c17e019":"markdown","2e623131":"markdown","337fc82a":"markdown","ab56df5a":"markdown","bdbde0a1":"markdown","fe65e294":"markdown","e8c37e4c":"markdown"},"source":{"6d07518b":"# importing necessary dependencies \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport time\n\n%matplotlib inline\n%xmode plain\n\nsns.set()","69c59630":"# Loading both test and train data\ndf = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","7a89ec38":"df.shape ,df_test.shape","d0405fd6":"df.columns","bb20eb09":"df.head()","39c3c31f":"df.tail()","d6cc7a9a":"fig = plt.figure(figsize=(16,9))\n\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222, sharey=ax1)\nax3 = fig.add_subplot(223, sharey=ax1)\nax4 = fig.add_subplot(224, sharey=ax1)\nax1.scatter(df['LandContour'], df['SalePrice'])\nax1.set_xlabel('LandContour')\nax2.scatter(df['OverallCond'], df['SalePrice'])\nax2.set_xlabel('OverallCond')\nax3.scatter(df['YearBuilt'], df['SalePrice'])\nax3.set_xlabel('YearBuilt')\nax4.scatter(df['GrLivArea'], df['SalePrice'])\nax4.set_xlabel('GrLivArea')","3eed2430":"fig = plt.figure(figsize=(16,9))\n\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222, sharey=ax1)\nax3 = fig.add_subplot(223, sharey=ax1)\nax4 = fig.add_subplot(224, sharey=ax1)\nax1.scatter(df['BedroomAbvGr'], df['SalePrice'])\nax1.set_xlabel('BedroomAbvGr')\nax2.scatter(df['LotArea'], df['SalePrice'])\nax2.set_xlabel('LotArea')\nax3.scatter(df['GarageArea'], df['SalePrice'])\nax3.set_xlabel('GarageArea')\nax4.scatter(df['1stFlrSF'], df['SalePrice'])\nax4.set_xlabel('1stFlrSF')","fca7b75e":"# Plotting correlation heatmap with the help of Seaborn\n_, figcorr = plt.subplots(figsize = (9,9))\nsns.heatmap(df.corr(), ax = figcorr, center = 0)","f0d4a2fa":"k = 10\ncorr = df.corr()\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df[cols].values.T)\n\nsns.set(font_scale=1.25)\n\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}\n                , yticklabels=cols.values, xticklabels=cols.values)\n","a18224ec":"sns.pairplot(df[cols[:]], size=2.5)","f4745b1d":"# we combine the two data frames for the train and test data\n# we split them later again, this makes the data preprocessing easier\ny_train = df['SalePrice'].values\ndf = df.drop(columns=['SalePrice'])\ndfc = pd.concat((df, df_test))\ndfc.head()","1def0e3f":"total = dfc.isnull().sum().sort_values(ascending=False)\npercent = (dfc.isnull().sum()\/dfc.isnull().count()).sort_values(ascending=False)\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\nmissing_data.head(20)","b22673bf":"dfc = dfc.drop(columns=missing_data[missing_data['Percent'] > 0.1 ].index.values, axis=1)","22965648":"dfc.head()","5b554fe1":"dfc = dfc.fillna(method='bfill')","efb6004a":"cols","62eacadd":"ids = dfc['Id']\ndfc = dfc.drop(columns = ['Id'])\ndfc = pd.get_dummies(dfc)","370ab475":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\ndfc = pd.DataFrame(np.log1p(dfc.values), columns=dfc.columns )\n# dfc = pd.DataFrame(sc.fit_transform(dfc.values), columns=dfc.columns )","116ae812":"x_train = dfc.iloc[0:df.shape[0], :].values\nx_validation = dfc.iloc[df.shape[0]:, :].values\n\nx_train, x_test , y_train, y_test = train_test_split(x_train, y_train,\n                                                         test_size = 0.30,\n                                                        random_state = 45\n                                                        )\nprint(x_train.shape, x_validation.shape)","78a9491f":"y_train = np.log1p(y_train.reshape(-1,1))\ny_test = np.log1p(y_test.reshape(-1,1))","8694b178":"# importing dependencies\nimport tensorflow.keras as keras\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom eli5.sklearn import PermutationImportance\nimport eli5\nimport os\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'","577964bd":"# Defining Neural Network Model Structure\n\ndef make_model():\n    \n    model = keras.models.Sequential()\n\n    model.add(keras.layers.Dense(units = 64 ,activation='relu', input_shape=x_train.shape[1:]))\n    model.add(keras.layers.BatchNormalization())\n    \n    model.add(keras.layers.Dense(units = 32, activation='relu'))\n    model.add(keras.layers.BatchNormalization())\n\n    model.add(keras.layers.Dense(units = 16, activation='relu'))\n    model.add(keras.layers.BatchNormalization())\n\n    model.add(keras.layers.Dense(units = 1))\n    \n    optim = keras.optimizers.SGD(lr = 0.001)\n    model.compile(loss='mse', optimizer=optim, metrics=['mse'])\n    \n    return model\n","cac49a47":"callbacks = [keras.callbacks.EarlyStopping(monitor = 'val_loss',\n                                  min_delta = 0,\n                                  patience = 5,\n                                  verbose = 0, \n                                  mode='auto')]\n\nmy_model = KerasRegressor(build_fn=make_model, verbose=0)\nmy_model.fit(x_train, y_train,  epochs = 1000 , verbose=0, validation_data=(x_test, y_test), callbacks = callbacks)","d2bbb244":"perm = PermutationImportance(my_model, random_state=1).fit(x_train,y_train, verbose=0)\neli5.show_weights(perm, feature_names = dfc.columns.tolist())","1bb201fa":"# sort the sale prices in ascending order and plot their respective predictions\n\ny_pred = my_model.predict(x_test)\n\nargs = np.argsort(y_test.squeeze(1))\n\nout  = [y_test.squeeze(1)[i] for i in args]\ny_out = [y_pred[i] for i in args]\n\nplt.plot(np.arange(len(args)), y_out, label='prediction')\nplt.plot(np.arange(len(args)), out,  label='sale price')\nplt.legend(loc='upper left')","65667b2d":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import plot_importance\n\n\nxgb = XGBRegressor(n=1000)\nxgb.fit(x_train, y_train, verbose=False)","5c45116d":"# sort the sale prices in ascending order and plot their respective predictions\n\ny_pred = xgb.predict(x_test)\n\nargs = np.argsort(y_test.squeeze(1))\n\nout  = [y_test.squeeze(1)[i] for i in args]\ny_out = [y_pred[i] for i in args]\n\nplt.plot(np.arange(len(args)), y_out, label='prediction')\nplt.plot(np.arange(len(args)), out,  label='sale price')\nplt.legend(loc='upper left')","c9bc60b3":"plot_importance(xgb, max_num_features=5)","0bb64704":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge, RidgeCV,  LassoCV","09f448d9":"lg = LinearRegression()\nlg.fit(x_train, y_train)","183558ec":"# sort the sale prices in ascending order and plot their respective predictions\n\ny_pred = lg.predict(x_test)\nargs = np.argsort(y_test.squeeze(1))\nout  = [y_test.squeeze(1)[i] for i in args]\ny_out = [y_pred[i] for i in args]\n\nplt.plot(np.arange(len(args)), y_out, label='prediction')\nplt.plot(np.arange(len(args)), out,  label='sale price')\nplt.legend(loc='upper left')","759eb3bb":"maxcoef = np.argsort(-np.abs(lg.coef_)).squeeze()\n\ncoef = lg.coef_[0][maxcoef-1]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(dfc.columns[maxcoef[i]], coef[i]))","7bdcf6f6":"# Fitting a Lasso Regression Model\n\nls = LassoCV()\nls.fit(x_train, y_train.squeeze())","b41f6a25":"# sort the sale prices in ascending order and plot their respective predictions\n\ny_pred = ls.predict(x_test)\nargs = np.argsort(y_test.squeeze(1))\nout  = [y_test.squeeze(1)[i] for i in args]\ny_out = [y_pred[i] for i in args]\n\nplt.plot(np.arange(len(args)), y_out, label='prediction')\nplt.plot(np.arange(len(args)), out,  label='sale price')\nplt.legend(loc='upper left')","7df25b9c":"maxcoef = np.argsort(-np.abs(ls.coef_))\ncoef = ls.coef_[maxcoef]\n\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(dfc.columns[maxcoef[i]], coef[i]))\n","29a71a36":"# fiting a Ridge Regression Model\n\nrg = RidgeCV()\nrg.fit(x_train, y_train)","4f69651e":"# sort the sale prices in ascending order and plot their respective predictions\n\ny_pred = rg.predict(x_test)\nargs = np.argsort(y_test.squeeze(1))\nout  = [y_test.squeeze(1)[i] for i in args]\ny_out = [y_pred[i] for i in args]\n\nplt.plot(np.arange(len(args)), y_out, label='prediction')\nplt.plot(np.arange(len(args)), out,  label='sale price')\nplt.legend(loc='upper left')","31692a39":"maxcoef = np.argsort(-np.abs(rg.coef_).squeeze())\ncoef = rg.coef_.squeeze()[maxcoef]\nfor i in range(0, 5):\n    print(\"{:.<025} {:< 010.4e}\".format(dfc.columns[maxcoef[i]], coef[i]))","feef8da5":"'''we're exponentiating the prediction to reverse the log transformation applied\n to the target during training'''\ny_pred_ls = np.exp(ls.predict(x_validation)) \ny_pred_lg = np.exp(rg.predict(x_validation).squeeze())\nprint(y_pred_lg)\ny_pred_ave = (y_pred_ls + y_pred_lg)\/2\n\nsubmit = pd.DataFrame({'Id': ids[df.shape[0]:].values, 'SalePrice':y_pred_ave})\nsubmit.to_csv('submission'+str(time.time())+'.csv', index=False)\nsubmit.head(10)\n","bee29539":"From the above we can see which features are highly correlated with the sale price most of which we had already discovered. Further, these to features align with our intuition as to what would\nmake the sale price of a house cheaper or expensive.\n\nThis is very interesting because from the full heatmap a lot of the features don't see to contribute to the sale price. \n\nLet's further investigate the map the pairwise correlations of these variables.","2cfa0328":"## Task: Predict sales prices\n\nOur data has about 80 Features. Because we can't look at all of them invidividually in relation to Sale Price. I have picked a few that I think would contribute heavily to the sale price:\n\n\u2022 LandContour\n\n\u2022 Overall Cond\n\n\u2022 Year Built\n\n\u2022 GarageArea\n\n\u2022 Lot Area\n\n\u2022 1st Floor Area\n\n\u2022 LivingArea\n\nI chose these features based on my intuition although we can confirm this later with our models.","acdab5be":"What we need to do next is apply the log transformation to our features so as to fix any skewness in the data that might affect our models.\n\nMore information on this can be found [here](http:\/\/onlinestatbook.com\/2\/transformations\/log.html).","ae3e488f":"Oh one last thing. Let's check how our target variable is distributed","4c95b9e0":"## Kaggle : Predicting the House Sale Prices using Regression Techniques\n\n``Sandile Shongwe``\n\n\n\"Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\"\n\nIn this notebook we are going to try and find out what factors contribute the most to the sale price of a house.\n\nThis work in this notebook has been largely motivated by the following notebooks:\n\n1. [Comprehensive data exploration with Python - Pedro Marcelino](https:\/\/www.kaggle.com\/dansbecker\/xgboost)\n\n2. [Simple linear regression models - Ricio Ventura](https:\/\/www.kaggle.com\/rbyron\/simple-linear-regression-models)\n\n3. [Stacked Regressions : Top 4% on LeaderBoard - Seringe](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard)\n\nThe notebook is divided into two main sections:\n1. Exploration of the data\n2. Implementation of Regression based Machine Learning models\n\nEnjoy... =)\n","4085cef6":"#### 3. Regression Techniques","210dbfa9":"From the plots above we can see that ``Land Contour`` is not showing a clear relationship with the say prices. But we can kind of see that Level land has a lot of the houses that are highly priced.\n\nFurther the overall conditiion of the house is also not a clear indicator of the priec but we can kind of see that houses in better condition tend to sell at a higher. However, surprisingly we see that houses with a overall quality of 5 has has sold the most houses and some of the most expensive house have come from there.\n\nWe also note that the pricing tends to a have expontential relation with the year in which the house was build. \n\nLastly, it is apparent from the 4th plot above that the sale price and the above size of the ground level living area have a liear relationship.\n\nLet's look at a few more variables to get a better view of the data.","00f11706":"#### Conclusion \n\nFrom this notebook, we can conclude that Regression with regularization performs better at estimating the price of a house given how our data has been preprocessed.\n\nIn terms of ranking the most notable ranking achieve were the following: <br\/>\n\u2022 Top 57% with XGBoost <br\/>\n\u2022 Top 53% with Lasso Regression <br\/>\n\u2022 Top 39% with the average predictions from Lasso Regression and Ridge Regression.\n\n","d0c1612d":"According to XGBoost the following features matter the most:\n    \n\u2022 TotalBsmtSF F11 <br\/>\n\u2022 LotArea F2 <br\/>\n\u2022 MasVnrArea F7 <br\/>\n\u2022 MSSubClass F2 <br\/>\n\u2022 LowQualFinSF F14 <br\/>\n\nXGBoost seems to be doing a much better job at the prediction than the neural network.","b5d4db84":"We see from more of the data that, the more area a lot, garage and the first floor has the more that house is going to cost.\n\nThe number of rooms don't necessarily have a relationship with the price. This makes sense because one house might have 2 bedrooms which are  equivalent in terms of total to the area of fours bedrooms in another house combined. \n\nTo make things bit simpler. Let's plot a correlation heatmap to have a broader view into the data.","fdf33ba2":"Now let's try to find outliers of the 10 features that are most correlated to Sale Price. Like before let's get a visual representation of this.","ee691d28":"The view is a lot clearer now. Some points worth noting:\n\n\u2666 There is a large number of variables that seem to have linear relationships\n\n\u2666 For instance the size of the first floor in Square feet and the ground living area.\n\n\u2666 The Sqaure feet of the first floor and the total basement square feet\n\nI could go on and on... it is just easier to look at the plots.\n\n\nWhat we're are going to do next is to do some feature engineering in preparation for implementing our\nmodels.","f81ea18c":"#### Data Preprocessing\n\nFirstly we are going to find out which features have the most NaNs","0dcfb3c9":"It is common practise to split data into the training \n    and testing sets. So let's do just that.","337b1940":"#### 2. XGBoost","0791f40d":"Let's drop the features that have more that 10% of their data missing.\n\nBut aren't we going to miss any information in doing so?\nWell... These features don't really contribute much to the sale price anyway. So I don't believe\nthat dropping them would severely impact our predictions.\n\nWe are going to do with the rest a back forward to cover up the NaNs. Although this process doesn't necessarily fill the NaNs with the correct values in practise it tends to help model learn better as opposed to dropping the entries with NaNs.","93cec68b":"Firstly, we are going to import and have a look at the data.","fafa332c":"Permutation importance measures the importance of a feature by consdering the score of the model in the absence of the feature\nhttps:\/\/eli5.readthedocs.io\/en\/latest\/autodocs\/sklearn.html#eli5.sklearn.permutation_importance.PermutationImportance\nhttps:\/\/stackoverflow.com\/questions\/45361559\/feature-importance-chart-in-neural-network-using-keras-in-python\n","6e7e876e":"## Implementing the models\n\nWe are going to implement 3 main techniques to estimate the sale of prices namely:\n1. Neural Net using keras\n2. Gradient Boosting technique using XGBoost\n3. Linear Regression with and without regularization\n\n#### 1. Neural Network using Keras","3a84fe91":"For the regression methods we see that:<br\/>\n\u2022 standard linear regression fails as we can clearly see that for\nsome instance it severely over-estimates and for some instance it severely underestimates.<br\/>\n\u2022 Lasso Regression and Ridge Regression seem to be doing a much better job at estimating and their performance appears to be very similar.<br\/>\n\u2022 Lasso Regression and Ridge Regression are performing better than XGBoost and The neural net <br>\n\u2022 The two latter regression methods have the following features to be of importance:<br\/>\n      \n        \u25cb GrLivArea \n        \u25cb OverallQual \n        \u25cb OverallCond \nThis kind of makes sense because overall quality and condition factors in a lot of other variables.\n","0c17e019":"We see that our neural network consider considers the following features to be important:<br\/>\n\u2022 2ndFlrSF <br\/>\n\u2022 MasVnrArea <br\/>\n\u2022 BsmtFinSF1 <br\/>\n\u2022 OpenPorchSF <br\/>\n\u2022 BsmtUnfSF <br\/>\n\nThe features don't really align with what the human considers when pricing a house. As can be seen by the graph below. The neural network is doing a poor job a predict the house prices.","2e623131":"While taking a look at the pairplots for Sale Price from just a few cells above( I did note show these again for the sake of brevity), there are a few values that may seem like outliers but in reality they don't seem that far from the rest of the points. For now, I am not going to drop them. If our model performs poorly. We may have a look into removing them.\n\nThe next step here is to find dummy variables for our categorical data.","337fc82a":"Looking at this somewhat overwhelming heatmap of the correlations we can see that as stated earlier variables such as Overall Quality, the size of the living area at Ground level and garage area have the highest correlation with the Sale Price of the house.\n\nLet's look closer at the features that a meaningful correlation with the Sale Price.","ab56df5a":"We also need to transform our target variable to make sure that is not skewed and \nmore robust to outliers. Typically a log is used to do this.","bdbde0a1":"next ...!","fe65e294":"#### Submission","e8c37e4c":"#### First we going to look at LandContour"}}