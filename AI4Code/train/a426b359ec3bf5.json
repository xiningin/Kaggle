{"cell_type":{"3d41288d":"code","bf3b64e3":"code","a4c42d38":"code","6068f929":"code","e29d1ba6":"code","3fca7673":"code","9a578ddb":"code","c55c67c9":"code","a868fb0a":"code","79cc9f8a":"code","bfa472a0":"code","210154b4":"code","b9f76153":"code","540d17ab":"code","2e9a0b14":"code","c396211a":"code","47507a74":"code","6a3a7102":"code","abf59cc0":"code","61b02b5f":"code","b176c3f7":"code","fa32155c":"code","80438dd4":"code","a4cf4651":"code","216b10eb":"code","07b9d333":"code","f368c24a":"code","6eafecfb":"code","267d7d4d":"code","2b121785":"code","742aa8a0":"code","93bc8d27":"code","6b83dca4":"code","282e9b4e":"code","c750f2a9":"code","606a7287":"code","3d131b1f":"code","ccab7016":"code","fcc0df2c":"code","85a754c8":"code","4d8f89e9":"code","e04fc33c":"code","297a458f":"code","ad1abb0a":"code","a16b8740":"code","d8ede760":"code","8d9d938e":"code","5fa84e63":"code","f8bdd42f":"code","5362a6cf":"code","edcb2b15":"code","e7c84539":"markdown","665604ae":"markdown","649065d3":"markdown","3298375f":"markdown","1e5636cd":"markdown","692f14c8":"markdown","672bf98b":"markdown","2e0c75c6":"markdown","c9a8c83c":"markdown","f5b94e9f":"markdown","6fbbce6e":"markdown","f303c0fd":"markdown","7f5ed6e0":"markdown","d1bbd12b":"markdown","a91861b2":"markdown","05990cc8":"markdown","18e7fd97":"markdown","191ea2ab":"markdown","84970760":"markdown","55b3c8be":"markdown","7b8dae48":"markdown","bef6f935":"markdown","bd107cf9":"markdown","e2c78ea0":"markdown","7c663c8a":"markdown","aa157437":"markdown","ee835c9d":"markdown","035ade6f":"markdown","6bd049ec":"markdown","2ad10003":"markdown","84e1aeaa":"markdown","242c1be5":"markdown","749d1cb9":"markdown","ee0df5cb":"markdown","42848da8":"markdown","b6a04595":"markdown","ee319f90":"markdown","b1ebbc82":"markdown","3aafa093":"markdown","0b68cafb":"markdown","dc7a8bf5":"markdown","8dd7d6b9":"markdown","d3033eaf":"markdown","dbf57dd4":"markdown","6f2b475e":"markdown","6a4578b4":"markdown","054177d5":"markdown","dc85fc5f":"markdown","e41a5ddc":"markdown","67bdec12":"markdown","6fe9f1ed":"markdown","ff5afd38":"markdown","fdc5dc0f":"markdown","d18e78bb":"markdown","5d63ad46":"markdown","89521020":"markdown","fb6d9e15":"markdown","6c834fe8":"markdown","ac30964e":"markdown","10652ed3":"markdown"},"source":{"3d41288d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bf3b64e3":"df = pd.read_csv(\"..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv\")","a4c42d38":"df.head()","6068f929":"df.shape","e29d1ba6":"df.drop([\"id\", \"name\", \"host_name\", \"last_review\"], axis = 1, inplace = True)","3fca7673":"df.isnull().sum()","9a578ddb":"df[\"reviews_per_month\"].fillna(df.reviews_per_month.mean(), inplace=True)","c55c67c9":"plt.figure(figsize = (10,6))\ndf[\"host_id\"].value_counts().head().sort_values().plot(kind = \"barh\", color = \"darkblue\")\nplt.xlabel(\"Count\", size = 14)\nplt.ylabel(\"Host ID\", size = 14)\nplt.title(\"Top 5 Hosts With Most Posts\", size = 18)","a868fb0a":"df.loc[lambda df: df['host_id'] == 219517861][\"neighbourhood\"].value_counts().plot(kind = \"bar\", \n                                                                                   figsize=(10,6), color = \"skyblue\")\nplt.xlabel(\"Neighbourhood\", size = 14)\nplt.ylabel(\"Count\", size = 14)","79cc9f8a":"print(\"There are\", df[\"neighbourhood_group\"].nunique(), \"distinct values.\")","bfa472a0":"plt.figure(figsize = (10,6))\ndf[\"neighbourhood_group\"].value_counts().sort_values().plot(kind = \"barh\", color = \"brown\")\nplt.xlabel(\"Count\", size = 14)\nplt.ylabel(\"Neighbourhood Group\", size = 14)","210154b4":"plt.figure(figsize = (10,6))\nng_p_mean_df = df.groupby(\"neighbourhood_group\")[\"price\"].agg(\"mean\")\nng_p_mean_df.sort_values().plot(kind = \"barh\", color = \"pink\")\nplt.xlabel(\"Price\", size = 14)\nplt.ylabel(\"Neighbourhood Group\", size = 14)\nplt.title(\"Average Price of Neighbourhood Groups\", size = 18)","b9f76153":"print(\"There are\", df[\"neighbourhood\"].nunique(), \"distinct values.\")","540d17ab":"plt.figure(figsize = (10,6))\ndf[\"neighbourhood\"].value_counts().head(10).sort_values().plot(kind = \"barh\")\nplt.xlabel(\"Count\", size = 14)\nplt.ylabel(\"Neighbourhood\", size = 14)\nplt.title(\"Top 10 Neighbourhood\", size = 18)","2e9a0b14":"plt.figure(figsize = (10,6))\ntop_20_n_p_mean_df = df.groupby(\"neighbourhood\")[\"price\"].agg(\"mean\").sort_values(ascending=False).head(20)\ntop_20_n_p_mean_df.sort_values(ascending=False).plot(kind = \"bar\", color = \"violet\")\nplt.xlabel(\"Neighbourhood\", size = 14)\nplt.ylabel(\"Price\", size = 14)\nplt.title(\"Top 20 Neighbourhood With Highest Mean Price\", size = 18)","c396211a":"plt.figure(figsize = (10,6))\nsns.scatterplot(df.longitude, df.latitude, hue=df.neighbourhood_group)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.title(\"Distribution of Neighbourhood Group with Respect to Latitude and Longitude\", fontsize=18)\nplt.legend(prop={\"size\":12})","47507a74":"plt.figure(figsize = (10,6))\nsns.scatterplot(df.longitude, df.latitude, hue=df.room_type)\nplt.ylabel(\"Latitude\", fontsize=14)\nplt.xlabel(\"Longitude\", fontsize=14)\nplt.title(\"Distribution of Room Type with Respect to Latitude and Longitude\", fontsize=18)\nplt.legend(prop={\"size\":12})","6a3a7102":"plt.figure(figsize = (10,6))\ndf[\"room_type\"].value_counts().sort_values().plot(kind = \"bar\", color = \"green\")\nplt.xlabel(\"Room Type\", size = 14)\nplt.ylabel(\"Count\", size = 14)","abf59cc0":"plt.figure(figsize = (10,6))\nr_p_mean_df = df.groupby(\"room_type\")[\"price\"].agg(\"mean\")\nr_p_mean_df.sort_values(ascending=True).plot(kind = \"barh\", color = \"gray\")\nplt.xlabel(\"Price\", size = 14)\nplt.ylabel(\"Room Type\", size = 14)\nplt.title(\"Room Types With Mean Price\", size = 18)","61b02b5f":"fig, ax = plt.subplots(1,2, figsize = (16,8))\nsns.distplot(df.price, color = \"darksalmon\", ax=ax[0]).set_title(\"Price Distribution Before Log Transformation\",\n                                                                size = 16)\nsns.distplot(np.log1p(df.price), color = \"darksalmon\", ax=ax[1]).set_title(\"Price Distribution After Log Transformation\",\n                                                                size = 16)\ndf[\"price\"] = np.log1p(df[\"price\"])","b176c3f7":"plt.figure(figsize = (10,6))\ndf_corr = df.corr()\nsns.heatmap(df_corr, annot=True, cmap=\"copper\")","fa32155c":"fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3, figsize=(12,8))\nax1.boxplot(df[\"price\"])\nax1.set_ylabel(\"Price(log)\", size=13)\nax2.boxplot(df[\"minimum_nights\"])\nax2.set_ylabel(\"Minimum Nights\", size=13)\nax3.boxplot(df[\"number_of_reviews\"])\nax3.set_ylabel(\"Number of Reviews\", size=13)\nax4.boxplot(df[\"reviews_per_month\"])\nax4.set_ylabel(\"Reviews Per Month\", size=13)\nax5.boxplot(df[\"calculated_host_listings_count\"])\nax5.set_ylabel(\"Calculated Host Listings Count\", size=13)\nax6.boxplot(df[\"availability_365\"])\nax6.set_ylabel(\"Availability 365\", size=13)\nplt.tight_layout(pad=3)","80438dd4":"df[\"neighbourhood\"].value_counts().describe()","a4cf4651":"top_10_neig = df[\"neighbourhood\"].value_counts().sort_values(ascending=False).head(10)\nsummation = top_10_neig.sum()\npercentage = (100 * summation) \/ df.shape[0]\nprint(round(percentage, 2), \"% of neighbourhood column consists of the 10 most common neighbourhood values.\")","216b10eb":"other_values = df[\"neighbourhood\"].value_counts().sort_values(ascending=False).tail(df[\"neighbourhood\"].nunique() - 10).index.tolist()\ndf_new = df.replace(other_values, \"Other\")\ndf_new[\"neighbourhood\"].nunique()","07b9d333":"df_dummy = pd.get_dummies(df_new, columns=[\"neighbourhood_group\", \"neighbourhood\", \"room_type\"], \n                          prefix=[\"ng\", \"n\", \"rt\"])\ndf_dummy.drop([\"host_id\"], axis=1, inplace=True)","f368c24a":"X = df_dummy.drop(\"price\", axis = 1)\ny = df_dummy[\"price\"]","6eafecfb":"scale = StandardScaler()\nX_scaled = scale.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.33, random_state = 42)\ndf_scaled = pd.DataFrame(X_scaled , columns = X.columns)\ndf_scaled.head()","267d7d4d":"def models(X_train, X_test, y_train, y_test):\n    \n    lr = LinearRegression()\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_test)\n    print(\"Linear Regression\")\n    print(\"-----------------\")\n    print(\"Test Score:\", r2_score(y_test, y_pred))\n    print(\"Train Score:\", lr.score(X_train, y_train))\n    print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n    print(\"Mean Squared Error\", mean_squared_error(y_test, y_pred))\n    print(\"\\n****************************************************\\n\")\n    \n    lasso = Lasso(alpha = 0.0001)\n    lasso.fit(X_train, y_train)\n    y_pred = lasso.predict(X_test)\n    print(\"Lasso\")\n    print(\"-----------------\")\n    print(\"Test Score:\", r2_score(y_test, y_pred))\n    print(\"Train Score:\", lasso.score(X_train, y_train))\n    print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n    print(\"Mean Squared Error\", mean_squared_error(y_test, y_pred))\n    print(\"\\n****************************************************\\n\")\n    \n    dtr = DecisionTreeRegressor(min_samples_leaf=25)\n    dtr.fit(X_train, y_train)\n    y_pred= dtr.predict(X_test)\n    print(\"DTR\")\n    print(\"-----------------\")\n    print(\"Test Score:\", r2_score(y_test, y_pred))\n    print(\"Train Score:\", dtr.score(X_train, y_train))\n    print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n    print(\"Mean Squared Error\", mean_squared_error(y_test, y_pred))\n    print(\"\\n****************************************************\\n\")\n    \n    rfr = RandomForestRegressor(random_state = 42)\n    rfr.fit(X_train, y_train)\n    y_pred= rfr.predict(X_test)\n    print(\"RFR\")\n    print(\"-----------------\")\n    print(\"Test Score:\", r2_score(y_test, y_pred))\n    print(\"Train Score:\", rfr.score(X_train, y_train))\n    print(\"Mean Absolute Error:\", mean_absolute_error(y_test, y_pred))\n    print(\"Mean Squared Error\", mean_squared_error(y_test, y_pred))\n    print(\"\\n****************************************************\\n\")","2b121785":"models(X_train, X_test, y_train, y_test)","742aa8a0":"lasso = Lasso(0.0001)\nlasso.fit(X_train, y_train)\nimportance = lasso.coef_\nfeatures = pd.DataFrame(importance, df_scaled.columns, columns=['coefficient'])\nfeatures.plot(kind = \"bar\", figsize = (10, 6)).set_title(\"Lasso Coefficients\", size = 16)","93bc8d27":"dtr = DecisionTreeRegressor(min_samples_leaf=25)\ndtr.fit(X_train, y_train)\nf_importance = pd.DataFrame({'Importance': dtr.feature_importances_}, \n                            index=df_scaled.columns).sort_values(by='Importance', ascending=False)\nf_importance.plot(kind = \"bar\", color=\"green\", figsize=(10,6)).set_title(\"DTR Feature Importances\", size = 16)","6b83dca4":"rfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\ny_pred= rfr.predict(X_test)\nr2_score(y_test, y_pred)\nf_importance = pd.DataFrame({'Importance': rfr.feature_importances_}, \n                            index=df_scaled.columns).sort_values(by='Importance', ascending=False)\nf_importance.plot(kind = \"bar\", figsize=(10,6)).set_title(\"RFR Feature Importances\", size = 16)","282e9b4e":"df[\"price\"] = np.expm1(df[\"price\"])","c750f2a9":"q1_price = df[\"price\"].quantile(0.25)\nq3_price = df[\"price\"].quantile(0.75)\niqr_price = q3_price - q1_price\nlower_limit_price = q1_price - 1.5 * iqr_price\nupper_limit_price = q3_price + 1.5 * iqr_price\n\ndf_filter_price = df[(df[\"price\"] > lower_limit_price) & (df[\"price\"] < upper_limit_price)]\n\nq1_min_nights = df[\"minimum_nights\"].quantile(0.25)\nq3_min_nights = df[\"minimum_nights\"].quantile(0.75)\niqr_min_nights = q3_min_nights - q1_min_nights\nlower_limit_min_nights = q1_min_nights - 1.5 * iqr_min_nights\nupper_limit_min_nights = q3_min_nights + 1.5 * iqr_min_nights\n\ndf_filter_min_nights = df_filter_price[(df_filter_price[\"minimum_nights\"] > lower_limit_min_nights) & \n                                       (df_filter_price[\"minimum_nights\"] < upper_limit_min_nights)]\n\nq1_num_rew = df[\"number_of_reviews\"].quantile(0.25)\nq3_num_rew = df[\"number_of_reviews\"].quantile(0.75)\niqr_num_rew = q3_num_rew - q1_num_rew\nlower_limit_num_rew = q1_num_rew - 1.5 * iqr_num_rew\nupper_limit_num_rew = q3_num_rew + 1.5 * iqr_num_rew\n\ndf_filter_num_rew = df_filter_min_nights[(df_filter_min_nights[\"number_of_reviews\"] > lower_limit_num_rew) & \n                                       (df_filter_min_nights[\"number_of_reviews\"] < upper_limit_num_rew)]\n\nq1_rpm = df['reviews_per_month'].quantile(0.25)\nq3_rpm = df['reviews_per_month'].quantile(0.75)\niqr_rpm = q3_rpm - q1_rpm\nlower_limit_rpm = q1_rpm - 1.5 * iqr_rpm\nupper_limit_rpm = q3_rpm + 1.5 * iqr_rpm\n\ndf_filter_rpm = df_filter_num_rew[(df_filter_num_rew[\"reviews_per_month\"] > lower_limit_rpm) & \n                                       (df_filter_num_rew[\"reviews_per_month\"] < upper_limit_rpm)]\n\nq1_hlc = df['calculated_host_listings_count'].quantile(0.25)\nq3_hlc = df['calculated_host_listings_count'].quantile(0.75)\niqr_hlc = q3_hlc - q1_hlc\nlower_limit_hlc = q1_hlc - 1.5 * iqr_hlc\nupper_limit_hlc = q3_hlc + 1.5 * iqr_hlc\n\ndf_filtered = df_filter_rpm[(df_filter_rpm[\"calculated_host_listings_count\"] > lower_limit_hlc) & \n                                       (df_filter_rpm[\"calculated_host_listings_count\"] < upper_limit_hlc)]","606a7287":"fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2,3, figsize=(12,8))\nax1.boxplot(df_filtered[\"price\"])\nax1.set_ylabel(\"Price\", size=13)\nax2.boxplot(df_filtered[\"minimum_nights\"])\nax2.set_ylabel(\"Minimum Nights\", size=13)\nax3.boxplot(df_filtered[\"number_of_reviews\"])\nax3.set_ylabel(\"Number of Reviews\", size=13)\nax4.boxplot(df_filtered[\"reviews_per_month\"])\nax4.set_ylabel(\"Reviews Per Month\", size=13)\nax5.boxplot(df_filtered[\"calculated_host_listings_count\"])\nax5.set_ylabel(\"Calculated Host Listings Count\", size=13)\nax6.boxplot(df_filtered[\"availability_365\"])\nax6.set_ylabel(\"Availability 365\", size=13)\nplt.tight_layout(pad=3)","3d131b1f":"other_values = df_filtered[\"neighbourhood\"].value_counts().sort_values(ascending=False).tail(217 - 30).index.tolist()\ndf_filtered_new = df_filtered.replace(other_values, \"Other\")\nsummation = df_filtered[\"neighbourhood\"].value_counts().sort_values(ascending=False).head(30).sum()\npercentage = (100 * summation) \/ df.shape[0]\nprint(round(percentage, 2), \"% of neighbourhood column consists of the 30 most common neighbourhood values.\")","ccab7016":"df_dummy_filtered = pd.get_dummies(df_filtered_new, columns=[\"neighbourhood_group\", \"neighbourhood\", \"room_type\"], \n                          prefix=[\"n_g\", \"n\", \"r_t\"])\ndf_dummy_filtered.drop([\"host_id\"], axis=1, inplace=True)","fcc0df2c":"X_filtered = df_dummy_filtered.drop(\"price\", axis = 1)\ny_filtered = df_dummy_filtered[\"price\"]\ny_filtered = np.log1p(y_filtered)","85a754c8":"scale = StandardScaler()\nX_scaled_filtered = scale.fit_transform(X_filtered)\nX_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered = train_test_split(X_scaled_filtered, y_filtered, \n                                                                                test_size = 0.33, random_state = 42)\ndf_scaled_filtered = pd.DataFrame(X_scaled_filtered , columns = X_filtered.columns)\ndf_scaled_filtered.head()","4d8f89e9":"models(X_train_filtered, X_test_filtered, y_train_filtered, y_test_filtered)","e04fc33c":"def cv_scores(X_train, y_train):\n    \n    print(\"Linear Regression\")\n    print(\"-----------------\")\n    lr = LinearRegression()\n    print(cross_val_score(lr, X_train, y_train, scoring='r2', cv=5).mean())\n    print(\"\\n****************************************************\\n\")\n    \n    print(\"Lasso\")\n    print(\"-----------------\")\n    lasso = Lasso(alpha = 0.0001)\n    print(cross_val_score(lasso, X_train, y_train, scoring='r2', cv=5).mean())\n    print(\"\\n****************************************************\\n\")\n    \n    print(\"DTR\")\n    print(\"-----------------\")\n    dtr = DecisionTreeRegressor(min_samples_leaf=25)\n    print(cross_val_score(dtr, X_train, y_train, scoring='r2', cv=5).mean())\n    print(\"\\n****************************************************\\n\")\n    \n    print(\"RFR\")\n    print(\"-----------------\")\n    rfr = RandomForestRegressor(random_state = 42)\n    print(cross_val_score(rfr, X_train, y_train, scoring='r2', cv=5).mean())\n    print(\"\\n****************************************************\\n\")","297a458f":"cv_scores(X_train_filtered, y_train_filtered)","ad1abb0a":"def param_tuning(X_train, y_train):\n    \n    print(\"Best Hyperparameters for Linear Regression\")\n    print(\"-----------------\")\n    param = {'fit_intercept':[True,False], \n             'normalize':[True,False], \n             'copy_X':[True, False]}\n    lr_random = RandomizedSearchCV(estimator = LinearRegression(), \n                                   param_distributions = param, n_iter = 100, cv = 3, \n                                   verbose=2, random_state=42, scoring='neg_mean_squared_error', n_jobs = -1)\n    lr_random.fit(X_train, y_train)\n    print(lr_random.best_params_)\n    print(lr_random.best_score_ * -1)\n    print(\"\\n****************************************************\\n\")\n    \n    \n    print(\"Best Hyperparameters for Lasso\")\n    print(\"-----------------\")\n    alpha = {\"alpha\": [5, 0.5, 0.05, 0.005, 0.0005, 1, 0.1, 0.01, 0.001, 0.0001, 0]}\n    lasso_random = RandomizedSearchCV(estimator = Lasso(), \n                                   param_distributions = alpha, n_iter = 100, cv = 3, \n                                   verbose=2, random_state=42, scoring='neg_mean_squared_error', n_jobs = -1)\n    lasso_random.fit(X_train, y_train)\n    print(lasso_random.best_params_)\n    print(lasso_random.best_score_ * -1)\n    print(\"\\n****************************************************\\n\")\n    \n    print(\"Best Hyperparameters for DTR\")\n    print(\"-----------------\")\n    param_dist = {\"criterion\": [\"mse\", \"mae\"],\n              \"min_samples_split\": [10, 20, 40],\n              \"max_depth\": [2, 6, 8],\n              \"min_samples_leaf\": [20, 40, 100],\n              \"max_leaf_nodes\": [5, 20, 100],\n              }\n    dtr_random = RandomizedSearchCV(estimator = DecisionTreeRegressor(), \n                                   param_distributions = param_dist, n_iter = 100, cv = 3, \n                                   verbose=2, random_state=42, scoring='neg_mean_squared_error', n_jobs = -1)\n    dtr_random.fit(X_train, y_train)\n    print(dtr_random.best_params_)\n    print(dtr_random.best_score_ * -1)\n    print(\"\\n****************************************************\\n\")\n    \n    \n    print(\"Best Hyperparameters for RFR\")\n    print(\"-----------------\")\n    random_grid = {'n_estimators': [int(i) for i in range(50, 400, 50)],\n                   'max_features': ['auto', 'sqrt'],\n                   'max_depth': [int(i) for i in range(10, 60, 10)],\n                   'min_samples_split': [2, 5, 10],\n                   'min_samples_leaf': [1, 2, 4],\n                   'bootstrap': [True, False]\n                  }\n    rfr_random = RandomizedSearchCV(estimator = RandomForestRegressor(), \n                                   param_distributions = random_grid, n_iter = 100, cv = 3, \n                                   verbose=2, random_state=42, scoring='neg_mean_squared_error', n_jobs = -1)\n    rfr_random.fit(X_train, y_train)\n    print(rfr_random.best_params_)\n    print(rfr_random.best_score_ * -1)\n    print(\"\\n****************************************************\\n\")","a16b8740":"#param_tuning(X_train_filtered, y_train_filtered)","d8ede760":"test_score_dict = {}\nmae_dict = {}\nmse_dict = {}\n\ndef modelTuned(X_train, y_train):\n    \n    lr = LinearRegression(normalize = True, \n                          fit_intercept = True, \n                          copy_X = True)\n    lr.fit(X_train, y_train)\n    y_pred = lr.predict(X_test_filtered)\n    print(\"Linear Regression\")\n    print(\"-----------------\")\n    print(\"Test Score:\", r2_score(y_test_filtered, y_pred))\n    print(\"Train Score:\", lr.score(X_train, y_train))\n    print(\"Mean Absolute Error:\", mean_absolute_error(y_test_filtered, y_pred))\n    print(\"Mean Squared Error\", mean_squared_error(y_test_filtered, y_pred))\n    print(\"\\n****************************************************\\n\")\n    test_score_dict[\"LR\"] = r2_score(y_test_filtered, y_pred)\n    mae_dict[\"LR\"] = mean_absolute_error(y_test_filtered, y_pred)\n    mse_dict[\"LR\"] = mean_squared_error(y_test_filtered, y_pred)\n    \n    lasso = Lasso(alpha = 0.0001)\n    lasso.fit(X_train, y_train)\n    y_pred = lasso.predict(X_test_filtered)\n    print(\"Lasso\")\n    print(\"-----------------\")\n    print(\"Test Score:\", r2_score(y_test_filtered, y_pred))\n    print(\"Train Score:\", lasso.score(X_train, y_train))\n    print(\"Mean Absolute Error:\", mean_absolute_error(y_test_filtered, y_pred))\n    print(\"Mean Squared Error\", mean_squared_error(y_test_filtered, y_pred))\n    print(\"\\n****************************************************\\n\")\n    test_score_dict[\"Lasso\"] = r2_score(y_test_filtered, y_pred)\n    mae_dict[\"Lasso\"] = mean_absolute_error(y_test_filtered, y_pred)\n    mse_dict[\"Lasso\"] = mean_squared_error(y_test_filtered, y_pred)\n    \n    dtr = DecisionTreeRegressor(min_samples_split = 10, \n                                min_samples_leaf = 40,\n                                max_leaf_nodes = 100, \n                                max_depth = 8, \n                                criterion = 'mse')\n    dtr.fit(X_train, y_train)\n    y_pred= dtr.predict(X_test_filtered)\n    print(\"DTR\")\n    print(\"-----------------\")\n    print(\"Test Score:\", r2_score(y_test_filtered, y_pred))\n    print(\"Train Score:\", dtr.score(X_train, y_train))\n    print(\"Mean Absolute Error:\", mean_absolute_error(y_test_filtered, y_pred))\n    print(\"Mean Squared Error\", mean_squared_error(y_test_filtered, y_pred))\n    print(\"\\n****************************************************\\n\")\n    test_score_dict[\"DTR\"] = r2_score(y_test_filtered, y_pred)\n    mae_dict[\"DTR\"] = mean_absolute_error(y_test_filtered, y_pred)\n    mse_dict[\"DTR\"] = mean_squared_error(y_test_filtered, y_pred)\n    \n\n    rfr = RandomForestRegressor(random_state = 42, \n                                n_estimators = 150,\n                                min_samples_split = 10,\n                                min_samples_leaf = 4,\n                                max_features = 'auto',\n                                max_depth = 10,\n                                bootstrap = True)\n    rfr.fit(X_train, y_train)\n    y_pred = rfr.predict(X_test_filtered)\n    print(\"RFR\")\n    print(\"-----------------\")\n    print(\"Test Score:\", r2_score(y_test_filtered, y_pred))\n    print(\"Train Score:\", rfr.score(X_train, y_train))\n    print(\"Mean Absolute Error:\", mean_absolute_error(y_test_filtered, y_pred))\n    print(\"Mean Squared Error\", mean_squared_error(y_test_filtered, y_pred))\n    print(\"\\n****************************************************\\n\")\n    test_score_dict[\"RFR\"] = r2_score(y_test_filtered, y_pred)\n    mae_dict[\"RFR\"] = mean_absolute_error(y_test_filtered, y_pred)\n    mse_dict[\"RFR\"] = mean_squared_error(y_test_filtered, y_pred)","8d9d938e":"modelTuned(X_train_filtered, y_train_filtered)","5fa84e63":"fig, (ax1, ax2, ax3) = plt.subplots(1, 3 ,figsize = (16,6))\nax1.set_title(\"R2 Scores\")\nax1.plot(list(test_score_dict.keys()), list(test_score_dict.values()), marker = \"o\", color = \"steelblue\")\nax2.set_title(\"MAE\")\nax2.plot(list(mae_dict.keys()), list(mae_dict.values()), marker = \"o\", color = \"green\")\nax3.set_title(\"MSE\")\nax3.plot(list(mse_dict.keys()), list(mse_dict.values()), marker = \"o\", color = \"orange\")","f8bdd42f":"rfr = RandomForestRegressor(random_state = 42, \n                                n_estimators = 150,\n                                min_samples_split = 10,\n                                min_samples_leaf = 4,\n                                max_features = 'auto',\n                                max_depth = 10,\n                                bootstrap = True)\nrfr.fit(X_train_filtered, y_train_filtered)\ny_pred_filtered = rfr.predict(X_test_filtered)\ncomp_airbnb = pd.DataFrame({\n        'Actual Values': np.array(np.expm1(y_test_filtered)),\n        'Predicted Values': np.expm1(y_pred_filtered)})\n\ncomp_airbnb.head(5)","5362a6cf":"fig, (ax1, ax2) = plt.subplots(1,2, figsize = (16,8))\nax1.scatter(np.array(np.expm1(y_test_filtered)),np.expm1(y_pred_filtered))\nax1.set_xlabel(\"True\", size = 14)\nax1.set_ylabel(\"Prediction\", size = 14)\nax2.plot(np.array(np.expm1(y_test_filtered)), label=\"True\")\nax2.plot(np.expm1(y_pred_filtered), label = \"Prediction\")\nax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', prop={'size': 12})","edcb2b15":"price_higher_250 = df[df[\"price\"] > 250].shape[0]\nprint(\"Number of data with price that more than 250 dollars in the data is\", price_higher_250, \n      \"out of\", df[\"price\"].shape[0])","e7c84539":"<h5>\u25cf As we have already determined that we have lots of distinct categorical values (221 values) in neighbourhood. And, that might cause some problems in model building steps.\n    \n<h5>Let's detect some rare values and transform these rare values into \"Other\" for one-hot encoding operation.","665604ae":"<h5>2-DTR Feature Importances<h5>","649065d3":"<h5>For the data we have, it's clearly seen that our model is not good at predicting high prices(about more than 250 dollars). We had not too much price values which are higher than 250 dollars in our original data frame. And, after appling IQR method, we have lost a little more. This problem might be fixed by adding more data with higher prices. Therefore, this dataset is not enough to predict housing with high prices.\n<h5> Any comments and suggestions are welcome.","3298375f":"<h3>1-Host ID","1e5636cd":"<h3>4-Latitude and Longitude","692f14c8":"<h5>\u25cf Mean Price of Entire home\/apt is almost twice of the others.<h5>","672bf98b":"<h3>Correlation Matrix","2e0c75c6":"<h5>Now, we have 11 distinct values in \"neighbourhood\" column. That's much more better for one-hot encoding operation.\n<h5>Let's apply one-hot encoding.\n<h5>\"host_id\" will not affect the price of housing. So, it is insignificant. We can drop it from our data frame.","c9a8c83c":"<h5>\u25cf The graph on the left shows that there is a positively skewed distribution. To make a better statisticial analysis and to get better scores, we have applied log transformation for the price column.","f5b94e9f":"<h5> Let's check the average price for each neighbourhood group.","6fbbce6e":"<h5>\u25cf Cross validation scores also show overfitting problem for RFR.","f303c0fd":"<h3>2-Neighbourhood Group","7f5ed6e0":"<h5>\u25cf Looks like \"neighbourhood_group\" column is largely made up of Manhattan and Brooklyn.","d1bbd12b":"<h5>From above bar graph, we can see that Entire home\/apt and Manhattan are the factors which increase the price(as we expected). Also, increasing of availability_365 is increasing the housing prices.\n<h5>And, we can also see that increasing of longitude decreases the price. Also, Brooklyn, Staten Island and Shared room are other factors for price to decrease.","a91861b2":"<h5>1-Lasso Feature Coefficients<h5>","05990cc8":"<h5>Now, let's dive into the each column and try to understand the data better.<h5>","18e7fd97":"<h5>Airbnb is a platform that connects people who want to rent out their homes with people who are looking for accommodations in that locale. So, from \"host_id\" column, we can easily analyze the hosts who share the most posts.<h5>","191ea2ab":"<h5> Let's build our models one more time with tuned parameters and check the results.","84970760":"# Importing Necessary Libraries","55b3c8be":"<h5> Let's visualize the result to distinguish better.","7b8dae48":"<h5>\u25cf Looks like \"room_type\" column is largely made up of Entire home\/apt and Private room.<h5>","bef6f935":"<h3>5-Room Type","bd107cf9":"<h5>\u25cf About 48 % of the \"neighbourhood\" column is made up of top 10 repetitive values.\n<h5>So, let's change other categorical values (except these 10 most repetitive values) as \"Other\".","e2c78ea0":"<h5>\u25cf We have 221 distinct values in \"neighbourhood\" column. That is pretty large and might affect the model performance after one-hot encoding operation. So, I will decrease the number of distinct values as much as I can before the one-hot encoding operation in the future operations.<h5>","7c663c8a":"# Model Building","aa157437":"<h5>From which neighborhoods did the \"host 219517861\" share posts?  ","ee835c9d":"# Removing Outliers (IQR Method)","035ade6f":"<h5>We have too much null values in \"reviews_per_month column\". Let's change all the null values with the mean value of \"reviews_per_month column\".<h5>","6bd049ec":"<h5>Let's check how many distinct value we have.","2ad10003":"<h5>\u25cf Average Price in Manhattan is pretty higher than other neighbourhood groups.<h5>","84e1aeaa":"<h5>Let's check feature importances of some models.","242c1be5":"<h5>\u25cf As it's seen we have an improvement. MAE and MSE has been decreased for each model. However, we still have RFR overfitting problem.<h5>\n<h5>Let's check cross validation scores.<h5>","749d1cb9":"# Data Preparation","ee0df5cb":"<h5>Boxplots are very useful tools to determine the outliers which are located in the data. An outlier is an observation that lies an abnormal distance from other values in a random sample from a population. And sometimes these outliers should be cleaned from the data to get more accurate models. Let's create some boxplots and check if there are outliers or not in our dataset.<h5>","42848da8":"# Model Comparison","b6a04595":"<h5> After execute the code above, we will get the following results,\n<h5> <u style=\"color:#21618C;\">Linear Regression<\/u> \n<h5 style=\"color:#E67E22;\"> Best parameters : <span style=\"padding-left:300px\"; > Mean Squared Error : <\/span> <\/h5>\n<h5>{'normalize': True, 'fit_intercept': True, 'copy_X': True} <span style=\"padding-left:65px\"; > 0.14374795083566586 <\/span> <\/h5>\n\n<h5> <u style=\"color:#21618C;\">Lasso<\/u> \n<h5 style=\"color:#E67E22;\"> Best parameters : <span style=\"padding-left:300px\"; > Mean Squared Error : <\/span> <\/h5>\n<h5>{'alpha': 0.0001} <span style=\"padding-left:315px\"; > 0.14372359111160216 <\/span> <\/h5>\n    \n<h5> <u style=\"color:#21618C;\">DTR<\/u> \n<h5 style=\"color:#E67E22;\"> Best parameters : <span style=\"padding-left:300px\"; > Mean Squared Error : <\/span> <\/h5>\n<h5>{'min_samples_split': 10, 'min_samples_leaf': 40, <br>'max_leaf_nodes': 100, 'max_depth': 8, 'criterion': 'mse'} <span style=\"padding-left:45px\"; > 0.14227165276822215 <\/span> <\/h5>\n    \n    \n<h5> <u style=\"color:#21618C;\">RFR<\/u> \n<h5 style=\"color:#E67E22;\"> Best parameters : <span style=\"padding-left:300px\"; > Mean Squared Error : <\/span> <\/h5>\n<h5>{'n_estimators': 150, 'min_samples_split': 10,<br> 'min_samples_leaf': 4,'max_features': 'auto',<br> 'max_depth': 10, 'bootstrap': True}<span style=\"padding-left:190px\"; > 0.13325989720571155 <\/span> <\/h5>\n","ee319f90":"<h5>A correlation matrix is a matrix which is showing the correlation between numerical variables. Each cell in the matrix shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses.\n    \n<h5> Let's take a look at the correlations.","b1ebbc82":"# Airbnb Data Analysis and Price Prediction","3aafa093":"<h3>Detection of Outliers","0b68cafb":"<h5>Price will be our target column in this project. So, let's check the distribution of price.","dc7a8bf5":"<h5>Some columns are insignificant for data analysis and prediction model. \"id\", \"name\", \"host_name\" and \"last_review\" will not affect the housing prices. So, let's drop them.","8dd7d6b9":"<h5>Let's check if there are null values or not.","d3033eaf":"<h3>6-Price","dbf57dd4":"<h5>Let's check how many distinct value we have.","6f2b475e":"# Hyperparameter Tuning Using RandomizedSearchCV","6a4578b4":"<h5> Scaling the data frame and splitting the data as train data and test data.","054177d5":"<h5> Before the model building section, we know that,\n<h5> \u25b6  Average price in Manhattan is highest when compared with others. So, that will be effective for high price prediction.\n<h5> \u25b6  Average price of Entire Home\/apt is largely higher than other room types. So, that will be effective for high price prediction.\n    \n<h5> After building our models, we can take a look at the feature importances and check these values if they have affect of increasing prices or not. ","dc85fc5f":"<h5> Now, let's compare the actual prices with predicted prices for RFR.","e41a5ddc":"<h5>\u25cf It's seen that RFR has the best R2 score, MAE and MSE. But, it has highly overfitting problem.","67bdec12":"<h2>Smart Pricing System<h2>","6fe9f1ed":"<h3>3-Neighbourhood","ff5afd38":"<h5>3-RFR Feature Importances<h5>","fdc5dc0f":"<h5>\u25cf As it's seen, we have too much outliers to handle except avabiality_365 column. \n    \n<h5>Firstly, let's continue our model building without removing outliers.<h5>","d18e78bb":"# Conclusion","5d63ad46":"# Exploratory Data Analysis and Visualizations","89521020":"<h5>I will increase the number of neigbourhood which will be shown in the data. From top 10 to top 30 to reach about 50 percent of content.<h5>","fb6d9e15":"<h5>Now, let's try remove some outliers by using IQR method and check the results one more time.<h5>","6c834fe8":"<h5>Let's apply hyperparameter tuning by using RandomizedSearchCV to improve our models and slightly decrease the overfitting problem.","ac30964e":"<h5> Linear Regression, Lasso, Decision Tree Regressor and Random Forest Regressor will be built in the model building section.","10652ed3":"<h5>Seperating price column from the original data frame."}}