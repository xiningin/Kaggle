{"cell_type":{"7af01aad":"code","2aa159c9":"code","60ba8f9d":"code","710c636f":"code","2affef52":"code","a1e84342":"code","09afc303":"code","ee139144":"code","87ac88f2":"code","c2886c86":"code","735e0100":"code","f1a2e48c":"code","6d419c58":"code","15695a1a":"code","f9763094":"code","75a1ad50":"code","90b96e11":"code","811e719b":"code","84436ac5":"code","4b4c838b":"code","684df2f8":"code","1786e331":"markdown","d4159cf4":"markdown","bcddf82b":"markdown","8039683f":"markdown","6281a56a":"markdown","46b6ae21":"markdown","436208ff":"markdown","66024e86":"markdown","30b81dbe":"markdown","2bc5c8e8":"markdown","0ea7c0d6":"markdown","0396ecde":"markdown","f96d7224":"markdown","0c651b9d":"markdown","688d14f3":"markdown","d9b1309c":"markdown","6f0a4358":"markdown","d6ad6267":"markdown"},"source":{"7af01aad":"import numpy as np\nimport pandas as pd\nimport time\n\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ndata = pd.read_csv('..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv')","2aa159c9":"data.head()","60ba8f9d":"data.info()","710c636f":"import warnings\nwarnings.filterwarnings('ignore')\n\nsns.countplot(data[\"fraudulent\"])\nplt.show()","2affef52":"x = data.loc[:,[\"company_profile\",\"description\",\"requirements\",\"benefits\"]]\ny = data[\"fraudulent\"]\n","a1e84342":"x.fillna(\" \",inplace=True)\nx.isnull().sum()","09afc303":"concat_data = []\nfor i in range(len(x)):\n    txt = x[\"company_profile\"][i] + \" \"\n    txt = txt + x[\"description\"][i] + \" \"\n    txt = txt + x[\"requirements\"][i] + \" \"\n    txt = txt + x[\"benefits\"][i]\n    concat_data.append(txt.strip())\n\n   ","ee139144":"concat_data[0]","87ac88f2":"pattern = \"[^a-zA-Z]\"\ncleanedTexts = []\nfor text in concat_data:\n    text = re.sub(pattern,\" \",text)\n    cleanedTexts.append(text.lower())\n","c2886c86":"cleanedTexts[0]","735e0100":"tokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(cleanedTexts)\n\nx_tokens = tokenizer.texts_to_sequences(cleanedTexts)","f1a2e48c":"print(x_tokens[0])","6d419c58":"seq_lens = [len(seq) for seq in x_tokens]\nq3 = np.quantile(seq_lens,.75)\nprint(q3)","15695a1a":"x_tokens_pad = np.asarray(pad_sequences(x_tokens,maxlen=int(q3)))","f9763094":"x_tokens_pad.shape","75a1ad50":"x_train,x_test,y_train,y_test = train_test_split(x_tokens_pad,y,test_size=0.2,random_state=42)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","90b96e11":"VOCAB_SIZE = 10000 + 1\nVEC_SIZE = 100\nTOKEN_SIZE = int(q3)\n","811e719b":"from tensorflow.compat.v1.keras.layers import CuDNNGRU\nmodel = keras.Sequential()\nmodel.add(layers.Embedding(input_dim=VOCAB_SIZE,\n                           output_dim=VEC_SIZE,\n                           input_length=TOKEN_SIZE\n                          ))\n\n\n\nmodel.add(CuDNNGRU(512,return_sequences=True))\nmodel.add(CuDNNGRU(1024,return_sequences=True))\nmodel.add(CuDNNGRU(2048))\nmodel.add(layers.Dense(1,activation=\"sigmoid\"))\n\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n","84436ac5":"model.summary()","4b4c838b":"hist = model.fit(x_train,y_train,validation_split=0.2,epochs=2)","684df2f8":"y_test = np.asarray(y_test)\ny_pred = model.predict_classes(x_test)\n\nprint(\"Accuracy score of model is {}%\".format(accuracy_score(y_pred=y_pred,y_true=y_test)*100))\n\nplt.subplots(figsize=(4,4))\nconf_matrix = confusion_matrix(y_pred=y_pred,y_true=y_test)\nsns.heatmap(conf_matrix,annot=True,fmt=\".1f\",linewidths=1.5)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"Actual Label\")\nplt.show()\n","1786e331":"* Now let's create a sequence that includes length of arrays and find Q3 value.","d4159cf4":"* Now let's check our data","bcddf82b":"### Part 1: Concatenating Text Parts\nFirst, we'll start with dropping redundant features and concatenate others.","8039683f":"* As you can see there are too many features in the dataset, we'll use company profile, description and requirements. And also fraudulent.","6281a56a":"### Part 3: Tokenizing and Padding\nYou know, in natural languages words are the representation of everything, such as we say *hi* when we see someone, h and i letters don't have any special meaning but hi has a special meaning. If 1 means hi, we can use it instead of hi.\n\nIn this part we'll convert words into integers and texts into sequences. \nIn deep learning we generally use dataset that has predefined shape. But in text dataset shapes might be different, such as one jobposting can have 100 words other can have 102 words. In order to solve this problem we can use different approaches, but in this kernel we'll use **padding**\n\nIn padding we will add some spaces to the texts and make all texts with same shape.","46b6ae21":"# Building Model\nIn this section I am going to build the model using keras API of Tensorflow. I'll use the developed version of RNNs, GRU, Gated Recurrent Unit. We don't use SimpleRNN, because it has a problem called *vanishing gradient* because of backpropagation.\n","436208ff":"# Training Model\nIn this section I am going to train model using prepared dataset.","66024e86":"# Conclusion\nThanks for your attention, if you have questions in your mind, feel free to ask in comment section. Also if you liked the kernel and upvote, I would be glad :)\n","30b81dbe":"* Now let's concatenate texts.","2bc5c8e8":"# Introduction\nHello people, welcome to this kernel. In this kernel I am going to classify jobpostings whether they are real or not. This dataset is small and you can handle that using traditional approachs (BoW,TF-IDF) but in this kernel I'll use word embeddings and RNNs.\n\n# Table of Content\n1. Data Preprocessing\n1. Building Model\n1. Training Model\n1. Testing Model\n1. Conclusion","0ea7c0d6":"### Part 2: Cleaning Texts Using Regular Expressions\nAs you can see in the texts there are too many redundant characters such as punctuation steps. In this part we'll clear texts using regular expressions.","0396ecde":"### Step 4: Train Test Splitting\nIn this section we'll split the dataset into train and test, to test dataset truly.","f96d7224":"* As we've seen from information table, there are NaN values in the set. We'll fill them with spaces.","0c651b9d":"* There are too many information here, let's move on to the next step.","688d14f3":"# Testing Model\nIn this section we'll test model using unused test set.","d9b1309c":"* As you can see most of the dataset is 0 (non-fraudulent) so we can consider this mission hard.","6f0a4358":"# Data Preprocessing\nIn this section I am going to prepare dataset in order to use in our neural network. Before starting, let's check the dataframe and class distribution.","d6ad6267":"* All texts will have shape 502"}}