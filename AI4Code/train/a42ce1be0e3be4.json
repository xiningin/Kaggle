{"cell_type":{"91b9ed29":"code","37ce779d":"code","5a6be70d":"code","47f36413":"code","9e5c8053":"code","da389975":"code","3d272f23":"code","3f910767":"code","17941cc6":"code","7c77a513":"code","554b14eb":"code","0aa7d55c":"code","a4305464":"code","5b6cc229":"code","57d58ac1":"code","3349b1f6":"code","5f0baeb5":"code","c4dd595b":"code","0df40595":"code","52c26f13":"code","67f58b6f":"code","18ed6097":"code","61719bd2":"code","f9633b4e":"code","130af98a":"code","b8231db0":"code","90bfadf7":"code","f1827f95":"code","4d958b7d":"code","dd95d11e":"code","0a16594e":"code","84dede2c":"code","f97bb141":"code","6a00176f":"markdown","12c0c2fc":"markdown","9b281757":"markdown","b0024156":"markdown","ff084104":"markdown","961a2779":"markdown","ca382ff1":"markdown","5e48c9c4":"markdown","094a0b99":"markdown"},"source":{"91b9ed29":"# Install required libs\n!pip install -U segmentation-models-pytorch albumentations --user ","37ce779d":"#!pip uninstall -y segmentation-models-pytorch","5a6be70d":"import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt","47f36413":"DATA_DIR = '.\/data\/'\n\n# load repo with data if it is not exists\nif not os.path.exists(DATA_DIR):\n    print('Loading data...')\n    os.system(\"wget -O train_images.zip https:\/\/datasets.aicrowd.com\/default\/aicrowd-practice-challenges\/public\/lndst\/v0.1\/train_images.zip\")\n    os.system(\"wget -O train_gt.zip https:\/\/datasets.aicrowd.com\/default\/aicrowd-practice-challenges\/public\/lndst\/v0.1\/train_gt.zip\")\n    os.system(\"wget -O test_images.zip https:\/\/datasets.aicrowd.com\/default\/aicrowd-practice-challenges\/public\/lndst\/v0.1\/test_images.zip\")\n    os.system(\"mkdir data\")\n    os.system(\"unzip -q train_images.zip -d data\/\")\n    os.system(\"unzip -q train_gt.zip -d data\/\")\n    os.system(\"unzip -q test_images.zip -d data\/\")\n\n    print('Done!')","9e5c8053":"x_train_dir = os.path.join(DATA_DIR, 'train_images')\ny_train_dir = os.path.join(DATA_DIR, 'train_gt')\n\nx_test_dir = os.path.join(DATA_DIR, 'test_images')","da389975":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()","3d272f23":"from torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset as BaseDataset","3f910767":"class Dataset(BaseDataset):\n    \"\"\"\n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    def __init__(\n            self, \n            images_dir, \n            masks_dir, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.ids = os.listdir(images_dir)\n        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]\n        self.masks_fps = [os.path.join(masks_dir, image_id[:-4] + '.png') for image_id in self.ids]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_fps[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = cv2.imread(self.masks_fps[i], 0)\n        mask = np.stack([mask], axis=-1).astype('float')\n\n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n        \n    def __len__(self):\n        return len(self.ids)","17941cc6":"class TestDataset(BaseDataset):\n    \"\"\"\n    Args:\n        images_dir (str): path to images folder\n        masks_dir (str): path to segmentation masks folder\n        class_values (list): values of classes to extract from segmentation mask\n        augmentation (albumentations.Compose): data transfromation pipeline \n            (e.g. flip, scale, etc.)\n        preprocessing (albumentations.Compose): data preprocessing \n            (e.g. noralization, shape manipulation, etc.)\n    \n    \"\"\"\n    \n    def __init__(\n            self, \n            images_dir, \n            augmentation=None, \n            preprocessing=None,\n    ):\n        self.ids = os.listdir(images_dir)\n        files_count = len(os.listdir(images_dir))\n        self.images_fps = [os.path.join(images_dir, 'image_%d.jpg'%(image_id)) for image_id in range(files_count)]\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        \n        # read data\n        image = cv2.imread(self.images_fps[i])\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        # apply augmentations\n        if self.augmentation:\n            sample = self.augmentation(image=image)\n            image = sample['image']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image)\n            image = sample['image'] \n\n        return image\n        \n    def __len__(self):\n        return len(self.ids)","7c77a513":"# Lets look at data we have\n\ndataset = Dataset(x_train_dir, y_train_dir)\n\nimage, mask = dataset[4] # get some sample\n\nvisualize(\n    image=image, \n    mask=mask.squeeze(),\n)","554b14eb":"# Lets look at data we have\n\ntest_dataset = TestDataset(x_train_dir)\n\nimage = test_dataset[4] # get some sample\nprint(image.shape)\n\nvisualize(\n    image=image, \n)","0aa7d55c":"import albumentations as albu\nalbu.__version__","a4305464":"def get_training_augmentation():\n    train_transform = [\n        albu.HorizontalFlip(p=0.5),\n        albu.PadIfNeeded(400, 400),\n        albu.IAAAdditiveGaussianNoise(p=0.2),\n        albu.IAAPerspective(p=0.5),\n        albu.OneOf(\n            [\n                albu.CLAHE(p=1),\n                albu.RandomBrightness(p=1),\n                albu.RandomGamma(p=1),\n            ],\n            p=0.9,\n        ),\n        albu.OneOf(\n            [\n                albu.IAASharpen(p=1),\n                albu.Blur(blur_limit=3, p=1),\n                albu.MotionBlur(blur_limit=3, p=1),\n            ],\n            p=0.9,\n        ),\n\n        albu.OneOf(\n            [\n                albu.RandomContrast(p=1),\n                albu.HueSaturationValue(p=1),\n            ],\n            p=0.9,\n        ),\n        albu.Resize(320, 320),\n    ]\n    return albu.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    \"\"\"Add paddings to make image shape divisible by 32\"\"\"\n    test_transform = [\n        albu.PadIfNeeded(400, 400),\n        albu.Resize(320, 320),\n    ]\n    return albu.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn):\n    \"\"\"Construct preprocessing transform\n    \n    Args:\n        preprocessing_fn (callbale): data normalization function \n            (can be specific for each pretrained neural network)\n    Return:\n        transform: albumentations.Compose\n    \n    \"\"\"\n    \n    _transform = [\n        albu.Lambda(image=preprocessing_fn),\n        albu.Lambda(image=to_tensor, mask=to_tensor),\n    ]\n    return albu.Compose(_transform)","5b6cc229":"#### Visualize resulted augmented images and masks\n\naugmented_dataset = Dataset(\n    x_train_dir, \n    y_train_dir, \n    augmentation=get_training_augmentation(), \n)\n\n# same image with different random transforms\nfor i in range(3):\n    image, mask = augmented_dataset[i]\n    print(image.shape, mask.shape)\n    visualize(image=image, mask=mask.squeeze())","57d58ac1":"import torch\nimport numpy as np\nimport segmentation_models_pytorch as smp","3349b1f6":"ENCODER = 'efficientnet-b7'\nENCODER_WEIGHTS = 'imagenet'\nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\nDEVICE = 'cuda'\n\n# create segmentation model with pretrained encoder\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","5f0baeb5":"train_dataset = Dataset(\n    x_train_dir, \n    y_train_dir, \n    augmentation=get_training_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n)\n\n# valid_dataset = Dataset(\n#     x_valid_dir, \n#     y_valid_dir, \n#     augmentation=get_validation_augmentation(), \n#     preprocessing=get_preprocessing(preprocessing_fn),\n# )\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=12)\n# valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)","c4dd595b":"# Dice\/F1 score - https:\/\/en.wikipedia.org\/wiki\/S%C3%B8rensen%E2%80%93Dice_coefficient\n# IoU\/Jaccard score - https:\/\/en.wikipedia.org\/wiki\/Jaccard_index\n\nloss = smp.utils.losses.DiceLoss()\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n    smp.utils.metrics.Fscore(),\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])","0df40595":"# create epoch runners \n# it is a simple loop of iterating over dataloader`s samples\ntrain_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\n\n# valid_epoch = smp.utils.train.ValidEpoch(\n#     model, \n#     loss=loss, \n#     metrics=metrics, \n#     device=DEVICE,\n#     verbose=True,\n# )","52c26f13":"max_score = 0\n\nfor i in range(0, 150):\n    \n    print('\\nEpoch: {}'.format(i))\n    train_logs = train_epoch.run(train_loader)\n    # valid_logs = valid_epoch.run(valid_loader)\n    \n    if max_score < train_logs['iou_score']:\n        max_score = train_logs['iou_score']\n        torch.save(model, '.\/best_model.pth')\n        print('Model saved!')\n        \n    if i == 50:\n        optimizer.param_groups[0]['lr'] = 1e-5\n        print('Decrease decoder learning rate to 1e-5!')","67f58b6f":"# load best saved checkpoint\nbest_model = torch.load('.\/best_model.pth')","18ed6097":"# create test dataset\nvalid_dataset = Dataset(\n    x_train_dir, \n    y_train_dir, \n    augmentation=get_validation_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n)\n\nvalid_dataloader = DataLoader(valid_dataset)\n\n# evaluate model on test set\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model=best_model,\n    loss=loss,\n    metrics=metrics,\n    device=DEVICE,\n)\n\nlogs = valid_epoch.run(valid_dataloader)","61719bd2":"# create test dataset\ntest_dataset = TestDataset(\n    x_test_dir, \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n)\n\ntest_dataloader = DataLoader(test_dataset)","f9633b4e":"best_model.eval()\nresults = []\n\nwith torch.no_grad():\n    for x in test_dataloader:\n        x = x.to(DEVICE)\n        prediction = best_model.forward(x)\n        results.append(prediction.cpu().numpy())","130af98a":"results = np.transpose(np.concatenate(results), (0, 2, 3, 1))[:, :, :, 0]\nresults.shape","b8231db0":"rescaled = []\nfor im in results:\n    im = cv2.resize(im, (400, 400))\n    rescaled.append(im)\nrescaled = np.stack(rescaled)\nresults = rescaled\nprint(results.shape)","90bfadf7":"idx = np.random.randint(len(test_dataset))\nfig, ax = plt.subplots(10, 2, figsize=(10, 50))\nfor idx in range(10):\n    ax[idx][0].imshow(plt.imread(test_dataset.images_fps[idx]))\n    ax[idx][0].set_title(test_dataset.images_fps[idx])\n    ax[idx][1].imshow(results[idx])\n    ax[idx][1].set_title('Image: %d'%(idx))\nplt.show()","f1827f95":"submission = np.reshape(results, (-1,))\nsubmission = (submission > 0.5).astype(np.int8)\nnp.save('submission.npy', submission)","4d958b7d":"# train dataset without transformations for image visualization\ntrain_dataset_vis = Dataset(\n    x_train_dir, y_train_dir, \n)","dd95d11e":"for i in range(5):\n    n = np.random.choice(len(train_dataset))\n    \n    image_vis = train_dataset_vis[n][0].astype('uint8')\n    image, gt_mask = train_dataset[n]\n    \n    gt_mask = gt_mask.squeeze()\n    \n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    pr_mask = best_model.predict(x_tensor)\n    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n        \n    visualize(\n        image=image_vis, \n        ground_truth_mask=gt_mask,\n        predicted_mask=pr_mask\n    )","0a16594e":"import tqdm\n\nbest_model.eval()\nTTA_STEPS = 10\n\npredictions = np.zeros((len(test_dataloader), 320, 320))\nwith torch.no_grad():\n    for i in tqdm.tqdm(range(TTA_STEPS)):\n        results = []\n        for x in test_dataloader:\n            x = x.to(DEVICE)\n            prediction = best_model.forward(x)\n            results.append(prediction.cpu().numpy())\n        results = np.transpose(np.concatenate(results), (0, 2, 3, 1))[:, :, :, 0]\n        predictions += results \/ TTA_STEPS","84dede2c":"plt.imshow(predictions[10])","f97bb141":"rescaled = []\nfor im in predictions:\n    im = cv2.resize(im, (400, 400))\n    rescaled.append(im)\nrescaled = np.stack(rescaled)\npredictions = rescaled\nprint(predictions.shape)\n\nsubmission = np.reshape(predictions, (-1,))\nsubmission = (predictions > 0.5).astype(np.int8)\nnp.save('submission_tta.npy', submission)","6a00176f":"Data augmentation is a powerful technique to increase the amount of your data and prevent model overfitting.  \nIf you not familiar with such trick read some of these articles:\n - [The Effectiveness of Data Augmentation in Image Classification using Deep\nLearning](http:\/\/cs231n.stanford.edu\/reports\/2017\/pdfs\/300.pdf)\n - [Data Augmentation | How to use Deep Learning when you have Limited Data](https:\/\/medium.com\/nanonets\/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced)\n - [Data Augmentation Experimentation](https:\/\/towardsdatascience.com\/data-augmentation-experimentation-3e274504f04b)\n\nSince our dataset is very small we will apply a large number of different augmentations:\n - horizontal flip\n - affine transforms\n - perspective transforms\n - brightness\/contrast\/colors manipulations\n - image bluring and sharpening\n - gaussian noise\n - random crops\n\nAll this transforms can be easily applied with [**Albumentations**](https:\/\/github.com\/albu\/albumentations\/) - fast augmentation library.\nFor detailed explanation of image transformations you can look at [kaggle salt segmentation exmaple](https:\/\/github.com\/albu\/albumentations\/blob\/master\/notebooks\/example_kaggle_salt.ipynb) provided by [**Albumentations**](https:\/\/github.com\/albu\/albumentations\/) authors.","12c0c2fc":"# Predictions with Test Time augmentation","9b281757":"### Dataloader\n\nWriting helper class for data extraction, tranformation and preprocessing  \nhttps:\/\/pytorch.org\/docs\/stable\/data","b0024156":"\n## Visualize predictions","ff084104":"## Loading data","961a2779":"## Test best saved model","ca382ff1":"For this example we will use **CamVid** dataset. It is a set of:\n - **train** images + segmentation masks\n - **validation** images + segmentation masks\n - **test** images + segmentation masks\n \nAll images have 320 pixels height and 480 pixels width.\nFor more inforamtion about dataset visit http:\/\/mi.eng.cam.ac.uk\/research\/projects\/VideoRec\/CamVid\/.","5e48c9c4":"### Augmentations","094a0b99":"## Create model and train"}}