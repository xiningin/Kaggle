{"cell_type":{"a4d5d7b6":"code","3272eb9e":"code","248d8d38":"code","aac5a3d5":"code","3fd52555":"code","bc1ebc45":"code","02c4f10e":"code","386ffcc9":"code","6d84656c":"code","b253ac0f":"code","1046540c":"code","98792a17":"code","ae0ad476":"code","2481dea4":"code","1b18fad1":"code","e07996e8":"code","686b6719":"code","674c767a":"code","56358a05":"code","5ed389c1":"code","38095d32":"code","5b61a493":"code","2a438206":"code","1e5f2a45":"code","6f6ea40c":"code","218412ff":"code","11e98ee6":"code","8969d3e3":"code","f2dac7ee":"code","6e183c66":"code","64107428":"code","8e4ef98c":"markdown","5f2bedb8":"markdown","e727daf4":"markdown","e701add9":"markdown","46fb495e":"markdown"},"source":{"a4d5d7b6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3272eb9e":"data = pd.read_csv(\"..\/input\/abcnews-date-text.csv\", error_bad_lines=False)","248d8d38":"data.head()","aac5a3d5":"text = data[['headline_text']]","3fd52555":"text.head()","bc1ebc45":"text['index'] = text.index","02c4f10e":"text.head()","386ffcc9":"documents = text","6d84656c":"documents.head()","b253ac0f":"print(\"Total length of the documents: {}\".format(len(documents)))","1046540c":"# importing the gensim and nltk libraries\n\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\n\nimport nltk\nnp.random.seed(42)","98792a17":"def preprocessing(sentence):\n    stemmer = SnowballStemmer('english')\n    return stemmer.stem(WordNetLemmatizer().lemmatize(sentence, pos='v'))\n\ndef preprocess(sentence):\n    result = []\n    \n    for token in gensim.utils.simple_preprocess(sentence):\n        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n            result.append(preprocessing(token))\n            \n    return result","ae0ad476":"sample = documents[documents['index'] == 4310].values[0][0]\n\nprint(\"Sample document is selected for pre-processing: {}\".format(sample))","2481dea4":"words = []\n\nfor word in sample.split(' '):\n    words.append(word)\n    \nprint(\"Words found after splitting the sample document: {}\".format(words))","1b18fad1":"print(\"Tokenized and lemmatized document: {}\".format(preprocess(sample)))","e07996e8":"# pre-processing all the documents\n\npreprocessed_documents = documents['headline_text'].map(preprocess)","686b6719":"preprocessed_documents[:10]","674c767a":"# creating a dictionary from the above processed documents\n\ndictionary = gensim.corpora.Dictionary(preprocessed_documents)","56358a05":"count = 0\n\nfor key, value in dictionary.iteritems():\n    print(\"Key: {} and Value: {}\".format(key, value))\n    count += 1\n    \n    if count > 10:\n        break","5ed389c1":"# filter out extreme tokens in the document\n\ndictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)","38095d32":"bag_of_words = [dictionary.doc2bow(document=document) for document in preprocessed_documents]","5b61a493":"bag_of_words[4310]","2a438206":"## preview of bag of words of our sample preprocessed document\n\nsample_bag_of_words = bag_of_words[4310]\n\nfor i in range(len(sample_bag_of_words)):\n    print(\"Word: {} (\\\"{}\\\") appears: {} times.\".format(sample_bag_of_words[i][0], dictionary[sample_bag_of_words[i][0]], sample_bag_of_words[i][1]))","1e5f2a45":"from gensim import corpora, models\n\ntfidf = models.TfidfModel(bag_of_words)\ncorpus_tfidf = tfidf[bag_of_words]\n\nfrom pprint import pprint\n\nfor document in corpus_tfidf:\n    pprint(document)\n    break","6f6ea40c":"# training our model using gensim LdaMulticore\n\nmodel = gensim.models.LdaMulticore(bag_of_words, num_topics=10, id2word=dictionary, passes=2, workers=2)","218412ff":"for index, topic in model.print_topics(-1):\n    print(\"Topic: {} \\n Words: {}\".format(index, topic))","11e98ee6":"tfidf_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n\nfor index, topic in tfidf_model.print_topics(-1):\n    print(\"Topic: {}, Words: {}\".format(index, topic))","8969d3e3":"preprocessed_documents[4310]","f2dac7ee":"for index, score in sorted(tfidf_model[bag_of_words[4310]], key=lambda tup: -1 * tup[1]):\n    print(\"\\nScore: {} \\t \\nTopic: {}\".format(score, tfidf_model.print_topics(index, 10)))","6e183c66":"test_document = \"How a Pentgon deal became an identity crisis for Google\"\n\nbag_of_words_vector = dictionary.doc2bow(preprocess(test_document))","64107428":"for index, score in sorted(tfidf_model[bag_of_words_vector], key=lambda tup: -1 * tup[1]):\n    print(\"Score: {} \\t Topic: {}\\n\".format(score, tfidf_model.print_topics(index, 5)))","8e4ef98c":"### Testing on unseen document","5f2bedb8":"## Data Pre-Processing","e727daf4":"# Running LDA using Bag of words ","e701add9":"# TF-IDF","46fb495e":"## Evaluation by classifing simple document using LDA Bag of words model"}}