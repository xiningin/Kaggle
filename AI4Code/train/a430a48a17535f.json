{"cell_type":{"4ae9d82d":"code","c1ee9276":"code","510d97b7":"code","fbae4792":"code","b887e22e":"code","a0fa9ae2":"code","c3213398":"code","8557b688":"markdown"},"source":{"4ae9d82d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1ee9276":"#Load Data\nx_train = pd.read_csv('\/kaggle\/input\/churn-model-data-set-competition-form\/X_train.csv')\ny_train = pd.read_csv('\/kaggle\/input\/churn-model-data-set-competition-form\/y_train.csv') #label column = 'Exited'","510d97b7":"#EDA\n#1.\ubcc0\uc218 \ud0d0\uc0c9\nprint('\ubcc0\uc218\\n', x_train.columns.item, '\\n')\nprint(x_train.info(),'\\n')\nprint('***************************************************')\n#2.Null \uac12 \ud655\uc778\nprint('Null \uac12 \ud655\uc778')\nprint(x_train.isnull().sum())\nprint('***************************************************')\n#3.Categorical \/ Numeric Variable \ub098\ub204\uae30\ncols = x_train.columns\ncat = [col for col in cols if x_train[col].dtype == 'object']\nnum = [col for col in cols if (x_train[col].dtype == 'int') | (x_train[col].dtype == 'float')]\nprint('Categorical \/ Numeric Variable \ub098\ub204\uae30')\nprint('Categorical Variable : ', len(cat), '\\nNumeric Variable : ', len(num))\nprint('***************************************************')\n#4.Categorical Variable \ud0d0\uc0c9\n#4-1.nunique\nprint('Categorical Variable nunique')\nfor cat_ in cat:\n    print(cat_,':',x_train[cat_].nunique())\nprint('***************************************************')\n#5.Numerical \ud0d0\uc0c9\n#5-1.Outlier \ud655\uc778\nprint('Outlier \ud655\uc778')\noutlier_list = []\ndef outlier_bound(df):\n    Q1, Q3 = df.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    min_bound = Q1 - IQR * 1.5\n    max_bound = Q3 + IQR * 1.5\n    return min_bound, max_bound\nfor col_ in num:\n    min_bound, max_bound = outlier_bound(x_train[col_])\n    outlier_idx = x_train[col_].loc[(x_train[col_] < min_bound) | (x_train[col_] > max_bound)].index\n    if len(outlier_idx) > 0:\n        outlier = {'column_name' : col_, 'idx' : outlier_idx}\n        outlier_list.append(outlier)\nprint(outlier_list)\nprint('***************************************************')\n#5-1.Correlation \ud655\uc778\nprint('\ubcc0\uc218\ubcc4 \uc0c1\uad00\uad00\uacc4 \ud655\uc778')\nprint(x_train.corr())\nprint('***************************************************')\n#6. Label Balance \ud655\uc778\nprint('Label Balance \ud655\uc778')\nprint(y_train['Exited'].value_counts())","fbae4792":"#Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\n#CustomerId, Surname \uc0ad\uc81c\nx_train.drop(['CustomerId', 'Surname'], axis =1, inplace = True)\nnum.remove('CustomerId')\ncat.remove('Surname')\n#Age \ubc94\uc8fc\ud654 \uc2e4\uc2dc (10\ub300\ubd80\ud130 90\ub300 \uae4c\uc9c0 \ubd84\ud3ec)\ncount, bins_divders = np.histogram(x_train['Age'], bins = 9)\nbins_names = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\nx_train['Age'] = pd.cut(x = x_train['Age'], bins = bins_divders, labels = bins_names, include_lowest = True)\n#Outlier \uc0ad\uc81c\nidx_list = []\nfor idx_ in outlier_list:\n    idx_list.append(idx_['idx'])\nidx_list = [j for i in idx_list for j in i]\nidx_list = set(idx_list)\nidx_list = list(idx_list)\nprint('Outlier \uc0ad\uc81c\uc804 Data length : ', len(x_train))\nx_train.drop(idx_list, axis = 0, inplace = True)\ny_train.drop(idx_list, axis = 0, inplace = True)\nprint('Outlier length : ', len(idx_list))\nprint('Outlier \uc0ad\uc81c\ud6c4 Data length : ', len(x_train))\n","b887e22e":"#Encoding & Scaling\nencoder = OneHotEncoder()\nscaler = MinMaxScaler()\npreprocessor = ColumnTransformer([\n    ('encoder', encoder, cat),\n    ('scaler', scaler, num)\n])\nx_train = preprocessor.fit_transform(x_train)\n#SMOTE\ub85c oversampling\nsm = SMOTE(random_state = 26)\nsm_x_train, sm_y_train = sm.fit_resample(x_train, y_train['Exited'])\nprint('Oversampling \uc804 Data shape\\X data : {}, y data : {}'.format(len(x_train), len(y_train)))\nprint('Oversampling \ud6c4 Data shape\\X data : {}, ny data : {}'.format(len(sm_x_train), len(sm_y_train)))\n#Data Split\nX_train, X_test, y_train, y_test = train_test_split(sm_x_train, sm_y_train, test_size = 0.33, random_state = 26)\nprint('Data Shape', X_train.shape, X_test.shape, y_train.shape, y_test.shape)","a0fa9ae2":"#Modeling & Evaluation (RandomForest)\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nrf_clf = RandomForestClassifier(random_state = 26)\nrf_clf.fit(X_train, y_train)\nprobs = rf_clf.predict_proba(X_test)[:,1]\npreds = rf_clf.predict(X_test)\nauc_score = roc_auc_score(y_test, probs)\nacc_score = accuracy_score(y_test, preds)\nprint('RandomForest Training Set Evaluation')\nprint('AUC : {:.4f}'.format(auc_score))\nprint('Acurracy : {:.4f}'.format(acc_score))\n\nsvm_clf = SVC(probability=True, random_state = 26)\nsvm_clf.fit(X_train, y_train)\nprobs = svm_clf.predict_proba(X_test)[:,1]\npreds = svm_clf.predict(X_test)\nauc_score = roc_auc_score(y_test, probs)\nacc_score = accuracy_score(y_test, preds)\nprint('\\nSVM Training Set Evaluation')\nprint('AUC : {:.4f}'.format(auc_score))\nprint('Acurracy : {:.4f}'.format(acc_score))","c3213398":"#Submittion\nx_sub = pd.read_csv('\/kaggle\/input\/churn-model-data-set-competition-form\/X_test.csv')\ny_sub = pd.read_csv('\/kaggle\/input\/churn-model-data-set-competition-form\/test_label\/y_test.csv')\n#CustomerId, Surname \uc0ad\uc81c\nx_sub.drop(['CustomerId', 'Surname'], axis =1, inplace = True)\n#Age \ubc94\uc8fc\ud654 \uc2e4\uc2dc (10\ub300\ubd80\ud130 90\ub300 \uae4c\uc9c0 \ubd84\ud3ec)\ncount, bins_divders = np.histogram(x_sub['Age'], bins = 9)\nbins_names = ['1', '2', '3', '4', '5', '6', '7', '8', '9']\nx_sub['Age'] = pd.cut(x = x_sub['Age'], bins = bins_divders, labels = bins_names, include_lowest = True)\n#Encoding & Scaling\ny_sub = np.array(y_sub['Exited'], dtype = 'int')\nx_sub = preprocessor.fit_transform(x_sub)\nprobs = rf_clf.predict_proba(x_sub)[:,1]\npreds = rf_clf.predict(x_sub)\nauc_score = roc_auc_score(y_sub, probs)\nacc_score = accuracy_score(y_sub, preds)\nprint('RF_Result')\nprint('AUC : {:.4f}'.format(auc_score))\nprint('Acurracy : {:.4f}'.format(acc_score))\n\n\nprobs = svm_clf.predict_proba(x_sub)[:,1]\npreds = svm_clf.predict(x_sub)\nauc_score = roc_auc_score(y_sub, probs)\nacc_score = accuracy_score(y_sub, preds)\nprint('\\nSVM_Result')\nprint('AUC : {:.4f}'.format(auc_score))\nprint('Acurracy : {:.4f}'.format(acc_score))","8557b688":"**EDA Insight**\n1. CustomerId\ub294 \ubb34\uc758\ubbf8\ud55c \ubcc0\uc218\ub85c \uc0ad\uc81c\ud544\uc694\n2. Null value\ub294 \uc5c6\uc74c\n3. Surname\uc740 \uc0ad\uc81c \n4. Outlier \uc874\uc7ac\ud568\uc73c\ub85c \uc0ad\uc81c (\ub2e8\uc21c \uc0ad\uc81c)\n5. \uac15\ud55c\uc0c1\uad00\uad00\uacc4\ub97c \uac00\uc9c0\ub294 \ubcc0\uc218\uc5c6\uc74c\n6. imbalanced label\uacfc row \uac2f\uc218\ub97c \uace0\ub824\ud574 oversampling \ud544\uc694 (SMOTE or \ub2e8\uc21c\uc99d\ub300)\n7. Age \ubcc0\uc218\ub294 \ubc94\uc8fc\ud654 \uc2e4\uc2dc \ud544\uc694"}}