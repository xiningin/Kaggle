{"cell_type":{"c2c91458":"code","77a98739":"code","afe351bb":"code","866668ea":"code","443ec347":"code","17975a75":"code","c5a00f4e":"code","fc25cdcf":"code","527c1801":"code","843fc4e8":"code","60f26d0f":"code","23fb4f9e":"code","6f673902":"code","b4de19e4":"code","1cacff99":"code","1e896e20":"code","15817b7d":"code","a1d29a92":"code","8be0cfc1":"code","ca897ad1":"code","f99c790c":"code","b02411d4":"code","163f8806":"code","be05d03a":"code","6922e791":"code","2bd69267":"code","4d486c1a":"code","ea25cc64":"code","9e03b5ca":"code","e4bc3373":"markdown","dad5cd96":"markdown"},"source":{"c2c91458":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","77a98739":"# My code begins from here........\n\ndf_train = pd.read_csv('\/kaggle\/input\/khanan\/Training.csv', skiprows = [2, 1])\ndf_test = pd.read_csv('\/kaggle\/input\/khanan\/Test.csv', skiprows = [2, 1])\nprint(df_test.shape, df_train.shape)","afe351bb":"df = df_train.append(df_test)\ndf.shape","866668ea":"df","443ec347":"df_act = df.drop('Selected', axis=1)","17975a75":"df_act = df_act.drop('HP', axis=1)\ntargets = df['HP']","c5a00f4e":"print(df_act, targets)\nprint(df_act.shape, targets.shape)","fc25cdcf":"df_act.head()","527c1801":"df_act.info()\n# basic info","843fc4e8":"df_act.isnull().sum()\n# that means there are no missing values","60f26d0f":"# distributions......\n\ndf_act.hist(figsize=(16, 16), bins=100, xlabelsize=12, ylabelsize=12);","23fb4f9e":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 5))\nsns.distplot(targets, color='b', bins=200, hist_kws={'alpha': 0.5});\n\n# most of the target values are between 12k and 14k","6f673902":"# distribution of the data using seaborn (basically the same thing as panda's hist)\ndims = (20, 10)\nfig, axes = plt.subplots(ncols=3, nrows=2, figsize=dims)\n\nfor i, ax in zip(range(6), axes.flat):\n    sns.distplot(df_act[df_act.columns[i]], ax=ax)\nplt.show()","b4de19e4":"for i in range(6):\n    for ii in range(6):\n        if i != ii:\n            x = i\n            y = ii\n            sns.scatterplot(df_act[df_act.columns[x]], df_act[df_act.columns[y]], markers='.', data=df_act, hue=targets)\n        plt.show()","1cacff99":"fig, axes = plt.subplots(1, 6, figsize=(15, 5))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_act.columns):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        sns.countplot(x = df_act.columns[i], alpha = 0.6, data = df_act, ax = ax)\n\nfig.tight_layout()","1e896e20":"\n# for establishing pairwise correlation between any two labels\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(df_act.corr(), annot=True)","15817b7d":"# preprocessing\n\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nmms.fit(df_act)\ndata_scaled = mms.transform(df_act)\n\n#NOTE ! while you try to use this model on new data you need to down scale the targets with max = 14206.0.\n#Else there would be ambiguity in the predictions\n\nmax = np.max(targets)\ntarget = targets\/max","a1d29a92":"print(target)\nprint(data_scaled)\nprint(max)","8be0cfc1":"# splittng the data in the form it was received\nfrom sklearn.model_selection import train_test_split\n\nX_train_all, X_test, y_train_all, y_test = train_test_split(data_scaled, target, test_size=0.2)\n\n# further splitting into validation set for training purpose\nX_train, X_val, y_train, y_val = train_test_split(X_train_all, y_train_all, test_size=0.1)","ca897ad1":"hidden_layers = 3 \nrate_of_learning = 1e-3 \n\nfrom tensorflow import keras\n\ndef model_for_tuning(layer1, layer2, layer3, hiddenN = hidden_layers, learnR = rate_of_learning, input_shape = [6]):\n    model = keras.models.Sequential()\n    model.add(keras.layers.InputLayer(input_shape=input_shape))\n    for layers in range(hiddenN):\n        model.add(keras.layers.Dense(layer1, activation=\"relu\"))\n        model.add(keras.layers.Dense(layer2, activation=\"relu\"))\n        model.add(keras.layers.Dense(layer3, activation=\"relu\"))\n    model.add(keras.layers.Dense(1))\n\n    optim = keras.optimizers.Nadam(lr = learnR)\n    model.compile(loss=\"mse\", optimizer = optim, metrics = [\"mape\"])\n\n    return model","f99c790c":"from scipy.stats import reciprocal\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# defining our hyperparameter space\nparams_set = {\n    \"learnR\": reciprocal(1e-5, 1e-1),\n    \"layer1\": np.arange(1, 200),\n    \"layer2\": np.arange(1, 200),\n    \"layer3\": np.arange(1, 200)\n}\n\n# wrapping\nreg_keras = keras.wrappers.scikit_learn.KerasRegressor(model_for_tuning)\n\n# using the 'best hyperparameters finder' module\nrsCV = RandomizedSearchCV(reg_keras, params_set, n_iter=10, cv=3)\nrsCV.fit(X_train, y_train, epochs=200, callbacks=[keras.callbacks.EarlyStopping(patience = 100)])\n\n","b02411d4":"print(rsCV.best_params_,\"\\n\\n\\n\", rsCV.best_score_)\n\n# model_ = rsCV.best_estimator_.model -> is throwing an unknown error\n","163f8806":"\nlearnR = 4.11747983602376e-05","be05d03a":"#model\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=[6]))\nfor layers in range(3):\n    model.add(keras.layers.Dense(155, activation=\"relu\"))\n    model.add(keras.layers.Dense(117, activation=\"relu\"))\n    model.add(keras.layers.Dense(140, activation=\"relu\"))\nmodel.add(keras.layers.Dense(1))\noptim = keras.optimizers.Nadam(lr = learnR)\nmodel.compile(loss=\"mse\", optimizer = optim, metrics = [\"mape\"])","6922e791":"model.fit(X_train, y_train, epochs=5000, validation_data = (X_val, y_val), callbacks=[keras.callbacks.EarlyStopping(patience = 1000)])","2bd69267":"#evaluation\nmodel.evaluate(X_test, y_test)","4d486c1a":"# predicting\npredictions = model.predict(X_test)\n\nprint('Predicted\\n', predictions[:20], '\\n\\nActual\\n', y_test.values[:20])","ea25cc64":"# predictions vs actual graphs \n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 5))\nplt.scatter(predictions*max, y_test*max, marker='.')\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","9e03b5ca":"# upscalling the predictions and actual targets\n\na = list(predictions*max)\nb = list(y_test*max)\n\n\ndata_frame = pd.DataFrame(a,columns=['Predictions'])\ndata_frame['Actual'] = pd.Series(b, index=data_frame.index)\n\ndata_frame.to_csv('model1.csv')\n","e4bc3373":"# EDA","dad5cd96":"# Notebook begins"}}