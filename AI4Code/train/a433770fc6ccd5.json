{"cell_type":{"9404b1aa":"code","66895ccf":"code","0f32e702":"code","8c6e26fb":"code","f58b4dbb":"code","3e06a019":"code","0551a40b":"code","9daa594a":"code","9bba6647":"code","00f061d7":"code","a5c63f7e":"code","31820cea":"code","70794626":"code","7aeb4a92":"code","663f244e":"code","9418baa5":"code","d6213293":"code","5979d386":"code","c6d53af0":"code","7a79dc31":"code","f673898e":"code","fbb1b840":"code","99702d3a":"code","84fc0ba0":"code","13fbf7f5":"code","bc5da829":"code","0c84eabc":"code","ef1f5a08":"code","36972921":"code","493f0dba":"code","402f9e2d":"code","7f1d4ccd":"markdown","33eea5da":"markdown","7af08b31":"markdown"},"source":{"9404b1aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66895ccf":"dataset_encoder=\"ISO-8859-1\"\ndataset_column=['sentiments','id','date','flag','user','text']\ndataset=pd.read_csv(\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\",encoding=dataset_encoder,names=dataset_column)","0f32e702":"dataset","8c6e26fb":"# remove the unneccesory columns\ndataset=dataset[['sentiments','text']]","f58b4dbb":"dataset","3e06a019":"dataset['sentiments'].unique()","0551a40b":"dataset['sentiments']=dataset['sentiments'].replace(4,1)","9daa594a":"dataset['sentiments'].unique()","9bba6647":"dataset.tail(10)","00f061d7":"#Ploting\nplt=dataset.groupby('sentiments').count().plot(kind='bar',title='Data Distribution',legend=True)\nplt.set_xticklabels([\"Negative\",\"Positive\"],rotation=0)","a5c63f7e":"# store the data in list\ntext,sentiment=list(dataset['text']),list(dataset['sentiments'])","31820cea":"# PreProcess the data","70794626":"# Defining dictionary containing all emojis with their meanings.\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n# the data i am using here is taken from google you can simply go through this on google and get this much emojies in text form .\n","7aeb4a92":"import nltk\nnltk.download('stopwords')","663f244e":"from nltk.corpus import stopwords\nstopwords.words('english')","9418baa5":"# here we get all the words of stopwords ","d6213293":"# Defining set containing all stopwords in english.\nstopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n             \"youve\", 'your', 'yours', 'yourself', 'yourselves']","5979d386":"def preprocess(textdata):\n    processedText = []\n    \n    # Create Lemmatizer and Stemmer.\n    wordLemm = WordNetLemmatizer()\n    \n    # Defining regex patterns.\n    import re\n    urlPattern        = r\"((http:\/\/)[^ ]*|(https:\/\/)[^ ]*|( www\\.)[^ ]*)\"\n    userPattern       = '@[^\\s]+'\n    alphaPattern      = \"[^a-zA-Z0-9]\"\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    \n    for tweet in textdata:\n        tweet = tweet.lower()\n        \n        # Replace all URls with 'URL'\n        tweet = re.sub(urlPattern,' URL',tweet)\n        # Replace all emojis.\n        for emoji in emojis.keys():\n            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])  \n        \n        # Replace @USERNAME to 'USER'.\n        tweet = re.sub(userPattern,' USER', tweet)        \n        # Replace all non alphabets.\n        tweet = re.sub(alphaPattern, \" \", tweet)\n        # Replace 3 or more consecutive letters by 2 letter.\n        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n        tweetwords = ''\n        for word in tweet.split():\n            # Checking if the word is a stopword.\n            #if word not in stopwordlist:\n            if len(word)>1:\n                # Lemmatizing the word.\n                word = wordLemm.lemmatize(word)\n                tweetwords += (word+' ')\n            \n        processedText.append(tweetwords)\n        \n    return processedText\n \n    ","c6d53af0":"from nltk.stem import WordNetLemmatizer\nprocessedtext = preprocess(text)","7a79dc31":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n                                                    test_size = 0.05, random_state = 0)","f673898e":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report\nvectorization=TfidfVectorizer(ngram_range=(1,2),max_features=500000)\nvectorization.fit(X_train)","fbb1b840":"X_train = vectorization.transform(X_train)\nX_test  = vectorization.transform(X_test)","99702d3a":"# data evaluation\ndef model_Evaluate(model):\n    \n    # Predict values for Test dataset\n    y_pred = model.predict(X_test)\n\n    # Print the evaluation metrics for the dataset.\n    print(classification_report(y_test, y_pred))\n    \n    cf_matrix = confusion_matrix(y_test, y_pred)\n    print(cf_matrix)","84fc0ba0":"from sklearn.linear_model import LogisticRegression\nregressor = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\nregressor.fit(X_train, y_train)\nmodel_Evaluate(regressor)","13fbf7f5":"import pickle\nfile=open('vetorizer.pickle','wb')\npickle.dump(vectorization,file)\nfile.close()","bc5da829":"file1=open('Regressor.pickle','wb')\npickle.dump(regressor,file1)\nfile1.close()","0c84eabc":"def load_models():\n    file=open('.\/Regressor.pickle','rb')\n    regression=pickle.load(file)\n    file.close()\n    \n    file=open('.\/vetorizer.pickle','rb')\n    vectorizer=pickle.load(file)\n    file.close()\n    \n    return vectorizer,regression\n    \n    ","ef1f5a08":"def predict(vectoriser, model, text):\n    # Predict the sentiment\n    textdata = vectorization.transform(preprocess(text))\n    sentiment = model.predict(textdata)\n    \n    # Make a list of text with sentiment.\n    data = []\n    for text, pred in zip(text, sentiment):\n        data.append((text,pred))\n        \n    # Convert the list into a Pandas DataFrame.\n    df = pd.DataFrame(data, columns = ['text','sentiment'])\n    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n    return df\n\nif __name__==\"__main__\":\n    # Loading the models.\n    #vectoriser, LRmodel = load_models()\n    \n    # Text to classify should be in a list.\n    text = [\"I hate twitter\",\n            \"May the Force be with you.\",\n            \"Mr. Stark, I don't feel so good\"]\n    \n    df = predict(vectorization, regressor, text)\n    print(df.head())","36972921":"def predict(vectorization,model,text):\n    textdata=vectorization.transform(preprocess(text))\n    sentiment=model.predict(textdata)\n    \n    data=[]\n    for text,pred in zip(text,sentiment):\n        data.append((text,pred))\n    \n    #converting list into df\n    dataset_created=pd.DataFrame(data,columns=['text','sentiment'])\n    dataset_created= dataset_created.replace([0,1],['Negative','Positive'])\n    return dataset_created\n\nif __name__=='__main__':\n    text=[\"I Love my India\",\"I am very sad\",\"Good to know You are Fine\",\"See You Soon\",\"Ayush have a Lot Of Patience\"]\n    dataset_Predict=predict(vectorization,regressor,text)\n    dataset_Predict\n    \n        ","493f0dba":"dataset_Predict.to_csv(\"Predicted_result.csv\")","402f9e2d":"dataset_Predict","7f1d4ccd":"for preprocess we have to remove all the emojies and stopword and make all letter to small and many more things. Here we gono do the same step by step.\n","33eea5da":"# Using the Model","7af08b31":"**TF-IDF Vectoriser**\nTF-IDF indicates what the importance of the word is in order to understand the document or dataset. Let us understand with an example. Suppose you have a dataset where students write an essay on the topic, My House. In this dataset, the word a appears many times; it\u2019s a high frequency word compared to other words in the dataset. The dataset contains other words like home, house, rooms and so on that appear less often, so their frequency are lower and they carry more information compared to the word. This is the intuition behind TF-IDF.\n\nTF-IDF Vectoriser converts a collection of raw documents to a matrix of TF-IDF features. The Vectoriser is usually trained on only the X_train dataset.\n\nngram_range is the range of number of words in a sequence. [e.g \"very expensive\" is a 2-gram that is considered as an extra feature separately from \"very\" and \"expensive\" when you have a n-gram range of (1,2)]\n\nmax_features specifies the number of features to consider. [Ordered by feature frequency across the corpus]"}}