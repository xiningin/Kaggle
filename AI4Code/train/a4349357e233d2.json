{"cell_type":{"43bd17aa":"code","d874cf4f":"code","202c845f":"code","b1bbeb93":"code","5b1c65a3":"code","059f2dcd":"code","9e0c4fe9":"code","26c3d869":"code","c64e95ef":"code","92dbdf4f":"code","1439d86b":"code","1598dd1d":"code","b97bdcf8":"code","ed913768":"code","5a040512":"code","16dea84e":"code","8472ff56":"code","af073f6a":"code","9f083bba":"code","b96fcea0":"code","ea7f6d8a":"code","a79d7468":"code","fb9aac6d":"code","629ebca7":"code","b555897f":"code","d8cf1e1d":"code","309a22e9":"code","72131a13":"markdown","3dab4418":"markdown","066a6953":"markdown","36e67a29":"markdown","cc5c8b80":"markdown","bcaced52":"markdown","f3dae866":"markdown","db560e00":"markdown","cec95a80":"markdown","c41a1b59":"markdown","88f27c43":"markdown"},"source":{"43bd17aa":"!pip install umap-learn\n!pip install bokeh\n!pip install timm","d874cf4f":"import copy\nimport os\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom matplotlib import pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport seaborn as sns\nfrom tqdm.auto import tqdm\nfrom skimage import io\nfrom umap import UMAP\n\nfrom bokeh.plotting import ColumnDataSource, figure, output_file, show, output_notebook\nfrom bokeh.transform import factor_cmap, factor_mark\nfrom bokeh.palettes import Category10\n\nfrom pytorch_lightning.metrics.functional import accuracy\nfrom torch import nn\nimport timm\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\nimport torch.nn.functional as F\nfrom argparse import Namespace\n\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader\n\nimport base64\nfrom io import BytesIO\nfrom skimage import io, filters, measure, morphology, img_as_ubyte\nfrom matplotlib import cm\n\n%matplotlib inline\noutput_notebook()","202c845f":"class CassavaDataset(Dataset):\n    def __init__(self, root, image_ids, labels, transform=None):\n        super().__init__()\n        self.root = root\n        self.image_ids = image_ids\n        self.labels = labels\n        self.targets = self.labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, idx):\n        label = self.labels[idx]\n        img = io.imread(os.path.join(self.root, self.image_ids[idx]))\n\n        if self.transform:\n            img = self.transform(img)\n\n        return img, label","b1bbeb93":"class LeafDoctorModel(pl.LightningModule):\n    def __init__(self, hparams):\n        super().__init__()\n        self.hparams = hparams or Namespace()\n        self.trunk = timm.create_model('efficientnet_b0', pretrained=True, num_classes=5)\n\n    def forward(self, x):\n        return self.trunk(x)\n\n    def predict_proba(self, x):\n        probabilities = nn.functional.softmax(self.forward(x), dim=1)\n        return probabilities\n\n    def predict(self, x):\n        return torch.max(self.forward(x), 1)[1]\n\n    def configure_optimizers(self):\n        trainable_params = list(filter(lambda p: p.requires_grad, self.parameters()))\n        optimizer = torch.optim.AdamW(trainable_params,\n                                      lr=self.hparams.lr,\n                                      weight_decay=self.hparams.weight_decay)\n\n        return optimizer\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        acc = accuracy(y_hat, y)\n        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n        self.log(\"train_loss\", loss, on_step=True, on_epoch=False, prog_bar=False, logger=True)\n        return loss","5b1c65a3":"def get_wrapper(transforms):\n    def wraps(img):\n        return transforms(image=np.array(img))['image']\n    return wraps\n\ndef get_train_transforms(width, height):\n    train_transforms = A.Compose([\n        A.RandomResizedCrop(width, height, scale=(0.1, 0.8)),\n        A.ToFloat(max_value=1.0),\n        A.ShiftScaleRotate(p=0.5),\n        A.HorizontalFlip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        A.CoarseDropout(p=0.5),\n        A.Cutout(p=0.5),\n        ToTensorV2(),\n    ])\n\n    return get_wrapper(train_transforms)\n\n\ndef get_test_transforms(width, height):\n    test_transforms = A.Compose([\n        A.ToFloat(max_value=1.0),\n        A.CenterCrop(width, height),\n        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n        ToTensorV2(),\n    ])\n\n    return get_wrapper(test_transforms)","059f2dcd":"train_csv = pd.read_csv('..\/input\/cassava-leaf-disease-classification\/train.csv')\ndataset = CassavaDataset(root='..\/input\/cassava-leaf-disease-classification\/train_images', image_ids=train_csv.image_id.values, labels=train_csv.label.values)","9e0c4fe9":"parameters = dict(\n    max_epochs=10,\n    batch_size=32,\n    data_loader_workers=4,\n    \n    lr=0.001,\n    weight_decay=0.0001,\n    \n    train_width=256,\n    train_height=256,\n    test_width=300,\n    test_height=300,\n    \n    gpus=-1,\n    precision=16,\n    amp_level='02'\n)","26c3d869":"hparams = Namespace(**parameters)","c64e95ef":"model = LeafDoctorModel(hparams)","92dbdf4f":"train_transform = get_train_transforms(parameters['train_width'], parameters['train_height'])\n\ndataset.transform = train_transform\n\ntrain_loader = DataLoader(dataset,\n                            batch_size=parameters['batch_size'],\n                            num_workers=parameters['data_loader_workers'],\n                            shuffle=True,\n                            pin_memory=True)","1439d86b":"trainer = Trainer.from_argparse_args(\n        hparams,\n        reload_dataloaders_every_epoch=True,\n        terminate_on_nan=True,\n        precision=hparams.precision,\n        amp_level=hparams.amp_level,\n)\n\n\ntrainer.fit(model, train_loader)","1598dd1d":"model = model.to('cuda')\n\nextractor = LeafDoctorModel(hparams)\nextractor.load_state_dict(model.state_dict())\nextractor = extractor.eval().to('cuda')\nextractor.trunk.classifier = nn.Identity()","b97bdcf8":"dataset.transform = get_test_transforms(parameters['test_width'], parameters['test_height'])\nloader = torch.utils.data.DataLoader(dataset, batch_size=20, num_workers=4)","ed913768":"vectors = []\nlabels = []\npredictions = []\nfor batch in tqdm(loader): \n    images, batch_labels = batch\n    images = images.to('cuda')\n    predictions += model.predict(images).tolist()\n    \n    vectors += extractor(images).tolist()\n    labels += batch_labels.tolist()\n            \npredictions = np.array(predictions)\nlabels = np.array(labels)\nsuccess = np.equal(predictions, labels)","5a040512":"print('Train accuracy', sum(success)\/len(success))","16dea84e":"reducer = UMAP(random_state=42)\ntransformed = reducer.fit_transform(vectors)\ntr_x, tr_y = transformed[:, 0], transformed[:, 1]","8472ff56":"plt.figure(figsize=(15, 10))\nsns.scatterplot(x=tr_x, y=tr_y, hue=np.array(labels), palette=\"deep\")\nplt.axis('off')\nplt.title('UMAP of NN extracted features by class')\nplt.show()","af073f6a":"plt.figure(figsize=(15, 10))\nsns.scatterplot(x=tr_x, y=tr_y, hue=success, palette=\"deep\")\nplt.axis('off')\nplt.title('UMAP of NN extracted features by succesful classification')\nplt.show()","9f083bba":"label_num_to_disease_map = {0: \"Cassava Bacterial Blight (CBB)\", \n                            1: \"Cassava Brown Streak Disease (CBSD)\", \n                            2: \"Cassava Green Mottle (CGM)\", \n                            3: \"Cassava Mosaic Disease (CMD)\", \n                            4: \"Healthy\"}","b96fcea0":"base_dir = '..\/input\/cassava-leaf-disease-classification\/train_images'\nimg_paths = [os.path.join(os.getcwd(), base_dir, img_id) for img_id in dataset.image_ids]\ndescs = [f'{l.item()} ({label_num_to_disease_map[l.item()]})' for l in labels]","ea7f6d8a":"# Source: https:\/\/github.com\/jni\/blob-explorer\/blob\/master\/picker.py#L119\ndef to_png(arr):\n    out = BytesIO()\n    im = Image.fromarray(arr)\n    im.save(out, format='png')\n    return out.getvalue()\n\ndef b64_image_files(images, colormap='magma'):\n    cmap = cm.get_cmap(colormap)\n    urls = []\n    for im in tqdm(images):\n        #png = to_png(img_as_ubyte(cmap(im)))\n        png = to_png(im)\n        url = 'data:image\/png;base64,' + base64.b64encode(png).decode('utf-8')\n        urls.append(url)\n    return urls","a79d7468":"def images_generator(dataset):\n    for i in list(range(len(dataset)))[:100]:\n        img = dataset[i][0]\n        yield np.array(img)","fb9aac6d":"dataset.transform = None","629ebca7":"image_urls = b64_image_files(images_generator(dataset))","b555897f":"source = ColumnDataSource(data=dict(\n    x=tr_x,\n    y=tr_y,\n    label=[str(l) for l in labels],\n    prediction=[str(p) for p in predictions],\n    success=[str(s) for s in success],\n    desc=descs,\n    imgs=img_paths,\n    image_urls=image_urls,\n))","d8cf1e1d":"TOOLTIPS = \"\"\"\n    <div>\n        <div>\n            <img\n                src=\"@image_urls\" height=\"200\" alt=\"@image_urls\" width=\"200\"\n                style=\"float: left; margin: 0px 15px 15px 0px;\"\n                border=\"2\"\n            ><\/img>\n        <\/div>\n        <div>\n            <div style=\"font-size: 15px; font-weight: bold;\">Label: @label<\/div>\n            <div style=\"font-size: 15px; font-weight: bold;\">Predicted: @prediction<\/div>\n            <div style=\"font-size: 12px; font-weight: bold;\">Success: @success<\/div>\n            <div style=\"font-size: 12px;\">@desc<\/div>\n            <div style=\"font-size: 12px; color: #966;\">[$index]<\/div>\n        <\/div>\n    <\/div5\n\"\"\"","309a22e9":"p = figure(plot_width=1000, plot_height=600, tooltips=TOOLTIPS,\n           title=\"UMAP: Mouse over the dots\")\n\nmapper = factor_cmap(field_name='label', palette=Category10[5], factors=['0', '1', '2', '3', '4'])\n\np.scatter('x', 'y', \n         color=mapper,\n         marker=factor_mark('success', ['circle', 'x'], [str(True), str(False)]),\n         size=10, \n         fill_alpha=0.5,\n         legend_field=\"desc\",\n         source=source)\n\np.legend.orientation = \"vertical\"\np.legend.location = \"top_right\"\noutput_file(\"umap.html\")\nshow(p)","72131a13":"# Train a small CNN","3dab4418":"# Installs, imports and utils","066a6953":"# What the result will look like:","36e67a29":"# Obtain feature vectors for all training images","cc5c8b80":"Bokeh requires image urls to display pictures. But we can not display pictures from disk in a kaggle kernel. I encode pictures into base64 urls, so that we can display an interactive visualization.\n\nIf you are running this on local computer, it will be much faster to just use `img_paths` in place of `img_urls` below, to render pictures from disk. If you do this, the base64 encoding code could be removed.","bcaced52":"![Screenshot%20from%202020-12-30%2014-49-57.png](attachment:Screenshot%20from%202020-12-30%2014-49-57.png)","f3dae866":"# Reduce dimensionality using UMAP","db560e00":"# Static visualizations","cec95a80":"# Bokeh to go interactive","c41a1b59":"[UMAP](https:\/\/umap-learn.readthedocs.io\/en\/latest\/#) is a dimensionality reduction technique. In this notebook I will show how to apply it to the features extracted by an EfficientNet-b0 model.\nThis allows us to:\n* See if the model is able to separate classes. In other words see if it learned anything good.\n* Find outliers and clusters among images.\n* Check on which images our model makes mistakes.\n\n[Bokeh](https:\/\/docs.bokeh.org\/en\/latest\/index.html) is a Python library for interactive visualizations. Static 2D pictures are nice, but have you every wanted to hover over a dot and see what picture corresponds to it?\nI aim to reduce the features extracted by the CNN to 2 dimensions, plot all the points along these 2 dimensions.\n* Each dot is colored according to image label.\n* A dot replaced by a cross if the model made an error on this image.\n* Howering over a dot displays a tooltip with the image thumbnail and detailed information.\n\n\nI will not be aiming for a good model here, just a basic one to demonstrate the visualization. You can apply with your own models!","88f27c43":"### Done! Hope you found this useful!\n\nYou can also download `umap.html` to analyze locally."}}