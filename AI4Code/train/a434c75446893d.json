{"cell_type":{"494d62bf":"code","31c09dd7":"code","48ef6296":"code","98f0e65c":"code","c0b14417":"code","343b2db4":"code","2e099a5b":"code","86995708":"code","7442e4b0":"code","83e287f6":"code","1c952078":"code","d9ad913e":"code","6953aa7a":"code","1a30f98c":"code","49f53ff0":"code","e91efd46":"code","15d81b17":"code","3a45e1d7":"code","274f4be9":"code","3d4413d8":"code","66316be6":"code","e69af05c":"code","4d342152":"code","29334925":"code","ec655a91":"code","f464f90a":"code","ddbe3dfe":"code","8a662db3":"code","45b28e85":"code","83c88a13":"code","c86ea176":"code","5e83d33b":"code","bdc3f118":"code","932e7c5b":"code","69ae599f":"code","d6b8b9f1":"code","4ca7615f":"code","06991b90":"code","af2c90d3":"code","62cc85ef":"code","6f386299":"code","498414f9":"code","95c74464":"code","c39a856c":"code","953ed478":"markdown","fe7f7d99":"markdown","66cb67f5":"markdown","727689e9":"markdown","6ca5a7f8":"markdown","0494f7f5":"markdown","acfc02f1":"markdown","3d158530":"markdown","d39aaf6f":"markdown","e6ac2fef":"markdown","3abd5551":"markdown","79e28cee":"markdown","1484a7b1":"markdown","2f06a428":"markdown","22628548":"markdown","44385259":"markdown","972461fa":"markdown","b341d872":"markdown","fd002c28":"markdown","624a13ad":"markdown"},"source":{"494d62bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","31c09dd7":"import gensim\nfrom gensim import utils\nimport sys\nimport nltk\nfrom sklearn.datasets import fetch_20newsgroups\nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk import download\nfrom nltk.corpus import stopwords\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n%matplotlib inline","48ef6296":"#model Google News, run once to download pre-trained vectors\n!wget https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz","98f0e65c":"model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)","c0b14417":"download('punkt')\ndownload('stopwords')# stopwords\n\nstop_words = stopwords.words('english')\n\ndef preprocess(text):\n    text = text.lower()\n    doc = word_tokenize(text)\n    doc = [word for word in doc if word not in stop_words]\n    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n    return doc","343b2db4":"# Fetch ng20 dataset\nng20 = fetch_20newsgroups(subset='all',\n                          remove=('headers', 'footers', 'quotes'))\n# text and ground truth labels\ntexts, y = ng20.data, ng20.target\n\n\nprint(type(texts))","2e099a5b":"corpus=[preprocess(text) for text in texts]","86995708":"def filter_docs(corpus, texts, labels, condition_on_doc):\n    \"\"\"\n    Filter corpus, texts and labels given the function condition_on_doc which takes\n    a doc.\n    The document doc is kept if condition_on_doc(doc) is true.\n    \"\"\"\n    number_of_docs = len(corpus)\n\n    if texts is not None:\n        texts = [text for (text, doc) in zip(texts, corpus)\n                 if condition_on_doc(doc)]\n\n    labels = [i for (i, doc) in zip(labels, corpus) if condition_on_doc(doc)]\n    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n\n    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n\n    return (corpus, texts, labels)","7442e4b0":"corpus,texts,labels=filter_docs(corpus,texts,y,lambda doc: (len(doc) != 0))\n","83e287f6":"def document_vector(word2vec_model, doc):\n    # remove out-of-vocabulary words\n    doc = [word for word in doc if word in word2vec_model.vocab]\n   \n    return np.mean(word2vec_model[doc], axis=0)","1c952078":"def has_vector_representation(word2vec_model, doc):\n    \"\"\"check if at least one word of the document is in the\n    word2vec dictionary\"\"\"\n    return not all(word not in word2vec_model.vocab for word in doc)","d9ad913e":"corpus, texts, y = filter_docs(corpus, texts, y, lambda doc: has_vector_representation(model, doc))","6953aa7a":"x =[]\nfor doc in corpus: #look up each doc in model\n    x.append(document_vector(model, doc))","1a30f98c":"X = np.array(x) #list to array","49f53ff0":"np.save('documents_vectors.npy', X)  #np.savetxt('documents_vectors.txt', X)\nnp.save('labels.npy', y)             #np.savetxt('labels.txt', y)","e91efd46":"X.shape, len(y)","15d81b17":"train=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","3a45e1d7":"train.head()","274f4be9":"test.head()","3d4413d8":"submission.head()","66316be6":"text_train,y=list(train.text),list(train.target)","e69af05c":"def preprocess_twitter(text):\n    text = text.lower()\n    doc = word_tokenize(text)\n    return doc\n","4d342152":"corpus_train=[preprocess_twitter(text) for text in text_train]","29334925":"def document_vector_twitter(word2vec_model, doc):\n    # remove out-of-vocabulary words\n\n    docs=[]\n    for word in doc:\n        if word in word2vec_model.vocab:\n            docs.append(word)\n\n        else:\n            docs.append('fire')\n   # return np.mean(word2vec_model[docs], axis=0)\n# because some words in a sentence are more important than others so by averaging we will decrease their importance \n# so just sum it to preserve the importance\n    return np.sum(word2vec_model[docs], axis=0)","ec655a91":"x_train =[]\nfor doc in corpus_train: #look up each doc in model\n    x_train.append(document_vector_twitter(model, doc))","f464f90a":"print(type(x_train))\nprint(type(x_train[0]))\nprint(len(x_train))\nprint((x_train[0].shape))","ddbe3dfe":"from sklearn.linear_model import LogisticRegression\nmodel_final = LogisticRegression(C=4,max_iter=3000)\nmodel_final.fit(x_train,y)","8a662db3":"test_text=list(test.text)\nprint(len(test_text))\n# preprocessing\n# Not done complete preprocessing like removing stopwords and removing numbers etc\n# because removing these words actually removes those docs that just have one word and that too a stopword so once if we remove that word our doc will be [] and our model cant make doc2vec for this\n\ncorpus_test=[preprocess_twitter(text) for text in test_text]\nlen(corpus_test)","45b28e85":"x_test =[]\nfor doc in corpus_test: #look up each doc in model\n    x_test.append(document_vector_twitter(model, doc))","83c88a13":"print(len(x_test))","c86ea176":"prediction=list(model_final.predict(x_test))","5e83d33b":"submission['target']=prediction\n\nsubmission.to_csv('submission1.csv', index=False)\nsubmission.head(10)","bdc3f118":"import re\nimport string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n\n# function for removing stopwords\n\ndef remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","932e7c5b":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus_tester = [\n    'This is the first document deed.',\n    'This document is the second document.',\n    'And this is the third one.',\n    'Is this the first document?',\n]\n\n\n#preprocessing\ntext_train=[clean_text(text) for text in corpus_tester]\n\n\n# Tokenizing \ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\ntext_train=[tokenizer.tokenize(text) for text in text_train]\n\n\n# stopwords removal\ntext_train=[remove_stopwords(text) for text in text_train]\n\n\n#lemmatization\n\nlmtzr= WordNetLemmatizer() \n\n# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntext_train=[combine_text(text) for text in text_train]\n\n# print(text_train)\n\n\n\n\nvectorizer = TfidfVectorizer(use_idf=True)\ntfidf_matrix= vectorizer.fit_transform(text_train)\n\n\n\n\ndef tester(word2vec,doc):\n    trainer=[] # List that will contain document embedding for all docs\n    for i in range(len(ls)):\n            final_doc=[] # list that will contain document embedding for a single document\n\n            # getting tfidf matrix for given document\n            first_vector_tfidfvectorizer=tfidf_matrix[i]\n\n            # place tf-idf values in a pandas data frame\n            df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n            df.sort_values(by=[\"tfidf\"],ascending=False)\n\n#             print(df)\n\n            for j in range(len(ls[i])):\n\n                word=(ls[i][j]) # storing current word of present document\n                \n                word=lmtzr.lemmatize(word) # lemmatizing current word\n\n                word_vector=model[word] # getting array of shape (300,) from word2vec model of given word\n\n                tf_idf_vector=df['tfidf'][str(word)] # getting TfIdf score that current word from TfIdf dataframe created above\n                \n               \n                word_vector=word_vector*tf_idf_vector #mutiplying TfIdf score of a present word with complete array we got from word2vec model\n                \n\n                final_doc.append(word_vector)\n                \n              \n\n            final_doc=np.asarray(final_doc) # converting list of word2vec to doc2vec\n            final_doc=np.sum(final_doc,axis=0) \n            \n            trainer.append(final_doc)\n    return trainer\n    \n\n\n#before passing it to function            \nls=[]\nfor i in range(len(text_train)):\n    ls.append(text_train[i].split(' '))\n\n\n\nf=tester(model,ls)","69ae599f":"y=list(train.target)","d6b8b9f1":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus_tester = list(train.text)\n\n\n#preprocessing\ntext_train=[clean_text(text) for text in corpus_tester]\n\n\n# Tokenizing \ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\ntext_train=[tokenizer.tokenize(text) for text in text_train]\n\n\n# stopwords removal\ntext_train=[remove_stopwords(text) for text in text_train]\n\n\n#lemmatization\nlmtzr= WordNetLemmatizer() \n\n\n# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntext_train=[combine_text(text) for text in text_train]\n\n\n\n\n\n\nvectorizer = TfidfVectorizer(use_idf=True)\ntfidf_matrix= vectorizer.fit_transform(text_train)\n\n\n\n\ndef tester(word2vec,doc,target_var):\n    trainer=[]\n    for i,y in zip(range(len(ls)),target_var):\n            final_doc=[]\n\n            # get the first vector out (for the first document)\n            first_vector_tfidfvectorizer=tfidf_matrix[i]\n\n            # place tf-idf values in a pandas data frame\n            df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n            df.sort_values(by=[\"tfidf\"],ascending=False)\n            \n            \n\n            for j in range(len(ls[i])):\n\n                word=(ls[i][j])\n\n                word=lmtzr.lemmatize(word)\n                \n                try:\n\n                    word_vector=model[word]\n                    tf_idf_vector=df['tfidf'][str(word)]\n\n                    word_vector=word_vector*tf_idf_vector\n\n                    final_doc.append(word_vector)\n                except :\n                    word=str('fire') # I am considering that those sentence which were skipped and whose documnt shape was ()\n                    # I have considered that those sentences as True comments and all those sentences consist of single word 'Fire'\n                    # Because It was th most used as we have seen from wordcloud\n\n                    word=lmtzr.lemmatize(word)\n                        \n                    word_vector=model[word]\n                        \n                    tf_idf_vector=df['tfidf'][str(word)]\n\n                    word_vector=word_vector*tf_idf_vector\n\n                    final_doc.append(word_vector)\n                    \n                    \n                    \n\n                        \n\n                        \n            final_doc=np.asarray(final_doc) # converting list of word2vec to doc2vec\n            final_doc=np.sum(final_doc,axis=0) \n            trainer.append(final_doc)\n\n\n    return trainer\n    \n\n\n#before passing it to function            \nls=[]\nfor i in range(len(text_train)):\n    ls.append(text_train[i].split(' '))\n\n\nf=tester(model,ls,y)","4ca7615f":"for i,j in  enumerate(f):\n    if j.shape!=(300,):\n        print(i,\" and it's shape is \",j.shape)","06991b90":"from sklearn.feature_extraction.text import TfidfVectorizer\ncorpus_tester = list(train.text)\n\n\n#preprocessing\ntext_train=[clean_text(text) for text in corpus_tester]\n\n\n# Tokenizing \ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\ntext_train=[tokenizer.tokenize(text) for text in text_train]\n\n\n# stopwords removal\ntext_train=[remove_stopwords(text) for text in text_train]\n\n\n#lemmatization\nlmtzr= WordNetLemmatizer() \n\n\n# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntext_train=[combine_text(text) for text in text_train]\n\n\n\n\n\n\nvectorizer = TfidfVectorizer(use_idf=True)\ntfidf_matrix= vectorizer.fit_transform(text_train)\n\n\n\n\ndef tester(word2vec,doc,target_var):\n    trainer=[]\n    for i,y in zip(range(len(ls)),target_var):\n            final_doc=[]\n\n            # get the first vector out (for the first document)\n            first_vector_tfidfvectorizer=tfidf_matrix[i]\n\n            # place tf-idf values in a pandas data frame\n            df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n            df.sort_values(by=[\"tfidf\"],ascending=False)\n            \n            \n\n            for j in range(len(ls[i])):\n\n                word=(ls[i][j])\n\n                word=lmtzr.lemmatize(word)\n                \n                try:\n\n                    word_vector=model[word]\n                    tf_idf_vector=df['tfidf'][str(word)]\n\n                    word_vector=word_vector*tf_idf_vector\n\n                    final_doc.append(word_vector)\n                except :\n                    word=str('fire') # I am considering that those sentence which were skipped and whose documnt shape was ()\n                    # I have considered that those sentences as True comments and all those sentences consist of single word 'Fire'\n                    # Because It was th most used as we have seen from wordcloud\n\n                    word=lmtzr.lemmatize(word)\n                        \n                    word_vector=model[word]\n                        \n                    tf_idf_vector=df['tfidf'][str(word)]\n\n                    word_vector=word_vector*tf_idf_vector\n\n                    final_doc.append(word_vector)\n                    \n                    \n                    \n\n                        \n\n                        \n            final_doc=np.asarray(final_doc) # converting list of word2vec to doc2vec\n            final_doc=np.sum(final_doc,axis=0) \n            trainer.append(final_doc)\n\n\n    return trainer\n    \n\n\n#before passing it to function            \nls=[]\nfor i in range(len(text_train)):\n    ls.append(text_train[i].split(' '))\n\n\nf=tester(model,ls,y)","af2c90d3":"for i,j in  enumerate(f):\n    if j.shape!=(300,):\n        print(i,\" and it's shape is \",j.shape)","62cc85ef":"\nfrom sklearn.linear_model import LogisticRegression\nmodel_final1 = LogisticRegression(C=4,max_iter=5000)\nmodel_final1.fit(f,y)","6f386299":"test_text=list(test.text)\nprint(len(test_text))\n","498414f9":"\ntest_text=list(test.text)\n\n\n#preprocessing\ntest_text=[clean_text(text) for text in test_text]\n\n\n# Tokenizing \ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\ntest_text=[tokenizer.tokenize(text) for text in test_text]\n\n\n# stopwords removal\ntest_text=[remove_stopwords(text) for text in test_text]\n\n\n#lemmatization\nlmtzr= WordNetLemmatizer() \n\n\n# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntest_text=[combine_text(text) for text in test_text]\n\n\n\n\n\n\nvectorizer = TfidfVectorizer(use_idf=True)\ntfidf_matrix_test= vectorizer.fit_transform(test_text)\n\n\n\n\ndef tester_test(word2vec,doc):\n    trainer=[]\n    for i in range(len(ls)):\n            final_doc=[]\n\n            # get the first vector out (for the first document)\n            first_vector_tfidfvectorizer=tfidf_matrix_test[i]\n\n            # place tf-idf values in a pandas data frame\n            df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names(), columns=[\"tfidf\"])\n            df.sort_values(by=[\"tfidf\"],ascending=False)\n            \n            \n\n            for j in range(len(ls[i])):\n\n                word=(ls[i][j])\n\n                word=lmtzr.lemmatize(word)\n                \n                try:\n\n                    word_vector=model[word]\n                    tf_idf_vector=df['tfidf'][str(word)]\n\n                    word_vector=word_vector*tf_idf_vector\n\n                    final_doc.append(word_vector)\n                except :\n                    word=str('fire') # I am considering that those sentence which were skipped and whose documnt shape was ()\n                    # I have considered that those sentences as True comments and all those sentences consist of single word 'Fire'\n                    # Because It was th most used as we have seen from wordcloud\n\n                    word=lmtzr.lemmatize(word)\n                        \n                    word_vector=model[word]\n                        \n                    tf_idf_vector=df['tfidf'][str(word)]\n\n                    word_vector=word_vector*tf_idf_vector\n\n                    final_doc.append(word_vector)\n                    \n                    \n                    \n\n                        \n\n                        \n            final_doc=np.asarray(final_doc) # converting list of word2vec to doc2vec\n            final_doc=np.sum(final_doc,axis=0) \n            trainer.append(final_doc)\n\n\n    return trainer\n    \n\n\n#before passing it to function            \nls=[]\nfor i in range(len(test_text)):\n    ls.append(test_text[i].split(' '))\n\n\nf_test=tester_test(model,ls)","95c74464":"prediction=list(model_final1.predict(f_test))","c39a856c":"submission['target']=prediction\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(10)\n\n","953ed478":"## preprocessing Training Data","fe7f7d99":"## Tokenising and Stop words removal ","66cb67f5":"# Different Techniques for Document Embeddings","727689e9":"# Using same technique to our Problem Statement\n","6ca5a7f8":"## Removing Empty Documents","0494f7f5":"## Removing OOV","acfc02f1":"## Applying Same Technique as Mentioned in above cell","3d158530":"# 1. Averaging word Embeddings to get Document Embedings","d39aaf6f":"## Testing data comes in","e6ac2fef":"# Problem solved","3abd5551":"## Sources\n\n1. code for [Averaging word embeddings to get doc2vec](https:\/\/github.com\/sdimi\/average-word2vec\/blob\/master\/notebook.ipynb)\n\n2. Best Gensim Tutorial  [here](https:\/\/www.machinelearningplus.com\/nlp\/gensim-tutorial\/#14howtotrainword2vecmodelusinggensim)\n\n3. Idea for different document embeddings [here](https:\/\/stackoverflow.com\/questions\/47727078\/what-does-a-weighted-word-embedding-mean)","79e28cee":"## For this technque I am using word2vec embeddings\nThis word vectors is trained on google news and provided by Google. Based on 100 billion words from Google News data, they trained model with 300 dimensions.\nMikolov et al. use skip-gram and negative sampling to build this model which is released in 2013.","1484a7b1":"#### To solve those documents where no word is present due to exception I have tried to use a Trick by taking the help of wordcloud of some other kernel writer\n#### In that kernel it was mentioned that word Fire is most used word in these tweets so we will use that word for handling the exception","2f06a428":"### Testing our technique on small corpus","22628548":"![here](https:\/\/lh3.googleusercontent.com\/ob9WJ7wwgYnjQL-os7VDKdRGCEQD9xvXZHML-fiLhFcA0l8TljIco1YyYenZGz32VD0rO0g5pXh1Xj0nA3XvN6IsTeP8zvuUQKSztri3CFy2mQIAPcYAWGXpsJB02piqlScheF11WBHuUHZWLIqlW4RtcsUjka5ZsP7BMtvI3mgPZARKk32roIDbvqQbKll-4f4xNM96100QpKxiZEJNhSxMDkVVilbpS_6NvgCAbWt1wnxak8bwCLCDF0mAJ4IOjT_QTHj5GsTUoHlpyrgAx7CTPw7Lj4rvsnrcY85oIehR5FCcIFF_ORbMZColH0Tq0BznT4IEbBGfUbtUaDMM5tTRa67wGQ2R52W2_V64Mfsb4ccGBlp8UJv27TmZG06MUa1gQCxdt6GQDnqy_gtvz9LGmGqrRr_fbWdwjtygmhcOfpdf1sIcBlXaQGNueJLUjK_kHrctc8ohYKqG377AzRpZEtsUwaUqiczzc5kTBy7pnaS5-TN1aGcuLcBMyNGJ0Yi90Fp_oatV_-sVtnG1OF_-jN9YvkWlYPY2JPV_YJfwhjfHE6TY5SbVLt0O1eeefKHZfxHhbYGy38LxX_iu6ZgagACoxQs51Zdo5sEs-kydN8VFKRkr9ymcb36aWEB-O0BsHv_-4Ug1wH0wQCLxYATfYoZkCUDwXgSGWKBOKTJ5CfnI1YZ-Hq7nFMwe168=w663-h883-no)","44385259":"### Function for preprocessing","972461fa":"Defining Model","b341d872":"## Testing Data Is In The House","fd002c28":"![Image](https:\/\/lh3.googleusercontent.com\/ltBKoXtX2zCo8i6tZeXBaFgC-3om-2347OqNzEZnIcs0Qdb46JZ-KXWAINz6znaZjt8WYRd49R7O6WfLYCNoGIjO8pey5X7tdFXlna2zZ7fPv66A_eYRvqH4yG1Wc_0w7WkrfszYrAdEHr4MmjRSQuudzjDHKsmx-NM05WJuoYod94JP0HgtyGFkSeT718UbCT2h0C64d1S400lWAKr4ISlTMaR_ihi6dI4oxaOvA3PPTkV95oabHWTrIBbtIgUVXO-RUWRBRohU9NKkOvvr4JT5kP-DLkFhJvOJuFT4YoFo8dT_NfFZGYXyoUDTb2MCMuHD1hdguSTklDy9fJz5-xtg_ttMam8-s0ydK0tZm40R33_tNKERd1KKmo9alb03s9EuRcFAnVf8Wci9sJQr_EerNKOnY-h8_dnvSbpQbrzNFtFEzYLcN38xYx4p7c0QR-IGo_ZqG8IjAbwM8XVmf2fowmnvfbY0OpBaLNsmoXTjhPLN52p3Vg_GmnHznd7b-PI_MRzEVZ5qsKgLROujp85OAxLQ-NwgnnaPQB9mnnRACgyNm-nd-jObTy1fCbzFJ6v-YqBj9XPynwZuAMI4z5bxd32wNbpuOB24uLYbX18Wv4TUnwqae-zGB4TykS-E1ivbbXKKb1UbvszbuOgYt9UhQE5VMLQohWZnXibRBnPglwrPPft2FOUiRfYQTTU=w713-h950-no)","624a13ad":"# 2. Technique 2 TFIDF weightage For Document Emeddings Made By Word2vec"}}