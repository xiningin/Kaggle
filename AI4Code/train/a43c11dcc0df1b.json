{"cell_type":{"79ebef26":"code","9a07e419":"code","206860f3":"code","15247cff":"code","9dc8c5fe":"code","ea4a314b":"code","f028fe1b":"code","bec0a965":"code","29bc074a":"code","ea5e30c2":"code","44277c0e":"code","f387849f":"code","f118fa68":"code","690f7b24":"code","98504df2":"code","626f7166":"code","f3d87242":"code","3f6d26b7":"code","60162dbd":"code","9997e2ed":"code","97673945":"code","83aae0c0":"code","7a71adbc":"code","6943e1ac":"code","5ecae6a1":"code","3223f617":"code","1c81f8fd":"code","ff089719":"code","4562cda3":"code","bf2a85f4":"code","b1ea6823":"code","bccb62a1":"code","7b8d1f90":"code","7481960d":"code","2ed58604":"code","4afdfe20":"code","d9c40ef3":"code","2fd98db0":"code","b4d63ebd":"code","43e998c5":"code","a53afe80":"code","6ce35b5a":"code","c5639bc4":"code","88293446":"markdown","418961fe":"markdown","e58cceb8":"markdown","5af2f3c6":"markdown","bfd09bbd":"markdown","4ef1d57b":"markdown","5527b047":"markdown","c909fde0":"markdown","f6fc8bf7":"markdown","b0e73b22":"markdown","7214e977":"markdown","a2969d7a":"markdown","a72ed0b1":"markdown","976971d9":"markdown","6cff49e6":"markdown","102dc5dc":"markdown","728f5305":"markdown","f3b63fc6":"markdown","2f92e23b":"markdown","dca2fb76":"markdown","9c951649":"markdown","4fb0d7bd":"markdown","9a186031":"markdown","13bdbd1a":"markdown","4e8014fd":"markdown","e2adebe3":"markdown","1ff761ee":"markdown","0a705394":"markdown","7a3b2c30":"markdown","fc414281":"markdown","2435b588":"markdown","cedfd2e5":"markdown","027fbc56":"markdown","a7c90106":"markdown","bb37712d":"markdown","07e6906f":"markdown","abd230b6":"markdown","865595f2":"markdown"},"source":{"79ebef26":"import os\nimport random, re, math, gc\nfrom collections import Counter, defaultdict\n\nfrom PIL import Image\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom kaggle_datasets import KaggleDatasets\nAUTO = tf.data.experimental.AUTOTUNE\n\nprint(tf.__version__)\nprint(tf.keras.__version__)","9a07e419":"def seed_everything(seed=1234):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'","206860f3":"image_shape = (512,512,3)\nimage_quality = 100\nSEED = 1234\nseed_everything(SEED)\nBATCH_SIZE = 16 \n\ntrain_fold = 5\ntest_fold = 6\n\n\nBASE_PATH = '..\/input\/siim-isic-melanoma-classification'\ntrain_metadata = pd.read_csv(os.path.join(BASE_PATH, 'train.csv'))\ntest_metadata = pd.read_csv(os.path.join(BASE_PATH, 'test.csv'))\nsample_submission = pd.read_csv(os.path.join(BASE_PATH, 'sample_submission.csv'))","15247cff":"train_metadata.tail()","9dc8c5fe":"test_metadata.tail()","ea4a314b":"print('Missing Values in Train_metadata in(%): \\n')\nmissing_values = train_metadata.isnull().sum() \/ len(train_metadata)\nmissing_values = missing_values[missing_values>0.0]\nprint(missing_values)\n\nprint('\\n\\nMissing Values in Test_metadata : \\n')\nmissing_values = test_metadata.isnull().sum() \/ len(test_metadata)\nmissing_values = missing_values[missing_values>0.0]\nprint(missing_values)","f028fe1b":"print('Unique values in column with frequency : ')\n\nprint('\\nsex : ', dict(train_metadata.sex.value_counts()))\nprint('\\nage_approx : ', dict(train_metadata.age_approx.value_counts()))\nprint('\\nanatom_site_general_challenge : ', dict(train_metadata.anatom_site_general_challenge.value_counts()))\nprint('\\ndiagnosis : ', dict(train_metadata.diagnosis.value_counts()))\nprint('\\nbenign_malignant : ', dict(train_metadata.benign_malignant.value_counts()))\nprint('\\ntarget : ', dict(train_metadata.target.value_counts()))","bec0a965":"def get_stratify_group(row):\n    return '{}_{}_{}'.format(row['anatom_site_general_challenge'],row['sex'],row['target'])","29bc074a":"train = train_metadata.copy()\n\ntrain_cat_list_patient_id = train['patient_id'].astype('category').cat.categories\ntrain['patient_id'] = train.patient_id.astype('category').cat.codes\n\ntrain['sex'] = train['sex'].fillna('unknown')\n# cat_list_sex = train['sex'].astype('category').cat.categories\n# train['sex'] = train.sex.astype('category').cat.codes\n\ntrain['age_approx'] = train['age_approx'].fillna(train.age_approx.mean())\ntrain['age_approx'] = train['age_approx'].astype('int')\n\ntrain['anatom_site_general_challenge'] = train['anatom_site_general_challenge'].fillna(\"unknown\")\n# cat_list_anatom_site_general_challenge = train['anatom_site_general_challenge'].astype('category').cat.categories\n# train['anatom_site_general_challenge'] = train['anatom_site_general_challenge'].astype('category').cat.codes\n\ntrain['stratify_group'] = train.fillna(\"NA\").apply(get_stratify_group, axis=1)\ntrain['stratify_group'] = train['stratify_group'].astype('category').cat.codes","ea5e30c2":"test = test_metadata.copy()\n\ntest_cat_list_patient_id = test['patient_id'].astype('category').cat.categories\ntest['patient_id'] = test.patient_id.astype('category').cat.codes\n\ntest['anatom_site_general_challenge'] = test['anatom_site_general_challenge'].fillna(\"unknown\")\n# test_cat_list_anatom_site_general_challenge = test['anatom_site_general_challenge'].astype('category').cat.categories\n# test['anatom_site_general_challenge'] = test['anatom_site_general_challenge'].astype('category').cat.codes\n\ntest['target'] = 2\n\ntest['stratify_group'] = test.fillna(\"NA\").apply(get_stratify_group, axis=1)\ntest['stratify_group'] = test['stratify_group'].astype('category').cat.codes","44277c0e":"def stratified_group_k_fold(X, y, groups, k, seed=None):\n    labels_num = np.max(y) + 1\n    y_counts_per_group = defaultdict(lambda: np.zeros(labels_num))\n    y_distr = Counter()\n    for label, g in zip(y, groups):\n        y_counts_per_group[g][label] += 1\n        y_distr[label] += 1\n\n    y_counts_per_fold = defaultdict(lambda: np.zeros(labels_num))\n    groups_per_fold = defaultdict(set)\n\n    def eval_y_counts_per_fold(y_counts, fold):\n        y_counts_per_fold[fold] += y_counts\n        std_per_label = []\n        for label in range(labels_num):\n            label_std = np.std([y_counts_per_fold[i][label] \/ y_distr[label] for i in range(k)])\n            std_per_label.append(label_std)\n        y_counts_per_fold[fold] -= y_counts\n        return np.mean(std_per_label)\n    \n    groups_and_y_counts = list(y_counts_per_group.items())\n    random.Random(seed).shuffle(groups_and_y_counts)\n\n    for g, y_counts in sorted(groups_and_y_counts, key=lambda x: -np.std(x[1])):\n        best_fold = None\n        min_eval = None\n        for i in range(k):\n            fold_eval = eval_y_counts_per_fold(y_counts, i)\n            if min_eval is None or fold_eval < min_eval:\n                min_eval = fold_eval\n                best_fold = i\n        y_counts_per_fold[best_fold] += y_counts\n        groups_per_fold[best_fold].add(g)\n\n    all_groups = set(groups)\n    \n    for i in range(k):\n        train_groups = all_groups - groups_per_fold[i]\n        test_groups = groups_per_fold[i]\n\n        train_indices = [i for i, g in enumerate(groups) if g in train_groups]\n        test_indices = [i for i, g in enumerate(groups) if g in test_groups]\n\n        yield train_indices, test_indices","f387849f":"%%time\n\ntrain['fold'] = 0\n\n\nfor fold_ind, (train_ind, val_ind) in enumerate(stratified_group_k_fold(train_metadata, \n                                                                        train.stratify_group.values, \n                                                                        train_metadata.patient_id.values, \n                                                                        k=train_fold, \n                                                                        seed=SEED)):\n    train.loc[val_ind,'fold'] = fold_ind\n\ntrain.fold.value_counts()","f118fa68":"sex_code = pd.get_dummies(train.sex, prefix='sex')\nanatom_site_general_challenge_code = pd.get_dummies(train.anatom_site_general_challenge, prefix='anatom_site')\nage_aprox_normalized = (train.age_approx-train.age_approx.mean())\/train.age_approx.std()\ntrain_coded = pd.concat([train.image_name, sex_code, age_aprox_normalized ,anatom_site_general_challenge_code, train.target, train.fold], axis=1)","690f7b24":"train_coded.head()","98504df2":"print(len(train_coded.columns))\ntrain_coded.columns","626f7166":"%%time\n\ntest['fold'] = 0\n\nfor fold_ind, (test_ind, val_ind) in enumerate(stratified_group_k_fold(test_metadata, \n                                                                       test.stratify_group.values, \n                                                                       test_metadata.patient_id.values, \n                                                                       k=test_fold, \n                                                                       seed=SEED)):\n    test.loc[val_ind,'fold'] = fold_ind\n\ntest.fold.value_counts()","f3d87242":"sex_code = pd.get_dummies(test.sex, prefix='sex')\nanatom_site_general_challenge_code = pd.get_dummies(test.anatom_site_general_challenge, prefix='anatom_site')\nage_aprox_normalized = (test.age_approx-test.age_approx.mean())\/test.age_approx.std()\ntest_coded = pd.concat([test.image_name, sex_code, age_aprox_normalized ,anatom_site_general_challenge_code, test.fold], axis=1)\ntest_coded['sex_unknown'] = 0","3f6d26b7":"test_coded.head()","60162dbd":"print(len(test_coded.columns))\ntest_coded.columns","9997e2ed":"def _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float \/ double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))","97673945":"def get_path(image_name, _set='train'):\n    if _set == 'train':    \n        return BASE_PATH + '\/jpeg\/train\/' + str(image_name) + '.jpg'\n    else:\n        return BASE_PATH + '\/jpeg\/test\/' + str(image_name) + '.jpg'","83aae0c0":"def hair_remove(image):\n    # convert image to grayScale\n    grayScale = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n    \n    # kernel for morphologyEx\n    kernel = cv2.getStructuringElement(1,(17,17))\n    \n    # apply MORPH_BLACKHAT to grayScale image\n    blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n    \n    # apply thresholding to blackhat\n    _,threshold = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n    \n    # inpaint with original image and threshold image\n    final_image = cv2.inpaint(image,threshold,1,cv2.INPAINT_TELEA)\n    \n    return final_image","7a71adbc":"def row_serialize(row, image_shape, _set='train'):\n    \n    image_path = get_path(row['image_name'],_set=_set)\n    \n    ## Method -1\n    \n#     raw_image = tf.keras.preprocessing.image.load_img(image_path,\n#                                                       color_mode='rgb',\n#                                                       target_size=image_shape\n#                                                      ).tobytes()\n\n    ## Method-2 (with 10% compression)\n    \n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, image_shape[:2], method=\"nearest\")\n    \n    image = hair_remove(image.numpy())\n    raw_image = tf.image.encode_jpeg(image, quality=image_quality, optimize_size=True)\n    del image\n    \n    ## Method-3 (with 5% compression)\n    \n#     image = cv2.imread(image_path)\n#     image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n#     image = cv2.resize(image,image_shape[:2])\n#     result, encimg = cv2.imencode('.jpg', image, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n#     raw_image = encimg.tostring()\n#     del image,result\n\n    feature = {\n        # _bytes_feature\n        'image': _bytes_feature(raw_image),\n        'image_name': _bytes_feature(str.encode(row['image_name'])),\n        \n        # _int64_feature\n        'sex_female': _int64_feature(row['sex_female']),\n        'sex_male': _int64_feature(row['sex_male']),\n        'sex_unknown': _int64_feature(row['sex_unknown']),\n        'anatom_site_head\/neck': _int64_feature(row['anatom_site_head\/neck']),\n        'anatom_site_lower extremity': _int64_feature(row['anatom_site_lower extremity']),\n        'anatom_site_oral\/genital': _int64_feature(row['anatom_site_oral\/genital']),\n        'anatom_site_palms\/soles': _int64_feature(row['anatom_site_palms\/soles']),\n        'anatom_site_torso': _int64_feature(row['anatom_site_torso']),\n        'anatom_site_unknown': _int64_feature(row['anatom_site_unknown']),\n        'anatom_site_upper extremity': _int64_feature(row['anatom_site_upper extremity']),\n        \n        # _float_feature\n        'age_approx': _float_feature(row['age_approx']),\n      }\n    \n    \n    if _set=='train':\n        feature['target'] = _int64_feature(row['target'])\n\n    \n    row_feature = tf.train.Example(features=tf.train.Features(feature=feature))\n    return row_feature.SerializeToString()","6943e1ac":"train_fold_path = '..\/working\/train_fold_tfrecords_{}x{}'.format(image_shape[0],image_shape[1])\ntry:\n    os.mkdir(train_fold_path)\nexcept OSError as error: \n    print(error, \"\\nIt's OK You are Good to GO\")     ","5ecae6a1":"%%time\nMAX_size = 1024\nfor fold, group in train_coded.groupby('fold'):\n    Max_sub_files = len(group)\/\/MAX_size\n    print('\\nWriting Fold : ',fold)\n    for file_id in range(Max_sub_files+1):\n        \n        if file_id != Max_sub_files:\n            num_image = MAX_size\n        else:\n            num_image = len(group)%MAX_size\n            \n        with tf.io.TFRecordWriter(train_fold_path + '\/tain_fold_{}_{}-{}.tfrec'.format(fold,file_id,num_image)) as writer:\n#             print(file_id*MAX_size,file_id*MAX_size + num_image, num_image)\n            for i in range(file_id*MAX_size,file_id*MAX_size + num_image):\n                row_feature_serialize = row_serialize(group.iloc[i], image_shape, _set='train')\n                writer.write(row_feature_serialize)\n                del row_feature_serialize\n\n                if i%100 == 0:\n                    print(i,', ',end='')\n    ","3223f617":"gc.collect()","1c81f8fd":"test_fold_path = '..\/working\/test_fold_tfrecords_{}x{}'.format(image_shape[0],image_shape[1])\ntry:\n    os.mkdir(test_fold_path)\nexcept OSError as error: \n    print(error, \"\\nIt's OK You are Good to GO\")","ff089719":"%%time\nMAX_size = 1024\nfor fold, group in test_coded.groupby('fold'):\n    Max_sub_files = len(group)\/\/MAX_size\n    print('\\nWriting Fold : ',fold)\n    for file_id in range(Max_sub_files+1):\n        \n        if file_id != Max_sub_files:\n            num_image = MAX_size\n        else:\n            num_image = len(group)%MAX_size\n            \n        with tf.io.TFRecordWriter(test_fold_path + '\/test_fold_{}_{}-{}.tfrec'.format(fold,file_id,num_image)) as writer:\n#             print(file_id*MAX_size,file_id*MAX_size + num_image, num_image)\n            for i in range(file_id*MAX_size,file_id*MAX_size + num_image):\n                row_feature_serialize = row_serialize(group.iloc[i], image_shape, _set='test')\n                writer.write(row_feature_serialize)\n                del row_feature_serialize\n\n                if i%100 == 0:\n                    print(i,', ',end='')\n    ","4562cda3":"gc.collect()","bf2a85f4":"# TEMP CODE (NEED TO REMOVE)\n\n# train_fold = 6\n# test_fold = 2","b1ea6823":"all_train_tfrecords_path = tf.io.gfile.glob(train_fold_path + '\/*.tfrec')\ntest_tfrecords_path = tf.io.gfile.glob(test_fold_path + '\/*.tfrec')\n\nval_tfrecords_path = {}\ntrain_tfrecords_path = {}\n\nfor i in range(train_fold):\n    val_tfrecords_path['fold_{}'.format(i)] = [path for path in all_train_tfrecords_path if f\"tain_fold_{i}_\" in path] \n    \n    train_tfrecords_path['fold_{}'.format(i)] = list(set(all_train_tfrecords_path) - set(val_tfrecords_path['fold_{}'.format(i)]))\n\nfor t,v in zip(train_tfrecords_path.items(),val_tfrecords_path.items()):\n    print('\\n',t[0])\n    for path in t[1]:\n        print('Train :'+path)\n    print()\n    for path in v[1]:\n        print('validation :'+path)","bccb62a1":"def count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, \n    # i.e. tain_fold_1-1852.tfrec = 1852 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","7b8d1f90":"N_TRAIN_IMGS = {f'fold_{i}': count_data_items(train_tfrecords_path[f'fold_{i}'])\n                for i in range(train_fold)}\n\nN_VAL_IMGS = {f'fold_{i}': count_data_items(val_tfrecords_path[f'fold_{i}'])\n                for i in range(train_fold)}\n\nN_TEST_IMGS = count_data_items(test_tfrecords_path)\n\nprint(f\"number test image is {N_TEST_IMGS}. It is common for all folds.\")\nprint(\"-\"*75)\nfor i in range(train_fold):\n    print(\"-\"*75)\n    print(f\"Fold {i}: {N_TRAIN_IMGS[f'fold_{i}']} training and {N_VAL_IMGS[f'fold_{i}']} validation images.\")\nprint(\"-\"*75)","7481960d":"def decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    # convert image to floats in [0, 1] range\n    image = tf.cast(image, tf.float32) \/ 255.0 \n    # explicit size needed for TPU\n    image = tf.reshape(image, image_shape)\n    return image","2ed58604":"def read_train_tfrecord(example):\n    tfrec_format = {\n        \n        # _bytes_feature\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \n        # _int64_feature\n        \"sex_female\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex_male\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex_unknown\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_head\/neck\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_lower extremity\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_oral\/genital\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_palms\/soles\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_torso\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_unknown\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_upper extremity\": tf.io.FixedLenFeature([], tf.int64),\n        \"target\": tf.io.FixedLenFeature([], tf.int64),\n        \n        # _float_feature\n        \"age_approx\": tf.io.FixedLenFeature([], tf.float32),\n    }\n    \n        \n        \n    example = tf.io.parse_single_example(example, tfrec_format)\n    \n    # image data\n    image = decode_image(example['image']) \n    \n    data={}\n    \n    # _bytes_feature\n    data['image_name']=image_name=tf.cast(example['image_name'], tf.string)\n    \n    # integer features\n    data['sex_female']=tf.cast(example['sex_female'], tf.int32)\n    data['sex_male']=tf.cast(example['sex_male'], tf.int32)\n    data['sex_unknown']=tf.cast(example['sex_unknown'], tf.int32)\n    data['anatom_site_head\/neck']=tf.cast(example['anatom_site_head\/neck'], tf.int32)\n    data['anatom_site_lower extremity']=tf.cast(example['anatom_site_lower extremity'], tf.int32)\n    data['anatom_site_oral\/genital']=tf.cast(example['anatom_site_oral\/genital'], tf.int32)\n    data['anatom_site_palms\/soles']=tf.cast(example['anatom_site_palms\/soles'], tf.int32)\n    data['anatom_site_torso']=tf.cast(example['anatom_site_torso'], tf.int32)\n    data['anatom_site_unknown']=tf.cast(example['anatom_site_unknown'], tf.int32)\n    data['anatom_site_upper extremity']=tf.cast(example['anatom_site_upper extremity'], tf.int32)\n    \n    # _float_feature\n    data['age_approx']=tf.cast(example['age_approx'], tf.float32)\n\n    target=tf.cast(example['target'], tf.int32)\n\n    return image, target, data","4afdfe20":"def read_test_tfrecord(example):\n    tfrec_format = {\n        \n        # _bytes_feature\n        \"image\": tf.io.FixedLenFeature([], tf.string), \n        \"image_name\": tf.io.FixedLenFeature([], tf.string),\n        \n        # _int64_feature\n        \"sex_female\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex_male\": tf.io.FixedLenFeature([], tf.int64),\n        \"sex_unknown\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_head\/neck\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_lower extremity\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_oral\/genital\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_palms\/soles\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_torso\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_unknown\": tf.io.FixedLenFeature([], tf.int64),\n        \"anatom_site_upper extremity\": tf.io.FixedLenFeature([], tf.int64),\n        \n        # _float_feature\n        \"age_approx\": tf.io.FixedLenFeature([], tf.float32),\n    }\n    \n        \n        \n    example = tf.io.parse_single_example(example, tfrec_format)\n    \n    # image data\n    image = decode_image(example['image']) \n    \n    data={}\n    \n    # _bytes_feature\n    data['image_name']=image_name=tf.cast(example['image_name'], tf.string)\n    \n    # integer features\n    data['sex_female']=tf.cast(example['sex_female'], tf.int32)\n    data['sex_male']=tf.cast(example['sex_male'], tf.int32)\n    data['sex_unknown']=tf.cast(example['sex_unknown'], tf.int32)\n    data['anatom_site_head\/neck']=tf.cast(example['anatom_site_head\/neck'], tf.int32)\n    data['anatom_site_lower extremity']=tf.cast(example['anatom_site_lower extremity'], tf.int32)\n    data['anatom_site_oral\/genital']=tf.cast(example['anatom_site_oral\/genital'], tf.int32)\n    data['anatom_site_palms\/soles']=tf.cast(example['anatom_site_palms\/soles'], tf.int32)\n    data['anatom_site_torso']=tf.cast(example['anatom_site_torso'], tf.int32)\n    data['anatom_site_unknown']=tf.cast(example['anatom_site_unknown'], tf.int32)\n    data['anatom_site_upper extremity']=tf.cast(example['anatom_site_upper extremity'], tf.int32)\n    \n    # _float_feature\n    data['age_approx']=tf.cast(example['age_approx'], tf.float32)\n    \n    return image, data","d9c40ef3":"def load_dataset(filenames, _set=\"train\", ordered=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files \n    # at once and disregarding data order. Order does not matter since we will \n    # be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        # disable order, increase speed\n        ignore_order.experimental_deterministic = False\n\n    # automatically interleaves reads from multiple files\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n    # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.with_options(ignore_order)\n    # returns a dataset of (image, label) pairs if labeled=True \n    # or (image, id) pairs if labeled=False\n    dataset = dataset.map(read_train_tfrecord if _set == \"train\" \n                          else read_test_tfrecord, num_parallel_calls=AUTO)\n    \n    return dataset","2fd98db0":"%%time\n\ntraining_dataset = load_dataset(train_tfrecords_path['fold_0'])\n\n# training_dataset.take(1)\n# <TakeDataset shapes: ((512, 512, 3), (), \n#                       {image_name: (), sex_female: (), sex_male: (), \n#                       sex_unknown: (), anatom_site_head\/neck: (), \n#                       anatom_site_lower extremity: (), anatom_site_oral\/genital: (), \n#                       anatom_site_palms\/soles: (), anatom_site_torso: (), \n#                       anatom_site_unknown: (), anatom_site_upper extremity: (), \n#                       age_approx: ()}), types: (tf.float32, tf.int32, \n#                                                 {image_name: tf.string, sex_female: tf.int32, \n#                                                  sex_male: tf.int32, sex_unknown: tf.int32, \n#                                                  anatom_site_head\/neck: tf.int32, \n#                                                  anatom_site_lower extremity: tf.int32, \n#                                                  anatom_site_oral\/genital: tf.int32, \n#                                                  anatom_site_palms\/soles: tf.int32, \n#                                                  anatom_site_torso: tf.int32, \n#                                                  anatom_site_unknown: tf.int32, \n#                                                  anatom_site_upper extremity: tf.int32, \n#                                                  age_approx: tf.float32})>\n\n\ntarget_map = {0:'benign',1:'malignant'}\nprint(\"Example of the training data:\\n\")\n\nfor image, target, data in training_dataset.take(1):\n    print(\"The image batch size:\", image.numpy().shape)\n    print(\"target name:\", target_map[target.numpy()])\n    print(\"image Name :\", data['image_name'].numpy())\n    print(\"sex_female :\", data['sex_female'].numpy())\n    print(\"sex_male :\", data['sex_male'].numpy())\n    print(\"sex_unknown :\", data['sex_unknown'].numpy())\n    print(\"age_approx(Rescaled) :\", data['age_approx'].numpy())\n    print(\"anatom_site_head\/neck :\", data['anatom_site_head\/neck'].numpy())\n    print(\"anatom_site_lower extremity :\", data['anatom_site_lower extremity'].numpy())\n    print(\"anatom_site_oral\/genital :\", data['anatom_site_oral\/genital'].numpy())\n    print(\"anatom_site_palms\/soles :\", data['anatom_site_palms\/soles'].numpy())\n    print(\"anatom_site_torso :\", data['anatom_site_torso'].numpy())\n    print(\"anatom_site_unknown :\", data['anatom_site_unknown'].numpy())\n    print(\"anatom_site_upper extremity :\", data['anatom_site_upper extremity'].numpy())\n    ","b4d63ebd":"%%time\n\nvalidation_dataset = load_dataset(train_tfrecords_path['fold_0'])\n\ntarget_map = {0:'benign',1:'malignant'}\nprint(\"Example of the validation data:\\n\")\n\nfor image, target, data in validation_dataset.take(1):\n    print(\"The image batch size:\", image.numpy().shape)\n    print(\"target name:\", target_map[target.numpy()])\n    print(\"image Name :\", data['image_name'].numpy())\n    print(\"sex_female :\", data['sex_female'].numpy())\n    print(\"sex_male :\", data['sex_male'].numpy())\n    print(\"sex_unknown :\", data['sex_unknown'].numpy())\n    print(\"age_approx(Rescaled) :\", data['age_approx'].numpy())\n    print(\"anatom_site_head\/neck :\", data['anatom_site_head\/neck'].numpy())\n    print(\"anatom_site_lower extremity :\", data['anatom_site_lower extremity'].numpy())\n    print(\"anatom_site_oral\/genital :\", data['anatom_site_oral\/genital'].numpy())\n    print(\"anatom_site_palms\/soles :\", data['anatom_site_palms\/soles'].numpy())\n    print(\"anatom_site_torso :\", data['anatom_site_torso'].numpy())\n    print(\"anatom_site_unknown :\", data['anatom_site_unknown'].numpy())\n    print(\"anatom_site_upper extremity :\", data['anatom_site_upper extremity'].numpy())","43e998c5":"def get_test_dataset(ordered=False):\n    dataset = load_dataset(test_tfrecords_path, _set=\"test\", ordered=ordered)\n    dataset = dataset.batch(BATCH_SIZE)\n    # prefetch next batch while training (autotune prefetch buffer size)\n    dataset = dataset.prefetch(AUTO)\n    \n    return dataset","a53afe80":"%%time\n\ntest_dataset = get_test_dataset()\n\nprint(\"Examples of the test data:\")\nfor image, data in test_dataset.take(2):\n    print(\"The image batch size:\", image.numpy().shape)\n    print(\"image Name :\", data['image_name'].numpy())\n    print(\"sex_female :\", data['sex_female'].numpy())\n    print(\"sex_male :\", data['sex_male'].numpy())\n    print(\"sex_unknown :\", data['sex_unknown'].numpy())\n    print(\"age_approx(Rescaled) :\", data['age_approx'].numpy())\n    print(\"anatom_site_head\/neck :\", data['anatom_site_head\/neck'].numpy())\n    print(\"anatom_site_lower extremity :\", data['anatom_site_lower extremity'].numpy())\n    print(\"anatom_site_oral\/genital :\", data['anatom_site_oral\/genital'].numpy())\n    print(\"anatom_site_palms\/soles :\", data['anatom_site_palms\/soles'].numpy())\n    print(\"anatom_site_torso :\", data['anatom_site_torso'].numpy())\n    print(\"anatom_site_unknown :\", data['anatom_site_unknown'].numpy())\n    print(\"anatom_site_upper extremity :\", data['anatom_site_upper extremity'].numpy())","6ce35b5a":"# Plot Batch\nfig = plt.figure(figsize=(15,15))\n\nfor i,image in enumerate(next(iter(train_dataset.unbatch().batch(20)))[0].numpy()):\n    \n    plt.subplot(4, 5, i+1)\n    plt.imshow(image)","c5639bc4":"# Plot Batch\nfig = plt.figure(figsize=(15,15))\n\nfor i,image in enumerate(next(iter(test_dataset.unbatch().batch(20)))[0].numpy()):\n    \n    plt.subplot(4, 5, i+1)\n    plt.imshow(image)","88293446":"## Hair Remove method","418961fe":"## Encoded-Test-Data Columns","e58cceb8":"## Testing One-Hot-Encoding","5af2f3c6":"## Make Folder test_fold_tfrecords","bfd09bbd":"## Make Folder train_fold_tfrecords","4ef1d57b":"## Take First-element of train-fold-1","5527b047":"# <font color='Blue'>Feel free to fork kernel and use SGK-Fold TFRecords + Tabular Data (One-Hot encoded) + Different Shape Images<\/font>","c909fde0":"## Data Overview","f6fc8bf7":"## Read train TFRecords","b0e73b22":"## Get Stratify Group Index","7214e977":"## Make train-validation fold","a2969d7a":"## Get Test-data element","a72ed0b1":"## Training-Data Stratified Group k fold","976971d9":"## get-test-tfrecords","6cff49e6":"## Plot test-batch Images","102dc5dc":"## WITH Hair remove","728f5305":"## Decode Image","f3b63fc6":"## Training-Data Stratified Group k fold","2f92e23b":"## Import Libraries","dca2fb76":"## Encoded-Train-Data Columns","9c951649":"## Write Testing TFRecords","4fb0d7bd":"## Stratified Group k-fold Function","9a186031":"## Set Seed","13bdbd1a":"## Write Trainging TFRecords","4e8014fd":"## Take First-element of validation-fold-1","e2adebe3":"## Data Count","1ff761ee":"## Read test TFRecords","0a705394":"# **Write TFRecords**\n sorce from TensorFlow's docs [here](https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord)","7a3b2c30":"## Plot train-Batch Images","fc414281":"### This Version contains Stratified-GroupKFold=5 **Image_size=(512,512,3)** Image-quality=100","2435b588":"* References\n    1. [how-to-create-tfrecords](https:\/\/www.kaggle.com\/cdeotte\/how-to-create-tfrecords)","cedfd2e5":"## Set Meta-Data and Load-CSV-Data","027fbc56":"* Ex. test_fold_3_0-1024.tfrec : \n\n    1. test tfrecord\n    2. 3rd fold\n    3. 0th file sub_id\n    4. 1024 tfrecord contains 1024 image","a7c90106":"## Training One-Hot-Encoding","bb37712d":"### Time required for 1 fold(1935 images) [CPU]\n\n*  **Method-1**\n    1. CPU times: user 16min 34s, sys: 53.4 s, total: 17min 28s\n    2. Wall time: 17min 29s\n\n\n*  **Method-2**\n    1. CPU times: user 2min 14s, sys: 38 s, total: 2min 52s\n    2. Wall time: 2min 42s\n\n\n*  **Mehod-3**\n    1. CPU times: user 6min, sys: 1min 35s, total: 7min 35s\n    2. Wall time: 6min 44s","07e6906f":"# Read TFRecords","abd230b6":"## Load Data","865595f2":"## Fill NAN Data"}}