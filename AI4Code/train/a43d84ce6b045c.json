{"cell_type":{"6d28ff02":"code","d84a84e0":"code","8357b2b5":"code","dd2d1cb4":"code","63c7d7af":"code","a012a619":"code","e5e0c5f3":"code","4d873fe4":"code","e1881437":"code","f4788870":"code","bd9dee8c":"code","08fd80d3":"code","b3f7edb5":"code","25fd9627":"code","72ed6e3e":"code","34ea7dbf":"code","eaff3df5":"code","c455cb9d":"code","cb50702e":"code","3906fe67":"code","033b4335":"code","fd163d93":"code","b465b469":"code","4e333e59":"code","968de986":"code","a28ddaa6":"code","0c3599c8":"code","32bb18a7":"code","66696f62":"code","fbf70f50":"code","696877e0":"code","2b23e56e":"code","08b9cf29":"code","4aea6c7d":"code","0f04141d":"code","8e94f257":"code","bc2a3144":"code","3a1ccaa4":"code","99d70dc3":"code","f726cf99":"code","b4950138":"code","1ee2fdc6":"code","57a4d025":"code","dabeacdf":"code","d16b14aa":"code","8cac2c5b":"code","93d453e3":"code","8df99bc1":"code","257711e2":"code","791be15a":"code","acc1f7dc":"code","5aecaf10":"code","35d1f4ab":"code","1774684e":"code","24127e42":"code","27ef9c6c":"code","c549f606":"code","2283bfa8":"markdown","ce7b94fa":"markdown","4ac8e7a8":"markdown","6e23a652":"markdown","84ca0b7d":"markdown","9bdfa9b3":"markdown","b18e8c25":"markdown","e44ecf00":"markdown","43e9547a":"markdown","6daf595e":"markdown","bddc3e6b":"markdown","be0d175b":"markdown","16d82a8a":"markdown","e65b2ba1":"markdown","8e474fe3":"markdown","0c135135":"markdown","722fe062":"markdown","8429a462":"markdown","51e4c8ba":"markdown","53716bec":"markdown","b8e95452":"markdown","3ba95b1d":"markdown","054e4fe4":"markdown","2f06eadf":"markdown","86046b28":"markdown","abee6396":"markdown","b14daecc":"markdown","66ea19d9":"markdown","abfd60be":"markdown","5aa05ae7":"markdown","fbf9b746":"markdown","827e1ce5":"markdown","a63ccc5b":"markdown","5b4741b9":"markdown","0e1bb85e":"markdown","e7f729c8":"markdown","a89ba456":"markdown","a8d1e756":"markdown","226c5ac8":"markdown","5150780e":"markdown","0e3d1ec2":"markdown","4890fba9":"markdown"},"source":{"6d28ff02":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d84a84e0":"#importing data\ndata_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndata = [data_train, data_test]","8357b2b5":"data_train.head()","dd2d1cb4":"data_train.info()","63c7d7af":"for df in data:\n    df = df.drop('PassengerId', axis=1, inplace=True)","a012a619":"sns.countplot(x='Survived', data=data_train)\nplt.show()","e5e0c5f3":"#spliting data_train into men and women\nmen = data_train[data_train['Sex'] == 'male']\nwomen = data_train[data_train['Sex'] == 'female']","4d873fe4":"figs, axes = plt.subplots(1, 2, figsize=(16,4))\n\n#age histplot from men\nplt.subplot(1, 2, 1)\nax0 = plt.hist(men[men['Survived']==1].Age.dropna(), bins=18, label='survived', color='skyblue', alpha=0.9)\nax0 = plt.hist(men[men['Survived']==0].Age.dropna(), bins=36, label='not survived', color='orange', alpha=0.5)\nax0 = plt.legend()\nax0 = plt.xlabel(\"Age\")\nax0 = plt.title(\"Male\")\n\n#age histplot for women\nplt.subplot(1, 2, 2)\nax1 = plt.hist(women[women['Survived']==1].Age.dropna(), bins=18, label='survived', color='skyblue', alpha=0.9)\nax1 = plt.hist(women[women['Survived']==0].Age.dropna(), bins=36, label='not survived', color='orange', alpha=0.5)\nax1 = plt.legend()\nax1 = plt.xlabel(\"Age\")\nax1 = plt.title(\"Female\")","e1881437":"data_train['Embarked'].value_counts()","f4788870":"figs, axes = plt.subplots(2, 2, figsize=(16,8))\n\nplt.subplot(2, 2, 1)\nax = sns.pointplot(x='Embarked', y='Survived', data=men).set_title('men')\n\nplt.subplot(2, 2, 2)\nax = sns.pointplot(x='Embarked', y='Survived', data=women).set_title('women')\n\nplt.subplot(2, 2, 3)\nax = sns.pointplot(x='Pclass', y='Survived', data=men)\n\nplt.subplot(2, 2, 4)\nax = sns.pointplot(x='Pclass', y='Survived', data=women)\n","bd9dee8c":"for df in data:\n    df['FamilySize'] = df['SibSp'] + df['Parch'] + 1","08fd80d3":"ax = sns.pointplot(x='FamilySize', y='Survived', data=data_train)","b3f7edb5":"figs, axes = plt.subplots(1, 2, figsize=(16,4))\n\n#age histplot from men\nplt.subplot(1, 2, 1)\nax0 = plt.hist(men[men['Survived']==1].Fare.dropna(), bins=36, label='survived', color='skyblue', alpha=0.9)\nax0 = plt.hist(men[men['Survived']==0].Fare.dropna(), bins=36, label='not survived', color='orange', alpha=0.5)\nax0 = plt.legend()\nax0 = plt.xlabel(\"Fare\")\nax0 = plt.title(\"Male\")\n\n#age histplot for women\nplt.subplot(1, 2, 2)\nax1 = plt.hist(women[women['Survived']==1].Fare.dropna(), bins=36, label='survived', color='skyblue', alpha=0.9)\nax1 = plt.hist(women[women['Survived']==0].Fare.dropna(), bins=36, label='not survived', color='orange', alpha=0.5)\nax1 = plt.legend()\nax1 = plt.xlabel(\"Fare\")\nax1 = plt.title(\"Female\")","25fd9627":"fig = plt.subplots(figsize=(10, 10))\n\nsns.heatmap(data_train.corr(), annot=True, square=True, cmap='coolwarm', annot_kws={'size': 8})\n\nplt.show()","72ed6e3e":"#visualusing missing values with heatmap\n\n#seting fig size\nplt.figure(figsize=(16,5))\n\n#data_train plot\nplt.subplot(1,2,1)\nsns.heatmap(data_train.isna().transpose(),\n            cmap=\"YlGnBu\",\n            cbar = False\n           )\n\n#data_test plot\nplt.subplot(1,2,2)\nsns.heatmap(data_test.isna().transpose(),\n            cmap=\"YlGnBu\",\n            cbar = False\n           )\nplt.show()","34ea7dbf":"for df in data:\n    mean = df['Age'].mean()\n    std = df['Age'].std()\n    is_null = df['Age'].isnull().sum()\n    random_age = np.random.randint(mean-std, mean+std, size=is_null)\n    \n    age_slice = df['Age'].copy()\n    age_slice[np.isnan(age_slice)] = random_age\n    df['Age'] = age_slice\n    df['Age'] = df['Age'].astype(int)","eaff3df5":"data_train['Embarked'].describe()","c455cb9d":"data_train['Embarked'] = data_train['Embarked'].fillna('S')","cb50702e":"mean = data_test['Fare'].mean()\ndata_test['Fare'] = data_test['Fare'].fillna(mean)","3906fe67":"for df in data:\n    df['Cabin'] = df['Cabin'].fillna('U')","033b4335":"for df in data:\n    df['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.')","fd163d93":"data_train['Title'].value_counts()","b465b469":"titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}","4e333e59":"for df in data:\n    df['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    df['Title'] = df['Title'].map(titles)\n    df['Title'] = df['Title'].fillna(1)\n    df['Title'] = df['Title'].astype(int)\n    df = df.drop('Name', axis=1, inplace=True)","968de986":"genders = {\"male\":0, \"female\":1}","a28ddaa6":"for df in data:\n    df['Sex'] = df['Sex'].map(genders)","0c3599c8":"for df in data:\n    df['CabinNum'] = df.Cabin.apply(lambda x: len(x.split()))","32bb18a7":"data_train['CabinNum'].value_counts()","66696f62":"ax = sns.pointplot(x='CabinNum', y='Survived', data=data_train)","fbf70f50":"for i in range (891):\n    data_train['Cabin'][i] = data_train['Cabin'][i][0]\n\nfor i in range (418):\n    data_test['Cabin'][i] = data_test['Cabin'][i][0]","696877e0":"cabin = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}","2b23e56e":"for df in data:\n    df['Cabin'] = df['Cabin'].map(cabin)\n    df['Cabin'] = df['Cabin'].fillna(8)\n    df['Cabin'] = df['Cabin'].astype(int)","08b9cf29":"ports = {\"S\": 0, \"C\": 1, \"Q\": 2}","4aea6c7d":"for df in data:\n    df['Embarked'] = df['Embarked'].map(ports)","0f04141d":"for df in data:\n    df['TicketLength'] = df.Ticket.apply(lambda x: len(x))\n    df.drop('Ticket', axis=1, inplace=True)","8e94f257":"data_train['TicketLength'].value_counts()","bc2a3144":"for i in range(891):\n    if data_train['TicketLength'][i] > 8:\n        data_train['TicketLength'][i] = 9\n    \nfor i in range(418):\n    if data_test['TicketLength'][i] > 8:\n        data_test['TicketLength'][i] = 9","3a1ccaa4":"ax = sns.pointplot(x='TicketLength', y='Survived', data=data_train)","99d70dc3":"pd.qcut(data_train.Fare, q = 7).head()","f726cf99":"data_train.head()","b4950138":"data_train.info()","1ee2fdc6":"data_test.info()","57a4d025":"#importing libraries\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nimport optuna","dabeacdf":"#spliting data\nx = data_train.drop(['Survived'], axis=1)\ny = data_train['Survived']","d16b14aa":"#spliding data into 5 folds\ncv = KFold(n_splits=5, random_state=22, shuffle=True)\n\n#number of trials for optuna\ntrials = 250","8cac2c5b":"def objective(trial):\n\n    #parameter range\n    param = {\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }\n    \n    model = XGBClassifier(**param)  \n    \n    #scoring\n    scores = cross_val_score(\n        model, x, y, cv=cv,\n        scoring=\"accuracy\"\n    )\n    return scores.mean()\n\n#creating optuna study with 100 trials, maximizing accuracy\nxgb_study = optuna.create_study(direction='maximize')\nxgb_study.optimize(objective, n_trials=trials)","93d453e3":"print('Number of finished trials:', len(xgb_study.trials))\nprint('Best trial:', xgb_study.best_trial.params)\n\n#redefining xgb with the best trial parameters\nxgb = XGBClassifier(**xgb_study.best_trial.params)","8df99bc1":"optuna.visualization.plot_optimization_history(xgb_study)","257711e2":"print(f'Highest Accuracy using XGBoost is {round(xgb_study.best_value,4)}')","791be15a":"def objective(trial):\n\n    #defining parameters range\n    param = {\n        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n        'max_depth': trial.suggest_int('max_depth', 4, 50),\n        'min_samples_split': trial.suggest_int('min_samples_split', 1, 150),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n    }\n    \n    #defining model\n    model = RandomForestClassifier(**param)  \n    \n    #scoring \n    scores = cross_val_score(\n        model, x, y, cv=cv,\n        scoring=\"accuracy\"\n    )\n    return scores.mean()\n\n#crating optuna study with 100 trials\nrf_study = optuna.create_study(direction='maximize')\nrf_study.optimize(objective, n_trials=trials)","acc1f7dc":"print('Number of finished trials:', len(rf_study.trials))\nprint('Best trial:', rf_study.best_trial.params)\n\n#redefining rf with the best trial parameters\nrf = RandomForestClassifier(**rf_study.best_trial.params)","5aecaf10":"optuna.visualization.plot_optimization_history(rf_study)","35d1f4ab":"print(f'Highest Accuracy using Random Forest is {round(rf_study.best_value,4)}')","1774684e":"#creating feature importance data frame using random forest built in method\nrf.fit(x,y)\nimportances = pd.DataFrame({'feature':x.columns,'importance':np.round(rf.feature_importances_,3)})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\n\n#ploting importances\nplt.figure(figsize=(10, 6))\nsns.barplot(x='importance', y=importances.index, data=importances)\n\nplt.xlabel('')\nplt.ylabel('')\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=15)\nplt.title('Random Forest Classifier Feature Importance', size=15)\nplt.show()","24127e42":"pred_rf = rf.predict(data_test)","27ef9c6c":"#creating data frame\nResults = pd.DataFrame(pred_rf, columns=['Survived'])\n\n#setting index, there are 892 passingers in train data\nResults['PassengerId'] = Results.index + 892\n\n#adding PassingerId column\nResults = Results[['PassengerId', 'Survived']]\n\n#exporting csv\nResults.to_csv('results8.csv', index=False)","c549f606":"Results.head()","2283bfa8":"#### Data description\n- Survived is the target variable we are trying to predict (0 or 1):                                                      \n    - 1 = Survived\n    - 0 = Not Survived\n- Pclass (Passenger Class) is the socio-economic status of the passenger and it is a categorical ordinal feature which has 3 unique values (1, 2 or 3):\n    - 1 = Upper Class\n    - 2 = Middle Class\n    - 3 = Lower Class\n- Name, Sex and Age are self-explanatory\n- SibSp is the total number of the passengers' siblings and spouse\n- Parch is the total number of the passengers' parents and children\n- Ticket is the ticket number of the passenger\n- Fare is the passenger fare\n- Cabin is the cabin number of the passenger\n- Embarked is port of embarkation and it is a categorical feature which has 3 unique values (C, Q or S):\n    - C = Cherbourg\n    - Q = Queenstown\n    - S = Southampton","ce7b94fa":"# Predicting the Survival of Titanic Passengers","4ac8e7a8":"We will place rare titles in one category and convert title data to numeric.","6e23a652":"### SibSp and Parch","84ca0b7d":"Embarked is correlated with survival depending on the gender. Women on ports S and Q have higher survival rate while man on port C have higher survival rate. Pclass is correlated with survival as well. Pclass 1 has the lowest survival rate for both genders.","9bdfa9b3":"### Target Distibution","b18e8c25":"# Feature Engineering","e44ecf00":"Filling the missing value with the mean in data_test","43e9547a":"### Name","6daf595e":"# Modeling","bddc3e6b":"### Feature Importance","be0d175b":"Data ready for machine learning","16d82a8a":"### Fare","e65b2ba1":"### Correlations","8e474fe3":"### Fare","0c135135":"## XGBoost","722fe062":"### Embarked","8429a462":"### Ticket","51e4c8ba":"# Missing values","53716bec":"Extracting title from name","b8e95452":"### Results","3ba95b1d":"We have many missing values so we will put them in a separate category","054e4fe4":"### Age\nLets fill age missing values with random values in range (mean-std, mean+std)","2f06eadf":"### Cabin","86046b28":"### Cabin","abee6396":"### Check Data","b14daecc":"### The challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. \nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n### Goal\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.\nFor each in the test set, you must predict a 0 or 1 value for the variable.\n\n### Metric\nYour score is the percentage of passengers you correctly predict. This is known as accuracy.\n\n### Introduction\nThe kerner has 4 main sections: data analysis, imputing missing values, feature engineering and modelling. We will do very simple imputations of missing values using mean and mode. We will add some new features and do label encoding on the categorical features. In the end, we will make XGBoost and Random Forest model and optimize them using Optuna. Uploading results from this kernel gave me 0.79 accuracy on the Kaggle leaderboard.","66ea19d9":"Target is slightly imbalanced. We won't do any balancing.","abfd60be":"We can see a high survival rate for family size between 2 and 4 and very low survival rate of families with more than 4 members.","5aa05ae7":"### Fare","fbf9b746":"We can see missing values in Age, Cabin and Embarked features. We have 5 features than need to be converted into numeric. Passenger id is not usefull for data analysis, thus we will drop that feature.","827e1ce5":"Extracting deck from cabin data","a63ccc5b":"SibSp and Parch make more sence combined","5b4741b9":"### Sex","0e1bb85e":"Random Forest has outperformed XGBoost, thus we will use it to generate the results.","e7f729c8":"# Data Analysis","a89ba456":"### Embarked\nSince we have only 3 missing values, we will fill the with the most common value.","a8d1e756":"We will use qcut() to evenly divide Fare data into categories","226c5ac8":"Woman have higher survival rate. Infants have higher survival rates. Women between 15 and 40 have high survival rates. Conclusion: age and sex have a signifiacant inpact on survival rates.","5150780e":"## Random Forest","0e3d1ec2":"### Pclass and Embarked","4890fba9":"### Age and Sex"}}