{"cell_type":{"d7382034":"code","93cddc37":"code","88bf9338":"code","3e223072":"code","35729a62":"code","bb227bc2":"code","0a3fbaf6":"code","c6e9bf86":"code","83c7bb96":"code","50bdb5a6":"code","f6a240a7":"code","207a6847":"code","fb2e8e00":"code","f6929be7":"code","7534e49d":"code","6aff81c7":"code","015d9a9e":"code","4c77a013":"code","4b196531":"code","2b819516":"code","fde75725":"code","5255cbd6":"code","2ae53c73":"code","2c1a6474":"code","c0a3014f":"code","c62af980":"code","6215939a":"code","728c411d":"code","3923319d":"code","5e4c2bc1":"code","b690e52b":"code","b30541d9":"code","7b653b86":"code","1cd83ff3":"code","dee3e248":"markdown","df4d1914":"markdown","ae993081":"markdown","97cf60db":"markdown","a1f1568f":"markdown","e2227440":"markdown","ba18df93":"markdown","13a3bd75":"markdown","b1e3c27b":"markdown","9c35d5dc":"markdown","e7bf73de":"markdown","49491cb0":"markdown","4c2b6094":"markdown","2d34f6d9":"markdown","5e6bdba2":"markdown","f4439a39":"markdown","0b253a77":"markdown","09b84bca":"markdown","0f23402c":"markdown","53b5f445":"markdown","4501148b":"markdown","84b28464":"markdown","dc55ac29":"markdown","af0cae67":"markdown","4c5f99b3":"markdown","7e9203db":"markdown","12eca450":"markdown","06e60e75":"markdown","258ae0d0":"markdown"},"source":{"d7382034":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom lofo import LOFOImportance, Dataset, plot_importance # to install !pip install lofo-importance\n%matplotlib inline\nfrom sklearn.metrics import make_scorer, mean_absolute_error, f1_score, accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, GridSearchCV, cross_val_score\nimport itertools\nimport optuna","93cddc37":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ncombine = [train_data, test_data]","88bf9338":"train_data.head()","3e223072":"print(train_data.info())\n\nprint('\\nNumber of rows : ', train_data.shape[0])\nprint('Number of columns : ', train_data.shape[1])\n\nprint('\\nTrain columns with null values: \\n', train_data.isnull().sum())\n\nprint('\\nNumber of total people who survived or dead ( 0 : Dead, 1: Survived )\\n', train_data['Survived'].value_counts().apply(lambda x:f'{x} ({x*100\/len(train_data):0.2f}%)'))\n\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\nprint(\"\\n% of women who survived:\", rate_women)\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\nprint(\"\\n% of men who survived:\", rate_men)","35729a62":"sns.set_style('whitegrid')\ns = sns.countplot(x='Survived', hue='Sex', data=train_data, palette='rainbow')\ns.set_title(\"Number of survived or dead categorized by gender\")\nplt.show()\nprint(\"> We see that survival rate of women is more than men\")","bb227bc2":"sns.set_style('whitegrid')\ns = sns.countplot(x='Pclass', hue='Survived', data=train_data, palette='rainbow')\ns.set_title(\"\\t\\t\\t\\t\\t\\tNumber of survived or dead categorized by passenger class\")\nplt.show()\nprint(\"> We see that survival rate of 1st and 2nd passenger class is higher than 3rd class\")","0a3fbaf6":"sns.set_style('whitegrid')\nsns.catplot(x='Pclass', col='Embarked', kind='count', data=train_data)\nprint(\"Number of people categorized by port of embarkation\")\nplt.show()\nprint(\"Hint : C = Cherbourg, Q = Queenstown, S = Southampton \")\nprint(\"> We see that most of people come from Southampton and chose 3rd class\")","c6e9bf86":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,6), constrained_layout=True, sharey=True)\n\n#Graphs\nage_surv = sns.histplot(data=train_data[train_data['Survived']==1],x='Age',bins=30,ax=axes[1])\nage_dead = sns.histplot(data=train_data[train_data['Survived']==0],x='Age',bins=30,ax=axes[0])\n\nfigtitle = fig.suptitle('Survival by Age',fontsize=24)\naxeszero_ylabel = axes[0].set_ylabel('Count',size=14)\naxeszero_yticklabel = axes[0].set_yticklabels([int(x) for x in age_dead.get_yticks()],size=12)\naxeszero_title = axes[0].set_title('Did Not Survive', fontsize=14)\naxeszero_xticklabel = axes[0].set_xticklabels([int(x) for x in age_dead.get_xticks()],size=12) \naxeszero_xlabel = axes[0].set_xlabel('Age',size=14)\naxesone_title = axes[1].set_title('Survived', fontsize=14)\naxesone_xticklabel = axes[1].set_xticklabels([int(x) for x in age_surv.get_xticks()],size=12)\naxesone_xlabel = axes[1].set_xlabel('Age',size=14)\nplt.show()\nprint(\"> We see that between 20-30 ages almost same probability of survived and didn't survived\")","83c7bb96":"#SibSp - Survived\nsns.set_style('whitegrid')\ng = sns.catplot(x = \"SibSp\", y = \"Survived\", data = train_data, kind = \"bar\", height= 9)\ng.set_ylabels(\"Probability of Survival\")\nplt.show()\nprint(\"Hint : SibSp = number of siblings \/ spouses \")","50bdb5a6":"#ParCh - Survived\nsns.set_style('whitegrid')\ng = sns.catplot(x = \"Parch\", y = \"Survived\", data = train_data, kind = \"bar\", height = 7)\ng.set_ylabels(\"Probability of Survival\")\nplt.show()\nprint(\"Hint : Parch = number of parents \/ children \")\nprint(\"> After we see that from last two visualization, small families have more chance to survive\")","f6a240a7":"plt.style.use(\"seaborn-whitegrid\")\nnum_col = [\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\ns = sns.heatmap(train_data[num_col].corr(), annot = True, fmt = \".2f\")\ns.set_title(\"Correlation between numerical features and target\")\nplt.show()\nprint(\"> We see that these features have not more affect to target\")","207a6847":"print('\\nTrain data with null values:')\ntrain_data.isnull().sum()\n\nprint('\\nTest data with null values:')\ntest_data.isnull().sum()","fb2e8e00":"for dataset in combine:\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# SITUATION OF TITLE COLUMN AND NUMBER OF EACH TITLE\ntrain_data['Title'].value_counts()","f6929be7":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \n                 \"Master\": 3, \"Dr\": 4, \"Rev\": 4, \"Col\": 4, \"Major\": 4, \"Mlle\": 4,\"Countess\": 4,\n                 \"Ms\": 4, \"Lady\": 4, \"Jonkheer\": 4, \"Don\": 4, \"Dona\" : 4, \"Mme\": 4,\"Capt\": 4,\"Sir\": 4 }\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)","7534e49d":"train_data['family_size'] = train_data['SibSp'] + train_data['Parch'] + 1\ntest_data['family_size'] = test_data['SibSp'] + test_data['Parch'] + 1","6aff81c7":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","015d9a9e":"#CABIN \nfor dataset in combine:\n    dataset['Has_Cabin'] = dataset[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","4c77a013":"Pclass1 = train_data[train_data['Pclass']==1]['Embarked'].value_counts()\nPclass2 = train_data[train_data['Pclass']==2]['Embarked'].value_counts()\nPclass3 = train_data[train_data['Pclass']==3]['Embarked'].value_counts()\n\ndf = pd.DataFrame([Pclass1, Pclass2, Pclass3])\ndf.index = ['1st class','2nd class', '3rd class']\ndf.plot(kind='bar',stacked=True, figsize=(10,5))","4b196531":"for dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","2b819516":"sns.factorplot(data = train_data , x = 'Pclass' , y = 'Age', kind = 'box')","fde75725":"def AgeImpute(df):\n    Age = df[0]\n    Pclass = df[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1: return 37\n        elif Pclass == 2: return 29\n        else: return 24\n    else:\n        return Age\n\n# Age Impute\ntrain_data['Age'] = train_data[['Age' , 'Pclass']].apply(AgeImpute, axis = 1)\ntest_data['Age'] = test_data[['Age' , 'Pclass']].apply(AgeImpute, axis = 1)","5255cbd6":"# FARE \ntest_data[\"Fare\"] = test_data[\"Fare\"].fillna(test_data[\"Fare\"].median())","2ae53c73":"features_drop = ['Cabin', 'Ticket', 'Name', 'PassengerId']\ntrain_data.drop(features_drop, axis=1, inplace=True)\ntest_data.drop(features_drop, axis=1, inplace=True)","2c1a6474":"print(\"AFTER FEATURE ENGINEERING\\n\")\nprint('Train data with null values:\\n', train_data.isnull().sum())\nprint('\\nTest data with null values:\\n', test_data.isnull().sum())","c0a3014f":"X = train_data.drop(['Survived'], axis = 1)\ny = train_data.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size = .3,\n                                                    random_state = 5,\n                                                   stratify = y)\n\nmodel = XGBClassifier(n_estimators=1000, learning_rate=0.05, max_depth = 10)\n# model.fit(X_train, y_train)","c62af980":"skf = StratifiedKFold(n_splits=5)\n\ncross_val_acc = []\n\nX_train = X_train.values\ny_train = y_train.values\n\nfor train, test in skf.split(X_train, y_train):\n    model.fit(X_train[train], y_train[train])\n    cross_val_acc.append(model.score(X_train[test], y_train[test]))\n    \n\nprint(\"All of means\")\nprint(cross_val_acc)\n\nprint(\"Mean of scores\")\nprint(np.mean(cross_val_acc))","6215939a":"gbm = lgb.LGBMClassifier(objective='binary')\n\ngbm.fit(X_train, y_train, eval_set = [(X_test, y_test)],\n        early_stopping_rounds=20,\n        verbose=10\n)\n\npre = gbm.predict(X_test, num_iteration=gbm.best_iteration_)\n\nprint('score', round(accuracy_score(y_test, pre)*100,2), '%')","728c411d":"# define the validation scheme\ncv = KFold(n_splits=4, shuffle=False)\nscorer = make_scorer(mean_absolute_error, greater_is_better=False)\n\n# define the binary target and the features\ntarget = \"Survived\"\nfeatures = [col for col in train_data.columns if col != target]\ndataset = Dataset(df=train_data, target=\"Survived\", features=features)\n# define the validation scheme and scorer. The default model is LightGBM\nlofo_imp = LOFOImportance(dataset, scoring=scorer, model=model, cv=cv)\n\n# get the mean and standard deviation of the importances in pandas format\nimportance_df = lofo_imp.get_importance()\n\n# plot the means and standard deviations of the importances\nplot_importance(importance_df)","3923319d":"# A parameter grid for XGBoost\nparams = {\n    'n_estimators': [100, 200, 300, 400, 500, 1000],\n    'learning_rate': [0.01, 0.05, 0.1, 0.15],\n    'max_depth': [3, 5, 7, 9, 11]\n}\n\n# define the validation scheme\ncv = KFold(n_splits=3, shuffle=True, random_state=None)\nscorer = make_scorer(f1_score, greater_is_better=True)\n\ngrid = GridSearchCV(estimator=model, \n                    param_grid=params, \n                    scoring=scorer, \n                    n_jobs=-1, \n                    cv=cv, \n                    verbose=1 )\ngrid.fit(X, y)\nprint('\\n Best estimator:')\nprint(grid.best_estimator_)\nprint('\\n Best score:')\nprint(grid.best_score_ * 2 - 1)\nprint('\\n Best parameters:')\nprint(grid.best_params_)","5e4c2bc1":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","b690e52b":"new_model = XGBClassifier()\nnew_model.set_params(**grid.best_params_)\n\nnew_model.fit(X_train, y_train)\n\ny_pred = grid.best_estimator_.predict(X)\ncm = confusion_matrix(train_data['Survived'], y_pred)\nnp.set_printoptions(precision=2)\n\nclass_names = ['Dead', 'Survived']\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cm, classes=class_names,\n                      title='Confusion matrix')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cm, classes=class_names, normalize=True, \n                      title='Normalized confusion matrix')","b30541d9":"import xgboost as xgb\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_test, label=y_test)\n\ndef objective(trial):\n    params = {\n        'booster':trial.suggest_categorical('booster', ['gbtree', 'dart', 'gblinear']),\n        'learning_rate':trial.suggest_loguniform(\"learning_rate\", 0.01, 0.1),\n        'max_depth':trial.suggest_int(\"max_depth\", 3, 11),\n        'subsample':trial.suggest_uniform(\"subsample\", 0.0, 1.0),\n        'colsample_bytree':trial.suggest_uniform(\"colsample_bytree\", 0.0, 1.0),\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n    # accuracy = accuracy_score(y_test, pred_labels)\n    f1_scores = f1_score(y_test, pred_labels)\n    return f1_scores\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=200, timeout=600)","7b653b86":"new_params = study.best_params\n\nnew_model2 = XGBClassifier(**new_params)\nnew_model2.fit(X, y)\npreds = new_model2.predict(X_test)\n\nprint('Optimized SuperLearner accuracy: ', accuracy_score(y_test, preds))\nprint('Optimized SuperLearner f1-score: ', f1_score(y_test, preds))\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","1cd83ff3":"all_accuracies = cross_val_score(estimator=new_model, X=X_train, y=y_train, cv=3)\n\nprint(\"\\n\\nAccuracy is the measure of how often the model is correct.\\n\")\n\nprint(\"All of accuracies are \\n\", all_accuracies)\nprint(\"\\nMean of accuracies \\n\", all_accuracies.mean())\nprint(\"\\nStandart deviation of accuracies \\n %\", ( all_accuracies.std() * 100))","dee3e248":"**How data looks**","df4d1914":"> There is just 1 missing value in test_data so median value of fare class is acceptable.","ae993081":"# DATA VISUALIZATION","97cf60db":"## **TEST DATA - PREDICTION**","a1f1568f":"**Some information about data**","e2227440":"> It's calculated by addition of SibSp and Parch number ( + 1 means that family at least one member )","ba18df93":"**LIGHTGBM PREDICTION**","13a3bd75":"**NAME - TITLE SPLIT and MAPPING**","b1e3c27b":"**XGBoost + k-fold**","9c35d5dc":"**TRAIN MODEL WITH BEST PARAMETERS and SHOW CONFUSION MATRIX**","e7bf73de":"# **OPTUNA HYPERPARAMETER SEARCH**","49491cb0":"**DROP UNNECESSARY COLUMNS**","4c2b6094":"# FEATURE ENGINEERING","2d34f6d9":"## ANALSYIS OF TITANIC DATASET","5e6bdba2":"**GENDER MAPPING**","f4439a39":"**AGE MISSING VALUES**","0b253a77":"**HAS CABIN**","09b84bca":"> Cabin column has too much missing values. So mapping for has cabin or not seems better solution. Because prediction of these missing values looks impossible and if we try to fill up them, it can directly affect learning of model as positively or negatively. So it's unguessable.","0f23402c":"* more than 50% of 1st class are from S embark\n* more than 50% of 2nd class are from S embark\n* more than 50% of 3rd class are from S embark\n\n> So we can fill up using S and then mapping for each letter because it has only 2 missing values as mentioned beginning of analysis.\n* S : 0 ( S : Southampton )\n* C : 1 ( C : Cherbourg   )\n* Q : 2 ( Q : Queentown   )","53b5f445":"**Read 'Training' and 'Test' data files**","4501148b":"**FEATURE IMPORTANCE BY LOFO**","84b28464":"**EMBARKED MAPPING**","dc55ac29":"## MODELING XGBOOST and LIGHTGBM\n\n**XGBOOST PREDICTION**\n","af0cae67":"> We can fill up by their median values because there are not too much missing values","4c5f99b3":"**FAMILY SIZE**","7e9203db":"# **GRID SEARCH**","12eca450":"> 0 assigned to female and 1 assigned to the male person","06e60e75":"> Depends on these values mapping for title column\n* Mr : 0\n* Miss : 1\n* Mrs: 2\n* Master: 3\n* Others : 4","258ae0d0":"**FARE CLASS MISSING VALUE**"}}