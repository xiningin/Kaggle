{"cell_type":{"cbd0d260":"code","63b402d0":"code","0e055db7":"code","57934fc2":"code","15516678":"code","f1e4e311":"code","7400d3b4":"code","877fb693":"code","ef5cfbcc":"code","0709449a":"code","98592824":"code","f27236c9":"code","f92271f8":"code","b001c1a5":"code","d96d0b63":"markdown","872e20c2":"markdown","b1550047":"markdown","f64abc74":"markdown","52b80211":"markdown","6fca5dbf":"markdown","77b53885":"markdown","f8908359":"markdown","8e2b017d":"markdown","c1cb5ae6":"markdown","aeb4f801":"markdown","e691b1a6":"markdown","edde26e3":"markdown"},"source":{"cbd0d260":"!pip install --upgrade pip\n!pip install pymap3d==2.1.0\n!pip install -U l5kit","63b402d0":"from typing import Dict\n\nfrom tempfile import gettempdir\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.resnet import resnet50\nfrom tqdm import tqdm\n\nfrom l5kit.configs import load_config_data\nfrom l5kit.data import LocalDataManager, ChunkedDataset\nfrom l5kit.dataset import AgentDataset, EgoDataset\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\nfrom l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\nfrom l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace\nfrom l5kit.geometry import transform_points\nfrom l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\nfrom prettytable import PrettyTable\nfrom pathlib import Path\n\nimport os","0e055db7":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"..\/input\/lyft-motion-prediction-autonomous-vehicles\"\ndm = LocalDataManager(None)\n# get config\ncfg = load_config_data(\"..\/input\/lyft-config-files\/agent_motion_config.yaml\")","57934fc2":"def build_model(cfg: Dict) -> torch.nn.Module:\n    # load pre-trained Conv2D model\n    model = resnet50(pretrained=True)\n\n    # change input channels number to match the rasterizer's output\n    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n    num_in_channels = 3 + num_history_channels\n    model.conv1 = nn.Conv2d(\n        num_in_channels,\n        model.conv1.out_channels,\n        kernel_size=model.conv1.kernel_size,\n        stride=model.conv1.stride,\n        padding=model.conv1.padding,\n        bias=False,\n    )\n    # change output size to (X, Y) * number of future states\n    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n\n    return model","15516678":"def forward(data, model, device, criterion):\n    inputs = data[\"image\"].to(device)\n    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1).to(device)\n    targets = data[\"target_positions\"].to(device)\n    # Forward pass\n    outputs = model(inputs).reshape(targets.shape)\n    loss = criterion(outputs, targets)\n    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n    loss = loss * target_availabilities\n    loss = loss.mean()\n    return loss, outputs","f1e4e311":"cfg['train_data_loader']['key'] = 'scenes\/train.zarr'\ncfg['train_data_loader']['num_workers'] = 8\ncfg['train_data_loader']['batch_size'] = 64","7400d3b4":"# ===== INIT DATASET\ntrain_cfg = cfg[\"train_data_loader\"]\nrasterizer = build_rasterizer(cfg, dm)\ntrain_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\ntrain_dataset = AgentDataset(cfg, train_zarr, rasterizer)\ntrain_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"], \n                             num_workers=train_cfg[\"num_workers\"])\nprint(train_dataset)","877fb693":"# ==== INIT MODEL\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = build_model(cfg).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.MSELoss(reduction=\"none\")","ef5cfbcc":"cfg[\"train_params\"][\"max_num_steps\"] = 10","0709449a":"# ==== TRAIN LOOP\ntr_it = iter(train_dataloader)\nprogress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\nlosses_train = []\nfor _ in progress_bar:\n    try:\n        data = next(tr_it)\n    except StopIteration:\n        tr_it = iter(train_dataloader)\n        data = next(tr_it)\n    model.train()\n    torch.set_grad_enabled(True)\n    loss, _ = forward(data, model, device, criterion)\n\n    # Backward pass\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    losses_train.append(loss.item())\n    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")","98592824":"# cfg['val_data_loader']['key'] = 'scenes\/validate.zarr'","f27236c9":"# # Loading validation dataset\n# eval_cfg = cfg[\"val_data_loader\"]\n# rasterizer = build_rasterizer(cfg, dm)\n# eval_zarr = ChunkedDataset(dm.require(eval_cfg[\"key\"])).open()\n# eval_dataset = AgentDataset(cfg, eval_zarr, rasterizer)\n# eval_dataloader = DataLoader(eval_dataset, \n#                              shuffle=eval_cfg[\"shuffle\"], \n#                              batch_size=eval_cfg[\"batch_size\"], \n#                              num_workers=eval_cfg[\"num_workers\"])\n# print(eval_dataset)","f92271f8":"# # ==== EVAL LOOP\n# model.eval()\n# torch.set_grad_enabled(False)\n\n# # store information for evaluation\n# future_coords_offsets_pd = []\n# timestamps = []\n\n# agent_ids = []\n# progress_bar = tqdm(eval_dataloader)\n# for data in progress_bar:\n#     _, ouputs = forward(data, model, device, criterion)\n#     future_coords_offsets_pd.append(ouputs.cpu().numpy().copy())\n#     timestamps.append(data[\"timestamp\"].numpy().copy())\n#     agent_ids.append(data[\"track_id\"].numpy().copy())","b001c1a5":"# error = compute_error_csv(eval_gt_path, pred_path)\n# print(f\"NLL: {error:.5f}\\nL2: {np.sqrt(2*error\/cfg['model_params']['future_num_frames']):.5f}\")","d96d0b63":"### Importing PyTorch and l5kit","872e20c2":"### Load validation dataset","b1550047":"### Build a baseline CNN with Resnet50 backbone\n\nSize of `num_targets`: 100","f64abc74":"### Train model\n\nTrain for 1000 steps. You can set the config parameter like this or change in config itself.","52b80211":"### Loading the data","6fca5dbf":"### Baseline Model\n\nThis baseline model is adopted from [Lyft's example](https:\/\/github.com\/lyft\/l5kit\/blob\/master\/examples\/agent_motion_prediction\/agent_motion_prediction.ipynb) on their l5kit repo.","77b53885":"### Build model, set optimizer and loss function","f8908359":"In the config file the `train_data_loader`'s key is a sample zarr file. Change this to `train.zarr` file either by doing something like below or chaning in the config file itself. If you are using kaggle's GPU, you can increase the batch size too. The default batch is 16 and it only takes around 2GB of GPU memory while you train. The number of workers to load the data is set to 16. You can reduce this a bit to put less work on the CPU.","8e2b017d":"Since we are loading the real training dataset and not just the sample set, the following code block will take quite a bit of time.","c1cb5ae6":"### Prepare data path and config file","aeb4f801":"### Installing l5kit","e691b1a6":"Config is where you can make your changes to have different `model_architecture`, `history_step_size`, `history_num_frames`, `batch_size`, etc. Inspect `cfg` for more details.","edde26e3":"Due to the fact that the following steps take way too long, they are commented out."}}