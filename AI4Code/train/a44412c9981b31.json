{"cell_type":{"5cd07142":"code","8e959245":"code","7a62552e":"code","010c380f":"code","16909c96":"code","89133253":"code","3a45240f":"code","205717f5":"code","3b8cddc6":"code","571f2b1f":"code","5dae843e":"code","e0f4ed2f":"code","59fe0950":"code","dcc5f565":"code","51412934":"code","310b1292":"code","06d889a6":"code","5e2ed425":"code","c6c0c7fc":"code","a537c5f0":"code","8c974bf0":"code","40f9a543":"code","f223360e":"code","80d4eb64":"code","cf53e9ef":"code","fd40b5e2":"code","1a9820f9":"code","ea279a8e":"code","09758d23":"markdown","7728af63":"markdown","ba1e194a":"markdown","74f70120":"markdown","1bb5e8db":"markdown","58f67a0f":"markdown","c3538093":"markdown","4a94de21":"markdown","9258c3b7":"markdown"},"source":{"5cd07142":"from datetime import datetime\nstart = datetime.now()","8e959245":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","7a62552e":"from pathlib import Path\nimport json\nimport re\nimport sys\nimport warnings","010c380f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport pickle\nfrom sklearn.model_selection import train_test_split","16909c96":"import tensorflow as tf\nfrom tensorflow import keras\n\nfrom keras.preprocessing import text, sequence\nfrom keras.models import Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Activation, Dropout, Embedding, BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import Input, GlobalMaxPool1D, Conv1D, MaxPooling1D, GRU, concatenate, CuDNNGRU\nfrom keras.layers import LSTM, CuDNNLSTM,  Bidirectional, SpatialDropout1D\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\nfrom keras import utils\nfrom keras.utils import to_categorical\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints\nfrom keras.callbacks import Callback\nfrom keras.optimizers import Adam\n\nkeras.__version__, tf.__version__","89133253":"VERSION = 'Deep_Learning_predictions'","3a45240f":"DATA_DIR = Path('..\/input')\n\nBEAUTY_JSON = DATA_DIR \/ 'beauty_profile_train.json'\nFASHION_JSON = DATA_DIR \/ 'fashion_profile_train.json'\nMOBILE_JSON = DATA_DIR \/ 'mobile_profile_train.json'\n\nBEAUTY_TRAIN_CSV = DATA_DIR \/ 'beauty_data_info_train_competition.csv'\nFASHION_TRAIN_CSV = DATA_DIR \/ 'fashion_data_info_train_competition.csv'\nMOBILE_TRAIN_CSV = DATA_DIR \/ 'mobile_data_info_train_competition.csv'\n\nBEAUTY_TEST_CSV = DATA_DIR \/ 'beauty_data_info_val_competition.csv'\nFASHION_TEST_CSV = DATA_DIR \/ 'fashion_data_info_val_competition.csv'\nMOBILE_TEST_CSV = DATA_DIR \/ 'mobile_data_info_val_competition.csv'","205717f5":"with open(BEAUTY_JSON) as f:\n     beauty_attribs = json.load(f)\n        \nwith open(FASHION_JSON) as f:\n     fashion_attribs = json.load(f)\n        \nwith open(MOBILE_JSON) as f:\n     mobile_attribs = json.load(f)\n\nbeauty_train_df = pd.read_csv(BEAUTY_TRAIN_CSV)\nfashion_train_df = pd.read_csv(FASHION_TRAIN_CSV)\nmobile_train_df = pd.read_csv(MOBILE_TRAIN_CSV)\n\nbeauty_test_df = pd.read_csv(BEAUTY_TEST_CSV)\nfashion_test_df = pd.read_csv(FASHION_TEST_CSV)\nmobile_test_df = pd.read_csv(MOBILE_TEST_CSV)","3b8cddc6":"len(beauty_train_df), len(fashion_train_df), len(mobile_train_df)","571f2b1f":"len(beauty_test_df), len(fashion_test_df), len(mobile_test_df)","5dae843e":"n_rows = len(beauty_test_df)*5 + len(fashion_test_df)*5 + len(mobile_test_df)*11 \nprint(n_rows)\nassert n_rows == 977987, \"Row numbers don't match!\"","e0f4ed2f":"max_words_beauty = 29\nmax_words_fashion = 32\nmax_words_mobile = 27\nbatch_size = 32\nepochs_beauty = [25, 25, 25, 25, 25]\nepochs_fashion = [20, 20, 20, 20, 20]\nepochs_mobile = [20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20]\nembed_size = 256","59fe0950":"def build_model(max_features, max_len, num_classes, lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, \n                 kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n    inp = Input(shape=(max_len,))\n    x = Embedding(max_features, embed_size)(inp)\n    x1 = SpatialDropout1D(spatial_dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences=True))(x1)\n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences=True))(x1)\n\n    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\n    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\n\n    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\n    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\n\n    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\n    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\n\n    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\n    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\n\n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\n                     avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(dense_units, activation='relu')(x))\n    x = BatchNormalization()(x)\n    x = Dropout(dr)(Dense(int(dense_units\/2), activation='relu')(x))\n    x = Dense(units=num_classes, activation='softmax')(x)\n    \n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=lr, decay=lr_d), metrics=[\"accuracy\"])\n\n    return model","dcc5f565":"def RNN_model(max_features, num_classes, maxlen):    \n    model = build_model(max_features, maxlen, num_classes,\n                         lr = 1e-4, lr_d = 0, units = 64, spatial_dr = 0.5, \n                         kernel_size1=4, kernel_size2=3, dense_units=32, dr=0.1, conv_size=32)\n    return model","51412934":"print(f'Total attributes for beauty products: {len(beauty_attribs)}')\nprint()\n\nfor i, attrib in enumerate(beauty_attribs, 1):\n    print(f'Attribute_{i}: {attrib} ({len(beauty_attribs[attrib])} classes)')\n\nall_beauty_titles = list(beauty_train_df.title)\nbeauty_tokenizer = Tokenizer()\nbeauty_tokenizer.fit_on_texts(all_beauty_titles)\ntokenized_train_titles = beauty_tokenizer.texts_to_sequences(list(beauty_train_df.title))\nprint()\nprint(f'Total {len(beauty_tokenizer.word_index)} unique tokens.')","310b1292":"map_beauty = []\nfor i, attrib in enumerate(beauty_attribs):\n    num_classes = len(beauty_attribs[attrib])\n    print(f'[Attrib_{i+1} {attrib}]: ({num_classes} classes)')\n    beauty_attrib_train_df = beauty_train_df[['title', attrib]].dropna()\n    titles = list(beauty_attrib_train_df.title)\n    tokenized_train_titles = beauty_tokenizer.texts_to_sequences(titles)\n\n    X = np.array(tokenized_train_titles)\n    X = sequence.pad_sequences(X, padding='post', maxlen=max_words_beauty)\n    y = to_categorical(beauty_attrib_train_df[attrib], num_classes)\n    \n    max_features = len(beauty_tokenizer.word_index) # how many unique words to use\n    model = RNN_model(max_features, num_classes, max_words_beauty)\n    \n    file_path = f\"{VERSION}_model_beauty_{attrib}.hdf5\"\n    \n    hist = model.fit(X, y, \n                    batch_size=batch_size,\n                    epochs=epochs_beauty[i],\n                    verbose=0)\n    model.save(file_path)\n    print()","06d889a6":"duration = datetime.now()-start\nprint('Total seconds:', duration.total_seconds())\nprint(f'Total minutes: {duration.total_seconds()\/60:.2f}')\nprint(f'Total hour: {duration.total_seconds()\/3600:.2f}')","5e2ed425":"print(f'Total attributes for fashion products: {len(fashion_attribs)}')\nprint()\n\nfor i, attrib in enumerate(fashion_attribs, 1):\n    print(f'Attribute_{i}: {attrib} ({len(fashion_attribs[attrib])} classes)')\n\nall_fashion_titles = list(fashion_train_df.title)\nfashion_tokenizer = Tokenizer()\nfashion_tokenizer.fit_on_texts(all_fashion_titles)\ntokenized_train_titles = fashion_tokenizer.texts_to_sequences(list(fashion_train_df.title))\nprint()\nprint(f'Total {len(fashion_tokenizer.word_index)} unique tokens.')","c6c0c7fc":"map_fashion = []\nfor i, attrib in enumerate(fashion_attribs):\n    num_classes = len(fashion_attribs[attrib])\n    print(f'[Attrib_{i+1} {attrib}]: ({num_classes} classes)')\n    fashion_attrib_train_df = fashion_train_df[['title', attrib]].dropna()\n    titles = list(fashion_attrib_train_df.title)\n    tokenized_train_titles = fashion_tokenizer.texts_to_sequences(titles)\n\n    X = np.array(tokenized_train_titles)\n    X = sequence.pad_sequences(X, padding='post', maxlen=max_words_fashion)\n    y = to_categorical(fashion_attrib_train_df[attrib], num_classes)\n    \n    max_features = len(fashion_tokenizer.word_index) # how many unique words to use\n    model = RNN_model(max_features, num_classes, max_words_fashion)\n    \n    file_path = f\"{VERSION}_model_fashion_{attrib}.hdf5\"\n    \n    hist = model.fit(X, y, \n                    batch_size=batch_size,\n                    epochs=epochs_fashion[i],\n                    verbose=0)\n    model.save(file_path)\n    print()","a537c5f0":"duration = datetime.now()-start\nprint('Total seconds:', duration.total_seconds())\nprint(f'Total minutes: {duration.total_seconds()\/60:.2f}')\nprint(f'Total hour: {duration.total_seconds()\/3600:.2f}')","8c974bf0":"print(f'Total attributes for mobile products: {len(mobile_attribs)}')\nprint()\n\nfor i, attrib in enumerate(mobile_attribs, 1):\n    print(f'Attribute_{i}: {attrib} ({len(mobile_attribs[attrib])} classes)')\n\nall_mobile_titles = list(mobile_train_df.title)\nmobile_tokenizer = Tokenizer()\nmobile_tokenizer.fit_on_texts(all_mobile_titles)\ntokenized_train_titles = mobile_tokenizer.texts_to_sequences(list(mobile_train_df.title))\nprint()\nprint(f'Total {len(mobile_tokenizer.word_index)} unique tokens.')","40f9a543":"map_mobile = []\nfor i, attrib in enumerate(mobile_attribs):\n    num_classes = len(mobile_attribs[attrib])\n    print(f'[Attrib_{i+1} {attrib}]: ({num_classes} classes)')\n    mobile_attrib_train_df = mobile_train_df[['title', attrib]].dropna()\n    titles = list(mobile_attrib_train_df.title)\n    tokenized_train_titles = mobile_tokenizer.texts_to_sequences(titles)\n\n    X = np.array(tokenized_train_titles)\n    X = sequence.pad_sequences(X, padding='post', maxlen=max_words_mobile)\n    y = to_categorical(mobile_attrib_train_df[attrib], num_classes)\n    \n    max_features = len(mobile_tokenizer.word_index) # how many unique words to use\n    model = RNN_model(max_features, num_classes, max_words_mobile)\n    \n    file_path = f\"{VERSION}_model_mobile_{attrib}.hdf5\"\n    \n    hist = model.fit(X, y, \n                    batch_size=batch_size,\n                    epochs=epochs_mobile[i],\n                    verbose=0)\n    model.save(file_path)\n    print()","f223360e":"duration = datetime.now()-start\nprint('Total seconds:', duration.total_seconds())\nprint(f'Total minutes: {duration.total_seconds()\/60:.2f}')\nprint(f'Total hour: {duration.total_seconds()\/3600:.2f}')","80d4eb64":"%%time\nupdated_beauty_title = list(beauty_test_df.title)\ntokenized_test_titles = beauty_tokenizer.texts_to_sequences(updated_beauty_title)\nX_test = np.array(tokenized_test_titles)\nX_test = sequence.pad_sequences(X_test, padding='post', maxlen=max_words_beauty)\n\npreds_all={}\nfor attrib in beauty_attribs:\n    print(f'Attribute: {attrib}')\n    print(f'Loading model...')\n    model=load_model(f'{VERSION}_model_beauty_{attrib}.hdf5')\n    print(f'Predicting...')\n    pred_attrib=model.predict(X_test)\n    print(f'Sorting predictions...')\n    pred_list=[]\n    for pred in pred_attrib:\n        pred_list.append(pred.argsort()[-2:][::-1])\n    preds_all[attrib]=np.array(pred_list.copy())\n    print()\n    \nprint(f'Saving to {VERSION}_pred_beauty.csv')\ntest_y_id=[] \ntest_y_predictions=[]\nfor i, itemid in enumerate(beauty_test_df.itemid):\n    for attrib in beauty_attribs:\n        test_y_id.append(str(itemid) + f'_{attrib}')\n        test_y_predictions.append(str(preds_all[attrib][i][0]) + ' ' + str(preds_all[attrib][i][1]))\n        \nbeauty_result_df = pd.DataFrame(\n    {'id': test_y_id, 'tagging': test_y_predictions},\n    columns = ['id', 'tagging'])\n\nbeauty_result_df.to_csv(f'{VERSION}_pred_beauty.csv', index=False)","cf53e9ef":"%%time\nupdated_fashion_title = list(fashion_test_df.title)\ntokenized_test_titles = fashion_tokenizer.texts_to_sequences(updated_fashion_title)\nX_test = np.array(tokenized_test_titles)\nX_test = sequence.pad_sequences(X_test, padding='post', maxlen=max_words_fashion)\n\npreds_all={}\nfor attrib in fashion_attribs:\n    print(f'Attribute: {attrib}')\n    print(f'Loading model...')\n    model=load_model(f'{VERSION}_model_fashion_{attrib}.hdf5')\n    print(f'Predicting...')\n    pred_attrib=model.predict(X_test)\n    print(f'Sorting predictions...')\n    pred_list=[]\n    for pred in pred_attrib:\n        pred_list.append(pred.argsort()[-2:][::-1])\n    preds_all[attrib]=np.array(pred_list.copy())\n    print()\n    \nprint(f'Saving to {VERSION}_pred_fashion.csv')\ntest_y_id=[] \ntest_y_predictions=[]\nfor i, itemid in enumerate(fashion_test_df.itemid):\n    for attrib in fashion_attribs:\n        test_y_id.append(str(itemid) + f'_{attrib}')\n        test_y_predictions.append(str(preds_all[attrib][i][0]) + ' ' + str(preds_all[attrib][i][1]))\n        \nfashion_result_df = pd.DataFrame(\n    {'id': test_y_id, 'tagging': test_y_predictions},\n    columns = ['id', 'tagging'])\n\nfashion_result_df.to_csv(f'{VERSION}_pred_fashion.csv', index=False)","fd40b5e2":"%%time\nupdated_mobile_title = list(mobile_test_df.title)\ntokenized_test_titles = mobile_tokenizer.texts_to_sequences(updated_mobile_title)\nX_test = np.array(tokenized_test_titles)\nX_test = sequence.pad_sequences(X_test, padding='post', maxlen=max_words_mobile)\n\npreds_all={}\nfor attrib in mobile_attribs:\n    print(f'Attribute: {attrib}')\n    print(f'Loading model...')\n    model=load_model(f'{VERSION}_model_mobile_{attrib}.hdf5')\n    print(f'Predicting...')\n    pred_attrib=model.predict(X_test)\n    print(f'Sorting predictions...')\n    pred_list=[]\n    for pred in pred_attrib:\n        pred_list.append(pred.argsort()[-2:][::-1])\n    preds_all[attrib]=np.array(pred_list.copy())\n    print()\n    \nprint(f'Saving to {VERSION}_pred_mobile.csv')\ntest_y_id=[] \ntest_y_predictions=[]\nfor i, itemid in enumerate(mobile_test_df.itemid):\n    for attrib in mobile_attribs:\n        test_y_id.append(str(itemid) + f'_{attrib}')\n        test_y_predictions.append(str(preds_all[attrib][i][0]) + ' ' + str(preds_all[attrib][i][1]))\n        \nmobile_result_df = pd.DataFrame(\n    {'id': test_y_id, 'tagging': test_y_predictions},\n    columns = ['id', 'tagging'])\n\nmobile_result_df.to_csv(f'{VERSION}_pred_mobile.csv', index=False)","1a9820f9":"combined_df = pd.concat([beauty_result_df, fashion_result_df, mobile_result_df], axis=0)\ncombined_df.to_csv(f'{VERSION}.csv', index=None)","ea279a8e":"duration = datetime.now()-start\nprint('Total seconds:', duration.total_seconds())\nprint(f'Total minutes: {duration.total_seconds()\/60:.2f}')\nprint(f'Total hour: {duration.total_seconds()\/3600:.2f}')","09758d23":"## Fashion","7728af63":"The neural network architecture is based on https:\/\/www.kaggle.com\/artgor\/movie-review-sentiment-analysis-eda-and-models \n\nActually this model is an overkill for this competition and takes too long (8 hours!) to train and predict. This is what happened when you copied someone's code without knowing (or assumed wrongly) what you were actually doing.\n\nTeam yellow's simple feedforward neural network (https:\/\/www.kaggle.com\/astraldawn\/yellow-feedforward-neural-network) is much better in terms of speed and performance.","ba1e194a":"# Hyperparameters","74f70120":"## Mobile","1bb5e8db":"# Import","58f67a0f":"# Inference","c3538093":"## Beauty","4a94de21":"# Models","9258c3b7":"# Training"}}