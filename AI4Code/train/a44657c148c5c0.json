{"cell_type":{"0a75371d":"code","b5912967":"code","c0b52269":"code","2f4e206e":"code","4588d4a3":"code","a046e502":"code","07d984f7":"code","6fde3545":"code","401f79dc":"code","b8340aab":"code","7422f1cc":"code","5aba3771":"code","6c41c7d3":"code","9bbf8a16":"code","fa44c6e2":"code","e1743ea5":"code","007df922":"code","31f55238":"code","4b585f2c":"code","048df537":"code","07d466fa":"code","2391ed83":"code","33aa9609":"code","eeb009df":"code","f63bf642":"code","2cb40fe6":"code","2fd97af6":"code","44f083c7":"code","3d85da4b":"code","aca5f090":"code","93565f4d":"code","a45fa954":"code","eebe0991":"code","55dba7cf":"code","34051b20":"code","8cc79619":"code","4c44b6c5":"code","3d1a8bc8":"code","8779623c":"code","d59ac5c3":"code","b5cb15c0":"code","23c50111":"code","aa72bf35":"code","bb6eb25c":"code","3d5d02a3":"code","3903ece8":"code","349f6d3b":"code","54f945d9":"code","d2e74b45":"code","7f8361ea":"code","6c77a926":"code","1ddd0eaa":"code","3df23ce9":"code","e3e13451":"code","63583ffe":"code","b9768953":"code","6f5b137b":"code","57cc62f4":"code","9dbbd97a":"code","07dc9e56":"code","f141744d":"code","1e0a9e5a":"code","9264f756":"code","064f5235":"code","f80dc9eb":"code","6c078dd6":"code","9dc598d7":"code","48822594":"code","9227f8ca":"markdown","71943abd":"markdown","a855cd39":"markdown","3cfa19d8":"markdown","6dd3c7a0":"markdown","6ce739a7":"markdown","9d959056":"markdown"},"source":{"0a75371d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.decomposition import PCA\n\n# Data splitting and model parameter search\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV                 \nfrom sklearn.model_selection import RandomizedSearchCV         ","b5912967":"from xgboost.sklearn import XGBClassifier\n\n\n# Model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\n\n# Model evaluation metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import confusion_matrix\n\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\n\n# Needed for Bayes optimization\n\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nimport time\nimport gc\nimport random\nfrom scipy.stats import uniform","c0b52269":"# Set option to dislay many rows\npd.set_option('display.max_columns', 100)","2f4e206e":"#Reading file \ndf= pd.read_csv(\"..\/input\/winequalityN.csv\")","4588d4a3":"df.head()","a046e502":"df.info()","07d984f7":"df.shape","6fde3545":"df.columns.values","401f79dc":"df.describe()","b8340aab":"df.isnull().any()","7422f1cc":"df.dropna(axis=0,inplace=True)","5aba3771":"df.shape","6c41c7d3":"import seaborn as sns\nsns.countplot(x = df.quality, data=df, hue='type')","9bbf8a16":"plt.figure(figsize=(14,14))\nsns.heatmap(df.iloc[:,0:13].corr(), cbar = True,  square = True, annot=True, cmap= 'ocean_r')","fa44c6e2":"fig = plt.figure(figsize=(22,10))\nfeatures = [\"total sulfur dioxide\", \"residual sugar\", \"volatile acidity\", \"total sulfur dioxide\", \"chlorides\", \"fixed acidity\", \"density\",\"sulphates\"]\n\nfor i in range(8):\n    ax1 = fig.add_subplot(2,4,i+1)\n    sns.boxplot(x=\"type\", y=features[i],data=df, palette=\"rocket\");","e1743ea5":"fig = plt.figure(figsize=(24,10))\nfeatures = [\"total sulfur dioxide\", \"residual sugar\", \"volatile acidity\", \"total sulfur dioxide\", \"chlorides\", \"fixed acidity\", \"citric acid\",\"sulphates\"]\n\nfor i in range(8):\n    ax1 = fig.add_subplot(2,4,i+1)\n    sns.barplot(x='quality', y=features[i],data=df, hue='type', palette='rocket')","007df922":"#Splitting data into predictors and target\nX = df.iloc[ :, 1:13]\ny = df.iloc[ : , 0]","31f55238":"X.head()","4b585f2c":"y.head()","048df537":"#  Map Target data to '1' and '0'\ny = y.map({'white':1, 'red' : 0})\ny.dtype ","07d466fa":"colnames = X.columns.tolist()\ncolnames","2391ed83":"# Split dataset into train and test parts\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.30,\n                                                    shuffle = True\n                                                    )","33aa9609":"X_train.shape ","eeb009df":"X_test.shape ","f63bf642":"y_train.shape","2cb40fe6":"y_test.shape","2fd97af6":"#Creating  pipelines\n\n#### Pipe using XGBoost and instantiating it.\n\nsteps_xg = [('sts', ss() ),\n            ('pca', PCA()),\n            ('xg',  XGBClassifier(silent = False,\n                                  n_jobs=2)        # Specify other parameters here\n            )\n            ]\n\npipe_xg = Pipeline(steps_xg)","44f083c7":"parameters = {'xg__learning_rate':  [0.05, 0.4],\n              'xg__n_estimators':   [50,  80],\n              'xg__max_depth':      [3,5],\n              'pca__n_components' : [5,8]\n              }          ","3d85da4b":"clf = GridSearchCV(pipe_xg,            # pipeline object\n                   parameters,         # possible parameters\n                   n_jobs = 2,         # USe parallel cpu threads\n                   cv =2 ,             # No of folds\n                   verbose =2,         # Higher the value, more the verbosity\n                   scoring = ['accuracy', 'roc_auc'],  # Metrics for performance\n                   refit = 'roc_auc'   # Refitting final model on what parameters?\n                                       # Those which maximise auc\n                   )","aca5f090":"start = time.time()\nclf.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","93565f4d":"f\"Best Score: {clf.best_score_} \"","a45fa954":"f\"Best Parameter set {clf.best_params_}\"","eebe0991":"y_pred_gs = clf.predict(X_test)","55dba7cf":"# Accuracy\naccuracy_gs = accuracy_score(y_test, y_pred_gs)\nf\"Accuracy: {accuracy_gs * 100.0}\"","34051b20":"plt.bar(clf.best_params_.keys(), clf.best_params_.values())\nplt.xticks(rotation=70)","8cc79619":"# Instantiate the importance object\nperm = PermutationImportance(\n                            clf,\n                            random_state=1\n                            )\n\n# fit data & learn\nstart = time.time()\nperm.fit(X_test, y_test)\nend = time.time()\n(end - start)\/60","4c44b6c5":"eli5.show_weights(\n                  perm,\n                  feature_names = colnames      # X_test.columns.tolist()\n                  )","3d1a8bc8":"fw = eli5.explain_weights_df(\n                  perm,\n                  feature_names = colnames      # X_test.columns.tolist()\n                  )\n\nfw","8779623c":"parameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,80),\n              'xg__max_depth':      range(3,5),\n              'pca__n_components' : range(5,7)}","d59ac5c3":"rs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=12,          # Max combination of\n                                            # parameter to try. Default = 10\n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 2,          # Use parallel cpu threads\n                        cv = 2               # No of folds.\n                                             # So n_iter * cv combinations\n                        )","b5cb15c0":"start = time.time()\nrs.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","23c50111":"f\"Best Score: {rs.best_score_} \"","aa72bf35":"f\"Best Parameter set: {rs.best_params_} \"","bb6eb25c":"# Make predictions\ny_pred_rs = rs.predict(X_test)","3d5d02a3":"# Accuracy\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\nf\"Accuracy: {accuracy_rs * 100.0}\"","3903ece8":"plt.bar(rs.best_params_.keys(), rs.best_params_.values())\nplt.xticks(rotation=50)","349f6d3b":"para_set = {\n           'learning_rate':  (0.3, 0.9),                 \n           'n_estimators':   (60,90),               \n           'max_depth':      (3,5),                 \n           'n_components' :  (5,7)          \n            }","54f945d9":"def xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    #Pipeling for Bayesian Optimization\n    pipe_xg1 = make_pipeline (ss(),\n                              PCA(n_components=int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs=2,\n                                           learning_rate=learning_rate,\n                                           max_depth=int(round(max_depth)),\n                                           n_estimators=int(round(n_estimators))\n                                           )\n                             )\n    #Fitting into pipeline \n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X= X_train,\n                                y = y_train,\n                                cv = 2,\n                                n_jobs = 2,\n                                scoring = 'f1'\n                                ).mean()             # taking mean of all results\n\n    return cv_result       #Returning final mean of all results of cross val score\n","d2e74b45":"xgBO = BayesianOptimization(\n                             xg_eval, \n                             para_set \n                             )","7f8361ea":"start = time.time()\nxgBO.maximize(init_points=5,    \n               n_iter=25,        \n               )\nend = time.time()\n(end-start)\/60","6c77a926":"#Results\nxgBO.res","1ddd0eaa":"#Best parametrs in maximizing the objective:\nxgBO.max","3df23ce9":"for features in xgBO.max.values():\n        print(features)","e3e13451":"plt.bar(features.keys(), features.values())\nplt.xticks(rotation=50)","63583ffe":"# Model with parameters of grid search\nmodel_gs = XGBClassifier(\n                    learning_rate = clf.best_params_['xg__learning_rate'],\n                    max_depth = clf.best_params_['xg__max_depth'],\n                    n_estimators=clf.best_params_['xg__n_estimators']\n                    )\n\n#  Model with parameters of random search\nmodel_rs = XGBClassifier(\n                    learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__n_estimators']\n                    )\n\n#  Model with parameters of Bayesian Optimization\nmodel_bo = XGBClassifier(\n                    learning_rate = xgBO.max['params']['learning_rate'],\n                    max_depth = int(xgBO.max['params']['max_depth']),\n                    n_estimators= int(xgBO.max['params']['n_estimators'])\n                    )","b9768953":"start = time.time()\nmodel_gs.fit(X_train, y_train)\nmodel_rs.fit(X_train, y_train)\nmodel_bo.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60","6f5b137b":"# Predictions with all the models\ny_pred_gs = model_gs.predict(X_test)\ny_pred_rs = model_rs.predict(X_test)\ny_pred_bo = model_bo.predict(X_test)","57cc62f4":"# Accuracy of all the models\naccuracy_gs = accuracy_score(y_test, y_pred_gs)\naccuracy_rs = accuracy_score(y_test, y_pred_rs)\naccuracy_bo = accuracy_score(y_test, y_pred_bo)\nprint(\"Accuracy by Grid Search           = \",accuracy_gs)\nprint(\"Accuracy by Random Search         = \",accuracy_rs)\nprint(\"Accuracy by Bayesian Optimization = \",accuracy_bo)","9dbbd97a":"# Get feature importances from all the models\nmodel_gs.feature_importances_\nmodel_rs.feature_importances_\nmodel_bo.feature_importances_\nplot_importance(model_gs)\nplot_importance(model_rs)\nplot_importance(model_bo)","07dc9e56":"#Confusion Matrix for Grid Search model\nconfusion_matrix(y_test,y_pred_gs)","f141744d":"#Confusion Matrix for Random Search model\nconfusion_matrix(y_test,y_pred_rs)","1e0a9e5a":"#Confusion Matrix for Bayesian Optimization model\nconfusion_matrix(y_test,y_pred_bo)","9264f756":"# Get probability of occurrence of each class\ny_pred_prob_gs = model_gs.predict_proba(X_test)\ny_pred_prob_rs = model_rs.predict_proba(X_test)\ny_pred_prob_bo = model_bo.predict_proba(X_test)","064f5235":"# calculate fpr, tpr values\nfpr_gs, tpr_gs, thresholds = roc_curve(y_test,\n                                 y_pred_prob_gs[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_rs, tpr_rs, thresholds = roc_curve(y_test,\n                                 y_pred_prob_rs[: , 1],\n                                 pos_label= 1\n                                 )\n\nfpr_bo, tpr_bo, thresholds = roc_curve(y_test,\n                                 y_pred_prob_bo[: , 1],\n                                 pos_label= 1\n                                 )\n\n","f80dc9eb":"fig = plt.figure(figsize=(12,10))  \nax = fig.add_subplot(111)   # Create axes\n\n#Connect diagonals\nax.plot([0, 1], [0, 1], ls=\"--\")  \n\n#Labels \nax.set_xlabel('False Positive Rate')  \nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve for models')\n\n#Set graph limits\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.0])\n\n#Plot each graph now\nax.plot(fpr_gs, tpr_gs, label = \"gs\")\nax.plot(fpr_rs, tpr_rs, label = \"rs\")\nax.plot(fpr_bo, tpr_bo, label = \"bo\")\n\n\n#Set legend and show plot\nax.legend(loc=\"lower right\")\nplt.show()\n","6c078dd6":"# Calculate AUC\nauc_gs = auc(fpr_gs,tpr_gs)\nauc_rs = auc(fpr_rs,tpr_rs)\nauc_bo = auc(fpr_bo,tpr_bo)","9dc598d7":"#Calculate Precision, Recall and F1 Score\nfrom sklearn.metrics import precision_recall_fscore_support\nprecision_gs,recall_gs,f1_gs,_ = precision_recall_fscore_support(y_test,y_pred_gs)\nprecision_rs,recall_rs,f1_rs,_ = precision_recall_fscore_support(y_test,y_pred_rs)\nprecision_bo,recall_bo,f1_bo,_ = precision_recall_fscore_support(y_test,y_pred_bo)","48822594":"pc = pd.DataFrame({ \"Classifiers\":[\"Grid Search\",\"Random Search\",'Bayesian Optimization'],\n                             \"Accuracy\": [accuracy_gs,accuracy_rs,accuracy_bo],\n                             \"Precision\": [precision_gs,precision_rs,precision_bo],\n                             \"Recall\":[recall_gs,recall_rs,recall_bo],\n                             \"f1_score\":[f1_gs,f1_rs,f1_bo],\n                             \"AUC\":[auc_gs,auc_rs,auc_bo]})\npc","9227f8ca":"# Performance Comparision of all models","71943abd":"# Confusion matrix for all the models","a855cd39":"Fitting parameters in model","3cfa19d8":"# Draw ROC curve","6dd3c7a0":"Bayesian Optimization","6ce739a7":"Grid Search","9d959056":"Random Search"}}