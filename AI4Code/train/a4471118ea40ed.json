{"cell_type":{"e8884b51":"code","1fa42afd":"code","cdb33c43":"code","55b51329":"code","4b27f9e5":"code","7a07bbf5":"code","7ff34745":"code","bc39be8a":"code","417beead":"code","dae4dac8":"code","c7f59eb6":"code","c8aafef9":"code","3af50e5e":"code","25096f52":"code","0cce39ca":"code","4642fa02":"code","cb1ff6d5":"code","1ca577ac":"code","cf2d9eda":"code","6ffe5148":"code","7f276554":"code","4d79552e":"code","c3ecee0c":"code","fd558c80":"code","bc0506a5":"code","74e62ff7":"code","8629271a":"code","57c15a93":"code","bee3f7b9":"markdown","197ae4f1":"markdown","0df70c5a":"markdown","8256ce2f":"markdown","ee9d4f74":"markdown","14a76f55":"markdown","23e565f2":"markdown","2ae87cd7":"markdown","500ae446":"markdown","2a69d602":"markdown","0741737b":"markdown","2750256b":"markdown","571c8143":"markdown","4d7ebbfb":"markdown","1f9bba53":"markdown","ee5cbd2f":"markdown","3bd344f7":"markdown","0d97708c":"markdown","900e99b6":"markdown","aec6b04e":"markdown","51936c09":"markdown","69731560":"markdown","ab0e6081":"markdown","847395a3":"markdown","5b463e4d":"markdown","00cd6df1":"markdown","758cea04":"markdown","209eda20":"markdown","99fa6aac":"markdown","1eb90473":"markdown","3bfc75f0":"markdown","9f5d2aba":"markdown","5d9dd2ed":"markdown","f317481b":"markdown"},"source":{"e8884b51":"%reset -f\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans # Class to develop kmeans model\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_score # base for clustering\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.mixture import GaussianMixture \n\n# Use white grid plot background from seaborn\nsns.set(font_scale=0.5, style=\"ticks\")\nsns.set_context(\"poster\", font_scale = .5, rc={\"grid.linewidth\": 0.6})\nimport warnings\nimport os\n%matplotlib inline\nplt.rcParams.update({'figure.max_open_warning': 0}) #just to suppress warning for max plots of 20\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n#os.chdir(\"..\/input\/ccdata\/\")\n#os.listdir()            # List all files in the folder","1fa42afd":"# Display output not only of last command but all commands in a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","cdb33c43":"# Set pandas options to display results\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 1000","55b51329":"# Load dataset\ncc = pd.read_csv(\"\/kaggle\/input\/ccdata\/CC GENERAL.csv\")\n#Let's try to analyze the dataset based on what is availiable with us\ncc.info()\ncc.head()\ncc.describe()","4b27f9e5":"cc.columns = [i.lower() for i in cc.columns]\ncc.columns","7a07bbf5":"del cc['cust_id']\ncc.columns","7ff34745":"def missing_columns_data(data):\n    miss      = data.isnull().sum()\n    miss_pct  = 100 * data.isnull().sum()\/len(data)\n    \n    miss_pct      = pd.concat([miss,miss_pct], axis=1)\n    missing_cols = miss_pct.rename(columns = {0:'Missings', 1: 'Missing pct'})\n    missing_cols = missing_cols[missing_cols.iloc[:,1]!=0].sort_values('Missing pct', ascending = False).round(1)\n    \n    return missing_cols  \n\nmissing = missing_columns_data(cc)\nmissing","bc39be8a":"null_counts = cc.isnull().sum()\/len(cc);\nplt.figure(figsize=(15,4));\nplt.xticks(np.arange(len(null_counts))+0.5,null_counts.index,rotation='vertical');\nplt.ylabel('Fraction of Rows with missing data');\nplt.bar(np.arange(len(null_counts)),null_counts);","417beead":"# Set up the matplotlib figure\nf, axes = plt.subplots(2, 2, figsize=(10, 7), sharex=True);\nsns.despine(left=True);\n\n# Plot a kernel density estimate and rug plot\nsns.distplot(cc.minimum_payments, hist=True, rug=True, color=\"r\", ax=axes[0, 1]);\nsns.distplot(cc.credit_limit, hist=True, rug=True, color=\"b\", ax=axes[0, 0]);\n\n# Plot a filled kernel density estimate\nsns.distplot(cc.minimum_payments,  color=\"g\",ax=axes[1, 1]);\nsns.distplot(cc.credit_limit, color=\"m\", ax=axes[1, 0]);\n\nplt.setp(axes, yticks=[]);\nplt.tight_layout();","dae4dac8":"values = { 'minimum_payments' : cc['minimum_payments'].median(),\n           'credit_limit' : cc['credit_limit'].median()\n          }\ncc.fillna(value = values, inplace = True)\nmissing = missing_columns_data(cc)\nmissing","c7f59eb6":"# Get column names first\nnames = cc.columns\n# Create the Scaler object\nscaler = preprocessing.StandardScaler()\n# Fit data on the scaler object\nscaled_df = scaler.fit_transform(cc)\n# Converting the numpy array into a pandas DataFrame\nscaled_df = pd.DataFrame(scaled_df, columns=names)\n# Normalizing the Data \nnormalized_df = normalize(scaled_df) #out\n# Converting the numpy array into a pandas DataFrame \ndf_out = pd.DataFrame(normalized_df,columns=names) # Normalized Dataframe","c8aafef9":"df_out.head()","3af50e5e":"fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(18, 18));\nfor i in range(4):\n    for j in range(4):\n        sns.distplot(df_out[df_out.columns[4 * i + j]], ax=axs[i,j]);\n        sns.despine();\nplt.show();\nfig, axs = plt.subplots(nrows=1, ncols=1, figsize=(4,4));\nsns.distplot(df_out.tenure, ax=axs);\nsns.despine();\nplt.show();","25096f52":"sns.set_style(\"darkgrid\");\nplt.figure(figsize=(17,10));\nsns.boxplot(data=df_out);\n#sns.violinplot(data=df_out);\n#sns.stripplot(data=df_out);\nplt.xticks(rotation=90);","0cce39ca":"#Using Pearson Correlation\nplt.figure(figsize=(12,10));\ncor = df_out.corr();\nsns.heatmap(cor, annot=True, cmap=plt.cm.PiYG);\nplt.title('Correlation Matrix')\nplt.show();","4642fa02":"# How many clusters?\n#     Use either AIC or BIC as criterion\n#     Ref: https:\/\/en.wikipedia.org\/wiki\/Akaike_information_criterion\n#          https:\/\/en.wikipedia.org\/wiki\/Bayesian_information_criterion\n#          https:\/\/www.quora.com\/What-is-an-intuitive-explanation-of-the-Akaike-information-criterion\nbic = []\naic = []\nfor i in range(8):\n    gm = GaussianMixture(\n                     n_components = i+1,\n                     n_init = 10,\n                     max_iter = 100);\n    gm.fit(df_out);\n    bic.append(gm.bic(df_out));\n    aic.append(gm.aic(df_out));","cb1ff6d5":"sns.set_style(\"whitegrid\")\n# Use white grid plot background from seaborn\nsns.set(font_scale=0.5, style=\"ticks\")\nsns.set_context(\"poster\", font_scale = .5, rc={\"grid.linewidth\": 0.6})\n#Draw aic ,bic on plot to understand\n\nn_clusters=np.arange(0, 8);\nfig, ax = plt.subplots(1, 2, figsize=(12,5));\n\nplt.subplot(1, 2, 1);\nplt.plot(n_clusters, aic,marker=\"o\", label='AIC');\nplt.title(\"AIC Scores\");\nplt.xticks(n_clusters);\nplt.xlabel(\"No. of clusters\");\nplt.ylabel(\"Scores\");\nplt.legend();\n\nplt.subplot(1, 2, 2);\nplt.plot(n_clusters, bic, marker=\"o\",label='BIC');\nplt.title(\"BIC Scores\");\nplt.xticks(n_clusters);\nplt.xlabel(\"No. of clusters\");\nplt.ylabel(\"Scores\");\nplt.legend();\n\nplt.show();","1ca577ac":"#Gussian Mixture\ngm = GaussianMixture(n_components = 3,\n                     n_init = 10,\n                     max_iter = 100);\ngm.fit(df_out);\n# Where are the cluster centers\ngm.means_\n# Did algorithm converge?\ngm.converged_\n#  How many iterations did it perform?\ngm.n_iter_\n#  Clusters labels\n#gm.predict(df_out)\n\n# Weights of respective gaussians.GMM can also be used as a generator\n#to generate data having similar pattern. All three Gaussians may generate data\n#as per their specifc pdf (prob density functions). But to generate as per same\n#pattern, as the original data, selection of data-sources (the three Gaussian)\n#has prob distribution. Weights describe the prob distribution.\n#Values of these weights are close to frequency of data-points per cluster\ngm.weights_\n\n#  What is the frequency of data-points for the three clusters.\nnp.unique(gm.predict(df_out), return_counts = True)[1]\/len(df_out)","cf2d9eda":"#Draw scatter plot to visualize the GMM output\nfig = plt.figure();\n\nplt.scatter(df_out.iloc[:, 0], df_out.iloc[:, 1],\n            c=gm.predict(df_out),\n            s=5,cmap =plt.cm.Greens\n           );\nplt.scatter(gm.means_[:, 0], gm.means_[:, 1],\n            marker='>',\n            s=100,               # marker size\n            linewidths=5,      # linewidth of marker edges\n            cmap =plt.cm.Greens\n            );\nplt.show()","6ffe5148":"gm = GaussianMixture(\n                     n_components = 3,\n                     n_init = 10,\n                     max_iter = 100)\ngm.fit(df_out)\n\ntsne = TSNE(n_components = 2, perplexity=50)\ntsne_out = tsne.fit_transform(df_out)\nplt.scatter(tsne_out[:, 0], tsne_out[:, 1],\n            marker='x',\n            s=5,              # marker size\n            linewidths=5,      # linewidth of marker edges\n            c=gm.predict(df_out)   # Colour as per gmm\n            )","7f276554":"#df_out['cluster'] = gm.predict(df_out)\n#len(df_out.cluster.unique())\narr=gm.predict(df_out);\ndf_cluster=pd.DataFrame(arr,columns=['cluster'])\ndf_out_cluster = pd.concat([df_out,df_cluster],axis=1)\n#df_cluster.head()\ndf_out_cluster.head()","4d79552e":"# Number of clients by cluster\ndf_out_cluster['cluster'].value_counts().plot.bar(figsize=(10,5), title='Clusterwise No. of Customers');\n\ndf_out_cluster['cluster'].value_counts()","c3ecee0c":"#pairwise relationships of key_features\n\nkey_features = [\"balance\", \"purchases\", \"cash_advance\",\"credit_limit\", \"payments\", \"minimum_payments\", \"tenure\",\"cluster\"]\n\n#key_features.append(\"cluster\")\nplt.figure(figsize=(25,25))\nsns.pairplot( df_out_cluster[key_features], hue=\"cluster\")","fd558c80":"densities = gm.score_samples(normalized_df)\ndensity_threshold = np.percentile(densities,4)\n\n# anomalies data\nanomalies = normalized_df[densities < density_threshold] # Data of anomalous customers\ndf_anomaly=pd.DataFrame(anomalies,columns=df_out.columns)\n# Unanomalous data\nunanomalous = normalized_df[densities >= density_threshold] # Data of unanomalous customers\ndf_unanomaly=pd.DataFrame(unanomalous,columns=df_out.columns)","bc0506a5":"#Density Plot Function to draw plots for Anomalous and Un-Anamalous data\ndef densityplots(df1,df2, label1 = \"Anomalous\",label2 = \"Normal\"):\n    # df1 and df2 are two dataframes\n    # As number of features are 17, we have 20 axes\n    fig, axes = plt.subplots(nrows=4, ncols=5, figsize=(18,15))\n    ax = axes.flatten()\n    fig.tight_layout()\n    # Do not display 18th, 19th and 20th axes\n    axes[3,3].set_axis_off()\n    axes[3,2].set_axis_off()\n    axes[3,4].set_axis_off()\n    # Below 'j' is not used.\n    for i,j in enumerate(df1.columns):\n        # https:\/\/seaborn.pydata.org\/generated\/seaborn.distplot.html\n        # For every i, draw two overlapping density plots in different colors\n        sns.distplot(df1.iloc[:,i],\n                     ax = ax[i],\n                     kde_kws={\"color\": \"k\", \"lw\": 3, \"label\": label1},   # Density plot features\n                     hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\"alpha\": 1, \"color\": \"g\"}) # Histogram features\n        sns.distplot(df2.iloc[:,i],\n                     ax = ax[i],\n                     kde_kws={\"color\": \"red\", \"lw\": 3, \"label\": label2},\n                     hist_kws={\"histtype\": \"step\", \"linewidth\": 3,\"alpha\": 1, \"color\": \"b\"})\n","74e62ff7":"# Draw density plots now from two dataframes of anomalous dataframe (df_anomaly) and un-anomalous dataframe (df_unanomaly)\ndensityplots(df_anomaly, df_unanomaly, label1 = \"Anomalous\",label2 = \"Normal\")","8629271a":"df_anomaly['type'] = 'anomalous'  \ndf_unanomaly['type'] = 'unanomalous'\ndf_all = pd.concat([df_anomaly,df_unanomaly])","57c15a93":"fig, axs = plt.subplots(nrows=4, ncols=4, figsize=(18, 18));\nfor i in range(4):\n    for j in range(4):\n        sns.boxplot(x = df_all['type'],  y = df_all[df_all.columns[4 * i + j]], ax=axs[i,j]);\n        sns.despine();\nplt.show();\nfig, axs = plt.subplots(nrows=1, ncols=1, figsize=(4,4));\nsns.boxplot(x = df_all['type'],y=df_all.tenure, ax=axs);\nsns.despine();\nplt.show();","bee3f7b9":"<a id=gmm><\/a>\n### <font color=crimson> GMM Clustering\nGaussian Mixture Models(GMM):\nA probabilistic approach to clustering addressing many of these problems. In this approach we describe each cluster by its centroid (mean), covariance , and the size of the cluster(Weight)\n\n    \n##### Bayesian information criterion (BIC) & Akaike information criterion (AIC)\nIn statistics, the Bayesian information criterion (BIC) is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC).\n\nWhen fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.\n    \nThis criterion gives us an estimation on how much is good the GMM in terms of predicting the data we actually have. The lower is the BIC, the better is the model to actually predict the data we have. In order to avoid overfitting, this technique penalizes models with big number of clusters. \nFollowing this criterion, the bigger the number of clusters, the better should be the model. Which means that the penalty BIC criteria gives to complex models do not save us from overfit.","197ae4f1":"<a id=viz_standardization><\/a>\n### <font color=crimson> Visualization - Feature standardization & observation-normalization","0df70c5a":"- Cluster 0: This customer group indicates a small group of customers who are small spenders with the lowest minimum payment.\n- Cluster 1: These customers purchase frequently with the high cash advance percentage. This group is using their credit cards for a small number of purchases and lower payments\/minimum payments.\n- Cluster 2 customers have maximum purchases, maximum limits but low cash advances","8256ce2f":"We can notice two things. The first is that the curves are fairly smooth and monotone. The second is that the curve follows different slopes in different part of it. Starting from these two observations, the temptation to check where the BIC curve change slope is big. ","ee9d4f74":" <a id=fillmissingval><\/a>\n <font color=crimson> Both 'minimum_payments' and 'credit_limit' features are right-skewed. Since both the features are right-skewed, we fill the missing values with median()","14a76f55":"<a id=fielddscr><\/a>\n<font color=\"crimson\">    \n#### Field Description\n<\/font>   \n- CUST_ID : Identification of Credit Card holder (Categorical)\n- BALANCE : Balance amount left in Customer account to make further purchases \n- BALANCE_FREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n- PURCHASES : Amount of purchases made from account\n- ONEOFF_PURCHASES : Maximum purchase amount done in one-go\n- INSTALLMENTS_PURCHASES : Amount of purchase done in installment\n- CASH_ADVANCE : Cash in advance given by the Customer\n- PURCHASES_FREQUENCY : How frequently the Purchases are being made,score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n- ONEOFF_PURCHASES_FREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n- PURCHASES_INSTALLMENTS_FREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n- CASH_ADVANCE_FREQUENCY : How frequently the cash in advance being paid\n- CASH_ADVANCE_TRX : Number of Transactions made with \"Cash in Advance\"\n- PURCHASES_TRX : Number of purchase transactions made\n- CREDIT_LIMIT : Limit of Credit Card for Customer \n- PAYMENTS : Amount of Payment done by Customer\n- MINIMUM_PAYMENTS : Minimum amount of payments made by Customer\n- PRC_FULL_PAYMENT : Percent of full payment paid by Customer\n- TENURE : Tenure of credit card service for Customer\n\n<font color=\"crimson\"> \n#### Findings from Dataset available:\n<\/font>    \n- Average\/Mean BALANCE is 1564 \n- BALANCE_FREQUENCY is frequently updated on average ~0.87\n- PURCHASES average is 1003\n- ONEOFF_PURCHASES average is 592\n- Average PURCHASES_FREQUENCY is around 0.5\n- Average ONEOFF_PURCHASES_FREQUENCY, PURCHASES_INSTALLMENTS_FREQUENCY, and CASH_ADVANCE_FREQUENCY are generally low\n- Average CREDIT_LIMIT ~ 4494\n- PRC_FULL_PAYMENT is 15%\n- Average TENURE is around 12 years\n\nStandard Deviation for 25% of the values is nearly 0 and for 75% of the values is towards higher side, which means that Dataset is right skewed.\n    <\/font>","23e565f2":"<a id=tsne><\/a>\n### <font color=crimson>T-SNE","2ae87cd7":"<a id=differences><\/a>\n### <font color=crimson> Anomalous clients vs Un-anomalous clients -Differences","500ae446":"<a id=heatmap><\/a>\n### <font color=crimson> Visualization - Heatmap","2a69d602":"\n <font color=\"crimson\"> <i>Convert the columnn names into Lower-case\n    ","0741737b":"<font color=crimson> Now there are no missing values left in the entire dataset","2750256b":"<a id=objectives><\/a>\n### <font color=crimson>Objectives\n\n- A) [Credit Card Customer-Segmentation](#gmm)\n- B) [Discover unusual or anomalous customers](#differences)\n- C) [Perform more extensive credit-card related analysis with such data](#relation)\n","571c8143":"![d-Central-bank-prime.jpg](attachment:d-Central-bank-prime.jpg)","4d7ebbfb":"Feature-wise Analysis can be tabulated as hereunder:\n\n| Feature | Type- Unanamolous | Type- Anamolous |\n| --- | --- | --- |\n|balance|Slightly More|Less|\n|balance_frequency|More|Less|\n|purchases|More|Less|\n|oneoff_purchases|More|Less|\n|installments_purchases|More|Less|\n|cash_advance|More|Less|\n|oneoff_purchases_frequency|More|Less|\n|purchases_installments_frequency|More for Lower Freq||\n|cash_advance_frequency|Less|More|\n|payments|More|Less|\n|minimum_payments|More|Less|\n|tenure|More|Less|\n","1f9bba53":" <font color=\"crimson\"> <i> We can see from the results that all features are numeric except for CUST_ID. Since the 'Customer Id' has no role to play in our future training, we simply delete the feature cust_id.","ee5cbd2f":"<a id=interpret><\/a>\n### <font color=crimson> Interpretation\n    - Frequency Related Variables\n        - balance_frequency: Balance is not updated frequently\n        - purchases_frequency & purchases_installments_frequency: Both are closely related to each other\n        - oneoff_purchases_frequency:is closely related to purchases\n        - cash_advance_frequency:is closely related to cash_advance_trx\n        \n    - Other Features\n        - cash_advance_trx: No. of Txn made with \"Cash in Advance\" is nearly equal to Frequeny of cash in advance being paid\n        - purchases_trx, purchases, oneoff_purchases_frequency: Have some relation among them. \n        - balance has a higher level of correlation with cash_advance, cash_advance_frequency and credit_limit.\n        - payments variable has a high correletion with purchases and oneoff_purchases.\n        - tenure has a negative correlation with cash_advance_trx and cash_advance_frequency variables.\n         ","3bd344f7":"***","0d97708c":"### <font color=crimson>Contents\n\n* [Brief Introduction](#intro)\n* [Objectives](#objectives)\n* [Libraries to be used](#libraries)\n* [Dataset Description and Usage](#datasetdscr)\n* [Field Description and Findings](#fielddscr)\n* [Data Cleaning](#dataclean)\n* [Missing Values Analysis](#missingval)\n* [Missing Values Visualization](#missingvalviz)\n* [Filling Missing Values](#fillmissingval)\n* [Feature standardization & observation-normalization](#standardization)\n* [Visualization - Feature standardization & observation-normalization](#viz_standardization)\n    * [Visualization-Distribution Plots](#distplt)\n    * [Visualization- Box Plots](#boxplt)\n    * [Visualization- Heatmap](#heatmap)\n    * [Interpretation](#interpret)\n* [GMM Clustering](#gmm)\n* [T-SNE](#tsne)\n* [Anomalous clients vs Un-anomalous clients -Differences](#differences)\n* [Relationships between anomalous & unanomalous variables\n](#relation)\n* [Conclusion\n](#conclusion)","900e99b6":"<a id=distplt><\/a>\n### <font color=crimson> Visualization - Distribution Plots","aec6b04e":"<a id=missingval><\/a>\n### <font color=\"crimson\"> Missing Values Analysis\n- Let's first define a Function called 'missing_columns_data'\n- Objective of the function is to filter out only those features which have Number of missing values and corresponding           missing percentange    ","51936c09":"<a id=datasetdscr><\/a>\n### <font color=crimson>Dataset Description and Usage","69731560":"<a id=libraries><\/a>\n### <font color=crimson> Libraries to be used","ab0e6081":"The case requires to develop a customer segmentation to define marketing strategy. The sample Dataset summarizes the usage behavior of around 9000 active credit card holders during the last 6 months. The file is at a customer level with 18 behavioral variables. The dataset consists of 18 features about the behaviour of credit card customers. These include variables such as the balance currently on the card, the number of purchases that have been made on the account, the credit limit, and many others\n\n##### Usage\n- i) Read Dataset into a DataFrame. Let us call this DataFrame as, 'cc'\n- ii) Change column names of 'cc' to lowercase unless you are comfortable otherwise.\n- iii) Drop cust_id column as it is of no help in aggregate analysis.\n- iv) Missing values filling:\n    - a) Check how many columns have missing values. We will fill them.\n    - b) Two columns have some missing values. Findout which ones. \n- v) Plot distribution plots of the these two columns having missing data using sns.distplot(). After looking at the                distribution plots of the two features, take a decision whether to fill NaN with column mean or column median.\n- vi) Feature standardization & observation-normalization: First, standardise all features using sklearn's StandardScaler(). This is column-wise standardization. Also row-wise normalize each observation using sklearn's normalize() function. Use default parameter values for both.\n- vii) Final output ie output of normalize() is a numpy array ('out' in the above code).Transform 'out' to a pandas DataFrame and assign column names as in our DataFrame, cc. Let us call this dataframe as: df_out\n- viii) Graphing: All columns in this dataframe (df_out) have proper names assigned (para vii above). The dataset offers a rich opportunity to draw graphs and understand interaction of features. Especially, use distplots, jointplots, violin plots or boxplots for (and between) various features. In fact, draw distplots for all features (using plt.subplots() and for-loop ) and draw a few combinations of jointplots. Interpret these graphs.Interpretation is important even though it may be just one-liner.\n- ix) How many GMM clusters? Perform GMM clustering. Use 'bic' and 'aic' values to decide how many GMM clusters. 3-clusters would be OK.\n- x) Perform GMM clustering with number of clusters as three.\n- xi) TSNE--Check if your clusters do not overlap:TSNE is an important technique to check if clusters do form cohesive gropus. Perform TSNE on this data and color points in TSNE graph as per values in gm.predict(data), ie colour point depending upon which point belongs to which cluster. TSNE plot will show clearly three distinct sets of Clusters. This implies that clustering has been proper. But is it perfect? The task of interpretation of clusters does remain and interpretation will decide if clustering is perfect or not. Interpretation is the job of domain expert.\n- xii) Anomalous clients vs Un-anomalous clients--differences: Discover anomalous clients ie with density less than 4%. Result will be a numpy array (say, 'anomalies'). Discover un-anomalous clients ie those with density greater than 4%. Result will be another numpy array (say, 'unanomalous ').       Transform both anomalous numpy array and unanomalous numpy array to two pandas DataFrames with column names as in para(ii).\n- xiii) Densityplots to distinguish anomalous from normal:Plot densityplots for each feature in 'df_anomaly' and 'df_unanomaly'. For the same feature in anomalous data and in           un-anamolous data, densityplots may be drawn on the same axes so that proper comparison can be made as to changes in           density per feature. This will help in proper interpretation of differences between anomalous and un-anomalous (ie             normal) customers. \n","847395a3":"<a id=conclusion><\/a>\n### <font color=crimson>Conclusion","5b463e4d":"<a id=standardization><\/a>\n### <font color=crimson> Feature standardization & observation-normalization","00cd6df1":"<a id=dataclean><\/a>\n### <font color=crimson>  Data Cleaning","758cea04":"<font color=crimson> 'minimum_payments' has 313 missing values whereas 'credit_limit' feature has only single missing value","209eda20":"<a id=relation><\/a>\n### <font color=crimson>Relationships between anomalous & unanomalous variables","99fa6aac":"<a id=intro><\/a>\n#        <font color=crimson>      &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;          <u>Credit Card Dataset for Clustering using GMM<\/u>\n\n> ### <font color=crimson><i>Brief Introduction<\/i> ","1eb90473":"* Pandas - for dataset handling\n* Numpy - Support for Pandas and calculations \n* Matplotlib - for visualization (Plotting graphs)\n* Plotly - for interative plots\n* Sklearn - for Data Processing and Clustering\n* Os - for OS related operations","3bfc75f0":"#### <font color=\"crimson\">Let's try to find missing values with the help of some visualization","9f5d2aba":"Customer segmentation is one of the most fundamental building blocks in getting to know customers. It is essential for industries where customer interaction is frequent and varied, as each interaction provides insight into opportunities and risks for every individual. Customer segmentation can be defined as the action of grouping similar customers into categories with the objective of customizing offerings and actions based on their profiles. The output is a list of customers, each tagged with segment flags using a segmentation \u2018model\u2019. The complexity of these models can range from a simple set of business rules \u2013 such as customers who haven\u2019t used their cards last year \u2013 to sophisticated data mining codes.(Reference:https:\/\/forteconsultancy.wordpress.com\/2014\/09\/12\/credit-card-customer-segmentation\/)","5d9dd2ed":"<a id=boxplt><\/a>\n### <font color=crimson> Visualization - Box Plots","f317481b":"<a id=missingvalviz><\/a>\n### <font color=\"crimson\"> Missing Values Visualization\n- Let's draw some visualizations of the features arisen as a result of the function 'missing_columns_data()'\n- We use Seaborn distplot() function to draw the plots"}}