{"cell_type":{"3c632a6a":"code","32450bab":"code","dae822f9":"code","c06321b3":"code","d0ac3356":"code","55a02892":"code","e22b4a74":"code","e78910ff":"code","5557bde9":"code","ca9dac24":"code","6c67da1e":"code","02f1e70f":"code","3e9aa6c8":"code","7d5a8815":"code","932809df":"code","fa5ae956":"code","b80f6e27":"code","4bf1172f":"code","e542e4ce":"code","fe12eb50":"code","b8565a40":"code","36d08f04":"code","c770528e":"code","bdd68761":"code","00e2b544":"code","85997a8e":"code","09b47a38":"code","3ed49ef0":"code","3773707a":"code","39f2bba3":"code","92c9600f":"code","d4fb0107":"code","b4f5240b":"code","b1beb830":"code","46e8f699":"code","dabe1576":"code","bd95115e":"code","2bb23fc3":"code","fa1633cd":"code","636bf574":"code","93c75b49":"code","fe97f98d":"code","5e0b055e":"code","423f8cb3":"code","6ef88e1c":"code","d5d12d84":"code","92bc1419":"code","c8864264":"code","3d2e5bc9":"code","ef15c5ff":"code","78bb72cc":"code","f0879129":"code","6ea21d4f":"code","de178e8f":"code","cacba552":"code","9c2485fb":"code","bd706538":"code","9e7e7838":"code","c776ae11":"code","eb55b1d5":"code","15602b4a":"code","51fb1f9e":"code","1c9c942d":"code","f4278e1c":"code","76eee663":"code","6af5980e":"code","7c30c619":"code","d08f9390":"code","b39e6074":"code","ec8dcaca":"code","2e7d60e3":"code","a1afdbf0":"markdown","9996f545":"markdown","525a006e":"markdown","d64e9877":"markdown","3988b0c2":"markdown","cbb0955d":"markdown","c1a2519e":"markdown","2e5428d2":"markdown","2e67090b":"markdown","c7d04ff4":"markdown","e4f949b7":"markdown","02b7b4d2":"markdown","81d3f161":"markdown","a4eebad0":"markdown","d93be6b7":"markdown","32339af0":"markdown","dc30c36b":"markdown","6d2ec2c0":"markdown","07971a25":"markdown","573f1ce6":"markdown","d61e48dc":"markdown","c7b33af3":"markdown","845df40b":"markdown","9c3f7231":"markdown","b6dc242b":"markdown","9a2dae0f":"markdown","bc67c779":"markdown","0ed13fb1":"markdown","c6bc652d":"markdown","a92c1d6c":"markdown","addb5b93":"markdown","af4964fa":"markdown","ae0f280e":"markdown","94dd5b29":"markdown","737d92c7":"markdown","8d765f3e":"markdown","ee9cec19":"markdown","7b5feddf":"markdown"},"source":{"3c632a6a":"# Import necessary modules for data analysis and data visualization. \nimport pandas as pd\nimport numpy as np\n\n# Some visualization libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n## Some other snippit of codes to get the setting right \n%matplotlib inline \nimport warnings ## importing warnings library. \nwarnings.filterwarnings('ignore') ## Ignore warning","32450bab":"#Loading the data\ncar = pd.read_csv(\"..\/input\/CarPrice_Assignment.csv\")\ncar.head()","dae822f9":"#checking basic details\ncar.shape","c06321b3":"car.info()","d0ac3356":"#checking for duplicated rows\ncar[car.duplicated()]","55a02892":"#removing car_ID column as it is insignificant \ncar.drop('car_ID', axis=1, inplace=True)","e22b4a74":"#Extracting car company name from CarName column\ncar['CarName'] = car['CarName'].apply(lambda x: x.split()[0])","e78910ff":"#checking the slpit and data quality issues\ncar['CarName'].value_counts().index.sort_values()","5557bde9":"#replacing car names with correct ones\ncar['CarName'] = car['CarName'].replace('maxda','mazda')\ncar['CarName'] = car['CarName'].replace('Nissan','nissan')\ncar['CarName'] = car['CarName'].replace('porcshce','porsche')\ncar['CarName'] = car['CarName'].replace('toyouta','toyota')\ncar['CarName'] = car['CarName'].replace('vokswagen','volkswagen')\ncar['CarName'] = car['CarName'].replace('vw','volkswagen')","ca9dac24":"#checking the car names again\ncar['CarName'].value_counts()","6c67da1e":"#renaming Carname column to companyname\ncar = car.rename(columns={'CarName':'CompanyName'})","02f1e70f":"#checking distribution of price column\nsns.distplot(car['price'], bins=50)\nplt.show()","3e9aa6c8":"#creating symbol function and applying it of symboling column\ndef symbol(x):\n    if x >= -3 & x <= -1:\n        return 'No Risk'\n    elif x>=0 and x <= 1:\n        return 'Low Risk'\n    else:\n        return 'High Risk'\ncar['symboling'] = car['symboling'].apply(symbol)","7d5a8815":"car.symboling.value_counts()","932809df":"#creating fuel economy metric\ncar['fueleconomy'] = (0.55 * car['citympg']) + (0.45 * car['highwaympg'])","fa5ae956":"#removing citympg and highwaympg cols as their effect is considered in fueleconomy\ncar.drop(['citympg','highwaympg'],axis=1, inplace=True)","b80f6e27":"car.head()","4bf1172f":"#recognising categorical and numerical features\ncat_features = car.dtypes[car.dtypes == 'object'].index\nprint('No of categorical fetures:',len(cat_features),'\\n')\nprint(cat_features)\nprint('*'*100)\n\nnum_features = car.dtypes[car.dtypes != 'object'].index\nprint('No of numerical fetures:',len(num_features),'\\n')\nprint(num_features)","e542e4ce":"car[cat_features].head()","fe12eb50":"car[num_features].head()","b8565a40":"#checking stats of numerical features\ncar[num_features].describe()","36d08f04":"#checking correlation of all numerical variables\nplt.figure(figsize=(10,8),dpi=100)\nsns.heatmap(car[num_features].corr(), annot=True)\nplt.show()","c770528e":"#checking the distribution of highly correlated numerical features with price variable\ncols = ['wheelbase','carlength', 'carwidth', 'curbweight', 'enginesize','horsepower']\nplt.figure(figsize=(20,4), dpi=100)\ni = 1\nfor col in cols:\n    plt.subplot(1,6,i)\n    #sns.distplot(car['price'])\n    sns.distplot(car[col])\n    i = i+1\nplt.tight_layout()\nplt.show()","bdd68761":"num_features","00e2b544":"#visualising all the numerical features against price column\n\nnr_rows = 5\nnr_cols = 3\nfrom scipy import stats\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3),dpi=200)\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(num_features):\n            sns.regplot(car[num_features[i]], car['price'], ax = axs[r][c])\n            stp = stats.pearsonr(car[num_features[i]], car['price'])\n            str_title = \"r = \" + \"{0:.3f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.3f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","85997a8e":"#removing these columns\ncar.drop(['carheight','stroke','compressionratio','peakrpm'],axis=1, inplace=True)","09b47a38":"#checking unique value counts of categorical features\ncar[cat_features].nunique().sort_values()","3ed49ef0":"#eda on categorical columns\ncols = ['fueltype','aspiration', 'doornumber','enginelocation','drivewheel']\ni = 1\nplt.figure(figsize=(17,8),dpi=100)\nfor col in cols:\n    plt.subplot(1,len(cols),i)\n    car[col].value_counts().plot.pie(autopct='%1.0f%%', startangle=90, shadow = True,colors = sns.color_palette('Paired'))\n    i = i+1\nplt.tight_layout()\nplt.show()","3773707a":"#dropping engine location as it is highly imbalanced\ncar.drop('enginelocation',axis=1,inplace=True)","39f2bba3":"#making countplot for all below categorical variables\ncols = ['symboling','carbody', 'enginetype', 'fuelsystem']\nnr_rows = 2\nnr_cols = 2\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*6.5,nr_rows*3),dpi=100)\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(cols):\n            sns.countplot(car[cols[i]], ax = axs[r][c])\n            \nplt.tight_layout()    \nplt.show()   ","92c9600f":"#visualising carname feature\nplt.figure(figsize=(10,4),dpi=100)\nsns.countplot(car['CompanyName'])\nplt.xticks(rotation=90)\nplt.show()","d4fb0107":"li_cat_feats = list(car.dtypes[car.dtypes=='object'].index)\nnr_rows = 5\nnr_cols = 2\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*6,nr_rows*3),dpi=200)\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y='price', data=car, ax = axs[r][c])\nplt.tight_layout()    \nplt.show() ","b4f5240b":"#removing doornumber from the dataset\ncar.drop('doornumber',axis=1, inplace=True)","b1beb830":"cat_features = car.dtypes[car.dtypes == 'object'].index","46e8f699":"car[cat_features].nunique().sort_values()","dabe1576":"#creating function for targe encoding\n#credits : https:\/\/maxhalford.github.io\/blog\/target-encoding-done-the-right-way\/\ndef calc_smooth_mean(df, by, on, m):\n    # Compute the global mean\n    mean = df[on].mean()\n\n    # Compute the number of values and the mean of each group\n    agg = df.groupby(by)[on].agg(['count', 'mean'])\n    counts = agg['count']\n    means = agg['mean']\n\n    # Compute the \"smoothed\" means\n    smooth = (counts * means + m * mean) \/ (counts + m)\n\n    # Replace each value by the according smoothed mean\n    return df[by].map(smooth)","bd95115e":"#performing target encoding with weight of 100\nfor col in cat_features:\n    car[col] = calc_smooth_mean(car,by=col, on='price', m=100)","2bb23fc3":"from sklearn.model_selection import train_test_split","fa1633cd":"df_train, df_test = train_test_split(car, test_size=0.3, random_state=42)\nprint(df_train.shape)\nprint(df_test.shape)","636bf574":"cols = df_train.columns","93c75b49":"#importing minmax scaler from sklearn.preprocessing and scaling the training dataframe\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_train[cols] = scaler.fit_transform(df_train[cols])","fe97f98d":"#transforming the test data set\ndf_test[cols] = scaler.transform(df_test[cols])","5e0b055e":"#checking minmax scaling\ndf_train.describe()","423f8cb3":"#checking correlation of train dataframe \nplt.figure(figsize=(15,15),dpi=100)\nsns.heatmap(df_train.corr(), cmap='RdYlBu')\nplt.show()","6ef88e1c":"#creating function for VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif(X_train):\n    vif = pd.DataFrame()\n    vif['Features'] = X_train.columns\n    vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","d5d12d84":"#creating X and y variables\ny_train = df_train.pop('price')\nX_train = df_train","92bc1419":"print(X_train.shape)","c8864264":"#feature selection using RFE\n#In this case we are have 57 features , lets select 20 features from the data using RFE and then we will \n# remove statistical insignificant variables one by one\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train,y_train)\n\nrfe = RFE(lr,10)\nrfe.fit(X_train,y_train)\n\nprint(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))\nprint('*'*100)\ncols_rfe = X_train.columns[rfe.support_]\nprint('Features with RFE support:')\nprint(cols_rfe)\nprint('*'*100)\nprint('Features without RFE support:')\ncols_not_rfe = X_train.columns[~rfe.support_]\nprint(cols_not_rfe)","3d2e5bc9":"#taking cols with RFE support\nX_train = X_train[cols_rfe]","ef15c5ff":"#checking VIF\nvif(X_train).head(10)","78bb72cc":"#removing carlength as it is having VIF\nX_train.drop('carlength', axis=1, inplace=True)\nvif(X_train).head()","f0879129":"#removing curbweight as it is having high VIF\nX_train.drop('curbweight', axis=1, inplace=True)\nvif(X_train).head()","6ea21d4f":"#removing carwidth as it is having high VIF\nX_train.drop('carwidth', axis=1, inplace=True)\nvif(X_train).head()","de178e8f":"#removing enginesize as it is having high VIF\nX_train.drop('enginesize', axis=1, inplace=True)\nvif(X_train).head()","cacba552":"#importing statsmodel\nimport statsmodels.api as sm","9c2485fb":"#Building the first model\nX_train_lr = sm.add_constant(X_train)\nlr_1 = sm.OLS(y_train,X_train_lr).fit()\nprint(lr_1.summary())\nprint(vif(X_train))","bd706538":"#removing enginetype as it is having p-value  and building 2nd model\nX_train.drop('enginetype', axis=1, inplace=True)\nX_train_lr = sm.add_constant(X_train)\nlr_2 = sm.OLS(y_train,X_train_lr).fit()\nprint(lr_2.summary())\nprint(vif(X_train))","9e7e7838":"#removing wheelbase as it is having high VIF building 3rd model\nX_train.drop('wheelbase', axis=1, inplace=True)\nX_train_lr = sm.add_constant(X_train)\nlr_3 = sm.OLS(y_train,X_train_lr).fit()\nprint(lr_3.summary())\nprint(vif(X_train))","c776ae11":"#calculating residuals\ny_train_pred = lr_3.predict(X_train_lr)\nresiduals = y_train-y_train_pred","eb55b1d5":"#plotting residuals\nplt.figure(dpi=100)\nsns.distplot(residuals)\nplt.xlabel('Residuals')\nplt.show()","15602b4a":"#checking mean of residuals\nnp.mean(residuals)","51fb1f9e":"#scatterplot of resuduals v\/s fitted values\nplt.figure(figsize=(16,5),dpi=100)\nplt.subplot(121)\nplt.scatter(y_train_pred,residuals)\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')\n\nplt.subplot(122)\nplt.scatter(y_train,residuals)\nplt.xlabel('Training Values')\nplt.ylabel('Residuals')\nplt.show()","1c9c942d":"plt.figure(dpi=100)\nsns.regplot(y_train_pred,residuals)\nplt.xlabel('Fitted Values')\nplt.ylabel('Residuals')","f4278e1c":"#checking the test data\ndf_test.describe()","76eee663":"#creating X and y for test dataframe\ny_test = df_test.pop('price')\nX_test = df_test\nX_test.head()","6af5980e":"X_train.columns","7c30c619":"#predicting test values\nX_test = X_test[X_train.columns]\nX_test = sm.add_constant(X_test)\ny_test_pred = lr_3.predict(X_test)","d08f9390":"#scatterplot of y_test and y_test_pred\nplt.scatter(y_test_pred,y_test)","b39e6074":"#importing necessary libraries and methods\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n#calculating r2_score \nr2_score(y_test,y_test_pred)","ec8dcaca":"#calculating mean squared error for test set\nmean_squared_error(y_test,y_test_pred)","2e7d60e3":"#calculating mean squared error for traning set\nmean_squared_error(y_train,y_train_pred)","a1afdbf0":"**fuel economy** is highly negatively correlated","9996f545":"## Understanding the data","525a006e":"#Multiple Linear Regression using RFE, Statsmodels and Mean Target Encoding\n## Problem Statement\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. \n<br>\nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\n- Which variables are significant in predicting the price of a car\n- How well those variables describe the price of a car\n<br>\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market.","d64e9877":"From the above graph, we can make the following inferences:\n- **stroke, compressionratio and peakrpm** have very low correlation with price variable\n- All these variables have high p value (> 0.05), we can say that they will be insignificant in our model building process, so we will remove them","3988b0c2":"We can easily draw following points :\n - Most number of cars runs on gasoline in America\n - Standard aspiration is preferred over turbo aspiration\n - There is only 1% cars which are having engine at rear position\n - People prefer front wheel derive cars","cbb0955d":" **As we can see, this dataset does not contain any missing value**","c1a2519e":"### EDA on Categorical Variables","2e5428d2":"As we can see from the above graphs residuals does not have any pattern with fitted and training values, so we can say that our model is good.","2e67090b":"## EDA","c7d04ff4":"As we can see from the above heatmap, **wheelbase, carlength, carwidth, curbweight, enginesize & horsepower** are highly positivily correlated with `price`","e4f949b7":"### EDA on numerical variables","02b7b4d2":"Here we will scale full dataframe using MinMax scaler","81d3f161":"### Train-Test split","a4eebad0":"Now we can see our model have all the variables having VIF less than 5 and all the p-values less than 0.05, so we can say that our model is good.","d93be6b7":"## Making predictions","32339af0":"## Data Preprocessing","dc30c36b":"Some useful points:\n - Cars with Low risk rating are more in number\n - people prefer Sedan and Hatchback cars\n - 4 cylinder OHC(Over head cam) type engine is more poplular in America","6d2ec2c0":"We are not having any pattern of residuals with either fitted value or training values ","07971a25":"All these features follows nearly normal distribution","573f1ce6":"symboling : Its assigned insurance risk rating, A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.(Categorical) \t\t\n","d61e48dc":"### Encoding of categorical variables","c7b33af3":"## Model building","845df40b":"Lets start building model and we will remove other statistically insignificant variables based on p-value and vif","9c3f7231":"We will use one-hot code encoding for this assignment.","b6dc242b":"## Derived Metrics","9a2dae0f":"As we can see , residuals are normally distributed and have a mean of zero","bc67c779":"We can see that we have almost linear relationship, so we can say that our model is good.","0ed13fb1":"As we can see r2-score of test set is `0.865` against r2-score of `0.884` and adjusted-r2 score of `0.881` of training data set","c6bc652d":"## Model Evaluation","a92c1d6c":"Now we can see we have unique car names","addb5b93":"Thank You for visiting the kernel : )","af4964fa":"- Here we can see various data quality issues \n  - mazda is written as `maxda`\n  - nissan is written as `Nissan`\n  - porsche is written as `porcshce`\n  - toyota is written as `toyouta`\n  - for volkswagen we have `vokswagen` and `vw`\n\n**We will correct the names one by one**","ae0f280e":"We can draw following points from the above graphs:\n - Price of car does not depend much on Doornumber of car\n - Price of car does depend on number brand name in the car <br>\nExcept doornumber , price of car depends on every other feature","94dd5b29":"## Residual Analysis","737d92c7":"As we can see we dont have any duplicate record.","8d765f3e":"## Data Cleaning","ee9cec19":"Lets visaualize effect of categorical variables on price","7b5feddf":"### Feature scaling"}}