{"cell_type":{"60ddbd9c":"code","43c50172":"code","3855f51e":"code","d5b151e9":"code","0b03f9aa":"code","311d693e":"code","768cde8f":"code","33f570e2":"code","3dc0ca43":"code","c99284ad":"code","3e451d2b":"code","ff62b350":"code","148e9382":"code","b0324963":"markdown","63379302":"markdown","b316e445":"markdown","135fddfc":"markdown","5e6d3eb0":"markdown","99f274bb":"markdown","bba1580c":"markdown","dd074f06":"markdown","9b1f203c":"markdown","428fabd5":"markdown","33f41c4d":"markdown","736c2f01":"markdown"},"source":{"60ddbd9c":"from six.moves.urllib.request import urlretrieve\nimport os\nimport numpy as np\nimport io, gzip, requests, gc\nimport keras\nimport keras.backend as K\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.layers import Input, Flatten, Dense, Dropout, Activation\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.optimizers import SGD\nfrom keras.applications.vgg16 import VGG16\nimport cv2\nimport h5py as h5py\nfrom keras.callbacks import EarlyStopping\nfrom sklearn import manifold\nfrom time import time\nimport matplotlib.pyplot as plt\nfrom matplotlib import offsetbox\nnp.random.seed(962342)\n\ntrain_image_url = \"http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/train-images-idx3-ubyte.gz\"\ntrain_label_url = \"http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/train-labels-idx1-ubyte.gz\"\ntest_image_url = \"http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/t10k-images-idx3-ubyte.gz\"\ntest_label_url = \"http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/t10k-labels-idx1-ubyte.gz\"\n","43c50172":"def extract_labels(url, num_images):\n    print (url)\n    filepath, _ = urlretrieve(url)\n    statinfo = os.stat(filepath)\n    with gzip.open(filepath) as bytestream:\n        bytestream.read(8)\n        buf = bytestream.read(1 * num_images)\n        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n        os.remove (filepath)\n        return labels\ndef extract_data(url, num_images):\n    print (url)\n    filepath, _ = urlretrieve(url)\n    statinfo = os.stat(filepath)\n    with gzip.open(filepath) as bytestream:\n        bytestream.read(8)\n        buf = bytestream.read(28 * 28 * num_images)\n        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n        data = data.reshape(num_images, 28,28)\n        os.remove (filepath)\n    return data\n\n\ntrain_labels = extract_labels(train_label_url, 60000)\ntrain_images_raw = extract_data(train_image_url, 60000)\n\ntest_labels = extract_labels(test_label_url, 10000)\ntest_images_raw = extract_data(test_image_url, 10000)\n","3855f51e":"train_images = train_images_raw.reshape(len(train_labels), 28*28)\ntest_images = test_images_raw.reshape(len(test_labels), 28*28)\nX_train = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\nX_test = test_images.reshape(test_images.shape[0], 28, 28, 1).astype('float32')\nX_train \/= 255\nX_test \/= 255\nX_train -= 0.5\nX_test -= 0.5\nX_train *= 2.\nX_test *= 2.\nY_train = train_labels\nY_test = test_labels\nY_train2 = keras.utils.to_categorical(Y_train).astype('float32')\nY_test2 = keras.utils.to_categorical(Y_test).astype('float32')","d5b151e9":"plt.rcParams['figure.figsize']=(20, 10)\n# Scale and visualize the embedding vectors\ndef plot_embedding1(X, Image, Y, title=None):\n    x_min, x_max = np.min(X, 0), np.max(X, 0)\n    X = (X - x_min) \/ (x_max - x_min)\n    plt.figure()\n    ax = plt.subplot(111)\n    for i in range(X.shape[0]):\n        plt.text(X[i, 0], X[i, 1], str(Y[i]),\n                 color=plt.cm.Set1(Y[i] \/ 10.),\n                 fontdict={'weight': 'bold', 'size': 9})\n\n    if hasattr(offsetbox, 'AnnotationBbox'):\n        # only print thumbnails with matplotlib > 1.0\n        shown_images = np.array([[1., 1.]])  # just something big\n        for i in range(X.shape[0]):\n            dist = np.sum((X[i] - shown_images) ** 2, 1)\n            if np.min(dist) < 4e-3:\n                # don't show points that are too close\n                continue\n            shown_images = np.r_[shown_images, [X[i]]]\n            imagebox = offsetbox.AnnotationBbox(\n                offsetbox.OffsetImage(Image[i], cmap=plt.cm.gray_r),\n                X[i])\n            ax.add_artist(imagebox)\n    plt.xticks([]), plt.yticks([])\n    if title is not None:\n        plt.title(title)\n","0b03f9aa":"num_classes = len(set(Y_train))\n\nimg_input = Input(shape = X_train.shape[1:]) \n\nvgg16 = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\nvgg16 = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(vgg16)\nvgg16 = MaxPooling2D((3, 3), strides=(2, 2), name='block1_pool')(vgg16)\n\n# Block 2\nvgg16 = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(vgg16)\nvgg16 = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(vgg16)\nvgg16 = MaxPooling2D((3, 3), strides=(2, 2), name='block2_pool')(vgg16)\n\n# Block 3\nvgg16 = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(vgg16)\nvgg16 = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(vgg16)\nvgg16 = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(vgg16)\nvgg16 = MaxPooling2D((3, 3), strides=(2, 2), name='block3_pool')(vgg16)\n'''\n# Block 4\nvgg16 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(vgg16)\nvgg16 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(vgg16)\nvgg16 = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(vgg16)\nvgg16 = MaxPooling2D((3, 3), strides=(2, 2), name='block4_pool')(vgg16)\n\n# Block 5\nvgg16 = Conv2D(512, (2, 2), activation='relu', padding='same', name='block5_conv1')(vgg16)\nvgg16 = Conv2D(512, (2, 2), activation='relu', padding='same', name='block5_conv2')(vgg16)\nvgg16 = Conv2D(512, (2, 2), activation='relu', padding='valid', name='block5_conv3')(vgg16)\nvgg16 = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(vgg16)\n'''\n# Top layers\nvgg16 = Flatten(name='flatten')(vgg16)\nvgg16 = Dense(1024, activation='relu')(vgg16)\nvgg16 = Dropout(0.5)(vgg16)\nvgg16 = Dense(1024, activation='relu')(vgg16)\nvgg16 = Dropout(0.5)(vgg16)\n\nvgg16 = Dense(num_classes, activation='softmax')(vgg16)","311d693e":"model = Model(img_input, vgg16)\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = \"adam\", metrics=['accuracy'])\nmodel.summary()\n\nmodelfit = model.fit(X_train, Y_train2, validation_data=(X_test, Y_test2), batch_size=100, verbose=1, epochs=20)\n\nscore = model.evaluate(X_test, Y_test2)\nprint(score)","768cde8f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sys\nimport os\nfrom six.moves.urllib.request import urlretrieve\nfrom keras import optimizers\n\nfrom sklearn import svm\nfrom sklearn.metrics import classification_report,accuracy_score\n\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import VGG16\n\nfrom six.moves.urllib.request import urlretrieve\nimport os\nimport numpy as np\nimport io, gzip, requests, gc\nimport keras\nimport keras.backend as K\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.models import Model, Sequential\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.layers import Input, Flatten, Dense, Dropout, Activation\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.optimizers import SGD\nfrom keras.applications.vgg16 import VGG16\nimport cv2\nimport h5py as h5py\nimport keras.models\nfrom keras.callbacks import EarlyStopping\nimport keras\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport gzip\n%matplotlib inline\nfrom keras.layers import Input,Conv2D,MaxPooling2D,UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import RMSprop\nfrom keras.applications.vgg16 import preprocess_input\nimport tensorflow as tf\nimport numpy as np\nimport cv2\nfrom keras import optimizers\nfrom sklearn.model_selection import train_test_split\n\n# Create dictionary of target classes\nlabel_dict = {\n 0: 'A',\n 1: 'B',\n 2: 'C',\n 3: 'D',\n 4: 'E',\n 5: 'F',\n 6: 'G',\n 7: 'H',\n 8: 'I',\n 9: 'J',\n}\n\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\" #model will be trained on GPU 1\n   \n","33f570e2":"BASE_URL = 'http:\/\/yann.lecun.com\/exdb\/mnist\/'\n\ndef increse_size(images, num_images):\n    train_data = []\n    for img in images:\n        resized_img = cv2.resize(img, (224, 224))\n        train_data.append(resized_img)\n    train_data = np.vstack(train_data)\n    return train_data  \ndef extract_data(file_name, num_images):\n    dimention_size = 28\n    url = BASE_URL + file_name\n    print (url)\n    filepath, _ = urlretrieve(url)\n    statinfo = os.stat(filepath)\n    with gzip.open(filepath) as bytestream:\n        bytestream.read(16)\n        buf = bytestream.read(dimention_size * dimention_size * num_images)\n        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n        data = data.reshape(num_images, dimention_size, dimention_size)\n        os.remove (filepath)\n    return increse_size(data, num_images)\ndef extract_labels(file_name, num_images):\n    url = BASE_URL + file_name\n    print (url)\n    filepath, _ = urlretrieve(url)\n    statinfo = os.stat(filepath)\n    with gzip.open(filepath) as bytestream:\n        bytestream.read(8)\n        buf = bytestream.read(1 * num_images)\n        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n        os.remove (filepath)\n        return (labels, num_images)\n","3dc0ca43":"train_data = extract_data('train-images-idx3-ubyte.gz', 6000)\nprint ('Training images loaded' )\ntest_data = extract_data('t10k-images-idx3-ubyte.gz', 999)\nprint ('Testing images loaded')\n\ntrain_labels = extract_labels('train-labels-idx1-ubyte.gz',6000)\nprint ('Training labels loaded')\ntest_labels = extract_labels('t10k-labels-idx1-ubyte.gz',999)\nprint ('Testing labels loaded')\n\n# Shapes of training set\nprint(\"Training set (images) shape: {shape}\".format(shape=train_data.shape))\n# Shapes of test set\nprint(\"Test set (images) shape: {shape}\".format(shape=test_data.shape))","c99284ad":"train_data = train_data.reshape(-1, 224,224, 1)\ntest_data = test_data.reshape(-1, 224,224, 1)\ntrain_data.shape, test_data.shape","3e451d2b":"plt.figure(figsize=[5,5])\n\n# Display the first image in training data\nplt.subplot(121)\ncurr_img = np.reshape(train_data[0], (224,224))\n#curr_img = train_data[0]\ncurr_lbl = train_labels[0]\nplt.imshow(curr_img, cmap='gray')\n#plt.title(\"(Label1: \" + str(label_dict[curr_lbl]) + \")\")\n#plt.title(\"(Label2: \" + str(curr_lbl) + \")\")\n\n# Display the first image in testing data\nplt.subplot(122)\ncurr_img = np.reshape(test_data[0], (224,224))\n#curr_img = test_data[0]\ncurr_lbl = test_labels[0]\nplt.imshow(curr_img, cmap='gray')\n#plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n#plt.title(\"(Label: \" + str(curr_lbl) + \")\")\n","ff62b350":"\nbatch_size = 128\nepochs = 30\ninChannel = 1\nx, y = 224, 224\ninput_img = Input(shape = (x, y, inChannel))\n\ntrain_X,valid_X,train_ground,valid_ground = train_test_split(train_data,\n                                                             train_data, \n                                                             test_size=0.2, \n                                                             random_state=13)\n\nvgg16 = VGG16(include_top=False,input_shape = (224, 224, 3))\nvgg16.summary()\n\nmodel = Model(input_img, vgg16.layers[0](input_img))\n\nmodel.compile(optimizer=optimizers.RMSprop(lr=2e-4),\n              loss='mean_squared_error',\n              metrics=['acc'])\nautoencoder_train = model.fit(train_X, train_X, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_X))\nloss = autoencoder_train.history['loss']\nval_loss = autoencoder_train.history['val_loss']\nplt.figure()\nplt.plot(range(len(loss)), loss, 'bo', label='Training loss')\nplt.plot(range(len(val_loss)), val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","148e9382":"from keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import VGG16\n# load the model\nmodel = VGG16()\n# load an image from file\nimage = load_img('..\/input\/tiger.jpg', target_size=(224, 224))\n# convert the image pixels to a numpy array\nimage = img_to_array(image)\n# reshape data for the model\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n# prepare the image for the VGG model\nimage = preprocess_input(image)\n# predict the probability across all output classes\nyhat = model.predict(image)\n# convert the probabilities to class labels\nlabel = decode_predictions(yhat)\n# retrieve the most likely result, e.g. highest probability\nlabel = label[0][0]\n# print the classification\nprint('%s (%.2f%%)' % (label[1], label[2]*100))","b0324963":"**Load Training and testing Images abd lables**","63379302":"**VGG16 model to train from scratch with MNIST dataset**","b316e445":"**Pre trained VGG16 model **","135fddfc":"**Resize the Images for VGG16**","5e6d3eb0":"**Pre process the MNIST dataset**","99f274bb":"Build Custom VGG16 model","bba1580c":"**Load training and testing MNIST images**","dd074f06":"**Build and Train the Model VGG16**","9b1f203c":"**Run Pre trained VGG16 model**","428fabd5":"**Print first image from Traing and testing data sets**","33f41c4d":"**Train VGG16 model and evaluate accuracy**","736c2f01":"**Utility methods to load MNIST data set**"}}