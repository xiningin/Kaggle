{"cell_type":{"122aa4e6":"code","dbd13f1d":"code","27622da5":"code","c4c2136e":"code","ef99a5d6":"code","9a80a343":"code","3b95c26d":"code","5ada2d31":"code","bcc37b1d":"code","6751c0d7":"code","31554f00":"code","a481aa4d":"code","c7f056a8":"code","1a0d1fae":"code","eec3fdaa":"code","bf52ce2a":"markdown","ecd8313f":"markdown","cdfe6cd8":"markdown","ae187797":"markdown"},"source":{"122aa4e6":"import re\nimport os\nimport gc\nimport json\nimport pickle\nimport fasttext\nimport Levenshtein\nimport numpy as np \nimport pandas as pd\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm import tqdm_notebook as tqdm \nfrom Levenshtein import ratio as levenshtein_distance\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\nfrom scipy import spatial\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom plotly import tools, subplots\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.express as px\npd.set_option('max_columns', 1000)\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\nimport lightgbm as lgb\nimport plotly.figure_factory as ff\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing import text, sequence\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom tqdm import tqdm_notebook as tqdm\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Masking\nfrom tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom multiprocessing import Pool\nprint (\" Data is imported to squeeze and play\")\n","dbd13f1d":"def build_train(train_path, n_rows=200000, sampling_rate=15):\n    with open(train_path) as f:\n        processed_rows = []\n\n        for i in tqdm(range(n_rows)):\n            line = f.readline()\n            if not line:\n                break\n\n            line = json.loads(line)\n\n            text = line['document_text'].split(' ')\n            question = line['question_text']\n            annotations = line['annotations'][0]\n\n            for i, candidate in enumerate(line['long_answer_candidates']):\n                label = i == annotations['long_answer']['candidate_index']\n\n                start = candidate['start_token']\n                end = candidate['end_token']\n\n                if label or (i % sampling_rate == 0):\n                    processed_rows.append({\n                        'text': \" \".join(text[start:end]),\n                        'is_long_answer': label,\n                        'question': question,\n                        'annotation_id': annotations['annotation_id']\n                    })\n\n        train = pd.DataFrame(processed_rows)\n        \n        return train\n","27622da5":"def build_test(test_path):\n    with open(test_path) as f:\n        processed_rows = []\n\n        for line in tqdm(f):\n            line = json.loads(line)\n\n            text = line['document_text'].split(' ')\n            question = line['question_text']\n            example_id = line['example_id']\n\n            for candidate in line['long_answer_candidates']:\n                start = candidate['start_token']\n                end = candidate['end_token']\n\n                processed_rows.append({\n                    'text': \" \".join(text[start:end]),\n                    'question': question,\n                    'example_id': example_id,\n                    'sequence': f'{start}:{end}'\n\n                })\n\n        test = pd.DataFrame(processed_rows)\n    \n    return test","c4c2136e":"directory = '\/kaggle\/input\/tensorflow2-question-answering\/'","ef99a5d6":"train_path = directory + 'simplified-nq-train.jsonl'\ntest_path = directory + 'simplified-nq-test.jsonl'\n\ntrain = build_train(train_path)\ntest = build_test(test_path)","9a80a343":"train.head()","3b95c26d":"test.head()","5ada2d31":"def compute_text_and_questions(train, test, tokenizer):\n    train_text = tokenizer.texts_to_sequences(train.text.values)\n    train_questions = tokenizer.texts_to_sequences(train.question.values)\n    test_text = tokenizer.texts_to_sequences(test.text.values)\n    test_questions = tokenizer.texts_to_sequences(test.question.values)\n    \n    train_text = sequence.pad_sequences(train_text, maxlen=300)\n    train_questions = sequence.pad_sequences(train_questions)\n    test_text = sequence.pad_sequences(test_text, maxlen=300)\n    test_questions = sequence.pad_sequences(test_questions)\n    \n    return train_text, train_questions, test_text, test_questions","bcc37b1d":"tokenizer = text.Tokenizer(lower=False, num_words=80000)\n\nfor text in tqdm([train.text, test.text, train.question, test.question]):\n    tokenizer.fit_on_texts(text.values)","6751c0d7":"train_target = train.is_long_answer.astype(int).values","31554f00":"train_text, train_questions, test_text, test_questions = compute_text_and_questions(train, test, tokenizer)\ndel train","a481aa4d":"def build_embedding_matrix(tokenizer, path):\n    embedding_matrix = np.zeros((tokenizer.num_words + 1, 300))\n    ft_model = fasttext.load_model(path)\n\n    for word, i in tokenizer.word_index.items():\n        if i >= tokenizer.num_words - 1:\n            break\n        embedding_matrix[i] = ft_model.get_word_vector(word)\n    \n    return embedding_matrix","c7f056a8":"def build_model(embedding_matrix):\n    embedding = Embedding(\n        *embedding_matrix.shape, \n        weights=[embedding_matrix], \n        trainable=False, \n        mask_zero=True\n    )\n    \n    q_in = Input(shape=(None,))\n    q = embedding(q_in)\n    q = SpatialDropout1D(0.2)(q)\n    q = Bidirectional(LSTM(100, return_sequences=True))(q)\n    q = GlobalMaxPooling1D()(q)\n    \n    \n    t_in = Input(shape=(None,))\n    t = embedding(t_in)\n    t = SpatialDropout1D(0.2)(t)\n    t = Bidirectional(LSTM(150, return_sequences=True))(t)\n    t = GlobalMaxPooling1D()(t)\n    \n    hidden = concatenate([q, t])\n    hidden = Dense(300, activation='relu')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(300, activation='relu')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    \n    out1 = Dense(1, activation='sigmoid')(hidden)\n    \n    model = Model(inputs=[t_in, q_in], outputs=out1)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n\n    return model","1a0d1fae":"from pathlib import Path\nPATH = Path('\/kaggle\/input\/tensorflow2-question-answering')\n!ls -1 {PATH}\nTEST_TOTAL = 346","eec3fdaa":"def get_joined_tokens(answer: dict) -> str:\n    return '%d:%d' % (answer['start_token'], answer['end_token'])\n\ndef get_pred(json_data: dict) -> dict:\n    ret = {'short': 'YES', 'long': ''}\n    candidates = json_data['long_answer_candidates']\n    \n    paragraphs = []\n    tokens = json_data['document_text'].split(' ')\n    for cand in candidates:\n        start_token = tokens[cand['start_token']]\n        if start_token == '<P>' and cand['top_level'] and cand['end_token']-cand['start_token']>35:\n            break\n    else:\n        cand = candidates[0]\n        \n    ret['long'] = get_joined_tokens(cand)\n    \n    id_ = str(json_data['example_id'])\n    ret = {id_+'_'+k: v for k, v in ret.items()} \n    return ret\n\npreds = dict()\n\nwith open(PATH \/ 'simplified-nq-test.jsonl', 'r') as f:\n    for line in tqdm(f, total=TEST_TOTAL):\n        json_data = json.loads(line) \n        prediction = get_pred(json_data)\n        preds.update(prediction)\n            \nsubmission = pd.read_csv(PATH \/ 'sample_submission.csv')\nsubmission['PredictionString'] = submission['example_id'].map(lambda x: preds[x])\nsubmission.to_csv('submission.csv', index=False)\n","bf52ce2a":"### **Preprocessing**","ecd8313f":"# Let's Create the Model","cdfe6cd8":"**Building the Train Data Set**","ae187797":"**Building the Test Data Set**"}}