{"cell_type":{"e3661c19":"code","ef4fded0":"code","617dfb92":"code","bd67af38":"code","bef43f1e":"code","166669a9":"code","4924d65f":"code","88886644":"code","fdeb2f04":"code","3e90f2b7":"code","62342625":"code","87a1bbe2":"code","2bb5f7bb":"code","916b605d":"code","1fb6faa9":"code","fb78c347":"code","cd354fb7":"code","9ad700b9":"code","93670ae5":"code","45623815":"code","fdf8f78c":"code","b21f5d00":"code","1a5a2694":"code","d2600a4d":"code","09073bf4":"markdown","576a8571":"markdown","eb3b4122":"markdown","341537dd":"markdown","5a14fa4a":"markdown","70f8881e":"markdown","57d13d16":"markdown","3c7fc3fb":"markdown","ddb6263c":"markdown","07f7c786":"markdown","be52f47c":"markdown","18f56c4c":"markdown","9a90607c":"markdown","d0ccb3c0":"markdown","02cf14c0":"markdown","625926f8":"markdown","d6862287":"markdown"},"source":{"e3661c19":"import os\nimport matplotlib.pyplot as plt\nimport cv2\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport pydicom\nimport numpy as np\nimport shutil\nfrom PIL import Image\nimport scipy\nimport torch \nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision import models , datasets\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time\nimport copy\n\nprint(\"All modules have been imported\")","ef4fded0":"!mkdir \"data\"\n!mkdir \"data\/0\"\n!mkdir \"data\/1\"\nlabels = pd.read_csv(\"..\/input\/png-dataset-for-rsna-mgmt-detection\/png_data\/png_voxel_converted_ds\/train_labels.csv\")","617dfb92":"main_folder_path = \"..\/input\/png-dataset-for-rsna-mgmt-detection\/png_data\/png_voxel_converted_ds\"\nmain_train_folder_path = os.path.join(main_folder_path  , \"train\")\nfor subject in tqdm(os.listdir(main_train_folder_path)):\n    subject_folder = os.path.join(main_train_folder_path , subject)\n    for mri_type in os.listdir(subject_folder):\n        mri_type_folder = os.path.join(subject_folder , mri_type)\n        for mri_image in os.listdir(mri_type_folder):\n            original_image_path = os.path.join(mri_type_folder , mri_image)\n            mri_image = subject +\"_\"+ mri_type +\"_\"+ mri_image\n            subject_num = int(subject)\n            idx = np.where(labels['BraTS21ID'] == subject_num)[0][0]\n            label = str(labels.loc[idx , 'MGMT_value'])\n            new_image_folder_path =os.path.join(\"data\" , label)\n            new_image_path = os.path.join(new_image_folder_path , mri_image)\n            if (Image.open(original_image_path).getcolors()==1): continue\n            shutil.copy(original_image_path , new_image_path)","bd67af38":"print(\"Images with label 0 = \" , len(os.listdir(\"data\/0\")) , \"Images with label 1 = \" , len(os.listdir(\"data\/1\")))","bef43f1e":"!mkdir \"data\/TRAIN\"\n!mkdir \"data\/TRAIN\/1\"\n!mkdir \"data\/TRAIN\/0\"\n!mkdir \"data\/VAL\"\n!mkdir \"data\/VAL\/0\"\n!mkdir \"data\/VAL\/1\"\n!mkdir \"data\/TEST\"\n!mkdir \"data\/TEST\/0\"\n!mkdir \"data\/TEST\/1\"","166669a9":"IMG_PATH = \".\/data\"\n\n#split the data into train\/test\/val\nfor CLASS in tqdm([\"0\" , \"1\"]):\n    IMG_NUM = len(os.listdir(IMG_PATH +\"\/\"+ CLASS))\n    for (n, FILE_NAME) in enumerate(os.listdir(IMG_PATH +\"\/\"+ CLASS)):\n            img = IMG_PATH+ '\/' +  CLASS + '\/' + FILE_NAME\n            if n <4000 :\n                shutil.copy(img, 'data\/TEST\/' + str(CLASS) + '\/' + FILE_NAME)\n            elif n < 0.9*IMG_NUM:\n                shutil.copy(img, 'data\/TRAIN\/'+ str(CLASS) + '\/' + FILE_NAME)\n            else:\n                shutil.copy(img, 'data\/VAL\/'+ str(CLASS) + '\/' + FILE_NAME)","4924d65f":"print(len(os.listdir(\"data\/TRAIN\/1\")))\nprint(len(os.listdir(\"data\/TRAIN\/0\")))\nprint(len(os.listdir(\"data\/VAL\/1\")))\nprint(len(os.listdir(\"data\/VAL\/0\")))\nprint(len(os.listdir(\"data\/TEST\/1\"))) \nprint(len(os.listdir(\"data\/TEST\/0\")))","88886644":"# Data augmentation and normalization for training\n# Just normalization for validation and testing\ndata_transforms = {\n    'TRAIN': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'VAL': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'TEST': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['TRAIN', 'VAL', 'TEST']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['TRAIN', 'VAL', 'TEST']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['TRAIN', 'VAL', 'TEST']}\nclass_names = image_datasets['TRAIN'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","fdeb2f04":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['TRAIN']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])","3e90f2b7":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25, is_inception=False):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['TRAIN', 'VAL']:\n            if phase == 'TRAIN':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'TRAIN'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    if is_inception and phase == 'TRAIN':\n                        # From https:\/\/discuss.pytorch.org\/t\/how-to-optimize-inception-model-with-auxiliary-classifiers\/7958\n                        outputs, aux_outputs = model(inputs)\n                        loss1 = criterion(outputs, labels)\n                        loss2 = criterion(aux_outputs, labels)\n                        loss = loss1 + 0.4*loss2\n                    else:\n                        outputs = model(inputs)\n                        loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n                    \n                    # backward + optimize only if in training phase\n                    if phase == 'TRAIN':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'TRAIN':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'VAL' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","62342625":"#RESNET\nresnet = models.resnet18(pretrained=True)\nnum_ftrs = resnet.fc.in_features\n\nresnet.fc = nn.Linear(num_ftrs, 2)\n\nresnet = resnet.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(resnet.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(resnet, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","87a1bbe2":"#ALEXNET\nalexnet = models.alexnet(pretrained=True)\nnum_ftrs = alexnet.classifier[6].in_features\n\nalexnet.classifier[6] = nn.Linear(num_ftrs, 2)\n\nalexnet = alexnet.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(alexnet.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(alexnet, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","2bb5f7bb":"#DENSENET\ndensenet = models.densenet121(pretrained=True)\nnum_ftrs = densenet.classifier.in_features\n\ndensenet.classifier = nn.Linear(num_ftrs,2)\n\ndensenet = densenet.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(densenet.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(densenet, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","916b605d":"#VGG11\nvgg11 = models.vgg11(pretrained=True)\nnum_ftrs = vgg11.classifier[6].in_features\n\nvgg11.classifier[6] = nn.Linear(num_ftrs,2)\n\nvgg11 = vgg11.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(vgg11.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(vgg11, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","1fb6faa9":"#VGG11_bn\nvgg11_bn = models.vgg11_bn(pretrained=True)\nnum_ftrs = vgg11_bn.classifier[6].in_features\n\nvgg11_bn.classifier[6] = nn.Linear(num_ftrs,2)\n\nvgg11_bn = vgg11_bn.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(vgg11_bn.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(vgg11_bn, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","fb78c347":"#VGG13\nvgg13 = models.vgg13(pretrained=True)\nnum_ftrs = vgg13.classifier[6].in_features\n\nvgg13.classifier[6] = nn.Linear(num_ftrs,2)\n\nvgg13 = vgg13.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(vgg13.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(vgg13, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","cd354fb7":"#VGG13_bn\nvgg13_bn = models.vgg13_bn(pretrained=True)\nnum_ftrs = vgg13_bn.classifier[6].in_features\n\nvgg13_bn.classifier[6] = nn.Linear(num_ftrs,2)\n\nvgg13_bn = vgg13_bn.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(vgg13_bn.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(vgg13_bn, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","9ad700b9":"#VGG16\nvgg16 = models.vgg16(pretrained=True)\nnum_ftrs = vgg16.classifier[6].in_features\n\nvgg16.classifier[6] = nn.Linear(num_ftrs,2)\n\nvgg16 = vgg16.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(vgg16.parameters(), lr=0.1)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\nmodel = train_model(vgg16, criterion, optimizer, step_lr_scheduler, num_epochs=15, is_inception=False)","93670ae5":"#VGG16_bn\nvgg16_bn = models.vgg16_bn(pretrained=True)\nnum_ftrs = vgg16_bn.classifier[6].in_features\n\nvgg16_bn.classifier[6] = nn.Linear(num_ftrs,2)\n\nvgg16_bn = vgg16_bn.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(vgg16_bn.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(vgg16_bn, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","45623815":"#VGG19\nvgg19 = models.vgg19(pretrained=True)\nnum_ftrs = vgg19.classifier[6].in_features\n\nvgg19.classifier[6] = nn.Linear(num_ftrs,2)\n\nvgg19 = vgg19.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(vgg19.parameters(), lr=0.1)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(vgg19, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","fdf8f78c":"#VGG19_bn\nvgg19_bn = models.vgg19_bn(pretrained=True)\nnum_ftrs = vgg19_bn.classifier[6].in_features\n\nvgg19_bn.classifier[6] = nn.Linear(num_ftrs,2)\n\nvgg19_bn = vgg19_bn.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer = optim.SGD(vgg19_bn.parameters(), lr=0.001)\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n\ntrain_model(vgg19_bn, criterion, optimizer, step_lr_scheduler, num_epochs=1, is_inception=False)","b21f5d00":"samp_subm = pd.read_csv(\"..\/input\/png-dataset-for-rsna-mgmt-detection\/png_data\/png_voxel_converted_ds\/sample_submission.csv\")\nprint('Samples test:', len(samp_subm))","1a5a2694":"samp_subm.head()","d2600a4d":"samp_subm.to_csv('submission.csv', index=False)","09073bf4":"Here I have attempted to fine tune the pre-trained Resnet, Alexnet, Densenet and all versions of VGG networks on the dataset obtained from the RSNA-MICCAI Brain Tumor Radiogenic Classification Challenge.","576a8571":"## Function for Training model","eb3b4122":"## Fine tuning Resnet18","341537dd":"## Train-validation-test split","5a14fa4a":"## New sorted folder with the data","70f8881e":"## Data augmentation and normalization","57d13d16":"## Fine tuning Alexnet","3c7fc3fb":"# Overview of the competition:\nhttps:\/\/www.kaggle.com\/c\/rsna-miccai-brain-tumor-radiogenomic-classification\/overview","ddb6263c":"Here I have given an outline on how to finetune some pytorch models. Please reach out if there is any doubt or feel free to comment if anything seems incorrect. Thanks!","07f7c786":"## Importing Libraries","be52f47c":"## Splitting the train folder into subfolders\nHere I separated the subjects having the MGMT biomarker (category 1) from the ones who don't have it (category 0) within the training folder. ","18f56c4c":"## Viewing training images","9a90607c":"## Finetuning Densenet121","d0ccb3c0":"## Fine tuning VGGnet","02cf14c0":"Now we will finetune all the versions of VGG models.","625926f8":"# Task\n\nTo predict the MGMT value of each subject in the test dataset.","d6862287":"# Dataset\nThe dataset consists of 4 types of mpMRI scans: \n* Fluid Attenuated Inversion Recovery\n* T1-weighted pre-contrast (T1w)\n* T1-weighted post-contrast (T1Gd)\n* T2-weighted (T2)\n\nThe 'train' folder contains the training images. This is accompanied by 'train-labels.csv' file which file containing the target MGMT_value for each subject in the training data (e.g. the presence of MGMT promoter methylation). The 'test' folder contains the test images. \n\nFurther details on the original dataset have been provided in the following **paper**:\nU.Baid, et al., \u201cThe RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification\u201d, arXiv:2107.02314, 2021. https:\/\/arxiv.org\/abs\/2107.02314"}}