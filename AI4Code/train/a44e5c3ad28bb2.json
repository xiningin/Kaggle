{"cell_type":{"d7b17d49":"code","d89e0e83":"code","8a8a8ba9":"code","da51789c":"code","7048fabb":"code","c9ae128c":"code","bfcfa986":"code","a3784805":"code","c9bf964d":"code","4c8c2339":"code","6478571b":"code","78da8aaa":"code","a2e71f74":"code","ed6d0638":"code","040041e5":"code","8df93e40":"code","53e267f1":"code","6bc587c2":"code","39d0615d":"code","cfd7c09e":"code","846de401":"code","6e461499":"code","70a3f281":"code","c2861ce1":"code","3db17699":"code","4ac9a427":"code","438d041a":"code","28a3fe62":"code","3f0597d0":"code","b283219c":"code","11f59c27":"code","200b022c":"code","bb4bc500":"code","e2a237f8":"code","2f741f2a":"code","f0cb8018":"markdown","20bc14fe":"markdown","3c1951e3":"markdown","4cb35d1d":"markdown","ef60dd92":"markdown","1a7fd18f":"markdown","bd970fee":"markdown","afa0a985":"markdown","eb06e83f":"markdown","c59f4fde":"markdown","324e5754":"markdown","86374f2e":"markdown","52724145":"markdown","86faca0b":"markdown","0ef905f3":"markdown","c4440dfd":"markdown","6983d813":"markdown","05c3de5d":"markdown","2fb5b1f2":"markdown","9c9dbb2b":"markdown","e345a537":"markdown","ccc1b0ea":"markdown","fd1ae5a3":"markdown","a849d0b1":"markdown","399684f6":"markdown","ef3228f9":"markdown","3424dd68":"markdown","30b30fb2":"markdown","dc003d79":"markdown"},"source":{"d7b17d49":"#import necessary libaries \n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\n%matplotlib inline      \nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\n\n#get the data and give it a good name  (df for DataFrame)\nhr_df = pd.read_csv (\"..\/input\/attrition\/HR_Employee_Attrition.csv\")\nhr_df.head() #good to get a first \"feel\" about the data, shows by default the first 5 lines","d89e0e83":"\"\"\"\nfirst of all we have to twirk around a bit with the options we are seing \n\nthis is important when we will later obsere our cleaned data \n\"\"\"\n\npd.set_option(\"display.max_columns\", None)\npd.set_option(\"display.max_rows\", None)","8a8a8ba9":"#we con not .loc everything we want as want after the chance of the .loc \n\nhr_df_firstpart = hr_df.loc[:, [\"Age\", \"Attrition\", \"BusinessTravel\", \"DailyRate\", \"Department\", \"DistanceFromHome\", \"Education\", \"EducationField\"]]\n#the second line was just there for me to check if the slicing has worked properly \nhr_df_firstpart\n\nhr_df_secondpart = hr_df.loc[: , [\"RelationshipSatisfaction\"]]\nhr_df_secondpart\n\nhr_df_thirdpart = hr_df.loc [: , [\"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\", \"WorkLifeBalance\", \"YearsAtCompany\", \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\"]]\nhr_df_thirdpart\n\n\n\"\"\"\nwe bring all datasets together again\n\nNOTE: first takes all the DataFrames and than you have to pass on the axis (by default = 0 (== rows))\n    you don\u00b4t want to sort anything so you are good advised to change the default of sort to False \n\"\"\"\n\nsns.set_style(\"whitegrid\")\n#here the true concatenation takes place \nnew_df = pd.concat ([hr_df_firstpart, hr_df_secondpart, hr_df_thirdpart], axis = 1, sort = False)\nnew_df.head()\n\n\n#if ony two sets of data have to be put togther there is a simpler way. ","da51789c":"\"\"\"\nI have shown you how to delete something manually. \nBut, Python offers an easy solution \n    \n    del df_name [\"column_name\"]\n    \n    \"EmployeeNumber\", \"StandardHours\n    \n    \nNOTE: if you start form a point after the line one, you get an error running this line \n    the reason for that is that you can not delete something twice! \n    \n    \n\"\"\"\n\n#unfortunately, I have to show it to you in that way, as you can not \n#delete something that twice! (the hr_df.head() is only to check what I still have to delete)\n\n\ndel hr_df [\"EmployeeCount\"]\nhr_df.head()\n\n\n\n\n#for the other variables I show a nother version \n#note: the axis has to be passed on again \n\n#again: sorry but I can run it twice so with \"#\" that you see what I have done\n\nhr_df = hr_df.drop (\"EmployeeNumber\", axis = 1)\nhr_df.head()\n\n\n#now if you don\u00b4t want to reasign the DataFrame, set inplace = True\n\nhr_df.drop(\"StandardHours\", axis = 1, inplace = True)\n\nhr_df.head()\n\n\n","7048fabb":"#inspect the data, to see if there are NaN or 0 values we have to replace\nhr_df.info()","c9ae128c":"#general idea about the dataset \n\nhr_df.shape #no brackets needed as this value is already calculated\n","bfcfa986":"\n#OVERVIEW: we see an statistical overview\n\n\nround (hr_df.describe())","a3784805":"\ndummy_gender = pd.get_dummies(hr_df [\"Gender\"])\ndummy_gender.head()","c9bf964d":"#NOTE: we can always delete the default dummy, is it is perfectly correlated with the non-default dummy (for binary variables)\n\n#again writne like that, because we already changed it before! \n\nhr_df [\"Gender\"] = dummy_gender [\"Female\"]\n","4c8c2339":"\"\"\" \nthe unit8 means: we have changed the datatype sucessfully and the \ntype is no a positive whole number between 0-255\n\n\"\"\"\n\nhr_df.dtypes\n","6478571b":"#convertion of the varialbes we want to have as dummies \nhr_dummy = pd.get_dummies(hr_df, columns=[\"Attrition\", \"JobRole\", \"Gender\", \"BusinessTravel\", \"Department\", \"EducationField\",\"MaritalStatus\",\"Over18\", \"OverTime\"])\nhr_dummy.head()","78da8aaa":"\nhr_dummy.info()","a2e71f74":"\"\"\"remove the variable of interst \nfrom the data and safe it in a new dataFrame\n\n\"\"\"\n\ncol_of_interest = \"Attrition_No\"\nfirst_col= hr_dummy.pop (col_of_interest)\nfirst_col.head()","ed6d0638":"\"\"\"\nnow we insert the variable (on the first possition)\n\nand we have a look if it worked! \n\"\"\"\n#we have to write it like that because otherwise we get an error (that the attribute already exist) by running it aother time \nhr_dummy.insert(0, col_of_interest, first_col)\nhr_dummy.head()","040041e5":"hr_dummy.shape","8df93e40":"\"\"\"\nnote: you have to specify the axis to 1 because we are intrested in deleting only colums and all values under it\n\"\"\"\n\nhr_dummy = hr_dummy.drop ([\"Attrition_Yes\", \"Gender_1\", \"OverTime_Yes\"], axis = 1)\n\nhr_dummy.head()","53e267f1":"#histogram for daily rate with seaborn as sns \n\n#shows histogram and KDE for univariate distribution in one step\n\nfig =sns.distplot(a= hr_dummy[\"DailyRate\"], kde= bool, color = \"darkgreen\", norm_hist = True, axlabel= \"Daily Rate\")\nfig.set_title(\"Daily Rate Distribution\", fontsize = 20)\nfig.figure.set_size_inches (10,7)\nplt.show()\n","6bc587c2":"\"\"\"\n\n\"\"\"\nplt.ylim(0, 1)\nfigure = sns.barplot(hr_dummy[\"YearsAtCompany\"], hr_dummy [\"Attrition_No\"])\n\nfigure.figure.set_size_inches (13,7)\nplt.savefig(\"Age_Attrition.png\")\nplt.show()\n","39d0615d":"\n#we change the colors to have a bit of variety in it \n\n#hear we are using the dataFrame before we have assigned the dummy variables\nplt.ylim(0, 1)\nfig = sns.barplot(hr_dummy[\"Department_Sales\"], hr_dummy [\"Attrition_No\"], palette = [\"red\", \"purple\", \"pink\"])\nfig.figure.set_size_inches (10,7)\nplt.show()\n","cfd7c09e":"#we change the colors to have a bit of variety in it \n\n#here we have a color that is shaded of the categories \nplt.ylim(0, 1)\nfigure = sns.barplot(hr_dummy[\"Department_Sales\"], hr_dummy [\"Attrition_No\"], palette = \"Greens_d\")\nfigure.figure.set_size_inches (10,7)\nfigure.set_title (\"Attrition Difference \\nBetween Departmet Sales and other Departments\", fontsize = 20)\nplt.show()\n","846de401":"figure = sns.boxplot(hr_dummy[\"DailyRate\"], color = \"pink\") \nfigure.figure.set_size_inches (10,7)\nfigure.set_title(\"Daily Rate Distribution\", fontsize =20)\n\nplt.show()","6e461499":"#scatter plot of daily rate and age \n\n\nfigure = sns.scatterplot(hr_df[\"DailyRate\"], hr_df[\"Age\"], color = \"pink\")\n\nfigure.figure.set_size_inches (10,7)\n#interpretation: seamingly no realationship \n\n\nplt.show()","70a3f281":"\"\"\"adding a regession to the data\n\nregplot is very flexible (accepts different data types, )\n\nx_bins: makes it more readable and basically puts all data into \n50 discrete bins, symontaniously it shows the confidence internal, the regression is still theorginal data\"\"\"  \n\nplt.ylim(0, 1)\nfigure = sns.regplot(hr_dummy[\"DailyRate\"], hr_dummy[\"Attrition_No\"], x_bins = 50, color= \"gold\")\n\nfigure.figure.set_size_inches (10,7)\n#persumably we dont knot the highest value we can insert a max () funcktin\nplt.yticks ([0.4, 0.5, 0.5,0.6,0.7,0.8,0.9, max(hr_dummy[\"Attrition_No\"]) ])\n\n#though out I will demonstrate different possibilies to set the axis \n# >> this is by far my prefered option to set the ticks \nplt.xticks(np.arange (0,1800, step = 200))\n#interpretation: almost no relationship and if very low (but positive) \n\nplt.show()","c2861ce1":"#what we see is a slight positiive correlation \n#we are showing the data with less bins \n#NOTE: the regression is made with the Unbinnded data \nplt.title (\"Daily Rate and Attrition\", fontsize= 20)\n\nfigure = sns.regplot(hr_dummy[\"DailyRate\"], hr_dummy[\"Attrition_No\"], x_estimator=np.mean, x_bins = 15, color = \"darkred\", ci = 80)\nfigure.figure.set_size_inches (10,7)\nplt.xlabel(\"Daily Rate in USD\", fontsize = 15)\nplt.ylabel(\"Attrition\", fontsize = 15)\nplt.yticks ([0.5,0.6,0.7,0.8,0.9,1])\nplt.xticks([0, 200,400,600,800,1000,1200,1400,1600])\nplt.show()","3db17699":"\"\"\"just to demonstrate what happens if we are not binning at all: \n -->> we can basically only observe that the data seams not have no influence with one another (no correlation)\n \n ci = confidence interval between 0 and 100\n \n color, can be choosen from all colors in python \n     for the color code, just google: color code python and you get a list \n    if you want to have a quick solution: you can basically use \n    all comon colors with dark or light before them and get a lot of nice colors \n    \n    note: all liberies have a slightly different color code, but you can always include \n    the package in your google search :)\n \"\"\"\nplt.figure(figsize = (10,7))\nplt.title (\"Daily Rate and Age\", fontsize = 20)\n\nx= hr_df[\"DailyRate\"]\n\ny= hr_df[\"Age\"]\nregressions_graph=sns.regplot(x,y, color = \"lime\", ci= 90)\nplt.yticks(np.arange(15, 65, step = 5))\nplt.xticks(np.arange(0,1700,step=200))\nregressions_graph.figure.set_size_inches (12,8)\nplt.show()","4ac9a427":"\"\"\"\nlets explore some other data viz types \n\nthat is already a bit more intersting, we see that the big belly in the blue \ngraphs is lower than the one in the orange no graph, that could show a slight indication \nthat people who left the company morelikely a lower sallary (irgnoring how \nmany people we have in the two categories)\n\"\"\"\n\nfigure = sns.violinplot(hr_dummy[\"Attrition_No\"],hr_dummy[\"DailyRate\"], palette= [\"pink\", \"lightblue\"], hue = hr_dummy [\"Gender_0\"])\nfigure.figure.set_size_inches (10,7)\nplt.show()","438d041a":"\"\"\"The same with a bar plot and truncated axis\"\"\"\nplt.ylim(0, 1)\nbins= [np.arange(15,65,step=10)]\n\nfigure = sns.barplot(hr_dummy[\"Age\"],hr_dummy[\"Attrition_No\"], palette= [\"pink\", \"lightblue\"], hue = hr_dummy [\"Gender_0\"])\nfigure.figure.set_size_inches (17,7)\nplt.show()","28a3fe62":"\"\"\"\nlets explore some other data viz types \n\nthat is already a bit more intersting, we see that the big belly in the blue \ngraphs is lower than the one in the orange no graph, that could show a slight indication \nthat people who left the company morelikely a lower sallary (irgnoring how \nmany people we have in the two categories)\n\"\"\"\n\nfigure = sns.violinplot(hr_dummy[\"Attrition_No\"],hr_dummy[\"DailyRate\"], palette= [\"pink\", \"lightblue\"])\nfigure.figure.set_size_inches (10,7)\nplt.show()","3f0597d0":"\"\"\"\nAn extremly related (less intuiteve but more intersting for statistical evaluations)\nchart is the boxplot \nwe clearly see here: the median daily rate of a person who left (attriton: yes) is lower \nthan for a person who stayed \n\nfurther: we see that the spread of the two distributions is very similair\nso wie have people erarning a lot and a litte per day in both categories \n\"\"\"\n\nfigure = sns.boxplot(hr_dummy[\"Attrition_No\"],hr_dummy[\"DailyRate\"], palette= [\"pink\", \"lightblue\"])\nfigure.figure.set_size_inches (10,7)\nplt.show()","b283219c":"\"\"\"\nAn extremly related (less intuiteve but more intersting for statistical evaluations)\nchart is the boxplot \nwe clearly see here: the median daily rate of a person who left (attriton: yes) is lower \nthan for a person who stayed \n\nfurther: we see that the spread of the two distributions is very similair\nso wie have people erarning a lot and a litte per day in both categories \n\"\"\"\n\nfigure = sns.boxplot(hr_dummy[\"Attrition_No\"],hr_dummy[\"DailyRate\"], palette= [\"pink\", \"lightblue\"], hue =hr_dummy [\"Gender_0\"])\nfigure.figure.set_size_inches (10,7)\nplt.show()","11f59c27":"\"\"\"\nthat looks intersting, however, there is a huge catch! \nthis graph by default is basically misleading and should not be used \nthe y axis is only from 680-ish to 840-ish and ignores the values below\nI show you how to change the axis and we see the a more similar picture \nthan we have seen before ...\n\"\"\"\n\nfigure = sns.lineplot(x = hr_dummy[\"Attrition_No\"],y = hr_dummy[\"DailyRate\"], color = \"navy\")\nfigure.figure.set_size_inches (10,7)\nplt.show()","200b022c":"#we take a value (500 between the min daily rate and the python suggested one )\n\nplt.ylim(500, 840)\nfigure = sns.set_style(\"whitegrid\")\nfigure = sns.lineplot(x = hr_dummy[\"Attrition_No\"],y = hr_dummy[\"DailyRate\"])\nfigure.figure.set_size_inches (10,7)\nplt.show()","bb4bc500":"#definition of the function for the headmap \n\ndef halfheatmap (df, mirrow, title ): \n    corr = df.corr()\n    fig, ax = plt.subplots(figsize = (12,12))\n    colormap = sns.diverging_palette (220,10, as_cmap = True)\n    ax.set_title(title, fontsize = 20)\n    \n    #mirrow basically means if we want to have every correlation in one or twice (we choose once)\n    if mirrow == True:\n        #Generate Heat Map, allow annotations and place floats in map\n        sns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n      #Apply xticks\n        plt.xticks(range(len(corr.columns)), corr.columns);\n      #Apply yticks\n        plt.yticks(range(len(corr.columns)), corr.columns)\n      #show plot\n    \n    else: \n        #drop selcorrelation\n        dropself= np.zeros_like (corr)\n        dropself [np.triu_indices_from(dropself)] = True\n        colormap = sns.diverging_palette (200,10,as_cmap = True)\n        sns.heatmap(corr, cmap=colormap, annot=False, fmt=\".2g\", mask=dropself, vmax = 1, vmin=-1)\n         # Apply xticks\n        plt.xticks(range(len(corr.columns)), corr.columns, rotation = 45);\n        # Apply yticks\n        plt.yticks(range(len(corr.columns)), corr.columns)\n        \n    # show plot and save it \n    plt.savefig(\"Happiness.Correlation.Heatmap.png\")\n    plt.show()\n    \nhalfheatmap(df = hr_df, mirrow = False, title = \"Correlation Heatmap\")\n\n","e2a237f8":"#get insides to the regession line \n\n\n\"\"\"confidence interval by default 95%\n\n\n#note: that does not work ! the reason for that is: \nwe need to define the variables prior to calling them for the summary statistics \n\n\n\nwe are now including more and more varibales and see that the R^2 increases \n\"\"\"\n\nimport statsmodels.api as sm \n\nx = hr_dummy[[\"DailyRate\", \"Age\", \"DailyRate\", \"DistanceFromHome\", \"Education\", \"EnvironmentSatisfaction\", \"HourlyRate\", \"JobInvolvement\", \"JobLevel\", \"JobSatisfaction\", \"MonthlyIncome\", \"MonthlyRate\", \"NumCompaniesWorked\", \"PercentSalaryHike\", \"PerformanceRating\", \"RelationshipSatisfaction\", \"StockOptionLevel\", \"TotalWorkingYears\", \"TrainingTimesLastYear\",\"WorkLifeBalance\", \"YearsAtCompany\", \"YearsInCurrentRole\", \"YearsSinceLastPromotion\", \"YearsWithCurrManager\", \"Gender_0\", \"JobRole_Healthcare Representative\", \"JobRole_Human Resources\", \"JobRole_Laboratory Technician\", \"JobRole_Manager\", \"JobRole_Manufacturing Director\", \"JobRole_Research Director\", \"JobRole_Research Scientist\", \"JobRole_Sales Executive\", \"JobRole_Sales Representative\", \"BusinessTravel_Non-Travel\", \"BusinessTravel_Travel_Frequently\", \"BusinessTravel_Travel_Rarely\", \"Department_Human Resources\", \"Department_Sales\", \"Department_Research & Development\", \"OverTime_No\", \"Over18_Y\",\"MaritalStatus_Single\", \"MaritalStatus_Married\", \"MaritalStatus_Divorced\" ]]\ny = hr_dummy[\"Attrition_No\"]\n\n\nX = sm.add_constant(x)\nmodel = sm.OLS(y, X)\nest = model.fit()\nprint(est.summary())","2f741f2a":"\n\"\"\"\nI got an error here and therefore, had to convert my int8 and int64\n    to floats \n \n\"\"\"\nhr_dummy = hr_dummy.astype (float)\n\n\nx = np.column_stack ([hr_dummy.iloc [:,1:]])\n\ny = hr_dummy[\"Attrition_No\"]\n\nX = sm.add_constant(x, prepend = True)\nmodel = sm.OLS(y, X).fit()\n\nprint(est.summary())","f0cb8018":"-->> so going to the next one: Matpotlib and Pandas (my favorits, you will see why) ","20bc14fe":"# Analysis includes","3c1951e3":"# Employee Attrition: Cleaning, Vizualizatin, OLS","4cb35d1d":"We are now sorting the DataFrame to have the variable we are most interested in in the beginnging (attrition)\n\n- we do that by pop () and insert()\n","ef60dd92":" # Concatenate (link together) ","1a7fd18f":"The data does not has any NaN so we don\u00b4t have to replace anything ","bd970fee":"## DELETE of unessesary attributes!","afa0a985":"# Interpretation: \n\nOVERVIEW:\n- of alll the variables we had in our dataset not all seem to have an significant effect on the variable of interest (Attrtion) \n- some of the variables have (although significant) have a very small influence on Attrtion\n- some variables are influencing Attrition positive others negative \n\n- Environment Satisfaction: has a possitive influence of 0.04 on the attrtion_no variable (so lowers the probability of an employee to leave the company on the 5 % sign. level) \n\n- Job Involvement: has also a 0.0566 positive and equaly sign. influence on the attrtion_no rate. \n\n- other variables with similar direction and sigfnidicant are Job Satisfaction, Relationship Satisfaction, WorkLifeBalance, and Years Since promotion \n\n- a negative ans signfificant effect (meaning it is more likely for an employee to leave the company if he\/ she has thouse variable, with all others keppt constant) \n    - Distance from home\n    - Years at company\n    - years since last promotion\n    - travel frequently companred to no travel \n\nNot significant: are for instant the Job Level, the difference if somone has to travel rarely compared to travel often or the deparmtents employees work in, education\n","eb06e83f":"## Visual Represenation","c59f4fde":"Hiring new employees implies direct (interviewing, search,...) and indirect (team-building, informal learning, ...) cost and companies are trying to keep their employees. Therefore,  companies are looking for insides from data to undestand which employees are leaving to company to ultimately reduce the attrition rate in the future.","324e5754":"\n\nData cleaning \n\nVisualization \n- histogram \n- scatter plot with multiple variables \n- scatter plot for 2 variables\n- violin plot \n- linear regession to scatter plot \n- Correlation heatmap \n& interpretations \n\nOLS Analysis \n& interpretations ","86374f2e":"please not we can also make histgrams with pure python, but I tell you, they are ot very handy to analyze and offer less options, and also why not using the great libaries that are out there\n\n","52724145":"# DUMMY Variables ","86faca0b":"with get_dummies ()\npandas does assign each value to ether true or false \n\nif there are more than two types (e.g. Department than there will generate new variables)","0ef905f3":"Note: cupped at one as an employee can ether leave or stay but not leave more than once!","c4440dfd":"Note: we are speaking about a binary variable and therefore a classifier is much prefered (to a linear regression)","6983d813":"Why are people leaving their (consultancy) company ? For the given dataset we have identified a few main drivers companies can focus on: \n    ","05c3de5d":"The data is publically avaialbe and shows data from a consulting company. ","2fb5b1f2":"# OLS with all Xns","9c9dbb2b":"\"\"\"\nthe first and formost cool thing about pandas is that you can read from loootts\nof different data sources (e.g. excel, internet sources, csv (comma - \nsepperated values ....))\n\nthe you ether build an DataFrame (we have done that) if you have 2 dimensional \ndata or you create a series \n\"\"\"\n","e345a537":"Yes, indeed, it worked all data is now a intand obviously we have more variables as we had to split them up! \n","ccc1b0ea":"Comment: \n    \n- We see all variables an their correlation with each other variable \n- We see that most variables are only weakly correlated with one another (light color tone)\n- If there is a correlation the correlation seems to be rather positive (red colors)\n- For instance: Jobs satisfaction and Years at company are postively correlated (not suprisingly)\n\n\nNote for the creation the heatmap**: \n\n- make sure that vmin and vmax are set to -1 and +1 repsectively. \n- set the color code appropriate (I suggest two colors to see the direction of the correlation directly) \n- be aware of the limitatios of the representation and only take it as an overview \n\n","fd1ae5a3":"# Correlation Heatmap","a849d0b1":"# Barplots \n\ncompares two things with one another\n\n","399684f6":"# OLS summary with statsmodels.api as sm\nwith several explanatory variables ","ef3228f9":"\"What makes employees leave the company?\" ","3424dd68":"Observation: \n- the data is complet and seems normaly distributed (indicated by median and mean close)\n- the variable we are interest (attrition yes \/no) in is missing (as it is of the type object) \n- and what is with the gender, educationfied, department,... we have to convert them to get them into the statistc \n- just nice to know: why are we doing all the convertion? because we are speaking with an computer that does not know what \"Sales\" means, but can handle numberical data very efficent and without errors ","30b30fb2":"\nDeleting the varialbes that are of no use for the analyzis\nthey are: count, number, standardHours\n\n    \nWhy deleting redundant or unnessesary data: because we are intersted in what realy drives the attriton\n    and without any statistical analysis we can say that a value that is same for \n    ever employee does not change a thing\n    (arguably, the employee ID could actually have a effect on the attrtion\n    as it could indicate the lenght of the employee being in the company, \n    but, as we aready established above, the years in the company are given \n    seperately and we don\u00b4t know if the ID has just been given randomly)","dc003d79":"## Matplotlib.pyplot and Pandas "}}