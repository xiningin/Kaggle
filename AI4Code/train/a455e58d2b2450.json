{"cell_type":{"04e6b552":"code","01829ab9":"code","890ca9ea":"code","4b888f34":"code","81eb7611":"code","4b3c4ca7":"code","1d9fe525":"code","7d8073d1":"code","74088688":"code","fa2f302c":"code","e46132ad":"code","1085a777":"code","b2314d08":"code","4d28ef10":"code","76410ead":"code","80827ef6":"code","0b90585d":"code","8efb381f":"code","7fbe294d":"code","1f090e22":"code","bf44c233":"code","9821cf99":"code","500dad19":"code","132d2db4":"code","99684368":"code","f45873ec":"code","6a70d659":"code","f1277724":"code","02ca892e":"code","293931ff":"markdown","05f43495":"markdown","89b9ca12":"markdown","9a301f78":"markdown"},"source":{"04e6b552":"# \u0438\u043c\u043f\u043e\u0440\u0442\u0438\u0440\u0443\u0435\u043c \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438\nimport os\nimport shutil\nimport torch\nimport torchvision\nimport collections\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import ToTensor, Resize, Compose\nfrom torchvision.utils import make_grid\nfrom torch.utils.data.dataloader import DataLoader\nfrom torch.utils.data import random_split\n%matplotlib inline\n","01829ab9":"# \u0441\u043e\u0437\u0434\u0430\u0451\u043c \u043d\u043e\u0432\u044b\u0435 \u043f\u0430\u043f\u043a\u0438 \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0430 \u043d\u0443\u0436\u043d\u044b\u0445 \u0444\u0430\u0439\u043b\u043e\u0432\ndirectoryies = ['Apple', 'Carambola', 'Pomegranate', 'Pear', 'Plum', 'Tomatoes', 'Muskmelon']\nparent_dir = '\/kaggle\/working'\nfor directory in directoryies:\n    path = os.path.join(parent_dir, directory)\n    os.mkdir(path)\n    \npath = '\/kaggle\/working\/'\nprint(os.listdir(path))","890ca9ea":"# \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u043d\u0443\u0436\u043d\u044b\u0435 \u0444\u0430\u0439\u043b\u044b \u0432 \u0441\u043e\u0437\u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0438\u0438\nsources = ['..\/input\/fruit-recognition\/Apple\/Total Number of Apples', \n           '..\/input\/fruit-recognition\/Carambola', \n           '..\/input\/fruit-recognition\/Pomegranate',\n           '..\/input\/fruit-recognition\/Pear',\n           '..\/input\/fruit-recognition\/Plum',\n           '..\/input\/fruit-recognition\/Tomatoes', \n           '..\/input\/fruit-recognition\/muskmelon']\n\ndestinations = ['..\/output\/kaggle\/working\/Apple',\n                '..\/output\/kaggle\/working\/Carambola', \n                '..\/output\/kaggle\/working\/Pomegranate', \n                '..\/output\/kaggle\/working\/Pear',\n                '..\/output\/kaggle\/working\/Plum',\n                '..\/output\/kaggle\/working\/Tomatoes', \n                '..\/output\/kaggle\/working\/Muskmelon']\n\nfor i in range(len(sources)):\n    destination = shutil.copytree(sources[i], destinations[i]) ","4b888f34":"# \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0435\u0441\u044f \u0438\u0437 \u0434\u0438\u0440\u0435\u043a\u043e\u0442\u0440\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u044b\ndata_dir = '..\/output\/kaggle\/working'\nclasses = os.listdir(data_dir)\nprint(classes)","81eb7611":"# \u043f\u0440\u0438\u0432\u0435\u0434\u0451\u043c \u043a \u043e\u0434\u043d\u043e\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u0443 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0438 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0438\u0445 \u0432 \u0442\u0435\u043d\u0437\u043e\u0440\u044b\ndsize = (248, 248)\ncomposed = Compose([Resize(dsize), ToTensor()])\n\ndataset = ImageFolder(data_dir, composed)\nlen(dataset)","4b3c4ca7":"# \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u043e\u043c \u043a\u043b\u0430\u0441\u0441\u0435\nclass_count = collections.Counter(classes[label] for img, label in dataset)\nclass_count","1d9fe525":"# \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432\nlen(classes)","7d8073d1":"# \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0435\u0434\u0438\u043d\u0438\u0447\u043d\u044b\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435\nimg, label = dataset[5]\nprint(img.shape, label)\nimg","74088688":"# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u0430 \ndef show_example(img, label):\n    print('Label: ', dataset.classes[label], \"(\"+str(label)+\")\")\n    plt.imshow(img.permute(1, 2, 0))\n    \nshow_example(*dataset[5])","fa2f302c":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0443\u044e \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0438\u0437 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\ntorch.manual_seed(43)\n\nval_size = 1500\ntest_size = 3000\ntrain_size = len(dataset) - val_size - test_size\n\ntrain_ds, val_ds, test_ds = random_split(dataset, [train_size, val_size, test_size])\nlen(train_ds), len(val_ds), len(test_ds)","e46132ad":"# \u0437\u0430\u0434\u0430\u0434\u0438\u043c \u0440\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430 \u0438 \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u0438 \u043f\u043e\u043b\u0443\u0447\u0438\u0432\u0448\u0438\u0445\u0441\u044f \u0432\u044b\u0431\u043e\u0440\u043e\u043a \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e DataLoader\nbatch_size = 32\ntrain_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\nval_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\ntest_loader = DataLoader(test_ds, batch_size*2, num_workers=4, pin_memory=True)","1085a777":"# \u043e\u0442\u043e\u0431\u0440\u0430\u0437\u0438\u043c \u043e\u0434\u0438\u043d \u0431\u0430\u0442\u0447\nfor images, _ in train_loader:\n    print('images.shape:', images.shape)\n    plt.figure(figsize=(16,8))\n    plt.axis('off')\n    plt.imshow(make_grid(images, nrow=8).permute((1, 2, 0)))\n    break","b2314d08":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u043f\u0440\u043e\u0441\u0442\u0443\u044e \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438\ndef accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))","4d28ef10":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0431\u0430\u0437\u043e\u0432\u044b\u0439 \u043a\u043b\u0430\u0441\u0441 \u043c\u043e\u0434\u0435\u043b\u0438 (\u0431\u0435\u0437 \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u044b)\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  \n        loss = F.cross_entropy(out, labels) \n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    \n        loss = F.cross_entropy(out, labels)   \n        acc = accuracy(out, labels)           \n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   \n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      \n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))","76410ead":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043e\u0446\u0435\u043d\u043a\u0438\ndef evaluate(model, val_loader):\n    outputs = [model.validation_step(batch) for batch in val_loader]\n    return model.validation_epoch_end(outputs)\n\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n    history = []\n    optimizer = opt_func(model.parameters(), lr)\n    for epoch in range(epochs):\n        # Training Phase \n        for batch in train_loader:\n            loss = model.training_step(batch)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        # Validation phase\n        result = evaluate(model, val_loader)\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","80827ef6":"# \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0434\u043e\u0441\u0442\u0443\u043f\u043d\u043e\u0441\u0442\u044c GPU\ntorch.cuda.is_available()","0b90585d":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f \u0434\u043e\u0441\u0442\u0443\u043f\u0430 \u043a GPU (\u0435\u0441\u043b\u0438 \u043d\u0435 \u0434\u043e\u0441\u0442\u0443\u043f\u0435\u043d, \u0442\u043e \u043a CPU)\ndef get_default_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n\ndevice = get_default_device()\ndevice","8efb381f":"# \u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0434\u043b\u044f \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 GPU\ndef to_device(data, device):\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        return len(self.dl)","7fbe294d":"# \u0444\u0443\u043d\u043a\u0446\u0438\u0438 \u0434\u043b\u044f \u043e\u0442\u043e\u0431\u043e\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043e\u0448\u0438\u0431\u043a\u0438 \u0438 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438\ndef plot_losses(history):\n    losses = [x['val_loss'] for x in history]\n    plt.plot(losses, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title('Loss vs. No. of epochs')\n\ndef plot_accuracies(history):\n    accuracies = [x['val_acc'] for x in history]\n    plt.plot(accuracies, '-x')\n    plt.xlabel('epoch')\n    plt.ylabel('accuracy')\n    plt.title('Accuracy vs. No. of epochs')","1f090e22":"# \u0437\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 GPU\ntrain_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)\ntest_loader = DeviceDataLoader(test_loader, device)","bf44c233":"# \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u0432\u0445\u043e\u0434\u0430 (\u0440\u0430\u0437\u043c\u0435\u0440 \u0442\u0435\u043d\u0437\u043e\u0440\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f) \u0438 \u0432\u044b\u0445\u043e\u0434\u0430 (\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043b\u0430\u0441\u0441\u043e\u0432)\ninput_size = 3*248*248\noutput_size = 7","9821cf99":"# \u0440\u0430\u0441\u0448\u0438\u0440\u044f\u0435\u043c \u043f\u0435\u0440\u0432\u043e\u043d\u043e\u0447\u0430\u043b\u044c\u043d\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c, \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u044f \u0430\u0440\u0445\u0438\u0442\u0435\u043a\u0442\u0443\u0440\u0443 \u043d\u0435\u0439\u043e\u0440\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438\nclass FruitRecognitionModel(ImageClassificationBase):\n    def __init__(self, in_size, out_size):\n        super().__init__()\n        self.linear1 = nn.Linear(in_size, 128)\n        self.linear2 = nn.Linear(128, 32)\n        self.linear3 = nn.Linear(32, out_size)\n        \n    def forward(self, xb):\n        # Flatten images into vectors\n        out = xb.view(xb.size(0), -1)\n        # Apply layers & activation functions\n        out = self.linear1(out)\n        out = F.relu(out)\n        out = self.linear2(out)\n        out = F.relu(out)\n        out = self.linear3(out)\n        return out","500dad19":"# \u043f\u0435\u0440\u0435\u043d\u043e\u0441\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 GPU\nmodel = to_device(FruitRecognitionModel(input_size, out_size=output_size), device)\nto_device(model, device)","132d2db4":"# \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u0438 \u043f\u043e\u0442\u0435\u0440\u0438 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\nhistory = [evaluate(model, val_loader)]\nhistory","99684368":"%%time\n# \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c, \u043f\u043e\u0434\u043e\u0431\u0440\u0430\u0432 \u043e\u043f\u044b\u0442\u043d\u044b\u043c \u043f\u0443\u0442\u0451\u043c \u043e\u043f\u0442\u0438\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0448\u0430\u0433 \u0438 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u044d\u043f\u043e\u0445\nhistory += fit(20, 0.01, model, train_loader, val_loader)","f45873ec":"# \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u043f\u043e\u0442\u0435\u0440\u044c\nplot_losses(history)","6a70d659":"# \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438\nplot_accuracies(history)","f1277724":"# \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\nevaluate(model, test_loader)","02ca892e":"# \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c\ntorch.save(model.state_dict(), 'fruit-recognition-model.pth')","293931ff":"## Training model on GPU","05f43495":"* \u0412 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u043d\u0430 \u043f\u0440\u0438\u043c\u0435\u0440\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 Fruit Recognition \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u0446\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0441 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u043c\u0438 \u0444\u0440\u0443\u043a\u0442\u0430\u043c\u0438 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043f\u0440\u043e\u0441\u0442\u043e\u0439 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0439 \u0441\u0435\u0442\u0438. ","89b9ca12":"# Fruit recognition with simple neural network","9a301f78":"## Exploring and preparing data"}}