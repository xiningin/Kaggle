{"cell_type":{"5d26faa3":"code","3f9486b8":"code","6a736c5f":"code","72251240":"code","a986b6f0":"code","89176295":"code","4ba3d36d":"code","184f61df":"code","64e4f3c6":"code","b3ad4068":"code","7f750ea9":"code","ccc9e2d0":"code","22a56da7":"code","45839320":"code","a929c638":"code","f112bc8d":"code","16fb8f9a":"code","36c46ba3":"code","7760de19":"code","7ba991c5":"code","e6c0a1b8":"code","fb0ed64e":"code","891b56e6":"code","ad845905":"code","2b50bad6":"code","c4e0bea1":"code","1c304897":"code","0ba33ab5":"code","a9e84d49":"code","535a8614":"code","a1717363":"code","9a7803d6":"code","f6754764":"code","8cf9e5c1":"code","a8a277e5":"code","d9de4a1a":"code","09d72821":"code","4bcc6bd3":"code","b5d3c9be":"code","ad956924":"code","c092af34":"code","5838c4ca":"code","edf53f64":"code","0918d6d4":"code","ca577116":"code","eca5499c":"code","40f1fa3a":"code","0bdcf8a0":"code","089fbd75":"code","6f41778d":"code","e09ec577":"code","8f7c4177":"code","bbf6374d":"code","c3628806":"code","812759d1":"code","474f9eb3":"code","4609228c":"code","8c0c6418":"code","b6055dd6":"code","c2e415aa":"code","712db258":"code","1a64cc8e":"markdown","0d32a3d2":"markdown","48625202":"markdown","d0344905":"markdown","e1af8d64":"markdown","47a1c15c":"markdown","d148d760":"markdown","30e00848":"markdown","75ee6e28":"markdown","2404d613":"markdown","5041c17a":"markdown","4d6b2c46":"markdown","40280a6c":"markdown","93bb1e33":"markdown","1cdb7e9e":"markdown","2baa51d6":"markdown","236a210c":"markdown","f7b207e7":"markdown","18c218a4":"markdown","9c805e99":"markdown","f57c189d":"markdown","4d347fa9":"markdown","bad6028f":"markdown","d0d5d38d":"markdown","ce0230fb":"markdown","234fd925":"markdown","f775568f":"markdown","8f98d5f3":"markdown","c3cd9fe6":"markdown","5124e6b2":"markdown","f10b64b9":"markdown","f250184b":"markdown","0f782154":"markdown"},"source":{"5d26faa3":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures,MaxAbsScaler, MinMaxScaler, FunctionTransformer, OneHotEncoder, KBinsDiscretizer\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.base import BaseEstimator,TransformerMixin\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer \n\nfrom sklearn.model_selection import cross_val_score, train_test_split as split\n\nfrom sklearn.neighbors import KNeighborsRegressor as KNNR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomTreesEmbedding\n\nfrom sklearn.metrics import roc_curve, mean_absolute_error, make_scorer\n\nimport warnings\nwarnings.filterwarnings('ignore')","3f9486b8":"#Data preprocessin functions:\n\ndef str_to_bool(s):\n    if s=='t':\n        return True\n    else: \n        return False\n    \n\ndef str_to_rate(s):\n    if pd.isnull(s)==False:\n        return float(s.replace('%',''))\n    else: \n        return s\n\ndef extract_list_val(s):\n    for c in ['{','}','\"']:\n        s=s.replace(c,'')\n    for c in ['\/',':',' ','-','.','&',')','(','\\'']:\n        s=s.replace(c,'_')\n    s=s.replace('matress','mattress')\n    return s.split(',')\n\ndef preprocessing_w_lat_long(data):\n    data=data[data['log_price']!=0]\n    data.drop(columns=['id','last_review', 'name', 'neighbourhood','host_has_profile_pic',\n                             'host_since', 'description', 'first_review','thumbnail_url','zipcode'],inplace =True)\n    data['property_type']=data['property_type'].replace(dict2)\n    # data['property_type']=data['property_type'].apply(extract_list_val).str.join(' ')#.str.replace(' ','_')\n    data['room_type']=data['room_type'].str.replace(' ','_')\n    data['bed_type']=data['bed_type'].str.replace(' ','_')\n    data['host_response_rate'] =data['host_response_rate'].apply(str_to_rate)\n    data['host_identity_verified']=data['host_identity_verified'].apply(str_to_bool)\n    data['instant_bookable']=data['instant_bookable'].apply(str_to_bool).astype(float)\n    data['cleaning_fee']=data['cleaning_fee'].apply(str_to_bool).astype(float)\n    data['amenities']=data['amenities'].apply(extract_list_val).str.join(' ')\n    return data\n\ndef preprocessing_no_lat_long(data):\n    data=data[data['log_price']!=0]\n    data=data[data['neighbourhood'].isna()==False]\n    data.drop(columns=['id','last_review', 'name', 'city','host_has_profile_pic',\n                           'host_since', 'description', 'first_review','thumbnail_url',\n                           'zipcode','longitude','latitude'],inplace =True)\n    data['property_type']=data['property_type'].replace(dict2)\n    data['room_type']=data['room_type'].str.replace(' ','_')\n    data['bed_type']=data['bed_type'].str.replace(' ','_')\n    data['host_response_rate'] =data['host_response_rate'].apply(str_to_rate)\n    data['host_identity_verified']=data['host_identity_verified'].apply(str_to_bool)\n    data['instant_bookable']=data['instant_bookable'].apply(str_to_bool).astype(float)\n    data['cleaning_fee']=data['cleaning_fee'].apply(str_to_bool).astype(float)\n    data['amenities']=data['amenities'].apply(extract_list_val).str.join(' ')\n    data['neighbourhood']=data['neighbourhood'].str.lower().replace('castle hill ','castle hill')\n    return data\n\n\n# scoring functions\ndef MAE(y_true, y_pred):\n    return mean_absolute_error(y_true, y_pred)\n\ndef MAPE(y_true, y_pred):\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n\n#functions to get columns for featrue tranformers   \ndef get_col_to_fillna_most_frequent(df):\n    return df[col_to_fillna_most_frequent]\n\ndef get_col_to_fillna_mean(df):\n    return df[col_to_fillna_mean]\n\ndef get_col_to_get_dummies(df):\n    return df[col_to_getdummies]\n\n\ndef get_col_to_get_dummies_NYC(df):\n    return df[col_to_getdummies_NYC]\n\ndef get_lat_long(df):\n    return df[['latitude','longitude']]\n\ndef get_amenities(df):\n    return df['amenities']\n\ndef get_amenities2(df):\n    return df[['amenities']]\n\ndef get_col_no_change(df):\n    return df[col_no_change]\n\ndef get_sum_amenities(s):\n    return pd.DataFrame(s.apply(lambda s: len(s.split(' '))))\n\n#functions for model fit, predict and results\ndef time_convert (t):\n    h,m1=divmod(t, 3600)\n    m,s=divmod(m1, 60) \n    return h, m ,s\n\ndef get_mean_cv_score(name, model, X_train, y_train, num_cv, n_verbose,scoring_param):\n    print ('Model: '+name)\n    print ('Begin CV fit')\n    t0 = time.time()\n    c_val=cross_val_score(model, X_train, y_train, cv=num_cv, verbose=n_verbose, scoring=scoring_param)\n    t1 = time.time()\n    h, m ,s=time_convert(t1-t0)\n    print('CV ended. Elapsed time: {0:.0f} hours, {1:.0f} minutes and {2:.0f} seconds'.format(h,m,s))\n    return -(c_val.mean().round(4))\n\n\ndef get_results (pipes_dict, X_train, y_train,  X_test, y_test, num_cv, \n                 n_verbose, scoring_param, df_data_and_results, df_all_results):\n    for name,model in pipes_dict.items():\n        mean_cv_score=get_mean_cv_score(name ,model, X_train, y_train, num_cv, n_verbose,scoring_param)\n        print ('Begin '+name+' model fit')\n        t1 = time.time()\n        model.fit(X_train,y_train)\n        t2 = time.time()\n        h, m ,s=time_convert(t2-t1)\n        print('Model fit ended. Elapsed time: {0:.0f} hours, {1:.0f} minutes and {2:.0f} seconds'.format(h,m,s))\n        y_pred=model.predict(X_test)\n        df_data_and_results[('Price_pred_'+name)]=np.exp(y_pred)\n        df_data_and_results[('Price_diff_'+name)]=np.exp(y_test)-np.exp(y_pred)\n        df_all_results=df_all_results.append ({ 'Model':name,\n                                                'CV_train_mean_MAE_score': mean_cv_score,\n                                                'Test_MAE_score':          round(MAE(y_test,y_pred), 4),\n                                                'Min_diff':                 df_data_and_results[('Price_diff_'+name)].min(),\n                                                'Max_diff':                 df_data_and_results[('Price_diff_'+name)].max(), \n                                                'Mean_diff':                df_data_and_results[('Price_diff_'+name)].mean(), \n                                                'Median_diff':              df_data_and_results[('Price_diff_'+name)].median(),\n                                                'STD_diff':                 df_data_and_results[('Price_diff_'+name)].std(),\n                                                '10th percentile':          df_data_and_results[('Price_diff_'+name)].quantile(q=[0.1,0.9], interpolation='linear')[0.1],\n                                                '90th percentile':          df_data_and_results[('Price_diff_'+name)].quantile(q=[0.1,0.9], interpolation='linear')[0.9]\n                                              },\n                                                ignore_index = True)\n        print('======================================================================================\\n')\n    return df_data_and_results, df_all_results\n\n#Graph generation functions:\ndef get_diff_hist (pipes_dict,df_data_and_results, df_all_results):\n    for name,model in pipes_dict.items():\n        fig, ax = plt.subplots(figsize=(14,5))\n        sns.set_context(rc={\"lines.linewidth\": 3.5})#\n        sns.distplot(df_data_and_results[('Price_diff_'+name)],ax=ax,color='red')\n        plt.title(name+' Price_diff histogram')\n        ax.set_xlim(df_all_results['Min_diff'].min(),df_all_results['Max_diff'].max())\n\ndef get_prediction_cluster_graph(pipes_dict, y_test, df_data_and_results):\n    for name,model in pipes_dict.items():\n        plt.figure()\n        plt.plot(y_test,np.log(df_data_and_results[('Price_pred_'+name)]),'.', label = 'Result Data')\n        plt.plot([2,9],[2,9], label = 'Ideal')\n        plt.axes().set_aspect('equal')\n        plt.legend()\n        plt.title(name+' (y_true,y_pred) vs. ideal')\n","6a736c5f":"data = pd.read_csv('..\/input\/airbnb-price-prediction\/train.csv')","72251240":"data.head()","a986b6f0":"data.info()","89176295":"pd.set_option('display.max_colwidth', 0)\ndata[['property_type','room_type','bed_type','cancellation_policy',\n      'cleaning_fee','city','host_has_profile_pic','host_identity_verified',\n     'host_response_rate','instant_bookable','neighbourhood']].agg(['unique']).transpose()","4ba3d36d":"data['property_type'].value_counts()","184f61df":"dict1 = {'Apartment':['Condominium','Timeshare','Loft','Serviced apartment','Guest suite'],\n         'House':['Vacation home','Villa','Townhouse','In-law','Casa particular'],\n         'Hotel1':['Dorm','Hostel','Guesthouse'],\n         'Hotel2':['Boutique hotel','Bed & Breakfast'],\n         'Other':['Island','Castle','Yurt','Hut','Chalet','Treehouse',\n                  'Earth House','Tipi','Cave','Train','Parking Space','Lighthouse',\n                 'Tent','Boat','Cabin','Camper\/RV','Bungalow']\n        }\ndict2 = {i : k for k, v in dict1.items() for i in v}\ndata['property_type'].replace(dict2).value_counts()","64e4f3c6":"data['neighbourhood'].nunique()","b3ad4068":"pd.DataFrame(data.groupby(['city'])['neighbourhood'].nunique())","7f750ea9":"data_clean_all=data.copy()\ndata_clean_all=preprocessing_w_lat_long(data_clean_all)\ndata_clean_all.head()","ccc9e2d0":"data_clean_all.info()","22a56da7":"data_clean_all.shape","45839320":"#columns to fill Nan with most frequent\ncol_to_fillna_most_frequent=['beds','bedrooms','bathrooms','host_identity_verified']\n\n#columns to fill Nan with mean\ncol_to_fillna_mean=['host_response_rate','review_scores_rating']\n\n#columns for get_dummies (one hot encoding)\ncol_to_getdummies=['property_type','room_type','bed_type','cancellation_policy','city']\n\n#columns that won't be changed\ncol_no_change=['accommodates','number_of_reviews']","a929c638":"city_price_df = data_clean_all.copy()[['city','log_price']]\ncity_price_df['Price'] = np.exp(city_price_df['log_price'])\ncity_price_df[['city','Price']].boxplot(by = 'city', figsize=(10, 8), vert=False)","f112bc8d":"#scoring functions\nMAE_scorer = make_scorer(MAE, greater_is_better=False)\nMAPE_scorer = make_scorer(MAPE, greater_is_better=False)","16fb8f9a":"KNN_neighbors=200\nRF_n_estimators=50\nRF_min_samples_split=50\nTSVD_n_components=10\nnum_cv=5\nn_verbose = 3\nscoring_param=MAE_scorer\n\nPCA_features=PCA()\nTruncatedSVD_features=TruncatedSVD(n_components=TSVD_n_components)\nlin_regressor= LinearRegression()\nRF_regressor = RandomForestRegressor(n_estimators=RF_n_estimators, min_samples_split=RF_min_samples_split)\nKNN_Reg = KNNR(n_neighbors=KNN_neighbors)","36c46ba3":"class MyTransformer(TransformerMixin, BaseEstimator):\n    '''A template for a custom transformer.'''\n\n    def __init__(self, model):\n        self.model=model\n        pass\n\n    def fit(self, X, y=None):\n        self.model.fit(X, np.exp(y))\n        return self\n\n    def transform(self, X):\n        # transform X via code or additional methods\n        return pd.DataFrame(self.model.predict(X))","7760de19":"Transformer_fillna_most_frequent = Pipeline([('Select_col_to_fillna_most_frequent', FunctionTransformer(func=get_col_to_fillna_most_frequent, validate=False)),\n                                            ('Fill_Null',                           SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n                                            ('To_float_transformer',                FunctionTransformer(func=lambda x: x.astype(float) ,validate=False))\n                                            ])","7ba991c5":"Transformer_fillna_mean = Pipeline([('Select_col_to_fillna_mean',    FunctionTransformer(func=get_col_to_fillna_mean, validate=False)),\n                                    ('Fill_Null',                    SimpleImputer(missing_values=np.nan, strategy='mean'))\n                                  ])","e6c0a1b8":"Transformer_OneHotEncoder= Pipeline([('Select_col_to_get_dummies',      FunctionTransformer(func=get_col_to_get_dummies, validate=False)),\n                                     ('OneHotEncoder_transform',        OneHotEncoder(handle_unknown='ignore'))\n                                  ])","fb0ed64e":"Transformer_amenities = Pipeline([('Select_col_to_get_amenities',  FunctionTransformer(func=get_amenities, validate=False)),\n                                  ('CountVectorizer_transform',    CountVectorizer(min_df=0.02)),\n                                  ('Feature_extractor_TSVD',     TruncatedSVD_features)\n                                 ])","891b56e6":"\nTransformer_sum_amenities=Pipeline([('Select_col_to_get_amenities',  FunctionTransformer(func=get_amenities, validate=False)),\n                                    ('Get_sum_amenities',            FunctionTransformer(func=get_sum_amenities, validate=False)),\n                                   ])","ad845905":"Transformer_get_columns = Pipeline ([('Select_col_no_change', FunctionTransformer(func=get_col_no_change, validate=False))])\n","2b50bad6":"Transformer_lat_long = Pipeline ([('Select_col_lat_long_price', FunctionTransformer(func=get_lat_long, validate=False)),                                \n                                  ('MyTransformer', MyTransformer(KNN_Reg))    \n                                ])","c4e0bea1":"FeatureUnionTransformer = FeatureUnion([('FTfillna_frequent',   Transformer_fillna_most_frequent),\n                                        ('FTfillna_mean',       Transformer_fillna_mean),\n                                        ('FTget_OneHotEncoder', Transformer_OneHotEncoder),                  \n                                        ('FTamenities',         Transformer_amenities),\n                                        ('FT_sum_amenities',    Transformer_sum_amenities),\n                                        ('FT_lat_long',         Transformer_lat_long),\n                                        ('FT_get_columns',      Transformer_get_columns)\n                                       ])","1c304897":"#Transformer with polynomial features\nFull_Transformer_poly = Pipeline([('Feature_Engineering', FeatureUnionTransformer),\n                                  ('Polynomial_Transformer', PolynomialFeatures(degree=2, interaction_only=True)),\n                                  ('Min_Max_Transformer', MaxAbsScaler())\n                                 ])\n\n#Transformer without polynomial features\nFull_Transformer = Pipeline([('Feature_Engineering', FeatureUnionTransformer),\n                             ('Min_Max_Transformer', MaxAbsScaler())\n                            ])\n","0ba33ab5":"lin_reg_pipe= Pipeline([('Feature_transformer', Full_Transformer),\n                        ('Linear_regressor', lin_regressor)\n                       ])\n\nlin_reg_poly_pipe= Pipeline([('Feature_transformer_poly', Full_Transformer_poly),\n                             ('Linear_regressor', lin_regressor)\n                            ])\n\nRF_pipe= Pipeline([('Feature_transformer', Full_Transformer),\n                    ('RFE_regressor', RF_regressor)\n                  ])\n\nRF_poly_pipe= Pipeline([('Feature_transformer_poly', Full_Transformer_poly),\n                        ('RFE_regressor', RF_regressor)\n                       ])","a9e84d49":"X_train_all, X_test_all, y_train_all, y_test_all = split(data_clean_all.drop(axis=1, columns=['log_price']), \n                                                         data_clean_all['log_price'], \n                                                         test_size =0.3, random_state=123)\n","535a8614":"#all regressors\npipes_dict_all = {'LG':lin_reg_pipe, 'LG_poly':lin_reg_poly_pipe,'RF':RF_pipe, 'RF_poly':RF_poly_pipe}\n\n#reset DF to collect results\ndf_all_results_all=pd.DataFrame(columns=['Model','CV_train_mean_MAE_score','Test_MAE_score',\n                                          'Min_diff','Max_diff','Mean_diff','Median_diff',\n                                          'STD_diff','10th percentile','90th percentile'])\n\ndf_data_and_results_all=X_test_all.copy()\ndf_data_and_results_all['Price_true']=np.exp(y_test_all)","a1717363":"#Run models\ndf_data_and_results_all, df_all_results_all =get_results(pipes_dict_all, X_train_all, y_train_all, X_test_all, y_test_all, \n                                                         num_cv, n_verbose, scoring_param, \n                                                         df_data_and_results_all, df_all_results_all)","9a7803d6":"df_all_results_all","f6754764":"get_prediction_cluster_graph(pipes_dict_all, y_test_all, df_data_and_results_all)","8cf9e5c1":"get_diff_hist(pipes_dict_all, df_data_and_results_all, df_all_results_all)","a8a277e5":"NYC_data=data[data['city']=='NYC'].copy()\nNYC_data=preprocessing_no_lat_long(NYC_data)\nNYC_data.head()","d9de4a1a":"NYC = pd.read_csv('..\/input\/nyc-neighborhoods\/NYC_neighborhoods.csv', encoding = \"ISO-8859-1\")\nNYC['neighbourhood']=NYC['neighbourhood'].str.lower().replace('castle hill ','castle hill')\nNYC['borough']=NYC['borough'].str.lower()","09d72821":"#number of neighbourhoods in each borough\nNYC['borough'].value_counts()","4bcc6bd3":"NYC_data=NYC_data.merge(NYC, on='neighbourhood',how='left').copy()\nNYC_data.drop(columns=['neighbourhood'],inplace =True)","b5d3c9be":"#number of listings in each borough\nNYC_data['borough'].value_counts()","ad956924":"NYC_data.head()","c092af34":"NYC_data.shape","5838c4ca":"col_to_getdummies_NYC=['property_type','room_type','bed_type','cancellation_policy','borough']","edf53f64":"X_train_NYC, X_test_NYC, y_train_NYC, y_test_NYC = split(NYC_data.drop(axis=1, columns=['log_price']), \n                                                         NYC_data['log_price'], \n                                                         test_size =0.3, random_state=123)","0918d6d4":"Transformer_OneHotEncoder_NYC= Pipeline([('Select_col_to_get_dummies',      FunctionTransformer(func=get_col_to_get_dummies_NYC, validate=False)),\n                                         ('OneHotEncoder_transform',        OneHotEncoder(handle_unknown='ignore'))\n                                        ])\n","ca577116":"FeatureUnionTransformer_NYC = FeatureUnion([('FTfillna_frequent',   Transformer_fillna_most_frequent),\n                                            ('FTfillna_mean',       Transformer_fillna_mean),\n                                            ('FTget_OneHotEncoder', Transformer_OneHotEncoder_NYC),                  \n                                            ('FTamenities',         Transformer_amenities),\n                                            ('FT_sum_amenities',    Transformer_sum_amenities),\n                                            ('FT_get_columns',      Transformer_get_columns)\n                                       ])\n","eca5499c":"Full_Transformer_poly_NYC = Pipeline([('Feature_Engineering', FeatureUnionTransformer_NYC),\n                                      ('Polynomial_Transformer', PolynomialFeatures(degree=2, interaction_only=True)),\n                                      ('Min_Max_Transformer', MaxAbsScaler())\n                                     ])\n\nFull_Transformer_NYC = Pipeline([('Feature_Engineering', FeatureUnionTransformer_NYC),\n                                 ('Min_Max_Transformer', MaxAbsScaler())\n                                ])","40f1fa3a":"lin_reg_pipe_NYC= Pipeline([('Feature_transformer', Full_Transformer_NYC),\n                            ('Linear_regressor', lin_regressor)\n                           ])\n\nlin_reg_poly_pipe_NYC= Pipeline([('Feature_transformer_poly', Full_Transformer_poly_NYC),\n                                 ('Linear_regressor', lin_regressor)\n                                ])\n\nRF_pipe_NYC= Pipeline([('Feature_transformer', Full_Transformer_NYC),\n                        ('RFE_regressor', RF_regressor)\n                      ])\n\nRF_poly_pipe_NYC= Pipeline([('Feature_transformer_poly', Full_Transformer_poly_NYC),\n                            ('RFE_regressor', RF_regressor)\n                           ])","0bdcf8a0":"#all regressors\npipes_dict_NYC = {'LG':lin_reg_pipe_NYC, 'LG_poly':lin_reg_poly_pipe_NYC,'RF':RF_pipe_NYC, 'RF_poly':RF_poly_pipe_NYC}\n\n\n#reset DF to collect results\ndf_all_results_NYC=pd.DataFrame(columns=['Model','CV_train_mean_MAE_score','Test_MAE_score',\n                                          'Min_diff','Max_diff','Mean_diff','Median_diff',\n                                          'STD_diff','10th percentile','90th percentile'])\n\ndf_data_and_results_NYC=X_test_NYC.copy()\ndf_data_and_results_NYC['Price_true']=np.exp(y_test_NYC)","089fbd75":"#Run models\ndf_data_and_results_NYC, df_all_results_NYC =get_results(pipes_dict_NYC, X_train_NYC, y_train_NYC, X_test_NYC, y_test_NYC, \n                                                         num_cv, n_verbose, scoring_param, \n                                                         df_data_and_results_NYC, df_all_results_NYC)","6f41778d":"df_all_results_NYC","e09ec577":"get_prediction_cluster_graph(pipes_dict_NYC, y_test_NYC, df_data_and_results_NYC)","8f7c4177":"get_diff_hist(pipes_dict_NYC, df_data_and_results_NYC, df_all_results_NYC)","bbf6374d":"data_NYC_lat_long=data[data['city']=='NYC'].copy()\ndata_NYC_lat_long=preprocessing_w_lat_long(data_NYC_lat_long)","c3628806":"data_NYC_lat_long.info()","812759d1":"data_NYC_lat_long.shape","474f9eb3":"X_train_NYC_ll, X_test_NYC_ll, y_train_NYC_ll, y_test_NYC_ll = split(data_NYC_lat_long.drop(axis=1, columns=['log_price']), \n                                                                     data_NYC_lat_long['log_price'], \n                                                                     test_size =0.3, random_state=123)","4609228c":"pipes_dict_all_2 = {'LG':lin_reg_pipe, 'LG_poly':lin_reg_poly_pipe,'RF':RF_pipe, 'RF_poly':RF_poly_pipe}\n\n#reset DF to collect results\ndf_all_results_NYC_ll=pd.DataFrame(columns=['Model','CV_train_mean_MAE_score','Test_MAE_score',\n                                          'Min_diff','Max_diff','Mean_diff','Median_diff',\n                                          'STD_diff','10th percentile','90th percentile'])\n\ndf_data_and_results_NYC_ll=X_test_NYC_ll.copy()\ndf_data_and_results_NYC_ll['Price_true']=np.exp(y_test_NYC_ll)","8c0c6418":"#Run models\ndf_data_and_results_NYC_ll, df_all_results_NYC_ll =get_results(pipes_dict_all_2, X_train_NYC_ll, y_train_NYC_ll, X_test_NYC_ll, y_test_NYC_ll, \n                                                               num_cv, n_verbose, scoring_param, \n                                                               df_data_and_results_NYC_ll, df_all_results_NYC_ll)","b6055dd6":"df_all_results_NYC_ll","c2e415aa":"get_prediction_cluster_graph(pipes_dict_all_2, y_test_NYC_ll, df_data_and_results_NYC_ll)","712db258":"get_diff_hist(pipes_dict_all_2, df_data_and_results_NYC_ll, df_all_results_NYC_ll)","1a64cc8e":"Using Polynomial features improves the MAE score for both LG and RF models. <br>\nRF performed better than LG, and it looks like RF with polynomial features is the model we would perfer for it's MAE score on test and other measurments. However, it's looks only slightly better than RF without polynomial features and takes substantially longer to run. So, if we prefer something quick for the loss of some percision, RF without polynomial features is the choice model.","0d32a3d2":"Neighbourhood feature has 619 values and some grouping of it is needed, to not create too many features","48625202":"We create lists of features for the tranformers:","d0344905":"The conclusions for this data set are similar to the other tests - Polynomail features improve MAE score, RF is better than LG, but not much difference between the 2 RF models. Run time over percision may be more important to us, when we consider which model to use. <br>\n<br>\nThe tranformer based on lat_long_price prediction with KNN regressor proved it significantly improves all the models scores when used, instead of grooping neighbourhoods by boroughs in NYC subset of the data.","e1af8d64":"Run the Models:","47a1c15c":"### What's next?","d148d760":"In this project we take data of Airbnb listings in the US and try to predict the price of stay in that listing. <br>\nThe source of the dataset: <br>\nhttps:\/\/www.kaggle.com\/stevezhenghp\/airbnb-price-prediction#train.csv\n<br>\n<br>\nThe data includes 74411 listings and 29 columns - including log_price, what we are trying to predict. <br>\nSome columns will not be used as features, such as ID and thumbnail URL, so we are left with 26 columns to process and consider as features. <br>\nThere was no information about this data so we assume that since all the listings are in the us, the price (or log_price) that we are trying to predict is the general pricing per 1 night stay of the listing, in USD, not for specific dates\/seasons and not including additional fees, i.e. cleaning and airbnb service fees.<br>\n<br>\nThe data is for 6 cities across the US: NYC, LA, San Francisco, Washington DC, Boston and Chicago ","30e00848":"The data we have consists of floats, integers, booleans and strings (some of which should be boolean, some should be floats, some should be handled as single entities). Also, for some features we have NANs that will have to be filled using tranfromers.","75ee6e28":"![image.png](attachment:image.png)","2404d613":"## NYC subset of the data","5041c17a":"A class created for the KNN regressor for lat-long-price:","4d6b2c46":"Run all the regression models","40280a6c":"Tranformers:","93bb1e33":"We decided to use 2 regression models: Linear Regression (LR) and Random Forest regressor (RF), for predicting log_price. <br>\nIn addition, KNN regressor was used to create a price grouping feature based on lat-long info.<br>\n<br>\nLR and RF ran with 2 types of transformers, with and without polynomioal features","1cdb7e9e":"Since city is irrelevant and borogh replaced neighborhood, we need to change the columns list of OnHotEncoder","2baa51d6":"Split the data to train and test:","236a210c":"We also dropped other features that we decided not to use, since we believe they won't contribute to the model:\nid, last_review, name, host_has_profile_pic (99% of hosts had profile pics), host_since, description, first_review, thumbnail_url and zipcode","f7b207e7":"For exploration purposes we look at the price be city box plots:","18c218a4":"the boxblot shows that they are many out liers twords the higher prices in all the cities, so the prices are quite skewed. predicting log_price can help with this skewness.","9c805e99":"### Exploration and Preprocessing","f57c189d":"### NYC - boroughs and no lat_long","4d347fa9":"For NYC there's a clear seperation of neighbourhoods to boroughs. on a subset of the data for NYC, we import and externel list and merge on neighbourhood, to create a borough column, instead of neighhborhood. <br>\nWe will compare the results with the lat_long_price transformer","bad6028f":"Helpful functions:","d0d5d38d":"## Regression models","ce0230fb":"Further work can try to explore the predictions that had the largest differences compared to the true test data, both over and under predictions (expecially over, since it seems all the models had more errors of over predictions). This can help identify what was unusual in the values of their features, and to adjust the model accordingly and improve it.<br>\nIn addition, some exploration of features sginifcance can be done, to see how useful our features are for the regression models. <br>\nAlso, there are a lot of models that we can try and use, which may perform better. For the purpose of this project we focused on 2 regressors.","234fd925":"Regression model pipelines:","f775568f":"Split the data:","8f98d5f3":"For property type we have 35 property types. To reduce number of features, property type is grouped in to 5 catagories: Apartment, House, Hotel1, Hotel2 and other, to use with OneHotEncoder <br>\n(Note: encodging and PCA yealded worse results then \"manual\" grouping)","c3cd9fe6":"# Airbnb Price Predictions","5124e6b2":"Since we have lat-long information of the properties, we decided to use KNN regressor (with price, not log price, as prediction). This will be one of the transformers. So for all the data, Neighborhood will be dropped. <br>\nLater we show neighborhood grouping in NYC only.","f10b64b9":"Adjust tranformers and models for the new data structure:","f250184b":"Again we see RF models have better results than LG, though adding polynomial features did not improve the test MAE score for RF. <br>\nSome of the measurments are better for RF with polynomial features, and some are better for RF without. For perdiction we can't say that one is much better than the other, so the choice model is again RF without polynomial features, for it's speed.<br>\nWe will show next the same models performences no NYC data with the lat_long_price KNN regressor as a transformer.","0f782154":"### NYC - lat_long"}}