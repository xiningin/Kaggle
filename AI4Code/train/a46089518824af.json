{"cell_type":{"d8b22ec8":"code","b3c9e408":"code","145eb3b1":"code","af77e85e":"code","023e7ce6":"code","fc64b11d":"code","2f6a8d64":"code","7177536b":"code","c5067fb1":"code","f4840e22":"code","f5ebb3a9":"code","66ff5590":"code","41b555b3":"code","150c52fb":"code","11e0347d":"code","e46e0902":"code","b5e9335b":"code","f5ed1ee2":"code","0bd6467e":"code","15b1dc19":"code","9cd7918d":"code","b939957f":"code","0fa5355c":"code","4bc378e9":"code","c7d6b619":"code","3f03ddd2":"code","b8ce3294":"markdown","3a08ad69":"markdown","b788a807":"markdown","ca97ceda":"markdown","0f7c7d44":"markdown","23658494":"markdown","a55ae3a9":"markdown","504a5f5c":"markdown","0a2ba8e9":"markdown","1638de47":"markdown","9961e70a":"markdown","ed308b58":"markdown","41d934b8":"markdown","6d56ad8d":"markdown","e0649a4d":"markdown","1f13baf1":"markdown"},"source":{"d8b22ec8":"import pandas as pd\nimport missingno as msno","b3c9e408":"#\u00a0Some constants\n#\u00a0Otherwise, pandas will try to interpret this column as an integer \n#\u00a0(which is wrong according to the competition's guidelines).\nVISITOR_ID_COL = \"fullVisitorId\"\nDTYPES = {VISITOR_ID_COL: 'str'}\nTARGET_COL = \"transactionRevenue\"\nTRAIN_DATA_PATH = \"..\/input\/train.csv\"","145eb3b1":"train_df = pd.read_csv(TRAIN_DATA_PATH, dtype=DTYPES)","af77e85e":"train_df.sample(2).T","023e7ce6":"msno.matrix(train_df)","fc64b11d":"RAW_TARGET_COL = \"totals\"\nraw_target_s = train_df[RAW_TARGET_COL]","2f6a8d64":"for index, raw_target_row in raw_target_s.sample(30).iteritems():\n    print(eval(raw_target_row))","7177536b":"records = []\nfor index, raw_target_row in raw_target_s.iteritems():\n    parsed_target_row = eval(raw_target_row)\n    records.append(parsed_target_row)\nparsed_target_df = pd.DataFrame(records)\n#\u00a0Don't forget the visitor id!\nparsed_target_df[VISITOR_ID_COL] = train_df[VISITOR_ID_COL]","c5067fb1":"parsed_target_df.sample(3).T","f4840e22":"msno.matrix(parsed_target_df)","f5ebb3a9":"def percentage_of_missing(df, col):\n    return 100 * df[col].isnull().sum() \/ df.shape[0]\n\nmissing_target_percent = percentage_of_missing(parsed_target_df, \n                                              TARGET_COL)","66ff5590":"\"The target column contains {}% missing data!\".format(missing_target_percent.round(2))","41b555b3":"target_df = (parsed_target_df.loc[:, [TARGET_COL, VISITOR_ID_COL]]\n                            .assign(**{TARGET_COL: lambda df: df[TARGET_COL].fillna(0.0)\n                                                                            .astype(int)}))","150c52fb":"target_df.sample(5)","11e0347d":"import seaborn as sns\nimport numpy as np\nimport matplotlib.pylab as plt\n\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n#\u00a0Since most of the transactions are 0$, I will remove these when plotting\n#\u00a0the distribution.\nsns.distplot(np.log(target_df.loc[lambda df: df[TARGET_COL] >0, \n                                  TARGET_COL]), ax=ax)\nax.set_xlabel(\"Log of transaction revenue ($)\")","e46e0902":"#\u00a0The same thing as above but this time aggregated using the \n#\u00a0visitor unique id.\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n#\u00a0Since most of the transactions are 0$, I will remove these when plotting\n#\u00a0the distribution.\n\ndef _log_sum_agg(g):\n    \"\"\" Take the natural logarithm of the aggregated sum\n    (+1 to avoid -inf for a 0 sum).\n    \"\"\"\n    return np.log(g.sum() + 1)\n\ngrouped_target_a = (target_df.groupby(VISITOR_ID_COL)\n                             .agg({TARGET_COL: _log_sum_agg})\n                             .values)\nsns.distplot(grouped_target_a[grouped_target_a > np.log(1)], ax=ax)\nax.set_xlabel(\"Log of sum of transaction revenue ($)\")","b5e9335b":"DATE_COL = \"date\"\nTMS_GMT_COL = \"tms_gmt\"\n# Here, I parse the DATE_COL to extract year, month, and day information \n#\u00a0(using there positions). Then, I build the TMS_GMT column (using pandas' \n#\u00a0to_datetime function) and extract additional calendar features: \n#\u00a0day of week, week of year, and day of year. \n#\u00a0Notice that I drop DATE_COL and TMS_GMT columns since these\n#\u00a0aren't numerical columns.\ndate_df = (train_df[[DATE_COL]].assign(year=lambda df: df[DATE_COL].astype(str)\n                                                                   .str[0:4]\n                                                                   .astype(int),\n                                       month=lambda df: df[DATE_COL].astype(str)\n                                                                    .str[4:6]\n                                                                    .astype(int),\n                                       day=lambda df: df[DATE_COL].astype(str)\n                                                                  .str[6:8]\n                                                                  .astype(int))\n                               .drop(DATE_COL, axis=1)\n                               .assign(tms_gmt=lambda df: pd.to_datetime(df))\n                               .assign(dow=lambda df: df[TMS_GMT_COL].dt.dayofweek,\n                                       woy=lambda df: df[TMS_GMT_COL].dt.week,\n                                       doy=lambda df: df[TMS_GMT_COL].dt.day)\n                               .drop(TMS_GMT_COL, axis=1))","f5ed1ee2":"date_df.sample(5)","0bd6467e":"records = []\nGEO_COL = \"geoNetwork\"\nfor index, row in train_df[GEO_COL].iteritems():\n    parsed_row = eval(row)\n    records.append(parsed_row)\n\ngeo_df = pd.DataFrame(records)","15b1dc19":"geo_df.sample(2).T","9cd7918d":"GEO_COLS_TO_KEEP = [\"country\", \"continent\"]\nengineered_train_df = (geo_df.loc[:, GEO_COLS_TO_KEEP]\n                             .pipe(pd.get_dummies)\n                             .pipe(pd.merge, date_df, \n                                   left_index=True,\n                                   right_index=True)\n                             .pipe(pd.merge, \n                                   train_df[[VISITOR_ID_COL]],\n                                   left_index=True,\n                                   right_index=True))","b939957f":"engineered_train_df.sample(2).T","0fa5355c":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import KFold","4bc378e9":"#\u00a0For reproducibility\nSEED = 314\nCV = 5\n#\u00a0Resources are limited! \nN_SAMPLES = 10000\nkf = KFold(CV, random_state=SEED)\n\n\nbenchmark = Lasso(random_state=SEED)\n\ndf = engineered_train_df.sample(N_SAMPLES).drop(VISITOR_ID_COL, axis=1)\n\n#\u00a0TODO: Do some cleaning and refactoring of the CV computation. \n#\u00a0Also check the grouping step...\n# LASSO warnings are annoying. :)\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\ncv_rmse = []\nfor train_index, test_index in kf.split(df):\n    train_features_df = df.iloc[train_index, :]\n    test_features_df = df.iloc[test_index, :]\n    train_target_s = target_df.loc[train_index, TARGET_COL]\n    test_target_df = target_df.iloc[test_index, :].reset_index(drop=True)\n    benchmark.fit(train_features_df, train_target_s)\n    test_target_df.loc[:, \"predictions\"] = benchmark.predict(test_features_df)\n    grouped_df  = (test_target_df.groupby(VISITOR_ID_COL)\n                                 .agg({\"predictions\": _log_sum_agg, \n                                       TARGET_COL: _log_sum_agg})\n                                 .reset_index())\n    rmse = ((grouped_df[\"predictions\"] - grouped_df[TARGET_COL]) ** 2).mean() ** 0.5\n    cv_rmse.append(rmse)\n\ncv_rmse = np.array(cv_rmse)","c7d6b619":"cv_rmse","3f03ddd2":"\"The mean CV RMSE for the benchmark is: {}\".format(cv_rmse.mean())","b8ce3294":"That's it for now, I hope you have enjoyed this introductory notebook. \nStay tuned for more!","3a08ad69":"#\u00a0LASSO as a benchmark","b788a807":"To make things simpler, I will only keep the `country` and `continent` features \nfrom the `geoNetwork` parsed column. I will also dummify these features. Finally, I will combine\nthe various engineered features. Let's do that!","ca97ceda":"In what follows, I will assume that a missing value for `transactionRevenue`\nmeans that the transaction value is 0 (even though it could be a \"real\" missing \nvalue). Let's fill the missing values with this information.","0f7c7d44":"Now that I have prepared some features, I will train a LASSO model (i.e. a linear regression model that\ndoes features selection automatically) and compute its CV score. \nNotice that I can't use the cross_val_score from sklearn since I need to aggregate the out-of-fold \npredictions before computing the score. ","23658494":"#\u00a0Unnesting the target","a55ae3a9":"As you can see, this is a nested column (it is a dict). Moreover, the \ntarget of interest `transactionRevenue` isn't always available. \nLet's unnest this column and explore the missing values.","504a5f5c":"Alright, after loading the data and having a look at some samples, the first\nthing one needs to do is extract the target for this problem and (basic for now) \nfeatures. Let's do that!","0a2ba8e9":"Let's check the distribution of transactionRevenue.","1638de47":"Waw, it seems that the target to predict is missing a lot of times. How many times?","9961e70a":"In this notebook, I will explore the GA Customer Revenue dataset, do some quick data preprocessing, \nand train a simple model that will serve as a benchmark.\n\nLet's get started!","ed308b58":"# Basic features extraction","41d934b8":"In order to build the benchmark model, one needs some features. Let's use the following ones: \n\n*  `date`: this is the date of the transaction. I assume that it is in UTC.\n* `geoNetwork`: this is a nested column that contains information about location of the transaction. \n\nIn what follows, I will extract these features and engineer some basic ones (day of week, month, year, and so on...)","6d56ad8d":"No missing data, awesome! Or maybe one shouldn't be that enthusisat since there are a lot of \nnested columns (more about this later) ;)","e0649a4d":"Awesome! Time to do some (basic) modeling.","1f13baf1":"So, where is the target? As mentioned in the competition's directions, it is inside\nthe `totals` column. Let's have a look, shall we?"}}