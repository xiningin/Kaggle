{"cell_type":{"85487069":"code","c36a093a":"code","4da0281a":"code","204da68b":"code","c7013ade":"code","1bd13614":"code","67cefbf3":"code","ad2bc594":"code","59645574":"code","5af249af":"code","7e3da003":"code","0d7cab3e":"code","f59e509a":"code","ac5b3bb5":"code","2e94bdea":"code","bc87aef5":"code","b3657fca":"code","9a423e6a":"code","09c01315":"code","712e1b37":"code","84f9a7cc":"code","3b64bcfb":"code","7eee2a16":"code","d62fe9cf":"code","9d830097":"code","3e87912a":"code","150a6e8c":"code","f2b39b8c":"code","7e0c24df":"code","f2e9b552":"code","2041a847":"code","1838cee2":"markdown","931678de":"markdown","1e05bc82":"markdown","0bc2d61f":"markdown","170c5713":"markdown","73e752ed":"markdown","98e36e98":"markdown","43ddac47":"markdown","64a89f27":"markdown","450de13b":"markdown","ec5091fe":"markdown","3803e21d":"markdown","bfd54b31":"markdown","24348fe7":"markdown","d311baa1":"markdown","b2307c01":"markdown"},"source":{"85487069":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Importing xgboost for creating a baseine\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBRFRegressor\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n# Tensorflow\nimport tensorflow.keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","c36a093a":"df= pd.read_csv('..\/input\/car-purchase-data\/Car_Purchasing_Data.csv')\ndf","4da0281a":"# df.Country.unique()\ndf.Country.value_counts()","204da68b":"df.isnull().sum()","c7013ade":"sns.heatmap(df.corr())","1bd13614":"df.hist(figsize=(15,10), color='#32a852', bins=30);","67cefbf3":"sns.pairplot(df);","ad2bc594":"X=df.drop(columns={'Car Purchase Amount','Customer Name', 'Customer e-mail', 'Country' }, axis=1)\ny=df['Car Purchase Amount']","59645574":"print(X.columns)\n","5af249af":"# Why Needed to change y Shape\n#   y.shape\n#   (500,)\n# Now after reshape :> (500, 1)\n\ny = y.values.reshape(-1,1)\ny.shape","7e3da003":"# Normalise the data (i.e 0 to 1)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_scale = scaler.fit_transform(X)\ny_scale = scaler.fit_transform(y)\n\nprint(X_scale)","0d7cab3e":"X_scaled = pd.DataFrame(X_scale, columns = ['Gender', 'Age', 'Annual Salary', 'Credit Card Debt', 'Net Worth'])\ny_scaled =pd.DataFrame(y_scale, columns = ['Car Purchase Amount'])\nprint(X_scaled)","f59e509a":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X_scaled,y_scaled,test_size=0.35)  # Default test_size=25%","ac5b3bb5":"print('Orignial Dataset ',X.shape)\nprint('Training Dataset ',X_train.shape)\nprint('Test Dataset ',X_test.shape)","2e94bdea":"#XGBoost hyper-parameter tuning\ndef hyperParameterTuning(X_train, y_train):\n    param_tuning = {\n        'learning_rate': [0.01, 0.1],\n        'max_depth': [3, 5, 7, 10],\n        'min_child_weight': [1, 3, 5],\n        'subsample': [0.5, 0.7],\n        'colsample_bytree': [0.5, 0.7],\n        'n_estimators' : [100, 200, 500],\n        'objective': ['reg:squarederror']\n    }\n\n    xgb_model = XGBRegressor()\n\n    gsearch = GridSearchCV(estimator = xgb_model,\n                           param_grid = param_tuning,                        \n                           #scoring = 'neg_mean_absolute_error', #MAE\n                           #scoring = 'neg_mean_squared_error',  #MSE\n                           cv = 5,\n                           n_jobs = -1,\n                           verbose = 1)\n\n    gsearch.fit(X_train,y_train)\n\n    return gsearch.best_params_","bc87aef5":"# Run only once that too with GPI mine took 13.8 min to execute with GPU\n# hyperParameterTuning(X_train, y_train)","b3657fca":"# Using Best fit\n\nxgb_model = XGBRegressor(colsample_bytree= 0.7,\n                         learning_rate= 0.1,\n                         max_depth= 3,\n                         min_child_weight= 1,\n                         n_estimators= 500,\n                         objective= 'reg:squarederror',\n                         subsample= 0.5)\n\n%time xgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_set=[(X_test, y_test)], verbose=False)\n\ny_pred_xgb = xgb_model.predict(X_test)\n\nmae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n\nprint(\"MAE: \", mae_xgb)","9a423e6a":"#Plot Real vs Predict\nplt.scatter(X_test['Net Worth'] * 0.092903, y_test,          color='blue', label='Real',    alpha=0.5)\nplt.scatter(X_test['Net Worth'] * 0.092903, y_pred_xgb,  color='red' , label='Predict', alpha=0.5)\nplt.title(\"Real vs Predict\")\nplt.legend(loc='best')\nplt.show()","09c01315":"print(xgb_model.score(X_test,y_test))","712e1b37":"# initializing the model\nmodel=Sequential()\n# adding layers\nmodel.add(Dense(35, input_dim=5, activation='relu')) #1st layer: I have 5 input, Dense means fully connected,  No of neurons\/units =25 & activation function =relu \nmodel.add(Dense(35, activation='relu'))             # hidden Layer\nmodel.add(Dense(1, activation='linear'))            # Output Layer , Activation Function : linear ( because now I need to predict certain value )\n\n# printing the summary\nmodel.summary() # Gives snapshot of model","84f9a7cc":"model.compile(optimizer='adam',loss='mean_squared_error') # As i wanna minimise Mean Square Error","3b64bcfb":"# Fit thr model with our training data\n\nepochs_hist = model.fit(X_train,y_train,batch_size=35,epochs=75,validation_split=.25,verbose=1) \n\n# validation split : we take the training data and devide it again to AVOID OVERFITTING\n","7eee2a16":"print(epochs_hist.history.keys())","d62fe9cf":"\n# visualizatiom \nplt.plot(epochs_hist.history['loss'])\nplt.plot(epochs_hist.history['val_loss'])\n\nplt.title('Model Loss Progression During Training\/Validation')\nplt.ylabel('Training and Validation Losses')\nplt.xlabel('Epoch Number')\nplt.legend(['Training Loss', 'Validation Loss']);\n","9d830097":"y_pred = model.predict(X_test)","3e87912a":"# type(y_pred) > numpy.ndarray\n# type(y_pred_xgb) > numpy.ndarray\n\n# Score= model.score(X_test, y_test)\n# accuracy_score(y_test, y_pred)","150a6e8c":"np.set_printoptions(precision=2)\ny_test= scaler.inverse_transform(y_test)\ny_pred= scaler.inverse_transform(y_pred)\ndf=np.concatenate((y_pred.reshape(-1,1), y_test.reshape(-1,1)),1)","f2b39b8c":"final = pd.DataFrame(data=df, columns=[\"Predicted\", \"Actual\"])\n\n\nsns.scatterplot(data=final)\nplt.title('ANN model');\n","7e0c24df":"model.compile(loss='mean_squared_error', optimizer='adam',metrics = [\"accuracy\"])","f2e9b552":"##########  Gender, Age, Annual Salary , Credit Card Debt, Net Worth\ninput_value= np.array([[1,50,300000,10000,1000000]])\nprint(\"Expected Purchase Amount \", model.predict(input_value))","2041a847":"# Checking prediction for my XGBoost Model\nprint(\"Prediction as per XGBoost \",xgb_model.predict(input_value))","1838cee2":"# Neural Network Optimization\n\n<table>\n  <tr>\n    <th>Optimizer<\/th>\n    <th>Complexity<\/th>\n    <th>Computaion expense<\/th>\n  <\/tr>\n  <tr> <!-- 1 -->\n    <td>GD<\/td>\n    <td>Low<\/td>\n    <td>High<\/td>\n  <\/tr>\n  <tr> <!-- 2 -->\n    <td>SGD<\/td>\n    <td>Low<\/td>\n    <td>Low<\/td>\n  <\/tr>\n     <tr> <!-- 3 -->\n    <td>SGD + Momentum<\/td>\n    <td>Medium<\/td>\n    <td>Low<\/td>\n  <\/tr>\n     <tr> <!-- 4 -->\n    <td>Adagrad<\/td>\n    <td>High<\/td>\n    <td>Medium<\/td>\n  <\/tr>\n     <tr> <!-- 5 -->\n    <td>RMS Prop<\/td>\n    <td>High<\/td>\n    <td>High<\/td>\n  <\/tr>\n     <tr> <!-- 6 -->\n    <td>Adam<\/td>\n    <td>High<\/td>\n    <td>High<\/td>\n  <\/tr>\n<\/table>\n\n**Gradient Descent Optimizer**\nGradient descent is probably the most popular and widely used out of all optimizers. It is a simple and effective method to find the optimum values for the neural network. The objective of all optimizers is to reach the global minima where the cost function attains the least possible value. If you try to visualize the cost function in three-dimension it would something like the figure shown below.\n\n\n**SGD with momentum**\nIt always works better than the normal Stochastic Gradient Descent Algorithm. The problem with SGD is that while it tries to reach minima because of the high oscillation we can\u2019t increase the learning rate. So it takes time to converge. In this algorithm, we will be using Exponentially Weighted Averages to compute Gradient and used this Gradient to update parameter.\n\n**Adagrad (Adaptive Gradient Algorithm)**\nWhatever the optimizer we learned till SGD with momentum, the learning rate remains constant. In Adagrad optimizer, there is no momentum concept so, it is much simpler compared to SGD with momentum.\nThe idea behind Adagrad is to use different learning rates for each parameter base on iteration. The reason behind the need for different learning rates is that the learning rate for sparse features parameters needs to be higher compare to the dense features parameter because the frequency of occurrence of sparse features is lower.\n\n**Adadelta**\nAdadelta is an extension of Adagrad that attempts to solve its radically diminishing learning rates. The idea behind Adadelta is that instead of summing up all the past squared gradients from 1 to \u201ct\u201d time steps, what if we could restrict the window size. For example, computing the squared gradient of the past 10 gradients and average out. This can be achieved using Exponentially Weighted Averages over Gradient.\n\n**RMSprop Optimizer**\nThe RMSprop optimizer is similar to the gradient descent algorithm with momentum. The RMSprop optimizer restricts the oscillations in the vertical direction. Therefore, we can increase our learning rate and our algorithm could take larger steps in the horizontal direction converging faster. The difference between RMSprop and gradient descent is on how the gradients are calculated. The following equations show how the gradients are calculated for the RMSprop and gradient descent with momentum. The value of momentum is denoted by beta and is usually set to 0.9. If you are not interested in the math behind the optimizer, you can just skip the following equations.\n\n\n**Adam optimizer**\nAdam optimizer is by far one of the most preferred optimizers. The idea behind Adam optimizer is to utilize the momentum concept from \u201cSGD with momentum\u201d and adaptive learning rate from \u201cAda delta\u201d.\n\n[**Must Read**](https:\/\/towardsdatascience.com\/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b)  \n[Source](https:\/\/towardsdatascience.com\/deep-learning-optimizers-436171c9e23f)\n\n### Conclusion\n1. one must use Adam (as it takes care of `conversion`, `Learning decay Problem`, `smoothening of the grades` but will be hungry for RAM probably more than ur chrome :-D\n2. If want program with low complexity one may use SGD or SGD with Momentum","931678de":"* The number of neurons in the `output layer equals` the `number of outputs associated with each input`.\n\n## How to decide no of layers?\nsome guidelines to know the number of hidden layers and neurons per each hidden layer in a classification problem:\n1. Based on the data, draw an expected decision boundary to separate the classes.\n2. Express the decision boundary as a set of lines. Note that the combination of such lines must yield to the decision boundary.\n3. The `number of selected lines` represents the number of hidden neurons in the first hidden layer.\n4. To connect the lines created by the previous layer, a new hidden layer is added. **Note that a new hidden layer is added each time you need to create connections among the lines in the previous hidden layer.**\n5. The `number of hidden neurons` in each **new hidden layer equals the number of connections to be made.**\n\n## How to decide no of units\/Neuron","1e05bc82":"# How to check score of ANN model ","0bc2d61f":"### Spliting the data set","170c5713":"## Train our Model","73e752ed":"<b>Default parameters<\/b>\n<br>max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:squarederror', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain'\n\n\n*Explanation of relevant parameters for this kernel.*\n\u200b\n\u200b\n* **booster:** Select the type of model to run at each iteration\n    * gbtree: tree-based models\n    * gblinear: linear models\n* **nthread:** default to maximum number of threads available if not set\n* **objective:** This defines the loss function to be minimized\n\u200b\n**Parameters for controlling speed**\n\u200b\n* **subsample:** Denotes the fraction of observations to be randomly samples for each tree\n* **colsample_bytree:** Subsample ratio of columns when constructing each tree.\n* **n_estimators:**  Number of trees to fit.\n\u200b\n**Important parameters which control overfiting**\n\u200b\n* **learning_rate:** Makes the model more robust by shrinking the weights on each step\n* **max_depth:** The maximum depth of a tree.\n* **min_child_weight:** Defines the minimum sum of weights of all observations required in a child.\n\n\n### Tuning the Hyper-Parameters\n\n**GridSearchCV params:**\n* **estimator:** estimator object\n* **param_grid :** dict or list of dictionaries\n* **scoring:** A single string or a callable to evaluate the predictions on the test set. If None, the estimator\u2019s score method is used.\n    * https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n* **n_jobs:** Number of jobs to run in parallel. None means. -1 means using all processors.\n* **cv:** cross-validation, None, to use the default 3-fold cross validation. Integer, to specify the number of folds in a (Stratified)KFold.\n","98e36e98":"* As **Age** increases purchasing power increases as shown in graph\n* As **Salary** increases purchasing power increases\n* **Credit card debt** have no effect on purchasing power\n* **Net Worth** Too have +ve relation with purchasing power\n\n[Note]: This is synthetic data thus contains minimum outliers","43ddac47":"<a id=3>\n    \n    \n# Setting up baseline with `XGBoost`\n    \n* Will use Randomized SearchCV","64a89f27":"## What is Neural Network?\nNeural Network is a series of algorithms that are trying to mimic the human brain and find the relationship between the sets of data. It is being used in various use-cases like in `regression`, `classification`, `Image Recognition` and many more.\n\nSome major differences between them are **biological neural network does parallel processing** whereas the **Artificial neural network (ANN) does series processing** also in the *former one processing is slower* (in millisecond) *while in the latter one processing is faster* (in a nanosecond).\n\n[Source](https:\/\/www.analyticsvidhya.com\/blog\/2021\/07\/understanding-the-basics-of-artificial-neural-network-ann\/)\n\n# Introduction to ANN\n\nThe units in the neural layer try to learn about the information gathered by weighing it *according to the ANN\u2019s internal system*. These guidelines allow units to generate a transformed result, which is then provided as an output to the next layer.\n\nAn additional set of learning rules makes use of backpropagation, **a process** through which the **ANN can adjust its output results by taking errors into account**. `Through backpropagation`, **each time the output is labeled as an error during the supervised training phase, the information is sent backward**. Each `weight is updated proportionally to how much they were responsible for the error.`\n\n`Hence`, the `error is used to recalibrate the weight of the ANN\u2019s unit connections to take into account the difference between the desired outcome and the actual one.` **In due time, the ANN will \u201clearn\u201d how to minimize the chance for errors and unwanted results**.\n\nTraining an artificial neural network involves choosing from allowed models for which there are several associated algorithms.\n\n* An ANN has several advantages but one of the most recognized of these is the fact that **it can actually learn from observing data sets**. In this way, ANN `is used as a random function approximation tool`. These types of tools help estimate the most cost-effective and ideal methods for arriving at solutions while defining computing functions or distributions.\n\n* `ANN takes data samples rather than entire data sets` to arrive at solutions, which saves both time and money. ANNs are considered fairly simple mathematical models to enhance existing data analysis technologies.\n\n#### Termologies\n1. **Epochs** :`One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.` Since one epoch is too big to feed to the computer at once we divide it in several smaller batches. \n**Why we use more than one Epoch?**\n> know it doesn\u2019t make sense in the starting that \u2014 passing the entire dataset through a neural network is not enough. And we need to pass the full dataset multiple times to the same neural network. But keep in mind that **we are using a limited dataset and to optimise the learning and the graph we are using Gradient Descent which is an iterative process**. So, updating the weights with single pass or one epoch is not enough. + `One epoch leads to underfitting of the curve in the graph`\n\n**So, what is the right numbers of epochs?**\n> Not defined number, but you can say that the numbers of epochs is related to how diverse your data is.\n\n2. **Batch** : Think of a batch as a for-loop iterating over one or more samples and making predictions. At the end of the batch, the predictions are compared to the expected output variables and an error is calculated. From this error, the update algorithm is used to improve the model, e.g. move down along the error gradient.\n3. **Batch Sizers** :`Total number of training examples present in a single batch.` **Batch size is a hyperparameter**\n4. **Iterations** : `Iterations is the **number of batches** needed to complete one epoch.`\n5. **validation_split** : Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling.\n\n**Note: The number of batches is equal to number of iterations for one epoch.**\n\n[Source for Validation_split](https:\/\/datascience.stackexchange.com\/questions\/38955\/how-does-the-validation-split-parameter-of-keras-fit-function-work)\n[Source 1](https:\/\/machinelearningmastery.com\/difference-between-a-batch-and-an-epoch\/)  \n[Source 2](https:\/\/www.techopedia.com\/definition\/5967\/artificial-neural-network-ann)  \n[Source 3](https:\/\/towardsdatascience.com\/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)","450de13b":"When Batch size was 25\nEpoch 50\/50\n10\/10 [==============================] - 0s 8ms\/step - loss: 3.5063e-04 - val_loss: 4.4070e-04  \nWhen Batch size was 35  \nEpoch 50\/50\n7\/7 [==============================] - 0s 14ms\/step - loss: 1.4218e-04 - val_loss: 1.7630e-04\n\nWhen Batch size was 35 ( but restarted our kernal)  \nEpoch 50\/50\n7\/7 [==============================] - 0s 12ms\/step - loss: 1.2834e-04 - val_loss: 1.2981e-04\n\nWhen Batch size was 35 & Unit 35 ( Restarted the kerna)  \nEpoch 50\/50\n7\/7 [==============================] - 0s 11ms\/step - loss: 1.2718e-04 - val_loss: 1.3947e-04\n\nWhen Batch size was 35 & Unit 35 `Epoch 75`( without Restarting the kerna)\nEpoch 75\/75\n7\/7 [==============================] - 0s 11ms\/step - loss: 1.1968e-05 - val_loss: 2.8188e-05\n\nWhen Batch size was 35 & Unit 35 `Epoch 75`( Restarted the kerna) \nEpoch 75\/75\n7\/7 [==============================] - 0s 12ms\/step - loss: 6.8962e-05 - val_loss: 1.6449e-04\n\n\n### the more epoch we have the more training the data again and again","ec5091fe":"## Model Evaluation","3803e21d":"## Best Parameters\n{'colsample_bytree': 0.7,\n 'learning_rate': 0.1,\n 'max_depth': 3,\n 'min_child_weight': 1,\n 'n_estimators': 500,\n 'objective': 'reg:squarederror',\n 'subsample': 0.5}\n","bfd54b31":"# Model Deployment","24348fe7":"## Prediction","d311baa1":"<a id=5>\n    \n    \n# ANN","b2307c01":"## Continution of optimizer\n\n1. Learning Rate : Between `0.1 to 1` (to get the best result)\n2. Size of batch : We need to do hyperparameter tunning to check optimum batchsize for each model\n"}}