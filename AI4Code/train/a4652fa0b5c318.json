{"cell_type":{"054e6274":"code","def77c94":"code","ab3c6210":"code","ed59520b":"code","082d0847":"code","fab3e13a":"code","cb42a70b":"code","1242ce06":"code","6df90009":"code","e8f13cb8":"code","02b31f1a":"code","9702e321":"code","d2160c63":"code","fd1e4f3a":"code","06730733":"code","94a08a8e":"code","7b1ba2d7":"code","8705c4c4":"code","0ae78fd4":"code","177e2afa":"code","fe575a0a":"code","0af2e8c0":"code","433528c2":"code","969a0913":"code","b87f6bd4":"code","e5a275f3":"code","10818e39":"code","6418e410":"markdown","6bb6d217":"markdown","c0c66efd":"markdown","58fa8d7c":"markdown","0fcd9eee":"markdown","ab4a2449":"markdown","a76e77cb":"markdown","0488faf4":"markdown","d45431c3":"markdown","3c6a2a4d":"markdown","81d2c84e":"markdown","1dcfbc54":"markdown","770b7b53":"markdown","7a2a659b":"markdown","e2058d88":"markdown","28d00f0e":"markdown","55985c68":"markdown","4faa7e6d":"markdown","5f6ac105":"markdown","0973c84b":"markdown","c0451ce2":"markdown","c9ffc416":"markdown","431ae6cf":"markdown","08239ad1":"markdown","930964eb":"markdown","a11b146c":"markdown","0b1d50a4":"markdown","730cbcf8":"markdown","203a791c":"markdown","14bde04c":"markdown","574247ae":"markdown","c8bab652":"markdown","290ceab1":"markdown","bf0b9fe2":"markdown","1b27ce94":"markdown","e3d02bdc":"markdown","e76aa49d":"markdown","5808dbb5":"markdown","8f70706e":"markdown"},"source":{"054e6274":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","def77c94":"!pip install ..\/input\/python-datatable\/datatable-0.11.0-cp37-cp37m-manylinux2010_x86_64.whl\nimport datatable as dt\ntrain = dt.fread(\"..\/input\/jane-street-market-prediction\/train.csv\").to_pandas() #, max_nrows=30000000\nfeatures = pd.read_csv(\"..\/input\/jane-street-market-prediction\/features.csv\")\nexample_test = pd.read_csv(\"..\/input\/jane-street-market-prediction\/example_test.csv\", nrows=10**3 )","ab3c6210":"print(f'train shape:{train.shape}')\nprint(f'features shape:{features.shape}')\n","ed59520b":"train.sample(3)","082d0847":"features.sample(3)","fab3e13a":"\nexample_test.sample(3)","cb42a70b":"train.describe()","1242ce06":"print(\"No. of columns containing null values\")\nprint(len(train.columns[train.isna().any()]))\n\nprint(\"No. of columns not containing null values\")\nprint(len(train.columns[train.notna().all()]))\n\nprint(\"the columns containing null values\")\nprint(train.columns[train.isna().any()])\n","6df90009":"\nprint(\"No. of columns containing null values\")\nprint(len(features.columns[features.isna().any()]))\n\nprint(\"No. of columns not containing null values\")\nprint(len(features.columns[features.notna().all()]))\n","e8f13cb8":"#discovering unique values in data\ncols = train.columns\nfor c in cols: \n    print(f' The unique values in {c} :{train[c].nunique()}')","02b31f1a":"plt.figure(figsize=(18,18))\ndata_corr=train.corr()\nsns.heatmap(data_corr, cmap = 'coolwarm')","9702e321":"data_corr.style.background_gradient(cmap='coolwarm', axis=None).set_precision(2)","d2160c63":"\n#data_corr[np.absolute(data_corr['feature_1']) > 0.25]['feature_1']","fd1e4f3a":"date = 0\nn_features = 130\n\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.histogram(\n    train[train[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[0, 700], \n    range_x=[-8, 8]\n)\n\nhist.show()","06730733":"date = 0\nn_features = 130\ncols = [f'feature_{i}' for i in range(1, n_features)]\nhist = px.scatter(\n    train[train[\"date\"] == date], \n    x=cols, \n    animation_frame='variable', \n    range_y=[-500, 6500], \n    range_x=[-10, 10]\n)\nhist.layout.showlegend = False\nhist.show()","94a08a8e":"def plot_outlier_graph_set(df):\n    n_features = 10\n    feature_cols = [f'feature_{i}' for i in range(1, n_features)]\n    for f in feature_cols:\n        fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n        sns.boxplot(y=f, data=df, ax=axs[0])\n        sns.boxenplot(y=f, data=df, ax=axs[1])\n        sns.violinplot(y=f, data=df, ax=axs[2]) \n        sns.stripplot(y=f, data=df, size=4, color=\".3\", linewidth=0, ax=axs[3])\n        fig.suptitle(f, fontsize=15, y=1.1)\n        axs[0].set_title('Box Plot')\n        axs[1].set_title('Boxen Plot')\n        axs[2].set_title('Violin Plot')\n        axs[3].set_title('Strip Plot')\n        plt.tight_layout()\n        plt.show()\n        \nplot_outlier_graph_set(train)","7b1ba2d7":"fig, ax = plt.subplots(figsize=(16, 6))\nfeature_1 = pd.Series(train['feature_1']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_1 (cumulative)\", fontsize=18);\nfeature_1.plot(lw=2);","8705c4c4":"fig, ax = plt.subplots(figsize=(16, 6))\nfeature_3 = pd.Series(train['feature_3']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_3 (cumulative)\", fontsize=18);\nfeature_3.plot(lw=2);","0ae78fd4":"fig, ax = plt.subplots(figsize=(16, 6))\nfeature_55 = pd.Series(train['feature_55']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_55 (cumulative)\", fontsize=18);\nfeature_55.plot(lw=2);","177e2afa":"fig, ax = plt.subplots(figsize=(16, 6))\nfeature_77 = pd.Series(train['feature_77']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_ylabel (\"feature_77  (cumulative)\", fontsize=18);\nfeature_77.plot(lw=2);","fe575a0a":"date_start = 0\ndate_end = 30\n\nfig = px.histogram(\n    train[(train.date >= date_start) & (train.date <= date_end)], \n    x=['resp_1', 'resp_2', 'resp_3', 'resp_4','resp'], \n    facet_col='variable', animation_frame='date')\nfig.show()","0af2e8c0":"date = 0\nhist = px.scatter(\n    train[train[\"date\"] == date], \n    x=['resp_1', 'resp_2', 'resp_3', 'resp_4','resp'], \n    animation_frame='variable', \n    range_y=[-500, 6500], \n    range_x=[-0.3, 0.3]\n)\nhist.layout.showlegend = False\nhist.show()","433528c2":"fig, ax = plt.subplots(figsize=(16, 6))\nbalance= pd.Series(train['resp']).cumsum()\nresp_1= pd.Series(train['resp_1']).cumsum()\nresp_2= pd.Series(train['resp_2']).cumsum()\nresp_3= pd.Series(train['resp_3']).cumsum()\nresp_4= pd.Series(train['resp_4']).cumsum()\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative return of Resps and time (500 days)\", fontsize=18)\nbalance.plot(lw=2)\nresp_1.plot(lw=2)\nresp_2.plot(lw=2)\nresp_3.plot(lw=2)\nresp_4.plot(lw=2)\nplt.legend(loc=\"upper left\")","969a0913":"def plot_outlier_graph_set1(df):\n    feature_cols = [\"resp\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\"]\n    for f in feature_cols:\n        fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n        sns.boxplot(y=f, data=df, ax=axs[0])\n        sns.boxenplot(y=f, data=df, ax=axs[1])\n        sns.violinplot(y=f, data=df, ax=axs[2]) \n        sns.stripplot(y=f, data=df, size=4, color=\".3\", linewidth=0, ax=axs[3])\n        fig.suptitle(f, fontsize=15, y=1.1)\n        axs[0].set_title('Box Plot')\n        axs[1].set_title('Boxen Plot')\n        axs[2].set_title('Violin Plot')\n        axs[3].set_title('Strip Plot')\n        plt.tight_layout()\n        plt.show()\n        \nplot_outlier_graph_set1(train)","b87f6bd4":"date_start = 0\ndate_end = 30\n\nfig = px.histogram(\n    train[(train.date >= date_start) & (train.date <= date_end)], \n    x=['weight'], \n    facet_col='variable', animation_frame='date')\nfig.show()","e5a275f3":"fig, ax = plt.subplots(figsize=(16, 6))\nweight= pd.Series(train['weight']).cumsum()\n\nax.set_xlabel (\"Trade\", fontsize=18)\nax.set_title (\"Cumulative return of weight and time  (500 days)\", fontsize=18)\nweight.plot(lw=2)\n\nplt.legend(loc=\"upper left\")","10818e39":"def plot_outlier_graph_set2(df):\n    feature_cols = [\"weight\"]\n    for f in feature_cols:\n        fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n        sns.boxplot(y=f, data=df, ax=axs[0])\n        sns.boxenplot(y=f, data=df, ax=axs[1])\n        sns.violinplot(y=f, data=df, ax=axs[2]) \n        sns.stripplot(y=f, data=df, size=4, color=\".3\", linewidth=0, ax=axs[3])\n        fig.suptitle(f, fontsize=15, y=1.1)\n        axs[0].set_title('Box Plot')\n        axs[1].set_title('Boxen Plot')\n        axs[2].set_title('Violin Plot')\n        axs[3].set_title('Strip Plot')\n        plt.tight_layout()\n        plt.show()\n        \nplot_outlier_graph_set2(train)","6418e410":"## Distribution of Resps\nThere are 500 days of data in train.csv . Let us take a look at the first 30th days","6bb6d217":"## Correlation coefficients\nCorrelation coefficients are used in statistics to measure how strong a relationship is between two variables","c0c66efd":"## Cumulative return of weight","58fa8d7c":"## Distribution of features\nThere are  500 days of data in train.csv . Let us take a look at date=0 (the first day)","0fcd9eee":"## Noisy features\nFeatures 3, 4, 5, 6, 8, 12, 37, 38, 72, 74, 78, 80, 83: all seem to look somewhat like this:","ab4a2449":"## The columns containing null values in features.csv","a76e77cb":"## Cumulative return of Resps","0488faf4":"## Discover outlier values in Weight","d45431c3":"## Discover outlier values in features\nI just have selected the first 10 features","3c6a2a4d":"<a id=\"61\"><\/a>\n# Features feature_{0...129}\n","81d2c84e":"## Hybrid features\nThat start off noisy, but then go linear; 55, 56, 57, 58, 59:","1dcfbc54":"## train.csv","770b7b53":"## Linear features\nFeatures 1, 7, 9, 11, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 41, 46, 47, 48, 49, 50, 51, 53, 54, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95 (strong change in gradient), 96, 97, 98, 99, 100, 101, 102 (strong change in gradient), 103, 104, 105, 106, 107 (strong change in gradient), 108, 110, 111, 113, 114, 115, 116, 117, 118, 119 (strong change in gradient), 120, 122, and 124. all seem to look somewhat like this:","7a2a659b":"![download.png](attachment:download.png)","e2058d88":"## References\n* https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance\n* https:\/\/www.kaggle.com\/blurredmachine\/jane-street-market-eda-viz-prediction\n* https:\/\/www.kaggle.com\/xhlulu\/jane-street-animated-and-interactive-eda","28d00f0e":"# Content\n1. [Intoduction](#1)\n2. [Evaluation](#2)\n3. [Import Libraries](#3)\n4. [Importing Data](#5)\n5. [Explore the features of data](#5)\n6. [Exploratory Data Analysis](#6)\n    * [Features feature_{0...129}](#61) \n    * [Resps](#62) \n    * [Wieghts](#63)  ","55985c68":"## Train Describe","4faa7e6d":"<a id=\"1\"><\/a>\n# 1-Introduction\nIn a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. As a result, products would always remain at their \u201cfair values\u201d and never be undervalued or overpriced. However, financial markets are not perfectly efficient in the real world.\n\nYour challenge will be to use the historical data, mathematical tools, and technological tools at your disposal to create a model that gets as close to certainty as possible. You will be presented with a number of potential trading opportunities, which your model must choose whether to accept or reject.","5f6ac105":"<a id=\"6\"><\/a>\n# 6-Exploratory Data Analysis","0973c84b":"## The columns containing null values in train.csv","c0451ce2":"![Stock_Trading_0.jpg](attachment:Stock_Trading_0.jpg)","c9ffc416":"## Distribution of Weight\nThere are 500 days of data in train.csv . Let us take a look at the first 30th days","431ae6cf":"## Discovering unique values in train.csv","08239ad1":"<a id=\"62\"><\/a>\n# Resps","930964eb":"<a id=\"2\"><\/a>\n# 2-Evaluation\nThis competition is evaluated on a utility score. Each row in the test set represents a trading opportunity for which you will be predicting an action value, 1 to make the trade and 0 to pass on it. Each trade j has an associated weight and resp, which represents a return.\n\nFor each date i, we define:\n![%D8%A7%D9%84%D8%AA%D9%82%D8%A7%D8%B7.JPG](attachment:%D8%A7%D9%84%D8%AA%D9%82%D8%A7%D8%B7.JPG)\nwhere |i| is the number of unique dates in the test set. The utility is then defined as:\n![a.JPG](attachment:a.JPG)","a11b146c":"<a id=\"4\"><\/a>\n# 4-Importing Data","0b1d50a4":"# PLEASE UPVOTE if you like this kernel.","730cbcf8":"## Linear,Noisy,Hybrid and Negative features\nThanks Carl McBride Ellis [here](https:\/\/www.kaggle.com\/carlmcbrideellis\/jane-street-eda-of-day-0-and-feature-importance)","203a791c":"<a id=\"63\"><\/a>\n# Weight","14bde04c":"## Spread of features\nThere are  500 days of data in train.csv . Let us take a look at date=0 (the first day)","574247ae":"## Discover outlier values in Resps","c8bab652":"## Spread of Resps\nThere are  500 days of data in train.csv . Let us take a look at date=0 (the first day)","290ceab1":"## Negative features\nFeatures 73, 75, 76, 77 (noisy), 79, 81(noisy), 82:","bf0b9fe2":"# Jane Street Market Prediction","1b27ce94":"## example_test.csv","e3d02bdc":"<a id=\"3\"><\/a>\n# 3-Import Libraries","e76aa49d":"<a id=\"5\"><\/a>\n# 5-Explore the features of data","5808dbb5":"## fearures.csv","8f70706e":"* I used datatable method to speed read train.csv, then convert to a pandas dataframe\n* Datatable (heavily inspired by R's data.table) can read large datasets fairly quickly and is often faster than pandas. It is specifically meant for data processing of tabular datasets with emphasis on speed and support for large sized data.\n* Documentation: https:\/\/datatable.readthedocs.io\/en\/latest\/index.html"}}