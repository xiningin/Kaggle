{"cell_type":{"4a0bce80":"code","13b87cfc":"code","05d4d753":"code","241a28a9":"code","af07d436":"code","e8c2adee":"code","3476b316":"code","f60af73a":"code","4887a621":"code","490fa368":"code","7c862442":"code","bfe9ceaa":"code","720885b2":"code","1aaf7b05":"code","355c157d":"code","2d060ce3":"code","760836da":"code","5a92438c":"code","0cdb7c4b":"code","f2ef7b7d":"code","16610910":"code","96f2e036":"code","8a977851":"code","68158f5b":"code","ef5b73f9":"code","2e6b91ea":"code","9364ac74":"code","7975f8ed":"code","049f14cc":"code","3a8bf06d":"code","258913c3":"markdown","3074429f":"markdown","75c60431":"markdown","2c0f066d":"markdown","bd21d6c3":"markdown","01b4f057":"markdown","ea3ebd39":"markdown","0e88936e":"markdown","e9739173":"markdown","aa59bfb2":"markdown","4b740e67":"markdown","3caa6f74":"markdown","0da02df2":"markdown","d95d20ed":"markdown","14f19051":"markdown","178a06bc":"markdown","b2ebcceb":"markdown","50792034":"markdown","dc594cbc":"markdown","2b5954c1":"markdown","7c9bf967":"markdown","9dcfcedf":"markdown","e197fe88":"markdown","88f7f499":"markdown"},"source":{"4a0bce80":"import json \nimport pandas as pd \nimport os\nfrom pathlib import Path\nfrom pandas.io.json import json_normalize \n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer","13b87cfc":"base_dir='\/kaggle\/input\/CORD-19-research-challenge\/'\ndir_list = ['biorxiv_medrxiv','noncomm_use_subset']","05d4d753":"raw_documents=[]\ndocuments_name=[]\ntitle_name=[]\ndoc_df=pd.DataFrame()\n\ndef getTextFromJSON(d):     \n    try:\n        bodytext_df = json_normalize(d['body_text'])\n        body_text_list = bodytext_df['text'].tolist()\n    except KeyError:\n        body_text_list=[]\n        \n    title_df = json_normalize(d['metadata'])\n    title = title_df['title'][0]\n\n    return {'body':body_text_list,'title':title};\n    \ndef pouplateRawDocumentsFromFiles(directory_in_str):\n    pathlist = Path(directory_in_str).glob('**\/*.json')\n    for path in pathlist:\n        path_in_str = str(path)\n        f = open(path_in_str)\n        d = json.load(f)\n        raw_documents.append(getTextFromJSON(d)['body'])\n        title_name.append(getTextFromJSON(d)['title'])\n        documents_name.append(path_in_str)\n    return ;\n\ndef loadData():     \n    for directory in dir_list:\n        pouplateRawDocumentsFromFiles(base_dir+directory)\n\n    doc_df['doc_content']=raw_documents\n    doc_df['doc_name']=documents_name\n    doc_df['doc_title']=title_name\n","241a28a9":"loadData()","af07d436":"print('The total number of files from folder(s) '+str(dir_list)+ ' are: '+str(len(raw_documents)))","e8c2adee":"doc_df.drop_duplicates(subset =\"doc_title\", keep = False, inplace = True) \nprint('The total number of files after removing duplicate files from folder(s) '+str(dir_list)+ ' are: '+str(len(doc_df)))","3476b316":"stop_words = stopwords.words('english')\nstop_words.extend(['copyright','peer','holder','preprint','author','funder','https','doi','org','reviewed',\n                    'bioRxiv','et','found','sought','wondered','reserved','rights','reuse',\n                    'permission','allowed','without','right','using','http','although','create','however',\n                    'cc','by','license','biorxiv','medrxiv','fig','http','1101','et','al','figure',\n                    'nc','nd','4','0','international','made','available','under','for','this',\n                    'preprint','which','was','not','10','2020','01','30','927574','ma','02','03',\n                    'could','non','within','il','three','value','shown','pcr','rnl','ml','min','mm','new','would',\n                    'related','thus','per','site','like','known','may','also','used','know','e','g','settings','usefulness'])","f60af73a":"def getPreProcessesText(raw_text_str):\n    lemmatizer = WordNetLemmatizer()\n\n    # Remove all the special characters\n    cleansed_text_str = re.sub(r'\\W', ' ', raw_text_str)\n    # Remove numbers \n    cleansed_text_str = re.sub(r' \\d+ ', ' ', cleansed_text_str)\n    # Remove all single characters\n    cleansed_text_str = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', cleansed_text_str)\n    # Remove single characters from the start\n    cleansed_text_str = re.sub(r'\\^[a-zA-Z]\\s+', ' ', cleansed_text_str) \n    # Substituting multiple spaces with single space\n    cleansed_text_str = re.sub(r'\\s+', ' ', cleansed_text_str, flags=re.I)\n    # Converting to Lowercase\n    cleansed_text_str = cleansed_text_str.lower()\n\n    # Lemmatization\n    cleansed_text_list = cleansed_text_str.split()\n    lemmatized_text_list = [lemmatizer.lemmatize(word) for word in cleansed_text_list]\n    lemmatized_text_stopped_list = [word for word in lemmatized_text_list if word not in stop_words]\n    preprocessed_text_str = ' '.join(lemmatized_text_stopped_list)\n    return preprocessed_text_str ","4887a621":"task1=\"What is known about transmission, incubation, and environmental stability?\\\nWhat do we know about natural history, transmission, and diagnostics for the virus?\\\nWhat have we learned about infection prevention and control?\\\nRange of incubation periods for the disease in humans (and how this varies across age and health status)\\\nand how long individuals are contagious, even after recovery.\\\nPrevalence of asymptomatic shedding and transmission (e.g., particularly children).\\\nSeasonality of virus transmission and virus spread.\\\nPhysical science of the coronavirus (e.g., charge distribution, adhesion to hydrophilic\/phobic surfaces, \\\nenvironmental survival to inform decontamination efforts for affected areas and \\\nprovide information about viral shedding).\\\nPersistence and stability on a multitude of substrates and sources \\\n(e.g., nasal discharge, sputum, urine, fecal matter, blood).\\\nPersistence of virus on surfaces of different materials (e,g., copper, stainless steel, plastic).\\\nNatural history of the virus and shedding of it from an infected person.\\\nImplementation of diagnostics and products to improve clinical processes.\\\nDisease models, including animal models for infection, disease and transmission.\\\nTools and studies to monitor phenotypic change and potential adaptation of the virus.\\\nImmune response and immunity.\\\nEffectiveness of movement control strategies to prevent secondary transmission in health care and community settings.\\\nEffectiveness of personal protective equipment (PPE) and \\\nits usefulness to reduce risk of transmission in health care and community settings.\\\nRole of the environment in transmission.\"","490fa368":"from nltk.tokenize import word_tokenize\nimport gensim.models.phrases\nfrom nltk import ngrams\n\ndef getBiGramList(sentence):\n    biGramList=[]\n    for grams in ngrams(sentence.split(), 2):\n        if grams[0] not in stop_words:\n            biGramList.append(grams[0]+' '+grams[1])\n    return biGramList;","7c862442":"raw_para_documents=[]\nraw_para_doc_name=[]\nraw_para_doc_title=[]\npara_df=pd.DataFrame()","bfe9ceaa":"BIGRAM_WHITE_LIST=getBiGramList(getPreProcessesText(task1))\nUNIGRAM_WHITE_LIST=['surfaces','asymptomatic','contagious']\nDOC_WHITE_LIST=UNIGRAM_WHITE_LIST+BIGRAM_WHITE_LIST\nDOC_BLACK_LIST=['The copyright holder for this preprint']\n\nVALID_PARAGRAPH_MIN_LENGHT=190\n\ndef doesParagraphContainAnyOfListNGrams(paragraph,selectionList):\n    isPresent=False;\n    for part in selectionList:\n        if part.lower() in paragraph.lower():\n            isPresent=True; \n            break;\n    return isPresent;\n\ndef isParagraphLengthValid(paragraph):\n    return len(paragraph)>=VALID_PARAGRAPH_MIN_LENGHT;\n\ndef populateRawParagraphsFromRawDocs():\n    for index, row in doc_df.iterrows():      \n        for paragraph in row['doc_content']: \n            if isParagraphLengthValid(paragraph):\n                if doesParagraphContainAnyOfListNGrams(paragraph, DOC_WHITE_LIST) and not doesParagraphContainAnyOfListNGrams(paragraph, DOC_BLACK_LIST): \n                    raw_para_documents.append(paragraph)\n                    raw_para_doc_name.append(row['doc_name'])\n                    raw_para_doc_title.append(row['doc_title'])\n    return;","720885b2":"populateRawParagraphsFromRawDocs()\nprint(len(raw_para_documents))\nprint('The total number of paragraphs for model training from folder(s) '+str(dir_list)+ ' are: '+str(len(raw_para_documents)))","1aaf7b05":"para_df['para_content']=raw_para_documents\npara_df['doc_name']=raw_para_doc_name\npara_df['doc_title']=raw_para_doc_title","355c157d":"documents=[]\n\ndef pouplateDocumentsFromFiles():\n    for raw_document in raw_para_documents:\n        documents.append(getPreProcessesText(raw_document))\n    \npouplateDocumentsFromFiles()\nprint(len(documents))","2d060ce3":"def trainDoc2Vec(tagged_data):\n    model=Doc2Vec(dm=0, vector_size=300, min_count=1, epochs=30, workers=16)\n    model.build_vocab(tagged_data)\n    model.train(tagged_data,total_examples=model.corpus_count,epochs=model.epochs)\n\n    model.save(\"d2vBodyCovid.model\")\n    print(\"Model Saved\")\n    return model;","760836da":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\n\ntagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(documents)]","5a92438c":"trainDoc2Vec(tagged_data)","0cdb7c4b":"from gensim.models.doc2vec import Doc2Vec\n\nmodel= Doc2Vec.load(\"d2vBodyCovid.model\")\n\ntransmissionQuery=\"What is known about transmission?\\\nIs the virus transmitted by aerosol, droplets, food, close contact, fecal matter, or water?\\\nPrevalence of asymptomatic shedding and virus transmission?\\\nHow does weather, heat, and humidity affect the tramsmission of 2019-nCoV?\\\nSeasonality of virus transmission and virus spread?\\\nDisease models, including animal models for infection, disease and transmission?\"\n\nincubationPeriodQuery=\"What is known about incubation?\\\nHow long is the incubation period in patients for covid,sars and mers virus in days?\\\nRange of incubation periods for the disease in humans ?\\\nHow the incubation period varies across age, health status?\\\nHow long individuals are contagious, even after recovery?\"\n\nenvironmentalStabilityPeriodQuery=\"What is known about environmental stability?\\\nHow long can the 2019-nCoV virus remain viable on common surfaces?\\\nWhat is known about charge distribution, adhesion to hydrophilic\/phobic surfaces?\\\nWhat is the environmental survival to inform decontamination efforts for affected areas \\\nand provide information about viral shedding?\\\nWhat do we know about physical science of the coronavirus?\"\n\ndiseaseQuery=\"What is known about diseases, symptoms associated with covid,sars,mers?\\\nWhat has been published about medical care?\\\nWhat do we know about vaccines and therapeutics\"\n\npreventionQuery=\"What is the immune system response to 2019-nCoV?\\\nCan 2019-nCoV infect patients a second time?\\\nEffectiveness of movement control strategies to prevent secondary transmission\\\nin health care and community settings?\\\nEffectiveness of personal protective equipment (PPE) and \\\nits usefulness to reduce risk of transmission in health care and community settings?\"\n\nquestions=[transmissionQuery,incubationPeriodQuery,environmentalStabilityPeriodQuery,diseaseQuery,preventionQuery]","f2ef7b7d":"NUM_OF_PARAGRAPHS_TO_PICK=60\nNUM_OF_PARAGRAPHS_TO_SUMMARIZE=30\nDOC_SUMMARY_RATIO=0.50","16610910":"from gensim.summarization.summarizer import summarize \nfrom gensim.summarization import keywords \n\ndef getDocFileName(doc_tuple):\n    return para_df['doc_name'].tolist()[int(doc_tuple[0])];\n\ndef getDocTitle(doc_tuple):\n    return para_df['doc_title'].tolist()[int(doc_tuple[0])];\n\ndef getRelevantLiterature(similarDoc,numOfParagraphs):\n    doc_count=0\n    search_result_docs=[]\n    search_result_titles=[]\n    result_df=pd.DataFrame()\n    for doc_tuple in similarDoc:  \n        if doc_count < numOfParagraphs:\n            search_result_docs.append(getDocFileName(doc_tuple))\n            search_result_titles.append(getDocTitle(doc_tuple))\n        doc_count=doc_count+1\n    result_df['doc_name']=search_result_docs\n    result_df['doc_title']=search_result_titles\n    result_df.drop_duplicates(subset =\"doc_name\", keep = False, inplace = True) \n    return result_df;\n\ndef getRelevantDocsWithSummary(doc_tuple):\n    relevant_doc=\"\"\n    docText=raw_para_documents[int(doc_tuple[0])]\n    try:\n        summary=summarize(docText, ratio=DOC_SUMMARY_RATIO)\n        if len(summary)>0:\n            relevant_doc=summary\n        else:\n            relevant_doc=docText\n    except:\n        relevant_doc=docText\n    return relevant_doc;\n\ndef classifyDocs(similarDoc,numOfParagraphs):\n    doc_count=0\n    relevant_docs=[]\n    other_docs=[]\n    for doc_tuple in similarDoc:  \n        if doc_count < numOfParagraphs:\n            relevant_doc=getRelevantDocsWithSummary(doc_tuple)\n            relevant_docs.append(relevant_doc) \n        else:\n            other_docs.append(raw_para_documents[int(doc_tuple[0])])\n        doc_count=doc_count+1\n    return {'relevant_docs':relevant_docs,'other_docs':other_docs};\n \ndef getFinalSummary(relevant_docs):\n    summary_str=''\n    for summary in relevant_docs:\n        summary_str = summary_str+' '+summary\n    summary = summarize(summary_str, ratio = DOC_SUMMARY_RATIO) \n    return summary;\n    \ndef getAnswer(question):    \n    questionVector = model.infer_vector(word_tokenize(getPreProcessesText(question)))\n    similar_doc = model.docvecs.most_similar([questionVector],topn=NUM_OF_PARAGRAPHS_TO_PICK)\n    classified_docs=classifyDocs(similar_doc,NUM_OF_PARAGRAPHS_TO_SUMMARIZE)\n    return {'result_summary':getFinalSummary(classified_docs['relevant_docs']),\n            'result_literature':getRelevantLiterature(similar_doc,NUM_OF_PARAGRAPHS_TO_SUMMARIZE)}\n\ndef getClassifiedDocs(question):    \n    questionVector = model.infer_vector(word_tokenize(getPreProcessesText(question)))\n    similar_doc = model.docvecs.most_similar([questionVector],topn=NUM_OF_PARAGRAPHS_TO_PICK)\n    classified_docs=classifyDocs(similar_doc,NUM_OF_PARAGRAPHS_TO_SUMMARIZE)\n    return classified_docs;\n    ","96f2e036":"from termcolor import colored\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\ndef showWordCloud(data,stop_words):\n    mpl.rcParams['font.size']=12                \n    mpl.rcParams['savefig.dpi']=100             \n    mpl.rcParams['figure.subplot.bottom']=.1 \n\n    stopwords = set(stop_words)\n    wordcloud_text_str=data\n\n    wordcloud = WordCloud(background_color='black',\n                            stopwords=stopwords,\n                            max_words=100,\n                            max_font_size=60, \n                            random_state=42\n                            ).generate(wordcloud_text_str)\n\n    fig = plt.figure(1)\n    plt.imshow(wordcloud)\n    plt.axis('off') \n    plt.show()\n    return;","8a977851":"import sys \nfrom termcolor import colored, cprint \n\ndef printColored(text):\n    print(colored(text, 'red', attrs=['reverse', 'blink']))\n    \ndef printColoredDfRow(text):\n    print(colored(text, 'blue', attrs=['reverse', 'blink']))\n    \ndef getHighlightedtext(text,highlightList):\n    return \" \".join(colored(t,'white','on_green') if getPreProcessesText(t) in highlightList else t for t in text.split())\n\ndef displayResultDf(resultDf):\n    for index, row in resultDf.iterrows(): \n        printColoredDfRow(str(index+1)+'. '+row['doc_title'])\n        print(row['doc_name'])","68158f5b":"def seeTheResults(questions):\n    for question in questions:\n        printColored(\"===========================QUESTION================================\")\n        print(question)\n        printColored(\"\\n\\n==================HIGHLIGHTS OF SEARCH RESULT======================\")\n        showWordCloud(getAnswer(question)['result_summary'],stop_words)\n        printColored(\"=====================ANSWER OF QUESTION============================\")\n        print(getHighlightedtext(getAnswer(question)['result_summary'],word_tokenize(getPreProcessesText(question))))\n        printColored(\"\\n\\n============MOST RELEVANT LITERATURE [TITLE(NAME)]=================\")\n        displayResultDf(getAnswer(question)['result_literature'])\n        printColored(\"\\n======================RESULT COMPLETED=============================\")\n        print('\\n' * 2)\n    return;","ef5b73f9":"seeTheResults(questions)","2e6b91ea":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\ndef getVecsAndLabels(query):\n    vecs=[]\n    labels=[]\n\n    taskVector = model.infer_vector(word_tokenize(query))\n    labels.append(\"QUERY_TASK\")\n    vecs.append(taskVector)\n\n    count=0\n    for summary in getClassifiedDocs(query)['relevant_docs']:\n        vecs.append(model.infer_vector(word_tokenize(getPreProcessesText(summary))))\n        labels.append(\"result\"+str(count))\n        count=count+1\n\n    count=0\n    for summary in getClassifiedDocs(query)['other_docs']:\n        vecs.append(model.infer_vector(word_tokenize(getPreProcessesText(summary))))\n        labels.append(\"d\"+str(count))\n        count=count+1\n        \n    return{'vecs':vecs,'labels':labels};\n\ndef tsne_plot(model,perplexity,query):\n    \"Creates and TSNE model and plots it\"\n    tokens = []\n    \n    vecs=getVecsAndLabels(query)['vecs']\n    labels=getVecsAndLabels(query)['labels']\n    \n    for docvec in vecs:\n        tokens.append(docvec)\n    \n    tsne_model = TSNE(perplexity=perplexity, n_components=2, init='pca', n_iter=250, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(12, 8)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()\n\ndef checkResultsFromDifferentDimensions(query):\n    for i in range(5):\n        print('View',i)\n        tsne_plot(model,i,query)","9364ac74":"checkResultsFromDifferentDimensions(incubationPeriodQuery)","7975f8ed":"testModel=False\n\npara_list=[]\ntest_rounds=3\nnumber_of_matching_docs=10","049f14cc":"def testModelConsistency(testQuery):\n    result_list=[]\n    for i in range(test_rounds):\n        model=trainDoc2Vec(tagged_data)\n        taskVector = model.infer_vector(word_tokenize(getPreProcessesText(testQuery)))\n        similar_doc = model.docvecs.most_similar([taskVector],topn=number_of_matching_docs)\n        result_list.append(similar_doc)\n        print(similar_doc)        \n    return result_list;\n\n\ndef intersection(lst1, lst2): \n    lst3 = [value for value in lst1 if value in lst2] \n    return lst3\n\ndef docMatchPercent(docIndex1,docIndex2):\n    res=(len(intersection(para_list[docIndex1],para_list[docIndex2]))\/number_of_matching_docs)*100\n    return res;\n\ndef printAvgMatchScoreAndPopulateParaList():\n    for res in result:\n        total=0\n        lst=[]\n        for tup in res:\n            lst.append(int(tup[0]))\n            total=total+tup[1]\n        print(total\/number_of_matching_docs)\n        para_list.append(lst)\n        \ndef printResultParaMatchingMatrix():\n    total=0\n    for i in range(test_rounds):\n        for j in range(test_rounds): \n            print(docMatchPercent(i,j),end=\" \")\n            if(i!=j):\n                total=total+docMatchPercent(i,j)\n        print(\"\")\n    avg_factor=(test_rounds*test_rounds)-test_rounds\n    print(\"\\nAverage result doc matching % across iterations=\"+str(total\/avg_factor))","3a8bf06d":"if(testModel):\n    result=testModelConsistency(incubationPeriodQuery)\n    print('Results of top '+ str(number_of_matching_docs) +' matching documents for '+ str(test_rounds) + ' test rounds:')\n    print(result)     \n    \nif(testModel):\n    print('\\nAverage document matching score in each iteration:')\n    printAvgMatchScoreAndPopulateParaList()\n    print('\\nMatrix showing doc matching % in each iteration:')\n    printResultParaMatchingMatrix()","258913c3":"* Since the outbreak of COVID-19, found an overwhelming number of papers on the topic have been published from across the world. As per the dataset under consideration, there are 51,000 scholarly articles already published about COVID-19, SARS-CoV-2, and related coronaviruses. \n* There might be bits of information scattered across this huge dataset that may be pieces of puzzle that will help a medical researcher or a pharmaceutical organization to find cure or vaccine.\n* Unfortunately, its humanely impossible to scan this enormous data and to make matters worse thousands might be getting added every week. \n* In conclusion, unless an efficient method is developed that helps a team to make sense of this growing data, valuable time will be wasted on repeated research\u2019s.","3074429f":"# Deduce the final answer","75c60431":"![abstract.png](attachment:abstract.png)","2c0f066d":"# Defining the boundary of a document for doc2vec \nThere are 3 options which have been considered for the doc2vec model. The options are based on defining the boundary of a document for training model.\n1. A sentence is a doc\n2. A paragraph is a doc **(selected)**\n3. A full file text is a doc.\n\n**Option2** from the above list is selected as the most preffered option for this solution.\nResons are:\n* A paragraph is a number of sentences grouped together and relating to one context. Or, a group of related sentences that develop a single point. \n* Hence it represents the context better whereas a sentence has a very less context for the information and a full document contains tens or hunderds of points\/contexts.\n* As the most similar documents are predicted by the doc2vec model based on the vector similarity of the query\/question text. \n* In this case, the questions which are asked from the model are in form of sentences \n* e.g. \"What is known about incubation? How long is the incubation period in patients for covid,sars and mers virus in days? Range of incubation periods for the disease in humans? How the incubation period varies across age, health status? How long individuals are contagious, even after recovery?\". \n* Since, the question itself is asked in form of a paragraph, the best chance of getting accurate match is to match the paragraph from the documents instead of snetence or paragraph.\n\nSO, FOR TRAINING DOC2VEC MODEL, EACH PARAGRAPH WILL BE CONSIDERED AS ONE DOC i.e. every paragraph is represented as vector.\n\nThe lineage of each paragraph is maintained, i.e. to which filename the paragraph belongs and title of the document is maintained throughout so that it can be published in final answer.\n\n# Extracting dataset required for the task\n* Based on the document paragraph profiling, there were many paragraphs which were smaller in length and contained lot of noise. To overcome this issue, a minimum valid paragraph length is configured as 190 for optimum results.\n* Considering the 'garbage in garbage out' philosophy, it is imperative to ***not*** consider noisy paragraphs. For doing so, a whitelist has been configured with the bigrams extracted from the task descritpion and some important unigrams which are relevant to the task at hand. Based, on these whitelist and a blacklist to remove know garbage, the paragraphs are selected to train the model.\n","bd21d6c3":"*The outcome is near to the asked question in at least one of the dimension views*","01b4f057":"# 1. Problem Statement ","ea3ebd39":"# Train Doc2vec Model\nThe doc2vec model is trained using the parameters(dm=0, vector_size=300, min_count=1, epochs=30).\n\nA testing code (written in last few cells of the notebook) is used to measure the outcomes of the trained model and decide on the tuning parameters of the model.(In Last Section - Test Model Consistency and Matching Accuracy for tuning model)\n\nThe two main things considered across the various attempts of model training were\n1. consistency of results\n2. average matching score \n\nThe doc2vec model was initially trained by manually providing the learning rates, then by directly training doc2vec with various configurations like varying ecpoch,dm, min_count, window size etc.\nThe best combination was observed with below parameters. \n\n1. The selected model has average consistency in results across model attempts as approx 80% (with all values lying in close proximity) \n\n2. Average best matching score approx. 0.67(again with all values lying in close proximity).\nThe tests are done with random question text inputs.","0e88936e":"# See the Results!!\n","e9739173":"# Thanks! ","aa59bfb2":"* Below is the wrapper function to do all the above mentioned tasks on each file's raw text and obtain a document list which contains the de-duplicated, valid, relevant, cleansed and lemmatized paragraph text as outcome. \nThis outcome is fed to doc2vec model.","4b740e67":"# 2. Solution  ","3caa6f74":"* The input dataset set before model training is cleansed by doing the text preporcessing. \n* The preprocessing of text data includes removal of special characters, numbers, single digit characters, multiple spaces. \n* Post that text is converted to lowercase to mitigate the case sensitivity. \n* Finally, the text is lemmatized so that all the different forms of same words can be converged to a single word. ","0da02df2":"# Pros and Cons of the Approach\n\nPros \n* Doc2vec is a simple, generic and easy to build model \n* Finds document of interest.\n* Answers all aspects of the question.\n* Gensim summarizer provides a insightful representation of the answer to the asked question.\n* This approach can be extended to answer all tasks of the challenge.\n\nCons\n* Answers will have bumps - Not very coherant as an outcome of fetching the most similar paragraph based on the query.\n* Query needs to be elaborate in form of a paragraph to get the best results as we consider paragraphs as vectors. \n* If the query text is as small as a single sentence ,then we answers are less accurate.\n* It is not a one-size-fits-all approach .The model needs to retrained specific to the task which covers a different set of questions","d95d20ed":"base_dir is the base location of whole dataset.\nJust by configuring the required data folders in dir_list, the model can be pointed to train and execute on the given set of folders.\nThe code will extract all the files with .json extension under the given folder list.\n\nThe model has been tested with three combination of folders.\n1. biorxiv_medrxiv\n2. biorxiv_medrxiv,noncomm_use_subset\n3. biorxiv_medrxiv,noncomm_use_subset,comm_use_subset\n\nDue to time constraints in tuning and building the correct doc2vec model, test with combination of all 4 folders(i.e. biorxiv_medrxiv,noncomm_use_subset,comm_use_subset,custom_license ) has not been performed in this research.","14f19051":"## How to interpret the result wrt to the task.\n\n* We will cover all aspects of transmission , incubation and environmental stability of the virus as required by the task.\n\n  1. A question will be displayed first.\n  2. To arrive at the answer to the question asked,  \n  3. At the very high level the approach is that:\n  \n*  Doc2vec model is built using the paragraphs extracted from relevant documents classified basis the task query.\n*  The similarity is computed between the query vector and the individual paragraph vectors.\n*  The model outputs the ranked similarity score of each of the paragraph vector against the query vector.\n*  Paragraphs with top 30 similarity  score are picked.\n*  To help with some insights ,individual summaries using the Gensim Sumarize which used the page ranking technique.\n*  To arrive at an optimum summary percentage ratio, tests were conducted with varied ratios,thereby 50% summary percent ratio was decided.\n*  The individual paragraph summary is finally summarised to answer some of the key parts of the question.\n*  Summary helps get a good gist of what the result looks like and gauge the result content satisfaction based on the query.  \n\n    4. The word cloud displayed is a quick representation ,to validate the result by looking at the top words . Idea is to have the top words as close to the context of question as possible.\n    5. This final summary is printed and it also highlights the key parts of the question which help getting a  quick perception of the result .\n    6. Furthermore as per the task requirement the outcome also consists of the list of documents  used the extract the above results.The list of these documents shown with the title of the document and the filename is ranked basis the closeness to the answer.Formore information these documents can be used for reference.","178a06bc":"# *How Close are we with the results?*\n* The closeness of the results to the question is depicted using a t-SNE visualisation .\n* The graph is plotted using the below vectors\n  1. Doc Vectors - Vector represented by individual paragraphs. The vector labels have been classified to mark the top 30 matches as results(e.g. result1) and remaining as docs(e.g. d1).\n  2. Query Vector - Vector is formed by the question\/ query text.\n\nFor e.g.  let us look at results wrt incubation queriesEach visualisation is shown with a different perplexity.\n\nThe graphs indicate that all the selected results vectors are near to the query\/question vector in at least one perplexity.","b2ebcceb":"# Data Cleansing\n* The dataset contains many duplicate files with same title and body texts. However, each paper has a unique paper id.\n* This causes problems in training of the model as well as gives duplicate results. Hence, files have been filtered based on uniqueness of the paper titles.","50792034":"# Importing Dataset\n\nThe first step is to import the json dataset and extract the relevant details viz. \n* Document Title, \n* File Name \n* Research Text (stored under body_text)","dc594cbc":"# 3.Schematic of the Solution ","2b5954c1":"# Test Model Consistency and Matching Accuracy for tuning model?\n* Measure the outcomes of the trained model and decide on the tuning parameters of the model.\n* The two main things considered are consistency of results and average matching score across the various attempts of model training.\n* The selected model has the average consistency in results across model attempts as approx 80% (with all values lying in close proximity) and average best matching score approx. 0.67(again with all values lying in close proximity).The tests are done with random question text inputs.","7c9bf967":"# Ask Questions to the Model","9dcfcedf":"* The solution being proposed as part of submission is to build an unsupervised learning that help match research papers relevancy corresponding to question being asked. \n* The solution uses \u201cdoc2vec\u201d algorithm that will assign numeric representation to documents in response to questions being posed to the model. \n* The questions themselves are grouped in paragraphs with one paragraph representing a single theme such as transmission, incubation, etc. \n* The response is presented in three stages \u2013 \n* Word Map \u2013 this map highlights the most found words in the document\u2019s dataset in response to the question being asked. The emphasis of the words depends on their frequency present in the answer to the question.\n* Answer to the question \u2013 the answer section essentially performs a text mining task taking the question paragraph as input and running against the research papers in the data set. It lists all the sentences within the papers present in the data which as per AI represents the closest proximity to the question being answered.\n* \tList of Research Paper \u2013 the final stage of response is list of research papers sorted on the rank that will assist a researcher to narrow the papers to look for in response to questions posted. The ranking is again based on closest proximity of the research paper to the question set posted. ","e197fe88":"Based on the document word profiling, below are the noise words which hold very little value in determining accurate results.\nHence, such words have been discared in model consideration.","88f7f499":"Below is the task decription of first task which is the scope for this submission i.e.\n* **What is known about transmission, incubation, and environmental stability?**\n"}}