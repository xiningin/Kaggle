{"cell_type":{"bcfbc338":"code","1f475fd9":"code","c7ee8b22":"code","a83052c3":"code","03c56b87":"code","104867ab":"code","e576c78c":"code","2ab33318":"code","abbd1c52":"code","019b894c":"code","725473fa":"code","c08181b0":"code","f21b05d1":"code","554f7f6c":"code","d3dd2e48":"code","1e0777ac":"code","9f3c5e29":"code","ce0b41c5":"code","4115bf02":"code","08c35f79":"code","07970378":"markdown","fbc4572d":"markdown"},"source":{"bcfbc338":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport scipy\nimport math\nimport scipy.stats as stats\nfrom sklearn import preprocessing\n\nfrom sklearn.preprocessing import StandardScaler\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f475fd9":"#Set Options\npd.options.display.max_columns\npd.options.display.max_rows = 500","c7ee8b22":"_2010_2011_data =pd.read_excel(\"\/kaggle\/input\/uci-online-retail-ii-data-set\/online_retail_II.xlsx\",sheet_name= \"Year 2010-2011\" )","a83052c3":"#Get copy of dataFrame\n_2010_2011_df = _2010_2011_data.copy()","03c56b87":"def setTotalPrice(data):\n    #Add a column for total price to calculate monetary attritube\n    data[\"TotalPrice\"] = data[\"Quantity\"]*data[\"Price\"]","104867ab":"setTotalPrice(_2010_2011_df)","e576c78c":"#Remove missing Values From CustomerID\n_2010_2011_df.dropna(subset= [\"Customer ID\"],inplace= True)\n#Remove zero negative quantity\ndeleteRows = _2010_2011_df[~_2010_2011_df['Quantity'] > 0].index\n_2010_2011_df.drop(deleteRows, axis=0,inplace=True)\n#Some rows start with C means refund so we will remove them\ndeleteRows =  _2010_2011_df[_2010_2011_df[\"Invoice\"].str.contains(\"C\", na=False)].index\n_2010_2011_df.drop(deleteRows, axis=0,inplace=True)","2ab33318":"def CalculateRFM(data):\n    #Calculate recency\n    #Find out the first and last order dates in the data.\n    max_date = data['InvoiceDate'].max()\n    import datetime as dt\n    today_date = dt.datetime(max_date.year,max_date.month,max_date.day)\n    recency_df = data.groupby(\"Customer ID\").agg({'InvoiceDate': lambda x: (today_date - x.max()).days})\n    recency_df.rename(columns={\"InvoiceDate\":\"Recency\"}, inplace= True)\n    #calculate Frequency\n    temp_df =  data.groupby(['Customer ID','Invoice']).agg({'Invoice': \"count\"}).groupby(['Customer ID']).agg({\"Invoice\": \"count\"})\n    freq_df = temp_df.rename(columns={\"Invoice\": \"Frequency\"})\n    monetary_df=data.groupby(\"Customer ID\").agg({'TotalPrice': \"sum\"})\n    monetary_df.rename(columns={\"TotalPrice\": \"Monetary\"}, inplace=True)\n    rfm = pd.concat([recency_df,freq_df,monetary_df], axis = 1)\n    return rfm ","abbd1c52":"_2010_2011_rfm = CalculateRFM(_2010_2011_df)","019b894c":"#Set RFM Score\ndef setRFMScore(rfm):\n    # Get RFM scores for 3 attribute\n    rfm[\"RecencyScore\"] = pd.qcut(rfm['Recency'],5, labels=[5,4,3,2,1])\n    #if you calculate only transaction operations(unique invoice per customer) add rank(method=\"first\")\n    #if you sum all operations in per invoice no need to add rank method\n    rfm[\"FrequencyScore\"] = pd.qcut(rfm['Frequency'].rank(method=\"first\"),5, labels=[1,2,3,4,5])\n    rfm[\"MonetaryScore\"] = pd.qcut(rfm['Monetary'],5, labels=[1,2,3,4,5])\n    rfm[\"RFM_SCORE\"] = rfm[\"RecencyScore\"].astype(str) +rfm[\"FrequencyScore\"].astype(str)+rfm[\"MonetaryScore\"].astype(str) ","725473fa":"setRFMScore(_2010_2011_rfm)","c08181b0":"def setSegment(rfm):\n    seg_map = {\n    r'[1-2][1-2]': 'Hibernating',\n    r'[1-2][3-4]': 'At Risk',\n    r'[1-2]5': 'Can\\'t Loose',\n    r'3[1-2]': 'About to Sleep',\n    r'33': 'Need Attention',\n    r'[3-4][4-5]': 'Loyal Customers',\n    r'41': 'Promising',\n    r'51': 'New Customers',\n    r'[4-5][2-3]': 'Potential Loyalists',\n    r'5[4-5]': 'Champions'}\n    rfm[\"Segment\"] = rfm[\"RecencyScore\"].astype(str) + rfm[\"FrequencyScore\"].astype(str)\n    rfm[\"Segment\"] = rfm[\"Segment\"].replace(seg_map,regex=True)","f21b05d1":"setSegment(_2010_2011_rfm)","554f7f6c":"_2010_2011_rfm.head(100)","d3dd2e48":"_2010_2011_rfm['RFM_Segment_Score'] = _2010_2011_rfm[['RecencyScore','FrequencyScore','MonetaryScore']].sum(axis=1)","1e0777ac":"_2010_2011_rfm['Score'] = 'Green'\n_2010_2011_rfm.loc[_2010_2011_rfm['RFM_Segment_Score']>5,'Score'] = 'Bronze' \n_2010_2011_rfm.loc[_2010_2011_rfm['RFM_Segment_Score']>7,'Score'] = 'Silver' \n_2010_2011_rfm.loc[_2010_2011_rfm['RFM_Segment_Score']>9,'Score'] = 'Gold' \n_2010_2011_rfm.loc[_2010_2011_rfm['RFM_Segment_Score']>10,'Score'] = 'Platinum'","9f3c5e29":"# define function for the values below 0\ndef neg_to_zero(x):\n    if x <= 0:\n        return 1\n    else:\n        return x\n# apply the function to Recency and MonetaryValue column \n_2010_2011_rfm['Recency'] = [neg_to_zero(x) for x in _2010_2011_rfm.Recency]\n_2010_2011_rfm['Monetary'] = [neg_to_zero(x) for x in _2010_2011_rfm.Monetary]\n# unskew the data\n_2010_2011_rfm_log = _2010_2011_rfm[['Recency', 'Frequency', 'Monetary']].apply(np.log, axis = 1).round(3)","ce0b41c5":"# scale the data\nscaler = StandardScaler()\nrfm_scaled = scaler.fit_transform(_2010_2011_rfm_log)\n# transform into a dataframe\nrfm_scaled = pd.DataFrame(rfm_scaled, index = _2010_2011_rfm.index, columns = _2010_2011_rfm_log.columns)","4115bf02":"from sklearn import cluster\nimport seaborn as sns\n# the Elbow method\nwcss = {}\nfor k in range(1, 11):\n    kmeans = cluster.KMeans(n_clusters= k, init= 'k-means++', max_iter= 300)\n    kmeans.fit(rfm_scaled)\n    wcss[k] = kmeans.inertia_\n# plot the WCSS values\nsns.pointplot(x = list(wcss.keys()), y = list(wcss.values()))\nplt.xlabel('K Numbers')\nplt.ylabel('WCSS')\nplt.show()","08c35f79":"# clustering\nclus = cluster.KMeans(n_clusters= 3, init= 'k-means++', max_iter= 300)\nclus.fit(rfm_scaled)\n# Assign the clusters to datamart\n_2010_2011_rfm['K_Cluster'] = clus.labels_\n_2010_2011_rfm.head(100)","07970378":"Therefore when it comes to K-means clustering, scaling and normalizing data is a critical step for preprocessing. If we check the distribution of RFM values, you can notice that they are right-skewed. It\u2019s not a good state to use without standardization. Let\u2019s transform the RFM values into log scaled first and then normalize them.","fbc4572d":"The values below or equal to zero go negative infinite when they are in log scale, I made a function to convert those values into 1 and applied it to Recency and Monetary column, using list comprehension like above. And then, a log transformation is applied for each RFM values. The next preprocessing step is scaling but it\u2019s simpler than the previous step. Using StandardScaler(), we can get the standardized values like below."}}