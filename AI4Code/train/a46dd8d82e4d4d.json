{"cell_type":{"f522261b":"code","b8dd9b4b":"code","bda05887":"code","c90684c0":"code","cffffba3":"code","2850c52f":"code","fd301766":"code","aabebc03":"code","84c7efa9":"code","d8843009":"code","853323ab":"code","9f65502c":"code","f41388e0":"code","5fa0becb":"code","675e9ca0":"code","72b2496c":"code","88070c15":"code","86a66a85":"code","2c467476":"code","5fe3ab63":"code","85b452ff":"code","0fadb239":"code","a55997ef":"markdown","91a6376a":"markdown","6159940b":"markdown"},"source":{"f522261b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport sklearn\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8dd9b4b":"train_df = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv', index_col =  [0])\ntest_df = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv', index_col = [0])","bda05887":"print(train_df.shape)\nprint(test_df.shape)","c90684c0":"train_df.describe()","cffffba3":"numerical_feature = [feature for feature in train_df.columns if train_df[feature].dtypes != 'O']\ntrain_df[numerical_feature].head()","2850c52f":"categorical_features=[]\nfor c in train_df.columns:\n    col_type = train_df[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        train_df[c] = train_df[c].astype('category')\n        categorical_features.append(c)\nprint (categorical_features)","fd301766":"lbls = {}\nfor col in train_df.select_dtypes(exclude = np.number).columns.tolist():\n    le = LabelEncoder().fit(pd.concat([train_df[col].astype(str),test_df[col].astype(str)]))   \n    train_df[col] = le.transform(train_df[col].astype(str))\n    test_df[col] = le.transform(test_df[col].astype(str))\n    lbls[col] = le\nprint('Categorical columns:', list(lbls.keys()))","aabebc03":"missing_count = train_df.isna().sum()\nmissing_df = (pd.concat([missing_count.rename('Missing count'),\n                     missing_count.div(len(train_df))\n                          .rename('Missing ratio')],axis = 1)\n             .loc[missing_count.ne(0)])\nmissing_df.sort_values(by = 'Missing ratio', ascending = False)","84c7efa9":"fig, ax =plt.subplots(1,2)\n\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(3,4))\nsns.set_context(\"paper\", font_scale=1.2)                                                  \nsns.countplot('diabetes_mellitus',data=train_df, ax=ax[0])\ntrain_df['diabetes_mellitus'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.2f%%',ax=ax[1])\nfig.show()","d8843009":"sns.displot(data = train_df, kind = 'hist', x = 'age', hue = 'diabetes_mellitus', multiple = 'stack',bins=25,height = 4, aspect = 1.7)","853323ab":"sns.displot(data = train_df, kind = 'hist', x = 'd1_glucose_max', hue = 'diabetes_mellitus', multiple = 'stack',bins=25,height = 4, aspect = 1.7)","9f65502c":"sns.displot(data = train_df, kind = 'hist', x = 'icu_id', hue = 'diabetes_mellitus', multiple = 'stack',bins=25,height = 4, aspect = 1.7)","f41388e0":"sns.displot(data = train_df, kind = 'hist', x = 'bmi', hue = 'diabetes_mellitus', multiple = 'stack',bins=25,height = 4, aspect = 1.7)\n","5fa0becb":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(3,4))\nsns.set_context(\"paper\", font_scale=1.2)   \nsns.catplot(x=\"ethnicity\", y=\"bmi\", hue=\"diabetes_mellitus\", data=train_df, kind=\"bar\",height = 4, aspect = 2.3)","675e9ca0":"plt.style.use('fivethirtyeight')\nsns.set_context(\"paper\", font_scale=1.1)   \ngcbest = [\"#0099ff\", \"#ff4d4d\"]\nsns.set_palette(gcbest,20)\nsns.catplot(x=\"icu_type\", y=\"diabetes_mellitus\", data=train_df, kind=\"bar\",height = 4, aspect = 2.3)","72b2496c":"train_df['comorbidity_score'] = train_df['aids'].values * 23 + train_df['cirrhosis'] * 4  + train_df['hepatic_failure'] * 16 + train_df['immunosuppression'] * 10 + train_df['leukemia'] * 10 + train_df['lymphoma'] * 13 + train_df['solid_tumor_with_metastasis'] * 11\ntest_df['comorbidity_score'] = test_df['aids'].values * 23 + test_df['cirrhosis'] * 4  + test_df['hepatic_failure'] * 16 + test_df['immunosuppression'] * 10 + test_df['leukemia'] * 10 + test_df['lymphoma'] * 13 + test_df['solid_tumor_with_metastasis'] * 11\ntrain_df['comorbidity_score'] = train_df['comorbidity_score'].fillna(0)\ntest_df['comorbidity_score'] = test_df['comorbidity_score'].fillna(0)\n\ntrain_df['gcs_sum'] = train_df['gcs_eyes_apache']+train_df['gcs_motor_apache']+train_df['gcs_verbal_apache']\ntest_df['gcs_sum'] = test_df['gcs_eyes_apache']+test_df['gcs_motor_apache']+test_df['gcs_verbal_apache']\ntrain_df['gcs_sum'] = train_df['gcs_sum'].fillna(0)\ntest_df['gcs_sum'] = test_df['gcs_sum'].fillna(0)\n\ntrain_df['abmi'] = train_df['age']\/train_df['bmi']\ntrain_df['agi'] = train_df['weight']\/train_df['age']\ntest_df['abmi'] = test_df['age']\/test_df['bmi']\ntest_df['agi'] = test_df['weight']\/test_df['age']","88070c15":"missing_feature = missing_df[missing_df['Missing ratio'] > 0.6].index\nmissing_feature","86a66a85":"train_df.columns","2c467476":"#train_df = train_df.drop(labels = missing_feature, axis = 1)\n#test_df = test_df.drop(labels = missing_feature, axis= 1)\n#print(train_df.shape)\n#print(test_df.shape)","5fe3ab63":"X = train_df.drop([\"diabetes_mellitus\"],axis=1)\ny = train_df[\"diabetes_mellitus\"]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False, stratify=y)","85b452ff":"from sklearn.base import clone\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization\n\nclass BayesianSearchCV:\n    '''\n    Bayesian Search with cross validation score.\n    \n    Arguments:\n    \n    base_estimator: sklearn-like model\n    param_bounds: dict\n        hyperparameter upper and lower bounds\n        example: {\n            'param1': [0, 10],\n            'param2': [-1, 2],\n        }\n    scoring: string or callable\n        scoring argument for cross_val_score\n    cv: int\n        number of folds\n    n_iter: int\n        number of bayesian optimization iterations\n    init_points: int\n        number of random iterations before bayesian optimization\n    random_state: int\n        random_state for bayesian optimization\n    int_parameters: list\n        list of parameters which are required to be of integer type\n        example: ['param1', 'param3']\n    '''\n    \n    def __init__(\n        self,\n        base_estimator,\n        param_bounds,\n        scoring,\n        cv=5,\n        n_iter=50,\n        init_points=10,\n        random_state=1,\n        int_parameters=[],\n    ):\n        self.base_estimator = base_estimator\n        self.param_bounds = param_bounds\n        self.cv = cv\n        self.n_iter = n_iter\n        self.init_points = init_points\n        self.scoring = scoring\n        self.random_state = random_state\n        self.int_parameters = int_parameters\n    \n    def objective(self, **params):\n        '''\n        We will aim to maximize this function\n        '''\n        # Turn some parameters into ints\n        for key in self.int_parameters:\n            if key in params:\n                params[key] = int(params[key])\n        # Set hyperparameters\n        self.base_estimator.set_params(**params)\n        # Calculate the cross validation score\n        cv_scores = cross_val_score(\n            self.base_estimator,\n            self.X_data,\n            self.y_data,\n            cv=self.cv,\n            scoring=self.scoring)\n        score = cv_scores.mean()\n        return score\n    \n    def fit(self, X, y):\n        self.X_data = X\n        self.y_data = y\n        \n        # Create the optimizer\n        self.optimizer = BayesianOptimization(\n            f=self.objective,\n            pbounds=self.param_bounds,\n            random_state=self.random_state,\n        )\n        \n        # The optimization itself goes here:\n        self.optimizer.maximize(\n            init_points=self.init_points,\n            n_iter=self.n_iter,\n        )\n        \n        del self.X_data\n        del self.y_data\n        \n        # Save best score and best model\n        self.best_score_ = self.optimizer.max['target']\n        self.best_params_ = self.optimizer.max['params']\n        for key in self.int_parameters:\n            if key in self.best_params_:\n                self.best_params_[key] = int(self.best_params_[key])\n        \n        self.best_estimator_ = clone(self.base_estimator)\n        self.best_estimator_.set_params(**self.best_params_)\n        self.best_estimator_.fit(X, y)\n        \n        return self\n    \n    def predict(self, X):\n        return self.best_estimator_.predict(X)\n    \n    def predict_proba(self, X):\n        return self.best_estimator_.predict_proba(X)","0fadb239":"model = lgb.LGBMClassifier()\n\n# Set only upper and lower bounds for each parameter\nparam_grid = {\n    'learning_rate': (0.001, 0.1),\n    'n_estimators': (10, 1000),\n    'num_leaves': (1, 300),\n    'reg_alpha': (0, 1),\n    'reg_lambda': (0, 10),\n    'min_split_gain': (0, 1),\n    'feature_fraction': (0.1, 0.9),\n    'bagging_fraction': (0.8, 1),\n    'min_child_weight': (5, 50),\n    'lambda_l2': (0, 3),\n    'min_split_gain': (0.0001, 0.1),\n\n    \n}\n\nmodel_bayes = BayesianSearchCV(\n    model, param_grid, cv=6, n_iter=50, scoring='accuracy',\n    int_parameters=['n_estimators', 'num_leaves'])\n\nmodel_bayes.fit(X, y)","a55997ef":"[](http:\/\/)","91a6376a":"### The distibution of BMI in diabetes patiend is slightly more skewed to the right (higher BMI scores)","6159940b":"### The higher the glucose_max the higher precented of patients with diabetes (especially >250 range)"}}