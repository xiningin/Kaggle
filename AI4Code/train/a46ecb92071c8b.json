{"cell_type":{"0a5f865b":"code","936173a0":"code","feeb9ab7":"code","59c13421":"code","318ee0ff":"code","f2dac0bd":"code","978c4857":"code","ba4f7641":"code","8475a316":"code","d74f88a9":"code","22372ec3":"code","40e4992b":"code","5f0d31d9":"code","d336cfef":"code","107b19cc":"code","9d2d9749":"code","9e1175b2":"code","68d4fe1a":"code","f6eeb9c1":"code","82a66430":"code","60949f67":"code","c91f0747":"code","7b1e6403":"code","701661d7":"code","261a644b":"code","1e623084":"code","d4fd8f28":"code","f6639b09":"code","bfeaf221":"code","2a4c392c":"code","501cbd6f":"code","0468f8ff":"code","317b78bb":"code","7c169ea2":"code","d432de91":"code","bb711a9a":"code","69ee6b88":"code","a0c8c2aa":"code","721cd748":"code","e91aea65":"code","9fa1716d":"markdown","2284bc40":"markdown","40f25eed":"markdown","dcb401d1":"markdown","c1376d08":"markdown","566083f7":"markdown","f8fd4235":"markdown","6eb40085":"markdown","424a16fe":"markdown","f11358f2":"markdown"},"source":{"0a5f865b":"import numpy as np\nimport pandas as pd\nimport math\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV ,RepeatedKFold\nfrom sklearn.model_selection import train_test_split,cross_val_score, KFold \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom sklearn.metrics import  r2_score \n#from sklearn.metrics import classification_report, confusion_matrix\nimport sklearn.metrics as sklm\nimport matplotlib.pyplot as plt\nimport seaborn as sns","936173a0":"data_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntrain= data_train.copy()\ntest = data_test.copy()\n","feeb9ab7":"train.head(5)\n\n","59c13421":"train.describe()","318ee0ff":"train.info()","f2dac0bd":"test.info()","978c4857":"# columns int \"train\" set have more than 80% of nan values\n\nnan_values_train = train.isnull().sum()\npourcentage_train = pd.DataFrame((np.array(nan_values_train)\/1459)*100 ,index=nan_values_train.index)\npourcentage_train[pourcentage_train[0]>80]","ba4f7641":"# columns int \"test\" set have more than 80% of nan values\n\nnan_values_test = test.isnull().sum()\npourcentage_test = pd.DataFrame((np.array(nan_values_test)\/1459)*100 ,index=nan_values_test.index)\npourcentage_test[pourcentage_test[0]>80]","8475a316":"train = train.drop('Id', axis=1)\ntest = test.drop('Id', axis=1)","d74f88a9":"train = train.drop(['Alley','PoolQC','Fence','MiscFeature'], axis=1)\n\ntest = test.drop(['Alley','PoolQC','Fence','MiscFeature'], axis=1)\n","22372ec3":"# filling the nan for tain set\nmissing_value_train = train.isnull().sum()\n\nfor col in list(missing_value_train.index):\n    if train[col].dtype == 'object':\n        train[col].fillna(train[col].value_counts().index[0], inplace=True)\n    else:\n        train[col].fillna(train[col].mean(), inplace=True)\n        \n# filling the nan for test set        \nmissing_value_test = test.isnull().sum()\n\nfor col in list(missing_value_test.index):\n    if test[col].dtype == 'object':\n        test[col].fillna(test[col].value_counts().index[0], inplace=True)\n    else:\n        test[col].fillna(test[col].mean(), inplace=True)\n","40e4992b":"x = pd.DataFrame(train.drop(\"SalePrice\",axis=1))\ny = train.iloc[:,-1]","5f0d31d9":"columns=['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n       'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'RoofStyle',\n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'MasVnrArea',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n       'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating', 'HeatingQC',\n       'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd',\n       'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType', 'GarageYrBlt',\n       'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual', 'GarageCond',\n       'PavedDrive', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n       'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition']","d336cfef":"labelencoder = LabelEncoder()\nfor colum in columns:\n    if train[colum].dtype == 'object':\n        labelencoder.fit(train[colum])\n        x[colum]=labelencoder.transform(x[colum])\n        test[colum]=labelencoder.transform(test[colum])\n        \n        \n    ","107b19cc":"print(\"x: \\n{}\".format(x.dtypes.value_counts()))\n","9d2d9749":"\ndf =x\ndf[\"SalePrice\"]=y\n","9e1175b2":"#get correlations of each features in dataset\n\ncorrmat = df.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(df[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","68d4fe1a":"\nfeature_selection=pd.DataFrame( corrmat[\"SalePrice\"].sort_values(ascending=False))\nfeature_selected=feature_selection.iloc[1:16]\nfeature_selected.plot(kind='barh')","f6eeb9c1":"feature_selected.index","82a66430":"x = x[['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd','MasVnrArea','GarageYrBlt','Fireplaces','BsmtFinSF1']]\ntest = test[['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath','TotRmsAbvGrd','YearBuilt','YearRemodAdd','MasVnrArea','GarageYrBlt','Fireplaces','BsmtFinSF1']]\n\n","60949f67":"\nprint(y.shape)\nprint(x.shape)\nX=x","c91f0747":" #Splitting the dataset into the Training set and Test set\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n","7b1e6403":"# Feature Scaling\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n","701661d7":"test = sc.transform(test)","261a644b":"\n#createing Root Mean Squared Logarithmic Error (RMSLE) function\n\ndef rmsle (y_test, y_pred):\n    return round(np.sqrt(sklm.mean_squared_error(y_test, y_pred)),10)","1e623084":"#set the KFold function\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n","d4fd8f28":"\n# define model\nmodel = Ridge()\n# define model evaluation method\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define grid\ngrid = dict()\ngrid['alpha'] = np.arange(0, 1, 0.01)\n# define search\nsearch = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# perform the search\nresults = search.fit(X_train, y_train)\n# summarize\nprint('MAE: %.3f' % results.best_score_)\nprint('Config: %s' % results.best_params_)","f6639b09":"\n## set Ridge Regulariztion (L2)\n## Find Alpha\n\nridge = Ridge()\nparam = {'alpha': [a for a in range(50, 70)]}\n\nridge_reg = GridSearchCV(ridge, param_grid=param, scoring='neg_mean_squared_error'\n                     , cv=10)\n\nridge_reg.fit(X_train, y_train)\nprint(f\"The best value in Alpha: {ridge_reg.best_params_}\")\nprint(f\"The best score: {math.sqrt(-ridge_reg.best_score_)}\")","bfeaf221":"ridge_mod = Ridge(alpha=52)\nridge_mod.fit(X_train, y_train)\nridge_mod_train = ridge_mod.predict(X_train)\nridge_mod_test = ridge_mod.predict(X_test)\n\nprint(f'Root Mean Square Error train =  {rmsle(y_train, ridge_mod_train)}')\nprint(f'Root Mean Square Error test =  {rmsle(y_test, ridge_mod_test)}')   \n\nMSEs = cross_val_score(ridge_mod, X, y, \n                       scoring='neg_mean_squared_error', \n                       cv=kfolds)\n\nfor i,j in enumerate(MSEs):\n    j= math.sqrt(np.mean(-j))\n    print(f'Fold {i}: {round(j,4)}')\n    \nprint(f'Mean RMSE in Ridge: {round(math.sqrt(np.mean(-MSEs)),10)}')","2a4c392c":"from sklearn.linear_model import Lasso\n\nlasso = Lasso()\nparams = {'alpha': [0.1, 1, 10]}\n\nlasso_reg = GridSearchCV(lasso, param_grid=param, cv=kfolds, scoring='neg_mean_squared_error')\n\nlasso_reg.fit(X_train, y_train)\nprint(f'The best value of lasso: {lasso_reg.best_params_}')","501cbd6f":"\nlasso_mod = Lasso(alpha=69)\nlasso_mod.fit(X_train, y_train)\nlasso_mod_train = lasso_mod.predict(X_train)\nlasso_mod_test = lasso_mod.predict(X_test)\n\nprint(f'R2 Score  train =  {str(r2_score(y_train, lasso_mod_train))}')\nprint(f'R2 Score test = {r2_score(y_test, lasso_mod_test)}')\n\nLasso_CV = Lasso(alpha=1)\nMSEs = cross_val_score(lasso_mod, X, y, scoring='neg_mean_squared_error', cv=kfolds)\n\nfor i,j in enumerate(MSEs):\n    j= math.sqrt(np.mean(-j))\n    print(f'Fold {i}: {round(j,4)}')\n\nprint(f'Mean Lasso: {round(math.sqrt(np.mean(-MSEs)),10)}')","0468f8ff":"## Set RandomForest model\nfrom sklearn.ensemble import RandomForestRegressor\nrandom_forest = RandomForestRegressor(n_estimators=2200,\n                                      max_depth=13,\n                                      min_samples_split=5,\n                                      min_samples_leaf=5,\n                                      max_features=None,\n                                      random_state=42,\n                                      oob_score=True\n                                     )\n\nrandom_for = random_forest.fit(X_train, y_train)\nrandom_for_mod = random_for.predict(X_test)\n\nprint(f'R2 Score test = {r2_score(y_test, random_for_mod)}')\nprint(f'Root Mean Square Error test = {rmsle(y_test, random_for_mod)}')","317b78bb":"## Set XGBRegressor model\nfrom xgboost import XGBRegressor\nxgb_regress = XGBRegressor(learning_rate=0.01,\n                         n_estimators=3600,\n                         max_depth=4, min_child_weight=1,\n                         gamma=0.6, scale_pos_weight=1, \n                         seed=27, reg_alpha=0.00006 )\n\nxg_mod = xgb_regress.fit(X_train, y_train)\nxg_pred = xg_mod.predict(X_test)\n\nprint(f'R2 Score test =  {r2_score (y_test, xg_pred)}') \nprint(f'Root Mean Square Error test = {rmsle(y_test, xg_pred)}')","7c169ea2":"from sklearn.ensemble import GradientBoostingRegressor\nlr_list = [0.001 ,0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\nfor learning_rate in lr_list :\n    gb_r = GradientBoostingRegressor(n_estimators=1500, learning_rate=learning_rate, max_features=2, max_depth=2, random_state=0)\n    gb_r.fit(X_train, y_train)\n    stacking_pred = gb_r.predict(X_test)\n    print(\"Learning rate: \", learning_rate)\n    print(f'R2 Score test =  {r2_score(y_test, stacking_pred)}')\n    print(f'Root Mean Square Error test = {rmsle(y_test, stacking_pred)}')\n    ","d432de91":"gb_r = GradientBoostingRegressor(n_estimators=2200, learning_rate= 0.1, max_features=2, max_depth=2, random_state=0)\ngb_r.fit(X_train, y_train)\nstacking_pred = gb_r.predict(X_test)\nprint(\"Learning rate: \", learning_rate)\nprint(f'R2 Score test =  {r2_score(y_test, stacking_pred)}')\nprint(f'Root Mean Square Error test = {rmsle(y_test, stacking_pred)}')","bb711a9a":"prdicted= xg_mod.predict(test)","69ee6b88":"\nsubmission = pd.DataFrame({\n    \"Id\": data_test[\"Id\"],\n    \"SalePrice\": prdicted\n    })","a0c8c2aa":"submission","721cd748":"submission.to_csv(\"submission_test_set.csv\", index=False)\nsubmission.head()","e91aea65":"#comapre between pridcted result and data result\nsale_price_compare = pd.DataFrame({\n    \"train sale price desc\": train.describe().iloc[:, -1],\n    \"test sale price desc\": submission.describe().iloc[:, -1]\n    },index=['count','mean','std','min','25%','50%','75%','max' ])\nsale_price_compare","9fa1716d":"Based on  **Root Mean Square Error test** we choose  **XGBRegressor model** \nlowest Root Mean Square Error test by 26581.0609996912","2284bc40":"for other columns, we will change by the most **frequent** if the type of data **object** and with the **mean** if the type of data does **not object (int float...)**","40f25eed":"Based on Pareto law  we will select 20% of the feature \nThe Pareto principle also called the Pareto law, the 80-20 principle or the 80-20 law, is an empirical phenomenon observed in certain fields: approximately 80% of the effects are the product of 20% of the causes\n in ours case we have 75 features if we take 20% thats make about 15 featue higher collorated ones \n\n\n\n  check for more about. [Pareto law](https:\/\/en.wikipedia.org\/wiki\/Pareto_principle)","dcb401d1":"**Cleaning the data**\n\nAfter exloring the data we will drop \"Id\" ,we found that **Alley** **PoolQC** **Fence** **MiscFeature** have more than 80% of data is NaN so the feature is not reliable we gonna exclude them","c1376d08":"**Importing Data**","566083f7":"# House Prices - Advanced Regression Techniques\nData fields\nHere's a brief version of what you'll find in the data description file.\n\nSalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.<br>\nMSSubClass: The building class<br>\nMSZoning: The general zoning classification<br>\nLotFrontage: Linear feet of street connected to property<br>\nLotArea: Lot size in square feet<br>\nStreet: Type of road access<br>\nAlley: Type of alley access<br>\nLotShape: General shape of property<br>\nLandContour: Flatness of the property<br>\nUtilities: Type of utilities available<br>\nLotConfig: Lot configuration<br>\nLandSlope: Slope of property<br>\nNeighborhood: Physical locations within Ames city limits<br>\nCondition1: Proximity to main road or railroad<br>\nCondition2: Proximity to main road or railroad (if a second is present)<br>\nBldgType: Type of dwelling<br>\nHouseStyle: Style of dwelling<br>\nOverallQual: Overall material and finish quality<br>\nOverallCond: Overall condition rating<br>\nYearBuilt: Original construction date<br>\nYearRemodAdd: Remodel date<br>\nRoofStyle: Type of roof<br>\nRoofMatl: Roof material<br>\nExterior1st: Exterior covering on house<br>\nExterior2nd: Exterior covering on house (if more than one material)<br>\nMasVnrType: Masonry veneer type<br>\nMasVnrArea: Masonry veneer area in square feet<br>\nExterQual: Exterior material quality<br>\nExterCond: Present condition of the material on the exterior<br>\nFoundation: Type of foundation<br>\nBsmtQual: Height of the basement<br>\nBsmtCond: General condition of the basement<br>\nBsmtExposure: Walkout or garden level basement walls<br>\nBsmtFinType1: Quality of basement finished area<br>\nBsmtFinSF1: Type 1 finished square feet<br>\nBsmtFinType2: Quality of second finished area (if present)<br>\nBsmtFinSF2: Type 2 finished square feet<br>\nBsmtUnfSF: Unfinished square feet of basement area<br>\nTotalBsmtSF: Total square feet of basement area<br>\nHeating: Type of heating<br>\nHeatingQC: Heating quality and condition<br>\nCentralAir: Central air conditioning<br>\nElectrical: Electrical system<br>\n1stFlrSF: First Floor square feet<br>\n2ndFlrSF: Second floor square feet<br>\nLowQualFinSF: Low quality finished square feet (all floors)<br>\nGrLivArea: Above grade (ground) living area square feet<br>\nBsmtFullBath: Basement full bathrooms<br>\nBsmtHalfBath: Basement half bathrooms<br>\nFullBath: Full bathrooms above grade<br>\nHalfBath: Half baths above grade<br>\nBedroom: Number of bedrooms above basement level<br>\nKitchen: Number of kitchens<br>\nKitchenQual: Kitchen quality<br>\nTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)<br>\nFunctional: Home functionality rating<br>\nFireplaces: Number of fireplaces<br>\nFireplaceQu: Fireplace quality<br>\nGarageType: Garage location<br>\nGarageYrBlt: Year garage was built<br>\nGarageFinish: Interior finish of the garage<br>\nGarageCars: Size of garage in car capacity<br>\nGarageArea: Size of garage in square feet<br>\nGarageQual: Garage quality<br>\nGarageCond: Garage condition<br>\nPavedDrive: Paved driveway<br>\nWoodDeckSF: Wood deck area in square feet<br>\nOpenPorchSF: Open porch area in square feet<br>\nEnclosedPorch: Enclosed porch area in square feet<br>\n3SsnPorch: Three season porch area in square feet<br>\nScreenPorch: Screen porch area in square feet<br>\nPoolArea: Pool area in square feet<br>\nPoolQC: Pool quality<br>\nFence: Fence quality<br>\nMiscFeature: Miscellaneous feature not covered in other categories<br>\nMiscVal: Value of miscellaneous feature<br>\nMoSold: Month Sold<br>\nYrSold: Year Sold<br>\nSaleType: Type of sale<br>\nSaleCondition: Condition of sale<br>","f8fd4235":"the \"info\"  function shows that there is a lot of missing data so as we calculate the percentage of the NaN value ","6eb40085":"**Importing Libraries**","424a16fe":"**> Finging the alpha parameter fo Ridge **","f11358f2":"**Exploring the data**"}}