{"cell_type":{"d006d57c":"code","8d29c4aa":"code","89b30026":"code","85c366af":"code","cafee502":"code","db3ae531":"code","9d239b67":"code","eeddc786":"code","561c1cf0":"code","06c68829":"code","22ff61e5":"code","a5c613dd":"code","6e24c33c":"code","bdf73638":"code","8911339a":"code","c61bcb50":"code","810e5754":"code","96f99227":"code","7bb7b8aa":"code","61464475":"code","13467b81":"code","9ca86101":"code","c8e3968e":"markdown","c3e6e957":"markdown","8a64a50b":"markdown","fbfb9735":"markdown","9697ffff":"markdown","39c9b620":"markdown","02e42391":"markdown","a5d89e3b":"markdown","f56b46e1":"markdown","1feddd17":"markdown","48c991c3":"markdown","8031aeee":"markdown","105c7f4c":"markdown","a93a8351":"markdown","2c557181":"markdown","b139bcf4":"markdown","4658d228":"markdown","aecef3c1":"markdown","bb61ca9a":"markdown","7356a5d9":"markdown","64a2cc78":"markdown","aee4ddb9":"markdown"},"source":{"d006d57c":"# Data Manipulation and Linear Algebra\nimport pandas as pd\nimport numpy as np\n\n# Plots\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore')","8d29c4aa":"df = pd.read_csv(\"..\/input\/churn-for-bank-customers\/churn.csv\")\ndf","89b30026":"drop_cols = [\"RowNumber\", \"CustomerId\", \"Surname\"]\ndf.drop(columns=drop_cols, inplace=True)","85c366af":"df.info() # Luckiley we dont have any null values","cafee502":"df.describe()","db3ae531":"plt.figure(dpi=80, figsize=(10, 8))\nsns.heatmap(df.corr(), annot=True, cmap=\"YlGnBu\") # Not many correlations between features\nplt.show()","9d239b67":"plt.figure(figsize=(15, 10), dpi=80)\n\nplt.subplot(221)\nsns.histplot(df[\"Age\"], kde=True) # Age is Right Skewed and most of the people are around 30 - 40 years old\n\nplt.subplot(222)\nsns.histplot(df[\"EstimatedSalary\"], kde=True) # Salary is Well Distributed\n\nplt.subplot(223)\nsns.histplot(df[\"CreditScore\"], kde=True) # Credit Score is Left Skewed and most people have 600 - 700 Credit Score\n\nplt.subplot(224)\nsns.histplot(df[\"Balance\"], kde=True) # Balance Data is also well distributed if 0 is ignored","eeddc786":"plt.figure(figsize=(10, 6))\n\nplt.subplot(121)\nsns.boxplot(x=\"IsActiveMember\", y=\"Age\", data=df)\nplt.title(\"IsActiveMember vs Age\", fontdict={\"fontsize\": 14, \"fontweight\":500})\n\nplt.subplot(122)\nsns.boxplot(x=\"Exited\", y=\"Age\", data=df)\nplt.title(\"Exited vs Age\", fontdict={\"fontsize\": 14, \"fontweight\":500})\n\n# People of Higher age i.e., 40 years and above tend to leave the bank more","561c1cf0":"plt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nsns.boxplot(x=\"Exited\", y=\"Balance\", data=df)\nplt.title(\"Exited vs Balance\", fontdict={\"fontsize\": 14, \"fontweight\":500})\n# If we observe Median value people with Higher balance tend to leave bank\n\nplt.subplot(122)\nsns.violinplot(x=\"Exited\", y=\"Balance\", hue=\"Gender\", data=df)\nplt.title(\"Exited vs Balance\", fontdict={\"fontsize\": 14, \"fontweight\":500})","06c68829":"plt.figure(figsize=(10, 6))\nsns.barplot(x=\"NumOfProducts\", y=\"Balance\", data=df)\nplt.title(\"NumOfProducts vs Balance\", fontdict={\"fontsize\": 14, \"fontweight\":500})\n# Most of the people have 1 or 4 products","22ff61e5":"sns.pairplot(df, hue=\"Gender\")\nplt.show()","a5c613dd":"df = pd.get_dummies(df)\ndf.reset_index(drop=True, inplace=True)\ndf","6e24c33c":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df['Exited']):\n    train = df.loc[train_index]\n    test = df.loc[test_index]","bdf73638":"X_train = train.drop(\"Exited\", axis=1)\ny_train = train[\"Exited\"]\n\nX_test = test.drop(\"Exited\", axis=1)\ny_test = test[\"Exited\"]","8911339a":"cols_to_scale = [\"CreditScore\", \"Age\", \"Balance\", \"EstimatedSalary\"]\n\nsc = StandardScaler()\n\nX_train[cols_to_scale] = sc.fit_transform(X_train[cols_to_scale])\nX_test[cols_to_scale] = sc.transform(X_test[cols_to_scale])","c61bcb50":"# Dataframe to store all the accuracy scores for Comparison and Analysis\nMLA_compare = pd.DataFrame()\n\ndef MLA_testing(MLA, X_train, X_test):\n    row_index = 0\n    for classifier in MLA:\n        classifier.fit(X_train, y_train)\n\n        y_pred = classifier.predict(X_test)\n        classifier_accuracy_score = accuracy_score(y_test, y_pred)\n\n        kfold_accuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n\n        MLA_name = classifier.__class__.__name__\n        MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n        MLA_compare.loc[row_index, 'Accuracy Score'] = classifier_accuracy_score*100\n        MLA_compare.loc[row_index, 'K-Fold Accuracy'] = kfold_accuracy.mean()*100\n\n        print(MLA_name, \"Done\")\n        row_index+=1","810e5754":"MLA = [\n    #Ensemble Methods\n    ensemble.AdaBoostClassifier(),\n    ensemble.BaggingClassifier(),\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    linear_model.RidgeClassifierCV(),\n    linear_model.SGDClassifier(),\n    \n    #Navies Bayes\n    naive_bayes.BernoulliNB(),\n    naive_bayes.GaussianNB(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    svm.LinearSVC(),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n    tree.ExtraTreeClassifier(),\n    \n    #Discriminant Analysis\n    discriminant_analysis.LinearDiscriminantAnalysis(),\n\n    XGBClassifier(eval_metric=\"logloss\", use_label_encoder=False),\n    CatBoostClassifier(silent=True)  \n    ]\n\nMLA_testing(MLA=MLA, X_train=X_train, X_test=X_test)","96f99227":"MLA_compare = MLA_compare.sort_values(by=\"K-Fold Accuracy\", ascending=False).reset_index(drop=True)\nMLA_compare","7bb7b8aa":"def MLA_testing(MLA, X_train, X_test, y_train, y_test):      \n    # Training The Model\n    MLA.fit(X_train, y_train)\n\n    # KFold Accuracies on Training Data\n    kfold_accuracy = cross_val_score(estimator = MLA, X = X_train, y = y_train, cv = 10, n_jobs=-1)\n    print(\"K-Fold Accuracies:\\n\", kfold_accuracy, \"\\n\")\n    \n    # Prediction on Testing Data\n    y_pred = cross_val_predict(estimator = MLA, X = X_test, y = y_test, cv = 10, n_jobs=-1)\n    \n    # Accuracy for y_test and y_pred\n    classifier_accuracy_score = accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score:\\n\", classifier_accuracy_score, \"\\n\")\n    \n    # Confusion Matrix\n    conf_mtx = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\\n\", conf_mtx, \"\\n\")\n    \n    # Classification Report\n    class_rep = classification_report(y_test, y_pred)\n    print(\"Classification Report:\\n\", class_rep, \"\\n\")\n    \n    try:\n        # Precision - Recall Curve\n        yhat = MLA.predict_proba(X_test)\n        precision, recall, _ = precision_recall_curve(y_test, yhat[:, 1])\n        \n        plt.figure(dpi=100, figsize=(15, 6))\n        plt.subplot(121)\n        sns.lineplot([0, 1], [1, 0], linestyle='--', label='No Skill')\n        sns.lineplot(recall, precision, marker='.', label=MLA.__class__.__name__)\n        plt.title(\"Recall vs Precision Curve\")\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.legend()\n        \n        # ROC Curve\n        plt.subplot(122)\n        sns.lineplot([0, 1], [0, 1], linestyle='--', label='No Skill')\n        fpr, tpr, _ = roc_curve(y_test, yhat[:, 1])\n        sns.lineplot(fpr, tpr, marker='.', label=MLA.__class__.__name__)\n        plt.title(\"ROC Curve\")\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.legend()\n        \n        plt.show()\n    except:\n        pass\n    \n    # Important Features for The Algorithms\n    imp_cols = pd.DataFrame()\n    imp_cols[\"Features\"] = X_train.columns\n    imp_cols[\"Importance\"] = MLA.feature_importances_\n    imp_cols = imp_cols.sort_values(by=\"Importance\", ascending=False)\n    \n    plt.figure(dpi=80, figsize=(10, 8))\n    sns.barplot(y=\"Features\", x=\"Importance\", data=imp_cols)\n    plt.title(\"Importance of Features\")\n    plt.show()","61464475":"cat_clf = CatBoostClassifier(silent=True)  \n\nMLA_testing(cat_clf, X_train, X_test, y_train, y_test)","13467b81":"gb_clf = ensemble.GradientBoostingClassifier()\n\nMLA_testing(gb_clf, X_train, X_test, y_train, y_test)","9ca86101":"rf_clf = ensemble.RandomForestClassifier()\n\nMLA_testing(rf_clf, X_train, X_test, y_train, y_test)","c8e3968e":"## GradientBoostingClassifier","c3e6e957":"## Stratified Train Test Split\n### Evenly Spreading the Dependent Variable \"status\" in Train and Test set","8a64a50b":"# EDA","fbfb9735":"## Catboot Classifier","9697ffff":"## Feature Scaling","39c9b620":"## Top 10 Best Performing Models","02e42391":"## NumOfProducts vs Balance","a5d89e3b":"## Pair Plot","f56b46e1":"# Trying Some Best Performing Models","1feddd17":"# Comparing Models","48c991c3":"# Machine Learning - Multiple Model Testing","8031aeee":"## RandomForestClassifier","105c7f4c":"## OneHotEncoding Data","a93a8351":"# Import Necessary Libraries","2c557181":"# Data Preprocessing","b139bcf4":"## Balance vs Exited","4658d228":"## Distribution Plots","aecef3c1":"## Plotting Correlation Heatmap Before Encoding Categorical Variables","bb61ca9a":"## Dropping some Columns of Less Importance","7356a5d9":"# Preparing Data","64a2cc78":"## Getting the Data ","aee4ddb9":"## Age vs IsActiveMember, Exited"}}