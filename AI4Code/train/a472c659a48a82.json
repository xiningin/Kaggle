{"cell_type":{"eeaf61af":"code","8478b571":"code","7166ea2a":"code","a6625ffc":"code","53f5538e":"code","2515d020":"code","09d24a2d":"code","0eeeb1fe":"code","4f139c79":"code","776a8184":"code","da4d1983":"code","5c5b180a":"code","c3f63ffd":"code","e4a52de8":"code","4aea3d9c":"code","5ceb9b02":"code","9d11f017":"code","0f1d97cf":"code","c79dfd4b":"code","33bfad26":"code","b1b1ff3e":"code","ca84e44c":"code","2f077391":"code","638d8ae7":"code","d0b0ec3f":"code","29682397":"code","ab33cfb6":"code","8d0f9c7f":"code","87942c06":"code","f63709d1":"code","acb9b8c9":"code","dc8f0b62":"code","859943f5":"code","d67b07c7":"code","3d99202b":"code","c510bcde":"code","de3aefd7":"code","07541eed":"markdown","a38726a8":"markdown","c055220c":"markdown","afcd77bc":"markdown","e8177fa8":"markdown","9737ed0c":"markdown","cfcbbde4":"markdown","690f9396":"markdown"},"source":{"eeaf61af":"#importing the required libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom scipy.fft import fft,fftfreq\nfrom scipy import signal\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.optimizers import Adam","8478b571":"#loading the dataset using pandas\nx_train = pd.read_csv(\"..\/input\/power-quality-distribution-dataset-2\/VoltageL1Train.csv\")\ny_train = pd.read_csv(\"..\/input\/power-quality-distribution-dataset-2\/outputTrain.csv\")\nx_test = pd.read_csv(\"..\/input\/power-quality-distribution-dataset-2\/VoltageL1Test.csv\")\ny_test = pd.read_csv(\"..\/input\/power-quality-distribution-dataset-2\/outputTest.csv\")","7166ea2a":"print(\"x_train\",x_train.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"x_test\",x_test.shape)\nprint(\"y_test\",y_test.shape)","a6625ffc":"#dropna() function is used to remove all those rows which contains NA values\nx_train.dropna(axis=0,inplace=True)\ny_train.dropna(axis=0,inplace=True)\nx_test.dropna(axis=0,inplace=True)\ny_test.dropna(axis=0,inplace=True)","53f5538e":"#shape of the data frames after dropping the rows containing NA values\nprint(\"x_train\",x_train.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"x_test\",x_test.shape)\nprint(\"y_test\",y_test.shape)","2515d020":"#here we are constructing the array which will finally contain the column names\nheader =[]\nfor i in range(1,x_train.shape[1]+1):\n    header.append(\"Col\"+str(i))","09d24a2d":"#assigning the column name array to the respectinve dataframes\nx_train.columns = header\nx_test.columns = header","0eeeb1fe":"#assinging the column name for the y_train and y_test\nheader = [\"output\"]\ny_train.columns = header\ny_test.columns = header","4f139c79":"x_train.head()","776a8184":"x_test.head()","da4d1983":"y_train.head()","5c5b180a":"y_test.head()","c3f63ffd":"#further splitting the train dataset to train and validation\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20, random_state=42)","e4a52de8":"print('x_train',x_train.shape)\nprint('y_train',y_train.shape)\nprint('x_val',x_val.shape)\nprint('y_val',y_val.shape)\nprint('x_test',x_test.shape)\nprint('y_test',y_test.shape)","4aea3d9c":"# get_dummies function is used here to perform one hot encoding of the y_* numpy arrays\ny_train_hot = pd.get_dummies(y_train['output'])\ny_test_hot = pd.get_dummies(y_test['output'])\ny_val_hot = pd.get_dummies(y_val['output'])","5ceb9b02":"y_train_hot.head()","9d11f017":"y_train_arr = y_train_hot.to_numpy()\ny_test_arr = y_test_hot.to_numpy()\ny_val_arr = y_val_hot.to_numpy()\nprint(\"y_train:\",y_train_arr.shape)\nprint(\"y_test:\",y_test_arr.shape)\nprint(\"y_val:\",y_val_arr.shape)\nno_of_classes = y_train_arr.shape[1]","0f1d97cf":"x_train_tr = x_train.to_numpy()\nx_test_tr = x_test.to_numpy()\nx_val_tr = x_val.to_numpy()","c79dfd4b":"for i in range(0,x_train.shape[0]):\n    x_train_tr[i][:] = np.abs(fft(x_train_tr[i][:]))\n    \nfor i in range(0,x_test.shape[0]):\n    x_test_tr[i][:] = np.abs(fft(x_test_tr[i][:]))\n\nfor i in range(0,x_val.shape[0]):\n    x_val_tr[i][:] = np.abs(fft(x_val_tr[i][:]))","33bfad26":"transform = StandardScaler()\nx_train_tr = transform.fit_transform(x_train)\nx_test_tr = transform.fit_transform(x_test)\nx_val_tr = transform.fit_transform(x_val)","b1b1ff3e":"print(\"Training\",x_train_tr.shape)\nprint(y_train_arr.shape)\nprint(\"Validation\",x_val_tr.shape)\nprint(y_val_arr.shape)\nprint(\"Test\",x_test_tr.shape)\nprint(y_test_arr.shape)\nsampling_rate = x_train_tr.shape[1]","ca84e44c":"#Reshaping the Data so that it could be used in 1D CNN\nx_train_re = x_train_tr.reshape(x_train_tr.shape[0],x_train_tr.shape[1], 1)\nx_test_re = x_test_tr.reshape(x_test_tr.shape[0],x_test_tr.shape[1], 1)\nx_val_re = x_val_tr.reshape(x_val_tr.shape[0],x_val_tr.shape[1], 1)","2f077391":"x_train_re.shape","638d8ae7":"#importing required modules for working with CNN\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D\nfrom tensorflow.keras.layers import Convolution1D, ZeroPadding1D, MaxPooling1D, BatchNormalization, Activation, Dropout, Flatten, Dense\nfrom tensorflow.keras.regularizers import l2","d0b0ec3f":"#initializing required parameters for the model\nbatch_size = 64\nnum_classes = 6\nepochs = 20\ninput_shape=(x_train_tr.shape[1], 1)","29682397":"model = Sequential()\nmodel.add(Conv1D(128, kernel_size=3,padding = 'same',activation='relu', input_shape=input_shape))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=(2)))\nmodel.add(Conv1D(128,kernel_size=3,padding = 'same', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling1D(pool_size=(2)))\nmodel.add(Flatten())\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))","ab33cfb6":"model.summary()","8d0f9c7f":"#compiling the model\n\nlog_dir = \"logs2\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nmodel.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])","87942c06":"#training the model\nhistory = model.fit(x_train_re, y_train_hot, batch_size=batch_size, epochs=epochs, validation_data=(x_val_re, y_val_hot), callbacks=[tensorboard_callback])\n","f63709d1":"%load_ext tensorboard\n%tensorboard --logdir logs2\/fit","acb9b8c9":"print(model.metrics_names)","dc8f0b62":"print(\"min val:\",min(history.history['val_accuracy']))\nprint(\"avg val\",np.mean(history.history['val_accuracy']) )\nprint(\"max val:\",max(history.history['val_accuracy']))\nprint()\nprint(\"min train:\",min(history.history['accuracy']))\nprint(\"avg train\",np.mean(history.history['accuracy']) )\nprint(\"max train:\",max(history.history['accuracy']))","859943f5":"pred_acc = model.evaluate(x_test_re,y_test_hot)\nprint(\"Test accuracy is {}\".format(pred_acc))","d67b07c7":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn","3d99202b":"array = confusion_matrix(y_test_hot.to_numpy().argmax(axis=1), model.predict(x_test_re).argmax(axis=1))","c510bcde":"array","de3aefd7":"to_cm = pd.DataFrame(array, index = [i for i in [\"Type-1\",\"Type-2\",\"Type-3\",\"Type-4\",\"Type-5\",\"Type-6\"]],\n                  columns = [i for i in [\"Type-1\",\"Type-2\",\"Type-3\",\"Type-4\",\"Type-5\",\"Type-6\"]])\nplt.figure(figsize = (13,9))\nsn.heatmap(to_cm, annot=True)","07541eed":"The data transformation steps employed here are as follows:<br>\n\n1) Fourier Transform<br>\n2) Normalization","a38726a8":"## Data Preprocessing","c055220c":"## Model evaluation","afcd77bc":"# Power Quality Classification using CNN","e8177fa8":"This segment of notebook contains all the preprocessing steps which are performed on the data.","9737ed0c":"This notebook focusses on developing a Convolutional Neural Network which classifies a particular power signal into its respective power quality condition. The dataset used here contains signals which belong to one of the 6 classes(power quality condition). This means that each signal is characterized by 256 data points. Here the signals provided are in time domain.","cfcbbde4":"## Model creation and training","690f9396":"### Data transformation"}}