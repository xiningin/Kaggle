{"cell_type":{"21bc69e8":"code","72f7934e":"code","30d68352":"code","552acd40":"code","dfd32a7e":"code","ade9a656":"code","0e523ead":"markdown","ef4fa0d3":"markdown","65946443":"markdown","84fdeb11":"markdown","bf01a287":"markdown","99f68b68":"markdown"},"source":{"21bc69e8":"import pandas  as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom   sklearn.metrics import accuracy_score\nfrom   sklearn.metrics import f1_score\nfrom   sklearn.metrics import precision_score\nfrom   sklearn.metrics import recall_score\nfrom   sklearn.metrics import confusion_matrix\nfrom   sklearn.model_selection import train_test_split\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","72f7934e":"#===========================================================================\n# read in the dataset\n#===========================================================================\nfrom sklearn.datasets import load_breast_cancer\nX, y = load_breast_cancer(return_X_y=True)\n\n#===========================================================================\n# split the data into train and test\n#===========================================================================\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n#===========================================================================\n# perform the classification\n#===========================================================================\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators=10,max_features=1,random_state=42)\nclassifier.fit(X_train, y_train);","30d68352":"predictions = classifier.predict(X_test)\n\nprint(\"The precision score is: %.2f\" % precision_score( y_test, predictions))\nprint(\"The recall score is: %.2f\" % recall_score( y_test, predictions), \"\\n\")\nprint(\"Accuracy score is: %.2f\" % accuracy_score( y_test, predictions))\nprint(\"The F1 score is: %.2f\" % f1_score( y_test, predictions))\n\ncm = confusion_matrix( y_test , predictions )\nplt.figure(figsize = (3,3))\nsn.heatmap(cm, annot=True, annot_kws={\"size\": 25}, fmt=\"d\", cmap=\"viridis\", cbar=False)\nplt.show()","552acd40":"discrimination_threshold = 0.25\npredictions = classifier.predict_proba(X_test)\npredictions = (predictions[::,1] > discrimination_threshold )*1\n\nprint(\"The recall score is: %.2f\" % recall_score( y_test, predictions))\nprint(\"The precision score is: %.2f\" % precision_score( y_test, predictions),\"\\n\")\nprint(\"Accuracy score is: %.2f\" % accuracy_score( y_test, predictions))\nprint(\"The F1 score is: %.2f\" % f1_score( y_test, predictions))\n\ncm = confusion_matrix( y_test , predictions )\nplt.figure(figsize = (3,3))\nsn.heatmap(cm, annot=True, annot_kws={\"size\": 25}, fmt=\"d\", cmap=\"viridis\", cbar=False)\nplt.show()","dfd32a7e":"discrimination_threshold = 0.75\npredictions = classifier.predict_proba(X_test)\npredictions = (predictions[::,1] > discrimination_threshold )*1\n\nprint(\"The precision score is: %.2f\" % precision_score( y_test, predictions))\nprint(\"The recall score is: %.2f\" % recall_score( y_test, predictions), \"\\n\")\nprint(\"Accuracy score is: %.2f\" % accuracy_score( y_test, predictions))\nprint(\"The F1 score is: %.2f\" % f1_score( y_test, predictions))\n\ncm = confusion_matrix( y_test , predictions )\nplt.figure(figsize = (3,3))\nsn.heatmap(cm, annot=True, annot_kws={\"size\": 25}, fmt=\"d\", cmap=\"viridis\", cbar=False)\nplt.show()","ade9a656":"from yellowbrick.classifier import DiscriminationThreshold\nvisualizer = DiscriminationThreshold(classifier, size=(1000, 500))\n\nvisualizer.fit(X_train, y_train)\nvisualizer.show();","0e523ead":"# Conclusion\nBy varing the discrimination threshold we can reduce instances of either false positives or false negatives, depending on the priorities of the problem we are dealing with, although this may come at the cost of the overall performance of the classication, for example measured by the accuracy, or by the $F_1$ score.\n# Related reading\n* [Confusion matrix](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix)\n* [False positives and false negatives](https:\/\/en.wikipedia.org\/wiki\/False_positives_and_false_negatives)\n* [F1 score](https:\/\/en.wikipedia.org\/wiki\/F1_score)\n* [Receiver operating characteristic curve](https:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic)\n* [Precision and recall](https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall)\n\n# Links\n* ['Wisconsin' breast cancer dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+(Diagnostic)) \n* [Discrimination Threshold Visualizer](https:\/\/www.scikit-yb.org\/en\/latest\/api\/classifier\/threshold.html) from [Yellowbrick](https:\/\/www.scikit-yb.org\/en\/latest\/)\n* [sklearn.metrics.accuracy_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html): this routine is analogous to the one used by kaggle to calculate your score on the leaderboard when you submit your file.\n* [sklearn.metrics.confusion_matrix](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html): this routine will calculate our confusion matrix.\n* [sklearn.metrics.f1_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.f1_score.html) which returns the $F_1$ score.","ef4fa0d3":"we can now see that we now have 12 false negatives, but no false positives (A *precision* of 1.00). This time our accuracy score has dropped even further, down to 0.92.\n\n# 3. Discrimination threshold plot\nWe have looked at thresholds of `0.5` and as well as `0.25` and `0.75` by hand. However, thanks to the \n [Discrimination Threshold Visualizer](https:\/\/www.scikit-yb.org\/en\/latest\/api\/classifier\/threshold.html) from [Yellowbrick](https:\/\/www.scikit-yb.org\/en\/latest\/) we can also automatically examine the range of values between 0 and 1, and see the results in a plot:","65946443":"we see that we have 1 false positive, and 4 false negatives.\n### Discrimination threshold = 0.25","84fdeb11":"# False positives, false negatives and the discrimination threshold\n\n## Introduction\nOn kaggle our measure of success is usually our position on the leaderboard, which is calculated using the [accuracy score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html). This score gives a number indicating what ratio of predictions were correct: a score of 0 tells us that not one single classification was correct, and a score of 1 is a perfect classification. However, this is far from the whole story. \n\nOne hardly ever scores a 0 or a 1, and a *confusion matrix* is a tool designed to help us understand a little better how well our classifier is performing. The confusion matrix  goes into a little more detail; this time it provides us with four values:\n* The number of times our classifier produced **true negatives** (TN)\n* The number of times our classifier produced **true positives** (TP)\n* The number of times our classifier produced **false positives** (FP)\n* The number of times our classifier produced **false negatives** (FN)\n\nwhich scikit-learn returns in the following format, hence the name matrix (Note that there is no standard convention for arrangement of this matrix):\n\n![image.png](attachment:image.png)\n\nWe can see now that the *accuracy score* is actually given by $\\frac{(TN + TP)}{(TN + TP + FP +FN)}$, in other words, the true values divided by all the values. Another performance measure is the **$F_1$ score**, which is given by:\n\n$$ F_1 = 2\\frac{precision . recall}{precision + recall}$$\n<br>\nwhere the *precision* is given by $\\frac{TP}{TP + FP}$, and *recall* by $\\frac{TP}{TP + FN}$.\n\n\n# Discrimination threshold\nWhen using [scikit-learn](https:\/\/scikit-learn.org\/stable\/index.html) it is usual to implement the `predict()` method corresponding to our estimator of choice on the test set, here `X_test`, *i.e.*\n> `predictions = classifier.predict(X_test)`\n\nThis will return a 1-dimensional [numpy.ndarray](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.ndarray.html) composed of 0's and 1's which correspond to our predictions. These are derived from looking at the probability estimates from the classifier, which are floating point numbers between 0 and 1.\nIf the probability estimate is less than `0.5`, the classification is deemed to be a `0`, and above `0.5` a `1`. This `0.5` is the natural choice for the *discrimination threshold*.\nHowever, many estimators also come with a `predict_proba()` method too. This will return an array of the raw probability estimates. With this in hand we can now choose our own discrimination threshold, for example like this:\n\n> `discrimination_threshold = 0.25\n>  predictions = classifier.predict_proba(X_test)\n>  predictions = (predictions[::,1] > discrimination_threshold )*1`\n\nin which we slice the numpy array, use a comparison operator which creates a Boolean array composed of either `True` or `False`, and finally use the `*1` to convert the Boolean values onto `0`'s and `1`'s.\n\n# False positives and false negatives\nWhy would we want to change the discrimination threshold when `0.5` seems eminently reasonable? Well there are circumstances where particularly wish to avoid having either false positives or false negatives. Let us provide and example; when testing for COVID-19, a false positive may lead to an individual having to be quarantined for a period, which may be inconvenient. However, a false negative my result in an infected individual subsequently mixing with uninfected people, and inadvertently spreading the virus. In this case a false negative would have far more undesirable consequences compared to a false positive result, and we may wish to compensate for this in our classification.\nAnother situation when one wishes to prioritise between false negatives and false positives is in cancer screening, which is the example we shall look at here.\n\n\n# The data \nFor data we shall use the ['Wisconsin' breast cancer dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+&#40;Diagnostic&#41; ) which is one of the example datasets that can be found on  [scikit learn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.load_breast_cancer.html#sklearn.datasets.load_breast_cancer).\n# The model\nFor demonstration purposes the we shall perform a simple  [random forest classification](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html). The parameters used here are far from optimal as our plan is actually *not* to aim for a good classification, but rather demonstrate the changes produced by varying the discrimination threshold.","bf01a287":"# Example:\n### Discrimination threshold = 0.5\nFirst let us take a look at the confusion matrix obtained with the 'default' discrimination threshold of `0.5` of the `predict()` method:","99f68b68":"we can now see that we have 8 false positives, but this time no false negatives (A *recall* of 1.00). However, we can see that our accuracy score has dropped from 0.97 to 0.94.\n\n### Discrimination threshold = 0.75"}}