{"cell_type":{"d584d62b":"code","b464c9f0":"code","3c3ab65c":"code","adc7abd8":"code","f869889f":"code","5c5915e6":"code","1ad824ef":"code","dbf30f41":"code","4479f24a":"code","ae65acc1":"code","20fe6bd7":"code","0b10ddd4":"code","3c39c18d":"code","5919f0c7":"code","711b3690":"code","b57afdec":"code","a52207b5":"code","c2f333c9":"code","60020b93":"code","09f3dc16":"code","e631afe6":"code","e596994e":"code","580ababe":"code","4984c913":"code","3e870c86":"code","749d45bc":"code","4739ab64":"code","d5f5d463":"code","4bdb8e18":"code","031be8bc":"code","8d6b6d77":"code","4dfdfa0a":"code","c4183b0d":"code","71d4eea4":"code","3e5073d8":"code","0e9381f8":"code","d5db9b2f":"code","ccca4496":"code","488cf2a1":"code","9b3127a4":"code","ca01a5db":"code","c536e6d7":"code","dead534e":"code","c13d57d5":"code","a8435dd7":"code","75df78bc":"code","92d4a463":"code","87e81224":"code","3b3c070e":"markdown","b5d77447":"markdown","f805f0f2":"markdown","9b2fb859":"markdown","7ec2f8a8":"markdown","0f689f94":"markdown","bc6f69b0":"markdown","626410e6":"markdown","44aa6d39":"markdown"},"source":{"d584d62b":"import wave\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nfrom datetime import datetime\nfrom os import listdir\nfrom os.path import isfile, join\nimport librosa\nimport librosa.display\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\n%matplotlib inline","b464c9f0":"print(tf.__version__)","3c3ab65c":"df_no_diagnosis = pd.read_csv('..\/input\/respiratory-sound-database\/demographic_info.txt', names = \n                 ['Patient number', 'Age', 'Sex' , 'Adult BMI (kg\/m2)', 'Child Weight (kg)' , 'Child Height (cm)'], delimiter = ' ')\n\ndiagnosis = pd.read_csv('..\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/patient_diagnosis.csv', names = ['Patient number', 'Diagnosis'])","adc7abd8":"df =  df_no_diagnosis.join(diagnosis.set_index('Patient number'), on = 'Patient number', how = 'left')\ndf['Diagnosis'].value_counts()","f869889f":"df","5c5915e6":"root = '..\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/audio_and_txt_files\/'\nfilenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]","1ad824ef":"def Extract_Annotation_Data(file_name, root):\n    tokens = file_name.split('_')\n    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n    return (recording_info, recording_annotations)","dbf30f41":"i_list = []\nrec_annotations = []\nrec_annotations_dict = {}\nfor s in filenames:\n    (i,a) = Extract_Annotation_Data(s, root)\n    i_list.append(i)\n    rec_annotations.append(a)\n    rec_annotations_dict[s] = a\nrecording_info = pd.concat(i_list, axis = 0)\nrecording_info.tail()","4479f24a":"no_label_list = []\ncrack_list = []\nwheeze_list = []\nboth_sym_list = []\nfilename_list = []\nfor f in filenames:\n    d = rec_annotations_dict[f]\n    no_labels = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 0)].index)\n    n_crackles = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 0)].index)\n    n_wheezes = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 1)].index)\n    both_sym = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 1)].index)\n    no_label_list.append(no_labels)\n    crack_list.append(n_crackles)\n    wheeze_list.append(n_wheezes)\n    both_sym_list.append(both_sym)\n    filename_list.append(f)","ae65acc1":"file_label_df = pd.DataFrame(data = {'filename':filename_list})\nfile_label_df","20fe6bd7":"audio_path = '..\/input\/respiratory-sound-database\/Respiratory_Sound_Database\/Respiratory_Sound_Database\/audio_and_txt_files\/'","0b10ddd4":"# adding columns in file_label_df\ndiagnosis = []\npatient_number = []\nrecording_index = []\nchest_location = []\nacquisition_mode = []\nrecording_equipment = []\nsample_rate = []\nduration = []\n\nfor i in tqdm(range(len(file_label_df['filename']))):\n  info = file_label_df['filename'][i].split('_')\n  patient_id, recording_idx, chest_loc, acq_mode, equipment = info\n  #sound_filename = audio_path + file_label_df['filename'][i] + '.wav'\n  #x, sr = librosa.load(sound_filename)\n  #dur = round(x.shape[0]\/sr, 2)\n  #sample_rate.append(sr)\n  #duration.append(dur)\n\n  diagnosis.append(df['Diagnosis'][int(patient_id) - 101])\n  patient_number.append(patient_id)\n  recording_index.append(recording_idx)\n  chest_location.append(chest_loc)\n  acquisition_mode.append(acq_mode)\n  recording_equipment.append(equipment)\n\nfile_label_df['Diagnosis'] = diagnosis\nfile_label_df['Patient Number'] = patient_number\nfile_label_df['Chest Location'] = chest_location\nfile_label_df['Acquisition Mode'] = acquisition_mode\nfile_label_df['Recording Equipment'] = recording_equipment\n#file_label_df['duration'] = duration\n#file_label_df['sample rate'] = sample_rate","3c39c18d":"diagnosis_3 = []\nfor i in range(len(file_label_df['Diagnosis'])):\n  diagnosis = file_label_df['Diagnosis'][i]\n  if diagnosis == 'COPD' or diagnosis == 'Bronchiectasis' or diagnosis == 'Asthma':\n    diagnosis_3.append('Chronic Disease')\n  elif diagnosis == 'URTI' or diagnosis == 'LRTI' or diagnosis == 'Pneumonia' or diagnosis == 'Bronchiolitis':\n    diagnosis_3.append('Non-Chronic Disease')\n  else:\n    diagnosis_3.append('Healthy')\n\nfile_label_df['3 label diagnosis'] = diagnosis_3","5919f0c7":"file_label_df","711b3690":"file_label_df['Diagnosis'].value_counts()","b57afdec":"file_label_df['3 label diagnosis'].value_counts()","a52207b5":"import librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport numpy as np\nimport glob\nimport soundfile\nfrom scipy.io.wavfile import read","c2f333c9":"i = 100\nsound_filename = audio_path + file_label_df['filename'][i] + '.wav'","60020b93":"import IPython.display as ipd\nipd.Audio(sound_filename, rate=16000)","09f3dc16":"# Displaying sound data \n\"\"\"\n  time series : amplitude of sound is varied in time domain (1D)\n  spectogram : amplitude of sound is varied in time and frequency domain (2D)\n\"\"\"\n# time series data\nsr_new = 16000 # resample 16 kHz\nx, sr = librosa.load(sound_filename, sr=sr_new) # x : time series data, sr : sample rate\n\n# Spectogram data \nX = librosa.stft(x)\nXdb = librosa.amplitude_to_db(abs(X))\n\n# Log-mel spectogram\nmel = librosa.feature.melspectrogram(y=x, sr=sr, n_mels=128, fmax=8000)\nlog_mel = librosa.power_to_db(mel, ref=np.max)\n\n\n# Mel-Frequency Cepstral Coefficient (MFCC)\nmfccs = librosa.feature.mfcc(x, sr=sr)\n\nfig, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\n\nlibrosa.display.waveplot(y=x, sr=sr, ax=ax[0, 0])\nax[0, 0].set(title='time series data')\n\nlibrosa.display.specshow(Xdb, x_axis='time',\n                         y_axis='log', sr=sr, ax=ax[0, 1])\nax[0, 1].set(title='spectogram data')\n\nlibrosa.display.specshow(log_mel, x_axis='time',\n                         y_axis='mel', sr=sr,\n                         fmax=8000, ax=ax[1, 0])\nax[1, 0].set(title='Log-Mel Spectogram')\n\n\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time', ax=ax[1, 1])\nax[1, 1].set(title='MFCC')\n\nplt.suptitle(\"Display of sound in varies domain\", fontsize=14);\n","e631afe6":"def preprocessing(audio_file, mode):\n    # we want to resample audio to 16 kHz\n    sr_new = 16000 # 16kHz sample rate\n    x, sr = librosa.load(audio_file, sr=sr_new)\n\n    # padding sound \n    # because duration of sound is dominantly 20 s and all of sample rate is 22050\n    # we want to pad or truncated sound which is below or above 20 s respectively\n    max_len = 5 * sr_new  # length of sound array = time x sample rate\n    if x.shape[0] < max_len:\n      # padding with zero\n      pad_width = max_len - x.shape[0]\n      x = np.pad(x, (0, pad_width))\n    elif x.shape[0] > max_len:\n      # truncated\n      x = x[:max_len]\n    \n    if mode == 'mfcc':\n      feature = librosa.feature.mfcc(x, sr=sr_new)\n    \n    elif mode == 'log_mel':\n      feature = librosa.feature.melspectrogram(y=x, sr=sr_new, n_mels=128, fmax=8000)\n      feature = librosa.power_to_db(feature, ref=np.max)\n    \n\n    return feature","e596994e":"# apply preprocessing to sound data\n\nlabels = []\nlabels_3 = []\npreprocessed_data = []\nfor i in tqdm(range(len(file_label_df['filename']))):\n  labels.append(file_label_df['Diagnosis'][i])\n  labels_3.append(file_label_df['3 label diagnosis'][i])\n  audio_file = audio_path + file_label_df['filename'][i] + '.wav'\n  data = preprocessing(audio_file, mode = 'mfcc')\n  preprocessed_data.append(data)\npreprocessed_data = np.array(preprocessed_data)\nlabels = np.array(labels)\nlabels_3 = np.array(labels_3)\n\n","580ababe":"preprocessed_data[0].shape","4984c913":"# Reshape data to suit input of model\npreprocessed_data = preprocessed_data.reshape((-1, 20, 157, 1))","3e870c86":"# one hot encoding labels\nencoder = LabelEncoder()\ni_labels = encoder.fit_transform(labels)\noh_labels = to_categorical(i_labels) \n\nencoder_3 = LabelEncoder()\ni_labels_3 = encoder_3.fit_transform(labels_3)\noh_labels_3 = to_categorical(i_labels_3) ","749d45bc":"print(list(encoder_3.classes_))","4739ab64":"# train test split\nx_train, x_test, y_train, y_test = train_test_split(preprocessed_data, oh_labels_3, stratify=oh_labels_3, \n                                                    test_size=0.2, random_state = 42)","d5f5d463":"from keras.layers.normalization import BatchNormalization\n\nnum_rows = 20\nnum_columns = 157\nnum_channels = 1\n\n\nnum_labels = oh_labels_3.shape[1]\nfilter_size = 2\n\n# Construct model \nmodel = Sequential()\nmodel.add(Conv2D(filters=16, kernel_size=filter_size,\n                 input_shape=(num_rows, num_columns, num_channels), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\n\nmodel.add(Conv2D(filters=32, kernel_size=filter_size, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(64, activation='relu'))\n\nmodel.add(Conv2D(filters=64, kernel_size=filter_size, activation='relu'))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Dropout(0.2))\n\n\nmodel.add(GlobalAveragePooling2D())\n\nmodel.add(Dense(num_labels, activation='softmax')) \n\nmodel.summary()","4bdb8e18":"# Compile the model\nmodel.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')","031be8bc":"# train model\nnum_epochs = 100\nnum_batch_size = 64\n\ncallbacks = [\n    ModelCheckpoint(\n        filepath='mymodel2_{epoch:02d}.h5',\n        # Path where to save the model\n        # The two parameters below mean that we will overwrite\n        # the current checkpoint if and only if\n        # the `val_accuracy` score has improved.\n        save_best_only=True,\n        monitor='val_accuracy',\n        verbose=1)\n]\nstart = datetime.now()\n\nhistory = model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs,\n          validation_data=(x_test, y_test), callbacks=callbacks, verbose=1)\n\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","8d6b6d77":"import matplotlib.image  as mpimg\nimport matplotlib.pyplot as plt\n\n#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs=range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot(epochs, acc, 'r')\nplt.plot(epochs, val_acc, 'b')\nplt.title('Training and validation accuracy')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot(epochs, loss, 'r')\nplt.plot(epochs, val_loss, 'b')\nplt.title('Training and validation loss')\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Loss\", \"Validation Loss\"])\n\nplt.figure()\n\n\n# Expected Output\n# A chart where the validation loss does not increase sharply!","4dfdfa0a":"# Evaluating the model on the training and testing set\nscore = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: \", score[1])\n\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: \", score[1])","c4183b0d":"preds = model.predict(x_test) # label scores \n\nclasspreds = np.argmax(preds, axis=1) # predicted classes \n\ny_testclass = np.argmax(y_test, axis=1) # true classes\n\nn_classes=3 # number of classes","71d4eea4":"# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(n_classes):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], preds[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n\nc_names = ['Chronic Disease', 'Healthy', 'Non-Chronic Disease']","3e5073d8":"# Plot ROC curves\nfig, ax = plt.subplots(figsize=(16, 10))\nax.plot([0, 1], [0, 1], 'k--')\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.05])\nax.set_xlabel('False Positive Rate')\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC Curve for Each Class')\nfor i in range(n_classes):\n    ax.plot(fpr[i], tpr[i], linewidth=3, label='ROC curve (area = %0.2f) for %s' % (roc_auc[i], c_names[i]))\nax.legend(loc=\"best\", fontsize='x-large')\nax.grid(alpha=.4)\nsns.despine()\nplt.show()","0e9381f8":"# Classification Report\nprint(classification_report(y_testclass, classpreds, target_names=c_names))","d5db9b2f":"# Confusion Matrix\nprint(confusion_matrix(y_testclass, classpreds))","ccca4496":"export_dir = 'saved_model\/my_model'\n\ntf.saved_model.save(model, export_dir=export_dir)","488cf2a1":"# saving model in HDF5 format\nmodel.save('prediction_lung_disease_model.h5')","9b3127a4":"# Here's a codeblock just for fun. You should be able to upload an sound here \n# and have it classified without crashing\nfrom google.colab import files\n\n# uploading data\nuploaded = files.upload()\nfn = list(uploaded.keys())","ca01a5db":"# load model\n# Recreate the exact same model, including its weights and the optimizer\nmodel_path = 'saved_model\/my_model'\nnew_model = tf.keras.models.load_model(model_path)\n\n# Show the model architecture\nnew_model.summary()","c536e6d7":"audio_path = '\/content\/files\/respiratory_sound_database\/Respiratory_Sound_Database\/audio_and_txt_files\/'","dead534e":"import numpy as np\nimport librosa\n\ni = 4\naudio_file = audio_path + file_label_df['filename'][i] + '.wav'\n\n# preprocessing sound\ndata = preprocessing(audio_file, mode='mfcc')\ndata = np.array(data)\nprint(data.shape)\ndata = data.reshape((20, 157, 1))\ndata = np.expand_dims(data, axis=0)\n\ndatas = np.vstack([data])\n\n# Predict sound data\nclasses = new_model.predict(datas, batch_size=10)\nidx = np.argmax(classes)\n\nc_names = ['Chronic Disease', 'Healthy', 'Non-Chronic Disease']\nprint('Lunion prediction: \\n{}'.format(c_names[idx]))\nprint('Actual label: \\n{}'.format(file_label_df['3 label diagnosis'][i]))\nprint('Confidence Percentage: {:.2f} %'.format(np.max(classes) * 100))","c13d57d5":"import pathlib","a8435dd7":"mode = \"Speed\" \n\nif mode == 'Storage':\n    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\nelif mode == 'Speed':\n    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\nelse:\n    optimization = tf.lite.Optimize.DEFAULT","75df78bc":"converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)# YOUR CODE HERE\n\n# Set the optimzations\nconverter.optimizations = [optimization]\n\n# Invoke the converter to finally generate the TFLite model\ntflite_model = converter.convert()","92d4a463":"tflite_model_file = pathlib.Path('.\/model.tflite')\ntflite_model_file.write_bytes(tflite_model)","87e81224":"# download saved_model\n!zip -r \/content\/saved_model.zip \/content\/saved_model\nfrom google.colab import files\nfiles.download(\"\/content\/saved_model.zip\")","3b3c070e":"# Exploratory Data Analysis ","b5d77447":"# Prediksi suara paru","f805f0f2":"Metric : Accuracy, waktu komputasi prediksi ","9b2fb859":"# Acquiring data","7ec2f8a8":"# Integration with mobile","0f689f94":"# Training and Testing","bc6f69b0":"Buat dataframe database suara paru","626410e6":"# Preprocessing","44aa6d39":"Save model as saved model format"}}