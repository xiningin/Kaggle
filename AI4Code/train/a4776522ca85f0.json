{"cell_type":{"3ebc7b6f":"code","63f8d742":"code","83271628":"code","83c63cee":"code","8f72dd51":"code","cae90f6e":"code","eacdd5b9":"code","21be2f85":"code","18faa77c":"code","1f585e9c":"code","fbca9da1":"code","dfd27aeb":"code","2fc31186":"code","6c48fa2c":"code","86a60767":"code","bb8f2b40":"code","46da59b2":"code","f9343c99":"code","c3e95c89":"code","c874731b":"code","06a75d2e":"code","012cbb08":"code","860cf57d":"code","be082f54":"code","93d95d72":"code","a0513590":"code","d2640183":"code","4f39b6b2":"code","5b996345":"code","0ca1de4f":"code","ada224c8":"code","7458b2c3":"code","d02c7a05":"code","e3da1311":"code","89149bb9":"code","c6a2e916":"code","c8a5baf9":"code","796cf246":"code","d89a7e27":"code","395c12c3":"code","2d85ffa1":"code","329d0ece":"code","84a282e8":"code","06050d5d":"code","96316d58":"code","c6411f48":"code","7f485ed5":"code","5c5bc128":"code","002a4331":"code","cf4d9a15":"code","27d11dbd":"code","c2990037":"code","42b57cc4":"code","55fdb0a3":"code","9a82c87b":"code","482fc5eb":"code","70b41e82":"code","a86d1708":"code","ff06d405":"code","1036ed59":"code","c7e6b126":"code","4c6200ef":"code","095e0bb7":"code","96dc0863":"code","11b9996b":"code","a502b040":"code","92d49684":"code","f732f339":"code","a2d78437":"code","15868cba":"code","50422db9":"code","77397c0c":"code","61ad7594":"code","3fcfe81d":"code","f50011b3":"code","4e755d47":"code","e349c561":"code","9b3e4bbe":"code","93799ea3":"code","fa46600b":"code","485e360d":"code","6436dfa3":"code","25cc33f0":"code","3765aebe":"code","fbf8ea03":"code","1938cdae":"code","251d6c6e":"code","b58c7b20":"code","f9fc98b2":"code","3fbbe67f":"markdown","a10b10f8":"markdown","cdaf4352":"markdown","39901bcd":"markdown","8c0ba2b5":"markdown","bc76fbff":"markdown","1def2397":"markdown","96c1608f":"markdown","f02f912c":"markdown","fdd58a1d":"markdown","fb46d5e8":"markdown","3df116f0":"markdown","f9afa098":"markdown","31aca276":"markdown","446977a4":"markdown","e11225b9":"markdown","3a120a15":"markdown","50665b99":"markdown","2e897400":"markdown","cc5e7758":"markdown","61c86e41":"markdown","d6c5e094":"markdown","7ceb38aa":"markdown","88717107":"markdown","1ac37e64":"markdown","c477f2e9":"markdown","415ab03b":"markdown","69a7dd72":"markdown","24039d99":"markdown","29a28457":"markdown","710ad4eb":"markdown","67cfc32d":"markdown","a98a92f7":"markdown","07ca0fc2":"markdown","cbef2984":"markdown","3b2469ae":"markdown","5691ba06":"markdown","6ec6e52e":"markdown","ce5afee1":"markdown","e1c2a0db":"markdown","2dc7930f":"markdown","3da00263":"markdown","c4f83d11":"markdown","7ab6c147":"markdown"},"source":{"3ebc7b6f":"import numpy as np\nimport scipy.stats as stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nsns.set(font_scale=1.5)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline","63f8d742":"train=pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","83271628":"test=pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","83c63cee":"train.head()","8f72dd51":"test.head()","cae90f6e":"train.shape","eacdd5b9":"test.shape","21be2f85":"train.isnull().sum()","18faa77c":"test.isnull().sum()","1f585e9c":"train.describe()","fbca9da1":"test.describe()","dfd27aeb":"corr=train.corr()","2fc31186":"sns.heatmap(corr, annot=True)","6c48fa2c":"train['Embarked'].value_counts()","86a60767":"train['Embarked']=train['Embarked'].fillna('S')","bb8f2b40":"train['Cabin']= train['Cabin'].fillna('Unknown')","46da59b2":"train['Cabin']= train['Cabin'].apply(lambda x: x[0])","f9343c99":"train['Cabin'].head()","c3e95c89":"train.groupby('Pclass')['Cabin'].value_counts()","c874731b":"train['Cabin'] = np.where((train.Pclass==1) & (train.Cabin=='U'),'T',\n                                            np.where((train.Pclass==2) & (train.Cabin=='U'),'D',\n                                                                        np.where((train.Pclass==3) & (train.Cabin=='U'),'E',train.Cabin\n                                                                                                    )))","06a75d2e":"train['Sex']=train['Sex'].apply(lambda x: 1 if x=='female' else 0 )","012cbb08":"guess_ages = np.zeros((2,3))\nguess_ages","860cf57d":"\nfor i in range(0, 2):\n    for j in range(0, 3):\n        guess_df = train[(train['Sex'] == i) & (train['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n        age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n        guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \nfor i in range(0, 2):\n    for j in range(0, 3):\n        train.loc[ (train.Age.isnull()) & (train.Sex == i) & (train.Pclass == j+1),'Age'] = guess_ages[i,j]\n\ntrain['Age'] = train['Age'].astype(int)\n\n","be082f54":"train.isnull().sum()","93d95d72":"test['Cabin']= test['Cabin'].fillna('Unknown')\ntest['Cabin']= test['Cabin'].apply(lambda x: x[0])    ","a0513590":"test.groupby('Pclass')['Cabin'].value_counts()","d2640183":"test['Cabin'] = np.where((test.Pclass==1) & (test.Cabin=='U'),'T',\n                                            np.where((test.Pclass==2) & (test.Cabin=='U'),'D',\n                                                                        np.where((test.Pclass==3) & (test.Cabin=='U'),'E',test.Cabin\n                                                                                                    )))","4f39b6b2":"test.groupby('Pclass')['Cabin'].value_counts()","5b996345":"guess_ages = np.zeros((2,3))\nguess_ages","0ca1de4f":"test['Sex']=test['Sex'].apply(lambda x: 1 if x=='female' else 0 )","ada224c8":"for i in range(0, 2):\n    for j in range(0, 3):\n        guess_df = test[(test['Sex'] == i) & (test['Pclass'] == j+1)]['Age'].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n        age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n        guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \nfor i in range(0, 2):\n    for j in range(0, 3):\n        test.loc[ (test.Age.isnull()) & (test.Sex == i) & (test.Pclass == j+1),'Age'] = guess_ages[i,j]\n\ntest['Age'] = test['Age'].astype(int)","7458b2c3":"test['Fare']=test['Fare'].fillna('unknown')","d02c7a05":"test.loc[test['Fare'] == 'unknown']","e3da1311":"train.groupby('Pclass')['Fare'].mean()","89149bb9":"test['Fare'].replace('unknown', 14, inplace=True)","c6a2e916":"test.isnull().sum()","c8a5baf9":"def subplot_histograms(dataframe, list_of_columns, list_of_titles, list_of_xlabels):\n    nrows = int(np.ceil(len(list_of_columns)\/2)) # Makes sure you have enough rows\n    fig, ax = plt.subplots(nrows=nrows, ncols=2, figsize=(14,24)) # You'll want to specify your figsize\n    ax = ax.ravel() # Ravel turns a matrix into a vector, which is easier to iterate\n    for i, column in enumerate(list_of_columns): # Gives us an index value to get into all our lists\n        ax[i].hist(dataframe[column], color='skyblue') # feel free to add more settings\n        ax[i].set_title(list_of_titles[i])\n        ax[i].set_xlabel(list_of_xlabels[i])\n       ","796cf246":"cols=['Pclass','Sex','Age','SibSp','Parch','Fare','Survived']\ntit=['Pclass','Sex','Age','Siblings','Parents and children','Fare','Survived']\nxs=['Pclass','Sex','Age','SibSp','Parch','Fare','Survived']\nsubplot_histograms(train,cols,tit,xs)","d89a7e27":"surv=pd.DataFrame(train.loc[train['Survived'] == 1])","395c12c3":"\n\ncols=['Pclass','Sex','Age','SibSp','Parch','Fare']\ntit=['Pclass','Sex','Age','Siblings','Parents and children','Fare']\nxs=['Pclass','Sex','Age','SibSp','Parch','Fare']\nsubplot_histograms(surv,cols,tit,xs)","2d85ffa1":"sns.heatmap(train.corr(),annot=True)","329d0ece":"fig, ax = plt.subplots()\nax.scatter(x = train['Survived'], y = train['Age'])\nplt.ylabel('Age', fontsize=13)\nplt.xlabel('Survived', fontsize=13)\nplt.title('')\nplt.show()","84a282e8":"#Deleting outliers\ntrain = train.drop(train[(train['Survived']== 1) & (train['Age']>79)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(x = train['Survived'], y = train['Age'])\nplt.ylabel('Age', fontsize=13)\nplt.xlabel('Survived', fontsize=13)\nplt.title('')\nplt.show()","06050d5d":"def percentage(part):\n    whole= train['Sex'].value_counts().sum()\n    percentage= (part\/whole)\n    return percentage\n\npercentage= train['Sex'].value_counts().apply(lambda x : percentage(x))\nprint (percentage)\n\n\n\nlabels = 'Male','Female' \nsizes = [65, 35]\nc=['#b39ab0','#e6e6fa']\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%',colors=c,\n        shadow=True, startangle=90)\nax1.axis('equal')  \nplt.title('Percentage of Male Vs. Female passengers')\nplt.show()\n","96316d58":"def percentage(part):\n    whole= surv['Sex'].value_counts().sum()\n    percentage= (part\/whole)\n    return percentage\n\npercentage= surv['Sex'].value_counts().apply(lambda x : percentage(x))\nprint (percentage)\n\n\n\nlabels = 'Female', 'Male'\nsizes = [68, 32]\nc=['#b39ab0','#e6e6fa']\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, autopct='%1.1f%%',colors=c,\n        shadow=True, startangle=90)\nax1.axis('equal')  \nplt.title('Percentage of Male Vs. Female survivors')\nplt.show()\n","c6411f48":"cat=train['Age']\ncat=cat.apply(lambda x:'<10' if x<11 else '11-18' if x<18  else '19-35' if x<36  else '36-60' if x<61  else '61-80')\n\nc= {'male': '#8c9fff' , 'female': '#ffb68c'}\nsns.countplot(x='Pclass', data = train , hue=cat, palette= 'Set2' )\nplt.title('Range of ages in each Pclass')\nplt.legend(bbox_to_anchor=(1,1), loc=2)","7f485ed5":"cat=surv['Age']\ncat=cat.apply(lambda x:'<10' if x<11 else '11-18' if x<18  else '19-35' if x<36  else '36-60' if x<61  else '61-80')\n\nc= {'male': '#8c9fff' , 'female': '#ffb68c'}\nsns.countplot(x='Pclass', data = surv , hue=cat, palette= 'Set2' )\nplt.title('Range of ages of survivors in each Pclass')\nplt.legend(bbox_to_anchor=(1,1), loc=2)","5c5bc128":"c= {'male': '#8c9fff' , 'female': '#ffb68c'}\nsns.countplot(x='Survived', data = train , hue=cat, palette= 'Set2' )\nplt.title('Range of ages for survivors')\nplt.legend(bbox_to_anchor=(1,1), loc=2)","002a4331":"df_plot=train.groupby(['Pclass', 'Survived']).size().reset_index().pivot(columns='Pclass', index='Survived', values=0)\ndf_plot.plot(kind='bar', stacked=True,colormap='Set2')\nplt.title('Number of survivors for each Pclass')\nplt.legend(bbox_to_anchor=(1,1), loc=2)","cf4d9a15":"\nsns.pairplot(train)","27d11dbd":"features_drop = ['PassengerId','Name', 'Ticket', 'Survived','Embarked','Cabin']\nselected_features=[c for c in train if c not in features_drop]\nselected_features","c2990037":"X_train = train[selected_features]\ny_train = train['Survived']\nX_test= test[selected_features]","42b57cc4":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\nX_test = pd.DataFrame(scaler.fit_transform(X_test), columns=X_test.columns)","55fdb0a3":"# from sklearn.preprocessing import MinMaxScaler\n# scaler = MinMaxScaler(feature_range=(0, 1))\n\n# x_train_scaled = scaler.fit_transform(X_train)\n# X_train = pd.DataFrame(x_train_scaled)\n\n# x_test_scaled = scaler.fit_transform(X_test)\n# X_test = pd.DataFrame(x_test_scaled)","9a82c87b":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score","482fc5eb":"baseline=y_train.value_counts(normalize=True)\nbaseline[0]","70b41e82":"knn = KNeighborsClassifier(n_neighbors=5) \nknn.fit(X_train, y_train)\nm_score= knn.score(X_train, y_train)\nprint('Model score: ', m_score)","a86d1708":"predictions = pd.DataFrame(knn.predict(X_test))\npredictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)","ff06d405":"predictions.to_csv('predictions_knn.csv')","1036ed59":"from sklearn.model_selection import GridSearchCV\nknn_params = {\n    'n_neighbors': range(1,100),\n    'weights':['uniform','distance'],\n    'metric':['euclidean','manhattan']}\nprint('Initialized parameters for Grid Search')\nprint(knn_params)","c7e6b126":"knn_gridsearch = GridSearchCV(KNeighborsClassifier(), \n                              knn_params, \n                              n_jobs=1, cv=5) # try verbose!\n\n\nknn_gridsearch.fit(X_train, y_train)\n","4c6200ef":"best_knn = knn_gridsearch.best_estimator_\nbest_knn.score(X_train, y_train)\npredictions = pd.DataFrame(best_knn.predict(X_test))","095e0bb7":"predictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)\npredictions.to_csv('predictions_gs_knn_afteroutliers.csv')","96dc0863":"from sklearn.tree import DecisionTreeClassifier","11b9996b":"dtc_params = {\n    'max_depth': range(1,20),\n    'max_features': [None, 'log2', 'sqrt'],\n    'min_samples_split': range(5,30),\n    'max_leaf_nodes': [None],\n    'min_samples_leaf': range(1,10)\n}\n\nfrom sklearn.model_selection import GridSearchCV\n# set the gridsearch\ndtc_gs = GridSearchCV(DecisionTreeClassifier(), dtc_params,  n_jobs=-1, cv=5)","a502b040":"dtc_gs.fit(X_train, y_train)","92d49684":"predictions = dtc_gs.best_estimator_.predict(X_test)\n# predictions.to_csv('predictions_dt_gs.csv')","f732f339":"predictions = pd.DataFrame(predictions)\npredictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)\npredictions.to_csv('predictions_dt_gs.csv')","a2d78437":"from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\nrf_params = {\n      'n_estimators': range(1,100),\n#     'max_features':[2, 3, 5, 7, 8],\n      'max_depth': range(1,20),\n     'criterion':['gini', 'entropy'],\n}","15868cba":"rf_g = RandomForestClassifier() ","50422db9":"gs = GridSearchCV(rf_g, param_grid=rf_params, cv=5, verbose = 1)#, refit=False) ","77397c0c":"gs=gs.fit(X_train, y_train)","61ad7594":"predictions = gs.best_estimator_.predict(X_test)\n","3fcfe81d":"predictions = pd.DataFrame(predictions)\npredictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)\npredictions.to_csv('predictions_RF_gs.csv')","f50011b3":"rf_params = {\n      'n_estimators': range(1,100),\n#     'max_features':[2, 3, 5, 7, 8],\n      'max_depth': range(1,20),\n     'criterion':['gini', 'entropy'],\n}","4e755d47":"et_g = ExtraTreesClassifier()","e349c561":"gs_et = GridSearchCV(rf_g, param_grid=rf_params, cv=5, verbose = 1)#, refit=False) ","9b3e4bbe":"gs_et =gs_et.fit(X_train, y_train)","93799ea3":"predictions = gs_et.best_estimator_.predict(X_test)","fa46600b":"predictions = pd.DataFrame(predictions)\npredictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)\npredictions.to_csv('predictions_ET_gs.csv')","485e360d":"from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nmodel = LogisticRegression()\nparams = {'C':np.logspace(-5,5,15),\n          'penalty':['l1'],\n          'fit_intercept':[True,False]}\ngs = GridSearchCV(estimator=model,\n                  param_grid=params,\n                  cv=5,\n                  scoring='accuracy',\n                  return_train_score=True)\ngs.fit(X_train,y_train)\nprint(gs.best_score_)\nprint(gs.score(X_train,y_train))\n","6436dfa3":"predictions= gs.predict(X_test)\npredictions = pd.DataFrame(predictions)\npredictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)\npredictions.to_csv('predictions_log_l1.csv')","25cc33f0":"model = LogisticRegression()\nparams = {'C':np.logspace(-5,5,15),\n          'penalty':['l2'], #Ridge\n          'fit_intercept':[True,False]}\ngs = GridSearchCV(estimator=model,\n                  param_grid=params,\n                  cv=5,\n                  scoring='accuracy',\n                  return_train_score=True)\ngs.fit(X_train,y_train)\nprint(gs.best_score_)\nprint(gs.score(X_train,y_train))","3765aebe":"predictions= gs.predict(X_test)\npredictions = pd.DataFrame(predictions)\npredictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)\npredictions.to_csv('predictions_log_l2.csv')","fbf8ea03":"X_train_s = train[['Sex','Pclass']]\ny_train_s = train['Survived']\nX_test_s= test[['Sex','Pclass']]","1938cdae":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_s = pd.DataFrame(scaler.fit_transform(X_train_s), columns=X_train_s.columns)\nX_test_s = pd.DataFrame(scaler.fit_transform(X_test_s), columns=X_test_s.columns)","251d6c6e":"rf_params = {\n      'n_estimators': range(1,100),\n#     'max_features':[2, 3, 5, 7, 8],\n      'max_depth': range(1,20),\n     'criterion':['gini', 'entropy'],\n}\nrf_g = RandomForestClassifier()\ngs = GridSearchCV(rf_g, param_grid=rf_params, cv=5, verbose = 1)#, refit=False) \ngs=gs.fit(X_train_s, y_train_s)\npredictions = gs.best_estimator_.predict(X_test_s)","b58c7b20":"predictions = pd.DataFrame(predictions)\npredictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)\npredictions.to_csv('predictions_RF_gs_s.csv')","f9fc98b2":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nX_train_s = train[['Sex','Pclass','Age']]\ny_train_s = train['Survived']\nX_test_s= test[['Sex','Pclass','Age']]\n\nscaler = StandardScaler()\nX_train_s = pd.DataFrame(scaler.fit_transform(X_train_s), columns=X_train_s.columns)\nX_test_s = pd.DataFrame(scaler.fit_transform(X_test_s), columns=X_test_s.columns)\n\nrf_params = {\n      'n_estimators': range(1,50),\n#     'max_features':[2, 3, 5, 7, 8],\n      'max_depth': range(1,20),\n     'criterion':['gini', 'entropy'],\n}\nrf_g = RandomForestClassifier()\ngs = GridSearchCV(rf_g, param_grid=rf_params, cv=5, verbose = 1)#, refit=False) \ngs=gs.fit(X_train_s, y_train_s)\npredictions = gs.best_estimator_.predict(X_test_s)\n\npredictions = pd.DataFrame(predictions)\npredictions['PassengerId']=[i for i in range(892, 1310)]\npredictions.rename(columns={0:'Survived'},inplace=True)\npredictions.set_index('PassengerId',inplace=True)\npredictions.to_csv('predictions_RF_gs_s2.csv')","3fbbe67f":"#### Filling the test data in the same way that we did with the train data ","a10b10f8":"#### For sex: Replacing male and female with 0 and 1","cdaf4352":"#### Rechecking remaining missing values if any","39901bcd":"#### Split data to train and test","8c0ba2b5":"### Conclusion\nIn this project, we created classification models; LogisticRegression, ExtraTreesClassifier, KNeighborsClassifier, RandomForestClassifier and DecisionTreeClassifier to predict whether or not a Titanic passenger has survived. We tested all models by submitting them individually in Kaggle. The best score was for RandomForestClassifier and DecisionTreeClassifier, in which we got a score of 0.76555.","bc76fbff":"#### print summary statistics","1def2397":"### Problem Statement:\nTitanic is a widely known disaster. However, it can be useful to learn from disasters. In this project, we acquired the dataset of the passengers of Titanic. It contains 10 features about approximately 1300 passengers; their name, ticket class, sex, age, ticket number, number of children and parents, number of siblings, ticket fare, cabin number, port of embarkation, and whether or not they survived.<br>\nWe wanted to predict whether or not a specific passenger has survived the tragedy. Therefore, We trained some classification models to predict that.","96c1608f":"#### Display count plot for range of ages in each Pclass","f02f912c":"![model2.png](attachment:model2.png)","fdd58a1d":"#### In this model, we created a K neighbors classifier after performing a grid search. We applied the model on the entire features in X_train.","fb46d5e8":"#### Display pie plot for percentage of Male Vs. Female passengers","3df116f0":"![Screen%20Shot%202019-11-05%20at%2011.18.35%20AM.png](attachment:Screen%20Shot%202019-11-05%20at%2011.18.35%20AM.png)","f9afa098":"#### Distributions of the features in the dataset","31aca276":"#### Correlation heatmap to see how features are correlated with SalePrice","446977a4":"## Preprocessing and Modeling","e11225b9":"#### For age, we created an array of 6 values of age. They are the median of age for each gender of each Pclass","3a120a15":"#### Display count plot for range of ages of ages for survivors","50665b99":"![Screen%20Shot%202019-11-05%20at%2011.18.35%20AM.png](attachment:Screen%20Shot%202019-11-05%20at%2011.18.35%20AM.png)","2e897400":"#### We imported StandardScaler and applied it to both X_train and X_test.","cc5e7758":"#### For cabin: Grouping cabins by Pclass, then selecting one cabin for each class randomly to fill the missing values ","61c86e41":"#### Distributions of the features in the dataset for the survivors only","d6c5e094":"#### Display the columns with null values and number of nulls","7ceb38aa":"## Data Cleaning and EDA","88717107":"![model9.png](attachment:model9.png)","1ac37e64":"#### In this model, we created a Random Forest after performing a grid search. We applied the model on the entire features in X_train.","c477f2e9":"### Filling null values","415ab03b":"![model3.png](attachment:model3.png)","69a7dd72":"#### In this model, we created a Logistic regression with ridge after performing a grid search. We applied the model on the entire features in X_train.","24039d99":"#### In this model, we created a Extra trees classifier after performing a grid search. We applied the model on the entire features in X_train.","29a28457":"#### In this model, we created a decision tree after performing a grid search. We applied the model on the entire features in X_train.","710ad4eb":"#### print the head of train and test data","67cfc32d":"### Data Visualization","a98a92f7":"#### print the shape of train and test data","07ca0fc2":"![model1.png](attachment:model1.png)","cbef2984":"#### Display pie plot for percentage of Male Vs. Female  survivors","3b2469ae":"#### For port of embarkation: Replacing missing values with S because it is the most repetitve value","5691ba06":"![model5.png](attachment:model5.png)","6ec6e52e":"#### Calculating the baseline","ce5afee1":"#### In this model, we created a Logistic regression with lasso after performing a grid search. We applied the model on the entire features in X_train.","e1c2a0db":"![Screen%20Shot%202019-11-05%20at%2011.19.40%20AM.png](attachment:Screen%20Shot%202019-11-05%20at%2011.19.40%20AM.png)","2dc7930f":"#### Display count plot for range of ages of survivors in each Pclass","3da00263":"![model4.png](attachment:model4.png)","c4f83d11":"#### Display a plot for range of survivors for each Pclass","7ab6c147":"#### In this model, we created a K neighbors classifier and applied the model on the entire features in X_train."}}