{"cell_type":{"6f214fb0":"code","4a4c0bfb":"code","c4b53f44":"code","bcd2ef71":"code","d69d109b":"code","566b21c0":"code","c838c690":"code","2b84fbea":"code","37dd8345":"code","e9603479":"code","9ae68817":"code","fdc17914":"code","a0432262":"code","6f405b6a":"code","fc0e7943":"code","5585e20b":"code","d4fa37e5":"markdown","1b5ef653":"markdown","ead79696":"markdown","770aca80":"markdown","7403c3cd":"markdown","19fbeb03":"markdown","8f456c4c":"markdown","ad947a7d":"markdown","f4ad2cfa":"markdown"},"source":{"6f214fb0":"import warnings\nwarnings.filterwarnings('ignore')\nimport logging\nimport os\nimport re\nimport collections\nimport copy\nimport datetime\nimport random\nimport traceback\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport scipy\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nimport tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch import nn\nfrom torch.nn import functional as F\n\nnltk.download('stopwords')\nlemmatizer = WordNetLemmatizer()\nstop_words_en = set(stopwords.words('english'))\nstemmer_en = SnowballStemmer('english')","4a4c0bfb":"dir_data = '..\/input\/nlp-getting-started'\nfile_test = 'test.csv'\nfile_train = 'train.csv'\n\nrandom_seed = 123\nrandom.seed(random_seed)\nnp.random.seed(random_seed)\ntorch.manual_seed(random_seed)\ntorch.cuda.manual_seed(random_seed)\ntorch.backends.cudnn.deterministic = True","c4b53f44":"config = {\n    'TextPreprocessor': {\n        'mode_stemming': True,\n        'mode_norm': True,\n        'mode_remove_stops': True,\n        'mode_drop_long_words': True,\n        'mode_drop_short_words': True,\n        'min_len_word': 3,\n        'max_len_word': 15,\n        'max_size_vocab': 50000,\n        'max_doc_freq': 0.9,\n        'min_count': 5,\n        'pad_word': '<PAD>',\n        'text_column': 'text'        \n    },\n    'PaddedSequenceDataset': {\n        'out_len': 100,\n        'pad_value': 0\n    },\n    'SkipGramNegativeSamplingTrainer': {\n        'emb_size': 100,\n        'sentence_lent': 20,\n        'radius': 5,\n        'negative_samples_n': 50,\n    },\n    'train': {\n        'lr': 1e-3,\n        'epoch_n': 500,\n        'batch_size': 64,\n        'early_stopping_patience': 40,\n        'max_batches_per_epoch_train': 2000\n    },\n    'Embeddings': {\n        'topk': 10,\n    }   \n}","bcd2ef71":"df_train = pd.read_csv(os.path.join(dir_data, file_train))\ndf_test = pd.read_csv(os.path.join(dir_data, file_test))\ndf_train.shape, df_test.shape","d69d109b":"class TextPreprocessor(object):\n    def __init__(self, config):\n        \"\"\"Preparing text features.\"\"\"\n\n        self._mode_stemming = config.get('mode_stemming', True)\n        self._mode_norm = config.get('mode_norm', True)\n        self._mode_remove_stops = config.get('mode_remove_stops', True)\n        self._mode_drop_long_words = config.get('mode_drop_long_words', True)\n        self._mode_drop_short_words = config.get('mode_drop_short_words', True)\n        self._min_len_word = config.get('min_len_word', 3)\n        self._max_len_word = config.get('max_len_word', 17)\n        self._max_size_vocab = config.get('max_size_vocab', 100000)\n        self._max_doc_freq = config.get('max_doc_freq', 0.8) \n        self._min_count = config.get('min_count', 5)\n        self._pad_word = config.get('pad_word', '<PAD>')\n        self._text_column = config.get('text_column', 'union_text')\n\n    def _clean_text(self, input_text):\n        \"\"\"Delete special symbols.\"\"\"\n\n        input_text = input_text.str.lower()\n        input_text = input_text.str.replace(r'[^a-z ]+', ' ') \n        input_text = input_text.str.replace(r'http', ' ')\n        input_text = input_text.str.replace(r' +', ' ')\n        input_text = input_text.str.replace(r'^ ', '')\n        input_text = input_text.str.replace(r' $', '')\n\n        return input_text\n\n\n    def _text_normalization_en(self, input_text):\n        '''Normalization of english text'''\n\n        return ' '.join([lemmatizer.lemmatize(item) for item in input_text.split(' ')])\n\n\n    def _remove_stops_en(self, input_text):\n        '''Delete english stop-words'''\n\n        return ' '.join([w for w in input_text.split() if not w in stop_words_en])\n\n\n    def _stemming_en(self, input_text):\n        '''Stemming of english text'''\n\n        return ' '.join([stemmer_en.stem(item) for item in input_text.split(' ')])\n\n\n    def _drop_long_words(self, input_text):\n        \"\"\"Delete long words\"\"\"\n        return ' '.join([item for item in input_text.split(' ') if len(item) < self._max_len_word])\n\n\n    def _drop_short_words(self, input_text):\n        \"\"\"Delete short words\"\"\"\n\n        return ' '.join([item for item in input_text.split(' ') if len(item) > self._min_len_word])\n        \n    def _build_vocabulary(self, tokenized_texts):\n        \"\"\"Build vocabulary\"\"\"\n        \n        word_counts = collections.defaultdict(int)\n        doc_n = 0\n\n        for txt in tokenized_texts:\n            doc_n += 1\n            unique_text_tokens = set(txt)\n            for token in unique_text_tokens:\n                word_counts[token] += 1\n                \n        word_counts = {word: cnt for word, cnt in word_counts.items()\n                       if cnt >= self._min_count and cnt \/ doc_n <= self._max_doc_freq}\n        \n        sorted_word_counts = sorted(word_counts.items(),\n                                    reverse=True,\n                                    key=lambda pair: pair[1])\n        \n        if self._pad_word is not None:\n            sorted_word_counts = [(self._pad_word, 0)] + sorted_word_counts\n            \n        if len(word_counts) > self._max_size_vocab:\n            sorted_word_counts = sorted_word_counts[:self._max_size_vocab]\n            \n        word2id = {word: i for i, (word, _) in enumerate(sorted_word_counts)}\n        word2freq = np.array([cnt \/ doc_n for _, cnt in sorted_word_counts], dtype='float32')\n        \n        return word2id, word2freq\n    \n    def transform(self, df):       \n        \n        if self._text_column == 'union_text':\n            columns_names = df.select_dtypes(include='object').columns\n            df[columns_names] = df[columns_names].astype('str')\n            for i in df.index:\n                df.loc[i, self._text_column] = ' '.join(df.loc[i, columns_names])\n            \n        df[self._text_column] = self._clean_text(df[self._text_column])\n        \n        if self._mode_norm:\n            df[self._text_column] = df[self._text_column].apply(self._text_normalization_en, 1)\n            \n        if self._mode_remove_stops:\n            df[self._text_column] = df[self._text_column].apply(self._remove_stops_en, 1)\n            \n        if self._mode_stemming:\n            df[self._text_column] = df[self._text_column].apply(self._stemming_en)\n            \n        if self._mode_drop_long_words:\n            df[self._text_column] = df[self._text_column].apply(self._drop_long_words, 1)\n            \n        if self._mode_drop_short_words:\n            df[self._text_column] = df[self._text_column].apply(self._drop_short_words, 1)\n            \n        df.loc[(df[self._text_column] == ''), (self._text_column)] = '<EMP>'\n        \n        tokenized_texts = [[word for word in text.split(' ')] for text in df[self._text_column]]\n        word2id, word2freq = self._build_vocabulary(tokenized_texts)\n\n        return tokenized_texts, word2id, word2freq\n    \n    def texts_to_token_ids(self, tokenized_texts, word2id):\n        \"\"\"Convert texts to Ids\"\"\"\n        \n        return [[word2id[token] for token in text if token in word2id]\n                for text in tokenized_texts]\n\nclass PaddedSequenceDataset(Dataset):\n    def __init__(self, config, texts, targets):\n        \n        self.texts = texts\n        self.targets = targets\n        self.out_len = config.get('out_len', 100)\n        self.pad_value = config.get('pad_value', 0)\n        \n    def _ensure_length(self, txt, out_len, pad_value):\n        if len(txt) < out_len:\n            txt = list(txt) + [pad_value] * (out_len - len(txt))\n        else:\n            txt = txt[:out_len]\n        return txt\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        txt = self.texts[item]\n\n        txt = self._ensure_length(txt, self.out_len, self.pad_value)\n        txt = torch.tensor(txt, dtype=torch.long)\n\n        target = torch.tensor(self.targets[item], dtype=torch.long)\n\n        return txt, target\n\ndef add_fake_token(word2id, token='<PAD>'):\n    word2id_new = {token: i + 1 for token, i in word2id.items()}\n    word2id_new[token] = 0\n    return word2id_new\n    \ndef make_diag_mask(size, radius):\n    \"\"\"Square matrix Size x Size with two bars of width radius along the main diagonal.\"\"\"\n    idxs = torch.arange(size)\n    abs_idx_diff = (idxs.unsqueeze(0) - idxs.unsqueeze(1)).abs()\n    mask = ((abs_idx_diff <= radius) & (abs_idx_diff > 0)).float()\n    return mask\n\nmake_diag_mask(10, 3)\n\n\nclass SkipGramNegativeSamplingTrainer(nn.Module):\n    def __init__(self, config, vocab_size):\n        super().__init__()\n        \n        self.emb_size = config.get('emb_size', 100)\n        self.sentence_len = config.get('sentence_len', 100)\n        self.radius = config.get('radius', 5)\n        self.negative_samples_n = config.get('negative_samples_n', 50)\n        self.vocab_size = vocab_size\n\n        self.center_emb = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0)\n        self.center_emb.weight.data.uniform_(-1.0 \/ self.emb_size, 1.0 \/ self.emb_size)\n        self.center_emb.weight.data[0] = 0\n\n        self.context_emb = nn.Embedding(self.vocab_size, self.emb_size, padding_idx=0)        \n        self.context_emb.weight.data.uniform_(-1.0 \/ self.emb_size, 1.0 \/ self.emb_size)\n        self.context_emb.weight.data[0] = 0\n\n        self.positive_sim_mask = make_diag_mask(self.sentence_len, self.radius)\n    \n    def forward(self, sentences):\n        \"\"\"sentences - Batch x MaxSentLength\"\"\"\n        batch_size = sentences.shape[0]\n        center_embeddings = self.center_emb(sentences)  # Batch x MaxSentLength x EmbSize\n\n        # evaluate the similarity with real neighboring words\n        positive_context_embs = self.context_emb(sentences).permute(0, 2, 1)  # Batch x EmbSize x MaxSentLength\n        positive_sims = torch.bmm(center_embeddings, positive_context_embs)  # Batch x MaxSentLength x MaxSentLength\n        positive_probs = torch.sigmoid(positive_sims)\n\n        # increase the estimate of the probability of meeting these pairs of words together\n        positive_mask = self.positive_sim_mask.to(positive_sims.device)\n        positive_loss = F.binary_cross_entropy(positive_probs * positive_mask,\n                                               positive_mask.expand_as(positive_probs))\n\n        # choose random \"negative\" words\n        negative_words = torch.randint(1, self.vocab_size,\n                                       size=(batch_size, self.negative_samples_n),\n                                       device=sentences.device)  # Batch x NegSamplesN\n        negative_context_embs = self.context_emb(negative_words).permute(0, 2, 1)  # Batch x EmbSize x NegSamplesN\n        negative_sims = torch.bmm(center_embeddings, negative_context_embs)  # Batch x MaxSentLength x NegSamplesN\n        \n        # decrease the score the probability of meeting these pairs of words together\n        negative_loss = F.binary_cross_entropy_with_logits(negative_sims,\n                                                           negative_sims.new_zeros(negative_sims.shape))\n\n        return positive_loss + negative_loss\n\n\ndef no_loss(pred, target):\n    \"\"\"Fictitious loss function - when the model calculates the loss function itself\"\"\"\n    return pred\n\ndef train_eval_loop(model, train_dataset, val_dataset, criterion,\n                    lr=1e-4, epoch_n=10, batch_size=32,\n                    early_stopping_patience=10, l2_reg_alpha=0,\n                    max_batches_per_epoch_train=10000,\n                    max_batches_per_epoch_val=1000,\n                    data_loader_ctor=DataLoader,\n                    optimizer_ctor=None,\n                    lr_scheduler_ctor=None,\n                    shuffle_train=True,\n                    dataloader_workers_n=0):\n    \"\"\"\n    Loop for training the model. After each epoch, the quality of the model is assessed by deferred sampling.\n    \n    \"\"\"\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    if optimizer_ctor is None:\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=l2_reg_alpha)\n    else:\n        optimizer = optimizer_ctor(model.parameters(), lr=lr)\n\n    if lr_scheduler_ctor is not None:\n        lr_scheduler = lr_scheduler_ctor(optimizer)\n    else:\n        lr_scheduler = None\n\n    train_dataloader = data_loader_ctor(train_dataset, batch_size=batch_size, shuffle=shuffle_train,\n                                        num_workers=dataloader_workers_n)\n    val_dataloader = data_loader_ctor(val_dataset, batch_size=batch_size, shuffle=False,\n                                      num_workers=dataloader_workers_n)\n\n    best_val_loss = float('inf')\n    best_epoch_i = 0\n    best_model = copy.deepcopy(model)\n\n    for epoch_i in range(epoch_n):\n        try:\n            epoch_start = datetime.datetime.now()\n            print('Epoch {}'.format(epoch_i))\n\n            model.train()\n            mean_train_loss = 0\n            train_batches_n = 0\n            for batch_i, (batch_x, batch_y) in enumerate(train_dataloader):\n                if batch_i > max_batches_per_epoch_train:\n                    break\n\n                batch_x = batch_x.to(device)\n                batch_y = batch_y.to(device)\n\n                pred = model(batch_x)\n                loss = criterion(pred, batch_y)\n\n                model.zero_grad()\n                loss.backward()\n\n                optimizer.step()\n\n                mean_train_loss += float(loss)\n                train_batches_n += 1\n\n            mean_train_loss \/= train_batches_n\n            print('Epoch: {} iterations, {:0.2f} sec.'.format(train_batches_n,\n                                                           (datetime.datetime.now() - epoch_start).total_seconds()))\n            print('Mean train loss', mean_train_loss)\n\n\n\n            model.eval()\n            mean_val_loss = 0\n            val_batches_n = 0\n\n            with torch.no_grad():\n                for batch_i, (batch_x, batch_y) in enumerate(val_dataloader):\n                    if batch_i > max_batches_per_epoch_val:\n                        break\n\n                    batch_x = batch_x.to(device)\n                    batch_y = batch_y.to(device)\n\n                    pred = model(batch_x)\n                    loss = criterion(pred, batch_y)\n\n                    mean_val_loss += float(loss)\n                    val_batches_n += 1\n\n            mean_val_loss \/= val_batches_n\n            print('Mean valid loss', mean_val_loss)\n\n            if mean_val_loss < best_val_loss:\n                best_epoch_i = epoch_i\n                best_val_loss = mean_val_loss\n                best_model = copy.deepcopy(model)\n                print('The new best model!')\n            elif epoch_i - best_epoch_i > early_stopping_patience:\n                print('The model has not improved over the last {} epochs, stop learning.'.format(\n                    early_stopping_patience))\n                break\n\n            if lr_scheduler is not None:\n                lr_scheduler.step(mean_val_loss)\n\n            print()\n        except KeyboardInterrupt:\n            print('Stopped by user.')\n            break\n        except Exception as ex:\n            print('Error: {}\\n{}'.format(ex, traceback.format_exc()))\n            break\n\n    return best_val_loss, best_model\n\n\nclass Embeddings(object):\n    def __init__(self, config, embeddings, word2id):\n        \n        self.topk = config.get('topk', 10)\n        self.embeddings = embeddings\n        self.embeddings \/= (np.linalg.norm(self.embeddings, ord=2, axis=-1, keepdims=True) + 1e-4)\n        self.word2id = word2id\n        self.id2word = {i: w for w, i in word2id.items()}\n\n    def most_similar(self, word):\n        return self.most_similar_by_vector(self.get_vector(word))\n\n    def analogy(self, a1, b1, a2):\n        a1_v = self.get_vector(a1)\n        b1_v = self.get_vector(b1)\n        a2_v = self.get_vector(a2)\n        query = b1_v - a1_v + a2_v\n        return self.most_similar_by_vector(query)\n\n    def most_similar_by_vector(self, query_vector):\n        similarities = (self.embeddings * query_vector).sum(-1)\n        best_indices = np.argpartition(-similarities, self.topk, axis=0)[:self.topk]\n        result = [(self.id2word[i], similarities[i]) for i in best_indices]\n        result.sort(key=lambda pair: -pair[1])\n        return result\n\n    def get_vector(self, word):\n        if word not in self.word2id:\n            raise ValueError('Uknown word \"{}\"'.format(word))\n        return self.embeddings[self.word2id[word]]\n\n    def get_vectors(self, *words):\n        word_ids = [self.word2id[i] for i in words]\n        vectors = np.stack([self.embeddings[i] for i in word_ids], axis=0)\n        return vectors","566b21c0":"train_tokens, word2id, word2freq = TextPreprocessor(config['TextPreprocessor']).transform(df_train)\ntest_tokens, _, _ = TextPreprocessor(config['TextPreprocessor']).transform(df_test)","c838c690":"train_token_ids = TextPreprocessor(config['TextPreprocessor']).texts_to_token_ids(test_tokens, word2id)\ntest_token_ids = TextPreprocessor(config['TextPreprocessor']).texts_to_token_ids(test_tokens, word2id)","2b84fbea":"train_dataset = PaddedSequenceDataset(\n    config['PaddedSequenceDataset'], \n    train_token_ids, \n    np.zeros(len(train_token_ids))\n)\ntest_dataset = PaddedSequenceDataset(\n    config['PaddedSequenceDataset'],\n    test_token_ids, \n    np.zeros(len(test_token_ids))\n)","37dd8345":"model = SkipGramNegativeSamplingTrainer(config['SkipGramNegativeSamplingTrainer'], len(word2id))","e9603479":"best_val_loss, best_model = train_eval_loop(\n    model,\n    train_dataset,\n    test_dataset,\n    no_loss,\n    lr=config['train']['lr'],\n    epoch_n=config['train']['epoch_n'],\n    batch_size=config['train']['batch_size'],\n    early_stopping_patience=config['train']['early_stopping_patience'],\n    max_batches_per_epoch_train=config['train']['max_batches_per_epoch_train'],\n    max_batches_per_epoch_val=len(test_dataset),\n    lr_scheduler_ctor=lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=1, verbose=True)\n)","9ae68817":"embeddings = Embeddings(\n    config['Embeddings'], \n    model.center_emb.weight.detach().cpu().numpy(), \n    word2id\n)","fdc17914":"train_tokens_clean = []\nfor i in train_tokens:\n    i = [w if w in word2id else '<PAD>' for w in i]\n    train_tokens_clean.append(i)","a0432262":"test_tokens_clean = []\nfor i in test_tokens:\n    i = [w if w in word2id else '<PAD>' for w in i]\n    test_tokens_clean.append(i)","6f405b6a":"embeddings.most_similar('california')","fc0e7943":"embeddings.most_similar('japan')","5585e20b":"embeddings.most_similar('forest')","d4fa37e5":"<a id='0'><\/a>\n\n+ [Classes](#Classes) \n+ [Pipline](#Pipline)\n  + [Train](#Train)\n  + [Test](#Test)","1b5ef653":"<a id='Pipline'><\/a>\n## Pipline","ead79696":"<a id='Train'><\/a>\n### Train","770aca80":"<a id='Classes'><\/a>\n## Classes","7403c3cd":"[Home](#0)","19fbeb03":"<a id='Test'><\/a>\n### Test","8f456c4c":"[Home](#0)","ad947a7d":"# Custom word2vec with PyTorch","f4ad2cfa":"[Home](#0)"}}