{"cell_type":{"df78e3fc":"code","d30168b7":"code","62ee3066":"code","e1f89d99":"code","db73cde2":"code","e24e7ed9":"code","41d60678":"code","980eb1e6":"code","0aaf3069":"code","4d14ff83":"code","36bf95f6":"code","b963ff53":"code","db2d9acc":"code","1327d425":"code","e8cc55fb":"code","b1a0b7cd":"code","ed88c6ed":"code","fd7aedd8":"code","fc171083":"code","3118f6db":"code","5249798e":"code","dea94114":"code","6dd854f7":"code","7631d81b":"code","29d99ecf":"code","f876a601":"code","ea8bb322":"code","0293ad08":"code","0d607d33":"code","a6e85cbe":"code","02990722":"code","4d5c052d":"code","419c8572":"code","1e6c4488":"code","33c60203":"code","ecbc2fc2":"code","738ea7c1":"code","0247d9ba":"code","0024f3bb":"code","18ff4eef":"markdown","eb4c32e7":"markdown","b1f60945":"markdown","f313ff27":"markdown"},"source":{"df78e3fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    print(dirname)\n    #for filename in filenames:\n    #   print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d30168b7":"# importing libraries \nimport nltk \nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize, sent_tokenize \n\nnltk.download('stopwords')\nnltk.download('punkt')\n\nfrom string import punctuation","62ee3066":"# Input text - to summarize  \ntext = open(\"\/kaggle\/input\/d\/aggarwalrahul\/nlp-specialization-data\/India_text_summ.txt\",'r',encoding='Latin-1')\ntext=text.read()\nprint(text)","e1f89d99":"# Tokenizing the text \nstopWords = list(stopwords.words(\"english\"))+list(punctuation)+list([0,1,2,3,4,5,6,7,8,9])\nstopWords[15:20]","db73cde2":"words = word_tokenize(text) \nwords[15:25]","e24e7ed9":"freqTable={}\n   \nfor word in words: \n    word = word.lower() \n    if word not in stopWords:\n        if word in freqTable: \n            freqTable[word] += 1\n        else: \n            freqTable[word] = 1","41d60678":"print(freqTable)","980eb1e6":"print(freqTable.items())","0aaf3069":"print(sorted(freqTable.items(), key = lambda x: x[1]))","4d14ff83":"sentences = sent_tokenize(text) \nfor sen in sentences:\n    print(sen,\"\\n\")","36bf95f6":"# Creating a dictionary to keep the score of each sentence \nsentence_weight = dict() \nfor sentence in sentences: \n    # print(sentence,'\\n')\n    for word, freq in freqTable.items():\n        # print('\\n' , word, freq)\n        if word in sentence.lower(): \n            # print('sentence_weight    ',sentence_weight)\n            if sentence in sentence_weight:     \n                sentence_weight[sentence] += freq \n            else: \n                sentence_weight[sentence] = freq \nsentence_weight","b963ff53":"sumValues = 0\nfor sentence in sentence_weight: \n    sumValues += sentence_weight[sentence] \nsumValues   ","db2d9acc":"# Average value of a sentence from the original text \naverage = int(sumValues \/ len(sentence_weight)) \n# print(average,sumValues,len(sentence_weight),sep='\\n\\n')\naverage","1327d425":"# Storing sentences into our summary. \nsummary = '' \ncounter=0\nfor sentence in sentences: \n    if (sentence in sentence_weight) and (sentence_weight[sentence] > (1.25* average)): \n        summary += \" \" + sentence \n        counter+=1\nprint(counter,summary,sep='\\n\\n')","e8cc55fb":"from gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = '\/kaggle\/input\/glove6b\/glove.6B.100d.txt'","b1a0b7cd":"# Extract word vectors\nword_embeddings = {}\nf = open(glove_input_file, encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","ed88c6ed":"len(word_embeddings)","fd7aedd8":"type(word_embeddings)","fc171083":"word_embeddings['the'].shape","3118f6db":"word_embeddings['the']","5249798e":"print(len(sentences))\nprint(sentences)","dea94114":"sentence_vectors = []\nfor i in sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])\/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)","6dd854f7":"len(sentence_vectors)","7631d81b":"sentence_vectors[21].shape","29d99ecf":"# similarity matrix\nsim_mat = np.zeros([len(sentences), len(sentences)])\nsim_mat.shape","f876a601":"sim_mat","ea8bb322":"i=0\nj=1\ncosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0][0]","0293ad08":"from sklearn.metrics.pairwise import cosine_similarity\n\nfor i in range(len(sentences)):\n    for j in range(len(sentences)):\n        if i != j:\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n\nprint(sim_mat.shape)","0d607d33":"sim_mat[0,:]","a6e85cbe":"sim_mat[21,:] ","02990722":"import networkx as nx\n\nnx_graph = nx.from_numpy_array(sim_mat)\nscores = nx.pagerank(nx_graph)","4d5c052d":"?nx.pagerank","419c8572":"ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)","1e6c4488":"# Specify number of sentences to form the summary\nsn = 5\n\n# Generate summary\nfor i in range(sn):\n    print(ranked_sentences[i][1])","33c60203":"! pip install numpy==1.20.0\n! pip install gensim==3.8.1","ecbc2fc2":"# Input text - to summarize  \ntext = open(\"\/kaggle\/input\/d\/aggarwalrahul\/nlp-specialization-data\/India_text_summ.txt\",'r',encoding='Latin-1')\ntext=text.read()\nprint(text)","738ea7c1":"## https:\/\/radimrehurek.com\/gensim_3.8.3\/summarization\/summariser.html\nfrom gensim.summarization import summarize","0247d9ba":"summarize(text,ratio = 0.25)\n","0024f3bb":"summarize(text, word_count = 125)","18ff4eef":"# Text Summarization - Manual Approach\n\nText summarization refers to the technique of shortening long pieces of text. The intention is to create a coherent and fluent summary having only the main points outlined in the document.\nAutomatic text summarization is a common problem in machine learning and natural language processing (NLP).\n\n* Input data\n* Word Tokenization\n* Stopwrod removal\n* Identify weight of each word\n* identify sentence weight on the basis of word weights\n* populate the top 5-10 maximum weight sentences","eb4c32e7":"# 3. Text Summarization Using TextRank","b1f60945":"# 3. Text Summarization Using Gensim\n\ngensim.summarization.summarizer.summarize(text, ratio=0.2, word_count=None, split=False)\n\n__Parameters__\n- text (str) \u2013 Given text.\n\n- ratio (float, optional) \u2013 Number between 0 and 1 that determines the proportion of the number of sentences of the original text to be chosen for the summary.\n\n- word_count (int or None, optional) \u2013 Determines how many words will the output contain. If both parameters are provided, the ratio will be ignored.\n\n- split (bool, optional) \u2013 If True, list of sentences will be returned. Otherwise joined strings will bwe returned.","f313ff27":"# Further Readings :\n* Text summarization using TextRank in NLP - https:\/\/medium.com\/data-science-in-your-pocket\/text-summarization-using-textrank-in-nlp-4bce52c5b390\n* Find the original article here - https:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/introduction-text-summarization-textrank-python\/\n* Cosine Similarity explanation - https:\/\/medium.com\/datadriveninvestor\/cosine-similarity-cosine-distance-6571387f9bf8"}}