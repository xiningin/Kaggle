{"cell_type":{"e38f5f2e":"code","71a76d31":"code","da29ff0d":"code","79abc96b":"code","f2710dd6":"code","acbdfca4":"code","7869151e":"code","4e1e9569":"code","cf4728d1":"code","1494f8aa":"code","31eeb7d3":"code","9539609c":"code","8b61f3cc":"code","a34f2a37":"code","39175acf":"markdown","de4154dc":"markdown","bbde4dbb":"markdown","429970fc":"markdown","5f062ea0":"markdown","1e77d10f":"markdown","da946260":"markdown","0fe4d321":"markdown","f3661735":"markdown","d9030b74":"markdown","6aec868d":"markdown","010ff3c0":"markdown"},"source":{"e38f5f2e":"import numpy as np\nimport os\nimport glob\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom random import shuffle\nfrom skimage.util.shape import view_as_blocks\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.convolutional import Convolution2D\nimport warnings\nwarnings.filterwarnings('ignore')","71a76d31":"train_size = 10000\ntest_size = 3000\n\ntrain = glob.glob(\"..\/input\/dataset\/train\/*.jpeg\")\ntest = glob.glob(\"..\/input\/dataset\/test\/*.jpeg\")\n\nshuffle(train)\nshuffle(test)\n\ntrain = train[:train_size]\ntest = test[:test_size]\n\npiece_symbols = 'prbnkqPRBNKQ'","da29ff0d":"def fen_from_filename(filename):\n  base = os.path.basename(filename)\n  return os.path.splitext(base)[0]","79abc96b":"print(fen_from_filename(train[0]))\nprint(fen_from_filename(train[1]))\nprint(fen_from_filename(train[2]))","f2710dd6":"f, axarr = plt.subplots(1,3, figsize=(120, 120))\n\nfor i in range(0,3):\n    axarr[i].set_title(fen_from_filename(train[i]), fontsize=70, pad=30)\n    axarr[i].imshow(mpimg.imread(train[i]))\n    axarr[i].axis('off')","acbdfca4":"def onehot_from_fen(fen):\n    eye = np.eye(13)\n    output = np.empty((0, 13))\n    fen = re.sub('[-]', '', fen)\n\n    for char in fen:\n        if(char in '12345678'):\n            output = np.append(\n              output, np.tile(eye[12], (int(char), 1)), axis=0)\n        else:\n            idx = piece_symbols.index(char)\n            output = np.append(output, eye[idx].reshape((1, 13)), axis=0)\n\n    return output\n\ndef fen_from_onehot(one_hot):\n    output = ''\n    for j in range(8):\n        for i in range(8):\n            if(one_hot[j][i] == 12):\n                output += ' '\n            else:\n                output += piece_symbols[one_hot[j][i]]\n        if(j != 7):\n            output += '-'\n\n    for i in range(8, 0, -1):\n        output = output.replace(' ' * i, str(i))\n\n    return output","7869151e":"def process_image(img):\n    downsample_size = 200\n    square_size = int(downsample_size\/8)\n    img_read = io.imread(img)\n    img_read = transform.resize(\n      img_read, (downsample_size, downsample_size), mode='constant')\n    tiles = view_as_blocks(img_read, block_shape=(square_size, square_size, 3))\n    tiles = tiles.squeeze(axis=2)\n    return tiles.reshape(64, square_size, square_size, 3)","4e1e9569":"def train_gen(features, labels, batch_size):\n    for i, img in enumerate(features):\n        y = onehot_from_fen(fen_from_filename(img))\n        x = process_image(img)\n        yield x, y\n\ndef pred_gen(features, batch_size):\n    for i, img in enumerate(features):\n        yield process_image(img)","cf4728d1":"model = Sequential()\nmodel.add(Convolution2D(32, (3, 3), input_shape=(25, 25, 3)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(Convolution2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(13))\nmodel.add(Activation('softmax'))\nmodel.compile(\n  loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","1494f8aa":"model.fit_generator(train_gen(train, None, 64), steps_per_epoch=train_size)","31eeb7d3":"res = (\n  model.predict_generator(pred_gen(test, 64), steps=test_size)\n  .argmax(axis=1)\n  .reshape(-1, 8, 8)\n)","9539609c":"pred_fens = np.array([fen_from_onehot(one_hot) for one_hot in res])\ntest_fens = np.array([fen_from_filename(fn) for fn in test])\n\nfinal_accuracy = (pred_fens == test_fens).astype(float).mean()\n\nprint(\"Final Accuracy: {:1.5f}%\".format(final_accuracy))","8b61f3cc":"def display_with_predicted_fen(image):\n    pred = model.predict(process_image(image)).argmax(axis=1).reshape(-1, 8, 8)\n    fen = fen_from_onehot(pred[0])\n    imgplot = plt.imshow(mpimg.imread(image))\n    plt.axis('off')\n    plt.title(fen)\n    plt.show()","a34f2a37":"display_with_predicted_fen(test[0])\ndisplay_with_predicted_fen(test[1])\ndisplay_with_predicted_fen(test[2])","39175acf":"### Functions to convert FEN to one-hot encoded vectors and vice-versa","de4154dc":"### Testing the model:","bbde4dbb":"### Define a model:","429970fc":"### Train the model:","5f062ea0":"### Plotting samples","1e77d10f":"### Data Import","da946260":"### Calculating an accuracy of the model:","0fe4d321":"### Functions for sampling batches for training and evaluation:","f3661735":"### Function to extract FEN from filename","d9030b74":"### Sample images and display predicted FEN ","6aec868d":"#### Examples:","010ff3c0":"#### Function to proccess an image:\n-  downsample an image to 200 by 200 pixel\n-  split an image of the chess board to 64 images of individual squares\n-  drop redundant dimensions, reshape"}}