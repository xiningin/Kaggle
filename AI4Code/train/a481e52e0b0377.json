{"cell_type":{"8569ea6d":"code","5df33225":"code","ce2636cc":"code","ccbc6f7d":"code","283038fb":"code","3a685fe2":"code","7d01a3e5":"code","4362d5cb":"code","2e4e0efb":"code","9bc71e36":"code","e81cd544":"code","57dfcf27":"code","f4c62b78":"code","dfd9d4ed":"code","7248ddd9":"code","51c487fa":"code","86093a04":"code","729ba874":"code","d4653f74":"code","f0fd2921":"code","f8a36366":"code","1ba5ea31":"code","ef2d204a":"code","6038cb9b":"code","8d0b8772":"code","185e79c6":"code","07446487":"code","e1f19942":"code","01d657c6":"code","9bc6f0a7":"code","856172e1":"code","94935faf":"code","7bd51722":"code","7066aeca":"code","04da00d3":"code","99d36887":"code","8ae68915":"code","ea252c5f":"code","087a25c6":"code","f9598cb1":"code","8139617f":"code","7b52b526":"code","ee7bd14b":"code","9669bde6":"code","99acfc3b":"code","50e6720b":"code","faf1e433":"code","fe9aa93f":"code","ad4559aa":"code","2c7fb72f":"code","d2a643b4":"code","bee0642b":"code","721c077a":"markdown","3ebf1e9e":"markdown","f500514b":"markdown","5411b683":"markdown","1fb54739":"markdown","436d61d8":"markdown","ce408b6f":"markdown","9344adc1":"markdown","b76523d4":"markdown","2c9c77a0":"markdown","a231c2a8":"markdown","f3a1b6be":"markdown","0db7408e":"markdown","c72ca898":"markdown","6fbe2378":"markdown","a259b0ce":"markdown","cc347d31":"markdown","87e04a1e":"markdown","c6b24fec":"markdown"},"source":{"8569ea6d":"import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\nfrom sklearn.impute import SimpleImputer\n# data standardization with  sklearn\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport cufflinks as cf\ncf.go_offline()\n\n# Preprocessing, modelling and evaluating\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport datetime\nimport lightgbm as lgb\n","5df33225":"data_set_path = '\/kaggle\/input\/credit-risk-classification-dataset\/'","ce2636cc":"cust_df= pd.read_csv(data_set_path+'customer_data.csv')\ncust_pymt_df= pd.read_csv(data_set_path+'payment_data.csv')","ccbc6f7d":"print('Size of Custommer data', cust_df.shape)\nprint('Size of Payment details', cust_pymt_df.shape)","283038fb":"cust_df.head()","3a685fe2":"cust_pymt_df.head()","7d01a3e5":"cust_df.describe(include='all')","4362d5cb":"# checking missing data\ndef get_missing_data(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = (df.isnull().sum()\/df.isnull().count()*100).sort_values(ascending = False)\n    percent = round(percent,2)\n    missing_df  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    print( missing_df.head(20))\n    return None","2e4e0efb":"get_missing_data(cust_df)","9bc71e36":"get_missing_data(cust_pymt_df)","e81cd544":"def data_profile(df):\n    stats = []\n    for col in df.columns:\n        stats.append((col, df[col].nunique(), df[col].isnull().sum() * 100 \/ df.shape[0], df[col].value_counts(normalize=True, dropna=False).values[0] * 100, df[col].dtype))\n\n    stats_df = pd.DataFrame(stats, columns=['Feature', 'Unique_values', 'Percentage of missing values', 'Percentage of values in the biggest category', 'type'])\n    stats_df.sort_values('Percentage of missing values', ascending=True)\n    return stats_df","57dfcf27":"data_profile(cust_df)","f4c62b78":"cust_df['fea_2'].fillna(cust_df['fea_2'].mean(),inplace=True)","dfd9d4ed":"cust_pymt_df['highest_balance'].fillna(0,inplace=True)","7248ddd9":"cust_pymt_df_prd = cust_pymt_df.groupby(['id','prod_code']).agg({'new_balance':'mean'}).reset_index().rename(columns={'new_balance':'new_balance_avg'})","51c487fa":"cust_pymt_df_prd = cust_pymt_df_prd[['id','prod_code','new_balance_avg']].drop_duplicates(keep='first')\ncust_df_mrg = pd.merge(cust_df,cust_pymt_df[['id','prod_code']],left_on=['id'],right_on=['id'],how='inner')","86093a04":"temp = cust_df[\"label\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\nplt.figure(figsize = (6,6))\nplt.title('Application Credit repayed - train dataset')\nsns.set_color_codes(\"pastel\")\nsns.barplot(x = 'labels', y=\"values\", data=df)\nlocs, labels = plt.xticks()\nplt.show()","729ba874":"cust_pymt_df_prd","d4653f74":"# Plot distribution of multiple features, with TARGET = 1\/0 on the same graph\ndef plot_distribution_comp(df,label,var,nrow=2):\n    \n    i = 0\n    t1 = df.loc[df[label] != 0]\n    t0 = df.loc[df[label] == 0]\n\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(nrow,2,figsize=(12,6*nrow))\n\n    for feature in var:\n        i += 1\n        plt.subplot(nrow,2,i)\n        sns.kdeplot(t1[feature], bw=0.5,label=\"label = 1\")\n        sns.kdeplot(t0[feature], bw=0.5,label=\"label = 0\")\n        plt.ylabel('Density plot', fontsize=12)\n        plt.xlabel(feature, fontsize=12)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();","f0fd2921":"print('Minimum new balance avg in the dataset :- ',cust_pymt_df_prd['new_balance_avg'].min())\nprint('Maximum new balance avg in the dataset :- ',cust_pymt_df_prd['new_balance_avg'].max())","f8a36366":"sns.distplot(np.log(cust_pymt_df_prd['new_balance_avg']+1))","1ba5ea31":"# Find correlations with the target and sort\ncorrelations = cust_df.corr()['label'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","ef2d204a":"correlations = cust_df.corr()\nplt.figure(figsize = (14, 12))\nsns.heatmap(correlations,cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","6038cb9b":"sns.pairplot(cust_df)","8d0b8772":"cust_df.columns","185e79c6":"Total_features=['fea_1', 'fea_2', 'fea_3', 'fea_4', 'fea_5', 'fea_6','fea_7', 'fea_8', 'fea_9', 'fea_10', 'fea_11']","07446487":"cat_features = ['fea_3','fea_5','fea_6','fea_9']\nnum_features = ['fea_1','fea_2','fea_4','fea_7','fea_8','fea_10','fea_11']","e1f19942":"def standerdisation(df,num_cols):\n# copy of datasets\n    df_stand = df.copy()\n\n    # apply standardization on numerical features\n    for i in num_cols:\n\n        # fit on training data column\n        scale = StandardScaler().fit(df_stand[[i]])\n\n        # transform the training data column\n        df_stand[i] = scale.transform(df_stand[[i]])\n        \n    return df_stand","01d657c6":"cust_df_std =  standerdisation(cust_df,num_features)","9bc6f0a7":"feat_cols=[cols for cols  in cust_df_std.columns if cols not in ['label','id']]","856172e1":"X_train,X_test,y_train,y_test=train_test_split(cust_df_std[feat_cols],cust_df_std[['label']],test_size=0.15)","94935faf":"X_train.reset_index(inplace=True)\nX_test.reset_index(inplace=True)\ny_train.reset_index(inplace=True)\ny_test.reset_index(inplace=True)\n\nX_train.drop(columns=['index'],axis=1,inplace=True)\nX_test.drop(columns=['index'],axis=1,inplace=True)\ny_train.drop(columns=['index'],axis=1,inplace=True)\ny_test.drop(columns=['index'],axis=1,inplace=True)\n\nprint('Shape of Train features :-',X_train.shape)\nprint('Shape of Test features  :-',X_test.shape)\n\nprint('Shape of Train Target   :-',y_train.shape)\nprint('Shape of Test Target    :-',y_test.shape)\ny_train = y_train['label'].copy()\ny_test = y_test['label'].copy()","7bd51722":"params = {'application': 'binary',\n          'boosting': 'gbdt',\n          'metric': 'auc',\n          'max_depth': 16,\n          'learning_rate': 0.05,\n          'bagging_fraction': 0.9,\n          'feature_fraction': 0.9,\n          'verbosity': -1,\n          'lambda_l1': 0.1,\n          'lambda_l2': 0.01,\n          'num_leaves': 500,\n          'min_child_weight': 3,\n          'data_random_seed': 17,\n         'nthreads':4}\n\n\nearly_stop = 500\nverbose_eval = 30\nnum_rounds = 600\n\nfolds = 5\nseed =10\nkf = KFold(n_splits = folds, shuffle = True, random_state=seed)\ndebug =False","7066aeca":"y_preds2 = np.zeros(X_test.shape[0])\nfeature_importance_df = pd.DataFrame()\ni = 0\nfor tr_idx, val_idx in kf.split(X_train, y_train):\n\n    train_X = X_train[feat_cols].iloc[tr_idx]\n    val_X = X_train[feat_cols].iloc[val_idx]\n    \n    train_y = y_train.iloc[tr_idx]\n    val_y = y_train.iloc[val_idx]\n    \n    lgb_train = lgb.Dataset(train_X, train_y, categorical_feature=cat_features)\n    lgb_eval = lgb.Dataset(val_X, val_y, categorical_feature=cat_features)\n    \n    model = lgb.train(params,\n                      train_set=lgb_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=(lgb_train, lgb_eval),\n                      verbose_eval=100)\n        \n    \n    y_preds2+= model.predict(X_test) \/ folds\n    \n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = train_X.columns\n    fold_importance_df[\"importance\"] = model.feature_importance()\n    fold_importance_df[\"fold\"] = i + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    if debug:    \n        print(\"debug:\",roc_auc_score(y_test, model.predict(X_test) \/ folds))  \n    i+=1\n    del train_X,lgb_train","04da00d3":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:100].index)\nprint(cols)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(10,15))\nsns.barplot(x=\"importance\",\n            y=\"Feature\",\n            data=best_features.sort_values(by=\"importance\",\n                                           ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()","99d36887":"cust_df[cust_df['id']==54982665]","8ae68915":"cust_pymt_df[cust_pymt_df['id']==54982665]","ea252c5f":"test = cust_pymt_df[cust_pymt_df['id']==54982665]","087a25c6":"test.groupby(['id','prod_code'])\\\n.agg({'new_balance':'mean','highest_balance':'mean','OVD_t1':'sum','OVD_t2':'sum','OVD_t3':'sum','OVD_sum':'sum','pay_normal':'mean','prod_limit':'max'})\\\n.reset_index()","f9598cb1":"cust_pymt_df_smmry = cust_pymt_df.groupby(['id','prod_code'])\\\n.agg({'new_balance':'mean','highest_balance':'mean','OVD_t1':'sum','OVD_t2':'sum','OVD_t3':'sum','OVD_sum':'sum','pay_normal':'mean'})\\\n.reset_index()","8139617f":"cust_pymt_df_smmry","7b52b526":"cust_pymt_df_smmry_out = pd.get_dummies(cust_pymt_df_smmry, columns=[\"prod_code\"])","ee7bd14b":"cust_pymt_df_smmry.columns\n\n['new_balance', 'highest_balance', 'OVD_t1', 'OVD_t2','OVD_t3', 'OVD_sum', 'pay_normal']","9669bde6":"cust_df.shape","99acfc3b":"cust_pymt_df_smmry['credit_positive']=0\ncust_pymt_df_smmry.loc[cust_pymt_df_smmry['new_balance'] >= 0,'credit_positive']=1\ncust_pymt_df_smmry['new_balance'] = abs(cust_pymt_df_smmry['new_balance'])\ncust_pymt_df_smmry['new_balance_mean'] = abs(cust_pymt_df_smmry['new_balance'] - cust_pymt_df_smmry['new_balance'].mean())\ncust_pymt_df_smmry['new_balance_min_std'] = cust_pymt_df_smmry['new_balance_mean'] \/ cust_pymt_df_smmry['new_balance'].std()","50e6720b":"cust_pymt_df_smmry","faf1e433":"cust_pymt_df_smmry['new_balance_log']=np.log(cust_pymt_df_smmry['new_balance']+1)\ncust_pymt_df_smmry['new_balance_min_std_log']=np.log(cust_pymt_df_smmry['new_balance_min_std']+1)\ncust_pymt_df_smmry['new_balance_mean_log']=np.log(cust_pymt_df_smmry['new_balance_mean']+1)","fe9aa93f":"cust_pymt_df_smmry.pivot(index='id', columns='prod_code', values=['new_balance_log', 'highest_balance', 'OVD_t1', 'OVD_t2','OVD_t3', 'OVD_sum', 'pay_normal','new_balance_min_std_log','new_balance_mean_log']).reset_index().fillna(0)","ad4559aa":"'''\nIn this dataset, each customer is classified as high or low credit risk according to the set of features and payment history. If label is 1, the customer is in high credit risk. Dataset imbalance ratio is 20%.\n\nData:\npayment_data.csv: customer\u2019s card payment history.\n\tid: customer id\n\tOVD_t1: number of times overdue type 1\n\tOVD_t2: number of times overdue type 2\n\tOVD_t3: number of times overdue type 3\n\tOVD_sum: total overdue days\n\tpay_normal: number of times normal payment\n\tprod_code: credit product code\n\tprod_limit: credit limit of product\n\tupdate_date: account update date\n\tnew_balance: current balance of product\n\thighest_balance: highest balance in history\n\treport_date: date of recent payment\ncustomer_data.csv: \n\n       customer\u2019s demographic data and category attributes which have been encoded. Category features are fea_1, fea_3, fea_5, fea_6, fea_7, fea_9.\n       label is 1, the customer is in high credit risk\n       label is 0, the customer is in low credit risk\n\nTasks:\n\u2022\tExplore data to give insights.\n\u2022\tBuild features from existing payment data.\n\u2022\tBuild model to predict high risk customer \n\u2022\tModel explanation and evaluation\n\n\n\n       customer\u2019s demographic data and category attributes which have been encoded. \n       Category features are fea_1, fea_3, fea_5, fea_6, fea_7, fea_9.\n       label is 1, the customer is in high credit risk\n       label is 0, the customer is in low credit risk\n'''","2c7fb72f":"cust_prd = cust_pymt_df[['id','prod_code']].drop_duplicates(keep='first')","d2a643b4":"cust_prd['id'].value_counts()","bee0642b":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of new_balance\")\nax = sns.distplot(cust_pymt_df[\"new_balance\"])","721c077a":"# <a id='1'>1. Introduction<\/a>","3ebf1e9e":"# <a id='4'> 4 Check for missing data<\/a>","f500514b":"### Import necessary libraries","5411b683":"## Check the balance of classes","1fb54739":"## Preparing data for the model","436d61d8":"# Defining Hyperparameters for Model","ce408b6f":"# Feature Importance","9344adc1":"# <a id='3'>3. Glimpse of Data<\/a>","b76523d4":"# <a id='5'>5. Data Exploration<\/a>","2c9c77a0":"## <a id='5-1'>5.1 Distribution of new_balance<\/a>","a231c2a8":" Customer Credit in Banking is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.","f3a1b6be":"## <a name=\"Introduction-Credit-Risk-Analysis\">Introduction : Credit Risk Analysis<\/a>\n\n#### <a name=\"credit_risk\"> Credit risk<\/a>\nCredit Risk is the probable risk of loss resulting from a borrower's failure to repay a loan or meet contractual obligations. If a company offers credit to its client,then there is a risk that its clients may not pay their invoices.                                \n\n#### <a name=\"credit_risk_types\">Types of Credit Risk<\/a>           \n __Good Risk__: An investment that one believes is likely to be profitable. The term most often refers to a loan made to a creditworthy person or company. Good risks are considered exceptionally likely to be repaid.               \n __Bad Risk__: A loan that is unlikely to be repaid because of bad credit history, insufficient income, or some other reason. A bad risk increases the risk to the lender and the likelihood of default on the part of the borrower.\n\n\n####  <a name=\"objective\">Objective<\/a>: \nBased on the attributes, classify a person as good or bad credit risk.\n#### <a name=\"dataset_description\">Dataset Description<\/a>: \nThe dataset contains 8250 entries for payments and 1125 Customers with 24 independent variables  and 1 target variable . In this dataset, each entry represents a person who takes a credit by a bank. Each person is classified as good or bad credit risks according to the set of attributes.The attributes are:                                  \n\n\n#### <a name=\"target_variable\">Target Variable<\/a>                                        \n* __Cost Matrix__  \n    * 1 = __Good Risk__\n    * 2 = __Bad Risk__\n","0db7408e":"# Modeling","c72ca898":"# ...In Progress","6fbe2378":"- <a href='#1'>1. Introduction<\/a>  \n- <a href='#2'>2. Retrieving the Data<\/a>\n- <a href='#3'>3. Glimpse of Data<\/a>\n- <a href='#4'> 4. Check for missing data<\/a>\n- <a href='#5'>5. Data Exploration<\/a>\n    - <a href='#5-1'>5.1 Distribution of New_balance<\/a>\n    - <a href='#5-2'>5.2 Distribution of high_balance<\/a>\n- <a href='#6'>6. Pearson Correlation of features<\/a>\n- <a href='#7'>7. Feature Importance using Random forest<\/a>","a259b0ce":"### Correlation - Heatmap","cc347d31":"### Checking the Overall Distribution","87e04a1e":" # <a id='2'>2. Retrieving the Data<\/a>","c6b24fec":"### Persona Level understanding"}}