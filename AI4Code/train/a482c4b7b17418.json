{"cell_type":{"12d6917f":"code","f19d38b8":"code","19c9a8da":"code","c0db5d5d":"code","036cc0bf":"code","ba8203a8":"code","fe1ac938":"code","1a56e83e":"code","4c0824a2":"code","22006f13":"code","c07896b7":"markdown","6888592e":"markdown","6f207748":"markdown","3285664e":"markdown","9ef7f461":"markdown","90a238df":"markdown"},"source":{"12d6917f":"# install Light Auto ML\n!pip install -U lightautoml","f19d38b8":"# Standard python libraries\nimport logging\nimport os\nimport time\nimport requests\nlogging.basicConfig(format='[%(asctime)s] (%(levelname)s): %(message)s', level=logging.INFO)\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport torch\n\n# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.tasks import Task\n","19c9a8da":"# Read the transformed test data\ntest_data = pd.read_csv('..\/input\/new-with-calibrated\/test_5_new_models.csv')\n# Read the transformed training data\ntrain_data = pd.read_csv('..\/input\/new-with-calibrated\/X_5_new_models.csv')\ntrain_data['target'] = pd.read_csv('..\/input\/new-with-calibrated\/y.csv')\n# Read the submission data\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","c0db5d5d":"# Define the run attributes\nN_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 3600 # Time in seconds for automl run - UPDATED VALUE FOR UTILIZATION\nTARGET_NAME = 'target' # Target column name","036cc0bf":"%%time\n# COMPETITION METRIC IS MAE SO WE SET IT FOR OUR TASK\ntask = Task('multiclass', metric='crossentropy')","ba8203a8":"%%time\n\nroles = {'target': TARGET_NAME}","fe1ac938":"%%time \n# CHANGED TabularAutoML to TabularUtilizedAutoML for timeout utilization\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'n_jobs': N_THREADS, 'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                      )\noof_pred = automl.fit_predict(train_data, roles = roles)\nlogging.info('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))","1a56e83e":"%%time\n\n# Fast feature importances calculation\nfast_fi = automl.get_feature_scores('fast')\nfast_fi.set_index('Feature')['Importance'].plot.bar(figsize = (20, 10), grid = True)","4c0824a2":"%%time\n\ntest_pred = automl.predict(test_data)\nlogging.info('Prediction for test data:\\n{}\\nShape = {}'\n              .format(test_pred, test_pred.shape))\n\nlogging.info('Check scores...')\nlogging.info('OOF score: {}'.format(mean_absolute_error(train_data[TARGET_NAME].values, oof_pred.data[:, 0])))","22006f13":"submission.iloc[:, 1:] = test_pred.data\nsubmission.to_csv('stacking_automl_5_with_calibrated_models.csv', index=False)","c07896b7":"# ================","6888592e":"# ![m'lady](https:\/\/repository-images.githubusercontent.com\/64991887\/dc855780-e34b-11ea-9ab8-e08ca33288b0)","6f207748":"# ================","3285664e":"# <span style=\"color:black\">***LightAutoML for Stacking***<\/span>\n#### <span style=\"color:black\">Hellow guys, here I am providing you with my second stage solution (staking). Hope you will enjoy it <\/span>.\n#### The main code is provided by [LightAutoML HW_1 with custom FE](https:\/\/www.kaggle.com\/alexryzhkov\/lightautoml-hw-1-with-custom-fe)","9ef7f461":"# *Good Luck!*","90a238df":"##### * The transformed data is mine. \n##### * It is the result from the prediction five models (LightGBM, XGBoost, CatBoost, Random Forest, and Logistic Regression). \n##### * For tree-based models, the data was left as is because they are already in the form of \"*Label Encoded*\" Data, while one-hot encoding was used for Logistic Regression as it expects the data to be normalized."}}