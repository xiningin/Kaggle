{"cell_type":{"ef949693":"code","fb45cfc7":"code","9427bb54":"code","250cf4fb":"code","46980f5b":"code","aeb3e7dc":"code","5f94081b":"code","7fb82851":"code","72ba8d6b":"code","fb423ee5":"code","5abc0a57":"code","8a8a35a7":"code","32a6ea28":"code","e74cb705":"code","009eac40":"code","da4e6a59":"code","2756cec8":"code","aa05e874":"code","ef15a06c":"code","b6903ed8":"code","cc81c8c6":"code","95173270":"code","73893693":"code","5864bac8":"markdown","d5809762":"markdown","f7a3f3eb":"markdown","da099d42":"markdown","e3d22e45":"markdown"},"source":{"ef949693":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb45cfc7":"!pip install -q efficientnet\n\nimport efficientnet.tfkeras as efn","9427bb54":"import re\nimport math\nimport numpy as np\nimport seaborn as sns","250cf4fb":"from kaggle_datasets import KaggleDatasets\nfrom matplotlib import pyplot as plt\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers","46980f5b":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\n    \nexcept ValueError:\n    tpu = None\n    \n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n    \nprint('REPLICAS : -> ', strategy.num_replicas_in_sync)","aeb3e7dc":"GCS_DS_PATH = KaggleDatasets().get_gcs_path('bach-tfrecords')","5f94081b":"TRAINING_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '\/train.*tfrecords')\nVALIDATION_FILENAMES = tf.io.gfile.glob(GCS_DS_PATH + '\/val.*tfrecords')\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nIMAGE_SIZE = [192,192]\nprint(TRAINING_FILENAMES)\nprint(VALIDATION_FILENAMES)\nprint(BATCH_SIZE)\nprint(IMAGE_SIZE)","7fb82851":"EPOCHS = 10\nSTEPS_PER_EPOCH = 15000 \/\/ BATCH_SIZE","72ba8d6b":"def decode_image(image_data):\n    print('About to decode image data...')\n    #image = tf.image.decode_jpeg(image_data, channels = 3)\n    #image = tf.io.parse_tensor(image_data, out_type = tf.uint8)\n    image = tf.io.decode_raw(image_data, tf.uint8)\n    print('Decoded JPEG...')\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n    print('Done decoding image')\n    return image","fb423ee5":"def read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image_raw\" : tf.io.FixedLenFeature([], tf.string),\n        \"label\" : tf.io.FixedLenFeature([], tf.int64)\n    }\n    \n    print('About to parse labeled tfrecord...')\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image_raw'])\n    print('Read Image Data...')\n    label = tf.cast(example['label'], tf.int32)\n    return image, label","5abc0a57":"def load_dataset(filenames, labeled = True, ordered = False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False\n    \n    print('About to Load TFRECORD Dataset...')\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads = tf.data.experimental.AUTOTUNE)\n    dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n    print('Read labeled TFRecords...')\n    return dataset","8a8a35a7":"def get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled = True)\n    print('loaded training dataset')\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    print('Successfully loaded training dataset')\n    return dataset","32a6ea28":"def get_validation_dataset():\n    dataset = load_dataset(VALIDATION_FILENAMES, labeled = True)\n    print('loaded training dataset')\n    dataset = dataset.repeat(1)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    print('Successfully loaded validation dataset')\n    return dataset","e74cb705":"histories = []","009eac40":"with strategy.scope():\n    enet = efn.EfficientNetB1(\n        input_shape = (IMAGE_SIZE[0], IMAGE_SIZE[1], 3),\n        weights = 'imagenet',\n        include_top = False\n    )\n    \n    enet.trainable = False\n    model = tf.keras.Sequential([\n        enet, \n        tf.keras.layers.GlobalMaxPooling2D(name = 'layer1'),\n        tf.keras.layers.Dropout(0.4),\n       # tf.keras.layers.Dense(1, activation = 'softmax')\n        tf.keras.layers.Dense(1, activation = 'sigmoid')\n    ])","da4e6a59":"\"\"\"# METRICS\n\nmodel.compile(optimizer = tf.keras.optimizers.Adam(lr = 0.0001),\n             loss = 'sparse_categorical_crossentropy',\n              metrics= ['categorical_accuracy']\n             )\"\"\"","2756cec8":"\"\"\"# METRICS\n\nmodel.compile(optimizer = 'tf.keras.optimizers.Adam(lr = 0.0001)',\n             loss = 'categorical_crossentropy',\n              metrics= ['accuracy']\n             )\"\"\"","aa05e874":"# METRICS\n\nmodel.compile(optimizer = 'rmsprop',\n             loss = 'binary_crossentropy',\n              metrics= ['accuracy']\n             )","ef15a06c":"training_history_effnet = model.fit(get_training_dataset(), \n         steps_per_epoch = STEPS_PER_EPOCH, \n         validation_data = get_validation_dataset(),\n          epochs = EPOCHS\n         )\n\nhistories.append(training_history_effnet)","b6903ed8":"import seaborn as sns; import matplotlib.pyplot as plt; sns.set()","cc81c8c6":"histories[0].history","95173270":"sns.lineplot(x = list(range(1,11)), y = histories[0].history['loss'], label = 'Training Loss', color = 'red');\nsns.lineplot(x = list(range(1,11)), y = histories[0].history['val_loss'], label = 'Validation Loss', color = 'black').set_title('Loss Plot vs Epoch');\nplt.show()","73893693":"sns.lineplot(x = list(range(1,11)), y = histories[0].history['accuracy'], label = 'Training Accuracy', color = 'black');\nsns.lineplot(x = list(range(1,11)), y = histories[0].history['val_accuracy'], label = 'Validation Accuracy', color = 'red').set_title('Accuracy Plot vs Epoch');\nplt.show()","5864bac8":"#There are NO Validation files.","d5809762":"#Writing the read functions again from scratch for TPU Training\n\nI don't have validation filenames or files, I changed val. to train. on 2nd line. It didn't work. So I changed back.","f7a3f3eb":"#Codes by Superficially Bot https:\/\/www.kaggle.com\/superficiallybot\/tfrecord-reading-catsdogs-tpu-training-effnet","da099d42":"Maybe next time I could see the Training and Validation Loss curves.","e3d22e45":"#Function Definition"}}