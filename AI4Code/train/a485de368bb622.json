{"cell_type":{"86a0f294":"code","9c3d8c4f":"code","80ff3fdd":"code","6878e04f":"code","bc47a7a4":"code","aaa83f60":"code","48993fd9":"code","07a97457":"code","f8d9e560":"code","34892351":"code","e9c56d8f":"code","8de36b78":"code","c33e507b":"code","46e305e9":"code","04cdb6bc":"code","b3027f88":"code","719844a6":"code","5d8b9b83":"code","c177a24a":"code","0c285efa":"code","be041615":"code","26db84b6":"code","8e8c3611":"code","af8dbdc2":"code","139272ac":"code","2044aaed":"code","41fcff27":"code","b7fbc36e":"code","b229fa35":"code","0fd8340d":"code","532be712":"code","19f18c58":"code","f72e1831":"code","0da63799":"code","2b3e3349":"code","7b5f7b09":"code","3ba4990f":"code","5dcfb900":"code","4d1cc8bc":"code","38fc69c3":"code","127d0136":"code","dab91759":"code","90977906":"code","f3559735":"code","a525c53f":"code","d58d9d42":"code","0390ff24":"code","1e647024":"code","87cd1721":"code","3c93135e":"code","592082cb":"code","4a77a54d":"code","36777c77":"code","42df2b28":"code","fc8324f7":"code","62fb65c3":"code","02572052":"code","cc8f562a":"code","3ddf55a6":"code","a459ada8":"code","72b5a537":"code","97f5f703":"code","f090a374":"code","e4f3b869":"code","36b8b96a":"code","0c69a610":"code","82276014":"code","5dfc439a":"code","4eff38c9":"code","aeaf3c16":"code","cd9a0cba":"code","236eb4e1":"code","85a58c6f":"code","9d97f1c3":"code","fa4be693":"code","229eaeb1":"code","16448cc8":"code","379621d8":"code","ad431862":"code","b2a2746b":"code","59b8b061":"code","ed9d9cbc":"code","4a61e024":"code","0eadebbe":"code","79bf04bc":"code","1a551015":"code","e1c2692e":"code","fd2d2727":"code","6367ce17":"code","11cce9f7":"code","7967f366":"code","641e3359":"code","3f5dcfd1":"code","c457c7c8":"code","fea06e59":"code","a597c7ab":"code","58168f53":"code","23d4a432":"code","e8b1dc9c":"code","ff3d9319":"code","bba3a428":"code","7dd66b40":"code","882f663d":"code","1a81de00":"code","e6373083":"code","70d36ce8":"markdown","90d97e01":"markdown","b87b7a0c":"markdown","a73444a8":"markdown","3f12cdf3":"markdown","82ff8289":"markdown","c3d28566":"markdown","4064ff60":"markdown","ca359843":"markdown","38bcb569":"markdown","8d2dcd0c":"markdown","d7a83866":"markdown","36dd19fe":"markdown","fa017ab2":"markdown","5e378220":"markdown","e7455e12":"markdown","e35bb3f7":"markdown","cb3bf178":"markdown","0336bbba":"markdown","d2dd4eb0":"markdown","f67122cf":"markdown","a0a1e1fe":"markdown","0b108853":"markdown","0b64e3ca":"markdown","f06de7f3":"markdown","36ae3576":"markdown","22abed0b":"markdown","4a9d821f":"markdown","fda844a2":"markdown","815ea5f3":"markdown","ec4b9b8e":"markdown","c12201a3":"markdown","58650d86":"markdown","27da1e15":"markdown","d9eda46d":"markdown","f2a0d771":"markdown","339768d4":"markdown"},"source":{"86a0f294":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9c3d8c4f":"items = pd.read_csv('..\/input\/items.csv')\nshops = pd.read_csv('..\/input\/shops.csv')\ncats = pd.read_csv('..\/input\/item_categories.csv')\ntrain = pd.read_csv('..\/input\/sales_train.csv')\n# set index to ID to avoid droping it later\n#test  = pd.read_csv('..\/input\/test.csv').set_index('ID')\ntest  = pd.read_csv('..\/input\/test.csv')","80ff3fdd":"train.head()","6878e04f":"test.head(20)","bc47a7a4":"train.item_price.describe()","aaa83f60":"plt.figure()\ntrain.item_cnt_day.plot(kind='box')\n\nplt.figure()\nplt.xlim(train.item_price.min(), train.item_price.max()*1,1)\ntrain.item_price.plot(kind='box')","48993fd9":"train = train[(train.item_price<100000) & (train.item_cnt_day<1001)]","07a97457":"neg_count = train[train.item_price < 0]","f8d9e560":"neg_count ","34892351":"median = train[(train.shop_id==32)&(train.item_id==2973)&(train.date_block_num==4)&(train.item_price>0)].item_price.median()\ntrain.loc[train.item_price<0, 'item_price'] = median\n","e9c56d8f":"shops","8de36b78":"# Then, rename to match the proper cases\n# \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\ntrain.loc[train.shop_id == 0, 'shop_id'] = 57\ntest.loc[test.shop_id == 0, 'shop_id'] = 57\n# \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\ntrain.loc[train.shop_id == 1, 'shop_id'] = 58\ntest.loc[test.shop_id == 1, 'shop_id'] = 58\n# \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\ntrain.loc[train.shop_id == 10, 'shop_id'] = 11\ntest.loc[test.shop_id == 10, 'shop_id'] = 11","c33e507b":"train.shop_id.unique()","46e305e9":"shops.head()","04cdb6bc":"shops.loc[shops.shop_name == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops['shop_name'].str.split(' ').map(lambda x: x[0])\nshops.loc[shops.city == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'\n","b3027f88":"shops","719844a6":"shops['city_code'] = LabelEncoder().fit_transform(shops['city'])\n","5d8b9b83":"shops.head()","c177a24a":"shops = shops[['shop_id','city_code']]\nshops.head()","0c285efa":"shops.city_code.unique().sum()","be041615":"cats.head(20)","26db84b6":"cats['split'] = cats['item_category_name'].str.split('-')\ncats.head()","8e8c3611":"cats['type'] = cats['split'].map(lambda x: x[0].strip())\ncats.head()","af8dbdc2":"cats['type_code'] = LabelEncoder().fit_transform(cats['type'])\ncats.head()","139272ac":"cats['subtype'] = cats['split'].map(lambda x: x[1].strip() if len(x) > 1 else x[0].strip())\ncats.head()","2044aaed":"cats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats.head()","41fcff27":"cats = cats[['item_category_id','type_code', 'subtype_code']]\ncats.head()","b7fbc36e":"items.head()","b229fa35":"items.drop(['item_name'], axis=1, inplace=True)\nitems.head()","0fd8340d":"len(list(set(test.item_id) - set(test.item_id).intersection(set(train.item_id)))), len(list(set(test.item_id))), len(test)","532be712":"matrix = []\ncols = ['date_block_num','shop_id','item_id']\nfor i in range(34):\n    sales = train[train.date_block_num==i]\n    matrix.append(np.array(list(product([i], sales.shop_id.unique(), sales.item_id.unique())), dtype='int16'))","19f18c58":"train[train.date_block_num ==1 ]","f72e1831":"matrix = pd.DataFrame(np.vstack(matrix), columns=cols)","0da63799":"matrix['date_block_num'] = matrix['date_block_num'].astype(np.int8)\nmatrix['shop_id'] = matrix['shop_id'].astype(np.int8)\nmatrix['item_id'] = matrix['item_id'].astype(np.int16)\nmatrix","2b3e3349":"matrix.sort_values(cols,inplace=True) \n","7b5f7b09":"matrix","3ba4990f":"train['revenue'] = train['item_price'] *  train['item_cnt_day']\ntrain.head()","5dcfb900":"group = train.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': ['sum']})\n","4d1cc8bc":"group","38fc69c3":"group.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\ngroup","127d0136":"matrix = pd.merge(matrix, group, on=cols, how='left')\n","dab91759":"matrix","90977906":"matrix['item_cnt_month'] = (matrix['item_cnt_month']\n                                .fillna(0)  #Replace NaN values by 0\n                                .clip(0,20) # NB clip target here\n                                .astype(np.float16))","f3559735":"matrix","a525c53f":"test['date_block_num'] = 34\ntest['date_block_num'] = test['date_block_num'].astype(np.int8)\ntest['shop_id'] = test['shop_id'].astype(np.int8)\ntest['item_id'] = test['item_id'].astype(np.int16)","d58d9d42":"\nmatrix = pd.concat([matrix, test], ignore_index=True, sort=False, keys=cols)\nmatrix.fillna(0, inplace=True) # 34 month\n","0390ff24":"matrix.head()","1e647024":"\nmatrix = pd.merge(matrix, shops, on=['shop_id'], how='left')  #merging on the shop_id level\nmatrix = pd.merge(matrix, items, on=['item_id'], how='left')  #merging on the item_id level\nmatrix = pd.merge(matrix, cats, on=['item_category_id'], how='left')\nmatrix['city_code'] = matrix['city_code'].astype(np.int8)\nmatrix['item_category_id'] = matrix['item_category_id'].astype(np.int8)\nmatrix['type_code'] = matrix['type_code'].astype(np.int8)\nmatrix['subtype_code'] = matrix['subtype_code'].astype(np.int8)\n","87cd1721":"matrix","3c93135e":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","592082cb":"matrix = lag_feature(matrix, [1,2,3,6,12], 'item_cnt_month')","4a77a54d":"matrix","36777c77":"group = matrix.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num'], how='left')\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_avg_item_cnt')\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)","42df2b28":"import time","fc8324f7":"group","62fb65c3":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_item_avg_item_cnt')\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","02572052":"matrix","cc8f562a":"ts = time.time()\ngroup = matrix.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_shop_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1,2,3,6,12], 'date_shop_avg_item_cnt')\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)\ntime.time() - ts","3ddf55a6":"matrix.head()","a459ada8":"group = matrix.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_cat_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','item_category_id'], how='left')\nmatrix['date_cat_avg_item_cnt'] = matrix['date_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_cat_avg_item_cnt')\nmatrix.drop(['date_cat_avg_item_cnt'], axis=1, inplace=True)","72b5a537":"matrix.head()","97f5f703":"group = matrix.groupby(['date_block_num', 'shop_id', 'item_category_id']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_cat_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_category_id'], how='left')\nmatrix['date_shop_cat_avg_item_cnt'] = matrix['date_shop_cat_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_cat_avg_item_cnt')\nmatrix.drop(['date_shop_cat_avg_item_cnt'], axis=1, inplace=True)","f090a374":"matrix.head()","e4f3b869":"group = matrix.groupby(['date_block_num', 'shop_id', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_type_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'type_code'], how='left')\nmatrix['date_shop_type_avg_item_cnt'] = matrix['date_shop_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_type_avg_item_cnt')\nmatrix.drop(['date_shop_type_avg_item_cnt'], axis=1, inplace=True)","36b8b96a":"group = matrix.groupby(['date_block_num', 'shop_id', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_shop_subtype_avg_item_cnt')\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)","0c69a610":"group = matrix.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'city_code'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_city_avg_item_cnt')\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)","82276014":"group = matrix.groupby(['date_block_num', 'item_id', 'city_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_item_city_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'city_code'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_item_city_avg_item_cnt')\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)","5dfc439a":"group = matrix.groupby(['date_block_num', 'type_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_type_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'type_code'], how='left')\nmatrix['date_type_avg_item_cnt'] = matrix['date_type_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_type_avg_item_cnt')\nmatrix.drop(['date_type_avg_item_cnt'], axis=1, inplace=True)","4eff38c9":"group = matrix.groupby(['date_block_num', 'subtype_code']).agg({'item_cnt_month': ['mean']})\ngroup.columns = [ 'date_subtype_avg_item_cnt' ]\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'subtype_code'], how='left')\nmatrix['date_subtype_avg_item_cnt'] = matrix['date_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], 'date_subtype_avg_item_cnt')\nmatrix.drop(['date_subtype_avg_item_cnt'], axis=1, inplace=True)","aeaf3c16":"group = train.groupby(['item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\ngroup","cd9a0cba":"\nmatrix = pd.merge(matrix, group, on=['item_id'], how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\nmatrix.head()","236eb4e1":"group = train.groupby(['date_block_num','item_id']).agg({'item_price': ['mean']})\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\ngroup.head()","85a58c6f":"matrix = pd.merge(matrix, group, on=['date_block_num','item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\n","9d97f1c3":"matrix.head()","fa4be693":"#add lags for the 'date_item_avg_item_price'\nlags = [1,2,3,4,5,6]\nmatrix = lag_feature(matrix, lags, 'date_item_avg_item_price')\n","229eaeb1":"# Calculating the variations on avg_price for each time lag\nfor i in lags:\n    matrix['delta_price_lag_'+str(i)] = \\\n        (matrix['date_item_avg_item_price_lag_'+str(i)] - matrix['item_avg_item_price']) \/ matrix['item_avg_item_price']\n","16448cc8":"matrix.head()","379621d8":"def select_trend(row):\n    for i in lags:\n        if row['delta_price_lag_'+str(i)]:\n            return row['delta_price_lag_'+str(i)]\n    return 0\n    ","ad431862":"matrix['delta_price_lag'] = matrix.apply(select_trend, axis=1)\nmatrix['delta_price_lag'] = matrix['delta_price_lag'].astype(np.float16)\nmatrix['delta_price_lag'].fillna(0, inplace=True)","b2a2746b":"matrix.head()","59b8b061":"features_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    features_to_drop += ['date_item_avg_item_price_lag_'+str(i)]\n    features_to_drop += ['delta_price_lag_'+str(i)]\n\nmatrix.drop(features_to_drop, axis=1, inplace=True)","ed9d9cbc":"matrix.head()","4a61e024":"group = train.groupby(['date_block_num','shop_id']).agg({'revenue': ['sum']})\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num','shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = group.groupby(['shop_id']).agg({'date_shop_revenue': ['mean']})\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['shop_id'], how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\n\nmatrix['delta_revenue'] = (matrix['date_shop_revenue'] - matrix['shop_avg_revenue']) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float16)\n\nmatrix = lag_feature(matrix, [1], 'delta_revenue')\n\nmatrix.drop(['date_shop_revenue','shop_avg_revenue','delta_revenue'], axis=1, inplace=True)","0eadebbe":"matrix['month'] = matrix['date_block_num'] % 12\n","79bf04bc":"matrix.head()","1a551015":"days = pd.Series([31,28,31,30,31,30,31,31,30,31,30,31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","e1c2692e":"matrix.head()","fd2d2727":"ts = time.time()\ncache = {}\nmatrix['item_shop_last_sale'] = -1\nmatrix['item_shop_last_sale'] = matrix['item_shop_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = str(row.item_id)+' '+str(row.shop_id)\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        matrix.at[idx, 'item_shop_last_sale'] = row.date_block_num - last_date_block_num\n        cache[key] = row.date_block_num \ntime.time() - ts ","6367ce17":"ts = time.time()\ncache = {}\nmatrix['item_last_sale'] = -1\nmatrix['item_last_sale'] = matrix['item_last_sale'].astype(np.int8)\nfor idx, row in matrix.iterrows():    \n    key = row.item_id\n    if key not in cache:\n        if row.item_cnt_month!=0:\n            cache[key] = row.date_block_num\n    else:\n        last_date_block_num = cache[key]\n        if row.date_block_num>last_date_block_num:\n            matrix.at[idx, 'item_last_sale'] = row.date_block_num - last_date_block_num\n            cache[key] = row.date_block_num\ntime.time() - ts","11cce9f7":"ts = time.time()\nmatrix['item_shop_first_sale'] = matrix['date_block_num'] - matrix.groupby(['item_id','shop_id'])['date_block_num'].transform('min')\nmatrix['item_first_sale'] = matrix['date_block_num'] - matrix.groupby('item_id')['date_block_num'].transform('min')\ntime.time() - ts","7967f366":"ts = time.time()\nmatrix = matrix[matrix.date_block_num > 11]\ntime.time() - ts","641e3359":"ts = time.time()\ndef fill_na(df):\n    for col in df.columns:\n        if ('_lag_' in col) & (df[col].isnull().any()):\n            if ('item_cnt' in col):\n                df[col].fillna(0, inplace=True)         \n    return df\n\nmatrix = fill_na(matrix)\ntime.time() - ts","3f5dcfd1":"matrix.columns","c457c7c8":"matrix.info()","fea06e59":"matrix.head()","a597c7ab":"import gc\nimport pickle","58168f53":"\nmatrix.to_pickle('data.pkl')\n#del matrix\ndel cache\ndel group\ndel items\ndel shops\ndel cats\ndel train\n# leave test for submission\ngc.collect();","23d4a432":"print(os.listdir(\"..\/working\"))","e8b1dc9c":"data = pd.read_pickle('data.pkl')\n","ff3d9319":"data = data[[\n    'date_block_num',\n    'shop_id',\n    'item_id',\n    'item_cnt_month',\n    'city_code',\n    'item_category_id',\n    'type_code',\n    'subtype_code',\n    'item_cnt_month_lag_1',\n    'item_cnt_month_lag_2',\n    'item_cnt_month_lag_3',\n    'item_cnt_month_lag_6',\n    'item_cnt_month_lag_12',\n    'date_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_1',\n    'date_item_avg_item_cnt_lag_2',\n    'date_item_avg_item_cnt_lag_3',\n    'date_item_avg_item_cnt_lag_6',\n    'date_item_avg_item_cnt_lag_12',\n    'date_shop_avg_item_cnt_lag_1',\n    'date_shop_avg_item_cnt_lag_2',\n    'date_shop_avg_item_cnt_lag_3',\n    'date_shop_avg_item_cnt_lag_6',\n    'date_shop_avg_item_cnt_lag_12',\n    'date_cat_avg_item_cnt_lag_1',\n    'date_shop_cat_avg_item_cnt_lag_1',\n    #'date_shop_type_avg_item_cnt_lag_1',\n    #'date_shop_subtype_avg_item_cnt_lag_1',\n    'date_city_avg_item_cnt_lag_1',\n    'date_item_city_avg_item_cnt_lag_1',\n    #'date_type_avg_item_cnt_lag_1',\n    #'date_subtype_avg_item_cnt_lag_1',\n    'delta_price_lag',\n    'month',\n    'days',\n    'item_shop_last_sale',\n    'item_last_sale',\n    'item_shop_first_sale',\n    'item_first_sale',\n]]","bba3a428":"X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num < 33]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","7dd66b40":"del data\ngc.collect();","882f663d":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n","1a81de00":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    n_estimators=1000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,    \n    seed=42)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric=\"rmse\", \n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds = 10)\n\ntime.time() - ts","e6373083":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.index, \n    \"item_cnt_month\": Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)\n\n# save predictions for an ensemble\npickle.dump(Y_pred, open('xgb_train.pickle', 'wb'))\npickle.dump(Y_test, open('xgb_test.pickle', 'wb'))","70d36ce8":"once again, take item_cnt_month and calculate the monthy mean but this time by 'item_category_id'","90d97e01":"# Keep numeric columns only (items file)","b87b7a0c":"The above figures exhibit the presence of outliers and suggest keeping only data that satisfies:\n$$\\text{item_price}\\leq 100000, \\qquad \\text{item_cnt_day}\\leq 1000$$","a73444a8":"Special Features","3f12cdf3":"# Explore the shops.csv file\nThere are repeated shops (by name), namely: \n* \u042f\u043a\u0443\u0442\u0441\u043a \u041e\u0440\u0434\u0436\u043e\u043d\u0438\u043a\u0438\u0434\u0437\u0435, 56\n* \u042f\u043a\u0443\u0442\u0441\u043a \u0422\u0426 \"\u0426\u0435\u043d\u0442\u0440\u0430\u043b\u044c\u043d\u044b\u0439\"\n* \u0416\u0443\u043a\u043e\u0432\u0441\u043a\u0438\u0439 \u0443\u043b. \u0427\u043a\u0430\u043b\u043e\u0432\u0430 39\u043c\u00b2\n\n","82ff8289":"taking the item_cnt_month mean by date_block_num and by 'item_id' \nIn other words, take a date_block_num, inside that take 'item_id' and then calulate the mean of item_cont_month\nthen merge group into matrix","c3d28566":"Producing lags brings a lot of nulls, fix them:","4064ff60":"Then, categorize using the LabelEncoder().fit_transform method","ca359843":"now calculate the item_cnt_month mean by shop_id for each month\nthen merge with matrix","38bcb569":"Note that shop_name beggins with the city name, then:\n* Change \"\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"\"  into \"\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"\"\n* Create a new column with the names fo the cities ","8d2dcd0c":"Append the test set to Matrix (like building the entire time series train+test as one)","d7a83866":"# Final preparations\nBecause of the using 12 as lag value drop first 12 months. Also drop all the columns with this month calculated values (other words which can not be calcucated for the test set).","36dd19fe":"The following line groups the data by date_block_num, shop_id and item_id, it then aggregates the sales by summing","fa017ab2":"There are 363 items in test that are not in Train, this could be due to Train.csv having only sold and returned items, this is, those 363 more items correspond  to **zero sales**  during the time period in question. \nIn the following we extract monthly sales and extend it with zero sales.\nTest set is a product of some shops and some items within 34 month. There are 5100 items * 42 shops = 214200 pairs","5e378220":"Selecting Features:","e7455e12":"Validation strategy is 34 month for the test set, 33 month for the validation set and 13-33 months for the train.\n\n","e35bb3f7":"Finally, use only the shop_id and city_code for the shop info. We want numbers only","cb3bf178":"Define the Subtypes based on  the second entry in split, this is, $y$ in $[x,y]$","0336bbba":"Sort the rows in matrix in ascending order","d2dd4eb0":"# Some modifications on the Test set\n* Set the date_block_num as 34 (the 34th month), the block of predictions\n* make it int8\n* make shop_id into int8 \n* make item_id into int16","f67122cf":"# Now explore the Cats file\nThis file contains data about item_category_name","a0a1e1fe":"use the sklearn the fit_transform method of LabelEncoder (sklearn) to label the cities","0b108853":"# Mean encoded features ","0b64e3ca":"Create HashTable with key equals to {shop_id,item_id} and value equals to date_block_num. Iterate data from the top. Foreach row if {row.shop_id,row.item_id} is not present in the table, then add it to the table and set its value to row.date_block_num. if HashTable contains key, then calculate the difference beteween cached value and row.date_block_num.","f06de7f3":"# Data Exploration: \n* Identifying Outliers w.r.t. item_price and item_cnt_day","36ae3576":"now, include shop_id as well to take the monthly mean of item_cnt","22abed0b":"Now, look for negative prices. Remember, we're still pre-processing Data. \n\nThere is one, and we can replace its price with the median (the point that splits data in half) considering similar items, this is, considering only item_id=2973 sold at shop_id=32","4a9d821f":"then clip(0,20) target value. This way train target will be similar to the test predictions.","fda844a2":"Keep numeric columns only","815ea5f3":"# Target Lags \n* apparently, lags are at the root of time-series analysis\n* basically they generate the next point given the previous ones\n\nWe define a function that creates the target lags from a Dataframe (df), lags and columns:","ec4b9b8e":"Last month shop revenue trend\n\n","c12201a3":"# Trend Features\nPrice trend for the last six months.","58650d86":"# PART II: Machine Learning\nXGBoost","27da1e15":"# Now, merge the engineered features into the matrix dataframe","d9eda46d":"Now make 'type' a numeric categorical variable using fit_transform","f2a0d771":"Months since the first sale for each shop\/item pair and for item only.","339768d4":"The lag_feature() function is used to modify matrix to contain lagged features for item_con_month\nbasically lagging by 1, 2, 3, 6 and 12 months (those are to be the time lags)"}}