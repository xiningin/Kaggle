{"cell_type":{"c3dac481":"code","e6548995":"code","4b6fc269":"code","a39d2d62":"code","11c1f65c":"code","3b08ade1":"code","8b3bd266":"code","34ee00b0":"code","f49e57c3":"code","46218982":"code","9827dc6a":"code","9d39738f":"code","c53929db":"code","2d4c572d":"code","7ff07a69":"code","c5b05827":"markdown","0073add3":"markdown","be53a59a":"markdown","ed41dcb5":"markdown","ef61369f":"markdown","695c2613":"markdown","abdb5eda":"markdown"},"source":{"c3dac481":"import matplotlib.pyplot as plt\nimport cv2\nimport imageio as io\n\nlat_D = 'max[log(D(x))+log(1-D(G(x))]'\nlat_G = 'min[log(D(x))+log(1-D(G(x))]'\n\nplt.figure(figsize=(12, 10))\nplt.imshow(io.imread(\"https:\/\/i.pinimg.com\/originals\/97\/c8\/0e\/97c80e536a8b8712a8299dddb9f14e07.jpg\"))\nplt.axis('off');\n\nplt.text(80, 220, \"Generator\", \n         fontdict={\n             \"fontsize\":15,\n             \"color\":\"#FF033E\",\n             \"weight\": \"bold\"\n             }\n         );\nplt.text(38, 230, r\"$%s$\" % lat_G, \n         fontdict={\n             \"fontsize\":13,\n             \"color\":\"black\",\n             \"weight\": \"bold\"\n             }\n         );\n                                                                                               \nplt.text(300, 120, r\"Discriminator\", \n         fontdict={\n             \"fontsize\":15,\n             \"color\":\"lime\",\n             \"weight\": \"bold\"\n             }\n         );\n         \nplt.text(260, 130, r\"$%s$\" % lat_D,\n         fontdict=\n         {\"fontsize\":13,\n          \"color\":\"white\",\n          \"weight\": \"bold\"\n          }\n         );","e6548995":"import os\nimport time\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW, Adam\n","4b6fc269":"class Config:\n    csv_path = ''\n    seed = 2021\n    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ndef seed_everything(seed: int):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n    \nconfig = Config()\nseed_everything(config.seed)","a39d2d62":"df_ptbdb = pd.read_csv('\/kaggle\/input\/heartbeat\/ptbdb_abnormal.csv')\ndf_mitbih = pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_train.csv')\ndf_ptbdb.head()","11c1f65c":"df_mitbih_train = pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_train.csv', header=None)\ndf_mitbih_test = pd.read_csv('\/kaggle\/input\/heartbeat\/mitbih_test.csv', header=None)\ndf_mitbih = pd.concat([df_mitbih_train, df_mitbih_test], axis=0)\ndf_mitbih.rename(columns={187: 'class'}, inplace=True)\n\nid_to_label = {\n    0: \"Normal\",\n    1: \"Artial Premature\",\n    2: \"Premature ventricular contraction\",\n    3: \"Fusion of ventricular and normal\",\n    4: \"Fusion of paced and normal\"\n}\ndf_mitbih['label'] = df_mitbih.iloc[:, -1].map(id_to_label)\nprint(df_mitbih.info())","3b08ade1":"df_mitbih.to_csv('data.csv', index=False)\nconfig.csv_path = 'data.csv'","8b3bd266":"df_mitbih['label'].value_counts()","34ee00b0":"percentages = [count \/ df_mitbih.shape[0] * 100 for count in df_mitbih['label'].value_counts()]\n\nfig, ax = plt.subplots(figsize=(12, 6))\nsns.countplot(\n    x=df_mitbih['label'],\n    ax=ax,\n    palette=\"bright\",\n    order=df_mitbih['label'].value_counts().index\n)\nax.set_xticklabels(ax.get_xticklabels(), rotation=15);\n\nfor percentage, count, p in zip(\n    percentages,\n    df_mitbih['label'].value_counts(sort=True).values,\n    ax.patches):\n    \n    percentage = f'{np.round(percentage, 2)}%'\n    x = p.get_x() + p.get_width() \/ 2 - 0.4\n    y = p.get_y() + p.get_height()\n    ax.annotate(str(percentage)+\" \/ \"+str(count), (x, y), fontsize=12, fontweight='bold')\n    \nplt.savefig('data_dist.png', facecolor='w', edgecolor='w', format='png',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)\nplt.savefig('data_dist.svg', facecolor='w', edgecolor='w', format='svg',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)","f49e57c3":"class ECGDataset(Dataset):\n\n    def __init__(self, df):\n        self.df = df\n        self.data_columns = self.df.columns[:-2].tolist()\n\n    def __getitem__(self, idx):\n        signal = self.df.loc[idx, self.data_columns].astype('float32')\n        signal = torch.FloatTensor([signal.values])                 \n        target = torch.LongTensor(np.array(self.df.loc[idx, 'class']))\n        return signal, target\n\n    def __len__(self):\n        return len(self.df)\n\ndef get_dataloader(label_name, batch_size):\n    df = pd.read_csv(config.csv_path)\n    df = df.loc[df['label'] == label_name]\n    df.reset_index(drop=True, inplace=True)\n    dataset = ECGDataset(df)\n    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=0)\n    return dataloader\n    \n    \ndataloader = get_dataloader(label_name='Artial Premature', batch_size=96)\n\nprint(len(dataloader))\nx,y = next(iter(dataloader))\nx.shape, y.shape","46218982":"import torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.fc1 = nn.Linear(256, 256)\n        self.fc2 = nn.Linear(256, 256)\n        self.fc3 = nn.Linear(256, 187)\n        self.rnn_layer = nn.LSTM(\n                input_size=187,\n                hidden_size=128,\n                num_layers=1,\n                bidirectional=True,\n                batch_first=True,\n            )\n    def forward(self, x):\n        x,_ = self.rnn_layer(x)\n        x = x.view(-1,256)\n        x = F.leaky_relu(self.fc1(x))\n        x = F.leaky_relu(self.fc2(x))\n        x = F.dropout(x, p=0.2)\n        x = self.fc3(x)\n        return x.unsqueeze(1)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        \n        self.rnn_layer = nn.LSTM(\n                input_size=187,\n                hidden_size=256,\n                num_layers=1,\n                bidirectional=True,\n                batch_first=True,\n            )\n        self.fc1 = nn.Linear(512, 512)\n        self.fc2 = nn.Linear(512, 256) \n        self.fc3 = nn.Linear(256, 1)\n\n    def forward(self, x):\n        x,_ = self.rnn_layer(x)\n        x = x.view(-1, 512)\n        x = F.leaky_relu(self.fc1(x))\n        x = F.leaky_relu(self.fc2(x))\n        x = F.dropout(x, p=0.2)\n        x = torch.sigmoid(self.fc3(x))\n        return x","9827dc6a":"class Trainer:\n    def __init__(\n        self,\n        generator,\n        discriminator,\n        batch_size,\n        num_epochs,\n        label\n    ):\n        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n        self.netG = generator.to(self.device)\n        self.netD = discriminator.to(self.device)\n        \n        self.optimizerD = Adam(self.netD.parameters(), lr=0.0002)\n        self.optimizerG = Adam(self.netG.parameters(), lr=0.0002)\n        self.criterion = nn.BCELoss()\n        \n        self.batch_size = batch_size\n        self.signal_dim = [self.batch_size, 1, 187]\n        self.num_epochs = num_epochs\n        self.dataloader = get_dataloader(\n            label_name=label, batch_size=self.batch_size\n        )\n        self.fixed_noise = torch.randn(self.batch_size, 1, 187,\n                                       device=self.device)\n        self.g_errors = []\n        self.d_errors = []\n        \n    def _one_epoch(self):\n        real_label = 1\n        fake_label = 0\n        \n        for i, data in enumerate(self.dataloader, 0):\n            ##### Update Discriminator: maximize log(D(x)) + log(1 - D(G(z))) #####\n            ## train with real data\n            self.netD.zero_grad()\n            real_data = data[0].to(self.device)\n            # dim for noise\n            batch_size = real_data.size(0)\n            self.signal_dim[0] = batch_size\n            \n            label = torch.full((batch_size,), real_label,\n                           dtype=real_data.dtype, device=self.device)\n            \n            output = self.netD(real_data)\n            output = output.view(-1)\n       \n            errD_real = self.criterion(output, label)\n            errD_real.backward()\n            D_x = output.mean().item()\n            \n            ## train with fake data\n            noise = torch.randn(self.signal_dim, device=self.device)\n            fake = self.netG(noise)\n            label.fill_(fake_label)\n            \n            output = self.netD(fake.detach())\n            output = output.view(-1)\n            \n            errD_fake = self.criterion(output, label)\n            errD_fake.backward()\n            D_G_z1 = output.mean().item()\n            errD = errD_real + errD_fake \n            self.optimizerD.step()\n            \n            ##### Update Generator: maximaze log(D(G(z)))  \n            self.netG.zero_grad()\n            label.fill_(real_label) \n            output = self.netD(fake)\n            output = output.view(-1)\n            \n            errG = self.criterion(output, label)\n            errG.backward()\n            D_G_z2 = output.mean().item()\n            self.optimizerG.step()\n            \n        return errD.item(), errG.item()\n        \n    def run(self):\n        for epoch in range(self.num_epochs):\n            errD_, errG_ = self._one_epoch()\n            self.d_errors.append(errD_)\n            self.g_errors.append(errG_)\n            if epoch % 300 == 0:\n                print(f\"Epoch: {epoch} | Loss_D: {errD_} | Loss_G: {errG_} | Time: {time.strftime('%H:%M:%S')}\")\n   \n                fake = self.netG(self.fixed_noise)\n                plt.plot(fake.detach().cpu().squeeze(1).numpy()[:].transpose())\n                plt.show()\n            \n        torch.save(self.netG.state_dict(), f\"generator.pth\")\n        torch.save(self.netG.state_dict(), f\"discriminator.pth\")","9d39738f":"g = Generator()\nd = Discriminator()","c53929db":"trainer = Trainer(\n    generator=g,\n    discriminator=d,\n    batch_size=96,\n    num_epochs=3000,\n    label='Fusion of ventricular and normal'\n)\ntrainer.run()","2d4c572d":"plt.figure(figsize=(12, 5))\nplt.title(\"GAN Errors in Training\")\nplt.plot(trainer.d_errors, label='Generator Error', c='#1CD3A2')\nplt.plot(trainer.g_errors, label='Discriminator Error', c='#FF0033')\nplt.xlabel(\"Epochs\")\nplt.ylabel('Error')\n_ = plt.legend()\nplt.savefig('Gan_Losses.png', facecolor='w', edgecolor='w', format='png',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)\nplt.savefig('Gan_Losses.svg', facecolor='w', edgecolor='w', format='svg',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)","7ff07a69":"df = pd.read_csv(config.csv_path)\ndf = df.loc[df['label'] == 'Fusion of ventricular and normal']\n\n# real signal\nN = 1\nreal_samples =  df.sample(N).values[:, :-2].transpose()\n\nsynthetic signal\nfake = trainer.netG(trainer.fixed_noise)\nindex = np.random.choice(fake.shape[0], N, replace=False) \nsynthetic_samples = fake.detach().cpu().squeeze(1).numpy()[index].transpose()\n\nfig, axs = plt.subplots(1, 2, figsize=(15, 4))\n\n\naxs[0].plot(real_samples, c='#007FFF')\naxs[0].set_title(\"Real\", fontsize= 12, weight=\"bold\")\n\n\naxs[1].plot(synthetic_samples, c=\"crimson\")\naxs[1].set_title(\"Synthetic\", fontsize= 12, weight=\"bold\")\n\nplt.suptitle('class \"Fusion of ventricular and normal\"', fontsize=18, y=1.05, weight=\"bold\")\nplt.tight_layout()\nplt.savefig('Fusion_of_ventricular_and_normal.png', facecolor='w', edgecolor='w', format='png',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)\nplt.savefig('Fusion_of_ventricular_and_normal.svg', facecolor='w', edgecolor='w', format='svg',\n        transparent=False, bbox_inches='tight', pad_inches=0.1)","c5b05827":"# Dataset","0073add3":"# Basic EDA","be53a59a":"# Generative Adversarial Network for ECG synthesis\nThis notebook is an addition to this [notebook](https:\/\/www.kaggle.com\/polomarco\/ecg-classification-cnn-lstm-attention-mechanism).","ed41dcb5":"# Conclusion\nSince this is a standard process for training a GAN model, we can observe that the Generator produces predominantly dominant signal types.\nMore specifically, we have a total of 803 signals of the '\"Fusion of ventricular and normal\"' class, most of which are very similar, and that's what GAN model learned to generate.","ef61369f":"# Models","695c2613":"# Training stage","abdb5eda":"![](https:\/\/64.media.tumblr.com\/66dc00e688f72af4d7b5d768005359c4\/094fefb7e60b17ee-9c\/s500x750\/3060385c5fdcb38f3a716d8a309a40e69e598a5f.png)"}}