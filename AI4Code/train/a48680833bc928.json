{"cell_type":{"ef4b94cd":"code","13d7db86":"code","95087220":"code","f18fe217":"code","95962bd6":"code","0682a3ce":"code","96a60bb5":"code","2d078919":"code","585d8738":"code","8f4e7025":"code","b04b8db6":"code","adc8dcec":"code","4b81413c":"code","33112755":"code","7d23821a":"code","9834d10a":"code","47436aa4":"code","0cb66f12":"code","87b01a62":"code","c5f58640":"code","2b0b0b3c":"code","f05340d2":"code","a065df47":"code","39383da0":"code","86f6f2fb":"code","7bf2d58e":"code","6fc28fcc":"code","a535c136":"code","215aa362":"code","ed0294c0":"code","0914ffcd":"code","37df7efe":"code","88eff9a8":"code","af355297":"code","8da78b66":"code","b7c8237a":"code","36c7b567":"code","5f7a1a79":"code","b08d7a0b":"code","16a6dff1":"code","5456e477":"code","36c99afc":"code","e1947855":"code","861106f7":"code","ba2d33c9":"code","2aba5aef":"code","33bf5e07":"code","540b0d45":"code","c818fe84":"code","f3787e2e":"code","49f33066":"code","9d1a0f2b":"code","f1bc3a2f":"code","e0275476":"code","aa90388a":"code","12bb1ef2":"code","35aa3e53":"code","68c9093e":"code","734206c1":"code","792355fb":"code","8f6950f7":"code","bcafd0f3":"code","3684009f":"code","a375386a":"code","a2bf2cb1":"code","0790a164":"code","efdc467a":"code","b2ef8e6a":"code","f0024dd0":"code","f7b2c013":"code","38750b4c":"code","e2a06e34":"code","ef50ad1e":"code","d870d6a6":"code","f0c5b0a6":"code","4f69fb9f":"code","6c80b9c3":"code","62180b9f":"code","07ce9760":"code","7f56e5d6":"code","ee5f6980":"code","c0ee388d":"code","71b8205d":"code","4fd90f01":"code","891f6c19":"code","8386ab56":"code","d7ab670e":"code","aee7556b":"code","1d7be9ca":"code","283727b8":"code","00c28a5f":"code","d76283ef":"code","19daa4c8":"code","d2e44cc0":"code","b6a39fcf":"code","8716b297":"code","5a9b1cd7":"code","d8d25bd7":"code","8cd5135d":"code","8977c8ef":"markdown","80aeda2a":"markdown","25534ca9":"markdown","4056f548":"markdown","4541bae0":"markdown","7aac9355":"markdown","ea347762":"markdown","0c8b9bce":"markdown","1d1f3f24":"markdown","4255cff9":"markdown","3024da35":"markdown","f866ea4a":"markdown","d000432f":"markdown","ddba415e":"markdown","6915e690":"markdown","b3270939":"markdown","026bbbc9":"markdown","97c401cc":"markdown","28d73ea2":"markdown","b5b9ee2f":"markdown","9e6052a9":"markdown","b8b6dbb0":"markdown","b5722cb3":"markdown","e7caf9e1":"markdown","59f38ed1":"markdown","be03ed0e":"markdown","b5116cd1":"markdown","2aec3f5c":"markdown","ffa482cb":"markdown","6a3ab9b0":"markdown","4124fd0a":"markdown","2df8bf3e":"markdown","94fc2992":"markdown","ea75ddea":"markdown","27639e9e":"markdown","20c65553":"markdown","41a076ed":"markdown","1dc5cb93":"markdown","75b1f336":"markdown","e22ac476":"markdown","61f78743":"markdown","80936044":"markdown","4774c469":"markdown","4d4062ab":"markdown","7d92a418":"markdown","92fa44c8":"markdown"},"source":{"ef4b94cd":"import numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_validate, learning_curve\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.feature_selection import RFE\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n%matplotlib inline","13d7db86":"data = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2021\/train.csv\")","95087220":"data.head()","f18fe217":"data.drop(columns=[\"id\"],inplace=True)","95962bd6":"data.shape","0682a3ce":"data.dtypes","96a60bb5":"data.describe().T","2d078919":"data.nunique()","585d8738":"numeric_features = data.select_dtypes(\"float64\").columns[:-1]\nnumeric_features","8f4e7025":"categorical_features = data.select_dtypes(\"object\").columns\ncategorical_features","b04b8db6":"target = 'target'","adc8dcec":"data[numeric_features].apply(lambda x: (np.abs((x.max() \/ x.min())-1) * 100),axis=0)","4b81413c":"data[numeric_features].apply(lambda x: ((x.std() \/ x.median())) * 100,axis=0)","33112755":"data.isnull().mean()","7d23821a":"for i in categorical_features:\n    print(f'{i}\\n{(np.round((data[i].value_counts() \/ len(data[i]))*100,3))}\\n\\n')","9834d10a":"data[numeric_features].hist(figsize=(20,20));","47436aa4":"for i in numeric_features:   \n    print(f\"Kolmogorov-Smirnov: {i} : {'Not Gaussian' if stats.kstest(data[i],'norm')[1]<0.05 else 'Gaussian'}\")","0cb66f12":"for i in numeric_features:   \n    stats.probplot(data[i],plot=plt)\n    plt.title(i)\n    plt.show()","87b01a62":"fig,ax = plt.subplots(5,2,figsize=(15,20),sharey=False)\nrow = col = 0\nfor n,i in enumerate(categorical_features):\n    if (n % 2 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.boxplot(x=data[i],y=data[target],ax=ax[row,col])\n    ax[row,col].set_title(f\"Target vs {i}\")\n    ax[row,col].set_xlabel(\"\")\n    col += 1\n    \n    \nplt.show();","c5f58640":"fig,ax = plt.subplots(7,2,figsize=(15,30),sharey=False)\nrow = col = 0\nfor n,i in enumerate(numeric_features):\n    if (n % 2 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.boxplot(y=data[i],ax=ax[row,col])\n    ax[row,col].set_title(f\"{i}\")\n    ax[row,col].set_ylabel(\"\")\n    col += 1\n    \n    \nplt.show();","2b0b0b3c":"fig = plt.figure(figsize=(12,12))\nsns.heatmap(data.corr(),mask=np.triu(data.corr()),annot=True,cbar=False,fmt=\".2f\",robust=True);","f05340d2":"fig,ax = plt.subplots(7,2,figsize=(15,40),sharey=False)\nrow = col = 0\nfor n,i in enumerate(numeric_features):\n    if (n % 2 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.scatterplot(x=i,y=target,data=data,ax=ax[row,col])\n    ax[row,col].set_title(f\"Target vs {i}\")\n    ax[row,col].set_xlabel(\"\")\n    col += 1\n    \n    \nplt.show();","a065df47":"fig = plt.figure(figsize=(12,12))\nsns.heatmap(data.corr(method='kendall'),mask=np.triu(data.corr()),annot=True,cbar=False,fmt=\".2f\",robust=True);","39383da0":"X_vif  = data[numeric_features].copy()","86f6f2fb":"vif =  [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\nprint(vif)","7bf2d58e":"flag = True\ncorrelated_features_to_delete = []\nwhile flag == True:\n    vif =  pd.Series([variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])])\n    if vif.max() >= 10:\n        max_vif_col = pd.Series(X_vif.columns)[vif.argmax()]\n        correlated_features_to_delete.append(max_vif_col)\n        X_vif.drop(columns=max_vif_col,inplace=True)\n    else:\n        flag = False","6fc28fcc":"correlated_features_to_delete","a535c136":"X_vif.head(2)","215aa362":"vif =  [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\nprint(vif)","ed0294c0":"multi_coll_features = pd.DataFrame({'feature':correlated_features_to_delete})","0914ffcd":"multi_coll_features.to_csv(\"correlated_features.csv\",index=False)","37df7efe":"data['target'].describe()","88eff9a8":"sns.boxplot(y=data['target']);","af355297":"data['target'].plot(kind=\"kde\");","8da78b66":"data['target'].quantile(0.01)","b7c8237a":"data['target'].quantile(0.995)","36c7b567":"stats.probplot(data['target'],plot=plt);","5f7a1a79":"non_rare = pd.DataFrame()\nfor i in categorical_features:\n    var_dist = data[i].value_counts().copy()\n    var_dist = (var_dist \/ var_dist.sum()).copy()\n    non_rare = pd.concat([non_rare,pd.DataFrame({i:var_dist[var_dist>0.05].index})],axis=1).copy()\n\nnon_rare.to_csv('.\/non_rare_categories.csv',index=False)","b08d7a0b":"non_rare = pd.read_csv('.\/non_rare_categories.csv')\nnon_rare","16a6dff1":"new_data = data.copy()\nfor i in non_rare.columns:\n    new_data.loc[(new_data[i].isin(non_rare[i]) == False), i] = \"Rare\"\n\nfig,ax = plt.subplots(10,2,figsize=(20,40))\nrow = col = 0\nfor n,i in enumerate(non_rare.columns):\n    cat_dist = data[i].value_counts().copy()\n    cat_dist = np.round((cat_dist \/ cat_dist.sum()) * 100,1).copy()\n    cat_dist.plot(kind=\"bar\",ax=ax[row,0],sharey=False)\n    ax[row,0].set_title(i + \" Before Adding Rare Label\")\n    for n,j in enumerate(cat_dist.index):\n        ax[row,0].text(x=n-0.2,y=cat_dist[j]+0.1,s=str(cat_dist[j]) + \"%\")\n    \n    \n    new_cat_dist = new_data[i].value_counts().copy()\n    new_cat_dist = np.round((new_cat_dist \/ new_cat_dist.sum()) * 100,1).copy()\n    new_cat_dist.plot(kind=\"bar\",ax=ax[row,1])\n    ax[row,1].set_title(i + \" After Adding Rare Label\")\n    for n,j in enumerate(new_cat_dist.index):\n        ax[row,1].text(x=n-0.2,y=new_cat_dist[j]+0.1,s=str(new_cat_dist[j]) + \"%\")\n    \n    \n    row += 1\nplt.show()","5456e477":"for i in non_rare.columns:\n    data.loc[(data[i].isin(non_rare[i]) == False), i] = \"Rare\"","36c99afc":"X = data.drop(columns=\"target\").copy()\ny = data[\"target\"].copy()","e1947855":"# target_non_outliers = y.loc[(y>=5) | (y<=10)].index\n# y = y[target_non_outliers].copy()\n# X = X.iloc[target_non_outliers,:].copy()","861106f7":"#correlated_features = list(pd.read_csv('.\/correlated_features.csv')['feature'])\n#correlated_features","ba2d33c9":"#X.drop(columns=correlated_features,inplace=True)","2aba5aef":"X.columns","33bf5e07":"x_col = list(X.columns)","540b0d45":"len(X.columns)","c818fe84":"#ct = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),categorical_features]],remainder='passthrough')","f3787e2e":"# pipeline = Pipeline(steps=[['ord_encoder',ct],\n#                           ['rfe',RFE(estimator=xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1))],\n#                           ['regressor',xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1)]])","49f33066":"# param_grid = {'rfe__n_features_to_select': range(8,21,2),\n#              'regressor__n_estimators':[200,500],\n#              'regressor__max_depth':[4,7,10,12],\n#              'regressor__reg_lambda':[0.01,0.1,1,10,100]}","9d1a0f2b":"# gscv = GridSearchCV(estimator=pipeline,\n#                    param_grid=param_grid,\n#                    scoring=\"neg_root_mean_squared_error\",\n#                    cv=2,\n#                    n_jobs=-1,\n#                    return_train_score=True,\n#                    verbose=11)","f1bc3a2f":"#gscv.fit(X,y)","e0275476":"#gscv.best_estimator_.get_params()","aa90388a":"#gscv.best_estimator_","12bb1ef2":"#gscv.best_score_ * -1","35aa3e53":"ct = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),categorical_features]],remainder='passthrough')","68c9093e":"pipeline = Pipeline(steps=[['ord_encoder',ct],\n                          ['rfe',RFE(estimator=xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1),\n                                    n_features_to_select=20)],\n                          ['regressor',xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1,\n                                                       max_depth=4,n_estimators=200,reg_lambda=100)]])","734206c1":"pipeline.fit(X,y)","792355fb":"features_after_oe = pd.Series(categorical_features)\nfeatures_after_oe = list(features_after_oe.append(pd.Series(x_col)[pd.Series(x_col).isin(features_after_oe)==False]))\nfeatures_after_oe","8f6950f7":"features_selected_rfe = []\nfor n,i in enumerate(features_after_oe):\n    if pipeline[\"rfe\"].support_[n] == True:\n        features_selected_rfe.append(i)\n        \n    print(f'{i}: {pipeline[\"rfe\"].support_[n]}')","bcafd0f3":"features_selected_rfe","3684009f":"feat_imp = (pd.DataFrame(pipeline['regressor'].get_booster().get_score(importance_type=\"gain\"),index=[0]).T).reset_index()\nfeat_imp['index'] = feat_imp['index'].str.replace('f',\"\").astype('int')\nfeat_imp.sort_values(by=\"index\",inplace=True)\nfeat_imp['index'] = features_selected_rfe\nfeat_imp.sort_values(by=0,ascending=False,inplace=True)\nfeat_imp.columns = [\"Feature\",\"Imp\"]\nfeat_imp","a375386a":"pd.DataFrame({'Feature':features_selected_rfe,'Imp':pipeline['regressor'].feature_importances_}).sort_values(by='Imp',ascending=False)","a2bf2cb1":"X = data.drop(columns=\"target\").copy()\ny = data[\"target\"].copy()","0790a164":"# target_non_outliers = y.loc[(y>=5) | (y<=10)].index\n# y = y[target_non_outliers].copy()\n# X = X.iloc[target_non_outliers,:].copy()","efdc467a":"#correlated_features = list(pd.read_csv('.\/correlated_features.csv')['feature'])\n#correlated_features","b2ef8e6a":"#X.drop(columns=correlated_features,inplace=True)","f0024dd0":"X.columns","f7b2c013":"X['cat2p6'] = X['cat2'] + X['cat6']\nX['cat6p1'] = X['cat6'] + X['cat1']\nX['cat2p1'] = X['cat2'] + X['cat1']\n\nX['cat2p0'] = X['cat2'] + X['cat0']\nX['cat6p0'] = X['cat6'] + X['cat0']\nX['cat1p0'] = X['cat1'] + X['cat0']\n","38750b4c":"new_categorical_features = list(categorical_features).copy()\nnew_categorical_features.extend(['cat2p6','cat6p1','cat2p1','cat2p0','cat6p0','cat1p0'])","e2a06e34":"new_categorical_features","ef50ad1e":"ct = ColumnTransformer(transformers=[['oe',OrdinalEncoder(),new_categorical_features]],remainder='passthrough')","d870d6a6":"pipeline = Pipeline(steps=[['ord_encoder',ct],\n                          ['rfe',RFE(estimator=xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1),\n                                    n_features_to_select=22)],\n                          ['regressor',xgb.XGBRegressor(tree_method='gpu_hist',random_state=11,n_jobs=-1,\n                                                       max_depth=4,n_estimators=200,reg_lambda=100)]])","f0c5b0a6":"cv = cross_validate(estimator=pipeline,X=X,y=y,scoring='neg_root_mean_squared_error',cv=5,n_jobs=-1,return_train_score=True)","4f69fb9f":"cv['train_score'] *-1","6c80b9c3":"np.mean(cv['train_score'] *-1)","62180b9f":"cv['test_score'] *-1","07ce9760":"np.mean(cv['test_score'] *-1)","7f56e5d6":"pipeline.fit(X,y)","ee5f6980":"train_size,train_scores,test_scores = learning_curve(estimator=pipeline,X=X,y=y,cv=5,scoring=\"neg_root_mean_squared_error\",random_state=42)\ntrain_scores = np.mean(-1*train_scores,axis=1)\ntest_scores = np.mean(-1*test_scores,axis=1)\nlc = pd.DataFrame({\"Training_size\":train_size,\"Training_loss\":train_scores,\"Validation_loss\":test_scores}).melt(id_vars=\"Training_size\")","c0ee388d":"sns.lineplot(data=lc,x=\"Training_size\",y=\"value\",hue=\"variable\");","71b8205d":"test = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')","4fd90f01":"test.columns","891f6c19":"test_ids = test[\"id\"].copy()","8386ab56":"test.drop(columns=\"id\",inplace=True)","d7ab670e":"non_rare = pd.read_csv('.\/non_rare_categories.csv')","aee7556b":"for i in non_rare.columns:\n    test.loc[(test[i].isin(non_rare[i]) == False), i] = \"Rare\"","1d7be9ca":"#correlated_features = list(pd.read_csv('.\/correlated_features.csv')['feature'])\n#correlated_features","283727b8":"#test.drop(columns=correlated_features,inplace=True)","00c28a5f":"test['cat2p6'] = test['cat2'] + test['cat6']\ntest['cat6p1'] = test['cat6'] + test['cat1']\ntest['cat2p1'] = test['cat2'] + test['cat1']\n\ntest['cat2p0'] = test['cat2'] + test['cat0']\ntest['cat6p0'] = test['cat6'] + test['cat0']\ntest['cat1p0'] = test['cat1'] + test['cat0']","d76283ef":"test.columns","19daa4c8":"prediction = pipeline.predict(test)","d2e44cc0":"pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/sample_submission.csv').head()","b6a39fcf":"len(test_ids) == len(prediction)","8716b297":"submission = pd.DataFrame({'id':test_ids,'target':prediction})","5a9b1cd7":"submission.head()","d8d25bd7":"submission.shape","8cd5135d":"submission.to_csv('submission_6.csv',index=False)","8977c8ef":"<h2>Looking at the above distribution plots, there is no huge change in the categorical variable distribution. Hence, the rare labels can be combined.","80aeda2a":"<h1>Creating New Features By Combining The Most Important Features","25534ca9":"<h2>Normality checks","4056f548":"<h1>EDA findings<\/h1>\n<h3><ol>\n    <li>No strong relationship seen between numeric and target vaiables.<\/li>\n    <li>Box plots reveal relationship among few categorical variables and target variable.<\/li>\n    <li>There exists multicollinearity among numeric variables.<\/li>\n    <li>There are few categorical variables with rare labels.<\/li>\n    <li>No numeric variable has Gaussian distribution.<\/li>\n    <li>Few numeric variables have outliers<\/li>\n    <li>There are no numeric variables with zero or near zero variance relative to median<\/li>","4541bae0":"<h2>Commenting the code, since, it takes hours for the GridSearch to complete.","7aac9355":"QQ-plot and Kolmogorov-Smirnov test confirms that no continuous variable is normally distributed.","ea347762":"<h2>Learning Curve","0c8b9bce":"<h2>Checking for multicollinearity after removing the correlated features","1d1f3f24":"<h2>Recursively eliminating multicollinarity in numeric features","4255cff9":"<h1>Tabular Playground- Feb 2021","3024da35":"<h2>Not removing correlated variables as it reduced the model performance","f866ea4a":"<h2>CV RMSE","d000432f":"<h3>Few categorical variables have rare labels, which can be combined together. cat4 maybe dropped as 99% of it is made by a single category","ddba415e":"<h2>Correlation","6915e690":"<h3>datatype of all the variables look good","b3270939":"{'memory': None,\n 'steps': [('rfe',\n   RFE(estimator=XGBRegressor(base_score=None, booster=None,\n                              colsample_bylevel=None, colsample_bynode=None,\n                              colsample_bytree=None, gamma=None, gpu_id=None,\n                              importance_type='gain', interaction_constraints=None,\n                              learning_rate=None, max_delta_step=None,\n                              max_depth=None, min_child_weight=None, missing=nan,\n                              monotone_constraints=None, n_estimators=100,\n                              n_jobs=-1, num_parallel_tree=None, random_state=11,\n                              reg_alpha=None, reg_lambda=None,\n                              scale_pos_weight=None, subsample=None,\n                              tree_method='gpu_hist', validate_parameters=None,\n                              verbosity=None),\n       n_features_to_select=18)),\n       \n       \n       \n       XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=0,\n                importance_type='gain', interaction_constraints='',\n                learning_rate=0.300000012, max_delta_step=0, max_depth=4,\n                min_child_weight=1, missing=nan, monotone_constraints='()',\n                n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=11,\n                reg_alpha=0, reg_lambda=100, scale_pos_weight=1, subsample=1,\n                tree_method='gpu_hist', validate_parameters=1, verbosity=None)]],\n       \n       ","026bbbc9":"Pipeline(steps=[('rfe',\n                 RFE(estimator=XGBRegressor(base_score=None, booster=None,\n                                            colsample_bylevel=None,\n                                            colsample_bynode=None,\n                                            colsample_bytree=None, gamma=None,\n                                            gpu_id=None, importance_type='gain',\n                                            interaction_constraints=None,\n                                            learning_rate=None,\n                                            max_delta_step=None, max_depth=None,\n                                            min_child_weight=None, missing=nan,\n                                            monotone_constraints=None,\n                                            n_estimators=10...\n                              colsample_bytree=1, gamma=0, gpu_id=0,\n                              importance_type='gain',\n                              interaction_constraints='',\n                              learning_rate=0.300000012, max_delta_step=0,\n                              max_depth=4, min_child_weight=1, missing=nan,\n                              monotone_constraints='()', n_estimators=200,\n                              n_jobs=-1, num_parallel_tree=1, random_state=11,\n                              reg_alpha=0, reg_lambda=100, scale_pos_weight=1,\n                              subsample=1, tree_method='gpu_hist',\n                              validate_parameters=1, verbosity=None)]])","97c401cc":"<h2>Target variable distribution","28d73ea2":"<h1>Feature Engineering<\/h1>","b5b9ee2f":"<h2>Looking at the distribution of variables before and after combining 'rare' labels would give an idea to proceed further. ","9e6052a9":"<h3>No continuous variable is linearly correlated with the target. However, there is a fair amount of multicollinearity.","b8b6dbb0":"<h3>All the numeric variables are pretty much unique","b5722cb3":"<h2>\n    Training RMSE is 0.829710734<br><br>\n    CV RMSE is 0.8454180152461298<br><br>\n    Submission score is 0.84502 (Top 53%) at the time of writing<br><br>\n    i.e. the model is not overfit, but the learning curve shows bias. The baseline model built using Random Forest had a CV RMSE of 0.859668. The final model built shows just a marginal improvement in performance compared to the baseline model.The top score in the leaderboard at the time of writing was 0.84100. The difference between my score and the top score is just 0.00402. Since the model has bias, using more complex models like catboost, LightGBM and rigorous hyperparameter tuning to reduce the bias might be helpful in increasing the score. Since, the model doesn't have high variance, the submission score and cv score are very close to each other. This might continue in the public leaderboard as well. REMOVING CORRELATED VARIABLES REDUCED THE MODEL PERFORMANCE.","e7caf9e1":"<h2>Outlier detection","59f38ed1":"<h2>Storing the non-rare labels (labels > 5%) in a csv file would help in test set preparation.","be03ed0e":"<h3>Few numeric variables have outliers as shown in box plots","b5116cd1":"<h2>Decoding the pipeline","2aec3f5c":"<h2>Looking for % change between min and max of numeric variables.","ffa482cb":"<h2>Storing numerical, categorical and target variables separately. This makes EDA simpler","6a3ab9b0":"<h2>Training RMSE","4124fd0a":"<h2>Finding Feature Importance For Further Feature Engineering","2df8bf3e":"<h3>The flat validation loss shows that adding additional examples will not help and training error is high and increasing, hence there is no overfitting for sure. But the high training error suggests the model is underfit.","94fc2992":"0.8540331315535704","ea75ddea":"<h3>No missing values","27639e9e":"<h3>Visually mean and median looks to be close for most of the variables implying no presence of outliers, however further investigation is required","20c65553":"<h1>Vanilla RF with cv=3 results<\/h1>\nTrain_RMSE: [0.32163441, 0.32111051, 0.32184251]\n<br>Test_RMSE: [0.8589247 , 0.86120012, 0.85887925]\n<br>Train_R2: [0.86871826, 0.86878779, 0.86855603]\n<br>Test_R2: [0.06125013, 0.06140248, 0.0612373 ]\n<br>Mean Test RMSE as % of Mean Target: 0.11529479488995717\n\n<br><h1>These results show that the model is extremely overfit.","41a076ed":"<h3>Dropping the ID column","1dc5cb93":"<h3>Visually few continuous variables seem to be Gaussian. However, we need to ensure this using Kolmogorov-Smirnov test and qq-plots.","75b1f336":"<h2>Looking for zero or near-zero variance in numeric variables by looking at std in realtion with median","e22ac476":"<h3>Most of the categorival variables show no relationship with the target. But few show up some sort of relationship.","61f78743":"<h3>All the continuous variables have good standard dev (variability) relative to the median.","80936044":"<h1>Test Set Preparation","4774c469":"<h3>Few numeric variables have huge % diff between min and max value","4d4062ab":"<h2>The above EDA shows that an extensive feature engineering is required for linear models to work on this data, since most of the assumptions like feature normality, non-multicollinearity, linear relationship b\/w input features and target are not met. The relationship between the input features and target is also not looking strong and simple. Hence, trying non-linear models may be helpful. We'll build a baseline model with RandomForest.","7d92a418":"<h2>Looking at cardinality of categorical features","92fa44c8":"<h3>The scatter plots, Pearson coeff and Kendall coeff show no linear or monotonic relationship between the numeric and target variables"}}