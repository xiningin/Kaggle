{"cell_type":{"996d915a":"code","5d0d4d5d":"code","76f75aab":"code","7f2cee2c":"code","9f68eccd":"code","d255ee74":"code","7dccf912":"code","537b78c8":"code","fdd522e5":"code","3f0fa90f":"code","eb936a90":"code","3834ba5e":"code","61f71811":"code","7b58bd41":"code","a019416f":"code","b57b4230":"markdown","002e98dd":"markdown","bad35693":"markdown","1a74272d":"markdown","d34d85ed":"markdown","26c6a1bc":"markdown"},"source":{"996d915a":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf","5d0d4d5d":"data = pd.read_csv('..\/input\/european-airlines-catastrophes\/train.csv')\ndata.head()","76f75aab":"data.isnull().sum()","7f2cee2c":"data.describe( include='all' )","9f68eccd":"data['scaleOfIncident'] = data['scaleOfIncident'].map({'low':0, 'medium':1, 'high':2, 'catastrophic':3})","d255ee74":"data","7dccf912":"data = data.drop(['Id','incidentCode','incidentID'], axis = 1)\ndata","537b78c8":"target = data['scaleOfIncident']\n\nfrom sklearn.preprocessing import MinMaxScaler as mms\n\nscaler = mms()\nx_scaled = scaler.fit_transform(data.drop(['scaleOfIncident'], axis = 1))\nx_scaled","fdd522e5":"from sklearn.model_selection import train_test_split as tts\n\nx_train, x_vld, y_train, y_vld = tts(x_scaled, target, test_size = 0.2, random_state = 42)","3f0fa90f":"dnn = tf.keras.models.Sequential() \ndnn.add(tf.keras.layers.Dense(units = 200, activation = 'relu'))\ndnn.add(tf.keras.layers.Dense(units = 200, activation = 'relu'))\ndnn.add(tf.keras.layers.Dense(units = 200, activation = 'relu'))\ndnn.add(tf.keras.layers.Dense(units = 200, activation = 'relu'))\ndnn.add(tf.keras.layers.Dense(units = 200, activation = 'relu'))\ndnn.add(tf.keras.layers.Dense(units = 4, activation = 'softmax'))\n\ndnn.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n\ndnn.fit(x_train, y_train, batch_size = 32, epochs = 20, validation_data = (x_vld, y_vld))","eb936a90":"data_test = pd.read_csv('..\/input\/european-airlines-catastrophes\/test.csv')\ndata_test","3834ba5e":"data_test = data_test.drop(['Id','incidentCode','incidentID'], axis = 1)\n\nx_test_scaled = scaler.transform(data_test)\nx_test_scaled","61f71811":"predictions = dnn.predict(x_test_scaled, verbose=2)\npredictions","7b58bd41":"pred = []\nlegend = {0:'low', 1:'medium', 2:'high', 3:'catastrophic'}\nfor i in predictions:\n    maximum = max(i)\n    for j in range(4):\n        if i[j] == maximum:\n            pred.append(legend[j])\nprint(pred)","a019416f":"file = open(\"\/kaggle\/working\/final.csv\", 'w')\nfile.write('Id,scaleOfIncident\\n')    \nfor i in range(1000):\n    num = 9000 + i\n    file.write(str(num) + ',' + str(pred[i]) + '\\n')  \n    print(num)","b57b4230":"# Preprocessing the Data","002e98dd":"# Scaling the Data","bad35693":"# Imported relevant libraries","1a74272d":"# Splitting Data into train and validation sets","d34d85ed":"# DNN","26c6a1bc":"# Exporting data into CSV file"}}