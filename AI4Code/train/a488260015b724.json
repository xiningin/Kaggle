{"cell_type":{"f9acbf8b":"code","38700193":"code","cd1f4995":"code","579c565b":"code","1f89c4a0":"code","c2c22e1c":"code","09a587bb":"code","93f0e837":"code","4a5663f4":"code","50a8300e":"code","34ae5b09":"code","7aaf317e":"code","5c6e4523":"code","aab186fc":"code","88888191":"code","538b40aa":"code","8e1dd8fb":"code","58c9725f":"code","8171f9a6":"code","72115976":"code","adf12281":"code","2eb89aed":"code","aea93d8b":"code","65f9119b":"code","79fa6427":"code","2bd668d5":"code","99977d19":"code","31fc066f":"code","849d7b95":"code","aa15e6b0":"code","be658a36":"markdown","3c1d63b0":"markdown","817a9298":"markdown","031f8b5f":"markdown"},"source":{"f9acbf8b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38700193":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn as sk\nfrom sklearn import metrics","cd1f4995":"path = r'\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/audi.csv'\ndata = pd.read_csv(path)","579c565b":"data","1f89c4a0":"# The data was cleaned prior to uploading on Kaggle but just checking for null values.\nprint(data.isnull().sum())","c2c22e1c":"data.describe()","09a587bb":"data_onehot = pd.get_dummies(data,columns=['model', 'transmission','fuelType'])","93f0e837":"def plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from scipy import stats\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );","4a5663f4":"plotting_3_chart(data_onehot, 'price')\n#skewness and kurtosis\nprint(\"Skewness: \" + str(data_onehot['price'].skew()))\nprint(\"Kurtosis: \" + str(data_onehot['price'].kurt()))","50a8300e":"from sklearn.model_selection import train_test_split\nX = data_onehot.drop(['price'],axis=1)\ny = data_onehot['price']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=25)","34ae5b09":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\nregressor.score(X,y)","7aaf317e":"results = X_test.copy()\nresults[\"predicted\"] = regressor.predict(X_test)\nresults[\"actual\"]= y_test\nresults = results[['predicted', 'actual']]\nresults['predicted'] = results['predicted'].round(2)\nresults","5c6e4523":"# binary encoding\n\nimport category_encoders as ce\ndata_bin = data.copy()\nencoder = ce.BinaryEncoder(cols=['model','transmission','fuelType'])\ndata_bin = encoder.fit_transform(data_bin)\ndata_bin","aab186fc":"plotting_3_chart(data_bin, 'price')\n#skewness and kurtosis\nprint(\"Skewness: \" + str(data_bin['price'].skew()))\nprint(\"Kurtosis: \" + str(data_bin['price'].kurt()))","88888191":"from sklearn.model_selection import train_test_split\nX = data_bin.drop(['price'],axis=1)\ny = data_bin['price']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=25)","538b40aa":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\nregressor.score(X,y)","8e1dd8fb":"results = X_test.copy()\nresults[\"predicted\"] = regressor.predict(X_test)\nresults[\"actual\"]= y_test\nresults = results[['predicted', 'actual']]\nresults['predicted'] = results['predicted'].round(2)\nresults","58c9725f":"## trainsforming target variable using numpy.log1p,\nlog_data = data_onehot.copy()\nlog_data[\"price\"] = np.log1p(log_data[\"price\"])\n\n## Plotting the newly transformed response variable\nplotting_3_chart(log_data, 'price')\n#skewness and kurtosis\nprint(\"Skewness: \" + str(log_data['price'].skew()))\nprint(\"Kurtosis: \" + str(log_data['price'].kurt()))","8171f9a6":"(log_data.corr()**2)[\"price\"].sort_values(ascending = False)[1:]","72115976":"from sklearn.model_selection import train_test_split\nX = log_data.drop(['price'],axis=1)\ny = log_data['price']\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=25)","adf12281":"# Much improved score after adjusting distribution\n\nfrom sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train,y_train)\nprint('Accuracy on Testing set: %.1f ' %(regressor.score(X_train,y_train)*100))","2eb89aed":"from sklearn.svm import SVR\npipeline_svr=Pipeline([('scalar1',StandardScaler()),\n                     ('pca1',PCA(n_components=2)),\n                     ('lr_classifier',SVR(kernel='linear'))])\npipeline_svr.fit(X_train, y_train)\npipeline_svr.score(X_test,y_test)","aea93d8b":"from sklearn.linear_model import Ridge,ElasticNet\nridge=Ridge(alpha=2,max_iter=1000,random_state=1)\nridge.fit(X_train,y_train)\nprint('Accuracy on Testing set: %.1f ' %(ridge.score(X_test,y_test)*100))","65f9119b":"from sklearn.ensemble import RandomForestRegressor\nrf_reg = RandomForestRegressor(random_state=1)\nrf_reg.fit(X_train, y_train)\nprint('Accuracy on Testing set: %.1f ' %(rf_reg.score(X_test,y_test)*100))","79fa6427":"from sklearn.ensemble import GradientBoostingRegressor\nGB=GradientBoostingRegressor(random_state=0)\nGB.fit(X_test,y_test)\nprint('Performance Score(GB): %.1f ' %(GB.score(X_test,y_test)*100))","2bd668d5":"from xgboost import XGBRegressor\nXGB=XGBRegressor(random_state=0)\nXGB.fit(X_train,y_train)\nprint('Performance score(XGB): %.1f ' %(XGB.score(X_test,y_test)*100))","99977d19":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","31fc066f":"results = X_test.copy()\nresults[\"predicted\"] = np.expm1(XGB.predict(X_test))\nresults[\"actual\"]= np.expm1(y_test)\nresults = results[['predicted', 'actual']]\nresults['predicted'] = results['predicted'].round(2)\nresults","849d7b95":"custom = X_test.copy()\ncustom = custom[custom['model_ A4'] == 1]\ncustom = custom[custom['year'] == 2018]\ncustom = custom[custom['transmission_Automatic'] == 1]\ncustom = custom.iloc[1].copy()\n# # custom['engineSize'] = 1.0\n# # custom['mpg'] = 65\ncustom['mileage'] = 15000\n# # custom['fuelType_Diesel'] = 0\n# # custom['fuelType_Petrol'] = 1\n\ncustom","aa15e6b0":"custom = custom.values.reshape(-1, 1)\nflat_list = []\nfor sublist in custom:\n    for item in sublist:\n        flat_list.append(item)\n\nprint(np.expm1(GB.predict([flat_list])))","be658a36":"# Feature Engineering  \nI will try out both one-hot and binary coding, and compare performance.","3c1d63b0":"# Conclusions  \nPerformance with binary encoding worse than using one-hot encoding.  \n\nNext, I'll try one-hot encoding, and transform the data to reduce skew.","817a9298":"![Image](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/7f\/Audi_logo_detail.svg\/1280px-Audi_logo_detail.svg.png)\n\n# We will use a simple linear regressor to model Audi prices  \nDuring the analysis, we will compare the performance of one-hot vs binary encoding.","031f8b5f":"# Conclusions   \nThe model performs well, and better than the Ford prediction model. This is possibly due to the smaller range of values in the Audi dataset, whereas for Ford we see a much more diverse price range making it difficult to model linearly.  \nNext, we will see how performance changes with different encoding."}}