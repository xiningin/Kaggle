{"cell_type":{"19c15bec":"code","4626b5ae":"code","5914e6bd":"code","e476319b":"code","1fdcf3b7":"code","6a50bf5a":"code","ce1f8ab0":"code","4267051b":"code","283b7f43":"code","cb05796b":"code","5f95f830":"code","2f5210c1":"code","8b4dfd17":"code","ef005306":"code","638002ed":"code","8bade0e1":"code","1e1541b9":"code","69e017c4":"code","aace6f95":"code","58c40643":"code","7517cc0b":"code","655e8760":"code","e8498f5b":"code","b648ac16":"markdown","60996d47":"markdown","ea8bec25":"markdown","347a143d":"markdown","cb9c3b90":"markdown","f25c8fbe":"markdown","c4cabd8e":"markdown","00c177f4":"markdown","ca092a47":"markdown","9a6fc94d":"markdown","88c0f12b":"markdown","298895bf":"markdown","1a15ea35":"markdown","1ab65cdb":"markdown","fa42f097":"markdown","89c385aa":"markdown","83f644f6":"markdown","cf02b643":"markdown","4c997fa2":"markdown","e5aca492":"markdown","1f137aa9":"markdown","a18bfd78":"markdown","8625a9de":"markdown"},"source":{"19c15bec":"# Standard Modules\nimport numpy as np\nimport pandas as pd\n# Graphic Plots\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set_style('darkgrid')\n# Extra Tools\nimport os\nimport warnings\nimport math\nimport random\nfrom sklearn.model_selection import train_test_split\nwarnings.filterwarnings('ignore')\n# PyTorch and Catalyst\nimport collections\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom catalyst.dl import utils\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl.callbacks import AccuracyCallback\nfrom catalyst.utils import set_global_seed, prepare_cudnn\n# For better plot visualization\n%config InlineBackend.figure_format = 'retina'\n# Enviroment PATH\nPATH = '..\/input\/digit-recognizer\/'\nprint(os.listdir(PATH))","4626b5ae":"SEED = 42\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\nset_global_seed(SEED)                       \nprepare_cudnn(deterministic=True)           ","5914e6bd":"FP16_PARAMS = None","e476319b":"%%writefile setup.sh\ngit clone https:\/\/github.com\/NVIDIA\/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/\nrm -rf .\/apex","1fdcf3b7":"%%capture\n!sh setup.sh\nFP16_PARAMS = dict(opt_level=\"O1\") ","6a50bf5a":"train_raw = pd.read_csv(PATH+'train.csv')\ntest_raw = pd.read_csv(PATH+'test.csv')\nsubmission = pd.read_csv(PATH+'sample_submission.csv')\ntrain_raw.head()","ce1f8ab0":"print(f'Train Samples: {train_raw.shape[0]}')\nprint(f'Test Samples: {test_raw.shape[0]}')","4267051b":"plt.figure(figsize=(10,6))\nsns.countplot(x='label', data=train_raw)\nplt.title('Digit Label Distribution');","283b7f43":"label_index = []\nfor i in range(10):\n  label_index.append(train_raw.index[train_raw['label'] == i][0])\n\nfig = plt.figure(figsize=(10,6))\n\nidx = 1\n\nfor i in label_index:\n  plt.subplot(2, 5, idx)\n  plt.imshow(train_raw.iloc[i,1:].values.reshape(28, 28))\n  plt.axis('off')\n  plt.suptitle('MNIST Digits')\n  plt.title(str(train_raw['label'].loc[i]))\n  idx = idx+1\n\nplt.show()","cb05796b":"x_train, x_val, y_train, y_val = train_test_split(train_raw.iloc[:, 1:], \n                                                  train_raw['label'], \n                                                  test_size=0.3, \n                                                  random_state=42)\n\nx_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nx_val.reset_index(drop=True, inplace=True)\ny_val.reset_index(drop=True, inplace=True)","5f95f830":"data_transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])","2f5210c1":"class MnistDataset(Dataset):\n# Get input, labels and transforms\n  def __init__(self, images, labels=None, transform=None):\n    self.x = images\n    self.y = labels\n    self.transform = transform\n# Get data length\n  def __len__(self):\n    return len(self.x)\n# Get data index\n  def __getitem__(self, i):\n    data = np.array(self.x.iloc[i, :], dtype='uint8').reshape([28, 28, 1])\n\n    if self.transform is not None:\n      data = self.transform(data)\n    if self.y is not None:\n      target = self.y.iloc[i]\n      return data, target\n    else:\n      return data","8b4dfd17":"num_class = 10\nbatch_size = 32\nnum_workers = 4\nn_iters = 5000\nn_epochs = int(n_iters\/(len(x_train)\/batch_size))","ef005306":"fake_labels = np.zeros(test_raw.shape[0])\nfake_labels = pd.Series(fake_labels)\nfake_labels = fake_labels.astype(int)","638002ed":"train = MnistDataset(x_train, y_train, transform=data_transform)\nval = MnistDataset(x_val, y_val, transform=data_transform)\ntest = MnistDataset(test_raw, fake_labels, transform=data_transform)","8bade0e1":"train_loader = DataLoader(train, batch_size=batch_size, shuffle=False, num_workers=num_workers)\nval_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=num_workers)\ntest_loader = DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=num_workers)","1e1541b9":"loaders = collections.OrderedDict()\nloaders[\"train\"] = train_loader\nloaders[\"valid\"] = val_loader","69e017c4":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","aace6f95":"class Net(nn.Module):    \n    def __init__(self):\n        super(Net, self).__init__()\n          \n        self.features = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n          \n        self.classifier = nn.Sequential(\n            nn.Dropout(p = 0.5),\n            nn.Linear(64 * 7 * 7, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Dropout(p = 0.5),\n            nn.Linear(512, 10),\n        )\n          \n        for m in self.features.children():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        \n        for m in self.classifier.children():\n            if isinstance(m, nn.Linear):\n                nn.init.xavier_uniform(m.weight)\n            elif isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n                \n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        \n        return x    ","58c40643":"%%time\n# Experiment\nlogdir = '.\/logs\/mnist'\n\nmodel = Net().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nrunner = SupervisedRunner()\n\nrunner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    loaders=loaders,\n    callbacks=[\n        AccuracyCallback(num_classes=num_class),\n    ],\n    logdir=logdir,\n    num_epochs=n_epochs,\n    fp16=FP16_PARAMS,\n    verbose=False\n)","7517cc0b":"!ls .\/logs\/mnist\/checkpoints","655e8760":"runner_out = runner.predict_loader(model=model, loader=test_loader, resume=f\"{logdir}\/checkpoints\/best.pth\")","e8498f5b":"_,results = torch.topk(torch.from_numpy(runner_out),1)\nsubmission.columns \nsubmission_final = pd.DataFrame({'ImageId':submission.ImageId,'Label':np.squeeze(results.numpy())},columns=['ImageId','Label'])\nsubmission_final.to_csv('submission_final.csv',index=False)\nprint(\"Predictions made and Submission file saved!\")","b648ac16":"# 3. Data Preparation for Catalyst\n\nWe start splitting the train dataframe in train and validations sets.","60996d47":"And save the results on submission file for the competition.","ea8bec25":"# 1. Introduction\n\nIn this Kernel I will develop my take on the classic competition of Digit Recognizer using the nice Python module called [Catalyst](https:\/\/github.com\/catalyst-team\/catalyst). It's good because you can write modular and reproducible code to future experiments.<br>\nNow import all required modules.","347a143d":"It's a personal choice, I create fake labels for the test set for avoid any error on Catalyst.","cb9c3b90":"Let's use [NVIDIA Apex](https:\/\/github.com\/NVIDIA\/apex) to work with [Mixed Precision Training](https:\/\/docs.nvidia.com\/deeplearning\/sdk\/mixed-precision-training\/index.html), also called Automatic Mixed Precision (AMP).\nSet it to use FP16 and install Apex.","f25c8fbe":"And set the enviroment settings for reprodutibility.","c4cabd8e":"And get these datasets and put they on Loaders. <br>\nLoaders in Catalyst provide parameters like batch size together the data.","00c177f4":"# 5. Predictions\n\nOk, let's run our prediction. Remember the fake labels? I used here to don't get errors on predict.","ca092a47":"![](https:\/\/raw.githubusercontent.com\/catalyst-team\/catalyst-pics\/master\/pics\/catalyst_logo.png)","9a6fc94d":"It's possible to check the saved models on the `logdir` directory defined early.","88c0f12b":"Although this is an image competition, is provided tabular data, where each feature column represents an image pixel. <br>\nLet's see how many samples (images) it have.","298895bf":"Check the label distribution to find any disbalance.","1a15ea35":"Time to see how the samples look like images. For this, they will reshaped for images with 28x28 pixels, because the product of this results in 784, the number of columns\/pixels in the dataset (except label column). ","1ab65cdb":"For the Catalyst, we have to write a class called dataset to prepare the data for the network. <br>\nOn the Dataset class is defined the input, label and any process, like tranformations, in the data.","fa42f097":"Now, we assemble each component created before and run the expirement on Catalyst. <br>\nLet's start training the network.","89c385aa":"As we have a Pandas Dataframe with pixel values, it's needed to transform in tensors, they will be input of the Network. <br>\nThe pixels are normalized with the transform.\nLet's just write how the transform will work.","83f644f6":"# 4. Model\n\nTime to build the model! This is a nice part on Catalyst, it's modular, is easy to change any part like the network. <br>\nIf you see on this competition, you will find a lot of interesting model networks, I think that the model that is provided [here](https:\/\/www.kaggle.com\/juiyangchang\/cnn-with-pytorch-0-995-accuracy) is good enough.","cf02b643":"Put the train and validation loaders in dictionares to feed the network.","4c997fa2":"# 6. Conclusion\n\nCatalyst is awesome module to work with Deep Learning, it is based on the great PyTorch, but without use any complicated code lines and is friendly user. <br>\nAnother advantage, due the PyTorch, is that easy to change and classification and segmentation tasks. Catalyst is really worth to give a try!\nAbout the Mixed Precision Training, it is so good, because the task gain more power to deal with big datasets, like images.","e5aca492":"Here, we define the parameters of the experiment, like Batch Size, Iterations, etc.","1f137aa9":"We have the advantage of use GPU, let's check for any available and save it for use on Catalyst.","a18bfd78":"Time to convert the data on Catalyst Dataset.","8625a9de":"# 2. Load Datasets and Explore the Images\n\nLoad the datasets in Pandas Dataframes and take a look on the Train Set to check the dataset format."}}