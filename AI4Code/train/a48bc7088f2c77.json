{"cell_type":{"e1716045":"code","61665a91":"code","06d742a0":"code","94f83c72":"code","89bb269e":"code","9967cb8f":"code","8a073970":"code","11414756":"code","7c74ea30":"code","6cc2edc8":"code","49be1bee":"code","7cd4e00b":"code","aa203bd5":"code","0237df42":"code","3c48d455":"code","7169ba67":"code","5642dd6f":"code","9c9edfb7":"code","4f35e831":"code","70bfd972":"code","564882f5":"code","ce271a6f":"code","8321d43a":"code","08f328f7":"code","11b97737":"code","81f9519f":"markdown","c4a4bda0":"markdown","37c099e4":"markdown"},"source":{"e1716045":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport cupy as cp # linear algebra\nimport cudf # data processing, CSV file I\/O (e.g. cudf.read_csv)\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom cuml.metrics import roc_auc_score\nimport shap\nimport gc\nfrom random import shuffle\nfrom sklearn.preprocessing import LabelEncoder\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61665a91":"import xgboost\nxgboost.__version__","06d742a0":"train = pd.read_csv('\/kaggle\/input\/tps-april-2021-feature-eng-only\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tps-april-2021-feature-eng-only\/test.csv')","94f83c72":"columns = test.columns[1:]\ncolumns","89bb269e":"len(columns)","9967cb8f":"train.shape","8a073970":"test.shape","11414756":"target = np.hstack([np.ones(train.shape[0]), np.zeros(test.shape[0])])","7c74ea30":"train_test = np.vstack([train[columns].values, test[columns].values])","6cc2edc8":"train_test.shape","49be1bee":"index = list(range(train_test.shape[0]))\nshuffle(index)","7cd4e00b":"train_test = train_test[index, :]\ntarget = target[index]","aa203bd5":"train_test[:,0]","0237df42":"train_test = train_test.astype(np.float)","3c48d455":"train, test, y_train, y_test = train_test_split(train_test, target, test_size=0.33, random_state=42)","7169ba67":"del train_test\ngc.collect()\ngc.collect()","5642dd6f":"train = xgboost.DMatrix(train, label=y_train)\nval = xgboost.DMatrix(test, label=y_test)","9c9edfb7":"%%time\nparam = {\n    'eta': 0.05,\n    'max_depth': 10,\n    'subsample': 0.8,\n    'colsample_bytree': 0.7,\n    'objective': 'reg:logistic',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist', \n    'predictor': 'gpu_predictor'\n}\nclf = xgboost.train(param, train, 600)","4f35e831":"preds = clf.predict(val)","70bfd972":"roc_auc_score(y_test, preds)","564882f5":"%%time\nshap_preds = clf.predict(val, pred_contribs=True)","ce271a6f":"shap_preds.shape","8321d43a":"shap.initjs()","08f328f7":"shap.summary_plot(shap_preds[:,:-1], pd.DataFrame(test, columns=columns))","11b97737":"shap.summary_plot(shap_preds[:,:-1], pd.DataFrame(test, columns=columns), plot_type=\"bar\")","81f9519f":"Seeems that thre is a discrepancy in Age and Fare in particular between the train and test sets.","c4a4bda0":"The AUC of 0.876 is actually incredibly high, especially for a synthetic dataset that has presumably been evenly split. The most likely reason for this is becuase the feature encoding that has been used left created certain artifacts that can indicate the provenance of individual data points. \n\nLet's now take a look at the feature importances.","37c099e4":"One of the main issues that make Kaggle (and for tahat matter any other) predictive modeling tricky are the discrepancies between the training and the test datasets. In order to get an idea of the magnitude of these differences, one of the more valuable tools to use is adversarial validation. With aversariel validation we try to build an auxiliary model that predicts whether given data points belong to the train and the test set. If we can make predictions with such a model with a high degree of confidence, then that usually means that the train and test sets are significantly different, and we need to be careful to make a model that will take that into the account.\n\nWe will make this adversarial validation notebook with the Rapids library. [Rapids](https:\/\/rapids.ai) is an open-source GPU accelerated Data Sceince and Machine Learning library, developed and mainatained by [Nvidia](https:\/\/www.nvidia.com). It is designed to be compatible with many existing CPU tools, such as Pandas, scikit-learn, numpy, etc. It enables **massive** acceleration of many data-science and machine learning tasks, oftentimes by a factor fo 100X, or even more. \n\nRapids is still undergoing developemnt, and only recently has it become possible to use RAPIDS natively in the Kaggle Docker environment. If you are interested in installing and riunning Rapids locally on your own machine, then you should [refer to the followong instructions](https:\/\/rapids.ai\/start.html).\n\nFor the modeling part we'll use the latest version of XGBoost, which allows for GPU accelerated calculation of Shapely Values. We'll use these \"SHAP\" values to calculate correct feature importances. Starting with the version 1.3, XGBoost supports fast calculation of the SHAP values on GPU. "}}