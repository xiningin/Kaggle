{"cell_type":{"d4aca82a":"code","962e61c1":"code","14d7b962":"code","0883843e":"code","11f59cca":"code","98f84ed6":"code","d1083976":"code","f1e61702":"code","cab3b114":"code","9938c122":"code","d15429b3":"code","32cd1ec8":"markdown","e0a3b5cb":"markdown","f8567127":"markdown","8bef75b6":"markdown","a7c95e09":"markdown","e6d435ae":"markdown","4669c4fc":"markdown","cb2e773b":"markdown","c96eaa28":"markdown","6345f1d2":"markdown"},"source":{"d4aca82a":"# Importing all necessary\nimport numpy as np\nimport gc\nimport os\nimport matplotlib.pyplot as plt\n\nfrom matplotlib.animation import ArtistAnimation\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom pylab import meshgrid\nfrom IPython.display import Image, display, HTML\n\nplt.style.use('bmh')","962e61c1":"# Main function\ndef f(x):\n    return x**2\n\n# Derivative of function\ndef f_prime(x):\n    return 2 * x","14d7b962":"fig, ax = plt.subplots(figsize = (10, 8))\n\n# Domain of function\nx = np.arange(-5, 5, 0.1)\n\nplt.plot(x, f(x), color = 'b', linewidth = 2, label = 'f(x)') # Plot the main function\nplt.plot(x, f_prime(x), color = 'r', label = \"f_prime(x)\") # Plot the derivative of function\nplt.scatter(0, 0, color = 'k', zorder = 2.5) # Global minimum of the function\n\n# Annotations block\nbbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\narrow_args = dict(arrowstyle = '->', color = 'k', linewidth = 2)\nax.annotate('Global minimum\\n$f\\'(x)=0$', xy = (0, 0), xytext = (-2, 8), bbox = bbox_args, arrowprops=arrow_args)\n\nplt.title(\"$f(x)=x^2$ and it's derivative $f'(x)=2x$\")\nplt.legend()\nplt.show()","0883843e":"def gradient_descent(function, derivative, start_x = -4, x_domain = (-5, 5, 0.1), iterations = 1000, lr = 0.1, precision = 0.001, \n                     figsize = (10, 8), name = 'anim.gif', fps = 5):\n    '''\n    Creates an animation of gradient descent from specific point to the closest local minimum and saves it at the \"name\" name.\n    function - described function, must be passed as function\n    derivative - derivative of \"function\", must be passed as function\n    start_x - starting X coordinate of point\n    x_range - domain of function as tuple - (left_bound, right_bound, step)\n    iterations - maximum number of iteration before stop\n    lr - learning rate\n    precision - desired precision of result, if step of gradient descent will be smalle than precision, then algorithm will stop\n    figsize - size of a plot\n    name - name of animation to save\n    fps - fps of animation\n    '''\n    \n    images = []    \n    fig, ax = plt.subplots(figsize = figsize)\n    x = np.arange(*x_domain)\n    px = float(start_x) # Starting x coordinate\n\n    # Function plot\n    f = plt.plot(x, function(x), color = 'k')\n\n    for frame in range(iterations):\n        old_px = px # Saving old px value to calculate step\n\n        # Plot point to track\n        py = function(px) # Y coordinate of point    \n        point = plt.scatter(px, py, color = 'r', zorder = 2.5)\n\n        # Plot tangent line to the graph at (px, py) point\n        slope = derivative(px)\n        y_intercept = py - slope * px\n        tx = np.arange(px - 1, px + 2, 2) # X coordinates of tangent line\n        ty = slope * tx + y_intercept # Y coordinates of tangent line\n        tangent = plt.plot(tx, ty, 'r--')    \n\n        # Calculate new value of px\n        px = px - lr * slope\n        step = abs(old_px - px)\n\n        # Plot text info\n        bbox_args = dict(boxstyle=\"round\", fc=\"0.8\")\n        arrow_args = dict(arrowstyle = '->', color = 'b', linewidth = 1)\n        text = f'Iteration: {frame}\\nPoint: ({px:.2f}, {py:.2f})\\nSlope: {slope:.2f}\\nStep: {step:.4f}'\n        text = ax.annotate(text, xy = (px, py), xytext = (0.7, 0.9), textcoords = 'axes fraction', bbox = bbox_args, arrowprops=arrow_args, fontsize = 12)\n\n        plt.title('Gradient descent animation')    \n        \n        # Stopping algorithm if desired precision have been met\n        if step <= precision:\n            text2 = plt.text(0.7, 0.1, 'Local minimum found', fontsize = 12, transform = ax.transAxes)\n            images.append([f[0], tangent[0], point, text, text2])\n            break\n\n        images.append([f[0], tangent[0], point, text])\n\n    anim = ArtistAnimation(fig, images) \n    anim.save(name, writer = 'imagemagic', fps = fps)","11f59cca":"# Creating animation\ngradient_descent(f, f_prime, start_x = -4, lr = 0.1, name = 'classic.gif')","98f84ed6":"# Displaying animation\nHTML('<img src=\".\/classic.gif\" \/>')","d1083976":"def f(x):\n    return np.sin(x)\n\ndef f_prime(x):\n    return np.cos(x)\n\ngradient_descent(f, f_prime, start_x = np.random.uniform(-5, 5), name = 'sin.gif')","f1e61702":"HTML('<img src=\".\/sin.gif\" \/>')","cab3b114":"# Defining main function and partial derivatives\ndef f(x, y):\n    return np.sin(1\/2 * x**2 - 1\/4 * y**2 + 3) * np.cos(2 * x + 1 - np.e**y)\n\ndef partial_x(x, y):\n    return x * np.cos(1\/2 * x**2 - 1\/4 * y**2 + 3) * np.cos(2 * x + 1 - np.e**y) - 2 * np.sin(1\/2 * x**2 - 1\/4 * y**2 + 3) * np.sin(2 * x + 1 - np.e**y)\n\ndef partial_y(x, y):\n    return -1\/2 * y * np.cos(1\/2 * x**2 - 1\/4 * y**2 + 3) * np.cos(2 * x + 1 - np.e**y) + np.e**y * np.sin(1\/2 * x**2 - 1\/4 * y**2 + 3) * np.sin(2 * x + 1 - np.e**y)\n\n\n# Domain of function\nx = np.arange(-1.5, 1.6, 0.01)\ny = np.arange(-1.5, 1.6, 0.01)\n\n# Creating (x, y) pairs and calculating Z coordiante\nX, Y = meshgrid(x, y)\nZ = f(X, Y)\n\n# Plot function\nfig = plt.figure(figsize = (20, 8))\nax = fig.add_subplot(1, 2, 1, projection = '3d')\nax.plot_surface(X, Y, Z, cmap = 'twilight')\nax.view_init(25, 100)\nax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z')\n\nfig.add_subplot(1, 2, 2)\nplt.contour(X, Y, Z, levels = 30, cmap = 'twilight')\n\nplt.show()","9938c122":"# List to store images\nimages = []\n\n# Function domain\nx = np.arange(-1.5, 1.6, 0.01)\ny = np.arange(-1.5, 1.6, 0.01)\n\n# Creating (x, y) pairs and calculating Z coordiante\nX, Y = meshgrid(x, y)\nZ = f(X, Y)\n\niterations = 200 # number of iterations before algorithm will stop\nlr = 0.01 # Learning rate\n\n# Plot main function\nfig = plt.figure(figsize = (20, 12))\nax = fig.add_subplot(111, projection = '3d')\nax.plot_surface(X, Y, Z, cmap = 'twilight', alpha = 0.9)\nax.view_init(45, 45)\nax.set_xlabel('X')\nax.set_ylabel('Y')\nax.set_zlabel('Z')\n\n# Starting coordinates of points\npx = np.array([-0.25, 0, 1.25, -1.25, 0, 0.25, -1])\npy = np.array([-1.25, -1.25, 0.25, 0.25, 0, 0.75, -1])\n\n# Main algorithm cycle\nfor frame in range(iterations):\n    # Calculating Z-coordinate for each point\n    pz = [f(p[0], p[1]) for p in zip(px, py)]\n    \n    # Plot points\n    points = ax.plot(px, py, pz, marker = '.', linestyle = 'None', color = 'r', markersize = 10, zorder = 2.5)\n    \n    # Calculating partial derivatives for each point\n    gx = np.array([partial_x(p[0], p[1]) for p in zip(px, py)])\n    gy = np.array([partial_y(p[0], p[1]) for p in zip(px, py)])\n    \n    # Subtract partial derivatives from x and y coordinates\n    px = px - lr * gx\n    py = py - lr * gy\n    \n    images.append([ax, points[0]])\n\nanim = ArtistAnimation(fig, images) \nanim.save('multivar.gif', writer = 'imagemagic', fps = 20)","d15429b3":"HTML('<img src=\".\/multivar.gif\" \/>')","32cd1ec8":"# Sin(x) function","e0a3b5cb":"Now the most interesting part - animation of gradient descent of multivariable function. In this example I want to take 7 points in different parts of the plot and to apply gradient descent algorithm to each of these points:","f8567127":"Let's try this on $f(x)=sin(x)$ function with random starting point:","8bef75b6":"# \"Classic\" example","a7c95e09":"On a plot above we can see our parabola and it's derivative (red line). Derivative it's nothing more than instanteneous rate of change of the function in certain point. In that particular case - when derivative of our function is equal to 0 - then we are at global minimum of our function.\n\nNow let's start realization of gradient descent algorithm.\n\nThe algorithm is very simple and can be divided in several steps:\n1. Define a starting x coordinate, from which we want to descent to local\/global minimum\n2. Calculate derivative at this point\n3. Subtract from starting point derivative at this point. To prevent divergence we need to multiply the derivative on small number \"alpha\", or learning rate.\n4. Repeat from step 2\n\nSo the main gradient descent formula can be written as: $x_{next} = x_{start} - \\alpha * derivative$","e6d435ae":"# Gradient descent of multivariable function","4669c4fc":"Now let's take something more interesting - a multivariable function with 2 input variables and 1 output.\n\n$f(x, y)=sin(\\frac{1}{2}x^2-\\frac{1}{4}y^2+3)cos(2x+1-e^y)$\n\nAs in previous example - to succesfully descent to local minimum we need to calculate derivaties but, because we have multivariable function, we need to calculate partial derivatives wit respect to X and Y:\n1. Partial derivative with respect to X:\n\n$\\frac{\\partial f}{\\partial x}=xcos(\\frac{1}{2}x^2-\\frac{1}{4}y^2+3)cos(2x+1-e^y)-2sin(\\frac{1}{2}x^2-\\frac{1}{4}y^2+3)sin(2x+1-e^y)$\n\n2. Partial derivative with respect to Y:\n\n$\\frac{\\partial f}{\\partial y}=-\\frac{1}{2}ycos(\\frac{1}{2}x^2-\\frac{1}{4}y^2+3)cos(2x+1-e^y)+e^ysin(\\frac{1}{2}x^2-\\frac{1}{4}y^2+3)sin(2x+1-e^y)$\n\nThese partial derivatives are necessary parts of gradient - the vector, wich shows direction of steepest ascend of the function:\n\n$\\nabla f(x, y)=\\begin{bmatrix} \\frac{\\partial f}{\\partial x} \\\\ \\frac{\\partial f}{\\partial y} \\end{bmatrix}$\n\nSo, if we take opposite of that vector - it will show us the direction of steepest descent. First - let's plot the function to see how it looks like.","cb2e773b":"I want to start with something simple - the \"classic\" parabola example. To make it we need two things - function and derivative of that function.\n\nAs a function I'm going to use: $f(x)=x^2$\n\nAnd we can easily calculate it's derivative: $f'(x)=2x$\n\nFirst - let's define function and plot it:","c96eaa28":"# Goal","6345f1d2":"Gradient descent - a very simple and powerfull algorithm that is used to find a local minimum of a function. In this kernel I want to create a couple of animations and show how this algorithm works so lets start."}}