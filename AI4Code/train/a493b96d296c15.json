{"cell_type":{"c029954b":"code","63d2cf06":"code","d2633d29":"code","f0080f1f":"code","5df49e58":"code","7825aef2":"code","7b374c46":"code","85b868c1":"code","faba83cf":"code","9f8f2529":"code","677b8649":"code","87e7981c":"code","70160460":"code","c60c3dab":"code","b05a5f9d":"code","4762179b":"code","e53ebf34":"code","c6cbb1b4":"code","851f3e1d":"code","c6152eae":"code","200ab7ad":"code","69908ed2":"code","d31e96a8":"code","7fee78cc":"code","37e45fc7":"code","d8c8da20":"code","5a2f8201":"code","b1db5730":"code","c219e7d2":"code","61fda657":"markdown"},"source":{"c029954b":"import sys\nimport pandas\nimport numpy\nimport sklearn\nimport keras\n\nprint('Python: {}'.format(sys.version))\nprint('Pandas: {}'.format(pandas.__version__))\nprint('Numpy: {}'.format(numpy.__version__))\nprint('Sklearn: {}'.format(sklearn.__version__))\nprint('Keras: {}'.format(keras.__version__))","63d2cf06":"import pandas as pd\nimport numpy as np\n\nnames = ['n_pregnant', 'glucose_concentration', 'blood_pressuer (mm Hg)', 'skin_thickness (mm)', 'serum_insulin (mu U\/ml)',\n        'BMI', 'pedigree_function', 'age', 'class']\n#df = pd.read_csv('..\/input\/diabetes.csv', names = names)\ndf = pd.read_csv('..\/input\/diabetes.csv')\ndf.head()","d2633d29":"# Describe the dataset\ndf.describe()","f0080f1f":"df[df['Glucose'] == 0]","5df49e58":"# Preprocess the data, mark zero values as NaN and drop\ncolumns = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n\nfor col in columns:\n    df[col].replace(0, np.NaN, inplace=True)\n    \ndf.describe()","7825aef2":"# Drop rows with missing values\ndf.dropna(inplace=True)\n\n# summarize the number of rows and columns in df\ndf.describe()","7b374c46":"# Convert dataframe to numpy array\ndataset = df.values\nprint(dataset.shape)","85b868c1":"# split into input (X) and an output (Y)\nX = dataset[:,0:8]\nY = dataset[:, 8].astype(int)","faba83cf":"print(X.shape)\nprint(Y.shape)\nprint(Y[:5])","9f8f2529":"# Normalize the data using sklearn StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X)","677b8649":"print(scaler)","87e7981c":"# Transform and display the training data\nX_standardized = scaler.transform(X)\n\ndata = pd.DataFrame(X_standardized)\ndata.describe()","70160460":"# import necessary sklearn and keras packages\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\n\n# Do a grid search for the optimal batch size and number of epochs\n# import necessary packages\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint","c60c3dab":"\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(16, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(8, input_dim = 16, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile the model\n    adam = Adam(lr = 0.01)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, verbose = 0)\n\n# define the grid search parameters\nbatch_size = [16, 32, 64,128]\nepochs = [2, 5, 10]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(batch_size=batch_size, epochs=epochs)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))\n","b05a5f9d":"best_batch_size = 64\nbest_epochs = 10 # 100","4762179b":"\n# Do a grid search for learning rate and dropout rate\n# import necessary packages\nfrom keras.layers import Dropout\n\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(learn_rate, dropout_rate):\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(16, input_dim = 8, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(8, input_dim = 16, kernel_initializer='normal', activation='relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile the model\n    adam = Adam(lr = learn_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = best_epochs, batch_size = best_batch_size, verbose = 0)\n\n# define the grid search parameters\nlearn_rate = [0.001, 0.01, 0.1]\ndropout_rate = [0.0, 0.2, 0.4,0.6]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(learn_rate=learn_rate, dropout_rate=dropout_rate)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","e53ebf34":"best_dropout_rate = 0.0\nbest_learn_rate = 0.01","c6cbb1b4":"\n# Do a grid search to optimize kernel initialization and activation functions\n# import necessary packages\n\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(activation, init):\n    # create model\n    model = Sequential()\n    model.add(Dense(8, input_dim = 8, kernel_initializer= init, activation= activation))\n    model.add(Dense(16, input_dim = 8, kernel_initializer= init, activation= activation))\n    model.add(Dense(8, input_dim = 16, kernel_initializer= init, activation= activation))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile the model\n    adam = Adam(lr = best_learn_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = best_epochs, batch_size = best_batch_size, verbose = 0)\n\n# define the grid search parameters\nactivation = ['softmax', 'relu', 'tanh', 'linear']\ninit = ['uniform', 'normal', 'zero']\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(activation = activation, init = init)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","851f3e1d":"best_activation = 'relu'\nbest_init = 'normal'","c6152eae":"\n# Do a grid search to find the optimal number of neurons in each hidden layer\n# import necessary packages\n\n# Define a random seed\nseed = 6\nnp.random.seed(seed)\n\n# Start defining the model\ndef create_model(neuron1, neuron2, neuron3):\n    # create model\n    model = Sequential()\n    model.add(Dense(neuron1, input_dim = 8, kernel_initializer= best_init, activation= best_activation))\n    model.add(Dense(neuron2, input_dim = neuron1, kernel_initializer= best_init, activation= best_activation))\n    model.add(Dense(neuron3, input_dim = neuron2, kernel_initializer= best_init, activation= best_activation))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # compile the model\n    adam = Adam(lr = best_learn_rate)\n    model.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\n    return model\n\n# create the model\nmodel = KerasClassifier(build_fn = create_model, epochs = best_epochs, batch_size = best_batch_size, verbose = 0)\n\n# define the grid search parameters\nneuron1 = [8, 16, 32]\nneuron2 = [16, 32, 64]\nneuron3 = [8, 16, 32]\n\n# make a dictionary of the grid search parameters\nparam_grid = dict(neuron1 = neuron1, neuron2 = neuron2, neuron3 = neuron3)\n\n# build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model, param_grid = param_grid, cv = KFold(random_state=seed), refit = True, verbose = 10)\ngrid_results = grid.fit(X_standardized, Y)\n\n# summarize the results\nprint(\"Best: {0}, using {1}\".format(grid_results.best_score_, grid_results.best_params_))\nmeans = grid_results.cv_results_['mean_test_score']\nstds = grid_results.cv_results_['std_test_score']\nparams = grid_results.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{0} ({1}) with: {2}'.format(mean, stdev, param))","200ab7ad":"best_neuron1 = 16\nbest_neuron2 = 16\nbest_neuron3 = 8","69908ed2":"from sklearn.model_selection import train_test_split\n#best model\nmodel = Sequential()\nmodel.add(Dense(best_neuron1, input_dim = 8, kernel_initializer= best_init, activation= best_activation))\nmodel.add(Dense(best_neuron2, input_dim = best_neuron1, kernel_initializer= best_init, activation= best_activation))\nmodel.add(Dense(best_neuron3, input_dim = best_neuron2, kernel_initializer= best_init, activation= best_activation))\nmodel.add(Dense(1, activation='sigmoid'))","d31e96a8":"# compile the model\nadam = Adam(lr = best_learn_rate)\nmodel.compile(loss = 'binary_crossentropy', optimizer = adam, metrics = ['accuracy'])\nckpt_model = 'pima-weights_best_t.hdf5'\ncheckpoint = ModelCheckpoint(ckpt_model, \n                            monitor='val_acc', \n                            verbose=1,\n                            save_best_only=True,\n                            mode='max')\ncallbacks_list = [checkpoint]\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=0)\n\nprint(X_train.shape)\nprint(X_test.shape)\n\nhistory = model.fit(X_train,\n                    y_train,\n                    validation_data=(X_test, y_test),\n                    nb_epoch=best_epochs,\n                    batch_size=best_batch_size,\n                    callbacks=callbacks_list,\n                    verbose=1)","7fee78cc":"model.load_weights(\"pima-weights_best_t.hdf5\")","37e45fc7":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.3f%%\" % (model.metrics_names[1], scores[1]*100))","d8c8da20":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Model accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","5a2f8201":"# Model Losss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'])\nplt.show()","b1db5730":"# confusion matrix\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n# Predict the values from the validation dataset\ny_pred = model.predict(X_test)\ny_final = (y_pred > 0.5).astype(int).reshape(X_test.shape[0])\n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_test, y_final) \n# plot the confusion matrix\nf,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","c219e7d2":"from sklearn.metrics import classification_report\n\n# Generate a classification report\nreport = classification_report(y_test, y_final, target_names=['0','1'])\n\nprint(report)","61fda657":"# Deep Learning Grid Search\n\nIn this project, we will learn how to use the scikit-learn grid search capability.\n\nWe are going to learn the following topics:\n\n* How to use Keras models in scikit-learn.\n* How to use grid search in scikit-learn.\n* How to tune batch size and training epochs.\n* How to tune learning rate\n* How to tune network weight initialization.\n* How to tune activation functions.\n* How to tune dropout regularization.\n* How to tune the number of neurons in the hidden layer."}}