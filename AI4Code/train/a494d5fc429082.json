{"cell_type":{"36bd830e":"code","5570bbe3":"code","7fca12e5":"code","8f3d7223":"code","ef2b8494":"code","084a7e14":"code","58fa248a":"code","80e8ec98":"code","13574905":"code","55c4b2c2":"code","b3c49480":"code","2d1408e2":"code","baaf6b74":"code","a14b110e":"code","9910b4b7":"code","977008cf":"code","a76b45fe":"code","bb7f024c":"code","1bfb96cf":"code","7c9803a8":"code","012236b7":"code","85f5494a":"code","6e2b3161":"code","06cb6efd":"code","5859e212":"markdown","c6363e07":"markdown","b4867b47":"markdown","93555bc3":"markdown","76e8db06":"markdown","a7de6777":"markdown","ce11524e":"markdown","89b2cae1":"markdown"},"source":{"36bd830e":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","5570bbe3":"#creating a dataframe object from out train csv file\nmovies_df=pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\nmovies_df = movies_df.iloc[:5000,:]","7fca12e5":"movies_df.info()\n#label=0 tells the review is positive and 1 tells that it was a negative review","8f3d7223":"movies_df.describe()","ef2b8494":"movies_df.head()","084a7e14":"print(movies_df.isnull().sum())\n#there are no null entries or NaN\n#changing column names\nmovies_df.columns=['review','label']","58fa248a":"sns.heatmap(movies_df.isnull(),yticklabels=False,cbar=False,cmap='Blues')\n#Everything is blue , there are no null values","80e8ec98":"movies_df['label'].hist(figsize=(13,5),color='g')\n#positive reviews are around 2500 and negative reviews are around 2500","13574905":"sns.countplot(movies_df['label'])\n#this gives a clearer and prioritized picture.","55c4b2c2":"positive=movies_df[movies_df['label']=='positive']\nnegative=movies_df[movies_df['label']=='negative']\n#gets the object for pos and neg reviews","b3c49480":"#grab the review column and convert into one massive string\nsentences=movies_df['review'].tolist()\nsentences=''.join(sentences)","2d1408e2":"from wordcloud import WordCloud\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(sentences))","baaf6b74":"#let's see the positive movie reviews' words\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(''.join(positive['review'].tolist())))","a14b110e":"#similarly, for negative movie reviews\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate(''.join(negative['review'].tolist())))","9910b4b7":"import string #for punctuation\nimport nltk #natural language tool kit\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nprint(string.punctuation)\nprint('------------------------------------------------------------------------------------------------------------------------')\nprint(stopwords.words('english'))\nredundant_words=['br']\ndef text_cleaning(sentence): #sentence is text(paragraph)\n    sentence_punc_removed=[letter for letter in sentence if letter not in string.punctuation]\n    sentence_punc_removed=''.join(sentence_punc_removed) # punctuation removed\n    #stopwords are filtered out\n    sentence_clean=[word for word in sentence_punc_removed.split() if word.lower() not in stopwords.words('english')]\n    # redundant words are filtered out\n    sentence_clean=[word for word in sentence_clean if word.lower() not in redundant_words ] ##array of words of that sentence\n    # lemmatization is done here.\n    final_sentence = [lemmatizer.lemmatize(word.lower()) for word in sentence_clean]\n    return final_sentence # ARRAY OF WORDS","977008cf":"movies_df_clean=movies_df['review'].apply(text_cleaning) # ARRAY OF ARRAY OF WORDS\n# print(len(movies_df_clean))\nsentences=movies_df_clean\nlistOfSentences=list()\nfor sentence in sentences:\n    listOfSentences.append(' '.join(sentence))\nfrom wordcloud import WordCloud\nplt.figure(figsize=(20,20))\nplt.imshow(WordCloud().generate('\\n'.join(listOfSentences)))","a76b45fe":"\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n#here, we performed data cleaning and count vectorization sequentially altogether !\n\nmovies_vectorizer=CountVectorizer(analyzer=text_cleaning,dtype='uint8').fit_transform(movies_df['review']) #transforms text into numeric vectorized format","bb7f024c":"print(type(movies_vectorizer)) # type vector\nX=movies_vectorizer.toarray() # type 2d matrix\nprint(X.shape)","1bfb96cf":"y=movies_df['label']","7c9803a8":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state = 42) #setting up train and test datasets\nX_test.shape","012236b7":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\n\nNB_classifier=MultinomialNB()\nsvc = LinearSVC()\nNB_classifier.fit(X_train,y_train) #training our model using training dataset\nsvc.fit(X_train, y_train)","85f5494a":"from sklearn.metrics import confusion_matrix , classification_report\n\ny_test_predictions=NB_classifier.predict(X_test)\ny_test_pred_svm = svc.predict(X_test)","6e2b3161":"cm=confusion_matrix(y_test,y_test_predictions)\ncm2=confusion_matrix(y_test,y_test_pred_svm)\n\nfig, axis = plt.subplots(1, 2, figsize=(15, 5))\n\nsns.heatmap(ax = axis[0],data= cm,annot=True)\naxis[0].set_title(\"With Naive Bayes\")\n\nsns.heatmap(ax = axis[1],data=cm2,annot=True)\naxis[1].set_title(\"With Linear SVC\")","06cb6efd":"print(classification_report(y_test,y_test_predictions))\nprint(classification_report(y_test,y_test_pred_svm))","5859e212":"## Assessing Performance and making Report\n\nWe are going to use confusion matrix which is going to tell how **OFTEN** our predictions are right in terms true class\n\n#### It lists both false positive and false negatives ","c6363e07":"# IMDB MOVIE REVIEWS\n#### dataset resources: https:\/\/www.kaggle.com\/lakshmi25npathi\/imdb-dataset-of-50k-movie-reviews\n#### A dataset containing two columns: reviews and sentiments (positive and negative) having 50k entries","b4867b47":"## MAKING OUR ML MODEL Using NAIVE BAYES AND LINEAR SVM (SVC) CLASSIFIER","93555bc3":"## Plotting the word cloud\n\nA word cloud contains collection of all possible words used in our dataset and represents them in pictorial form","76e8db06":"## Exploring our dataset","a7de6777":"## DATA CLEANING\n\nFor efficient data analysis, we only need those words which add value to our predictions. Unneccesary punctuation marks and stop words are to be removed.\nSome stopWords which are most commonly used are 'I','We','They','and' etc etc.\nMoreover, lemmatize and stemming are used to reduce a word to its root and stems. This is useful for tokenization.","ce11524e":"## FEATURE EXTRACTION\n\n### TOKENIZATION \/ COUNT VECTORIZER\n\n##### tokenization is a beautiful concept that helps to convert our textual data into some vectorized numeric form\n##### our count vectorizer is going to pick up unique words from our text and then find out the frequency of that particular word for each row and make a 2D vector accordingly.","89b2cae1":"# AND WE ARE DONE :)\n"}}