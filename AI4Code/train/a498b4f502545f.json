{"cell_type":{"56538b82":"code","8487ded2":"code","460bf136":"code","da6c41a2":"code","b1fa4c5c":"code","5f90e87d":"code","c0376b72":"code","62ccaee9":"code","793614c6":"code","f555eb16":"code","39e41e0b":"code","1721113b":"code","5156a7ba":"code","553e9a17":"code","2fecb89f":"code","566bf806":"code","b086b1ba":"code","1e79c9c9":"code","ebe9c309":"code","eb22e5fb":"code","6e6636e5":"code","c33aa874":"code","a81406fd":"code","8736a4c7":"code","5105a6ff":"code","9e1100da":"code","4bab7f54":"code","9421d266":"code","58082f72":"code","2a1986ce":"code","63042269":"code","8f303477":"code","f4fc6d9c":"code","2ac6cbb4":"code","21acf101":"code","5b1426e5":"markdown","5a21dbf3":"markdown","8d7da154":"markdown","cd417648":"markdown","90722339":"markdown","f95ecf09":"markdown","ef25ba12":"markdown","96382ecd":"markdown","d05af871":"markdown"},"source":{"56538b82":"# Importing some tools\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8487ded2":"df_train = pd.read_csv('..\/input\/job-salary-prediction\/Train_rev1.zip', compression='zip', header=0, sep=',', quotechar='\"')\ndf_train.head()","460bf136":"df_train.describe()","da6c41a2":"df_train.info()","b1fa4c5c":"# Check missing values\ndf_train.isna().sum()","5f90e87d":"# Check for string label \nfor label,content in df_train.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","c0376b72":"# Check for numerical label\nfor label,content in df_train.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","62ccaee9":"# This will turn all of the string value into category values\nfor label, content in df_train.items():\n    if pd.api.types.is_string_dtype(content):\n        df_train[label] = content.astype(\"category\").cat.as_ordered()\n","793614c6":"# Filling missing values\nfor label,content in df_train.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to indicate whether sample had missing value\n        df_train[label+\"is_missing\"]=pd.isnull(content)\n        # Turn categories into numbers and add+1\n        df_train[label] = pd.Categorical(content).codes+1","f555eb16":"df_train.isna().sum()","39e41e0b":"ms = df_train[\"SalaryNormalized\"][:10].plot.barh(figsize=(15,10))","1721113b":"df_train[\"SalaryNormalized\"].hist()","5156a7ba":"# For more security,copy the train set\ndf_tmp = df_train.copy()","553e9a17":"df_tmp.head()","2fecb89f":"# Split the data into X & y\nX = df_tmp.drop(\"SalaryNormalized\",axis=1)\ny = df_tmp[\"SalaryNormalized\"]","566bf806":"# # Let's build a machine learning model \nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\nmodel = RandomForestRegressor(n_jobs=-1)\nmodel.fit(X_train,y_train)","b086b1ba":"# Evaluate model using mean absolute error\nfrom sklearn.metrics import mean_absolute_error\ny_preds_0 = model.predict(X_test)\nmae_rf = mean_absolute_error(y_test,y_preds_0)\nmae_rf","1e79c9c9":"from sklearn.model_selection import RandomizedSearchCV\nnp.random.seed(42)\ngrid = {\n    \"n_estimators\":np.arange(10,100,10),\n    \"max_depth\":[None,3,5,10],\n    \"min_samples_split\":np.arange(2,20,2),\n    \"min_samples_leaf\":np.arange(1,20,2),\n    \"max_features\": [0.5,1,\"sqrt\",\"auto\"],\n    \"max_samples\":[10000,12000,15000,20000]\n}\nrs_model = RandomizedSearchCV(\nRandomForestRegressor(n_jobs=-1,\n                     random_state=42),\n                    param_distributions = grid,\n                     n_iter=5,\n                    cv=5,\n                    verbose=True)\nrs_model.fit(X_train,y_train)","ebe9c309":"rs_model.best_params_","eb22e5fb":"# Choose the best performance\ny_preds_rs = rs_model.predict(X_test)\nmae_hyp = mean_absolute_error(y_test,y_preds_rs)\nmae_hyp,mae_rf","6e6636e5":"# Importing test data\ndf_test = pd.read_csv('..\/input\/job-salary-prediction\/Test_rev1.zip', compression='zip', header=0, sep=',', quotechar='\"')","c33aa874":"# Check for missing values\ndf_test.isna().sum()","a81406fd":"df_test.head()","8736a4c7":"# Check for string label\nfor label,content in df_test.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","5105a6ff":"# Check for numerical label\nfor label,content in df_test.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","9e1100da":"# This will turn all of the string value into category values\nfor label, content in df_test.items():\n    if pd.api.types.is_string_dtype(content):\n        df_test[label] = content.astype(\"category\").cat.as_ordered()\n# Filling missing values\nfor label,content in df_test.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to indicate whether sample had missing value\n        df_test[label+\"is_missing\"]=pd.isnull(content)\n        # Turn categories into numbers and add+1\n        df_test[label] = pd.Categorical(content).codes+1\nX_test.shape,y_test.shape        ","4bab7f54":"# Reshape X_train & df_test\nset(X_train.columns)-set(df_test.columns)","9421d266":"df_test[\"SalaryRaw\"] = False\ndf_test[\"SalaryRawis_missing\"] = False","58082f72":"X_train.shape,df_test.shape","2a1986ce":"# Make predictions\ny_preds = model.predict(df_test)","63042269":"# Format predictions into the same format Kaggle is after\ndf_preds = pd.DataFrame()\ndf_preds[\"Id\"] = df_test[\"Id\"]\ndf_preds[\"SalaryNormalized\"] = y_preds\ndf_preds.head()\ndf_preds.to_csv(\".\/\/Submission.csv\",index=False)","8f303477":"# Find feature importance of our best model\nmodel.feature_importances_","f4fc6d9c":"# Helper function for plotting feature importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                        \"feature_importances\": importances})\n          .sort_values(\"feature_importances\", ascending=False)\n          .reset_index(drop=True))\n    \n    # Plot the dataframe\n    fig, ax = plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature importance\")\n    ax.invert_yaxis()","2ac6cbb4":"plot_features(X_train.columns,model.feature_importances_)","21acf101":"df_tmp[\"SalaryRaw\"].value_counts()","5b1426e5":"### Feature Importance\n\nFeature importance seeks to figure out which different attributes of the data were most importance when it comes to predicting the **target variable** (SalaryNormalized).","5a21dbf3":"# Data Exploration","8d7da154":"# Hyerparameter tuning with RandomizedSearchCV","cd417648":"# Modeling","90722339":"# **Job Salary Prediction**\nThis notebook looks into using various Python-based machine learning and data science libraries in an attempt to build a machine learning model capable of predicting Job Salary.\n\nWe're going to take the following approach:\n1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n5. Modelling\n6. Experimentation\n\n# 1. Problem Definition\n\nSuccessful models will incorporate some analysis of the impact of including different keywords or phrases, as well as making use of the structured data fields like location, hours or company.  Some of the structured data shown (such as category) is 'inferred' by Adzuna's own processes, based on where an ad came from or its contents, and may not be \"correct\" but is representative of the real data.\n\nYou will be provided with a training data set on which to build your model, which will include all variables including salary.  A second data set will be used to provide feedback on the public leaderboard.  After approximately 6 weeks, Kaggle will release a final data set that does not include the salary field to participants, who will then be required to submit their salary predictions against each job for evaluation.\n# 2. Data\n\nThe main dataset consists of a large number of rows representing individual job ads, and a series of fields about each job ad\n\n# 3.Evaluation\n\nOur evaluation data set is simply a random subset of ads for which we know the salary, that were not included in the training and public testing datasets.\n\nThe evaluation metric for this competition is Mean Absolute Error\n\nSample submission files can be downloaded from the data page. Submission files should be formatted as follows:\n\nHave a header: \"Id,SalaryNormalized\"\nContain two columns\nId: Id for the ads in the validation set in sorted order\nSalaryNormalized: Your predicted salary for the job ad\n# 4. Features\n\nThese fields are as follows:\n\nId - A unique identifier for each job ad\n\nTitle - A freetext field supplied to us by the job advertiser as the Title of the job ad.  Normally this is a summary of the job title or role.\n\nFullDescription - The full text of the job ad as provided by the job advertiser.  Where you see ***s, we have stripped values from the description in order to ensure that no salary information appears within the descriptions.  There may be some collateral damage here where we have also removed other numerics.\n\nLocationRaw - The freetext location as provided by the job advertiser.\n\nLocationNormalized - Adzuna's normalised location from within our own location tree, interpreted by us based on the raw location.  Our normaliser is not perfect!\n\nContractType - full_time or part_time, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n\nContractTime - permanent or contract, interpreted by Adzuna from description or a specific additional field we received from the advertiser.\n\nCompany - the name of the employer as supplied to us by the job advertiser.\n\nCategory - which of 30 standard job categories this ad fits into, inferred in a very messy way based on the source the ad came from.  We know there is a lot of noise and error in this field.\n\nSalaryRaw - the freetext salary field we received in the job advert from the advertiser.\n\nSalaryNormalised - the annualised salary interpreted by Adzuna from the raw salary.  Note that this is always a single value based on the midpoint of any range found in the raw salary.  This is the value we are trying to predict.\n\nSourceName - the name of the website or advertiser from whom we received the job advert. \n\nAll of the data is real, live data used in job ads so is clearly subject to lots of real world noise, including but not limited to: ads that are not UK based, salaries that are incorrectly stated, fields that are incorrectly normalised and duplicate adverts.\nLocation Tree\nThis is a supplemental data set that describes the hierarchical relationship between the different Normalised Locations shown in the job data.  It it is likely that there are meaningful relationships between the salaries of jobs in a similar geographical area, for example average salaries in London and the South East are higher than in the rest of the UK.","f95ecf09":"# Make predictions","ef25ba12":"# Evaluation","96382ecd":"# Data Visualization","d05af871":"# Load data"}}