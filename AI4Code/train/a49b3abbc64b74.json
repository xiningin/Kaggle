{"cell_type":{"6a94b85c":"code","d35c0962":"code","eded2456":"code","5b2422f4":"code","e59509f6":"code","35b72a14":"code","0aa2025d":"code","b3a9193e":"code","1f67766f":"code","9c8be503":"code","61a34f96":"code","89c1e036":"code","d17b8e83":"code","e9f2b525":"code","d9720e47":"code","19bafac7":"code","f3aea4cb":"code","4da15eb9":"code","77ce56d4":"code","e7e38133":"code","f7ebd19e":"code","5e9beff5":"code","76b0cf70":"code","c920f1d4":"code","f990460c":"code","ee8cecba":"code","0435cb44":"code","3729280e":"code","f4ca3b30":"code","ac6f6158":"code","a257c985":"code","7b7ed5a3":"code","758ac110":"code","b0056c62":"code","a4a77097":"code","b13ab113":"code","988ed049":"code","e45713b7":"code","35729693":"code","d03a00c2":"code","06ea550f":"code","ab85ffc4":"code","ae574cf3":"code","2da53936":"code","59860374":"code","f3b81d25":"markdown","1b3186a2":"markdown","af3fbea6":"markdown","6b4cee66":"markdown","8ffa1389":"markdown","3a1fa8ac":"markdown","e8d7e21a":"markdown","628e8e80":"markdown","d1bc56f8":"markdown","fd7e72b0":"markdown","eca3b969":"markdown","c800ee3f":"markdown","78a24951":"markdown","fed4bbfb":"markdown","37e4e01e":"markdown","cf51855d":"markdown","1a1d49ed":"markdown","4b304d3b":"markdown","29dc53df":"markdown","01fbd70b":"markdown","17907485":"markdown","996fcf43":"markdown","305ee856":"markdown"},"source":{"6a94b85c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected = True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d35c0962":"data = pd.read_csv('\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ndata.head()","eded2456":"data.info()","5b2422f4":"numerical_features = [\"Spending Score (1-100)\", \"Annual Income (k$)\", 'Age']\ncategorical_features = ['Gender']","e59509f6":"def plot_num_cat(feature, target, figsize=None):\n    # Attrition vs Age Distribution\n    fig = plt.figure(figsize=(10,6))\n\n    for value in data[target].unique():\n        sns.kdeplot(data[data[target]==value][feature])\n\n    fig.legend(labels=data[target].unique())\n    plt.title('{} distribution based on {}'.format(feature, target))\n    plt.show()\n    \ndef plot_num_num(feature, target):\n    sns.regplot(x=feature, y=target, data=data, color='#244747')\n    plt.show()\n    \ndef plot_cat_cat(feature, target):\n    plot_data = data.groupby([feature, target])[feature].agg({'count'}).reset_index()\n\n    fig = px.sunburst(plot_data, path=[feature, target], values='count', #color_continuous_scale='gray', color=feature, \n                      title='Affect of {} on Customer {}'.format(feature, target), width = 600, height = 600)\n    \n    fig.update_layout(plot_bgcolor='white', title_font_family='Calibri Black', title_font_color='#221f1f', \n                      title_font_size=22, title_x=0.5)\n    fig.update_traces(textinfo = 'label + percent parent')\n    fig.show()","35b72a14":"# looking at gender distribution\nplt.figure(figsize=(5, 5))\npatches, texts, autotexts = plt.pie(data['Gender'].value_counts(), autopct='%1.2f%%', \n                                    labels=data.groupby('Gender').count().reset_index()['Gender'],\n                                    shadow=True, startangle=90, explode=(0.05, 0.05), colors=['#91b8bd', '#244747']);\nplt.setp(texts, size=15);\nplt.setp(autotexts, size=15, color='white');\nplt.text(-1.65, 1.3, 'Male & Female Distribution', fontfamily='serif', fontsize=17, fontweight='bold');\nplt.text(-1.65, 1.15, 'Females are slightly more than Males', fontfamily='serif', fontsize=12);\nplt.show()","0aa2025d":"# Looking at annual income distribution\nplt.figure(figsize=(10, 5));\nsns.displot(x=\"Annual Income (k$)\", data=data, kde=True, bins=20, color='#244747');\nplt.text(0, 28, 'Annual Income Distribution', fontfamily='serif', fontsize=17, fontweight='bold');","b3a9193e":"# looking at spending score distribution\nplt.figure(figsize=(10, 10))\nsns.displot(x=\"Spending Score (1-100)\", data=data, palette='husl', kde=True, bins=20, color='#244747');\nplt.text(-20, 23, 'Spending Score Distribution', fontfamily='serif', fontsize=17, fontweight='bold');\nplt.show()","1f67766f":"# looking at age distribution\nplt.figure(figsize=(10,5))\nsns.histplot(x=\"Age\", data=data, palette='husl', kde=True, bins=20, color='#244747');\nplt.text(15, 25, 'Age Distribution', fontfamily='serif', fontsize=17, fontweight='bold');","9c8be503":"for feature in numerical_features:\n    for target in categorical_features:\n        plot_num_cat(feature, target)","61a34f96":"plt.figtext(0.1, 1, \"How Numerical variables relate to each other\", fontfamily='serif', fontsize=14, fontweight='bold')\nfor feature, target in list(itertools.combinations(numerical_features, 2)):\n    plot_num_num(feature, target)\n","89c1e036":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nimport joblib","d17b8e83":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    le = LabelEncoder()\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    #print(feature)\n    \n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))","e9f2b525":"plt.figure(figsize=(5, 3))\nsns.heatmap(round(data[numerical_features].corr(method='spearman'), 2), \n            annot=True, mask=None, cmap='GnBu')\nplt.show()","d9720e47":"plt.figure(figsize=(5, 3))\nsns.heatmap(round(df[categorical_features+numerical_features].corr(method='spearman'), 2), annot=True,\n            mask=None, cmap='GnBu')\nplt.show()","19bafac7":"from statsmodels.stats.outliers_influence import variance_inflation_factor","f3aea4cb":"# Calculating VIF\nvif = pd.DataFrame()\ntemp = df.dropna()\nvif[\"variables\"] = [feature for feature in categorical_features+numerical_features if feature not in []]\nvif[\"VIF\"] = [variance_inflation_factor(temp[vif['variables']].values, i) for i in range(len(vif[\"variables\"]))]\nprint(vif)","4da15eb9":"NumericData = data[[feature for feature in numerical_features if feature not in []]]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(10,5))\nplt.figtext(0.1, 1, \"Boxplots for Numerical variables\", fontfamily='serif', fontsize=17, fontweight='bold')\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt, palette=['#244247', '#91b8bd', 'gray'])\nbp.set_xticklabels(bp.get_xticklabels(), rotation=0)\nplt.show()","77ce56d4":"# Percentage of outliers present in each variable\noutlier_percentage = {}\nfor feature in numerical_features:\n    tempData = data.sort_values(by=feature)[feature]\n    Q1, Q3 = tempData.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    Lower_range = Q1 - (1.5 * IQR)\n    Upper_range = Q3 + (1.5 * IQR)\n    outlier_percentage[feature] = round((((tempData<(Q1 - 1.5 * IQR)) | (tempData>(Q3 + 1.5 * IQR))).sum()\/tempData.shape[0])*100,2)\noutlier_percentage","e7e38133":"df = data.copy()\npath = '\/kaggle\/working'\nfor i, feature in enumerate(categorical_features):\n    \n    le = LabelEncoder()\n    ohe = OneHotEncoder(sparse=False)\n\n    # create directory to save label encoding models\n    if not os.path.exists(os.path.join(path, \"TextEncoding\")):\n        os.makedirs(os.path.join(path, \"TextEncoding\"))\n\n    # perform label encoding\n    le.fit(df[feature])\n    # save the encoder\n    joblib.dump(le, open(os.path.join(path, \"TextEncoding\/le_{}.sav\".format(feature)), 'wb'))\n    \n    # transfrom training data\n    df[feature] = le.transform(df[feature])\n\n    # get classes & remove first column to elude from dummy variable trap\n    columns = list(map(lambda x: feature+' '+str(x), list(le.classes_)))[1:]\n    \n    # save classes\n    joblib.dump(columns, \n                open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'wb'))\n    # load classes\n    columns = joblib.load(\n        open(os.path.join(path, \"TextEncoding\/le_{}_classes.sav\".format(feature)), 'rb'))\n\n    if len(le.classes_)>2:\n        # perform one hot encoding\n        ohe.fit(df[[feature]])\n        # save the encoder\n        joblib.dump(ohe, open(os.path.join(path, \"TextEncoding\/ohe_{}.sav\".format(feature)), 'wb'))\n\n        # transfrom training data\n        # removing first column of encoded data to elude from dummy variable trap\n        tempData = ohe.transform(df[[feature]])[:, 1:]\n\n        # create Dataframe with columns as classes\n        tempData = pd.DataFrame(tempData, columns=columns)\n    else:\n        tempData = df[[feature]]\n    \n    # create dataframe with all the label encoded categorical features along with hot encoding\n    if i==0:\n        encodedData = pd.DataFrame(data=tempData, columns=tempData.columns.values.tolist())\n    else:\n        encodedData = pd.concat([encodedData, tempData], axis=1)","f7ebd19e":"# merge numerical features and categorical encoded features\ndf = df[numerical_features]\ndf = pd.concat([df, encodedData], axis=1)\ndf.info()","5e9beff5":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, rand_score\nfrom sklearn.mixture import GaussianMixture\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.neighbors import NearestNeighbors","76b0cf70":"# Rescaling to [0,1]\n#scaler = StandardScaler()\n#scaler.fit(train_data[feature_cols])\n#train_data[feature_cols] = scaler.transform(train_data[feature_cols])","c920f1d4":"feature_cols = [feature for feature in df.columns if feature not in(['Gender', 'Age'])]\ntrain_data = df.copy()[feature_cols]\nprint('features used- ', feature_cols)","f990460c":"# Using ELBOW Method to figure out number of clusters\ninertia=[]\nsilhouetteScore = []\nfor i in range(2,11):\n    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n    kmeans.fit(train_data)\n    inertia.append(kmeans.inertia_)\n    silhouetteScore.append(silhouette_score(train_data, kmeans.predict(train_data)))\n\nfig, ax1 = plt.subplots(figsize=(8, 5))\nfig.text(0.1, 1, 'Spending Score (1-100) and Annual Income (k$)', fontfamily='serif', fontsize=12, fontweight='bold')\nfig.text(0.1, 0.95, 'We want to select a point where Inertia is low & Silhouette Score is high, and the number of clusters is not overwhelming for the business.',\n         fontfamily='serif',fontsize=10)\nfig.text(1.4, 1, 'Inertia', fontweight=\"bold\", fontfamily='serif', fontsize=15, color='#244747')\nfig.text(1.51, 1, \"|\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='black')\nfig.text(1.53, 1, 'Silhouette Score', fontweight=\"bold\", fontfamily='serif', fontsize=15, color='#91b8bd')\n\nax1.plot(range(2,11), inertia, '-', color='#244747', linewidth=5)\nax1.plot(range(2,11), inertia, 'o', color='#91b8bd')\nax1.set_ylabel('Inertia')\n\nax2 = ax1.twinx()\nax2.plot(range(2,11), silhouetteScore, '-', color='#91b8bd', linewidth=5)\nax2.plot(range(2,11), silhouetteScore, 'o', color='#244747', alpha=0.8)\nax2.set_ylabel('Silhouette Score')\n\nplt.xlabel('Number of clusters')\nplt.show()","ee8cecba":"model = KMeans(n_clusters=5, init='k-means++', random_state=111, algorithm='elkan')\ny = model.fit_predict(train_data[feature_cols])","0435cb44":"# Visualizing all the clusters \nplt.figure(figsize=(10,5))\nsns.scatterplot(x=train_data[feature_cols[0]], y=train_data[feature_cols[1]], \n                hue=y, palette=sns.color_palette('hls', len(np.unique(y))), s=100)\n#sns.scatterplot(x=model.cluster_centers_[:, 0], y=model.cluster_centers_[:, 1], label='Centroids', s=150, color='orange')\nplt.title('Cluster of Customers'.format(feature_cols[0], feature_cols[1]), size=15, pad=10)\nplt.xlabel(feature_cols[0], size=12)\nplt.ylabel(feature_cols[1], size=12)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","3729280e":"feature_cols = [feature for feature in df.columns if feature not in(['Gender'])]\ntrain_data = df.copy()[feature_cols]\nprint('features used- ', feature_cols)","f4ca3b30":"# Using ELBOW Method to figure out number of clusters\ninertia=[]\nsilhouetteScore = []\nfor i in range(2,11):\n    kmeans = KMeans(n_clusters= i, init='k-means++', random_state=0)\n    kmeans.fit(train_data)\n    inertia.append(kmeans.inertia_)\n    silhouetteScore.append(silhouette_score(train_data, kmeans.predict(train_data)))\n\nfig, ax1 = plt.subplots(figsize=(8, 5))\nfig.text(0.1, 1, 'Spending Score (1-100) and Annual Income (k$)', fontfamily='serif', fontsize=12, fontweight='bold')\nfig.text(0.1, 0.95, 'We want to select a point where Inertia is low & Silhouette Score is high, and the number of clusters is not overwhelming for the business.',\n         fontfamily='serif',fontsize=10)\nfig.text(1.4, 1, 'Inertia', fontweight=\"bold\", fontfamily='serif', fontsize=15, color='#244747')\nfig.text(1.51, 1, \"|\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='black')\nfig.text(1.53, 1, 'Silhouette Score', fontweight=\"bold\", fontfamily='serif', fontsize=15, color='#91b8bd')\n\nax1.plot(range(2,11), inertia, '-', color='#244747', linewidth=5)\nax1.plot(range(2,11), inertia, 'o', color='#91b8bd')\nax1.set_ylabel('Inertia')\n\nax2 = ax1.twinx()\nax2.plot(range(2,11), silhouetteScore, '-', color='#91b8bd', linewidth=5)\nax2.plot(range(2,11), silhouetteScore, 'o', color='#244747', alpha=0.8)\nax2.set_ylabel('Silhouette Score')\n\nplt.xlabel('Number of clusters')\nplt.show()","ac6f6158":"model = KMeans(n_clusters=6, init='k-means++', random_state=19, algorithm='elkan')\ny = model.fit_predict(train_data)","a257c985":"fig = px.scatter_3d(train_data, x=\"Annual Income (k$)\", y=\"Spending Score (1-100)\", z=\"Age\",\n                    color=y, opacity=0.8, size=y+1)\nfig.show()","7b7ed5a3":"train_data['cluster'] = y\ntrain_data.groupby(['cluster']).agg(['mean', 'median']).reset_index()","758ac110":"data['cluster'] = train_data['cluster']\nplot_cat_cat('cluster', 'Gender')","b0056c62":"for feature in ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']:\n    plot_num_cat(feature, 'cluster')","a4a77097":"feature_cols = [feature for feature in df.columns if feature not in(['Gender'])]\ntrain_data = df.copy()[feature_cols]\nprint('features used- ', feature_cols)","b13ab113":"# Number of clusters is determined using elbow method above\nmodel = GaussianMixture(n_components=6)\ny = model.fit_predict(train_data[feature_cols])","988ed049":"# Visualizing all the clusters \nplt.figure(figsize=(10,5))\nsns.scatterplot(x=train_data[feature_cols[0]], y=train_data[feature_cols[1]], \n                hue=y, palette=sns.color_palette('hls', 5), s=100)\nplt.title('Cluster of Customers'.format(feature_cols[0], feature_cols[1]), size=15, pad=10)\nplt.xlabel(feature_cols[0], size=12)\nplt.ylabel(feature_cols[1], size=12)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","e45713b7":"feature_cols = [feature for feature in df.columns if feature not in(['Gender', 'Age'])]\ntrain_data = df.copy()[feature_cols]\nprint('features used- ', feature_cols)","35729693":"plt.figure(figsize = (12, 5))\nplt.text(5, 465, 'Spending Score (1-100) and Annual Income (k$)', fontfamily='serif', fontsize=15, fontweight='bold')\nplt.text(5, 440, 'The no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.',\n         fontfamily='serif',fontsize=12)\ndendo = dendrogram(linkage(train_data[feature_cols], method = 'ward'))\nplt.plot([115]*2000, color='r')\nplt.plot([240]*2000, color='r')\nplt.text(5, -50, 'Here, we can have either 5 clusters or 3 clusters',\n         fontfamily='serif',fontsize=12)\nplt.show()","d03a00c2":"model = AgglomerativeClustering(n_clusters = 5, affinity='euclidean', linkage='ward')\ny = model.fit_predict(train_data[feature_cols])","06ea550f":"# Visualizing all the clusters \nplt.figure(figsize=(10,5))\nsns.scatterplot(x=train_data[feature_cols[0]], y=train_data[feature_cols[1]], \n                hue=y, palette=sns.color_palette('hls', 5), s=100)\nplt.title('Cluster of Customers'.format(feature_cols[0], feature_cols[1]), size=15, pad=10)\nplt.xlabel(feature_cols[0], size=12)\nplt.ylabel(feature_cols[1], size=12)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","ab85ffc4":"feature_cols = [feature for feature in df.columns if feature not in(['Gender', 'Age'])]\ntrain_data = df.copy()[feature_cols]\nprint('features used- ', feature_cols)","ae574cf3":"from sklearn.neighbors import NearestNeighbors\n# finding nearest points distance for every row in data\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(train_data)\ndistances, indices = nbrs.kneighbors(train_data)\n\n# Plotting K-distance Graph\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.figure(figsize=(10,5))\nplt.text(-10, 17, 'TK-distance Graph', fontfamily='serif', fontsize=15, fontweight='bold')\nplt.text(-10, 16, 'The optimum value of epsilon is at the point of maximum curvature in the K-Distance Graph, which is 6 in this case.',\n        fontfamily='serif', fontsize=12)\nplt.plot(distances)\nplt.xlabel('Data Points sorted by distance', fontsize=14)\nplt.ylabel('Epsilon', fontsize=14)\nplt.show()","2da53936":"model = DBSCAN(eps=6, min_samples=3)\ny = model.fit_predict(train_data)","59860374":"# Visualizing all the clusters \nplt.figure(figsize=(10,5))\nsns.scatterplot(x=train_data[feature_cols[0]], y=train_data[feature_cols[1]], \n                hue=y, palette=sns.color_palette('hls', len(np.unique(y))), s=100)\nplt.title('Cluster of Customers'.format(feature_cols[0], feature_cols[1]), size=15, pad=10)\nplt.xlabel(feature_cols[0], size=12)\nplt.ylabel(feature_cols[1], size=12)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","f3b81d25":"# CORRELATION","1b3186a2":"Elbow is present at 5, Sihouette Score is max at 5 as well and hence n_clusters=5","af3fbea6":"# Looking at Outliers","6b4cee66":"### Label encoding categorical features for correlation","8ffa1389":"All features can be included","3a1fa8ac":"# Traning ML Model","e8d7e21a":"# Model 2: Gaussian Mixture","628e8e80":"Elbow is present at 6, Sihouette Score is max at 6 as well and hence n_clusters=6","d1bc56f8":"# Analyzing features using VIF","fd7e72b0":"**Observations-**\n* Spending Score- Females spend more than Males and most of it lies between 40 - 60\n* Annual Income- Males have slightly higher income, most of the people have income between 40 - 90\n* Age- Females are usually older","eca3b969":"* Cluster 0 - Moderate spending score, Moderate income, young age - Valuable\n* Cluster 1 - Low spending score, High income, moderate age - Targets\n* Cluster 2 - High spending score, High income, young age - Most Valuable\n* Cluster 3 - Moderate spending score, Moderate income, old age - Less Valuable\n* Cluster 4 - High spending score, low income, young age - More Valuable\n* Cluster 5 - Low spending score, low income, moderate age - Least Valuable","c800ee3f":"# Model 1: KMeans","78a24951":"Hence no null values","fed4bbfb":"# Model 3: Hierarchical Model","37e4e01e":"# Handling Categorical Features (Label and One Hot Encoding)","cf51855d":"## Interpreting model","1a1d49ed":"# Model Interpretation \n* Cluster 0 -> Earning low but Spending is high\n* Cluster 1 -> average in terms of Earning and Spending \n* Cluster 2 -> Earning high and also Spending high\n* Cluster 3 -> Earning high but Spending less\n* Cluster 4 -> Earning less and Spending less","4b304d3b":"**Observations-**\n* People with higher age usually have low Spending Score","29dc53df":"### Bivariate Analysis Correlation plot for numerical features","01fbd70b":"### Bivariate Analysis Correlation plot with the Categorical variables","17907485":"# EDA","996fcf43":"# Model 4: DBSCAN","305ee856":"**Observations-**\n* Cluster 0 - Valuable\n    * Age- 10 to 40\n    * Spending Score- 30 to 60\n    * Annual Income- 30 to 70\n* Cluster 1 - Targets\n    * Age- 30 to 60\n    * Spending Score- 0 to 30\n    * Annual Income- 60 to 110\n* Cluster 2 - Most Valuable\n    * Age- 20 to 45\n    * Spending Score- 60 to 100\n    * Annual Income- 60 to 110\n* Cluster 3 - Less Valuable\n    * Age- 40 to 70\n    * Spending Score- 30 to 60\n    * Annual Income- 30 to 70\n* Cluster 4 - More Valuable\n    * Age- 10 to 40\n    * Spending Score- 60 to 100\n    * Annual Income- 10 to 40\n* Cluster 5 - Least Valuable\n    * Age- 20 to 65\n    * Spending Score- 0 to 40\n    * Annual Income- 10 to 40"}}