{"cell_type":{"4bdeebca":"code","157d7a67":"code","bc6b81e9":"code","fea61f31":"code","981ffc77":"code","0aace86d":"code","502c7917":"code","7098056d":"code","c790e947":"code","e608fcbc":"code","80c420ae":"code","1b4e690e":"code","856af557":"code","69859eaa":"code","87c4cc69":"code","131e40f6":"code","dafa6486":"code","200504b3":"code","0460e90c":"code","6aa62195":"code","87397d51":"code","9c4c730f":"code","2db0d5de":"code","570ced4e":"code","841fed1b":"code","31e44e72":"code","dee1caeb":"code","99e3ca44":"code","ef48ee38":"code","66473f78":"code","0fd31f18":"code","afd90778":"code","dc16b765":"code","c205b901":"code","66a3f039":"code","7182f74e":"code","674ae5d6":"code","4f5fe9cf":"code","a8b397ae":"code","6725fd9f":"code","70e6e7fe":"code","cac6862d":"code","3d62b6e4":"code","810c275e":"code","a5d8bf11":"code","ac1a9e7f":"code","0eae9100":"code","c76e76ad":"code","b5062535":"code","6d126869":"code","3f3aef5e":"code","ebf2f7ae":"code","bf383ef5":"code","a1779e0c":"code","f60baf59":"code","2ed758e0":"code","f762aef1":"code","9271762f":"code","ef435bca":"code","511a22d6":"code","0565b969":"code","85e3a94d":"code","8c95b73f":"code","72c57542":"code","f495a946":"code","6ed2065e":"code","31197d75":"code","8acb10dd":"code","8d15637a":"code","e672b4b7":"code","fc670d89":"code","126f60ac":"code","c4d43dad":"code","c99e6f4c":"code","fcf11c5b":"code","e6a40435":"code","d33a3d63":"code","adf53ada":"code","b57010f6":"code","9c08a743":"code","33023152":"code","07160152":"code","ab18d638":"code","8fdea2be":"code","2644ee6b":"code","a9a381bf":"code","0a520aaa":"code","de2a3094":"code","259963f9":"code","9806078e":"code","29de4f7d":"code","715edf3c":"code","f9cbfd40":"code","37c03411":"code","d1f64ba0":"code","6f371c20":"code","0df4e7f0":"code","bb002c9e":"code","a398f546":"code","2ca0a9e3":"code","09d5668e":"code","65cf183f":"code","7bc27ae2":"code","70481970":"code","e7bfe3c9":"code","269be536":"code","909144c2":"code","55f1326e":"code","ec161ec7":"code","c4050a7b":"code","c24a95a8":"code","dd75620c":"code","909624b7":"code","e0166d66":"code","b66ebdf7":"code","f8f6c331":"code","b372a091":"code","9df5421b":"code","05622961":"code","044be84b":"code","7670fa4c":"code","e74cb3d4":"code","c212d0ed":"markdown","e652bf4d":"markdown","e44d7c5c":"markdown","c1c874d0":"markdown","ff106825":"markdown","fb4cffe4":"markdown","c8ab7592":"markdown","05c37267":"markdown","130a6417":"markdown","6129083d":"markdown","aa4f4da6":"markdown","b96eba79":"markdown","341283f2":"markdown","146ff948":"markdown"},"source":{"4bdeebca":"#importing the required packages\/libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nimport warnings # supress warnings\nwarnings.filterwarnings('ignore')","157d7a67":"#importing the data set\nTelecom_dataset=pd.read_csv(\"\/kaggle\/input\/telecom_churn_case_study.csv\")","bc6b81e9":"Telecom_dataset.head()","fea61f31":"print(Telecom_dataset.shape)\n","981ffc77":"Telecom_dataset.describe()","0aace86d":"Telecom_dataset.info()","502c7917":"#Checking for null values\nTelecom_dataset.isnull().sum()","7098056d":"#Removing unwanted columns\nTelecom_DS1 = Telecom_dataset.drop(['mobile_number','circle_id','loc_og_t2o_mou','std_og_t2o_mou','loc_ic_t2o_mou'],1)","c790e947":"#Checking the data types of all the columns\nTelecom_DS1.columns.to_series().groupby(Telecom_DS1.dtypes).groups","e608fcbc":"#Removing the Date Time columns as not needed\nTelecom_DS1 = Telecom_DS1.drop(['last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8',\n        'last_date_of_month_9', 'date_of_last_rech_6', 'date_of_last_rech_7',\n        'date_of_last_rech_8', 'date_of_last_rech_9',\n        'date_of_last_rech_data_6', 'date_of_last_rech_data_7',\n        'date_of_last_rech_data_8', 'date_of_last_rech_data_9'],1)","80c420ae":"Telecom_DS1.columns.to_series().groupby(Telecom_DS1.dtypes).groups","1b4e690e":"null_columns=list(Telecom_DS1.columns[round(100*(Telecom_DS1.isnull().sum()\/len(Telecom_DS1.index)), 2) > 40])\nnull_columns","856af557":"Telecom_DS1.shape","69859eaa":"#impiting null values with 0\nTelecom_DS1.fillna(0,inplace=True)","87c4cc69":"null_columns=list(Telecom_DS1.columns[round(100*(Telecom_DS1.isnull().sum()\/len(Telecom_DS1.index)), 2) > 40])\nnull_columns","131e40f6":"Telecom_DS1['tot_data_rech_6']=Telecom_DS1['total_rech_data_6'] * Telecom_DS1['av_rech_amt_data_6']\nTelecom_DS1['tot_data_rech_7']=Telecom_DS1['total_rech_data_7'] * Telecom_DS1['av_rech_amt_data_7']\nTelecom_DS1['tot_data_rech_8']=Telecom_DS1['total_rech_data_8'] * Telecom_DS1['av_rech_amt_data_8']\nTelecom_DS1['tot_data_rech_9']=Telecom_DS1['total_rech_data_9'] * Telecom_DS1['av_rech_amt_data_9']","dafa6486":"Telecom_DS1['tot_rech_amt_6']=Telecom_DS1['tot_data_rech_6'] + Telecom_DS1['total_rech_amt_6']\nTelecom_DS1['tot_rech_amt_7']=Telecom_DS1['tot_data_rech_7'] + Telecom_DS1['total_rech_amt_7']\nTelecom_DS1['tot_rech_amt_8']=Telecom_DS1['tot_data_rech_8'] + Telecom_DS1['total_rech_amt_8']\nTelecom_DS1['tot_rech_amt_9']=Telecom_DS1['tot_data_rech_9'] + Telecom_DS1['total_rech_amt_9']","200504b3":"Telecom_DS1.shape","0460e90c":"#Determining the average revenue of all customers\nTelecom_DS1['av_rech_amt']=((Telecom_DS1['tot_rech_amt_6'] + Telecom_DS1['tot_rech_amt_7'])\/2)","6aa62195":"#Determining the 70th percentile of average recharge amount\nTelecom_DS1['av_rech_amt'].quantile(0.7)","87397d51":"#Filtering High Value Customers\nTelecom_DS1=Telecom_DS1.loc[(Telecom_DS1['av_rech_amt'] > 478.0)]","9c4c730f":"Telecom_DS1.shape","2db0d5de":"def conditions(Telecom_DS1):\n    if (Telecom_DS1['total_ic_mou_9'] <=0 ) & (Telecom_DS1['total_og_mou_9'] <= 0) & (Telecom_DS1['vol_2g_mb_9'] <= 0) & (Telecom_DS1['vol_3g_mb_9']<=0):\n        return 1\n    else:\n        return 0\n    \nTelecom_DS1['Churn'] = Telecom_DS1.apply(conditions, axis=1)\n","570ced4e":"Telecom_DS1['Churn'].value_counts()","841fed1b":"# Removing attributes of Churn Phase to get final dataset for modelling\ncol_churn = [col for col in Telecom_DS1.columns if '_9' not in col]\nTelecom_DS2 = Telecom_DS1[col_churn]\nTelecom_DS2.shape","31e44e72":"Telecom_DS2.info()","dee1caeb":"Telecom_DS2.describe()","99e3ca44":"X=Telecom_DS2.drop(['Churn'],1)\nX.head()","ef48ee38":"y=Telecom_DS2['Churn']\ny.head()","66473f78":"# Split the dataset into 70% train and 30% test\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","0fd31f18":"scaler = MinMaxScaler()\ncol_list=list(X_train.columns)\nX_train[col_list] = scaler.fit_transform(X_train[col_list])\n\nX_train.head()","afd90778":"corr=Telecom_DS2.corr()","dc16b765":"plt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(Telecom_DS2.corr(),annot = True)\nplt.show()","c205b901":"upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.75\nto_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\nprint(len(to_drop))\nto_drop","66a3f039":"#Dropping the Highly correlated columns\nX_test = X_test.drop(X_test[to_drop], 1)\nX_train = X_train.drop(X_train[to_drop], 1)","7182f74e":"print(X_train.shape)\nprint(X_test.shape)","674ae5d6":"X_train.head()","4f5fe9cf":"from sklearn.decomposition import PCA \n\npca = PCA(svd_solver='randomized', random_state=42)\n\nX_train_pca = pca.fit_transform(X_train)  \n  \npca.components_","a8b397ae":"pca.explained_variance_ratio_","6725fd9f":"#Plotting the scree plot\n%matplotlib inline\nfig = plt.figure(figsize = (16,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\n#plt.xticks(range(min(valueX), max(valueX)+1))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","70e6e7fe":"from sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=15)","cac6862d":"X_train_pca = pca_final.fit_transform(X_train)  \n","3d62b6e4":"X_train.shape","810c275e":"X_train_pca.shape","a5d8bf11":"X_train_pca","ac1a9e7f":"pca_final.components_","0eae9100":"X_train_pca.shape","c76e76ad":"y_train.shape","b5062535":"import statsmodels.api as sm\nX_train_sm=sm.add_constant(X_train_pca)\nlogm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nlogm1=logm1.fit()\nlogm1.summary()","6d126869":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","3f3aef5e":"X_train_pca_df=pd.DataFrame(X_train_pca[1:,1:],    # values\n#             index=X_train_pca[1:,0],    # 1st column as index\n#             columns=X_train_pca[0,1:]  # 1st row as the column names\n                           ) ","ebf2f7ae":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_pca_df.columns\nvif['VIF'] = [variance_inflation_factor(X_train_pca_df.values, i) for i in range(X_train_pca_df.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","bf383ef5":"# Getting the predicted values on the train set\ny_train_pred = logm1.predict(X_train_sm)","a1779e0c":"y_train_pred[:10]","f60baf59":"y_train_pred_final = pd.DataFrame({'Churn':y_train.values, 'Churn_Prob':y_train_pred})\n#y_train_pred_final['CustID'] = y_train.index\ny_train_pred_final.tail(20)","2ed758e0":"y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","f762aef1":"y_train_pred_final['Churn'].value_counts()","9271762f":"y_train_pred_final['predicted'].value_counts()","ef435bca":"from sklearn import metrics\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\nprint(confusion)","511a22d6":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))","0565b969":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","85e3a94d":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","8c95b73f":"# Let us calculate specificity\nTN \/ float(TN+FP)","72c57542":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","f495a946":"# positive predictive value \nprint (TP \/ float(TP+FP))","6ed2065e":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","31197d75":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","8acb10dd":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )","8d15637a":"draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","e672b4b7":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","fc670d89":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","126f60ac":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","c4d43dad":"y_train_pred_final['final_predicted'] = y_train_pred_final.Churn_Prob.map( lambda x: 1 if x > 0.3 else 0)\n\ny_train_pred_final.head()","c99e6f4c":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.final_predicted)","fcf11c5b":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.final_predicted )\nconfusion2","e6a40435":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","d33a3d63":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","adf53ada":"# Let us calculate specificity\nTN \/ float(TN+FP)","b57010f6":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","9c08a743":"# Positive predictive value \nprint (TP \/ float(TP+FP))","33023152":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","07160152":"#Precision: TP \/ TP + FP\nconfusion[1,1]\/(confusion[0,1]+confusion[1,1])","ab18d638":"#Recall: TP \/ TP + FN\nconfusion[1,1]\/(confusion[1,0]+confusion[1,1])","8fdea2be":"from sklearn.metrics import precision_recall_curve","2644ee6b":"y_train_pred_final.Churn, y_train_pred_final.predicted","a9a381bf":"p, r, thresholds = precision_recall_curve(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)","0a520aaa":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","de2a3094":"X_test_pca = pca_final.fit_transform(X_test)  ","259963f9":"X_test_sm = sm.add_constant(X_test_pca)","9806078e":"y_test_pred = logm1.predict(X_test_sm)","29de4f7d":"y_test_pred_df = pd.DataFrame(y_test_pred)","715edf3c":"y_test_df = pd.DataFrame(y_test)","f9cbfd40":"# Removing index for both dataframes to append them side by side \ny_test_pred_df.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","37c03411":"y_pred_final = pd.concat([y_test_df, y_test_pred_df],axis=1)","d1f64ba0":"y_pred_final= y_pred_final.rename(columns={ 0 : 'Churn_Prob'})","6f371c20":"y_pred_final.tail()","0df4e7f0":"y_pred_final['final_predicted'] = y_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)","bb002c9e":"y_pred_final.head()","a398f546":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.Churn, y_pred_final.final_predicted)","2ca0a9e3":"confusion2 = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.final_predicted )\nconfusion2","09d5668e":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","65cf183f":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","7bc27ae2":"# Let us calculate specificity\nTN \/ float(TN+FP)","70481970":"# Importing decision tree classifier from sklearn library\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Fitting the decision tree with default hyperparameters, apart from\n# max_depth which is 5 so that we can plot and read the tree.\ndt_default = DecisionTreeClassifier(max_depth=5)\ndt_default.fit(X_train, y_train)","e7bfe3c9":"# Let's check the evaluation metrics of our default model\n\n# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Making predictions\ny_pred_default = dt_default.predict(X_test)\n\n# Printing classification report\nprint(classification_report(y_test, y_pred_default))","269be536":"# Printing confusion matrix and accuracy\nprint(confusion_matrix(y_test,y_pred_default))\nprint(accuracy_score(y_test,y_pred_default))","909144c2":"# Importing required packages for visualization\nfrom IPython.display import Image  \nfrom sklearn.externals.six import StringIO  \nfrom sklearn.tree import export_graphviz\nimport pydotplus, graphviz\n\n# Putting features\nfeatures = list(X_train.columns[1:])\nfeatures.append(list(y_train))","55f1326e":"# plotting tree with max_depth=3\ndot_data = StringIO()  \nexport_graphviz(dt_default, out_file=dot_data,\n                feature_names=features, filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","ec161ec7":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(1, 40)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","c4050a7b":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","c24a95a8":"# plotting accuracies with max_depth\nplt.figure()\n#plt.plot(scores[\"param_max_depth\"], \n#         scores[\"mean_train_score\"], \n#         label=\"training accuracy\")\nplt.plot(scores[\"param_max_depth\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"max_depth\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","dd75620c":"# so we will take 5 folds","909624b7":"# GridSearchCV to find optimal max_depth\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(5, 200, 20)}\n\n# instantiate the model\ndtree = DecisionTreeClassifier(criterion = \"gini\", \n                               random_state = 100)\n\n# fit tree on training data\ntree = GridSearchCV(dtree, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\ntree.fit(X_train, y_train)","e0166d66":"# scores of GridSearch CV\nscores = tree.cv_results_\npd.DataFrame(scores).head()","b66ebdf7":"# plotting accuracies with min_samples_leaf\nplt.figure()\n#plt.plot(scores[\"param_min_samples_leaf\"], \n#         scores[\"mean_train_score\"], \n#         label=\"training accuracy\")\nplt.plot(scores[\"param_min_samples_leaf\"], \n         scores[\"mean_test_score\"], \n         label=\"test accuracy\")\nplt.xlabel(\"min_samples_leaf\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()\n","f8f6c331":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n    'criterion': [\"entropy\", \"gini\"]\n}\n\nn_folds = 5\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\ngrid_search = GridSearchCV(estimator = dtree, param_grid = param_grid, \n                          cv = n_folds, verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)","b372a091":"# cv results\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results","9df5421b":"# printing the optimal accuracy score and hyperparameters\nprint(\"best accuracy\", grid_search.best_score_)\nprint(grid_search.best_estimator_)","05622961":"# tree with max_depth = 3\nclf_gini = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=3, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\nclf_gini.fit(X_train, y_train)\n\n# score\nprint(clf_gini.score(X_test,y_test))","044be84b":"# plotting tree with max_depth=3\ndot_data = StringIO()  \nexport_graphviz(clf_gini, out_file=dot_data,feature_names=features,filled=True,rounded=True)\n\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())","7670fa4c":"# classification metrics\nfrom sklearn.metrics import classification_report,confusion_matrix\ny_pred = clf_gini.predict(X_test)\nprint(classification_report(y_test, y_pred))","e74cb3d4":"# confusion matrix\nprint(confusion_matrix(y_test,y_pred))","c212d0ed":"# 1. Data Understanding and Preparation","e652bf4d":"## Making predictions on the test set\n","e44d7c5c":"## Tagging the churners","c1c874d0":"X_train_pca=np.transpose(X_train_pca)","ff106825":"##### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0","fb4cffe4":"### Model Building:\n\n#### Building a classification model with Logistic regression","c8ab7592":"### PCA for dimensionality reduction","05c37267":"### Checking the corelation and dropping the highly correlated columns","130a6417":"## Determining the high value customers:\nThose who have recharged with an amount more than or equal to X, where X is the 70th percentile of the average recharge amount in the first two months (the good phase).\nThe formula for the same is represented by:\nTotal recharge amount --> total recharge + total data recharge amount\nAlso, data recharge amount --> number of recharges * average recharge amount\n\ntot_data_rech= total_rech_data * av_rech_amt_data\n\ntot_rech_amt= total_rech_amt + tot_data_rech\n\nTo get this we need to handle the null values in columns av_rech_amt_data and total_rech_data","6129083d":"### Plotting the ROC Curve","aa4f4da6":"### Finding Optimal Cutoff Point","b96eba79":"#### From the curve above, 0.1 is the optimum point to take it as a cutoff probability.","341283f2":"More than 90% of the information is being explained by 15 components","146ff948":"## Churn Prediction for Telecom Industry:\n\nBuild models to predict churn. The predictive model that you\u2019re going to build will serve two purposes:\n\nIt will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase). By knowing this, the company can take action steps such as providing special plans, discounts on recharge etc.\n\nIt will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks."}}