{"cell_type":{"bcceaf38":"code","637592a8":"code","5777ac3d":"code","9cc3a0fe":"code","4400bdce":"code","6804a2e2":"code","d02e997b":"code","1c39ef10":"code","93d4e960":"code","cbd85584":"code","f63fd76f":"code","be8343c2":"code","0cd9d089":"code","6730138e":"code","add5ec33":"code","a8a77fa3":"code","4d1db42a":"code","6bb02563":"code","8dd0db1e":"code","5a33f6ea":"code","1bafeb27":"code","16e27c77":"code","8104c8e7":"code","5ec77a2d":"code","dc2da5bf":"code","c18bbf7c":"code","1640f34f":"code","0ba6bfea":"code","88ee1ed4":"code","e0b222df":"code","96e8416c":"code","6f773867":"code","c7e6b5f4":"markdown","242ac9bb":"markdown","cfac3417":"markdown","e992113d":"markdown","ca59c4e1":"markdown","2f0d8c64":"markdown","bc3d3c40":"markdown","f0302a5d":"markdown","b9a4a53b":"markdown","aa5223dc":"markdown","eaa4cd44":"markdown","ced61ddc":"markdown","d8e32315":"markdown","60bd2e8d":"markdown","5c04f46c":"markdown","ab0dd1a0":"markdown","3c416f60":"markdown","50162396":"markdown","699d8720":"markdown","d478db25":"markdown","da07f684":"markdown","95bd4e5a":"markdown","5e14e600":"markdown","12a6b039":"markdown","b14bd496":"markdown","475e8b7d":"markdown","c559878a":"markdown","7f331935":"markdown","737a0d85":"markdown"},"source":{"bcceaf38":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport pandas as pd\nimport numpy as np\nimport pyodbc\nfrom pandas import DataFrame\n\nimport string\nimport nltk\nfrom sklearn import re\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\nfrom nltk.stem import PorterStemmer\nfrom nltk import FreqDist \nfrom nltk.stem.porter import *\nimport re\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport csv\nimport datetime\nfrom datetime import datetime\nfrom datetime import date, timedelta\nfrom plotly import tools\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n#Model Selection and Validation\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\n","637592a8":"sample_submission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","5777ac3d":"train = train.astype({\"id\" : int, \"target\" : int, \"text\" : str})\ntrain.head()","9cc3a0fe":"df_combined = train.append(test, ignore_index=True, sort = False)\ndf_combined = df_combined.astype({\"text\" : str})\ndf_combined.head()","4400bdce":"df_combined = df_combined.loc[df_combined[\"keyword\"].notnull()]\ndf_combined = df_combined.loc[df_combined[\"text\"].notnull()]\ndf_combined.shape","6804a2e2":"sns.barplot(y=train['keyword'].value_counts()[:30].index,x=train['keyword'].value_counts()[:30],orient='h')\nplt.figure(figsize=(26, 18))","d02e997b":"# REMOVE @User\n\ndef remove_pattern(input_text, pattern):\n    r = re.findall(pattern, input_text)\n    for i in r:\n        input_text = re.sub(i, '', input_text)\n        \n    return input_text  ","1c39ef10":"df_combined['tidy_tweet'] = np.vectorize(remove_pattern)(df_combined['text'], \"@[\\w]*\")\ndf_combined['tidy_tweet'] = df_combined['tidy_tweet'].str.replace('[^\\w\\d#\\s]',' ')\ndf_combined['tidy_tweet'] = df_combined['tidy_tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf_combined.tail()","93d4e960":"#REMOVE Stop words\n\nstop = stopwords.words('english')\ndf_combined[\"tidy_tweet\"] = df_combined[\"tidy_tweet\"].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ndf_combined['tidy_tweet'] = df_combined['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\ndf_combined.head()","cbd85584":"df_train_disaster = df_combined.loc[df_combined[\"target\"] == 1.0]\ndf_train_nondisaster = df_combined.loc[df_combined[\"target\"] == 0.0]\ndf_test = df_combined.loc[df_combined[\"target\"].isnull()]\ndf_train_disaster.tail()","f63fd76f":"seperated_words_dis = df_train_disaster['tidy_tweet'].apply(lambda x: x.split())\nseperated_words_dis","be8343c2":"# from nltk.corpus import words\n# sample = []\n# output = []\n# word = words.words()\n# x = np.array(word)\n# word = np.unique(x)\n\n# for text in seperated_words_dis:\n#     for j in text:\n#         for k in word:\n#             if ([j]) == ([k]):\n#                 sample.append(j)\n#     output.append(sample)\n#     sample = []\n# output\n\n# from nltk.tokenize.treebank import TreebankWordDetokenizer\n# output1 = []\n# for i in output:\n#     output1 = [' '.join(x) for x in output]\n# output1\n\n# disaster_tweet = pd.DataFrame(output1)\n# disaster_tweet = disaster_tweet.rename(columns = {0 : \"Tweet_tweeked\"})\n# df_train_disaster = df_train_disaster.reset_index()\n# df_train_disaster = df_train_disaster.astype({\"id\" : int, \"target\" : int})\n# df_train_disaster = df_train_disaster.merge(disaster_tweet, how = 'inner', left_index = True, right_index = True)\n# df_train_disaster = df_train_disaster.filter([\"id\", \"keyword\", \"location\", \"text\", \"target\", \"Tweet_tweeked\"])\n# df_train_disaster.head()","0cd9d089":"train_disaster = pd.read_csv(\"..\/input\/twitter-competition\/train_disaster.csv\")\ntrain_disaster = train_disaster.filter([\"id\", \"keyword\", \"location\", \"text\", \"target\", \"Tweet_tweeked\"])\n\ntrain_nondisaster = pd.read_csv(\"..\/input\/twitter-competition\/train_nondisaster.csv\")\ntrain_nondisaster = train_nondisaster.filter([\"id\", \"keyword\", \"location\", \"text\", \"target\", \"Tweet_tweeked\"])\n\ntweeked_test = pd.read_csv(\"..\/input\/twitter-competition\/tweeked_test.csv\")\ntweeked_test = tweeked_test.filter([\"id\", \"keyword\", \"location\", \"text\", \"Tweet_tweeked\"])","6730138e":"train_disaster = train_disaster.filter([\"id\", \"keyword\", \"location\", \"text\", \"target\", \"Tweet_tweeked\"])\ntrain_disaster = train_disaster.astype({\"id\": int, \"target\": int})\ntrain_disaster = train_disaster.loc[train_disaster[\"Tweet_tweeked\"] != \"none\"]\ntrain_disaster = train_disaster.loc[train_disaster[\"Tweet_tweeked\"].notnull()]\ntrain_disaster[\"target\"] = train_disaster[\"target\"].replace({1: \"real\", 0 : \"not_real\"})\ntrain_disaster.shape","add5ec33":"train_nondisaster = train_nondisaster.filter([\"id\", \"keyword\", \"location\", \"text\", \"target\", \"Tweet_tweeked\"])\ntrain_nondisaster = train_nondisaster.astype({\"id\": int, \"target\": int})\ntrain_nondisaster = train_nondisaster.loc[train_nondisaster[\"Tweet_tweeked\"] != \"none\"]\ntrain_nondisaster = train_nondisaster.loc[train_nondisaster[\"Tweet_tweeked\"].notnull()]\ntrain_nondisaster[\"target\"] = train_nondisaster[\"target\"].replace({1: \"real\", 0 : \"not_real\"})\ntrain_nondisaster.shape","a8a77fa3":"tweeked_test = tweeked_test.filter([\"id\", \"keyword\", \"location\", \"text\", \"Tweet_tweeked\"])\ntweeked_test = tweeked_test.astype({\"id\": int})\n# tweeked_test = tweeked_test.loc[tweeked_test[\"Tweet_tweeked\"] != \"none\"]\n# tweeked_test = tweeked_test.loc[tweeked_test[\"Tweet_tweeked\"].notnull()]\ntweeked_test.shape","4d1db42a":"#Train Disaster Data:\n\ncomment_words = ' '\n# iterate through the csv file \nfor val in train_disaster.Tweet_tweeked: \n    # typecaste each val to string \n    val = str(val)\n    # split the value \n    tokens = val.split()  \n    for words in tokens: \n        comment_words = comment_words + words + ' '\n\nwordcloud = WordCloud(width = 700, height = 700, background_color ='black', min_font_size = 8).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (7, 7), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","6bb02563":"ax = train_disaster['keyword'].value_counts().sort_values().plot(kind='barh',\n                                    figsize=(15,40),\n                                    title=\"Number for each Keyword\")\nax.set_xlabel(\"Keywords\")\nax.set_ylabel(\"Frequency\")","8dd0db1e":"#Train Non-Disaster Data:\n\ncomment_words = ' ' \n# iterate through the csv file \nfor val in train_nondisaster.Tweet_tweeked: \n    # typecaste each val to string \n    val = str(val)\n    # split the value \n    tokens = val.split()  \n    for words in tokens: \n        comment_words = comment_words + words + ' '\n\nwordcloud = WordCloud(width = 700, height = 700, background_color ='black', min_font_size = 8).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (7, 7), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","5a33f6ea":"ax = train_nondisaster['keyword'].value_counts().sort_values().plot(kind='barh',\n                                    figsize=(15,40),\n                                    title=\"Number for each Keyword\")\nax.set_xlabel(\"Keywords\")\nax.set_ylabel(\"Frequency\")","1bafeb27":"#Test Data:\n\ncomment_words = ' '\n# iterate through the csv file \nfor val in tweeked_test.Tweet_tweeked: \n    # typecaste each val to string \n    val = str(val)\n    # split the value \n    tokens = val.split()  \n    for words in tokens: \n        comment_words = comment_words + words + ' '\n\nwordcloud = WordCloud(width = 700, height = 700, background_color ='black', min_font_size = 8).generate(comment_words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (7, 7), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","16e27c77":"ax = tweeked_test['keyword'].value_counts().sort_values().plot(kind='barh',\n                                    figsize=(15,40),\n                                    title=\"Number for each Keyword\")\nax.set_xlabel(\"Keywords\")\nax.set_ylabel(\"Frequency\")","8104c8e7":"#Combine the Train Disaster and Non-Disaster Data:\ntrain = train_disaster.append(train_nondisaster, ignore_index = True)","5ec77a2d":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(train.Tweet_tweeked).toarray()\nlabels = train.target\nfeatures.shape","dc2da5bf":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\n\nX_train, X_test, y_train, y_test = train_test_split(train['Tweet_tweeked'], train['target'], random_state = 0)\n\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\n\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n\nclf = MultinomialNB().fit(X_train_tfidf, y_train)","c18bbf7c":"clf.predict(count_vect.transform(X_test))","1640f34f":"models = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0)]\n\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n    for fold_idx, accuracy in enumerate(accuracies):\n        entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\ncv_df","0ba6bfea":"import seaborn as sns\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","88ee1ed4":"cv_df.groupby('model_name').accuracy.mean()","e0b222df":"model = LinearSVC()\nX1_train, X1_test, y1_train, y1_test, indices_train, indices_test = train_test_split(features, labels, train.index, test_size=0.33, random_state=0)\nmodel.fit(X1_train, y1_train)\ny1_pred = model.predict(X1_test)\n\nfrom sklearn.metrics import confusion_matrix\n\nconf_mat = confusion_matrix(y1_test, y1_pred)\n\nfrom sklearn import metrics\nprint(metrics.classification_report(y1_test, y1_pred, target_names=train['target'].unique()))","96e8416c":"sample_submission","6f773867":"tweeked_test = tweeked_test.filter([\"id\", \"keyword\", \"location\", \"text\", \"Tweet_tweeked\"])\ny_pre=clf.predict(count_vect.transform(tweeked_test[\"Tweet_tweeked\"].values.astype('U')))\ny_pre = pd.DataFrame(y_pre)\nsub = tweeked_test.merge(y_pre, how = \"inner\", left_index = True, right_index = True)\nsub = sub.rename(columns = { 0 : 'Target'})\nsub[\"Target\"] = sub[\"Target\"].replace({\"real\" : 1, \"not_real\": 0})\nFinal_sub = sub.filter([\"id\", \"Target\"])\nprint(Final_sub)\nFinal_sub.to_csv('submission.csv',index=False)","c7e6b5f4":"Remove the stop words and words less than 2 as they would not make any much difference:","242ac9bb":"### Import the data from source","cfac3417":"![twitter-activism-1577394747095-videoSixteenByNineJumbo1600.jpg](attachment:twitter-activism-1577394747095-videoSixteenByNineJumbo1600.jpg)","e992113d":"### CLEANING THE DATA ","ca59c4e1":"Combining the train and test data for easier data pre-processing:","2f0d8c64":"### What's this Kernel:\n* Basic EDA\n* Preprocessing the data\n* Comparing with English dictionary and identifying the good English words in the tweets\n* Word Cloud\n* TF-IDF and Multinomial Naive Bayes\n* Comparing with other models\n* Submission","bc3d3c40":"#### Finally, The highest is achieved with MultinomialNB - Naive Bias with 0.6917 accuracy","f0302a5d":"***TEST DATA***","b9a4a53b":"*The same logic is used to train-nondisaster and test data set.*","aa5223dc":"#### REMOVE @User from the tweets:","eaa4cd44":"***TF-IFD Vectorizer***\n\nCalculate the Term Frequency, Inverse Document Frequency from the data points:","ced61ddc":"*Split the Train data comprising the disaster and non-disaster data into X_train, X_test, y_train, y_test performing Naive Bias*","d8e32315":"### Model Validation","60bd2e8d":"### WORD CLOUDS","5c04f46c":"### Submission","ab0dd1a0":"Preprocessing the input data:","3c416f60":"***DISASTER DATA***","50162396":"#### Remove punctuations and make the tweets lower case:","699d8720":"Multinomial Naive Bayes is a specialized version of Naive Bayes that is designed more for text documents. Whereas simple naive Bayes would model a document as the presence and absence of particular words, multinomial naive bayes explicitly models the word counts and adjusts the underlying calculations to deal with in.\n\nCombining probability distribution of P with fraction of documents belonging to each class.\nFor class j, word i at a word frequency of f:\n![1.gif](attachment:1.gif)\n\nIn order to avoid underflow, we will use the sum of logs:\n![2.gif](attachment:2.gif)\n![3.gif](attachment:3.gif)\n\nOne issue is that, if a word appears again, the probability of it appearing again goes up. In order to smooth this, we take the log of the frequency:\n![4.gif](attachment:4.gif)\n\nAlso, in order to take stop words into account, we will add a Inverse Document Frequency (IDF)weight on each word:\n![5.gif](attachment:5.gif)\n![6.gif](attachment:6.gif)\n\n","d478db25":"Using the below code to compare the words in the tweet with the english dictionary to identify the good words. (Since it takes so much time to process, have uploaded the csv files after execution)","da07f684":"![conceptual-tag-cloud-containing-purposely-260nw-357542537.webp](attachment:conceptual-tag-cloud-containing-purposely-260nw-357542537.webp)","95bd4e5a":"Benchmarking with the following four models:\n\n* Logistic Regression\n* (Multinomial) Naive Bayes\n* Linear Support Vector Machine\n* Random Forest","5e14e600":"### Miltinomial NB","12a6b039":"***NON DISASTER DATA***","b14bd496":"*Predict the results using the input X_test (Test data from the train data*","475e8b7d":"### Split the Train data into Disaster and Non-Disaster and then Test","c559878a":"#### Exploring the Keyword","7f331935":"### MODEL","737a0d85":"### Compare to English Dictionary"}}