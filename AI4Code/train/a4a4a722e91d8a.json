{"cell_type":{"16a53507":"code","4e5705e5":"code","8316179f":"code","7b15b912":"code","151acdba":"code","155eb25e":"code","d147a466":"code","cd16bd5a":"code","51ae1e6b":"code","9b87bc1e":"code","cfab82d4":"code","83387824":"code","1026fba0":"code","17d08f8d":"code","64c869c1":"code","bd62037f":"code","3598ffea":"code","7055fa84":"code","f87ffaa2":"code","0d24fa80":"code","0a932ab0":"code","056c3556":"code","6e366627":"code","876ab7c4":"code","c2be68db":"code","6fa1e875":"code","7f3d7a66":"code","4409dd56":"code","584574c6":"code","ffbbf24d":"code","aab547ae":"code","631258a9":"code","01fa4a30":"code","fae16b54":"code","1b7ee439":"code","18b09f59":"code","8a990a3d":"code","ee2d6352":"code","c3fed433":"code","42ac67ab":"code","8a9be328":"code","7dcf0221":"code","2ff5859a":"code","a5625f5b":"code","424da9d6":"code","5ad6c306":"code","4e35ba75":"code","958cae97":"code","0c6f7260":"code","cf92c80b":"code","46f87152":"markdown","d4efefe8":"markdown","9baed005":"markdown","886d3b7f":"markdown","d8337d3d":"markdown","89c19dbd":"markdown","d320e5f3":"markdown","20c7bf9e":"markdown","d143f170":"markdown","6206ed66":"markdown","811fcadf":"markdown","b9dc7174":"markdown","f854056c":"markdown","9017db16":"markdown","d9aec0c8":"markdown","2985747b":"markdown","139f59ef":"markdown","8517dfd5":"markdown","3e35a786":"markdown","e936dc01":"markdown","4ab1f46e":"markdown","483988dd":"markdown","c38e28a7":"markdown","6e6ab5dc":"markdown","46813b34":"markdown","eb6b1fec":"markdown","f5a111a4":"markdown","c06e2377":"markdown","0568694c":"markdown","2f6f8755":"markdown","b0784cda":"markdown","948b5021":"markdown","ab18f245":"markdown","b7c27dc3":"markdown","36fa929a":"markdown","69ea07a6":"markdown","74c9b8e4":"markdown","17e5bf70":"markdown","1b2b7a82":"markdown","96357f42":"markdown","e1fe12c8":"markdown","8f0ec0f2":"markdown"},"source":{"16a53507":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e5705e5":"import pandas as pd # \"default\" library to deal with data files\nimport seaborn as sns # \"default\" library to data viz\nimport matplotlib.pyplot as plt # we need this api to set up plots","8316179f":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","7b15b912":"train_data.head()","151acdba":"train_data.shape","155eb25e":"test_data.shape","d147a466":"train_data.describe()","cd16bd5a":"train_data['Name'].value_counts()","51ae1e6b":"train_data[\"PassengerId\"].value_counts()","9b87bc1e":"ticket_train_data = train_data[\"Ticket\"].value_counts()\nticket_train_data","cfab82d4":"# A ticket can be repeated how many times?\nset(ticket_train_data.to_list())\n# as we see, a ticket can be repeated 1 to 7 times!","83387824":"plt.title(\"Ticket Repetions Counting\")\nplt.xlabel(\"Repetitions\")\nplt.ylabel(\"Countings\")\nsns.barplot(x=ticket_train_data.value_counts().index, \n             y = ticket_train_data.value_counts(normalize=True))","1026fba0":"train_data = train_data.drop(columns=['PassengerId', 'Name']) # removing 'PassengerId' and 'Name' column\ntrain_data.columns                                            # checking if everything is fine.","17d08f8d":"for clmn_name in train_data.columns.to_list():\n    print(f'variable {clmn_name} assumes {len(train_data[clmn_name].unique().tolist())} values')","64c869c1":"train_data_survivors = train_data[train_data['Survived']==1]\ntrain_data_deceased = train_data[train_data['Survived']== 0]\ntrain_data_survivors['Survived'].unique()","bd62037f":"train_data_deceased['Survived'].unique()","3598ffea":"def eda_barplot(title, xlbl, ylbl, column, normlz = False):\n    plt.title(title)\n    plt.xlabel(xlbl)\n    plt.ylabel(ylbl)\n    sns.barplot(x= train_data[column].value_counts().index, \n                 y = train_data[column].value_counts(normalize=normlz))","7055fa84":"eda_barplot(\"survivors distribution\", \"survived?\", \"counting\", \"Survived\",True)","f87ffaa2":"sns.catplot(x=\"Pclass\", hue=\"Sex\", kind=\"count\", \n            col='Survived',data=train_data)","0d24fa80":"sns.catplot(x='Pclass',y='Age',hue='Sex',\n            col='Survived',data=train_data, kind='box')","0a932ab0":"sns.catplot(x='Pclass',y='Fare',hue='Sex',\n            col='Survived',data=train_data, kind='box')","056c3556":"# let's create a new column named \"Relatives\", which contains the sum of \"Parch\" and \"SibSp\"\nrelatives_column = train_data['Parch'] + train_data['SibSp'] \ntrain_data[\"Relatives\"] = relatives_column\ntrain_data[['Parch','SibSp','Relatives']]","6e366627":"sns.catplot(x=\"Relatives\", kind=\"count\", \n            col='Survived',data=train_data)","876ab7c4":"sns.catplot(x=\"Embarked\",col='Survived',\n            data=train_data, kind=\"count\")","c2be68db":"# importing stuff\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","6fa1e875":"# separating features and labels \ny = train_data[\"Survived\"]\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Embarked\",\"Relatives\"]\nX = train_data[features]","7f3d7a66":"# train \/ validation split\n\nX_train, X_validation, y_train, y_validation = train_test_split(X, y , test_size = 0.33, random_state = 42)","4409dd56":"ensemble_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nensemble_model.fit(X_train, y_train)","584574c6":"# let's check our training data\nX_train","ffbbf24d":"# one-hot encoding our features\nX_train = pd.get_dummies(X_train)\nX_train","aab547ae":"X_validation = pd.get_dummies(X_validation) ","631258a9":"ensemble_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nensemble_model.fit(X_train, y_train)","01fa4a30":"predictions = ensemble_model.predict(X_validation)","fae16b54":"y_validation","1b7ee439":"# including confusion matrix\nfrom sklearn.metrics import confusion_matrix","18b09f59":"confusion_matrix(y_validation, predictions)","8a990a3d":"ensemble_model_v2 = RandomForestClassifier(n_estimators=100, max_depth=5, criterion='entropy',random_state=1, verbose=1)\nensemble_model_v2.fit(X_train, y_train)\npredictions_v2 = ensemble_model_v2.predict(X_validation)","ee2d6352":"confusion_matrix(y_validation, predictions_v2)","c3fed433":"# importing Random Search CV\nfrom sklearn.model_selection import RandomizedSearchCV\nimport numpy as np","42ac67ab":"param_dist = dict(criterion=['gini','entropy'], \n                  max_depth=np.arange(5,15).tolist(),\n                  max_features= ['auto','sqrt','log2'])","8a9be328":"clf = RandomizedSearchCV(ensemble_model_v2, param_dist, random_state=0)\nsearch = clf.fit(X_train, y_train)","7dcf0221":"search.best_params_","2ff5859a":"predictions_v2 = ensemble_model_v2.predict(X_validation)\nconf_mat=confusion_matrix(y_validation, predictions_v2)\nconf_mat","a5625f5b":"# accuracy\n(conf_mat[0,0]+conf_mat[1,1])\/np.sum(conf_mat)","424da9d6":"# precision\nconf_mat[0,0]\/np.sum(conf_mat[0,:])","5ad6c306":"# recall\nconf_mat[0,0]\/np.sum(conf_mat[:,0])","4e35ba75":"test_data","958cae97":"# let's create a new column named \"Relatives\", which contains the sum of \"Parch\" and \"SibSp\"\nrelatives_column = test_data['Parch'] + test_data['SibSp'] \ntest_data[\"Relatives\"] = relatives_column","0c6f7260":"features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\",\"Embarked\",\"Relatives\"]\nX_test = pd.get_dummies(test_data[features])\nX_test","cf92c80b":"survived_test = ensemble_model_v2.predict(X_test)","46f87152":"Let's understand our results...\n\nTo understand what we got, we need the concepts of [precision and recall](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/precision-and-recall)\n\n* The diagional sum divided by the matrix sum is the accuracy (how many guesses were correct)\n* The top right element divided by the matrix top line sum gives us the precision rate (```What proportion of positive identifications was actually correct?```)\n* The top right element divided by the matrix leftmost column sum gives us the recall rate(```What proportion of actual positives was identified correctly?```)\n(all these details are computed below)\n\nIf our accuracy is greater than 0.5, we have a model that performs better than random guessing (classifying instances by flipping a coin, for example). But that's not enough. Let's check the other metrics.\n\nThe precision rate indicates how many samples labeled \"positive\" (passenger survived) were indeed positive. It indicates that our rescuing robot will not pick deceased passengers up.\n\nThe recall rate indicates how many positive samples were labeled correctly. It means the percentage of how many survivors **were not forgotten** by our rescuing robot.","d4efefe8":"Notice we are having a marginal improvement on our model. I did it by tweaking the ```criterion``` from 'gini' to 'entropy'. Of course I can tweak other parameters, like ```max_depth```, for example. How to tweak a lot of criterions? It would take a lot of time!\n\nI'll tell the computer to look for parameters for me. That's what's called a _parameter search_. I can look randomly for it, or in a grid-like manner. [Random Search is actually a better way](https:\/\/medium.com\/@senapati.dipak97\/grid-search-vs-random-search-d34c92946318), so I'll use it.\n\nAgain, [Sklearn has its implementation of such method](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html).","9baed005":"### What's a Random Forest?","886d3b7f":"Above we have the very first cell of this notebook. It is there by default, so I'm not changing it. Below we have important imports.","d8337d3d":"How many samples and variables do we have?","89c19dbd":"Looking at the nature of some features, we can guess they're useless. This means such variables carry little (or no) information related to our main question (the probability of a passenger to survive).\n\nLet's consider the columns named ```Name```, ```PassengerId``` and ```Ticket```. They are just strings specifying who we're talking about, and hardly would be related to the probability of surviving the shipwreck. One way of checking this out is to check if there is any distribution related to suck variables. To do so we [count how many times the unique values appear](https:\/\/www.w3resource.com\/pandas\/series\/series-value_counts.php) ","d320e5f3":"Since tickets' values are basically unique, I'll drop this column as well. But apparently we can play around [to find some relations between people with such codes](https:\/\/www.kaggle.com\/c\/titanic\/discussion\/11127). \n\nThen _maybe_ it will be important to use this column later.","20c7bf9e":"### Fitting model...","d143f170":"## Imports","6206ed66":"Our dataset ```train_data``` has both features and labels. We need to separate them up. In the previou analysis we saw some features are somehow correlated to the probability of surviving:\n* 3rd class male passengers survived less than others;\n* Women survived more than men;\n* etc...\nThen these features are informative to our model. They are essential to our model. But since this is an experimentation, I'll use other features.","811fcadf":"We already have the test split. It is the amount of features not labeled previously. IMy task is to label it.\nLet's suppose I'll train my model using all the data I have (```train_data```). I supposed a lot of things, and I'll keep supposing some things (when I choose a parameter arbitrary, I'm actually implicitly making suppositions). How to know they are correct? Only on the ```test_data```? Hm... not a good idea. [I need to create another partition: a validation one](https:\/\/developers.google.com\/machine-learning\/crash-course\/validation\/another-partition).","b9dc7174":"### One Hot encoding","f854056c":"As seen in the [tutorial](https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial), let's use  an [ensemble method named Random Forest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) first. Let's import them.","9017db16":"We know some tickets appear more times than others. But how does this distribution work? Well, we can see below the most part of tickets' codes appear only once. All others countings sums up only 20% of the dataset. It means the amount of repeated tickets are not that relevant. We can consider the \"ticket\" column made of unique values, basically.","d9aec0c8":"As said above, I'm about to use a Random Forest classifier. But what is a _Random Forest_? A random forest is an ensemble learning method. \"Model Ensembles\" comes from a very simple idea: instead of learning one model only, learn several of them and combine them! \n\n[Sklearn](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) then defines:\n\n```A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.```","2985747b":"What happened? The features ```Pclass```, ```SibSp``` , ```Parch```, and ```Relatives``` were numerical already, then it remained unchanged. The features ```Embarked``` and ```Sex``` were categorical, and they can assume 3 and 2 possible values each. It was created a new column (technical term is Pandas Series) for each of such possible values. \n\nE.g.: the first entry has a feature ```Sex``` labeled with _male_. The new column ```Sex_male``` is marked with a '1', indicating such sample was originally labeled _male_. \n\nNow let's do the same thing with the ```Validation``` set, since it'll be used to check if our model is doing a good job classificating the instances.","139f59ef":"#### Fare\n\nIn the [Titanic Movie FAQ](http:\/\/www.jamescamerononline.com\/TitanicFAQ.htm) there is the question: _How much did a typical 1st,2nd and 3rd ticket cost?_ There we see the following answer:\n1. The 1st class ticket cost varied from $ \\$ 150 $ up to $ \\$4350$\n2. The 2nd class ticket cost something around  $ \\$60$\n3. 3rd class passengers paid between $ \\$15$ and $ \\$40$\n\nSuch information matches the graphs' results: 1st class fare spreads more than the other two.\n","8517dfd5":"## Exploratory Data Analysis","3e35a786":"### What are the characteristics of the passengers (Of those ones who survived, and of thoses who doesn't)?\n\n1. How does the fare of the passengers change based on the sex and class of who survived\/who doesn't?\n2. How does the passengers' age change based on the sex and class of who survived\/who doesn't?\n3. Being with relatives (siblings\/spouses\/parents\/children) influenced the chance of surviving?\n4. Did the port you embarked influence the chance of survive?\n\nTo know more about the meaning of each feature, please [click here](https:\/\/www.kaggle.com\/c\/titanic\/data). I'll be using [categorical data plotting](https:\/\/seaborn.pydata.org\/tutorial\/categorical.html) and [boxplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.boxplot.html#seaborn.boxplot) to do the analysis.","e936dc01":"### Initial Distribution of Survivors\n_The vast majority didn't_","4ab1f46e":"#### Age\n\nIn general, the spread of 1st class' ages is greater than all other spreads. Furthermore, the whiskers decrease as the classes progress. It means the first class passengers had several different ages during the event, while the other classes passengers were more age-specific (notice the lack of 'old' third class passengers).\n\nFor me it was very curious the spread of 2nd class male survivors' agesocorre: we have children there! Something that doesn't occur anywhere else.","483988dd":"Now I just need to use the ensemble model to make predictions based on the test data.","c38e28a7":"#### Relatives\n\n_People with more relatives aboard were less likely to survive_ :(\n","6e6ab5dc":"### Fitting model again\n\nOk! Now we can fit the ensemble model again:","46813b34":"Let's import our data and take a look at some train samples! As can be read [here](https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial), there are 11 features (the second column contains the samples' labels). ","eb6b1fec":"#### Embarked\n\nSurprisingly, those who embarked on Southampton's port were more likely to survive. Those who embarked on Queenstown's were more likely not to survive.","f5a111a4":"### 'Survivors' dataset\n\nLet's chop our dataset so we have two chunks: one for the survivors and other one for the deceased. Let's compare and see how the distribution of features changes based on them.","c06e2377":"## Machine Learning","0568694c":"### Importing Stuff","2f6f8755":"As we can see in the [countplot](https:\/\/seaborn.pydata.org\/generated\/seaborn.countplot.html) above, the vast majority of passengers died in the shipwreck. Notice the columns' height: for the survivors, they're much lower than for the others. Furthermore, notice survived way more than men. \n\nA sad thing that leap to the eye is how many 3rd class men (and women) died.","b0784cda":"### Getting Train and Validation sets ","948b5021":"There are an unique value for each ```PassengerID``` and for each ```Name```, but it does not happen to the ticket value. Let's investigate it (that's why I created a variable based on ticket counting)!","ab18f245":"### How many values each variable assume?","b7c27dc3":"### Let's evaluate the results...\n\nI'll use a metric called [Confusion Matrix](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html) (to know more please [go here](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/true-false-positive-negative), or [here](https:\/\/developers.google.com\/machine-learning\/glossary#confusion_matrix)). It is an way of evaluating how good our model performs, at the same time we account for true\/false positives\/negatives. ","36fa929a":"### Getting features to train the Model","69ea07a6":"Below we have the confusion matrix for our results. What it means? First, consider \"positive\" means \"to be labeled 1\", and \"negative\" \"to be labeled 0\". Then\n\n* The top left element is the count of how many true negatives we have. It is, how many negative examples were correctly classified as \"negatives\";\n* The top right element is the count of how many false positives we have. It is, how many negatives examples were wrongly classified as \"positives\";\n* The down left element is the count of how many false negatives we have. It is, how many positive examples were wrongly classified as \"negatives\";\n* The down right element is the count of how many true positives we have. It is, how many positive examples were correctly classified as \"positives\".","74c9b8e4":"The error above is due to the fact some of our features assume non-numeric values. As can be [seen on the doc](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit), ```RandomForest.fit()``` needs numeric features. To know more about such Error, please access [this StackOverFlow link](https:\/\/stackoverflow.com\/questions\/30384995\/randomforestclassfier-fit-valueerror-could-not-convert-string-to-float).\n\nOk, now what? How to solve such issue? We can actually [map our non-numerical categorical features into numerical features](https:\/\/developers.google.com\/machine-learning\/crash-course\/representation\/feature-engineering). Some of our features are 'categorical', which means they assume some few discrete values. The method I'll be using is called [one-hot encoding](https:\/\/developers.google.com\/machine-learning\/glossary#one-hot_encoding), which turns each sample into a sparse vector (_sparse vectors_ have more zeroed inputs than non-zero ones) that detects if a specific feature value appeared.\n\nHow to one-hot encode? Luckily, [Pandas has such method](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.get_dummies.html) ready to use a line of code away!","17e5bf70":"That said, let's fit our model!","1b2b7a82":"## Final Submission\n\nI must submit my predictions on test data. First, let's suit the ```test_data``` dataset just like what I did at ```train_data```\/```validation_data```.","96357f42":"### Plot code","e1fe12c8":"### Training again...","8f0ec0f2":"## First analysis (removing useless variables)"}}