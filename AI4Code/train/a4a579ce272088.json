{"cell_type":{"663cba67":"code","580f6c23":"code","1457a9d3":"code","e02fa2bb":"code","ca181fcc":"code","7d37dcd8":"code","1c328f96":"code","61fe363a":"code","cb121d57":"code","621eff8b":"code","8ee8c1aa":"code","257c75d8":"code","123f222c":"code","98903b42":"code","efb4c44b":"code","3bc26a10":"code","c0274e14":"code","b1257562":"code","b8fdc78f":"code","abf4aa8c":"markdown","4e3fbb65":"markdown","95637116":"markdown","b4148afb":"markdown","bcea107f":"markdown","fdc64f2a":"markdown","13a0757e":"markdown","d0e297a7":"markdown","e796b9ef":"markdown","b3e66ffa":"markdown","60aa6a34":"markdown"},"source":{"663cba67":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","580f6c23":"iris=pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')","1457a9d3":"iris","e02fa2bb":"input_point=pd.DataFrame({'SepalLengthCm': [4.7], 'SepalWidthCm': [3.7], 'PetalLengthCm': [2], 'PetalWidthCm': [0.3]})","ca181fcc":"new=iris.append(input_point,ignore_index=True)","7d37dcd8":"new","1c328f96":"new.drop(columns='Id',inplace=True)","61fe363a":"train=pd.get_dummies(new.iloc[:,:-1].astype(str))","cb121d57":"train","621eff8b":"train['Species']=iris['Species']","8ee8c1aa":"train","257c75d8":"class NaiveBayes():\n\n    def __init__(self):\n        self.prob=dict()   # It stores, for every column, the probabilities of the unique values of every column given a class \n        self.target=''     # Stores the target column name   \n        self.cl_prob=[]    # Stores the probabilities of every class\n  \n    def fit(self,data,target):\n\n        # This function trains the NaiveBayes model. \n        # Params: \n        # data: The data to train(should contain the target column)\n        # target: Mention the name of the target column\n\n        print('Starting Training...')\n\n        self.target=target\n\n        # Extracting the columns except the target column\n        columns=data.columns.to_list()\n        columns.remove(self.target) \n\n        # Generating probabilities for every unique value in every column for every class  \n        for column in columns:\n            self.prob[column]=pd.crosstab(data[column],data[self.target])\n        for key in self.prob.keys():\n            for cl in self.prob[key].columns:\n                self.prob[key][cl]=self.prob[key][cl]\/self.prob[key][cl].sum()\n\n        # Calculating the probabilities of every individual class\n        cl_count=data[self.target].value_counts()\n        self.cl_prob=cl_count\/cl_count.sum()\n\n        print('Training Complete!')\n\n\n    def predict(self,test):\n\n        # This function predicts the class for a given data\n        # Params:\n        # test: The data to predict the class. Should be a DataFrame.\n\n        output=np.array([])  # Stores the predicted classes for every row\n\n        # For every row predicting the class label\n        for row in range(test.shape[0]):\n\n\n            y=dict()       # Stores the probabilities corresponding to every class\n            for cl in self.cl_prob.index.to_list():\n                y[cl]=self.cl_prob[cl]\n                for column in test.columns:\n                    try:\n                        y[cl]=y[cl]*self.prob[column][cl][test.loc[row][column]]\n                    except:\n                        pass\n\n            output=np.append(output,max(y, key=y.get))   # Get the class with maximum probability and store it in output \n        return output","123f222c":"nb=NaiveBayes()","98903b42":"train.iloc[:-1,:]","efb4c44b":"nb.fit(train.iloc[:-1,:],'Species')","3bc26a10":"test=train.iloc[-1:,:-1].reset_index(drop=True)","c0274e14":"test","b1257562":"results=nb.predict(test)","b8fdc78f":"results","abf4aa8c":"### The input point to predict","4e3fbb65":"### The training data","95637116":"### Predicting now","b4148afb":"### The input flower belongs to iris setosa ","bcea107f":"### Did not encode the target column as it is already categorical","fdc64f2a":"## The General Class of Naive Bayes model","13a0757e":"### Encoding the data","d0e297a7":"## This data is numerical but Naive Bayes requires categorical columns. \nWe are going to encode the columns using pd.get_dummies() but to maintain the same encoding for the prediction point, we are going to concatenate prediction point with the training data. Once the encoding is done, we will train our model with the training data only and not pass the input point to the training process.","e796b9ef":"### Input Point","b3e66ffa":"### Prediction results","60aa6a34":"### Concatenating input point with training data"}}