{"cell_type":{"7e905cdb":"code","1622d665":"code","4b25c374":"code","c0ed2b45":"code","064a56f3":"code","603cd466":"code","9f1772ef":"code","982dc105":"code","c3c8b674":"code","6d629b68":"code","72db44f9":"code","677b53e1":"code","8806bf22":"code","1e28ae4f":"code","7b7a4efc":"code","45063e8c":"code","4bc74b55":"code","0e521315":"code","96ddba74":"code","aa64b628":"code","b032f6b4":"code","392b0177":"code","2022c04f":"code","7438f266":"code","72c3b11b":"code","bcba0bc8":"code","f0209bf5":"code","a89d3fa7":"code","eb65463a":"code","163f7a47":"markdown","22887333":"markdown","a8619f2a":"markdown","3ba7c362":"markdown","6a7c2a15":"markdown","1f71f870":"markdown","6ea623c7":"markdown","79c8a03c":"markdown","db799a2a":"markdown","7c5bdc27":"markdown","d3a85585":"markdown","bb9f9c9e":"markdown","8b87df1d":"markdown","96b1136f":"markdown","766afa00":"markdown","2ad8611d":"markdown"},"source":{"7e905cdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nimport sys\nimport os\n!pip install tldextract -q\nimport tldextract\nimport warnings\nimport regex as re\nimport eli5\nfrom typing import *\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1622d665":"from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.pipeline import FeatureUnion, Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn import svm\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import LinearSVC\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom urllib.parse import urlparse\nfrom nltk.tokenize import RegexpTokenizer\n\nwarnings.filterwarnings(\"ignore\")","4b25c374":"df=pd.read_csv(r'\/kaggle\/input\/web-page-phishing-detection-dataset\/dataset_phishing.csv')\ndf.head()","c0ed2b45":"df_grp = df.groupby([\"url\"])[[\"status\"]].sum().reset_index()\ndf_grp.head()","064a56f3":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\ndef parse_url(url: str) -> Optional[Dict[str, str]]:\n    try:\n        no_scheme = not url.startswith('https:\/\/') and not url.startswith('http:\/\/')\n        if no_scheme:\n            parsed_url = urlparse(f\"http:\/\/{url}\")\n            return {\n                \"scheme\": None, # not established a value for this\n                \"netloc\": parsed_url.netloc,\n                \"path\": parsed_url.path,\n                \"params\": parsed_url.params,\n                \"query\": parsed_url.query,\n                \"fragment\": parsed_url.fragment,\n            }\n        else:\n            parsed_url = urlparse(url)\n            return {\n                \"scheme\": parsed_url.scheme,\n                \"netloc\": parsed_url.netloc,\n                \"path\": parsed_url.path,\n                \"params\": parsed_url.params,\n                \"query\": parsed_url.query,\n                \"fragment\": parsed_url.fragment,\n            }\n    except:\n        return None","603cd466":"df_grp[\"parsed_url\"] = df_grp.url.apply(parse_url)\ndf_grp","9f1772ef":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\ndf_grp = pd.concat([\n    df_grp.drop(['parsed_url'], axis=1),\n    df_grp['parsed_url'].apply(pd.Series)\n], axis=1)\ndf_grp","982dc105":"df_grp = df_grp[~df_grp.netloc.isnull()]\ndf_grp","c3c8b674":"df_grp[\"length\"] = df_grp.url.str.len()","6d629b68":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\n#The TLD is then extracted using a python library, and if no TLD is present simply add 'None'.\n\ndf_grp[\"tld\"] = df_grp.netloc.apply(lambda nl: tldextract.extract(nl).suffix)\ndf_grp['tld'] = df_grp['tld'].replace('','None')","72db44f9":"#Next is a regex to determine if the URL is an IP address.\n\ndf_grp[\"is_ip\"] = df_grp.netloc.str.fullmatch(r\"\\d+\\.\\d+\\.\\d+\\.\\d+\")","677b53e1":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\ndf_grp['domain_hyphens'] = df_grp.netloc.str.count('-')\ndf_grp['domain_underscores'] = df_grp.netloc.str.count('_')\ndf_grp['path_hyphens'] = df_grp.path.str.count('-')\ndf_grp['path_underscores'] = df_grp.path.str.count('_')\ndf_grp['slashes'] = df_grp.path.str.count('\/')","8806bf22":"df_grp['full_stops'] = df_grp.path.str.count('.')","1e28ae4f":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\ndef get_num_subdomains(netloc: str) -> int:\n    subdomain = tldextract.extract(netloc).subdomain \n    if subdomain == \"\":\n        return 0\n    return subdomain.count('.') + 1\n\ndf_grp['num_subdomains'] = df_grp['netloc'].apply(lambda net: get_num_subdomains(net))","7b7a4efc":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\ntokenizer = RegexpTokenizer(r'[A-Za-z]+')\ndef tokenize_domain(netloc: str) -> str:\n    split_domain = tldextract.extract(netloc)\n    no_tld = str(split_domain.subdomain +'.'+ split_domain.domain)\n    return \" \".join(map(str,tokenizer.tokenize(no_tld)))\n         \ndf_grp['domain_tokens'] = df_grp['netloc'].apply(lambda net: tokenize_domain(net))","45063e8c":"df_grp['path_tokens'] = df_grp['path'].apply(lambda path: \" \".join(map(str,tokenizer.tokenize(path))))","4bc74b55":"df_grp.columns.tolist()","0e521315":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\n#The labels are now extracted and the URL column removed.\n\ndf_grp_y = df_grp['status'] #It was df_grp_y = df_grp['label'] But label Disappeared? Check columns tolist above\ndf_grp.drop('status', axis=1, inplace=True) #Where label disappeared? Label IS BACK!\ndf_grp.drop('url', axis=1, inplace=True)\ndf_grp.drop('scheme', axis=1, inplace=True)\ndf_grp.drop('netloc', axis=1, inplace=True)\ndf_grp.drop('path', axis=1, inplace=True)\ndf_grp.drop('params', axis=1, inplace=True)\ndf_grp.drop('query', axis=1, inplace=True)\ndf_grp.drop('fragment', axis=1, inplace=True)\ndf_grp","96ddba74":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\nclass Converter(BaseEstimator, TransformerMixin):\n    def fit(self, x, y=None):\n        return self\n\n    def transform(self, data_frame):\n        return data_frame.values.ravel()","aa64b628":"X_train, X_test, y_train, y_test = train_test_split(df_grp, df_grp_y, test_size=0.2)","b032f6b4":"numeric_features = ['length', 'domain_hyphens', 'domain_underscores', 'path_hyphens', 'path_underscores', 'slashes', 'full_stops', 'num_subdomains']\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', MinMaxScaler())])","392b0177":"categorical_features = ['tld', 'is_ip']\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","2022c04f":"vectorizer_features = ['domain_tokens','path_tokens']\nvectorizer_transformer = Pipeline(steps=[\n    ('con', Converter()),\n    ('tf', TfidfVectorizer())])","7438f266":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\nvectorizer_features = ['domain_tokens','path_tokens']\nvectorizer_transformer = Pipeline(steps=[\n    ('con', Converter()),\n    ('tf', TfidfVectorizer())])","72c3b11b":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features),\n        ('domvec', vectorizer_transformer, ['domain_tokens']),\n        ('pathvec', vectorizer_transformer, ['path_tokens'])\n    ])\n\nsvc_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LinearSVC())])\n\nlog_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', LogisticRegression())])\n\nnb_clf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('classifier', MultinomialNB())])\nsvc_clf.fit(X_train, y_train)\nlog_clf.fit(X_train, y_train)\nnb_clf.fit(X_train, y_train)","bcba0bc8":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\ndef results(name: str, model: BaseEstimator) -> None:\n    preds = model.predict(X_test)\n\n    print(name + \" score: %.3f\" % model.score(X_test, y_test))\n    print(classification_report(y_test, preds))\n    labels = ['Good', 'Bad']\n\n    conf_matrix = confusion_matrix(y_test, preds)\n\n    font = {'family' : 'normal',\n            'size'   : 14}\n\n    plt.rc('font', **font)\n    plt.figure(figsize= (10,6))\n    sns.heatmap(conf_matrix, xticklabels=labels, yticklabels=labels, annot=True, fmt=\"d\", cmap='Greens')\n    plt.title(\"Confusion Matrix for \" + name)\n    plt.ylabel('True Class')\n    plt.xlabel('Predicted Class')","f0209bf5":"results(\"SVC\" , svc_clf)\nresults(\"Logistic Regression\" , log_clf)\nresults(\"Naive Bayes\" , nb_clf)","a89d3fa7":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models\n\n\nonehot_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['cat'].named_steps['onehot'].get_feature_names(input_features=categorical_features))\ndomvect_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['domvec'].named_steps['tf'].get_feature_names())\npathvect_columns = list(svc_clf.named_steps['preprocessor'].named_transformers_['pathvec'].named_steps['tf'].get_feature_names())\nnumeric_features_list = list(numeric_features)\nnumeric_features_list.extend(onehot_columns)\nnumeric_features_list.extend(domvect_columns)\nnumeric_features_list.extend(pathvect_columns)\neli5.explain_weights(svc_clf.named_steps['classifier'], top=20, feature_names=numeric_features_list)","eb65463a":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Tyler Sullivan and Matthew Franglen for the script')","163f7a47":"\"As we see the NB performs best out of the three followed by MultinomialNB. While Logistic Regression performs the worst, we can see it produces less false negatives than Naive Bayes.\"\n\n\"Also if numerical features are removed, logistic regression performs better. I wouldn't know why this would be the case and would be interested to hear some ideas for it.\"","22887333":"#The first meaningful bit of data to extract is the length of the URL.","a8619f2a":"The only categorical feature is TLD . (Those will be vectorized domain_tokens and path_tokens) and OneHot encoding will be used for this. Interestingly there is no difference between using this or converting and using the TfidfVectorizer. However, using OneHot encoding makes the TLD obvious in the feature importance section.","3ba7c362":"As previous notebooks have shown, the lexical features of the URL will be important. In this instance, I have decided to separate the tokens from the path and the domain itself. My thinking here is that the same word in a path and domain may have very different meanings. By this i mean if you see 'paypal' in a URL path, it may be a malicious URL which is trying to seem legitimate, but 'paypal' in the domain may be more legitimate.","6a7c2a15":"The next step is to link all the transformers together in a ColumnTransformer, and create a pipeline for each classifier.","1f71f870":"The next few sections relate to certain punctuation in the URL which may be an indicator one way or another that a URL is malicious. My reasoning behind this is that typosquatted domains (which are almost always malicious) may contain this punctation to appear similar to a legitimate domain. There may also be more of each in the path of the URL for a legitimate URL as blogs often use underscores in a URL.","6ea623c7":"#Feature Importance\n\nFinally, to see what features are most strongly weighted to the SVC classifier I use eli5 to show this. It is worth noting that weights may be high for rarer features and should be taken with a grain of salt.","79c8a03c":"#Code by Tyler Sullivan and Matthew Franglen https:\/\/www.kaggle.com\/tylersullivan\/classifying-phishing-urls-three-models","db799a2a":"CountVectorizer and TfidfVectorizer produce very similar results, but with the best performing model (spoiler its SVC) Tfidf slightly improved the score.","7c5bdc27":"Full stops in the path could indicate that theres an attempt to fool a user into thinking a domain is legit. For example, attacker.com\/paypal.com may be used to trick a user. Full stops may also be a sign of files in the URL such as shell.exe","d3a85585":"#Results","bb9f9c9e":"The numeric features need their own pipeline to scale the data, MinMaxScaler was used as MultinomialNB needs no negative values to work.","8b87df1d":"#Don't change any word in the snippet below! ","96b1136f":"Similar to the previous datapoint, getting the full stops in a subdomain will count how many subdomains are present. Lots may be another visual trick such as paypal.com.attacker.com\/","766afa00":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcRKAt3ZQq4ZYJEMWpqcILtjKPXToNMeGTu0KaDeeURlhgYv00tdTLMD8WpSx2JseUfEMOcJ8J7Ou3PRZw&usqp=CAU)ashish-pal.medium.com","2ad8611d":"#Training\n\nWhen using pipelines and vectorizers, you need a converter to feed the vectorizer every word of that column. It cannot add the values one row at a time and so a converter class must be created."}}