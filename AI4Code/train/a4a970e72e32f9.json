{"cell_type":{"68691fb0":"code","0e4ff8e4":"code","ca72d7c3":"code","2246aed5":"code","8af7ce8b":"code","c48ed63a":"code","b9bf16ae":"code","818803e1":"code","37d8375a":"code","87a16373":"code","852976a4":"code","64e0ad57":"code","403e9d8d":"code","ce9e4f69":"code","38c8108b":"code","1c4b7521":"code","cb6914ec":"code","fb3ee405":"code","e9f22062":"code","86e16c04":"code","a33496bf":"code","50f37f36":"code","f9292bc4":"code","d583dbe8":"code","ff2c6415":"code","536dcf51":"code","278948bc":"code","b9add58f":"code","32038f35":"markdown","304550bc":"markdown","5e3f6e01":"markdown","47b8b215":"markdown","e73ddbc9":"markdown","220f3ccc":"markdown","a8310609":"markdown","a9c829c3":"markdown"},"source":{"68691fb0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (11.0, 9.0)\n\n#Supressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","0e4ff8e4":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","ca72d7c3":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","2246aed5":"#Checking for empty or null records\nnull_data = train_df.isnull().sum()\nprint('Total null data in the training dataset is {}'.format(sum(null_data)))","8af7ce8b":"sns.countplot(train_df['target'])","c48ed63a":"bought = train_df[train_df['target'] == 1].shape[0]\n\nprint('Training dataset has {} records out of which {}% have done transaction'.format(train_df.shape[0], 100*(bought\/train_df.shape[0])))","b9bf16ae":"#We will use 150000 records for training and the rest for testing out model\n\n#For training\ndf_train_bal = train_df[:150000]\n\n#For testing\ndf_test_bal = train_df[150000:]","818803e1":"#Creating a balanced dataset with comparable counts of both the outcome classes 0 and 1\ndf_train_bal_0 = df_train_bal[df_train_bal['target'] == 0]\ndf_train_bal_1 = df_train_bal[df_train_bal['target'] == 1]\n\n#Selected 50000 non-transactions\ndf_train_bal_0 = df_train_bal_0.sample(50000)\n\n#Final trainign dataset with balanced records, Randomize the dataset\ndf_train_bal = df_train_bal_0.append(df_train_bal_1).sample(frac=1)","37d8375a":"df_train_bal['target'].value_counts()","87a16373":"corr = df_train_bal.corr()\nsns.heatmap(corr)","852976a4":"corr['target'].sort_values(ascending=False)","64e0ad57":"#For training\nX = df_train_bal.drop(['target','ID_code'], axis=1)\nX.head()","403e9d8d":"y = df_train_bal.iloc[:,1:2]\n\nprint('Unique target values are :')\nprint(y['target'].value_counts())","ce9e4f69":"#For testing\nX_test = df_test_bal.drop(['target','ID_code'], axis=1)\n\ny_test = df_test_bal.iloc[:,1:2]","38c8108b":"from sklearn.linear_model import LogisticRegression\n\nlr_model = LogisticRegression()\nlr_model.fit(X, y)","1c4b7521":"lr_model.score(X_test, y_test)","cb6914ec":"#Cross Validation\nfrom sklearn.model_selection import cross_val_score\n\n# Perform 6-fold cross validation\nscores = cross_val_score(lr_model, X_test, y_test, cv=6)\n\nprint('Cross-validated scores:', scores)","fb3ee405":"#Processing actual Testing data before submission\ntest_df_clean = test_df.drop('ID_code', axis=1)\ntest_df_clean.head()","e9f22062":"#Applying PCA on both training and test dataset\nfrom sklearn.decomposition import PCA\n\npca = PCA()\n#Training data\nX_train_pca = pca.fit_transform(X)","86e16c04":"#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Satander Dataset Explained Variance')\nplt.show()","a33496bf":"#Now applyting PCA for 100 componenets\npca_best = PCA(n_components=100)\nX_train_pca = pca_best.fit_transform(X)\nX_test_pca = pca_best.transform(X_test)\nX_actual_test = pca_best.transform(test_df_clean)\n\n#Coveting numpy array back to Dataframe\nX_train_pca = pd.DataFrame(X_train_pca)\nX_test_pca= pd.DataFrame(X_test_pca)\nX_actual_test = pd.DataFrame(X_actual_test)","50f37f36":"#Applying best param to LR model\nlr_model_best = LogisticRegression(C=10)\nlr_model_best.fit(X_train_pca, y)","f9292bc4":"#Cross Validation of Best Model\nfrom sklearn.model_selection import cross_val_score\n\n# Perform 6-fold cross validation\nscores = cross_val_score(lr_model_best, X_test_pca, y_test, cv=6)\n\nprint('Cross-validated scores:', scores)","d583dbe8":"test_pred = lr_model_best.predict(X_actual_test)","ff2c6415":"submission = pd.DataFrame({\n        \"ID_code\": test_df[\"ID_code\"],\n        \"target\": test_pred\n    })","536dcf51":"submission.to_csv('submission.csv', index=False)","278948bc":"submission.head(10)","b9add58f":"submission['target'].value_counts()","32038f35":"The data is highly imbalanced. We need to balance it before training.","304550bc":"### **Creating balanced training dataset**\n","5e3f6e01":"So we get approximately 91% accuracy without any tuning or enhancement. Now let's use PCA for Feature Reduction.","47b8b215":"Above dataset looks much more balanced now.","e73ddbc9":"##  Applying PCA","220f3ccc":"The above heatmap does not give much information on the feature correlation. Let's check tha actual correlation values.","a8310609":"## On Test Dataset","a9c829c3":"Above plot clearly shows that 100 features explains  approximately 90% of the variance."}}