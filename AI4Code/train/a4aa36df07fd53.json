{"cell_type":{"8f872f39":"code","1e953520":"code","2d1ba83d":"code","8e0c83b2":"code","109a8218":"code","4d712a90":"code","bc11ccd3":"code","08075360":"code","ffc3aeb1":"code","4af01998":"code","9942098c":"code","33d9b814":"code","6c4c284e":"code","460c2817":"code","9c95991d":"code","ad635a76":"code","89b99885":"code","7ec06bb9":"code","d0a0e105":"code","ae4fb73c":"code","0918675a":"code","e8d75340":"code","8a9d5a62":"code","1e54dc3d":"code","a42a6dc6":"code","86907c98":"code","8a981778":"code","0f3f6ff6":"code","999864e7":"code","98bde3ac":"code","511e4670":"code","66702366":"code","97e97d16":"code","11f339a4":"code","cf0dbf69":"code","e38a728c":"code","aa86bbf4":"code","0093f948":"code","e97303d7":"code","ffb1d3a3":"code","23a370b1":"code","50295b6f":"code","aa1e4e48":"code","224de3db":"code","4eebc0a8":"code","3b4fb3e0":"code","11f6bc4f":"code","ecb49b6e":"code","b6ff8582":"markdown","8f892a8f":"markdown","289a7473":"markdown","6a57156a":"markdown","4364a231":"markdown","ba25ed87":"markdown","f9b180f7":"markdown","2371ed3c":"markdown","3ebbe424":"markdown","fc0288e7":"markdown","b86bd511":"markdown","c0a1b260":"markdown","4f948f34":"markdown","434794a4":"markdown","316a9886":"markdown","45dbe589":"markdown","7ccc22a2":"markdown","40dc4557":"markdown","94c41e2b":"markdown"},"source":{"8f872f39":"import numpy as np \nimport pandas as pd\nimport random as rn\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(1)\nrn.seed(1)\n\n# Loading the multiple choices dataset, we will not look to the free form data on this study\nmc = pd.read_csv('..\/input\/kaggle-survey-2018\/multipleChoiceResponses.csv', low_memory=False)\n\n# Separating questions from answers\n# This Series stores all questions\nmcQ = mc.iloc[0,:]\n# This DataFrame stores all answers\nmcA = mc.iloc[1:,:]","1e953520":"print(mcQ.shape)\nmcQ.describe()","2d1ba83d":"print(\"Jumlah Kolom = {}\".format(mcA.shape[1]))\nprint(\"Jumlah Baris = {}\".format(mcA.shape[0]))\nmcA.describe()","8e0c83b2":"mcA.head(2)","109a8218":"# Set ipython's max row display\npd.set_option('display.max_row', 1000)\n# Set iPython's max column width to 50\npd.set_option('display.max_columns', 50)\npd.set_option('max_colwidth',100)\nmcQ","4d712a90":"# Creating a table with personal data\npersonal_data = mcA.iloc[:,[0,1,2,3,4,5,6,7,8,9,10,11,12,22,84,86,108,124,126,127,128,129]].copy()\n\n# renaming columns\ncols = ['survey_duration', 'gender', 'gender_text', 'age', 'country', 'education_level', 'undergrad_major', 'role', 'role_text',\n        'employer_industry', 'employer_industry_text', 'years_experience', 'yearly_compensation','primary_analize_tool', 'most_used_prog_lang',\n        'most_reccom_prog_lang','most_used_ml_lib','most_used_vis_lib','coding_time','coding_time_years','ml_exp_years','are_you_data_scientist']\npersonal_data.columns = cols\n\n# Drop text based features\npersonal_data.drop(['gender_text', 'role_text', 'employer_industry_text'], axis=1, inplace=True)\n\n\npersonal_data.head(5)","bc11ccd3":"personal_data.shape","08075360":"personal_data.isnull().any()","ffc3aeb1":"personal_data['survey_duration'].unique()","4af01998":"personal_data['gender'].unique()","9942098c":"personal_data['age'].unique()","33d9b814":"personal_data['country'].unique()","6c4c284e":"personal_data['education_level'].unique()","460c2817":"personal_data['undergrad_major'].unique()","9c95991d":"personal_data['role'].unique()","ad635a76":"\npersonal_data['employer_industry'].unique()","89b99885":"\npersonal_data['years_experience'].unique()","7ec06bb9":"\npersonal_data['yearly_compensation'].unique()","d0a0e105":"\npersonal_data['primary_analize_tool'].unique()","ae4fb73c":"\npersonal_data['most_used_prog_lang'].unique()","0918675a":"\npersonal_data['most_reccom_prog_lang'].unique()","e8d75340":"\npersonal_data['most_used_ml_lib'].unique()","8a9d5a62":"\npersonal_data['most_used_vis_lib'].unique()","1e54dc3d":"\npersonal_data['coding_time'].unique()","a42a6dc6":"\npersonal_data['coding_time_years'].unique()","86907c98":"\npersonal_data['ml_exp_years'].unique()","8a981778":"\npersonal_data['are_you_data_scientist'].unique()","0f3f6ff6":"# dropping all NaN and I do not wish to disclose my approximate yearly compensation, because we are only interested in respondents that revealed their earnings\npersonal_data = personal_data[~personal_data['yearly_compensation'].isnull()].copy()\nnot_disclosed = personal_data[personal_data['yearly_compensation'] == 'I do not wish to disclose my approximate yearly compensation'].index\npersonal_data = personal_data.drop(list(not_disclosed), axis=0)\nprint(\"Done!\")","999864e7":"compensation = personal_data.yearly_compensation.str.replace(',', '').str.replace('500000\\+', '500-500000').str.split('-')\npersonal_data['yearly_compensation_numerical'] = compensation.apply(lambda x: (int(x[0]) * 1000 + int(x[1]))\/ 2) \/ 1000 # it is calculated in thousand dollars\n#personal_data = personal_data.drop(['yearly_compensation'], axis=1)\nprint('Dataset Shape: ', personal_data.shape)\npersonal_data.head(3)","98bde3ac":"# Finding the compensation that separates the Top 20% most welll paid from the Bottom 80%\ntop20flag = personal_data.yearly_compensation_numerical.quantile(0.8)\ntop20flag","511e4670":"# Creating a flag to identify who belongs to the Top 20%\npersonal_data['top20'] = personal_data.yearly_compensation_numerical > top20flag\n\n# creating data for future mapping of values\ntop20 = personal_data.groupby('yearly_compensation', as_index=False)['top20'].min()\nprint(\"Done!\")","66702366":"# Some helper functions to make our plots cleaner with Plotly\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)\n\n\ndef gen_xaxis(title):\n    \"\"\"\n    Creates the X Axis layout and title\n    \"\"\"\n    xaxis = dict(\n            title=title,\n            titlefont=dict(\n                color='#AAAAAA'\n            ),\n            showgrid=False,\n            color='#AAAAAA',\n            )\n    return xaxis\n\n\ndef gen_yaxis(title):\n    \"\"\"\n    Creates the Y Axis layout and title\n    \"\"\"\n    yaxis=dict(\n            title=title,\n            titlefont=dict(\n                color='#AAAAAA'\n            ),\n            showgrid=False,\n            color='#AAAAAA',\n            )\n    return yaxis\n\n\ndef gen_layout(charttitle, xtitle, ytitle, lmarg, h, annotations=None):  \n    \"\"\"\n    Creates whole layout, with both axis, annotations, size and margin\n    \"\"\"\n    return go.Layout(title=charttitle, \n                     height=h, \n                     width=800,\n                     showlegend=False,\n                     xaxis=gen_xaxis(xtitle), \n                     yaxis=gen_yaxis(ytitle),\n                     annotations = annotations,\n                     margin=dict(l=lmarg),\n                    )\n\n\ndef gen_bars(data, color, orient):\n    \"\"\"\n    Generates the bars for plotting, with their color and orient\n    \"\"\"\n    bars = []\n    for label, label_df in data.groupby(color):\n        if orient == 'h':\n            label_df = label_df.sort_values(by='x', ascending=True)\n        if label == 'a':\n            label = 'lightgray'\n        bars.append(go.Bar(x=label_df.x,\n                           y=label_df.y,\n                           name=label,\n                           marker={'color': label},\n                           orientation = orient\n                          )\n                   )\n    return bars\n\n\ndef gen_annotations(annot):\n    \"\"\"\n    Generates annotations to insert in the chart\n    \"\"\"\n    if annot is None:\n        return []\n    \n    annotations = []\n    # Adding labels\n    for d in annot:\n        annotations.append(dict(xref='paper', x=d['x'], y=d['y'],\n                           xanchor='left', yanchor='bottom',\n                           text= d['text'],\n                           font=dict(size=13,\n                           color=d['color']),\n                           showarrow=False))\n    return annotations\n\n\ndef generate_barplot(text, annot_dict, orient='v', lmarg=120, h=400):\n    \"\"\"\n    Generate the barplot with all data, using previous helper functions\n    \"\"\"\n    layout = gen_layout(text[0], text[1], text[2], lmarg, h, gen_annotations(annot_dict))\n    fig = go.Figure(data=gen_bars(barplot, 'color', orient=orient), layout=layout)\n    return iplot(fig)\n\nprint(\"Done!\")","97e97d16":"from pandas.api.types import CategoricalDtype\n# transforming compensation into category type and ordering the values\ncateg = ['0-10,000', '10-20,000', '20-30,000', '30-40,000', '40-50,000',\n         '50-60,000', '60-70,000', '70-80,000', '80-90,000', '90-100,000',\n         '100-125,000', '125-150,000', '150-200,000', '200-250,000', '250-300,000',\n         '300-400,000', '400-500,000', '500,000+']\ncat_type = CategoricalDtype(categories=categ, ordered=True)\npersonal_data.yearly_compensation = personal_data.yearly_compensation.astype(cat_type)\n# Kode diatas untuk memastikan bahwa urutan penampilannya sesuai dengan yang kita inginkan walaupun kita sudah menset sebagai kategori diatas\n\n# Counting the quantity of respondents per compensation\nbarplot = personal_data.yearly_compensation.value_counts(sort=False).to_frame().reset_index()\nbarplot.columns = ['yearly_compensation', 'qty']\n\n# mapping back to get top 20% label\nbarplot = barplot.merge(top20, on='yearly_compensation')\nbarplot.columns = ['x', 'y', 'top20']\n\n# apply color for top 20% and bottom 80%\nbarplot['color'] = barplot.top20.apply(lambda x: 'mediumaquamarine' if x else 'lightgray') \n\n# Create title and annotations\ntitle_text = ['<b>How Much Does Kagglers Get Paid?<\/b>', 'Yearly Compensation (K USD)', 'Quantity of Respondents']\nannotations = [{'x': 0.06, 'y': 2200, 'text': '80% of respondents earn up to USD 90k','color': 'gray'},\n              {'x': 0.51, 'y': 1100, 'text': '20% of respondents earn more than USD 90k','color': 'mediumaquamarine'}]\n\n# call function for plotting\ngenerate_barplot(title_text, annotations)\n\nprint(\"Done!\")","11f339a4":"# creating masks to identify students and not students\nis_student_mask = (personal_data['role'] == 'Student') | (personal_data['employer_industry'] == 'I am a student')\nnot_student_mask = (personal_data['role'] != 'Student') & (personal_data['employer_industry'] != 'I am a student')\n\n# Counting the quantity of respondents per compensation (where is student)\nbarplot = personal_data[is_student_mask].yearly_compensation.value_counts(sort=False).to_frame().reset_index()\nbarplot.columns = ['yearly_compensation', 'qty']\n\n# mapping back to get top 20% label\nbarplot = barplot.merge(top20, on='yearly_compensation')\nbarplot.columns = ['x', 'y', 'top20']\n\n# apply color for top 20% and bottom 80%\nbarplot['color'] = barplot.top20.apply(lambda x: 'mediumaquamarine' if x else 'lightgray') \n\n# title and annotations\ntitle_text = ['<b>Do Students Get Paid at All?<\/b><br><i>only students<\/i>', 'Yearly Compensation (K USD)', 'Quantity of Respondents']\nannotations = [{'x': 0.06, 'y': 1650, 'text': '75% of students earn up to USD 10k','color': 'crimson'}]\n\n# ploting\ngenerate_barplot(title_text, annotations)","cf0dbf69":"# Calculates compensation per education level\nbarplot = personal_data[not_student_mask].groupby(['education_level'], as_index=False)['yearly_compensation_numerical'].mean()\nbarplot['no_college'] = (barplot.education_level == 'No formal education past high school') | \\\n                        (barplot.education_level == 'Doctoral degree')\n\n# creates a line break for better visualisation\nbarplot.education_level = barplot.education_level.str.replace('study without', 'study <br> without')\n\nbarplot.columns = ['y', 'x', 'no_college']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.no_college.apply(lambda x: 'coral' if x else 'a')\n\n# Add title and annotations\ntitle_text = ['<b>Impact of Formal Education on Compenstaion<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (K USD)', 'Level of Education']\nannotations = []\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300)","e38a728c":"# Calculates compensation per industry\nbarplot = personal_data[not_student_mask].groupby(['employer_industry'], as_index=False)['yearly_compensation_numerical'].mean()\n\n# Flags the top 5 industries to add color\nbarplot['best_industries'] = (barplot.employer_industry == 'Medical\/Pharmaceutical') | \\\n                             (barplot.employer_industry == 'Insurance\/Risk Assessment') | \\\n                             (barplot.employer_industry == 'Military\/Security\/Defense') | \\\n                             (barplot.employer_industry == 'Hospitality\/Entertainment\/Sports') | \\\n                             (barplot.employer_industry == 'Accounting\/Finance')\n\nbarplot.columns = ['y', 'x', 'best_industries']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.best_industries.apply(lambda x: 'darkgoldenrod' if x else 'a')\n\ntitle_text = ['<b>Average Compensation per Industry | Top 5 in Color<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (K USD)', 'Industry']\nannotations = []\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300, h=600)","aa86bbf4":"# Calculates compensation per role\nbarplot = personal_data[not_student_mask].groupby(['role'], as_index=False)['yearly_compensation_numerical'].mean()\n\n# Flags the top 5 roles to add color\nbarplot['role_highlight'] = (barplot.role == 'Data Scientist') | \\\n                        (barplot.role == 'Product\/Project Manager') | \\\n                        (barplot.role == 'Consultant') | \\\n                        (barplot.role == 'Data Journalist') | \\\n                        (barplot.role == 'Manager') | \\\n                        (barplot.role == 'Principal Investigator') | \\\n                        (barplot.role == 'Chief Officer')\n\nbarplot.columns = ['y', 'x', 'role_highlight']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.role_highlight.apply(lambda x: 'mediumvioletred' if x else 'lightgray')\n\ntitle_text = ['<b>Average Compensation per Role | Top 7 in Color<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (USD)', 'Job Title']\nannotations = [{'x': 0.6, 'y': 11.5, 'text': 'The first step into the ladder<br>of better compensation is<br>becoming a Data Scientist','color': 'mediumvioletred'}]\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300, h=600)","0093f948":"# Calculates compensation per role\nbarplot = personal_data[not_student_mask].groupby(['most_used_prog_lang'], as_index=False)['yearly_compensation_numerical'].mean()\n\n# Flags the top 5 roles to add color\n\nbarplot['role_highlight'] = (barplot.most_used_prog_lang == 'Scala') \n\nbarplot.columns = ['y', 'x','role_highlight']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.role_highlight.apply(lambda x: 'mediumvioletred' if x else 'lightgray')\n\ntitle_text = ['<b>Most Used Programming Language<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (K USD)', 'Language']\nannotations = [{'x': 0.8, 'y': 11.5, 'text': 'Scalla, Go, <br> Aduh saya ga tahu','color': 'mediumvioletred'}]\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300, h=600)\n","e97303d7":"# Replacing long country names\npersonal_data.country = personal_data.country.str.replace('United Kingdom of Great Britain and Northern Ireland', 'United Kingdom')\npersonal_data.country = personal_data.country.str.replace('United States of America', 'United States')\npersonal_data.country = personal_data.country.str.replace('I do not wish to disclose my location', 'Not Disclosed')\npersonal_data.country = personal_data.country.str.replace('Iran, Islamic Republic of...', 'Iran')\npersonal_data.country = personal_data.country.str.replace('Hong Kong \\(S.A.R.\\)', 'Hong Kong')\npersonal_data.country = personal_data.country.str.replace('Viet Nam', 'Vietnam')\npersonal_data.country = personal_data.country.str.replace('Republic of Korea', 'South Korea')\n\n# Calculates compensation per country\nbarplot = personal_data[not_student_mask].groupby(['country'], as_index=False)['yearly_compensation_numerical'].mean()\n\n# Flags the top 10 countries to add color\nbarplot['country_highlight'] = (barplot.country == 'United States') | \\\n                               (barplot.country == 'Switzerland') | \\\n                               (barplot.country == 'Australia') | \\\n                               (barplot.country == 'Israel') | \\\n                               (barplot.country == 'Denmark') | \\\n                               (barplot.country == 'Canada') | \\\n                               (barplot.country == 'Hong Kong') | \\\n                               (barplot.country == 'Norway') | \\\n                               (barplot.country == 'Ireland') | \\\n                               (barplot.country == 'United Kingdom')\n\nbarplot.columns = ['y', 'x', 'country_highlight']\nbarplot = barplot.sort_values(by='x', ascending=True)\nbarplot['color'] = barplot.country_highlight.apply(lambda x: 'mediumseagreen' if x else 'lightgray')\n\ntitle_text = ['<b>Average Compensation per Country - Top 10 in Color<\/b><br><i>without students<\/i>', 'Average Yearly Compensation (USD)', 'Country']\nannotations = []\n\ngenerate_barplot(title_text, annotations, orient='h', lmarg=300, h=1200)","ffb1d3a3":"personal_data.head(5)\n","23a370b1":"model_data = personal_data.drop(['yearly_compensation','yearly_compensation_numerical'], axis=1)\nmodel_data.head(5)","50295b6f":"model_data.shape","aa1e4e48":"# All OHE\ncategorical_features= [\n    'gender', \n    'age',\n    'country', \n    'education_level', \n    'undergrad_major', \n    'role', \n    'employer_industry', \n    'years_experience', \n    'primary_analize_tool', \n    'most_used_prog_lang', \n    'most_reccom_prog_lang', \n    'most_used_ml_lib', \n    'most_used_vis_lib', \n    'coding_time', \n    'coding_time_years', \n    'ml_exp_years', \n    'are_you_data_scientist',\n]\nfor whichColumn in categorical_features:\n    dummy = pd.get_dummies(model_data[whichColumn].astype('category'))\n    columns = dummy.columns.astype(str).tolist()\n    columns = [whichColumn  + '_' + w for w in columns]\n    dummy.columns = columns\n    model_data = pd.concat((model_data, dummy), axis=1)\n    model_data = model_data.drop([whichColumn], axis=1)\n    \nmodel_data.shape","224de3db":"model_data.head(5)","4eebc0a8":"from sklearn.model_selection import train_test_split\nmodel_data = model_data.reset_index(drop=True)\nmodel_data.survey_duration = model_data.survey_duration.astype(int)\nlabel  = np.array(model_data['top20'].values.astype(int))\nmodel_data = model_data.drop(['top20'], axis=1)\n\n#Main and Test Split\ntrain_index, test_index = train_test_split(model_data.index.values, shuffle=True, test_size=0.3)\nmain_data = model_data.iloc[train_index].reset_index(drop=True)\nmain_label = label[train_index]\ntest_data  = model_data.iloc[test_index].reset_index(drop=True)\ntest_label = label[test_index]\n\n#Train and Validation Split\ntrain_index, valid_index = train_test_split(main_data.index.values, shuffle=True, test_size=0.3)\ntrain_data = main_data.iloc[train_index].reset_index(drop=True)\ntrain_label = main_label[train_index]\nvalid_data  = main_data.iloc[valid_index].reset_index(drop=True)\nvalid_label = main_label[valid_index]\n\nprint(\"Train Data Shape {}\".format(train_data.shape))\nprint(\"Valid Data Shape {}\".format(valid_data.shape))\nprint(\"Test Data Shape {}\".format(test_data.shape))","3b4fb3e0":"#Simple Model by Logistic Regression\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\nnp.random.seed(1)\nrn.seed(1)\n\nclf = LogisticRegression(random_state=0).fit(train_data, train_label)\n\nprint(\"Train Data Accuracy {}\".format(clf.score(train_data, train_label)))\nprint(\"Valid Data Accuracy {}\".format(clf.score(valid_data, valid_label)))\nprint(\"Test Data Accuracy {}\".format(clf.score(test_data, test_label)))","11f6bc4f":"#Complex Model by Xgboost\nimport xgboost as xgb\nfrom sklearn.metrics import accuracy_score\nnp.random.seed(1)\nrn.seed(1)\n\nparams = {\n    #'gamma':2,\n    'learning_rate': 0.1, \n    #'eval_metric':'auc',\n    'nthread':4, \n    #'max_depth': 18, \n    'booster': 'gbtree', \n    'tree_method':'exact', \n    'objective':'binary:logistic', \n    'seed': 0, \n    'subsample': 0.8, \n    'colsample_bytree': 0.8, \n    'colsample_bylevel':1,\n    'alpha':0, \n    'lambda':0,\n    'silent':1,\n    'disable_default_eval_metric':1\n}\n\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()        \n    predsTres = [1 if a >= 0.5 else 0 for a in preds]\n    return 'Accuracy', accuracy_score(predsTres,  labels)\n\nd_train = xgb.DMatrix(train_data.values, label=train_label)\nd_valid = xgb.DMatrix(valid_data.values, label=valid_label)\nd_test = xgb.DMatrix(test_data.values, label=test_label)\nwatchlist = [(d_train,'Train'), (d_valid,'Valid')]\n\nXGB = xgb.train(params, d_train , 100, watchlist, verbose_eval=10, feval=evalerror)        \nresultTrain  = [1 if a >= 0.5 else 0 for a in XGB.predict(d_train)]\nresultValid  = [1 if a >= 0.5 else 0 for a in XGB.predict(d_valid)]\nresultTest   = [1 if a >= 0.5 else 0 for a in XGB.predict(d_test)]","ecb49b6e":"print(\"Train Data Accuracy {}\".format(accuracy_score(resultTrain,  train_label)))\nprint(\"Valid Data Accuracy {}\".format(accuracy_score(resultValid, valid_label)))\nprint(\"Test Data Accuracy {}\".format(accuracy_score(resultTest, test_label)))","b6ff8582":"### Should you get formal education?\nSeberapa penting sih sekolah tinggi tinggi dalam dunia data science in terms of seberapa tinggi gaji pada umumnya","8f892a8f":"### Which industry should you target?\nIndustri apa saja dalam penerapan data science yang berani membayar mahal?","289a7473":"Kita hapus dulu variable yang jelas jelas merupakan target. Tidak valid model untuk memprediksi gaji tapi meminta inputan gaji kan?","6a57156a":"Untuk mengetahui summary dari data yang kita miliki jalankan :","4364a231":"Ternyata ada data NaN, coba kita cek Nan untuk semua tipe variable, run:","ba25ed87":"# Pelatihan Data Science Workshop PPI Hsinchu  \n\nKernel Asli : https:\/\/www.kaggle.com\/andresionek\/what-makes-a-kaggler-valuable\/notebook?utm_medium=social&utm_source=twitter.com&utm_campaign=Weekly-Kernel-Awards\n\nSelamat Datang di python jupiter notebook! Kaggle menyediakan komputasi online gratis dimana setiap sesi memiliki batasan:\nRun time : 6 Hours\nMemory : 16 GB\nEnvirontment : https:\/\/github.com\/Kaggle\/docker-python\/blob\/master\/Dockerfile\n\nDalam sesi komputasi ini kita tidak perlu melakukan instalasi apapun lagi karena hampir semua libray penting sudah di sediakan dalam environtmentnya. Untuk tahu lebih jelas apa yang sudah terinstall bisa akses link environtment diatas. Kita juga diberi batasan hanya diperbolehkan menjalankan komputasi sebanyak maksimal 25 script saja dalam waktu yang bersamaan. \nIni bisa dimanfaatkan untuk mencoba berbagai macam ide nantinya kalau kita ingin mencoba ide mana yang berhasil dan tidak dalam iterasi model matematis. \n\n## Survey Komunitas Kaggle 2018  \n\nSeperti yang sudah dijelaskan sebelumnya kita tidak akan bermain langsung dengan data finhacks dikarenakan saya tidak diperbolehkan untuk menyebarluaskan data dan model dalam bentuk apapun, oleh karena itu kita akan mencoba menerapkan dan berlatih teknik teknik dasar yang saya ketahui dengan menggunakan dataset hasil survey komunitas kaggle 2018 ini.\n\nSurvey ini di isi oleh 23,859 responden yang pengisi survey diberi pertanyaan seputar kegiatan dan posisi mereka sehari hari yang memiliki keterkaitan dengan datascience. Seperti framework apa yang mereka gunakan dan lain sebagainya. \n\nSebelum memulai ada baiknya untuk mengamati sejenak semua elemen yang ada di komputer ini.\n\n## Memuat Data  \n\nLangkah pertama dalam setiap analisis data adalah memuat datanya sendiri, Disini kita menggunakan \"pandas\" sebagai data manipulator. Jalankan cell berikut untuk load data ke dalam environtment.\n","f9b180f7":"# Model Building\n\nSekarang mari kita coba membuat model matematis untuk memprediksi kemungkinan seseorang termasuk top 20% dari segi gaji.\nKita review lagi data yang kita punya:","2371ed3c":"## Objective\n\nKita hendak membangun model dimana dengan memasukkan latar belakang ilmu data science seseorang kita bisa menebak berapa persen kemungkinannya menerima gaji sebesar 100.000 keatas.\n\n## Data Cleaning\n\n### Missing value\n\nSudah kita amati bersama banyak kolom \/ variabel yang mengandung data NaN, langkah paling simple adalah mendelete data yang mengandung NaN. Tapi tidak selalu menghapus data dengan NaN itu merupakan langkah yang bagus. Di sini coba kita hapus data yang mengandung NaN tapi hanya untuk kolom Gaji. Karena kolom ini yang akan kita gunakan sebagai target latihan.\n","3ebbe424":"Untuk melihat seperti apa data sesungguhnya silahkan run:","fc0288e7":"---\n# Exploratory Data Analysis \n\nSekarang mari kita coba plot informasi informasi yang ada didalam dataset ini. Kernel asli sudah menyediakan banyak sekali bantuan untuk mempermudah kita dalam membuat grafik yang cantik. Fungsi fungsi yang disediakan menggunakan plotly sebagai library untuk plotting.\n\n### Finding the Top 20% most well paid","b86bd511":"Saatnya kita pecah data menjadi 3. Train Data, Validation Data dan Test Data","c0a1b260":"## Building a model\n\nada banyak macam tipe model. Kita akan mencoba yang simple vs kompleks. \nLogistic Regresion Vs Gradient Boosted Macine\n","4f948f34":"Mari kita perhatikan seksama bentuk data kita setelah one hot encoding","434794a4":"### Should You Aim at the C-level?\nCFO, CEO, dan C C lainnya seberapa besar gaji mereka di compare dengan pekerja bawahan seperti saya?","316a9886":"### Which countries pay more?\nNegara mana yang paling bersahabat dengan data scientist? (Gajinya gede maksudnya)","45dbe589":"## One Hot Encoding\n\nNah dari hasil eksplorasi tadi jelas terlihat kalau hampir semua data merupakan data kategori. Kebanyakan model tidak dapat memproses variabel dalam bentuk kategori. Salah satu bentuk feature engineering yang paling umum adalah mengkonversinya kedalam bentuk biner 1 dan 0","7ccc22a2":"Bagaimana dengan pelajar? Dimana posisi mereka?","40dc4557":"## Simplfikasi Data \n\nUntuk mempermudah proses analisa sebagian besar data kita drop saja agar pelatihan bisa lebih simple dan dapat dengan mudah dipahami.","94c41e2b":"Train set error mengecil tapi kenapa valid set error membesar? Ada apa?"}}