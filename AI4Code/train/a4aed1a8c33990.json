{"cell_type":{"ea2449d4":"code","aba34f38":"code","40a62abb":"code","4b4b7e7b":"code","e3b5cf43":"code","4ff87feb":"code","08ffcecc":"code","b3fa3f20":"code","e3701a74":"code","2b623459":"code","6fcd5dff":"code","0b19422c":"code","d1e299a7":"code","1baff5d7":"code","e6813c91":"code","f8e11b30":"code","01ba2c9a":"code","56b03969":"code","a182c540":"code","6aece507":"code","4c6e0937":"code","8e3e8ebe":"code","77172fd7":"code","f00c8657":"code","dc4ad779":"code","3ec238d4":"code","a524abec":"code","4a25d4d6":"code","ce7f8697":"code","e5cb0c3a":"code","438673ee":"code","2cbb5a53":"code","773e7111":"code","09033548":"markdown","a3f3ba57":"markdown","3a16d992":"markdown","e947b341":"markdown","881dfa83":"markdown","a2986235":"markdown","a0490c9b":"markdown","8ed7fd08":"markdown","24e22784":"markdown","03788857":"markdown","db9f7113":"markdown","bad9bb88":"markdown"},"source":{"ea2449d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aba34f38":"import seaborn as sb\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport sklearn.metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom scipy import interp\nfrom itertools import cycle\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom yellowbrick.classifier import ClassificationReport\nfrom yellowbrick.classifier import ROCAUC\nfrom yellowbrick.classifier import DiscriminationThreshold\nfrom yellowbrick.classifier import PrecisionRecallCurve\nimport warnings","40a62abb":"warnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)","4b4b7e7b":"data = pd.read_csv('\/kaggle\/input\/fetal-health-classification\/fetal_health.csv')\ndata.head()","e3b5cf43":"data.info()","4ff87feb":"#rename columns\ndata.columns = ['FHR', 'AC', 'FM', 'UC', 'DL', 'DS', 'DP', 'ASTV', 'MSTV',\n               'ALTV', 'MLTV', 'Width', 'Min', 'Max', 'NMax', 'Nzeros', \n                'Mode', 'Mean', 'Median', 'Variance', 'Tendency', 'Class']","08ffcecc":"data.columns","b3fa3f20":"#convert target class to Int\n\ndata.Class = data.Class.astype('int')","e3701a74":"label_encoder = preprocessing.LabelEncoder()\n\ny_en = label_encoder.fit_transform(data.Class)","2b623459":"data = data.drop(['Class'], axis = 1)\ndata.head()","6fcd5dff":"#0-Normal\n#1-Suspect\n#2-Pathologic\n\ny_en","0b19422c":"#AC - # of accelerations per second\n#FM - # of fetal movements per second\n#UC - # of uterine contractions per second\n#DL - # of light decelerations per second\n#DS - # of severe decelerations per second\n#DP - # of prolongued decelerations per second\n\n#All the variables are measured per second and has very low values, this cause scaling issues in our models. \n#So we convert them into per min, as FHR is also  measured per min.\n\nclms = ['AC', 'FM', 'UC', 'DL', 'DS', 'DP']\n\nfor column in clms:\n    data[column] = data[column]*60\n    \ndata.head(10)","d1e299a7":"plt.figure(figsize=[15,10])\nx=data.corr()\nsb.heatmap(x,annot=True)","1baff5d7":"plt.figure(figsize = [10, 8])\nsb.distplot(data['FHR'])","e6813c91":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.FHR, palette=\"deep\")","f8e11b30":"sb.pairplot(data[['AC', 'UC', 'DL', 'FM', 'DS', 'DP']])","01ba2c9a":"def zero_table(df):\n    for column in df.columns:\n        zero_count = (df[column] == 0).sum()\n        if zero_count != 0:\n            zero_percentage = 100*zero_count\/len(df[column])\n            if zero_percentage > 60:\n                print(\"%s has %s Zeros\" % (column, zero_count))\n                print(\"Percentage of Zeros %0.1f%%\" % (zero_percentage))\n                print(\"-\"*25)","56b03969":"zero_table(data)","a182c540":"sb.pairplot(data[['AC', 'UC', 'DL']])","6aece507":"plt.figure(figsize=[15,8])\nsb.scatterplot(x = data['ASTV'], y = data['MSTV'],\n              hue = y_en, palette=\"deep\")","4c6e0937":"plt.figure(figsize=[15,8])\nsb.scatterplot(x = data['ALTV'], y = data['MLTV'],\n              hue = y_en, palette=\"deep\")","8e3e8ebe":"plt.figure(figsize = [10, 8])\nsb.distplot(data['Variance'])","77172fd7":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.Variance, palette=\"deep\")","f00c8657":"plt.figure(figsize = [10, 8])\nsb.distplot(data['Width'])","dc4ad779":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.Width, palette=\"deep\")","3ec238d4":"plt.figure(figsize = [10, 8])\nsb.distplot(data['Max'])","a524abec":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.Max, palette=\"deep\")","4a25d4d6":"plt.figure(figsize=[15,8])\nsb.violinplot(x=y_en, y=data.NMax, palette=\"deep\")","ce7f8697":"plt.figure(figsize=[15,8])\nsb.scatterplot(x=data.FHR, y=data.Variance, hue=y_en, palette=\"deep\")","e5cb0c3a":"def classifier_results(x, y):\n    \n    x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.3)\n\n    classifiers = {\n        'L1 logistic': LogisticRegression(penalty = 'l2', solver = 'saga', \n                            multi_class = 'multinomial', max_iter = 10000),\n        'L2 logistic (Multinomial)': LogisticRegression(penalty = 'l1', solver = 'saga', \n                             multi_class = 'multinomial', max_iter = 10000),\n        'L2 logistic (OvR)': LogisticRegression(penalty='l2', solver='saga',\n                       multi_class='ovr', max_iter=10000),\n        'Linear SVC': SVC(kernel='linear', probability=True),\n\n    }\n    \n    class_names = ['Normal', 'Suspect', 'Pathologic']\n\n    for index, (name, classifier) in enumerate(classifiers.items()):\n        classifier.fit(x_train, y_train)\n        y_pred = classifier.predict(x_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        print(\"Accuracy (test) for %s: %0.1f%% \" % (name, accuracy * 100))\n        print('-'*40)\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        visualizer = ClassificationReport(classifier, classes=class_names, support=True, ax=ax)\n        visualizer.fit(x_train, y_train)       \n        visualizer.score(x_test, y_test)       \n        visualizer.show()\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        cm = ConfusionMatrix(classifier, classes = class_names, ax=ax)\n        cm.fit(x_train, y_train)\n        cm.score(x_test, y_test)\n        cm.show()\n        \n        y_lb = label_binarize(y_en, classes=[0, 1, 2])\n        n_classes = y_lb.shape[1]\n        \n        x2_train,x2_test,y2_train,y2_test = train_test_split(data, y_lb, test_size=0.3)\n\n        estimator = OneVsRestClassifier(classifier)\n        y2_dist = estimator.fit(x2_train, y2_train).decision_function(x2_test)\n        y2_pred = estimator.predict(x2_test)\n        \n        fpr = dict()\n        tpr = dict()\n        roc_auc = dict()\n\n        # Compute ROC curve and ROC area for each class\n        for i in range(n_classes):\n            fpr[i], tpr[i], _ = roc_curve(y2_test[:, i], y2_dist[:, i])\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        # Compute micro-average ROC curve and ROC area\n        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y2_test.ravel(), y2_dist.ravel())\n        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n        \n        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n        # Then interpolate all ROC curves at this points\n        mean_tpr = np.zeros_like(all_fpr)\n        for i in range(n_classes):\n            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n        # Finally average it and compute AUC\n        mean_tpr \/= n_classes\n\n        fpr[\"macro\"] = all_fpr\n        tpr[\"macro\"] = mean_tpr\n        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n        # Plot all ROC curves\n        plt.figure(figsize=[15,7])\n        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n                 label='micro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"micro\"]),\n                 color='deeppink', linestyle=':', linewidth=4)\n\n        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n                 label='macro-average ROC curve (area = {0:0.2f})'\n                       ''.format(roc_auc[\"macro\"]),\n                 color='navy', linestyle=':', linewidth=4)\n\n        colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n        for i, color in zip(range(n_classes), colors):\n            plt.plot(fpr[i], tpr[i], color=color, lw=2,\n                     label='ROC curve of class {0} (area = {1:0.2f})'\n                     ''.format(i+1, roc_auc[i]))\n\n        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n        plt.xlim([0.0, 1.0])\n        plt.ylim([0.0, 1.05])\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.title('Some extension of Receiver operating characteristic to multi-class')\n        plt.legend(loc=\"lower right\")\n        plt.show()","438673ee":"classifier_results(data, y_en)","2cbb5a53":"def RF_AdB_GNB_classifier_results(x, y):\n    \n    x_train,x_test,y_train,y_test = train_test_split(x, y, test_size=0.3)\n\n    classifiers = {\n        'RandomForest': RandomForestClassifier(),\n        'AdaBoost': AdaBoostClassifier(),\n        'GaussianNB': GaussianNB()\n\n    }\n    \n    class_names = ['Normal', 'Suspect', 'Pathologic']\n\n    for index, (name, classifier) in enumerate(classifiers.items()):\n        classifier.fit(x_train, y_train)\n        y_pred = classifier.predict(x_test)\n        accuracy = accuracy_score(y_test, y_pred)\n        print(\"Accuracy (test) for %s: %0.1f%% \" % (name, accuracy * 100))\n        print('-'*40)\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        visualizer = ClassificationReport(classifier, classes=class_names, support=True, ax=ax)\n        visualizer.fit(x_train, y_train)        # Fit the visualizer and the model\n        visualizer.score(x_test, y_test)        # Evaluate the model on the test data\n        visualizer.show()\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        cm = ConfusionMatrix(classifier, classes = class_names, ax=ax)\n        cm.fit(x_train, y_train)\n        cm.score(x_test, y_test)\n        cm.show()\n\n        fig, ax = plt.subplots(figsize=(12, 7))\n        roc = ROCAUC(classifier, classes=class_names, ax=ax)\n        roc.fit(x_train, y_train)        # Fit the training data to the visualizer\n        roc.score(x_test, y_test)        # Evaluate the model on the test data\n        roc.show()\n        \n        fig, ax = plt.subplots(figsize=(12, 7))\n        prc = PrecisionRecallCurve(classifier,\n                                   classes=class_names,\n                                   colors=[\"purple\", \"cyan\", \"blue\"],\n                                   iso_f1_curves=True,\n                                   per_class=True,\n                                   micro=False, ax=ax)\n        prc.fit(x_train, y_train)\n        prc.score(x_test, y_test)\n        prc.show()","773e7111":"RF_AdB_GNB_classifier_results(data, y_en)","09033548":"# **Visualization**","a3f3ba57":"* distplots are great to know about the distribution of variable. \n* But if we have a classification prob with multiple classes and we would like to know about the distribution of single variable for different class violinplots helps alot.","3a16d992":"For better understanding of variables please refer to [this link](http:\/\/oacapps.med.jhmi.edu\/OBGYN-101\/Text\/Labor%20and%20Delivery\/electronic_fetal_heart_monitorin.htm)\n\n* LB - FHR baseline (beats per minute)\n* AC - # of accelerations per second\n* FM - # of fetal movements per second\n* UC - # of uterine contractions per second\n* DL - # of light decelerations per second\n* DS - # of severe decelerations per second\n* DP - # of prolongued decelerations per second\n* ASTV - percentage of time with abnormal short term variability\n* MSTV - mean value of short term variability\n* ALTV - percentage of time with abnormal long term variability\n* MLTV - mean value of long term variability\n* Width - width of FHR histogram\n* Min - minimum of FHR histogram\n* Max - Maximum of FHR histogram\n* Nmax - # of histogram peaks\n* Nzeros - # of histogram zeros\n* Mode - histogram mode\n* Mean - histogram mean\n* Median - histogram median\n* Variance - histogram variance\n* Tendency - histogram tendency\n\n* Class - fetal state class code (N= Normal ; S= Suspect ; P= Pathologic )","e947b341":"* We can see that the most Pathologic cases are found at mean FHR and medium to high variance values.","881dfa83":"**Following variables has maximum correlations.**\n* Min and width has -0.9 \n* Min and NMax has -0.67 \n* Min and MSTV has -0.62  \n* Mode and Mean has 0.89 \n* Mode and Median has 0.93 \n* Mean and Median has 0.95","a2986235":"## **Please Upvote if you find this notebook useful**","a0490c9b":"## ***Observed that Random Forest Classifer did best of all the classifiers.***","8ed7fd08":"* We can see that Normal clases have variance values concentrated more at Zero and slightly distrubuted for low values\n* For Suspect cases the variance concentrated more at Zero and very low values at small varince values.\n* For Pathologic case, the variance values are almost equally distributed among all the values.","24e22784":"# Model Fitting","03788857":"Few graphs in plot are very sparse, becouse they have more Zero's. ","db9f7113":"* We can observe that Suspect and Pathologic cases are increasing as ALTV increase.","bad9bb88":"* We can observe that Suspect and Pathologic cases are increasing as ASTV increase.\n* Suspect and Pathologic cases are increasing as MSTV increasing by keeping ASTV constant."}}