{"cell_type":{"0310d1d4":"code","e77784ea":"code","3e4c24a6":"code","6c8c75b6":"code","df3b915d":"code","d1bca6a0":"code","550a6459":"code","95947934":"code","fb425f8a":"code","744dde69":"code","06e0dc5b":"code","b2357dd5":"code","59ce4c64":"code","a236d552":"code","1f153ec1":"code","0d773daa":"code","b3f8092f":"code","ffbc38c9":"code","575a63cf":"code","d17691e7":"code","61958c59":"code","e366f70f":"code","f6dd08df":"code","bffb2c9f":"code","9ec88f3f":"code","6733d77c":"code","96bab21c":"code","1eb25e5d":"markdown","4485a7f7":"markdown","32d4085c":"markdown","f477abea":"markdown","fe2119ea":"markdown","8ced92b1":"markdown","5cb0e4f0":"markdown","8a8b95ea":"markdown","c9eefda3":"markdown","cd390a2d":"markdown","3a731de6":"markdown"},"source":{"0310d1d4":"!pip install timm effdet","e77784ea":"import os, sys\nimport glob\nimport pickle\nfrom collections import OrderedDict, namedtuple, deque\nfrom copy import deepcopy\nimport copy\nimport colorsys\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport h5py\nfrom tqdm import tqdm\n\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nfrom PIL import Image\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n\nimport torch\nfrom torch import nn, optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n\n\n\nimport torch\nimport torchvision\nimport torch.nn.functional as F\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torchvision.models.detection.backbone_utils import BackboneWithFPN\n\n\nfrom torch import nn, optim\nimport torch.optim.lr_scheduler as lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n# sys.path.append('.\/yolov5')\n# from utils.loss import ComputeLoss\n# from utils.general import non_max_suppression\n\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain, DetBenchPredict\n","3e4c24a6":"# # # # # # # # # # # # # # # # # # # # # # # #\n# # # # # # #  Global Configs   # # # # # # # # \n# # # # # # # # # # # # # # # # # # # # # # # #\n\nGLOBAL_N_WARMUP_EPOCHS = 5\nGLOBAL_N_DECAY_EPOCHS = 60\nGLOBAL_N_EPOCHS = GLOBAL_N_WARMUP_EPOCHS + GLOBAL_N_DECAY_EPOCHS + 15 + 50\n\nGLOBAL_CONTINUE_TRAINING = False\n\nGLOBAL_LR = 1e-4\n\nGLOBAL_GRAD_STEPS = 5\nGLOBAL_BATCH_SIZE = 3 #4\nGLOBAL_N_WORKERS = 5\nGLOBAL_DEVICE = None # uses cuda:0 if it is available\n\nGLOBAL_I_FOLD = 0\nGLOBAL_CLS = None\n\nGLOBAL_MODEL_RESOLUTION = (1024, 768) \n\n\nEVAL_CKPTS = True # True=Inference,  False=Training\n\nUSE_NIH = False\nDOWNSAMPLE_FACTOR = 2\n\nCKPTS_v = [\n    '..\/input\/effdet-d2-v2-ckpts\/F0_E62_ModelX_V19_T0.344_V0.381.ckpt', \n    '..\/input\/effdet-d2-v2-ckpts\/F0_E74_ModelX_V19_T0.348_V0.381.ckpt',\n    '..\/input\/effdet-d2-v2-ckpts\/F0_E82_ModelX_V19_T0.361_V0.383.ckpt',\n]\n\n# # # # # # # # # # # # # # # # # # # # # # # #\n# # # # # # # # # # # # # # # # # # # # # # # #","6c8c75b6":"TRAIN_DS_NAME = 'train_nih' if USE_NIH else 'train'\n\n\n# Data directories list\nDS_DIR_v = [\n    \"..\/input\/vinbigdata-chest-xray-abnormalities-detection\"]\n\nfor DS_DIR in DS_DIR_v:\n    if os.path.exists(DS_DIR):\n        print(f' DS_DIR Found: \"{DS_DIR}\"')\n        break\nelse:\n    raise Exception(' Dataset not found.')\n    \n    \n\n\nDS_PATH = '..\/input\/train-test-ds-bbox-cache'","df3b915d":"class2str_v = [\n    'Aortic enlargement',\n    'Atelectasis',\n    'Calcification',\n    'Cardiomegaly',\n    'Consolidation',\n    'ILD',\n    'Infiltration',\n    'Lung Opacity',\n    'Nodule\/Mass',\n    'Other lesion',\n    'Pleural effusion',\n    'Pleural thickening',\n    'Pneumothorax',\n    'Pulmonary fibrosis',\n    'No Finding',\n]\n\n\nclass2color_v = [\n    tuple(round(i * 255) for i in colorsys.hsv_to_rgb(i_c\/len(class2str_v), 1, 1))\n    for i_c in range(len(class2str_v))\n]","d1bca6a0":"class FastSMA():\n    def __init__(\n        self,\n        iterator=None,\n        maxlen=1000,\n        label='mean = ',\n        print_format='0.02f',\n        save_filename='loss_trn.fsma'):\n        \n        \n        assert type(maxlen) == int and maxlen > 0, 'ERROR length must be a positive int.'\n        self.maxlen = maxlen\n        self.label = label\n        self.print_format = print_format\n        self.save_filename = save_filename\n        self.clear()\n\n        if iterator is not None:\n            if type(next(iter(iterator))) in [list, tuple, np.ndarray]:\n                for i in iterator:\n                    self.append(*i)\n            else:\n                for i in iterator:\n                    self.append(i)\n            \n        return None\n\n    def append(self, v, i_step=None):        \n        self.cumsum += v\n        \n        if i_step is None:\n            i_step = self.step_hist_v[-1] + 1 if len(self.step_hist_v)  else 0\n        \n        if len(self.history) == self.maxlen:\n            self.cumsum -= self.history[0]\n            self.last_mean = self.cumsum \/ self.maxlen\n        else:\n            self.last_mean = self.cumsum \/ (len(self.history) + 1) \n\n        self.history.append( v )\n\n        self.sma_hist_v.append(self.last_mean)\n        self.val_hist_v.append(v)\n        self.step_hist_v.append( i_step )\n            \n        return None\n\n    def mean(self):\n        return self.last_mean\n\n    def clear(self):\n        self.history = deque(maxlen=self.maxlen)\n        self.cumsum = 0.0\n        self.last_mean = 0.0\n        self.sma_hist_v = []\n        self.val_hist_v = []\n        self.step_hist_v = []\n        \n        \n        self.key2save_v = [\n            'history',\n            'cumsum',\n            'last_mean',\n            'sma_hist_v',\n            'val_hist_v',\n            'step_hist_v',\n        ]\n        return None\n        \n    def get_sma_history(self):\n        return np.array(self.step_hist_v), np.array(self.sma_hist_v)\n    \n    def get_val_history(self):\n        return np.array(self.step_hist_v), np.array(self.val_hist_v)\n\n    def __str__(self):\n        return ('{}{:'+self.print_format+'}').format(self.label, self.mean())\n\n    def __repr__(self):\n        return self.__str__()\n    \n    \n    def save(self, filename=None):\n        if filename is None:\n            filename = self.save_filename\n            \n        to_save_d = {}\n        for key2save in self.key2save_v:\n            to_save_d[key2save] = deepcopy( getattr(self, key2save) )\n        \n        with open(filename, 'wb') as f:\n            f.write( pickle.dumps(to_save_d) )\n                \n        print(f' Saved complete: \"{filename}\"')\n            \n        return None\n\n    def load(self, filename=None):\n        if filename is None:\n            filename = self.save_filename\n            \n        with open(filename, 'rb') as f:\n            data = f.read()\n            \n        to_save_d = pickle.loads(data)\n        \n        \n        for k, v in to_save_d.items():\n            if k in self.key2save_v:\n                setattr(self, k, v)\n\n\n        self.maxlen = self.history.maxlen\n        print(f' Restored: \"{filename}\"')\n        \n        return self\n    \n    \n    def plot(self, label=None, do_show=False):\n        x, y = self.get_sma_history()\n        plt.plot(x, y, label=label)\n        plt.grid()\n        \n        if do_show:\n            plt.show()\n        \n        return None\n\n\n    def plot_sma(self, label='', step=500):\n        if label == '':\n            label = os.path.split( self.save_filename )[-1].replace('.fsma', '')\n            \n        step_v, loss_v = self.get_val_history()\n        \n        m = loss_v.shape[0] % step\n        loss_v = loss_v[m:].reshape(-1, step).mean(axis=-1)\n        step_v = step_v[m:].reshape(-1, step)[:,-1]\n\n        plt.plot(step_v, loss_v, label=label)\n\n        return None","550a6459":"def batch_merge(samples_v):\n    ret_batch_d = {k : [] for k in samples_v[0].keys()}\n    \n    for sample_d in samples_v:\n        for k, v in sample_d.items():\n            ret_batch_d[k].append( v )\n        \n    ret_batch_d['image'] = np.stack(ret_batch_d['image'])\n    \n#     for sample_d in samples_v:\n#         for k, v in sample_d.items():\n#             if type(v) == np.ndarray:\n#                 ret_batch_d[k].append( torch.from_numpy(v) ) \n#             else:\n#                 ret_batch_d[k].append( v )\n                \n#     ret_batch_d['image'] = torch.stack(ret_batch_d['image'])\n    \n    return ret_batch_d\n    \n\n\n\ndef data2tensor(data, device=None, pin_memory=False):\n    data['image'] = torch.tensor(data['image'], device=device, pin_memory=pin_memory)\n    \n    for k in data.keys():\n        if type(data[k][0]) is np.ndarray:\n            for i_s in range(len(data['image'])):\n                data[k][i_s] = torch.tensor( data[k][i_s], device=device, pin_memory=pin_memory)\n                \n    return data\n    \n    \n\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # \n    \n\ndef calc_iou(bb0, bb1):\n    if (len(bb0.shape) == 2):\n        bb0 = bb0.T\n        \n    if (len(bb1.shape) == 2):\n        bb1 = bb1.T\n        \n\n    bb0_x0, bb0_y0, bb0_x1, bb0_y1 = bb0\n    bb1_x0, bb1_y0, bb1_x1, bb1_y1 = bb1\n    \n    assert (bb0_x0 < bb0_x1).all()\n    assert (bb0_y0 < bb0_y1).all()\n    assert (bb1_x0 < bb1_x1).all()\n    assert (bb1_y0 < bb1_y1).all()\n\n    # determine the coordinates of the intersection rectangle\n    x_left   = np.maximum(bb0_x0, bb1_x0)\n    y_top    = np.maximum(bb0_y0, bb1_y0)\n    x_right  = np.minimum(bb0_x1, bb1_x1)\n    y_bottom = np.minimum(bb0_y1, bb1_y1)\n\n#     if (x_right < x_left).all(axis=0) or (y_bottom < y_top).all(axis=0):\n#         return np.zeros( out_dim )\n    \n    ret_mask = ~( (x_right < x_left) + (y_bottom < y_top) )\n\n    # The intersection of two axis-aligned bounding boxes is always an\n    # axis-aligned bounding box\n    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n\n    # compute the area of both AABBs\n    bb0_area = (bb0_x1 - bb0_x0) * (bb0_y1 - bb0_y0)\n    bb1_area = (bb1_x1 - bb1_x0) * (bb1_y1 - bb1_y0)\n    \n    iou = intersection_area \/ (bb0_area + bb1_area - intersection_area)\n    \n    \n    return iou * ret_mask\n\n\ndef join_preds(bbox_v, p_det_v=None, mode='p_det_weight'):\n    \n    if p_det_v is None:\n        p_det_v = np.ones(bbox_v.shape[0])\n        \n    if mode == 'p_det_weight' or mode == 'p_det_weight_pmean':\n        typed_p_det_v = p_det_v.astype(bbox_v.dtype)\n        p_v = ( typed_p_det_v \/ typed_p_det_v.sum() )[:,None]\n        \n        bbox = (bbox_v * p_v).sum(axis=0)\n        p = p_det_v.mean()\n    \n    elif mode == 'p_det_weight_psum':\n        typed_p_det_v = p_det_v.astype(bbox_v.dtype)\n        p_v = ( typed_p_det_v \/ typed_p_det_v.sum() )[:,None]\n        \n        bbox = (bbox_v * p_v).sum(axis=0)\n        p = p_det_v.sum()\n    \n    elif mode == 'median' or mode == 'median_pmean':\n        bbox = np.median(bbox_v, axis=0)\n        p = p_det_v.mean()\n\n    elif mode == 'p_det_max':\n        i_max = p_det_v.argmax()\n        \n        bbox = bbox_v[i_max]\n        p    = p_det_v[i_max]\n    \n    elif mode == 'random':\n        i_max = np.random.randint(0, p_det_v.shape[0])\n        \n        bbox = bbox_v[i_max]\n        p    = p_det_v[i_max]\n        \n    else:\n        raise Exception(f'Unknown mode \"{mode}\"')\n        \n        \n    return bbox, p\n    \n\ndef clean_predictions(preds_v, iou_th=0.1, mode='p_det_weight', consensus_level=1, n_models2ensemble=1):\n    ret_preds_v = []\n    for pred_d in preds_v:\n        \n        cls_v = pred_d['cls']\n        \n        if 'bbox' in pred_d.keys():\n            bbox_key = 'bbox'\n        else:\n            bbox_key = 'bboxes'\n            \n        bbox_v = pred_d[bbox_key]\n        \n        if 'p_det' in pred_d.keys():\n            ret_p_det = True\n            p_det_v = pred_d['p_det']\n        else:\n            ret_p_det = False\n            p_det_v = np.ones(pred_d['cls'].shape)\n        \n        \n        if 'rad_id' in pred_d.keys():\n            ret_rad_id = True\n            rad_id_v = pred_d['rad_id']\n        else:\n            ret_rad_id = False\n\n\n        if 'model_id' in pred_d.keys():\n            model_id_v = pred_d['model_id']\n        else:\n            model_id_v = np.zeros(pred_d['cls'].shape, dtype=np.int)\n            \n            \n        new_cls_v = []\n        new_bbox_v = []\n        new_p_det_v = []\n        new_rad_id_v = []\n        for i_c in np.unique(cls_v):\n            f_c = (cls_v == i_c)\n            \n            n_c = f_c.sum()\n            if n_c == 1:\n                if consensus_level > 1 and i_c != -1:\n                    continue\n                    \n                    \n                if ret_rad_id:\n                    if i_c == -1:\n                        n_rads = rad_id_v.size\n                        \n                        if n_rads < consensus_level:\n                            continue\n                            \n                        else:\n                            if n_rads > 1:\n                                new_rad_id_v.append( np.concatenate(rad_id_v, axis=-1) )\n                            else:\n                                new_rad_id_v.append( rad_id_v[f_c][0] )\n                    else:\n                        new_rad_id_v.append( rad_id_v[f_c][0] )\n                    \n\n                new_cls_v.append( i_c )\n                new_bbox_v.append( bbox_v[f_c][0] )\n                new_p_det_v.append( p_det_v[f_c][0] )\n                \n                \n                \n            else:\n                f_cls_v = cls_v[f_c]\n                f_bbox_v = bbox_v[f_c]\n                f_p_det_v = p_det_v[f_c]\n                f_model_id_v = model_id_v[f_c]\n\n                if ret_rad_id:\n                    f_rad_id_v = rad_id_v[f_c]\n                    \n                to_join_idxs_v = []\n                for i in range(0, n_c):\n                    idxs_s = set( np.argwhere( calc_iou(f_bbox_v[i], f_bbox_v) > iou_th ).T[0] )\n                    \n#                     print(idxs_s)\n                    for i in range(len(to_join_idxs_v)):\n                        if len( idxs_s.intersection(to_join_idxs_v[i]) ) > 0:\n                            to_join_idxs_v[i] = to_join_idxs_v[i].union(idxs_s)\n                            break\n                            \n                    else:\n                        to_join_idxs_v.append(idxs_s)\n                    \n                for to_join_idxs in to_join_idxs_v:\n                    to_join_idxs = list(to_join_idxs)\n                    \n                    if len(to_join_idxs) < consensus_level:\n                        continue\n                        \n                    bbox, p_det = join_preds(\n                        f_bbox_v[to_join_idxs],\n                        f_p_det_v[to_join_idxs],\n                        mode=mode,\n                    )\n\n                    if n_models2ensemble > 1:\n                        ens_prop = len( np.unique(f_model_id_v[to_join_idxs]) ) \/ n_models2ensemble\n                        p_det = p_det * ens_prop\n                        \n                    new_cls_v.append( i_c )\n                    new_bbox_v.append( bbox )\n                    new_p_det_v.append( p_det )\n                    \n                    if ret_rad_id:\n                        new_rad_id_v.append( np.concatenate(f_rad_id_v[to_join_idxs], axis=-1))\n        \n        ret_preds_d = {\n            'cls': np.array(new_cls_v),\n            bbox_key: np.array(new_bbox_v),\n        }\n        \n        if ret_p_det:\n            ret_preds_d['p_det'] = np.array(new_p_det_v)\n            \n        if ret_rad_id:\n            ret_preds_d['rad_id'] = np.array(new_rad_id_v, dtype=object)\n            \n        for k in pred_d.keys():\n            if k not in ['cls', bbox_key, 'p_det', 'rad_id', 'model_id']:\n                ret_preds_d[k] = pred_d[k]\n        \n        ret_preds_v.append(ret_preds_d)\n    \n    return ret_preds_v\n\n\n\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #\n\ndef evalueate_dataset(\n    ds,\n    model,\n    det_th=0.25,\n    unscale_bboxes=True,\n    batch_size=16,\n    num_workers=8,\n    pin_memory=True,\n    do_clean_predictions=True,\n    clean_iou_th=0.10,\n    clean_mode='p_det_weight',\n    do_TTA=False,\n    TTA_clean_iou_th=0.2,\n    TTA_clean_mode='median_pmean',\n):\n    \n    ds_iter = tqdm(DataLoader(\n        ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        collate_fn=batch_merge))\n    \n    \n    ret_preds_v = []\n    for data in ds_iter:\n        if do_TTA:\n            pred_v = model.predict_TTA(\n                data,\n                det_th=det_th,\n                unscale_bboxes=unscale_bboxes,\n                TTA_clean_iou_th=TTA_clean_iou_th,\n                TTA_clean_mode=TTA_clean_mode,\n                )\n\n        else:\n            pred_v = model.predict(\n                data,\n                det_th=det_th,\n                unscale_bboxes=unscale_bboxes)\n        \n        if do_clean_predictions:\n            pred_v = clean_predictions(\n                pred_v,\n                iou_th=clean_iou_th,\n                mode=clean_mode)\n            \n        for i_s, pred_d in enumerate(pred_v):\n            pred_d['sample_id']      = data['sample_id'][i_s]\n            pred_d['original_shape'] = data['original_shape'][i_s]\n            \n            \n        ret_preds_v.extend(pred_v)\n\n    return ret_preds_v\n    \n    \n    \ndef pred_to_str(pred_d):\n    cls_v = pred_d['cls']\n    bbox_v = pred_d['bbox']\n    p_det_v = pred_d['p_det']\n    \n    if len(cls_v) == 0:\n        ret_s = '14 1 0 0 1 1'\n    \n    else:\n        s_v = []\n        for cls, p_det, bbox in zip(cls_v.astype(np.int), p_det_v, np.round(bbox_v).astype(np.int)):\n            s = '{} {:0.05} {} {} {} {}'.format(\n                int(cls),\n                p_det,\n                *bbox\n            )\n            \n            s_v.append(s)\n            \n        ret_s = ' '.join(s_v)\n    \n    return ret_s\n\n\ndef predictions_to_df(\n    preds_v,\n    save_path=None,\n):\n    pred_summary_d = {\n        'image_id':[],\n        'PredictionString':[]\n    }\n    \n    for pred_d in preds_v:\n        pred_str = pred_to_str(pred_d)\n        pred_summary_d['image_id'].append( pred_d['sample_id'] )\n        pred_summary_d['PredictionString'].append( pred_str )\n        \n    pred_summary_df = pd.DataFrame(pred_summary_d)\n    \n    if save_path is not None:\n        pred_summary_df.to_csv(\n            save_path,\n            index=None)\n        \n        print(f' Saved submission: \"{save_path}\"')\n        \n    return pred_summary_df\n\n\ndef read_prediction_csv(filename='.\/ds_tst_F0_V6_JustCLS0_1.25x.csv'):\n    sub_df = pd.read_csv(filename)\n    \n    preds_v = []\n    for sample_id, preds in sub_df.values:\n        preds_split = preds.split()\n\n        pred_d = {\n            'sample_id': sample_id,\n            'cls':    [],\n            'bbox': [],\n            'p_det':  [],\n            }\n\n\n        for i in range(0, len(preds_split), 6):\n            cls, p_det, x_min, y_min, x_max, y_max = [float(x) for x in preds_split[i:i+6]]\n            cls = int(cls)\n            \n            if cls != 14:\n                bboxes = np.array([x_min, y_min, x_max, y_max])\n                pred_d['cls'].append(cls)\n                pred_d['bbox'].append(bboxes)\n                pred_d['p_det'].append(p_det)\n\n        pred_d['cls']    = np.array( pred_d['cls'] )\n        pred_d['bbox']   = np.array( pred_d['bbox'] )\n        pred_d['p_det']  = np.array(pred_d['p_det'] )\n        \n        preds_v.append(pred_d)\n        \n    return preds_v\n    \n\n# Validation functions\n\ndef voc_ap(recall, precision, use_07_metric=False):\n    \"\"\"\n    Reference:\n        https:\/\/github.com\/wang-tf\/pascal_voc_tools\/blob\/master\/pascal_voc_tools\/Evaluater\/tools.py\n        \n    ap = voc_ap(recall, precision, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses  the\n    VOC 07 11 point method (default: False).\n    Please make shure that recall and precison are sorted by scores.\n    Args:\n        recall: the shape of (n,) ndarray;\n        precision: the shape of (n,) ndarray;\n        use_07_metric: if true, the 11 points method will be used.\n    Returns:\n        the float number result of average precision.\n    \"\"\"\n    if use_07_metric:\n        # 11 point metric\n        ap = 0.\n        for t in np.arange(0., 1.1, 0.1):\n            if np.sum(recall >= t) == 0:\n                p = 0\n            else:\n                p = np.max(precision[recall >= t])\n            ap = ap + p \/ 11.\n    else:\n        # correct AP calculation\n        # first append sentinel values at the end\n        mrec = np.concatenate(([0.], recall, [1.]))\n        mpre = np.concatenate(([0.], precision, [0.]))\n\n        # compute the precision envelope\n        for i in range(mpre.size - 1, 0, -1):\n            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n\n        # to calculate area under PR curve, look for points\n        # where X axis (recall) changes value\n        i = np.where(mrec[1:] != mrec[:-1])[0]\n\n        # and sum (\\Delta recall) * prec\n        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n    return ap\n\n\ndef compute_overlaps(boxes, one_box):\n    \"\"\"\n    Reference:\n        https:\/\/github.com\/wang-tf\/pascal_voc_tools\/blob\/master\/pascal_voc_tools\/Evaluater\/tools.py\n        \n    iou = compute_overlaps(boxes, one_box)\n    compute intersection over union of ndarray.\n    The format of one_box is [xmin, ymin, xmax, ymax].\n    Args:\n        boxes: the (n, 4) shape ndarray, ground truth boundboxes;\n        bb: the (4,) shape ndarray, detected boundboxes;\n    Returns:\n        a (n, ) shape ndarray.\n    \"\"\"\n    # compute overlaps\n    # intersection\n    ixmin = np.maximum(boxes[:, 0], one_box[0])\n    iymin = np.maximum(boxes[:, 1], one_box[1])\n    ixmax = np.minimum(boxes[:, 2], one_box[2])\n    iymax = np.minimum(boxes[:, 3], one_box[3])\n    iw = np.maximum(ixmax - ixmin + 1., 0.)\n    ih = np.maximum(iymax - iymin + 1., 0.)\n    inters = iw * ih\n\n    # union\n    boxes_area = (boxes[:, 2] - boxes[:, 0] + 1.) * (boxes[:, 3] -\n                                                     boxes[:, 1] + 1.)\n    one_box_area = (one_box[2] - one_box[0] + 1.) * (one_box[3] - one_box[1] +\n                                                     1.)\n    iou = inters \/ (one_box_area + boxes_area - inters)\n\n    return iou\n\n\ndef voc_eval(class_recs: dict,\n             detect: dict,\n             iou_thresh: float = 0.5,\n             use_07_metric: bool = False):\n    \"\"\"\n    Reference:\n        https:\/\/github.com\/wang-tf\/pascal_voc_tools\/blob\/master\/pascal_voc_tools\/Evaluater\/tools.py\n        \n    recall, precision, ap = voc_eval(class_recs, detection,\n                                [iou_thresh],\n                                [use_07_metric])\n    Top level function that does the PASCAL VOC evaluation.\n    Please make sure that the class_recs only have one class annotations.\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    Args:\n        class_recalls: recalls dict of a class\n            class_recs[image_name]={'bbox': []}.\n        detection: Path to annotations\n            detection={'image_ids':[], bbox': [], 'confidence':[]}.\n        [iou_thresh]: Overlap threshold (default = 0.5)\n        [use_07_metric]: Whether to use VOC07's 11 point AP computation\n            (default False)\n    Returns:\n        a dict of result including true_positive_number, false_positive_number,\n        recall, precision and average_precision.\n    Raises:\n        TypeError: the data format is not np.ndarray.\n    \"\"\"\n    # format data\n    # class_rec data load\n    npos = 0\n    for imagename in class_recs.keys():\n        if not isinstance(class_recs[imagename]['bbox'], np.ndarray):\n            raise TypeError\n        detected_num = class_recs[imagename]['bbox'].shape[0]\n        npos += detected_num\n        class_recs[imagename]['det'] = [False] * detected_num\n\n    # detections data load\n    image_ids = detect['image_ids']\n    confidence = detect['confidence']\n    BB = detect['bbox']\n    if not isinstance(confidence, np.ndarray):\n        raise TypeError\n    if not isinstance(BB, np.ndarray):\n        raise TypeError\n    \n    # sort by confidence\n    sorted_ind = np.argsort(-confidence)\n    BB = BB[sorted_ind]\n    image_ids = [image_ids[x] for x in sorted_ind]\n\n    # go down dets and mark TPs and FPs\n    nd = len(image_ids)\n    tp = np.zeros(nd)\n    fp = np.zeros(nd)\n    for d in range(nd):\n        R = class_recs[image_ids[d]]\n        bb = BB[d, :].astype(float)\n        iou_max = -np.inf\n        BBGT = R['bbox'].astype(float)\n\n        if BBGT.size > 0:\n            overlaps = compute_overlaps(BBGT, bb)\n            iou_max = np.max(overlaps)\n            iou_max_index = np.argmax(overlaps)\n\n        if iou_max > iou_thresh:\n            if not R['det'][iou_max_index]:\n                tp[d] = 1.\n                R['det'][iou_max_index] = 1\n            else:\n                fp[d] = 1.\n        else:\n            fp[d] = 1.\n\n    # compute precision recall\n    fp = np.cumsum(fp)\n    tp = np.cumsum(tp)\n    true_positive_number = tp[-1] if len(tp) > 0 else 0\n    false_positive_number = fp[-1] if len(fp) > 0 else 0\n\n    recall = tp \/ np.maximum(float(npos), np.finfo(np.float64).eps)\n    # avoid divide by zero in case the first detection matches a difficult ground truth\n    precision = tp \/ np.maximum(tp + fp, np.finfo(np.float64).eps)\n    average_precision = voc_ap(recall, precision, use_07_metric)\n\n    result = {}\n    result['true_positive_number'] = true_positive_number\n    result['false_positive_number'] = false_positive_number\n    result['positive_number'] = npos\n    result['recall'] = recall\n    result['precision'] = precision\n    result['average_precision'] = average_precision\n\n    return result\n\n\n\ndef make_class_detection_d(preds_v, n_classes=15):\n    detection_v = [ {\n        'image_ids': [],\n        'bbox': [],\n        'confidence': [],\n    } for i in range(n_classes)]\n    \n    \n    for pred_d in preds_v:\n        sample_id = pred_d['sample_id']\n        if len(pred_d['bbox']) > 0:\n            for bbox, p_det, cls in zip(pred_d['bbox'], pred_d['p_det'], pred_d['cls']):\n                detection_v[cls]['image_ids'].append(sample_id) \n                detection_v[cls]['bbox'].append(bbox) \n                detection_v[cls]['confidence'].append(p_det) \n        else:\n            bbox = np.array([0.0, 0.0, 1.0, 1.0])\n            p_det = 1.0\n            cls = n_classes-1\n            \n            detection_v[cls]['image_ids'].append(sample_id) \n            detection_v[cls]['bbox'].append(bbox) \n            detection_v[cls]['confidence'].append(p_det) \n            \n            \n    for cls in range(len(detection_v)):\n        for k in detection_v[cls].keys():            \n            detection_v[cls][k] = np.array(detection_v[cls][k])\n    \n    return detection_v\n\n\ndef make_class_gt_d(gt_df, n_classes=15):\n    \n    class_gt_v = []\n    for class_id in range(n_classes):\n        class_gt_d = {}\n        for k in gt_df.image_id.unique():\n            class_gt_d[k] = {'bbox': []}\n        \n        class_gt_v.append( class_gt_d )\n    \n    for image_id, class_id, x_min, y_min, x_max, y_max in gt_df[['image_id', 'class_id', 'x_min', 'y_min', 'x_max', 'y_max']].values:\n        bbox = (x_min, y_min, x_max, y_max)\n        \n        class_gt_v[class_id][image_id]['bbox'].append(bbox)\n        \n    for cls in range(len(class_gt_v)):\n        for k in class_gt_v[cls].keys():\n            class_gt_v[cls][k]['bbox'] = np.array(class_gt_v[cls][k]['bbox'])\n            \n    return class_gt_v\n        \n\n\n\n\n\n\n        \ndef calc_metrics(preds_v, gt_df, iou_thresh=0.4, show_summay=True, n_classes=15, use_07_metric=False):\n    class_recs_v = make_class_gt_d(gt_df, n_classes=n_classes)\n    detect_v     = make_class_detection_d(preds_v, n_classes=n_classes)\n    \n    mAP = 0.0\n    metrics_v = []\n    for i_class in range(n_classes):\n        class_val_d = voc_eval(\n            class_recs_v[i_class],\n             detect_v[i_class],\n             iou_thresh=iou_thresh,\n             use_07_metric=use_07_metric)\n        \n        metrics_v.append(class_val_d)\n        \n        mAP += class_val_d['average_precision']\n    \n    mAP \/= n_classes\n    \n    summary_df = pd.DataFrame(\n        columns=['Cls', 'TP', 'FP', 'P', 'Prec', 'Rec', 'AP']\n    )\n    for i_c, m_d in enumerate(metrics_v):\n        summary_df = summary_df.append({\n            'Cls': i_c,\n            'TP': m_d['true_positive_number'],\n            'FP': m_d['false_positive_number'],\n            'P': m_d['positive_number'],\n            'Prec': m_d['precision'][-1] if len(m_d['precision']) > 0 else 0.0,\n            'Rec': m_d['recall'][-1] if len(m_d['recall']) > 0 else 0.0,\n            'AP': m_d['average_precision'],\n        },\n        ignore_index=True,)\n        \n    summary_df = summary_df.astype({\n            'Cls': np.int,\n            'TP': np.int,\n            'FP': np.int,\n            'P': np.int,\n            'Prec': np.float32,\n            'Rec': np.float32,\n            'AP': np.float32,\n        })\n\n    summary_df = summary_df.set_index( 'Cls' )\n\n    if show_summay:\n        print(' Summary:')\n        print( summary_df )\n        print(f'mAP = {mAP:0.04f}')\n    \n    return metrics_v, summary_df, mAP\n\n\n\ndef plot_PvsR_curve(metrics_v, class2color_v, do_show=True):\n    \n    fig = plt.figure(0, figsize=(20,10))\n    for i_c, metrics_d in enumerate(metrics_v):\n        plt.plot(\n            metrics_d['recall'],\n            metrics_d['precision'],\n            label=f'cls={i_c}',\n            c=np.array(class2color_v[i_c])\/255 )\n\n\n    plt.xlabel('recall')\n    plt.ylabel('precision')\n\n    plt.xlim( (0,1) )\n    plt.ylim( (0,1) )\n\n    plt.legend()\n    \n    if do_show:\n        plt.show()\n    \n    return fig\n    \n    \n    \ndef filter_det_th(preds_v, det_th=0.15):\n    if det_th == 0.0:\n        return copy.deepcopy(preds_v)\n\n    new_pred_v = []\n    for pred_d in preds_v:\n        \n        if len(pred_d['p_det']) > 0:\n            if type(det_th) is float:\n                det_th_v = det_th\n            else:\n                det_th_v = np.zeros(pred_d['p_det'].shape[0])\n\n                for i_c, i_cls in enumerate(pred_d['cls']):\n                    det_th_v[i_c] = det_th[i_cls]\n                    \n            f = pred_d['p_det'] >= det_th_v\n        else:\n            f = None\n            \n        new_pred_d = {}\n        for k in pred_d.keys():\n            if (f is not None) and (k in ['cls', 'p_det', 'bbox']):\n                new_pred_d[k] = pred_d[k][f]\n                \n            else:\n                new_pred_d[k] = pred_d[k]\n        \n        new_pred_v.append(new_pred_d)\n        \n    return new_pred_v\n\n\n\n\ndef join_predictions(preds_v_list=[], add_model_id=False):\n    \"\"\" Join Predictions from differetn models. \"\"\"\n    \n    preds_v_list = copy.deepcopy(preds_v_list)\n    \n    for i in range(len(preds_v_list)):\n        assert len(preds_v_list[0]) == len(preds_v_list[i]), 'ERROR, pred_v have differetn sizes'\n        preds_v_list[i] = sorted(preds_v_list[i], key=lambda l: l['sample_id'])\n        \n    \n    ret_pred_v = []\n    for p_v in zip(*preds_v_list):\n        for p in p_v[1:]:\n            assert p_v[0]['sample_id'] == p['sample_id']\n        \n        sample_id = p_v[0]['sample_id']\n        p_v = [p  for p in p_v if len(p['cls']) > 0]\n        \n        \n        keys_v = ['bbox', 'cls', 'p_det']\n        if len(p_v) and 'model_id' in p_v[0].keys():\n            keys_v.append('model_id')\n\n        elif add_model_id:\n            keys_v.append('model_id')\n            \n            for i_m in range(len(p_v)):\n                p_v[i_m]['model_id'] = i_m * np.ones_like(p_v[i_m]['cls'])\n                \n\n        if len(p_v) > 0:\n            pred_d = {\n                'sample_id': sample_id\n            }\n            \n            \n            for k in keys_v:\n                pred_d[k] = np.concatenate([p[k]  for p in p_v], axis=0)\n                \n        else:\n            pred_d = {\n                'sample_id': sample_id\n            }\n\n            for k in keys_v:\n                pred_d[k] = np.array([])\n    \n        ret_pred_v.append(pred_d)\n    \n    return ret_pred_v\n\n\ndef norm_p_det(pred_v):\n    p_det_v = []\n    for pred_d in pred_v:\n        if len(pred_d['p_det']) > 0:\n            p_det_v.append( pred_d['p_det'] )\n    \n    p_det_v = np.concatenate(p_det_v)\n    p_det_max = p_det_v.max()\n    \n    \n    print('p_det_max =', p_det_max)\n    if p_det_max > 1.0:\n        ret_pred_v = copy.deepcopy(pred_v)\n        for pred_d in ret_pred_v:\n            if len(pred_d['p_det']) > 0:\n                pred_d['p_det'] = pred_d['p_det'] \/ p_det_max\n    \n    else:\n        print('skipping norm_p_det')\n        \n        return pred_v\n        \n    return ret_pred_v\n\n\n\n\ndef fix_boxes(preds_v, img_shapes_d=None):\n    \"\"\"\n    Fixes:\n    - p_det > 1.0 or p_det < 0.0\n    - xmax - xmin > 0\n    - ymax - ymin > 0\n    \n    if img_shapes_d is provided: also clips the bboxes using the sample shape.\n    \"\"\"\n    \n    for preds_d in preds_v:\n        if len(preds_d['cls']) > 0:\n            if img_shapes_d is not None:\n                x_max, y_max = img_shapes_d[ preds_d['sample_id'] ]\n                \n                EPS = 0.0\n                preds_d['bbox'] = np.clip(\n                    preds_d['bbox'],\n                    np.array([EPS, EPS, EPS, EPS], dtype=preds_d['bbox'].dtype),\n                    np.array([x_max-EPS, y_max-EPS, x_max-EPS, y_max-EPS], dtype=preds_d['bbox'].dtype) )\n            \n            \n            dx_dy = preds_d['bbox'][:,2:] - preds_d['bbox'][:,:2]\n\n            f0 = (dx_dy <= 1).any(axis=-1)\n            f1 = (preds_d['p_det']<=0) + (preds_d['p_det']>1.0)\n            \n            if f0.any() or f1.any():\n                print('.', end='')\n                f = ~(f0 + f1)\n                for k in ['p_det', 'bbox', 'cls']:\n                    preds_d[k] = preds_d[k][f]\n\n    return None\n\n\n\n\ndef add_class_14(\n    preds_v,\n    pred_clf_c14_filename='2-cls test pred.csv',\n    low_threshold=0.00,\n    high_threshold=0.99,\n    rm_preds_high_th=True\n    ):\n\n    cls_df = pd.read_csv(pred_clf_c14_filename)\n\n    class_14_d = {}\n    for sample_id, p_cls in cls_df.values:\n        p_14 = 1.0 - p_cls\n\n        if p_14 < low_threshold:\n            # Keep, do nothing.\n            class_14_d[sample_id] = 0.0\n\n        elif p_14 >= high_threshold:\n            # Replace, remove all \"det\" preds.\n            class_14_d[sample_id] = 1.0\n\n        else:\n            # Add, keep \"det\" preds and add normal pred.\n            class_14_d[sample_id] = p_14\n            \n    \n    \n    ret_preds_v = copy.deepcopy(preds_v)\n                                \n    for pred_d in tqdm(ret_preds_v):\n        default_case = False\n        p_14 = class_14_d[ pred_d['sample_id'] ]\n\n        if p_14 == 1:\n            if rm_preds_high_th:\n                pred_d['bbox']  = np.array([[0.0, 0.0, 1.0, 1.0]])\n                pred_d['cls']   = np.array([14])\n                pred_d['p_det'] = np.array([1.0])\n                \n            else:\n                default_case = True\n\n        elif p_14 == 0.0:\n            continue\n\n        else:\n            default_case = True\n            \n        if default_case:\n            if len(pred_d['bbox']) > 0 and 14 not in pred_d['cls']:\n                pred_d['bbox'] = np.append(pred_d['bbox'], np.array([[0.0, 0.0, 1.0, 1.0]]), axis=0)\n                pred_d['cls']  = np.append(pred_d['cls'], 14)\n                pred_d['p_det'] = np.append(pred_d['p_det'], p_14)\n\n            else:\n                pred_d['bbox'] = np.array([[0, 0, 1, 1]])\n                pred_d['cls']  = np.array([14], dtype=np.int)\n                pred_d['p_det'] = np.array([p_14])\n    \n    \n    return ret_preds_v\n\n\n\n\ndef plot_sample(img, bbox_v=None, cls_v=None, original_shape=None, ax=None, class2color_v=None):\n\n    if class2color_v is None:\n        class2color_v = [\n                tuple(round(i * 255) for i in colorsys.hsv_to_rgb(i_c\/15, 1, 1))\n                for i_c in range(15)\n            ]\n            \n            \n    if type(img) is torch.Tensor:\n        img = img.clone().detach().cpu().numpy()\n        \n        \n    if type(bbox_v) is torch.Tensor:\n        bbox_v = bbox_v.clone().detach().cpu().numpy()\n    \n    if type(cls_v) is torch.Tensor:\n        cls_v = cls_v.clone().detach().cpu().numpy()\n    \n    \n    if img.shape[0] < 8:\n        img = img.transpose( (1,2,0) )\n    \n    if len(img.shape) == 3 and img.shape[2] > 3:\n        img = img[...,:3]\n        \n    if img.max() <= 1.0:\n        img = (255*img).astype(np.uint8)\n    \n    \n    if ax is None:\n        do_show = True\n        f, ax = plt.subplots(1, figsize=(15,15))\n    else:\n        do_show = False\n        \n    if original_shape is not None:\n        org_h, org_w = original_shape\n        img_h, img_w = img.shape[:2]\n\n        scale = np.array([\n            img_w\/org_w,\n            img_h\/org_h,\n            img_w\/org_w,\n            img_h\/org_h,\n        ])\n        \n    else:\n        scale = np.ones(4)\n    \n    if (bbox_v is not None) and (cls_v is not None):\n        for cls, (x0, y0, x1, y1) in zip(cls_v, (bbox_v * scale).astype(np.int)):\n            img = cv2.rectangle(img, (x0, y0), (x1, y1), class2color_v[cls])\n        \n    ax.imshow(img)\n    \n    if do_show:\n        plt.show()\n        \n    return None\n\n\n\n\ndef scale_bboxes(bbox, scale_wh=[1.0, 1.0], original_shape=None):\n\n    if type(scale_wh) is float:\n        scale_wh = scale_wh * np.ones(2)\n    else:\n        scale_wh = np.array(scale_wh)\n        \n    p_ll = bbox[:,:2]\n    p_ur = bbox[:,2:]\n\n    p_c  = 0.5 * (p_ur + p_ll)\n    p_wh = 0.5 * scale_wh * (p_ur - p_ll)\n\n    p_ur = p_c + p_wh\n    p_ll = p_c - p_wh\n\n    new_bbox = np.concatenate([p_ll, p_ur], axis=-1)\n    \n    if original_shape is not None:\n        new_bbox = np.clip(\n            new_bbox,\n            np.zeros(4),\n            np.array([original_shape[1], original_shape[0], original_shape[1], original_shape[0]]))\n    \n    \n    f = ( (new_bbox[:,2:] - new_bbox[:,:2]) > 0 ).all(axis=-1)\n    new_bbox = new_bbox[f]\n    return new_bbox\n\n\n\n\n\ndef wbf_ensemble(\n    to_ens_preds_v,\n    samples_shapes_d,\n    iou_th=0.6,\n    skip_box_th=0.01,\n    conf_type='avg'\n):\n    \n    n_models = len(to_ens_preds_v)\n\n    ens_preds_v = join_predictions(\n        to_ens_preds_v,\n        add_model_id=True)\n\n\n\n    boxes_v  = [[] for i_m in range(n_models)]\n    scores_v = [[] for i_m in range(n_models)]\n    labels_v = [[] for i_m in range(n_models)]\n\n    ret_preds_v = []\n    for pred_d in ens_preds_v:\n        if len( pred_d['cls'] ) > 0:\n            h, w = samples_shapes_d[pred_d['sample_id']]\n            scale = np.array([w, h, w, h])\n\n            boxes_v  = []\n            scores_v = []\n            labels_v = []\n\n            for i_m in range(n_models):\n                f_m = (pred_d['model_id'] == i_m)\n\n                boxes_v.append( pred_d['bbox'][f_m] \/ scale )\n                scores_v.append( pred_d['p_det'][f_m] )\n                labels_v.append( pred_d['cls'][f_m] )\n\n\n            boxes, scores, labels = weighted_boxes_fusion(\n                boxes_list=boxes_v,\n                scores_list=scores_v,\n                labels_list=labels_v,\n                weights=None,\n                iou_thr=iou_th,\n                skip_box_thr=skip_box_th,\n                conf_type=conf_type,\n                allows_overflow=False,\n            )\n\n            ret_pred_d = {\n                'sample_id': pred_d['sample_id'],\n                'bbox':      boxes * scale,\n                'cls':       labels.astype(np.int),\n                'p_det':     scores,\n            }\n\n        else:\n            ret_pred_d = copy.deepcopy( pred_d )\n            del(ret_pred_d['model_id'])\n\n        ret_preds_v.append( ret_pred_d )\n        \n        \n    return ret_preds_v\n","95947934":"def save_obj(obj, filename):\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f)\n    print(f'Saved: {filename}')\n    return None\n\ndef load_obj(filename):\n    with open(filename, 'rb') as f:\n        obj = pickle.load(f)\n    print(f'Loaded: {filename}')\n    return obj","fb425f8a":"def read_dicom_image(\n    path,\n    voi_lut=True,\n    fix_monochrome=True,\n    do_norm=True):\n\n    # Original from: https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    dicom = pydicom.read_file(path)\n\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \n    # \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.max(data) - data\n\n    if do_norm:\n        max_value = (2 ** dicom.BitsStored) - 1\n        data = data \/ max_value\n\n        assert (data.max() <= 1.0) and (data.min() >= 0.0), f'Normalization ERROR in file: \"{path}\"'\n\n#     if do_norm:\n#         max_val = np.max(data)\n#         min_val = np.min(data)\n#         data = (data - min_val) \/ (max_val- min_val)\n\n    return data.astype(np.float32)\n\ndef read_image_file(\n    path,\n    do_norm=True,\n    monocrome=True,\n):\n    \n    data = cv2.imread(path)\n    \n    if do_norm:\n        max_value = 255\n        data = data \/ max_value\n        \n        assert (data.max() <= 1.0) and (data.min() >= 0.0), f'Normalization ERROR in file: \"{path}\"'\n    \n    if monocrome:\n        if len(data.shape) == 3:\n            data = data[:,:,0]\n        \n    return data.astype(np.float32)","744dde69":"def calc_image_features(image):\n    img_exp  = image ** 3.0\n    img_uint = (255 * image).astype(np.uint8)\n    img_equ  = cv2.equalizeHist(img_uint)\n    img_edge = cv2.Canny(img_equ, 50, 130)\n\n    img_ret = np.concatenate(\n        [\n            image[:,:,None],\n            img_exp[:,:,None],\n            img_equ[:,:,None].astype(np.float32)  \/ 255,\n            img_edge[:,:,None].astype(np.float32) \/ 255,\n            ],\n        axis=-1)\n\n    return img_ret","06e0dc5b":"class FoldDataset():\n    def __init__(\n        self,\n        ds_path='.\/',\n        ds_name='test',\n        images_dir='.\/test_images',\n        \n        model_resolution=(512, 512),\n        df_path=None,\n        \n        mode='none',\n        \n        i_fold=0,\n        n_folds=5,\n        test_split=0.1,\n        \n        do_augmentation=True,\n        \n        downsample_factor=2,\n        remove_classes_v=None,\n        select_classes_v=None,\n        reclass_samples=True,\n        \n        \n        show_warnings=True,\n        \n        do_random_shuffle=True,\n        random_seed=3128,\n        sample_bbox_and_class=False,\n        clean_boxes=True,\n        clean_mode='random',\n        clean_iou_th=0.1,\n        consensus_level=1,\n        \n        effdet_xy_invert=False,\n        \n        no_finding_cls=class2str_v.index('No Finding'),\n        \n        use_img_cache=False,\n    ):\n        \n        \n        self.ds_path = ds_path\n        self.ds_name = ds_name\n        \n        self.model_resolution = model_resolution\n        self.bboxes_df_path   = df_path\n        self.i_fold           = i_fold\n        self.n_folds          = n_folds\n        self.do_augmentation  = do_augmentation\n        \n        self.images_dir        = images_dir\n        self.downsample_factor = downsample_factor\n        self.remove_classes_v  = remove_classes_v\n        self.select_classes_v  = select_classes_v\n        self.reclass_samples   = reclass_samples\n        \n        self.show_warnings     = show_warnings\n        self.test_split        = test_split\n        \n        self.do_random_shuffle = do_random_shuffle\n        self.random_seed       = random_seed\n        \n        self.sample_bbox_and_class = sample_bbox_and_class\n        \n        self.clean_boxes  = clean_boxes\n        self.clean_mode   = clean_mode\n        self.clean_iou_th = clean_iou_th\n        self.consensus_level = 1 if consensus_level is None else consensus_level\n        \n        self.effdet_xy_invert = effdet_xy_invert\n        self.no_finding_cls = no_finding_cls\n        \n        \n        self.use_img_cache = use_img_cache\n        \n        if (self.select_classes_v is not None) and (14 in self.select_classes_v):\n            self.select_classes_v.remove(14)\n            self.select_classes_v.append(-1)\n            \n        if (self.select_classes_v is not None) and self.reclass_samples:\n            self.cls_orig2new_d = {}\n            self.cls_new2orig_d = {}\n            \n            for i_cls_new, i_cls in enumerate(sorted(self.select_classes_v)):\n                if -1 in self.select_classes_v:\n                    i_cls_new -= 1\n                \n                self.cls_orig2new_d[14] = len(self.select_classes_v) - 1\n                self.cls_new2orig_d[len(self.select_classes_v) - 1] = 14\n                    \n                self.cls_orig2new_d[i_cls]     = i_cls_new\n                self.cls_new2orig_d[i_cls_new] = i_cls\n                    \n        \n        self.mode = mode.lower()\n        posible_modes_v = ['cv_trn', 'cv_val', 'cv_tst', 'none']\n        assert self.mode in posible_modes_v, f'Parameter \"mode\" must be in: {posible_modes_v}'\n        \n        if mode == 'cv_trn':\n            self.do_CV  = True\n            self.iter_trn = True\n            self.iter_val = False\n            self.iter_tst = False\n            \n        elif mode == 'cv_val':\n            self.do_CV  = True\n            self.iter_trn = False\n            self.iter_val = True\n            self.iter_tst = False\n            \n        elif mode == 'cv_tst':\n            self.do_CV  = True\n            self.iter_trn = False\n            self.iter_val = False\n            self.iter_tst = True\n        \n        elif mode == 'none':\n            self.do_CV  = False\n            self.iter_trn = False\n            self.iter_val = False\n            self.iter_tst = False\n        \n        \n        \n        self.h5_path = os.path.join(self.ds_path, f'{self.ds_name}.h5df')\n        self.misc_path = os.path.join(self.ds_path, f'{self.ds_name}.pickle')\n        self.cache_path = os.path.join(self.ds_path, f'{self.ds_name}.cache')\n        \n        self.f_h5 = None\n        \n        if self.use_img_cache:\n            if not os.path.exists(self.h5_path) or not os.path.exists(self.misc_path):\n                self._gen_images_ds()\n\n            self.open_h5_file()\n            \n        self.read_misc_d()\n        \n        self.bbox_df = None\n        \n        self._set_all_samples_ids()\n        \n        self._build_albumentations()\n\n        self._read_bboxes_df_or_cache()        \n            \n        self.update_fold_filter()\n        \n        return None\n    \n    \n    def _read_bboxes_df_or_cache(self):\n        if self.bboxes_df_path is not None:\n            if not os.path.exists(self.cache_path):\n                self.read_bboxes_df()\n                self._save_ds_state()\n                \n            else:\n                self._load_ds_state()\n                \n        return None\n    \n    def _save_ds_state(self):    \n        to_save_d = {\n            'bbox_df': self.bbox_df,\n            'class_to_sample_v': self.class_to_sample_v,\n            'bbox_d': self.bbox_d,\n        }\n        \n        save_obj(to_save_d, self.cache_path)\n        \n        return None\n    \n    \n    def _load_ds_state(self):\n        state_d = load_obj( self.cache_path )\n        \n        for k, v in state_d.items():\n            self.__setattr__(k, v)\n        \n        return None\n    \n    def _set_all_samples_ids(self):\n        self.all_sample_ids = np.array( sorted( self.misc_d.keys() ) )\n        \n        if self.bbox_df is not None:\n            assert ( self.all_sample_ids == np.array(sorted(self.bbox_df['image_id'].unique()) ) ).all()\n        \n        \n        if self.do_random_shuffle:\n            np.random.seed(self.random_seed)\n            \n        return None\n    \n    \n    def _build_bbox_d(self):\n        self.bbox_d  = {}\n        self.class_to_sample_v = [ [] for i in range( self.bbox_df.class_id.max() + 1)]\n        \n#         i = 0\n        sample_it = tqdm(self.all_sample_ids, desc='Building bboxes')\n        for s_id in sample_it:\n            sample_bboxes_df = self.bbox_df[self.bbox_df.image_id == s_id]\n\n            cls = sample_bboxes_df['class_id'].values\n            bboxes = sample_bboxes_df[['x_min', 'y_min', 'x_max', 'y_max']].values\n            rad_id = sample_bboxes_df[ ['rad_id'] ].values\n            \n            for i_c in np.unique(cls):\n                self.class_to_sample_v[i_c].append( s_id )\n                \n            # Class 14 filter\n            f_14 = (cls==self.no_finding_cls)\n            if f_14.any():\n                i_14 = f_14.argmax()\n                \n                orig_img_h, orig_img_w = self.misc_d[s_id]\n                bboxes[i_14] = np.array([0, 0, orig_img_w, orig_img_h])\n                cls[i_14]    = -1  # if I add 1 will be the class  0 (backfround for effdet)\n                \n                f_14 = ~f_14\n                f_14[i_14] = True\n                bboxes = bboxes[f_14]\n                cls    = cls[f_14]\n            \n            f_nan = np.isnan(bboxes).any(axis=-1)\n            if f_nan.any():\n                orig_img_h, orig_img_w = self.misc_d[s_id]\n                bboxes[f_nan] = np.array([0, 0, orig_img_w, orig_img_h])\n                \n            self.bbox_d[s_id] = {\n                'bboxes': bboxes,\n                'cls':    cls,\n                'rad_id': rad_id,\n            }\n            \n#             i += 1\n            \n#             if i ==500:\n#                 break\n\n        for i_c in range(len(self.class_to_sample_v)):\n            self.class_to_sample_v[i_c] = np.array(self.class_to_sample_v[i_c])\n\n        return None\n\n\n\n    def read_bboxes_df(self):\n        self.bbox_df = pd.read_csv(\n            self.bboxes_df_path,\n            dtype={\n                'x_min':np.float32,\n                'y_min':np.float32,\n                'x_max':np.float32,\n                'y_max':np.float32,\n            })\n        \n#         self.bbox_df.fillna(0.0)\n        \n        self._build_bbox_d()\n        return None\n        \n        \n    def update_fold_filter(self):\n        assert self.i_fold >= 0 and self.i_fold < self.n_folds, 'ERROR, incorrect i_fold.'\n        \n        # Apply sample class filtering\n        if self.select_classes_v is None:\n            self.selected_sample_ids = self.all_sample_ids\n            \n        else:\n            self.selected_sample_ids = set()\n            for i_c in self.select_classes_v:\n                self.selected_sample_ids = self.selected_sample_ids.union( self.class_to_sample_v[i_c] )\n            \n            self.selected_sample_ids = np.array( sorted(self.selected_sample_ids) )\n            \n            \n        if self.remove_classes_v is not None:\n            self.selected_sample_ids = set(self.selected_sample_ids)\n            \n            for i_c in self.remove_classes_v:\n                self.selected_sample_ids = self.selected_sample_ids.difference( self.class_to_sample_v[i_c] )\n            \n            self.selected_sample_ids = np.array( sorted(self.selected_sample_ids) )\n        # # # # # # # # # # # # # # # # #\n        \n        n_samples = len( self.selected_sample_ids )\n        n_samples_cv = int( (1.0-self.test_split) * n_samples)\n        \n        if self.do_CV:\n            if self.iter_tst:\n                f_samples = np.zeros(n_samples, dtype=np.bool)\n                f_samples[n_samples_cv:] = True\n                \n            else:\n                n_samples_per_fold = n_samples_cv \/\/ self.n_folds\n                f_samples = np.zeros(n_samples, dtype=np.bool)\n\n                if self.i_fold < self.n_folds - 1:\n                    f_samples[ self.i_fold * n_samples_per_fold: (self.i_fold+1) * n_samples_per_fold] = True\n                else:\n                    f_samples[ self.i_fold * n_samples_per_fold: n_samples_cv] = True\n\n                if self.iter_trn:\n                    f_samples[:n_samples_cv]  = ~(f_samples[:n_samples_cv])\n                    \n        else:\n            f_samples = np.ones(n_samples, dtype=np.bool)\n        \n        \n        if self.bbox_df is not None and self.consensus_level > 1:\n            for i_s in np.argwhere(f_samples).T[0]:\n                \n                sample_id = self.selected_sample_ids[i_s]\n                \n                x = clean_predictions(\n                    [ self.bbox_d[ sample_id ] ],\n                    iou_th=self.clean_iou_th,\n                    mode=self.clean_mode,\n                    consensus_level=self.consensus_level,\n                )\n\n                if x[0]['bboxes'].shape[0] == 0:\n                    f_samples[i_s] = False\n        \n        \n        self.fold_sample_filter = f_samples \n        self.fold_samples = self.selected_sample_ids[self.fold_sample_filter]\n\n        return None\n    \n    \n    def get_image(self, sample_id):\n        \n        if self.use_img_cache:\n            image = self.f_h5[sample_id][:]\n        \n        else:\n            file_path = os.path.join(self.images_dir, f'{sample_id}.dicom')\n            image, image_shape, file_id = self.read_and_downsample_dicom_image(\n                file_path\n            )\n        \n        return image\n    \n    \n    def __getitem__(self, index):\n        \n        if index < 0:\n            index = self.__len__() + index\n        \n        sample_id = self.fold_samples[index]\n        \n        image = self.get_image(sample_id)\n        \n        original_shape = self.misc_d[sample_id]\n        \n        img_h, img_w = image.shape\n        orig_img_h, orig_img_w = original_shape\n        \n        scale_h = img_h \/ orig_img_h\n        scale_w = img_w \/ orig_img_w\n        \n        image = calc_image_features(image)\n        \n        sample = {\n            'sample_id': sample_id,\n            'image': image,\n            'original_shape': self.misc_d[sample_id],\n        }\n        \n        \n        if self.bbox_df is not None:\n            # The images are already resized.\n            # Format at this point is (x0,y0,x1,y1)\n            scale_v = np.array([scale_w, scale_h, scale_w, scale_h])\n            sample['bboxes'] = self.bbox_d[sample_id]['bboxes'] * scale_v\n            sample['cls'] = self.bbox_d[sample_id]['cls']\n            \n            if self.select_classes_v is not None:\n                f = np.zeros(sample['cls'].shape[0], dtype=np.bool)\n                    \n                for i in self.select_classes_v:\n                    f[sample['cls'] == i] = True\n\n                sample['bboxes'] = sample['bboxes'][f]\n                sample['cls']    = sample['cls'][f]\n                \n                if self.reclass_samples:\n                    for i_c, i_cls in enumerate(sample['cls']):\n                        sample['cls'][i_c] = self.cls_orig2new_d[i_cls]\n                    \n            \n            if self.sample_bbox_and_class:\n                idx = np.random.randint(0, sample['cls'].shape[0])\n                \n                (x0, y0, x1, y1) = sample['bboxes'][idx].astype(np.int)\n                \n                d_x = np.random.randint(0, (x1-x0)\/\/4 )\n                d_y = np.random.randint(0, (y1-y0)\/\/4 )\n                x0 = max(0, x0-d_x)\n                y0 = max(0, y0-d_y)\n                \n                x1 = min(sample['image'].shape[1], x1+d_x)\n                y1 = min(sample['image'].shape[0], y1+d_y)\n                \n                sample['image'] = sample['image'][y0:y1, x0:x1]\n                sample['cls'] = sample['cls'][idx:idx+1]\n                sample['bboxes'] = np.array( [ (0, 0, sample['image'].shape[1], sample['image'].shape[0]) ] )\n                \n            if self.clean_boxes:\n                to_clean_d = {'bbox': sample['bboxes'], 'cls':sample['cls']}\n                cleaned_d = clean_predictions(\n                    [to_clean_d],\n                    iou_th=self.clean_iou_th,\n                    mode=self.clean_mode,\n                    consensus_level=self.consensus_level\n                )[0]\n                \n                sample['bboxes'] = cleaned_d['bbox']\n                sample['cls'] = cleaned_d['cls']\n                \n        else:\n            sample['bboxes'] = np.array( [ (0, 0, image.shape[1], image.shape[0]) ] )\n            sample['cls']    = np.array( [-1] )\n        \n        \n\n        if self.do_augmentation:\n            for i_try in range(10):\n                try:\n                    transform_sample = self.TR_trn(**sample)\n                    if len(transform_sample['bboxes']) > 0:\n                        # Updating sample\n                        sample = transform_sample\n                        break\n\n                except Exception as e:\n                    pass\n\n            else:\n                if self.show_warnings:\n                    print(f' - WARNING: Imposible to Augmentate image, idx={index}.', file=sys.stderr)\n                    \n                # doing a basic transform\n                sample = self.TR_val(**sample)\n                \n        else:\n            sample = self.TR_val(**sample)\n        \n        sample['image'] = sample['image'].transpose([2,0,1])\n        sample['bboxes'] = np.array(sample['bboxes'], dtype=np.float32)\n        if self.effdet_xy_invert:\n            # Formating: (x0,y0,x1,y1) to (y0,x0,y1,x1) for EffDet\n            sample['bboxes'] = sample['bboxes'][:, [1,0,3,2]]\n        \n        sample['cls'] = np.array(sample['cls'], dtype=np.int)\n#         sample['extra'] = np.array([sample['s_norm'], sample['a_norm'], sample['d_norm']], dtype=np.float32).T\n        \n        \n        return sample\n    \n    \n    def open_h5_file(self, mode='r'):\n        if self.f_h5 is not None:\n            self.close_file()\n            \n        if mode == 'w':\n            if os.path.exists(self.h5_path):\n                print(f' The file \"{self.h5_path}\" already exists, if you continue the file will be deleted. Contunue (y\/n) ?')\n                r = input()\n                if r.lower() != 'y':\n                    print('Operation aborted.')\n                    sys.exit()\n            \n        self.f_h5 = h5py.File( self.h5_path, mode)\n        return None\n    \n    \n    def close_file(self):\n        if self.f_h5 is not None:\n            self.f_h5.close()\n            \n        return None\n    \n    \n    def read_misc_d(self):\n        self.misc_d = load_obj( self.misc_path )\n        return None\n        \n        \n    def save_misc_d(self, warn=True):\n        if warn and self.misc_d:\n            if os.path.exists(self.misc_d):\n                print(f' The file \"{self.misc_d}\" already exists, if you continue the file will be deleted. Contunue (y\/n) ?')\n                r = input()\n                if r.lower() != 'y':\n                    print('Operation aborted.')\n                    sys.exit()\n            \n        \n        save_obj(self.misc_d, self.misc_path)\n        return None\n    \n    \n    def downsample_img(\n        self,\n        image,\n        downsample_factor=2):\n        \n        new_dims_v = (\n            image.shape[1] \/\/ downsample_factor,\n            image.shape[0] \/\/ downsample_factor )\n        \n        image_rs = cv2.resize(\n            image,\n            new_dims_v\n        )\n        \n        return image_rs\n    \n    \n    def read_and_downsample_dicom_image(self, file_path):\n        file_id, file_ext = os.path.splitext(os.path.basename(file_path))\n        \n        if file_ext == '.dicom':\n            image = read_dicom_image(file_path)\n\n        else:\n            image = read_image_file(file_path)\n                \n        image_shape = image.shape\n\n        image_rs = self.downsample_img(\n            image,\n            downsample_factor=self.downsample_factor)\n        \n        return image_rs, image_shape, file_id\n    \n    \n    def _gen_images_ds(self):\n        all_files_v = glob.glob(os.path.join(self.images_dir, '*.dicom') ) + glob.glob(os.path.join(self.images_dir, '*.png') )\n        \n        self.open_h5_file(mode='w')\n        self.misc_d = {}\n        \n        file_it = tqdm( all_files_v )\n        for file_path in file_it:\n            image_rs, image_shape, file_id = self.read_and_downsample_dicom_image(file_path)\n            \n            file_it.set_description(file_id)\n            self.f_h5.create_dataset(file_id, data=image_rs)\n            self.misc_d[file_id] = image_shape\n        \n        self.close_file()\n        self.save_misc_d(warn=False)\n        \n        return None\n    \n\n    def __len__(self):\n        return len(self.fold_samples)\n    \n    \n    def _build_albumentations(self):\n        self.TR_trn = A.Compose(\n            [\n#                 A.RandomSizedCrop(\n#                     min_max_height=[int(0.7*self.image_resolution[0]), int(1.0*self.image_resolution[0])],\n#                     height=int(0.9*self.image_resolution[0]),\n#                     width=int(0.9*self.image_resolution[1]), \n#                     p=0.5),\n                \n                \n                A.Crop(\n                    x_min=128,\n                    y_min=128,\n                    x_max=128,\n                    y_max=128,\n                    p=0.5),\n\n#                     A.OneOf([\n#                         A.HueSaturationValue(\n#                             hue_shift_limit=0.2,\n#                             sat_shift_limit= 0.2,\n#                             val_shift_limit=0.2,\n#                             p=0.9),\n\n                A.RandomBrightnessContrast(\n                    brightness_limit=0.2, \n                    contrast_limit=0.2,\n                    p=0.9\n                ),\n\n#                     ],\n#                         p=0.9),\n\n#                     A.ToGray(p=0.01),\n\n                A.HorizontalFlip(p=0.5),\n\n    #             A.VerticalFlip(p=0.5),\n\n    #             A.RandomRotate90(p=0.5),\n\n                A.Rotate(\n                    limit=5,\n                    p=0.6,\n                ),\n\n#                     A.Transpose(p=0.5),\n\n#                 A.Blur(blur_limit=3, p=0.6),\n\n                A.OneOf([\n                    A.Blur(blur_limit=3, p=1.0),\n                    A.MedianBlur(blur_limit=3, p=1.0)\n                    ],\n                    p=0.5),\n\n                \n                A.Resize(\n                    height=self.model_resolution[0],\n                    width=self.model_resolution[1],\n                    p=1.0),\n                    \n                A.Cutout(\n                    num_holes=int(0.05 * np.prod(self.model_resolution) \/ (min(self.model_resolution)\/\/20)**2),\n                    max_h_size=min(self.model_resolution)\/\/20,\n                    max_w_size=min(self.model_resolution)\/\/20,\n                    fill_value=0,\n                    p=0.5),\n\n#                 ToTensorV2(p=1.0),\n                    \n            ],\n\n            p=1.0, \n            bbox_params=A.BboxParams(\n                format='pascal_voc',\n                min_area=0, \n                min_visibility=0,\n                label_fields=['cls']\n            )\n            )\n        \n        if self.sample_bbox_and_class:\n            self.TR_trn = A.Compose(\n            [\n\n                A.HorizontalFlip(p=0.5),\n\n                A.Rotate(\n                    limit=15,\n                    p=0.6,\n                ),\n\n#                     A.Transpose(p=0.5),\n\n                A.Blur(blur_limit=3, p=0.6),\n\n                A.Cutout(\n                    num_holes=10,\n                    max_h_size=5,\n                    max_w_size=5,\n                    fill_value=0,\n                    p=0.5),\n\n                A.Resize(\n                    height=self.model_resolution[0],\n                    width=self.model_resolution[1],\n                    p=1.0),\n\n#                 ToTensorV2(p=1.0),\n            ],\n\n            p=1.0, \n            bbox_params=A.BboxParams(\n                format='pascal_voc',\n                min_area=0, \n                min_visibility=0,\n                label_fields=['cls']\n            )\n            )\n            \n        \n        \n        self.TR_val = A.Compose(\n            [\n                A.Resize(\n                    height=self.model_resolution[0],\n                    width=self.model_resolution[1],\n                    p=1.0),\n\n#                 ToTensorV2(p=1.0),\n            ], \n\n            p=1.0, \n            bbox_params=A.BboxParams(\n                format='pascal_voc',\n                min_area=0, \n                min_visibility=0,\n                label_fields=['cls']\n            )\n        )\n\n        \n        return None\n\n    \n#     def get_GT_Dataframe(self, merge_mode='first'):\n#         \"\"\" Reeturns the fold GT Dataframe\"\"\"\n#         gt_df = self.bbox_df[self.bbox_df.image_id.isin(self.fold_samples)].copy()\n#         gt_df.fillna(0, inplace=True)\n#         gt_df.loc[gt_df[\"class_id\"] == 14, ['x_max', 'y_max']] = 1.0\n        \n        \n        \n#         gb = gt_df.groupby(['image_id', 'class_id'])\n\n#         if merge_mode == 'mean':\n#             gt_df = gb.agg(lambda x: ' '.join(np.unique(x.values)) if type(x.values[0]) is str else x.values.mean() ).reset_index()\n\n#         elif merge_mode == 'first':\n#             gt_df = gb.first().reset_index()\n\n#         elif merge_mode is None:\n#             gt_df = gt_df.copy()\n\n#         else:\n#             raise Exception(f'Mode \"{merge_mode}\" unknown.')\n        \n        \n#         if (self.select_classes_v is not None):\n#             selected_cls_v = self.select_classes_v\n#             if 14 in self.select_classes_v or -1 in self.select_classes_v:\n#                 selected_cls_v = self.select_classes_v + [14]\n            \n#             gt_df = gt_df[ gt_df.class_id.isin(selected_cls_v) ]\n#             gt_df.class_id = gt_df.class_id.map(lambda x: self.cls_orig2new_d[x])\n                    \n#         return gt_df\n    \n    \n    def get_GT_Dataframe(\n        self, \n        clean_mode='random',\n        clean_iou_th=0.1,\n        consensus_level=None):\n        \"\"\" Reeturns the fold GT Dataframe\"\"\"\n\n        if consensus_level is None:\n            consensus_level = self.consensus_level\n            \n        ret_d = {\n            'image_id': [],\n            'class_id': [],\n            'class_name': [],\n            'rad_id': [],\n\n            'x_min': [],\n            'y_min': [],\n            'x_max': [],\n            'y_max': [],\n        }\n\n        for sample_id in self.fold_samples:\n            sample_d = clean_predictions(\n                [self.bbox_d[sample_id]],\n                iou_th=clean_iou_th,\n                mode=clean_mode,\n                consensus_level=consensus_level,\n            )[0]\n\n\n            for i_c, rad_id, bboxes in zip(sample_d['cls'], sample_d['rad_id'], sample_d['bboxes']):\n                if i_c == -1:\n                    i_c = 14\n\n                ret_d['image_id'].append(sample_id)\n                ret_d['class_id'].append(i_c)\n                ret_d['class_name'].append( class2str_v[i_c] )\n                ret_d['rad_id'].append(' '.join(rad_id) )\n\n                if i_c == 14:\n                    ret_d['x_min'].append( 0.0 )\n                    ret_d['y_min'].append( 0.0 )\n                    ret_d['x_max'].append( 1.0 )\n                    ret_d['y_max'].append( 1.0 )\n\n                else:\n                    x_min, y_min, x_max, y_max = bboxes\n                    ret_d['x_min'].append( x_min )\n                    ret_d['y_min'].append( y_min )\n                    ret_d['x_max'].append( x_max )\n                    ret_d['y_max'].append( y_max )\n\n        gt_df = pd.DataFrame(ret_d)\n\n\n        if (self.select_classes_v is not None):\n            selected_cls_v = self.select_classes_v\n            if 14 in self.select_classes_v or -1 in self.select_classes_v:\n                selected_cls_v = self.select_classes_v + [14]\n\n            gt_df = gt_df[ gt_df.class_id.isin(selected_cls_v) ]\n            gt_df.class_id = gt_df.class_id.map(lambda x: self.cls_orig2new_d[x])\n\n\n        return gt_df\n    \n    def get_samples_wh_v(self, cls, all_samples=True):\n        ds = self\n\n        wh_v = []\n        img_h, img_w = self.model_resolution\n        \n        if all_samples:\n            samples_d_v = self.all_sample_ids\n        else:\n            samples_d_v = self.fold_samples\n            \n        for sample_id in samples_d_v:\n            sample_d = self.bbox_d[sample_id]\n            bboxes = sample_d['bboxes'][ sample_d['cls'] == cls ]\n\n            if bboxes.shape[0] > 0:\n                org_h, org_w = self.misc_d[sample_id]\n                scale = np.array( [img_w\/org_w, img_h\/org_h] )\n\n                wh = (bboxes[:,2:] - bboxes[:,:2]) * scale\n                wh_v.append(wh)\n\n        if len(wh_v) > 0:\n            wh_v = np.vstack(wh_v)\n        else:\n            wh_v = np.array([])\n\n        return wh_v\n    \n    def get_sample_idx_from_id(self, s_id):\n        return np.argwhere(self.fold_samples == s_id).T[0]\n\n\n    def plot_sample(self, idx=0, verbose=False, do_show=True):\n        sample = self[idx]\n\n        plt.figure(0, figsize=(20,20))\n\n        img = (255*sample['image'].transpose((1,2,0))).astype(np.uint8)\n\n        for cls, bbox in zip(sample['cls'], sample['bboxes']):\n            (x0, y0, x1, y1) = bbox.astype(np.int)\n            img = cv2.rectangle(img, (x0,y0), (x1, y1), class2color_v[cls] )\n            if verbose:\n                print(' - cls={cls}   bbox={bbox}')\n                \n        plt.imshow( img )\n        \n        if do_show:\n            plt.show()\n\n        return None\n\n\n\n    \n    def filter_radiologists(self, rad_id_v=['R13']):\n        self.fold_samples = self.selected_sample_ids[self.fold_sample_filter]\n        self._read_bboxes_df_or_cache()\n        self.bbox_df = self.bbox_df[self.bbox_df.rad_id.isin(rad_id_v)]\n\n        f_samples = np.ones(\n            self.fold_samples.shape[0],\n            dtype=np.bool)\n\n        for sample_id, bbox_d in self.bbox_d.items():\n            n_boxes = bbox_d['rad_id'].shape[0]\n\n            f = np.zeros(n_boxes, dtype=np.bool)\n            for rad_id in rad_id_v:\n                f += (bbox_d['rad_id'].T[0] == rad_id)\n\n            n_rads = f.sum()\n            if n_rads == 0:\n                i = np.argwhere(self.fold_samples == sample_id).T[0]\n\n                if i.shape[0] > 0:\n                    f_samples[i[0]] = False\n\n            if bbox_d['cls'][0] == -1:\n                if n_rads > 0:\n                    bbox_d['rad_id'] = bbox_d['rad_id'][f]\n                continue\n\n\n    #         print(bbox_d['rad_id'].T[0], f)\n            if n_rads == 0:\n                original_shape = self.misc_d[sample_id]\n                bbox_d['bboxes'] = np.array([ [0.0, 0.0, original_shape[1], original_shape[0] ]])\n                bbox_d['cls'] = np.array([-1])\n                bbox_d['rad_id'] = np.array([['NoRad']])\n\n            else:\n                for k in bbox_d.keys():\n                    bbox_d[k] = bbox_d[k][f]\n\n\n        self.fold_samples = self.fold_samples[f_samples]\n\n    #         print(sample_id)\n    #         print(bbox_d)\n    #         print(self.bbox_d[sample_id])\n    #         print()\n    #         break\n        return None\n\n\n    def filter_class(self, cls_v=[0]):\n        self.fold_samples = self.selected_sample_ids[self.fold_sample_filter]\n        self._read_bboxes_df_or_cache()\n        \n        self.bbox_df = self.bbox_df[\n            self.bbox_df.image_id.isin(\n                self.bbox_df[ self.bbox_df.class_id.isin(cls_v) ].image_id.unique() \n            )\n        ]\n\n        f_samples = np.zeros(\n            self.fold_samples.shape[0],\n            dtype=np.bool)\n        \n        \n        for i_sample, sample_id in enumerate(self.fold_samples):\n            bbox_d = self.bbox_d[sample_id]\n\n            for cls in cls_v:\n                if (cls in bbox_d['cls']):\n                    f_samples[i_sample] = True\n                    break\n\n        self.fold_samples = self.fold_samples[f_samples]\n\n        return None\n\n    \nds = FoldDataset(\n    ds_path='..\/input\/train-test-ds-bbox-cache\/',\n    ds_name='train',\n    images_dir=os.path.join(DS_DIR, 'train'),\n    model_resolution=(512, 512),\n    df_path=os.path.join(DS_DIR, 'train.csv'),\n    \n    mode='cv_trn',\n    i_fold=1,\n    n_folds=5,\n    test_split=0.1,\n    \n    do_augmentation=False,\n    downsample_factor=2,\n    remove_classes_v=[14],\n    select_classes_v=None,\n    show_warnings=False,\n    random_seed=3128,\n)\n\n\n# ds = FoldDataset(\n#     ds_path=DS_PATH,\n#     ds_name='train',\n#     images_dir=os.path.join(DS_DIR, 'train'),\n#     model_resolution=(512, 512),\n#     df_path=os.path.join(DS_DIR, 'train.csv'),\n    \n#     mode='cv_val',\n#     i_fold=1,\n#     n_folds=5,\n#     test_split=0.1,\n    \n#     do_augmentation=False,\n#     downsample_factor=DOWNSAMPLE_FACTOR,\n#     remove_classes_v=[],\n#     select_classes_v=None,\n#     reclass_samples = True,\n#     show_warnings=False,\n#     random_seed=3128,\n    \n#     clean_boxes=True,\n#     clean_mode='random',\n#     clean_iou_th=0.1,\n    \n#     consensus_level=2,\n# )\n\n# len(ds)","b2357dd5":"def cos_decay(start_val=1.0, end_val=1e-4, steps=100):\n    return lambda x: ((1 - np.cos(x * np.pi \/ steps)) \/ 2) * (end_val - start_val) + start_val\n\ndef linear_warmup(start_val=1e-4, end_val=1.0, steps=5):\n    return lambda x: x \/ steps * (end_val - start_val) + start_val  # linear\n\ndef scheduler_lambda(lr_frac=1e-4, warmup_epochs=5, cos_decay_epochs=60):\n    if warmup_epochs > 0:\n        lin = linear_warmup(start_val=lr_frac, end_val=1.0, steps=warmup_epochs)\n        \n    cos = cos_decay(start_val=1.0, end_val=lr_frac, steps=cos_decay_epochs)\n    \n    def f(x):\n        if x < warmup_epochs:\n            return lin(x)\n        \n        elif x <= (warmup_epochs + cos_decay_epochs):\n            return cos(x - warmup_epochs)\n        \n        else:\n            return lr_frac\n        \n    return f\n\n\n\nclass ModelX(nn.Module):\n    def __init__(\n        self,\n        model_resolution=(768, 512), \n        n_input_channels=3,\n        n_classes=14, # Not counting background\n        n_extras=0,\n        extra_loss_weight=1.0,\n\n        init_lr=1e-3,\n        use_scheduler=True,\n        n_warmup_epochs=5,\n        n_decay_epochs=60,\n        lr_prop=1e-4,\n        \n        n_steps_grad_update=1,\n        optimizer_name='adam',\n        clip_grad_norm=5.0,\n        weight_decay=0.0,\n        \n        use_pretrained_model=True,\n        \n        backbone_name='yolo',#'tf_efficientdet_d4',\n        trainable_backbone_layers=3,\n        \n        anchors_d=None,\n        \n        start_ckpt=None,\n        \n        checkpoint_base_path='.\/model_checkpoint',\n        model_name=' ModelX_v1',\n        device=None,\n        parallelize_backbone=False,\n        ):\n        \n        super().__init__()\n        \n        self.model_resolution = np.array(model_resolution) \n        self.n_input_channels = n_input_channels\n        self.n_classes        = n_classes\n        self.n_extras         = n_extras\n        self.extra_loss_weight = extra_loss_weight\n        \n        self.lr               = init_lr\n        self.use_scheduler    = use_scheduler\n        self.n_warmup_epochs  = n_warmup_epochs\n        self.n_decay_epochs   = n_decay_epochs\n        self.lr_prop          = lr_prop\n    \n        \n        self.optimizer_name   = optimizer_name\n        self.clip_grad_norm   = clip_grad_norm\n        self.weight_decay     = weight_decay\n        self.n_steps_grad_update = n_steps_grad_update\n        \n        self.use_pretrained_model = use_pretrained_model\n        self.backbone_name        = backbone_name.lower()\n        \n        self.trainable_backbone_layers = trainable_backbone_layers\n        \n        self.checkpoint_base_path = checkpoint_base_path\n        self.model_name           = model_name\n        \n        self.parallelize_backbone = parallelize_backbone\n        \n        self.use_effdet = False\n        self.use_yolo = False\n        \n        self.start_ckpt = start_ckpt\n        \n        self.anchors_d = anchors_d\n        \n        \n        self.do_bbox_cleaning = True\n        \n        print(f'New Model: \"{self.model_name}\"')\n        \n        if self.parallelize_backbone:\n            print(' - Setting main device = cuda:0')\n            device = 'cuda:0'\n            \n        self.optimizer = None\n        self.scheduler = None\n        # Seting device\n        self.set_device(device)\n        \n        # Creating output dir\n        if not os.path.exists(self.checkpoint_base_path):\n            print(f'Creating save dir: \"{self.checkpoint_base_path}\"')\n            os.makedirs(self.checkpoint_base_path)        \n        \n        \n        # Building Backbone\n        self._build_backbone()\n        \n        \n        # Moving model to device\n        self.to(self.device)\n        \n        if self.start_ckpt is not None:\n            self.restore_checkpoint(self.start_ckpt)\n        \n        if self.parallelize_backbone:\n            self._parallelize_backbone()\n         \n        \n        # Model Summary\n        self.calc_total_weights()\n        \n        \n        # Building Optimizers\n        self.build_optimizer()   \n        \n        return None\n    \n    \n    @torch.jit.ignore\n    def get_trainable_weights(self, verbose=True):\n        \n        if self.use_effdet:\n            use_extra_net = ('extra_net' in dir(self.backbone)) and (self.backbone.extra_net is not None)\n            \n            if self.parallelize_backbone:\n                trainable_params_v = sum([\n\n                    list( self.backbone.backbone.module.conv_stem.parameters() ),\n                    list( self.backbone.backbone.module.bn1.parameters() ),\n\n                    list( self.backbone.fpn.parameters() ),\n                    list( self.backbone.class_net.parameters() ),\n                    list( self.backbone.box_net.parameters() ),\n                    list( self.backbone.extra_net.parameters() ) if use_extra_net else [],\n                ], [])\n\n            else:\n                trainable_params_v = sum([\n\n                    list( self.backbone.backbone.conv_stem.parameters() ),\n                    list( self.backbone.backbone.bn1.parameters() ),\n\n                    list( self.backbone.fpn.parameters() ),\n                    list( self.backbone.class_net.parameters() ),\n                    list( self.backbone.box_net.parameters() ),\n                    list( self.backbone.extra_net.parameters() ) if use_extra_net else [],\n                ], [])\n                \n                trainable_params_v\n\n        else:\n            trainable_params_v = [p for p in self.parameters() if p.requires_grad ]\n        \n        if verbose:\n            n_w = 0\n            for p in trainable_params_v:\n                n_w += np.prod(p.shape)\n\n            print(f' - Total trainable weights: {n_w\/1e6:0.03} M')\n\n            \n        return trainable_params_v\n    \n    \n    \n    @torch.jit.ignore\n    def build_optimizer(self, params_v=None):\n        if params_v is None:\n            params_v = self.get_trainable_weights(verbose=False)\n#             params_v = self.parameters()\n        \n        param_id_v = [id(p) for p in params_v]\n        \n        if self.optimizer_name.lower() == 'adam':\n            if self.weight_decay > 0.0:\n                pg0, pg1, pg2 = [], [], []  # optimizer parameter groups\n                for k, v in self.named_modules():\n                    if hasattr(v, 'bias') and isinstance(v.bias, nn.Parameter):\n                        if id(v.bias) in param_id_v:\n                            pg2.append(v.bias)  # biases, no decay\n                        else:\n                            v.bias.requires_grad_(False)\n                            \n                    if isinstance(v, nn.BatchNorm2d):\n                        if id(v.weight) in param_id_v:\n                            pg0.append(v.weight)  # weights, no decay\n                        else:\n                            v.weight.requires_grad_(False)\n                        \n                    elif hasattr(v, 'weight') and isinstance(v.weight, nn.Parameter):\n                        if id(v.weight) in param_id_v:\n                            pg1.append(v.weight)  # weights, decay\n                        else:\n                            v.weight.requires_grad_(False)\n\n                # Weights Nodecay\n                self.optimizer = optim.Adam(pg0, lr=self.lr)\n                \n                # Weights Decay\n                self.optimizer.add_param_group({'params': pg1, 'weight_decay': self.weight_decay})\n                \n                # Biasese NoDecay\n                self.optimizer.add_param_group({'params': pg2})\n\n                del pg0, pg1, pg2\n\n            else:   \n                self.optimizer = optim.Adam(\n                    params_v,\n                    lr=self.lr,\n                    weight_decay=self.weight_decay,\n                )\n            \n        else:\n            raise Exception(f'Un implemented optimizer: {self.optimizer_name}')\n        \n    \n        self.optimizer.zero_grad()\n        self.opt_step = 0\n        \n        if self.use_scheduler:\n            self._build_scheduler()\n            \n        \n        if self.use_yolo:\n            self.yolo_compute_loss = ComputeLoss(\n                self.backbone,\n                autobalance=False)\n        \n        self.get_trainable_weights(verbose=True)\n        return self.optimizer\n    \n    \n    @torch.jit.ignore\n    def _build_scheduler(self, LAST_EPOCH=None):\n        \n        self.lr_lf = scheduler_lambda(\n            lr_frac=self.lr_prop,\n            warmup_epochs=self.n_warmup_epochs,\n            cos_decay_epochs=self.n_decay_epochs)\n\n        self.scheduler = lr_scheduler.LambdaLR(\n            self.optimizer,\n            lr_lambda=self.lr_lf)\n    \n        if LAST_EPOCH is not None:\n            print(' - Scheduler, Setting last_epoch =', LAST_EPOCH)\n            self.scheduler.last_epoch = LAST_EPOCH-1\n            self.scheduler.step()\n            \n        return None\n        \n        \n        \n    @torch.jit.ignore\n    def scheduler_step(self, verbose=True):\n        if self.use_scheduler:\n            self.scheduler.step()\n            \n        else:\n            print(' - WARNING: scheduler not configured.', file=sys.stderr)\n            \n        lr = self.get_lr()\n        if self.use_scheduler and verbose:\n            print(f' - Scheduler: LR={lr:0.02e}')\n        return lr\n    \n    \n    @torch.jit.ignore\n    def yolo_check_anchors(self, wh_v, anchors=None, thr=4.0):\n        imgsz = max(self.model_resolution)\n\n        if anchors is None:\n            anchors = self.yolo_get_anchors()\n\n\n        def metric(k, wh):  # compute metric\n            r = wh[:, None] \/ k[None]\n            x = np.minimum(r, 1. \/ r).min(axis=2)  # ratio metric\n            best = x.max(1)  # best_x\n            aat = (x > 1. \/ thr).sum(axis=1).mean()  # anchors above threshold\n            bpr = (best > 1. \/ thr).mean()  # best possible recall\n            return bpr, aat\n\n        bpr, aat = metric( anchors, wh_v )\n\n        ret_d = {\n            'best_possible_recall':bpr,\n            'anchors_above_threshold':aat\n        }\n        return ret_d\n\n\n    @torch.jit.ignore\n    def yolo_get_anchors(self):\n        assert self.use_yolo\n\n        # Not implemented if Data Parallel\n        m = self.backbone.model[-1]\n\n        anchor_grid = m.anchor_grid.clone().cpu().view(-1, 2).numpy()\n    #     anchors = m.anchors.clone().cpu().numpy()\n    #     stride = m.stride.clone().cpu().numpy()\n\n    #     ret_d = {\n    #         'anchor_grid': anchor_grid,\n    #         'anchors': anchors,\n    #         'stride': stride,\n    #     }\n\n        return anchor_grid\n\n    @torch.jit.ignore\n    def yolo_set_anchors(self, new_anchors):\n        assert self.use_yolo\n\n        # Not implemented if Data Parallel\n        m = self.backbone.model[-1]\n\n        def check_anchor_order(m):\n            # Check anchor order against stride order for YOLOv5 Detect() module m, and correct if necessary\n            a = m.anchor_grid.prod(-1).view(-1)  # anchor area\n            da = a[-1] - a[0]  # delta a\n            ds = m.stride[-1] - m.stride[0]  # delta s\n            if da.sign() != ds.sign():  # same order\n                print('Reversing anchor order')\n                m.anchors[:] = m.anchors.flip(0)\n                m.anchor_grid[:] = m.anchor_grid.flip(0)\n\n\n        new_anchors = torch.tensor(new_anchors, device=m.anchors.device).type_as(m.anchors)\n        m.anchor_grid[:] = new_anchors.clone().view_as(m.anchor_grid)  # for inference\n        m.anchors[:] = new_anchors.clone().view_as(m.anchors) \/ m.stride.to(m.anchors.device).view(-1, 1, 1)  # loss\n\n        check_anchor_order(m)\n        return None\n\n\n    @torch.jit.ignore\n    def yolo_kmean_anchors(self, wh_v,thr=4.0, gen=1000, verbose=False, do_plot=False):\n        from scipy.cluster.vq import kmeans\n\n        \"\"\" Creates kmeans-evolved anchors from training dataset\n\n            Arguments:\n                wh_v: labels wh\n                thr: anchor-label wh ratio threshold hyperparameter hyp['anchor_t'] used for training, default=4.0\n                gen: generations to evolve anchors using genetic algorithm\n                verbose: print all results\n\n            Return:\n                k: kmeans evolved anchors\n\n            Usage:\n                from utils.autoanchor import *; _ = kmean_anchors()\n        \"\"\"\n        \n        assert self.use_yolo\n\n        # Not implemented if Data Parallel\n        m = self.backbone.model[-1]\n        n = m.anchor_grid.numel() \/\/ 2  # number of anchors\n\n        \n        img_size = max(self.model_resolution)\n\n        thr = 1. \/ thr\n        prefix = 'autoanchor: '\n\n        def metric(k, wh):  # compute metrics\n            r = wh[:, None] \/ k[None]\n            x = np.minimum(r, 1.\/r).min(axis=2)  # ratio metric\n            # x = wh_iou(wh, np.array(k))  # iou metric\n            return x, x.max(axis=1)  # x, best_x\n\n        def anchor_fitness(k):  # mutation fitness\n            _, best = metric(np.array(k), wh)\n            return (best * (best > thr)).mean()  # fitness\n\n        def print_results(k):\n\n            x, best = metric(k, wh0)\n            bpr, aat = (best > thr).mean(), (x > thr).mean() * n  # best possible recall, anch > thr\n            print(f'{prefix}thr={thr:.2f}: {bpr:.4f} best possible recall, {aat:.2f} anchors past thr')\n            print(f'{prefix}n={n}, img_size={img_size}, metric_all={x.mean():.3f}\/{best.mean():.3f}-mean\/best, '\n                  f'past_thr={x[x > thr].mean():.3f}-mean: ', end='')\n\n            for i, x in enumerate(k):\n                print('%i,%i' % (round(x[0]), round(x[1])), end=',  ' if i < len(k) - 1 else '\\n')  # use in *.cfg\n            return k\n\n\n        # Get label wh\n        wh0 = wh_v.copy()  # wh\n\n        # Filter\n        i = (wh0 < 3.0).any(1).sum()\n        if i and verbose:\n            print(f'{prefix}WARNING: Extremely small objects found. {i} of {len(wh0)} labels are < 3 pixels in size.')\n\n        wh = wh0[(wh0 >= 2.0).any(1)]  # filter > 2 pixels\n        # wh = wh * (np.random.rand(wh.shape[0], 1) * 0.9 + 0.1)  # multiply by random scale 0-1\n\n        # Kmeans calculation\n        if verbose:\n            print(f'{prefix}Running kmeans for {n} anchors on {len(wh)} points...')\n\n        s = wh.std(0)  # sigmas for whitening\n        k, dist = kmeans(wh \/ s, n, iter=30)  # points, mean distance\n        k *= s\n        wh = np.array(wh)  # filtered\n        wh0 = np.array(wh0)  # unfiltered\n\n        k = k[np.argsort(k.prod(1))]  # sort small to large\n        if verbose:\n            k = print_results(k)\n\n        # Evolve\n        npr = np.random\n        f, sh, mp, s = anchor_fitness(k), k.shape, 0.9, 0.1  # fitness, generations, mutation prob, sigma\n        if verbose:\n            pbar = tqdm(range(gen), desc=f'{prefix}Evolving anchors with Genetic Algorithm:')  # progress bar\n        else:\n            pbar = range(gen)\n        for _ in pbar:\n            v = np.ones(sh)\n            while (v == 1).all():  # mutate until a change occurs (prevent duplicates)\n                v = ((npr.random(sh) < mp) * npr.random() * npr.randn(*sh) * s + 1).clip(0.3, 3.0)\n            kg = (k.copy() * v).clip(min=2.0)\n            fg = anchor_fitness(kg)\n            if fg > f:\n                f, k = fg, kg.copy()\n                if verbose:\n                    pbar.desc = f'{prefix}Evolving anchors with Genetic Algorithm: fitness = {f:.4f}'\n                    print_results(k)\n\n        ret_k = k[np.argsort(k.prod(1))]  # sort small to large\n\n        if verbose:\n            print_results(ret_k)\n\n        # Plot\n        if do_plot:\n            k_v, d_v = [None] * 20, [None] * 20\n            for i in range(1, 21):\n                k, d = kmeans(wh \/ s, i)  # points, mean distance\n\n                k_v[i-1] = s * k\n                d_v[i-1] = s * d\n\n            fig, ax = plt.subplots(1, 2, figsize=(14, 7), tight_layout=True)\n            for n_k, k in enumerate(k_v):\n                ax[0].plot(k[:,0], k[:,1], 'o', label=f'k={n_k+1}')\n\n            ax[0].set_xlabel('W sentroid')\n            ax[0].set_ylabel('H sentroid')\n            ax[0].set_xlim( (0, img_size) )\n            ax[0].set_ylim( (0, img_size) )\n            ax[0].set_title('Points: k_Mean')\n\n            ax[1].plot(np.arange(1,21), d_v, marker='.')\n            ax[1].set_xlabel('Number of sentroids')\n            ax[1].set_ylabel('ErrMeanDistance')\n            ax[1].set_title('Points: ErrMeanDistance')\n\n            plt.show()\n\n\n            # plot wh\n            fig, ax = plt.subplots(1, 2, figsize=(14, 7), tight_layout=True)\n            ax[0].hist(wh_v[:, 0], bins=100, range=(0,img_size))\n            ax[0].set_title('W dist')\n            ax[1].hist(wh_v[:, 1], bins=100, range=(0,img_size))\n            ax[1].set_title('H dist')\n\n            plt.show()\n\n\n        return ret_k.astype(np.float32)\n\n    @torch.jit.ignore\n    def _build_backbone(self):\n        \n        if 'efficientdet' in self.backbone_name:\n            self.use_effdet = True\n            \n            self.backbone_config = get_efficientdet_config(self.backbone_name)\n            self.backbone_config.num_classes = self.n_classes\n            self.backbone_config.image_size  = tuple([int(i) for i in self.model_resolution])\n            self.backbone_config.extra_variables = self.n_extras\n            self.backbone_config.extra_loss_weight = self.extra_loss_weight\n\n            self.backbone = EfficientDet(\n                self.backbone_config,\n                pretrained_backbone=self.use_pretrained_model)\n\n            if self.n_input_channels != 3:\n                first_conv = self.backbone.backbone.conv_stem\n\n                # Updating first ConvHead\n                self.backbone.backbone.conv_stem = nn.Conv2d(\n                    self.n_input_channels,\n                    first_conv.out_channels,\n                    kernel_size=first_conv.kernel_size,\n                    stride=first_conv.stride,\n                    padding=first_conv.padding,\n                    bias=False,\n                )\n\n                # Deleting unused conv\n                del(first_conv)\n\n            self.net_labeler_train   = DetBenchTrain(self.backbone)\n            self.net_labeler_predict = DetBenchPredict(self.backbone)\n            \n        elif self.backbone_name == 'fasterrcnn_resnet50_fpn':\n                self.backbone = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n                    pretrained=False,\n                    progress=True,\n                    num_classes=self.n_classes+1,\n                    pretrained_backbone=self.use_pretrained_model,\n                    trainable_backbone_layers=self.trainable_backbone_layers)\n            \n        elif 'fasterrcnn_' in self.backbone_name:\n\n            self.layers_d = {}\n\n            timm_backbone = timm.create_model(\n                model_name=self.backbone_name.replace('fasterrcnn_', ''),\n                pretrained=self.use_pretrained_model,\n                num_classes=0,\n                in_chans=self.n_input_channels,\n            )\n\n\n            assert timm_backbone.global_pool is not None, 'ERROR, there is not a global_pool layer'\n            timm_backbone.global_pool = nn.Identity()        \n\n            self.return_layers = OrderedDict()\n            self.in_channels_list = []\n            featmap_names = []\n            i_l = 0\n            for layer_info_d in timm_backbone.feature_info:\n                layer_name = layer_info_d['module']\n                layer_chs = layer_info_d['num_chs']\n\n                if 'layer' not in layer_name:\n                    continue\n\n                self.in_channels_list.append(layer_chs)\n                self.return_layers[layer_name] = f'{i_l}'\n                featmap_names.append(f'{i_l}')\n\n                i_l += 1\n\n            featmap_names.append('pool')\n            assert len(featmap_names)  > 0\n\n            self.n_featmap_layers = len(featmap_names)\n\n            N_OUT_CHANNELS_FPN = 256\n\n            fpn_backbone = BackboneWithFPN(\n                timm_backbone, \n                return_layers=self.return_layers,\n                in_channels_list=self.in_channels_list, \n                out_channels=N_OUT_CHANNELS_FPN,\n                extra_blocks=None)\n\n            \n            if self.anchors_d is None:\n                ANCHOR_SIZES  = ( (16,), (32,), (64,), (128,), (256,) )\n#                 ANCHOR_SIZES  = ( (8,16), (32,64), (64,128), (128,256), (256,448) )\n                ASPECT_RATIOS = ( (0.33, 0.5, 1.0, 2.0, 3.0), ) * len(ANCHOR_SIZES)\n            else:\n                ANCHOR_SIZES = self.anchors_d['ANCHOR_SIZES']\n                ASPECT_RATIOS = self.anchors_d['ASPECT_RATIOS']\n                \n                print(' - Using custom anchors: ')\n                print(' |-> ANCHOR_SIZES:',  ANCHOR_SIZES)\n                print(' |-> ASPECT_RATIOS:', ASPECT_RATIOS)\n                \n            \n            anchor_generator = AnchorGenerator(\n                    sizes=ANCHOR_SIZES,\n                    aspect_ratios=ASPECT_RATIOS,\n                )\n\n            roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n                featmap_names=featmap_names,\n                output_size=7,\n                sampling_ratio=2)\n\n            self.backbone = FasterRCNN(\n                fpn_backbone,\n                num_classes=self.n_classes+1,\n                rpn_anchor_generator=anchor_generator,\n                box_roi_pool=roi_pooler)\n\n\n            self.layers_d['timm_backbone'] = timm_backbone\n            self.layers_d['fpn_backbone'] = fpn_backbone\n            self.layers_d['anchor_generator'] = anchor_generator\n            self.layers_d['roi_pooler'] = roi_pooler\n\n\n        elif 'yolo' in self.backbone_name:\n            self.use_yolo = True\n            self.backbone = torch.hub.load(\n                'ultralytics\/yolov5',\n                self.backbone_name,\n                pretrained=self.use_pretrained_model,\n                channels=self.n_input_channels,\n                classes=self.n_classes, #+1, # First Class will be background\n                autoshape=False,\n            )\n\n            # Model parameters\n            nc = self.n_classes\n            imgsz = max(self.model_resolution)\n\n            nl = self.backbone.model[-1].nl\n\n            hyp = {}\n            hyp['box'] = 0.03 * 100\n            hyp['cls'] = 0.25 * 100\n            hyp['obj'] = 1.00 * 100\n\n            hyp['cls_pw'] = 0.60\n            hyp['obj_pw'] = 0.90\n\n            hyp['fl_gamma'] = 3.0\n\n            hyp['box'] *= 3. \/ nl  # scale to layers\n            hyp['cls'] *= nc \/ 80. * 3. \/ nl  # scale to classes and layers\n            hyp['obj'] *= (imgsz \/ 640) ** 2 * 3. \/ nl  # scale to image size and layers\n            hyp['anchor_t'] = 2.0\n\n            self.backbone.hyp = hyp\n            self.backbone.gr = 1.0\n            \n            \n            for i, (n,p) in enumerate(self.named_parameters()):\n                p.requires_grad_(True)\n                \n                \n                \n        else:\n            raise Exception('Unknown backbone name: \"{}\"'.format(self.backbone_name))\n\n        return None\n    \n    \n    \n    def forward(self, *args):\n        return self.backbone(*args)\n    \n    @torch.jit.ignore\n    def get_epoch_from_ckpt_path(self, ckpt_path):\n        epoch = None\n        for s in os.path.split( ckpt_path )[-1].split('_'):\n            if len(s) > 0 and s[0].upper() == 'E':\n                try:\n                    epoch = int(s[1:])\n                    \n                    break\n                except:\n                    continue\n\n        return epoch\n    \n    @torch.jit.ignore\n    def find_last_saved_ckpt(self):\n        all_path_v = []\n        for ckpt_path in glob.glob(os.path.join(self.checkpoint_base_path, '*.ckpt')):\n            \n            epoch = self.get_epoch_from_ckpt_path(ckpt_path)\n            all_path_v.append( (epoch, ckpt_path) )\n\n        if len(all_path_v) > 0:\n            all_path_v.sort(key=lambda l:l[0])\n            return all_path_v[-1]\n        else:\n            return None, None\n        \n        \n        \n    @torch.jit.ignore\n    def save_checkpoint(\n        self,\n        step=0,\n        loss=None,\n        file_name='weights.ckpt',\n        verbose=True,\n        ):\n        \n        \n        base_path = self.checkpoint_base_path\n        \n        PATH = os.path.join(base_path, file_name)\n        \n        torch.save({\n            'step': step,\n            'model_state_dict':     self.state_dict(),\n            'optimizer_state_dict': self.optimizer.state_dict() if self.optimizer is not None else None,\n            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler is not None else None,\n            'loss': loss,\n            }, PATH)\n        \n        if verbose:\n            print(f' Saved checkpoint: {PATH}.')\n            \n        return PATH\n    \n    @torch.jit.ignore\n    def restore_checkpoint(\n        self, \n        PATH=None, \n        load_optimizer=True,\n        load_scheduler=True,\n        verbose=True):\n        \n        \n        if PATH is None:\n            print('Restoring last epoch.')\n            LAST_EPOCH, PATH = model.find_last_saved_ckpt()\n            \n        else:\n            LAST_EPOCH = None\n        \n        \n        checkpoint = torch.load(\n            PATH,\n            map_location=self.device,)\n        \n\n        if 'model_state_dict' in checkpoint.keys():\n            saved_state_dict = checkpoint['model_state_dict']\n            \n        else:\n            saved_state_dict = checkpoint\n            \n        current_state_dict = self.state_dict()\n        new_state_dict = OrderedDict()\n        for key in current_state_dict.keys():\n            if (key in saved_state_dict.keys()) and (saved_state_dict[key].shape == current_state_dict[key].shape):\n                new_state_dict[key] = saved_state_dict[key]\n\n            else:\n                load_optimizer = False\n                if key not in saved_state_dict.keys():\n                    print(f' - WARNING: key=\"{key}\" not found in saved checkpoint.\\n   Weights will not be loaded.', file=sys.stderr)\n                else:\n                    s0 = tuple(saved_state_dict[key].shape)\n                    s1 = tuple(current_state_dict[key].shape)\n                    print(f' - WARNING: shapes mismatch in \"{key}\": {s0} vs {s1}.\\n   Weights will not be loaded.', file=sys.stderr)\n                new_state_dict[key] = current_state_dict[key]\n        \n        self.load_state_dict( new_state_dict )\n        \n        if self.optimizer is not None:\n            if load_optimizer:\n                try:\n                    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n                except Exception as e:\n                    print(' - WARNING: ERROR while loading the optimizer. The optimizer will be reseted.', file=sys.stderr)\n                    self.build_optimizer()\n            else:\n                print(' - WARNING: Optimizer will not be loaded.', file=sys.stderr)\n                \n                \n                \n        if self.scheduler is not None:\n            if load_scheduler:\n                try:\n                    self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n                except Exception as e:\n                    print(' - WARNING: ERROR while loading the scheduler. The Scheduler will be reseted.', file=sys.stderr)\n                    \n                    if LAST_EPOCH is None:\n                        LAST_EPOCH = self.get_epoch_from_ckpt_path(PATH)\n                        \n                    self._build_scheduler(LAST_EPOCH=LAST_EPOCH)\n                    \n            else:\n                print(' - WARNING: Scheduler will not be loaded.', file=sys.stderr)\n                \n        if verbose:\n            print(f' - Restored checkpoint: {PATH}.')\n        \n        return checkpoint \n    \n    @torch.jit.ignore\n    def calc_total_weights(self, verbose=True):\n        n_w = 0\n        for p in self.parameters():\n            n_w += np.prod(p.shape, dtype=np.int)\n        \n        if verbose:\n            print(' - Total weights: {:0.02f}M'.format(n_w\/1e6))\n        \n        return n_w\n    \n    \n    @torch.jit.export\n    def flip(self, tensor, dim=1):\n        \"\"\" Just flip a tensro dim.\"\"\"\n        fliped_idx    = torch.arange(tensor.size(dim)-1, -1, -1).long().to(self.device)\n        fliped_tensor = tensor.index_select(dim, fliped_idx)\n        return fliped_tensor\n    \n    @torch.jit.ignore\n    def set_device(self, device=None):\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        elif type(device) == str:\n            self.device = torch.device( device )\n        else:\n            raise Exception('Not implemented')\n            \n        print(f' - Selecting device: {self.device}')\n        return self.device\n    \n    \n    @torch.jit.ignore\n    def unscale_bboxes(self, bboxes, original_shape):\n        \n        model_img_h, model_img_w = self.model_resolution\n        orig_img_h, orig_img_w   = original_shape\n\n        scale_h = model_img_h \/ orig_img_h\n        scale_w = model_img_w \/ orig_img_w\n\n        # x0, y0, x1, y1\n        scale_v = np.array([scale_w, scale_h, scale_w, scale_h])\n\n        bboxes = bboxes \/ scale_v\n        \n        return bboxes\n        \n        \n    @torch.jit.ignore\n    def clean_bboxes(self, pred_d, original_shape=None):\n        \n        EPS = 1e-8\n        if len(pred_d['bbox']) == 0:\n            return None\n            \n        if original_shape is not None:\n            y_max, x_max = original_shape\n            \n        else:\n            y_max, x_max = self.model_resolution \n        \n\n        pred_d['bbox'] = np.clip(\n            pred_d['bbox'],\n            np.array([EPS, EPS, EPS, EPS], dtype=pred_d['bbox'].dtype),\n            np.array([x_max-EPS, y_max-EPS, x_max-EPS, y_max-EPS], dtype=pred_d['bbox'].dtype) )\n\n        f = ((pred_d['bbox'][:, 2:] - pred_d['bbox'][:, :2]) <= 0.0).any(axis=-1)\n        if f.any():\n            nf = ~f\n            pred_d['bbox']  = pred_d['bbox'][nf]\n            pred_d['cls']   = pred_d['cls'][nf]\n            pred_d['p_det'] = pred_d['p_det'][nf]\n\n        return None\n    \n    \n    @torch.jit.ignore\n    def get_bboxes(self, outputs, det_th, data=None, unscale_bboxes=False):\n        preds_v = []\n        \n        if data is not None:\n            assert len(data['sample_id']) == outputs.shape[0]\n            \n        for i_sample, img_pred in enumerate(outputs.detach().cpu().numpy()):\n            \n            bboxes, p_det, idx_class = np.split(img_pred, [4,5], axis=-1)\n            f_det = p_det[:,0] > det_th\n            \n            if unscale_bboxes:\n                original_shape = data['original_shape'][i_sample]\n                bboxes = self.unscale_bboxes(bboxes, original_shape)\n            else:\n                original_shape = None\n            \n            \n            pred_d = {\n                'bbox': bboxes[f_det],\n                'p_det': p_det[f_det, 0],\n                'cls':  idx_class[f_det, 0].astype(int) - 1, # We must substract 1 to the class number\n            }\n            \n            if self.do_bbox_cleaning:\n                self.clean_bboxes(\n                    pred_d,\n                    original_shape\n                )\n            \n            preds_v.append(pred_d)\n            \n        return preds_v\n    \n    \n    \n    def xyxy2nxywh(self, x, ret_array=None):\n        if ret_array is None:\n            x_ret = torch.zeros(x.shape, dtype=torch.float32, device=self.device)\n        else:\n            assert (ret_array.shape == x.shape)\n            x_ret = ret_array\n\n\n        x_ret[:,0] = (x[:,0] + x[:,2]) \/ (2*self.model_resolution[1])\n        x_ret[:,1] = (x[:,1] + x[:,3]) \/ (2*self.model_resolution[0])\n        x_ret[:,2] = (x[:,2] - x[:,0]) \/ self.model_resolution[1]\n        x_ret[:,3] = (x[:,3] - x[:,1]) \/ self.model_resolution[0]\n\n        return x_ret \n\n    def nxywh2xyxy(self, x, ret_array=None):\n        if ret_array is None:\n            x_ret = torch.zeros(x.shape, dtype=torch.float32, device=self.device)\n        else:\n            assert (ret_array.shape == x.shape)\n            x_ret = ret_array\n\n        x_ret[:,0] = (x[:,0] - 0.5 * x[:,2]) * self.model_resolution[1]\n        x_ret[:,1] = (x[:,1] - 0.5 * x[:,3]) * self.model_resolution[0]\n        x_ret[:,2] = (x[:,0] + 0.5 * x[:,2]) * self.model_resolution[1]\n        x_ret[:,3] = (x[:,1] + 0.5 * x[:,3]) * self.model_resolution[0]\n\n        return x_ret\n    \n    \n    def yolo_build_target(self, data):\n        sample_icxywh_v = []\n        for i_s, (bboxes, cls) in enumerate( zip(data['bboxes'], data['cls']) ):\n            f = (cls >= 0)  # filtering background\n            bboxes = bboxes[f]\n            cls    = cls[f]\n            \n            n_boxes = len(bboxes)\n            \n            sample_icxywh = torch.empty(\n                (n_boxes, 6),\n                dtype=torch.float32,\n                device=self.device,\n            )\n            sample_icxywh[:, 0] = i_s\n            sample_icxywh[:, 1] = cls # + 1 # Class 0 will be background\n\n            _ = self.xyxy2nxywh(\n                bboxes,\n                ret_array=sample_icxywh[:,2:]\n            )\n\n            sample_icxywh_v.append(sample_icxywh)\n        \n        target = torch.vstack(sample_icxywh_v)\n        return target\n\n\n\n    @torch.jit.ignore\n    def predict(\n        self,\n        data,\n        det_th=0.4, \n        output_losses=False,\n        training=False,\n        filter_boxes=True,\n        unscale_bboxes=False,\n        yolo_iou_thres=1.0,\n    ):\n        \n        if training:\n            self.train()\n            torch.set_grad_enabled(True)\n            \n        else:\n            self.eval()\n            torch.set_grad_enabled(False)\n        \n        if type(data['image']) is not torch.Tensor:\n            data = data2tensor(data, device=self.device)\n            \n        else:\n            for k in data:\n                if data[k] is torch.Tensor:\n                    data[k].to(self.device)\n                    \n            \n        images = data['image']\n        if self.use_effdet:\n            if output_losses:\n                target_d = {\n                    'bbox': [x[:, [1,0,3,2]] for x in data['bboxes']],\n                    'cls':  [x + 1 for x in data['cls']],   # We must sum 1 to the class number\n                    'img_scale': None,\n                    'img_size': None,\n                }\n\n                if self.n_extras > 0:\n                    target_d['extra'] = [x for x in data['extra']]\n\n                # dict with: 'loss', 'class_loss', 'box_loss'\n                outputs = self.net_labeler_train.forward(\n                    images,\n                    target_d\n                )\n\n                if not training and filter_boxes:\n                    outputs['detections'] = self.get_bboxes(outputs['detections'], det_th, data, unscale_bboxes)\n\n            else:\n                outputs = self.net_labeler_predict.forward(\n                    images,\n                )\n\n                if filter_boxes:\n                    outputs = self.get_bboxes(outputs, det_th, data, unscale_bboxes)\n        \n        elif self.use_yolo:\n            outputs = None\n            if output_losses:\n                if not self.training:\n                    self.train()\n                    \n                yolo_target = self.yolo_build_target(data)\n                yolo_out = self(images)\n                \n                loss, (lbox, lobj, lcls, loss2) = self.yolo_compute_loss(\n                    yolo_out,\n                    yolo_target)\n                \n                outputs = {\n                    'loss': loss,\n                    'class_loss': lcls,\n                    'box_loss': lbox + lobj,\n                }\n            \n            if filter_boxes:\n                if self.training:\n                    self.eval()\n                \n                yolo_out = self(images)\n                \n                pred_v = non_max_suppression(\n                    yolo_out[0],\n                    conf_thres=det_th,\n                    iou_thres=yolo_iou_thres,\n                )\n                \n                outputs_v = []\n                for i_sample, p_matrix in enumerate(pred_v):\n                    p_matrix = p_matrix.detach().cpu().numpy()\n                    \n                    \n                    bbox   = p_matrix[:,:4]\n                    scores = p_matrix[:,4]\n                    cls    = p_matrix[:,5].astype(np.int) # - 1 # Class 0 will be background\n                    \n                    \n                    if unscale_bboxes:\n                        original_shape = data['original_shape'][i_sample]\n                        bbox = self.unscale_bboxes(\n                            bbox,\n                            original_shape)\n                        \n                    else:\n                        original_shape = None\n                \n                    f = (scores > det_th) # * (cls != -1)\n                    \n                    pred_d = {\n                            'bbox' : bbox[f],\n                            'cls': cls[f], \n                            'p_det': scores[f],\n                        }\n                    \n                    if self.do_bbox_cleaning:\n                        self.clean_bboxes(\n                            pred_d,\n                            original_shape,\n                        )\n                    \n                    outputs_v.append(\n                        pred_d\n                    )\n                    \n                    \n                if outputs is None:\n                    outputs = outputs_v\n                    \n                else:\n                    outputs['detections'] = outputs_v\n                    \n            \n        else:\n            outputs = None\n            \n            if output_losses:\n                if not self.training:\n                    self.train()\n                    \n                target = []\n                for cls, bboxes in zip( data['cls'], data['bboxes'] ):\n                    target.append(\n                        {\n                            'boxes': bboxes,\n                            'labels': cls + 1,# We must sum 1 to the class number\n\n                        }\n                    )\n    \n                loss_d = self(images, target)\n\n                outputs = {\n                    'loss': loss_d['loss_classifier'] + loss_d['loss_objectness'] + loss_d['loss_box_reg'] + loss_d['loss_rpn_box_reg'],\n                    'class_loss': loss_d['loss_classifier'],\n                    'box_loss': loss_d['loss_objectness'] + loss_d['loss_box_reg'] + loss_d['loss_rpn_box_reg'],\n                }\n            \n                \n            if filter_boxes:\n                if self.training:\n                    self.eval()\n                    \n                pred_v = self(images)\n                \n                outputs_v = []\n                for i_sample, pred_d in enumerate(pred_v):\n                    bbox   = pred_d['boxes'].detach().cpu().numpy()\n                    scores = pred_d['scores'].detach().cpu().numpy()\n                    cls    = pred_d['labels'].detach().cpu().numpy()- 1 # We must substract 1 to the class number\n                    \n                    \n                    if unscale_bboxes:\n                        original_shape = data['original_shape'][i_sample]\n                        bbox = self.unscale_bboxes(\n                            bbox,\n                            original_shape)\n                        \n                    else:\n                        original_shape = None\n                \n                    f = scores > det_th\n                    \n                    pred_d = {\n                            'bbox' : bbox[f],\n                            'cls': cls[f],\n                            'p_det': scores[f],\n                        }\n                    \n                    if self.do_bbox_cleaning:\n                        self.clean_bboxes(\n                            pred_d,\n                            original_shape,\n                        )\n                    \n                    outputs_v.append(\n                        pred_d\n                    )\n                    \n                    \n                        \n                if outputs is None:\n                    outputs = outputs_v\n                    \n                else:\n                    outputs['detections'] = outputs_v\n                \n        return outputs\n    \n    \n    @torch.jit.ignore\n    def predict_TTA(\n        self,\n        data,\n        det_th=0.00,\n        unscale_bboxes=True,\n        TTA_clean_iou_th=0.2,\n        TTA_clean_mode='median_pmean',\n        TTA_max_angle=5,\n        TTA_delta_angle=1,\n    ):\n\n\n        output_v = []\n        for i_b in range(data['image'].shape[0]):\n            tta_pred_v = []\n            model_id = 0\n            for angle in np.arange(0, TTA_max_angle+1, TTA_delta_angle):\n                for do_flop in [True, False]:\n                    aug_v = []\n                    un_aug_v = []\n                    if do_flop:\n                        aug_v.append( A.HorizontalFlip(p=1.0, always_apply=True) )\n\n                    aug_v.append( A.Rotate(p=1.0, limit=(angle,  angle), always_apply=True) )\n                    aug = A.Compose(\n                        aug_v,\n                        p=1.0)\n\n                    un_aug_v.append( A.Rotate(p=1.0, limit=(-angle,  -angle), always_apply=True) )\n                    if do_flop:\n                        un_aug_v.append( A.HorizontalFlip(p=1.0, always_apply=True) )\n\n                    un_aug = A.Compose(\n                        un_aug_v,\n                        bbox_params=A.BboxParams(\n                            format='pascal_voc',\n                            min_area=0, \n                            min_visibility=0,\n                            label_fields=['cls', 'p_det']),\n                        p=1.0)\n\n\n                    tta_image = aug(\n                        image=data['image'][i_b].transpose((1,2,0))\n                    )['image'].transpose((2,0,1))[None,...]\n\n                    data_tta = {\n                        'sample_id': data['sample_id'][i_b:i_b+1],\n                        'original_shape': data['original_shape'][i_b:i_b+1],\n                        'image': tta_image,\n                    }\n\n                    aug_pred_v = self.predict(\n                        data_tta,\n                        det_th=det_th,\n                        output_losses=False,\n                        training=False,\n                        filter_boxes=True,\n                        unscale_bboxes=unscale_bboxes,\n                    )\n\n\n                    orig_h, orig_w = data['original_shape'][i_b]\n\n                    data_untta = un_aug(\n                        image=np.ones( (orig_h, orig_w, 3) ),\n                        bboxes=aug_pred_v[0]['bbox'],\n                        cls=aug_pred_v[0]['cls'],\n                        p_det=aug_pred_v[0]['p_det'],\n                    )\n\n\n                    tta_pred_v.append(\n                        {\n                            'bbox':np.array(data_untta['bboxes']),\n                            'cls':np.array(data_untta['cls']),\n                            'p_det':np.array(data_untta['p_det']),\n                            'model_id': model_id * np.ones(len(data_untta['cls']), dtype=np.int)\n                        }\n                    )\n\n                    model_id += 1\n\n            tta_pred_d = {}\n            for k in tta_pred_v[0].keys():\n                tta_pred_d[k] = np.concatenate( [pred_d[k] for pred_d in tta_pred_v if len(pred_d[k]) > 0], axis=0)\n\n            output_v.append(tta_pred_d)\n\n        output_v = clean_predictions(\n            output_v,\n            iou_th=TTA_clean_iou_th,\n            mode=TTA_clean_mode,\n            consensus_level=1,\n            n_models2ensemble=model_id,\n        )\n\n        output_v = filter_det_th(\n            output_v,\n            det_th)\n\n        return output_v\n    \n    @torch.jit.ignore\n    def train_step(self, data):\n        \n        \n        outputs = self.predict(\n            data=data,\n            output_losses=True,\n            training=True,\n            filter_boxes=False\n        )\n        \n        loss = outputs['loss']\n        \n        if self.n_steps_grad_update != 1:\n            loss = loss \/ self.n_steps_grad_update\n        \n        # Backward pass\n        loss.backward()\n        \n        if self.clip_grad_norm > 0.0:\n            torch.nn.utils.clip_grad_norm_(\n                self.parameters(),\n                self.clip_grad_norm)\n        \n        if self.opt_step != 0 and (self.opt_step % self.n_steps_grad_update) == 0:\n            self.optimizer.step()\n            self.optimizer.zero_grad()\n        \n#         trn_batch_loss = loss.item()\n        \n        self.opt_step += 1\n        return outputs\n\n\n    @torch.jit.ignore\n    def put_boxes(self, preds_v, data, do_plot=False):\n        BS = len( preds_v )\n\n        assert BS == data['image'].shape[0], 'Wrong Batch Size'\n\n\n        box_image_v = []\n        for i_b in range(BS):\n\n            pred_d = preds_v[i_b]\n\n            image = data['image'][i_b].cpu().numpy().transpose([1,2,0])\n            box_image = (image * 255)\n\n\n            if len(box_image.shape) == 3 and box_image.shape[-1] > 3:\n                box_image = box_image[:,:,:3]\n\n            elif len(box_image.shape) == 3:\n                box_image = box_image[:,:,0]\n\n            if len(box_image.shape) == 2:\n                box_image = box_image[:,:, None] * np.ones(3)\n\n            box_image = box_image.copy().astype(np.uint8)\n\n            bboxes_v = data['bboxes'][i_b]\n            cls_v = data['cls'][i_b]\n            if type(bboxes_v) == torch.Tensor:\n                bboxes_v = bboxes_v.detach().cpu().numpy()\n            \n            if type(cls_v) == torch.Tensor:\n                cls_v = cls_v.detach().cpu().numpy()\n                \n            for bbox, idx_class in zip( bboxes_v, cls_v):\n                (x0, y0, x1, y1) = bbox.astype(np.int)\n                \n                box_image = cv2.rectangle(\n                    box_image,\n                    (x0,y0),\n                    (x1,y1),\n                    class2color_v[idx_class],\n                    thickness=2,\n                )\n\n#                 box_image = cv2.putText(\n#                     box_image,\n#                     class2str_v[idx_class] + '(GT)',\n#                     org=(x0, y0-3),\n#                     fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n#                     fontScale=0.3,\n#                     color=class2color_v[idx_class],\n#                     thickness=1,\n#                     lineType=cv2.LINE_AA,\n#                     bottomLeftOrigin=False\n#                     )      \n\n            \n            for bbox, idx_class, p_det in zip( pred_d['bbox'], pred_d['cls'], pred_d['p_det']):\n\n                (x0, y0, x1, y1) = bbox.astype(np.int)\n\n                box_image = cv2.rectangle(\n                    box_image,\n                    (x0,y0),\n                    (x1,y1),\n                    class2color_v[idx_class],\n                    thickness=1,\n                    )\n\n                box_image = cv2.putText(\n                    box_image,\n                    class2str_v[idx_class] + f'({p_det:0.1f})',\n                    org=(x0, y0-3),\n                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n                    fontScale=0.3,\n                    color=class2color_v[idx_class],\n                    thickness=1,\n                    lineType=cv2.LINE_AA,\n                    bottomLeftOrigin=False\n                    )\n            \n\n            box_image_v.append(box_image)\n\n            if do_plot:\n                plt.figure(0, figsize=(20,20) )\n                plt.imshow(box_image)\n                plt.show()\n\n        return np.array( box_image_v )\n    \n    \n    @torch.jit.ignore\n    def set_lr(self, new_lr=1e-3):\n        for param_group in self.optimizer.param_groups:\n            if 'lr' in param_group.keys():\n                param_group['lr'] = new_lr\n        \n        self.lr = new_lr\n        \n        return None\n    \n    \n    @torch.jit.ignore\n    def get_lr(self, new_lr=1e-3):\n        if self.optimizer is not None:\n            to_ret = None\n            \n            # Checking lr of optimizer.\n            for param_group in self.optimizer.param_groups:\n                if to_ret is None:\n                    to_ret = param_group['lr']\n            \n#             if to_ret != self.lr:\n#                 print(\"WARNING, optimizer's learning_rate != self.lr. (self.lr will be setted)\", file=sys.stderr)\n#                 self.lr = to_ret\n        else:\n            to_ret = self.lr\n        \n        return to_ret\n    \n    \n    @torch.jit.ignore\n    def _parallelize_backbone(self, devices_v=None):\n        if devices_v is None:\n            self.devices_v = [torch.device(i) for i in range(torch.cuda.device_count())]\n        else:\n            self.devices_v = [torch.device(i) for i in devices_v]\n        \n        \n        if self.use_effdet:\n            self.backbone.backbone = nn.DataParallel(self.backbone.backbone, self.devices_v)\n            self.backbone.fpn = nn.DataParallel(self.backbone.fpn, self.devices_v)\n            self.backbone.class_net = nn.DataParallel(self.backbone.class_net, self.devices_v)\n            self.backbone.box_net = nn.DataParallel(self.backbone.box_net, self.devices_v)\n\n            self.net_labeler_train   = nn.DataParallel( DetBenchTrain(self.backbone) , self.devices_v)\n            self.net_labeler_predict = nn.DataParallel( DetBenchPredict(self.backbone), self.devices_v)\n            \n        else:\n            self.backbone = nn.DataParallel(self.backbone, self.devices_v)\n        \n        print(' - DataParallel, using devices:', [f'{d.type}:{d.index}'for d in self.devices_v])\n        return None\n    ","59ce4c64":"# backbone_name = 'yolov5l'\n# backbone_name = f'fasterrcnn_resnet101d'\nbackbone_name = f'tf_efficientdet_d2'\n\nN_FOLDS = 5\nI_FOLD  = GLOBAL_I_FOLD\nN_EPOCHS = GLOBAL_N_EPOCHS\nN_WARMUP_EPOCHS = GLOBAL_N_WARMUP_EPOCHS\nN_DECAY_EPOCHS  = GLOBAL_N_DECAY_EPOCHS\n\nCLS = GLOBAL_CLS\n\nRAD_ID = None\n\nVERSION = 19\nCONSENSUS_LEVEL = None\nTEST_SPLIT = 0.05\nCLEAN_BOXES = False\nCLEAN_IOU_TH = 0.8\n\ncls_nofinding = len(class2str_v) - 1\n\nif CLS is None:\n    SELECT_CLASSES = None\n    CLS = 'All'\nelse:\n    SELECT_CLASSES = [CLS, 14] #[2, 5, 6, 9, 11, 13, 14]\n    \nREMOVE_CLASSES = None # [cls_nofinding]\n\n# R8 R9 R10 = 3093 samples\n# R11 to R17 = 203 samples\nif RAD_ID is None:\n    RAD_ID  = 'RA'\n    RAD_ID_FILTER_V = None\nelse:\n    RAD_ID_FILTER_V = [RAD_ID] + [f'R{i}' for i in range(11,18)]\n\n\nif SELECT_CLASSES:\n    N_CLASSES = len(SELECT_CLASSES)\n    \nelif REMOVE_CLASSES:\n    N_CLASSES = len(class2str_v) - len(REMOVE_CLASSES) + (1 if cls_nofinding in REMOVE_CLASSES else 0)\n    \nelse:\n    N_CLASSES = len(class2str_v)  # counting class 14\n    \nMODEL_RESOLUTION = GLOBAL_MODEL_RESOLUTION\n\n\nif 'fasterrcnn' in backbone_name:\n    anchors_d = get_anchos_from_cls(\n        CLS,\n        model_resolution=MODEL_RESOLUTION\n    )\n    \nelse:\n    anchors_d = None\n    ","a236d552":"model_cfg_d = {\n    'model_resolution': MODEL_RESOLUTION, \n    'n_input_channels': 4,\n    \n    'anchors_d' : anchors_d,\n    \n    'n_classes': N_CLASSES-1,\n    'n_extras': 0,\n    'extra_loss_weight':6.0,\n    \n    'use_pretrained_model': True,\n#     'backbone_name':f'fasterrcnn_resnet{RN}_fpn',\n#     'backbone_name':f'tf_efficientdet_d{D}',\n#     'backbone_name':f'fasterrcnn_resnet{RN}',\n    \n    'backbone_name':backbone_name,\n    \n    'init_lr': GLOBAL_LR,\n    'use_scheduler':True,\n    'n_warmup_epochs':N_WARMUP_EPOCHS,\n    'n_decay_epochs':N_DECAY_EPOCHS,\n    'lr_prop':1e-3,\n    \n    'optimizer_name': 'adam',\n    'n_steps_grad_update': GLOBAL_GRAD_STEPS,\n    \n    'start_ckpt' : None, #'resnet200d_fold0.0_best_loss_init_weights.ckpt',\n    'parallelize_backbone':False,\n    \n    'clip_grad_norm':3.0,\n    'weight_decay': 1e-5,\n    'model_name':f'ModelX_V{VERSION}',\n    'checkpoint_base_path':f'.\/{backbone_name}_F{I_FOLD}_{RAD_ID}_C{CLS}_V{VERSION}',\n    \n    'device': GLOBAL_DEVICE,\n}","1f153ec1":"if True:\n    N_HIST = 1000\n    model  = ModelX(**model_cfg_d)\n\n    loss_names_v = ['loss', 'box_loss', 'class_loss'] #, 'extra_loss']\n\n\n    trn_fsma_d = {}\n    for k in loss_names_v:\n        trn_fsma_d[k] = FastSMA(\n            maxlen=N_HIST,\n            label='mean = ',\n            print_format='0.02f',\n            save_filename=os.path.join(model.checkpoint_base_path, f'{k}_trn.fsma')\n        )\n\n    val_fsma_d = {}\n    for k in loss_names_v:\n        val_fsma_d[k] = FastSMA(\n            maxlen=N_HIST,\n            label='mean = ',\n            print_format='0.02f',\n            save_filename=os.path.join(model.checkpoint_base_path, f'{k}_val.fsma')\n        )\n        \n        \n    %matplotlib inline","0d773daa":"n_w = 0\nprint('Optimizable parameters:')\nfor i_p, (n, p) in enumerate( list( model.named_parameters() )):\n    if p.requires_grad:\n        print('{:4d}  {:s}  {:50s}  {:}'.format(i_p, 'OPT' if p.requires_grad else '---',  n, p.shape) )\n        n_w += np.prod(p.shape)\n    else:\n        print('{:4d}  {:s}  {:50s}  {:}'.format(i_p, 'OPT' if p.requires_grad else '---',  n, p.shape) )\n        \nprint(f'Total optimizable weights: {n_w\/1e6:0.02f} Mw')","b3f8092f":"def batch_merge(samples_v):\n    ret_batch_d = {k : [] for k in samples_v[0].keys()}\n    \n    for sample_d in samples_v:\n        for k, v in sample_d.items():\n            ret_batch_d[k].append( v )\n        \n    ret_batch_d['image'] = np.stack(ret_batch_d['image'])\n    \n    return ret_batch_d\n\n\ndef data2tensor(data, device=None, pin_memory=False):\n    data['image'] = torch.tensor(data['image'], device=device, pin_memory=pin_memory)\n\n    for k in data.keys():\n        if type(data[k][0]) is np.ndarray:\n            for i_s in range(len(data['image'])):\n                data[k][i_s] = torch.tensor( data[k][i_s], device=device, pin_memory=pin_memory)\n                \n    return data\n\n\ndef load_fold_ds(\n    i_fold,\n    N_FOLDS=5,\n    SELECT_CLASSES=None,\n    REMOVE_CLASSES=None,\n    CONSENSUS_LEVEL=2,\n    TEST_SPLIT=0.1,\n    CLEAN_BOXES=True,\n    CLEAN_IOU_TH=0.5,\n    RAD_ID_FILTER_V=None,\n    TRAIN_DS_NAME=TRAIN_DS_NAME,\n    DS_PATH=DS_PATH,\n):\n    ds_trn = FoldDataset(\n        ds_path=DS_PATH,\n        ds_name=TRAIN_DS_NAME,\n        images_dir=os.path.join(DS_DIR, 'train'),\n        model_resolution=MODEL_RESOLUTION,\n        df_path=os.path.join(DS_DIR, 'train.csv'),\n        \n        mode='cv_trn',\n        i_fold=i_fold,\n        n_folds=N_FOLDS,\n        test_split=TEST_SPLIT,\n        \n        do_augmentation=True,\n        downsample_factor=2,\n        remove_classes_v=REMOVE_CLASSES,\n        select_classes_v=SELECT_CLASSES,\n        show_warnings=False,\n        do_random_shuffle=True,\n        random_seed=3128,\n        \n        clean_boxes=CLEAN_BOXES,\n        clean_mode='random',\n        clean_iou_th=CLEAN_IOU_TH,\n        \n        consensus_level=CONSENSUS_LEVEL,\n    )\n\n    ds_val = FoldDataset(\n        ds_path=DS_PATH,\n        ds_name=TRAIN_DS_NAME,\n        images_dir=os.path.join(DS_DIR, 'train'),\n        model_resolution=MODEL_RESOLUTION,\n        df_path=os.path.join(DS_DIR, 'train.csv'),\n        \n        mode='cv_val',\n        i_fold=i_fold,\n        n_folds=N_FOLDS,\n        test_split=TEST_SPLIT,\n        \n        do_augmentation=False,\n        downsample_factor=2,\n        remove_classes_v=REMOVE_CLASSES,\n        select_classes_v=SELECT_CLASSES,\n        show_warnings=False,\n        do_random_shuffle=True,\n        random_seed=3128,\n        \n        clean_boxes=CLEAN_BOXES,\n        clean_mode='random',\n        clean_iou_th=CLEAN_IOU_TH,\n        \n        consensus_level=CONSENSUS_LEVEL,\n    )\n    \n    \n    if RAD_ID_FILTER_V is not None and len(RAD_ID_FILTER_V) > 0:\n        ds_trn.filter_radiologists(RAD_ID_FILTER_V)\n        ds_val.filter_radiologists(RAD_ID_FILTER_V)\n    \n    return ds_trn, ds_val\n\n\ndef load_tst_ds(\n    i_fold,\n    N_FOLDS=5,\n    SELECT_CLASSES=None,\n    REMOVE_CLASSES=None,\n    CONSENSUS_LEVEL=2,\n    TEST_SPLIT=0.1,\n    CLEAN_BOXES=True,\n    CLEAN_IOU_TH=0.5,\n    RAD_ID_FILTER_V=None,\n    TRAIN_DS_NAME=TRAIN_DS_NAME,\n    DS_PATH=DS_PATH,\n):\n    \n    ds_tst = FoldDataset(\n        ds_path=DS_PATH,\n        ds_name='test',\n        images_dir=os.path.join(DS_DIR, 'test'),\n        model_resolution=MODEL_RESOLUTION,\n        df_path=None,\n\n        mode='none',\n        i_fold=i_fold,\n        n_folds=N_FOLDS,\n        test_split=TEST_SPLIT,\n\n        do_augmentation=False,\n        downsample_factor=2,\n        remove_classes_v=[],\n        select_classes_v=None,\n\n        show_warnings=False,\n        do_random_shuffle=False,\n        random_seed=3128,\n\n        clean_boxes=CLEAN_BOXES,\n        clean_mode='random',\n        clean_iou_th=CLEAN_IOU_TH,\n\n        consensus_level=None,\n    )\n\n\n    ds_tst_oof = FoldDataset(\n        ds_path=DS_PATH,\n        ds_name=TRAIN_DS_NAME,\n        images_dir=os.path.join(DS_DIR, 'train'),\n        model_resolution=MODEL_RESOLUTION,\n        df_path='train.csv',\n\n        mode='cv_tst',\n        i_fold=i_fold,\n        n_folds=N_FOLDS,\n        test_split=TEST_SPLIT,\n\n        do_augmentation=False,\n        downsample_factor=2,\n        remove_classes_v=REMOVE_CLASSES,\n        select_classes_v=SELECT_CLASSES,\n        show_warnings=False,\n        do_random_shuffle=False,\n        random_seed=3128,\n\n\n        clean_boxes=CLEAN_BOXES,\n        clean_mode='random',\n        clean_iou_th=CLEAN_IOU_TH,\n\n        consensus_level=CONSENSUS_LEVEL,\n    )\n    \n    if RAD_ID_FILTER_V is not None and len(RAD_ID_FILTER_V) > 0:\n        ds_tst_oof.filter_radiologists(RAD_ID_FILTER_V)\n        \n    return ds_tst, ds_tst_oof","ffbc38c9":"ds_trn, ds_val = load_fold_ds(\n    I_FOLD,\n    N_FOLDS,\n    SELECT_CLASSES,\n    REMOVE_CLASSES,\n    CONSENSUS_LEVEL,\n    TEST_SPLIT,\n    CLEAN_BOXES,\n    CLEAN_IOU_TH,\n    RAD_ID_FILTER_V,\n    TRAIN_DS_NAME,\n    DS_PATH,\n)\n\n\n\n\nds_tst, ds_tst_oof = load_tst_ds(\n    I_FOLD,\n    N_FOLDS,\n    SELECT_CLASSES,\n    REMOVE_CLASSES,\n    CONSENSUS_LEVEL,\n    TEST_SPLIT,\n    CLEAN_BOXES,\n    CLEAN_IOU_TH,\n    RAD_ID_FILTER_V,\n    TRAIN_DS_NAME,\n    DS_PATH,\n)\n\n\n# Using FATIH's VALIDATION SAMPLES\n\nval_fatih_samples_id_v = load_obj('..\/input\/validation-samples\/fatihs_validation_samples.pickle')\ntrn_fatih_samples_id_v = np.array( [i_s for i_s in ds_trn.all_sample_ids if i_s not in val_fatih_samples_id_v] )\n\nds_trn.fold_samples = trn_fatih_samples_id_v\nds_val.fold_samples = val_fatih_samples_id_v\n\ndel(ds_tst_oof)\n\nprint(f'- TRN samples: {len(ds_trn)}' )\nprint(f'- VAL samples: {len(ds_val)}' )\nprint(f'- TST samples: {len(ds_tst)}' )","575a63cf":"def train_one_epoch(\n    model,\n    i_fold,\n    i_epoch,\n    ds_trn,\n    trn_fsma_d,\n    batch_size=1,\n    shuffle=True,\n    num_workers=16,\n    pin_memory=False):\n    \n    trn_iter = tqdm(DataLoader(\n        ds_trn,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        collate_fn=batch_merge))\n    \n    # Train Step\n    epoch_losses_d = {k:[] for k in trn_fsma_d.keys()}\n    for i_step, trn_data in enumerate(trn_iter):\n        trn_loss_d = model.train_step(\n            trn_data\n        )\n        \n        for k, fsma in trn_fsma_d.items():\n            l = trn_loss_d[k].item()\n            fsma.append(l)\n            epoch_losses_d[k].append(l)\n            \n            \n        trn_iter.set_description('[TRN_F={:d}E={:d}_L={:0.03f}_B={:0.03f}_C={:0.03f}_X={:0.03f}]'.format(\n            i_fold,\n            i_epoch,\n            trn_fsma_d['loss'].mean(),\n            trn_fsma_d['box_loss'].mean(),\n            trn_fsma_d['class_loss'].mean(),\n            trn_fsma_d['extra_loss'].mean() if 'extra_loss' in trn_fsma_d.keys() else 0.0,\n            ) )\n        \n#         if i_step == 2:\n#             break\n    \n    for k in epoch_losses_d.keys():\n        epoch_losses_d[k] = np.mean(epoch_losses_d[k])\n        \n    return epoch_losses_d\n\n\ndef validate_one_epoch(\n    model,\n    i_fold,\n    i_epoch,\n    ds_val,\n    val_fsma_d,\n    batch_size=1,\n    shuffle=False,\n    num_workers=16,\n    pin_memory=False,\n    \n    det_th=0.00,\n    clear_predictions=True,\n    clear_iou_th=0.50,\n    clear_mode='p_det_weight',\n):\n    \n    gt_df = ds_val.get_GT_Dataframe()\n    \n    val_iter = tqdm(DataLoader(\n        ds_val,\n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        collate_fn=batch_merge))\n\n    # Validation step\n    epoch_losses_d = {k:[] for k in val_fsma_d.keys()}\n    \n    all_preds_v = []\n    for i_step, val_data in enumerate(val_iter):\n        val_loss_d = model.predict(\n            val_data,\n            output_losses=True,\n            training=False,\n            det_th=det_th,\n            filter_boxes=True,\n            unscale_bboxes=True)\n        \n        pred_v = val_loss_d['detections']\n        \n        if clear_predictions:\n            pred_v =  clean_predictions(\n                pred_v,\n                iou_th=clear_iou_th,\n                mode=clear_mode)\n            \n        for i_s, pred_d in enumerate(pred_v):\n            pred_d['sample_id']      = val_data['sample_id'][i_s]\n            pred_d['original_shape'] = val_data['original_shape'][i_s]\n            \n        all_preds_v += pred_v\n        # all_preds_v calculation\n            \n            \n        for k, fsma in val_fsma_d.items():\n            l = val_loss_d[k].item()\n            fsma.append(l)\n            epoch_losses_d[k].append(l)\n            \n                \n                \n        val_iter.set_description('[VAL_F={:d}E={:d}_L={:0.03f}_B={:0.03f}_C={:0.03f}_X={:0.03f}]'.format(\n            i_fold,\n            i_epoch,\n            val_fsma_d['loss'].mean(),\n            val_fsma_d['box_loss'].mean(),\n            val_fsma_d['class_loss'].mean(),\n            val_fsma_d['extra_loss'].mean() if 'extra_loss' in val_fsma_d.keys() else 0.0,\n            ) )\n        \n        \n#         if i_step == 2:\n#             break\n            \n            \n    for k in epoch_losses_d.keys():\n        epoch_losses_d[k] = np.mean(epoch_losses_d[k])\n    \n    try:\n        metrics_v, summary_df, mAP = calc_metrics(\n            all_preds_v,\n            gt_df,\n            iou_thresh=0.4,\n            show_summay=False,\n            n_classes=N_CLASSES\n        )\n\n        fig = plot_PvsR_curve(metrics_v, class2color_v, False)\n        \n        \n        \n        epoch_losses_d['metrics_v'] = metrics_v\n        epoch_losses_d['summary_df'] = summary_df\n        epoch_losses_d['mAP'] = mAP\n        epoch_losses_d['fig'] = fig\n        \n            \n    except:\n        print('Problems with calc_metrics or plot_PvsR_curve')\n    \n    \n    return epoch_losses_d\n\n\n\ndef save_model(\n    model,\n    trn_fsma_d,\n    val_fsma_d,\n    i_epoch,\n    i_fold):\n        \n    f_sma_trn = trn_fsma_d['loss']\n    f_sma_val = val_fsma_d['loss']\n    \n    model_filename = f'F{i_fold:}_E{i_epoch:}_{model.model_name}_T{f_sma_trn.mean():0.03f}_V{f_sma_val.mean():0.03f}.ckpt'\n    \n    save_path = model.save_checkpoint(\n        step={'i_epoch':i_epoch, 'i_fold': i_fold},\n        loss={'trn':f_sma_trn.mean(), 'val':f_sma_val.mean()},\n        file_name=model_filename,\n        verbose=True,\n    )\n    \n    for k, fsma in trn_fsma_d.items():\n        fsma.save()\n    \n    for k, fsma in val_fsma_d.items():\n        fsma.save()\n    \n    return save_path\n\n\nclass Logger:\n    def __init__(\n        self,\n        log_folder='.\/folder',\n        log_name='fold_cls',\n        \n    ):\n        self.log_folder = log_folder\n        self.log_name = log_name\n        \n        if not os.path.exists(self.log_folder):\n            os.makedirs( self.log_folder )\n            \n        return None\n\n\n    def log(\n        self,\n        data,\n        step='',\n        do_show=True):\n        \n        if type(data) in [list, tuple]:\n            for d in data:\n                self.log(\n                    d,\n                    step=step,\n                    do_show=do_show,\n                )\n\n        elif type(data) == str:\n            with open(os.path.join(self.log_folder, self.log_name + '.txt'), 'a') as f:\n                f.write(data + '\\n')\n                \n            if do_show:\n                print(data)\n\n        elif type(data) == plt.Figure:\n            data.savefig(os.path.join(self.log_folder, f'S={step}'+self.log_name+'.png'))\n            \n            if do_show:\n                data.show()\n                plt.show()\n\n        else:\n            with open(os.path.join(self.log_folder, self.log_name + '.txt'), 'a') as f:\n                f.write(repr(data) + '\\n')\n                \n            if do_show:\n                print(data)\n        \n    \n        return None","d17691e7":"if GLOBAL_CONTINUE_TRAINING:\n    last_epoch, model_path = model.find_last_saved_ckpt()\n    _ = model.restore_checkpoint(\n        model_path\n    )\n\n    for k, fsma in trn_fsma_d.items():\n        fsma.load()\n\n    for k, fsma in val_fsma_d.items():\n        fsma.load()","61958c59":"if not EVAL_CKPTS:\n    if GLOBAL_CONTINUE_TRAINING and last_epoch is not None:\n        START_EPOCH = last_epoch + 1\n        model.scheduler_step()\n\n        assert model.scheduler.last_epoch == START_EPOCH, 'Scheduler ERROR'\n\n    else:\n        START_EPOCH = 0\n\n    N_EPOCHS = GLOBAL_N_EPOCHS\n    N_WORKERS = GLOBAL_N_WORKERS\n    BATCH_SIZE = GLOBAL_BATCH_SIZE\n    PIN_MEMORY = True\n\n    VAL_AFTER_EPOCH = 0\n    VAL_EVERY_N_EPOCHS = 1\n\n    # To ensure that the last epoch will be saved\n    N_EPOCHS = N_EPOCHS - (N_EPOCHS%VAL_EVERY_N_EPOCHS) \n\n    L = Logger(\n        log_folder=os.path.join(model.checkpoint_base_path, 'log'),\n        log_name=f'log_CLS{CLS}_F{I_FOLD}'\n    )\n\n    torch.cuda.empty_cache()\n\n\n\n    if model.anchors_d is not None:\n        L.log(' - Using custom anchors: ', do_show=False)\n        L.log(f\" |-> INPUT_SHAPE: {model.model_resolution}\", do_show=False)\n        L.log(f\" |-> ANCHOR_SIZES: {model.anchors_d['ANCHOR_SIZES']}\", do_show=False)\n        L.log(f\" |-> ASPECT_RATIOS: {model.anchors_d['ASPECT_RATIOS']}\", do_show=False)\n\n\n    for i_epoch in range(START_EPOCH, N_EPOCHS + 1):\n        L.log('\\n - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\\n ')\n        L.log(f'Starting: Epoch = {i_epoch}  Fold = {I_FOLD}   Class = {CLS}   LR={model.get_lr():0.02e}')\n\n\n        trn_loss_epoch_d = train_one_epoch(\n            model,\n            I_FOLD,\n            i_epoch,\n            ds_trn,\n            trn_fsma_d,\n            batch_size=BATCH_SIZE,\n            shuffle=True,\n            num_workers=N_WORKERS,\n            pin_memory=PIN_MEMORY)\n\n\n        show_val_sum = False\n        if (i_epoch % VAL_EVERY_N_EPOCHS) == 0 and i_epoch >= VAL_AFTER_EPOCH:\n            show_val_sum = True\n\n            val_loss_epoch_d = validate_one_epoch(\n                model,\n                I_FOLD,\n                i_epoch,\n                ds_val,\n                val_fsma_d,\n                batch_size=BATCH_SIZE,\n                shuffle=False,\n                num_workers=N_WORKERS,\n                pin_memory=PIN_MEMORY)        \n\n\n        L.log(f' Epoch {i_epoch} Summary:')\n        L.log(' - trn_loss_epoch_d:')\n        for k, v in trn_loss_epoch_d.items():\n            if type(v) in [float, np.float, np.float32, np.float64]:\n                L.log(f'  |-> {k} = {v:0.04f}')\n\n        if show_val_sum:\n            L.log(' - val_loss_epoch_d:')\n            for k, v in val_loss_epoch_d.items():\n                if type(v) in [float, np.float, np.float32, np.float64]:\n                    L.log(f'  |-> {k} = {v:0.04f}')\n\n            if 'fig' in val_loss_epoch_d.keys():\n                L.log(val_loss_epoch_d['fig'], step=i_epoch)\n\n            if 'summary_df' in val_loss_epoch_d.keys():\n                L.log(val_loss_epoch_d['summary_df'])\n\n\n\n            save_path = save_model(\n                model,\n                trn_fsma_d,\n                val_fsma_d,\n                i_epoch,\n                I_FOLD)\n\n            L.log(f' - Ckpt saved: \"{save_path}\" \\n')\n\n        lr = model.scheduler_step(False)\n        L.log(f' - Scheduler: New lr = {lr:0.02e}')\n","e366f70f":"def filter_wh(\n    pred_v,\n    img_shape_d,\n    min_wh_v,\n    max_wh_v,\n    min_a_v,\n    max_a_v,\n):\n    \n    pred_v = copy.deepcopy(pred_v)\n    \n    for pred_d in pred_v:\n        if pred_d['cls'].shape[0] == 0:\n            continue\n\n        sample_id = pred_d['sample_id']\n        wh_bb = (pred_d['bbox'][:,2:]-pred_d['bbox'][:,:2]) \/ img_shape_d[sample_id][::-1]\n        a_bb = wh_bb[:,0] * wh_bb[:,1]\n\n        f = np.ones(pred_d['cls'].shape[0], dtype=np.bool)\n        for i, i_c in enumerate(pred_d['cls']):\n            if (wh_bb[i] > max_wh_v[i_c]).any() \\\n            or (wh_bb[i] < min_wh_v[i_c]).any() \\\n            or(a_bb[i] > max_a_v[i_c]).any() \\\n            or (a_bb[i] < min_a_v[i_c]).any():\n                \n                f[i] = False\n                \n\n        for k in ['bbox', 'cls', 'p_det']:\n            pred_d[k] = pred_d[k][f]\n    \n    return pred_v\n\n\ndef get_max_wh_v(ds, sigmas=3.0, N_CLASSES=15, do_plotting=False):\n    max_wh_v = []\n    min_wh_v = []\n    \n    max_a_v = []\n    min_a_v = []\n    for i_c in range(N_CLASSES-1):\n        wh = ds.get_samples_wh_v(i_c) \/ ds.model_resolution[::-1]\n        wh_mean = wh.mean(axis=0)\n        wh_std = wh.std(axis=0)\n        max_wh_v.append( wh_mean + sigmas * wh_std )\n        min_wh_v.append( wh_mean - sigmas * wh_std )\n        \n        a = wh[:,0] * wh[:,1]\n        a_mean = a.mean(axis=0)\n        a_std = a.std(axis=0)\n        max_a_v.append( a_mean + sigmas * a_std )\n        min_a_v.append( a_mean - sigmas * a_std )\n        \n        if do_plotting:\n            plt.hist(a, bins=300, label=f'c{i_c}')\n            plt.legend()\n            plt.grid()\n            plt.show()\n        \n        \n    max_wh_v = np.clip( np.array(max_wh_v), 0, 1)\n    min_wh_v = np.clip( np.array(min_wh_v), 0, 1)\n    \n    max_a_v = np.clip( np.array(max_a_v), 0, 1)\n    min_a_v = np.clip( np.array(min_a_v), 0, 1)\n    \n    \n    return min_wh_v, max_wh_v, min_a_v, max_a_v\n\n\n","f6dd08df":"min_wh_v, max_wh_v, min_a_v, max_a_v = get_max_wh_v(ds=ds_trn, sigmas=3, N_CLASSES=N_CLASSES)","bffb2c9f":"if EVAL_CKPTS:\n    for ckpt_path in CKPTS_v:\n        epoch = model.get_epoch_from_ckpt_path(ckpt_path)\n        \n        _ = model.restore_checkpoint(ckpt_path)\n\n        pred_v = evalueate_dataset(\n                ds_val,\n                model,\n                det_th=0.00,\n                unscale_bboxes=True,\n                batch_size=5,\n                num_workers=8,\n                pin_memory=False,\n                do_clean_predictions=False,\n                clean_iou_th=0.4,\n                clean_mode='p_det_weight',\n                do_TTA=False,\n                TTA_clean_iou_th=0.2,\n                TTA_clean_mode='median_pmean',\n            )\n\n        exec(f'preds_v_val_{epoch} = pred_v')\n        \n        \n        pred_v = evalueate_dataset(\n            ds_tst,\n            model,\n            det_th=0.00,\n            unscale_bboxes=True,\n            batch_size=5,\n            num_workers=8,\n            pin_memory=False,\n            do_clean_predictions=False,\n            clean_iou_th=0.4,\n            clean_mode='p_det_weight',\n            do_TTA=False,\n            TTA_clean_iou_th=0.2,\n            TTA_clean_mode='median_pmean',\n        )\n    \n        exec(f'preds_v_tst_{epoch} = pred_v')","9ec88f3f":"FILTER_TH = 0.05","6733d77c":"if EVAL_CKPTS:\n    to_ensemble_v = [\n        preds_v_val_82,\n        preds_v_val_74,\n        preds_v_val_62,\n    ]\n\n    to_ensemble_v = [filter_det_th(pred_v, FILTER_TH) for pred_v in to_ensemble_v]\n\n\n    ens_val_pred_v = join_predictions(\n        to_ensemble_v, \n        add_model_id=True)\n\n    print('Ensembing:',len(to_ensemble_v), 'models')\n\n    pred_v = ens_val_pred_v\n\n    pred_v =  clean_predictions(\n                    pred_v,\n                    iou_th=0.6,\n                    mode='p_det_weight_pmean',\n                    consensus_level=1,\n                    n_models2ensemble=len(to_ensemble_v),\n    )\n\n    pred_v = filter_wh(\n        pred_v=pred_v,\n        img_shape_d=ds_val.misc_d,\n        min_wh_v=min_wh_v,\n        max_wh_v=max_wh_v,\n        min_a_v=min_a_v,\n        max_a_v=max_a_v,\n    )\n\n    pred_v = norm_p_det(pred_v)\n\n    pred_v = filter_det_th(pred_v, FILTER_TH)\n\n    gt_df = ds_val.get_GT_Dataframe(clean_mode='p_det_max')\n\n    metrics_v, summary_df, mAP = calc_metrics(\n                pred_v,\n                gt_df,\n                iou_thresh=0.4,\n                show_summay=True,\n                n_classes=N_CLASSES\n            )\n\n    print('mAP-AP14: {:0.04f}'.format(summary_df.AP.values[:-1].sum()\/15))\n\n    _ = plot_PvsR_curve(metrics_v, class2color_v, do_show=True)\n    \n    \n    \n    preds_df = predictions_to_df(\n        pred_v,\n        f'ds_val_F{I_FOLD}_{RAD_ID}_V{VERSION}_{backbone_name}_EnsE82E74E62_DetTH{FILTER_TH:0.02f}_woCLS14Filter.csv')","96bab21c":"if EVAL_CKPTS:\n    to_ens_tst_v = [\n        preds_v_tst_82,\n        preds_v_tst_74,\n        preds_v_tst_62,\n    ]\n\n    to_ens_tst_v = [filter_det_th(pred_v, FILTER_TH) for pred_v in to_ens_tst_v]\n\n    ens_tst_pred_v = join_predictions(\n        to_ens_tst_v, \n        add_model_id=True)\n\n    print('TST Ensemble:',len(to_ens_tst_v), 'models')\n\n    pred_v = ens_tst_pred_v\n\n    pred_v =  clean_predictions(\n                    pred_v,\n                    iou_th=0.6,\n                    mode='p_det_weight_pmean',\n                    consensus_level=1,\n                    n_models2ensemble=len(to_ens_tst_v),\n    )\n\n    pred_v = filter_wh(\n        pred_v=pred_v,\n        img_shape_d=ds_tst.misc_d,\n        min_wh_v=min_wh_v,\n        max_wh_v=max_wh_v,\n        min_a_v=min_a_v,\n        max_a_v=max_a_v,\n    )\n\n    pred_v = norm_p_det(pred_v)\n\n    pred_v = filter_det_th(pred_v, FILTER_TH)\n    \n    preds_df = predictions_to_df(\n        pred_v,\n        f'ds_tst_F{I_FOLD}_{RAD_ID}_V{VERSION}_{backbone_name}_EnsE82E74E62_DetTH{FILTER_TH:0.02f}_woCLS14Filter.csv')\n    \n    \n    # Applying class 14 filtering \n    pred_v = add_class_14(\n        pred_v,\n        pred_clf_c14_filename='..\/input\/vinbigdata-2class-prediction\/2-cls test pred.csv',\n        rm_preds_high_th=False,\n    )\n\n    preds_df = predictions_to_df(\n        pred_v,\n        f'ds_tst_F{I_FOLD}_{RAD_ID}_V{VERSION}_{backbone_name}_EnsE82E74E62_DetTH{FILTER_TH:0.02f}_wCLS14Filter.csv')","1eb25e5d":"# Loading Last Ckpt","4485a7f7":"> # DS_VAL Ensemble","32d4085c":"# filtering width, height and area of predictions","f477abea":"# Detector Training","fe2119ea":"# Model","8ced92b1":"# Building Model","5cb0e4f0":"# Detector Inference","8a8b95ea":"# Dataset handler","c9eefda3":"# DS_TST Ensemble","cd390a2d":"# Dataset Inference","3a731de6":"# Training Datasets"}}