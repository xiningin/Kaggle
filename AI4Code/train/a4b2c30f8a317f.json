{"cell_type":{"794e616a":"code","8f716e64":"code","23c32809":"code","52e8b0de":"code","1d4b0af3":"code","30d5cb59":"code","8127d648":"code","06e53b61":"code","813f23d1":"code","9af8c517":"code","5e43caaa":"code","838b7e25":"code","d1d3e173":"code","fceaebd1":"code","e70b4ea0":"code","5ea100b2":"code","e025a00d":"code","7d5c5a89":"code","9950bdd9":"code","c2f6b93b":"code","4ee9d37b":"code","30f8804a":"code","cf85349b":"markdown","cb0c2e21":"markdown","44686e66":"markdown","0bb41bea":"markdown","02e4ee4f":"markdown","b3a541c4":"markdown","85330e24":"markdown","989033ab":"markdown","9dddd442":"markdown","d9bc0dba":"markdown","46840052":"markdown","2537800c":"markdown","c671227e":"markdown","7b1ab362":"markdown","2e93f0c1":"markdown"},"source":{"794e616a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8f716e64":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as XGB","23c32809":"df = pd.read_csv('..\/input\/body-fat-prediction-dataset\/bodyfat.csv')","52e8b0de":"df.head()","1d4b0af3":"df.isnull().sum()","30d5cb59":"df.info()","8127d648":"df.shape","06e53b61":"data = df.copy()","813f23d1":"data = data.drop(columns=['BodyFat','Density'], axis=1)\nfeatures = list(data.columns)","9af8c517":"data.head()","5e43caaa":"sns.pairplot(data)","838b7e25":"correlation = df[features].corr(method='spearman')","d1d3e173":"plt.figure(figsize=(15,10))\nsns.heatmap(correlation, annot=True, vmin=-1, vmax=1)\nplt.show()","fceaebd1":"X = data\ny = df['BodyFat']","e70b4ea0":"\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","5ea100b2":"logs = []","e025a00d":"regr = LinearRegression()\nregr.fit(X_train, y_train)\ny_pred = regr.predict(X_test)\nscore = regr.score(X_test, y_test)\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(f\"Score --> {score}\")\nprint(f\"RMSE --> {rmse}\")\n\nlog = {\"name\": \"linear_regression\", \"score\": score, \"rmse\": rmse}\nlogs.append(log)","7d5c5a89":"lasso_regr = Lasso(alpha=0.5)\nlasso_regr.fit(X_train, y_train)\ncv_score = cross_val_score(lasso_regr, X_train, y_train, cv=10)\nprint(f\"CV Score --> {np.mean(cv_score)}\")\ny_pred = lasso_regr.predict(X_test)\nprint(f\"Score --> {lasso_regr.score(X_test, y_test)}\")\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(f\"RMSE --> {rmse}\")\n\nlog = {\"name\": \"lasso\", \"score\": np.mean(cv_score), \"rmse\": rmse}\nlogs.append(log)","9950bdd9":"ridge_regr = Ridge(alpha=0.5)\nridge_regr.fit(X_train, y_train)\ncv_score = cross_val_score(ridge_regr, X_train, y_train, cv=10)\nprint(f\"CV Score --> {np.mean(cv_score)}\")\ny_pred = ridge_regr.predict(X_test)\nprint(f\"Score --> {ridge_regr.score(X_test, y_test)}\")\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(f\"RMSE --> {rmse}\")\n\nlog = {\"name\": \"ridge\", \"score\": np.mean(cv_score), \"rmse\": rmse}\nlogs.append(log)","c2f6b93b":"xgb_regr = XGB.XGBRegressor(learning_rate = 0.01, n_estimators=1000)\nxgb_regr.fit(X_train, y_train)\ncv_score = cross_val_score(xgb_regr, X_train, y_train, cv=10)\nprint(f\"CV Score --> {np.mean(cv_score)}\")\ny_pred = regr.predict(X_test)\nprint(f\"Score --> {xgb_regr.score(X_test, y_test)}\")\nrmse = mean_squared_error(y_test, y_pred, squared=False)\nprint(f\"RMSE --> {rmse}\")\n\nlog = {\"name\": \"XGBoost\", \"score\": np.mean(cv_score), \"rmse\": rmse}\nlogs.append(log)","4ee9d37b":"logs","30f8804a":"x = []\ny = []\nz = []\nfor log in logs:\n    x.append(log['name'])\n    y.append(log['score'])\n    z.append(log['rmse'])\n    \n# sns.barplot(x,y)\n# sns.barplot(x,z)\n\nplt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.barplot(x,y)\nplt.title(\"Models and their accuracy\")\n\nplt.subplot(2,2,2)\nsns.barplot(x,z)\nplt.title(\"Models and their rmse\")\n\nplt.show()","cf85349b":"## Check for null values","cb0c2e21":"No feature is correlated to another feature , hence all the features can be taken for our analysis","44686e66":"# EDA","0bb41bea":"# Ridge","02e4ee4f":"# Split data","b3a541c4":"# XGBoost","85330e24":"# Import libraries","989033ab":"# Linear Regression","9dddd442":"# Lasso","d9bc0dba":"### If you found this notebook useful, consider upvoting. Thank you","46840052":"## Read the data","2537800c":"**This is a work in progress. In the subsequent versions, I will try to increase the accuracy.**","c671227e":"Drop the columns as specified in the question","7b1ab362":"# Models comparison","2e93f0c1":"**Lasso regression technique gives the highest accuracy of 69.5%**"}}