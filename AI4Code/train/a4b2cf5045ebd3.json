{"cell_type":{"cd4e8ce6":"code","6f7516c2":"code","68713ebf":"code","fa85df19":"code","76c06b31":"code","bc205df1":"code","0bcce2d5":"code","d3c6e040":"code","14d52b4d":"code","a799924d":"code","ad0cd6ee":"code","f02a75ed":"code","5d0600c4":"code","3f9a9d97":"code","0ff47e37":"code","9790e79f":"code","e92889dc":"code","11e99fca":"code","99210d5e":"code","43621c85":"code","d49d03b7":"code","d737f07a":"code","45db8bf6":"code","c26372e4":"code","afa436af":"code","b2c25113":"code","9902a1f3":"code","8e096b3e":"code","6f568dbf":"code","9b1e0db7":"code","de7bc922":"code","f4fdef4e":"code","7ebf60ec":"code","a2d9d90e":"code","5dcecedd":"code","c32c5d77":"code","80588091":"code","2753261d":"code","05fbf4ce":"code","b5e1a842":"code","f37a78a5":"code","cad3bf6c":"code","901e8be4":"code","232d1d44":"code","6b08b267":"code","240d7591":"code","dbe54772":"markdown"},"source":{"cd4e8ce6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6f7516c2":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.impute import KNNImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import Lasso,Ridge,BayesianRidge,ElasticNet,HuberRegressor,LinearRegression,LogisticRegression,SGDRegressor\nfrom sklearn.metrics import mean_squared_error","68713ebf":"xtrain = pd.read_csv('..\/input\/used-car-price-dataset-competition-format\/X_train.csv')\nxtest = pd.read_csv('..\/input\/used-car-price-dataset-competition-format\/X_test.csv')\nytrain= pd.read_csv('..\/input\/used-car-price-dataset-competition-format\/y_train.csv')\nytest=pd.read_csv('..\/input\/used-car-price-dataset-competition-format\/test_label\/y_test.csv')","fa85df19":"print(xtrain.shape)\nprint(ytrain.shape)\nprint(xtest.shape)\nprint(ytest.shape)\n","76c06b31":"xtest","bc205df1":"df = pd.concat([xtrain, ytrain], axis=1)\ndf.drop(df.columns[-2],axis=1,inplace=True)\ndf","0bcce2d5":"xtest=pd.concat([xtest, ytest], axis=1)\nxtest.drop(xtest.columns[-2],axis=1,inplace=True)\nxtest","d3c6e040":"df=pd.concat([df, xtest], axis=0)\ndf.reset_index(inplace = True)","14d52b4d":"def describe(df):                        # Function to explore major elements in a Dataset\n                                         # Will help to find null values present and deal with them\n  columns=df.columns.to_list()           # Function will help to directly find numerical and categorical columns\n  ncol=df.describe().columns.to_list()\n  ccol=[]\n  for i in columns:\n    if(ncol.count(i)==0):\n      ccol.append(i)\n    else:\n      continue\n  print('Name of all columns in the dataframe:')\n  print(columns)\n  print('')\n  print('Number of columns in the dataframe:')\n  print(len(columns))\n  print('')\n  print('Name of all numerical columns in the dataframe:')\n  print(ncol)\n  print('')\n  print('Number of numerical columns in the dataframe:')\n  print(len(ncol))\n  print('')\n  print('Name of all categorical columns in the dataframe:')\n  print(ccol)\n  print('')\n  print('Number of categorical columns in the dataframe:')\n  print(len(ccol))\n  print('')\n  print('------------------------------------------------------------------------------------------------')\n  print('')\n  print('Number of Null Values in Each Column:')\n  print('')\n  print(df.isnull().sum())\n  print('')\n  print('')\n  print('Number of Unique Values in Each Column:')\n  print('')\n  print(df.nunique())\n  print('')\n  print('')\n  print('Basic Statistics and Measures for Numerical Columns:')\n  print('')\n  print(df.describe().T)\n  print('')\n  print('')\n  print('Other Relevant Metadata Regarding the Dataframe:')\n  print('')\n  print(df.info())\n  print('')\n  print('')","a799924d":"describe(df)","ad0cd6ee":"fig = plt.figure(figsize=(16,8))\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nax=sns.boxplot(y='price', x='brand',data=df,palette=\"colorblind\")\nax.set(xlabel='Brand Name', ylabel='Market Value $',title='Brand vs Market Value')","f02a75ed":"df.groupby(by=['brand']).mean()","5d0600c4":"df.groupby(by=['brand']).mean().sort_values(['price'],ascending=False)","3f9a9d97":"# Ordinal Numeric Variables since some brands have higher selling value\n\ndf.replace({'brand' : { 'audi' : 1, 'bmw' : 2, 'merc' : 3, 'vw': 4, 'toyota': 5, 'ford': 6, 'hyundi': 7, 'skoda': 8, 'vauxhall':9 }},inplace=True)","0ff47e37":"fig = plt.figure(figsize=(12,8))\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nax=sns.boxplot(y='price', x='transmission',data=df,palette=\"colorblind\")\nax.set(xlabel='transmission', ylabel='Market Value $',title='Transmission vs Market Value')","9790e79f":"df['transmission'].value_counts()\ndf=df[df.transmission!='Other']","e92889dc":"fig = plt.figure(figsize=(12,8))\nplt.style.use('seaborn')\nplt.tight_layout()\nsns.set_context('talk')\nax=sns.boxplot(y='price', x='fuelType',data=df,palette=\"colorblind\")\nax.set(xlabel='Fuel Type', ylabel='Market Value $',title='Fuel Type vs Market Value')","11e99fca":"df['fuelType'].value_counts()\ndf=df[df.fuelType!='Electric']","99210d5e":"plt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('Engine Size vs Price in $')\nsns.scatterplot( x=\"engineSize\",y='price', hue=\"transmission\",data=df)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Transmission')","43621c85":"plt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('MPG vs Price in $')\nsns.scatterplot( x=\"mpg\",y='price',data=df)\n","d49d03b7":"plt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('Year vs Price in $')\nsns.scatterplot( x=\"year\",y='price',data=df)\n","d737f07a":"plt.tight_layout()\nplt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(8, 6))\nsns.set_context('talk')\nplt.title('Mileage vs Price in $')\nsns.scatterplot( x=\"mileage\",y='price', hue=\"brand\",data=df)\nplt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0, title='Transmission')","45db8bf6":"def outliers(df_column):\n  q75, q25 = np.percentile(df_column, [75 ,25]) \n  iqr = q75 - q25\n  print('q75: ',q75)\n  print('q25: ',q25)\n  print('Inter Quartile Range: ',round(iqr,2))\n  print('Outliers lie before', q25-1.8*iqr, 'and beyond', q75+1.8*iqr) \n\n  # Usually 1.5 times IQR is considered, but we have used 1.8 for broader range since datapoints are very less\n\n  print('Number of Rows with Left Extreme Outliers:', len(df[df_column <q25-1.8*iqr]))\n  print('Number of Rows with Right Extreme Outliers:', len(df[df_column>q75+1.8*iqr]))\n  plt.tight_layout()\n  plt.style.use('seaborn')\n  sns.set_context('notebook')\n  sns.histplot(data=df, x=df_column,multiple=\"stack\")\n  print('')","c26372e4":"outliers(df['mpg'])","afa436af":"df=df[df.mpg<300]","b2c25113":"outliers(df['tax'])","9902a1f3":"outliers(df['engineSize'])","8e096b3e":"def OHE(dfcolumn):\n  global df\n  dfcolumn.nunique()\n  len(df.columns)\n  finallencol = (dfcolumn.nunique() - 1) + (len(df.columns)-1)\n  dummies = pd.get_dummies(dfcolumn, drop_first=True, prefix=dfcolumn.name)\n  df=pd.concat([df,dummies],axis='columns')\n  df.drop(columns=dfcolumn.name,axis=1,inplace=True) # We have to drop columns to aviod multi-collinearity\n  if(finallencol==len(df.columns)):\n    print('One Hot Encoding was successful!') \n    print('')\n  else:\n    print('Error in OHE XXXX')\n  return df","6f568dbf":"OHE(df['transmission'])\nOHE(df['fuelType'])\n","9b1e0db7":"df.drop(['index'],axis=1,inplace=True)\ndf","de7bc922":"vif = df.copy()\nvif.drop(['model', 'price'], axis=1,inplace=True)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(vif.values, i)\n                          for i in range(len(vif.columns))]","f4fdef4e":"vif_data","7ebf60ec":"vif = df.copy()\nvif.drop(['model','price','year'], axis=1,inplace=True)\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(vif.values, i)\n                          for i in range(len(vif.columns))]","a2d9d90e":"vif_data","5dcecedd":"# Scale Data For Higher Efficiency\nfrom sklearn.preprocessing import StandardScaler # Converts Columnar Data into Standard Normal Distribution\nscaler=StandardScaler()\nscaler.fit(vif)\nscaled_data=scaler.transform(vif)\nscaled_data","c32c5d77":"from sklearn.decomposition import PCA # Reduce Dimensions by Principal Component Analysis To Compensate for Variables with High VIF\npca=PCA(n_components=5)\npca.fit(scaled_data)\nx_pca=pca.transform(scaled_data)\nx_pca","80588091":"print(x_pca.shape)\nprint(df['price'].shape)\n","2753261d":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['price'], test_size=0.2, random_state=42)\nreg = LinearRegression()\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Linear Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Linear Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","05fbf4ce":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['price'], test_size=0.2, random_state=42)\nreg=Ridge(alpha=0.01)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Ridge Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Ridge Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","b5e1a842":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['price'], test_size=0.2, random_state=42)\nreg = RandomForestRegressor(random_state=123)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of Random Forest Regressor Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of Random Forest Regressor Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","f37a78a5":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['price'], test_size=0.2, random_state=42)\nreg=ElasticNet(alpha=0.005)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of ElacticNet Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of ElacticNet Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","cad3bf6c":"from sklearn.tree import DecisionTreeRegressor\nx_train, x_test, y_train, y_test = train_test_split(x_pca, df['price'], test_size=0.2, random_state=4)\nreg=DecisionTreeRegressor()\nreg.fit(x_train, y_train)\nprint('Test Accuracy of DecisionTree Regression: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of DecisionTree Regression:',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","901e8be4":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 103, stop = 300, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n","232d1d44":"rf = RandomForestRegressor()\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(x_train, y_train)","6b08b267":"rf_random.best_params_","240d7591":"x_train, x_test, y_train, y_test = train_test_split(x_pca, df['price'], test_size=0.2, random_state=4)\nreg=RandomForestRegressor(n_estimators=168, min_samples_split= 5, min_samples_leaf= 1, max_features= 'sqrt',max_depth= 100, bootstrap= False)\nreg.fit(x_train, y_train)\nprint('Test Accuracy of RandomForestRegressor: ',round(100*reg.score(x_test, y_test),2),'%')\nprint('')\nprint('Train Accuracy of RandomForestRegressor :',round(100*reg.score(x_train, y_train),2),'%')\nprint('')\ny_pred=reg.predict(x_test)\nprint('Mean Squared Error (MSE): ',round(np.sqrt(mean_squared_error(y_test,y_pred)),4))","dbe54772":"All models are subject to betterment with more stringent hyper-parameter tuning. This can be achieved by random selection, brute force methods, etc. Various other classifiers can also be used, but the most standard classifiers have been considered in this notebook.\n\nThe best score was **92.57%** with the best **MSE being around 4900**\n\nCar brand has been transformed into **ordinal catergorical variable**.\n\nRecommend standard practices for data transformation, outlier detection, and vizualization have been incorporated in this notebook"}}