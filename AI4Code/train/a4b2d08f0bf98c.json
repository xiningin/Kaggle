{"cell_type":{"64cb5457":"code","fc9887be":"code","724150c8":"code","2a616974":"code","3adce836":"code","71090890":"code","760eb286":"code","f6a282a8":"code","65e786e3":"code","c291fa42":"code","8a5e5b53":"code","31c361b0":"code","b55eca87":"code","1b8f0b53":"code","08247799":"code","ab561ded":"code","ce289da9":"code","d3020ecf":"code","c8576a13":"code","a0d6e71a":"code","09636538":"code","a9d3bf2e":"code","f7dd0929":"code","0d4971af":"code","852b0b4d":"code","17420151":"code","e130da08":"code","cc43eb71":"code","efae5ae3":"code","cdba93f9":"code","c37db809":"code","7721e961":"code","3ca996e6":"code","5cb957cc":"code","6a1501cb":"markdown","27aa06ed":"markdown"},"source":{"64cb5457":"#!pip install --upgrade linear-tree","fc9887be":"# Regular imports\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport lightgbm as lhgbm\n\n#For Visualizations\nimport pandas_profiling as pp\nimport matplotlib as mpl\nimport seaborn as sns\n\n#Ensemble Models\nimport xgboost as xgb\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set()\nplt.style.use('classic')\npd.set_option('max_columns', None)\n\nfrom sklearn.linear_model import *\n\n\n#To Split Data \nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n\n#To Preprocess Data \nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, OrdinalEncoder, FunctionTransformer, OneHotEncoder, KBinsDiscretizer, PolynomialFeatures, PowerTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neighbors import LocalOutlierFactor\n\n#To Pipeline the process \nfrom sklearn.compose import ColumnTransformer, TransformedTargetRegressor\nfrom sklearn.pipeline import Pipeline\n\n# used in Utilities\/Functions section\nfrom scipy import stats\n\n## Import Models\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n#from lineartree import  LinearTreeRegressor, LinearBoostRegressor\n\n\n# Import metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n\n\n\n# variables\ntarget_feature = 'target' \nrandom_seed    = 42","724150c8":"#Python libraries and their versions used for this problem\nprint('SciKit Learn:',sk.__version__)\nprint('Pandas:',pd.__version__)\nprint('Numpy:',np.__version__)\nprint('Seaborn:',sns.__version__)\nprint('MatPlot Library:', mpl.__version__)\nprint('XG Boost:',xgb.__version__)\nprint('Pandas Profiling:', pp.__version__)\nprint('LightGBM:', lhgbm.__version__)","2a616974":"#Define Utilities\/Functions\n\n# Function to show essential info about Dataset\ndef ShowEssentialInfo(df):\n    # Check No of rows & columns\n    print(\"\\n\",'*** Shape:',df.shape)\n\n    # Check Info about Object types of data\n    print(\"\\n\",'*** Info:')\n    df.info()\n\n    #Count missing values \n    print(\"\\n\",'*** Missing values:')\n    print(df.isnull().sum())\n\n    #Statistics of numerical columns\n    print(\"\\n\",'*** Data Statistics:')\n    print(df.describe(include='all'))\n\ndef treatoutliers(df=None, columns=None, factor=1.5, method='IQR', treatment='cap'):\n\n    for column in columns:\n        if method == 'STD':\n            permissable_std = factor * df[column].std()\n            col_mean = df[column].mean()\n            floor, ceil = col_mean - permissable_std, col_mean + permissable_std\n        elif method == 'IQR':\n            Q1 = df[column].quantile(0.25)\n            Q3 = df[column].quantile(0.75)\n            IQR = Q3 - Q1\n            floor, ceil = Q1 - factor * IQR, Q3 + factor * IQR\n#         print(floor, ceil)\n        if treatment == 'remove':\n            print(treatment, column)\n            df = df[(df[column] >= floor) & (df[column] <= ceil)]\n        elif treatment == 'cap':\n            print(treatment, column)\n            df[column] = df[column].clip(floor, ceil)\n\n    return df\n    \ndef get_sample_dataset(df, categorical_features, limit=1000):\n    rows = []\n    df_sub = pd.DataFrame()\n    for x in categorical_features:\n        for idx,name in enumerate(df[x].value_counts().index.tolist()):\n            nrows = df[x].value_counts()[idx]\n            nsample = min(nrows, limit)\n            data = X_full.loc[X_full[x] == name].sample(n=nsample)\n            #print(data.info())\n            df_sub = df_sub.append(data)\n    return df_sub\n\ndef log_model(model):\n    return TransformedTargetRegressor(model, func=np.log1p, inverse_func=np.expm1)","3adce836":"X_full = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\nX_test_full = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n#X_full = pd.concat([X_full,X_test_full]) \n\ntarget_feature ='target'","71090890":"# Identify Numeric \/ Categorical features this would be used to transform features later \n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [cname for cname in X_full.columns if\n                    X_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_full.columns if \n                X_full[cname].dtype in ['int64', 'float64']\n                 ]\n\n#remove target column from Numeric \/ categorical features\nif numeric_features.count(target_feature) == 1:\n    numeric_features.remove(target_feature)\nelif  categorical_features.count(target_feature) == 1:\n    categorical_features.remove(target_feature)\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\nprint('categorical_features({}):'.format(len(categorical_features)),categorical_features)\nprint('numeric_features({}):'.format(len(numeric_features)), numeric_features)\nprint('my_features:({})'.format(len(my_features)), my_features)","760eb286":"ShowEssentialInfo(X_full)","f6a282a8":"ShowEssentialInfo(X_test_full)","65e786e3":"%%time\nlof = LocalOutlierFactor()\nyhat = lof.fit_predict(X_full[numeric_features])\n# select all rows that are not outliers\nmask = yhat != -1","c291fa42":"pd.DataFrame(yhat).value_counts()","8a5e5b53":"# let's visualize the no of distinct categories for the categorical features \nX_full[categorical_features].nunique().plot.bar(figsize=(12,6))\n\n# add labels and title\nplt.ylabel('Number of unique categories')\nplt.xlabel('Variables')\nplt.title('Cardinality')","31c361b0":"#sns.lmplot(x=\"cont1\", y=\"target\", data=X_full[:1000], order=1)","b55eca87":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=X_full, orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns', fontsize=16);","1b8f0b53":"\n#remove outliers from target column \n#for colName in [['target']]:\n    #X_full = treatoutliers(df=X_full,columns=colName, treatment='remove')         \n    \n#Quantile-based Flooring and Capping\nfor colName in [['target','cont0','cont6','cont8']]:\n    X_full = treatoutliers(df=X_full,columns=colName, treatment='cap')      ","08247799":"## Box Plot for Outliers\nfig = plt.figure(figsize=(18,6))\nsns.boxplot(data=X_full, orient=\"h\", palette=\"Set2\");\nplt.xticks(fontsize= 14)\nplt.title('Box plot of numerical columns after handling', fontsize=16);","ab561ded":"#X_random_subset = get_sample_dataset(X_full, categorical_features, 100)\nX_random_subset = X_full.copy()\n\n# Remove rows with missing target, separate target from predictors\n#X_random_subset.dropna(axis=0, subset=[target_feature], inplace=True)\n#y = X_random_subset[target_feature]\n#X_random_subset.drop([target_feature], axis=1, inplace=True)\n\nX_random_subset.dropna(axis=0, subset=[target_feature], inplace=True)\ny = X_random_subset.pop(target_feature)\n","ce289da9":"ShowEssentialInfo(X_random_subset)\n","d3020ecf":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(X_random_subset, y)","c8576a13":"def plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \nprint(mi_scores)\n# print(mi_scores.tail(20))  # uncomment to see bottom 20\n\nplt.figure(dpi=100, figsize=(8, 5))\nplot_mi_scores(mi_scores)\n# plot_mi_scores(mi_scores.tail(20))  # uncomment to see bottom 20","a0d6e71a":"X_full['logtarget'] = np.log2(X_full['target'])","09636538":"# Plot against Target feature\nfeature = \"cont12\"\nsns.lmplot(\n    x=feature, y=target_feature, hue=\"cat5\", col=\"cat5\",\n    data=X_full.sample(n=10000), scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);\n\nfeature = \"cont10\"\nsns.lmplot(\n    x=feature, y=target_feature, hue=\"cat5\", col=\"cat5\",\n    data=X_full.sample(n=10000), scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);\n\nlog_target= 'logtarget'\n# Plot against log of Target feature\nfeature = \"cont12\"\nsns.lmplot(\n    x=feature, y=log_target, hue=\"cat5\", col=\"cat5\",\n    data=X_full.sample(n=10000), scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);\n\nfeature = \"cont10\"\nsns.lmplot(\n    x=feature, y=log_target, hue=\"cat5\", col=\"cat5\",\n    data=X_full.sample(n=10000), scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);\n","a9d3bf2e":"#Drop logtarget as there is no much difference by adding this transformation. \nX_full.drop(columns=['logtarget'])","f7dd0929":"# Since we have more data using 90\/10 split\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_random_subset, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=random_seed)","0d4971af":"# Identify Numeric \/ Categorical features this would be used to transform features later \n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_features  = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() <= 15 and \n                    X_train_full[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumeric_features  = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ['int64', 'float64']\n                 ]\n\n#remove target column from Numeric \/ categorical features\nif numeric_features.count(target_feature) == 1:\n    numeric_features.remove(target_feature)\nelif  categorical_features.count(target_feature) == 1:\n    categorical_features.remove(target_feature)\n\n# Keep selected columns only\nmy_features = categorical_features + numeric_features\n\nprint('categorical_features({}):'.format(len(categorical_features)),categorical_features)\nprint('numeric_features({}):'.format(len(numeric_features)), numeric_features)\nprint('my_features:({})'.format(len(my_features)), my_features)","852b0b4d":"X_train = X_train_full[my_features]\nX_valid = X_valid_full[my_features]\nX_test = X_test_full[my_features]","17420151":"ShowEssentialInfo(X_train)","e130da08":"%%time\n\n# Preprocessing for numerical data\nnumerical_transformer = Pipeline(steps=[\n       ('imputer', SimpleImputer(strategy='mean')) ## To Deal with Missing Values\n        #,('PowerT',PowerTransformer())\n        #,('Poly', PolynomialFeatures(interaction_only=True))\n        # ,('Kbins',KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans'))\n        ,('Outlierscaler', RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True))     ## To Deal with Outliers\n        ,('scaler', StandardScaler())  ## To minimise the impact of Outliers\n       #,('scaler', MinMaxScaler())      ## To minimise the impact of Outliers\n      \n])\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')) \n    #,('onehot', OneHotEncoder(handle_unknown='ignore'))\n    ,('scaler', OrdinalEncoder())\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numeric_features)\n        #,('cat', categorical_transformer, categorical_features)\n    ],\n    remainder=\"passthrough\"\n  )","cc43eb71":"X_train_pp = preprocessor.fit_transform(X_train[numeric_features])","efae5ae3":"%%time\n#from sklearn.kernel_ridge import KernelRidge\n#from sklearn.svm import SVR\n#from sklearn.tree import DecisionTreeRegressor\n#from sklearn.neighbors import KNeighborsRegressor\n#from sklearn.gaussian_process import GaussianProcessRegressor\n#from sklearn.neural_network import MLPRegressor\n\nnRows = len(X_train_pp)\n#nRows = 30000\n\n# Define the model Parameters, can be optimized using either Optuna or Grid Search CV\n\n#lgbm_params = {'n_estimators' : 15000, 'max_depth' : 2, 'learning_rate' : 0.1, 'subsample' : 0.95, 'colsample_bytree' : 0.85, 'reg_alpha' : 30.0, 'reg_lambda' : 25.0 , 'num_leaves' : 4, 'max_bin' : 512, 'random_state' : random_seed}\n#xgb_params = {'n_estimators': 15000, 'max_depth': 3, 'learning_rate': 0.036, 'gamma': 0, 'min_child_weight': 1, 'subsample': 0.79, 'colsample_bytree': 0.112, 'reg_alpha': 23.132, 'reg_lambda': 0.0009, 'random_state':random_seed}\n#xgb_params = {'n_estimators': 3337, 'max_depth': 3, 'learning_rate': 0.048, 'gamma': 0.9, 'min_child_weight': 3, 'subsample': 1.0, 'colsample_bytree': 0.5, 'reg_alpha': 1.0, 'reg_lambda': 0.9, 'random_state' : random_seed}\n#xgb_params = {'n_estimators': 3248, 'max_depth': 3, 'learning_rate': 0.08, 'gamma': 0.2, 'min_child_weight': 3, 'subsample': 1.0, 'colsample_bytree': 0.5, 'reg_alpha': 1.0, 'reg_lambda': 0.8, 'random_state':random_seed}\n\nlgbm_params = {'n_estimators' : 15000, 'max_depth' : 2, 'learning_rate' : 0.1, 'subsample' : 0.95, 'colsample_bytree' : 0.85, 'reg_alpha' : 30.0, 'reg_lambda' : 25.0 , 'num_leaves' : 4, 'random_state' : random_seed, 'device':'gpu'}\nxgb_params = {'n_estimators': 15000, 'max_depth': 3, 'learning_rate': 0.036, 'gamma': 0, 'min_child_weight': 1, 'subsample': 0.79, 'colsample_bytree': 0.112, 'reg_alpha': 23.132, 'reg_lambda': 0.0009, 'random_state':random_seed, 'tree_method':'gpu_hist', 'predictor':'gpu_predictor'}\n\n\nestimators = {\n              #'mlp': MLPRegressor(activation='tanh', hidden_layer_sizes= (100,3) ,learning_rate='adaptive', solver='adam', max_iter=100),\n              #'dtr': DecisionTreeRegressor(max_depth=3),\n              #'rdg': Ridge(alpha=0.05, normalize=True),\n              #'las': Lasso(alpha=0.5, normalize=True),\n              #'lin': LinearRegression(normalize=True),\n              #'bag': BaggingRegressor(RandomForestRegressor(n_estimators=50, random_state=random_seed, max_depth=3), n_estimators=2, random_state=random_seed, max_samples=0.8, max_features=0.7, bootstrap=True, bootstrap_features=True, n_jobs=-1), \n              #'ran': RandomForestRegressor(n_estimators=5000, random_state=random_seed, max_depth=4),\n              'xgb' : XGBRegressor(**xgb_params),\n              'lgbm': LGBMRegressor(**lgbm_params)  \n              #'ltr' : LinearBoostRegressor(Ridge(), loss='square', n_estimators=200)\n}\n\n\nfor name, estimator in estimators.items():\n    estimator = estimator.fit(X_train_pp[:nRows], y_train[:nRows])","cdba93f9":"%%time\nX_valid_pp = preprocessor.fit_transform(X_valid[numeric_features])\nn_estimators, n_samples = len(estimators), X_valid_pp.shape[0]\ny_individual = np.zeros((n_samples, n_estimators))\n\nX_test_pp = preprocessor.fit_transform(X_test[numeric_features])\nn_estimators, n_samples = len(estimators), X_test_pp.shape[0]\ny_test_individual = np.zeros((n_samples, n_estimators))\n\nfor i, (model, estimator) in enumerate(estimators.items()):\n    y_individual[:, i] = estimator.predict(X_valid_pp)\n    y_test_individual[:,i] = estimator.predict(X_test_pp)\n    if model =='xgb':\n        xgb.plot_importance(estimator)\n    \n\ny_final     = np.mean(y_individual, axis=1)\ny_final_test = np.mean(y_test_individual, axis=1)","c37db809":"print('RMSE:',mean_squared_error(y_valid, y_final, squared=False))","7721e961":"sns.distplot(y_final, bins=30)\nplt.xlabel('Residuals')","3ca996e6":"import statsmodels.api as sm\nsm.qqplot(y_final)\n","5cb957cc":"# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': y_final_test})\noutput.to_csv('submission.csv', index=False)","6a1501cb":"All Columns are weekly correlated to target column","27aa06ed":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#print(X_random_subset['cat9'].value_counts(()).index.tolist())\n\n## Feature Selection using variance_inflation_factor\ndef cal_vif(X, thresh=5):\n    output = pd.DataFrame()\n    k =X_vif.shape[1]\n    vif = [variance_inflation_factor(X_vif.values, i) for i in range(k)]\n    for i in range(1,k):\n        print('Iteration No ', i)\n        print(vif)\n        a = int(np.argmax(vif))\n        if(vif[a]<=thresh):\n            print('break')\n            break\n        if(i==1):\n            print('i=1', i)\n            output=X_vif.drop(X_vif.columns[a], axis=1)  \n        elif(i>1):\n            print('i>1', i)\n            output=output.drop(output.columns[a], axis=1)\n        l = len(output.columns)    \n        vif=[variance_inflation_factor(output.values, j) for j in range(l)]\n    return(output)     \n\n\n# creating dummies for categorical_features\n\n# the independent variables set\nX_vif=X_Full.copy()\ny_vif=X_vif[target_feature]\nX_vif=X_vif.drop(target_feature,1)\n\n#{'A':0, 'B':1, 'C':2, 'D':3, 'E':5, 'F':6,'G':7,'H':8,'I':9,'J':10,'K':11,'L':12,'M':13,'N':14,'O':15}\nX_vif[categorical_features] = X_vif[categorical_features] = X_vif[categorical_features].applymap(lambda x: ord(x)-65)\n\nselected_features = cal_vif(X_vif, 6)\nselected_features.head()"}}