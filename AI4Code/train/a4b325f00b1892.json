{"cell_type":{"09d70ff1":"code","9233ec69":"code","d87a5b98":"code","3d66291e":"code","6a37f5a8":"code","6e0cbc0e":"code","63e7d8fc":"code","827eb570":"code","0f1cd522":"code","76f92046":"code","fa45f732":"code","e6cb746a":"code","f045619c":"code","4dc337f1":"code","1f492e72":"code","4ae2a10e":"code","72fc194b":"code","1035e334":"code","8d8e7663":"code","7292f2b2":"code","b9e31813":"code","dc5b33fd":"code","c0a1a528":"code","89a1e2a1":"code","fcf7213c":"code","2f08ceb0":"code","c1efc6a2":"code","ac18171c":"code","777d17e4":"code","4a768268":"code","1d26045e":"code","6448a1d2":"code","267fc4f8":"code","a58f6f61":"markdown","0c2438f7":"markdown","e1c213e5":"markdown","c561c8fe":"markdown","00ee4f3d":"markdown","95b7fa2e":"markdown","02ef7fe3":"markdown","efca03e8":"markdown","002ef5df":"markdown","3d04d0f0":"markdown","3441566f":"markdown","38c6e6b3":"markdown","89a5a376":"markdown","954861b6":"markdown","21ac9046":"markdown","2eb8f15d":"markdown","47548039":"markdown","f848f8b2":"markdown","62d24cee":"markdown","5f6d34f7":"markdown"},"source":{"09d70ff1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n# data visualization libraries.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9233ec69":"# in read_csv() function, we have passed the location of input data file. \n\nred_wine_data = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","d87a5b98":"# Take a closer look at the data,  we take help of the \".head()\" function of th pandas library which returns the first five \n# observations of the data set. \n\n# similarly, tail() returns the last five observations of the dataset.\n\nred_wine_data.head()","3d66291e":"#shape\n\nred_wine_data.shape","6a37f5a8":"red_wine_data.describe()","6e0cbc0e":"red_wine_data.columns","63e7d8fc":"red_wine_target = red_wine_data['quality']\nred_wine_target.head()","827eb570":"red_wine_target.unique()","0f1cd522":"red_wine_target.value_counts()","76f92046":"red_wine_data.rename(columns={'fixed acidity':'fixed_acidity', 'volatile acidity': 'volatile_acidity', 'citric acid':'citric_acid', \n                              'residual sugar':'residual_sugar','free sulfur dioxide':'free_sulfur_dioxide', 'total sulfur dioxide':'total_sulfur_dioxide',},\n                        inplace = True)\n","fa45f732":"red_wine_data.columns","e6cb746a":"# info()\nred_wine_data.info()","f045619c":"# missing values\n\n#isna()\n\nred_wine_data.isna().sum()","4dc337f1":"# isnull()\n\nred_wine_data.isnull().sum()","1f492e72":"# check for duplicates\n\n# duplicate()\n\nduplicate_entries = red_wine_data[red_wine_data.duplicated()]\nduplicate_entries.shape","4ae2a10e":"red_wine_data.hist(bins=10, figsize=(16,12))\nplt.show()","72fc194b":"red_wine_data.corr()","1035e334":"plt.figure(figsize=(16,12))\nsns.heatmap(red_wine_data.corr(), cmap='bwr', annot =True) # annot = True: to display the correlation value in the graph\n","8d8e7663":"# count plot\n\nplt.figure(figsize=(12,8))\nsns.countplot(red_wine_data.quality)","7292f2b2":"sns.pairplot(red_wine_data)","b9e31813":"# sns.boxplot(y, red_wine_data['alcohol'], palette='GnBu_d')\n# plt.title(\"Boxplot of Quality of alcohol\")\n# plt.show()","dc5b33fd":"bins = (2, 6, 8)\ngroup_names = ['bad','good']\n\nred_wine_data['quality'] = pd.cut(red_wine_data['quality'], bins = bins, labels = group_names)\n\nlabel_quality = LabelEncoder()\n\n# bad will be 0 and good will be 1\n\nred_wine_data['quality'] = label_quality.fit_transform(red_wine_data['quality'])\nprint(red_wine_data['quality'].value_counts())\nsns.countplot(red_wine_data['quality'])\nplt.show()\n               ","c0a1a528":"y = red_wine_data['quality']                            # set 'quality' as target variable(y)\nx = red_wine_data.drop(['quality'], axis=1)            # all coumns except quality are input variables(X)","89a1e2a1":"y","fcf7213c":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.25,  random_state = 50)","2f08ceb0":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.fit_transform(x_test)\n\nx_test","c1efc6a2":"dtc = DecisionTreeClassifier(max_depth=200)\ndtc.fit(x_train, y_train)\ndtc_pred = dtc.predict(x_test)\ndtc_acc = accuracy_score(y_test, dtc_pred)\ndtc_acc","ac18171c":"Ks = 100\nmean_acc = np.zeros(Ks-1)\n\nfor n in range(1, Ks):\n    dtc = DecisionTreeClassifier(max_depth = n).fit(x_train, y_train)\n    yhat = dtc.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    \nmean_acc","777d17e4":"print(\"the best accuracy is\", mean_acc.max(), \"with depth =\", mean_acc.argmax()+1)","4a768268":"cf = metrics.classification_report(yhat, y_test)\nprint(cf)","1d26045e":"rfc = RandomForestClassifier()\nrfc.fit(x_train, y_train)\nrfc_preds = rfc.predict(x_test)\nrfc_acc = accuracy_score(rfc_preds, y_test)\nprint (100*rfc_acc)","6448a1d2":"Ks = 100\nmean_acc = np.zeros(Ks-1)\n\nfor n in range(1, Ks):\n    rfc = RandomForestClassifier(n_estimators = n).fit(x_train, y_train)\n    yhat = rfc.predict(x_test)\n    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n    \nmean_acc","267fc4f8":"print ('The best accuracy is ', 100*mean_acc.max(), \"with n_estimator = \", mean_acc.argmax()+1)","a58f6f61":"# Decision Tree ","0c2438f7":"Observations:\n* The distribution of the attribute \u201calcohol\u201d seems to be positively skewed i.e the curve is shifted towards the left.\n* The attributes 'density' and 'pH' are quite normally distributed.\n* Now looking at the attribute quality, we can observe that the wines with average quality (i.e. quality rating 5 to 7) are more than wines with bad(1-4) or good(8-10) quality.","e1c213e5":"# Random Forest Classifier","c561c8fe":"Observations:\n* Alcohol has the highest positive correlation with wine quality, followed by the various other variables such as acidity, sulphates, density & chlorides.\n* There is a relatively high positive correlation between fixed_acidity and citric_acid, fixed_acidity and density.\n* There is a relatively high negative correlation between fixed_acidity and pH.\n* Density has a strong positive correlation with fixed_acidity, whereas it has a strong negative correlation with alcohol.\n* citric acid & volatile acidity have negative correlation.\n* free sulphur dioxide & total sulphur dioxide have positive correlation.","00ee4f3d":"The average(5-7) quality of wines are more than good(1-4) and bad(8-10) quality of wines.","95b7fa2e":"Dviding wine as good and bad by giving the limit for the quality.","02ef7fe3":"Histograms use bars to visualize data as well. Many people may not even realize there is a difference between a histogram and a bar chart. They practically look the same from a distance.\n\nThe key is that a histogram looks solely at quantitative variables while a bar chart looks at categorical variables. That\u2019s why the bars in a histogram are typically grouped together without spacing in between the bars.","efca03e8":"Scaling","002ef5df":"The describe() function in pandas is very handy in getting various summary statistics. This function returns the count, mean, standard deviation, minimum and maximum values and the quantiles of the data","3d04d0f0":"* The above plot shows the increase in the quality of wine with an increase in alcohol. The quality of the wine is directly related to the amount of alcohol in the wine. More the alcohol in the wine, the better will be the quality.\n* Also, the points lying outside the whiskers(the lines extending from the rectangular box) are the outliers.","3441566f":"# Data Visualization","38c6e6b3":"# Import Libraries","89a5a376":"# Correlation","954861b6":"# Preprocesser Encoding","21ac9046":"Correlation:\nCorrelation is a statistical measure. Data correlation is a way to understand the relationship between multiple values or features in your dataset.\n\nEvery single successful data science project revolves around finding accurate correlations between the input and target variables. However more than often, we oversee how crucial correlation analysis is. \n\nIt is recommended to perform correlation analysis before and after data gathering and transformation phases of a data science project.\n\n There are three different types of correlations:\n\n* Positive Correlation: Two features (variables) can be positively correlated with each other. It means that when the value of one variable increases then the value of the other variable(s) also increases (also decreases when the other decreases).\nEg. The more time you spend running on a treadmill, the more calories you will burn.\n\n* Negative Correlation: Two features (variables) can be negatively correlated with each other. This occurs when the value of one variable increases and the value of another variable(s) decreases (inversely proportional).\nEg. As the weather gets colder, air conditioning costs decrease.\n\n* No Correlation: Two features might not have any relationship with each other. This happens when the value of a variable is changed then the value of the other variable is not impacted.\nEg. There is no relationship between the amount of tea drunk and level of intelligence.\n* Each of these correlation types exists in a spectrum represented by values from -1 to +1 where slight or high positive correlation features can be like 0.5 or 0.7.\n* A very strong and perfect positive correlation is represented by a correlation score of 0.9 or 1.\n* If there is a strong negative correlation, it will be represented by a value of -0.9 or -1. Values close to zero indicates no correlation.","2eb8f15d":"# Exploratory Data Analysis","47548039":"# Data Modeling","f848f8b2":"Decision Tree Classification Report","62d24cee":"A box plot is a great way to get a visual sense of an entire range of data. It can tell you about your outliers and what their values are. It can also tell you if your data is symmetrical, how tightly your data is grouped, and if and how your data is skewed.\n\nBox plots divides data into its quartiles. The \u201cbox\u201d shows a user the data set between the first and third quartiles.\n\nThe median gets drawn somewhere inside the box and then you see the most extreme non-outliers to finish the plot. Those lines are known as the \u201cwhiskers\u201d. If there are any outliers then those can be plotted as well.\n\nWith box plots you can answer how diverse or uniform your data might be. You can identify what is normal and what is extreme. Box plots help give a shape to your data that is broad without sacrificing the ability to look at any piece and ask more questions.\n\nIt displays the five-number summary of a set of data. The five-number summary is:\n\n* minimum\n* first quartile (Q1)\n* median\n* third quartile (Q3)\n* maximum","5f6d34f7":"Rename columns"}}