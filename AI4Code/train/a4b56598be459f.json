{"cell_type":{"42f11d36":"code","691c3e7c":"code","d7f073ab":"code","6dde1d36":"code","a4218c4c":"code","acf1386e":"code","550eeb8f":"code","394a477d":"code","9d792555":"code","4d7512b4":"code","265cb669":"code","73e20d54":"code","bd20a5dd":"code","d3d01459":"code","2796f87a":"code","681994a7":"code","027aeff5":"code","007fa939":"code","855cf48f":"code","4caa6ade":"code","8d437501":"code","9780b570":"code","8970182f":"code","7dc3ebaa":"code","69f0e93d":"code","2da81378":"code","9e7ace6e":"code","6c92dcde":"markdown","e89e942a":"markdown"},"source":{"42f11d36":"import numpy as np\nimport pandas as pd\nimport nltk\nimport re","691c3e7c":"# Loading the dataset of tweets\n\ndf_train=pd.read_csv('..\/input\/train.csv')\ndf_test=pd.read_csv('..\/input\/test.csv')","d7f073ab":"# Information about the dataset\n\nprint(df_train.info())\nprint(df_train.head())","6dde1d36":"df_test.info()","a4218c4c":"# Checking Class Distribution\n\ndf_train['label'].value_counts()","acf1386e":"# Storing the tweets and the labels\n\ntweets = df_train['tweet'].str.lower()\ntweets_test = df_test['tweet'].str.lower()\nY = df_train['label']\n\ntweets","550eeb8f":"# Replacing @handle with the word USER\n\ntweets = tweets.str.replace(r'@[\\S]+', 'user')\ntweets_test = tweets_test.str.replace(r'@[\\S]+', 'user')\n\n# Replacing the Hast tag with the word hASH\n\ntweets = tweets.str.replace(r'#(\\S+)','hash')\ntweets_test = tweets_test.str.replace(r'#(\\S+)','hash')\n\n# Removing the all the Retweets\n\ntweets = tweets.str.replace(r'\\brt\\b',' ')\ntweets_test = tweets_test.str.replace(r'\\brt\\b',' ')\n\ntweets","394a477d":"df_train['tweet'].str.extractall(r'((www\\.[\\S]+)|(http?:\/\/[\\S]+))')","9d792555":"# Replacing the URL or Web Address\n\ntweets = tweets.str.replace(r'((www\\.[\\S]+)|(http?:\/\/[\\S]+))','URL')\ntweets_test = tweets_test.str.replace(r'((www\\.[\\S]+)|(http?:\/\/[\\S]+))','URL')\n\n# Replacing Two or more dots with one\n\ntweets = tweets.str.replace(r'\\.{2,}', ' ')\ntweets_test = tweets_test.str.replace(r'\\.{2,}', ' ')","4d7512b4":"# Removing all the special Characters\n\ntweets = tweets.str.replace(r'[^\\w\\d\\s]',' ')\ntweets_test = tweets_test.str.replace(r'[^\\w\\d\\s]',' ')\n\n# Removing all the non ASCII characters\n\ntweets = tweets.str.replace(r'[^\\x00-\\x7F]+',' ')\ntweets_test = tweets_test.str.replace(r'[^\\x00-\\x7F]+',' ')\n\n# Removing the leading and trailing Whitespaces\n\ntweets = tweets.str.replace(r'^\\s+|\\s+?$','')\ntweets_test = tweets_test.str.replace(r'^\\s+|\\s+?$','')\n\n# Replacing multiple Spaces with Single Space\n\ntweets = tweets.str.replace(r'\\s+',' ')\ntweets_test = tweets_test.str.replace(r'\\s+',' ')\n\ntweets","265cb669":"# Removing the Stopwords\n\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\ntweets = tweets.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n\ntweets_test = tweets_test.apply(lambda x: ' '.join(word1 for word1 in x.split() if word1 not in stop_words))","73e20d54":"# Removing the words stem using Snowball Stemmer\n\nfrom nltk.stem import *\n\nSS = SnowballStemmer(\"english\")\n\ntweets = tweets.apply(lambda x: ' '.join(SS.stem(word) for word in x.split()))\n\ntweets_test = tweets_test.apply(lambda x: ' '.join(SS.stem(word1) for word1 in x.split()))\n\ntweets","bd20a5dd":"from nltk.tokenize import word_tokenize\n\n# Creating a Bag of Words\n\nwords = []\nwords_test = []\n\nfor text in tweets:\n    word = word_tokenize(text)\n    for i in word:\n        words.append(i)\n        \n        \nfor text in tweets_test:\n    word1 = word_tokenize(text)\n    for j in word1:\n        words_test.append(i)","d3d01459":"from nltk.probability import FreqDist\n\nwords = nltk.FreqDist(words)\nwords_test = nltk.FreqDist(words_test)\n\nprint (\"Total Number of words in train set {}\".format(len(words)))\nprint (\"First 30 most common words in train set {}\".format(words.most_common(30)))","2796f87a":"# Choosing the first 5000 words as Features\n\nword_features = list(words.keys())[:5000]","681994a7":"# Finding if a word in the word_features is present in the tweets\ndef finding_features(tweet):\n    text = word_tokenize(tweet)\n    features = {}\n    for i in word_features:\n        features[i] = (i in text)\n    return features\n\n\n# Zipping the Processed tweets with the Labels\ntweets_featlab = zip(tweets, Y)","027aeff5":"# Calling the finding_feature function for all the tweets\nfeature_set = [(finding_features(TW) ,label) for (TW,label) in tweets_featlab ]","007fa939":"# Calling the finding_feature function for all the test tweets\nfeature_test = tuple(finding_features(TW) for (TW) in tweets_test)","855cf48f":"seed=1\nnp.random.seed = seed\nnp.random.shuffle = feature_set\n\n# Splitting Training and Testing Datasets\nfrom sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(feature_set, test_size = 0.25, random_state = seed)\n\nprint ('Training Size: {}'.format(len(train)))\nprint ('Testing Size: {}'.format(len(test)))","4caa6ade":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom nltk.classify.scikitlearn import SklearnClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","8d437501":"# Model_1 DecisionTreeClassifier\n\nnltk_model1 = SklearnClassifier(DecisionTreeClassifier())\nnltk_model1.train(train)\n\naccuracy = nltk.classify.accuracy(nltk_model1, test)*100\nprint (\"Accuracy of Decision tree: {}\".format(accuracy))\n\n# Predictions for the test data set\npredictions_1 = nltk_model1.classify_many(feature_test)","9780b570":"# Model_2 Stochastic Gradient Descent\n\nnltk_model2 = SklearnClassifier(SGDClassifier(max_iter = 1000))\nnltk_model2.train(train)\n\naccuracy = nltk.classify.accuracy(nltk_model2, test)*100\nprint (\"Accuracy of SGD: {}\".format(accuracy))","8970182f":"# Model_3 RandomForestClassifier\n\nnltk_model3 = SklearnClassifier(RandomForestClassifier())\nnltk_model3.train(train)\n\naccuracy = nltk.classify.accuracy(nltk_model3, test)*100\nprint (\"Accuracy of Random Forest: {}\".format(accuracy))","7dc3ebaa":"# Model_4 Logistic Regression\n\nnltk_model4 = SklearnClassifier(LogisticRegression())\nnltk_model4.train(train)\n\naccuracy = nltk.classify.accuracy(nltk_model4, test)*100\nprint (\"Accuracy of Logistic Regression: {}\".format(accuracy))","69f0e93d":"test_1, label = zip(*feature_set)\n\n# Classification Report and Confusion Matrix for the Models\nmodels = [nltk_model1, nltk_model2, nltk_model3, nltk_model4]\nclassifiers = ['Decision Tree', 'SGD', 'Random forest', 'Logistic Regression']\ni = 0\nfor model in models:\n    predictions = model.classify_many(test_1)\n    class_=classifiers[i]\n    print (\"Report for {}\".format(class_))\n    print (classification_report(label, predictions))\n    pd.DataFrame(confusion_matrix(label, predictions))\n    i+=1","2da81378":"# Predictions for the test data set using the Best Model\npredictions_x = nltk_model1.classify_many(feature_test)","9e7ace6e":"# Saving the Result as csv\n\ndf_test['label'] = predictions_x\ndf_test.to_csv('Final CSV', columns = ['id','label'], index = False)","6c92dcde":"# Modelling","e89e942a":"# PREPROCESSING"}}