{"cell_type":{"576a69df":"code","7d957768":"code","54f1277c":"code","6013dddc":"code","285b4adb":"code","4a216d82":"code","fc3bc479":"code","399791cb":"code","597e7f1d":"code","3ae12254":"code","eae38b62":"code","e25e1f99":"code","f5f4346e":"code","a6a48562":"code","49873d1f":"code","7d324f5d":"code","317c0919":"code","42596e3f":"code","080a2603":"code","50aaf0f7":"code","9f06295a":"code","d00e47cd":"code","dac63435":"code","45a0636e":"markdown","8bf79918":"markdown","2b55863b":"markdown","59412e13":"markdown","41c876ea":"markdown"},"source":{"576a69df":"import re\nimport nltk\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nps = PorterStemmer()\nwordnet = WordNetLemmatizer()","7d957768":"message_df = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\")\nmessage_df.head()","54f1277c":"message_df.columns","6013dddc":"# Dropping the redundent looking collumns (for this project)\nto_drop = ['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4']\nmessage_df = message_df.drop(message_df[to_drop], axis=1)\n# Renaming the columns because I feel fancy today \nmessage_df.rename(columns = {\"v1\":\"label\", \"v2\":\"message\"}, inplace = True)\nmessage_df.head()","285b4adb":"message_df.shape","4a216d82":"message_df[\"message\"][0]","fc3bc479":"message_df[\"label\"].value_counts()","399791cb":"sns.countplot(x= message_df[\"label\"])\nplt.show()","597e7f1d":"message_df[message_df[\"label\"] == \"spam\"].head(10)","3ae12254":"sentences = message_df[\"message\"]\nsentences","eae38b62":"def preprocess_text_stemming(sentences):\n    '''\n    Removing Stop Words, Punctuation, Numbers and Performing Stemming\n    '''\n    corpus = []\n    for i in range(len(sentences)):\n        review = re.sub(\"[^a-zA-Z]\",\" \", sentences[i])\n        review = review.lower()\n        words = review.split()\n        words = [ps.stem(word) for word in words if not word in set(stopwords.words(\"english\"))]\n        new_sentence = \" \".join(words)\n        corpus.append(new_sentence)\n        \n    return corpus\n\ndef preprocess_text_lemma(sentences):\n    '''\n    Removing Stop Words, Punctuation, Numbers and Performing lemmatization\n    '''\n    corpus = []\n    for i in range(len(sentences)):\n        review = re.sub(\"[^a-zA-Z]\",\" \", sentences[i])\n        review = review.lower()\n        words = review.split()\n        words = [wordnet.lemmatize(word) for word in words if not word in set(stopwords.words(\"english\"))]\n        new_sentence = \" \".join(words)\n        corpus.append(new_sentence)\n        \n    return corpus","e25e1f99":"%%time\n# cleaned_corpus = preprocess_text_stemming(sentences)\ncleaned_corpus = preprocess_text_lemma(sentences)\nprint(len(cleaned_corpus))\ncleaned_corpus[:5]","f5f4346e":"words = nltk.word_tokenize(\" \".join(cleaned_corpus))\nprint(\"Total unique Words: \",len(set(words)))\nwords[:10]","a6a48562":"print(\"Total Words: \", len(words))\nprint(\"Total unique Words: \",len(set(words)))","49873d1f":"cv = CountVectorizer(max_features = 2500) # considering only the top 2500 features only\nX = cv.fit_transform(cleaned_corpus).toarray()\nX","7d324f5d":"X.shape","317c0919":"y = pd.get_dummies(message_df[\"label\"])\ny = y.iloc[:,1].values # Considering only one value, since from its we can predict the next","42596e3f":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.2, random_state = 41)\n\nprint(\"Shape of X_train: \", X_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)\nprint(\"Shape of y_train: \", y_train.shape)\nprint(\"Shape of y_test: \", y_test.shape)","080a2603":"model = MultinomialNB().fit(X_train,y_train)","50aaf0f7":"y_pred = model.predict(X_test)","9f06295a":"accuracy_score(y_test, y_pred)","d00e47cd":"confusion_matrix(y_test, y_pred)","dac63435":"print(classification_report(y_test, y_pred))","45a0636e":"## Train Test Split","8bf79918":"## Modelling","2b55863b":"## Text Preprocessing ","59412e13":"## BOW","41c876ea":"## Validation"}}