{"cell_type":{"edde2d39":"code","3ca96196":"code","c651cf57":"code","618db692":"code","7aab8b08":"code","e32a100e":"code","6797063f":"code","24cd045a":"code","2ed9ecbf":"code","32a56a10":"code","5831f725":"code","9a7f1933":"code","5bcb5d41":"code","81fcbaff":"code","4e6171f3":"code","19095688":"code","0240d8eb":"code","4fe36629":"code","fd1f1647":"code","bdb96715":"code","82916c4f":"code","47e08b00":"code","39375976":"code","10668856":"code","5f3624a4":"code","1e363d4d":"code","bd837e94":"code","1787791d":"code","2954c728":"code","add61116":"code","8542ab23":"code","97f6781c":"code","039fc577":"code","39392132":"code","b758d696":"code","3c3ec648":"code","45edc4d7":"code","19bbd9a3":"code","f4992f06":"code","944ef0f7":"code","6c0db1a6":"code","12b50f25":"code","73331d21":"markdown","a0db5844":"markdown","b15379a2":"markdown","36b31089":"markdown","47f0e1ff":"markdown","806e9722":"markdown","8858c09d":"markdown","09d899cc":"markdown","f9b47727":"markdown","bde16c4d":"markdown","2eaf28d7":"markdown","3060b124":"markdown","b4fd9e0e":"markdown","c58ceb70":"markdown","c8d246d9":"markdown","74926c82":"markdown","744351dd":"markdown","7d2d7b6f":"markdown","5a14f9d8":"markdown","60a58f38":"markdown","8bde53fd":"markdown","6870e2d6":"markdown","3a8888e7":"markdown","9802f281":"markdown","7bc16cb3":"markdown"},"source":{"edde2d39":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3ca96196":"import pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization","c651cf57":"df = pd.read_csv('\/kaggle\/input\/insurance-premium-prediction\/insurance.csv')","618db692":"type(df)","7aab8b08":"df.head(10)","e32a100e":"df.info()","6797063f":"df.isnull().sum()","24cd045a":"df.region.unique()","2ed9ecbf":"#check missing values\nplt.figure(figsize=(12,4))\nsns.heatmap(df.isnull(),cbar=False,cmap='viridis',yticklabels=False)\nplt.title('Missing value in the dataset');","32a56a10":"sns.lmplot(x='bmi',y='expenses',data=df,aspect=2,height=6)\nplt.xlabel('Boby Mass Index$(kg\/m^2)$: as Independent variable')\nplt.ylabel('Insurance Charges: as Dependent variable')\nplt.title('Charge Vs BMI');","5831f725":"df.corr()","9a7f1933":"# correlation plot\ncorr = df.corr()\nsns.heatmap(corr, cmap = 'Wistia', annot= True);","5bcb5d41":"f= plt.figure(figsize=(12,4))\n\nax=f.add_subplot(121)\nsns.distplot(df['expenses'],bins=40,color='r',ax=ax)\nax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.distplot(np.log10(df['expenses']),bins=40,color='b',ax=ax)\nax.set_title('Distribution of insurance charges in $log$ sacle')\nax.set_xscale('log');\n\n","81fcbaff":"f = plt.figure(figsize=(24,6))\nax = f.add_subplot(141)\nsns.violinplot(x='sex', y='expenses',hue='smoker',data=df,palette='Wistia',ax=ax,split=True)\nax.set_title('Violin plot of expenses vs sex')\n\nax = f.add_subplot(142)\nsns.violinplot(x='smoker', y='expenses',hue='sex',data=df,palette='magma',ax=ax,split=True)\nax.set_title('Violin plot of expenses vs smoker');\n\nax = f.add_subplot(143)\nsns.violinplot(x='region', y='expenses',hue='sex',data=df,palette='rainbow',ax=ax,split=True)\nax.set_title('Violin plot of expenses vs Region');\n\nax = f.add_subplot(144)\nsns.violinplot(x='region', y='expenses',hue='smoker',data=df,palette='husl',ax=ax,split=True)\nax.set_title('Violin plot of expenses vs Region');","4e6171f3":"f = plt.figure(figsize=(14,6))\nsns.violinplot(x='children', y='expenses',hue='sex',data=df,palette='magma',split=True)","19095688":"f = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.scatterplot(x='age',y='expenses',data=df,palette='magma',hue='smoker',ax=ax)\nax.set_title('Scatter plot of Charges vs age')\n\nax = f.add_subplot(122)\nsns.scatterplot(x='bmi',y='expenses',data=df,palette='viridis',hue='smoker')\nax.set_title('Scatter plot of Charges vs bmi')\n","0240d8eb":"#sns.lmplot(x='bmi',y='expenses',data=df,aspect=2,height=6)\n# f = plt.figure(figsize=(14,6))\n# ax = f.add_subplot(121)\n# sns.lmplot(x='age',y='expenses',data=df,palette='magma',hue='smoker',aspect=2,height=6,ax=ax)\n# ax.set_title('Scatter plot of Charges vs age')\n\n# ax = f.add_subplot(122)\n# sns.lmplot(x='bmi',y='expenses',data=df,palette='viridis',hue='smoker',aspect=2,height=6,ax=ax)\n# ax.set_title('Scatter plot of Charges vs bmi')\n\nsns.lmplot(x='age',y='expenses',data=df,palette='magma',hue='smoker',aspect=2,height=6)\nax.set_title('Scatter plot of Charges vs age')\nsns.lmplot(x='bmi',y='expenses',data=df,palette='viridis',hue='smoker',aspect=2,height=6)\nax.set_title('Scatter plot of Charges vs bmi')","4fe36629":"# Dummy variable\ncategorical_columns = ['sex','smoker','region']\ndf = pd.get_dummies(data = df,\n               columns = categorical_columns,\n                    drop_first = True\n               )\ndf.info()","fd1f1647":"df.head()","bdb96715":"from sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\ndf[['age','bmi']]=scaler.fit_transform(df[['age','bmi']])\n","82916c4f":"df.head()","47e08b00":"X=df.drop(columns=['expenses'],axis=1) # collecting only the features.","39375976":"X","10668856":"y=df['expenses'] # collecting Label values","5f3624a4":"y","1e363d4d":"from scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df['expenses'],alpha=0.05)\nf= plt.figure(figsize=(12,4))\n\nax=f.add_subplot(121)\nsns.distplot((df['expenses']),bins=40,color='r',ax=ax)\n#ax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.distplot(y_bc,bins=40,color='b',ax=ax)\n#ax.set_title('Distribution of insurance charges in $log$ sacle')\n#ax.set_xscale('log');","bd837e94":"# QQ Plot\nimport pylab \nimport scipy.stats as stats\n\n# q-q plot\nstats.probplot(y_bc, dist=\"norm\", plot=pylab)\n","1787791d":"# QQ Plot\nimport pylab \nimport scipy.stats as stats\n\n# q-q plot\nstats.probplot(y, dist=\"norm\", plot=pylab)","2954c728":"# Shapiro-Wilk Test\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom scipy.stats import shapiro\n\n# normality test\nstat, p = shapiro(y_bc)\nprint('Statistics=%.3f, p=%.10f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","add61116":"# D'Agostino and Pearson's Test\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom scipy.stats import normaltest\n\nstat, p = normaltest(y_bc)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","8542ab23":"y_log=np.log(y)","97f6781c":"from scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df['expenses'],alpha=0.05)\nf= plt.figure(figsize=(12,4))\n\nax=f.add_subplot(121)\nsns.distplot((df['expenses']),bins=40,color='r',ax=ax)\n#ax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.distplot(y_log,bins=40,color='b',ax=ax)\n#ax.set_title('Distribution of insurance charges in $log$ sacle')\n#ax.set_xscale('log');","039fc577":"# Shapiro-Wilk Test\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom scipy.stats import shapiro\n\n# normality test\nstat, p = shapiro(y_log)\nprint('Statistics=%.3f, p=%.10f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","39392132":"# D'Agostino and Pearson's Test\nfrom numpy.random import seed\nfrom numpy.random import randn\nfrom scipy.stats import normaltest\n\nstat, p = normaltest(y_log)\nprint('Statistics=%.3f, p=%.3f' % (stat, p))\n# interpret\nalpha = 0.05\nif p > alpha:\n\tprint('Sample looks Gaussian (fail to reject H0)')\nelse:\n\tprint('Sample does not look Gaussian (reject H0)')","b758d696":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y_log,test_size=0.6,random_state=23)","3c3ec648":"# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\n\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train)\n\ny_pred_LR = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk_LR = mean_squared_error(y_pred_LR, y_test)\n\n# R_square\nR_square_sk_LR = lin_reg.score(X_test,y_test)\n\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk_LR)\nprint('R square obtain for scikit learn library is :',R_square_sk_LR)\n","45edc4d7":"# Scikit Learn module\nfrom sklearn.linear_model import Ridge\n\n\nlin_reg_ridge = Ridge()\nlin_reg_ridge.fit(X_train,y_train)\n\ny_pred_LR_ridge = lin_reg_ridge.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk_LR_ridge = mean_squared_error(y_pred_LR_ridge, y_test)\n\n# R_square\nR_square_sk_LR_ridge = lin_reg_ridge.score(X_test,y_test)\n\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk_LR_ridge)\nprint('R square obtain for scikit learn library is :',R_square_sk_LR_ridge)\n","19bbd9a3":"# Scikit Learn module\nfrom sklearn.linear_model import Lasso\n\n\nlin_reg_lasso = Lasso()\nlin_reg_lasso.fit(X_train,y_train)\n\ny_pred_LR_lasso = lin_reg_lasso.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk_lasso = mean_squared_error(y_pred_LR_lasso, y_test)\n\n# R_square\nR_square_sk_lasso = lin_reg_lasso.score(X_test,y_test)\n\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk_lasso)\nprint('R square obtain for scikit learn library is :',R_square_sk_lasso)\n","f4992f06":"# Support Vector Machine's \nfrom sklearn.svm import SVR\n\nSVM_R = SVR(kernel='rbf')\nSVM_R.fit(X_train, y_train)\n\ny_pred_svm = SVM_R.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk_svm = mean_squared_error(y_pred_svm, y_test)\n\n# R_square\nR_square_sk_svm = SVM_R.score(X_test,y_test)\n\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk_svm)\nprint('R square obtain for scikit learn library is :',R_square_sk_svm)","944ef0f7":"MSE = [J_mse_sk_LR,J_mse_sk_LR_ridge,J_mse_sk_lasso,J_mse_sk_svm]\nR_square = [R_square_sk_LR, R_square_sk_LR_ridge,R_square_sk_lasso,R_square_sk_svm]\nmodels = ['Linear Regression', 'Linear Regression (Ridge)' , 'Linear Regression (Lasso)', 'Support Vector Machine']","6c0db1a6":"sns.barplot(MSE, models, color=\"g\")\n#plt.xlim([70,100])\nplt.xlabel('Mean Square Error')\nplt.title('Mean Square Error')\nplt.show()","12b50f25":"sns.barplot(R_square, models, color=\"b\")\nplt.xlim([-0.1,1.0])\nplt.xlabel('R-Square')\nplt.title('R-Square')\nplt.show()","73331d21":"We reject the data as our hypothesis is failing. Since none of the test is passed. we will not use the results of box-cox transformation. We will just use the log transformation of expense column. ","a0db5844":"Lets import the required libraries, if any other libraries are required we will import  it in the required cell. ","b15379a2":"Still the log data of 'expense' column is failing. We will go with it as we have no other option. ","36b31089":"As can be seen above, the continous columns of Age and Bmi are compared with expenses and it can be found that the smoker expenses are higher in comparison to non smokers. \n\nAs this data can be classified easily, it can be predicted that modeling this type of data with **Support Vector Machine Algorithm** will provide better results. Lets check whether our prediction works well or not. ","47f0e1ff":"It is found that the MSE with the Support Vector Machine (SVM) model is lowest of all the tried models. and the R-Square value is highest among all. \n\nThus SVM seems to be the best for this data.","806e9722":"Before we dig in further, lets check if we can transform the label data into normal distribution using various methods. \n\nOne of the methods used is BoxCox Transformation.\n* A Box Cox transformation is a transformation of non-normal dependent variables into a normal shape. Normality is an important assumption for many statistical techniques; if your data isn\u2019t normal, applying a Box-Cox means that you are able to run a broader number of tests.\n\nAt the core of the Box Cox transformation is an exponent, lambda (\u03bb), which varies from -5 to 5. All values of \u03bb are considered and the optimal value for your data is selected; The \u201coptimal value\u201d is the one which results in the best approximation of a normal distribution curve. The transformation of Y has the form:\n\n$y( \\lambda ) = \\begin{cases} \\frac{ y^{\\lambda}-1 }{\\lambda}  & \\lambda  \\neq  0\\\\log(y) & \\lambda = 0\\end{cases}  $\n\nThis test only works for positive data. However, Box and Cox did propose a second formula that can be used for negative y-values:\n\n$y( \\lambda ) = \\begin{cases} \\frac{ (y+\\lambda_{2})^{\\lambda_{1} }-1 }{\\lambda_{1}}  & \\lambda_{1}  \\neq  0\\\\log(y+\\lambda_{2}) & \\lambda_{1} = 0\\end{cases} $","8858c09d":"In the above plot if the data is on the line then the data is said to be normally distributed. Majority of the data is found to be on the red line. ","09d899cc":"It can be observed that the R square value using the Lasso Regression is negative, model prediction is not following the trend of the data. We will now perform the Support Vector Machine model on our data.","f9b47727":"# Support Vector Machine","bde16c4d":"We transformed the categorical variables with **One Hot Encoding** and removing the first columns to avoid the dummy trap. ","2eaf28d7":"It can be found that the smokers tend to have higher expenses in comparison to others categorical variables. Not much difference is found between male and female.","3060b124":"It is found that the data after boxcox transformation seems to be normally distributed. Before we got with these values, we need to check few tests and plots.","b4fd9e0e":"There seems to be good relation between age and bmi with expenses.","c58ceb70":"We reject the data as our hypothesis is failing.","c8d246d9":"Lets repeat the same test on our original Expenses data. Which we know that is not normally distributed and data should be away from the redline. \n\nThe same is observed in the figure below.","74926c82":"# Lasso Linear Regression","744351dd":"Median for all the people with any number of children remain the same. ","7d2d7b6f":"# Shapiro Wilk Test","5a14f9d8":"# Linear Regression","60a58f38":"As can be seen above, the continous columns of Age and Bmi are compared with expenses and it can be found that the smoker expenses are higher in comparison to non smokers. \n\nAs this data can be classified easily, it can be predicted that modeling this type of data with **Support Vector Machine Algorithm** will provide better results. Lets check whether our prediction works well or not. ","8bde53fd":"Lets transform the continous data (numerical data) onto the same scale of (0,1).","6870e2d6":"Splitting the required data into training and test dataset.","3a8888e7":"# D'Agostino and Pearson's Test","9802f281":"# Ridge Linear Regression ","7bc16cb3":"The log data distribution of expenses data seems to have normal distribution. "}}