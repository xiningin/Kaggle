{"cell_type":{"627b5cb6":"code","34301327":"code","e751d0f9":"code","e0859d4a":"code","0cfe8d7f":"code","f91c4d2d":"code","00eca963":"code","db51f861":"code","882c61b6":"code","c0998bf8":"markdown","7dd47543":"markdown","0d911d00":"markdown","c85a9e9a":"markdown","26cda52f":"markdown","737e2bbe":"markdown"},"source":{"627b5cb6":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datatable as dt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n","34301327":"from sklearn.preprocessing import StandardScaler\n\ndef create_row_features(df):\n    df['row_sum'] = df.sum(axis=1)\n    df['row_mean'] = df.mean(axis=1)\n    df['row_std'] = df.std(axis=1)\n    \n    return df\n\n\n\ntrain1 = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\n\ntrain=create_row_features(train1)\ntest=create_row_features(test)\n\nX = train.drop(columns=['id','target'])\ny = train['target']\n\ntest = test.drop('id',axis=1)\n\nscaler = StandardScaler()\n\nX=scaler.fit_transform(X)\nX_test=scaler.transform(test)\n","e751d0f9":"#train=np.array(X)\ntrain = X.reshape(-1, 60, X.shape[-1])\nX_test=X_test.reshape(-1, 60, X_test.shape[-1])\ny=y.to_numpy().reshape(-1,60)\ntrain.shape,X_test.shape","e0859d4a":"from tensorflow.keras import Sequential\nfrom tensorflow.keras import layers\n\n\n\ndef Lstm_model():\n    model = Sequential([\n                layers.Input(shape = train.shape[-2:]),\n                layers.Bidirectional(layers.LSTM(98, return_sequences=True)),\n                layers.Dense(156, activation='selu'),\n                layers.Bidirectional(layers.LSTM(46, return_sequences=True)),\n                layers.Dense(64, activation='selu'),\n                layers.Dense(1,activation='sigmoid')])\n    \n    return model","0cfe8d7f":"from sklearn.model_selection import KFold\nfrom tensorflow.keras import optimizers\n\n\ncv = KFold(n_splits=5, shuffle=True, random_state=1)\ni =0\npredictions = []\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(train, y)):\n    X_train, y_train = train[idx_train], y[idx_train]\n    X_valid, y_valid = train[idx_valid], y[idx_valid]\n    \n    print(X_train.shape,y_train.shape)\n    model = Lstm_model()\n    \n    model.compile(\n        optimizer=optimizers.Adam(learning_rate=1e-5),\n        loss='binary_crossentropy',\n        metrics=['AUC',\n                  tf.keras.metrics.BinaryAccuracy(),\n                  tf.keras.metrics.FalseNegatives()])\n\n    \n    print(f\"training for Fold {i+1}\")\n    \n    LSTM = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        batch_size=512,\n        epochs=70)\n                 \n                 \n    prediction = model.predict(X_test)\n    predictions.append(prediction)","f91c4d2d":"predict=[]\nfor l in range(5):\n    flat_ls = [item for sublist in predictions[l] for item in sublist]\n    predict.append(flat_ls)   \n\nsub=np.zeros(540000)\nprint(sub.shape)\nfor pred in predict:\n    pred=np.array(pred).reshape(540000)\n    sub+=pred\n\n    \nsub=sub\/5\n\nsubmission['target'] = sub\nsubmission.to_csv('lstm.csv', index=False)","00eca963":"from lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score","db51f861":"test = test.drop('id',axis=1)","882c61b6":"X = train1.drop(columns=['id','target'])\ny = train1['target']\n\nskf=StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\npred=[]\n\n\nfor fold,(idx1,idx2) in enumerate(skf.split(X,y)):\n    \n    Xtrain,ytrain=X.iloc[idx1],y.iloc[idx1]\n    Xvalid,yvalid=X.iloc[idx2],y.iloc[idx2]\n    \n    model=LGBMClassifier(n_estimators=1500,learning_rate=0.02,device='gpu',num_leaves=900,\n                         min_child_samples=60)\n    \n    model.fit(Xtrain,ytrain,eval_set=[(Xvalid,yvalid)],early_stopping_rounds=100,verbose=50)\n    \n    valid_pred = model.predict_proba(Xvalid)[:,1]\n    test_pred = model.predict_proba(test)[:,1]\n    \n    pred.append(test_pred)\n    \n    score = roc_auc_score(yvalid, valid_pred)\n    print(score)\n    ","c0998bf8":"# LGBM","7dd47543":"## <span style=\"background:red;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Work In progress<\/span>\n\n* **This is the summary of Learning through compitition Google ventilator pressure prediction**\n\n* **LSTM and RNN are not good for tabular data, this are very different flavour Neural network, although they outperforms LGBM,XGBOOST and feed forward NN in time series data**\n\n* **LGBM model** [Google brain GCVPP](https:\/\/www.kaggle.com\/shivansh002\/lgbm-lover-s)\n\n* **LSTM model** [Google brain GCVPP](https:\/\/www.kaggle.com\/shivansh002\/i-am-groot-tpu-war)\n\n","0d911d00":"**LSTM expects inputs to be 3 Dimensional**","c85a9e9a":"## <span style=\"background:RED;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">LSTM <\/span>\n\n* **RNN have their own problems like vanishing gradients and exploding gradient**\n* **here is monstrous looking LSTM architecture but i'll try to explain in more philosophical way ;)**\n\n\n![lstm](https:\/\/miro.medium.com\/max\/700\/1*qToyitOZkf7Nhvr1LwxWgQ.png)\n\n****\n# part1\n* **Forget gate**\n* **identifies only the relevent information from previous state**\n* **previous state: \"shyam likes ice-cream but raju like bread\"**\n* **current state: \"raju will meet alia in evening\"**\n* **in LSTM cell the info about raju is relevent because its being used in current state while the info of shyam is irrelevent.** \n* **the input from previous state is compared with current state, so that may be past data is significant for future data**\n\n# part2\n* **input gate**\n* **next part decide what new information to store in the cell state how much of past information should be added to current state**\n\n# part3\n* **output gate**\n* **Finally, we need to decide what we\u2019re going to output. This output will be based on our cell state, but will be a filtered version**","26cda52f":"## <span style=\"background:red;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Recurrent Neural Networks<\/span>\n\n* **In Our beloved feed forward neural network information only flows in single direction**\n* **traditional neural network is not good for sequential data**\n* **data like time series and text.**\n* **decision at a node(single neural cell) is made only on current input, no data of past and no future scope**\n\n****\n![fnn](https:\/\/th.bing.com\/th\/id\/OIP.W_Ff5HIQ15akx48m1PAe7AAAAA?pid=ImgDet&rs=1)\n\n****\n\n* **A recurrent neural network is a traditional neural network except that a memory-state is added to the neurons.**\n* **the output from node in not only sent to next layer and also to itself**\n\n\n![df](https:\/\/cdn.guru99.com\/images\/tensorflow\/082918_1006_RNNRecurren1.png)\n\n****\n\n* **this gif makes concept clear the input \"NYU NLP Rocks !\" its for single RNN cell**\n\n\n![rnn](https:\/\/i.ibb.co\/LdL7CM5\/rnn-move-rnn-cell.gif)\n","737e2bbe":"## <span style=\"background:Red;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Data preparation<\/span>"}}