{"cell_type":{"9dce51a4":"code","38bebbbc":"code","dbedb772":"code","69002298":"code","70558481":"code","542e1741":"code","e6f9a253":"code","a852f8ff":"code","780777b2":"code","ba30116b":"code","afec6515":"code","dcf98327":"code","bf1049c6":"code","9d3f8df4":"markdown","0703f943":"markdown","b0976729":"markdown","35419b35":"markdown"},"source":{"9dce51a4":"import torch\nimport torch.nn as nn\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt","38bebbbc":"# Define torch device\ntorch_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","dbedb772":"#\n# Functions for easy training\n# 'train' function is based on trainer object\n# trainer object need implement a run_batch method\n#\n# def run_batch(self, batch, training=True):\n# # batch is a dictionary with data for training\n# # Run training if training=True, else calcule criterions only\n# # Return criterions dictionary, including loss\n#     return {\n#         'criterion_a': <a criterion>\n#         'loss': <loss criterion>\n#     }\n#\n\ndef run_minibatches(trainer, batch, minibatch_size, training=True):\n    data_size = batch[list(batch.keys())[0]].size(0)\n    minibatch_N = data_size \/\/ minibatch_size\n    minibatch_start = 0\n    minibatch = {}\n    criterions_sum = {}\n    while minibatch_start + minibatch_size <= data_size:\n        # Select minibatch and run trainer\n        for key in batch:\n            minibatch[key] = batch[key][minibatch_start:minibatch_start+minibatch_size]\n        criterions = trainer.run_batch(minibatch, training=training)\n        # Update sum of criterions\n        for c in criterions:\n            criterions_sum[c] = criterions[c] + (criterions_sum[c] if c in criterions_sum else 0)\n        # Increment minibatch start\n        minibatch_start += minibatch_size\n    # Return mean of criterions\n    return {c : criterions_sum[c]\/minibatch_N for c in criterions_sum}\n\ndef run_round_and_save_history(i, trainer, loader, history, minibatch_size, training=True):\n    batch = loader if type(loader) is dict else  loader(i)\n    criterions = run_minibatches(trainer, batch, minibatch_size, training)\n    for c in criterions:\n        if c in history:\n            history[c].append(criterions[c])\n        else:\n            history[c] = [criterions[c]]\n    return criterions\n\n# Run complete training\ndef train(trainer, max_rounds, minibatch_size, train_data_loader, valid_data_loader, \n          loss_criterion='loss', stop_function=None, selection='last', vervose=False, temp_path='\/tmp\/'):\n    random_int = random.randint(0, 1000000)              # Random int for temporal saving\n    # Verify selection choice\n    if selection not in ['last', 'min-valid-loss']:\n        raise Exception(\"Invalid Selection\")\n    # Init values \n    train_criterions_history = {}\n    valid_criterions_history = {}\n    min_valid_loss = np.inf\n    for i in range(max_rounds):\n        # Training\n        run_round_and_save_history(i, trainer, train_data_loader, \n                                   train_criterions_history, minibatch_size, training=True)\n        # Validation\n        if valid_data_loader is not None:\n            run_round_and_save_history(i, trainer, valid_data_loader, \n                                       valid_criterions_history, minibatch_size, training=False)\n            valid_loss = valid_criterions_history[loss_criterion][-1]\n        # Save best model if is necessary\n        if selection == 'min-valid-loss' and valid_loss < min_valid_loss:\n            min_valid_loss = valid_loss\n            torch.save(trainer.model, temp_path + str(random_int) + '.pt')          \n        # Check stop function if it was provided\n        if (stop_function is not None) and (stop_function(train_criterions_history, valid_criterions_history)):\n            break\n    if selection == 'min-valid-loss':\n        best_model = torch.load(temp_path + str(random_int) + '.pt')\n    elif selection == 'last':\n        best_model = trainer.model    \n    return best_model, train_criterions_history, valid_criterions_history\n\n","69002298":"# Univariate normal distribution model definition\n# out:     mean,    std\n# ranges: [-1, 1],  [0, 2]\nclass UnivariateNormalDistribution(nn.Module):\n    \n    def __init__(self):\n        nn.Module.__init__(self)\n        self.l1 = nn.Linear(1, 100, bias=True)\n        self.a1 = nn.ReLU()\n        self.l2 = nn.Linear(100, 50)\n        self.a2 = nn.ReLU()\n        self.l3 = nn.Linear(50, 2)\n        self.a3 = nn.Tanh()\n    \n    def forward(self):\n        x = torch.ones(size=(1, 1)).to(self.l1.weight.device)\n        o1 = self.a1(self.l1(x))\n        o2 = self.a2(self.l2(o1))\n        o =  self.a3(self.l3(o2))\n        return o[0,0], o[0,1] + 1.0\n","70558481":"# Univariate normal distribution trainer definition\nclass UnivariateNormalDistributionTrainer():\n    \n    def __init__(self, model):\n        self.model = model\n        # Define optimizer\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    # Define normal pdf for calculate log-likelihood\n    def normal_pdf(self, x, mean, std):\n         return torch.exp(-0.5*((x-mean)\/std)**2)\/(std*np.sqrt(2*np.pi))\n    \n    # run_batch implementation\n    def run_batch(self, batch, training=True):\n        x = batch['X']                      # Extract data from dictionary\n        mean, std = self.model()            # get distribution from model\n        p = self.normal_pdf(x, mean, std)   # Calculate individual probability\n        log_likelihood = torch.log(p).sum() # Calculate log_likelihood\n        loss = -log_likelihood              # Define loss to minimice\n        # Define loss criterion and others criterions to register history\n        criterions = {\n            'log-likelihood': log_likelihood,\n            'loss': loss\n        }\n        # Run training if training is True\n        if training:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n        return criterions\n        ","542e1741":"# Define 'Real' distribution, limit to model ranges\nreal_mean = -0.5\nreal_std = 1.2\n# Generate samples\nX = torch.normal(real_mean, real_std, size=(10000, 1)).to(torch_device) # 1000 sambles\ndata = {'X': X} # Define data dictionary\n# define model\nmodel = UnivariateNormalDistribution().to(torch_device)\ntrainer = UnivariateNormalDistributionTrainer(model)\n\n# Start train\ntrained_model, train_history, _ = train(\n    trainer, \n    max_rounds = 20, \n    minibatch_size = 128, \n    train_data_loader = data, \n    valid_data_loader = None\n)\n\ntrained_mean, trained_std = trained_model()\ntrained_mean = trained_mean.data.item() # Extract value\ntrained_std = trained_std.data.item() # Extract value\n\nprint('real_mean:\\t', real_mean)\nprint('trained_mean:\\t', trained_mean)\nprint('real_std:\\t', real_std)\nprint('trained_std:\\t', trained_std)\n\nplt.figure(figsize=[8, 4])\nplt.plot(train_history['log-likelihood'])\nplt.xlabel('Epoch')\nplt.ylabel('log-likelihood')\nplt.show()","e6f9a253":"# Conditional univariate normal distribution model definition\n# out:     mean,    std\n# ranges: [-1, 1],  [0, 2]\nclass ConditionalUnivariateNormalDistribution(nn.Module):\n    \n    def __init__(self):\n        nn.Module.__init__(self)\n        self.l1 = nn.Linear(4, 100, bias=False)\n        self.a1 = nn.ReLU()\n        self.l2 = nn.Linear(100, 50, bias=False)\n        self.a2 = nn.ReLU()\n        self.l3 = nn.Linear(50, 2, bias=False)\n        self.a3 = nn.Tanh()\n    \n    def forward(self, x):\n        o1 = self.a1(self.l1(x))\n        o2 = self.a2(self.l2(o1))\n        o =  self.a3(self.l3(o2))\n        return o[:,0].view(-1,1), o[:,1].view(-1,1) + 1.0","a852f8ff":"# Conditional Univariate normal distribution trainer definition\nclass ConditionalUnivariateNormalDistributionTrainer():\n    \n    def __init__(self, model):\n        self.model = model\n        # Define optimizer\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    # Define normal pdf for calculate log-likelihood\n    def normal_pdf(self, x, mean, std):\n         return torch.exp(-0.5*((x-mean)\/std)**2)\/(std*np.sqrt(2*np.pi))\n    \n    # run_batch implementation\n    def run_batch(self, batch, training=True):\n        x = batch['X']                      # Extract data from dictionary\n        y = batch['Y']                      # Extract data from dictionary\n        mean, std = self.model(x)           # get distributions from model\n        p = self.normal_pdf(y, mean, std)   # Calculate individual probability\n        log_likelihood = torch.log(p).sum() # Calculate log_likelihood\n        loss = -log_likelihood              # Define loss to minimice\n        # Define loss criterion and others criterions to register history\n        criterions = {\n            'log-likelihood': log_likelihood,\n            'loss': loss\n        }\n        # Run training if training is True\n        if training:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n        return criterions\n        ","780777b2":"# Define 'Real' distribution, limit to model ranges\ninit_x = torch.rand(size=[5, 4])\ninit_mean = (-0.7 + init_x[:,0]*init_x[:,1]).view(-1,1)\ninit_std  = (0.5 + 1.5*(init_x[:,2] + init_x[:,3])\/2.0).view(-1,1)\n\nN_samples_per_dist = 4000\n\n# Generate samples\nX =       init_x.repeat([N_samples_per_dist, 1])\nmean = init_mean.repeat([N_samples_per_dist, 1])\nstd =   init_std.repeat([N_samples_per_dist, 1])\nY = torch.normal(mean, std)\n\n# Shuffle Data\nr_index=torch.randperm(X.size(0))\nX =       X[r_index]\nmean = mean[r_index]\nstd =   std[r_index]\nY =       Y[r_index]\n\ndata = {'X': X, 'Y': Y} # Define data dictionary","ba30116b":"# define model\nmodel = ConditionalUnivariateNormalDistribution().to(torch_device)\ntrainer = ConditionalUnivariateNormalDistributionTrainer(model)\n\n# Start train\ntrained_model, train_history, _ = train(\n    trainer, \n    max_rounds = 40, \n    minibatch_size = 128, \n    train_data_loader = data, \n    valid_data_loader = None\n)\n\ntrained_mean, trained_std = trained_model(init_x)\n\nprint(\"mean:      Real vs Trained\")\nprint(torch.cat([init_mean, trained_mean], axis=1) )\n\nprint(\"\\nstd:      Real vs Trained\")\nprint(torch.cat([init_std, trained_std], axis=1) )\n\nplt.figure(figsize=[8, 4])\nplt.plot(train_history['log-likelihood'])\nplt.xlabel('Epoch')\nplt.ylabel('log-likelihood')\nplt.show()","afec6515":"# Discrete distribution model definition\n# out:     N_states discrete Distribution\n# ranges:  0 -> 1\nclass DiscreteDistribution(nn.Module):\n    \n    def __init__(self, N_states):\n        nn.Module.__init__(self)\n        self.l1 = nn.Linear(1, 100, bias=False)\n        self.a1 = nn.ReLU()\n        self.l2 = nn.Linear(100, 100, bias=False)\n        self.a2 = nn.ReLU()\n        self.l3 = nn.Linear(100, N_states, bias=False)\n        self.a3 = nn.Softmax(1)\n    \n    def forward(self):\n        x = torch.ones(size=(1, 1)).to(self.l1.weight.device)\n        o1 = self.a1(self.l1(x))\n        o2 = self.a2(self.l2(o1))\n        o =  self.a3(self.l3(o2))\n        return o[0, :]\n    \n","dcf98327":"# Discrete distribution trainer definition\nclass DiscreteDistributionTrainer():\n    \n    def __init__(self, model):\n        self.model = model\n        # Define optimizer\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n    \n    # run_batch implementation\n    def run_batch(self, batch, training=True):\n        x = batch['X']                      # Extract data from dictionary\n        discrete_dist = self.model()        # get distribution from model\n        p = discrete_dist[x]                # Calculate individual probability\n        log_likelihood = torch.log(p).sum() # Calculate log_likelihood\n        loss = -log_likelihood              # Define loss to minimice\n        # Define loss criterion and others criterions to register history\n        criterions = {\n            'log-likelihood': log_likelihood,\n            'loss': loss\n        }\n        # Run training if training is True\n        if training:\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n        return criterions\n","bf1049c6":"# Define 'Real' distribution, limit to model ranges\nreal_P = [0.1, 0.5, 0.2, 0.2]\n\n# Generate samples\nN_samples = 10000\nX_np = np.random.choice(range(len(real_P)), N_samples, p = real_P)\nX = torch.Tensor(list(X_np)).type(torch.long).view(-1,1).to(torch_device)\n\ndata = {'X': X} # Define data dictionary\n\n# define model\nmodel = DiscreteDistribution(4).to(torch_device)\ntrainer = DiscreteDistributionTrainer(model)\n\n# Start train\ntrained_model, train_history, _ = train(\n    trainer, \n    max_rounds = 40, \n    minibatch_size = 128, \n    train_data_loader = data, \n    valid_data_loader = None\n)\n\ntrained_P = trained_model()\ntrained_P = list(trained_P.cpu().detach().numpy())\n\nprint('real_P:\\t\\t',  \"[{:.3f} {:.3f} {:.3f} {:.3f}]\".format(*real_P))\nprint('trained_P:\\t', \"[{:.3f} {:.3f} {:.3f} {:.3f}]\".format(*trained_P))\n\nplt.figure(figsize=[8, 4])\nplt.plot(train_history['log-likelihood'])\nplt.xlabel('Epoch')\nplt.ylabel('log-likelihood')\nplt.show()","9d3f8df4":"## Test 1: Training marginal univariate normal distribution","0703f943":"## Training utils functions","b0976729":"## Test 2: Training conditional univariate normal distribution","35419b35":"## Test 3: Training Discrete distribution"}}