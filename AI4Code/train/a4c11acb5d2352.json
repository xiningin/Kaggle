{"cell_type":{"e05e19d9":"code","7ffa0c04":"code","e011939f":"code","5b753036":"code","978192f9":"code","70bf5559":"code","921dba0c":"code","c413be0a":"code","bc90e734":"code","89b36c47":"code","48274469":"code","721ae472":"code","2f487d6c":"code","cee27a9f":"code","0c847551":"code","bfcb784b":"code","5da4f136":"code","380080b2":"code","fc975e91":"code","c958dd21":"code","a5cd90ed":"code","0279ab91":"code","4cd76692":"code","7bf643ce":"code","9f5f2a01":"code","cb6ef098":"code","d10eb187":"code","ac14b98d":"code","352bac9e":"code","5de5ba3d":"code","314db8bf":"code","934d9423":"code","301e7c88":"code","b9f18291":"code","c4d71c8d":"code","5625e9d4":"code","e6f8ccb4":"code","5f94de1f":"code","5af1452b":"code","966efa94":"code","a5d89fe1":"code","5edee295":"markdown","f6996b50":"markdown","b0e03c01":"markdown","7369e4c3":"markdown","a0ba01bb":"markdown","0db3a709":"markdown","26fe03f6":"markdown","a5e90954":"markdown","8f530f85":"markdown","79f041c2":"markdown","17cc07d4":"markdown","4f38cc33":"markdown","56f7fab1":"markdown","b6bee436":"markdown","d2b02133":"markdown","0858518f":"markdown","13350155":"markdown"},"source":{"e05e19d9":"# Import libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.compose import TransformedTargetRegressor\nfrom sklearn.neighbors import KernelDensity","7ffa0c04":"import warnings\nimport matplotlib.cbook\nwarnings.filterwarnings(\"ignore\",category=Warning)","e011939f":"# Import dataset\ndf = pd.read_csv(\"\/kaggle\/input\/automobile-insurance\/insurance_claims.csv\")","5b753036":"# Shape of original dataset\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","978192f9":"df.head()","70bf5559":"# Dropping unnecessary and irrelevant features, including target associated features (Property\/Injury\/Vehicle Claims) and empty columns.\ndf = df.drop(columns=[\n                      'policy_number', '_c39', 'property_claim', 'injury_claim', \n                      'vehicle_claim', 'policy_bind_date','insured_zip','incident_date',\n                      'incident_location'\n                      ])","921dba0c":"# Shape of current dataset\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","c413be0a":"df.info()","bc90e734":"# Correlation Matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for Dataset', fontsize=15)\n    plt.show()","89b36c47":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, y, nGraphShown, nGraphPerRow, typ):\n    ds = pd.concat([df, y], axis=1)\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        columnName = df.columns[i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            if typ == 'hist':\n              sns.histplot(x=columnName, data=ds)\n            if typ == 'violin':\n              sns.violinplot(x=columnName, y='total_claim_amount', data=ds)\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1, w_pad = 1, h_pad = 1)\n    plt.show()","48274469":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()","721ae472":"# Density Plot and Histogram\ndef plotHistogram(data):\n  sns.distplot(data, hist=True, kde=True, \n              bins=100, color = 'darkblue', \n              hist_kws={'edgecolor':'black'},\n              kde_kws={'linewidth': 4}, )","2f487d6c":"numericals = df.select_dtypes(include=np.number).columns.tolist()\ncategoricals = df.select_dtypes(include='object').columns.tolist()\n# Sanity check\nprint(len(numericals)+len(categoricals))","cee27a9f":"# Data description\ndf[categoricals].describe(include=np.object)","0c847551":"# Replace '?' with NAs\ndf = df.replace('?','NA')","bfcb784b":"# Inspection of categorical features using Histogram\nplotPerColumnDistribution(df[categoricals], df['total_claim_amount'], len(df[categoricals]), 4, 'hist')","5da4f136":"# Inspection of categorical features with respect to target variable using Violin Plot\nplotPerColumnDistribution(df[categoricals], df['total_claim_amount'], len(df[categoricals]), 4, 'violin')","380080b2":"# Handling 'policy_csl' feature\ncsl = {\n    '100\/300':1, \n    '250\/500':2, \n    '500\/1000':3\n    }\ndf['policy_csl'] = df['policy_csl'].replace(csl)","fc975e91":"# Handling 'Education Level' - Replacing education degrees with average cummulative years of study per degree\nedu = {\n       'JD': 21,\n       'High School': 12,\n       'Associate': 14,\n       'MD': 21,\n       'Masters': 18,\n       'PhD': 21,\n       'College': 16,\n       }\ndf['insured_education_level'] = df['insured_education_level'].replace(edu)\ndf['insured_education_level'].value_counts()","c958dd21":"# 'Incident Type' - Merging 'Parked Car' w\/ 'Vehicle Theft' and 'Single Collision' w\/ 'Multi Collision'\ndf['incident_type'] = df['incident_type'].replace(['Parked Car','Vehicle Theft'], 1)\ndf['incident_type'] = df['incident_type'].replace(['Single Vehicle Collision','Multi-vehicle Collision'], 2)\ndf['incident_type'].value_counts()","a5cd90ed":"# Replacing NAs with 'Unknown'\ndf = df.replace('NAN', 'Unknown')\ndf.dtypes","0279ab91":"# Correlation matrix\nplotCorrelationMatrix(df, 7)","4cd76692":"# Scatter matrix\nplotScatterMatrix(df, 20, 10)","7bf643ce":"plotHistogram(df['total_claim_amount'])","9f5f2a01":"plotHistogram(np.log(df['total_claim_amount']))","cb6ef098":"df.loc[df['total_claim_amount'] < np.exp(9.55), 'claim_level'] = 1\ndf.loc[df['total_claim_amount'] >= np.exp(9.55), 'claim_level'] = 2","d10eb187":"# General Hyperparameters\nRATIO = 0.75 # Train-Test split ratio\nSEED = 321 # Random seed","ac14b98d":"# Dividing the data into features and label\ndf = pd.get_dummies(df)\nX = df.drop(['claim_level','total_claim_amount'], axis=1)\nY = df['claim_level']\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size = RATIO,\n                                                    random_state = SEED, stratify=df['claim_level'])\n","352bac9e":"# Auxiliary Function - General Classifier\ndef ClassificationModelPredict(classifier, title):\n    model = classifier.fit(x_train, y_train)\n    y_hat = model.predict(x_test)\n    claim_level_predictions.loc[title, 'Accuracy Score']  = accuracy_score(y_test, y_hat)","5de5ba3d":"claim_level_predictions = pd.DataFrame()\n\n# Train classifier and predict\nClassificationModelPredict(LogisticRegression(), 'Logistic Regression')\nClassificationModelPredict(DecisionTreeClassifier(), 'Decision Tree')\n\n# Show results\nprint(claim_level_predictions)","314db8bf":"LVC = df[df['claim_level'] == 1]\nHVC = df[df['claim_level'] == 2]","934d9423":"def RegressionModel(regressor, title, stage):\n  model = regressor.fit(x_train, y_train)\n  scores = cross_val_score(model, X, Y, scoring='neg_mean_absolute_percentage_error', cv=5)\n  regression_results.loc[title, stage] = np.abs(np.mean(scores))","301e7c88":"# General Hyperparameters\nRATIO = 0.7 # Train-Test split ratio\nSEED = 1234 # Random seed","b9f18291":"# Shape of current dataset\nnRow, nCol = HVC.shape\nprint(f'There are {nRow} rows and {nCol} columns')","c4d71c8d":"X = HVC.drop('total_claim_amount', axis=1).to_numpy()\nY = HVC['total_claim_amount'].to_numpy()\nx_train, x_test, y_train, y_test = train_test_split(X, Y)","5625e9d4":"regression_results = pd.DataFrame()\nstage = 'HVC'\nRegressionModel(LinearRegression(),'Linear Regression', stage)\nRegressionModel(DecisionTreeRegressor(), 'Decision Tree', stage)\nRegressionModel(RandomForestRegressor(), 'Random Forest', stage)\nRegressionModel(GradientBoostingRegressor(), 'Gradient Boost', stage)\nRegressionModel(AdaBoostRegressor(), 'Ada Boost', stage)","e6f8ccb4":"# Shape of current dataset\nnRow, nCol = LVC.shape\nprint(f'There are {nRow} rows and {nCol} columns')","5f94de1f":"X = LVC.drop('total_claim_amount', axis=1).to_numpy()\nY = LVC['total_claim_amount'].to_numpy()\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size = RATIO, random_state = SEED)","5af1452b":"stage = 'LVC'\nRegressionModel(LinearRegression(),'Linear Regression', stage)\nRegressionModel(DecisionTreeRegressor(), 'Decision Tree', stage)\nRegressionModel(RandomForestRegressor(), 'Random Forest', stage)\nRegressionModel(GradientBoostingRegressor(), 'Gradient Boost', stage)\nRegressionModel(AdaBoostRegressor(), 'Ada Boost', stage)\nprint(regression_results)","966efa94":"WES = (\n        regression_results['HVC'].iloc[np.argmin(regression_results['HVC'])]*HVC.shape[0]+\n        regression_results['LVC'].iloc[np.argmin(regression_results['LVC'])]*LVC.shape[0]\n        )\/df.shape[0]\nprint(\"Weighted Effective Score: \", WES)","a5d89fe1":"X = df.drop('total_claim_amount', axis=1).to_numpy()\nY = df['total_claim_amount'].to_numpy()\nx_train, x_test, y_train, y_test = train_test_split(X, Y, train_size = RATIO, random_state = SEED)\nstage = 'Benchmark'\nRegressionModel(LinearRegression(),'Linear Regression', stage)\nRegressionModel(DecisionTreeRegressor(), 'Decision Tree', stage)\nRegressionModel(RandomForestRegressor(), 'Random Forest', stage)\nRegressionModel(GradientBoostingRegressor(), 'Gradient Boost', stage)\nRegressionModel(AdaBoostRegressor(), 'Ada Boost', stage)\nprint(regression_results)\n\nprint(\"Best Score for Benchmark Model: \", regression_results['Benchmark'].iloc[np.argmin(regression_results['Benchmark'])])","5edee295":"## Feature Engineering of Numerical Features","f6996b50":"### LVC Modeling","b0e03c01":"Auxiliary Function:","7369e4c3":"Dividing to categorical and numerical features","a0ba01bb":"From the Histogram we can conclude that the claims can be easily devided into two groups - Low Value Claims (from 0 to ~```$```10,000 and High Value Claims (from ~```$```10,000 on). Before I'll make it to the main mission of predicting claim value, I'll try first to predict the value level of each claim. which is a classification problem.\n\nIn order to better identify the seperation point between two groups, I'll plot the same Histogram for the log transformation of the 'total_claim_amount' feature:","0db3a709":"From the histogram it can estimated that the seperation value is ~exp(9.55). Let's classify the data accordingly:","26fe03f6":"### Claim Value Prediction\n---\nAt this stage we'll execute serveral regression algorithms for each claim value subclass. Since both subclasses has relatively few instances, I'll take advatage of the K-folds method in order to assess the real score of each regressor. The main performance measurement will be the Mean Absolout Percentage Error. This is the ultimative performance measurement since our target variable varies across wide range of values (from 0 to ~$115K). ","a5e90954":"The results suggest that the best regressor for the HVC class is the Adaptive Boost algorithm while using log tansformation, and the best regressor for the LVC is the Decision Tree algorithm while using log transformation too.\n\nLet us calculate the effective weighted score for the whole model using the following formula:\n\n$$\nWeighted \\space Effective \\space Score = \\frac{HVC Score*n_{rows HVC} + LVC Score*n_{rows LVC}}{Total \\space n_{rows}} \n$$","8f530f85":"# **Claim Prediction for Automobile Insurance Firms Using Combined Hierarchical Models**\n\nAbstract: The following notebook based on available public data (published on Kaggle.com at Dec 2018) from automobile insurance company. This project aims to perform claim assessment while utilizing combined hierarchical classification and regression based models. This includes a highballing class predication for each claim as the first stage (classification model), and more specific claim estimtation model (regression model) for each stage. \n\nThe general process can be easily visualize with the following figure:\n\n![](https:\/\/i.ibb.co\/ZhzjWjG\/Screen-Shot-2021-05-04-at-16-43-05.png)\n\nThis model provides automobile insurance companies with more accurate predication with respect to the best benchmark model. The scope of this work as well as its results is limited to the information and knowledge embedded at the provided dataset from Kaggle.","79f041c2":"Auxiliary Functions for Data Visualization","17cc07d4":"## Data Preprocessing","4f38cc33":"## Target variable inspection","56f7fab1":"### Feature Engineering of Categorical Features","b6bee436":"## Claim Level Class Prediction\n\nLet us create the first stage's model that would be a clssifier which aims to predict the claim class. The performance measurement for this model will be the Accuracy Score measurement : \n\n$$\nAccuracy = \\frac{TP + TN}{TP + FP + FN + TN}\n$$","d2b02133":"Decision Tree outperforms perfrectly while trying to predict claim value class. Let's use this labels in order to allow prediction at the second stage. ","0858518f":"### Data Partitioning\n","13350155":"### HVC Modeling"}}