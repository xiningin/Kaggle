{"cell_type":{"234e8200":"code","0b2c9591":"code","6ab39bb8":"code","1cc78360":"code","f5664a53":"code","f69947d3":"code","e77ca1b9":"code","9ff9fb16":"code","d85e5f17":"code","4ee401ce":"code","dd9d61a0":"code","b0efc817":"code","f45702d5":"code","4dc90cd7":"code","ccb8aad2":"code","4715486d":"code","07d1c05f":"code","772073dc":"code","4c095968":"code","bc1a74f5":"code","74861696":"code","dd87af55":"code","c9634248":"code","e71b7033":"code","1a93f52a":"code","73bfb170":"code","440f033e":"code","5c6c38b8":"code","e86025d8":"code","aff0a8c9":"code","c460ce18":"code","0eeb9f1b":"code","0a76c979":"code","d4aa2e40":"code","675344b1":"code","7eccefac":"code","b3fb375f":"code","c3251794":"code","5198e0bd":"code","cb5d7565":"code","2406d913":"markdown","92f6f7e3":"markdown","da5c8a75":"markdown","9b3312e2":"markdown","4ba7844b":"markdown","52336dbe":"markdown","ec537d29":"markdown","41823db4":"markdown","b506230f":"markdown","effe1b30":"markdown","2871c282":"markdown","fdcf4c37":"markdown","ebf05762":"markdown","b7257254":"markdown","5803db2f":"markdown","ab263927":"markdown","2aea939f":"markdown","01a97114":"markdown","8c147c33":"markdown","ff5a0ebc":"markdown","76a1eaf0":"markdown","ae01de54":"markdown","d94d9d77":"markdown","b12c455c":"markdown","b4406eb7":"markdown","ee6c4937":"markdown","c082fff9":"markdown","3036288a":"markdown"},"source":{"234e8200":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tqdm import tqdm\nimport os\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\nfrom sklearn.metrics import classification_report,confusion_matrix\nimport ipywidgets as widgets\nimport io\nfrom PIL import Image\nfrom IPython.display import display,clear_output\nfrom warnings import filterwarnings\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0b2c9591":"colors_dark = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\ncolors_red = [\"#331313\", \"#582626\", '#9E1717', '#D35151', '#E9B4B4']\ncolors_green = ['#01411C','#4B6F44','#4F7942','#74C365','#D0F0C0']\n\nsns.palplot(colors_dark)\nsns.palplot(colors_green)\nsns.palplot(colors_red)","6ab39bb8":"labels = ['glioma_tumor','no_tumor','meningioma_tumor','pituitary_tumor']","1cc78360":"X_train = []\ny_train = []\nimage_size = 150\nfor i in labels:\n    folderPath = os.path.join('..\/input\/brain-tumor-classification-mri','Training',i)\n    for j in tqdm(os.listdir(folderPath)):\n        img = cv2.imread(os.path.join(folderPath,j))\n        img = cv2.resize(img,(image_size, image_size))\n        X_train.append(img)\n        y_train.append(i)\n        \nfor i in labels:\n    folderPath = os.path.join('..\/input\/brain-tumor-classification-mri','Testing',i)\n    for j in tqdm(os.listdir(folderPath)):\n        img = cv2.imread(os.path.join(folderPath,j))\n        img = cv2.resize(img,(image_size,image_size))\n        X_train.append(img)\n        y_train.append(i)\n        \nX_train = np.array(X_train)\ny_train = np.array(y_train)","f5664a53":"k=0\nfig, ax = plt.subplots(1,4,figsize=(20,20))\nfig.text(s='Sample Image From Each Label',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=0.62,x=0.4,alpha=0.8)\nfor i in labels:\n    j=0\n    while True :\n        if y_train[j]==i:\n            ax[k].imshow(X_train[j])\n            ax[k].set_title(y_train[j])\n            ax[k].axis('off')\n            k+=1\n            break\n        j+=1","f69947d3":"X_train, y_train = shuffle(X_train,y_train, random_state=101)","e77ca1b9":"X_train.shape","9ff9fb16":"X_train,X_test,y_train,y_test = train_test_split(X_train,y_train, test_size=0.1,random_state=101)","d85e5f17":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","4ee401ce":"sns.countplot(y_train)","dd9d61a0":"sns.countplot(y_test)","b0efc817":"y_train_new = []\nfor i in y_train:\n    y_train_new.append(labels.index(i))\ny_train = y_train_new\ny_train = tf.keras.utils.to_categorical(y_train)\n\n\ny_test_new = []\nfor i in y_test:\n    y_test_new.append(labels.index(i))\ny_test = y_test_new\ny_test = tf.keras.utils.to_categorical(y_test)","f45702d5":"effnet = EfficientNetB0(weights='imagenet',include_top=False,input_shape=(image_size,image_size,3))","4dc90cd7":"model = effnet.output\nmodel = tf.keras.layers.GlobalAveragePooling2D()(model)\nmodel = tf.keras.layers.Dropout(rate=0.5)(model)\nmodel = tf.keras.layers.Dense(4,activation='softmax')(model)\nmodel = tf.keras.models.Model(inputs=effnet.input, outputs = model)","ccb8aad2":"model.summary()","4715486d":"model.compile(loss='categorical_crossentropy',optimizer = 'Adam', metrics= ['accuracy'])","07d1c05f":"tensorboard = TensorBoard(log_dir = 'logs')\ncheckpoint = ModelCheckpoint(\"effnet.h5\",monitor=\"val_accuracy\",save_best_only=True,mode=\"auto\",verbose=1)\nreduce_lr = ReduceLROnPlateau(monitor = 'val_accuracy', factor = 0.3, patience = 2, min_delta = 0.001,\n                              mode='auto',verbose=1)","772073dc":"history = model.fit(X_train,y_train,validation_split=0.1, epochs =12, verbose=1, batch_size=32,\n                   callbacks=[tensorboard,checkpoint,reduce_lr])","4c095968":"filterwarnings('ignore')\n\nepochs = [i for i in range(12)]\nfig, ax = plt.subplots(1,2,figsize=(14,7))\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nval_acc = history.history['val_accuracy']\nval_loss = history.history['val_loss']\n\nfig.text(s='Epochs vs. Training and Validation Accuracy\/Loss',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=1,x=0.28,alpha=0.8)\n\nsns.despine()\nax[0].plot(epochs, train_acc, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label = 'Training Accuracy')\nax[0].plot(epochs, val_acc, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Accuracy')\nax[0].legend(frameon=False)\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Accuracy')\n\nsns.despine()\nax[1].plot(epochs, train_loss, marker='o',markerfacecolor=colors_green[2],color=colors_green[3],\n           label ='Training Loss')\nax[1].plot(epochs, val_loss, marker='o',markerfacecolor=colors_red[2],color=colors_red[3],\n           label = 'Validation Loss')\nax[1].legend(frameon=False)\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Training & Validation Loss')\n\nfig.show()","bc1a74f5":"pred = model.predict(X_test)\npred = np.argmax(pred,axis=1)\ny_test_new = np.argmax(y_test,axis=1)","74861696":"print(classification_report(y_test_new,pred))","dd87af55":"fig,ax=plt.subplots(1,1,figsize=(14,7))\nsns.heatmap(confusion_matrix(y_test_new,pred),ax=ax,xticklabels=labels,yticklabels=labels,annot=True,\n           cmap=colors_green[::-1],alpha=0.7,linewidths=2,linecolor=colors_dark[3])\nfig.text(s='Heatmap of the Confusion Matrix',size=18,fontweight='bold',\n             fontname='monospace',color=colors_dark[1],y=0.92,x=0.28,alpha=0.8)\n\nplt.show()","c9634248":"def img_pred(upload):\n    for name, file_info in uploader.value.items():\n        img = Image.open(io.BytesIO(file_info['content']))\n    opencvImage = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n    img = cv2.resize(opencvImage,(150,150))\n    img = img.reshape(1,150,150,3)\n    p = model.predict(img)\n    p = np.argmax(p,axis=1)[0]\n\n    if p==0:\n        p='Glioma Tumor'\n    elif p==1:\n        print('The model predicts that there is no tumor')\n    elif p==2:\n        p='Meningioma Tumor'\n    else:\n        p='Pituitary Tumor'\n\n    if p!=1:\n        print(f'The Model predicts that it is a {p}')","e71b7033":"uploader = widgets.FileUpload()\ndisplay(uploader)","1a93f52a":"button = widgets.Button(description='Predict')\nout = widgets.Output()\ndef on_button_clicked(_):\n    with out:\n        clear_output()\n        try:\n            img_pred(uploader)\n            \n        except:\n            print('No Image Uploaded\/Invalid Image File')\nbutton.on_click(on_button_clicked)\nwidgets.VBox([button,out])","73bfb170":"def generate_adversary(image, label):\n  image = tf.cast(image, tf.float32)\n\n  with tf.GradientTape() as tape:\n    tape.watch(image)\n    prediction = model(image)\n    loss = tf.keras.losses.MSE(label, prediction)\n  gradient = tape.gradient(loss, image)\n  sign_grad = tf.sign(gradient)\n\n  return sign_grad","440f033e":"def print_shapes(x_train, x_test, y_train, y_test):\n  print(f\"x_train: {x_train.shape}\\n\"\\\n      f\"x_test: {x_test.shape}\\n\"\\\n      f\"y_train: {y_train.shape}\\n\"\\\n      f\"y_test: {y_test.shape}\\n\")\nprint_shapes(X_train, X_test, y_train, y_test)","5c6c38b8":"from random import randint\nheight, width, channels = 150, 150, 3\n\nrand_idx = randint(0,2936)\nimage = X_train[rand_idx].reshape((1, height, width, channels))\nlabel = y_train[rand_idx]\n\nprint(f'Prediction from CNN: {labels[np.where(label==1)[0][0]]}')\nplt.figure(figsize=(3,3))\nplt.imshow(image.reshape((height, width, channels)))\nplt.show()","e86025d8":"perturbations = generate_adversary(image,label).numpy()\nadversarial = image + (perturbations * 0.88)","aff0a8c9":"fig, (ax1,ax2) = plt.subplots(1, 2, sharey=True)\nax1.imshow(image.reshape(height,width, channels))\nax1.set_title(\"Original Image\")\nax2.imshow(adversarial.reshape(height,width, channels))\nax2.set_title(\"Image with Adversary\")\nplt.show()","c460ce18":"print(f'Normal Image Prediction: {labels[model.predict(image).argmax()]}')\nprint(f\"Adversary Prediction: {labels[model.predict(adversarial).argmax()]}\")","0eeb9f1b":"def adversary_generator(batch_size):\n  while True:\n    images = []\n    labels = []\n    for batch in range(batch_size):\n      N = randint(0, 2936)\n      label = y_train[N]\n      image = X_train[N].reshape((1,height, width, channels))\n\n      perturbations = generate_adversary(image, label).numpy()\n      adversarial = image + (perturbations * 0.88)\n\n      images.append(adversarial)\n      labels.append(label)\n\n      if batch%100 == 0:\n        print(f\"{batch} images generated\")\n\n    images = np.asarray(images).reshape((batch_size, height, width, channels))\n    labels = np.asarray(labels)\n\n    yield images, labels","0a76c979":"x_adversarial, y_adversarial = next(adversary_generator(327))\nad_acc = model.evaluate(x_adversarial, y_adversarial, verbose=0)\nprint(f\"Accuracy on Adversarial Examples: {ad_acc[1]*100}\")","d4aa2e40":"history = model.fit(x_adversarial, y_adversarial,validation_split=0.1, epochs =12, verbose=1, batch_size=32,\n                   callbacks=[tensorboard,checkpoint,reduce_lr])","675344b1":"pred = model.predict(x_adversarial)\npred = np.argmax(pred,axis=1)\ny_test_new = np.argmax(y_adversarial,axis=1)","7eccefac":"print(classification_report(y_test_new,pred))","b3fb375f":"from random import randint\nheight, width, channels = 150, 150, 3\n\nrand_idx = randint(0,2936)\nimage = X_train[rand_idx].reshape((1, height, width, channels))\nlabel = y_train[rand_idx]\n\nprint(f'Prediction from CNN: {labels[np.where(label==1)[0][0]]}')\nplt.figure(figsize=(3,3))\nplt.imshow(image.reshape((height, width, channels)))\nplt.show()","c3251794":"perturbations = generate_adversary(image,label).numpy()\nadversarial = image + (perturbations * 0.88)","5198e0bd":"fig, (ax1,ax2) = plt.subplots(1, 2, sharey=True)\nax1.imshow(image.reshape(height,width, channels))\nax1.set_title(\"Original Image\")\nax2.imshow(adversarial.reshape(height,width, channels))\nax2.set_title(\"Image with Adversary\")\nplt.show()","cb5d7565":"print(f'Normal Image Prediction: {labels[model.predict(image).argmax()]}')\nprint(f\"Adversary Prediction: {labels[model.predict(adversarial).argmax()]}\")","2406d913":"We finally compile our model.","92f6f7e3":"# **Adversarial Training**","da5c8a75":"---","9b3312e2":"This is where you can upload the image by clicking on the **Upload** button:","4ba7844b":"---","52336dbe":"**GlobalAveragePooling2D** -> This layer acts similar to the Max Pooling layer in CNNs, the only difference being is that it uses the Average values instead of the Max value while *pooling*. This really helps in decreasing the computational load on the machine while training.\n<br><br>\n**Dropout** -> This layer omits some of the neurons at each step from the layer making the neurons more independent from the neibouring neurons. It helps in avoiding overfitting. Neurons to be ommitted are selected at random. The **rate** parameter is the liklihood of a neuron activation being set to 0, thus dropping out the neuron\n\n**Dense** -> This is the output layer which classifies the image into 1 of the 4 possible classes. It uses the **softmax** function which is a generalization of the sigmoid function.","ec537d29":"# <center>Thank You!","41823db4":"---","b506230f":"After uploading the image, you can click on the **Predict** button below to make predictions:","effe1b30":"---","2871c282":"# **Adversarial Attack**","fdcf4c37":"# Introduction","ebf05762":"---","b7257254":"# Bonus Content: Widgets","5803db2f":"we have made these Widgets in which we can upload images from our local machine and predict whether the MRI scan has a Brain Tumour or not and to classify which Tumor it is.<br>\nUnfortunately, it doesn't work on Kaggle but you can play around with this by downloading the notebook on your machine :)","ab263927":"**Callbacks** -> Callbacks can help you fix bugs more quickly, and can help you build better models. They can help you visualize how your model\u2019s training is going, and can even help prevent overfitting by implementing early stopping or customizing the learning rate on each iteration.<br><br>\nBy definition, \"A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training.\"\n\nIn this notebook, I'll be using **TensorBoard, ModelCheckpoint and ReduceLROnPlateau** callback functions","2aea939f":"# Data Preperation","01a97114":"# Evaluation","8c147c33":"# Importing Libraries","ff5a0ebc":"In this, <br>\n0 - Glioma Tumor<br>\n1 - No Tumor<br>\n2 - Meningioma Tumor<br>\n3 - Pituitary Tumor<br>","76a1eaf0":"---","ae01de54":"# Training The Model","d94d9d77":"# Prediction","b12c455c":"# **After Training**","b4406eb7":"---","ee6c4937":"# Transfer Learning","c082fff9":"---","3036288a":"---"}}