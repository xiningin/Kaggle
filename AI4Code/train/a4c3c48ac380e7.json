{"cell_type":{"f17fe5f9":"code","584e71bf":"code","dc2cb085":"code","0c2d26d8":"code","38a63ed6":"code","a34d1d03":"code","cc97bab3":"code","ea9c0b9b":"code","233e90ba":"code","6175e269":"code","e2d7c5d0":"code","404e95f1":"code","a6455a29":"code","56437605":"code","104273ef":"code","a685fa6e":"code","21b4aa90":"code","f4a2ccab":"code","49dfa8db":"code","6cd4df30":"code","7fe2330c":"code","dda93edc":"code","de285365":"code","007e7be0":"code","e0e4e884":"code","48f3091f":"code","a517af71":"code","e931c292":"code","5c776c62":"code","a85deed1":"code","7e2df9ac":"code","9cb8c3d3":"code","6b0094b6":"code","bec076d4":"code","88a07f94":"code","d9493582":"code","76f6e65c":"code","0057cdb9":"code","398b5c4e":"code","6390bd1c":"code","e594db57":"code","19b79757":"code","3de0dc90":"code","3bd7cce2":"code","a843811a":"code","86588f3d":"code","c48cd8b3":"code","1a4c7f94":"code","7fe110ff":"code","5ce03cae":"code","889895ef":"code","f6161f28":"code","0ad56710":"code","aa10a6ca":"code","0d7df541":"code","e707f2e5":"code","238266b2":"code","befc769d":"code","41ec0259":"code","168a45ba":"code","65680f25":"code","38c8d707":"code","d752d28c":"code","996bcb54":"code","293f9c32":"code","a447dad8":"code","174d710e":"code","3750c321":"code","09da0fb4":"code","85b82731":"code","6773a5e3":"code","d76c29da":"code","a26c45c4":"code","e8e4ebed":"code","1f0dd7de":"code","6715c665":"code","0ed80d81":"code","79ba5515":"code","6b075a84":"code","8a2e348e":"code","5955693b":"code","b9740acd":"markdown","5617693e":"markdown","db4eb96b":"markdown","f7dd0165":"markdown","b63538ee":"markdown","5dc50b76":"markdown","a0c6ac0e":"markdown","5f2cba44":"markdown","8eef3128":"markdown","64583837":"markdown","0e53b0ea":"markdown","d9cb22bb":"markdown","b009d28b":"markdown","474e7dc0":"markdown","f955f53e":"markdown","18f1f6a9":"markdown","5b997c22":"markdown","7b7b1044":"markdown"},"source":{"f17fe5f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","584e71bf":"#OS and System specific\nimport os\nimport sys\nassert sys.version_info >= (3,5)\n#data manipulation\nimport numpy as np\nimport pandas as pd\nfrom collections import Counter\n#visualization imports\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport seaborn as sns\n%matplotlib inline\n#consistent sized plots\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 12,5\nrcParams['axes.labelsize'] = 12\nrcParams['xtick.labelsize'] = 12\nrcParams['ytick.labelsize'] = 12\nrcParams['axes.edgecolor'] = 'blue'\nrcParams['axes.titlesize'] = 14\n#set a no limit on the column table to be viewed in the notebook\npd.options.display.max_columns = None\n#handle unwanted warnings\nimport warnings\nwarnings.filterwarnings(action='ignore',message='')\n#preprocessing libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import SimpleImputer\n#sci-kit learn imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\n#modeling\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n#cross validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n#model evaluation\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\n#neural network model\nimport tensorflow as tf","dc2cb085":"'''Load the training and test data '''\ntrain = pd.read_csv('..\/input\/dare-in-reality\/train.csv',engine='python',delimiter=',')\ntrain_weather = pd.read_csv('..\/input\/dare-in-reality\/train_weather.csv',engine='python',delimiter=',')\n\ntest = pd.read_csv('..\/input\/dare-in-reality\/test.csv',engine='python',delimiter=',')\ntest_weather = pd.read_csv('..\/input\/dare-in-reality\/test_weather.csv',engine='python',delimiter=',')","0c2d26d8":"'''view the train data'''\ntrain.head(3)","38a63ed6":"'''view the train weather data'''\ntrain_weather.head(3)","a34d1d03":"'''check the column names'''\ntrain.columns","cc97bab3":"'''get rid of the leading and lagging blank spaces to avoid confusion'''\ntrain.columns = train.columns.str.replace(' ','')\ntrain_weather.columns = train_weather.columns.str.replace(' ','')\ntest.columns = test.columns.str.replace(' ','')\ntest_weather.columns = test_weather.columns.str.replace(' ','')","ea9c0b9b":"'''check the cleaned column names for one of the dataset'''\ntrain.columns","233e90ba":"'''check info '''\ntrain.info()","6175e269":"'''check weather info '''\ntrain_weather.info()","e2d7c5d0":"'''check for the null values across features'''\ntrain.isnull().sum()","404e95f1":"'''create list of all categorical features'''\ncat_features =  train.select_dtypes(include='object').columns.to_list()\nprint('Categorical features are the following: {}'.format(cat_features))\nprint('Total number of Categorical features: {}'.format(len(cat_features)))","a6455a29":"'''list of numerical features including the target column'''\nnum_features =  train.select_dtypes(exclude='object').columns.to_list()\nprint('Numerical features are the following: {}'.format(num_features))\nprint('Total number of Numerical features: {}'.format(len(num_features)))","56437605":"len(train.columns)","104273ef":"'''check the unique values in the cat features'''\nfor col in cat_features: \n    print('Total unique values in {} is {}'.format(col,train[col].nunique()))   ","a685fa6e":"'''check the unique values in the numerical features'''\nfor col in num_features: \n    print('Total unique values in {} is {}'.format(col,train[col].nunique())) ","21b4aa90":"train['DRIVER_NUMBER'].value_counts()","f4a2ccab":"train['DRIVER_NAME'].value_counts()","49dfa8db":"train['LOCATION'].value_counts().sort_values(ascending=False)","6cd4df30":"train_weather['LOCATION'].value_counts().sort_values(ascending=False)","7fe2330c":"test_weather['LOCATION'].value_counts()","dda93edc":"test['LOCATION'].value_counts()","de285365":"'''Mean lap time per driver'''\ntrain.groupby(by=['NUMBER','DRIVER_NAME'])['LAP_TIME'].mean()","007e7be0":"plt.figure(figsize=(17,7))\nsns.histplot(data=train, x=\"LAP_TIME\", hue=\"EVENT\", multiple=\"stack\")\nplt.title('Histogram of Lap Time')\nplt.show()","e0e4e884":"plt.figure(figsize=(17,7))\nsns.kdeplot(data=train, x=\"LAP_TIME\", hue=\"LOCATION\", multiple=\"stack\")\nplt.title('KDE plot of Lap Time Location Wise')\nplt.show()","48f3091f":"sns.set_theme(style=\"ticks\")\n\n# Initialize the figure with a logarithmic x axis\nf, ax = plt.subplots(figsize=(17, 17))\n\n\n# Plot the orbital period with horizontal boxes\nsns.boxplot(x=\"LAP_TIME\", y=\"DRIVER_NAME\", data=train,\n            whis=[0, 100], width=.6, palette=\"vlag\")\n\n# Add in points to show each observation\nsns.stripplot(x=\"LAP_TIME\", y=\"DRIVER_NAME\", data=train,\n              size=4, color=\".3\", linewidth=0)\n\nplt.title('Box plot of the Lap Time per Driver')\nplt.xlabel('Lap Time with marked observations')\nplt.ylabel('Driver Name')\n# Tweak the visual presentation\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\nsns.despine(trim=True, left=True)","a517af71":"sns.set_theme(style=\"ticks\")\n\n# Initialize the figure with a logarithmic x axis\nf, ax = plt.subplots(figsize=(17, 17))\n\n\n# Plot the orbital period with horizontal boxes\nsns.boxplot(x=\"LAP_TIME\", y=\"LOCATION\", data=train,\n            whis=[0, 100], width=.6, palette=\"vlag\")\n\n# Add in points to show each observation\nsns.stripplot(x=\"LAP_TIME\", y=\"LOCATION\", data=train,\n              size=4, color=\".3\", linewidth=0)\n\nplt.title('Box plot of the Lap Time per Location')\nplt.xlabel('Lap Time with marked observations')\nplt.ylabel('Location')\n# Tweak the visual presentation\nax.xaxis.grid(True)\nax.set(ylabel=\"\")\nsns.despine(trim=True, left=True)","e931c292":"'''Mean lap time per driver location wise'''\ntrain.groupby(by=['LOCATION','DRIVER_NAME'])['LAP_TIME'].mean()","5c776c62":"'''Mean lap time per driver event wise'''\ntrain.groupby(by=['EVENT','DRIVER_NAME'])['LAP_TIME'].mean()","a85deed1":"'''handle the time columns in the dataset'''\ndef time2seconds(time):\n    if type(time) != str:\n        return time\n    parts = [float(p) for p in time.split(':')]\n    parts = [p * (60 ** i) for i, p in enumerate(reversed(parts))]\n    return sum(parts)\n\nTIME_COLUMNS = ['ELAPSED', 'S1', 'S2', 'S3', 'S1_LARGE', 'S2_LARGE', 'S3_LARGE', 'HOUR','PIT_TIME']\nfor col in TIME_COLUMNS:\n    train[col] = train[col].apply(time2seconds)\n    test[col] = test[col].apply(time2seconds)","7e2df9ac":"train.dtypes","9cb8c3d3":"import statistics\nfor col in train.select_dtypes(exclude='object').columns:\n    print(col)\n    print(statistics.variance(train[col]))","6b0094b6":"'''drop The redundant column DRIVER NUMBER'''\ntrain.drop(['DRIVER_NUMBER'],axis=1,inplace=True)\ntest.drop('DRIVER_NUMBER',axis=1,inplace=True)","bec076d4":"'''check the train weather data'''\ntrain_weather.info()","88a07f94":"'''drop the time related columns from the weather data'''\ntrain_weather.drop(['TIME_UTC_SECONDS','TIME_UTC_STR'],axis=1,inplace=True)\ntest_weather.drop(['TIME_UTC_SECONDS','TIME_UTC_STR'],axis=1,inplace=True)","d9493582":"train_weather.columns","76f6e65c":"'''converting from cat to numerical columns'''\ncat_to_num = ['AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED','RAIN']\nfor col in cat_to_num:\n    train_weather[col] = train_weather[col].str.replace(',','.')\n    \ntest_cat_to_num = ['AIR_TEMP', 'TRACK_TEMP', 'HUMIDITY', 'PRESSURE', 'WIND_SPEED']\nfor col in test_cat_to_num:\n    test_weather[col] = test_weather[col].str.replace(',','.')","0057cdb9":"test_weather.dtypes","398b5c4e":"'''check the RAIN as it is of different dtype in train and test dataset'''\ntrain_weather['RAIN'].value_counts()","6390bd1c":"'''convert RAIN to int type'''\nval = ['0.01','0.25','0.03,0.02']\nfor values in val:\n    train_weather['RAIN'] = train_weather['RAIN'].str.replace(values,'0')","e594db57":"'''convert to numerical types'''\ntrain_weather['HUMIDITY'] = train_weather['HUMIDITY'].astype(float)\ntrain_weather['RAIN'] = train_weather['RAIN'].astype(float)","19b79757":"train_weather.dtypes","3de0dc90":"train_weather['AIR_TEMP']","3bd7cce2":"columns = ['AIR_TEMP','TRACK_TEMP','PRESSURE','WIND_SPEED']\nfor col in columns:\n    for i in range(len(train_weather)):\n        train_weather[col][i] = train_weather[col].str.split('.')[i][0]\n        \nfor col in columns:\n    for i in range(len(test_weather)):\n        test_weather[col][i] = test_weather[col].str.split('.')[i][0]","a843811a":"'''convert all to numeric type'''\nfor col in columns:\n    train_weather[col] = train_weather[col].astype(float)\n    test_weather[col] = test_weather[col].astype(float)\n","86588f3d":"train_weather.dtypes","c48cd8b3":"test_weather.dtypes","1a4c7f94":"test_weather['HUMIDITY'] = test_weather['HUMIDITY'].astype(float)","7fe110ff":"test_weather.dtypes","5ce03cae":"'''drop the crossing finish line in pit from the dataset '''\ntrain['CROSSING_FINISH_LINE_IN_PIT'].nunique()","889895ef":"'''drop the crossing finish line in pit as it contains only one unique value and has a lot of missing values'''\ntrain.drop('CROSSING_FINISH_LINE_IN_PIT',axis=1,inplace=True)\ntest.drop('CROSSING_FINISH_LINE_IN_PIT',axis=1,inplace=True)","f6161f28":"train.info()","0ad56710":"'''relook at the features with null values'''\n#percentage of null values against the total\nround(train.isnull().sum()\/len(train) * 100,2)","aa10a6ca":"'''drop the features with more than 70% of rows as null values'''\ntrain.drop(['PIT_TIME','GROUP','POWER'],axis=1,inplace=True)\ntest.drop(['PIT_TIME','GROUP','POWER'],axis=1,inplace=True)","0d7df541":"'''drop the target column from test dataset'''\ntest.drop('LAP_TIME',axis=1,inplace=True)","e707f2e5":"'''rename the EVENT column in train_weather to EVENTS as in the test_weather'''\ntrain_weather.rename(columns={'EVENT':'EVENTS'},inplace=True)","238266b2":"'''merge with the weather dataset'''\ntrain_merge = pd.merge(train,train_weather,on=['LOCATION'],how='outer')\ntest_merge = pd.merge(test,test_weather,on=['LOCATION'],how='outer')","befc769d":"train_merge.info()","41ec0259":"'''NUMBER and DRIVER_NAME can be interchageably used and one can overfit the model'''\ntrain_merge.drop('NUMBER',axis=1,inplace=True)\ntest_merge.drop('NUMBER',axis=1,inplace=True)","168a45ba":"test_merge.select_dtypes(include='object').columns.to_list()","65680f25":"'''drop the redundant EVENTS feature and retain only EVENT feature'''\ntrain_merge.drop('EVENTS',axis=1,inplace=True)\ntest_merge.drop('EVENTS',axis=1,inplace=True)","38c8d707":"'''create a list of categorical and numerical columns'''\ncat_columns = train_merge.select_dtypes(include='object').columns.to_list()\nnum_columns = train_merge.select_dtypes(exclude='object').columns.to_list()","d752d28c":"X = train_merge.drop('LAP_TIME',axis=1)\ny = train_merge[['LAP_TIME']]\n\nX_train,X_dev,y_train,y_dev = train_test_split(X,y,test_size=0.2,random_state=42,stratify=X['LOCATION'])","996bcb54":"'''encode the categorical columns'''\nencoder = LabelEncoder()\nfor col in cat_columns:\n    X_train[col] = encoder.fit_transform(X_train[col])\n    X_dev[col] = encoder.transform(X_dev[col])\n    test_merge[col] = encoder.transform(test_merge[col])","293f9c32":"'''impute the missing numerical values, use a simple imputer. An iterative imputer will yield better result but will run slow'''\nimputer = SimpleImputer(strategy='median')\nX_train = imputer.fit_transform(X_train)\nX_dev = imputer.transform(X_dev)\ntest_merge = imputer.transform(test_merge)","a447dad8":"'''check the shape of the prepared data'''\nX_train.shape, X_dev.shape, test_merge.shape","174d710e":"'''scale and transform the data'''\nscaler = StandardScaler()\npt = PowerTransformer(method='yeo-johnson')\n\n#standardize\nX_train = scaler.fit_transform(X_train)\nX_dev = scaler.transform(X_dev)\ntest_merge = scaler.transform(test_merge)\n\n#power transform to make the data more Gaussian like\nX_train = pt.fit_transform(X_train)\nX_dev = pt.transform(X_dev)\ntest_merge = pt.transform(test_merge)","3750c321":"X_train","09da0fb4":"'''principal component analysis'''\nn_comp = [2,4,6,8,10,12]\nfor comp in n_comp:\n    pca = PCA(n_components=comp)\n    X_train_pca = pca.fit_transform(X_train)\n    print('With number of principal components {}, total explained variation is :- '.format(comp))\n    print(sum(pca.explained_variance_ratio_))","85b82731":"'''apply the principal component based on the previous analysis'''\npca = PCA(n_components=12)\n\nX_train = pca.fit_transform(X_train)\nX_dev = pca.transform(X_dev)\ntest_merge = pca.transform(test_merge)","6773a5e3":"y_train = np.ravel(y_train)\ny_dev = np.ravel(y_dev)","d76c29da":"'''cross validation with linear models. Append more models like ensemble and boosting models to compare performance'''\ncv = KFold(n_splits=10,shuffle=True,random_state=42)\nregressors = [LinearRegression(),Ridge(),Lasso()] \n\n#test on a couple of regressors\nfor reg in regressors:\n    scores = cross_val_score(reg,X_train,y_train,cv=cv,scoring='neg_mean_squared_log_error',verbose=0)\n    print('Regressor {}'.format(reg))\n    print('Scores of various folds {}'.format(scores))\n    print('Average Score over all the folds{}'.format(np.mean(abs(scores))))\n    print('\\n')","a26c45c4":"'''Instead of using the plain vanilla Linear Regression, L2 regularization with Ridge is more appropriate'''\nreg = Ridge(random_state=42,alpha=1.0)\n#fit the model\nreg.fit(X_train,y_train)\ntrain_pred = reg.predict(X_train)\ndev_pred = reg.predict(X_dev)\n\n#evaluation metrics\nprint('Root Mean Squared Error')\nprint(f'Training data {np.sqrt(mean_squared_error(y_train,train_pred))}')\nprint(f'Training Dev data {np.sqrt(mean_squared_error(y_dev,dev_pred))}')\nprint('\\n')\n\nprint('Mean Squared Log Error')\nprint(f'Training data {(mean_squared_log_error(y_train,train_pred))}')\nprint(f'Training Dev data {(mean_squared_log_error(y_dev,dev_pred))}')\n","e8e4ebed":"'''predict on the test data and create submission file'''\ndef test_predict(model):\n    test_pred = model.predict(test_merge)\n    submission = pd.DataFrame(test_pred,columns=['LAP_TIME'])\n    filename = \"submission\" + str(model) + '.csv' \n    submission.to_csv(filename,index=False)","1f0dd7de":"'''predict using the ML model and generate submission file'''\ntest_predict(reg)","6715c665":"'''define a neural network model'''\ndef nn_model(optimizer='adam',kernel_init='glorot_uniform',activation='relu'):\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Dense(units=64,input_dim=X_train.shape[1],activation=activation,kernel_initializer=kernel_init))\n    model.add(tf.keras.layers.Dense(units=32,activation=activation,kernel_initializer=kernel_init))\n    model.add(tf.keras.layers.Dense(units=16,activation=activation,kernel_initializer=kernel_init))\n    model.add(tf.keras.layers.Dense(units=1,kernel_initializer=kernel_init))\n    model.compile(loss=tf.keras.losses.MeanSquaredLogarithmicError(),optimizer=optimizer,metrics=['mean_squared_logarithmic_error'])\n    return model    ","0ed80d81":"model = nn_model()\nhistory = model.fit(X_train,y_train,epochs=100,validation_data=(X_dev,y_dev),batch_size=512)","79ba5515":"'''visualize the nn model results'''\nnn_results = pd.DataFrame(history.history)\nnn_results.head()","6b075a84":"'''plot the performance metric over train and train-dev data'''\nnn_results[['loss','val_loss']].plot()\nplt.title('Model Performance Plot 1')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.grid()\nplt.show()","8a2e348e":"'''plot the performance metric over train and train-dev data'''\nnn_results[['mean_squared_logarithmic_error','val_mean_squared_logarithmic_error']].plot()\nplt.title('Model Performance Plot 2')\nplt.xlabel('Epochs')\nplt.ylabel('Mean Squared Logarithmic Error')\nplt.grid()\nplt.show()","5955693b":"'''generate submission file using nn model '''\ntest_predict(model)","b9740acd":"_The weather information of the Location 8 is present only in the test weather dataset_","5617693e":"- The train data has a mix of string and numerical data\n- There are a number of features which have missing values\n- The target variable LAP_TIME is a numerical feature in the dataset\n- As per the dataset description, S1,S2 and S3 features are reflecting the time but the datatype is in object \n- Weather data has no null values \n- Weather data contains information related to the environment and track","db4eb96b":"# _Problem Statement_\n\nIn the heat of a Formula E race, teams need fast access to insights that can help drivers make split-second decisions and cross the finish line first. Can your data-science skills help Envision Racing, one of the founding teams in the championship, take home even more trophies?\n\nBuild a machine learning model that predicts the Envision Racing drivers\u2019 lap times for the all-important qualifying sessions that determine what position they start the race in. Winning races involves a combination of both a driver\u2019s skills and data analytics. To help the team you\u2019ll need to consider several factors that affect performance during a session, including weather, track conditions, and a driver\u2019s familiarity with the track.","f7dd0165":"- _The data types of all features in the train and test weather dataset is same_\n- _We have made an approximation of the weather features by truncating the addtional values post the commas. Also not sure what these values with multiple commas would represent in case of air temperature etc._","b63538ee":"- _There are many categorical columns in the weather dataset which should be numerical instead. Example Air temp,humidity, pressure, wind speed etc_\n- _On closer look there are many values which have comma , spearating the numbers and hence these are represented as objects instead of numerical_\n- _In the european style comma separates the integers instead of the decimal points_\n- _there are values where there are multiple commas_","5dc50b76":"# _Modeling_","a0c6ac0e":"- _There are many values which have two decimals or earlier commas in the original dataset_","5f2cba44":"# _Load the Data_","8eef3128":"# _Import Libraries_","64583837":"_Two drivers share the same Number 6 while the remaining drivers have a unique number associated to them_","0e53b0ea":"# _EDA & Data Preparation_","d9cb22bb":"_The weather information of the LOCATION 8 is not present in the train_weather but only in the train dataset_","b009d28b":"# _Please leave a comment and upvote if you liked the notebook.Thanks!_","474e7dc0":"There are 4 features which have a very high percentage of null values compared to the overall size of the train dataset\n- CROSSING_FINISH_LINE_IN_PIT,PIT_TIME, GROUP, POWER are the features which have larget set of missing values\n- S3, KPH, S1_LARGE,S2_LARGE,S3_LARGE AND S2 have much fewer null values\n\nBefore deciding on any strategy it is better to explore the features as they currently are and then decide the imputing method or even to exclude the feature if the need be. Statistical tests can also help in deciding the feature list for predictive model.","f955f53e":"- _The performance of the model on both the training and train-dev data is close. The model is not overfitting._","18f1f6a9":"- _There does not appear to be much variation of the Lap time with respect to the Location except that the location 8 seems to have a different distribution compared to the other locations_\n- _One way to handle this feature is to replace all but Location 8 by a single identifier_","5b997c22":"# _Dataset Description_ \n\n\n<b> train.csv <\/b> - 10276 rows x 25 columns (Includes target column as LAP_TIME)\n\n<b> Attributes <\/b>\n\n- NUMBER: Number in sequence\n- DRIVER_NUMBER: Driver number\n- LAP_NUMBER: lap number\n- LAP_TIME: Lap time in seconds\n- LAP_IMPROVEMENT: Number of Lap Improvement\n- CROSSING_FINISH_LINE_IN_PIT\n- S1: Sector 1 in [min:sec.microseconds]\n- S1_IMPROVEMENT: Improvement in sector 1\n- S2: Sector 2 in [min:sec.microseconds]\n- S2_IMPROVEMENT: Improvement in sector 2\n- S3: Sector 3 in [min:sec.microseconds]\n- S3_IMPROVEMENT: Improvement in sector 3\n- KPH: speed in kilometer\/hour\n- ELAPSED: Time elapsed in [min:sec.microseconds]\n- HOUR: in [min:sec.microseconds]\n- S1_LARGE: in [min:sec.microseconds]\n- S2_LARGE: in [min:sec.microseconds]\n- S3_LARGE: in [min:sec.microseconds]\n- DRIVER_NAME: Name of the driver\n- PIT_TIME: time taken to car stops in the pits for fuel and other consumables to be renewed or replenished\n- GROUP: Group of driver\n\n- TEAM: Team name\n- POWER: Brake Horsepower(bhp)\n- LOCATION: Location of the event\n- EVENT: Free practice or qualifying\n\n<b> test.csv <\/b> - 420 rows x 25 columns(Includes target column as LAP_TIME)\nsubmission.csv -Please check the Evaluation section for more details on how to generate a valid submission.\n\n<b> The challenge is to predict the LAP_TIME for the qualifying groups of location 6, 7 and 8. <\/b>","7b7b1044":"# _Neural network model_"}}