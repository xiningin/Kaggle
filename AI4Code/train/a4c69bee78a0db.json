{"cell_type":{"e70bb603":"code","4d6106cb":"code","d551e119":"code","8ba53034":"code","8076b823":"code","85c56b6f":"code","f0a3a3cd":"code","f67fdba5":"code","6cfc79fd":"code","5e43c97f":"code","fc037836":"code","7c558260":"code","c1d72618":"code","56f4f681":"code","260ad79a":"code","09e2b25b":"code","7cba7e8d":"code","f30f0db5":"code","0c66fb2e":"code","4b2f0767":"code","d8cb64d7":"code","546b60c1":"code","55939946":"code","927c3160":"code","20d8c70e":"code","6d7900f0":"code","f37c7894":"code","b2ef856e":"code","ca8227bc":"code","a29eae70":"code","baa94541":"code","85b7f732":"code","67d8b2ec":"code","55d16a7d":"code","3fe7dabf":"code","f7c23bd8":"code","72329db0":"code","b6a94cb8":"code","8bd718a7":"code","39378ffb":"code","e6c673d4":"code","41709da2":"code","f3988f53":"code","e4184523":"code","1624b450":"code","38a874cc":"code","af99b53f":"markdown","348785e4":"markdown","61f53bdf":"markdown","3f373829":"markdown","f9778635":"markdown","49b7c8ed":"markdown","b4620411":"markdown","1dfb9d96":"markdown","49e3f083":"markdown","8ed59748":"markdown","e21c201c":"markdown","074c651f":"markdown","cbea109d":"markdown","51558c49":"markdown","37d606b3":"markdown","c4051ff9":"markdown","ff87b6bc":"markdown","a9f1cd4e":"markdown","08140a6a":"markdown","ac02dd06":"markdown","e5c55dbb":"markdown"},"source":{"e70bb603":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4d6106cb":"#Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns \nsns.set(style=\"white\")\nsns.set(style=\"whitegrid\", color_codes=True)\n\n# we want our plots to appear inside the notebook\n%matplotlib inline \n\n# Models from Scikit-Learn\nfrom sklearn import preprocessing\nplt.rc(\"font\", size=14)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import accuracy_score","d551e119":"df = pd.read_csv(\"\/kaggle\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")\n\n#dimension of data\nprint(df.shape)","8ba53034":"df.head()","8076b823":"df.tail()","85c56b6f":"# Let's find out how many of each class there\ndf[\"stroke\"].value_counts().plot(kind=\"bar\",color=[\"red\", \"blue\"]);\ndf[\"stroke\"].value_counts()","f0a3a3cd":"df.info() ","f67fdba5":"#checking for missing value percent\nround(df.isnull().sum()\/df.shape[0]*100,2) #3.93% of BMI is missing","6cfc79fd":"df.isnull().sum().sum() #201 people's BMI index is missing","5e43c97f":"#Let's impute missing BMI values with \"mean\"\nval = [\"bmi\"] \nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values= np.nan, strategy=\"mean\" )\ndf[val] = imputer.fit_transform(df[val])","fc037836":"#df2 has imputed BMI values\ndf.describe()","7c558260":"#Let's frop the ID column\ndata = df.drop(columns=['id'], axis=1)","c1d72618":"data.head()","56f4f681":"print(data.shape)","260ad79a":"cols = list(data.columns)\ncols","09e2b25b":"cat_data = [x for x in data.columns if data[x].dtype == \"object\"]\nnum_data = [y for y in data.columns if data[y].dtype != \"object\"]","7cba7e8d":"for i in cat_data:\n    print(i,\" = \",data[i].unique())","f30f0db5":"cat_data","0c66fb2e":"num_data","4b2f0767":"import warnings\nwarnings.filterwarnings(\"ignore\")","d8cb64d7":"for i in cat_data:\n    plt.figure(figsize=(8,5))\n    sns.countplot(data[i], palette=\"pastel\")\n    plt.title(i,fontsize=15,color=\"b\")\n    plt.show()","546b60c1":"data['gender'].value_counts()","55939946":"#Remove the \"other\" gender row\n\ndata = data[data.gender!=\"Other\"]","927c3160":"# Check the distribution of the age column with a histogram\ndata.age.plot.hist();","20d8c70e":"# Check the distribution of the age, hypertension,heart_disease,av_glucose_level, bmi and stroke columns with a histogram\nfor i in num_data:    \n    fig = plt.figure(figsize=(8,5))\n    sns.histplot(data[i],kde=True, palette=\"pastel\")\n    plt.title(i,fontsize=12,color=\"r\")\n    plt.show()","6d7900f0":"  sns.displot(data, x='age', y='bmi',height=6, aspect=1)","f37c7894":"g = sns.FacetGrid(data, row=\"heart_disease\", col=\"hypertension\", hue=\"gender\", height=4, aspect=1.4, palette=\"viridis\")\ng.map(sns.scatterplot, \"age\", \"bmi\")\ng.add_legend()","b2ef856e":"g = sns.FacetGrid(data, row=\"ever_married\", col=\"stroke\", hue=\"work_type\", height=4)\ng.map(sns.scatterplot, \"avg_glucose_level\", \"age\")\ng.add_legend()","ca8227bc":"data_dummies = pd.get_dummies(data, columns=['gender','ever_married', 'Residence_type','work_type','smoking_status'], drop_first=True)","a29eae70":"print(data_dummies.shape)\ndata_dummies.head()","baa94541":"# Let's make a pretty correlation matrix \ncorr_matrix = data_dummies.corr()\nfig, ax = plt.subplots(figsize=(15, 10))\nax = sns.heatmap(corr_matrix,\n                 annot=True,\n                 linewidths=0.5,\n                 fmt=\".2f\",\n                 cmap=\"YlGnBu\");\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","85b7f732":"from sklearn.model_selection import train_test_split,cross_val_predict,StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,roc_auc_score,roc_curve\nfrom imblearn.pipeline import Pipeline as imbPipe\nfrom imblearn.over_sampling import SMOTE","67d8b2ec":"# Split data into train and test sets\nnp.random.seed(42)\n\nX = data_dummies.drop(\"stroke\", axis = 1)\ny = data_dummies[\"stroke\"]\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2)","55d16a7d":"X_train","3fe7dabf":"y_train, len(y_train)","f7c23bd8":"#We form a pipline with standard scaler, smote and the model. Standard scaler is needed to uniform the values. \n#Smote is necessary cause our data is imbalanced\n\nLogistic_pipeline = imbPipe([\n    \n    (\"scaler\", StandardScaler()),\n    (\"smote\", SMOTE(random_state=42,n_jobs=-1)),\n    (\"logistic\",LogisticRegression(solver='lbfgs', max_iter=1000))\n])\n\n\ny_pred = cross_val_predict(Logistic_pipeline, X_train, y_train, cv = 3)\nprint(classification_report(y_train, y_pred))","72329db0":"# Plot ROC curve and calculate and calculate AUC metric\nLogistic_pipeline.fit(X_train, y_train)\nplot_roc_curve(Logistic_pipeline, X_test, y_test)","b6a94cb8":"y_pred=Logistic_pipeline.predict(X_test)","8bd718a7":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_pred):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_pred),\n                     annot=True,\n                     cbar=False,\n                     fmt='g')\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n    \nplot_conf_mat(y_test, y_pred)","39378ffb":"KNN_pipeline = imbPipe([\n    \n    (\"scaler\", StandardScaler()),\n    (\"smote\", SMOTE(random_state=42,n_jobs=-1)),\n    (\"KNN\", KNeighborsClassifier())\n])\n\n\ny_pred = cross_val_predict(KNN_pipeline, X_train, y_train, cv = 3)\nprint(classification_report(y_train, y_pred))","e6c673d4":"Random_Forest_pipeline = imbPipe([\n    \n    (\"scaler\", StandardScaler()),\n    (\"smote\", SMOTE(random_state=42,n_jobs=-1)),\n    (\"Random Forest\", RandomForestClassifier(random_state=42))\n])\n\n\ny_pred = cross_val_predict(Random_Forest_pipeline, X_train, y_train, cv = 3)\nprint(classification_report(y_train, y_pred))","41709da2":"Random_Forest_pipeline = imbPipe([\n    (\"scaler\", StandardScaler()),\n    (\"smote\", SMOTE(random_state=42,n_jobs=-1)),\n    (\"rfc\", RandomForestClassifier(random_state=42))\n])\n\nparams={\n    'rfc__n_estimators': [100, 200],\n    'rfc__max_features': [7,8],\n    'rfc__min_samples_leaf': [5,6],\n    'rfc__min_samples_split': [15,20]   \n}\n\nrfc_grid = GridSearchCV(Random_Forest_pipeline, params, cv=3,n_jobs=-1,scoring=\"f1\")\nrfc_grid.fit(X_train, y_train)\nprint(\"Best Parameters for Model:  \",rfc_grid.best_params_)","f3988f53":"#Let's place best params into the Random Forest model\nRandom_Forest_pipeline = imbPipe([\n                                  (\"scaler\", StandardScaler()),\n                                  (\"smote\", SMOTE(random_state=42,n_jobs=-1)),\n                                  (\"Random Forest\", RandomForestClassifier(random_state=42,max_features= 7, min_samples_leaf= 5, min_samples_split= 15, n_estimators= 100))\n                                  ])\n\n\ny_pred = cross_val_predict(Random_Forest_pipeline, X_train, y_train, cv = 3)\nprint(classification_report(y_train, y_pred))","e4184523":"# Plot ROC curve and calculate and calculate AUC metric\nRandom_Forest_pipeline.fit(X_train, y_train)\nplot_roc_curve(Random_Forest_pipeline, X_test, y_test)","1624b450":"y_pred_test=Random_Forest_pipeline.predict(X_test)\nprint(classification_report(y_test, y_pred_test))","38a874cc":"sns.set(font_scale=1.5)\n\ndef plot_conf_mat(y_test, y_pred):\n    \"\"\"\n    Plots a nice looking confusion matrix using Seaborn's heatmap()\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_pred),\n                     annot=True,\n                     cbar=False,\n                     fmt='g')\n    plt.xlabel(\"True label\")\n    plt.ylabel(\"Predicted label\")\n    \n    bottom, top = ax.get_ylim()\n    ax.set_ylim(bottom + 0.5, top - 0.5)\n    \nplot_conf_mat(y_test, y_pred_test)","af99b53f":"<a id=\"3.3\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.3. Focus on cross features<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","348785e4":"We can see that the target variable \"stroke\" is very imbalanced : 249 Stroke wrt to 4861 healty people","61f53bdf":"According to data at hand, the stroke happens among mostly married people (interesting:))","3f373829":"<a id=\"4.2\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.2.Model Selection<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n","f9778635":"Let's find the best params for Random Forest","49b7c8ed":"BMI figures are more dense in young ages and on 60+ ages. The data has also some outliers","b4620411":"---\n* **1.Logistic Regression**\n---\n","1dfb9d96":"<a id=\"3.1\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.1 Focus on the categorical features<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n","49e3f083":"<a id=\"4\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>4. Detailed Analysis and Model Selection<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n","8ed59748":"---\n* **3. Random Forest**\n---","e21c201c":"---\n* **2. KNN**\n---","074c651f":"# Predicting Stroke Using Machine Learning\n\nThis notebook is designed to predict whether or not someone is a stroke candidate based on their medical backround, using selected data science libraries and machine learning classifications models. \n\nThe notebook consist of 4 parts :\n\n<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Content<\/h3>\n\n* [1. Preparing the Tools](#1)\n* [2. About the Dataset](#2)\n* [3. Data Exploration (Explaratory Data Analysis or EDA)](#3)\n    - [3.1 Focus on the categorical features](#3.1)\n    - [3.2 Focus on the numerical features](#3.2)\n* [4. Detailed Analysis and Model Selection](#4) \n    - [4.1 Preparing the data and the Correlation Matrix](#4.1)\n    - [4.2 Model Selection](#4.2)\n    \n\n------------------------------------------------------------------------------------------------------------------\nFirst, let's look in out data in detail\n1. Problem definition\n2. Data\n3. Evaluation\n4. Features\n\n\n## 1.Problem Definition\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\nOur top priority in this health problem is to identify patients with a stroke.\n\n## 2.Data\n\nThe dataset is downloaded from https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset\n\n## 3.Evaluation\n\nEvaluation using F1-Score (given the output class imbalance)\n\n## 4. Features\n\nInformation about the data :\n \n1. id: unique identifier\n2. gender: \"Male\", \"Female\" or \"Other\"\n3. age: age of the patient\n4. hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n5. heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n6. ever_married: \"No\" or \"Yes\"\n7. work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n8. Residence_type: \"Rural\" or \"Urban\"\n9. avg_glucose_level: average glucose level in blood\n10. bmi: body mass index\n11. smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n12. stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","cbea109d":"<a id=\"3\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3. Data Exploration (exploratory data analysis or EDA)<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n","51558c49":"We cannot really say that high BMI and having a heart_disease cause stroke. ","37d606b3":"The goal here is to find out more about the data and become a subject matter export on the dataset you're working with.\n\n1. What is the shape of the target variable?\n2. What kind of data do we have and how do we treat different types?\n3. What's missing from the data and how do you deal with it?\n4. Where are the outliers and why should you care about them?\n5. How can you add, change or remove features to get more out of your data?","c4051ff9":"With the best parametrics, we've reruned the model. The results are still very poor but the ROC curve's performance is high","ff87b6bc":"<a id=\"3.2\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>3.2. Focus on the numerical features<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n","a9f1cd4e":"We can see that the model is not overfitted but need to work in detail to improve the F1 score","08140a6a":"<a id=\"4.1\"><\/a>\n<font color=\"dimgrey\" size=+2.0><b>4.1.Preparing The Data and Correlation Matrix<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n","ac02dd06":"<a id=\"2\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>2. About the Dataset<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","e5c55dbb":"<a id=\"1\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>1. Preparing the Tools<\/b><\/font>\n\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\nWe're going to use pandas, Matplotlib, Seaborn and NumPy for data analysis and manipulation, then sklearn to create the models "}}