{"cell_type":{"d1ddde63":"code","62c1e012":"code","a74c071f":"code","9fbe06db":"code","1dd4661b":"code","c2f0447f":"code","8381dfc1":"code","94d878a8":"code","7a86f053":"code","31b1145e":"code","56993457":"code","951df0ef":"code","75a97004":"code","6b41a13a":"code","4cbadb46":"code","57d852ab":"code","ab31888f":"code","7ed4e57f":"code","473f0c4b":"code","2803b5b4":"code","71a524c5":"code","7e4814bb":"code","2520e97f":"code","95cee279":"code","fd91f942":"code","02c66eda":"code","4eb2f46b":"code","c15e22e8":"code","1d03e63d":"code","84e8a0f7":"code","3f90f460":"code","a5226a5a":"code","391da372":"code","2d72c386":"code","41c23137":"code","6391e4f9":"markdown","15d3b188":"markdown","fab8bdd3":"markdown","9aa56b73":"markdown","f1f2004d":"markdown","868be45e":"markdown","9e25c211":"markdown","c449f757":"markdown","63ddeb47":"markdown","5dac9ded":"markdown","c9888156":"markdown","3118ae1e":"markdown","5c2d23c1":"markdown","73da291a":"markdown","7d3e0121":"markdown","d3e5cf4f":"markdown","b4652c3d":"markdown","f0c99387":"markdown","ab3c0e27":"markdown","b876f82e":"markdown","1d6692a1":"markdown","3bddbb00":"markdown","cc355e0b":"markdown","c2b54fb3":"markdown","952b12e7":"markdown","473f4604":"markdown"},"source":{"d1ddde63":"\nimport pandas as pd# offers data structures and operations for manipulating numerical tables and time series \nimport numpy as np#library contains a large number of mathematical, algebraic, and transformation functions\nfrom sklearn import metrics#includes score functions, performance metrics and pairwise metrics and distance computations\nimport matplotlib.pyplot as plt#used in python scripts, shell, web application servers and other graphical user interface toolkits\nimport seaborn as sns#for statistics visualization with the best statistical tasks built with-in.\n%matplotlib inline","62c1e012":"import numpy as np\nimport os\nimport sys\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv1D, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool1D, Dropout, Flatten, Dense\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport cv2\nimport IPython\nfrom six.moves import urllib\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom keras import optimizers\nfrom keras.layers import BatchNormalization\n\nprint(tf.__version__)","a74c071f":"from sklearn.datasets import load_boston\nboston = load_boston()","9fbe06db":"data = pd.DataFrame(boston.data)","1dd4661b":"# See head of the dataset\ndata.head()","c2f0447f":"data.columns = boston.feature_names\ndata.head()","8381dfc1":"#Adding target variable to dataframe\ndata['PRICE'] = boston.target \n# Median value of owner-occupied homes in $1000s","94d878a8":"#Check the shape of dataframe\ndata.shape","7a86f053":"data.columns","31b1145e":"data.dtypes","56993457":"data.PRICE","951df0ef":"data.nunique()","75a97004":"# Check for missing values\ndata.isnull().sum()","6b41a13a":"# See rows with missing values\ndata[data.isnull().any(axis=1)]","4cbadb46":"# Viewing the data statistics\ndata.describe()","57d852ab":"\n\nfor i in data.select_dtypes(include=np.number).columns:\n    sns.boxplot(x=data[i], palette=\"pink\")\n    plt.show()","ab31888f":"#replaced the outliers with iqr treatment\nfor i in data.columns:\n    q1=data[i].quantile(0.25)\n    q3=data[i].quantile(0.75)\n    iqr=q3-q1\n    upperbound=q3+ 1.5*iqr\n    lowerbound=q1- 1.5*iqr\n    data[i]=data[i].apply(lambda x:data[i].quantile(0.99) \n                          if x>upperbound else data[i].quantile(0.01)\n                         if x<lowerbound else x)","7ed4e57f":"for i in data.select_dtypes(include=np.number).columns:\n    sns.boxplot(x=data[i], palette=\"pink\")\n    plt.show()","473f0c4b":"# Finding out the correlation between the features\ncorr = data.corr()\ncorr.shape","2803b5b4":"# Plotting the heatmap of correlation between features\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='pink')","71a524c5":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.pairplot(data, height=2.5)\nplt.tight_layout()","7e4814bb":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nimport tensorflow as tf\nx, y = boston.data, boston.target\nprint(x.shape) \nx = x.reshape(x.shape[0], x.shape[1], 1)\nprint(x.shape)","2520e97f":"xtrain, xtest, ytrain, ytest=train_test_split(x, y, test_size=0.15)","95cee279":"model = keras.Sequential()\nmodel.add(Conv1D(32, kernel_size=7, activation='relu',input_shape=(13,1)))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(32, kernel_size=1, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv1D(32, kernel_size=1, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=5,padding='same'))\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(Dense(1))\nmodel.compile(optimizer=optimizers.Adam(lr=0.01), loss='mse', metrics=['mae'])","fd91f942":"model.summary()","02c66eda":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","4eb2f46b":"y_pred=model.predict(xtrain)","c15e22e8":"plt.scatter(ytrain,y_pred)\nplt.xlabel(\"prices\")\nplt.ylabel(\"predicted prices\")\nplt.title(\"prices vs predicted prices\")\nplt.show()","1d03e63d":"y_pred.shape","84e8a0f7":"res=(ytrain-y_pred)[[1],:]","3f90f460":"#checking residuals\nplt.scatter(y_pred,res)\nplt.xlabel(\"predicted\")\nplt.ylabel(\"residuals\")\nplt.title(\"predicted vs residuals\")\nplt.show()","a5226a5a":"history = model.fit(xtrain, ytrain, validation_split=0.33, batch_size=11,epochs=40, verbose=0)","391da372":"# list all data in history\nprint(history.history.keys())","2d72c386":"plt.plot(history.history['mae'])\nplt.plot(history.history['val_mae'])\nplt.title('Training and Validation Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","41c23137":"# summarize history for loss\nplt.plot(history.history['loss'], color=\"green\")\nplt.plot(history.history['val_loss'], color=\"pink\")\nplt.title('Training and Validation Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","6391e4f9":"###  2.Importing the Boston Housing dataset","15d3b188":"\nEach record in the database describes a Boston suburb or town.","fab8bdd3":"### 10.Conclusion:\n<B>Once a model has been trained on a given set of data, it can now be used to make predictions on new sets of input data.\nThis model for the prediction can not be considered as a perfect(ideal) model, since the accuracy of the model is not that good. Though we can use these predictions to gain information about data where the value of the target variable is unknown, such as data the model was not trained on.<br>\nThis model to help price homes owned by our clients that they wish to sell.\n* What price would we recommend each client sell his\/her home at?\n* Do these prices seem reasonable given the values for the respective features?\n<\/B><br><br>\n\n#### This model is an exploratory attempt to use Deep learning algorithms in estimating housing prices, and then compare their results\n#### The study in this field has moved a small step ahead in providing some methodological and empirical contributions to property appraisal, and presenting an alternative approach to the valuation of housing prices. \n\n","9aa56b73":"#### checking for outliers","f1f2004d":"1. Title: Boston Housing Data\n\n2. Sources:\n   (a) Origin:  This dataset was taken from the StatLib library which is\n                maintained at Carnegie Mellon University.\n   (b) Creator:  Harrison, D. and Rubinfeld, D.L. 'Hedonic prices and the \n                 demand for clean air', J. Environ. Economics & Management,\n                 vol.5, 81-102, 1978.\n   (c) Date: July 7, 1993\n\n\n3. Number of Instances: 506\n\n4. Number of Attributes: 13 continuous attributes (including \"class\"\n                         attribute \"PRICE\"), 1 binary-valued attribute.\n","868be45e":"* CRIM per capita crime rate by town <br>\n* ZN proportion of residential land zoned for lots over 25,000 sq.ft. <br>\n* INDUS proportion of non-retail business acres per town <br>\n* CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) <br>\n* NOX nitric oxides concentration (parts per 10 million) <br>\n* RM average number of rooms per dwelling <br>\n* AGE proportion of owner-occupied units built prior to 1940 <br>\n* DIS weighted distances to five Boston employment centres <br>\n* RAD index of accessibility to radial highways <br>\n* TAX full-value property-tax rate per 10,000usd <br>\n* PTRATIO pupil-teacher ratio by town <br>\n* B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town <br>\n* LSTAT % lower status of the population <br>","9e25c211":"#### Checking the graphs again: after removing the outliers","c449f757":"## 1.Importing the necessary libraries","63ddeb47":"### 6.Preprocessing:\n* finding null values\n* finding the outliers","5dac9ded":"#### Treatment of outliers","c9888156":"## Shubhi VAshistha\n## 20030242065","3118ae1e":"\n#### NO null value recorded in the dataset","5c2d23c1":"### 8. Developing a Model","73da291a":"#### Calculate and show pairplot","7d3e0121":"### 9.Final prediction and visualization","d3e5cf4f":"Conditions before creating the model:\n* <b>Validation<\/b>: hold-out\n* <b>Layer details<\/b>: the number and type of layers to be used for the model to be created\n* <b>Loss function<\/b>: mse\n* <b>Metrics<\/b>: mae\n* <b>Plots<\/b>: Val_acc and Train_Acc, Val_loss and Train_loss\n* The MaxPooling layers need to follow the convolutional layers and use the Flatten() layer before the Dense layer\n* <B>For Conv layers<\/B>: kernel size is 7\n* <B>For MaxPooling layers<\/B>: kernel size is 5\n* Add the BatchNormalization layer after any Convolutional layer\n* <b>Layer details<\/b>:3 1D Conv, 1 MaxPooling1D, 1 Dropout, 1 Dense 32, 32, 32, 1,\n* <B>No. of units<\/B>: relu in the first 3 layers and no activation in the final layer, \n* <B>optimizer<\/B>:adam \n* <B>learning_rate<\/B>:0.01\n* <b>dropout rate<\/b> = 0.5","b4652c3d":"The Boston housing data was collected in 1978 and each of the 506 entries represent aggregated data about 14 features for homes from various suburbs in Boston, Massachusetts. For the purposes of this project, the following preprocessing steps have been made to the dataset:\n\n* 16 data points have an 'PRICE' value of 50.0. These data points likely contain missing or censored values.\n* 1 data point has an 'RM' value of 8.78. This data point can be considered an outlier.\n* The features 'RM', 'LSTAT', 'PTRATIO', and 'PRICE' are essential.\n* The feature 'PRICE' has been multiplicatively scaled to account for 35 years of market inflation.","f0c99387":"### 5.Checking the unique values","ab3c0e27":"### 3.Initializing the dataframe","b876f82e":"### train test split","1d6692a1":"#### Adding the feature names to the dataframe","3bddbb00":"The model.summary() is textual and includes information about:\n\n* The layers and their order in the model.\n* The output shape of each layer.\n* The number of parameters (weights) in each layer.\n* The total number of parameters (weights) in the model.","cc355e0b":"### 7.Exploratory Data Analysis","c2b54fb3":"### 4.Description of each feature:\n","952b12e7":"# Boston house DNN","473f4604":"## Following are the steps:\n* Importing the necessary libraries\n* Importing the Boston Housing dataset\n* Initializing the dataframe\n* Description of each feature\n* Checking the unique values\n* Preprocessing:<br>*finding null values<br>*finding the outliers\n* Exploratory Data Analysis\n* Developing the model\n* Final prediction and visualization\n* Conclusion"}}