{"cell_type":{"234cf826":"code","2ed08e96":"code","20c99889":"code","66baa1d0":"code","4526b459":"code","112fb218":"code","3d445e65":"code","4f7ed8ec":"code","e32e9a8d":"code","f06df3f2":"code","2010d043":"code","e316fcad":"code","fc101053":"code","348a1f52":"code","b6a0e830":"code","3e00d179":"code","3ec8d1d8":"code","97be0bbe":"code","574aba89":"code","80fea111":"code","3d3dc71f":"code","d5f9565c":"code","2b773e24":"code","b7dfd921":"code","9a441b4b":"code","bed04a55":"code","bc83f959":"code","2ae8c1a7":"code","bf3f0f92":"code","e447d345":"code","ca1c02bc":"code","28504c29":"code","e4fb9a1f":"code","6484f6bf":"code","bb2f1120":"code","bc9fe6ee":"code","0ad59a38":"code","d28d1ba3":"code","ffd63883":"code","b20e9c77":"code","72b871db":"code","6058c763":"code","88838f6e":"code","1b0ae9fe":"code","c1074608":"code","85073d8d":"code","2fc3ee37":"markdown","27038f50":"markdown","f4e92a4a":"markdown","c1b43ed5":"markdown","23a18b48":"markdown","39b8d65c":"markdown","1d96368d":"markdown","5d4ddc5b":"markdown","44914979":"markdown","83292079":"markdown","8d59e0dd":"markdown","79bbf721":"markdown","a2ec36e1":"markdown","17f4ced4":"markdown","9800b516":"markdown","7f4a081d":"markdown","394ab0c1":"markdown","b7d9545d":"markdown","e70cc435":"markdown","22c85bd2":"markdown","3c02a852":"markdown","787b19aa":"markdown","01a9bc8a":"markdown","fe6ddb28":"markdown","0b8d4104":"markdown"},"source":{"234cf826":"# importing basic libraries and dataset\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport pandas_profiling\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\ntrain_data= pd.read_csv('..\/input\/titanic\/train.csv')\n\ntest_data= pd.read_csv('..\/input\/titanic\/test.csv')\ntest_data['Survived']= np.nan\nfull_data= pd.concat([train_data,test_data])","2ed08e96":"full_data.profile_report()","20c99889":"# missingno is a python library used to visualiza missing data\nimport missingno as msno\nmsno.matrix(full_data);","66baa1d0":"print(\"Percentages of missing values: \")\nfull_data.isnull().mean().sort_values(ascending = False)","4526b459":"from statistics import mode\nfull_data[\"Embarked\"] = full_data[\"Embarked\"].fillna(mode(full_data[\"Embarked\"]))","112fb218":"sns.heatmap(full_data.corr(),cmap='viridis');","3d445e65":"full_data['Fare'] = full_data.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))\nfull_data['Age'] = full_data.groupby('Pclass')['Age'].transform(lambda x: x.fillna(x.median()))","4f7ed8ec":"full_data['Cabin'].isna().sum()\/len(full_data)","e32e9a8d":"full_data.drop('Cabin',axis=1,inplace=True)","f06df3f2":"full_data.info()","2010d043":"embarked = pd.get_dummies(full_data[['Embarked','Sex']],drop_first=True)\nfull_data = pd.concat([full_data,embarked],axis=1)","e316fcad":"Name1 = full_data['Name'].apply(lambda x : x.split(',')[1])","fc101053":"full_data['Title'] = Name1.apply(lambda x : x.split('.')[0])","348a1f52":"full_data['Title'].value_counts(normalize=True)*100","b6a0e830":"full_data['Title'] = full_data['Title'].replace([ ' Don', ' Rev', ' Dr', ' Mme',' Ms', ' Major', ' Lady', ' Sir', ' Mlle', ' Col', ' Capt',' the Countess', ' Jonkheer', ' Dona'], 'Other')","3e00d179":"full_data['Title'].unique()","3ec8d1d8":"embarked = pd.get_dummies(full_data['Title'],drop_first=True)\nfull_data = pd.concat([full_data,embarked],axis=1)","97be0bbe":"full_data.drop(['PassengerId','Name','Sex','Ticket','Title','Embarked'],axis=1,inplace=True)","574aba89":"full_data.info()","80fea111":"test = full_data[full_data['Survived'].isna()].drop(['Survived'], axis = 1)\ntrain = full_data[full_data['Survived'].notna()]","3d3dc71f":"train = train.astype(np.int64)\ntest = test.astype(np.int64)","d5f9565c":"train.shape,test.shape","2b773e24":"sns.countplot(x='Survived',data=train_data,hue='Sex');","b7dfd921":"sns.countplot(x='Survived',data=train_data,hue='Pclass');","9a441b4b":"sns.distplot(train['Age'],kde=False,color='darkred',bins=30);","bed04a55":"sns.countplot(x='SibSp',data=train);","bc83f959":"sns.countplot(x='Parch',data=train);","2ae8c1a7":"train['Fare'].hist(color='green',bins=40,figsize=(12,6))\nplt.xlabel('Fare');","bf3f0f92":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'], axis = 1), \n                                                    train['Survived'], test_size = 0.2, \n                                                    random_state = 2)","e447d345":"logisticRegression = LogisticRegression(max_iter = 10000)\nlogisticRegression.fit(X_train, y_train)\npredictions = logisticRegression.predict(X_test)\nprint(confusion_matrix(y_test, predictions))\nprint(classification_report(y_test, predictions))","ca1c02bc":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score,cross_val_predict","28504c29":"kf = KFold(n_splits = 5)\nscore = cross_val_score(logisticRegression, train.drop('Survived', axis = 1),train['Survived'], cv = kf)\nprint(f\"Accuracy after cross validation is {score.mean()*100}\")","e4fb9a1f":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping","6484f6bf":"model = Sequential()\nmodel.add(Dense(units=12,activation='tanh'))\nmodel.add(Dense(units=100,activation='tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=100,activation='tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=100,activation='tanh'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\n\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n\nmodel.fit(x=X_train.values, \n          y=y_train.values, \n          epochs=600,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","bb2f1120":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot();","bc9fe6ee":"dnn_predictions = model.predict_classes(X_test)\nprint(classification_report(y_test,dnn_predictions))\nprint(confusion_matrix(y_test,dnn_predictions))","0ad59a38":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)\nprint(classification_report(y_test,rfc_pred))\nprint(confusion_matrix(y_test,rfc_pred))","d28d1ba3":"param_grid = { \n    'criterion' : ['gini', 'entropy'],\n    'n_estimators': [100, 300,500],\n    'max_features': ['auto', 'log2'],\n    'max_depth' : [3,5, 7,9]    \n}\n\nfrom sklearn.model_selection import GridSearchCV\nrandomForest_CV = GridSearchCV(estimator = rfc, param_grid = param_grid, cv = 5)\nrandomForest_CV.fit(X_train, y_train)\ngrid_pred = randomForest_CV.predict(X_test)\nprint(classification_report(y_test,grid_pred))\nprint(confusion_matrix(y_test,grid_pred))","ffd63883":"randomForest_CV.best_params_","b20e9c77":"from xgboost import plot_importance,XGBClassifier","72b871db":"xgb = XGBClassifier().fit(X_train, y_train)\nxgb_pred = xgb.predict(X_test)\nprint(confusion_matrix(y_test, xgb_pred))\nprint(classification_report(y_test, xgb_pred))","6058c763":"print(\"Feature Importance\")\nplot_importance(xgb);","88838f6e":"test['Survived'] = logisticRegression.predict(test)\ntest['PassengerId'] = test_data['PassengerId']\ntest[['PassengerId', 'Survived']].to_csv('lm_submission.csv', index = False)","1b0ae9fe":"test['Survived'] = model.predict_classes(test.iloc[:,:12])\ntest[['PassengerId', 'Survived']].to_csv('dnn_submission.csv', index = False)","c1074608":"test['Survived'] = rfc.predict(test.iloc[:,:12])\ntest[['PassengerId', 'Survived']].to_csv('rfc_submission.csv', index = False)","85073d8d":"test['Survived'] = xgb.predict(test.iloc[:,:12])\ntest[['PassengerId', 'Survived']].to_csv('xgb_submission.csv', index = False)","2fc3ee37":"## 1.2 Converting categorical columns.","27038f50":"Let's improve our accuracy by using N-fold cross-validation.","f4e92a4a":"# 3. Applying Logistic Regression.","c1b43ed5":"Kindly,provide feedback and help me to grow.\nUpvote if you like my analysis.","23a18b48":"Mostly people on board were without their siblings or spouse.","39b8d65c":"We will drop PassengerId and Ticket column as it doesn't seem important.Name too is not of much significance but salutation can be of importance.","1d96368d":"Mostly people on board were aged between 20-40.","5d4ddc5b":"Mostly people on board were travelling alone.","44914979":"Somehow, those who survived had more ratio of females and vice versa.  ;)","83292079":"It has improved to 81%.\n","8d59e0dd":"Now, let's retrieve our training and test data. And then convert each feature to integer.","79bbf721":"Now we will convert categorical columns into numerical using dummy variables.","a2ec36e1":"It provides less accuracy than logistic regression","17f4ced4":"Except first four titles all form less than 1% of the data.So, we will combine them into one category and then form dummy variables.","9800b516":"Null values in Survived column are of the test dataset. Age and Cabin columns contain many missing values.","7f4a081d":"# 2.Exploratory data analysis","394ab0c1":"# 6. Applying XGBoost","b7d9545d":"Embarked and Fare have less than 1% missing values.So, we will simply fill them with mode and median.\nTo fill missing Age values I will find most correlated factor with age.","e70cc435":"# 1. Feature Engineering\n\n## 1.1 Dealing with missing values","22c85bd2":"So, we will fill Age and Fare column with help of Pclass feature.","3c02a852":"Fare seems to be mostly below 100.","787b19aa":"Almost 3\/4th data is missing in Cabin feature.So, we will drop this column.","01a9bc8a":"# 7. Submitting predictions.","fe6ddb28":"# 4. Applying deep neural network","0b8d4104":"# 5. Applying Random Forest."}}