{"cell_type":{"41472c2c":"code","bc8b953a":"code","6f46ecdd":"code","db7fafc6":"code","75ba3ac3":"code","eebef78c":"code","c4584bce":"code","6644dfe8":"code","770d6159":"code","8fa12cac":"code","e1de7fe9":"code","b22c864e":"code","5bb3c3f0":"code","507792c2":"code","0c9777b8":"code","6b1b4f36":"code","98f22ba7":"code","c66f59a6":"code","2e863834":"code","8f00a232":"code","28321d3f":"code","806d83b0":"code","c61f1627":"code","f58a57ca":"code","0566184d":"code","3585fd54":"code","414ec9ad":"code","5f75bc4a":"code","7e9d887f":"code","5f8916ad":"code","cc709e6e":"code","2415b097":"code","53167f8d":"code","b5e8fb2e":"code","37759cf8":"code","8d9fec3a":"code","4f3620ef":"code","aa4e4ffc":"code","8d56c846":"markdown","98161c24":"markdown","b01f5777":"markdown","679312a2":"markdown","4aab24b8":"markdown","2a9448c6":"markdown","8f269d04":"markdown","ee8f11d7":"markdown","3edc7565":"markdown","55376333":"markdown","a388e457":"markdown","e4072051":"markdown","969bdc9a":"markdown","0b74a07b":"markdown","be7dc657":"markdown","c0eb9e03":"markdown","d1388249":"markdown","1f248c19":"markdown","32148a9b":"markdown","df879888":"markdown","91e2f7cc":"markdown","540b2c12":"markdown","95664b61":"markdown","67ba7dc4":"markdown","caff87d4":"markdown","f5475f59":"markdown","1ff9b9c0":"markdown","9afd391b":"markdown","6bc02222":"markdown"},"source":{"41472c2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc8b953a":"import matplotlib.pyplot as plt\nimport seaborn as sbn\nsbn.set_style('darkgrid')","6f46ecdd":"dataset = pd.read_csv(r'\/kaggle\/input\/insurance\/insurance.csv')\ndataset","db7fafc6":"dataset.describe()","75ba3ac3":"fig, axs = plt.subplots(ncols = 3, figsize = (20,5))\nsbn.distplot(dataset['age'], ax = axs[0])\nsbn.distplot(dataset['bmi'], ax = axs[1])\nsbn.distplot(dataset['charges'], ax = axs[2])","eebef78c":"fig2, axs2 = plt.subplots(ncols = 2, figsize = (15,6))\nsbn.countplot(dataset['sex'], ax = axs2[0])\nsbn.countplot(dataset['region'], ax = axs2[1])","c4584bce":"fig3, axs3 = plt.subplots(ncols = 1, figsize = (15,10))\nsbn.boxplot(x = dataset['region'], y = dataset['charges'], hue = dataset['sex'], ax = axs3)","6644dfe8":"sbn.pairplot(dataset.select_dtypes(exclude = ['object']).drop([\"children\"], axis = 1))","770d6159":"fig4, axs4 = plt.subplots(ncols = 1, figsize = (15,10))\nsbn.boxplot(x = dataset['children'], y = dataset['charges'], ax = axs4)","8fa12cac":"fig5, axs5 = plt.subplots(ncols = 1, figsize = (7,5))\nsbn.boxplot(x = dataset['smoker'], y = dataset['charges'], ax = axs5)","e1de7fe9":"fig6, axs6 = plt.subplots(ncols = 2, figsize = (15,10))\nsbn.scatterplot(x = dataset[dataset['smoker'] == 'yes']['age'], y = dataset[dataset['smoker'] == 'yes']['charges'], ax = axs6[0]).set_title(\"Smoker = yes\")\nsbn.scatterplot(x = dataset[dataset['smoker'] == 'no']['age'], y = dataset[dataset['smoker'] == 'no']['charges'], ax = axs6[1]).set_title(\"Smoker = no\")","b22c864e":"sbn.distplot(dataset['bmi']).set_title(\"Distribution of bmi variable\")","5bb3c3f0":"dataset['overweight'] = np.where(dataset['bmi']>30, 'yes', 'no')","507792c2":"fig7, axs7 = plt.subplots(ncols = 2, figsize = (15,10))\nsbn.scatterplot(x = dataset[(dataset['smoker'] == 'yes')]['age'], y = dataset[(dataset['smoker'] == 'yes')]['charges'], ax = axs7[0], hue = dataset['overweight']).set_title(\"Smoker = yes\")\nsbn.scatterplot(x = dataset[(dataset['smoker'] == 'no')]['age'], y = dataset[(dataset['smoker'] == 'no')]['charges'], ax = axs7[1], hue = dataset['overweight']).set_title(\"Smoker = No\")","0c9777b8":"dataset['num_family'] = np.where(dataset['children']>0, 'yes', 'no')","6b1b4f36":"fig8, axs8 = plt.subplots(ncols = 4, figsize = (20,10))\nsbn.scatterplot(x = dataset[(dataset['smoker'] == 'no')]['age'], y = dataset[(dataset['smoker'] == 'no')]['charges'], hue = dataset['region'], ax = axs8[0]).set_title(\"Smoker = No\")\nsbn.scatterplot(x = dataset[(dataset['smoker'] == 'no')]['age'], y = dataset[(dataset['smoker'] == 'no')]['charges'], hue = dataset['sex'], ax = axs8[1]).set_title(\"Smoker = No\")\nsbn.scatterplot(x = dataset[(dataset['smoker'] == 'no')]['age'], y = dataset[(dataset['smoker'] == 'no')]['charges'], hue = dataset['children'], ax = axs8[2]).set_title(\"Smoker = No\")\nsbn.scatterplot(x = dataset[(dataset['smoker'] == 'no')]['age'], y = dataset[(dataset['smoker'] == 'no')]['charges'], hue = dataset['num_family'], ax = axs8[3]).set_title(\"Smoker = No\")","98f22ba7":"dataset = dataset.drop(['overweight', 'num_family'], axis = 1)","c66f59a6":"from sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedKFold, cross_val_score, GridSearchCV\nfrom sklearn.compose import ColumnTransformer,TransformedTargetRegressor\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestRegressor\nfrom keras.wrappers.scikit_learn import KerasRegressor","2e863834":"X_num = dataset.select_dtypes(exclude = ['object']).drop(['charges'], axis = 1)\ny = dataset['charges']\nX_cat = dataset.select_dtypes(include = ['object'])\nX = pd.concat([X_num, X_cat], axis = 1)\ncat_index = dataset.select_dtypes(include = ['object']).columns","8f00a232":"trans_steps= [('cat', OneHotEncoder(drop = 'first'), cat_index)]\ncols_transform = ColumnTransformer(transformers = trans_steps)\npipeline = Pipeline(steps = [('trans', cols_transform), ('model', LinearRegression())])\ncv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 0)\nbaseline = cross_val_score(estimator = pipeline, X =X, y = y, scoring = 'r2', cv = cv, n_jobs = -1)","28321d3f":"sbn.boxplot(baseline, orient = 'v').set_title(\"R2 of baseline model\")","806d83b0":"y = dataset['charges']\nX = dataset.drop(['charges'], axis = 1)","c61f1627":"X.var()","f58a57ca":"X['children'] = X['children'].astype(str)","0566184d":"cat_index = dataset.select_dtypes(include = ['object']).columns\ntrans_steps= [('cat', OneHotEncoder(drop = 'first'), cat_index)]\ncols_transform = ColumnTransformer(transformers = trans_steps)\npipeline = Pipeline(steps = [('trans', cols_transform), ('model', LinearRegression())])\ncv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 0)\nbaseline_child_str = cross_val_score(estimator = pipeline, X =X, y = y, scoring = 'r2', cv = cv, n_jobs = -1)\nsbn.boxplot(data = [baseline, baseline_child_str], orient = 'v').set_title(\"Baseline model (left), baseline with cat. children (right)\")","3585fd54":"cat_index = X.select_dtypes(include = ['object']).columns\nnum_index = X.select_dtypes(exclude = ['object']).columns\ntrans_steps= [('cat', OneHotEncoder(drop = 'first'), cat_index), ('standarz', StandardScaler(),num_index)]\ncols_transform = ColumnTransformer(transformers = trans_steps)\npipeline = Pipeline(steps = [('trans', cols_transform), ('model', LinearRegression())])\nttregressor = TransformedTargetRegressor(regressor = pipeline, transformer = StandardScaler())\ncv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 0)\nbaseline_trans = cross_val_score(estimator = ttregressor, X =X, y = y, scoring = 'r2', cv = cv, n_jobs = -1)\nsbn.boxplot(data = [baseline, baseline_child_str, baseline_trans], orient = 'v').set_title(\"Baseline model (left), baseline with cat. children (center) and cat.+stand (right)\")","414ec9ad":"from sklearn.preprocessing import PolynomialFeatures\nr2_degrees = []\nfor i in range(0,6):\n    cat_index = X.select_dtypes(include = ['object']).columns\n    num_index = X.select_dtypes(exclude = ['object']).columns\n    trans_steps= [('cat', OneHotEncoder(drop = 'first'), cat_index), ('standarz', StandardScaler(),num_index), ('poly', PolynomialFeatures(degree = i), num_index)]\n    cols_transform = ColumnTransformer(transformers = trans_steps)\n    pipeline = Pipeline(steps = [('trans', cols_transform), ('model', LinearRegression())])\n    ttregressor = TransformedTargetRegressor(regressor = pipeline, transformer = StandardScaler())\n    cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 0)\n    cvs_i = cross_val_score(estimator = ttregressor, X =X, y = y, scoring = 'r2', cv = cv, n_jobs = -1)\n    r2_degrees.append(cvs_i)\nsbn.boxplot(data = r2_degrees).set_title(\"R2 with different Polynomial degrees\")","5f75bc4a":"r2_deg1_rfe = []\nfor i in range(1,15):\n    cat_index = X.select_dtypes(include = ['object']).columns\n    num_index = X.select_dtypes(exclude = ['object']).columns\n    trans_steps= [('cat', OneHotEncoder(drop = 'first'), cat_index),('standarz', StandardScaler(),num_index)]\n    cols_transform = ColumnTransformer(transformers = trans_steps)\n    pipeline = Pipeline(steps = [('trans', cols_transform),('rfe', RFE(estimator = LinearRegression(),n_features_to_select = i)),('model', LinearRegression())])\n    ttregressor = TransformedTargetRegressor(regressor = pipeline, transformer = StandardScaler())\n    cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 0)\n    cvs_i = cross_val_score(estimator = ttregressor, X =X, y = y, scoring = 'r2', cv = cv, n_jobs = -1)\n    r2_deg1_rfe.append(np.mean(cvs_i))\nplt.plot(r2_deg1_rfe)\nplt.title(\"R2 with degree = 1. Different variables\")","7e9d887f":"def columns_transform(X, degree = 1):\n    ss = StandardScaler()\n    pf = PolynomialFeatures(degree = degree)\n    X_num = X.select_dtypes(exclude = ['object'])\n    X_cat = X.select_dtypes(include = ['object'])\n    X_num = pd.DataFrame(data = pf.fit_transform(X_num), columns = pf.get_feature_names(X_num.columns))\n    X_num = pd.DataFrame(data = ss.fit_transform(X_num), columns = X_num.columns)\n    X_cat = pd.get_dummies(X_cat).drop(['region_northeast', 'children_0','sex_female', 'smoker_no'], axis = 1)\n    X_join = pd.concat([X_num, X_cat], axis = 1)\n    return X_join\n\ndef scaler(X):\n    ss = StandardScaler()\n    X_scaled = ss.fit_transform(X.values.reshape(-1,1))\n    X = pd.DataFrame(data = X_scaled, columns = ['charges'])\n    return X","5f8916ad":"X_trans = columns_transform(X)\nrfe = RFE(estimator = Ridge(), n_features_to_select = 4)\nrfe.fit(X_trans, scaler(y))\nprint('Most important features: ',X_trans.columns[rfe.support_])\nX_trans_4 = X_trans[X_trans.columns[rfe.support_]]","cc709e6e":"ridge_params = {'alpha':[0.01,0.05,0.1,0.30,0.50,0.60,0.70,0.80,0.90,1,2,2.1,2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9,3,3.5,4,5,6,7,8,9,10,15]}\nridge_grid = GridSearchCV(estimator = Ridge(), param_grid = ridge_params, scoring = 'r2', refit = True, cv = cv,verbose = 3, n_jobs = -1)\nridge_grid.fit(X_trans_4,scaler(y))\nprint(\"Best alpha: \", ridge_grid.best_params_)","2415b097":"best_linearRegressor = []\npipeline = Pipeline(steps = [('model', Ridge(alpha = 2.4))])\ncv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 0)\nbest_linearRegressor = cross_val_score(estimator = ttregressor, X =X, y = y, scoring = 'r2', cv = cv, n_jobs = -1)\nsbn.boxplot(data = [baseline, best_linearRegressor]).set_title(\"R2 of baseline model VS Best Regression Model\")","53167f8d":"r2_degrees = []\nfor i in range(0,6):\n    cat_index = X.select_dtypes(include = ['object']).columns\n    num_index = X.select_dtypes(exclude = ['object']).columns\n    trans_steps= [('cat', OneHotEncoder(drop = 'first'), cat_index), ('standarz', StandardScaler(),num_index), ('poly', PolynomialFeatures(degree = i), num_index)]\n    cols_transform = ColumnTransformer(transformers = trans_steps)\n    pipeline = Pipeline(steps = [('trans', cols_transform), ('model', RandomForestRegressor())])\n    ttregressor = TransformedTargetRegressor(regressor = pipeline, transformer = StandardScaler())\n    cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 0)\n    cvs_i = cross_val_score(estimator = ttregressor, X =X, y = y, scoring = 'r2', cv = cv, n_jobs = -1)\n    r2_degrees.append(cvs_i)\nsbn.boxplot(data = r2_degrees).set_title(\"R2 with different Polynomial degrees\")","b5e8fb2e":"r2_deg2_rfe_rf = []\nfor i in range(1,25):\n    cat_index = X.select_dtypes(include = ['object']).columns\n    num_index = X.select_dtypes(exclude = ['object']).columns\n    trans_steps= [('cat', OneHotEncoder(drop = 'first'), cat_index),('poly', PolynomialFeatures(degree = 2), num_index),('standarz', StandardScaler(),num_index)]\n    cols_transform = ColumnTransformer(transformers = trans_steps)\n    pipeline = Pipeline(steps = [('trans', cols_transform),('rfe', RFE(estimator = LinearRegression(),n_features_to_select = i)),('model', RandomForestRegressor())])\n    ttregressor = TransformedTargetRegressor(regressor = pipeline, transformer = StandardScaler())\n    cv = RepeatedKFold(n_splits = 4, n_repeats = 3, random_state = 0)\n    cvs_i = cross_val_score(estimator = ttregressor, X =X, y = y, scoring = 'r2', cv = cv, n_jobs = -1)\n    r2_deg2_rfe_rf.append(np.mean(cvs_i))\nplt.plot(r2_deg2_rfe_rf)\nplt.title(\"R2 with degree = 2. Different variables\")","37759cf8":"x_rfr_final = columns_transform(X, 2)\ny_rfr_final = scaler(y)\nrfe_final_rfr = RFE(estimator = RandomForestRegressor(), n_features_to_select = 11)\nrfe_final_rfr.fit(x_rfr_final,y_rfr_final)","8d9fec3a":"print(\"Best params: \", x_rfr_final[x_rfr_final.columns[rfe_final_rfr.support_]].columns)","4f3620ef":"params_rfr = {'n_estimators':[50,100,150],\n              'min_samples_split':[2,3,4,5,6,7,8,9,10,15,20],\n              'min_samples_leaf':[2,3,4,5,6,7,8,9,10,20],\n              'n_jobs':[-1]\n             }\ngrid = GridSearchCV(estimator = RandomForestRegressor(), param_grid = params_rfr, scoring = 'r2', refit = True, verbose = 3)\ngrid.fit(x_rfr_final,y_rfr_final)\nprint('Best params: ', grid.best_params_) ","aa4e4ffc":"final_rfr_model = RandomForestRegressor(min_samples_leaf = 9, min_samples_split = 20, n_estimators = 50)\ncv = cv = RepeatedKFold(n_splits = 5, n_repeats = 5, random_state = 0)\nfinal_rfr_score = cross_val_score(estimator = final_rfr_model, X =x_rfr_final, y = y_rfr_final, scoring = 'r2', cv = cv, n_jobs = -1)\nsbn.boxplot(data = [baseline, best_linearRegressor, final_rfr_score]).set_title(\"Baseline Model vs Best Regression vs Best RandomForest\")","8d56c846":"### The following part of the notebook will be used to work with Random Forest Regressors. Regarding the steps made with the Linear Regression, we will import the pipeline and transformations done to the original dataset. It means that we will apply standarization, explore polynomial features and finally fine-tune with model parameters. \n\n### Thus, first step will be testing different polynomial features. \n","98161c24":"As we previously saw, bmi variable has a bell shape. We will create a variable called \"overweight\" to state if the beneficiary has overweight or not and see if it helps to explain the two groups formed in the previous age \/ charge scatterplot.","b01f5777":"### Well, not too much effect! Statistically speaking, it does not make any difference but improves the  model by an 0.23%. Although the impact is quite limited, I decided to leave categorization of the variable.\n\n### The next step will be standarizing numerical variable and check the impact on model performance.","679312a2":"# Random Forest","4aab24b8":"### As we can see in the previous boxplot, the model performs practically the same for degree 1 and 2. For that reason and for computational efficiency, I will use no power transformation for the  the following phase, which is Recursive feature elimination (RFE). \n\n### For that purpose, I import the previous pipeline, with the diffence that I will loop over different number of features to include in the model.","2a9448c6":"### As we can see, the dataset is quite well distributed across different categories, so no modifications will be needed regarding this fact.\n\n### We can also check how the previous variables affect the explained variable: \n","8f269d04":"### Beautiful!!Numerical input standarization clearly improve the linear regression model. Said that, we will use categorization of \"children\" variable + stantarization of X's variables as well as y variable for the following tests.\n\n### Now, I will add polynomial features to the regression model. The first step on this stage will be building a pipeline to test different degrees and crossvalidate them to select the best one. ","ee8f11d7":"### With this last step, I have done regarding Linear Regression models. In the previous boxplot we can see the increase in the performance that we have achieved compared with the baseline model (only dummy variables).","3edc7565":"### In this model, the only involved preprocessing activity will be transorming categorical variables into their dummy version. It means that, instead of categorical values, new binary variables will be created.\n### To avoid collinearity, I will order OneHotEncoder function to drop the first values. (E.g: sex has \"male\" and \"female\". Thus, one of both will be dropped). \n\n### Prior to everything, I load usefull libraries that will be used later one.","55376333":"# Baseline model","a388e457":"### We can say that there is a positive relationship between age and charge. Nevertheless, there may be another factor that makes a split in the response: \n* In the case of non-smokers, there are two groups: one  with a very low dispersion and a charge up to 15.000, and another one with much higher fees and higher dispersion. \n* Regarding smokers, there are also two separate groups. \n\n### Now, we will try to include BMI into the equation to seee if it can explain that separation. ","e4072051":"### In the case of smokers, the fact of suffering from overweith clearly separates the two groups, where smokers with overweight pay  much more for their medical insurance.\n### Since bmi is useless to explain the same fact for non-smokers, we will make similar plots with other variables. We will also include a binarized variable from the number of children (have \/ don't have children):","969bdc9a":"### As we can see in the dataset, we have the following variables:\n\n#### Predictors:\n* age: that describe how old the beneficiary is. \n* sex: insurance contractor gender:male,female.\n* bmi, body mass index. Numeric variable that measures it the contractor suffers from over\/underweight.\n* children, number of children covered by the insurance policy.\n* region: beneficiary's residential area.\n\n#### Predicted variable:\n\ncharges: medical cost billed to beneficiary.\n","0b74a07b":"### Good! Here we see that, beyond the 11th feature, the R2 remains stable in ~83%. It means that we don't need to complicate the model more than necessary.Thus, we will use the first 11 variables for our model.\n\n### Once we have completed both polynomial features and RFE, I will try to fine-tune the model with the random forest parameters. ","be7dc657":"### From the previous pairplot, we can see that: \n\n* There is a slight slope in the \"age\" vs \"bmi\" slope. \n* There are three \"lines\" with increasing dispersion in the \"age\" vs \"charges\" plot, maning that there might be other variables affecting the relationship between age and charges.\n\n### Let's see the impact that number of children under the policy may have:","c0eb9e03":"# Conclusions\n\n### After fine-tunning the last model, Random forest Regression, I can say with confidence that this last model provides the best fit for the given dataset. Said that, I would also like to point out several lessons that I have learned by conducting this analysis: \n\n### * Firstly,speding time with preprocessing is really worth it. We all are always looking and testing different algorithms, and that is quite ok. Nevertheless, we should not forget that a simple preprocessing can add extra performance to the model with simple transformations and calculations. \n### * Secondly, comparing and testing different algorithms is part of our job. So no discussion here regarding wether to stick to one technique or try different ones.\n\n\n### As previously stated, if we had to choose one of them, it is clear that we would choose the last one: Random Forest Regressio, since it reaches the highest R2 metric. \n### A preprocessing technique that I have not used is outlier elimination and it would be a good idea to insert it within the pipeline and check the impact on performance. \n\n### A model that I have not tested is Artificial Neural Network, which is a quite powerful one. I will definitely use it in future versions and exercises. ","d1388249":"### From the previous table, we can see that: \n1. The variance of the variable \"children\" is far smaller than the rest. It may be taken into account when tunning the baseline model.\n2. Variables \"age\" and \"bmi\" has similar shape except for the mean and maximum value.\n\n### Now, distribution of age, bmi and charges will be plotted:","1f248c19":"### From previous plots, we can see:\n1. Only \"bmi\" variable has a normal shape\n2. \"charges\" appears to highly skewed. \n3. Regarding \"age\", there are more patients in their early-twenties than the rest of age ranges.\n\n### Since we have two categorical variablese, wen can also check if the dataset has more data for a certain value or values or is well distributed across different categories.","32148a9b":"### Bingo!Here we have a factor that significantly affect \"charges\"! We will further explore the relationship between smoker, bmi and age and their response to charges.\n","df879888":"### What we can see in the previous line plot is how the R2 of the model increases with the number of features used.It is noticeable that, after the 4rd included feature , we obtain a similar performance. Thus, we will use the 4 most important variables for our model and drop the rest.\n\n### We have conducted: \n\n### * Variables standarization\n### * No polynomial transformation\n### * 4 features maximum.\n\n### The last thing that I am trying is to fine-tune the regression model through alpha parameter of Ridge Regression. Since GridSearchCV will be ued, I will manually modify the dataset, so we only have to loop over the alpha parameter without need of pipeline.\n\n### Below, my custom funtions for this purpose. ","91e2f7cc":"### As we stated previously, factor \"children\" has a very little variance compared to the rest of numerical variables within the dataset. For that reason, the following step will be transforming \"children\" variable to categorical data and check if such modification helps to improve the model performance.","540b2c12":"# EDA:","95664b61":"### As we can see in the previous boxplot,scores across different degrees of polynomial features are practically the same. Nevertheless, degree 2 seems to provide a slight advantage to model. For that reason, I will use degree = 2 for the following step: RFE.","67ba7dc4":"### It is not clear that the number of children can significantly affect the variable \"charges\". We see it in the overlapping boxplots.\n\n### Now, we will analyse the relationship between smoker and the explained variable","caff87d4":"### With available data and categorized as it is, we can't explain why non-mokers pay more money than others as we have done with smokers(overweight caused such difference).\n\n### With the las graph we conclude de EDA section, having analized dataset characteristics as well ass relationships between varaibles. In the following part, we will build a baseline model against which we will compare some other tunned versions.","f5475f59":"### From the previous boxplot we can see that:\n* The dataset can have outliers, that will be further analysed. \n* Neither \"sex\" nor \"region\" has a very significant impact on \"charges\". Said that, we should further explore the rest of variables in the dataset. \n\n### In order to see the influence of age and bmi on charges, we will make a pairplot:","1ff9b9c0":"### I have used a simple pipeline that first apply OneHotEncoder to categorized variables and leave numerical ones the same. Afther such transformation, a simple linear regression is created and cross-validated. \n### As we can see, the baseline model predicts the medical charges with an R2 of 61%. \n\n### Now, I will try to fine-tune the model as well as trying different regressor to see if we can improve the previous result. ","9afd391b":"  #                          Tunning the baseline model","6bc02222":"#                            What determines your insurance fee?\n\n\n### In the provided dataset can be found several variables that could explain how much money patients are paying for their medical attendance. What we see is **what variables** under **which models** help predicting the explained variable along with the goodness of fit. "}}