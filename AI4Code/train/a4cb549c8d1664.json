{"cell_type":{"e3eff299":"code","ed8e7f6b":"code","3ef32370":"code","e438e993":"code","6b15b996":"code","5e3fa07b":"code","bf8a8f68":"code","36321680":"code","9dc1f047":"code","b882f5c6":"code","51b1dcdb":"code","17d69463":"code","05d44479":"code","d765afc8":"code","f1b6b43e":"code","07fdda21":"code","4db575c5":"code","c70ca294":"code","d2535c9d":"code","20a5183e":"code","3ebfb463":"code","29979578":"code","58ce5700":"code","d0c4dde8":"code","72cae078":"code","2f16ab52":"code","72273439":"code","a3263a40":"code","dd0081b3":"code","b3f087eb":"code","65a09fad":"code","40499e4a":"code","ec279409":"code","b3d4220d":"code","6c838ede":"code","b091a8d7":"code","820b3646":"code","9e02e283":"code","e3486f74":"code","48e39c83":"code","1d58dc00":"code","3393f71f":"code","cce96673":"code","3ee82955":"code","d6852982":"markdown","87c86840":"markdown","34bbe5b7":"markdown","7ef8645f":"markdown","143cf50e":"markdown","841c230c":"markdown","3bfe1934":"markdown","defbc2bd":"markdown","c39ae881":"markdown","8939f9d7":"markdown","c10aeb26":"markdown","9368dc57":"markdown","c68c87e2":"markdown","16c649e5":"markdown","8ad3008f":"markdown","15cbce91":"markdown","76d4a105":"markdown","411722b1":"markdown","e68a4930":"markdown","d3125ef7":"markdown","be8e7567":"markdown","b2706202":"markdown"},"source":{"e3eff299":"import gc\nimport re #regular expressions\nimport os\nimport time\nimport pickle \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom itertools import product\nfrom xgboost import XGBRegressor, plot_importance\nfrom matplotlib.pylab import rcParams\nfrom sklearn.preprocessing import LabelEncoder\n\nsns.set(style=\"darkgrid\")\nrcParams['figure.figsize'] = 12, 4","ed8e7f6b":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3ef32370":"items = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\ncats = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ntrain = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')","e438e993":"plt.figure(figsize=(10, 4))\nplt.xlim(-100, 3000)\nflierprops = dict(marker='o', markerfacecolor='green', markersize=6,\n                  linestyle='none', markeredgecolor='black') #style of outliers\nsns.boxplot(x=train.item_cnt_day, flierprops=flierprops)\n\nplt.figure(figsize=(10,4))\nplt.xlim(train.item_price.min(), train.item_price.max()*1.1)\nsns.boxplot(x=train.item_price, flierprops=flierprops)","6b15b996":"train = (\n    train\n    [\n        (train['item_price'] > 0) &\n        (train['item_price'] < 300000) &\n        (train['item_cnt_day'] < 1000)\n    ]\n    .reset_index(drop = True)\n)\n\ntrain.loc[train['item_cnt_day'] < 0, 'item_cnt_day'] = 0","5e3fa07b":"for i in [(0, 57), (1, 58), (10, 11)]:\n    train.loc[train['shop_id'] == i[0], 'shop_id'] = i[1]\n    test.loc[test['shop_id'] == i[0], 'shop_id'] = i[1]","bf8a8f68":"shops.loc[shops['shop_name'] == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"', 'shop_name'] = '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"'\nshops['city'] = shops.shop_name.str.split(' ').map(lambda x: x[0])\nshops['category'] = shops.shop_name.str.split(' ').map(lambda x: x[1])\nshops.loc[shops['city'] == '!\u042f\u043a\u0443\u0442\u0441\u043a', 'city'] = '\u042f\u043a\u0443\u0442\u0441\u043a'","36321680":"categories = []\nfor categ in shops['category'].unique():\n    if len(shops[shops['category'] == categ]) > 4:\n        categories.append(categ)\nshops['category'] = shops['category'].apply(lambda x: x if x in categories else 'other')","9dc1f047":"shops['shop_category'] = LabelEncoder().fit_transform(shops['category'])\nshops['shop_city'] = LabelEncoder().fit_transform(shops['city'])\nshops = shops[['shop_id', 'shop_category', 'shop_city']]\n#shops","b882f5c6":"cats['type_code'] = (\n    cats['item_category_name']\n    .apply(\n        lambda x: x.split(' ')[0]\n    )\n    .astype(str)\n)\ncats.loc[\n    (cats['type_code'] == '\u0418\u0433\u0440\u043e\u0432\u044b\u0435') |\n    (cats['type_code'] == '\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b'),\n    'category'\n] = '\u0418\u0433\u0440\u044b'\n#cats.head()","51b1dcdb":"categories = []\nfor categ in cats['type_code'].unique():\n    if len(cats[cats['type_code'] == categ]) > 4: \n        categories.append(categ)\ncats['type_code'] = cats['type_code'].apply(lambda x: x if x in categories else 'etc')","17d69463":"cats['type_code'] = LabelEncoder().fit_transform(cats['type_code'])\ncats['split'] = (\n    cats['item_category_name']\n    .apply(lambda x: x.split('-'))\n)\ncats['subtype'] = (\n    cats['split']\n    .apply(\n        lambda x: x[1].strip() if len(x) >= 2 else x[0].strip()\n    )\n)\ncats['subtype_code'] = LabelEncoder().fit_transform(cats['subtype'])\ncats = cats[['item_category_id', 'subtype_code', 'type_code']]","05d44479":"def name_correction(x):\n    x = x.lower() #lower case\n    x = x.partition('[')[0] # partition by square brackets\n    x = x.partition('(')[0] # partition by curly brackets\n    x = re.sub('\\W+', ' ', x) # remove special characters\n    x = x.replace('  ', ' ') # replace double spaces with single spaces\n    x = x.strip() # remove leading and trailing white space\n    return x","d765afc8":"# split item names by first bracket\nitems['name1'], items['name2'] = items['item_name'].str.split('[', 1).str\nitems['name1'], items['name3'] = items['item_name'].str.split('(', 1).str\n\n# replace special characters and turn to lower case\nitems['name2'] = items['name2'].str.replace('\\W+', ' ').str.lower()\nitems['name3'] = items['name3'].str.replace('\\W+', ' ').str.lower()\n\n# fill nulls with '0'\nitems = items.fillna('0')\n\nitems['item_name'] = items['item_name'].apply(lambda x: name_correction(x))\n\n# return all characters except the last if name 2 is not \"0\" - the closing bracket\nitems['name2'] = items['name2'].apply(lambda x: x[:-1] if x != '0' else '0')","f1b6b43e":"items['type'] = (\n    items['name2']\n    .apply(\n        lambda x: x[0:8] if x.split(' ')[0] == 'xbox' else x.split(' ')[0]\n    )\n)\n\nitems.loc[\n    (items['type'] == 'x360') |\n    (items['type'] == 'xbox360') |\n    (items['type'] == 'xbox 360'),\n    'type'\n] = 'xbox 360'\nitems.loc[items['type'] == '', 'type'] = 'mac'\nitems.type = (\n    items['type']\n    .apply(\n        lambda x: x.replace(' ', '')\n    )\n)\nitems.loc[\n    (items['type'] == 'pc' ) |\n    (items['type'] == 'p\u0441') |\n    (items['type'] == 'p\u0441'),\n    'type'\n] = 'p\u0441'\n\nitems.loc[items['type'] == '\u0440s3' , 'type'] = '\u0440s3'","07fdda21":"group_sum = (\n    items\n    .groupby('type')\n    .agg({'item_id': 'count'})\n    .reset_index()\n)\n\ndrop_cols = []\nfor categ in group_sum['type'].unique():\n    if group_sum.loc[(group_sum['type'] == categ), 'item_id'].values[0] <= 39:\n        drop_cols.append(categ)\n\nitems['name2'] = (\n    items['name2']\n    .apply(\n        lambda x: 'other' if x in drop_cols else x\n    )\n)\nitems = items.drop(['type'], axis=1)","4db575c5":"items['name2'] = LabelEncoder().fit_transform(items['name2'])\nitems['name3'] = LabelEncoder().fit_transform(items['name3'])\n\nitems.drop(['item_name', 'name1'], axis=1, inplace=True)\n#items.head()","c70ca294":"matrix = []\ncols  = ['date_block_num', 'shop_id', 'item_id']\nfor i in range(34):\n    sales = train[train['date_block_num'] == i]\n    matrix.append(\n        np.array(\n            list(product(\n                [i],\n                sales['shop_id'].unique(),\n                sales['item_id'].unique()\n            )),\n            dtype = np.int16\n        )\n    )\n\nmatrix = pd.DataFrame(np.vstack(matrix), columns=cols)\nmatrix = matrix.astype({\n    'date_block_num': np.int8, \n    'shop_id': np.int8, \n    'item_id': np.int16\n})\nmatrix.sort_values(cols, inplace=True)","d2535c9d":"# create revenue column\ntrain['revenue'] = train['item_cnt_day'] * train['item_price']","20a5183e":"group = (\n    train\n    .groupby(['date_block_num', 'shop_id', 'item_id'])\n    .agg({\n        'item_cnt_day': 'sum'\n    })\n)\ngroup.columns = ['item_cnt_month']\ngroup.reset_index(inplace=True)\nmatrix = pd.merge(matrix, group, on=cols, how='left')\nmatrix['item_cnt_month'] = (\n    matrix['item_cnt_month']\n    .fillna(0)\n    .astype(np.float16)\n)","3ebfb463":"#Create test set for 34th month.\ntest['date_block_num'] = 34\ntest = (\n    test\n    .astype({\n        'date_block_num': np.int8, \n        'shop_id': np.int8, \n        'item_id': np.int16\n    })\n)","29979578":"#Concatenate train and test\nmatrix = pd.concat(\n    [matrix, test.drop(['ID'], axis=1)],\n    ignore_index=True, sort=False, keys=cols\n)\nmatrix.fillna(0, inplace=True)","58ce5700":"#Add all our data categories to matrix\nmatrix = pd.merge(matrix, shops, on='shop_id', how='left')\nmatrix = pd.merge(matrix, items, on='item_id', how='left')\nmatrix = pd.merge(matrix, cats, on='item_category_id', how='left')\nmatrix = (\n    matrix\n    .astype({\n        'shop_city': np.int8,\n        'shop_category': np.int8,\n        'item_category_id': np.int8,\n        'subtype_code': np.int8,\n        'name2': np.int8,\n        'name3': np.int16,\n        'type_code': np.int8\n    })\n)","d0c4dde8":"# Define a lag feature function\ndef lag_feature(df, lags, cols):\n    for col in cols:\n        tmp = df[['date_block_num', 'shop_id', 'item_id', col]]\n        for i in lags:\n            shifted = tmp.copy()\n            shifted.columns = ['date_block_num', 'shop_id', 'item_id', col + \"_lag_\" + str(i)]\n            shifted['date_block_num'] = shifted['date_block_num'] + i\n            df = pd.merge(df, shifted, on=['date_block_num', 'shop_id', 'item_id'], how='left')\n    return df","72cae078":"#Add item_cnt_month lag features.\nmatrix = lag_feature(matrix, [1, 2, 3], ['item_cnt_month'])","2f16ab52":"#Add the previous month's average item_cnt.\ngroup = (\n    matrix\n    .groupby('date_block_num')\n    .agg({\n        'item_cnt_month' : 'mean'\n    })\n)\ngroup.columns = ['date_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on='date_block_num', how=\"left\")\nmatrix['date_avg_item_cnt'] = matrix['date_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_avg_item_cnt'])\nmatrix.drop(['date_avg_item_cnt'], axis=1, inplace=True)","72273439":"#Add lag values of item_cnt_month for month \/ item_id.\ngroup = (\n    matrix\n    .groupby(['date_block_num', 'item_id'])\n    .agg({\n        'item_cnt_month': 'mean'\n    })\n)\ngroup.columns = ['date_item_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id'], how='left')\nmatrix['date_item_avg_item_cnt'] = matrix['date_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1, 2, 3], ['date_item_avg_item_cnt'])\nmatrix.drop(['date_item_avg_item_cnt'], axis=1, inplace=True)","a3263a40":"#Add lag values for item_cnt_month for every month \/ shop combination.\ngroup = (\n    matrix\n    .groupby(['date_block_num', 'shop_id'])\n    .agg({\n        'item_cnt_month': 'mean'\n    })\n)\ngroup.columns = ['date_shop_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id'], how='left')\nmatrix['date_shop_avg_item_cnt'] = matrix['date_shop_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1, 2, 3], ['date_shop_avg_item_cnt'])\nmatrix.drop(['date_shop_avg_item_cnt'], axis=1, inplace=True)","dd0081b3":"#Add lag values for item_cnt_month for month\/shop\/item.\ngroup = (\n    matrix\n    .groupby(['date_block_num', 'shop_id', 'item_id'])\n    .agg({\n        'item_cnt_month': 'mean'\n    })\n)\ngroup.columns = ['date_shop_item_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'item_id'], how='left')\nmatrix['date_shop_item_avg_item_cnt'] = matrix['date_shop_item_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1, 2, 3], ['date_shop_item_avg_item_cnt'])\nmatrix.drop(['date_shop_item_avg_item_cnt'], axis=1, inplace=True)","b3f087eb":"#Add lag values for item_cnt_month for month\/shop\/item subtype.\ngroup = (\n    matrix\n    .groupby(['date_block_num', 'shop_id', 'subtype_code'])\n    .agg({\n        'item_cnt_month': 'mean'\n    })\n)\ngroup.columns = ['date_shop_subtype_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_id', 'subtype_code'], how='left')\nmatrix['date_shop_subtype_avg_item_cnt'] = matrix['date_shop_subtype_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_shop_subtype_avg_item_cnt'])\nmatrix.drop(['date_shop_subtype_avg_item_cnt'], axis=1, inplace=True)","65a09fad":"#Add lag values for item_cnt_month for month\/city.\ngroup = (\n    matrix\n    .groupby(['date_block_num', 'shop_city'])\n    .agg({\n        'item_cnt_month': 'mean'\n    })\n)\ngroup.columns = ['date_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'shop_city'], how='left')\nmatrix['date_city_avg_item_cnt'] = matrix['date_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_city_avg_item_cnt'])\nmatrix.drop(['date_city_avg_item_cnt'], axis=1, inplace=True)","40499e4a":"#Add lag values for item_cnt_month for month\/city\/item.\ngroup = (\n    matrix\n    .groupby(['date_block_num', 'item_id', 'shop_city'])\n    .agg({\n        'item_cnt_month': 'mean'\n    })\n)\ngroup.columns = ['date_item_city_avg_item_cnt']\ngroup.reset_index(inplace=True)\n\nmatrix = pd.merge(matrix, group, on=['date_block_num', 'item_id', 'shop_city'], how='left')\nmatrix['date_item_city_avg_item_cnt'] = matrix['date_item_city_avg_item_cnt'].astype(np.float16)\nmatrix = lag_feature(matrix, [1], ['date_item_city_avg_item_cnt'])\nmatrix.drop(['date_item_city_avg_item_cnt'], axis=1, inplace=True)","ec279409":"group = (\n    train\n    .groupby('item_id')\n    .agg({\n        'item_price': 'mean'\n    })\n)\ngroup.columns = ['item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = matrix.merge(group, on='item_id', how='left')\nmatrix['item_avg_item_price'] = matrix['item_avg_item_price'].astype(np.float16)\n\n\ngroup = (\n    train\n    .groupby(['date_block_num', 'item_id'])\n    .agg({\n        'item_price': 'mean'\n    })\n)\ngroup.columns = ['date_item_avg_item_price']\ngroup.reset_index(inplace=True)\n\nmatrix = matrix.merge(group, on=['date_block_num', 'item_id'], how='left')\nmatrix['date_item_avg_item_price'] = matrix['date_item_avg_item_price'].astype(np.float16)\nlags = [1, 2, 3]\nmatrix = lag_feature(matrix, lags, ['date_item_avg_item_price'])\n\nfor i in lags:\n    matrix['delta_price_lag_' + str(i)] = (\n        matrix['date_item_avg_item_price_lag_' + str(i)] -\\\n        matrix['item_avg_item_price']\n    ) \/ matrix['item_avg_item_price']\n\ndef select_trends(row) :\n    for i in lags:\n        if row['delta_price_lag_' + str(i)]:\n            return row['delta_price_lag_' + str(i)]\n    return 0\n\nmatrix['delta_price_lag_'] = matrix.apply(select_trends, axis=1)\nmatrix['delta_price_lag_'] = matrix['delta_price_lag_'].astype(np.float16)\nmatrix['delta_price_lag_'].fillna(0, inplace=True)\n\nfeatures_to_drop = ['item_avg_item_price', 'date_item_avg_item_price']\nfor i in lags:\n    features_to_drop.append('date_item_avg_item_price_lag_' + str(i))\n    features_to_drop.append('delta_price_lag_' + str(i))\nmatrix.drop(features_to_drop, axis=1, inplace=True)","b3d4220d":"group = (\n    train\n    .groupby(['date_block_num', 'shop_id'])\n    .agg({\n        'revenue': 'sum'\n    })\n)\ngroup.columns = ['date_shop_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = matrix.merge(group, on=['date_block_num', 'shop_id'], how='left')\nmatrix['date_shop_revenue'] = matrix['date_shop_revenue'].astype(np.float32)\n\ngroup = (\n    group\n    .groupby('shop_id')\n    .agg({\n        'date_block_num': 'mean'\n    })\n)\ngroup.columns = ['shop_avg_revenue']\ngroup.reset_index(inplace=True)\n\nmatrix = matrix.merge(group, on='shop_id', how='left')\nmatrix['shop_avg_revenue'] = matrix['shop_avg_revenue'].astype(np.float32)\nmatrix['delta_revenue'] = (\n    matrix['date_shop_revenue'] - matrix['shop_avg_revenue']\n) \/ matrix['shop_avg_revenue']\nmatrix['delta_revenue'] = matrix['delta_revenue'].astype(np.float32)\n\nmatrix = lag_feature(matrix, [1], ['delta_revenue'])\nmatrix['delta_revenue_lag_1'] = matrix['delta_revenue_lag_1'].astype(np.float32)\nmatrix.drop(\n    ['date_shop_revenue', 'shop_avg_revenue', 'delta_revenue'],\n    axis=1, inplace=True\n)","6c838ede":"#Add month and number of days in each month to matrix\nmatrix['month'] = matrix['date_block_num'] % 12\ndays = pd.Series([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31])\nmatrix['days'] = matrix['month'].map(days).astype(np.int8)","b091a8d7":"#Add the month of each shop and item first sale.\nmatrix['item_shop_first_sale'] = (\n    matrix['date_block_num'] - matrix.groupby(['item_id', 'shop_id'])['date_block_num'].transform('min')\n)\nmatrix['item_first_sale'] = (\n    matrix['date_block_num'] - matrix.groupby(['item_id'])['date_block_num'].transform('min')\n)","820b3646":"#Delete first three months from matrix. They don't have lag values.\nmatrix = matrix[matrix['date_block_num'] >= 4]\nmatrix.head().T","9e02e283":"data = matrix.copy()","e3486f74":"data[data['date_block_num'] == 34].shape","48e39c83":"#Use month 34 as validation for training.\nX_train = data[data.date_block_num <= 32].drop(['item_cnt_month'], axis=1)\nY_train = data[data.date_block_num <= 32]['item_cnt_month']\nX_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\nY_valid = data[data.date_block_num == 33]['item_cnt_month']\nX_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)","1d58dc00":"Y_train = Y_train.clip(0, 20)\nY_valid = Y_valid.clip(0, 20)","3393f71f":"model = XGBRegressor(\n    max_depth=10,\n    n_estimators=1000,\n    min_child_weight=0.5, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.1,\n    seed=42\n)\n\nmodel.fit(\n    X_train, \n    Y_train, \n    eval_metric='rmse',\n    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n    verbose=True, \n    early_stopping_rounds=20\n)","cce96673":"Y_pred = model.predict(X_valid).clip(0, 20)\nY_test = model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    'ID': test.index, \n    'item_cnt_month': Y_test\n})\nsubmission.to_csv('xgb_submission.csv', index=False)","3ee82955":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1, 1, figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nplot_features(model, (10, 14))","d6852982":"Change some shop names and add 'city' and 'category' columns to dataframe.","87c86840":"Clean item names","34bbe5b7":"Clean item type","7ef8645f":"### xgboost","143cf50e":"### Outliers","841c230c":"### Introduction","3bfe1934":"### Item Categories Data Cleaning","defbc2bd":"Several entries looks like the data for same stores but for different period.","c39ae881":"## 2. Data preparation & Feature Enginering","8939f9d7":"Feature Enginering. Add lags to matrix.","c10aeb26":"The goal of the competition to predict future sales of items in set of '1C' company's stores for one month given historical data.","9368dc57":"Use only large enough categories","c68c87e2":"Remove outliers, chosing thresholds visually - the items sold more than 1000 in one day, and the item with price higher than 300 thounds.","16c649e5":"* Add average item price to matix. \n* Add lag values of item price per month.\n* Add delta price values - how current month average pirce relates to global average.","8ad3008f":"## 1. Data Cleaning","15cbce91":"Also remove rows with negative price value and make zero negative item_cnt_day.","76d4a105":"### Item Data Cleaning","411722b1":"* Add total shop revenue per month to matrix. \n* Add lag values of revenue per month.\n* Add delta revenue values - how current month revenue relates to global average. ","e68a4930":"Create matrix format dataframe for every month, shop and item id to aggregate data to monthly data. 'Item_cnt_day' summed up to ' item_cnt_month'.","d3125ef7":"### Shop Dataframe Cleaning","be8e7567":"#### Libraries and data","b2706202":"## 3. Modelling"}}