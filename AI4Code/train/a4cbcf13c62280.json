{"cell_type":{"81ea1094":"code","0616ce93":"code","7ba9561d":"code","c32601b9":"code","b5a0489e":"code","7d795603":"code","525ab547":"code","9c07cf17":"code","a5cf582d":"code","db9cc205":"code","56dbf489":"code","e87a5e84":"code","fa88cc53":"code","40350d13":"code","0eb02af3":"code","0a8c234f":"code","6bd2fe40":"code","7f92567f":"code","a7d6bbdc":"code","6340f20c":"code","27fc6c09":"code","24281cd4":"code","13aba08d":"code","eca7d5fc":"code","23c1e8ae":"code","d9363c93":"code","71376e94":"code","36b73fe1":"code","c89b4df9":"code","e6e996d3":"code","0b23066d":"code","277b290a":"code","b78f57da":"code","14252f69":"code","a94a042e":"code","9d746c7a":"code","188cf55a":"code","1b5e7fdf":"code","99c2610b":"code","d3b982ff":"code","93d30d60":"code","e69854d2":"code","55bf68a9":"code","246ac84b":"code","fc4d6ccb":"code","b94c36d2":"code","37ad3518":"markdown","e0f0281c":"markdown","dafb9de3":"markdown","fdf75eac":"markdown","4a37172a":"markdown","aa96446e":"markdown","09abe471":"markdown","9ff3c8b4":"markdown","ea3301ac":"markdown","b690c1bb":"markdown","ad1ec86f":"markdown","13197e8a":"markdown","4a915cd1":"markdown","7aa61e4f":"markdown","c0b6efd9":"markdown","568e8593":"markdown","ae6f3c06":"markdown","ad3583e8":"markdown","9266b94f":"markdown","3be67ba6":"markdown","12173e3d":"markdown","ab5092e8":"markdown","2bd717a4":"markdown","59f6f557":"markdown","62d69704":"markdown","04520ca9":"markdown","5a4fd68a":"markdown","855fc6f5":"markdown","56f95edf":"markdown","97327795":"markdown"},"source":{"81ea1094":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\n%matplotlib inline","0616ce93":"from xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\n\nimport optuna","7ba9561d":"from sklearn.metrics import log_loss\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom mlxtend.preprocessing import minmax_scaling\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, KFold, LeaveOneGroupOut\n","c32601b9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        ","b5a0489e":"DF1 = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv')\n\nDF2 = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv')\n\nSAM = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","7d795603":"MV1 = DF1.isnull().sum()\nMV2 = DF2.isnull().sum()\n\nprint(f'Missing Value 1:  {MV1[MV1 > 0]}')\nprint(f'Missing Value 2:  {MV2[MV2 > 0]}')","525ab547":"display(DF1, DF2)\n\n# display(DF1.describe().transpose())\n# display(DF2.describe().transpose())","9c07cf17":"print('=' * 40)\nDF1.info(memory_usage='deep')\nprint('=' * 40)\nDF2.info(memory_usage='deep')\nprint('=' * 40)","a5cf582d":"data1 = DF1.copy()\ndata2 = DF2.copy()\n\ncolumns = data2.columns[1:]\ndisplay(columns)","db9cc205":"data1['loss'].value_counts().plot(figsize=(16, 8), kind='bar')","56dbf489":"data1['loss'].value_counts().plot(figsize=(10, 10), kind='pie')\n\ndata1['loss'].value_counts(normalize=True)","e87a5e84":"X = data1.drop(columns = ['id','loss'])\ndisplay(X)","fa88cc53":"y = data1.loss\ndisplay(y)","40350d13":"display(y.min() , y.max())","0eb02af3":"XX = data2.drop(columns = ['id'])\ndisplay(XX)","0a8c234f":"yc = y.copy()\n\nyc = np.clip(yc, 0, 1)\ndisplay(yc)","6bd2fe40":"display(yc.min() , yc.max())","7f92567f":"yc.value_counts().plot(figsize=(4, 4), kind='bar')","a7d6bbdc":"yc.value_counts().plot(figsize=(5, 5), kind='pie')\n\nyc.value_counts(normalize=True)","6340f20c":"train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.50, random_state=123) \n\nval_X.to_csv(\"val_X.csv\",index=False)\nval_y.to_csv(\"val_y.csv\",index=False)","27fc6c09":"train_X, val_X, train_yc, val_yc = train_test_split(X, yc, test_size=0.50, random_state=123)\n\nval_yc.to_csv(\"val_yc.csv\",index=False)","24281cd4":"X_scaled = minmax_scaling(X, columns=X.columns)\n# display(X_scaled)","13aba08d":"XX_scaled = minmax_scaling(XX, columns=XX.columns)\n# display(XX_scaled)","eca7d5fc":"model1v = XGBRegressor(max_depth=7,\n                       n_estimators=2500,\n                       learning_rate=0.008,\n                       subsample=0.84,\n                       booster= 'gbtree',\n                       tree_method= 'gpu_hist',\n                       colsample_bytree= 0.70,\n                       reg_lambda= 5,\n                       reg_alpha= 32,\n                       n_jobs= 4,  \n                       alpha=0.5,\n                       random_state=123)                                  \n    \nmodel1v.fit(train_X, train_y)\noof_pred1 = model1v.predict(val_X)\noof_pred1 = np.clip(oof_pred1, y.min(), y.max())\n\nprint(30 * '=')\nprint(f'Mean Error: {np.sqrt(mean_squared_error(val_y, oof_pred1))}')\nprint(30 * '=')","23c1e8ae":"model1v.feature_importances_","d9363c93":"model1 = XGBRegressor(max_depth=7,                     \n                      n_estimators=2500,\n                      learning_rate=0.008,\n                      subsample=0.84,\n                      booster= 'gbtree',\n                      tree_method= 'gpu_hist',\n                      colsample_bytree= 0.70,\n                      reg_lambda= 5,\n                      reg_alpha= 32,\n                      n_jobs= 4,            \n                      alpha=0.5,\n                      random_state=123)   \n\nmodel1.fit(X, y)\npred1 = model1.predict(XX)\npred1 = np.clip(pred1, y.min(), y.max())\ndisplay(pred1, pred1.shape) ","71376e94":"sub1 = SAM.copy()\n\nsub1.iloc[:, 1] = pred1.data\ndisplay(sub1)","36b73fe1":"sub1.to_csv(\"submission_xgb.csv\",index=False)\n# Public Score: 7.87965","c89b4df9":"model2v = CatBoostRegressor(depth=6,\n                            iterations=1600,\n                            learning_rate=0.024,\n                            l2_leaf_reg=20,\n                            random_strength=1.5,\n                            grow_policy='Depthwise',\n                            leaf_estimation_method='Newton', \n                            bootstrap_type='Bernoulli',\n                            thread_count=4,\n                            verbose=False,\n                            loss_function='RMSE',\n                            eval_metric='RMSE',\n                            od_type='Iter',\n                            task_type='GPU',\n                            early_stopping_rounds=500,\n                            random_state=123)    \n\nmodel2v.fit(train_X, train_y, verbose=200)\noof_pred2 = model2v.predict(val_X)\noof_pred2 = np.clip(oof_pred2, y.min(), y.max())\n\nprint(30 * '=')\nprint(f'Mean Error: {np.sqrt(mean_squared_error(val_y, oof_pred2))}')\nprint(30 * '=')","e6e996d3":"model2v.feature_importances_","0b23066d":"model2 = CatBoostRegressor(depth=6,                     \n                           iterations=1600,\n                           learning_rate=0.024,\n                           l2_leaf_reg=20,\n                           random_strength=1.5,\n                           grow_policy='Depthwise',\n                           leaf_estimation_method='Newton', \n                           bootstrap_type='Bernoulli',\n                           thread_count=4,\n                           verbose=False,\n                           loss_function='RMSE',\n                           eval_metric='RMSE',\n                           od_type='Iter',\n                           task_type='GPU',\n                           early_stopping_rounds=500,\n                           random_state=123)    \n\nmodel2.fit(X, y)\npred2 = model2.predict(XX)\npred2 = np.clip(pred2, y.min(), y.max())\ndisplay(pred2, pred2.shape) ","277b290a":"sub2 = SAM.copy()\n\nsub2.iloc[:, 1] = pred2.data\ndisplay(sub2)","b78f57da":"sub2.to_csv(\"submission_catboost.csv\",index=False)\n# Public Score: 7.87995","14252f69":"a1 = model1v.feature_importances_\na2 = model2v.feature_importances_\n\naxis_x  = X.columns.values\naxis_y1 = minmax_scaling(a1, columns=[0])\naxis_y2 = minmax_scaling(a2, columns=[0])\n\nplt.style.use('seaborn-whitegrid') \nplt.figure(figsize=(16, 6), facecolor='lightgray')\nplt.title(f'\\nX G B o o s t  &  C a t B o o s t\\n\\nF e a t u r e   I m p o r t a n c e s\\n', fontsize=14)  \n\nplt.scatter(axis_x, axis_y1, s=20, label='XGBoost') \nplt.scatter(axis_x, axis_y2, s=20, label='CatBoost')\n\nplt.legend(fontsize=12, loc=2)\nplt.show()","a94a042e":"def best_blend(coeff):    \n    oof_pred = (oof_pred1 * coeff) + (oof_pred2 * (1.0 - coeff))   \n    mae = np.sqrt(mean_squared_error(val_y, oof_pred)) \n    return mae\n\nresults = {}\nfor i in range(0, 11):       \n    results[0.1 * i] = best_blend(0.1 * i)  \n    \nplt.plot(list(results.keys()), list(results.values()))\nplt.show()   ","9d746c7a":"pred = (pred1 * 0.50) + (pred2 * (1.0 - 0.50)) ","188cf55a":"sub3 = SAM.copy()\n\nsub3.iloc[:, 1] = pred.data\ndisplay(sub3)","1b5e7fdf":"sub3.to_csv(\"submission3.csv\",index=False)\n# Public Score: 7.87752","99c2610b":"path0 = '..\/input\/tps8-787043\/submission.csv' \n\nsub787043 = pd.read_csv(path0)","d3b982ff":"path1 = '..\/input\/tps8-786780\/voting.csv' \n\nsub786780 = pd.read_csv(path1)","93d30d60":"path2 = '..\/input\/tps8-786132\/submit.csv' \n\nsub786132 = pd.read_csv(path2)","e69854d2":"path3 = '..\/input\/tps8-785626\/LightAutoML_utilized_submission.csv' \n\nsub785626 = pd.read_csv(path3)","55bf68a9":"def ensembling(main, support, coeff): \n    \n    suba  = main.copy() \n    subav = suba.values\n       \n    subb  = support.copy()\n    subbv = subb.values    \n           \n    ense  = main.copy()    \n    ensev = ense.values  \n \n    for i in range (len(main)):\n        \n        pera = subav[i, 1]\n        perb = subbv[i, 1]\n        per = (pera * coeff) + (perb * (1.0 - coeff))   \n        ensev[i, 1] = per\n        \n    ense.iloc[:, 1] = ensev[:, 1]  \n    \n    ###############################    \n    X  = suba.iloc[:, 1]\n    Y1 = subb.iloc[:, 1]\n    Y2 = ense.iloc[:, 1]\n    \n    plt.style.use('seaborn-whitegrid') \n    plt.figure(figsize=(9, 9), facecolor='lightgray')\n    plt.title(f'\\nE N S E M B L I N G\\n')   \n      \n    plt.scatter(X, Y1, s=1.5, label='Support')    \n    plt.scatter(X, Y2, s=1.5, label='Generated')\n    plt.scatter(X, X , s=0.1, label='Main(X=Y)')\n    \n    plt.legend(fontsize=12, loc=2)\n    #plt.savefig('Ensembling_1.png')\n    plt.show()     \n    ###############################   \n    ense.iloc[:, 1] = ense.iloc[:, 1].astype(float)\n    hist_data = [subb.iloc[:, 1], ense.iloc[:, 1], suba.iloc[:, 1]] \n    group_labels = ['Support', 'Ensembling', 'Main']\n    \n    fig = ff.create_distplot(hist_data, group_labels, bin_size=.2, show_hist=False, show_rug=False)\n    fig.show()   \n    ###############################       \n    \n    return ense      \n","246ac84b":"sub4 = ensembling(sub787043, sub3, 0.60)\n\nsub5 = ensembling(sub786780, sub4, 0.60)\n\nsub6 = ensembling(sub786132, sub5, 0.50)\n\nsub7 = ensembling(sub785626, sub6, 0.70)","fc4d6ccb":"display(sub7)","b94c36d2":"sub4.to_csv(\"submission4.csv\",index=False)\nsub5.to_csv(\"submission5.csv\",index=False)\nsub6.to_csv(\"submission6.csv\",index=False)\n\nsub7.to_csv(\"submission_final.csv\",index=False)\n!ls","37ad3518":"## CatBoostRegressor\n\n### Validation Model - 2","e0f0281c":"Thanks to: @oxzplvifi https:\/\/www.kaggle.com\/oxzplvifi\/tabular-denoising-residual-network","dafb9de3":"Thanks to: @tensorchoko https:\/\/www.kaggle.com\/tensorchoko\/tabular-aug-2021-lightgbm","fdf75eac":"<div class=\"alert alert-success\">  \n<\/div>","4a37172a":"Thanks to: @alexryzhkov https:\/\/www.kaggle.com\/alexryzhkov\/lightautoml-classifier-regressor-mix","aa96446e":"## Final Result","09abe471":"## Best Blend","9ff3c8b4":"## Model - 2","ea3301ac":"<div class=\"alert alert-success\">  \n<\/div>","b690c1bb":"## Scaling","ad1ec86f":"<div class=\"alert alert-success\">  \n<\/div>","13197e8a":"<div class=\"alert alert-success\">  \n<\/div>","4a915cd1":"## Data Set","7aa61e4f":"<div class=\"alert alert-success\">  \n<\/div>","c0b6efd9":"<div class=\"alert alert-success\">  \n<\/div>","568e8593":"<div class=\"alert alert-success\">  \n<\/div>","ae6f3c06":"Thanks to: @dmitryuarov https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm","ad3583e8":"<div class=\"alert alert-success\">  \n<\/div>","9266b94f":"<div class=\"alert alert-success\">  \n<\/div>","3be67ba6":"<div class=\"alert alert-success\">  \n<\/div>","12173e3d":"<div class=\"alert alert-success\">  \n<\/div>","ab5092e8":"<div>\n    <h1 align=\"center\">XGBoost & CatBoost<\/h1>    \n    <h1 align=\"center\">Tabular Playground Series - Aug 2021<\/h1> \n    <h4 align=\"center\">By: Somayyeh Gholami & Mehran Kazeminia<\/h4>\n<\/div>","2bd717a4":"<div class=\"alert alert-success\">  \n<\/div>","59f6f557":"<div class=\"alert alert-success\">\n    <h1 align=\"center\">If you find this work useful, please don't forget upvoting :)<\/h1>\n<\/div>","62d69704":"<div class=\"alert alert-success\">  \n<\/div>","04520ca9":"<div class=\"alert alert-success\">  \n<\/div>","5a4fd68a":"## Feature Importances","855fc6f5":"## Model - 1","56f95edf":"## XGBRegressor\n\n### Validation Model - 1","97327795":"## Split"}}