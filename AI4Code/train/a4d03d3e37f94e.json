{"cell_type":{"d9755389":"code","77c31075":"code","ce0d10d5":"code","c8ca3844":"code","909c701a":"code","c51b745a":"code","fd964321":"code","5dac448e":"code","b5b329d5":"code","14d83a28":"code","96774075":"code","4d7bac4f":"code","04092b59":"code","4e2ab997":"code","f9013582":"code","90a6d5a4":"code","d7cce0fa":"code","97c39878":"code","b7b554b1":"code","67ab8bbe":"code","20511ea3":"code","b45cd433":"code","063ecd97":"code","3e75649c":"code","a471918b":"code","4e3517a8":"code","38c35435":"code","c59ea2b3":"code","bee818d2":"code","2ecd4b55":"code","6b542dd1":"code","101a9eb3":"code","bc6169b7":"code","5cf1fa83":"code","ca6110d8":"code","315fa23e":"code","721a47dd":"code","3f6f7779":"code","abc627c1":"code","ec24012a":"code","0f07a28a":"code","7f15ec12":"code","be0ef83b":"code","54a1deaf":"code","4889ea3b":"code","5d715f35":"code","f4ca12fe":"code","11630f9e":"code","29fe950f":"code","b84ff463":"code","ddbb716a":"code","0aa69244":"code","385c3c2e":"code","3071aba4":"code","cd5cfd89":"code","ee9ec15e":"code","cb0f2b1e":"code","bf53e847":"code","1edcd604":"code","82f1f51b":"code","acc6fdec":"code","5799c7a1":"code","4730f74e":"code","fbb757dc":"code","4f14407b":"code","48c972b1":"code","a9f756ca":"code","ccdb83d3":"code","5ac25e90":"code","c8ff25cb":"code","bdaa5e84":"code","43b09be4":"code","266bc463":"code","d95dcbab":"code","9a8ce350":"code","a04a0469":"code","468b949d":"code","7151a431":"code","d14eaa5c":"code","c5db57d7":"code","6e724d11":"code","e4c16640":"code","dbe58d1f":"code","90f0b6e8":"code","73248e23":"code","58f37c30":"code","4562519d":"code","c670ded1":"code","b518a441":"markdown","c4106045":"markdown","d1e16d86":"markdown","7089b18a":"markdown","97eaea06":"markdown","400d649d":"markdown","c1d0236a":"markdown","e5232007":"markdown","42f8f0bd":"markdown","d8c75234":"markdown","809b99ec":"markdown","342aa6d0":"markdown","a40826bb":"markdown","1382af1b":"markdown","9ff67678":"markdown","f136679b":"markdown","a19c9dce":"markdown","398da2bb":"markdown","26e0ebde":"markdown","3849ff61":"markdown","d7826a5e":"markdown","92e6a79c":"markdown","7f884197":"markdown","01a54841":"markdown","f4a1f25a":"markdown","54a3c4d2":"markdown","8915334e":"markdown","1db938e8":"markdown","6f03a8ea":"markdown","dedfe0c3":"markdown","f0a2fa9a":"markdown","7ac6bd0a":"markdown","a909cc6f":"markdown","bd712f42":"markdown","0719e9ce":"markdown","e7aa4151":"markdown","454ac474":"markdown","b44f2d18":"markdown","e01ce74b":"markdown","b523733b":"markdown","25e03683":"markdown","14bf863c":"markdown","3e173bd1":"markdown","ccd6975b":"markdown","4c9b6379":"markdown","1dfc091f":"markdown","64fe63fd":"markdown","5de728fc":"markdown","a3d8b159":"markdown"},"source":{"d9755389":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","77c31075":"%%time\ntrain = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')","ce0d10d5":"train.head()","c8ca3844":"test.head()","909c701a":"def num_of_words(df):\n    df['word_count'] = df['tweet'].apply(lambda x : len(str(x).split(\" \")))\n    print(df[['tweet','word_count']].head())","c51b745a":"num_of_words(train)","fd964321":"num_of_words(test)","5dac448e":"def num_of_chars(df):\n    df['char_count'] = df['tweet'].str.len() ## this also includes spaces\n    print(df[['tweet','char_count']].head())","b5b329d5":"num_of_chars(train)","14d83a28":"num_of_chars(test)","96774075":"def avg_word(sentence):\n    words = sentence.split()    \n    return (sum(len(word) for word in words)\/len(words))","4d7bac4f":"def avg_word_length(df):\n    df['avg_word'] = df['tweet'].apply(lambda x: avg_word(x))\n    print(df[['tweet','avg_word']].head())","04092b59":"avg_word_length(train)","4e2ab997":"avg_word_length(test)","f9013582":"import nltk\nfrom nltk.corpus import stopwords\nset(stopwords.words('english'))","90a6d5a4":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","d7cce0fa":"def stop_words(df):\n    df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n    print(df[['tweet','stopwords']].head())","97c39878":"stop_words(train)","b7b554b1":"stop_words(test)","67ab8bbe":"def hash_tags(df):\n    df['hashtags'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\n    print(df[['tweet','hashtags']].head())","20511ea3":"hash_tags(train)","b45cd433":"hash_tags(test)","063ecd97":"def num_numerics(df):\n    df['numerics'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\n    print(df[['tweet','numerics']].head())","3e75649c":"num_numerics(train)","a471918b":"num_numerics(test)","4e3517a8":"def num_uppercase(df):\n    df['upper_case'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\n    print(df[['tweet','upper_case']].head())","38c35435":"num_uppercase(train)","c59ea2b3":"num_uppercase(test)","bee818d2":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n          'This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',\n         ]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())","2ecd4b55":"print(X.toarray())","6b542dd1":"vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\nX2 = vectorizer2.fit_transform(corpus)\nprint(vectorizer2.get_feature_names())","101a9eb3":"print(X2.toarray())","bc6169b7":"from sklearn.feature_extraction.text import HashingVectorizer\ncorpus = [\n          'This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',\n         ]\nvectorizer = HashingVectorizer(n_features=2**4)\nX = vectorizer.fit_transform(corpus)\nprint(X.shape)","5cf1fa83":"def lower_case(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    print(df['tweet'].head())","ca6110d8":"lower_case(train)","315fa23e":"lower_case(test)","721a47dd":"def punctuation_removal(df):\n    df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n    print(df['tweet'].head())","3f6f7779":"punctuation_removal(train)","abc627c1":"punctuation_removal(test)","ec24012a":"from nltk.corpus import stopwords\nstop = stopwords.words('english')","0f07a28a":"def stop_words_removal(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n    print(df['tweet'].head())","7f15ec12":"stop_words_removal(train)","be0ef83b":"stop_words_removal(test)","54a1deaf":"freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\nfreq","4889ea3b":"freq = list(freq.index)","5d715f35":"def frequent_words_removal(df):    \n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    print(df['tweet'].head())","f4ca12fe":"frequent_words_removal(train)","11630f9e":"frequent_words_removal(test)","29fe950f":"freq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\nfreq","b84ff463":"freq = list(freq.index)","ddbb716a":"def rare_words_removal(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    print(df['tweet'].head())","0aa69244":"rare_words_removal(train)","385c3c2e":"rare_words_removal(test)","3071aba4":"from textblob import TextBlob","cd5cfd89":"def spell_correction(df):\n    return df['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))","ee9ec15e":"spell_correction(train)","cb0f2b1e":"spell_correction(test)","bf53e847":"def tokens(df):\n    return TextBlob(df['tweet'][1]).words","1edcd604":"tokens(train)","82f1f51b":"tokens(test)","acc6fdec":"from nltk.stem import PorterStemmer\nst = PorterStemmer()","5799c7a1":"def stemming(df):\n    return df['tweet'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","4730f74e":"stemming(train)","fbb757dc":"stemming(test)","4f14407b":"from textblob import Word","48c972b1":"def lemmatization(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n    print(df['tweet'].head())","a9f756ca":"lemmatization(train)","ccdb83d3":"lemmatization(test)","5ac25e90":"from textblob import TextBlob","c8ff25cb":"def combination_of_words(df):\n    return (TextBlob(df['tweet'][0]).ngrams(2))","bdaa5e84":"combination_of_words(train)","43b09be4":"combination_of_words(test)","266bc463":"def term_frequency(df):\n    tf1 = (df['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\n    tf1.columns = ['words','tf']\n    return tf1.head()","d95dcbab":"term_frequency(train)","9a8ce350":"term_frequency(test)","a04a0469":"tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1.head()","468b949d":"tf2 = (test['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf2.columns = ['words','tf']\ntf2.head()","7151a431":"tf1 = (train['tweet'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']","d14eaa5c":"for i,word in enumerate(tf1['words']):\n    tf1.loc[i, 'idf'] = np.log(train.shape[0]\/(len(train[train['tweet'].str.contains(word)])))","c5db57d7":"tf1['tfidf'] = tf1['tf'] * tf1['idf']\ntf1","6e724d11":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n stop_words= 'english',ngram_range=(1,1))\ntrain_vect = tfidf.fit_transform(train['tweet'])\ntrain_vect","e4c16640":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ntrain_bow = bow.fit_transform(train['tweet'])\ntrain_bow","dbe58d1f":"def polarity_subjectivity(df):\n    return df['tweet'][:5].apply(lambda x: TextBlob(x).sentiment)","90f0b6e8":"polarity_subjectivity(train)","73248e23":"polarity_subjectivity(test)","58f37c30":"def sentiment_analysis(df):\n    df['sentiment'] = df['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n    return df[['tweet','sentiment']].head()","4562519d":"sentiment_analysis(train)","c670ded1":"sentiment_analysis(test)","b518a441":"## **2.5 Number of special characters** <a class=\"anchor\" id=\"2.5\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- One more interesting feature which we can extract from a tweet is to calculate the number of hashtags in it. It also helps in extracting extra information from our text data.\n\n- Here, we make use of the `starts with` function because hashtags always appear at the beginning of a word.","c4106045":"## **3.7 Rare Words Removal**  <a class=\"anchor\" id=\"3.7\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Now, we will remove rarely occurring words from the text. \n- Because they\u2019re so rare, the association between them and other words is dominated by noise. \n- We can replace rare words with a more general form and then this will have higher counts.","d1e16d86":"<a class=\"anchor\" id=\"0\"><\/a>\n# **A Beginners Guide to dealing with Text Data**\n\n\nHello friends,\n\n\nIn this notebook, we will discuss several common ways to deal with text data. We will discuss different feature extraction methods. We will start with some basic techniques which will lead into advanced Natural Language Processing techniques. We will also learn about pre-processing of the text data in order to extract better features from clean data.\n\nSo, let's dive in.","7089b18a":"## **3.10 Stemming** <a class=\"anchor\" id=\"3.10\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- [Stemming](https:\/\/www.geeksforgeeks.org\/introduction-to-stemming\/) refers to the removal of suffices, like \u201cing\u201d, \u201cly\u201d, \u201cs\u201d, etc. by a simple rule-based approach. \n\n- So, stemming takes a word and refers it back to its base or root form. **Stems**, **Stemming**, **Stemmed** and **Stemtization** are all based on the single word **stem**.\n\n- For this purpose, we will use *PorterStemmer* from the NLTK library.","97eaea06":"## **4.2 Term Frequency**  <a class=\"anchor\" id=\"4.2\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Term frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n\n- Therefore, we can generalize term frequency as:\n\n**TF = (Number of times term T appears in the particular row) \/ (number of terms in that row)**\n\n\n- We will create a Term-Frequency table of a tweet as follows-","400d649d":"## **2.6 Number of numerics** <a class=\"anchor\" id=\"2.6\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Just like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It is a useful feature that should be run while doing similar exercises. For example - ","c1d0236a":"# **3. Basic Text Processing** <a class=\"anchor\" id=\"3\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- So far, we have learned how to extract basic features from text data. \n\n- Now, we move onto text and feature extraction. \n\n- Our first step should be to clean the data in order to obtain better features. \n\n- We will achieve this by doing some of the basic pre-processing steps on our training data.\n\n- So, let\u2019s get into it.\n","e5232007":"## **4.6 Sentiment Analysis**  <a class=\"anchor\" id=\"4.6\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- Now we come to our problem which was to detect the sentiment of the tweet. So, before applying any ML\/DL models (which can have a separate feature detecting the sentiment using the textblob library).\n\n- We will check the sentiment of the first few tweets as follows -","42f8f0bd":"- We can see that the TF-IDF has penalized words like \u2018don\u2019t\u2019, \u2018can\u2019t\u2019, and \u2018use\u2019 because they are commonly occurring words. However, it has given a high weight to \u201cdisappointed\u201d since that will be very useful in determining the sentiment of the tweet.\n\n- We don\u2019t have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:","d8c75234":"The more the value of IDF, the more unique is the word.","809b99ec":"We have discussed several common ways of dealing with text data in this notebook. These methods will give us a basic understanding of how to deal with text data in predictive modeling. ","342aa6d0":"# **1. Introduction to Natural Language Processing** <a class=\"anchor\" id=\"1\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- In a data scientist's journey, we will often come across a term **Natural Language Processing (NLP)**. But, have you ever wondered what do we mean by Natural Language Processing. In short, NLP is the art of understanding the meaning and influence of words.\n\n\n- In terms of wikipedia - \n\n\n[Natural Language Processing](https:\/\/en.wikipedia.org\/wiki\/Natural_language_processing) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n\nChallenges in natural language processing frequently involve [speech recognition](https:\/\/en.wikipedia.org\/wiki\/Speech_recognition), [natural language understanding](https:\/\/en.wikipedia.org\/wiki\/Natural-language_understanding) and [natural language generation](https:\/\/en.wikipedia.org\/wiki\/Natural-language_generation).","a40826bb":"<a class=\"anchor\" id=\"0.1\"><\/a>\n# **Table of Contents** \n\n1.\t[Introduction to Natural Language Processing](#1)\n2.\t[Basic feature extraction using text data](#2)\n   - 2.1 [Count number of words](#2.1)\n   - 2.2 [Count number of characters](#2.2)\n   - 2.3 [Average word length](#2.3)\n   - 2.4 [Number of stopwords](#2.4)\n   - 2.5 [Number of special characters](#2.5)\n   - 2.6 [Number of numerics](#2.6)\n   - 2.7 [Number of uppercase words](#2.7)\n3.\t[Basic Text Pre-processing of text data](#3)\n   - 3.1 [CountVectorization](#3.1)\n   - 3.2 [HashingVectorizer](#3.2)\n   - 3.3 [Lower casing](#3.3)\n   - 3.4 [Punctuation removal](#3.4)\n   - 3.5 [Stopwords removal](#3.5)\n   - 3.6 [Frequent words removal](#3.6)\n   - 3.7 [Rare words removal](#3.7)\n   - 3.8 [Spelling correction](#3.8)\n   - 3.9 [Tokenization](#3.9)\n   - 3.10 [Stemming](#3.10)\n   - 3.11 [Lemmatization](#3.11)\n4.\t[Advance Text Processing](#4)\n   - 4.1 [N-grams](#4.1)\n   - 4.2 [Term Frequency](#4.2)\n   - 4.3 [Inverse Document Frequency](#4.3)\n   - 4.4 [Term Frequency-Inverse Document Frequency (TF-IDF)](#4.4)\n   - 4.5 [Bag of Words](#4.5)\n   - 4.6 [Sentiment Analysis](#4.6)\n5.  [References](#5)\n\n","1382af1b":"## **4.1 N-grams** <a class=\"anchor\" id=\"4.1\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- [N-grams](https:\/\/kavita-ganesan.com\/what-are-n-grams\/#.Xo3jccgzbIU) are the combination of multiple words used together. Ngrams with N=1 are called **unigrams**. Similarly, **bigrams (N=2)**, **trigrams (N=3)** and so on.\n\n- **Unigrams** do not usually contain as much information as compared to **bigrams** and **trigrams**. The basic principle behind n-grams is that they capture the language structure, like what letter or word is likely to follow the given one. \n\n- The longer the n-gram (the higher the n), the more context you have to work with. Optimum length really depends on the application \u2013 if your n-grams are too short, you may fail to capture important differences. On the other hand, if they are too long, you may fail to capture the \u201cgeneral knowledge\u201d and only stick to particular cases.\n\n- Now, we will extract bigrams from our tweets using the ngrams function of the textblob library.","9ff67678":"- We can see that word counts in every tweet has been calculated above.","f136679b":"# **4. Advanced Text Processing** <a class=\"anchor\" id=\"4\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Until now, we have covered all the basic pre-processing steps which are helpful in order to clean our data.\n\n- Now,we will move on and focus on advanced text-processing or NLP techniques.","a19c9dce":"## **3.4 Punctuation Removal** <a class=\"anchor\" id=\"3.4\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- The next step is to remove punctuation as it doesn\u2019t add any extra information while treating text data. \n\n- Therefore removing all instances of it will help us reduce the size of the training data.","398da2bb":"### **Preview dataset**","26e0ebde":"[Go to Top](#0)\t","3849ff61":"## **2.1 Count number of words** <a class=\"anchor\" id=\"2.1\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- One of the most basic requirement in NLP analysis is to count the number of words in each tweet. The idea behind this is that **the negative sentiments contain a lesser amount of words than the positive ones**.\n\n- We can accomplish the above task (count the number of words) by using the **split** function in python as follows-","d7826a5e":"- All these pre-processing steps are essential and help us in reducing our vocabulary clutter so that the features produced in the end are more effective.","92e6a79c":"##  **2.2 Count number of characters**  <a class=\"anchor\" id=\"2.2\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- We can also calculate the number of characters in every tweet. The intuition is same as above.\n\n- This can be accomplised by calculating the length of the tweet as follows -","7f884197":"## **3.3 Lower Casing** <a class=\"anchor\" id=\"3.3\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Another pre-processing step which we will do is to transform our tweets into lower case. \n- This avoids having multiple copies of the same words. \n- For example, while calculating the word count, \u2018Analytics\u2019 and \u2018analytics\u2019 will be taken as different words.","01a54841":"## **3.11 Lemmatization** <a class=\"anchor\" id=\"3.11\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- [Lemmatization](https:\/\/www.geeksforgeeks.org\/python-lemmatization-with-nltk\/) is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n\n- Lemmatization is a more effective option than stemming because it converts the word into its root word, rather than just stripping the suffices. \n\n- Lemmatization makes use of the vocabulary and does a morphological analysis to obtain the root word. Therefore, we usually prefer using lemmatization over stemming.","f4a1f25a":"### **Load necessary libraries**","54a3c4d2":"## **2.3 Average word length** <a class=\"anchor\" id=\"2.3\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Now, number of words and number of characters are important. But, there is another feature which is also important is **average word length** of each tweet. This feature can help us to improve our model.\n\n- We can accomplish the above task by simply taking the sum of the length of all the words and divide it by the total length of the tweet.","8915334e":"## **4.3 Inverse Document Frequency (IDF)** <a class=\"anchor\" id=\"4.3\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- The intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it\u2019s appearing in all the documents.\n\n- Therefore, the IDF of each word is the log of the ratio of the total number of rows to the number of rows in which that word is present.\n\n- IDF can be calculated as follows -\n\n   **IDF = log(N\/n)**, \n   \n where, N is the total number of rows and n is the number of rows in which the word was present.\n\n- Now, we will calculate IDF for the same tweets for which we calculated the term frequency.","1db938e8":"## **3.9 Tokenization** <a class=\"anchor\" id=\"3.9\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- [Tokenization](https:\/\/www.geeksforgeeks.org\/nlp-how-tokenizing-text-sentence-words-works\/) refers to dividing the text into a sequence of words or sentences. \n\n- In our example, we have used the textblob library to first transform our tweets into a blob and then converted them into a series of words.","6f03a8ea":"# **5. References** <a class=\"anchor\" id=\"5\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\nThis notebook is based on excellent article by Shubham Jain - \n\n- [Ultimate Guide to deal with Text Data](https:\/\/www.analyticsvidhya.com\/blog\/2018\/02\/the-different-methods-deal-text-data-predictive-python\/)","dedfe0c3":"**I hope you find this kernel useful and your <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n\n","f0a2fa9a":"## **3.1 CountVectorization** <a class=\"anchor\" id=\"3.1\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- **[CounterVectorization](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html)** is a SciKitLearn library takes any text document and returns each unique word as a feature with the count of number of times that word occurs.\n\n- While this can generate lot of features with some extremely useful parameters that help avoid that including stop_words, n_grams, and max_features. \n\n- **Stop words** generates a list of words that will not be included as a feature. The primary use of this is the \u201cEnglish\u201d dictionary where it will get rid of insignificant words like \u201cis, the, a, it, as \u201cwhich can appear quite frequently, but have little to no influence on our end goal. \n\n- **ngrams_range** selects how you can group words together. Instead of having NLP return each word separately, we can get results like \u201cHello again\u201d if it equals 2 or \u201cSee you later\u201d if it equals 3. \n\n- **max_features** is how many features you choose to create. If we choose it to equal none it means that we will get every and all words as features, but if we set it equal to 50 you will only get the 50 most frequently used words.\n\n\n","7ac6bd0a":"- We can see that character counts in every tweet has been calculated above.\n\n- The above calculation will also include the number of spaces, which we can remove, if required.","a909cc6f":"Now, we will remove these words as their presence will not of any use in classification of our text data.","bd712f42":"## **2.4 Number of stopwords** <a class=\"anchor\" id=\"2.4\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Generally, while solving any NLP problem, the first thing we do is to remove the stopwords. But, what are stop words?\n\n- **Stop Words** : A stop word is a commonly used word such as `the`, `a`, `an`, `in` which are filtered out before or after processing of natural language data (text). Sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n\n- For more information on stop words, please visit the following links-\n\n- https:\/\/en.wikipedia.org\/wiki\/Stop_words\n\n- https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n\n- https:\/\/kavita-ganesan.com\/what-are-stop-words\/#.XowrncgzbIU\n\n\n- To check the list of stopwords we can type the following commands.","0719e9ce":"Some of the most useful NLP techniques currently used in practice are:-\n\n- **CountVectorization**, \n- **Hashing Vectorization**, \n- **Term Frequency-Inverse Document Frequency (TF-IDF)**, \n- **Lemmatization**, \n- **Stemming**, \n- **Parsing**, and \n- **Sentiment Analysis**.","e7aa4151":"- We can see that all the punctuation, including \u2018#\u2019 and \u2018@\u2019, has been removed from the training data.","454ac474":"## **3.8 Spelling Correction**  <a class=\"anchor\" id=\"3.8\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n\n- Now tweets can be filled with plethora of spelling mistakes. Our task is to rectify these spelling mistakes.\n\n- In that context, spelling correction is a useful pre-processing step because this also will help us in reducing multiple copies of words. For example, \u201cAnalytics\u201d and \u201canalytcs\u201d will be treated as different words even if they are used in the same sense.\n\n- To accomplish the above task, we will use the textblob library as follows-\n","b44f2d18":"## **4.4 Term Frequency \u2013 Inverse Document Frequency (TF-IDF)** <a class=\"anchor\" id=\"4.4\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- **TF-IDF** is the multiplication of the TF and IDF which we calculated again below for convinience.","e01ce74b":"## **3.5 Stop Words Removal**  <a class=\"anchor\" id=\"3.5\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- As we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. \n- For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries.","b523733b":"### **Read dataset**","25e03683":"## **4.5 Bag of Words** <a class=\"anchor\" id=\"4.5\"><\/a>\n\n[Table of Contents](#0.1)\n\n\n- [Bag of Words (BoW)](https:\/\/machinelearningmastery.com\/gentle-introduction-bag-words-model\/) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document.\n\n- For implementation, sklearn provides a separate function for it as shown below:","14bf863c":"I hope you find this notebook useful and enjoyable. Your comments and feedback are most welcome.\n\nThank you","3e173bd1":"## **3.2 HashingVectorizer** <a class=\"anchor\" id=\"3.2\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- [Hashing Vectorizer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.HashingVectorizer.html) converts text to a matrix of occurrences using the \u201chashing trick\u201d.\n\n\n- It converts a collection of text documents to a matrix of token occurrences.\n\n- It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=\u2019l1\u2019 or projected on the euclidean unit sphere if norm=\u2019l2\u2019.\n\n- This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n\n- Each word is mapped to a feature and using the hash function converts it to a hash. \n\n- If the word occurs again in the body of the text it is converted to that same feature which allows us to count it in the same feature without retaining a dictionary in memory.","ccd6975b":"## **3.6 Frequent Words Removal**  <a class=\"anchor\" id=\"3.6\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- We can also remove commonly occurring words from our text data.\n\n- First, let\u2019s check the 10 most frequently occurring words in our text data then take call to remove or retain.\n","4c9b6379":"## **2.7 Number of Uppercase words**  <a class=\"anchor\" id=\"2.7\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Anger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words.","1dfc091f":"- We can count the number of stopwords as follows-","64fe63fd":"- We can can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model.","5de728fc":"# **2. Basic feature extraction using text data** <a class=\"anchor\" id=\"2\"><\/a>\n\n\n[Table of Contents](#0.1)\n\n\n- Suppose, we don't have sufficient knowledge of Natural Language Processing.\n\n- We can still use some basic feature extraction techniques to extract features from text data.\n\n- In this section, we will discuss some basic feature extraction techniques.\n\n- But, first let's import the data.\n\n- We have used the [Twitter Sentiment Analysis](https:\/\/www.kaggle.com\/arkhoshghalb\/twitter-sentiment-analysis-hatred-speech) data for this notebook.\n","a3d8b159":"- We can see that *dysfunctional* has been transformed into *dysfunct*, among other changes."}}