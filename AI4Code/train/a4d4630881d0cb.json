{"cell_type":{"db58b34e":"code","7c7355aa":"code","00e49a26":"code","52805438":"code","64f7f690":"code","67c423e6":"code","abbece8b":"code","7fba8cf2":"code","1b1d1da0":"code","0ea49b96":"code","dca5c31f":"code","d856fce7":"code","da53e0d9":"code","1356da3c":"markdown","e8fa783f":"markdown","b775ba7d":"markdown","30adb50d":"markdown","34c5ff0b":"markdown","552b3972":"markdown","b4e98641":"markdown","7732681b":"markdown","b40f6a6c":"markdown"},"source":{"db58b34e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7c7355aa":"from transformers import AutoTokenizer, TFBertForSequenceClassification\nfrom nltk.corpus import stopwords, wordnet\nfrom transformers import *\nfrom nltk import download\nimport tensorflow as tf\nfrom io import open\nimport numpy as np\nimport json\nimport re\n\n#tpu\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","00e49a26":"# download('stopwords')\ndownload('wordnet')\n\n#Get rid of noise from dataset\ndef clean_str(string):\n    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\.]\", \" \", string)\n    string = re.sub(r\"\\'s\", \" \\'s\", string)\n    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n    string = re.sub(r\"\\'re\", \" \\'re\", string)\n    string = re.sub(r\"\\'d\", \" \\'d\", string)\n    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" \\( \", string)\n    string = re.sub(r\"\\)\", \" \\) \", string)\n    string = re.sub(r\"\\?\", \" \\? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n\n    word_list = string.split(' ')\n    string = \"\"\n    for word in word_list:\n        if word not in stopwords.words('english'):\n            if wordnet.synsets(word):\n                string = string + word + \" \"\n    return string.strip().lower()\n\n#Political Bias Stuff - modified from https:\/\/github.com\/icoen\/CS230P\/blob\/master\/RNN\/data_helpers2.py\ndef load_data_and_labels(positive_data_file, negative_data_file):\n    positive_examples = list(positive_data_file)#open(positive_data_file, \"r\", encoding='utf-8').readlines())\n    positive_examples = [s.strip() for s in positive_examples]\n    negative_examples = list(negative_data_file)#open(negative_data_file, \"r\", encoding='utf-8').readlines())\n    negative_examples = [s.strip() for s in negative_examples]\n\n    x_text = positive_examples + negative_examples\n    x_text = [clean_str(sent) for sent in x_text]\n\n    positive_labels = [[1] for _ in positive_examples]\n    negative_labels = [[0] for _ in negative_examples]\n\n    y = np.concatenate([positive_labels, negative_labels], 0)\n    return [x_text, y]\n","52805438":"#returns tokenized data\ndef tokenize_sentences(sentences, tokenizer, max_seq_len = 50):\n    tokenized_sentences = []\n    for sentence in sentences:\n        tokenized_sentence = tokenizer.texts_to_sequences(sentence) #tokenizer.encode(sentence, add_special_tokens = True, max_length = max_seq_len)\n        tokenized_sentences.append(tokenized_sentence)\n    return tokenized_sentences\n\ndef preprocessPoliticalData(dem_file,rep_file):\n    print(\"Loading data...\")\n    x_text, y = load_data_and_labels(dem_file,rep_file)\n    #AutoTokenizer.from_pretrained('bert-base-uncased', do_lowercase=True)\n    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000, filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=\" \", oov_token=\"<OOV>\")\n    tokenizer.fit_on_texts(x_text)\n    x_train = tokenizer.texts_to_sequences(x_text)\n#     x_train = tokenize_sentences(x_text, tokenizer)\n    x = np.array(list(tf.keras.preprocessing.sequence.pad_sequences(x_train, 50, padding='post', truncating='post')))\n\n    shuffle_indices = np.random.permutation(np.arange(len(y)))\n    x_train = x[shuffle_indices]\n    y_train = y[shuffle_indices]\n\n    del x, y\n    return x_train, y_train","64f7f690":"conservative_dataset = pd.read_csv(\"..\/input\/political-tweets-from-twitter-bots\/conservative_bot_tweets.csv\")\nliberal_dataset = pd.read_csv(\"..\/input\/political-tweets-from-twitter-bots\/liberal_bot_tweets.csv\")\n\nconservative_tweets = str(conservative_dataset.iloc[:,3])\nliberal_tweets = str(liberal_dataset.iloc[:,3])","67c423e6":"x_train, y_train = preprocessPoliticalData(liberal_tweets, conservative_tweets)\n# x_vtext, y_val = preprocessPoliticalData('\/kaggle\/input\/political-tweets\/repfullval.txt', '\/kaggle\/input\/political-tweets\/repfullval.txt')","abbece8b":"from tensorflow.keras.layers import Activation, Dense, Dropout, Input, Embedding, GRU, GlobalMaxPooling1D, Bidirectional\nfrom tensorflow.keras.models import Model\n\ndef biGRUnet():\n    inputs = Input(name='inputs',shape=[x_train.shape[1]])\n    layer = Embedding(10000,50,input_length=x_train.shape[1])(inputs)\n    layer = Bidirectional(GRU(64, return_sequences = True), merge_mode='concat')(layer)\n    layer = GlobalMaxPooling1D()(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","7fba8cf2":"from tensorflow.keras.optimizers import RMSprop\nprint(x_train.shape[1])\nwith tpu_strategy.scope():\n    model = biGRUnet()\n    model.summary()\n    model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","1b1d1da0":"model.fit(x_train,y_train,batch_size=64 * tpu_strategy.num_replicas_in_sync,epochs=100)\n          #validation_data=(x_vtext, y_val))","0ea49b96":"model.save(\"model.h5\")","dca5c31f":"from transformers import TFBertForSequenceClassification\n\nlearning_rate = 2e-5\nwith tpu_strategy.scope():\n    bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n    bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])","d856fce7":"bert_model.fit(x_train,y_train,batch_size=32 * tpu_strategy.num_replicas_in_sync,epochs=100)#, validation_data=(x_vtext, y_val))","da53e0d9":"bert_model.save(\"bert.h5\")","1356da3c":"Tokenize Validation Data","e8fa783f":"Dataset Cleaning\/Preprocessing","b775ba7d":"Compile the model","30adb50d":"Tokenization of the Data","34c5ff0b":"Activate TPU","552b3972":"This is an experimental version of a political bias model that I have created. You can check out the website here:\nhttp:\/\/sentimeant.herokuapp.com\/","b4e98641":"BERT Model","7732681b":"Train the model","b40f6a6c":"Get Data"}}