{"cell_type":{"e6a63883":"code","3c569e4b":"code","0e0164c1":"code","8b10e5f6":"code","173c5d7a":"code","b9aa040f":"code","e4973604":"code","dcd3d40a":"code","a8e7ad2b":"code","b5cb400a":"code","1bb8ee62":"code","8d5c7d19":"code","a1b49d69":"code","5210187f":"code","9d119b58":"code","8efad536":"code","c008438c":"code","890a1f29":"code","e386cd52":"code","bc964655":"code","0226cb33":"code","b9f67ac0":"code","d5494b4f":"code","fdb0c96c":"code","96aa00a6":"code","fece08ad":"code","1585f724":"code","a49bb775":"markdown","e88a1035":"markdown","8fd80888":"markdown","1658677a":"markdown","28690923":"markdown","1965ff32":"markdown","f3049aeb":"markdown","31104a2e":"markdown","2ee89a3d":"markdown","69cbd03a":"markdown","e69d96c9":"markdown","eeb276b5":"markdown","a89a8cdb":"markdown","2dba6688":"markdown","ce6553a2":"markdown","0a619b56":"markdown","da8570b5":"markdown","fd32da94":"markdown","46efe9f9":"markdown","327e142a":"markdown","de004cef":"markdown","e0b06444":"markdown","44df10df":"markdown"},"source":{"e6a63883":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3c569e4b":"import pandas as pd\nimport numpy as np\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn import metrics   #Additional scklearn functions\nfrom sklearn.model_selection import GridSearchCV   #Perforing grid search\n\nimport matplotlib.pylab as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 12, 4","0e0164c1":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","8b10e5f6":"train.head()","173c5d7a":"test.info()","b9aa040f":"# \ucc98\ub9ac\ud558\uae30 \ubcf5\uc7a1\ud55c Column\uc740 \uc81c\uac70\ndel train['Ticket']; del test['Ticket']\ndel train['Cabin']; del test['Cabin']\ndel train['Name']; del test['Name']","e4973604":"# train, test\uc5d0 \ub2e4\ub978 Category \uc874\uc7ac \uac00\ub2a5\ntest.insert(loc=1, column='Survived', value=0)\ntotal = pd.concat([train, test], axis=0)","dcd3d40a":"# One hot encoding\nsex = pd.get_dummies(total['Sex'])\nembarked = pd.get_dummies(total['Embarked'])","a8e7ad2b":"# \uae30\uc874 \uceec\ub7fc \uc81c\uac70\ndel total['Sex']\ndel total['Embarked']","b5cb400a":"total = pd.concat([total, sex, embarked], axis=1)\ntotal['Family'] = total['Parch'] + total['SibSp']","1bb8ee62":"# one hot \uceec\ub7fc\uc774 \uc788\ub294 train, test \ntrain = total[0:len(train)]\ntest = total[len(train):]","8d5c7d19":"train.head()","a1b49d69":"test.head()","5210187f":"target = 'Survived'\nIDcol = 'PassengerId'","9d119b58":"def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=100):\n   \n    # get new n_estimator\n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n                          metrics='error', early_stopping_rounds=early_stopping_rounds)\n        alg.set_params(n_estimators=cvresult.shape[0])\n        print(alg)\n    \n    # Fit the algorithm on the data\n    alg.fit(dtrain[predictors], dtrain['Survived'], eval_metric='error')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n        \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"Training Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Survived'].values, dtrain_predictions))","8efad536":"predictors = [x for x in train.columns if x not in [target, IDcol]]\nxgb1 = XGBClassifier(\n    learning_rate =0.1,\n    n_estimators=1000,\n    max_depth=5,\n    min_child_weight=1,\n    gamma=0,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective= 'binary:logistic',\n    nthread=-1,\n    scale_pos_weight=1,\n    seed=2019\n)\nmodelfit(xgb1, train, predictors)","c008438c":"param_test1 = {\n 'max_depth':range(3,10,3),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate=0.1, \n                                                  n_estimators=1000, \n                                                  max_depth=5, \n                                                  min_child_weight=1, \n                                                  gamma=0, \n                                                  subsample=0.8, \n                                                  colsample_bytree=0.8,\n                                                  objective= 'binary:logistic', \n                                                  nthread=-1, \n                                                  scale_pos_weight=1, seed=2019),\nparam_grid = param_test1, scoring='accuracy',n_jobs=-1,iid=False, cv=5, verbose=10)\ngsearch1.fit(train[predictors],train[target])\ngsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","890a1f29":"param_test2 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch2 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, \n                                                  n_estimators=1000, \n                                                  max_depth=3,\n                                                  min_child_weight=5, \n                                                  gamma=0, \n                                                  subsample=0.8, \n                                                  colsample_bytree=0.8,\n                                                  objective= 'binary:logistic', \n                                                  thread=-1, \n                                                  scale_pos_weight=1,\n                                                  seed=2019), \n                        param_grid = param_test2, scoring='accuracy', n_jobs=-1, iid=False, cv=5)\ngsearch2.fit(train[predictors],train[target])\ngsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_","e386cd52":"param_test3 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, \n                                                  n_estimators=1000, \n                                                  max_depth=3,\n                                                  min_child_weight=5, \n                                                  gamma=0, \n                                                  subsample=0.8, \n                                                  colsample_bytree=0.8,\n                                                  objective= 'binary:logistic', \n                                                  thread=-1, \n                                                  scale_pos_weight=1,\n                                                  seed=2019), \n                        param_grid = param_test3, scoring='accuracy', n_jobs=-1, iid=False, cv=5, verbose=10)\ngsearch3.fit(train[predictors],train[target])\ngsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_","bc964655":"param_test4 = {\n 'subsample':[i\/100.0 for i in range(40,80)],\n}\ngsearch4 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, \n                                                  n_estimators=1000, \n                                                  max_depth=3,\n                                                  min_child_weight=5, \n                                                  gamma=0, \n                                                  subsample=0.6, \n                                                  colsample_bytree=0.8,\n                                                  objective= 'binary:logistic', \n                                                  thread=-1, \n                                                  scale_pos_weight=1,\n                                                  seed=2019), \n                        param_grid = param_test4, scoring='accuracy', n_jobs=-1, iid=False, cv=5, verbose=10)\ngsearch4.fit(train[predictors],train[target])\ngsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_","0226cb33":"param_test5 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch5 = GridSearchCV(estimator = XGBClassifier(learning_rate =0.1, \n                                                  n_estimators=1000, \n                                                  max_depth=3,\n                                                  min_child_weight=5, \n                                                  gamma=0, \n                                                  subsample=0.67, \n                                                  colsample_bytree=0.8,\n                                                  objective= 'binary:logistic', \n                                                  thread=-1, \n                                                  scale_pos_weight=1,\n                                                  seed=2019), \n                        param_grid = param_test5, scoring='accuracy', n_jobs=-1, iid=False, cv=5, verbose=10)\ngsearch5.fit(train[predictors],train[target])\ngsearch5.cv_results_, gsearch5.best_params_, gsearch5.best_score_","b9f67ac0":"predictors = [x for x in train.columns if x not in [target, IDcol]]\nxgb1 = XGBClassifier(\n    learning_rate =0.01,\n    n_estimators=5000,\n    max_depth=3,\n    min_child_weight=5,\n    gamma=0,\n    reg_alpha=1e-05,\n    subsample=0.67,\n    colsample_bytree=0.8,\n    objective= 'binary:logistic',\n    nthread=-1,\n    scale_pos_weight=1,\n    seed=2019\n)\nmodelfit(xgb1, train, predictors)","d5494b4f":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')","fdb0c96c":"seeds = [2015, 2016, 2017, 2018, 2019]\npredictors = [x for x in train.columns if x not in [target, IDcol]]\n\nfor seed in seeds:\n    xgb1 = XGBClassifier(\n        learning_rate =0.01,\n        n_estimators=5000,\n        max_depth=3,\n        min_child_weight=5,\n        gamma=0,\n        reg_alpha=1e-05,\n        subsample=0.67,\n        colsample_bytree=0.8,\n        objective= 'binary:logistic',\n        nthread=-1,\n        scale_pos_weight=1,\n        seed=seed\n    )\n    modelfit(xgb1, train, predictors)\n    sample_submission['Survived'] += xgb1.predict(test[test.columns[2:]])","96aa00a6":"sample_submission['Survived'] = sample_submission['Survived'] > 2.5","fece08ad":"sample_submission['Survived'] = sample_submission['Survived'].apply(lambda x: int(x))","1585f724":"sample_submission.to_csv('.\/my_third_submission.csv', index=False)","a49bb775":"2. Boost Parameter\n    - eta: Learning rate(\uc77c\ubc18\uc801\uc73c\ub85c 0.01 - 0.2)\n    - min_child_weight: min_child_weight\ub97c \uae30\uc900\uc73c\ub85c \ucd94\uac00 \ubd84\uae30 \uacb0\uc815(\ud06c\uba74 Underfitting)\n    - max_depth: Tree \uae4a\uc774 \uc218\n    - max_leaf_node: \ud558\ub098\uc758 \ud2b8\ub9ac\uc5d0\uc11c node \uac1c\uc218\n    - gamma: split \ud558\uae30 \uc704\ud55c \ucd5c\uc18c\uc758 loss \uac10\uc18c \uc815\uc758\n    - subsample: \ub370\uc774\ud130 \uc911 \uc0d8\ud50c\ub9c1(0.5 - 1)\n    - colsample_bytree: column \uc911 sampling(0.5 - 1)\n    - colsample_bylevel: \uac01 level\ub9c8\ub2e4 \uc0d8\ud50c\ub9c1 \ube44\uc728\n    - lambda: L2 nrom\n    - alpha: L1 norm\n    - scale_pos_weight: positive, negative weight \uc9c0\uc815\n    - \uae30\ud0c0 \ub4f1","e88a1035":"## 3-6. Learning Rate \uac10\uc18c","8fd80888":"## 3-3. Gamma\ub97c \ud29c\ub2dd\ud55c\ub2e4.","1658677a":"## 3-1. Learning rate\uc640  estimator \uc218\ub97c \uace0\uc815\ud55c\ub2e4.","28690923":"- Parameter \uc885\ub958\n    - General Parameter: \uc804\uccb4 \uae30\ub2a5\uc744 \uac00\uc774\ub4dc\n    - Boost Parameter: \uac01\uac01\uc758 step\uc5d0\uc11c booster \uac00\uc774\ub4dc\n    - Learning Task Parameter: \ucd5c\uc801\ud654 \uc218\ud589 \uac00\uc774\ub4dc","1965ff32":"# Xgboost \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd","f3049aeb":"1. General Parameter\n    - booster: tree \uae30\ubc18 \ubaa8\ub378 \/ \uc120\ud615 \ubaa8\ub378\n    - silent: \uba54\uc138\uc9c0 \uc870\uc808\n    - nthread: \ubcd1\ub82c \ucc98\ub9ac \uc870\uc808","31104a2e":"## 1. Xgboost \uc7a5\uc810\n\nXgboost\ub294 \uae30\uc874 GBDT \ubaa8\ub378\uc5d0 \ube44\ud574\uc11c \ub2e4\uc74c \uae30\ub2a5\uc774 \uc788\ub2e4.\n\n- \uc815\uaddc\ud654(Regularization)\n- \ubcd1\ub82c \ucc98\ub9ac\n- \uace0\uc218\uc900\uc758 \uc720\uc5f0\uc131\n- \uacb0\uce21\uce58 \ucc98\ub9ac\n- Tree Pruning\n- \ub0b4\uc7a5 Cross Validation\n- \uae30\uc874 \ubaa8\ub378\uc5d0 \uc774\uc5b4\uc11c \uc7ac\ud559\uc2b5\ud560 \uc218 \uc788\uc74c","2ee89a3d":"\ucd08\uae30\uac12\uc740 \ub2e4\uc74c\uacfc \uac19\uc774 \uc120\uc815\ud55c\ub2e4.\n\n1. max_depth = 5: \ubcf4\ud1b5 4-6 \ub97c \uc2dc\uc791\uc810\uc73c\ub85c \ud55c\ub2e4.\n\n2. min_child_weight = 1 : \ud5a5\ud6c4\uc5d0 \ud29c\ub2dd\ud560 \uac83\uc774\ub2e4.\n\n3. gamma = 0 :  0.1 - 0.2\ub85c \uc2dc\uc791\ud574\ub3c4 \ub41c\ub2e4. \uadf8\ub7f0\ub370 \uc5b4\uc9dc\ud53c \ud29c\ub2dd\ud560 \uac83\uc774\ub2e4.\n\n4. subsample, colsample_bytree = 0.8 : \ubcf4\ud1b5 0.5 - 0.9\ub85c \uc2dc\uc791\ud55c\ub2e4.\n\n5. scale_pos_weight = 1: Because of high class imbalance.\n","69cbd03a":"### \ub370\uc774\ud130 \uc815\uc81c\ud558\uae30\n\n\uc5ec\uae30\uc5d0\uc11c\ub294 Hyperparameter Tuning\uc774 \ubaa9\uc801\uc774\ubbc0\ub85c \uc804\ucc98\ub9ac\ub97c \uc790\uc138\ud788 \ud558\uc9c0\ub294 \uc54a\uaca0\uc2b5\ub2c8\ub2e4.","e69d96c9":"Table of Contents\n- Xgboost \uc7a5\uc810\n- Xgboost Parameter \uc774\ud574\ud558\uae30\n- Parameter Tuning \ud558\uae30\n","eeb276b5":"## 3-5. Regularization Parameter \ud29c\ub2dd","a89a8cdb":"## 3-4. subsample and colsample_bytree\ub97c \ud29c\ub2dd\ud55c\ub2e4.\n","2dba6688":"## 2. Xgboost Hyperparameter\n\nXgboost\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 Hyperparameter\uac00 \uc788\uc2b5\ub2c8\ub2e4.","ce6553a2":"3. Learning Task Parameter\n    - object: \ubaa9\uc801\ud568\uc218 \uc885\ub958\n        - binary:logistic(\uc774\uc9c4 \ubd84\ub958)\n        - multi:softmax(\ub2e4\uc911 \ubd84\ub958)\n        - multi-softprob(\ub2e4\uc911 \ud655\ub960)    \n    - eval_metric: \ud3c9\uac00 \uc9c0\ud45c\n        - rmse \u2013 root mean square error\n        - mae \u2013 mean absolute error\n        - logloss \u2013 negative log-likelihood\n        - error \u2013 Binary classification error rate (0.5 threshold)\n        - merror \u2013 Multiclass classification error rate\n        - mlogloss \u2013 Multiclass logloss\n        - auc: Area under the curve\n    - seed\n","0a619b56":"\ub2e4\uc74c \uae00\uc744 \ucc38\uace0 \ubc0f \ubc88\uc5ed\ud588\uc2b5\ub2c8\ub2e4. <br\/>\n[Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)","da8570b5":"## 3-2. max_depth\uc640 min_child_weight\ub97c \ud29c\ub2dd\ud55c\ub2e4.","fd32da94":"## \uc608\uce21 \ubaa8\ub378 \ud568\uc218 \uc0dd\uc131","46efe9f9":"## 5. \uacb0\uacfc \uc81c\ucd9c","327e142a":"## 3. \uc77c\ubc18\uc801\uc778 Hyperparameter \ud29c\ub2dd \ubc29\ubc95\n\n1. high learning rate(0.05 - 0.3)\ub97c \uc120\ud0dd\ud558\uace0 \uc774 \ud559\uc2b5\ub960\uc5d0 \ub9de\ub294 tree \uac1c\uc218\ub97c \uc120\uc815\ud55c\ub2e4.\n2. tree-specific parameter\ub97c \uc218\uc815\ud55c\ub2e4.\n    - max_depth, min_child_weight, gamma, subsample, colsample_bytree\n3. regularization parameter\ub97c \uc218\uc815\ud55c\ub2e4.\n4. \ud559\uc2b5\ub960\uc744 \ub0ae\ucd94\uace0 \ub2e4\uc2dc \ubc18\ubcf5\ud55c\ub2e4.","de004cef":"## 3-4-2. subsample \ucd94\uac00 \ud29c\ub2dd\ud558\uae30","e0b06444":"# 4. seed\ubcc4 \uc559\uc0c1\ube14 \ud6c4 \uacb0\uacfc \uc81c\ucd9c","44df10df":"> ## 3. \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd"}}