{"cell_type":{"854c8cf9":"code","ddd610e9":"code","d31ae682":"code","96a5ecfe":"code","97b31b27":"code","abfd808c":"code","46dc7ff0":"code","7527c90d":"code","33d9fcd6":"code","687fd795":"code","e1dff14d":"code","0645a80a":"code","c71b80de":"code","384e9259":"code","8a65566a":"code","5d2a757e":"code","0239ac00":"code","01ffff44":"code","2721ed05":"code","b9acc7d8":"code","19bff0a4":"code","58bf1f51":"code","f8ff2b24":"code","0050abb6":"code","fe4feb81":"code","b6e21e4b":"code","4463fb33":"code","7dc679da":"code","5bb7ae83":"code","c5fd15c2":"code","295b4441":"code","f59c2f21":"code","bd03a60b":"code","9ab45d4f":"code","8bb6c7fb":"code","5ce5dd17":"code","dc2aa72c":"code","c7225e74":"code","d0528326":"code","9e6631c9":"code","7c74a640":"code","299198e1":"code","4356f62b":"code","e26864dc":"code","52eb6432":"code","42868194":"code","d7f614ce":"code","e5c214b8":"code","600a8c70":"markdown","a7d58813":"markdown","612b36f9":"markdown","3dbfca41":"markdown","980da189":"markdown","db0f66f2":"markdown","5e78f8c0":"markdown","6dea70c4":"markdown","9e0008dd":"markdown","2473a7db":"markdown","2e465bf1":"markdown","abdca58e":"markdown","8385f8d7":"markdown","66e68637":"markdown","68022dc3":"markdown","fd51e566":"markdown","512b48b4":"markdown","bc18e804":"markdown","edd7650c":"markdown","7324e648":"markdown"},"source":{"854c8cf9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import confusion_matrix, classification_report","ddd610e9":"df = pd.read_csv(\"..\/input\/beginner-datasets\/beginner_datasets\/bank.csv\")","d31ae682":"df.head()","96a5ecfe":"df.poutcome.value_counts()","97b31b27":"df.info()","abfd808c":"df.describe()","46dc7ff0":"# catplot\n\nsns.catplot(x=\"deposit\", y=\"age\", data=df);\n\n# using jitter = False option doesn't give any info here and","7527c90d":"#boxplot\n\nsns.catplot(y=\"deposit\", x=\"age\", kind=\"box\", data=df);","33d9fcd6":"# hue = \"education\"\n\nsns.catplot(x=\"deposit\", y=\"age\",hue=\"education\", kind=\"box\", data=df);","687fd795":"# kind=\"boxen\"\n\nsns.catplot(x=\"deposit\", y=\"age\",hue=\"education\", kind=\"boxen\", data=df);","e1dff14d":"sns.catplot(x=\"education\", y=\"age\", hue=\"deposit\",\n            kind=\"violin\", data=df);","0645a80a":"# to be more space effecient use split=True\n\nsns.catplot(x=\"education\", y=\"age\", hue=\"deposit\",\n            kind=\"violin\",split=True, data=df);","c71b80de":"sns.catplot(x=\"education\", y=\"age\", hue=\"deposit\", kind=\"bar\", data=df);","384e9259":"sns.catplot(x=\"education\", kind=\"count\", palette=\"ch:.25\", data=df);","8a65566a":"sns.catplot(x=\"education\", y=\"age\", hue=\"deposit\", kind=\"point\", data=df);","5d2a757e":"sns.violinplot(x=df.deposit, y=df.age)","0239ac00":"# using row instead of col cause there are a lot of values\n\nsns.catplot(x=\"education\", y=\"age\", hue=\"deposit\",\n            row=\"job\", aspect=0.7,\n            kind=\"box\", data=df);","01ffff44":"# got my hands on this cool code from the website abt violine plot\n\nf, ax = plt.subplots(figsize=(8, 8))\n\n# Show each distribution with both violins and points\nsns.violinplot(x=\"deposit\",y=\"age\",data=df, palette=\"Set3\", inner=\"points\", bw =.2, cut=2, linewidth=3)\n\nsns.despine(left=True)\n\nf.suptitle('Deposit by age', fontsize=18, fontweight='bold')\nax.set_xlabel(\"Weight (g)\",size = 16,alpha=0.7)\nax.set_ylabel(\"Feed\",size = 16,alpha=0.7);","2721ed05":"# i have seen a guy decrease the coding by writing a function where he feels necessary so i am going to try the same...\n# (i haven't been using functions a lot... it means i practice more but if i save time i can work on something else)\n# a function to do the ploting:\n\n\ndef eda(col):\n    print(\"--------------\",col,\"-----------------\")\n    if df[col].dtype == 'object':\n        # bar\n        df[col].value_counts().plot.bar(figsize=(10,5)); \n        plt.show()\n        # count\n        cat = sns.catplot(y=col, hue=\"deposit\", kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=df);\n        cat.set_xlabels(fontsize = 15)\n        cat.set_ylabels(fontsize = 15)\n        plt.show()\n        print(\"ratio of yes\/no based on category of \" + col, end='\\n\\n')\n        for cat in df[col].unique():\n            print(cat,\": \" ,df.groupby([col]).deposit.value_counts()[cat]['yes']\/df.groupby([col]).deposit.value_counts()[cat]['no'])\n    else:\n        sns.displot(df[col], color='r', kde=True,);\n        plt.show()        \n        sns.barplot(x=df['deposit'], y=df[col]);\n        plt.show()        \n        # box\n        print(\"box\")\n        sns.catplot(y=\"deposit\", x=col, kind=\"box\", data=df);\n        plt.show()\n        # voilin\n        sns.catplot(x=\"deposit\", y=col,\n            kind=\"violin\",split=True, data=df);\n        plt.show()\n        sns.boxplot(x=df[col]);\n        plt.show()\n    print(\"\\n\\n\\n\\n\")","b9acc7d8":"eda(\"age\")\n\n# right skewed\n# there are obviously a lot of outliers...\n# violine plot says the ones with has have a bit of greater density of outliers","19bff0a4":"eda('job')\n# gonna try one hot encoding...","58bf1f51":"eda('marital')","f8ff2b24":"eda(\"education\")","0050abb6":"eda(\"default\")","fe4feb81":"eda(\"balance\")","b6e21e4b":"eda(\"housing\")","4463fb33":"eda(\"loan\")","7dc679da":"eda(\"contact\")","5bb7ae83":"eda(\"day\")\n# we have 3 peaks","c5fd15c2":"eda(\"month\")","295b4441":"eda(\"duration\")","f59c2f21":"eda(\"campaign\")","bd03a60b":"eda(\"pdays\")","9ab45d4f":"eda(\"previous\")","8bb6c7fb":"eda(\"poutcome\")","5ce5dd17":" cat = sns.catplot(y='deposit', kind=\"count\",\n            palette=\"pastel\", edgecolor=\".6\",\n            data=df);","dc2aa72c":"df['deposit'].value_counts().plot.bar(figsize=(10,5)); ","c7225e74":"# get dataframe of categorical values after one hot encoding\nencoder = OneHotEncoder(drop='first')\narry = []\nfor col in df.columns:\n    if df[col].dtype == 'object':\n        arry.append(pd.DataFrame(encoder.fit_transform(df[[col]]).toarray()).add_suffix(col))\n        \ncat_df = pd.concat([arry[0],arry[1],arry[2],arry[3],arry[4],arry[5],arry[6],arry[7],arry[8],arry[9]], axis=1)\ncat_df.head()","d0528326":"# get all numeric columns after normalizing\nscaler = StandardScaler()\narry =[]\nfor col in df.columns:\n    if df[col].dtype != 'object':\n        arry.append(pd.DataFrame(scaler.fit_transform(df[[col]]), columns=[col]))\n        \nnumeric_df = pd.concat([arry[0],arry[1],arry[2],arry[3],arry[4],arry[5],arry[6]], axis=1)\nnumeric_df","9e6631c9":"y = cat_df['0deposit']\nX = pd.concat([cat_df.drop(['0deposit'], axis=1), numeric_df], axis=1)","7c74a640":"X_train,X_test,y_train,y_test = train_test_split(X ,y, test_size = 0.3, random_state=42)","299198e1":"dt_model = DecisionTreeClassifier()\ndt_model.fit(X_train,y_train)\ny_pred = dt_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\ndt_model.score(X_test,y_test)\n\n# then random forest will perform better","4356f62b":"rf_model = RandomForestClassifier()\nrf_model.fit(X_train,y_train)\ny_pred = rf_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nrf_model.score(X_test,y_test)","e26864dc":"knn_model = KNeighborsClassifier()\nknn_model.fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nknn_model.score(X_test,y_test)","52eb6432":"xg_model = XGBClassifier(verbosity = 0)\nxg_model.fit(X_train,y_train)\ny_pred = knn_model.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nknn_model.score(X_test,y_test)","42868194":"def clustering_approach(X,y, models,type = \"none\"):\n    \n    dfs = {}\n    X_cls = {}\n    y_cls = {}\n    X_scaled = {}\n    X_train, X_test, y_train, y_test = {},{},{},{}\n    y_pred = {}\n    models_out = {}\n    \n    # create knn model and predict\n    knn_clf = KNeighborsClassifier()\n    knn_clf.fit(X,y)\n    df = pd.concat([X,y],axis=1) # so we can later separate x and y for each cluster\n    df['knn_clf'] = knn_clf.predict(X)\n    no_cls = knn_clf.classes_\n    # get the dataframes, apply std.scaler, form train, test sets, apply models\n    for cls in knn_clf.classes_:\n        print(\"--------------The {} cluster's results-------------------\".format(cls),end=\"\\n\\n\")\n        dfs[cls] = df[df['knn_clf'] == cls].iloc[:,:-1]\n        \n        X_cls[cls] = dfs[cls].iloc[:,:-1]\n        y_cls[cls] = dfs[cls].iloc[:,-1]\n        scaler = StandardScaler()\n        X_scaled[cls] = scaler.fit_transform(X_cls[cls])\n#         X_scaled[cls] = pd.DataFrame(X_scaled[cls],columns=df.columns[:-1])\n    \n        X_train[cls],X_test[cls],y_train[cls],y_test[cls] = train_test_split(X_scaled[cls],y_cls[cls],test_size=0.3,random_state=3)\n        print(\"here\")\n        # type can be used for analyzing... eg: confusion matrix\n        for model in models:\n            model.fit(X_train[cls], y_train[cls])\n            y_pred[cls] = model.predict(X_test[cls])\n            print(model)\n            print(model.score(X_test[cls],y_test[cls]))\n            print(confusion_matrix(y_pred[cls], y_test[cls]), end=\"\\n\\n\")\n            models_out[str(model) + str(cls)] = model\n            \n    \n    return [X_train, X_test, y_train, y_test,knn_clf, models_out]","d7f614ce":"models = [LogisticRegression(), RandomForestClassifier(), KNeighborsClassifier()\n          , XGBClassifier(verbosity = 0),LGBMClassifier(),SVC()] \n","e5c214b8":"X_train_clus, X_test_clus, y_train_clus, y_test_clus,clusterer, models = clustering_approach(X,y,models)","600a8c70":"The first is the familiar boxplot(). This kind of plot shows the three quartile values of the distribution along with extreme values. The \u201cwhiskers\u201d extend to points that lie within 1.5 IQRs of the lower and upper quartile, and then observations that fall outside this range are displayed independently. This means that each value in the boxplot corresponds to an actual observation in the data.","a7d58813":"# Imports","612b36f9":"As the size of the dataset grows, categorical scatter plots become limited in the information they can provide about the distribution of values within each category. When this happens, there are several approaches for summarizing the distributional information in ways that facilitate easy comparisons across the category levels","3dbfca41":"https:\/\/www.statisticshowto.com\/probability-and-statistics\/descriptive-statistics\/box-plot\/","980da189":"### Lets go through each feature","db0f66f2":"# Model building","5e78f8c0":"A point plot represents an estimate of central tendency for a numeric variable by the position of scatter plot points and provides some indication of the uncertainty around that estimate using error bars.","6dea70c4":"A related function, boxenplot(), draws a plot that is similar to a box plot but optimized for showing more information about the shape of the distribution. It is best suited for larger datasets:","9e0008dd":"### Ok guys, i want to explore some more plots today so i'll be referecing the seaborn documentation about how to plot data when one of the features is categorical...\n\nhttps:\/\/seaborn.pydata.org\/tutorial\/categorical.html","2473a7db":"### Don't forget to leave a like or upvote if this was worth your time","2e465bf1":"**Note on Outliers:**\n\nData sets can sometimes contain outliers that are suspected to be anomalies (perhaps because of data collection errors or just plain old flukes). If outliers are present, the whisker on the appropriate side is drawn to 1.5 * IQR rather than the data minimum or the data maximum. Small circles or unfilled dots are drawn on the chart to indicate where suspected outliers lie. Filled circles are used for known outliers.\n\nother info... do read\n\nhttps:\/\/www.purplemath.com\/modules\/boxwhisk3.htm#:~:text=The%20%22interquartile%20range%22%2C%20abbreviated,box%2Dand%2Dwhisker%20plot.&text=The%20IQR%20tells%20how%20spread,far%22%20from%20the%20central%20value.","abdca58e":"#### XGBoost performs the best individually on each cluster","8385f8d7":"# EDA","66e68637":"# Feature Engineering","68022dc3":"https:\/\/mode.com\/blog\/violin-plot-examples\/\n\nThe box plot is an old standby for visualizing basic distributions. It's convenient for comparing summary statistics (such as range and quartiles), but it doesn't let you see variations in the data. For multimodal distributions (those with multiple peaks) this can be particularly limiting.\n\nBut fret not\u2014this is where the violin plot comes in. A violin plot is a hybrid of a box plot and a kernel density plot, which shows peaks in the data.\n\nthe same summary statistics as box plots:\n\n - the white dot represents the median\n\n - the thick gray bar in the center represents the interquartile range\n\n - the thin gray line represents the rest of the distribution, except for points that are determined to be \u201coutliers\u201d using a method that is a function of the interquartile range.","fd51e566":"**How to read? or what info you get?**\n\nThe box and whiskers chart shows you how your data is spread out. Five pieces of information (the \u201cfive number summary\u201c) are generally included in the chart:\n\n- The minimum (the smallest number in the data set). The minimum is shown at the far left of the chart, at the end of the left \u201cwhisker.\u201d or sleeping T shape\n\n- First quartile, Q1, is the far left of the box (or the far right of the left whisker).\n\n- The median is shown as a line in the center of the box.\n\n- Third quartile, Q3, shown at the far right of the box (at the far left of the right whisker).\n\n- The maximum (the largest number in the data set), shown at the far right of the box","512b48b4":"### There is a clustering approach that we can use... first we cluster the data then we determine which model performs best for each of the clusters.... later this way we can while predicting find the cluster a set of data belongs to and use the model which performs best for that cluster\n\nand ofcourse we need more metrics then just accuracy score","bc18e804":"### we can try to transform the features but logistic is not a good option here at all... most of the numeric data is really skewed, decision tree will work, if we use something like k nearest neighbours then it will be helpful if we perform feature scaling so lets just start off with that","edd7650c":"### This is a classification problem... i think it is about bank marketing analysis (it wasn't specified but there is a similar dataset available on kaggle by the name)","7324e648":"# Thank you"}}