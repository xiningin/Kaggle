{"cell_type":{"26f8eeca":"code","ef50a116":"code","27d33b4d":"code","07700436":"code","40e2a7a7":"code","b97c5dad":"code","dc699b69":"code","bb43086d":"code","3f8c22b8":"code","15316de9":"code","d50ee665":"code","c6336b28":"code","bace9d2c":"code","da6177ff":"code","346bb75f":"code","1c840659":"code","f8eb45ff":"code","834f8c40":"code","8c64538d":"code","17905c05":"code","02e9d617":"code","d2678eae":"code","b390db18":"code","1bc75d33":"markdown","29ab5515":"markdown","9a868f5a":"markdown","b510f8be":"markdown","3387b286":"markdown","33cc2722":"markdown","743d7fa9":"markdown","cbe441dc":"markdown"},"source":{"26f8eeca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ef50a116":"# Import dataset as train\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv', nrows=2005000)\ntrain.info()","27d33b4d":"# Drop rows with 'weight'=0 \n# Trades with weight = 0 were intentionally included in the dataset for completeness, \n# although such trades will not contribute towards the scoring evaluation\ntrain = train[train['weight']!=0]\n\n# Create 'action' column (dependent variable)\n# The 'action' column is defined as such because of the evaluation metric used for this project.\n# We want to maximise the utility function and hence pi where pi=\u2211j(weightij\u2217respij\u2217actionij)\n# Positive values of resp will increase pi\ntrain['action'] = train['resp'].apply(lambda x:x>0).astype(int)","07700436":"features = [col for col in list(train.columns) if 'feature' in col]","40e2a7a7":"X = train[features]\ny = train['action']\n\n# Next, we hold out part of the training data to form the hold-out validation set\ntrain_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.2)","b97c5dad":"# First, we want to check if the target class is balanced or unbalanced in the training data\nsns.set_palette(\"colorblind\")\nax = sns.barplot(train_y.value_counts().index, train_y.value_counts()\/len(train_y))\nax.set_title(\"Proportion of trades with action=0 and action=1\")\nax.set_ylabel(\"Percentage\")\nax.set_xlabel(\"Action\")\nsns.despine();\n# Target class is fairly balanced with almost 50% of trades corresponding to each action","dc699b69":"# Next, we plot a diagonal correlation heatmap to see if there are strong correlations between the features\n\n# Compute the correlation matrix\n#corr = train_x.corr()\n\n# Generate a mask for the upper triangle\n#mask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Set up the matplotlib figure\n#f, ax = plt.subplots(figsize=(12, 10))\n\n# Generate a custom diverging colormap\n#cmap = sns.diverging_palette(20, 230, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\n#sns.heatmap(corr, mask=mask, cmap=cmap, vmin=-1, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n\n# There are strong correlations between several of the features","bb43086d":"# Finally, we investigate if there are missing values and we impute them\nmissing_values = pd.DataFrame()\nmissing_values['feature'] = features\nmissing_values['num_missing'] = [train_x[i].isna().sum() for i in features]\nmissing_values.T\n# There are quite a lot of missing values across the features","3f8c22b8":"train_median = train_x.median()\n# Impute medians in both training set and the hold-out validation set\ntrain_x = train_x.fillna(train_median)\nvalid_x = valid_x.fillna(train_median)","15316de9":"# Before we perform PCA, we need to normalise the features so that they have zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(train_x)\ntrain_x_norm = scaler.transform(train_x)\n\npca = PCA()\ncomp = pca.fit(train_x_norm)\n\n# We plot a graph to show how the explained variation in the 129 features varies with the number of principal components\nplt.plot(np.cumsum(comp.explained_variance_ratio_))\nplt.grid()\nplt.xlabel('Number of Principal Components')\nplt.ylabel('Explained Variance')\nsns.despine();\n\n# The first 15 principal components explains about 80% of the variation\n# The first 40 principal components explains about 95% of the variation","d50ee665":"# Using the first 70 principal components, we apply the PCA mapping\n# From here on, we work with only 70 features instead of the full set of 129 features\npca = PCA(n_components=70).fit(train_x_norm)\ntrain_x_transform = pca.transform(train_x_norm)","c6336b28":"# Transform the validation set\nvalid_x_transform = pca.transform(scaler.transform(valid_x))","bace9d2c":"# We create the XGboost-specific DMatrix data format from the numpy array. \n# This data structure is optimised for memory efficiency and training speed\ndtrain = xgb.DMatrix(train_x_transform, label=train_y)\ndvalid = xgb.DMatrix(valid_x_transform, label=valid_y)","da6177ff":"# The objective function is passed an Optuna specific argument of trial\ndef objective(trial):\n    \n# params specifies the XGBoost hyperparameters to be tuned\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 200, 600),\n        'max_depth': trial.suggest_int('max_depth', 10, 25),\n        'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n        'subsample': trial.suggest_uniform('subsample', 0.50, 1),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.50, 1),\n        'gamma': trial.suggest_int('gamma', 0, 10),\n        'tree_method': 'gpu_hist',  \n        'objective': 'binary:logistic'\n    }\n    \n    bst = xgb.train(params, dtrain)\n    preds = bst.predict(dvalid)\n    pred_labels = np.rint(preds)\n# trials will be evaluated based on their accuracy on the test set\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy","346bb75f":"if __name__ == \"__main__\":\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=30, timeout=600)\n\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","1c840659":"best_params = trial.params\nbest_params['tree_method'] = 'gpu_hist' \nbest_params['objective'] = 'binary:logistic'","f8eb45ff":"# Fit the XGBoost classifier with optimal hyperparameters\noptimal_clf = xgb.XGBClassifier(**best_params)","834f8c40":"optimal_clf.fit(train_x_transform, train_y)","8c64538d":"# Plot how the best accuracy evolves with number of trials\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show();","17905c05":"# We can also plot the relative importance of different hyperparameter settings\nfig = optuna.visualization.plot_param_importances(study)\nfig.show();","02e9d617":"# We impute the missing values with the medians\ndef fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","d2678eae":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","b390db18":"for (test_df, sample_prediction_df) in iter_test:\n    wt = test_df.iloc[0].weight\n    if(wt == 0):\n        sample_prediction_df.action = 0 \n    else:\n        sample_prediction_df.action = optimal_clf.predict(pca.transform(scaler.transform(fillna_npwhere(test_df[features].values,train_median[features].values))))\n    env.predict(sample_prediction_df)","1bc75d33":"# 3) Exploratory data analysis","29ab5515":"# 1) Import important libraries and packages","9a868f5a":"# 5) Train XGBoost classifier + Tune hyperparameters using Optuna","b510f8be":"# Kaggle Project: Identifying Profitable Trades using XGBoost\n\nThe efficient market hypothesis posits that markets cannot be beaten because asset prices will always reflect the fundamental value of the assets. In a perfectly efficient market, buyers and sellers would have all the agency and information needed to make rational trading decisions. \n\nIn reality, financial markets are not efficient. The purpose of this trading model is to identify arbitrage opportunities to \"buy low and sell high\". In other words, we exploit market inefficiencies to identify and decide whether to execute profitable trades.\n\nThe dataset, provided by Jane Street, contains an anonymized set of 129 features representing real stock market data. Each row in the dataset represents a trading opportunity, for which I predict an action value: 1 to make the trade and 0 to pass on it. Due to the high dimensionality of the dataset, I use Principal Components Analysis (PCA) to identify features to be used for supervised learning. The intuition is to compress the dataset and use it more efficiently. I then use XGBoost (extreme gradient boosting) - a hugely popular ML library due to its superior execution speed and model performance - to predict profitable trades. I also use Optuna (an automatic hyperparameter optimization software framework) to tune the hyperparameters of the classification model.\n\nPlease upvote if you find this notebook helpful! \ud83d\ude0a Thank you! I would also be very happy to receive feedback on my work.","3387b286":"# 6) Fit classifier on unseen test set","33cc2722":"# Acknowledgements\nhttps:\/\/towardsdatascience.com\/pca-using-python-scikit-learn-e653f8989e60\n\nhttps:\/\/www.kaggle.com\/saurabhshahane\/voting-classifier-beginners\n\nhttps:\/\/www.kaggle.com\/harshitt21\/jane-street-basic-eda-xgb-baseline\n\nhttps:\/\/www.kaggle.com\/eudmar\/jane-street-eda-pca-ensemble-methods\n\nhttps:\/\/www.kaggle.com\/gogo827jz\/optimise-speed-of-filling-nan-function?scriptVersionId=48926407\n\nhttps:\/\/github.com\/datacamp\/Machine-Learning-With-XGboost-live-training\/blob\/master\/notebooks\/Machine-Learning-with-XGBoost-solution.ipynb\n\nhttps:\/\/www.kaggle.com\/marketneutral\/purged-time-series-cv-xgboost-optuna\n\nhttps:\/\/www.kaggle.com\/miklgr500\/optuna-xgbclassifier-parameters-optimize\n\nhttps:\/\/github.com\/optuna\/optuna\/blob\/master\/examples\/xgboost_simple.py","743d7fa9":"# 2) Load and clean dataset","cbe441dc":"# 4) Principal Components Analysis"}}