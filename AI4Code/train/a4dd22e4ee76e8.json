{"cell_type":{"7de7fc2a":"code","bdb5b5d8":"code","16da3b7c":"code","31b32052":"code","7cc4b1c4":"code","73c9e5a6":"code","55f0fb33":"code","76afbc17":"code","bd547b57":"code","7cee85a1":"code","f851d7c0":"code","21550bcf":"code","beb34d57":"code","a34c436f":"code","964d32bf":"code","cc95c6e3":"code","30a834b8":"code","bb9cf15d":"code","d5940a5a":"code","e49526aa":"code","0ca7a5b1":"code","1d90a44a":"code","3a2bf888":"code","dffeff30":"code","cbac749e":"code","8dc87f7c":"code","ab15e3e6":"code","e4497629":"code","1ffa54ca":"code","a075ce0a":"code","c7443079":"code","a562516c":"code","62552e67":"code","8f9eaa48":"code","30193839":"code","d0c92fd4":"code","a726e5de":"code","8c17f41d":"markdown","05635b7b":"markdown","27d9c79b":"markdown","fc901779":"markdown","a90ef373":"markdown","4ef933a3":"markdown","fdde5092":"markdown","54159644":"markdown","4b3e29cf":"markdown","44eaade1":"markdown","46d1f2a1":"markdown","00b59b1a":"markdown","364c5cec":"markdown","af9340fe":"markdown","de480c85":"markdown","0f33474e":"markdown","e830bc6a":"markdown","01833742":"markdown","beef9c08":"markdown","298b90d7":"markdown","676d2c15":"markdown","2214a25b":"markdown","8ff134a4":"markdown","67365b5d":"markdown"},"source":{"7de7fc2a":"from IPython.core.display import HTML\nHTML('<div class=\"tenor-gif-embed\" data-postid=\"5662253\" data-share-method=\"host\" data-width=\"100%\" data-aspect-ratio=\"1.7913669064748199\"><a href=\"https:\/\/tenor.com\/view\/gotta-catch-em-all-gif-5662253\">Gotta Catch Em All GIF<\/a> from <a href=\"https:\/\/tenor.com\/search\/gottacatchemall-gifs\">Gottacatchemall GIFs<\/a><\/div><script type=\"text\/javascript\" async src=\"https:\/\/tenor.com\/embed.js\"><\/script>')","bdb5b5d8":"import re\n# import tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nimport os\nfrom IPython.display import display\n# print(\"Tensorflow version \" + tf.__version__)\n\n# example of using the vgg16 model as a feature extraction model\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.applications.vgg16 import decode_predictions\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\n","16da3b7c":"data = pd.read_csv(\"..\/input\/pokemon-images-and-types\/pokemon.csv\")\n# Define function to extract row \ndef get_row(df,col,name):\n    return df[df[col]==name]\n\npokedex = \"..\/input\/pokemon-images-and-types\/images\/images\/\"\nfilenames = os.listdir(pokedex)\npnames, ptypes = [],[]\nfor path in os.listdir(pokedex):\n    name = re.search(r\"^([^.]*).*\", path).group(1)\n    pnames.append(name.capitalize())\n    ptypes.append(data['Type1'][int(get_row(data,'Name',name).index.values)]) \n        \nN_IMAGES = len(filenames)\nprint(\"Number of pokemon: \", N_IMAGES)\n# Create new data frame\nlst1 = filenames\nlst2 = pnames\nlst3 = ptypes \npokemon_data = pd.DataFrame({'Filename':lst1,'Pokemon':lst2, 'Primary Type':lst3}) #.sort_values('B')#\ndisplay(data,pokemon_data.head())","31b32052":"fig = plt.figure(16, figsize=(20, 15))\nnum_plots = 8\nimages = [] \nfor i in range(N_IMAGES):        \n    image = mpimg.imread(pokedex + pokemon_data['Filename'][i])\n    images.append(image)\n    if 0 <= i <= 7: \n        name = pokemon_data['Pokemon'][i]\n        pokemon_type = pokemon_data['Primary Type'][i]\n        a = fig.add_subplot(2, num_plots, i + 1).set_title(name+\": \"+pokemon_type, size=15)\n        plt.imshow(image)\n        plt.axis(\"off\")\nplt.show()\nplt.tight_layout()\n\n# CHECK IMAGE SIZES\nsizes = []\nfor i in range(N_IMAGES):\n    sizes.append( mpimg.imread(pokedex + pokemon_data['Filename'][i]).shape)\n\nprint(\"The size of the first image is: \",sizes[0])\nprint(\"The unique sizes of the image dataset: \", np.unique(sizes,axis=0))\n# len(images)\n","7cc4b1c4":"# import cv2     # Image Processing\n# fig = plt.figure(16, figsize=(20, 15)) # Create a figure\n# IMAGE_SIZE =   (64, 64)#  re-shape first two dimensions \n# adj_images, numbered_labels = [], []\n# for i in range(N_IMAGES):     \n#         image = mpimg.imread(pokedex + pokemon_data['Filename'][i])\n#         image = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR) #cv2.COLOR_BGR2RGB) # turns to RGB colors #COLOR_BGR2GRAY) # turn to gray scale#\n#         image = cv2.resize(image, IMAGE_SIZE) # compress the dimensions\n#         adj_images.append(image)\n#         if 0 <= i <= 7:  \n#             name = pokemon_data['Pokemon'][i]\n#             pokemon_type = pokemon_data['Primary Type'][i]\n#             a = fig.add_subplot(2, num_plots, i +1 ).set_title(name+\": \"+pokemon_type, size=15)\n#             plt.imshow(image)\n#             plt.axis(\"off\")\n# plt.show()\n# plt.tight_layout()\n","73c9e5a6":"train, valid, test = np.split(pokemon_data, [int(0.6*len(pokemon_data)), int(0.8*len(pokemon_data))])    \n \n# [len(train)\/len(df),len(valid)\/len(df),len(test)\/len(df)]\n\n# len(np.unique(valid.index.values))\n\ntrain, valid, test = train.reset_index(), valid.reset_index(), test.reset_index()\n\ndisplay(train,valid,test)","55f0fb33":"## Load data using ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n\n# All images will be rescaled by 1.\/255\ndatagen = ImageDataGenerator(rescale=1\/255)\n\n# Flow training images in batches of 32 using train_datagen generator\ntrain_generator = datagen.flow_from_dataframe(\ndataframe=train,\ndirectory=\"..\/input\/pokemon-images-and-types\/images\/images\/\",\nx_col=\"Filename\",\ny_col=\"Primary Type\",\nsubset=\"training\",\nbatch_size=50,\nseed=0,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(120,120))\n\nvalid_generator = datagen.flow_from_dataframe(\ndataframe=valid,\ndirectory=\"..\/input\/pokemon-images-and-types\/images\/images\/\",\nx_col=\"Filename\",\ny_col=\"Primary Type\",\nsubset=\"training\",\nbatch_size=50,\nseed=0,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(120,120))\n\ntest_generator = datagen.flow_from_dataframe(\ndataframe=test,\ndirectory=\"..\/input\/pokemon-images-and-types\/images\/images\/\",\nx_col=\"Filename\",\ny_col=\"Primary Type\",\nsubset=\"training\",\nbatch_size=50,\nseed=0,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(120,120))\n\n\n### CHECK if images are correct\nsample_training_images, _ = next(train_generator)\n\n# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 8, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nplotImages(sample_training_images[:8])","76afbc17":"sample_training_images[0].shape","bd547b57":"# Load the VGG16 model\n# from keras.applications import VGG16\nfrom keras.applications.vgg16 import VGG16\n# https:\/\/machinelearningmastery.com\/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models\/\n# load model without classifier layers\nmodel = VGG16(include_top=False, input_shape=(120, 120, 3))","7cee85a1":"# def CNN_new(train_generator,valid_generator,num_epochs):\n#     # DESIGN  and COMPILE \n#     # CNN architecture: 5*[Convolutional layer + MaxPooling]-> Flatten -> Dense -> Dropout -> Dense (Output)\n#     from keras.models import Sequential\n#     from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n#     # Clear previous models   # Calculate the number of classes in the train set\n#     keras.backend.clear_session()\n#     num_classes = len(mydict)\n#     model = Sequential([\n#         # Note the input shape is the desired size of the image 120 x 120  \n#         # This is the first convolution\n#         Conv2D(16, (3,3), activation='relu', input_shape = sample_training_images[0].shape),\n#         MaxPooling2D(2, 2),\n#         # The second convolution\n#         Conv2D(32, (3,3), activation='relu'),\n#         MaxPooling2D(2,2),\n#         # The third convolution\n#         Conv2D(64, (3,3), activation='relu'),\n#         MaxPooling2D(2,2),\n#         # The fourth convolution\n#         Conv2D(64, (3,3), activation='relu'),\n#         MaxPooling2D(2,2),\n#         # The fifth convolution\n#         Conv2D(64, (3,3), activation='relu'),\n#         MaxPooling2D(2,2),\n#         # Flatten the results to feed into a DNN\n#         Flatten(),\n#         # 512 neuron hidden layer\n#         Dense(1024, activation='relu'),\n# #         Dropout(0.2),\n#         Dense(num_classes, activation='sigmoid')\n#     ])\n\n#     model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n#     # model.summary()\n\n#     history = model.fit_generator(\n#           train_generator,\n#           steps_per_epoch=10,  \n#           epochs=num_epochs,\n#           validation_data = valid_generator,\n#           validation_steps=10,\n#           verbose=2)\n#     return history\n\n# # \n","f851d7c0":"# # get layers from VGG change output\ninputs = model.input                                          # input placeholder\noutputs = [layer.output for layer in model.layers]  \ndisplay(inputs,outputs)\nmodel.output\nlen(outputs)","21550bcf":"# from keras.models import Sequential\nfrom keras.layers import Flatten, Dense #, Conv2D, MaxPooling2D, Dropout\n# # add new classifier layers\nflatten1 = Flatten(input_shape=(3,3,512))(model.output)\ndense1 = Dense(512, activation='relu')(flatten1)\noutput = Dense(18, activation='softmax')(dense1)\n# define new model\nmodel = keras.Model(inputs=model.inputs, outputs=output)\nmodel.summary()","beb34d57":"# compile\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","a34c436f":"num_epoches = 10\nhistory = model.fit(\n      train_generator,\n      steps_per_epoch=10,  \n      epochs=num_epoches,\n      verbose=2,\n      validation_data = valid_generator,\n      validation_steps=10)\n","964d32bf":"model.fit_generator( train_generator, \n                    epochs=num_epoches, \n                    steps_per_epoch=16,\n                    validation_data=valid_generator,\n                    validation_steps=8)","cc95c6e3":"results = model.evaluate_generator(test_generator, steps=20, verbose=1)\nprint(\"loss: \",results[0],\"accuracy: \",results[1])","30a834b8":"import seaborn as sns\nplt.figure(figsize=(10,5))\nptypes = pokemon_data['Primary Type']\nprint(\"Number of Primary Types:\",len(np.unique(ptypes)))\nplot1 = sns.countplot(ptypes)\nplot1.set_xlabel(\"Pokemon Primary Type\")\nplot1.set_ylabel(\"Frequency\")\nplot1.set_xticklabels(plot1.get_xticklabels(),rotation=45,size=12)\nfor p in plot1.patches:\n    plot1.annotate('{:}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()+1))","bb9cf15d":"pokemon_data.head()\n# Find all types with group sizes greater than 40 \ng40 = [el for el in ptypes.value_counts()[ptypes.value_counts()>40].index]\ndisplay(g40 )\n\ndf=pd.DataFrame([],columns=pokemon_data.columns)\nfor el in g40:\n    data = pokemon_data.set_index('Primary Type').loc[el].reset_index()\n    df = df.append(data)\ndf","d5940a5a":"# Split data \n\ntrain, valid, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])    \n# traindf = \n[len(train)\/len(df),len(valid)\/len(df),len(test)\/len(df)]\n\nlen(np.unique(valid.index.values))\n\ntrain, valid, test = train.reset_index(), valid.reset_index(), test.reset_index()\n\n# train, valid, test\nvalid.nunique()\n# str_types = ['Water','Fire']\n# train2 = train.set_index('Primary Type').loc[str_types].reset_index()\n# test2 = valid.set_index('Primary Type').loc[str_types].reset_index() ","e49526aa":"## Load data using ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# All images will be rescaled by 1.\/255\ndatagen = ImageDataGenerator(rescale=1\/255)\n\n# Flow training images in batches of 32 using train_datagen generator\ntrain_generator = datagen.flow_from_dataframe(\ndataframe=train,\ndirectory=\"..\/input\/pokemon-images-and-types\/images\/images\/\",\nx_col=\"Filename\",\ny_col=\"Primary Type\",\nsubset=\"training\",\nbatch_size=32,\nseed=19,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(120,120))\n\nvalid_generator = datagen.flow_from_dataframe(\ndataframe=valid,\ndirectory=\"..\/input\/pokemon-images-and-types\/images\/images\/\",\nx_col=\"Filename\",\ny_col=\"Primary Type\",\nsubset=\"training\",\nbatch_size=32,\nseed=102,\nshuffle=True,\nclass_mode=\"categorical\",\ntarget_size=(120,120))\n\n\n### CHECK if images are correct\nsample_training_images, _ = next(train_generator)\n\n# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\ndef plotImages(images_arr):\n    fig, axes = plt.subplots(1, 8, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nplotImages(sample_training_images[:8])","0ca7a5b1":"vgg_model = VGG16(include_top=False, input_shape=(120, 120, 3))\ndef VGG(traindf,validdf,num_epochs):\n    # All images will be rescaled by 1.\/255\n    datagen = ImageDataGenerator(rescale=1\/255)\n    # Flow training images in batches of 32 using train_datagen generator\n    train_generator = datagen.flow_from_dataframe(dataframe=traindf,\n    directory=\"..\/input\/pokemon-images-and-types\/images\/images\/\",\n    x_col=\"Filename\", y_col=\"Primary Type\",\n    subset=\"training\", batch_size=32, seed=1,\n    class_mode=\"categorical\",target_size=(120,120))\n    # valid\n    valid_generator = datagen.flow_from_dataframe(dataframe=validdf,\n    directory=\"..\/input\/pokemon-images-and-types\/images\/images\/\",\n    x_col=\"Filename\", y_col=\"Primary Type\",\n    subset=\"training\", batch_size=32, seed=1,\n    class_mode=\"categorical\",target_size=(120,120))\n    # DESIGN\n    # CNN architecture: 5*[Convolutional layer + MaxPooling]-> Flatten -> Dense -> Dropout -> Dense (Output)\n    from keras.models import Sequential\n    from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n    # # add new classifier layers\n    flatten1 = Flatten(input_shape=(3,3,512))(vgg_model.output)\n    dense1 = Dense(512, activation='relu')(flatten1)\n    num_classes = len(traindf['Primary Type'].unique())\n    output = Dense(num_classes, activation='softmax')(dense1)\n    # define new model\n    model = keras.Model(inputs=vgg_model.inputs, outputs=output)\n \n    # Clear previous models\n#     keras.backend.clear_session()\n#     num_classes = len(traindf['Primary Type'].unique())\n#     model = Sequential([       \n#         Conv2D(16, (3,3), activation='relu', input_shape = sample_training_images[0].shape), MaxPooling2D(2, 2),\n#         Conv2D(32, (3,3), activation='relu'), MaxPooling2D(2,2),\n#         Conv2D(64, (3,3), activation='relu'), MaxPooling2D(2,2),\n#         Conv2D(64, (3,3), activation='relu'), MaxPooling2D(2,2),\n#         Conv2D(64, (3,3), activation='relu'), MaxPooling2D(2,2),\n#         # Flatten the results to feed into a DNN\n#         Flatten(), \n#         Dense(512, activation='relu'),\n#         Dense(num_classes, activation='sigmoid')\n#     ])\n    # compile\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # fit\n    history = model.fit(train_generator, steps_per_epoch=10, epochs=num_epochs, verbose=2,\n          validation_data = valid_generator, validation_steps=10)\n    return history\n\nhistory1 = VGG(train,valid,10)\n","1d90a44a":"#  Dataset with only ['Water','Normal','Bug']\nnum_epoches = 10\nstr_types = ['Water','Grass','Fire']\ntrain2 = train.set_index('Primary Type').loc[str_types].reset_index()\ntest2 = valid.set_index('Primary Type').loc[str_types].reset_index()  \nVGG(train2,test2,num_epoches)\nresults = model.evaluate_generator(test_generator, steps=20, verbose=1)\nprint(\"loss: \",results[0],\"accuracy: \",results[1])","3a2bf888":"#  Dataset with only Water and Normal Pokemon\nstr_types = ['Water','Fire']\ntrain2 = train.set_index('Primary Type').loc[str_types].reset_index()\ntest2 = valid.set_index('Primary Type').loc[str_types].reset_index()  \nVGG(train2,test2,10)\n\n\n","dffeff30":"names = ['Pikachu','Bulbasaur','Charmander', 'Squirtle','Snorlax']\n# pokemon_data.set_index(['Pokemon''])#.loc[names]#['Filename'].values \n\n# pokemon_data.loc[[n in pokemon_data.Pokemon for n in names]]\ntarget_ind = [np.argmax(n == pokemon_data.Pokemon) for n in names]\n# pokemon_data.loc[target_ind[0]]\n\n ","cbac749e":"### Image Augmentation\nimport math\nfrom numpy import expand_dims\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom time import time\nimport cv2\nstart= time()\n# re.search(r\"^([^.]*).*\", lst1a[1]).group(1)\ndef pokemon_aug(images,num):\n    imgs = []    \n    for i in range(50):\n        # expand dimension to one sample\n        image = cv2.cvtColor(images[num], cv2.COLOR_BGRA2BGR)  # IMPORTANT HERE TO CONVERT TO 3 DIMS\n        samples = expand_dims(image, 0)\n        # create image data augmentation generator\n        datagen =  ImageDataGenerator(1\/255,\n          rotation_range=20,\n          width_shift_range=0.2,\n          height_shift_range=0.2,\n          shear_range=0.2,\n          zoom_range=0.2,\n          horizontal_flip=True,\n          fill_mode='nearest')\n        it = datagen.flow(samples, batch_size=1)\n        batch = it.next()[0]\n        imgs.append(batch)\n    return np.array(imgs,dtype=float)\n\n# Create augmented images of pokemon \n# Record indices and types from original pokemon table\naug_images,aug_index,aug_names = [],[],[]\nfor num in target_ind:\n    aug_img = pokemon_aug(images,num)\n    aug_images.extend(aug_img)\n    # Record index from original pokemon table\n#     aug_index.extend(len(aug_img) * [num])\n    # Record types of the pokemon\n    aug_names.extend(len(aug_img)* [pokemon_data['Pokemon'][num]])    \nend= time()\nend-start","8dc87f7c":"fig = plt.figure(10, figsize=(20, 15))\nfig.subplots_adjust(hspace=0.5, wspace=0.1)\nfor i in range(100):  \n#     n = aug_index[i]\n    a = fig.add_subplot(10, 10, i+1 ).set_title( aug_names[i], size=12)\n    plt.imshow(aug_images[i])                                              \n    plt.axis(\"off\")\n","ab15e3e6":"# Convert categorical variables \nfrom keras.utils.np_utils import to_categorical \ndef convert2numbers(Y):\n    mydict = {types:i for i, types in enumerate(np.unique(Y))}\n    # def convert2num(string):\n    #     return int(np.argwhere([str(a) == string for a in mydict.keys()]))  \n    dummy_labels = [int(el) for el in pd.DataFrame(Y).replace(mydict).values]\n    #     dummy_labels = [convert2num(el) for el in Y]\n    return to_categorical(dummy_labels,num_classes = len(mydict)), mydict\n\n#     return np.array(dummy_labels, dtype = 'int32') \nY , mydict = convert2numbers(aug_names)","e4497629":"# Convert back to label\ndef convert2labels(Y,mydict):\n    return [list(mydict.keys())[(np.argwhere(Y)[n][1])] for n in range(len(Y))]\nconvert2labels([Y[50,:]],mydict)\n","1ffa54ca":"# Split Data\nfrom sklearn.model_selection import train_test_split\nX = np.array(aug_images,dtype=float)\n# Y = np.array(dummy_labels, dtype = 'int32') \nXtrain, Xvalid, Ytrain, Yvalid = train_test_split(X, Y, test_size=0.3)\ndisplay(Xtrain.shape,Xvalid.shape)\n# Ytrain\n\n# Fit to Generator \ndatagen = ImageDataGenerator()\ndatagen.fit(Xtrain)\ntrain_generator = datagen.flow(Xtrain, Ytrain, batch_size = 1000) # Take large batch size \nvalid_generator = datagen.flow(Xvalid, Yvalid, batch_size = 1000)\n","a075ce0a":"vgg_model = VGG16(include_top=False, input_shape=(120, 120, 3))\ndef pokedex(train_generator, valid_generator,num_epochs):\n    # 5 charcter pokedex\n    num_classes = 5\n    # DESIGN\n    # CNN architecture: 5*[Convolutional layer + MaxPooling]-> Flatten -> Dense -> Dropout -> Dense (Output)\n    from keras.models import Sequential\n    from keras.layers import Flatten, Dense, Conv2D, MaxPooling2D, Dropout\n    # # add new classifier layers\n    flatten1 = Flatten(input_shape=(3,3,512))(vgg_model.output)\n    dense1 = Dense(512, activation='relu')(flatten1)\n    output = Dense(num_classes, activation='softmax')(dense1)         \n    # define new model\n    model = keras.Model(inputs=vgg_model.inputs, outputs=output)\n\n    # compile\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    # fit\n    history = model.fit(train_generator, steps_per_epoch=10, epochs=num_epochs, verbose=2,\n          validation_data = valid_generator, validation_steps=10)\n    return history\n\n","c7443079":"# fit \nhistory = pokedex(train_generator, valid_generator,15)","a562516c":"cd \/kaggle\/working\/","62552e67":"# !pwd\n# directory=\"..\/input\/pokemon-images-and-types\/images\/images\/\",\nimport os \nimport shutil \nos.mkdir('\/kaggle\/working\/pikachu')\nos.mkdir('\/kaggle\/working\/snorlax')","8f9eaa48":"dir = \"..\/input\/pokemon2\"\nfile_from = os.path.join(dir+\"\/pikachu.jpg\")\nshutil.copy(file_from, '\/kaggle\/working\/pikachu')\n\nfile_from = os.path.join(dir+\"\/snorlax.jpg\")\nshutil.copy(file_from, '\/kaggle\/working\/snorlax')\n# for file in source_files:\n#     file_from = os.path.join(dir+\"\/\"+file)\n#     print(\"copying {} to {}\".format(file_from, new_folder))\n#     shutil.copy(file_from, new_folder)","30193839":"# Flow training images in batches of 32 using train_datagen generator\ndatagen = ImageDataGenerator()\ntest_generator = datagen.flow_from_directory(\n# directory=\"..\/input\/pokemon\",\ndirectory=\"\/kaggle\/working\",\nshuffle = False,\nclass_mode=\"binary\",\ncolor_mode=\"rgb\",\ntarget_size=(120,120),\nclasses=['pikachu','snorlax'],\nbatch_size=1)","d0c92fd4":"results = history.model.predict_generator(test_generator, steps=3, verbose=1)\nresults","a726e5de":"convert2labels(results,mydict)","8c17f41d":"\n\n# What's that Pokemon? - Convolutional Neural Networks\n\n\n\n[Edward Toth, PhD, University of Sydney]\n\n- e-mail: eddie_toth@hotmail.com\n- Add me on: https:\/\/www.linkedin.com\/in\/edward-toth\/ \n- Join the community: https:\/\/www.meetup.com\/Get-Singapore-Meetup-Group\/\n- Data Avenger: https:\/\/data-avenger.mailchimpsites.com\/\n\n\nThree things you'll learn:\n1. Transfer learning \n2. Image generators\n3. Frequency problem\n\n\n\n\n\nDatasets: <br>\nhttps:\/\/www.kaggle.com\/vishalsubbiah\/pokemon-images-and-types <br>","05635b7b":"## If training set and validation set do not both have 18 classes, reload notebook","27d9c79b":"Generator ","fc901779":"### Model Performance (10 epoches)\n- Approximate accuracy (18 Categories): 0.1 - 0.16\n","a90ef373":"* ### The END!","4ef933a3":"If the above download fails, then create your own CNN","fdde5092":"### Model Performance (10 epoches)\n- Accuracy (18 Categories): 0.1 - 0.17\n- Accuracy (7 Categories*): 0.14 - 0.3\n- Accuracy (3 Categories*): 0.5 - 0.73\n- Accuracy (2 Categories*): 0.6 - 0.94\n\n\\* these categories have at leat 40 images \n\n## PART 1 DONE","54159644":"### View images and check sizes","4b3e29cf":"## ImageDataGenerator\n\n- Problem: load a dataset but not enough memory on your device.\n- Solution:  data generators for loading and processing images for deep learning models.\n\n\nFurther Resources:\n- https:\/\/towardsdatascience.com\/keras-data-generators-and-how-to-use-them-b69129ed779c\n- https:\/\/machinelearningmastery.com\/how-to-load-large-datasets-from-directories-for-deep-learning-with-keras\/","44eaade1":"<a id = \"3\"><\/a><br>\n## Data Prepration\n- View images and check sizes\n- Split images into train, valid, test set \n- Introducing ImageDataGenerator\n\n[Back to Top](#TOP)","46d1f2a1":" \n## <center> WTF is Pandas: Online Course <center> \n    Stage 1   \n    - Learn to speak Python-tongue\n    (Python basics, Conditional\/Loop statements, Functions)\n    Stage 2 \n    - Unleash your inner Pandas\n    (Create Series\/DataFrame, Styling, Read files and so on)\n    Stage 3 \n    - Mystical arts of Data Manipulation\n    (Clean, Prepare, Merge data and much more Magic)\n    Stage 4 \n    - Mind-Body-Data Connection\n    (Understanding Data with Statistics, Grouping and Visualization) \n    \n\n![PANDAS.jpg](attachment:PANDAS.jpg)","00b59b1a":"<a id = \"5\"><\/a><br>\n# Frequency Problem \nEVEN A CNN WITH so MANY LAYERS, it's hard to classify pokemon based on their type. WHY?\n- Sh**ty results dues to low frequency for particular primary types \n- Hard to differientiate certain Pokemon because they have multiple types\n- Visualize the frequency of different Pokemon primary types ","364c5cec":"#### Previous Tutorial:\n- CIFAR-10 (horse, automobile, ship, etc.) has 6,000 images in each category\n- MNIST (digits 0 to 10) has 6,000 images in each category\n\n\n### What you could do? \n- Classify a Pokemon image with a certain Primary type \n- Classify a Pokemon image with both Primary and Secondary type \n\n### Problem\n- Primary type of Water has at least 100 cases while there are only 3 Pokemon with Flying type \n- For a deep learning techniques, this just isn't enough data! \n\n### Solutions\n- Increase the amount of data in each class (flying, ice, fairy, etc.) \n- Use an existing deep learning model that has been trained on similar images \n- Look at data class with larger samples (>40)\n\n<!--\n### What to do instead? Generative Adversarial Networks (GANs)\n- Generate Pokemon based on training images \n- Convert pokemon primary types (labels) into a numeric dummy variable\n- Also create a function to convert dummy variable back to labels\n-->\n\n## Solution: Look at data class with larger samples (>40)\n","af9340fe":"### View the augmentation of selected pokemon","de480c85":"<a id = \"0\"><\/a><br>\nTable of Contents\n1. [Load Libraries](#1)  \n1. [Read Data](#2)\n1. [Data Preparation](#3) \n1. [LAZY WAY: Transfer Learning](#4)   \n   a) Design <br \/>\n   b) Compile<br \/>\n   c) Fit <br \/>\n   d) Assess <br \/>\n1. [Frequency Problem ](#5)\n\n    ","0f33474e":"<a id = \"2\"><\/a><br>\n## Read data \n\nPredict the primary type of Pokemon based on images:\n- View pokemon number of images\n- Get data into a useable format\n \n","e830bc6a":"### Split images into train, valid, test set","01833742":"## Say HELLooo!\n\n[Edward Toth, PhD, University of Sydney]\n\n- e-mail: eddie_toth@hotmail.com\n- Add me on: https:\/\/www.linkedin.com\/in\/edward-toth\/ \n- Join the community: https:\/\/www.meetup.com\/Get-Singapore-Meetup-Group\/\n- Data Avenger: https:\/\/data-avenger.mailchimpsites.com\/","beef9c08":"New Data","298b90d7":"<a id = \"1\"><\/a><br>\n## Load Libraries","676d2c15":"<a id = \"6\"><\/a><br>    \n### 2. Compile\n`model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])`\n- Loss: `categorical_crossentropy` is used for because data is in categories with primary Pokemon types (water, fire, grass, etc.)\n- Optimizer: `adam` has an adaptive learning rate (updates the model), useful for deep learning.\n- Metrics: `accuracy` for training set is recorded.\n\n","2214a25b":"<a id = \"7\"><\/a><br>\n### 3. Fit\n\n- train_generator, valid_generator\n-  steps_per_epoch\n-  validation_steps","8ff134a4":"\n# Create a Pokedex using Data Augmentation\n[https:\/\/machinelearningmastery.com\/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks\/]\n\nImage augmentation involes adding slight adjustments to each of the pokemon images\n- flipping images horizontally\n- rotating the axes\n- shifting width or height, etc. \n\n1. Generate 50 augmented images for each pokemon \n\n2. We also want to record the indices and primary types from original pokemon table for each augmented image.\n\n3. Plot the new frequencies of each category\n\n","67365b5d":"<a id = \"4\"><\/a><br>\n# LAZY WAY: Transfer Learning\n## Convolutional Neural Networks (CNNs) \n\n\n### VGG16 \n\n- Convolutional Neural Network with 16 Layers \n- Pre-trained model on images with 1000 classes (automobile, horse, etc.)\n-  Training size (1.3M images), validation (50K images), and testing (100K images).\n- Input shape has to be (224, 224, 3)\n\n### 1. Design \n- Convolutional filters: extracts image features (like sharpening)\n- MaxPooling Layers reduce the number of image features (like pixelating)\n \n\n\nMore info at:\n- https:\/\/keras.io\/api\/applications\/vgg\/\n- https:\/\/arxiv.org\/pdf\/1409.1556.pdf\n\n\n\n\n    \n\n"}}