{"cell_type":{"82235ebf":"code","86ce33d1":"code","5ce7dac2":"code","b6fde573":"code","a46f6260":"code","06fe036f":"code","4addab34":"code","7fc2eb25":"code","4800ca4f":"code","327cc112":"code","b68029d1":"code","d3f50cb2":"code","63b16806":"code","f72c3806":"code","552e83d1":"code","73f27fca":"code","6b23098c":"code","4af5f0b4":"code","fb4f200a":"code","d98148ec":"code","c90a44b0":"code","574a85ad":"code","0a361cac":"code","1ff71f80":"code","136f7fc2":"code","91cc79a7":"code","0cd5389a":"code","f4c3c934":"code","d1dff5f9":"code","103fd47e":"code","ff612557":"code","6ab5f32c":"code","de1b0ae8":"code","77e887dd":"code","d3887a3e":"code","5784cfbc":"code","c6b17700":"code","849be9ce":"code","711bc894":"markdown","d94adc66":"markdown","40ac0286":"markdown"},"source":{"82235ebf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86ce33d1":"import numpy as np\nimport pandas as pd\n\n# Text prerocessing libraries\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# XG Boost\nimport  xgboost as xgb\nfrom xgboost import XGBClassifier\n\n# Sklearn\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import  f1_score\nfrom sklearn import preprocessing ,decomposition, model_selection,metrics,pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n# Matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n","5ce7dac2":"# Reading the datasets\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nprint('Training data shape..' + str(train.shape))\ntrain.head()","b6fde573":"# Reading the test data\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nprint('Test data shape.. ' + str(test.shape))\ntest.head()","a46f6260":"# Basic EDA\ntrain.isnull().sum()","06fe036f":"test.isnull().sum()","4addab34":"# Exploring the data columns\ntrain['target'].value_counts()","7fc2eb25":"sns.barplot(train['target'].value_counts().index,train['target'].value_counts(),palette='coolwarm')","4800ca4f":"# Let Look at what disaster andn non disaster tweet looks like\ndisaster_tweets = train[train['target']==1]['text']\ndisaster_tweets.values[1]","327cc112":"non_disaster_tweets  = train[train['target']==0]['text']\nnon_disaster_tweets.values[1]","b68029d1":"sns.barplot(y=train['keyword'].value_counts()[:20].index,x=train['keyword'].value_counts()[:20],orient='h')","d3f50cb2":"train.loc[train['text'].str.contains('disaster',na=False,case=False)].target.value_counts()","63b16806":"train.columns","f72c3806":"# Exploring the location columns\ntrain['location'].value_counts()[:5]","552e83d1":"# Replacing the ambigious locations name with Standard names\ntrain['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\n\nsns.barplot(y=train['location'].value_counts()[:5].index,x=train['location'].value_counts()[:5],\n            orient='h')","73f27fca":"# Data Cleaning\n\n# A quick glance over the existing data\ntrain['text'][:5]","6b23098c":"# Applyig=ng a first round of text cleaning technique\n\ndef clean_text(text):\n    '''Make text lower case , removw text in squre backets ,remove links, remove punctuation\n    and remove words containing numbers\n    '''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]','',text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+','',text)\n    text = re.sub('<,*?>+','',text)\n    text = re.sub('[%s]' % re.escape(string.punctuation),'',text)\n    text = re.sub('\\n','',text)\n    text = re.sub('\\w*\\d\\w*','',text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))\n\n# Let's take a look at the updated text\ntrain['text'].head()","4af5f0b4":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(disaster_tweets))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(non_disaster_tweets))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","fb4f200a":"# Tokenizing\ntext = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","d98148ec":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","c90a44b0":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\n\n\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","574a85ad":"# Stemming and Lemmatization examples\ntext = \"feet cats wolves talked\"\n\ntokenizer = nltk.tokenize.TreebankWordTokenizer()\ntokens = tokenizer.tokenize(text)\n\n# Stemmer\nstemmer = nltk.stem.PorterStemmer()\nprint(\"Stemming the sentence: \", \" \".join(stemmer.stem(token) for token in tokens))\n\n# Lemmatizer\nlemmatizer=nltk.stem.WordNetLemmatizer()\nprint(\"Lemmatizing the sentence: \", \" \".join(lemmatizer.lemmatize(token) for token in tokens))","0a361cac":"# After preprocessing, the text format\ndef combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","1ff71f80":"# text preprocessing function\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","136f7fc2":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","91cc79a7":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","0cd5389a":"from sklearn.linear_model import LogisticRegression","f4c3c934":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","d1dff5f9":"clf.fit(train_vectors, train[\"target\"])","103fd47e":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","ff612557":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","6ab5f32c":"clf_NB.fit(train_vectors, train[\"target\"])","de1b0ae8":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","77e887dd":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","d3887a3e":"import xgboost as xgb\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","5784cfbc":"import xgboost as xgb\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","c6b17700":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","849be9ce":"submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)","711bc894":"#### Token normalization\n- Token normalisation means converting different tokens to their base forms. This can be done either by:\n\n- Stemming : removing and replacing suffixes to get to the root form of the word, which is called the stem for instance cats - cat, wolves - wolv\n- Lemmatization : Returns the base or dictionary form of a word, which is known as the lemma","d94adc66":"### TF = (Number of times term t appears in a document)\/(Number of terms in the document)\n##### Inverse Document Frequency: is a scoring of how rare the word is across documents.\n\n### IDF = 1+log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in.","40ac0286":"#### Stopwords Removal\nNow, let's get rid of the stopwords i.e words which occur very frequently but have no possible value like a, an, the, are etc."}}