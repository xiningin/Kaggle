{"cell_type":{"de9f9062":"code","ce0ec222":"code","72b976bb":"code","559628d4":"code","f240ced2":"code","c87ec77d":"code","ae7ff117":"code","ee0b0d6c":"code","b1703d1a":"code","310e26f0":"code","241c7deb":"code","7d98ebd7":"code","e1ae4af1":"code","b0a86275":"code","dac4e471":"code","5b0a39e2":"code","b4a1b61b":"code","20b9b1cf":"code","565b7a36":"code","88ebccb9":"code","6ba20bd6":"code","0663de48":"code","c95612e3":"markdown","bbdd3d5e":"markdown","dff0a1ac":"markdown","e7e3649b":"markdown","0f0b36d0":"markdown","d6f900b6":"markdown","896a5c15":"markdown","6226ae3d":"markdown","ab94ef65":"markdown"},"source":{"de9f9062":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.regularizers import l2","ce0ec222":"train = pd.read_csv('\/kaggle\/input\/challenges-in-representation-learning-facial-expression-recognition-challenge\/train.csv')\nprint(train.shape)","72b976bb":"train.head()","559628d4":"emotion_prop = (train.emotion.value_counts() \/ len(train)).to_frame().sort_index(ascending=True)\n\nemotion_prop","f240ced2":"emotions = ['Angry','Disgust','Fear','Happy','Sad','Surprise','Neutral']","c87ec77d":"palette = ['orchid', 'lightcoral', 'orange', 'gold', 'lightgreen', 'deepskyblue', 'cornflowerblue']\n\nplt.figure(figsize=[12,6])\n\nplt.bar(x=emotions, height=emotion_prop['emotion'], color=palette, edgecolor='black')\n    \nplt.xlabel('Emotion')\nplt.ylabel('Proportion')\nplt.title('Proportion of Emotion Labels')\nplt.show()","ae7ff117":"def pixels_to_array(pixels):\n    array = np.array(pixels.split(),'float64')\n    return array\n\ndef image_reshape(data):\n    image = np.reshape(data['pixels'].to_list(),(data.shape[0],48,48,1))\n    return image","ee0b0d6c":"train['pixels'] = train['pixels'].apply(pixels_to_array)\nX = image_reshape(train)\ny = train['emotion']","b1703d1a":"plt.figure(figsize=[12,12])\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow(X[i],cmap=\"gray\")\n    plt.title(emotions[y[i]])\n    plt.axis(\"off\")\nplt.show()","310e26f0":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=1)","241c7deb":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_valid.shape)\nprint(y_valid.shape)","7d98ebd7":"np.random.seed(1)\ntf.random.set_seed(1)\n\ncnn = Sequential([\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same', input_shape=(48,48,1)),\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.25),\n    BatchNormalization(),\n\n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(128, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n    \n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    Conv2D(64, (3,3), activation = 'relu', padding = 'same'),\n    MaxPooling2D(2,2),\n    Dropout(0.5),\n    BatchNormalization(),\n\n    Flatten(),\n    \n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(64, activation='relu'),\n    Dropout(0.25),\n    BatchNormalization(),\n    Dense(7, activation='softmax')\n])\n\ncnn.summary()","e1ae4af1":"opt = tf.keras.optimizers.Adam(0.001)\ncnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])","b0a86275":"%%time \n\nh1 = cnn.fit(\n    X_train, y_train, \n    batch_size=256,\n    epochs = 20,\n    verbose = 1,\n    validation_data = (X_valid, y_valid)\n)","dac4e471":"history = h1.history\nprint(history.keys())","5b0a39e2":"epoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","b4a1b61b":"tf.keras.backend.set_value(cnn.optimizer.learning_rate, 0.0001)","20b9b1cf":"%%time \n\nh2 = cnn.fit(\n    X_train, y_train, \n    batch_size=256,\n    epochs = 20,\n    verbose = 1,\n    validation_data = (X_valid, y_valid)\n)","565b7a36":"for k in history.keys():\n    history[k] += h2.history[k]\n\nepoch_range = range(1, len(history['loss'])+1)\n\nplt.figure(figsize=[14,4])\nplt.subplot(1,2,1)\nplt.plot(epoch_range, history['loss'], label='Training')\nplt.plot(epoch_range, history['val_loss'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Loss'); plt.title('Loss')\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(epoch_range, history['accuracy'], label='Training')\nplt.plot(epoch_range, history['val_accuracy'], label='Validation')\nplt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.title('Accuracy')\nplt.legend()\nplt.tight_layout()\nplt.show()","88ebccb9":"cnn.save('fer_model_v01.h5')\npickle.dump(history, open(f'fer_v01.pkl', 'wb'))","6ba20bd6":"print(X_train[0].shape)\nprint(type(y_train),type(X_train[0]))#,y_train[0])\ntemp = cnn.predict(X_train[0].reshape((1,48,48,1)))\n#print(temp)\n#print(temp[0].index(max(temp[0])))\n#plt.imshow(X[0],cmap=\"gray\")\n#plt.show()","0663de48":"from sklearn.model_selection import KFold\nimport numpy as np\nnum_folds = 10","c95612e3":"## Split Data","bbdd3d5e":"## Save Model and History","dff0a1ac":"## Build Network","e7e3649b":"## Label Distribution","0f0b36d0":"## Import Packages","d6f900b6":"## Load Training DataFrame","896a5c15":"## Train Network","6226ae3d":"## View Sample of Images","ab94ef65":"# **Facial Expression Recognition Training Notebook**\n### Sara Manrriquez"}}