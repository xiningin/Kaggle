{"cell_type":{"0fa046d4":"code","0f6d7fa6":"code","ed3e41f3":"code","123a17b9":"code","a4669525":"code","53f0b7a5":"code","19cf9ba4":"code","72bd767e":"code","0e23dd5b":"code","2e82ca7a":"code","feb401ed":"code","ace9d4a8":"code","a54beaa0":"code","f28e12e5":"code","c6ac19f2":"code","e10b7ed1":"code","5d971ca0":"code","2389dc19":"code","540a1ac0":"code","fb1c1049":"code","62f06f28":"code","65dda9d8":"code","fc585272":"code","07b2fab1":"code","7629f7e2":"code","6d89b8a1":"code","eee7482c":"code","b970435f":"code","bd907d8f":"code","1025480f":"code","e6f89e16":"code","309b9a00":"code","1368ab38":"code","d8def905":"code","c7929be0":"code","6e6b537f":"code","515d8d88":"code","8a16b23f":"code","7809c5d2":"code","30aa410f":"code","f69679d6":"code","4804a0e1":"code","663ad083":"code","99728d49":"code","d3419c67":"code","efc7510c":"code","8a5071fa":"code","5de514be":"code","d08f4436":"code","bfcdd228":"code","c529d312":"code","43734ac7":"code","52a9e539":"code","99fcb8c6":"code","a007d284":"code","ac9a3de3":"code","aab67d69":"code","386a4385":"code","7b279a27":"code","551303b4":"code","16a9c199":"code","c00293a6":"code","b600b23e":"code","23a4ae1d":"code","4b512845":"code","39e1769a":"code","95b36b3a":"code","f7259afb":"code","bcd9edc7":"code","e1f9ba94":"code","a4eead8b":"code","bf834c2e":"code","72d64898":"code","9ec5c714":"code","fa090072":"markdown","926eee99":"markdown","8e285642":"markdown","137cb643":"markdown","1c986140":"markdown","85ad5257":"markdown","7404a501":"markdown","3abaccef":"markdown","2ce9f5f0":"markdown","85e4c659":"markdown"},"source":{"0fa046d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# linear algebra\nimport numpy as np\n# data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\n#plt.style.use(\"seaborn-whitegrid\")\nimport matplotlib.pyplot as plt\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n#Import the libraries\nimport math\nimport pandas_datareader as web\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')","0f6d7fa6":"df=pd.read_excel(\"\/kaggle\/input\/stockprediction\/StockPrediction(3).xlsx\")","ed3e41f3":"#Show the data \ndf","123a17b9":"df.columns = ['Date', 'Price', 'Opening', 'Dailymax', 'Dailymin', 'Volume', 'Difference']\ndf.head()","a4669525":"df.Date=pd.to_datetime(df.Date, dayfirst=True)\ndf = df.sort_values('Date')\ndf.head()","53f0b7a5":"df.set_index(\"Date\", inplace=True)\ndf.head()","19cf9ba4":"# The result shows that we have 3545 rows or days the stock price was recorded, and 6 columns.\ndf.shape","72bd767e":"df.isna()\ndf[df[\"Volume\"].isnull()]","0e23dd5b":"df.isnull().sum()","2e82ca7a":"df[df[\"Volume\"].isnull()]","feb401ed":"df.isnull().sum()","ace9d4a8":"df.Volume=df.Volume.fillna(method=\"ffill\")","a54beaa0":"df.isna().sum()","f28e12e5":"#Visualize the closing price history\nplt.figure(figsize=(16,8))\nplt.title('Price History')\nplt.plot(df['Price'])\nplt.xlabel('Date',fontsize=18)\nplt.ylabel('Price USD ($)',fontsize=18)\nplt.show()","c6ac19f2":"#Create a new dataframe with only the 'Opening' column\ndata = df.filter(['Price'])\n#Converting the dataframe to a numpy array\ndataset = data.values\n#Get \/Compute the number of rows to train the model on\ntraining_data_len = math.ceil( len(dataset) *.8) \ntraining_data_len","e10b7ed1":"#Scale the all of the data to be values between 0 and 1 \nscaler = MinMaxScaler(feature_range=(0, 1)) \nscaled_data = scaler.fit_transform(dataset)\nscaled_data","5d971ca0":"#Create the scaled training data set \ntrain_data = scaled_data[0:training_data_len  , : ]\n#Split the data into x_train and y_train data sets\nx_train=[]\ny_train = []\nfor i in range(60,len(train_data)):\n    x_train.append(train_data[i-60:i,0])\n    y_train.append(train_data[i,0])\n    if i<=60:\n        print(x_train)\n        print(y_train)\n        print()\n    \n","2389dc19":"#Convert x_train and y_train to numpy arrays\nx_train, y_train = np.array(x_train), np.array(y_train)","540a1ac0":"#Reshape the data into the shape accepted by the LSTM\nx_train = np.reshape(x_train, (x_train.shape[0],x_train.shape[1],1))\nx_train.shape","fb1c1049":"#Build the LSTM network model\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True,input_shape=(x_train.shape[1],1)))\nmodel.add(LSTM(units=50, return_sequences=False))\nmodel.add(Dense(units=25))\nmodel.add(Dense(units=1))","62f06f28":"#Compile the model\nmodel.compile(optimizer='adam', loss='mean_squared_error')","65dda9d8":"#Train the model\nmodel.fit(x_train, y_train, batch_size=1, epochs=1)","fc585272":"test_data = scaled_data[training_data_len - 60: , : ]\n#Create the x_test and y_test data sets\nx_test = []\ny_test =  dataset[training_data_len : , : ] #Get all of the rows from index 3545 to the rest and all of the columns (in this case it's only column 'Close'), so 3545 - 2776 = 769 rows of data\nfor i in range(60,len(test_data)):\n    x_test.append(test_data[i-60:i,0])","07b2fab1":"#Convert x_test to a numpy array \nx_test = np.array(x_test)","7629f7e2":"#Reshape the data into the shape accepted by the LSTM\nx_test = np.reshape(x_test, (x_test.shape[0],x_test.shape[1],1))","6d89b8a1":"#Getting the models predicted price values\npredictions = model.predict(x_test) \npredictions = scaler.inverse_transform(predictions)#Undo scaling","eee7482c":"#Calculate\/Get the value of RMSE\nrmse=np.sqrt(np.mean(((predictions- y_test)**2)))\nrmse","b970435f":"#Plot\/Create the data for the graph\ntrain = data[:training_data_len]\nvalid = data[training_data_len:]\nvalid['Predictions'] = predictions\n#Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Price USD ($)', fontsize=18)\nplt.plot(train['Price'])\nplt.plot(valid[['Price', 'Predictions']])\nplt.legend(['Train', 'Val', 'Predictions'], loc='lower right')\nplt.show()","bd907d8f":"#Show the valid and predicted prices\nvalid","1025480f":"from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nprint(\"MSE:\"+str(mean_squared_error(predictions,y_test)))\nprint(\"RMSE:\"+str(np.sqrt(mean_squared_error(predictions,y_test))))\nprint(\"MSLE:\"+str(mean_squared_log_error(predictions,y_test)))\nprint(\"RMSLE:\"+str(np.sqrt(mean_squared_error(predictions,y_test))))\nprint(\"MAE:\"+str(mean_squared_error(predictions,y_test)))\n","e6f89e16":"df_price=pd.read_excel(\"\/kaggle\/input\/stockprediction\/StockPrediction(3).xlsx\")","309b9a00":"df_price.columns = ['Date', 'Price', 'Opening', 'Dailymax', 'Dailymin', 'Volume', 'Difference']\ndf_price.head()","1368ab38":"df_price.Date=pd.to_datetime(df_price.Date, dayfirst=True)\ndf_price = df_price.sort_values('Date')\ndf_price.head()","d8def905":"df_price.set_index(\"Date\", inplace=True)\ndf_price.head()","c7929be0":"df_price.isna()\ndf_price[df_price[\"Volume\"].isnull()]","6e6b537f":"df_price.Volume=df_price.Volume.fillna(method=\"ffill\")\ndf_price.isna().sum()","515d8d88":"#Create a new dataframe\nnew_df = df_price.filter(['Price'])","8a16b23f":"#Get teh last 60 day closing price \nlast_60_days = new_df[-60:].values","7809c5d2":"#Scale the data to be values between 0 and 1\nlast_60_days_scaled = scaler.transform(last_60_days)","30aa410f":"#Create an empty list\nX_test = []","f69679d6":"#Append teh past 60 days\nX_test.append(last_60_days_scaled)","4804a0e1":"#Convert the X_test data set to a numpy array\nX_test = np.array(X_test)","663ad083":"#Reshape the data\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))","99728d49":"#Get the predicted scaled price\npred_price = model.predict(X_test)","d3419c67":"#undo the scaling \npred_price = scaler.inverse_transform(pred_price)\nprint(pred_price)","efc7510c":"#Get the quote\ndf_price2=pd.read_excel(\"\/kaggle\/input\/stockprediction\/StockPrediction(3).xlsx\")\ndf_price2.columns = ['Date', 'Price', 'Opening', 'Dailymax', 'Dailymin', 'Volume', 'Difference']\nprint(df_price2['Price'])","8a5071fa":"sd=pd.read_excel(\"\/kaggle\/input\/stockprediction\/StockPrediction(3).xlsx\")","5de514be":"sd.columns = ['Date', 'Price', 'Opening', 'Dailymax', 'Dailymin', 'Volume', 'Difference']\nsd.head()","d08f4436":"sd.Date=pd.to_datetime(sd.Date, dayfirst=True)\nsd = sd.sort_values('Date')\nsd.head()","bfcdd228":"sd.set_index(\"Date\", inplace=True)\nsd.head()","c529d312":"sd.Volume=sd.Volume.fillna(method=\"ffill\")\nsd.isna().sum()","43734ac7":"#Visualize the closing price history\nplt.figure(figsize=(16,8))\nplt.title('Price History')\nplt.plot(sd['Price'])\nplt.xlabel('Date',fontsize=18)\nplt.ylabel('Price USD ($)',fontsize=18)\nplt.show()","52a9e539":"sd.drop(labels= [\"Opening\", \"Dailymax\", \"Dailymin\", \"Volume\", \"Difference\"],axis=1,inplace=True)\nsd.head()","99fcb8c6":"sd.shift(1)","a007d284":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(sd)","ac9a3de3":"sd_diff=sd.diff(periods=1)\n#integrated of price 1, denoted by d (for diff), one of the parameter of ARIMA model\nsd_diff=sd_diff[1:]\nsd_diff.head()","aab67d69":"plot_acf(sd_diff)","386a4385":"sd_diff.plot(figsize=(20,5), title=\"Diff\")","7b279a27":"X1=sd.values\ntrain1=X1[0:2658] #2658 data as train data\ntest1=X1[2658:] #887 data as test\npredictions1=[]","551303b4":"test1.size","16a9c199":"from statsmodels.tsa.ar_model import AR\nfrom sklearn.metrics import mean_squared_error\nmodel_ar=AR(train1)\nmodel_ar_fit=model_ar.fit()","c00293a6":"predictions1=model_ar_fit.predict(start=2658, end=3545)","b600b23e":"test1","23a4ae1d":"plt.plot(test1)\nplt.plot(predictions1, color='red')","4b512845":"sd.plot()","39e1769a":"from statsmodels.tsa.arima_model import ARIMA","95b36b3a":"#p,d,q p=periods taken for autoregresive model\n#d-> Integrated price, difference\n#q period in moving average model\nmodel_arima=ARIMA(train1, order=(2,1,0))\nmodel_arima_fit=model_arima.fit()","f7259afb":"predictions1=model_arima_fit.forecast(steps=887)[0]\npredictions1","bcd9edc7":"plt.plot(test1)\nplt.plot(predictions1, color='red')","e1f9ba94":"print(model_arima_fit.aic)","a4eead8b":"import itertools\np=d=q=range(0,5)\npdq=list(itertools.product(p,d,q))\npdq","bf834c2e":"for param in pdq:\n    try:\n        model_arima=ARIMA(train1, order=param)\n        model_arima_fit=model_arima.fit()\n        print(model_arima_fit.aic)\n    except:\n        continue","72d64898":"from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nprint(\"MSE:\"+str(mean_squared_error(predictions1,test1)))\nprint(\"RMSE:\"+str(np.sqrt(mean_squared_error(predictions1,test1))))\nprint(\"MSLE:\"+str(mean_squared_log_error(predictions1,test1)))\nprint(\"RMSLE:\"+str(np.sqrt(mean_squared_error(predictions1,test1))))\nprint(\"MAE:\"+str(mean_squared_error(predictions1,test1)))","9ec5c714":"from sklearn.metrics import mean_squared_error, mean_squared_log_error, mean_absolute_error\nprint(\"MSE:\"+str(mean_squared_error(predictions,y_test)))\nprint(\"RMSE:\"+str(np.sqrt(mean_squared_error(predictions,y_test))))\nprint(\"MSLE:\"+str(mean_squared_log_error(predictions,y_test)))\nprint(\"RMSLE:\"+str(np.sqrt(mean_squared_error(predictions,y_test))))\nprint(\"MAE:\"+str(mean_squared_error(predictions,y_test)))","fa090072":"## LTSM BETTER THAN ARIMA WHEN WE LOOK AT THE MSE\nFor the estimator to be a good one, a small MSE is better since it implies agreement between the prediction and the reality. As others have said, MSE is the mean of the squared difference between your estimate and the data. ... Smaller MSE generally indicates a better estimate, at the data points in question.\n## ARIMA BETTER THAN LTSM WHEN WE LOOK AT THE RMSE\nIt means that there is no absolute good or bad threshold, however you can define it based on your DV. For a datum which ranges from 0 to 1000, an RMSE of 0.7 is small, but if the range goes from 0 to 1, it is not that small anymore.\n## MSLE\nLong answer: the ideal MSE isn't 0, since then you would have a model that perfectly predicts your training data, but which is very unlikely to perfectly predict any other data. What you want is a balance between overfit (very low MSE for training data) and underfit (very high MSE for test\/validation\/unseen data).\n## LTSM BETTER THAN ARIMA WHEN WE LOOK AT THE RMSLE\nThe RMSE is the square root of the variance of the residuals. It indicates the absolute fit of the model to the data\u2013how close the observed data points are to the model's predicted values. ... Lower values of RMSE indicate better fit.\n\n## MAE\nSimilarities: Both MAE and RMSE express average model prediction error in units of the variable of interest. ... Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable","926eee99":"# Autoreggressive AR Model","8e285642":"## Test data set\n","137cb643":"# LTSM","1c986140":"# ARIMA","85ad5257":"# ARIMA MODEL","7404a501":"# Stationarity\nmeans mean, variance and covariance is constant over periods[](http:\/\/)","3abaccef":"# LTSM RESULT","2ce9f5f0":"# ARIMA RESULT","85e4c659":"# Handling Missing Values"}}