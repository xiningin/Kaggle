{"cell_type":{"5bbfde34":"code","a66918e0":"code","848b97ca":"code","d1cb5441":"code","086e4465":"code","91b3ff02":"code","8f5f40ef":"code","1b54a906":"code","5b027c92":"code","cb88925b":"code","291a4ba9":"code","2dab333e":"code","1948d256":"code","86fb4467":"code","2b690f67":"code","3afcf34a":"code","402ca50e":"code","97d63cd8":"code","c59e7216":"code","ed2efafd":"code","c7c86a95":"code","1f2b3079":"code","a6a56127":"code","17fc299f":"code","dca2e7a6":"code","a2ac4328":"code","3731a872":"code","0b216bb1":"code","88e2b218":"code","6acdacaa":"code","eeee918b":"code","871c7bee":"code","7c5fe049":"code","3337c30e":"code","517bd3f0":"code","89e46047":"code","aeb8514c":"code","653548a2":"code","dfb60dd3":"code","1d3b4dc3":"code","cff1b400":"code","f26acf6e":"code","7e4385a0":"code","04d6b8fa":"code","b0381c1f":"code","bbf23d8d":"code","cf7fea67":"code","9f2dfc81":"code","4ef630e4":"code","7b695b7b":"markdown","5536dcac":"markdown","3c4cafed":"markdown","682c9338":"markdown","3392b004":"markdown","e3069505":"markdown","e47f94ef":"markdown","9cd3fca6":"markdown","9a52f9eb":"markdown","b7f3359b":"markdown","8b8f8ba2":"markdown"},"source":{"5bbfde34":"import os\nimport numpy as np \nimport pandas as pd \nfrom pprint import *\n\n# Generic\nfrom sklearn.model_selection import train_test_split, StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler, MinMaxScaler\nfrom sklearn.metrics import make_scorer, accuracy_score, mean_squared_error\nfrom category_encoders import MEstimateEncoder\nfrom IPython.display import SVG \n\n# Normalize features with high skewness and kurtosis\n## import packages\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# DNN\nimport tensorflow as tf\nimport keras_tuner as kt\nimport keras\nfrom keras.utils.vis_utils import model_to_dot \nfrom keras.utils import plot_model \n\n\n# set random seed\nfrom numpy.random import seed\nseed(42)\ntf.random.set_seed(42)\n\nfrom keras import regularizers\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\nfrom bayes_opt import BayesianOptimization\nfrom keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\nfrom keras import layers\n\nfrom math import floor\nfrom functools import  wraps, partial, WRAPPER_ASSIGNMENTS\n\nLeakyReLU = LeakyReLU(alpha=0.1)\n\n# Plots\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)\n","a66918e0":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","848b97ca":"df = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\n\ny = df.target\ndf.head()","d1cb5441":"df.hist(bins=50, figsize=(20,15))\nplt.show()","086e4465":"df['target_cat'] = pd.cut(df['target'], bins = [0, 5.5, 7, 8, 9, 10, np.inf], labels=[\"0-5.5\", \"5.5-7\", \"7-8\", \"8-9\", '9-10', \"10-\"])\ndf['target_cat'].hist()","91b3ff02":"test = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", index_col='id')\n\ntest.head()","8f5f40ef":"submission = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/sample_submission.csv')\nsubmission","1b54a906":"df.info()","5b027c92":"df.describe()","cb88925b":"# stratify split\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nsplit\nfor train_index, valid_index in split.split(df, df['target_cat']):\n    strat_train = df.loc[train_index]\n    strat_valid = df.loc[valid_index]","291a4ba9":"print(strat_train['target_cat'].value_counts()\/len(strat_train))\nstrat_valid['target_cat'].value_counts()\/len(strat_valid)\n","2dab333e":"y_train = strat_train.target\ny_valid = strat_valid.target","1948d256":"strat_train.index = strat_train.id\nstrat_valid.index = strat_valid.id\nstrat_train.drop(['target', \"target_cat\", 'id'], axis=1, inplace=True)\nstrat_valid.drop(['target', \"target_cat\", 'id'], axis=1, inplace=True)","86fb4467":"categorical_feats = [i for i in strat_train.columns if strat_train[i].dtype == \"object\"]\npprint(categorical_feats)\n\nnumerical_feats = strat_train.select_dtypes(include=[\"float64\"]).columns\nnumerical_feats","2b690f67":"X_train = strat_train\nX_valid = strat_valid","3afcf34a":"# X_train, X_valid, y_train, y_valid = train_test_split(df, y, train_size = 0.8,\n#                                                   test_size=0.2, random_state=0)","402ca50e":"X_train","97d63cd8":"from scipy import stats\nz_scores = stats.zscore(pd.DataFrame(y_train))\n# calculate z-scores of `df`\n\nabs_z_scores = np.abs(z_scores)\nprint(abs_z_scores)\nfiltered_entries = (abs_z_scores < 1.5).all(axis=1)\nprint(filtered_entries)\ny_train = y_train[filtered_entries]\nX_train = X_train[filtered_entries]","c59e7216":"def hist_qq_box(series, variable_name, figsize=(20,14) ):\n    '''\n    series: Pandas Series (1 col from a dataframe)\n    figsize: tuple with width, length of figure\n    variable_name: variable name to write on plots\n    rtype: None\n    '''\n\n    ## Visualization\n    fig = plt.figure(constrained_layout=True, figsize=figsize)\n    grid = gridspec.GridSpec(ncols=3, nrows=4, figure=fig)\n     # Histrogram\n    ax1 = fig.add_subplot(grid[0,:])\n    sns.distplot(series,ax=ax1, fit=norm)\n    ax1.set_title(f\"Histogram of {variable_name}\")\n    # QQplot\n    ax2 = fig.add_subplot(grid[2:,:2])\n    stats.probplot(series,plot=ax2)\n    ax2.set_title(f\"QQplot of {variable_name}\")\n     # Boxplot\n    ax3 = fig.add_subplot(grid[2:,2])\n    sns.boxplot(series,ax=ax3,orient=\"v\")\n    ax3.set_title(f\"Boxplot of {variable_name}\")\n\n    plt.show()","ed2efafd":"## Kurtosis and skewness of SalePrice\nprint( 'excess kurtosis of normal distribution (should be 0): {}'.format( y_train.kurt() ))\nprint( 'skewness of normal distribution (should be 0): {}'.format( y_train.skew() ))\n\nhist_qq_box(y_train, 'target')","c7c86a95":"skewness = X_train[numerical_feats].skew().sort_values(ascending=False)\nkurtosis = X_train[numerical_feats].kurt().sort_values(ascending=False)\n\ndf_norm = pd.concat([skewness,kurtosis],axis=1,keys=[\"Skewness\",\"Kurtosis\"])\n\ndf_norm","1f2b3079":"X_train","a6a56127":"X_train[categorical_feats].apply(pd.Series.value_counts)","17fc299f":"def target_encode_dataset(X_train, y_train, X_valid, test, categorical_feats):\n\n    # Encoding split\n    X_encode = pd.concat([X_train,y_train],axis =1).sample(frac=0.20, random_state=0)\n    y_encode = X_encode.pop(\"target\")\n\n    # Training split\n    X_pretrain_en = pd.concat([X_train,y_train],axis =1).drop(X_encode.index)\n    y_train_en = X_pretrain_en.pop(\"target\")\n\n    # Create the MEstimaterEconder\n    encoder = MEstimateEncoder(cols=categorical_feats, m=5.0)\n\n    # Fit the encoder on the encoding split\n    encoder.fit(X_encode, y_encode)\n\n    # Encode the training split\n    X_train_en = encoder.transform(X_train)\n    X_valid_en = encoder.transform(X_valid)\n    X_test_en = encoder.transform(test)\n    \n    return X_train_en, X_valid_en, X_test_en\n\n# target encode\nX_train, X_valid, test = target_encode_dataset(X_train, y_train, X_valid, test, categorical_feats)","dca2e7a6":"X_train","a2ac4328":"def scale_datasets(x_train, x_valid, x_test):\n    \"\"\"\n    Standard Scale test and train data\n    Z - Score normalization\n    \"\"\"\n    standard_scaler = StandardScaler()\n    \n    x_train_scaled = pd.DataFrame(\n      standard_scaler.fit_transform(x_train),\n      columns=x_train.columns\n    )\n    x_valid_scaled = pd.DataFrame(\n      standard_scaler.transform(x_valid),\n      columns = x_valid.columns\n    )\n    x_test_scaled = pd.DataFrame(\n        standard_scaler.transform(x_test),\n        columns = x_test.columns)\n    return x_train_scaled, x_valid_scaled, x_test_scaled\n\n\n# scale the dataset\nx_train_scaled, x_valid_scaled, x_test_scaled = scale_datasets(X_train, X_valid, test)","3731a872":"X_nn = x_train_scaled\nX_val_nn = x_valid_scaled\ny_nn = y_train\ny_val_nn = y_valid\n\ninput_shape = [X_nn.shape[1]] # has to be inside list\ninput_shape","0b216bb1":"X_nn","88e2b218":"y_nn","6acdacaa":"sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\ntf.config.get_visible_devices()","eeee918b":"# activation_funct = ['relu', \"selu\", \"elu\"]\n# activation_funct = ['relu', 'elu']\nactivation_funct = ['selu']\ndefault_activation = 'selu'","871c7bee":"# to record an agreement level between HyperBand, Random and Baysian with same Structure","7c5fe049":"def build_model(hp):\n    \n    model = tf.keras.Sequential()\n\n    # Tune the number of units in the first Dense layer\n    # Choose an optimal value between 32-512\n    hp_units1 = hp.Int('units1', min_value=256, max_value=512, step=32, default=512)\n    hp_units2 = hp.Int('units2', min_value=128, max_value=512, step=32, default=256)\n    hp_units3 = hp.Int('units3', min_value=64, max_value=512, step=32, default=128)\n    hp_units4 = hp.Int('units4', min_value=32, max_value=512, step=32, default=64)\n    \n    hp_activation1 = hp.Choice('dense_activation1', values=activation_funct, default=default_activation)\n    hp_activation2 = hp.Choice('dense_activation2', values=activation_funct, default=default_activation)\n    hp_activation3 = hp.Choice('dense_activation3', values=activation_funct, default=default_activation)\n    hp_activation4 = hp.Choice('dense_activation4', values=activation_funct, default=default_activation)\n    \n#     regularizer_l1 = hp.Float('reg_l1', min_value = 1e-3, max_value= 0.5, default= 1e-2)\n    regularizer_l2 = hp.Float('lambda_l2', min_value = 1e-3, max_value= 5, default= 1e-2)\n    \n    hp_learning_rate = hp.Float('learning_rate', min_value=1e-3, max_value=3e-2, default=4e-3)\n    \n#     momentum = hp.Float('momentum', default=0.0, min_value = 1e-4, max_value= 0.99)\n\n    \n    model.add(tf.keras.layers.BatchNormalization(input_shape=input_shape))\n    model.add(tf.keras.layers.Dropout(0.2))\n    \n    model.add(tf.keras.layers.Dense(units=hp_units1, activation=hp_activation1,  kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(regularizer_l2)))\n    model.add(tf.keras.layers.Dropout(0.5))\n    \n    model.add(tf.keras.layers.Dense(units=hp_units2, activation=hp_activation2,  kernel_initializer='lecun_normal', kernel_regularizer=regularizers.l2(regularizer_l2)))\n    model.add(tf.keras.layers.Dropout(0.5))\n\n    \n    model.add(tf.keras.layers.Dense(units=hp_units3, activation=hp_activation3, kernel_initializer='lecun_normal',  kernel_regularizer=regularizers.l2(regularizer_l2)))\n    model.add(tf.keras.layers.Dropout(0.5))\n    \n    model.add(tf.keras.layers.Dense(units=hp_units4, activation=hp_activation4, kernel_initializer='lecun_normal',  kernel_regularizer=regularizers.l2( regularizer_l2)))\n    model.add(tf.keras.layers.Dropout(0.5))\n\n    model.add(tf.keras.layers.Dense(1))\n\n\n    model.compile(\n      optimizer=tf.keras.optimizers.RMSprop(learning_rate=hp_learning_rate),\n      loss='mse',\n      metrics=['mse', 'mae']\n    )\n\n    return model\n","3337c30e":"# SVG(model_to_dot(build_model, show_shapes=True).create(prog=\"dot\", format=\"svg\"))","517bd3f0":"# BayesianOptimization algorithm from keras tuner\nobj = kt.Objective('val_mse', direction='min')\n\ntuner = kt.BayesianOptimization(\n    build_model,\n    objective=obj,\n    max_trials=40,\n    alpha=0.0005, # default: 0.0001; noise expected\n    beta=3,  #default: 2.6; bigger does more exploration \n    seed=42,\n    num_initial_points=3,\n    hyperparameters=None,\n    tune_new_entries=True,\n    allow_new_entries=True,\n    directory='keras_tuner_dir',\n    project_name='keras_tuner_demo'\n)\n\nstop_early = [tf.keras.callbacks.EarlyStopping(monitor='val_mse', patience=4)] # tf.keras.callbacks.ModelCheckpoint(filepath=\"best_model.h5\",monitor=\"val_mse\",save_best_only=True)]","89e46047":"tuner.search_space_summary()","aeb8514c":"tuner.search(X_nn, y_nn, \n             validation_data=(X_val_nn, y_val_nn), \n             epochs=30, batch_size=200, callbacks=stop_early)","653548a2":"for hp_param in [f'units{i}' for i in range(1,5)] + ['learning_rate'] + ['lambda_l2']:\n    print(hp_param, tuner.get_best_hyperparameters()[0].get(hp_param))\n    \nfor hp_param in [f'dense_activation{i}' for i in range(1,5)]:\n    print(hp_param, tuner.get_best_hyperparameters()[0].get(hp_param))","dfb60dd3":"best_model = tuner.get_best_models()[0]\nbest_model.build(X_nn.shape)\nbest_model.summary()\n","1d3b4dc3":"best_model.evaluate(X_val_nn, y_val_nn)","cff1b400":"history = best_model.fit(\n    X_nn, \n    y_nn,\n    validation_data=(X_val_nn, y_val_nn),\n    epochs=40,\n    batch_size=200,\n    callbacks=stop_early\n)","f26acf6e":"val_loss_per_epoch = history.history['val_loss']\nbest_epoch = val_loss_per_epoch.index(max(val_loss_per_epoch)) \nprint('Best epoch: %d' % (best_epoch,))","7e4385a0":"history = best_model.fit(\n    X_nn, \n    y_nn,\n    validation_data=(X_val_nn, y_val_nn),\n    epochs=best_epoch,\n    batch_size=200\n)","04d6b8fa":"mean_squared_error(best_model.predict(X_val_nn),y_valid)","b0381c1f":"history_df","bbf23d8d":"%matplotlib inline\n%config InlineBackend.figure_format = 'svg'\ndef plot_metric(history, metric):\n    train_metrics = history.history[metric]\n    val_metrics = history.history['val_'+metric]\n    epochs = range(1, len(train_metrics) + 1)\n    plt.plot(epochs, train_metrics)\n    plt.plot(epochs, val_metrics)\n    plt.title('Training and validation '+ metric)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([\"train_\"+metric, 'val_'+metric])\n    plt.show()\n    \nplot_metric(history, 'mse')","cf7fea67":"import matplotlib.pyplot as plt\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, loss_values, 'bo', label='Training loss')\nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show(\n\n","9f2dfc81":"predictions = best_model.predict(test)","4ef630e4":"submission['target'] = predictions\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission","7b695b7b":"## Deep learning simple DNN","5536dcac":" create objective function","3c4cafed":"## Dataset splits","682c9338":"## Target encoding","3392b004":"# Data cleaning","e3069505":"make sure we're using Kaggle's GPU:","e47f94ef":"check that the stratification was performed correctly:","9cd3fca6":"correct skewness, kurtosis","9a52f9eb":"# 30 Days of ML","b7f3359b":"we must stratify the splits","8b8f8ba2":"## Overview"}}