{"cell_type":{"aa735744":"code","ef5e4c03":"code","465f043c":"code","a18cbed8":"code","9520d31c":"code","e7f036a5":"code","8160c088":"code","eb6c4a09":"code","084a0a08":"code","45a6f76d":"code","b22c87fa":"code","474d0132":"code","d56d4687":"code","14e0673d":"code","c82bc585":"code","c20e686b":"code","81eacace":"code","f400dc33":"code","477b9c44":"code","b234fe5d":"code","20f215bf":"code","00a72973":"code","fc528da1":"code","aea9b53d":"markdown","21f11857":"markdown","2e3fb052":"markdown","c710f413":"markdown","7f159da0":"markdown","832ebcce":"markdown","edcabbd6":"markdown","c21c697c":"markdown","323bcc9f":"markdown"},"source":{"aa735744":"!git clone https:\/\/github.com\/OpenNMT\/OpenNMT-py\/\n!pip install -r OpenNMT-py\/requirements.opt.txt\n!pip install gcsfs\n!pip install rouge > \/dev\/null","ef5e4c03":"import numpy as np \nimport pandas as pd \nfrom IPython.display import display, Markdown\n\nfrom rouge import Rouge \nimport os\nfrom sklearn.utils import shuffle\nfrom pathlib import Path\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","465f043c":"PATH_TO_FINNEWS = Path('gs:\/\/cryptosum_dataset')\nPATH_TO_CRYPTONEWS = Path('..\/input\/news-about-major-cryptocurrencies-20132018-40k\/')","a18cbed8":"train_df_crypto = pd.read_csv(PATH_TO_CRYPTONEWS \/ 'crypto_news_parsed_2013-2017_train.csv')\nvalid_df = pd.read_csv(PATH_TO_CRYPTONEWS \/ 'crypto_news_parsed_2018_validation.csv')\ntrain_df_fin = pd.read_csv(PATH_TO_FINNEWS \/ 'Financial_news_dataset.csv')","9520d31c":"train_df_fin.head()","e7f036a5":"valid_df['text'].fillna(' ', inplace=True)\n\ntrain_df_crypto = train_df_crypto.dropna()\ntrain_df_crypto = train_df_crypto[train_df_crypto['title']!=' ']\n\ntrain_df_fin = train_df_fin.dropna()\ntrain_df_fin = train_df_fin[train_df_fin['title']!=' ']\ntrain_df_fin = train_df_fin[train_df_fin['text']!=' ']\ntrain_df_fin = train_df_fin[train_df_fin['title'].str.len()>15]\ntrain_df_fin = train_df_fin[train_df_fin['text'].str.len()>15]","8160c088":"train_df_fin['text'].apply(lambda s: len(s.split())).describe()","eb6c4a09":"train_df_fin['title'].apply(lambda s: len(s.split())).describe()","084a0a08":"train_df_crypto['title'].apply(lambda s: len(s.split())).describe()","45a6f76d":"max_str_length = 400\nmax_title_length = 15\n\ntitle_val = '<t> ' + valid_df['title'].apply(lambda x : ' '.join(x.split()[:max_title_length])) + ' <\/t>' + ' \\n'\ntext_val = valid_df['text'].apply(lambda x : ' '.join(x.split()[:max_str_length])) + ' \\n'\n\ntitle_tr = '<t> ' + train_df_crypto['title'].apply(lambda x : ' '.join(x.split()[:max_title_length])) + ' <\/t>' + ' \\n'\ntext_tr = train_df_crypto['text'].apply(lambda x : ' '.join(x.split()[:max_str_length]))  + ' \\n'\n\ntitle_tr_fin = '<t> ' + train_df_fin['title'].apply(lambda x : ' '.join(x.split()[:max_title_length])) + ' <\/t>' + ' \\n'\ntext_tr_fin = train_df_fin['text'].apply(lambda x : ' '.join(x.split()[:max_str_length]).replace('\\n', '')) + ' \\n'\n\ntrain_text = text_tr.values.tolist() + text_tr_fin.values.tolist()\ntrain_target = title_tr.values.tolist() + title_tr_fin.values.tolist() ","b22c87fa":"train_text, train_target = shuffle(train_text, train_target)","474d0132":"with open(\"val_text.txt\", 'w') as f:\n    f.writelines(text_val.values.tolist())\nwith open(\"val_target.txt\", 'w') as f:\n    f.writelines(title_val.values.tolist())\n\nwith open(\"tr_target.txt\", 'w') as f:\n    f.writelines(train_target)\nwith open(\"tr_text.txt\", 'w') as f:\n    f.writelines(train_text)","d56d4687":"!python OpenNMT-py\/translate.py \\\n-gpu 0 \\\n-batch_size 20 \\\n-beam_size 3 \\\n-model \/kaggle\/input\/onmt-bilstm-copy-attn\/BiLSTM_copy_attn_step_80000.pt \\\n-src val_text.txt \\\n-output predicted_title.txt \\\n-min_length 8 \\\n-max_length 15 \\\n-length_penalty avg \\\n-alpha 0.8 \\\n-verbose \\\n-replace_unk \\\n-dynamic_dict \\\n-block_ngram_repeat 3 \\\n-ignore_when_blocking \".\" \"<\/t>\" \"<t>\" ","14e0673d":"!sed -i 's\/ <\\\/t>\/\/g' predicted_title.txt \n!sed -i 's\/<t> \/\/g' predicted_title.txt","c82bc585":"with open('predicted_title.txt', 'r') as f:\n    pred_titles = f.readlines()\npred_titles = [x.replace('\\n', '').lower() for x in pred_titles]\n\ntrue_val_titles = valid_df['title'].str.lower().tolist()\n\nimport string\npunctuation = string.punctuation\n\ntrue_titles = []\nfor tr in true_val_titles:\n    for p in punctuation:\n        tr = tr.replace(p, f' {p} ')\n    true_titles.append(tr.lower().replace('  ', ' '))\n\npredicted_titles = []\nfor pred in pred_titles:\n    for p in punctuation:\n        pred = pred.replace(p, f' {p} ')\n    predicted_titles.append(pred.lower().replace('  ', ' '))","c20e686b":"from rouge import Rouge\nrouge = Rouge()\nscores = rouge.get_scores(hyps=predicted_titles, refs=true_titles, avg=True, ignore_empty=True)","81eacace":"scores","f400dc33":"final_metric = (scores['rouge-1']['f'] + scores['rouge-2']['f'] + scores['rouge-l']['f']) \/ 3\nfinal_metric","477b9c44":"scores_by_example = rouge.get_scores(hyps=predicted_titles, refs=true_titles, avg=False, ignore_empty=True)\nscores_by_example = np.array([(x['rouge-1']['f'] + x['rouge-2']['f'] + x['rouge-l']['f']) \/ 3 for x in scores_by_example])","b234fe5d":"def print_result(index):\n    display(Markdown('> **Rouge:** ' + str(round(scores_by_example[index], 3))))\n    display(Markdown('> **Title:** ' + valid_df['title'].iloc[index]))\n    display(Markdown('> **Generated:** ' + pred_titles[index]))\n    display(Markdown('> **Text:** ' + valid_df['text'].iloc[index]))\n    print('_' * 60)","20f215bf":"top_best_10 = scores_by_example.argsort()[-10:]\ntop_worst_10 = scores_by_example.argsort()[:10]","00a72973":"for i in top_best_10:\n    print_result(i)","fc528da1":"for i in top_worst_10:\n    print_result(i)","aea9b53d":"# Eyeballing the results: good and bad cases","21f11857":"Simple preprocessing. \nRemove rows if they have empty text or title, also remove rows with a short text.","2e3fb052":"<code>!python OpenNMT-py\/train.py \\\n-data preprocessed_data\/cryptodata \\\n-save_model model\/BiLSTM_copy_attn \\\n-copy_attn \\\n-global_attention mlp \\\n-word_vec_size 128 \\\n-rnn_size 256 \\\n-layers 1 \\\n-encoder_type brnn \\\n-train_steps 80000 \\\n-valid_steps 500 \\\n-save_checkpoint_steps 5000 \\\n-max_grad_norm 2 \\\n-dropout 0.3 \\\n-batch_size 32 \\\n-valid_batch_size 32 \\\n-optim sgd \\\n-learning_rate 1 \\\n-copy_loss_by_seqlength \\\n-reuse_copy_attn \\\n-bridge \\\n-seed 17 \\\n-world_size 1 \\\n-gpu_ranks 0 <\/code>","c710f413":"# Preprocessing the data\nFor the current dataset, I additionally truncate the source length at 400 tokens and the target at 15. Also, note that in current dataset models work better if the target surrounds sentences with tags such that a sentence looks like  `<t> w1 w2 w3 <\/t>` ","7f159da0":"# Evaluation","832ebcce":"# **Read data**\n\nIn addition to dataset [News about major cryptocurrencies 2013-2018 (40k)](https:\/\/www.kaggle.com\/kashnitsky\/news-about-major-cryptocurrencies-20132018-40k) i used\n[US Financial News Articles](https:\/\/www.kaggle.com\/jeet2016\/us-financial-news-articles)\n","edcabbd6":"# Train the pointer-generator model for 80000 steps. \n[pointer-generator networks (See 2017) ](https:\/\/arxiv.org\/pdf\/1704.04368.pdf)\n\n\n> **copy_attn:** This is the most important option, since it allows the model to copy words from the source.\n\n> **global_attention mlp: **This makes the model use the attention mechanism introduced by [Bahdanau et al.](https:\/\/arxiv.org\/abs\/1409.0473) instead of that by [Luong et al.](https:\/\/arxiv.org\/abs\/1508.04025)(global_attention dot).\n\n> **reuse_copy_attn:** This option reuses the standard attention as copy attention. Without this, the model learns an additional attention that is only used for copying.\n\n> **copy_loss_by_seqlength:** This modifies the loss to divide the loss of a sequence by the number of tokens in it. In practice, we found this to generate longer sequences during inference. However, this effect can also be achieved by using penalties during decoding.\n\n> **bridge:** This is an additional layer that uses the final hidden state of the encoder as input and computes an initial hidden state for the decoder. Without this, the decoder is initialized with the final hidden state of the encoder directly.\n\n> **optim SGD:** Alternative training procedures such as adam with initial learning rate 0.001 converge faster than sgd, but achieve slightly worse results. I additionally set the maximum norm of the gradient to 2, and renormalize if the gradient norm exceeds this value.\n\n> I am using using a 128-dimensional word-embedding, and 256-dimensional 1 layer LSTM. On the encoder side, I use a bidirectional LSTM (brnn), which means that the 256 dimensions are split into 128 dimensions per direction.\n\n> I additionally set the maximum norm of the gradient to 2, and renormalize if the gradient norm exceeds this value. \n\n**commands used:**","c21c697c":"Since I am using [copy-attention](https:\/\/papers.nips.cc\/paper\/5866-pointer-networks.pdf) in the model, I need to preprocess the dataset such that source and target are aligned and use the same dictionary. This is achieved by using the options dynamic_dict and share_vocab. \n\n**Command used:**","323bcc9f":"<code>!python OpenNMT-py\/preprocess.py \\\n-train_src tr_text.txt \\\n-train_tgt tr_target.txt \\\n-valid_src val_text.txt  \\\n-valid_tgt val_target.txt \\\n-dynamic_dict \\\n-share_vocab \\\n-overwrite \\\n-shard_size 20000 \\\n-src_vocab_size 50000 \\\n-tgt_vocab_size 50000 \\\n-save_data preprocessed_data\/cryptodata \\\n-tgt_seq_length 20 \\\n-src_seq_length 405 \\\n-seed 17 \\\n-lower  <\/code>"}}