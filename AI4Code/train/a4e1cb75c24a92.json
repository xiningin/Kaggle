{"cell_type":{"dfe41d62":"code","130d7448":"code","06b08264":"code","57c3781d":"code","67e22431":"code","ddad1a3f":"code","4559b320":"code","a737fde4":"code","8548b539":"code","d1f74633":"code","7b33d3e0":"code","65fa3dd5":"code","6155b20c":"code","13d8ff82":"code","c8cf33e3":"code","2c3176c6":"code","286b9260":"code","7b52a0a2":"code","09317df2":"code","3161cbce":"code","e27a51a2":"code","520fedc9":"code","b111959c":"code","2dc06c04":"code","3d612eae":"code","3ef63f7f":"code","5b9e8f93":"code","eed6e634":"code","636363c5":"code","50db38c6":"code","53f05ed9":"code","2dfbfaa3":"code","9c1a5967":"code","4f204935":"code","dd468450":"code","2c7aca10":"code","2d57d931":"code","27b6bce0":"code","9a1b5f07":"code","01ce3d55":"code","2075372b":"code","5ac01688":"code","3cd9fc7a":"code","8f91f504":"code","68c9d083":"code","66385ccb":"code","08a9e79c":"code","c6cadc9f":"code","ff36e926":"code","720867f1":"code","6a07f849":"code","977470aa":"code","c64f9323":"code","a9b117d5":"code","7ac9dcdc":"code","e7a05048":"code","92c20c20":"code","87cbe1fa":"code","b4ab3635":"markdown","8e510759":"markdown","c89d451e":"markdown","d83bdbaf":"markdown","a837a13a":"markdown","45763018":"markdown","0c4b6ddc":"markdown","647fb3d6":"markdown","4c87f449":"markdown","c5af8f8a":"markdown","23b59bef":"markdown","50ea237e":"markdown","8a507ed4":"markdown","68f0d835":"markdown","2dd71378":"markdown","5e77e7a2":"markdown"},"source":{"dfe41d62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","130d7448":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn import preprocessing \nfrom category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn import preprocessing \nfrom sklearn.impute import SimpleImputer","06b08264":"df_asset = pd.read_csv('..\/input\/g-research-crypto-forecasting\/asset_details.csv')\ndf_asset","57c3781d":"df = pd.read_csv('..\/input\/g-research-crypto-forecasting\/train.csv')\ndf","67e22431":"df.head()","ddad1a3f":"df.tail()","4559b320":"df.shape","a737fde4":"df.size","8548b539":"df.dtypes","d1f74633":"df.columns","7b33d3e0":"df.info()","65fa3dd5":"df.describe()","6155b20c":"df.isnull().sum()","13d8ff82":"#df.duplicated().sum()","c8cf33e3":"df.skew()","2c3176c6":"df.corr()","286b9260":"# Column with null values and their count\nc = 0\nis_null = []\nfor i in df.columns:\n    if df[i].isnull().sum()>0:\n        is_null.append(i)\n        print(i,df[i].isnull().sum())\n        c = c+1\nprint('Number of columns containing null values are:',c)","7b52a0a2":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in df.columns:\n    count_outliers(df,i)","09317df2":"df.dropna(axis = 0, inplace = True)","3161cbce":"df.isnull().sum()","e27a51a2":"df2 = pd.read_csv('..\/input\/g-research-crypto-forecasting\/example_test.csv')\ndf2","520fedc9":"# dropping last two columns since they are not useful\ndf2.drop(['group_num','row_id'],axis = 1,inplace = True)\ndf2","b111959c":"df2.head()","2dc06c04":"df2.tail()","3d612eae":"df2.shape","3ef63f7f":"df2.size","5b9e8f93":"df2.dtypes","eed6e634":"df2.columns","636363c5":"df2.info()","50db38c6":"df2.describe()","53f05ed9":"df2.isnull().sum()","2dfbfaa3":"df2.duplicated().sum()","9c1a5967":"df2.skew()","4f204935":"df2.corr()","dd468450":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            k.append(i)\n            print('Count of outliers are:',x+y)\nglobal k\nk = []\nfor i in df2.columns:\n    count_outliers(df2,i)","2c7aca10":"!pip install dataprep","2d57d931":"! python -m pip install \"dask[dataframe]\" --upgrade  # or python -m pip install","27b6bce0":"from dataprep.eda import plot, plot_correlation, create_report, plot_missing","9a1b5f07":"plot(df_asset)","01ce3d55":"create_report(df_asset)","2075372b":"plot(df2)","5ac01688":"create_report(df2)","3cd9fc7a":"! pip install Autoviz","8f91f504":"! pip install xlrd","68c9d083":"from autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\ndftc = AV.AutoViz(\n    filename='', \n    sep='' , \n    depVar='Asset_ID', \n    dfte=df, \n    header=0, \n    verbose=1, \n    lowess=False, \n    chart_format='png', \n    max_rows_analyzed=300000, \n    max_cols_analyzed=30\n)","66385ccb":"from autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\ndftc = AV.AutoViz(\n    filename='', \n    sep='' , \n    depVar='Asset_ID', \n    dfte=df2, \n    header=0, \n    verbose=1, \n    lowess=False, \n    chart_format='png', \n    max_rows_analyzed=300000, \n    max_cols_analyzed=30\n)","08a9e79c":"from autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\ndftc = AV.AutoViz(\n    filename='', \n    sep='' , \n    depVar='Asset_Name', \n    dfte=df_asset, \n    header=0, \n    verbose=1, \n    lowess=False, \n    chart_format='png', \n    max_rows_analyzed=300000, \n    max_cols_analyzed=30\n)","c6cadc9f":"from autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\ndftc = AV.AutoViz(\n    filename='', \n    sep='' , \n    depVar='Asset_ID', \n    dfte=df_asset, \n    header=0, \n    verbose=1, \n    lowess=False, \n    chart_format='png', \n    max_rows_analyzed=300000, \n    max_cols_analyzed=30\n)","ff36e926":"import gresearch_crypto\nenv = gresearch_crypto.make_env()","720867f1":"from pandas_profiling import ProfileReport","6a07f849":"report = ProfileReport(df2)\nreport","977470aa":"report = ProfileReport(df_asset)\nreport","c64f9323":"X = df.drop('Target',axis = 1)\nY = df['Target']\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3,random_state=44)","a9b117d5":"import lightgbm as ltb\nmodel = ltb.LGBMRegressor(n_estimators=10,learning_rate=0.09)\nmodel.fit(X_train, Y_train)\nprint(); print(model)    \nexpected_y  = Y_test\npredicted_y = model.predict(X_test)","7ac9dcdc":"print(metrics.r2_score(expected_y, predicted_y))","e7a05048":"pred1 = model.predict(df2)\npred1","92c20c20":"iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\nfor (df2, sample_prediction_df) in iter_test:\n    model = ltb.LGBMRegressor(n_estimators=10,learning_rate=0.09)\n    model.fit(X_train, Y_train)\n    df2.drop(['row_id'],axis=1,inplace = True)\n    pred2 = model.predict(df2)\n    sample_prediction_df['Target'] = pred2 # make your predictions here\n    env.predict(sample_prediction_df)   # register your predictions","87cbe1fa":"sample_prediction_df","b4ab3635":"> # Loading asset details dataset","8e510759":"> ## Data prep","c89d451e":"> # LGBMRegression prediction","d83bdbaf":"# Cleaning and preprocessing of Test dataset","a837a13a":"# Data Visualization with Autoviz","45763018":"> # Data Preprocessing","0c4b6ddc":"> # Data Visualization","647fb3d6":"> # Loading Train Data","4c87f449":"> # Exploratory Data Analysis of Test data","c5af8f8a":"> # Exploratory Data Analysis of train data","23b59bef":"> # Count of outliers in each numerical columns","50ea237e":"> # Count of outliers in test data","8a507ed4":"# Pandas Profiling","68f0d835":"> # Feature Selection","2dd71378":"> # Importing Libraries","5e77e7a2":"## Visualization for train data"}}