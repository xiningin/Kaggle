{"cell_type":{"6fabef0c":"code","3fc73ab2":"code","495ddf71":"code","7eed4046":"code","0e1233e2":"code","81a1fc4b":"code","affbc72c":"code","efe8244d":"code","f4d8e07f":"code","6b92e1ed":"code","83d73513":"code","da4279ec":"code","d7855a05":"code","6dcae1ce":"code","db31175a":"code","c568a3b6":"code","cef22e0c":"code","f574cbe8":"code","cc97c667":"code","66cd5e34":"code","95792227":"code","8a5101ff":"code","1ab00568":"code","67f07b94":"code","a79b7c8d":"code","3a014c76":"code","6c5ed1dd":"code","1f890111":"code","aee5224b":"code","d0b39682":"code","2963fa55":"code","743f5625":"code","5e6f867e":"code","c75a8799":"code","7e67e0a0":"code","c535a462":"code","b519bdf4":"code","26072845":"code","80a69d77":"code","84797aa3":"code","7f121e4c":"code","ef80f069":"code","e93570ed":"code","d581d32d":"code","8507dd65":"code","4bdf569b":"code","e63e43d6":"code","b9a14e96":"code","d5ce188d":"code","add0c3e3":"code","570f2374":"code","23bfb50e":"code","8c84d897":"code","b0ea7d27":"code","a0edb694":"code","9353ea73":"code","ddf3cc55":"code","4d450bf9":"code","16129a82":"code","bdb2f4c7":"code","daeb1b42":"code","b59edcaa":"code","19ab1149":"code","5b53a845":"code","c9a7bd33":"code","ea436bc9":"code","89621016":"code","9263424a":"code","7fe3d66d":"code","bdc2a06c":"code","d41ccbb7":"code","57878068":"code","cb54aff6":"code","20e2359d":"code","3848f060":"code","14527c44":"code","974ce18f":"markdown","b23dc3ae":"markdown","2bed1449":"markdown","6f89389e":"markdown","80e5cfd3":"markdown","be790f37":"markdown","5ebb6100":"markdown","98734de1":"markdown","c6b921ea":"markdown","3804cb26":"markdown","91e0ef03":"markdown","b4e90733":"markdown","4c6e7a4b":"markdown","6fa0f38b":"markdown","2b1d62f3":"markdown","2b6d56e0":"markdown","a34c8513":"markdown","e823e122":"markdown","d8775c0b":"markdown","2cd1a65b":"markdown","78d94dd3":"markdown","c38d40be":"markdown","75ccebf0":"markdown","026a5ecd":"markdown","dd89071f":"markdown","974de6f5":"markdown","00d8948b":"markdown","5e356adf":"markdown","12357979":"markdown","4435948b":"markdown","8179b4ae":"markdown","96c10928":"markdown","a85e8655":"markdown","1375a0c1":"markdown","aa167abd":"markdown","3d7e8571":"markdown","957947f4":"markdown","d4eea95a":"markdown","c5924529":"markdown","9058e2e0":"markdown","17e611d3":"markdown","bb8e3354":"markdown","2df517e2":"markdown","d3855c35":"markdown","222182f1":"markdown","30ffcf54":"markdown","b46b46a2":"markdown","1c24d2ee":"markdown","78eeaa8c":"markdown","6166f329":"markdown","02aee3c2":"markdown"},"source":{"6fabef0c":"import pandas as pd #Analysis \nimport matplotlib.pyplot as plt #Visulization\nimport seaborn as sns #Visulization\nimport numpy as np #Analysis \nfrom scipy.stats import norm #Analysis \nfrom sklearn.preprocessing import StandardScaler #Analysis \nfrom scipy import stats #Analysis \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport gc\n\nimport os\nimport string\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","3fc73ab2":"import os\nprint(os.listdir(\"..\/input\"))","495ddf71":"df_train = pd.read_csv(\"..\/input\/kuc-hackathon-winter-2018\/drugsComTrain_raw.csv\", parse_dates=[\"date\"])\ndf_test = pd.read_csv(\"..\/input\/kuc-hackathon-winter-2018\/drugsComTest_raw.csv\", parse_dates=[\"date\"])","7eed4046":"print(\"Train shape :\" ,df_train.shape)\nprint(\"Test shape :\", df_test.shape)","0e1233e2":"df_train.head()","81a1fc4b":"print(\"unique values count of train : \" ,len(set(df_train['uniqueID'].values)))\nprint(\"length of train : \" ,df_train.shape[0])","affbc72c":"df_all = pd.concat([df_train,df_test])","efe8244d":"condition_dn = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\ncondition_dn[0:20].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Top20 : The number of drugs per condition.\", fontsize = 20)","f4d8e07f":"df_all[df_all['condition']=='3<\/span> users found this comment helpful.'].head(3)","6b92e1ed":"condition_dn = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\n\ncondition_dn[condition_dn.shape[0]-20:condition_dn.shape[0]].plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Bottom20 : The number of drugs per condition.\", fontsize = 20)","83d73513":"df_train['review'][1]","da4279ec":"df_train['review'][2]","d7855a05":"#https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc kernel \nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(df_all[\"review\"], title=\"Word Cloud of review\")","6dcae1ce":"from collections import defaultdict\ndf_all_6_10 = df_all[df_all[\"rating\"]>5]\ndf_all_1_5 = df_all[df_all[\"rating\"]<6]","db31175a":"## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from rating  8 to 10 review ##\nfreq_dict = defaultdict(int)\nfor sent in df_all_1_5[\"review\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from rating  4 to 7 review ##\nfreq_dict = defaultdict(int)\nfor sent in df_all_6_10[\"review\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of rating 1 to 5\", \n                                          \"Frequent words of rating 6 to 10\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","c568a3b6":"freq_dict = defaultdict(int)\nfor sent in df_all_1_5[\"review\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in df_all_6_10[\"review\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent biagrams of rating 1 to 5\", \n                                          \"Frequent biagrams of rating 6 to 10\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig['layout'].update(height=1200, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","cef22e0c":"freq_dict = defaultdict(int)\nfor sent in df_all_1_5[\"review\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\nfreq_dict = defaultdict(int)\nfor sent in df_all_6_10[\"review\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent trigrams of rating 1 to 5\", \n                                          \"Frequent trigrams of rating 6 to 10\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig['layout'].update(height=1200, width=1600, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')","f574cbe8":"freq_dict = defaultdict(int)\nfor sent in df_all_1_5[\"review\"]:\n    for word in generate_ngrams(sent,4):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n\nfreq_dict = defaultdict(int)\nfor sent in df_all_6_10[\"review\"]:\n    for word in generate_ngrams(sent,4):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'red')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent 4-grams of rating 1 to 5\", \n                                          \"Frequent 4-grams of rating 6 to 10\"])\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig['layout'].update(height=1200, width=1600, paper_bgcolor='rgb(233,233,233)', title=\"4-grams Count Plots\")\npy.iplot(fig, filename='word-plots')","cc97c667":"rating = df_all['rating'].value_counts().sort_values(ascending=False)\nrating.plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Count of rating values\", fontsize = 20)","66cd5e34":"# Code in https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo\n# SRK - Simple Exploration Notebook \n\ncnt_srs = df_all['date'].dt.year.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('year', fontsize=12)\nplt.ylabel('', fontsize=12)\nplt.title(\"Number of reviews in year\")\nplt.show()","95792227":"df_all['year'] = df_all['date'].dt.year\nrating = df_all.groupby('year')['rating'].mean()\nrating.plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Mean rating in year\", fontsize = 20)","8a5101ff":"# Code in https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo\n# SRK - Simple Exploration Notebook \n\ncnt_srs = df_all['date'].dt.month.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('month', fontsize=12)\nplt.ylabel('', fontsize=12)\nplt.title(\"Number of reviews in month\")\nplt.show()","1ab00568":"df_all['month'] = df_all['date'].dt.month\nrating = df_all.groupby('month')['rating'].mean()\nrating.plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Mean rating in month\", fontsize = 20)","67f07b94":"df_all['day'] = df_all['date'].dt.day\nrating = df_all.groupby('day')['rating'].mean()\nrating.plot(kind=\"bar\", figsize = (14,6), fontsize = 10,color=\"green\")\nplt.xlabel(\"\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Mean rating in day\", fontsize = 20)","a79b7c8d":"plt.figure(figsize=(14,6))\nsns.distplot(df_all[\"usefulCount\"].dropna(),color=\"green\")\nplt.xticks(rotation='vertical')\nplt.xlabel('', fontsize=12)\nplt.ylabel('', fontsize=12)\nplt.title(\"Distribution of usefulCount\")\nplt.show()","3a014c76":"df_all[\"usefulCount\"].describe()","6c5ed1dd":"percent = (df_all.isnull().sum()).sort_values(ascending=False)\npercent.plot(kind=\"bar\", figsize = (14,6), fontsize = 10, color='green')\nplt.xlabel(\"Columns\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Total Missing Value \", fontsize = 20)","1f890111":"print(\"Missing value (%):\", 1200\/df_all.shape[0] *100)","aee5224b":"df_train = df_train.dropna(axis=0)\ndf_test = df_test.dropna(axis=0)","d0b39682":"df_all = pd.concat([df_train,df_test]).reset_index()\ndel df_all['index']\npercent = (df_all.isnull().sum()).sort_values(ascending=False)\npercent.plot(kind=\"bar\", figsize = (14,6), fontsize = 10, color='green')\nplt.xlabel(\"Columns\", fontsize = 20)\nplt.ylabel(\"\", fontsize = 20)\nplt.title(\"Total Missing Value \", fontsize = 20)","2963fa55":"all_list = set(df_all.index)\nspan_list = []\nfor i,j in enumerate(df_all['condition']):\n    if '<\/span>' in j:\n        span_list.append(i)","743f5625":"new_idx = all_list.difference(set(span_list))\ndf_all = df_all.iloc[list(new_idx)].reset_index()\ndel df_all['index']","5e6f867e":"df_condition = df_all.groupby(['condition'])['drugName'].nunique().sort_values(ascending=False)\ndf_condition = pd.DataFrame(df_condition).reset_index()\ndf_condition.tail(20)","c75a8799":"df_condition_1 = df_condition[df_condition['drugName']==1].reset_index()\ndf_condition_1['condition'][0:10]","7e67e0a0":"all_list = set(df_all.index)\ncondition_list = []\nfor i,j in enumerate(df_all['condition']):\n    for c in list(df_condition_1['condition']):\n        if j == c:\n            condition_list.append(i)\n            \nnew_idx = all_list.difference(set(condition_list))\ndf_all = df_all.iloc[list(new_idx)].reset_index()\ndel df_all['index']","c535a462":"from bs4 import BeautifulSoup\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer","b519bdf4":"stops = set(stopwords.words('english'))\n#stops","26072845":"#https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc kernel \nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(stops, title=\"Word Cloud of stops\")","80a69d77":"not_stop = [\"aren't\",\"couldn't\",\"didn't\",\"doesn't\",\"don't\",\"hadn't\",\"hasn't\",\"haven't\",\"isn't\",\"mightn't\",\"mustn't\",\"needn't\",\"no\",\"nor\",\"not\",\"shan't\",\"shouldn't\",\"wasn't\",\"weren't\",\"wouldn't\"]\nfor i in not_stop:\n    stops.remove(i)","84797aa3":"from sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\nfrom bs4 import BeautifulSoup\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","7f121e4c":"stemmer = SnowballStemmer('english')\n\ndef review_to_words(raw_review):\n    # 1. Delete HTML \n    review_text = BeautifulSoup(raw_review, 'html.parser').get_text()\n    # 2. Make a space\n    letters_only = re.sub('[^a-zA-Z]', ' ', review_text)\n    # 3. lower letters\n    words = letters_only.lower().split()\n    # 5. Stopwords \n    meaningful_words = [w for w in words if not w in stops]\n    # 6. Stemming\n    stemming_words = [stemmer.stem(w) for w in meaningful_words]\n    # 7. space join words\n    return( ' '.join(stemming_words))","ef80f069":"%time df_all['review_clean'] = df_all['review'].apply(review_to_words)","e93570ed":"# Make a rating\ndf_all['sentiment'] = df_all[\"rating\"].apply(lambda x: 1 if x > 5 else 0)","d581d32d":"df_train, df_test = train_test_split(df_all, test_size=0.33, random_state=42) ","8507dd65":"# https:\/\/github.com\/corazzon\/KaggleStruggle\/blob\/master\/word2vec-nlp-tutorial\/tutorial-part-1.ipynb\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.pipeline import Pipeline\n\nvectorizer = CountVectorizer(analyzer = 'word', \n                             tokenizer = None,\n                             preprocessor = None, \n                             stop_words = None, \n                             min_df = 2, # \ud1a0\ud070\uc774 \ub098\ud0c0\ub0a0 \ucd5c\uc18c \ubb38\uc11c \uac1c\uc218\n                             ngram_range=(4, 4),\n                             max_features = 20000\n                            )\nvectorizer","4bdf569b":"#https:\/\/stackoverflow.com\/questions\/28160335\/plot-a-document-tfidf-2d-graph\npipeline = Pipeline([\n    ('vect', vectorizer),\n])","e63e43d6":"%time train_data_features = pipeline.fit_transform(df_train['review_clean'])\n%time test_data_features = pipeline.fit_transform(df_test['review_clean'])","b9a14e96":"from tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Bidirectional, LSTM, BatchNormalization, Dropout\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences","d5ce188d":"#Source code in keras \uae40\ud0dc\uc601'blog\n# 0. Package\nimport numpy as np\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport random\n\n# 1. Dataset\ny_train = df_train['sentiment']\ny_test = df_test['sentiment']\nsolution = y_test.copy()\n\n# 2. Model Structure\nmodel = keras.models.Sequential()\n\nmodel.add(keras.layers.Dense(200, input_shape=(20000,)))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dropout(0.5))\n\nmodel.add(keras.layers.Dense(300))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Activation('relu'))\nmodel.add(keras.layers.Dropout(0.5))\n\nmodel.add(keras.layers.Dense(100, activation='relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\n# 3. Model compile\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","add0c3e3":"model.summary()","570f2374":"# 4. Train model\nhist = model.fit(train_data_features, y_train, epochs=10, batch_size=64)\n\n# 5. Traing process\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nfig, loss_ax = plt.subplots()\n\nacc_ax = loss_ax.twinx()\n\nloss_ax.set_ylim([0.0, 1.0])\nacc_ax.set_ylim([0.0, 1.0])\n\nloss_ax.plot(hist.history['loss'], 'y', label='train loss')\nacc_ax.plot(hist.history['acc'], 'b', label='train acc')\n\nloss_ax.set_xlabel('epoch')\nloss_ax.set_ylabel('loss')\nacc_ax.set_ylabel('accuray')\n\nloss_ax.legend(loc='upper left')\nacc_ax.legend(loc='lower left')\n\nplt.show()\n\n# 6. Evaluation\nloss_and_metrics = model.evaluate(test_data_features, y_test, batch_size=32)\nprint('loss_and_metrics : ' + str(loss_and_metrics))","23bfb50e":"sub_preds_deep = model.predict(test_data_features,batch_size=32)","8c84d897":"from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import confusion_matrix\n\n#folds = KFold(n_splits=5, shuffle=True, random_state=546789)\ntarget = df_train['sentiment']\nfeats = ['usefulCount']\n\nsub_preds = np.zeros(df_test.shape[0])\n\ntrn_x, val_x, trn_y, val_y = train_test_split(df_train[feats], target, test_size=0.2, random_state=42) \nfeature_importance_df = pd.DataFrame() \n    \nclf = LGBMClassifier(\n        n_estimators=2000,\n        learning_rate=0.05,\n        num_leaves=30,\n        #colsample_bytree=.9,\n        subsample=.9,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=2,\n        silent=-1,\n        verbose=-1,\n        )\n        \nclf.fit(trn_x, trn_y, \n        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n        verbose=100, early_stopping_rounds=100  #30\n    )\n\nsub_preds = clf.predict(df_test[feats])\n        \nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = feats\nfold_importance_df[\"importance\"] = clf.feature_importances_\nfeature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)","b0ea7d27":"solution = df_test['sentiment']\nconfusion_matrix(y_pred=sub_preds, y_true=solution)","a0edb694":"len_train = df_train.shape[0]\ndf_all = pd.concat([df_train,df_test])\ndel df_train, df_test;\ngc.collect()","9353ea73":"df_all['date'] = pd.to_datetime(df_all['date'])\ndf_all['day'] = df_all['date'].dt.day\ndf_all['year'] = df_all['date'].dt.year\ndf_all['month'] = df_all['date'].dt.month","ddf3cc55":"from textblob import TextBlob\nfrom tqdm import tqdm\nreviews = df_all['review_clean']\n\nPredict_Sentiment = []\nfor review in tqdm(reviews):\n    blob = TextBlob(review)\n    Predict_Sentiment += [blob.sentiment.polarity]\ndf_all[\"Predict_Sentiment\"] = Predict_Sentiment\ndf_all.head()","4d450bf9":"np.corrcoef(df_all[\"Predict_Sentiment\"], df_all[\"rating\"])","16129a82":"np.corrcoef(df_all[\"Predict_Sentiment\"], df_all[\"sentiment\"])","bdb2f4c7":"reviews = df_all['review']\n\nPredict_Sentiment = []\nfor review in tqdm(reviews):\n    blob = TextBlob(review)\n    Predict_Sentiment += [blob.sentiment.polarity]\ndf_all[\"Predict_Sentiment2\"] = Predict_Sentiment","daeb1b42":"np.corrcoef(df_all[\"Predict_Sentiment2\"], df_all[\"rating\"])","b59edcaa":"np.corrcoef(df_all[\"Predict_Sentiment2\"], df_all[\"sentiment\"])","19ab1149":"#\ubb38\uc7a5\uae38\uc774 (\uc904\ubc14\uafc8\ud45c\uc2dc\uac00 \uba87\ubc88\ub098\uc654\ub294\uc9c0 \uc148)\ndf_all['count_sent']=df_all[\"review\"].apply(lambda x: len(re.findall(\"\\n\",str(x)))+1)\n\n#Word count in each comment:(\ub2e8\uc5b4\uac2f\uc218)\ndf_all['count_word']=df_all[\"review_clean\"].apply(lambda x: len(str(x).split()))\n\n#Unique word count(unique\ud55c \ub2e8\uc5b4 \uac2f\uc218)\ndf_all['count_unique_word']=df_all[\"review_clean\"].apply(lambda x: len(set(str(x).split())))\n\n#Letter count(\ub9ac\ubdf0\uae38\uc774)\ndf_all['count_letters']=df_all[\"review_clean\"].apply(lambda x: len(str(x)))\n\n#punctuation count(\ud2b9\uc218\ubb38\uc790)\ndf_all[\"count_punctuations\"] = df_all[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n#upper case words count(\uc804\ubd80\ub2e4 \ub300\ubb38\uc790\uc778 \ub2e8\uc5b4 \uac2f\uc218)\ndf_all[\"count_words_upper\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n#title case words count(\uccab\uae00\uc790\uac00 \ub300\ubb38\uc790\uc778 \ub2e8\uc5b4 \uac2f\uc218)\ndf_all[\"count_words_title\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n#Number of stopwords(\ubd88\uc6a9\uc5b4 \uac2f\uc218)\ndf_all[\"count_stopwords\"] = df_all[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stops]))\n\n#Average length of the words(\ud3c9\uade0\ub2e8\uc5b4\uae38\uc774)\ndf_all[\"mean_word_len\"] = df_all[\"review_clean\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","5b53a845":"df_all['season'] = df_all[\"month\"].apply(lambda x: 1 if ((x>2) & (x<6)) else(2 if (x>5) & (x<9) else (3 if (x>8) & (x<12) else 4)))","c9a7bd33":"df_train = df_all[:len_train]\ndf_test = df_all[len_train:]","ea436bc9":"from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\n\n#folds = KFold(n_splits=5, shuffle=True, random_state=546789)\ntarget = df_train['sentiment']\nfeats = ['usefulCount','day','year','month','Predict_Sentiment','Predict_Sentiment2', 'count_sent',\n 'count_word', 'count_unique_word', 'count_letters', 'count_punctuations',\n 'count_words_upper', 'count_words_title', 'count_stopwords', 'mean_word_len', 'season']\n\nsub_preds = np.zeros(df_test.shape[0])\n\ntrn_x, val_x, trn_y, val_y = train_test_split(df_train[feats], target, test_size=0.2, random_state=42) \nfeature_importance_df = pd.DataFrame() \n    \nclf = LGBMClassifier(\n        n_estimators=10000,\n        learning_rate=0.10,\n        num_leaves=30,\n        #colsample_bytree=.9,\n        subsample=.9,\n        max_depth=7,\n        reg_alpha=.1,\n        reg_lambda=.1,\n        min_split_gain=.01,\n        min_child_weight=2,\n        silent=-1,\n        verbose=-1,\n        )\n        \nclf.fit(trn_x, trn_y, \n        eval_set= [(trn_x, trn_y), (val_x, val_y)], \n        verbose=100, early_stopping_rounds=100  #30\n    )\n\nsub_preds = clf.predict(df_test[feats])\n        \nfold_importance_df = pd.DataFrame()\nfold_importance_df[\"feature\"] = feats\nfold_importance_df[\"importance\"] = clf.feature_importances_\nfeature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)","89621016":"confusion_matrix(y_pred=sub_preds, y_true=solution)","9263424a":"cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n    by=\"importance\", ascending=False)[:50].index\n\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,10))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","7fe3d66d":"# import dictionary data\nword_table = pd.read_csv(\"..\/input\/dictionary\/inquirerbasic.csv\")","bdc2a06c":"word_table.head()","d41ccbb7":"##1. make list of sentiment\n#Positiv word list   \ntemp_Positiv = []\nPositiv_word_list = []\nfor i in range(0,len(word_table.Positiv)):\n    if word_table.iloc[i,2] == \"Positiv\":\n        temp = word_table.iloc[i,0].lower()\n        temp1 = re.sub('\\d+', '', temp)\n        temp2 = re.sub('#', '', temp1) \n        temp_Positiv.append(temp2)\n\nPositiv_word_list = list(set(temp_Positiv))\nlen(temp_Positiv)\nlen(Positiv_word_list)  #del temp_Positiv\n\n#Negativ word list          \ntemp_Negativ = []\nNegativ_word_list = []\nfor i in range(0,len(word_table.Negativ)):\n    if word_table.iloc[i,3] == \"Negativ\":\n        temp = word_table.iloc[i,0].lower()\n        temp1 = re.sub('\\d+', '', temp)\n        temp2 = re.sub('#', '', temp1) \n        temp_Negativ.append(temp2)\n\nNegativ_word_list = list(set(temp_Negativ))\nlen(temp_Negativ)\nlen(Negativ_word_list)  #del temp_Negativ","57878068":"##2. counting the word 98590\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(vocabulary = Positiv_word_list)\ncontent = df_test['review_clean']\nX = vectorizer.fit_transform(content)\nf = X.toarray()\nf = pd.DataFrame(f)\nf.columns=Positiv_word_list\ndf_test[\"num_Positiv_word\"] = f.sum(axis=1)\n\nvectorizer2 = CountVectorizer(vocabulary = Negativ_word_list)\ncontent = df_test['review_clean']\nX2 = vectorizer2.fit_transform(content)\nf2 = X2.toarray()\nf2 = pd.DataFrame(f2)\nf2.columns=Negativ_word_list\ndf_test[\"num_Negativ_word\"] = f2.sum(axis=1)","cb54aff6":"##3. decide sentiment\ndf_test[\"Positiv_ratio\"] = df_test[\"num_Positiv_word\"]\/(df_test[\"num_Positiv_word\"]+df_test[\"num_Negativ_word\"])\ndf_test[\"sentiment_by_dic\"] = df_test[\"Positiv_ratio\"].apply(lambda x: 1 if (x>=0.5) else (0 if (x<0.5) else 0.5))\n\ndf_test.head()","20e2359d":"def userful_count(data):\n    grouped = data.groupby(['condition']).size().reset_index(name='user_size')\n    data = pd.merge(data,grouped,on='condition',how='left')\n    return data\n#___________________________________________________________\ndf_test =  userful_count(df_test) \ndf_test['usefulCount'] = df_test['usefulCount']\/df_test['user_size']","3848f060":"df_test['deep_pred'] = sub_preds_deep\ndf_test['machine_pred'] = sub_preds\n\ndf_test['total_pred'] = (df_test['deep_pred'] + df_test['machine_pred'] + df_test['sentiment_by_dic'])*df_test['usefulCount']","14527c44":"df_test = df_test.groupby(['condition','drugName']).agg({'total_pred' : ['mean']})\ndf_test","974ce18f":"We will delete because the percentage is lower than 1%.","b23dc3ae":"First, let's see what words are used as stopwords. There are many words that include not, like needn't. These words are key parts of emotional analysis, so we will remove them from stopwords.","2bed1449":"Next up, it's Word Cloud.","6f89389e":"DrugName is closely related to condition, so we have analyzed them together. The unique values of the two variables are 3671 and 917, respectively, and there are about 4 drugs for each condition. Let's go ahead and visualize this in more detail.","80e5cfd3":"Next, we will check the number of reviews and percentage of ratings according to weather.","be790f37":"### 3.3 Dictionary_Sentiment_Analysis","5ebb6100":"We normalized useful count.","98734de1":"### 2.3 Review Preprocessing","c6b921ea":"We counted the number of words in review_clean which are included in dictionary.","3804cb26":"Next, let's have a look at the review. First, noticeable parts are the html strings like \\ r \\ n, and the parts that express emotions in parentheses such as (very unusual for him) and (a good thing) and words in capital letters like MUCH.","91e0ef03":"This is a hackathon for college students who are playing kaggle, but it is not a objective but make a objective for each team. Personally, this is the first time I have ever been in the field of natural language processing. If you see a strange part,  leave a comment , I will try to study and revise it.\n\n---\n\nOur team *Recommendation Medicines : Using a review* that fit the patient's condition. The process proceeds in the order of **data exploration  - data preprocessing - model - conclusion - limit**. In the data exploration part, we will look at data types with visualization techniques and statistical techniques. Through this process, we can set the topic, preprocess the data to fit the objective, and create various variables to fit model.  At the model part, emotion analysis using word dictionary, n-gram applying deep learning, etc. were used. In order to compensate the limitation of natural language processing, Lightgbm machine learning model was used and reliability was further secured through usefulcount. Finally, I will introduce the limitations and weak points of conclusion and analysis of the project.\n\n---\n\nTeam Information\n- Member names : Hyunwoo Kim, Juyeon Park, Jiye Lee, Eunjoo Min, Sumin Song.\n- University name : Hayang University.\n- Club name : TEAM-EDA.\n- Team name : TEAM-EDA.","b4e90733":"When you use 1-gram, you can see that the top 5 words have the same contents, although the order of left (negative) and right (positive) are different. This means when we analyze the text with a single corpus, it does not classify the emotion well. So, we will expand the corpus.","4c6e7a4b":"We will delete these parts in preprocessing as well.","6fa0f38b":"KUC Hackathon Winter 2018 : What can you do with the Drug Review dataset?:\n \n TEAM EDA's second project, following the last Beginer challenge: House price Advanced Regression (https:\/\/www.kaggle.com\/chocozzz\/beginner-challenge-house-prices) . The Drug Review dataset on the link (https:\/\/www.kaggle.com\/jessicali9530\/kuc-hackathon-winter-2018).","2b1d62f3":"### 2.2 Condition Preprocessing","2b6d56e0":"Interestingly, you can see that the average rating differs by year, but it is similar by month.","a34c8513":"As mentioned earlier, we have normalized usefulCount by condition to solve the problem that usefulCount shows bias depending on condition. You can then add three predicted emotion values and multiply them by the normalized usefulCount to get the predicted value.\n\nNow, we can recommend drug by condition in order of final predicted value.","e823e122":"In conclusion, these are the limitations we had during the project.\n\n1. Sentiment analysis using sentiment word dictionary has low reliability when the number of positive and negative words is small. For example, if there are 0 positive words and 1 negative word, it is classified as negative. Therefore, if the number of sentiment words is 5 or less, we could exclude the observations.\n2. To ensure the reliability of the predicted values, we normalized usefulCount and multiplied it to the predicted values. However, usefulCount may tend to be higher for older reviews as the number of cumulated site visitors increases. Therefore, we should have also considered time when normalizing usefulCount.\n3. If the emotion is positive, the reliability should be increased to the positive side, and if it is negative, the reliability should be increased toward the negative side. However, we simply multiplied the usefulCount for reliability and did not consider this part. So we should have multiplied considering the sign of usefulCount according to different kinds of emotion.\n\n\n","d8775c0b":"We will add variables for higher accuracy.","2cd1a65b":"This is the result of looking at the data through the head () command. There are six variables except for the unique ID that identifies the individual, and review is the key variable.","78d94dd3":"Next, we will look for relationship between rating and weather. First of all, we will count the number of ratings.","c38d40be":"## 3. Model","75ccebf0":"We checked whether the day of the week affects the rating like salary day, but it does not make a big difference.","026a5ecd":"In addition, there were some words with errors like didn&# 039;t for didn't, and also characters like ...","dd89071f":"If you look at the distribution of usefulCount, you can see that the difference between minimum and maximum is 1291, which is high. In addition, the deviation is huge, which is 36. The reason for this is that the more drugs people look for, the more people read the review no matter their contents are good or bad, which makes the usefulcount very high. So when we create the model, we will normalize it by conditions, considering people's accessibility.","974de6f5":"We will delete the sentences with the form above.","00d8948b":"Most people choose four values; 10, 9, 1, 8, and the number of 10 is more than twice as many as the others. With this, we can see that the percentage of positives is higher than negative, and people's reactions are extreme.","5e356adf":"Likewise, in 2-gram, the contents of the top five corpus are similar, and it is hard to classify positive and negative. In addition, 'side effects' and 'side effects.' are interpreted differently, which means preprocessing of review data is necessary. However, you can see that this is better to classify emotions rather than previous 1-grams, like side effects, weight gain, and highly recommend.","12357979":"As you can see from the picture above, the number of drugs for top eight conditions is about 100 for each condition. On the other hand, it should be noted that the phrase \"3<\/span> users found this comment helpful\" appears in the condition, which seems like an error in the crawling process. I have looked into it to see in more details.","4435948b":"## 5. Limitations","8179b4ae":"Our team set the topic as recommending the right medicine for the patient's condition with reviews and proceeded the project according to the topic with the data exploration, data preprocessing and modeling. In the data exploration section, we looked at the forms of data using visualization techniques and statistical techniques. We also looked for n-grams that can best represent emotions, and the relationship with date and rating. The next step was to preprocess the data according to the topic we set, such as removing the condition that has only one drug for recommendation. In the process of modeling, we used deep learning model with n-gram, and additionally used a machine learning model called Lightgbm to overcome the limitation of natural language processing. In addition, we conducted emotional analysis using emotional word dictionary to overcome limitations of package formed with movie data. In addition, we nomalized usefulcount by condition for better reliability. These steps allowed us to calculate the final predicted value and recommend the appropriate drug for each condition according to the order of the value.","96c10928":"## 2. Date Preprocessing","a85e8655":"Next, we will classify 1 ~ 5 as negative, and 6 ~ 10 as positive, and we will check through 1 ~ 4 grams which corpus best classifies emotions.","1375a0c1":"### 2.1. Missing Values Removal","aa167abd":"### 1.3 Missing value","3d7e8571":"From 3-gram you can see that there is a difference between positive and negative corpus. Bad side effects, birth control pills, negative side effects are corpus that classify positive and negative. However, both positive and negative parts can be thought that it has missing parts that reverses the context, such as' not' in front of a corpus.","957947f4":"![](http:\/\/cfile29.uf.tistory.com\/image\/996BDD3B5BFF269023E706)","d4eea95a":"- \\r\\n : we need to convert html grammer\n- ... , &#039; : deal with not alphabet","c5924529":"### 1.2. Data understanding\n\nFirst, we will start exploring variables, starting from uniqueID. We compared the unique number of unique IDs and the length of the train data to see if the same customer has written multiple reviews, and there weren't more than one reviews for one customer.","9058e2e0":"Clearly, 4-gram classifies emotions much betther than other grams. Therefore, we will use 4-gram to build deep learning model.","17e611d3":"We added a season variable.","bb8e3354":"Because the package used for prediction of 'Predict value' is formed with movie review data, it can be unsuitable for this project which analyzes reviews for drugs. To make up for this, we conducted additional emotional analysis using the Harvard emotional dictionary.","2df517e2":"These are additional explanations for variables.\n\n- drugName (categorical): name of drug \n- condition (categorical): name of condition\n- review (text): patient review \n- rating (numerical): 10 star patient rating \n- date (date): date of review entry \n- usefulCount (numerical): number of users who found review useful\n\nThe structure of the data is that a patient with a unique ID purchases a drug that meets his condition and writes a review and rating for the drug he\/she purchased on the date. Afterwards, if the others read that review and find it helpful, they will click usefulCount, which will add 1 for the variable.","d3855c35":"### 3.2 Lightgbm","222182f1":"To improve the low accuracy, we will use machine learning. First of all, this is the sentiment analysis model using only usefulCount.","30ffcf54":"## 4. Result","b46b46a2":"We defined Positiv_ratio = the number of positive words \/ (the number of positive words+the number of negative words) If the ratio is lower than 0.5, we classified as negative and if it's higher than 0.5, we classified as positive. With remainders, we classified as neutral, which includes the sentence without either positive or negative words.","1c24d2ee":"## 1. Exploration Data Analysis\n\n### 1.1. Data understanding\n\n\nFirst we will import Train data and Test data. The sizes of the two data are as follows:\n\nIt was data from https:\/\/archive.ics.uci.edu\/ml\/datasets\/Drug+Review+Dataset+%28Drugs.com%29 and crawled reviews from online pharmaceutical review sites.","78eeaa8c":"### 3.1. Deep Learning Model Using N-gram","6166f329":"It is expected that for structure of '<\/ span> users found this comment helpful.' phrase, there will be not only 3, but also 4 as shown above, and other numbers as well. We will remove these data in the future preprocessing.\n\nThe following are the low 20 conditions of 'drugs per condition'. As you can see, the number is all 1. Considering the recommendation system, it is not feasible to recommend with that when there is only one product. Therefore, we will analyze only the conditions that have at least 2 drugs per condition.","02aee3c2":"Next, we will delete conditions with only one drug."}}