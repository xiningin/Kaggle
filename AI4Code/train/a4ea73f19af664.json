{"cell_type":{"8e2119a8":"code","48535a63":"code","2528e099":"code","b3676883":"code","c6c099aa":"code","b798d58f":"code","16c81be4":"code","cd9c32c0":"code","473550ec":"code","d61fc519":"code","62a18ee2":"code","44f4d8cd":"code","dd3c370a":"code","a52c30eb":"code","6874066a":"code","f1bde7de":"code","1406428c":"markdown","004be5aa":"markdown"},"source":{"8e2119a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48535a63":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","2528e099":"model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])","b3676883":"\nmodel.compile(optimizer='sgd', loss='mean_squared_error')","c6c099aa":"import numpy as np\nx = np.array([-1.0,0.0,1.0,2.0], dtype=float)\ny = np.array([-3.0,-1.0,1.0,3.0], dtype=float)\n","b798d58f":"model.fit(x,y, epochs=500)","16c81be4":"model.predict([10.0])","cd9c32c0":"import tensorflow as tf\nprint(tf.__version__)\n\nclass myCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if(logs.get('loss')<0.4):\n      print(\"\\nReached 60% accuracy so cancelling training!\")\n      self.model.stop_training = True","473550ec":"callbacks = myCallback()\n","d61fc519":"\n\n\nmnist = tf.keras.datasets.fashion_mnist\n(training_images, training_labels), (test_images, test_labels) = mnist.load_data()","62a18ee2":"\ntraining_images=training_images\/255.0\ntest_images=test_images\/255.0","44f4d8cd":"model = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\nmodel.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])\n","dd3c370a":"import tensorflow as tf\nfrom os import path, getcwd, chdir\n\n# DO NOT CHANGE THE LINE BELOW. If you are developing in a local\n# environment, then grab mnist.npz from the Coursera Jupyter Notebook\n# and place it inside a local folder and edit the path to that location\npath = f\"{getcwd()}\/..\/tmp2\/mnist.npz\"","a52c30eb":"class mycallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy')>0.998):\n            print('\\nReached 99.8% accuracy so cancelling training!')\n            self.model.stop_training=True","6874066a":"# GRADED FUNCTION: train_mnist_conv\ndef train_mnist_conv():\n    \n    callbacks = mycallback()\n    mnist = tf.keras.datasets.mnist\n    (training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n    \n    training_images = training_images.reshape(60000,28,28,1)\n    training_images = training_images \/ 255.0\n    test_images = test_images.reshape(10000, 28,28,1)\n    test_images = test_images \/ 255.0\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)),\n        tf.keras.layers.MaxPooling2D(2,2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(256, activation='relu'),\n        tf.keras.layers.Dense(10, activation='softmax')\n    ])\n\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    # model fitting\n    history = model.fit(\n      training_images, training_labels, epochs = 10, callbacks = [callbacks]\n    )\n    # model fitting\n    return history.epoch, history.history['accuracy'][-1]\n\n","f1bde7de":"_,_ = train_mnist_conv()","1406428c":"# # mnsit dataset","004be5aa":"# with function"}}