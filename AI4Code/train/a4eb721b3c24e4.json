{"cell_type":{"d70ee703":"code","5ea4c96e":"code","07feaeb6":"code","a93f1462":"code","bf6fc15a":"code","29ebe734":"code","4290d70b":"code","31ff3ec8":"code","5f913085":"code","ba7f83ea":"code","5bd71d25":"code","b85fe170":"code","35308622":"code","f476ca3f":"code","796b337d":"code","2d862990":"code","9e5d806a":"code","b5b83f1a":"code","3c0d8f28":"code","98ba1953":"code","fbdf79d3":"code","b168e6a3":"code","c7a51968":"code","a133f5b7":"code","33a66869":"code","7c55e511":"code","30e31aba":"code","4c8b9064":"code","2337b617":"code","f77ea4ef":"code","65562628":"code","10c497aa":"code","b61da3f4":"code","17356a50":"code","ebaa60e2":"code","4208e0e7":"code","c262f0de":"code","a5907b2d":"code","58b1382d":"code","e1442ddb":"code","44d6a806":"code","05e45332":"code","a07f9fcc":"markdown","1e91e1e7":"markdown","a91e5e83":"markdown","22ea3a5a":"markdown","8afee451":"markdown","41ffccae":"markdown","0dd9f444":"markdown","13af4dc1":"markdown","1bd34d90":"markdown","09dbbf4f":"markdown","41fcba03":"markdown","b39099c9":"markdown","24a9d428":"markdown","95b2eb48":"markdown","cde311f7":"markdown","e6eb05e1":"markdown","998bdca6":"markdown","df8ac4dc":"markdown","6f142bf1":"markdown","20cc579e":"markdown","8806ed71":"markdown","8658c0be":"markdown","7ff45321":"markdown","e885d0ac":"markdown","3c74f7dd":"markdown","6eb5ec91":"markdown","47889b23":"markdown","265b4c7c":"markdown","f0a413f9":"markdown","8ca5a5a1":"markdown","33042ae3":"markdown","e7225dd9":"markdown","f5e93ef0":"markdown","3e33b6eb":"markdown","2a497d4e":"markdown","3cd1de67":"markdown","294e37c2":"markdown","5726a62c":"markdown","2c8a1ce1":"markdown","332d84be":"markdown","70e78e0c":"markdown","6b646922":"markdown","e54e02c8":"markdown","84dfc3cc":"markdown","835e1c68":"markdown","f4d092a9":"markdown","e3caffb2":"markdown","43020e95":"markdown","3631ee1b":"markdown","597ab439":"markdown"},"source":{"d70ee703":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # plotting\nimport matplotlib.pyplot as plt #\u00a0plot handling\nimport time # timer and stuff\nimport warnings # warning handling\n\n# Kaggle file system steup\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nwarnings.filterwarnings(\"ignore\")","5ea4c96e":"#\u00a0Load the csv file\nmobiles = pd.read_csv('..\/input\/ukrainian-market-mobile-phones-data\/phones_data.csv', index_col=0)","07feaeb6":"print(f\"There are {mobiles.shape[0]} rows and {mobiles.shape[1]} columns.\")\nprint(f\"There are {mobiles.isna().sum().sum()} missing values which represents {round((mobiles.isna().sum().sum() \/ (mobiles.shape[0] * mobiles.shape[1])) * 100, 2)}% of the data.\")\nprint(f\"Columns : {mobiles.columns.tolist()}\")","a93f1462":"mobiles.head()","bf6fc15a":"mobiles.describe()","29ebe734":"mobiles.describe(include=['object'])","4290d70b":"((mobiles.isna().sum()[mobiles.isna().sum()  > 0 ] \/ mobiles.shape[0] * 100).apply(lambda x: round(x, 1))).astype(str) + '%'","31ff3ec8":"# Dropping the columns that I can't handle\nmobiles_names = mobiles['model_name']\nmobiles       = mobiles.drop(columns=['model_name'])\n\n# Convert release_date to datetime type\nmobiles['release_date'] = pd.to_datetime(mobiles['release_date'])","5f913085":"mobiles.head()","ba7f83ea":"# Extract columns that are neither object nor datetime\nnumericals   = mobiles.dtypes[(mobiles.dtypes!='O') & (mobiles.dtypes!='<M8[ns]')].index.tolist()\n\n# Extract categorical variables which are objects here\ncategoricals = mobiles.dtypes[mobiles.dtypes == 'O'].index.tolist()","5bd71d25":"#\u00a0Constats for the EDA plots\nWIDTH  = 20\nHEIGHT = 8","b85fe170":"def plot_numerical(frame, column, categorical=None, ax=None, n_row=None, n_col=None):\n    #\u00a0Simple\n    if categorical is None:\n        sns.histplot(data=frame, x=column, ax=ax[n_row][n_col])\n    \n    # With category\n    else:\n        sns.histplot(data=frame, x=column, hue=categorical, ax=ax[n_row][n_col], legend=False)","35308622":"n_row = -1\n\n#\u00a0Setup a grid of (no. of rows, no. of plots on a row) with figure size\nfig, ax = plt.subplots(len(numericals), 1 + len(categoricals), figsize=(WIDTH, HEIGHT * (2 + len(categoricals))))\n\n#\u00a0Plot the figure for numericals\nfor numerical in numericals:\n    # Increment\n    n_col = 0\n    n_row += 1\n    \n    # Single distribution plotting\n    plot_numerical(mobiles, numerical, categorical=None, ax=ax, n_row=n_row, n_col=n_col)\n    n_col += 1\n    \n    # Distribution plotting by category\n    for categorical in categoricals:\n        plot_numerical(mobiles, numerical, categorical=categorical, ax=ax, n_row=n_row, n_col=n_col)\n        n_col += 1\n\n# Display the plot\nplt.show()","f476ca3f":"by_col = 'os'\n\nsns.pairplot(mobiles[numericals + [by_col]], hue=by_col)\nplt.show()","796b337d":"by_col = 'brand_name'\n\nsns.pairplot(mobiles[numericals + [by_col]], hue=by_col)\nplt.show()","2d862990":"def plot_categorical(frame, column):\n    #\u00a0Count plot\n    sns.countplot(frame[column])","9e5d806a":"titles = ['Brand', 'Operating System']\n\nfor categorical, title in zip(categoricals, titles):\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    plot_categorical(mobiles, categorical)\n    plt.title(title)\n    \n    if title == 'Brand':\n        plt.xticks(rotation=90)\n\n    plt.xlabel('')\n    plt.show()","b5b83f1a":"earliest = mobiles['release_date'].min().strftime(\"%B %d, %Y\")\nlatest   = mobiles['release_date'].max().strftime(\"%B %d, %Y\")\n\nprint(f\"Release dates are between {earliest} and {latest}\")","3c0d8f28":"for price in [ f\"{e}_price\" for e in ['lowest', 'highest', 'best']]:\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    sns.lineplot(data=mobiles, x='release_date', y=price, hue='os')\n    plt.show()","98ba1953":"def print_phone(phone):\n    space = 30\n    print(f\"{'Name'.rjust(space)} : {phone['model_name']}\")\n    print(f\"{'Price'.rjust(space)} : [{phone['lowest_price']}; {phone['highest_price']}]\")\n    print(f\"{'Popularity'.rjust(space)} : {phone['popularity']}\")\n    print(f\"{'Brand (OS)'.rjust(space)} : {phone['brand_name']} ({phone['os']})\")","fbdf79d3":"for i in range(5):\n    phone = mobiles.nlargest(5, 'highest_price').join(mobiles_names[mobiles.nlargest(5, 'highest_price').index]).iloc[i]\n    print_phone(phone)\n    print()","b168e6a3":"for i in range(5):\n    phone = mobiles.nsmallest(5, 'highest_price').join(mobiles_names[mobiles.nsmallest(5, 'highest_price').index]).iloc[i]\n    print_phone(phone)\n    print()","c7a51968":"data = mobiles.copy()\n\nfeatures = [\n    'brand_name',\n    'os',\n    'popularity',\n    'sellers_amount',\n    'screen_size',\n    'memory_size',\n    'battery_size',\n]\n\nnums  = ['popularity', 'sellers_amount', 'screen_size', 'memory_size', 'battery_size']\ncats  = ['brand_name', 'os']\n\nTARGET = 'best_price'","a133f5b7":"fig, ax= plt.subplots(1, 2, figsize=(WIDTH, HEIGHT\/2))\n\nsns.histplot(mobiles[TARGET], ax=ax[0])\nax[0].title.set_text(f'{TARGET} distribution')\n\nsns.histplot(mobiles[TARGET].apply(np.log), ax=ax[1])\nax[1].title.set_text(f'Log scaled {TARGET} distribution')\n\nplt.show()","33a66869":"data[TARGET] = data[TARGET].apply(np.log)","7c55e511":"# data['release_date'] = data['release_date'].apply( lambda x: x.timestamp())","30e31aba":"from sklearn.preprocessing import StandardScaler\n\n# Init. the scaler\nscaler = StandardScaler()\n\n# Fitting the scaler to the data\nscaled_data = scaler.fit_transform(data[nums])\ndata[nums]  = pd.DataFrame(columns=nums, data=scaled_data)","4c8b9064":"data[cats] = data[cats].fillna('Unknown')","2337b617":"fill_data  = data.groupby(cats, sort=False)[nums].apply(lambda x: x.ffill().bfill())\n\ndata.loc[fill_data.index, nums] = fill_data","f77ea4ef":"print(f\"There are {data[nums].isna().sum().sum()} missing values which represents {round((data[nums].isna().sum().sum() \/ (data[nums].shape[0] * data[nums].shape[1])) * 100, 2)}% of the data.\")","65562628":"data[nums] = data[nums].fillna(data[nums].median())","10c497aa":"print(f\"There are {data[nums].isna().sum().sum()} missing values which represents {round((data[nums].isna().sum().sum() \/ (data[nums].shape[0] * data[nums].shape[1])) * 100, 2)}% of the data.\")","b61da3f4":"#\u00a0One-hot encoding\noh_cats = pd.get_dummies(data[cats])\n\n#\u00a0Concatenate the on-hot encoded categorial variables to the data frame\ndata = pd.concat([\n    data.drop(columns=cats),\n    oh_cats\n], axis=1)\n\n# Correct features\nfor cat in cats:\n    if cat in features:\n        features.remove(cat)\n        \nfeatures = features + oh_cats.columns.tolist()","17356a50":"from sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\n\nSEED  = 42\n\ndef modelling(X, y, model, f_importance=False, fit=False):\n    # Type of modelling : Train & Test basic splitting\n    importance, tt_train_score, tt_test_score  = train_test_model(X, y, model, f_importance=f_importance)\n    \n    # Type of modelling : KFold Train & Test splitting\n    kf_train_score, kf_test_score = kfold_model(X, y, model)\n    \n    if fit:\n        model.fit(X, y)\n        return model, tt_test_score, kf_test_score\n    \n    return (importance, tt_train_score, tt_test_score, kf_train_score, kf_test_score) if f_importance else (tt_train_score, tt_test_score, kf_train_score, kf_test_score)\n\ndef train_test_model(X, y, model, f_importance=True):\n    \n    importance = None\n    \n    # Train & test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=SEED)\n    \n    # Fitting\n    model.fit(X_train, y_train)\n    \n    # Scores\n    train_pred = model.predict(X_train)\n    test_pred  = model.predict(X_test)\n    \n    train_score = mean_squared_error(y_train, model.predict(X_train))\n    test_score = mean_squared_error(y_test, model.predict(X_test))\n    \n    # Feature importances\n    if f_importance:\n        try:\n            try:\n                importance = model.feature_importances_\n            except:\n                try:\n                    importance = model.coef_\n                except:\n                    pass\n            \n            features   = X.columns.tolist()\n            importance = pd.Series(index=features, data=importance)\n            return importance, train_score, test_score\n        except:\n            pass\n        \n    # Model, RMSE on train, RMSE on test\n    return importance, train_score, test_score\n\ndef kfold_model(X, y, model):\n    # Parameters & variables\n    K            = 5\n    kf           = KFold(K)\n    train_scores = list() \n    test_scores  = list() \n    \n    # Looping over the folds\n    for train_index, test_index in kf.split(X):\n        \n        # Define datasets\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        # Fitting\n        model.fit(X_train, y_train)\n        \n        # Scores\n        train_pred = model.predict(X_train)\n        test_pred  = model.predict(X_test)\n        \n        train_score = mean_squared_error(y_train, model.predict(X_train))\n        test_score = mean_squared_error(y_test, model.predict(X_test))\n        \n        # Increments\n        train_scores.append(train_score)\n        test_scores.append(test_score)\n    \n    kf_train_score = np.mean(train_scores)\n    kf_test_score  = np.mean(test_scores)\n    \n    return kf_train_score, kf_test_score","ebaa60e2":"#\u00a0Classic linear regressor\nfrom sklearn.linear_model import LinearRegression, Ridge, SGDRegressor\n\n# Regressors with variable selection\nfrom sklearn.linear_model import ElasticNet, Lars, Lasso, LassoLars\n\n# Bayesian regressor\nfrom sklearn.linear_model import ARDRegression, BayesianRidge\n\n# XGBoost\nfrom xgboost import XGBRegressor\n\nmodels = [\n    LinearRegression(),\n    Ridge(),\n    SGDRegressor(),\n    ElasticNet(),\n    Lars(),\n    LassoLars(),\n    ARDRegression(),\n    BayesianRidge(),\n    XGBRegressor()\n]","4208e0e7":"result_cols = ['name', 'basic_train', 'basic_test', 'kf_train', 'kf_test']\nimportances = dict()\n\n# Model analysis DataFrame\nmodel_analysis = pd.DataFrame(columns=result_cols)\n\n# Splitting X & y\nX = data[features]\ny = data[TARGET]\n\nfor model in models:\n    print(f\"{type(model).__name__.rjust(20)}...\", end='')\n    \n    #\u00a0Function for modelling\n    importance, tt_train_score, tt_test_score, kf_train_score, kf_test_score = modelling(X, y, model, f_importance=True, fit=False)\n    \n    #\u00a0Add data from modelling\n    model_analysis = model_analysis.append(\n        pd.Series(\n            index=result_cols, \n            data=np.array([\n                type(model).__name__,\n                tt_train_score,\n                tt_test_score,\n                kf_train_score,\n                kf_test_score\n            ])), \n        ignore_index=True)\n    \n    # Add data for importance analysis\n    importances[type(model).__name__] = importance\n    print(f\" ended !\")","c262f0de":"print('Ranking based on test RSME : ')\nprint()\nprint(model_analysis[['name', 'basic_test', 'kf_test']].sort_values(by=['basic_test'], ascending=True))","a5907b2d":"print('Ranking based on cross validation test RSME : ')\nprint()\nprint(model_analysis[['name', 'basic_test', 'kf_test']].sort_values(by=['kf_test'], ascending=True))","58b1382d":"def plot_importance(series):\n    # Sort values\n    data = series.apply(np.abs).sort_values(ascending=False)\n    \n    # Plot\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    data.plot(kind='bar')\n    plt.title(data.name)\n    plt.show()\n\n_ = pd.DataFrame.from_dict(importances)[['BayesianRidge', 'SGDRegressor', 'XGBRegressor']].apply(lambda x: plot_importance(x), axis=0)","e1442ddb":"data[TARGET]","44d6a806":"# Model init\nmodel = XGBRegressor()\n\n#\u00a0Training\n_ = model.fit(X, y)","05e45332":"for brand in mobiles.brand_name.unique():\n    #\u00a0Create plot\n    plt.figure(figsize=(WIDTH, HEIGHT))\n    \n    # Filter\n    query = f\"brand_name=='{brand}'\"\n    \n    # Real data\n    sns.lineplot(data=mobiles.query(query), x='release_date', y=TARGET, label=\"Real data\")\n    \n    # Predictions\n    sns.lineplot(x=mobiles.loc[X.index].query(query)['release_date'], y=np.exp(model.predict(X.iloc[mobiles.query(query).index,:])), label=\"Predictions\")\n    \n    #\u00a0Display\n    plt.title(f\"Temporal evolution of {brand} smartphone prices : Real data vs predictions\")\n    plt.show()","a07f9fcc":"### Modelling function ","1e91e1e7":"___","a91e5e83":"Because there is still missing data because the aggregation could not be done on all data, we will input the rest with the median.","22ea3a5a":"## Predictions analysis","8afee451":"> Some descriptive stats of the numerical variables.","41ffccae":"### Functions","0dd9f444":"___","13af4dc1":"## 1. Pre-Processing","1bd34d90":"> This is the percentage of missing data by columns\/variables.","09dbbf4f":"# Conclusion\n\nIt was a very interesting dataset to use! Though the main challenges here would be to select the most interesting features in order to predict the prices  in the future with the Time-Series variable (because I don't know yet, I am working on it).\n\nSo hope you enjoyed, don't forget to upvote, thank you.","41fcba03":"# Imports","b39099c9":"___","24a9d428":"### Training and evaluation","95b2eb48":"#\u00a0EDA a.k.a Exploratory Data Analysis","cde311f7":"### Top 5 cheapest mobile phones","e6eb05e1":"> Because the data looks more *Gaussian* in log scale we will predict the log scale of the target variable then we will put it to exponential for the real predictions.","998bdca6":"#\u00a0Load the data","df8ac4dc":"##\u00a0Data Cleaning","6f142bf1":"## 2. Model","20cc579e":"> Plotting lowest_price, highest_price and best_price","8806ed71":"### Functions","8658c0be":"#### By brand","7ff45321":"___","e885d0ac":"### Categorical variables - One-hot enconding","3c74f7dd":"### Numerical & Categorical variables - Imputation\n- Fill missing categorical values with `'Unknown'`.\n- In our case we will impute the missing numericals values with the median grouped by the `brand_name` and `os`.","6eb5ec91":"## Miscellaneous","47889b23":"### Top 5 priciest mobile phones","265b4c7c":"## Interesting features\n\n- `brand_name` : The brand name has a big influence in most of the models, it can be mostly seen as minima the third most important feature among the others. Though we can see that most of the models give the ***Apple \uf8ff*** brand a big importance. We suppose that ***Apple \uf8ff*** smartphones are the most expensive so they are easy to identify.\n\n- `screen_size` & `memory_size` : If we isolate the ***Apple \uf8ff*** brand, we suppose that those technical characteristcs are the most important when evaluating the price of a smartphone.","f0a413f9":"#### By os","8ca5a5a1":"> First 5 rows of the DataFrame.","33042ae3":"## 1. Numerical variables","e7225dd9":"## 2. Categorical variables","f5e93ef0":"### Prices evolution\n\n> Plotting only by os because there are too many brand names","3e33b6eb":"# Table of contents\n\n- Imports\n- Load the data\n- Basic insights\n- EDA | Exploratory Data Analysis\n  1. Numerical variables\n  2. Categorical variables\n  3. Time-Series\n- Modelling\n  1. Pre-Processing\n  2. Model\n- Post-Modelling\n  1. Feature importance\n  2. Predictions analysis\n- Conclusion","2a497d4e":"##\u00a0Dimensions","3cd1de67":"#\u00a0Post-modelling analysis","294e37c2":"> Some descriptive stats of the categorical variables.","5726a62c":"###\u00a0Analysis","2c8a1ce1":"###\u00a0Analysis","332d84be":"# Modelling\n\nIn order to accomplish the task, we will select to predict **best_price** variable. In order to do that here are the details about the issue we are going to deal with...\n\n- Type of issue : **Regression**\n- Type of variables : **Numerical, Categorical and Time-Series**","70e78e0c":"### Date variable\nWe convert datetime type to timestamp (float) for the modelling part. So the bigger the timestamp, the earlier the phone was released.\nWe won't take into account the date in this notebook version...","6b646922":"#### Distribution","e54e02c8":"___","84dfc3cc":"## 3. Time-Series","835e1c68":"###\u00a0Plot pairing","f4d092a9":"___","e3caffb2":"# Mobile Phones Market Data\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1137520\/1908197\/93084e667e82983099e2b7611faa9407\/dataset-cover.png?t=2021-02-04-08-14-14\" style=\"align:center\">\n\n<br>\n\n# Introduction\n\nThis notebook uses the [Mobile Phones Market Data](https:\/\/www.kaggle.com\/artempozdniakov\/ukrainian-market-mobile-phones-data) - *Data with prices and parameters of smartphones, which can be bought in Ukraine.* dataset given by [Artem Pozdniakov](https:\/\/www.kaggle.com\/artempozdniakov).\n\nThe objective of this notebook is to accomplish the following tasks:\n- **Predict Prices** \n- **Exploratory Data Analysis**\n\n> *The dataset set contains data about the mobile phones which were released in past 4 years and which can be bought in Ukraine. Dataset contains the model name, brand name and operating system of the phone and it's popularity. It also has it's financial characteristics like lowest\/highest\/best price and sellers amount. And some of the characteristics like screen\/battery size, memory amount and release date. This data can be useful for improving your machine learning, analysis and vizualization, missing data filling skills. I'm waiting for your notebooks! :) Good luck!* - **@artempozdniakov**","43020e95":"### Date interval","3631ee1b":"# Basic insights","597ab439":"### Numerical variables - Scaling\/Normalizing\n\nIn this part you can chose either to scale or normalize your data. It might sometimes help the model to get better results, though it is a hypothesis."}}