{"cell_type":{"88306578":"code","4da650f1":"code","d52d22f6":"code","ab496e2e":"code","92b41d99":"code","e864d4a4":"code","40a6396c":"code","7d018431":"code","23333149":"code","8571770c":"code","bfd42e67":"code","0775df54":"code","5f9ee28e":"code","1b0e17e7":"code","53f96e89":"code","7dc95f76":"code","735051be":"code","608e2c69":"code","b9c41213":"code","b5b4b276":"code","9f50217a":"code","3b837be5":"code","cb98668c":"code","2ad2ce73":"code","50afebe2":"code","fae5c938":"code","24208f83":"code","8aeab8ee":"code","f532fff5":"code","09a5ff02":"code","dc21d8db":"code","86795d6f":"code","e5c2eb28":"code","021eb062":"code","b7494e7e":"code","018dcaf2":"code","2e8cd305":"markdown","97cf941d":"markdown","59a43e31":"markdown","2bce829d":"markdown","c4a29a7b":"markdown","97d1ea60":"markdown","8ec5f312":"markdown","c807cc85":"markdown","409995bb":"markdown","626771a8":"markdown","e61dfe7c":"markdown","83961de4":"markdown","034a5cbc":"markdown","edf829e6":"markdown","7066cc68":"markdown","7183e838":"markdown","d200b8ec":"markdown","1733cb5e":"markdown","6edf8f9d":"markdown","3910dc91":"markdown","efdea69e":"markdown","9a199066":"markdown","6acee142":"markdown","00b8fb67":"markdown","fcf74841":"markdown","515d0001":"markdown","0663b619":"markdown","1890de67":"markdown"},"source":{"88306578":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime as dt\nimport time\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split as split\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom pandas.tools.plotting import scatter_matrix\nimport warnings\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder, KBinsDiscretizer, MaxAbsScaler\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nsns.set()\nimport math\n\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4da650f1":"#Load Data and encode to latin\nacc = pd.read_csv('..\/input\/Accident_Information.csv', encoding = 'latin')\nveh = pd.read_csv('..\/input\/Vehicle_Information.csv', encoding = 'latin')\n\n# Merging two data sets into one with inner join by index\ndf = pd.merge(veh, acc, how = 'inner', on = 'Accident_Index')\n\n#Check data sample\nprint(df.shape)\ndf.head()","d52d22f6":"#Distribution of original data by targets\n\nax = sns.countplot(x = df.Accident_Severity ,palette=\"Set2\")\nsns.set(font_scale=1)\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfig = plt.gcf()\nfig.set_size_inches(8,4)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()\/len(df.Accident_Severity)), (p.get_x()+ 0.3, p.get_height()+10000))\n\nplt.title('Distribution of 2 Million Targets',)\nplt.xlabel('Accident Severity')\nplt.ylabel('Frequency [%]')\nplt.show()\n","ab496e2e":"# Creating weights that are opposite to the weights of target\nweights = np.where(df['Accident_Severity'] == 'Slight', .2, .8)\n\n#Sampling only 30% of the data with new weights  \ndf = df.sample(frac=0.3, replace=True, weights=weights)\nprint(df.shape)\n#df.Accident_Severity.value_counts(normalize=True)","92b41d99":"#Distribution of sample data by targets\n\nax = sns.countplot(x = df.Accident_Severity ,palette=\"Set2\")\nsns.set(font_scale=1.5)\nax.set_xlabel(' ')\nax.set_ylabel(' ')\nfig = plt.gcf()\nfig.set_size_inches(8,4)\nfor p in ax.patches:\n    ax.annotate('{:.2f}%'.format(100*p.get_height()\/len(df.Accident_Severity)), (p.get_x()+ 0.3, p.get_height()+10000))\n\nplt.title('Distribution of 600K Targets',)\nplt.xlabel('Accident Severity')\nplt.ylabel('Frequency [%]')\nplt.show()\n","e864d4a4":"#Missing values for each column\nnull_count = df.isnull().sum()\nnull_count[null_count>0]#.plot('bar', figsize=(30,10))","40a6396c":"(df.Age_of_Vehicle\n .value_counts()\n .plot(title = \"Age of Vehicle\", \n       logx = True, \n       figsize=(14,5)))\n\nprint('Min:',    df.Age_of_Vehicle.min(), '\\n'\n      'Max:',    df.Age_of_Vehicle.max(), '\\n'\n      'Median:', df.Age_of_Vehicle.median())","7d018431":"(df['Engine_Capacity_.CC.']\n .plot('hist',\n       bins = 1000,\n       title = \"Engine Capacity\", \n       figsize=(14,5),\n       logx = True\n      ))\n\nprint('Min:',    df['Engine_Capacity_.CC.'].min(), '\\n'\n      'Max:',    df['Engine_Capacity_.CC.'].max(), '\\n'\n      'Median:', df['Engine_Capacity_.CC.'].median())","23333149":"df2 = df[['Accident_Index', '1st_Road_Class','Day_of_Week', 'Junction_Detail','Light_Conditions', 'Number_of_Casualties',\n          'Number_of_Vehicles', 'Road_Surface_Conditions', 'Road_Type', 'Special_Conditions_at_Site', 'Speed_limit',\n          'Time', 'Urban_or_Rural_Area', 'Weather_Conditions', 'Age_Band_of_Driver', 'Age_of_Vehicle',\n          'Hit_Object_in_Carriageway', 'Hit_Object_off_Carriageway', 'make', 'Engine_Capacity_.CC.', 'Sex_of_Driver',\n          'Skidding_and_Overturning', 'Vehicle_Manoeuvre', 'Vehicle_Type', 'Accident_Severity'\n         ]]","8571770c":"plt.figure(figsize=(9,5))\nsns.heatmap(df2.corr(),linewidths=.5,cmap=\"YlGnBu\")\nplt.show()","bfd42e67":"plt.figure(figsize=(14,5))\nsns.distplot(df2.Number_of_Vehicles).set_xlim(0,20)\nprint('Min:',    df2.Number_of_Vehicles.min(), '\\n'\n      'Max:',    df2.Number_of_Vehicles.max(), '\\n'\n      'Median:', df2.Number_of_Vehicles.median())","0775df54":"plt.figure(figsize=(14,5))\nsns.distplot(df2.Number_of_Casualties).set_xlim(0,20)\nprint('Min:',    df2.Number_of_Casualties.min(), '\\n'\n      'Max:',    df2.Number_of_Casualties.max(), '\\n'\n      'Median:', df2.Number_of_Casualties.median())","5f9ee28e":"time_x = pd.to_datetime(df2['Time'], format='%H:%M').dt.hour\nplt.figure(figsize=(14,5))\ntime_x.value_counts().sort_index().plot('area')","1b0e17e7":"df2['Accident_Severity'] = df2['Accident_Severity'].replace(['Serious', 'Fatal'], 'Serious or Fatal')\ndf2 = pd.get_dummies(df2, columns=['Accident_Severity'])\ndf2 = df2.drop('Accident_Severity_Serious or Fatal', axis=1)\ndf2.Accident_Severity_Slight.value_counts(normalize=True)","53f96e89":"plt.figure(figsize=(14,5))\nacc_slight = df2.Accident_Severity_Slight == 1\nacc_severe = df2.Accident_Severity_Slight == 0\n\nsns.kdeplot(df2.Number_of_Casualties[acc_slight],shade=True,color='Blue', label='Slight').set_xlim(0,20)\nsns.kdeplot(df2.Number_of_Casualties[acc_severe],shade=True,color='Red', label='Severe').set_xlim(0,20)\n\nplt.title('Number of Casualties dist by accident severity')\nplt.show()\n\n#print(\"we can see distribution between failed (under 2000), and successful (bigger the 2000)\")\n","7dc95f76":"plt.figure(figsize=(14,5))\n\nsns.kdeplot(df2.Number_of_Vehicles[acc_slight],shade=True,color='Blue', label='Slight').set_xlim(0,20)\nsns.kdeplot(df2.Number_of_Vehicles[acc_severe],shade=True,color='Red', label='Severe').set_xlim(0,20)\n\nplt.title('Number of vehicles dist by accident severity')\nplt.show()\n\n#print(\"we can see distribution between failed (under 2000), and successful (bigger the 2000)\")\n","735051be":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20,10))\nplt.subplots_adjust(hspace=1.4)\n\n(df2.groupby(['Age_Band_of_Driver'])\n .mean()\n ['Accident_Severity_Slight']\n .sort_index()\n .plot\n .bar(title = \"Mean Age Band of Driver vs. Accident Severity\",\n      ax = axes[0,0]))\n\n(df2.groupby(['Speed_limit'])\n .mean()\n ['Accident_Severity_Slight']\n .sort_index()\n .plot\n .bar(title = \"Mean Speed limit vs. Accident Severity\",\n      ax = axes[0,1]))\n\n(df2.groupby(['Urban_or_Rural_Area'])\n .mean()\n ['Accident_Severity_Slight']\n .sort_index()\n .plot\n .bar(title = \"Mean Urban or Rural Area vs. Accident Severity\",\n      ax = axes[1,0]))\n\n(df2.groupby(['Vehicle_Manoeuvre'])\n .mean()\n ['Accident_Severity_Slight']\n .sort_values()\n .plot\n .bar(title = \"Mean Vehicle Manoeuvre vs. Accident Severity\",\n      ax = axes[1,1]))\n\nplt.show()","608e2c69":"X = df2.drop(['Accident_Index','Accident_Severity_Slight'], axis=1)\ny = df2.Accident_Severity_Slight\nprint(X.shape,\n      y.shape)","b9c41213":"def get_Speed_limit(df):\n    return df[['Speed_limit']]\n\nFullTransformerOnSpeedLimit = Pipeline([(\"Select_Speed_Limit\", FunctionTransformer(func=get_Speed_limit, validate=False)),\n                                        (\"Fill_Null\",          SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n                                        (\"One_Hot_Encoder\",    OneHotEncoder(sparse = False, handle_unknown='ignore'))\n                                       ])\n\n#FullTransformerOnSpeedLimit.fit_transform(X[:5000], y[:5000])","b5b4b276":"def get_Time(df):\n    return pd.to_datetime(df['Time'], format='%H:%M').dt.time\n\ndef find_time_group(time_object):\n    if time_object<pd.datetime.time(pd.datetime(2000,1,1,5,0)):\n        return 'Night'\n    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,7,0)):\n        return 'Early Morning'\n    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,10,0)):\n        return 'Morning'\n    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,15,0)):\n        return 'Midday'\n    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,18,0)):\n        return 'Afternoon'\n    elif time_object<pd.datetime.time(pd.datetime(2000,1,1,20,0)):\n        return 'Evening'\n    elif time_object<=pd.datetime.time(pd.datetime(2000,1,1,23,59)):\n        return 'Late Evening'\n    return np.nan\n\nFullTransformerOnTime = Pipeline([(\"Select_Time\",     FunctionTransformer(func=get_Time, validate=False)),\n                                  (\"Group_Time\",      FunctionTransformer(func=lambda x: x.apply(find_time_group).to_frame(), validate=False)),\n                                  (\"Fill_Null\",       SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n                                  (\"One_Hot_Encoder\", OneHotEncoder(sparse = False, handle_unknown='ignore'))\n                                 ])\n\n#FullTransformerOnTime.fit_transform(X[:5000], y[:5000])","9f50217a":"def get_Age_of_Vehicle(df):\n    return df[['Age_of_Vehicle']]\n\nFullTransformerOnAgeofVehicle = Pipeline([(\"Select_Age_of_Vehicle\", FunctionTransformer(func=get_Age_of_Vehicle, validate=False)),\n                                          (\"Fill_Null\",             SimpleImputer(missing_values=np.nan, strategy='median'))\n                                         ])\n\n#FullTransformerOnAgeofVehicle.fit_transform(X[:5000], y[:5000])","3b837be5":"def get_make(df):\n    list_of_small_makers = list(df['make'].value_counts()[df['make'].value_counts() < 2000].index)\n    return df['make'].replace(list_of_small_makers, 'Other').to_frame()\n\nFullTransformerOnMake = Pipeline([(\"Select_Make\",      FunctionTransformer(func=get_make, validate=False)),\n                                   (\"Fill_Null\",       SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='Other')),\n                                   (\"One_Hot_Encoder\", OneHotEncoder(sparse = False, handle_unknown='ignore'))])\n\n#FullTransformerOnMake.fit_transform(X[:5000], y[:5000])","cb98668c":"def get_Engine_Capacity(df):\n    return df[['Engine_Capacity_.CC.']]\n\nFullTransformerOnEngineCapacity = Pipeline([(\"Select_Engine_Capacity\",       FunctionTransformer(func=get_Engine_Capacity, validate=False)),\n                                            (\"Fill_Null\",                    SimpleImputer(missing_values=np.nan, strategy='most_frequent')),\n                                            (\"Car_Types_by_Engine_Capacity\", KBinsDiscretizer(n_bins=7, encode='ordinal', strategy='quantile')),\n                                            (\"One_Hot_Encoder\",              OneHotEncoder(sparse = False, handle_unknown='ignore'))\n                                           ])\n\n#FullTransformerOnEngineCapacity.fit_transform(X[:5000], y[:5000])\n#FullTransformerOnEngineCapacity.named_steps[\"Car_Types_by_Engine_Capacity\"].bin_edges_[0]","2ad2ce73":"def get_columns_to_one_hot(df):\n    return df[['1st_Road_Class', 'Day_of_Week', 'Junction_Detail', 'Light_Conditions', 'Number_of_Casualties', \n               'Number_of_Vehicles', 'Road_Surface_Conditions', 'Road_Type', 'Special_Conditions_at_Site', \n               'Urban_or_Rural_Area', 'Weather_Conditions', 'Age_Band_of_Driver', 'Hit_Object_in_Carriageway',\n               'Hit_Object_off_Carriageway', 'Sex_of_Driver', 'Skidding_and_Overturning',\n               'Vehicle_Manoeuvre', 'Vehicle_Type'\n              ]]\n\nDataToOneHotTransformerOnColumns = Pipeline([(\"Select_Columns\",  FunctionTransformer(func=get_columns_to_one_hot, validate=False)),\n                                             (\"One_Hot_Encoder\", OneHotEncoder(sparse = False, handle_unknown='ignore'))])\n\n#DataToOneHotTransformerOnColumns.fit_transform(X[:5000], y[:5000])","50afebe2":"FeatureUnionTransformer = FeatureUnion([\n                                        (\"FTAgeofVehicle\",   FullTransformerOnAgeofVehicle),\n                                        (\"FTEngineCapacity\", FullTransformerOnEngineCapacity),\n                                        (\"FTMake\",           FullTransformerOnMake),\n                                        (\"FTSpeedLimit\",     FullTransformerOnSpeedLimit),\n                                        (\"FTTime\",           FullTransformerOnTime),\n                                        (\"OHEColumns\",       DataToOneHotTransformerOnColumns)])\n\n#FeatureUnionTransformer.fit_transform(X[:5000], y[:5000])","fae5c938":"Full_Transformer = Pipeline([\n                           (\"Feature_Engineering\", FeatureUnionTransformer),\n                           (\"Min_Max_Transformer\", MaxAbsScaler())\n                           ])\n\n#Full_Transformer.fit(X[:5000], y[:5000])","24208f83":"X_train, X_test, y_train, y_test = split(X, y)","8aeab8ee":"%%time\n\nclf = LogisticRegression(class_weight = \"balanced\")\n\nFull_Transformer.fit(X_train)\nX_train_transformed = Full_Transformer.transform(X_train)\nclf.fit(X_train_transformed, y_train)\n\nX_test_transformed = Full_Transformer.transform(X_test)\n\ny_pred = clf.predict(X_test_transformed)\n\nprint('Classification Report:',classification_report(y_test, y_pred))\n\nprint('Score:',roc_auc_score(y_test.values, clf.predict_proba(X_test_transformed)[:, 1]))","f532fff5":"%%time\n\nclf = RandomForestClassifier(n_estimators=100, n_jobs=3)\n\nFull_Transformer.fit(X_train)\nX_train_transformed = Full_Transformer.transform(X_train)\nclf.fit(X_train_transformed, y_train)\n\nX_test_transformed = Full_Transformer.transform(X_test)\n\ny_pred = clf.predict(X_test_transformed)\n\nprint('Classification Report:',classification_report(y_test, y_pred))\n\nprint('Score:',roc_auc_score(y_test.values, clf.predict_proba(X_test_transformed)[:, 1]))","09a5ff02":"LogisticRegression_Full_Estimator = Pipeline([\n                                              (\"Feature_Engineering\", FeatureUnionTransformer),\n                                              (\"Min_Max_Transformer\", MaxAbsScaler()),\n                                              (\"Clf\",                 LogisticRegression(class_weight = \"balanced\"))\n                                             ])\n\n#LogisticRegression_Full_Estimator.fit(X[:5000], y[:5000])","dc21d8db":"%%time\n\nLogisticRegression_Full_Estimator.fit(X_train, y_train)\nLogisticRegression_Full_Estimator.predict(X_train)\nLogisticRegression_Full_Estimator.predict(X_test)\n\nprint('Classification Report:' '\\n',\n      classification_report(y_test, LogisticRegression_Full_Estimator.predict(X_test)))\nprint('Score:',roc_auc_score(y_test.values, LogisticRegression_Full_Estimator.predict_proba(X_test)[:, 1]))","86795d6f":"RandomForest_Full_Estimator = Pipeline([\n                                        (\"Feature_Engineering\", FeatureUnionTransformer),\n                                        (\"Min_Max_Transformer\", MaxAbsScaler()),\n                                        (\"Clf\",                 RandomForestClassifier(n_estimators=100, n_jobs=3))\n                                       ])\n\n#RandomForest_Full_Estimator.fit(X[:5000], y[:5000])","e5c2eb28":"%%time\n\nRandomForest_Full_Estimator.fit(X_train, y_train)\nRandomForest_Full_Estimator.predict(X_train)\nRandomForest_Full_Estimator.predict(X_test)\n\nprint('Classification Report:' '\\n',\n      classification_report(y_test, RandomForest_Full_Estimator.predict(X_test)))\nprint('Score:',roc_auc_score(y_test.values, RandomForest_Full_Estimator.predict_proba(X_test)[:, 1]))","021eb062":"#%%time\n#scoreTest_RF = []\n#scoreTrain_RF = []\n#for number in range(1,10):\n#    clf = RandomForestClassifier(max_depth = number,n_estimators = 100, n_jobs=3, class_weight = \"balanced\")\n#    Full_Transformer.fit(X_train)\n#    X_train_transformed = Full_Transformer.transform(X_train)\n#    clf.fit(X_train_transformed, y_train)\n#    X_test_transformed = Full_Transformer.transform(X_test)\n#    y_score_train = clf.predict_proba(X_train_transformed)[:,1]\n#    y_score_test = clf.predict_proba(X_test_transformed)[:,1]\n#\n#    scoreTrain_RF.append(round(roc_auc_score(y_train, y_score_train) , 3))\n#    scoreTest_RF.append(round(roc_auc_score(y_test, y_score_test) , 3))\n    ","b7494e7e":"#pd.DataFrame({'test roc score':scoreTest_RF,'train roc score':scoreTrain_RF}).plot(grid = True)\n#plt.xlabel('Max depth')\n#plt.ylabel('Score')\n#plt.title(\"RandomForestClassifier\")\n#plt.show()","018dcaf2":"#%%time\n#cls_RF = RandomForestClassifier(max_depth = np.array(scoreTest_RF).argmax(),n_estimators = 100, n_jobs=3, class_weight = \"balanced\")\n#Full_Transformer.fit(X_train)\n#X_train_transformed = Full_Transformer.transform(X_train)\n#clf.fit(X_train_transformed, y_train)\n#X_test_transformed = Full_Transformer.transform(X_test)\n#\n#print(\"RF roc_train:\",round(roc_auc_score(y_train, cls_RF.predict_proba(X_train_transformed)[:,1]) , 3))\n#print(\"RF roc_test:\",round(roc_auc_score(y_test, cls_RF.predict_proba(X_test_transformed)[:,1]) , 3))\n#\n#print('RF_cross_score:', cross_val_score(cls_RF, X_train_transformed, y_train, cv=5, scoring='roc_auc').mean())","2e8cd305":"## **From multiclass to two-classes**","97cf941d":"## **Correlation matrix**","59a43e31":"## **4.3 Using the Full Estimator**\n## Logistic Regression","2bce829d":"# **3. Training\/Predicting Pipeline**\n## **Transform Speed Limit**","c4a29a7b":"## **4.1 Logistic Regression**","97d1ea60":"## **Transform Make**","8ec5f312":"# **Outline**\n1. **Introduction**\n2. **Data preparation **\n    * 2.1 Load data\n    * 2.2 Sample the data\n    * 2.3 Check for missing values (NaN)\n    * 2.4 Exploratory Visualization\n    * 2.5 Create a new dataframe\n    * 2.6 Split features and targets from the data\n3. **Training\/Predicting Pipeline**\n4. **Prediction and submission**\n    * 4.1 Logistic Regression\n    * 4.2 Random Forest Classifier\n    * 4.3 Using the Full Estimator","c807cc85":"## **2.5 Create a new dataframe** \n### with only the features we need and want, **25 features overall**","409995bb":"## **2.6 Split features and targets from the data**","626771a8":"## **Number of vehicles distribution**","e61dfe7c":"## **2.4 Exploratory Visualization**\n### **Age of Vehicle**","83961de4":"## **Number of casualties distribution**","034a5cbc":"## **Feature Union**","edf829e6":"![car-crash.png](attachment:car-crash.png)","7066cc68":"## **Data To OneHot Transformer On Columns**","7183e838":"## **Transform Time**","d200b8ec":"## **2.2 Sample the data** \n### by reducing rows with Slight Accident Severity\n","1733cb5e":"# **1. Introduction**\n\nThe UK government collects and publishes (usually on an annual basis) detailed information about traffic accidents across the country. This information includes, but is not limited to, geographical locations, weather conditions, type of vehicles, number of casualties and vehicle manoeuvres, making this a very interesting and comprehensive dataset for analysis and research.\n\nThe data come from the Open Data website of the UK government, where they have been published by the Department of Transport.\n\nThe dataset comprises of two csv files:\n\n* Accident_Information.csv: every line in the file represents a unique traffic accident (identified by the Accident_Index column), featuring various properties related to the accident as columns. Date range: 2005-2017\n* Vehicle_Information.csv: every line in the file represents the involvement of a unique vehicle in a unique traffic accident, featuring various vehicle and passenger properties as columns. Date range: 2004-2016\n\nOur target is to predict the accident severity. The severity is devided to two catagories; severe and slight.\n\nWe had more than 2 million observations and close to 60 features. So, we sampled the data into about 600K observations and 23 features.\n\nTwo models were selected - Logistic Regression and the Random Forest Classifier.","6edf8f9d":"## ** Distribution of accidents over the day**","3910dc91":"### **Engine capacity feature**","efdea69e":"## **4.2 Random Forest Classifier**","9a199066":"## **Transform Engine Capacity**","6acee142":"## **2.3 Check for missing values (NaN)**\n### some will be filled, some will get omitted","00b8fb67":"## **Transform Age of Vehicle**","fcf74841":"# **2. Data Preparation**\n## **2.1 Load Data**","515d0001":"## Random Forest Classifier","0663b619":"# **Accident Severity Classification**","1890de67":"# **4. Prediction and submission**"}}