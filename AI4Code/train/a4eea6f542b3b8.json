{"cell_type":{"c3d1334e":"code","bfed14e2":"code","e8e28e54":"code","48b3f8d7":"code","57bd6475":"code","a53ca117":"code","410ad440":"code","25c0305b":"code","0cf5f201":"code","af786a2e":"code","0823afd7":"code","7accd0cb":"code","bf72d9f7":"code","fd5c7162":"code","eaedcf72":"code","28f9c1b6":"code","9d2777fc":"markdown","b4830e55":"markdown","793a27bd":"markdown","4f4879fd":"markdown","9c74a4fd":"markdown","c0a01342":"markdown","0ca82d90":"markdown","8c70898b":"markdown","dd785175":"markdown","f06fb3e9":"markdown","447149d0":"markdown"},"source":{"c3d1334e":"%%capture\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bfed14e2":"import os\nimport ast\nimport PIL\nimport cv2\nimport pandas as pd\nfrom os import listdir\nfrom os.path import isfile,join\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go","e8e28e54":"DATA_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'\nimages_path = join(DATA_PATH,'train_images')\ndf_train = pd.read_csv(join(DATA_PATH,'train.csv'))","48b3f8d7":"df_train = pd.read_csv(join(DATA_PATH,'train.csv'))\ndf_train[\"img_path\"] = os.path.join(DATA_PATH, \"train_images\")+\"\/video_\"+df_train.video_id.astype(str)+\"\/\"+df_train.video_frame.astype(str)+\".jpg\"","57bd6475":"from PIL import Image\n\ndef video_stats(path):\n    # Lookfor files within video folder\n    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n    # Filter files by extension\n    onlyfiles = [f for f in onlyfiles if f.endswith(\".jpg\")]\n    im = Image.open(join(path,onlyfiles[0]))\n    width, height = im.size\n    print(f'Number of frames: {len(onlyfiles)}')\n    print(f'Frames with size (w,h): ({width},{height})')","a53ca117":"# Video 0\nprint('Video 0 Stats:')\nvideo_stats(join(images_path,'video_0'))\n\n# Video 1\nprint(\"\\n\",'Video 1 Stats:')\nvideo_stats(join(images_path,'video_1'))\n\n# Video 2\nprint(\"\\n\",'Video 2 Stats:')\nvideo_stats(join(images_path,'video_2'))","410ad440":"df_train_video_group = df_train.groupby(\"video_id\")[\"video_frame\"].max()\nfig = px.bar(df_train_video_group, \n             color=px.colors.qualitative.Plotly[:3],\n             labels={\"sequence\":\"Video id\", \"value\":\"N\u00b0 of frames\", \"variable\":\"Original Column Name\"},\n             title=\"Number of frames per video ID\")\n\nfig.update_layout(xaxis=dict(type='category'), showlegend=False)\nfig.show()","25c0305b":"df_train_sequence_group = df_train.groupby(\"sequence\")[[\"sequence_frame\", \"video_id\"]].max().sort_values(by=\"video_id\")\ndf_train_sequence_group[\"video_id\"] = df_train_sequence_group[\"video_id\"].astype(str) # For label color mode\n\nfig = px.bar(df_train_sequence_group, \n             color=\"video_id\",\n             labels={\"sequence\":\"Sequence id\", \"value\":\"Number Of Frames\", \"variable\":\"Original Column Name\"},\n             title=\"Number Of Frames In Each Sequence\")\n\nfig.update_layout(xaxis=dict(type='category'), showlegend=True)\nfig.show()","0cf5f201":"# Train stats\nsamples_without_annotations=len(df_train[df_train['annotations']=='[]'])\n#ax = sns.barplot(x=['Without bbox','With bbox'], y=[samples_without_annotations,(len(df_train) - samples_without_annotations)])\ncolors = ['lightslategray',] * 2 \ncolors[1] = 'crimson'\nlabels = ['Without bbox','With bbox']\n\nfig = go.Figure([go.Bar(x=labels, \n                        y=[samples_without_annotations, len(df_train) - samples_without_annotations],\n                        marker_color=px.colors.qualitative.Plotly[:2])])\nfig.show()\nprint(f'Number of training samples: {len(df_train)}')\nprint(f'Training samples without object labels: {samples_without_annotations}')\nprint(f'Training samples with object labels: {len(df_train) - samples_without_annotations}')","af786a2e":"df_train[\"annotations\"] = df_train[\"annotations\"].apply(lambda x: ast.literal_eval(x))\ndf_train[\"num_boxes\"] = df_train[\"annotations\"].apply(len)\ndf_train[\"video_id\"] = df_train[\"video_id\"].astype(str)\ndf_train[\"sequence\"] = df_train[\"sequence\"].astype(str)","0823afd7":"df_annotations_count = df_train.groupby('num_boxes')['annotations'].count()","7accd0cb":"df_annotations_count = df_annotations_count.drop([0]);","bf72d9f7":"# Information about the number of samples with bounding boxes\nfig = px.bar(df_annotations_count)\nfig.update_layout(xaxis=dict(type='category'), showlegend=False)\nfig.show()","fd5c7162":"# View nunber of bounding boxes per frame in each sequence\nfig = px.histogram(df_train, x=\"sequence\", color=\"num_boxes\",\n             labels={\"sequence\":\"Sequence ID\", \"num_boxes\":\"N\u00b0 of Boxes per frame\"},\n             title=\"Number of annotations in each sequence\")\nfig.show()","eaedcf72":"import tensorflow as tf\nimport matplotlib.pyplot as plt\n\ndef tf_load_img(img_path, reshape_to=None):\n    if reshape_to is None:\n        return tf.image.decode_image(tf.io.read_file(img_path), channels=3)\n    else:\n        return tf.image.resize(tf.image.decode_image(tf.io.read_file(img_path), channels=3), reshape_to)\n    \ndef get_tl_br(bbox):\n    \"\"\" Return the top-left and bottom-right bounding box \"\"\"\n    return (bbox['x'], bbox['y']), (bbox['x']+bbox[\"width\"], bbox['y']+bbox[\"height\"])\n    \ndef plot_image(img_path, annotations=None, **kwargs):\n    \"\"\" Plot an image and bounding boxes \"\"\"\n    img = np.array(tf_load_img(img_path))\n    \n    if annotations:\n        plt.figure(figsize=(20,10))\n        for i, bbox in enumerate(annotations):\n            tl_box, br_box = get_tl_br(bbox)\n            img = cv2.rectangle(img, tl_box, br_box, (255-2*i,14*i,0), 4)\n        plt.imshow(img)\n        plt.axis(False)\n        plt.title(f\"Bounding boxes plotted: ({len(annotations)})\")\n    else:\n        plt.figure(figsize=(20,10))\n        plt.imshow(img)\n        plt.axis(False)\n        plt.title(\"No bounding boxes within the image\")\n    plt.tight_layout()\n    plt.show()","28f9c1b6":"num_bbox_for_visualization = [1,3,7,15]\nfor num_bbox in sorted(num_bbox_for_visualization):\n    ex_row = df_train[df_train.num_boxes==num_bbox].reset_index(drop=True).iloc[0]\n    plot_image(**ex_row)","9d2777fc":"## 2 Train dataframe analysis","b4830e55":"# EDA - Exploratory data analysis\n\nUpvote if you found useful","793a27bd":"### 2.1 Video details","4f4879fd":"## Content\n\n1. **Video frames numbers**\n2. **Train dataframe analysis: video analysis and annotations numbers (per video and sequence)**\n3. **Visualizing some training examples**\n","9c74a4fd":"# Dataset overview\n## 1. Video frames stats","c0a01342":"<p align = \"center\">\n<img src = \"https:\/\/econews.com.au\/wp-content\/uploads\/2016\/10\/Crown-of-thorns-starfish-attached-healthy-coral-reef-Mackay-.jpg\">\n<\/p>\n<p align = \"center\">\nCrown-of-thorns starfish (Image from econews.com.au)\n<\/p>","0ca82d90":"## 3 Visualizing some training examples","8c70898b":"However, each video has several sequences. Let's see in a more detailed way.","dd785175":"### 2.2 Annotation analysis","f06fb3e9":"- `video_id`\u00a0- ID number of the video the image was part of. The video ids are not meaningfully ordered.\n- `video_frame`\u00a0- The frame number of the image within the video. Expect to see occasional gaps in the frame number from when the diver surfaced.\n- `sequence`\u00a0- ID of a gap-free subset of a given video. The sequence ids are not meaningfully ordered.\n- `sequence_frame`\u00a0- The frame number within a given sequence.\n- `image_id`\u00a0- ID code for the image, in the format '{video_id}-{video_frame}'\n- `annotations`\u00a0- The bounding boxes of any starfish detections in a string format that can be evaluated directly with Python. Does not use the same format as the predictions you will submit. Not available in\u00a0**test.csv.**\u00a0A bounding box is described by the pixel coordinate (x_min, y_min) of its upper left corner within the image together with its width and height in pixels.","447149d0":"Display the number of frames per video"}}