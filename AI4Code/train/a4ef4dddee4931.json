{"cell_type":{"63ebbf92":"code","75b9e2cb":"code","e37b46a3":"code","4be7a975":"code","576b5612":"code","2fb7918f":"code","364e6d52":"code","bc6af028":"code","637d3baa":"code","3a27700f":"code","23d86e34":"code","605b0490":"code","2cd47346":"code","74c7306e":"code","426a88c7":"code","a335c379":"code","698372cd":"code","f85555f2":"code","138ee7e8":"code","70f7bdeb":"code","a925b1d9":"code","014ab86b":"markdown","a5d1315e":"markdown","5027bed9":"markdown","19d59e49":"markdown","66683d5a":"markdown","212c3e1c":"markdown","71782954":"markdown","3cb8d90f":"markdown","946673cd":"markdown","3830ed6d":"markdown","61c67c45":"markdown","4b4ef727":"markdown","eb619013":"markdown"},"source":{"63ebbf92":"# Data Preprocessing and Model Building Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nimport lightgbm as lgb\nimport optuna\nfrom catboost import CatBoostRegressor\n\n# Libraries for Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')\n\n# Fearture Tools: For Feature Engineering\nimport featuretools as ft\nimport optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances\n\n# For Not displaying Warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")","75b9e2cb":"df_train = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df_train.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\nf, ax = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\nf.suptitle('Distribution of Numerical Features', fontsize=16)\nsns.distplot(df_train['cont0'], ax=ax[0, 0])\nsns.distplot(df_train['cont1'], ax=ax[0, 1])\nsns.distplot(df_train['cont2'], ax=ax[0, 2])\nsns.distplot(df_train['cont3'], ax=ax[0, 3])\n\nsns.distplot(df_train['cont4'], ax=ax[1, 0])\nsns.distplot(df_train['cont5'], ax=ax[1, 1])\nsns.distplot(df_train['cont6'], ax=ax[1, 2])\nsns.distplot(df_train['cont7'], ax=ax[1, 3])\n\nsns.distplot(df_train['cont8'], ax=ax[2, 0])\nsns.distplot(df_train['cont9'], ax=ax[2, 1])\nsns.distplot(df_train['cont10'], ax=ax[2, 2])\nsns.distplot(df_train['cont11'], ax=ax[2, 3])\n\nsns.distplot(df_train['cont12'], ax=ax[3, 0])\nsns.distplot(df_train['cont13'], ax=ax[3, 1])\nf.delaxes(ax[3, 2])\nf.delaxes(ax[3, 3])\nplt.tight_layout()\nplt.show();","e37b46a3":"f, ax = plt.subplots(nrows=3, ncols=3, figsize=(12, 12))\nf.suptitle('Distribution of Categorical Features', fontsize=16)\nsns.countplot(df_train['cat0'], ax=ax[0, 0])\nsns.countplot(df_train['cat1'], ax=ax[0, 1])\nsns.countplot(df_train['cat2'], ax=ax[0, 2])\nsns.countplot(df_train['cat3'], ax=ax[1, 0])\nsns.countplot(df_train['cat4'], ax=ax[1, 1])\nsns.countplot(df_train['cat5'], ax=ax[1, 2])\nsns.countplot(df_train['cat6'], ax=ax[2, 0])\nsns.countplot(df_train['cat7'], ax=ax[2, 1])\nsns.countplot(df_train['cat8'], ax=ax[2, 2])\n\nplt.tight_layout()\nplt.show();","4be7a975":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\n######## For Training set\n# initial set up of FeatureTool\nes = ft.EntitySet(id = 'data')\nes.entity_from_dataframe(entity_id = 'original_train_data', \n                         dataframe = df[numerical_cols], \n                         index='id') \n\n#creating separate dataframe with feature Engineered columns\nfeature_matrix_train, feature_defs_train = ft.dfs(entityset = es,                                          \n                                      target_entity = 'original_train_data',                               \n                                      trans_primitives = ['add_numeric', 'multiply_numeric'],  \n                                      verbose=1)  \n\n# adding new features into original dataframe\nfor i in feature_matrix_train.iloc[:,14:].columns:\n    df[i] = feature_matrix_train[i]\n\n\n######## Using same technique on Test Set\n\n# initial set up of FeatureTool\nes = ft.EntitySet(id = 'data')\nes.entity_from_dataframe(entity_id = 'original_test_data', \n                         dataframe = df_test[numerical_cols], \n                         index='id') \n\n#creating separate dataframe with feature Engineered columns\nfeature_matrix_test, feature_defs_test = ft.dfs(entityset = es,                                          \n                                      target_entity = 'original_test_data',                               \n                                      trans_primitives = ['add_numeric', 'multiply_numeric'],  \n                                      verbose=1)  \n\n# adding new features into original dataframe\nfor i in feature_matrix_test.iloc[:,14:].columns:\n    df_test[i] = feature_matrix_test[i]\n\n#adding new features to numerical columns list\nnumerical_cols = list(feature_matrix_train.columns)\nuseful_features = object_cols + numerical_cols\n\n# Model 1: XGB using FeatureTool\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])    \n\n\n    model = XGBRegressor(\n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=11000,\n        learning_rate=0.03875678489649957,\n        reg_lambda=0.000899770960119882,\n        reg_alpha=0.00026578249068599785,\n        subsample=0.9532248864902615,\n        colsample_bytree=0.16561394428070153,\n        max_depth=3,\n    )\n    \n    \n    model.fit(xtrain, ytrain)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_XGB\"]\nfinal_valid_predictions.to_csv(\"train_pred_XGB.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_XGB\"]\nsample_submission.to_csv(\"test_pred_XGB.csv\", index=False)","576b5612":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"rmse\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"n_estimators\": 10000,\n        \"early_stopping_round\": 300,\n        \"device\": \"gpu\",\n        \"gpu_platform_id\": 0,\n        \"gpu_device_id\": 0,\n    }\n    \n    param2 = {\n        'lambda_l1': 0.00472279780583036, \n        'lambda_l2': 2.9095205689488508e-05, \n        'num_leaves': 158, \n        'feature_fraction': 0.7386878356648194, \n        'bagging_fraction': 0.8459744550725283, \n        'bagging_freq': 2, \n        'max_depth': 2, \n        'max_bin': 249, \n        'learning_rate': 0.044738463593017294,\n        'min_child_samples': 13\n    }\n    param.update(param2)\n    \n    lgb_train = lgb.Dataset(xtrain, ytrain)\n    lgb_valid = lgb.Dataset(xvalid, yvalid, reference=lgb_train)\n\n    model = lgb.train(param, lgb_train, valid_sets=[lgb_valid], verbose_eval=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n    \n    \nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_LGB\"]\nfinal_valid_predictions.to_csv(\"train_pred_LGB.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_LGB\"]\nsample_submission.to_csv(\"test_pred_LGB.csv\", index=False)","2fb7918f":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n    \n    temp_test_feat \/= 5\n    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    df = pd.concat(temp_df)\n    \n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    params = {'learning_rate': 0.07853392035787837, 'reg_lambda': 1.7549293092194938e-05,\n              'reg_alpha': 14.68267919457715, 'subsample': 0.8031450486786944, 'colsample_bytree': 0.170759104940733, \n              'max_depth': 3}\n    \n    model = XGBRegressor(\n        random_state=0, \n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor=\"gpu_predictor\",\n        n_estimators=5000,\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_XGB1\"]\nfinal_valid_predictions.to_csv(\"train_pred_XGB1.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_XGB1\"]\nsample_submission.to_csv(\"test_pred_XGB1.csv\", index=False)","364e6d52":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    params = {\n        'random_state': 1, \n        'booster': 'gbtree',\n        'n_estimators': 10000,\n        'learning_rate': 0.03628302216953097,\n        'reg_lambda': 0.0008746338866473539,\n        'reg_alpha': 23.13181079976304,\n        'subsample': 0.7875490025178415,\n        'colsample_bytree': 0.11807135201147481,\n        'max_depth': 3\n    }\n    \n    model = XGBRegressor(\n        n_jobs=-1,\n        **params\n    )\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_XGB3\"]\nfinal_valid_predictions.to_csv(\"train_pred_XGB3.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_XGB3\"]\nsample_submission.to_csv(\"test_pred_XGB3.csv\", index=False)","bc6af028":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\ndf_test = df_test[useful_features]  \n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n    \n    temp_test_feat \/= 5\n    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    df = pd.concat(temp_df)\n    \n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n    \ndef run(trial):\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain['target']\n        yvalid = xvalid['target']\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        ordinal_encoder = preprocessing.OrdinalEncoder()\n        xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n        xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n        \n        \n        cat_parameters_1 = {'iterations':trial.suggest_int(\"iterations\", 5000, 8000),\n             'learning_rate':trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n                            'l2_leaf_reg':trial.suggest_int(\"l2_leaf_reg\", 5, 200),\n             'random_strength':trial.suggest_float(\"random_strength\", 0.1, 5),'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        \n        \n        \n        model = CatBoostRegressor(**cat_parameters_1, task_type = 'GPU')\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n        \n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","637d3baa":"# Plotting Important Parameters\nplot_param_importances(study)","3a27700f":"# Plotting history of Study\nplot_optimization_history(study)","23d86e34":"# CAtBoost\n\ndf = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if 'cat' in col]\nnumerical_cols = [col for col in useful_features if col.startswith(\"cont\")]\ndf_test = df_test[useful_features]\n\n\nfor col in object_cols:\n    temp_df = []\n    temp_test_feat = None\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        feat = xtrain.groupby(col)[\"target\"].agg(\"mean\")\n        feat = feat.to_dict()\n        xvalid.loc[:, f\"tar_enc_{col}\"] = xvalid[col].map(feat)\n        temp_df.append(xvalid)\n        if temp_test_feat is None:\n            temp_test_feat = df_test[col].map(feat)\n        else:\n            temp_test_feat += df_test[col].map(feat)\n    \n    temp_test_feat \/= 5\n    df_test.loc[:, f\"tar_enc_{col}\"] = temp_test_feat\n    df = pd.concat(temp_df)\n    \n\nuseful_features = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\nobject_cols = [col for col in useful_features if col.startswith(\"cat\")]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n    \n    valid_ids = xvalid.id.values.tolist()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ordinal_encoder = preprocessing.OrdinalEncoder()\n    xtrain[object_cols] = ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols] = ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols] = ordinal_encoder.transform(xtest[object_cols])\n    \n    \n    params = {'od_type':'Iter','iterations': 5376,\n 'learning_rate': 0.023320327820924816,\n 'l2_leaf_reg': 181,\n 'random_strength': 3.391745124598448,'grow_policy':'Lossguide',\n                        'leaf_estimation_method':'Newton', \n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n\n    \n    model = CatBoostRegressor(**params, task_type = 'GPU')\n    model.fit(xtrain, ytrain, verbose =500)\n    \n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n\nprint(np.mean(scores), np.std(scores))\nfinal_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_CatB\"]\nfinal_valid_predictions.to_csv(\"train_pred_CatB.csv\", index=False)\n\nsample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.columns = [\"id\", \"pred_CatB\"]\nsample_submission.to_csv(\"test_pred_CatB.csv\", index=False)\n","605b0490":"df = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\n\ndf1 = pd.read_csv(\".\/train_pred_XGB.csv\")\ndf2 = pd.read_csv(\".\/train_pred_LGB.csv\")\ndf3 = pd.read_csv(\".\/train_pred_XGB1.csv\")\ndf4 = pd.read_csv(\".\/train_pred_XGB3.csv\")\ndf5 = pd.read_csv(\".\/train_pred_CatB.csv\")\n\n\ndf_test1 = pd.read_csv(\".\/test_pred_XGB.csv\")\ndf_test2 = pd.read_csv(\".\/test_pred_LGB.csv\")\ndf_test3 = pd.read_csv(\".\/test_pred_XGB1.csv\")\ndf_test4 = pd.read_csv(\".\/test_pred_XGB3.csv\")\ndf_test5 = pd.read_csv(\".\/test_pred_CatB.csv\")\n\ndf = df.merge(df1, on=\"id\", how=\"left\")\ndf = df.merge(df2, on=\"id\", how=\"left\")\ndf = df.merge(df3, on=\"id\", how=\"left\")\ndf = df.merge(df4, on=\"id\", how=\"left\")\ndf = df.merge(df5, on=\"id\", how=\"left\")\n\ndf_test = df_test.merge(df_test1, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test2, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test3, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test4, on=\"id\", how=\"left\")\ndf_test = df_test.merge(df_test5, on=\"id\", how=\"left\")\n\ndf.head()","2cd47346":"useful_features = [\"pred_XGB\",\"pred_XGB3\", \"pred_XGB1\", \"pred_LGB\",\"pred_CatB\"] \n    \ndef run(trial):\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain['target']\n        yvalid = xvalid['target']\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n        reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n        reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n        subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n        colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n        max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n        \n        model = XGBRegressor(\n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        predictor=\"gpu_predictor\",\n        n_estimators=12000,\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth)\n        \n        \n        model.fit(xtrain, ytrain, early_stopping_rounds=300,eval_set=[(xvalid, yvalid)],  verbose=1000)\n        \n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n        \n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","74c7306e":"# Plotting Important Parameters\nplot_param_importances(study)","426a88c7":"# Plotting history of Study\nplot_optimization_history(study)","a335c379":"useful_features = [\"pred_XGB\",\"pred_XGB3\", \"pred_XGB1\", \"pred_LGB\",\"pred_CatB\"]\n   \n    \ndef run(trial):\n    for fold in range(5):\n        xtrain =  df[df.kfold != fold].reset_index(drop=True)\n        xvalid = df[df.kfold == fold].reset_index(drop=True)\n        xtest = df_test.copy()\n\n        ytrain = xtrain['target']\n        yvalid = xvalid['target']\n\n        xtrain = xtrain[useful_features]\n        xvalid = xvalid[useful_features]\n        \n        \n        cat_parameters_1 = {'iterations':trial.suggest_int(\"iterations\", 5000, 8000),\n             'learning_rate':trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True),\n                            'l2_leaf_reg':trial.suggest_int(\"l2_leaf_reg\", 5, 200),\n             'random_strength':trial.suggest_float(\"random_strength\", 0.1, 5),'grow_policy':'Depthwise',\n                        'leaf_estimation_method':'Newton', 'od_type':'Iter',\n             'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n        \n        \n        \n        \n        model = CatBoostRegressor(**cat_parameters_1)\n        model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n        \n        preds_valid = model.predict(xvalid)\n        rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    return rmse\n\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)\n\nstudy.best_params","698372cd":"# Plotting Important Parameters\nplot_param_importances(study)","f85555f2":"# Plotting history of Study\nplot_optimization_history(study)","138ee7e8":"useful_features = [\"pred_XGB\",\"pred_XGB1\", \"pred_XGB3\", \"pred_LGB\"]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n        \n    model = XGBRegressor(\n        random_state=fold,\n        tree_method=\"gpu_hist\",\n        gpu_id=1,\n        predictor=\"gpu_predictor\",\n        n_estimators=12000,\n        learning_rate=0.024882747879237756,\n        reg_lambda= 0.000827524788778563,\n        reg_alpha=8.633187860723039e-07,\n        subsample=0.16151014613960882,\n        colsample_bytree=0.6057298386505088,\n        max_depth=1)\n        \n    model.fit(xtrain, ytrain, early_stopping_rounds=300,eval_set=[(xvalid, yvalid)],  verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n    \nprint(np.mean(scores), np.std(scores))","70f7bdeb":"useful_features = [\"pred_XGB\",\"pred_XGB1\", \"pred_XGB3\", \"pred_LGB\"]\ndf_test = df_test[useful_features]\n\n\nfinal_test_predictions = []\nscores = []\nfor fold in range(5):\n    xtrain =  df[df.kfold != fold].reset_index(drop=True)\n    xvalid = df[df.kfold == fold].reset_index(drop=True)\n    xtest = df_test.copy()\n\n    ytrain = xtrain.target\n    yvalid = xvalid.target\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]    \n    \n    cat_parameters_1 = {'iterations': 6487,\n 'learning_rate': 0.040692021622714805,\n 'l2_leaf_reg': 99,\n 'random_strength': 2.0689175540262017,'grow_policy':'Depthwise',\n                    'leaf_estimation_method':'Newton', 'od_type':'Iter',\n         'bootstrap_type':'Bayesian','thread_count':-1,'verbose':False,'loss_function':'RMSE','eval_metric':'RMSE'}\n\n    model = CatBoostRegressor(**cat_parameters_1)\n    model.fit(xtrain, ytrain, early_stopping_rounds=300, eval_set=[(xvalid, yvalid)], verbose=1000)\n    preds_valid = model.predict(xvalid)\n    test_preds = model.predict(xtest)\n    final_test_predictions.append(test_preds)\n    rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n    print(fold, rmse)\n    scores.append(rmse)\n    \nprint(np.mean(scores), np.std(scores))","a925b1d9":"sample_submission.target = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","014ab86b":"# **CatBoost using Target Encoding**\n### **Model Tuning**","a5d1315e":"# **Final Model CatBoost**","5027bed9":"# **Blending All Models**","19d59e49":"# **Final Model XGB**","66683d5a":"# **Model 3: XGB + Target Encoding**","212c3e1c":"# **XGB3: Ordinal Encoding**\n\n","71782954":"# **Final Model 1: Tuning XGB**","3cb8d90f":"## Refrences: \n1. https:\/\/www.kaggle.com\/abhishek\/code : Optimization, Blending etc\n2. https:\/\/www.kaggle.com\/kingoffitpredict\/time-optimization#Featuretools : Feature Tools","946673cd":"# **Final Model 2: Tuning CatBoost**","3830ed6d":"# **Select the best of two**","61c67c45":"# **Model 2: Light GBM**","4b4ef727":"### **Model building using tuned Parameters**","eb619013":"# **Thank you**"}}