{"cell_type":{"d82be48b":"code","1fc7695b":"code","467606e9":"code","661be866":"code","98509c43":"code","3ec2a5f8":"code","552dc4ed":"code","4fd09ecd":"code","bb82dd1a":"code","d3909763":"code","e3fadd76":"code","965ccc79":"code","04bf4a29":"code","fcfa18c9":"code","b2120523":"code","2c88c181":"code","40899699":"code","93316b5f":"code","bb9dde18":"code","ba5b9373":"code","62d098cf":"code","73223cca":"code","710884c2":"code","fd0087ba":"code","e5a77ddb":"code","ec33fc1c":"code","b7b5dad3":"code","4f59c7e1":"code","c6bcde61":"code","a2453fd2":"code","060821ed":"code","1e141ba3":"code","676ba33e":"code","735acf7b":"code","a4c39dc9":"code","8c092546":"code","426ba7e5":"code","8ab4565e":"code","c967517e":"code","af628bcb":"code","c6e8c207":"code","151b23b2":"code","13850d27":"code","12cf2996":"code","85dedd48":"code","98cdd853":"markdown","f2dccd1b":"markdown","21b89b10":"markdown","35206aef":"markdown","7173a985":"markdown","4deefaa7":"markdown","eb0d72b2":"markdown","66ff623c":"markdown","d9471394":"markdown","98287509":"markdown","565bbedf":"markdown","116877df":"markdown","394c09e2":"markdown","68580cb1":"markdown","a54bbce3":"markdown","48432910":"markdown","5e391497":"markdown","288186fc":"markdown","ed5d7372":"markdown","8b958743":"markdown","7fb98942":"markdown","d1de77eb":"markdown"},"source":{"d82be48b":"import torch\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom scipy.stats import norm\n#from sklearn.utils import resample","1fc7695b":"df = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv') #Reading the csv","467606e9":"df.head() #Displaying the first 5 rows of the dataframe ","661be866":"df.count().sort_values() #Checking how many values each column have.","98509c43":"df.isna().sum().sort_values() #Checking how many missing values each column have.","3ec2a5f8":"df = df.drop(columns=['Sunshine','Evaporation','Cloud3pm','Cloud9am','Location','Date'],axis=1) #We are droping all columns with less than 60% of data","552dc4ed":"df = df.dropna(how='any') #We are dropping all the rows with any missing value.","4fd09ecd":"from scipy import stats\n\nz = np.abs(stats.zscore(df._get_numeric_data()))\nprint(z)\ndf= df[(z < 3).all(axis=1)]\nprint(df.shape)","bb82dd1a":"df['RainToday'].replace({'No': 0, 'Yes': 1},inplace = True)\ndf['RainTomorrow'].replace({'No': 0, 'Yes': 1},inplace = True)","d3909763":"categorical_columns = ['WindGustDir', 'WindDir3pm', 'WindDir9am']\ndf = pd.get_dummies(df, columns=categorical_columns)","e3fadd76":"from sklearn import preprocessing\n\nscaler = preprocessing.MinMaxScaler()\nscaler.fit(df)\ndf = pd.DataFrame(scaler.transform(df), index=df.index, columns=df.columns)","965ccc79":"from sklearn.feature_selection import SelectKBest, chi2\n\nX = df.loc[:,df.columns!='RainTomorrow']\ny = df[['RainTomorrow']]\n\nselector = SelectKBest(chi2, k=3)\nselector.fit(X, y)\n\nX_new = selector.transform(X)\nprint(X.columns[selector.get_support(indices=True)]) #top 3 columns","04bf4a29":"df[[\"Humidity3pm\",\"RainTomorrow\"]].groupby([\"RainTomorrow\"], as_index = False).mean().sort_values(by = \"RainTomorrow\").style.background_gradient(\"Reds\")","fcfa18c9":"plt.figure(figsize=(13,10))\nplt.subplot(2,2,1)\nplt.hist(df[\"Humidity3pm\"], color=\"orange\")\nplt.xlabel(\"Humidity3pm\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Humidity3pm histogram\", color = \"black\", fontweight='bold', fontsize = 11)\nplt.subplot(2,2,2)\nsns.distplot(df[\"Humidity3pm\"], fit=norm, color=\"orange\")\nplt.title(\"Humidity3pm Distplot\", color = \"black\", fontweight='bold', fontsize = 11)\nplt.subplot(2,2,3)\nstats.probplot(df[\"Humidity3pm\"], plot = plt)\n\nplt.show()","b2120523":"df[[\"Rainfall\",\"RainTomorrow\"]].groupby([\"RainTomorrow\"], as_index = False).mean().sort_values(by = \"RainTomorrow\").style.background_gradient(\"Reds\")","2c88c181":"plt.figure(figsize=(13,10))\nplt.subplot(2,2,1)\nplt.hist(df[\"Rainfall\"], color=\"purple\")\nplt.xlabel(\"Rainfall\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Rainfall histogram\", color = \"black\", fontweight='bold', fontsize = 11)\nplt.subplot(2,2,2)\nsns.distplot(df[\"Rainfall\"], fit=norm, color=\"purple\")\nplt.title(\"Rainfall Distplot\", color = \"black\", fontweight='bold', fontsize = 11)\nplt.subplot(2,2,3)\nstats.probplot(df[\"Rainfall\"], plot = plt)\n\nplt.show()","40899699":"df[[\"RainToday\",\"RainTomorrow\"]].groupby([\"RainTomorrow\"], as_index = False).mean().sort_values(by = \"RainTomorrow\").style.background_gradient(\"Reds\")","93316b5f":"plt.figure(figsize=(13,10))\nplt.subplot(2,2,1)\nplt.hist(df[\"RainToday\"], color=\"blue\")\nplt.xlabel(\"RainToday\")\nplt.ylabel(\"Frequency\")\nplt.title(\"RainToday histogram\", color = \"black\", fontweight='bold', fontsize = 11)\nplt.subplot(2,2,2)\nsns.distplot(df[\"RainToday\"], fit=norm, color=\"blue\")\nplt.title(\"RainToday Distplot\", color = \"black\", fontweight='bold', fontsize = 11)\n\nplt.show()","bb9dde18":"X = df[['Humidity3pm','Rainfall','RainToday']]\ny = df[['RainTomorrow']]","ba5b9373":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","62d098cf":"from imblearn.over_sampling import SMOTE\nimport collections","73223cca":"sm = SMOTE(random_state=14)\nX_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)","710884c2":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix, classification_report","fd0087ba":"from sklearn.linear_model import LogisticRegression","e5a77ddb":"clf_logreg = LogisticRegression(random_state=0)\nclf_logreg.fit(X_train,y_train)\n\ny_pred_1 = clf_logreg.predict(X_test)\nscore_1 = accuracy_score(y_test,y_pred_1)\n\nprint('Accuracy :',score_1)","ec33fc1c":"cm = confusion_matrix(y_test, y_pred_1)\nclasses = ['No rain', 'Raining']\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)\nhmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\nhmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nhmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","b7b5dad3":"from sklearn.tree import DecisionTreeClassifier","4f59c7e1":"clf_dt = DecisionTreeClassifier(random_state=0)\nclf_dt.fit(X_train,y_train)\n\ny_pred_2 = clf_dt.predict(X_test)\nscore_2 = accuracy_score(y_test,y_pred_2)\n\nprint('Accuracy :',score_2)","c6bcde61":"cm = confusion_matrix(y_test, y_pred_2)\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)\nhmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\nhmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nhmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","a2453fd2":"from sklearn.ensemble import RandomForestClassifier","060821ed":"clf_rf = RandomForestClassifier(n_estimators=100, max_depth=4,random_state=0)\nclf_rf.fit(X_train,y_train)\n\ny_pred_3 = clf_rf.predict(X_test)\nscore_3 = accuracy_score(y_test,y_pred_3)\n\nprint('Accuracy :',score_3)","1e141ba3":"cm = confusion_matrix(y_test, y_pred_3)\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)\nhmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\nhmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nhmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","676ba33e":"from sklearn.neighbors import KNeighborsClassifier","735acf7b":"knn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(X_train, y_train)\n\ny_pred_4 = knn.predict(X_test)\nscore_4 = accuracy_score(y_test,y_pred_4)\n\nprint('Accuracy :',score_4)","a4c39dc9":"cm = confusion_matrix(y_test, y_pred_4)\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)\nhmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\nhmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nhmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","8c092546":"X_train = torch.from_numpy(X_train.to_numpy()).float()\ny_train = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n\nX_test = torch.from_numpy(X_test.to_numpy()).float()\ny_test = torch.squeeze(torch.from_numpy(y_test.to_numpy()).float())","426ba7e5":"class Net(nn.Module):\n  def __init__(self, n_features): # NN Constructor\n    super(Net, self).__init__() #super constructor\n    self.fc1 = nn.Linear(n_features, 32) #Input to Hidden Layer\n    self.fc2 = nn.Linear(32, 16) #Input to Hidden Layer\n    self.fc3 = nn.Linear(16, 8) #Hidden to Hidden Layer\n    self.fc4 = nn.Linear(8, 1) #Hidden to Output Layer\n    \n  def forward(self, x): #passing outputs to another layers\n    x = F.relu(self.fc1(x)) # In to Hid Layer (relu)\n    x = F.relu(self.fc2(x)) # In to Hid Layer (relu)\n    x = F.relu(self.fc3(x)) # Hid to Hid Layer (relu)\n    return torch.sigmoid(self.fc4(x)) # Hid to Out Layer (sigmoid)\n\nnet = Net(X_train.shape[1]) ","8ab4565e":"#https:\/\/towardsdatascience.com\/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a\ncriterion = nn.BCELoss() #Binary Cross Entropy Loss","c967517e":"optimizer = optim.Adam(net.parameters(), lr=0.001) #Optimizer","af628bcb":"def calculate_accuracy(y_true, y_pred):\n  predicted = y_pred.ge(.5).view(-1)\n  return (y_true == predicted).sum().float() \/ len(y_true)","c6e8c207":"def round_tensor(t, decimal_places=3):\n  return round(t.item(), decimal_places)","151b23b2":"for epoch in range(1000):\n    y_pred = net(X_train)\n    y_pred = torch.squeeze(y_pred)\n    train_loss = criterion(y_pred, y_train)\n    if epoch % 50 == 0:\n      train_acc = calculate_accuracy(y_train, y_pred)\n      y_test_pred = net(X_test)\n      y_test_pred = torch.squeeze(y_test_pred)\n      test_loss = criterion(y_test_pred, y_test)\n      test_acc = calculate_accuracy(y_test, y_test_pred)\n      print(\nf'''epoch {epoch}\nTrain set - loss: {round_tensor(train_loss)}, accuracy: {round_tensor(train_acc)}\nTest  set - loss: {round_tensor(test_loss)}, accuracy: {round_tensor(test_acc)}\n''')\n    optimizer.zero_grad()\n    train_loss.backward()\n    optimizer.step()","13850d27":"y_pred_5 = net(X_test)\ny_pred_5 = y_pred_5.ge(.5).view(-1).cpu()\nscore_5 = accuracy_score(y_test,y_pred_5)\ny_test = y_test.cpu()\nprint(classification_report(y_test, y_pred_5, target_names=classes))","12cf2996":"cm = confusion_matrix(y_test, y_pred_5)\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)\nhmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\nhmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nhmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","85dedd48":"print(f'Scores for different models: \\nLinear Regresion {score_1} \\nDecision Tree {score_2} \\nRandom Forest {score_3} \\nKNeighborsClassifier {score_4} \\nNeural Network {score_5}')","98cdd853":"### 1.6. Analysing the top 3 columns","f2dccd1b":"### 2.5. Neural Network (built in pyTorch)\n","21b89b10":"# SnowyCocoon - Predicting the rain in Australia\n\n## It's my first task comission\/notebook. I'm still learning and very probably, I'll update this notebook later on :).\n\nWe will be comparing 5 different methods to predict the rain:\n1. Logistic Regresion\n3. Decision Trees\n2. Random Forest\n4. KNeighborsClassifier\n5. Neural Network (Built in pyTorch)\n\nDataset from:\n- https:\/\/www.kaggle.com\/jsphyg\/weather-dataset-rattle-package\n\n\nList of notebooks that helped me with the process:\n- https:\/\/www.kaggle.com\/aninditapani\/will-it-rain-tomorrow\n- https:\/\/www.kaggle.com\/prashant111\/logistic-regression-classifier-tutorial\n- https:\/\/www.kaggle.com\/rafetcan\/red-wine-quality-classification-95-76-acc\n\n","35206aef":"### 1.5. Selecting the features to include in our model","7173a985":"#### 1.6.2. Rainfall","4deefaa7":"### 2.3. Random Forest","eb0d72b2":"### 1.8. Upsampling (not used)\n\nUpsampling can really help us to get more equal f1 score (between 2 classes), but we dont want do use it here. We can recive better accuracy without up\/downsampling because there are more days without rain.","66ff623c":"### 2.4. KNeighborsClassifier","d9471394":"### 2.6. Results","98287509":"### 1.1. Handling the NA\/Null\/Empty values","565bbedf":"Probability Plots are looking already fine, because we've already deleted the outliers!","116877df":"#### 1.6.1. Humidity at 3 PM","394c09e2":"### 1.2. Dealing with outliers\nhttps:\/\/en.wikipedia.org\/wiki\/Outlier\n\nWe are calculating the Z-score of for every value in the dataframe. If the Z-score is going to be bigger than 3, then we are going to delete the whole row with one or more outliers. The highes the Z-score is, the more unusual the data is!\n\n![title](https:\/\/raw.githubusercontent.com\/SnowyCocoon\/Data-Science-Projects\/main\/11.%20Rain%20Classification%20using%205%20different%20classification%20models\/Img1.png)\n\nsource(https:\/\/en.wikipedia.org\/wiki\/Z-test)\n\n![title](https:\/\/github.com\/SnowyCocoon\/Data-Science-Projects\/blob\/main\/11.%20Rain%20Classification%20using%205%20different%20classification%20models\/Img2.png?raw=true)\n\nsource (https:\/\/www.dummies.com\/education\/math\/statistics\/how-to-calculate-a-confidence-interval-for-a-population-mean-when-you-know-its-standard-deviation\/)","68580cb1":"#### 1.6.3. Rain Today","a54bbce3":"# 2. Creating and Fitting the Models + Evaluation","48432910":"# 0. Importing the libriaries and data","5e391497":"### 1.4. Standarizing\/Normalizing our data","288186fc":"### 1.7. Spliting the data","ed5d7372":"### 1.3. Dealing with categorical data (in string format)","8b958743":"### 2.1. Linear Regresion","7fb98942":"### 2.2. Decision Tree","d1de77eb":"# 1. Preprocessing the data"}}