{"cell_type":{"a29f19fe":"code","28d4b6a1":"code","6d17507b":"code","e4d53f72":"code","2b357ac7":"code","34428a5b":"code","d47b3f54":"code","feab3b61":"code","53488e0f":"code","11024df8":"code","67a482dc":"code","56d3816c":"code","a87d95b8":"code","0d7aa28b":"code","c52c0d5c":"code","3df0f9ea":"code","308042fd":"code","60c60dc9":"code","ad3d41ae":"code","e6f8612e":"code","7331072a":"code","bad6e27c":"code","9001ffe3":"code","48b4e711":"code","12aa258f":"code","98355ee3":"code","3cc6c5a6":"code","3c9a2a24":"code","0e4755fb":"code","7503b080":"code","0f684149":"code","cc6892e5":"code","030faf51":"code","42f8c1d0":"code","ac53e594":"code","49aa6f1b":"code","987c219c":"code","70bdb2de":"code","a360da7b":"code","ff7f2eb0":"code","5b71b540":"code","9b4be126":"code","9997a31d":"code","fa5dae17":"code","2ca9a5ae":"code","d2dd59e1":"code","ceb1e261":"code","2d0506c1":"code","dbfe095d":"code","0084ae6c":"code","717a67b6":"code","5ec12633":"code","2dd551b4":"code","319ecc9f":"code","54384e97":"code","8cc3d4cd":"code","b457a265":"code","6c93b85e":"code","f8f97b99":"code","32147958":"code","d9601b73":"code","6748842a":"code","db8650b4":"code","3caadbc5":"code","307928e5":"code","99cb8b8a":"code","53e438bb":"code","69c2ad60":"code","d363e03a":"code","af200ea4":"code","71acdc44":"code","2f7f1a1a":"code","3e6a6f47":"code","06ea6cef":"code","3051f5c5":"code","11f7a920":"code","9372409b":"code","8b5aed36":"code","549890f8":"code","7fd178c2":"markdown","7a1bac9a":"markdown","14b3706b":"markdown","6acecbd7":"markdown","e38dc883":"markdown","75a214ee":"markdown","69245987":"markdown"},"source":{"a29f19fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","28d4b6a1":"import pandas as pd\n# Train data \ndf = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf.head()\n","6d17507b":"# Test Data\ndf1 = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf1.head()","e4d53f72":"df.columns","2b357ac7":"# importig the required Libraries for Data explorations #\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","34428a5b":"df.shape","d47b3f54":"df1.shape\n# SalesPrice needs to be predicted which is a missing column in test dataset","feab3b61":"df['LotFrontage'].isnull().sum()","53488e0f":"# Lets identify what uis the percentage of missing values in each of this columns #\n\n\nfeatures_missing = [i for i in df.columns if df[i].isnull().sum() >=1]\nfeatures_missing","11024df8":"features_missing1 = [i for i in df1.columns if df1[i].isnull().sum() >=1]\nfeatures_missing1","67a482dc":"for i in features_missing:\n    print(i,round(df[i].isnull().mean(),2),'% of missing values')","56d3816c":"for i in features_missing1:\n    print(i,round(df1[i].isnull().mean(),2),'% of missing values')","a87d95b8":"import matplotlib.pyplot as plt\nfor i in features_missing:\n    data = df.copy()\n    # Replace the missing values by 1, else make it zero\n    data[i] = np.where(data[i].isnull(),1,0)\n    data.groupby(i)['SalePrice'].median().plot.bar()\n    plt.title(i)\n    plt.show()\n    \n    # As pet the observation, we will replace thus missing values through feature engineering steps","0d7aa28b":"# Identify the list if numerical variables #\n\nfeatures_numerical = [i for i in df.columns if df[i].dtype!='O']\n\nfeatures_numerical","c52c0d5c":"# Same way we can identify the datatime featurws, we can help us to identify the no. of days  \/ no. of years between\n# the car was built and when it was sold #\n\nyear_feature =[i for i in df.columns if 'Yr' in i or 'Year' in i]\nyear_feature","3df0f9ea":"for i in year_feature:\n    print(df[i].head())","308042fd":"# Letsa analyze the relation between the year sold and sales price (time series analysis)\n\ndf.groupby('YrSold')['SalePrice'].median().plot()\nplt.title('Yrsold Vs. Selling Price')\nplt.show()","60c60dc9":"# Lets try to understand the relation between year sold with other 3 variables :\n# yearbuilt, 'YearRemodAdd', 'GarageYrBlt to understant teh relationship with the selling proce of house \n\nfor i in year_feature:\n    if i!='YrSold':\n        data=df.copy()\n    # this will give the no. of days #\n        print(\"for the feature\",i)\n        data[i] = data['YrSold'] - data[i]\n        plt.scatter(data[i],data['SalePrice'])\n        plt.xlabel(i)\n        plt.ylabel('SalePrice')\n        plt.show()\n        \n# From this graphs it can interpreted that as the no. of years passed by are less, the selling price\n# turns to be very high  and vice versa #","ad3d41ae":"df.info()","e6f8612e":"len(df['ScreenPorch'].unique())","7331072a":"#From the numerical variables, we cna have 2 types of variables : Discrete and continous\n\n# Discrete variables #\n\ndiscrete_features = [i for i in features_numerical if len(df[i].unique()) <25 and i not in year_feature]\n\nprint(df[discrete_features].head())","bad6e27c":"len(discrete_features)","9001ffe3":"# Lets plot relatoon between target variable Salesprice and discrete features #\n\nfor i in discrete_features:\n    data = df.copy()\n    data.groupby(i)['SalePrice'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('SalePrice')\n    plt.show()","48b4e711":"# Lets try ti understand the distributon if this continous variables #\n\ncontinous_features = [i for i in features_numerical if i not in discrete_features and i not in year_feature+['Id']]\nlen(continous_features)","12aa258f":"# Plotting the distribution if this variables #\n\nfor i in continous_features:\n    df[i].hist(bins=25)\n    plt.xlabel(i)\n    plt.ylabel('count')\n    plt.show()\n\n\n# only the few oif the variables have the gaussian distrivution, rest of them have skewed distribution # ","98355ee3":"# Before applying the logarthmic trasnformations, if we check the scatter plot #\n\n\nfor i in continous_features:\n    data = df.copy()\n    if 0 in df[i].unique():\n        pass\n    else:\n        plt.scatter(data['SalePrice'],data[i])\n        plt.xlabel(i)\n        plt.ylabel('SalePrice')\n        plt.title(i)\n        plt.show()","3cc6c5a6":"# As some of yhis continous variables are having skewed distributions, we will apply log transformations\n#to this variables #\n\nfor i in continous_features:\n    data = df.copy()\n    if 0 in df[i].unique():\n        pass\n    else:\n        data[i] = np.log(data[i])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.scatter(data['SalePrice'],data[i])\n        plt.xlabel(i)\n        plt.ylabel('SalePrice')\n        plt.show()","3c9a2a24":"# checking the outliers in the data #\n\nfor i in continous_features:\n    data = df.copy()\n    if 0 in data[i].unique():\n        pass\n    else:\n        data[i] =np.log(data[i])\n        data.boxplot(column=i)\n        plt.ylabel(i)\n        plt.show()","0e4755fb":"# Categorical variables #\n\ncategorical_features = [i for i in df.columns if df[i].dtypes=='O']\n\nlen(categorical_features)\n\n# Cardinality : How many categories are there in each of the variables #\n","7503b080":"# Cardinality of the variables #\n\nfor i in categorical_features:\n    print(i, 'has',df[i].nunique(),'unique categories')\n    \n# Some of the varuables has a large no. of cardinalities (largw no. of categories, which will create \n#a lot of data complexities while doung the one hot encoding)\n# Such variables needs to be handled separately instead of using one hot encoding #","0f684149":"for i in categorical_features:\n    data = df.copy()\n    data.groupby(i)['SalePrice'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('SalePrice')\n    plt.show()","cc6892e5":"# We will droop the ID column and the variables : Alley, fence, poolQc, fence and Miscfeature where the missjng values are greater than 50%\n# in both the train and test data\n# Train data\ndf_n = df.drop(['Id','Alley','PoolQC','Fence','MiscFeature'],axis=1)\ndf_n.shape","030faf51":"# test data\ndf1_n = df1.drop(['Id','Alley','PoolQC','Fence','MiscFeature'],axis=1)\ndf1_n.shape","42f8c1d0":"# Train Data # Imputing the categorical variables with mode values \n\nfor i in df_n.columns:\n    if df_n[i].dtype=='O':\n        df_n[i]= df_n[i].fillna(df_n[i].mode()[0])","ac53e594":"# test Data # Imouting the categorical variables with mode values \n\nfor i in df1_n.columns:\n    if df1_n[i].dtype=='O':\n        df1_n[i]= df1_n[i].fillna(df1_n[i].mode()[0])","49aa6f1b":"# Train data, for the numerical variables where we will replace missing \n# values with median instead of mean as from EDA, we came to know that there are outliers in the data #\n\nfor i in df_n.columns:\n    if df_n[i].dtype=='float64' or df_n[i].dtype=='int64':\n        df_n[i]= df_n[i].fillna(df_n[i].median())","987c219c":"# test data, for the numerical variables where we will replace missing \n# values with median instead of mean as from EDA, we came to know that there are outliers in the data #\n\nfor i in df1_n.columns:\n    if df1_n[i].dtype=='float64' or df1_n[i].dtype=='int64':\n        df1_n[i]= df1_n[i].fillna(df1_n[i].median())","70bdb2de":"print(df1_n.isnull().sum().sum())\nprint(df_n.isnull().sum().sum())","a360da7b":"print(df_n.duplicated().sum())\n\nprint(df1_n.duplicated().sum())","ff7f2eb0":"# Temporal variables #\n# Train data #\nfor i in year_feature:\n    if i!='YrSold':\n    # this will give the no. of days #\n        print(\"for the feature\",i)\n        df_n[i] = df_n['YrSold'] - df_n[i]\ndf_n.head()","5b71b540":"df_n[year_feature].head()","9b4be126":"# Test data #\nfor i in year_feature:\n    if i!='YrSold':\n    # this will give the no. of days #\n        df1_n[i] = df1_n['YrSold'] - df1_n[i]\ndf1_n.head()","9997a31d":"df1_n[year_feature].head()","fa5dae17":"# Feature Transformation : Converting all the categorical variables into numerical type #\n\n#From checking categories of categorical variables for train and test data it can be understood \n# that are some additional categories mentioned for test data categorical variables whicgh are not mentioned \n# in the train data \n\n# So we if do dummy encoding for both train and test data individually, then model will not ne able to understand\n# some of the categories if the test data, in such cases its better to combine this data and perform dummt encoding#","2ca9a5ae":"y_train = df_n['SalePrice']\nx_train = df_n.drop(['SalePrice'],axis=1)\nx_train.shape","d2dd59e1":"# Lets combine rhe data from train and test #\n\ndf_new = pd.concat([x_train,df1_n],axis=0)\ndf_new.tail()","ceb1e261":"df_new.shape","2d0506c1":"df_cat = pd.get_dummies(df_new,drop_first=True)\ndf_cat.head()","dbfe095d":"# Splitting the total daata set back into train and test back\n\n\nx_train = df_cat.iloc[:1460,:]\n\nx_train.shape","0084ae6c":"x_test =  df_cat.iloc[1460:,:]\n\nx_test.shape","717a67b6":"# Applying the min max scaler to the training data #\n# As we are using Linear regressor, we need to apply scaling in order to make the gradient descent converge faster to the global minima #\n\nfrom sklearn.preprocessing import MinMaxScaler\nmn = MinMaxScaler()\n\nnorm = mn.fit(x_train)\n\nx = pd.DataFrame(norm.transform(x_train),columns=x_train.columns)\nx.head()","5ec12633":"x_te = pd.DataFrame(norm.transform(x_test),columns = x_test.columns)\nx_te.head()\n\n# test dataset #","2dd551b4":"# Linear Regression #\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\n\nlin_reg =  LinearRegression()\nmse = cross_val_score(lin_reg,x,y_train,scoring='neg_mean_absolute_error',cv=5)\nmse_mean = mse.mean()\nmse_mean","319ecc9f":"# Ridge Regression #\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nridge = Ridge()\nparameters = {'alpha':[1e-15,1e-10,1e-7,1e-5,1e-2,1,5,10,20,40,60,80,100]}\nrd_regressor = GridSearchCV(ridge,parameters, scoring='neg_mean_squared_error',cv=5)\n\nrd_regressor.fit(x,y_train)","54384e97":"print(rd_regressor.best_score_)\nprint(rd_regressor.best_estimator_)","8cc3d4cd":"mo_rd_rg = Ridge(alpha=10)\nmo_rd_rg.fit(x,y_train)\n","b457a265":"# Lasso Regression #\n\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nlasso = Lasso()\nparameters = {'alpha':[1e-15,1e-10,1e-7,1e-5,1e-2,1,5,10,20,40,60,80,100]}\nla_regressor = GridSearchCV(lasso,parameters, scoring='neg_mean_squared_error',cv=5)\n\nla_regressor.fit(x,y_train)","6c93b85e":"print(la_regressor.best_score_)\nprint(la_regressor.best_estimator_)","f8f97b99":"import xgboost","32147958":"from xgboost import XGBRegressor\n\nreg = xgboost.XGBRegressor()\nreg.fit(x_train,y_train)\n","d9601b73":"# Predicting the test data set #\n\ny_pred = reg.predict(x_test)\ny_pred","6748842a":"# Converting this array into data frame #\n\ny_pr = pd.DataFrame(y_pred)\ny_pr.head()","db8650b4":"# Creating a sample submission file #\n\nsub_2 = pd.concat([df1['Id'],y_pr],axis=1)\nsub_2.head()","3caadbc5":"sub_2.columns = ['Id','SalePrice']\nsub_2.head()","307928e5":"sub_2.to_csv('sub_2.csv',index=False)\n\n#Score of 0.1405 ( Rank of 2336)","99cb8b8a":"# Hyoer parameters #\nn_estimators = [100,500,900,1000,1200,1500]\nmax_depth=[2,3,5,10,15]\nlearning_rate=[0.05,0.1,0.15,0.2,0.3]\nmin_child_weight=[1,2,3,4]\nbase_score = [0.25,0.5,0.75,1]\nbooster =['gbtree','gblinear']\n\nhyp_grid = {'n_estimators':n_estimators,'max_depth':max_depth,'learning_rate':learning_rate,'min_child_weight':min_child_weight,\n            'booster':booster,'base_score':base_score}","53e438bb":"from sklearn.model_selection import RandomizedSearchCV","69c2ad60":"regressor =  xgboost.XGBRegressor()","d363e03a":"# Random Search CV for this model #\n\nrandom_cv = RandomizedSearchCV(estimator=regressor,param_distributions=hyp_grid,cv=5,n_iter=50,scoring='neg_mean_absolute_error',n_jobs=4,verbose=5,return_train_score=True,random_state=1)\n","af200ea4":"random_cv.fit(x_train,y_train)","71acdc44":"random_cv.best_estimator_\n","2f7f1a1a":"model = xgboost.XGBRegressor(base_score=1, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=3,\n             min_child_weight=1, monotone_constraints='()',\n             n_estimators=900, n_jobs=0, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","3e6a6f47":"model.fit(x_train,y_train)","06ea6cef":"# saving this Xgboost Model #\nimport pickle\n\npickle.dump(model,open('model.pkl','wb'))","3051f5c5":"# Predicting the test data set #\n\ny_pred = model.predict(x_test)\ny_pred","11f7a920":"# Converting this array into data frame #\n\ny_pr = pd.DataFrame(y_pred)\ny_pr.head()","9372409b":"# Creating a sample submission file #\n\nsub_3 = pd.concat([df1['Id'],y_pr],axis=1)\nsub_3.head()","8b5aed36":"sub_3.columns = ['Id','SalePrice']\nsub_3.head()","549890f8":"sub_3.to_csv('sub_3.csv',index=False)\n# Score of 0.1322, rank of #1657","7fd178c2":"# Model Building using different approaches Linear Regression, Lasso and Ridge and XGB regressor #","7a1bac9a":"# Feature Engineering #","14b3706b":"# Hyper parameter tuning #","6acecbd7":"# Exploratory Data Analysis ","e38dc883":"# Xgboosting Model #","75a214ee":"# Data Cleaning # ","69245987":"# Steps for building solution for this problem statement\n\na) Problem Statement : To predict the price if the house through regression techniques\nb) Exploratory Data Analysis : Understanding the hidden relationship within the data\nc) Data Cleaning : Missing values, Outliers, duplicates\n\ni) Handling Missing Values : Delete the fields \/ columns which have greater than 50% of missing values and for rest of the columns impute them with mean, median and mode\nii) Check for duplicate values : Remove the duplicate enteries in the data\niii) Handling Outliers : If we are applying tree based algorithms we need not handle outliers separately\nd) Feature Engineeering : Transformation, scaling, extraction, etc. \nin order to make the data ready for the model building\ni) Temporal Values (We convert this year variables into difference between yr. sold and ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt'] to understand the duration when the house had been restructured\nii. Transforming the Categorical variables the. one hot encoding \/ dummy encoding\niii. Standardize the variables values (Min max, standard scaler) - Feature Scaling\nNot required if are using the tree based algorithms\ne) Model Building - Xgboost Regressor\nf) Hyper parameter tuning \ng) Model Validation"}}