{"cell_type":{"8174cfd4":"code","442b9193":"code","61e6993b":"code","78bba4dc":"code","41dde9a2":"code","eeff1c8f":"code","e46deae3":"code","66064f3f":"code","22930125":"code","7149a18f":"code","546a1efb":"code","ab67735a":"code","a8523443":"markdown","c7b49890":"markdown","a8680198":"markdown","a2ac9931":"markdown","38333ae4":"markdown","f80d0cdc":"markdown","6b18edd5":"markdown","ed08ff68":"markdown","8b30d91a":"markdown","32ac43b4":"markdown"},"source":{"8174cfd4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","442b9193":"def createModel(input_shape):\n    model = Sequential()\n    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n    model.add(Conv2D(32, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n \n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n \n    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))\n    model.add(Conv2D(64, (3, 3), activation='relu'))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(0.25))\n \n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(10, activation='softmax'))\n     \n    return model","61e6993b":"from tensorflow.keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint(x_train.shape)\nprint(x_train.shape[0])\nprint(x_test.shape[0])\n\nprint (y_test[0])\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\nprint (y_test[0])","78bba4dc":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nclass_names = ['airplane','automobile','bird','cat','deer',\n               'dog','frog','horse','ship','truck']\n\nfig, axes = plt.subplots(3, 3)\nfig.subplots_adjust(hspace=0.6, wspace=0.3)\n\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(x_train[i])\n\n    xlabel = \"class: {0}\".format(class_names[y_train[i].argmax()])\n\n    # Show the classes as the label on the x-axis.\n    ax.set_xlabel(xlabel)\n\n    # Remove ticks from the plot.\n    ax.set_xticks([])\n    ax.set_yticks([])","41dde9a2":"x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255","eeff1c8f":"opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n\nmodel = createModel(x_train.shape[1:])\n\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])","e46deae3":"model.summary()","66064f3f":"import time\n\nstart_time = time.time()\n\nmodel.fit(x_train, y_train,\n          batch_size=32,\n          epochs=10,\n          validation_data=(x_test, y_test),\n          shuffle=True)\n\nelapsed_time = time.time() - start_time\n\nprint(elapsed_time)","22930125":"scores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","7149a18f":"start_time = time.time()\n\nmodel.fit(x_train, y_train,\n          batch_size=32,\n          epochs=50,\n          validation_data=(x_test, y_test),\n          shuffle=True)\n\nelapsed_time = time.time() - start_time\n\nprint(elapsed_time)","546a1efb":"scores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])","ab67735a":"for x in range(100):\n    test_image = x_test[x,:]\n    predicted_cat = model.predict(np.array([test_image])).argmax()\n    label = y_test[x].argmax()\n    if (predicted_cat != label):\n        plt.title('Prediction: %s Label: %s' % (class_names[predicted_cat], class_names[label]))\n        plt.imshow(test_image)\n        plt.show()","a8523443":"Let's have Keras output the topology to make sure it matches what we want.","c7b49890":"Now let's actually train our neural network. In the interests of time, we'll just run 10 epochs because we haven't taken steps you'd normally take to make this run faster. But, let's see how long it takes to train this network over 10 epochs:","a8680198":"Article   from : https:\/\/sundog-education.com\/\n\n\nLet's do an image classification problem, with a data set of 32x32 color images that are categorized into one of ten classifications of what's in them. (32x32 is a very small image! So this isn't easy.)\n\nWe're just going to show that this can work for now, and expand upon this example later in the course to make it work better and much more efficiently.\n\nCNN's are very computationally intensive, and the only way you can run them in a reasonable amount of time is by installing the tensorflow-gpu package. This is a little bit tricky, so it's OK if you just want to look at this notebook without actually running it yourself. The specifics of getting tensorflow-gpu running depends on both the version of Tensorflow you installed, and your operating system. Please refer to https:\/\/www.tensorflow.org\/install\/ for instructions. You may have to create a developer account with NVidia in order to get some of the components you need. Note you need an NVidia GPU for tensorflow-gpu to work at all; if you don't have one, just watch.\n\nLet's start by defining the topology of our CNN. We'll start very simply - a single Conv2D layer, followed by a Dropout layer to prevent overfitting, which then gets flattened into a conventional deep neural network, with 512 neurons, a 50% dropout layer, and the final layer for our 10 possible classification results.","a2ac9931":"60%? Not that great. The accuracy was improving pretty quickly with every epoch though, so if we let it run longer we'd probably get much better results. It just takes an annoyingly long amount of time to do that, even when using tensorflow-gpu. But, we'll address that shortly!\n\nTo get an intuitive feel of just how good or bad our resulting model is, let's go through the first 100 test images and display the ones it got wrong. Are these errors understandable, or just a model that's embarassingly worse than a human?","38333ae4":"Yeah, it's pretty bad. But CNN's are actually very powerful - we just haven't tuned it with the right topology, we haven't made it efficient enough to run enough epochs, and there are some other tricks we can use as well. Don't give up on them! We're about to improve on this model substantially","f80d0cdc":"Just like we had to convert our labels into a format appropriate for a neural network, we also need to convert our feature data (the images themselves) into a suitable format. Neural network like normalized data as inputs, so we'll convert our image pixel data from 8-bit values that range from 0-255 into floating point values that range from 0-1:","6b18edd5":"Now we can create and compile our CNN model. We'll use the RMSprop optimizer, create our model, and compile it.","ed08ff68":"Now, let's measure the accuracy of our CNN against our test data:","8b30d91a":"We're going to train this neural network with the \"cifar10\" data set, which is built into Keras. It's 50,000 training images, and 10,000 test images - again, all of them 32x32 color images that represent one of ten different things.\n\nLet's load it up, and convert the labels into \"one-hot\" format which makes them suitable for use in a neural network. After printing the shape of the training data and the size of the training and testing data sets as a sanity check, we'll illustrate one-hot, or \"categorical\" encoding by displaying one raw label and its one-hot representation as a binary array.","32ac43b4":"It's always a good idea to get an intuitive feel of the data you're dealing with, so let's display nine sample images from our training data, along with their labels. You can see here what the different categories represent, too."}}