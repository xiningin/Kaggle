{"cell_type":{"1de55940":"code","7679d869":"code","8a0efc02":"code","8609abbe":"code","dc810fd4":"code","7253a8a5":"code","710bf78c":"code","ee712559":"code","c11ce7eb":"code","533567b4":"code","84f87271":"code","e439fa4c":"code","08e29dea":"code","b4304fc7":"code","52a28b9e":"code","567038b3":"code","2083f6b0":"code","6c0e766c":"code","b3255443":"code","47d0b328":"code","4b81d2c4":"code","dfdddff1":"code","5dba6a2c":"markdown","f07c15a1":"markdown","b74c163d":"markdown","90fceeb3":"markdown","27b73736":"markdown","17126123":"markdown","e136342b":"markdown","e8f19ff7":"markdown","88f8966b":"markdown","dda5b951":"markdown","6f67e89f":"markdown","ed0bae1c":"markdown","3ac23cae":"markdown","1b228eae":"markdown","da819efe":"markdown"},"source":{"1de55940":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7679d869":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom keras.utils import to_categorical\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nimport datetime\ntf.config.run_functions_eagerly(True)","8a0efc02":"df_test = pd.read_csv (r'..\/input\/fashionmnist\/fashion-mnist_test.csv')\ndf_train = pd.read_csv (r'..\/input\/fashionmnist\/fashion-mnist_train.csv') ","8609abbe":"a = df_train\ntrain_f = a.drop('label',1)\nb=df_test\ntest_features=b.drop('label',1)\ntrain_l=a['label']\ntest_labels=b['label']\ntrain_features, val_features, train_labels, val_labels = train_test_split(train_f, train_l, test_size=0.2, random_state=1)\nprint(\"shape of training features : \"+ str(train_features.shape))\nprint(\"shape of training labels\" + str(train_labels.shape))\n\nprint(\"shape of valuation features \" + str(val_features.shape))\nprint(\"shape of valuation labels\" + str(val_labels.shape))\n\nprint(\"shape of testing features\" + str(test_features.shape))\nprint(\"shape of testing labels\" + str(test_labels.shape))","dc810fd4":"train_features = train_features.to_numpy()\ntrain_labels = train_labels.to_numpy()\nval_features = val_features.to_numpy()\nval_labels = val_labels.to_numpy()\ntest_features = test_features.to_numpy()\ntest_labels = test_labels.to_numpy()\n    ","7253a8a5":"for i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train_features[i].reshape(28,28)) #imshow  takes an array ( with dimension = 2, RGB or B\/W) and gives you the image that corresponds to it\nplt.show()   ","710bf78c":"plt.figure()\nx1 = train_labels\nplt.hist(x1, range = (0, 9), bins = 10, color = 'blue',\n            edgecolor = 'black')\nplt.xlabel('classes')\nplt.ylabel('number')\nplt.title('histogram for training labels')\n\nplt.figure()\nx2 = test_labels\nplt.hist(x2, range = (0, 9), bins = 10, color = 'red',\n            edgecolor = 'black')\nplt.xlabel('classes')\nplt.ylabel('number')\nplt.title('histogram for testing labels')","ee712559":"X=StandardScaler().fit_transform(train_features)\nX_test=StandardScaler().fit_transform(test_features)","c11ce7eb":"model2=svm.SVC(kernel='poly',degree=2)\nmodel2.fit(X,train_labels)","533567b4":"\nmodel2.predict([X_test[0]])\ny1 = test_labels\ny2 = model2.predict(X_test)\n","84f87271":"print('the accuracy of SVM is : ' + str(accuracy_score(y1, y2, normalize=True)))","e439fa4c":"model1=KNeighborsClassifier(n_neighbors=3) # we choose n=3\nmodel1.fit(X,train_labels)\ny1 = test_labels\ny2_K = model1.predict(X_test)\n","08e29dea":"print(\"the accuracy of KNN is : \" + str(accuracy_score(y1, y2_K, normalize=True)))","b4304fc7":"model4=tf.keras.Sequential()\nmodel4.add(tf.keras.layers.Dense(150))\nmodel4.add(tf.keras.layers.Dense(10))\n\nmodel4.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\nlog_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\nhistory1 = model4.fit(X,train_labels,\n               epochs=20,\n               callbacks=[tensorboard_callback],\n               validation_data=(X_test,test_labels))\n\n","52a28b9e":"score2 = model4.evaluate(X_test, test_labels, verbose=0)","567038b3":"print('the accuracy of NN is : ' + str(score2[1]))","2083f6b0":"accuracy1 = history1.history['accuracy']\nval_accuracy1 = history1.history['val_accuracy']\nloss1 = history1.history['loss']\nval_loss1 = history1.history['val_loss']\nepochs1 = range(len(accuracy1))\nplt.plot(epochs1, accuracy1, 'bo', label='Training accuracy')\nplt.plot(epochs1, val_accuracy1, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs1, loss1, 'bo', label='Training loss')\nplt.plot(epochs1, val_loss1, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","6c0e766c":"input_shape = (28, 28, 1) \n\n\nmodel5 = tf.keras.models.Sequential()\n#the first convolution layer\nmodel5.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 kernel_initializer='he_normal',\n                 input_shape=input_shape))\nmodel5.add(tf.keras.layers.MaxPooling2D((2, 2)))\n\n\n#the second convolution layer\nmodel5.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\nmodel5.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n\n\n#the third convolution layer\nmodel5.add(tf.keras.layers.Conv2D(128, (3, 3), activation='relu'))\n\n\n#classificator, a fully connected neural network that takes the outcome of the convolution as an input\nmodel5.add(tf.keras.layers.Flatten())\nmodel5.add(tf.keras.layers.Dense(128, activation='relu'))\nmodel5.add(tf.keras.layers.Dropout(0.3))\nmodel5.add(tf.keras.layers.Dense(10, activation='softmax'))\n\nmodel5.compile(loss=tf.keras.losses.categorical_crossentropy,\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])\n\nmodel5.summary()","b3255443":"#reshaping the data for the convolution\ntrain_featuresQ=train_features.reshape(train_features.shape[0],28,28,1)\ntest_featuresQ=test_features.reshape(test_features.shape[0],28,28,1)\nval_featuresQ=val_features.reshape(val_features.shape[0],28,28,1)\n\ntrain_featuresQ= train_featuresQ.astype('float32')\ntest_featuresQ = test_featuresQ.astype('float32')\nval_featuresQ = val_featuresQ.astype('float32')\ntrain_featuresQ \/= 255\ntest_featuresQ \/= 255\nval_featuresQ \/= 255\n\ntrain_labelsQ=to_categorical(train_labels)\ntest_labelsQ=to_categorical(test_labels)\nval_labelsQ=to_categorical(val_labels)\n\nhistory = model5.fit(train_featuresQ,train_labelsQ,\n                    batch_size=512,\n                    epochs=15,\n                    verbose=1,\n                    validation_data=(val_featuresQ, val_labelsQ))\n\n\n","47d0b328":"score = model5.evaluate(test_featuresQ, test_labelsQ, verbose=0)","4b81d2c4":"print('the accuracy of CNN is : ' + str(score[1]))","dfdddff1":"%matplotlib inline\naccuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","5dba6a2c":"> the accuracy of NN is : 0.8461999893188477","f07c15a1":"Transforming the Dataframes into arrays","b74c163d":"**Let's see what are the features and the labels**","90fceeb3":"**Scaling the data**","27b73736":"> the accuracy of SVM is : 0.8901","17126123":"We evaluate the accuracy of our model","e136342b":"# **Loading, visualising the dataset, and preparing the data**","e8f19ff7":"# **Algorithm number 1 : Support Vector Machine**","88f8966b":"# **Algorithm number 3 : Fully connected neural network**","dda5b951":"# **Algorithm number 2 : K-nearest neighbours**\n","6f67e89f":"Let's visualise some of the images that we are trying to classify","ed0bae1c":"> the accuracy of CNN is : 0.9185000061988831","3ac23cae":"> the accuracy of KNN is : 0.8532","1b228eae":"# **Algorithm number 4 : Convolution Neural network**","da819efe":"We scale the training data and feed it to the SVC algorithm"}}