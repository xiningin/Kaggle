{"cell_type":{"e2acd698":"code","ab82ac67":"code","18d453d5":"code","4ffc6795":"code","08de89bd":"code","11d82c42":"code","6f3620c1":"code","65202779":"code","b533809c":"code","94525dce":"code","e77d55fb":"code","6f94c8b6":"code","44f046a6":"code","e7cd8d1b":"code","f24b4b12":"code","d70c7835":"code","1557dbef":"code","6fc891a0":"code","cb159bf7":"code","b0b0af8c":"code","496232e9":"code","ece49610":"code","17bd98cb":"code","32255706":"code","462d555d":"code","c65dfab3":"code","e8acb990":"code","19fe2cd5":"code","6b95e614":"code","c1877909":"code","bfddff5b":"code","389942ec":"code","b0ae76ed":"code","7b9a0cb4":"code","079ded16":"code","b6d1de75":"code","7f2ca6b2":"code","1f24da3c":"code","b98d7e5d":"code","409196b5":"code","2c8f63e0":"code","344c5bf6":"code","09bdf2e9":"code","c5827a6e":"code","015e83d6":"code","54de256b":"code","fcd136bc":"code","f16a6747":"code","aac83482":"code","600120cb":"code","3cff6f9b":"code","c79c60c9":"code","0927af40":"code","da0dc874":"code","bc9f295e":"code","0766f8aa":"code","37d53864":"code","60a76b11":"code","3256e0e6":"code","b685b46c":"code","fedb9a40":"code","e456214d":"code","221cb324":"code","6ebab4ce":"code","e08ce2b0":"code","d2948cee":"code","d286ac0c":"code","6afd2fd4":"code","bcbc8747":"code","68f26b7a":"code","07876f4d":"code","7f59cd3d":"code","d3f248dc":"code","ec3e2d5d":"code","0b76c395":"code","2ff2ef2c":"code","c0a29429":"code","ec5d8fe3":"code","f81dd624":"code","bb7548e7":"code","12478ed5":"code","f4004bae":"code","8ad31ee9":"code","b9818161":"code","935a6a3d":"code","5f857e1f":"code","95744004":"code","0b807d91":"code","33cd8c15":"markdown","4fadddb5":"markdown","24ee54d1":"markdown","c7ddd0e3":"markdown","24c9e1b9":"markdown","230d17ce":"markdown","b39cf725":"markdown","970648b6":"markdown","62b8c7a4":"markdown","91014a26":"markdown","aedb39c0":"markdown","5de89869":"markdown","e513dcef":"markdown","86f47182":"markdown","3e4d4606":"markdown","7483b456":"markdown","59bfacc8":"markdown","44bcbdcd":"markdown","7b48fa52":"markdown","0d305c43":"markdown","db5439f7":"markdown","b2d9ae41":"markdown","124b5717":"markdown","11a3a905":"markdown","588b4214":"markdown","51a48a74":"markdown","ac52b601":"markdown","ac85744b":"markdown","b6623a72":"markdown","b02430b1":"markdown","bbe19d2b":"markdown","f2a885af":"markdown","497ff2ff":"markdown","5f92e668":"markdown","f769aadb":"markdown","50d6b135":"markdown","45301e00":"markdown"},"source":{"e2acd698":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#Plot imports\nimport seaborn as sns\npd.set_option('display.max_columns', None)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# ML modeling imports\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error as mse\n","ab82ac67":"\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","18d453d5":"\nTRAIN = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\nTEST = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","4ffc6795":"TRAIN.head()","08de89bd":"# MSSubClass, MoSold and YrSold are actually a categorical variables, so converting them to categorical variables (object type) and dropping ID\n\nTRAIN['MSSubClass'] = TRAIN['MSSubClass'].astype('object')\nTRAIN['MoSold'] = TRAIN['MoSold'].astype('object')\nTRAIN['YrSold'] = TRAIN['YrSold'].astype('object')\n\n\nTEST['MSSubClass'] = TEST['MSSubClass'].astype('object')\nTEST['MoSold'] = TEST['MoSold'].astype('object')\nTEST['YrSold'] = TEST['YrSold'].astype('object')\n","11d82c42":"# Separate categorical from numerical(continuous) variables and store them in separate lists\n\ncat_cols_TRAIN = []\ncont_cols_TRAIN = []\n\nfor i in TRAIN.columns:\n    if TRAIN[i].dtypes == 'object':\n        cat_cols_TRAIN.append(i)\n    else:\n        cont_cols_TRAIN.append(i)","6f3620c1":"cat_cols_TEST = []\ncont_cols_TEST = []\n\nfor i in TEST.columns:\n    if TEST[i].dtypes == 'object':\n        cat_cols_TEST.append(i)\n    else:\n        cont_cols_TEST.append(i)","65202779":"sns.boxplot(TRAIN['SalePrice'])\nplt.show()","b533809c":"from scipy.stats import norm\n(avge, std_dev) = norm.fit(TRAIN['SalePrice'])\nplt.figure(figsize = (20,10))\nsns.distplot(a=TRAIN['SalePrice'],hist=True,kde=True,fit=norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel('Sale Price in US$')\nplt.legend(['Sale Price ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(avge, std_dev)],\n            loc='best')\nplt.show()","94525dce":"TRAIN['SalePrice'] = np.log(TRAIN['SalePrice'])\nTRAIN['SalePrice'].head()","e77d55fb":"(avge, std_dev) = norm.fit(TRAIN['SalePrice'])\nplt.figure(figsize = (20,10))\nsns.distplot(a=TRAIN['SalePrice'],hist=True,kde=True,fit=norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel('Sale Price in US$')\nplt.legend(['Sale Price ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(avge, std_dev)],\n            loc='best')\nplt.show()","6f94c8b6":"sns.boxplot(TRAIN['SalePrice'], orient='v')\nplt.show()","44f046a6":"def outliers(variable):\n  sorted(TRAIN[variable])\n  Q1,Q3 = np.percentile(TRAIN[variable],[25,75])\n  IQR = Q3-Q1\n  lr = Q1 - (1.5*IQR)\n  ur = Q3 + (1.5*IQR)\n  return ur,lr\n\nur,lr = outliers('SalePrice')\n\nTRAIN = TRAIN.drop(TRAIN[(TRAIN['SalePrice']<lr ) | (TRAIN['SalePrice']>ur)].index)","e7cd8d1b":"TRAIN.shape","f24b4b12":"sns.boxplot(TRAIN['SalePrice'], orient='v')\nplt.show()","d70c7835":"(avge, std_dev) = norm.fit(TRAIN['SalePrice'])\nplt.figure(figsize = (20,10))\nsns.distplot(a=TRAIN['SalePrice'],hist=True,kde=True,fit=norm)\nplt.title('SalePrice distribution vs Normal Distribution', fontsize = 13)\nplt.xlabel('Sale Price in US$')\nplt.legend(['Sale Price ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(avge, std_dev)],\n            loc='best')\nplt.show()","1557dbef":"TRAIN[cont_cols_TRAIN].hist(figsize=(20,20))\nplt.show()","6fc891a0":"list1=['LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','LowQualFinSF','BsmtHalfBath','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal']\ndesc1 = TRAIN[list1].describe().transpose()\ndesc1['coeff_of_var'] = desc1['std']\/desc1['mean']","cb159bf7":"desc1","b0b0af8c":"desc1[desc1['coeff_of_var']>3].T.columns","496232e9":"dropped_columns = ['BsmtFinSF2', 'LowQualFinSF', 'BsmtHalfBath', '3SsnPorch','ScreenPorch', 'PoolArea', 'MiscVal']\n\nTRAIN.drop(dropped_columns,axis=1, inplace=True)\nTEST.drop(dropped_columns,axis=1, inplace=True)","ece49610":"cat_cols_TEST","17bd98cb":"cat_cols_TRAIN","32255706":"# Correlation Matrix\n\nf, ax = plt.subplots(figsize=(30, 25))\ncorr_matrix = TRAIN.corr('pearson')\nmask = np.triu(np.ones_like(corr_matrix, dtype=bool))\ncmap = sns.diverging_palette(300, 50, as_cmap=True)\nsns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","462d555d":"cat_cols_TRAIN = []\ncont_cols_TRAIN = []\n\nfor i in TRAIN.columns:\n    if TRAIN[i].dtypes == 'object':\n        cat_cols_TRAIN.append(i)\n    else:\n        cont_cols_TRAIN.append(i)","c65dfab3":"cat_cols_TEST = []\ncont_cols_TEST = []\n\nfor i in TEST.columns:\n    if TEST[i].dtypes == 'object':\n        cat_cols_TEST.append(i)\n    else:\n        cont_cols_TEST.append(i)","e8acb990":"def get_missing_stats(df, col_list, threshold=0):\n    total = len(df)\n    for col in col_list:\n        null = df[col].isnull().sum()\n        if null > 0 and null\/total >= threshold:\n            print(col)\n            if df[col].dtypes == 'object':\n                print(df[col].value_counts())\n            print(f'Missing values: {null} of {total}')\n            print(f'Percent missing values: {round((null*100)\/total, 2)}%\\n')","19fe2cd5":"get_missing_stats(TRAIN, cont_cols_TRAIN)","6b95e614":"get_missing_stats(TEST, cont_cols_TEST)","c1877909":"imputer = SimpleImputer(strategy='median')\nTRAIN[cont_cols_TRAIN[:-1]] = imputer.fit_transform(TRAIN[cont_cols_TRAIN[:-1]])\nTEST[cont_cols_TEST] = imputer.transform(TEST[cont_cols_TEST])","bfddff5b":"get_missing_stats(TRAIN, cont_cols_TRAIN)","389942ec":"get_missing_stats(TEST, cont_cols_TEST)","b0ae76ed":"get_missing_stats(TRAIN, cat_cols_TRAIN)","7b9a0cb4":"get_missing_stats(TEST, cat_cols_TEST)","079ded16":"get_missing_stats(TRAIN, cat_cols_TRAIN, 0.4)","b6d1de75":"get_missing_stats(TEST, cat_cols_TEST, 0.4)","7f2ca6b2":"TRAIN.drop(['PoolQC'], axis=1, inplace=True)\nTEST.drop(['PoolQC'], axis=1, inplace=True)","1f24da3c":"na_cols = ['Alley','FireplaceQu','Fence','MiscFeature']\n\nna_imputer = SimpleImputer(strategy='constant', fill_value='N.A')\nTRAIN[na_cols] = na_imputer.fit_transform(TRAIN[na_cols])\nTEST[na_cols] = na_imputer.transform(TEST[na_cols])","b98d7e5d":"cat_cols_TRAIN = []\ncont_cols_TRAIN = []\n\nfor i in TRAIN.columns:\n    if TRAIN[i].dtypes == 'object':\n        cat_cols_TRAIN.append(i)\n    else:\n        cont_cols_TRAIN.append(i)","409196b5":"cat_cols_TEST = []\ncont_cols_TEST = []\n\nfor i in TEST.columns:\n    if TEST[i].dtypes == 'object':\n        cat_cols_TEST.append(i)\n    else:\n        cont_cols_TEST.append(i)","2c8f63e0":"mf_imputer = SimpleImputer(strategy='most_frequent')\nTRAIN[cat_cols_TRAIN] = mf_imputer.fit_transform(TRAIN[cat_cols_TRAIN])\nTEST[cat_cols_TEST] = mf_imputer.transform(TEST[cat_cols_TEST])","344c5bf6":"get_missing_stats(TRAIN, cat_cols_TRAIN)","09bdf2e9":"get_missing_stats(TEST, cat_cols_TEST)","c5827a6e":"plt.figure(figsize=(20, 20))\nsns.heatmap(TRAIN.drop(['SalePrice'], axis=1).corr(), annot=True)\nplt.show()","015e83d6":"cor = TRAIN.drop(['SalePrice'], axis=1).corr()\nfor i, col in enumerate(cor.columns):\n    for row in cor.index[i+1:]:\n        if col != row and cor[col][row] > 0.7:\n            print(f'({row}, {col}): {cor[col][row]}')","54de256b":"sc = StandardScaler()\nTRAIN[cont_cols_TRAIN[1:-1]] = sc.fit_transform(TRAIN[cont_cols_TRAIN[1:-1]])\nTEST[cont_cols_TEST[1:]] = sc.transform(TEST[cont_cols_TEST[1:]])","fcd136bc":"cat_cols_TRAIN = []\ncont_cols_TRAIN = []\n\nfor i in TRAIN.columns:\n    if TRAIN[i].dtypes == 'object':\n        cat_cols_TRAIN.append(i)\n    else:\n        cont_cols_TRAIN.append(i)","f16a6747":"cat_cols_TEST = []\ncont_cols_TEST = []\n\nfor i in TEST.columns:\n    if TEST[i].dtypes == 'object':\n        cat_cols_TEST.append(i)\n    else:\n        cont_cols_TEST.append(i)","aac83482":"TRAIN = pd.get_dummies(TRAIN, drop_first=True, columns=cat_cols_TRAIN)\nTEST = pd.get_dummies(TEST, drop_first=True, columns=cat_cols_TEST)","600120cb":"TRAIN.head()","3cff6f9b":"TEST.head()","c79c60c9":"TRAIN.columns","0927af40":"TEST.columns","da0dc874":"TRAIN_col_list = TRAIN.columns.sort_values()\nTEST_col_list = TEST.columns.sort_values()","bc9f295e":"## making the list of columns which are present in both the test and the train dataset\ncompatible_list = set(TRAIN_col_list).intersection(TEST_col_list)","0766f8aa":"TRAIN_2 = TRAIN[compatible_list]","37d53864":"TRAIN_2.head()","60a76b11":"TRAIN_2.shape","3256e0e6":"TEST_2 = TEST[compatible_list]","b685b46c":"TEST_2.head()","fedb9a40":"TEST_2.shape","e456214d":"import statsmodels.api as sm","221cb324":"# copying all predictor variables into X and Target variable in Y\nX = TRAIN_2\nY = TRAIN['SalePrice']","6ebab4ce":"X.head()","e08ce2b0":"Y.head()","d2948cee":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.2, random_state=42)","d286ac0c":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(x_train, y_train)","6afd2fd4":"# NB: our model expects inputs as numpy arrays. Also, the prediction y_pred is a numpy array. \n# To display y-pred against y_test (which is still a dataframe), we need to reshape it by accessing the values, \n# thus add .values to the reshape method.\n\ny_pred = regressor.predict(x_test)\nnp.set_printoptions(precision=2)  #to keep decimal places to 2\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.values.reshape(len(y_test),1)),1))\n\n","bcbc8747":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","68f26b7a":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\npoly_reg = PolynomialFeatures(degree = 2)\n# x_poly = poly_reg.fit_transform(x_train)\n# regressor2 = LinearRegression()\n# regressor2.fit(x_poly, y_train)\n","07876f4d":"x_poly = poly_reg.fit_transform(x_train)","7f59cd3d":"regressor2 = LinearRegression()\nregressor2.fit(x_poly, y_train)","d3f248dc":"y_pred2 = regressor2.predict(poly_reg.transform(x_test))\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred2.reshape(len(y_pred2),1), y_test.values.reshape(len(y_test),1)),1))","ec3e2d5d":"# Evaluating the polynomial regression Model Performance\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred2)","0b76c395":"from sklearn.tree import DecisionTreeRegressor\nregressor3 = DecisionTreeRegressor(random_state = 0)\nregressor3.fit(x_train, y_train)","2ff2ef2c":"#predicting the test set results with decision tree model\n\ny_pred3 = regressor3.predict(x_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred3.reshape(len(y_pred3),1), y_test.values.reshape(len(y_test),1)),1))","c0a29429":"# Evaluating the decision tree regression Model Performance\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred3)","ec5d8fe3":"from sklearn.ensemble import RandomForestRegressor\nregressor4 = RandomForestRegressor(n_estimators = 10, random_state = 0)\nregressor4.fit(x_train, y_train)","f81dd624":"#predicting the test set results with the random forest model\n\ny_pred4 = regressor4.predict(x_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred4.reshape(len(y_pred4),1), y_test.values.reshape(len(y_test),1)),1))","bb7548e7":"# Evaluating the random forest regression Model Performance\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred4)","12478ed5":"from sklearn.svm import SVR\nregressor5 = SVR(kernel = 'rbf')\nregressor5.fit(x_train, y_train)","f4004bae":"#predicting the test set results with the SVR model\n\ny_pred5 = regressor5.predict(x_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate((y_pred5.reshape(len(y_pred5),1), y_test.values.reshape(len(y_test),1)),1))","8ad31ee9":"# Evaluating the SVR Model Performance\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_pred5)","b9818161":"Y_PRED = regressor.predict(TEST_2)","935a6a3d":"Y_PRED","5f857e1f":"# To reverse the standard scalar from the predicted sales prices\n\nY_PRED = np.expm1(Y_PRED)","95744004":"\nsubmission = pd.DataFrame({'Id': TEST['Id'], 'SalePrice' : Y_PRED})\nprint(submission)","0b807d91":"submission.to_csv(\"submission.csv\", index = False, header = True)","33cd8c15":"Training the Polynomial Regression model on the Training set","4fadddb5":"Removing the outliers","24ee54d1":"Training the Support Vector Regression (SVR) model on the Training set","c7ddd0e3":"Here we can see that there are several variables where the coefficient of Variation (std\/mean) is extremely high accompanied by very few non zero values. These data in these variables have very high variability.\n\nWhile we will typically include data with high variability, in this case we will ignore those variables where the upper quantile is also 0 and the coeficient of variation is above 3. Its very much like having the missing values","24c9e1b9":"Training the Decision Tree Regression model on the Training set","230d17ce":"## Splitting the TRAIN dataset into the training set and test set","b39cf725":"## Scale variables: Feature scaling on the numeric values","970648b6":"## Data Preprocessing","62b8c7a4":"**Check train and test for compatability: if both train and test data have thesame columns**","91014a26":"### One-hot encoding of categorical columns","aedb39c0":"Checking the correlation between various variables","5de89869":"## Using the multiple linear regression model to predict the TEST set (TEST_2) results","e513dcef":"Drawing histograms to understand the data distribution in the continuous variables","86f47182":"Since the multiple linear regression model produced the best r squared score (0.89), it is fair to say this is the best model to use for predicting the house prices","3e4d4606":"Training the Random Forest Regression model on the Training set","7483b456":"All missing values have now been taken care of.","59bfacc8":"Some columns seet to have quite a number of missing values. Let's analyse the columns with missing frequency greater than 40%","44bcbdcd":"**Let's check and impute missing values in continuous variables**","7b48fa52":"In the above we can notice the following:\n\nfor most continuous variables, the scales vary widely, hence we will need to standardise the data.\nVariables such as YearBuilt, GarageYrBlt are left skewed, but still more and more houses are build in the recent years and more and more garages are built in the later years. Hence we will not check or treat any ourliers for these variables.\nSimilarly, variables like EnclosedPorch, OpenPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal have overwhelming number of values close to 0. This means they actually may be significant for determining the Sale Price.\nHowever before dropping these variables we will look at their value counts as well as correlation martix.\nWe have the following variables for which the values are heavily right skewed.\n\n'LoTArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtHalfBath', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal'\n\nLets check the descriptive values for these variables","0d305c43":"### **Checking for multicolinearity among the exploratory variables**","db5439f7":"Check for ouliers in the target variable","b2d9ae41":"Now the test and train dataframes are perfectly compatible. We will now create a feature set and the outcome variable on the train dataset","124b5717":"Training the Multiple Linear Regression model on the Training set","11a3a905":"## **Exploratory Data Analysis**","588b4214":"As we can see, the SalesPrice does not have a normal distribution. It is skewed to the left. Let's convert this to a natural log and then see the distribution.","51a48a74":"Now here we can see that there are a lot of variables in the train which are not there in the test and vice versa could also be possible.","ac52b601":"# Modelling","ac85744b":"## Creating A feature set (X) and Outcome Variable (Y)\n","b6623a72":"From the plots above, we can see that few varibles are highly correlated, and it makes sense for them to be correlated. Hence, for now we might want to keep them.","b02430b1":"It is now certain that all the houses surveyed did not have all the 80 features. It may be because the houses did not have these features or the houses had these features but were not reported.  \nSo, for now we can impute the missing values (with freq. > 40%) with N.A, where as missing values of columns with freq < 40% with most frequent value.\nSince 99% of PoolQC data is missing, we drop it\n","bbe19d2b":"### checking the target variable","f2a885af":"Let's check and impute missing values in categorical columns","497ff2ff":"#### **importing the test and train datasets**","5f92e668":"Evaluating the multiple regression Model Performance","f769aadb":"Since we do not have an extreme case of missing values, we can simply replace them by the median (as the data seems to be highly variable and median is a robust metric) of training set in both the training and test set.","50d6b135":"So, the missing values for numerical columns have disappeared from the numerical columns. Now, lets hit the categorical columns.","45301e00":"Predicting the Test set results"}}