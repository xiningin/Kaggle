{"cell_type":{"e0c1005c":"code","9c59db45":"code","4569313f":"code","7533aaae":"code","4a046cd0":"code","5d788807":"code","353157e5":"code","daf373e6":"code","005cfcb7":"code","d4f31356":"code","f2b0aca9":"code","b3eae92d":"code","f50e6c7a":"code","2985cdcb":"code","46cbd9da":"code","89248507":"code","40f0d9bf":"code","6af1c96c":"code","dbe4d114":"code","d2451ab0":"code","ac370d76":"code","ce159e54":"code","130b4167":"code","b20ddd30":"code","eded395b":"code","9e23cb8b":"code","b620bfba":"code","75eb8d61":"code","c2f41bfe":"code","49f17279":"code","b3a352e4":"code","358708e2":"code","fb3916fb":"code","100bd426":"code","6ca1b3ae":"code","122d3cff":"code","fdbf4255":"code","277bb306":"code","7ae32f5d":"code","7b8b9c61":"code","01080ff6":"code","dd660abb":"code","b8d0d679":"code","746af385":"code","a7f4f1f9":"code","e9905c27":"code","e4aaed66":"code","e377334a":"code","b3caee68":"code","3c799a04":"code","87c187d5":"code","57809d8a":"code","33df8c16":"code","8a140ae6":"code","cbf4d8c5":"code","559917f9":"code","56abc576":"code","0a14f3dd":"code","53e3ab5a":"code","636e3333":"code","5729d9bb":"code","53c5d0db":"code","eab1c112":"code","e4856227":"code","fe311ea0":"code","9e0a3a75":"code","9afeda93":"code","4f41b487":"code","a50c0f9b":"code","09432006":"code","afc210e9":"code","dbd8ef33":"code","a43e48fa":"code","9afd353b":"code","7b239722":"code","0b34c62c":"code","f7407f83":"code","e10f8899":"code","4d366411":"code","25198a9e":"code","db55dac7":"code","c2777f3a":"code","a152052b":"code","c4233dda":"code","afa84fa1":"code","0ff590a4":"code","b4d9ffbd":"code","5e92da3f":"code","2c99daa6":"code","03eb4fec":"code","7cf0c4fe":"code","08c5cd73":"code","1c998f97":"code","2c3606e4":"code","a29c921f":"code","adca0529":"code","1c9923c0":"code","b8d2ba2b":"code","9d1d6152":"code","ff06aa79":"code","9d6f2aa4":"code","35ce1972":"code","ec84abad":"code","2237e6f4":"code","e65ab332":"code","65970633":"code","495fb474":"code","e4c4bfc3":"code","46885a67":"code","8419dbb8":"code","9b83115c":"code","b8927691":"code","dfd1c6bf":"code","00e02e6e":"code","b8ebce72":"code","d63a29fc":"code","6d399b84":"code","f1f4c412":"code","bae74648":"code","0e6a5f85":"code","4117ec10":"code","2e1a9fa6":"code","abcbbb14":"code","22c370f7":"code","c7fd439c":"code","82f296a0":"code","bacfd012":"code","4c89ecb9":"code","0ee8cebf":"code","57c33200":"code","8dafa577":"code","35978d1d":"code","7193ac42":"code","cfddac4f":"code","e7d3323b":"code","a0c60f1c":"code","93db15e7":"code","9d54837e":"code","5d2875cf":"code","1de8319e":"code","896a04a0":"code","4a7128ed":"code","c4f4fa5c":"code","75457c9a":"code","1fb172c4":"code","6709c28e":"code","017cb25a":"code","a084e722":"code","db276f9c":"code","1be1f413":"code","fb535daa":"code","2d0fbe2a":"code","334c8f8a":"code","fcd338df":"code","1ab65974":"code","036bd4af":"code","796c83d9":"code","7a3295cb":"code","d5cd3ed4":"code","2c1e8181":"code","60e2b4e5":"code","4667708d":"code","96e4c205":"code","a544c787":"code","4cb2b2f7":"code","7f4b2b55":"code","e8a1ba1e":"code","a8037b77":"code","f687df4c":"code","618ad0ac":"code","ffc92b68":"code","650ed332":"code","d5524e5f":"code","893d550f":"code","8434ee1e":"code","cd7f21ca":"code","7f71a501":"code","e5b6d54e":"code","cba04ceb":"code","76bbd6e1":"code","d5151b4d":"code","ba891079":"code","a4673666":"code","e04b49af":"code","11a52fdd":"code","53f10473":"code","2eb584ab":"code","2ccbcc74":"code","31636c80":"code","c8cacdf7":"code","bc9245d5":"code","98210adb":"code","e19f10b9":"code","6643b50e":"markdown","9611b4e1":"markdown","5ca48d85":"markdown","a97ab3f8":"markdown","9dcecee5":"markdown","146a5cb5":"markdown","9c743942":"markdown","e1a5b7ca":"markdown","927d4bb6":"markdown","62f973d2":"markdown","c5487b91":"markdown","17670dbf":"markdown","41e7e48a":"markdown","65182087":"markdown","dd743dea":"markdown","36327ef8":"markdown","d2fefdf0":"markdown","5a92d45b":"markdown","eeac425f":"markdown","d05fc439":"markdown","bc9e5408":"markdown","e35e9ae4":"markdown","16cb32f7":"markdown","996a33b4":"markdown","202f7316":"markdown","0e6f83b7":"markdown","65b36805":"markdown","51e58926":"markdown","94b416a3":"markdown","7ba88099":"markdown","c0fe6a71":"markdown","05bc8a56":"markdown","46e02e36":"markdown","5102cff5":"markdown","ca4a9e54":"markdown","f32f1c5d":"markdown","a6ef3ee9":"markdown","d20f3a11":"markdown","aa6fa462":"markdown","87273c0a":"markdown","e09b6e90":"markdown","9ac05c24":"markdown","e133cbde":"markdown","4cd6c11b":"markdown","069c69e7":"markdown","22af2534":"markdown","6f6bf5c9":"markdown","58857dda":"markdown","dce3185d":"markdown","6fb83323":"markdown","ff15823f":"markdown","e38bfa27":"markdown","9d565195":"markdown","253b9bc7":"markdown","7f7ccc86":"markdown","1d325f94":"markdown","10824c5b":"markdown","c70fa557":"markdown","861325ab":"markdown","021a4b30":"markdown","561ecf9b":"markdown","da6d760e":"markdown","a8e0d710":"markdown","cf24bcd4":"markdown","df5a7516":"markdown","e64cff2c":"markdown","8fc14c2f":"markdown","08a6cc06":"markdown","419e4d90":"markdown","dcd633fe":"markdown","0ddf7774":"markdown","19b1cc0b":"markdown","865d64a2":"markdown","b675e852":"markdown","5f5b4138":"markdown","ab2800d7":"markdown","bfb212ee":"markdown","9ff8a248":"markdown","6b49be49":"markdown","95cf2bbb":"markdown","61c94f76":"markdown","2d370629":"markdown","fdd093f8":"markdown","22446829":"markdown","4e9facc9":"markdown","ccbc0307":"markdown","6eb790a4":"markdown","da5e4ab2":"markdown","bf4bcea2":"markdown","9c832674":"markdown","81d2b139":"markdown","664dad2a":"markdown","6039d3b2":"markdown","b1331908":"markdown","2ae70033":"markdown","ecc1cc5c":"markdown","4cb454d7":"markdown","5064c176":"markdown","8b6b2421":"markdown","0bc79b15":"markdown","ee83f42b":"markdown","17d34db9":"markdown","b9c88a3f":"markdown","c9f8b3f9":"markdown","95cab344":"markdown","83c172fd":"markdown","30b8861f":"markdown","fd90d3d9":"markdown","02755a9e":"markdown","ed73d0b5":"markdown","02014921":"markdown","328f9b11":"markdown","2bfddfe7":"markdown","fc7f0f3b":"markdown","9c50bd54":"markdown"},"source":{"e0c1005c":"from IPython.display import Image\nImage(\"..\/input\/aceawaterimages\/toppic.jpeg\")","9c59db45":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport scipy\nfrom datetime import datetime, date \n# from termcolor import colored\nimport re\nimport math\nimport missingno as mnso\nfrom functools import partial\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes._axes import _log as matplotlib_axes_logger\n# matplotlib_axes_logger.setLevel('ERROR')\n\nimport tensorflow as tf\nimport keras\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.decomposition import PCA\n\nimport statsmodels\nimport statsmodels.stats.stattools\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import grangercausalitytests\nfrom statsmodels.tsa import vector_ar\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.vector_ar import vecm\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\n\nimport warnings\nfrom statsmodels.tools.sm_exceptions import ValueWarning, HypothesisTestWarning\nwarnings.simplefilter('ignore', ValueWarning)\nwarnings.simplefilter('ignore', HypothesisTestWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\ndatapath = '..\/input\/acea-water-prediction\/'\nmodelpath = '..\/input\/aceawatermodels\/'","4569313f":"# plotting functions\ndef feature_plots(data):\n    \"\"\"Plot each group of features of aquifer dataframe\"\"\"\n    date_flag = 0\n    df = data.copy()\n    if df is pd.Series: \n        df = df.to_frame()\n    # create Date column\n    if not 'Date' in df.columns.values:\n        df['Date'] = df.index\n        date_flag = 1\n    # plot Rainfalls\n    if df.columns.str.contains('Rainfall.*').any():\n        sns.relplot(x='Date', y='value', col='Feature', col_wrap=5, kind='line', \n                    data = df.filter(regex='Date|^Rainfall_.+').melt(\n                            id_vars='Date', var_name='Feature', ignore_index=False))\n        plt.suptitle('Rainfalls', y=1.02)\n    # plot gounrdwaters and Volumes\n    if df.columns.str.contains('Depth_.*').any():\n        sns.relplot(x='Date', y='value', hue='Feature', kind='line', \n                    data = df.filter(regex='Date|^Depth_.+').melt(\n                            id_vars='Date', var_name='Feature', ignore_index=False))\n        plt.suptitle('Groundwaters', y=1.02)\n    if df.columns.str.contains('Volume.*').any():\n        sns.relplot(x='Date', y='value', hue='Feature', kind='line', \n                    data = df.filter(regex='Date|^Volume_.+').melt(\n                            id_vars='Date', var_name='Feature', ignore_index=False))\n        plt.suptitle('Volumes', y=1.02)\n    # plot Temperatures\n    if df.columns.str.contains('Temperature.*').any():\n        sns.relplot(x='Date', y='value', col='Feature', kind='line', \n                    data = df.filter(regex='Date|^Temperature.+').melt(\n                            id_vars='Date', var_name='Feature', ignore_index=False))\n        plt.suptitle('Temperatures', y=1.02)\n    # plot Hydrometry\n    if df.columns.str.contains('Hydrome.*').any():\n        sns.relplot(x='Date', y='value', col='Feature', kind='line', \n                    data = df.filter(regex='Date|^Hydrometry_.+').melt(\n                            id_vars='Date', var_name='Feature', ignore_index=False))\n        plt.suptitle('Hydrometries', y=1.02)\n    # plot Flow Rate\n    if df.columns.str.contains('Flow.*').any():\n        sns.relplot(x='Date', y='value', hue='Feature', kind='line', \n                    data = df.filter(regex='Date|^Flow_.+').melt(\n                            id_vars='Date', var_name='Feature', ignore_index=False))\n        plt.suptitle('Flow Rates', y=1.02)\n    if df.columns.str.contains('Lake.*').any():\n        sns.relplot(x='Date', y='value', hue='Feature', kind='line', \n                    data = df.filter(regex='Date|^Lake_.+').melt(\n                            id_vars='Date', var_name='Feature', ignore_index=False))\n        plt.suptitle('Lake Levels', y=1.02)\n    plt.show()\n    if date_flag:\n        df.drop(columns='Date', inplace=True)\n        \ndef water_spring_feature_plots(df):\n    \"\"\"Plot each group of features of aquifer dataframe\"\"\"\n    date_flag = 0\n    # create Date column\n    if not 'Date' in df.columns.values:\n        df['Date'] = df.index\n        date_flag = 1\n    # plot rainfalls\n    sns.relplot(x='Date', y='value', col='Feature', col_wrap=5, kind='line',\n                data = df.filter(regex='Date|^Rainfall_.+').melt(\n                        id_vars='Date', var_name='Feature', ignore_index=False))\n    plt.suptitle('Rainfalls', y=1.02)\n    # plot groundwater depths\n    sns.relplot(x='Date', y='value', hue='Feature', kind='line',\n                data = df.filter(regex='Date|^Depth_.+').melt(\n                        id_vars='Date', var_name='Feature', ignore_index=False))\n    plt.suptitle('Groundwaters', y=1.02)\n    # plot flow rate\n    sns.relplot(x='Date', y='value', hue='Feature', kind='line',\n                data = df.filter(regex='Date|^Flow_.+').melt(\n                        id_vars='Date', var_name='Feature', ignore_index=False))\n    plt.suptitle('Flow Rates', y=1.02)\n    # plot temperatures\n    sns.relplot(x='Date', y='value', col='Feature', kind='line',\n                data = df.filter(regex='Date|^Temperature.+').melt(\n                        id_vars='Date', var_name='Feature', ignore_index=False))\n    plt.suptitle('Temperatures', y=1.02)\n    if date_flag:\n        df.drop(columns='Date', inplace=True)\n\ndef missingval_plot(df, figsize=(20,6), show=True):\n    \"\"\"\n    Visualize index location of missin values of each feature.\n    Doesn't work for 1-dim df.\n    df: pd.DataFrame \n    \"\"\"\n    # check all are bool\n    if (df.dtypes != bool).any():\n        df = df.reset_index().T.isna()\n    f, ax = plt.subplots(nrows=1, ncols=1, figsize=figsize)\n    g = sns.heatmap(df, cmap='Blues', cbar=True, yticklabels=df.index.values)\n    # customize colorbar\n    colorbar = g.collections[0].colorbar\n    colorbar.set_ticks([0, 1])\n    colorbar.set_ticklabels(['non-missing', 'missing'])\n    # customize title\n    ax.set_title('Distribution of Missing Values', fontsize=16)\n    # customize font size in ticks\n    for tick in ax.xaxis.get_major_ticks():\n        tick.label.set_fontsize(12) \n    for tick in ax.yaxis.get_major_ticks():\n        tick.label.set_fontsize(12)\n    if show: \n        plt.show()\n\ndef corrtri_plot(df, figsize=(10,10)):\n    \"\"\"correlation plot of the dataframe\"\"\"\n    # sns.set() #: will cause plt warning later in lag_plot\n    c = df.corr()\n    mask = np.triu(c.corr(), k=1)\n    plt.figure(figsize=figsize)\n    plt.tick_params(axis='both', which='major', labelsize=10, \n                    bottom=False, labelbottom=False, \n                    top=False, labeltop=True)\n    g = sns.heatmap(c, annot=True, fmt='.1f', cmap='coolwarm', \n                    square=True, mask=mask, linewidths=1, cbar=False)\n    plt.show()\n\ndef periodogram(series, division=8):\n    \"\"\"plot periodogram of the series to find most important frequency\/periodicity\"\"\"\n    dt = 1\n    T = len(series.index)\n    t = np.arange(0, T, dt)\n    f = series.fillna(0)\n    n = len(t)\n    fhat = np.fft.fft(f, n, )\n    PSD = np.conj(fhat) \/ n\n    freq = np.arange(n) # * (1\/(dt*n))\n    L = np.arange(1, np.floor(n\/division), dtype=\"int\") # n\/4, otherwise too long of a stretch to see anything\n    \n    plt.plot(freq[L], np.abs(PSD[L]), linewidth=2, label='Lag Importance')\n    plt.xlim(freq[L[0]], freq[L[-1]])\n    plt.legend()\n    plt.hlines(0, xmin=0, xmax=n, colors='black')\n    plt.title('Periodogram of ' + series.name)\n    plt.show()\n    \ndef rfft_plot(series, ylim=(0,400)):\n    \"\"\"plot real valued fourier transform to find most important frequency\/periodicity\"\"\"\n    fft = tf.signal.rfft(series)\n    f_per_dataset = np.arange(0, len(fft))\n\n    n_samples_d = len(series)\n    d_per_year = 365.2524\n    years_per_dataset = n_samples_d\/(d_per_year)\n    f_per_year = f_per_dataset\/years_per_dataset\n\n    plt.step(f_per_year, np.abs(fft))\n    plt.xscale('log')\n    plt.ylim(*ylim)\n    plt.xlim([0.1, max(plt.xlim())])\n    plt.xticks([1, 4, 12, 52, 365.2524], labels=['1\/Year', '1\/quarter', '1\/month', '1\/week', '1\/day'])\n    _ = plt.xlabel('Frequency (log scale)')\n    plt.show()\n\ndef metrics_plot(valdict, testdict=None, metrics='', verbose=False):\n    \"\"\"\n    valdict\/testdict (same shape): the dict form {keys : {xtick:value}} where the `metrics'\n             should be part of the `keys'. The inner dict will be plotted as a bar.\n    metrics: ylabels of the plot and also a list containing part\/all of the key(s) of \n             valdict\/testdict.\n    verbose: report or not.\n    \"\"\"\n    x = np.arange(len(valdict))\n    width=0.3\n    testdict_flag = False\n    if testdict is not None: \n        testdict_flag = True\n        x2 = np.arange(len(testdict)) + 0.17\n        x = x-0.17\n    \n    fig, axs = plt.subplots(1, len(metrics), figsize=(10,5))\n    for i, key in enumerate(metrics):\n        val_performance = [valdict[model][key] for model in valdict.keys()]\n        axs[i].bar(x, val_performance, width, label='Validation')\n        axs[i].set_xticks(np.arange(len(val_performance)))\n        axs[i].set_xticklabels(list(valdict.keys()), rotation=90)\n        axs[i].set(ylabel=key)\n        if testdict_flag:\n            test_performance = [testdict[model][key] for model in testdict.keys()]\n            axs[i].bar(x2, test_performance, width, label='Test')\n        _ = axs[i].legend()\n    fig.tight_layout(pad=2)\n        \n    if verbose: \n        for name, value_dict in valdict.items():\n            print(f'{name:16s}')\n            for key, val in value_dict.items():\n                if key not in metrics: continue\n                print(f'  {key:<20s}: {val:6.4f}')\n                \ndef lag_plot(df, lag=1, redcols=None, figsize=(20,15)):\n    \"\"\"\n    plot t+lag against t of each feature in df.\n    df: pd.dataframe\n    redcols: list\/array of column names to be colored with red.\n    \"\"\"\n    plt.figure(figsize=figsize)\n    for i, col in enumerate(df.columns):\n        ax = plt.subplot(len(df.columns)\/\/5 +1 , 5, i+1)\n        color = 'k'\n        if redcols is not None and col in redcols:\n            color = 'r'\n        pd.plotting.lag_plot(df[col], lag=lag)\n        plt.title(col, color=color)\n    ax.figure.tight_layout(pad=0.5)\n    \ndef acpac_plot(data, figsize=(10,5)):\n    \"\"\"Autocorrelation and Partial-aurocorrelation plots.\"\"\"\n    for i, col in enumerate(data.columns):\n        fig, ax = plt.subplots(1,2,figsize=figsize)\n        plot_acf(data[col], lags=30, title='AC: ' + data[col].name, \n                 ax=ax[0])  # missing='drop'\n        plot_pacf(data[col], lags=30, title='PAC: ' + data[col].name, \n                 ax=ax[1])\n        plt.show()\n\ndef pca_plot(data, n_comp=None, regex=None, figsize=(5,3)):\n    \"\"\"\n    Plot n_comp pricipal components of data via PCA.\n    data:   pd.DataFrame \/ np.ndarray\n    regex:  string pattern to filter data. \n            Use all data if not specified.\n    n_comp: number of components desired in PCA. \n            Default to data column numbers if not specified.\n    \"\"\"\n    fig = plt.figure(figsize=figsize)\n    ax = fig.add_axes([0,0,1,1])\n    x = data\n    if regex is not None: \n        x = x.filter(regex=regex)\n    xSfit = StandardScaler().fit_transform(x)\n    if n_comp is None:\n        n_comp = xSfit.shape[1]\n    pca = PCA(n_components=n_comp)\n    pca.fit(xSfit)\n    v = pca.explained_variance_ratio_.round(2)\n    xtick = range(1,n_comp+1)\n    ax.bar(xtick,v) # range(1,n_comp+1)\n    plt.xticks(xtick, x.columns, rotation='vertical')\n    plt.xlabel(\"PCA components\")\n    plt.title(\"Variance Explained by each dimension\")\n    plt.show()\n","7533aaae":"# data engineering functions\ndef data_frequency_transform(data, freq, logic_dict):\n    \"\"\"\n    Downsampling (daily) data to weekly\/monthly\/user-defined(freq) data via methods in logic_dict.\n    Inputs: \n        data: pd.DataFrame\n        freq: a string expressing desired frequency to be transform. Eg, '2D', '7D', 'W', 'M', 'Q'.\n        logic_dict: a dict specifying which methods (value) is used for which features\/cols (key).\n    \"\"\"\n    collector = []\n    for regex, method in logic_dict.items(): \n        tmp = data.filter(regex=regex).resample(freq, label='right', closed='right').agg(method)\n        collector.append(tmp)\n    return pd.concat(collector, axis=1)\n\ndef fillna(data, approach, **kwargs):\n    \"\"\"\n    fill nan with the specified approach.\n    data: pd.DataFrame\n    approach: {'constant', 'mean', 'interpolate', 'movingavg', 'regression'}\n    \"\"\"\n    if approach == 'constant':\n        return data.fillna(-99999)\n    elif approach == 'mean':\n        return data.fillna(data.mean(axis=0, skipna=True))\n    elif approach == 'median':\n        return data.fillna(data.median(axis=0, skipna=True))\n    elif approach == 'interpolate':\n        # data cannot contain datetime columns\n        # kwargs: method, axis, limit, limit_direction, limit_area, inplace\n        return data.interpolate(**kwargs)\n    elif approach == 'movingavg':\n        # kwargs: window, min_periods, center, win_type\n        return data.fillna(data.rolling(**kwargs).mean())\n    elif approach == 'regression':\n        # kwargs: window\n        df = data.copy()\n        names = df.columns.drop('Date', errors='ignore')\n        assert df.iloc[0].isna().sum() == 0\n        for t in range(1, df.shape[0]):\n            for feature in names:\n                if not np.isnan(df[feature].iloc[t]):\n                    continue\n                parameters = names.drop(feature)\n                parameters = df[parameters].iloc[t].dropna().index\n                model = linear_model.LinearRegression()\n                window = kwargs.get('window', 1)\n                front = max(t-window,0)\n                X = df[parameters].iloc[front:t]\n                y = df[feature].iloc[front:t] \n                model.fit(X=X, y=y)\n                df[feature].iloc[t] = model.predict(df[parameters].iloc[t:t+1])\n        return df\n    else:\n        raise ValueError('Keyword not found for approach')\n\n# PCA requires no missing values and components to be stationary.\ndef get_component_number(data, min_variance_share=0.8, leave_out_regex = \"Depth.*\"):\n    \"\"\"\n    Return the number of components that first retrieve the desired share of variance \n    via Principle Component Analysis.\n    Inputs: \n        data: pd.DataFrame \n        min_variance_share: desired described share of variance\n        leave_out_regex: pattern of columns in data that ought to be ignored.\n    \"\"\"\n    names = data.columns.drop('Date', errors='ignore').drop(\n        data.filter(regex=leave_out_regex).columns, errors='ignore')\n    x = data.loc[:, names].values\n    x = StandardScaler().fit_transform(x)\n    explained_variance = 0\n    n = 0\n    while explained_variance < min_variance_share:\n        n = n + 1\n        pca = PCA(n_components=n)\n        #principalComponents = pca.fit_transform(x)\n        pca.fit(x)\n        explained_variance = pca.explained_variance_ratio_.sum()\n    print('Share of decribed variance: ', explained_variance)\n    print('Number of components: ', n)\n    return n\n\n# Buids you a new dataset with only the desired number of components. Inuputs:\ndef build_pca_data(data, component_number=5, leave_out_regex=\"Depth.*\", concat=True):\n    \"\"\"\n    Builds a new df with only the desired number of components via PCA.\n    Inputs: \n        data: pd.DataFrame \n        component_number: desired number of components.\n        leave_out_regex: pattern of columns in data that ought to be ignored.\n    \"\"\"\n    ori_cols = data.columns\n    names = data.columns.drop('Date', errors='ignore').drop(\n        data.filter(regex=leave_out_regex).columns, erros='ignore')\n    x = data.loc[:, names].values\n    # y = data.loc[:, target].values\n    x = StandardScaler().fit_transform(x)\n    pca = PCA(n_components=component_number)\n    principalComponents = pca.fit_transform(x)\n    principalDf = pd.DataFrame(data=principalComponents)  # , columns=['PC1', 'PC2', 'PC3'])\n    if concat: \n        target = data.filter(regex=leave_out_regex).columns\n        finalDf = pd.concat([principalDf, df[target]], axis=1)\n        df.reindex(columns = ori_cols)\n        return finalDf\n    return principalDf\n\ndef percent_change(data):\n    \"\"\"\n    Calculate the %change between the last value and the mean of previous values.\n    Makes data more comparable if the abs values of the data change a lot.\n    Usage: df.rolling(window=20).aggregate(percent_change)\n    \"\"\"\n    previous_values = data[:-1]\n    last_value = values[-1]\n    return (last_value - np.mean(previous_values)) \/ np.mean(previous_values)\n\ndef replace_outliers(df, window=7, k=3, method='mean'):\n    \"\"\"\n    Inputs:\n        df: pd.DataFrame\/Series\n        window: rolling window size to decide rolling mean\/std\/median.\n        k: multiplier of rolling std.\n    Return: \n        pd.DataFrame of the same shape as input df.\n    \"\"\"\n    # de-mean\n    mean_series = df.rolling(window).mean()[window-1:]\n    abs_meandiff = (df[window-1:] - mean_series).abs()\n    std_series = df.rolling(window).std()[window-1:]\n    median_series = df.rolling(window).median()[window-1:]\n    # identify >k(=3 in default) standard deviations from zero\n    this_mask = abs_meandiff > (std_series * k)\n    tmp = df[:window-1].astype(bool)\n    tmp.values[:] = False\n    this_mask = pd.concat([tmp, this_mask], axis=0)\n    # Replace these values with the median accross the data\n    if method == 'median':\n        to_use = median_series\n    elif method == 'mean':\n        to_use = mean_series\n    else: \n        raise ValueError(f'method {method} not found.')\n    return df.mask( this_mask, to_use )\n    \ndef scale(train, val, test, approach='MinMax'):\n    \"\"\"scale train (and test) data via MinMaxScalar\/StandardScalar\"\"\"\n    if approach == 'MinMax':\n        scaler = MinMaxScaler(feature_range=(-1, 1))\n    elif approach == 'Standard':\n        scaler = StandardScaler()\n    # save DataFrame Info\n    # fit and transform train\n    scaler = scaler.fit(train)\n    train_scaled = pd.DataFrame(scaler.transform(train), columns=train.columns, index=train.index)\n    # transform val\/test\n    val_scaled = pd.DataFrame(scaler.transform(val), columns=val.columns, index=val.index)\n    test_scaled = pd.DataFrame(scaler.transform(test), columns=test.columns, index=test.index)\n    return scaler, train_scaled, val_scaled, test_scaled\n\n# primary function: used by following self-defined classes\ndef inverse_scale(scaler, data, indices):\n    \"\"\"\n    Applies to both single and multi-timestep predictions and groundtruths.\n    Inputs:\n        scaler: MinMaxScaler\/StandardScaler obj from sklearn which \n                has fitted the orignal form of data.\n        data: scaled 3D nparray (batch, timestep, n_out). Doesn't accept 2D nparray.\n        indices: desired column indices used in scaler to scale each corresponding data columns back.\n    Return: \n        inverse-scaled 3D array of the same shape as input data.\n    PS: To recover 2D data back, please directly use scaler.inverse_transform().\n    \"\"\"\n    batch_size, timestep, n_out = data.shape[0], data.shape[1], data.shape[2]\n    \n    if 'MinMaxScaler' in str(type(scaler)):\n        return ((data.reshape(-1, n_out) - scaler.min_[indices]) \/ scaler.scale_[indices]\n               ).reshape(batch_size, timestep, n_out)\n    elif 'StandardScaler' in str(type(scaler)):\n        return (data.reshape(-1, n_out) * scaler.scale_[indices] + scaler.mean_[indices]\n               ).reshape(batch_size, timestep, n_out)\n\n# primary function: used by following self-defined classes\ndef inverse_diff(history, pred, start=1):\n    \"\"\"\n    Recover corresponding value of prediction in differenced feature back to the one before diff().\n    Applies to both single and multi-timestep predictions of a single feature.\n    Inputes: \n        history: 1d array\/series of the feature.\n        pred: 2d nparray (batch, timestep) corresponding to the feature, or \n              1d nparray (collection of 1-step-ahead predictions) corresponding to the feature.\n        start: the starting position in history to add on values to pred.\n               Usually start = pred[0]'s index -1 since pred is based on previous obersvation \n               due to diff().\n    Return: \n        nparray with same shape as pred.\n    \"\"\"\n    # if I1, nth steps and mode='normal': varlag+steps-1 :  varlag+steps-1+len(pred)\n    # if I1, n-steps and mode='last': gt[-steps-1 :-1]\n    # if I0, nth steps and mode='normal': varlag+steps : varlag+steps+len(pred)\n    # if I0, n-steps and mode='last': gt[ -steps: ]\n    if type(history) is pd.Series:\n        history = history.values\n    if pred.ndim == 2: \n        # supports 2D last-mode calculation (reshape 1D pred to (1, -1) before calling the function)\n        # supports 3D normal-mode calculation\n        batch_size, timestep = pred.shape[0], pred.shape[1]\n        return pred.cumsum(axis=1) + \\\n            history[start:start+batch_size].repeat(timestep).reshape(batch_size, timestep)\n    if pred.ndim == 1: \n        # only supports normal-mode model calculation.\n        return pred + history[start:start+len(pred)]\n    \n","4a046cd0":"# testing functions\ndef adftest(series, verbose=True, **kwargs):\n    \"\"\"adfuller + printing\"\"\"\n    # kwargs: maxlag, regression, autolag\n    from statsmodels.tsa.stattools import adfuller\n    res = adfuller(series.values, **kwargs)\n    if verbose:\n        print('ADF Statistic: {:13f} \\tp-value: {:10f}'.format(res[0], res[1]))\n        if 'autolag' in kwargs.keys():\n            print('IC: {:6s} \\t\\t\\tbest_lag: {:9d}'.format(kwargs['autolag'], res[2]))\n        print('Critical Values: ', end='')\n        for key, value in res[4].items():\n            print('{:2s}: {:>7.3f}\\t'.format(key, value), end='')\n    return res\n\ndef adfuller_table(df, verbose=False, alpha=0.05, **kwargs):\n    \"\"\"iterate over adftest() to generate a table\"\"\"\n    columns = [f'AIC_{int(alpha*100)}%level', 'AIC_bestlag', f'BIC_{int(alpha*100)}%level', 'BIC_bestlag']\n    table = pd.DataFrame(columns=columns)\n    for col in df.columns: \n        row = []\n        for autolag in ['AIC', 'BIC']:\n            res = adftest(df[col], verbose=verbose, autolag=autolag, **kwargs)\n            sig = True if abs(res[0])>abs(res[4][f'{int(alpha*100)}%']) else False\n            row.extend([sig, res[2]])\n        table = table.append(pd.Series(row, index=table.columns, name=col))\n    table.index.name = 'ADFuller Table alpha={}'.format(alpha)\n    return table\n\ndef grangers_causation_table(data, xnames, ynames, maxlag, test='ssr_chi2test', alpha=None):\n    \"\"\"\n    Check Granger Causality of all possible combinations of the Time series.\n    The values in the table are the P-Values\/boolean (reject H0 or not). \n    H0: X does not cause Y (iff coefs of X on Y is 0)\n    Inputs:\n        data      : pandas dataframe containing the time series variables\n        xnames    : list of TS variable names to test granger causality on ynames.\n        ynames    : list of TS variable names to be granger predicted.\n        maxlag    : max lags.\n        test      : 'ssr_ftest', 'ssr_chi2test', 'lrtest', 'params_ftest'\n        alpha     : significance level. Return boolean table if alpha is specified != None.\n    \"\"\"\n    res = pd.DataFrame(np.zeros((len(xnames), len(ynames))), columns=ynames, index=xnames)\n    for c in res.columns:\n        for r in res.index:\n            test_result = grangercausalitytests(data[[r, c]], maxlag=maxlag, verbose=False)\n            p_values = [ round(test_result[i+1][0][test][1],4) for i in range(maxlag) ]\n            min_p_value = np.min(p_values)\n            res.loc[r, c] = min_p_value\n    res.columns = res.columns + '_y'\n    res.index =  res.index + '_x'\n    if alpha is None: \n        res.index.name = 'Granger Causation Table'\n        return res\n    res.index.name = 'Granger Causation Table alpha={}'.format(alpha)\n    return res < alpha\n","5d788807":"class WindowGenerator:\n    \"\"\"\n    Initialize a window generator that helps slice train\/val\/test_df into inputs and labels\n    that is suitable for supervised learning of machine learning approaches.\n    \n    Explanation for input_width, label_width, shift and total_window_size:\n        Given inputs of size (batch, input_width, features), we wish to train a model that \n        its output shape is equivalent to the labels shape (batch, labels_width, features)\n        which has a timestep gap = shift. total_window_size = input_width+shift.\n    Eg, w=WindowGenerator(input_width=2, label_width=1, shift=1, ...)\n        Given past 2 hrs, predict the target values in the next hr. (total window is 3)\n    Eg, w=WindowGenerator(input_width=2, label_width=1, shift=2, ...)\n        Given past 2 hrs, predict the result values in the 2nd hr. (total window is 4)\n    Eg, w=WindowGenerator(input_width=2, label_width=2, shift=2, ...)\n        Given past 2 hrs, predict the result values in next 2 hrs. (total window is 4)\n    \"\"\"\n    \n    def __init__(self, input_width, label_width, shift, train_df, val_df, test_df, \n                 batch_size=1, shuffle=True, mask=None, label_columns=None):\n        assert input_width + shift >= label_width, \"total_window_size can't be negative.\"\n        # Store the raw data. [batch_size, timestep, features]\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n        self.batch_size = batch_size\n        self.shuffle = shuffle # make_dataset\n\n        # Work out the label column indices.\n        self.column_indices = {name: i for i, name in enumerate(train_df.columns)}\n        self.label_columns = train_df.columns\n        if label_columns is not None:\n            self.label_columns = [label_columns] if type(label_columns) is str else label_columns\n        self.label_columns_indices = [self.column_indices[la] for la in self.label_columns]\n        \n\n        # Work out the window parameters.\n        self.input_width = input_width\n        self.label_width = label_width\n        self.shift = shift\n\n        self.total_window_size = input_width + shift\n\n        self.input_slice = slice(0, input_width) #slice from idx=0 till idx=input_width-1(included)\n        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n        \n        self.label_start = self.total_window_size - self.label_width\n        self.labels_slice = slice(self.label_start, None) # slice from idx=label_start wihout end\n        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n        \n        # mask\n        self.mask = mask \n        if mask is not None: \n            assert mask.shape[0] == self.test_df.shape[0]\n            self.mask = mask[self.label_columns].values[self.label_indices[0]:]\n        \n    def __repr__(self):\n        return '\\n'.join([\n            f'Total window size: {self.total_window_size}',\n            f'Input indices: {self.input_indices}',\n            f'Label indices: {self.label_indices}',\n            f'shuffle: {self.shuffle}',\n            f'Label column name(s): {self.label_columns}'])\n\n    def split_window(self, features):\n        \"\"\"\n        Split data into inputs and labels.\n        features: [batch=None, self.total_window_size, features=None]\n        \"\"\"\n        inputs = features[:, self.input_slice, :]\n        # return all features if self.label_columns is None\n        # Ow, return only featrues in self.label_columns in order\n        labels = features[:, self.labels_slice, :]\n        if self.label_columns is not None:\n            labels = tf.stack(\n                [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n                axis=-1)\n        # Slicing doesn't preserve static shape information, so set the shapes\n        # manually. This way the `tf.data.Datasets` are easier to inspect.\n        inputs.set_shape([None, self.input_width, None])\n        labels.set_shape([None, self.label_width, None])\n\n        return inputs, labels\n    \n    def inverse_transform_evaluate(self, model, scaler=None, diff=None, history=None, which='test', \n                               multioutput='uniform_average', return_dict=False, inplace=True):\n        # doesn't provide mask evaluation\n        assert which in ['val', 'test'], f'not allowed input: which={which}'\n        # default test mode\n        pred = model.predict(self.test)\n        outputs = self.y_test\n        start = len(self.train_df)+len(self.val_df)+self.label_indices[0]\n        if which == 'val':\n            pred = model.predict(self.val)\n            outputs = self.y_val\n            start = len(self.train_df)+self.label_indices[0]\n        # invert scale\n        if scaler is not None:\n            pred = inverse_scale(scaler, pred, self.label_columns_indices)\n            outputs = inverse_scale(scaler, outputs, self.label_columns_indices)\n        # invert diff\n        if diff is not None: \n            assert history is not None, 'diff is provided but history is not.'\n            for i, col in enumerate(self.label_columns):\n                if col not in diff:\n                    continue\n                pred[:,:,i] = inverse_diff(history[col], pred[:,:,i], start)\n                outputs[:,:,i] = inverse_diff(history[col], outputs[:,:,i], start )\n        # metrics\n        rmse, mae = self._core_evaluate(outputs, pred, multioutput)\n        if inplace: \n            self._multiout = True if multioutput=='raw_values' else False\n            self.pred = pred\n            self.outputs = outputs\n            self.rmse = rmse\n            self.mae = mae\n        if return_dict:\n            return {'root_mean_squared_error': rmse, 'mean_absolute_error': mae}\n        return rmse, mae\n    \n    def _core_evaluate(self, y_true, y_pred, multioutput):\n        # doesn't provide mask evaluation\n        n_out = len(self.label_columns)\n        rmse = mean_squared_error(y_true.reshape(-1, n_out), y_pred.reshape(-1, n_out), \n                                 multioutput=multioutput, squared=False)\n        mae = mean_absolute_error(y_true.reshape(-1, n_out), y_pred.reshape(-1, n_out), \n                                  multioutput=multioutput)\n        return rmse, mae\n    \n    def sample_plot(self, plot_cols, model=None, max_subplots=3, figsize=(15, 10)):\n        inputs, labels = self.example # [batch_size, timestep, features]\n        plt.figure(figsize=figsize, )\n        if type(plot_cols) is str: \n            plot_cols = [plot_cols]\n        plot_cols_index = [self.column_indices[col] for col in plot_cols]\n        \n        max_n = min(max_subplots, len(inputs)) # arg vs batch_size\n        max_j = len(plot_cols)\n        for n in range(max_n):\n            for j in range(max_j):\n                ax = plt.subplot(max_n, max_j, max_j*n + j+1)\n                ax.figure.tight_layout(pad=1.0)\n                # plot_cols[j] as Inputs\n                plt.title(f'y:{plot_cols[j]} [scaled]')\n                plt.plot(self.input_indices, inputs[n, :, plot_cols_index[j]],\n                         label='Inputs', marker='.', zorder=-10)\n                # Groundtruth of plots_col[j] == labels[label_col_index] (same string)\n                if self.label_columns:\n                    tmp = {name: i for i, name in enumerate(self.label_columns)}\n                    label_col_index = self.tmp.get(plot_cols[j], None)\n                else:\n                    label_col_index = plot_cols_index[j]\n                if label_col_index is None:\n                    continue\n                plt.scatter(self.label_indices, labels[n, :, label_col_index], \n                            edgecolors='k', label='Groundtruth', c='#2ca02c', s=64)\n                # Prediction of plots_col[j]\n                if model is not None:\n                    predictions = model(inputs)\n                    plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n                              marker='X', edgecolors='k', label='Prediction', c='#ff7f0e', s=64)\n                if n == 0:\n                    plt.legend()\n                plt.xlabel(f'Sample {n} in batch.  Timestep [scaled]')\n                \n    def plot(self, plot_cols=None, model=None, figsize=(20, 15), use_mask=False, ):\n        \"\"\"\n        Plot model prediction and groundtruth (untransformed) if model is provided.\n        Otherwise, plot prediciton and groundtruth (transformed) stored inside from \n        inverse_transform_evaluate(..., inplace=True), ie, self.pred\/outputs. \n        Output error if neither model nor self.pred\/outputs is found.\n        \"\"\"\n        assert isinstance(use_mask, bool)\n        labels = self.test_df[self.label_columns]\n        dates = labels[self.label_columns[0]].index[self.label_indices[0]:]\n        if plot_cols is None: \n            plot_cols = self.label_columns\n        if type(plot_cols) is str: \n            plot_cols = [plot_cols]\n        assert set(plot_cols).issubset(set(self.label_columns)), 'plot_cols must be a subset of targets'\n        label2idx = {name:i for i, name in enumerate(self.label_columns)}\n        plot_cols_index = [label2idx[x] for x in plot_cols]\n        \n        # pred & gt\n        if model is not None: \n            pred = model.predict(self.test)\n            gt = labels[self.label_columns].values[self.label_indices[0]:]\n            rmse, mae = self._core_evaluate(gt, pred, multioutput='raw_values')\n        else:       # fetch from inner storage\n            pred = self.pred\n            gt = self.outputs\n            if self._multiout:\n                rmse, mae = self.rmse, self.mae\n            else: \n                rmse, mae = self._core_evaluate(gt, pred, multioutput='raw_values')\n            if gt.shape[1] == 1: \n                gt = gt.reshape(-1, gt.shape[2])\n            else: \n                collector = []\n                collector.append( [gt[batch,0,:] for batch in range(gt.shape[0])] )\n                collector.append( gt[-1, 1:, :] )\n                gt = np.concatenate(collector)\n\n        # plot\n        plt.figure(figsize=figsize, )\n        for j, name in enumerate(plot_cols):\n            ax = plt.subplot(len(plot_cols), 1,  j+1)\n            j = plot_cols_index[j]\n            # groundtruth\n            if use_mask:\n                mask = self.mask.copy()\n                plt.scatter(dates[~mask[:,j]], gt[~mask[:,j], j], \n                            c='k', linestyle='-', alpha=0.7, marker='+', label='Groundtruth')\n            else: \n                plt.scatter(dates, gt[:, j], \n                            c='k', linestyle='-', alpha=0.7, marker='+', label='Groundtruth')\n            # prediction: (batch_size, timestep, features)\n            if self.label_width == 1: # normal mode\n                plt.plot(dates, pred[:, 0, j],\n                         c='red', linestyle='-', alpha=0.5, marker='+', label='Prediction')\n            else:                     # last mode\n                plt.plot(dates[-pred.shape[1]:], pred[-1, :, j],\n                         c='red', linestyle='-', alpha=0.5, marker='+', label='Prediction')\n            # layout\n            ax.legend()\n            plt.gca().xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%y-%b'))\n            outstr = 'transformed' if model is not None else 'inversed transformed'\n            plt.title('{} [{}]: RMSE={:6.4f}, MAE={:6.4f} (imputation incl.)'.format(\n                name, outstr, rmse[j], mae[j]), )\n        ax.figure.tight_layout(pad=1.0)\n        plt.show()\n\n    def _make_dataset(self, data):\n        \"\"\"convert df\/data to timeseries dataset format of (inputs, labels) pairs\"\"\"\n        data = np.array(data, dtype=np.float32)\n        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n          data=data,\n          targets=None,\n          sequence_length=self.total_window_size,\n          sequence_stride=1,\n          shuffle=self.shuffle,\n          batch_size=self.batch_size,)\n        # apply split window\n        ds = ds.map(self.split_window)\n        return ds\n    \n    @property\n    def train(self):\n        return self._make_dataset(self.train_df)\n\n    @property\n    def val(self):\n        return self._make_dataset(self.val_df)\n\n    @property\n    def test(self):\n        return self._make_dataset(self.test_df)\n\n    @property\n    def example(self):\n        \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n        result = getattr(self, '_example', None)\n        if result is None:\n            # No example batch was found, so get one from the `.train` dataset\n            result = next(iter(self.train))\n            # And cache it for next time\n            self._example = result\n        return result\n    \n    @property\n    def X_train(self):    \n        return np.vstack( list(self.train.map(lambda x,y: x).as_numpy_iterator())\n                ).reshape(-1, self.input_width, len(self.column_indices) )\n    \n    @property\n    def X_val(self):    \n        return np.vstack( list(self.val.map(lambda x,y: x).as_numpy_iterator())\n                ).reshape(-1, self.input_width, len(self.column_indices) )\n    \n    @property\n    def X_test(self):    \n        return np.vstack( list(self.test.map(lambda x,y: x).as_numpy_iterator())\n                ).reshape(-1, self.input_width, len(self.column_indices) )\n    \n    @property\n    def y_train(self):    \n        return np.vstack( list(self.train.map(lambda x,y: y).as_numpy_iterator())\n                ).reshape(-1, self.label_width, len(self.label_columns) )\n    \n    @property\n    def y_val(self):    \n        return np.vstack( list(self.val.map(lambda x,y: y).as_numpy_iterator())\n                ).reshape(-1, self.label_width, len(self.label_columns) )\n    \n    @property\n    def y_test(self):\n        return np.vstack( list(self.test.map(lambda x,y: y).as_numpy_iterator())\n                ).reshape(-1, self.label_width, len(self.label_columns) )\n\nclass Baseline(tf.keras.Model):\n    \"\"\"Single-step naive baseline: y(t+1) = y(t)\"\"\"\n    def __init__(self, label_index=None, return_sequence=False):\n        super().__init__()\n        self.label_index = [label_index] if type(label_index) is int else label_index\n        self.return_sequence = return_sequence\n\n    def call(self, inputs):\n        if self.label_index is None:\n            return inputs\n\n        if self.return_sequence:\n            result = tf.stack( [inputs[:, :, j] for j in self.label_index] ,  axis=2)\n            return result\n        else: \n            result = tf.stack( [inputs[:, -1, j] for j in self.label_index] ,  axis=1)\n            return result[:, tf.newaxis, :]\n\nclass MultiStepBaseline(tf.keras.Model):\n    \"\"\"\n    Multistep naive baseline\n        mode='last': y([t+1,...,t+k]) = y([t, t, ..., t])\n        mode='repeat': y([t+1,...,t+k]) = y([t-k+1, t-k+2, ..., t])\n    \"\"\"\n    def __init__(self, out_steps, label_index=None, mode='last'):\n        super().__init__()\n        self.out_steps = out_steps\n        self.label_index = [label_index] if type(label_index) is int else label_index\n        self.mode = mode\n        \n    def call(self, inputs):\n        if self.label_index is None and self.mode=='last': \n            return tf.tile(inputs[:, -1:, :], [1, self.out_steps, 1])\n        elif self.label_index is None and self.mode=='repeat':\n            return inputs\n        \n        if self.mode == 'last':\n            result = tf.stack( [inputs[:, -1, j] for j in self.label_index] ,  axis=1)\n            return tf.tile(result[:, tf.newaxis, :], [1, self.out_steps, 1])\n        elif self.mode == 'repeat':\n            k = inputs.shape[1]\n            # within 1 cycle repeat\n            if self.out_steps <= k:\n                return tf.stack([inputs[:, -self.out_steps:, j] for j in self.label_index], axis=2)\n            # cyclic repeat\n            cycle = self.out_steps \/\/ k + 1\n            cut = cycle*k - self.out_steps\n            tmp = tf.stack( [ inputs[:,:,j] for j in self.label_index ], axis=2)\n            results = tf.tile(tmp, [1, cycle, 1])\n            return results[:, cut:, :]\n        else: \n            raise ValueError(f'No method called {self.mode}')\n    \ndef compile_and_fit(model, window, max_epochs=30, patience=2, lr_rate=0.001, verbose=2):\n    \"\"\"\n    Compile the given model and fit it via given window. \n    \n    Inputs: \n        model: self-defined machine learning model\n        window: object defined by class WindowGenerator()\n        max_epochs: desired maximum epochs when fitting model to training data\n        patience: Stop fitting if val_loss doesn't decrease for n=patience epochs.\n        verbose: show training progress in different ways if 1 or 2. Turn off if 0.\n    \"\"\"\n    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n                                                      patience=patience, \n                                                      mode='min')\n    model.compile(loss=tf.losses.MeanSquaredError(),\n                  optimizer=tf.optimizers.Adam(lr_rate),\n                  metrics=[tf.metrics.RootMeanSquaredError(), tf.metrics.MeanAbsoluteError()])\n\n    history = model.fit(window.train, epochs=max_epochs,\n                        validation_data=window.val,\n                        callbacks=[early_stopping], verbose=verbose)\n    return history\n\ndef feature_importance(model, Xdata=None, batch_size=1, scale=0.5, \n                       verbose=False, method='const', alpha=0.05):\n    \"\"\"\n    Perturb on Xdata and detect feature importance. The larger the rmse\/effect is, the more \n    important the feature is.\n    Inputs:\n        model: that accepts 3-dim data (Xdata)\n        Xdata: 3-dim input excluding groundtruth. If None, create a random normal data automatically.\n        scale: float in [0,1]. Determine the stdandard deviation of the perturbation. (~N(0, sigma))\n        alpha: significance level\n    Returns: \n        list of rmse between original prediction and prediction after perturbation of each feature.\n    \"\"\"\n    def data_gen(batch, input_width, n_out):\n        while True:\n            x = np.random.rand(batch, input_width, n_out)  # batch x time x features\n            yield x\n    # generate base data\n    if Xdata is None: \n        g = data_gen(batch_size, model.input_shape[1], model.input_shape[-1]) \n        x = np.concatenate([next(g)[0] for _ in range(50)]) # Get a sample of data\n    else: \n        x = Xdata.copy()\n    # calculate perturbation effects\n    perturbation = None\n    if method == 'const':\n        perturbation = scale\n    elif method == 'random':\n        perturbation = np.random.normal(0.0, scale=scale, size=x.shape[:2])\n    orig_out = model.predict(x)\n    N = orig_out.size\n    collector = []\n    direction_collector = []\n    for i in range(model.input_shape[-1]):  # iterate over the three features\n        new_x = x.copy()\n        new_x[:, :, i] = new_x[:, :, i] + perturbation\n        perturbed_out = model.predict(new_x)\n        diff = perturbed_out - orig_out\n        \n        effect = ((diff) ** 2).mean() ** 0.5\n        collector.append(effect)\n        \n        pos, neg = (diff > 0).sum(), (diff < 0).sum()\n        if pos > N*(1-alpha): \n            symbol = '+'\n        elif neg > N*(1-alpha): \n            symbol = '-'\n        else: \n            symbol = ''\n        direction_collector.append( symbol )\n        \n        if verbose: \n            print('Variable {:>4}, perturbation effect: {:>8.4f}'.format(i+1, effect))\n    return collector, direction_collector\n\ndef feature_importance_plot(importance, direction=None, names=None, model_str='LSTM', figsize=(10,8)):\n    # https:\/\/www.analyseup.com\/learn-python-for-data-science\/python-random-forest-feature-importance-plot.html\n    \n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    if names is None: \n        names = np.arange(importance)+1\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n    \n    #Define size of bar plot\n    plt.figure(figsize=figsize)\n    #Plot Searborn bar chart\n    adjusted_posi = 0.2\n    g = sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    if direction is not None:\n        for index, (_, row) in enumerate(fi_df.iterrows()):\n            g.text(row.feature_importance, index+adjusted_posi,\n                   direction[index], color='black', ha=\"center\", size=15)\n    #Add chart labels\n    plt.title( model_str + ' Feature Importance')\n    plt.xlabel('Feature Importance (rmse)')\n    plt.ylabel('Feature Names')","353157e5":"class VARresults:\n    \n    def __init__(self, obj, test, mask=None, label_columns=None):\n        \"\"\"\n        obj: VARResultsWrapper from statsmodels package.\n        label_columns: list of strings as prediction targets.\n        mask: shape should be in line with test.\n        \"\"\"\n        assert 'VARResultsWrapper' in str(type(obj))\n        # test must be pd.DataFrame\/Series\n        self.model = obj\n        self.test = test \n        self.column_indices = {name: i for i, name in enumerate(test.columns)}\n        \n        self.label_columns = test.columns\n        if label_columns is not None:\n            self.label_columns = [label_columns] if type(label_columns) is str else label_columns\n        self.label_columns_indices = [self.column_indices[la] for la in self.label_columns]\n        \n        self.gt = self.test[self.label_columns].values[self.model.k_ar:]\n        self.mask = mask \n        if mask is not None: \n            assert mask.shape[0] == self.test.shape[0]\n            self.mask = np.array(mask)[self.model.k_ar:, self.label_columns_indices]\n        \n    def initialise(self):\n        \"\"\"Initialise gt and delete pred\"\"\"\n        self.gt = self.test[self.label_columns].values[self.model.k_ar:]\n        if hasattr(self, 'pred'):\n            del self.pred\n        \n    def durbin_watson_test(self, verbose=False):\n        res = statsmodels.stats.stattools.durbin_watson(self.model.resid)\n        for col, value in zip(self.model.names, res):\n            print( '{:32s}: {:>6.2f}'.format(col, value))\n        if verbose: \n            print( '\\n'.join([\n                'Durbin-Watson test-stat of autocorrelation of error terms:', \n                'd=0: perfect autocorrelation', \n                'd=2: no autocorrelation', \n                'd=4: perfect negative autocorrelation.' \n            ]))\n        return res\n    \n    def residac_plot(self, cols=None, figsize=(16, 8), ylim=(-.3, .3)):\n        \"\"\"\n        cols: can be integer\/str list.\n        \"\"\"\n        plt.figure(figsize=figsize)\n        # case pd.Series\n        if self.model.resid.ndim ==1:\n            ax = pd.plotting.autocorrelation_plot(self.model.resid)\n            ax.set(ylim=ylim)\n            plt.title(f'Residual Autocorrelation: {resid.name}')\n            plt.show()\n\n        # case pd.DataFrame         \n        assert cols is not None, \"cols is required since model dimension > 1.\"\n        if type(cols) is str: cols = [cols]\n        resid = self.model.resid\n        if type(resid) is np.ndarray: \n            resid = pd.DataFrame(resid, columns=self.endog.columns)\n        for count, (name, series) in enumerate(resid[cols].iteritems()):\n            ax = plt.subplot( len(cols)\/\/3 +1, 3, count+1)\n            ax.figure.tight_layout(pad=0.5)\n            ax.set(ylim=ylim)\n            pd.plotting.autocorrelation_plot(series, ax=ax)\n            plt.title(f'Residual Autocorrelation: {name}')\n        plt.tight_layout()\n        plt.show()\n    \n    def predict(self, steps=1, mode='normal', inplace=False):\n        \"\"\"\n        steps: n-steps-ahead prediction if steps=n.\n        mode: 'normal': predict for each sample.\n              'last': only predict the last sample. \n                      Eg, predict 14 days in a row from last sample.\n        Return if inplace=False: \n            2D-array (predictions, features).\n        \"\"\"\n        assert mode in ['normal', 'last'], 'Error: mode value not allowed'\n        varlag = self.model.k_ar\n        pred = []\n        \n        if mode == 'normal':\n            pred_collector = []\n            for s in range(self.test.shape[0] -varlag +1 -steps):\n                pred = self.model.forecast(y=self.test.values[s:s+varlag], steps=steps)\n                pred_collector.append(pred)\n            pred = np.stack(pred_collector)[:, :, self.label_columns_indices]\n            pred = pred[:,-1,:]\n        elif mode == 'last':\n            s = -steps -varlag\n            pred = self.model.forecast(y=self.test.values[s:s+varlag], steps=steps)\n            pred = pred[:, self.label_columns_indices]\n        if inplace: \n            self.mode = mode\n            self.steps = steps\n            self.pred = pred\n            return ;\n        return pred\n        \n    def inverse_transform(self, diff, history, start=None, inplace=False):\n        \"\"\"\n        Inverse transform in-built pred and test on features in the diff list \n        based on history and start and the mode you use in predict() function.\n        Applies to both 3D or 2D arrays.\n        Return: \n            pred, gt (if inplace=False).\n        \"\"\"\n        varlag = self.model.k_ar\n        pred = self.pred.copy()\n        gt = self.gt.copy()\n        if start is None: \n            start = -gt.shape[0] -1\n            \n        for i, col in enumerate(self.label_columns):\n            if col not in diff: \n                continue\n            if pred.ndim == 2:\n                if self.mode == 'normal':\n                    pred[:,i] = inverse_diff(history[col], pred[:,i], start=start)\n                elif self.mode == 'last':\n                    # -1: based on previous day: only works for I1 variable.\n                    pred[:,i] = pred[:,i].cumsum() + history[col][-self.steps-1] \n            elif pred.ndim ==3:\n                pred[:,:,i] = inverse_diff(history[col], pred[:,:,i], start=start)\n            gt[:,i] = inverse_diff(history[col], gt[:,i], start=start)\n\n        if inplace:\n            self.pred = pred\n            self.gt = gt\n            return ;\n        return pred, gt\n    \n    def evaluate(self, y_true=None, y_pred=None, multioutput='uniform_average', use_mask=True):\n        \"\"\"\n        Calculate rmse and mae. \n        mode: can be ['raw_values', 'uniform_average'].\n        use_mask: boolean or matrix. \n            Mask prediction and groudtruth when calculating performance.\n            It should be the same shape as y_true and y_pred.\n        Return: \n            rmse, mae\n        \"\"\"                 \n        if y_pred is None: \n            y_pred = self.pred.copy()\n        if y_true is None:\n            y_true = self.gt.copy()\n            if self.mode == 'normal':\n                y_true = y_true[self.steps-1:]\n            elif self.mode == 'last':\n                y_true = y_true[-self.steps:]\n        if use_mask is True: \n            mask = self.mask.copy()\n            if self.mode == 'normal':\n                mask = mask[self.steps-1:]\n            elif self.mode == 'last': \n                mask = mask[-self.steps:]\n        elif type(use_mask) is [pd.Series, pd.DataFrame, np.array, np.ndarray]:\n            mask = use_mask\n        else:\n            mask = None\n        return self._core_evaluate(y_true, y_pred, multioutput, mask)\n    \n    def _core_evaluate(self, y_true, y_pred, multioutput, mask=None):\n        \"\"\"inner private function. Shouldn't be called from outside by via member functions.\"\"\"\n        if mask is not None: \n            rmse, mae = [], []\n            for j in range(len(self.label_columns)):\n                rmsee = mean_squared_error(y_true[~mask[:,j], j], y_pred[~mask[:,j], j], squared=False)\n                maee = mean_absolute_error(y_true[~mask[:,j], j], y_pred[~mask[:,j], j])\n                rmse.append(rmsee)\n                mae.append(maee)\n            rmse = np.array(rmse)\n            mae = np.array(mae)\n            if multioutput == 'uniform_average':\n                rmse = rmse.mean()\n                mae = mae.mean()\n        else:\n            rmse = mean_squared_error(y_true, y_pred, multioutput=multioutput, squared=False)\n            mae = mean_absolute_error(y_true, y_pred, multioutput=multioutput)\n        return rmse, mae\n    \n    def inverse_transform_evaluate(self, diff, history, start=None, steps=1, \n                                   mode='normal', multioutput='uniform_average', use_mask=False):\n        \"\"\"\n        do predict(), inverse_transform(), and evaluate() all at once, but with \n        inplace=True setting, which is fixed and not adjustable.\n        Return: \n            rmse, mae (follow from evaluate())\n        \"\"\"\n        inplace = True\n        if inplace:\n            self.predict(steps, mode, inplace=inplace)\n            self.gt = self.test[self.label_columns].values[self.model.k_ar:] # initialise\n            self.inverse_transform(diff, history, start, inplace=inplace)\n        return self.evaluate(multioutput=multioutput, use_mask=use_mask)\n        \n    def plot(self, figsize=(20,15), use_mask=False):\n        \"\"\"\n        plot prediction(red line) and groundtruth(black point).\n        Inputs: \n            figsize: figure size.\n            use_mask: evaluate mae, rmse and plot the groundtruth with or without mask.\n        \"\"\"\n        rmse, mae = self.evaluate(multioutput='raw_values', use_mask=use_mask)\n        \n            \n        plt.figure(figsize=figsize)\n        for j in range(len(self.label_columns)):\n            ax = plt.subplot(len(self.label_columns), 1, j+1)\n            dates = test[self.label_columns[j]].index[self.model.k_ar:]\n            # groundtruth\n            if use_mask: \n                mask = self.mask.copy() if use_mask == True else use_mask\n                ax.scatter(dates[~mask[:,j]], self.gt[~mask[:,j], j], \n                           c='k', linestyle='-', alpha=0.7, marker='+', label='Groundtruth')\n            else: \n                ax.scatter(dates, self.gt[:, j], \n                           c='k', linestyle='-', alpha=0.7, marker='+', label='Groundtruth')\n            # prediction\n            if self.mode == 'normal':\n                ax.plot(dates[self.steps-1:], self.pred[:,j],\n                        c='red', linestyle='-', alpha=0.5, marker='+', label='Prediction')\n            elif self.mode == 'last':\n                ax.plot(dates[-self.steps:], self.pred[:,j],\n                       c='r', linestyle='-', alpha=0.5, marker='+', label='Prediction')\n            # title and x\/y-labels\n            ax.set(title='{:>32s}: RMSE={:6.4f}, MAE={:6.4f}'.format(targets[j], rmse[j], mae[j]), )\n            plt.gca().xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%y-%b'))\n            ax.legend()\n        ax.figure.tight_layout(pad=1)\n        plt.show()\n        \n    def irplot(self, impulse, response, periods=14, orth=False, method='asym', \n           repl=1000, alpha=0.05, seed=None, fontsize=16, supfontsize=24,\n           figsize=(17,30), max_col_wrap=3, rect=(0,0,1,0.96)):\n        \"\"\"\n        Plot Impulse Reponse of `impulse' on `response'.\n        Inputs:\n            impulse: a list of strings containing which features cause impulse(s) on response.\n            response: a list of strings containing which features response the shock.\n            periods: the number of periods in which a shock is observed in plots.\n            orth: bool. Plot Orthogonalised IR if True.\n            method: {'asym', 'efron', 'hall'}\n                'asym': use asymptotic stderr as error band.\n                'efron': standard percentile method via bootsrapping.\n                'hall': bias-corrected percentile method via bootstrapping.\n            repl: number of repetitions of bootstrapping.\n            alpha: significance level.\n            seed: seed for bootstrapping \n            fontsize: legend and subtitles font sizes.\n            supfontsize: super title font size. \n            figsize: size of whole figure.\n            max_col_wrp: desired max possible columns when plotting.\n                If impulse (columns) number < max_col_wrap, then plot upto impulse numbers.\n            rect: 4-tuple. tight_layout argument.\n            \n        Recommend setting for 5x5: max_col_wrap=5, figsize=(18,20) rect=(0,0,1,0.94~0.95)\n        \"\"\"\n        assert method in ['asym', 'efron', 'hall']\n        if type(impulse) is str: \n            impulse = [impulse]\n        if type(response) is str: \n            response = [response]\n        impulse_indices = [self.column_indices[x] for x in impulse]\n        response_indices = [self.column_indices[x] for x in response]\n        # fetch obj\n        irf = self.model.irf(periods, )\n        irfcoefs = irf.orth_irfs if orth else irf.irfs\n        \n        # case Asymptotic CI\n        if method == 'asym':\n            # shape: (periods, K, K)\n            q = scipy.stats.norm.ppf(1-alpha\/2)\n            lower = irfcoefs - q * irf.stderr(orth=orth)\n            upper = irfcoefs + q * irf.stderr(orth=orth)\n            \n        # case Efron's CI\n        elif method == 'efron':\n            lower, upper = self.model.irf_errband_mc(orth=orth, repl=repl, steps=periods, \n                           signif=alpha, seed=seed)\n        # case Hall's CI\n        elif method == 'hall':\n            # shape: (simulation, periods, K, K)\n            ma_coll = self.model.irf_resim(orth=orth, repl=repl, steps=periods, seed=seed)\n            low_idx = int(round(alpha \/ 2 * repl) - 1)\n            upp_idx = int(round((1 - alpha \/ 2) * repl) - 1)\n            \n            ma_diff = ma_coll - irfcoefs\n            ma_sort = np.sort(ma_diff, axis=0)  # sort to get quantiles\n            lower, upper = irfcoefs - ma_sort[upp_idx], irfcoefs - ma_sort[low_idx]\n        \n        # plot 0\n        signif = (1-alpha)*100\n        signif = int(signif) if signif.is_integer() else round(signif, 1)\n        m, n = len(response), len(impulse)\n        threshold = min(n, max_col_wrap)\n        x = np.arange(periods+1)\n        fig = plt.figure(figsize=figsize)\n        title = 'Forecast Error Impulse Response (FEIR)'\n        if orth: \n            title = 'Orthogonalised Impulse Response (OIR)'\n        plt.suptitle(title, fontsize=supfontsize,)\n        for i, residx in enumerate(response_indices):\n            for j, impidx in enumerate(impulse_indices):\n                ax = plt.subplot( math.ceil((m*n)\/threshold), threshold, i*n + j+1)\n\n                ax.plot( irfcoefs[:, residx, impidx], c='b', label='Response')\n                ax.plot( lower[:, residx, impidx], c='r', linestyle='-', alpha=1,\n                         label='Upper {:.0%} {:s} CI'.format(1-alpha, method.capitalize()))\n                ax.plot( upper[:, residx, impidx], c='r', linestyle='-', alpha=1,\n                         label='Lower {:.0%} {:s} CI'.format(1-alpha, method.capitalize()))\n                ax.axhline(c='k', )#linewidth=1) #hlines\n                ax.fill_between(x, lower[:, residx, impidx], upper[:, residx, impidx], color='b', \n                           alpha=.1, label='{:.0%} {:s} CI'.format(1-alpha, method.capitalize()) )\n                \n                # ax.set_title(f'{response[i]} response to {impulse[j]}', fontsize=fontsize)\n                ax.set_title(f'{impulse[j]} shock to\\n {response[i]}', fontsize=fontsize)\n        handles, labels = ax.get_legend_handles_labels()\n        fig.legend(handles, labels, loc='upper right', fontsize=fontsize)\n        plt.tight_layout(pad=0.5, rect=rect) # (left, bottom, right, top)\n        plt.show()\n\n    def fevd_plot(self, periods=5, top=3, figsize=(14,8), max_str_split=2):\n        \"\"\"\n        Calculate Forecast Error Variance Decomposition of each target features\n        and plot it.\n        Inputs: \n            periods: desired lags to draw on the figure\n            top: desired top k components will be selected and shown in each lag.\n            figsize: figure size.\n        \"\"\"\n        # initialization\n        res = self.model.fevd(periods)\n        idx2col = np.array(list( self.column_indices.items() ))\n        idx2col.sort(axis=-1)\n        idx2col = idx2col[:, -1]\n        # results\n        targets_fevd = res.decomp[self.label_columns_indices, :, :] # (target y, timestep, components x)\n        topidx = np.argsort(targets_fevd, axis=-1)[:, :, -top:]\n        topidx = topidx[:, :, ::-1]\n        data = np.take_along_axis(targets_fevd, topidx, axis=-1) * 100\n        names = idx2col[ topidx ]\n        \n        index = np.arange(periods) + 0.3\n        bar_width = 0.4\n        columns = ['lag %i' %x for x in np.arange(periods)]\n        rows = ['top %i' %(i+1) for i in range(top)]\n        colors = plt.cm.BuPu(np.linspace(0, periods*0.1, top))\n        \n        for i in range(len(self.label_columns)):\n            # initialization\n            y_offset = np.zeros(periods)\n            cell_text = []\n            plt.figure(figsize=figsize)\n            # main\n            for f in range(top):\n                plt.bar(index, data[i, :, f], bar_width, bottom=y_offset, color=colors[f])\n                y_offset = y_offset + data[i, :, f]\n                cell_text.append( list(\n                    '\\n_'.join(re.split('_(?!to)', x, maxsplit=max_str_split)) for x in names[i, :, f]\n                ) )\n            # table\n            the_table = plt.table(cellText=cell_text, rowLabels=rows, rowColours=colors,\n                              colLabels=columns, cellLoc='center',\n                              loc='bottom', bbox=[0,-0.8,1,0.8])\n            the_table.auto_set_font_size(False)\n            the_table.set_fontsize(12)\n            the_table.scale(1,4)\n            # adjust layout\n            plt.subplots_adjust(left=0.0, bottom=0.2)\n            plt.ylabel(\"Percentage(%)\")\n            values = np.arange(0, 110, 10)\n            plt.yticks(values)\n            plt.xticks([])\n            plt.title('Forecast Erro Variance Decomposition\\nfor {}'.format(self.label_columns[i]))\n            plt.show()\n    \ndef var_error_I2(test, var_fit, targets, steps=1, I1=[], I2=[]):\n    print('VAR: Number of prediction steps: ', steps)\n    for i in range(0, len(targets)):\n        varobj = VARresults(var_fit, test, label_columns=targets[i])\n        beginning = varlag - 1 + steps\n        end = test.shape[0]\n        y = test[targets[i]][beginning:end]\n        length = y.shape[0]\n        if targets[i] in I2:\n            y      = np.cumsum(y)  # first accumulation\n            y      = y + y[0]      # first term correction\n            y      = y.cumsum()    # second accumulation\n            y      = y + y[0]      # second term correction\n            yhat   = np.cumsum(varobj.predict(steps=steps, inplace=False))  # [:, steps-1])\n            yhat   = yhat + yhat[0]  # first term correction\n            yhat   = np.cumsum(yhat)\n            yhat   = yhat + yhat[0]  # second term correction\n            rmse   = mean_squared_error(y, yhat)**0.5\n            mae    = mean_absolute_error(y, yhat)\n            print('{:<28s}: RMSE={:>7.4f}; MAE={:>7.4f}'.format(targets[i], rmse, mae))\n        elif targets[i] in I1 and targets[i] not in I2:\n            absolute_change = y.abs().sum()\n            y      = y.cumsum()\n            y      = y + y[0]        # term correction\n            yhat   = np.cumsum(varobj.predict(steps=steps, inplace=False))  # [:, steps-1])\n            yhat   = yhat + yhat[0]  # term correction\n            rmse   = mean_squared_error(y, yhat)**0.5\n            mae    = mean_absolute_error(y, yhat)\n            print('{:<28s}: RMSE={:>7.4f}; MAE={:>7.4f}'.format(targets[i], rmse, mae))\n        else:\n            yhat   = varobj.predict(steps=steps, inplace=False)  # [:, steps - 1]\n            rmse   = mean_squared_error(y, yhat)**0.5\n            mae    = mean_absolute_error(y, yhat)\n            absolute_change = y.diff().abs().sum()\n            print('{:<28s}: RMSE={:>7.4f}; MAE={:>7.4f}'.format(targets[i], rmse, mae))","daf373e6":"class VECM:\n    \"\"\"\n    det: {\"nc\", \"co\", \"ci\", \"lo\", \"li\"}\n        \"nc\"- no deterministic terms\n        \"co\"- constant outside the cointegration relation\n        \"ci\"- constant within the cointegration relation\n        \"lo\"- linear trend outside the cointegration relation\n        \"li\"- linear trend within the cointegration relation\n    \"\"\"\n    # shared functions\n    irplot = VARresults.irplot # only method 'asym' can work for VECM model.\n    durbin_watson_test =  VARresults.durbin_watson_test\n    residac_plot = VARresults.residac_plot\n    _core_evaluate = VARresults._core_evaluate\n    \n    def __init__(self, endog, test, det=None, mask=None, label_columns=None):\n        \"\"\"\n        endog: training dataset.\n        test: testing dataset.\n        det: please see docstring of build().\n        label_columns: list of strings as prediction targets.\n        mask: shape should be in line with test.\n        \"\"\"\n        # endog, test must be pd.DataFrame\n        self.endog = endog\n        self.test = test\n        # initialization \n        self.lag = np.nan\n        self.coint_rank = np.nan\n        self.det = det\n        if self.det is None: \n            self.det = 'co'\n        \n        self.column_indices = {name: i for i, name in enumerate(test.columns)}\n        self.label_columns = test.columns\n        if label_columns is not None:\n            self.label_columns = [label_columns] if type(label_columns) is str else label_columns\n        self.label_columns_indices = [self.column_indices[la] for la in self.label_columns]\n        \n        # halfway initialization\n        self.gt = self.test[self.label_columns].values\n        self.mask = np.array(mask)\n        \n    def __repr__(self):\n        return '\\n'.join([\n            f'Endogenous data shape: {self.endog.shape}', \n            f'Selected lag order: {self.lag}', \n            f'Cointegration rank: {self.coint_rank}', \n            f'Deterministic term: {self.det}'\n        ])\n    \n    def select_lag(self, maxlags=15, det=None, ic=None, \n                   verbose=False, inplace=False):\n        \"\"\"\n        Inputs:\n            det: {\"nc\", \"co\", \"ci\", \"lo\", \"li\"}\n                \"nc\"- no deterministic terms\n                \"co\"- constant outside the cointegration relation\n                \"ci\"- constant within the cointegration relation\n                \"lo\"- linear trend outside the cointegration relation\n                \"li\"- linear trend within the cointegration relation\n            ic: {\"aic\", \"bic\", \"hqic\", \"fpe\"}\n                the desired information criteria to use.\n            inplace: bool\n                store lag inside if True.\n        Returns: \n            lag: scalar\n        \"\"\"\n        if det is None: \n            det = self.det\n        res = vecm.select_order(self.endog, maxlags=maxlags, deterministic=det)\n        \n        if verbose: \n            res.summary()\n            \n        if ic is None:\n            return res.selected_orders\n        lag = res.selected_orders[ic.lower()]\n        if inplace==True: \n            self.lag = lag\n        return lag\n    \n    def select_coint_rank(self, det_order=0, diff_lag=None, method='trace', alpha=0.05, \n                          verbose=False, inplace=False):  \n        # method is trace of maxeig\n        \"\"\"\n        Perform Cointegration Johansen test based on trace\/maxeig to select coint_rank.\n        Inputs: \n            det_order : int\n                * -1 - no deterministic terms\n                * 0 - constant term\n                * 1 - linear trend\n            diff_lag: number of lagged difference in model.\n            method: can be trace or maxeig test method.\n            alpha: significance level of the test.\n        Return:\n            scalar cointegration rank\n        \"\"\"\n        if diff_lag is None: \n            diff_lag = self.lag\n        coint_obj = vecm.select_coint_rank(self.endog, det_order=det_order, \n            k_ar_diff=diff_lag, method=method, signif=alpha)\n        \n        if verbose: \n            coint_obj.summary()\n        if inplace == True: \n            self.coint_rank = coint_obj.rank\n        return coint_obj.rank\n    \n    def build(self, diff_lag=None, coint_rank=None, det=None, **kwargs):\n        \"\"\"\n        Build inplace VECM model. Update parameters if arg(s) is provided.\n        Inputs:\n            diff_lag: number of lagged difference in model.\n            det : {\"nc\", \"co\", \"ci\", \"lo\", \"li\"}\n                \"nc\"- no deterministic terms\n                \"co\"- constant outside the cointegration relation\n                \"ci\"- constant within the cointegration relation\n                \"lo\"- linear trend outside the cointegration relation\n                \"li\"- linear trend within the cointegration relation\n        \"\"\"\n        if diff_lag is None: \n            diff_lag = self.lag \n        if coint_rank is None: \n            coint_rank = self.coint_rank\n        if det is None: \n            det = self.det\n        self.model = vecm.VECM(self.endog, k_ar_diff=diff_lag, coint_rank=coint_rank, \n                               deterministic=det, **kwargs\n                              ).fit()\n        self.diff_lag = diff_lag\n        self.coint_rank = self.coint_rank\n        self.det = det\n        \n    def predict(self, steps, inplace=False):\n        \"\"\"\n        steps: n-steps-ahead prediction if steps=n. \n        Only 'last' mode is available.\n        Return if inplace=False: \n            2D-array (predictions, features).\n        \"\"\"\n        pred = self.model.predict(steps)[:, self.label_columns_indices]\n        if inplace:\n            self.steps = steps\n            self.pred = pred \n            return;\n        return pred\n\n    def evaluate(self, y_true=None, y_pred=None, multioutput='uniform_average', use_mask=False):\n        \"\"\"\n        Calculate rmse and mae. \n        mode: can be ['raw_values', 'uniform_average'].\n        use_mask: boolean or matrix. \n            Mask prediction and groudtruth when calculating performance.\n            If true, use in-stored mask default to mask[self.steps:, self.label_columns].\n            It should be the same shape as y_true and y_pred.\n        Return: \n            rmse, mae\n        \"\"\" \n        if y_pred is None: \n            y_pred = self.pred.copy()\n        if y_true is None: \n            y_true = self.gt.copy()\n            y_true = y_true[:steps]\n        \n        if type(use_mask) in [pd.Series, pd.DataFrame]:\n            mask = use_mask.values\n        elif type(use_mask) in [np.array, np.ndarray]:\n            mask = use_mask\n        elif use_mask is True: \n            mask = self.mask.copy()\n            mask = mask[:y_pred.shape[0], self.label_columns_indices ]\n        else:\n            mask = None\n        return self._core_evaluate(y_true, y_pred, multioutput, mask)\n        \n    def coint_matrix(self, give='beta', prec=2):\n        \"\"\"\n        give: {'beta', 't_stats_beta'}\n              The desired target to be returned.\n        \"\"\"\n        if give=='beta': \n            values = self.model.beta\n        elif give == 't_stats_beta':\n            values = self.model.tvalues_beta\n        values = np.round(values, prec)\n        res = pd.DataFrame(values, columns=self.endog.columns , index=self.endog.columns)\n        return res\n        \n# outside class\ndef vecm_error_summary(data, nobs, k_ar_diff=1, coint_rank=21, deterministic='ci', \n                       targets=None):\n    if targets is None: \n        targets = data.filter(regex='Depth_*').columns\n    train_data, test_data = data[0:-nobs], data[-nobs:]\n    train_data.index = pd.DatetimeIndex(train_data.index).to_period('D')\n    test_data.index = pd.DatetimeIndex(test_data.index).to_period('D')\n    vecm_model = vecm.VECM(train_data, k_ar_diff=k_ar_diff, coint_rank=coint_rank, \n                           deterministic=deterministic)\n    vecm_fit = vecm_model.fit()\n    # Check if residuals are autocorrelated\n    out = statsmodels.stats.stattools.durbin_watson(vecm_fit.resid)\n    print( '\\n'.join([\n        'Durbin-Watson test-stat of autocorrelation of error terms:', \n        'd=0: perfect autocorrelation', \n        'd=2: no autocorrelation', \n        'd=4: perfect negative autocorrelation.' \n    ]))\n    for col, value in zip(train_data.columns, out):\n        print((col), ':', round(value, 2))\n\n    forecast = vecm_fit.predict(nobs)\n    forecast = pd.DataFrame(forecast, index=test_data.index, columns=test_data.columns)\n    print('\\n\\nRMSE and MAE of predicted target variables')\n    for col in targets:  # train_data.columns:\n        rmse = mean_squared_error(forecast[col], test_data[col],\n                                  squared=False)\n        mae = mean_absolute_error(forecast[col], test_data[col])\n        print('{}:\\nRMSE = {:6.4f} | MAE = {:6.4f}'.format(col, rmse, mae))\n\ndef vecm_coint_matrix(data, give='beta', k_ar_diff=1, coint_rank=21, deterministic='ci'):\n    vecm_model = VECM(data, k_ar_diff=k_ar_diff, coint_rank=coint_rank, deterministic=deterministic)\n    vecm_fit = vecm_model.fit()\n    if give == 'beta':\n        values = vecm_fit.beta\n    elif give == 't_stats_beta':\n        values = vecm_fit.tvalues_beta\n    values = np.around(values, 2)\n    res = pd.DataFrame(values, index=data.columns)\n    return res\n\ndef vecm_rmse_mae(train, test, steps=1, vecm_order=1, vecm_coint=1):\n    test_len = test.shape[0] - steps\n    print('Prediction horizon:', steps)\n    vecm_order = vecm_order\n    for i in targets_indices:\n        yhat = np.array([])\n        y    = test.iloc[steps:(test_len+steps), i]\n        for t in range(0, test_len):\n            vecmobj  = vecm.VECM(endog=train.append(test.iloc[0:t, ]), k_ar_diff=vecm_order, \n                                 coint_rank=vecm_coint, deterministic=\"ci\")\n            vecm_fit = vecmobj.fit()\n            yhat     = np.append(yhat, vecm_fit.predict(steps=steps)[steps-1, i])\n        if test.columns[i] in I2:\n            yhat = np.cumsum(yhat)\n            y    = np.cumsum(y)\n        else:\n            pass\n        rmse = mean_squared_error(y, yhat)**0.5\n        mae  = mean_absolute_error(y, yhat)\n        print(test.columns[i], ' : RMSE = ', round(rmse, 4), '; MAE = ', round(mae, 4))","005cfcb7":"# parameters setting\ndataset = 'Aquifer_Auser.csv'\ntargets = ['Depth_to_Groundwater_SAL', 'Depth_to_Groundwater_CoS', 'Depth_to_Groundwater_LT2'] # orders matter!\n\n# read dataset as DataFrame\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'], date_parser=dateparse)\n\n# re-order features\norderlist = ['Rain.*', 'Temp.*', 'Volume.*', 'Hydrom.*', \n             *targets, 'Depth_to_Groundwater_PAG', 'Depth_to_Groundwater_DIEC']\ncollector = []\nfor regex in orderlist: \n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\ndf.index.freq = 'd'\ndf.sort_index(inplace=True)\n\n# Set weird values to nan\n## set nan to Groundwater whose values are 0\ndf[df.filter(regex='Depth_.*').columns] = df.filter(regex='Depth_.*').replace(0, np.nan)\n## set nan to Volume whose values are 0\ndf[df.filter(regex='Volume_.*').columns] = df.filter(regex='Volume_.*').replace(0, np.nan)\n## set nan to Temperature whose values are 0\ndf[df.filter(regex='Temperature_.*').columns] = df.filter(regex='Temperature_.*').replace(0, np.nan)\n\n# visual graph of nan locations for each field\nmissingval_plot(df, show=False)\nplt.axvline(x = 5780, c='r') # 2014-01-01\nplt.show()","d4f31356":"# Dealing with missing values\n## drop old\/unuseful data\ntodrop = df[df.Volume_CSA.isna()].index.max()\ndf = df[df.index > todrop].copy() # remaining data starts from 2014-01-01\n\n## drop columns (to be further confirmed)\n# df.drop(columns=['Temperature_Ponte_a_Moriano', 'Hydrometry_Piaggione'], inplace=True)\n","f2b0aca9":"# correlation plot\nfeature_plots(df)\ncorrtri_plot(df)","b3eae92d":"# Feature engineering & imputation\ndfimputed = df.copy()\n# Record nan positions before imputation\nmask = df.isna()\n\n## drop this one with too many missing\ndfimputed.drop(columns='Temperature_Ponte_a_Moriano', inplace=True)\n\n## Engineer 1\n# dfimputed['Volume_gp1'] = dfimputed[['Volume_POL', 'Volume_CC1', 'Volume_CC2']].mean(axis=1)\n# dfimputed['Volume_gp2'] = dfimputed[['Volume_CSA', 'Volume_CSAL']].mean(axis=1)\n\n## imputation\ndfimputed = fillna(dfimputed, 'interpolate', method='linear').copy()\n## Drop na again for nonimputed observations.\ntodrop = dfimputed[dfimputed['Depth_to_Groundwater_CoS'].isna()].index.max()\ndfimputed = dfimputed[dfimputed.index > todrop].copy()\nassert dfimputed.isna().any().any() == False\n\n## Engineer 2: rolling\n# tmp = dfimputed.filter(regex='Rain.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('sum')\n# tmp = dfimputed.filter(regex='Vol.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# tmp = dfimputed.filter(regex='Temp.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# tmp = dfimputed.filter(regex='Hydro.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# dfimputed.dropna(inplace=True)\n\n\n## pca \ntmp = ['Rainfall_Tereglio_Coreglia_Antelminelli', 'Rainfall_Gallicano', \n       'Rainfall_Pontetetto']\ntmp.extend(dfimputed.filter(regex='Rain.*').columns.drop(tmp).tolist())\npca_plot(dfimputed[tmp], figsize=(5,2))\n\npca_plot(dfimputed[['Temperature_Monte_Serra', 'Temperature_Orentano',\n                    'Temperature_Lucca_Orto_Botanico']], figsize=(5,2))\ntmp = ['Volume_POL', 'Volume_CC1', \n       'Volume_CSA', 'Volume_CC2', 'Volume_CSAL']\npca_plot(dfimputed[tmp], figsize=(5,2))\n\npca_plot(dfimputed.filter(regex='Hydro.*'), figsize=(5,2))\n\npca_plot(dfimputed.filter(regex='Depth.*'), figsize=(5,2))\n\n## Dorp due to pca\ntmp = ['Rainfall_Tereglio_Coreglia_Antelminelli', 'Rainfall_Pontetetto', \n      'Rainfall_Monte_Serra']\ndfimputed.drop(columns=dfimputed.filter(regex='Rain.*').columns.drop(tmp), inplace=True)\ndfimputed.drop(columns=['Temperature_Orentano', 'Temperature_Lucca_Orto_Botanico'], inplace=True)\ntmp = ['Volume_POL', 'Volume_CC1', 'Volume_CSA']\ndfimputed.drop(columns=dfimputed.filter(regex='Vol.*').columns.drop(tmp), inplace=True)\ndfimputed.drop(columns='Hydrometry_Piaggione', inplace=True)\ndfimputed.drop(columns='Depth_to_Groundwater_DIEC', inplace=True)\n\n## Reorder \ncollector = []\nfor regex in orderlist: # in Sec 2.1.1\n    collector.append(dfimputed.filter(regex=regex).columns)\ndfimputed = dfimputed.reindex(columns= np.concatenate(collector) )\n\n\n## so does mask\nmask.drop(columns='Temperature_Ponte_a_Moriano')\nmask = mask[mask.index > todrop].copy()\n# mask['Volume_gp2'] = mask['Volume_gp1'] = False\n# mask = mask[29:]\nfor col in df.columns.drop(dfimputed.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True)\nassert mask.shape == dfimputed.shape, f'mask shape {mask.shape} doesn\\'t match dfimputed shape {dfimputed.shape}'\nmask = mask.reindex(columns=dfimputed.columns)","f50e6c7a":"# Chekc stationarity\n## adftest\nadftable = adfuller_table(dfimputed, verbose=False, alpha=0.05, maxlag=15, regression='ct')\n# display(adftable)\n\n## collect non-stationary features\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\nprint('Differenced features: ', I1)","2985cdcb":"## (Cont'd) take diff() on cols that do not pass the adftest. Here we adopt AIC.\ndfstationary = dfimputed.copy()\ndfstationary.loc[:,I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)\n\n## adftest again\nadftable = adfuller_table(dfstationary, verbose=False, maxlag=15, regression='ct')\n# display(adftable) # This time all features pass ADF test in both AIC and BIC criteria.","46cbd9da":"# Check granger causality\ngrtable = grangers_causation_table(dfstationary, xnames=dfimputed.columns.drop(labels=targets), \n                          ynames=targets, maxlag=14 ,alpha=0.05)\ndisplay(grtable)\n\n## Manually drop non Granger-cause variables\n# tmp = ['Volume_gp1', 'Volume_gp2']\n# dfstationary.drop(columns=tmp, inplace=True)\n# mask.drop(columns=tmp, inplace=True)","89248507":"# Deal with Time Periodicity \nyear = 365.2425\ndfstationary['year_sin'] = np.sin(dfstationary.index.dayofyear * 2 * np.pi \/ year)\ndfstationary['year_cos'] = np.cos(dfstationary.index.dayofyear * 2 * np.pi \/ year)\nmask['year_sin'] = False\nmask['year_cos'] = False","40f0d9bf":"# Split data\nN = dfstationary.shape[0]\ntrain, val, test = dfstationary[:int(0.8*N)], dfstationary[int(0.8*N):int(0.9*N)], dfstationary[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in targets]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","6af1c96c":"# Get sense: ac & pac plots\nacpac_plot(dfstationary[targets], figsize=(13, 2))","dbe4d114":"# VAR model \nvar_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=33, trend='c')\nprint(res.selected_orders)\nvarlag = res.selected_orders['aic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test, mask=tmask[1:] , label_columns=targets) #[1:]\nprint('The model we are using: {1:}-dim VAR({0:}).'.format(*varobj.model.coefs.shape[:2]))","d2451ab0":"# Durbin_watson test of residuals\nprint('Durbin-Watson test:')\n_ = varobj.durbin_watson_test()","ac370d76":"# Adjusted Portmannteau-test & S-K test\ndisplay(var_model.test_whiteness(nlags=varlag*5, signif=0.05, adjusted=True).summary())\ndisplay(varobj.model.test_normality().summary())\n# Residual Autocorrelations\nvarobj.residac_plot(cols=targets, figsize=(15,3))","ce159e54":"# 1-step-ahead Prediction\n## paramters setting:\nsteps=1\nmode = 'normal'\nuse_mask = True\nmultioutput = 'raw_values' \n\n## after trransformation\nrmse, mae = varobj.inverse_transform_evaluate(\n    I1, dfimputed, steps=steps, mode=mode, multioutput=multioutput, use_mask=use_mask)\nprint('uniform rmse : {:8.4f}'.format(rmse.mean()))\nprint('uniform mae: {:8.4f}'.format(mae.mean()))\n\n## plot\nvarobj.plot(use_mask=True, figsize=(20, 10))","130b4167":"# Last Prediction\n## paramters setting:\n\nsteps=[7, 14]\nmode = 'last'\nuse_mask = True\nmultioutput = 'raw_values' \n\nfor step in steps: \n    ## prediction\n    rmse, mae = varobj.inverse_transform_evaluate(I1, dfimputed,steps=step, mode=mode, \n                                                  multioutput=multioutput, use_mask=use_mask)\n    print('uniform rmse with {:>} step: {:8.4f}'.format(step, rmse.mean() ))\n    print('uniform mae with {:>} step: {:8.4f}'.format(step, mae.mean() ))\n    ## plot\n    varobj.plot(use_mask=use_mask, figsize=(20, 10))","b20ddd30":"# OIR Plot Analysis\nprint('OIR for {}'.format(targets[0]))\nvarobj.irplot(impulse=train.columns, periods=30, response=targets[0], orth=True,\n             figsize=(17,15), method='hall')","eded395b":"# OIR Plot Analysis\nprint('OIR for {}'.format(targets[1]))\nvarobj.irplot(impulse=train.columns, periods=30, response=targets[1], orth=True,\n             figsize=(17,15), method='hall')","9e23cb8b":"# OIR Plot Analysis\nprint('OIR for {}'.format(targets[2]))\nvarobj.irplot(impulse=train.columns, periods=15, response=targets[2], orth=True,\n             figsize=(17,15), method='hall')","b620bfba":"# FEVD Plot Analysis \nvarobj.fevd_plot(periods=11, top=5, figsize=(14, 9), max_str_split=3) \n# put smaller periods or tune-up max_str_split(max=3) if texts overlap.","75eb8d61":"Image(\"..\/input\/aceawaterimages\/auser1.png\")","c2f41bfe":"Image(\"..\/input\/aceawaterimages\/auser2.png\")","49f17279":"# Feature engineering & imputation\ndfimputed = df.copy()\n# Record nan positions before imputation\nmask = df.isna()\n\n## drop this one with too many missing\ndfimputed.drop(columns='Temperature_Ponte_a_Moriano', inplace=True)\n\n## Engineer 1\n# dfimputed['Volume_gp1'] = dfimputed[['Volume_POL', 'Volume_CC1', 'Volume_CC2']].mean(axis=1)\n# dfimputed['Volume_gp2'] = dfimputed[['Volume_CSA', 'Volume_CSAL']].mean(axis=1)\n\n## imputation\ndfimputed = fillna(dfimputed, 'interpolate', method='linear').copy()\n## Drop na again for nonimputed observations.\ntodrop = dfimputed[dfimputed['Depth_to_Groundwater_CoS'].isna()].index.max()\ndfimputed = dfimputed[dfimputed.index > todrop].copy()\nassert dfimputed.isna().any().any() == False\n\n## Engineer 2: rolling\n# tmp = dfimputed.filter(regex='Rain.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('sum')\n# tmp = dfimputed.filter(regex='Vol.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# tmp = dfimputed.filter(regex='Temp.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# tmp = dfimputed.filter(regex='Hydro.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# dfimputed.dropna(inplace=True)\n\n## pca \n# tmp = ['Rainfall_Tereglio_Coreglia_Antelminelli', 'Rainfall_Gallicano', \n#        'Rainfall_Pontetetto']\n# tmp.extend(dfimputed.filter(regex='Rain.*').columns.drop(tmp).tolist())\n# pca_plot(dfimputed[tmp], figsize=(5,2))\n# pca_plot(dfimputed[['Temperature_Monte_Serra', 'Temperature_Orentano',\n#                     'Temperature_Lucca_Orto_Botanico']], figsize=(5,2))\n# tmp = ['Volume_POL', 'Volume_CC1', \n#        'Volume_CSA', 'Volume_CC2', 'Volume_CSAL']\n# pca_plot(dfimputed[tmp], figsize=(5,2))\n# pca_plot(dfimputed.filter(regex='Hydro.*'), figsize=(5,2))\n# pca_plot(dfimputed.filter(regex='Depth.*'), figsize=(5,2))\n\n## Dorp due to pca\ntmp = ['Rainfall_Tereglio_Coreglia_Antelminelli', 'Rainfall_Pontetetto', \n      'Rainfall_Monte_Serra']\ndfimputed.drop(columns=dfimputed.filter(regex='Rain.*').columns.drop(tmp), inplace=True)\ndfimputed.drop(columns=['Temperature_Orentano', 'Temperature_Lucca_Orto_Botanico'], inplace=True)\ntmp = ['Volume_POL', 'Volume_CC1', 'Volume_CSA']\ndfimputed.drop(columns=dfimputed.filter(regex='Vol.*').columns.drop(tmp), inplace=True)\ndfimputed.drop(columns='Hydrometry_Piaggione', inplace=True)\ndfimputed.drop(columns='Depth_to_Groundwater_DIEC', inplace=True)\n\n## Reorder \ncollector = []\nfor regex in orderlist: # in Sec 2.1.1\n    collector.append(dfimputed.filter(regex=regex).columns)\ndfimputed = dfimputed.reindex(columns= np.concatenate(collector) )\n\n\n## so does mask\nmask.drop(columns='Temperature_Ponte_a_Moriano')\nmask = mask[mask.index > todrop].copy()\n# mask['Volume_gp2'] = mask['Volume_gp1'] = False\n# mask = mask[29:]\nfor col in df.columns.drop(dfimputed.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True)\nassert mask.shape == dfimputed.shape, f'mask shape {mask.shape} doesn\\'t match dfimputed shape {dfimputed.shape}'\nmask = mask.reindex(columns=dfimputed.columns)","b3a352e4":"# Split data\nN = dfimputed.shape[0]\ntrain, val, test = dfimputed[:int(0.8*N)], dfimputed[int(0.8*N):int(0.9*N)], dfimputed[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in [targets[0]]]\n\n# Scaling and Normalization \n# normalization won't make sense if the TS is trending upward\/downward. This is why stationary first.\nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","358708e2":"# Initialize data loader: 1-step-ahead prediction\n## parameters setting\ninput_width = varlag     # suggested by VAR\nlabel_width = 1       \nshift = 1\nbatch_size = 4\n\n## generate window\nwindow = WindowGenerator(input_width, label_width, shift, train_scaled, val_scaled, test_scaled, \n                         batch_size=batch_size, shuffle=False, mask=tmask, \n                         label_columns=targets)\nprint(window)\nprint('Inputs\/labels shapes of window: {} {}'.format(\n    window.example[0].shape, window.example[1].shape))","fb3916fb":"# 1-step-ahead LSTM prediction\n\nlstm_model = None\nif os.path.isfile(os.path.join(modelpath, 'auser.keras')):\n    lstm_model = tf.keras.models.load_model( os.path.join(modelpath, 'auser.keras'))\nelse: \n    ## Parameters setting:\n    MAX_EPOCHS = 100\n    patience = 10        # stop training if loss increase for 'patience' periods in total.\n    lr_rate = 1e-3\n    train_verbose = 0    # 0: no output, 1: progress, 2: report\n    lstm_kernel = 16\n    stateful = False     # batchsize must be 1 if True. (takes 10 mins)\n    dropout = 0.2        # dropout rate in [0,1]\n    \n    ## fixed setting\n    n_features = train_scaled.shape[1]\n    n_out = len(targets)\n    OUT_STEPS = window.label_width\n    ## RNN: LSTM\n    lstm_model = tf.keras.Sequential([\n        # Shape [batch, time, features] => [batch, lstm_units]\n        tf.keras.layers.LSTM(lstm_kernel, return_sequences=True, stateful=stateful),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.LSTM(int(lstm_kernel\/2), return_sequences=False, stateful=stateful),\n        # Shape => [batch, out_steps*features]\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(OUT_STEPS*n_out),\n        # Shape => [batch, out_steps, features]\n        tf.keras.layers.Reshape([OUT_STEPS, n_out])\n    ])\n    ## compile and fit\n    print('window RNN')\n    print('Xtrain shape: ', window.example[0].shape, 'ytrain shape: ', window.example[1].shape, end='')\n    print('\\toutput shape: ', lstm_model(window.example[0]).shape, end='\\n\\n')\n    history = compile_and_fit(lstm_model, window, MAX_EPOCHS, patience, lr_rate, verbose=train_verbose)\n    ## save model\n    lstm_model.save(os.path.join(modelpath, 'auser.keras'), overwrite=False)\n    \n## evaluate on test\nprint('evaluation before scaling back:')\nlstm_model.evaluate(window.test, verbose=2, return_dict=True)\n#window.plot(lstm_model, use_mask=True)\n\n## inverse_transform_evaluate on test\ndi = window.inverse_transform_evaluate(lstm_model, scaler, which='test', return_dict=True)\nprint('evaluation after scaling back: {0}: {2:6.4f} - {1}: {3:6.4f}'.format(*di.keys(), *di.values()))\nwindow.plot(use_mask=False, figsize=(20, 15))","100bd426":"# LSTM feature importance plot\nimportance, direction = feature_importance(lstm_model, window.X_test, scale=0.2, method='const')\nfeature_importance_plot(importance, names=train_scaled.columns) ","6ca1b3ae":"# parameters setting\ndataset = 'Aquifer_Doganella.csv'\ntargets = []\nfor i in range(9):\n    targets.append('Depth_to_Groundwater_Pozzo_%i' %(i+1))\n\n# read dataset as DataFrame\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'], date_parser=dateparse)\ndf.rename(columns={'Volume_Pozzo_5+6':'Volume_Pozzo_5_6'}, inplace=True)\n\n# re-order features\norderlist = ['Rain.*', 'Temp.*', 'Volume.*', *targets]\ncollector = []\nfor regex in orderlist: \n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\ndf.index.freq = 'd'\ndf.sort_index(inplace=True)\ndf['year'] = df.index.year\n\n# set weird values to nan\ndf[df.filter(regex='Depth_.*').columns] = df.filter(regex='Depth_.*').replace(0, np.nan)\n\n# record missing positions\nmaskdf = df.isna()\n\n# missing plot\nmissingval_plot(df, show=False)\nplt.axvline(x = 4663, c='r') # 2016-10-07\nplt.show()\n\n## Rainfall: missing in 2015-2018\n## Temperature: missing in 2015-2018\n## Volume: past missing till 2016-10-07\n## Groundewater: past missing till 2015-06-01","122d3cff":"# imputation I\n\n## for Rainfall & Temperature\ncols = df.filter(regex='Rainfall.*').columns\n### summarize missing values by year \n# display(df[cols].isna().groupby(df.index.year).sum())\nfig, ax = plt.subplots(1,2, figsize=(10,3))\ndf.Rainfall_Monteporzio.groupby(df.index.year).plot(\n    xlabel='Rainfall_Monteporzio: before imputation', ax=ax[0])\ndf.Rainfall_Velletri.groupby(df.index.year).plot(\n    xlabel='Rainfall_Velletri: before imputation', ax=ax[1])\nplt.show()\n\n### main\ndf.loc[:'2015-01-01', cols] = fillna(df.loc[:'2015-01-01', cols], approach='interpolate').copy()\ndf.loc['2019-01-01':, cols] = fillna(df.loc['2019-01-01':, cols], approach='interpolate').copy()\n\nmissing_year = [2015, 2016, 2017, 2018]\nfor missing_y in missing_year: \n    # use previouse 4 years to impute next year\n    base_idx = df.query('year >= @missing_y - 4 and year < @missing_y').index\n    missing_idx = df[df.index.year == missing_y].index \n    dfbase = df.loc[base_idx, cols].copy()\n    dfbase['month'] = dfbase.index.month\n    dfbase['day'] = dfbase.index.day\n    res = dfbase.groupby(['month', 'day']).agg('mean')\n    if df.loc[missing_idx, cols].shape[0] == 365:\n        res.drop(index=(2,29), inplace=True)\n    df.loc[missing_idx, cols] = res.values\ndf[cols] = fillna(df[cols], approach='interpolate')\nassert df[cols].isna().sum().sum() == 0\n\n### plot again\nfig, ax = plt.subplots(1,2, figsize=(10,3))\ndf.Rainfall_Monteporzio.groupby(df.index.year).plot(\n    xlabel='Rainfall_Monteporzio: after imputation', ax=ax[0])\ndf.Rainfall_Velletri.groupby(df.index.year).plot(\n    xlabel='Rainfall_Velletri: after imputation', ax=ax[1])\nplt.show()","fdbf4255":"# imputation II \n\n## for Temperature\ncols = df.filter(regex='Temp.*').columns\n### summarize missing values by year \n# display(df[cols].isna().groupby(df.index.year).sum())\nfig, ax = plt.subplots(1,2, figsize=(10,3))\ndf.Temperature_Monteporzio.groupby(df.index.year).plot(\n    xlabel='Temperature_Monteporzio: before imputation', ax=ax[0])\ndf.Temperature_Velletri.groupby(df.index.year).plot(\n    xlabel='Temperature_Velletri: before imputation', ax=ax[1])\nplt.show()\n\n### main\ndf.loc[:'2015-01-01', cols] = fillna(df.loc[:'2015-01-01', cols], approach='interpolate').copy()\ndf.loc['2019-01-01':, cols] = fillna(df.loc['2019-01-01':, cols], approach='interpolate').copy()\n\nmissing_year = [2015, 2016, 2017, 2018]\nfor missing_y in missing_year: \n    # use previouse 4 years to impute next year\n    base_idx = df.query('year >= @missing_y - 4 and year < @missing_y').index\n    missing_idx = df[df.index.year == missing_y].index \n    \n    dfbase = df.loc[base_idx, cols].copy()\n    dfbase['month'] = dfbase.index.month\n    dfbase['day'] = dfbase.index.day\n    res = dfbase.groupby(['month', 'day']).agg('mean')\n    if df.loc[missing_idx, cols].shape[0] == 365:\n        res.drop(index=(2,29), inplace=True)\n    df.loc[missing_idx, cols] = res.values\ndf[cols] = fillna(df[cols], approach='interpolate')\nassert df[cols].isna().sum().sum() == 0\n\n### plot again\nfig, ax = plt.subplots(1,2, figsize=(10,3))\ndf.Temperature_Monteporzio.groupby(df.index.year).plot(\n    xlabel='Temperature_Monteporzio: after imputation', ax=ax[0])\ndf.Temperature_Velletri.groupby(df.index.year).plot(\n    xlabel='Temperature_Velletri: after imputation', ax=ax[1])\nplt.show()","277bb306":"# imputation III\ndf = df['2015-06-01':]\nmaskdf = maskdf['2015-06-01':]\n## for Depth_to_Groundwater: linear interpolation (rude but easy)\ncols = df.filter(regex='Depth.*').columns\n# display(df[cols].isna().groupby(df.index.year).sum())\n\ndf[cols] = fillna(df[cols], approach='interpolate')\nassert df[cols].isna().sum().sum() == 0","7ae32f5d":"# imputation IV: still keep missing data before 2016-10-07\ndf = df['2016-10-07':]\nmaskdf = maskdf['2016-10-07':]\n## for Volume_Pozzo_xxx: linear interpolation\ncols = df.filter(regex='Volume.*').columns\n# display(df[cols].isna().groupby(df.index.year).sum())\ndf[cols] = df[cols].replace(0, np.nan)\ndf.loc[:, cols] = fillna(df[cols], approach='interpolate').copy()\nassert df[cols].isna().sum().sum() == 0\nassert df.shape == maskdf.shape","7b8b9c61":"plt.figure(figsize=(14,6))\nfor i, col in enumerate(df.filter(regex='Vol.*').columns): \n    plt.subplot(2,4,i+1)\n    df[col].plot(xlabel=col)\nplt.tight_layout()\nplt.show()","01080ff6":"# Feature engineering\ndfextended = df.copy()\nmask = maskdf.copy()\ndfextended.drop(columns='year', inplace=True, errors='ignore')\nmask.drop(columns='year', inplace=True, errors='ignore')\n\n## Engineer 1\ntmp = ['Volume_Pozzo_2', 'Volume_Pozzo_4', 'Volume_Pozzo_5_6', 'Volume_Pozzo_8']\ndfextended['Volume_gp1'] = dfextended[tmp].mean(axis=1)\ndfextended.drop(columns=tmp, inplace=True)\n\n## Engineer 2: rolling\n# tmp = dfextended.filter(regex='Rain.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('sum')\n# tmp = dfextended.filter(regex='Temp.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('mean')\n# tmp = dfextended.filter(regex='Vol.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('mean')\n# dfextended.dropna(inplace=True)\n\n## pca \n# pca_plot(dfextended[['Rainfall_Velletri', 'Rainfall_Monteporzio']])\n# pca_plot(dfextended[['Temperature_Velletri', 'Temperature_Monteporzio']])\n# tmp = ['Volume_gp1']\n# tmp.extend(dfextended.filter(regex='Vol.*').columns.drop(tmp).tolist())\n# pca_plot(dfextended[tmp])\n# pca_plot(dfextended.filter(regex='Depth.*'))\n\n## Drop due to pca\n# dfextended.drop(columns=['Rainfall_Monteporzio'], inplace=True)\n# dfextended.drop(columns=['Temperature_Monteporzio'], inplace=True)\n# dfextended.drop(columns=['Volume_Pozzo_7'], inplace=True)\n\n## Reorder \ncollector = []\nfor regex in orderlist: # in Sec 2.2\n    collector.append(dfextended.filter(regex=regex).columns)\ndfextended = dfextended.reindex(columns= np.concatenate(collector) )\n\n## so does mask\nmask['Volume_gp1'] = False\n# mask = mask[29:] # due to rolling\nfor col in df.columns.drop(dfextended.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True, errors='ignore')\nassert mask.shape == dfextended.shape, f'mask & dfextended shape don\\'t match: {mask.shape}, {dfextended.shape}'\nmask = mask.reindex(columns=dfextended.columns)","dd660abb":"# Check stationarity\n## adftest\nadftable = adfuller_table(dfextended, verbose=False, alpha=0.05, maxlag=30, regression='c')\n# display(adftable)\n\n## collect non-stationary features\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\nprint('Differenced features: ', I1)\n\n## (Cont'd) take diff() on cols that do not pass the adftest. Here we adopt AIC.\ndfstationary = dfextended.copy()\ndfstationary.loc[:,I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)\n\n## adftest again\nadftable = adfuller_table(dfstationary, verbose=False, maxlag=30, regression='c')\n# display(adftable) # This time all features pass ADF test in both AIC and BIC criteria.","b8d0d679":"# Check granger causality\ngrtable = grangers_causation_table(dfstationary, xnames=dfextended.columns.drop(labels=targets), \n                          ynames=targets, maxlag=30 ,alpha=0.05)\ndisplay(grtable)\n# # Manually drop non Granger-cause variables\n# tmp = ['Rainfall_Velletri', 'Temperature_Monteporzio']\n# dfstationary.drop(columns=tmp, inplace=True)\n# mask.drop(columns=tmp, inplace=True)","746af385":"# Deal with Time Periodicity \nyear = 365.2425\ndfstationary['year_sin'] = np.sin(dfstationary.index.dayofyear * 2 * np.pi \/ year)\ndfstationary['year_cos'] = np.cos(dfstationary.index.dayofyear * 2 * np.pi \/ year)\nmask['year_sin'] = False\nmask['year_cos'] = False","a7f4f1f9":"# Split data\nN = dfstationary.shape[0]\ntrain, val, test = dfstationary[:int(0.8*N)], dfstationary[int(0.8*N):int(0.9*N)], dfstationary[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in targets]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","e9905c27":"# Get sense: ac & pac plots\n# acpac_plot(dfstationary[targets], figsize=(10, 2) )","e4aaed66":"# VAR model \nvar_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=30, trend='c')\nprint(res.selected_orders)\nvarlag = res.selected_orders['aic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test, mask=tmask[1:] , label_columns=targets) #[1:]\nprint('The model we are using: {1:}-dim VAR({0:}).'.format(*varobj.model.coefs.shape[:2]))","e377334a":"# Durbin_watson test of residuals\nprint('Durbin-Watson test:')\n_ = varobj.durbin_watson_test()","b3caee68":"# Adjusted Portmannteau-test & S-K test\ndisplay(var_model.test_whiteness(nlags=varlag*5, signif=0.05, adjusted=True).summary())\ndisplay(varobj.model.test_normality(signif=0.05).summary())\n# Residual Autocorrelations\nvarobj.residac_plot(cols=targets)","3c799a04":"# 1-step-ahead Prediction\n## paramters setting:\nsteps=1\nmode = 'normal'\nuse_mask = True\nmultioutput = 'raw_values' \n\n## after trransformation\nrmse, mae = varobj.inverse_transform_evaluate(\n    I1, dfextended, steps=steps, mode=mode, multioutput=multioutput, use_mask=True)\nprint('uniform rmse: {:8.4f}'.format(rmse.mean()))\nprint('uniform mae: {:8.4f}'.format(mae.mean()))\n\n## plot\nvarobj.plot(use_mask=True)","87c187d5":"# Last Prediction\n## paramters setting:\n\nsteps=[7, 14]\nmode = 'last'\nuse_mask = True\nmultioutput = 'raw_values' \n\nfor step in steps: \n    ## prediction\n    rmse, mae = varobj.inverse_transform_evaluate(I1, dfextended,steps=step, mode=mode, \n                                                  multioutput=multioutput, use_mask=use_mask)\n    print('uniform rmse with {:>} step: {:8.4f}'.format(step, rmse.mean() ))\n    print('uniform mae with {:>} step: {:8.4f}'.format(step, mae.mean() ))\n    ## plot\n    varobj.plot(use_mask=use_mask, figsize=(20, 15))","57809d8a":"# OIR Plot Analysis\n# are of interest in econometric studies: they are the estimated responses to a unit impulse in one \n# of the variables.\n# varobj.irplot(impulse=train.columns, periods=15, response=targets, orth=True,\n#              figsize=(17,100), method='hall')","33df8c16":"# FEVD Plot Analysis \n# varobj.fevd_plot(periods=11, top=5, figsize=(14, 9), max_str_split=3) ","8a140ae6":"Image(\"..\/input\/aceawaterimages\/doganella1.png\")","cbf4d8c5":"Image(\"..\/input\/aceawaterimages\/doganella2.png\")","559917f9":"# Feature engineering & imputation\n## no mask to deal with\ndfextended = df.copy()\n\n## Reorder \ncollector = []\nfor regex in orderlist:\n    collector.append(dfextended.filter(regex=regex).columns)\ndfextended = dfextended.reindex(columns= np.concatenate(collector) )\n\n# Deal with Time Periodicity \nyear = 365.2425\ndfextended['year_sin'] = np.sin(dfextended.index.dayofyear * 2 * np.pi \/ year)","56abc576":"# Split data\nN = dfextended.shape[0]\ntrain, val, test = dfextended[:int(0.8*N)], dfextended[int(0.8*N):int(0.9*N)], dfextended[int(0.9*N):]\n\n## define indices\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in targets]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes are:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","0a14f3dd":"# Initialize data loader: 1-step-ahead prediction\n## parameters setting\ninput_width = varlag     # suggested by VAR\nlabel_width = 1       \nshift = 1\nbatch_size = 2\n## generate window\nwindow = WindowGenerator(input_width, label_width, shift, train_scaled, val_scaled, test_scaled, \n                         batch_size=batch_size, shuffle=False, mask=None, \n                         label_columns=targets)\nprint(window)\nprint('Inputs\/labels shapes of window: {} {}'.format(\n    window.example[0].shape, window.example[1].shape))","53e3ab5a":"# 1-step-ahead LSTM prediction\n\nlstm_model = None\nif os.path.isfile(os.path.join(modelpath, 'doganella.keras')):\n    lstm_model = tf.keras.models.load_model(os.path.join(modelpath, 'doganella.keras'))\nelse: \n    # ## Parameters setting:\n    MAX_EPOCHS = 200\n    patience = 10        # stop training if loss increase for 'patience' periods in total.\n    lr_rate = 1e-3\n    train_verbose = 0    # 0: no output, 1: progress, 2: report\n    lstm_kernel = 16\n    stateful = False     # batchsize must be 1 if True. (takes 10 mins)\n    dropout = 0.2        # dropout rate in [0,1]\n\n    ## fixed setting\n    n_features = train_scaled.shape[1]\n    n_out = len(targets)\n    OUT_STEPS = window.label_width\n\n    ## RNN: LSTM\n    lstm_model = tf.keras.Sequential([\n        # Shape [batch, time, features] => [batch, lstm_units]\n        tf.keras.layers.LSTM(lstm_kernel, return_sequences=True, stateful=stateful),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.LSTM(int(lstm_kernel\/2), return_sequences=False, stateful=stateful),\n        # Shape => [batch, out_steps*features]\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(OUT_STEPS*n_out),\n        # Shape => [batch, out_steps, features]\n        tf.keras.layers.Reshape([OUT_STEPS, n_out])\n    ])\n    ## compile and fit\n    print('window RNN')\n    print('Xtrain shape: ', window.example[0].shape, 'ytrain shape: ', window.example[1].shape, end='')\n    print('\\toutput shape: ', lstm_model(window.example[0]).shape, end='\\n\\n')\n    history = compile_and_fit(lstm_model, window, MAX_EPOCHS, patience, lr_rate, verbose=train_verbose)\n    ## save model\n    lstm_model.save(os.path.join(modelpath, 'doganella.keras'), overwrite=False)\n    \n## evaluate on test\nprint('evaluation before scaling back:')\nlstm_model.evaluate(window.test, verbose=2, return_dict=True)\n#window.plot(lstm_model, use_mask=True)\n\n## inverse_transform_evaluate on test\ndi = window.inverse_transform_evaluate(lstm_model, scaler, which='test', return_dict=True)\nprint('evaluation after scaling back: {0}: {2:6.4f} - {1}: {3:6.4f}'.format(*di.keys(), *di.values()))\nwindow.plot(use_mask=False, figsize=(20, 20))","636e3333":"# LSTM feature importance plot\nimportance, direction = feature_importance(lstm_model, window.X_test, scale=0.2, method='const')\nfeature_importance_plot(importance, direction, names=train_scaled.columns)","5729d9bb":"# parameters setting\ndataset = 'Aquifer_Luco.csv'\ntargets = ['Depth_to_Groundwater_Podere_Casetta']\n\n# read dataset as DataFrame\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'], date_parser=dateparse)\n\n# re-order features\norderlist = ['Rain.*', 'Temp.*', 'Volume.*', 'Depth.*']\ncollector = []\nfor regex in orderlist: \n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\ndf.index.freq = 'd'\ndf.sort_index(inplace=True)\n\n# Set weird values to nan\n## set nan to Groundwater whose values are 0\ndf[df.filter(regex='Temperature_.*').columns] = df.filter(regex='Temperature_.*').replace(0, np.nan)\ndf['Depth_to_Groundwater_Pozzo_4'].replace(0, np.nan, inplace=True)\n\n# visual graph of nan locations for each field\nmissingval_plot(df, show=False)\nplt.axvline(x = 6453, c='r') # 2017-09-01\nplt.axvline(x = 6951, c='r') # 2019-01-11\nplt.show()\n\n# Depth_to_Groundwater_Podere_Casetta: missing after 2019-01-12\n# Depth_to_Groundwater_1,3,4: mass missing till 2017-08-31 (included)\n# Rainfall_Siena_Poggio_al_Vento & Temperature_Siena_Poggio_al_Vento: mmass missing till 2017-11-22\n# Volumes: missing till 2014-12-31 (included)","53c5d0db":"# Dealing with missing values\ndf = df['2017-09-01':'2019-01-12']\n\n## drop columns\ndf.drop(columns=['Rainfall_Siena_Poggio_al_Vento'], inplace=True)\ndf.drop(columns=['Temperature_Siena_Poggio_al_Vento'], inplace=True)","eab1c112":"# correlation plot\nfeature_plots(df)\ncorrtri_plot(df)","e4856227":"# Feature engineering & imputation\ndfimputed = df.copy()\n# Record nan positions before imputation\nmask = df.isna()\n\n# imputation\ndfimputed = fillna(dfimputed, approach='regression', window=1)\nassert dfimputed.isna().any().any() == False\n\n## Engineer 1\n# tmp = dfimputed.filter(regex='Rain.*').columns\n# dfimputed['Rainfall_sum'] = dfimputed[tmp].sum(axis=1)\n# dfimputed.drop(columns=tmp, inplace=True)\n\n# tmp = dfimputed.filter(regex='Temp.*').columns\n# dfimputed['Temperature_mean'] = dfimputed[tmp].mean(axis=1)\n# dfimputed.drop(columns=tmp, inplace=True)\n\n# tmp = dfimputed.filter(regex='Vol.*').columns\n# dfimputed['Volume_sum'] = dfimputed[tmp].sum(axis=1)\n# dfimputed.drop(columns=tmp, inplace=True)\n\n## Engineer 2: rolling\n# tmp = dfimputed.filter(regex='Rain.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('sum')\n# tmp = dfimputed.filter(regex='Vol.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# tmp = dfimputed.filter(regex='Temp.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# dfimputed.dropna(inplace=True)\n\n## pca \n# tmp = ['Rainfall_Monticiano_la_Pineta', 'Rainfall_Simignano', 'Rainfall_Montalcinello']\n# tmp.extend(dfimputed.filter(regex='Rain.*').columns.drop(tmp).tolist())\n# pca_plot(dfimputed[tmp])\n\n# tmp = ['Temperature_Monteroni_Arbia_Biena']\n# tmp.extend(dfimputed.filter(regex='Temp.*').columns.drop(tmp).tolist())\n# pca_plot(dfimputed[tmp])\n# tmp = ['Volume_Pozzo_1', 'Volume_Pozzo_3', 'Volume_Pozzo_4']\n# tmp.extend(dfimputed.filter(regex='Vol.*').columns.drop(tmp).tolist())\n# pca_plot(dfimputed[tmp])\n\n# pca_plot(dfimputed.filter(regex='Depth.*'))\n\n## Dorp due to pca\n# tmp = ['Rainfall_Montalcinello', 'Rainfall_Monticiano_la_Pineta', 'Rainfall_Simignano']\n# dfimputed.drop(columns=dfimputed.filter(regex='Rain.*').columns.drop(tmp), inplace=True)\n# tmp = ['Temperature_Monteroni_Arbia_Biena']\n# dfimputed.drop(columns=dfimputed.filter(regex='Temp.*').columns.drop(tmp), inplace=True)\n# # tmp = ['Volume_sum', 'Volume_Pozzo_1', 'Volume_Pozzo_3']\n# # dfimputed.drop(columns=dfimputed.filter(regex='Vol.*').columns.drop(tmp), inplace=True)\n# dfimputed.drop(columns='Depth_to_Groundwater_Pozzo_4', inplace=True)\n\n## Reorder \ncollector = []\nfor regex in orderlist: # in Sec 2.1.1\n    collector.append(dfimputed.filter(regex=regex).columns)\ndfimputed = dfimputed.reindex(columns= np.concatenate(collector) )\n\n\n## so does mask\n# mask['Volume_sum'] = mask['Rainfall_sum'] = mask['Temperature_mean'] = False\n# mask = mask[29:]\nfor col in df.columns.drop(dfimputed.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True)\nassert mask.shape == dfimputed.shape, f'mask shape {mask.shape} doesn\\'t match dfimputed shape {dfimputed.shape}'\nmask = mask.reindex(columns=dfimputed.columns)","fe311ea0":"# Chekc stationarity\n## adftest\nadftable = adfuller_table(dfimputed, verbose=False, alpha=0.05, maxlag=21, regression='ct')\n# display(adftable)\n\n## collect non-stationary features\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\nprint('Differenced features: ', I1)\n\n## (Cont'd) take diff() on cols that do not pass the adftest. Here we adopt AIC.\ndfstationary = dfimputed.copy()\ndfstationary.loc[:,I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)\n\n## adftest again\nadftable = adfuller_table(dfstationary, verbose=False, maxlag=21, regression='ct')\n# display(adftable) # This time all features pass ADF test in both AIC and BIC criteria.","9e0a3a75":"# Check granger causality\ngrtable = grangers_causation_table(dfstationary, xnames=dfimputed.columns.drop(labels=targets), \n                          ynames=targets, maxlag=20 ,alpha=0.05)\ndisplay(grtable)\n## Manually drop non Granger-cause variables\n# fail_feature = grtable[grtable.iloc[:,0] == False].index.str.strip('_x').values\n# print('features not pass Granger:\\n', fail_feature)\n# dfstationary.drop(columns=fail_feature, inplace=True)\n# mask.drop(columns=fail_feature, inplace=True)","9afeda93":"# Deal with Time Periodicity \nyear = 365.2425\n\ndfstationary['year_sin'] = np.sin(dfstationary.index.dayofyear * 2 * np.pi \/ year)\nmask['year_sin'] = False\nprint(dfstationary.shape, mask.shape)","4f41b487":"# Split data\nN = dfstationary.shape[0]\ntrain, val, test = dfstationary[:int(0.8*N)], dfstationary[int(0.8*N):int(0.9*N)], dfstationary[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in targets]\n\n# Scaling and Normalization \n# normalization won't make sense if the TS is trending upward\/downward. This is why stationary first.\nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","a50c0f9b":"# Get sense: ac & pac plots\nacpac_plot(dfstationary[targets], figsize=(10, 3))","09432006":"# VAR model \nvar_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=20, trend='ct')\nprint(res.selected_orders)\nvarlag = res.selected_orders['aic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test, mask=tmask[1:] , label_columns=targets) #[1:]\nprint('The model we are using: {1:}-dim VAR({0:}).'.format(*varobj.model.coefs.shape[:2]))","afc210e9":"# Durbin_watson test of residuals\nprint('Durbin-Watson test:')\n_ = varobj.durbin_watson_test()","dbd8ef33":"# Adjusted Portmannteau-test & S-K test\ndisplay(var_model.test_whiteness(nlags=varlag*4, signif=0.05, adjusted=True).summary())\ndisplay(varobj.model.test_normality().summary())\n# Residual Autocorrelations\nvarobj.residac_plot(cols=targets, figsize=(15, 3))","a43e48fa":"# 1-step-ahead Prediction\n## paramters setting:\nsteps=1\nmode = 'normal'\nuse_mask = True\nmultioutput = 'raw_values' \n\n## after trransformation\nrmse, mae = varobj.inverse_transform_evaluate(\n    I1, dfimputed, steps=steps, mode=mode, multioutput=multioutput, use_mask=use_mask)\nprint('uniform rmse: {:8.4f}'.format(rmse.mean()))\nprint('uniform mae: {:8.4f}'.format(mae.mean()))\n\n## plot\nvarobj.plot(use_mask=True, figsize=(20, 2))","9afd353b":"# Last Prediction\n## paramters setting:\n\nsteps=[7, 14]\nmode = 'last'\nuse_mask = True\nmultioutput = 'raw_values' \nfor step in steps: \n    ## prediction\n    rmse, mae = varobj.inverse_transform_evaluate(I1, dfimputed,steps=step, mode=mode, \n                                                  multioutput=multioutput, use_mask=use_mask)\n    print('uniform rmse with {:>} step: {:8.4f}'.format(step, rmse.mean() ))\n    print('uniform mae with {:>} step: {:8.4f}'.format(step, mae.mean() ))\n    ## plot\n    varobj.plot(use_mask=use_mask, figsize=(20, 2))","7b239722":"# OIR Plot Analysis\n# varobj.irplot(impulse=train.columns, periods=15, response=targets, orth=False,\n#              figsize=(17,15), method='hall')","0b34c62c":"# FEVD Plot Analysis\nvarobj.fevd_plot(periods=15, top=5, figsize=(17, 10), max_str_split=3) ","f7407f83":"Image(\"..\/input\/aceawaterimages\/luco1.png\")","e10f8899":"Image(\"..\/input\/aceawaterimages\/luco2.png\")","4d366411":"# Feature engineering & imputation\ndfimputed = df.copy()\n# Record nan positions before imputation\nmask = df.isna()\n\n# imputation\ndfimputed = fillna(dfimputed, approach='interpolate')\nassert dfimputed.isna().any().any() == False\n\n## Reorder \ncollector = []\nfor regex in orderlist: # in Sec 2.1.1\n    collector.append(dfimputed.filter(regex=regex).columns)\ndfimputed = dfimputed.reindex(columns= np.concatenate(collector) )\n\n## so does mask\n# mask['Temperature_mean'] = False\nfor col in df.columns.drop(dfimputed.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True)\nassert mask.shape == dfimputed.shape, f'mask shape {mask.shape} doesn\\'t match dfimputed shape {dfimputed.shape}'\nmask = mask.reindex(columns=dfimputed.columns)\n# print(dfimputed.shape, mask.shape)\n\n# Deal with Time Periodicity \n# year = 365.2425\n# dfimputedputed['year_sin'] = np.sin(dfimputed.index.dayofyear * 2 * np.pi \/ year)\n# mask['year_sin'] = False","25198a9e":"# Split data\nN = dfimputed.shape[0]\ntrain, val, test = dfimputed[:int(0.8*N)], dfimputed[int(0.8*N):int(0.9*N)], dfimputed[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in [targets[0]]]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","db55dac7":"# Initialize data loader: 1-step-ahead prediction\n## parameters setting\ninput_width = varlag     # suggested by VAR\nlabel_width = 1       \nshift = 1\nbatch_size = 1\n## generate window\nwindow = WindowGenerator(input_width, label_width, shift, train_scaled, val_scaled, test_scaled, \n                         batch_size=batch_size, shuffle=False, mask=tmask, \n                         label_columns=targets)\nprint(window)\nprint('Inputs\/labels shapes of window: {} {}'.format(\n    window.example[0].shape, window.example[1].shape))","c2777f3a":"# 1-step-ahead LSTM prediction\n\nlstm_model = None\nif os.path.isfile(os.path.join(modelpath, 'luco.keras')):\n    lstm_model = tf.keras.models.load_model(os.path.join(modelpath, 'luco.keras'))\nelse: \n    # ## Parameters setting:\n    MAX_EPOCHS = 100\n    patience = 10        # stop training if loss increase for 'patience' periods in total.\n    lr_rate = 1e-3\n    train_verbose = 0    # 0: no output, 1: progress, 2: report\n    lstm_kernel = 16\n    stateful = False     # batchsize must be 1 if True. (takes 10 mins)\n    dropout = 0.2        # dropout rate in [0,1]\n\n    ## fixed setting\n    n_features = train_scaled.shape[1]\n    n_out = len(targets)\n    OUT_STEPS = window.label_width\n\n    ## RNN: LSTM\n    lstm_model = tf.keras.Sequential([\n        # Shape [batch, time, features] => [batch, lstm_units]\n        tf.keras.layers.LSTM(lstm_kernel, return_sequences=True, stateful=stateful),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.LSTM(int(lstm_kernel\/2), return_sequences=False, stateful=stateful),\n        # Shape => [batch, out_steps*features]\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(OUT_STEPS*n_out),\n        # Shape => [batch, out_steps, features]\n        tf.keras.layers.Reshape([OUT_STEPS, n_out])\n    ])\n    ## compile and fit\n    print('window RNN')\n    print('Xtrain shape: ', window.example[0].shape, 'ytrain shape: ', window.example[1].shape, end='')\n    print('\\toutput shape: ', lstm_model(window.example[0]).shape, end='\\n\\n')\n    history = compile_and_fit(lstm_model, window, MAX_EPOCHS, patience, lr_rate, verbose=train_verbose)\n    ## save model\n    lstm_model.save(os.path.join(modelpath, 'luco.keras'), overwrite=False)\n    \n## evaluate on test\nprint('evaluation before scaling back:')\nlstm_model.evaluate(window.test, verbose=2, return_dict=True)\n\n## inverse_transform_evaluate on test\ndi = window.inverse_transform_evaluate(lstm_model, scaler, which='test', return_dict=True)\nprint('evaluation after scaling back: {0}: {2:6.4f} - {1}: {3:6.4f}'.format(*di.keys(), *di.values()))\nwindow.plot(use_mask=False, figsize=(20, 3))","a152052b":"# LSTM feature importance plot\nimportance, direction = feature_importance(lstm_model, window.X_test, scale=0.2, method='const')\nfeature_importance_plot(importance, names=train_scaled.columns)","c4233dda":"# parameters setting\ndataset = 'Aquifer_Petrignano.csv'\ntargets = ['Depth_to_Groundwater_P24', 'Depth_to_Groundwater_P25']\n\n# read dataset as DataFrame\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'], date_parser=dateparse)\n\n# re-order features\norderlist = ['Rain.*', 'Temp.*', 'Volume.*', 'Hydrom.*', 'Depth.*']\ncollector = []\nfor regex in orderlist: \n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\ndf.index.freq = 'd'\ndf.sort_index(inplace=True)\n\n# Set weird values to nan\n## set nan to Groundwater whose values are 0\ndf.loc['2015-04-25':'2016-01-01', 'Temperature_Petrignano'].replace(0, np.nan, inplace=True)\ndf.Volume_C10_Petrignano.replace(0, np.nan, inplace=True)\ndf.Hydrometry_Fiume_Chiascio_Petrignano.replace(0, np.nan, inplace=True)\n\n# visual graph of nan locations for each field\nmissingval_plot(df, show=False)\nplt.axvline(x = 1024, c='r') # 2009-01-11\nplt.show()\n\n# mass missing till 2018-12-31 (included) and small missing in betwwen 2015-04-25 & 2015-09-21","afa84fa1":"# Dealing with missing values\ntodrop = df[df['Rainfall_Bastia_Umbra'].isna()].index.max()\ndf = df[df.index > todrop]\n","0ff590a4":"# correlation plot\nfeature_plots(df)\ncorrtri_plot(df)","b4d9ffbd":"# Feature engineering & imputation\ndfimputed = df.copy()\n# Record nan positions before imputation\nmask = df.isna()\n\n# imputation\ndfimputed = fillna(dfimputed, approach='interpolate')\nassert dfimputed.isna().any().any() == False\n\n## Engineer 1\n# tmp = dfimputed.filter(regex='Temp.*').columns\n# dfimputed['Temperature_mean'] = dfimputed[tmp].mean(axis=1)\n# dfimputed.drop(columns=tmp, inplace=True)\n\n# tmp = ['Hydrometry_Fiume_Chiascio_Petrignano']\n# dfimputed[tmp] = replace_outliers(dfimputed[tmp], window=7, k=1, method='mean')\n\n# tmp = ['Volume_C10_Petrignano']\n# dfimputed[tmp] = replace_outliers(dfimputed[tmp], window=7, k=1, method='mean')\n\n## Engineer 2: rolling\n# tmp = dfimputed.filter(regex='Rain.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('sum')\n# tmp = dfimputed.filter(regex='Vol.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# tmp = dfimputed.filter(regex='Temp.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# tmp = dfimputed.filter(regex='Hydro.*').columns\n# dfimputed[tmp] = dfimputed[tmp].rolling(30).agg('mean')\n# dfimputed.dropna(inplace=True)\n\n## pca \n# pca_plot(dfimputed.filter(regex='Temp.*')  )\n\n## Dorp due to pca\n# dfimputed.drop(columns='Temperature_Petrignano', inplace=True)\n# dfimputed.drop(columns='Depth_to_Groundwater_P25', inplace=True)\n\n## Reorder \ncollector = []\nfor regex in orderlist: # in Sec 2.1.1\n    collector.append(dfimputed.filter(regex=regex).columns)\ndfimputed = dfimputed.reindex(columns= np.concatenate(collector) )\n\n## so does mask\n# mask['Temperature_mean'] = False\n# mask = mask[29:]\nfor col in df.columns.drop(dfimputed.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True)\nassert mask.shape == dfimputed.shape, f'mask shape {mask.shape} doesn\\'t match dfimputed shape {dfimputed.shape}'\nmask = mask.reindex(columns=dfimputed.columns)","5e92da3f":"# Chekc stationarity\n## adftest\nadftable = adfuller_table(dfimputed, verbose=False, alpha=0.05, maxlag=16, regression='ct')\n# display(adftable)\n\n## collect non-stationary features\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\nprint('Differenced features: ', I1)\n\n## (Cont'd) take diff() on cols that do not pass the adftest. Here we adopt AIC.\ndfstationary = dfimputed.copy()\ndfstationary.loc[:,I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)\n\n## adftest again\nadftable = adfuller_table(dfstationary, verbose=False, maxlag=16, regression='ct')\n# display(adftable) # This time all features pass ADF test in both AIC and BIC criteria.","2c99daa6":"# Check granger causality\ngrtable = grangers_causation_table(dfstationary, xnames=dfimputed.columns.drop(labels=targets), \n                          ynames=targets, maxlag=15 ,alpha=0.05)\ndisplay(grtable)","03eb4fec":"# Deal with Time Periodicity \nyear = 365.2425\ndfstationary['year_sin'] = np.sin(dfstationary.index.dayofyear * 2 * np.pi \/ year)\nmask['year_sin'] = False","7cf0c4fe":"# Split data\nN = dfstationary.shape[0]\ntrain, val, test = dfstationary[:int(0.8*N)], dfstationary[int(0.8*N):int(0.9*N)], dfstationary[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in [targets[0]]]\n\n# Scaling and Normalization \n# normalization won't make sense if the TS is trending upward\/downward. This is why stationary first.\nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","08c5cd73":"# Get sense: ac & pac plots\nacpac_plot(dfstationary[targets], figsize=(10, 3))","1c998f97":"# VAR model \nvar_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=37, trend='ct')\nprint(res.selected_orders)\nvarlag = res.selected_orders['aic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test, mask=tmask[1:] , label_columns=targets) #[1:]\nprint('The model we are using: {1:}-dim VAR({0:}).'.format(*varobj.model.coefs.shape[:2]))","2c3606e4":"# Durbin_watson test of residuals\nprint('Durbin-Watson test:')\n_ = varobj.durbin_watson_test()\n# comment: all values are near 2 => no significant serial correlation in the residual.","a29c921f":"# Adjusted Portmannteau-test & S-K test\ndisplay(var_model.test_whiteness(nlags=varlag*4, signif=0.05, adjusted=True).summary())\ndisplay(varobj.model.test_normality().summary())\n# Residual Autocorrelations\nvarobj.residac_plot(cols=targets, figsize=(15, 3))","adca0529":"# 1-step-ahead Prediction\n## paramters setting:\nsteps=1\nmode = 'normal'\nuse_mask = True\nmultioutput = 'raw_values' \n## after trransformation\nrmse, mae = varobj.inverse_transform_evaluate(\n    I1, dfimputed, steps=steps, mode=mode, multioutput=multioutput, use_mask=use_mask)\nprint('uniform rmse: {:8.4f}'.format(rmse.mean()))\nprint('uniform mae: {:8.4f}'.format(mae.mean()))\n\n## plot\nvarobj.plot(use_mask=True, figsize=(20, 10))","1c9923c0":"# Last Prediction\n## paramters setting:\nsteps=[7, 14]\nmode = 'last'\nuse_mask = False\nmultioutput = 'raw_values' \nfor step in steps: \n    ## prediction\n    rmse, mae = varobj.inverse_transform_evaluate(I1, dfimputed,steps=step, mode=mode, \n                                                  multioutput=multioutput, use_mask=use_mask)\n    print('uniform rmse with {:>} step: {:8.4f}'.format(step, rmse.mean() ))\n    print('uniform mae with {:>} step: {:8.4f}'.format(step, mae.mean() ))\n    ## plot\n    varobj.plot(use_mask=use_mask, figsize=(20, 10))","b8d2ba2b":"# OIR Plot Analysis\nvarobj.irplot(impulse=train.columns, periods=15, response=targets[0], orth=True,\n             figsize=(17,10), method='hall')","9d1d6152":"# OIR Plot Analysis\nvarobj.irplot(impulse=['Volume_C10_Petrignano'], periods=15, \n              response=['Hydrometry_Fiume_Chiascio_Petrignano'], orth=True, supfontsize=14, fontsize=12,\n              figsize=(7,4), method='hall')","ff06aa79":"# FEVD Plot Analysis \nvarobj.fevd_plot(periods=11, top=5, figsize=(14, 9), max_str_split=3) ","9d6f2aa4":"Image(\"..\/input\/aceawaterimages\/petrignano.png\")","35ce1972":"# Feature engineering & imputation\ndfimputed = df.copy()\n# Record nan positions before imputation\nmask = df.isna()\n\n# imputation\ndfimputed = fillna(dfimputed, approach='interpolate')\nassert dfimputed.isna().any().any() == False\n\n## Reorder \ncollector = []\nfor regex in orderlist: # in Sec 2.1.1\n    collector.append(dfimputed.filter(regex=regex).columns)\ndfimputed = dfimputed.reindex(columns= np.concatenate(collector) )\n\n## so does mask\n# mask['Temperature_mean'] = False\nfor col in df.columns.drop(dfimputed.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True)\nassert mask.shape == dfimputed.shape, f'mask shape {mask.shape} doesn\\'t match dfimputed shape {dfimputed.shape}'\nmask = mask.reindex(columns=dfimputed.columns)\n# print(dfimputed.shape, mask.shape)\n\n# Deal with Time Periodicity \nyear = 365.2425\ndfimputed['year_sin'] = np.sin(dfimputed.index.dayofyear * 2 * np.pi \/ year)\nmask['year_sin'] = False","ec84abad":"# Split data\nN = dfimputed.shape[0]\ntrain, val, test = dfimputed[:int(0.8*N)], dfimputed[int(0.8*N):int(0.9*N)], dfimputed[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in [targets[0]]]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","2237e6f4":"# Initialize data loader: 1-step-ahead prediction\n## parameters setting\ninput_width = varlag     # suggested by VAR\nlabel_width = 1       \nshift = 1\nbatch_size = 2\n\n## generate window\nwindow = WindowGenerator(input_width, label_width, shift, train_scaled, val_scaled, test_scaled, \n                         batch_size=batch_size, shuffle=False, mask=tmask, \n                         label_columns=targets)\nprint(window)\nprint('Inputs\/labels shapes of window: {} {}'.format(\n    window.example[0].shape, window.example[1].shape))","e65ab332":"# 1-step-ahead LSTM prediction\n\nlstm_model = None\nif os.path.isfile(os.path.join(modelpath, 'petrignano.keras')):\n    lstm_model = tf.keras.models.load_model(os.path.join(modelpath, 'petrignano.keras'))\nelse: \n    # ## Parameters setting:\n    MAX_EPOCHS = 100\n    patience = 10        # stop training if loss increase for 'patience' periods in total.\n    lr_rate = 1e-3\n    train_verbose = 0    # 0: no output, 1: progress, 2: report\n    lstm_kernel = 16\n    stateful = False     # batchsize must be 1 if True. (takes 10 mins)\n    dropout = 0.2        # dropout rate in [0,1]\n\n    ## fixed setting\n    n_features = train_scaled.shape[1]\n    n_out = len(targets)\n    OUT_STEPS = window.label_width\n\n    ## RNN: LSTM\n    lstm_model = tf.keras.Sequential([\n        # Shape [batch, time, features] => [batch, lstm_units]\n        tf.keras.layers.LSTM(lstm_kernel, return_sequences=True, stateful=stateful),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.LSTM(int(lstm_kernel\/2), return_sequences=False, stateful=stateful),\n        # Shape => [batch, out_steps*features]\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(OUT_STEPS*n_out),\n        # Shape => [batch, out_steps, features]\n        tf.keras.layers.Reshape([OUT_STEPS, n_out])\n    ])\n    ## compile and fit\n    print('window RNN')\n    print('Xtrain shape: ', window.example[0].shape, 'ytrain shape: ', window.example[1].shape, end='')\n    print('\\toutput shape: ', lstm_model(window.example[0]).shape, end='\\n\\n')\n    history = compile_and_fit(lstm_model, window, MAX_EPOCHS, patience, lr_rate, verbose=train_verbose)\n    ## save model\n    lstm_model.save(os.path.join(modelpath, 'petrignano.keras'), overwrite=False)\n    \n## evaluate on test\nprint('evaluation before scaling back:')\nlstm_model.evaluate(window.test, verbose=2, return_dict=True)\n#window.plot(lstm_model, use_mask=True)\n\n## inverse_transform_evaluate on test\ndi = window.inverse_transform_evaluate(lstm_model, scaler, which='test', return_dict=True)\nprint('evaluation after scaling back: {0}: {2:6.4f} - {1}: {3:6.4f}'.format(*di.keys(), *di.values()))\nwindow.plot(use_mask=False, figsize=(20, 10))","65970633":"# LSTM feature importance plot\nimportance, direction = feature_importance(lstm_model, window.X_test, scale=0.2, method='const')\nfeature_importance_plot(importance, direction, names=train_scaled.columns)","495fb474":"# parameters setting\ndataset = 'Lake_Bilancino.csv'\ntargets = ['Flow_Rate', 'Lake_Level']\n\n# read dataset as DataFrame\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'], date_parser=dateparse)\n\n# re-order features\norderlist = ['Rain.*', 'Temp.*', *targets]\ncollector = []\nfor regex in orderlist: \n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\ndf.index.freq = 'd'\ndf.sort_index(inplace=True)\n\n# Set weird values to nan\n# visual graph of nan locations for each field\nmissingval_plot(df,show=False)\n# mass missing till 2004-01-01 (included)","e4c4bfc3":"# Dealing with missing values\ndf = df['2004-01-02':]\n# no mask since no missing data after 2014-01-02","46885a67":"# correlation plot\nfeature_plots(df.filter(regex='Rain.*'))\nplt.subplot(1,3,1)\ndf.Temperature_Le_Croci.plot(xlabel='', title='Temperature_Le_Croci', figsize=(20,5))\nplt.subplot(1,3,2)\ndf.Flow_Rate.plot(xlabel='', title='Flow_Rate')\nplt.subplot(1,3,3)\ndf.Lake_Level.plot(xlabel='', title='Lake_Level')\nplt.show()\ncorrtri_plot(df)","8419dbb8":"# feature plot\nrfft_plot(df.Lake_Level, ylim=(0, 4000))\nrfft_plot(df.Flow_Rate, ylim=(0, 4000))","9b83115c":"# Feature engineering & imputation\ndfextended = df.copy()\n# Record nan positions before imputation\n\n## Engineer 1\n# tmp = dfextended.filter(regex='Rainfall.*').columns\n# dfextended['Rainfall_mean'] = dfextended[tmp].sum(axis=1)\n# dfextended.drop(columns=tmp, inplace=True)\n\n## Engineer 2: rolling\n# tmp = dfextended.filter(regex='Rain.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('sum')\n# tmp = dfextended.filter(regex='Vol.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('mean')\n# tmp = dfextended.filter(regex='Temp.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('mean')\n# dfextended.dropna(inplace=True)\n\n## pca \n# ['Rainfall_S_Piero', 'Rainfall_Mangona', 'Rainfall_Le_Croci', 'Rainfall_S_Agata']\n# pca_plot(dfextended.filter(regex='Rain.*')  )\n\n## Drop due to pca\n# tmp = ['Rainfall_S_Piero']\n# dfextended.drop(columns=dfextended.filter(regex='Rain.*').columns.drop(tmp), inplace=True)\n\n## Reorder \ncollector = []\nfor regex in orderlist:\n    collector.append(dfextended.filter(regex=regex).columns)\ndfextended = dfextended.reindex(columns= np.concatenate(collector) )","b8927691":"# Chekc stationarity\n## adftest\nadftable = adfuller_table(dfextended, verbose=False, alpha=0.05, maxlag=11, regression='ct')\n# display(adftable)\n\n## collect non-stationary features\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\nprint('Differenced features: ', I1)","dfd1c6bf":"## (Cont'd) take diff() on cols that do not pass the adftest. Here we adopt AIC.\ndfstationary = dfextended.copy()\ndfstationary.loc[:,I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)\n\n## adftest again\nadftable = adfuller_table(dfstationary, verbose=False, alpha=0.05, maxlag=13, regression='ct')\n# display(adftable) # This time all features pass ADF test in both AIC and BIC criteria.","00e02e6e":"# # first difference on other features\n# I11 = adftable[adftable['AIC_5%level'] == False].index.values\n# dfstationary.loc[:,I11] = dfstationary[I11].diff()\n# dfstationary.dropna(inplace=True)\n# adftable = adfuller_table(dfstationary, verbose=False, alpha=0.05, maxlag=13, regression='ct')\n# # display(adftable) # This time all features pass ADF test in both AIC and BIC criteria.\n\n# ## update I1\n# I1 = np.append(I1, I11)\n# print('Differenced Features:', I1)","b8ebce72":"# Check granger causality\ngrtable = grangers_causation_table(dfstationary, xnames=dfextended.columns.drop(labels=targets), \n                          ynames=targets, maxlag=10, alpha=0.05)\ndisplay(grtable)\n\nfail_feature = grtable[grtable.iloc[:,0] == False].index.str.strip('_x').values\n# Manually drop non Granger-cause variables\n# dfstationary.drop(columns=fail_feature, inplace=True)","d63a29fc":"# Deal with Time Periodicity \nyear = 365.2425\ndfstationary['year_sin'] = np.sin(dfstationary.index.dayofyear * 2 * np.pi \/ year)","6d399b84":"# Split data\nN = dfstationary.shape[0]\ntrain, val, test = dfstationary[:int(0.8*N)], dfstationary[int(0.8*N):int(0.9*N)], dfstationary[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in [targets[0]]]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","f1f4c412":"# Get sense: ac & pac plots\nacpac_plot(dfstationary[targets], figsize=(10,3))","bae74648":"# VAR model \nvar_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=30, trend='ct')\nprint(res.selected_orders)\nvarlag = res.selected_orders['aic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test, mask=None , label_columns=targets) #[1:]\nprint('The model we are using: {1:}-dim VAR({0:}).'.format(*varobj.model.coefs.shape[:2]))","0e6a5f85":"# Durbin_watson test of residuals\nprint('Durbin-Watson test:')\n_ = varobj.durbin_watson_test()","4117ec10":"# Adjusted Portmannteau-test & S-K test\ndisplay(var_model.test_whiteness(nlags=varlag*4, signif=0.05, adjusted=True).summary())\ndisplay(varobj.model.test_normality().summary())\n# Residual Autocorrelations\nvarobj.residac_plot(cols=targets, figsize=(15, 3))","2e1a9fa6":"# 1-step-ahead Prediction\n## paramters setting:\nsteps=1\nmode = 'normal'\nuse_mask = False\nmultioutput = 'raw_values' \n## after trransformation\nrmse, mae = varobj.inverse_transform_evaluate(\n    I1, dfextended, steps=steps, mode=mode, multioutput=multioutput, use_mask=use_mask)\nprint('uniform rmse: {:8.4f}'.format(rmse.mean()))\nprint('uniform mae: {:8.4f}'.format(mae.mean()))\n## plot\nvarobj.plot(use_mask=use_mask, figsize=(20,10))","abcbbb14":"# Last Prediction\n## paramters setting:\nsteps=[7, 14, 30]\nmode = 'last'\nuse_mask = False\nmultioutput = 'raw_values' \nfor step in steps: \n    ## prediction\n    rmse, mae = varobj.inverse_transform_evaluate(I1, dfextended,steps=step, mode=mode, \n                                                  multioutput=multioutput, use_mask=use_mask)\n    print('uniform rmse with {:>} step: {:8.4f}'.format(step, rmse.mean() ))\n    print('uniform mae with {:>} step: {:8.4f}'.format(step, mae.mean() ))\n    ## plot\n    varobj.plot(use_mask=use_mask, figsize=(20, 10))","22c370f7":"# targets plot\nplt.subplot(2,1,1)\ndf['Flow_Rate'].plot(figsize=(15, 3), ylabel='Flow_Rate', xlabel='')\nplt.subplot(2,1,2)\ndf['Lake_Level'].plot(figsize=(15, 3), ylabel='Lake_Level')\nplt.tight_layout()\nplt.show()","c7fd439c":"# OIR Plot Analysis\nvarobj.irplot(impulse=train.columns, periods=15, response=targets, orth=True,\n             figsize=(17,20), method='hall')","82f296a0":"# FEVD Plot Analysis \nvarobj.fevd_plot(periods=11, top=5, figsize=(14, 9), max_str_split=3) ","bacfd012":"Image(\"..\/input\/aceawaterimages\/bilancino1.png\")","4c89ecb9":"Image(\"..\/input\/aceawaterimages\/bilancino2.png\")","0ee8cebf":"# Feature engineering & imputation\n## no mask to deal with\ndfextended = df.copy()\n## Reorder \ncollector = []\nfor regex in orderlist:\n    collector.append(dfextended.filter(regex=regex).columns)\ndfextended = dfextended.reindex(columns= np.concatenate(collector) )\n# Deal with Time Periodicity \nyear = 365.2425\ndfextended['year_sin'] = np.sin(dfextended.index.dayofyear * 2 * np.pi \/ year)","57c33200":"# Split data\nN = dfextended.shape[0]\ntrain, val, test = dfextended[:int(0.8*N)], dfextended[int(0.8*N):int(0.9*N)], dfextended[int(0.9*N):]\n\n## define indices\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in targets]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes are:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","8dafa577":"# Initialize data loader: 1-step-ahead prediction\n## parameters setting\ninput_width = varlag     # suggested by VAR\nlabel_width = 1       \nshift = 1\nbatch_size = 2\n## generate window\nwindow = WindowGenerator(input_width, label_width, shift, train_scaled, val_scaled, test_scaled, \n                         batch_size=batch_size, shuffle=False, mask=None, \n                         label_columns=targets)\nprint(window)\nprint('Inputs\/labels shapes of window: {} {}'.format(\n    window.example[0].shape, window.example[1].shape))","35978d1d":"# 1-step-ahead LSTM prediction\n\nlstm_model = None\nif os.path.isfile(os.path.join(modelpath, 'bilancino.keras')):\n    lstm_model = tf.keras.models.load_model(os.path.join(modelpath, 'bilancino.keras'))\nelse: \n    # ## Parameters setting:\n    MAX_EPOCHS = 100\n    patience = 10        # stop training if loss increase for 'patience' periods in total.\n    lr_rate = 1e-3\n    train_verbose = 0    # 0: no output, 1: progress, 2: report\n    lstm_kernel = 16\n    stateful = False     # batchsize must be 1 if True. (takes 10 mins)\n    dropout = 0.2        # dropout rate in [0,1]\n\n    ## fixed setting\n    n_features = train_scaled.shape[1]\n    n_out = len(targets)\n    OUT_STEPS = window.label_width\n\n    ## RNN: LSTM\n    lstm_model = tf.keras.Sequential([\n        # Shape [batch, time, features] => [batch, lstm_units]\n        tf.keras.layers.LSTM(lstm_kernel, return_sequences=True, stateful=stateful),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.LSTM(int(lstm_kernel\/2), return_sequences=False, stateful=stateful),\n        # Shape => [batch, out_steps*features]\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(OUT_STEPS*n_out),\n        # Shape => [batch, out_steps, features]\n        tf.keras.layers.Reshape([OUT_STEPS, n_out])\n    ])\n    ## compile and fit\n    print('window RNN')\n    print('Xtrain shape: ', window.example[0].shape, 'ytrain shape: ', window.example[1].shape, end='')\n    print('\\toutput shape: ', lstm_model(window.example[0]).shape, end='\\n\\n')\n    history = compile_and_fit(lstm_model, window, MAX_EPOCHS, patience, lr_rate, verbose=train_verbose)\n    ## save model\n    lstm_model.save(os.path.join(modelpath, 'bilancino.keras'), overwrite=False)\n    \n## evaluate on test\nprint('evaluation before scaling back:')\nlstm_model.evaluate(window.test, verbose=2, return_dict=True)\n#window.plot(lstm_model, use_mask=True)\n\n## inverse_transform_evaluate on test\ndi = window.inverse_transform_evaluate(lstm_model, scaler, which='test', return_dict=True)\nprint('evaluation after scaling back: {0}: {2:6.4f} - {1}: {3:6.4f}'.format(*di.keys(), *di.values()))\nwindow.plot(use_mask=False, figsize=(20, 10))","7193ac42":"# LSTM feature importance plot\nimportance, direction = feature_importance(lstm_model, window.X_test, scale=0.2, method='const')\nfeature_importance_plot(importance, direction, names=train_scaled.columns)","cfddac4f":"# parameters setting\ndataset = 'River_Arno.csv'\ntargets = ['Hydrometry_Nave_di_Rosano']\n\n# read dataset as DataFrame\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'], date_parser=dateparse)\n\n# re-order features\norderlist = ['Rain.*', 'Temp.*', *targets] # 14 rainfalls, 1 Temperature, 1 Hydrometry\ncollector = []\nfor regex in orderlist: \n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\ndf.index.freq = 'd'\ndf.sort_index(inplace=True)\n# df.describe()\n\n# Set weird values to nan\n# visual graph of nan locations for each field\nmissingval_plot(df, show=False)\nplt.axvline(x = 2191, c='r') # 2004-01-01\nplt.axvline(x = 3473, c='r') # 2007-07-06\nplt.show()\n# mass missing in 1998-01-01 -- 2003-12-31 and 2007-07-07 -- 2020-06-30","e7d3323b":"# Dealing with missing values\ndf = df['2004-01-01':'2007-07-06']\n# imputation & create mask\nmaskdf = df.isna()\ndf = fillna(df, approach='regression')","a0c60f1c":"# correlation plot\nfeature_plots(df)\ncorrtri_plot(df)","93db15e7":"# Feature engineering & imputation\ndfextended = df.copy()\nmask = maskdf.copy()\n\n## Engineer 1\n# tmp = dfextended.filter(regex='Rainfall.*').columns\n# dfextended['Rainfall_mean'] = dfextended[tmp].sum(axis=1)\n# dfextended.drop(columns=tmp, inplace=True)\n\n## Engineer 2: rolling\n# tmp = dfextended.filter(regex='Rain.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('sum')\n# tmp = dfextended.filter(regex='Vol.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('mean')\n# tmp = dfextended.filter(regex='Temp.*').columns\n# dfextended[tmp] = dfextended[tmp].rolling(30).agg('mean')\n# dfextended.dropna(inplace=True)\n\n## pca \n# pca_plot(dfextended.filter(regex='Rain.*')  )\n## Drop due to pca\n# tmp = ['Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_Stia', 'Rainfall_Bibbiena']\n# dfextended.drop(columns=dfextended.filter(regex='Rain.*').columns.drop(tmp), inplace=True)\n\n## Reorder \ncollector = []\nfor regex in orderlist:\n    collector.append(dfextended.filter(regex=regex).columns)\ndfextended = dfextended.reindex(columns= np.concatenate(collector) )\n\n## so does mask\n# mask['Rainfall_mean'] = False\n# mask = mask[29:]\nfor col in df.columns.drop(dfextended.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True)\nassert mask.shape == dfextended.shape, f'mask shape {mask.shape} doesn\\'t match dfimputed shape {dfextended.shape}'\nmask = mask.reindex(columns=dfextended.columns)","9d54837e":"# Chekc stationarity\n## adftest\nadftable = adfuller_table(dfextended, verbose=False, alpha=0.05, maxlag=5, regression='ct')\n# display(adftable)\n\n## collect non-stationary features\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\nprint('Differenced features: ', I1)","5d2875cf":"## (Cont'd) take diff() on cols that do not pass the adftest. Here we adopt AIC.\ndfstationary = dfextended.copy()\ndfstationary.loc[:,I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)\n\n## adftest again\nadftable = adfuller_table(dfstationary, verbose=False, alpha=0.05, maxlag=5, regression='ct')\n# display(adftable) # This time all features pass ADF test in both AIC and BIC criteria.","1de8319e":"# Check granger causality\ngrtable = grangers_causation_table(dfstationary, xnames=dfextended.columns.drop(labels=targets), \n                          ynames=targets, maxlag=3,alpha=0.05)\ndisplay(grtable)\n# record features that don't pass Granger\nfail_feature = grtable[grtable.iloc[:,0] == False].index.str.strip('_x').values\n\n# Manually drop non Granger-cause variables\n# dfstationary.drop(columns=fail_feature, inplace=True)\n# mask.drop(columns=fail_feature, inplace=True)","896a04a0":"# Deal with Time Periodicity \nyear = 365.2425\ndfstationary['year_sin'] = np.sin(dfstationary.index.dayofyear * 2 * np.pi \/ year)","4a7128ed":"# Split data\nN = dfstationary.shape[0]\ntrain, val, test = dfstationary[:int(0.8*N)], dfstationary[int(0.8*N):int(0.9*N)], dfstationary[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in [targets[0]]]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","c4f4fa5c":"# Get sense: ac & pac plots\nacpac_plot(dfstationary[targets], )","75457c9a":"# VAR model \nvar_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=33, trend='ct')\nprint(res.selected_orders)\nvarlag = res.selected_orders['aic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test, mask=tmask[1:] , label_columns=targets) #[1:]\nprint('The model we are using: {1:}-dim VAR({0:}).'.format(*varobj.model.coefs.shape[:2]))","1fb172c4":"# Durbin_watson test of residuals\nprint('Durbin-Watson test:')\n_ = varobj.durbin_watson_test()","6709c28e":"# Adjusted Portmannteau-test & S-K test\ndisplay(var_model.test_whiteness(nlags=varlag*4, signif=0.05, adjusted=True).summary())\ndisplay(varobj.model.test_normality().summary())\n# Residual Autocorrelations\nvarobj.residac_plot(cols=targets, figsize=(15, 3))","017cb25a":"# 1-step-ahead Prediction\n## paramters setting:\nsteps=1\nmode = 'normal'\nuse_mask = True\nmultioutput = 'raw_values' \n\n## after trransformation\nrmse, mae = varobj.inverse_transform_evaluate(\n    I1, dfextended, steps=steps, mode=mode, multioutput=multioutput, use_mask=use_mask)\nprint('uniform rmse: {:8.4f}'.format(rmse.mean()))\nprint('uniform mae: {:8.4f}'.format(mae.mean()))\n\n## plot\nvarobj.plot(use_mask=use_mask, figsize=(20, 5))","a084e722":"# Last Prediction\n## paramters setting\nsteps=[7, 14]\nmode = 'last'\nuse_mask = True\nmultioutput = 'raw_values' \n## prediction\nfor step in steps:\n    rmse, mae = varobj.inverse_transform_evaluate(\n        I1, dfextended, steps=step, mode=mode, multioutput=multioutput, use_mask=use_mask)\n    print('uniform rmse with {:>} step: {:8.4f}'.format(step, rmse.mean()))\n    print('uniform mae with {:>} step: {:8.4f}'.format(step, mae.mean()))\n    ## plot\n    varobj.plot(use_mask=use_mask, figsize=(20, 5))","db276f9c":"# OIR Plot Analysis\nvarobj.irplot(impulse=train.columns, periods=15, response=targets, orth=True,\n             figsize=(17,20), method='hall')","1be1f413":"# FEVD Plot Analysis \nvarobj.fevd_plot(periods=11, top=5, figsize=(14, 9), max_str_split=3) ","fb535daa":"# Feature engineering & imputation\ndfextended = df.copy()\nmask = maskdf.copy()\n\n## FE\n# tmp = ['Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_Stia', 'Rainfall_Bibbiena']\n# dfextended.drop(columns=dfextended.filter(regex='Rain.*').columns.drop(tmp), inplace=True)\n\n## Reorder \ncollector = []\nfor regex in orderlist:\n    collector.append(dfextended.filter(regex=regex).columns)\ndfextended = dfextended.reindex(columns= np.concatenate(collector) )\n\n# Deal with Time Periodicity \nyear = 365.2425\ndfextended['year_sin'] = np.sin(dfextended.index.dayofyear * 2 * np.pi \/ year)\nmask['year_sin'] = False\n\nfor col in df.columns.drop(dfextended.columns, errors='ignore'): \n    mask.drop(columns=col, inplace=True)\nassert mask.shape == dfextended.shape, f'mask shape {mask.shape} doesn\\'t match dfextended shape {dfextended.shape}'\nmask = mask.reindex(columns=dfextended.columns)","2d0fbe2a":"# Split data\nN = dfextended.shape[0]\ntrain, val, test = dfextended[:int(0.8*N)], dfextended[int(0.8*N):int(0.9*N)], dfextended[int(0.9*N):]\n_, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\n\n## define indices\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in [targets[0]]]\n\n# Scaling and Normalization \nscaler, train_scaled, val_scaled, test_scaled = scale(train, val, test, approach='MinMax')\nprint('train\/val\/test shapes are:', train_scaled.shape, val_scaled.shape, test_scaled.shape)","334c8f8a":"# Initialize data loader: 1-step-ahead prediction\n## parameters setting\ninput_width = varlag     # suggested by VAR\nlabel_width = 1       \nshift = 1\nbatch_size = 4\n## generate window\nwindow = WindowGenerator(input_width, label_width, shift, train_scaled, val_scaled, test_scaled, \n                         batch_size=batch_size, shuffle=False, mask=tmask, \n                         label_columns=targets)\nprint(window)\nprint('Inputs\/labels shapes of window: {} {}'.format(\n    window.example[0].shape, window.example[1].shape))","fcd338df":"# 1-step-ahead LSTM prediction\n\nlstm_model = None\nif os.path.isfile(os.path.join(modelpath, 'arno.keras')):\n    lstm_model = tf.keras.models.load_model(os.path.join(modelpath, 'arno.keras'))\nelse: \n    # ## Parameters setting:\n    MAX_EPOCHS = 100\n    patience = 10        # stop training if loss increase for 'patience' periods in total.\n    lr_rate = 1e-3\n    train_verbose = 0    # 0: no output, 1: progress, 2: report\n    lstm_kernel = 16\n    stateful = False     # batchsize must be 1 if True. (takes 10 mins)\n    dropout = 0.3        # dropout rate in [0,1]\n\n    n_features = train_scaled.shape[1]\n    n_out = len(targets)\n    OUT_STEPS = window.label_width\n\n    ## RNN: LSTM\n    lstm_model = tf.keras.Sequential([\n        # Shape [batch, time, features] => [batch, lstm_units]\n        tf.keras.layers.LSTM(lstm_kernel, return_sequences=True, stateful=stateful),\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.LSTM(int(lstm_kernel\/2), return_sequences=False, stateful=stateful),\n        # Shape => [batch, out_steps*features]\n        tf.keras.layers.Dropout(dropout),\n        tf.keras.layers.Dense(OUT_STEPS*n_out),\n        # Shape => [batch, out_steps, features]\n        tf.keras.layers.Reshape([OUT_STEPS, n_out])\n    ])\n    ## compile and fit\n    print('window RNN')\n    print('Xtrain shape: ', window.example[0].shape, 'ytrain shape: ', window.example[1].shape, end='')\n    print('\\toutput shape: ', lstm_model(window.example[0]).shape, end='\\n\\n')\n    history = compile_and_fit(lstm_model, window, MAX_EPOCHS, patience, lr_rate, verbose=train_verbose)\n    ## save model\n    lstm_model.save(os.path.join(modelpath, 'arno.keras'), overwrite=False)\n    \n## evaluate on test\nprint('evaluation before scaling back:')\nlstm_model.evaluate(window.test, verbose=2, return_dict=True)\n#window.plot(lstm_model, use_mask=True)\n\n## inverse_transform_evaluate on test\ndi = window.inverse_transform_evaluate(lstm_model, scaler, which='test', return_dict=True)\nprint('evaluation after scaling back: {0}: {2:6.4f} - {1}: {3:6.4f}'.format(*di.keys(), *di.values()))\nwindow.plot(use_mask=True, figsize=(20, 6))","1ab65974":"# LSTM feature importance plot\nimportance, _ = feature_importance(lstm_model, window.X_test, scale=0.2, method='const')\nfeature_importance_plot(importance, names=train_scaled.columns)","036bd4af":"dataset = 'Water_Spring_Amiata.csv'\ntargets = ['Flow_Rate_Bugnano', 'Flow_Rate_Arbure', 'Flow_Rate_Ermicciolo', 'Flow_Rate_Galleria_Alta']\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\n\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'], date_parser=dateparse)\n\n# The order of the variables matters for the later analysis.\norderlist = ['Rain.*', 'Temp.*', 'Depth.*', *targets]\ncollector = []\nfor regex in orderlist:\n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\ndf.index.freq = 'd'\ndf.sort_index(inplace=True)\n\n# Set weird values to nan\n## set nan to Groundwater whose values are 0\ndf[df.filter(regex='Depth_.*').columns] = df.filter(regex='Depth_.*').replace(0, np.nan)\n## set nan to Volume whose values are 0\ndf[df.filter(regex='Flow_.*').columns] = df.filter(regex='Flow.*').replace(0, np.nan)","796c83d9":"# water_spring_feature_plots(df)\nmissingval_plot(df.reset_index().T.isna())","7a3295cb":"# Record nan positions before imputation\nmask = df.isna()\ndfimputed = df.copy()\ndfimputed = fillna(dfimputed, approach='interpolate', method='linear').copy()\ntodrop = df[df.Temperature_Abbadia_S_Salvatore.isna()].index.max()\ndfimputed = dfimputed[dfimputed.index > todrop].copy()\nmask = mask[mask.index > todrop].copy()","d5cd3ed4":"# Stationarity tests\nadftable = adfuller_table(dfimputed, verbose=False, alpha=0.05, maxlag=30, regression='ct')\ndisplay(adftable)\n\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\ndfstationary = dfimputed.copy()\ndfstationary.loc[:, I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)","2c1e8181":"adftable = adfuller_table(dfstationary, verbose=False, alpha=0.05, maxlag=30, regression='ct')\ndisplay(adftable)\n\nI2 = adftable[adftable['AIC_5%level'] == False].index.values\ndfstationary2 = dfstationary.copy()\ndfstationary2.loc[:, I2] = dfstationary[I2].diff()\ndfstationary2.dropna(inplace=True)\n\ncorrtri_plot(dfstationary2)","60e2b4e5":"N = dfstationary2.shape[0]\ntrain, val, test = dfstationary2[:int(0.8*N)], dfstationary2[int(0.8*N):int(0.9*N)], dfstationary2[int(0.9*N):]\ntrmask, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in targets]","4667708d":"acpac_plot(dfstationary2[targets], figsize=(10, 2))","96e4c205":"# VAR model \nvar_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=10, trend='ct')\nprint('Lag orders accoring to information criteria:')\nprint(res.selected_orders)\nvarlag = res.selected_orders['aic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test , label_columns=targets)  # , mask=tmask[1:])\nprint('\\nDurbin-Watson test:')\n_ = varobj.durbin_watson_test()\ndisplay(var_model.test_whiteness(nlags=varlag*5, signif=0.05, adjusted=False).summary())\ndisplay(varobj.model.test_normality().summary())\nvarobj.residac_plot(cols=targets, figsize=(12, 5))","a544c787":"# OIR analysis leads to insignificant results\nif len(I2)>0:\n    j = 2\nelif len(I1):\n    j=1\nelse:\n    j=0\nvarobj = VARresults(var_model, test, mask=tmask[j:], label_columns=targets) #\nvarobj.inverse_transform_evaluate(I1, dfimputed, use_mask=True)\nvarobj.fevd_plot(top=7, figsize=(12, 10))\n\nvarobj.plot(use_mask=True, figsize=(20, 12))","4cb2b2f7":"mode = 'normal'\nuse_mask = True\nmultioutput = 'raw_values' \n\n## after trransformation\nsteps=1\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=7\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=14\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=28\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)","7f4b2b55":"dfvecm = dfimputed.copy()\ndfvecm.loc[:, I2] = dfimputed[I2].diff()\ndfvecm.dropna(inplace=True)\nN = dfvecm.shape[0]\ntrain, test = dfvecm[:int(0.9*N)], dfvecm[int(0.9*N):]\n_, tmask = mask[:int(0.9*N)], mask[int(0.9*N):]","e8a1ba1e":"vecmobj = VECM(train.astype(float), test, det='ci', mask=tmask[1:], label_columns=targets)\n\nvecm_order = vecm.select_order(data=train, maxlags=7)\nvecm_order = vecm_order.selected_orders\nprint('VECM order: ', vecm_order)\nprint('\\n')\nvecm_order = vecm_order['aic']\nvecm_coint = vecm.select_coint_rank(endog=train, k_ar_diff=1, det_order=0, method='maxeig')\nprint(vecm_coint.summary())\nprint('\\n')\nvecm_coint = vecm.select_coint_rank(endog=train, k_ar_diff=1, det_order=0, method='trace')\nprint(vecm_coint.summary())\nprint('\\n')\nvecm_coint = vecm_coint.rank\nprint('Chosen cointegration order is ', vecm_coint, '.')\n\nres = vecmobj.select_lag(maxlags=12, ic='aic', inplace=True, verbose=True) \nres = vecmobj.select_coint_rank(method='trace', alpha=0.05, inplace=True, verbose=True)\nvecmobj.build()\n\nprint(end='\\n'*2)\nprint(vecmobj, end='\\n'*2)\nprint('Durbin-Watson test:')\nvecmobj.durbin_watson_test()\nvecmobj.residac_plot(cols=targets, figsize=(12,5))","a8037b77":"print('OIR of Flow_Rates to Flow_Rates')\nvecmobj.irplot(impulse=targets, response=targets, orth=True, method='asym', figsize=(17, 15))\nprint('OIR of Rainfalls to Flow_Rates')\nvecmobj.irplot(impulse=df.filter(regex=\"Rain.*\", axis=1).columns, response=targets, orth=True, method='asym', figsize=(17, 15))\nprint('OIR of Depth_to_Groundwaters to Flow_Rates')\nvecmobj.irplot(impulse=df.filter(regex=\"Depth.*\", axis=1).columns, response=targets, orth=True, method='asym', figsize=(17, 15))","f687df4c":"steps = [1, 7, 14, 28]\nfor s in steps:\n    vecm_rmse_mae(steps=s, train=train, test=test, vecm_order=vecm_order, vecm_coint=vecm_coint)\n    print('\\n')","618ad0ac":"dataset = 'Water_Spring_Lupa.csv'\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'], date_parser=dateparse)\ntargets = df.filter(regex='Flow.*', axis=1).columns\n\n# The order of the variables matters for the later analysis.\norderlist = ['Rain.*', 'Temp.*', 'Depth.*', *targets]\ncollector = []\nfor regex in orderlist:\n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\ndf.index.freq = 'd'\ndf.sort_index(inplace=True)\n\n# Set weird values to nan\n## set nan to Groundwater whose values are 0\ndf[df.filter(regex='Flow_.*').columns] = df.filter(regex='Flow_.*').replace(0, np.nan)","ffc92b68":"missingval_plot(df.reset_index().T.isna())\n\nfig, ax = plt.subplots(1,2)\ndf.Rainfall_Terni.plot(ax=ax[0], xlabel='', title='Rainfall_Terni', figsize=(18,5))\ndf.Flow_Rate_Lupa.plot(ax=ax[1], xlabel='', title='Flow_Rate_Lupa')\nplt.show()\n","650ed332":"# Record nan positions before imputation\nmask = df.isna()\ndfimputed = df.copy()\n\ndfimputed = fillna(dfimputed, approach='interpolate', method='linear').copy()\n\ntodrop = '2009-02-19'\ndfimputed = dfimputed[dfimputed.index > todrop].copy()\n\nmask = mask[mask.index > todrop].copy()\ndfimputed = dfimputed.drop('Date', axis=1, errors='ignore')","d5524e5f":"# Stationarity tests\nadftable = adfuller_table(dfimputed, verbose=False, alpha=0.05, maxlag=60, regression='c') # c instead of ct\ndisplay(adftable)\n\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\ndfstationary = dfimputed.copy()\ndfstationary.loc[:, I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)","893d550f":"adftable = adfuller_table(dfstationary, verbose=False, alpha=0.05, maxlag=60, regression='ct') # c instead of ct\nI2 = adftable[adftable['AIC_5%level'] == False].index.values\ndfstationary2 = dfstationary.copy()\ndfstationary2.loc[:, I2] = dfstationary[I2].diff()\ndfstationary2.dropna(inplace=True)\n\ncorrtri_plot(dfstationary2, figsize=(5,5))\n\nadftable = adfuller_table(dfstationary2, verbose=False, alpha=0.05, maxlag=60, regression='ct') # c instead of ct","8434ee1e":"N = dfstationary2.shape[0]\ntrain, val, test = dfstationary2[:int(0.8*N)], dfstationary2[int(0.8*N):int(0.9*N)], dfstationary2[int(0.9*N):]\ntrmask, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in targets]","cd7f21ca":"acpac_plot(dfstationary2[targets], figsize=(12, 4))\nperiodogram(dfstationary2[targets[0]], division=2)","7f71a501":"# VAR model \nvar_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=50, trend='ct')\nprint('Lag orders accoring to information criteria:')\nprint(res.selected_orders)\nvarlag = res.selected_orders['bic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test , label_columns=targets)\nprint('\\nDurbin-Watson test:')\n_ = varobj.durbin_watson_test()\n\ndisplay(var_model.test_whiteness(nlags=varlag*5, signif=0.05, adjusted=False).summary())\ndisplay(varobj.model.test_normality().summary())\n\nvarobj.residac_plot(cols=targets, figsize=(15,3))","e5b6d54e":"mode = 'normal'\nuse_mask = True\nmultioutput = 'raw_values' \n\nsteps=1\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=7\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=14\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=28\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)","cba04ceb":"# OIR analysis leads to insignificant results\nif len(I2)>0:\n    j = 2\nelif len(I1):\n    j=1\nelse:\n    j=0\nvarobj = VARresults(var_model, test, mask=tmask[j:], label_columns=targets) #\nvarobj.inverse_transform_evaluate(I1, dfimputed, use_mask=True)\n\nvarobj.plot(use_mask=True, figsize=(10,8))\n\nvarobj.irplot(impulse=train.columns, response=targets, orth=True, method='asym', figsize=(12, 4), \n             fontsize=10, supfontsize=13)","76bbd6e1":"dataset = 'Water_Spring_Madonna_di_Canneto.csv'\ndateparse = lambda x: datetime.strptime(x, '%d\/%m\/%Y')\ntargets = ['Flow_Rate_Madonna_di_Canneto']\ndf = pd.read_csv(os.path.join(datapath, dataset), index_col='Date', parse_dates=['Date'])\n\n# The order of the variables matters for the later analysis.\norderlist = ['Rain.*', 'Temp.*', 'Depth.*', *targets]\ncollector = []\nfor regex in orderlist:\n    collector.append(df.filter(regex=regex).columns)\ndf = df.reindex(columns= np.concatenate(collector) )\n\n# define frequency and sort accordingly.\n#df.index.freq = 'd'\ndf.sort_index(inplace=True)\n\n# Set weird values to nan\n## set nan to Groundwater whose values are 0\ndf[df.filter(regex='Depth_.*').columns] = df.filter(regex='Depth_.*').replace(0, np.nan)\n## set nan to Volume whose values are 0\ndf[df.filter(regex='Flow_.*').columns] = df.filter(regex='Flow.*').replace(0, np.nan)\n## set nan to Temperature whose values are 0\ndf[df.filter(regex='Temperature_.*').columns] = df.filter(regex='Temperature_.*').replace(0, np.nan)","d5151b4d":"# water_spring_feature_plots(df)\nmissingval_plot(df.reset_index().T.isna())\n\nfig, ax = plt.subplots(1,2)\ndf.Rainfall_Settefrati.plot(ax=ax[0], xlabel='', title='Rainfall_Settefrati', figsize=(20,5))\ndf.Temperature_Settefrati.plot(ax=ax[1], xlabel='', title='Temperature_Settefrati')\nplt.show()\ndf.Flow_Rate_Madonna_di_Canneto.plot(xlabel='', title='Flow_Rate_Madonna_di_Canneto', figsize=(20,5))\nplt.show()","ba891079":"# Record nan positions before imputation\nmask = df.isna()\ndfimputed = df.copy()\n\ntodrop = df[df.Rainfall_Settefrati.isna()].index.min()\n\ndfimputed = dfimputed[dfimputed.index < todrop].copy()\nmask = mask[mask.index < todrop].copy()\n\ndfimputed = fillna(dfimputed, approach='interpolate', method='linear').copy()\n\ntodrop = '2015-01-03'\ndfimputed = dfimputed[dfimputed.index > todrop].copy()\nmask = mask[mask.index > todrop].copy()","a4673666":"# Stationarity tests\nadftable = adfuller_table(dfimputed, verbose=False, alpha=0.05, maxlag=30, regression='ct')\ndisplay(adftable)\n\nI1 = adftable[adftable['AIC_5%level'] == False].index.values\ndfstationary = dfimputed.copy()\ndfstationary.loc[:, I1] = dfstationary[I1].diff()\ndfstationary.dropna(inplace=True)","e04b49af":"adftable = adfuller_table(dfstationary, verbose=False, alpha=0.05, maxlag=35, regression='ct')\nprint('Checking again with an ADF test for stationarity: ')\ndisplay(adftable)\n\nI2 = adftable[adftable['AIC_5%level'] == False].index.values\ndfstationary2 = dfstationary.copy()\ndfstationary2.loc[:, I2] = dfstationary[I2].diff()\ndfstationary2.dropna(inplace=True)\n\ncorrtri_plot(dfstationary2, figsize=(7,7))\n\ndfstationary2 = dfstationary2.drop('Date', axis=1, errors='ignore')","11a52fdd":"N = dfstationary2.shape[0]\ntrain, val, test = dfstationary2[:int(0.8*N)], dfstationary2[int(0.8*N):int(0.9*N)], dfstationary2[int(0.9*N):]\ntrmask, vmask, tmask = mask[:int(0.8*N)], mask[int(0.8*N):int(0.9*N)], mask[int(0.9*N):]\ncolumn_indices = {name: i for i, name in enumerate(train.columns)}\ntargets_indices = [column_indices[ta] for ta in targets]","53f10473":"acpac_plot(dfstationary2[targets], figsize=(10, 3) )\nperiodogram(dfstationary2[targets[0]], division=2)","2eb584ab":"var_model = VAR(endog=train.append(val))\nres = var_model.select_order(maxlags=50, trend='ct')\nprint('Lag orders accoring to information criteria:')\nprint(res.selected_orders)\nvarlag = res.selected_orders['bic']\nvar_model = var_model.fit(varlag)\n# put result into self-defined class\nvarobj = VARresults(var_model, test , label_columns=targets)\nprint('\\n Durbin-Watson test:')\n_ = varobj.durbin_watson_test()\n\ndisplay(var_model.test_whiteness(nlags=varlag*5, signif=0.05, adjusted=False).summary())\ndisplay(varobj.model.test_normality().summary())\n\nvarobj.residac_plot(cols=targets, figsize=(15,3))","2ccbcc74":"mode = 'normal'\nuse_mask = True\nmultioutput = 'raw_values' \n## after transformation\nsteps=1\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=7\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=14\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)\nprint('\\n')\nsteps=28\nvar_error_I2(steps=steps, I1=I1, I2=I2, test=test, var_fit=var_model, targets=targets)","31636c80":"# OIR analysis leads to insignificant results\nif len(I2)>0:\n    j = 2\nelif len(I1):\n    j=1\nelse:\n    j=0\nvarobj = VARresults(var_model, test, mask=tmask[j:], label_columns=targets) #\nvarobj.inverse_transform_evaluate(I1, dfimputed, use_mask=True)\nvarobj.fevd_plot(top=3)\n\nvarobj.plot(use_mask=True, figsize=(19, 5))\nvarobj.irplot(impulse=train.columns, response=targets, orth=True, method='asym', figsize=(19, 5), \n             fontsize=12, supfontsize=16)","c8cacdf7":"dfvecm = dfimputed.copy()\ndfvecm.loc[:, I2] = dfimputed[I2].diff()\ndfvecm.dropna(inplace=True)\ndfvecm=dfvecm.drop('Date', axis=1, errors='ignore')\nN = dfvecm.shape[0]\ntrain, test = dfvecm[:int(0.9*N)], dfvecm[int(0.9*N):]\n_, tmask = mask[:int(0.9*N)], mask[int(0.9*N):]","bc9245d5":"vecmobj = VECM(train.astype(float), test, det='ci', mask=tmask[1:], label_columns=targets)\n\nvecm_order = vecm.select_order(data=train, maxlags=20)\n#print(vecm_order.summary())\nvecm_order = vecm_order.selected_orders\nprint('VECM order: ', vecm_order)\nprint('\\n')\nvecm_order = vecm_order['aic']\nvecm_coint = vecm.select_coint_rank(endog=train, k_ar_diff=1, det_order=0, method='maxeig')\nprint(vecm_coint.summary())\nprint('\\n')\nvecm_coint = vecm.select_coint_rank(endog=train, k_ar_diff=1, det_order=0, method='trace')\nprint(vecm_coint.summary())\nprint('\\n')\nvecm_coint = vecm_coint.rank\nprint('Chosen cointegration order is ', vecm_coint, '.')\n\nres = vecmobj.select_lag(maxlags=12, ic='aic', inplace=True, verbose=True) \n#print('lag {} has been stored.'.format(res))\nres = vecmobj.select_coint_rank(method='trace', alpha=0.05, inplace=True, verbose=True)\n#print('coint_rank {} has been stored.'.format(res))\nvecmobj.build()\n\nprint(end='\\n'*2)\nprint(vecmobj, end='\\n'*2)\nprint('Durbin-Watson test:')\nvecmobj.durbin_watson_test()\n\nvecmobj.residac_plot(cols=targets, figsize=(17,4))","98210adb":"vecmobj.irplot(impulse=targets, response=targets, orth=True, method='asym', figsize=(8, 4), fontsize=13, supfontsize=14)\nvecmobj.irplot(impulse=df.filter(regex=\"Rain.*\", axis=1).columns, response=targets, orth=True, \n               method='asym', figsize=(8, 4), fontsize=13, supfontsize=14)\nvecmobj.irplot(impulse=df.filter(regex=\"Temp.*\", axis=1).columns, response=targets, orth=True, \n               method='asym', figsize=(8, 4), fontsize=13, supfontsize=14)","e19f10b9":"steps = [1, 7, 14, 28]\nfor s in steps:\n    vecm_rmse_mae(steps=s, train=train, test=test, vecm_order=vecm_order, vecm_coint=vecm_coint)\n    print('\\n')","6643b50e":"# Libraries, Functions, and Classes","9611b4e1":"## Data Cleansing\n\nThe Aquifer Auser dataset contains mass missing values, especially for features *Volume_CSA and Volume_CSAL*, till 2013-12-31. We drop them (all data before the red vertical line) and impute the rest with simple linear interpolation in the next section. We also **drop Temperature_Ponte_a_Moriano** since it contains too many missings and the temperature pattern can still be captured by the other temperature features.\n\nA first glance at correlation triangle plot shows that our target features *Depth_to_Groundwater_SAL, _CoS and _LT2* are negatively correlated to temperatures, and positively correlated to Hydrometries, Volumes, and Groundwaters. It seems the target features possess a yearly pattern so we will try to explore it (not shown here).\n\nIn the Volume features plot, it seems like the Volume_CSA * \\_CSAL have opposite pattern in the beginning of 2016 but have same suddenly-jump-and-back pattern in 2020. On the other hand, Volume_CC1 * \\_CC2 have same pattern throughout the dataset and in 2020 they jump simultaneously at exactly the same time when Volume_CSA\/\\_CSAL suddenly decrease. A story might be that the drinking water plants switch from on place to another making this pattern un-natural. Thus, it would be intuitive to group the volume features when training the model. However, we put the result of that setting into the report at the end of next section and only present the best model in next section in order for better story telling over OIR analysis.\n\nMask on missing\/imputed data is used in order to get more accurate rmse\/mae performance of models.","5ca48d85":"# Aquifer Petrignano","a97ab3f8":"We use the sensitivity analysis to determine the feature importance, that is, when perturbing a particular feature based on testing data, given the values of the other feature unchanged, how much will rmse change by comparing the prediction before and after perturbation. A feature is more important if the larger the rmse is.\n\nIn the figure below we find that **Temperature_Monteroni_Arbia_Biena, Depth_to_Groundwater_Pozzo_4, and Depth_to_Groundwater_Pozzo_3** etc. are important features to LSTM. We summarize the comparison of important features in the next section.\n\nIt's worth to notice that the LSTM model takes very different featuers imporance into account comapred to VAR model. Due to the mass missing and difficulty of this dataset, we probably have non-linear pattern so VAR models may not perform well; and we have limited data available to train LSTM so that LSTM model result may not be reliable as well. We thus present all results but do not conclude which features are indeed important confirmed by both models.\n","9dcecee5":"# Water Spring Madonna di Canneto\n\nNow we get to analyse the water spring Madonna di Canneto. This dataset consists of one rainfall, one temperature and one depth to groundwater level variable.","146a5cb5":"The FEVD plot is shown below. It illustrates which (orthogonised) feature shocks contribute to the error variance of our target features in different lags. \n\nTo save space again, we won't draw the FEVD figures, but they give similar conclusion as what we have seen in OIR analysis. We omit this part.","9c743942":"# Water Spring Lupa\n\nNow, we will turn to the water spring Lupa. The dataset is different from the others because it only has two variables: The rainfall in Terni and the flow rate of the water spring Lupa which is a time series with a peculiar behavior.","e1a5b7ca":"We use the sensitivity analysis to determine the feature importance, that is, when perturbing a particular feature based on testing data, given the values of the other feature unchanged, how much will rmse change by comparing the prediction before and after perturbation. A feature is more important if the larger the rmse is.\n\nIn the figure below we find that *Rainfall_Le_Croci, Temperature_Firenze, Rainfall_Cavallina* etc. are important features to LSTM. Compared to OIR analysis in VAR, **only Rainfall_Cavallina, Rainfall_Le_Croci** are in common.\n\nWe conclude that Rainfall_Cavallina and Le_Croci are most important feature to Hydrometry_Nave_di_Rosano. Other previous mentioned features are sub-important, part of them to linear model and part of them to non-linear LSTM model.","927d4bb6":"The VECM model choice seems to be fine: The Durbin-Watson test does not reveal any issues with short-term  autocorrelation. After having chosen the lag order and the order of cointegration, we can analyse the quality of the model fit.","62f973d2":"## Bilancino: Daily LSTM\n\nLSTM model doesn't require data to be stationary but better scaled since tanh() is used by LSTM. We then use `MinMaxScaler()` to scale data into $[-1,1]$. One can also use alternative scaler such as `StandardScaler()` by simply changing the `approach='Standard'` in the scale function. `year_sin` is also included to make model learn the yearly pattern better. \n\nHere we use the same lag as VAR to train the LSTM in order for 1D prediction performance comparison. The model can thus be futher fine tuned based on different lags together with other parameters, which we will omit here (by fixing lag=varlag) and focus only on the feature importance learned by the LSTM.\n\nWe didn't implement the LSTM model that predicts 7D\/14D in a row since it's intuitive to directly predict on target features instead of predicting on all features and then feed the prediction to the model again, which is like the behaviour of VAR model.","c5487b91":"The flow rate of Lupa and the rainfall in Terni are stationary but very laggy, indicating a long-memory process. We will still continue with a VAR process: First, AR processes are often able to deliver decent predictions even if there is fractional integration in the data. Second, with a VAR process we are able to utilise the rainfall data for our predictions.","17670dbf":"The Durbin-Watson and the Portmanteau test indicate that there is no major autocorrelation. The autocorrelation plots still show some peaks which is typical for the water spring datasets.","41e7e48a":"The flow rates appear to have a regular pattern.","65182087":"## VAR Class","dd743dea":"## Madonna di Canneto: Daily VECM","36327ef8":"1D predictions of the model is neither good nor bad but the 7D\/14D are the best among the settings we've tried (reported at the end of this section).\n\nThe 1D prediction error mainly comes from the sudden jump target features: 1, 2, 8. <br\/>\nFor the 7D\/14D prediction errors, they mainly come from 1,3,5,7,9.\n\nThe model seems can only capture long term patterns since it always predict a line rather than a curve.","d2fefdf0":"We use the sensitivity analysis to determine the feature importance, that is, when perturbing a particular feature based on testing data, given the values of the other feature unchanged, how much will rmse change by comparing the prediction before and after perturbation. A feature is more important if the larger the rmse is.\n\nIn the figure below we find that *Flow_Rate*, *Temperature_Le_Croci*, and 'Rainfall_Mangona' etc. are important features to LSTM. We summarize the comparison of important features in the next section.\n\nWe conclude that besides target features, all rainfalls except Cavallina and S_Agata important feature to LSTM when predicting Lake_Level and Flow_Rate.","5a92d45b":"In the following, some plots will reveal some features of the data that are relevant for the later analysis. In order to have a time slot that is representative of all the variables, we will use those data from 2016 onwards. The depths to groundwater \u2013 as in the previous datasets \u2013 appear very laggy and non-stationary. The same case can eb made for flow rates.","eeac425f":"We've implemented various setting of model and find that it is difficlut to conclude which one is the best: some performs the best on 1D when using rmse as the measure but it no longer is the best model when using mae measure.\nAt least we still can get some insights from these experiments.\n\nFirstly, models created by summary features or PCA performs generally better on longer term predictions. Secondly, \ndropping features either via Granger or via PCA makes rmse\/mae smaller, meaning that multiple features are reptead inforamtion or just a noise to confuse the model. Thirdly, smoothing all features except the target does the sub-optimal performance, indicating that this transformation doens't make the model learn daily pattern. Finally, `year_sin` genearlly doesn't improve performance, so the target doesn't possess a cycle pattern in testing data.\n\nSince VAR model is linear and thus can only capture local linear prediction, we present LSTM as well trying to capture the possible non-linear relationship between target and features in the following.","d05fc439":"## Data Cleansing","bc9e5408":"The 1D prediction of baseline is good but not the 7D\/14D predictions. Prediction errors are roughly the same for both features.","e35e9ae4":"Since forecast error impulse response (FEIR) only analyzes shock occuring in one variable at a time, contemporaneous correlation of error terms cannot be captured by it. We then use orthogonalized impulse response (OIR) to solve this issue, which rely on stronger assumption and thus the order of features in model matters. Besides, the Hall's confidence interval is used since it usually has bias corrected effect compared to Efron.\n\nThe feature order we choose is **Rainfall $\\rightarrow$ Temperature $\\rightarrow$ Volumes ($\\rightarrow$ Hydrometries) $\\rightarrow$ Depth_to_Groundwaters**. \n\nActually, the order between rainfall and temperature can be reversed since they affect each other. Depth_to_Groundwaters is put at the end because we want to see how the other features affect the targets. Volumes (the amount taken from the dinking water treatment plant) clearly affects Hydrometries, so volumes are put before hydrometries.\n\nThe meaning of OIR is to see whether one standard deviation shock to x causes significant (increase\/decrease standard deviation) response in y for m periods. We show response of the first target feature only since the two targets are perfectly correlated with each other and the level patterns in data are the same. \n\nSince OIR plot will compain Non-Positive-Definite-Matrix Error of the error term, so Choelsky decomposition cannot be applied, we then turn to FEIR plot (`orth=False`). Unfortunately, none of the features causes a significant orthogonalized impulse response to our target feature *Depth_to_Groundwater_Podere_Casetta*. FEIR plot is thus not showned here. We move on to FEVD plot in the following to see further analysis of these features.\n","16cb32f7":"From the volumes plots below, we may sum Volume_Pozzo_2, \\_4, \\_5_6, and \\_8 due to the similar reason as specified in Section 3.1 (Aquifer Auser), ie., they possess similar pattern. We also try various grouping and find that this setting performs the best on 1D prediction.","996a33b4":"Since forecast error impulse response (FEIR) only analyzes shock occuring in one variable at a time, contemporaneous correlation of error terms cannot be captured by it. We then use orthogonalized impulse response (OIR) to solve this issue, which rely on stronger assumption and thus the order of features in model matters. Besides, the Hall's confidence interval is used since it usually has bias corrected effect compared to Efron.\n\nThe feature order we choose is **Rainfall $\\rightarrow$ Temperature $\\rightarrow$ Flow_Rate $\\rightarrow$ Lake_Level**. Actually, the order between rainfall and temperature can be reversed since they affect each other, and it seems like the lake level reaches its peak when flow_rate increase base on the figures in `df['Flow_Rate'].plot(); df['Lake_Level'].plot()`. But one can also explain the relation reversely, so Flow_Rate and Lake_Level order can be exchanged. Here we just assume the order and see the effect of each feature on targets. At least it makes sense that rainfall and temperature can affect the flow rate and lake level.\n\nThe meaning of OIR is to see whether one standard deviation shock to x causes significant (increase\/decrease standard deviation) response in y for m periods. \n\nWe find that in the OIR plot the **Rainfall_S_Piero, Rainfall_Mangona, Rainfall_Cavallina, and Rainfall_Le_Croci** shocks cause significant positive response to *Flow_Rate*. **Although Flow_Rate doesn't response immediately to Lake_Level shock due to the orthogonalized assumption (Cholesky decomposition\/order selection in previous paragraph), but var lags help it to response significant after lag 8** with value= 0.1 standard deviation as shown in the plot. TemperatureLe_Croci schock is also significant but the effect is only half of 0.1.\n\n**Rainfall_S_Piero and Rainfall_Mangona** causes significantly positive effect to *Lake_Level*. **Flow_Rate** cuases significantly NEGATIVE effect to *Lake_Level*, the later the larger. This matches the observation that a drop of *Lake_Level* starts when a shock to Flow_Rate appears after a while in the *Flow_Rate* and *Lake_Level* level plots (colored blue). **Intuitively, an increase of flow rate speeds up the reduction of lake levels.**","202f7316":"Since forecast error impulse response (FEIR) only analyzes shock occuring in one variable at a time, contemporaneous correlation of error terms cannot be captured by it. We then use orthogonalized impulse response (OIR) to solve this issue, which rely on stronger assumption and thus the order of features in model matters. Besides, the Hall's confidence interval is used since it usually has bias corrected effect compared to Efron.\n\nThe feature order we choose is **Rainfall $\\rightarrow$ Temperature $\\rightarrow$ Hydrometry**, so the former can have a shock on later but not in the other way around. The meaning of OIR is to see whether one standard deviation shock to x causes significant (increase\/decrease standard deviation) response in y for m periods. \n\nWe find that in the OIR plot the *Rainfall_Le_Croci, Rainfall_Cavallina, Rainfall_Stia, Rainfall_S_Savino and Rainfall_Bibbiena* shocks cause significant positive response to *Hydrometry_Nave_di_Rosano*. Although some other Rainfall are significant but the response value is not large nor long enough. (As for the Temperature, none of the rainfalls cause it significantly. To save space, we didn't show the graph here but one can change `response='Temperature_Firenze'` to see the result.)\n    ","0e6f83b7":"## Luco: Daily LSTM\n\nLSTM model doesn't require data to be stationary but better scaled since tanh() is used by LSTM. We then use `MinMaxScaler()` to scale data into $[-1,1]$. One can also use alternative scaler such as `StandardScaler()` by simply changing the `approach='Standard'` in the scale function. `year_sin` is NOT included based on previous discussions of the VAR part.\n\nHere we use the same lag as VAR to train the LSTM in order for 1D prediction performance comparison. The model can thus be futher fine tuned based on different lags together with other parameters, which we will omit here (by fixing lag=varlag) and focus only on the feature importance learned by the LSTM.\n\nWe didn't implement the LSTM model that predicts 7D\/14D in a row since it's intuitive to directly predict on target features instead of predicting on all features and then feed the prediction to the model again, which is like the behaviour of VAR model.\n\nNotice: due to the large lag(=20), the training\/val\/test sample sizes are small. Machine learning models usually require a huge amount of samples and thus may not perform well on small samples. Our goal here is to explore the important features to LSTM via the sensitivity analysis.","65b36805":"We will only use the data between 2015 and 2019 for our analysis.","51e58926":"## Import Libraries","94b416a3":"The prediction seems quite OK on 14D\/30D, but summarize all rainfalls by averaging performs much better. Actually, all the models of different setting do not vary much on 1D prediction.","7ba88099":"## Amiata: Daily VAR","c0fe6a71":"## Summary\n\nThe VECM model leads to lower prediction MAE and RMSE compared than the VAR model for all predictions with a prediction horizon larger than a week. Even though the ADF test did not indicate the need of differencing, cointegration seems seems to benefit the prediction qulaity. VECM also requires a lower lag order than the VAR specification, even though it is not fully capable of reducing the high-lag autocorrelation.","05bc8a56":"## Testing Functions","46e02e36":"## Summary\n\nWe summarize here what we've gotten from the above analysis <br\/>\nTarget feature: Depth_to_Groundwater_Podere_Casetta <br\/>\nTesting size: 50.\n\nMost important Features: <br\/>\nFrom the VAR model: 1D\/7D\/14D RMSE - .0466\/.0577\/.0394 (inverse transformd)\n - Rainfall_Pentolina, Volume_Pozzo_3, Rainfall_Montalcinello, Temperature_Monteroni_Arbia_Biena, and Rainfall_Mensano\n\nFrom the LSTM model: 1D RMSE - 0.0464 (inverse transformed and includes imputed values)\n - (Depth_to_Groundwater_Podere_Casetta), \n - (Temperature_Monteroni_Arbia_Biena, Depth_to_Groundwater_Pozzo_4, and Depth_to_Groundwater_Pozzo_3)\n\nCommon important features: \n - Inconclusive (see the previouse comment)\n","5102cff5":"## Machine Learning Class","ca4a9e54":"# River Arno","f32f1c5d":"## VECM Class","a6ef3ee9":"We can clearly see here that the flow rate of water spring Lupa has several structural breaks in which a change in the trend occurs. These shifts do not seem to be directly associated with the rainfall in Terni over time \u2013 only that the previous increase in the flow rate (or the reduction in the absolute of the flow rate) may have a causal relationship \u2013 and the later OIR plots will indicate this causality.","d20f3a11":"# Introduction","aa6fa462":"As shown in the following, the model perform well on 1D, last 7D, and last 14D predictions: the prediction pattern looks similar to the ground truth data. Looking at the individual rmse\/mae measures (shown in titles of graphs), the main errors comes from the prediction on *Depth_to_Groundwater_SAL*. We believe that our model learn necessary info to do make the predictions. So we move on to OIR\/FEVD analysis.\n","87273c0a":"The FEVD plot is shown below. It illustrates which (orthogonised) feature shocks contribute to the error variance of our target feature *Hydrometry_Nave_di_Rosano* in different lags. In lag 0, *Hydrometry shock* is the top 1 that contributes to the error variance of itself, followed by *Rainfall_Stia, Le_Croci, S_Savino, and Montevarchi*. It becomes stable that the top 5 features are *Rainfall_Le_Croci, Hydrometry, Rainfall_Cavallina, Stia, and S_Savino* in later periods. Note that these rainfalls contribute above 40% FEVD to the target. Another 40% comes from the Hydrometry feature itself.","e09b6e90":"## Summary\n\nWe summarize here what we've gotten from the above analysis <br\/>\nTarget feature: Depth_to_Groundwater_SAL, Depth_to_Groundwater_CoS, Depth_to_Groundwater_LT2 <br\/>\nTesting size: 230.\n\nMost important Features: <br\/>\nFrom the VAR model: 1D\/7D\/14D RMSE - .0516\/.0457\/.0583 (inverse transformd)\n - For SAL: Rainfall_Tereglio_Coreglia_Antelmineli, Rainfall_Pontetetto, Hydrometry_Monte_S_Quirico, Volume_CSA, Depth_to_Groundwater_PAG\n - For CoS: Rainfall_Pontetetto, Rainfall_Monte_Serra, Rainfall_Tereglio_Coreglia_Antelmineli, Volume_POL\n - For LT2: Rainfall_Tereglio_Coreglia_Antelminelli, Depth_to_Groundwater_CoS, Rainfall_Pontetetto, Rainfall_Monte_Serra\n\nFrom the LSTM model: 1D RMSE - 0.2089 (inverse transformed and includes imputed values)\n - (Depth_to_Groundwater_SAL, Depth_to_Groundwater_CoS, Depth_to_Groundwater_LT2),\n - Hydrometry_Monte_S_Quirico, Rainfall_Tereglio_Coreglia_Antelminelli\n\nCommon important features: \n - For SAL: Hydrometry_Monte_S_Quirico, Rainfall_Tereglio_Coreglia_Antelminelli\n - For CoS: Rainfall_Tereglio_Coreglia_Antelminelli\n - For LT2: Rainfall_Tereglio_Coreglia_Antelminelli","9ac05c24":"## Summary\n\nWe summarize here what we've gotten from the above analysis <br\/>\nTarget feature: Depth_to_Groundwater_Pozzo_1,2,3,4,5,6,7,8,9. (Abbreviated as GW) <br\/>\nTesting size: 137.\n\nMost important Features: <br\/>\nFrom the VAR model: 1D\/7D\/14D RMSE - .0516\/.0457\/.0583 (inverse transformd)\n1. Volume_Pozzo_1, GW7\n2. GW4\n3. GW6\n4. GW2, GW5, GW6\n5. None\n6. GW2, GW4\n7. GW1\n8. None\n9. None\n\nPS. (GW1,GW7) affect each other, so do (GW2, GW4, GW6). Unfortunately, neither rainfalls nor temperatures have significant IR on our target features.","e133cbde":"## Preface\n\nIn a time in which climate change causes irregular rainfall while consumers and regulators demand water that is free of chemical pollutants, it is increasingly important to find sustainable sources of water. This is a large challenge because sustainable water treatments and water reclamation are costly. If one could accurately predict the water availability in rivers, lakes, the ground and in water springs, it would be easier to delegate between natural water sources and expensive water treatments. In 2019, people in the region of Toscana paid the highest water prices in Italy. We will be investigating nine datasets and try to predict water levels and flows. \n\nThe datasets contain information like various rainfalls or temperatures, flow rates from water strings, hydrometries from river or a lake and groundwater levels. In the story we tell about how these variables act together, the weather is the exogenous force. Rain pours down and collects into rivers from where it fills lakes and later reaches aquifers, depending on the permeability of the soil. These water bodies may exercise pressure on each other, causing the flow rates of water springs to change. The order of these events is important in our search for causal relationships.\n\nA large part of our work concentrates on the use of parametric models. VAR and VECM models offer nice ways to infer causal relationships. We will use orthogonalised impulse responses (OIR) and forecast error variance decompositions (FEVD) in order to explain the evolution of water levels and flow rates. In addition to this, we also use a long short-term memory (LSTM) neural network as an alternative method to make predictions.\n\nOur models have some imitations: VAR and VECM serve as linear predictors, and the LSTM is sensitive to initial hidden states and requires large datasets to be trained to precision. However, the reasons for choice of our models are as follows: The VAR and VECM models can be expected to be the leased biased due to their linear properties \u2013 possibly at the cost of a higher RMSE\/MAE. Contrary to complex machine learning models, our models allow for the investigation of causal relationships. The low bias in our models allows for reliable interpretations of the OIR plots which are useful to determine causal relationships between features \u2013 such tools are not available for black box models.","4cd6c11b":"## Bilancino: Daily VAR\n\nWe present the baseline model here which includes all features without further engineering but stationary. All features pass the ADFuller test and Granger test, so no need to difference or drop features. \n\nNotice that the model we're **presenting here performs the best on 1-step-ahead prediction (1D)**. Another model that performs the best on the last 14D, and additionally the last 30D, is the one that average all rainfall features, which will not be presented here but reported at the end of this section.","069c69e7":"We use the sensitivity analysis to determine the feature importance, that is, when perturbing a particular feature based on testing data, given the values of the other feature unchanged, how much will rmse change by comparing the prediction before and after perturbation. A feature is more important if the larger the rmse is.\n\nIn the figure below we find that **Rainfall_Le_Croci, Rainfall_Cavallina, Temperature_Le_Croci, and Rainfall_Mangona** are important features to LSTM. '+' sign means when adding epsilon on each testing observation, 95% of which makes higher prediction than original prediction. We summarize the comparison of important features in the summary section.","22af2534":"## Data Cleansing\n\nThe Lake Bilancino dataset only contains missing value till 2004-01-01, so we drop them and focus on data during 2004-01-02 - 2020-06-30. A first glance at correlation triangle plot shows that our target features *Flow_Rate* and *Lake_Level* seems have no correlation with the other features. Fourier plot indicates both features have a yearly frequency. We will explore more feature engineering in the next session. Notice that we didn't use mask here to correct rmse\/mae performance since no missing value needs to be imputed in the data after 2004-01-02.","6f6bf5c9":"## Doganella: Daily LSTM\n\nLSTM model doesn't require data to be stationary but better scaled since tanh() is used by LSTM. We then use `MinMaxScaler()` to scale data into $[-1,1]$. One can also use alternative scaler such as `StandardScaler()` by simply changing the `approach='Standard'` in the scale function. `year_sin` is added into the LSTM model for experiment.\n\nHere we use the same lag as VAR to train the LSTM in order for 1D prediction performance comparison. The model can thus be futher fine tuned based on different lags together with other parameters, which we will omit here (by fixing lag=varlag) and focus only on the feature importance learned by the LSTM.\n\nWe didn't implement the LSTM model that predicts 7D\/14D in a row since it's intuitive to directly predict on target features instead of predicting on all features and then feed the prediction to the model again, which is like the behaviour of VAR model.\n\nNotice: due to the large lag(=2), the training\/val\/test sample sizes are small. Machine learning models usually require a huge amount of samples and thus may not perform well on small samples. Our goal here is to explore the important features to LSTM via the sensitivity analysis.","58857dda":"The orthogonalized impulse responses were mostly insignificant which is why we will rely on forecast error decomposition. We can clearly see that the flow rate of Bugnano and of Arbure seem to be determining a large portion of the variance. Assuming that the water springs are located on different water bodies, this indicated that all these water bodies are impacted simultaneously by rainfall and the resulting water flows. However, if the slow decay in the flow rate proportions indicate that the entire process of rainfall, the filling of aquifers and the water flow from the spings takes time and is laggy. In the FEVD plot for the water spring Bungnano, we can observe  that the variance proportion of variables other than water springs grows over time. Also, if variables with many lags alone were good predictors by themselves, the order of the VAR as suggested by AIC would be larger. This may indicate that a long-memory process may lead to a better performance. This means that a shock may lead to a relatively persistent change in the water flow. This memory could be caputured by a VECM model. Here are the MAE and RMSE for the VAR model.","dce3185d":"## Petrignano: Daily LSTM\n\nLSTM model doesn't require data to be stationary but better scaled since tanh() is used by LSTM. We then use `MinMaxScaler()` to scale data into $[-1,1]$. One can also use alternative scaler such as `StandardScaler()` by simply changing the `approach='Standard'` in the scale function. `year_sin` is also included to make model learn the yearly pattern better. \n\nHere we use the same lag as VAR to train the LSTM in order for 1D prediction performance comparison. The model can thus be futher fine tuned based on different lags together with other parameters, which we will omit here (by fixing lag=varlag) and focus only on the feature importance learned by the LSTM.\n\nWe didn't implement the LSTM model that predicts 7D\/14D in a row since it's intuitive to directly predict on target features instead of predicting on all features and then feed the prediction to the model again, which is like the behaviour of VAR model.","6fb83323":"## Data Cleansing\n\nThe Aquifer Petrignano dataset only contains, as shown below, missing values before 2009-01-01 and in between 2015-04-25 and 2015-09-21. We drop the massing missing part before 2009-01-01 and impute the rest with simple linear interpolation in the next section.\n\nA first glance at correlation triangle plot shows that our target features *Depth_to_Groundwater_P24 & _P25* are perfect correlated with each other and some what correlated with *Volume_C10_Petrignano* and *Hydrometry_Fiume_Chiascio_Petrignano*. It's unclear whether target features possess a yearly pattern so we will try to explore it.\n\nMask on missing\/imputed data is used in order to get more accurate rmse\/mae performance of models.","ff15823f":"As one can see, the 1-step-ahead prediction and 7 days in a row prediction are not bad but the model breaks when predicting 14 days in a row. Our goal here, however, is to do the OIR analysis and select feature based on it to create the best model. We've also tried other possible feature engineering and re-train the VAR model, which will be discussed later in this section.","e38bfa27":"The FEVD plot is shown below. It illustrates which (orthogonised) feature shocks contribute to the error variance of our target features in different lags. \n\nThe top important features are: <br\/>\nDepth_to_Groundwater_SAL: \n - Rainfall_Tereglio_Coreglia_Antelmineli, Rainfall_Pontetetto, Hydrometry_Monte_S_Quirico (accounts for >30% after lag 2)\n \nDepth_to_Groundwater_CoS:\n - Rainfall_Tereglio_Coreglia_Antelmineli, Rainfall_Pontetetto, Rainfall_Monte_Serra (accounts for >30% after lag 2)\n \nDepth_to_Groundwater_LT2: \n - Rainfall_Tereglio_Coreglia_Antelmineli, Depth_to_Groundwater_CoS, Rainfall_Pontetetto, Rainfall_Monte_Serra (accounts for >25% after lag 2)\n","9d565195":"## Madonna di Canneto: Daily VAR","253b9bc7":"After differencing all non-stationary variables, we check for stationarity again. The depth to groundwater in David Lazzaretti is still non-stationary so we difference it for a second time.","7f7ccc86":"The OIR plots show that the flow rates have strong effects on each other. The rainfall in Laghetto Verde very clearly increases the absolute flow rate of Galleria Alta. An increase in the level of the depth to groundwater in David Lazzaretti bears a long-lasting effect on the flow rate in Ermicciolo.","1d325f94":"## Auser: Daily LSTM\n\nLSTM model doesn't require data to be stationary but better scaled since tanh() is used by LSTM. We then use `MinMaxScaler()` to scale data into $[-1,1]$. One can also use alternative scaler such as `StandardScaler()` by simply changing the `approach='Standard'` in the scale function. `year_sin` is NOT included based on previous discussions in the VAR part.\n\nHere we use the same lag as VAR to train the LSTM in order for 1D prediction performance comparison. The model can thus be futher fine tuned based on different lags together with other parameters, which we will omit here (by fixing lag=varlag) and focus only on the feature importance learned by the LSTM.\n\nWe didn't implement the LSTM model that predicts 7D\/14D in a row since it's intuitive to directly predict on target features instead of predicting on all features and then feed the prediction to the model again, which is like the behaviour of VAR model.\n\nNotice: due to the large lag(=7), the training\/val\/test sample sizes are small. Machine learning models usually require a huge amount of samples and thus may not perform well on small samples. Our goal here is to explore the important features to LSTM via the sensitivity analysis.","10824c5b":"# References\n\n* [Cross-batch statefulness- Working with RNNs](https:\/\/keras.io\/guides\/working_with_rnns\/)\n* [Time Series Forecasting, Tensorflow](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/time_series#top_of_page)\n* [Combine LSTM and VAR for Multivariate Time Series Forecasting](https:\/\/towardsdatascience.com\/combine-lstm-and-var-for-multivariate-time-series-forecasting-abdcb3c7939b)\n* [How to Build VAR model](https:\/\/www.machinelearningplus.com\/time-series\/vector-autoregression-examples-python\/)\n* [statsmodels.tsa.vector_ar.var_model](https:\/\/www.statsmodels.org\/dev\/_modules\/statsmodels\/tsa\/vector_ar\/var_model.html#VARResults.irf_errband_mc)\n* [The Unreasonable Effectiveness of Recurrent Neural Networks](http:\/\/karpathy.github.io\/2015\/05\/21\/rnn-effectiveness\/)\n* [Aquifer-Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Aquifer#:~:text=An%20aquifer%20is%20an%20underground,of%20aquifers%20is%20called%20hydrogeology.)\n* [How Stream is Measures](https:\/\/www.usgs.gov\/special-topic\/water-science-school\/science\/how-streamflow-measured?qt-science_center_objects=0#qt-science_center_objects)","c70fa557":"We've implemented various setting of model and find that experiment **No.4 performs the best** on 1D normal prediction and last 14 days prediction. Besides, year_sin usually help improve performance. Thirdly, summarize all rainfall features with a mean in experiemnt No. 3 also performs well compared to the others. Some setting fail Portmanteau test but sill perform better than the one does. \n\nSince VAR model is linear and thus can only capture local linear prediction, we present LSTM as well trying to capture the possible non-linear relationship between target and features in the following.","861325ab":"## Arno: Daily LSTM\n\nLSTM model doesn't require data to be stationary but better scaled since tanh() is used by LSTM. We then use `MinMaxScaler()` to scale data into $[-1,1]$. One can also use alternative scaler such as `StandardScaler()` by simply changing the `approach='Standard'` in the scale function. `year_sin` is also included to make model learn the yearly pattern better. \n\nHere we use the same lag as VAR to train the LSTM in order for 1D prediction performance comparison. The model can thus be futher fine tuned based on different lags together with other parameters, which we will omit here (by fixing lag=varlag) and focus only on the feature importance learned by the LSTM.\n\nWe didn't implement the LSTM model that predicts 7D\/14D in a row since it's intuitive to directly predict on target features instead of predicting on all features and then feed the prediction to the model again, which is like the behaviour of VAR model.","021a4b30":"Since forecast error impulse response (FEIR) only analyzes shock occuring in one variable at a time, contemporaneous correlation of error terms cannot be captured by it. We then use orthogonalized impulse response (OIR) to solve this issue, which rely on stronger assumption and thus the order of features in model matters. Besides, the Hall's confidence interval is used since it usually has bias corrected effect compared to Efron.\n\nThe feature order we choose is **Rainfall $\\rightarrow$ Temperature $\\rightarrow$ Volumes $\\rightarrow$ Hydrometries $\\rightarrow$ Depth_to_Groundwaters**. \n\nActually, the order between rainfall and temperature can be reversed since they affect each other. Depth_to_Groundwaters is put at the end because we want to see how the other features affect the targets. Volumes (the amount taken from the dinking water treatment plant) clearly affects Hydrometries, so volumes are put before hydrometries.\n\nThe meaning of OIR is to see whether one standard deviation shock to x causes significant (increase\/decrease standard deviation) response in y for m periods. Symbols mean the response direction.\n\nFor target feature Depth_to_Groundwater_SAL: \n - Rainfall_Pontetetto(+), Rainfall_Tereglio_Coreglia_Antelminelli(+), Volume_CSA(-), Hydrometry_Monte_S_Quirico(+), \nDepth_to_Groundwater_PAG(+)\n\nFor target feature Depth_to_Groundwater_CoS:\n - Rainfall_Pontetetto(+), Rainfall_Monte_Serra(+), Rainfall_Tereglio_Coreglia_Antelminellim(+)\nVolume_POL(+), \n\nFor target feature Depth_to_Groundwater_LT2:\n - Rainfall_Tereglio_Coreglia_Antelminelli(+-), Depth_to_Groundwater_CoS(+)\n \nGenerally, the symbol directions make sense (except the Rainfall_Tereglio_Coreglia_Antelminelli for the final feature): A positive shock to rainfall adds water to aquifer; A positive shock to volume (meaning less water taken from drinking water plant since volume is itself negative) leaves more water in aquifer; a positive shock to hydrometry means also a positive response to groundwater.","561ecf9b":"We add `year_sin` feature in to check if model can learn better pattern to predict targets. Models that don't include this feature are also reported at the end of this section. \n\nThe model passes the Durbin Watson Test, meaning that there is no significant serial correlation in residual of each feature. However, it doesn't pass the Portmanteau test, indicating that residual autocorrelatios exists somewhere whit the first 60 lags. It either means that noises are large or the true relationship between features is non-linear. Later we also provide LSTM to check performance.\n\nFailing to pass normality test means that asymptotic standard errors are no longer valid and bootstrapping method such as Efron's or Hall's confidence interval should be used instead when checking impulse response.","da6d760e":"## Petrignano: Daily VAR\n\nWe present the baseline model here which includes all features without further engineering but being stationary. The target features are first differenced and all features pass the Granger test at 5% level of lag=15 (chosen by later VAR model). \n\nNotice that the model we're **presenting here is not the best** model, but helps us on OIR analysis, further feature selection, and finally the best model is created. It is created by selecting the important features via OIR plots, dropping features via PCA and first differencing on target features, which will be reported at the end of this section.","a8e0d710":"We add `year_sin` feature in to check if model can learn better pattern to predict targets. Models that don't include this feature are also reported at the end of this section. \n\nThe baseline model sort of passes the Durbin Watson Test, meaning that there is no significant serial correlation in residual of each feature. Some features get scores around 1.6 which is a bit low. Although we didn't drop them in order to see full OIR analysis, it would be interesting as further studies to see that if dropping the features will make performance of the model better.\n\nThe baseline model, however, doesn't pass the Portmanteau test, indicating that residual autocorrelatios exists somewhere whit the first 60 lags. It either means that noises are large or the true relationship between features is non-linear. Later we also provide LSTM to check performance.\n\nFailing to pass normality test means that asymptotic standard errors are no longer valid and bootstrapping method such as Efron's or Hall's confidence interval should be used instead when checking impulse response.","cf24bcd4":"Compared to VAR model, our LSTM doesn't perform well (even though the rmse\/mae for LSTM includes imputed observations while VAR has masked them). One can fine tune the model to achieve better results (here we fix the lag in order for consistency with VAR model). We instead put efforts on explaining feature importance for LSTM.\n\nWe use the sensitivity analysis to determine the feature importance, that is, when perturbing a particular feature based on testing data, given the values of the other feature unchanged, how much will rmse change by comparing the prediction before and after perturbation. A feature is more important if the larger the rmse is.\n\nIn the figure below we find that, besides the target features themselves, **Hydrometry_Monte_S_Quirico and Rainfall_Tereglio_Coreglia_Antelminelli** are most important features to LSTM. The other features are sub-important. We summarize the comparison of important features in the next section.","df5a7516":"## Data Cleansing","e64cff2c":"The FEVD plot is shown below. It illustrates which (orthogonised) feature shocks contribute to the error variance of our target features in different lags. \n\nBoth target features have similar pattern in FEVD plots. The top 3 features excluding the *Depth_to_Groundwater_P24\/P25* itself are *Volume_C10_Petrignano, Temperature_Bastia_Umbra, and Rainfall_Bastia_Umbra*, which contribute to roughtly 30%-50%, 30%-50%, and 2%-5% of the error variance in later periods, respectively. The fifth important feature is ignored due to too small contributions.","8fc14c2f":"The FEVD plot is shown below. It illustrates which (orthogonised) feature shocks contribute to the error variance of our target features in different lags. \n\nFor the target feature *Flow_Rate*, the top 3 features excluding the Flow_rate itself are **Rainfall_S_Piero, Rainfall_Mangona, and Rainfall_Cavallina**, which contribute roughly 10%-20% of the error variance in later periods. Rainfall_Le_Croci & Temperature_Le_Croci, and Lake_Level have similar but small effecs.\n\nFor the target feature *Lake_Level*, **Rainfall_S_Piero, Rainfall_Mangona, and Flow_Rate** are most important features accounting for 20% error variance in later lags.\n\nThe FEVD analysis are (usually) aligned with the OIR analysis.","08a6cc06":"We've implemented various settings of models and are reported below. As far what we have implmented is the model No. 4, which performs the best on 14D among our settings. From the report we find that:\n\n1. `year_sin & year_cos` doesn't help for better pattern learning.\n2. rolling mean of 30 days on all features may or may not be helpful on long term prediction.\n3. The naive baseline model (No. 3), which includes only the nine target features, performs the best on 1D. \n4. Perhaps 3. is alinged with our findings that none of the temperatures and rainfalls have significant OIR on target features.\n5. It's hard to decrease RMSE\/MAE on 1D prediction for all settings probably due to the difficulty in predicting sudden jumps, as discussed early in this section.\n\nSince VAR model is linear and thus can only capture local linear prediction, we present LSTM as well trying to capture the possible non-linear relationship between target and features in the following.","419e4d90":"## Luco: Daily VAR\n\nWe present the baseline model here which includes all features without further engineering but being stationary. All temperatures, Volume_Pozzo_3, and all Depth_to_Groundwaters, including our target feature, are thus first differenced. \n\nHalf of the rainfall features and all the volume features together with Depth_to_Groundwater_Pozzo_3 do NOT pass the Grager in the setting of baseline model (5% level with lag=20), as showned below, but we still inclue them in the following setting and put the one that excludes them into the report at the end of this section.\n\nNotice that the model we're **presenting here is not the best** model, but helps us on OIR analysis, further feature selection, and finally the best model is created. \n\nIn addition, since the data left is not much, the VAR lag order cannot be chosen too large or otherwise the model will complain a maxlag too large error because the total variables will be larger than the observations we have.","dcd633fe":"We've implemented various setting of model and find that **model No. 5 and 9 performs the best** on 1D and 7D\/14D , respectively.\n\n1. Selected features presented as far (ie, features listed in No.5) are indeed important features for the model to perform the best. (better than put everything inside, ie., baseline model.)\n2. The setting of grouping volumes in No. 6 performs nearly best compared to No. 5.\n3. Based on No. 6 but smooth features via rolling mean of 30 days can improve long term (7D\/14D) predictions a lot, as shown in No. 9.\n\nSince VAR model is linear and thus can only capture local linear prediction, we present LSTM as well trying to capture the possible non-linear relationship between target and features in the following.","0ddf7774":"We've implemented various setting of model and reported their performance here. As far we have shown that the baseline model predict best on 1D and its interesting and consistency feature explanation. By the report we also find that experiment **No.3 perform the best** on 14D\/30D predictions. Findings from the report are given below: \n1. summarizing rainfalls via average helps capture info and improve long term predictions, though the 1D prediction is also not bad compared to No.1 baseline.\n2. `year_sin` in most cases help capture long term pattern and thus improve performance.\n3. Rolling method in No.5 and No.6 doesn't make model perform better. Intuition that 30 days smoothing should help capture long term prediction breaks.\n\nFailing PT test means that the model may be interfered by short but large noise or that a linear model may not capture all necessary information to predict target. Hence some info are left in the error lags.\n\nSince VAR model is linear and thus can only capture local linear prediction, we present LSTM as well trying to capture the possible non-linear relationship between target and features in the following.","19b1cc0b":"## Arno: Daily VAR\n\nIn this section we implement the baseline daily VAR model including all features and only the Temperature_Firenze is differenced to make the data stationary. We find that 5 rainfall features and the Temperature_Firenze don't pass the Granger test at 5% level of lag=3 (chosen by later VAR model). However, we still include them in our model in order to see OIR on those features as well. The model that excludes the non-Granger features and other possible variations are reported at the end of this section.\n\nNotice that the model we're **presenting here is not the best** model, but helps us on OIR analysis, further feature selection, and finally the best model is created, which is also reported at the end.","865d64a2":"As shown in the following, neither the 1D nor the 7D\/14D performs well for our baseline model. Compared to this, the best model we present at the end of this section can reduce rmse to roughly 0.05, 0.05, and 0.04 on 1D\/7D\/14D predictions. However, the prediction plot of it still doesn't look good. We reckon that this is because the ground truth possesses the pattern of a horizontal line with several suddenly jumpus, which makes the VAR model (even non-linear\/ML model) hard to predict them correctly. Further feature transformation\/engineer can also be conducted but then we can only explain OIR on the transformed or summarized features instead of the original features.\n\nAn observation worth noticed is that even though the **rmse\/mae seems very small (at 1e-2 level), the performance shouldn't be concluded as a good one** since it's caused by the small values of the groud truth itself, not because the model really learn the pattern.","b675e852":"![arno](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1162902\/1948532\/arno.png?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210217%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210217T042609Z&X-Goog-Expires=172799&X-Goog-SignedHeaders=host&X-Goog-Signature=6216ada801bb58c39c0ff30ba1ddad1e00e1d1524960490b8c1330c51144f7c926bc74d6bcf5f67139052c64aacd07995ff3ca8d18245d570298ebc690a119082b4c9d706bbd81b20f2972c22e0968737cfc64b13555bac161079f0908b66589ac1cd51381216c5770765c4796d9227ff739c8edf9e2540825e914ad8e6e5e191295bca901f560a2a16aef7c927af4c4ee57a314e216f9a1b9fddd4fd0e4b163eeee0d1d181da0cc7b921a1b2da061e6cd44d32c14fdc902e3133629b26703f9960840c72a9593a934ec649a7e8db72ea947bf46f1200379f7523370e08c9a8409a56c494561ba3edb031aa82e9512742f691a0be8ad784cecb1af7dd7689cd9)","5f5b4138":"In the following, some plots will reveal some features of the data that are relevant for the later analysis. In order to have a time slot that is representative of all the variables, we will use those data from 2016 onwards. The depths to groundwater \u2013 as in the previous datasets \u2013 appear very laggy and non-stationary. The same case can be made for flow rates.","ab2800d7":"In the following, some plots will reveal some features of the data that are relevant for the later analysis. Given that the trends of the flow rate are long-lasting, the linear interpolations seems to be a particulary suitable method to imputed the few missing values.","bfb212ee":"## Data Engineering Functions","9ff8a248":"## Data Cleansing\n\nThe Aquifer Luco dataset only contains large missing values so we can only focus on 2017-09-01 - 2019-01-11, where all features are preserved and can be imputed easily by linear regression. (as shown in between red vertical lines in the folloing figure)\n\nWe also drop the two features **Rainfall_Siena_Poggio_al_Vento**, and **Temperature_Siena_Poggio_al_Vento** since we have tested on the model that includes them in various settings but usually performs poorer than the one without them.\n\nA first glance at correlation triangle plot shows that our target feature *Depth_to_Groundwater_Podere_Casetta* is correlated all *Temperatures* and the *Volume_Pozzo_4*. It's unclear whether target features possess a yearly pattern so we will try to explore it.\n\nMask on missing\/imputed data is used in order to get more accurate rmse\/mae performance of models.","6b49be49":"## Data Cleansing\n\nThe Aquifer Doganella dataset contains large missing values for various features: \n - Rainfall: missing in 2015-2018\n - Temperature: missing in 2015-2018\n - Volume: missing from past to 2016-10-07\n - Groundwater: missing from past to 2015-06-01\n\nWe firstly impute data in 2015-2018 and then select observations after 2016-10-07 due to mass missing in the past.<br\/> \nThe imputation on Rainfalls and Temperatures uses past 4 years as a rolling window to caculate the mean value of the same date. So 365(366) days are calculated and imputed to the target year. The target year is looped over 2015-2018. <br\/>\nAs for imputation on volumes and groundwaters, we simply use linear interpolation since the missing patterns are not complicated. <br\/>\nFinally, all data before 2016-10-07 are dropped due to mass missing. \n\nWe will omit the pairwise correlation plot of features and directly move on to prediction and OIR\/FEVD part since it's complicated to get insight of all nine target features.\n\nMask on missing\/imputed data is used in order to get more accurate rmse\/mae performance of models.","95cf2bbb":"## Summary\n\nWhile the impulse response function for rainfall indicates that rain slightly increases the flow rate (in the absolute), an shock on the flow rate to itself causes a rippeling over the long run. This emphasizes the importance observing this water spring closely, since a short-term shock can have severe long-lasting effects. Since the ADF test resulted in both variables being stationary and rainfall and flow rate are not comoving, fitting a VECM model does not bring any extra benefits.","61c94f76":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Preface\" data-toc-modified-id=\"Preface-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Preface<\/a><\/span><\/li><li><span><a href=\"#Common-Settings\" data-toc-modified-id=\"Common-Settings-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>Common Settings<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Libraries,-Functions,-and-Classes\" data-toc-modified-id=\"Libraries,-Functions,-and-Classes-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Libraries, Functions, and Classes<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Import Libraries<\/a><\/span><\/li><li><span><a href=\"#Plotting-Functions\" data-toc-modified-id=\"Plotting-Functions-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Plotting Functions<\/a><\/span><\/li><li><span><a href=\"#Data-Engineering-Functions\" data-toc-modified-id=\"Data-Engineering-Functions-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Data Engineering Functions<\/a><\/span><\/li><li><span><a href=\"#Testing-Functions\" data-toc-modified-id=\"Testing-Functions-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;<\/span>Testing Functions<\/a><\/span><\/li><li><span><a href=\"#Machine-Learning-Class\" data-toc-modified-id=\"Machine-Learning-Class-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;<\/span>Machine Learning Class<\/a><\/span><\/li><li><span><a href=\"#VAR-Class\" data-toc-modified-id=\"VAR-Class-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;<\/span>VAR Class<\/a><\/span><\/li><li><span><a href=\"#VECM-Class\" data-toc-modified-id=\"VECM-Class-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;<\/span>VECM Class<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Aquifer-Auser\" data-toc-modified-id=\"Aquifer-Auser-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Aquifer Auser<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Auser:-Daily-VAR\" data-toc-modified-id=\"Auser:-Daily-VAR-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Auser: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Auser:-Daily-LSTM\" data-toc-modified-id=\"Auser:-Daily-LSTM-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Auser: Daily LSTM<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Aquifer-Doganella\" data-toc-modified-id=\"Aquifer-Doganella-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Aquifer Doganella<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Doganella:-Daily-VAR\" data-toc-modified-id=\"Doganella:-Daily-VAR-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Doganella: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Doganella:-Daily-LSTM\" data-toc-modified-id=\"Doganella:-Daily-LSTM-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Doganella: Daily LSTM<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Aquifer-Luco\" data-toc-modified-id=\"Aquifer-Luco-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Aquifer Luco<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Luco:-Daily-VAR\" data-toc-modified-id=\"Luco:-Daily-VAR-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Luco: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Luco:-Daily-LSTM\" data-toc-modified-id=\"Luco:-Daily-LSTM-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Luco: Daily LSTM<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Aquifer-Petrignano\" data-toc-modified-id=\"Aquifer-Petrignano-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Aquifer Petrignano<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Petrignano:-Daily-VAR\" data-toc-modified-id=\"Petrignano:-Daily-VAR-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;<\/span>Petrignano: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Petrignano:-Daily-LSTM\" data-toc-modified-id=\"Petrignano:-Daily-LSTM-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;<\/span>Petrignano: Daily LSTM<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Lake-Bilancino\" data-toc-modified-id=\"Lake-Bilancino-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Lake Bilancino<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Bilancino:-Daily-VAR\" data-toc-modified-id=\"Bilancino:-Daily-VAR-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;<\/span>Bilancino: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Bilancino:-Daily-LSTM\" data-toc-modified-id=\"Bilancino:-Daily-LSTM-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;<\/span>Bilancino: Daily LSTM<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#River-Arno\" data-toc-modified-id=\"River-Arno-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>River Arno<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Arno:-Daily-VAR\" data-toc-modified-id=\"Arno:-Daily-VAR-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;<\/span>Arno: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Arno:-Daily-LSTM\" data-toc-modified-id=\"Arno:-Daily-LSTM-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;<\/span>Arno: Daily LSTM<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Water-Spring-Amiata\" data-toc-modified-id=\"Water-Spring-Amiata-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Water Spring Amiata<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Amiata:-Daily-VAR\" data-toc-modified-id=\"Amiata:-Daily-VAR-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;<\/span>Amiata: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Amiata:-Daily-VECM\" data-toc-modified-id=\"Amiata:-Daily-VECM-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;<\/span>Amiata: Daily VECM<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-9.4\"><span class=\"toc-item-num\">9.4&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Water-Spring-Lupa\" data-toc-modified-id=\"Water-Spring-Lupa-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Water Spring Lupa<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-10.1\"><span class=\"toc-item-num\">10.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Lupa:-Daily-VAR\" data-toc-modified-id=\"Lupa:-Daily-VAR-10.2\"><span class=\"toc-item-num\">10.2&nbsp;&nbsp;<\/span>Lupa: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-10.3\"><span class=\"toc-item-num\">10.3&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Water-Spring-Madonna-di-Canneto\" data-toc-modified-id=\"Water-Spring-Madonna-di-Canneto-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;<\/span>Water Spring Madonna di Canneto<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Data-Cleansing\" data-toc-modified-id=\"Data-Cleansing-11.1\"><span class=\"toc-item-num\">11.1&nbsp;&nbsp;<\/span>Data Cleansing<\/a><\/span><\/li><li><span><a href=\"#Lupa:-Daily-VAR\" data-toc-modified-id=\"Lupa:-Daily-VAR-11.2\"><span class=\"toc-item-num\">11.2&nbsp;&nbsp;<\/span>Lupa: Daily VAR<\/a><\/span><\/li><li><span><a href=\"#Lupa:-Daily-VECM\" data-toc-modified-id=\"Lupa:-Daily-VECM-11.3\"><span class=\"toc-item-num\">11.3&nbsp;&nbsp;<\/span>Lupa: Daily VECM<\/a><\/span><\/li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-11.4\"><span class=\"toc-item-num\">11.4&nbsp;&nbsp;<\/span>Summary<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><li><span><a href=\"#References\" data-toc-modified-id=\"References-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;<\/span>References<\/a><\/span><\/li><\/ul><\/div>","2d370629":"# Aquifer Doganella","fdd093f8":"## Summary\n\nVAR and VECM deliver mixed prediction qualities. For Galleria Alta and Abure, the VECM is able to offer better results for prediction horizons of two weeks and above. As it comes to the water spring Bugnano, the VECM seems to offer better prediction results, while VAR predictions for Ermicciole is clearly better than the VECM prediction. ","22446829":"Since forecast error impulse response (FEIR) only analyzes shock occuring in one variable at a time, contemporaneous correlation of error terms cannot be captured by it. We then use orthogonalized impulse response (OIR) to solve this issue, which rely on stronger assumption and thus the order of features in model matters. Besides, the Hall's confidence interval is used since it usually has bias corrected effect compared to Efron.\n\nThe feature order we choose is **Rainfall $\\rightarrow$ Temperature $\\rightarrow$ Volumes $\\rightarrow$ Hydrometries $\\rightarrow$ Depth_to_Groundwaters**. \n\nActually, the order between rainfall and temperature can be reversed since they affect each other. Depth_to_Groundwaters is put at the end because we want to see how the other features affect the targets. Volumes (the amount taken from the dinking water treatment plant) clearly affects Hydrometries, so volumes are put before hydrometries.\n\nThe meaning of OIR is to see whether one standard deviation shock to x causes significant (increase\/decrease standard deviation) response in y for m periods. We show response of the first target feature only since the two targets are perfectly correlated with each other and the level patterns in data are the same. \n\nWe find that in the OIR plot the *Volume_C10_Petrignano* shock causees significant positive response to our *Depth_to_Groundwaters* at lag 0. This makes sense since less water taken from plant (note: Volume is negative, so one positive shock to volume means less water taken) results in more groundwaters. Notice that the volume feature also causes significant positive response to *Hydrometry_Flume_Chiascio_Petrignano* (but not the other features). \n\nThe feature *Temperature_Bastia_Umbra* also causes significant NEGATIVE response to the target features since higher temperatures vaporizes groundwaters quicker. *ainfall_Bastia_Umbra* also has some positive effect on groundwaters.\n","4e9facc9":"## Lupa: Daily VAR","ccbc0307":"## Data Cleansing","6eb790a4":"Rainfall, Temperature and target features represent yearly pattern, so we add `year_sin` feature in hope that the model can predict better by learning yearly pattern. Models that don't include this feature are also reported at the end of this section. \n\nThe model passes the Durbin Watson Test, meaning that there is no significant (5%) serial correlation in residual of each feature. It doesn't pass the Portmanteau test, indicating that some residual autocorrelatios within 30 lags are nonzero. From the residual AC plot we can see there exists a periodic bar that streches the 95% boundary, meaning that the model may not capture some info (probably due to non-linear relationship). Later, we present LSTM to check the issue.\n\nFailing to pass normality test means that asymptotic standard errors are no longer valid and bootstrapping method such as Efron's or Hall's confidence interval should be used instead when checking impulse response.","da5e4ab2":"Although short-term autocorrelation does not cause any problems (as indicated by the Durbin-Watson test), there is some autocorrelations at high lag orders as can be seen in the autocorrelation plot and in the Portmanteau-test at larger lag orders.","bf4bcea2":"# Aquifer Luco","9c832674":"ACEA Smart Water Feature Analysis via VAR, VECM and LSTM\n=====","81d2b139":"# Aquifer Auser","664dad2a":"# Water Spring Amiata\n\nNow we get to analyse the water springs and try to estimate the rate at which water flows out of the water springs. The water that flows out of water springs stems from a groundwater body. The groundwater body gets filled with rainwater such that it overflows and releases the excess water through the spring. Therefore, we will assume in our analysis that rain and temperature are exogeneously impacting groundwater levels. Finally, higher groundwater levels should increase the flowing rate of the water springs.","6039d3b2":"The plots show that there is very large autocorrelation in the data. As we will see next, this explains the need for many lag terms.","b1331908":"The VECM model choice seems to be fine: The Durbin-Watson test does not reveal any issues with autocorrelation. However, there still seems to be some autocorrelation left at high lag orders. After having chosen the lag order and the order of cointegration, we can analyse the quality of the model fit.","2ae70033":"## Data Cleansing\n\nAs shown in the following missing value plot, since the dataset of River Arno contains mass missing data before 2004-01-01 and after 2007-07-07, we drop them and focus our prediction task only on the period 2004-01-01 - 2007-07-06. Linear regression is used to impute missing data during the period.","ecc1cc5c":"Rainfall and Temperature represents yearly pattern, so we add `year_sin` feature in hope that the model can predict better by learning yearly pattern. Models that don't include this feature are also reported at the end of this section. Here our model is trained on training plus validation data and is tested on testing data. It passes the Durbin Watson Test, meaning that there is no significant (5%) serial correlation in residual of each feature.\n\nAlthouth the model passes the Portmanteau Test, meaning that residual auto correlation as a whole is zero, it doesn't pass normality test. So asymptotic standard errors are no longer valid and bootstrapping method such as Efron's or Hall's confidence interval should be used instead when checking impulse response.","4cb454d7":"We add `year_sin & year_cos` feature in to check if model can learn better pattern to predict targets. Models that don't include this feature are also reported at the end of this section. \n\nThe best model passes the Durbin Watson Test, meaning that there is no significant serial correlation in residual of each feature. However, it doesn't pass the Portmanteau test, indicating that residual autocorrelatios exists somewhere whit the first 35 lags. It either means that noises are large or the true relationship between features is non-linear. Later we also provide LSTM to check performance.\n\nFailing to pass normality test means that asymptotic standard errors are no longer valid and bootstrapping method such as Efron's or Hall's confidence interval should be used instead when checking impulse response.","5064c176":"## Common Settings\n\nIn some sections, **if there is no VECM model**, it means the VECM we've implemented performs poorer than VAR and should be thus dropped. (This means that the loss of estimations efficiency outweighs the gain from cointegration. So we won't look into cointegration matrix to extract insight.)\n\nAll datasets are cut into train\/val\/testing dataset with ratios 90\/10\/10 out of 100. <br\/>\nWhile VAR (require stationary) and VECM (do not need stationary) are trained on training and validation dataset, LSTM model is only trained on training data and is early stopped (and fine tuned) via validation dataset.\n\nAll models are tested on testing dataset of various size dependent on the amount of non-missing observations available in different data. The measure we use is RMSE and MAE.\n\nTo verify the quality of our time series models, we perform a [Durbin-Watson](https:\/\/en.wikipedia.org\/wiki\/Durbin%E2%80%93Watson_statistic) test and a [Portmanteau test](https:\/\/en.wikipedia.org\/wiki\/Portmanteau_test). While a Durbin-Watson test checks if the error terms are autocorrelated at a lag order of one, the Portmanteau test checks if the autocorrelation of several lags is zero. We always chose the number of lags to be a multiple of the respective model and usually found that there was still autocorrelation left in the error terms. We also checked with a [test for normality(Jarque\u2013Bera test)](https:\/\/en.wikipedia.org\/wiki\/Jarque%E2%80%93Bera_test) of the error terms which was denied in all cases. For the VECM, since it detects conintegration relations among features and thus do not require stationary data, we use [Johansen cointegration test](https:\/\/en.wikipedia.org\/wiki\/Johansen_test) to help select the conitegration rank.\n\nFinally, OIR\/FEVD analysis of VAR and sensitivity analysis of LSTM are reported to get insight of feature importance.","8b6b2421":"## Auser: Daily VAR\n\nWe present the best model here. <br\/>\nThe process to get the best model is: \n1. Conduct analysis on baseline VAR model which includes all features (except the missing Temperature_Ponte_a_Moriano), then pick up the significant features based on OIR plot. \n2. Conduct PCA to further reduce features \n3. make features stationary by first differencing and train the model.\n\nThe features we select based on step 1 and 2 are: <br\/>\nRainfall:  Tereglio_Coreglia_Antelminelli, Pontetetto, and Monte_serra <br\/>\nTemperature: Monte_serra <br\/>\nVolume: POL, CC1, and CSA <br\/>\nHydrometry: Monte_S_Quirico <br\/>\nGroudwater: All except DIEC\n\nAs for Granger causality test, none of the selected features fail Granger causality on all three target featres. So do not drop any feature due to Granger.\n\nFirst differenced features in step 3 are: <br\/>\nTemperature_Monte_Serra, Volume_POL, Volume_CC1, and Depth_to_Groundwater_LT2","0bc79b15":"## Amiata: Daily VECM\n\nNext, we will continue with the fitting of a VECM model.","ee83f42b":"We use the sensitivity analysis to determine the feature importance, that is, when perturbing a particular feature based on testing data, given the values of the other feature unchanged, how much will rmse change by comparing the prediction before and after perturbation. A feature is more important if the larger the rmse is.\n\nAs one can see, the LSTM model perform very worse on the prediction tasks. So we omit the discussion here.\nBut one thing worth notice is that it seems like the nine target features are most important to the LSTM (which is the same as what we've seen in the VAR part), followed by volume features.\n\nWe conclude that besides target features, all rainfalls except Cavallina and S_Agata important feature to LSTM when predicting Lake_Level and Flow_Rate.","17d34db9":"## Summary\n\nWe summarize here what we've gotten from the above analysis.<br\/>\nTarget feature: Hydrometry_Nave_di_Rosano.<br\/>\nTesting size: 129\n\nMost important Features:<br\/>\nFrom the VAR model: 1D\/7D\/14D RMSE - .1172\/.0486\/.0686 (inverse transformed)\n - Rainfall_Le_croci, Rainfall_Cavallina, Rainfall_Stia, Rainfall_S_Savino\n\nFrom the LSTM model: 1D - .1358 (inverse transformed and includes imputed values)\n - Rainfall_Le_Croci, Temperature_Firenze, Rainfall_Cavallina\n\nCommon important features: Rainfall_Le_Croci, Rainfall_Cavallina","b9c88a3f":"# Lake Bilancino","c9f8b3f9":"## Summary\n\nWe summarize here what we've gotten from the above analysis.<br\/>\nTarget feature: Flow_Rate, Lake_Level.<br\/>\nTesting size: 238.\n\nMost important Features:<br\/>\nFrom the VAR model: 1D\/7D\/14D RMSE - 0.1004\/0.2999\/0.2709 (inverse transformd)\n - For Depth_to_Groundwater_P24: Volume_C10_Petrignano, Temperature_Bastia_Umbra, and Rainfall_Bastia_Umbra\n - For Depth_to_Groundwater_P25: same\n\nFrom the LSTM model: 1D RMSE - 0.2346 (inverse transformed and includes imputed values)\n - (Depth_to_Groundwater_P24, Depth_to_Groundwater_P25), \n - (Volume_C10_Petrignano, Temperature_Bastia_Umbra, and Rainfall_Bastia_Umbra)\n\nCommon important features: \n - For Depth_to_Groundwater_P24: Volume_C10_Petrignano, Temperature_Bastia_Umbra, and Rainfall_Bastia_Umbra\n - For Depth_to_Groundwater_P25: same","95cab344":"We drop all data points before 2009-02-19.","83c172fd":"We've implemented various setting of model and find that experiment **No.3 performs the best** on 1D\/7D\/14D predictions. The model No.3 is created by selecting features via OIR of baseline (No.1 model) and then dropping unnecessary features via PCA. First difference on targets features are still required to make data stationary.\n\n`year_sin` helps improve longer term prediction but not on 1D prediction, and smoothing all features except targets via rolling mean on both window=7, 30 are not helpful here. All the models here fail the Portmanteau test. This is probably why VAR model prefers predicting a long term trend instead of jumps in the prediction figures.\n\nSince VAR model is linear and thus can only capture local linear prediction, we present LSTM as well trying to capture the possible non-linear relationship between target and features in the following.","30b8861f":"# Conclusion\n\nWe've implemented VAR, VECM, and LSTM models to predict 1 day, last 7 days, and last 14 days performance. Feature analysis is not only given in the VAR model via OIR and FEVD plots but is also presented in LSTM via sensitivity analysis. In the best situation, a feature is double confirmed to be important when both feature analysis in VAR and LSTM models give it top ranks. In VAR, we not only prudently derive models and give various testing to see appropriateness of the model, but also conduct various settings on models and report them in each section. Although LSTM is not perfectly fine tuned, in most cases it provides decent forecasts and gives reasonable results about feature importance. In the Doganella and Luco datasets where missing data provide an obstacle, we are conservative to give conclusions. And when calculating the MAE\/RMSE in each dataset, we scale the data back to the original levels in order to accurately reflect the true performance.\n\nIn the future, further studies can be done to create models that evaluate predictions over longer horizons. Features may interact in different patterns and early warning signals of shortening water supply may be uncovered. Also, since the data revealed long memory in many cases, a multivariate version of fractional integration (i.e. processes with an order of integration between zero and one) may be useful to deal with large lag orders. Such models may require fewer parameters and can be used to predict over long horizons. We attempted to tackle the problem of large lags with the use of VECM models. \n","fd90d3d9":"Rainfall leads to a stronger flow rate at Madonna di Canneto as does the temperature in Settefrati. A sudden shock in the flow rate seems to be followed by a slow normalization of the flow rate. This indicates that a change in the flow rate means that this change will persist over several weeks. It also means that this is a long-memory process.","02755a9e":"Since forecast error impulse response (FEIR) only analyzes shock occuring in one variable at a time, contemporaneous correlation of error terms cannot be captured by it. We then use orthogonalized impulse response (OIR) to solve this issue, which rely on stronger assumption and thus the order of features in model matters. Besides, the Hall's confidence interval is used since it usually has bias corrected effect compared to Efron.\n\nThe feature order we choose is **Rainfall $\\rightarrow$ Temperature $\\rightarrow$ Volumes ($\\rightarrow$ Hydrometries) $\\rightarrow$ Depth_to_Groundwaters**. \n\nActually, the order between rainfall and temperature can be reversed since they affect each other. Depth_to_Groundwaters is put at the end because we want to see how the other features affect the targets. Volumes (the amount taken from the drinking water treatment plant) clearly affects Hydrometries, so volumes are put before hydrometries.\n\nThe meaning of OIR is to see whether one standard deviation shock to x causes significant (increase\/decrease standard deviation) response in y for m periods. \n\n\nTo save space we directly write OIR restuls instead of prining them out: <br\/>\nformat- target feature number: impotant features (GWi means Depth_to_Groundwater_Pozzo_i)\n1. Volume_Pozzo_1, GW7\n2. GW4\n3. GW6\n4. GW2, GW5, GW6\n5. None\n6. GW2, GW4\n7. GW1\n8. None\n9. None\n\nAs one can see, (GW1,GW7) affect each other, so do (GW2, GW4, GW6). Unfortunately, neither rainfalls nor temperatures have significant IR on our target features.","ed73d0b5":"## Plotting Functions","02014921":"We add `year_sin` feature in to check if model can learn better pattern to predict targets. Models that don't include this feature are also reported at the end of this section. \n\nThe baseline model passes the Durbin Watson Test, meaning that there is no significant (5%) serial correlation in residual of each feature. But it doesn't pass the Portmanteau test, indicating that residual autocorrelatios exists somewhere whit the first 60 lags. It either means that noises are large or the true relationship between features is non-linear. Later we also provide LSTM to check performance.\n\nFailing to pass normality test means that asymptotic standard errors are no longer valid and bootstrapping method such as Efron's or Hall's confidence interval should be used instead when checking impulse response.","328f9b11":"## Doganella: Daily VAR\n\nWe present the model No. 4 (reported at the end of this section) which performs the best on 1D prediction. The model is obtained by:\n - sum up Volume_Pozzo_2, \\_4, \\_5_6, \\_8 to a new feature Volume_gp1 and drop them.\n - first difference on Temperature_Monteporzio, Temperature_Velletri, Volume_Pozzo_1 and 3, Depth_to_Groundwater: 1, 2, 3, 4, 5, 6, 8, 9.\n \nOther possible settings are also reported at the end of this section.","2bfddfe7":"## Summary\n\nWe summarize here what we've gotten from the above analysis.<br\/>\nTarget feature: Flow_Rate, Lake_Level.<br\/>\nTesting size: 603.\n\nMost important Features:<br\/>\nFrom the VAR model: 1D\/7D\/14D\/30D RMSE - .8862\/.1205\/.1081\/.2770 (inverse transformd)\n - For Flow_Rate:  Rainfall_S_Piero, Rainfall_Mangona, Rainfall_Cavallina, and Rainfall_Le_Croci \n - For Lake_Level: Rainfall_S_Piero, Rainfall_Mangona, Flow_Rate\n\nFrom the LSTM model: 1D RMSE - 1.1897 (inverse transformed)\n - (Lake_Level, Flow_Rate), \n - (Rainfall_Le_Croci, Rainfall_Cavallina, Temperature_Le_Croci, and Rainfall_Mangona)\n\nCommon important features: \n - For Flow_Rate: Rainfall_Mangona, Rainfall_Cavallina, and Rainfall_Le_Croci \n - For Lake_Level: Rainfall_Mangona, Flow_Rate","fc7f0f3b":"The FEVD plot is shown below. It illustrates which (orthogonised) feature shocks contribute to the error variance of our target features in different lags. \n\nBoth target features have similar pattern in FEVD plots. The top 3 features excluding our target features are **Rainfall_Pentolina, Volume_Pozzo_3, Rainfall_Montalcinello, Temperature_Monteroni_Arbia_Biena, and Rainfall_Mensano**, which contribute to 70% of the error variance of our target feature in the beginning but later decrease to on 50%.","9c50bd54":"The FEVD plot reveals that the most important feature that describes the flow rate at Madonna di Canneto is the flow rate itself. Also, the time series relies on many lags. This means that a shock in the flow rate leads to a relatively persistent change in the water flow. This long-term memory could be caputured with a VECM model."}}