{"cell_type":{"877545a2":"code","bd6da35b":"code","de7d07b5":"code","ae01deb9":"code","9cdcf85a":"code","25f92bba":"code","e11f2cd4":"code","b4852f43":"code","2a812d3b":"code","c78c6199":"code","d5797d7f":"code","e78626ca":"code","4f4d262d":"code","04c81cba":"code","64682c7e":"code","934ffb7b":"code","3734c41d":"code","4edda572":"code","2f1e7ae0":"code","86966750":"code","43937367":"code","4e6ab570":"code","ab298140":"code","142992a4":"code","b99379d4":"code","1aee5899":"code","d4b2eb48":"code","7d1d0773":"code","302e50af":"code","3fe8298d":"code","03bc1e6d":"code","59b116a9":"code","1116cb5d":"code","c20e5cf3":"code","b4ee821b":"code","a9b69a19":"code","b337f67f":"code","921e9cea":"code","9d3d5d6f":"code","cf956d47":"code","b651c016":"code","df087281":"code","534deb24":"code","2a40ef6f":"code","067fa473":"code","b5866133":"code","1b457099":"code","7e88287f":"code","f1300bca":"markdown","8275150e":"markdown","80ff31a9":"markdown","a3ef79d0":"markdown","14da85cf":"markdown","24c78a37":"markdown","8b0805e4":"markdown","6d59ce72":"markdown","de358038":"markdown","c3abc44d":"markdown","7e7fa0c7":"markdown","bd3cef35":"markdown","dc0321b2":"markdown","d001af5a":"markdown","d7751223":"markdown","a8863216":"markdown","852bf38a":"markdown","f497d48e":"markdown"},"source":{"877545a2":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/bn8rVBuIcFg?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","bd6da35b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de7d07b5":"import pandas as pd\ncal = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsteval = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nprice = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv') ","ae01deb9":"import numpy as np\nimport plotly\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nid_list = sorted(list(set(steval['id'])))\nd_cols = [col for col in steval.columns if 'd_' in col]\nx_1 = steval.loc[steval['id'] == id_list[0]].set_index('id')[d_cols].values[0][:200]\nx_2 = steval.loc[steval['id'] == id_list[12]].set_index('id')[d_cols].values[0][300:500]\nx_3 = steval.loc[steval['id'] == id_list[36]].set_index('id')[d_cols].values[0][600:800]\nx_4 = steval.loc[steval['id'] == id_list[64]].set_index('id')[d_cols].values[0][1000:1200]\nx_5 = steval.loc[steval['id'] == id_list[128]].set_index('id')[d_cols].values[0][1300:1500]\nx_6 = steval.loc[steval['id'] == id_list[256]].set_index('id')[d_cols].values[0][1600:1800]\nfig = make_subplots(rows=6, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=True, \n                         mode='lines', name=\"1st sample\",\n                         marker=dict(color=\"cadetblue\")), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=True,\n                         mode='lines', name=\"2nd sample\",\n                         marker=dict(color=\"firebrick\")), row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=True,\n                         mode='lines', name=\"3rd sample\",\n                         marker=dict(color=\"yellowgreen\")), row=3, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=x_4, showlegend=True,\n                         mode='lines', name=\"4th sample\",\n                         marker=dict(color=\"orangered\")), row=4, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_5)), y=x_5, showlegend=True,\n                         mode='lines', name=\"5th sample\",\n                         marker=dict(color=\"mediumpurple\")), row=5, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_6)), y=x_6, showlegend=True,\n                         mode='lines', name=\"6th sample\", \n                         marker=dict(color=\"navy\")), row=6, col=1)\n\nfig.update_layout(height=1200, width=900, title_text=\"Observing some randomly chosen sales trend :\")\nfig.show()","9cdcf85a":"steval.head()","25f92bba":"#For the remaining 28 days d1942 to d1969, filling with zero\n\nimport numpy as np\nfor i in range(1942,1970):\n    col = 'd_' + str(i)\n    steval[col] = 0\n    steval[col] = steval[col].astype(np.int16)","e11f2cd4":"#Check size\nsteval.info()","b4852f43":"#Check size\ncal.info()","2a812d3b":"#Check size\nprice.info()","c78c6199":"# Taken from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\nimport numpy as np\ndef reduce_mem_usage(df):\n   \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","d5797d7f":"reduce_mem_usage(steval)","e78626ca":"reduce_mem_usage(price)","4f4d262d":"reduce_mem_usage(cal)","04c81cba":"sales = pd.melt(steval, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()","64682c7e":"sales.head()","934ffb7b":"sales = pd.merge(sales, cal, on='d', how='left')\nsales = pd.merge(sales, price, on=['store_id','item_id','wm_yr_wk'], how='left') ","3734c41d":"sales.info()","4edda572":"#Encode categorical variables. Store the categories along with their codes\nd_id = dict(zip(sales.id.cat.codes, sales.id))\nd_item_id = dict(zip(sales.item_id.cat.codes, sales.item_id))\nd_dept_id = dict(zip(sales.dept_id.cat.codes, sales.dept_id))\nd_cat_id = dict(zip(sales.cat_id.cat.codes, sales.cat_id))\nd_store_id = dict(zip(sales.store_id.cat.codes, sales.store_id))\nd_state_id = dict(zip(sales.state_id.cat.codes, sales.state_id))","2f1e7ae0":"#Removing \"d_\" prefix from the values of column \"d\"\nsales.d = sales['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\ncols = sales.dtypes.index.tolist()\ntypes = sales.dtypes.values.tolist()\nfor i,type in enumerate(types):\n    if type.name == 'category':\n        sales[cols[i]] = sales[cols[i]].cat.codes","86966750":"#Dropping date column        \nsales.drop('date',axis=1,inplace=True)","43937367":"lags = [1,2,4,8,16,32]\nfor lag in lags:\n    sales['sold_lag_'+str(lag)] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)","4e6ab570":"#Clear some space\nimport gc\ngc.collect()","ab298140":"#Combination of two vars with \"sold\" and their mean\nsales['item_sold_avg'] = sales.groupby('item_id')['sold'].transform('mean').astype(np.float16)\nsales['state_sold_avg'] = sales.groupby('state_id')['sold'].transform('mean').astype(np.float16)\nsales['store_sold_avg'] = sales.groupby('store_id')['sold'].transform('mean').astype(np.float16)\nsales['cat_sold_avg'] = sales.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\nsales['dept_sold_avg'] = sales.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\n\n#Combination of three vars with \"sold\" and their mean\nsales['cat_dept_sold_avg'] = sales.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\nsales['store_item_sold_avg'] = sales.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\nsales['cat_item_sold_avg'] = sales.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\nsales['dept_item_sold_avg'] = sales.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\nsales['dept_store_sold_avg'] = sales.groupby(['dept_id','store_id'])['sold'].transform('mean').astype(np.float16)\n\n#Combination of four vars with \"sold\" and their mean\nsales['store_cat_dept_sold_avg'] = sales.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\nsales['store_cat_item_sold_avg'] = sales.groupby(['store_id','cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\n\n#Some more combinations can be incorporated here, but to avoid memory allocation warning, the above combinations would suffice ","142992a4":"gc.collect()","b99379d4":"id_list = sorted(list(set(sales['id'])))\nsold_avg_cols = [col for col in sales.columns if '_sold_avg' in col]\nx_1 = sales.loc[sales['id'] == id_list[0]].set_index('id')[sold_avg_cols].values[0][:]\nx_2 = sales.loc[sales['id'] == id_list[12]].set_index('id')[sold_avg_cols].values[0][:]\nx_3 = sales.loc[sales['id'] == id_list[36]].set_index('id')[sold_avg_cols].values[0][:]\nx_4 = sales.loc[sales['id'] == id_list[64]].set_index('id')[sold_avg_cols].values[0][:]\nx_5 = sales.loc[sales['id'] == id_list[128]].set_index('id')[sold_avg_cols].values[0][:]\nx_6 = sales.loc[sales['id'] == id_list[256]].set_index('id')[sold_avg_cols].values[0][:]\nfig = make_subplots(rows=6, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=True, \n                         mode='lines+markers', name=\"First sample\",\n                         marker=dict(color=\"cadetblue\")), row=1, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=True,\n                         mode='lines+markers', name=\"Second sample\",\n                         marker=dict(color=\"firebrick\")), row=2, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=True,\n                         mode='lines+markers', name=\"Third sample\",\n                         marker=dict(color=\"yellowgreen\")), row=3, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_4)), y=x_4, showlegend=True,\n                         mode='lines+markers', name=\"Fourth sample\",\n                         marker=dict(color=\"orangered\")), row=4, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_5)), y=x_5, showlegend=True,\n                         mode='lines+markers', name=\"Fifth sample\",\n                         marker=dict(color=\"mediumpurple\")), row=5, col=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(x_6)), y=x_6, showlegend=True,\n                         mode='lines+markers', name=\"Sixth sample\", \n                         marker=dict(color=\"navy\")), row=6, col=1)\n\nfig.update_layout(height=1500, width=900, title_text=\"Observing some randomly chosen '_sold_avg' trends after applying lag:\")\nfig.show()","1aee5899":"gc.collect()","d4b2eb48":"sales['rolling_sold_mean'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.rolling(window=6).mean()).astype(np.float16)","7d1d0773":"gc.collect()","302e50af":"sales['expanding_sold_mean'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)","3fe8298d":"gc.collect()","03bc1e6d":"# Moving Average Trends\nsales['daily_avg_sold'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sold'].transform('mean').astype(np.float16)\nsales['avg_sold'] = sales.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sold'].transform('mean').astype(np.float16)\nsales['selling_trend'] = (sales['daily_avg_sold'] - sales['avg_sold']).astype(np.float16)\nsales.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)","59b116a9":"# Since we introduced lags till 32 days, data for first 31 days should be removed.\nsales = sales[sales['d']>=32]","1116cb5d":"gc.collect()","c20e5cf3":"# Save data for training\nsales.to_pickle('salesdata.pkl') #to_pickle: serializes an object to file\ndel sales","b4ee821b":"gc.collect()","a9b69a19":"data = pd.read_pickle('salesdata.pkl')\nvalidation = data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]\neval_prediction = test['sold']\nvalidation_prediction = validation['sold']","b337f67f":"gc.collect()","921e9cea":"#Get the store ids\nstores = steval.store_id.cat.codes.unique().tolist()\nfor store in stores:\n    df = data[data['store_id']==store]","9d3d5d6f":"#Split the data\nX_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']\nX_valid, y_valid = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']\nX_test = df[df['d']>=1942].drop('sold',axis=1)","cf956d47":"gc.collect()","b651c016":"from lightgbm import LGBMRegressor as lgb\nfrom hyperopt import hp, tpe, fmin\nfrom sklearn.model_selection import cross_val_score\n\nvalgrid = {'n_estimators':hp.quniform('n_estimators', 900, 1200, 100),\n           'learning_rate':hp.quniform('learning_rate', 0.1, 0.4, 0.1),\n           'max_depth':hp.quniform('max_depth', 4,8,1),\n           'num_leaves':hp.quniform('num_leaves', 25,75,25),\n           'subsample':hp.quniform('subsample', 0.5, 0.9, 0.1),\n           'colsample_bytree':hp.quniform('colsample_bytree', 0.5, 0.9, 0.1),\n           'min_child_weight':hp.quniform('min_child_weight', 200, 500, 100) \n          }\n\ndef objective(params):\n    params = {'n_estimators': int(params['n_estimators']),\n              'learning_rate': params['learning_rate'],\n              'max_depth': int(params['max_depth']),\n              'num_leaves': int(params['num_leaves']),\n              'subsample': params['subsample'],\n              'colsample_bytree': params['colsample_bytree'],\n              'min_child_weight': params['min_child_weight']}\n    \n    lgb_a = lgb(**params)\n    score = cross_val_score(lgb_a, X_train, y_train, cv=2, n_jobs=-1).mean()\n    return score\n\nbestP = fmin(fn= objective, space= valgrid, max_evals=20, rstate=np.random.RandomState(123), algo=tpe.suggest)","df087281":"gc.collect()","534deb24":"import lightgbm\n\nmodel = lightgbm.LGBMRegressor(\n        n_estimators = int(bestP['n_estimators']),\n        learning_rate = bestP['learning_rate'],\n        subsample = bestP['subsample'],\n        colsample_bytree = bestP['colsample_bytree'],\n        max_depth = int(bestP['max_depth']),\n        num_leaves = int(bestP['num_leaves']),\n        min_child_weight = int(bestP['min_child_weight']))\n\nprint('Prediction for Store: {}**'.format(d_store_id[store]))\nmodel.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid,y_valid)], eval_metric='rmse', verbose=20, early_stopping_rounds=20)\nvalidation_prediction[X_valid.index] = model.predict(X_valid)\neval_prediction[X_test.index] = model.predict(X_test)\nfilename = 'model'+str(d_store_id[store])+'.pkl'","2a40ef6f":"import joblib\n\njoblib.dump(model, filename)\ndel model, X_train, y_train, X_valid, y_valid\ngc.collect()","067fa473":"# Validation results \nvalidation = steval[['id']+['d_' + str(i) for i in range(1914,1942)]]\nvalidation['id']=pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv').id\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n\n# Evaluation results\ntest['sold'] = eval_prediction\nevaluation = test[['id','d','sold']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n\n# Mapping category ids to their categories\nevaluation.id = evaluation.id.map(d_id)","b5866133":"gc.collect()","1b457099":"# Check submission file\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\nsubmit.head()","7e88287f":"# Submission\nsubmit.to_csv('submission.csv',index=False)\nprint(\"Submission successful\")","f1300bca":"Next, we will merge calendar and price datasets with sales data to make it a single working data set.","8275150e":"We will now save the model.","80ff31a9":"Now, we will check the sizes of three datasets and reduce them to avoid getting memory allocation alert.","a3ef79d0":"We will now reduce the sizes of three datasets.","14da85cf":"We will now apply LightGBM regressor with the best parameters to get the prediction results.","24c78a37":"For day indicators (variables with prefix \"d\") in \"steval\" data set, the values can be more conveniently handled if they are reshaped to put into rows instead of columns. Because in the next step, we will actually merge the \"steval\" dataset with two other data sets (calendar and prices); and notably, in \"calendar\" data set, the column \"d\" has \"d\" prefixed values arranged in vertical format (unlike \"steval\" where they are arranged horizontally). So, we will apply \"melt\" function on \"steval\" which will unpivot the dataframe from wide format (horizontal) to long format (vertical). This will help in merging the data set with calendar data set.","8b0805e4":"# Introduction\nWelcome to the \"M5 Forecasting - Accuracy\" competition! In this competition, contestants are challenged to forecast future sales at Walmart based on heirarchical sales in the states of California, Texas, and Wisconsin. Forecasting sales, revenue, and stock prices is a classic application of machine learning in economics, and it is important because it allows investors to make guided decisions based on forecasts made by algorithms.\n\nIn this kernel, I will briefly explain the structure of dataset. Then, I will visualize the dataset using Matplotlib and Plotly. And finally, I will demonstrate how this problem can be approached with a variety of forecasting algorithms.\n\nTo get started, here is an excellent video about how to approach time series forecasting:","6d59ce72":"## .................... The End ..............................","de358038":"We will encode the categorical variables to numericals. And for convenience, we will store their categories (a list) along with their codes (another list) in separate dictionaries, so that we can use them again during submission.","c3abc44d":"Since the set of full training labels are released now, we can ignore \"sales_train_validation\" data set and proceed working with \"sales_train_evaluation\" instead. We will check some randomly selected id's trend now, just to get a view of how it is varying with time.","7e7fa0c7":"We see that evaluation data set has columns from \"d_1\" till \"d_1941\". Since we need to get the result for next 28 days i.e. from \"d_1942\" till \"d_1969\", we can fill them up with zero values, initially.","bd3cef35":"Now, the sales dataframe looks like below, compact, easy to merge with Calendar data set matched on column \"d\".","dc0321b2":"Checking all the columns in our final working data set :","d001af5a":"Now, we will introduce lags into this data set. I have introduced lags at a sequence {2^0,2^1,2^2,2^3,2^4,2^5}. You can introduce lags at some other sequence like {1,3,6,12,24,36} etc.","d7751223":"By observation, we can spot intermittency in all the samples, especially in second and fourth samples, intermittency can be spotted very prominently (Y-axis value is dropping to zero quite frequently).","a8863216":"Now, we can spot some similarity. First, second, fourth, fifth samples show somewhat similar trend now. Third and sixth ones show similar trends as well.\n\n\nNow, we will use \"rolling window\" and \"expanding window\" concepts where the sold-mean of a particular window size will be stored in two different variables. It is important to decide the size of rolling window (for getting Moving Average). Longer the rolling window size, smoother the rolling window mean estimates become. For deciding expanding window size, suppose, the number of increments between successive rolling windows is 1 period; then it partitions the entire data set into N = T \u2013 m + 1 subsamples (ref: the picture below)image.png\n![image.png](attachment:image.png)\n\n(pic taken from mathworks.com). Here, we have taken window size = 6 and expanding window size = 2 respectively.","852bf38a":"We will use LightGBM regressor model to train data. For getting the best set of hyperparameters for the LightGBM regressor, we will use hyperopt for tuning.","f497d48e":"## The dataset\nThe dataset consists of five .csv files.\n\n* calendar.csv - Contains the dates on which products are sold. The dates are in a yyyy\/dd\/mm format.\n\n* sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913].\n\n* submission.csv - Demonstrates the correct format for submission to the competition.\n\n* sell_prices.csv - Contains information about the price of the products sold per store and date.\n\n* sales_train_evaluation.csv - Available one month before the competition deadline. It will include sales for [d_1 - d_1941].\n\nIn this competition, we need to forecast the sales for [d_1942 - d_1969]. These rows form the evaluation set. The rows [d_1914 - d_1941] form the validation set, and the remaining rows form the training set. Now, since we understand the dataset and know what to predict, let us visualize the dataset."}}