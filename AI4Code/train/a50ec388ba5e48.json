{"cell_type":{"817be962":"code","b3c57948":"code","8dd2125d":"code","3916f875":"code","1aa590aa":"code","a2c26f69":"code","2d8d2755":"code","93996ebc":"code","34c37862":"code","9d6a0dfb":"code","71287cb5":"code","6b70cb8f":"code","8bd23fb5":"code","cc71f129":"code","ce4df87b":"code","98d8ac77":"code","a57ad705":"code","0627f95f":"code","c333fbe5":"code","3beb48e4":"code","de83fdcc":"code","4fdf20cf":"code","1f07d8e0":"code","82db17e3":"code","2fd0b9db":"code","f3363575":"code","9ed82ca3":"code","5aa326f5":"code","17da4880":"code","6bc8de40":"code","2dafa4e6":"code","d93a5d71":"code","a8fc8fdc":"code","a39ab746":"code","0d381f6e":"code","724e4294":"markdown","bd348636":"markdown","ac6cabd0":"markdown","2a338fb7":"markdown","ef72eddd":"markdown","762a877b":"markdown","be6b0f44":"markdown","5de839c5":"markdown","eb68b45a":"markdown","3a46a709":"markdown","9fdb4e56":"markdown","1136ae59":"markdown","9ee9110c":"markdown","41b212ee":"markdown","13b0fb1d":"markdown","f44cf382":"markdown","a690176d":"markdown","ad55e103":"markdown","4779f47d":"markdown","003a87eb":"markdown","6dd574b8":"markdown","4a2686a3":"markdown","190727fd":"markdown","3caa93ac":"markdown","a971c381":"markdown","66d2c08e":"markdown","f9e253d8":"markdown","f611f5c9":"markdown","0681f7e9":"markdown","4ae14ab3":"markdown","2cce17fd":"markdown","706b80d9":"markdown","247fe094":"markdown","f43f121e":"markdown","ddcb109f":"markdown","f46da73e":"markdown","9ae7c81d":"markdown","3679b907":"markdown"},"source":{"817be962":"n = 5\nprint(n)","b3c57948":"n += 1\nprint(n)","8dd2125d":"import pandas","3916f875":"titanicDataset = pandas.read_csv('\/kaggle\/input\/titanicdataset\/titanic.csv')","1aa590aa":"titanicDataset","a2c26f69":"titanicDataset.info()","2d8d2755":"titanicDataset.sample(10)","93996ebc":"print('Survived Mean: ', titanicDataset['Survived'].mean())\nprint('Survived Variance: ', titanicDataset['Survived'].var())\nprint('Survived Mode: ', titanicDataset['Survived'].mode()[0])","34c37862":"columnName = 'XXXX'\n\nprint(columnName + ' Mean: ', titanicDataset[columnName].mean())\nprint(columnName + ' Variance: ', titanicDataset[columnName].var())\nprint(columnName + ' Mode: ', titanicDataset[columnName].mode()[0])","9d6a0dfb":"print('Fare Mean:', titanicDataset['Fare'].mean())\nprint('Fare Variance:', titanicDataset['Fare'].var())\nprint('Fare Mode:', titanicDataset['Fare'].mode()[0])","71287cb5":"trueOrFalse = titanicDataset['Pclass'] == 1\nprint(trueOrFalse)","6b70cb8f":"titanicDataset[trueOrFalse]","8bd23fb5":"firstClass = titanicDataset[titanicDataset['Pclass'] == 1]\nsecondClass = titanicDataset[titanicDataset['Pclass'] == 2]\nthirdClass = titanicDataset[titanicDataset['Pclass'] == 3]","cc71f129":"import matplotlib.pyplot as plt","ce4df87b":"firstClassFareMean = firstClass['Fare'].mean()\nsecondClassFareMean = secondClass['Fare'].mean()\nthirdClassFareMean = thirdClass['Fare'].mean()\n\nfirstClassFareVar = firstClass['Fare'].var()\nsecondClassFareVar = secondClass['Fare'].var()\nthirdClassFareVar= thirdClass['Fare'].var()\n\nfirstClassFareMode = firstClass['Fare'].mode()[0]\nsecondClassFareMode = secondClass['Fare'].mode()[0]\nthirdClassFareMode = thirdClass['Fare'].mode()[0]","98d8ac77":"xValues = ['First', 'Second', 'Third']\nfareMeans = [firstClassFareMean, secondClassFareMean, thirdClassFareMean]\n\nplt.bar(x=xValues, height=fareMeans, color=['r', 'g', 'b'])\n\nplt.xlabel('Class')\nplt.ylabel('Mean Fare')\nplt.title('Titanic Mean Fare by Class')\nplt.show()","a57ad705":"xValues = ['First', 'Second', 'Third']\nfareVariances = [firstClassFareVar, secondClassFareVar, thirdClassFareVar]\n\nplt.bar(x=xValues, height=fareVariances, color=['r', 'g', 'b'])\n\nplt.xlabel('Class')\nplt.ylabel('Fare Variances')\nplt.title('Titanic Fare Variance by Class')\nplt.show()","0627f95f":"xValues = ['First', 'Second', 'Third']\nfareModes = [firstClassFareMode, secondClassFareMode, thirdClassFareMode]\n\nplt.bar(x=xValues, height=fareModes, color=['r', 'g', 'b'])\n\nplt.xlabel('Class')\nplt.ylabel('Fare Modes')\nplt.title('Titanic Mode Fare by Class')\nplt.show()","c333fbe5":"print('Most expensive ticket:', firstClass['Fare'].max())","3beb48e4":"numPassengersFirst = firstClass.shape[0]\nnumPassengersSecond = secondClass.shape[0]\nnumPassengersThird = thirdClass.shape[0]\n\nprint('First Class Passengers:', numPassengersFirst)\nprint('Second Class Passengers:', numPassengersSecond)\nprint('Third Class Passengers:', numPassengersThird)","de83fdcc":"categories = ['First', 'Second', 'Third']\nsizes = [numPassengersFirst, numPassengersSecond, numPassengersThird]\n\nplt.pie(x=sizes, labels=categories, autopct='%1.1f%%', colors=['r', 'g', 'y'])\nplt.title('Total passengers by class')\nplt.show()","4fdf20cf":"def mapGenderToNumbers(x):\n    \n    if x == 'male':\n        return 1\n    \n    if x == 'female':\n        return 0\n    \n    else:\n        return x","1f07d8e0":"titanicDataset['Gender'] = titanicDataset['Gender'].apply(mapGenderToNumbers)\nprint(titanicDataset['Gender'])","82db17e3":"def mapEmbarkedToNumbers(x):\n    \n    if x == 'S':\n        return 1\n    \n    if x == 'C':\n        return 2\n    \n    if x == 'Q':\n        return 3\n    \n    else:\n        return x\n    \ntitanicDataset['Embarked'] = titanicDataset['Embarked'].apply(mapEmbarkedToNumbers)\nprint(titanicDataset['Embarked'])","2fd0b9db":"def extractTitle(x):\n    \n    title = x.split(',')[1].split(\".\")[0].strip()\n    \n    return title\n\ntitanicDataset['Title'] = titanicDataset['Name'].apply(extractTitle)","f3363575":"print(titanicDataset['Title'].value_counts())","9ed82ca3":"def isTitleSpecial(x):\n    \n    specialTitles = ['Dr', 'Rev', 'Col', 'Major', 'Lady', 'Sir', 'Jonkheer', 'Capt', 'Don']\n    \n    if x in specialTitles:\n        return 1\n    \n    else:\n        return 0\n\ntitanicDataset['SpecialTitle'] = titanicDataset['Title'].apply(isTitleSpecial)","5aa326f5":"meanAge = titanicDataset['Age'].mean()\ntitanicDataset['Age'] = titanicDataset['Age'].fillna(value=meanAge)\n\nmodeEmbarked = titanicDataset['Embarked'].mode()[0]\ntitanicDataset['Embarked'] = titanicDataset['Embarked'].fillna(value=modeEmbarked)\nprint(titanicDataset.info())","17da4880":"validationRatio = 0.2\n\nvalidationSet = titanicDataset.sample(frac=validationRatio)\ntrainingSet = titanicDataset.drop(validationSet.index)","6bc8de40":"trainingInputData = trainingSet[['Pclass', 'Gender', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'SpecialTitle']]\ntrainingPredictionData = trainingSet['Survived']\n\nvalidationInputData = validationSet[['Pclass', 'Gender', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'SpecialTitle']]\nvalidationPredictionData = validationSet['Survived']","2dafa4e6":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(max_iter=500, random_state=0)","d93a5d71":"mlp.fit(trainingInputData, trainingPredictionData)","a8fc8fdc":"mlp.score(trainingInputData, trainingPredictionData)","a39ab746":"mlp.score(validationInputData, validationPredictionData)","0d381f6e":"myExample = {\n    'Pclass' : [3],\n    'Gender': [1],\n    'Age': [20],\n    'SibSp': [0],\n    'Parch': [0],\n    'Fare': [0],\n    'Embarked': [1],\n    'SpecialTitle': [0]\n}\n\nprint(mlp.predict(pandas.DataFrame(myExample)))","724e4294":"If you can understand the interaction above, then you should have no problem completing the rest of this lesson.","bd348636":"Next, we'll set the machine to learn on the training data, through the built-in *fit* function","ac6cabd0":"You should be able to see a *mean* of 0.38, a *variance* of 0.23 and a *mode* of 0. \n\nBecause the 'Survived' column is either 1 if the passenger survived and 0 otherwise, we can say that its *mean* represents the proportion of all passengers in the dataset that survived, namely $\\frac{38}{100} = $ 38\\%. This is equivalent to the average chance of selecting someone from the data set that survived the event. \n\nA mean of 38% tells us that fewer than 50% of people (i.e. 1 in 2) would survive the sinking. **A person aboard the Titanic was more likely to not survive**.\n\nThe variance gives us a measure of how much variance there is in this probability, but it's important to note that **the variance is hard to use on its own**. To get an idea of whether this value is small or larger, we'll have to compare it to another variance. But more on that later...\n\nThe *mode* being 0 is what we would expect from the *mean*. If 0 is the most common value, then, of course, the average would be under 0.5, given that we only have possible values 0 and 1.\n\nTry investigating a column on your own! Simply specify a column name between the quotation marks, and run the cell! Your options include: 'Pclass', 'Age', 'SibSp', 'Parch' (let's save 'Fares' for later)","2a338fb7":"This simply says that for any input x, if it's 'male' output 1 and otherwise output 0.\n\nThen, we simply pass it to the built-in *apply* functionality provided by Pandas and replace our old column values with the new ones. We could also define a new column if we wanted to, but given that we are simply re-writing the same information in a different format there isn't any harm in replacing our values.","ef72eddd":"We can easily select multiple columns from our dataset as our input data by listing them, while our prediction data will just be the 'Survived' column.","762a877b":"Calculating the mean, variance and mode for each is still as easy as before. But now we'll have to print 3x3 = 9 values. Printing and then comparing 9 numbers sounds pretty tedious. There's gotta be a better way to do this!\n\n## Data Visualization\n\nData Visualization is all about presenting data in clear, easy to understand ways. Different graph types are appropriate depending on what we're comparing and what we're trying to find out, but we'll see a couple of variants through this lesson.\n\nWe'll start with a **Bar Graph**, which is really handy at *comparing the same column values for different categories*, which is exactly what we're trying to do. Here's an example of a bar graph:\n\n<img src='https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fmoore2teach.files.wordpress.com%2F2015%2F02%2Fbargraph.png&f=1&nofb=1' width='50%'>\n\n\nTo create our own, we'll make use of the most popular and powerful Python package for creating graphs: **matplotlib** (some of these data scientists should really investigate the data of some better names)\n\nIt's not too difficult to use, and even if you don't have any programming experience at all you'll be able to follow along with no problems.\n\nJust as before we have to import it first. But this time we'll give assign it a nicer name because the whole thing is kind of a mouthful.","be6b0f44":"For this final part of the workbook we'll just utilize another python *framework*, namely **Scikit-Learn**. This one is really handy because it handles everything on the machine learning side for us! We simply need to provide it with the input data and the correct responses (i.e. the exam materials)\n\nWe'll start by importing and then initializing a pre-made machine learning model! You can think of this to be the machine before it has started doing any practice exams.\n\nYou'll notice a *max_iter* parameter there. This basically specifies how many practice exams we allow the machine to take while training. The parameter *random_state* is what's called a *seed*, and it just makes it so that every time train the model we obtain the same results. ","5de839c5":"You should now have a better idea of what was causing the large variance earlier! While average second and third-class tickets are somewhat comparable, with at around 20 and 15 pounds each, a first-class ticket on average more than 80 pounds! Just to give you a rough idea, when taking inflation into account, 80 pounds back in 1912 was equivalent to around **9200** pounds in 2020! \n\nLet's see the variance and mode as well!","eb68b45a":"# Notebook Environments\nThe page you're on probably looks like any other webpage that you've visited. There's some written text, with a few buttons and menus in all the usual places. But what if I was to tell you that you're looking at a Data Scientist's most trusted tool, an **interactive notebook**. \n\nThese are environments which enable you to edit and run code from a variety of programming languages, directly from your browser. In our lesson, we'll be using **Python**, but don't be discouraged if you've never programmed in Python (or any other language for that matter)! This workbook is designed so that people from all background can complete it!\n\n\nWhat really makes notebooks useful is that they are composed out **Code Cells** (think small snippets of code) which can be run in any order and as many times as we want. What we do in one cell will be seen in every other cell, which is super handy for quick, incremental experimentation.\n\n\nNotebooks also allow for **Text Cells** (sometimes referred to as markdown cells), which holds the same kinds of information which you might put in a text editor, like Microsoft Word. In fact, the text you've been reading has been text cells all along! You can edit code cells by **double-clicking** on them. If you happen to do it by accident, don't worry! Pressing **CTRL + Z** (repeatedly if need be) will undo any unwanted changes, while **CTRL + Enter** will 'run' the cell, bringing you back out of the editing mode.\n\nBelow you will find two **code cells**. The first one initializes a variable **n** to be the number 5 and prints its value. The second one adds 1 to the variable, before also printing its value. If you're not familiar with variables, just think of them as boxes within the code which keep track of different values. As for text cells, you can run the code in a cell by clicking on it, and then pressing **CTRL + Enter**\n\nTry running the two cells in a different order and for a different number of times. Can you explain the numbers that are being printed?","3a46a709":"Now it's your turn to investigate the dataset! If there's anything you're particularly curious about, feel free to use the **mean, variance, mode, conditionals and visualization** to gain some insight!\n\nIf you're not sure what you could do, here are a few suggestions:\n* Does the age differ across the different classes?\n* How does the gender of a person influence their chances of survival?\n* How many people embarked in each location?\n\nPick one of these and investigate in the empty code cells below. You can always delete cells, or move them up and down through the commands in the top right, while adding cells can be done through the buttons you see right below each cell.\n\n## Discussion 4\n\n* What insight have you gained? \n* How do you explain it?\n* In what way did you visualize the data? If you can, show your figure.","9fdb4e56":"To check that the machine isn't just rote-memorizing the correct answers and is actually gaining some insight, we're going to select a portion of the inputData (exam questions) which we don't show to the program. This is called the **validation data set**, while the other part of the data set is called the **training data set**. \n\nThe idea is to let the machine learn on the *training* set as normal, but now at the very end we test how well it is able to **generalize to questions it hasn't seen before** but checking how well it performs on the validation set. \n\n## Discussion 7\n\n* What is the benefit of having a larger training set?\n* What is the benefit of having a larger validation set?\n\nA good compromise is to have 20% of the data for validation, while the remained 80% is used for training. \n\nWe can use the built-in sample function to randomly select 20% of the dataset for the validation set, then construct the training set by removing those rows from the original dataset.","1136ae59":"# How Pandas can help with Data Science\n![](https:\/\/images.theconversation.com\/files\/350851\/original\/file-20200803-22-dfm95n.jpg?ixlib=rb-1.1.0&rect=0%2C750%2C5760%2C2880&q=45&auto=format&w=1356&h=668&fit=crop)\n\nUnfortunately, real-life Panda bears aren't very good at data science, which is a real shame because it could have really helped us, humans, out. Not to mention how cute they would have looked doing it.\n\nWe do have the next best panda-related solution though! The Pandas Python Library! If you're wondering, the name actually comes from 'Panel Data', and not from everyone's favourite black-and-white animal. \n\nYou can think of libraries as tools that are implemented in python for us, saving us the hassle of doing it ourselves! Plus, the people who build are really good at programming and spend a lot of time on them, so their solutions are probably better than what we could come up with.\n\nThe Pandas library provides most of the tools necessary for working with data of any kind. Plus, as mentioned it was made by some really smart people to be extra fast, meaning we can work with *lots* of data at the same time.\n\nWe'll start by importing Pandas. This is necessary because by default Python assumes we don't use any additional libraries.","9ee9110c":"Now let's see the number of occurrences of every title through Pandas *value_count* built-in function","41b212ee":"* What are a few interesting things that you notice about the data? \n* What seem to be the biggest factors that are affecting survival?\n\nNow run the code cell again to get a new set of 10 random passengers. Can you notice the same patterns as for the previous 10?\n\n\n## Discussion 3 \n\nDo you think we can get a good sense of the whole dataset just by looking at a few samples at a time? \n\n## Mean, Variance, Mode\n\nWe've talked about why it's important to examine the whole dataset, instead of just a few samples. But what would be a good way of summarising a data series (in our case a column)?\n\nIf we're dealing with numbers, the most popular options are:\n\n1. The **mean** tells us what the *average* value is for a series of points, but adding them all up and dividing by the number of elements we added. For example, the mean of the series $[1, 3, 5, 7]$ is $\\frac{1 + 3 + 5 + 7}{4} = 4$. This is a simple, intuitive technique which tells the value we *expect* to see if we were to randomly choose a point from the series. \n\n2. A metric closely related to the *mean* is the **variance**. This represents the *average* of the squared differences between each element and the mean of the series. For the same series as before, we know the mean is 4. This gives us the variance of $\\frac{(1-4)^2 + (3-4)^2 + (5-4)^2 + (7-4)^2}{4} = 5$. This is an important metric because the mean alone might not capture enough information about a series. Let's take for example the series $[-5, 5, 5, -5]$ and $[0, 0, 0, 0]$. They will both have the same mean, namely $0$, even though they look very different. The variance tells us how much the values in the series *vary* from the mean, which also serves to tell us how much we should trust a sampled value from a series. The reason why we square the differences is so that we'll only add positive numbers in our average, and to make up for it we can take the square root of it, which is called the **standard deviation**\n\n3. The **mode** simply tells us the element in a series that occurs most often. For example, in the series $[1, 3, 3, 1, 5, 1, 7]$, the mode of the is $1$. This is another piece of information that's separate from the mean and the variance, but that could be very important depending on the application. \n\nFor non-number data (for example names), things get a bit tricker so we'll talk about them a little bit later. \n\nLet's compute the metrics discussed above for some of our columns. Pandas make this really easy for us: We can select the column we're interested in by calling *titanicDataset[columnName]*, and there are built-in functions for the mean, the variance and the mode.\n\nRun the cell below to print these for the 'Survived' column.","13b0fb1d":"That's it, Pandas has loaded the information from the file into the variable *titanicDataset*. Let's have a look at what we're working with.\n\n*Note* that in notebook environments, if the last line in a cell doesn't assign the return value to any variable, then it will have the same behaviour as if we displayed it using the base *print* function. That's what's happening here as well.","f44cf382":"You'll see we get back a list of True or False elements. The value corresponding to each passenger will be True if that passenger was travelling in first class, and False otherwise. \n\nTo select only the rows that have a value of True from our data set, it's just one more step. We supply this True\/False list back to the original data set","a690176d":"This gives us a **partial** dataset of the original, on which we can do all of the same operations! Let's save the partial datasets for each of the three classes first. Note that we don't need to calculate or save the list of True\/False values separately, we can pass it to Pandas directly like so","ad55e103":"Among some other things, we can see the *data type* for each column.\n\n* int64 values refer to **integers**. That is to say, whole numbers like -6, 0 or 123.\n* object in our case refers to **strings**. If you're not familiar with them, don't worry, they're just sequences of letters (and words)\n* float64 refers to **floats**. These are used in programming to represent values with decimal points like -1.5, 0.2 or 52.1\n\nWe can also see the *non-null count*, which just tells us for how many of the rows we have an entry in that column. Since the total number of rows is 891, any non-null count that's smaller than that means we have some passengers for which we don't know those pieces of information.\n\n* Age is the first such column we see, with 177 passengers for which the age isn't known\n* Meanwhile, the cabin number is only known for 204 passengers, but that's most likely just because only people in first class had cabins.\n* There are also 2 people for which we do not know where they embarked.\n\nThis goes to show you that datasets **are not perfect**. While this one might be excused as it is based on information from over 100 years ago, even modern datasets are going to have some wrong points. To collect valuable insights from data, however, we don't need perfect data sets, we just need the **overwhelming majority** of entries to be correct.\n\n## Why it's important to use statistics\n\nRun the code cell below to examine 10 random rows (passengers) from our data set. ","4779f47d":"What values do you see? Are you surprised? Based on your intuition, do you think the variance is low or high? \n\nLet's finally use the same metrics for the fares this time. ","003a87eb":"We can see that while the average ticket cost was 33 pounds, the most common ticket was just 8.05! What's more, look at that *variance*!\n\nWhat do you think is causing so much variance in the cost of tickets?\n\nTo investigate further, we'll need a way of selecting specific rows of our dataset based on certain conditions. Luckily Pandas has us covered once more!\n\n## Conditional Statements in Pandas\n\nIt's super easy to verify which rows of the dataset meet certain criteria. If you're familiar with programming, this is simply done through a boolean statement. If you've never heard of these before, don't worry! Just think of them as **questions** to which the answer is either True or False. \n\nFor example, if we wanted to *ask* for each row in the dataset: \" Is the value in the column Pclass equal to 1?\", we can do it as in the code cell below. If you've not seen the symbol '==' before, you can think of it as the way to ask 'is it equal to', as opposed to a single '=' which assigns a value to a variable. Run it and see what happens.","6dd574b8":"We can see now that the only column which has fewer than 891 entries in the cabin, which we decided was impossible to accurately fill in the missing values.\n\n# Can machine predict who survives?\n\n**Machine Learning** is a branch of Artificial Intelligence in which programs learn to make decisions based on examples that they are being shown. \n\nYou can think of this as having a computer take an exam on a topic which it doesn't have any information on. After completing the exam, the marker tells the computer which answers it got right and which ones it got wrong but doesn't explicitly tell it what the mistakes were. The computer tries to learn both from its mistakes and what it got correct to gain more knowledge on the exam topic. It continues taking the exam that exam and getting feedback until it is able to answer all the questions correctly.\n\nIn practice, the exam questions are replaced by **input data**, while the answers would be what we want our program to **predict** about the data. In our case, we could have our input data be the passenger information in the dataset, while the prediction would be whether or not that person survives.\n\n## Discussion 6\n\n* What information do you think would be relevant to include in the input? Which factors did we discover affect a person's survival?\n* Can you think of any issues that would arise from only verifying the computer on the one exam?\n* Do you think it's possible to predict who would survive with 100% accuracy?\n","4a2686a3":"It should take a few seconds for the training to finish. Now we can use the built-in score function which will tell us how accurate our model is (i.e. what percentage of samples are correctly classified). \n\nLet's try it on the training set first:","190727fd":"From the graph's perspective, we'll need to label our x-axis to denote which of the classes the values correspond to, while the height of the bars (the y-axis value) will simply correspond to the value calculated earlier. \n\nWe simply put this information in lists (as denoted by the square brackets), before passing it to the bar graph function of matplotlib, along with the colours (red, green, black), that each bar should have.\n\nWe also add overall labels on the x and y-axes, as well as a title, but these are not vital.\n\nFinally, we tell matplotlib to show our image!","3caa93ac":"So we can see a little bit of a drop of accuracy, but making the right classification 80% of the time is pretty impressive. Keep in mind that the machine never practised on this data, so this tells us that it was able to gain some insights into what would make someone survive or not, as opposed to memorizing the correct labels for the training set.\n\n\n* Try experimenting with different proportions for the training and validation sets. What is the performance like and how do you explain it?\n\n\nFinally, we can use the model we trained to come up with any new passengers and predict whether or not they would survive or not, using the built-in *predict* function.\n\nFeel free to modify the value for each of the columns by changing the number between the brackets. Then, simply run the cell to see whether our machine thinks this fictional passenger would survive.\n\n* Try experimenting with different values to see what makes our model classify examples one way or another!","a971c381":"Next step is to collect our data into variables.","66d2c08e":"It looks like not only did the first class fares have the highest averages, they have huge variances as well! This means that while the mean was 80 pounds, the range of prices paid was really big. Looking at the mode far, it seems like the most common first-class ticket was only 25 pounds. \n\nCurious what most expensive ticket in first class was? Run the cell below","f9e253d8":"Our model can correctly classify 83% of the samples in the training set! Considering how there must have been a considerable luck component in surviving, that is a pretty good accuracy.\n\nLet's see if the machine can generalize as well:","f611f5c9":"512 pound! That's the equivalent of a whopping **60,000 pounds** in 2020!\n\nWe haven't yet examined the number of passengers in each class. The *shape* of each data set tells us the number of rows and columns.","0681f7e9":"# Overview of Data Science\n\n## What is data?\nYou can think of data simply as **information**. You might think that sounds pretty abstract, and you'd be right! That's because data takes countless shapes and forms! Examples you might have already known include your Google search history, the items you've bought online or the weather conditions over the past year. But if you were to, for example, write down what time you woke up every day, you would, in fact, be collecting data, because you would be capturing that *information*.\n\n## How is data collected?\nFrom the last example, you might have realised that there is no shortage of ways to collect data. That being said, it's really with the advent of the internet, and especially smartphones that the aggregation of data has ramped up. Almost everything you do on an app or website is recorded, and it you don't even need to be logged in for companies to be able to know who you are. \n\nFor a glimpse into the scale and variety of data that are being collected **every minute** of the day, have a look at the image below.\n\n<img src=https:\/\/i.imgur.com\/NAtOTFU.jpg width=\"80%\">\n\n## Why is data useful?\nAs we said before, data really only represents **information**, which is really valuable. The data contains insights which can be used to make more informed decisions, leading to more successful outcomes. For example, think of the advertisements that you come across on a website like YouTube. If they showed you random ads (for anything from food, sports, video games or makeup), then you'd probably be interested (and click) on very few of the ones you are being shown. But if the website knows that you are playing video games and are not really interested in sports, instead of prompting you to buy a t-shirt from the local sports team, they could give you adverts about the latest games being released.\n\nWhile this is just one example, you can be certain that **every company** in the world is recognizing the benefits that data can bring them.\n\n## What is Data Science? Why should you care about it?\nAs we have seen before, record amounts of data are being generated nowadays. Data Science is all about gaining those insights hidden inside the data. \n\nBecause data is becoming central to the way our society operates, we must understand what kinds of data are being harvested from by and through which means, but also how that information will be used later on. But to understand how Data Science is affecting us, we need to first understand the basics of Data Science itself. And that's what this lesson is all about!\n\nYou should also know that many companies are looking to expand their capabilities of leveraging their data, but there is a severe shortage of people with the right skillset. This makes Data Scientists one of the highest paying professions right now, and there are no signs that this will change any time soon. So if you found this tutorial interesting, then this might be a career worth considering. \n\n## Discussion 1\n\n1. Which of the products in the image do you use? How do you think your data is being collected?\n2. How do you think that data is being used?\n3. Have you ever stopped using (or not signed-up to) a service because you were concerned about how your data is being used?","4ae14ab3":"We can see that the 4 most popular titles correspond to common titles (while 'Master' might sound fancy, it's actually just the common way to refer to a young boy). Later down there are also two occurrences of for Mademoiselle which is the french equivalent of Miss, and one occurrence of Madame which is the counter part of Mrs,\n\nThere are 27 people with other titles in our data set, including Doctors, Reverends, a Sir, a Lady and a Countess. \n\nWe could come up with many different categories to divide all these names, taking into account possible factors like age, gender, royalty, or military. We're going to keep things simple though, and simply take into account whether the title is a *special* one, denoting that by 1, or a common one marking it as 0. ","2cce17fd":"# Hi there!\n\nI'm so glad you can to join us for this special introduction to Data Science! By the end of this workbook, you will:\n\n1. Acquire a basic understanding of data itself, as well as data science\n2. Familiarise yourself with python notebook environments, as well as a few python frameworks\n3. Explore, summarise and visualize a real-world data set\n4. Perform feature engineering and data cleaning\n5. Teach a machine to learn from a dataset and make accurate predictions that go beyond what it was trained on.\n\nHopefully, these all sound interesting to you! Best of luck :)\n\nFor starters, click the button **Copy and Edit** in the top right corner. This will get everything set up for you to complete this lesson.","706b80d9":"## How to handle missing data? \n\nEarlier in the workbook, we discovered that some columns didn't have values for all passengers. Concretely, we were missing 177 'Age' values, 687 'Cabin' values and 2 'Embarked' values. \n\nWe'll touch on how machines can learn from data in just a little bit, but one thing that we can say now is that machine can only learn if they see examples of the same kind of data. For example, if we wanted to use the 'Age' column, we would need to either remove the passengers with missing entries from the dataset or fill in the missing spots with something. Otherwise, we wouldn't be able to use the 'Age' information.\n\n\n## Discussion 5\n\nFor each of the columns\n\n* Do you think we can come up with good enough substitutes for the missing entries?\n* Which metric could you use to approximate the missing entries?\n\n\nFortunately, Pandas once again has our backs and provides the built-in functionality of filling in missing entries. We simply have to specify which value(s) it should use.","247fe094":"## Feature Engineering\n\nWhat if we wanted to include the name column? Is there any way to represent the information (that could be relevant for survival) as a number?\n\nThis is where **Feature Engineering** comes in! You can think of it as the process of creating new features (or columns) from existing ones. \n\nWhile it's hard to believe that there might be a pattern of names that are more likely than others to survive the sinking, there is one piece of information in that column which would be interesting to analyze: the **title**! Does being a lady, a doctor or a major impact the chances of survival?\n\nThe first thing to do is to extract just the title from the rest of the name. We can do this with the function defined below. Feel free to have a look at it but don't worry too much if there's anything you don't understand.","f43f121e":"Running the cell just above, you should see a table with lots of different values. That is our *data* (or at least the first and last 5 examples of it).  Each row corresponds to a single passenger, while each column representing a different piece of information about them.\n\nAt the very bottom, we can see that our table has 891 rows and 12 columns, meaning we have 12 points of information for 891 different passengers.\n\nThis might look a little intimidating right now, so let's take a step back. Let's first by figure out what *kinds* of data we are working with. \n\nThe columns that you see above can be explained as follows:\n* PassengerId - This is just a number that is assigned to each passenger, which can be used to uniquely identify them. It's not particularly useful for us but Pandas requires such a column\n* Survived - This is a binary value (either 0 or 1). It is 1 if the person survived the event and 0 otherwise.\n* Pclass - This value can be 1, 2 or 3 and it specifies if the passenger was travelling first, second, or third class.\n* Name - The name of the passenger, formatted as (last name), (title) (first name)\n* Gender - Specifies whether the passenger was male or female\n* Age - This tells us the age (in years) of the person. Usually a whole number, but can differ in the cases where the passenger was under 1 year old. For example, a 6-month-old baby would appear with the age of 0.5\n* SibSp - This tells us the number of siblings (mostly for children) and spouses (mostly for adults) that the passenger had on board\n* Parch - This one indicates the number of parents (mostly for children) and children (mostly for adults) that the passenger had on board.\n* Ticket - The actual ticket number that the passenger had \n* Fare - Tells us how much the passenger paid for the ticket\n* Cabin - Gives us the cabin that the passenger stayed in. Only applicable for people in first class\n* Embarked - While the ship started in Southampton, on its way to New York City it made the first stop in Cherbourg, France and then a further stop in Queenstown, Ireland. This column tells us where the passenger got aboard, with possible values S, C, or Q. \n\n## Discussion 2\n\n* How do you think this data was collected? What challenges would you expect if tried to gather the information yourself?\n* Which column seems most interesting to you?\n* How do you think each column affects the survival chance of the passenger? How big would the impact be?","ddcb109f":"And just like that, we've gained access to a data scientist's best friend!\n# The Titanic - Who survives on a sinking ship?\n<img src=https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fd\/RMS_Titanic_3.jpg\/1920px-RMS_Titanic_3.jpg width=\"70%\">\n\n## The Story\n\nYou're most likely familiar with the most famous ship in the world. The RMS Titanic was the largest passenger ship of its time. On the 10th of April 1912, it began its maiden voyage from Southampton to New York City but sank just 5 days later in the Atlantic Ocean after a collision with an iceberg. Since at the time people almost universally thought that the vessel was unsinkable, it only carried a total of 20 lifeboats, which could carry a total of 1,178 people, about half of the number of people aboard. To make things worse, the crew were barely trained to use the lifeboats and many of them were only half-filled before being released. This led to the death of 1,500 out of the estimated 2,224 passengers and crew aboard.\n\nWhile the event shocked the world back in 1912, no one could have predicted the cultural impact that the Titanic would become. In a large part, this was owed to the massively successful adaptation by James Cameron.\n\n## The Data\n\nDuring this tutorial, we'll be working with a dataset which gives specifies the details of some **real** passengers of the titanic, including whether they were among the survivors or not.\n\n## The Tasks\n\n1. We'll explore and visualize the data\n2. We'll investigate the factors that could influence someone's survival probability.\n3. We'll see if a simple machine learning solution can accurately how likely someone is to survive, given the information we know about them.\n\n### Introduction to the data\n\nThe first thing we need to do is load the dataset itself. Pandas is already helping us out on that end, we can simply specify where the file containing our data is, and it will read everything for us. For this tutorial, I've already made sure that you have the file in the right place, so you just need to run the code cell below.","f46da73e":"# Data Manipulation\n\nSo far we've discussed possible metrics and graphs for representing numerical values. But how could we leverage the other kinds of information we have in our data set? \n\nWhile there are techniques of working directly on sentences and words, these are kind of complicated. Most of the time we'll be trying to convert *string* values to *numbers*. This way we'll be able to apply the powerful techniques discussed above.\n\nFor example, while the column 'Gender' has the strings 'male' and 'female' as possible values, we can easily map this into a number by saying we mark male with 1 and female with 0 (or the other way around). The result would be a column that looks just like the 'Survived' column. \n\nPandas provides a way to easily apply a function over a whole column. Don't worry if you're not familiar with defining functions in programming languages, the only thing you need to know is that they take a value as input and use it to produce an output.\n\nLet's define our function first","9ae7c81d":"For the 'Embarked' column, there are three valid values, namely 'S', 'C', and 'Q'. We can transform 'S', 'C' and 'Q' into 1, 2, and 3, which will give us results similar to the 'Pclass' column. \n","3679b907":"So most people were in third class with 491 which is to be expected, as it was the cheapest option, but actually, our data set has more passengers in first-class than in second!\n\nIf we wanted to we could also use a graph with a bar for each class to visualize theses results. But because we know that these 3 classes add up to the total number of passengers (891), it would be useful to actually see what proportion they each take up. \n\nThat's where **pie charts** come in! They represent how much of the total *pie* each category takes up. \n\nTo make a pie chart using matplotlib, we just need to provide the categories, along with their respective sizes"}}