{"cell_type":{"b4c6c365":"code","9654e00b":"code","265ae531":"code","942ab294":"code","acdd3b8e":"code","5034e497":"code","c77c322b":"code","50263eaa":"code","6fb9f9f8":"markdown","8235ada4":"markdown","4b1c3541":"markdown","4f82ad99":"markdown","62fc36e9":"markdown","60e6a917":"markdown","f550d811":"markdown"},"source":{"b4c6c365":"import tensorflow as tf\nimport numpy as np\nfrom sklearn.datasets import load_boston\nimport matplotlib.pyplot as plt\n\nboston=load_boston()\nx_train, y_train = boston.data, boston.target.reshape(-1,1)","9654e00b":"weights = np.array([-1.08011358e-01, 4.64204584e-02,  2.05586264e-02,  2.68673382e+00,\n                    -1.77666112e+01,  3.80986521e+00,  6.92224640e-04, -1.47556685e+00,\n                    3.06049479e-01, -1.23345939e-02, -9.52747232e-01,  9.31168327e-03,\n                    -5.24758378e-01])\nbias = np.array([36.459488385090246])","265ae531":"from sklearn import metrics\npred = np.matmul(x_train, weights) + bias\nprint(np.mean(np.square(y_train-pred)))\nprint(metrics.mean_squared_error(y_train, pred))","942ab294":"print(pred.shape)\nprint(y_train.shape)\nprint((y_train- pred).shape)\nprint((pred-y_train).shape)\nprint(np.ndim(pred))\nprint(np.ndim(y_train))","acdd3b8e":"print(np.mean(np.square(y_train-pred.reshape(-1,1))))\nprint(metrics.mean_squared_error(y_train, pred))","5034e497":"tf.reset_default_graph()\n\nW=tf.Variable(tf.zeros((13,1)), dtype=tf.float32)\nb=tf.Variable(0.0)\nX=tf.placeholder(tf.float32, shape=(None,x_train.shape[-1]), name='input')\nY=tf.placeholder(tf.float32, shape=(None,1), name='ouput')\n\nY_= tf.matmul(X, W) + b\n\nloss=tf.reduce_mean(tf.square(Y_-Y))\noptimizer = tf.train.GradientDescentOptimizer(0.000001)\ntrain=optimizer.minimize(loss)\ninit=tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    epochs=1000\n    sess.run(init)\n    points=[ [],[] ]\n    for i in range(epochs):\n        if(i%100==0):\n            print(i,sess.run(loss,feed_dict={X: x_train,Y:y_train}))\n        sess.run(train,feed_dict={X: x_train,Y:y_train})\n        if(i%2==0):\n            points[0].append(1+i)\n            points[1].append(sess.run(loss,feed_dict={X: x_train,Y:y_train})) \n    plt.plot(points[0],points[1],'r--')\n    plt.axis([0,epochs,0,600])#\n    plt.show()","c77c322b":"from sklearn.preprocessing import StandardScaler \n\nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)","50263eaa":"optimizer = tf.train.GradientDescentOptimizer(0.1)\ntrain=optimizer.minimize(loss)\n\nwith tf.Session() as sess:\n    epochs=300\n    sess.run(init)\n    points=[ [],[] ]\n    for i in range(epochs):\n        if(i%100==0):\n            print(i,sess.run(loss,feed_dict={X: x_train,Y:y_train}))\n        sess.run(train,feed_dict={X: x_train,Y:y_train})\n        if(i%2==0):\n            points[0].append(1+i)\n            points[1].append(sess.run(loss,feed_dict={X: x_train,Y:y_train})) \n    plt.plot(points[0],points[1],'r--')\n    plt.axis([0,epochs,0,600])#\n    plt.show()","6fb9f9f8":"Recently I am diving into tensorflow and I had this idea of implementing some machine learning algorithms in tensorflow, and I came across this problem on stackoverflow in which the asker tried to apply linear regression on the Boston Housing dataset.\n\nSo I got the idea of the dataset and I decided to try it myself.","8235ada4":"The first strange thing is when I calculate the mean square error manually and use the function from sklearn, the results are different. So I print the array 'pred' and array y_train","4b1c3541":"It can be seen from the figure above, GradientDescent has not reached the global minima yet but the loss already seemed to saturate. Also the learning rate is very small (hard to guess). With such small learning rate a lot more epochs are needed!","4f82ad99":"Now it is clear that the error was caused by not understanding the broadcasting rule of numpy.\n\nBroadcasting two arrays together follows these rules:\n\n1. If the arrays do not have the same rank, prepend the shape of the lower rank array with 1s until both shapes have the same length.\n\n2. The two arrays are said to be compatible in a dimension if they have the same size in the dimension, or if one of the arrays has size 1 in that dimension.\n\n3. The arrays can be broadcast together if they are compatible in all dimensions.\n4. After broadcasting, each array behaves as if it had shape equal to the elementwise maximum of shapes of the two input arrays.\n5. In any dimension where one array had size 1 and the other array had size greater than 1, the first array behaves as if it were copied along that dimension\n\nAccording to the first rule, `pred` will be prepended with 1 so it becomes shape \\[1,506\\] and then follows rule 5, the shape of pred becomes \\[506,506\\]","62fc36e9":"Now we can see that GradientDescent succesfully finds the global minima and reasonably fast, the learning rate is also easier to guess.","60e6a917":"Now it is the same. So get to the topic of feature scaling.","f550d811":"I first loaded the dataset, and I got some weights that is supposed to be optimal with linear regression (from the internet), I wanted to know what is the minimal loss with linear regression"}}