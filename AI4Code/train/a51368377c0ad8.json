{"cell_type":{"c6250094":"code","824e72c7":"code","3c033364":"code","1beda7aa":"code","45f93a49":"code","e4c4b9bf":"code","ff18d7bd":"code","822e80d4":"code","f4170d7a":"code","37a999cd":"code","55db4f0a":"code","7f7a0877":"code","946624b8":"code","b480e88e":"code","da2518da":"code","b4a2b93a":"code","106e1bfe":"code","ddc8f0bc":"code","9d8d222c":"code","2ab34306":"code","c46bebc1":"code","f7c6cf42":"code","626940d5":"code","94c76c90":"code","8aea7c88":"code","990ed251":"code","b7d1407a":"code","4efc5e99":"code","2e91e54c":"code","2857734e":"code","41e166f0":"code","42b29f4f":"code","f34cb6db":"code","c614e3b5":"code","5d6c0cc2":"code","da0014c8":"code","e86e1488":"code","775286f3":"code","840e2985":"code","447ac7b4":"code","cdf0a48c":"code","d4e27dff":"code","f3d00298":"code","f17b5baf":"code","0b185bbf":"code","b61673f6":"code","8f857c01":"code","40ece75a":"code","2accb351":"code","b57987da":"code","4a5f0b45":"code","79ab67b2":"code","1784f12e":"code","df3d0588":"code","7caddbd9":"code","b733fe7c":"code","0f1a27cc":"code","26bca672":"code","3fbf9b46":"code","4427df4c":"code","fc8c3d13":"code","bf55be6d":"code","44484236":"code","6575676a":"code","7631617d":"code","ef50dd35":"code","bde45e84":"code","cd52a9b4":"code","fbd510e5":"code","11bc6299":"code","b795956a":"code","d5187647":"code","3363e1a2":"code","dd05f82c":"code","27fed3e1":"markdown","f73b6c89":"markdown","60b3918b":"markdown","0289b1ee":"markdown","adf80cb0":"markdown","7040e57e":"markdown","1fbcf929":"markdown","acaae668":"markdown","ee37b7ac":"markdown","4af1b27b":"markdown","121bfdc4":"markdown","2d99c95f":"markdown","2d7a75e1":"markdown","d6767d16":"markdown","0ccebcfc":"markdown","e60dd958":"markdown","4896360c":"markdown","aec2a9bc":"markdown","52925bce":"markdown","565b9b56":"markdown","c5de7dcf":"markdown","cb61078d":"markdown","afbc4cc5":"markdown","b96de7fc":"markdown","a4f134c6":"markdown","092447dd":"markdown","9d2bae97":"markdown","c2518d75":"markdown","2f53c41f":"markdown","af30d93b":"markdown","c61a3656":"markdown","53bf1db1":"markdown","3d6acc35":"markdown","6d07b2fa":"markdown","7d92ca36":"markdown","a6efc0db":"markdown","8a71ee05":"markdown","a35c4a45":"markdown","b26a250e":"markdown","64f51c31":"markdown"},"source":{"c6250094":"# `BertTokenizer.detokenize` is not in `tf-text` stable yet (currently 2.4.3).\n# !pip install -q tf-nightly\n# !pip install -q tensorflow-text-nightly","824e72c7":"!pip install -q tensorflow-text","3c033364":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nimport os\nimport re\nimport random\nimport time\nimport collections, json","1beda7aa":"# GPU Check\ntf.config.list_physical_devices(\"GPU\")","45f93a49":"# Image features\n\nPATH = \"..\/input\/m2transformerpreprocessedimage\/features\/\"","e4c4b9bf":"captions_path = \"..\/input\/coco-2017-dataset\/coco2017\/annotations\/captions_train2017.json\"\nwith open(captions_path, \"r\") as f:\n    annotations = json.load(f)","ff18d7bd":"features_path_to_caption = collections.defaultdict(list)\nfor val in annotations[\"annotations\"]:\n    features_path = PATH + \"%012d.npy\" % (val[\"image_id\"])\n    if os.path.exists(features_path):\n        features_path_to_caption[features_path].append(val[\"caption\"])","822e80d4":"train_captions = []\nfeature_name_vector = []\n\nfor path in features_path_to_caption.keys():\n    caption_list = features_path_to_caption[path]\n    train_captions.extend(caption_list)\n    feature_name_vector.extend([path] * len(caption_list))","f4170d7a":"# @title Load objects detections model names\nALL_MODELS = {\n'CenterNet HourGlass104 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_512x512\/1',\n'CenterNet HourGlass104 Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_512x512_kpts\/1',\n'CenterNet HourGlass104 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_1024x1024\/1',\n'CenterNet HourGlass104 Keypoints 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/hourglass_1024x1024_kpts\/1',\n'CenterNet Resnet50 V1 FPN 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v1_fpn_512x512\/1',\n'CenterNet Resnet50 V1 FPN Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v1_fpn_512x512_kpts\/1',\n'CenterNet Resnet101 V1 FPN 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet101v1_fpn_512x512\/1',\n'CenterNet Resnet50 V2 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v2_512x512\/1',\n'CenterNet Resnet50 V2 Keypoints 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/centernet\/resnet50v2_512x512_kpts\/1',\n'EfficientDet D0 512x512' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d0\/1',\n'EfficientDet D1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d1\/1',\n'EfficientDet D2 768x768' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d2\/1',\n'EfficientDet D3 896x896' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d3\/1',\n'EfficientDet D4 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d4\/1',\n'EfficientDet D5 1280x1280' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d5\/1',\n'EfficientDet D6 1280x1280' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d6\/1',\n'EfficientDet D7 1536x1536' : 'https:\/\/tfhub.dev\/tensorflow\/efficientdet\/d7\/1',\n'SSD MobileNet v2 320x320' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/2',\n'SSD MobileNet V1 FPN 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v1\/fpn_640x640\/1',\n'SSD MobileNet V2 FPNLite 320x320' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/fpnlite_320x320\/1',\n'SSD MobileNet V2 FPNLite 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/ssd_mobilenet_v2\/fpnlite_640x640\/1',\n'SSD ResNet50 V1 FPN 640x640 (RetinaNet50)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet50_v1_fpn_640x640\/1',\n'SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet50_v1_fpn_1024x1024\/1',\n'SSD ResNet101 V1 FPN 640x640 (RetinaNet101)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet101_v1_fpn_640x640\/1',\n'SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet101_v1_fpn_1024x1024\/1',\n'SSD ResNet152 V1 FPN 640x640 (RetinaNet152)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet152_v1_fpn_640x640\/1',\n'SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)' : 'https:\/\/tfhub.dev\/tensorflow\/retinanet\/resnet152_v1_fpn_1024x1024\/1',\n'Faster R-CNN ResNet50 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_640x640\/1',\n'Faster R-CNN ResNet50 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_1024x1024\/1',\n'Faster R-CNN ResNet50 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet50_v1_800x1333\/1',\n'Faster R-CNN ResNet101 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_640x640\/1',\n'Faster R-CNN ResNet101 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_1024x1024\/1',\n'Faster R-CNN ResNet101 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet101_v1_800x1333\/1',\n'Faster R-CNN ResNet152 V1 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_640x640\/1',\n'Faster R-CNN ResNet152 V1 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_1024x1024\/1',\n'Faster R-CNN ResNet152 V1 800x1333' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/resnet152_v1_800x1333\/1',\n'Faster R-CNN Inception ResNet V2 640x640' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/inception_resnet_v2_640x640\/1',\n'Faster R-CNN Inception ResNet V2 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/faster_rcnn\/inception_resnet_v2_1024x1024\/1',\n'Mask R-CNN Inception ResNet V2 1024x1024' : 'https:\/\/tfhub.dev\/tensorflow\/mask_rcnn\/inception_resnet_v2_1024x1024\/1'\n}","37a999cd":"#@title Model Selection { display-mode: \"form\", run: \"auto\" }\nmodel_display_name = 'SSD MobileNet V2 FPNLite 640x640' # @param ['CenterNet HourGlass104 512x512','CenterNet HourGlass104 Keypoints 512x512','CenterNet HourGlass104 1024x1024','CenterNet HourGlass104 Keypoints 1024x1024','CenterNet Resnet50 V1 FPN 512x512','CenterNet Resnet50 V1 FPN Keypoints 512x512','CenterNet Resnet101 V1 FPN 512x512','CenterNet Resnet50 V2 512x512','CenterNet Resnet50 V2 Keypoints 512x512','EfficientDet D0 512x512','EfficientDet D1 640x640','EfficientDet D2 768x768','EfficientDet D3 896x896','EfficientDet D4 1024x1024','EfficientDet D5 1280x1280','EfficientDet D6 1280x1280','EfficientDet D7 1536x1536','SSD MobileNet v2 320x320','SSD MobileNet V1 FPN 640x640','SSD MobileNet V2 FPNLite 320x320','SSD MobileNet V2 FPNLite 640x640','SSD ResNet50 V1 FPN 640x640 (RetinaNet50)','SSD ResNet50 V1 FPN 1024x1024 (RetinaNet50)','SSD ResNet101 V1 FPN 640x640 (RetinaNet101)','SSD ResNet101 V1 FPN 1024x1024 (RetinaNet101)','SSD ResNet152 V1 FPN 640x640 (RetinaNet152)','SSD ResNet152 V1 FPN 1024x1024 (RetinaNet152)','Faster R-CNN ResNet50 V1 640x640','Faster R-CNN ResNet50 V1 1024x1024','Faster R-CNN ResNet50 V1 800x1333','Faster R-CNN ResNet101 V1 640x640','Faster R-CNN ResNet101 V1 1024x1024','Faster R-CNN ResNet101 V1 800x1333','Faster R-CNN ResNet152 V1 640x640','Faster R-CNN ResNet152 V1 1024x1024','Faster R-CNN ResNet152 V1 800x1333','Faster R-CNN Inception ResNet V2 640x640','Faster R-CNN Inception ResNet V2 1024x1024','Mask R-CNN Inception ResNet V2 1024x1024']\nmodel_handle = ALL_MODELS[model_display_name]\n\nprint('Selected model:'+ model_display_name)\nprint('Model Handle at TensorFlow Hub: {}'.format(model_handle))","55db4f0a":"print('loading model...')\nhub_model = hub.load(model_handle)\nprint('model loaded!')","7f7a0877":"def load_img(path):\n    im = tf.io.read_file(path)\n    im = tf.io.decode_jpeg(im, channels=3)\n    return im\n\nsample_image_file = \"..\/input\/coco-2017-dataset\/coco2017\/train2017\/000000000009.jpg\"\nsample_im = load_img(sample_image_file)\n\nplt.imshow(sample_im)\nplt.axis(\"off\")\nplt.show()\n\n# Reshape to (1, height, width, channel) for detections\nsample_im = tf.expand_dims(sample_im, axis=0)","946624b8":"%%time\n\nresult = hub_model(sample_im)","b480e88e":"def get_image_detections(result, max_detections=20):\n    detections = result[\"detection_boxes\"]\n    best_ind = tf.image.non_max_suppression(boxes=result[\"detection_boxes\"][0],\n                                          scores=result[\"detection_scores\"][0],\n                                          max_output_size=max_detections)\n\n    detections = tf.gather(detections, best_ind, axis=1)\n    return detections  # (1, num_detections, bbox)\n\ndetections = get_image_detections(result) ","da2518da":"colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 0.0]])\ntest_im = tf.image.draw_bounding_boxes(images=sample_im.numpy(), \n                                       boxes=detections, \n                                       colors=colors)\n\nplt.figure(figsize=(8, 8))\nplt.imshow( (test_im \/ 255.)[0] )\nplt.axis(\"off\")\nplt.show()","b4a2b93a":"def extract_image_regions(image, detections):\n  \n    h, w = image.shape[1:3]\n    patches = detections * [h, w, h, w]\n    regions = []\n    coors = tf.cast(patches, tf.int32) # (1, num_detections, 4)\n\n    # Remove the first axis before iterate\n    for coor in tf.squeeze(coors):\n        y_min, x_min, y_max, x_max = coor\n        regions.append(image[:, y_min:y_max, x_min:x_max, : ])\n\n    return regions\n\nim_regions = extract_image_regions(sample_im, detections)\nplt.imshow(im_regions[0][0])\nplt.axis(\"off\")\nplt.show()","106e1bfe":"image_features_extractor = tf.keras.applications.ResNet101(include_top=False, \n                                                           weights=\"imagenet\", \n                                                           pooling=\"avg\")\nimage_features_extractor.trainable = False\nimage_features_extractor(tf.random.uniform((1, 200, 100, 3)))","ddc8f0bc":"def detect_and_encode_image(image, max_detections=20):\n    \"\"\"\n    Perform an object detection model from a specific image to produce bbox on \n    detected objects. The bbox result then become the coordinates for image\n    regions that'll be extracted from the images. The results of the image regions \n    before will be encoded with pretrained-CNN model.\n\n    args:\n    image: A 4D tensor with shape [1, height, width, channel].\n    max_detections: Total detections needed from object detection model, cap to 300.\n\n    return:\n    image_regions_features \n    \"\"\"\n    assert max_detections <= 300, \"max detections limits is 300\"\n\n    # Feed image into object detection model\n    object_detection_result = hub_model(image)\n\n    # Get 'max_detections' bounding boxes\n    detections = get_image_detections(object_detection_result, max_detections=max_detections)\n\n    # Extract the region of image based on the bounding boxes values\n    regions = extract_image_regions(image, detections)\n\n    features = np.zeros( (len(regions), 2048) )\n\n    for index, r in enumerate(regions):\n        feature = image_features_extractor(r)\n        features[index] = feature\n\n    return features # (max_detections, 2048)","9d8d222c":"%%time\n\nsample_feature = detect_and_encode_image(sample_im, max_detections=20)\nsample_feature.shape","2ab34306":"BATCH_SIZE = 32","c46bebc1":"tfr_path = \"..\/input\/m2transformertfr\"","f7c6cf42":"# Dataset pipeline\ndef make_feature_dataset():\n    # Read TFRecord file\n    filenames = tf.io.gfile.glob(tfr_path + \"\/img_detections\/*.tfrec\")\n    filenames = sorted(filenames)\n    feature_dataset = tf.data.TFRecordDataset(filenames)\n    features = {\"feature_raw\": tf.io.FixedLenFeature([], tf.string)}\n    \n    # Parse the input tf.train.Example proto using the dictionary above.\n    def _parse_feature_function(example_proto):\n        return tf.io.parse_single_example(example_proto, features)\n\n    # Parse our serialized tensor\n    def _parse_tensor(example_proto):\n        features = _parse_feature_function(example_proto)\n        tensors = tf.io.parse_tensor(features[\"feature_raw\"], out_type=tf.float64)\n        tensors.set_shape((None, 20, 2048)) # set_shape for TPU\n        return tensors\n    \n    return (feature_dataset.map(_parse_tensor, num_parallel_calls=tf.data.AUTOTUNE)\n                           .unbatch()\n                           .repeat()\n                           .batch(BATCH_SIZE)\n                           .prefetch(tf.data.AUTOTUNE))","626940d5":"feature_dataset = make_feature_dataset()\nfeature_dataset","94c76c90":"VOCAB_SIZE = 8000\n\nbert_tokenizer_params=dict(lower_case=True)\nreserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n\nbert_vocab_args = dict(\n    # The target vocabulary size\n    vocab_size = VOCAB_SIZE,\n    # Reserved tokens that must be included in the vocabulary\n    reserved_tokens=reserved_tokens,\n    # Arguments for `text.BertTokenizer`\n    bert_tokenizer_params=bert_tokenizer_params,\n    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n    learn_params={},\n)","8aea7c88":"tokenizer = text.BertTokenizer(tfr_path + \"\/cap_vocab.txt\", **bert_tokenizer_params)","990ed251":"# Add start and end token in captions\nSTART = tf.argmax(tf.constant(reserved_tokens) == \"[START]\")\nEND = tf.argmax(tf.constant(reserved_tokens) == \"[END]\")\n\ndef add_start_end(ragged):\n    count = ragged.bounding_shape()[0]\n    starts = tf.fill([count,1], START)\n    ends = tf.fill([count,1], END)\n    return tf.concat([starts, ragged, ends], axis=1)\n\n\ndef tokenize_text(text):\n    # Tokenize captions\n    text = tokenizer.tokenize(text) # (batch, word, wordpiece)\n\n    # Merge the wordpiece axis\n    text = text.merge_dims(-2, -1) # (batch, word)\n\n    # Concat [START] and [END] tokens\n    text = add_start_end(text)  \n\n    # Convert ragged.tensor into tf.tensor\n    text = text.to_tensor()\n\n    return text","b7d1407a":"def make_caption_dataset():\n    # Caption text datasets\n    filenames = tf.io.gfile.glob(tfr_path + \"\/captions\/*.tfrec\")\n    filenames = sorted(filenames)\n    caption_dataset = tf.data.TFRecordDataset(filenames)\n    features = {\"caption\": tf.io.FixedLenFeature([], tf.string)}\n    \n    def _parse_feature_function(example_proto):\n        return tf.io.parse_single_example(example_proto, features)\n    \n    def _parse_caption(caption):\n        features = _parse_feature_function(caption) # parse example\n        captions = tf.io.parse_tensor(features[\"caption\"], tf.string)\n        return captions\n    \n    return (caption_dataset.map(_parse_caption, num_parallel_calls=tf.data.AUTOTUNE)\n                           .unbatch()\n                           .repeat()\n                           .batch(BATCH_SIZE)\n                           .map(tokenize_text, num_parallel_calls=tf.data.AUTOTUNE)\n                           .prefetch(tf.data.AUTOTUNE))","4efc5e99":"caption_dataset = make_caption_dataset()\ncaption_dataset","2e91e54c":"def get_angles(pos, i, d_model):\n    angle_rates = 1 \/ np.power(10000, (2 * (i\/\/2)) \/ np.float32(d_model))\n    return pos * angle_rates","2857734e":"def positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n\n    pos_encoding = angle_rads[np.newaxis, ...]\n\n    return tf.cast(pos_encoding, dtype=tf.float32)","41e166f0":"n, d = 2048, 512\npos_encoding = positional_encoding(n, d)\nprint(pos_encoding.shape)\npos_encoding = pos_encoding[0]\n\n# Juggle the dimensions for the plot\npos_encoding = tf.reshape(pos_encoding, (n, d\/\/2, 2))\npos_encoding = tf.transpose(pos_encoding, (2, 1, 0))\npos_encoding = tf.reshape(pos_encoding, (d, n))\n\nplt.pcolormesh(pos_encoding, cmap='RdBu')\nplt.ylabel('Depth')\nplt.xlabel('Position')\nplt.colorbar()\nplt.show()","42b29f4f":"def create_padding_mask(seq, m_slots=None):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n  \n    # Memory slots pads\n    if m_slots:\n        m_pad = tf.zeros((tf.shape(seq)[0], m_slots))  # (batch_size, m_slots)\n        seq = tf.concat([seq, m_pad], axis=1)\n\n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)","f34cb6db":"x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]], dtype=tf.float32)\ncreate_padding_mask(x, None)","c614e3b5":"def create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len)","5d6c0cc2":"x = tf.random.uniform((1, 3))\ntemp = create_look_ahead_mask(x.shape[1])\ntemp","da0014c8":"def scaled_dot_product_attention(q, k, v, mask):\n    \"\"\"Calculate the attention weights.\n    q, k, v must have matching leading dimensions.\n    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n    The mask has different shapes depending on its type(padding or look ahead)\n    but it must be broadcastable for addition.\n\n    Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n    Returns:\n    output, attention_weights\n    \"\"\"\n\n    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk \/ tf.math.sqrt(dk)\n\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n\n    # softmax is normalized on the last axis (seq_len_k) so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n    return output, attention_weights","e86e1488":"def print_out(q, k, v):\n    temp_out, temp_attn = scaled_dot_product_attention(\n      q, k, v, None)\n    print('Attention weights are:')\n    print(temp_attn)\n    print('Output is:')\n    print(temp_out)","775286f3":"np.set_printoptions(suppress=True)\n\ntemp_k = tf.constant([[10, 0, 0],\n                      [0, 10, 0],\n                      [0, 0, 10],\n                      [0, 0, 10]], dtype=tf.float32)  # (4, 3)\n\ntemp_v = tf.constant([[1, 0],\n                      [10, 0],\n                      [100, 5],\n                      [1000, 6]], dtype=tf.float32)  # (4, 2)","840e2985":"# This `query` aligns with the second `key`,\n# so the second `value` is returned.\ntemp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\nprint_out(temp_q, temp_k, temp_v)","447ac7b4":"class MultiHeadAttentionMemory(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, m_slots):\n        super(MultiHeadAttentionMemory, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n\n        assert d_model % self.num_heads == 0\n\n        self.depth = d_model \/\/ self.num_heads\n        self.m_slots = m_slots\n\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n\n        if self.m_slots:\n            self.mk, self.mv = self.get_memories(m_slots)\n\n        self.dense = tf.keras.layers.Dense(d_model)\n  \n\n    def get_memories(self, m_slots):\n        mk = self.add_weight(name=\"memory_k\", \n                            shape=(1, m_slots, self.d_model), \n                            initializer=tf.keras.initializers.RandomNormal(stddev=1\/self.d_model))\n        mv = self.add_weight(name=\"memory_v\", \n                            shape=(1, m_slots, self.d_model), \n                            initializer=tf.keras.initializers.RandomNormal(stddev=1\/self.m_slots))\n        return mk, mv\n\n    def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n\n    def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # ...\n        v = self.wv(v)  # ...\n\n        q = self.split_heads(q, batch_size) # (batch_size, num_heads, seq_len, depth)\n        k = self.split_heads(k, batch_size)  \n        v = self.split_heads(v, batch_size)\n    \n        if self.m_slots:\n            m_k = self.split_heads(self.mk, batch_size=1) # (batch_size, num_heads, memory_slots, depth)\n            m_v = self.split_heads(self.mv, batch_size=1) # ...\n            k = tf.concat(\n              [k, tf.tile(m_k, [batch_size, 1, 1, 1])], axis=2) # (batch_size, num_heads, memory_slots + seq_len_k, depth)\n            v = tf.concat(\n              [v, tf.tile(m_v, [batch_size, 1, 1, 1])], axis=2) # ...\n\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k + memory_slots)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n    \n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n        concat_attention = tf.reshape(scaled_attention,\n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n    \n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n        return output, attention_weights","cdf0a48c":"temp_mham = MultiHeadAttentionMemory(d_model=512, num_heads=8, m_slots=40)\n\ny = tf.random.uniform((64, 60, 2048))\nsample_mask = create_padding_mask(tf.reduce_sum(y, axis=-1), m_slots=40)\n\noutput, attn = temp_mham(y, k=y, q=y, mask=sample_mask)\noutput.shape, attn.shape","d4e27dff":"sample_mask.shape","f3d00298":"def point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n  ])","f17b5baf":"sample_ffn = point_wise_feed_forward_network(512, 2048)\nsample_ffn(tf.random.uniform((64, 50, 512))).shape","0b185bbf":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, m_slots, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n\n        self.mha = MultiHeadAttentionMemory(d_model, num_heads, m_slots)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n\n    def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        out1 = self.dropout1(attn_output, training=training) # (batch_size, input_seq_len, d_model)\n\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n\n        return out2","b61673f6":"sample_encoder_layer = EncoderLayer(512, 8, 40, 2048)\n\nsample_encoder_layer_output = sample_encoder_layer(\n    tf.random.uniform((64, 20, 2048)), False, None)\n\nsample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)","8f857c01":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, m_slots, dff, rate=0.1):\n        super(Encoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.enc_layers = [EncoderLayer(d_model, num_heads, m_slots, dff, rate)\n                           for _ in range(num_layers)]\n\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.dropout(x, training=training)\n\n        enc_outputs = []\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](x, training, mask)\n            enc_outputs.append(x)\n\n        return x, enc_outputs  # (batch_size, input_seq_len, d_model)","40ece75a":"sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, m_slots=40, dff=2048)\nsample_encoder_output, sample_all_encoder_output = sample_encoder(\n    tf.random.uniform((64, 20, 2048)), False, None\n)\nsample_encoder_output.shape, sample_all_encoder_output[0].shape","2accb351":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, m_slots, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n\n        self.self_attn = MultiHeadAttentionMemory(d_model, num_heads, m_slots)\n\n        self.enc_attn = MultiHeadAttentionMemory(d_model, num_heads, m_slots)\n\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n\n        self.fc_alphas = [tf.keras.layers.Dense(d_model) for i in range(num_layers)]\n\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n\n\n    def call(self, x, enc_outputs, training, look_ahead_mask, padding_mask):\n    \n        assert len(enc_outputs) == len(self.fc_alphas)\n\n        N = len(enc_outputs)\n\n        # Self-Attention\n        self_out, self_attn_weights = self.self_attn(x, x, x, look_ahead_mask)\n        self_out = self.dropout1(self_out, training=training)\n\n        mesh_outs = []\n        for i in range(N):\n            # Cross-Attention between self_out and enc_outputs\n            cross_out, cross_attn_weights = self.enc_attn(enc_outputs[i], \n                                                        enc_outputs[i], \n                                                        self_out, \n                                                        padding_mask)\n      \n            alpha_out = tf.nn.sigmoid(\n                self.fc_alphas[i](tf.concat([self_out, cross_out], axis=-1))\n            )\n            mesh_outs.append(alpha_out * cross_out)\n    \n        mesh_out = tf.add_n(mesh_outs) \/ tf.math.sqrt(float(N))\n        mesh_out = self.dropout2(mesh_out, training=training)\n        mesh_out = self.layernorm1(mesh_out + self_out)\n    \n        ffn_out = self.ffn(mesh_out)\n        ffn_out = self.dropout3(ffn_out, training=training)\n        ffn_out = self.layernorm2(ffn_out + mesh_out) # (batch_size, seq_len_q, d_model)\n\n        return ffn_out, self_attn_weights, cross_attn_weights","b57987da":"# Decoder layer don't issue memory slots\nsample_decoder_layer = DecoderLayer(num_layers=2, d_model=512, num_heads=8, m_slots=None, dff=2048)\nsample_decoder_output, _, _ = sample_decoder_layer(\n    tf.random.uniform((64, 30, 2048)), sample_all_encoder_output, False, None, None\n)\nsample_decoder_output.shape","4a5f0b45":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, m_slots, dff, target_vocab_size,\n                   maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n\n        self.decoder_layers = [\n          DecoderLayer(num_layers, d_model, num_heads, m_slots, dff, rate) \n          for _ in range(num_layers) \n        ]\n        self.dropout = tf.keras.layers.Dropout(rate)\n\n  \n    def call(self, x, enc_outputs, training, look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n\n        x = self.embedding(x)\n        x *= tf.math.sqrt(float(self.d_model))\n        x += self.pos_encoding[:, :seq_len, :]\n\n        x = self.dropout(x, training=training)\n\n        for i in range(self.num_layers):\n            x, self_attn_weights, cross_attn_weights = self.decoder_layers[i](x, enc_outputs, training, \n                                                                              look_ahead_mask, padding_mask)\n\n        return x, self_attn_weights, cross_attn_weights","79ab67b2":"sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, m_slots=None, dff=2048, \n                         target_vocab_size=8000, maximum_position_encoding=5000)\n\nsample_decoder_output, self_attn, cross_attn = sample_decoder(\n    tf.random.uniform((64, 30)), sample_all_encoder_output, False, None, None\n)\nsample_decoder_output.shape, self_attn.shape, cross_attn.shape","1784f12e":"class M2_Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, m_slots, dff, target_vocab_size, \n               pe_target, rate=0.1):\n        super(M2_Transformer, self).__init__()\n\n        self.encoder = Encoder(num_layers, d_model, num_heads, m_slots, dff)\n        # Decoder didn't use memory slots\n        self.decoder = Decoder(num_layers, d_model, num_heads, None, dff, \n                                 target_vocab_size, pe_target)\n\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n\n\n    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n        enc_output, all_enc_outputs = self.encoder(inp, training, mask=enc_padding_mask)\n        dec_output, self_attn, cross_attn = self.decoder(\n            tar, all_enc_outputs, training, look_ahead_mask, dec_padding_mask\n        )\n\n        final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n\n        return final_output","df3d0588":"sample_m2_transformer = M2_Transformer(num_layers=2, d_model=512, num_heads=8, m_slots=40, dff=2048,\n                                       target_vocab_size=8000, pe_target=5000)\n\nx = tf.random.uniform((64, 20, 2048))\ny = tf.random.uniform((64, 25))\n\nsample_enc_mask = create_padding_mask(tf.reduce_sum(x, axis=-1), m_slots=40)\nsample_dec_mask = create_padding_mask(tf.reduce_sum(x, axis=-1), m_slots=None)\n\nsample_final_output = sample_m2_transformer(\n    inp=x, \n    tar=y, \n    training=False,\n    enc_padding_mask=sample_enc_mask,\n    look_ahead_mask=None, \n    dec_padding_mask=sample_dec_mask\n)\n\nsample_final_output.shape","7caddbd9":"num_layers = 6\nd_model = 512\nm_slots = 40\ndff = 2048\nnum_heads = 8\ndropout_rate = 0.1","b733fe7c":"optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)","0f1a27cc":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True, reduction='none')","26bca672":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_sum(loss_) \/ tf.reduce_sum(mask)\n\nsample_real = tf.constant([1, 0, 1])\nsample_pred = tf.constant([[0.1, 0.9], [0.1, 0.9], [0.1, 0.9]])\nloss_function(sample_real, sample_pred)","3fbf9b46":"def accuracy_function(real, pred):\n    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    accuracies = tf.math.logical_and(mask, accuracies)\n\n    accuracies = tf.cast(accuracies, dtype=tf.float32)\n    mask = tf.cast(mask, dtype=tf.float32)\n\n    return tf.reduce_sum(accuracies)\/tf.reduce_sum(mask)\n\n# sample_real = tf.constant([[1, 1, 2, 0]])\n# sample_pred = tf.constant([[[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 0]]])\n# accuracy_function(sample_real, sample_pred)","4427df4c":"train_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.Mean(name='train_accuracy')","fc8c3d13":"m2_transformer = M2_Transformer(num_layers=num_layers, \n                                d_model=d_model, \n                                num_heads=num_heads, \n                                m_slots=m_slots, \n                                dff=dff,\n                                target_vocab_size=VOCAB_SIZE, \n                                pe_target=1000,\n                                rate=dropout_rate)","bf55be6d":"def create_masks(inp, tar):\n    # Encoder padding mask\n    enc_padding_mask = create_padding_mask(inp, m_slots=m_slots)\n\n    # Used in the 2nd attention block in the decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = create_padding_mask(inp)\n\n    # Used in the 1st attention block in the decoder.\n    # It is used to pad and mask future tokens in the input received by\n    # the decoder.\n    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n\n    return enc_padding_mask, combined_mask, dec_padding_mask","44484236":"checkpoint_path = \".\/checkpoints\/train\"\n\nckpt = tf.train.Checkpoint(transformer=m2_transformer,\n                           optimizer=optimizer)\n\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print('Latest checkpoint restored!!')","6575676a":"# Restore from previous training ckpt\nprev_ckpt = tf.train.latest_checkpoint(\"..\/input\/m2checkpoint\/checkpoints\/train\")\nckpt.restore(prev_ckpt)","7631617d":"EPOCHS = 20","ef50dd35":"# The @tf.function trace-compiles train_step into a TF graph for faster\n# execution. The function specializes to the precise shape of the argument\n# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n# batch sizes (the last batch is smaller), use input_signature to specify\n# more generic shapes.\n\ntrain_step_signature = [\n    tf.TensorSpec(shape=(None, 20, 2048), dtype=tf.float64),\n    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n]\n\n@tf.function(input_signature=train_step_signature)\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n\n    # Remove depth from inp before create mask\n    inp_mask = tf.reduce_sum(inp, axis=-1)\n\n    # Create masks\n    inp_padding_mask, tar_look_ahead_mask, tar_padding_mask = create_masks(inp_mask, tar_inp)\n\n    with tf.GradientTape() as tape:\n        predictions = m2_transformer(inp=inp,\n                                     tar=tar_inp,\n                                     training=True,\n                                     enc_padding_mask=inp_padding_mask,\n                                     look_ahead_mask=tar_look_ahead_mask,\n                                     dec_padding_mask=tar_padding_mask)\n        loss = loss_function(tar_real, predictions)\n\n    gradients = tape.gradient(loss, m2_transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, m2_transformer.trainable_variables))\n\n    train_loss(loss)\n    train_accuracy(accuracy_function(tar_real, predictions))","bde45e84":"train_dataset = tf.data.Dataset.zip((feature_dataset, caption_dataset))","cd52a9b4":"# %%time\n\n# for feature, caption in train_dataset.take(1):\n#     train_step(feature, caption)","fbd510e5":"steps_per_epoch = len(train_captions) \/\/ BATCH_SIZE\ntrain_dataset = train_dataset.take(steps_per_epoch)","11bc6299":"for epoch in range(EPOCHS):\n    start = time.time()\n\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n\n    # inp -> image features, tar -> caption\n    for (batch, (inp, tar)) in enumerate(train_dataset):\n        train_step(inp, tar)\n\n        if batch % 50 == 0:\n            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n        \n\n    if (epoch + 1) % 5 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n\n    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n\n    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')","b795956a":"# val_cap_path = \"..\/input\/coco-2017-dataset\/coco2017\/annotations\/captions_val2017.json\"\n# with open(val_cap_path, \"r\") as file:\n#     val_annot = json.load(file)","d5187647":"# val_dir = \"..\/input\/coco-2017-dataset\/coco2017\/val2017\"\n# val_len = 10\n# val_img_dict = val_annot[\"images\"][:val_len]\n# val_img_paths = []\n\n# for info in val_img_dict:\n#     image_path = os.path.join(val_dir, info[\"file_name\"])\n#     val_img_paths.append(image_path)\n    \n# val_img_paths","3363e1a2":"# sample_path = \"..\/input\/sample-photos\/brs.png\"\n# sample_val = load_img(sample_path)\n# plt.imshow(sample_val);","dd05f82c":"# def evaluate(im_path):\n    \n#     image = load_img(im_path)\n#     encoded_detections = detect_and_encode_image(image[tf.newaxis, ...])\n#     encoded_detections = tf.expand_dims(encoded_detections, axis=0)\n    \n#     output = START[tf.newaxis, tf.newaxis]\n    \n#     # Remove depth from inp before create mask\n#     inp_mask = tf.reduce_sum(encoded_detections, axis=-1)\n    \n    \n#     for i in range(25):\n#         # Create masks\n#         inp_padding_mask, tar_look_ahead_mask, tar_padding_mask = create_masks(inp_mask, output)\n        \n#         predictions = m2_transformer(inp=encoded_detections,\n#                                        tar=output,\n#                                        training=False,\n#                                        enc_padding_mask=inp_padding_mask,\n#                                        look_ahead_mask=tar_look_ahead_mask,\n#                                        dec_padding_mask=tar_padding_mask)\n        \n        \n#         predictions = predictions[:, -1:, :]\n#         predicted_id = tf.argmax(predictions, axis=-1)\n#         output = tf.concat([output, predicted_id], axis=-1)\n        \n#         if predicted_id == END:\n#             break\n            \n#     words = tokenizer.detokenize(output)\n#     text = tf.strings.reduce_join(words, separator=' ', axis=-1)\n    \n#     print(text.numpy())\n#     plt.imshow(image)\n#     plt.axis(\"off\")\n#     plt.show()\n        \n#     return text, output\n\n# predicted = evaluate(sample_path)","27fed3e1":"## Encoder Layer","f73b6c89":"# Loss and Metric","60b3918b":"## Tf.Records for image features","0289b1ee":"## Text Dataset","adf80cb0":"# Setup","7040e57e":"## Scaled dot product attention","1fbcf929":"Code Test","acaae668":"Code Test","ee37b7ac":"# Text Preprocessing","4af1b27b":"# Create M2-Transformer","121bfdc4":"# Image Preprocessing\nSince we already have the preprocessed image. I will show you how the image processing process.","2d99c95f":"# Optimizer","2d7a75e1":"## Bert tokenizer dependencies","d6767d16":"Code Test","0ccebcfc":"## Point wise feed forward network","e60dd958":"## Extract image regions","4896360c":"## Masking","aec2a9bc":"## Image features from ResNet101","52925bce":"Code Test","565b9b56":"Code Test","c5de7dcf":"## Loading Selected Model from Tensorflow Hub","cb61078d":"## Setup bert tokenizer","afbc4cc5":"# Prepare MS-COCO Datasets\nWe'll only use the caption of COCO 2017 Datasets as the images have been preprocessed from this [notebook](https:\/\/www.kaggle.com\/rakkaalhazimi\/m2-transformer-data?scriptVersionId=61388283).","b96de7fc":"# Evaluate","a4f134c6":"# Set Hyperparameters","092447dd":"## Positional Encoding","9d2bae97":"## Get Sample Image","c2518d75":"# Training and Checkpoints","2f53c41f":"## Visualize the Bounding Box","af30d93b":"## Encoder","c61a3656":"## Decoder Layer","53bf1db1":"## Coco Validation Data","3d6acc35":"## Decoder","6d07b2fa":"Code Test","7d92ca36":"# Meshed-Memory-Transformer\nPaper can be found [here](https:\/\/arxiv.org\/pdf\/1912.08226v2.pdf)","a6efc0db":"## Libraries","8a71ee05":"Code Test","a35c4a45":"## Text tokenizer","b26a250e":"## Multi Head Attention Memory\n","64f51c31":"Code Test"}}