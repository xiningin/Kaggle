{"cell_type":{"46477ff2":"code","10059849":"code","e55c1710":"code","2b0dc8bf":"code","cf0a21f0":"code","8548ccbc":"code","a5e02f7c":"code","7b914aee":"code","4c2a09ea":"code","3a92291d":"code","678267d8":"code","1ab89679":"code","bddbcfa5":"code","556a9559":"code","053c4d14":"code","e68a54c9":"code","6f966152":"code","62f224b6":"code","c5a9904b":"code","baf8f7d6":"code","79531bd9":"code","7899ad24":"code","9f6e58ba":"code","d27aaff4":"code","bb429c2e":"code","86037747":"code","77283b0b":"code","596e3371":"code","e555565e":"code","c8433662":"code","24811225":"code","2a0266cf":"code","e2036112":"code","b6bbab95":"code","503a0335":"code","fe598a59":"code","e2844466":"code","a99a1976":"code","0bf57b38":"code","fd08aaef":"code","8b2a53eb":"code","b205a6e1":"code","a60b3822":"code","85d81318":"code","4e02c0a2":"code","ecaf1221":"code","303a4211":"code","425d2a71":"markdown","bdf9a09b":"markdown","393977fd":"markdown","357df1f4":"markdown","c9e79721":"markdown","308b73df":"markdown","8e1b759d":"markdown","c9d4131e":"markdown","a7651a0e":"markdown","a9f207a9":"markdown","d654866b":"markdown","498a4aed":"markdown","353c53c2":"markdown","31bce29c":"markdown","aad3b590":"markdown","1fde48cb":"markdown","f8daeed5":"markdown","7065ca36":"markdown","a20f05b6":"markdown","6b74c5ef":"markdown","daa0b83b":"markdown","f93f8c5a":"markdown","9c247b01":"markdown","9ecceb0d":"markdown","cd5b3cff":"markdown","1d2e9501":"markdown","bf243ce9":"markdown","c4837d68":"markdown","bf76c9f9":"markdown","29150a47":"markdown","7b6ba21d":"markdown","6424f376":"markdown","9d545c21":"markdown","625d6b70":"markdown","e4da3dfe":"markdown","5d19a6be":"markdown","6799cd93":"markdown","5e5cc891":"markdown","2beafb03":"markdown","52d9fec3":"markdown","c6c72349":"markdown","5918f468":"markdown","9df92a73":"markdown","25a04508":"markdown","c019b419":"markdown","a2af866a":"markdown","535a1850":"markdown","fad271a9":"markdown","d669fe22":"markdown","e7235b7b":"markdown","ea86bfac":"markdown","6894955c":"markdown","2c7852c8":"markdown","9be909ac":"markdown","45044df1":"markdown","6f64a44a":"markdown","37a6b623":"markdown","1a0753b3":"markdown","14d9b279":"markdown","1f7a9629":"markdown","2107866b":"markdown","653cf92b":"markdown","52e803d5":"markdown","3afc5563":"markdown","afd4095e":"markdown","cbc79212":"markdown","99667e15":"markdown","8c0d9dab":"markdown","ef8d48bf":"markdown","433130e4":"markdown","e861a5f7":"markdown","34daf85a":"markdown","a569aa90":"markdown","dcd74673":"markdown","32aa4161":"markdown"},"source":{"46477ff2":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport sys\nimport os","10059849":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")","e55c1710":"print(\"There are\",len(df.columns),\"columns:\")\nfor x in df.columns:\n    sys.stdout.write(str(x)+\", \")","2b0dc8bf":"df=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","cf0a21f0":"print(df.info())","8548ccbc":"print(df.head())","a5e02f7c":"fig,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","7b914aee":"print(\"Not Having Research:\",len(df[df.Research == 0]))\nprint(\"Having Research:\",len(df[df.Research == 1]))\ny = np.array([len(df[df.Research == 0]),len(df[df.Research == 1])])\nx = [\"Not Having Research\",\"Having Research\"]\nplt.bar(x,y)\nplt.title(\"Research Experience\")\nplt.xlabel(\"Canditates\")\nplt.ylabel(\"Frequency\")\nplt.show()","4c2a09ea":"y = np.array([df[\"TOEFL Score\"].min(),df[\"TOEFL Score\"].mean(),df[\"TOEFL Score\"].max()])\nx = [\"Worst\",\"Average\",\"Best\"]\nplt.bar(x,y)\nplt.title(\"TOEFL Scores\")\nplt.xlabel(\"Level\")\nplt.ylabel(\"TOEFL Score\")\nplt.show()","3a92291d":"df[\"GRE Score\"].plot(kind = 'hist',bins = 200,figsize = (6,6))\nplt.title(\"GRE Scores\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"Frequency\")\nplt.show()","678267d8":"plt.scatter(df[\"University Rating\"],df.CGPA)\nplt.title(\"CGPA Scores for University Ratings\")\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"CGPA\")\nplt.show()","1ab89679":"plt.scatter(df[\"GRE Score\"],df.CGPA)\nplt.title(\"CGPA for GRE Scores\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"CGPA\")\nplt.show()","bddbcfa5":"df[df.CGPA >= 8.5].plot(kind='scatter', x='GRE Score', y='TOEFL Score',color=\"red\")\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"TOEFL SCORE\")\nplt.title(\"CGPA>=8.5\")\nplt.grid(True)\nplt.show()","556a9559":"s = df[df[\"Chance of Admit\"] >= 0.75][\"University Rating\"].value_counts().head(5)\nplt.title(\"University Ratings of Candidates with an 75% acceptance chance\")\ns.plot(kind='bar',figsize=(20, 10))\nplt.xlabel(\"University Rating\")\nplt.ylabel(\"Candidates\")\nplt.show()","053c4d14":"plt.scatter(df[\"CGPA\"],df.SOP)\nplt.xlabel(\"CGPA\")\nplt.ylabel(\"SOP\")\nplt.title(\"SOP for CGPA\")\nplt.show()","e68a54c9":"plt.scatter(df[\"GRE Score\"],df[\"SOP\"])\nplt.xlabel(\"GRE Score\")\nplt.ylabel(\"SOP\")\nplt.title(\"SOP for GRE Score\")\nplt.show()","6f966152":"# reading the dataset\ndf = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")\n\n# it may be needed in the future.\nserialNo = df[\"Serial No.\"].values\n\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})","62f224b6":"y = df[\"Chance of Admit\"].values\nx = df.drop([\"Chance of Admit\"],axis=1)\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)","c5a9904b":"# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train[x_train.columns] = scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns] = scalerX.transform(x_test[x_test.columns])","baf8f7d6":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train,y_train)\ny_head_lr = lr.predict(x_test)\n\nprint(\"real value of y_test[1]: \" + str(y_test[1]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]: \" + str(y_test[2]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[2],:])))\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test,y_head_lr))\n\ny_head_lr_train = lr.predict(x_train)\nprint(\"r_square score (train dataset): \", r2_score(y_train,y_head_lr_train))","79531bd9":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators = 100, random_state = 42)\nrfr.fit(x_train,y_train)\ny_head_rfr = rfr.predict(x_test) \n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test,y_head_rfr))\nprint(\"real value of y_test[1]: \" + str(y_test[1]) + \" -> the predict: \" + str(rfr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]: \" + str(y_test[2]) + \" -> the predict: \" + str(rfr.predict(x_test.iloc[[2],:])))\n\n\ny_head_rf_train = rfr.predict(x_train)\nprint(\"r_square score (train dataset): \", r2_score(y_train,y_head_rf_train))","7899ad24":"from sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor(random_state = 42)\ndtr.fit(x_train,y_train)\ny_head_dtr = dtr.predict(x_test) \n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test,y_head_dtr))\nprint(\"real value of y_test[1]: \" + str(y_test[1]) + \" -> the predict: \" + str(dtr.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test[2]: \" + str(y_test[2]) + \" -> the predict: \" + str(dtr.predict(x_test.iloc[[2],:])))\n\ny_head_dtr_train = dtr.predict(x_train)\nprint(\"r_square score (train dataset): \", r2_score(y_train,y_head_dtr_train))","9f6e58ba":"y = np.array([r2_score(y_test,y_head_lr),r2_score(y_test,y_head_rfr),r2_score(y_test,y_head_dtr)])\nx = [\"LinearRegression\",\"RandomForestReg.\",\"DecisionTreeReg.\"]\nplt.bar(x,y)\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Regressor\")\nplt.ylabel(\"r2_score\")\nplt.show()","d27aaff4":"print(\"real value of y_test[5]: \" + str(y_test[5]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[5],:])))\nprint(\"real value of y_test[5]: \" + str(y_test[5]) + \" -> the predict: \" + str(rfr.predict(x_test.iloc[[5],:])))\nprint(\"real value of y_test[5]: \" + str(y_test[5]) + \" -> the predict: \" + str(dtr.predict(x_test.iloc[[5],:])))\n\nprint()\n\nprint(\"real value of y_test[50]: \" + str(y_test[50]) + \" -> the predict: \" + str(lr.predict(x_test.iloc[[50],:])))\nprint(\"real value of y_test[50]: \" + str(y_test[50]) + \" -> the predict: \" + str(rfr.predict(x_test.iloc[[50],:])))\nprint(\"real value of y_test[50]: \" + str(y_test[50]) + \" -> the predict: \" + str(dtr.predict(x_test.iloc[[50],:])))","bb429c2e":"red = plt.scatter(np.arange(0,80,5),y_head_lr[0:80:5],color = \"red\")\ngreen = plt.scatter(np.arange(0,80,5),y_head_rfr[0:80:5],color = \"green\")\nblue = plt.scatter(np.arange(0,80,5),y_head_dtr[0:80:5],color = \"blue\")\nblack = plt.scatter(np.arange(0,80,5),y_test[0:80:5],color = \"black\")\nplt.title(\"Comparison of Regression Algorithms\")\nplt.xlabel(\"Index of Candidate\")\nplt.ylabel(\"Chance of Admit\")\nplt.legend((red,green,blue,black),('LR', 'RFR', 'DTR', 'REAL'))\nplt.show()","86037747":"df[\"Chance of Admit\"].plot(kind = 'hist',bins = 200,figsize = (6,6))\nplt.title(\"Chance of Admit\")\nplt.xlabel(\"Chance of Admit\")\nplt.ylabel(\"Frequency\")\nplt.show()","77283b0b":"# reading the dataset\ndf = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")\n\n# it may be needed in the future.\nserialNo = df[\"Serial No.\"].values\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\n\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})\ny = df[\"Chance of Admit\"].values\nx = df.drop([\"Chance of Admit\"],axis=1)\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train, y_test = train_test_split(x,y,test_size = 0.20,random_state = 42)\n\n# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train[x_train.columns] = scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns] = scalerX.transform(x_test[x_test.columns])\n\ny_train_01 = [1 if each > 0.8 else 0 for each in y_train]\ny_test_01  = [1 if each > 0.8 else 0 for each in y_test]\n\n# list to array\ny_train_01 = np.array(y_train_01)\ny_test_01 = np.array(y_test_01)","596e3371":"from sklearn.linear_model import LogisticRegression\nlrc = LogisticRegression()\nlrc.fit(x_train,y_train_01)\nprint(\"score: \", lrc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(lrc.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(lrc.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_lrc = confusion_matrix(y_test_01,lrc.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_lrc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,lrc.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,lrc.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,lrc.predict(x_test)))","e555565e":"cm_lrc_train = confusion_matrix(y_train_01,lrc.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_lrc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","c8433662":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(x_train,y_train_01)\nprint(\"score: \", svm.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(svm.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(svm.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_svm = confusion_matrix(y_test_01,svm.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_svm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,svm.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,svm.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,svm.predict(x_test)))","24811225":"cm_svm_train = confusion_matrix(y_train_01,svm.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_svm_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","2a0266cf":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train_01)\nprint(\"score: \", nb.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(nb.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(nb.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_nb = confusion_matrix(y_test_01,nb.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,nb.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,nb.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,nb.predict(x_test)))","e2036112":"cm_nb_train = confusion_matrix(y_train_01,nb.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_nb_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","b6bbab95":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train,y_train_01)\nprint(\"score: \", dtc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(dtc.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(dtc.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_dtc = confusion_matrix(y_test_01,dtc.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,dtc.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,dtc.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,dtc.predict(x_test)))","503a0335":"cm_dtc_train = confusion_matrix(y_train_01,dtc.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_dtc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","fe598a59":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrfc.fit(x_train,y_train_01)\nprint(\"score: \", rfc.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(rfc.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(rfc.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_rfc = confusion_matrix(y_test_01,rfc.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,rfc.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,rfc.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,rfc.predict(x_test)))","e2844466":"cm_rfc_train = confusion_matrix(y_train_01,rfc.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_rfc_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","a99a1976":"from sklearn.neighbors import KNeighborsClassifier\n\n# finding k value\nscores = []\nfor each in range(1,50):\n    knn_n = KNeighborsClassifier(n_neighbors = each)\n    knn_n.fit(x_train,y_train_01)\n    scores.append(knn_n.score(x_test,y_test_01))\n    \nplt.plot(range(1,50),scores)\nplt.xlabel(\"k\")\nplt.ylabel(\"accuracy\")\nplt.show()\n\nknn = KNeighborsClassifier(n_neighbors = 3) # n_neighbors = k\nknn.fit(x_train,y_train_01)\nprint(\"score of 3 :\",knn.score(x_test,y_test_01))\nprint(\"real value of y_test_01[1]: \" + str(y_test_01[1]) + \" -> the predict: \" + str(knn.predict(x_test.iloc[[1],:])))\nprint(\"real value of y_test_01[2]: \" + str(y_test_01[2]) + \" -> the predict: \" + str(knn.predict(x_test.iloc[[2],:])))\n\n# confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn = confusion_matrix(y_test_01,knn.predict(x_test))\n# print(\"y_test_01 == 1 :\" + str(len(y_test_01[y_test_01==1]))) # 29\n\n# cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.title(\"Test for Test Dataset\")\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.show()\n\nfrom sklearn.metrics import precision_score, recall_score\nprint(\"precision_score: \", precision_score(y_test_01,knn.predict(x_test)))\nprint(\"recall_score: \", recall_score(y_test_01,knn.predict(x_test)))\n\nfrom sklearn.metrics import f1_score\nprint(\"f1_score: \",f1_score(y_test_01,knn.predict(x_test)))","0bf57b38":"cm_knn_train = confusion_matrix(y_train_01,knn.predict(x_train))\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm_knn_train,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"predicted y values\")\nplt.ylabel(\"real y values\")\nplt.title(\"Test for Train Dataset\")\nplt.show()","fd08aaef":"y = np.array([lrc.score(x_test,y_test_01),svm.score(x_test,y_test_01),nb.score(x_test,y_test_01),dtc.score(x_test,y_test_01),rfc.score(x_test,y_test_01),knn.score(x_test,y_test_01)])\n#x = [\"LogisticRegression\",\"SVM\",\"GaussianNB\",\"DecisionTreeClassifier\",\"RandomForestClassifier\",\"KNeighborsClassifier\"]\nx = [\"LogisticReg.\",\"SVM\",\"GNB\",\"Dec.Tree\",\"Ran.Forest\",\"KNN\"]\n\nplt.bar(x,y)\nplt.title(\"Comparison of Classification Algorithms\")\nplt.xlabel(\"Classfication\")\nplt.ylabel(\"Score\")\nplt.show()","8b2a53eb":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")\ndf=df.rename(columns = {'Chance of Admit ':'ChanceOfAdmit'})\nserial = df[\"Serial No.\"]\ndf.drop([\"Serial No.\"],axis=1,inplace = True)\ndf = (df- np.min(df))\/(np.max(df)-np.min(df))\ny = df.ChanceOfAdmit \nx = df.drop([\"ChanceOfAdmit\"],axis=1)","b205a6e1":"# for data visualization\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 1, whiten= True )  # whitten = normalize\npca.fit(x)\nx_pca = pca.transform(x)\nx_pca = x_pca.reshape(400,)\ndictionary = {\"x\":x_pca,\"y\":y}\ndata = pd.DataFrame(dictionary)\nprint(\"data:\")\nprint(data.head())\nprint(\"\\ndf:\")\nprint(df.head())","a60b3822":"df[\"Serial No.\"] = serial\nfrom sklearn.cluster import KMeans\nwcss = []\nfor k in range(1,15):\n    kmeans = KMeans(n_clusters=k)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\nplt.plot(range(1,15),wcss)\nplt.xlabel(\"k values\")\nplt.ylabel(\"WCSS\")\nplt.show()\n\nkmeans = KMeans(n_clusters=3)\nclusters_knn = kmeans.fit_predict(x)\n\ndf[\"label_kmeans\"] = clusters_knn\n\n\nplt.scatter(df[df.label_kmeans == 0 ][\"Serial No.\"],df[df.label_kmeans == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(df[df.label_kmeans == 1 ][\"Serial No.\"],df[df.label_kmeans == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(df[df.label_kmeans == 2 ][\"Serial No.\"],df[df.label_kmeans == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\ndf[\"label_kmeans\"] = clusters_knn\nplt.scatter(data.x[df.label_kmeans == 0 ],data[df.label_kmeans == 0].y,color = \"red\")\nplt.scatter(data.x[df.label_kmeans == 1 ],data[df.label_kmeans == 1].y,color = \"blue\")\nplt.scatter(data.x[df.label_kmeans == 2 ],data[df.label_kmeans == 2].y,color = \"green\")\nplt.title(\"K-means Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","85d81318":"df[\"Serial No.\"] = serial\n\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nmerg = linkage(x,method=\"ward\")\ndendrogram(merg,leaf_rotation = 90)\nplt.xlabel(\"data points\")\nplt.ylabel(\"euclidean distance\")\nplt.show()\n\nfrom sklearn.cluster import AgglomerativeClustering\nhiyerartical_cluster = AgglomerativeClustering(n_clusters = 3,affinity= \"euclidean\",linkage = \"ward\")\nclusters_hiyerartical = hiyerartical_cluster.fit_predict(x)\n\ndf[\"label_hiyerartical\"] = clusters_hiyerartical\n\nplt.scatter(df[df.label_hiyerartical == 0 ][\"Serial No.\"],df[df.label_hiyerartical == 0].ChanceOfAdmit,color = \"red\")\nplt.scatter(df[df.label_hiyerartical == 1 ][\"Serial No.\"],df[df.label_hiyerartical == 1].ChanceOfAdmit,color = \"blue\")\nplt.scatter(df[df.label_hiyerartical == 2 ][\"Serial No.\"],df[df.label_hiyerartical == 2].ChanceOfAdmit,color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"Candidates\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()\n\nplt.scatter(data[df.label_hiyerartical == 0 ].x,data.y[df.label_hiyerartical == 0],color = \"red\")\nplt.scatter(data[df.label_hiyerartical == 1 ].x,data.y[df.label_hiyerartical == 1],color = \"blue\")\nplt.scatter(data[df.label_hiyerartical == 2 ].x,data.y[df.label_hiyerartical == 2],color = \"green\")\nplt.title(\"Hierarchical Clustering\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Chance of Admit\")\nplt.show()","4e02c0a2":"print(df.head())","ecaf1221":"fig,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(df.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\nplt.show()","303a4211":"df = pd.read_csv(\"..\/input\/Admission_Predict.csv\",sep = \",\")\ndf=df.rename(columns = {'Chance of Admit ':'Chance of Admit'})\nnewDF = pd.DataFrame()\nnewDF[\"GRE Score\"] = df[\"GRE Score\"]\nnewDF[\"TOEFL Score\"] = df[\"TOEFL Score\"]\nnewDF[\"CGPA\"] = df[\"CGPA\"]\nnewDF[\"Chance of Admit\"] = df[\"Chance of Admit\"]\n\ny_new = df[\"Chance of Admit\"].values\nx_new = df.drop([\"Chance of Admit\"],axis=1)\n\n# separating train (80%) and test (%20) sets\nfrom sklearn.model_selection import train_test_split\nx_train_new, x_test_new,y_train_new, y_test_new = train_test_split(x_new,y_new,test_size = 0.20,random_state = 42)\n\n# normalization\nfrom sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx_train[x_train.columns] = scalerX.fit_transform(x_train[x_train.columns])\nx_test[x_test.columns] = scalerX.transform(x_test[x_test.columns])\n\nfrom sklearn.linear_model import LinearRegression\nlr_new = LinearRegression()\nlr_new.fit(x_train_new,y_train_new)\ny_head_lr_new = lr_new.predict(x_test_new)\n\nfrom sklearn.metrics import r2_score\nprint(\"r_square score: \", r2_score(y_test_new,y_head_lr_new))\n","425d2a71":"* Candidates with high GRE scores usually have a high CGPA score.","bdf9a09b":"### <a id='comparisonOfRegression'>Comparison of Regression Algorithms<\/a>","393977fd":"### <a id='gnb'>Gaussian Naive Bayes<\/a>","357df1f4":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 26\n* Predicted 0: 3\n\nFor Actual 0: 51\n* Predicted 1: 1\n* Predicted 0: 50","c9e79721":"### <a id='prepareForClustering'>Preparing Data for Clustering<\/a>","308b73df":"### <a id='correlationForFeature'>Correlation between All Columns<\/a>","8e1b759d":"Finding the k value:\n* As a result of the test, the best k value is 3.","c9d4131e":"GRE Score:\n* This histogram shows the frequency for GRE scores.\n* There is a density between 310 and 330. Being above this range would be a good feature for a candidate to stand out.","a7651a0e":"TOEFL Score:\n* The lowest TOEFL score is 92 and the highest Toefl score is 120. The average is 107.41.","a9f207a9":"## <a id='regression'>REGRESSION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS<\/a>","d654866b":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 28\n* Predicted 0: 1\n\nFor Actual 0: 51\n* Predicted 1: 2\n* Predicted 0: 49","498a4aed":"* The dendrogram method is used to determine the best number of clusters for hierarchical clustering. The number is 3 again.","353c53c2":"### <a id='svm'>Support Vector Machine<\/a>","31bce29c":"## [INTRODUCTION](#introduction)\n* ### [Purpose](#purpose) \n* ### [Dataset](#dataset)\n\n## [A FIRST LOOK AT THE DATASET](#firstLook)\n* ### [Importing Libraries and Reading the Dataset](#importAndRead) \n* ### [Basic Information about Dataset](#basicInfo) \n* ### [Correlation between All Columns](#correlation) \n\n## [DATA VISUALIZATION TO UNDERSTAND THE DATASET](#visualization )\n\n## [REGRESSION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS)](#regression)\n* ### [Preparing Data for Regression](#prepareForRegression) \n* ### [Linear Regression](#linearRegression) \n* ### [Random Forest Regression](#rfRegression) \n* ### [Decision Tree Regression](#dtRegression) \n* ### [Comparison of Regression Algorithms](#comparisonOfRegression) \n\n## [CLASSIFICATION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS)](#classification)\n* ### [Preparing Data for Classification](#prepareForClassification) \n* ### [Logistic Regression](#lr) \n* ### [Support Vector Machine](#svm) \n* ### [Gaussian Naive Bayes](#gnb)\n* ### [Decision Tree Classification](#dtc) \n* ### [Random Forest Classification](#rfc) \n* ### [K Nearest Neighbors Classification](#knnc)  \n* ### [Comparison of Classification Algorithms](#comparisonOfClassification) \n\n## [CLUSTERING ALGORITHMS (UNSUPERVISED MACHINE LEARNING ALGORITHMS)](#clustering)\n* ### [Preparing Data for Clustering](#prepareForClustering) \n* ### [Principal Component Analysis](#pca) \n* ### [K-means Clustering](#kmeans) \n* ### [Hierarchical Clustering](#hierarchical ) \n* ### [Comparison of Clustering Algorithms](#comparisonOfClustering) \n\n## [THE THREE IMPORTANT FEATURES](#feature)\n* ### [Correlation between All Columns](#correlationForFeature) \n* ### [The Three Features for Linear Regression](#ThreeLinearRegression) ","aad3b590":"### <a id='dtc'>Decision Tree Classification<\/a>","1fde48cb":"* Candidates with high GRE scores usually have a high SOP score.","f8daeed5":"* The 3 most important features for admission to the Master: CGPA, GRE SCORE, and TOEFL SCORE\n* The 3 least important features for admission to the Master: Research, LOR, and SOP","7065ca36":"Note about r2_score:\n* It is the regression score function. \n* The best possible score is 1.0 for r2_score.\n* It may be negative.","a20f05b6":"### <a id='purpose'>Purpose<\/a>\n\nTo apply for a master's degree is a very expensive and intensive work. With this kernel, students will guess their capacities and they will decide whether to apply for a master's degree or not.","6b74c5ef":"## <a id='classification'>CLASSIFICATION ALGORITHMS (SUPERVISED MACHINE LEARNING ALGORITHMS)<\/a>","daa0b83b":"## <a id='feature'>THE THREE IMPORTANT FEATURES<\/a>","f93f8c5a":"Test for Train Dataset:","9c247b01":"Normalization (min-max scaling):\n* It makes values  scaled to a fixed range (0-1).","9ecceb0d":"### <a id='dtRegression'>Decision Tree Regression<\/a>","cd5b3cff":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 22\n* Predicted 0: 7\n* According to Confusion Matrix, the model predicted that 23 candidate's Chances of Admit are greater than 80%. In reality, 22 of them have a Chance of Admit greater than 80%. In total, 29 candidate's Chances of Admit are greater than 80%.\n\nFor Actual 0: 51\n* Predicted 1: 1\n* Predicted 0: 50\n* According to Confusion Matrix, the model predicted that 57 candidate's Chances of Admit are less than or equal to 80%. In reality, 50 of them have a Chance of Admit less than or equal to 80%. In total, 51 candidate's Chances of Admit are less than or equal to 80%.","1d2e9501":"### <a id='correlation'>Correlation between All Columns<\/a>","bf243ce9":"* Linear regression and random forest regression algorithms were better than decision tree regression algorithm.","c4837d68":"### <a id='ThreeLinearRegression'>The Three Features for Linear Regression<\/a>","bf76c9f9":"### <a id='lr'>Logistic Regression<\/a>","29150a47":"### <a id='hierarchical '>Hierarchical Clustering<\/a>","7b6ba21d":"This is the estimate and the actual acceptance possibilities made with 3 regression algorithms for test samples with 0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75 indexes:","6424f376":"### <a id='comparisonOfClustering'>Comparison of Clustering Algorithms<\/a>","9d545c21":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 25\n* Predicted 0: 4\n\nFor Actual 0: 51\n* Predicted 1: 1\n* Predicted 0: 50","625d6b70":"train_test_split:\n* It splits the data into random train (80%) and test (20%) subsets.","e4da3dfe":"### <a id='basicInfo'>Basic Information about Dataset<\/a>\n\n","5d19a6be":"Test for Train Dataset:","6799cd93":"* All classification algorithms achieved around 90% success. The most successful one is Gaussian Naive Bayes with 96% score.","5e5cc891":"## <a id='introduction'>CLUSTERING ALGORITHMS (UNSUPERVISED MACHINE LEARNING ALGORITHMS)<\/a>","2beafb03":"### <a id='rfc'>Random Forest Classification<\/a>","52d9fec3":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 22\n* Predicted 0: 7\n\nFor Actual 0: 51\n* Predicted 1: 1\n* Predicted 0: 50","c6c72349":"Note about score:\n* It is the mean accuracy for test data and labels.\n\nNote about precision:\n* precision =  TP \/ (TP + FP)\n\nNote about recall:\n* recall = TP \/ (TP + FN)\n\nNote about F!:\n* F1 = 2 \/ ((1\/precision)+(1\/recall))","5918f468":"CGPA Scores for University Ratings:\n* As the quality of the university increases, the CGPA score increases.","9df92a73":"### <a id='prepareForClassification'>Preparing Data for Classification<\/a>","25a04508":"* These are the regression estimates for samples with 5 and 50 indexes:","c019b419":"## <a id='visualization'>DATA VISUALIZATION TO UNDERSTAND THE DATASET<\/a>","a2af866a":"### <a id='rfRegression'>Random Forest Regression<\/a>","535a1850":"Test for Train Dataset:","fad271a9":"### <a id='kmeans'>K-means Clustering<\/a>","d669fe22":"### <a id='dataset'>Dataset<\/a>\n\nThis dataset is created for prediction of graduate admissions and the dataset link is below: \n* https:\/\/www.kaggle.com\/mohansacharya\/graduate-admissions\n\nFeatures in the dataset:\n* GRE Scores (290 to 340) \n* TOEFL Scores (92 to 120) \n* University Rating (1 to 5) \n* Statement of Purpose (1 to 5) \n* Letter of Recommendation Strength (1 to 5) \n* Undergraduate CGPA (6.8 to 9.92) \n* Research Experience (0 or 1) \n* Chance of Admit (0.34 to 0.97)","e7235b7b":"df.info ():\n* It was used to find the number of samples and the number of features. \n* There is no string or null data.\n* Data types are int64 and float64.\n* Memory usage: 28.2 KB","ea86bfac":"### <a id='linearRegression'>Linear Regression<\/a>","6894955c":"Test for Train Dataset:","2c7852c8":"## <a id='firstLook'>A FIRST LOOK AT THE DATASET<\/a>","9be909ac":"df.head() and df.tail():\n* They are good to test if a feature is useful for future works.\n* Serial No. may be deleted because it stores index values. In the future this column does not benefit.","45044df1":"Test for Train Dataset:","6f64a44a":"* The first results for Linear Regression (7 features):\n<br> r_square score:  0.821208259148699\n\n* The results for Linear Regression now (3 features):                               \nr_square score:  0.8212241793299223\n\n* The two results are very close. If these 3 features (CGPA, GRE SCORE, and TOEFL SCORE) are used instead of all 7 features together, the result is not bad and performance is increased because less calculation is required.                                 \n","37a6b623":"[](http:\/\/)* All features (x) were collected in one feature with Principal Component Analysis.","1a0753b3":"* If a candidate's Chance of Admit is greater than 80%, the candidate will receive the 1 label.\n* If a candidate's Chance of Admit is less than or equal to 80%, the candidate will receive the 0 label.","14d9b279":"Confusion Matrix:\n\nFor Actual 1: 29\n* Predicted 1: 26\n* Predicted 0: 3\n\nFor Actual 0: 51\n* Predicted 1: 2\n* Predicted 0: 49","1f7a9629":"Having Research or not:\n* The majority of the candidates in the dataset have research experience.\n* Therefore, the Research will be a unimportant feature for the Chance of Admit. The correlation between Chance of Admit and Research was already lower than other correlation values.","2107866b":"### <a id='knnc'>K Nearest Neighbors Classification<\/a>\n","653cf92b":"### <a id='importAndRead'>Importing Libraries and Reading the Dataset<\/a>","52e803d5":"## <a id='introduction'>INTRODUCTION<\/a>","3afc5563":"* K-means Clustering and Hierarchical Clustering are similarly.","afd4095e":"* Candidates who graduate from good universities are more fortunate to be accepted.","cbc79212":"### <a id='prepareForRegression'>Preparing Data for Regression<\/a>","99667e15":"### <a id='pca'>Principal Component Analysis<\/a>","8c0d9dab":"* The elbow method is used to determine the best number of clusters for k-means clustering. The number is 3.","ef8d48bf":"Comment:\n* Because most candidates in the data have over 70% chance, many unsuccessful candidates are not well predicted.","433130e4":"* Candidates with high CGPA scores usually have a high SOP score.","e861a5f7":"Some important information:\n* There are 9 columns: Serial No., GRE Score, TOEFL Score, University Rating, SOP, LOR , CGPA, Research, Chance of Admit\n* There are no null records. It's good.\n* There are 400 samples in total. That's enough.","34daf85a":"* The 3 most important features for admission to the Master: CGPA, GRE SCORE, and TOEFL SCORE\n* The 3 least important features for admission to the Master: Research, LOR, and SOP","a569aa90":"### <a id='comparisonOfClassification'>Comparison of Classification Algorithms<\/a>","dcd74673":"Test for Train Dataset:","32aa4161":"* Serial No. is deleted because it stores index values. In the future this column does not benefit for regression algorithms."}}