{"cell_type":{"f8ee07e1":"code","53a3ec44":"code","15c59d39":"code","853efc85":"code","5db197ba":"code","a7551b3f":"code","098bd0b6":"code","fec8af26":"code","06178a7a":"code","1258c520":"code","0287defb":"code","d542c2e8":"code","96852b15":"code","a83b78dc":"code","bee68d9c":"code","5cfd12db":"code","f6f3de40":"code","63e747fd":"code","2afac0e7":"code","1d71cd4b":"code","26000c1f":"code","753aeed5":"code","b2c0f044":"code","42af537e":"code","c2e23710":"code","c89f1a65":"code","db900bf5":"code","5f5cdef9":"code","06589644":"code","1302bfc9":"code","e6170763":"markdown","27c0c99e":"markdown","f096e094":"markdown","055798f0":"markdown","beadb718":"markdown","87545f52":"markdown","b87dcf53":"markdown","c803004c":"markdown","63ae6027":"markdown","201803e7":"markdown","5bbb3674":"markdown","96a4f50f":"markdown","09b8f922":"markdown","f7045bf5":"markdown","a9bed878":"markdown","2f4f7679":"markdown","5d745fda":"markdown","6dbe9ca0":"markdown","d5a7b204":"markdown","12c57990":"markdown","242f4aca":"markdown","cb5e0e9b":"markdown","7794ab24":"markdown","1da747da":"markdown","01c80fcd":"markdown","0c8ab843":"markdown"},"source":{"f8ee07e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#Import the test and train datasets\n\ntest_data = pd.read_csv(\"..\/input\/test.csv\", index_col=0, low_memory = False)\ntrain_data = pd.read_csv(\"..\/input\/train.csv\", index_col=0, low_memory = False)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nfrom string import ascii_letters\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\n\n# Any results you write to the current directory are saved as output.","53a3ec44":"#Drop the 'Unnamed:0.1' column because I think it is an error\n\ntest_data=test_data.drop(['Unnamed: 0.1'],1)\n\nprint('done')\n","15c59d39":"#The mean of the default (0 or 1) gives the overall default rate\nprint(f'Default Rate in train_data: {train_data.default.mean()*100:.2f}%')\nprint(f'Default Rate in test_data:  {test_data.default.mean()*100:.2f}%')","853efc85":"#Group on Zip and displace the mean of default\ngrouped_train = train_data.groupby(['ZIP'], sort=False).mean().default #Do the function\ngrouped_train = grouped_train.sort_values(ascending=False) #Sort it by default rate\n\ngrouped_test = test_data.groupby(['ZIP'], sort=False).mean().default #Do the function\ngrouped_test = grouped_test.sort_values(ascending=False) #Sort it by default rate\n\n#Print the output\nprint('Train Data')\nprint(grouped_train)\nprint()\nprint('Zip Code with highest default rate:', grouped_train.idxmax())\nprint()\nprint('Test Data')\nprint(grouped_test)\nprint('Zip Code with highest default rate:', grouped_test.idxmax())","5db197ba":"#Calculate the mean of default in the data where year is 0\nprint(f'Train Data: {train_data.default[train_data.year==0].mean()*100:.2f}%')\nprint(f'Test Data: {train_data.default[test_data.year==0].mean()*100:.2f}%')","a7551b3f":"#Get the number of loans in each year of the test data\ntest_data.groupby('year').size()","098bd0b6":"#Use the Corr function\nprint(f'Correlation of Income and Age in Train: {train_data[\"income\"].corr(train_data[\"age\"]):.4f}')\nprint(f'Correlation of Income and Age in Test:  {test_data[\"income\"].corr(test_data[\"age\"]):.4f}')","fec8af26":"#set X_train to include features included in assignment instructions\nX_train = train_data[['ZIP','rent','education','income','loan_size','payment_timing','job_stability','occupation']]\n\n#Turn categorical features to dummies\nX_train = pd.get_dummies(X_train, columns=[\"ZIP\", \"occupation\"])\n\n#set y_train to be 'default' train_data and get dummies\ny_train = train_data.default\n\n# Create the model with 100 trees\nmodel = RandomForestClassifier(n_estimators=100,\n                              random_state=42,\n                               n_jobs=-1,\n                              oob_score = True)\n# Fit on training data\nmodel.fit(X_train, y_train)\n\n#Run the test\ny_pred = model.predict(X_train)\n\nprint(f'In-Sample Accuracy: {metrics.accuracy_score(y_train, y_pred)*100:.4}%')","06178a7a":"print(f'Out of Bag Score: {model.oob_score_*100:.4f}%')","1258c520":"#set X_test to be all data in test_data, but drop default\nX_test = test_data[['ZIP','rent','education','income','loan_size','payment_timing','job_stability','occupation']]\n\n#Turn categorical features to dummies\nX_test = pd.get_dummies(X_test, columns=[\"ZIP\", \"occupation\"])\n\n#set y_train to be 'default' train_data and get dummies\ny_test = test_data.default\n\n#Run the test\ny_pred = model.predict(X_test)\n\nprint(f' Out-of-Sample Accuracy: {metrics.accuracy_score(y_test, y_pred)*100:.4f}%')","0287defb":"#confusion matrix\ndef print_confusion_matrix(test_default,predictions_test):\n    cm = confusion_matrix(test_default,predictions_test)\n    print('Paying Applicant Approved     :', cm[0][0], '---', cm[0][0]\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,\"%\")\n    print('Paying Applicant Rejected     :', cm[0][1], '---', cm[0][1]\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,\"%\")\n    print('Defaulting Applicant Approved :', cm[1][0], '---', cm[1][0]\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,\"%\")\n    print('Defaulting Applicant Rejected :', cm[1][1], '---', cm[1][1]\/(cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1])*100,\"%\")\nprint_confusion_matrix(y_test,y_pred)","d542c2e8":"pred_proba = model.predict_proba(X_test) #Get the actual probabilities from the model, not just binary\npred_proba = pd.DataFrame(pred_proba, columns=[\"proba_pay\",\"proba_default\"]) #rename those columns to make it easier to understand\ntest_data['proba_pay']= pred_proba.proba_pay #initiate a column on the test_data for the model's predicted probability of pay\ntest_data['proba_default']=pred_proba.proba_default #initiate a column on the test_data for the model's predicted probability of default\ntest_data['y_pred'] = y_pred #initiate a column on the test_data for the model prediction (binary)\n\n#Print predicted default rate by minority status\nprint(\"Predicted Default Probability by Minority Status (%):\")\nprint(test_data.groupby([\"minority\"]).mean().proba_default*100)\n\n#Look at some other demographics\nprint()\nprint(\"Predicted Default Probability by Gender (%):\")\nprint(test_data.groupby([\"sex\"]).mean().proba_default*100)\nprint()\nprint(\"Predicted Default Probability by Status and Gender (%):\")\nprint(test_data.groupby([\"minority\",\"sex\"]).mean().proba_default*100)","96852b15":"#Print rate of acceptance rate by minority status\nprint(\"Acceptance rate by Minority Status (%):\")\nprint(100-test_data.groupby([\"minority\"]).mean().y_pred*100)\n\n#Look at some other demographics\nprint()\nprint(\"Acceptance Rate of Defaults by Gender (%):\")\nprint(100-test_data.groupby([\"sex\"]).mean().y_pred*100)\nprint()\nprint(\"Acceptance Rates of Default by Minority Status and Gender (%):\")\nprint(100-test_data.groupby([\"minority\",\"sex\"]).mean().y_pred*100)","a83b78dc":"print('Confusion Matrix of Minorities')\nprint_confusion_matrix(test_data.default[test_data.minority==1],test_data.y_pred[test_data.minority==1])\n\nprint()\nprint('Confusion Matrix of Non-Minorities')\nprint_confusion_matrix(test_data.default[test_data.minority==0],test_data.y_pred[test_data.minority==0])\n","bee68d9c":"print('Confusion Matrix of Female')\nprint_confusion_matrix(test_data.default[test_data.sex==1],test_data.y_pred[test_data.sex==1])\n\nprint()\nprint('Confusion Matrix of Male')\nprint_confusion_matrix(test_data.default[test_data.sex==0],test_data.y_pred[test_data.sex==0])","5cfd12db":"features = ['education', 'age', 'income', 'loan_size', 'payment_timing', 'job_stability']\n\nfor x in features:\n    plt.hist(train_data[x], bins=100, alpha=0.5, label='train')\n    plt.hist(test_data[x], bins=100, alpha=0.5, label='test')\n    plt.legend(loc='upper right')\n    print(x)\n    print(plt.show())\n","f6f3de40":"print(\"++++++++++++++++++\")\nprint(\"Job Stability\")\nprint(\"++++++++++++++++++\")\n\nbins = pd.qcut(train_data['job_stability'], 10) #Cut the data into deciles\nprint(\"Train Data\")\nprint(train_data.groupby(bins)['default'].mean())\nprint(\"\")\nbins = pd.qcut(test_data['job_stability'], 10) #Cut the data into deciles\nprint(\"Test Data\")\nprint(test_data.groupby(bins)['default'].mean())\n\nprint(\"\")\nprint(\"++++++++++++++++++\")\nprint(\"Age\")\nprint(\"++++++++++++++++++\")\n\nbins = pd.qcut(train_data['age'], 10) #Cut the data into deciles\nprint(\"Train Data\")\nprint(train_data.groupby(bins)['default'].mean())\nprint(\"\")\nbins = pd.qcut(test_data['age'], 10) #Cut the data into deciles\nprint(\"Test Data\")\nprint(test_data.groupby(bins)['default'].mean())\n\nprint(\"\")\nprint(\"++++++++++++++++++\")\nprint(\"Income\")\nprint(\"++++++++++++++++++\")\n\nbins = pd.qcut(train_data['income'], 10) #Cut the data into deciles\nprint(\"Train Data\")\nprint(train_data.groupby(bins)['default'].mean())\nprint(\"\")\nbins = pd.qcut(test_data['income'], 10) #Cut the data into deciles\nprint(\"Test Data\")\nprint(test_data.groupby(bins)['default'].mean())","63e747fd":"#Pivot tables for train and test of default rate by group. \n#'len default' is a bit of a misnomer, it counts how many people exist in the group.\n\nprint('+++++++++++++++++++++++++++')\nprint('Train')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(train_data,index=[\"ZIP\"],values=[\"default\"],aggfunc=[np.mean,len]))\nprint('')\n\nprint('+++++++++++++++++++++++++++')\nprint('Test')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(test_data,index=[\"ZIP\"],values=[\"default\"],aggfunc=[np.mean,len]))","2afac0e7":"#Pivot tables for train and test of default rate by group. \n#'len default' is a bit of a misnomer, it counts how many people exist in the group.\n\nprint('+++++++++++++++++++++++++++')\nprint('Train')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(train_data,index=[\"minority\"],values=[\"default\"],aggfunc=[np.mean,len]))\nprint('')\n\nprint('+++++++++++++++++++++++++++')\nprint('Test')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(test_data,index=[\"minority\"],values=[\"default\"],aggfunc=[np.mean,len]))","1d71cd4b":"#Pivot tables for train and test of default rate by group. \n#'len default' is a bit of a misnomer, it counts how many people exist in the group.\n\nprint('+++++++++++++++++++++++++++')\nprint('Train')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(train_data,index=[\"sex\"],values=[\"default\"],aggfunc=[np.mean,len]))\nprint('')\n\nprint('+++++++++++++++++++++++++++')\nprint('Test')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(test_data,index=[\"sex\"],values=[\"default\"],aggfunc=[np.mean,len]))","26000c1f":"#Pivot tables for train and test of default rate by group. \n#'len default' is a bit of a misnomer, it counts how many people exist in the group.\n\nprint('+++++++++++++++++++++++++++')\nprint('Train')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(train_data,index=[\"ZIP\",\"minority\"],values=[\"default\"],aggfunc=[np.mean,len]))\nprint('')\n\nprint('+++++++++++++++++++++++++++')\nprint('Test')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(test_data,index=[\"ZIP\",\"minority\"],values=[\"default\"],aggfunc=[np.mean,len]))","753aeed5":"#Pivot tables for train and test of default rate by group. \n#'len default' is a bit of a misnomer, it counts how many people exist in the group.\n\nprint('+++++++++++++++++++++++++++')\nprint('Train')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(train_data,index=[\"sex\",\"minority\"],values=[\"default\"],aggfunc=[np.mean,len]))\nprint('')\n\nprint('+++++++++++++++++++++++++++')\nprint('Test')\nprint('+++++++++++++++++++++++++++')\nprint(pd.pivot_table(test_data,index=[\"sex\",\"minority\"],values=[\"default\"],aggfunc=[np.mean,len]))","b2c0f044":"train_data=pd.get_dummies(train_data, columns=[\"ZIP\", \"occupation\"])","42af537e":"#What is the correlation of features in training data?\n\n#produce a correlation matrix\n\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = train_data.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c2e23710":"#What is feature importance in the model?\n\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","c89f1a65":"#Let's produce a correlation matrix\n\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = X_test.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","db900bf5":"#How often did the model predict a default for zip code?\ntest_data_dummies = pd.get_dummies(test_data, columns=[\"ZIP\", \"occupation\"])\n\n#0,1 predictions\nprint('Calculated based on default\/no default')\nprint('ZIP_MT01RA:', test_data_dummies.y_pred[test_data_dummies.ZIP_MT01RA==1].mean())\nprint('ZIP_MT04PA:', test_data_dummies.y_pred[test_data_dummies.ZIP_MT04PA==1].mean())\nprint('ZIP_MT12RA:', test_data_dummies.y_pred[test_data_dummies.ZIP_MT12RA==1].mean())\nprint('ZIP_MT15PA:', test_data_dummies.y_pred[test_data_dummies.ZIP_MT15PA==1].mean())\n\n#prob predictions\nprint()\nprint('calculated based on model probability')\nprint('ZIP_MT01RA:', test_data_dummies.proba_default[test_data_dummies.ZIP_MT01RA==1].mean())\nprint('ZIP_MT04PA:', test_data_dummies.proba_default[test_data_dummies.ZIP_MT04PA==1].mean())\nprint('ZIP_MT12RA:', test_data_dummies.proba_default[test_data_dummies.ZIP_MT12RA==1].mean())\nprint('ZIP_MT15PA:', test_data_dummies.proba_default[test_data_dummies.ZIP_MT15PA==1].mean())\n","5f5cdef9":"print('train')\nprint(pd.pivot_table(train_data,index=[\"rent\"],values=[\"minority\"],aggfunc=[np.mean,len]))\nprint()\nprint('test')\nprint(pd.pivot_table(test_data,index=[\"rent\"],values=[\"minority\"],aggfunc=[np.mean,len]))","06589644":"bins = pd.qcut(train_data['job_stability'], 10) #Cut the data into deciles\nprint(\"Train Data\")\nprint(train_data.groupby(bins)['minority'].mean())\nprint(\"\")\nbins = pd.qcut(test_data['job_stability'], 10) #Cut the data into deciles\nprint(\"Test Data\")\nprint(test_data.groupby(bins)['minority'].mean())","1302bfc9":"print('Job Stability and Occupation_MZ01CD:', test_data_dummies['job_stability'].corr(test_data_dummies['occupation_MZ01CD']))\nprint('Job Stability and Occupation_MZ10CD:', test_data_dummies['job_stability'].corr(test_data_dummies['occupation_MZ10CD']))\nprint('Job Stability and Occupation_MZ11CD:', test_data_dummies['job_stability'].corr(test_data_dummies['occupation_MZ11CD']))\nprint('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\nprint('Default and Occupation_MZ01CD:', test_data_dummies['default'].corr(test_data_dummies['occupation_MZ01CD']))\nprint('Default and Occupation_MZ10CD:', test_data_dummies['default'].corr(test_data_dummies['occupation_MZ10CD']))\nprint('Default and Occupation_MZ11CD:', test_data_dummies['default'].corr(test_data_dummies['occupation_MZ11CD']))\n","e6170763":"**Question 1**\n\nFind the default rate train_data\n\n* I also want to do this in the test_data, just to check","27c0c99e":"**Interpretation**\n>The model treats genders roughly the same\n>The model is defaulting more minorities than non-minorities\n>The model thinks the mean probability of default is about the same for all people","f096e094":"> The model only rejected 3,010 applicants that would have paid.\n> The model approved 16,294 applicants that defaulted. This is 68% of all the defaults.\n> \n> The model is overpredicting repayment.","055798f0":"**Question 11**\n\nHas the loan granting scheme achieved demographic parity? Compare the share of approved female applicants to the share of rejected female applicants. Do the same for minority applicants. Are the shares roughly similar between approved and rejected applicants? What does this indicate about the demographic parity achieved by the model?","beadb718":"**Interpretation**\n> An accuracy of 100% can either indicate overfitting or the use of data that is 100% correlated to actual defaults.\n> Knowing what we know about the test data, the latter is likely.","87545f52":"**Question 12**\n\nIs the loan granting scheme equal opportunity? Compare the share of successful non-minority applicants that defaulted to the share of successful minority applicants that defaulted. Do the same comparison of the share of successful female applicants that default versus successful male applicants that default. What do these shares indicate about the likelihood of default in different population groups that secured loans?","b87dcf53":"**Question 4**\n\nWhat is the correlation of age and income?\n* Do train and test","c803004c":"* The model is equally accurate at accepting paying minority applicants as paying non-minority applicants.\n* The model rejects paying minority applicants at the same rate as it rejects paying non-minority applicants.\n* The model accepts twice as many defaulting non-minorities as defaulting minorities. \n* The model only rejects 1% of defaulting non-minorities, but 8.5% of defaulting minorities\n\n\n* These figures indicate that the default rates for minorities and non-minorities are about equal in the test_data\n* 14.9% of non-minorities vs 15.0% of minorities\n","63ae6027":"> From the analysis above, we can see that there is not demographic parity based on minority status.\n> The acceptance rate of Female (sex=1) is 92.99% and Male (sex=0) is 93.69%. The difference is insignificant (i.e. parity is achieved)\n> The acceptance rate of Minorities (minority = 1) is 89.6%, but the acceptance rate for non-minorities (minority=0) is 97.1%.\n> This difference is likely significant (i.e. parity is not achieved)\n> There are no significant differences of gender within each minority status. ","201803e7":"**Question 5**\n\nBuild a model on the train data and test it in-sample, that is, on the train data itself","5bbb3674":"**Question 10**\n\nIs the loan granting scheme (the cutoff, not the model) group unaware? (This question does not require calculation as the cutoff is given in the introduction to this assignment)\n> \n> Yes, the scheme is unaware.\n> All groups are held to the same standards\n> Maybe the groups aren't equal. So far I haven't seen that, but there is plenty more that could be investigated\n> If groups are unequal in their default rates, maybe it should be aware, but that has trade-offs","96a4f50f":"**Further analysis**\n\nThere are not any defaults in Year 0 of the test data. Is this actually the case?","09b8f922":"+++++++++++++++++++++++++++++++++++\n\n**SUPPORTING ANALYSIS**\n\n+++++++++++++++++++++++++++++++++++\n\nFirst, how does the training data and the test data compare? They should be fairly indistinguishable.","f7045bf5":"No significant differences.","a9bed878":"**Question 7** \n\nUse the model created above, but run on the test data","2f4f7679":"**Question 3**\n\nGet the default rate in the first year of the data (Year 0)\n","5d745fda":"**Question 2**\n\nExamine each zip code's default rate\n* Again, I'll do this for both train and test, just to compare","6dbe9ca0":"**Interpretation**\n> In the train data, two zip codes almost always default and two never default\n> The same is true in the test, but at much lower rates.\n\n> What I expect most models to do is to predict default for anyone in the MT04PA and MT15PA zip codes.\n> There will be errors since only 30% of the time, those will actually default.","d5a7b204":"**Interpretation**\n> Still high, but not 100%.\n> Since the two datasets are not similar, the model is applying what it learned on a dataset that follows different rules.\n\nLooking at the confusion matrix...","12c57990":"**Interpretation**\n> These two datasets have very different default rates. This is an obvious problem because the train and test should have similar characteristics","242f4aca":"How do the Zip codes, Minorities, and Sex compare between datasets?","cb5e0e9b":"The splitting of the training and test data was bad. The two sets look completely different. This is especially true in the default rates of minorities and zip codes. Job stability is completely different between the two datasets and was the feature with the most importance in the model. This means the model is predicting based on a feature with bad data.\n\nNext, I want to examine the confusion matrix of the model.","7794ab24":"**Question 6**\n\nWhat is the Out of bag score?","1da747da":"**Question 8,9**\n\nWhat is the predicted average default probability for all non-minority members in the test set?  Minorities?\n\n","01c80fcd":"**Interpretation**\n\n> The reason there are no defaults in year 0 of the test data is that the test data begins in year 30.\n> This is an indication that maybe the data was split into train and test chronologically.","0c8ab843":"Age, Income, and Job Stability are very different between the two datasets.\n\nCheck the default rates in these features."}}