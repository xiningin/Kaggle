{"cell_type":{"4f70b5cb":"code","8cd83d15":"code","a39eee4f":"code","64309f87":"code","b98f7d82":"code","af8b7945":"code","995a9387":"code","66a9948a":"code","6d4f5f02":"code","5729ba27":"code","ed2bd5ee":"code","f35b548a":"code","52b3621e":"code","2a039f1e":"code","bdd1df78":"code","9e3adcd7":"code","789f920b":"code","74f08853":"code","4b2c3e43":"code","05bb4f5b":"code","dba5f3a2":"code","6fb9903e":"code","2c553ebc":"code","51698a7d":"code","a8e97c1a":"code","977e0c3f":"code","9c6bf9e7":"code","3d13e960":"code","04d97156":"markdown","173339fa":"markdown","44fc1cd2":"markdown","f223b6b0":"markdown","a6ac068f":"markdown","9403c60c":"markdown","4011a906":"markdown","20a858f5":"markdown","9415d953":"markdown","6c4599b3":"markdown","8911304a":"markdown","c3be739f":"markdown","348d122d":"markdown","21d9dc1f":"markdown","3f848416":"markdown","8dd80119":"markdown","8435663e":"markdown","0aeab38f":"markdown","8cf736cd":"markdown","7eccac58":"markdown","c45a2b55":"markdown","3f926858":"markdown","75a568e5":"markdown","81049dc5":"markdown"},"source":{"4f70b5cb":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport datetime as dt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler as ss","8cd83d15":"# Data reading and printing head of the data \ndata = pd.read_excel(\"\/kaggle\/input\/online-retail-data-set-from-uci-ml-repo\/Online Retail.xlsx\")\ndata.head()","a39eee4f":"#Checking the shape of the data set\ndata.shape","64309f87":"#checking the basic information\/details of the data\ndata.info(),data.describe()","b98f7d82":"# Detailing the Country distribution and customerid\ncountry_data = data[['Country','CustomerID']].drop_duplicates()\ncountry_data.groupby(['Country']).agg({'CustomerID' : 'count'}).sort_values('CustomerID',ascending = False).reset_index().rename(columns = {'CustomerID':'CustomerID Count'})","af8b7945":"#Creating a duplicate of the data \ndata1 = data.copy()","995a9387":"#filtering out the data set for UK only\ndata = data[data['Country'] == 'United Kingdom'].reset_index(drop = True)\n#data = data.query(\"Country == 'United Kingdom'\")\ndata.shape","66a9948a":"#checking for null values\ndata.isna().sum()","6d4f5f02":"#Dropping the rows where customerID is missing\ndata = data[pd.notnull(data['CustomerID'])]\n\n#Checking the description of the data\ndata.describe()","5729ba27":"#filtering data for positive quantity values\ndata = data.query(\"Quantity > 0\")\ndata.shape","ed2bd5ee":"#Adding new columns as total amount\ndata['TotalAmount'] = data['UnitPrice']*data['Quantity']","f35b548a":"# For recency will check what was the last date of transaction\n#First will convert the InvoiceDate as date variable\ndata['InvoiceDate']=pd.to_datetime(data['InvoiceDate'])\ndata['InvoiceDate'].max()","52b3621e":"#RFM factors calculation:\nLatest_date = dt.datetime(2011,12,10)\nRFM_data = data.groupby('CustomerID').agg({'InvoiceDate' : lambda x :(Latest_date - x.max()).days,\n                                          'InvoiceNo' : 'count','TotalAmount' : 'sum'}).reset_index()\n\n#converting the names of the columns\nRFM_data.rename(columns = {'InvoiceDate' : 'Recency',\n                          'InvoiceNo' : \"Frequency\",\n                          'TotalAmount' : \"Monetary\"},inplace = True)\nRFM_data.head()","2a039f1e":"# RFM_data Description\/ Summary\nRFM_data.iloc[:,1:4].describe()","bdd1df78":"#Visualizing the Recency, Frequency and Monetary distributions.\ni = 0\nfig = plt.figure(constrained_layout = True,figsize = (20,5))\ngs = GridSpec(1, 3, figure=fig)    \n\ncol = ['red','blue','green']\nfor var in list(RFM_data.columns[1:4]):\n    plt.subplot(gs[0,i])\n    sns.distplot(RFM_data[var],color= col[i])\n    plt.title('Skewness ' + ': ' + round(RFM_data[var].skew(),2).astype(str))\n    i= i+1","9e3adcd7":"#Segmentation :\n#Here, we will divide the data set into 4 parts based on the quantiles.\nquantiles = RFM_data.drop('CustomerID',axis = 1).quantile(q = [0.25,0.5,0.75])\nquantiles.to_dict()","789f920b":"#Creating the R,F and M scoring\/segement function\n#[1] Recency scoring (Negative Impact : Higher the value, less valuable)\ndef R_score(var,p,d):\n    if var <= d[p][0.25]:\n        return 1\n    elif var <= d[p][0.50]:\n        return 2\n    elif var <= d[p][0.75]:\n        return 3\n    else:\n        return 4\n#[2] Frequency and Monetary (Positive Impact : Higher the value, better the customer)\ndef FM_score(var,p,d):\n    if var <= d[p][0.25]:\n        return 4\n    elif var <= d[p][0.50]:\n        return 3\n    elif var <= d[p][0.75]:\n        return 2\n    else:\n        return 1\n\n#Scoring:\nRFM_data['R_score'] = RFM_data['Recency'].apply(R_score,args = ('Recency',quantiles,))\nRFM_data['F_score'] = RFM_data['Frequency'].apply(FM_score,args = ('Frequency',quantiles,))\nRFM_data['M_score'] = RFM_data['Monetary'].apply(FM_score,args = ('Monetary',quantiles,))\nRFM_data.head()","74f08853":"#Now we will create : RFMGroup and RFMScore\nRFM_data['RFM_Group'] = RFM_data['R_score'].astype(str) + RFM_data['F_score'].astype(str) + RFM_data['M_score'].astype(str)\n\n#Score\nRFM_data['RFM_Score'] = RFM_data[['R_score','F_score','M_score']].sum(axis = 1)\nRFM_data.head()","4b2c3e43":"#Creating the Customer segments\/ Loyality_level\nloyalty_level = ['True Lover','Flirting','Potential lover','Platonic Friend']\ncuts = pd.qcut(RFM_data['RFM_Score'],q = 4,labels=loyalty_level)\nRFM_data['RFM_Loyality_level'] = cuts.values\nRFM_data.tail(15)","05bb4f5b":"# Recency V\/s Frequency\nfig = px.scatter(RFM_data,x = \"Recency\", y = \"Frequency\",color = \"RFM_Loyality_level\")\nfig.show()\n\n# Frequency V\/s Monetary\nfig = px.scatter(RFM_data,x = \"Monetary\", y = \"Frequency\",color = \"RFM_Loyality_level\")\nfig.show()\n\n# Monetary V\/s Recency\nfig = px.scatter(RFM_data,x = \"Monetary\", y = \"Recency\",color = \"RFM_Loyality_level\")\nfig.show()","dba5f3a2":"# First will focus on the negativ and zero before the transformation.\ndef right_treat(var):\n    if var <= 0:\n        return 1\n    else:\n        return var\n\n# Describing the data\nRFM_data.describe()","6fb9903e":"#Applying on the data.\nRFM_data['Recency'] = RFM_data['Recency'].apply(lambda x : right_treat(x))\nRFM_data['Monetary'] = RFM_data['Monetary'].apply(lambda x : right_treat(x))\n\n#Checking the Skewness of R, F and M\nprint('Recency Skewness : ' + RFM_data['Recency'].skew().astype(str))\nprint('Frequency Skewness : ' + RFM_data['Frequency'].skew().astype(str))\nprint('Monetary Skewness : ' + RFM_data['Monetary'].skew().astype(str))","2c553ebc":"#Log Transformation\nlog_RFM_data = RFM_data[['Recency','Frequency','Monetary']].apply(np.log,axis = 1).round(4)","51698a7d":"#Plot after transformation for the distributions :\ni = 0\nfig = plt.figure(constrained_layout = True,figsize = (20,5))\ngs = GridSpec(1, 3, figure=fig)    \n\ncol = ['red','blue','green']\nfor var in list(log_RFM_data.columns[0:3]):\n    plt.subplot(gs[0,i])\n    sns.distplot(log_RFM_data[var],color= col[i])\n    plt.title('Skewness ' + ': ' + round(log_RFM_data[var].skew(),2).astype(str))\n    i= i+1\nlog_RFM_data.describe()","a8e97c1a":"#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nScaled_RFM_data = ss.fit_transform(log_RFM_data)\nScaled_RFM_data = pd.DataFrame(Scaled_RFM_data,columns=log_RFM_data.columns,index=log_RFM_data.index)","977e0c3f":"# Will search the optimal number of cluster based on the Elbow Method as below:\nSS_distance = {}\nfor k in range(1,20):\n    mod = KMeans(n_clusters= k, max_iter=1000,init = 'k-means++')\n    mod = mod.fit(Scaled_RFM_data)\n    SS_distance[k] = mod.inertia_\n\n#Plotting the sum of square distance values and numbers of clusters\nplt.figure(figsize = (15,5))\nsns.pointplot(x = list(SS_distance.keys()), y = list(SS_distance.values()))\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Sum of square Distances\")\nplt.title(\"Elbow Techinque to find the optimal cluster size\")","9c6bf9e7":"# Now we will perform K- means clustering on the data set.\nKM_clust = KMeans(n_clusters= 3, init = 'k-means++',max_iter = 1000)\nKM_clust.fit(Scaled_RFM_data)\n\n# Mapping on the data\nRFM_data['Cluster'] = KM_clust.labels_\nRFM_data['Cluster'] = 'Cluster' + RFM_data['Cluster'].astype(str)\nRFM_data.head()","3d13e960":"# Recency V\/s Frequency\nfig = px.scatter(RFM_data,x = 'Recency',y = 'Frequency', color = 'Cluster')\nfig.show()\n\n# Frequency V\/s Monetary\nfig = px.scatter(RFM_data,x = 'Monetary',y = 'Frequency', color = 'Cluster')\nfig.show()\n\n# Recency V\/s Monetary\nfig = px.scatter(RFM_data,x = 'Monetary',y = 'Recency', color = 'Cluster')\nfig.show()\n","04d97156":"# How the K-means algorithm works ?\n---\n<i> Clustering is the process of dividing the entire data into groups (also known as clusters) based on the patterns in the data.<i>   \n    \n<i> To process the learning data, the K-means algorithm in data mining starts with a first group of randomly selected centroids, which are used as the beginning points for every cluster, and then performs iterative (repetitive) calculations to optimize the positions of the centroids.<\/i>\n    \n<img alt=\"3d scatter plot for MS Excel\" class=\"n3VNCb\" src=\"https:\/\/www.doka.ch\/3dscatterplotrotateanim.gif\" data-noaft=\"1\" jsname=\"HiaYvf\" jsaction=\"load:XAeZkd,gvK6lb;\" style=\"width: 378px; height: 378px; margin: 0px;\">\n\n---\n### Data Set Information : Online Retail Data set\n<i> This is a transnational data set which contains all the transactions occurring between 01\/12\/20210 and 09\/12\/2011 for a UK-based and registered non-store online retail.  \n    Data can be downloaded from kaggle <a href = https:\/\/www.kaggle.com\/vijayuv\/onlineretail> Online Retail Data Set - Kaggle <\/a>, and also available on <a href = https:\/\/archive.ics.uci.edu\/ml\/datasets\/online+retail > UCI Machine Learning Repository.<a\/><\/i>   \n\n### Attribute Information:\n\n* InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n* StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n* Description: Product (item) name. Nominal.\n* Quantity: The quantities of each product (item) per transaction. Numeric.\n* InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n* UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n* CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n* Country: Country name. Nominal, the name of the country where each customer resides\n","173339fa":"<a id = \"K_Means\"><a\/><b>\n# K-Means Clustering Technique : \n---\nTo create the customer segementation based on the K-Means Clustering based on the R, F, and M Scores:  \nBefore that we will bring them into same scale and normalise them.","44fc1cd2":"### Visualization for Recency, Frequency and Monetory : RFM_Loyality_level","f223b6b0":"<i> From above distribution plots we can observe that all of these three metrics are positively skewed or right skewed. And also as per skewness coefficient values indicating same.","a6ac068f":"<i> From the above summary report we can see that in Quantity columns the minimum value is -80995.000000, which is not be possible as the quantity value can't be negative. And will filter out all the rows where the Quantity values is positive.","9403c60c":"<i> All the variables are right skewed, so will make log transformation of it.","4011a906":"<i><b> We have classified our customer into four segments based on their R,F and M scores.","20a858f5":"<i> <b>Till Here we have done the data cleaning and preprocessing parts. And data is good to use.","9415d953":"<i> From above we can see that except Frequency , Monetary and Recency having 0 values. will keep take care of it.","6c4599b3":"# Thank You, Readers!\n\nIts great for me that you have read this notebook and would like to know about yours feedback for improvements as i believe sharing of knowledge will make us more powerful. Also it will be a kind help to know yours comment on this and please do upvote if yoy like it, once again thank you so much.\n\n<img alt=\"Emoticon Thank You Stock Illustrations \u2013 132 Emoticon Thank You ...\" class=\"n3VNCb\" src=\"https:\/\/thumbs.dreamstime.com\/b\/cute-blushing-yellow-emoticon-banner-illustration-thanks-quote-thank-you-greeting-card-concept-cute-yellow-d-smiley-face-159876347.jpg\" data-noaft=\"1\" jsname=\"HiaYvf\" jsaction=\"load:XAeZkd;\" style=\"width: 381px; height: 162.877px; margin: 28.0613px 0px;\">","8911304a":"# What is RFM technique ?\n___\n<i> RFM (Recency, Frequency, Monetary) analysis is a proven marketing model for behavior based customer segmentation. It groups customers based on their transaction history \u2013 how recently, how often and how much did they buy.\n\nRFM helps divide customers into various categories or clusters to identify customers who are more likely to respond to promotions and also for future personalization services.<i>\n\n<center><img alt=\"High Value Customer\u201c ..what is it? | by Ryan Seitz | Medium\" class=\"n3VNCb\" src=\"https:\/\/miro.medium.com\/max\/638\/0*JddVeZpHXdElEec_\" data-noaft=\"1\" jsname=\"HiaYvf\" jsaction=\"load:XAeZkd,gvK6lb;\" style=\"width: 800px; height: 400px; margin: 2.73307px 0px;\">","c3be739f":"<i> To evaluate the Monetary values at customer level we wil requires the total amount for each purchses","348d122d":"<a id = \"RFM\"><a\/><b>\n# RFM Modeling Technique :\n---\n<i> <b> Here we will calculate the Recency, Frequency and Monetary for the customers and those are defined as ;   \n* Recency : How much time has elapsed since a customer's last activity or transaction with the brand?   \n* Frequency : How often has a customer transacted or interacted with the brand during a particular period of time?  \n* Monetary : How much a customer has spent with the brand during a particular period of time?","21d9dc1f":"### Visualization for Recency, Frequency and Monetory : Cluster Groups","3f848416":"<i> From above result, we can observe that average recency of the customers are 92 days (approx), an average customer are purchasing the product 90 times and spending an average 1863.91 unitprice.","8dd80119":"<i>So we wil filter out the data\/rows where the CustomerID is missing because those will not make any sence as the cutomer level attributes.","8435663e":"<i> So we can see that mostly customers are from the UK. So we will keep the data for the United Kingdom only and will filter out all others country data.","0aeab38f":"# Libraries :","8cf736cd":"<i> so the last date was 2011-12-09 for transaction. So will take latest date for benchmark as 2011-12-10, for the calculation of the Recency.","7eccac58":"--- \n## To segment the customers will calculate :\n* [RFM Scores for each customers](#RFM)<br>\n* [Create clusters using K-means](#K_Means)<b>","c45a2b55":"<i> Therefore all the variables are now approximately normally distributed. Will make all of them on the same scale as Monetary is little large in values.","3f926858":"<i> RFM Scores have been calculated now we will use this score to make segments of the customers and define level of loyality.","75a568e5":"<i> We can observe that as the number of cluster increases the sum of square distance are becoming lesser. And will take the count of cluster where this elbow is bending. In our cases, sum of square distance is dramatically decreasing at K = 3, so this is optimal value to choose for no of clusters.","81049dc5":"> <i> Clusters have been created based on the values of recency, frequency and monetary with the help of K-Means Clustering."}}