{"cell_type":{"7ad18684":"code","1df8beb3":"code","a391a5ba":"code","68c947ad":"code","20906172":"code","d6a70627":"code","2ad47f30":"code","c4fd7add":"code","7ae767fc":"code","87a0829c":"code","0cbb3eb1":"code","b0f4703a":"code","08e8551b":"code","f92e61be":"code","bb1698de":"code","a7415299":"code","5403e68a":"code","19aebedc":"code","1d94597e":"code","3a47073a":"code","8dfb0273":"code","9a033ca4":"code","16c6cacf":"code","3337d82a":"code","ef51630f":"code","e25169f8":"code","967e51d2":"code","6aaee2a1":"code","74fcbaa0":"code","f7b65c71":"code","e0dc426f":"code","ba7db67b":"code","25a269f9":"code","ac587541":"code","85c24afe":"code","1085586f":"code","01697a82":"code","4fd36253":"code","b86a9760":"code","a00d2843":"code","b00fb24e":"code","1fc7dd80":"code","09f73953":"code","4cc6e3ef":"code","020abcc0":"code","b92c6e12":"code","d4d87e99":"code","380a20bd":"code","2e43ffa7":"code","77ea3405":"code","eed2ebe5":"code","77145013":"code","ab4dbe9c":"code","fed8aaa1":"code","212b71ca":"code","68307df9":"code","487a340f":"code","8610113e":"code","72f9508d":"code","f392972f":"code","01cfe727":"code","91e7ae84":"code","c5b869b7":"code","c0cf2d12":"code","a5ef5c94":"markdown","6f004d25":"markdown","941745a6":"markdown","81b0d35b":"markdown","0e1da3c0":"markdown","b72508e1":"markdown","ce37c31c":"markdown","d15a19ea":"markdown","f80895e3":"markdown","18e43f58":"markdown","7273ba2d":"markdown","2f07d5d8":"markdown","68c89cd4":"markdown","1ba5b6ae":"markdown","7cdbb23a":"markdown","7dbb988c":"markdown","7ac8f818":"markdown","535df23e":"markdown","d28735a8":"markdown","0f33dccb":"markdown","7248c092":"markdown","bb2bde4b":"markdown","ccb92a27":"markdown","0885787c":"markdown","646c1270":"markdown","b7e84d5e":"markdown","6286c151":"markdown","b5c62501":"markdown","b17c1bfc":"markdown"},"source":{"7ad18684":"# Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats import norm\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style=\"darkgrid\", color_codes=True,)\nsns.set(font_scale=1)\n\n#Data preprocessing libraries\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\n\n#linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n#AND suppressing warning; they tend to ruin an easy overview\nimport warnings\nwarnings.filterwarnings(\"ignore\")","1df8beb3":"# read train and test dataset\ntrain_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest_target = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","a391a5ba":"# dropping ID column from dataframe\ntrain_df.head()","68c947ad":"test_df.head()","20906172":"# Shape of training data\n\nprint(\"Training data set shape => (Rows, Columns)= \",train_df.shape)\nprint(\"Testing data set shape => (Rows, Columns)= \",test_df.shape)","d6a70627":"def check_features_count():\n    train_cols = train_df.columns\n    test_cols = test_df.columns\n    train_num_cols = train_df._get_numeric_data().columns\n    test_num_cols = test_df._get_numeric_data().columns\n    print(\"Train numerical columns \\n\",len(train_num_cols))\n    print(\"Test numerical columns \\n\",len(test_num_cols))\n\n    train_cat_cols = list(set(train_cols) - set(train_num_cols))\n    test_cat_cols = list(set(test_cols) - set(test_num_cols))\n\n    print('\\n')\n    print(\"Train catagorical columns \\n\",len(train_cat_cols))\n    print(\"Test catagorical columns \\n\",len(test_cat_cols))\n\ndef check_features():\n    train_cols = train_df.columns\n    test_cols = test_df.columns\n    train_num_cols = train_df._get_numeric_data().columns\n    test_num_cols = test_df._get_numeric_data().columns\n\n    train_cat_cols = list(set(train_cols) - set(train_num_cols))\n    test_cat_cols = list(set(test_cols) - set(test_num_cols))\n    \n    return [train_num_cols,test_num_cols,train_cat_cols,test_cat_cols]\n\ncheck_features_count()","2ad47f30":"  \nstat_features_columns=check_features()\ntrain_num_cols = stat_features_columns[0]\ntest_num_cols = stat_features_columns[1]\ntrain_cat_cols = stat_features_columns[2]\ntest_cat_cols = stat_features_columns[3]\n\nprint(\"train_num_cols\",train_num_cols)\nprint(\"\\n test_num_cols\",test_num_cols)\nprint(\"\\n train_cat_cols\",train_cat_cols)\nprint(\"\\n test_cat_cols\",test_cat_cols)","c4fd7add":"unique_count_list=[]\nfor col_name in train_cat_cols:\n    temp = (col_name, train_df[col_name].nunique())\n    unique_count_list.append(temp)\ncat_unique_value_df = pd.DataFrame(data=unique_count_list,columns = ['cat_feature_column_names','unique_value_count'])\n\ncat_unique_value_df.sort_values('unique_value_count',ascending=False)","7ae767fc":"# Total Feature column count\n\ndef features_with_dummy():\n    cat_feature_column_count = cat_unique_value_df['unique_value_count'].sum()\n\n    total_feature_column_count = len(train_num_cols)+cat_feature_column_count\n    \n    return total_feature_column_count\n\nprint(\"Total Feature column count before Feature Engineering = \",features_with_dummy())","87a0829c":"train_test_df = pd.concat([train_df,test_df])\ntrain_test_df","0cbb3eb1":"# Missing data analysis\ndef missing_data_analysis(dataset,title,col):\n    list_missing_data = []\n\n    for column_name in dataset:\n        temp = (column_name,dataset[column_name].isnull().sum(),dataset[column_name].isnull().sum()\/dataset.shape[0])\n        list_missing_data.append(temp)\n\n    missing_data_df = pd.DataFrame(data = list_missing_data, columns = [\"feature_columns\",col,\"percent\"])\n    missing_data_df = missing_data_df[missing_data_df[col] > 0].sort_values(col,ascending=False)\n    return missing_data_df\n    \ntrain_test_missing_df = missing_data_analysis(train_test_df,'dataset feature column missing data analysis','missing_count')    \n\ntrain_test_missing_df","b0f4703a":"# Missing Data chart analysis\n\nf, ax = plt.subplots(figsize=(15, 3))\nplt.xticks(rotation='90')\nsns.barplot(x=train_test_missing_df['feature_columns'], y=train_test_missing_df['percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Missing data chart for dataset',color='b',size=20)\nplt.show()\n    \n#missing_data_chart(data=train_missing_df,ax=1,suptitle=\"Train missing data chart\")\n#missing_data_chart(data=test_missing_df,ax=2,suptitle=\"Test missing data chart\")","08e8551b":"print(\"missing column count in data set: \",len(train_test_missing_df))\nprint()\n\nlist_missing_columns=list(train_test_missing_df['feature_columns'])\nset_mis=set(list_missing_columns)\nlist_missing_columns=list(set_mis)\nprint(\"test and train missing columns: \\n\",list_missing_columns)","f92e61be":"None_to_columns=['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond', 'GarageFinish', 'GarageQual', 'GarageType', 'BsmtCond', 'BsmtQual', 'BsmtExposure', 'BsmtFinType2', 'BsmtFinType1', 'MasVnrType']\nzero_to_columns=['BsmtFullBath','BsmtHalfBath','GarageCars','BsmtUnfSF','TotalBsmtSF','BsmtFinSF2','BsmtFinSF1','SalePrice']\nmedian_to_columns=['LotFrontage','MasVnrArea','GarageArea']\nmode_to_columns=['MSZoning','Exterior1st','Exterior2nd','SaleType','Utilities','GarageYrBlt']\n# GarageYrBlt is ordinal numerical value, hence it will changed to catagorical feature. \n\ndef fillna(feature_columns,dataset,value):\n    for col in feature_columns:\n        dataset[col]=dataset[col].fillna(value)\n        \ndef fillna_med(feature_columns,dataset):\n    for col in feature_columns:\n        dataset[col]=dataset[col].fillna(dataset[col].median())\n        \ndef fillna_mode(feature_columns,dataset):\n    for col in feature_columns:\n        dataset[col]=dataset[col].fillna(dataset.loc[:,col].mode()[0])     \n        \n        \nfillna(feature_columns=None_to_columns,dataset=train_test_df,value=\"None\")\n\nfillna(feature_columns=zero_to_columns,dataset=train_test_df,value=0)\n\nfillna_med(feature_columns=median_to_columns,dataset=train_test_df)\n\nfillna_mode(feature_columns=mode_to_columns,dataset=train_test_df)\n\n\ntrain_test_df['Functional']=train_test_df['Functional'].fillna('Typ')\ntrain_test_df['KitchenQual']=train_test_df['KitchenQual'].fillna('TA')\ntrain_test_df['Electrical']=train_test_df['Electrical'].fillna('SBrkr')\n","bb1698de":"train_df = train_test_df[train_test_df['Id']<1461]\ntest_df = train_test_df[train_test_df['Id']>1460]","a7415299":"missing_data_analysis(train_df,'dataset feature column missing data analysis','missing_count')","5403e68a":"Ordinal_train_df = train_df[['YearBuilt','YearRemodAdd','GarageYrBlt']].copy()\nOrdinal_train_df['que1'] = Ordinal_train_df['YearBuilt'][(Ordinal_train_df['YearBuilt'] >= Ordinal_train_df['YearRemodAdd']) & (Ordinal_train_df['YearBuilt'] <= Ordinal_train_df['GarageYrBlt'])]\nOrdinal_train_df['que2'] = Ordinal_train_df['YearBuilt'][(Ordinal_train_df['YearBuilt'] >= Ordinal_train_df['YearRemodAdd'])]\nOrdinal_train_df['que3'] = Ordinal_train_df['YearBuilt'][(Ordinal_train_df['YearBuilt'] <= Ordinal_train_df['GarageYrBlt'])]\nOrdinal_train_df['que4'] = Ordinal_train_df['YearRemodAdd'][(Ordinal_train_df['YearRemodAdd'] <= Ordinal_train_df['GarageYrBlt'])]\nOrdinal_train_df.isnull().sum()","19aebedc":"train_df.drop(['GarageYrBlt'], axis=1,inplace=True)\ntest_df.drop(['GarageYrBlt'], axis=1,inplace=True)","1d94597e":"convert_dict = {'MSSubClass': str, \n                'YearBuilt': str,\n                'YearRemodAdd': str, \n                'MoSold': str,\n                'YrSold': str\n               } \n  \ntrain_df = train_df.astype(convert_dict)\ntest_df = test_df.astype(convert_dict)","3a47073a":"stat_features_columns=check_features()\ntrain_num_cols = stat_features_columns[0]\ntest_num_cols = stat_features_columns[1]\ntrain_cat_cols = stat_features_columns[2]\ntest_cat_cols = stat_features_columns[3]\n\nprint(\"train_num_cols\",train_num_cols)\nprint(\"\\n test_num_cols\",test_num_cols)\nprint(\"\\n train_cat_cols\",train_cat_cols)\nprint(\"\\n test_cat_cols\",test_cat_cols)","8dfb0273":"i = 1\nplt.figure(figsize = (30,70))\n\nfor col in train_cat_cols:\n    plt.subplot(13,4,i)\n    sns.boxplot(x = train_df[col],y=train_df['SalePrice'])\n    i=i+1\n    plt.xticks(rotation='45',size=8)\n    plt.yscale('log')\n\nplt.show()","9a033ca4":"train_df.drop(['Utilities','Condition2','RoofMatl','YrSold','MoSold'],axis=1,inplace=True)\ntest_df.drop(['Utilities','Condition2','RoofMatl','YrSold','MoSold'],axis=1,inplace=True)\n\ncheck_features_count()","16c6cacf":"# pearson Correlated matrix\nf, ax = plt.subplots(figsize=(18, 15))\n\ncorr_matrix = train_df.corr(method ='pearson')\n\nsns.heatmap(corr_matrix,annot=True,annot_kws={'size': 7},vmax=.9,fmt=\".1g\",cmap='coolwarm',ax=ax,)\nplt.title(\"Correlated Matrix of quantitative inputs\", size = 20,color = 'g')\n","3337d82a":"train_df.drop(['GarageCars'],axis=1,inplace=True)\ntest_df.drop(['GarageCars'],axis=1,inplace=True)","ef51630f":"stat_features_columns=check_features()\ntrain_num_cols = stat_features_columns[0]\ntest_num_cols = stat_features_columns[1]\ntrain_cat_cols = stat_features_columns[2]\ntest_cat_cols = stat_features_columns[3]\n","e25169f8":"i = 1\nplt.figure(figsize = (15,50))\n\nfor col in train_num_cols:\n    plt.subplot(10,4,i)\n    sns.regplot(x = train_df[col],y=train_df['SalePrice'],color=\".1\")\n    i=i+1\n    stp = stats.pearsonr(train_df[col], train_df['SalePrice'])\n    str_title = \"r = \" + \"{0:.3f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.3f}\".format(stp[1])\n    plt.title(str_title,fontsize=11)\n    plt.xticks(rotation='45',size=6)\n\n    plt.tight_layout() \nplt.show()\n","967e51d2":"stat_features_columns=check_features()\ntrain_num_cols = stat_features_columns[0]\ntest_num_cols = stat_features_columns[1]\ntrain_cat_cols = stat_features_columns[2]\ntest_cat_cols = stat_features_columns[3]\n\nprint(\"train_num_cols\",train_num_cols)\nprint(\"\\n test_num_cols\",test_num_cols)\nprint(\"\\n train_cat_cols\",train_cat_cols)\nprint(\"\\n test_cat_cols\",test_cat_cols)","6aaee2a1":"collist=[]\nlists=[]\nfor col in train_num_cols: \n    temp = train_df[col].astype(int)\n    r,p = stats.pearsonr(temp, train_df['SalePrice'])\n    if int(round(p, 2)*100)>5:\n        lists.append((col,round(p, 2), int(round(p, 2)*100)))\n        collist.append(col)\ndf=pd.DataFrame(lists,columns=[col,'p value','p %']).sort_values('p %',ascending=False)\ndf","74fcbaa0":"corr_matrix_with_SalePrice = train_df.corr()['SalePrice']\n\nplt.figure(figsize=(10,8))\ncorr_matrix_with_SalePrice.sort_values(axis=0,ascending=True).plot(kind='barh')\nplt.xlabel('Correlation against SalePrice',color='b')","f7b65c71":"corr_matrix_with_SalePrice=corr_matrix_with_SalePrice.where(corr_matrix_with_SalePrice>0.50).dropna()\ncorr_matrix_with_SalePrice=corr_matrix_with_SalePrice.sort_values(ascending=True)\ncorr_matrix_with_SalePrice","e0dc426f":"high_corr_df = corr_matrix_with_SalePrice.to_frame()\nhigh_corr_features = high_corr_df.index\nprint(high_corr_features)","ba7db67b":"stat_features_columns=check_features()\ntrain_num_cols = stat_features_columns[0]\ntest_num_cols = stat_features_columns[1]\ntrain_cat_cols = stat_features_columns[2]\ntest_cat_cols = stat_features_columns[3]\n\nprint(\"train_num_cols\",train_num_cols)\nprint(\"\\n test_num_cols\",test_num_cols)\nprint(\"\\n train_cat_cols\",train_cat_cols)\nprint(\"\\n test_cat_cols\",test_cat_cols)","25a269f9":"for col in collist:\n    if col == 'Id':\n        pass\n        # Id will be dropped before modelling\n    else:\n        train_df.drop([col],axis=1,inplace=True)\n        test_df.drop([col],axis=1,inplace=True)","ac587541":"high_corr_features","85c24afe":"sns.pairplot(train_df[high_corr_features],size = 2.5)\nplt.show()","1085586f":"def feature_engg(df):\n    \n    df['total_area'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'] + df['GrLivArea']\n    df['porch_area'] = df['WoodDeckSF'] + df['OpenPorchSF'] + df['EnclosedPorch'] + df['ScreenPorch']\n    df['total_bathroom'] = df['BsmtFullBath'] + df['FullBath'] + (df['HalfBath']*0.5)\n    \n    \n    df['has_pool'] = df['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['has_2ndfloor'] = df['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['has_garage'] = df['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    df['has_bsmt'] = df['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    df['has_fireplace'] = df['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n    \n    print(\"Before dropping\",df.shape)\n    drop_list = ['TotalBsmtSF','1stFlrSF','2ndFlrSF','GrLivArea',\\\n                 'WoodDeckSF','OpenPorchSF','EnclosedPorch','ScreenPorch',\\\n                 'BsmtFullBath','FullBath','HalfBath',\\\n                 'PoolArea','2ndFlrSF','GarageArea','TotalBsmtSF','Fireplaces']\n    df=df.drop(drop_list,axis=1)\n    print(\"After dropping\",df.shape)\n    \n    return df\n\ntrain_df = feature_engg(train_df)\ntest_df = feature_engg(test_df)\n","01697a82":"# Distribution of sale prices\n\nf, ax = plt.subplots(2,2,figsize=(12, 8))\n\nsns.distplot(train_df['SalePrice'],fit=norm, color='b',ax=ax[0][0])\nsns.distplot(np.log1p(train_df['SalePrice']),color='b',ax=ax[0][1])\nstats.probplot(train_df['SalePrice'],plot=ax[1][0])\nstats.probplot(np.log1p(train_df['SalePrice']),plot=ax[1][1])\nplt.xlabel('SalePrice')\nplt.ylabel('Number of occurances')\nplt.tight_layout()","4fd36253":"train_df['SalePrice']= np.log1p(train_df[\"SalePrice\"])\n","b86a9760":"#importing the necessary libraries\n# from sklearn.preprocessing import LabelEncoder\n\n# #getting categorical variables\n# cat_feat = list(train_df.dtypes[train_df.dtypes == 'object'].index)\n\n# #Encoding the categorical variables\n# for c in cat_feat:\n#     lbl = LabelEncoder() \n#     lbl.fit(list(train_df[c].values)) \n#     train_df[c] = lbl.transform(list(train_df[c].values))\n# print(train_df.shape)\n# train_df.describe()","a00d2843":"#Concat and splitting so we get match columns after get_dummies\ntrain_test_df = pd.concat([train_df,test_df])\ntrain_test_df","b00fb24e":"# Get dummy variables to all catagorical columns\nprint(\"shape of train_test_df before cat to dummy variable\",train_test_df.shape)\n\ntrain_test_df = pd.get_dummies(train_test_df,drop_first=True)\n\nprint(\"shape of train_test_df before cat to dummy variable\",train_test_df.shape)\n\ntrain_test_df.head()","1fc7dd80":"train_df = train_test_df[train_test_df['Id']<1461]\ntest_df = train_test_df[train_test_df['Id']>1460]","09f73953":"X = train_test_df[train_test_df['Id']<1461]\ny = X[\"SalePrice\"]\nX = X.drop([\"SalePrice\",'Id'],axis=1)","4cc6e3ef":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n","020abcc0":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","b92c6e12":"# def feature_scale(df):\n#     sc = StandardScaler()\n#     return sc.fit_transform(df)\n# X_train[X_train.columns] = feature_scale(X_train[X_train.columns])\n","d4d87e99":"# #importing minmax scaler from sklearn.preprocessing and scaling the training dataframe\n# from sklearn.preprocessing import MinMaxScaler\n# def feature_scale_minmax(df):\n#     sc_minmax = MinMaxScaler()\n#     return sc_minmax.fit_transform(df)\n# X_train[X_train.columns] = feature_scale_minmax(X_train[X_train.columns])\n# X_train.describe()","380a20bd":"# X_test[X_test.columns] = feature_scale_minmax(X_test[X_test.columns])\n# X_test.describe()","2e43ffa7":"sk_linear_reg = LinearRegression()\n\nsk_linear_reg.fit(X = X_train,y=y_train)","77ea3405":"y_pred_sk_Linear = sk_linear_reg.predict(X_test)\nprint(len(y_pred_sk_Linear))","eed2ebe5":"for y in y_pred_sk_Linear:\n    print(np.expm1(y))","77145013":"print(\"Scikit learn linear regression - Goodness score - R**2 value = \", sk_linear_reg.score(X_train,y_train))","ab4dbe9c":"test_pred_table_linear={\"y_test\":list(np.expm1(y_test)),\"y_pred\":list(np.expm1(y_pred_sk_Linear))}\n\ntest_pred_table_linear = pd.DataFrame.from_dict(test_pred_table_linear)\ntest_pred_table_linear['difference']=test_pred_table_linear['y_test']-test_pred_table_linear['y_pred']\ntest_pred_table_linear['error_percent']=abs(test_pred_table_linear['difference']\/test_pred_table_linear['y_test'])*100\n\nprint(\"Error percentage mean from test data\",test_pred_table_linear['error_percent'].mean())\ntest_pred_table_linear.describe()\n","fed8aaa1":"test_pred_table_linear.sort_values('error_percent')","212b71ca":"sk_tree = tree.DecisionTreeRegressor()\n\nsk_tree.fit(X_train,y_train)\n\nprint(\"Scikit learn decision tree - Goodness score - R**2 value = \", sk_tree.score(X_train,y_train))","68307df9":"y_pred_sk_tree = sk_tree.predict(X_test)\nprint(len(y_pred_sk_tree))","487a340f":"test_pred_table_tree={\"y_test\":list(np.expm1(y_test)),\"y_pred\":list(np.expm1(y_pred_sk_tree))}\n\ntest_pred_table_tree = pd.DataFrame.from_dict(test_pred_table_tree)\ntest_pred_table_tree['difference']=test_pred_table_tree['y_test']-test_pred_table_tree['y_pred']\ntest_pred_table_tree['error_percent']=abs(test_pred_table_tree['difference']\/test_pred_table_tree['y_test'])*100\n\nprint(\"Error percentage mean from test data\",test_pred_table_tree['error_percent'].mean())\n","8610113e":"test_pred_table_tree.describe()","72f9508d":"test_pred_table_tree.sort_values('error_percent')","f392972f":"sk_knn = tree.DecisionTreeRegressor()\n\nsk_knn.fit(X_train,y_train)\n\nprint(\"Scikit learn decision tree - Goodness score - R**2 value = \", sk_knn.score(X_train,y_train))","01cfe727":"y_pred_sk_knn = sk_knn.predict(X_test)\nprint(len(y_pred_sk_knn))","91e7ae84":"test_pred_table_knn={\"y_test\":list(np.expm1(y_test)),\"y_pred\":list(np.expm1(y_pred_sk_knn))}\n\ntest_pred_table_knn = pd.DataFrame.from_dict(test_pred_table_knn)\ntest_pred_table_knn['difference']=test_pred_table_knn['y_test']-test_pred_table_knn['y_pred']\ntest_pred_table_knn['error_percent']=abs(test_pred_table_knn['difference']\/test_pred_table_knn['y_test'])*100\n\nprint(\"Error percentage mean from test data\",test_pred_table_knn['error_percent'].mean())\n","c5b869b7":"test_pred_table_knn.describe()","c0cf2d12":"test_pred_table_knn.sort_values('error_percent')","a5ef5c94":"## Correlation matrix ","6f004d25":"## Outliers","941745a6":"### Handling the skewness\n> Normal distribution target will have good result\n","81b0d35b":"# 2. Missing Data Handling","0e1da3c0":"## Distribution of sale prices","b72508e1":"## >>>>>>>>>>>>>>>>>>>>>>>> Exploratory Data Analysis completed <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<","ce37c31c":"> fig 0,0 diagram shows the salary distribution is positive skewness\n\n> fig 0,1 diagram shows the logarithmic of sales in normal distribution\n\n> fig 1,1 diagram shows the sales in probability chart highly deviates from the line\n\n> fig 1,1 diagram shows the logarithmic of sales are fit to nor","d15a19ea":"## Detailed analysis of missing values in Train and Test Dataset\n\n![image.png](attachment:image.png)","f80895e3":"# 5.2 Scikit Decision tree algorithim","18e43f58":"## >>>>>>>>>>>>>>>>>>>>>>>> Feature engineering completed <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<","7273ba2d":"* Only 11 houses are not built the garage the same year the house built. so we can elimate the GarageYrBlt","2f07d5d8":"## Feature Extraction","68c89cd4":"# 5. Modelling","1ba5b6ae":"## Feature scalling","7cdbb23a":"# 4. Model Data Preparation","7dbb988c":"\n# 3. Feature Engineering","7ac8f818":"As per test data description **MSSubClass** is \"The building class\", hence this is supposed to be Catagorical input. so this should be changed to str type.\n\n**MSSubClass**,**'YearBuilt'**, **YearRemodAdd'**,**'MoSold'**, **'YrSold'** are catagorical input column feature but they are ordinal.","535df23e":"# 5.3 K Nearest Neighbors(KNN)","d28735a8":"### Numerical column features relationship with Saleprice","0f33dccb":"Instead of using Label and One Hot encoder, pd.get_dummies will do the same job","7248c092":"# 5.1 Scikit Linear regression","bb2bde4b":"# 1. Exploratory Data Analysis","ccb92a27":"#### Checking numerical and catagorical input features in train and test datasets","0885787c":"#### as per above pair plot, outliers are\n\n* 1stFlrSF above 3000 \n* GrLivArea above 4000 \n* SalePrice above 400000 \n\nRemoving the above outliers","646c1270":"## >>>>>>>>>>>>>>>>>>>>>>>> Data cleaning completed  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<","b7e84d5e":"GarageArea and GarageCars are 90% correlelated, hence we can remove GarageCars from feature columns","6286c151":"**From the above figures:**\n\nBelow columns have less than 0.05 P-value and less correlation against SalePrice (target variable). those can be dropped due to strong null hypotheisis or these feature are not significant to target output or create noise to the data set.\n\n* BsmtFinSF2 \n* BsmtHalfBath\n* MiscVal\n* Id\n* LowQualFinSF\n* 3SsnPorch\n\n","b5c62501":"########################### Data preparation completed #################################","b17c1bfc":"From the above chart following data are inbalanced or no effect to Saleprice (Target variable) so the feature will create noise to the target variable, hence dropping those feature columns\n* Utilities\n* Condition2\n* RoofMatl\n* YrSold\n* MoSold\n"}}