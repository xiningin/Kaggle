{"cell_type":{"184f9fb5":"code","600156e7":"code","1a634d6b":"code","e068cac4":"code","04a4ccfc":"code","78cfc644":"markdown","4944c075":"markdown","65ee3a63":"markdown","cce5cdf0":"markdown"},"source":{"184f9fb5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","600156e7":"import tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom keras import backend as K\n\ndef conv_block(x, num_filters):\n\n    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    x = Conv2D(num_filters, (3, 3), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n\n    return x\n\ndef build_model():\n    size = 256\n    num_filters = [16, 32, 48, 64]\n    inputs = Input((size, size, 3))\n\n    skip_x = []\n    x = inputs\n    ## Encoder\n    for f in num_filters:\n        x = conv_block(x, f)\n        skip_x.append(x)\n        x = MaxPool2D((2, 2))(x)\n\n    ## Bridge\n    x = conv_block(x, num_filters[-1])\n\n    num_filters.reverse()\n    skip_x.reverse()\n    ## Decoder\n    for i, f in enumerate(num_filters):\n        x = UpSampling2D((2, 2))(x)\n        xs = skip_x[i]\n        x = Concatenate()([x, xs])\n        x = conv_block(x, f)\n\n    ## Output\n    x = Conv2D(1, (1, 1), padding=\"same\")(x)\n    x = Activation(\"sigmoid\")(x)\n\n    return Model(inputs, x)\n\n","1a634d6b":"import os\nimport numpy as np\nimport cv2\nfrom glob import glob\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\n# Load Dataset and split with Random seed\n\ndef load_data(path, split=0.1):\n    images = sorted(glob(os.path.join(path, \"images\/*\")))\n    masks = sorted(glob(os.path.join(path, \"masks\/*\")))\n\n#     images = sorted(glob(os.path.join(path, \"train\/images\/*\")))\n#     masks = sorted(glob(os.path.join(path, \"train\/masks\/*\")))\n\n\n    total_size = len(images)\n    print(total_size)\n    valid_size = int(split * total_size)\n    test_size = int(split * total_size)\n\n\n    train_x, valid_x = train_test_split(images, test_size=valid_size, random_state=42)\n    train_y, valid_y = train_test_split(masks, test_size=valid_size, random_state=42)\n   \n\n    train_x, test_x = train_test_split(train_x, test_size=test_size, random_state=42)\n    train_y, test_y = train_test_split(train_y, test_size=test_size, random_state=42)\n    print(len(train_x),len(valid_x),len(test_x))\n\n    return (train_x, train_y), (valid_x, valid_y), (test_x,test_y)\n\n# Read and Resize Image to 256(coloured)\ndef read_image(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (256, 256))\n    x = x\/255.0\n    return x\n\n# Read and Resize Mask to 256(Grayscaled)\ndef read_mask(path):\n    path = path.decode()\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (256, 256))\n    x = x\/255.0\n    x = np.expand_dims(x, axis=-1)\n    return x\n\n\n# Tensor element type for model(can change to 32bit for speed)\ndef tf_parse(x, y):\n    def _parse(x, y):\n        x = read_image(x)\n        y = read_mask(y)\n        return x, y\n\n    x, y = tf.numpy_function(_parse, [x, y], [tf.float64, tf.float64])\n    x.set_shape([256, 256, 3])\n    y.set_shape([256, 256, 1])\n    return x, y\n# creates a dataset with a separate element for each row of the input tensor\n# we will then use batch and repeat method to convert Dataset into batches\ndef tf_dataset(x, y, batch=8):\n    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n    dataset = dataset.map(tf_parse)\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    return dataset\n\n# def matthews_correlation_coefficient(y_true, y_pred):\n#     tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n#     tn = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n#     fp = K.sum(K.round(K.clip((1 - y_true) * y_pred, 0, 1)))\n#     fn = K.sum(K.round(K.clip(y_true * (1 - y_pred), 0, 1)))\n\n#     num = tp * tn - fp * fn\n#     den = (tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)\n#     return num \/ K.sqrt(den + K.epsilon())\n# matthews_correlation_coefficient","e068cac4":"import os\n\nimport numpy as np\nimport cv2\nfrom glob import glob\nimport tensorflow as tf\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard\n# from data import load_data, tf_dataset\n# from model import build_model\n\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) \/ (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\nif __name__ == \"__main__\":\n    ## Dataset\n    path = \"..\/input\/kvasirv2\/Kvasirv2\"\n    (train_x, train_y), (valid_x, valid_y), (test_x,test_y) = load_data(path)\n    \n    ## Hyperparameters\n    batch = 8\n    lr = 1e-4\n    epochs = 200\n    # np.reshape(train_x,(50,8))\n    train_dataset = tf_dataset(train_x, train_y, batch=batch)\n    valid_dataset = tf_dataset(valid_x, valid_y, batch=batch)\n    image_size = 256\n#     obj=ResUnetPlusPlus(input_size=image_size)\n    model = build_model()\n\n    opt = tf.keras.optimizers.Adam(lr)\n    metrics = [\"acc\", tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), iou]\n    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=metrics)\n\n    callbacks = [\n        ModelCheckpoint(\"files\/model.h5\"),\n        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4),\n        CSVLogger(\"files\/data.csv\"),\n        TensorBoard(),\n        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=False)\n    ]\n    try:\n        path=os.path.join(mypath, n)\n        img=cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    \n        img=cv2.resize(img, (img_rows, img_cols))\n    except Exception as e:\n        \n        \n        \n\n      \n        train_steps = len(train_x)\/\/batch\n        valid_steps = len(valid_x)\/\/batch\n\n    if len(train_x) % batch != 0:\n        train_steps += 1\n    if len(valid_x) % batch != 0:\n        valid_steps += 1\n\n    model.fit(train_dataset,validation_data=valid_dataset,epochs=epochs,steps_per_epoch=train_steps,validation_steps=valid_steps,callbacks=callbacks)","04a4ccfc":"import os\nimport numpy as np\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.utils import CustomObjectScope\nfrom tqdm import tqdm\n# from data import load_data, tf_dataset\n# from train import iou\n# create data generator\ndef iou(y_true, y_pred):\n    def f(y_true, y_pred):\n        intersection = (y_true * y_pred).sum()\n        union = y_true.sum() + y_pred.sum() - intersection\n        x = (intersection + 1e-15) \/ (union + 1e-15)\n        x = x.astype(np.float32)\n        return x\n    return tf.numpy_function(f, [y_true, y_pred], tf.float32)\n\ndef read_image(path):\n    x = cv2.imread(path, cv2.IMREAD_COLOR)\n    x = cv2.resize(x, (256, 256))\n    x = x\/255.0\n    return x\n\ndef read_mask(path):\n    x = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n    x = cv2.resize(x, (256, 256))\n    x = np.expand_dims(x, axis=-1)\n    return x\n\ndef mask_parse(mask):\n    mask = np.squeeze(mask)\n    mask = [mask, mask, mask]\n    mask = np.transpose(mask, (1, 2, 0))\n    return mask\n\nif __name__ == \"__main__\":\n    ## Dataset\n    path = \"..\/input\/kvasirv2\/Kvasirv2\"\n    batch_size = 8\n   \n    (train_x, train_y), (valid_x, valid_y), (test_x, test_y) = load_data(path)\n    \n\n    test_dataset = tf_dataset(test_x, test_y, batch=batch_size)\n\n    test_steps = (len(test_x)\/\/batch_size)\n    if len(test_x) % batch_size != 0:\n        test_steps += 1\n\n    with CustomObjectScope({'iou': iou}):\n        model = tf.keras.models.load_model(\"files\/model.h5\")\n\n#     model.evaluate(test_dataset, steps=test_steps)\n\n    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):\n        x = read_image(x)\n        y = read_mask(y)\n        y_pred = model.predict(np.expand_dims(x, axis=0))[0] > 0.5\n        h, w, _ = x.shape\n        white_line = np.ones((h, 10, 3)) * 255.0\n\n        all_images = [\n            x * 255.0, white_line,\n            mask_parse(y), white_line,\n            mask_parse(y_pred) * 255.0\n        ]\n        image = np.concatenate(all_images, axis=1)\n        cv2.imwrite(f\"files\/{i}.png\", image)","78cfc644":"Load data","4944c075":"Testing","65ee3a63":"Train","cce5cdf0":"Unet Code"}}