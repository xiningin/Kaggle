{"cell_type":{"cd4ea4e6":"code","d107e9fb":"code","0a6abba2":"code","0622921e":"code","cbb9d459":"code","308033be":"code","d9516844":"code","ebb62cce":"code","2264743b":"code","8588fe32":"code","31caf6ce":"code","41842c66":"code","5fa82ee0":"code","b7de6bab":"code","68ce3b8a":"code","d5a7d5da":"code","c6561a4c":"code","5e5455bc":"markdown","0023a92f":"markdown","161cdb48":"markdown","578351bb":"markdown","b886ad30":"markdown","1f02dd06":"markdown","f28a4b2b":"markdown","cabeb239":"markdown","855a3d6d":"markdown","ac2c4f33":"markdown","04aa3c80":"markdown","75a95d94":"markdown"},"source":{"cd4ea4e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d107e9fb":"dataset=pd.read_csv(\"..\/input\/data.csv\")","0a6abba2":"dataset.head() #lets take a look at our data structure","0622921e":"dataset.shape #Let's take a look at the dimensions of our data","cbb9d459":"d = {'M':1,'B':0}\ndataset = dataset.replace(d) #replace M\/B with 0 or 1 for the neural net classification\ndataset = dataset.drop(['Unnamed: 32'],axis=1) #remove column 32 - unknown purpose\ndataset = dataset.drop(['id'],axis=1) #remove the id - not needed for the neural net","308033be":"dataset_temp = dataset.drop(['diagnosis'],axis=1) #make a temporary dataset with only our feature vectors\nX = np.array(dataset_temp).T #create our Numpy array of feature vectors to be used in our neural net\nY = np.array(dataset['diagnosis']).T #create our Numpy array of diagnosis to be used in our neural net\nY = Y.reshape(1,569)","d9516844":"#Now, let's normalized our feature vector.  We will force the mean of each column to 0, and divide by the maximum\nX_mean = np.mean(X,axis=1,keepdims=True) #Find the mean of each feature\nX_max = np.max(X,axis=1,keepdims=True) #Find the maximum of each feature\nX_normalized = (X-X_mean)\/(X_max) #Normalizing our dataset by subtracting the mean and dividing by the max","ebb62cce":"#Now, let's split our dataset into two segments\n# 1) Training set to train our neural net\n# 2) A cross validation set to test the accuracy of our neural net\n\n#We'll take the first 380 samples for our training set\nX_train = X_normalized[:,:380]\nY_train = Y[:,:380]\n\n#We will take the remaining 189 for our cross-validation set\nX_cv = X_normalized[:,381:]\nY_cv = Y[:,381:]","2264743b":"#We will now define our sigmoid function to be used in the output layer of our neural network (L3)\ndef sigmoid(z):\n    s = 1\/(1+np.exp(-z))\n    return s","8588fe32":"#We will now define our tanh(x) function to be used in hidden layers of our neural network (L1, L2)\n#Note that the tanh(x) function allows better centering of data than the sigmoid function.  This is why it will be used in our hidden layers.\n\ndef tanh(z):\n    s = (np.exp(z) - np.exp(-z)) \/ (np.exp(z) + np.exp(-z))\n    return s","31caf6ce":"#Now, let's define our forward propogation function.\ndef forward_prop(X,W1,W2,W3,b1,b2,b3):\n    \n    #First layer forward propogation\n    Z1 = np.dot(W1,X)\n    A1 = tanh(Z1 + b1)\n    #Second layer forward propogation\n    Z2 = np.dot(W2,A1)\n    A2 = tanh(Z2 + b2)\n    #Third layer forward propogation\n    Z3 = np.dot(W3,A2)\n    A3 = sigmoid(Z3 + b3) #A3 will produce our probability vector\n    \n    cache = {    \n                  \"Z1\": Z1,\n                  \"A1\": A1,\n                  \"Z2\": Z2,\n                  \"A2\": A2,\n                  \"Z3\": Z3,\n                  \"A3\": A3\n            }\n    return cache","41842c66":"#Now we will perform gradient descent for our neural network in the following steps:\n#1) Start by randomly initializing our weight and intercept parameters\n#2) Run forward propogation through our neural network\n#3) Calculate the derivatives of our weights and intercept parameters via back propogation\n#4) Refine our parameters using derivatives from (3)\n#5) Reiterate 1 - 4 \n\ndef gradient_descent(iterations,X,Y,alpha):\n    \n    #Randomly initialized our parameters before running the algorithm\n    W1 = np.random.randn(3,30)*0.01\n    b1 = np.random.rand(3,1)\n    W2 = np.random.randn(2,3)*0.01\n    b2 = np.random.rand(2,1)\n    W3 = np.random.rand(1,2)*0.01\n    b3 = np.random.rand(1,1)\n    dummy,m = X.shape\n    \n    caches = [] #we will store our cost at each iteration in this array\n    count_vector = [] #We will store our iteration count in this array\n    count = 0\n    \n    for i in range (1,iterations):\n        \n            count = count + 1\n            \n            count_vector.append(count)\n        \n            params = forward_prop(X,W1,W2,W3,b1,b2,b3) #forward propogation using our parameters\n            \n            #Define our values to be used in back propogation using the dictionary of values created from running forward_prop\n            Z1 = params['Z1']\n            Z2 = params['Z2']\n            Z3 = params['Z3']\n            A1 = params['A1']\n            A2 = params['A2']\n            A3 = params['A3']\n            \n            #Define our cost function, append the cost of each iteration to caches\n            cost = -(1 \/ m)*np.sum(np.multiply(Y,np.log(A3)) + np.multiply((1-Y),np.log(1-A3)))\n            caches.append(cost)\n            \n            #Back propogation for layer 3\n            dA3 = -Y\/A3 + (1-Y)\/(1-A3)\n            dZ3 = dA3 * sigmoid(Z3)*(1-sigmoid(Z3))\n            dW3 = (1 \/ m)*np.dot(dZ3,A2.T)\n            db3 = (1 \/ m)*np.sum(dZ3,axis=1,keepdims=True)\n            \n            #Back propogation for layer 2\n            dA2 = np.dot(W3.T,dZ3)\n            dZ2 = dA2*(1-np.power(tanh(Z2),2))\n            dW2 = (1 \/ m)*np.dot(dZ2,A1.T)\n            db2 = (1 \/ m)*np.sum(dZ2,axis=1,keepdims=True)\n            \n            #Back propogation for layer 1\n            dA1 = np.dot(W2.T,dZ2)\n            dZ1 = dA1*(1-np.power(tanh(Z1),2))\n            dW1 = (1 \/ m)*np.dot(dZ1,X.T)\n            db1 = (1 \/ m)*np.sum(dZ1,axis=1,keepdims=True)\n            \n            #Redefine our weight parameters using the derivatives calculated in back propogation\n            W1 = W1 - alpha*dW1\n            W2 = W2 - alpha*dW2\n            W3 = W3 - alpha*dW3\n            \n            #Redefine our weight parameters using the derivatives calculated in back propogation\n            b1 = b1 - alpha*db1\n            b2 = b2 - alpha*db2\n            b3 = b3 - alpha*db3\n        \n    return W1,W2,W3,b1,b2,b3,count_vector,caches","5fa82ee0":"#Lets see if our algorithm is working.  We should see a declining learning curve with iteration, which eventually flatterns out\n#This will help us determine the appropriate number of iterations to run to determine the appropriate parameters\n#Note: we will use a learning rate of 0.5 for now\n\nW1,W2,W3,b1,b2,b3,count,caches = gradient_descent(1000,X_cv,Y_cv,0.5)\n\nplt.plot(count,caches,label='Cost')\n\nplt.xlabel('Iteration')\nplt.ylabel('Cost')\n\nplt.title(\"Cost vs. Iteration\")\n\nplt.legend()\n\nplt.show()","b7de6bab":"\ndef predict(X,Y,iterations,alpha,X_train,Y_train):\n\n    W1,W2,W3,b1,b2,b3,count,caches = gradient_descent(iterations,X_train,Y_train,alpha)\n    \n    Z1 = np.dot(W1,X)\n    A1 = tanh(Z1 + b1)\n    Z2 = np.dot(W2,A1)\n    A2 = tanh(Z2 + b2)\n    Z3 = np.dot(W3,A2)\n    A3 = sigmoid(Z3 + b3)\n    \n    dummy,m = A3.shape\n    Y_prediction = np.zeros((1, m))\n    \n    for i in range(m):\n        \n        Y_prediction[0, i] = 1 if A3[0, i] > 0.5 else 0\n        \n    return Y_prediction","68ce3b8a":"#Lets see how accurate the predictions made by our neural network are compared to the training set and cross validation set\nprint(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(predict(X_train,Y_train,1000,0.5,X_train,Y_train) - Y_train)) * 100))\nprint(\"Cross validation accuracy: {} %\".format(100 - np.mean(np.abs(predict(X_cv,Y_cv,1000,0.5,X_train,Y_train) - Y_cv)) * 100))","d5a7d5da":"dummy,m1 = X_train.shape\ndummy,m2 = X_cv.shape\n\ntrain_predict = predict(X_train,Y_train,1000,0.5,X_train,Y_train)\nCV_predict = predict(X_cv,Y_cv,1000,0.5,X_train,Y_train)\ncount_true_pos = 0\ncount_train_pos = 0\n\ncount_true_pos_cv = 0\ncount_cv_pos = 0\n\nfor i in range (1,m1):\n    if train_predict[0,i] == 1 and Y_train[0,i] == 1:\n        count_true_pos = count_true_pos + 1\n    if Y_train[0,i] == 1:\n        count_train_pos = count_train_pos + 1\n        \nfor i in range (1,m2):\n    if CV_predict[0,i] == 1 and Y_cv[0,i] == 1:\n        count_true_pos_cv = count_true_pos_cv + 1\n    if Y_cv[0,i] == 1:\n        count_cv_pos = count_cv_pos + 1\n        \nprint(str(count_true_pos) + \" positives predicted on the training set\")\nprint(str(count_train_pos) + \" true positives are in the training set\")\nprint(\"The accuracy of true positives on the training set is: {} %\".format(100-np.abs(100*((count_true_pos - count_train_pos)\/count_train_pos))))\nprint(\"----------------------------------------------------------------\")\nprint(str(count_true_pos_cv) + \" positives predicted on the cross validation set\")\nprint(str(count_cv_pos) + \" true positives are in the cross validation set\")\nprint(\"The accuracy of true positives on the cross validation set is: {} %\".format(100-np.abs(100*((count_true_pos_cv - count_cv_pos)\/count_true_pos_cv))))\n\n    ","c6561a4c":"count_true_neg = 0\ncount_train_neg = 0\n\ncount_true_neg_cv = 0\ncount_cv_neg = 0\n\nfor i in range (1,m1):\n    if train_predict[0,i] == 0 and Y_train[0,i] == 0:\n        count_true_neg = count_true_neg + 1\n    if Y_train[0,i] == 0:\n        count_train_neg = count_train_neg + 1\n        \nfor i in range (1,m2):\n    if CV_predict[0,i] == 0 and Y_cv[0,i] == 0:\n        count_true_neg_cv = count_true_neg_cv + 1\n    if Y_cv[0,i] == 0:\n        count_cv_neg = count_cv_neg + 1\n        \nprint(str(count_true_neg) + \" negatives predicted on the training set\")\nprint(str(count_train_neg) + \" true negatives are in the training set\")\nprint(\"The accuracy of true negatives on the training set is: {} %\".format(100-np.abs(100*((count_true_neg - count_train_neg)\/count_train_neg))))\nprint(\"----------------------------------------------------------------\")\nprint(str(count_true_neg_cv) + \" negatives predicted on the cross validation set\")\nprint(str(count_cv_neg) + \" true negatives are in the cross validation set\")\nprint(\"The accuracy of true negatives on the cross validation set is: {} %\".format(100-np.abs(100*((count_true_neg_cv - count_cv_neg)\/count_true_neg_cv))))","5e5455bc":"Our predictions look great overall!  However, it is important to see the accuracy of predictions for both the malignant and benign tumors individually.\n\n(E.g. is our accuracy only high because we're able to predict 100% of the negatives, but a smaller fraction (say, 80%) of the positives?)\n\nLet's start by taking a look at how accurately we can predict malignant tumors:","0023a92f":"# Breast Cancer Tumor Diagnosis - Neural Network from First Principals\n\nHi there, welcome to my first Kaggle Kernel!  The purpose of this Kernel is to develop a neural network that is capable of accurately predicting whether a tumor is benign or malignant, given several measurements.\n\nThe dataset we will be using is the 'Breast Cancer Wisconsin (Diagnostic) Data Set'.  The dataset contains 569 patients with breast tumors.  Several measurements and a diagnosis of malignant and benign are available for each patient.  The measurements will be used as features to make our predictions on the diagnosis. \n\nThe neural network we will be developing will be derived from first principals (this means no Python packages such as scikit-learn will be used).\n\nLets begin by loading in our dataset and taking a look at it's structure.","161cdb48":"Accuracy looks great on predicting malignant tumors for both our training set and cross-validation set.\n\nNow, let's take a look at how accurate the neural network is on predicting benign tumors:","578351bb":"Our gradient descent algorithm is complete.  Before proceeding, we should make sure that our algorithm has been coded correctly, and is behaving as expected.  Recall that we store our cost function values in an array.  We are now going to use them to validate that our algoirthm is behaving appropriately.\n\nIf our algorithm is working as intended, we should see, in general, a decreasing cost function with each iteration.  Additionally, we should see our cost function eventually flatten out.  This means that adding additional iterations doesn't help improve it much. Using our cross-validation set, let's select a learning rate (alpha) of 0.5, and plot our cost function value vs. our iterations over 1000 iterations to see if this pattern can be observed:","b886ad30":"We now have our vector of predictions, and we are ready to test the accuracy of our neural network.  \n\nLet's test the accuracy on both the training set and cross-validation set.  We should expect to see the accuracy on the training set to be a little higher, due to the weight parameters being biased towards what it has been trained with.","1f02dd06":"We now have our activation functions, and can proceed with defining our forward propogation algorithm.\n\nTo optimize our code, we will be using a vectorized implementation of forward propogation.  To do so, the following formulas will be used:\n\n*Note: x means matrix multiplication*\n\n**Layer 1**\n\n* Z1 = W1.T x X where W1 represents our matrix of weights in L1, and X represents our feature matrix of measures\n* A1 = tanh(Z1 + b1) where b1 represents our intercept term for our first layer\n\n**Layer 2**\n\n* Z2 = W2.T x A1 where W2 represents our matrix of weights in L2\n* A2 = tanh(Z2 + b2) where b2 represents our intercept term for our second layer\n\n**Layer 3**\n\n* Z3 = W3.T x A2 where W3 represents our matrix of weights in L3\n* A3= tanh(Z3 + b3) where b3 represents our intercept term for our second layer\n\nFinally, we will store all of the above Al and Zl values in a data dictionary, which will be useful for our back propogation and gradient descent algorithm.\n","f28a4b2b":"Now that we have our forward propogation algorithm, we can proceed with back propogation and gradient descent.  The following steps will be carried out to perform gradient descent, and determine our optimized weight values for each layer:\n\n1. Start by randomly initializing our weight and intercept parameters\n1. Run forward propogation through our neural network\n1. Calculate the derivatives of our weights and intercept parameters via back propogation\n1. Refine our parameters using derivatives from (3)\n1. Reiterate 1 - 4 x times\n\nOur cost function will be defined by the following:\n\n>cost = -(1 \/ m)*np.sum(np.multiply(Y,np.log(A3)) + np.multiply((1-Y),np.log(1-A3)))\n\n>Where m is the number of training examples we are using. \n\n>*This is the general cost function often used when the output layer activation is a sigmoid function.  For those familiar with statistics, this is derived using maximum likelihood.*\n\n\nThe following general formulas will be used for calculating our weight derivatives for each layer (note these are partial derivatives w.r.t to the cost function described above).  \n\nThe general results can be derived using the chain rule (many times!):\n\n>Note: * means element wise multiplication\n\n> dZ[l] = dA[l]  * g'[l]{Z[l]} where g'[l]{Z[l]} is the derivative of our activation function at layer l, evaluated at Z[l]\n\n>dA[l-1] = W[l].T x dZ[l]\n\n>Finally,\n\n>**dW[l] = (1 \/ m)*dZ[l] x A[l-1].T**\n\n>and\n\n>**db[l] =(1 \/ m) * column sum of dZ[l]**\n\n\nOur parameters will be updated using the following formula after each iteration to bring the cost function closer to it's global minimum:\n\nalpha = 'learning rate' hyperparameter\n\n\n>W1 := W1 - alpha*dW1\n\n>W2 := W2 - alpha*'W2\n\n>W3 := W3 - alpha*W3\n\n>b1 := b1 - alpha*b1\n\n>b2 := b2 - alpha*db2\n\n>b3 := b3 - alpha*db3\n\n\nAfter each parameter update, we will rerun our forward propogation and calculate our cost function.  We will then store it in an array so we can use it in later analysis of our models performance.\n\nOne final note on the formulas above: for the first iteration of back propogation, dA[3] = -Y\/A3 + (1-Y)\/(1-A)","cabeb239":"Our data is almost ready to be used to create our neural network.  Before doing so, we will split our data into a test set and a cross-validation set.\n\nThe training set will use the first 380 patients, and the cross-validation set will use the remaining 188.  Our neural network will be trained on the training set, and it's accuracy will be validated on the cross-validation set.","855a3d6d":"The data has 569 rows, each representing a single patient, with 28 measurements each (note: there are 31 columns, but column 1 is an ID, column 2 is a diagnosis, and the 32nd is not a measurement)\n\nWe are now going to proceed by massaging the data.  The following steps will be taken to prepare the data for our neural network:\n*  Replace the diagnosis of \"M\" for malignant with 1, and \"B\" for benign with 0\n*  Remove the 32nd column (not sure why this is included, it is a column of 'NaN' values.  Perhaps this was erroneously included in the dataset)\n* Remove the 'id' column.  These values will not be needed to create our neural network.\n*  Split our dataset into two: a calssification vector (diagnosis) and our feature matrix (the 28 measurements)\n* Normalize our feature matrix by forcing the mean of each measurement  to 0, and dividing each measurement by the maximum value of that measurement in the dataset.","ac2c4f33":"Accuracy looks good on predicting benign tumors for the training set and cross validation set\n\nIt looks like we have successfully created a neural network to accurately predict malignant or benign brain tumors!\n\n#This concludes the Kernel.  Thank you for viewing!","04aa3c80":"Now that our data has been massaged and split into our training and cross-validation sets, we can discuss the structure of the neural network.\n\n* The neural network will have one input layer, two hidden layers, and one output layer.\n* Hidden layer 1 will have 3 nodes, and hidden layer 2 will have 2 nodes.\n* The two hidden layers will use a tanh(x) activation function, and the output layer will use a sigmoid activation function. \n\n*Note:  I have decided to use tanh(x) rather than sigmoid(x) in the hidden layers, as this will allow the data to be more centered*\n\nSee below for a visual representation of our neural network:\n\n<img src=\"https:\/\/i.imgur.com\/ZNLunIV.png\" width=\"500px\"\/>\n\nWe will now define our activation functions to be used in our hidden layers:","75a95d94":"Our algorithm is behaving as expected.  Now to complete our predictions vector.\n\nRecall that the output of our neural network after performing forward propogation is a probability that a tumor is malignant, given our input vector X.  We therefore must set a certain 'threshold probability' for when we classify a tumor as malignant or benign.  Let's set our threshold to be 50%.  That is, if our model outputs a prediction of 0.5 or higher, we will classify the tumor as malignant.  Let's put this idea into code:"}}