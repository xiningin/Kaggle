{"cell_type":{"dfa04c6c":"code","7f34f063":"code","80ded9d1":"code","98506ab2":"code","1582dcb1":"code","3de5359b":"code","8b610a51":"code","1fa76c0f":"code","8b6854cb":"code","c46818d5":"code","fb78b56d":"markdown","79ecaddf":"markdown","b17ab426":"markdown","baa261d7":"markdown","e73e7b12":"markdown","d83770b8":"markdown","c45f82b1":"markdown"},"source":{"dfa04c6c":"!pip install tensorflow==1.14","7f34f063":"# computational\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom sklearn.preprocessing import StandardScaler\n\n# functional programming\nfrom functools import partial\n\n# plotting\nimport matplotlib.pyplot as plt\n\n# plot utilities\n%matplotlib inline\nplt.style.use('ggplot')\ndef plt_left_title(title): plt.title(title, loc=\"left\", fontsize=18)\ndef plt_right_title(title): plt.title(title, loc='right', fontsize=13, color='grey')\n\n# use eager execution for better ease-of-use and readability\n# tf.enable_eager_execution()\n\n# aliases\ntfk = tf.keras\ntfd = tfp.distributions\n\n# helper function, mostly for plotting\ndef logistic(x, w, b):\n    exp_term = np.exp(-(x * w + b))\n    return 1 \/ (1 + exp_term)\n\nprint(f\"            tensorflow version: {tf.__version__}\")\nprint(f\"tensorflow probability version: {tfp.__version__}\")","80ded9d1":"data = np.array([[66.,  0.],\n [70.,  1.],\n [69.,  0.],\n [68.,  0.],\n [67.,  0.],\n [72.,  0.],\n [73.,  0.],\n [70.,  0.],\n [57.,  1.],\n [63.,  1.],\n [70.,  1.],\n [78.,  0.],\n [67.,  0.],\n [53.,  1.],\n [67.,  0.],\n [75.,  0.],\n [70.,  0.],\n [81.,  0.],\n [76.,  0.],\n [79.,  0.],\n [75.,  1.],\n [76.,  0.],\n [58.,  1.]])\n\n# xs: temperature\n# ys: o-ring failure (1 == failure occurred)\nn_observations = data.shape[0]\nxs, ys = data[:, 0, np.newaxis], data[:, 1, np.newaxis]\nscaler = StandardScaler()\nxs = scaler.fit_transform(xs)\nxs_test = np.linspace(xs.min(), xs.max(), 53)[:, np.newaxis]\n\ndef plot_training_data(): \n    plt.figure(figsize=(12, 7))\n    plt.scatter(xs, ys, c=\"#619CFF\", label=\"observed\", s=200)\n    plt.xlabel(\"Temperature (scaled)\")\n    plt.ylabel(\"O-Ring Failure\")\n    \ndef plt_left_title(title): plt.title(title, loc=\"left\", fontsize=18)\ndef plt_right_title(title): plt.title(title, loc='right', fontsize=13, color='grey')\n    \nprint(f\"{n_observations} observations\")\nprint(f\"{int(ys.sum())} failures\")","98506ab2":"# train an artificial neural net\nnn_model = tfk.Sequential([\n    tfk.layers.Dense(1,\n                     activation=tf.nn.sigmoid)\n])\nnn_model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n                 loss=\"binary_crossentropy\")\nnn_model.fit(xs, ys,\n             epochs=800,\n             verbose=False);\nprint(f\"estimated w: {nn_model.layers[0].get_weights()[0]}\")\nprint(f\"estimated b: {nn_model.layers[0].get_weights()[1]}\")","1582dcb1":"plot_training_data()\nplt_left_title(\"Challenger Dataset\")\nplt_right_title(\"How can we quantify uncertainty in our prediction?\")\n\n# plot neural network result\nplt.plot(xs_test,\n         nn_model.predict(xs_test),\n         \"g\", linewidth=4,\n         label=\"artificial neural net prediction\")\nplt.legend(loc=\"center right\");","3de5359b":"# placeholder variables\nx = tf.placeholder(shape=[None, 1], dtype=tf.float32)\ny = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n\n# flipout layer, which will yield distributions on our weights\n# in this case, only one weight and a bias term, each with normal priors\nlayer = tfp.layers.DenseFlipout(1, \n                                activation=None,\n                                kernel_posterior_fn=tfp.layers.default_mean_field_normal_fn(),\n                                bias_posterior_fn=tfp.layers.default_mean_field_normal_fn())\n\n# make a prediction\nlogits = layer(x)\n# those predictions are parameters for bernoulli distributions \nlabels_dist = tfd.Bernoulli(logits=logits)\n\n# use evidence-lower bound (ELBO) as the loss\nneg_log_likelihood = -tf.reduce_mean(labels_dist.log_prob(y))\nkl = sum(layer.losses) \/ n_observations\nelbo_loss = neg_log_likelihood + kl\n\n# make predictions, and check accuracy\npredictions = tf.cast(logits > 0, dtype=tf.int32)\ncorrect_predictions = tf.equal(predictions, y)\naccuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n\n# minimize ELBO\noptimizer = tf.train.AdamOptimizer(learning_rate=0.01)\ntrain_op = optimizer.minimize(elbo_loss)","8b610a51":"n_steps = 1300\nn_posterior_samples = 125\n\nhistory_loss = []\nhistory_acc = []\n\ncandidate_ws = []\ncandidate_bs = []\n\ninit_op = tf.global_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n    \n    # run training loop\n    print(\"Start training...\")\n    for step in range(n_steps):\n        # feed in training data\n        feed_dict = {x: xs, y: ys}\n        \n        # execute the graph \n        _ = sess.run(train_op, feed_dict=feed_dict)\n        \n        # determine loss and accuracy\n        loss_value, acc_value = sess.run([elbo_loss, accuracy],feed_dict=feed_dict)\n        \n        if ((step + 1) % 100) == 0:\n            print(f\"{'-'*50} step {step + 1}\")\n            print(f\"Loss {loss_value:.3f}, Accuracy: {acc_value:.3f}\")\n        \n        # record loss and accuracy\n        history_loss.append(loss_value)\n        history_acc.append(acc_value)\n    \n    print(\"Done training!\\n\")\n    print(f\"Taking {n_posterior_samples} samples from posterior distributions on weights\\n\")\n    \n    w_draw = layer.kernel_posterior.sample(seed=27)\n    b_draw = layer.bias_posterior.sample(seed=27)\n    \n    for mc in range(n_posterior_samples):\n        w_, b_ = sess.run([w_draw, b_draw])\n        candidate_ws.append(w_)\n        candidate_bs.append(b_)\n        \n    print(\"Sampling complete. Samples are stored in numpy arrays:\")\n    print(f\"  weight: candidate_ws\")\n    print(f\"    bias: candidate_bs\")\n        \ncandidate_ws = np.array(candidate_ws).reshape(-1, 1).astype(np.float32)\ncandidate_bs = np.array(candidate_bs).astype(np.float32)","1fa76c0f":"fig = plt.figure(figsize=(14, 5))\nax1, ax2 = fig.subplots(1, 2)\nax1.plot(history_loss)\nax1.set_title(\"ELBO loss\", loc=\"left\", fontsize=18)\n\nax2.plot(history_acc)\nax2.set_title(\"Accuracy\", loc=\"left\", fontsize=18)\nax2.set_title(\"noise is expected\", loc='right', fontsize=13, color='grey')\n\nplt.show();","8b6854cb":"fig = plt.figure(figsize=(14, 5))\nax1, ax2 = fig.subplots(1, 2)\n\nax1.hist(candidate_ws);\nax1.set_title(f\"$w$ posterior\", loc=\"left\", fontsize=18)\nax1.set_title(f\"mean = {candidate_ws.mean():.3f}\", loc='right', fontsize=13, color='grey')\n\nax2.hist(candidate_bs)\nax2.set_title(f\"$b$ posterior\", loc=\"left\", fontsize=18)\nax2.set_title(f\"mean = {candidate_bs.mean():.2f}\", loc='right', fontsize=13, color='grey')\n\nprint(f\"{n_posterior_samples} posterior samples\")","c46818d5":"plot_training_data()\nplt_left_title(\"Challenger Data\")\nplt_right_title(f\"{n_posterior_samples} draws from posterior\")\n\n# plot candidate curves\nplt.plot(xs_test, \n         logistic(xs_test, candidate_ws.T, candidate_bs.T), \n         'r', alpha=0.2, linewidth=0.5);\n# this is a placeholder for the labels -- no data is plotted\nplt.plot([], [], 'r',\n         label=f\"candidate curves\")\n# plot candidate curve based on mean weights\nplt.plot(xs_test, \n         logistic(xs_test, candidate_ws.mean(), candidate_bs.mean()), \n         '--', c='darkred', linewidth=4, \n         label=\"mean candidate curve\")\n# plot neural network result\nplt.plot(xs_test,\n         nn_model.predict(xs_test),\n         \"g\", linewidth=4,\n         label=\"artificial neural net\")\nplt.legend(loc=\"center right\", facecolor=\"white\");","fb78b56d":"Now we can train our model. We'll keep track of the following:\n\n- model loss\n- model accuracy (this does not affect training)\n- candidate $w$'s (weights)\n- candidate $b$'s (biases)\n\nRemeber that the weights and biases are not single numbers, but random variables. Furthermore, they will most likely not follow any distribution we've seen before, we we need to take random samples from them. \n\nBelow we train the model, and then plot the loss and accuracy.","79ecaddf":"So how do we quantify uncertainty? There are two ways:\n\n1. Instead of assuming $w$ and $b$ are numbers, assume they are randomly distributed\n2. Have the output of our model be a Bernoulli distribution, to address any randomness in the dependent variable\n\nIn order to incorporate these changes, we have to make some adjustments. They are:\n\n1. Use the `tensorflow_probability.layers.DenseFlipout` layer, which allows us to impose distributions on our hidden layer weights\n2. Use the [Evidence Lower Bound (ELBO)](https:\/\/en.wikipedia.org\/wiki\/Kullback%E2%80%93Leibler_divergence) as our loss function\n\nThe following code makes the proper changes, and was adapted from the [tensorflow examples](https:\/\/github.com\/tensorflow\/probability\/blob\/master\/tensorflow_probability\/examples\/logistic_regression.py).","b17ab426":"Below we train an Artificial Neural Network. It has one hidden layer with a weight and bias ($w$ and $b$), along with a sigmoid activation function. This basically constructs the logistic function, and our loss function helps us find the ideal weights and bias.\n\nA few things to note:\n- `xs` is the data for independent variable (temperature) and they are scaled to have mean 0 and standard deviation 1\n- `ys` is the data for the dependent variable (O-Ring failure) and they take on values 0 and 1","baa261d7":"# Logistic Regression with Bayesian Neural Networks\n\nWe'll look at **logistic regression** from a Bayesian Standpoint using [TensorFlow Probability](https:\/\/www.tensorflow.org\/probability\/).\n\nIn logistic regression we have:\n\n- any number of independent variables\n- a dependent variable that take the value 0 or 1\n\nIn order to describe this relationship, we want to fit a curve of the form:\n\n$$\nf(x) = \\frac{1}{1 + \\exp \\{ - (w x + b) \\}}\n$$\n\nThe parameters $w$ and $b$ determine the shape of the curve, so we want to find the best values for $w$ and $b$.\n\nWe'll attempt to fit a curve for the data relating temperature to O-Ring Failures for the Challenger Disaster.\n\nThe plot below shows the data along with a possible logistic curve. This curve seems like a good fit, but we don't have a  sense of how it compares to similar curves. In other words, we have no way of describing the _uncertainty_ of our predictions.\n\n**Bayesian Neural Networks** help us quantify uncertainty.\n\n> This is based on an example in [Probabilistic Programming and Bayesian Methods for Hackers](https:\/\/github.com\/CamDavidsonPilon\/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers\/blob\/master\/Chapter2_MorePyMC\/Ch2_MorePyMC_TFP.ipynb) by Cam Davidson-Pilon, and ported to TensorFlow by Matthew McAteer","e73e7b12":"Next, let's take a look at how the $w$'s and $b$'s are distributed. \n\n> Note: These distributions were determined during the model training, and all of the math is handled in `tfp.layers.DenseFlipout`\n\nThey don't follow any recognizable distribution, and both take on mostly negative values.","d83770b8":"Was our our original prediction better? In terms of minimizing loss, yes. In terms of giving us a better understanding of the underlying model, not so much.\n\nThe Bayesian approach gives us much more information with just a few changes, and lets us address the uncertainty in our data, our models, and our assumptions.","c45f82b1":"Lastly, we'll plot 125 possible logistic curves for this data. \n\nThis plot gives us a better sense of the uncertainty in our model. We can make a few observations:\n- a positive-sloping curve is very unlikely, given the data\n- the mean of the candidate curves (dotted line) is less aggressive than our original prediction\n- the original prediction is not an unlikely candidate, given the data"}}