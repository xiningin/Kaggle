{"cell_type":{"5f736e4f":"code","339d11a5":"code","10ce16ea":"code","afb252b6":"code","3deca883":"code","2ac062c8":"code","76ab3bc7":"code","1ebe1e9b":"code","d5f4294a":"code","ff6a0840":"code","e000bea3":"code","8322c7db":"code","cceba57e":"markdown","2f6c161e":"markdown","7c96ecb3":"markdown","bf9a2364":"markdown"},"source":{"5f736e4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","339d11a5":"data = pd.read_csv('\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv')\nprint(plt.style.available) # look at available plot styles\nplt.style.use('ggplot')","10ce16ea":"# To see features and target variable \ndata.head()","afb252b6":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","3deca883":"data.describe()","2ac062c8":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 100,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","76ab3bc7":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","1ebe1e9b":"# KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x,y)\nprediction = knn.predict(x)\nprint('Prediction: {}'.format(prediction))","d5f4294a":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\nknn = KNeighborsClassifier(n_neighbors = 3)\nx,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\n#print('Prediction: {}'.format(prediction))\nprint('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test)) # accuracy","ff6a0840":"# Model complexity\nneig = np.arange(1, 25)\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over different values of k\nfor i, k in enumerate(neig):\n    # k from 1 to 25(exclude)\n    knn = KNeighborsClassifier(n_neighbors=k)\n    # Fit with knn\n    knn.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy.append(knn.score(x_train, y_train))\n    # test accuracy\n    test_accuracy.append(knn.score(x_test, y_test))\n\n# Plot\nplt.figure(figsize=[13,8])\nplt.plot(neig, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neig, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.title('-value VS Accuracy')\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.xticks(neig)\nplt.savefig('graph.png')\nplt.show()\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))","e000bea3":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators = 100,random_state = 1)\nrf.fit(x_train,y_train)\n\nprint(\"random forest accuracy: \",rf.score(x_test,y_test))","8322c7db":"# Model complexity\nestimators = np.arange(1,200)\ntrain_accuracy1 = []\ntest_accuracy1 = []\n# Loop over different values of k\nfor i, j in enumerate(estimators):\n    rf = RandomForestClassifier(n_estimators = j,random_state = 1)\n    rf.fit(x_train,y_train)\n    #train accuracy\n    train_accuracy1.append(rf.score(x_train,y_train))\n    # test accuracy\n    test_accuracy1.append(rf.score(x_test,y_test))\n\n# Plot\n\nprint(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy1),1+test_accuracy1.index(np.max(test_accuracy1))))","cceba57e":"Read the data\n","2f6c161e":"pd.plotting.scatter_matrix:\n\ngreen: normal and red: abnormal\nc: color\nfigsize: figure size\ndiagonal: histohram of each features\nalpha: opacity\ns: size of marker\nmarker: marker type","7c96ecb3":"\nEXPLORATORY DATA ANALYSIS (EDA)\n\nhead(): default value of it shows first 5 rows(samples). If you want to see for example 100 rows just write head(100)","bf9a2364":"How many normal(green) and abnormal(red) classes are there.\n\nSearborn library has countplot() that counts number of classes\nAlso you can print it with value_counts() method"}}