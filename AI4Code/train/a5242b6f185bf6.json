{"cell_type":{"4a17bb81":"code","b1de2a35":"code","7983de0f":"code","4fb12e7a":"code","b8461718":"code","4a78f8de":"code","4885dcca":"code","5a577a71":"code","6dbe9e65":"code","f60700df":"code","1e4ba567":"code","26ae054f":"code","c97899a9":"code","b094d75f":"code","098eaa38":"code","0b71494f":"code","06c21946":"code","10019d63":"code","aec453bd":"code","c42b5205":"code","4711025d":"code","921ef70d":"code","3db4e2cb":"code","bc93b27e":"code","265f66a5":"code","f092b8bb":"code","f0980c4c":"code","937eb8e8":"code","01ebc195":"code","d15e0072":"code","85867245":"code","aecbaba1":"code","c9ea90b7":"code","5b877735":"markdown","0a2f33d3":"markdown","554537db":"markdown","8b3cf4a7":"markdown","ede7aadd":"markdown","3a543b01":"markdown","4b234a57":"markdown","9481e287":"markdown","f058bb27":"markdown","788f7d13":"markdown","2794e3aa":"markdown","8f043f52":"markdown","2b127e05":"markdown","375c43a7":"markdown","6e663945":"markdown","32dbb720":"markdown","6a12eff3":"markdown","2e6d7f85":"markdown","cc7570f0":"markdown","5ebb9b6f":"markdown","d3a6cbef":"markdown","6ba0428f":"markdown","1801fdfe":"markdown","ea91350c":"markdown","43df9332":"markdown","64d7fc5c":"markdown","8dd3d811":"markdown","e0206dcc":"markdown"},"source":{"4a17bb81":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b1de2a35":"train = pd.read_csv(\"..\/input\/glass-quality-prediction\/Glass_Quality_Participants_Data\/Train.csv\")\ntrain.head()","7983de0f":"train.shape","4fb12e7a":"train.columns","b8461718":"train.isnull().sum()","4a78f8de":"train.dtypes","4885dcca":"train.describe()","5a577a71":"train.columns","6dbe9e65":"plt.figure(figsize=(16,10))\n\nplt.subplot(2,2,1)\ntrain.boxplot(column=['ymin', 'ymax'])\n\nplt.subplot(2,2,2)\ntrain.boxplot(column=['pixel_area', 'log_area'])\n\nplt.subplot(2,2,3)\ntrain.boxplot(column=['max_luminosity', 'thickness'])\n\nplt.subplot(2,2,4)\ntrain.boxplot(column=['xmin', 'xmax'])\n","f60700df":"plt.figure(figsize=(14,8))\nclr=['red','blue','lime','orange','teal','red','blue','lime']\ncolumns = ['max_luminosity', 'thickness', 'xmin', 'xmax', 'ymin', 'ymax', 'pixel_area', 'log_area']\nfor i,j in zip(range(1,9),columns):\n    plt.subplot(4,2,i)\n    train[j].hist(color = clr[i-1], label=j)\n    plt.legend()\n    ","1e4ba567":"plt.figure(figsize=(14,8))\ntrain[columns].plot(kind='density', subplots=True, \n                                                    layout=(4,2), sharex=False,\n                                                    sharey=False, figsize=(14,6))\nplt.show()","26ae054f":"train.columns","c97899a9":"plt.figure(figsize=(14,12))\n\nplt.subplot(4,2,1)\ntrain.grade_A_Component_1.value_counts().plot(kind='bar', label = 'grade_A_Component_1')\nplt.legend()\n\nplt.subplot(4,2,2)\ntrain.grade_A_Component_2.value_counts().plot(kind='bar', label = 'grade_A_Component_2')\nplt.legend()\n\nplt.subplot(4,2,3)\ntrain.x_component_1.value_counts().plot(kind='bar', label = 'x_component_1')\nplt.legend()\n\nplt.subplot(4,2,4)\ntrain.x_component_2.value_counts().plot(kind='bar', label = 'x_component_2')\nplt.legend()\n\nplt.subplot(4,2,5)\ntrain.x_component_3.value_counts().plot(kind='bar', label = 'x_component_3')\nplt.legend()\n\nplt.subplot(4,2,6)\ntrain.x_component_4.value_counts().plot(kind='bar', label = 'x_component_4')\nplt.legend()\n\nplt.subplot(4,2,7)\ntrain.x_component_4.value_counts().plot(kind='bar', label = 'x_component_4')\nplt.legend()\n","b094d75f":"train['class'].value_counts().plot(kind='bar')","098eaa38":"train['class'].value_counts()","0b71494f":"import seaborn as sns\nsns.set(style=\"ticks\")\n\nsns.pairplot(train)","06c21946":"train.columns","10019d63":"train.reset_index(drop=True, inplace=True)","aec453bd":"x = train.drop(['class'], axis=1)\ny = train['class']","c42b5205":"x_copy = x.copy()","4711025d":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 100)","921ef70d":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# feature extraction\nmodel = LogisticRegression(solver='lbfgs')\nrfe = RFE(model, 3)\nfit = rfe.fit(x, y)\n\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n\n\ndf_feat = pd.DataFrame(fit.ranking_, x.columns)\ndf_feat.rename(columns = {0:\"Feature_Ranking\"}, inplace=True)\n","3db4e2cb":"df_feat.sort_values(by=\"Feature_Ranking\").plot(kind='bar', figsize=(18,7))","bc93b27e":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.metrics import classification_report\n","265f66a5":"from sklearn.ensemble import RandomForestClassifier\n\n#making the instance\nmodel= RandomForestClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_rf = clf.fit(X_train, y_train)\n\n#Predict\npredictions = best_clf_rf.predict(X_test)\n\n#Check Prediction Score\nprint(\"Accuracy of Random Forest: \",accuracy_score(y_test, predictions))\n\n#Print Classification Report\nprint(\"Confusion matrix \\n\",confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\n","f092b8bb":"#RF On Full data\n\n#making the instance\nmodel= RandomForestClassifier(random_state=1234)\n\n#Hyper Parameters Set\nparam_grid = {'criterion':['gini','entropy'],\n          'n_estimators':[10,15,20,25,30],\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[123],\n          'n_jobs':[-1]}\n\n# Create grid search object\nclf = GridSearchCV(model, param_grid=param_grid, n_jobs=-1, cv=10)\n\n# Fit on data\nbest_clf_rf1 = clf.fit(x, y)","f0980c4c":"test = pd.read_csv('..\/input\/glass-quality-prediction\/Glass_Quality_Participants_Data\/Test.csv')\n\ntest.shape","937eb8e8":"test.head(5)","01ebc195":"test.columns","d15e0072":"test_for_prediction = test[['grade_A_Component_1', 'grade_A_Component_2', 'max_luminosity',\n       'thickness', 'xmin', 'xmax', 'ymin', 'ymax', 'pixel_area', 'log_area',\n       'x_component_1', 'x_component_2', 'x_component_3', 'x_component_4',\n       'x_component_5']]","85867245":"#Define predict function\n\ndef predict_file(model, model_instance, test_data):\n    prediction_var = \"prediction_from\" + model\n    file_name = \"Final_output_prediction_from_\" + model + \".xlsx\"\n    prediction_var  = model_instance.predict_proba(test_data)\n    df_prediction_var = pd.DataFrame(prediction_var, columns=[1,2])\n    df_prediction_var.to_excel(file_name)\n    print(\"{} created.\".format(file_name))","aecbaba1":"predict_file(\"rf_classifier\", best_clf_rf, test_for_prediction)","c9ea90b7":"predict_file(\"rf1_classifier\", best_clf_rf1, test_for_prediction)","5b877735":"# Step4: Separating X and Y","0a2f33d3":"### 2.3.4 Density Plots Of Continuous Variables ","554537db":"We humans have been using glass since ancient times for a variety of applications from building construction to making decorative objects. With technology, glass and its applications have evolved, and today, we have different varieties of glass used for very different purposes from a computer monitor to a bulletproof car window depending on the grade of the glass produced. And not all grades or varieties are manufactured the same way. In this data science challenge, you as a data scientist must use the given data to predict the grade of the glass produced based on the given factors.\n\nGiven are 15 distinguishing factors that can provide insight into what grade of the glass is being produced. Your objective as a data scientist is to build a machine learning model that can predict the grade of glass based on the given factors.","8b3cf4a7":"## 4.2 Split Data","ede7aadd":"### 2.4.2 Scatterplot Matrix","3a543b01":"## 2.1 Missing Data Analysis ","4b234a57":"## 6.2 Importing and Model Fitting","9481e287":"# Problem Statement","f058bb27":"# Phase1: Model Building On Training Data","788f7d13":"# Phase2: Applying Model On Test Data","2794e3aa":"### 6.2.2 Random Forest","8f043f52":"Bi-variate Analysis finds out the relationship between two variables. Here, we look for association and disassociation between variables at a pre-defined significance level. We can perform bi-variate analysis for any combination of categorical and continuous variables. The combination can be: Categorical & Categorical, Categorical & Continuous and Continuous & Continuous. Different methods are used to tackle these combinations during analysis process.","2b127e05":"## 4.1 Re-setting Index Before Splitting","375c43a7":"# Step6: Model Building","6e663945":"<b> From the plots we can see that, there are lots of outliers in each varibale. <\/b>","32dbb720":"# Step5: Creating Train and Test Set In Ratio 80:20","6a12eff3":"# Step1: Read Data","2e6d7f85":"### 2.3.1 Box Plot of CONTINUOUS variables ","cc7570f0":"# 2.4 Bi-variate Analysis","5ebb9b6f":"Below are the steps involved to understand, clean and prepare your data for building your predictive model:\n\n1. Variable Identification\n2. Univariate Analysis\n3. Bi-variate Analysis\n4. Missing values treatment\n5. Outlier treatment\n6. Variable transformation\n7. Variable creation","d3a6cbef":"## 2.2 Data Type Analysis ","6ba0428f":"# Step2: Exploratory Data Analysis","1801fdfe":"### 2.3.5 Target Variable Plot","ea91350c":"## 6.1 Identification Of Best Features","43df9332":"## 2.3 Univariate Analysis\n\nAt this stage, we explore variables one by one. Method to perform uni-variate analysis will depend on whether the variable type is categorical or continuous. Let\u2019s look at these methods and statistical measures for categorical and continuous variables individually:\n\n<b> Continuous Variables:- <\/b> In case of continuous variables, we need to understand the central tendency and spread of the variable. These are measured using various statistical metrics such as Histogram and Bar plots: ","64d7fc5c":"### 2.3.5 Discrete Variables Plot","8dd3d811":"### 2.3.3 Histogram Plots Of Continuous Variables ","e0206dcc":"### 2.3.2 Plot for Continuous variables"}}