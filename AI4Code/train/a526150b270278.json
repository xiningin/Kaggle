{"cell_type":{"ef9e4156":"code","e06646a2":"code","6c74bf1f":"code","6ae467df":"code","e66c386e":"code","35a780e8":"code","dba0f294":"code","bbc08ff0":"code","bc085dbd":"code","d1aa3372":"code","05c317a1":"code","1b038b04":"code","9fc1f7d6":"code","c61c63df":"code","f54544b8":"code","935dd9d6":"code","3202b855":"markdown","39b25726":"markdown","26cf6830":"markdown","d449b663":"markdown","ef205474":"markdown","ba3138b3":"markdown","e9ab7602":"markdown","e5679cff":"markdown","15bb6fb5":"markdown","987a27f3":"markdown","2709918a":"markdown","304a68f9":"markdown","d9a9da15":"markdown","894aa0cc":"markdown","55dd9553":"markdown","b3c62d05":"markdown","f9c44a69":"markdown","e5292e2a":"markdown","0ab718ef":"markdown","39414798":"markdown","925c9226":"markdown"},"source":{"ef9e4156":"import wandb\nfrom pytorch_lightning.loggers import WandbLogger\n\nwandb.login()","e06646a2":"!pip install  torch==1.7.1 --quiet\n!pip install pytorch_lightning==1.4.5 --quiet","6c74bf1f":"# Helper libraries\nimport os\nimport gc\ngc.enable()\nimport math\nimport json\nimport time\nimport random\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom sklearn import model_selection\nfrom collections import OrderedDict\n\n#Pytorch, transformers\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Parameter\nimport torch.optim as optim\nfrom torch.utils.data import (\n    Dataset, DataLoader,\n    SequentialSampler, RandomSampler\n)\nfrom torch.utils.data.distributed import DistributedSampler\n\ntry:\n    from apex import amp\n    APEX_INSTALLED = True\nexcept ImportError:\n    APEX_INSTALLED = False\n\nimport transformers\nfrom transformers import (\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    AutoModel,\n    AutoTokenizer,\n    get_cosine_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    logging,\n    MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n)\nlogging.set_verbosity_warning()\nlogging.set_verbosity_error()\n\n\n#Import pytorch lightning: \nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\nprint(f\"Apex AMP Installed :: {APEX_INSTALLED}\")\nMODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","6ae467df":"train = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv')\ntest = pd.read_csv('..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv')\nexternal_mlqa = pd.read_csv('..\/input\/mlqa-hindi-processed\/mlqa_hindi.csv')\nexternal_xquad = pd.read_csv('..\/input\/mlqa-hindi-processed\/xquad.csv')\nexternal_train = pd.concat([external_mlqa, external_xquad])","e66c386e":"def create_folds(data, num_splits):\n    '''\n        This function is used for creating the folds for k-fold splits\n        \n        Args: \n            - data: Dataframe\n            - num_splits: int: Number of splits needed\n        \n        Returns: \n            - df_fold: Dataframe with folds\n    '''\n    \n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1).reset_index(drop=True)\n\n \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, random_state=42, shuffle=True)\n    \n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data['language'])):\n        data.loc[v_, 'kfold'] = f\n\n    # return dataframe with folds\n    return data","35a780e8":"def set_seed(seed):\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    \n\n\ndef optimal_num_of_loader_workers():\n    num_cpus = multiprocessing.cpu_count()\n    num_gpus = torch.cuda.device_count()\n    optimal_value = min(num_cpus, num_gpus*4) if num_gpus else num_cpus - 1\n    return optimal_value\n","dba0f294":"# Necessary helper function: \n\ndef convert_answers(row):\n    return {'answer_start': [row[0]], 'text': [row[1]]}\n","bbc08ff0":"train = create_folds(train, num_splits=5)\nexternal_train[\"kfold\"] = -1\nexternal_train['id'] = list(np.arange(1, len(external_train)+1))\ndf = pd.concat([train, external_train]).reset_index(drop=True)\n\ndf['answers'] = df[['answer_start', 'answer_text']].apply(convert_answers, axis=1)","bc085dbd":"class Config():\n    # model\n    model_type = 'xlm_roberta'\n    model_name_or_path = \"deepset\/xlm-roberta-base-squad2\"\n    config_name = \"deepset\/xlm-roberta-base-squad2\"\n    fp16 = True if APEX_INSTALLED else False\n    fp16_opt_level = \"O1\"\n    gradient_accumulation_steps = 2\n\n    # tokenizer\n    tokenizer_name = \"deepset\/xlm-roberta-base-squad2\"\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    max_seq_length = 384\n    doc_stride = 128\n\n    # train\n    epochs = 1\n    train_batch_size = 1\n    eval_batch_size = 1\n\n    # optimizer\n    optimizer_type = 'AdamW'\n    learning_rate = 1.5e-5\n    weight_decay = 1e-2\n    epsilon = 1e-8\n    max_grad_norm = 1.0\n\n    # scheduler\n    decay_name = 'linear-warmup'\n    warmup_ratio = 0.1\n\n    # logging\n    logging_steps = 10\n\n    # evaluate\n    output_dir = 'output'\n    seed = 42","d1aa3372":"args = Config()\ntokenizer = args.tokenizer","05c317a1":"def prepare_train_features(example):\n    tokenizer = args.tokenizer\n    example[\"question\"] = example[\"question\"].lstrip()\n    tokenized_example = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        truncation=\"only_second\",\n        max_length=args.max_seq_length,\n        stride=args.doc_stride,\n        return_overflowing_tokens=True,\n        return_offsets_mapping=True,\n        padding=\"max_length\",\n    )\n\n    sample_mapping = tokenized_example.pop(\"overflow_to_sample_mapping\")\n    offset_mapping = tokenized_example.pop(\"offset_mapping\")\n\n    features = []\n    for i, offsets in enumerate(offset_mapping):\n        feature = {}\n\n        input_ids = tokenized_example[\"input_ids\"][i]\n        attention_mask = tokenized_example[\"attention_mask\"][i]\n\n        feature['input_ids'] = input_ids\n        feature['attention_mask'] = attention_mask\n        feature['offset_mapping'] = offsets\n\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n        sequence_ids = tokenized_example.sequence_ids(i)\n\n        sample_index = sample_mapping[i]\n        answers = example[\"answers\"]\n\n        if len(answers[\"answer_start\"]) == 0:\n            feature[\"start_position\"] = cls_index\n            feature[\"end_position\"] = cls_index\n        else:\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            token_start_index = 0\n            while sequence_ids[token_start_index] != 1:\n                token_start_index += 1\n\n            token_end_index = len(input_ids) - 1\n            while sequence_ids[token_end_index] != 1:\n                token_end_index -= 1\n\n            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n                feature[\"start_position\"] = cls_index\n                feature[\"end_position\"] = cls_index\n            else:\n                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n                    token_start_index += 1\n                feature[\"start_position\"] = token_start_index - 1\n                while offsets[token_end_index][1] >= end_char:\n                    token_end_index -= 1\n                feature[\"end_position\"] = token_end_index + 1\n\n        features.append(feature)\n    return features\n\n\ndef make_loader(  fold\n):\n    train_set, valid_set = df[df['kfold']!=fold], df[df['kfold']==fold]\n    \n    train_features, valid_features = [[] for _ in range(2)]\n    for i, row in train_set.iterrows():\n        train_features += prepare_train_features(row)\n    for i, row in valid_set.iterrows():\n        valid_features += prepare_train_features( row)\n\n    train_dataset = DatasetRetriever(train_features)\n    valid_dataset = DatasetRetriever(valid_features)\n    print(f\"Num examples Train= {len(train_dataset)}, Num examples Valid={len(valid_dataset)}\")\n    \n    train_sampler = RandomSampler(train_dataset)\n    valid_sampler = SequentialSampler(valid_dataset)\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=args.train_batch_size,\n        sampler=train_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True,\n        drop_last=False \n    )\n\n    valid_dataloader = DataLoader(\n        valid_dataset,\n        batch_size=args.eval_batch_size, \n        sampler=valid_sampler,\n        num_workers=optimal_num_of_loader_workers(),\n        pin_memory=True, \n        drop_last=False\n    )\n\n    return train_dataloader, valid_dataloader","1b038b04":"class DatasetRetriever(Dataset):\n    def __init__(self, features, mode='train'):\n        super(DatasetRetriever, self).__init__()\n        self.features = features\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, item):   \n        feature = self.features[item]\n        if self.mode == 'train':\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':torch.tensor(feature['offset_mapping'], dtype=torch.long),\n                'start_position':torch.tensor(feature['start_position'], dtype=torch.long),\n                'end_position':torch.tensor(feature['end_position'], dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids':torch.tensor(feature['input_ids'], dtype=torch.long),\n                'attention_mask':torch.tensor(feature['attention_mask'], dtype=torch.long),\n                'offset_mapping':feature['offset_mapping'],\n                'sequence_ids':feature['sequence_ids'],\n                'id':feature['example_id'],\n                'context': feature['context'],\n                'question': feature['question']\n            }","9fc1f7d6":"def loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    total_loss = (start_loss + end_loss) \/ 2\n    return total_loss","c61c63df":"class Model(pl.LightningModule):\n    \n    def __init__(self, fold):\n        super(Model, self).__init__()\n        self.config = args\n        self.model_config = AutoConfig.from_pretrained(self.config.config_name)\n        self.model = AutoModel.from_pretrained(self.config.model_name_or_path, config=self.model_config)\n        self.qa_outputs = nn.Linear(self.model_config.hidden_size, 2)\n        self.dropout = nn.Dropout(self.model_config.hidden_dropout_prob)\n        self._init_weights(self.qa_outputs)\n        \n        train_dataloader, val_dataloader= make_loader(fold)\n        \n        self._train_dataloader = train_dataloader\n        self._val_dataloader = val_dataloader\n        self.all_targets=[]\n        self.all_preds=[]\n        self.train_loss=0\n        self.val_loss=0\n        self.t_data_size=0\n        self.v_data_size=0\n        self.automatic_optimization = True\n        \n    \n    \n    def forward(self, input_ids, attention_mask ):\n        '''\n            The forward step performs the next step for the model while training. \n        '''\n    \n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n        )\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]\n        \n        # sequence_output = self.dropout(sequence_output)\n        qa_logits = self.qa_outputs(sequence_output)\n        \n        start_logits, end_logits = qa_logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits\n        \n    \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n\n        \n    def configure_optimizers(self):\n        param_optimizer = list(self.model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.weight\",]\n        optimizer_grouped_parameters = [\n                {\n                    \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n                    \"weight_decay_rate\": self.config.weight_decay\n                    },\n                {\n                    \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n                    \"weight_decay_rate\": 0.0\n                    },\n                ]\n        optimizer = AdamW(\n                optimizer_grouped_parameters,\n                lr=self.config.learning_rate,\n                eps = self.config.epsilon,\n                correct_bias=True\n                )\n        \n\n        # Defining LR Scheduler\n        \n        \n        self.num_training_steps= len(self._train_dataloader)*self.config.epochs\n        self.num_warmup_steps= self.num_training_steps * self.config.warmup_ratio\n        \n        scheduler = get_linear_schedule_with_warmup( optimizer, \\\n                            num_warmup_steps = self.num_warmup_steps, num_training_steps= self.num_training_steps)\n        \n        \n        self.scheduler=scheduler\n        self.optimizer=optimizer\n        \n        return {\n            'optimizer': self.optimizer,\n            'lr_scheduler': {\n                'scheduler': self.scheduler,\n                'monitor': 'val_loss',\n            }\n        }\n    \n    \n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        \n        '''\n            NOTE: Pass the data to self(param1, param2) instead of self.model(param1, param2) as this allows data to return in tensor format to the loss function, instead of strings. \n            This was a critical point, as without this, there were continuous errors to get the loss function working. \n        '''\n        outputs_start, outputs_end = self(\n                input_ids,\n                attention_mask=attention_mask\n                )\n        \n       \n        \n        loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n    \n        \n        self.train_loss +=(loss.item() * input_ids.size(0))\n        self.t_data_size+= input_ids.size(0)\n        \n        epoch_loss=self.train_loss\/self.t_data_size\n        self.log('train_loss', epoch_loss, on_epoch=True, prog_bar=True, logger=True)\n        tqdm_dict={'train_loss':loss}\n        self.scheduler.step()\n        self.optimizer.zero_grad()\n        \n        output = OrderedDict({\n            \"loss\": loss,\n            \"progress_bar\": tqdm_dict,\n            \"log\": tqdm_dict\n            })\n\n        return output\n    \n    \n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        attention_mask = batch[\"attention_mask\"]\n        targets_start = batch[\"start_position\"]\n        targets_end = batch['end_position']\n        \n        #Note: Same as training step, pass to self instead of self.model\n        outputs_start, outputs_end = self(\n                input_ids,\n                attention_mask=attention_mask\n                )\n        \n        loss = loss_fn((outputs_start, outputs_end), (targets_start, targets_end))\n        \n        self.val_loss +=(loss.item() * input_ids.size(0))\n        self.v_data_size+=input_ids.size(0)\n        \n        epoch_loss=self.val_loss\/self.v_data_size\n        \n        logs = {\n            \"val_loss\": epoch_loss,\n        }\n\n        \n        self.log_dict(logs, on_epoch=True, prog_bar=True, logger=True)\n        \n        output = OrderedDict({\n            \"val_loss\": loss,\n            })\n        return output\n    \n    def validation_end(self, outputs):\n        val_loss = sum([out[\"val_loss\"] for out in outputs]) \/ len(outputs)\n        result = {\"progress_bar\": tqdm_dict, \"log\": tqdm_dict, \"val_loss\": val_loss}\n        return result\n   \n    def train_dataloader(self):\n        return self._train_dataloader\n\n    def val_dataloader(self):\n        return self._val_dataloader\n","f54544b8":"lr_monitor = LearningRateMonitor(logging_interval='step')\n# Checkpoint\ncheckpoint_callback = ModelCheckpoint(monitor='val_loss',\n                                      save_top_k=1,\n                                      save_last=True,\n                                      save_weights_only=True,\n                                      filename='checkpoint\/{epoch:02d}-{val_loss:.4f}',\n                                      verbose=False,\n                                      mode='min')\n\n# Earlystopping\nearlystopping = EarlyStopping(monitor='val_loss', patience=5, mode='min')","935dd9d6":"#fold1 train\nfor fold in range(1):\n    print();print()\n    print('-'*50)\n    print(f'FOLD: {fold}')\n   \n    model = Model(fold)\n    # instrument experiment with W&B\n    wandb_logger = WandbLogger(project='Chaii_QA', log_model='all',job_type='train')\n    trainer = pl.Trainer(logger=wandb_logger,max_epochs=args.epochs,callbacks=[earlystopping,lr_monitor, checkpoint_callback],checkpoint_callback=True, gpus=1, accumulate_grad_batches=2)\n    # log gradients and model topology\n    wandb_logger.watch(model)\n    trainer.fit(model)\n    wandb.finish()\n    print('-'*50)","3202b855":"![image.png](attachment:05df7b33-4106-446d-9cd4-808b50c3b8ba.png)","39b25726":"![image.jpg](data:image\/png;base64,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)","26cf6830":"# Dataset class and Dataloader","d449b663":"# W&B Outputs","ef205474":"# Preparing Data: ","ba3138b3":"If you found this useful, please consider upvoting. Drop a comment if there are some potential tricks to optimise the model better. Thanks a lot! ","e9ab7602":"All of this training was done with XLM-Roberta-Base, as that fit easily into the GPU reuirements for Kaggle. This notebook was trained on just one fold to showcase the way a pytorch lighning framework can be used for the quesiton answering task. The same can also be modified to use for any particular task need. \n\n","e5679cff":"# Installing and Importing Libraries","15bb6fb5":"# Loading data","987a27f3":"# Defining the model with pytorch lightning\n","2709918a":"Since we are also exploring Wandb, initializing that","304a68f9":"Over the course of the month, I have learnt a lot about transformers and pytorch. During one such lesson, I stumbled across Pytorch-lightning and how it can create a general framework for the pytorch deep learning model that we are building. \nKeeping that in mind, I set across learning about how to structure regular pytorch code into lightning code. This is one such attempt at that, with WanDB to showcase the ML-OPS part of the training. ","d9a9da15":"# Introduction","894aa0cc":"Also, over the course of a lot of learning, I realised that most of the tutorials that can be found easily on the web generally use a prepackaged model such as BertSequenceClassification, this made it difficult to figure out where the _forward_ step should go, or where the linear layers should have gone. Overcoming that obstacle was the main challenge of this notebook, along with finding a way to fit the model on Kaggle's less optimal memory provision. \n\nThus, this notebook highlights upon: \n1. Use of custom model (XLM Roberta) with pytorch lightning. \n2. Weights and Biases to showcase the ML-OPS\n\n\n","55dd9553":"# Note: \nThe model building aspect was taken from: \n- https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit\n\nGive Torch an upvote for his tremendous baseline code, as that allowed me to write the code for this. ","b3c62d05":"The index of necessary functions in order: \n- **create_folds**: Kfold data prep\n- **fix_all_seeds**: Allows us to reproduce it: Code from: https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit\n- **optimal_num_of_loader_workers**: Find the optimal number of workers based on config. Code from: https:\/\/www.kaggle.com\/rhtsingh\/chaii-qa-5-fold-xlmroberta-torch-fit","f9c44a69":"# Defining Configuration","e5292e2a":"Installing pytorch lightning and torch to a newer version as torch 1.7.0 has an error while training with lightning. ","0ab718ef":"# Necessary Functions","39414798":"See the report at: https:\/\/wandb.ai\/sralli\/Chaii_QA\/reports\/Chaii-QA-W-B---VmlldzoxMDA1OTUz?accessToken=5f30ug11tuv0ze1w2zccmya0izaalhjd3a1hs7vdzq5bt4b6auakrqospelbo2rn\n\n","925c9226":"# Run Model with Pytorch lightning and evaluate with WanDB"}}