{"cell_type":{"fbb4262f":"code","417b7618":"code","495b5c73":"code","692ce65a":"code","31d4be3b":"code","c06a17bf":"code","a6ce7751":"code","ef5a70d6":"code","a5cf1988":"code","6962d854":"code","6bc45d08":"code","20ecee39":"markdown"},"source":{"fbb4262f":"from pathlib import Path\n\ndata_dir = Path('\/kaggle\/input\/coleridgeinitiative-show-us-the-data')\ntest_dir = data_dir\/'test'\n\nimport json\n\ndef get_document_text(filename, test=False):\n    if test:\n        filepath = test_dir\/(filename+'.json')\n    else:\n        filepath = train_dir\/(filename+'.json')\n        \n    with open(filepath, 'r') as f:\n        return \" \".join([_['text'] for _ in json.load(f)])\n    return \"\"","417b7618":"import re\nalphabets= \"([A-Za-z])\"\nprefixes = re.compile(\"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt)[.]\")\nsuffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\nstarters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\nacronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\nwebsites = re.compile(\"[.](co|net|org|io|gov|edu|us)\")\netal = re.compile(r\"(\\bet al)[.]\")\nurls = re.compile(\"(www)[.]\")\ndigits =  re.compile(\"[.]([0-9])\")\n\ndef split_into_sentences(text):\n    text = \" \" + text + \"  \"\n    text = text.replace(\"\\n\",\" \")\n    text = prefixes.sub(\"\\\\1<prd>\",text)\n    text = websites.sub(\"<prd>\\\\1\",text)\n    text = urls.sub(\"\\\\1<prd>\",text)\n    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n    text = etal.sub(\"\\\\1<prd>\", text)\n    text = digits.sub(\"<prd>\\\\1\",text)\n    if \"\u201d\" in text: text = text.replace(\".\u201d\",\"\u201d.\")\n    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n    text = text.replace(\".\",\".<stop>\")\n    text = text.replace(\"?\",\"?<stop>\")\n    text = text.replace(\"!\",\"!<stop>\")\n    text = text.replace(\"<prd>\",\".\")\n    sentences = text.split(\"<stop>\")\n    if sentences[-1] == '':\n        sentences = sentences[:-1]\n    sentences = [s.strip() for s in sentences]\n    return sentences\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower().strip())\n\ndef shorten_sentences(sentences, max_length=80, overlap=25):\n    \"\"\"\n    If a sentence is longer than `max_length`, break it into chunks of \n    length `max_length` with an overlap of length `overlap`.\n    \n    e.g. if the sentence has 50 tokens, max_length is 20, and overlap is 10.\n    Then the first sentence will be token_i where i in [0,20)\n    Second sentence will be token_i in [10,30).\n    Third sentence [20, 40)\n    Fourth [30, 50)\n    Fifth [40, 60)\n    \"\"\"\n    shortened_sentences = []\n    for sentence in sentences:\n        words = sentence.split()\n        num_words = len(words)\n        if num_words > max_length:\n            for start_index in range(0, num_words, max_length - overlap):\n                shortened_sentences.append(' '.join(words[start_index:start_index+max_length]))\n        else:\n            shortened_sentences.append(sentence)\n    return shortened_sentences","495b5c73":"# regex_match_sentences = [] # list of list of sentences found through regex, each element has a list of sentences, one element per id\n# ids = []\n\n\n# keywords = ['longitudinal', \"national\", \"data\", \"model\", \"questionnaire\", \"from\",  \"according\", \"\\buse\\b\", \"\\busing\", \"participants\", \"cohort\", \"studies\", \"study\", \"survey\", \"sample\", \"results\"]\n# keyword_pattern = re.compile(r\"|\".join(keywords))\n\n# for filename in test_dir.iterdir():\n#     file_id = filename.stem\n#     ids.append(file_id)\n#     document_text = get_document_text(file_id, test=True)\n#     sentences = split_into_sentences(document_text)\n    \n#     file_sentences = [sentence for sentence in sentences if keyword_pattern.search(sentence.lower())]            \n    \n#     regex_match_sentences.append(file_sentences)\n\nimport fasttext\n\nft_model = fasttext.load_model(\"..\/input\/coleridge-fasttext-classification\/fasttext_model_coleridge.bin\")\n\nfound_sentences = [] #sentences to be ran through NER model later, one element for each file id\nids = []\n\nfor filename in test_dir.iterdir():\n    file_id = filename.stem\n    ids.append(file_id)\n    document_text = get_document_text(file_id, test=True)\n    sentences = split_into_sentences(document_text)\n    \n    file_sentences = []\n    for sentence in sentences:\n        result = ft_model.predict(sentence.lower())\n        if \"has_dataset\" in result[0][0]:\n            file_sentences.append(sentence)\n    \n    found_sentences.append(file_sentences)","692ce65a":"# import torch\n# from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n\n# classifier_approved_sentences = []\n\n# batch_size = 8\n# tokenizer_path = \"..\/input\/roberta-tokenizer\"\n# text_classifier_model_path = '..\/input\/coleridge-text-class-robertalarge\/output-roberta-large'\n\n# config = AutoConfig.from_pretrained(text_classifier_model_path)\n# tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, config=config)\n# model = AutoModelForSequenceClassification.from_pretrained(text_classifier_model_path)\n\n# model.to(\"cuda\")\n# model.eval()\n\n# with torch.no_grad():\n\n#     for sentences in found_sentences:\n#         file_sentences = []\n#         for i in range(0, len(sentences), batch_size):\n#             batch = sentences[i:i+batch_size]\n#             inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n#             input_ids = inputs[\"input_ids\"].to(\"cuda\")\n#             attention_mask = inputs[\"attention_mask\"].to(\"cuda\")\n#             outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n#             logits = outputs.logits\n# #             probas = logits.softmax(-1).tolist()\n#             predictions = logits.argmax(-1).tolist()\n#             for offset, prediction in enumerate(predictions):\n#                 if prediction == 1:\n#                     file_sentences.append(sentences[i+offset])\n#         classifier_approved_sentences.append(file_sentences)","31d4be3b":"import gc\n\ndel ft_model\n# del found_sentences\n# del tokenizer\n# del config\n\ngc.collect()","c06a17bf":"from transformers import pipeline\n\nelectra_model_path = \"..\/input\/electra-squad2\/electra_squad2\"\n\nmodel = pipeline(\"question-answering\", model=electra_model_path, tokenizer=electra_model_path, device=0)","a6ce7751":"all_predictions = []\n\nquestion_keywords = [\"survey\", \"study\", \"questionnaire\"]\nquestion_template = \"What is the name of the {keyword} being used?\"\n\n\nfor sentences in found_sentences:\n    file_predictions = []\n    unq_preds = set()\n    for sentence in sentences:\n        output = \"\"\n        for keyword in question_keywords:\n            if re.search(keyword, sentence.lower()):\n                output = model(question=question_template.format(keyword=keyword), context=sentence)\n                break\n        if output == \"\":\n            output = model(question=question_template.format(keyword=\"data source\"), context=sentence)     \n        if output[\"score\"] > 0.75:\n            answer = output[\"answer\"]\n            if clean_text(answer) not in unq_preds:\n                file_predictions.append(answer)\n                unq_preds.add(clean_text(answer))\n        \n    all_predictions.append(file_predictions)","ef5a70d6":"del model\ngc.collect()","a5cf1988":"prediction_strings = []\n\nfor file_predictions in all_predictions:\n    temp_predictions = []\n    for pred in file_predictions:\n        words = pred.split()\n        if len(words) == 1 and words[0].isupper():\n            temp_predictions.append(clean_text(pred))\n        else:\n            try:\n                if words[0][0].islower() and words[1][0].islower():\n                    continue\n            except IndexError:\n                pass\n            if \"et al.\" in pred:\n                continue\n            if pred.islower():\n                continue\n            temp_predictions.append(clean_text(pred))\n    \n    prediction_strings.append(\"|\".join(temp_predictions))\n","6962d854":"# prediction_strings = [\"|\".join(x) for x in all_predictions]","6bc45d08":"import pandas as pd\nsubmission_df = pd.DataFrame(data={\"Id\":ids, \"PredictionString\":prediction_strings})\n\nsubmission_df.to_csv(\"submission.csv\", index=False)\nsubmission_df","20ecee39":"# Basic approach using regex and an electra model trained on squad2"}}