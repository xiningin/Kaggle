{"cell_type":{"574101db":"code","34055342":"code","7af9f56e":"code","4c051771":"code","9b7249ac":"code","9af5d076":"code","be2b46c3":"markdown","688d8b06":"markdown","a50d5289":"markdown","4efef1e2":"markdown","4c80e712":"markdown","cbf375c0":"markdown","9f06eba8":"markdown","979449f8":"markdown","b99600a8":"markdown"},"source":{"574101db":"!pip install BorutaShap","34055342":"!pip install scikit-learn -U","7af9f56e":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nfrom time import time\nimport pprint\nimport joblib\nfrom functools import partial\n\n# Suppressing warnings because of skopt verbosity\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Feature selection\nfrom BorutaShap import BorutaShap\nfrom xgboost import XGBRegressor\n\n# Validation\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold","4c051771":"## Loading data \nX_train = pd.read_feather('..\/input\/training-data-to-feather-python-r-low-mem\/train.feather')\nX_train = X_train.set_index(['row_id', 'time_id', 'investment_id'])\n\n# Sampling for speeding up things\nX_train = X_train.sample(n=300_000, random_state=0)\ngroups = np.array(X_train.index.get_level_values('time_id'))\n\n# target\ny_train = X_train.target\nX_train = X_train.drop('target', axis='columns')","9b7249ac":"folds = 5\n\n# Note we use a group k-fold based on time\nkf = GroupKFold(n_splits=folds)\n\nselected_columns = list()\n    \nfor k, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train, groups)):\n    \n    print(f\"FOLD {k+1}\/{folds}\")\n    \n    model = XGBRegressor(\n        n_estimators=1000,\n        max_leaves=64,\n        max_depth=8,\n        learning_rate=0.1,\n        subsample=0.8,\n        colsample_bytree=0.6,\n        reg_alpha=0.1,\n        reg_lambda=0.1,\n        random_state=0,\n        tree_method='gpu_hist',\n        gpu_id=0,\n        predictor='gpu_predictor'\n     )\n    \n    Feature_Selector = BorutaShap(model=model,\n                                  importance_measure='shap', \n                                  classification=False)\n\n    Feature_Selector.fit(X=X_train.iloc[train_idx, :], y=y_train.iloc[train_idx], \n                         n_trials=100, random_state=0)\n    \n    Feature_Selector.plot(which_features='all', figsize=(24,12))\n    \n    selected_columns.append(sorted(Feature_Selector.Subset().columns))\n    \n    print(f\"Selected features at fold {k+1} are: {selected_columns[-1]}\")","9af5d076":"final_selection = sorted({item for selection in selected_columns for item in selection})\nprint(final_selection)","be2b46c3":"#### Here we finally have the good set of features to be used in this competition (at least using XGBoost)","688d8b06":"#### Now we pick our best model and let Boruta-SHAP run a few experiments (usually 50 are enough) before getting the results.\n\n#### We cross-validate our experiments in order to ascertain that we are indeed picking the right variables\n\n#### as the results are prepared and we can plot them to visualize the Z-scores intervals of our features. That will signal us the confidence of the choice made by the algorithm in selecting or rejecting features.\n\n#### Please notice that the last two features are noisy features used by Boruta-SHAP to fgure out the important features. Clearly they are non-significant.\n\n#### Cross-validation takes time. Meanwhile we can grab a cup of coffee and relax as Boruta-SHAP is doing all the heavy-lift job.\n\n![immagine.png](attachment:8530c9d8-3db0-4a8e-b4a0-6d5e5c1cf18f.png)","a50d5289":"![immagine.png](attachment:941cf5aa-8564-475b-ac05-5e2505a85605.png)","4efef1e2":"# Feature selection with Boruta-SHAP to select meaningful features \n# for Ubiquant Market Prediction competition","4c80e712":"Gradient Boosting incorporates feature selection, since the trees spit only on significant features (or at least they should). In reality, this is not always true as sometimes noisy, irrelevant splits may appear in the tree. Moreover, working with not useful features will cause your training to go slower.\n\nGenerally, widely recognized benefits of featue selection are:\n\n* simplification of models to make them easier to interpret\n* shorter training times,\n* to avoid the curse of dimensionality,\n* more generalization by reducing overfitting (reduction of variance)","cbf375c0":"Boruta-SHAP is a package combining Boruta (https:\/\/github.com\/scikit-learn-contrib\/boruta_py), a feature selection method based on repeated tests of the importance of a feature in a model, with the interpretability method SHAP (https:\/\/christophm.github.io\/interpretable-ml-book\/shap.html).\n\nBoruta-SHAP, developed by Eoghan Keany (https:\/\/github.com\/Ekeany\/Boruta-Shap), is extremely simple to use: get your best model, let it run some time on Boruta-SHAP and evaluate the results!\n\np.s\np.s. You can read more about Boruta-SHAP on this Medium article by the author: https:\/\/medium.com\/analytics-vidhya\/is-this-the-best-feature-selection-algorithm-borutashap-8bc238aa1677","9f06eba8":"#### Let's start by uploading packages and data","979449f8":"## Happy Kaggling!","b99600a8":"#### In this competition there are quite a few of features and cases. Is there a way to eliminate the unuseful ones? \n\nFeature selection has important advantages:\n 1. by training new useful models that others don't have in their ensemble\n 2. by making your models run better"}}