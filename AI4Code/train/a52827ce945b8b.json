{"cell_type":{"730aac78":"code","2e0b9f2e":"code","0ccaf646":"code","b3da4231":"code","4732ee1c":"code","377d2777":"code","681cc8d0":"code","7729b85a":"code","8762246d":"code","3cc560bb":"code","60b85ab0":"code","9a56eca7":"code","c741acbb":"code","fa470367":"code","e63b9e70":"code","72a7216d":"markdown","95f388d8":"markdown","ead2e590":"markdown","4e747483":"markdown","8f3ad012":"markdown","d86d8288":"markdown","ae042175":"markdown","0da3f237":"markdown","dd8b74d2":"markdown","5a4ff590":"markdown","e10b787e":"markdown","4d861836":"markdown","936d4428":"markdown","97d0afe7":"markdown","bcb5c959":"markdown","1efc122e":"markdown","c852fee8":"markdown","d3998fa4":"markdown","b77f2a46":"markdown","413465d0":"markdown","83204b29":"markdown","36d4ccdf":"markdown","99b30632":"markdown"},"source":{"730aac78":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Load Data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n #Concatenate train & test\ntrain_objs_num = len(train)\ny = train['Survived']\ndataset = pd.concat(objs=[train.drop(columns=['Survived']), test], axis=0)\ntrain.head()","2e0b9f2e":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data.head()","0ccaf646":"plt.figure(figsize=(15,8))\nsns.distplot(dataset.Pclass, bins =30)","b3da4231":"dropRows = dataset\ndropRows.dropna(inplace = True)\ndropRows.isnull().sum()","4732ee1c":"df = dataset\ndf.dropna(how = 'all',inplace = True)\ndf.isnull().sum()","377d2777":"df = dataset\ndf.dropna(axis = 1,inplace = True)","681cc8d0":"df = dataset\ndf.Age.fillna(-1,inplace=True)\ndf.Age.isnull().sum()","7729b85a":"df = train\ndf.Age.isnull().sum()","8762246d":"mean = df.Age.mean()\nmean","3cc560bb":"df.Age.replace(np.NaN, mean).head(10)","60b85ab0":"df = train\ndf.Age.fillna(df.Age.median(),inplace=True)\ndf.head()","9a56eca7":"data=train\ndata['Embarked'].fillna(data['Embarked'].mode()[0], inplace=True)\ndata.head()","c741acbb":"df = train\ndf.Cabin.head(10)","fa470367":"df.Cabin.fillna('Unknown').head(10)","e63b9e70":"import pandas_profiling \ntrain.profile_report()","72a7216d":"We can drop the row if all values are missing","95f388d8":"It is clear that missing values are present, and for features (cabin, age) the percentage of null values is huge.\n\nNow we need to check if NaN values are replaced with extreme values on some features. \nHow to do that? let's take for example the pclass feature. We know from the data description that it should be one of three values {1, 2, 3}, so after plotting the histogram if we see that it has a small peak of -1 values, we can assume that missing values were replaced by organizers of the dataset.","ead2e590":"## Dealing with missing values\n\nFirstly we cannot simply ignore missing values in a dataset. We must handle them in some way for the very practical reason that most algorithms do not accept missing values, and leaving them may cause bias.\n\nThere are two recommended approaches for dealing with missing values:\n1. Drop records (or features) that have missing values.\n2. fill missing values by some other observations.\n\nSurprisingly, none of them is accurate, and each one of them has its downs.\n\n### Why?\n\nDropping missing values is sub-optimal. You may think to make our lives easier, we can try to just drop the rows with Missing Values! But doing that wouldn\u2019t be wise as it could lead to a loss of information and\/or allow some degree of bias\n\nYou shouldn\u2019t be surprised but you will find yourself always on the offensive to deal with missing values in a dataset. More often than not, Missing Values represent major bits & pieces of information, so most of the time it is risky to just drop it!\n\nFilling missing values is sub-optimal because the value was originally missing but you filled it in, which always leads to a loss in information, no matter how sophisticated our filling method is.\n\nAs such it\u2019s necessary to know the category of these missing values, which can be:\n\n1. **Missing Completely At Random (MCAR):** The fact that any missing observation is completely unrelated to the values of the data for the other variables or the non-missing data elements in the variable for the missing data. I.e. the missing elements in the data matrix X are located completely at random. The statistical advantage of data that are MCAR is that the analysis remains unbiased, and the estimated parameters are not biased by the absence of the data. \n\n2. **Missing At Random (MAR):** where the probability distribution of the pattern of missingness is functionally dependent upon the observable component in the record. MCAR is a special case of MAR. Example: you have a question on a survey asking if the survey participant is a drug addict and another question that asks if the survey participant has less than one alcoholic drink per year. Assume the answer to the alcoholic drink question is always observable, then the probability that someone fails to answer the drug addict question is most likely functionally dependent upon their answer to the alcoholic drink question.\n\n3. **Not Missing At Random (NMAR):** E.g. if students skipped a question in a questionnaire where they were asked to tell whether or not they used drugs because they feared that they would be expelled from school.\n\nIf the data records are MCAR Then you can delete records with missing data.\n","4e747483":"Here we drop a row with any missing value. \nHere we lose valuable info about the other features which are perfectly filled and informative.","8f3ad012":"2. **Replacing With Mean\/Median\/Mode:**\nThis method can apply to features that contain numerical data (like age). We can calculate Mean, Median or Mode of all values of the feature and then replace NaN values on this feature with the calculated value. It is an approximation that will add variance to the dataset, but it yields better results compared to removing data.","d86d8288":"Let\u2019s get started.\n\nFirst let us load the data ","ae042175":"### Median:\nSuitable if there are outliers","0da3f237":"### Mean:\nSuitable if there are no outliers","dd8b74d2":"## Approach 1: drop data\nIf we have enough samples in the dataset, you can drop a row if it has a missing value for a feature, or drop a feature if it has more than 70% of it missing values. ","5a4ff590":"### Pros:\n* This is a better approach when the data size is small\n* It can prevent data loss which results in removal of the rows and columns\n\n### Cons:\n* Imputing the approximations add variance and bias","e10b787e":"3. **Assign null values to unique categories:** A categorical feature will have a definite number of possibilities, such as gender, for example. Since they have a definite number of classes, we can assign another class for the missing values. It can be something like \"Unknown\"","4d861836":"## PandasProfiling\nPython package to help understand data quickly","936d4428":"## Reasons for missing data\n\n| Reasons for missing Data |  \n|--|\n|Data doesn't exist  |  \n|Data not collected due to human error.  |  \n|Data deleted accidently  |  \n|Corrupt data  |\n|Noisy, and inconsistent  |","97d0afe7":"## Why is it important?\n\n### Missing\/false data present various problems. \n1. The absence of data reduces statistical power. \n2. The false data can cause bias in the estimation of parameters. \n3. It can reduce the representativeness of the samples. \n4. It may complicate the analysis of the study. \n\nEach of these distortions may threaten the validity of the trials and can lead to invalid conclusions.","bcb5c959":"# Useful tools:","1efc122e":"# Data Cleaning Techniques - Titanic Dataset\n\n## What is Data Cleaning?\nData cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a recordset, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. [[Wikipedia]](https:\/\/en.wikipedia.org\/wiki\/Data_cleansing)\n\n\nData cleaning is not simply about erasing information to make space for new data, but rather finding a way to maximize a data set\u2019s accuracy without necessarily deleting information. For one, data cleaning includes more actions than removing data, such as fixing spelling and syntax errors, standardizing data sets, and correcting mistakes such as empty fields, missing codes, and identifying duplicate data points. Data cleaning is considered a foundational element of the data science basics, as it plays an important role in the analytical process and uncovering reliable answers.","c852fee8":"as we see on the figure above it has no outliers.","d3998fa4":"**Cons of Approach 1 :** We lose info and it works poorly if the percentage of the missing data is high.","b77f2a46":"## Approach 2: filling missing data\n\n1. **Constant value:** \nWe can replace missing values with an outlier like -999, -1, etc. It is useful because it will group the NaN values as a separate group represented with a constant value. But the downside of this method that the performance of the linear models will suffer ","413465d0":"# Summary:\n\n1. The choice of the method to fill NaN values depends on the situation.\n2. Usual ways to deal with missing values is to replace them with mean, median, mode, or outliers.\n3. Missing values could be already replaced by something by organizers. You can ensure this by plotting histograms to see outliers.\n4. Different ways of handling missing data have different effects on the performance of models.\n5. It is useful to use tools for a quicker view of missing values like Profiling.","83204b29":"### Mode:\nSuitable for categorical features","36d4ccdf":"Drop feature that has missing values","99b30632":"Due to the different data types, missing values could look like NaN values, empty strings, null, or outliers like minus 999. Sometimes they can contain useful information by themselves, like what was the reason for missing value occurring here? How to use them effectively?\n\nSo because the missing value can be replaced by some other value other than NaN, they can be hidden from us. Therefore, it is always beneficial to plot a histogram to identify those values.\n\nFirst, let's see the null values."}}