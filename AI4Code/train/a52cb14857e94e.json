{"cell_type":{"3320d054":"code","645048a6":"code","fb6f478b":"code","a44358ea":"code","196d26fc":"code","37da23f4":"code","84558bf7":"code","43815af8":"code","3a6116ce":"code","ef1623c2":"code","e42d2729":"code","dc163212":"code","39236127":"code","e656c496":"code","9a27cc70":"code","f3f76812":"code","44b926b4":"code","99369c9f":"code","6a9cf3b4":"code","4301d30b":"code","2b2cd692":"code","a9f9fd9e":"code","d5337973":"code","020185cf":"code","46f3204b":"code","ac7c4cfd":"code","1d13df71":"code","4d16711c":"code","07f55227":"code","cd384e8f":"code","93dc4077":"code","392c498c":"code","871fe19a":"code","9b59f7f3":"code","df34a804":"code","db905eae":"code","c6562862":"code","c52fad1e":"code","936c24a6":"code","c672f5c9":"code","e38248e8":"code","ef22e014":"code","5f93c7ac":"code","0fedb4b9":"code","60964ba5":"code","3a49b30a":"code","431d156d":"code","d1392b81":"code","14db5d40":"code","05ab4013":"code","59585af6":"code","61334124":"code","19117c25":"code","c25f8ee5":"code","037b4863":"code","773e0d0d":"code","55d564a4":"code","f9d01f09":"code","c4001d0e":"code","5d503953":"code","25779acb":"code","6b9c9cc2":"code","2822b1b7":"code","66564b8f":"code","0f513d36":"code","0741d5c7":"code","178264f1":"code","cb3c9399":"code","70afa370":"code","7e42dc92":"code","2d377acc":"code","55897422":"code","adf72f3d":"code","fa14991e":"code","28789dc2":"code","4e23f61f":"code","be87c390":"code","7a4c3381":"code","b449373c":"code","70ab5531":"code","e3550948":"code","81c686d3":"code","7fbf7970":"markdown","ab02c530":"markdown","8da37e9f":"markdown","1126d626":"markdown","af06e536":"markdown","f9b0d87c":"markdown","15c3e822":"markdown","7ddb6137":"markdown","43560fff":"markdown","75de428b":"markdown","54223e37":"markdown","e282d311":"markdown","42f7b47e":"markdown","3cd5d0da":"markdown","a811fbf3":"markdown","fcdd5b03":"markdown","ad64e547":"markdown","8b1a76ad":"markdown","745ea4d2":"markdown","8d409974":"markdown","13f1dd85":"markdown","139bb9b8":"markdown","9c44f11d":"markdown","7bd48dbd":"markdown","8cd4c4cd":"markdown","144b706b":"markdown","0609fce9":"markdown","fc37b841":"markdown","568dba7e":"markdown","ab63dc5f":"markdown","7f60a14b":"markdown","38727684":"markdown","04a529eb":"markdown","6bc1765e":"markdown"},"source":{"3320d054":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_columns', 500)\nimport plotly.offline as py\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly import tools\n\nfrom math import sqrt\nfrom numpy import concatenate\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.metrics.pairwise import euclidean_distances\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\nfrom sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n\n# multivariate output stacked lstm example\nfrom numpy import array\nfrom numpy import hstack\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau","645048a6":"demo = pd.read_csv('..\/input\/train_OwBvO8W\/demographics.csv')\nprint(demo.shape)\ndemo.head()","fb6f478b":"event = pd.read_csv('..\/input\/train_OwBvO8W\/event_calendar.csv')\nprint(event.shape)\nevent['YearMonth']  = pd.to_datetime(event['YearMonth'],format='%Y%m')\nevent.head()","a44358ea":"historical = pd.read_csv('..\/input\/train_OwBvO8W\/historical_volume.csv')\nprint(historical.shape)\nhistorical['YearMonth'] = pd.to_datetime(historical['YearMonth'],format='%Y%m')\nhistorical.head()","196d26fc":"soda = pd.read_csv('..\/input\/train_OwBvO8W\/industry_soda_sales.csv')\nprint(soda.shape)\nsoda['YearMonth'] = pd.to_datetime(soda['YearMonth'],format='%Y%m')\nsoda.head()","37da23f4":"industry = pd.read_csv('..\/input\/train_OwBvO8W\/industry_volume.csv')\nprint(industry.shape)\nindustry['YearMonth'] = pd.to_datetime(industry['YearMonth'],format='%Y%m')\nindustry.head()","84558bf7":"price = pd.read_csv('..\/input\/train_OwBvO8W\/price_sales_promotion.csv')\nprint(price.shape)\nprice['YearMonth'] = pd.to_datetime(price['YearMonth'],format='%Y%m')\nprice.head()","43815af8":"weather = pd.read_csv('..\/input\/train_OwBvO8W\/weather.csv')\nprint(weather.shape)\nweather['YearMonth'] = pd.to_datetime(weather['YearMonth'],format='%Y%m')\nweather.head()","3a6116ce":"sku = historical.merge(price,on=['Agency','SKU','YearMonth'],how='left')\nsku = sku.merge(soda,on=['YearMonth'],how='left')\nsku = sku.merge(industry,on='YearMonth',how='left')\nsku = sku.merge(event,on=['YearMonth'],how='left')\nprint(sku.shape)\nsku.head()","ef1623c2":"sku.describe()","e42d2729":"agency = weather.merge(demo,on=['Agency'],how='left')\nprint(agency.shape)\nagency.head()","dc163212":"agency.describe()","39236127":"df = sku.merge(agency,on=['YearMonth','Agency'],how='left')\nprint(df.shape)\ndf.head()","e656c496":"df = pd.get_dummies(df, columns= ['SKU'], dummy_na= False)\ndf.head()","9a27cc70":"df.describe()","f3f76812":"train_df = df.drop(columns=['Price','Sales','Promotions'])\ntrain_df.set_index('YearMonth',inplace=True)\ntrain_df.head()","44b926b4":"test = pd.read_csv('..\/input\/test_8uviCCm\/volume_forecast.csv')\nprint(test.shape)\ntest.head()","99369c9f":"n_date = len(train_df.index.unique())\ntes4 = pd.date_range(start='1\/1\/2013', end='31\/12\/2017',freq='M')\ntes3 = list(tes4)*len(test)\ntes1 = list(test.Agency)*len(tes4)\ntes2 = list(test.SKU)*len(tes4)","6a9cf3b4":"test_df = pd.DataFrame({'Agency':tes1,'SKU':tes2,'Volume':np.nan})\ntest_df.sort_values(['Agency','SKU'],inplace=True,ascending=False)\ntest_df.reset_index(inplace=True,drop=True)\ntest_df.loc[:,'YearMonth'] = tes3\ntest_df['YearMonth'] = test_df['YearMonth'].dt.floor('d') - pd.offsets.MonthBegin(1)\nprint(test_df.shape)\ntest_df.head()","4301d30b":"test_df = test_df.merge(weather,on=['YearMonth','Agency'],how='left')\ntest_df = test_df.merge(demo,on='Agency',how='left')\ntest_df = test_df.merge(industry,on='YearMonth',how='left')\ntest_df = test_df.merge(soda,on=['YearMonth'],how='left')\ntest_df = test_df.merge(event,on=['YearMonth'],how='left')\ntest_df = pd.get_dummies(test_df, columns= ['SKU'], dummy_na= False)\ntest_df.set_index('YearMonth',inplace=True)\ntest_df = test_df[train_df.columns]\nprint(test_df.shape)\ntest_df.head()","2b2cd692":"tes = ['Agency_06','Agency_14']*len(price.SKU.unique())*len(tes4)\ntes.sort()\ntes5 = list(price.SKU.unique())*2*len(tes4)\ndf_agen = pd.DataFrame({'Agency':tes,'SKU':tes5,'YearMonth':np.NaN,'Volume':np.NaN})\ndf_agen.sort_values(['Agency','SKU'],inplace=True)\ndf_agen.loc[:,'YearMonth'] = list(tes4)*2*25\ndf_agen.loc[:,'YearMonth'] = df_agen.loc[:,'YearMonth'].dt.floor('d') - pd.offsets.MonthBegin(1)\nprint(df_agen.shape)\ndf_agen.head()","a9f9fd9e":"df_agen = df_agen.merge(weather,on=['YearMonth','Agency'],how='left')\ndf_agen = df_agen.merge(demo,on='Agency',how='left')\ndf_agen = df_agen.merge(industry,on='YearMonth',how='left')\ndf_agen = df_agen.merge(soda,on=['YearMonth'],how='left')\ndf_agen = df_agen.merge(event,on=['YearMonth'],how='left')\ndf_agen = pd.get_dummies(df_agen, columns= ['SKU'], dummy_na= False)\ndf_agen.set_index('YearMonth',inplace=True)\ndf_agen = df_agen[train_df.columns]\nprint(df_agen.shape)\ndf_agen.head()","d5337973":"# Function to calculate missing values by column# Funct \ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","020185cf":"# Missing values statistics\nmissing_values = missing_values_table(train_df)","46f3204b":"# Missing values statistics\nmissing_values = missing_values_table(test_df)\nmissing_values.head()","ac7c4cfd":"corr = train_df[train_df.columns[:18]].corr()\nplt.figure(figsize=(12,12))\nsns.heatmap(corr,vmin=-1,cmap='coolwarm', annot=True, fmt = \".2f\")","1d13df71":"train_df.drop(columns=['FIFA U-17 World Cup','Football Gold Cup'],inplace=True)\ntest_df.drop(columns=['FIFA U-17 World Cup','Football Gold Cup'],inplace=True)\ndf_agen.drop(columns=['FIFA U-17 World Cup','Football Gold Cup'],inplace=True)","4d16711c":"x_call = train_df.columns[2:]\nX = train_df[x_call]\ny = train_df['Volume']","07f55227":"std_call = ['Soda_Volume','Industry_Volume','Avg_Max_Temp','Avg_Population_2017','Avg_Yearly_Household_Income_2017']\nscaller = StandardScaler()\nstd = pd.DataFrame(scaller.fit_transform(X[std_call]),columns=std_call)\nstd_test = pd.DataFrame(scaller.transform(test_df[std_call]),columns=std_call)\nstd_agen = pd.DataFrame(scaller.transform(df_agen[std_call]),columns=std_call)","cd384e8f":"X_std = X.copy()\nX_std.loc[:,std_call] = std.values\ntest_df_std = test_df.copy() \ndf_agen_std = df_agen.copy()\ntest_df_std.loc[:,std_call] = std_test.values \ndf_agen_std.loc[:,std_call] = std_agen.values","93dc4077":"print(X_std.shape)\nX_std.head()","392c498c":"X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.30, random_state = 217,shuffle=True)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","871fe19a":"# Spot all methods want to be used\nmodels = []\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('LR', LinearRegression()))\nmodels.append(('GB', GradientBoostingRegressor()))\nmodels.append(('LG', LGBMRegressor()))\nmodels.append(('KN', KNeighborsRegressor()))\nmodels.append(('XG', XGBRegressor(objective='reg:squarederror')))","9b59f7f3":"results = pd.DataFrame({'Score':['fit_time', 'score_time', 'test_R_Square', 'test_MSE', 'test_MAE']})\nfor name, model in models:\n    # Spot all scorers want to be used\n    scorer = {'R_Square' : 'r2',\n              'MSE'  : 'neg_mean_squared_error',\n              'MAE' : 'neg_mean_absolute_error'}\n        \n    # Cross Validation Model\n    kfold = KFold(n_splits=5, random_state=217,shuffle=True)\n    cv_results = cross_validate(model,X_train, y_train,cv=kfold,scoring=scorer)\n    cv_results['test_R_Square'] = cv_results['test_R_Square']*100\n    cv_results['test_MSE'] = np.log(np.sqrt(np.abs(cv_results['test_MSE'])))*10\n    cv_results['test_MAE'] = np.log(np.abs(cv_results['test_MAE']))*10\n    results[name] = pd.DataFrame(cv_results).mean().values","df34a804":"results","db905eae":"model_name = ['RandomForest', 'LinearRegression', 'GradientBoosting', 'KNeighbors', 'LGBM', 'XGBRegressor']\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=results.iloc[2,1:],\n    name='R Square',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=results.iloc[3,1:],\n    name='logRMSE*10',\n    marker_color='lightsalmon'\n))\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=results.iloc[4,1:],\n    name='logMAE*10',\n    marker_color='mediumslateblue'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.layout.update(barmode='group', xaxis_tickangle=-45)\nfig.show()","c6562862":"model_rf = RandomForestRegressor()\n\nmodel_rf.fit(X_train, y_train)\n\npredictions = model_rf.predict(X_test)\nprint(\"R Square: %.3f\" % r2_score(y_test, predictions))\nprint(\"RMSE: %f\" % np.sqrt(mean_squared_error(y_test, predictions)))\nprint(\"MAE: %f\" % mean_absolute_error(y_test, predictions))","c52fad1e":"model_name  = ['Random Forest']\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=[r2_score(y_test, predictions)*100],\n    name='R Square',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=np.log([np.sqrt(mean_squared_error(y_test, predictions))])*10,\n    name='logRMSE*10',\n    marker_color='lightsalmon'\n))\nfig.add_trace(go.Bar(\n    x=model_name,\n    y=np.log([mean_absolute_error(y_test, predictions)])*10,\n    name='logMAE*10',\n    marker_color='mediumslateblue'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.layout.update(barmode='group', xaxis_tickangle=-45)\nfig.show()","936c24a6":"model_lg = LGBMRegressor()\n\nmodel_lg.fit(X_train, y_train)\n\npredictions = model_lg.predict(X_test)\nprint(\"R Square: %.3f\" % r2_score(y_test, predictions))\nprint(\"RMSE: %f\" % np.sqrt(mean_squared_error(y_test, predictions)))\nprint(\"MAE: %f\" % mean_absolute_error(y_test, predictions))","c672f5c9":"model_fix = RandomForestRegressor()\nmodel_fix.fit(X_std, y)","e38248e8":"def plot_feature_importances(df,n):\n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (12, 12))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:n]))), \n            df['importance_normalized'].head(n), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:n]))))\n    ax.set_yticklabels(df['feature'].head(n))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","ef22e014":"# Extract feature importances\nfeature_importance = model_fix.feature_importances_\nfeature_importances = pd.DataFrame({'feature': x_call, 'importance': feature_importance})","5f93c7ac":"feature_importance = plot_feature_importances(feature_importances,30)","0fedb4b9":"print(test_df.shape)\ntest_df.head()","60964ba5":"pred_test = model_fix.predict(test_df_std[x_call])\ntest_df.loc[:,'Volume'] = pred_test\nprint(test_df.shape)\ntest_df.head()","3a49b30a":"def split(x):\n    x = x.split('_')\n    return x[1]+'_'+x[2]\n\ntest_df.loc[:,'SKU'] = test_df[test_df.columns[17:]].idxmax(axis=1).apply(split).values","431d156d":"test_df.drop(columns=test_df.iloc[:,17:-1].columns,inplace=True)\ntest_df.reset_index(inplace=True)\nprint(test_df.shape)\ntest_df.head()","d1392b81":"pivot = pd.pivot_table(test_df, values='Volume', index='YearMonth', columns=['Agency','SKU'])\nprint(pivot.shape)\npivot.head()","14db5d40":"# split a multivariate sequence into samples\ndef split_sequences(sequences, n_steps):\n    X, y = list(), list()\n    for i in range(len(sequences)):\n        # find the end of this pattern\n        end_ix = i + n_steps\n        # check if we are beyond the dataset\n        if end_ix > len(sequences)-1:\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","05ab4013":"dataset = np.array(pivot)\n# choose a number of time steps\nn_steps = 12\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]","59585af6":"# split into train and validating\ntrain_X, train_y = X[:-12, :], y[:-12,:]\nval_X, val_y = X[-12:-3, :], y[-12:-3,:]\ntest_X, test_y = X[-3:, :], y[-3:,:]","61334124":"# define model\nmodel = Sequential()\nmodel.add(LSTM(1024, activation='relu', return_sequences=True, input_shape=(n_steps, n_features),recurrent_dropout=0.2))\nmodel.add(LSTM(512, activation='relu',return_sequences=True,recurrent_dropout=0.2))\nmodel.add(LSTM(256, activation='relu',return_sequences=True,recurrent_dropout=0.1))\nmodel.add(LSTM(128, activation='relu',recurrent_dropout=0.1))\nmodel.add(Dense(n_features))\nmodel.compile(optimizer='adam', loss='mse')","19117c25":"# fit model\nhistory = model.fit(train_X, train_y, epochs=712, batch_size=32, verbose=0, shuffle=False,validation_data=(val_X, val_y))","c25f8ee5":"model.summary()","037b4863":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                x=list(range(1,722)),\n                y=history.history['loss'],\n                name=\"Train\",\n                line_color='deepskyblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=list(range(1,722)),\n                y=history.history['val_loss'],\n                name=\"Test\",\n                line_color='dimgray',\n                opacity=0.8))\n\n# Use date string to set xaxis range\nfig.layout.update(title_text=\"Forecasting Score\")\nfig.show()","773e0d0d":"# demonstrate prediction\ntrain_yhat = model.predict(train_X, verbose=0)\nprint(\"RMSE of training data : %.3f\" % np.sqrt(mean_squared_error(train_y,train_yhat)))","55d564a4":"# demonstrate prediction\nval_yhat = model.predict(val_X, verbose=0)\nprint(\"RMSE of training data : %.3f\" % np.sqrt(mean_squared_error(val_y,val_yhat)))","f9d01f09":"# demonstrate prediction\ntest_yhat = model.predict(test_X, verbose=0)\nprint(\"RMSE of testing data : %.3f\" % np.sqrt(mean_squared_error(test_y,test_yhat)))","c4001d0e":"# demonstrate prediction\nx_input = X[len(X)-1,:]\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)","5d503953":"pivot.loc['2018-01-01',:] = yhat\npivot.index = pd.to_datetime(pivot.index)","25779acb":"vol_for = pd.read_csv('..\/input\/test_8uviCCm\/volume_forecast.csv')\nprint(vol_for.shape)\nvol_for.head()","6b9c9cc2":"def volume(row):\n    agen = row.Agency\n    cost = row.SKU\n    return pivot[agen,cost]['2018-01-01']\n\nvol_for['Volume'] = vol_for.apply(volume,axis=1)\nprint(vol_for.shape)","2822b1b7":"vol_for.head(20)","66564b8f":"vol_for.tail(20)","0f513d36":"df_agen.head()","0741d5c7":"pred = model_fix.predict(df_agen_std[x_call])\ndf_agen.loc[:,'Volume'] = pred\nprint(df_agen.shape)\ndf_agen.head()","178264f1":"df_agen.loc[:,'SKU'] = df_agen[df_agen.columns[17:]].idxmax(axis=1).apply(split).values\ndf_agen.drop(columns=df_agen.iloc[:,17:-1].columns,inplace=True)\ndf_agen.reset_index(inplace=True)\nprint(df_agen.shape)\ndf_agen.head()","cb3c9399":"pivot_agen = pd.pivot_table(df_agen.reset_index(), values='Volume', index='YearMonth', columns=['Agency','SKU'])\nprint(pivot_agen.shape)\npivot_agen.head()","70afa370":"dataset = np.array(pivot_agen)\n# choose a number of time steps\nn_steps = 12\n# convert into input\/output\nX, y = split_sequences(dataset, n_steps)\n# the dataset knows the number of features, e.g. 2\nn_features = X.shape[2]","7e42dc92":"# split into train and validating\ntrain_X, train_y = X[:-12, :], y[:-12,:]\nval_X, val_y = X[-12:-3, :], y[-12:-3,:]\ntest_X, test_y = X[-3:, :], y[-3:,:]","2d377acc":"# define model\nmodel = Sequential()\n# define model\nmodel = Sequential()\nmodel.add(LSTM(1024, activation='relu', return_sequences=True, input_shape=(n_steps, n_features),recurrent_dropout=0.2))\nmodel.add(LSTM(512, activation='relu',return_sequences=True,recurrent_dropout=0.2))\nmodel.add(LSTM(256, activation='relu',return_sequences=True,recurrent_dropout=0.1))\nmodel.add(LSTM(128, activation='relu',recurrent_dropout=0.1))\nmodel.add(Dense(n_features))\nmodel.compile(optimizer='adam', loss='mse')","55897422":"# fit model\nhistory = model.fit(train_X, train_y, epochs=712, batch_size=32, verbose=0, shuffle=False,validation_data=(val_X, val_y))","adf72f3d":"model.summary()","fa14991e":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                x=list(range(1,722)),\n                y=history.history['loss'],\n                name=\"Train\",\n                line_color='deepskyblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=list(range(1,722)),\n                y=history.history['val_loss'],\n                name=\"Test\",\n                line_color='dimgray',\n                opacity=0.8))\n\n# Use date string to set xaxis range\nfig.layout.update(title_text=\"Forecasting Score\")\nfig.show()","28789dc2":"# demonstrate prediction\ntrain_yhat = model.predict(train_X, verbose=0)\nprint(\"RMSE of training data : %.3f\" % np.sqrt(mean_squared_error(train_y,train_yhat)))","4e23f61f":"# demonstrate prediction\nval_yhat = model.predict(val_X, verbose=0)\nprint(\"RMSE of training data : %.3f\" % np.sqrt(mean_squared_error(val_y,val_yhat)))","be87c390":"# demonstrate prediction\ntest_yhat = model.predict(test_X, verbose=0)\nprint(\"RMSE of testing data : %.3f\" % np.sqrt(mean_squared_error(test_y,test_yhat)))","7a4c3381":"# demonstrate prediction\nx_input = X[len(X)-1,:]\nx_input = x_input.reshape((1, n_steps, n_features))\nyhat = model.predict(x_input, verbose=0)","b449373c":"pivot_agen.loc['2018-01-01',:] = yhat\npivot_agen.index = pd.to_datetime(pivot_agen.index)\npivot_agen.tail()","70ab5531":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_01'],\n                name=\"Agency_06 with SKU_01\",\n                line_color='deepskyblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_02'],\n                name=\"Agency_06 with SKU_02\",\n                line_color='darkgoldenrod',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_03'],\n                name=\"Agency_06 with SKU_03\",\n                line_color='dimgray',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_04'],\n                name=\"Agency_06 with SKU_04\",\n                line_color='aquamarine',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_05'],\n                name=\"Agency_06 with SKU_03\",\n                line_color='lightpink',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_01'],\n                name=\"Agency_14 with SKU_01\",\n                line_color='cornflowerblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_02'],\n                name=\"Agency_14 with SKU_02\",\n                line_color='lawngreen',\n                opacity=0.8))\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_04'],\n                name=\"Agency_14 with SKU_04\",\n                line_color='lightsalmon',\n                opacity=0.8))\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_05'],\n                name=\"Agency_14 with SKU_05\",\n                line_color='indianred',\n                opacity=0.8))\n\n# Use date string to set xaxis range\nfig.layout.update(title_text=\"Top Four Recommendation SKU for Agency_06 & Agency_14\")\nfig.show()","e3550948":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_02'],\n                name=\"Agency_06 with SKU_02\",\n                line_color='darkgoldenrod',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_06','SKU_01'],\n                name=\"Agency_06 with SKU_01\",\n                line_color='dimgray',\n                opacity=0.8))\n\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_01'],\n                name=\"Agency_14 with SKU_01\",\n                line_color='cornflowerblue',\n                opacity=0.8))\n\nfig.add_trace(go.Scatter(\n                x=pivot_agen.index,\n                y=pivot_agen['Agency_14','SKU_02'],\n                name=\"Agency_14 with SKU_02\",\n                line_color='lawngreen',\n                opacity=0.8))\n\n# Use date string to set xaxis range\nfig.layout.update(title_text=\"Recommendation SKU for Agency_06 & Agency_14\")\nfig.show()","81c686d3":"sku_recom = pd.read_csv('..\/input\/test_8uviCCm\/sku_recommendation.csv')\ntes = pivot_agen.loc['2018-01-01',:].reset_index()\ntes.columns = ['Agency','SKU','Volume']\ntes_1 = tes[tes.Agency=='Agency_06']\ntes_2 = tes[tes.Agency=='Agency_14']\ntes_3 = list(tes_1.loc[tes_1['Volume'].nlargest(2).index,'SKU']) + list(tes_2.loc[tes_2['Volume'].nlargest(2).index,'SKU'])\nsku_recom.SKU = tes_3\nprint(sku_recom)","7fbf7970":"By the score above, we decide Random Forest Regressor to build the final model.","ab02c530":"### Loading the needs library ","8da37e9f":"## 5.1 Build Model using LSTM","1126d626":"## 1. Introduction","af06e536":"### 2.4 Merge all dataset and preparing data training","f9b0d87c":"## 6.2 Forecast Volume Industry one month a head","15c3e822":"### 3.3 Splitting Data ","7ddb6137":"## 4. Build Model to predict Volume Industry","43560fff":"### 3.1 Check Null and Missing Values","75de428b":"# 5. Forecasting Volume Industry","54223e37":"# 6. SKU Recommendations for Agency_06 & Agency_14","e282d311":"## 6.1 Predict Volume Industry","42f7b47e":"## 5.3 Forecasting of Volume Industry for one month a head","3cd5d0da":"# Volume Forecasting : SKU future volume analysis and prediction","a811fbf3":"## 6.3 SKU Recommendations for Agency_06 and Agency_14 ","fcdd5b03":"### 2.3 Merge all features that depends on Agency","ad64e547":"### 6.2.1 Training Model ","8b1a76ad":"There is no missing values in all dataset.","745ea4d2":"## 5.2 Testing Model ","8d409974":"So, both Agency, Agency_06 and AGency_14, are recommended to handle the same SKU. Those are SKU_01 and SKU_02.","13f1dd85":"## 4.3 Feature Importance","139bb9b8":"Country Beeristan, a high potential market, accounts for nearly 10% of Stallion & Co.\u2019s global beer sales. Stallion & Co. has a large portfolio of products distributed to retailers through wholesalers (agencies). There are thousands of unique wholesaler-SKU\/products combinations. In order to plan its production and distribution as well as help wholesalers with their planning, it is important for Stallion & Co. to have an accurate estimate of demand at SKU level for each wholesaler.<br>\n\nCurrently demand is estimated by sales executives, who generally have a \u201cfeel\u201d for the market and predict the net effect of forces of supply, demand and other external factors based on past experience. The more experienced a sales exec is in a particular market, the better a job he does at estimating. Joshua, the new Head of S&OP for Stallion & Co. just took an analytics course and realized he can do the forecasts in a much more effective way. He approaches you, the best data scientist at Stallion, to transform the exercise of demand forecasting.<br>","9c44f11d":"### 2.6 Preparing SKU Recomendations Data ","7bd48dbd":"## 4.2 Training Model","8cd4c4cd":"## 2. Collection of Data","144b706b":"## 4.4. Predicting The Volume Industry","0609fce9":"### 2.5 Preparing Testing Data ","fc37b841":"## 3. Data Preparation and Data Distribution","568dba7e":"## 4.1 Choosing  Model ","ab63dc5f":"### 3.2 Standardize Data","7f60a14b":"### 2.2 Merge all features that depends on SKU","38727684":"### Data Training","04a529eb":"### 6.2.2 Testing Model ","6bc1765e":"###  2.1 Load Data"}}