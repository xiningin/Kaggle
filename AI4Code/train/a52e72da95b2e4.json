{"cell_type":{"2bc2f5bb":"code","643f36c2":"code","6ebbf50d":"code","664c1d19":"code","36843d16":"code","e874af7f":"code","f674a71e":"code","ba31448b":"code","8ecf3817":"code","61bf78db":"code","f439984f":"code","4e7d5822":"code","0938b373":"code","7c8d897e":"code","f28a2a34":"code","376f0e83":"code","798fd4dd":"code","fe99aaa3":"code","b3421250":"code","a4211bd3":"code","36265ecb":"code","091536e5":"code","a6689c6b":"code","36521456":"code","4fe5d9dc":"code","a51664a5":"code","43b73183":"markdown","264fa1ae":"markdown","10a63d91":"markdown","4362d17e":"markdown","ac95b8c4":"markdown","075c9fec":"markdown","41964eb0":"markdown","4738780d":"markdown","0a15b784":"markdown","999b872e":"markdown","bdb50bcb":"markdown","7093cd8f":"markdown","e96346e8":"markdown","895babd1":"markdown","936bdda6":"markdown","5cbfaff2":"markdown","5b4039b8":"markdown","f11f3e73":"markdown","94d780c7":"markdown"},"source":{"2bc2f5bb":"import numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n!pip install contractions\nimport contractions\nimport re\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud","643f36c2":"train_data = pd.read_csv('\/kaggle\/input\/emotions-dataset-for-nlp\/train.txt', names=['text', 'emotion'], sep=';')\nval_data = pd.read_csv('\/kaggle\/input\/emotions-dataset-for-nlp\/val.txt', names=['text', 'emotion'], sep=';')\ntest_data = pd.read_csv('\/kaggle\/input\/emotions-dataset-for-nlp\/test.txt', names=['text', 'emotion'], sep=';')\ntrain_data.head()","6ebbf50d":"data = {'Train Data': train_data, 'Validation Data': val_data, 'Test Data': test_data}\nfor temp in data:\n    print(temp)\n    print(data[temp].isnull().sum())\n    print('*'*20)","664c1d19":"bar, ax = plt.subplots(1,3, figsize=(30, 10))\nfor index, temp in enumerate(data):\n    sns.countplot(ax = ax[index],x = 'emotion', data = data[temp])\n    ax[index].set_title(temp+' Class Frequency', size=14)\n    ax[index].set_ylabel('Frequency', size=14)\n    ax[index].set_xlabel(temp+' Class', size=14)","36843d16":"def plot_cloud(wordcloud, temp):\n    plt.figure(figsize=(10, 10))\n    plt.title(temp+' Word Cloud', size = 16)\n    plt.imshow(wordcloud) \n    # No axis details\n    plt.axis(\"off\");","e874af7f":"for temp in data:\n    temp_text = ' '.join([sentence for sentence in data[temp].text])\n    wordcloud = WordCloud(width = 600, height = 600).generate(temp_text)\n    plot_cloud(wordcloud, temp)","f674a71e":"def preprocess(sentence):\n    stop_words = set(stopwords.words('english'))\n    lemmatizer = WordNetLemmatizer()\n    sentence = re.sub('[^A-z]', ' ', sentence)\n    negative = ['not', 'neither', 'nor', 'but', 'however', 'although', 'nonetheless', 'despite', 'except',\n                        'even though', 'yet']\n    stop_words = [z for z in stop_words if z not in negative]\n    preprocessed_tokens = [lemmatizer.lemmatize(contractions.fix(temp.lower())) for temp in sentence.split() if temp not in stop_words] #lemmatization\n    return ' '.join([x for x in preprocessed_tokens]).strip()\n","ba31448b":"train_data['text'] = train_data['text'].apply(lambda x: preprocess(x))\nval_data['text'] = val_data['text'].apply(lambda x: preprocess(x))\ntest_data['text'] = test_data['text'].apply(lambda x: preprocess(x))","8ecf3817":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\ntrain_x, train_y = ros.fit_resample(np.array(train_data['text']).reshape(-1, 1), np.array(train_data['emotion']).reshape(-1, 1))\ntrain = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text', 'emotion'])","61bf78db":"from sklearn import preprocessing\nle = preprocessing.OneHotEncoder()\ny_train= le.fit_transform(np.array(train['emotion']).reshape(-1, 1)).toarray()\ny_test= le.fit_transform(np.array(test_data['emotion']).reshape(-1, 1)).toarray()\ny_val= le.fit_transform(np.array(val_data['emotion']).reshape(-1, 1)).toarray()","f439984f":"from transformers import RobertaTokenizerFast\ntokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")","4e7d5822":"def roberta_encode(data,maximum_length) :\n  input_ids = []\n  attention_masks = []\n  \n\n  for i in range(len(data.text)):\n      encoded = tokenizer.encode_plus(\n        \n        data.text[i],\n        add_special_tokens=True,\n        max_length=maximum_length,\n        pad_to_max_length=True,\n        \n        return_attention_mask=True,\n        \n      )\n      \n      input_ids.append(encoded['input_ids'])\n      attention_masks.append(encoded['attention_mask'])\n  return np.array(input_ids),np.array(attention_masks)","0938b373":"max_len = max([len(x.split()) for x in train_data['text']])\ntrain_input_ids,train_attention_masks = roberta_encode(train, max_len)\ntest_input_ids,test_attention_masks = roberta_encode(test_data, max_len)\nval_input_ids,val_attention_masks = roberta_encode(val_data, max_len)","7c8d897e":"def create_model(bert_model, max_len):\n    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n\n    output = bert_model([input_ids,attention_masks])\n    output = output[1]\n\n    output = tf.keras.layers.Dense(6, activation='softmax')(output)\n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","f28a2a34":"from transformers import TFRobertaModel\nroberta_model = TFRobertaModel.from_pretrained('roberta-base')","376f0e83":"model = create_model(roberta_model, max_len)\nmodel.summary()","798fd4dd":"history = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_val), epochs=4,batch_size=100)","fe99aaa3":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","b3421250":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","a4211bd3":"result = model.predict([test_input_ids,test_attention_masks])\ny_pred = np.zeros_like(result)\ny_pred[np.arange(len(result)), result.argmax(1)] = 1","36265ecb":"from sklearn.metrics import accuracy_score, f1_score\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy ', accuracy)\nf1 = f1_score(y_test, y_pred, average = 'macro')\nprint('F1 Score :', f1)","091536e5":"# Save the weights\nmodel.save_weights('my_checkpoint')","a6689c6b":"def plot_result(result):\n    sns.barplot(x = 'Category', y = 'Confidence', data = result)\n    plt.xlabel('Categories', size=14)\n    plt.ylabel('Confidence', size=14)\n    plt.title('Emotion Classification', size=16)","36521456":"def roberta_inference_encode(data,maximum_length) :\n    input_ids = []\n    attention_masks = []\n  \n\n  \n    encoded = tokenizer.encode_plus(\n    data,\n    add_special_tokens=True,\n    max_length=maximum_length,\n    pad_to_max_length=True,\n\n    return_attention_mask=True\n\n    )\n\n    input_ids.append(encoded['input_ids'])\n    attention_masks.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_masks)","4fe5d9dc":"def inference(text_sentence, max_len):\n    preprocessed_text = preprocess(text_sentence)\n    input_ids, attention_masks = roberta_inference_encode(preprocessed_text, maximum_length = max_len)\n    model = create_model(roberta_model, 43)\n    model.load_weights('my_checkpoint')\n    result = model.predict([input_ids, attention_masks])\n    #le.categories_[0] = ['anger' 'fear' 'joy' 'love' 'sadness' 'surprise']\n    result = pd.DataFrame(dict(zip(list(le.categories_[0]), [round(x*100, 2)for x in result[0]])).items(), columns = ['Category', 'Confidence'])\n    plot_result(result)\n    return result","a51664a5":"result = inference(\"I am unhappy\", max_len)\nprint(result)","43b73183":"**How RoBERTa is better than BERT ??**\n\nChanges in Pre-Training:\n* without NSP objective \n* with dynamic mask generation\n\nChanges in Data:\n* Trained on more data (16GB BERT vs 160GB RoBERTa)\n* Trained on large batches\n\n**Note:** Pre-requistics is to go through BERT\n\nReferences:\n* Bert: https:\/\/arxiv.org\/pdf\/1810.04805.pdf\n* The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning): http:\/\/jalammar.github.io\/illustrated-bert\/\n* The Illustrated Transformer: https:\/\/jalammar.github.io\/illustrated-transformer\/\n* Attention Is All You Need: https:\/\/arxiv.org\/pdf\/1706.03762.pdf\n* Query,key,value vector: https:\/\/stats.stackexchange.com\/questions\/421935\/what-exactly-are-keys-queries-and-values-in-attention-mechanisms","264fa1ae":"# Pre-Processing","10a63d91":"# Create Model","4362d17e":"Applying OneHotEncoder on target of all dataset","ac95b8c4":"# Exploratory Data Analysis EDA","075c9fec":"There is a very helpful function called encode_plus provided in the Tokenizer class. It can seamlessly perform the following operations:\n\n* Tokenize the text and Add special tokens - [CLS] and [SEP]\n* create input IDs\n* Pad the sentences to a maximum length\n* Create attention masks for the above PAD tokens\n\n**Note:** RoBERTa uses byte-level Byte-Pair Encoding (BPE) in contrast to BERT\u2019s character-level BPE.","41964eb0":"**Preprocessing includes:**\n1. Removing stopwords (without removing negative words)\n2. Expand Contractions\n3. Lemmatization\n\n**Note:** Negative words are removed from the set of stopwords as it makes \"I am not happy\" to \"happy\" after preprocessing. In short, it can change the semantic meaning of sentence and result into wrong training.","4738780d":"**Class Distribution in Train, val and test dataset**","0a15b784":"**Note:** As class imbalanced is evident, RandomOverSampler is used to add data(repetition) to all classes except highest frequency class.","999b872e":"**Accuracy and F1 Score of Model**","bdb50bcb":"# Import Libraries","7093cd8f":"# Encoding","e96346e8":"**Plotting Accuracy and Loss (Training and Validation)**","895babd1":"# Model Training","936bdda6":"**Check for null values in train, val and test dataset**","5cbfaff2":"**RoBERTa Model prediction on Test Data**","5b4039b8":"**Word Cloud** to get most frequent words.","f11f3e73":"It is evident that dataset is highly imbalanced. \"Joy\" class has highest frequency  and 'Surprise' have least frequency in all three datasets.","94d780c7":"# Model Inference"}}