{"cell_type":{"48465ec9":"code","dbf57f1c":"code","cd587068":"code","1cd6b7ea":"code","d8b95c23":"code","c29a1cb3":"code","df37a5f4":"code","297fd696":"code","9daa6d29":"code","3bcb0ed4":"code","1a817765":"code","e55abe4e":"code","3be780ab":"code","293828b4":"code","ad9d2cda":"code","3aa713bc":"code","a8b52971":"code","031a1d06":"code","8a76786c":"code","f0302be7":"code","dc45052a":"code","b35a1d66":"code","444fb75c":"code","b6df4c20":"code","10b17a0b":"code","69fdddac":"code","18521377":"code","8ad4926d":"code","19f3119f":"code","48e0c78b":"code","b8321960":"code","02e2fb56":"code","4247791b":"code","1dc4c982":"code","6018466b":"code","0f6742cc":"code","64b619ef":"code","52929018":"code","402a945e":"code","9b65cc07":"code","9fdb12aa":"code","d3ebdd5e":"code","896c1163":"code","76eaf69c":"code","1fa60d79":"code","447c0c1b":"code","af1df26f":"code","1adb7dd9":"code","93a84b0f":"code","b87fc48d":"code","fae7b4c8":"code","cc99b1fc":"code","c5859d77":"code","d6a198b8":"code","599780f1":"code","b3e680f1":"code","c88785fd":"code","e6040783":"code","e3acb5b6":"code","244f5788":"code","031391b0":"code","300b468c":"code","b9d815a1":"markdown","de9e1183":"markdown","8a9bde4c":"markdown","6effa564":"markdown","b6ded6a7":"markdown","f5c40205":"markdown","dc5f9bac":"markdown","257b64f5":"markdown","1a466b4b":"markdown","0ac1f002":"markdown","3ccdb875":"markdown","e0a26ae8":"markdown","c1c42dc8":"markdown","192a9f7f":"markdown","0ca9f43e":"markdown","7b14a378":"markdown","14b27426":"markdown","c646bcda":"markdown","dc3814f9":"markdown","b8531eb8":"markdown","2127180b":"markdown","07fc5ab2":"markdown","39dbbfdb":"markdown","47813c49":"markdown","5af92538":"markdown","85d1cc21":"markdown","7c7626ff":"markdown","bd462dc1":"markdown","ac7fa4a9":"markdown","0e002390":"markdown","3f96d3c4":"markdown","c76eb91b":"markdown","df7e353a":"markdown","7aa87c84":"markdown","270fe9d2":"markdown","805ec84b":"markdown","86880c14":"markdown","95cffd30":"markdown","115112ee":"markdown","757bdfdb":"markdown","fe0a6fea":"markdown","e3b48535":"markdown","5bc06607":"markdown","e1af25ac":"markdown","570289be":"markdown","3e9b5f6c":"markdown","6ec0e097":"markdown","2c02d62e":"markdown","7ce1cded":"markdown","0854e81e":"markdown","6b96759c":"markdown","b047df71":"markdown","188cd845":"markdown","12928806":"markdown","74448839":"markdown","1638851d":"markdown","d9087936":"markdown","cdf61a18":"markdown","83dc437a":"markdown","e34008a8":"markdown","e0b47bac":"markdown","e85cb73a":"markdown","487c5030":"markdown","070c84d0":"markdown","16b3750b":"markdown"},"source":{"48465ec9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport seaborn as sns\nsns.set(style=\"ticks\", color_codes=True)\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dbf57f1c":"emp_gen_info = pd.read_csv(\"..\/input\/hr-analytics-case-study\/general_data.csv\")\nemp_survey = pd.read_csv(\"..\/input\/hr-analytics-case-study\/employee_survey_data.csv\")\nman_survey = pd.read_csv(\"..\/input\/hr-analytics-case-study\/manager_survey_data.csv\")\nin_time = pd.read_csv(\"..\/input\/hr-analytics-case-study\/in_time.csv\")\nout_time = pd.read_csv(\"..\/input\/hr-analytics-case-study\/out_time.csv\")","cd587068":"print(\"emp_gen_info:\",emp_gen_info.shape)\nprint(\"emp_survey:\",emp_survey.shape)\nprint(\"man_survey:\",man_survey.shape)","1cd6b7ea":"emp_gen_info.head()","d8b95c23":"emp_survey.head()","c29a1cb3":"man_survey.head()","df37a5f4":"emp_gen_info.set_index('EmployeeID', inplace=True)\nemp_survey.set_index('EmployeeID', inplace=True)\nman_survey.set_index('EmployeeID', inplace=True)","297fd696":"Employee =  pd.concat([emp_gen_info, emp_survey, man_survey], axis = 1)\nEmployee.head()","9daa6d29":"Employee.T.apply(lambda columns: columns.nunique(), axis=1)","3bcb0ed4":"Employee.drop(['EmployeeCount', 'Over18','StandardHours'], axis=1,inplace = True)","1a817765":"Employee.dtypes","e55abe4e":"from sklearn.preprocessing import LabelEncoder\nlabelEncoder_X = LabelEncoder()\nEmployee['BusinessTravel'] = labelEncoder_X.fit_transform(Employee['BusinessTravel'])\nEmployee['Department'] = labelEncoder_X.fit_transform(Employee['Department'])\nEmployee['EducationField'] = labelEncoder_X.fit_transform(Employee['EducationField'])\nEmployee['Gender'] = labelEncoder_X.fit_transform(Employee['Gender'])\nEmployee['JobRole'] = labelEncoder_X.fit_transform(Employee['JobRole'])\nEmployee['MaritalStatus'] = labelEncoder_X.fit_transform(Employee['MaritalStatus'])\nEmployee['Attrition'] = labelEncoder_X.fit_transform(Employee['Attrition'])","3be780ab":"Employee.dtypes","293828b4":"Employee.isnull().any()","ad9d2cda":"meanOfNumCompaniesWorked = Employee[\"NumCompaniesWorked\"].astype('float').mean(axis = 0 )\nEmployee[\"NumCompaniesWorked\"].replace(np.nan,meanOfNumCompaniesWorked,inplace = True)\n\nmeanOfTotalWorkingYears = Employee[\"TotalWorkingYears\"].astype('float').mean(axis = 0 )\nEmployee[\"TotalWorkingYears\"].replace(np.nan,meanOfTotalWorkingYears,inplace = True)\n\nmeanOfEnvironmentSatisfaction = round(Employee[\"EnvironmentSatisfaction\"].astype('float').mean(axis = 0 ))\nEmployee[\"EnvironmentSatisfaction\"].replace(np.nan,meanOfEnvironmentSatisfaction,inplace = True)\n\nmeanOfJobSatisfaction = round(Employee[\"JobSatisfaction\"].astype('float').mean(axis = 0 ))\nEmployee[\"JobSatisfaction\"].replace(np.nan,meanOfJobSatisfaction,inplace = True)\n\nmeanOfWorkLifeBalance = round(Employee[\"WorkLifeBalance\"].astype('float').mean(axis = 0 ))\nEmployee[\"WorkLifeBalance\"].replace(np.nan,meanOfWorkLifeBalance,inplace = True)\n","3aa713bc":"Employee.head()","a8b52971":"import datetime as dt","031a1d06":"print(\"in_time:\",in_time.shape)\nin_time.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True)\nin_time.set_index('EmployeeID', inplace=True)\nin_time.head()","8a76786c":"print(\"out_time:\",in_time.shape)\nout_time.rename(columns={'Unnamed: 0':'EmployeeID'}, inplace=True)\nout_time.set_index('EmployeeID', inplace=True)\nout_time.head()","f0302be7":"in_time_stamp = in_time.apply(pd.to_datetime) #converting into timestamp object\nout_time_stamp = out_time.apply(pd.to_datetime)\ndf_working_hours = (out_time_stamp - in_time_stamp)# time spent in the company\ndf_working_hours.head()\n","dc45052a":"df_working_time= df_working_hours \/ np.timedelta64(1, 'h') #converting time spent to float value\ndf_working_time.head()\nmean_working_hours = df_working_time.mean(axis=1)# mean time spend in company in above period\nEmployee['MeanWorkingHours'] = mean_working_hours\nemp_gen_info['MeanWorkingHours'] = mean_working_hours #** adding to the dataframe","b35a1d66":"import calendar","444fb75c":"in_time_stamp.head()\nweek_day = in_time_stamp.dropna(how='all', axis=1) # drop columns with every value as NaT as they are public holidays\nweek_day.head()\ncol_list =list( week_day.columns)#converting columns to list\ndf_col_list = pd.DataFrame(col_list)#coverting list to pandas dataframe\ndf_col_list_stamp = df_col_list.apply(pd.to_datetime)#convert df to datetime object\ndf_col_list_stamp[0] = df_col_list_stamp[0].dt.weekday # df to day of week 0 for monday and 6 for sunday\nweekday_list=list(df_col_list_stamp[0])# creating list of weekdays for renaming columns\nweek_day.columns = [weekday_list]#renaming columns\nEmployee['TotalLeave']=week_day.isna().sum(axis=1)#Adding total leaves to the employee dataframe\nemp_gen_info['TotalLeave']=week_day.isna().sum(axis=1)\nEmployee['LeaveMonFri']=week_day[[0,4]].isna().sum(axis=1) # leave on monday and friday\nemp_gen_info['LeaveMonFri']=week_day[[0,4]].isna().sum(axis=1)","b6df4c20":"Employee.hist(figsize=(30,20),grid = False);","10b17a0b":"plt.figure(figsize=(25,6))\n\nplt.subplot(1,3,1)\nsns.countplot(x='BusinessTravel', hue='Attrition', data=emp_gen_info, palette='pastel');\nplt.title('BusinessTravel')\n\nplt.subplot(1,3,2)\nsns.countplot(x='Department', hue='Attrition', data=emp_gen_info, palette='pastel');\nplt.title('Department')\n\nplt.subplot(1,3,3)\nsns.countplot(x='StockOptionLevel', hue='Attrition', data=emp_gen_info, palette='pastel');\nplt.title('StockOptionLevel')\n\nplt.show()\n","69fdddac":"plt.figure(figsize=(25,6))\n\nplt.subplot(1,3,1)\nsns.kdeplot(emp_gen_info['Age'][emp_gen_info.Attrition=='Yes'], shade=True, color='orangered')\nsns.kdeplot(emp_gen_info['Age'][emp_gen_info.Attrition=='No'], shade=True, color='royalblue')\nplt.title('Distribution Of Age', fontsize=13)\nplt.ylabel('Probability Density')\nplt.legend(['Attrition (YES)','Attrition (NO)'])\n\nplt.subplot(1,3,2)\nsns.kdeplot(emp_gen_info['DistanceFromHome'][emp_gen_info.Attrition=='Yes'], shade=True, color='orangered')\nsns.kdeplot(emp_gen_info['DistanceFromHome'][emp_gen_info.Attrition=='No'], shade=True, color='royalblue')\nplt.title('Distribution of Distance From Home', fontsize=13)\nplt.legend(['Attrition (YES)','Attrition (NO)'])\n\nplt.subplot(1,3,3)\nsns.kdeplot(emp_gen_info['TotalWorkingYears'][emp_gen_info.Attrition=='Yes'], shade=True, color='orangered')\nsns.kdeplot(emp_gen_info['TotalWorkingYears'][emp_gen_info.Attrition=='No'], shade=True, color='royalblue')\nplt.title('Distribution of Total Working Years', fontsize=13)\nplt.legend(['Attrition (YES)','Attrition (NO)'])\n\n\nplt.show()","18521377":"plt.figure(figsize=(25,6))\n\nplt.subplot(1,3,1)\nsns.violinplot(data=Employee, x='Attrition', y='JobSatisfaction', palette='pastel')\nplt.title('Job Satisfaction')\n\nplt.subplot(1,3,2)\nsns.violinplot(data=Employee, x='Attrition', y='EnvironmentSatisfaction', palette='pastel')\nplt.title('Environment Satisfaction')\n\nplt.subplot(1,3,3)\nsns.violinplot(data=Employee, x='Attrition', y='JobInvolvement', palette='pastel')\nplt.title('JobInvolvement')\n\nplt.show()","8ad4926d":"\nmatrix = np.triu(Employee.corr())\nplt.figure(figsize = (25, 15))\nsns.heatmap(Employee.corr(), annot = True, linewidth = 0.02,cmap = 'RdYlGn', mask=matrix)\nplt.show()","19f3119f":"plt.figure(figsize=(24,8))\nplt.subplot(1,3,1)\nsns.boxplot(x='Attrition', y='MeanWorkingHours', data=Employee, palette='pastel');\nplt.subplot(1,3,2)\nsns.boxplot(x='Attrition', y='Age', data=Employee, palette='pastel');\nplt.subplot(1,3,3)\nsns.boxplot(x='Attrition', y='TotalWorkingYears', data=Employee, palette='pastel');\nplt.show()\n","48e0c78b":"plt.figure(figsize=(25,6))\nsns.kdeplot(emp_gen_info['MeanWorkingHours'][emp_gen_info.Attrition=='Yes'], shade=True, color='orangered');\nsns.kdeplot(emp_gen_info['MeanWorkingHours'][emp_gen_info.Attrition=='No'], shade=True, color='royalblue');\nplt.title('Distribution Of MeanWorkingHours', fontsize=13)\nplt.ylabel('Probability Density')\nplt.legend(['Attrition (YES)','Attrition (NO)']);\n","b8321960":"from sklearn.preprocessing import StandardScaler, Normalizer\n","02e2fb56":"scaler = StandardScaler()\nscaler.fit(Employee)\nEmployee_scaled = scaler.transform(Employee)","4247791b":"from sklearn.decomposition import PCA","1dc4c982":"pca = PCA(n_components=2)\nEmployee_pca = pca.fit_transform(Employee_scaled)\nEmployee_pca.shape","6018466b":"plt.figure(figsize=(10,8))\n\nplt.scatter(Employee_pca[:,0],Employee_pca[:,1],c=Employee['Attrition']);\n","0f6742cc":"from sklearn.manifold import TSNE, Isomap\n","64b619ef":"iso = Isomap(n_components=3, n_neighbors=20)\nEmployee_iso = iso.fit_transform(Employee_scaled)\nEmployee_scaled.shape","52929018":"plt.figure(figsize=(10,8))\nplt.scatter(Employee_iso[:,0],Employee_iso[:,1],c=Employee['Attrition']);","402a945e":"from sklearn.model_selection import train_test_split","9b65cc07":"y = Employee['Attrition']\nx = Employee.drop('Attrition', axis = 1)","9fdb12aa":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix","d3ebdd5e":"X_train_lr,X_test_lr, y_train_lr, y_test_lr = train_test_split(x,y, test_size = 0.20, random_state=39)","896c1163":"X_train_lr = scaler.fit_transform(X_train_lr)\nX_test_lr = scaler.fit_transform(X_test_lr)\nprint(X_train_lr.shape,X_test_lr.shape)","76eaf69c":"LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train_lr,y_train_lr)\nLR","1fa60d79":"yhat_LR = LR.predict(X_test_lr)\nprint(yhat_LR[0:5])\nprint(y_test_lr[0:5])","447c0c1b":"from sklearn import metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import jaccard_similarity_score","af1df26f":"print(\"LogisticRegression's Accuracy: \", metrics.accuracy_score(y_test_lr,yhat_LR))\nprint(\"FI SCORE: \", f1_score(y_test_lr, yhat_LR, average='weighted') )\nprint(\"jaccard_similarity_score: \", jaccard_similarity_score(y_test_lr, yhat_LR)) ","1adb7dd9":"from sklearn.tree import DecisionTreeClassifier","93a84b0f":"X_train_dt,X_test_dt, y_train_dt, y_test_dt= train_test_split(x,y, test_size = 0.20, random_state=41)","b87fc48d":"X_train_dt = scaler.fit_transform(X_train_dt)\nX_test_dt = scaler.fit_transform(X_test_dt)\nprint(X_train_dt.shape,X_test_dt.shape)","fae7b4c8":"DT = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 10)\nDT.fit(X_train_dt,y_train_dt)","cc99b1fc":"yhat_DT = DT.predict(X_test_dt)\nprint (yhat_DT [0:5])\nprint (y_test_dt [0:5])","c5859d77":"print(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test_dt,yhat_DT))\nprint(\"FI SCORE: \", f1_score(y_test_dt, yhat_DT, average='weighted') )\nprint(\"jaccard_similarity_score: \", jaccard_similarity_score(y_test_dt, yhat_DT)) ","d6a198b8":"from sklearn.ensemble import RandomForestClassifier","599780f1":"X_train_rf,X_test_rf, y_train_rf, y_test_rf= train_test_split(x,y, test_size = 0.20, random_state=41)","b3e680f1":"X_train_rf = scaler.fit_transform(X_train_rf)\nX_test_rf = scaler.fit_transform(X_test_rf)\nprint(X_train_rf.shape,X_test_rf.shape)","c88785fd":"RF = RandomForestClassifier()\nRF.fit(X_train_rf,y_train_rf)","e6040783":"yhat_RF = RF.predict(X_test_rf)\nprint (yhat_RF [0:5])\nprint (y_test_rf [0:5])","e3acb5b6":"print(\"Random Forest's Accuracy: \", metrics.accuracy_score(y_test_rf,yhat_RF))\nprint(\"FI SCORE: \", f1_score(y_test_rf, yhat_RF, average='weighted') )\nprint(\"jaccard_similarity_score: \", jaccard_similarity_score(y_test_rf, yhat_RF)) ","244f5788":"importances = RF.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in RF.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1][:40]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train_rf.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\nplt.figure()\nfig = plt.figure(figsize=(25, 10));\nplt.title(\"Relative Feature importances\")\nplt.bar(range(X_train_rf.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train_rf.shape[1]), indices)\nplt.xlim([-1, X_train_rf.shape[1]])\nplt.show();","031391b0":"plt.figure(figsize=(25,6))\nplt.subplot(1,3,1)\nsns.boxplot(x='Attrition', y='MeanWorkingHours', data=Employee, palette='pastel');\nplt.subplot(1,3,2)\nsns.boxplot(x='Attrition', y='Age', data=Employee, palette='pastel');\nplt.subplot(1,3,3)\nsns.boxplot(x='Attrition', y='TotalWorkingYears', data=Employee, palette='pastel');\nplt.show()","300b468c":"plt.figure(figsize=(25,6))\nplt.subplot(1,3,1)\nsns.boxplot(x='Attrition', y='MonthlyIncome', data=Employee, palette='pastel');\nplt.subplot(1,3,2)\nsns.boxplot(x='Attrition', y='YearsAtCompany', data=Employee, palette='pastel');\nplt.subplot(1,3,3)\nsns.boxplot(x='Attrition', y='DistanceFromHome', data=Employee, palette='pastel');\nplt.show()","b9d815a1":"###### from the above co relation matrix it is clear that [meanworking hours],[age],[totalworking years], are highly co-related with attrition than others let's plot those and see what data has to say","de9e1183":"* scaling","8a9bde4c":"# 6) Observations and Results","6effa564":"## 3.2) Dimension reduction using isomap","b6ded6a7":"###### from this distribution we can say employees who stay spend less time in the office usually around 6 to 8 hours but those who left were working for more hours","f5c40205":"## 4.1) Logistic regression","dc5f9bac":"###### From the graph it can be seen that\n1. Those employees who left spent more working hoyrs working in the company\n2. Those who left company were of less age\n3. Employees who were working for less years are more likely to leave","257b64f5":"## 1.1 importing needed libraries","1a466b4b":"### Normalizing Data\n#### It is important to normalize data so that all the feature are between fixed range ","0ac1f002":"## 2.1 Univariate analysis\n###### let's see what's in data","3ccdb875":"## 1.2 Exploring in_time and out_time dataframes","e0a26ae8":"###### checking leaves on particular days [mondays and fridays]","c1c42dc8":"###### We have two tables remaining [in_time]&[out_time]. Containing the daily in time and out time of the employees. Let's see how we can make sense of this data","192a9f7f":"-----","0ca9f43e":"# 4)Modeling","7b14a378":"-----","14b27426":"* training","c646bcda":"* scale","dc3814f9":"* train","b8531eb8":"* training","2127180b":"###### Checking unique values in every column","07fc5ab2":"###### Total time Spent Every day","39dbbfdb":"-------","47813c49":"### Analysis of Attrition among some of these groups","5af92538":"## 4.2) Decision Tree Classification","85d1cc21":"###### Check for any null values in the dataframe","7c7626ff":"* Evaluation","bd462dc1":"###### Isomap stands for isometric mapping. Isomap is a non-linear dimensionality reduction method based on the spectral theory which tries to preserve the geodesic distances in the lower dimension\n* Idea is similar to pca reducing dimensionaity by preserving the key properties like distance between the points \n* But here the distance is geodesic distances rather than the Euclidean distances","ac7fa4a9":"* Evaluate","0e002390":"-----","3f96d3c4":"###### from the  above graps we gan get an idea of what attrition among these groups look like but we cannot deduce that what features effect the attrition most. For that we can use corelation matrix which gives linear corelation of each of these groups with one another, also we can deduce that what factors are most effecting the attrition \n\n----","c76eb91b":"###### combining these three columns for better understanding of data.","df7e353a":"------------","7aa87c84":"### From the above plot it is evident that some features impact the attrition more than others those features are \n### Mean Working Hours\n### Age\n### Monthly Income\n### Total Working Years\n### Years At Company\n### Distance From Home\n#### these features are the major features which effect attrition the most","270fe9d2":"* predicting","805ec84b":"# 1) Loading and Understanding Data\n----\n","86880c14":"------","95cffd30":"* split","115112ee":"###### As  per the above data we will find out  [total working hours spent by employee in the company] , [leaves taken on friday and monday] ","757bdfdb":"-------","fe0a6fea":"-----","e3b48535":"# 3) Dimensionality Reduction","5bc06607":"# 5) Relative feature Importance","e1af25ac":"###### as there are columns [EmployeeCount],[Over18],[StandardHours],having non unique values thus can be removed as they provide no additional information","570289be":"* scale","3e9b5f6c":"* predicting","6ec0e097":"## 2.2) corelation matrix \n###### corelation gives us understanding about how strongly the features are linearly corelated (whether positively or negatively)","2c02d62e":"###### There is some data of object type which must be converted to numerical data  ","7ce1cded":"* split","0854e81e":"#### The Employees Who spend more hours in the office are more likely to quit than others and employees who are young and had spent less time with company and has been working for lesser number of years in the industry can quit working compared to others.","6b96759c":"# 7) Conclusion","b047df71":"###### check data types","188cd845":"## 4.3) Random Forest","12928806":"###### as there is some data missing in the columns named [NumCompaniesWorked] & [TotalWorkingYears] we will replace these values by the mean of their respective column","74448839":"* predict","1638851d":"# Introduction\n#### HR Analytics Case Study Dataset provides informations about job satisfaction, working enviornment, job involvement and other general data about the employee.In this kernel my focus will be on the important question - ** Why employees leave the company?**  The goal of this kernel is to predict Attrition and analyzing the factors causing it.\n\n-----------\n","d9087936":"######  Principal component analysis (PCA) is a technique for reducing the dimensionality datasets, increasing interpretability but at the same time minimizing information loss. \n* Basic idea is to reduce the dimensions of high dimensional data\n* Preserving the Key Properties like distances between the datapoints\n* so that the data visualization becomes easy","cdf61a18":"# 2) Data Exploration & Visualization","83dc437a":"-----","e34008a8":"* Evaluation","e0b47bac":"###### mean of the total time spent","e85cb73a":"* ","487c5030":"* split","070c84d0":"# **HR Analytics: Exploratory data analysis and modeling**\n\n![Employee-Attrition-Turnover-1024x437.jpg](attachment:Employee-Attrition-Turnover-1024x437.jpg)\n\n-------\n","16b3750b":"## 3.1) Dimensionality Reduction using PCA"}}