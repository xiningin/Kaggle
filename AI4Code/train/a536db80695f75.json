{"cell_type":{"b56db5b4":"code","c8c0e80a":"code","fd1cd883":"code","09418612":"code","b03e322c":"code","b40a236c":"code","570018b2":"code","be2fd5ac":"code","1f1eb655":"code","db6ee35e":"code","3ead14b1":"code","5eef4033":"code","f8f4b62f":"code","78503c76":"code","a4907320":"code","05d3c118":"code","3b45f3f6":"code","0ad485f6":"code","f2fc0344":"code","aeea31c6":"code","ba52e44d":"code","1faab05b":"code","769912cb":"code","dc4e43f9":"code","17d9b6de":"code","25222e50":"code","ad4a0cff":"markdown","6581aba0":"markdown","793c4a2e":"markdown","dd1920dd":"markdown","94cf59e6":"markdown","7eb459f6":"markdown","14909277":"markdown","2faf6688":"markdown","ce52cb84":"markdown","1b8e1e6b":"markdown","0025cfd6":"markdown","f915ac6f":"markdown","f0572ed5":"markdown","b3f9d7ea":"markdown"},"source":{"b56db5b4":"\n#IMPORTING REQUIRED LIBRARIES\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nfrom transformers import *\nimport tokenizers\nprint('TF version',tf.__version__)\n\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","c8c0e80a":"#Training data\ntrain = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","fd1cd883":"train.head(5)","09418612":"test.head()","b03e322c":"#missing value checking\n\ntrain.isnull().sum()","b40a236c":"test.isnull().sum() #no null values","570018b2":"train.loc[train['text'].isnull()] #both text and selected_text have null values","be2fd5ac":"# we are dropping this rows\ntrain.dropna(axis = 0, how ='any',inplace=True)\ntrain.reset_index(inplace=True)\ntrain.shape","1f1eb655":"# Function to print height of barcharts on the bars\n\ndef barh(ax): # for getting height\n    \n    for p in ax.patches:\n        val = p.get_height() #height of the bar\n        x = p.get_x()+ p.get_width()\/2 # x- position \n        y = p.get_y() + p.get_height()\/2 #y-position\n        ax.annotate(round(val,2),(x,y))\n","db6ee35e":"#checking distribution of sentiment values in train data \n\nplt.figure(figsize=(10,7))\nax0 = sns.countplot(train['sentiment'],order=train['sentiment'].value_counts().index)\nbarh(ax0)\nplt.show()","3ead14b1":"#checking distribution of sentiment values in test data\n\nplt.figure(figsize=(10,7))\nax1 = sns.countplot(test['sentiment'],order=test['sentiment'].value_counts().index)\nbarh(ax1)\nplt.show()","5eef4033":"import re \nt=[]\ne=[]\nfor i in range(1,train.shape[0]):\n    t.append(re.search('https?:\/\/\\S+|www\\.\\S+',train['text'].iloc[i]))\n    e.append(re.search('https?:\/\/\\S+|www\\.\\S+',train['selected_text'].iloc[i]))\n    \nhttp = pd.DataFrame({'text http': t, 'sel_text http':e})\nhttp.loc[~http['text http'].isnull()]","f8f4b62f":"#separating text according to sentiments:\n\npositive_text = train[train['sentiment'] == 'positive']['selected_text']\nnegative_text = train[train['sentiment'] == 'negative']['selected_text']\nneutral_text = train[train['sentiment'] == 'neutral']['selected_text']","78503c76":"#text cleaning functions\n\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower() #lowercase\n    text = re.sub('\\[.*?\\]', '', text) #removing anything written inside []\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)# removing urls\n    text = re.sub('<.*?>+', '', text) #removing anything written inside <>\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) #removing punctuations\n    text = re.sub('\\n', '', text) #removing '\\n' (new lines)\n    text = re.sub('\\w*\\d\\w*', '', text) #removing anything that is like hello34zero ,i.e. any string that has a digits inside\n    return text\n\n\ndef text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(tokenized_text)\n    return combined_text","a4907320":"#cleaning text\n\npositive_text_clean = positive_text.apply(lambda x: text_preprocessing(x))\nnegative_text_clean = negative_text.apply(lambda x: text_preprocessing(x))\nneutral_text_clean = neutral_text.apply(lambda x: text_preprocessing(x))","05d3c118":"#importing word cloud\n\nfrom wordcloud import WordCloud\n\nplt.figure(figsize = (20,7))\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(positive_text_clean))\nplt.imshow(wordcloud1)\nplt.axis('off')\nplt.title('Positive text',fontsize=30);\n\nplt.figure(figsize = (20,7))\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(negative_text_clean))\nplt.imshow(wordcloud2)\nplt.axis('off')\nplt.title('Negative text',fontsize=30);\n\nplt.figure(figsize = (20,7))\nwordcloud3 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(neutral_text_clean))\nplt.imshow(wordcloud3)\nplt.axis('off')\nplt.title('Neutral text',fontsize=30);\n","3b45f3f6":"#tokenizer\nPATH = '..\/input\/tf-roberta\/'\n\n#ByteLevelBPETokenizer has some additional convenience features like lowercase=True and addprefixspace=True.Hence we are using this\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)","0ad485f6":"# calculating length of the longest text\n\nMAX_LEN = 0\n\nfor text in train['text']:\n\n    # Tokenize the text and add special tokens i.e [`<s>]` and `[SEP]`\n    input_ids = tokenizer.encode(text, add_special_tokens=True)\n\n    # Update the maximum sentence length.\n    MAX_LEN = max(MAX_LEN, len(input_ids))\n\n\nprint('Max length: ', MAX_LEN)","f2fc0344":"#sentiment encoding\n\nsent = ['positive','negative','neutral']\nsent_id =[]\nfor i in sent:\n    sent_id.append(tokenizer.encode(i).ids[0])\nsentiment_id = dict(zip(sent,sent_id))\nsentiment_id","aeea31c6":"#creating RoBERTa format inputs (input_ids,attention_mark,start_tokens,end_tokens)\n\nct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(train.shape[0]):\n    \n    # Finding overlap between text and selected_text\n    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n#     print(k)\n    text2 = \" \".join(train.loc[k,'selected_text'].split())\n    idx = text1.find(text2)\n    chars = np.zeros((len(text1)))\n    chars[idx:idx+len(text2)]=1\n    if text1[idx-1]==' ':\n        chars[idx-1] = 1 \n    enc = tokenizer.encode(text1) \n#     print(enc)\n        \n    # ID_OFFSETS\n    offsets = []; idx=0\n    for t in enc.ids:\n        w = tokenizer.decode([t])\n        offsets.append((idx,idx+len(w)))\n        idx += len(w)\n    \n    # START & END TOKENS\n    toks = []\n    for i,(a,b) in enumerate(offsets):\n        sm = np.sum(chars[a:b])\n        if sm>0: toks.append(i) \n        \n    s_tok = sentiment_id[train.loc[k,'sentiment']]\n    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask[k,:len(enc.ids)+5] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+1] = 1\n        end_tokens[k,toks[-1]+1] = 1","ba52e44d":"ct = test.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","1faab05b":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    #we have to target variables (start_token and end_token). hence we are creating two models to classify values(1 or 0) for these variables\n    \n    '''\n    The Conv1D are essential because they preserve spatial information. We want our model to predict a start index and end index which are spatial.\n    If you use GlobalAveragePooling1D you will lose spatial information. You will know which texts are positive, negative, neutral but you won't \n    know where the words are located.\n    '''\n    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","769912cb":"#defining the metric\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n","dc4e43f9":"jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\noof_start = np.zeros((input_ids.shape[0],MAX_LEN))\noof_end = np.zeros((input_ids.shape[0],MAX_LEN))\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n\nskf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\nfor fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n\n    print('#'*25)\n    print('### FOLD %i'%(fold+1))\n    print('#'*25)\n    \n    K.clear_session()\n    model = build_model()\n        \n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch') #to save best weights \n        \n    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n        epochs=5, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n        [start_tokens[idxV,], end_tokens[idxV,]])) \n    \n    print('Loading model...')\n    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n    \n    print('Predicting OOF...')\n    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n    \n    print('Predicting Test...')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n    preds_start += preds[0]\/skf.n_splits\n    preds_end += preds[1]\/skf.n_splits\n    \n    # DISPLAY FOLD JACCARD\n    all = []\n    for k in idxV:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n        if a>b: \n            st = train.loc[k,'text'] # IMPROVE CV\/LB with better choice here\n        else:\n            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n            enc = tokenizer.encode(text1)\n            st = tokenizer.decode(enc.ids[a-1:b])\n        all.append(jaccard(st,train.loc[k,'selected_text']))\n    jac.append(np.mean(all))\n    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n    print()","17d9b6de":"result= []\nfor k in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[k,])\n    b = np.argmax(preds_end[k,])\n    if a>b: \n        st = test.loc[k,'text']\n    else:\n        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    result.append(st)\nresult[:5]","25222e50":"test['selected_text'] = result\ntest[['textID','selected_text']].to_csv('submission.csv',index=False)\npd.read_csv('submission.csv').head(10)","ad4a0cff":"### Build roBERTa Model (Custom question answer machine)","6581aba0":"it seems that http tags are present in both text and selected_text. Hence it is decided not to remove these. Instead we will create an attention mechanism. So our model will be trained and hence will select the same characters which were selected in the train model.","793c4a2e":"#### Both train and test have the distribution in the same order ---> neural > positive > negative","dd1920dd":"\n#### So we are converting the text according to RoBERTa format:\n\n\n![roberta%20format.jpg](attachment:roberta%20format.jpg)\n\n\n![roberts%20tokend.png](attachment:roberts%20tokend.png)\n\nhttps:\/\/huggingface.co\/transformers\/glossary.html#attention-mask\n","94cf59e6":"### Submission:","7eb459f6":"### Creating inputs for RoBERTa","14909277":"#### Metric:\n\nThe metric in this competition is the word-level Jaccard score. Jaccard Score is a measure of how similar\/dissimilar two sets are. The higher the score, the more similar the two strings. The idea is to find the number of common tokens and divide it by the total number of unique tokens. Its expressed in the mathematical terms by,\n![lMHa8CL.png](attachment:lMHa8CL.png)\n![jaccard-index-391304.jpg](attachment:jaccard-index-391304.jpg)","2faf6688":"### Training :","ce52cb84":"### Encoding Train data:","1b8e1e6b":"## Word Clouds:","0025cfd6":"### Predicting:","f915ac6f":"### Lets's do sentiment wise analysis:","f0572ed5":"#### Special thanks to :\n    1. Abhishek Thakur: https:\/\/www.kaggle.com\/abhishek\/roberta-inference-5-folds\n    2. Mr_KnowNothing : https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model\/notebook\n    3. Parul Pandey : https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert\/notebook\n    4. Chris Deotte : https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\/notebook","b3f9d7ea":"\n### Encoding test data"}}