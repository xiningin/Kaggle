{"cell_type":{"62e7faf8":"code","28808086":"code","6e3514ad":"code","0ee32d6d":"code","6d8e92b9":"code","7bb934fb":"code","410d2699":"code","36e66840":"code","8cfc106d":"code","c4672259":"code","0cb1847c":"code","75d3db46":"code","5667b976":"code","1aa26376":"code","07078ce6":"code","d630f43e":"code","3250c148":"code","4118c147":"code","6603d338":"code","65a36b8d":"code","27e6fbe0":"code","4bac5a1a":"code","5ec00ae3":"code","6782e699":"code","7274b4b5":"code","a058e92d":"code","cec4028e":"code","4a1e2ce6":"code","06fee158":"code","9902ef10":"code","9a48fc36":"code","15630de8":"code","b76723b7":"code","7c678341":"code","7dd28e12":"code","6833a6d8":"code","af33c6fc":"code","2a14e7f6":"code","ee64b855":"code","b3f03454":"code","c47c8258":"code","58346399":"code","dff9a554":"code","129fbe6f":"code","375ca793":"code","d006e6a0":"code","88b6757c":"code","74bd86d7":"code","9fa449a3":"code","0ddec037":"code","bd762ba2":"code","cca70935":"code","7588ce9c":"code","5854fc6a":"code","053a7caf":"code","128e2dff":"code","c172c4cc":"code","e8d150a5":"code","d2055c22":"code","cf1f3ad1":"code","bb558985":"code","cbcbfba4":"code","2b2c31cc":"code","4b3d94c3":"code","c2657a5b":"code","c929dedd":"markdown","143b496b":"markdown","f30f0b1b":"markdown","846dc398":"markdown","444e1501":"markdown","d14cb0de":"markdown","c9d7d94d":"markdown","f2ab8a63":"markdown","0e9c0e10":"markdown","05ad8ab4":"markdown","d05415cd":"markdown","3a9b3e30":"markdown","e68c3709":"markdown","f843f0e2":"markdown","f0014e0e":"markdown","b117c282":"markdown","8116ab03":"markdown","0b4f8463":"markdown","571a2200":"markdown","94ea0d6f":"markdown","7504576b":"markdown","3b817f7a":"markdown","f49f8b36":"markdown","8f2ddacb":"markdown","c6418cac":"markdown","af3d41fb":"markdown","5f527341":"markdown","c2dc19ed":"markdown","3c259293":"markdown","39ce6c5e":"markdown","89005689":"markdown","4f68ab39":"markdown","f3353f80":"markdown","19fb2cf8":"markdown","46a53a75":"markdown","9a724340":"markdown"},"source":{"62e7faf8":"import operator\nimport time\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\nfrom itertools import combinations\nfrom scipy.stats import ttest_ind\nimport statsmodels.api as sm\nimport scipy.stats as sst\nimport warnings\nfrom datetime import datetime\nimport plotly.express as px\nfrom pandas import Series\nimport datatable as dt\nfrom sklearn import metrics\nimport itertools\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler,MinMaxScaler, RobustScaler,OrdinalEncoder\n\nfrom catboost import CatBoost,CatBoostClassifier, Pool\nfrom catboost.utils import get_roc_curve\nimport lightgbm\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score , train_test_split, GridSearchCV,RandomizedSearchCV\nfrom xgboost import XGBClassifier\n\nimport pickle\n\nfrom sklearn.linear_model import LogisticRegression,Ridge\nfrom sklearn.ensemble import RandomForestRegressor \n\nfrom sklearn.metrics import confusion_matrix,plot_confusion_matrix,auc, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score,precision_recall_curve,average_precision_score\n\nfrom sklearn.preprocessing import PowerTransformer, QuantileTransformer\n\n%matplotlib inline\n\nRANDOM_SEED = 2021\nwarnings.simplefilter('ignore')","28808086":"def plot_roc_curve(y_true, y_score):\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    auc = roc_auc_score(y_true, y_score)\n\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \ndef test_model_roc_cm(X_test,y_test):\n    proba = model.predict_proba(X_test)[:, 1]\n    plot_roc_curve(y_test, proba)\n\n    axs = plot_grid(2, figsize=(800\/72, 300\/72))\n    plot_cmatrix(model, X_test, y_test, title='\u0410\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f', display_labels=['0', '1'], ax=axs[0]);\n    plot_cmatrix(model, X_test, y_test, title='\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f', \n                 normalize='pred', display_labels=['0', '1'], ax=axs[1]);\n    \ndef plot_cmatrix(model, X, y, title=None, **kwargs):\n    disp = plot_confusion_matrix(model, X_test, y_test, **kwargs)\n    disp.ax_.set_ylabel('\u0418\u0441\u0442\u0438\u043d\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441')\n    disp.ax_.set_xlabel('\u041f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u0439 \u043a\u043b\u0430\u0441\u0441')\n    if title:\n        disp.ax_.set_title(title)\n        \ndef plot_grid(nplots, max_cols=2, figsize=(800\/72, 600\/72)):\n    ncols = min(nplots, max_cols)\n    nrows = (nplots \/\/ ncols) + min(1, (nplots % ncols))\n    fig, axs = plt.subplots(ncols=ncols, nrows=nrows, figsize=figsize, constrained_layout=True)\n    if nrows == 1:\n        return axs\n    return [axs[index \/\/ ncols, index % ncols] for index in range(nplots)]    \ndef prepare_space(df, num_features, bin_features, cat_features, target,scaler):\n    if scaler is None:\n        scaler = StandardScaler()\n    X_num = scaler.fit_transform(df[num_features].values)\n    X_bin = df[bin_features].values\n    X_cat = OneHotEncoder(sparse=False).fit_transform(df[cat_features].values)\n    X = np.hstack([X_num, X_bin, X_cat])\n    Y = df[target].values\n    return X, Y\ndef prepare_space_with_poly(df, num_features, bin_features, cat_features, target,scaler):\n    if scaler is None:\n        scaler = StandardScaler()\n    poly = PolynomialFeatures(2)\n    x_p = poly.fit_transform(df[num_cols].values)\n    X_num = scaler.fit_transform(x_p)\n    X_bin = df[bin_features].values\n    X_cat = OneHotEncoder(sparse=False).fit_transform(df[cat_features].values)\n    X = np.hstack([X_num, X_bin, X_cat])\n    Y = df[target].values\n    return X, Y\ndef prepare_space_with_poly_no_one_hot(df, num_features, bin_features, cat_features, target,scaler):\n    if scaler is None:\n        scaler = StandardScaler()\n    poly = PolynomialFeatures(2)\n    x_p = poly.fit_transform(df[num_cols].values)\n    X_num = scaler.fit_transform(x_p)\n    X_bin = df[bin_features].values\n    X = np.hstack([X_num, X_bin])\n    Y = df[target].values\n    return X, Y\ndef plot_roc_curve(y_true, y_score):\n    fpr, tpr, _ = roc_curve(y_true, y_score)\n    auc = roc_auc_score(y_true, y_score)\n\n    plt.figure()\n    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.4f)' % auc)\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \ndef test_model_roc_cm(model, X_test,y_test):\n    try:\n        proba = model.predict_proba(X_test)[:, 1]\n        plot_roc_curve(y_test, proba)\n\n        axs = plot_grid(2, figsize=(800\/72, 300\/72))\n        plot_cmatrix(model, X_test, y_test, title='\u0410\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f', display_labels=['0', '1'], ax=axs[0]);\n        plot_cmatrix(model, X_test, y_test, title='\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f', \n                     normalize='pred', display_labels=['0', '1'], ax=axs[1]);\n    except Exception as e:\n        proba = model.predict(X_test)#[:, 1]\n        plot_roc_curve(y_test, proba)\n\n        axs = plot_grid(2, figsize=(800\/72, 300\/72))\n        plot_cmatrix(model, X_test, y_test, title='\u0410\u0431\u0441\u043e\u043b\u044e\u0442\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f', display_labels=['0', '1'], ax=axs[0]);\n        plot_cmatrix(model, X_test, y_test, title='\u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f', \n                     normalize='pred', display_labels=['0', '1'], ax=axs[1]);\n        print(e)\n        return proba","6e3514ad":"class AnalyzeNumCol():\n    '''Visualize methods and outliers'''\n    def __init__(self, df, col):\n        self.df = df[col]\n        self.col = col\n        self.col_log = LogDf(df, col).col_log\n        self.df_log = LogDf(df, col).df_col_log()\n    \n    def show_plots(self, size = 5, log = False):\n        \n        if log:\n            data = self.df_log\n            column = self.col_log\n        else:\n            data = self.df\n            column = self.col\n        \n        f = plt.figure()\n        f.clear()\n        plt.close(f)\n\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (3*size,size))\n        \n        fig.suptitle('Histogram, QQ-plot and boxplot for {0} '.format(column))\n        ax1.hist(data.dropna(), bins = 20, histtype = 'bar', align = 'mid', rwidth = 0.8, color = 'red')\n        fig = sm.qqplot(data.dropna(), fit = True, line ='45', ax=ax2) \n        \n        \n        \n        ax3.boxplot(data.dropna(), vert = False)  # \u0432\u044b\u0431\u0440\u043e\u0441\u044b\n     \n        plt.show\n        \n    def emission_limits(self, log = False): # \u0433\u0440\u0430\u043d\u0438\u0446\u044b \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432\n        if log:\n            data = self.df_log\n            column = self.col_log\n        else:\n            data = self.df\n            column = self.col\n        \n        q1 = data.quantile(q = 0.25) \n        q3 = data.quantile(q = 0.75) \n        IQR = q3 - q1\n        return q1 - 1.5*IQR, q3 + 1.5*IQR\n\n    def amount_emissions(self, log = False): \n        if log:\n            data = self.df_log\n            column = self.col_log\n        else:\n            data = self.df\n            column = self.col\n\n        minb, maxb = self.emission_limits(log)\n        return len(data[data < minb])+ len(data[data > maxb])\n\n    \n# \u043a\u043b\u0430\u0441\u0441 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\n    \nclass LogDf(): \n    def __init__(self, df, col):\n        self.df = df\n        self.col = col\n        self.col_log = col + '_log'\n        \n    def df_col_log(self):      \n        return  self.df[self.col].apply(lambda x: np.log(x + 1)) ","0ee32d6d":"# \u043c\u0435\u0442\u0440\u0438\u043a\u0430 \u0434\u043b\u044f \u043e\u0446\u0435\u043d\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438\n\nclass MatrixModel(): \n    '''ROC curve and confusion matrix'''\n    def __init__(self, model, X_test, y_test):\n        self.X_test = X_test\n        self.y_test = y_test\n        self.y_pred = model.predict(X_test)\n        self.probs = model.predict_proba(X_test)[:,1]\n    \n    def roc_curve_plot(self):\n        fpr, tpr, threshold = roc_curve(self.y_test, self.probs)\n        roc_auc = roc_auc_score(self.y_test, self.probs)\n\n        plt.figure()\n        plt.plot([0, 1], label='Baseline', linestyle='--')\n        plt.plot(fpr, tpr, label = 'Regression')\n        plt.title('Logistic Regression ROC AUC = %0.10f' % roc_auc)\n        plt.ylabel('True Positive Rate')\n        plt.xlabel('False Positive Rate')\n        plt.legend(loc = 'lower right')\n        plt.show()\n    \n    def confusion_matrix_plot(self):\n        tn, fp, fn, tp = confusion_matrix(self.y_test, self.y_pred).ravel()\n        cf_matrix = np.array([[tp,fp],[fn,tn]])\n        group_names = ['TP','FP','FN','TN']\n        group_counts = ['{0:0.0f}'.format(value) for value in cf_matrix.flatten()]\n        labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n        labels = np.asarray(labels).reshape(2,2)\n        plt.figure()\n        sns.heatmap(cf_matrix, annot=labels, annot_kws={\"size\": 20}, fmt='', cmap= 'spring', cbar = False, \\\n                 xticklabels = ['\u0414\u0435\u0444\u043e\u043b\u0442','\u041d\u0435 \u0434\u0435\u0444\u043e\u043b\u0442'], yticklabels= ['\u0414\u0435\u0444\u043e\u043b\u0442','\u041d\u0435 \u0434\u0435\u0444\u043e\u043b\u0442'])\n        plt.title('\u041c\u0430\u0442\u0440\u0438\u0446\u0430 \u043e\u0448\u0438\u0431\u043e\u043a \u0434\u043b\u044f default')\n        plt.show()\n        \n    def get_metrics(self):\n        result = pd.Series({\n            'accuracy' : accuracy_score(self.y_test, self.y_pred),\n            'precision' : precision_score(self.y_test, self.y_pred),\n            'recall' : recall_score(self.y_test, self.y_pred),\n            'F1' : f1_score(self.y_test, self.y_pred),\n            'ROC_AUC': roc_auc_score(self.y_test, self.probs) \n        })\n        return result\n    \n    def recall_precision_plot(self):\n        precisions, recalls, _ = precision_recall_curve(self.y_test, self.y_pred)\n        ap = average_precision_score(self.y_test, self.y_pred)        \n        \n        plt.figure()\n        \n        plt.step(recalls, precisions, color='b', alpha=0.2, where='post')\n        plt.fill_between(recalls, precisions, step='post', alpha=0.2, color='lime')\n        plt.xlabel('Recall');\n        plt.ylabel('Precision');\n        plt.title('Recall-precision curve, \u043f\u043b\u043e\u0449\u0430\u0434\u044c \u043f\u043e\u0434 \u043a\u0440\u0438\u0432\u043e\u0439 = %0.10f' % ap)\n        plt.grid(True)\n\n        plt.show()","6d8e92b9":"def test_models(models, X, y, iterations = 100):\n    results = {}\n    for i in models:\n        rauc_train = []\n        rauc_test = []\n        for j in range(iterations):\n            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)\n            rauc_test.append(metrics.roc_auc_score(y_test, models[i].fit(X_train, y_train).predict(X_test)))\n            rauc_train.append(metrics.roc_auc_score(y_train, models[i].fit(X_train, y_train).predict(X_train)))\n        results[i] = [np.mean(rauc_train), np.mean(rauc_test)]\n    return pd.DataFrame(results)","7bb934fb":"%time \ndata_directory = '\/kaggle\/input\/sf-dst-scoring\/'\ndf_train = pd.read_csv(data_directory+'train.csv')\ndf_test = pd.read_csv(data_directory+'test.csv')\nsample_submission = pd.read_csv(data_directory+'\/sample_submission.csv')","410d2699":"print('Train dataset')\ndf_train.info()","36e66840":"print('Test dataset')\ndf_test.info()","8cfc106d":"sample_submission.info()","c4672259":"# Unite train and test datasets\ndf_train['Train'] = 1 # mark as train \ndf_test['Train'] = 0 # mark as test\ndf_test['default'] = -1\n\ndata = pd.concat([df_train, df_test], ignore_index=True)\n\n# num of unique values, first 10 unique values, null values count, type\ndata.agg({'nunique', lambda s: s.unique()[:10]})\\\n    .append(pd.Series(data.isnull().sum(), name='null'))\\\n    .append(pd.Series(data.dtypes, name='dtype'))\\\n    .transpose()","0cb1847c":"data.isna().sum()","75d3db46":"#import pandas_profiling\n#pp_data = pandas_profiling.ProfileReport(data)\n#pp_data","5667b976":"fig, axes = plt.subplots(1, 3, figsize=(10,7))\nfor i,col in enumerate(['decline_app_cnt', 'bki_request_cnt', 'income']):\n    data[col] = np.log(data[col] + 1)\n    sns.distplot(data[col][data[col] > 0].dropna(), ax=axes.flat[i],kde = False, rug=False,color=\"r\")\n    ","1aa26376":"target = 'default'\ntime_cols = ['app_date']\ncat_cols = ['education',  'home_address', 'work_address', 'sna', 'first_time']\nbin_cols = ['sex', 'car', 'car_type', 'good_work', 'foreign_passport']\nnum_cols = ['age','decline_app_cnt','region_rating','score_bki','bki_request_cnt','income','delta_time','bki_age_reg']","07078ce6":"data['app_date'] = pd.to_datetime(data['app_date'], format='%d%b%Y')\ndata_min = min(data['app_date'])\ndata['days'] = (data['app_date'] - data_min).dt.days.astype('int')","d630f43e":"data['day'] = data['app_date'].dt.day\ndata['month'] = data['app_date'].dt.month","3250c148":"NY_2020 = pd.to_datetime('31\/12\/2020')\ndata['delta_time'] = (NY_2020 - data['app_date']).dt.days\ndata[data.month == 1]['delta_time'].head()","4118c147":"data.drop(['day', 'app_date','month'],  axis = 1, inplace = True)","6603d338":"data.isna().sum()","65a36b8d":"data.education.value_counts()","27e6fbe0":"data.education.fillna('SCH', inplace=True)\n\n# Studying attempt\n# Seems the same as LablelEncoder\n#ordinal_enc = OrdinalEncoder()\n#data[['education']] = ordinal_enc.fit_transform(data[['education',]])\n\ndata['education'] = LabelEncoder().fit_transform(data['education'])\n","4bac5a1a":"data.education.value_counts()","5ec00ae3":"# Encode binary columns\n\nlabel_encoder = LabelEncoder()\n\nfor column in bin_cols:\n    data[column] = label_encoder.fit_transform(data[column])","6782e699":"data.describe()","7274b4b5":"cols = data.columns\nfor col in cols:\n    if data[col].dtype != 'O' and len(data[col].unique())<=20:\n        print('Unique values in colum {0} : {1}'.format(col,data[col].unique()))\n    else:\n        print('Numeric column: {}'.format(col))","a058e92d":"age = AnalyzeNumCol(data[data.Train == 1], 'age')\nage.show_plots()\nage.show_plots(log = True)","cec4028e":"data.age = (data.age + 1).transform(np.log)","4a1e2ce6":"decline_app_cnt = AnalyzeNumCol(data[data.Train == 1], 'decline_app_cnt')\ndecline_app_cnt.show_plots()\ndecline_app_cnt.show_plots(log = True)","06fee158":"print('Sum of outliers for decline_app_cnt: ', decline_app_cnt.amount_emissions())\nprint('Sum of outliers for log(decline_app_cnt): ', decline_app_cnt.amount_emissions(log = True))","9902ef10":"data.decline_app_cnt = (data.decline_app_cnt + 1).transform(np.log)","9a48fc36":"score_bki = AnalyzeNumCol(data[data.Train == 1], 'score_bki')\nscore_bki.show_plots()\nscore_bki.show_plots(log=True)","15630de8":"print('Sum of outliers for score_bki: ', score_bki.amount_emissions())\nprint('Sum of outliers for log(score_bki): ', score_bki.amount_emissions(log = True))","b76723b7":"bki_request_cnt = AnalyzeNumCol(data[data.Train == 1], 'bki_request_cnt')\nbki_request_cnt.show_plots()\nbki_request_cnt.show_plots(log = True)","7c678341":"print('Sum of outliers for bki_request_cnt: ', bki_request_cnt.amount_emissions())\nprint('Sum of outliers for log(bki_request_cnt): ', bki_request_cnt.amount_emissions(log = True))","7dd28e12":"data.bki_request_cnt = (data.bki_request_cnt + 1).transform(np.log)","6833a6d8":"income = AnalyzeNumCol(data[data.Train == 1], 'income')\nincome.show_plots()\nincome.show_plots(log = True)","af33c6fc":"print('Sum of outliers for income: ', income.amount_emissions())\nprint('Sum of outliers for log(income): ', income.amount_emissions(log = True))","2a14e7f6":"data.income = data.income.transform(np.log)","ee64b855":"delta_time = AnalyzeNumCol(data[data.Train == 1], 'delta_time')\ndelta_time.show_plots()\ndelta_time.show_plots(log = True)","b3f03454":"fig, ax = plt.subplots(1,1, figsize = (20,20))\nax = sns.heatmap(data.corr(),annot = True, cmap = 'plasma')","c47c8258":"data['bki_age_reg'] = (data['score_bki']\/data['age'])*data['region_rating']","58346399":"# mean income by age\nmean_income = data.groupby('age')['income'].mean().to_dict()\ndata['mean_income_age'] = data['age'].map(mean_income)","dff9a554":"# Max income by age\nmax_income = data.groupby('age')['income'].max().to_dict()\ndata['max_income_age'] = data['age'].map(max_income)","129fbe6f":"# and normalize it\ndata[\"normalized_income\"] = abs((data.income - data.mean_income_age)\/ data.max_income_age)","375ca793":"# requests to BKI by age\nmean_bki = data.groupby('age')['bki_request_cnt'].mean().to_dict()\ndata['mean_requests_age'] = data['age'].map(mean_bki)","d006e6a0":"# mean BKI requests by income\nmean_bki_inc = data.groupby('income')['bki_request_cnt'].mean().to_dict()\ndata['mean_requests_income'] = data['income'].map(mean_bki_inc)","88b6757c":"# Mean income by region\nmean_income_rat = data.groupby('region_rating')['income'].mean().to_dict()\ndata['mean_income_region'] = data['region_rating'].map(mean_income_rat)","74bd86d7":"# need to rename the columns\nnum_cols = ['age','decline_app_cnt','score_bki','income','bki_request_cnt', 'mean_income_age','region_rating','max_income_age', 'normalized_income',\n       'mean_requests_age', 'mean_requests_income', 'mean_income_region','delta_time','bki_age_reg'] # numerical\ncat_cols = ['education','work_address','home_address','sna','first_time'] # categorical\nbin_cols = ['sex','car','car_type','good_work','foreign_passport'] # binary","9fa449a3":"# Plot boxplots for numeric feats\nfig, axes = plt.subplots(7, 2, figsize=(15,15))\nplt.subplots_adjust(wspace = 0.5)\naxes = axes.flatten()\n#data[num_cols]\nfor i in range(len(num_cols)-1):\n    sns.boxplot(x=\"default\", y=num_cols[i], data=data[data.Train == 1], orient = 'v', ax=axes[i])","0ddec037":"imp_cat = pd.Series(mutual_info_classif(data[bin_cols + cat_cols], data['default'],\n                                     discrete_features =True), index = bin_cols + cat_cols)\nimp_cat.sort_values(inplace = True)\nimp_cat.plot(kind = 'barh', color='pink')","bd762ba2":"data_temp = data.loc[data['Train'] == 1] \nimp_num = pd.Series(f_classif(data_temp[num_cols], data_temp['default'])[0], index = num_cols)\nimp_num.sort_values(inplace = True)\nimp_num.plot(kind = 'barh', color='pink')","cca70935":"#scaler = StandardScaler()\n#data[num_cols] = scaler.fit_transform(data[num_cols].values)\n\n#scaler = MinMaxScaler()\n#data[num_cols] = scaler.fit_transform(data[num_cols].values)\n\nscaler = RobustScaler()\ndata[num_cols] = scaler.fit_transform(data[num_cols].values)","7588ce9c":"\ntrain = data[data.Train == 1]\ntest = data[data.Train == 0]\ntrain = train.drop(['Train','client_id'], axis=1)\ntest = test.drop(['Train','client_id','default'], axis=1)\n\nX = train.drop('default', axis=1).values\ny = train['default'].values\n\n#Previous realization\n#X, y = prepare_space_with_poly(train, num_cols, bin_cols, cat_cols, target,StandardScaler())\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED,shuffle=True)","5854fc6a":"lr = LogisticRegression(\n    class_weight='balanced', #param for disbalanced classes | our case\n    max_iter = 1000,\n    random_state=RANDOM_SEED\n).fit(X_train, y_train)\ntest_model_roc_cm(lr, X_test,y_test)","053a7caf":"# This cross-validation object is a variation of KFold that returns stratified folds. \n# The folds are made by preserving the percentage of samples for each class.\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n\n# LogisticRegression\nlr = LogisticRegression(\n                            max_iter = 1000,\n                            random_state=RANDOM_SEED)\n# CATBOOST\ncatbst = CatBoostClassifier(\n                           iterations=1000,\n                           metric_period=100,\n                           random_seed=RANDOM_SEED,\n                           depth=6,\n                           l2_leaf_reg=5,\n                           eval_metric='AUC',\n                           custom_loss=['AUC', 'Accuracy'],\n                           use_best_model=True,\n                           auto_class_weights='Balanced',\n                           early_stopping_rounds=20,\n                           learning_rate=0.01591,\n                           boosting_type='Ordered',\n                           bootstrap_type='MVS',\n                           leaf_estimation_method='Newton'\n                           )\n# LGBM\nlgbm = lightgbm.LGBMClassifier(\n                            application='binary',\n                            objective='binary',\n                            metric='auc',\n                            is_unbalance='true',\n                            boosting='gbdt',\n                            num_leaves=40,\n                            bagging_freq=20,\n                            learning_rate=0.05,\n                            verbose=0,\n                            class_weight='balanced',\n                            random_state = RANDOM_SEED)\n\nxgb = XGBClassifier(learning_rate=0.02, \n                    n_estimators=600,\n                    objective='binary:logistic',\n                    max_depth=5,\n                    gamma=5,\n                    min_child_weight=5,\n                    colsample_bytree=1.0,\n                    subsample=1.0,\n                    silent=True, nthread=1)\n\nfor fold,(idxT, idxV) in enumerate(kf.split(X, y)):\n    print('FOLD \u2116', fold)\n\n    xgb.fit( \n            X=X[idxT],\n            y=y[idxT],\n            eval_set=[(X[idxT],y[idxT]),(X[idxV], y[idxV])],\n            early_stopping_rounds=10,\n            verbose=0\n            )\n    \n    lr.fit(X[idxT], y[idxT])\n    X_test = X[idxV]\n    y_test = y[idxV]\n\n\n    #test_model_roc_cm(lr, X_test,y_test)\n    train_pool = Pool(X[idxT], label=y[idxT])\n    test_pool = Pool(X[idxV],label=y[idxV])\n\n    catbst.fit(train_pool,eval_set=test_pool,use_best_model=True, verbose_eval=False)\n\n    lgbm.fit( X=X[idxT],\n               y=y[idxT],\n               eval_set=(X[idxV], y[idxV]),\n               #num_boost_round=5000,\n               early_stopping_rounds=50,\n             verbose=0)\n    \nprint('Logistic Regression results:')\ntest_model_roc_cm(lr, X_test,y_test)\nprint('XGB results:')\ntest_model_roc_cm(xgb, X_test,y_test)\nprint('Catboost results:')\ntest_model_roc_cm(catbst, X_test,y_test)\nprint('LGBM results:')\ntest_model_roc_cm(lgbm, X_test,y_test)","128e2dff":"y_pred_lr = lr.predict_proba(test)\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred_lr[:,1]})\nresults_df.to_csv('submission_log_reg.csv', index=False)\n\ny_pred_catb = catbst.predict_proba(test)\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred_catb[:,1]})\nresults_df.to_csv('submission_catboost.csv', index=False)\n\ny_pred_lgbm = lgbm.predict_proba(test)\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred_lgbm[:,1]})\nresults_df.to_csv('submission_lgbm.csv', index=False)\n\n# come across with an error while trying to predict_proba for test as pandas df\n# helped converting to ndarray via test.values\n# https:\/\/stackoverflow.com\/questions\/42338972\/valueerror-feature-names-mismatch-in-xgboost-in-the-predict-function\ny_pred_xgb = xgb.predict_proba(test.values)\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred_xgb[:,1]})\nresults_df.to_csv('submission_xgb.csv', index=False)","c172c4cc":"params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","e8d150a5":"xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)\nfolds = 4\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = RANDOM_SEED)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=RANDOM_SEED )\n\nrandom_search.fit(X, y)","d2055c22":"print('\\n All results:')\nprint(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nresults = pd.DataFrame(random_search.cv_results_)\nresults.to_csv('xgb-random-grid-search-results-01.csv', index=False)","cf1f3ad1":"y_pred_xgb = random_search.predict_proba(test.values)\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred_xgb[:,1]})\nresults_df.to_csv('submission_radomized_search_xgb.csv', index=False)","bb558985":"y_pred_ensmb = (y_pred_catb + y_pred_xgb) \/ 2\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred_ensmb[:,1]})\nresults_df.to_csv('submission_ensemble_catb_xgb.csv', index=False)","cbcbfba4":"y_pred_ensmb = (y_pred_catb  + y_pred_lr + y_pred_lgbm + y_pred_xgb) \/ 4\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred_ensmb[:,1]})\nresults_df.to_csv('submission_ensemble_all_models.csv', index=False)","2b2c31cc":"test_ps = test\n\n#y_lr_pred_prob = lr.predict_proba(X)[:,1]\ny_catbst_pred_prob = catbst.predict_proba(test)[:,1]\n#y_lgbm_pred_prob = lgbm.predict_proba(X)[:,1]\n#test_ps['default_proba'] = (y_lr_pred_prob + y_catbst_pred_prob + y_lgbm_pred_prob) \/ 3\n\ntest_ps['default_proba'] = y_catbst_pred_prob\ntrain_part = test_ps[test_ps.default_proba * 100 > 80]\ntrain_part.loc[ train_part['default_proba']>=0.5, 'default' ] = 1\ntrain_part.loc[ train_part['default_proba']<0.5, 'default' ] = 0\ntrain_part = train_part.drop(['default_proba'],axis=1)\n\nprint(\"Length of PL rows to concatanate with Train:\", len(train_part))\nnew_train = pd.concat([train, train_part], ignore_index=True)\nX_new_train = new_train.drop('default', axis=1).values\ny_new_train = new_train['default'].values","4b3d94c3":"lgbm.fit(X_new_train, y_new_train)\n\ny_pred_log_reg_with_pl = lgbm.predict_proba(test.drop(columns=['default_proba']))\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred_log_reg_with_pl[:,1]})\nresults_df.to_csv('submission_y_pred_log_reg_with_pl.csv', index=False)","c2657a5b":"# CATBOOST\ncatbst_ps = CatBoostClassifier(\n                           iterations=1000,\n                           metric_period=100,\n                           random_seed=RANDOM_SEED,\n                           depth=6,\n                           l2_leaf_reg=5,\n                           eval_metric='AUC',\n                           custom_loss=['AUC', 'Accuracy'],\n                           use_best_model=True,\n                           early_stopping_rounds=10,\n                           learning_rate=0.05,\n                           boosting_type='Ordered',\n                           bootstrap_type='MVS',\n                           leaf_estimation_method='Newton'\n                           )\n\nfor fold,(idxT, idxV) in enumerate(kf.split(X, y)):\n    print('FOLD \u2116', fold)\n    train_pool = Pool(X[idxT], label=y[idxT])\n    test_pool = Pool(X[idxV],label=y[idxV])\n    catbst_ps.fit(train_pool,eval_set=test_pool,use_best_model=True, verbose_eval=False)\n\ny_pred = catbst_ps.predict_proba(test.drop(columns=['default_proba']))\nresults_df = pd.DataFrame(data={'client_id':df_test['client_id'], 'default':y_pred[:,1]})\nresults_df.to_csv('submission_catboost_with_pL.csv', index=False)","c929dedd":"![Credit Score](https:\/\/www.nfcc.org\/wp-content\/uploads\/2020\/09\/bigstock-Credit-Score-Concept-Business-384487778-768x477.jpg)","143b496b":"# Datasets","f30f0b1b":"**Numerical**\n> [Compute](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.f_classif.html) the ANOVA F-value for the provided sample","846dc398":"# [Pseudo Labeling](https:\/\/towardsdatascience.com\/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af#) \n> Notice : Not recommended to use in prod","444e1501":"**Conclusion**\n\nOutliers absent. Notice, after log age, distribution become more normal.","d14cb0de":"## Fill education NaN with most common","c9d7d94d":"### 3.1 Age","f2ab8a63":"# Libs","0e9c0e10":"Do nothing","05ad8ab4":"# If you find this notebook helpful, please don't forget to upvote ^\n","d05415cd":"Let's notice that datetime and generated from this feature new columns, plays important role in scoring models, so generate new feature as delta from now till the end of 2020.","3a9b3e30":"**Conclusion**\n\n* Drop day,app_date, month","e68c3709":"### Fast and furious EDA","f843f0e2":"As we remeber we have disbalanced classes, we have to use class_weight in models","f0014e0e":"**Conclusion**\n\nLet's apply log function to it, it makes us happier","b117c282":"**Conclusion**\n\n* Average age for default clients is lower then for not default.\n* decline_app_cnt is higher for default clients.\n* score_bki is higher for default clients.\n* bki_request_cnt for defaut clients is much more higher then for others.\n* INcome for defaults is cheaper.","8116ab03":"## Time","0b4f8463":"### Income","571a2200":"**Categorical**\n> [Estimate](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.mutual_info_classif.html) mutual information for a discrete target variable.","94ea0d6f":"**Conclusion**:\n\nOk, log it too","7504576b":"# XGB with RandomizedSearchCV","3b817f7a":"### Delta_time","f49f8b36":"### 3.4 bki_request_cnt","8f2ddacb":"**Coclusion**\n> Keep calm and do nothing with it","c6418cac":"# to do\n* Optimize hyperparams for catboost with [optuna](https:\/\/optuna.org\/)","af3d41fb":"### Missing values in Education and default columns","5f527341":"# 3. Feature Engineering","c2dc19ed":"# Utils","3c259293":"#  EDA","39ce6c5e":"# Ensemble","89005689":"### 3.2 Decline_app_cnt","4f68ab39":"### 3.3 Score_bki","f3353f80":"**Conclusion**\n\n> Score_bki is normally distributed. Let's see outliers","19fb2cf8":"Dataset description\n\nclient_id - identification\n\neducation - education level\n\nsex - sex\n\nage - age\n\ncar - binary\/ has a car or not\n\ncar_type - whether car is international\n\ndecline_app_cnt - declined application count in the past\n\ngood_work - binary\/ has 'good' work or not\n\nbki_request_cnt - requests to BKI\n\nhome_address - category of home address\n\nwork_address - category of work address\n\nincome - income\n\nforeign_passport - binary\/ has foreign passport\n\nsna - connection with bank employee\n\nfirst_time - age of information about the client\n\nscore_bki - BKI score\n\nregion_rating - region rating\n\napp_date - application date\n\ndefault - default flag","46a53a75":"**Conclusion**\n\n**decline_app_cnt** is pretty skewed to the left. Applying log will help\n\nLet's see outliers","9a724340":"Seems to be CatBoost better then others. Let's rock the results as submisiion files"}}