{"cell_type":{"63c7ef65":"code","2bdf6cd3":"code","8c87c091":"code","ef8a5cfa":"code","2c176c31":"code","d4a1209d":"code","ef7b3667":"code","21f6ae07":"code","f853e29a":"code","4493949e":"code","a9172ae7":"code","acbf3cd8":"code","0ae036d9":"code","01ca354e":"code","d3b3ff33":"code","fc1e4089":"code","1ac39a5b":"code","3690d064":"code","3753fe37":"code","ac112e37":"code","89ce9731":"code","ae00db98":"code","08cbd00b":"code","c04b626c":"code","357e058e":"code","a2fc35e0":"code","d2145753":"code","6fa7da50":"code","152e1497":"code","10ff15b3":"code","5e2e6b28":"code","e6f739d2":"code","d2035cdf":"code","66243dbd":"code","f23d5b5d":"code","bf9492de":"code","e75d8a10":"code","387b309d":"code","fc530949":"code","d79aa014":"code","9540328d":"code","97020c50":"code","39f122a4":"code","91673758":"code","d2dfc141":"code","8b1048b9":"code","7e1fd5a8":"markdown","85e46ec2":"markdown","01247e49":"markdown","95132d8b":"markdown","54e4ce90":"markdown","7be46b40":"markdown","961f955b":"markdown","de10606b":"markdown","8df4dc65":"markdown","026256a0":"markdown","45c413ce":"markdown","ae004390":"markdown","10175a52":"markdown","8c05a85d":"markdown","c80bdec6":"markdown","35fdc14b":"markdown","457dc95e":"markdown","e4e25dc5":"markdown","cd9212d0":"markdown","241041b3":"markdown","fa501ed9":"markdown","f9bcce6d":"markdown","31254edd":"markdown","e622db66":"markdown","f6d8a11b":"markdown","1a3e7b14":"markdown","ea0f3d7f":"markdown","86ae926d":"markdown","adce05d9":"markdown","10df750f":"markdown","7222028d":"markdown","8d2ef0c7":"markdown","f033f1e9":"markdown","a5ee4bcf":"markdown","194fce65":"markdown","780b482e":"markdown","7adbb7d8":"markdown","0a8e1ae5":"markdown","c6f61002":"markdown","9634aa33":"markdown","c9c6eac1":"markdown","d9cdf75a":"markdown","2292ea90":"markdown","2f163586":"markdown","da2a35fd":"markdown","866a827e":"markdown","8a8cb8f4":"markdown","53a71eaf":"markdown","267b831a":"markdown","f956d4b5":"markdown","496321d0":"markdown","71377d98":"markdown","65cb551d":"markdown","066d78e5":"markdown","d9a60fdb":"markdown","02d506b8":"markdown","b3088964":"markdown","0d3ab5a8":"markdown","7fe9bfcf":"markdown","8c15c007":"markdown","fdb4b798":"markdown"},"source":{"63c7ef65":"#Importing the data analysis libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n#Importing the visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#Ensuring that we don't see any warnings while running the cells\nimport warnings\nwarnings.filterwarnings('ignore') \n\n#Importing the counter\nfrom collections import Counter\n\n#Importing sci-kit learn libraries that we will need for this project\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\n","2bdf6cd3":"#Reading the data from the given files and creating a training and test dataset\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","8c87c091":"train.sample(10)","ef8a5cfa":"train.describe(include=\"all\")","2c176c31":"def detect_outliers(dataframe, n, features):\n    \n    outliers_indices = []\n    \n    for feature in features:\n        \n        #determining the upper and lower quartiles\n        Quart1 = dataframe[feature].quantile(0.25)\n        Quart3 = dataframe[feature].quantile(0.75)\n        \n        #determining the upper and lower outlier thresholds to remove the outliers\n        upper_outlier_threshold = Quart3 + (Quart3 - Quart1) * 1.5\n        lower_outlier_threshold = Quart1 - (Quart3 - Quart1) * 1.5\n        \n        #finding the outliers and saving their indices in the form of a list, according to the given threshold\n        feature_outliers_list = dataframe[(dataframe[feature] > upper_outlier_threshold) | (dataframe[feature] < lower_outlier_threshold)].index\n        \n        #appending the outliers for each feature to the main outliers_indices list\n        outliers_indices.extend(feature_outliers_list)\n        \n    #Selecting features that have more than 2 outliers\n    return list(a for a, b in Counter(outliers_indices).items() if b > n)","d4a1209d":"outliers = detect_outliers(train, 2, [\"Age\", \"SibSp\", \"Fare\", \"Parch\"])\ntrain.loc[outliers]","ef7b3667":"train = train.drop(outliers, axis = 0).reset_index(drop = True)","21f6ae07":"print(pd.isnull(train).sum())","f853e29a":"df =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\ndf.describe(include = \"all\")","4493949e":"sns.heatmap(df[[\"Survived\",\"Age\",\"Pclass\", \"SibSp\", \"Parch\", \"Fare\"]].corr(), cmap = 'coolwarm', annot = True)","a9172ae7":"#My function function to visualize and count the values in each category of each feature\ndef bar_plot(variable):\n    \n    #This code is used to solve problem when there are no survivors for a category, which causes an error in the display code\n    feature_categories = df[variable].sort_values().unique()\n    for category in feature_categories:\n        temp_series = df[\"Survived\"][df[variable] == category].value_counts(normalize = True)\n        if temp_series.shape == (1,):\n            temp_series = temp_series.append(pd.Series([0], index=[1]))\n        elif temp_series.shape == (0,):\n            continue\n        print(\"Fraction of {} = {} who survived:\".format(variable, category), temp_series[1])\n    #visualize\n    sns.barplot(x = df[variable],y = df[\"Survived\"],  data = df).set_title('Fraction Survived With Respect To {}'.format(variable))","acbf3cd8":"bar_plot(\"Pclass\")","0ae036d9":"bar_plot(\"Sex\")","01ca354e":"bar_plot(\"SibSp\")","d3b3ff33":"bar_plot(\"Parch\")","fc1e4089":"#Filling missing value\n#Since Embarked only has 2 missing values, I will use the mode from the Series to determine the missing values\ndf[\"Embarked\"] = df[\"Embarked\"].fillna(df['Embarked'].mode()[0])\nbar_plot(\"Embarked\")","1ac39a5b":"sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train, size=6, kind=\"count\")","3690d064":"df[\"Cabin\"] = df[\"Cabin\"].notnull().astype(int)\n\npclass1 = df[\"Cabin\"][df[\"Pclass\"] == 1].value_counts()[1]\nprint(\"recorded Cabins with pclass1 = 1: {}\".format(pclass1))\n\nsns.barplot(x=\"Pclass\", y=\"Cabin\", data=df)","3753fe37":"# Exploring Age vs Sex, Parch , Pclass and SibSP\ng = sns.factorplot(y=\"Age\",x=\"Sex\",data=df, kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Pclass\", data=df, kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=df, kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=df, kind=\"box\")\n","ac112e37":"#Using a heatmap to determine the correlation between the remaining features\nsns.heatmap(df[[\"Age\",\"Pclass\",\"SibSp\", \"Parch\"]].corr(), cmap = 'coolwarm', annot = True)","89ce9731":"#small function that will remove the missing values in the age column\ndef fill_age_missing_values(df):\n    Age_Nan_Indices = list(df[df[\"Age\"].isnull()].index)\n\n    #for loop that iterates over all the missing age indices\n    for index in Age_Nan_Indices:\n        #temporary variables to hold SibSp, Parch and Pclass values pertaining to the current index\n        temp_Pclass = df.iloc[index][\"Pclass\"]\n        temp_SibSp = df.iloc[index][\"SibSp\"]\n        temp_Parch = df.iloc[index][\"Parch\"]\n        age_median = df[\"Age\"][((df[\"Pclass\"] == temp_Pclass) & (df[\"SibSp\"] == temp_SibSp) & (df[\"Parch\"] == temp_Parch))].median()\n        if df.iloc[index][\"Age\"]:\n            df[\"Age\"].iloc[index] = age_median\n        if np.isnan(age_median):\n            df[\"Age\"].iloc[index] = df[\"Age\"].median()\n    return df","ae00db98":"#Using the function to remove missing values in both train and test set\ndf = fill_age_missing_values(df)\ndf.describe(include=\"all\")","08cbd00b":"df[\"Age\"].isnull().sum()","c04b626c":"df.head()","357e058e":"#Creating a new column(\"Title\") using list comprehension\ndf[\"Title\"] = pd.Series([name.split(\",\")[1].split(\".\")[0].strip() for name in df[\"Name\"]])\ndf.head()","a2fc35e0":"pd.crosstab(df['Title'], df['Sex'])","d2145753":"# Convert to categorical values Title \ndf[\"Title\"] = df[\"Title\"].replace(['Lady', 'the Countess', 'Countess', 'Don', 'Jonkheer', 'Dona', 'Sir'], 'Royals')\ndf[\"Title\"] = df[\"Title\"].replace(['Col', 'Dr', 'Major', 'Capt'], 'Professionals')\ndf[\"Title\"] = df[\"Title\"].replace([\"Ms\", \"Mme\", \"Mlle\", \"Mrs\"], 'Miss')\ndf[\"Title\"] = df[\"Title\"].replace(['Master', 'Rev'], 'Mas\/Rev')\ndf[\"Title\"] = df[\"Title\"].map({\"Mas\/Rev\": 0, \"Miss\": 1, \"Mr\": 2, \"Royals\": 3, \"Professionals\": 4})","6fa7da50":"pd.crosstab(df['Title'], df['Sex'])","152e1497":"sns.factorplot(x=\"Title\",y=\"Survived\",data=df, kind=\"bar\").set_xticklabels([\"Master\",\"Miss\",\"Mr\",\"Royals\",\"Professionals\"]).set_ylabels(\"survival probability\")","10ff15b3":"df[\"Ftotal\"] = 1 + df[\"SibSp\"] + df[\"Parch\"]","5e2e6b28":"sns.factorplot(x=\"Ftotal\",y=\"Survived\",data=df, kind=\"bar\").set_ylabels(\"survival probability\")","e6f739d2":"df[\"Age\"] = df[\"Age\"].astype(int)\ndf.loc[(df['Age'] <= 2), 'Age Group'] = 'Baby' \ndf.loc[((df[\"Age\"] > 2) & (df['Age'] <= 10)), 'Age Group'] = 'Child' \ndf.loc[((df[\"Age\"] > 10) & (df['Age'] <= 19)), 'Age Group'] = 'Young Adult'\ndf.loc[((df[\"Age\"] > 19) & (df['Age'] <= 60)), 'Age Group'] = 'Adult'\ndf.loc[(df[\"Age\"] > 60), 'Age Group'] = 'Senior'\ndf[\"Age Group\"] = df[\"Age Group\"].map({\"Baby\": 0, \"Child\": 1, \"Young Adult\": 2, \"Adult\": 3, \"Senior\": 4})","d2035cdf":"df.sample(5)","66243dbd":"df.describe(include = \"all\")","f23d5b5d":"sns.distplot(df[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(df[\"Fare\"].skew())).legend(loc=\"best\")","bf9492de":"# Apply log to Fare to reduce skewness distribution\ndf[\"Fare\"] = df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\nsns.distplot(df[\"Fare\"], color=\"g\", label=\"Skewness : %.2f\"%(df[\"Fare\"].skew())).legend(loc=\"best\")","e75d8a10":"df[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\": 1})\ndf.sample(5)","387b309d":"df[\"Embarked\"] = df[\"Embarked\"].map({\"C\": 0, \"Q\": 1, \"S\": 2})\ndf.sample(5)","fc530949":"passenger_ID = pd.Series(df[\"PassengerId\"], name = \"PassengerId\")\ndf = df.drop([\"Name\", \"PassengerId\", \"SibSp\", \"Parch\", \"Age\", \"Ticket\"], axis=1)\ndf.sample(5)","d79aa014":"df = pd.get_dummies(df, columns = [\"Title\"])\ndf = pd.get_dummies(df, columns = [\"Embarked\"])\ndf = pd.get_dummies(df, columns = [\"Pclass\"])\ndf = pd.get_dummies(df, columns = [\"Age Group\"])","9540328d":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:].drop([\"Survived\"], axis = 1)","97020c50":"#StratifiedKFold aims to ensure each class is (approximately) equally represented across each test fold\nk_fold = StratifiedKFold(n_splits=5)\n\nX_train = train.drop(labels=\"Survived\", axis=1)\ny_train = train[\"Survived\"]\n\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\n\n# Creating objects of each classifier\nLG_classifier = LogisticRegression(random_state=0)\nSVC_classifier = SVC(kernel=\"rbf\", random_state=0)\nKNN_classifier = KNeighborsClassifier()\nNB_classifier = GaussianNB()\nDT_classifier = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\nRF_classifier = RandomForestClassifier(n_estimators=200, criterion=\"entropy\", random_state=0)\n\n#putting the classifiers in a list so I can iterate over there results easily\ntitanic_classifiers = [LG_classifier, SVC_classifier, KNN_classifier, NB_classifier, DT_classifier, RF_classifier]\n\n#This dictionary is just to grad the name of each classifier\nclassifier_dict = {\n    0: \"Logistic Regression\",\n    1: \"Support Vector Classfication\",\n    2: \"K Nearest Neighbor Classification\",\n    3: \"Naive bayes Classifier\",\n    4: \"Decision Trees Classifier\",\n    5: \"Random Forest Classifier\",\n}\n\ntitanic_results = pd.DataFrame({'Model': [],'Mean Accuracy': [], \"Standard Deviation\": []})\n\n#Iterating over each classifier and getting the result\nfor i, classifier in enumerate(titanic_classifiers):\n    classifier_scores = cross_val_score(classifier, X_train, y_train, cv=k_fold, n_jobs=2, scoring=\"accuracy\")\n    titanic_results = titanic_results.append(pd.DataFrame({\"Model\":[classifier_dict[i]], \n                                                           \"Mean Accuracy\": [classifier_scores.mean()],\n                                                           \"Standard Deviation\": [classifier_scores.std()]}))","39f122a4":"print (titanic_results.to_string(index=False))","91673758":"from sklearn.model_selection import GridSearchCV\n\nRF_classifier = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nRF_paramgrid = {\"max_depth\": [None],\n                  \"max_features\": [1, 3, 10],\n                  \"min_samples_split\": [2, 3, 10],\n                  \"min_samples_leaf\": [1, 3, 10],\n                  \"bootstrap\": [False],\n                  \"n_estimators\" :[100,200,300],\n                  \"criterion\": [\"entropy\"]}\n\n\nRF_classifiergrid = GridSearchCV(RF_classifier, param_grid = RF_paramgrid, cv=k_fold, scoring=\"accuracy\", n_jobs= -1, verbose=1)\n\nRF_classifiergrid.fit(X_train,y_train)\n\nRFC_optimum = RF_classifiergrid.best_estimator_\n\n# Best Accuracy Score\nRF_classifiergrid.best_score_","d2dfc141":"IDtest = passenger_ID[train.shape[0]:].reset_index(drop = True)","8b1048b9":"from sklearn.ensemble import RandomForestClassifier\nX_train = train.drop(labels=\"Survived\", axis=1)\ny_train = train[\"Survived\"]\n\n# Feature Scaling\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(test)\n\nRFC_optimum.fit(X_train, y_train)\n\ntest_predictions = pd.Series(RFC_optimum.predict(X_test).astype(int), name=\"Survived\")\ntitanic_results = pd.concat([IDtest, test_predictions], axis = 1)\ntitanic_results.to_csv('submission.csv', index=False)","7e1fd5a8":"Observations:\n* K nearest neighbors and Naive bayes have accuracies below 80% and thus I will not consider them anymore.\n* Even though Logistic regression does a job aswell as the other classifiers, it's accuracy was the lowest among the remaining 5, so I will not use this classifier further\n* Random Forest classifier gives the best results among all classifiers so I will use this classifier as the final classifier for my submission but not before tuning hyper parameters using GridSearchSV","85e46ec2":"## 6.4 - Solving fare skewness","01247e49":"# 7 - Building\/Training our model","95132d8b":"## 4.5 - Parch","54e4ce90":"## 7.1 - Seperating Train\/Test dataset","7be46b40":"It's highly like that recorded cabin values would mean a higher economic status and those who don't have a separate cabin belong to lower economic status.\nLet's check the correlation of recorded cabin values with the Pclass to determine if there is any merit in our claim","961f955b":"## 4.3 - Sex","de10606b":"For some features we have categories and these categories require dummies variables","8df4dc65":"## 6.6 - Mapping categorical embarked feature","026256a0":"To remove outliers I used the code from [here](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling) as guidance\n* We will use 1.5 times the Inter Quartile Range to have close 1% of outliers as compared to 1 times IQR which can give 5% outliers and 2 times IQR which would include most extreme outliers","45c413ce":"The \"Title\" can be extracted from the \"Name\" column by splitting the name and the title is between the comma and the period. ex. Braund, \"Mr\"(Title). Owen Harris","ae004390":"Observations: \n* Fare has a significant effect on the Survival rate, Age has a negative impact whereas Parch and Sibsp have a small impact\n\nAssumptions:\n* High Fare payers may get preference due to t","10175a52":"## 3.1 Observing the data","8c05a85d":"As it was assumed that most of the recorded cabins indeed belong to people with the highest economic status (80%) and thus cabin is directly correlated with the Pclass and having both Pclass and Cabin for out features in modeling will be redundant so we will remove the Cabin column when modeling.","c80bdec6":"Observations:\n* Only Age(170), Cabin(680) and Embarked(2) have missing values\n* The remaining columns have 0 missing values","35fdc14b":"# 3 - Dataset Analysis","457dc95e":"Now we can drop the seperate SibSp and Parch columns","e4e25dc5":"First we will combine the train and test data to ensure that we implement the feature engineering on all data, and we don't have discrepancies when modeling and evaluating.\nWe will split the dataframe again after the feature engineering process.","cd9212d0":"## 6.2 - Family Total","241041b3":"## 6.7 - Removing non-essential features","fa501ed9":"Observations:\n* Mr is the first category (Mr)\n* Miss\/Mme\/Mlle\/Mrs\/Ms can be combined into a single category as they resemble the same Title (Miss)\n* Jonkhee\/Lady\/Countess\/Sir\/Don\/Dona can be categorized into Royals (Royals)\n* Dr\/Major\/Col can be categorized into Professional Category (Professionals)\n* Master\/Rev can be separate category considered both resemble leaders (Mas\/Rev)\n\nWe have in total 5 separate categories","f9bcce6d":"Comments:\n* I decided to use the median technique rather than the mean technique to fill in the missing values to ensure I don't get values in decimal points\n* I used a conditional statement where we check to determine the median age when all three conditions are satified, the conditions being\n    * Same Pclass number\n    * Same number of Siblings\/Spouses (SibSp)   \n    * Same number of Parents\/Children (Parch)\n    \nAs evident from the description of the dataframe above, we have removed all the NaN values in the Age column","31254edd":"Observations:\n* People with smaller familes (1, 2, 3) had a better survival rate than people with larger familes\n* Contrary to expectations, people with no family members had a lower survival rate than people with a few family members","e622db66":"## Comparing the effect of different features on the survival","f6d8a11b":"## 4.6 - Embarked","1a3e7b14":"# 2 - Importing the Dataset","ea0f3d7f":"## 3.4 - Joining Train\/Test","86ae926d":"Observations:\n* of all the females, almost 75% survived\n* of all the males, only about 19% survived\n* This bar plot shows that females are more likely to survive than males ","adce05d9":"Observations:\n* This feature had some unexpected results\n* passengers no Siblings or Spouses had a lower survival rate (35%) than those with 1(56%) or 2(47%) Siblings\/Spouses\n* passengers with 3 or 4 Siblings\/Spouses had a lower survival rate than passengers with 0 Siblings\/Spouses\n* There were no survivors of passengers with siblings more than 4\n\nAssumption:\nThese observations can have meaning considering passenfers with too many Siblings\/Spouses could have been killed in trying to save their large families, and passengers with zero Siblings\/Spouses might not have been given preference.","10df750f":"## 6.5 - Mapping categorical sex feature","7222028d":"Some of these features now are reduntant as we have used feature engineering to extract the important details, such features are:\n* Name, after we extracted the Title feature, it is not useful anymore\n* Parch and SibSp feature as we have combined the 2 into a single feature called Ftotal\n* Ticket  and PassnegerID features as they gives nothing significant for the determination\n* Age as we made age groups instead","8d2ef0c7":"# 1 - Importing the Libraries","f033f1e9":"Observations:\n* of all the Pclass 1 passengers, more than 60% survived\n* of all the Pclass 2 passengers, just over 47% survived\n* of all the Pclass 3 passengers, only 24% survived\n* This bar plot shows that higher economic status of the passengers will have a higher the survival rate","a5ee4bcf":"## 7.2 -  Modelling various classifiers","194fce65":"I used help from [here](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\/) on how to tackle the fare column as fare values were highly skewed towards the low end.","780b482e":"## 4.4 - SibSp","7adbb7d8":"## 4.7 - Cabin","0a8e1ae5":"# 6 - Feature Engineering","c6f61002":"## 4.2 - PClass","9634aa33":"Now the fare is much less skewed as skewness went from 4.51 to 0.56","c9c6eac1":"This validates the assumption about embarked that we made earlier:\n* Queenstown(Q) has almost exclusively Pclass = 3 passengers\n* Southampton(S) has a majority of Pclass = 3 passengers\n* Cherbourg(C) has a majority of Pclass = 1 passengers\n\nThus explaining why Cherbourg(C) has a higher survival rate than the other two embarked locations","d9cdf75a":"Observations:\n* 3 outliers are due to a very high fare\n* 7 Outliers are due to a very high number of SibSp","2292ea90":"Observations:\n* Gender has no effect on the Age feature thus it's safe to assume that to impute the Age column we don't have to consider the age\n* In general, The higher the Pclass, the older the people\n* Parch has a mixed correlation with Age so will need to keep this\n* In general, The lesser the SibSp count, the older the age","2f163586":"## 4.1 - Correlation heatmap","da2a35fd":"# 4 - Visualizing and Comparing the Features","866a827e":"## 6.8 - Get categorical dummies","8a8cb8f4":"## 5.1 - Age","53a71eaf":"If you find this notebook helpful, please upvote and if you have any questions, feel free to ask in the comment section.\nHave a great day :)","267b831a":"Let's convert the Parch and SibSp into a single feature known as Ftotal","f956d4b5":"# Titanic survival determination\nStep by Step Guide:\n1. Importing the Libraries\n2. Importing the Dataset \n3. Dataset Analysis\n    * 3.1 Observing the data\n    * 3.2 Removing outliers   \n    * 3.3 Determining missing values\n    * 3.4 Joining Train\/Test Data\n4. Visualizing and Comparing Features\n    * 4.1 Correlation heatmap \n    * 4.2 Pclass\n    * 4.3 Sex\n    * 4.4 SibSp\n    * 4.5 Parch \n    * 4.6 Embarked\n    * 4.7 Cabin    \n5. Removing Missing Values\n    * 5.1 Age\n6. Feature Engineering \n    * 6.1 Title feature\n    * 6.2 Family total\n    * 6.3 Creating age groups\n    * 6.4 Solving fare skewness \n    * 6.5 Mapping categorical sex feature\n    * 6.6 Mapping categorical embarked feature\n    * 6.7 Removing non-essential features\n    * 6.8 Get categorical dummies\n7. Building\/Training\/Evaluating our models\n    * 7.1 Seperating Train\/Test dataset\n    * 7.2 Modelling various classifiers\n    * 7.3 Hyperparameter tuning\n    * 7.4 Submitting","496321d0":"## 7.3 - Hyperparameter Tuning","71377d98":"## 7.4 - Submitting","65cb551d":"Getting the Title feature from the name","066d78e5":"# 5 - Removing missing values","d9a60fdb":"## 6.3 - Grouping age groups","02d506b8":"Observations:\n* Passengers that embarked from Cherbourg(C), have the highest survival rate at almost 55%\n* Passengers that embarked from Queenstown(Q), have a 40% survical rate\n* Passengers that embarked from Southampton(S), have the lowest survival rate at just over 30%\n\nAssumption: \n* People from Cherbourg may have gotten seat\/cabins that are located on the top or closer to the top and they had easier access for escape or these passengers collectively had a higher economic status than the other places\n\nLet's verify that assumption by correlating the Pclass with the Embarked","b3088964":"Since we have a very big variation in age, we can engineerinf the age feature into age groups","0d3ab5a8":"Inspecting the correlations of various features on age to determine best technique to impute the missing Age data","7fe9bfcf":"# 6.1 - Title feature","8c15c007":"## 3.2 Removing outliers","fdb4b798":"## 3.3 Determining the missing values"}}