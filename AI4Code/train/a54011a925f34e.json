{"cell_type":{"a725ce90":"code","ccb18382":"code","ec4fed38":"code","b4386537":"code","96548f7a":"code","269b341c":"code","a1baeaf7":"code","9f6523a8":"code","e7d8a644":"code","b0989e57":"code","eaa262cc":"code","47fc95ea":"code","708ab972":"code","d76e7ff9":"code","0d149b66":"markdown","099c6199":"markdown","6ac81a6a":"markdown"},"source":{"a725ce90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccb18382":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\nimport PIL.Image\n","ec4fed38":"#train_data\ntrain_data=pd.read_csv('..\/input\/wikipedia-image-caption\/image_data_test\/image_pixels\/test_image_pixels_part-00000.csv', sep='\\t',names=['image_url', 'b64_bytes', 'metadata_url'])\ntrain_data.head()","b4386537":"#test data\ntest_data = pd.read_csv('..\/input\/wikipedia-image-caption\/test.tsv', sep='\\t')\ntest_data.head()","96548f7a":"#submission File\nsub_file = pd.read_csv('..\/input\/wikipedia-image-caption\/sample_submission.csv')\nsub_file.head()\n","269b341c":"#dataset info\nprint(\"shape of train file\",train_data.shape)\nprint(\"shape of test file\",test_data.shape)\nprint(\"shape of Submission file\",sub_file.shape)","a1baeaf7":"train_data.info()","9f6523a8":"#language distribution plot\nplt.figure(figsize=(18, 8))\nsns.set(style=\"darkgrid\")\nsns.countplot(test_data[\"language\"])\nplt.show()","e7d8a644":"import urllib\ndef get_links(df, num):\n    return df.image_url[10:num].values\n\nlinks = get_links(train_data, 21)\n\ndef load_images(links):\n    images = []\n    \n    for link in links:\n        URL = link\n        try:\n\n            with urllib.request.urlopen(URL) as url:\n                with open('.\/temp.jpg', 'wb') as f:\n                    f.write(url.read())\n\n            img = PIL.Image.open('.\/temp.jpg')\n            img = np.asarray(img)\n            images.append(img)\n        except:\n            continue\n    return images\n\ndef display_images(images, title=None): \n    f, ax = plt.subplots(2,5, figsize=(18,12))\n    if title:\n        f.suptitle(title, fontsize = 30)\n\n    for i, image_id in enumerate(images):\n        ax[i\/\/5, i%5].imshow(image_id) \n   \n        ax[i\/\/5, i%5].axis('off')\n\n    plt.show() \n#code taken from https:\/\/www.kaggle.com\/hijest\/wikipedia-image-caption-matching-starter-eda ","b0989e57":"images = load_images(links)\ndisplay_images(images)","eaa262cc":"import matplotlib.pyplot as plt\nimport squarify    \nsquarify.plot(sizes=test_data['language'].value_counts().values, \n              label=test_data['language'].value_counts().index )\nplt.axis('off')\nplt.show()","47fc95ea":"df = pd.read_csv('..\/input\/wikipedia-image-caption\/train-00000-of-00005.tsv', sep='\\t',nrows=100)\ndf.head()","708ab972":"from wordcloud import WordCloud, STOPWORDS \ncloud = WordCloud(collocations = False,stopwords={'nan'},background_color='white').generate(\" \".join(df['page_title'].astype(str)))\nplt.figure(figsize=(16, 10))\nplt.title('WordCloud ',fontsize=20,pad=40)\nplt.imshow(cloud,interpolation='bilinear')\nplt.axis('off')","d76e7ff9":"#Work in progress","0d149b66":"description: most of the images have english caption. ","099c6199":"<p> Description:\nA picture is worth a thousand words, yet sometimes a few will do. We all rely on online images for knowledge sharing, learning, and understanding. Even the largest websites are missing visual content and metadata to pair with their images. Captions and \u201calt text\u201d increase accessibility and enable better search. The majority of images on Wikipedia articles, for example, don't have any written context connected to the image. Open models could help anyone improve accessibility and learning for all.\n\nCurrent solutions rely on simple methods based on translations or page interlinks, which have limited coverage. Even the most advanced computer vision image captioning isn't suitable for images with complex semantics.\n\nIn this competition, you\u2019ll build a model that automatically retrieves the text closest to an image. Specifically, you'll train your model to associate given images with article titles or complex captions, in multiple languages. The best models will account for the semantic granularity of Wikipedia images.<\/p>","6ac81a6a":"## **Wikipedia - Image\/Caption Matching**\n#### Retrieve captions based on images"}}