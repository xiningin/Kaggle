{"cell_type":{"98baa5c3":"code","72a9bb00":"code","d37b0929":"code","18e767f1":"code","b36863af":"code","c6b8c837":"code","dab1e80a":"code","cc167c6e":"code","b2440e28":"code","b3601b66":"code","53769505":"markdown","9765291a":"markdown","877f270d":"markdown","adf5d808":"markdown","8f47447c":"markdown","4bb6234b":"markdown","4c792cac":"markdown","56a2c49a":"markdown","074ce128":"markdown","5289d513":"markdown","867a8167":"markdown"},"source":{"98baa5c3":"import numpy as np\nimport pandas as pd\nimport os, math, sys\nimport time, datetime\nimport glob, itertools\nimport argparse, random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nfrom torch.autograd import Variable\nfrom torchvision.models import vgg19\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import save_image, make_grid\n\nimport plotly\nfrom scipy import signal\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\nimport warnings\nwarnings.filterwarnings(\"ignore\")","72a9bb00":"# path to pre-trained models\npretrained_model_path = \"..\/input\/cyclegan-translating-edges-shoes-pytorch\/saved_models\"\n# epoch to start training from\nepoch_start = 2\n# number of epochs of training\nn_epochs = 3\n# name of the dataset\ndataset_path = \"..\/input\/edges2shoes-dataset\"\n# size of the batches\"\nbatch_size = 4\n# adam: learning rate\nlr = 0.00012\n# adam: decay of first order momentum of gradient\nb1 = 0.5\n# adam: decay of first order momentum of gradient\nb2 = 0.999\n# epoch from which to start lr decay\ndecay_epoch = 1\n# number of cpu threads to use during batch generation\nn_workers = 8\n# size of image height\nimg_height = 256\n# size of image width\nimg_width = 256\n# number of image channels\nchannels = 3\n# interval between saving generator outputs\nsample_interval = 100\n# interval between saving model checkpoints\ncheckpoint_interval = -1\n# number of residual blocks in generator\nn_residual_blocks = 9\n# cycle loss weight\nlambda_cyc = 10.0\n# identity loss weight\nlambda_id = 5.0\n# Development \/ Debug Mode\ndebug_mode = False\n\n# Create images and checkpoint directories\nos.makedirs(\"images\", exist_ok=True)\nos.makedirs(\"saved_models\", exist_ok=True)","d37b0929":"def to_rgb(image):\n    rgb_image = Image.new(\"RGB\", image.size)\n    rgb_image.paste(image)\n    return rgb_image\n\n\nclass ReplayBuffer:\n    def __init__(self, max_size=50):\n        assert max_size > 0\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return Variable(torch.cat(to_return))\n","18e767f1":"class ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None, mode=\"train\"):\n        self.transform = transforms.Compose(transforms_)\n        self.files = sorted(glob.glob(os.path.join(root, mode) + \"\/*.*\"))\n        if debug_mode:\n            self.files = self.files[:100]\n\n    def __getitem__(self, index):\n\n        img = Image.open(self.files[index % len(self.files)])\n        w, h = img.size\n        img_A = img.crop((0, 0, w \/ 2, h))\n        img_B = img.crop((w \/ 2, 0, w, h))\n\n        if np.random.random() < 0.5:\n            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n\n        img_A = self.transform(img_A)\n        img_B = self.transform(img_B)\n\n        return {\"A\": img_A, \"B\": img_B}\n\n    def __len__(self):\n        return len(self.files)","b36863af":"# Image transformations\ntransforms_ = [\n    transforms.Resize((img_height, img_width), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n]\n\n# Training data loader\ntrain_dataloader = DataLoader(\n    ImageDataset(f\"{dataset_path}\", transforms_=transforms_),\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=n_workers,\n)\n# Test data loader\ntest_dataloader = DataLoader(\n    ImageDataset(f\"{dataset_path}\", transforms_=transforms_, mode=\"val\"),\n    batch_size=1,\n    shuffle=True,\n    num_workers=1,\n)","c6b8c837":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n        if hasattr(m, \"bias\") and m.bias is not None:\n            torch.nn.init.constant_(m.bias.data, 0.0)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_features):\n        super(ResidualBlock, self).__init__()\n\n        self.block = nn.Sequential(\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n            nn.ReLU(inplace=True),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(in_features, in_features, 3),\n            nn.InstanceNorm2d(in_features),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\n\nclass GeneratorResNet(nn.Module):\n    def __init__(self, input_shape, num_residual_blocks):\n        super(GeneratorResNet, self).__init__()\n\n        channels = input_shape[0]\n\n        # Initial convolution block\n        out_features = 64\n        model = [\n            nn.ReflectionPad2d(channels),\n            nn.Conv2d(channels, out_features, 7),\n            nn.InstanceNorm2d(out_features),\n            nn.ReLU(inplace=True),\n        ]\n        in_features = out_features\n\n        # Downsampling\n        for _ in range(2):\n            out_features *= 2\n            model += [\n                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Residual blocks\n        for _ in range(num_residual_blocks):\n            model += [ResidualBlock(out_features)]\n\n        # Upsampling\n        for _ in range(2):\n            out_features \/\/= 2\n            model += [\n                nn.Upsample(scale_factor=2),\n                nn.Conv2d(in_features, out_features, 3, stride=1, padding=1),\n                nn.InstanceNorm2d(out_features),\n                nn.ReLU(inplace=True),\n            ]\n            in_features = out_features\n\n        # Output layer\n        model += [nn.ReflectionPad2d(channels), nn.Conv2d(out_features, channels, 7), nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, input_shape):\n        super(Discriminator, self).__init__()\n\n        channels, height, width = input_shape\n\n        # Calculate output shape of image discriminator (PatchGAN)\n        self.output_shape = (1, height \/\/ 2 ** 4, width \/\/ 2 ** 4)\n\n        def discriminator_block(in_filters, out_filters, normalize=True):\n            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n            if normalize:\n                layers.append(nn.InstanceNorm2d(out_filters))\n            layers.append(nn.LeakyReLU(0.2, inplace=True))\n            return layers\n\n        self.model = nn.Sequential(\n            *discriminator_block(channels, 64, normalize=False),\n            *discriminator_block(64, 128),\n            *discriminator_block(128, 256),\n            *discriminator_block(256, 512),\n            nn.ZeroPad2d((1, 0, 1, 0)),\n            nn.Conv2d(512, 1, 4, padding=1)\n        )\n\n    def forward(self, img):\n        return self.model(img)","dab1e80a":"# Losses\ncriterion_GAN = torch.nn.MSELoss()\ncriterion_cycle = torch.nn.L1Loss()\ncriterion_identity = torch.nn.L1Loss()\n\ncuda = torch.cuda.is_available()\n\ninput_shape = (channels, img_height, img_width)\n\n# Initialize generator and discriminator\nG_AB = GeneratorResNet(input_shape, n_residual_blocks)\nG_BA = GeneratorResNet(input_shape, n_residual_blocks)\nD_A = Discriminator(input_shape)\nD_B = Discriminator(input_shape)\n\nif cuda:\n    G_AB = G_AB.cuda()\n    G_BA = G_BA.cuda()\n    D_A = D_A.cuda()\n    D_B = D_B.cuda()\n    criterion_GAN.cuda()\n    criterion_cycle.cuda()\n    criterion_identity.cuda()\n\nif epoch_start != 0:\n    # Load pretrained models\n    G_AB.load_state_dict(torch.load(f\"{pretrained_model_path}\/G_AB.pth\"))\n    G_BA.load_state_dict(torch.load(f\"{pretrained_model_path}\/G_BA.pth\"))\n    D_A.load_state_dict(torch.load(f\"{pretrained_model_path}\/D_A.pth\"))\n    D_B.load_state_dict(torch.load(f\"{pretrained_model_path}\/D_B.pth\"))\nelse:\n    # Initialize weights\n    G_AB.apply(weights_init_normal)\n    G_BA.apply(weights_init_normal)\n    D_A.apply(weights_init_normal)\n    D_B.apply(weights_init_normal)\n\n# Optimizers\noptimizer_G = torch.optim.Adam(\n    itertools.chain(G_AB.parameters(), G_BA.parameters()), lr=lr, betas=(b1, b2)\n)\noptimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=(b1, b2))\noptimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=(b1, b2))\n\nTensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n\n# Buffers of previously generated samples\nfake_A_buffer = ReplayBuffer()\nfake_B_buffer = ReplayBuffer()\n\ntrain_counter = []\ntrain_losses_gen, train_losses_id, train_losses_gan, train_losses_cyc = [], [], [], []\ntrain_losses_disc, train_losses_disc_a, train_losses_disc_b = [], [], []\n\ntest_counter = [2*idx*len(train_dataloader.dataset) for idx in range(epoch_start+1, n_epochs+1)]\ntest_losses_gen, test_losses_disc = [], []\n","cc167c6e":"for epoch in range(epoch_start, n_epochs):\n    \n    #### Training\n    loss_gen = loss_id = loss_gan = loss_cyc = 0.0\n    loss_disc = loss_disc_a = loss_disc_b = 0.0\n    tqdm_bar = tqdm(train_dataloader, desc=f'Training Epoch {epoch} ', total=int(len(train_dataloader)))\n    for batch_idx, batch in enumerate(tqdm_bar):\n\n        # Set model input\n        real_A = Variable(batch[\"A\"].type(Tensor))\n        real_B = Variable(batch[\"B\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n\n        ### Train Generators\n        G_AB.train()\n        G_BA.train()\n        optimizer_G.zero_grad()\n        # Identity loss\n        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n        loss_identity = (loss_id_A + loss_id_B) \/ 2\n        # GAN loss\n        fake_B = G_AB(real_A)\n        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n        fake_A = G_BA(real_B)\n        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n        loss_GAN = (loss_GAN_AB + loss_GAN_BA) \/ 2\n        # Cycle loss\n        recov_A = G_BA(fake_B)\n        loss_cycle_A = criterion_cycle(recov_A, real_A)\n        recov_B = G_AB(fake_A)\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\n        loss_cycle = (loss_cycle_A + loss_cycle_B) \/ 2\n        # Total loss\n        loss_G = lambda_id * loss_identity + loss_GAN + lambda_cyc * loss_cycle\n        loss_G.backward()\n        optimizer_G.step()\n\n        ### Train Discriminator-A\n        D_A.train()\n        optimizer_D_A.zero_grad()\n        # Real loss\n        loss_real = criterion_GAN(D_A(real_A), valid)\n        # Fake loss (on batch of previously generated samples)\n        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n        # Total loss\n        loss_D_A = (loss_real + loss_fake) \/ 2\n        loss_D_A.backward()\n        optimizer_D_A.step()\n\n        ### Train Discriminator-B\n        D_B.train()\n        optimizer_D_B.zero_grad()\n        # Real loss\n        loss_real = criterion_GAN(D_B(real_B), valid)\n        # Fake loss (on batch of previously generated samples)\n        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n        # Total loss\n        loss_D_B = (loss_real + loss_fake) \/ 2\n        loss_D_B.backward()\n        optimizer_D_B.step()\n        loss_D = (loss_D_A + loss_D_B) \/ 2\n\n        ### Log Progress\n        loss_gen += loss_G.item(); loss_id += loss_identity.item(); loss_gan += loss_GAN.item(); loss_cyc += loss_cycle.item()\n        loss_disc += loss_D.item(); loss_disc_a += loss_D_A.item(); loss_disc_b += loss_D_B.item()\n        train_counter.append(2*(batch_idx*batch_size + real_A.size(0) + epoch*len(train_dataloader.dataset)))\n        train_losses_gen.append(loss_G.item()); train_losses_id.append(loss_identity.item()); train_losses_gan.append(loss_GAN.item()); train_losses_cyc.append(loss_cycle.item())\n        train_losses_disc.append(loss_D.item()); train_losses_disc_a.append(loss_D_A.item()); train_losses_disc_b.append(loss_D_B.item())\n        tqdm_bar.set_postfix(Gen_loss=loss_gen\/(batch_idx+1), identity=loss_id\/(batch_idx+1), adv=loss_gan\/(batch_idx+1), cycle=loss_cyc\/(batch_idx+1),\n                            Disc_loss=loss_disc\/(batch_idx+1), disc_a=loss_disc_a\/(batch_idx+1), disc_b=loss_disc_b\/(batch_idx+1))\n\n    #### Testing\n    loss_gen = loss_id = loss_gan = loss_cyc = 0.0\n    loss_disc = loss_disc_a = loss_disc_b = 0.0\n    tqdm_bar = tqdm(test_dataloader, desc=f'Testing Epoch {epoch} ', total=int(len(test_dataloader)))\n    for batch_idx, batch in enumerate(tqdm_bar):\n\n        # Set model input\n        real_A = Variable(batch[\"A\"].type(Tensor))\n        real_B = Variable(batch[\"B\"].type(Tensor))\n        # Adversarial ground truths\n        valid = Variable(Tensor(np.ones((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n        fake = Variable(Tensor(np.zeros((real_A.size(0), *D_A.output_shape))), requires_grad=False)\n\n        ### Test Generators\n        G_AB.eval()\n        G_BA.eval()\n        # Identity loss\n        loss_id_A = criterion_identity(G_BA(real_A), real_A)\n        loss_id_B = criterion_identity(G_AB(real_B), real_B)\n        loss_identity = (loss_id_A + loss_id_B) \/ 2\n        # GAN loss\n        fake_B = G_AB(real_A)\n        loss_GAN_AB = criterion_GAN(D_B(fake_B), valid)\n        fake_A = G_BA(real_B)\n        loss_GAN_BA = criterion_GAN(D_A(fake_A), valid)\n        loss_GAN = (loss_GAN_AB + loss_GAN_BA) \/ 2\n        # Cycle loss\n        recov_A = G_BA(fake_B)\n        loss_cycle_A = criterion_cycle(recov_A, real_A)\n        recov_B = G_AB(fake_A)\n        loss_cycle_B = criterion_cycle(recov_B, real_B)\n        loss_cycle = (loss_cycle_A + loss_cycle_B) \/ 2\n        # Total loss\n        loss_G = loss_GAN + lambda_cyc * loss_cycle + lambda_id * loss_identity\n\n        ### Test Discriminator-A\n        D_A.eval()\n        # Real loss\n        loss_real = criterion_GAN(D_A(real_A), valid)\n        # Fake loss (on batch of previously generated samples)\n        fake_A_ = fake_A_buffer.push_and_pop(fake_A)\n        loss_fake = criterion_GAN(D_A(fake_A_.detach()), fake)\n        # Total loss\n        loss_D_A = (loss_real + loss_fake) \/ 2\n\n        ### Test Discriminator-B\n        D_B.eval()\n        # Real loss\n        loss_real = criterion_GAN(D_B(real_B), valid)\n        # Fake loss (on batch of previously generated samples)\n        fake_B_ = fake_B_buffer.push_and_pop(fake_B)\n        loss_fake = criterion_GAN(D_B(fake_B_.detach()), fake)\n        # Total loss\n        loss_D_B = (loss_real + loss_fake) \/ 2\n        loss_D = (loss_D_A + loss_D_B) \/ 2\n        \n        ### Log Progress\n        loss_gen += loss_G.item(); loss_id += loss_identity.item(); loss_gan += loss_GAN.item(); loss_cyc += loss_cycle.item()\n        loss_disc += loss_D.item(); loss_disc_a += loss_D_A.item(); loss_disc_b += loss_D_B.item()\n        tqdm_bar.set_postfix(Gen_loss=loss_gen\/(batch_idx+1), identity=loss_id\/(batch_idx+1), adv=loss_gan\/(batch_idx+1), cycle=loss_cyc\/(batch_idx+1),\n                            Disc_loss=loss_disc\/(batch_idx+1), disc_a=loss_disc_a\/(batch_idx+1), disc_b=loss_disc_b\/(batch_idx+1))\n        \n        # If at sample interval save image\n        if random.uniform(0,1)<0.4:\n            # Arrange images along x-axis\n            real_A = make_grid(real_A, nrow=1, normalize=True)\n            real_B = make_grid(real_B, nrow=1, normalize=True)\n            fake_A = make_grid(fake_A, nrow=1, normalize=True)\n            fake_B = make_grid(fake_B, nrow=1, normalize=True)\n            # Arange images along y-axis\n            image_grid = torch.cat((real_A, fake_B, real_B, fake_A), -1)\n            save_image(image_grid, f\"images\/{batch_idx}.png\", normalize=False)\n\n    test_losses_gen.append(loss_gen\/len(test_dataloader))\n    test_losses_disc.append(loss_disc\/len(test_dataloader))\n\n    # Save model checkpoints\n    if np.argmin(test_losses_gen) == len(test_losses_gen)-1:\n        # Save model checkpoints\n        torch.save(G_AB.state_dict(), \"saved_models\/G_AB.pth\")\n        torch.save(G_BA.state_dict(), \"saved_models\/G_BA.pth\")\n        torch.save(D_A.state_dict(), \"saved_models\/D_A.pth\")\n        torch.save(D_B.state_dict(), \"saved_models\/D_B.pth\")\n        ","b2440e28":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_gen, mode='lines', name='Train Gen Loss (Loss_G)'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_id, mode='lines', name='Train Gen Identity Loss'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_gan, mode='lines', name='Train Gen GAN Loss'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_cyc, mode='lines', name='Train Gen Cyclic Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses_gen, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Gen Loss (Loss_G)'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Generator Loss\",\n    xaxis_title=\"Number of training examples seen (A+B)\",\n    yaxis_title=\"Generator Losses\"),\nplotly.offline.plot(fig, filename = 'plotly_gen_losses.html')\nfig.show()","b3601b66":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc, mode='lines', name='Train Disc Loss (Loss_D)'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc_a, mode='lines', name='Train Disc-A Loss'))\nfig.add_trace(go.Scatter(x=train_counter, y=train_losses_disc_b, mode='lines', name='Train Disc-B Loss'))\nfig.add_trace(go.Scatter(x=test_counter, y=test_losses_disc, marker_symbol='star-diamond', \n                         marker_color='orange', marker_line_width=1, marker_size=9, mode='markers', name='Test Disc Loss (Loss_G)'))\nfig.update_layout(\n    width=1000,\n    height=500,\n    title=\"Train vs. Test Discriminator Loss\",\n    xaxis_title=\"Number of training examples seen (A+B)\",\n    yaxis_title=\"Discriminator Losses\"),\nplotly.offline.plot(fig, filename = 'plotly_disc_losses.html')\nfig.show()","53769505":"### Define Dataset Class","9765291a":"### Get Train\/Test Dataloaders","877f270d":"## Introduction\n### In this notebook we use [CycleGAN](https:\/\/arxiv.org\/abs\/1703.10593) to translate Shoe-Edges to Shoes using [Edges2shoes Dataset](https:\/\/www.kaggle.com\/balraj98\/edges2shoes-dataset).","adf5d808":"### Define Utilities","8f47447c":"<img src=\"https:\/\/user-images.githubusercontent.com\/7057863\/49054732-354dad80-f1c3-11e8-8406-5b3570a8cc47.png\" width=\"800\" height=\"800\"\/>\n<h4><\/h4>\n<h4><center>Image Source:  <a href=\"https:\/\/people.eecs.berkeley.edu\/~tinghuiz\/projects\/pix2pix\/datasets\/\">Edges2shoes Dataset<\/a><\/center><\/h4>","4bb6234b":"### Libraries \ud83d\udcda\u2b07","4c792cac":"### Work in Progress ...\n\n### Note: The order of the below saved images are (from left-to-right): \n\n* Real Shoe Edge\n* Fake Shoe (from Gen-AB)\n* Real Shoe\n* Fake Shoe Edge (from Gen-BA)","56a2c49a":"<h3><center>CycleGAN Model Architecture<\/center><\/h3>\n<img src=\"https:\/\/miro.medium.com\/max\/700\/1*_KxtJIVtZjVaxxl-Yl1vJg.png\" width=\"900\" height=\"900\"\/>\n<h4><\/h4>\n<h4><center>Image Source:  <a href=\"https:\/\/arxiv.org\/pdf\/1703.10593.pdf\">Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks [C. Jun-Yan Zhu et al.]<\/a><\/center><\/h4>","074ce128":"### Settings","5289d513":"### Train CycleGAN","867a8167":"### Define Model Classes"}}