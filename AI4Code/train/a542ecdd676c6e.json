{"cell_type":{"a96e53d7":"code","5c481945":"code","cc42e826":"code","205b9405":"code","584c9cf2":"code","c9dc5b17":"code","757c7c37":"code","8760fdfa":"code","1014a35d":"code","0a627fcb":"code","3a39b12f":"code","1b1a958e":"code","594e1000":"code","d1a34078":"code","ad846ad9":"code","68fe93f8":"code","4b1e4551":"code","0f34c134":"code","8b46e3e3":"code","0849b9e4":"code","91361b8c":"markdown","a23a6229":"markdown"},"source":{"a96e53d7":"#from google.colab import drive\n#drive.mount('\/content\/gdrive')\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom pylab import rcParams\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\n#from keras.callbacks import ModelCheckpoint, EarlyStoping\nfrom keras import regularizers\n\nfolder_pathin = '..\/input\/fraud-detection-processed-dara\/'\nX_train = pd.read_csv(f'{folder_pathin}X_ae_150_train.csv')\nX_valid = pd.read_csv(f'{folder_pathin}X_ae_150_test.csv')\ny_valid = pd.read_csv(f'{folder_pathin}y_ae_test.csv')","5c481945":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(X_train)\nscaler.transform(X_train)\nX_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\nX_valid = pd.DataFrame(scaler.transform(X_valid), columns=X_valid.columns)","cc42e826":"train_data, valid_data = train_test_split(X_train, test_size=0.2, random_state=42)\n\ninput_dim = train_data.shape[1]","205b9405":"train_data.shape","584c9cf2":"nb_epoch = 200\nbatch_size = 2048\n\n#encoding_dim = int(input_dim\/2)\nencoding_dim = input_dim\nlearning_rate = 1e-3","c9dc5b17":"from keras.layers import Dense, BatchNormalization, Activation\nimport tensorflow as tf\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense, Layer, InputSpec\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers, activations, initializers, constraints, Sequential\nfrom keras import backend as K\nfrom keras.constraints import UnitNorm, Constraint","757c7c37":"from keras.callbacks import ModelCheckpoint, EarlyStopping\n\nDO_TRANING = True\n \n#autoencoder.compile(optimizer='adam', \n#                    loss='mean_squared_error', \n#                    metrics=['accuracy'])\n \ncheckpointer = ModelCheckpoint(filepath=\"model.h5\",\n                               verbose=0,\n                               save_best_only=True)\n\nearlystoping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')","8760fdfa":"def train_AE(autoencoder):\n  if DO_TRANING:\n     history = autoencoder.fit(train_data, train_data,\n                               epochs=nb_epoch,\n                               batch_size=batch_size,\n                               shuffle=True,\n                               validation_data=(valid_data, valid_data),\n                               verbose=1,\n                               callbacks=[checkpointer, earlystoping]).history\n\n     # Model loss\n     plt.plot(history['loss'])\n     plt.plot(history['val_loss'])\n     plt.title('model loss')\n     plt.ylabel('acc')\n     plt.xlabel('epoch')\n     plt.legend(['train', 'val'], loc='upper right');\n     plt.show()\n     \n  return(autoencoder)","1014a35d":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)\n\ndef result_visualization(autoencoder):\n  predictions = autoencoder.predict(X_valid)\n\n  # X_test \u30c7\u30fc\u30bf\u3068\u518d\u73fe\u30c7\u30fc\u30bf\u306e mse(\u5e73\u5747\uff12\u4e57\u8aa4\u5dee)\u3092\u8a08\u7b97\u3059\u308b\n  mse = np.mean(np.power(X_valid - predictions, 2), axis=1)\n\n  error_df = pd.DataFrame(mse).join(y_valid)\n  error_df = error_df.rename(columns = {0:\"reconstruction_error\", \"isfraut\":\"true_class\"})\n  #error_df = error_df.rename(columns = {\"reconstruction_error\":0, \"true_class\":\"isfraut\"})\n  error_df = error_df.rename(columns = {\"isfraud\":\"true_class\"})\n  \n  print(\"true_class = 1.0\")\n  print(error_df[error_df.true_class == 1.0].describe())\n  print(\" \")\n  print(\"true_class = 0.0\")\n  print(error_df[error_df.true_class == 0.0].describe())\n  print(\" \")\n  \n  fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n  roc_auc = auc(fpr, tpr)\n\n  plt.title('ROC curve')\n  plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n  plt.legend(loc='lower right')\n  plt.plot([0,1],[0,1],'r--')\n  plt.xlim([-0.001, 1])\n  plt.ylim([0, 1.001])\n  plt.ylabel('True Positive Rate')\n  plt.xlabel('False Positive Rate')\n  plt.show();\n\n\n  # get data(precision, recall, th)\n  precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n\n  # Precision & Recall  vs  Reconstruction error\n  plt.plot(th, precision[1:], 'b', label='Precision curve', color='r')\n  plt.plot(th, recall[1:], 'b', label='Recall curve', color='b')\n  plt.title('Precision and Recall  vs  mse')\n  plt.xlabel('mse')\n  plt.ylabel('Precision and Recall')\n  #plt.xlim(0, 250)\n  plt.xlim([0, 1.0]) #I'm sorry that I should have put this.\n  plt.legend(loc=1)\n  plt.show()\n  \n  print(\" \")\n  # Reconstruction error for different classes\n  thresholds = [0.1,0.15, 0.2,0.25, 0.3]\n  \n  for threshold in thresholds:\n      \n    groups = error_df.groupby('true_class')\n    fig, ax = plt.subplots()\n\n    for name, group in groups:\n        ax.plot(group.index, group.reconstruction_error, marker='o', ms=0.1, linestyle='',\n                label= \"Fraud\" if name == 1 else \"Normal\")\n    ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"g\", zorder=100, label='Threshold')\n    ax.legend()\n    ax.set_ylim(0, 1.0)  \n    #ax2.set_ylim(0, .12)  \n    plt.title(\"mse for different classes\")\n    plt.ylabel(\"mse\")\n    plt.xlabel(\"Data point index\")\n    plt.show();\n\n    # Confusion matrix\n    y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n    conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n\n    plt.figure(figsize=(5, 5))\n    LABELS = [\"Normal\", \"Fraud\"]\n    sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\", square = True);\n    plt.title(\"Confusion matrix\")\n    plt.ylabel('True class')\n    plt.xlabel('Predicted class')\n    plt.show()","0a627fcb":"# baseline_model\n\nencoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True) \ndecoder = Dense(input_dim, activation=\"relu\", use_bias = True)\n\nautoencoder_0 = Sequential()\nautoencoder_0.add(encoder)\nautoencoder_0.add(decoder)\n\nautoencoder_0.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_0.summary()\n\nautoencoder_trained_0 = train_AE(autoencoder_0)\n\nresult_visualization(autoencoder_trained_0)","3a39b12f":"#Autoencoder Optimization\nclass DenseTied(Layer):\n    def __init__(self, units,\n                 activation=None,\n                 use_bias=True,\n                 kernel_initializer='glorot_uniform',\n                 bias_initializer='zeros',\n                 kernel_regularizer=None,\n                 bias_regularizer=None,\n                 activity_regularizer=None,\n                 kernel_constraint=None,\n                 bias_constraint=None,\n                 tied_to=None,\n                 **kwargs):\n        self.tied_to = tied_to\n        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.input_spec = InputSpec(min_ndim=2)\n        self.supports_masking = True\n                \n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n\n        if self.tied_to is not None:\n            self.kernel = K.transpose(self.tied_to.kernel)\n            self._non_trainable_weights.append(self.kernel)\n        else:\n            self.kernel = self.add_weight(shape=(input_dim, self.units),\n                                          initializer=self.kernel_initializer,\n                                          name='kernel',\n                                          regularizer=self.kernel_regularizer,\n                                          constraint=self.kernel_constraint)\n        if self.use_bias:\n            self.bias = self.add_weight(shape=(self.units,),\n                                        initializer=self.bias_initializer,\n                                        name='bias',\n                                        regularizer=self.bias_regularizer,\n                                        constraint=self.bias_constraint)\n        else:\n            self.bias = None\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) >= 2\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        return tuple(output_shape)\n\n    def call(self, inputs):\n        output = K.dot(inputs, self.kernel)\n        if self.use_bias:\n            output = K.bias_add(output, self.bias, data_format='channels_last')\n        if self.activation is not None:\n            output = self.activation(output)\n        return output","1b1a958e":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True) \ndecoder = DenseTied(input_dim, activation=\"relu\", tied_to=encoder, use_bias = True)\nautoencoder_1 = Sequential()\nautoencoder_1.add(encoder)\nautoencoder_1.add(decoder)\nautoencoder_1.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_1.summary()\n\nautoencoder_trained_1 = train_AE(autoencoder_1)\n\nresult_visualization(autoencoder_trained_1)","594e1000":"class WeightsOrthogonalityConstraint (Constraint):\n    def __init__(self, encoding_dim, weightage = 1.0, axis = 0):\n        self.encoding_dim = encoding_dim\n        self.weightage = weightage\n        self.axis = axis\n        \n    def weights_orthogonality(self, w):\n        if(self.axis==1):\n            w = K.transpose(w)\n        if(self.encoding_dim > 1):\n            m = K.dot(K.transpose(w), w) - K.eye(self.encoding_dim)\n            return self.weightage * K.sqrt(K.sum(K.square(m)))\n        else:\n            m = K.sum(w ** 2) - 1.\n            return m\n\n    def __call__(self, w):\n        return self.weights_orthogonality(w)","d1a34078":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias=True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0)) \ndecoder = Dense(input_dim, activation=\"relu\", use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=1))\n\nautoencoder_2 = Sequential()\nautoencoder_2.add(encoder)\nautoencoder_2.add(decoder)\n\nautoencoder_2.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_2.summary()\n\nautoencoder_trained_2 = train_AE(autoencoder_2)\n\nresult_visualization(autoencoder_trained_2)","ad846ad9":"class UncorrelatedFeaturesConstraint (Constraint):\n    \n    def __init__(self, encoding_dim, weightage = 1.0):\n        self.encoding_dim = encoding_dim\n        self.weightage = weightage\n    \n    def get_covariance(self, x):\n        x_centered_list = []\n\n        for i in range(self.encoding_dim):\n            x_centered_list.append(x[:, i] - K.mean(x[:, i]))\n        \n        x_centered = tf.stack(x_centered_list)\n        covariance = K.dot(x_centered, K.transpose(x_centered)) \/ tf.cast(x_centered.get_shape()[0], tf.float32)\n        \n        return covariance\n            \n    # Constraint penalty\n    def uncorrelated_feature(self, x):\n        if(self.encoding_dim <= 1):\n            return 0.0\n        else:\n            output = K.sum(K.square(self.covariance - K.dot(self.covariance, K.eye(self.encoding_dim))))\n            return output\n\n    def __call__(self, x):\n        self.covariance = self.get_covariance(x)\n        return self.weightage * self.uncorrelated_feature(x)","68fe93f8":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, activity_regularizer=UncorrelatedFeaturesConstraint(encoding_dim, weightage = 1.)) \ndecoder = Dense(input_dim, activation=\"relu\", use_bias = True)\n\nautoencoder_3 = Sequential()\nautoencoder_3.add(encoder)\nautoencoder_3.add(decoder)\n\nautoencoder_3.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_3.summary()\n\nautoencoder_trained_3 = train_AE(autoencoder_3)\n\nresult_visualization(autoencoder_trained_3)","4b1e4551":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, kernel_constraint=UnitNorm(axis=0)) \ndecoder = Dense(input_dim, activation=\"relu\", use_bias = True, kernel_constraint=UnitNorm(axis=1))\nautoencoder_4 = Sequential()\nautoencoder_4.add(encoder)\nautoencoder_4.add(decoder)\nautoencoder_4.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                    optimizer='adam')\nautoencoder_4.summary()\n\nautoencoder_trained_4 = train_AE(autoencoder_4)\n\nresult_visualization(autoencoder_trained_4)","0f34c134":"encoder = Dense(encoding_dim, activation=\"relu\", input_shape=(input_dim,), use_bias = True, kernel_regularizer=WeightsOrthogonalityConstraint(encoding_dim, weightage=1., axis=0), kernel_constraint=UnitNorm(axis=0)) \ndecoder = DenseTied(input_dim, activation=\"relu\", tied_to=encoder, use_bias = False)\nautoencoder_5 = Sequential()\nautoencoder_5.add(encoder)\nautoencoder_5.add(decoder)\nautoencoder_5.compile(metrics=['accuracy'],\n                    loss='mean_squared_error',\n                      optimizer='adam')\nautoencoder_5.summary()\n\nautoencoder_trained_5 = train_AE(autoencoder_5)\n\nresult_visualization(autoencoder_trained_5)","8b46e3e3":"autoencoder_trained_0.save(\"model_0.h5\")","0849b9e4":"autoencoder_trained_4.save(\"model_4.h5\")","91361b8c":"Well, I feel that it is really difficult to set proper threshold.\nThe Auc and ROC carve are not so bad, but I can't take advantage of them. So, plz give me advice! ","a23a6229":"In this notebook, I will try to build some Autoencoder models for anomaly detection. \nMy autoencoder models are mainly based on this article \nhttps:\/\/towardsdatascience.com\/build-the-right-autoencoder-tune-and-optimize-using-pca-principles-part-ii-24b9cca69bd6\n\nAlso, every training data is pre-processed. (I refered following notebook and pick up top-150 features)\nhttps:\/\/www.kaggle.com\/artgor\/eda-and-models\n\nI built 5 models and the last one is a conbination of others."}}