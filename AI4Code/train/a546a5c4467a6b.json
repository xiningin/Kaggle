{"cell_type":{"b5a02b9b":"code","a710a82f":"code","57600b7f":"code","00fb5508":"code","e70a3076":"code","add10af8":"code","cffada0a":"code","19a241d6":"code","6fcf93f5":"code","d6bab383":"code","a019fa65":"code","4b28cd96":"code","572dc136":"code","85e96a3c":"code","e699387e":"code","c5608154":"code","d5add31f":"code","c920513a":"code","d7d2bff2":"code","3a98437f":"code","ddaa83f2":"markdown","0e85c2b3":"markdown","f0d8a93b":"markdown","ff805523":"markdown","86e32603":"markdown","8fbd755d":"markdown","6671a2d6":"markdown","f89f2436":"markdown","f48f9f87":"markdown","659bc9e5":"markdown","46c81c2c":"markdown","8446b8ab":"markdown","48519a33":"markdown","9ec87bcb":"markdown","ad3a49b1":"markdown","5398065e":"markdown","768498ee":"markdown","a2eb7ff3":"markdown"},"source":{"b5a02b9b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\nimport scipy.stats as stats\nfrom sklearn.utils.fixes import loguniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\nSEED = 3423512\nN_ITER_SEARCH = 100","a710a82f":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv', index_col='PassengerId')\ntrain_df.head()","57600b7f":"test_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv', index_col='PassengerId')\ntest_df.head()","00fb5508":"def derive_features(df_in):\n    df = df_in.copy()\n    df['family_size'] = df['SibSp'] + df['Parch'] + 1\n    df['fare_per_passenger'] = df['Fare'] \/ df['family_size']\n    \n    return(df)\n\ntrain_df_enriched = derive_features(train_df)\ntest_df_enriched = derive_features(test_df)\n\ndisplay(train_df_enriched.head())\ndisplay(test_df_enriched.head())","e70a3076":"# features = ['Pclass', 'Sex', 'Age', 'family_size', 'fare_per_passenger']\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", 'Age', 'Fare']\n# features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n\ny_train_dev = train_df_enriched[\"Survived\"]\n\n# Keep only selected 'features' and transform to dummy variable if necessary\nX_train_dev_feats = pd.get_dummies(train_df_enriched[features], drop_first=True)\nX_test_feats = pd.get_dummies(test_df_enriched[features], drop_first=True)\n\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nimp.fit(X_train_dev_feats)\n\n# Impute missing values and put back in dataframes\nX_train_dev = pd.DataFrame(\n    imp.transform(X_train_dev_feats), \n    index=X_train_dev_feats.index,\n    columns=X_train_dev_feats.columns\n)\nX_test = pd.DataFrame(\n    imp.transform(X_test_feats), \n    index=X_test_feats.index,\n    columns=X_test_feats.columns\n)\n\n# Split all data available for training in a train and a dev set\nX_train, X_dev, y_train, y_dev = train_test_split(\n    X_train_dev, y_train_dev, \n    test_size=0.25, \n    random_state=SEED,\n    stratify=y_train_dev\n)","add10af8":"print(\"The size of X_train: {}\".format(X_train.shape))\nprint(\"The size of y_train: {}\".format(y_train.shape))\nprint(\"The size of X_dev: {}\".format(X_dev.shape))\nprint(\"The size of y_dev: {}\".format(y_dev.shape))\nprint(\"The size of X_test: {}\".format(X_test.shape))","cffada0a":"display(X_train.head())\ndisplay(y_train.head())\ndisplay(X_dev.head())\ndisplay(y_dev.head())","19a241d6":"gb = GradientBoostingClassifier(\n    learning_rate=0.01,\n    n_estimators=60,\n    subsample=0.6,\n    max_depth=3,\n    random_state=SEED,\n    max_features=3\n#     min_samples_leaf=25,\n#     min_samples_split=100\n)\n\ngb_scores = {}\n\n# Fit the model on different folds of the training data and get the score \n# Score is using accuracy by default for GBC\ngb_scores['train_cv'] = cross_val_score(gb, X_train, y_train, cv=5)\nprint('CV scores: ', gb_scores['train_cv'])\nprint('Mean CV score: ', np.mean(gb_scores['train_cv']))\n","6fcf93f5":"gb.fit(X_train, y_train)\n\ngb_scores['train'] = gb.score(X_train, y_train)\nprint('Train score:   ', gb_scores['train'])\n\ngb_scores['dev'] = gb.score(X_dev, y_dev)\nprint('Dev score:     ', gb_scores['dev'])","d6bab383":"gb_feature_importance = pd.Series(gb.feature_importances_, index=X_train.columns)\nax = (\n    gb_feature_importance\n        .sort_values()\n        .plot(kind='barh')\n)","a019fa65":"y_dev_pred = pd.Series(gb.predict(X_dev), index=X_dev.index, name='Survived_pred')\n\ndev_df_pred = pd.concat(\n    [train_df, X_dev, y_dev_pred],\n    axis=1,\n    join='inner'\n)\n\ndisplay(dev_df_pred[\n    (dev_df_pred['Survived'] != dev_df_pred['Survived_pred'])\n])","4b28cd96":"# Instatiate a specific GB model for the search\ngb_rs = GradientBoostingClassifier(random_state=SEED)\n\n# specify parameters and distributions to sample from\nparam_dist = {\n    'learning_rate': loguniform(1e-3, 1e0),\n    'n_estimators': stats.randint(1, 200),\n    'subsample': stats.uniform(0.01, 0.99),\n    'max_depth': stats.randint(2, 10),\n    'max_features': stats.randint(2, len(features))\n}\n\n# run search\ngb_random_search = RandomizedSearchCV(\n    gb_rs, param_distributions=param_dist, n_iter=N_ITER_SEARCH,\n    cv=5,\n    verbose=1,\n    random_state=SEED\n)\ngb_random_search.fit(X_train, y_train)","572dc136":"gb_rs_best = gb_random_search.best_estimator_\n\nprint(gb_rs_best)\nprint(gb_random_search.best_params_)\nprint(gb_random_search.best_score_)","85e96a3c":"gb_rs_scores = {}\n\ngb_rs_scores['train_cv'] = cross_val_score(gb_rs_best, X_train, y_train, cv=5)\nprint('CV scores: ', gb_rs_scores['train_cv'])\nprint('Mean CV score: ', np.mean(gb_rs_scores['train_cv']))\n\ngb_rs_scores['train'] = gb_rs_best.score(X_train, y_train)\nprint('Train score:   ', gb_rs_scores['train'])\n\ngb_rs_scores['dev'] = gb_rs_best.score(X_dev, y_dev)\nprint('Dev score:     ', gb_rs_scores['dev'])","e699387e":"xgb = XGBClassifier(\n    learning_rate=0.02,\n#     min_split_loss=2,\n    max_depth=3,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_lambda=50, # L2 regularization\n#     reg_alpha=10, # L1 regularization\n    n_estimators=80,\n    objective='reg:logistic',\n    use_label_encoder=False,\n    random_state=SEED\n)\n\nxgb_scores = {}\n\n# Fit the model on different folds of the training data and get the score \n# Score is using accuracy by default for GBC\nxgb_scores['train_cv'] = cross_val_score(xgb, X_train, y_train, cv=5)\nprint('CV scores: ', xgb_scores['train_cv'])\nprint('Mean CV score: ', np.mean(xgb_scores['train_cv']))\n","c5608154":"xgb.fit(X_train, y_train)\n\nxgb_scores['train'] = xgb.score(X_train, y_train)\nprint('Train score:   ', xgb_scores['train'])\n\nxgb_scores['dev'] = xgb.score(X_dev, y_dev)\nprint('Dev score:     ', xgb_scores['dev'])","d5add31f":"# Instatiate a specific GB model for the grid search\nxgb_rs = XGBClassifier(\n    objective='reg:logistic',\n    use_label_encoder=False,\n    random_state=SEED\n)\n\n# specify parameters and distributions to sample from\nxgb_param_dist = {\n    'learning_rate': loguniform(1e-3, 1e0),\n    'n_estimators': stats.randint(1, 200),\n    'subsample': stats.uniform(0.01, 0.99),\n    'max_depth': stats.randint(2, 10),\n    'colsample_bytree': stats.uniform(0.01, 0.99),\n    'reg_lambda': loguniform(1, 1000)\n}\n\n# run search\nxgb_random_search = RandomizedSearchCV(\n    xgb_rs, param_distributions=xgb_param_dist, n_iter=N_ITER_SEARCH,\n    cv=5,\n    verbose=1,\n    random_state=SEED\n)\nxgb_random_search.fit(X_train, y_train)","c920513a":"xgb_rs_best = xgb_random_search.best_estimator_\n\nprint(xgb_rs_best)\nprint(xgb_random_search.best_params_)\nprint(xgb_random_search.best_score_)","d7d2bff2":"xgb_rs_scores = {}\n\nxgb_rs_scores['train_cv'] = cross_val_score(xgb_rs_best, X_train, y_train, cv=5)\nprint('CV scores: ', xgb_rs_scores['train_cv'])\nprint('Mean CV score: ', np.mean(xgb_rs_scores['train_cv']))\n\nxgb_rs_scores['train'] = xgb_rs_best.score(X_train, y_train)\nprint('Train score:   ', xgb_rs_scores['train'])\n\nxgb_rs_scores['dev'] = xgb_rs_best.score(X_dev, y_dev)\nprint('Dev score:     ', xgb_rs_scores['dev'])","3a98437f":"models = {\n    'gb': gb, \n    'gb_rs': gb_rs_best, \n    'xgb': xgb, \n    'xgb_rs': xgb_rs_best\n}\n\nfor key, model in models.items():\n    # Create submission\n    y_test_pred = model.predict(X_test)\n\n    output = pd.DataFrame({'PassengerId': test_df.index, 'Survived': y_test_pred})\n    output.to_csv(key + '_submission.csv', index=False)","ddaa83f2":"## XGBoost\n\nInstead of the gradient boosting implementation from scikit-learn, we try the XGBoost implementation of gradient boosting. This model is compatible with scikit-learn and provides similar methods. ","0e85c2b3":"A quick glance at the list of passengers seems to indicate we are primarily wrong for passengers that survived, whereas our model predicted that they are not. The data includes a lot of names that include 'Master' and 'Miss', so perhaps this is something worthwhile to include in the model.   ","f0d8a93b":"### Determine scores","ff805523":"## Prediction errors\n\nDisplay all passengers that we did not predict correctly using our gradient boost model. ","86e32603":"### Hyperparameter tuning\n\nAlso for the XGBoost model we will try to find the best hyperparameters using a randomized search. ","8fbd755d":"# Fit model\n\nNow that the data is prepared and split into a train and dev set, we can start to fit models. \n\n## Gradient boosting with manual hyperparameter tuning\n\nFirst, we will try fitting a model and tune the hyperparameters manually. We try to get a good estimate of the performance of the model by using cross-validation to determine the accuracy. We can further verify this by calculating the accuracy on the dev-set. This is data never seen by the model as we did not use it during training.","6671a2d6":"# Prepare data\nBefore fitting the model we create the training set with all the features prepared.\n\n## Derive additional features\nWe derive additional variables that could have predictive value in our model. ","f89f2436":"# Create Submissions","f48f9f87":"## Split training data\nTo provide an unbiased estimate of model performance, we split the provided training data into a train and a dev set. We will fit the model on the train set and evaluate its performance, after tuning hyperparameters, on the dev set. ","659bc9e5":"After several iterations, we were able to find hyperparameters that result in a model that has decent accuracy given the used parameters. It also does not appear to be overfitting as the accuracy on the train set is similar to the accuracy determined using cross-validation. ","46c81c2c":"# Load data\n\n## Training data","8446b8ab":"Now we also fit the model on the full training data and get the accuracy on that dataset. The accuracy on the full data should not be signicantly higher or lower than the CV estimate of the accuracy. Otherwise we are probably respectively overfitting or underfitting. \nIn addition, we will do a final estimate of the model accuracy on data that the model has never seen: the dev-set.  ","48519a33":"## Feature importance\n\nDetermine importance of the selected features in the gradient boosting model.","9ec87bcb":"The best XGBoost model from the randomized search also shows some overfitting. The accuracy on the full train set is higher than the accuracy determined using cross-validation on the train set.","ad3a49b1":"### Determine scores","5398065e":"The 'best' model from the randomized search appears to overfit badly. The accuracy on the full train set is much higher than the accuracy determined using cross-validation on the train set.","768498ee":"## Gradient boosting with grid search for hyperparameters\n\n### Hyperparameter tuning\nInstead of selecting the hyperparameters manually, we are going to search for the optimal parameters to train the gradient boost classifier with using randomized search. ","a2eb7ff3":"## Test data"}}