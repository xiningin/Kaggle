{"cell_type":{"f42ef4a2":"code","02ec3803":"code","06193ee1":"code","0da31738":"code","a249614a":"code","0f18fe4d":"code","e27e4bb1":"code","2bc7a265":"code","cba706fd":"code","1df9ef29":"code","2217928c":"code","c52f52ad":"code","58b5ecbe":"code","26a4b3fa":"code","1f71d095":"code","bcdacc61":"code","1a4c51b1":"code","81039c43":"code","7803b4b6":"code","00cc3eac":"code","5a4802ac":"code","bec463c9":"code","46ad35ec":"code","34e1d559":"code","8413da3b":"code","ba70d1a3":"code","867af633":"code","8ac1eb67":"code","b0f6479c":"code","0287a3e8":"code","8e107273":"code","93ce57e2":"code","18fd9ff2":"code","a7634479":"code","abd05006":"code","bceb1196":"code","598a8740":"code","66719493":"code","4cef42b9":"code","b97071cb":"code","487e6ad1":"code","db53e5d0":"code","17449d76":"code","60dfd196":"code","0611343a":"code","0d7cc2a2":"code","9aec9609":"code","a2575053":"code","cc591cad":"code","ebe8f122":"code","727e484a":"code","1e7abdde":"code","6f639ed2":"code","14291afd":"code","3c94a64c":"code","ad0aad28":"code","6d3061e3":"code","6075baf1":"code","d441432c":"code","90f672bb":"code","f4f35778":"code","ac7fe701":"code","78621086":"code","3a0b9cc6":"code","3a79008c":"code","500c1303":"code","a577b971":"code","bc72cbf1":"code","0f86be65":"code","ea89596d":"code","98e09cf3":"code","d237a95d":"code","61088571":"code","fddacf6f":"code","039563c7":"code","8167311a":"code","1ee7ff31":"code","78897f93":"code","b23b9738":"code","00f73397":"code","295af979":"code","73a128b2":"code","2425f5c8":"code","c4596b09":"code","ff5eecf8":"code","556ea50c":"code","d5e99488":"code","a7effe2e":"code","d4040309":"code","4acb279c":"code","1b1b955d":"markdown","5b672d92":"markdown","d7441977":"markdown","3bc7f518":"markdown","b6428f9d":"markdown","0252a262":"markdown","0d136a1b":"markdown","79724c34":"markdown","c836c394":"markdown","6f16b3f6":"markdown","c48da1e0":"markdown","c4a9da19":"markdown","c18f0531":"markdown","48b51c30":"markdown","4e72bce5":"markdown","5ac76247":"markdown","d66fdf6d":"markdown","0c9fd871":"markdown","5b64ebf7":"markdown","9ca39908":"markdown","76abbc36":"markdown","04d6544f":"markdown","032b827c":"markdown","e67d819f":"markdown","729bfd80":"markdown","f6031203":"markdown","a43f7746":"markdown","99f6318c":"markdown","bd6c72fb":"markdown","9f535e5e":"markdown","0ddf6181":"markdown","dc30c261":"markdown","e02496c5":"markdown","209e2ec4":"markdown","83724590":"markdown","99f92ebd":"markdown","0e6a203d":"markdown","523157fb":"markdown","831aa3da":"markdown","fab7ec8d":"markdown","6096a2e6":"markdown","ae53ece9":"markdown","0a71fe8c":"markdown","5e7ff675":"markdown","9c638647":"markdown","ddb53224":"markdown","6f885074":"markdown","51a02373":"markdown","8522e251":"markdown","a464f032":"markdown","9e5d7ead":"markdown","f1b58a22":"markdown","d97645ce":"markdown","d06ccdcd":"markdown","7b87b1f9":"markdown"},"source":{"f42ef4a2":"#Importing the libraries\n\nimport math, time, random, datetime #Python Imports\n\nimport numpy as np   # Data Manipulation\nimport pandas as pd\n\nimport matplotlib.pyplot as plt # Visualization \nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize  # Preprocessing\n\nimport catboost   # Machine learning\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom IPython.display import clear_output","02ec3803":"# ignore warnings for now\nimport warnings\nwarnings.filterwarnings('ignore')","06193ee1":"# Function to plot counts and distibutions of a local variable and target variable side by side\n\n\ndef plot_count_dist(data, bin_df, label_column, target_column, figsize=(25, 5), use_bin_df=False):\n  \n    \n    if use_bin_df:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=bin_df)\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], kde_kws={'label': 'Survived'})\n        sns.distplot(data.loc[data[label_column] == 0][target_column], kde_kws={'label': 'Did not survive'})\n        plt.legend()\n        plt.show()\n        \n    else:\n        fig = plt.figure(figsize=figsize)\n        plt.subplot(1, 2, 1)\n        sns.countplot(y=target_column, data=data)\n        plt.subplot(1, 2, 2)\n        sns.distplot(data.loc[data[label_column] == 1][target_column], kde_kws={'label': 'Survived'})\n        sns.distplot(data.loc[data[label_column] == 0][target_column], kde_kws={'label': 'Did not Survive'})\n        plt.legend()\n        plt.show()","0da31738":"dataset=pd.read_csv('..\/input\/titanic\/train.csv')\ntestset=pd.read_csv('..\/input\/titanic\/test.csv')\ngender_sub=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nID = testset[\"PassengerId\"]\n","a249614a":"dataset.head(10) #View the data","0f18fe4d":"dataset.info()","e27e4bb1":"dataset.describe()","2bc7a265":"# Plot graphic of missing values\nmissingno.matrix(dataset, figsize = (20,8))","cba706fd":"dataset.isnull().sum() #Number of NaN in the dataset","1df9ef29":"testset.isnull().sum() #Number of NaN in the testset","2217928c":"df_dis = pd.DataFrame() #discretised continuous variables\ndf_con = pd.DataFrame() #continuous variables","c52f52ad":"dataset.dtypes","58b5ecbe":"# Plot of People SURVIVED and those who didnt survive\nfig = plt.figure(figsize=(20,2))\nsns.countplot(y='Survived', data=dataset);\nprint(dataset.Survived.value_counts())","26a4b3fa":"# Let's add this to our subset dataframes\ndf_dis['Survived'] = dataset['Survived']\ndf_con['Survived'] = dataset['Survived']","1f71d095":"sns.displot(data = dataset.Pclass,  kde=True, height=4, aspect=2)","bcdacc61":"dataset.Pclass.isnull().sum()","1a4c51b1":"df_dis['Pclass'] = dataset['Pclass']\ndf_con['Pclass'] = dataset['Pclass']","81039c43":"dataset.Name.head(20)","7803b4b6":"# Graphical distribution of Sex\nplt.figure(figsize=(20, 2))\nsns.countplot(y=\"Sex\", data=dataset);","00cc3eac":"dataset.Sex.head(20)","5a4802ac":"# Missing values \ndataset.Sex.isnull().sum()","bec463c9":"# Adding Sex to the subset dataframes\n#ENCODING\n\ndf_dis['Sex'] = dataset['Sex']\ndf_dis['Sex'] = np.where(df_dis['Sex'] == 'female', 1, 0) \n\ndf_con['Sex'] = dataset['Sex']","46ad35ec":"fig = plt.figure(figsize=(15,10))\nsns.distplot(df_dis.loc[df_dis['Survived'] == 1]['Sex'], kde_kws={'label': 'Survived'});\nsns.distplot(df_dis.loc[df_dis['Survived'] == 0]['Sex'], kde_kws={'label': 'Did not survive'});\nplt.legend()\nplt.show()","34e1d559":"df_dis.head()","8413da3b":"dataset.Age.isnull().sum()","ba70d1a3":"dataset.SibSp.isnull().sum()","867af633":"dataset.SibSp.value_counts() ","8ac1eb67":"# Add SibSp to subset dataframes\ndf_dis['SibSp'] = dataset['SibSp']\ndf_con['SibSp'] = dataset['SibSp']","b0f6479c":"plot_count_dist(dataset, \n                bin_df=df_dis, \n                label_column='Survived', \n                target_column='SibSp', \n                figsize=(20, 10))","0287a3e8":"dataset.Parch.isnull().sum()","8e107273":"dataset.Parch.value_counts()","93ce57e2":"# Lets's Add Parch to subset dataframes\ndf_dis['Parch'] = dataset['Parch']\ndf_con['Parch'] = dataset['Parch']","18fd9ff2":"plot_count_dist(dataset, \n                bin_df=df_dis,\n                label_column='Survived', \n                target_column='Parch', \n                figsize=(20, 10))","a7634479":"dataset.Ticket.isnull().sum()","abd05006":"sns.countplot(y=\"Ticket\", data=dataset);","bceb1196":"print(\"There are total {} unique Ticket values.\".format(len(dataset.Ticket.unique())))","598a8740":"dataset.Fare.isnull().sum()","66719493":"sns.countplot(y=\"Fare\", data=dataset);","4cef42b9":"print(\"There are {} unique Fare values.\".format(len(dataset.Fare.unique())))","b97071cb":"dataset.Fare.dtype #Check The Datatype","487e6ad1":"df_con['Fare'] = dataset['Fare'] \ndf_dis['Fare'] = pd.cut(dataset['Fare'], bins=5) # discretised","db53e5d0":"df_dis.head()","17449d76":"df_con.head()","60dfd196":"plot_count_dist(data=dataset,\n                bin_df=df_dis,\n                label_column='Survived', \n                target_column='Fare', \n                figsize=(20,10), \n                use_bin_df=True)","0611343a":"dataset.Cabin.isnull().sum()","0d7cc2a2":"dataset.Embarked.isnull().sum()","9aec9609":"dataset.Embarked.value_counts()","a2575053":"sns.countplot(y='Embarked', data=dataset)","cc591cad":"df_dis['Embarked'] = dataset['Embarked']\ndf_con['Embarked'] = dataset['Embarked']\nprint(len(df_con)) # The Length of our sub dataframe before removing the values.","ebe8f122":"df_con = df_con.dropna(subset=['Embarked'])\ndf_dis = df_dis.dropna(subset=['Embarked'])\nprint(len(df_con)) # The Length of our sub dataframe after removing the values.","727e484a":"df_dis.head()","1e7abdde":"df_con.head()","6f639ed2":"# One-hot encoder\none_hot_cols = df_dis.columns.tolist()\none_hot_cols.remove('Survived')\ndf_dis_enc = pd.get_dummies(df_dis, columns=one_hot_cols)\n\ndf_dis_enc.head()","14291afd":"#One-hot encoder\n\n'''\nX = df_con.iloc[:,:].values\n\n#Encode Pclass\nlabelencoedr_X_1= LabelEncoder()\nX[:,1] = labelencoedr_X_1.fit_transform(X[:,1])\n\n#Encode Sex\nlabelencoedr_X_2= LabelEncoder()\nX[:,2] = labelencoedr_X_2.fit_transform(X[:,2])\n\n#Categorical Encode Embarked\n\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [6])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\nprint(X)\n'''\n\n# Instead of using Label encoder we have a better approcah\n\ndf_con_embarked_one_hot = pd.get_dummies(df_con['Embarked'], \n                                     prefix='embarked')\n\ndf_con_sex_one_hot = pd.get_dummies(df_con['Sex'], \n                                prefix='sex')\n\ndf_con_plcass_one_hot = pd.get_dummies(df_con['Pclass'], \n                                   prefix='pclass')\n\n\n# Combine the one hot encoded columns with df_con_enc\ndf_con_enc = pd.concat([df_con, \n                        df_con_embarked_one_hot, \n                        df_con_sex_one_hot, \n                        df_con_plcass_one_hot], axis=1)\n\n# Drop the original categorical columns (because now they've been one hot encoded)\ndf_con_enc = df_con_enc.drop(['Pclass', 'Sex', 'Embarked'], axis=1)","3c94a64c":"df_con_enc.head()","ad0aad28":"#Lets Isolate Y-Train from dataset\n\n#dataframe = df_dis_enc\ndataframe = df_con_enc\n\nX_train = dataframe.drop('Survived', axis=1)\ny_train = dataframe.Survived ","6d3061e3":"#Logistic Regression\nstart_time = time.time()\n\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)\n\nlog_time = (time.time() - start_time)\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nacc_log = format(accuracies.mean()*100)\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))\n\n# LOG accuracy\nacc_logm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_logm)","6075baf1":"#K-Nearest Neighbour\nstart_time = time.time()\n\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)\n\nknn_time = (time.time() - start_time)\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nacc_knn = format(accuracies.mean()*100)\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))\n\n# KNN accuracy\nacc_knnm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_knnm)","d441432c":"#SVM\nstart_time = time.time()\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)\nsvm_time = (time.time() - start_time)\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nacc_svm = format(accuracies.mean()*100)\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=svm_time))\n\n# SVM accuracy\nacc_svmm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_svmm)","90f672bb":"#Kernel SVM\nstart_time = time.time()\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\nksvm_time = (time.time() - start_time)\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nacc_ksvm = format(accuracies.mean()*100)\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=ksvm_time))\n\n# KSVM accuracy\nacc_ksvmm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_ksvmm)","f4f35778":"#Decision Tree Classifier\nstart_time = time.time()\n\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\ndtc_time = (time.time() - start_time)\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nacc_dtc = format(accuracies.mean()*100)\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dtc_time))\n\n# dtc accuracy\nacc_dtcm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_dtcm)","ac7fe701":"#Random Forest Classification\nstart_time = time.time()\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\nrfc_time = (time.time() - start_time)\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nacc_rfc = format(accuracies.mean()*100)\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=rfc_time))\n\n# RFC accuracy\nacc_rfcm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_rfcm)","78621086":"#XGBOOST\nstart_time = time.time()\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, y_train)\nxgb_time = (time.time() - start_time)\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nacc_xgb = format(accuracies.mean()*100)\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=xgb_time))\n\n# XGB accuracy\nacc_xgbm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_xgbm)","3a0b9cc6":"#Gradient Boost Classifier\nstart_time = time.time()\n\n\nclassifier = GradientBoostingClassifier()\nclassifier.fit(X_train, y_train)\n\ngbc_time = (time.time() - start_time)\n\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nacc_gbc = format(accuracies.mean()*100)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbc_time))\n\n# GCB accuracy\nacc_gbcm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_gbcm)\n","3a79008c":"#Gaussian Naive Bayes\nstart_time = time.time()\n\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\nnb_time = (time.time() - start_time)\n#K-Fold\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\nacc_nb = format(accuracies.mean()*100)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=nb_time))\n\n# nbm accuracy\nacc_nbm = round(classifier.score(X_train, y_train) * 100, 2)\nprint(acc_nbm)","500c1303":"#Catboost\nstart_time = time.time()\n\ncat_features = np.where(X_train.dtypes != np.float)[0]\nprint(cat_features)\n\nfrom catboost import CatBoostClassifier\ncatboost = CatBoostClassifier() \ncatboost.fit(X_train, y_train, cat_features)\nclear_output() #To clear the output since catboost may have a very troublesome elongated list of every single epoch\n\ncat_time = (time.time() - start_time)\n\n# CatBoost accuracy\nacc_catboost = round(catboost.score(X_train, y_train) * 100, 2)\nprint(acc_catboost)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=cat_time))","a577b971":"#Feature Importance\ndef feature_importance(model, data):\n    \n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': data.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\n    fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n    return fea_imp","bc72cbf1":"feature_importance(catboost, X_train)","0f86be65":"from sklearn.model_selection import cross_val_score\n\n\naccuracies = cross_val_score(estimator = catboost, X = X_train, y = y_train, cv = 10);\nclear_output() #To clear the output since catboost may have a very troublesome elongated list of every single epoch\n\nprint(\"Accuracy:{:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation:{:.2f} %\".format(accuracies.std()*100))\nacc_cat = accuracies.mean() *100\n\n","ea89596d":"acc_cat = accuracies.mean() *100\nacc_cat = str(acc_cat)","98e09cf3":"# K-Fold Accuracy Result\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'SVM', \n              'Kernel SVM', 'Decision Tree Classifier', \n              'Random Forest Classification', 'XGBOOST',\n              'Gradient Boost Classifier', 'Naive Bayes', 'CatBoost'],\n    'Score': [\n        acc_log, \n        acc_knn,  \n        acc_svm, \n        acc_ksvm, \n        acc_dtc, \n        acc_rfc,\n        acc_xgb,\n        acc_gbc,\n        acc_nb,\n        acc_cat\n        \n    ]})\nprint(\"(K-Fold) Cross Validation Accuracy Score\")\nmodels.sort_values(by='Score', ascending=False)","d237a95d":"# Mean Accuracy Result\nmodels = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'SVM', \n              'Kernel SVM', 'Decision Tree Classifier', \n              'Random Forest Classification', 'XGBOOST',\n              'Gradient Boost Classifier', 'Naive Bayes', 'CatBoost'],\n    'Score': [\n        acc_logm, \n        acc_knnm,  \n        acc_svmm, \n        acc_ksvmm, \n        acc_dtcm, \n        acc_rfcm,\n        acc_xgbm,\n        acc_gbcm,\n        acc_nbm,\n        acc_catboost\n    ]})\nprint(\"Mean Accuracy Score\")\nmodels.sort_values(by='Score', ascending=False)","61088571":"# We need our test dataframe to look like this one\nX_train.head()","fddacf6f":"# Our test dataframe has some columns our model hasn't been trained on\ntestset.head()","039563c7":"testset.info()","8167311a":"testset.isnull().sum() #CHECK FOR ANY NULL ENTRIES","1ee7ff31":"# One hot encode the columns in the test data frame (like X_train)\ntest_embarked_one_hot = pd.get_dummies(testset['Embarked'], \n                                       prefix='embarked')\n\ntest_sex_one_hot = pd.get_dummies(testset['Sex'], \n                                prefix='sex')\n\ntest_plcass_one_hot = pd.get_dummies(testset['Pclass'], \n                                   prefix='pclass')","78897f93":"# Combine the test one hot encoded columns with test\ntest = pd.concat([testset, \n                  test_embarked_one_hot, \n                  test_sex_one_hot, \n                  test_plcass_one_hot], axis=1)","b23b9738":"# Let's look at test, it should have one hot encoded columns now\ntest.head()","00f73397":"test.isnull().sum() #CHECK FOR NULL ENTRIES IN THE TEST DATASET","295af979":"X_test = test.drop(['PassengerId', 'Pclass', 'Name', 'Age', 'Ticket', 'Cabin' , 'Embarked', 'Sex'], axis=1)\nX_test","73a128b2":"X_test.isnull().sum() #CHECK FOR NULL VALUES","2425f5c8":"X_test = X_test.fillna(method='ffill') #USE FILLNA to FILL the ONE SINGLE NULL VALUE in FARE COLUMN","c4596b09":"X_test.isnull().sum() #No Null Values!! Perfect!!","ff5eecf8":"# Make a prediction using the CatBoost model on the wanted columns\npredictions = catboost.predict(X_test)","556ea50c":"predictions[:20]","d5e99488":"submission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions","a7effe2e":"submission['Survived'] = submission['Survived'].astype(int)\nsubmission.head()","d4040309":"submission.to_csv('submit.csv', index=False)","4acb279c":"submission_check = pd.read_csv('submit.csv')\nsubmission_check.head()","1b1b955d":"Here in the figure we can clearly tell that the number of mnissing values (NaN) are found and present significantly in the 'Age' and 'Cabin' Column of the dataset. For machine learning to work and function as desired we either need to remove this NaN value and substitute it with a mean or a null parameter that wont afffect the outcome of the ML model.\n\nKnowing this informaiton will help with your EDA and figuring out what kind of data cleaning and preprocessing is needed.","5b672d92":"Each row has a unique name, and it serves a purpose very similar to passenger ID.\n\n\nBecause of so many different names and to keep this fast, we won't move forward using the name variable.","d7441977":"LOADING THE DATASET AND ANALYSING THE DATA.","3bc7f518":"# **MODEL RESULTS**\n\nWhich model has the highest score?","b6428f9d":"Since there are no missing values lets check the data before adding it to our sub dataframe.\n\nLets Visualise the Kinds of tickets there are in this column.","0252a262":"That's better than Tickets but still way to crowded. Lets check for any chance of using it to cut into bins and using Categorical Encoding. ","0d136a1b":"**A QUICK DETOUR**\n\nVisualise the counts of SibSp and the distribution of the values against Survived","79724c34":"Since this is alredy a binary variable it can be added directly to our sub dataframe.\nWe will now check for missing values.","c836c394":"**A QUICK DETOUR**\n\nVisualise the counts of Parch and the distribution of the values against Survived","6f16b3f6":"From the graph on the right we can conclude that even though a lot of people died a majority of the survior had 1 Spouse or Sibling and the Majority of People who did not survive had 0 Spouses or Sibling on board the ship.","c48da1e0":"Since there are no missing values we will now check the data and add it to our sub data frame if its of any use for our ML algorithm.","c4a9da19":"That's way too crowded for us to have any conclusion, but this is definitely not a good sign.","c18f0531":"# FEATURE: AGE\n\nThis column denotes the age of the passenger.\n\nWe will check for missing values.","48b51c30":"#  **EXPLORATORY DATA ANALYSIS**\nIn this step we will intialise the notebook.\n\nThe first step to the begining of each project is EDA. Here we explore the data provided and then determine the problem statement. \n\nWe will import the data and libraries that will be required for us throughtout this project.\n\nThis notebook was made possible by the guidance from Daniel Bourke(Youtube).","4e72bce5":"We want to make predictions on the same kind of columnns our model is trained on.\n\nSo we have to select the subset of right columns of the test dateframe, encode them and make a prediciton with our model.","5ac76247":"Since there are no missing values, lets check the data and add to our sub data frame if it is viable.","d66fdf6d":"# FEATURE: SibSp\n\nThis column contains the number of siblings\/spouses the passenger has aboard the Titanic.\n\nWe will check for missing values.","0c9fd871":"# FEATURE: Pclass\n\nThe ticket class of the passenger is in the column Pclass.\n\n* Values is 1 = 1st Class, \n* Values is 2 = 2nd Class, \n* Values is 3 = 3rd Class.\n\n**Let's plot the distribution**\n\n\nWe will look at the distribution of each feature first if we can to understand what kind of spread there is across the dataset.\n\nFor example, if there are values which are completely outside of the distribution, we may not want to include them in our model.","5b64ebf7":"# **BUILDING A MACHINE LEARNING MODEL**","9ca39908":"We will remove those two missing rows. and then add to our sub dataframe.","76abbc36":"Here we can conclude that there are No Parents or Childern for the majority of Passengers (678 Passengers). While 118 Passengers had atleast one Parent or Child. 80 Passengers had 2 Parents or child. 5 Passengers had 3 Parents or Child. 4 Passengers had 4 Parents or Child and only 1 Had 6 Parents or Child.","04d6544f":"There are a total of 248 unique Fare values. Since Fare is a float (number) let's add it as it is to our continuous sub dataframe, we'll have to cut it into bins.","032b827c":"**The columns in the dataset are :-** \n\nPassengerId : \nPassenger ID number of the member onboard.\n\nSurvived : \nPassegner survived or not if **0** then the passenger **did not survive**, else **1** then the **passenger did survive**.\n\n\nPclass : \nA proxy for socio-economic status where **1st = Upper Class, 2nd = Middle Class, 3rd = LowerClass**.\n\nAge : \nAge of the passenger.\n\nSibsp : \nSiblings or Spouses aboard the Titanic.\n\nParch : \nNumber of parents \/ children aboard the Titanic.\n\n\nFare : \nPassenger fare.\n\nSex : \nGender of the passengers.\n\nEmbarked : \nPort of Bording the ship **(C = Cherbourg, Q = Queenstown, S = Southampton)**.","e67d819f":"From the graph its clearly visible, that the majority of surviving people (denoted by blue) are having a value of 0.75 - 1.00 which denotes the majority of survivors are women.","729bfd80":"Embarked is a categorical variable because there are 3 categories which a passenger could have boarded on.\n\nLets Visualise the data in the Embarked Column.","f6031203":"**A QUICK DETOUR**\n\nLet's visualise the Fare bin counts as well as the Fare distribution versus Survived.","a43f7746":"# **DATATYPES & FEATURES** \n\nFeatures with a datatype of object could be considered categorical features and those which are floats or ints (numbers) could be considered numerical features.\n\n\nBut as we further look into  our dataset we will find that there may exist numericals features that are categorical features aswell.\n\nOur goal is to figure out the best way to process the data so our machine learning model can learn from it.\n\nIdeally, all the features will be encoded into a numerical value of some kind.\n\n**We will now explore all the features of the dataset one by one.**","99f6318c":"687 NaN is way too many to even consdider cabin for our Sub Dataframe.","bd6c72fb":"# FEATURE: Name\n\nThis column has the name of the passengers.","9f535e5e":"#  **ENCODING OF DATA**\n\nWe have our two sub dataframes ready, now we can encode the data in our sub dataframes.\n\nWe will encode our binned dataframe (df_dis) with one-hot encoding and our continuous dataframe (df_con) with the label encoding function from sklearn.","0ddf6181":"**FEATURES CATBOOST RELIES ON**","dc30c261":"By the accuracy of **80.64**, **Gradient boost classifier** has the best performance.","e02496c5":"From the left graph we can conclude that even though a lot of people died, More People who had one and two Parents or Child did Survive(Denoted by blue) as compared to those who didnt who had no Parents or Child.","209e2ec4":"Here we can tell that the majority of passengers had no siblings or spouse (608), while 209 Passengers had atleast one Sibling or Spouse, 28 Passengers had 2 Siblings or Spouses, 18 Passengers had 4 Siblings or Spouses , 16 Passengers had 3 Siblings or Spouses, 7 Passengers had 8 Siblings or Spouses and 5 Passengers had 5 Siblings or Spouses on board.","83724590":"In this column out of 891 Age Values 177 are NaN. Which is more than 1\/4th of the data set. \n\nHowever it must be tempting to use fill in these values with mean or medians of the average but doing so on this data set will further add bias and variance.\nSo it is more viable option to avoid using this in our sub dataframe.","99f92ebd":"From the graph on the left it is clearly visible that the passenger who payed the less amount of fare contibuted to tha majority of death (Orange Spike in the beginning) while the passenger who paid the greater amount of fare (Upwards of 75 on X-Axis) have survived marginally.","0e6a203d":"# **SUBMISSION**","523157fb":"# **MISSING VALUES & DATA PREPROCESSING**","831aa3da":"# FEATURE: SEX\n\nThis column denotes the sex of the passenger.","fab7ec8d":"Since there are no missing values we can further continue our analysis.\n\nLets Visualiase the data.\n","6096a2e6":"# FEATURE: Parch\n\nThis columns has the number of parents\/children the passenger has aboard the Titanic.\n\nWe will check for missng values.\n","ae53ece9":"# FEATURE: Survived\nSurvived column tells us whether the passenger survived or not.\n\nIf the Values is 0, the passenger did NOT survive.\nif the values is 1, the passenger did survive.\n\nThis is the dependant variable, viz the variable we want our machine learning model to predict based off all the others.\nHence this column is not found in the test dataset.","0a71fe8c":"# FEATURE: Embarked\n\nThis column contains the port where the passenger boarded the Titanic. \n\nHere: \n*  C = Cherbourg\n*  Q = Queenstown\n*  S = Southampton\n\n\nLets check for missing values","5e7ff675":"**WE WILL NEED TWO DATA FRAMES FOR OUR ANALYSIS**\n\nOne for exploring discretised continuous variables and another for exploring continuous variables. \nDiscretised continious variables are continuous variables which have been sorted into some kind of category.","9c638647":"**A QUICK DETOUR**\n\nWith the alredy processed data we can compare the SEX variable and SURVIVAL. \n\nSince theyre already binary we can use seaborn to plot the graphs and observe the data.","ddb53224":"# FEATURE: Ticket\n\nThis column has the ticket number of the boarding passenger.\n\nLets check for missing values.\n","6f885074":"# FEATURE: PassengerID\n\nPassengerID is nothing but the chronological list of numberings for all passengers.","51a02373":"# FEATURE: Cabin\n\nThis column contains the cabin number where the passenger was staying\n\nLets check for null values.","8522e251":"We can see with this feature, the values are numerical (1, 2 and 3) but they are categories.\n\nHow do we know this? Because a passenger in Class 3 doesn't necessarily equal a passenger in Class 2 + a passenger in Class 1.\n\nNow we shall check for missing variables before adding them to the sub dataframe.","a464f032":"Since here are no missing variables we can add this to our sub dataframes.","9e5d7ead":"# FEATURE: Fare\n\nThis column contains the price of the ticket.\n\nLets check for any missing values.","f1b58a22":"Here we have two missing values. \n\nSince the missing value is so low, we can safely ignore them and remove them insted of substituing them with another category. ","d97645ce":"There are over 681 Unique values. So its for our better good that we do not use Ticket in our Sub dataframe.\n","d06ccdcd":"**TRAINING DATASET ANALYSIS.**","7b87b1f9":"We will use one hot encoder for Binned Variables."}}