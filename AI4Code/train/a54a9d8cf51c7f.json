{"cell_type":{"ab49290e":"code","5941f01e":"code","78f5faf2":"code","3fab2cd8":"code","54f4adec":"code","ebfd4a1e":"code","d3a2b8d5":"code","b07871a3":"code","0f45355d":"code","7f6680ad":"code","6d6dfc7c":"code","26c508af":"code","aeea4060":"code","20543fb6":"code","d7915dd7":"code","abff1251":"code","b59be781":"code","e71626dd":"code","70736328":"code","9c7a3fea":"code","744abfdc":"code","e25f9bfc":"code","22ff7e9c":"code","fbebdd5e":"code","97150024":"code","9eae572c":"code","12a906b9":"code","264e13ee":"code","83b3bb0a":"code","2a289f4f":"code","1060fa76":"code","5c69b713":"markdown","bdb56e08":"markdown","e851839e":"markdown","a20fc2d6":"markdown","289080da":"markdown","c74203a8":"markdown","41e1c052":"markdown","882159ae":"markdown","fc8d8879":"markdown","d0b1530e":"markdown","66c1918f":"markdown","047d9f90":"markdown","77fda93b":"markdown","c2f47acb":"markdown","1c766c4f":"markdown","415936ea":"markdown","c5461f69":"markdown"},"source":{"ab49290e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5941f01e":"import json\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence \nfrom keras.callbacks import EarlyStopping","78f5faf2":"from sklearn.feature_extraction.text import CountVectorizer\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\nfrom sklearn.model_selection import train_test_split","3fab2cd8":"%matplotlib inline\nimport os\nimport re","54f4adec":"news_data = pd.read_json('..\/input\/json-data-news-headlines\/sarcasm_json.json', lines='True')","ebfd4a1e":"news_data.head()","d3a2b8d5":"data_clean = news_data[['is_sarcastic','headline']]","b07871a3":"data_clean.head()","0f45355d":"data_clean['headline_lengths'] = data_clean['headline'].map(str).apply(len)","7f6680ad":"data_clean.head(25)","6d6dfc7c":"data_clean['headline'] = data_clean['headline'].apply(lambda x: x.lower())\ndata_clean['headline'] = data_clean['headline'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))","26c508af":"data_clean.head()","aeea4060":"sns.countplot(data_clean.is_sarcastic)","20543fb6":"from bs4 import BeautifulSoup\nimport nltk\nfrom sklearn.preprocessing import LabelBinarizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport re,string,unicodedata","d7915dd7":"stop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","abff1251":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            final_text.append(i.strip())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_between_square_brackets(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndata_clean['headline']=data_clean['headline'].apply(denoise_text)","b59be781":"plt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data_clean[data_clean.is_sarcastic == 0].headline))\nplt.imshow(wc , interpolation = 'bilinear')","e71626dd":"plt.figure(figsize = (20,20)) # Text that is Not Sarcastic\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(data_clean[data_clean.is_sarcastic == 1].headline))\nplt.imshow(wc , interpolation = 'bilinear')","70736328":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=data_clean[data_clean['is_sarcastic']==1]['headline'].str.len()\nax1.hist(text_len,color='red')\nax1.set_title('Sarcastic text')\ntext_len=data_clean[data_clean['is_sarcastic']==0]['headline'].str.len()\nax2.hist(text_len,color='green')\nax2.set_title('Not Sarcastic text')\nfig.suptitle('Characters in texts')\nplt.show()","9c7a3fea":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntext_len=data_clean[data_clean['is_sarcastic']==1]['headline'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Sarcastic text')\ntext_len=data_clean[data_clean['is_sarcastic']==0]['headline'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('Not Sarcastic text')\nfig.suptitle('Words in texts')\nplt.show()","744abfdc":"for idx,row in data_clean.iterrows():\n    row[1] = row[1].replace('rt',' ')\n    \nmax_fatures = 2000\ntokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(data_clean['headline'].values)\nX = tokenizer.texts_to_sequences(data_clean['headline'].values)\nX = pad_sequences(X)","e25f9bfc":"Y = pd.get_dummies(data_clean['is_sarcastic']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.33, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","22ff7e9c":"from random import random\nfrom numpy import array\nfrom numpy import cumsum\nfrom matplotlib import pyplot\nfrom pandas import DataFrame\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import TimeDistributed\nfrom keras.layers import Bidirectional","fbebdd5e":"# create a sequence classification instance\ndef get_sequence(n_timesteps):\n    # create a sequence of random numbers in [0,1]\n    X = array([random() for _ in range(n_timesteps)])\n    # calculate cut-off value to change class values\n    limit = n_timesteps\/4.0\n    # determine the class outcome for each item in cumulative sequence\n    y = array([0 if x < limit else 1 for x in cumsum(X)])\n    # reshape input and output data to be suitable for LSTMs\n    X = X.reshape(1, n_timesteps, 1)\n    y = y.reshape(1, n_timesteps, 1)\n    return X, y","97150024":"n_timesteps = 10\nembed_dim = 128\nlstm_out = 196\n\nmodel = Sequential()\nmodel.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(n_timesteps, 1), merge_mode='concat'))\nmodel.add(TimeDistributed(Dense(1, activation='sigmoid')))\nmodel.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])","9eae572c":"from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","12a906b9":"def get_bi_lstm_model(n_timesteps, mode):\n    model = Sequential()\n    model.add(Bidirectional(LSTM(20, return_sequences=True), input_shape=(n_timesteps, 1), merge_mode=mode))\n    model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n    return model\n\ndef get_lstm_model(n_timesteps, backwards):\n    model = Sequential()\n    model.add(LSTM(20, input_shape=(n_timesteps, 1), return_sequences=True, go_backwards=backwards))\n    model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n    return model\n\ndef train_model(model, n_timesteps):\n    loss = list()\n    for _ in range(250):\n        # generate new random sequence\n        X,y = get_sequence(n_timesteps)\n        # fit model for one epoch on this sequence\n        hist = model.fit(X, y, epochs=1, batch_size=1, verbose=0)\n        loss.append(hist.history['loss'][0])\n    return loss","264e13ee":"n_timesteps = 10\nresults = DataFrame()\nmodel = get_bi_lstm_model(n_timesteps, 'concat')\nresults['bilstm_con'] = train_model(model, n_timesteps) \n# lstm forwards\nmodel = get_lstm_model(n_timesteps, False)\nresults['lstm_forw'] = train_model(model, n_timesteps)\n# lstm backwards\nmodel = get_lstm_model(n_timesteps, True)\nresults['lstm_back'] = train_model(model, n_timesteps)\n# line plot of results\nresults.plot()\npyplot.show()","83b3bb0a":"# train LSTM\nfor epoch in range(100):\n    # generate new random sequence\n    X,y = get_sequence(n_timesteps)\n    # fit model for one epoch on this sequence\n    model.fit(X, y, epochs=5, batch_size=3, verbose=2)","2a289f4f":"# evaluate LSTM\nX,y = get_sequence(n_timesteps)\nyhat = model.predict(X, verbose=0)\nfor i in range(n_timesteps):\n    print('Expected:', y[0, i], 'Predicted', yhat[0, i])","1060fa76":"headline = ['Nonsense speaker']\nheadline = tokenizer.texts_to_sequences(headline)\nheadline = pad_sequences(headline, maxlen=29, dtype='int32', value=0)","5c69b713":"# Conclusion\n\nFrom the above result we can clearly see that \n\nWhen the expected result is - Sarcasm -> NO -> 0 is expected, we are getting the equivalent predictions as 0.01069996 for example,  which is showing expected \n\nAnd for the statement which is Sarcastic the model is accurately determining.","bdb56e08":"# Implementation steps\n\nInstructions to perform all the below steps are mentioned in the question notebook with the respective marks.\n\n1. Read and explore the data\n2. Drop one column\n3. Get length of each sentence\n4. Define parameters\n5. Get indices for words\n6. Create features and labels\n7. Get vocab size\n8. Create a weight matrix using GloVe embeddings\n9. Define and compile a Bidirectional LSTM model\n10. Fit the model and check the validation accuracy","e851839e":"Drop one column","a20fc2d6":"Lest Display the WordCloud for those words which are not Sarcastics  using label - 0","289080da":"lets visualize the statistics","c74203a8":"Lets Train the model","41e1c052":"We can see that in the given data, around 13000 headlines are sarcastic and from the above visualization we can also conclude that the given dataset is Balanced. So that we can more correclty define the accuracy for sarcastic and non sarcastic sentense.","882159ae":"Same thing for those words which are sarcastic in given data set","fc8d8879":"We can see that the Bidirectional LSTM log loss - shown by Blue color is going down sooner to a lower value and generally staying lower","d0b1530e":"Define and Compile the Bidirectional LSTM model","66c1918f":"Lets visualize the statistics for the number of sarcastic and non sarcastic words in the setenses","047d9f90":"We can see the accuracy ranges from 90% to 100%","77fda93b":"lets have Train and Test data set from the given data set","c2f47acb":"Lets clean the dataset for Headlines to make valid string for the headlines","1c766c4f":"Lets Clean the data for the head line strings","415936ea":"Get Length of each sentense","c5461f69":"Using tokenizer to standardize and vectorize and convert the text into the sequences"}}