{"cell_type":{"f649a1ae":"code","cd42e4d9":"code","73731383":"code","ace2491a":"code","e4ce3bf7":"code","7192b930":"code","48fb37a0":"code","326f2537":"code","c4fa105c":"code","f138db32":"code","be89384a":"code","3027bff8":"code","415c2382":"code","464ade4c":"code","198039b0":"code","c2383630":"code","0acd9e57":"code","1555f438":"code","05342645":"code","d1118bb2":"code","e8e55308":"code","5bcb8053":"code","a295bf24":"code","46325e22":"code","521721bc":"code","b42952f9":"code","015850b9":"code","9cee7582":"code","1c006d17":"markdown","f7e29074":"markdown","1926b9b2":"markdown","75387532":"markdown","5c27dcfd":"markdown","3483de07":"markdown","d78275bb":"markdown","6f800f3a":"markdown","7edb70b8":"markdown","786658e4":"markdown","a6849ae1":"markdown","6719e9f4":"markdown","989ad24f":"markdown","98761d3a":"markdown","a66fd38c":"markdown","53f7ecac":"markdown"},"source":{"f649a1ae":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cd42e4d9":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/train.csv\")\ndf.head()","73731383":"text = []\n\nfor row in df[\"text\"][df[\"author\"] == \"EAP\"]:\n    text.append(str(row))\n    \ncorpusEAP = \" \".join(text)\ncorpusEAP = corpusEAP[0:100000]","ace2491a":"len(corpusEAP)","e4ce3bf7":"import spacy\nnlp = spacy.load('en',disable=['parser', 'tagger','ner']) # only for tokenisation\n","7192b930":"def separate_punc(doc_text):\n    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n ']","48fb37a0":"\ntokens = separate_punc(corpusEAP)\nlen(tokens)","326f2537":"# organize into sequences of tokens\ntrain_len = 25+1\n\n# Empty list of sequences\ntext_sequences = []\n\nfor i in range(train_len, len(tokens)):\n    \n    # Grab train_len# amount of characters\n    seq = tokens[i-train_len:i]\n    \n    # Add to list of sequences\n    text_sequences.append(seq)","c4fa105c":"print (' '.join(text_sequences[0]))\nprint (' '.join(text_sequences[1]))","f138db32":"len(text_sequences)","be89384a":"from keras.preprocessing.text import Tokenizer\n# integer encode sequences of words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_sequences)\nsequences = tokenizer.texts_to_sequences(text_sequences)\nprint (sequences[0])","3027bff8":"print (tokenizer.index_word)\nprint ()\nprint (\" --------------------- \")\nprint (len(tokenizer.word_counts))","415c2382":"sequences = np.array(sequences)\nsequences[0]","464ade4c":"X = sequences[:,:-1]\ny = sequences[:,-1]","198039b0":"print (X.shape, y.shape)\n","c2383630":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Embedding\n\nvocabulary_size = len(tokenizer.word_counts)\nvocabulary_size = vocabulary_size + 1\nseq_len = X.shape[1]\n\nmodel = Sequential()\nmodel.add(Embedding(vocabulary_size, 25, input_length=seq_len))\nmodel.add(LSTM(150, return_sequences=True)) # to stack LSTM we need return seq \nmodel.add(LSTM(150))\nmodel.add(Dense(150, activation='relu'))\n\nmodel.add(Dense(vocabulary_size, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","0acd9e57":"model_test = Sequential()\nmodel_test.add(Embedding(input_dim = vocabulary_size, output_dim = 2, input_length=seq_len))\nmodel_test.compile('rmsprop', 'mse')\noutput_array = model_test.predict(X)\nprint (output_array.shape)\nout = pd.DataFrame(output_array[0])\nout.head()\n","1555f438":"from keras.utils import to_categorical\ny = to_categorical(y, num_classes=vocabulary_size)\n# fit model\nmodel.fit(X, y, batch_size=256, epochs=100,verbose=1)","05342645":"from keras.preprocessing.sequence import pad_sequences\ndef generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n    '''\n    INPUTS:\n    model : model that was trained on text data\n    tokenizer : tokenizer that was fit on text data\n    seq_len : length of training sequence\n    seed_text : raw string text to serve as the seed\n    num_gen_words : number of words to be generated by model\n    '''\n    \n    # Final Output\n    output_text = []\n    \n    # Intial Seed Sequence\n    input_text = seed_text\n    \n    # Create num_gen_words\n    for i in range(num_gen_words):\n        \n        # Take the input text string and encode it to a sequence\n        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n        \n        # Pad sequences to our trained rate \n        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n        \n        # Predict Class Probabilities for each word\n        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n        \n        # Grab word\n        pred_word = tokenizer.index_word[pred_word_ind] \n        \n        # Update the sequence of input text (shifting one over with the new word)\n        input_text += ' ' + pred_word\n        \n        output_text.append(pred_word)\n        \n    # Make it look like a sentence.\n    return ' '.join(output_text)","d1118bb2":"text_sequences[100]","e8e55308":"\nseed_text = ' '.join(text_sequences[100])\ngenerate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=10)","5bcb8053":"df.head()","a295bf24":"import random\n\ndf_augumented = pd.DataFrame(columns=['text', 'author'])\n\nfor i in range (200):\n    random_pick = random.randint(0,len(text_sequences))\n    seed_text = ' '.join(text_sequences[random_pick])\n    text = generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=20)\n    df_augumented[\"text\"]\n    df_augumented = df_augumented.append({'text': text, \"author\" : \"EAP\"}, ignore_index=True)\n    \ndf_augumented.head()    \n    ","46325e22":"data_prev = df[[\"text\", \"author\"]]\ndata = pd.concat([data_prev, df_augumented], axis = 0)\ndata.shape","521721bc":"data['author_num'] = data[\"author\"].map({'EAP':0, 'HPL':1, 'MWS':2})","b42952f9":"X = data['text']\ny = data['author_num']\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(stop_words = 'english')\nX_train_matrix = vect.fit_transform(X_train) \nfrom sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train_matrix, y_train)\nprint(clf.score(X_train_matrix, y_train))\nX_test_matrix = vect.transform(X_test) \nprint (clf.score(X_test_matrix, y_test))\npredicted_result=clf.predict(X_test_matrix)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,predicted_result))","015850b9":"X = data['text']\ny = data['author_num']\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer(stop_words = 'english')\nX_train_matrix = vect.fit_transform(X) \nfrom sklearn.naive_bayes import MultinomialNB\nclf=MultinomialNB()\nclf.fit(X_train_matrix, y)\n","9cee7582":"test = pd.read_csv(\"\/kaggle\/input\/spooky-author-identification\/test.csv\")\ntest_matrix = vect.transform(test[\"text\"])\npredicted_result = clf.predict_proba(test_matrix)\nresult=pd.DataFrame()\nresult[\"id\"]=test[\"id\"]\nresult[\"EAP\"]=predicted_result[:,0]\nresult[\"HPL\"]=predicted_result[:,1]\nresult[\"MWS\"]=predicted_result[:,2]\nresult.head()\nresult.to_csv(\"submission_v5.csv\", index=False)","1c006d17":"# X and y and Multinomial Naive Bayes\n\n* with augumented data","f7e29074":"# Train the model","1926b9b2":"## Create Sequences of Tokens","75387532":"* How embedding works in Keras\n* input_dim: int > 0. Size of the vocabulary, i.e. maximum integer index + 1.\n* output_dim: int >= 0. Dimension of the dense embedding. indicates the size of the embedding vectors\n\neach input integer is used as the index to access a table that contains all posible vectors. That is the reason why it needs to specify the size of the vocabulary as the first argument (so the table can be initialized).Once the network has been trained, we can get the weights of the embedding layer,and can be thought as the table used to map integers to embedding vectors.the underlying automatic differentiation engines (e.g., Tensorflow or Theano) manage to optimize these vectors associated to each input integer just like any other parameter.\n\n* it also possible to use pretained embedding https:\/\/blog.keras.io\/using-pre-trained-word-embeddings-in-a-keras-model.html","5c27dcfd":"* in my previous notebook https:\/\/www.kaggle.com\/guidosalimbeni\/multinomial-naive-bayes the recall for EAP was lower ... so maybe it worked to augument the data ","3483de07":"# Read the text column and convert into a text corpus","d78275bb":"\n\n### augument the data for EAP","6f800f3a":"### Keras Tokenization","7edb70b8":"### Train \/ Test Split","786658e4":"## concat","a6849ae1":"# Generating New Text","6719e9f4":"# Classification","989ad24f":"### Tokenize and Clean Text","98761d3a":"# LSTM based model\n\n* LSTM layer take a 3D tensor with shape (batch_size, timesteps, input_dim). \n* in this case the correct shape is generated from the embedding layer","a66fd38c":"# Submission","53f7ecac":"### Convert to Numpy Matrix"}}