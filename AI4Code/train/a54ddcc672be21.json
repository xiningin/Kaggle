{"cell_type":{"13214fbc":"code","20ca97a3":"code","dc0d7f97":"code","781b866f":"code","eb0d5f97":"code","5e91af22":"code","4539d826":"code","1a74af6b":"code","627d1e6b":"code","914b5805":"code","dd26b9c9":"code","e7275ec3":"code","c7ca2492":"code","f2211247":"code","dbbffa40":"code","81ffd53d":"code","7acae57a":"code","c7c850da":"code","a0f5ef8c":"code","c371a8f1":"code","9308dcbc":"code","b6e8b0f0":"code","3750f270":"code","224c99db":"code","b2019447":"code","2a5dd9c6":"code","52b13859":"code","5657bb34":"code","640cec3a":"code","a9696422":"code","7b0d502a":"code","6052265f":"code","18c6f5e5":"code","757b35a2":"code","d0fffabd":"code","814e28df":"code","c1deccc6":"code","b1e94fbd":"code","e7a297ac":"markdown","54543411":"markdown","026bcf4a":"markdown","02bef164":"markdown","edaa4890":"markdown","4f10cbc2":"markdown","cae31c14":"markdown","154c8540":"markdown","25491550":"markdown","f72e5948":"markdown","6733c9da":"markdown","6321a535":"markdown","a78598c5":"markdown","1fb783c7":"markdown","ca3e2044":"markdown","2948b857":"markdown","9556c920":"markdown","b82c37a9":"markdown","d959e325":"markdown","1feb168d":"markdown","5bed6214":"markdown","a7cad306":"markdown","b99408c3":"markdown","395f4fde":"markdown","4f0c5d8a":"markdown","80972129":"markdown","a6239d42":"markdown","e85dc2aa":"markdown","ab0f7ac8":"markdown","a1b93d26":"markdown"},"source":{"13214fbc":"# import all libraries and dependencies for dataframe\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime, timedelta\n\n# import all libraries and dependencies for data visualization\npd.options.display.float_format='{:.4f}'.format\nplt.rcParams['figure.figsize'] = [8,8]\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', -1) \nsns.set(style='darkgrid')\nimport matplotlib.ticker as ticker\nimport matplotlib.ticker as plticker\n\n# import all libraries and dependencies for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.base import TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import r2_score","20ca97a3":"# Reading the advertising company file on which analysis needs to be done\ndf_sales = pd.read_csv('..\/input\/sales-advertisment\/advertising.csv')","dc0d7f97":"df_sales.head()","781b866f":"df_sales.shape","eb0d5f97":"df_sales.info()","5e91af22":"df_sales.describe()","4539d826":"# Calculating the Missing Values % contribution in DF\n\ndf_null = df_sales.isna().mean().round(4) * 100\n\ndf_null.sort_values(ascending=False).head()","1a74af6b":"df_sales.dtypes","627d1e6b":"# Outlier Analysis of target variable with maximum amount of Inconsistency\n\noutliers = ['Sales']\nplt.rcParams['figure.figsize'] = [8,8]\nsns.boxplot(data=df_sales[outliers], orient=\"v\", palette=\"Set1\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Sales Range\", fontweight = 'bold')\nplt.xlabel(\"Continuous Variable\", fontweight = 'bold')","914b5805":"# checking for duplicates\n\ndf_sales.loc[df_sales.duplicated()]","dd26b9c9":"plt.figure(figsize=(8,8))\n\nplt.title('Sales Distribution Plot')\nsns.distplot(df_sales['Sales'])","e7275ec3":"sns.pairplot(df_sales)","c7ca2492":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (10, 8))\ndf_corr = df_sales.corr()\nax = sns.heatmap(df_corr, annot=True, cmap=\"RdYlGn\") \nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","f2211247":"# We specify this so that the train and test data set always have the same rows, respectively\n# We divide the df into 70\/30 ratio\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(df_sales, train_size = 0.7, test_size = 0.3, random_state = 100)","dbbffa40":"df_train.head()","81ffd53d":"cols = ['TV','Radio','Newspaper']","7acae57a":"# Scatter Plot of independent variables vs dependent variables\n\nfig,axes = plt.subplots(1,3,figsize=(20,6))\nfor seg,col in enumerate(cols):\n    x,y = seg\/\/3,seg%3\n    an=sns.scatterplot(x=col, y='Sales' ,data=df_sales, ax=axes[y])\n    plt.setp(an.get_xticklabels(), rotation=45)\n   \nplt.subplots_adjust(hspace=0.5)","c7c850da":"y_train = df_train.pop('Sales')\nX_train = df_train","a0f5ef8c":"X_train_1 = X_train['TV']","c371a8f1":"# Add a constant\nX_train_1c = sm.add_constant(X_train_1)\n\n# Create a first fitted model\nlr_1 = sm.OLS(y_train, X_train_1c).fit()","9308dcbc":"# Check parameters created\n\nlr_1.params","b6e8b0f0":"# Let's visualise the data with a scatter plot and the fitted regression line\n\nplt.scatter(X_train_1c.iloc[:, 1], y_train)\nplt.plot(X_train_1c.iloc[:, 1], 6.9487 + 0.0545*X_train_1c.iloc[:, 1], 'r')\nplt.show()","3750f270":"# Print a summary of the linear regression model obtained\nprint(lr_1.summary())","224c99db":"X_train_2 = X_train[['TV', 'Radio']]","b2019447":"# Add a constant\nX_train_2c = sm.add_constant(X_train_2)\n\n# Create a second fitted model\nlr_2 = sm.OLS(y_train, X_train_2c).fit()","2a5dd9c6":"lr_2.params","52b13859":"print(lr_2.summary())","5657bb34":"X_train_3 = X_train[['TV', 'Radio', 'Newspaper']]","640cec3a":"# Add a constant\nX_train_3c = sm.add_constant(X_train_3)\n\n# Create a third fitted model\nlr_3 = sm.OLS(y_train, X_train_3c).fit()","a9696422":"lr_3.params","7b0d502a":"print(lr_3.summary())","6052265f":"# Predicting the price of training set.\ny_train_sales = lr_2.predict(X_train_2c)","18c6f5e5":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_sales), bins = 20)\nfig.suptitle('Error Terms Analysis', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)","757b35a2":"y_test = df_test.pop('Sales')\nX_test = df_test","d0fffabd":"# Adding constant\nX_test_1 = sm.add_constant(X_test)\n\nX_test_new = X_test_1[X_train_2c.columns]","814e28df":"# Making predictions using the final model\ny_pred = lr_2.predict(X_test_new)","c1deccc6":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)   \nplt.xlabel('y_test ', fontsize=18)                       \nplt.ylabel('y_pred', fontsize=16)    ","b1e94fbd":"r2_score(y_test, y_pred)","e7a297ac":"#### Understanding the dataframe","54543411":"### RMSE Score","026bcf4a":"## Step 5: Building a linear model","02bef164":"### Dividing into X and Y sets for the model building","edaa4890":"### Business Goal\nPredicting the Sales based on various advertisment factors.","4f10cbc2":"## Step 6: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of it.","cae31c14":"#### Visualizing the distribution of Sales","154c8540":"### Adding another variable\n\nThe R-squared value obtained is `0.81`. Since we have two more variables, we can try do better than this. So let's go ahead and add another variable, i.e. `Radio`.","25491550":"We need to do some basic cleansing activity in order to feed our model the correct data.","f72e5948":"## Step 4: Splitting the Data into Training and Testing Sets\n\nAs we know, the first basic step for regression is performing a train-test split.","6733c9da":"#### Model Conclusions:\n- R-sqaured and Adjusted R-squared - 0.910 and 0.909 - 90% variance explained.\n- F-stats and Prob(F-stats) (overall model fit) - 692.3 and 2.38e-72(approx. 0.0) - Model fit is significant and explained 90%<br> variance is just not by chance.\n- p-values - p-values for all the coefficients seem to be less than the significance level of 0.05. - meaning that all the <br>predictors are statistically significant.","6321a535":"#### Visualization of Heat map","a78598c5":"- We can see there is a line we can fit Sales vs TV","1fb783c7":"#### Equation of Line to predict the Sales","ca3e2044":"## Step 3: Visualising the Data\n\n- Here we will identify if some predictors directly have a strong association with the outcome variable `Sales`","2948b857":"#### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated","9556c920":"We have achieved a R-squared of `0.91` by picking the right variables.\nHence we can use these two variables to predict `Sales` using model `lr_2`.","b82c37a9":"**The R2 score of Training set is 0.90 and Test set is 0.87 which is quite close.\nHence, We can say that our model is good enough to predict the Sales using below predictor variables**\n- TV\n- Radio","d959e325":"#### Insights: \n- There are no such value that falls under the outliers","1feb168d":"## Step 1: Reading and Understanding the Data","5bed6214":"### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated","a7cad306":"#### Visualising Numeric Variables\n\nPairplot of all the numeric variables","b99408c3":"#### Dividing test set into X_test and y_test","395f4fde":"## Step 2: Cleaning the Data","4f0c5d8a":"## Step 8: Model Evaluation\n\nLet's now plot the graph for actual versus predicted values.","80972129":"#### Insights:\n- `TV` seems to have a linear relationship with Sales.\n","a6239d42":"$ Sales = 4.4251 +  0.0540  \\times  TV  + 0.1137  \\times  Radio $","e85dc2aa":"## Step 7: Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final model.","ab0f7ac8":"* The R-squared incresed from 0.81 to 0.91 ","a1b93d26":"### Adding another variable\n\nThe R-squared value obtained is `0.91`. Since we have one more variable, we can add and see the results."}}