{"cell_type":{"73af13f9":"code","236207c0":"code","902da7f9":"code","4f33d6c7":"code","8dfc3364":"code","a96ccce2":"code","e365a81a":"code","a00b88a5":"code","41f6b189":"code","99b86d7f":"code","da2a0989":"code","2539002b":"code","5f422358":"code","f459ac53":"code","fc83f5ce":"code","7f56361e":"code","65e3ee8c":"code","2b865cbe":"code","cb1ba4b8":"code","eea65602":"code","88e61daa":"code","02236dd3":"code","1271e97e":"code","d379e949":"markdown","c7637381":"markdown","60079911":"markdown","71d7b512":"markdown","85967f30":"markdown","d2c76de7":"markdown","483f33d1":"markdown","c9c2bca8":"markdown","f872a413":"markdown","57e012d4":"markdown","346391fa":"markdown","38b9ff7b":"markdown","887c202c":"markdown","6fafaf96":"markdown","a367fcd6":"markdown","ce0ad5a3":"markdown","81751262":"markdown"},"source":{"73af13f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","236207c0":"df = pd.read_csv('..\/input\/quora-question-pairs\/train.csv.zip')","902da7f9":"df.head()","4f33d6c7":"test_data = pd.read_csv('..\/input\/quora-question-pairs\/test.csv.zip')","8dfc3364":"test_data.head()","a96ccce2":"X_train = df.iloc[:,:5].values\nY_train = df.iloc[:,5:].values","e365a81a":"X_testq1 = test_data.iloc[:400001,1:2].values\nX_testq2 = test_data.iloc[:400001, 2:].values","a00b88a5":"s1 = X_train[:,3]\ns2 = X_train[:,4]","41f6b189":"def tokenize(s):\n    tokens = []\n    tokens = [word_tokenize(str(sentence)) for sentence in s]\n\n    rm1 = []\n    for w in tokens:\n        sm = re.sub('[^A-Za-z]',' ', str(w))\n        x = re.split(\"\\s\", sm)\n        rm1.append(x)\n        \n    return rm1","99b86d7f":"def lower_case(s):\n    #Removing whitespaces    \n    for sent in s:\n        while '' in sent:\n            sent.remove('')\n\n    # Lowercasing\n    low = []\n    for i in s:\n        i = [x.lower() for x in i]\n        low.append(i)\n        \n    return low\n    ","da2a0989":"def lemmatize(s):\n    lemma = []\n    wnl = WordNetLemmatizer()\n    for doc in s:\n        tokens = [wnl.lemmatize(w) for w in doc]\n        lemma.append(tokens)\n\n    # Removing Stopwords\n    filter_words = []\n    Stopwords = set(stopwords.words('english'))\n\n    #ab = spell('nd')\n    for sent in lemma:\n        tokens = [w for w in sent if w not in Stopwords]\n        filter_words.append(tokens)\n\n    space = ' ' \n    sentences = []\n    for sentence in filter_words:\n        sentences.append(space.join(sentence))\n        \n    return sentences","2539002b":"# sent1 = tokenize(s1)\n# sent2 = tokenize(s2)\n# q1 = lower_case(sent1)\n# q2 = lower_case(sent2)\n# listq1 = lemmatize(q1)\n# listq2 = lemmatize(q2)\n# sent1_t = tokenize(X_test_q1)\n# sent2_t = tokenize(X_test_q2)\n# q1_t = lower_case(sent1_t)\n# q2_t = lower_case(sent2_t)\n# listq1 = lemmatize(q1_t)\n# listq2 = lemmatize(q2_t)","5f422358":"MAX_NB_WORDS = 200000\ntokenizer = Tokenizer(num_words = MAX_NB_WORDS)\ntokenizer.fit_on_texts(list(df['question1'].values.astype(str))+list(df['question2'].values.astype(str)))\n","f459ac53":"# X_train_q1 = tokenizer.texts_to_sequences(np.array(listq1))\nX_train_q1 = tokenizer.texts_to_sequences(df['question1'].values.astype(str))\nX_train_q1 = pad_sequences(X_train_q1, maxlen = 30, padding='post')\n\n# X_train_q2 = tokenizer.texts_to_sequences(np.array(listq2))\nX_train_q2 = tokenizer.texts_to_sequences(df['question2'].values.astype(str))\nX_train_q2 = pad_sequences(X_train_q2, maxlen = 30, padding='post')\n","fc83f5ce":"X_test_q1 = tokenizer.texts_to_sequences(X_testq1.ravel())\nX_test_q1 = pad_sequences(X_test_q1,maxlen = 30, padding='post')\n\nX_test_q2 = tokenizer.texts_to_sequences(X_testq2.astype(str).ravel())\nX_test_q2 = pad_sequences(X_test_q2, maxlen = 30, padding='post')","7f56361e":"word_index = tokenizer.word_index","65e3ee8c":"embedding_index = {}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.200d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_index[word] = vectors\n    f.close()","2b865cbe":"embedding_matrix = np.random.random((len(word_index)+1, 200))\nfor word, i in word_index.items():\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","cb1ba4b8":"# Model for Q1\nimport tensorflow as tf\nfrom tensorflow.keras.layers import BatchNormalization\nmodel_q1 = tf.keras.Sequential()\nmodel_q1.add(Embedding(input_dim = len(word_index)+1,\n                       output_dim = 200,\n                      weights = [embedding_matrix],\n                      input_length = 30))\nmodel_q1.add(LSTM(128, activation = 'tanh', return_sequences = True))\nmodel_q1.add(Dropout(0.2))\nmodel_q1.add(LSTM(128, return_sequences = True))\nmodel_q1.add(LSTM(128))\nmodel_q1.add(Dense(60, activation = 'tanh'))\nmodel_q1.add(Dense(2, activation = 'sigmoid'))","eea65602":"# Model for Q2\nmodel_q2 = tf.keras.Sequential()\nmodel_q2.add(Embedding(input_dim = len(word_index)+1,\n                       output_dim = 200,\n                      weights = [embedding_matrix],\n                      input_length = 30))\nmodel_q2.add(LSTM(128, activation = 'tanh', return_sequences = True))\nmodel_q2.add(Dropout(0.2))\nmodel_q2.add(LSTM(128, return_sequences = True))\nmodel_q2.add(LSTM(128))\nmodel_q2.add(Dense(60, activation = 'tanh'))\nmodel_q2.add(Dense(2, activation = 'sigmoid'))","88e61daa":"# Merging the output of the two models,i.e, model_q1 and model_q2\nmergedOut = Multiply()([model_q1.output, model_q2.output])\n\nmergedOut = Flatten()(mergedOut)\nmergedOut = Dense(100, activation = 'relu')(mergedOut)\nmergedOut = Dropout(0.2)(mergedOut)\nmergedOut = Dense(50, activation = 'relu')(mergedOut)\nmergedOut = Dropout(0.2)(mergedOut)\nmergedOut = Dense(2, activation = 'sigmoid')(mergedOut)","02236dd3":"new_model = Model([model_q1.input, model_q2.input], mergedOut)\nnew_model.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy',\n                 metrics = ['accuracy'])\nhistory = new_model.fit([X_train_q1,X_train_q2],Y_train, batch_size = 2000, epochs = 10)","1271e97e":"y_pred = new_model.predict([X_test_q1, X_test_q2], batch_size=2000, verbose=1)\ny_pred += new_model.predict([X_test_q1, X_test_q2], batch_size=2000, verbose=1)\ny_pred \/= 2","d379e949":"In the solution below, we will be using word embedding methods like GloVe and deep learning methods like LSTM to predict whether the two given questions are duplicates. We will be calculating the similarity between the two texts. Scroll down to understand what methods are used and how they are used to solve this NLP problem. The text preprocessing steps given below are applied to all the NLP problems and are integral part of any NLP problem. If any questions, please comment below.","c7637381":"# Read the train and test dataset","60079911":"GloVe (Global Vectors) is a model for distributed representations. The model is an unsupervised learning algorithm for obtaining vector representation for words. This is taken care of by mapping words into meaningful space where the distance between the words is related to semantic similarity. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.","71d7b512":"## Normalization\nNormalization is the process of converting the token into its basic form (morpheme). Inflection is removed from the token to get the base form of the word. It helps in reducing the number of unique tokens and redundancy in data. It reduces the data dimensionality and removes variation of a word from text.\nThere are two techniques to perform normalization. They are Stemming and Lemmatization.\n\n### Stemming\nStemming is the elementary rule-based process of removal of inflectional forms from a token. The token is converted into its root form. For example, the word \u2018troubled\u2019 is converted into \u2018trouble\u2019 after performing stemming. \n\nThere are different algorithms for stemming but the most common algorithm, which is also known to be empirically effective for English, is Porter\u2019s Algorithm. Porter\u2019s Algorithm consists of 5 phases of word reductions applied sequentially.\n\nSince stemming follows crude heuristic approach that chops off the end of the tokens in the hope of correctly transforming into its root form, it sometimes may generate non-meaningful terms. For example, it may convert the token \u2018increase\u2019 into \u2018increas\u2019, causing the token to lose its meaning.\n\n### Lemmatization\nLemmatization is similar to stemming, difference being that lemmatization refers to doing things properly with use of vocabulary and morphological analysis of words, aiming to remove inflections from the word and to return base or dictionary form of that word, also known as lemma. It does full morphological analysis of the word to accurately identify the lemma for each word. It may use a dictionary such as Wordnet for mapping or some other rule-based approaches.\n\nWe have used the Lemmatization to perform normalizaion. You can use Stemming as well since it has been found that the results yielded by Lemmatization and Stemming are not much different.","85967f30":"In the above dataset, we are given question pairs and a 'is_duplicate' column which tells us whether the question pairs are duplicate of each other or not. We'll train our model over this train dataset.","d2c76de7":"# Text Pre-processing\nText Preprocessing is the first step in the pipeline of Natural Language Processing (NLP), with potential impact in its final process. Text Preprocessing is the process of bringing the text into the form that is predictable and analyzable for a specific task. A task is the combination of approach and domain. For example, extracting top keywords with TF-IDF (approach) from Tweets (domain) is an example of task. The main objective of text preprocessing is to break the text into a form that machine learning algorithms can digest. In this kernel, we will perform the task of text preprocessing on a corpus of quora question pairs and then use the filtered dataset to analyse the similarity between the question pairs","483f33d1":"## Loading Glove word embedding","c9c2bca8":"Long Short Term Memory is a variation of RNN which is used to eliminate the **Vanishing Gradients Problem**. It is much more powerful and complex than other variations of RNN, i.e., GRU. \n\nLSTM operates using three gates: **Input gate, Forget gate and Output gate**. Let\u2019s take an example to understand the procedure of LSTM.\nThe first step in our LSTM is to decide what information we\u2019re going to throw away from the cell state. This decision is made by a sigmoid layer called the forget gate layer. It looks at ht\u22121and xt, and outputs a number between 0 and 1 for each number in the cell state Ct\u22121. A 1 represents \u201ccompletely keep this\u201d while a 0 represents \u201ccompletely get rid of this.\u201d\nThe next step is to decide what new information we\u2019re going to store in the cell state. This has two parts. First, a sigmoid layer called the input gate layer decides which values we\u2019ll update. Next, a tanh layer creates a vector of new candidate values, C~t, that could be added to the state. In the next step, we\u2019ll combine these two to create an update to the state.\nFinally, we need to decide what we\u2019re going to output. First, we run a sigmoid layer which decides what parts of the cell state we\u2019re going to output. Then, we put the cell state through tanh (to push the values to be between \u22121 and 1) and multiply it by the output of the sigmoid gate, so that we only output the parts we decided to.\n","f872a413":"## Padding and sequencing\nNow, we convert all the questions in column 'question1' and 'question2', of both train and test set, into sequences, i.e, in the form of numbers since machine can only process numbers and not texts. \nWe define the maximum length of each question and the questions which contain less than the required length are padded with zeros to make the length of the sentence equal to the mentioned length.\n\nFor example, maxlen = 5\n\nsent1 = ['I', 'love','apples']\n\nAfter sequencing,\n\nsent1 = [1,2,3]\n\nAfter padding,\n\nsent1 = [1,2,3,0,0]","57e012d4":"# Keras text preprocessing\n\nWhen approaching a NLP problem, either you can perform all the above mentioned steps in that order or you can use Keras' Tokenizer class to perform tokenization. In this project, I have tried out Keras' Tokenizer class and it also works pretty good","346391fa":"## Tokenization\nTokenization is defined as a process to split the text into smaller units, i.e., tokens, perhaps at the same time throwing away certain characters, such as punctuation. Tokens could be words, numbers, symbols, n-grams, and characters. N-grams is a combination of n words or characters together. Tokenization does this task by locating word boundaries.\n\nInput: Friends, Romans, Countrymen, lend me your ears\n\nOutput: ['Friends','Romans','Countrymen','lend','me','your','ears']             \n\nMost widely used tokenization process is white space tokenization. In this process, the entire text is split into words by splitting them from whitespaces.\n\nWe have defined a function that will tokenuze the train and test questions in the dataset\n","38b9ff7b":"## Lowercasing\nThis is the simplest technique of text preprocessing which consists of lowercasing each single token of the input text.. It helps in dealing with sparsity issues in the dataset. For example, a text is having mixed-case occurrences of the token \u2018Canada\u2019, i.e., at some places token \u2018canada\u2019 is and in other \u2018Canada\u2019 is used. To eliminate this variation, so that it does not cause further problems, we use lowercasing technique to eliminate the sparsity issue and reduce the vocabulary size.\n\nDespite its excellence in reducing sparsity issue and vocabulary size, it sometimes impacts system\u2019s performance by increasing ambiguity. For example, \u2018Apple is the best company for smartphones \u2018. Here when we perform lowercasing, Apple is transformed into apple and this creates ambiguity as the model is unaware that apple is a company or a fruit and there are higher chances that it may interpret apple as fruit. \n","887c202c":"## Stopwords\nIn the above function, you can see stopwords are being removed from the questions. What are stopwords and why are they removed?\nStop-words are commonly used words in a language. Examples are \u2018a\u2019, \u2019an\u2019, \u2019the\u2019, \u2019is\u2019, \u2019what\u2019 etc. Stop-words are removed from the text so that we can concentrate on more important words and prevent stop-words from being analyzed. If we search \u2018what is text preprocessing\u2019, we want to focus more on \u2018text preprocessing\u2019 rather than \u2018what is\u2019. ","6fafaf96":"# LSTM coding begins !!","a367fcd6":"This is our test data. We'll calculate the similarity between these question pairs to get the idea whether the are duplicates or not.","ce0ad5a3":"MAX_NB_WORDS is a constant which indicates the maximum number of words that should be present.\nNext, we fit out Tokenizer on all the questions in column 'question1' and 'question2'.","81751262":"## Vanishing Gradients Problem\nThe problem that vanilla RNNs face is vanishing gradients problem. Vanilla RNNS are incapable of handling long range dependencies. By long range dependencies, we mean that if we are trying to generate a word based on previous inputs then if the gap between the word to be generated and the previous input considered is small, RNNs will do a great job and able to predict the next word but if the gap is large, RNNs fail to perform. This is called the vanishing gradients problem. \n"}}