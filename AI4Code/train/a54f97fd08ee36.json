{"cell_type":{"f5a1e52c":"code","e946ab27":"code","565e2c2f":"code","a27d646b":"code","aed35ddb":"code","33e461fc":"code","1eb3c76d":"markdown","ce5d6e9e":"markdown","fa748031":"markdown","decdd987":"markdown","c1eec036":"markdown","25c62e5b":"markdown"},"source":{"f5a1e52c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e946ab27":"# roc curve for logistic regression model with optimal threshold\nfrom numpy import sqrt\nfrom numpy import argmax\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve\nfrom matplotlib import pyplot\n\n# generate dataset\nX, y = make_classification(n_samples=10000, n_features=2, n_redundant=0,\n\tn_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=4)\n\n# split into train\/test sets\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2, stratify=y)\n\n# fit a model\nmodel = LogisticRegression(solver='lbfgs')\nmodel.fit(trainX, trainy)\n\n# predict probabilities\nyhat = model.predict_proba(testX)\n\n# keep probabilities for the positive outcome only\nyhat = yhat[:, 1]\n\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(testy, yhat)","565e2c2f":"# calculate the g-mean for each threshold\ngmeans = sqrt(tpr * (1-fpr))\n\n# locate the index of the largest g-mean\nix = argmax(gmeans)\nprint('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))","a27d646b":"for i in enumerate(gmeans):\n    print(i)","aed35ddb":"# plot the roc curve for the model\npyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\npyplot.plot(fpr, tpr, marker='.', label='Logistic')\npyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n\n# axis labels\npyplot.xlabel('False Positive Rate')\npyplot.ylabel('True Positive Rate')\npyplot.legend()\n\n# show the plot\npyplot.show()","33e461fc":"...\n# calculate roc curves\nfpr, tpr, thresholds = roc_curve(testy, yhat)\n\n# get the best threshold\nJ = tpr - fpr\nix = argmax(J)\nbest_thresh = thresholds[ix]\nprint('Best Threshold=%f' % (best_thresh))","1eb3c76d":"#### Goal is to balance between the true positive and the true negative rate. Use Geometric Mean","ce5d6e9e":"best threshold is the black point","fa748031":"we get the same answer as the g-mean","decdd987":"### Youden's J Statistik for Finding the Best Threshold\nBased on: https:\/\/machinelearningmastery.com\/threshold-moving-for-imbalanced-classification\/","c1eec036":"#### Now using Youden's J Statistic, which is faster. It balances true positive and true negative rate similarly.","25c62e5b":"#### The best gmean is chosen"}}