{"cell_type":{"b36eb838":"code","4107030c":"code","1e612ab3":"code","a231cf66":"code","8428daed":"code","7f08ebf5":"code","801a97a5":"code","bc88635f":"code","58f9495f":"code","e0a862dd":"code","71deeaa3":"code","83fc741f":"code","179e73fd":"code","06c85e8c":"code","809a23c2":"code","28e48d9b":"code","64b759da":"code","851d3955":"code","fa3ed477":"code","96101c05":"code","353b4d08":"code","b070135c":"code","491d4f35":"code","7d300905":"code","1e972dc2":"code","16ed7913":"code","fa69a1c2":"code","a9f23dd2":"code","f68949ee":"code","1fee0522":"code","2c594769":"code","9376fb0d":"code","5e0fe519":"code","c70377c3":"code","34f026ac":"code","c1adfb27":"code","81d7aee3":"code","8493c3a5":"code","0b847963":"code","3910febb":"code","27b773ce":"code","b22d6a67":"code","0963f872":"code","ddaac30d":"code","72f01ca7":"code","b894c0b9":"code","35cd2bf7":"code","2c0b6689":"code","f877eaa1":"code","9ee9f7d9":"code","d0ef7338":"code","051ba6bd":"code","59ac3d56":"code","775dc43f":"code","e203aed3":"code","84350e2a":"code","199eb687":"code","ebf121bf":"code","c03038bb":"code","b271c5ee":"code","80b87e4c":"code","1c274f8a":"code","043b6a4f":"code","a7019c48":"code","08c32880":"code","9d22954f":"code","2db17b52":"code","a4bbdb84":"code","e7967cdc":"code","44608f9b":"code","e1c798b4":"code","d5409151":"code","8fb5e0fc":"code","e13feaae":"code","e7d6821e":"code","bd263acd":"markdown","56683f54":"markdown","49f346e8":"markdown","4c80a260":"markdown","866e92de":"markdown","e7702b91":"markdown","442f6f35":"markdown"},"source":{"b36eb838":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport sklearn\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import SimpleImputer, MissingIndicator\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder, Normalizer, StandardScaler, OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin, clone\nimport sklearn_pandas\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.special import boxcox1p\nimport csv\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sys\nimport scipy\n\nprint('Environment specification:\\n')\nprint('python', '%s.%s.%s' % sys.version_info[:3])\n\nfor mod in np, scipy, sns, sklearn, pd:\n    print(mod.__name__, mod.__version__)\n","4107030c":"# train data\ndata_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n# test data\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","1e612ab3":"\nall_data = pd.concat((data_df.loc[:,:], test_df.loc[:, :]))\nall_data.head()","a231cf66":"# Basic summary:\ndata_df['SalePrice'].describe()","8428daed":"# The Density Plot of SalePrice\nsns.distplot(data_df['SalePrice'])","7f08ebf5":"sns.boxplot(data_df['SalePrice'])","801a97a5":"\n# Positive Skeweness:\ndata_df['SalePrice'].skew()","bc88635f":"data_df[\"SalePrice\"] = np.log1p(data_df[\"SalePrice\"])","58f9495f":"# SalePrice after Log-transformation\nsns.distplot(data_df[\"SalePrice\"])\nplt.title(\"Density plot of SalePrice after Log Transformation\")","e0a862dd":"y_train = data_df[\"SalePrice\"]","71deeaa3":"col_nan = data_df.isna().sum() \/ data_df.shape[0]","83fc741f":"plt.figure(figsize=(8, 5))\nsns.set(font_scale=1.2)\ncol_nan[col_nan > 0.01].plot(kind = \"barh\")\nplt.title(\"Features with the highest percentage of Nan values\")","179e73fd":"\"\"\" In some of my trials I removed columns with more than 90% od NaN values, \n    but finally I missed this step and left them for further analysis.\"\"\"\n\n# drop_out_columns = data_df.columns[[inx for inx, i in enumerate(col_nan >= 0.95) if i==True]]\n# data_df = data_df.drop(drop_out_columns, axis=1)\n# test_df = test_df.drop(drop_out_columns, axis=1)","06c85e8c":"# Dropping columns for both train and test dataset\ndata_df = data_df.drop(\"Id\", axis=1)\ntest_df = test_df.drop(\"Id\", axis=1)\n","809a23c2":"data_df = data_df.drop([\"Street\", \"Utilities\"], axis=1)\ntest_df = test_df.drop([\"Street\", \"Utilities\"], axis=1)","28e48d9b":"def remove_outliers(dataset, threshold, columns=None, removed = False):\n    \"\"\" \n    Z-score method.\n    Function returns a dataframe without rows labeled as 'outliers' according to the given threshold.  \n    ---------------\n    If columns = None, transform all numerical columns.\n    If removed = True, return also dataframe with removed rows.\n    \"\"\"\n    if columns==None:\n        numerics = ['int64','float64']\n        columns = dataset.select_dtypes(include=numerics).columns\n    \n    tmp = dataset.copy()\n    z = np.abs(stats.zscore(tmp[columns]))\n    outliers = [row.any() for row in (z > threshold)]  \n    outliers_idxs = tmp.index[outliers].tolist()\n    print(\"Number of removed rows = {}\".format(len(outliers_idxs)))\n    if removed: return dataset.drop(outliers_idxs), tmp.loc[outliers]\n    else: return dataset.drop(outliers_idxs)","64b759da":"\n# clear_data is a dataframe with train data after removing outliers\n\n# clear_data, removed_data = remove_outliers(data_df, threshold = 3, removed=True, \n#                              columns=['GrLivArea'])","851d3955":"plt.figure(figsize=(8, 5))\nsns.set(font_scale=1.2)\nsns.scatterplot(data_df[\"GrLivArea\"], data_df[\"SalePrice\"])\n# plt.vlines(4500, ymax=800000, ymin=0)\nplt.title(\"GrLivArea vs SalePrice\")","fa3ed477":"# I decided to remove those records where 'GrLivArea' is more than 4500. We can see on plot that they have a vey low price.\nclear_data = data_df.drop(data_df[(data_df['GrLivArea']>4500)].index)","96101c05":"# Concatenate all data together - both train and test\ntrain_ = clear_data.drop(['SalePrice'], axis=1)\nall_data = pd.concat([data_df, test_df]).reset_index(drop=True)\n","353b4d08":"print(\"Is there YearBuilt more than 2017 ? : \", all_data[all_data.YearBuilt > 2017].count()[0] != 0)\nprint(\"Is there GarageYrBlt more than 2017 ? : \", all_data[all_data.GarageYrBlt > 2017].count()[0] != 0)","b070135c":"\nall_data[all_data.GarageYrBlt > 2017].GarageYrBlt #It seems like it is a typo","491d4f35":"all_data.loc[2590, 'GarageYrBlt'] = 2007\n","7d300905":"neigh_lot_frontage = all_data.groupby('Neighborhood')['LotFrontage'].agg([\"mean\", \"median\"])\nneigh_lot_frontage['avg_mean_median'] = (neigh_lot_frontage['mean'] + neigh_lot_frontage['median'] )\/ 2\nneigh_lot_frontage","1e972dc2":"# transformation into medians\nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","16ed7913":"\ndef convert_to_string(df, columns):\n    df[columns] = df[columns].astype(str)\n    return df","fa69a1c2":"num_features = all_data.select_dtypes(include=['int64','float64']).columns\nnum_features_to_constant = ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', \"MasVnrArea\"] \nnum_features_to_median = [feature for feature in num_features if feature not in num_features_to_constant + [\"SalePrice\"]]","a9f23dd2":"num_to_categ_features = ['MSSubClass', 'OverallCond']#, 'YrSold', 'MoSold']\n\nall_data = convert_to_string(all_data, columns = num_to_categ_features)","f68949ee":"# Generating numerical features as input to DataFrameMapper.  \nnumeric_features_median = sklearn_pandas.gen_features(columns=[num_features_to_median], \n                                               classes=[{'class': SimpleImputer, \n                                                         'strategy': 'median', \n                                                         'missing_values' : np.nan}])\n\nnumeric_features_zero = sklearn_pandas.gen_features(columns=[num_features_to_constant], \n                                               classes=[{'class': SimpleImputer, \n                                                         'strategy': 'constant',\n                                                         'fill_value' : 0, \n                                                         'missing_values' : np.nan}])\n\nmissing_val_imputer = sklearn_pandas.DataFrameMapper(numeric_features_median + numeric_features_zero)\n\n# Fitting\nimputed_median = missing_val_imputer.fit(all_data)\n\n# Transformation\nimputed_features = imputed_median.transform(all_data)\n\n# Putting into dataframe\nimputed_df = pd.DataFrame(imputed_features, index=all_data.index, columns=num_features_to_median + num_features_to_constant)","1fee0522":"# Selecting category features\ncat_feats = all_data.select_dtypes(include=['object']).columns","2c594769":"none_conversion = [(\"MasVnrType\",\"None\"),\n                  (\"BsmtQual\",\"NA\"), \n                  (\"Electrical\", \"SBrkr\"),\n                  (\"BsmtCond\",\"TA\"),\n                  (\"BsmtExposure\",\"No\"),\n                  (\"BsmtFinType1\",\"No\"),\n                  (\"BsmtFinType2\",\"No\"),\n                  (\"CentralAir\",\"N\"),\n                  (\"Condition1\",\"Norm\"), \n                  (\"Condition2\",\"Norm\"),\n                  (\"ExterCond\",\"TA\"),\n                  (\"ExterQual\",\"TA\"), \n                  (\"FireplaceQu\",\"NA\"),\n                  (\"Functional\",\"Typ\"),\n                  (\"GarageType\",\"No\"), \n                  (\"GarageFinish\",\"No\"), \n                  (\"GarageQual\",\"NA\"), \n                  (\"GarageCond\",\"NA\"), \n                  (\"HeatingQC\",\"TA\"), \n                  (\"KitchenQual\",\"TA\"), \n                  (\"Functional\",\"Typ\"), \n                  (\"GarageType\",\"No\"), \n                  (\"GarageFinish\",\"No\"), \n                  (\"GarageQual\",\"No\"), \n                  (\"GarageCond\",\"No\"), \n                  (\"HeatingQC\",\"TA\"), \n                  (\"KitchenQual\",\"TA\"),\n                  (\"MSZoning\", \"None\"),\n                  (\"Exterior1st\", \"VinylSd\"), \n                  (\"Exterior2nd\", \"VinylSd\"), \n                  (\"SaleType\", \"WD\")]","9376fb0d":"def none_transform(df, conversion_list):\n    ''' Function that converts missing categorical values \n    into specific strings according to \"conversion_list\" \n    \n    Returns the dataframe after transformation.\n    '''\n    for col, new_str in conversion_list:\n        df.loc[:, col] = df.loc[:, col].fillna(new_str)\n    return df","5e0fe519":"\n# Applying the \"none_transform\" function \nall_data = none_transform(all_data, none_conversion)","c70377c3":"len(all_data.columns)","34f026ac":"\n# collecting the numeric features without considering SalePrice\nnumeric_features = [feat for feat in num_features if feat not in ['SalePrice']] \n\n# selecting columns with skew more than 0.5\nskewed_features = all_data[num_features].apply(lambda x: x.dropna().skew())\nskewed_features = skewed_features[skewed_features > 0.5].index\nprint(\"\\nHighly skewed features: \\n\\n{}\".format(skewed_features.tolist()))","c1adfb27":"\n# Applying log-transformation \n# all_data[skewed_features] = np.log1p(all_data[skewed_features])# test_df[skewed_features] = np.log1p(test_df[skewed_features])","81d7aee3":"#The \u201coptimal lambda\u201d is the one that results in the best approximation of a normal distribution curve. I selected lambda= 0.15.\n\nlambda_ = 0.15\nfor feature in skewed_features:\n    all_data[feature] = boxcox1p(all_data[feature], lambda_)","8493c3a5":"class OrderedLabelTransformer(BaseEstimator, TransformerMixin):\n    orderDict = {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5}\n    \n    @staticmethod\n    def get_dict(X):\n        FirstDict = {\"Po\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4}\n        SecondDict = {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5}\n        ThirdDict = {\"NA\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4}\n        for d in [FirstDict, SecondDict, ThirdDict]:\n            if set(X) == set(d): \n                return d\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        def get_label(t):\n            return self.orderDict[t]\n        return np.array([get_label(n) for n in X])","0b847963":"class NeighborhoodTransformer(BaseEstimator, TransformerMixin):\n    neighborhoodsmap = {'StoneBr' : 2, 'NridgHt' : 2, 'NoRidge': 2, \n                        'MeadowV' : 0, 'IDOTRR' : 0, 'BrDale' : 0 ,\n                        'CollgCr': 1, 'Veenker' : 1, 'Crawfor' : 1,\n                        'Mitchel' : 1, 'Somerst' : 1, 'NWAmes' : 1,\n                        'OldTown' : 1, 'BrkSide' : 1, 'Sawyer' : 1, \n                        'NAmes' : 1, 'SawyerW' : 1, 'Edwards' : 1,\n                        'Timber' : 1, 'Gilbert' : 1, 'ClearCr' : 1,\n                        'NPkVill' : 1, 'Blmngtn' : 1, 'SWISU' : 1,\n                        'Blueste': 1}\n\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        def get_label(t):\n            return self.neighborhoodsmap[t]\n        return np.array([get_label(n) for [n] in X])","3910febb":"\n# Generating features:\norder_feats = [\"ExterQual\", \"ExterCond\", \"HeatingQC\", \"KitchenQual\", \"BsmtQual\", \n               \"BsmtCond\", \"FireplaceQu\", \"GarageQual\", \"GarageCond\"]\n\noriginal_features_df = all_data[order_feats + ['Neighborhood']] # we need to save original values for one-hot encoding\n\norder_features = sklearn_pandas.gen_features(order_feats, [OrderedLabelTransformer])\nneighb_features = [(['Neighborhood'], [NeighborhoodTransformer()])]\n\n# Pipeline\nlabel_encoder = sklearn_pandas.DataFrameMapper(neighb_features + order_features)\n\n# The list with order of column names\ncols = [\"Neighborhood\"] + order_feats\n\n# Transformation both train and test set\ntransformed_feats = label_encoder.fit_transform(all_data)\n# Putting transformed features into dataframe\ntransformed_df = pd.DataFrame(transformed_feats, index=all_data.index, columns=cols)","27b773ce":"original_features_df.shape\n","b22d6a67":"# feature without any transformation till now\nrest_features = set(pd.concat([imputed_df, original_features_df],axis=1).columns).symmetric_difference(set(all_data.columns))\nrest_features_df = all_data[list(rest_features)]","0963f872":"\nall_data = pd.concat([imputed_df, original_features_df, rest_features_df],axis=1)","ddaac30d":"\nall_data.shape","72f01ca7":"# Total Squere Feet for house\nall_data[\"TotalSqrtFeet\"] = all_data[\"GrLivArea\"] + all_data[\"TotalBsmtSF\"]\n# test_df[\"TotalSqrtFeet\"] = test_df[\"GrLivArea\"] + test_df[\"TotalBsmtSF\"]\n\n# Total number of bathrooms\nall_data[\"TotalBaths\"] = all_data[\"BsmtFullBath\"] + (all_data[\"BsmtHalfBath\"]  * .5) + all_data[\"FullBath\"] + (all_data[\"HalfBath\"]* .5)\n# test_df[\"TotalBaths\"] = test_df[\"BsmtFullBath\"] + (test_df[\"BsmtHalfBath\"]  * .5) + test_df[\"FullBath\"] + (test_df[\"HalfBath\"]* .5)","b894c0b9":"# If the house has a garage\nall_data['Isgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has a fireplace\nall_data['Isfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has a pool\nall_data['Ispool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has second floor\nall_data['Issecondfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has Open Porch\nall_data['IsOpenPorch'] = all_data['OpenPorchSF'].apply(lambda x: 1 if x > 0 else 0)\n\n# If the house has Wood Deck\nall_data['IsWoodDeck'] = all_data['WoodDeckSF'].apply(lambda x: 1 if x > 0 else 0)","35cd2bf7":"all_data = all_data.drop([\"SalePrice\"], axis = 1)\n\nhot_one_features = pd.get_dummies(all_data).reset_index(drop=True)\nhot_one_features.shape","2c0b6689":"all_data = pd.concat([transformed_df, hot_one_features],axis=1)","f877eaa1":"\ntrain_preprocessed = all_data.iloc[:len(data_df),:]\ntest_preprocessed = all_data.iloc[len(train_preprocessed):,:]\nprint(len(test_preprocessed) == len(test_df))","9ee9f7d9":"# model","d0ef7338":"X_train = train_preprocessed","051ba6bd":"from sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\nfrom sklearn.linear_model import ElasticNet, Lasso, ElasticNetCV\nfrom sklearn.ensemble import  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.base import RegressorMixin\nimport lightgbm as lgb","59ac3d56":"print('Environment specification:\\n')\nfor mod in sklearn, xgb, lgb:\n    print(mod.__name__, mod.__version__)","775dc43f":"def rmse(model):\n    n_folds=5\n    kfold = KFold(n_folds, random_state=42, shuffle=True).get_n_splits(X_train)\n    rmse_score = np.sqrt(-cross_val_score(model, X_train, y_train, scoring = \"neg_mean_squared_error\", cv = kfold, verbose = -1, n_jobs=-1))\n    return(np.mean(rmse_score))","e203aed3":"\nlr_model = make_pipeline(RobustScaler(), LinearRegression()) #TODO: why Robust Scaler?\n\nlr_model.fit(X_train, y_train)\ny_train_pred = lr_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for Linear Regression: {:.3f}\".format(rmse(lr_model)))","84350e2a":"sns.set(font_scale=1.5)\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_train, y_train_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs. Predicted Prices\")","199eb687":"# Residual plot - result should be randomly located around the 0 value\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_train_pred, y_train_pred - y_train)\nplt.title(\"Residual Plot\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")","ebf121bf":"lasso_model = make_pipeline(RobustScaler(), \n                         LassoCV(alphas = [0.0004, 0.0005, 0.0006],\n                                 random_state = 0,\n                                 cv = 10))\n\nlasso_model.fit(X_train, y_train)\n\ny_train_pred = lasso_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\n# print(\"Best alpha : {}\", lasso_model.alpha_)\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for LASSO: {:.3f}\".format(rmse(lasso_model)))","c03038bb":"gbr = GradientBoostingRegressor(random_state=0)\nparam_grid = {'n_estimators': [2500],\n              'max_features': [13],\n              'max_depth': [5],\n              'learning_rate': [0.05],\n              'subsample': [0.8],\n             'random_state' : [5]}\n                              \ngb_model = GridSearchCV(estimator=gbr, param_grid=param_grid, n_jobs=1, cv=5)\ngb_model.fit(X_train, y_train)","b271c5ee":"# Plotting predictions\nplt.figure(figsize=(10,6))\nsns.scatterplot(y_train_pred, y_train)\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted Prices\")\nplt.ylabel(\"Real  Prices\")\nplt.show","80b87e4c":"gbr = GradientBoostingRegressor(random_state=0)\nparam_grid = {'n_estimators': [2500],\n              'max_features': [13],\n              'max_depth': [5],\n              'learning_rate': [0.05],\n              'subsample': [0.8],\n             'random_state' : [5]}\n                              \ngb_model = GridSearchCV(estimator=gbr, param_grid=param_grid, n_jobs=1, cv=5)\ngb_model.fit(X_train, y_train)","1c274f8a":"y_train_pred = gb_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\nprint('Best Parameters: {}'.format(gb_model.best_params_))\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for GB: {:.3f}\".format(rmse(gb_model)))","043b6a4f":"en_model = ElasticNetCV(alphas = [0.0001, 0.0003, 0.0004, 0.0006], \n                        l1_ratio = [.9, .92], \n                        random_state = 0,\n                        cv=10)","a7019c48":"xgbreg = xgb.XGBRegressor(seed=0)\nparam_grid2 = {'n_estimators': [2000], \n              'learning_rate': [0.05],\n              'max_depth': [3, 7],\n              'subsample': [0.8],\n              'colsample_bytree': [0.45, 0.75]}\n    \nxgb_model = GridSearchCV(estimator=xgbreg, param_grid=param_grid2, n_jobs=1, cv=10)\nxgb_model.fit(X_train, y_train)\n\ny_train_pred = xgb_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\nprint('\\n\\nBest Parameters: {}'.format(xgb_model.best_params_))\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for XGB: {:.3f}\".format(rmse(xgb_model)))","08c32880":"en_model.fit(X_train, y_train)\n\ny_train_pred = en_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)","9d22954f":"print(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for ElasticNet: {:.3f}\".format(rmse(en_model)))","2db17b52":"lgb_model = lgb.LGBMRegressor(objective='regression', num_leaves=5,\n                              learning_rate=0.05, n_estimators=800,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nlgb_model.fit(X_train, y_train)\n\ny_train_pred = lgb_model.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))\nprint(\"RMSE score for LGBMRegressor: {:.4f}\".format(rmse(lgb_model)))","a4bbdb84":"from sklearn.ensemble import BaggingRegressor\n\nmodel2 = sklearn.ensemble.BaggingRegressor(base_estimator = en_model, n_estimators = 50, \n                                           max_samples = 30, max_features = 200, verbose = 3, n_jobs = 3)\nmodel2.fit(X_train, y_train)\n\ny_train_pred = model2.predict(X_train)\nMSE_train = np.mean((y_train_pred - y_train)**2)\n\nprint(\"Mean Squared Error = {:.8f}\".format(MSE_train))","e7967cdc":"print(\"RMSE score for BaggingRegressor: {:.4f}\".format(rmse(model2)))","44608f9b":"from mlxtend.regressor import StackingCVRegressor\nfrom sklearn.pipeline import make_pipeline","e1c798b4":"lasso_model = make_pipeline(RobustScaler(),\n                      LassoCV(max_iter=1e7, alphas = [0.0005],\n                              random_state = 42, cv=5))\n\nelasticnet_model = make_pipeline(RobustScaler(), \n                           ElasticNetCV(max_iter=1e7, alphas=[0.0005], \n                                        cv=5, l1_ratio=0.9))\n\nlgbm_model = make_pipeline(RobustScaler(),\n                        lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                                      learning_rate=0.05, n_estimators=800,\n                                      max_bin = 55, bagging_fraction = 0.8,\n                                      bagging_freq = 5, feature_fraction = 0.23,\n                                      feature_fraction_seed = 9, bagging_seed=9,\n                                      min_data_in_leaf = 6, \n                                      min_sum_hessian_in_leaf = 11))\n\nxgboost_model = make_pipeline(RobustScaler(),\n                        xgb.XGBRegressor(learning_rate = 0.01, n_estimators=3400, \n                                     max_depth=3,min_child_weight=0 ,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective= 'reg:linear',nthread=4,\n                                     scale_pos_weight=1,seed=27, \n                                     reg_alpha=0.00006))","d5409151":"stack_regressor = StackingCVRegressor(regressors=(lasso_model, elasticnet_model, xgboost_model, lgbm_model), \n                               meta_regressor=xgboost_model, use_features_in_secondary=True)","8fb5e0fc":"stack_model = stack_regressor.fit(np.array(X_train),  np.array(y_train))\nen_preds = en_model.predict(test_preprocessed)\nlasso_preds = lasso_model.predict(test_preprocessed)\nstack_gen_preds = stack_model.predict(test_preprocessed)\nlgbm_preds = lgb_model.predict(test_preprocessed)\n# Weighted predictions\nstack_preds = ((0.2*en_preds) + (0.25*lasso_preds) + (0.15*lgbm_preds) + (0.4*stack_gen_preds))","e13feaae":"predictions_df = pd.DataFrame(np.expm1(stack_preds), \n                              index = test_preprocessed.index+1, \n                              columns=[\"SalePrice\"])\npredictions_df.index.name = \"Id\"\npredictions_df.head()","e7d6821e":"predictions_df[\"SalePrice\"].to_csv(\"my_predictions.csv\", header=True)\n","bd263acd":"# cloumns with Nan values","56683f54":"# submission","49f346e8":"# split data---> train\/test","4c80a260":"# 1.set up","866e92de":"# standardlize","e7702b91":"# explore data and anlysis","442f6f35":"# 2.reading  train and test as dataframe"}}