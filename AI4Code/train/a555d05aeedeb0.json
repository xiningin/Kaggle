{"cell_type":{"df55345c":"code","597fcc90":"code","b650cb27":"code","99fdcabc":"code","5309fb54":"code","d890a014":"code","035217b7":"code","fb99cad2":"code","eae6f471":"code","75527a1d":"code","0af9a9bc":"code","39c7f044":"code","702dc328":"code","5b83a8a5":"code","1c0631c6":"code","9dbd416d":"code","b59984b8":"code","d417d2be":"code","e73a5d5c":"code","6b8b595c":"code","d9be6500":"code","7fac74b0":"code","050e3ffe":"code","a6142042":"code","5e2ebc38":"code","d46480aa":"code","010be618":"code","c5efd3f1":"code","e299edbb":"code","170138a0":"code","e478e403":"code","ccc98dfe":"code","0b937c62":"code","fd2716f8":"code","5993ebea":"markdown","3e6ef5d4":"markdown","295110e8":"markdown","414638d8":"markdown","186cf903":"markdown","5905c6bc":"markdown","d7c0aab0":"markdown","f931db2e":"markdown","9696f944":"markdown","b098d397":"markdown","144a9558":"markdown","2fbd1b3d":"markdown","2ea708ee":"markdown","c5f98aeb":"markdown","6b061dbd":"markdown","92a8454a":"markdown","91491855":"markdown","f8526590":"markdown","078a4975":"markdown","b77235f0":"markdown","c7fb7c5a":"markdown","98be500f":"markdown","35a719ad":"markdown","744cae58":"markdown","6e691275":"markdown","46540dd4":"markdown","5a41452e":"markdown","0e50ae02":"markdown","790d7be1":"markdown","cdea00cf":"markdown","63b9ed4b":"markdown","c278d99e":"markdown","02f05eee":"markdown","c03a5187":"markdown","21d5f381":"markdown","c5ce5fb2":"markdown","f239a188":"markdown","19773e1a":"markdown","f2d94567":"markdown","d43ce82f":"markdown","8198a78a":"markdown","f4ab5084":"markdown","206a19e5":"markdown","0757a516":"markdown","57e3ea77":"markdown","60cae3fa":"markdown","cf99f5e7":"markdown","e3c5fa36":"markdown","c34e5d6c":"markdown","156799e5":"markdown","373c78da":"markdown","4979a88d":"markdown","b3b8cf3c":"markdown","5a77902f":"markdown","2ad35a88":"markdown","0c314b24":"markdown","8db0de7a":"markdown","29592bc0":"markdown","f566e198":"markdown","5ea953bb":"markdown","e89db257":"markdown","f2609334":"markdown","63261afd":"markdown","9759b8d0":"markdown"},"source":{"df55345c":"# conda create -n msc-ds-core-pds-assignment-1 python=3.6\n# source activate msc-ds-core-pds-assignment-1\n# pip install -r requirements.txt","597fcc90":"from __future__ import print_function\nimport sys\nimport os\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# We need this for palette management\nfrom matplotlib.colors import ListedColormap\nimport seaborn as sns\nfrom sodapy import Socrata\n\nfrom IPython.display import display, HTML\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# Matplotlib config\n%matplotlib inline\n\n# Seaborn config\nsns.set(style = \"darkgrid\")\n\n# Set a random seed so that possible random operations performed by us can be replicated by others.\nnp.random.seed(19730618)","b650cb27":"# Global functions\n\n# Helper function that will redirect printing to stderr.\n# Used when printing for exception handling etc.\ndef print_stderr(*args, **kwargs):\n    print(*args, file = sys.stderr, **kwargs)\n\n# Returns the ceiling of a number, rounded according to the 'to' parameter\n# i.e. to thousands, to hundred-thousands etc.\ndef ceiling(num, to = 100000):\n    y, z = divmod(num, to)\n    \n    return int(to * (y + 1))\n\ndef normalize(df_column):\n    return (df_column - df_column.min()) \/ (df_column.max() - df_column.min())","99fdcabc":"crimes_csv = \"..\/input\/city_of_chicago_crimes_2001_to_present.csv\"\n\ncrimes_df = pd.read_csv(crimes_csv, delimiter = \",\", header = 0)","5309fb54":"crimes_df.head(5)","d890a014":"crimes_df.columns","035217b7":"crimes_df = crimes_df.drop(columns = [\n    \"ID\", \n    \"Case Number\", \n    \"Block\", \n    \"IUCR\", \n    \"Description\", \n    \"Location Description\", \n    \"Arrest\",\n    \"Domestic\",\n    \"FBI Code\", \n    \"Year\", \n    \"Updated On\",\n    \"Location\"\n])","fb99cad2":"# Replace any spaces with underscores\ncrimes_df.columns = crimes_df.columns.str.replace(\" \", \"_\")\n\n# Make sure we strip any invisible white spaces\ncrimes_df.columns = crimes_df.columns.str.strip()\n\n# Set all columns names to lower case\ncrimes_df.columns = crimes_df.columns.str.lower()","eae6f471":"crimes_df.columns","75527a1d":"crimes_df.isnull().any()","0af9a9bc":"crimes_df[\"beat\"] = crimes_df[\"beat\"].fillna(value = 0.0)\ncrimes_df[\"district\"] = crimes_df[\"district\"].fillna(value = 0.0)\ncrimes_df[\"ward\"] = crimes_df[\"ward\"].fillna(value = 0.0)\ncrimes_df[\"community_area\"] = crimes_df[\"community_area\"].fillna(value = 0.0)\ncrimes_df[\"x_coordinate\"] = crimes_df[\"x_coordinate\"].fillna(value = 0.0)\ncrimes_df[\"y_coordinate\"] = crimes_df[\"y_coordinate\"].fillna(value = 0.0)\ncrimes_df[\"latitude\"] = crimes_df[\"latitude\"].fillna(value = 0.0)\ncrimes_df[\"longitude\"] = crimes_df[\"longitude\"].fillna(value = 0.0)","39c7f044":"crimes_df.isnull().any()","702dc328":"for column in crimes_df.columns:\n    print(\"Column name: \", column, \" \/ Data type: \", type(crimes_df[column][0]))","5b83a8a5":"# Including the date format dramatically increases performance.\ncrimes_df[\"date\"] = pd.to_datetime(crimes_df[\"date\"], format = \"%m\/%d\/%Y %I:%M:%S %p\", utc = True)\ncrimes_df[\"beat\"] = crimes_df[\"beat\"].astype(np.int64)\ncrimes_df[\"district\"] = crimes_df[\"district\"].astype(np.int64)\ncrimes_df[\"ward\"] = crimes_df[\"ward\"].astype(np.int64)\ncrimes_df[\"community_area\"] = crimes_df[\"community_area\"].astype(np.int64)","1c0631c6":"crimes_df = crimes_df.set_index(\"date\")\ncrimes_df = crimes_df.sort_index()","9dbd416d":"crimes_df.index","b59984b8":"crimes_df = crimes_df.loc['2001-01-01':'2017-12-31']","d417d2be":"# Convert primary_type to a categorical variable\ncrimes_df[\"primary_type\"] = crimes_df[\"primary_type\"].astype(\"category\")","e73a5d5c":"###\n# Group our data\n##\n# Group our data by year\ncrimes_gb_y = crimes_df.groupby(by = [crimes_df.index.year], axis = 0)\n\n# Count the number of incidents of our grouped set\ncrimes_gb_y_pt_count = crimes_gb_y[\"primary_type\"].count()\n\n# Get the max crime count so that we can normalize the plotting axis\ncrimes_gb_y_pt_max = crimes_gb_y_pt_count.max()\n\n###\n# Plot our data\n##\nplot_main_title_font = {\n    \"family\": \"sans serif\",\n    \"color\":  \"black\",\n    \"weight\": \"bold\",\n    \"size\": 18,\n}\n\nplot_axis_title_font = {\n    \"family\": \"sans serif\",\n    \"color\":  \"black\",\n    \"weight\": \"bold\",\n    \"size\": 14,\n}\n\nax = crimes_gb_y_pt_count.plot(\n    kind = \"line\", \n    marker = \".\",\n    stacked = False, \n    figsize = (16, 10), \n    use_index = True, \n    legend = False\n)\n\n# Normalize the y-axis values\nax.set_ylim(0, ceiling(crimes_gb_y_pt_max, to = 1000000))\n\n# Set graph title\nax.set_title(label = \"Yearly count of crimes\", fontdict = plot_main_title_font, pad = 20)\n\n# Set axis proper labels\nax.set_xlabel(xlabel = \"Year\", fontdict = plot_axis_title_font, labelpad = 20)\nax.set_ylabel(ylabel = \"Incident frequency\", fontdict = plot_axis_title_font, labelpad = 20)\n\n# Set x-axis ticks to be the index of our series object\nax.set_xticks(ticks = crimes_gb_y_pt_count.index);\n\n# Auto-format year labels\nax.get_figure().autofmt_xdate();","6b8b595c":"###\n# Compute some statistics\n##\n# Convert the series to a dataframe\ncrimes_gb_y_pt_count_df = crimes_gb_y_pt_count.to_frame()\n\n# Compute the per year incident count difference\ncrimes_gb_y_pt_count_df[\"diff_from_previous_year\"] = crimes_gb_y_pt_count_df[\"primary_type\"].diff(periods = 1)\n\n# Compute the per year incident difference as a percentage\ncrimes_gb_y_pt_count_df[\"diff_from_previous_year_perc\"] = crimes_gb_y_pt_count_df[\"primary_type\"].pct_change(periods = 1)\n\ncrimes_gb_y_pt_count_df","d9be6500":"num_primary_types = 10\nnum_months = 12\n\n###\n# Group our data\n##\n# Group our data by month\ncrimes_gb_mpt = crimes_df.groupby(by = [crimes_df.index.month, \"primary_type\"], axis = 0)\n\n# Count the number of incidents of our grouped set\ncrimes_gb_mpt_pt_count = crimes_gb_mpt[\"primary_type\"].count()\n\n# We may chose to display only some of the crime class, for readability\nif num_primary_types != None:\n    crimes_gb_mpt_pt_count = crimes_gb_mpt_pt_count.nlargest(num_primary_types * num_months)\n\ncrimes_gb_mpt_pt_count_un = crimes_gb_mpt_pt_count.unstack()\n\n###\n# Plot our data\n##\nmonths = {\n    1: \"January\",\n    2: \"February\",\n    3: \"March\",\n    4: \"April\",\n    5: \"May\",\n    6: \"June\",\n    7: \"July\",\n    8: \"August\",\n    9: \"September\",\n    10: \"October\",\n    11: \"November\",\n    12: \"December\",\n}\n\nplot_main_title_font = {\n    \"family\": \"sans serif\",\n    \"color\":  \"black\",\n    \"weight\": \"bold\",\n    \"size\": 18,\n}\n\nplot_axis_title_font = {\n    \"family\": \"sans serif\",\n    \"color\":  \"black\",\n    \"weight\": \"bold\",\n    \"size\": 14,\n}\n\n# Since the catecorical variable 'primary_type' has many levels,  \n# we are required to create a color map such that different levels\n# can be as distinguishable as possible...\ncat_cmap = ListedColormap(sns.color_palette(\"BrBG\", 10).as_hex())\n\nax = crimes_gb_mpt_pt_count_un.plot(\n    kind = \"bar\", \n    stacked = True, \n    figsize = (14, 10), \n    use_index = False, \n    legend = True,\n    cmap = cat_cmap\n)\n\n# Normalize the y-axis values\nax.set_ylim(0, ceiling(crimes_gb_mpt_pt_count.max(), to = 1000000))\n\n# Set graph title\nax.set_title(label = \"Aggregated monthly count of crimes\", fontdict = plot_main_title_font, pad = 20)\n\n# Set axis proper labels\nax.set_xlabel(xlabel = \"Month\", fontdict = plot_axis_title_font, labelpad = 20)\nax.set_ylabel(ylabel = \"Aggregated incident frequency\", fontdict = plot_axis_title_font, labelpad = 20)\n\n# Set x-axis ticks and corresponding labels to be the index of our series object\nax.set_xticks(ticks = crimes_gb_mpt_pt_count.index.levels[0] - 1);\nax.set_xticklabels(labels = [months[x] for x in crimes_gb_mpt_pt_count.index.levels[0]])\n\n# Auto-format year labels\nax.get_figure().autofmt_xdate();\n\nax.legend(bbox_to_anchor = (1, 1.01));","7fac74b0":"# We need this for palette management\nfrom matplotlib.colors import ListedColormap\n\nnum_primary_types = 10\nnum_hours = 24\n\n###\n# Group our data\n##\n# Group our data by hour\ncrimes_gb_hpt = crimes_df.groupby(by = [crimes_df.index.hour, \"primary_type\"], axis = 0)\n\n# Count the number of incidents of our grouped set\ncrimes_gb_hpt_pt_count = crimes_gb_hpt[\"primary_type\"].count()\n\n# We may chose to display only some of the crime class, for readability\nif num_primary_types != None:\n    crimes_gb_hpt_pt_count = crimes_gb_hpt_pt_count.nlargest(num_primary_types * num_hours)\n    \ncrimes_gb_hpt_pt_count_un = crimes_gb_hpt_pt_count.unstack()\n\n###\n# Plot our data\n##\nplot_main_title_font = {\n    \"family\": \"sans serif\",\n    \"color\":  \"black\",\n    \"weight\": \"bold\",\n    \"size\": 18,\n}\n\nplot_axis_title_font = {\n    \"family\": \"sans serif\",\n    \"color\":  \"black\",\n    \"weight\": \"bold\",\n    \"size\": 14,\n}\n\n# Since the catecorical variable 'primary_type' has many levels,  \n# we are required to create a color map such that different levels\n# can be as distinguishable as possible...\ncat_cmap = ListedColormap(sns.color_palette(\"BrBG\", 10).as_hex())\n\nax = crimes_gb_hpt_pt_count_un.plot(\n    kind = \"bar\", \n    stacked = True, \n    figsize = (14, 10), \n    use_index = False, \n    legend = True,\n    cmap = cat_cmap\n)\n\n# Normalize the y-axis values\nax.set_ylim(0, ceiling(crimes_gb_hpt_pt_count.max(), to = 1000000))\n\n# Set graph title\nax.set_title(label = \"Aggregated hourly count of crimes\", fontdict = plot_main_title_font, pad = 20)\n\n# Set axis proper labels\nax.set_xlabel(xlabel = \"Hour\", fontdict = plot_axis_title_font, labelpad = 20)\nax.set_ylabel(ylabel = \"Aggregated incident frequency\", fontdict = plot_axis_title_font, labelpad = 20)\n\n# Rotate x-axis labels so as to appear vertical\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)\n\nax.legend(bbox_to_anchor = (1, 1.01));","050e3ffe":"fig, (ax1, ax2) = plt.subplots(nrows = 2, ncols = 1, sharex = False, sharey = False, squeeze = True)\n\n###\n# Plot 1\n##\nsub_ax_1 = crimes_gb_mpt_pt_count_un.plot(\n    kind = \"bar\", \n    stacked = True, \n    figsize = (20, 20), \n    use_index = False, \n    legend = True,\n    cmap = cat_cmap,\n    ax = ax1\n)\n\n# Normalize the y-axis values\nsub_ax_1.set_ylim(0, ceiling(crimes_gb_mpt_pt_count.max(), to = 1000000))\n\n# Set graph title\nsub_ax_1.set_title(label = \"Aggregated monthly count of crimes\", fontdict = plot_main_title_font, pad = 20)\n\n# Set axis proper labels\nsub_ax_1.set_xlabel(xlabel = \"Month\", fontdict = plot_axis_title_font, labelpad = 20)\nsub_ax_1.set_ylabel(ylabel = \"Aggregated incident frequency\", fontdict = plot_axis_title_font, labelpad = 20)\n\n# Set x-axis ticks and corresponding labels to be the index of our series object\nsub_ax_1.set_xticks(ticks = crimes_gb_mpt_pt_count.index.levels[0] - 1);\nsub_ax_1.set_xticklabels(labels = [months[x] for x in crimes_gb_mpt_pt_count.index.levels[0]])\n\n# Auto-format year labels\nsub_ax_1.get_figure().autofmt_xdate();\n\nsub_ax_1.legend(bbox_to_anchor = (1, 1.01));\n\n\n###\n# Plot 2\n##\nsub_ax_2 = crimes_gb_hpt_pt_count_un.plot(\n    kind = \"bar\", \n    stacked = True, \n    figsize = (20, 20), \n    use_index = False, \n    legend = True,\n    cmap = cat_cmap,\n    ax = ax2\n);\n\n# Normalize the y-axis values\nsub_ax_2.set_ylim(0, ceiling(crimes_gb_hpt_pt_count.max(), to = 1000000))\n\n# Set graph title\nsub_ax_2.set_title(label = \"Aggregated hourly count of crimes\", fontdict = plot_main_title_font, pad = 20)\n\n# Set axis proper labels\nsub_ax_2.set_xlabel(xlabel = \"Hour\", fontdict = plot_axis_title_font, labelpad = 20)\nsub_ax_2.set_ylabel(ylabel = \"Aggregated incident frequency\", fontdict = plot_axis_title_font, labelpad = 20)\n\n# Rotate x-axis labels so as to appear vertical\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(0)\n\nsub_ax_2.legend(bbox_to_anchor = (1, 1.01))\n\nfig.subplots_adjust(hspace = 0.5)\nfig.savefig(fname = \"crime_trends.pdf\", dpi = 150, papertype = \"a4\", format = \"pdf\", facecolor = \"w\", edgecolor = \"b\", orientation = \"landscape\", pad_inches = 3);","a6142042":"###\n# Group our data\n##\n# Group our data by month\ncrimes_gb_mpt = crimes_df.groupby(by = [\"primary_type\", crimes_df.index.month], axis = 0)\n\n# Count the number of incidents of our grouped set\ncrimes_gb_mpt_count = crimes_gb_mpt[\"primary_type\"].count()\n\ncrimes_gb_mpt_count_un = crimes_gb_mpt_count.unstack()\n\n###\n# Plot our data\n##\nplot_main_title_font = {\n    \"family\": \"sans serif\",\n    \"color\":  \"black\",\n    \"weight\": \"bold\",\n    \"size\": 18,\n}\n\nplot_axis_title_font = {\n    \"family\": \"sans serif\",\n    \"color\":  \"black\",\n    \"weight\": \"bold\",\n    \"size\": 14,\n}\n\nfig, ax = plt.subplots(figsize = (12, 16))\nsns.heatmap(data = crimes_gb_mpt_count_un, annot = True, fmt = \".0f\", linewidths = .0, center = crimes_gb_mpt_count.max() \/ 2, cmap = \"YlGnBu\", ax = ax)\n\n# Set graph title\nax.set_title(label = \"Heatmap of crimes per month \/ incident class\", fontdict = plot_main_title_font, pad = 20)\n\n# Set axis proper labels\nax.set_xlabel(xlabel = \"Month\", fontdict = plot_axis_title_font, labelpad = 20)\nax.set_ylabel(\"Incident class\", fontdict = plot_axis_title_font, labelpad = 20);","5e2ebc38":"import matplotlib\nimport matplotlib.cm as cm\n\n# Required for palette management\nfrom matplotlib.colors import ListedColormap\n\nimport folium\n\n# Required for creating the heatmap of crimes\nfrom folium import plugins\n\ndef create_heatmap(df, group_by = \"district\", point_radius = 10, heatmap_radius = 30):\n    # Work on our dataset\n    # Work on a copy of the original dataframe\n    df_c = df.copy(deep = True)\n    \n    # Exclude 0s\n    df_c = df_c[(df_c[\"latitude\"] != 0) & (df_c[\"longitude\"] != 0)]\n    \n    # Group by the supplied field, perform aggregations\n    # and get back our summarized dataframe\n    df_c_gb_df = df_c.groupby(by = [group_by], axis = 0).agg({\"latitude\": [\"mean\"], \"longitude\": [\"mean\"], \"primary_type\": [\"count\"]})\n    \n    # Create an array required for the heatmap plugin\n    df_c_gb_df_lat_long_arr = df_c_gb_df[[\"latitude\", \"longitude\"]].values\n    \n    # Find the map location start positions\n    lat_start = df_c_gb_df[\"latitude\"][\"mean\"].median()\n    long_start = df_c_gb_df[\"longitude\"][\"mean\"].median()\n    \n    # Create the map var\n    folium_map = folium.Map(\n        tiles = \"Stamen Toner\",\n        location = [lat_start, long_start],\n        zoom_start = 10\n    )\n    \n    # Create a descriptive string from the group_by argument\n    # Will be used for map tooltip display purposes\n    geo_aggr_level_name = group_by.replace(\"_\", \" \").strip().capitalize()\n    \n    # Create points on the map\n    for i in range(0, len(df_c_gb_df.index)):\n        \n        lat = df_c_gb_df.loc[df_c_gb_df.index[i], [\"latitude\", \"mean\"]][0]\n        long = df_c_gb_df.loc[df_c_gb_df.index[i], [\"longitude\", \"mean\"]][0]\n        geo_area_id = df_c_gb_df.index[i]\n        crime_count = df_c_gb_df.loc[df_c_gb_df.index[i], [\"primary_type\", \"count\"]][0]\n        \n        circle_marker = folium.CircleMarker(\n            location = [lat, long],\n            radius = point_radius,\n            tooltip = \\\n                geo_aggr_level_name + \" \" + \"<strong>\" + str(geo_area_id) + \"<\/strong>\" + \\\n                \"<hr\/>\" + \\\n                \"Crime count \" + \"<strong>\" + str(int(crime_count)) + \"<\/strong>\",\n            color = \"#FFFFFF\",\n            fill = True,\n            fill_color = \"#000000\"\n        )\n        \n        circle_marker.add_to(folium_map);\n    \n    # Add the heatmap\n    folium_map.add_child(plugins.HeatMap(df_c_gb_df_lat_long_arr, radius = heatmap_radius))\n    \n    return folium_map","d46480aa":"# Animated\nfrom datetime import datetime, timedelta\n\nimport matplotlib\nimport matplotlib.cm as cm\n\n# Required for palette management\nfrom matplotlib.colors import ListedColormap\n\nimport folium\n\n# Required for creating the heatmap of crimes\nfrom folium import plugins\n\ndef create_animated_heatmap(df, group_by = \"district\", heatmap_radius = 30):\n    # Work on our dataset\n    # Work on a copy of the original dataframe\n    df_c = df.copy(deep = True)\n    \n    # Exclude 0s\n    df_c = df_c[(df_c[\"latitude\"] != 0) & (df_c[\"longitude\"] != 0)]\n    \n    # Group by the supplied field, perform aggregations\n    # and get back our summarized dataframe\n    df_c_gb_df = df_c.groupby(by = [df_c.index.year, group_by], axis = 0).agg({\"latitude\": [\"mean\"], \"longitude\": [\"mean\"], \"primary_type\": [\"count\"]})\n    \n    # Flatten the grouped dataset\n    df_map = df_c_gb_df.reset_index()\n    \n    # Pick the required columns for creating the animated heatmap\n    df_map = df_map[[\"latitude\", \"longitude\", \"primary_type\", \"date\"]]\n    \n    # Create an array of data points required by the animated heatmap plugin\n    df_map_data_gb_d = df_map.groupby(by = \"date\")\n    df_map_data_arr = [item.tolist() for item in [df_map_data_gb_d.get_group(g)[[\"latitude\", \"longitude\", \"primary_type\"]].values for g in df_map_data_gb_d.groups]]\n    \n    # Create an array of time intervals required by the animated heatmap plugin\n    df_map_time_arr = np.unique(df_map[[\"date\"]].values)\n    \n    # Flatten and format date from %Y to %Y-%m-%d\n    df_map_time_arr = [str(year) + \"-01-01\" for year in df_map_time_arr]\n    \n    # Find the map location start positions\n    lat_start = df_c_gb_df[\"latitude\"][\"mean\"].median()\n    long_start = df_c_gb_df[\"longitude\"][\"mean\"].median()\n    \n    # Create the map var\n    folium_map = folium.Map(\n        tiles = \"Stamen Toner\",\n        location = [lat_start, long_start],\n        zoom_start = 10\n    )\n    \n    heat_map_with_time = plugins.HeatMapWithTime( \n        data = df_map_data_arr,\n        index = df_map_time_arr,\n        radius = heatmap_radius,\n        display_index = True,\n        auto_play = True\n    )\n\n    heat_map_with_time.add_to(folium_map)\n    \n    return folium_map","010be618":"create_heatmap(crimes_df, group_by = \"beat\", point_radius = 2, heatmap_radius = 30)","c5efd3f1":"create_heatmap(crimes_df, group_by = \"ward\", point_radius = 2, heatmap_radius = 30)","e299edbb":"create_heatmap(crimes_df, group_by = \"district\", point_radius = 2, heatmap_radius = 30)","170138a0":"create_heatmap(crimes_df, group_by = \"community_area\", point_radius = 2, heatmap_radius = 30)","e478e403":"create_animated_heatmap(crimes_df, group_by = \"district\", heatmap_radius = 30)","ccc98dfe":"def pareto(df, group_by = \"district\"):\n    # Work on our dataset\n    # Work on a copy of the original dataframe\n    df_c = crimes_df.copy(deep = True)\n    \n    # Group by the provided geographical level,\n    # counting crimes instances\n    df_c_gb_pt_df = df_c.groupby(by = [group_by], axis = 0).agg({\"primary_type\": [\"count\"]})\n    \n    # Sort by crimes count, descending\n    df_c_gb_pt_df = df_c_gb_pt_df.sort_values(by = (\"primary_type\", \"count\"), ascending = False)\n    \n    # Calculate the percentage of crimes each geographic region\n    # contributes to the whole\n    df_c_gb_pt_df[\"percentage\"] = df_c_gb_pt_df[(\"primary_type\", \"count\")] \/ df_c_gb_pt_df[(\"primary_type\", \"count\")].sum()\n    \n    # Calculate the running sum of percentages\n    df_c_gb_pt_df[\"percentage_cumsum\"] = df_c_gb_pt_df[\"percentage\"].cumsum()\n    \n    # Return the percentage of observations up to and including contribution\n    # to 80% of cases, divided by the total number of observations.\n    return df_c_gb_pt_df[df_c_gb_pt_df[\"percentage_cumsum\"] <= .8].iloc[:, 0].count() \/ df_c_gb_pt_df.iloc[:, 0].count()","0b937c62":"print(\"Pareto ratio: \", pareto(crimes_df, group_by = \"district\"))","fd2716f8":"# https:\/\/dev.socrata.com\/foundry\/data.cityofchicago.org\/6zsd-86xi\n# Create a client to the City of Chicago data source\n\n# Some variables required for accessing the City of Chicago API\ndomain = \"data.cityofchicago.org\"\n# Ommited for privacy\napp_token = \"<your_app_token>\"\ndataset_identifier = \"6zsd-86xi\"\n\nstarting_date = \"2001-01-01T00:00:00\"\n\n# Dataset throttle control\ndata_limit = 1000\ndata_offset = 0\n\ntemp_data = []\n\n# The City of Chicago API feeds request utilising a paging mechanism.\n# Therefore, we need to provide a way to page our requests to the API by utilising\n# 'limit' and 'offset' variables in our query.\n# Since there is no clear way for the API to indicate the end-of-data-stream location,\n# we need a way to break from the while loop. This will happen when the length of the\n# data returned from the API call is 0.\n\n# Initialize the length variable to -1.\ndata_len = -1\n\nwith Socrata(domain=domain, app_token=app_token) as client:\n    # Make sure we have the correct data set\n    metadata = client.get_metadata(dataset_identifier = dataset_identifier)\n    print(metadata[\"name\"])\n    print(metadata[\"description\"])\n    \n    # Extract the dataset column names from the dataset metadata\n    data_columns = [x[\"name\"] for x in metadata[\"columns\"]]\n    \n    while(data_len != 0):\n        try:\n            # Retrieve paged data, starting from 2010-01-01T00:00:00\n            data = client.get(dataset_identifier = dataset_identifier, where = \"date >= '\" + starting_date + \"'\", order = \"date ASC\", content_type = \"json\", limit = data_limit, offset = data_offset)\n            # Append the retrieved data to a temporary array\n            temp_data += data\n            # Set the offset to the next 'page' of data\n            data_offset += 1000\n            # Evaluate the data array length so that we know when to break from the while loop\n            data_len = len(data)\n            print(\"Data offset: \", data_offset)\n        except Exception as e:\n            print_stderr(e)\n            break\n    \n    # Convert the entire array of data to a Pandas dataframe\n    crimes_df = pd.DataFrame.from_records(data = temp_data)","5993ebea":"Let us proceed with examining which columns contain null values:","3e6ef5d4":"#### Question","295110e8":"#### Grouping and visualizing crime at various geographical levels","414638d8":"We are effectively being asked: \"*Does **80%** of crime come from **20%** of the neighbors?*\".","186cf903":"Render some of the above plots in the same figure, by using the [matplotlib.subplots](https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.pyplot.subplots.html) function. Save the figures in a crime_trends.pdf","5905c6bc":"#### Grouping by 'beat'\n\nIndicates the beat where the incident occurred.\n\nA beat is the smallest police geographic area \u2013 each beat has a dedicated police beat car. Three to five beats make up a police sector, and three sectors make up a police district. The Chicago Police Department has 22 police districts.\n\nSee the beats at https:\/\/data.cityofchicago.org\/d\/aerh-rz74.\n\nSource: [Chicago Crime Crimes Data - 2001 to present](https:\/\/data.cityofchicago.org\/Public-Safety\/Crimes-2001-to-present\/ijzp-q8t2\/data)","d7c0aab0":"### 1 Parsing the CSV data file","f931db2e":"Clearly, crime rates have fallen dramatically within the span of years we are examining. Let's present the changes numerically:","9696f944":"Chicago crime data can been retrieved from the [Chicago Crime Crimes Data - 2001 to present](https:\/\/data.cityofchicago.org\/Public-Safety\/Crimes-2001-to-present\/ijzp-q8t2\/data) by either: \n\n- (1) a direct download of a dataset in CSV format or\n- (2) by making use of the [Socrata Open Data API](https:\/\/dev.socrata.com\/) API and the corresponding [SODA Python library](https:\/\/github.com\/xmunoz\/sodapy).\n\nFor the purpose of this EDA, the author chose to use a downloaded CSV file. Method (2) is mentioned at the end of this report (Appendix: Acquiring data by performing SODA API calls), for posterity :-)","b098d397":"...and check:","144a9558":"We can safely **discard** the following columns, not required for our analysis:\n\n- ID\n- Case Number\n- Block\n- IUCR\n- Description\n- Location Description\n- Arrest\n- Domestic\n- FBI Code\n- Year (we will be using a DatetimeIndex, year value is not required)\n- Updated On\n- Location (we do have latitude and longitude values already)","2fbd1b3d":"#### Grouping by 'district'\n\nIndicates the police district where the incident occurred.\n\nSee the districts at https:\/\/data.cityofchicago.org\/d\/fthy-xz3r.\n\nSource: [Chicago Crime Crimes Data - 2001 to present](https:\/\/data.cityofchicago.org\/Public-Safety\/Crimes-2001-to-present\/ijzp-q8t2\/data)","2ea708ee":"#### Answer","c5f98aeb":"Examination of historical data shows that, by and large, **concentration of crime remains largely unchanged** at the geographical level with only minor flunctuation occuring at the neighborhood level.","6b061dbd":"It is interesting to visualize historical changes in **crime density through time**. Let us plot an animation of our data points at the district level:","92a8454a":"Let us look at the trends in crime, per crime class, at an hourly level:","91491855":"### 1 Trend Analysis","f8526590":"...and check:","078a4975":"#### Question","b77235f0":"### Acquiring data by performing SODA API calls","c7fb7c5a":"First things first, let us take a look at the overall trend of crime within the years. We need to get a \"feel\" for the overall trend in crime in the City of Chicago, so as to reinforce our story-telling:","98be500f":"Let us replace null values. We choose to fill nulls instead of droping corrsponding rows making sure, however, that we exclude such values from our analysis. For instance, since we will be using *latitude* and *longitude* for part 3 of the assignment, we should make sure to exclude such coordinates.\n\nThe rationale behind not dropping rows that contain nulls is that we may loose valuable information (e.g. crime event counts) if we chose to drop an observation based on incomplete information (e.g. missing location location).","35a719ad":"Observe how the heatmap reinforces our previous observations i.e. that predominant crime classes are **theft**, **battery**, **criminal damage** and **narcotics** (all highlighted with deeper colors). Also, how seasonality is depicted in the gradient of the colors at the row level, reinforcing our analysis that months **July** and **August** are those when peaks are observed.","744cae58":"### 3 Plotting Geo Data","6e691275":"### 2 Plotting Heatmaps","46540dd4":"This notebook was created as part of an assignment for the M.Sc. in Data Science at the Athens University of Economics and Business.","5a41452e":"It appears that the **most meaningful level of aggregation** is at the **dirstrict** level. We can clearly visually identify **3-5 districts** where crime is higher, compared to the rest.","0e50ae02":"...and its columns:","790d7be1":"We can clearly see that the following columns have been loaded with defaults, in terms of their data type:\n\n- *date*: data type is **str**, should be **datetime64**\n- *beat*: data type is **float64**, should be **int64**\n- *district*: data type is **float64**, should be **int64**\n- *ward*: data type is **float64**, should be **int64**\n- *community_area*: data type is **float64**, should be **int64**\n\nWe should convert the columns to appropriate datatypes for (i) alignment with original dataset types and (ii) performance gains (clearly, using a **numpy.float64** when a **numpy.int64** is required will lower our memory footprint). \n\nLet's proceed with our transformation:","cdea00cf":"This method could be employed in case we would require a more flexible approach to acquiring data from the City of Chicago API.\n\nIn order to use the API, it is preferrable - although not mandatory - to create an API token through the [City of Chicago Data portal](https:\/\/data.cityofchicago.org). Registering and acquiring an API token allows for querying the API outside the global anonymous shared resource pool, thus increasing query limits.\n\nA word of warning: this code is definitely incomplete and is a just a first attempt to access the SODA API.","63b9ed4b":"#### Answer","c278d99e":"Finally, we need to **exclude year 2018** for which we have inconclusive data that might introduce bias in our analysis. Therefore, we will chose the range of date between 2001-01-01 and 2017-12-31:","02f05eee":"We also need to 'pythonize' our column names. Previous visual inspection showed that we need to replace spaces and fix capitalization.","c03a5187":"Our data pre-processing work is now complete, on with the analysis!","21d5f381":"Another convenient way to spot trends in time-series data is using [Heatmaps](https:\/\/en.wikipedia.org\/wiki\/Heat_map).\n\n\"Convert\" some of the histograms above to heatmaps to illustrate your observations to the stakeholders. To do so, the x-axis of the histogram should be a time element and the y-axis a frequency count for each crime type.","c5ce5fb2":"#### Answer","f239a188":"***","19773e1a":"#### Answer","f2d94567":"## Required libraries, configuration and global functions","d43ce82f":"Let's take a look at the dataset:","8198a78a":"Let us dive deeper and examine the intra-year (at the month level) and intra-day (at the hour level) trends in the number and type of crimes.\n\nLet us look at the **monthly trends in crime**, per crime class:","f4ab5084":"The diversity of the dataset, in terms of aggregated information that can be achieved by grouping at various geographical levels (e.g. community, district etc.) presents us with the problem of striking the correct balance between noise and meaningful, actionable information.\n\nWhy not, then, visualize our data at various levels and see what story they tell?\n\nLet us create a generic function that will require the geographical aggregation level and will present us with group points and a heatmap of crime at the specific aggregation level.\n\nNote that since the data, for anonymity purposes, do not provide us with pin-point accuracy in terms of geolocation, we are given the flexibility to assume a flat-earth (?!) situation, where grouping geopoints will be done by simply taking the mean of each component (latitude, longitude).","206a19e5":"## Inspecting, cleaning and transforming the dataset","0757a516":"## Questions","57e3ea77":"Check again for nulls in the dataset:","60cae3fa":"Let us also take a look at the data type of each column, in order to identify columns that need transformation (e.g. dates that have not been automatically converted to datetime objects, reduntant types in terms of accuracy etc.):","cf99f5e7":"Suppose the Police would like to increase patrols in certain neighborhoods - areas. Officials are asking you \"What neighborhoods should we prioritize ?\"\n\nMake use of the Geo-related variables in the dataset to find the most \"dangerous\" neighborhoods and plot the results accordingly.\n\nYou can use [Folium](https:\/\/python-visualization.github.io\/folium\/docs-v0.6.0\/quickstart.html) (recommended) or [GeoPandas](http:\/\/geopandas.org\/) for this.","e3c5fa36":"**Bonus**","c34e5d6c":"**Bonus**","156799e5":"## Acquiring data","373c78da":"## Assignment 1: Analyzing Chicago Crime Data","4979a88d":"## Appendix","b3b8cf3c":"#### Grouping by 'communit_area'\n\nIndicates the community area where the incident occurred. Chicago has 77 community areas. \n\nSee the community areas at https:\/\/data.cityofchicago.org\/d\/cauq-8yn6.\n\nSource: [Chicago Crime Crimes Data - 2001 to present](https:\/\/data.cityofchicago.org\/Public-Safety\/Crimes-2001-to-present\/ijzp-q8t2\/data)","5a77902f":"Are there any intra-year (at the month level) and intra-day (at the hour level) trends in the number and type of crimes ? To answer these, you should plot histogram(s) with the appropriate frequencies for each type of crime (you may choose between Primary Type and Description ) and then comment on the results. The solution involves choosing the appropriate number and structure of histograms (e.g. using subplots, stacked bars or not).","2ad35a88":"**Note**: you can tweak the parameter 'num_primary_types' to limit the classes of crimes displayed in the graph above. Setting the parameter to None will provide you with the graph of the entire dataset.\n\nVisual inspection again shows a periodic trend in accumulated crimes. It is evident that the most \"quiet\" hours are between **01:00 am** and **08:00 am**. This trend follows the same pattern for all crime classes.","0c314b24":"We conclude that **~60%** of **districts** are responsible for **~80%** of crime.\n\nTherefore, the Pareto principle **does not apply**.","8db0de7a":"During your presentation, an officer asks you: \"Does the [Pareto Principle](https:\/\/en.wikipedia.org\/wiki\/Pareto_principle) apply to the neighborhoods ?\". How would you respond to that ?","29592bc0":"## Environment setup","f566e198":"#### Question","5ea953bb":"As a next step, since we will be mostly working with time series, we need to set a DatetimeIndex on our dataframe:","e89db257":"#### Grouping by 'ward'\n\nThe ward (City Council district) where the incident occurred.\n\nSee the wards at https:\/\/data.cityofchicago.org\/d\/sp34-6z76.\n\nSource: [Chicago Crime Crimes Data - 2001 to present](https:\/\/data.cityofchicago.org\/Public-Safety\/Crimes-2001-to-present\/ijzp-q8t2\/data)","f2609334":"**Note**: you can tweak the parameter 'num_primary_types' to limit the classes of crimes displayed in the graph above. Setting the parameter to None will provide you with the graph of the entire dataset.\n\nVisual inspection shows seasonality in accumulated crimes which also applies to crime classes. One can clearly see that the month less affected by crime is **February**, whereas crime incidents peak during **July** and **August**.\n\nAnother remark is that the most common classes of crime are **theft** and **battery**, with **criminal damage** and **narcotics** cases following close. These four classes account for a large percentage of all crimes.","63261afd":"## Description","9759b8d0":"#### Animated concentration of crime at the district level"}}