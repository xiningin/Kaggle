{"cell_type":{"a1390cb8":"code","6fb205c6":"code","5beeb98d":"code","2798e842":"code","2f0e0a36":"code","2c4e29aa":"code","4e46e1f1":"code","c29a96c8":"code","c48fd5c4":"code","82bd80e8":"code","a7ff891e":"code","9093d4b2":"code","41f74fc4":"code","a4aa7210":"code","3f1be4bf":"code","ab8e445a":"code","290fc769":"code","a35af9e2":"code","c4a1daf8":"code","7bf1902b":"code","175087ba":"code","ebf21b00":"code","5853b679":"code","6c59be06":"code","c32eed46":"code","dc914d00":"code","b2da42b8":"code","72ca865d":"code","162c3fe8":"code","623c45be":"code","a014df7d":"code","d2f55a43":"code","8f79beae":"code","72446179":"markdown","9a873379":"markdown","398ae08d":"markdown","059ad18f":"markdown","368dbdaf":"markdown","4e7015c4":"markdown","4d477194":"markdown","24008183":"markdown","005dfbd2":"markdown","2ba36ac7":"markdown","75517953":"markdown","e291e19d":"markdown","64f3c6ae":"markdown","a024922b":"markdown","2d42acd6":"markdown","483af15f":"markdown","8b76fe74":"markdown","5575bc54":"markdown","3a7aa898":"markdown","8a9e0b73":"markdown","9e3e9287":"markdown","cdb61f6f":"markdown","1e17e119":"markdown","f90db187":"markdown","5b2f1b16":"markdown","23f03c32":"markdown","d832175f":"markdown","893a3fb7":"markdown","c4f61627":"markdown","06ba8252":"markdown"},"source":{"a1390cb8":"# Importing all the required libraries\nimport os\nimport re\nimport numpy as np\nimport pandas as pd\nfrom nltk.stem import PorterStemmer\nfrom matplotlib import pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\n\nstopwords = set(STOPWORDS)","6fb205c6":"stopwords.add('will')\nstopwords.add('ve')\nstopwords.add('now')\nstopwords.add('gonna')\nstopwords.add('wanna')\nstopwords.add('lol')\nstopwords.add('via')\n            ","5beeb98d":"os.chdir('\/kaggle\/input\/nlp-getting-started\/')","2798e842":"train = pd.read_csv('train.csv')\ntest = pd.read_csv('test.csv')","2f0e0a36":"# Data Overview\ntrain.head()","2c4e29aa":"test.head()","4e46e1f1":"print('Null values in Train data set%')\ntrain.isnull().sum()\/len(train)*100","c29a96c8":"print('Null values in Test dataset %')\ntest.isnull().sum()\/len(test)*100","c48fd5c4":"# Combining all the text data \ntweets_data = pd.concat([train, test], axis=0, sort=False, ignore_index=True)\ntweets_data.head()","82bd80e8":"# Data cleaning steps\ndef clean_data(df, text_col, new_col='cleaned_text', stemming=False, lemmatization=True):\n    \n    '''\n    It will remove the noise from the text data(@user, characters not able to encode\/decode properly)    \n    ----Arguments----\n    df : Data Frame\n    col : column name (string)\n    steming : boolean\n    lemmatization : boolean\n    '''\n    tweets_data = df.copy() # deep copying the data in order to avoid any change in the main data col  \n    \n    # Creating one more new column for new text transformation steps\n    tweets_data[new_col] = tweets_data[text_col]\n    \n    # removing @<userid>, as it is very common in the twitter data\n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : re.sub(\n        '@[A-Za-z0-9_]+', '', x)) \n    \n    # Removing &amp \n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub('&amp',' ', str(x)))\n    \n    # Removing URLs from the data\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub('https?:\\\/\\\/[a-zA-z0-9\\.\\\/]+','',\n                                                                     str(x)))\n    tweets_data[new_col] = tweets_data[new_col].str.lower()\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\\u2019\", \"\\'\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\s\\'\", \" \", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"won\\'t\", \"will not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"can\\'t\", \"can not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"don\\'t\", \"do not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"dont\", \"do not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"n\\\u2019t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"n\\'t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'re\", \" are\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(\"\\'s\", \" is\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\\u2019d\", \" would\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'ll\", \" will\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'t\", \" not\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'ve\", \" have\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'m\", \" am\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\n\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\r\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\'\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r\"\\\"\", \"\", str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'[?|!|\\'|\"|#]',r'', str(x)))\n    tweets_data[new_col] = tweets_data[new_col].map(lambda x: re.sub(r'[.|,|)|(|\\|\/]',r' ', str(x)))\n    \n    # Trimming the sentences\n    tweets_data[new_col] = tweets_data[new_col].str.strip() \n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : re.findall(\n       \"[A-Za-z0-9]+\", x))\n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : \" \".join(x))\n    \n    # Remove stopwords\n    tweets_data[new_col] = tweets_data[new_col].apply(\n        lambda x : ['' if word in stopwords else word for word in x.split()])\n    \n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : \" \".join(x))\n        \n    # Removing extra spaces\n    tweets_data[new_col] = tweets_data[new_col].apply(lambda x : re.sub(\"\\s+\", \" \", x))\n    \n    # lemmatization\n    if lemmatization:\n        \n        lemma = WordNetLemmatizer()\n        \n        tweets_data[new_col] = tweets_data[new_col].apply(lambda sentence : \n                                         [lemma.lemmatize(word,'v') for word in sentence.split(\" \")])\n        \n        tweets_data[new_col] = tweets_data[new_col].apply(lambda x : \" \".join(x))\n     \n    # Stemming code\n    if stemming:\n        stemming = PorterStemmer()\n        \n        tweets_data[new_col] = tweets_data[new_col].apply(lambda sentence : \n                                         [stemming.stem(x) for x in sentence.split(\" \")])\n        \n        tweets_data[new_col] = tweets_data[new_col].apply(lambda x : \" \".join(x))\n\n    return tweets_data\n","a7ff891e":"tweets_data = clean_data(tweets_data, \"text\", \n                         'cleaned_text', \n                         lemmatization=True,\n                         stemming=False)\n\npd.set_option('display.max_colwidth', -1)\n\nprint('----- Text Before And After Cleaning -----')\n\ntweets_data[['text', 'cleaned_text']].head(10)","9093d4b2":"stemming = PorterStemmer()\nprint(f\"Runs converted to {stemming.stem('runs')}\")\nprint(f\"Stemming converted to {stemming.stem('stemming')}\")","41f74fc4":"wordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10)\nwordcloud.generate(\" \".join(tweets_data['cleaned_text']))\n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None, dpi=80) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  ","a4aa7210":"print('% of classes')\nprint(tweets_data['target'].value_counts() \/ len(train) * 100)","3f1be4bf":"vec = TfidfVectorizer(ngram_range=(1,5),\n                      #max_features=10000,\n                      min_df=3,\n                      stop_words='english')\n\ntfidf_matrix = vec.fit_transform(tweets_data['cleaned_text'])\n\ntfidf_matrix.shape","ab8e445a":"tfidf_matrix = pd.DataFrame(tfidf_matrix.toarray(),\n                            columns = vec.get_feature_names(),\n                            dtype='float32')\n\nprint(\"Shape of the dataframe \",tfidf_matrix.shape)\nprint('Data Frame Info')\ntfidf_matrix.info()","290fc769":"# Prepare the data set for model training\n\nX = tfidf_matrix.iloc[range(0, train.shape[0]), :]\n\ntest_dataset = tfidf_matrix.iloc[train.shape[0]:, :] \n                           \ny = tweets_data.loc[0:train.shape[0]-1, 'target']\n\nx_train, x_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    random_state=123, \n                                                    test_size = 0.3)","a35af9e2":"clf = LogisticRegression(max_iter=1500,\n                        solver='lbfgs')\n\nclf.fit(x_train, y_train)","c4a1daf8":"print(\"F1 Score is \", f1_score(y_test, clf.predict(x_test)))\nconfusion_matrix(y_test, clf.predict(x_test))","7bf1902b":"clf.fit(X, y)\n\nact_pred = clf.predict(test_dataset)\nact_pred = act_pred.astype('int')\n\nsubmission_file = pd.DataFrame({'id' : test['id'],\n                               'target' : act_pred})\n\nsubmission_file.to_csv('\/kaggle\/working\/sub_140120_v0.4lr.csv', index = False)","175087ba":"nv = GaussianNB()\nnv.fit(x_train, y_train)","ebf21b00":"print(\"F1 Score is \", f1_score(y_test, nv.predict(x_test)))\nprint('Confusion Matrix')\nconfusion_matrix(y_test, nv.predict(x_test))","5853b679":"rf = RandomForestClassifier(n_estimators=1500,\n                            max_depth=6,\n                            oob_score=True)\nrf.fit(x_train, y_train)","6c59be06":"print(\"F1 Score is \", f1_score(y_test, rf.predict(x_test)))\nprint('--------Confusion Matrix---------')\nconfusion_matrix(y_test, rf.predict(x_test))","c32eed46":"rf.fit(X, y)\n\nact_pred = rf.predict(test_dataset)\nact_pred = act_pred.astype('int')\n\nsubmission_file = pd.DataFrame({'id' : test['id'],\n                               'target' : act_pred})\n\nsubmission_file.to_csv('\/kaggle\/working\/sub_140120_v0.1rf.csv', index = False)","dc914d00":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(max_depth=6,\n                    learning_rate=0.3,\n                    n_estimators=1500,\n                    objective='binary:logistic',\n                    random_state=123,\n                    n_jobs=4)\n\nxgb.fit(x_train, y_train)","b2da42b8":"print(\"F1 Score is \", f1_score(y_test, xgb.predict(x_test)))\nprint('--------Confusion Matrix---------')\nconfusion_matrix(y_test, xgb.predict(x_test))","72ca865d":"xgb.fit(X, y)\n\nact_pred = xgb.predict(test_dataset)\nact_pred = act_pred.astype('int')\n\nsubmission_file = pd.DataFrame({'id' : test['id'],\n                               'target' : act_pred})\n\nsubmission_file.to_csv('\/kaggle\/working\/sub_130120_v0.1xgb.csv', index = False)","162c3fe8":"import gensim.models.word2vec as w2v\nimport nltk\nimport multiprocessing\nfrom nltk.tokenize import TweetTokenizer","623c45be":"num_of_features = 300\nmin_word_count = 3\nnum_of_threads = multiprocessing.cpu_count()\ncontext_size = 7\ndownsampling = 1e-3\n","a014df7d":"vecs = w2v.Word2Vec(sg=1,\n                   seed=123,\n                   workers=num_of_threads,\n                   size=num_of_features,\n                   min_count=min_word_count,\n                   window=context_size,\n                   sample=downsampling)","d2f55a43":"tokens_vec = []\ntokens = TweetTokenizer()\ntokens_vec = tweets_data['cleaned_text'].apply(lambda x : tokens.tokenize(x))","8f79beae":"vecs.build_vocab(tokens_vec.values.tolist())","72446179":"Wildfire seems to be one of the major issue.","9a873379":"Importing data to memory using pandas","398ae08d":"### Combining all the data train and test, for cleaning purpose","059ad18f":"### Stemming of Text - Stemming tries to convert word into it's root form","368dbdaf":"### Adding few mords in stopwords - got from wordcloud below","4e7015c4":"How many null values are there?","4d477194":"### Using Word2vec now\n","24008183":"### 2. Naive Bayes Classifier","005dfbd2":"### Below function contains the data cleaning steps using regex","2ba36ac7":"### 4. XGBOOST","75517953":"# Learn And Train Different Types Models\nMy main objective is not to treat any model as a black box, let also know what is going on behind when we fit any model. \nWhen we we will discuss anything about models just keep in mind the below steps needed for a model\n> 1. Objective\n> 2. Model structure (e.g. variables, formula, equation)\n> 3. Model assumptions\n> 4. Parameter estimates and interpretation\n> 5. Model fit (e.g. goodness-of-fit tests and statistics)\n> 6. Model selection","e291e19d":"Testing on the test data set","64f3c6ae":"As we will only deal with teh text column, so we will not be focusing on the missing values in other columns","a024922b":"## Learn Machine Learning models with examples\n#### The motive behind working on this Kernal is to revise the basic steps or formal mathmatical calculation running behind the models and not treating any model as Black Box\nPlease comment the suggestion or upvote if you like.","2d42acd6":"Checking on test dataset","483af15f":"### Fitting or Training Linear Regression","8b76fe74":"Training on complete dataset for Prediction","5575bc54":"### Preparing data using TF-IDF [Term Frequency Inverse Document Matrix]","3a7aa898":"### Importing all the required Libs","8a9e0b73":"Changing the current directory using python os module to the data directory","9e3e9287":"Checking score on the test data set","cdb61f6f":"### Splitting the data for testing ","1e17e119":"### %cent of classes","f90db187":"### Plotting Word Cloud","5b2f1b16":"### Summary\n\n1. __Objective__ : Is to build a mode the expected value of Y as the function of X\n2. __Model Structure__: p = e^(b0+b1x1+b2x2+...bkXk)\/(1+e^(b0+b1x1+b2x2+...bkXk))\n- __Model Assumption__: \n   1. Independent variables should be linearly dependent on log odds.\n   2. No Multicolinearity - Independent features(X) should not be correlated with each other.\n   3. Needed a larger sample size.\n   4. Dependent variable must be binary or ordinal.\n- __Parameter Estimation__:\n   1. b0(beta) or Intercept : It's basically a constant which means the predicted or avg value of y when the independent variables are 0.\n   2. b1, b2,b3. : It means a slope also defines the association between Y and X. In other terms - By changing 1 unit value of X1 the y will change by b1 times.\n - __Model fit(goodness-of-fit tests)__:\n    1. Accuracy(If dataset is balanced)\n    2. F1 score, precision, recall, auc-roc score, etc\n- __Model Selection__ : Removing unwanted features using Lasso or l1 penality or using Ridge(l2) penality ","23f03c32":"### 3. Random Forest","d832175f":"## 1. Logistic Regression \n\n- It is a statistical technique that is used to map the the values of a depedent variable(Y) to it's indepedendent or predictor variables(X).\n- It is used for classification purpose either binary like yes\/no or ordinal like good, better, best.\n- Also known as logit or log odds - odds means the probability of success divided by probability of failure the function used for parameter estimation other function used is the sigmoid function.\n\n  Logistic function - Calculations\n > log(p\/1-p) = b + b1x1 + b2x2 + ... + bkxk | where __p__ is the probaility and __x__ is the feature \n\n > log(p\/1-p) is known as log odds\n\n > p\/1-p = e^(b + b1x1 + b2x2 +...+ bkXk)\n\n > p = 1\/(1 + e^(b + b1x1 + b2x2 +...+ bkXk))\n","893a3fb7":"Doing basic checks on the data","c4f61627":"*Note* : Only done for Logistics Regression- Few more things to be added.\n Work In Progress. \n Will do for other algos also.","06ba8252":"### Testing on Test Data Set"}}