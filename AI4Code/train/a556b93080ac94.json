{"cell_type":{"a292af25":"code","c6343e7d":"code","eb0e9ca1":"code","3438a968":"code","8b405723":"code","fc44cf4a":"code","e3702217":"code","7c0b655a":"code","acfeaeb0":"code","acba14ab":"code","ee50a23e":"code","6be2371e":"code","08f98415":"code","3ff131ad":"code","06cd2f28":"code","524ed97b":"code","a87e9bbd":"code","a56bacf7":"markdown","ef58e93e":"markdown"},"source":{"a292af25":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nfrom matplotlib.lines import Line2D\n\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import IsolationForest\n\nimport optuna\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# import warnings\n# warnings.simplefilter(action='ignore', category=UserWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6343e7d":"# Read the data\ntrain = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col='id')\ntest = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col='id')\nsample = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv', index_col='id')","eb0e9ca1":"# managing Categoriacal\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler","3438a968":"y = train[\"target\"]\nX_original = train.drop(\"target\", axis = 1)\nX_test_original = test.copy()","8b405723":"X_reindexed = X_original.set_index(pd.Index(range(len(X_original))))\n\nX_test_reindexed = X_test_original.set_index(pd.Index(range(len(X_test_original))))\n\ny.index = range(len(y))","fc44cf4a":"num_attribs = ['cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6',\n       'cont7', 'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13']\ncat_attribs = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8',\n       'cat9']","e3702217":"full_pipeline = ColumnTransformer([\n    (\"num\", StandardScaler(), num_attribs),\n    (\"cat\", OneHotEncoder(), cat_attribs)\n])","7c0b655a":"X_prepared = full_pipeline.fit_transform(X_reindexed)\n\nX_test_prepared = full_pipeline.transform(X_test_reindexed)","acfeaeb0":"X = pd.DataFrame(X_prepared, \n                                    index = X_reindexed.index\n                                    )\nX_test = pd.DataFrame(X_test_prepared, \n                                    index = X_test_reindexed.index\n                                    )","acba14ab":"# Calculating edges of target bins to be used for stratified split\ntarget_bin_edges = np.histogram_bin_edges(y, bins=10)\ntarget_bin_edges[0] = -np.inf\ntarget_bin_edges[-1] = np.inf\ntarget_bins = pd.cut(y, target_bin_edges, labels=np.arange(10))\ntarget_bins.value_counts()","ee50a23e":"def train_model_optuna_xgb(trial, X_train, X_valid, y_train, y_valid):\n    \"\"\"\n    A function to train a model using different hyperparamerters combinations provided by Optuna. \n    Loss of validation data predictions is returned to estimate hyperparameters effectiveness.\n    \"\"\"\n    preds = 0\n    \n    LGBM_params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.004,\n        \"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n        \n    #A set of hyperparameters to optimize by optuna\n    xgb_params = {\n                 \"n_estimators\": trial.suggest_categorical('n_estimators', [10000]),\n                 \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.8),\n                 \"subsample\": trial.suggest_float('subsample', 0.5, 0.95),\n                 \"colsample_bytree\": trial.suggest_float('colsample_bytree', 0.5, 0.95),\n                 \"max_depth\": trial.suggest_int(\"max_depth\", 5, 16),\n                 \"booster\": trial.suggest_categorical('booster', [\"gbtree\"]),\n                 \"tree_method\": trial.suggest_categorical('tree_method', [\"gpu_hist\"]),\n                 \"reg_lambda\": trial.suggest_float('reg_lambda', 2, 100),\n                 \"reg_alpha\": trial.suggest_float('reg_alpha', 1, 50),\n                 \"random_state\": trial.suggest_categorical('random_state', [42]),\n                 \"n_jobs\": trial.suggest_categorical('n_jobs', [4]),\n                    }\n\n    # Model loading and training\n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    print(f\"Number of boosting rounds: {model.best_iteration}\")\n    oof = model.predict(X_valid)\n    oof[oof<0] = 0\n    \n    return np.sqrt(mean_squared_error(y_valid, oof))","6be2371e":"X.head(10)","08f98415":"y","3ff131ad":"%%time\n# Splitting data into train and valid folds using target bins for stratification\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_idx, valid_idx in split.split(X, target_bins):\n\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    \n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n# Setting optuna verbosity to show only warning messages\n# If the line is uncommeted each iteration results will be shown\noptuna.logging.set_verbosity(optuna.logging.WARNING)\ntime_limit = 3600 * 3\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(lambda trial: train_model_optuna_xgb(trial, \n                                                X_train, \n                                                X_valid,\n                                                y_train, \n                                                y_valid),\n               n_trials = 500,\n               timeout=time_limit\n              )\n # Showing optimization results\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial parameters:', study.best_trial.params)\nprint('Best score:', study.best_value)","06cd2f28":"# Hyperparameters optimized by Optuna in previous session \n# on a 5h GPU work\n\n\nxgb_params = study.best_trial.params\n# xgb_params = {'n_estimators': 10000,\n#               'learning_rate': 0.29873139617214056,\n#               'subsample': 0.5401733475481303,\n#               'colsample_bytree': 0.8467251191663571,\n#               'max_depth': 7, 'booster': 'gbtree',\n#               'tree_method': 'gpu_hist',\n#               'reg_lambda': 45.10866423667691,\n#               'reg_alpha': 33.05306225737401,\n#               'random_state': 42,\n#               'n_jobs': 4}","524ed97b":" %%time\nsplits = 10\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X, target_bins)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    preds += model.predict(X_test) \/ splits\n    model_fi += model.feature_importances_\n    oof_preds[valid_idx] = model.predict(X_valid)\n    oof_preds[oof_preds < 0] = 0\n#     fold_rmse = np.sqrt(mean_squared_error(y_scaler.inverse_transform(np.array(y_valid).reshape(-1,1)), y_scaler.inverse_transform(np.array(oof_preds[valid_idx]).reshape(-1,1))))\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n#         print(f\"Trees: {model.tree_count_}\")\n    total_mean_rmse += fold_rmse \/ splits\nprint(f\"\\nOverall RMSE: {total_mean_rmse}\") ","a87e9bbd":"# xgb public Score: \npredictions = pd.DataFrame()\npredictions[\"id\"] = test.index\npredictions[\"target\"] = preds\n\npredictions.to_csv('submission_xgb.csv', index=False, header=predictions.columns)\npredictions.head()","a56bacf7":"# Data Preparation","ef58e93e":"# Optuna"}}