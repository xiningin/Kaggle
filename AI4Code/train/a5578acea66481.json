{"cell_type":{"d535d02a":"code","57e05a2c":"code","51254b36":"code","8ae1028e":"code","c3d91c65":"code","10f7428f":"code","baaf090b":"code","32366d4d":"code","f928ac68":"code","3c5d84b4":"code","98e39fa8":"code","2da8160c":"code","f51856c1":"code","a81661d6":"code","704dd2ec":"code","8653b05c":"code","419fa5ea":"code","6cf9a64b":"code","b758458e":"code","a45b5859":"code","9de23791":"code","9766d822":"code","5d355ddf":"code","febcde13":"code","f5d549ff":"code","62f76755":"code","796fbf0f":"code","f7b761a1":"code","9b2e9158":"code","18878fdd":"code","6727a7eb":"markdown","90989999":"markdown","5b6be500":"markdown","1db4832d":"markdown","20a1b69a":"markdown","f92cc8c6":"markdown","5b2cf286":"markdown","17d86232":"markdown","17edfd32":"markdown","12c30190":"markdown","772c698a":"markdown","3798f0ed":"markdown","2ef1523b":"markdown","f9aceabb":"markdown","0aae94f8":"markdown"},"source":{"d535d02a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","57e05a2c":"df_train = pd.read_csv('..\/input\/cat-in-the-dat-ii\/train.csv')\ndf_train.head()","51254b36":"df_test = pd.read_csv('..\/input\/cat-in-the-dat-ii\/test.csv')\ndf_test.head()","8ae1028e":"df_train.duplicated().sum()","c3d91c65":"round(df_train.isnull().sum()\/df_train.shape[0],2)","10f7428f":"binc = [\"bin_0\",\"bin_1\",\"bin_2\",\"bin_3\",\"bin_4\"]\nnomc = [\"nom_0\",\"nom_1\",\"nom_2\",\"nom_3\",\"nom_4\",\"nom_5\",\"nom_6\",\"nom_7\",\"nom_8\",\"nom_9\"]\nordc = [\"ord_0\",\"ord_1\",\"ord_2\",\"ord_3\",\"ord_4\",\"ord_5\"]\ndatec = [\"day\",\"month\"]\nmisscol = binc + ordc\n\ndef fill_na(data,colist):\n    \n    if len(colist)==10:# fill nom_\n        for col in colist:\n            data[col] = data[col].fillna(data[col].value_counts(normalize=True).idxmin())\n    elif len(colist)==11: #fill_binc+ordc\n        for col in colist:\n            data[col] = data[col].fillna(\"Missing\")\n    else:\n        for col in colist:\n            data[col] = data[col].fillna(data[col].median())\n    return data","baaf090b":"df_train = fill_na(df_train,misscol)\ndf_train = fill_na(df_train,nomc)\ndf_train = fill_na(df_train,datec)\n#---------------------------------\ndf_test = fill_na(df_test,misscol)\ndf_test = fill_na(df_test,nomc)\ndf_test = fill_na(df_test,datec)\nprint('df_train null_num: ',df_train.isnull().all().sum())\nprint('df_test null num: ',df_test.isnull().all().sum())\nprint(df_train.head())","32366d4d":"for col in binc:\n    print(\"This is {} unqiue values:\".format(col))\n    print(df_train[col].unique())\n    print(\"-\"*25)","f928ac68":"bin_num = {\"Missing\":0.0,0.0:1.0,1.0:2.0}\nbin_3 = {\"Missing\":0.0,\"F\":1.0,\"T\":2.0}\nbin_4 = {\"Missing\":0.0,\"N\":1.0,\"Y\":2.0}\ndf_train[\"bin_0\"] = df_train[\"bin_0\"].map(bin_num)\ndf_train[\"bin_1\"] = df_train[\"bin_1\"].map(bin_num)\ndf_train[\"bin_2\"] = df_train[\"bin_2\"].map(bin_num)\ndf_train[\"bin_3\"] = df_train[\"bin_3\"].map(bin_3)\ndf_train[\"bin_4\"] = df_train[\"bin_4\"].map(bin_4)\n#-------------------------------------------------\ndf_test[\"bin_0\"] = df_test[\"bin_0\"].map(bin_num)\ndf_test[\"bin_1\"] = df_test[\"bin_1\"].map(bin_num)\ndf_test[\"bin_2\"] = df_test[\"bin_2\"].map(bin_num)\ndf_test[\"bin_3\"] = df_test[\"bin_3\"].map(bin_3)\ndf_test[\"bin_4\"] = df_test[\"bin_4\"].map(bin_4)","3c5d84b4":"for col in ordc:\n    print(\"This is {} unqiue values:\".format(col))\n    print(df_train[col].unique())\n    print(\"-\"*60)","98e39fa8":"from sklearn.preprocessing import LabelEncoder\nord_0 = {\"Missing\":0.0}\nord_1 = {\"Missing\":0.0,\"Novice\":1.0,\"Contributor\":2.0,\"Expert\":3.0,\"Master\":4.0,\"Grandmaster\":5.0}\nord_2 = {\"Missing\":0.0,\"Freezing\":1.0,\"Cold\":2.0,\"Warm\":3.0,\"Hot\":4.0,\"Boiling Hot\":5.0,\"Lava Hot\":6.0}\nord_3 = {\"Missing\":0.0,\"a\":1.0,\"b\":2.0,\"c\":3.0,\"d\":4.0,\"e\":5.0,\"f\":6.0,\"g\":7.0,\"h\":8.0,\"i\":9.0,\"j\":10.0,\"k\":11.0,\"l\":12.0,\"m\":13.0,\"n\":14.0,\"o\":15.0}\nord_4 = {\"Missing\":0.0,\"A\":1.0,\"B\":2.0,\"C\":3.0,\"D\":4.0,\"E\":5.0,\"F\":6.0,\"G\":7.0,\"H\":8.0,\"I\":9.0,\"J\":10.0,\"K\":11.0,\"L\":12.0,\n         \"M\":13.0,\"N\":14.0,\"O\":15.0,\"P\":16.0,\"Q\":17.0,\"R\":18.0,\"S\":19.0,\"T\":20.0,\"U\":21.0,\"V\":22.0,\"W\":23.0,\"X\":24.0,\"Y\":25.0,\"Z\":26.0}\n\nlab = LabelEncoder()\ndf_train[\"ord_0\"] = df_train[\"ord_0\"].replace(ord_0)\ndf_train[\"ord_1\"] = df_train[\"ord_1\"].map(ord_1)\ndf_train[\"ord_2\"] = df_train[\"ord_2\"].map(ord_2)\ndf_train[\"ord_3\"] = df_train[\"ord_3\"].map(ord_3)\ndf_train[\"ord_4\"] = df_train[\"ord_4\"].map(ord_4)\nlab.fit(df_train[\"ord_5\"].values)\ndf_train['ord_5'] = lab.transform(df_train[\"ord_5\"])\n\n\ndf_test[\"ord_0\"] = df_test[\"ord_0\"].replace(ord_0)\ndf_test[\"ord_1\"] = df_test[\"ord_1\"].map(ord_1)\ndf_test[\"ord_2\"] = df_test[\"ord_2\"].map(ord_2)\ndf_test[\"ord_3\"] = df_test[\"ord_3\"].map(ord_3)\ndf_test[\"ord_4\"] = df_test[\"ord_4\"].map(ord_4)\ndf_test['ord_5'] = lab.transform(df_test[\"ord_5\"])\n","2da8160c":"for col in nomc:\n    print(\"This is {} unqiue values:\".format(col))\n    print(df_train[col].unique())\n    print(\"-\"*60)","f51856c1":"lab1 = LabelEncoder()\nlab1.fit(df_train[\"nom_0\"])\ndf_train[\"nom_0\"] = lab1.transform(df_train[\"nom_0\"])\ndf_test[\"nom_0\"] = lab1.transform(df_test[\"nom_0\"])\n\nlab2 = LabelEncoder()\nlab2.fit(df_train[\"nom_1\"])\ndf_train[\"nom_1\"] = lab2.transform(df_train[\"nom_1\"])\ndf_test[\"nom_1\"] = lab2.transform(df_test[\"nom_1\"])\n\nlab3 = LabelEncoder()\nlab3.fit(df_train[\"nom_2\"])\ndf_train[\"nom_2\"] = lab3.transform(df_train[\"nom_2\"])\ndf_test[\"nom_2\"] = lab3.transform(df_test[\"nom_2\"])\n\nlab4 = LabelEncoder()\nlab4.fit(df_train[\"nom_3\"])\ndf_train[\"nom_3\"] = lab4.transform(df_train[\"nom_3\"])\ndf_test[\"nom_3\"] = lab4.transform(df_test[\"nom_3\"])\n\nlab5 = LabelEncoder()\nlab5.fit(df_train[\"nom_4\"])\ndf_train[\"nom_4\"] = lab5.transform(df_train[\"nom_4\"])\ndf_test[\"nom_4\"] = lab5.transform(df_test[\"nom_4\"])\n","a81661d6":"import re\ndf_train[\"nom_5\"] = df_train[\"nom_5\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_train[\"nom_6\"] = df_train[\"nom_6\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_train[\"nom_7\"] = df_train[\"nom_7\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_train[\"nom_8\"] = df_train[\"nom_8\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_train[\"nom_9\"] = df_train[\"nom_9\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\n#---------------------------------------------------------------------------------------------------------------\ndf_test[\"nom_5\"] = df_test[\"nom_5\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_test[\"nom_6\"] = df_test[\"nom_6\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_test[\"nom_7\"] = df_test[\"nom_7\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_test[\"nom_8\"] = df_test[\"nom_8\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_test[\"nom_9\"] = df_test[\"nom_9\"].apply(lambda x:len(set(re.sub(r'\\D',\"\",x))) + len(set(re.sub(r'\\d',\"\",x))))\ndf_train.head()","704dd2ec":"plt.figure(figsize=(30,15))\ncorr = df_train.corr()\nsns.heatmap(corr,annot=True)","8653b05c":"from sklearn.ensemble import RandomForestClassifier\ntarget = df_train[[\"target\"]]\nfeature = df_train.drop(columns=[\"target\",\"id\"])\nrf = RandomForestClassifier()\nrf.fit(feature,target.values.reshape(-1,1))\nim = pd.DataFrame()\nplt.figure(figsize=(15,10))\ng = sns.barplot(y=rf.feature_importances_,x=feature.columns,edgecolor=\"black\",linewidth=3,color=\"#87ceed\")\ng.spines[['top', 'right','bottom']].set_visible(False)\ng.axhline(0.02,color='r',linewidth=1)\nplt.show()","419fa5ea":"feature = feature.drop(columns=[\"bin_0\",\"bin_1\",\"bin_2\"])\ndf_test = df_test.drop(columns=[\"bin_0\",\"bin_1\",\"bin_2\"])","6cf9a64b":"target.value_counts(normalize=True)","b758458e":"from sklearn.model_selection import StratifiedKFold,cross_val_score\nfrom sklearn.metrics import classification_report\nimport xgboost as xgb \nstf = StratifiedKFold(random_state=123,shuffle=False)\nfor train_index,test_index in stf.split(feature,target):\n    x_train,x_test = feature.iloc[train_index],feature.iloc[test_index]\n    y_train,y_test = target.iloc[train_index],target.iloc[test_index]\nxgb = xgb.XGBClassifier(learning_rate=0.1, n_estimators=400, max_depth=5, min_child_weight=1, gamma=0,\n                         subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, seed=27)\nxgb.fit(x_train,y_train)\npre_xgb = xgb.predict(x_test)\nxgb_prob = xgb.predict_proba(x_test)[:,1]","a45b5859":"print(classification_report(y_test,pre_xgb))","9de23791":"rf_1 = RandomForestClassifier()\nrf_1.fit(x_train,y_train)\nrf_1_pre = rf_1.predict(x_test)\nrf_prob = rf_1.predict_proba(x_test)[:,1]\nprint(classification_report(y_test,rf_1_pre))","9766d822":"from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\nfig,ax = plt.subplots(1,2,figsize=(10,6))\ncmxg = confusion_matrix(y_test,pre_xgb)  #xgboost\ndisp = ConfusionMatrixDisplay(cmxg)\ndisp.plot(ax=ax[0])\n\n\ncm = confusion_matrix(y_test,rf_1_pre)  #RF\ndis = ConfusionMatrixDisplay(cm)\ndis.plot(ax=ax[1])\nplt.show()","5d355ddf":"from sklearn.metrics import roc_auc_score\nprint(\"xgboost roc auc score:\",roc_auc_score(y_test,xgb_prob))\nprint(\"RF roc auc score:\",roc_auc_score(y_test,rf_prob))\n","febcde13":"from mlxtend.classifier import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\nimport matplotlib.gridspec as gridspec\n\nLR = LogisticRegression()\nstack = StackingClassifier(classifiers=[xgb,rf_1],meta_classifier=LR)\nstack.fit(x_train,y_train)\n","f5d549ff":"stack_pre = stack.predict(x_test)\nstack_prob = stack.predict_proba(x_test)[:,1]","62f76755":"print(classification_report(y_test,stack_pre))","796fbf0f":"from sklearn.metrics import ConfusionMatrixDisplay,confusion_matrix\nfig,ax = plt.subplots(1,2,figsize=(16,6))\n\n\ncmxg = confusion_matrix(y_test,pre_xgb)  #xgboost\ndisp = ConfusionMatrixDisplay(cmxg)\ndisp.plot(ax=ax[0])\n\n\ncm = confusion_matrix(y_test,rf_1_pre)  #RF\ndis = ConfusionMatrixDisplay(cm)\ndis.plot(ax=ax[1])\nplt.show()\n\n\nscm = confusion_matrix(y_test,stack_pre)  #stack\ndistack = ConfusionMatrixDisplay(scm)\ndistack.plot()\n","f7b761a1":"print(\"stack roc auc score:\",roc_auc_score(y_test,stack_prob))","9b2e9158":"ids = df_test[[\"id\"]]\ntest_data = df_test.drop(columns=[\"id\"])\nsubmisson_pre = stack.predict(test_data.values)\nsub = pd.DataFrame()\nsub[\"id\"] = ids\nsub[\"target\"] = submisson_pre\nsub.head()","18878fdd":"sub.to_csv(\"submission.csv\",index=None)","6727a7eb":"- **According to the importance features of random forest screening, we plan to eliminate the feature columns from bin_0 to bin_2.**","90989999":"- **It can be seen that the data is seriously unbalanced, so when dividing the data and the training data, use hierarchical cross-validation.**","5b6be500":"- **bin----Duality**\n- **nom----Nominal characteristics**\n- **ord----Sequence feature**\n- **day or month ----Date characteristics**","1db4832d":"<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> 2. Dealing with missing values  <\/center> \n <hr>\n <ul>\n     <li>Use median, mean, and mode to fill numerical missing values.<\/li>\n     <li>Use linear interpolation to fill in missing values.<\/li>\n     <li>Use KNN algorithm to fill in numerical missing values.<\/li>\n     <li>Randomly choose to fill in missing values of categorical features.<\/li>\n     <li>For categorical features, add a new column to mark the missing values.<\/li>\n     <li>Fill missing values of categorical features as Missing's special markers.<\/li>\n     <li>Fill missing values into the category with the largest proportion.<\/li>\n <\/ul>\n <hr>\n <p style=\"color:\"#fff7f7>\ud83d\udca5My Choice\uff1a<\/p>\n <ul>\n <li>For Bin_0~4, fill missing values as Missing markers.<\/li>\n <li>For nom_0~9, fill in missing values with the smallest proportion of categories.<\/li>\n <li>For ord_0~5  fill missing values as Missing markers.<\/li>\n <li>For the numeric characteristics of the date column, use the median to fill.<\/li>  \n <\/ul>\n<\/div>\n","20a1b69a":"<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> 3. Encoding process  <\/center> \n<hr>\n<p>\ud83d\udca6Due to the use of algorithms such as XGboost, one-hot encoding is avoided as much as possible, and features are transformed according to label encoding.\n   At the same time, for features containing letters and values, the corresponding letters and numbers will be extracted as new column identifiers.\n    \n<\/p>\n<hr>\n<\/div>","f92cc8c6":"<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> 4. Feature Screening  <\/center> \n<\/div>","5b2cf286":"- Regarding nom_5 to nom_9 as a password, calculate the length of distinct values.","17d86232":"- dealing wirh bin_columns","17edfd32":"- **Score decreased**","12c30190":"- **null_num percent--3%**","772c698a":"- **Xgboost on the left, random forest on the right.**\n- **The actual number is 0, and the number of predictions is 0. Random forest is stronger than xgboost.**\n- **The actual number is 1, and the number of predictions is 1. Xgboost forest is stronger than Random forest.**\n\n","3798f0ed":"<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> <h1>Classification coding challenge \ud83d\udea9<\/h1> <\/center> \n<\/div>","2ef1523b":"<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> 1. Simple view of data  <\/center> \n<\/div>","f9aceabb":"<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> 5 Model XGBoost and RandomForest  <\/center> \n<\/div>","0aae94f8":"<div style=\"color: #fff7f7;\n           display:fill;\n           border-radius:10px;\n           border-style: solid;\n           border-color:#424949;\n           text-align:center;\n           background-color:#8ac6d1 ;\n           font-size:20px;\n           letter-spacing:0.5px;\n           padding: 0.7em;\n           text-align:left\">  \n<center> 6 Stack  <\/center> \n<\/div>"}}