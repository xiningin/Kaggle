{"cell_type":{"75a1f724":"code","4b3f579f":"code","5166a5d9":"code","746123ea":"code","d822ee05":"code","5d5b66f3":"code","15c66a51":"code","07084543":"code","d54cd434":"code","e6a66b15":"code","6453e2fd":"code","f7f58db3":"code","d9ef74b2":"code","5cfcc3aa":"code","4f17463a":"code","5d27710d":"code","829a2dee":"code","85425924":"markdown","f0fa7fe9":"markdown","3219fbfa":"markdown","cc4c4ce0":"markdown","b05986b8":"markdown","4fcff1e6":"markdown","dcf143a1":"markdown","59c351b7":"markdown","62152c52":"markdown","af616f10":"markdown","26be9777":"markdown","7fbdd308":"markdown","1fd723d1":"markdown"},"source":{"75a1f724":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport glob\nfrom pathlib import Path\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nimport cv2\nfrom PIL import Image\nfrom IPython.display import Image\nfrom IPython.display import display\n\nfrom collections import defaultdict\nfrom operator import itemgetter\n\npd.set_option('display.max_rows', 500)","4b3f579f":"main_folder = \"..\/input\/celeba-dataset\/\"\ntrain_path = main_folder + \"img_align_celeba\/img_align_celeba\/\"\ndata_dir = pathlib.Path(train_path)","5166a5d9":"def load_data(main_folder, train_path):\n    df_attr = pd.read_csv(main_folder + 'list_attr_celeba.csv')\n    df_attr['path'] = train_path + df_attr['image_id']\n    df_attr[\"Attractive\"] = df_attr[\"Attractive\"].astype('category')\n    df_attr[\"Attractive\"] = df_attr[\"Attractive\"].cat.codes\n\n    df_partition = pd.read_csv(main_folder + \"list_eval_partition.csv\")\n    df_attr['partition'] = df_partition.partition\n\n    # Filter Male instances\n    df_attr = df_attr[df_attr['Male'] == 1]\n\n    # Filter columns\n    cols = ['image_id', 'Attractive', 'path', 'partition']\n    df_attr = df_attr[cols]\n    \n    return df_attr\n\nload_data(main_folder, train_path)","746123ea":"df_attr.head()","d822ee05":"def configure_for_performance(ds):\n    #ds = ds.cache()\n    ds = ds.shuffle(buffer_size=1000)\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n\ndef decode_img(file_path, label):\n    img = tf.io.read_file(file_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    return tf.image.resize(img, [size, size]), label\n\ndef build_ds(fname, labels):\n    ds = tf.data.Dataset.from_tensor_slices((fname, labels))\n    ds = ds.shuffle(len(fname))\n    ds = ds.map(decode_img, num_parallel_calls=AUTOTUNE)\n    ds = configure_for_performance(ds)\n    return ds","5d5b66f3":"batch_size = 128\nsize = 128\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nfname_train = df_attr[df_attr['partition'] == 0]['path']\nlabels_train = df_attr[df_attr['partition'] == 0]['Attractive']\n\nfname_val = df_attr[df_attr['partition'] == 1]['path']\nlabels_val = df_attr[df_attr['partition'] == 1]['Attractive']\n\nfname_test = df_attr[df_attr['partition'] == 2]['path']\nlabels_test = df_attr[df_attr['partition'] == 2]['Attractive']\n\ntrain_ds = build_ds(fname_train, labels_train)\nval_ds = build_ds(fname_val, labels_val)\ntest_ds = build_ds(fname_test, labels_test)\n\ndata = {\n    'train_ds': train_ds,\n    'val_ds': val_ds, \n    'test_ds': test_ds\n}","15c66a51":"class Tuner(object):\n\n    def __init__(self, architecture, data, classes, epochs, batch_size):\n        self.input_shape = (128, 128, 3)\n\n        self.base_arch = architecture\n        self.nn = self.download_network()\n        self.nn.trainable = False\n\n        self.classes = classes\n\n        self.train_ds = data['train_ds']\n        self.val_ds = data['val_ds']\n        self.test_ds = data['test_ds']\n\n        self.EPOCHS = epochs\n        self.BATCH_SIZE = batch_size\n\n        self.model = self.build()\n        self.predictions = None\n        self.score = None\n\n        self.best_weights = None\n\n    def download_network(self):\n        '''\n        Download the requested CNN with imagenet weights\n        '''\n        nn = None\n\n        if self.base_arch == 'VGG16':\n            nn = tf.keras.applications.VGG16(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'VGG19':\n            nn = tf.keras.applications.VGG19(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'InceptionV3':\n            nn = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'DenseNet121':\n            nn = tf.keras.applications.DenseNet121(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'DenseNet201':\n            nn = tf.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'ResNet152V2':\n            nn = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'MobileNet':\n            nn = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=self.input_shape)\n        elif self.base_arch == 'MobileNetV2':\n            nn = tf.keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=self.input_shape)\n\n        return nn\n\n    def run(self):\n        '''\n        Main driver for Learner object\n        '''\n        self.fine_tune()\n\n    def build(self):\n        '''\n        Build model. Add Dense layer to topless base CNN.\n        '''\n\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Input(shape=self.input_shape))\n        model.add(tf.keras.layers.experimental.preprocessing.Rescaling(1.\/255))\n        model.add(self.nn)\n        model.add(tf.keras.layers.Flatten())\n        model.add(tf.keras.layers.Dropout(0.25))\n        model.add(tf.keras.layers.Dense(1024, activation='relu'))\n        model.add(tf.keras.layers.BatchNormalization())\n        model.add(tf.keras.layers.Dropout(0.25))\n        model.add(tf.keras.layers.Dense(self.classes, activation='softmax'))\n        print (model.summary())\n\n        return model\n\n    def load_weights(self, name):\n        '''\n        Load the best checkpointed weights.\n        '''\n        print('\\nLoading best accuracy weights.')\n        self.model.load_weights(name)\n\n    def fine_tune(self):\n        '''\n        Fine-tune network in 2 phases\n        '''\n\n        print (\"\\nPhase A - Training Fully Connected Layers\\n\")\n        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.001), metrics=['accuracy'])\n\n        # Define checkpoint to save best Phase 1 weights\n        best_weights_ph1 = self.base_arch + \"_ph1_weights.hdf5\"\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(best_weights_ph1, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True, verbose=1)\n\n        history = self.model.fit(\n            self.train_ds,\n            epochs=self.EPOCHS,\n            validation_data=self.val_ds,\n            callbacks=[checkpoint])\n        \n        # Store the best phase 1 accuracy\n        best_acc_ph1 = max(history.history[\"val_accuracy\"])\n        print('\\n\\nMax validation accuracy:', best_acc_ph1)\n\n        print('\\nRestoring best weights and predicting validation set.')\n        self.load_weights(best_weights_ph1)\n\n        # Make predictions based on best phase 1 weights\n        self.predict()\n\n        self.plot_loss(history, self.EPOCHS, 'Transfer Learning: ' + self.base_arch + ' Ph A')\n\n\n        print (\"\\nPhase B  - Fine Tune all Layers \\n\")\n        # Set full original CNN as trainable\n        self.nn.trainable = True\n\n        self.model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=1e-5), metrics=['accuracy'])\n\n        # Define checkpoint to save best Phase 2 weights\n        best_weights_ph2 = self.base_arch + \"_ph2_weights.hdf5\"\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(best_weights_ph2, monitor=\"val_accuracy\", mode=\"max\", save_best_only=True, verbose=1)\n\n        # Fine-tune the full CNN + FC\n        history = self.model.fit(\n            self.train_ds,\n            epochs=self.EPOCHS,\n            validation_data=self.val_ds,\n            callbacks=[checkpoint])\n\n        # Store the best phase 2 accuracy\n        best_acc_ph2 = max(history.history[\"val_accuracy\"])\n        print('\\n\\nMax validation accuracy:', best_acc_ph2)\n\n        # Only if Phase 2 fine-tuning resulted in a better accuracy than phase 1,\n        # restore best phase 2 weights and update Tuner predictions.\n        if best_acc_ph2 > best_acc_ph1:\n            print('\\nPhase 2 resulted in better accuracy than Phase 1.')\n            print('Restoring best weights of Ph2 and predicting validation set.')\n            self.load_weights(best_weights_ph2)\n            self.predict()\n\n        self.plot_loss(history, self.EPOCHS, ' Transfer Learning: ' + self.base_arch + ' Ph B')\n    \n    def predict(self):\n        '''\n        Get predictions and score for validation set.\n        '''\n        print('\\nPredicting test set classes.')\n        self.score = self.model.evaluate(self.test_ds, verbose=0)\n        print('Test set score:', self.score)\n        self.predictions = self.model.predict(self.test_ds, batch_size=self.BATCH_SIZE)\n        print('Done')\n\n    def plot_loss(self, history, epochs, name):\n        print('\\n\\n')\n        plt.figure(figsize=(12,8))\n        plt.plot(np.arange(0, epochs), history.history[\"loss\"], label=\"train_loss\")\n        plt.plot(np.arange(0, epochs), history.history[\"val_loss\"], label=\"val_loss\")\n        plt.plot(np.arange(0, epochs), history.history[\"accuracy\"], label=\"train_acc\")\n        plt.plot(np.arange(0, epochs), history.history[\"val_accuracy\"], label=\"val_acc\")\n        plt.title(\"Training Loss and Accuracy - {}\".format(name))\n        plt.xlabel(\"Epoch #\")\n        plt.ylabel(\"Loss\/Accuracy\")\n        plt.legend()\n        plt.show()","07084543":"NET = 'DenseNet201'\nEPOCHS = 5\nBATCH_SIZE = 128\nCLASSES = 2\n\ntuner = Tuner(NET, data, CLASSES, EPOCHS, BATCH_SIZE)\ntuner.run()","d54cd434":"def plot(img):\n    plt.imshow(img)\n    plt.show()\n    \ndef process_image(path):\n    img = cv2.imread(str(path))\n    img = img[:,:,::-1]\n    \n    x = 100\n    y = 0\n    h = 200\n    w = 180\n    img = img[y:y+h, x:x+w]\n\n    img = cv2.resize(img, (128, 128))    \n    img = np.expand_dims(img, axis=0)\n    \n    return img","e6a66b15":"def get_predictions(pathlist):\n    print('\\nPredicting face attractiveness...')\n    results = defaultdict()\n    \n    for path in pathlist:\n        img = process_image(path)\n        pred = tuner.model.predict(img)\n        \n        name = str(path).split('\/')[-1].split('.')[0].replace('_', ' ')\n        team = str(path).split('\/')[-2]\n        #results[name] = pred[0][1]\n        results[name] = defaultdict()\n        results[name]['pred'] = pred[0][1]\n        results[name]['team'] = team\n        results[name]['path'] = str(path)\n        \n    return results","6453e2fd":"pathlist = Path('..\/input\/bundesliga14teams\/').glob('*\/*\/*.jpg')\nres = get_predictions(pathlist)","f7f58db3":"df = pd.DataFrame(res)\ndf = df.transpose()\ndf.sort_values(by=['pred'], inplace=True, ascending=False)\ndf","d9ef74b2":"top = df.iloc[:20]\ntop","5cfcc3aa":"low = df.iloc[-20:]\nlow","4f17463a":"def show_picture(path):\n    img = cv2.imread(str(path))\n    img = img[:,:,::-1]\n    plt.imshow(img)\n    plt.show()\n\ndef show_pictures(players):\n    \n    plt.figure(figsize=(20,20)) # specifying the overall grid size\n    \n    for i, (name, data) in enumerate(players.iterrows()):\n        path = '.\/' + data['path'].replace('\\\\', '\/')\n        plt.subplot(5,5,i+1)    # the number of images in the grid is 5*5 (25)\n        img = cv2.imread(str(path))\n        img = img[:,:,::-1]\n        plt.imshow(img)\n        plt.title(name + ' - ' + str(round(data['pred'], 4)))\n\n    plt.show()","5d27710d":"show_pictures(top)","829a2dee":"show_pictures(low)","85425924":"## What happened?\n\nBy chance I noticed that the well-known CelebA dataset not only consists of 200,000+ face images, but also comes with 40 attributes, of which one is \"Attractive\". So a little \"fun project\" came to my mind in which I would be training a neural network to classify faces from being attractive or not.\u00a0\n\n*After all, more than 200K photos should be enough data for an AI to learn what attractiveness is, right?* \n\nBeing passionate about soccer I then planned to classify the faces of soccer players into being attractive or not and let the neural network find something like the most attractive team per major series, e.g. Bundesliga, Premier League, Primera Division, Serie A, etc\u2026\n\nThe tendency I saw after classifying the faces from players of the Top 4 Bundesliga 2019\/2020 teams, however, was too shocking to continue.\n\n*The neural network believed that colored players are extremely likely to be unattractive.*","f0fa7fe9":"Like for all Transfer Learning projects I use my personal Tuner class, which handles all relevant aspects, including:\n1. Downloading the pre-trained network(s)\n2. Specifying the Dense Layers and compiling the neural network\n3. Handling both fine-tuning stages (stage 1: training dense, stage 2: fine-tuning original pre-trained CNN)\n4. Saving\/restoring weight files\n5. Visualizing training progress\n6. etc\u2026","3219fbfa":"And now we're ready to fine-tune a DenseNet201 architecture pretrained on imagenet to learn classify attractiveness in human faces.","cc4c4ce0":"After 5 epochs for each fine-tuning stage, the model reached an accuracy of 83% on the test set.","b05986b8":"Next the data pipeline had to be prepared.","4fcff1e6":"*The tendency is pretty obvious. 15 out of the 20 players with the lowest probability of being attractive are colored indicating an extreme racial bias\u200a-\u200alikely introduced by the\u00a0dataset.*","dcf143a1":"## What did I do to get\u00a0there?\n\nLet's see what I did to get these results. I will not go into the details of what a Neural Network, CNN, Transfer Learning and other technical aspects are.\nAs usual the dataset and attributes for the faces need to be loaded.","59c351b7":"Now let's see the predictions of the **20 players with the lowest probability of being attractive** in the eyes of the neural network:","62152c52":"From the resulting predictions let's look at those **20 players with the highest probability of being attractive** in the eyes of the neural network:","af616f10":"# How I created a Racist AI by a naive dataset selection","26be9777":"I accidentally build an AR system. Wait, not that kind of AR, that is supposed to be a multi-billion dollar technology:  \nAugmented Reality. Instead, what I built was an Artificial Racist system, made possible mainly by a naive dataset selection.\n\nNeither am I proud of this, nor does it reflect my personal attitude, but I feel it is still right to share the story and  \nraise once more awareness about this topic. In case there is no naming for it yet, let's call them ARI:  \nArtificial Racist Intelligence.","7fbdd308":"With such unexpected results even trying to bump up the test accuracy to 90%, modifying the Dense layers behind the pre-trained CNN or changing the crop settings during the soccer player predictions would not change the overall picture.\u00a0\nFor me, this little (supposed to be) fun project was stopped here, mainly due to a pre-mature selection of the dataset.\u00a0\nSome learnings that came with it or just got re-confirmed are:\n1. Maybe the most imporant one: Do not trust a dataset because it is large and the amount of data will get you to the finish line.\n2. A simple attribute (-1, 1) for a complex feature like attractiveness is not sufficient to build a non-biased system.\n3. Without being careful one extremely easy builds racists AI.\n4. Newer research introduced attractiveness computation by using facial landmarks.","1fd723d1":"## The Predictions\n\nNow we're set to predict the attractiveness of the soccer players. As I live in Germany I started with the top 4 of the Bundesliga season 2019\/2020. I downloaded the signature card style player photographs from https:\/\/www.weltfussball.de\/. A little bit of cropping was required to meet similar image properties compared to the CelebA dataset."}}