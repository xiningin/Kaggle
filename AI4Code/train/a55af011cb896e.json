{"cell_type":{"82ea3864":"code","324ace8d":"code","910bb860":"code","fd754fdf":"code","24f91fa6":"code","d5f28784":"code","c576bfe6":"code","66ddf011":"code","9b58516c":"code","a72bd7fb":"code","1f3ef190":"code","d460e2b5":"code","0075e1eb":"code","383b3987":"code","c29e2751":"code","2b17ae81":"code","f56cc5b1":"code","7457e605":"code","7e0f44f0":"code","3246ac91":"markdown","13140c85":"markdown","d770d01a":"markdown","2272fb74":"markdown","aac74da8":"markdown","66f2be34":"markdown","a26ca748":"markdown","c9c357d8":"markdown","8c974004":"markdown","bfdf2f34":"markdown","35d97422":"markdown","868cf4fc":"markdown","b423e01b":"markdown","e7a00088":"markdown","907fdd6f":"markdown"},"source":{"82ea3864":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns\nimport tensorflow as tf\nimport keras_tuner as kt\nfrom keras_tuner import RandomSearch\nfrom tensorflow import feature_column\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold","324ace8d":"def checkNull_fillData(df):\n    for col in df.columns:\n        if len(df.loc[df[col].isnull() == True]) != 0:\n            if df[col].dtype == \"float64\" or df[col].dtype == \"int64\":\n                df.loc[df[col].isnull() == True,col] = df[col].median()\n            else:\n                df.loc[df[col].isnull() == True,col] = df[col].mode()[0]\n                \ndef getColumnDevide(df):\n    str_list = [] \n    num_list = []\n    for colname, colvalue in train.iteritems():\n        if type(colvalue[1]) == str:\n            str_list.append(colname)\n        else:\n            num_list.append(colname)\n            \n    return str_list,num_list\n","910bb860":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ndrop_elements = ['PassengerId', 'Name', 'Ticket','Fare']\ntrain = train.drop(drop_elements, axis = 1)\n\ncheckNull_fillData(train)\n\ntrain[\"Cabin_type\"] = train[\"Cabin\"].apply(lambda cabin: cabin[0])\ntrain[\"family_member_size\"] = 1 + train[\"SibSp\"] + train[\"Parch\"]\n\ndrop_elements = ['SibSp', 'Parch','Cabin']\ntrain = train.drop(drop_elements, axis = 1)","fd754fdf":"str_list,num_list = getColumnDevide(train)\nprint(str_list)\nprint(\"\")\nnum_list = [col for col in num_list if col != 'Survived']\nprint(num_list)","24f91fa6":"train.isnull().sum()","d5f28784":"categorical_columns = str_list\n\ncategorical_features = [\n    feature_column.indicator_column(\n        feature_column.categorical_column_with_vocabulary_list(key, sorted(list(train[key].unique())))\n    ) for key in categorical_columns\n]\ncategorical_features","c576bfe6":"numeric_columns = num_list\nnumerical_features = [feature_column.numeric_column(key) for key in numeric_columns]\nnumerical_features","66ddf011":"input_dictionary = dict()\ninputs = dict()\nfor item in numerical_features:\n    inputs[item.key] = tf.keras.layers.Input(name=item.key, shape=(), dtype=\"float64\")\nfor item in categorical_features:\n    dtype = None\n    if train[item.categorical_column.key].dtype == object:\n        dtype = tf.string\n    else:\n        dtype = tf.int64\n    inputs[item.categorical_column.key] = tf.keras.layers.Input(name=item.categorical_column.key, shape=(), dtype=dtype)\n    \nprint(input_dictionary)\nprint(\"\")\nprint(inputs)","9b58516c":"class BinaryCrossEntropyWithWeights(tf.keras.losses.Loss):\n\n    def __init__(self, negative_weights, positive_weights):\n        super().__init__()\n        self.negative_weights = negative_weights\n        self.positive_weights = positive_weights\n        \n    def call(self, y_true, y_pred):\n        print(y_true, y_pred)\n        y_true = tf.cast(y_true, y_pred.dtype)\n        pos = self.positive_weights * y_true * tf.math.log(y_pred + tf.keras.backend.epsilon())\n        neg = self.negative_weights * (1.0 - y_true) * tf.math.log(1.0 - y_pred + tf.keras.backend.epsilon())\n        return -(pos + neg)","a72bd7fb":"def get_weights():\n    total = train.shape[0]\n    pos_count = (train[\"Survived\"] == 1).sum()\n    neg_count = (train[\"Survived\"] == 0).sum()\n    negtive_weights =  pos_count \/ total\n    positive_weights = neg_count \/ total\n    return negtive_weights, positive_weights","1f3ef190":"negtive_weights, positive_weights = get_weights()\nbce_with_weights = BinaryCrossEntropyWithWeights(negtive_weights, positive_weights)","d460e2b5":"def build_model(hp):\n    x = tf.keras.layers.DenseFeatures(numerical_features + categorical_features, name='deep')(inputs)\n    for i in range(hp.Int(\"depth\", min_value=3, max_value=8)):\n        x = tf.keras.layers.Dense(hp.Choice(\"width\", values=[4, 8, 16, 32, 64]), activation=\"relu\")(x)\n        x = tf.keras.layers.Dropout(hp.Choice(\"dropout\", values=[0.1, 0.2, 0.3, 0.4, 0.5]))(x)\n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n    optimizer = tf.keras.optimizers.Adam(hp.Float(\"learning_rate\", min_value=1e-5, max_value=5e-3))\n    model.compile(loss=bce_with_weights, optimizer=optimizer, metrics=[\"accuracy\"])\n    return model","0075e1eb":"batch_size = 32\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, train[\"Survived\"])):\n    fold = index + 1\n    train_features = train.iloc[train_indices]\n    val_features = train.iloc[valid_indices]\n    train_file_name = \"train_fold_%d.csv\"%(fold)\n    val_file_name = \"val_fold_%d.csv\"%(fold)\n    train_features.to_csv(train_file_name, index=False)\n    val_features.to_csv(val_file_name, index=False)\n    train_ds = tf.data.experimental.make_csv_dataset(train_file_name, batch_size=batch_size, label_name=\"Survived\", shuffle=True)\n    val_ds = tf.data.experimental.make_csv_dataset(val_file_name, batch_size=val_features.shape[0], label_name=\"Survived\", shuffle=False)\n    tuner = kt.RandomSearch(\n        build_model,\n        objective='val_accuracy',\n        max_trials=100\n    )\n    tuner.search(train_ds.take(train_features.shape[0] \/\/ batch_size + 1), epochs=10, validation_data=val_ds.take(1))\n    break","383b3987":"best_model = tuner.get_best_models()[0]\nbest_model","c29e2751":"tf.keras.utils.plot_model(best_model, show_shapes=False, rankdir='LR')","2b17ae81":"best_model.summary()","f56cc5b1":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ndrop_elements = ['PassengerId', 'Name', 'Ticket']\ntest = test.drop(drop_elements, axis = 1)\n\ncheckNull_fillData(test)\n\ntest[\"Cabin_type\"] = test[\"Cabin\"].apply(lambda cabin: cabin[0])\ntest[\"family_member_size\"] = 1 + test[\"SibSp\"] + test[\"Parch\"]\n\ndrop_elements = ['SibSp', 'Parch','Cabin']\ntest = test.drop(drop_elements, axis = 1)","7457e605":"test_path = \"test.csv\"\ntest_file = test.to_csv(test_path, index=False)\ntest_ds = tf.data.experimental.make_csv_dataset(test_path, batch_size=test.shape[0], shuffle=False)","7e0f44f0":"y_pred = best_model.predict(test_ds.take(1)).reshape(-1)\ny_pred = np.array(y_pred > 0.5, dtype=int)\n\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nsubmission.Survived = y_pred\nsubmission.to_csv(\"submission.csv\", index=False)","3246ac91":"# build model","13140c85":"# number columns","d770d01a":"# make model ","2272fb74":"# search best model","aac74da8":"# submission","66f2be34":"# common function","a26ca748":"# train data process","c9c357d8":"# show model","8c974004":"# test data to file and to dataframe ","bfdf2f34":"# set input data","35d97422":"## category columns","868cf4fc":"# devide columns","b423e01b":"# train data process \n1.data load \\\n2.drop insignificant columns \\\n3.missing data process \\\n4.make new feature ","e7a00088":"# model summary","907fdd6f":"# object function"}}