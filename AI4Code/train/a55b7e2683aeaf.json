{"cell_type":{"3f15cda7":"code","2fcede77":"code","21741f82":"code","c29856c8":"code","b0fb9ef8":"code","4b15bdf1":"code","0cc8511b":"code","bdd1267b":"code","caa845f5":"code","656a82f4":"code","b112163b":"code","124d6ecf":"code","eff1775d":"code","a84589b5":"code","36f026a0":"code","6744a900":"code","b568609d":"code","cf20ff61":"code","4ebd58e3":"code","5c402afe":"code","1e5ff325":"code","0e5ad442":"code","92c316b6":"code","ecae381a":"code","35a543d0":"code","5fd01253":"code","6e72db7d":"code","2f836366":"code","59756945":"code","273bda42":"code","3de34dbe":"code","b1a663ff":"code","47c224f7":"code","d952e34e":"code","b92708ab":"code","a3064d48":"code","d03aa5df":"code","f17d5255":"code","a8e727cc":"code","75e2b34a":"code","ff291060":"code","f1b94407":"code","e138a55d":"code","89f90009":"code","1d6aed01":"code","be8a5557":"code","d0706d85":"code","3f5528e4":"code","1b2b4eb0":"code","31088c2c":"code","7226126e":"code","e594fa8c":"code","3dbd9315":"code","b01a68a8":"code","8813f075":"code","60c71cc7":"code","cb5185ff":"code","ba0abb43":"code","017caf1c":"code","8b6e9fdb":"code","93bceb99":"code","eccdbebe":"code","b5fbcbb1":"code","441a5bf2":"code","a850daf1":"code","ba6d787a":"code","e533d6a3":"code","949f96f4":"code","8900dcef":"code","5174a510":"code","9c2b086d":"code","ec24ff8c":"code","b838b17d":"code","a79ab8e8":"code","c2aafe91":"code","5131558b":"code","940d8a73":"code","e38c2656":"code","536ba8fd":"markdown","bd9ab754":"markdown","03df35f9":"markdown","4b30b907":"markdown","593d9e8d":"markdown","832513e9":"markdown","fb18bd78":"markdown","dd9878c3":"markdown","d3aad20a":"markdown","9ceeb270":"markdown","2e8d9d8c":"markdown","0e2abb50":"markdown","cd4f0ff7":"markdown","4b72a204":"markdown","de23b935":"markdown","54d007b8":"markdown","7e4e27c5":"markdown","e553aaf1":"markdown","c1c849e2":"markdown","ad2a6629":"markdown","1b777965":"markdown","cc7494e6":"markdown","0ac7c37c":"markdown","a152872d":"markdown","f0aaacec":"markdown","84067e90":"markdown","530dd55e":"markdown","8e8e9a8c":"markdown","1f1a3bcb":"markdown","98f2f99a":"markdown","5db69278":"markdown","75988979":"markdown"},"source":{"3f15cda7":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\nfrom sklearn.model_selection import cross_validate, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report, plot_roc_curve\nfrom sklearn.model_selection import train_test_split, cross_validate\n\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\nfrom pandas.api.types import CategoricalDtype\n\npd.set_option('display.float_format', lambda x: '%.2f' % x)     # virg\u00fclden sonra 5 dijit g\u00f6ster.\npd.set_option('display.max_columns', None)                      # b\u00fct\u00fcn sat\u0131rlar\u0131 g\u00f6ster\npd.set_option('display.max_rows', None)                         # b\u00fct\u00fcn s\u00fctunlar\u0131 g\u00f6ster\n\n\nimport warnings\nfrom warnings import filterwarnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning) \n\nfilterwarnings(\"ignore\",category=DeprecationWarning)","2fcede77":"df = pd.read_csv(\"..\/input\/diabetesdataset\/diabetes.csv\")\ndf.head()","21741f82":"def upper_col_name(dataframe):\n    upper_cols = [col.upper() for col in dataframe.columns]\n    dataframe.columns = upper_cols\n    return dataframe\n\ndf = upper_col_name(df)\n\ndf.head()","c29856c8":"# Genel Exploration for Dataset\ndef check_df(dataframe, head=5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\n    \ncheck_df(df)   ","b0fb9ef8":"# Selection of Categorical and Numerical Variables:\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n\n    cat_cols = cat_cols + num_but_cat\n\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\" and \"ID\" not in col]\n\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","4b15bdf1":"print(cat_cols)","0cc8511b":"print(num_cols)","bdd1267b":"# General Exploration for Categorical Variables:\n\ndef cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe, palette=\"Set2\")\n        plt.show()\n\n\nfor col in cat_cols:\n    cat_summary(df, col, plot=True)        ","caa845f5":"# General Exploration for Numerical Variables:\n\ndef num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n    if plot:\n        dataframe[numerical_col].hist(bins=20, edgecolor='#C39BD3', color='#C39BD3')\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()\n\nfor col in num_cols:\n    num_summary(df, col, plot=True)         ","656a82f4":"# Check the features containing NaN values:\n\ndef missing_values_df(dataframe, na_col_name=False):\n    na_cols = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    na_cols_number = dataframe[na_cols].isnull().sum()\n    na_cols_ratio = dataframe[na_cols].isnull().sum() \/ dataframe.shape[0]\n    missing_values_table = pd.DataFrame({\"Missing_Values (#)\": na_cols_number, \\\n                                         \"Ratio (%)\": na_cols_ratio * 100,\n                                         \"Type\" : dataframe[na_cols].dtypes})\n    print(missing_values_table)\n    print(\"************* Number of Missing Values *************\")\n    print(dataframe.isnull().sum().sum())\n    if na_col_name:\n        print(\"************* Nullable variables *************\")\n        return na_cols\n\nmissing_values_df(df)","b112163b":"min_zero_val = [col for col in df.columns if (df[col].min() == 0 and \"OUTCOME\" not in col)]\nmin_zero_val = [col for col in min_zero_val if \"PREGNANCIES\" not in col]\nmin_zero_val","124d6ecf":"# Let's replace zero values with NaN values:\n\ndf[min_zero_val] =  df[min_zero_val].replace(0, np.NaN)","eff1775d":"# Descriptive Statistics\n\ndf.describe().T","a84589b5":"# Let's check the missing values again:\n\nmissing_values_df(df)","36f026a0":"# One Hot Encoding:\ndff = pd.get_dummies(df[cat_cols + num_cols], drop_first=True)\ndff.head()","6744a900":"# Scaling:\nscaler = MinMaxScaler()\ndff = pd.DataFrame(scaler.fit_transform(dff), columns=dff.columns)","b568609d":"dff.head()","cf20ff61":"# KNN Imputing:\n\nfrom sklearn.impute import KNNImputer\nimputer = KNNImputer(n_neighbors=5)\ndff = pd.DataFrame(imputer.fit_transform(dff), columns=dff.columns)\ndff.head()","4ebd58e3":"# Inverse scaling values: \ndff = pd.DataFrame(scaler.inverse_transform(dff), columns=dff.columns)","5c402afe":"dff.head()","1e5ff325":"dff.describe().T","0e5ad442":"# Fill NaN Val\u0131es:\n\ndf[\"GLUCOSE\"] = dff[[\"GLUCOSE\"]]\ndf[\"BLOODPRESSURE\"] = dff[[\"BLOODPRESSURE\"]]\ndf[\"SKINTHICKNESS\"] = dff[[\"SKINTHICKNESS\"]]\ndf[\"INSULIN\"] = dff[[\"INSULIN\"]]\ndf[\"BMI\"] = dff[[\"BMI\"]]","92c316b6":"missing_values_df(df)","ecae381a":"df.loc[((df['GLUCOSE']) < 140), \"IGT\"] = \"NORMAL\"\ndf.loc[(df['GLUCOSE'] >= 140) & (df['GLUCOSE'] <=199), \"IGT\"] = \"OBSERVE\"\ndf.loc[((df['GLUCOSE']) > 200), \"IGT\"] = \"DANGEROUS\"\ndf.head()","35a543d0":"# NEW_BMI_LABEL:\n\ndf[\"NEW_BMI_LABEL\"] = np.where(df['BMI'] < 21, \"STARVATION\", np.NaN)\ndf[\"NEW_BMI_LABEL\"] = np.where(((df['BMI'] >= 21) & (df['BMI'] <= 25)), \"HEALTHY\", df[\"NEW_BMI_LABEL\"])\ndf[\"NEW_BMI_LABEL\"] = np.where(((df['BMI'] > 25) & (df['BMI'] <= 30)), \"OVERWEIGHT\" , df[\"NEW_BMI_LABEL\"])\ndf[\"NEW_BMI_LABEL\"] = np.where(((df['BMI'] >30) & (df['BMI'] <= 40)), \"OBESE\", df[\"NEW_BMI_LABEL\"]) \ndf[\"NEW_BMI_LABEL\"] = np.where(df['BMI'] > 40, \"VERY_OBESE\" , df[\"NEW_BMI_LABEL\"])\n\ndf.groupby(\"NEW_BMI_LABEL\").agg({\"OUTCOME\": [\"mean\", \"count\"]})","5fd01253":"# NEW_INSULIN_LEVEL:\n\ndf[\"NEW_INSULIN_LEVEL\"] = np.where( df['INSULIN'] < 120, \"NORMAL\", np.NaN)\ndf[\"NEW_INSULIN_LEVEL\"] = np.where( ((df['INSULIN'] >= 120) & (df['INSULIN'] < 166)),  \"RISK\", df[\"NEW_INSULIN_LEVEL\"]  )\ndf[\"NEW_INSULIN_LEVEL\"] = np.where( df['INSULIN'] >= 166 , \"HIGH_RISK\", df[\"NEW_INSULIN_LEVEL\"])\n\ndf.groupby(\"NEW_INSULIN_LEVEL\").agg({\"OUTCOME\": [\"mean\", \"count\"]})","6e72db7d":"# Glukoz Levels:\n\ndf[\"NEW_GLUCOSE_LEVEL\"] = np.where(df['GLUCOSE']< 70, \"LOW\", np.NaN)\ndf[\"NEW_GLUCOSE_LEVEL\"] = np.where(((df['GLUCOSE'] >= 70 ) & (df['GLUCOSE'] <= 99)), \"NORMAL\", df[\"NEW_GLUCOSE_LEVEL\"])\ndf[\"NEW_GLUCOSE_LEVEL\"] = np.where(((df['GLUCOSE'] >= 100) & (df['GLUCOSE'] <= 125)), \"PREDIABETES\" , df[\"NEW_GLUCOSE_LEVEL\"])\ndf[\"NEW_GLUCOSE_LEVEL\"] = np.where(df['GLUCOSE'] >= 126, \"DIABETES\" , df[\"NEW_GLUCOSE_LEVEL\"])\n\ndf.groupby(\"NEW_GLUCOSE_LEVEL\").agg({\"OUTCOME\": [\"mean\", \"count\"]})","2f836366":"# Blood Pleasure Levels:\ndf[\"NEW_BLOOD_PLEASURE_LEVEL\"] = np.where(df['BLOODPRESSURE']< 61, \"NORMAL\", np.NaN)\ndf[\"NEW_BLOOD_PLEASURE_LEVEL\"] = np.where(((df['BLOODPRESSURE'] >= 61 ) & (df['BLOODPRESSURE'] <= 75)), \"LOW\", df[\"NEW_BLOOD_PLEASURE_LEVEL\"])\ndf[\"NEW_BLOOD_PLEASURE_LEVEL\"] = np.where(((df['BLOODPRESSURE'] >= 75 ) & (df['BLOODPRESSURE'] <= 90)), \"PREHYPERTENSION_1\", df[\"NEW_BLOOD_PLEASURE_LEVEL\"])\ndf[\"NEW_BLOOD_PLEASURE_LEVEL\"] = np.where(((df['BLOODPRESSURE'] >= 91 ) & (df['BLOODPRESSURE'] <= 100)), \"PREHYPERTENSION_2\", df[\"NEW_BLOOD_PLEASURE_LEVEL\"])\ndf[\"NEW_BLOOD_PLEASURE_LEVEL\"] = np.where(df['BLOODPRESSURE'] > 100, \"HYPERTENSION\" , df[\"NEW_BLOOD_PLEASURE_LEVEL\"])\n\ndf.groupby(\"NEW_BLOOD_PLEASURE_LEVEL\").agg({\"OUTCOME\": [\"mean\", \"count\"]})","59756945":"# Diabetes Pedigreee Function Level:\n \ndf[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"] = np.where(df['DIABETESPEDIGREEFUNCTION']<= 0.3, \"LOW\", \"OTHER\")\ndf[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"] = np.where(((df['DIABETESPEDIGREEFUNCTION'] > 0.3) & (df['DIABETESPEDIGREEFUNCTION'] <= 0.6)), \"MEDIUM\", df[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"])\ndf[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"] = np.where(((df['DIABETESPEDIGREEFUNCTION'] > 0.6) & (df['DIABETESPEDIGREEFUNCTION'] <= 0.8)), \"HIGH\", df[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"])\ndf[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"] = np.where(df['DIABETESPEDIGREEFUNCTION'] > 0.8, \"VERY_HIGH\" , df[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"])\n\ndf.groupby(\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\")[\"OUTCOME\"].mean()","273bda42":"# Skin Thickness Level:\n \ndf[\"NEW_SKINTHICKNESS_LEVEL\"] = np.where(df['SKINTHICKNESS']<= 20, \"LOW\", \"OTHER\")\ndf[\"NEW_SKINTHICKNESS_LEVEL\"] = np.where(((df['SKINTHICKNESS'] > 20) & (df['SKINTHICKNESS'] <= 30)), \"MEDIUM\", df[\"NEW_SKINTHICKNESS_LEVEL\"])\ndf[\"NEW_SKINTHICKNESS_LEVEL\"] = np.where(df['SKINTHICKNESS'] > 30, \"HIGH\" , df[\"NEW_SKINTHICKNESS_LEVEL\"])\n\ndf.groupby(\"NEW_SKINTHICKNESS_LEVEL\").agg({\"OUTCOME\": [\"mean\", \"count\"]})","3de34dbe":" # Pregnancies Level:\n\ndf[\"NEW_PREGNANCIES_LEVEL\"] = np.where(df['PREGNANCIES']< 3, \"LOW\", \"OTHER\")\ndf[\"NEW_PREGNANCIES_LEVEL\"] = np.where(((df['PREGNANCIES'] >= 3) & (df['PREGNANCIES'] <= 5)), \"MEDIUM\", df[\"NEW_PREGNANCIES_LEVEL\"])\ndf[\"NEW_PREGNANCIES_LEVEL\"] = np.where(df['PREGNANCIES'] > 5, \"HIGH\" , df[\"NEW_PREGNANCIES_LEVEL\"])\n\ndf.groupby(\"NEW_PREGNANCIES_LEVEL\").agg({\"OUTCOME\": [\"mean\", \"count\"]})","b1a663ff":"# Age Level:\n\nbins = [0, 25, 38, int(df[\"AGE\"].max())]\nage_labels = [str(int(df[\"AGE\"].min())-1)+'_25', '26_38', '39_'+ str(int(df[\"AGE\"].max()))]\n\ndf[\"NEW_AGE_CAT\"] = pd.cut(df[\"AGE\"], bins, labels=age_labels)","47c224f7":"# Other Features: \n\ndf[\"IS_PREGNANCIES_YES\"] = np.where(df[\"PREGNANCIES\"] >0 , 1, 0)\ndf[\"NEW_AGE_SKINTHICKNESS\"]  = df[\"AGE\"] \/ df[\"SKINTHICKNESS\"]\ndf[\"NEW_SKINTHICKNESS_PREGNANCIES\"]  = df[\"PREGNANCIES\"] \/ df[\"SKINTHICKNESS\"]\ndf[\"NEW_BMI_PREGNANCIES\"]  = df[\"PREGNANCIES\"] \/ df[\"BMI\"]\ndf[\"NEW_SKINTHICKNESS_BLOODPRESSURE\"]  = df[\"SKINTHICKNESS\"] * df[\"BMI\"]\ndf[\"NEW_BMI_SKINTHICKNESS\"]  = df[\"SKINTHICKNESS\"] * df[\"BMI\"]\ndf[\"NEW_AGE_BLOODPRESSURE\"]  = df[\"AGE\"] * df[\"BLOODPRESSURE\"]\ndf[\"NEW_GLUCOSE_BLOODPRESSURE_AGE\"]  = df[\"AGE\"] * df[\"BLOODPRESSURE\"] *  df[\"GLUCOSE\"] \ndf[\"NEW_GLUCOSE_DPF_INSULIN\"] = df[\"GLUCOSE\"] * df[\"DIABETESPEDIGREEFUNCTION\"] * df[\"INSULIN\"] \ndf[\"NEW_GLUCOSE_DPF\"] = df[\"GLUCOSE\"] * df[\"DIABETESPEDIGREEFUNCTION\"] \ndf[\"NEW_GLUCOSE_DPF_RATIO\"] = df[\"GLUCOSE\"] \/ df[\"DIABETESPEDIGREEFUNCTION\"] \ndf[\"NEW_AGE_BMI\"]  = df[\"AGE\"] * df[\"BMI\"]\ndf[\"NEW_AGE_INSULIN\"]  = df[\"AGE\"] * df[\"INSULIN\"]\ndf[\"NEW_GLUCOSE_INSULIN\"]  = df[\"GLUCOSE\"] * df[\"INSULIN\"]\ndf[\"NEW_BMI_AGE_DPF\"] = df[\"BMI\"] * df[\"DIABETESPEDIGREEFUNCTION\"] *df[\"AGE\"]\ndf[\"NEW_GLUCOSE_INSULIN_DPF\"] = df[\"GLUCOSE\"] * df[\"INSULIN\"] *df[\"DIABETESPEDIGREEFUNCTION\"]","d952e34e":"# Let's check \"inf, -inf\" values:\n\ndf[df.isin([np.nan, np.inf, -np.inf]).any(axis=1)].shape[0]","b92708ab":"# Let's define num_cols again due to derived new features :\n\nnum_cols = grab_col_names(df)[1]","a3064d48":"def outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    q1 = dataframe[col_name].quantile(q1)  # 1.\u00c7eyrek\n    q3 = dataframe[col_name].quantile(q3)  # 3.\u00c7eyrek\n    interquantile_range = q3 - q1  # range'i hesaplayal\u0131m\n    low_limit = q1 - 1.5 * interquantile_range # low & up limit:\n    up_limit = q3 + 1.5 * interquantile_range\n    return low_limit, up_limit","d03aa5df":"# Let's check if features include outliers :\n\ndef check_outlier(dataframe,col_name, q1=0.05, q3=0.95, plot=False):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name, q1, q3)\n    if dataframe[(dataframe[col_name] < low_limit) | (dataframe[col_name] > up_limit)].any(axis=None):\n        print(col_name + \":\" + \"Outliers Have been detected\")\n    if plot:\n        sns.boxplot(x=dataframe[col_name], palette=\"Set3\")\n        plt.show()\n    \nfor col in num_cols:\n    print(col, \":\", check_outlier(df, col, plot=True))\n","f17d5255":"df.describe().T","a8e727cc":"cat_cols, num_cols, cat_but_car = grab_col_names(df)\ndf_ = df[num_cols]","75e2b34a":"clf = LocalOutlierFactor(n_neighbors=5)\nclf.fit_predict(df_)\n\ndf_scores = clf.negative_outlier_factor_\ndf_scores[0:10]","ff291060":"# LOF Visualization: \n\nscores = pd.DataFrame(np.sort(df_scores))\nscores.plot(stacked=True, xlim=[0, 10], style='.-')\nplt.show()","f1b94407":"np.sort(df_scores)[0:10]  ","e138a55d":"threshold = np.sort(df_scores)[1]\nthreshold","89f90009":"# Let's look at the observations which are below selected LOF threshold value and then remove this observations:\n\ndf[df_scores < threshold].head()","1d6aed01":"df.drop(df[df_scores < threshold].index, inplace=True)","be8a5557":"df[\"NEW_BMI_LABEL\"] = df[\"NEW_BMI_LABEL\"].astype(CategoricalDtype([\"STARVATION\", \"HEALTHY\", \"OVERWEIGHT\", \"OBESE\",\"VERY_OBESE\"],ordered=True)) \ndf[\"NEW_INSULIN_LEVEL\"] = df[\"NEW_INSULIN_LEVEL\"].astype(CategoricalDtype([\"NORMAL\", \"RISK\", \"HIGH_RISK\"],ordered=True)) \ndf[\"NEW_GLUCOSE_LEVEL\"] = df[\"NEW_GLUCOSE_LEVEL\"].astype(CategoricalDtype([\"LOW\", \"NORMAL\", \"PREDIABETES\", \"DIABETES\"],ordered=True)) \ndf[\"NEW_BLOOD_PLEASURE_LEVEL\"] = df[\"NEW_BLOOD_PLEASURE_LEVEL\"].astype(CategoricalDtype([\"LOW\", \"NORMAL\", \"PREHYPERTENSION_1\", \"PREHYPERTENSION_2\", \"HYPERTENSION\"],ordered=True)) \ndf[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"] = df[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL\"].astype(CategoricalDtype([\"LOW\", \"MEDIUM\", \"HIGH\", \"VERY_HIGH\"],ordered=True)) \ndf[\"NEW_SKINTHICKNESS_LEVEL\"] = df[\"NEW_SKINTHICKNESS_LEVEL\"].astype(CategoricalDtype([\"LOW\", \"MEDIUM\", \"HIGH\"],ordered=True)) \ndf[\"NEW_PREGNANCIES_LEVEL\"] = df[\"NEW_PREGNANCIES_LEVEL\"].astype(CategoricalDtype([\"LOW\", \"MEDIUM\", \"HIGH\"],ordered=True)) \n","d0706d85":"# Rare Analysing: \n\ndef rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\n\nrare_analyser(df, \"OUTCOME\", cat_cols)","3f5528e4":"# According to the explanation above, there is no feature that was considered a worthless column.\n\nuseless_cols = [col for col in df.columns if ( ( df[col].nunique() == 2 \n                                            and (df[col].value_counts() \/ len(df) < 0.01).any(axis=None))\n                                            | df[col].nunique() == 1 )]\nprint(useless_cols)","1b2b4eb0":"df.dtypes","31088c2c":"cat_cols = grab_col_names(df)[0]\ncat_cols = [col for col in num_cols if \"OUTCOME\" not in col]\ncat_cols","7226126e":"# Let's define cat cols for one-hot-encoding:\n\ndef ohe_cols(dataframe):\n    ohe_col_names = [col for col in dataframe.columns if (dataframe[col].dtype not in [\"float64\", \"int64\"] and 10 >= dataframe[col].nunique() >= 2)]\n    return ohe_col_names\n\n\ndef one_hot_encoder(dataframe, ohe_col_names, drop_first=True):\n    dms = pd.get_dummies(dataframe[ohe_col_names], drop_first=drop_first)   \n    df_ = dataframe.drop(columns=ohe_col_names, axis=1)                      \n    dataframe = pd.concat([df_, dms],axis=1)                                 \n    return dataframe","e594fa8c":"ohe_col_list = ohe_cols(df)\nohe_col_list","3dbd9315":"df = one_hot_encoder(df, ohe_col_list)","b01a68a8":"df.head()","8813f075":"# Risk Segment:\n\ndf[\"RISK_SEGMENT\"] = df[\"NEW_BMI_LABEL_OBESE\"] + df[\"NEW_BMI_LABEL_OVERWEIGHT\"] +   df[\"NEW_BMI_LABEL_VERY_OBESE\"] \\\n                     + df[\"NEW_INSULIN_LEVEL_RISK\"] + df[\"NEW_INSULIN_LEVEL_HIGH_RISK\"] \\\n                     + df[\"NEW_GLUCOSE_LEVEL_PREDIABETES\"] +  df[\"NEW_GLUCOSE_LEVEL_DIABETES\"] \\\n                     + df[\"NEW_BLOOD_PLEASURE_LEVEL_PREHYPERTENSION_1\"] +  df[\"NEW_BLOOD_PLEASURE_LEVEL_PREHYPERTENSION_2\"] +  df[\"NEW_BLOOD_PLEASURE_LEVEL_HYPERTENSION\"]  \\\n                     + df[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL_VERY_HIGH\"]   +  df[\"NEW_DIABETESPEDIGREEFUNCTION_LEVEL_HIGH\"] \\\n                     + df[\"NEW_SKINTHICKNESS_LEVEL_HIGH\"] + \\\n                     + df[\"NEW_PREGNANCIES_LEVEL_HIGH\"] ","60c71cc7":"df.groupby(\"RISK_SEGMENT\").agg({\"OUTCOME\": [\"mean\",\"count\"]})","cb5185ff":"# Let's merge some groups:\n\ndf.loc[df[\"RISK_SEGMENT\"] == 0,\"RISK_SEGMENT\"] = df.loc[df[\"RISK_SEGMENT\"] == 0,\"RISK_SEGMENT\"].replace({0: 1})\ndf.loc[df[\"RISK_SEGMENT\"] == 7,\"RISK_SEGMENT\"] = df.loc[df[\"RISK_SEGMENT\"] == 7,\"RISK_SEGMENT\"].replace({7: 6})","ba0abb43":"df.groupby(\"RISK_SEGMENT\").agg({\"OUTCOME\": [\"mean\",\"count\"]})","017caf1c":"# Scaling:\n\ndef StandartScaling(dataframe, col_name):\n    ss = StandardScaler()\n    dataframe[col_name] = ss.fit_transform(dataframe[col_name])\n    return dataframe\n\ndef MinMaxScaling(dataframe, col_name):\n    mms = MinMaxScaler()\n    dataframe[col_name] = mms.fit_transform(dataframe[col_name])\n    return dataframe\n\ndef RobustScaling(dataframe, col_name):\n    rs = RobustScaler()\n    dataframe[col_name] = rs.fit_transform(dataframe[col_name])\n    return dataframe\n\n\ndef Scaling(dataframe, method):\n    numerical_cols = grab_col_names(dataframe)[1]\n    if method == \"StandartScaling\":\n        StandartScaling(dataframe, numerical_cols)\n    elif method == \"MinMaxScaling\":\n        MinMaxScaling(dataframe, numerical_cols)\n    else:\n        RobustScaling(dataframe, numerical_cols)\n    return dataframe","8b6e9fdb":"# Let's apply Robust Scaling function for scaling features:\n\ndf[num_cols] = (Scaling(df[num_cols], \"RobustScaling\"))","93bceb99":"df.head()","eccdbebe":"df.dtypes","b5fbcbb1":"conv_cols = [col for col in df.columns if df[col].dtype == \"uint8\"]\ndf[conv_cols] = df[conv_cols].astype(\"int64\")","441a5bf2":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","a850daf1":"def high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    num_cols = grab_col_names(dataframe)[1]\n    corr = dataframe[num_cols].corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(np.bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] > corr_th)]\n    if plot:\n        sns.set(rc={'figure.figsize': (15, 15)})\n        sns.heatmap(corr, cmap=\"RdBu\",  annot=True, annot_kws={\"size\": 9}, fmt= '.2f')\n        plt.show()\n    return drop_list\n","ba6d787a":"df.dtypes","e533d6a3":"# Let's observe the high correlated features:\n\nhigh_correlated_cols(df, plot=True, corr_th=0.90)","949f96f4":"df.isnull().sum().sum()","8900dcef":"# Defining dependent and independent variables:\n\nX = df.drop(\"OUTCOME\", axis=1)\ny = df[[\"OUTCOME\"]] ","5174a510":"# Base Models:\n\nclassifiers = [('LR', LogisticRegression(solver='liblinear')),\n               ('KNN', KNeighborsClassifier()),\n               (\"SVC\", SVC()),\n               (\"CART\", DecisionTreeClassifier()),\n               (\"RF\", RandomForestClassifier()),\n               ('Adaboost', AdaBoostClassifier()),\n               ('GBM', GradientBoostingClassifier()),\n               ('XGBoost', XGBClassifier(eval_metric='mlogloss')),\n               ('LightGBM', LGBMClassifier()),\n               ('CatBoost', CatBoostClassifier(verbose=False))\n               ]\n\n\nfor name, classifier in classifiers:\n    cv_results = cross_validate(classifier, X, y, cv=3, scoring=[\"roc_auc\"])\n    print(f\"AUC: {round(cv_results['test_roc_auc'].mean(),4)} ({name}) \")\n","9c2b086d":"cart_params = {'max_depth': range(3, 8),\n               \"min_samples_split\": range(5, 20)}\n\nknn_params = {\"n_neighbors\": range(2, 50)}\n\nadaboost_params = {\"n_estimators\": [200, 500],\n                   \"learning_rate\": [0.1, 0.01]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1],\n                   \"n_estimators\": [300, 500, 750],\n                   \"colsample_bytree\": [0.5, 0.7, 1]}\n\nrf_params = {\"max_depth\": [5, 8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [8, 15, 20],\n             \"n_estimators\": [200, 500]}\n\ngbm_params= {\"learning_rate\": [0.01,0.001],\n            \"max_depth\": [5,None],\n            \"max_features\": [\"auto\",\"sqrt\"],\n            \"n_estimators\": [250,500]}\n\ncatb_params = {\"iterations\":[250,500] ,\n             \"learning_rate\": [0.01,0.05, 0.1],\n             \"depth\": [3, 6,None]}\n\n\nclassifiers = [(\"CART\", DecisionTreeClassifier(), cart_params),\n               (\"KNN\", KNeighborsClassifier(), knn_params),\n               (\"AdaBoost\", AdaBoostClassifier(), adaboost_params),\n               (\"LightGBM\", LGBMClassifier(), lightgbm_params),\n               (\"RF\", RandomForestClassifier(), rf_params),\n               (\"GBM\", GradientBoostingClassifier(), gbm_params),\n               (\"CatBoost\",CatBoostClassifier(verbose=False), catb_params)]\n\nbest_models = {}\nfor name, classifier, params in classifiers:\n    print(f\"########## {name} ##########\")\n    cv_results = cross_validate(classifier, X, y, cv=3, scoring=[\"roc_auc\"])\n    print(f\"AUC (Before): {round(cv_results['test_roc_auc'].mean(),4)}\")\n\n\n    gs_best = GridSearchCV(classifier, params, cv=3, n_jobs=-1, verbose=False).fit(X, y.values.ravel())\n    final_model = classifier.set_params(**gs_best.best_params_)\n\n    cv_results = cross_validate(final_model, X, y, cv=3, scoring=[\"roc_auc\"])\n    print(f\"AUC (After): {round(cv_results['test_roc_auc'].mean(), 4)}\")\n    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n\n    best_models[name] = final_model","ec24ff8c":"# Let's fit model based on optimum parameters:\n\ngbm_final_model = best_models['GBM'].fit(X, y)\ny_pred = gbm_final_model.predict(X) ","b838b17d":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\n\nplot_importance(gbm_final_model, X)","a79ab8e8":"drop_list = [\"NEW_GLUCOSE_INSULIN\",\n             \"PREGNANCIES\",\n             \"SKINTHICKNESS\" ,\"NEW_SKINTHICKNESS_BLOODPRESSURE\",\n             \"NEW_AGE_BLOODPRESSURE\",\n             \"DIABETESPEDIGREEFUNCTION\", \n             \"NEW_GLUCOSE_INSULIN\",\n             \"NEW_GLUCOSE_INSULIN_DPF\"]\n\ndf.drop(columns = drop_list, inplace=True)","c2aafe91":"# Model Performance:\n\ncv_results = cross_validate(gbm_final_model, X, y, cv=5, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n\nprint(cv_results['test_accuracy'].mean())\nprint(cv_results['test_f1'].mean())\nprint(cv_results['test_roc_auc'].mean())","5131558b":"# CONFUSION MATRIX\n\ndef plot_confusion_matrix(y, y_pred, conf_matrix=False):\n    acc = round(accuracy_score(y, y_pred), 2)\n    cm = confusion_matrix(y, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\".0f\")\n    plt.xlabel('y_pred')\n    plt.ylabel('y')\n    sns.set(rc={'figure.figsize': (3, 3)})\n    plt.title('Accuracy Score: {0}'.format(acc), size=2)\n    plt.show()\n    if conf_matrix:\n        cf_matrix = confusion_matrix(y,y_pred)\n        print(cf_matrix)\n\nplot_confusion_matrix(y, y_pred, conf_matrix=True )","940d8a73":"random_user1 = X.sample(1, random_state=112)\nrandom_user2 = X.sample(1, random_state=1)\nrandom_user3 = X.sample(1, random_state=110)","e38c2656":"# Prediction:\n\nprint(\"1st user is \", np.where(gbm_final_model.predict(random_user1)>0,\"Diabetes.\", \"Not Diabetes.\"))\nprint(\"2nd user is \", np.where(gbm_final_model.predict(random_user2)>0,\"Diabetes.\", \"Not Diabetes.\"))\nprint(\"3rd user is \", np.where(gbm_final_model.predict(random_user3)>0,\"Diabetes.\", \"Not Diabetes.\"))","536ba8fd":"We'll use GBM Model for feature selection:","bd9ab754":"<a id=\"section-eight\"><\/a>\n# **Summary**\n\n**1. Dataset was read.**\n\n**2. Exploratory Data Analysis:**\n\n* It was observed that there were 768 observations and 9 variables in the data set. \n\n* Data exploration stage has been completed by examining descriptive statistics and seperating categorical and numeric columns.\n\n**3. Data PreProcessing & Feature Engineering:**\n\n* It was observed that there is no feature which includes missing values, but when the data set has been handled in detailed, it was noticed that some features such as Glucose, Blood Pressure, Skin Thickness, Insulin and BMI cannot be zero. It means that these values can be handled as missing values. Also, NA values of dataset were filled by using KNN Imputer.\n\n* Outliers were dropped by using LOF method.\n\n* Class distributions of variables were analyzed. Useless features which do not carry parser information were dropped.\n\n* Dummy variables were created by using One Hot Encoding.\n\n* The X variables were normalized by using Robust Scaling.\n\n**4. Model Building & Evaluation:**\n\n* X and y variables were determined.\n\n* Base models were improved (KNN, C&R Tree, Random Forest, SVC, Extra Trees, GBM, Light GBM, XGBoost, CatBoost)\n\n* Observing the performance of the base models, GBM were chosen as the final model and  for Feature Selection. By using the values of Feature Importance, high correlated features were removed.\n\n* The final model was created and model performance metrics were observed.\n\n* Estimation was realized for selected 3-randomly users.\n\n\nThank you for your comments and votes:)","03df35f9":"Outlier values have been detected only for \"SKINTHICKNESS\" and \"INSULIN\":","4b30b907":"<a id=\"section-one\"><\/a>\n# **Exploratory Data Analysis**","593d9e8d":"Number of pregnancy can be zero, but values such as Glucose, Blood Pressure, Skin Thickness, Insulin and BMI cannot be zero. These values should be replaced with NULL values by handling as missing values.","832513e9":"We'll handle GBM Model for model performance and prediction:","fb18bd78":"<a id=\"section-seven\"><\/a>\n# **Prediction**","dd9878c3":"**One - Hot Encoding:**","d3aad20a":"<a id=\"section-five\"><\/a>\n# **Feature Selection**","9ceeb270":"<a id=\"section-four\"><\/a>\n# **Automated Hyperparameter Optimization**\n\n","2e8d9d8c":"***We will be performing the other data preprocessing steps after deriving the new variables.***","0e2abb50":"Now, observing the values of feature importance, we can remove high correlated columns:","cd4f0ff7":"***1. Missing Values***","4b72a204":"<a id=\"section-two\"><\/a>\n# **Data Preprocessing & Feature Engineering**\n\nThis part consists of 4 steps which are below:\n\n1. Missing Values  \n2. Outliers \n3. Rare Encoding, Label Encoding, One-Hot Encoding \n4. Feature Scaling  ","de23b935":"**Import Data**","54d007b8":"# **Diabetics Prediction with Automated Machine Learning**\n\n![image.png](attachment:e521e5a0-b97c-4591-9555-9dae3fedf3a1.png)","7e4e27c5":"**3. Rare Encoding , Label & One-Hot Encoding:**","e553aaf1":"<a id=\"section-three\"><\/a>\n# **Modelling**\n","c1c849e2":"**2. Outliers:** ","ad2a6629":"***Correlation Matrix:***","1b777965":"Firstly, fit the base models and then we can observe the new results of the models by removing above high correlated columns  according the features importances.","cc7494e6":" # **Dataset Story**\n \n * The dataset is part of a dataset from the National Institutes of Diabetes-Digestive-Kidney Diseases in the USA. Data used for diabetes research on Pima Indian women aged 21 and over living in Phoenix, the 5th largest city of the State of Arizona in the USA. \n\n* It consists of 768 observations and 8 numerical independent variables. The target variable is specified as \"outcome\"; 1 diabetes test result being positive, 0 indicates negative.\n\n# **Features**\n\n* Pregnancies: Number of pregnancies\n\n* Glucose: Product code (2 hour plasma glucose concentration in oral glucose tolerance test)\n\n* BloodPressure: Blood Pressure (mm Hg)(mm Hg)\n\n* SkinThickness: Skin Thickness\n\n* Insulin: 2-hour serum insulin (mu U\/ml)\n\n* DiabetesPedigreeFunction: Function which gives 2 hour plasma glucose concentration in oral glucose tolerance test \n\n* Age \u2013 Age (years)\n\n* Outcome: Have the disease (1) or not (0)","0ac7c37c":"* The normal  glucose level is less than 140 mg\/dL. So, we calculated IGT (impaired glucose tolerance) based on this information.","a152872d":"***LOF(Local Outliers Factor) method:***\n\nLet's handle outliers by using  \"LOF (Local Outlier Factor)\".","f0aaacec":" # **Methodology**\n \n \n* [Exploratory Data Analysis](#section-one)\n* [Data Preprocessing & Feature Engineering](#section-two)\n* [Modelling](#section-three)\n* [Automated Hyperparameter Optimization](#section-four)\n* [Feature Selection](#section-five)\n* [Model Performance](#section-six)\n* [Prediction](#section-seven)\n* [Summary](#section-eight)\n","84067e90":"Now, let's define the orderedness of the categorical variables:","530dd55e":"<a id=\"section-six\"><\/a>\n# **Model Performance**","8e8e9a8c":"**Import Libraries & Setting Configurations**","1f1a3bcb":"Let's choose a random user to predict if the person is diabetes.","98f2f99a":"**Rare Encoding:**\n\nLet's consider the class distributions of categorical variables:\n\n- If a categorical variable with 2-classes and the distribution ratio of one of them is less than 1%, we can remove this variable, because it does not carry information.\n\n- If the feature has more than 2-classes and  at least  2-classes of distribution rates of them is below 1%, we can merge these classes as only one class that is called \"RARE\".","5db69278":" # **Goal**\n\n\nThe purpose of this project is to predict diabetes based on diagnostic measurement metrics for early diagnosis of diabetic people.","75988979":"**Dealing with missing values by using KNN Imputer:**"}}