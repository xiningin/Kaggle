{"cell_type":{"15070af2":"code","d4a6b8a5":"code","9d0f661e":"code","e5468df2":"code","aeb74822":"code","2aeb37be":"code","c795ebd0":"code","39775d44":"code","a950d073":"code","7ce20d92":"code","89db37d8":"code","1faeaff1":"code","9e9967cf":"code","4ef9abb3":"code","74dda1ad":"code","f095d13c":"code","601288b6":"code","2d82d03a":"code","5734273e":"code","583cde67":"code","82aaa865":"code","b546fbc6":"code","b79a5327":"code","4b1bd3c2":"code","9d3cc2cb":"code","0d2c917d":"code","5b6a08a3":"markdown","17524a4f":"markdown","b40a1484":"markdown","7987c463":"markdown","5a4218ba":"markdown"},"source":{"15070af2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n#!pip install pycountry-convert\n#!pip install pandas-profiling\nimport warnings\nwarnings.filterwarnings('ignore') #Used only for maintaining a clean notebook. Not a best practice.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_rows',9999)\npd.set_option('display.max_columns',9999)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\nimport pandas_profiling\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfiles = []\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        files.append(os.path.join(dirname, filename))\ndf = pd.read_csv(files[0])\n# Any results you write to the current directory are saved as output.","d4a6b8a5":"df.describe()","9d0f661e":"df.info()","e5468df2":"df.select_dtypes('object')","aeb74822":"df.select_dtypes('int')","2aeb37be":"df.select_dtypes('float')","c795ebd0":"canceled_corr = df.corr()['is_canceled'].abs()\nrelation =  pd.Series(df.corr()['is_canceled']\/df.corr()['is_canceled'].abs(),name='sign')\npd.concat([canceled_corr,relation],axis=1).sort_values(by='is_canceled',ascending=False)","39775d44":"df['is_canceled'].value_counts()\/df['is_canceled'].shape[0]","a950d073":"df.dropna(subset=['country'],inplace=True)","7ce20d92":"y = df['is_canceled']\nX = df.drop(columns=['is_canceled','reservation_status','reservation_status_date','arrival_date_year','lead_time'])\n\nX['fulfilled_room_request'] = np.where(X['assigned_room_type']==X['reserved_room_type'],1,0) # Requested room was granted to customer\nX['company'] = np.where(X['company'].notnull(),1,0) # the data shows whether it was a company booking or not\nX['agent'] = np.where(X['agent'].notnull(),1,0) # if booked by an agent\nX['children'] = X['children'].fillna(0) ","89db37d8":"cat_cols = X.select_dtypes('object').columns.tolist()","1faeaff1":"num_cols = (X.select_dtypes('float').columns.tolist())+(X.select_dtypes('int').columns.tolist())","9e9967cf":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom sklearn.metrics import recall_score\n\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nonehot = OneHotEncoder(sparse=False,handle_unknown='ignore')\nstandard = StandardScaler()\ncol_transformer = ColumnTransformer(transformers=[('scaler',standard,num_cols),('onehot',onehot,cat_cols)],)\n\n\nlog = Pipeline(steps=[('transform',col_transformer),('model',LogisticRegression(random_state=11))])\nrf = Pipeline(steps=[('transform',col_transformer),('model',RandomForestClassifier(random_state=43))])\nsv = Pipeline(steps=[('transform',col_transformer),('model',LinearSVC())])\nbase = Pipeline(steps=[('transform',col_transformer),('model',DummyClassifier(random_state=19))])\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)\n","4ef9abb3":"np.mean(cross_val_score(log,X_train,y_train,cv=3,scoring='recall'))","74dda1ad":"np.mean(cross_val_score(rf,X_train,y_train,cv=3,scoring='recall'))","f095d13c":"np.mean(cross_val_score(sv,X_train,y_train,cv=3,scoring='recall'))","601288b6":"np.mean(cross_val_score(base,X_train,y_train,cv=3,scoring='recall'))","2d82d03a":"from sklearn.model_selection import GridSearchCV\n\nrf_params = {\n    'model__max_depth':[20,100,None],\n    'model__n_estimators':[x for x in range(200,2000,200)]\n}\n\nlog_params = {\n    'model__C':np.logspace(-4,4,10)\n}\n\nrf_grid = GridSearchCV(rf,rf_params,cv=3,scoring='recall',verbose=True)\nlog_grid = GridSearchCV(log,log_params,cv=3,scoring='recall',verbose=True)","5734273e":"rf_grid.fit(X_train,y_train)","583cde67":"log_grid.fit(X_train,y_train)","82aaa865":"rf_grid.best_estimator_","b546fbc6":"log_grid.best_estimator_","b79a5327":"best_log = Pipeline(steps=[('transformer',col_transformer),('model',LogisticRegression(C=166.81005372000558))])","4b1bd3c2":"best_rf = Pipeline(steps=[('transformer',col_transformer),('model',RandomForestClassifier(max_depth=100,n_estimators=400))])","9d3cc2cb":"best_log.fit(X_train,y_train)\nlog_train_preds = best_log.predict(X_train)\nlog_test_preds = best_log.predict(X_test)\nprint('Train Score: ',recall_score(y_train,log_train_preds))\nprint('Test Score: ',recall_score(y_test,log_test_preds))","0d2c917d":"best_rf.fit(X_train,y_train)\nrf_train_preds = best_rf.predict(X_train)\nrf_test_preds = best_rf.predict(X_test)\nprint('Train Score: ',recall_score(y_train,rf_train_preds))\nprint('Test Score: ',recall_score(y_test,rf_test_preds))","5b6a08a3":"> RandomForest overfits but still retains a higher test score while LogisticRegression maintains a stable performance. Given its speed and stability, we will stick with LogisticRegression","17524a4f":"# 3 Hyperparameter Tuning","b40a1484":"# 4 Final Validation","7987c463":"# 2 Base Model & Comparison","5a4218ba":"# 1 Cleaning, Wrangling, & Feature Engineering"}}