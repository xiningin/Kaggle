{"cell_type":{"0986613e":"code","06b928ec":"code","e658b13f":"code","a7a05d00":"code","89dd66e4":"code","95e3e7a1":"code","3440bb10":"code","d5606822":"code","2837f686":"code","f07b0c65":"code","37d071c2":"code","5645cfa0":"code","00981582":"code","ff8e86a0":"code","dcf0e254":"code","c6088db3":"code","386a3b12":"code","ae079bda":"code","1de62c18":"code","1f9e3650":"code","26cea88c":"code","ea9c2be2":"code","252e0c6d":"code","cb5ee9a3":"code","d37a5a5c":"code","b3134340":"markdown","4af4b6f4":"markdown","50627df6":"markdown","efab026f":"markdown","d90aadb9":"markdown","746bb74a":"markdown","324d92cf":"markdown"},"source":{"0986613e":"# Matplotlib config\n%matplotlib inline\n%config InlineBackend.figure_formats = ['svg']\n%config InlineBackend.rc = {'figure.figsize': (5.0, 3.0)}\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport keras\nfrom keras.datasets import fashion_mnist","06b928ec":"(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()","e658b13f":"print('Train set: ' + str(X_train.shape))\nprint('Test set: ' + str(X_test.shape))","a7a05d00":"# Normalization\nX_train_clean = X_train \/ 255.\nX_test_clean = X_test \/ 255.","89dd66e4":"# Print one imige of the normalized train set\nprint(np.argmax(y_train[50000]))\nplt.imshow(X_train_clean[50000])\nplt.show()","95e3e7a1":"#Add image noise in the normalized train and test datasets\nX_train_noise = X_train_clean + np.random.normal(0, 0.1, (60000, 28, 28))\nX_test_noise = X_test_clean + np.random.normal(0, 0.1, (10000, 28, 28))","3440bb10":"# Print one imige of the normalized train set with image noise\nprint(np.argmax(y_train[50000]))\nplt.imshow(X_train_noise[50000])\nplt.show()","d5606822":"print('Train set: ' + str(X_train_noise.shape))\nprint('Test set: ' + str(X_test_noise.shape))","2837f686":"# Concatenate togeter the normalized train (than test) set with noise and the one without noise\n\nX_train = np.concatenate((X_train_noise, X_train_clean), axis=0)\nX_test = np.concatenate((X_test_noise, X_test_clean), axis=0)\n\ny_train = np.concatenate((y_train, y_train), axis=0)\ny_test = np.concatenate((y_test, y_test), axis=0)","f07b0c65":"print('Train set: ' + str(X_train.shape))\nprint('Test set: ' + str(X_test.shape))","37d071c2":"# One image with noise in the final train dataset\n\nprint(np.argmax(y_train[50000]))\nplt.imshow(X_train[50000])\nplt.show()","5645cfa0":"# One image without noise in the final train dataset\n\nprint(np.argmax(y_train[110000]))\nplt.imshow(X_train[110000])\nplt.show()","00981582":"#Change the type\nX_train = X_train.astype(np.float32)\nX_test = X_test.astype(np.float32)","ff8e86a0":"#Reshape in new variables (by adding the color channel no. 1 for black\/white)\nX_train_cnn = X_train.reshape(-1, 28, 28, 1)\nX_test_cnn = X_test.reshape(-1, 28, 28, 1)\n\nfrom tensorflow.keras.utils import to_categorical\ny_train_cnn = to_categorical(y_train)\ny_test_cnn = to_categorical(y_test)","dcf0e254":"print('Train set: ' + str(X_train_cnn.shape))\nprint('Test set: ' + str(X_test_cnn.shape))","c6088db3":"print(y_train_cnn[110000])\nprint(np.argmax(y_train_cnn[110000]))\nplt.imshow(X_train_cnn[110000])\nplt.show()","386a3b12":"#Loading Tensorflow\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import RMSprop\n\nmodel = Sequential()\nmodel.add(Conv2D(28, kernel_size=(3, 3), input_shape=(28, 28, 1), activation=\"relu\", padding=\"same\", kernel_regularizer=keras.regularizers.l2(0.01)))\nmodel.add(Conv2D(28, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(56, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(Conv2D(56, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(112, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(Conv2D(112, kernel_size=(3, 3), activation=\"relu\", padding=\"same\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(224, activation=\"relu\"))\nmodel.add(Dense(112, activation=\"relu\"))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation=\"softmax\"))\n\nmodel.summary()\n\nmodel.compile(optimizer=RMSprop(), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","ae079bda":"from keras.preprocessing.image import ImageDataGenerator\n\ngen = ImageDataGenerator(width_shift_range=3, height_shift_range=3, zoom_range=0.1, horizontal_flip=True)\n\nmodel.fit_generator(gen.flow(X_train_cnn, y_train_cnn, batch_size=128, shuffle=True), epochs=10, workers=8)","1de62c18":"#Fitting further with a smaller learning-rate (0.0001)\nmodel.compile(optimizer=RMSprop(0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nmodel.fit_generator(gen.flow(X_train_cnn, y_train_cnn, batch_size=128, shuffle=True), epochs=10, workers=8)","1f9e3650":"#Fitting further with a smaller learning-rate (0.00001)\nmodel.compile(optimizer=RMSprop(0.00001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nmodel.fit_generator(gen.flow(X_train_cnn, y_train_cnn, batch_size=128, shuffle=True), epochs=10, workers=8)","26cea88c":"print(model.evaluate(X_train_cnn, y_train_cnn))\nprint(model.evaluate(X_test_cnn, y_test_cnn))","ea9c2be2":"model.save(\"Fashion-MNIST_Cezi.h5\")","252e0c6d":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\n\nprint(y_test_cnn[100])\nprint(np.argmax(y_test_cnn[100]))\nplt.imshow(X_test_cnn[100], cmap=\"gray_r\")\nplt.show()","cb5ee9a3":"pred = model.predict(X_test_cnn)","d37a5a5c":"import numpy as np\nprint(pred[100])\nprint(np.argmax(pred[100]))","b3134340":"**<center><font size=6>Fashion MNIST with CNN and new data generation<\/font><\/center>**\n***\n\n**Date**: 05.04.2021\n\n**Table of Contents**\n- <a href='#intro'>1. The data<\/a> \n- <a href='#proc'>2. Preprocessing the data<\/a>\n- <a href='#fit'>3. Fitting and validating the model: CNN<\/a>\n- <a href='#pred'>4. Predicting with CNN<\/a>","4af4b6f4":"# <a id='fit'>3. Fitting and testing the model: CNN<\/a>","50627df6":"# <a id='proc'>2. Preprocessing the data<\/a>","efab026f":"Labels\n\nEach training and test example is assigned to one of the following labels:\n\n|Class|Meaning| \n| :- | :- |\n| 0 | T-shirt\/top | \n| 1 | Trouser | \n| 2 | Pullover | \n| 3 | Dress | \n| 4 | Coat | \n| 5 | Sandal | \n| 6 | Shirt | \n| 7 | Sneaker | \n| 8 | Bag | \n| 9 | Ankle boot | ","d90aadb9":"Labels\n\nEach training and test example is assigned to one of the following labels:\n\n|Class|Meaning| \n| :- | :- |\n| 0 | T-shirt\/top | \n| 1 | Trouser | \n| 2 | Pullover | \n| 3 | Dress | \n| 4 | Coat | \n| 5 | Sandal | \n| 6 | Shirt | \n| 7 | Sneaker | \n| 8 | Bag | \n| 9 | Ankle boot | ","746bb74a":"# <a id='intro'>1. The data<\/a>","324d92cf":"# <a id='pred'>4. Predicting with CNN<\/a>"}}