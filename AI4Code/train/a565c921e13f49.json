{"cell_type":{"122609a8":"code","f4382422":"code","74336ef6":"code","3265bbd7":"code","5b7a9c21":"code","f0164865":"code","4b4bc11a":"code","88250ebf":"code","8ea70381":"code","3ce7e6ad":"code","e1710966":"code","ea7662cf":"code","f477fe58":"code","75850582":"code","04e97324":"code","af582bf8":"code","dc35fd73":"code","b67a38d9":"code","d5bd93c2":"code","84a9f395":"code","507e9497":"code","4f06c862":"code","647bf046":"code","866261f6":"code","2abdbef1":"code","a3838ae8":"code","d9d96c65":"markdown","1ccbdf35":"markdown","9dc25697":"markdown","518924f7":"markdown","71a9a807":"markdown","01c75214":"markdown","bf9646ed":"markdown","09a24566":"markdown","3aea6381":"markdown","f80a48b9":"markdown","32fe42ab":"markdown","c8eed165":"markdown","85f3194b":"markdown","d46681fe":"markdown","cb8a7710":"markdown","164190c5":"markdown","7cc3c9cd":"markdown","6fb4c53b":"markdown","c8c75662":"markdown","90e574d8":"markdown","972b2a27":"markdown","199313c4":"markdown","4825b928":"markdown","733a3b56":"markdown","0c9c304b":"markdown","dc5d8fd2":"markdown"},"source":{"122609a8":"import time\nimport math\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f4382422":"# Importing the dataset\ndf = pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')\nprint(df.head())\nprint(df.info())\nprint(df.isnull().sum())\n# Looking for how many unique values there are in the categorical features\nprint('Different regions - ', df['region'].unique())\nprint('Different numbers of children - ', df['children'].unique())","74336ef6":"# Adding a log charges as well as charges to train a model to later\ndf['log_charges'] = np.log(df['charges'])\n\n# PLotting the distribution of charges and log charges to check for normal distribution\nfig, ax = plt.subplots(nrows=2)\nsns.distplot(df['charges'], ax=ax[0], hist=False).set_title('charges')\nsns.distplot(df['log_charges'], ax=ax[1], hist=False).set_title('log charges')\nplt.subplots_adjust(hspace=0.7)\nplt.show()","3265bbd7":"print('bmi correlation - ', df['charges'].corr(df['bmi']))\nprint('age correlation - ', df['charges'].corr(df['age']))","5b7a9c21":"# Adjusting the font size of the figures to match the overall figure size\nsns.set(font_scale = 0.5)\n\n# Plotting box and whiskers to show the distribution of charges according to these features\nfig = plt.figure(dpi=220)\nplt.rcParams[\"axes.labelsize\"] = 8\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nax = fig.add_subplot(2, 2, 1)\nsns.boxplot(data=df, x='smoker', y='charges', ax=ax)\nax = fig.add_subplot(2, 2, 2)\nsns.boxplot(data=df, x='region', y='charges', ax=ax)\nax = fig.add_subplot(2, 2, 3)\nsns.boxplot(data=df, x='sex', y='charges', ax=ax)\nax = fig.add_subplot(2, 2, 4)\nsns.boxplot(data=df, x='children', y='charges', ax=ax)\nplt.show()\nsns.set(font_scale = 1)\n\n# How many are in each category of children\nprint(df['children'].value_counts())","f0164865":"# Scatter plots to show the correlation between features and charges\nsns.lmplot(data=df, x='age', y='charges', hue='smoker')\nplt.show()\nsns.lmplot(data=df, x='bmi', y='charges', hue='smoker')\nplt.show()","4b4bc11a":"fig, ax = plt.subplots(nrows=2, dpi=110)\nfig.tight_layout(pad=3.0)\n# Plotting for smokers with bmi over 30\nsns.distplot(df[(df['smoker'] == 'yes') & (df['bmi'] > 30)][\"charges\"], ax=ax[0], hist=False,\n             label='bmi > 30').set_title('smokers')\n# Plotting for smokers with bmi under 30\nsns.distplot(df[(df['smoker'] == 'yes') & (df['bmi'] < 30)][\"charges\"], ax=ax[0], hist=False, label='bmi < 30')\n# Plotting for non smokers\nsns.distplot(df[(df['smoker'] == 'no')][\"charges\"], ax=ax[1], hist=False).set_title('non-smokers')\nax[0].legend()\nplt.show()","88250ebf":"# Creates a new categorical feature for bmi (30 is the cuttoff point for obese)\ndf['bmi_cat'] = np.nan\ndf.loc[df['bmi'] <= 30, 'bmi_cat'] = 0\ndf.loc[df['bmi'] > 30, 'bmi_cat'] = 1\n\nsns.lmplot(data=df[df['smoker'] == 'yes'], x='bmi', y='charges', hue='bmi_cat')\nplt.show()\nsns.lmplot(data=df[df['smoker'] == 'no'], x='bmi', y='charges', hue='bmi_cat')\nplt.show()","8ea70381":"sns.lmplot(data=df[df['smoker'] == 'no'], x='age', y='charges', hue='sex', fit_reg=False)\nplt.show()\nsns.lmplot(data=df[df['smoker'] == 'no'], x='age', y='charges', hue='region', fit_reg=False)\nplt.show()\nsns.lmplot(data=df[df['smoker'] == 'no'], x='age', y='charges', hue='children', fit_reg=False)\nplt.show()","3ce7e6ad":"# Showing the correlation for various subgroups within the data\nprint('bmi correlation (smokers) - ', df[df['smoker'] == 'yes']['charges'].corr(df['bmi']), '\\n')\nprint('bmi correlation (smokers & bmi > 30) - ', \n      df[(df['smoker'] == 'yes') & (df['bmi'] > 30)][\"charges\"].corr(df['bmi']))\nprint('bmi correlation (smokers & bmi < 30) - ', \n      df[(df['smoker'] == 'yes') & (df['bmi'] > 30)]['charges'].corr(df['bmi']), '\\n')\nprint('bmi correlation (non-smokers) - ', df[(df['smoker'] == 'no')]['charges'].corr(df['bmi']))\nprint('age correlation (smokers) - ', df[df['smoker'] == 'yes']['charges'].corr(df['age']))\nprint('age correlation (non-smokers) - ', df[(df['smoker'] == 'no')]['charges'].corr(df['age']))","e1710966":"# Replacing yes\/no with 1\/0 to aid in modelling\ndf['smoker'].replace(('yes', 'no'), (1, 0), inplace=True)\ndf['sex'].replace(('male', 'female'), (1, 0), inplace=True)\n\n# Converts categorical data with more than two categories into a set of new features\n# one for each different value in the old feature and only 1\/0 as possible values\ndummies = pd.get_dummies(df['region'])\ndf = pd.concat((df, dummies), axis = 1)\ndf = df.drop('region', axis = 1)\n\n# Creating a Pearson correlation matrix to visualise correlations between features\ndf_corr = df.drop(['log_charges', 'bmi_cat'], axis = 1)\ncorr = df_corr.corr()\nfig, ax = plt.subplots(dpi=200)\nsns.heatmap(corr, cmap = 'Wistia', annot= True, ax=ax, annot_kws={\"size\": 6})\nplt.show()","ea7662cf":"# A new df where we split up the number of children into new features to check for correlations\ndf_children = df[['charges', 'children']]\n\ndummies_children = pd.get_dummies(df_children['children'])\ndf_children = pd.concat((df_children, dummies_children), axis = 1)\ndf_children = df_children.drop('children', axis = 1)\n\nprint('0 children - ', df_children['charges'].corr(df_children[0]))\nprint('1 children - ', df_children['charges'].corr(df_children[1]))\nprint('2 children - ', df_children['charges'].corr(df_children[2]))\nprint('3 children - ', df_children['charges'].corr(df_children[3]))\nprint('4 children - ', df_children['charges'].corr(df_children[4]))\nprint('5 children - ', df_children['charges'].corr(df_children[5]))","f477fe58":"def linear_func(X, th0, th1):\n    return (X * th1) + th0\n\ndef cost_func(X, y, th0, th1):\n    mm = len(X)\n    return (1\/2*mm) * np.sum((linear_func(X, th0, th1) - y)**2)\n\n# The partial derivative of the cost function with respect to theta 0\ndef grad_descent_th0(X, y, alpha, th0, th1):\n    mm = len(X)\n    return (alpha \/ mm) * np.sum(linear_func(X, th0, th1) - y)\n\n# The partial derivative of the cost function with respect to theta 1\ndef grad_descent_th1(X, y, alpha, th0, th1):\n    mm = len(X)\n    return (alpha \/ mm) * np.sum((linear_func(X, th0, th1) - y)*X)","75850582":"# Creating simple dummy values to fit a linear model to\nX = np.array([1,2,3,4])\ny = np.array([2,4,6,8])\n\nplt.scatter(X, y)\nplt.show()\n\n# these are our inital guesses for the values of theta0 and theta1 (they do not have to be good guesses)\nth0 = -15\nth1 = 16\n# The optimization parameter controls how quickly we descend the gradient\nalpha = 0.1\nloops = 1000\n\nfor ii in range(loops):\n    # We must store the values in temporary holders so we can use the same values for both gradiant descent parts\n    temp_th0 = th0 - grad_descent_th0(X, y, alpha, th0, th1)\n    temp_th1 = th1 - grad_descent_th1(X, y, alpha, th0, th1)\n    th0 = temp_th0\n    th1 = temp_th1\n    \nprint('theta0 - ', th0)\nprint('theta1 - ', th1)\nprint('final cost - ', cost_func(X, y, th0, th1))\n\n# Plotting the old data plus our predictions\nplt.plot(X, linear_func(X, th0, th1), label='calculated')\nplt.scatter(X, y, label='original data')\nplt.legend()\nplt.show()","04e97324":"X = df[df['smoker'] == 0]['age'].values\ny = df[df['smoker'] == 0]['charges'].values\nprint(df.head())\nprint(X[:5])\nprint(y[:5])","af582bf8":"# Our chosen data set\nplt.scatter(X, y)\nplt.show()\n\nth0 = -15\nth1 = 16\nalpha = 0.001\nloops = 100000\nprint('initial cost - ', cost_func(X, y, th0, th1))\n\nfor ii in range(loops):\n    temp_th0 = th0 - grad_descent_th0(X, y, alpha, th0, th1)\n    temp_th1 = th1 - grad_descent_th1(X, y, alpha, th0, th1)\n    th0 = temp_th0\n    th1 = temp_th1\n    \nprint('theta0 - ', th0)\nprint('theta1 - ', th1)\nprint('final cost - ', cost_func(X, y, th0, th1))\n\nplt.plot(X, linear_func(X, th0, th1), label='calculated', c='r', linewidth=4)\nplt.scatter(X, y, label='original data', c='b')\nplt.legend()\nplt.show()","dc35fd73":"# The number of features and instances for our dummy data\ninstances = 50\nfeatures = 3\n\n# Creating random values for our target weights\nthetai = np.random.randint(1, 9, features+1)\nprint(thetai)\n\n# Creating the input data randomly\ndf_inputs = pd.DataFrame()\n# The bias 'feature' should all be ones so we add the same bias for each instance\ndf_inputs['bias'] = [1] * instances\nfor ii in range(features):\n    df_inputs['feat_'+str(ii)] = np.random.randint(1,9,instances)\n\n# creating an empty data series for our taget values\ndf_targets = pd.DataFrame()\ndf_targets['target'] = [0] * instances\n\n# Creating the targets from the random weights and feature values so we have a good fit to aim for\nfeat_bias = df_inputs.columns.tolist()\nfor ii, col in enumerate(feat_bias):\n    df_targets['target'] += (df_inputs[col] * thetai[ii])\n\n# requires .float() to match the weight tensor created below\ntargets = torch.tensor(df_targets.values).float()\ninputs = torch.tensor(df_inputs.values).float()\n\nprint(df_inputs.head())\nprint(df_targets.head())","b67a38d9":"def linear_func_mv(x, weights):\n    return  x @ weights.t()\n\ndef cost_func_mv(X, y, weights):\n    mm = len(X)\n    return (1\/2*mm) * torch.sum((linear_func_mv(X, weights) - y)**2)","d5bd93c2":"def mv_linear_regression_alg(inputs, targets, loops, alpha):\n\n    # require_grad = True in order to backwards compute derivatives of the weights\n    # number of weight coefficients equal to the number of features\n    w = torch.randn(1, inputs.shape[1], requires_grad=True)\n\n    print('initial cost - ', cost_func_mv(inputs, targets, w))\n    print('Improving parameters...')\n\n    cost_tracker = []\n\n    t_mv1 = time.time()\n    for i in range(loops):\n        # Calculate the cost each time to analyse the current paramters\n        cost = cost_func_mv(inputs, targets, w)\n        # Adds the cost to a list to we ca see it change over time\n        cost_tracker.append(cost_func_mv(inputs, targets, w).detach().numpy())\n        # Calculating the partial derivatives\n        cost.backward()\n        # stop autograd from tracking history on Tensor\n        with torch.no_grad():\n            w -= w.grad * alpha\n            # Resetting calculated derivatives to 0\n            w.grad.zero_()\n    t_mv2 = time.time()\n\n    print(f'...time taken is {t_mv2 - t_mv1} s to complete {loops} loops')\n    print('final cost - ', cost_func_mv(inputs, targets, w))\n\n    return w, cost_tracker\n\nalpha_mv = 1e-5\nloops_mv = 3000\nw, costs = mv_linear_regression_alg(inputs, targets, loops_mv, alpha_mv)\n\nplt.plot(costs)\nplt.title('The cost converging to its minimum')\nplt.xlabel('Iteration number')\nplt.ylabel('Cost')\nplt.show()\n\nprint('\\nRESULTS\\n')\n# Getting our calculated values for target from our calculated parameters\ndf_targets['results'] = linear_func_mv(inputs, w).t().detach().numpy()[0]\nprint('Pearson R - ', df_targets['target'].corr(df_targets['results']))\nprint('weights - ', thetai)\nprint('calculated weights - ', w)\n\nplt.scatter(df_targets['target'].values, df_targets['results'].values)\nplt.title('Correlation between calculated and target values')\nplt.xlabel('Target values')\nplt.ylabel('Calculated values')\nplt.show()","84a9f395":"df = pd.read_csv('\/kaggle\/input\/insurance\/insurance.csv')\n\ninstances_tot = df.shape[0]\n\n# Splitting our dataset into a test and train to test our model and avoid over parameterisation\ndf_train = df[:int((instances_tot * 4)\/5)]\ndf_test = df[int((instances_tot * 4)\/5):]\n\n# Inputting the features we want for our model\ndf_edit = pd.DataFrame()\nbias = [1] * int((instances_tot * 4)\/5)\ndf_edit['bias'] = bias\ndf_edit['smoker'] = df_train['smoker']\ndf_edit['bmi'] = df_train['bmi']\ndf_edit['age'] = df_train['age']\ndf_edit['age^2'] = df_train['age'] ** 2\ndf_edit['log_charges'] = np.log(df_train['charges'])\n\ndf_edit['bmi_cat'] = np.nan\ndf_edit.loc[df_edit['bmi'] <= 30, 'bmi_cat'] = 0\ndf_edit.loc[df_edit['bmi'] > 30, 'bmi_cat'] = 1\n\ndf_edit['smoker'].replace(('yes', 'no'), (1, 0), inplace=True)\ndf_edit['age'] = df_edit['age'] \/ (df_edit['age'].max() - df_edit['age'].min())\ndf_edit['age^2'] = df_edit['age^2'] \/ (df_edit['age^2'].max() - df_edit['age^2'].min())\ndf_edit['bmi'] = df_edit['bmi'] \/ (df_edit['bmi'].max() - df_edit['bmi'].min())\n\n# splitting the features and the target ot create our inputs\ndf_inputs = df_edit.drop('log_charges', axis=1)\ndf_targets = pd.DataFrame()\ndf_targets['targets'] = df_edit['log_charges']\n\nprint(df_inputs.head())\nprint(df_targets.head())","507e9497":"# Our inputs must be torch tensors\ninputs = torch.tensor(df_inputs.values).float()\ntargets = torch.tensor(df_targets.values).float()\nloops = 10000\nalpha = 3e-7\n# Calculating our parameters\nw, costs = mv_linear_regression_alg(inputs, targets, loops, alpha)\n\nprint('\\nRESULTS\\n')\nplt.plot(costs)\nplt.title('The cost converging to its minimum')\nplt.xlabel('Iteration number')\nplt.ylabel('Cost')\nplt.ylim(0, 5e5)\nplt.show()\nprint(df_inputs.columns.tolist())\nprint(w)\n# Calculating our values for the target from our calculated parameters\ndf_targets['results'] = linear_func_mv(inputs, w).detach().numpy()\nprint(df_targets.head())\nprint(df_targets['targets'].corr(df_targets['results']))\n\nplt.scatter(df_targets['targets'].values, df_targets['results'].values)\nplt.title('Correlation between calculated and target values')\nplt.xlabel('Target values')\nplt.ylabel('Calculated values')\nplt.show()","4f06c862":"# Creating our smokers group and forming inputs and targets\ndf_smokers_i = df_edit[df_edit['smoker'] == 1]\ndf_smokers_t = pd.DataFrame()\ndf_smokers_t['targets'] = df_smokers_i['log_charges']\ndf_smokers_i = df_smokers_i.drop(['log_charges', 'smoker'], axis=1)\n\nprint('\\nSmokers gradient descent\\n')\ninputs_s = torch.tensor(df_smokers_i.values).float()\ntargets_s = torch.tensor(df_smokers_t.values).float()\nloops = 10000\nalpha = 3e-7\nw_s, costs_s = mv_linear_regression_alg(inputs_s, targets_s, loops, alpha)\n\nplt.plot(costs_s)\nplt.title('The cost converging to its minimum')\nplt.xlabel('Iteration number')\nplt.ylabel('Cost')\nplt.ylim(0, 5e5)\nplt.show()\n\n# Creating our non smokers group and forming inputs and targets\ndf_nonsmokers_i = df_edit[df_edit['smoker'] == 0]\ndf_nonsmokers_t = pd.DataFrame()\ndf_nonsmokers_t['targets'] = df_nonsmokers_i['log_charges']\ndf_nonsmokers_i = df_nonsmokers_i.drop(['log_charges', 'smoker'], axis=1)\n\nprint('\\nNon-smokers gradient descent\\n')\ninputs_ns = torch.tensor(df_nonsmokers_i.values).float()\ntargets_ns = torch.tensor(df_nonsmokers_t.values).float()\nloops = 10000\nalpha = 3e-7\nw_ns, costs_ns = mv_linear_regression_alg(inputs_ns, targets_ns, loops, alpha)\n\nplt.plot(costs_ns)\nplt.title('The cost converging to its minimum')\nplt.xlabel('Iteration number')\nplt.ylabel('Cost')\nplt.ylim(0, 5e5)\nplt.show()","647bf046":"print('\\nSMOKER RESULTS\\n')\n# looking at the names of the features with the calculated parameters for them\nprint(df_smokers_i.columns.tolist())\nprint(w_s)\ndf_smokers_t['results'] = linear_func_mv(inputs_s, w_s).detach().numpy()\n\nprint('\\nNON SMOKER RESULTS\\n')\nprint(df_nonsmokers_i.columns.tolist())\nprint(w_ns)\ndf_nonsmokers_t['results'] = linear_func_mv(inputs_ns, w_ns).detach().numpy()","866261f6":"# Combining our final results for smokers and non smokers\ndf_final_i = pd.concat([df_nonsmokers_i, df_smokers_i])\ndf_final_i = df_final_i.sort_index()\ndf_final_t = pd.concat([df_nonsmokers_t, df_smokers_t])\ndf_final_t = df_final_t.sort_index()\nprint(df_final_t.head())\nprint(df_final_t['targets'].corr(df_final_t['results']))\n\nplt.scatter(df_final_t['targets'].values, df_final_t['results'].values)\nplt.title('Correlation between calculated and target values')\nplt.xlabel('Target values')\nplt.ylabel('Calculated values')\nplt.show()","2abdbef1":"# Separating the inputs and the target data in our test group\ndf_targets_test = pd.DataFrame()\ndf_targets_test['targets'] = df_test['charges']\n\ndf_inputs_test = df_test.drop('charges', axis=1)\n\nprint(df_inputs_test.head())\nprint(df_targets_test.head())","a3838ae8":"def predict_changes(dfin, dfout):\n    \n    # The old indexing creates problems when operating across different dataframes\n    dfin = dfin.reset_index(drop=True)\n    dfout = dfout.reset_index(drop=True)\n    \n    # We dont want the smoker feature in our inputs but we still need to to treat our two groups separately\n    dfout['smoker'] = dfin['smoker']\n    \n    # Creating our inputs in the same way as we did to train the model\n    dfin_edit = pd.DataFrame()\n    dfin_edit['bias'] = [1] * dfin.shape[0]\n    \n    dfin_edit['bmi'] = dfin['bmi']\n    dfin_edit['age'] = dfin['age']\n    dfin_edit['age^2'] = dfin['age'] ** 2\n    dfin_edit['bmi_cat'] = np.nan\n    dfin_edit.loc[dfin_edit['bmi'] <= 30, 'bmi_cat'] = 0\n    dfin_edit.loc[dfin_edit['bmi'] > 30, 'bmi_cat'] = 1\n\n    dfin_edit['age'] = dfin_edit['age'] \/ (dfin_edit['age'].max() - dfin_edit['age'].min())\n    dfin_edit['age^2'] = dfin_edit['age^2'] \/ (dfin_edit['age^2'].max() - dfin_edit['age^2'].min())\n    dfin_edit['bmi'] = dfin_edit['bmi'] \/ (dfin_edit['bmi'].max() - dfin_edit['bmi'].min())\n    \n    # Go through each row, check if they are smoker, and apply the correct parameters accordingly\n    dfout['results_log'] = np.nan\n    for index, row in dfin_edit.iterrows():\n        if dfout.loc[index, 'smoker'] == 'yes':\n            xx = torch.tensor(row.values).float()\n            dfout.loc[index, 'results_log'] = (xx @ w_s.t()).detach().numpy()[0]\n        if dfout.loc[index, 'smoker'] == 'no':\n            xx = torch.tensor(row.values).float()\n            dfout.loc[index, 'results_log'] = (xx @ w_ns.t()).detach().numpy()[0]\n\n    # Our model predicts log(charges). We want to see what the actual charges are\n    dfout['results'] = math.e ** dfout['results_log']\n    \n    print(dfout['targets'].corr(dfout['results']))\n    plt.scatter(dfout['targets'].values, dfout['results'].values)\n    plt.title('Correlation between calculated and target values')\n    plt.xlabel('Target values')\n    plt.ylabel('Calculated values')\n    plt.show()\n    \n    # returns the predicted charges as a list (not part of a dataframe)\n    return dfout['results'].values\n\n\ndf_test['predicted_charges'] = predict_changes(df_inputs_test, df_targets_test)\n\nprint(df_test.head())","d9d96c65":"With the categorical data, it is clear the biggest correlation to charges is with smoking. There appears to be some correlation with sex and region but not large. The number of children seems to have some effect on the charges but I am not sure how significant it is. Firstly the number of instances with 4 or 5 children is very low so I would not trust that data. Secondly, the 0 children category does not seem to fit an overall trend. I would therefore split this category up for a linear regresssion analysis into its contituent categories, rather than leave it as an ordinal feature (0-5).","1ccbdf35":"It seems important to split the smokers and non-smokers into different groups to fit a line trhough them. The comparison to the above lmplot with them combined shows a completely different gradient of the line of best fit. We can see that actually having a bmi of over 30 doesnt change the gradiant but the bias is significantly increased. \n\nFinally, we confirm that there is no split of the non-smokers along the bmi = 30 line.","9dc25697":"Here we set up the health insurance data ready for linear regression.\n\n- The first 'feature' is the bias. Any features which are only 0 or 1 act as a sort of bias only applied to the instances with a 1. The bias will then be whatever weight corresponds to that features. Our 'bias' feature is 1 for every instance and so it acts like a lypical bias.\n- The second feature is 'smoker' which is either 0 or 1. This assumes there is a bias which is added for being a smoker.\n- 'bmi' and 'age' are both continuous features. These have been scaled to be be at a similar range to the categorical features) by dividing by the range.\n- age$^{2}$ is included as a feature as there appears to be some exponential component to the age component to cost. This can be treated as just another linear feature and is again scaled down to the a similar range.\n- The final feature is bmi_cat which is whether the bmi is over 30 or not. We saw earlier that, at least for smokers, these two groups seem to be treated differently.\n- 'sex', 'region' and number of children have all been omitted as there is very little correlation to charges and in some cases there is a correlation with our chosen features.\n\nThe target has been changed to $\\log_{e}{charges}$ to give it a more normal distribution.","518924f7":"A similar process is done here as when our data was first prepared for liner regression. We should have the input data in the same form so that the weights correspond to the appropriate features.\n\nOur predicted values match the target values very well showing we have created a good model for predicting health insurance charges.","71a9a807":"There are some slight correlations between the number of children and charges but no overall trend so we were right to split this feature up.\n\n## Univariate linear regression from scratch\n\nNow I will demonstrate linear regression with just one variable from scratch. In this case we will choose a feature that we know has good correlation with charges. We will therefore choose age for non-smokers.\n\nRegressors are a group of machine learning algorithms which can predict a **continuous value** from a set of **features** for a given **instance**. \n\n- Continious value - if the value can be any value (rather than a set of values).\n- Feature - a property of something (height, count, colour etc).\n- Instance - a single example of something being analysed.\n\nA univariate linear regressor predicts a value of something from a single features and assumes the relationship between the feature and the predicted value is linear. It does this by calculating the gradient and the y-intercept of a line of best fit through a scatter plot of the feature and the value of interest. One can then predict an answer for any value of the feature.","01c75214":"# The aim of this notebook is to do an exploratory data analysis of health insurance data, and then use a linear regression model to be able to predict health insurance charges.\n\n* An exploratory data analysis\n* Univariate linear regression from scratch\n* Multivariate linear regression from scratch\n\n## The exploratory data analysis","bf9646ed":"The linear_func_mv is our new model for a multivariate linear regression. In this case the matrix of input features are multiplied by the transpose of the weights vector. This now includes the bias as a sort of dummy feature which always equals 1. In this way the bias can be optimised with the weights. The cost function is the same but now takes the weight vector as an argument in the calculated y-value.","09a24566":"We will now look at our test dataframe to give an indication on how we would use our model to predict charges. The data would be given in this form so we will create a function which gives us the appropriate charges from this.","3aea6381":"We can see the results of treating these two groups separately. The smoker bias is now shown in the different overall bias for each. The correlation with bmi is completely different with non-smokers having almost no correlatin with bmi. There also appears to be a different correlation with age across the two groups with there being more exponential character in the smokers. This makes sense as smoking related health issues are much more likely to appear in older age. We also see a small bmi>30 bias in the smoker category as we expected.","f80a48b9":"The cost function rapidly falls but still needs a lot more iterations to reach a sufficiently low cost to match the input parameters. When plotting the calculated values against the target ones, if they all match we should get a diagonal line of data points (at $y=1x+0$).","32fe42ab":"The distribution of the charges shows a the data is positively skewed. When the log is taken, the data looks much better. The presence of peaks in the distribution suggests to me there are groups within the instances.","c8eed165":"With this knowledge in mind we can now show the distribution of charges but split it into our three easily defined groups: non-smokers, smokers with bmi > 30 and smokers with bmi <30. As predicted there is a tail on some of the peak which we can see on the above scatter plots as (outlier) although it is very consistent so these will be for some medical reason. Interestingly, the same tail does not exist in the smokers with bmi < 30.\n\nWe will inesitigate whether one of the other features can account for these instances later.","85f3194b":"In order to improve on our line parameters, we can take the partial derivative of the cost function and use it to make a new guess for the parameters. \n$$\\theta_{0}' = \\theta_{0} - \\alpha \\frac{\\partial J}{\\partial \\theta_{0}}$$\n$$\\theta_{1}' = \\theta_{1} - \\alpha \\frac{\\partial J}{\\partial \\theta_{1}}$$\nThe optimization parameter $\\alpha$ allows us to control the speed of descent. We then just plug our new values in and repeat the process of optimization untill a satisfactory point is reached. This could be when the partial derivative is 0, when the cost function cannot be reduced significantly further or due to time restraints. We pick the number of loops to get us to this end point.","d46681fe":"The data given is quite a small data-set with only 1338 people. There are a set of features describing the age, sex, bmi and reigion for each instance, as well as information on the number of children and whether they are a smoker or not. There are four categories in the region: southeast, southwest, northeast and northwest. The data is completely clean with no typos or missing data.\n\nThis is all compared against the charges on the health insurance.","cb8a7710":"There are no large correlations with charges other than bmi, age and obviously smoker. Here we are also looking for correlations between the features. We see a correlation between people living in the southeast and bmi which we should look out for when buliding the model.","164190c5":"The equation for the line of best fit (our model) is simply $$h_{\\theta}(x) = \\theta_{1}x + \\theta_{0}$$\nWe start with guesses for the $\\theta$ parameters and calculate a random line. This obviously will not be a good fit to the data but we need to quantify this so we can improve upon it. To do this we use the cost function which evaluates the quality of the fit $$J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x) - y)^{2}$$\nIf the fit of the model is perfect, $h_{\\theta}(x)$ will be the same as $y$ and so $J(\\theta)$ (the cost) will be $0$. Any deviation from this perfect fit will increase the cost as the difference between the calculated and actual y value is squared. If we take the partial derivative of the cost function with respect to each parameter at a time.\n$$\\frac{\\partial J}{\\partial \\theta_{0}} = \\frac{1}{m}\\sum_{i=1}^{m}((\\theta_{1}x + \\theta_{0}) - y)^{2}$$ \n$$\\frac{\\partial J}{\\partial \\theta_{1}} = \\frac{1}{m}\\sum_{i=1}^{m}((\\theta_{1}x + \\theta_{0}) - y)^{2}x$$\nWe find the slope of the cost function for any set of values of the parameters. Our aim is to change the parameters to reduce this slope to 0 (the minima) and in so doing, minimize $J(\\theta).","7cc3c9cd":"This fit seemds very good with a Pearson correlation factor of 0.87. The cost function runs down to its minimum consistently showing the gradient descent has worked properly. Our calculated values match the target values reasonably well but there seems to be a couple of trends not quite captured by our model as the error seems systematic. There is also a second group of instances which don't seem to be captured anywhere near as well. \n\nMy theory here is that the smokers and non smokers are treated very differently by the insurance company and there is more than a simple bias between them. For instance, we know that for smokers there is a increase in charge at bmi over 30 but this is not the case for non-smokers. I will therefore split smokers and non smokers, treat them differently and combine them at the end.","6fb4c53b":"Both smokers and non-smokers converge nicely with a gradient descent which always decreases. The final cost for smokers is very low showing we have a very good model for this category. For non-smokers it is not quite so low but is still much lower than the final cost when both categories are treated the same.","c8c75662":"To work with the health insurance data we have I have chosen a group of the data with a fairly good correlation and without obvious subgroups.","90e574d8":"On this much larger and more varied dataset we still get a good fit to the data. It requires a much smaller learning rate and many more loops.\n\n## Multivariate linear regression from scratch\n\nWe now go back to our full data set to use all of these features to create a linear regression model. Starting with some dummy data.","972b2a27":"We now see the results of our new model. The line is much more accurately up the middle with some obvious subgroups at the higher charge range. However these have now been treated appropriately as they are falling on the diagonal line. There are still some values which have a much lower calculated value than the target. Based on the cost functions these are probably from the non-smokers primarily and are likely the outliers that we saw before. I could not find anything in the data given to explain these higher charges and so its not surprising that the model does not accurately predict them.","199313c4":"We can now see correlations for our groups. As indicated earlier, the correlation between bmi and charges for smokers is probably not so large but just has a shift upwards at bmi=30. Interestingly we see almost no correlation between charges and bmi for non-smokers.","4825b928":"We use torch for its ability to treat our parameters, inputs and targets as tensors. Using torch we can compute gradients of the loss function for each partial derivative on the fly.","733a3b56":"There is a significant correlation between both bmi and age with charges. I am slightly surprised at how low the correlation is though, especially with bmi which I would have thought would have a significant effect on health. I will look into why this is later.","0c9c304b":"The scatter plot of age and charges shows there is an obvious distinction with how the smoking and non-smoking instances are treated. The smokers appear to be split again into another two groups while the non-smoking is probably all one group but with a set of outliers. These will probably be due to some other health concerns and is probably mirrored in in the smoking group; you can see the set of outliers at the top of the plot.\n\nThe scatter plot of bmi against charges shows this distinction between smokers and non-smokers again. Here though we see how the smoker group is subdivided: those with below and those with above a bmi of 30. There does not appear to be the same distinction for non-smokers. We cannot see from these plots what would cause the particularly high charges in some instances.","dc5d8fd2":"Here we are looking for a reason for the unusually high charges on some instances. We have taken the non-smokers but can find no correlation between any of our remaining features. This suggests a medical reason which is not in our data. We therefor will not be able to model this and it will certainly cause some error in our final model. However, the number of instances and the spread is far to consistent to assume these are outliers and remove them.\n\nWe can also see some exponential behaviour. This will need to be included when making a predictive model."}}