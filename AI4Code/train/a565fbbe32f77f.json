{"cell_type":{"6c60e075":"code","453aabe7":"code","6de87877":"code","746c5251":"code","e96a71c0":"code","83be2e30":"code","3b5679d4":"code","bd6603e0":"code","8a74899a":"code","b2d61200":"code","8fe6bbc7":"code","bee223b1":"code","6e905f15":"code","f83f6b46":"code","871b025e":"code","1f9df347":"code","7d2c25dc":"code","f4196351":"code","f2502e60":"code","63c2b274":"code","330bdf89":"code","642ccf3b":"code","65af00a6":"code","661d8eb5":"code","9b2911f6":"code","a369c383":"code","b0ffa3cf":"code","981b08ac":"code","b4c00bf6":"code","7e97b480":"code","fe6c5be3":"code","2b20c44f":"code","811eac4d":"code","ab24ee81":"code","ef8b8a41":"code","37f52546":"code","eb402220":"code","95986519":"code","53dd9ec2":"code","68689658":"code","ec0f8aca":"code","809fbb8c":"code","2c606bf4":"code","11f91098":"code","3455c4c7":"code","3f1adf62":"code","99e1b1d0":"code","7a57b16a":"code","e8ccff94":"code","919e3715":"code","aeeca2a4":"code","58a39a92":"code","92cdcde8":"code","2af25128":"code","aeb29620":"code","21bfc4af":"code","d0ad14df":"code","66c43b14":"code","3f109256":"code","9ddb1923":"code","a7c28708":"code","c3408c9f":"code","41abdf37":"code","737c1405":"code","3da6f54d":"code","b93700b5":"code","62bc8043":"code","03813fff":"code","5a438e0e":"code","61eb03fa":"code","e42e38e4":"code","e3d00ad7":"code","bfaa7c8b":"code","1c248fb7":"code","7b605199":"code","076fcf5a":"code","ab3dc076":"code","7ffc95b6":"code","f6f47cab":"code","8745f826":"code","d28c0819":"code","f77a876b":"code","f97fa143":"code","facd0dc4":"code","f0fb89f6":"code","284e72e6":"code","c020a035":"code","734e9eee":"code","1236d643":"code","5971f21e":"code","348a32cd":"code","e1effee5":"code","fb59b51d":"code","ba45955b":"code","54739aa1":"code","48c547c7":"code","f95cc286":"code","8adbd174":"code","d111fd1b":"code","24190dfc":"code","00d6a231":"code","e32d701c":"code","742bcede":"markdown","e8f86982":"markdown","8e4844c8":"markdown","f420de3c":"markdown","37dac738":"markdown","5f656a8b":"markdown","75e3ec01":"markdown","4d8609cc":"markdown","b660bdac":"markdown","29c1799f":"markdown","0987e1d2":"markdown"},"source":{"6c60e075":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","453aabe7":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostClassifier, Pool\n#import datawig","6de87877":"df = pd.read_csv('..\/input\/jobathon-analytics-vidhya\/train.csv')\ntest = pd.read_csv('..\/input\/jobathon-analytics-vidhya\/test.csv')\nprint(df.shape,test.shape)","746c5251":"df.head()","e96a71c0":"test.head(1)","83be2e30":"df.info()","3b5679d4":"test.info()","bd6603e0":"#categorizing features\nnum_attribs = ['ID','Region_Code','Upper_Age', 'Lower_Age','Reco_Policy_Premium']\ncat_attribs = ['City_Code','Accomodation_Type','Reco_Insurance_Type',\n               'Is_Spouse','Health Indicator','Holding_Policy_Duration','Holding_Policy_Type','Reco_Policy_Cat',]\ntarget = ['Response']\nlen(df.columns) == len(num_attribs)+len(cat_attribs)+len(target)","8a74899a":"df.describe()","b2d61200":"fig = df[num_attribs].hist(figsize=(20,15))\n# can combine upper age and lower age","8fe6bbc7":"fig = df[['Reco_Policy_Premium']].boxplot(figsize=(8,5))\n#presence of Outliers","bee223b1":"fig = df[['Upper_Age','Lower_Age']].boxplot(figsize=(8,5))","6e905f15":"sns.countplot(x='Response',data=df)\n#Imbalanced dataset","f83f6b46":"df['Response'].value_counts()","871b025e":"num_attribs1 = [i for i in num_attribs if i not in ['Upper_Age', 'Lower_Age']]\nprint(num_attribs)\nprint(num_attribs1)","1f9df347":"# Baseline Model\n\ndef compare_model(data,testing):\n    \n    num_attribs = ['Region_Code','Upper_Age', 'Lower_Age','Reco_Policy_Premium']\n    cat_attribs = ['City_Code','Accomodation_Type','Reco_Insurance_Type',\n               'Is_Spouse','Health Indicator','Holding_Policy_Duration','Holding_Policy_Type','Reco_Policy_Cat']\n    target = ['Response']\n    \n    if testing==1:\n        #testing part\n        new_features_num = []\n        new_features_cat = []\n        drop_features = []\n        num_attribs = [i for i in num_attribs if i not in drop_features]\n        cat_attribs = [i for i in cat_attribs if i not in drop_features]\n        num_attribs = num_attribs + new_features_num\n        cat_attribs = cat_attribs + new_features_cat\n        data = data.drop(drop_features,axis=1)\n    \n    #preprocessing\n    \n    imputer  = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n    data['Health Indicator'] = imputer.fit_transform(data['Health Indicator'].values.reshape(-1,1))\n    data['Holding_Policy_Duration'] = imputer.fit_transform(data['Holding_Policy_Duration'].values.reshape(-1,1))\n    data['Holding_Policy_Type'] = imputer.fit_transform(data['Holding_Policy_Type'].values.reshape(-1,1))\n    \n    \n    print(\"num attribs :{}\".format(num_attribs))\n    print(\"cat attribs :{}\".format(cat_attribs))\n    \n    \n    X=data[num_attribs+cat_attribs]\n    \n    y=data[target]\n    \n    print('Data shape {} X shape {} y shape {} '.format(data.shape,X.shape,y.shape))\n\n    full_pipeline = ColumnTransformer([\n        (\"num\", StandardScaler(), num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs)\n    ],remainder='passthrough')\n    \n\n    X = full_pipeline.fit_transform(X)\n    \n    #Train-Test split\n    print(\"X shape {} and y shape {}\".format(X.shape,y.shape))\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42,stratify=y)\n    print('X_train shape {} X_test shape {} y_train shape {} y_test shape {}'.format(X_train.shape,X_test.shape,y_train.shape,y_test.shape))\n    \n    #Model1\n    dc = DecisionTreeClassifier(random_state=0)\n    dc.fit(X_train,y_train)\n    y_pred_dc = dc.predict(X_test)\n    dc_acc = accuracy_score(y_test, y_pred_dc, normalize=True)\n    roc_auc_dc = roc_auc_score(y_test, dc.predict_proba(X_test)[:, 1])\n    \n    #Model2\n    rf = RandomForestClassifier(random_state=0)\n    rf.fit(X_train,y_train)\n    y_pred_rf = rf.predict(X_test)\n    rf_acc = accuracy_score(y_test, y_pred_rf, normalize=True)\n    roc_auc_rf = roc_auc_score(y_test, rf.predict_proba(X_test)[:,1])\n    \n    #Model3\n    lr = LogisticRegression(random_state=0).fit(X, y)\n    y_pred_lr = lr.predict(X_test)\n    lr_acc = accuracy_score(y_test, y_pred_lr, normalize=True)\n    roc_auc_lr = roc_auc_score(y_test, lr.decision_function(X_test))\n    \n    \n    print('Tree accuracy: {} , Forest accuracy:{} , Log Reg accuracy:{}'.format(round(dc_acc,4),round(rf_acc,4),round(lr_acc,4)))\n    print('Tree roc_auc: {} , Forest roc_auc:{} , Log Reg roc_auc:{}'.format(round(roc_auc_dc,4),round(roc_auc_rf,4),round(roc_auc_lr,4))) ","7d2c25dc":"compare_model(df,0)","f4196351":"cat_attribs","f2502e60":"k=0\nfor i in cat_attribs:\n    print('Column : {}'.format(i))\n    print(df[i].value_counts())\n    print('**'*50)\n    k+=1\nprint(\"Total {} category columns are there\".format(k))\n\n#","63c2b274":"def percent(axs,m):\n    k=0\n    for p in axs.patches:\n        k+=1\n        if k==1:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[0]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==3:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[0]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==2:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[1]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==4:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[1]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n            \n\ndef percent4(axs,m):\n    k=0\n    for p in axs.patches:\n        k+=1\n        if k==1:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[1]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==3:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[3]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==2:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[2]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==4:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[4]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==5:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[1]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==6:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[2]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==7:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[3]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')\n        if k==8:\n            percentage = '{:.1f}%'.format(100 * (p.get_height()\/df[m].value_counts()[4]))\n            x = p.get_x() + p.get_width()\/2\n            y = p.get_height()\/2\n            axs.annotate(percentage, (x, y),ha='center')","330bdf89":"fig,axs = plt.subplots(2,2,figsize=(15,10))\nfig.suptitle('Visualising')\n\nax1 = sns.countplot(ax=axs[0,0],x=\"Accomodation_Type\",hue='Response',data=df)\npercent(ax1,\"Accomodation_Type\")\n\nax2 = sns.countplot(ax=axs[0,1],x=\"Reco_Insurance_Type\",hue='Response', data=df)\npercent(ax2,\"Reco_Insurance_Type\")\n\nax3 = sns.countplot(ax=axs[1,0],x=\"Is_Spouse\",hue='Response', data=df)\npercent(ax3,\"Is_Spouse\")\n\nax4=sns.countplot(ax=axs[1,1],x=\"Holding_Policy_Type\",hue='Response', data=df)\npercent4(ax4,\"Holding_Policy_Type\")\n\n","642ccf3b":"fig,axs = plt.subplots(2,2,figsize=(16,10))\nfig.suptitle('Visualising')\n\nax1 = sns.countplot(ax=axs[0,0],x='City_Code',hue='Response',data=df)\nax1.set_xticklabels(ax1.get_xticklabels(),rotation=90)\nax2 = sns.countplot(ax=axs[0,1],x='Health Indicator',hue='Response', data=df)\nax3 = sns.countplot(ax=axs[1,0],x='Holding_Policy_Duration',hue='Response', data=df)\nax4=sns.countplot(ax=axs[1,1],x='Reco_Policy_Cat',hue='Response', data=df)","65af00a6":"def yn_ratio(data,column):\n    dg = data[[column,'Response']].groupby([column])\n    dg = dg.Response.value_counts().to_frame() \n    \n    C,Yes,No=[],[],[]\n    for i in range(len(dg)):\n        if i%2!=0:\n            Yes.append(dg.iloc[i,0])\n        if i%2==0:\n            No.append(dg.iloc[i,0])\n            C.append(str(dg.index[i][0]))\n            \n    dic = {column:C,'Yes':Yes,'No':No}\n    dgv = pd.DataFrame(dic)\n    dgv['Y\/N'] = dgv['Yes']\/dgv['No']\n \n    plt.figure(figsize=(14,6))\n    ax = sns.barplot(dgv[column],dgv['Y\/N'])\n    plt.xticks(rotation=45)\n    #return dgv","661d8eb5":"yn_ratio(df,'City_Code')","9b2911f6":"yn_ratio(df,'Health Indicator')","a369c383":"yn_ratio(df,'Holding_Policy_Duration')","b0ffa3cf":"yn_ratio(df,'Reco_Policy_Cat')","981b08ac":"df.columns","b4c00bf6":"#Test 1 : Averaging upper_age and lower_age\ndf1 = df.copy()\ndf1['mean_age'] = (df1.Upper_Age + df1.Lower_Age)\/2\ndf1.head(1)\n\n#result : same roc in all, so not advisable","7e97b480":"#Test 2: Changing Region code to category\n\ndf1 = df.copy()\ndf1['reg_code'] = df1['Region_Code'].apply(lambda x : 'A' if x<=1000 else\n                                           ('B' if 1000<x<=2000 else\n                                            ('C' if 2000<x<=3000 else ('D' if 3000<x<=4000 \n                                                                       else ('E' if 4000<x<=5000 else 'F')))))\nsns.countplot(df1['reg_code'],order=['A','B','C','D','E','F'])\n\n#result1 : All 3 roc are lower than baseline, not advisable","fe6c5be3":"#Test 3 : Dropping Region Code\n#result : roc dropped by 2-3%, Not advisable","2b20c44f":"#Test 4 : Difference of Upper and Lower Age\ndf1 = df.copy()\ndf1['mean_age'] = df1.Upper_Age - df1.Lower_Age\ndf1.head(1)\n#result : No such difference, not advisable","811eac4d":"sns.scatterplot(data=df, x='Response', y=\"Reco_Policy_Premium\")","ab24ee81":"#Test 5 : Removing outliers from Reco Policy Premium\ndf1 = df.copy()\ndf1 = df1.drop(df1.loc[df1.Reco_Policy_Premium>32000].index)\n#result : roc improved by 1% , Advisable","ef8b8a41":"#Test 6 : New feature showing 1 if Health indicator =X7 or Reco policy cat = 15 or city_code=c30\ndf1 = df.copy()\nnf = []\nfor i in range(len(df1)):\n    if (df1['Health Indicator'].iloc[i]=='X7' or df1['Reco_Policy_Cat'].iloc[i]== 15 or df1['City_Code'].iloc[i]=='C30'):\n        nf.append(1)\n    else:\n        nf.append(0)\ndf1['nf'] = nf\ndf1.loc[(df1['Health Indicator']=='X7') | (df1['Reco_Policy_Cat']== 15) | (df1['City_Code']=='C30')].head(2)\n\n#result : Same, not advisable","37f52546":"#Test7 : replacing 'Health Indicator','Holding_Policy_Duration','Reco_Policy_Cat' and 'City_Code' by their y\/n percentage\ndf1 = df.copy()\ndgv_cc = yn_ratio(df1,'City_Code').set_index('City_Code').to_dict()\ndgv_hi = yn_ratio(df1,'Health Indicator').set_index('Health Indicator').to_dict()\ndgv_hpd = yn_ratio(df1,'Holding_Policy_Duration').set_index('Holding_Policy_Duration').to_dict()\ndgv_rpc = yn_ratio(df1,'Reco_Policy_Cat').set_index('Reco_Policy_Cat').to_dict()\n\ncc,hi,hpd,rpc = [],[],[],[]\nfor i in range(len(df1)):\n    city = df1.City_Code.iloc[i]\n    yn_cc = (dgv_cc['Y\/N'][city])\n    cc.append(yn_cc)\n    \n    health = df1['Health Indicator'].iloc[i]\n    yn_hi = (dgv_hi['Y\/N'][health])\n    hi.append(yn_hi)\n    \n    hold = df1.Holding_Policy_Duration.iloc[i]\n    yn_hpd = (dgv_hpd['Y\/N'][hold])\n    hpd.append(yn_hpd)\n    \n    reco = df1.Reco_Policy_Cat.iloc[i]\n    yn_rpc = (dgv_rpc['Y\/N'][str(reco)])\n    rpc.append(yn_rpc)\n    \ndf1['cc'] = cc\ndf1['hi'] = hi\ndf1['hpd'] = hpd\ndf1['rpc'] = rpc\n\n#result : roc declined , not advisable","eb402220":"df1.head(2)","95986519":"#Test8 : Imputing missing values by machine learning\ndf1 = df.copy()\npd.isnull(df1).sum().sort_values(ascending=False)\n\n#result : roc increased, advisable","53dd9ec2":"# Imputing Data using DeepLearning by Datawig library\n\ndef impute(data):\n    col = ['Health Indicator','Holding_Policy_Duration','Holding_Policy_Type']\n    for name in col:\n        index_null=[]\n        imputed_list = []\n        completed = []\n        input_col = [ 'City_Code', 'Region_Code', 'Accomodation_Type',\n       'Reco_Insurance_Type', 'Upper_Age', 'Lower_Age', 'Is_Spouse','Reco_Policy_Cat', 'Reco_Policy_Premium'] + completed\n        \n        for i in range(len(data)):\n            if pd.isnull(data[name]).iloc[i]==True:\n                index_null.append(i)\n    \n        index_nn = list(set(data.index)-set(index_null))\n\n        df1_train, df1_test = data.loc[index_nn],data.loc[index_null]\n\n        #Initialize a SimpleImputer model\n        imputer = datawig.SimpleImputer(\n            input_columns= input_col,output_column= name, output_path = 'imputer_model_'+name+'_test')\n\n        #Fit an imputer model on the train data\n        imputer.fit(train_df=df1_train)\n\n        #Impute missing values and return original dataframe with predictions\n        imputed = imputer.predict(df1_test)\n        \n        imputed_list = []\n        for i in range(len(data)):\n            if i%5000==0:\n                print('J'+str(i))\n            if pd.isnull(data[name].iloc[i])==True:\n                idx = data.ID.iloc[i]\n                val = imputed.loc[imputed.ID==idx][name+'_imputed'].iloc[0]\n                imputed_list.append(val)\n            else:\n                val = data[name].iloc[i]\n                imputed_list.append(val)\n        data['imp_'+name] = imputed_list\n\n        completed.append('imp_'+name)\n        print('column {} is imputed with {} imputations'.format(name,len(imputed[name])))\n        \n        \n# Generated three new_features imp_Health Indicator,imp_Holding_Policy_Duration,imp_Holding_Policy_Type which contains the values of \n#'Holding_Policy_Duration', 'Holding_Policy_Type' and 'Health Indicator' alongwith their imputed values. \n#data is stored in ..\/input\/avjobhack\/df1.csv","68689658":"df1 = pd.read_csv('..\/input\/avjobhack\/df1.csv')\npd.isnull(df1).sum().sort_values(ascending=False)","ec0f8aca":"# Test 9 : removing outliers from region_code\ndf1 = df.copy()\nsns.boxplot(df1['Region_Code'])\ndf1 = df1.drop(df1.loc[df1.Region_Code>5800].index)\n\n#result : roc decreased, not advisable","809fbb8c":"test['Holding_Policy_Type'] = np.where(pd.isnull(test['Holding_Policy_Type']),test['Holding_Policy_Type'],test['Holding_Policy_Type'].astype(str))\nimpute(test)\n\n# result is stored in ..\/input\/avhacktest1\/test_fin.csv","2c606bf4":"test = pd.read_csv('..\/input\/avhacktest1\/test_fin.csv')","11f91098":"test.head()","3455c4c7":"train = pd.read_csv('..\/input\/avjobhack\/df1.csv')\ntrain.info()","3f1adf62":"train = train.drop(['Health Indicator', 'Holding_Policy_Duration', 'Holding_Policy_Type'],axis=1)\ntrain = train.drop(train.loc[train.Reco_Policy_Premium>32000].index)\ntrain.info()","99e1b1d0":"def data_prep(data):\n    \n    num_attribs = ['Region_Code','Upper_Age', 'Lower_Age','Reco_Policy_Premium']\n    cat_attribs = ['City_Code','Accomodation_Type','Reco_Insurance_Type',\n               'Is_Spouse','imp_Health Indicator','imp_Holding_Policy_Duration','imp_Holding_Policy_Type','Reco_Policy_Cat',]\n    target = ['Response']\n    \n    print(\"num attribs :{}\".format(num_attribs))\n    print(\"cat attribs :{}\".format(cat_attribs))\n        \n    X=data[num_attribs+cat_attribs]\n    y=data[target]\n    \n    print('Data shape {} X shape {} y shape {} '.format(data.shape,X.shape,y.shape))\n\n    full_pipeline = ColumnTransformer([\n        (\"num\", StandardScaler(), num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs)\n    ],remainder='passthrough')\n    \n    X = full_pipeline.fit_transform(X) \n    \n    return X,y \n\nX_train,y_train = data_prep(train)","7a57b16a":"print(type(X_train.toarray()))\nprint(type(y_train))","e8ccff94":"X_train.shape","919e3715":"#XGBoost\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics     #Additional scklearn functions\nfrom sklearn.model_selection import GridSearchCV #Perforing grid search\n\ntrain \ndef modelfit(alg, X,y,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(X, label=y.values)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(X, y,eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(X)\n    dtrain_predprob = alg.predict_proba(X)[:,1]\n        \n    #Print model report:\n    print(\"\\nModel Report\")\n    print(\"Accuracy : %.4g\" % metrics.accuracy_score(y.values, dtrain_predictions))\n    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, dtrain_predprob))\n      \n    return alg     \n    ","aeeca2a4":"xgb1 = XGBClassifier(\n learning_rate =0.1,\n n_estimators=1000,\n max_depth=5,\n min_child_weight=1,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=4,\n scale_pos_weight=1,\n seed=27)\n\nmodel_xgb1 = modelfit(xgb1,X_train,y_train)","58a39a92":"model_xgb1","92cdcde8":"param_test1 = {\n 'max_depth':range(3,10,2),\n 'min_child_weight':range(1,6,2)\n}\ngsearch1 = GridSearchCV(estimator = model_xgb1, \n param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(X_train,y_train)\n#gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_","2af25128":"gsearch1.best_params_, gsearch1.best_score_","aeb29620":"param_test2 = {\n 'max_depth':[4,5,6],\n 'min_child_weight':[1,2,3,4,5,6,7]\n}\ngsearch2 = GridSearchCV(estimator = model_xgb1, \n param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch2.fit(X_train,y_train)\ngsearch2.best_params_, gsearch2.best_score_","21bfc4af":"model_xgb1","d0ad14df":"param_test3 = {\n 'gamma':[i\/10.0 for i in range(0,5)]\n}\ngsearch3 = GridSearchCV(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=6,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=237, n_jobs=4, nthread=4, num_parallel_tree=1,\n              objective='binary:logistic', random_state=27, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, seed=27, subsample=0.8,\n              tree_method='exact', use_label_encoder=True,\n              validate_parameters=1, verbosity=None), \n param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch3.fit(X_train,y_train)\ngsearch3.best_params_, gsearch3.best_score_","66c43b14":"model_xgb2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.8, gamma=0.2, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=6,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=237, n_jobs=4, nthread=4, num_parallel_tree=1,\n              objective='binary:logistic', random_state=27, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, seed=27, subsample=0.8,\n              tree_method='exact', use_label_encoder=True,\n              validate_parameters=1, verbosity=None)","3f109256":"param_test4 = {\n 'subsample':[i\/10.0 for i in range(6,10)],\n 'colsample_bytree':[i\/10.0 for i in range(6,10)]\n}\ngsearch4 = GridSearchCV(estimator = model_xgb2, \n param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch4.fit(X_train,y_train)\ngsearch4.best_params_, gsearch4.best_score_","9ddb1923":"model_xgb3 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.6, gamma=0.2, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_delta_step=0, max_depth=6,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=237, n_jobs=4, nthread=4, num_parallel_tree=1,\n              objective='binary:logistic', random_state=27, reg_alpha=0,\n              reg_lambda=1, scale_pos_weight=1, seed=27, subsample=0.9,\n              tree_method='exact', use_label_encoder=True,\n              validate_parameters=1, verbosity=None)","a7c28708":"param_test6 = {\n 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n}\ngsearch6 = GridSearchCV(estimator = model_xgb3, \n param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch6.fit(X_train,y_train)\ngsearch6.best_params_, gsearch6.best_score_","c3408c9f":"model_xgb4 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.6, gamma=0.2, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.01, max_delta_step=0, max_depth=6,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=237, n_jobs=4, nthread=4, num_parallel_tree=1,\n              objective='binary:logistic', random_state=27, reg_alpha=1e-05,\n              reg_lambda=1, scale_pos_weight=1, seed=27, subsample=0.9,\n              tree_method='exact', use_label_encoder=True,\n              validate_parameters=1, verbosity=None)","41abdf37":"model_xgb4.fit(X_train,y_train)","737c1405":"def pred(test,model):\n    test = pd.read_csv('test_fin.csv')\n    num_attribs = ['Region_Code','Upper_Age', 'Lower_Age','Reco_Policy_Premium']\n    cat_attribs = ['City_Code','Accomodation_Type','Reco_Insurance_Type',\n               'Is_Spouse','imp_Health Indicator','imp_Holding_Policy_Duration','imp_Holding_Policy_Type','Reco_Policy_Cat']    \n\n        \n    X=test[num_attribs+cat_attribs]\n\n    \n    print('Data shape {} X shape {}  '.format(test.shape,X.shape))\n\n    full_pipeline = ColumnTransformer([\n        (\"num\", StandardScaler(), num_attribs),\n        (\"cat\", OneHotEncoder(), cat_attribs)\n    ],remainder='passthrough')\n    \n    X_test = full_pipeline.fit_transform(X) \n    print(X_test.shape)\n    \n    y_fin = model.predict(X_test)\n    \n    return y_fin","3da6f54d":"y_fin = pred(test,model_xgb4)","b93700b5":"y_fin","62bc8043":"fin = pd.DataFrame(y_fin, columns = ['Response'])\nfin.head()","03813fff":"fin.Response.value_counts()","5a438e0e":"fin['ID'] = test['ID']\nfin.head()","61eb03fa":"fin.to_csv('fin_xgb2.csv',index=False)","e42e38e4":"n_estimators = [100, 300, 500, 800, 1200]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10] ","e3d00ad7":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [50,100, 300, 500, 800, 1200]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10,15] \nmax_features = [2, 3, 4 , 5,6,7,8]\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf, max_features=max_features)\n\nforest = RandomForestClassifier()\n\ngridF = RandomizedSearchCV(forest, hyperF, cv = 3, verbose = 1, n_jobs = -1)\nbestF = gridF.fit(X_train,y_train)","bfaa7c8b":"bestF.best_estimator_","1c248fb7":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [40,45,50,55,60,65]\nmax_depth = [10,13,15,17,19]\nmin_samples_split = [2,3,1,4,5]\nmin_samples_leaf = [3,4, 5,7,9] \nmax_features = [5,6,7,8,9,10,11,12,13]\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf, max_features=max_features)\n\nforest = RandomForestClassifier()\n\ngridF = RandomizedSearchCV(forest, hyperF, cv = 3, verbose = 1, n_jobs = -1)\nbestF = gridF.fit(X_train,y_train)","7b605199":"bestF.best_estimator_","076fcf5a":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [55,60,65]\nmax_depth = [19,22,25]\nmin_samples_split = [5,8,10]\nmin_samples_leaf = [2,4,6] \nmax_features = [13,17,20]\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf, max_features=max_features)\n\nforest = RandomForestClassifier()\n\ngridFin = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, n_jobs = -1)\nbestF = gridFin.fit(X_train,y_train)","ab3dc076":"RF = bestF.best_estimator_","7ffc95b6":"RF.fit(X_train,y_train)","f6f47cab":"y_rf = pred(test,RF)","8745f826":"fin = pd.DataFrame(y_rf, columns = ['Response'])\nfin.head()","d28c0819":"fin['ID'] = test['ID']\nfin.Response.value_counts()","f77a876b":"fin.to_csv('fin_rf.csv',index=False)","f97fa143":"# Credit for below part : shobhit upadhyaya ,Github link : https:\/\/github.com\/asingleneuron","facd0dc4":"df1 = pd.read_csv('..\/input\/avjobhack\/df1.csv')\ntest = pd.read_csv('..\/input\/avhacktest1\/test_fin.csv')\ndf1 = df1.drop(df1.loc[df1.Reco_Policy_Premium>32000].index)","f0fb89f6":"df1['imp_Holding_Policy_Type'] = df1['imp_Holding_Policy_Type'].astype(str)\ntest['imp_Holding_Policy_Type'] = test['imp_Holding_Policy_Type'].astype(str)\n\ndf1['Reco_Policy_Cat'] = df1['Reco_Policy_Cat'].astype(str)\ntest['Reco_Policy_Cat'] = test['Reco_Policy_Cat'].astype(str)\n\ndf1['Region_Code'] = df1['Region_Code'].astype(str)\ntest['Region_Code'] = test['Region_Code'].astype(str)","284e72e6":"cols_to_remove = ['ID','Health Indicator','Holding_Policy_Duration','Holding_Policy_Type']\ntarget = 'Response'\n\n_X = df1.drop(cols_to_remove + [target], axis=1)\ny = df1[target]\n_XTEST = test.drop(cols_to_remove , axis=1)","c020a035":"_X.shape, _XTEST.shape","734e9eee":"X_all = pd.concat([_X, _XTEST]).reset_index(drop=True)\nX_all.shape","1236d643":"X_all.info()","5971f21e":"cat_columns = []\nfor col in X_all.select_dtypes('object').columns:\n    print(col)\n    cat_columns.append(col)\n    le = LabelEncoder()\n    X_all[col] = le.fit_transform(X_all[col])","348a32cd":"X = X_all[:len(y)]\nXTEST = X_all[len(y):]\nX.shape, XTEST.shape","e1effee5":"NUM_OF_BOOST_ROUND = 10000\nEARLY_STOPPING = 300","fb59b51d":"cat_features_index = [i for i,col in enumerate(X.columns) if col in cat_columns]\ncat_features_index","ba45955b":"X.columns","54739aa1":"X_train , X_valid, y_train, y_valid = train_test_split(X,y, \n                                                       test_size=0.2, \n                                                       random_state=56, \n                                                       stratify=y)","48c547c7":"params = {\n    'cat_features': cat_features_index,\n    'eval_metric': 'AUC',\n    'random_seed': 56,\n    'n_estimators': NUM_OF_BOOST_ROUND,\n}","f95cc286":"bst = CatBoostClassifier(**params, early_stopping_rounds=EARLY_STOPPING)\n_ = bst.fit(X_train, y_train, eval_set=(X_valid,y_valid), plot=True, verbose=False)","8adbd174":"ypred_cat = bst.predict_proba(X_valid)[:,1]\nroc_auc_score(y_valid, ypred_cat)","d111fd1b":"f_importance_df = pd.DataFrame(bst.get_feature_importance(), columns=['importance'], index=X_valid.columns)\nf_importance_df = f_importance_df.sort_values(by='importance', ascending=False)","24190dfc":"plt.figure(figsize=(10,5))\nsns.barplot(x=f_importance_df.importance[:500], y=f_importance_df.index[:500]);","00d6a231":"ypred_test = bst.predict_proba(XTEST)[:,1]\npred = pd.DataFrame(ypred_test)\npred.to_csv('pred.csv')","e32d701c":"sns.distplot(ypred_test)","742bcede":"### XGBoost","e8f86982":"## EDA Part1","8e4844c8":"## Data Preparation","f420de3c":"### Random Forest Classifier","37dac738":"A little story before we move on to this part, the Catboost classifier part was added after the competition to the notebook. I tried XGBoost, Random Forest but was stuck in 0.68 ROC-AUC. I even tried in another google colab notebook parallely Light GBM and KerasClassifier(Neural Network) but alas none could cross the 0.70 mark.\n\nBut then today in the competition forum discussion part I saw Shobhit saying a magic word can increase your score to 0.80 and then I applied CatBoost. What to do sometimes, luck is not on your side :)","5f656a8b":"## Feature Enginnering","75e3ec01":"## baseline model","4d8609cc":"#### CatBoost Classifier gives almost 0.80 ROC_AUC_SCORE","b660bdac":"### CatBoost Classifier","29c1799f":"## Modeling","0987e1d2":"## EDA part2"}}