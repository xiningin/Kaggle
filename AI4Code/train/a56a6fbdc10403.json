{"cell_type":{"87c71eb6":"code","2844bec5":"code","5a750232":"code","fca05f56":"code","4e4e5b7e":"code","443aea19":"code","270ee916":"code","91e4263b":"code","4639af61":"code","eb78e273":"code","4c945a68":"code","462237a1":"code","b2d6f5da":"code","de440b40":"code","1de61fa3":"code","bbf529d9":"code","4ae126b4":"code","9bf47577":"markdown","78597122":"markdown"},"source":{"87c71eb6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.covariance import MinCovDet\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2844bec5":"train_df=pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\n#df2=pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntarget_df=pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntest_df =pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_df.head(1)","5a750232":"\"\"\"\nfrom scipy import stats\nnums=[]   \nfor i in range(206):\n    a=target_df2.iloc[:,i]\n    num=0\n    col=[]\n    for j in train_df2:\n        \n        one_data=train_df2.loc[a[a==1].index,j]\n        zero_data=train_df2.loc[a[a==0].index,j]\n        a2=stats.mannwhitneyu(one_data,zero_data, alternative='two-sided')\n        if a2.pvalue <0.05:\n            num+=1\n            col.append(j)\n    if num ==0:\n        nums.append(i)\n    print(i,\"\u306f\u6709\u610f\u5dee\u304c\u751f\u3058\u308b\u8aac\u660e\u5909\u6570\u304c\",num,\"\u500b\u3067\",col)\n    print(\"---next_target------\")\nprint(\"\u6709\u610f\u5dee\u304c\u751f\u3058\u306a\u3044\u76ee\u7684\u5909\u6570\u306f\",nums)\n\"\"\"\n","fca05f56":"tar=target_df.set_index('sig_id')\ntar2=tar.sum()\ntar2.plot(kind=\"hist\")\n#tar2.plot(kind=\"box\")","4e4e5b7e":"#\u30c0\u30a6\u30f3\u30b5\u30f3\u30d7\u30ea\u30f3\u30b0 #df2\u306ftarget\u3067\u306a\u3051\u308c\u3070\u3044\u3051\u3044\u306a\u3044\ndef down_sampling(df1,df2,percent=0.5,kotei=True):\n    d1 = df1.copy()\n    d2 = df2.copy()\n    num = int(len(df1)*percent)\n    if ('sig_id' in d1.columns) or ('sig_id' in d2.columns):\n        d1 = d1.set_index('sig_id')\n        d2 = d2.set_index('sig_id')\n    \n    dd1 = d1.sample(n=num)\n    dd2 = d2.loc[dd1.index]\n    return dd1,dd2","443aea19":"def proceccing2(train3,mcd=None,jogen=None):\n    train4 = train3.copy()\n    if mcd==None:\n        mcd =MinCovDet(support_fraction=0.7)\n        mcd.fit(train4)\n    dis = mcd.mahalanobis(train4)\n    \n    \n    se=pd.Series(dis,index=train4.index)\n    \n    if jogen==None:\n        num =int(len(se)*0.01)\n        #    print(num)\n        iqr = se.quantile(0.75)-se.quantile(0.25)\n        jogen2= se.quantile(0.75) + 1.5*iqr\n        se_idx=se[se>jogen2].index\n        \n    else:\n        se_idx=se[se>jogen].index\n        jogen2=0\n    train4[\"out_flag\"]= 0\n    train4.loc[se_idx,\"out_flag\"]=1\n    return train4,mcd,jogen2","270ee916":"#\u30c0\u30df\u30fc\u5909\u6570\u5316\u3092\u884c\u3044\u3002\u4e3b\u6210\u5206\u5206\u6790\u3067comp\u5206\u306b\u5217\u3092\u7e8f\u3081\u308b\ndef proceccing(d,comp=10,test=False,mcd=None,):\n    df = d.copy()\n    data={\"trt_cp\":0,'ctl_vehicle':1}\n    df['cp_type']=df['cp_type'].map(data)\n    data={\"D1\":0,'D2':1}\n    df['cp_dose']=df['cp_dose'].map(data)\n    #df=df.drop('sig_id',axis=1)\n    pca = PCA(n_components=comp,random_state=0,whiten=False)\n    train2 = df.set_index('sig_id')\n    pca.fit(train2)\n    train3=pd.DataFrame(pca.transform(train2))\n    train3.index= train2.index\n    \n    return train3","91e4263b":"#\u5f15\u6570\u306e\u4ed5\u69d8df1\u3068df2\u306f\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3067\u3064\u306a\u3052\u308b\u4e8b\u304c\u51fa\u6765\u308b\u3002\ndef under_sampling(df1,df2):#df2\u304ctarget\u306e1\u5217\u306e\u30b7\u30ea\u30fc\u30ba\n    d1 = df1.copy()\n    d2 = df2.copy()\n    \n    #positive\u306e\u884c\u3092\u5168\u3066\u629c\u304d\u51fa\u3059\u3002\n    dd2_1 = d2[d2==1]\n    num_1 = len(dd2_1)#1\u306e\u884c\u306e\u7dcf\u6570\n    \n    a = d2[d2==0]\n    num_0 =len(a)#0\u306e\u884c\u306e\u6570\n    if num_1*20 < num_0:\n        num =num_1*20\n    else:\n        num = num_0\n    \n    ##negative\u306e\u884c\u3092\u5168\u3066\u629c\u304d\u51fa\u3059\u3002\n    dd2_0 = d2[d2==0]\n    if num != 0:\n        #0\u306e\u884c\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u629c\u304d\u51fa\u3059\uff08\u4ef6\u6570\u306fnum\u306e10\u500d)\n        dd2_0_2 = dd2_0.sample(random_state=10,n=num)\n    else:\n        #1\n        dd2_0_2 = dd2_0.sample(random_state=10,n=100)\n    \n    #d1\u304b\u3089positive\u5206\u3068\u30cd\u30ac\u30c6\u30a3\u30d6\u5206\u306e\u30c7\u30fc\u30bf\u3092\u629c\u304d\u51fa\u3059\n    d1_1 =d1.loc[dd2_1.index]\n    d1_0 = d1.loc[dd2_0_2.index]\n    \n    \n    d11 = pd.concat([d1_1,d1_0])#\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u7d50\u5408\uff08\u884c\u65b9\u5411\uff09\n    d22 = pd.concat([dd2_1,dd2_0_2])#\u8a13\u7df4\u30c7\u30fc\u30bf\u306e\u7d50\u5408\uff08\u884c\u65b9\u5411\uff09\n    return d11,d22","4639af61":"\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation,Dropout\nfrom keras.optimizers import SGD,Adam\nfrom keras.layers.normalization import BatchNormalization\nfrom sklearn.preprocessing import StandardScaler\n\ntrain_df2 = proceccing(train_df,comp=100)#pca\ntrain_df2,mcd,jogen=proceccing2(train_df2)#\u5916\u308c\u5024\u30d5\u30e9\u30b0\u3092\u7acb\u3066\u308b\ntarget_df2 = target_df.set_index('sig_id')\n    \nsc_x =StandardScaler()\n#sc_t =StandardScaler()\nsc_df2=sc_x.fit_transform(train_df2)\n#sc_target2=sc_t.fit_transform(target_df2)\n\n","eb78e273":"#t=target_df2.iloc[:,i]\nmodel = Sequential()\nmodel.add(Dense(units=64, activation='relu', input_dim=101))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(rate=0.5))\nmodel.add(Dense(units=32, activation='relu', input_dim=64))\nmodel.add(BatchNormalization())\n    #model.add(Dropout(rate=0.5))\nmodel.add(Dense(units=target_df2.shape[1], activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01))\n    #model.fit(train_x, train_t, epochs=50, batch_size=10) # \u30a8\u30dd\u30c3\u30af50, \u30df\u30cb\u30d0\u30c3\u30c1 10\nep=30\nbatch_size=128\nstack =model.fit(sc_df2,target_df2, epochs=ep, batch_size=batch_size,validation_split=0.2,shuffle=True)","4c945a68":"\"\"\"scores=[]\nfor i in range(206):\n    t=target_df2.iloc[:,i]\n    model = Sequential()\n    model.add(Dense(units=64, activation='relu', input_dim=100))\n    model.add(BatchNormalization())\n    model.add(Dropout(rate=0.5))\n    model.add(Dense(units=32, activation='relu', input_dim=64))\n    model.add(BatchNormalization())\n    #model.add(Dropout(rate=0.5))\n    model.add(Dense(units=1, activation=\"sigmoid\"))\n    model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.01))\n    #model.fit(train_x, train_t, epochs=50, batch_size=10) # \u30a8\u30dd\u30c3\u30af50, \u30df\u30cb\u30d0\u30c3\u30c1 10\n    ep=15\n    batch_size=256\n    stack =model.fit(sc_df2,t, epochs=ep, batch_size=batch_size,validation_split=0.2,shuffle=True,verbose=0)\n    scores.append(stack.history['val_loss'][-1])\n    print(stack.history['loss'][-1],stack.history['val_loss'][-1])\n    print(\"next\")\n    # \u30a8\u30dd\u30c3\u30af50, \u30df\u30cb\u30d0\u30c3\u30c1 10\nprint(\"finish\")\n\"\"\"","462237a1":"import matplotlib.pyplot as plt\nx = list(range(ep))\nplt.plot(x[2:], stack.history['loss'][2:], label=\"loss\")\nplt.title(\"loss\")\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.plot(x[2:], stack.history['val_loss'][2:], label=\"val_loss\")\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()","b2d6f5da":"submit = test_df[['sig_id']]\nsubmit = submit.copy()\nli=submit.values.reshape(-1)\ntest_df2=proceccing(test_df,comp=100)\ntest_df2,m,j=proceccing2(test_df2,mcd,jogen)\n\ntest_df2 = sc_x.fit_transform(test_df2)\npred=model.predict(test_df2)\nsubmit2=pd.DataFrame(pred,index=li)\nsubmit2=submit2.reset_index()\nsubmit2.columns=target_df.columns\nsubmit2.head()","de440b40":"\"\"\"\ntrain_df2 = proceccing(train_df,comp=100)\ntarget_df2 = target_df.set_index('sig_id')\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\nscores=[]\nmodels=[]\navgs=[]\ntarget_num = target_df2.shape[1]\nlog_loss=[]\ncol=[\"name\",\"accuracy\",\"train_sum\",\"train_num\",\"precision\",\"recall\",\"f1_score\"]\nfor i in range(target_num):\n    \n    #while True:\n    #train,target=down_sampling(train_df2,target_df2,percent=0.3)\n\n\n    y = target_df2.iloc[:,i].values\n    print(y.shape)\n    x = train_df2.values\n    \n    \n    dtrain = xgb.DMatrix(x, label=y)\n    #dtest = xgb.DMatrix(X_test, label=y_test)\n    # \u5b66\u7fd2\u7528\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\n    xgb_params = {\n        'max_depth': 5,\n        # \u4e8c\u5024\u5206\u985e\u554f\u984c\n        'objective': 'binary:logistic',\n        # \u8a55\u4fa1\u6307\u6a19\n        'eval_metric': 'logloss',\n        \n    }\n    #evallist = [(dvalid, 'eval'), (dtrain, 'train')]\n    # \u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3059\u308b\n    bst = xgb.train(xgb_params,\n                    dtrain,\n                    num_boost_round=10  # \u5b66\u7fd2\u30e9\u30a6\u30f3\u30c9\u6570\u306f\u9069\u5f53\n                    \n                    )\n    \n    #log_loss\u306e\u8a08\u7b97\n    #\uff11\u306e\u78ba\u7387\n    proba = bst.predict(dtrain)\n    t=0\n    for j in range(len(y)):\n        tar = y[j]\n        if tar ==0:\n            p = np.log(1-proba[i])\n        else:\n            p = np.log(proba[i])\n        t += p\n    t = -t\/len(y)\n    log_loss.append(t)\n            \n        \n    \n    \n    \n    #score=[target_df2.columns[i],accuracy_score(y_pred=model.predict(X_test),y_true=y_test),y_train.sum(),len(y_train),score_p,score_r,score_f]\n    #scores.append(score)\n    print(i,\"end_logloss\",t)\n#print(\"total_avg\",np.mean(avgs))\n\"\"\"","1de61fa3":"\"\"\"\n\nimport matplotlib.pyplot as plt\nfor i in result:\n    if i ==\"name\":\n        continue\n    result[i].plot(kind=\"hist\",title=i)\n    plt.show()\n\"\"\"","bbf529d9":"\"\"\"\nsubmit = test_df[['sig_id']]\nsubmit = submit.copy()\ntest_df2=proceccing(test_df)\n#test_df2.head()\nfor i in range(target_df2.shape[1]):\n    try:\n        y1 = models[i].predict_proba(test_df2)\n        y1 = y1[:,1]\n        #xgboost\n        #dtest = xgb.DMatrix(test_df2)\n        #y1 = models[i].predict(dtest)\n\n        #y1 = models[i].predict(test_df2)\n       \n        #y11=list(map(lambda x:round(x[1],5),y1))\n        y111 =list(map(lambda x: 0.0001 if x==0 else x,y1))\n        submit[target_df2.columns[i]]=y111\n    except:\n        print(i)\nsubmit.shape\n\"\"\"","4ae126b4":"submit2.to_csv('submission.csv',index=False)","9bf47577":"#\u3000Xgboost\u306b\u3088\u308b\u89e3","78597122":"\u30e9\u30d9\u30eb\u306e\u6570\u304c\u7d50\u69cb\u307e\u3061\u307e\u3061\u306a\u306e\u3067\u3001\uff13\u3064\u3050\u3089\u3044\u306b\u30b0\u30eb\u30fc\u30d7\u5206\u3051\u3057\u3066\u30e2\u30c7\u30eb\u3092\u4f5c\u3063\u3066\u307f\u308b"}}