{"cell_type":{"911f954b":"code","353e1876":"code","b7bb976c":"code","61a22fa6":"code","0ffd3c18":"code","11319e72":"code","9e60ed75":"code","6f6f66ae":"code","6c04bbf5":"code","5e221989":"code","388f183c":"code","62fc8edf":"code","2f67b952":"code","5b7dacf5":"code","63ec73da":"code","21654002":"code","15bf236b":"code","0d718861":"code","eb279eb6":"code","980b86ec":"code","a7db0107":"code","0dd833c7":"code","7ba18ba3":"code","ff92eab2":"code","5e358320":"code","ed98dd36":"code","77a1a162":"code","7a9009c9":"code","07761a7e":"code","95ee8192":"code","7e1951e8":"code","63f4f774":"code","3f607aa8":"code","1fd8f1d4":"code","4da3967e":"code","aa4ac50e":"code","c4c597a6":"code","f6cf12d4":"code","9cae4e41":"code","1dcfe42c":"code","29249a37":"code","6d61fa7d":"code","5dd1d642":"code","11818c63":"code","7971a29f":"code","190e3339":"code","3df4e7f1":"code","8e03e7f1":"code","74b47c2d":"code","2c2c5866":"code","b4b375ee":"code","f7bbcb36":"code","b5b081f5":"code","52958a2e":"code","a8718491":"code","f8fb9086":"code","748368cf":"code","5736e934":"code","fa5b89b2":"code","5986b80f":"code","1408813d":"code","c18c0079":"markdown","2bb24bda":"markdown","faac01c1":"markdown","e314aea3":"markdown","305918b1":"markdown","8b93563c":"markdown","47e7c592":"markdown","2fb079c1":"markdown","bf493981":"markdown","4cb4e00c":"markdown","debaba60":"markdown","abda777f":"markdown","707619e7":"markdown","b558b6cf":"markdown","5c0aee4b":"markdown","e4bbf883":"markdown","ab29b37b":"markdown","94dfdf0c":"markdown","84096ee5":"markdown","0640ea79":"markdown","cd83b8bf":"markdown","fe7ccef3":"markdown","ed7ee1d5":"markdown","7ea89e16":"markdown","37a7b04b":"markdown","699990c5":"markdown","b831b104":"markdown","8b3b0025":"markdown","7de04948":"markdown","0f1499d7":"markdown","76fc7702":"markdown","eb0da23e":"markdown","e22807b5":"markdown","c6d132cf":"markdown","a89984c4":"markdown","66be448e":"markdown"},"source":{"911f954b":"import numpy as np # linear algebra\nimport pandas as pd # data processing\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 5)\npd.set_option('display.width', 1000)\n        \nimport os\nprint(os.listdir(\"..\/input\"))\n\n#Import visualization libraries\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n# Seaborn is also a visualization lib, it's actually pretty cool\nimport seaborn as sns\n\nfrom collections import Counter\n\n# Suppress Future Warnings (Safe Step)\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","353e1876":"# Load train and test data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# head method prints the first 5 rows of a DataFrame\nprint(train.head())","b7bb976c":"# Information about the dataframe\ntrain.info()","61a22fa6":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\ntrain.loc[Outliers_to_drop]","0ffd3c18":"# Drop outliers with the help of pandas drop method\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","11319e72":"# Fill missing values with NaN\ntrain = train.fillna(np.nan)\ntest = test.fillna(np.nan)\n\nprint(\"============= Train Data Info =============\")\nprint(train.info())\nprint(\"============= Test Data Info =============\")\nprint(test.info())\n\n# Check for Null values\nprint(\"=========== Missing Values Train =============\")\nprint(train.isnull().sum())\nprint(\"=========== Missing Values Test =============\")\nprint(test.isnull().sum())","9e60ed75":"train[\"Cabin\"][train[\"Cabin\"].notnull()].head()","6f6f66ae":"# Replace the Cabin number by the type of cabin 'X' if not\ndef fix_cabin(data):\n    data[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in data['Cabin'] ])\nfix_cabin(train)\nfix_cabin(test)","6c04bbf5":"# IMPUTING MISSING VALUES\ndef fill_missing_values(df):\n    ''' This function imputes missing values with median for numeric columns \n        and most frequent value for categorical columns'''\n    missing = df.isnull().sum()\n    missing = missing[missing > 0]\n    for column in list(missing.index):\n        if df[column].dtype == 'object':\n            df[column].fillna(df[column].value_counts().index[0], inplace=True)\n        elif df[column].dtype == 'int64' or 'float64' or 'int16' or 'float16':\n            df[column].fillna(df[column].median(), inplace=True)","5e221989":"fill_missing_values(train)\nfill_missing_values(test)","388f183c":"# Check for Null values\nprint(\"=========== Missing Values Train =============\")\nprint(train.isnull().sum())\nprint(\"=========== Missing Values Test =============\")\nprint(test.isnull().sum())","62fc8edf":"# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \ng = sns.heatmap(train.iloc[:, 1:].corr(),\n                annot=True, \n                fmt = \".2f\", \n                cmap = \"cool\")","2f67b952":"# Explore SibSp feature vs Survived\ng = sns.catplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","5b7dacf5":"# Explore Parch feature vs Survived\ng  = sns.catplot(x=\"Parch\", y=\"Survived\", data=train,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","63ec73da":"# Explore Age vs Survived\ng = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","21654002":"# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","15bf236b":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")","0d718861":"# Explore Pclass vs Survived\ng = sns.catplot(x=\"Pclass\",y=\"Survived\",data=train,kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","eb279eb6":"# Explore Embarked vs Survived \ng = sns.catplot(x=\"Embarked\", y=\"Survived\",  data=train,\n                   height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","980b86ec":"# Explore Pclass vs Embarked \ng = sns.catplot(\"Pclass\", col=\"Embarked\",  data=train,\n                   height=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","a7db0107":"g = sns.countplot(train[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","0dd833c7":"g = sns.catplot(y=\"Survived\",x=\"Cabin\",data=train,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")","7ba18ba3":"# Get Title from Name\ndef get_title(data):\n    data_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in data[\"Name\"]]\n    data[\"Title\"] = pd.Series(data_title)\n    data[\"Title\"].head()\nget_title(train)\ntrain['Title'].head()","ff92eab2":"get_title(test)\ntest['Title'].head()","5e358320":"g = sns.countplot(x=\"Title\",data=train)\ng = plt.setp(g.get_xticklabels(), rotation=45)","ed98dd36":"# Convert to categorical values Title \ndef convert_title(data):\n    data[\"Title\"] = data[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col',\n                                             'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    data[\"Title\"] = data[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n    data[\"Title\"] = data[\"Title\"].astype(int)\nconvert_title(train)\nconvert_title(test)","77a1a162":"g = sns.catplot(x=\"Title\",y=\"Survived\",data=train,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","7a9009c9":"# Drop Name variable\ntrain.drop(labels = [\"Name\"], axis = 1, inplace = True)\ntest.drop(labels=[\"Name\"], axis=1, inplace=True)","07761a7e":"# Create a family size descriptor from SibSp and Parch\ntrain[\"Fsize\"] = train[\"SibSp\"] + train[\"Parch\"] + 1\ntest[\"Fsize\"] = test[\"SibSp\"] + train[\"Parch\"] + 1","95ee8192":"g = sns.catplot(x=\"Fsize\",y=\"Survived\",data=train, kind='bar')\ng = g.set_ylabels(\"Survival Probability\")","7e1951e8":"# Create new feature of family size\ndef create_fsize(data):\n    data['Single'] = data['Fsize'].map(lambda s: 1 if s == 1 else 0)\n    data['SmallF'] = data['Fsize'].map(lambda s: 1 if  s == 2  else 0)\n    data['MedF'] = data['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n    data['LargeF'] = data['Fsize'].map(lambda s: 1 if s >= 5 else 0)\ncreate_fsize(train)\ncreate_fsize(test)","63f4f774":"g = sns.catplot(x=\"Single\",y=\"Survived\",data=train,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"SmallF\",y=\"Survived\",data=train,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"MedF\",y=\"Survived\",data=train,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"LargeF\",y=\"Survived\",data=train,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","3f607aa8":"# convert to indicator values Title and Embarked \ndef encode_train_embark(data):\n    '''Encoding Categorical variable Embarked'''\n    data = pd.get_dummies(data, columns = [\"Title\"])\n    data = pd.get_dummies(data, columns = [\"Embarked\"], prefix=\"Em\")\nencode_train_embark(train)\nencode_train_embark(test)","1fd8f1d4":"train['Cabin'] = train['Cabin'].map({'X': 1, 'C': 2, 'B': 3, 'D': 4, 'E': 5, 'A': 6, 'F': 7, 'G': 8, 'T': 9})\ntest['Cabin'] = test['Cabin'].map({'X': 1, 'C': 2, 'B': 3, 'D': 4, 'E': 5, 'A': 6, 'F': 7, 'G': 8, 'T': 9})","4da3967e":"train.isnull().sum()","aa4ac50e":"# Create categorical values for Pclass\ntrain[\"Pclass\"] = train[\"Pclass\"].astype(\"category\")\ntrain = pd.get_dummies(train, columns = [\"Pclass\"],prefix=\"Pc\")\n\ntest[\"Pclass\"] = test[\"Pclass\"].astype(\"category\")\ntest = pd.get_dummies(test, columns = [\"Pclass\"],prefix=\"Pc\")","c4c597a6":"# Drop useless variables \ntrain.drop(labels = [\"PassengerId\", \"Ticket\"], axis = 1, inplace=True)\ntest.drop(labels = [\"PassengerId\", \"Ticket\"], axis=1, inplace=True)","f6cf12d4":"from sklearn.preprocessing import LabelEncoder\ndef impute_cats(df):\n    '''This function converts categorical and non-numeric \n       columns into numeric columns to feed into a ML algorithm'''\n    # Find the columns of object type along with their column index\n    object_cols = list(df.select_dtypes(exclude=[np.number]).columns)\n    object_cols_ind = []\n    for col in object_cols:\n        object_cols_ind.append(df.columns.get_loc(col))\n\n    # Encode the categorical columns with numbers    \n    label_enc = LabelEncoder()\n    for i in object_cols_ind:\n        df.iloc[:,i] = label_enc.fit_transform(df.iloc[:,i])","9cae4e41":"# Impute the categorical values\nimpute_cats(train)\nimpute_cats(test)\nprint(\"Train Dtype counts: \\n{}\".format(train.dtypes.value_counts()))\nprint(\"Test Dtype counts: \\n{}\".format(test.dtypes.value_counts()))","1dcfe42c":"train.info()","29249a37":"test.info()","6d61fa7d":"# import the models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n# Package for stacking models\nfrom vecstack import stacking","5dd1d642":"# Seperate out feature and target variables\ny = train['Survived']\nX = train.drop('Survived', axis=1)","11818c63":"def train_model(classifier, name=\"Classifier\"):\n    '''This function is used to train and print the accuracy of our models'''\n    \n    folds = StratifiedKFold(n_splits=5, random_state=42)\n    accuracy = np.mean(cross_val_score(classifier, X, y, scoring=\"accuracy\", cv=folds, n_jobs=-1))\n    if name not in alg_list: alg_list.append(name)\n    print(f\"{name} Accuracy: {accuracy}\")\n    return accuracy","7971a29f":"# Lists that keep track cross val means and algorithm names\ncv_means = []\nalg_list = []","190e3339":"# Initialize the model\nlog_reg = LogisticRegression(C=5, penalty='l2',random_state=42)\n# Validate the model\nlog_reg_acc = train_model(log_reg, \"Logistic Regression\")\ncv_means.append(log_reg_acc)\n# Fit the best performing model to training data\nlog_reg.fit(X, y)","3df4e7f1":"# Initialize the model\nsvm = SVC(C=5, random_state=42)\n# Validate the model\nsvm_acc = train_model(svm, \"Support Vector Machine\")\ncv_means.append(svm_acc)\n# Fit the best performing model to training data\nsvm.fit(X, y)","8e03e7f1":"# Initialize the model\nrf = RandomForestClassifier(n_estimators=300, max_depth=25, \n                                min_samples_split=2, min_samples_leaf=2,\n                                max_features=\"log2\", random_state=12)\n# Validate the model\nrf_acc = train_model(rf, \"Random Forest\")\ncv_means.append(rf_acc)\n# Fit the best performing model to training data\nrf.fit(X, y)","74b47c2d":"# Initialize the model\nlda = LinearDiscriminantAnalysis(solver='lsqr')\n# Validate the model\nlda_acc = train_model(lda, \"Linear Discriminant Analysis\")\ncv_means.append(lda_acc)\n# Fit the best performing model to training data\nlda.fit(X, y)","2c2c5866":"# Initialize the model\nmlp = MLPClassifier(hidden_layer_sizes=(50, 10), activation='relu', solver='adam', \n                    alpha=0.01, batch_size=32, learning_rate='constant', \n                    shuffle=False, random_state=42, early_stopping=True, \n                    validation_fraction=0.2, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)\n# Validate the model\nmlp_acc = train_model(mlp, \"MLP\")\ncv_means.append(mlp_acc)\n# Fit the best performing model to training data\nmlp.fit(X, y)","b4b375ee":"# Initialize the model\nxgb = XGBClassifier(max_depth=5, learning_rate=0.1, n_jobs=-1, nthread=-1, \n                    gamma=0.06, min_child_weight=5, \n                    subsample=1, colsample_bytree=0.9, \n                    reg_alpha=0, reg_lambda=0.5, \n                    random_state=42)\n# Validate the model\nxgb_acc = train_model(xgb, \"XgBoost\")\ncv_means.append(xgb_acc)\n# Fit the best performing model to training data\nxgb.fit(X, y)","f7bbcb36":"# Initialize the model\nlgbm = LGBMClassifier(num_leaves=31, learning_rate=0.1, \n                      n_estimators=64, random_state=42, n_jobs=-1)\n# Validate the model\nlgbm_acc = train_model(lgbm, \"LGBM\")\ncv_means.append(lgbm_acc)\n# Fit the best performing model to training data\nlgbm.fit(X, y)","b5b081f5":"# Create a performance DF with score and Algorithm name\nperformance_df = pd.DataFrame({\"Algorithms\": alg_list, \"CrossValMeans\":cv_means})\n\n# Plot the performace of all models\ng = sns.barplot(\"CrossValMeans\",\"Algorithms\", data = performance_df, palette=\"Set3\",orient = \"h\")\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","52958a2e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","a8718491":"# First level models\nmodels = [rf, lda, lgbm]","f8fb9086":"# Perform Stacking\nS_train, S_test = stacking(models,\n                           X_train, y_train, X_test,\n                           regression=False,\n                           mode='oof_pred_bag',\n                           n_folds=5,\n                           save_dir=None,\n                           needs_proba=False,\n                           random_state=42,\n                           stratified=True,\n                           shuffle=True,\n                           verbose=2\n                          )","748368cf":"# Fit the 2nd level model on the output of level 1\nxgb.fit(S_train, y_train)","5736e934":"# Make predictions on the localized test set\nstacked_pred = xgb.predict(S_test)\nprint('Final prediction score: [%.8f]' % accuracy_score(y_test, stacked_pred))","fa5b89b2":"y1_pred_L1 = models[0].predict(test)\ny2_pred_L1 = models[1].predict(test)\ny3_pred_L1 = models[2].predict(test)\nS_test_L1 = np.c_[y1_pred_L1, y2_pred_L1, y3_pred_L1]","5986b80f":"test_stacked_pred = xgb.predict(S_test_L1)\nxgb.fit(X, y)\nxgb_pred = xgb.predict(test)\nrf_pred = rf.predict(test)\nlda_pred = lda.predict(test)","1408813d":"old_test = pd.read_csv('..\/input\/titanic\/test.csv')\n\nsubmission = pd.DataFrame({'PassengerId':old_test['PassengerId'], 'Survived': test_stacked_pred})\nxgb_sub = pd.DataFrame({'PassengerId':old_test['PassengerId'], 'Survived': xgb_pred})\nrf_sub = pd.DataFrame({'PassengerId':old_test['PassengerId'], 'Survived': rf_pred})\nlda_sub = pd.DataFrame({'PassengerId':old_test['PassengerId'], 'Survived': lda_pred})\n\nsubmission.to_csv(\"stacked_submission.csv\", index=False)\nxgb_sub.to_csv(\"xgboost_submission.csv\", index=False)\nrf_sub.to_csv(\"random_forest_submission.csv\", index=False)\nlda_sub.to_csv(\"lda_submission.csv\", index=False)\n\nbest_score = pd.read_csv('..\/input\/private-best-score\/best_score (1).csv')\nbest_score.to_csv('best_score.csv', index=False)","c18c0079":"### LightGBM (Light Gradient Boosting)","2bb24bda":"Boys, Girls and Women have higher chance of survival where the men have the lowest. Which again makes sense.","faac01c1":"One interesting observation from the above graph is how Passenger class and their survival are correlated.","e314aea3":"### If you like this kernel please consider giving it an UPVOTE.","305918b1":"### Performance","8b93563c":"More people from unknown cabin have lost their lives, while B, D, and E have the highest survival probability.","47e7c592":"### Support Vector Machine Classifier","2fb079c1":"We can see only about 4 titles repeating often so we keep them and name all the others \"rare\"","bf493981":"## Building, Training and Validating our Models\nThis is where real Machine Learning is done. There must be one thing which must be pretty clear to you now. Machine Learning needs Data Science skills as well. Without the above steps we can't build an accurate model. \n\nI've tuned the hyperparameter based on previous experience, you might have to try and a lot of variables to pick the best performing set of parameters or use something like GridSearchCV.","4cb4e00c":"Stacking is an ensemble learning technique that uses predictions from multiple models to build a new model. This model is used for making predictions on the test set. We pick some of the best performing models to be the first layer of the stack while XGB is set at layer 2 to make the final prediction. We use a package called **vecstack** to implement model stacking.  It's actually very easy to use, you can have a look at the [documentation](https:\/\/github.com\/vecxoz\/vecstack) for more information.","debaba60":"As we have seen before larger families and lone passengers have the least chance of survival.","abda777f":"We can see a lot of missing values these, we'll go on to fix them later. Before I get started I always like to drop the outliers in the data, if there aren't many of course.","707619e7":"### Random Forest","b558b6cf":"There 4 graphs again reiterate the same fact in much more detail.","5c0aee4b":"## Predictions\nNow it is finally time to make predictions on the real world test data. The approach here might look strange to you. You can visit [this link](https:\/\/github.com\/vecxoz\/vecstack\/issues\/4) to understand how it is done.","e4bbf883":"### Multi-Layer Perceptron","ab29b37b":"Look at that, if we had dropped cabin we'd have lost so many information. And also if we had imputed it with most frequest value it wouldn't have made sense. It looks much better to have it as a seperate feature.","94dfdf0c":"### Linear Discriminant Analysis","84096ee5":"Here it looks like passengers from Cherbourg have had the highest chance of survival when compared to Queenstown and Southamptom.","0640ea79":"Again people with more number of children\/parents on board haven'y survived, neither did people who were travelling alone. Poeple with Parch size of 3 have survived maybe it was the right size to help each other out. ","cd83b8bf":"That tiny spike on the left handside shows that more young kids on board survived. I'll leave the rest to you.","fe7ccef3":"## Model Stacking","ed7ee1d5":"## Exploratory Data Analysis (EDA)\nIn statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. We're going to do exactly that in this section, get ready for some cool visualizations.","7ea89e16":"## Feature Engineering\nFeature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. For example we are extracting titles from names and see the relations with other data points if it is significant then we can think about adding them as a seperate feature. Also machine learning algorithms can only deal with numbers as long as we're not talking about sequence models, so we'll fix categorical and test based featured as well.\n\nNote: I have applied the same feature engineering to the test data as well otherwise we'll have a feature mismatch while making prediction.","37a7b04b":"## Introduction\nThis tutorial is for people who are relatively new to machine learning. I'll get you through all the steps from importing the data to making predictions on the test set. I don't think there is a better way to start machine learning than from the Titanic Survival Prediction Challenge here on Kaggle. It is the first thing you should be able to solve on your own after learning ML. When I first started out I didn't do all that well, infact my first submission scored only 0.36, more than anything I didn't know what to do even after knowing what pandas, sklearn etc.. was all about. If you're in a similar position then it is for you too. If you read through the code you must be able to understand what I'm doing and how it must be done. And don't expect every ML problem to be as simple as this one, this is just to get you started. Anyways what you learn from this notebook may be applicable to some other real-world problems as well. Let's get started. \n\nAnd one more thing,\n\n#### If you find the kernel useful or cool please give an UPVOTE to show your appreciation. \n(I'm sure this kernel would be a lot helpful for beginners)","699990c5":"### XGBoost (Extreme Gradient Boosting)","b831b104":"Here it's clear that people with more number of siblings\/spouse onboard had a lesser chance of survival. And people who were alone also had relatively low chances of survival. ","8b3b0025":"### Logistic Regression","7de04948":"## Detecting and Fixing Outliers\nI wrote a function here to detect outliers. Feel free to use this fuction for your other projects as well. Honestly, we don't always keep writing the same functions over and over again for all projects, we write once and copy if it is ever needed again.","0f1499d7":"## Loading the Data\nThe first step in any Dat Science workflow is to load the data. It's commonly done using pandas's `pd.read_csv` function. It is also one of the most used functions in Data Science.","76fc7702":"We can see clearly that the youth have survived more than elderly men, and they've also lost more lives which means there were more youths onboard. ","eb0da23e":"## Submission Files\nI'm leaving you to play around with this kernel to improve this even further. This is just for illustration purposes still this kernel can score 79\/80 on the public Lb as I've tested. However if you're a leaderboard pixie and want to climb the LB to have some fun (let's accept as beginners we all wanted that), I'm giving you my Personal Best submission file as well.","e22807b5":"No surprise, first class passengers have survived more than the rest. Unfortunately lower class passengers mostly didn't make it.","c6d132cf":"Most people from Southamptom belong to class 3 that's why they have the lowest chances of survival. When you look at Cherbourg there are very less people but most of them are higher class passengers while Queenstown have mostly lower class passengers.","a89984c4":"## Fixing Missing Values\nEverything will be pretty much intuitive here except for the way I'm dealing the cabin feature of our data. When a ship sinks and if you're inside, the most important thing that decides whether you live or you die is the cabin you're in. And cabin data is largely unknown. For this reason I havn't imputed cabin's values with the most frequently occuring value, instead we keep unknown cabins as a seperate value as you'll see.","66be448e":"Whoa, a staggering number of females have survived compared to men, which can only mean women and children were given higher preference to get into life boats."}}