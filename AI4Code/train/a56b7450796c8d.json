{"cell_type":{"8f5edd90":"code","dd9ed438":"code","82e420e7":"code","ab3ce0bf":"code","d68e27a0":"code","2e3f3448":"code","14de5422":"code","62cc4fde":"code","a5af05d2":"code","f6c93cd7":"code","c285be63":"code","d85ba436":"code","dadae7fd":"code","d340021f":"code","58ff099b":"code","f80eb3fc":"code","c9ca0b6b":"code","2b86418d":"code","d1d7653f":"code","6842e6a8":"code","76bfc304":"code","029d6eab":"markdown","cb898661":"markdown","c09403e9":"markdown","c3836ba5":"markdown","d9f131ba":"markdown","289a3aff":"markdown","19a417e5":"markdown","94c542d3":"markdown","3551719b":"markdown","2b12f851":"markdown"},"source":{"8f5edd90":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\nfrom optuna import Trial\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","dd9ed438":"train = pd.read_csv('..\/input\/pceo-ai-club1\/train.csv', index_col=\"student_id\") #\ubc18\ub4dc\uc2dc index_col\uc744 student_id\ub85c \uc124\uc815\ud574\uc8fc\uc138\uc694\ntest = pd.read_csv('..\/input\/pceo-ai-club1\/test.csv', index_col=\"student_id\")\nsample_submission = pd.read_csv('..\/input\/pceo-ai-club1\/sample_submission.csv', index_col=\"student_id\")","82e420e7":"train.head()","ab3ce0bf":"test.head()","d68e27a0":"train.isnull().any()","2e3f3448":"y = train.pretest\nX = train.drop([\"pretest\"], axis=1)","14de5422":"s = (X.dtypes == 'object')\nobject_cols = list(s[s].index)\nobject_cols","62cc4fde":"label_X = X.copy()\nlabel_test = test.copy()\n\nordinal_encoder = OrdinalEncoder()\nlabel_X[object_cols] = ordinal_encoder.fit_transform(X[object_cols])\nlabel_test[object_cols] = ordinal_encoder.transform(test[object_cols])","a5af05d2":"label_X.head()","f6c93cd7":"label_test.head()","c285be63":"X_train, X_val, y_train, y_val = train_test_split(label_X, y, test_size=0.2, random_state=3)","d85ba436":"def objective(trial):\n    # To select which parameters to optimize, please look at the XGBoost documentation:\n    # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n    param = {\n        'tree_method':'gpu_hist',  # Use GPU acceleration\n        'lambda': trial.suggest_loguniform(\n            'lambda', 1e-3, 10.0\n        ),\n        'alpha': trial.suggest_loguniform(\n            'alpha', 1e-3, 10.0\n        ),\n        'colsample_bytree': trial.suggest_categorical(\n            'colsample_bytree', [0.5,0.6,0.7,0.8,0.9,1.0]\n        ),\n        'subsample': trial.suggest_categorical(\n            'subsample', [0.6,0.7,0.8,1.0]\n        ),\n        'learning_rate': trial.suggest_categorical(\n            'learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]\n        ),\n        'n_estimators': trial.suggest_categorical(\n            \"n_estimators\", [150, 200, 300, 3000]\n        ),\n        'max_depth': trial.suggest_categorical(\n            'max_depth', [4,5,7,9,11,13,15,17]\n        ),\n        'random_state': 42,\n        'min_child_weight': trial.suggest_int(\n            'min_child_weight', 1, 300\n        ),\n    }\n    model = XGBRegressor(**param)  \n    \n    model.fit(X_train,y_train,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(X_val)\n    \n    mae = mean_absolute_error(y_val, preds)\n    \n    return mae","dadae7fd":"study = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=200)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","d340021f":"def lgbm_objective(trial):\n    # To select which parameters to optimize, please look at the XGBoost documentation:\n    # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"mae\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    model = LGBMRegressor(**param)  \n    \n    model.fit(X_train,y_train,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(X_val)\n    \n    mae = mean_absolute_error(y_val, preds)\n    \n    return mae","58ff099b":"def cat_objective(trial):\n    # To select which parameters to optimize, please look at the XGBoost documentation:\n    # https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\n    param = {\n        \"objective\": \"regression\",\n        \"metric\": \"mae\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n    model = LGBMRegressor(**param)  \n    \n    model.fit(X_train,y_train,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n    \n    preds = model.predict(X_val)\n    \n    mae = mean_absolute_error(y_val, preds)\n    \n    return mae","f80eb3fc":"study = optuna.create_study(direction='minimize')\nstudy.optimize(lgbm_objective, n_trials=200)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","c9ca0b6b":"best_params = {'lambda': 0.005741219280777098, \n               'alpha': 2.1759591892131596, \n               'colsample_bytree': 0.7, \n               'subsample': 0.7, \n               'learning_rate': 0.02, \n               'n_estimators': 3000, \n               'max_depth': 17, \n               'min_child_weight': 49}\nmodel = XGBRegressor(**best_params)  \n    \nmodel.fit(X_train,y_train,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n\npreds = model.predict(X_val)\n\nmae = mean_absolute_error(y_val, preds)\n\nprint(mae)","2b86418d":"lgbm_params = {'lambda_l1': 0.031412754431276696, \n               'lambda_l2': 2.0633166117061854e-06, \n               'num_leaves': 163, \n               'feature_fraction': 0.42972618195865353, \n               'bagging_fraction': 0.7334168518029209, \n               'bagging_freq': 3, \n               'min_child_samples': 5}\n\nmodel = LGBMRegressor(**lgbm_params)  \n    \nmodel.fit(X_train,y_train,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n\npreds = model.predict(X_val)\n\nmae = mean_absolute_error(y_val, preds)\n\nprint(mae)","d1d7653f":"result = model.predict(label_test)","6842e6a8":"submission = pd.DataFrame(result,columns=[\"pretest\"], index=label_test.index)","76bfc304":"submission.to_csv(\"submission_optuna_lgbm.csv\")","029d6eab":"### Data Preprocessing","cb898661":"pretest \uac12\uc774 \uc6b0\ub9ac\uac00 \uc608\uce21\ud574\uc57c \ud558\ub294 \uc2dc\ud5d8 \uc810\uc218\uc785\ub2c8\ub2e4.","c09403e9":"\uc704\uc5d0\uc11c \ubcf4\ub2e4\uc2dc\ud53c \ub370\uc774\ud130\uc5d0 null \uac12\uc740 \uc5c6\uc2b5\ub2c8\ub2e4.","c3836ba5":"n_students \uc774\uc678\uc758 feature\ub4e4\uc740 \ubaa8\ub450 categorical\uc785\ub2c8\ub2e4. (cateogical => \ubb38\uc790 \ud615\ud0dc)","d9f131ba":"### Model predict & submission","289a3aff":"### \ub370\uc774\ud130 \ubc0f \ub77c\uc774\ube0c\ub7ec\ub9ac \ub85c\ub4dc","19a417e5":"# Hyperparameter Tuning using Optuna\n\noptuna\ub77c\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc774\uc6a9\ud558\uc5ec \uc190\uc27d\uac8c Hyperparameter\ub97c \ucd5c\uc801\ud654 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. objective \ud568\uc218\uc5d0\uc11c \ubaa8\ub378\uacfc \uadf8 \ubaa8\ub378\uc5d0 \ub9de\ub294 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub9cc \uc218\uc815\ud574\uc8fc\uba74 Xgboost\uac00 \uc544\ub2cc \ub2e4\ub978 \ubaa8\ub378\ub4e4\ub3c4 \uc2e4\ud5d8\ud574 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. ","94c542d3":"### Model train","3551719b":"categorical variable\uc744 \uc22b\uc790\ub85c \uc798 \ubc14\uafb8\uc5b4 \uc8fc\uc5c8\uc2b5\ub2c8\ub2e4. Ordinal Encoder\uc5d0 \ub300\ud574 \uc78a\uc5b4\ubc84\ub9b0 \ubd84\ub4e4\uc740 [kaggle course](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)\ub97c \ucc38\uace0\ud558\uc138\uc694.","2b12f851":"### Super Simple EDA"}}