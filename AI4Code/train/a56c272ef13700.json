{"cell_type":{"9e54d26c":"code","f50d454b":"code","343f928b":"code","c893a02d":"code","18d0fea5":"code","bf1be61d":"code","bc79b701":"code","7ec87ad0":"code","06836058":"code","29f9019b":"code","7475524b":"code","15c59a19":"code","27da1fac":"code","f782d8a8":"code","f0318752":"code","89696edd":"code","a02f3083":"code","a4dde32e":"code","be5be0de":"code","54b3036a":"code","b47e2b23":"code","5be49129":"code","515d84f0":"code","3edb8c3b":"code","d9c76f75":"code","99b94770":"code","db385a9c":"code","b89e7484":"code","bda40a83":"code","6b1156ef":"code","ce978df0":"code","6cd23569":"code","8ed433b1":"code","25832acb":"code","57a9faa7":"code","66c56bf6":"code","6e2e996b":"code","4ccfdab7":"code","13f73d6d":"code","d6a44896":"code","6f169818":"code","04343b9b":"code","0dd7546a":"code","c72521c0":"code","50863022":"code","f31ddfc2":"code","7f1d69d7":"code","e4b2d6c7":"code","e25f6961":"code","3cfa67fb":"code","29d13ed2":"code","b9823457":"code","311e39af":"code","f37df401":"markdown","3480b08c":"markdown","78db494e":"markdown","8a461953":"markdown","40ee6d11":"markdown","da300d06":"markdown","58dfb08c":"markdown","e26338ba":"markdown","fe2ad282":"markdown","d48030d2":"markdown","4e5d374a":"markdown","cf7a608d":"markdown","4b68a0f3":"markdown"},"source":{"9e54d26c":"\n# import statements\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling \n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n","f50d454b":"# read the data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_x = train_data.loc[:, train_data.columns != \"Survived\"]\ntrain_y = train_data.loc[:, train_data.columns == \"Survived\"]\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_x = test_data\nfull_data = train_x.append(test_x)\nprint (\"full data shape\",full_data.shape)\nprint (\"train_x shape\",train_x.shape)","343f928b":"full_data[full_data['Age'].isnull()]","c893a02d":"ftest_pid = test_data[\"PassengerId\"]\nftest_pid","18d0fea5":"test_data.head()","bf1be61d":"train_data.describe()","bc79b701":"test_data.describe()","7ec87ad0":"# import warnings\n# warnings.filterwarnings('ignore')\n# profile = pandas_profiling.ProfileReport(train_data)\n# profile","06836058":"# heat map of features\nplt.rcParams['figure.figsize'] = (7, 7)\nplt.style.use('ggplot')\n\nsns.heatmap(train_data.corr(), annot = True, cmap = 'Wistia')\nplt.title('Heatmap for the Dataset', fontsize = 20)\nplt.show()","29f9019b":"plt.rcParams['figure.figsize'] = (15,5)\nsns.distplot(train_data['Fare'], kde=False, rug=True, bins=90)\nplt.title('Distribution of fare', fontsize=20)\nplt.show()\n","7475524b":"plt.rcParams['figure.figsize'] = (15,5)\nsns.distplot(train_data['Age'], kde=False, rug=True)\nplt.title('Distribution of age', fontsize=20)\nplt.show()","15c59a19":"# box plot of fare\nplt.rcParams['figure.figsize'] = (15,8)\nsns.boxplot(train_data['Survived'], full_data['Fare'])\nplt.title('box plot of fare with target')\nplt.show()","27da1fac":"# boxen plot of Age\nplt.rcParams['figure.figsize'] = (15,8)\nsns.boxplot(train_data['Survived'], full_data['Age'])\nplt.title('boxen plot of age with target')\nplt.show()","f782d8a8":"# boxen plot of Age\nplt.rcParams['figure.figsize'] = (15,8)\nsns.boxenplot(train_data['Survived'], full_data['Fare'])\nplt.title('boxen plot of age with target')\nplt.show()","f0318752":"# pair plot of different feature\ntmp_df = train_data[['Survived', 'Fare', 'Age']]\nsns.pairplot(tmp_df)\nplt.show()","89696edd":"\n# bar graph with hue\nsns.catplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", kind=\"bar\", data=train_data)\n# sns.catplot(x=\"Sex\", y=\"Survived\", kind=\"bar\", data=train_data)\nplt.show()","a02f3083":"sns.catplot(x=\"Survived\", y=\"Fare\", hue=\"Sex\", kind=\"swarm\", data=train_data)\nplt.show()","a4dde32e":"# sns.relplot(x=\"SibSp\", y=\"Ticket\", hue=\"Survived\", data=train_data)\n# plt.show()\n","be5be0de":"full_data.iloc[891:895,:]","54b3036a":"# simpleImpute of Age feature\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan,strategy='mean')\nimp = imp.fit(train_data[[\"Age\"]])\n# full_data[\"Age\"] = pd.DataFrame(imp.transform(full_data[[\"Age\"]]))\n# test_data[\"Age\"] = pd.DataFrame(imp.transform(test_data[[\"Age\"]]))\n# sns.distplot(full_data[\"Age\"], kde=False, rug=True)\n# plt.show()\ntmp_age = pd.DataFrame(imp.transform(full_data[[\"Age\"]]))\ntmp_age = tmp_age.rename(columns={0:\"Age\"})\nfull_data.drop(['Age'],axis=1,inplace=True)\n","b47e2b23":"full_data.iloc[415:420,:]","5be49129":"result = pd.concat([full_data, tmp_age], axis=1, join='inner')\n","515d84f0":"full_data = result\nfull_data","3edb8c3b":"# # IterativeImpute\n# from sklearn.experimental import enable_iterative_imputer\n# from sklearn.impute import IterativeImputer\n# imp = IterativeImputer(max_iter=10, random_state=0)\n# imp.fit(train_x[[\"Age\"]])\n# tmp_age1 = pd.DataFrame(imp.transform(train_data[[\"Age\"]]))\n# tmp_age2 = pd.DataFrame(imp.transform(test_data[[\"Age\"]]))\n# sns.distplot(tmp_age1, kde=False, rug=True)\n# plt.show()","d9c76f75":"full_data=full_data.drop(columns=[\"PassengerId\",\"Name\", \"Cabin\", \"Ticket\"])\nfull_data.columns","99b94770":"# using one hot encoder from pandas.get_dummies\nonehot_embark = pd.get_dummies(full_data[\"Embarked\"], prefix=\"Embark\")\nonehot_pclass = pd.get_dummies(full_data[\"Pclass\"], prefix=\"Pclass\")\n# full_data.drop(columns=[\"Embarked\",\"Pclass\"],axis=1,inplace=True)\n# full_data = pd.concat([full_data, onehot_embark, onehot_pclass], axis=1,join='inner')\n# full_data\n\n","db385a9c":"result = pd.concat([full_data, onehot_embark, onehot_pclass], axis=1,join='inner')\nresult","b89e7484":"full_data = result.copy()","bda40a83":"full_data.drop(columns=[\"Embarked\",\"Pclass\"],axis=1,inplace=True)\nfull_data","6b1156ef":"# modify cat string values of Sex column to int values\nfull_data.loc[full_data.Sex==\"male\",\"Sex\"] = 0\nfull_data.loc[full_data.Sex==\"female\", \"Sex\"] = 1\nfull_data","ce978df0":"tmp_full_data = full_data.copy()\ntmp_data = full_data.copy()","6cd23569":"#experiment the data; To check new features such as age range, Fare range, (sibsp*parch)->family column.\n#experimentation is at the end of this notebook\n#Even if something happens to full_data or tmp_full_data. we can simply execute this one cell to start at this point.\nexp_full_data = tmp_data.copy() ","8ed433b1":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntmp_age = pd.DataFrame(tmp_full_data[\"Age\"][0:891])\nfull_age_0 = pd.DataFrame(tmp_full_data[\"Age\"][:])\nscaler.fit(tmp_age)\nfull_age_1 = pd.DataFrame(scaler.transform(full_age_0))\nfull_age_1 = full_age_1.rename(columns={0:\"Age\"})\nfull_age_1","25832acb":"sns.distplot(full_age_0)\nplt.show()","57a9faa7":"sns.distplot(full_age_1)\nplt.show()","66c56bf6":"full_data.drop(columns=[\"Age\"], axis=1, inplace=True)\nfull_data","6e2e996b":"full_data = pd.concat([full_data, full_age_1], axis=1, join=\"inner\")\nfull_data","4ccfdab7":"tmp_full_data = full_data.copy()","13f73d6d":"scaler = StandardScaler()\ntmp_fare = pd.DataFrame(tmp_full_data[[\"Fare\",\"SibSp\",\"Parch\"]][0:891])\ntmp_fare\nfull_fare_0 = pd.DataFrame(tmp_full_data[[\"Fare\",\"SibSp\",\"Parch\"]][:])\nscaler.fit(tmp_fare)\nfull_fare_1 = pd.DataFrame(scaler.transform(full_fare_0))\nfull_fare_1 = full_fare_1.rename(columns={0:\"Fare\",1:\"SibSp\",2:\"Parch\"})\nfull_fare_1","d6a44896":"full_data.drop(columns=[\"Fare\",\"SibSp\",\"Parch\"], axis=1, inplace=True)\nfull_data = pd.concat([full_data, full_fare_1], axis=1, join=\"inner\")\nfull_data","6f169818":"full_data_copy = full_data.copy()\nfull_data.describe()","04343b9b":"from sklearn.model_selection import train_test_split\n#following is the final test\nx_ftest = full_data.iloc[891:,:]\nprint (\"shape of x_ftest\",x_ftest.shape)\n# (418,11)\nxdf = pd.DataFrame(full_data.iloc[:891,:])\nydf = train_y\nx_train,x_test,y_train, y_test = train_test_split(xdf,ydf,test_size=0.25,random_state=None)\nprint(\"Shape of x_train :\", x_train.shape)\nprint(\"Shape of x_test :\", x_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","0dd7546a":"# Random forests is well known for classification problems.\n# But it is suffering from overfitting. Submission acc is 73.2%\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import confusion_matrix\n# from sklearn.metrics import classification_report\n\n# model = RandomForestClassifier(n_estimators=100)\n# model.fit(x_train, y_train)\n# y_pred = model.predict(x_test)\n\n# # evaluating the model\n# print(\"Training Accuracy :\", model.score(x_train, y_train))\n# print(\"Testing Accuracy :\", model.score(x_test, y_test))\n\n# # cofusion matrix\n# cm = confusion_matrix(y_test, y_pred)\n# plt.rcParams['figure.figsize'] = (5, 5)\n# sns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu')\n\n# # classification report\n# cr = classification_report(y_test, y_pred)\n# print(cr)","c72521c0":"# random forest algorithm is overfitting. So, lets use logistic regression with regularization.\n# final subimission accuracy of this model is 79.4%\n# but lets try using SVM\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nmodel = LogisticRegression(random_state=0).fit(x_train, y_train)\ny_pred = model.predict(x_test)\ny_pred_quant = model.predict_proba(x_test)[:, 1]\n# evaluating the model\nprint(\"Training Accuracy :\", model.score(x_train, y_train))\nprint(\"Testing Accuracy :\", model.score(x_test, y_test))\n\n# cofusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.rcParams['figure.figsize'] = (5, 5)\nsns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu')\n\n# classification report\ncr = classification_report(y_test, y_pred)\nprint(cr)","50863022":"# # submission acc is 75.5% \n# from sklearn import svm\n# from sklearn.metrics import confusion_matrix\n# from sklearn.metrics import classification_report\n# model = svm.SVC(kernel='linear')\n# model.fit(x_train, y_train)\n# y_pred = model.predict(x_test)\n# # evaluating the model\n# print(\"Training Accuracy :\", model.score(x_train, y_train))\n# print(\"Testing Accuracy :\", model.score(x_test, y_test))\n\n# # cofusion matrix\n# cm = confusion_matrix(y_test, y_pred)\n# plt.rcParams['figure.figsize'] = (5, 5)\n# sns.heatmap(cm, annot = True, annot_kws = {'size':15}, cmap = 'PuBu')\n\n# # classification report\n# cr = classification_report(y_test, y_pred)\n# print(cr)","f31ddfc2":"# y_fpred = pd.DataFrame(model.predict(x_ftest))\n# y_fpred = y_fpred.rename(columns={0:\"Survived\"})\n# print (y_fpred)\n# fresult = pd.concat([ftest_pid, y_fpred],axis=1,join='inner')\n# print (fresult)\n# fresult.to_csv(\"result_with_norm_svm.csv\", index=False)","7f1d69d7":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_quant)\n\nfig, ax = plt.subplots()\nax.plot(fpr, tpr)\nax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"-\", c=\".3\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\n\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.title('ROC curve for titanic classifier', fontweight = 30)\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.show()","e4b2d6c7":"from sklearn.metrics import auc\nauc = auc(fpr, tpr)\nprint(\"AUC Score :\", auc)","e25f6961":"# Learn this\n\n\n# importing ML Explanability Libraries\n#for purmutation importance\nimport eli5 \nfrom eli5.sklearn import PermutationImportance\n\n#for SHAP values\nimport shap \nfrom pdpbox import pdp, info_plots #for partial plots\n\n# let's check the importance of each attributes\n\nperm = PermutationImportance(model, random_state = 0).fit(x_test, y_test)\neli5.show_weights(perm, feature_names = x_test.columns.tolist())\n\n","3cfa67fb":"# plotting the partial dependence plot for num_major_vessels\n\nbase_features = full_data.columns.values.tolist()\n\nfeat_name = 'Sex'\npdp_dist = pdp.pdp_isolate(model=model, dataset=x_test, model_features = base_features, feature = feat_name)\n\npdp.pdp_plot(pdp_dist, feat_name)\nplt.show()","29d13ed2":"exp_full_data.describe()","b9823457":"# understand how to use this method of graph\n# this is for continuous variables.\ngrid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend();","311e39af":"# this is for categorical values\ngrid = sns.FacetGrid(train_data, row='Embarked', size=2.2, aspect=1.6)\ngrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ngrid.add_legend()","f37df401":"# Understand data\nFirst take a statistical glance of data\n","3480b08c":" # Fill the missing data \nUse sklearn.SimpleImputer to fill the missing values with (mean, median, mode) of the data <br>\nSimpleImputer: Uses single column values to calculate that column NaN's  <br>\nIterativeImputer: Uses entire set of feature dimentions to calculate the missing value. <br>\nNOTE: we can also use imputer for categorical (text) based features","78db494e":"## Experimentation in feature engineering, feature transformation\n1. Turning continuous numerical features into bins and then bins into ordinal features <br>\n    Age -> bins <br>\n    Fare -> bins <br>\n    When should we do this? How do we know converted bins works better than continuous values. <br>\n   \n   \n2. A new Feature creation from original features. <br>\n    Name -> Titles <br>\n    Because there is a strong correlation of Name titles to survival <br>\n    (Parch + sibsp) -> Family_size ??<br>\n    Family_size ==1 -> alone ??<br>\n    \n    \n3. Fill missing values of Age.<br>\n    Instead of taking mean of all the train records. There is a correlation \"age\" and \"pclass x sex\" <br>\n    Take => (pclass 1 x male) age mean and fill the missing records of pclass1 and male <br>\n    \n4. \n   \n","8a461953":"# Transform cat values in to new features\nThis transformation enables ml algorithms to better understand the data <br>\nFrom now on use full_data","40ee6d11":"Below boxplot tells us distribution of data with respect to target.","da300d06":"# Modelling \nHere we need to try out different ML models and analyze their performance.\n","58dfb08c":"# Drop any unnessasary columns\nuseless columns such as name, id, etc. \nThese columns doesn't help ML algo in any way.<br>\nHere, Parch and Ticket feature has a lot in common. Both 76% filled by single passengers, 14%","e26338ba":"## TODO\ndo something about very high fares. ","fe2ad282":" For categorical data, \n * we can use swarm plot\n * box plots with hue","d48030d2":"Above boxplot shows us:\n* survived -> paid high fare\n* Lot of outliers ","4e5d374a":"# split the data in to train, validation, test set\nOut of all the train data split it into 60%train, 20%test, 20%test. <br>\nbut here we dont have much train data and it's my first practise. so Train-> 70%train, 30%Test.","cf7a608d":"This is a fairly complex beginner machine learning problem. One can effectively learn Feature transformation and engineering techniques, Data visualization methods from this problem. I had fun solving this problem. Please provide comments on how to improve this solution. <br>\nThanks. <br>\n\nSubmission Acc: 79.5 (Top 15%)\n","4b68a0f3":"# Normalize the data\nNormalize all the feature vectors. Here we used standard normalization <br>\nIf we don't normalize, we get this result\n>        Training Accuracy : 0.9839486356340289\n        Testing Accuracy : 0.8283582089552238\n                      precision    recall  f1-score   support\n                    0       0.84      0.89      0.86       164\n                   1       0.81      0.73      0.77       104\n            accuracy                           0.83       268\n           macro avg       0.82      0.81      0.82       268\n        weighted avg       0.83      0.83      0.83       268\n        \nIf we normalize, we get following result\n\n>             Training Accuracy : 0.9839486356340289\n            Testing Accuracy : 0.8022388059701493\n                          precision    recall  f1-score   support\n                       0       0.84      0.84      0.84       167\n                       1       0.74      0.74      0.74       101\n                accuracy                           0.80       268\n               macro avg       0.79      0.79      0.79       268\n            weighted avg       0.80      0.80      0.80       268\n\nThere is lot of overfitting going on. \n\n"}}