{"cell_type":{"0f99794d":"code","12c949f5":"code","6f72b514":"code","48563559":"code","f26b0164":"code","895c5491":"code","fb0d3a58":"markdown","936bc1c3":"markdown","52db62b7":"markdown","713a2cbd":"markdown"},"source":{"0f99794d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('display.max_rows',None)\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndata_raw = pd.read_csv('\/kaggle\/input\/titanic\/train.csv',index_col='PassengerId')\n\n# drop columns won't be used.\ndata = data_raw.drop(['Survived','Cabin','Name','Ticket'],axis=1)\n\n\n# Any results you write to the current directory are saved as output.","12c949f5":"# Preprocess Missing data and object data.\n\n# Impute Embarked using fillna with most frequent value(S).\ndata.Embarked.fillna('S',inplace=True)\n\n# Encode Sex and Embarked\nfrom sklearn.preprocessing import OneHotEncoder\none_hot_enc = OneHotEncoder(handle_unknown='ignore')\nEmbarked_enc=pd.DataFrame(one_hot_enc.fit_transform(data[['Embarked']]).toarray(),\n                         index=data.index, \n                         columns = one_hot_enc.get_feature_names(['Embarked']))\ndata_enc=pd.concat([Embarked_enc,data],axis=1).drop(['Embarked'],axis=1)\n\n\ndata_enc['Sex'] = np.where(data_enc.Sex=='female',1,0)\n","6f72b514":"# Extract honorifics from names that contain the infomation in terms of passengers's Age.\nlist = []\nfor i in range(1,len(data_raw)+1):\n    list.append(data_raw.Name.loc[i].split(\",\")[1].split()[0])\n    \nhonorifics=pd.DataFrame(data = list , columns=['honorifics'], index = data_raw.index)\nhonorifics.honorifics=np.where(honorifics.honorifics.isin(['Mr.','Miss.','Mrs.','Master.','Dr.']),\n                    honorifics.honorifics, 'Rare')\n\n# This concat is for checking whether extracting was done well.\nname=pd.concat([honorifics,data_raw.Name],axis=1)\n\n# Encode it and concatenate it to data_enc\n_name=pd.DataFrame(one_hot_enc.fit_transform(honorifics[['honorifics']]).toarray(),\n            index = honorifics.index, columns = one_hot_enc.get_feature_names(['']))\ndata_enc_name = pd.concat([data_enc,_name],axis=1)\n\n# Split into two data set.\ntrain = data_enc_name[data.Age.isnull()==False]\ntest = data_enc_name[data.Age.isnull()==True]","48563559":"plt.figure(figsize=(14,10))\nrelevant=['Pclass','SibSp','Parch','_Master.','_Miss.','_Mrs.','_Miss.','_Rare']\nsns.heatmap(train.corr(),annot=True,center=0,cmap= 'coolwarm')","f26b0164":"train_relevant=train[relevant]\n\n# Split and train model\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_relevant, train.Age, \n                                                    test_size=0.2, random_state=0)\n\nfrom sklearn.ensemble import RandomForestRegressor\nrfr=RandomForestRegressor(n_estimators=1000, random_state=0)\nrfr.fit(x_train, y_train)\n\n# Get predication array to inpect how accurate the model is.\npred=rfr.predict(x_test)\nMAE_rfr=np.abs(pred-y_test).mean()\n\n# To inspect, Categorize pred and y_test date.\npred_cat=pd.cut(x=pred, bins=[0,18,30, pred.max()], labels=[2,0,1])\ny_test_cat=pd.cut(x=y_test, bins=[0,18,30, pred.max()], labels=[2,0,1])\n\n# Inspect how accurate the model is.\nresult_rfr = np.where(pred_cat==y_test_cat,1,0)\naccuracy_rfr = result_rfr.sum()\/len(result_rfr)\nprint(\"The accuracy from Random Forest Regression :\",accuracy_rfr)\n","895c5491":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100, random_state=0)\n\n# To use classifier, Categorize data first.\ny_train_cat = pd.cut(x = y_train,bins=[0,18,30, y_train.max()], labels=[2,0,1])\ny_test_cat = pd.cut(x = y_test,bins=[0,18,30, y_test.max()], labels=[2,0,1])\n\nrfc.fit(x_train , y_train_cat)\npred = rfc.predict(x_test)\n\nresult_rfc = np.where(pred==y_test_cat, 1, 0)\naccuracy_rfc = result_rfc.sum()\/len(result_rfc)\nprint(\"The accuracy from Random Forest Classifier :\",accuracy_rfc)","fb0d3a58":"It'd be the one of important kicks in this notebook. That is to make the colunms of name the numerical values. ","936bc1c3":"There Two diffenrent ways. One is first, to categorize the column of Age value into categorical numbers. and then to predict. Of cource, with this way will be used along with Classification model. \n\nThe other one is to use regression and then to categorize.  \nLet's make 2 cases and compare them","52db62b7":"We need to take a look what kinds of relationship there are among them.  \nSo I will use heatmap to distinguish whih feature we should use or not.","713a2cbd":"The way that categorizing data first and then training model with Classifier is more accurater then another way that training model with Regression first and then categorizing data."}}