{"cell_type":{"ed446bb5":"code","893467fb":"code","78cb2c1d":"code","42d6b589":"code","5b233fb6":"code","89d3d70b":"code","5dfa9738":"code","ace0395d":"code","9861b476":"code","c7f3eec3":"code","496aa3ac":"code","664190c6":"code","160c56b2":"code","322f2df1":"code","1e1e2b20":"code","8f2513c4":"code","71559ead":"code","01ce79f1":"code","33406f97":"code","9c6eaf57":"code","d68ea834":"code","3275cf8d":"code","1db29623":"code","685491ef":"code","bbca6708":"code","e70cee3a":"code","fc5ca570":"code","bc8671a9":"code","7874eb9f":"code","a8dab591":"code","f74178c8":"code","474b4ef6":"code","694b5c99":"code","1b6b1d37":"markdown","0f5a28b8":"markdown","c07eadae":"markdown","5eb955a1":"markdown","521da991":"markdown","1e6dbf56":"markdown","a1da27b1":"markdown","35619b8c":"markdown","de544f73":"markdown","6978f3b1":"markdown","50c50fe9":"markdown","a12a758c":"markdown","75207fd3":"markdown"},"source":{"ed446bb5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\n\n\nfrom xgboost import XGBClassifier\n\nsns.set(style='white', context='notebook', palette='deep')","893467fb":"income_data = pd.read_csv('\/kaggle\/input\/adult-census-income\/adult.csv')","78cb2c1d":"# initial Samples\nincome_data.sample(n=10)","42d6b589":"print(\"Categorical features data uniquesness\")\nfor i in income_data.columns.tolist():\n    if income_data[i].dtype !='int64':\n        print(f\"{income_data[i].name} ---> {len(income_data[i].unique().tolist())} unique values ----> {income_data[i].unique()}\")\n        print(\"####\")","5b233fb6":"income_data.columns.tolist()","89d3d70b":"datashape = income_data.shape\nprint(f\"Total {datashape[0]} samples along with {datashape[1]} features are present in the dataset\")","5dfa9738":"#Grilling the data description\nincome_data.info()","ace0395d":"#lookup by selective feature dtypes \nnumerics = ['int64','float64']\nselective_df = income_data.select_dtypes(include=numerics)\nprint(f\"Total {len(selective_df.columns)} features are of numeric types\")\nselective_df.sample(n=10)","9861b476":"numerics = ['object']\nselective_df = income_data.select_dtypes(include=numerics)\nprint(f\"Total {len(selective_df.columns)} features are of non-numeric types\")\nprint(\"Taht includes the target feature dtype as well\")\nselective_df.sample(n=10)","c7f3eec3":"# Check the distribution of 'workclass' feature\nincome_data['workclass'].value_counts().plot(kind='bar')","496aa3ac":"workclass_mapper = {\n    'State-gov': \"other\",\n    'Self-emp-not-inc': \"other\",\n    'Federal-gov': \"other\",\n    'Local-gov': \"other\",\n    '?': \"other\",\n    'Self-emp-inc': \"other\",\n    'Without-pay': \"other\",\n    'Never-worked': \"other\"\n}\nincome_data[\"workclass\"] = income_data[\"workclass\"].map(workclass_mapper).fillna(income_data[\"workclass\"])\nincome_data['workclass'].value_counts().plot(kind='bar')","664190c6":"# Check the distribution of 'education' feature\nincome_data['education'].value_counts().plot(kind='bar')","160c56b2":"mapper ={'11th': 'other',\n         'Masters': 'other',\n         '9th': 'other',\n         'Assoc-acdm': 'other',\n         'Assoc-voc': 'other',\n         '7th-8th': 'other',\n         'Doctorate': 'other',\n         'Prof-school': 'other',\n         '5th-6th': 'other',\n         '10th': 'other',\n         '1st-4th': 'other',\n         'Preschool': 'other',\n         '12th': 'other'}\n\nincome_data['education'] = income_data['education'].map(mapper).fillna(income_data['education'])\n# Re-engineered 'education' feature\nincome_data['education'].value_counts().plot(kind='bar')","322f2df1":"# Check the distribution of 'marital-status' feature\nincome_data['marital.status'].value_counts().plot(kind='bar')","1e1e2b20":"mapper = {'Divorced': 'other',\n 'Married-spouse-absent': 'other',\n 'Separated': 'other',\n 'Married-AF-spouse': 'other',\n 'Widowed': 'other'}\nincome_data['marital.status'] = income_data['marital.status'].map(mapper).fillna(income_data['marital.status'])\n# Re-engineered 'marital-status' feature\nincome_data['marital.status'].value_counts().plot(kind='bar')","8f2513c4":"# Check the distribution of 'occupation' feature\nincome_data['occupation'].value_counts().plot(kind='bar')","71559ead":"mapper = {\n '?': 'Prof-specialty',\n 'Protective-serv': 'other',\n 'Armed-Forces': 'other',\n }\n\nincome_data['occupation'] = income_data['occupation'].map(mapper).fillna(income_data[\"occupation\"])\nincome_data['occupation'].value_counts().plot(kind='bar')","01ce79f1":"income_data.drop(columns=[\"fnlwgt\"], axis=1, inplace=True)","33406f97":"# this will provide the exploratory stats about numerical features\nincome_data.describe()","9c6eaf57":"#Target Lookup\n#As Income is to be predicted based on the available predictors\/independent variables\n#we need to look into the number of classes the prediction must fall into.\nincome_data[\"income\"].unique()","d68ea834":"## Feature Correlation\nplt.figure(figsize=(12,10))\ncor = income_data.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","3275cf8d":"income_series = income_data[\"income\"]\nsns.countplot(income_series)\nplt.figure(figsize=(10,10))\nplt.show()","1db29623":"labeled_income_data = income_data.apply(LabelEncoder().fit_transform)\nlabeled_income_data.head()","685491ef":"y = labeled_income_data[\"income\"]\nX = labeled_income_data.drop(\"income\", axis=1)\nX.head()","bbca6708":"labeled_income_data[\"income\"].unique()","e70cee3a":"labeled_income_data.head()","fc5ca570":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)","bc8671a9":"Model_Accuracy_dict = {}","7874eb9f":"def confusion_metric(y_test,y_pred):\n    # Calculate the confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_test, y_pred=y_pred)\n\n    # Print the confusion matrix using Matplotlib\n    fig, ax = plt.subplots(figsize=(9, 9))\n    ax.matshow(conf_matrix, cmap=plt.cm.Wistia, alpha=0.5)\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            ax.text(x=j,\n                    y=i,\n                    s=conf_matrix[i, j],\n                    va='center',\n                    ha='center',\n                    size='xx-large')\n    plt.xlabel('Predictions', fontsize=18)\n    plt.ylabel('Actuals', fontsize=18)\n    plt.title('Confusion Matrix', fontsize=18)\n    plt.show()\n\ndef roc_curve_draw(model,X_test,y_test):\n    lr_probs = model.predict_proba(X_test)\n\n    # keep probabilities for the positive outcome only\n    lr_probs = lr_probs[:, 1]\n\n    ns_probs = [0 for _ in range(len(y_test))]\n\n    # calculate scores\n    ns_auc = roc_auc_score(y_test, ns_probs)\n    lr_auc = roc_auc_score(y_test, lr_probs)\n\n\n    # calculate roc curves\n    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)\n\n    plt.figure(figsize=(13,9))\n    # plot the roc curve for the model\n    plt.plot(ns_fpr, ns_tpr, linestyle='-', label='income')\n    plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')\n\n    # axis labels\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n\n    # show the legend\n    plt.legend()\n\n    # show the plot\n    plt.show()","a8dab591":"randomForest = RandomForestClassifier(max_depth=10, max_features=10, n_estimators=25)\nrandomForest.fit(X_train,y_train)\npred = randomForest.predict(X_test)\nModel_Accuracy_dict[\"Random Forest Classier\"] = accuracy_score(pred,y_test)\nModel_Accuracy_dict[\"Random_Forest_CLF_REPORT\"] = classification_report(y_test,pred)\nprint(accuracy_score(pred,y_test))\n\nroc_curve_draw(randomForest,X_test,y_test)\nconfusion_metric(pred,y_test)","f74178c8":"xgb = XGBClassifier()\nxgb.fit(X_train, y_train)\npred = xgb.predict(X_test)\nModel_Accuracy_dict[\"XG Boost Classier\"] = accuracy_score(pred,y_test)\nModel_Accuracy_dict[\"XGB_Classification_Report\"] = classification_report(y_test,pred)\nprint(\"\\n\\nAccuracy :: \",accuracy_score(pred,y_test))\nroc_curve_draw(xgb,X_test,y_test)\nconfusion_metric(pred,y_test)","474b4ef6":"print(Model_Accuracy_dict[\"Random_Forest_CLF_REPORT\"])","694b5c99":"print(Model_Accuracy_dict[\"XGB_Classification_Report\"])","1b6b1d37":"### Marital-Status ","0f5a28b8":"### Workclass","c07eadae":"## Final Comments::","5eb955a1":"Reason of seleccting Tree based ensemble algorithms for model training.\n<ol>\n    <li>As it is pretty much clear that the data is imbalanced and not on the same scale as well.\n    <li>Tree based approach are safe from unscaled\/unnormalized data s well.\n    <li>Pretty much outliers are also present in the dataset as most of the features deviate from the normal guassian.\n    <li>Tree based approach is much safer to high variance and noise, as they are having the ensemble of weak learner.\n    <li>we can be sure of controlled fitting (No over-fit as their main motive\/behaviour is to reduce variance while having a frozen bias)\n<\/ol>","521da991":"## Model buuilding and Metrices","1e6dbf56":"It is evident that we have 2 classes of the pred, one would be above 50k and another would be below 50k","a1da27b1":"### Data Imbalance lookup","35619b8c":"Let's investigate the data based on the set of targets,","de544f73":"if we look at the overall data sample count, we have quite less samples(around more than half) for people who are earning <50k we can apply **SMOTE** to balance it.","6978f3b1":"## Exploratory Data Analysis","50c50fe9":"### Occupation","a12a758c":"### Education","75207fd3":"**By looking at the roc and confusion metric it is pretty similar conclusion, but if we closely observe the classification report against per class predictions. XGBoost classifier seems having an upper hand based on the overall correct class predictions**<br>\nAs it is evident, since beginning that data has target  imbalance and somewhat biased features as well.\n \nStill if look closely to the classification report section per class basis, xgboost classifier has better and balanced precision and recall per class.<br> It reflects this behaviour in  its oveall accuracy as well. ROC as well not distracted against the results.\n"}}