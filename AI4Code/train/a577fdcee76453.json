{"cell_type":{"0d0d04f3":"code","e3026167":"code","2abce923":"code","24f7a1de":"code","28f4cd75":"code","e285b3c1":"code","2a2ddc8c":"code","d942fa27":"code","2b5c4335":"code","2b08fe28":"code","8fee604f":"code","3edacd5a":"code","cb08ca57":"code","01b66cda":"code","51fd5a48":"code","a2e2542b":"code","30798388":"code","2bd64e86":"code","264d338f":"code","7044e8c6":"code","1d05e972":"code","8137b5a8":"code","afb4bfe0":"code","3cd8ca19":"code","85e0ef79":"code","f180579a":"code","52cbab2c":"code","62b636db":"code","96c8660b":"markdown","92cb4cf7":"markdown","9c2174cf":"markdown","0766883b":"markdown","581859cc":"markdown","12b7abe3":"markdown","6889c781":"markdown","41491ca6":"markdown","664b1b4c":"markdown","89e7e5a6":"markdown","526a24bf":"markdown","c4aa7c5b":"markdown","8d0afd23":"markdown","de75a466":"markdown"},"source":{"0d0d04f3":"# Author : Sagar Bapodara (This is my first submission in a kaggle competition)","e3026167":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re \nfrom nltk.corpus import stopwords \nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score \nprint('Dependencies Loaded Successfully')","2abce923":"# Importing the stopwords\nimport nltk\nnltk.download('stopwords')\nprint(stopwords.words('english')) #English Stopwords","24f7a1de":"news_data = pd.read_csv('..\/input\/fake-news\/train.csv')\nnews_data.head(10)","28f4cd75":"news_data.shape","e285b3c1":"print(news_data['label'].value_counts())","2a2ddc8c":"# Checking for missing values in the dataset\nnews_data.isnull().sum()","d942fa27":"# Replacing the null values with emtpy strings \nnews_data = news_data.fillna('')","2b5c4335":"# Checking for missing values again\nnews_data.isnull().sum()","2b08fe28":"#merging the author name and news title \nnews_data['content'] = news_data['author']+' '+news_data['title']","8fee604f":"print(news_data['content'])","3edacd5a":"# separating the data & label\nX = news_data.drop(columns='label', axis=1)\nY = news_data['label']","cb08ca57":"print(X)","01b66cda":"print(Y)","51fd5a48":"port_stem = PorterStemmer()","a2e2542b":"def stemming(content):\n    stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n    stemmed_content = stemmed_content.lower()\n    stemmed_content = stemmed_content.split()\n    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n    stemmed_content = ' '.join(stemmed_content)\n    return stemmed_content","30798388":"news_data['content'] = news_data['content'].apply(stemming)","2bd64e86":"print(news_data['content'])","264d338f":"# Seperating the data and the label \n\nX = news_data['content'].values\nY = news_data['label'].values","7044e8c6":"# converting the textual data into numerical data \nvectorizer = TfidfVectorizer()\nvectorizer.fit(X)\nX = vectorizer.transform(X)","1d05e972":"X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=2)","8137b5a8":"model = LogisticRegression()","afb4bfe0":"model.fit(X_train, Y_train)","3cd8ca19":"X_train_prediction = model.predict(X_train)","85e0ef79":"training_data_accuracy = accuracy_score(X_train_prediction, Y_train)","f180579a":" print('Accuracy score of training data :', training_data_accuracy)","52cbab2c":"X_test_prediction = model.predict(X_test)\ntesting_data_accuracy = accuracy_score(X_test_prediction, Y_test)","62b636db":" print('Accuracy score of testing data :', testing_data_accuracy)","96c8660b":"# Classification Plan :\n#### To use '*Title*' and '*Author*' data columns to make predictions","92cb4cf7":"# Importing Data ","9c2174cf":"#### 1 : Fake News, 0 : Real News","0766883b":"## If you found this useful, kindly upvote and comment your views :) ","581859cc":"# Training the Logistic Regression Model","12b7abe3":"# Basic Data Analysis","6889c781":"# Importing Dependencies","41491ca6":"#### A. On Training Data","664b1b4c":"# Model Evaluation","89e7e5a6":"# Vectorizing the content data","526a24bf":"# Splitting the dataset into Training(80%) and Testing(20%)","c4aa7c5b":"#### B. On Testing Data","8d0afd23":"# Stemming Process \n#### In short : Reducing a word to its root word, removing prefix and suffix ","de75a466":"#### Defining the stemmer function"}}