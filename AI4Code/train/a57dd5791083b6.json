{"cell_type":{"c9d01fe2":"code","b62ed19f":"code","23647557":"code","06a335b3":"code","dd6252e4":"code","60b29b3b":"code","759a20c1":"code","9cecda43":"code","b84b30c1":"code","1a5c60a7":"code","dd2a96b5":"code","9e7e925f":"code","a598bddc":"code","055f38e9":"code","246c9336":"code","9929641c":"code","d1da59fb":"code","faac1f3d":"code","dd1d7654":"code","ddaef433":"code","35283f59":"code","7ad969b3":"code","402f5a24":"code","4189c71b":"code","e49ff9b9":"code","b1f840ab":"code","5265e36e":"code","46d21782":"code","30b1cdde":"code","cba4bca3":"code","a0f327bc":"code","bc627f30":"code","624d41cb":"code","c980e2ba":"code","34e36eed":"code","3f0eccdf":"code","dd65b4a6":"code","04546389":"code","b3d2ac8e":"code","7b9a1ef2":"code","af0afe20":"code","78e7a1da":"code","653b75ca":"code","e381582b":"code","b0739ccb":"code","1cc13db8":"code","03c12c61":"code","263145c1":"code","5e173c6c":"code","0c82acb0":"code","0e91fb2d":"code","dcd3fffe":"code","f94845ae":"code","9e5628d1":"code","4880b653":"code","0af792ec":"code","2368ad5b":"code","43e5308f":"code","29085458":"code","8eff4be2":"code","74754fb3":"code","854f1746":"code","d27f1110":"code","120884fe":"code","12dac93f":"code","523c688c":"code","db44c161":"code","83902963":"code","ea718e27":"code","63f6c757":"code","9036da50":"code","d628dfe2":"code","e519b66a":"code","5de38ae8":"code","3f56489d":"code","d91ad081":"code","853b86d0":"code","6b86b1ed":"code","170bf832":"code","caa84ee8":"code","fcc3d77b":"code","caad796e":"code","6cf2af1f":"code","a3c836f1":"code","eaac659b":"code","6c70fe2c":"code","0dd05188":"code","02e95546":"code","8a7d3aad":"code","7684a480":"code","796bc56b":"code","f6eb9cf8":"code","2e64e7c0":"code","5d047ad6":"code","601bb322":"code","cee4814d":"code","3e2f8c8e":"code","b020cb7d":"code","e309748a":"code","43dc63f3":"code","344d25f0":"code","92c282b4":"code","fd0236e4":"code","1391b489":"code","8c115b59":"code","7a748926":"code","65afd13c":"code","83fe92c8":"code","a38ff45e":"code","9f7c9e9d":"code","e215bb2f":"code","49e5c598":"code","ec40d18e":"code","7b723490":"code","4c7f2627":"code","5f9a8801":"code","f56ce0ee":"code","63b579a6":"code","8115e2a0":"code","c48097c3":"code","de0820e6":"markdown","4c852b4f":"markdown","6c4457d5":"markdown","b44282d4":"markdown","8e8cb4f6":"markdown","ab0ed0a7":"markdown","c6bb1d44":"markdown","2b3f5c65":"markdown","7a4cb7ca":"markdown","5a6d6890":"markdown","d3fc2246":"markdown","1d60f52c":"markdown","15d0340b":"markdown","97d39543":"markdown","47b4d71d":"markdown","0fa3c4bd":"markdown","588e8a58":"markdown","931fc347":"markdown","a19ce987":"markdown","3421e2c3":"markdown","c21bab87":"markdown","3306f5d8":"markdown","073afd28":"markdown","8a1a5978":"markdown","675406e0":"markdown","1cf54fa2":"markdown","ae9fe80c":"markdown","5219ae37":"markdown"},"source":{"c9d01fe2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b62ed19f":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","23647557":"train.describe()","06a335b3":"train","dd6252e4":"test","60b29b3b":"train['SalePrice'].describe()","759a20c1":"sns.distplot(train['SalePrice'])\n","9cecda43":"sns.distplot(np.log(train['SalePrice']))","b84b30c1":"plt.boxplot(train['SalePrice'])\n\n# there are some outliers here i guess","1a5c60a7":"numeric_features = train.select_dtypes(include=[np.int, np.float])","dd2a96b5":"numeric_features.columns","9e7e925f":"cat_features = train.drop(numeric_features.columns, axis=1)","a598bddc":"cat_features.shape","055f38e9":"numeric_features.shape","246c9336":"#Dropping the column ID\nnumeric_features.drop('Id', inplace=True, axis= 1)","9929641c":"numeric_features","d1da59fb":"numeric_features.isnull().any()","faac1f3d":"['GarageYrBlt','MasVnrArea','LotFrontage']","dd1d7654":"for i in numeric_features.columns:\n    print(numeric_features[i].nunique())","ddaef433":"discrete_var = []\nfor i in numeric_features.columns:\n    if numeric_features[i].nunique()<= 15:\n        print(\"{}  has the \\n{}\".format(i,numeric_features[i].value_counts()))","35283f59":"# we will drop PoolArea as most of the values are zero\n","7ad969b3":"numeric_features.shape\n","402f5a24":"\nnumeric_features.corr()['SalePrice'].sort_values(ascending=True)","4189c71b":"f, ax =plt.subplots(figsize = (16,12))\nsns.heatmap(numeric_features.corr(), vmax = 1, square = True)","e49ff9b9":"numeric_features[['GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','SalePrice']].corr()","b1f840ab":"# Lets see correlation one more time , Here will emphasis on the variables which are more corelated to target columns and we will do \n#","5265e36e":"highcorr_numfr = list(numeric_features.corr()['SalePrice'].sort_values(ascending = False)[0:10].index.drop(['GarageArea','1stFlrSF']))","46d21782":"highcorr_numfr","30b1cdde":"\n#sns.pairplot(numeric_features[highcorr_numfr])","cba4bca3":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nsale_norm = ss.fit_transform(numeric_features['SalePrice'].values.reshape(-1,1))","a0f327bc":"sale_norm[sale_norm.reshape(1,-1).argsort()][0][0:5]","bc627f30":"sale_norm[sale_norm.reshape(1,-1).argsort()][0][-5:]","624d41cb":"# For GrLivArea, TotalBsmtSF, we have to chek with SalePrice","c980e2ba":"#f, ax = plt.figure(figsize=(16,12))\nsns.scatterplot(numeric_features['GrLivArea'], numeric_features['SalePrice'])","34e36eed":"numeric_features.drop(numeric_features['GrLivArea'].argsort()[-2:], inplace=True)","3f0eccdf":"numeric_features","dd65b4a6":"# Lets plot again and see ouliers are gone or not \nsns.scatterplot(numeric_features['GrLivArea'], numeric_features['SalePrice'])","04546389":"# Lets look the outlier of Total BsmtSF\nsns.scatterplot(numeric_features['TotalBsmtSF'], numeric_features['SalePrice'])","b3d2ac8e":"figure = plt.figure()\nfig = stats.probplot(numeric_features['TotalBsmtSF'], plot = plt)\nplt.show()","7b9a1ef2":"# I will delete the two data row which are outliers in 'GrLivAres', from data which will be used for categorical data analysis\ntrain.drop(numeric_features['GrLivArea'].argsort()[-2:], inplace=True)","af0afe20":"cat_features.columns","78e7a1da":"# Lets get the categorical data from train data set\ncat_features = train[cat_features.columns]","653b75ca":"cat_features.info()","e381582b":"\n# Lets see null how many percent is the null values of the data\nnull_percent =(cat_features.isnull().sum()\/cat_features.isnull().count()).sort_values(ascending= False)*100\n\n","b0739ccb":"null_percent.index[:14]","1cc13db8":"cat_features.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond',\n       'GarageQual', 'GarageFinish', 'GarageType', 'BsmtFinType2',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond'], axis=1,inplace=True)","03c12c61":"cat_features.isnull().sum().sort_values()","263145c1":"# There are now two cloumns which contains the null values and less than 9 row, So wildrop those row\ncat_features.dropna(axis=0, inplace=True)","5e173c6c":"# Lets calculate the number unique of categories predent inside each column\nn_unique = {}\nfor i in cat_features.columns:\n    n_unique[i] = cat_features[i].nunique()","0c82acb0":"n_unique","0e91fb2d":"cat_features['MSZoning'].value_counts()[:3]","dcd3fffe":"d = {}\nfor i in cat_features.columns:\n    d[i] =cat_features[i].value_counts()[:3]\n    ","f94845ae":"d","9e5628d1":"for i in ['ExterQual','ExterCond','HeatingQC','KitchenQual']:\n    print(cat_features[i].value_counts())","4880b653":"cat_features[['ExterQual','ExterCond','HeatingQC','KitchenQual']]\n","0af792ec":"def encoder(x):\n    if x == 'Ex':\n        return 4\n    elif x == 'Gd':\n        return 3\n    elif x == 'TA':\n        return 2\n    elif x == 'Fa':\n        return 1\n    elif x == 'PO' or x == 'Po':\n        return 0\n    else:\n        return x","2368ad5b":"# We have done lable encoder manually on the columns\nfor i in ['ExterQual','ExterCond','HeatingQC','KitchenQual']:\n    cat_features[i] = cat_features[i].map(encoder)","43e5308f":"# We have converted this 'Y' and 'N' into 1 and 0\ncat_features['CentralAir'] =cat_features['CentralAir'].map(lambda x: 1 if x=='Y'else 0)","29085458":"# Now deal with Paved Drive which has three categories\ndef ypn(x):\n    if x=='Y':\n        return 2\n    elif x == 'P':\n        return 1\n    elif x == 'N':\n        return 0\n    else:\n        return x","8eff4be2":"cat_features['PavedDrive'] = cat_features['PavedDrive'].map(ypn)","74754fb3":"cat_features[['ExterQual','ExterCond','HeatingQC','KitchenQual','PavedDrive','CentralAir']]","854f1746":"# First get the coulmn name which are remained to preprocess\nremaining = cat_features.columns.drop(['ExterQual','ExterCond','HeatingQC','KitchenQual','PavedDrive','CentralAir'])","d27f1110":"remaining","120884fe":"dummies = pd.get_dummies(cat_features[remaining])","12dac93f":"dummies","523c688c":"# We will join the cat_features and dummies\ncat_features = cat_features.join(other = dummies)","db44c161":"# Lets drop the columns , from which we have got the dummies i.e. remaining\ncat_features.drop(remaining, inplace= True, axis=1)","83902963":"numeric_features","ea718e27":"cat_features","63f6c757":"final_train =cat_features.join(other = numeric_features, on = cat_features.index, how= 'inner')","9036da50":"final_train.fillna","d628dfe2":"# Still there are some null values in LotFrontage,GarageYrBlt, Which we havenot checked in numerica feature exploration, mistake happend, Lets correct now\nfinal_train['LotFrontage']=final_train['LotFrontage'].fillna(final_train['LotFrontage'].median())","e519b66a":"final_train['GarageYrBlt'] = final_train['GarageYrBlt'].fillna(final_train['GarageYrBlt'].median())","5de38ae8":"final_train[['GarageYrBlt','MasVnrArea','LotFrontage']].isna().sum()","3f56489d":"final_train","d91ad081":"test.drop(['Id','PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond',\n       'GarageQual', 'GarageFinish', 'GarageType', 'BsmtFinType2',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond'], axis=1,inplace=True)","853b86d0":"test.isnull().sum()","6b86b1ed":"train['MSZoning'].value_counts()","170bf832":"test['MSZoning'] = test['MSZoning'].fillna('RL')\n# Filling the 'RL' value in missing place , RL is most common","caa84ee8":"train['LotArea'].median()","fcc3d77b":"train1 = train.copy()","caad796e":"train","6cf2af1f":"#remove outliers\ntrain.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\ntrain.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","a3c836f1":"train1 = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest1 = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","eaac659b":"#remove outliers\ntrain1.drop(train1[(train1['OverallQual']<5) & (train1['SalePrice']>200000)].index, inplace=True)\ntrain1.drop(train1[(train1['GrLivArea']>4500) & (train1['SalePrice']<300000)].index, inplace=True)\ntrain1.reset_index(drop=True, inplace=True)","6c70fe2c":"train_labels = train1['SalePrice']","0dd05188":"full_features = pd.concat((train1,test1))\nfull_features.drop('SalePrice', axis=1, inplace=True)","02e95546":"full_features.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond',\n       'GarageQual', 'GarageFinish', 'GarageType', 'BsmtFinType2',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond'], axis=1,inplace=True)","8a7d3aad":"for i in ['ExterQual','ExterCond','HeatingQC','KitchenQual']:\n    full_features[i] = full_features[i].map(encoder)","7684a480":"full_features['CentralAir'] = full_features['CentralAir'].map(lambda x: 1 if x=='Y'else 0)","796bc56b":"full_features['PavedDrive'] = full_features['PavedDrive'].map(ypn)","f6eb9cf8":"full_features.isnull().sum()","2e64e7c0":"full_features[numeric_features.columns.drop('SalePrice')].isnull().sum()","5d047ad6":"numeric_features.columns.drop('SalePrice')","601bb322":"full_features[numeric_features.columns.drop('SalePrice')].isnull().sum()","cee4814d":"for i in numeric_features.columns.drop('SalePrice'):\n    full_features[i] = full_features[i].fillna(train[i].median())","3e2f8c8e":"full_features['SaleType'].value_counts()","b020cb7d":"full_features.isna().sum()","e309748a":"full_features['MSZoning'].isna().sum()","43dc63f3":"full_features['MSZoning'].value_counts().index[0]","344d25f0":"full_features['MSZoning'].fillna(full_features['MSZoning'].value_counts().index[0]).isna().sum()","92c282b4":"for i in full_features.columns:\n    full_features[i]=full_features[i].fillna(full_features[i].value_counts().index[0])","fd0236e4":"full_features = pd.get_dummies(full_features).reset_index(drop = True)","1391b489":"full_features.drop('Id',axis = 1, inplace=True)","8c115b59":"train2 = full_features.iloc[:len(train1),:]","7a748926":"train2['SalePrice'] = train1['SalePrice']","65afd13c":"test2 = full_features.iloc[len(train1):,:]","83fe92c8":"from sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.model_selection import KFold, GridSearchCV, train_test_split,cross_val_score\nfrom sklearn.metrics import mean_squared_error\n","a38ff45e":"X = train2.drop('SalePrice', axis=1)\ny = train2['SalePrice']","9f7c9e9d":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 42)\nr_scale = RobustScaler().fit(X_train)\nX_train_scale = r_scale.transform(X_train)\nX_test_scale = r_scale.transform(X_test)\nlr = LinearRegression().fit(X_train_scale, y_train)\ny_pred = lr.predict(X_test_scale)\nmean_squared_error(y_test, y_pred)","e215bb2f":"# I will use cross_validation, of the 10 folds \nkfold = KFold(n_splits = 10, random_state=42, shuffle=True)","49e5c598":"tree_reg = RandomForestRegressor()","ec40d18e":"#parameters for the Grid Search CV\npara = {'n_estimators':[700,500,200],'max_depth':[50,80]}\n","7b723490":"forest_reg = GridSearchCV(tree_reg, param_grid=para, cv= kfold)","4c7f2627":"forest_reg.fit(X, y)","5f9a8801":"forest_reg.best_params_","f56ce0ee":"forest_reg.best_score_","63b579a6":"y_pred2 = forest_reg.predict(test2)","8115e2a0":"output = pd.DataFrame({'Id':test1['Id'],'SalePrice':y_pred2})","c48097c3":"output.to_csv('random_forest_solution', index= False)","de0820e6":"Lets See the Realtion between the variables","4c852b4f":"We have do all the transformation on test set also, Lets do it.\n","6c4457d5":"lets see wich are the discrete variable, we will apply the criteria of 15, i,e, \nThe variable which has less than 15 diffrent values we will consider them as a discrete variables","b44282d4":"This Linear model has given me 0.69 score on test set, which is not enough good, So I would check different model","8e8cb4f6":"Now lets deal with remaining categorical features, We will do OneHotEncoding","ab0ed0a7":"There is no null values in the numeric features","c6bb1d44":"* To deal with categorical features we have two option 1- Ordinal Encoder, 2- OneHotEncoder\n* Thise are two class methodes from sklearn.preprocessing\n1. Ordeinal Encoder is for the categoeries like [worst, bad, good, excelent]\n2. OneHotEncoder will create new colummns with each features and put 1 or 0 if corresponding feature is present on not for the given instance.\n\n","2b3f5c65":"\n\n1.There is high correlation between independent variable itself Garage Cars, Garage Area obivious garage area pedend on number of cars\nwe can park, So we need to drop one of the variable from it (because of Data Redundancy)\n2. TotalBsmtSF and 1stFlrSF are highly correlated to each other Lets see in detailed below","7a4cb7ca":"There is outlier in GrLivArea, Lets get there locations","5a6d6890":"# 2 Lets look at the Categorical features\n","d3fc2246":"* PoolQC, MiscFeature, Alley, Fence has more than 75% null values in there columns, So we can ingore them\nFireplaceQU is column represeting firelace quality but lots of houses are't have fire place, So in my openion it is better to drop it\n* Also, GarageCond, GarageQual...... and BsmtFinType2.... Also have null values for the same data row, where the Basement and garage is not availabe in the house or we don't have data for same, But we already have numeric features for the Basemanr and Garage,\nWe will drop them also for now...(we will chek definetly come here if we require )\n* So take away from here is **DROP \n            ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageCond',\n       'GarageQual', 'GarageFinish', 'GarageType', 'BsmtFinType2',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond']**","1d60f52c":"Lets work with missing Data","15d0340b":"Here are the step toward this problem explained in Appendix B [Hands-On Machine Learning with Scikit-Learn, Keras & TensorFlow](http:\/\/https:\/\/www.amazon.in\/Hands-Machine-Learning-Scikit-Learn-Tensor\/dp\/9352139054\/ref=asc_df_9352139054\/?tag=googleshopdes-21&linkCode=df0&hvadid=397006573861&hvpos=&hvnetw=g&hvrand=1830573946496308315&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=1007789&hvtargid=pla-838380762468&psc=1&ext_vrnc=hi)\n1. Frame the Problem and Look at the Big picture (Which is Done by Kaggle for us)\n2. Get The data (Again Thanks to Kaggle & and those... who has actually callected it)\n3. Explore the Data (This is the first step which we will actually do)\n4. Prepare the Data (step 3 and 4 is the part of EDA)\n5. Shortlist the Promising Models\n6. Prepare and Present your solution.\n7. Launch it","97d39543":"> So there are some descrite values","47b4d71d":"We can find two important inferences from above\n* 1 it doesn't contain any numeric features (except target)\n* 2 There are null values present ","0fa3c4bd":"**Now its time to check the categories and there trend**","588e8a58":"Now we will see the top three categories by number of there instances presented  ","931fc347":"I fill like ther are some outliers in SalePrice, GrLivArea, TotalBsmtSF, Lets dive into that","a19ce987":"Lets check the data with in itself, for that i will split the train data into two parts to check the score","3421e2c3":"* Lets do EDA of the numeric features","c21bab87":"This four feature (ExterQual,ExterCond,HeatingQC,KitchenQual) contains the five catogaries as followes\n \n Ex = Excellent , Gd = Good, TA = Typical\/Average, Fa = Fair, PO = Poor\n This kind of fratures we can transform into Ordinal Encoding","3306f5d8":"cat_features are the categorical features","073afd28":"As we see log function distribution of the sales price is nomale, Need to look into that","8a1a5978":"GarageCars is more correlated to target compaired to Garage Area so we will drop GarageArea, same for 1stFlrSF\n","675406e0":"Now we have delt with ExterQual,ExterCond,HeatingQC,KitchenQual Which depict the quality \nThere two more features CentralAir and PavedDrive\n","1cf54fa2":"As we see the shape of the numeric and categorical features , There are difference , we havn't deleted the rows where catfeatures have null values, So we will do here inner join (Nice idea right)","ae9fe80c":"There is outlier at highend but right now lets egnore them","5219ae37":"\n\n\n# Exploratory Data Analysis"}}